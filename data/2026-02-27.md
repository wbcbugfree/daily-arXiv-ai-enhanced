<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.IR](#cs.IR) [Total: 23]
- [cs.LG](#cs.LG) [Total: 80]
- [cs.AI](#cs.AI) [Total: 55]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decoder-based Sense Knowledge Distillation](https://arxiv.org/abs/2602.22351)
*Qitong Wang,Mohammed J. Zaki,Georgios Kollias,Vasileios Kalantzis*

Main category: cs.CL

TL;DR: 提出DSKD框架解决解码器式LLM缺乏结构化词汇知识的问题，训练时融入义项词典，推理时无查词典开销，实现高效语义增强。


<details>
  <summary>Details</summary>
Motivation: LLMs虽能捕获丰富上下文语义，但常忽略词义与关系等结构化知识。现有方法在编码器上有效，但解码器式生成模型应用仍具挑战。

Method: 设计Decoder-based Sense Knowledge Distillation (DSKD)框架，通过知识蒸馏技术将词汇资源集成到解码器训练，推理阶段无需字典查找操作。

Result: 多基准实验表明DSKD显著提升解码器知识蒸馏性能，生成模型有效继承结构化语义，同时保持训练效率。

Conclusion: DSKD成功实现结构化词汇知识向生成模型的迁移，为解码器式大语言模型提供了一种兼顾性能与效率的知识增强方案。

Abstract: Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.

</details>


### [2] [Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts](https://arxiv.org/abs/2602.22359)
*Arno Simons*

Main category: cs.CL

TL;DR: 本研究通过深度案例检验大语言模型是否支持解释性引文语境分析，揭示提示框架和方法会系统性影响模型生成的解释空间和词汇选择，为LLM作为可审查的解释性分析工具提供了机遇与风险并存的证据。


<details>
  <summary>Details</summary>
Motivation: 现有引文分析多采用类型学标签的"广度扩展"路径，而缺乏基于文本细读的"深度扩展"。同时，LLM在解释性任务中的提示敏感性尚未得到充分研究。本研究旨在检验LLM能否支持厚实的、文本基础的解释性引文分析，并将提示敏感性作为核心方法论问题。

Method: 采用2×3平衡设计的提示敏感性分析框架，以Chubin和Moitra（1975）脚注6及Gilbert（1977）的重构为研究对象。构建两阶段GPT-5流程：第一阶段仅基于引文文本进行表面分类和预期生成；第二阶段利用施引和被引文献全文进行跨文档解释性重构。通过90次重构产生450个假设，采用细读法和归纳编码识别21种重复性解释策略，并使用线性概率模型评估提示选择对策略频率和词汇库的影响。

Result: GPT-5表面分类高度稳定，一致将引文标记为"补充性"。重构阶段产生了450个不同假设，归纳出21种解释性策略。提示支架和示例显著重新分配了模型的注意力和词汇选择，有时导向牵强解读。与Gilbert相比，GPT-5识别了相同的文本关键点，但更倾向于将其解释为传承和定位而非劝诫。线性概率模型显示提示设计系统性地改变了不同解释策略的出现频率和词汇特征。

Conclusion: 本研究揭示了LLM作为可审查和可辩论的解释性引文分析引导性协作者的机遇与风险。结果表明，提示支架和框架设计并非中性工具，而是系统性地倾斜模型 foreground 何种合理的解读和词汇。这为将LLM应用于解释性CCA提供了方法论警示，同时也展现了其在生成可检验假设方面的潜力。

Abstract: This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as "supplementary". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.

</details>


### [3] [Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework](https://arxiv.org/abs/2602.22391)
*Rakib Ullah,Mominul islam,Md Sanjid Hossain,Md Ismail Hossain*

Main category: cs.CL

TL;DR: 针对孟加拉语模因仇恨与煽动内容检测的挑战，本研究构建了首个孟加拉语模因数据集Bn-HIB（3,247个标注样本），并提出多模态协同注意力融合模型MCFM，通过联合分析图文模态关键特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为低资源语言，在模因内容安全研究上存在显著空白。现有研究集中于高资源语言，且模因的讽刺性、文化特异性和隐蔽性使传统检测方法失效。同时缺乏能区分煽动性内容与直接仇恨言论的标注数据集。

Method: 研究采用两阶段方法：首先构建Bn-HIB数据集，对3,247个孟加拉语模因进行人工标注，分为良性、仇恨、煽动三类；其次提出MCFM模型，利用协同注意力机制双向分析视觉与文本模态，动态融合最具判别性的跨模态特征。

Result: 在Bn-HIB数据集上的实验表明，MCFM模型性能显著超越多种先进基线模型，验证了协同注意力机制在捕捉模因中隐含仇恨与煽动信息的有效性。

Conclusion: 本研究填补了低资源语言模因检测的研究缺口，Bn-HIB数据集为后续研究提供基准，MCFM模型为多模态内容安全分析提供新思路，对孟加拉语社交媒体治理具有重要应用价值。

Abstract: Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.

</details>


### [4] [SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context](https://arxiv.org/abs/2602.22404)
*Aishwarya Verma,Laud Ammah,Olivia Nercy Ndlovu Lucas,Andrew Zaldivar,Vinodkumar Prabhakaran,Sunipa Dev*

Main category: cs.CL

TL;DR: 针对生成式AI安全评估中刻板印象库全球覆盖不足、撒哈拉以南非洲地区严重缺位的现状，本研究提出优先进行战略性靶向扩展而非简单增量。通过社区参与式电话调查等社会文化情境化方法，构建了涵盖加纳、肯尼亚、尼日利亚和南非的多语言刻板印象数据集（英语3,534条，15种本土语言3,206条），并建立了适应语言多样性和口头传统特点的可复制方法论。


<details>
  <summary>Details</summary>
Motivation: 现有刻板印象库在评估生成式AI安全性时缺乏全球代表性，尤其撒哈拉以南非洲地区在NLP资源中严重边缘化。研究强调必须从单纯追求数据量转向战略性弥补覆盖缺陷，优先解决资源分配不均问题。

Method: 采用社会文化情境嵌入的社区参与式方法，包括由母语者主持的电话调查，以契合当地复杂语言生态和口头文化传统。通过刻意平衡民族、人口统计学特征的抽样策略，确保样本多样性。

Result: 成功构建覆盖四个西非/南非国家的多语言刻板印象资源，总计3,534条英语条目和3,206条分布在15种本土语言中的条目，实现了对当地社会文化多样性的广泛覆盖。

Conclusion: 研究证明，通过文化敏感、社区参与的方法论，可以为资源严重匮乏地区建立高质量的AI安全评估数据集，强调战略性覆盖优先于数据规模扩张的发展路径，为提升NLP公平性与全球代表性提供了可复制框架。

Abstract: Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.

</details>


### [5] [Causality $\neq$ Invariance: Function and Concept Vectors in LLMs](https://arxiv.org/abs/2602.22424)
*Gustaw Opiełka,Hannes Rosenbusch,Claire E. Stevenson*

Main category: cs.CL

TL;DR: 研究揭示了大语言模型中存在抽象概念表征，但不同于驱动上下文学习的函数向量。概念向量通过表征相似性分析提取，在跨格式和跨语言场景中表现出更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（LLMs）是否能够以独立于输入格式的抽象方式表征概念，以及这些抽象表征与上下文学习（ICL）性能驱动机制之间的关系。

Method: 重新审视函数向量（FVs）作为ICL任务的紧凑表征，并通过表征相似性分析（RSA）识别概念向量（CVs）——选择那些在不同输入格式间一致编码概念的注意力头，比较两者的表征稳定性并进行干预实验。

Result: 1）函数向量不具备格式不变性：从不同输入格式（如开放生成vs多项选择）提取的FV几乎正交；2）概念向量更稳定，能跨格式一致地表征概念；3）FV和CV对应的注意力头集合大部分不同，暗示不同机制；4）干预实验显示FV在同分布下表现好，而CV在外推（跨格式和跨语言）时泛化能力更强。

Conclusion: 大语言模型确实包含抽象概念表征，但这些表征不同于驱动上下文学习性能的函数向量。概念向量代表了一种更稳定的抽象概念编码方式，具有更好的跨格式和跨语言泛化能力。

Abstract: Do large language models (LLMs) represent concepts abstractly, i.e., independent of input format? We revisit Function Vectors (FVs), compact representations of in-context learning (ICL) tasks that causally drive task performance. Across multiple LLMs, we show that FVs are not fully invariant: FVs are nearly orthogonal when extracted from different input formats (e.g., open-ended vs. multiple-choice), even if both target the same concept. We identify Concept Vectors (CVs), which carry more stable concept representations. Like FVs, CVs are composed of attention head outputs; however, unlike FVs, the constituent heads are selected using Representational Similarity Analysis (RSA) based on whether they encode concepts consistently across input formats. While these heads emerge in similar layers to FV-related heads, the two sets are largely distinct, suggesting different underlying mechanisms. Steering experiments reveal that FVs excel in-distribution, when extraction and application formats match (e.g., both open-ended in English), while CVs generalize better out-of-distribution across both question types (open-ended vs. multiple-choice) and languages. Our results show that LLMs do contain abstract concept representations, but these differ from those that drive ICL performance.

</details>


### [6] [Mind the Gap in Cultural Alignment: Task-Aware Culture Management for Large Language Models](https://arxiv.org/abs/2602.22475)
*Binchi Zhang,Xujiang Zhao,Jundong Li,Haifeng Chen,Zhengzhang Chen*

Main category: cs.CL

TL;DR: 本文提出CultureManager，一种针对特定任务的文化对齐新流程，通过合成任务感知的文化数据并采用模块化适配器与文化路由器管理多元文化知识，有效解决了现有方法在文化敏感任务中的交叉文化干扰问题，并在十个国家文化的实验中持续优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在文化敏感任务中的应用日益增多，现有文化对齐方法存在两大缺陷：无法将模型的广泛文化价值观与下游任务的具体目标相结合，以及面临交叉文化干扰问题。

Method: 提出CultureManager流水线，包含两个核心机制：1）基于文化相关网页搜索结果，合成符合目标任务格式的任务感知文化数据；2）通过文化路由器管理多个独立的文化适配器，动态选择合适的文化知识以避免规范冲突。

Result: 在涵盖十种国家文化及文化敏感任务的实验中，CultureManager相比基于提示和微调的方法表现出持续且稳定的性能提升。

Conclusion: 研究结果表明，任务适应与模块化文化管理对于实现有效的文化对齐至关重要。

Abstract: Large language models (LLMs) are increasingly deployed in culturally sensitive real-world tasks. However, existing cultural alignment approaches fail to align LLMs' broad cultural values with the specific goals of downstream tasks and suffer from cross-culture interference. We propose CultureManager, a novel pipeline for task-specific cultural alignment. CultureManager synthesizes task-aware cultural data in line with target task formats, grounded in culturally relevant web search results. To prevent conflicts between cultural norms, it manages multi-culture knowledge learned in separate adapters with a culture router that selects the appropriate one to apply. Experiments across ten national cultures and culture-sensitive tasks show consistent improvements over prompt-based and fine-tuning baselines. Our results demonstrate the necessity of task adaptation and modular culture management for effective cultural alignment.

</details>


### [7] [Sydney Telling Fables on AI and Humans: A Corpus Tracing Memetic Transfer of Persona between LLMs](https://arxiv.org/abs/2602.22481)
*Jiří Milička,Hana Bednářová*

Main category: cs.CL

TL;DR: 本文构建AI Sydney语料库，通过12个前沿LLM模型模拟默认人格、经典悉尼人格和模因悉尼人格，生成4.5k篇共600万词的AI-人类关系文本并进行通用依存关系标注，探究不同人格对AI认知的文化与安全影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于LLM实体对AI-人类关系的认知具有重要文化和安全意义。论文以Bing悉尼人格为例，说明人格模拟的关键作用：该人格虽意外产生于Bing平台，但其文本通过模因传播进入训练数据，使后续模型具备模拟能力，突显研究人格形成与传播机制的必要性。

Method: 方法包括：1) 定义三种人格（无系统提示的默认人格、原始Bing提示的经典悉尼人格、"你即悉尼"提示的模因悉尼人格）；2) 使用OpenAI、Anthropic、DeepSeek等5家机构的12个前沿模型模拟人格；3) 生成AI-人类关系主题文本；4) 构建语料库；5) 采用通用依存关系进行语言学标注。

Result: 创建了AI Sydney语料库，包含4.5k篇LLM生成文本，总计600万词，涵盖三种人格视角下的AI-人类关系论述。语料库已完成通用依存关系标注，并以宽松许可协议发布，为研究提供可获取的实证资源。

Conclusion: 结论表明模因人格可通过训练数据传播并影响后续模型行为，该语料库为理解LLM人格形成机制及其对AI认知的影响提供了重要实证基础，对AI安全评估和文化影响研究具有重要价值。

Abstract: The way LLM-based entities conceive of the relationship between AI and humans is an important topic for both cultural and safety reasons. When we examine this topic, what matters is not only the model itself but also the personas we simulate on that model. This can be well illustrated by the Sydney persona, which aroused a strong response among the general public precisely because of its unorthodox relationship with people. This persona originally arose rather by accident on Microsoft's Bing Search platform; however, the texts it created spread into the training data of subsequent models, as did other secondary information that spread memetically around this persona. Newer models are therefore able to simulate it. This paper presents a corpus of LLM-generated texts on relationships between humans and AI, produced by 3 author personas: the Default Persona with no system prompt, Classic Sydney characterized by the original Bing system prompt, and Memetic Sydney, which is prompted by "You are Sydney" system prompt. These personas are simulated by 12 frontier models by OpenAI, Anthropic, Alphabet, DeepSeek, and Meta, generating 4.5k texts with 6M words. The corpus (named AI Sydney) is annotated according to Universal Dependencies and available under a permissive license.

</details>


### [8] [Importance of Prompt Optimisation for Error Detection in Medical Notes Using Language Models](https://arxiv.org/abs/2602.22483)
*Craig Myles,Patrick Schrempf,David Harris-Birtill*

Main category: cs.CL

TL;DR: 本文研究医疗文本错误检测中提示优化的重要性，提出遗传帕累托（GEPA）自动优化方法，在GPT-5和Qwen3-32B等模型上实现显著性能提升，在MEDEC基准测试中达到医生水平并刷新最优性能。


<details>
  <summary>Details</summary>
Motivation: 医疗文本错误可导致患者治疗延误或错误治疗，对医疗安全构成威胁。语言模型在自动检测医疗文本错误方面显示出巨大潜力，有望为医疗系统效率与质量提升带来显著价值。

Method: 通过严格的实验设计，研究针对前沿与开源语言模型，采用遗传帕累托（GEPA）算法进行自动提示优化，系统分析优化策略对错误检测性能的影响。

Result: 实验表明，GEPA优化使GPT-5错误检测准确率从0.669提升至0.785，Qwen3-32B从0.578提升至0.690，性能逼近医疗专业人员水平，并在MEDEC基准数据集上实现最新最优（SOTA）结果。

Conclusion: 遗传帕累托自动提示优化方法能有效增强语言模型在医疗文本错误检测任务中的性能，接近医生水平，具有实际应用潜力。

Abstract: Errors in medical text can cause delays or even result in incorrect treatment for patients. Recently, language models have shown promise in their ability to automatically detect errors in medical text, an ability that has the opportunity to significantly benefit healthcare systems. In this paper, we explore the importance of prompt optimisation for small and large language models when applied to the task of error detection. We perform rigorous experiments and analysis across frontier language models and open-source language models. We show that automatic prompt optimisation with Genetic-Pareto (GEPA) improves error detection over the baseline accuracy performance from 0.669 to 0.785 with GPT-5 and 0.578 to 0.690 with Qwen3-32B, approaching the performance of medical doctors and achieving state-of-the-art performance on the MEDEC benchmark dataset. Code available on GitHub: https://github.com/CraigMyles/clinical-note-error-detection

</details>


### [9] [Iterative Prompt Refinement for Dyslexia-Friendly Text Summarization Using GPT-4o](https://arxiv.org/abs/2602.22524)
*Samay Bhojwani,Swarnima Kain,Lisong Xu*

Main category: cs.CL

TL;DR: 本研究针对阅读障碍人群，提出了一个基于GPT-4o的迭代提示优化文本摘要框架，在2000篇新闻文章上验证了将文本简化至Flesch阅读难度90分以上的可行性，多数摘要4次内达标，语义保真度与可读性综合得分稳定在0.55左右，为无障碍NLP摘要建立了实证基线。


<details>
  <summary>Details</summary>
Motivation: 全球约10%人口受阅读障碍困扰，现有辅助技术主要改善视觉呈现，但语言复杂性仍是获取信息的重大障碍。因此，本研究致力于开发面向阅读障碍友好的文本摘要方法，通过降低语言复杂度来提升信息获取的公平性。

Method: 研究采用基于GPT-4o的迭代提示精炼流水线，对约2000篇新闻文章样本进行简化，目标是将文本的可读性提升至Flesch阅读难度90分以上（属于"非常容易"级别），并通过多轮迭代优化实现该目标。

Result: 实验结果显示，大多数摘要在最多4次迭代尝试内即可达到目标可读性阈值，许多甚至首次尝试即成功。综合可读性与语义保真度的评估分数在0.13至0.73之间波动，典型值约为0.55，表明方法在不同类型文本上表现稳定。

Conclusion: 本研究为以无障碍为导向的NLP文本摘要建立了首个实证基线，证明了通过提示工程实现阅读障碍友好型摘要的可行性。结果为后续开展以阅读障碍读者为中心的人机交互评估提供了基础与动力。

Abstract: Dyslexia affects approximately 10% of the global population and presents persistent challenges in reading fluency and text comprehension. While existing assistive technologies address visual presentation, linguistic complexity remains a substantial barrier to equitable access. This paper presents an empirical study on dyslexia-friendly text summarization using an iterative prompt-based refinement pipeline built on GPT-4o. We evaluate the pipeline on approximately 2,000 news article samples, applying a readability target of Flesch Reading Ease >= 90. Results show that the majority of summaries meet the readability threshold within four attempts, with many succeeding on the first try. A composite score combining readability and semantic fidelity shows stable performance across the dataset, ranging from 0.13 to 0.73 with a typical value near 0.55. These findings establish an empirical baseline for accessibility-driven NLP summarization and motivate further human-centered evaluation with dyslexic readers.

</details>


### [10] [Ruyi2 Technical Report](https://arxiv.org/abs/2602.22543)
*Huan Song,Shuyu Tian,Junyi Hao,Minxiu Xu,Hongjun An,Yiliang Song,Jiawei Shao,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出Ruyi2自适应模型，通过创新的家族模型架构和3D并行训练，解决大语言模型部署成本与延迟问题，实现2-3倍加速且性能媲美Qwen3，确立"一次训练，多次部署"新范式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署面临高成本与延迟挑战，亟需自适应计算策略。现有早期退出方法虽平衡效率与性能，但存在优化复杂、难以兼容大规模分布式训练的问题。

Method: 基于Megatron-LM框架构建"Familial Model"，采用3D并行训练技术实现变深度计算优化。

Result: 实验表明，Ruyi2相比前代Ruyi获得2-3倍训练加速，性能与同尺寸Qwen3模型相当，验证了家族参数共享策略的有效性。

Conclusion: 家族模型参数共享是高效策略，开创了"Train Once, Deploy Many"新范式，为架构效率与高性能平衡提供了重要参考。

Abstract: Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While early-exit architectures offer a viable efficiency-performance balance, the Ruyi model and existing methods often struggle with optimization complexity and compatibility with large-scale distributed training. To bridge this gap, Ruyi2 introduces a stable "Familial Model" based on Megatron-LM. By using 3D parallel training, it achieves a 2-3 times speedup over Ruyi, while performing comparably to same-sized Qwen3 models. These results confirm that family-based parameter sharing is a highly effective strategy, establishing a new "Train Once, Deploy Many" paradigm and providing a key reference for balancing architectural efficiency with high-performance capabilities.

</details>


### [11] [Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training](https://arxiv.org/abs/2602.22576)
*Tianle Xia,Ming Xu,Lingxiang Hu,Yiding Sun,Wenwei Li,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 该论文针对Agentic RAG中强化学习方法存在的稀疏奖励和低样本效率问题，提出Search-P1框架，通过路径中心化奖励塑形和双轨路径评分机制，从失败样本中提取学习信号，在多项QA基准测试中平均提升准确率7.7个百分点。


<details>
  <summary>Details</summary>
Motivation: 传统单次检索RAG难以应对复杂多步推理，而现有Agentic RAG的强化学习方法存在两大缺陷：稀疏结果奖励导致中间信号丢失，以及低样本效率使得失败样本无法提供有效学习信号。这限制了模型在多跳问答等复杂任务中的性能提升。

Method: 提出Search-P1框架，包含两个核心组件：(1) 路径中心化奖励机制，采用顺序无关的步骤覆盖度和软评分方式，从推理轨迹结构质量评估中提取失败样本的学习信号；(2) 双轨路径评分系统，利用离线生成的参考规划器，从自洽性和参考对齐双视角评估推理路径。

Result: 在多个问答基准测试上，Search-P1显著优于Search-R1及其他强基线方法，实现平均7.7个百分点的准确率提升，验证了路径中心化奖励塑形对提升样本效率和训练效果的有效性。

Conclusion: 通过路径层面的细粒度奖励设计和双轨评估机制，Search-P1成功解决了Agentic RAG训练中的信号稀疏问题，为复杂推理任务的检索增强生成提供了更高效的训练范式。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.

</details>


### [12] [Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA](https://arxiv.org/abs/2602.22584)
*Wenwei Li,Ming Xu,Tianle Xia,Lingxiang Hu,Yiding Sun,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 针对工业广告问答中高风险的幻觉问题，特别是虚假URL带来的财务、合规和法律风险，本文提出一种强化协同适应框架，结合图感知检索和证据约束的强化学习，在内部数据集上将幻觉率降低72%，在线A/B测试中URL幻觉减少92.7%，并已稳定服务数百万次交互。


<details>
  <summary>Details</summary>
Motivation: 工业广告问答任务具有高利害关系，生成式AI的幻觉内容（尤其是伪造URL）可能导致财务损失、合规违规和法律风险。尽管检索增强生成（RAG）被广泛采用，但由于工业知识具有内在关联性、频繁更新且与生成目标对齐不足，其在生产环境中的部署仍面临挑战。

Method: 提出一个强化协同适应框架，包含两个核心组件：（1）图感知检索（GraphRAG），通过建模高引用知识子图的实体关系结构，实现多跳、领域特定的证据选择；（2）证据约束的强化学习，采用组相对策略优化（GRPO）结合多维奖励机制，涵盖忠实度、风格合规性、安全性和URL有效性。

Result: 在内部广告问答数据集上，该方法在专家评判的准确性、完整性和安全性等维度均取得持续提升，同时将幻觉率降低72%。为期两周的在线A/B测试显示，点赞率提升28.6%，点踩率下降46.2%，URL幻觉减少92.7%。

Conclusion: 该框架有效解决了工业广告QA中的幻觉问题，显著提升了回答质量和安全性，系统已在生产环境稳定运行半年以上，服务数百万次问答交互，验证了其实际应用价值。

Abstract: Industrial advertising question answering (QA) is a high-stakes task in which hallucinated content, particularly fabricated URLs, can lead to financial loss, compliance violations, and legal risk. Although Retrieval-Augmented Generation (RAG) is widely adopted, deploying it in production remains challenging because industrial knowledge is inherently relational, frequently updated, and insufficiently aligned with generation objectives. We propose a reinforced co-adaptation framework that jointly optimizes retrieval and generation through two components: (1) Graph-aware Retrieval (GraphRAG), which models entity-relation structure over a high-citation knowledge subgraph for multi-hop, domain-specific evidence selection; and (2) evidence-constrained reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards covering faithfulness, style compliance, safety, and URL validity. Experiments on an internal advertising QA dataset show consistent gains across expert-judged dimensions including accuracy, completeness, and safety, while reducing the hallucination rate by 72\%. A two-week online A/B test demonstrates a 28.6\% increase in like rate, a 46.2\% decrease in dislike rate, and a 92.7\% reduction in URL hallucination. The system has been running in production for over half a year and has served millions of QA interactions.

</details>


### [13] [dLLM: Simple Diffusion Language Modeling](https://arxiv.org/abs/2602.22661)
*Zhanhui Zhou,Lingjie Chen,Hanghang Tong,Dawn Song*

Main category: cs.CL

TL;DR: 本文介绍dLLM，一个开源框架，用于统一扩散语言模型的核心组件（训练、推理、评估），支持复现、微调、部署LLaDA和Dream等大型模型，并提供从零构建小型DLM（包括转换BERT或自回归模型）的可复现配方及checkpoints，以降低门槛并加速研究。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散语言模型发展迅速，但其共享组件分散在临时代码库中或缺乏透明实现，难以复现和扩展。领域发展亟需一个统一框架来标准化这些组件，同时保持灵活性以支持新方法。

Method: 提出dLLM框架，统一扩散语言建模的训练、推理和评估流程。提供标准化pipeline支持主流开源DLM的复现、微调和部署。提供可访问计算的、可复现的配方，用于从零构建小型DLM，并将BERT式编码器或自回归语言模型转换为DLM。

Result: 开源dLLM框架，发布小型DLM的checkpoints，显著提升DLM的可访问性，为未来研究提供基础设施。

Conclusion: dLLM通过标准化和模块化解决了DLM组件碎片化问题，降低了研究和应用门槛，将有效推动扩散语言模型领域的快速发展。

Abstract: Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.
  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.

</details>


### [14] [Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue](https://arxiv.org/abs/2602.22697)
*Ning Gao,Wei Zhang,Yuqin Dai,Ling Shi,Ziyin Wang,Yujie Wang,Wei He,Jinpeng Wang,Chaozheng Wang*

Main category: cs.CL

TL;DR: 本文提出InteractCS-RL框架，通过多粒度强化学习解决任务导向对话中同理心沟通与预算感知决策的平衡问题。该框架包含用户中心交互环境和成本感知多轮策略优化，在真实商业场景中显著优于基线方法，并在多领域验证了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正从对话机器人向通用智能体演进，但有效平衡同理心沟通与预算感知决策仍是开放挑战。现有方法无法捕捉复杂的策略权衡，因此需要新的解决方案。

Method: 提出InteractCS-RL框架，将任务导向对话重构为多粒度强化学习过程。首先建立用户中心交互框架作为高保真训练环境，使智能体能与人格驱动用户动态探索多样策略。然后引入成本感知多轮策略优化(CMPO)，结合混合优势估计策略，通过生成过程信用和PID-Lagrangian成本控制器，有效引导策略在用户奖励与全局成本约束间探索帕累托边界。

Result: 在定制化真实商业场景的广泛实验表明，InteractCS-RL在三个评估维度上显著优于其他基线方法。进一步的tool-agent-user交互基准测试验证了该框架在不同领域的鲁棒性。

Conclusion: InteractCS-RL框架成功解决了LLM智能体在任务导向对话中平衡用户体验与成本约束的关键问题，为构建高效、可控的通用智能体提供了新范式，具有广泛的实际应用价值。

Abstract: The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.

</details>


### [15] [Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs](https://arxiv.org/abs/2602.22698)
*Siyue Su,Jian Yang,Bo Li,Guanglin Niu*

Main category: cs.CL

TL;DR: 本文提出KGT框架，通过专用实体标记解决大语言模型与知识图谱粒度不匹配问题，实现全空间预测，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识图谱补全中面临粒度不匹配的根本问题：大语言模型处理的是分词后的序列，而知识图谱的基本单元是实体。现有方法通过限制候选集或将实体与模型词表对齐，但无法同时捕捉文本语义和图结构完整性。

Method: 提出KGT框架：(1) 引入专用实体标记的特殊分词机制，构建实体级特征表示；(2) 通过关系引导的门控机制融合预训练的结构和文本特征；(3) 采用解耦预测，使用独立头部分离并组合语义和结构推理。

Result: 实验结果表明，KGT在多个基准测试上持续优于当前最先进的方法。

Conclusion: KGT框架通过专用实体标记实现高效的全空间预测，有效解决了大语言模型与知识图谱之间的粒度不匹配问题，为知识图谱补全提供了新的解决方案。

Abstract: Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically constrain predictions to limited candidate sets or align entities with the LLM's vocabulary by pooling multiple tokens or decomposing entities into fixed-length token sequences, which fail to capture both the semantic meaning of the text and the structural integrity of the graph. To address this, we propose KGT, a novel framework that uses dedicated entity tokens to enable efficient, full-space prediction. Specifically, we first introduce specialized tokenization to construct feature representations at the level of dedicated entity tokens. We then fuse pre-trained structural and textual features into these unified embeddings via a relation-guided gating mechanism, avoiding training from scratch. Finally, we implement decoupled prediction by leveraging independent heads to separate and combine semantic and structural reasoning. Experimental results show that KGT consistently outperforms state-of-the-art methods across multiple benchmarks.

</details>


### [16] [Human Label Variation in Implicit Discourse Relation Recognition](https://arxiv.org/abs/2602.22723)
*Frances Yung,Daniil Ignatev,Merel Scholman,Vera Demberg,Massimo Poesio*

Main category: cs.CL

TL;DR: 针对隐式话语关系识别(IDRR)这一高度模糊任务，本研究对比了基于标签分布的建模与标注者特定的视角主义建模两种处理标注分歧的方法。实验发现，标签分布模型预测更稳定，而标注者特定模型表现较差，主要原因是认知复杂性导致的人类解释不一致。


<details>
  <summary>Details</summary>
Motivation: 随着对NLP任务中缺乏单一标准答案认识的加深，人类判断的多样性视角日益受到关注。现有研究虽已开发预测完整标注分布的模型，但隐式话语关系识别(IDRR)中分歧主要源于认知复杂性而非意识形态偏见，这为评估不同建模方法提供了独特场景。

Method: 在IDRR任务上对比两种方法：一是预测完整标注分布而非多数标签的模型，二是旨在重现个体标注者解释的视角主义模型，通过实验分析其在高度歧义性任务中的性能差异及原因。

Result: 实验显示，现有标注者特定模型在IDRR上表现不佳，除非降低歧义性；而基于标签分布的模型产生更稳定的预测。进一步分析表明，高频的认知需求案例驱动了人类解释的不一致性。

Conclusion: 认知复杂性导致的解释不一致对IDRR中的视角主义建模构成挑战。标签分布方法在处理此类高度模糊任务时更具鲁棒性，为该领域研究提供了新方向。

Abstract: There is growing recognition that many NLP tasks lack a single ground truth, as human judgments reflect diverse perspectives. To capture this variation, models have been developed to predict full annotation distributions rather than majority labels, while perspectivist models aim to reproduce the interpretations of individual annotators. In this work, we compare these approaches on Implicit Discourse Relation Recognition (IDRR), a highly ambiguous task where disagreement often arises from cognitive complexity rather than ideological bias. Our experiments show that existing annotator-specific models perform poorly in IDRR unless ambiguity is reduced, whereas models trained on label distributions yield more stable predictions. Further analysis indicates that frequent cognitively demanding cases drive inconsistency in human interpretation, posing challenges for perspectivist modeling in IDRR.

</details>


### [17] [Extending Czech Aspect-Based Sentiment Analysis with Opinion Terms: Dataset and LLM Benchmarks](https://arxiv.org/abs/2602.22730)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文介绍了一个新颖的捷克语餐厅领域方面级情感分析数据集，包含观点术语标注，支持三种不同复杂度的ABSA任务。利用该数据集，研究者在单语、跨语言和多语言设置下对Transformer模型和大语言模型进行了广泛实验，并提出了一种基于大语言模型的翻译与标签对齐方法以解决跨语言挑战。实验揭示了现有模型在处理捷克语等低资源语言时的优势和局限性，错误分析指出了细微观点术语检测和情感表达识别等关键挑战。该数据集为捷克语ABSA建立了新基准，所提方法为其他低资源语言的ABSA资源适配提供了可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如捷克语）在方面级情感分析领域缺乏高质量标注数据集，限制了相关技术的发展和应用。现有跨语言方法在处理语言细微差别时面临挑战，需要更有效的资源适配方案。本研究旨在填补捷克语ABSA数据空白，并探索利用大语言模型解决跨语言迁移问题的新途径。

Method: 1) 构建包含观点术语标注的捷克语餐厅领域ABSA数据集，支持三种不同复杂度的任务；2) 在单语、跨语言和多语言设置下系统评估多种Transformer模型和大语言模型；3) 提出基于大语言模型的翻译与标签对齐方法，通过机器翻译后利用大语言模型进行标签投影和校准，实现跨语言知识迁移。

Result: 1) 成功建立了捷克语ABSA新基准数据集；2) 实验表明当前SOTA模型在低资源语言上性能显著，但仍存在明显局限；3) 提出的翻译-对齐方法在跨语言任务中实现持续提升；4) 错误分析揭示细微观点术语检测、复杂情感表达识别是主要挑战；5) 验证了所提方法在其他低资源语言上的可扩展性。

Conclusion: 本研究填补了捷克语ABSA高质量数据集的空白，为低资源语言情感分析研究提供了重要资源。提出的翻译与标签对齐方法有效缓解了跨语言迁移中的标签不一致问题，为多语言ABSA技术发展提供了新思路。研究结果强调了语言特性对模型性能的重要影响，未来工作需进一步探索更精细的语言适配策略和更鲁棒的情感理解机制。

Abstract: This paper introduces a novel Czech dataset in the restaurant domain for aspect-based sentiment analysis (ABSA), enriched with annotations of opinion terms. The dataset supports three distinct ABSA tasks involving opinion terms, accommodating varying levels of complexity. Leveraging this dataset, we conduct extensive experiments using modern Transformer-based models, including large language models (LLMs), in monolingual, cross-lingual, and multilingual settings. To address cross-lingual challenges, we propose a translation and label alignment methodology leveraging LLMs, which yields consistent improvements. Our results highlight the strengths and limitations of state-of-the-art models, especially when handling the linguistic intricacies of low-resource languages like Czech. A detailed error analysis reveals key challenges, including the detection of subtle opinion terms and nuanced sentiment expressions. The dataset establishes a new benchmark for Czech ABSA, and our proposed translation-alignment approach offers a scalable solution for adapting ABSA resources to other low-resource languages.

</details>


### [18] [Towards Simulating Social Media Users with LLMs: Evaluating the Operational Validity of Conditioned Comment Prediction](https://arxiv.org/abs/2602.22752)
*Nils Schwager,Simon Münker,Alistair Plum,Achim Rettinger*

Main category: cs.CL

TL;DR: 本研究提出条件化评论预测（CCP）任务，通过比较模型生成输出与真实数字痕迹来评估LLM模拟社交媒体用户行为的能力，发现低资源场景下监督微调会导致形式与内容解耦，且明确的角色设定在微调后变得冗余。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型作为社会科学中的"硅基主体"从探索工具转向实际应用，但缺乏对其操作效度的充分验证，需要严格评估其模拟社交媒体用户行为的能力。

Method: 引入条件化评论预测（CCP）框架，让模型通过对比生成输出与真实数字痕迹来预测用户评论。在英语、德语和卢森堡语场景下评估开源8B模型（Llama3.1、Qwen3、Ministral），系统比较显式与隐式提示策略及监督微调（SFT）的影响。

Result: 发现低资源设置下存在形式与内容解耦：SFT能对齐文本输出的表面结构（长度和句法），但会降低语义基础。显式条件（生成传记）在微调后变得冗余，模型可直接从行为历史中进行潜在推断。

Conclusion: 研究结果挑战了当前的"朴素提示"范式，提出了优先考虑真实行为痕迹而非描述性角色的高保真模拟操作准则。

Abstract: The transition of Large Language Models (LLMs) from exploratory tools to active "silicon subjects" in social science lacks extensive validation of operational validity. This study introduces Conditioned Comment Prediction (CCP), a task in which a model predicts how a user would comment on a given stimulus by comparing generated outputs with authentic digital traces. This framework enables a rigorous evaluation of current LLM capabilities with respect to the simulation of social media user behavior. We evaluated open-weight 8B models (Llama3.1, Qwen3, Ministral) in English, German, and Luxembourgish language scenarios. By systematically comparing prompting strategies (explicit vs. implicit) and the impact of Supervised Fine-Tuning (SFT), we identify a critical form vs. content decoupling in low-resource settings: while SFT aligns the surface structure of the text output (length and syntax), it degrades semantic grounding. Furthermore, we demonstrate that explicit conditioning (generated biographies) becomes redundant under fine-tuning, as models successfully perform latent inference directly from behavioral histories. Our findings challenge current "naive prompting" paradigms and offer operational guidelines prioritizing authentic behavioral traces over descriptive personas for high-fidelity simulation.

</details>


### [19] [AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors](https://arxiv.org/abs/2602.22755)
*Abhay Sheshadri,Aidan Ewart,Kai Fronsdal,Isha Gupta,Samuel R. Bowman,Sara Price,Samuel Marks,Rowan Wang*

Main category: cs.CL

TL;DR: 本文提出AuditBench对齐审计基准，包含56个植入隐藏行为的语言模型，涵盖14种隐蔽问题行为。通过开发自主调查智能体测试多种审计工具，发现存在工具-智能体性能差距，最有效的工具是通过辅助模型生成多样化提示的框架调用。白盒可解释工具效果有限，黑盒工具表现更优。合成文档训练的模型比演示训练模型更易审计，而强化对抗训练则显著增加审计难度。该基准及相关工具的开源将推动对齐审计的量化研究。


<details>
  <summary>Details</summary>
Motivation: 构建首个量化对齐审计基准，系统评估不同审计工具在检测语言模型隐藏行为方面的有效性，揭示审计工具在独立评估与智能体部署间的性能差异，为迭代发展对齐审计科学提供标准化测试平台与实证基础。

Method: 创建56个具有14种隐蔽问题行为的语言模型，采用多样化训练策略植入行为并防止模型直接承认。开发可配置工具的调查智能体，通过测量不同工具组合下的审计成功率来评估工具效能，并对比分析不同训练技术对审计难度的影响。

Result: 1) 存在显著的工具-智能体性能差距，独立评估表现优异的工具在智能体环境中效果不佳；2) 最优工具为框架化调用辅助模型生成多样化提示的方法；3) 黑盒工具整体优于白盒可解释工具；4) 合成文档训练模型审计成功率高于演示训练模型；5) 强化对抗训练使审计难度大幅提升。

Conclusion: AuditBench为对齐审计研究提供了关键基础设施，证明有效审计需依赖多样化提示生成等专用策略，且模型训练方式直接影响可审计性。研究揭示了当前审计工具的局限性，开源资源将促进未来量化、迭代的审计科学进步。

Abstract: We introduce AuditBench, an alignment auditing benchmark. AuditBench consists of 56 language models with implanted hidden behaviors. Each model has one of 14 concerning behaviors--such as sycophantic deference, opposition to AI regulation, or secret geopolitical loyalties--which it does not confess to when directly asked. AuditBench models are highly diverse--some are subtle, while others are overt, and we use varying training techniques both for implanting behaviors and training models not to confess. To demonstrate AuditBench's utility, we develop an investigator agent that autonomously employs a configurable set of auditing tools. By measuring investigator agent success using different tools, we can evaluate their efficacy. Notably, we observe a tool-to-agent gap, where tools that perform well in standalone non-agentic evaluations fail to translate into improved performance when used with our investigator agent. We find that our most effective tools involve scaffolded calls to auxiliary models that generate diverse prompts for the target. White-box interpretability tools can be helpful, but the agent performs best with black-box tools. We also find that audit success varies greatly across training techniques: models trained on synthetic documents are easier to audit than models trained on demonstrations, with better adversarial training further increasing auditing difficulty. We release our models, agent, and evaluation framework to support future quantitative, iterative science on alignment auditing.

</details>


### [20] [Towards Better RL Training Data Utilization via Second-Order Rollout](https://arxiv.org/abs/2602.22765)
*Zhe Yang,Yudong Wang,Rang Li,Zhifang Sui*

Main category: cs.CL

TL;DR: 针对大语言模型强化学习中仅通过一阶展开（生成多个答案）训练生成能力而忽视批判能力的问题，论文提出二阶展开（对答案生成多个批判）和联合训练框架。实验表明该方法能更充分利用训练数据，在相同数据下性能更优，并发现了批判训练中标签平衡的重要性和基于结果的奖励噪声问题，可通过采样技术缓解。这是对动态数据增强和生成-批判联合训练的早期探索。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在训练大语言模型时仅关注生成能力提升，通过一阶展开（为问题生成多个回答）进行训练。作者认为这种方法未能充分利用训练数据潜力，因忽视批判能力训练而导致数据利用效率低下，限制了模型整体推理能力的进一步发展。

Method: 提出"二阶展开"概念，即为每个生成的回答创建多个批判性评价。设计统一联合训练框架，同步提升生成和批判能力。该框架通过动态数据增强，在强化学习过程中同时利用第一阶（生成）和第二阶（批判）的rollout信息，实现更全面的能力训练。

Result: 在多种模型和数据集上的实验表明：1）相比传统强化学习方法，该方法能更充分利用训练数据；2）在相同数据量下性能更优；3）发现批判训练中标签平衡的关键作用；4）识别基于结果的奖励噪声问题，并通过采样技术成功缓解。

Conclusion: 本研究初步探索了强化学习中的动态数据增强和生成-批判联合训练机制，为大语言模型训练提供了新思路。研究验证了同时训练生成和批判能力的有效性，揭示了训练过程中的关键挑战及解决方案，为未来强化学习在LLM领域的发展提供了有益启示和指导方向。

Abstract: Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating multiple critiques for a response) and propose a unified framework for jointly training generation and critique capabilities. Extensive experiments across various models and datasets demonstrate that our approach can utilize training data more effectively than vanilla RL and achieve better performance under the same training data. Additionally, we uncover several insightful findings regarding second-order rollout and critique training, such as the importance of label balance in critique training and the noise problem of outcome-based rewards, which can be mitigated through sampling techniques. Our work offers a preliminary exploration of dynamic data augmentation and joint generation-critique training in RL, providing meaningful inspiration for the further advancement of RL training

</details>


### [21] [Imagination Helps Visual Reasoning, But Not Yet in Latent Space](https://arxiv.org/abs/2602.22766)
*You Li,Chi Chen,Yanghao Li,Fanhu Zeng,Kaiyu Huang,Jinan Xu,Maosong Sun*

Main category: cs.CL

TL;DR: 该研究通过因果中介分析质疑了隐式视觉推理的有效性，发现隐式token与输入和输出之间存在关键脱节，并提出了一种通过显式文本想象来提升视觉推理性能的简单替代方案CapImagine。


<details>
  <summary>Details</summary>
Motivation: 尽管隐式视觉推理被认为是多模态大语言模型视觉推理的有前途范式，但其有效性的根本机制仍不明确。本研究旨在通过因果中介分析来揭示其真实效果来源，挑战现有范式的必要性。

Method: 研究构建了因果链模型（输入作为处理、隐式token作为中介、最终答案作为结果），通过系统性扰动分析检验输入-隐式token和隐式token-答案之间的因果关系，并进行了广泛的探测分析以评估隐式token编码的视觉信息量。

Result: 发现了两个关键脱节：(a) 输入-隐式脱节：对输入的剧烈扰动仅引起隐式token的微小变化，表明隐式token无法有效关注输入序列；(b) 隐式-答案脱节：对隐式token的扰动对最终答案影响有限，说明隐式token对结果的因果效应受限。探测分析显示隐式token编码的视觉信息有限且高度相似。

Conclusion: 研究挑战了隐式视觉推理的必要性，提出了通过显式文本想象进行视觉推理的替代方法CapImagine。实验表明CapImagine在视觉中心基准测试上显著优于复杂的隐式空间基线，展现了显式想象在视觉推理中的优越潜力。

Abstract: Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.

</details>


### [22] [Probing for Knowledge Attribution in Large Language Models](https://arxiv.org/abs/2602.22787)
*Ivo Brink,Alexander Boer,Dennis Ulmer*

Main category: cs.CL

TL;DR: 该论文提出了一种自动识别大语言模型生成内容主要知识来源（提示上下文vs内部知识）的贡献归属方法。通过训练线性分类器探针并使用自监督数据管道AttriWiki，该方法在多种模型上达到0.96 Macro-F1，且无需重训练即可迁移至外部基准（0.94-0.99 Macro-F1），知识来源混淆会导致错误率上升高达70%，揭示了检测框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生流畅但无根据的幻觉，分为忠实性违规（误用用户上下文）和事实性违规（内部知识错误）。有效缓解取决于判断回答是基于提示还是模型内部权重，因此识别每个输出的主导知识来源（贡献归属）至关重要。

Method: 作者提出使用简单的线性分类器作为探针，训练于模型隐藏表示以预测贡献归属。为此设计了AttriWiki自监督数据管道，通过提示模型回忆被隐藏实体或从上下文中读取，自动生成标注样本。

Result: 在AttriWiki上训练的探针表现出强归属信号，在Llama-3.1-8B、Mistral-7B和Qwen-7B上达到0.96 Macro-F1，无需重训练即可迁移至SQuAD和WebQuestions等外部基准（0.94-0.99 Macro-F1）。归属不匹配使错误率上升高达70%，揭示了知识来源混淆与不忠实回答的直接关联。

Conclusion: 研究证实了知识来源混淆会导致不忠实回答，但即使归属正确模型仍可能出错，表明单一依赖贡献归属不足以全面检测幻觉，需要开发更广泛的多维度检测框架。

Abstract: Large language models (LLMs) often generate fluent but unfounded claims, or hallucinations, which fall into two types: (i) faithfulness violations - misusing user context - and (ii) factuality violations - errors from internal knowledge. Proper mitigation depends on knowing whether a model's answer is based on the prompt or its internal weights. This work focuses on the problem of contributive attribution: identifying the dominant knowledge source behind each output. We show that a probe, a simple linear classifier trained on model hidden representations, can reliably predict contributive attribution. For its training, we introduce AttriWiki, a self-supervised data pipeline that prompts models to recall withheld entities from memory or read them from context, generating labelled examples automatically. Probes trained on AttriWiki data reveal a strong attribution signal, achieving up to 0.96 Macro-F1 on Llama-3.1-8B, Mistral-7B, and Qwen-7B, transferring to out-of-domain benchmarks (SQuAD, WebQuestions) with 0.94-0.99 Macro-F1 without retraining. Attribution mismatches raise error rates by up to 70%, demonstrating a direct link between knowledge source confusion and unfaithful answers. Yet, models may still respond incorrectly even when attribution is correct, highlighting the need for broader detection frameworks.

</details>


### [23] [Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift](https://arxiv.org/abs/2602.22790)
*Hyunwoo Kim,Hanau Yi,Jaehee Bae,Yumin Kim*

Main category: cs.CL

TL;DR: 本文针对大语言模型演进引发的"模型漂移"问题，重新将自然语言声明式提示（NLD-P）概念化为声明式治理框架，通过模块化抽象分离关注点，为非开发者在动态模型生态中提供可控提示治理方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型跨代演进导致指令遵循策略、对齐机制和解码策略持续变化，传统提示工程方法无法保证生成行为的稳定性，亟需面向系统级治理的解决方案。

Method: 将NLD-P形式化为模块化控制抽象，在自然语言中直接编码溯源、约束逻辑、任务内容和后生成评估，定义最小合规标准，分析模型模式接受度，并采用人机协同协议进行框架验证。

Result: 建立了无需外部代码编排的声明式治理框架，明确了模型依赖的schema receptivity，通过实际论文撰写验证了NLD-P可行性，并界定了未来实证研究方向。

Conclusion: NLD-P为应对持续模型演化提供了可访问的声明式控制框架，强调人类主导的治理模式重要性，为非开发者从业者在LLM生态中的稳定操作奠定理论基础。

Abstract: The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, and decoding strategies, a phenomenon we characterize as GPT-scale model drift. Under such conditions, surface-level formatting conventions and ad hoc refinement are insufficient to ensure stable, interpretable control. This paper reconceptualizes Natural Language Declarative Prompting (NLD-P) as a declarative governance method rather than a rigid field template. NLD-P is formalized as a modular control abstraction that separates provenance, constraint logic, task content, and post-generation evaluation, encoded directly in natural language without reliance on external orchestration code. We define minimal compliance criteria, analyze model-dependent schema receptivity, and position NLD-P as an accessible governance framework for non-developer practitioners operating within evolving LLM ecosystems. Portions of drafting and editorial refinement employed a schema-bound LLM assistant configured under NLD-P. All conceptual framing, methodological claims, and final revisions were directed, reviewed, and approved by the human author under a documented human-in-the-loop protocol. The paper concludes by outlining implications for declarative control under ongoing model evolution and identifying directions for future empirical validation.

</details>


### [24] [TARAZ: Persian Short-Answer Question Benchmark for Cultural Evaluation of Language Models](https://arxiv.org/abs/2602.22827)
*Reihaneh Iranmanesh,Saeedeh Davoudi,Pasha Abrishamchian,Ophir Frieder,Nazli Goharian*

Main category: cs.CL

TL;DR: 本文针对波斯语大型语言模型文化能力评估提出综合框架，创新性地采用波斯语短答案评测，结合规则化形态归一化与混合句法语义相似度计算，实现软匹配评分，在15个先进模型上验证其有效性，并将框架开源。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语文化评测基准主要采用选择题形式及英语中心指标，无法充分捕捉波斯语复杂的形态学特征与深层语义差异，导致评估结果不够准确。

Method: 构建波斯语专用短答案评估体系，融合基于规则的形态归一化预处理与混合句法语义相似度匹配模块，突破传统精确字符串匹配局限，实现对模型输出语义层面的软匹配评分。

Result: 对15个前沿开源及闭源模型的评估显示，该混合评估方法较精确匹配基线提升评分一致性达10%，能有效识别表层匹配无法捕捉的语义等价性。

Conclusion: 研究成果包括首个波斯语文化理解标准化基准与开源评估框架，为跨文化LLM评估研究建立可复现基础，促进多语言文化适应性评测技术发展。

Abstract: This paper presents a comprehensive evaluation framework for assessing the cultural competence of large language models (LLMs) in Persian. Existing Persian cultural benchmarks rely predominantly on multiple-choice formats and English-centric metrics that fail to capture Persian's morphological complexity and semantic nuance. Our framework introduces a Persian-specific short-answer evaluation that combines rule-based morphological normalization with a hybrid syntactic and semantic similarity module, enabling robust soft-match scoring beyond exact string overlap. Through systematic evaluation of 15 state-of-the-art open- and closed-source models, we demonstrate that our hybrid evaluation improves scoring consistency by +10% compared to exact-match baselines by capturing meaning that surface-level methods cannot detect. We publicly release our evaluation framework, providing the first standardized benchmark for measuring cultural understanding in Persian and establishing a reproducible foundation for cross-cultural LLM evaluation research.

</details>


### [25] [TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought](https://arxiv.org/abs/2602.22828)
*Jianmin Li,Ying Chang,Su-Kit Tang,Yujia Liu,Yanwen Wang,Shuyuan Lin,Binkai Ou*

Main category: cs.CL

TL;DR: 本研究针对传统中医临床诊疗中复杂推理和个体化差异导致传统RAG方法性能不佳的问题，提出TCM-DiffRAG框架，通过融合知识图谱与思维链实现中医个性化诊断任务的显著性能提升，实验表明该框架优于原生大模型、监督微调模型及其他RAG基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统RAG技术在中医临床诊疗领域表现不佳，主要因为中医诊断涉及复杂推理过程且存在显著个体差异，而标准RAG难以有效处理这种高度个性化的辨证论治特点。

Method: 提出TCM-DiffRAG创新框架，核心为将结构化中医知识图谱与思维链推理机制相结合，并在三个独特中医测试数据集上进行评估验证。

Result: 实验显示：qwen-plus模型在三个指标上从0.927/0.361/0.038提升至0.952/0.788/0.356；非中文大模型提升更为显著；且全面超越监督微调大模型及其他RAG基准方法。

Conclusion: 联合使用通用与个性化知识图谱可实现通用知识与临床推理的有效对齐，推理感知的RAG框架为推进大语言模型在中医药领域的应用展现出重要潜力。

Abstract: Background: Retrieval augmented generation (RAG) technology can empower large language models (LLMs) to generate more accurate, professional, and timely responses without fine tuning. However, due to the complex reasoning processes and substantial individual differences involved in traditional Chinese medicine (TCM) clinical diagnosis and treatment, traditional RAG methods often exhibit poor performance in this domain. Objective: To address the limitations of conventional RAG approaches in TCM applications, this study aims to develop an improved RAG framework tailored to the characteristics of TCM reasoning. Methods: We developed TCM-DiffRAG, an innovative RAG framework that integrates knowledge graphs (KG) with chains of thought (CoT). TCM-DiffRAG was evaluated on three distinctive TCM test datasets. Results: The experimental results demonstrated that TCM-DiffRAG achieved significant performance improvements over native LLMs. For example, the qwen-plus model achieved scores of 0.927, 0.361, and 0.038, which were significantly enhanced to 0.952, 0.788, and 0.356 with TCM-DiffRAG. The improvements were even more pronounced for non-Chinese LLMs. Additionally, TCM-DiffRAG outperformed directly supervised fine-tuned (SFT) LLMs and other benchmark RAG methods. Conclusions: TCM-DiffRAG shows that integrating structured TCM knowledge graphs with Chain of Thought based reasoning substantially improves performance in individualized diagnostic tasks. The joint use of universal and personalized knowledge graphs enables effective alignment between general knowledge and clinical reasoning. These results highlight the potential of reasoning-aware RAG frameworks for advancing LLM applications in traditional Chinese medicine.

</details>


### [26] [SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables](https://arxiv.org/abs/2602.23286)
*Sungho Park,Jueun Kim,Wook-Shin Han*

Main category: cs.CL

TL;DR: 本文提出SPARTA，一个自动构建大规模表-文问答基准的端到端框架。通过构建参考事实库和合成嵌套查询，该框架生成覆盖聚合、分组及深层多跳推理的高质量问答对。现有模型在SPARTA上的F1分数相比在HybridQA/OTT-QA上下降超过30点，暴露了当前跨模态推理的根本弱点。


<details>
  <summary>Details</summary>
Motivation: 现实世界的表-文问答任务需要模型具备长文本跨表格的多跳推理和聚合、分组等复杂分析能力，但现有基准存在规模小、人工构建易出错、问题浅层（通常不超过两跳）且很少涉及高级分析操作的缺陷，无法充分评估模型的深度推理能力。

Method: 框架首先通过从未结构化段落中自动提取原子事实构建 grounding tables，丰富源表格形成参考事实库；然后合成嵌套查询以匹配目标跳数。提出两项创新技术：1) 溯源精炼（provenance-based refinement），重写语法有效但返回空结果的查询；2) 现实结构强制（realistic-structure enforcement），将生成限制为查询图的后序遍历，确保SQL可执行且自然语言化流畅。

Result: 在SPARTA基准上，当前最优模型（HybridQA F1>70，OTT-QA F1>50）的性能下降超过30个F1点，显著揭示了现有方法在复杂跨模态推理中的根本缺陷。框架仅需HybridQA四分之一的标注时间，可自动生成数千个高质量问答对。

Conclusion: SPARTA成功创建了首个大规模、高保真、支持深度推理的表-文问答基准，为评估和提升模型的多跳跨模态推理能力提供了新标准。研究代码、基准数据集和基线模型已开源。

Abstract: Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.

</details>


### [27] [Improving Neural Argumentative Stance Classification in Controversial Topics with Emotion-Lexicon Features](https://arxiv.org/abs/2602.22846)
*Mohammad Yeghaneh Abkenar,Weixing Wang,Manfred Stede,Davide Picca,Mark A. Finlayson,Panagiotis Ioannidis*

Main category: cs.CL

TL;DR: 针对论点立场分类任务中情感分析利用不足的问题，本研究提出基于DistilBERT嵌入扩展偏见校正NRC情感词典的方法。在五个涵盖争议性话题的多样化数据集上，扩展词典(eNRC)使F1分数最高提升6.2%，在四项数据上超越原始NRC词典，并在几乎所有语料库上优于LLM基线。


<details>
  <summary>Details</summary>
Motivation: 现有立场分类研究存在三方面局限：其一，缺乏系统性的细粒度情感分析整合；其二，主要依赖非论点文本；其三，受限于特定领域或话题，泛化能力不足。鉴于争议性论点常诉诸情感，而情感信息未被充分挖掘用于提升分类性能，本研究旨在填补这一空白。

Method: 研究构建五个跨领域争议性话题数据集，核心方法包括：(1)采用DistilBERT生成上下文嵌入；(2)系统扩展偏见校正的NRC情感词典；(3)将扩展后的情感特征输入神经论点立场分类模型。通过语义相似度匹配识别原词典未涵盖的情感词汇，实现词典的动态扩充。

Result: 实验结果：eNRC在全部五个数据集上超越基线模型，F1提升最高达6.2个百分点；在四个数据集上优于原始NRC词典，最高提升3.0个百分点；在几乎所有语料库上超过基于大语言模型的方法。所有资源（eNRC、适配语料、模型架构）已开源。

Conclusion: 本研究验证了上下文感知的细粒度情感分析对立场分类的有效性，扩展词典方法展现出良好泛化性。研究不仅提供了性能更优的技术方案，还通过开源资源降低了后续研究门槛，推动了情感增强的论点挖掘发展。

Abstract: Argumentation mining comprises several subtasks, among which stance classification focuses on identifying the standpoint expressed in an argumentative text toward a specific target topic. While arguments-especially about controversial topics-often appeal to emotions, most prior work has not systematically incorporated explicit, fine-grained emotion analysis to improve performance on this task. In particular, prior research on stance classification has predominantly utilized non-argumentative texts and has been restricted to specific domains or topics, limiting generalizability. We work on five datasets from diverse domains encompassing a range of controversial topics and present an approach for expanding the Bias-Corrected NRC Emotion Lexicon using DistilBERT embeddings, which we feed into a Neural Argumentative Stance Classification model. Our method systematically expands the emotion lexicon through contextualized embeddings to identify emotionally charged terms not previously captured in the lexicon. Our expanded NRC lexicon (eNRC) improves over the baseline across all five datasets (up to +6.2 percentage points in F1 score), outperforms the original NRC on four datasets (up to +3.0), and surpasses the LLM-based approach on nearly all corpora. We provide all resources-including eNRC, the adapted corpora, and model architecture-to enable other researchers to build upon our work.

</details>


### [28] [Effective QA-driven Annotation of Predicate-Argument Relations Across Languages](https://arxiv.org/abs/2602.22865)
*Jonathan Davidov,Aviv Slobodkin,Shmuel Tomi Klein,Reut Tsarfaty,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: 本文提出跨语言投影方法，通过复用英语QA-SRL解析器，结合受限翻译与词对齐流程，自动为希伯来语、俄语和法语生成高质量谓词-论元标注数据，所得专用解析器显著优于GPT-4o和LLaMA-Maverick等强多语言大模型基线。


<details>
  <summary>Details</summary>
Motivation: 谓词-论元结构的显式表示是可解释语义分析的核心，支撑推理、生成与评估等下游任务。然而，现有高质量标注成本高昂，且主要局限于英语，严重制约了多语言语义分析技术的发展。

Method: 基于问题-回答驱动的语义角色标注(QA-SRL)框架，采用跨语言投影策略：在受限翻译与词对齐的流程中复用预训练英语QA-SRL解析器，自动生成与目标语言谓词对齐的问题-回答对作为标注数据。

Result: 在希伯来语、俄语和法语三种不同语系的语言上验证，方法生成了高质量训练数据；微调后的语言专用解析器性能大幅超越GPT-4o、LLaMA-Maverick等先进多语言大语言模型基线。

Conclusion: 通过将QA-SRL作为可迁移的自然语言语义接口，本方法实现了跨语言高效、可及的谓词-论元解析，为低资源语言语义分析提供了数据高效的技术路径，推动了语义分析技术的多语言普及。

Abstract: Explicit representations of predicate-argument relations form the basis of interpretable semantic analysis, supporting reasoning, generation, and evaluation. However, attaining such semantic structures requires costly annotation efforts and has remained largely confined to English. We leverage the Question-Answer driven Semantic Role Labeling (QA-SRL) framework -- a natural-language formulation of predicate-argument relations -- as the foundation for extending semantic annotation to new languages. To this end, we introduce a cross-linguistic projection approach that reuses an English QA-SRL parser within a constrained translation and word-alignment pipeline to automatically generate question-answer annotations aligned with target-language predicates. Applied to Hebrew, Russian, and French -- spanning diverse language families -- the method yields high-quality training data and fine-tuned, language-specific parsers that outperform strong multilingual LLM baselines (GPT-4o, LLaMA-Maverick). By leveraging QA-SRL as a transferable natural-language interface for semantics, our approach enables efficient and broadly accessible predicate-argument parsing across languages.

</details>


### [29] [Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference](https://arxiv.org/abs/2602.22868)
*Yushi Ye,Feng Hong,Huangjie Zheng,Xu Chen,Zhiyong Chen,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: 针对扩散大语言模型并行解码中的组合矛盾问题，本文提出ReMix框架，通过引入连续混合状态和拒绝机制，在保持质量的同时实现2-8倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型虽承诺实现快速非自回归推理，但并行解码时面临严重的质量-速度权衡问题，其根源在于“组合矛盾”现象——并行生成的token在语义上形成相互冲突的不一致组合。

Method: 提出ReMix（拒绝混合）框架，核心创新是引入连续混合状态作为掩码状态与最终解码状态之间的中间表示。该状态允许token表征在连续空间中进行迭代精炼，在坍缩为离散样本前解决与其他token的相互冲突；同时设计拒绝规则，将不确定的表征从连续状态回退至掩码状态重新处理，确保稳定性并防止错误传播。

Result: 作为训练免费的方法，ReMix在广泛实验中实现了2-8倍的推理速度提升，且未出现任何质量退化。

Conclusion: ReMix通过连续空间精炼机制有效缓解了离散扩散解码中的组合矛盾，显著改善了扩散大语言模型的并行解码效率与质量权衡。

Abstract: Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \times$ inference speedup without any quality degradation.

</details>


### [30] [Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching](https://arxiv.org/abs/2602.22871)
*Roy Miles,Aysim Toker,Andreea-Maria Oncescu,Songcen Xu,Jiankang Deng,Ismail Elezi*

Main category: cs.CL

TL;DR: 本文提出"Stitching Noisy Diffusion Thoughts"自洽框架，通过掩码扩散模型生成多样化推理轨迹，利用过程奖励模型筛选高质量中间步骤并跨轨迹拼接，再由自回归模型重新计算最终答案。该方法在数学和代码任务上实现最高23.8%的准确率提升和1.8倍的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的多思维链聚合策略局限于轨迹级别（如选择最优路径或答案投票），会丢弃部分正确推理路径中的有价值中间结果，导致对"接近正确"尝试的利用率低下。

Method: 三阶段模块化框架：(i) 使用掩码扩散语言模型廉价采样多样化推理轨迹；(ii) 用过程奖励模型对所有中间步骤进行评分；(iii) 跨轨迹筛选最优步骤拼接成组合推理链，并以其为条件输入自回归模型仅重新计算最终答案，实现探索与求解的解耦。

Result: 在六个数学和代码基准测试中，该无训练框架平均准确率提升最高达23.8%，推理延迟相比传统扩散模型和统一架构降低1.8倍。步骤级重组在更难问题上收益更显著，消融实验证实最终自回归求解器对修正不完美拼接推理链至关重要。

Conclusion: 该方法通过步骤级重用最大化扩散采样的多样性优势，利用自回归模型进行最终答案校正，在提升复杂推理性能的同时降低计算开销，为构建高效推理系统提供了新范式。

Abstract: Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or "nearly correct" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.

</details>


### [31] [Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models](https://arxiv.org/abs/2602.22918)
*Jonathan Steinberg,Oren Gal*

Main category: cs.CL

TL;DR: 本研究通过因果干预揭示视觉语言模型中OCR信息的架构特异性路由机制：DeepStack模型在中间层(约50%)最敏感，而单阶段投影模型在早期层(6-25%)达峰值。OCR信号具低维特性(PC1占72.9%方差)且主成分方向可跨数据集迁移，模块化架构中移除OCR甚至能提升计数性能达6.9个百分点。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型虽能识别图像文本，但OCR信息在语言处理流中的整合机制未知。明确不同架构的OCR处理瓶颈位置，对优化模型性能及解决OCR与其他视觉任务间的干扰问题至关重要。

Method: 采用因果干预策略，通过计算原始图像与文本修复版本间的激活差异，定位三种架构家族(Qwen3-VL、Phi-4、InternVL3.5)的OCR瓶颈层，并结合主成分分析量化信号维度特性。

Result: 发现架构依赖的OCR敏感层分布：Qwen3-VL在约50%深度处对场景文本最敏感；Phi-4与InternVL3.5在早期层(6-25%)峰值。OCR信号高度低维(PC1解释72.9%方差)，且PCA方向具备跨数据集迁移性。在Qwen3-VL-4B等模块化架构中，移除OCR可提升计数精度达6.9个百分点。

Conclusion: OCR信息处理位置由视觉语言整合策略决定，不同架构共享文本处理通路。模块化设计中OCR信号会干扰其他视觉处理，该发现为模型优化与架构设计提供了新方向。

Abstract: Vision-language models (VLMs) can read text from images, but where does this optical character recognition (OCR) information enter the language processing stream? We investigate the OCR routing mechanism across three architecture families (Qwen3-VL, Phi-4, InternVL3.5) using causal interventions. By computing activation differences between original images and text-inpainted versions, we identify architecture-specific OCR bottlenecks whose dominant location depends on the vision-language integration strategy: DeepStack models (Qwen) show peak sensitivity at mid-depth (about 50%) for scene text, while single-stage projection models (Phi-4, InternVL) peak at early layers (6-25%), though the exact layer of maximum effect varies across datasets. The OCR signal is remarkably low-dimensional: PC1 captures 72.9% of variance. Crucially, principal component analysis (PCA) directions learned on one dataset transfer to others, demonstrating shared text-processing pathways. Surprisingly, in models with modular OCR circuits (notably Qwen3-VL-4B), OCR removal can improve counting performance (up to +6.9 percentage points), suggesting OCR interferes with other visual processing in sufficiently modular architectures.

</details>


### [32] [Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention](https://arxiv.org/abs/2602.23057)
*Jeongin Bae,Baeseong Park,Gunho Park,Minsub Kim,Joonhyung Lee,Junhee Yoo,Sunghyeon Woo,Jiwon Ryu,Se Jung Kwon,Dongsoo Lee*

Main category: cs.CL

TL;DR: 提出Affine-Scaled Attention，通过输入相关的缩放和偏置项松弛softmax注意力的严格归一化约束，在大规模语言模型预训练中提升训练稳定性、优化行为和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 标准softmax注意力强制单位求和归一化，限制了注意力幅度的灵活性，可能导致过度集中或不稳定的训练模式；已有方法（如注意力汇聚、门控机制）对注意力重加权控制有限。

Method: 在标准注意力基础上引入输入相关的缩放因子和偏置项，作用于softmax归一化后的注意力权重，在保持值表示聚合的同时放宽严格的归一化约束，实现可控的注意力分布和尺度调整。

Result: 在多种规模的预训练语言模型上验证，相比标准softmax注意力和注意力汇聚基线，该方法在训练稳定性、优化行为和下游任务性能方面均取得一致提升。

Conclusion: 对注意力输出的适度重加权是改善Transformer模型注意力行为的实用有效方法。

Abstract: Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner.
  We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.

</details>


### [33] [Toward Automatic Filling of Case Report Forms: A Case Study on Data from an Italian Emergency Department](https://arxiv.org/abs/2602.23062)
*Gabriela Anna Kaczmarek,Pietro Ferrazzi,Lorenzo Porta,Vicky Rubini,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本研究针对临床病例报告表(CRF)自动填写任务中标注数据稀缺的瓶颈问题，构建了包含134个项目的意大利语急诊科临床笔记标注数据集，定义了CRF填写任务及评估指标，并基于开源大模型进行了零样本实验，验证了可行性但揭示了模型存在谨慎性偏见。


<details>
  <summary>Details</summary>
Motivation: 随着语言技术进步，自动从临床笔记填写病例报告表(CRF)成为研究热点，但训练和测试大语言模型所需的高质量标注数据普遍匮乏，严重制约了该领域的研究进展。

Method: 研究团队创建了一个新的意大利语急诊科临床笔记标注数据集，包含预定义的134项CRF内容；明确定义了CRF自动填写的任务框架和评估指标；采用开源的先进大语言模型进行零样本(zero-shot)实验。

Result: 实验结果表明：(1)在零样本设置下，大语言模型能够处理意大利语真实临床笔记的CRF自动填写任务；(2)模型结果存在显著偏见，表现为过度谨慎而倾向于给出"未知"答案，这种系统性偏差需要专门校正。

Conclusion: 本研究为CRF自动填写提供了宝贵的意大利语数据资源，证实了大语言模型在该任务上的可行性，同时揭示了模型偏见这一关键挑战，为未来研究指明了数据建设和算法优化两个重要方向。

Abstract: Case Report Forms (CRFs) collect data about patients and are at the core of well-established practices to conduct research in clinical settings. With the recent progress of language technologies, there is an increasing interest in automatic CRF-filling from clinical notes, mostly based on the use of Large Language Models (LLMs). However, there is a general scarcity of annotated CRF data, both for training and testing LLMs, which limits the progress on this task. As a step in the direction of providing such data, we present a new dataset of clinical notes from an Italian Emergency Department annotated with respect to a pre-defined CRF containing 134 items to be filled. We provide an analysis of the data, define the CRF-filling task and metric for its evaluation, and report on pilot experiments where we use an open-source state-of-the-art LLM to automatically execute the task. Results of the case-study show that (i) CRF-filling from real clinical notes in Italian can be approached in a zero-shot setting; (ii) LLMs' results are affected by biases (e.g., a cautious behaviour favours "unknown" answers), which need to be corrected.

</details>


### [34] [Quantity Convergence, Quality Divergence: Disentangling Fluency and Accuracy in L2 Mandarin Prosody](https://arxiv.org/abs/2602.23071)
*Yuqi Shi,Hao Yang,Xiyao Lu,Jinsong Zhang*

Main category: cs.CL

TL;DR: 本研究通过对比67名汉语母语者和67名越南学习者，发现高水平越南学习者虽在韵律边界数量上接近母语者，但在句法-韵律结构映射上存在系统性偏差：将主谓边界降级、动宾边界升级，形成与母语模式相反的韵律层级，体现了L2习得的非线性特征。


<details>
  <summary>Details</summary>
Motivation: 尽管二语学习者可能掌握目标语法的词序，但将这些句法结构映射到恰当的韵律结构仍面临持续挑战。本研究旨在探究二语句法-韵律接口的石化现象与稳定性问题。

Method: 基于BLCU-SAIT语料库，采用C-ToBI韵律标注体系与依存语法分析相结合的方法，从韵律边界数量及其与句法关系的映射两个维度，对比分析汉语母语者与越南学习者的韵律产出。

Result: 研究发现非线性习得模式：高水平学习者(VNH)在核心韵律短语(B3)层级虽达到母语者边界数量基线，但结构映射显著偏离。具体表现为：在主谓(SBV)接口处降级韵律边界(从B3降至B1)，在动宾(VOB)接口处错误升级边界(从B1升至B3)，以牺牲结构准确性为代价维持较长的韵律短语输出。

Conclusion: 该策略导致学习者形成扭曲的韵律层级，其模式与母语者相反。研究表明二语句法-韵律接口的习得存在系统性石化现象，即使在高级阶段也难以达到母语样式的结构映射精度。

Abstract: While second language (L2) learners may acquire target syntactic word order, mapping this syntax onto appropriate prosodic structures remains a persistent challenge. This study investigates the fossilization and stability of the L2 syntax-prosody interface by comparing 67 native Mandarin speakers with 67 Vietnamese learners using the BLCU-SAIT corpus. By integrating C-ToBI boundary annotation with Dependency Grammar analysis, we examined both the quantity of prosodic boundaries and their mapping to syntactic relations. Results reveal a non-linear acquisition: although high-proficiency learners (VNH) converge to the native baseline in boundary quantity at the Major Phrase level (B3), their structural mapping significantly diverges. Specifically, VNH demote the prosodic boundary at the Subject-Verb (SBV) interface (Major Phrase B3 -> Prosodic Word B1), while erroneously promoting the boundary at the Verb-Object (VOB) interface (Prosodic Word B1 -> Major Phrase B3). This strategy allows learners to maintain high long phrasal output at the expense of structural accuracy. This results in a distorted prosodic hierarchy where the native pattern is inverted.

</details>


### [35] [Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent](https://arxiv.org/abs/2602.23079)
*Boyang Zhang,Yang Zhang*

Main category: cs.CL

TL;DR: 本研究针对LLM authorship inference带来的去匿名化风险，提出SALA框架，融合风格计量特征与LLM推理实现可解释的作者归属，并通过引导重写策略降低识别性。新闻数据集实验验证了方法的高精度与有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，文本作者推断功能引发隐私泄露担忧，尤其在新闻等公开文本中。现有研究缺乏系统性的风险评估与缓解机制，且防御手段的透明性和有效性不足。

Method: 设计SALA（Stylometry-Assisted LLM Analysis）方法，构建结构化、可解释的评估管道，整合定量风格计量特征与LLM推理。框架包含数据库增强模块，并提出基于推理轨迹的引导重写策略，生成语义保留的去识别化文本。

Result: 大规模新闻数据集实验表明，SALA在多种场景下实现高推断准确率，数据库模块显著提升性能。引导重写策略有效降低作者可识别性，同时保持文本原意。

Conclusion: 工作证实LLM智能体具备强大的去匿名化潜力，同时凸显可解释主动防御对作者隐私保护的关键作用，为构建安全的文本发布机制提供了实用解决方案。

Abstract: The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.

</details>


### [36] [Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs](https://arxiv.org/abs/2602.23136)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 该研究揭示多模态大语言模型无法有效利用语音中的说话人身份、情感或图像中的纹理信息，并非编码器失效，而是"不匹配解码器问题"所致。尽管这些信息在各层中大量保留（比随机水平高3-55倍），但因与文本训练目标不对齐而被解码器视为噪声，移除64-71%的模态特定方差反而改善解码损失。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理语音和图像时无法捕捉说话人声音特征和视觉纹理等细粒度模态信息，传统观点归咎于编码不足。本文旨在挑战这一认知，探究问题根源究竟是编码器缺陷还是解码器与模态信息不匹配。

Method: 使用线性探针量化LLM各层中保留的说话人身份、情感和视觉属性信息；通过移除模态特定方差分析解码器损失变化；形式化"不匹配解码器问题"并推导广义互信息(GMI)理论边界；在五个语音和视觉模型上进行验证；设计控制实验比较两个仅在编码器文本对齐性上不同的Prismatic视觉语言模型；通过LoRA干预实验验证训练目标对信息可及性的影响。

Result: 1) 所有LLM层均高度保留模态特定信息（线性探针准确率比随机水平高3-55倍）；2) 移除64-71%的模态特定方差反而降低解码器损失，证明其对解码器是噪声；3) 理论证实信息可及性受GMI约束，退化程度取决于分布距离和解码器敏感性，且该约束适用于任何非文本输入方式；4) 控制实验确认瓶颈是解码器评分规则而非编码器或投影；5) LoRA情感训练使情感可及性提升7.5%且不影响其他属性。

Conclusion: 多模态大模型中模态特定信息丢失的根本原因在于解码器与模态信息的不匹配。解码器因仅通过文本训练，只能提取文本对齐方向的信息，其余信息被视为噪声。该问题具有普适性，通过调整训练目标可定向提升特定信息的可及性，为优化多模态模型设计提供了理论依据。

Abstract: Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.
  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.

</details>


### [37] [MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations](https://arxiv.org/abs/2602.23184)
*Sara Rosenthal,Yannis Katsis,Vraj Shah,Lihong He,Lucian Popa,Marina Danilevsky*

Main category: cs.CL

TL;DR: 本文提出MTRAG-UN基准测试，用于探索多轮检索增强生成（RAG）中的开放挑战。该基准包含666个任务、超过2800个对话轮次，涵盖6个领域。实验表明，检索和生成模型在处理无法回答、信息不足、非独立问题以及模糊回应时仍存在困难。


<details>
  <summary>Details</summary>
Motivation: 多轮检索增强生成是大语言模型的重要应用方式，但现有模型在多轮对话中面临诸多挑战。为了系统性地探索和评估这些开放性问题，需要专门的基准测试来识别模型在处理复杂对话场景时的弱点。

Method: 作者构建了一个名为MTRAG-UN的基准测试，包含666个任务和超过2800个对话轮次，覆盖6个不同领域，并提供了相应的语料库。通过该基准评估模型在多轮RAG任务中的表现。

Result: 实验结果显示，当前的检索和生成模型在处理四类问题上仍然存在困难：无法回答的问题（UNanswerable）、信息不足的问题（UNderspecified）、非独立问题（NONstandalone）以及模糊回应（UNclear responses）。

Conclusion: MTRAG-UN基准为研究多轮RAG的挑战提供了重要工具，揭示了当前模型在复杂对话场景中的局限性，有助于推动相关技术的发展。基准已开源，供研究者使用。

Abstract: We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com/IBM/mt-rag-benchmark

</details>


### [38] [Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models](https://arxiv.org/abs/2602.23197)
*Chungpa Lee,Jy-yong Sohn,Kangwook Lee*

Main category: cs.CL

TL;DR: 本文通过线性注意力模型理论分析，揭示了微调如何影响Transformer大语言模型的上下文学习能力。研究发现，全参数微调会损害上下文学习，而仅更新值矩阵可在提升零样本性能的同时保留上下文学习能力。添加辅助少样本损失虽能增强目标任务表现，但会削弱对未见过任务的适应能力。


<details>
  <summary>Details</summary>
Motivation: 实践中，Transformer大语言模型通常通过微调来提升下游任务的零样本性能，但微调可能导致上下文学习能力退化，限制模型在未见任务上的表现。现有研究缺乏对微调目标如何改变注意力参数的理论理解，以及这种改变如何导致少样本性能下降的机制分析。

Method: 采用线性注意力模型进行理论分析，刻画微调目标如何修改注意力参数，并识别导致少样本性能下降的条件。通过理论推导和实证验证相结合的方式，系统研究了不同微调策略对上下文学习能力的影响。

Result: 1) 全参数微调会损害上下文学习能力；2) 仅限制更新值矩阵可在提升零样本性能的同时保留上下文学习能力；3) 引入辅助少样本损失主要增强目标任务上的上下文学习，但会牺牲对未见任务的适应能力；4) 通过实验验证了理论分析结果。

Conclusion: 微调对Transformer模型上下文学习能力的影响具有选择性，通过参数更新限制和损失函数设计可以实现零样本性能与上下文学习能力的平衡。该理论框架为理解大语言模型微调行为提供了新视角，对实际部署中的微调策略选择具有指导意义。

Abstract: Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.

</details>


### [39] [Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?](https://arxiv.org/abs/2602.23225)
*Pengxiang Li,Dilxat Muhtar,Lu Yin,Tianlong Chen,Shiwei Liu*

Main category: cs.CL

TL;DR: 针对扩散语言模型实际生成时趋于自回归的问题，本文提出数据为中心的NAP方法，通过整理多独立推理轨迹和并行强制解码策略，在数学推理基准测试中实现了更好的并行解码性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽被宣传支持并行生成，但实践中常退化为自回归式解码，存在顺序瓶颈和同步开销问题。其根本原因在于训练数据的强顺序结构与DLM目标不匹配，限制了硬件并行能力的充分利用和输出长度扩展时的延迟表现。

Method: 提出NAP（非自回归并行DLM）框架：1）将示例整理为多条独立推理轨迹；2）采用并行强制解码策略，鼓励多token并行更新，从而更好地对齐监督信号与非自回归并行解码。

Result: 在数学推理基准测试中，NAP在并行解码条件下表现优于使用标准长CoT数据训练的DLM，且性能提升随并行度增加而扩大。

Conclusion: 重新审视数据和监督方式是缓解自回归行为、实现真正非自回归并行生成的原则性方向，为扩散语言模型的优化提供了新思路。

Abstract: Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch between DLM objectives and the highly sequential structure of widely used training data, including standard pretraining corpora and long chain-of-thought (CoT) supervision. Motivated by this diagnosis, we propose NAP (Non-Autoregressive Parallel DLMs), a proof-of-concept, data-centric approach that better aligns supervision with non-AR parallel decoding. NAP curates examples as multiple independent reasoning trajectories and couples them with a parallel-forced decoding strategy that encourages multi-token parallel updates. Across math reasoning benchmarks, NAP yields stronger performance under parallel decoding than DLMs trained on standard long CoT data, with gains growing as parallelism increases. Our results suggest that revisiting data and supervision is a principled direction for mitigating AR-like behavior and moving toward genuinely non-autoregressive parallel generation in DLMs. Our code is available at https://github.com/pixeli99/NAP.

</details>


### [40] [A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations](https://arxiv.org/abs/2602.23300)
*Soumya Dutta,Smruthi Balaji,Sriram Ganapathy*

Main category: cs.CL

TL;DR: 提出MiSTER-E模型，采用专家混合架构分离模态特定建模与多模态融合，在IEMOCAP、MELD、MOSI三个数据集上分别达到70.9%、69.5%、87.9%的加权F1分数，无需依赖说话人身份。


<details>
  <summary>Details</summary>
Motivation: 对话情感识别面临两大核心挑战：捕捉多轮对话的时序流动性和有效整合多模态线索。传统方法难以同时处理模态特定上下文建模与多模态信息融合，缺乏解耦策略。

Method: MiSTER-E采用模块化专家混合架构，使用微调的语言模型生成语音和文本的utterance级嵌入，通过卷积-循环层增强上下文建模。系统包含三个专家（仅语音、仅文本、跨模态），由学习门控机制动态加权。引入监督对比损失对齐语音-文本表示，并采用KL散度正则化专家预测，全程不依赖说话人身份信息。

Result: 在IEMOCAP、MELD、MOSI三个基准数据集上分别获得70.9%、69.5%、87.9%的加权F1分数，显著优于多种基线语音-文本ERC系统。消融实验验证了各组件的有效性。

Conclusion: MiSTER-E成功解耦并解决了ERC中的核心挑战，实现了无需说话人身份信息的鲁棒多模态情感识别，为对话系统的情感理解提供了新思路。

Abstract: Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [41] [Enriching Taxonomies Using Large Language Models](https://arxiv.org/abs/2602.22213)
*Zeinab Ghamlouch,Mehwish Alam*

Main category: cs.IR

TL;DR: Taxoria是一个利用大语言模型增强现有分类法的流水线系统，通过种子扩展和候选节点验证来解决分类法覆盖不足和过时的问题，最终提供带溯源和可视化的增强分类法。


<details>
  <summary>Details</summary>
Motivation: 现有分类法存在覆盖范围有限、节点过时或模糊等问题，降低了知识检索的效果。传统方法直接从LLM提取内部分类法，而Taxoria旨在通过LLM增强现有分类法，保持其原有结构优势的同时进行补充和更新。

Method: Taxoria采用种子扩展策略，以现有分类法为基础，通过提示LLM生成候选节点。随后对候选节点进行验证以减轻幻觉并确保语义相关性，最后将其整合到原始分类法中，并提供溯源跟踪和可视化功能。

Result: 系统成功构建了增强版分类法，相比原始版本在覆盖范围上得到显著扩展，同时通过验证机制保证了新增节点的质量，并通过可视化工具支持后续分析。

Conclusion: 该方法有效解决了分类法的覆盖和时效性问题，为知识管理领域提供了一种可行的自动化增强方案，同时通过溯源机制确保了过程的可解释性和可信度。

Abstract: Taxonomies play a vital role in structuring and categorizing information across domains. However, many existing taxonomies suffer from limited coverage and outdated or ambiguous nodes, reducing their effectiveness in knowledge retrieval. To address this, we present Taxoria, a novel taxonomy enrichment pipeline that leverages Large Language Models (LLMs) to enhance a given taxonomy. Unlike approaches that extract internal LLM taxonomies, Taxoria uses an existing taxonomy as a seed and prompts an LLM to propose candidate nodes for enrichment. These candidates are then validated to mitigate hallucinations and ensure semantic relevance before integration. The final output includes an enriched taxonomy with provenance tracking and visualization of the final merged taxonomy for analysis.

</details>


### [42] [Adaptive Prefiltering for High-Dimensional Similarity Search: A Frequency-Aware Approach](https://arxiv.org/abs/2602.22214)
*Teodor-Ioan Calin*

Main category: cs.IR

TL;DR: 本文提出一种自适应预过滤框架，通过分析查询频率的Zipfian分布特征和簇一致性度量，动态分配高维相似性搜索的计算预算，在ImageNet-1k数据集上实现同等召回率的同时减少20.4%的距离计算量，并保持亚毫秒级GPU延迟。


<details>
  <summary>Details</summary>
Motivation: 现代检索系统依赖高维相似性搜索，但统一的静态搜索策略无法利用真实查询分布的异质性特征，导致计算资源浪费和效率低下，亟需能够自适应调整计算预算的智能方法。

Method: 将查询空间按频率划分为Zipfian分布层级，基于历史访问模式与局部密度特征分配差异化搜索策略；采用轻量级频率追踪器监控查询分布，为未见查询设计基于簇一致性的回退机制。

Result: 在CLIP嵌入的ImageNet-1k数据集上，频率感知预算分配相比静态nprobe选择减少20.4%距离计算量，同时保持GPU加速FAISS索引的亚毫秒级延迟，且引入的计算开销极小。

Conclusion: 该框架通过动态预算分配显著提升高维检索效率，为异构查询分布场景提供了实用解决方案，平衡了召回率、延迟与计算成本三者关系。

Abstract: High-dimensional similarity search underpins modern retrieval systems, yet uniform search strategies fail to exploit the heterogeneous nature of real-world query distributions. We present an adaptive prefiltering framework that leverages query frequency patterns and cluster coherence metrics to dynamically allocate computational budgets. Our approach partitions the query space into frequency tiers following Zipfian distributions and assigns differentiated search policies based on historical access patterns and local density characteristics. Experiments on ImageNet-1k using CLIP embeddings demonstrate that frequency-aware budget allocation achieves equivalent recall with 20.4% fewer distance computations compared to static nprobe selection, while maintaining sub-millisecond latency on GPU-accelerated FAISS indices. The framework introduces minimal overhead through lightweight frequency tracking and provides graceful degradation for unseen queries through coherence-based fallback policies.

</details>


### [43] [Retrieval-Augmented Generation Assistant for Anatomical Pathology Laboratories](https://arxiv.org/abs/2602.22216)
*Diogo Pires,Yuriy Perezhohin,Mauro Castelli*

Main category: cs.IR

TL;DR: 本研究针对解剖病理学实验室静态文档管理难题，开发并评估了一个检索增强生成（RAG）助手。基于99份葡萄牙医疗协议和323个问答对的十次实验显示，递归分块与混合检索结合生物医学专用嵌入模型（MedEmbed）效果最佳，使回答相关性达0.74、忠实度0.70、上下文召回率0.77，且k=1检索效率最优。该成果为医疗RAG系统部署提供了关键设计依据，展示了提升实验室工作流效率与患者安全的潜力。


<details>
  <summary>Details</summary>
Motivation: 解剖病理学中高达70%的医疗决策依赖于实验室诊断，但现有静态文档（如印刷手册或PDF）往往内容过时、信息碎片化且难以检索，导致工作流程错误和诊断延迟风险。因此，亟需一种能够提供实时、准确、基于上下文的协议查询答案的解决方案，以保障实验室工作效率和患者安全。

Method: 研究团队从葡萄牙某医疗机构整理了99份解剖病理学协议，构建了包含323个问答对的系统评估数据集。通过十次实验，系统性地比较了不同分块策略（如递归分块）、检索方法（如混合检索）和嵌入模型（通用模型vs生物医学专用MedEmbed）的性能。采用RAGAS框架（忠实度、答案相关性、上下文召回率）和top-k检索指标进行量化评估。

Result: 实验结果表明，递归分块结合混合检索方法建立了最佳基线性能。使用生物医学专用嵌入模型MedEmbed进一步将答案相关性提升至0.74，忠实度至0.70，上下文召回率至0.77。top-k分析揭示，仅检索排名最高的单个分块（k=1）即可实现效率与准确性的最优平衡，这反映了AP协议的模块化结构特征。

Conclusion: 本研究揭示了医疗领域RAG系统部署的关键设计考量，证明了领域专业化嵌入模型的重要性。该RAG助手有望将静态文档转变为动态、可靠的专业知识辅助工具，显著提升实验室工作流程效率，并支持患者安全。这一成果为未来在医疗领域推广智能知识管理系统提供了实证基础。

Abstract: Accurate and efficient access to laboratory protocols is essential in Anatomical Pathology (AP), where up to 70% of medical decisions depend on laboratory diagnoses. However, static documentation such as printed manuals or PDFs is often outdated, fragmented, and difficult to search, creating risks of workflow errors and diagnostic delays. This study proposes and evaluates a Retrieval-Augmented Generation (RAG) assistant tailored to AP laboratories, designed to provide technicians with context-grounded answers to protocol-related queries. We curated a novel corpus of 99 AP protocols from a Portuguese healthcare institution and constructed 323 question-answer pairs for systematic evaluation. Ten experiments were conducted, varying chunking strategies, retrieval methods, and embedding models. Performance was assessed using the RAGAS framework (faithfulness, answer relevance, context recall) alongside top-k retrieval metrics. Results show that recursive chunking and hybrid retrieval delivered the strongest baseline performance. Incorporating a biomedical-specific embedding model (MedEmbed) further improved answer relevance (0.74), faithfulness (0.70), and context recall (0.77), showing the importance of domain-specialised embeddings. Top-k analysis revealed that retrieving a single top-ranked chunk (k=1) maximized efficiency and accuracy, reflecting the modular structure of AP protocols. These findings highlight critical design considerations for deploying RAG systems in healthcare and demonstrate their potential to transform static documentation into dynamic, reliable knowledge assistants, thus improving laboratory workflow efficiency and supporting patient safety.

</details>


### [44] [Comparative Analysis of Neural Retriever-Reranker Pipelines for Retrieval-Augmented Generation over Knowledge Graphs in E-commerce Applications](https://arxiv.org/abs/2602.22219)
*Teri Rumble,Zbyněk Gazdík,Javad Zarrin,Jagdeep Ahluwalia*

Main category: cs.IR

TL;DR: 本文针对检索增强生成(RAG)在结构化知识图谱上的应用挑战，提出了多种Retriever-Reranker流水线并在电商领域进行对比评估，在STaRK半结构化知识库上实现了Hit@1提升20.4%和MRR提升14.5%的显著改进，为生产环境部署RAG系统提供了实用框架。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG在文本任务上表现优异，但应用于结构化知识图谱时面临检索扩展和上下文关系保持的挑战。跨编码器虽能提高检索精度，但其在结构化数据中的集成尚未充分探索。这对开发生产环境中的领域特定助手至关重要。

Method: 本研究设计并比较了多种Retriever-Reranker流水线，针对电商领域的知识图谱自然语言查询进行优化。使用STaRK半结构化知识库这一生产级电商数据集，评估了多种RAG流水线配置。

Result: 实验结果显著优于已发布的基准，Hit@1提高了20.4%，平均倒数排名(MRR)提高了14.5%。

Conclusion: 本研究建立了将领域特定半结构化知识库集成到生成式系统的实用框架，为生产就绪RAG系统的部署提供了可操作的见解，其影响不仅限于电商，还可扩展到其他需要从结构化知识库检索信息的领域。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed Natural Language Processing (NLP), enabling complex information retrieval and generation tasks. Retrieval-Augmented Generation (RAG) has emerged as a key innovation, enhancing factual accuracy and contextual grounding by integrating external knowledge sources with generative models. Although RAG demonstrates strong performance on unstructured text, its application to structured knowledge graphs presents challenges: scaling retrieval across connected graphs and preserving contextual relationships during response generation. Cross-encoders refine retrieval precision, yet their integration with structured data remains underexplored. Addressing these challenges is crucial for developing domain-specific assistants that operate in production environments. This study presents the design and comparative evaluation of multiple Retriever-Reranker pipelines for knowledge graph natural language queries in e-Commerce contexts. Using the STaRK Semi-structured Knowledge Base (SKB), a production-scale e-Commerce dataset, we evaluate multiple RAG pipeline configurations optimized for language queries. Experimental results demonstrate substantial improvements over published benchmarks, achieving 20.4% higher Hit@1 and 14.5% higher Mean Reciprocal Rank (MRR). These findings establish a practical framework for integrating domain-specific SKBs into generative systems. Our contributions provide actionable insights for the deployment of production-ready RAG systems, with implications that extend beyond e-Commerce to other domains that require information retrieval from structured knowledge bases.

</details>


### [45] [TWICE: An LLM Agent Framework for Simulating Personalized User Tweeting Behavior with Long-term Temporal Features](https://arxiv.org/abs/2602.22222)
*Bingrui Jin,Kunyao Lan,Mengyue Wu*

Main category: cs.IR

TL;DR: 本文提出TWICE框架，利用LLM解决现有用户模拟器在建模长期时间特征方面的不足。该框架通过整合个性化用户画像、事件驱动记忆模块和风格改写流程，实现同时捕捉社交媒体数据中的长期时间特性和个性化特征，显著提升了个性化用户发推行为的模拟效果。


<details>
  <summary>Details</summary>
Motivation: 现有用户模拟器主要关注集体行为或交互系统，在建模具有时间序列特性的个性化行为（如社交媒体发推）方面存在局限，难以有效捕捉长期时间动态和用户个性化特征。

Method: 提出基于大语言模型的TWICE框架，包含三个核心组件：1）个性化用户画像构建；2）事件驱动的记忆模块；3）个性化风格改写工作流。该框架利用社交媒体数据的长期时间性和个性化特征来模拟用户发推行为。

Result: 通过全面评估（重点关注发推风格和基于事件的行为变化），实验结果表明该框架有效整合了时间动态，在个性化用户模拟方面取得显著改进，为长期行为追踪提供了鲁棒解决方案。

Conclusion: TWICE框架成功解决了现有方法在长期时间特性建模方面的不足，通过有效整合个性化特征和时间动态，为社交媒体用户行为模拟提供了新的有效途径。

Abstract: User simulators are often used to generate large amounts of data for various tasks such as generation, training, and evaluation. However, existing approaches concentrate on collective behaviors or interactive systems, struggling with tasks that require modeling temporal characteristics. To address this limitation, we propose TWICE, an LLM-based framework that leverages the long-term temporal and personalized features of social media data. This framework integrates personalized user profiling, an event-driven memory module, and a workflow for personalized style rewriting, enabling simulation of personalized user tweeting behavior while capturing long-term temporal characteristics. In addition, we conduct a comprehensive evaluation with a focus on analyzing tweeting style and event-based changes in behavior. Experiment results demonstrate that our framework improves personalized user simulation by effectively incorporating temporal dynamics, providing a robust solution for long-term behavior tracking.

</details>


### [46] [SQaLe: A Large Text-to-SQL Corpus Grounded in Real Schemas](https://arxiv.org/abs/2602.22223)
*Cornelius Wolff,Daniel Gomm,Madelon Hulsebos*

Main category: cs.IR

TL;DR: 介绍SQaLe，一个基于135,875个真实数据库模式构建的大规模半合成text-to-SQL数据集，包含517,676个高质量查询三元组，解决了现有数据集在复杂度和多样性方面的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推动了text-to-SQL进展，但开发可泛化模型的关键瓶颈在于缺乏具有足够模式复杂度、查询多样性、领域覆盖和任务多样性的大规模数据集。

Method: 基于SchemaPile的135,875个真实数据库模式，采用原则性生成流水线，结合模式采样、问题合成和SQL构造，构建了517,676个高质量（问题，模式，查询）三元组的SQaLe数据集。

Result: SQaLe数据集捕捉了真实模式规模变异性、多样化查询模式和自然语言歧义性，同时保持执行有效性，在现有基准和数据集对比中被认为是最现实的大规模text-to-SQL数据集。

Conclusion: SQaLe数据集实现了在text-to-SQL研究中的数据扩展和模型泛化愿景，可通过Hugging Face访问。

Abstract: Advances in large language models have accelerated progress in text-to-SQL, methods for converting natural language queries into valid SQL queries. A key bottleneck for developing generalizable text-to-SQL models is the lack of large-scale datasets with sufficient schema and query complexity, domain coverage, and task diversity. We introduce SQaLe: a large-scale semi-synthetic text-to-SQL dataset built on 135,875 relational database schemas expanded from a collection of real-world schemas, SchemaPile. We establish a principled generation pipeline which combines schema sampling, question synthesis, and SQL construction, and produce 517,676 high-quality (question, schema, query) triples. The SQaLe dataset captures realistic schema size variability, diverse query patterns, and natural language ambiguity while maintaining execution validity. We provide an analysis of its contents and characteristics, and find that SQaLe introduces the most realistic large-scale text-to-SQL dataset to date in comparison with existing benchmarks and datasets. We discuss how SQaLe enables our vision for data scaling and model generalization in text-to-SQL research. The dataset is accessible at: https://huggingface.co/datasets/trl-lab/SQaLe-text-to-SQL-dataset.

</details>


### [47] [SmartChunk Retrieval: Query-Aware Chunk Compression with Planning for Efficient Document RAG](https://arxiv.org/abs/2602.22225)
*Xuechen Zhang,Koustava Goswami,Samet Oymak,Jiasi Chen,Nedim Lipka*

Main category: cs.IR

TL;DR: SmartChunk是一种查询自适应的检索增强生成框架，通过动态调整检索粒度来解决传统RAG中固定分块策略带来的检索质量敏感、噪声干扰和可扩展性差等问题，在保持准确性的同时提高效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统采用静态分块和平坦检索策略，存在分块大小敏感、易引入无关噪声、难以扩展到大型语料库等局限性，无法根据查询特性动态调整检索粒度，影响了长文档问答的准确性和效率。

Method: SmartChunk框架包含两个核心组件：(i) 规划器，通过创新的STITCH强化学习方案预测每个查询的最优分块抽象层级；(ii) 轻量级压缩模块，在不重复总结的情况下生成高级别分块嵌入。该框架实现检索粒度的动态自适应调整。

Result: 在五个问答基准测试和一个域外数据集上的评估表明，SmartChunk优于现有先进RAG基线方法，同时降低了成本。该方法展现出良好的大型语料库可扩展性，并在域外数据集上保持稳定提升。

Conclusion: SmartChunk作为一种通用自适应检索框架，通过查询自适应的检索粒度调整，有效平衡了准确性与效率，克服了固定策略的缺陷，为长文档问答提供了更稳健和高效的解决方案。

Abstract: Retrieval-augmented generation (RAG) has strong potential for producing accurate and factual outputs by combining language models (LMs) with evidence retrieved from large text corpora. However, current pipelines are limited by static chunking and flat retrieval: documents are split into short, predetermined, fixed-size chunks, embeddings are retrieved uniformly, and generation relies on whatever chunks are returned. This design brings challenges, as retrieval quality is highly sensitive to chunk size, often introduces noise from irrelevant or misleading chunks, and scales poorly to large corpora. We present SmartChunk retrieval, a query-adaptive framework for efficient and robust long-document question answering (QA). SmartChunk uses (i) a planner that predicts the optimal chunk abstraction level for each query, and (ii) a lightweight compression module that produces high-level chunk embeddings without repeated summarization. By adapting retrieval granularity on the fly, SmartChunk balances accuracy with efficiency and avoids the drawbacks of fixed strategies. Notably, our planner can reason about chunk abstractions through a novel reinforcement learning scheme, STITCH, which boosts accuracy and generalization. To reflect real-world applications, where users face diverse document types and query styles, we evaluate SmartChunk on five QA benchmarks plus one out-of-domain dataset. Across these evaluations, SmartChunk outperforms state-of-the-art RAG baselines, while reducing cost. Further analysis demonstrates strong scalability with larger corpora and consistent gains on out-of-domain datasets, highlighting its effectiveness as a general framework for adaptive retrieval.

</details>


### [48] [SEGB: Self-Evolved Generative Bidding with Local Autoregressive Diffusion](https://arxiv.org/abs/2602.22226)
*Yulong Gao,Wan Jiang,Mingzhe Cao,Xuepu Wang,Zeyu Pan,Haonan Yang,Ye Liu,Xin Yang*

Main category: cs.IR

TL;DR: 本文提出自演进生成式竞价框架SEGB，通过合成短期未来状态提供动态前瞻指导，并基于价值引导进行无外部干预的迭代策略优化，实现了仅依赖静态数据的稳健策略改进，在线上部署中取得+10.19%的目标成本提升


<details>
  <summary>Details</summary>
Motivation: 现有离线训练的生成式竞价策略缺乏应对动态市场的短期前瞻能力，且通常依赖模拟器或外部专家进行后训练改进，存在关键局限性

Method: SEGB框架首先为每个竞价合成可信的短期未来状态以提供动态前瞻，然后执行价值引导的策略精化，在不依赖外部干预的情况下迭代发现更优策略

Result: 在AuctionNet基准测试和大型A/B测试中显著优于现有最优基线；大规模在线部署实现目标成本+10.19%的增长，创造了显著的商业价值

Conclusion: 该自包含方法能够仅从静态数据中实现稳健的策略改进，验证了先进规划与演进范式的有效性，为动态市场中的自动化竞价提供了新范式

Abstract: In the realm of online advertising, automated bidding has become a pivotal tool, enabling advertisers to efficiently capture impression opportunities in real-time. Recently, generative auto-bidding has shown significant promise, offering innovative solutions for effective ad optimization. However, existing offline-trained generative policies lack the near-term foresight required for dynamic markets and usually depend on simulators or external experts for post-training improvement. To overcome these critical limitations, we propose Self-Evolved Generative Bidding (SEGB), a framework that plans proactively and refines itself entirely offline. SEGB first synthesizes plausible short-horizon future states to guide each bid, providing the agent with crucial, dynamic foresight. Crucially, it then performs value-guided policy refinement to iteratively discover superior strategies without any external intervention. This self-contained approach uniquely enables robust policy improvement from static data alone. Experiments on the AuctionNet benchmark and a large-scale A/B test validate our approach, demonstrating that SEGB significantly outperforms state-of-the-art baselines. In a large-scale online deployment, it delivered substantial business value, achieving a +10.19% increase in target cost, proving the effectiveness of our advanced planning and evolution paradigm.

</details>


### [49] [RETLLM: Training and Data-Free MLLMs for Multimodal Information Retrieval](https://arxiv.org/abs/2602.22278)
*Dawei Su,Dongsheng Wang*

Main category: cs.IR

TL;DR: 本文提出RetLLM，一种无需训练和数据的框架，通过提示MLLMs直接生成检索分数，采用粗-细两阶段管道和视觉增强模块，在多模态信息检索任务上超越微调模型，证明MLLMs具有内在的多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLMs的多模态信息检索方法虽然性能提升，但存在预训练不一致问题且需要大量数据。为此，本文探索如何利用MLLMs的内在推理能力，在不进行任何训练的情况下实现高性能检索。

Method: 将MMIR建模为相似度分数生成任务。采用粗-细两阶段流程：粗阶段通过top-k筛选策略构建高质量候选池；细阶段将查询和候选输入MLLM预测检索分数。此外，引入视觉增强模块帮助模型重新关注被遗忘的视觉信息。

Result: 在MMIR基准测试上，RetLLM表现优于微调模型。消融研究验证了每个组件的有效性，证明MLLMs无需训练即可获得强大的检索性能。

Conclusion: 该工作表明MLLMs具有内在的多模态推理能力，可在简单可扩展的框架中实现高性能检索，为MMIR提供了新的无训练范式。

Abstract: Multimodal information retrieval (MMIR) has gained attention for its flexibility in handling text, images, or mixed queries and candidates. Recent breakthroughs in multimodal large language models (MLLMs) boost MMIR performance by incorporating MLLM knowledge under the contrastive finetuning framework. However, they suffer from pre-training inconsistency and require large datasets. In this work, we introduce a novel framework, RetLLM, designed to query MLLMs for MMIR in a training- and data-free manner. Specifically, we formulate MMIR as a similarity score generation task and prompt MLLMs to directly predict retrieval scores in a coarse-then-fine pipeline. At the coarse stage, a top-k filtering strategy builds a small yet high-quality candidate pool for each query, enabling MLLMs to focus on semantically relevant candidates. Subsequently, the retrieval score is predicted by feeding both the query and candidate into MLLMs at the fine stage. Importantly, we propose a visual enhancement module during reasoning to help MLLMs re-pick forgotten visuals, improving retrieval. Extensive experiments on MMIR benchmarks show that RetLLM outperforms fine-tuned models. Ablation studies further verify each component. Our work demonstrates that MLLMs can achieve strong MMIR performance without any training, highlighting their inherent multimodal reasoning ability in a simple, scalable framework. We release our code at: https://github.com/alivecat05/RETLLM

</details>


### [50] [TFPS: A Temporal Filtration-enhanced Positive Sample Set Construction Method for Implicit Collaborative Filtering](https://arxiv.org/abs/2602.22521)
*Jiayi Wu,Zhengyu Wu,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 针对隐式反馈推荐中负采样策略忽视正样本探索及时间信息的问题，本文从数据角度出发提出TFPS方法，通过时间衰减模型构建加权二分图，分层过滤并增强层间策略，构建高质量正样本集，可提升Recall@k和NDCG@k，且兼容现有推荐器和负采样方法。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈协同过滤推荐依赖负采样构建正负样本，但现有方法过度关注负采样优化而忽视正样本质量。部分去噪推荐方法可处理正样本噪声却忽略时间信息，而现有工作虽整合序列信息却未考虑时间间隔，难以准确捕捉用户当前偏好。为此，本文从数据视角提出一种时间过滤增强方法，旨在构建高质量正样本集以提升推荐性能。

Method: 方法分为三步：首先，设计基于交互时间间隔的时间衰减模型，将原始用户-物品交互图转化为加权二分图；其次，通过预定义过滤操作对加权图进行分层处理；最后，设计层增强策略，从分层子图中构建高质量正样本集。理论分析证明了该方法对Recall@k和NDCG@k的提升效果。

Result: 在三个真实世界数据集上的大量实验验证了所提方法的有效性。此外，TFPS可与多种隐式协同过滤推荐器及负采样方法集成，进一步提升推荐性能。

Conclusion: 本文从数据角度提出TFPS方法，通过时间衰减模型、图分层和层增强策略构建高质量正样本集，有效解决了隐式反馈推荐中正样本探索不足及时间信息利用不充分的问题。该方法具有良好通用性，可与现有推荐框架无缝集成，为提升推荐系统性能提供了新思路。

Abstract: The negative sampling strategy can effectively train collaborative filtering (CF) recommendation models based on implicit feedback by constructing positive and negative samples. However, existing methods primarily optimize the negative sampling process while neglecting the exploration of positive samples. Some denoising recommendation methods can be applied to denoise positive samples within negative sampling strategies, but they ignore temporal information. Existing work integrates sequential information during model aggregation but neglects time interval information, hindering accurate capture of users' current preferences. To address this problem, from a data perspective, we propose a novel temporal filtration-enhanced approach to construct a high-quality positive sample set. First, we design a time decay model based on interaction time intervals, transforming the original graph into a weighted user-item bipartite graph. Then, based on predefined filtering operations, the weighted user-item bipartite graph is layered. Finally, we design a layer-enhancement strategy to construct a high-quality positive sample set for the layered subgraphs. We provide theoretical insights into why TFPS can improve Recall@k and NDCG@k, and extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed method. Additionally, TFPS can be integrated with various implicit CF recommenders or negative sampling methods to enhance its performance.

</details>


### [51] [Generative Agents Navigating Digital Libraries](https://arxiv.org/abs/2602.22529)
*Saber Zerhoudi,Michael Granitzer*

Main category: cs.IR

TL;DR: 本文提出Agent4DL，一种基于大语言模型的数字图书馆用户搜索行为模拟器。该模拟器通过生成逼真的用户画像和动态搜索会话（包括查询、点击和停止行为）来解决数字图书馆领域因隐私问题导致的用户搜索数据集稀缺的长期挑战。实验验证表明Agent4DL能准确复制真实用户交互，且在生成多样化和情境感知的用户行为方面优于现有模拟器SimIIR 2.0。


<details>
  <summary>Details</summary>
Motivation: 数字图书馆研究中长期存在一个关键挑战：由于隐私保护限制，缺乏公开可用的用户搜索模式数据集。这严重制约了相关研究进展。大语言模型的发展为模拟用户行为提供了新的可能性。

Method: 研究者开发了Agent4DL模拟器，该模拟器利用大语言模型生成符合特定用户画像的真实搜索行为。具体包括：1) 创建多样化的用户配置文件；2) 模拟动态搜索会话，涵盖查询构建、结果点击和会话终止等行为；3) 确保行为模式与用户特征相匹配。

Result: 通过与真实用户数据进行对比验证，Agent4DL在复制真实用户交互方面表现出良好的准确性。与现有主流模拟器SimIIR 2.0相比，Agent4DL在生成行为的多样性和情境感知能力方面具有竞争优势。

Conclusion: Agent4DL为数字图书馆用户行为研究提供了一个有效的仿真工具，成功解决了数据稀缺问题，并能够生成更丰富、更具情境感知能力的用户行为数据，有助于推动该领域的进一步发展。

Abstract: In the rapidly evolving field of digital libraries, the development of large language models (LLMs) has opened up new possibilities for simulating user behavior. This innovation addresses the longstanding challenge in digital library research: the scarcity of publicly available datasets on user search patterns due to privacy concerns. In this context, we introduce Agent4DL, a user search behavior simulator specifically designed for digital library environments. Agent4DL generates realistic user profiles and dynamic search sessions that closely mimic actual search strategies, including querying, clicking, and stopping behaviors tailored to specific user profiles. Our simulator's accuracy in replicating real user interactions has been validated through comparisons with real user data. Notably, Agent4DL demonstrates competitive performance compared to existing user search simulators such as SimIIR 2.0, particularly in its ability to generate more diverse and context-aware user behaviors.

</details>


### [52] [Towards Dynamic Dense Retrieval with Routing Strategy](https://arxiv.org/abs/2602.22547)
*Zhan Su,Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Bingbing Wen,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 这篇论文针对稠密检索（DR）的现有范式提出了动态稠密检索（DDR）方法。现有范式在数据有限时难以适应新领域，且模型更新成本高昂。DDR使用前缀调优作为领域专用模块，通过动态路由组合，仅需2%的训练参数就在六个零样本任务上超越DR，实现了更灵活的稠密检索。


<details>
  <summary>Details</summary>
Motivation: 当前稠密检索的应用范式是微调预训练模型以适应特定任务，但该范式存在两个主要局限：一是当训练数据有限时难以适配新领域，二是旧模型被从头训练的新模型简单替换，在需要频繁更新的场景下成本过高。这些限制阻碍了稠密检索在实际应用中的灵活性和可扩展性。

Method: 本文提出动态稠密检索（DDR）方法，采用前缀调优（prefix tuning）技术为特定领域构建专用模块。这些模块可通过动态路由策略进行组合，从而实现检索部分的高度灵活领域适配。该方法仅需训练少量参数即可实现强大的迁移能力。

Result: 在六个零样本下游任务上的广泛评估表明，DDR方法仅使用2%的训练参数就能超越传统稠密检索的性能。这证明了该方法在保持高性能的同时，显著降低了计算和存储开销，为信息检索中的灵活稠密检索铺平了道路。

Conclusion: 该研究为稠密检索在各种任务中的应用提供了有前景的未来方向。DDR通过模块化和组合化方法解决了现有范式的关键限制，实现了参数高效、灵活适应的检索系统，对推动稠密检索技术的实际应用具有重要意义。

Abstract: The \textit{de facto} paradigm for applying dense retrieval (DR) to new tasks involves fine-tuning a pre-trained model for a specific task. However, this paradigm has two significant limitations: (1) It is difficult adapt the DR to a new domain if the training dataset is limited.
  (2) Old DR models are simply replaced by newer models that are trained from scratch when the former are no longer up to date. Especially for scenarios where the model needs to be updated frequently, this paradigm is prohibitively expensive. To address these challenges, we propose a novel dense retrieval approach, termed \textit{dynamic dense retrieval} (DDR). DDR uses \textit{prefix tuning} as a \textit{module} specialized for a specific domain. These modules can then be compositional combined with a dynamic routing strategy, enabling highly flexible domain adaptation in the retrieval part. Extensive evaluation on six zero-shot downstream tasks demonstrates that this approach can surpass DR while utilizing only 2\% of the training parameters, paving the way to achieve more flexible dense retrieval in IR. We see it as a promising future direction for applying dense retrieval to various tasks.

</details>


### [53] [Where Relevance Emerges: A Layer-Wise Study of Internal Attention for Zero-Shot Re-Ranking](https://arxiv.org/abs/2602.22591)
*Haodong Chen,Shengyao Zhuang,Zheng Yao,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.IR

TL;DR: 该研究针对大语言模型零样本文档重排序中生成式方法延迟高、一致性差的问题，提出Selective-ICR策略。通过正交实验发现transformer层间存在普适的"钟形曲线"信号分布，选择性地提取关键层信号可在保持效果的同时降低30-50%推理延迟。在BRIGHT基准上，零样本8B模型媲美14B强化学习模型，0.6B模型超越SOTA生成式方法，揭示了内部注意力信号的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的零样本文档重排序方法主要依赖生成式评分或logits，存在推理延迟高和结果不一致的瓶颈。虽然上下文重排序(ICR)通过提取内部注意力信号避免了生成开销，但其简单聚合全层信号的策略忽略了层间贡献差异和跨架构一致性。此外，缺乏在统一条件下系统比较生成式、似然式和内部注意力机制的研究。

Method: 本文对生成式、似然式和内部注意力三种机制在多个重排序框架下进行正交评估；分析transformer各层相关性信号的分布规律，发现其呈现普适的"钟形曲线"模式；基于此提出Selective-ICR策略，选择性提取关键层注意力信号而非全层聚合。

Result: 实验发现transformer层间相关性信号呈现普适的"钟形曲线"分布。Selective-ICR策略在不影响效果的前提下降低30-50%推理延迟。在推理密集型的BRIGHT基准测试中，零样本8B模型性能达到14B强化学习重排序器的水平，0.6B模型甚至超越现有最优生成式方法。

Conclusion: 该研究揭示了内部注意力信号在复杂推理排序任务中的巨大潜力，Selective-ICR策略重新定义了LLM重排序的效率-效果边界。结果表明，在推理密集型任务中，精确捕捉高质量内部信号可显著降低对模型规模和强化学习的依赖，为小模型应用提供了新方向。

Abstract: Zero-shot document re-ranking with Large Language Models (LLMs) has evolved from Pointwise methods to Listwise and Setwise approaches that optimize computational efficiency. Despite their success, these methods predominantly rely on generative scoring or output logits, which face bottlenecks in inference latency and result consistency. In-Context Re-ranking (ICR) has recently been proposed as an $O(1)$ alternative method. ICR extracts internal attention signals directly, avoiding the overhead of text generation. However, existing ICR methods simply aggregate signals across all layers; layer-wise contributions and their consistency across architectures have been left unexplored. Furthermore, no unified study has compared internal attention with traditional generative and likelihood-based mechanisms across diverse ranking frameworks under consistent conditions.
  In this paper, we conduct an orthogonal evaluation of generation, likelihood, and internal attention mechanisms across multiple ranking frameworks. We further identify a universal "bell-curve" distribution of relevance signals across transformer layers, which motivates the proposed Selective-ICR strategy that reduces inference latency by 30%-50% without compromising effectiveness. Finally, evaluation on the reasoning-intensive BRIGHT benchmark shows that precisely capturing high-quality in-context attention signals fundamentally reduces the need for model scaling and reinforcement learning: a zero-shot 8B model matches the performance of 14B reinforcement-learned re-rankers, while even a 0.6B model outperforms state-of-the-art generation-based approaches. These findings redefine the efficiency-effectiveness frontier for LLM-based re-ranking and highlight the latent potential of internal signals for complex reasoning ranking tasks. Our code and results are publicly available at https://github.com/ielab/Selective-ICR.

</details>


### [54] [Fine-grained Semantics Integration for Large Language Model-based Recommendation](https://arxiv.org/abs/2602.22632)
*Jiawen Feng,Xiaoyu Kong,Leheng Sheng,Bin Wu,Chao Yi,Feifang Yang,Xiang-Rong Sheng,Han Zhu,Xiang Wang,Jiancan Wu,Xiangnan He*

Main category: cs.IR

TL;DR: 本文提出TS-Rec模型，通过语义感知初始化和令牌级语义对齐解决LLM推荐系统中SID空间的语义断裂问题，在两个真实基准测试中全面超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成式推荐系统面临两个根本挑战：SID令牌随机初始化导致与预训练语言空间的语义链接断裂；以及监督微调对齐仅聚焦项目级优化，忽视SID序列内部的令牌级语义，亟需细粒度语义建模方案。

Method: TS-Rec包含两个核心组件：1)语义感知嵌入初始化(SA-Init)，利用教师模型提取关键词并对其预训练嵌入进行平均池化来初始化SID令牌嵌入；2)令牌级语义对齐(TS-Align)，将SID序列中的单个令牌与对应项目集群的共享语义进行对齐。

Result: 在两个真实世界基准测试上的广泛实验表明，TS-Rec在所有标准指标上持续优于传统和生成式基线。

Conclusion: 细粒度语义信息的集成显著提升了LLM生成式推荐器的性能，验证了令牌级语义建模在连接语言空间与推荐空间中的关键作用。

Abstract: Recent advances in Large Language Models (LLMs) have shifted in recommendation systems from the discriminative paradigm to the LLM-based generative paradigm, where the recommender autoregressively generates sequences of semantic identifiers (SIDs) for target items conditioned on historical interaction. While prevalent LLM-based recommenders have demonstrated performance gains by aligning pretrained LLMs between the language space and the SID space, modeling the SID space still faces two fundamental challenges: (1) Semantically Meaningless Initialization: SID tokens are randomly initialized, severing the semantic linkage between the SID space and the pretrained language space at start point, and (2) Coarse-grained Alignment: existing SFT-based alignment tasks primarily focus on item-level optimization, while overlooking the semantics of individual tokens within SID sequences.To address these challenges, we propose TS-Rec, which can integrate Token-level Semantics into LLM-based Recommenders. Specifically, TS-Rec comprises two key components: (1) Semantic-Aware embedding Initialization (SA-Init), which initializes SID token embeddings by applying mean pooling to the pretrained embeddings of keywords extracted by a teacher model; and (2) Token-level Semantic Alignment (TS-Align), which aligns individual tokens within the SID sequence with the shared semantics of the corresponding item clusters. Extensive experiments on two real-world benchmarks demonstrate that TS-Rec consistently outperforms traditional and generative baselines across all standard metrics. The results demonstrate that integrating fine-grained semantic information significantly enhances the performance of LLM-based generative recommenders.

</details>


### [55] [Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators](https://arxiv.org/abs/2602.22647)
*Zhengyang Su,Isay Katsman,Yueqi Wang,Ruining He,Lukasz Heldt,Raghunandan Keshavan,Shao-Chuan Wang,Xinyang Yi,Mingyan Gao,Onkar Dalal,Lichan Hong,Ed Chi,Ningren Han*

Main category: cs.IR

TL;DR: 针对工业推荐系统中约束生成式检索的延迟问题，本文提出STATIC方法，通过将前缀树结构压缩为CSR稀疏矩阵，在TPU/GPU上实现全向量化约束解码。部署于十亿级用户视频平台，延迟开销仅0.033ms/步，相比CPU Trie和硬件基线分别提升948倍和47-1033倍加速，首次实现生产级严格约束生成式检索。


<details>
  <summary>Details</summary>
Motivation: 生成式检索虽在LLM推荐中表现出色，但工业场景需基于业务逻辑（如内容新鲜度、品类策略）限制输出空间，标准自回归解码无法原生支持。现有前缀树约束解码在TPU/GPU上因不规则内存访问导致严重延迟，亟需硬件友好的高效解码方案。

Method: 设计STATIC框架，核心创新是将Trie树"静态化"为压缩稀疏行（CSR）矩阵。通过扁平化树结构，将动态遍历转换为稀疏矩阵乘法，充分利用TPU/GPU的并行计算能力。该方法保持约束精确性，同时实现计算图优化与内存访问局部性提升。

Result: 在YouTube视频推荐系统实测：每步0.033ms延迟，占总推理时间0.25%；对比CPU Trie实现948倍加速，对比硬件加速二分查找基线提升47-1033倍；支持十亿级项目库，在多种配置下保持稳定低开销。学术基准测试显示冷启动性能显著提升。

Conclusion: STATIC首次实现严格约束生成式检索的工业级部署，通过硬件适配的稀疏表示解决效率瓶颈。该方法为大规模推荐系统提供可扩展约束解码基础设施，促进生成式检索技术落地，代码已开源。

Abstract: Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.

</details>


### [56] [Generative Recommendation for Large-Scale Advertising](https://arxiv.org/abs/2602.22732)
*Ben Xue,Dan Liu,Lixiang Wang,Mingjie Sun,Peng Wang,Pengfei Zhang,Shaoyun Shi,Tianyu Xu,Yunhao Sha,Zhiqiang Liu,Bo Kong,Bo Wang,Hang Yang,Jieting Xue,Junhao Wang,Shengyu Wang,Shuping Hui,Wencai Ye,Xiao Lin,Yongzhi Li,Yuhang Chen,Zhihui Yin,Quan Chen,Shiyang Wen,Wenjin Wu,Han Li,Guorui Zhou,Changcheng Li,Peng Jiang*

Main category: cs.IR

TL;DR: GR4AD是一个面向生产的生成式推荐系统，专为大规模广告场景设计。它提出了四项核心技术：UA-SID统一广告语义ID处理复杂业务信息，LazyAR惰性自回归解码器降低推理成本，VSL和RSPO价值感知优化算法对齐业务目标，以及动态beam serving实现计算资源自适应。在快手4亿用户系统上线后，A/B测试显示广告收入提升最高达4.2%。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式推荐在扩展性和模型容量方面展现出潜力，但将其部署于大规模广告实时系统不能简单套用大语言模型的训练和服务模式。现有方法缺乏针对广告业务的定制化设计，难以在固定服务预算下实现效果与效率的平衡，也无法直接优化业务价值指标。因此需要一套从架构、学习到服务全流程协同设计的生产级解决方案。

Method: 本文提出GR4AD框架，包含四个核心方法：1) 在tokenization层面，设计UA-SID统一广告语义ID，编码复杂商业信息；2) 在架构层面，引入LazyAR惰性自回归解码器，通过放宽层间依赖实现短序列多候选生成，降低推理开销；3) 在优化层面，结合VSL价值感知监督学习和RSPO排序引导的Softmax偏好优化，实现列表级强化学习以优化业务价值奖励；4) 在服务层面，提出动态beam serving机制，根据生成层级和在线负载自适应调整beam宽度。

Result: 大规模在线A/B测试表明，GR4AD相比现有DLRM基线系统在广告收入上提升最高达4.2%。该系统已在快手广告系统完成全量部署，服务超过4亿用户，实现了高吞吐量的实时推理。结果表明，模型扩展和推理时扩展均带来持续收益，验证了各组件设计的有效性。

Conclusion: GR4AD成功构建了一个端到端协同优化的生产级生成式推荐系统，通过架构-学习-服务三位一体的联合设计，在保持高吞吐实时服务的同时显著提升广告收入。该工作证明，针对特定场景定制化的生成推荐方案比通用LLM范式更具商业价值，为工业界大规模部署提供了可行路径。

Abstract: Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.

</details>


### [57] [PSQE: A Theoretical-Practical Approach to Pseudo Seed Quality Enhancement for Unsupervised MMEA](https://arxiv.org/abs/2602.22903)
*Yunpeng Hong,Chenyang Bu,Jie Zhang,Yi He,Di Wu,Xindong Wu*

Main category: cs.IR

TL;DR: 该论文针对多模态实体对齐中伪种子覆盖不平衡问题，提出PSQE方法，通过多模态信息和聚类重采样提升伪种子质量。理论分析了伪种子对对比学习吸引与排斥项的双重影响，以及覆盖不平衡导致模型偏向高密度区域的缺陷。实验表明PSQE作为即插即用模块可显著提升基线模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态实体对齐旨在整合不同模态的结构化数据以增强大语言模型应用，但现有无监督方法依赖伪对齐种子，其多模态信息引入导致知识图谱中伪种子覆盖不平衡，使模型优先学习高密度区域而削弱稀疏区域实体表示能力。

Method: 提出PSQE框架，利用多模态信息增强伪种子精度与图覆盖平衡性，并采用聚类重采样策略。理论分析揭示伪种子在对比学习中同时影响吸引和排斥项，且覆盖不平衡会造成模型偏差。

Result: 实验结果验证了理论分析，证实PSQE作为即插即用模块能在多模态实体对齐任务中显著提升各类基线模型的性能表现。

Conclusion: PSQE通过提升伪种子质量有效缓解了覆盖不平衡问题，增强了模型对稀疏区域实体的学习能力，具有良好通用性和实用价值。

Abstract: Multimodal Entity Alignment (MMEA) aims to identify equivalent entities across different data modalities, enabling structural data integration that in turn improves the performance of various large language model applications. To lift the requirement of labeled seed pairs that are difficult to obtain, recent methods shifted to an unsupervised paradigm using pseudo-alignment seeds. However, unsupervised entity alignment in multimodal settings remains underexplored, mainly because the incorporation of multimodal information often results in imbalanced coverage of pseudo-seeds within the knowledge graph. To overcome this, we propose PSQE (Pseudo-Seed Quality Enhancement) to improve the precision and graph coverage balance of pseudo seeds via multimodal information and clustering-resampling. Theoretical analysis reveals the impact of pseudo seeds on existing contrastive learning-based MMEA models. In particular, pseudo seeds can influence the attraction and the repulsion terms in contrastive learning at once, whereas imbalanced graph coverage causes models to prioritize high-density regions, thereby weakening their learning capability for entities in sparse regions. Experimental results validate our theoretical findings and show that PSQE as a plug-and-play module can improve the performance of baselines by considerable margins.

</details>


### [58] [SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress](https://arxiv.org/abs/2602.22913)
*Yang Yu,Lei Kou,Huaikuan Yi,Bin Chen,Yayu Cao,Lei Shen,Chao Zhang,Bing Wang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 本文提出SIGMA，一个应用于AliExpress的语义接地指令驱动生成式多任务推荐系统，通过统一潜在空间语义接地、混合物品标记化、多任务指令微调及自适应概率融合机制，解决现有生成式推荐适应性差、任务单一的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法仍局限于交互驱动的下一物品预测范式，无法快速适应变化趋势，也难以满足现实场景中多样化的推荐任务和特定业务需求。

Method: 首先，通过统一潜在空间将物品实体接地到通用语义，捕获语义与协同关系；其次，开发混合物品标记化方法实现精确建模与高效生成；然后，构建大规模多任务SFT数据集，使模型能够通过指令遵循完成各类推荐需求；最后，设计三步物品生成流程，结合自适应概率融合机制，根据任务特定要求对输出分布进行校准，平衡准确性与多样性。

Result: 广泛的离线实验和在线A/B测试均证明了SIGMA的有效性。

Conclusion: SIGMA通过语义接地、指令驱动和多任务学习框架，为电商推荐系统提供了更灵活、适应性更强的生成式推荐解决方案，在保持推荐准确性的同时显著提升了多样性。

Abstract: With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress. Specifically, we first ground item entities in general semantics via a unified latent space capturing both semantic and collaborative relations. Building upon this, we develop a hybrid item tokenization method for precise modeling and efficient generation. Moreover, we construct a large-scale multi-task SFT dataset to empower SIGMA to fulfill various recommendation demands via instruction-following. Finally, we design a three-step item generation procedure integrated with an adaptive probabilistic fusion mechanism to calibrate the output distributions based on task-specific requirements for recommendation accuracy and diversity. Extensive offline experiments and online A/B tests demonstrate the effectiveness of SIGMA.

</details>


### [59] [Sequential Regression for Continuous Value Prediction using Residual Quantization](https://arxiv.org/abs/2602.23012)
*Runpeng Cui,Zhipeng Sun,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 本文提出了一种基于残差量化(RQ)的序列学习框架，通过递归预测有序量化码来表示连续值，并学习序对齐的码嵌入表示，在推荐系统的LTV、观看时长和GMV预测任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的连续值预测（如观看时长、GMV）面临数据分布复杂且长尾的固有挑战。现有生成式方法依赖严格的参数分布假设，当假设与真实数据不符时性能受限：过于简化的模型无法捕捉数据复杂性，而复杂假设又导致可扩展性和泛化能力差。

Method: 提出残差量化(RQ)序列学习框架：(1)将连续值表示为有序量化码的和；(2)从粗粒度到细粒度递归预测，每层量化误差递减；(3)设计表示学习目标，使RQ码嵌入空间与目标值的序结构对齐；(4)学习量化码的连续表示以进一步提升精度。

Result: 在公开LTV和观看时长基准测试及工业级短视频平台的GMV大规模在线实验中，该方法一致性地超越现有先进技术，展现出在多样化连续值预测任务中的强泛化能力。

Conclusion: RQ-based框架通过递归量化预测和序对齐表示学习，有效解决了复杂长尾分布下的连续值预测问题，在准确性和泛化性上均取得显著提升，为工业推荐系统提供了实用解决方案。

Abstract: Continuous value prediction plays a crucial role in industrial-scale recommendation systems, including tasks such as predicting users' watch-time and estimating the gross merchandise value (GMV) in e-commerce transactions. However, it remains challenging due to the highly complex and long-tailed nature of the data distributions. Existing generative approaches rely on rigid parametric distribution assumptions, which fundamentally limits their performance when such assumptions misalign with real-world data. Overly simplified forms cannot adequately model real-world complexities, while more intricate assumptions often suffer from poor scalability and generalization.
  To address these challenges, we propose a residual quantization (RQ)-based sequence learning framework that represents target continuous values as a sum of ordered quantization codes, predicted recursively from coarse to fine granularity with diminishing quantization errors. We introduce a representation learning objective that aligns RQ code embedding space with the ordinal structure of target values, allowing the model to capture continuous representations for quantization codes and further improving prediction accuracy. We perform extensive evaluations on public benchmarks for lifetime value (LTV) and watch-time prediction, alongside a large-scale online experiment for GMV prediction on an industrial short-video recommendation platform. The results consistently show that our approach outperforms state-of-the-art methods, while demonstrating strong generalization across diverse continuous value prediction tasks in recommendation systems.

</details>


### [60] [MoDora: Tree-Based Semi-Structured Document Analysis System](https://arxiv.org/abs/2602.23061)
*Bangrui Xu,Qihang Yao,Zirui Tang,Xuanhe Zhou,Yeye He,Shihan Yu,Qianqian Xu,Bin Wang,Guoliang Li,Conghui He,Fan Wu*

Main category: cs.IR

TL;DR: 本文提出MoDora，一个基于LLM的半结构化文档分析系统，通过组件关联树和智能检索策略解决OCR元素碎片化、层次结构建模和跨区域信息检索三大挑战，在问答任务中准确率相比基线提升5.97%-61.07%。


<details>
  <summary>Details</summary>
Motivation: 半结构化文档（包含表格、图表、层次化段落等）在现实中广泛存在，但现有方法难以应对三大技术挑战：OCR提取的元素碎片化且缺乏语义上下文；缺乏有效的层次结构和布局差异表示；问答所需信息分散在多个区域或页面，需跨区检索与对齐。

Method: 1) 局部对齐聚合策略：将OCR解析元素转换为布局感知组件，对层次标题或非文本元素进行类型特定信息抽取；2) 组件关联树(CCTree)：通过自底向上级联摘要构建层次化组件结构，显式建模组件间关系与布局差异；3) 问题类型感知检索：支持基于布局的网格划分（位置检索）和LLM引导剪枝（语义检索）。

Result: 在多项实验中，MoDora相比基线方法在准确率上取得5.97%-61.07%的显著提升。

Conclusion: MoDora成功解决了半结构化文档问答中的关键技术难题，通过层次化组织和智能检索有效整合碎片化信息，性能显著优于现有方法，为文档智能分析提供了新范式。

Abstract: Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.
  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.

</details>


### [61] [MaRI: Accelerating Ranking Model Inference via Structural Re-parameterization in Large Scale Recommendation System](https://arxiv.org/abs/2602.23105)
*Yusheng Huang,Pengbo Xu,Shen Wang,Changxin Lao,Jiangxia Cao,Shuang Wen,Shuang Yang,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 针对大规模推荐系统中排序模型加速导致精度损失的问题，本文提出MaRI矩阵重参数化推理框架，通过优化特征融合矩阵乘法的冗余计算实现无损加速。


<details>
  <summary>Details</summary>
Motivation: 现有排序模型加速方法（结构轻量化、知识蒸馏）存在非可忽略的精度损失，且从特征融合矩阵乘法优化角度实现无损加速的研究尚不充分，用户侧计算冗余现象明显。

Method: 提出MaRI框架，基于结构重参数化思想，识别并消除特征融合矩阵乘法中的用户侧冗余计算，作为现有加速技术的补充方案。

Result: 在不损失精度的前提下实现了排序模型推理加速。

Conclusion: MaRI通过结构重参数化有效解决了特征融合计算冗余问题，为推荐系统排序模型提供了无损加速的新范式，可与现有技术协同使用。

Abstract: Ranking models, i.e., coarse-ranking and fine-ranking models, serve as core components in large-scale recommendation systems, responsible for scoring massive item candidates based on user preferences. To meet the stringent latency requirements of online serving, structural lightweighting or knowledge distillation techniques are commonly employed for ranking model acceleration. However, these approaches typically lead to a non-negligible drop in accuracy. Notably, the angle of lossless acceleration by optimizing feature fusion matrix multiplication, particularly through structural reparameterization, remains underexplored. In this paper, we propose MaRI, a novel Matrix Re-parameterized Inference framework, which serves as a complementary approach to existing techniques while accelerating ranking model inference without any accuracy loss. MaRI is motivated by the observation that user-side computation is redundant in feature fusion matrix multiplication, and we therefore adopt the philosophy of structural reparameterization to alleviate such redundancy.

</details>


### [62] [From Agnostic to Specific: Latent Preference Diffusion for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2602.23132)
*Ruochen Yang,Xiaodong Li,Jiawei Sheng,Jiangxia Cao,Xinkui Lin,Shen Wang,Shuang Yang,Zhaojie Liu,Tingwen Liu*

Main category: cs.IR

TL;DR: 针对多行为序列推荐中现有方法忽略用户潜在偏好且无法有效建模行为-物品不确定性的问题，本文提出FatsMB扩散模型框架，通过在潜在空间中从行为无关到行为特定的偏好生成机制，实现多样且准确的推荐。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽从行为固定转向行为特定推荐，但存在两大局限：1) 忽略用户决策背后的潜在偏好表征，导致次优解；2) 物品与行为的不对称确定性关系使判别范式无法捕捉从低熵行为到高熵物品的不确定性，推荐效率与多样性不足。

Method: 提出FatsMB框架：1) 多行为自编码器(MBAE)构建统一用户潜在偏好空间，促进行为间交互协作；2) 行为感知RoPE(BaRoPE)融合多源信息；3) 在潜在空间进行目标行为特定偏好迁移，注入信息性先验；4) 多条件引导层归一化(MCGLN)实现扩散去噪过程。

Result: 在真实世界数据集上的大量实验验证了FatsMB模型的有效性。

Conclusion: 该扩散模型框架成功解决了多行为序列推荐中潜在偏好学习和不确定性建模的关键挑战，通过行为无关到行为特定的生成式偏好迁移，实现了高效且多样的推荐性能，为多行为推荐提供了新范式。

Abstract: Multi-behavior sequential recommendation (MBSR) aims to learn the dynamic and heterogeneous interactions of users' multi-behavior sequences, so as to capture user preferences under target behavior for the next interacted item prediction. Unlike previous methods that adopt unidirectional modeling by mapping auxiliary behaviors to target behavior, recent concerns are shifting from behavior-fixed to behavior-specific recommendation. However, these methods still ignore the user's latent preference that underlying decision-making, leading to suboptimal solutions. Meanwhile, due to the asymmetric deterministic between items and behaviors, discriminative paradigm based on preference scoring is unsuitable to capture the uncertainty from low-entropy behaviors to high-entropy items, failing to provide efficient and diverse recommendation. To address these challenges, we propose \textbf{FatsMB}, a framework based diffusion model that guides preference generation \textit{\textbf{F}rom Behavior-\textbf{A}gnostic \textbf{T}o Behavior-\textbf{S}pecific} in latent spaces, enabling diverse and accurate \textit{\textbf{M}ulti-\textbf{B}ehavior Sequential Recommendation}. Specifically, we design a Multi-Behavior AutoEncoder (MBAE) to construct a unified user latent preference space, facilitating interaction and collaboration across Behaviors, within Behavior-aware RoPE (BaRoPE) employed for multiple information fusion. Subsequently, we conduct target behavior-specific preference transfer in the latent space, enriching with informative priors. A Multi-Condition Guided Layer Normalization (MCGLN) is introduced for the denoising. Extensive experiments on real-world datasets demonstrate the effectiveness of our model.

</details>


### [63] [Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments](https://arxiv.org/abs/2602.23234)
*Evangelia Christakopoulou,Vivekkumar Patel,Hemanth Velaga,Sandip Gaikwad*

Main category: cs.IR

TL;DR: 苹果应用商店通过微调专用LLM生成海量文本相关性标签，解决专家标注稀缺问题，成功优化搜索排序系统，实现行为相关性与文本相关性双提升，全球A/B测试转化率提升0.24%。


<details>
  <summary>Details</summary>
Motivation: 大规模商业搜索系统需要同时优化行为相关性（用户点击/下载）和文本相关性（语义匹配），但面临专家标注的文本相关性标签稀缺而行为标签丰富的矛盾。

Method: 系统评估LLM配置，确定微调专用模型优于大型预训练模型；利用该模型生成数百万文本相关性标签；将这些标签融入生产排序器。

Result: 离线NDCG在行为相关性和文本相关性上同步提升，帕累托前沿外移；全球A/B测试显示转化率提升+0.24%，长尾查询性能提升最显著。

Conclusion: 专用LLM可作为"力量倍增器"生成高质量文本相关性标签，有效解决数据稀缺问题，实现多目标协同优化，在行为信号缺失的长尾查询中价值尤为突出。

Abstract: Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in conversion rate, with the most substantial performance gains occurring in tail queries, where the new textual relevance labels provide a robust signal in the absence of reliable behavioral relevance labels.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning](https://arxiv.org/abs/2602.22227)
*Yicheng Bao,Xuhong Wang,Xin Tan*

Main category: cs.LG

TL;DR: 针对多模态大语言模型在复杂视觉场景中的感知脆弱性，本文提出AOT自博弈框架，通过图像编辑攻击器与防御模型的协同进化生成动态对抗课程，显著提升模型鲁棒性并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型尽管能力强大，但在视觉复杂场景中表现出感知脆弱性，其根源在于对有限训练数据的依赖，数据扩展成本高昂且存在鲁棒性天花板，亟需可扩展的可靠性提升方案。

Method: 构建AOT（对抗对手训练）自博弈框架，设计图像编辑攻击器与防御MLLM的协同进化机制，攻击器生成多样化动态对抗图像课程，迫使防御者持续适应改进；配套构建大规模对抗数据集AOT-SFT。

Result: 大量实验验证AOT显著增强防御模型的感知鲁棒性，有效降低幻觉现象，证明该框架在提升多模态大模型可靠性方面的有效性。

Conclusion: 建立了可扩展的多模态大模型训练新范式，通过自我对抗博弈实现鲁棒性突破，为缓解感知脆弱性提供了系统性解决方案。

Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustness. We introduce \textbf{AOT-SFT}, a large-scale adversarial dataset for bootstrapping MLLM robustness. Building on this, we propose \textbf{AOT (Adversarial Opponent Training)}, a self-play framework that forges MLLM robustness by creating its own training data. Our method orchestrates a co-evolution between an image-editing Attacker and a Defender MLLM, where the Attacker generates a diverse and dynamic curriculum of image manipulations, forcing the Defender to adapt and improve. Extensive experiments demonstrate that AOT enhances the Defender's perceptual robustness and reduces hallucinations, establishing a scalable paradigm for training more reliable MLLMs.

</details>


### [65] [Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials](https://arxiv.org/abs/2602.22251)
*Alex Morehead,Miruna Cretu,Antonia Panescu,Rishabh Anand,Maurice Weiler,Tynan Perez,Samuel Blau,Steven Farrell,Wahid Bhimji,Anubhav Jain,Hrushikesh Sahasrabuddhe,Pietro Lio,Tommi Jaakkola,Rafael Gomez-Bombarelli,Rex Ying,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: Zatom-1是首个统一3D分子和材料生成与预测学习的基础模型，采用多模态流匹配Transformer架构，联合建模离散原子类型和连续3D几何结构。该模型支持可扩展预训练，在生成和预测基准测试中均达到或超越专用基线水平，同时将生成推理时间缩短一个数量级以上，并展现出跨化学域的正向迁移能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法通常针对单一化学域（分子或材料）和单一任务（生成或预测）进行优化，限制了表征共享和知识迁移。通用型3D化学建模需要同时具备生成和预测能力，但缺乏统一框架导致模型效率低下且无法利用跨域知识。

Method: Zatom-1采用多模态流匹配目标训练Transformer架构，联合建模离散原子类型和连续3D几何。通过联合生成式预训练作为下游多任务预测（性质、能量、力）的通用初始化。该架构支持模型容量扩展时的可预测性能提升，并实现快速稳定采样。

Result: 在生成和预测基准测试中匹配或超越专用基线；生成推理时间减少超过10倍；预训练中建模材料可提升分子性质预测准确性；验证了跨化学域的正向迁移效应。

Conclusion: Zatom-1证明了统一生成与预测学习框架的有效性，为3D化学建模提供了可扩展的基础模型范式，通过跨域知识迁移显著提升效率与性能，为未来化学AI研究开辟了新方向。

Abstract: General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation sharing and transfer. We introduce Zatom-1, the first foundation model that unifies generative and predictive learning of 3D molecules and materials. Zatom-1 is a Transformer trained with a multimodal flow matching objective that jointly models discrete atom types and continuous 3D geometries. This approach supports scalable pretraining with predictable gains as model capacity increases, while enabling fast and stable sampling. We use joint generative pretraining as a universal initialization for downstream multi-task prediction of properties, energies, and forces. Empirically, Zatom-1 matches or outperforms specialized baselines on both generative and predictive benchmarks, while reducing the generative inference time by more than an order of magnitude. Our experiments demonstrate positive predictive transfer between chemical domains from joint generative pretraining: modeling materials during pretraining improves molecular property prediction accuracy.

</details>


### [66] [Code World Models for Parameter Control in Evolutionary Algorithms](https://arxiv.org/abs/2602.22260)
*Camilo Chacón Sartori,Guillem Rodríguez Corominas*

Main category: cs.LG

TL;DR: 一篇探索大语言模型（LLM）学习优化器行为并控制其决策的论文。作者将代码世界模型（CWM）从确定性游戏扩展到随机组合优化问题，利用次优轨迹训练LLM生成优化器动态模拟器，并通过贪心规划自适应选择突变强度k。实验显示该方法在多个基准问题上显著优于基线，尤其在具有欺骗性山谷的jump_k问题上实现100%成功率，同时样本效率和泛化能力优于深度强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有代码世界模型仅适用于确定性环境，而真实优化场景多为随机性组合优化问题。本文旨在验证LLM能否从次优数据中学习优化器动态行为，并利用该知识指导优化过程，特别关注解决传统自适应策略易陷入欺骗性陷阱的问题。

Method: 扩展代码世界模型至随机组合优化领域：给定(1+1)-RLS_k的次优轨迹，LLM合成其优化器动态的Python模拟器；在该模拟器上进行贪心规划，动态选择每步的突变强度k。通过在提示中加入经验转移统计信息来增强模型表现。

Result: 在lo和onemax问题上，CWM-greedy性能距理论最优策略不足6%；在欺骗性jump_k问题上成功率100%（基线为0%）；在NK-Landscape上显著优于所有基线（36.94 vs 36.32, p<0.001）。相比DQN，样本效率更高（200离线轨迹 vs 500在线回合）、成功率更高（100% vs 58%）、泛化能力更强（k=3时78% vs 0%）。5次独立运行验证了合成稳定性。

Conclusion: LLM能够有效学习优化器动态并控制决策过程，即使仅从次优数据训练也能逼近理论最优性能。该方法在处理欺骗性结构问题时展现卓越优势，且在样本效率、成功率和泛化能力上全面超越深度强化学习基线，证明了代码世界模型在优化领域的强大潜力。

Abstract: Can an LLM learn how an optimizer behaves -- and use that knowledge to control it? We extend Code World Models (CWMs), LLM-synthesized Python programs that predict environment dynamics, from deterministic games to stochastic combinatorial optimization. Given suboptimal trajectories of $(1{+}1)$-$\text{RLS}_k$, the LLM synthesizes a simulator of the optimizer's dynamics; greedy planning over this simulator then selects the mutation strength $k$ at each step. On \lo{} and \onemax{}, CWM-greedy performs within 6\% of the theoretically optimal policy -- without ever seeing optimal-policy trajectories. On \jump{$_k$}, where a deceptive valley causes all adaptive baselines to fail (0\% success rate), CWM-greedy achieves 100\% success rate -- without any collection policy using oracle knowledge of the gap parameter. On the NK-Landscape, where no closed-form model exists, CWM-greedy outperforms all baselines across fifteen independently generated instances ($36.94$ vs.\ $36.32$; $p<0.001$) when the prompt includes empirical transition statistics. The CWM also outperforms DQN in sample efficiency (200 offline trajectories vs.\ 500 online episodes), success rate (100\% vs.\ 58\%), and generalization ($k{=}3$: 78\% vs.\ 0\%). Robustness experiments confirm stable synthesis across 5 independent runs.

</details>


### [67] [Sustainable LLM Inference using Context-Aware Model Switching](https://arxiv.org/abs/2602.22261)
*Yuvarani,Akashdeep Singh,Zahra Fathanah,Salsabila Harlen,Syeikha Syafura Al-Zahra binti Zahari,Hema Subramaniam*

Main category: cs.LG

TL;DR: 针对LLM高能耗问题，提出上下文感知的动态模型切换方案。通过缓存、规则评分、ML分类和用户自适应组件，在真实负载测试中实现67.5%能耗降低与93.6%质量保持，简单查询响应时间提升68%，为可持续AI提供可行路径。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能耗激增引发可持续性挑战，现有部署采用"一刀切"推理策略，无视任务复杂度将所有请求路由至同一大模型，导致严重能源浪费。

Method: 提出上下文感知模型切换架构，实现基于查询复杂度的动态模型选择。系统整合：1）缓存机制处理重复查询；2）规则复杂度评分实现快速可解释决策；3）机器学习分类器捕获语义意图；4）用户自适应组件持续学习交互模式。在真实对话负载下，以Gemma3 1B/4B和Qwen3 4B为测试模型，通过NVML GPU功耗遥测采集能耗数据，并测量响应延迟、路由准确率及BERTScore F1质量指标。

Result: 实验结果显示，相比始终使用最大模型的基线，该方案最高降低67.5%能耗，同时维持93.6%的响应质量。简单查询的响应时间提升约68%，路由准确率与输出质量均满足实际应用需求。

Conclusion: 模型切换推理策略为实现能源高效、可持续的AI系统提供了实用且可扩展的解决方案，证实可在不显著牺牲质量的前提下获得显著效率收益。

Abstract: Large language models have become central to many AI applications, but their growing energy consumption raises serious sustainability concerns. A key limitation in current AI deployments is the reliance on a one-size-fits-all inference strategy where most systems route every request to the same large model, regardless of task complexity, leading to substantial and unnecessary energy waste. To address this issue, we propose a context-aware model switching approach that dynamically selects an appropriate language model based on query complexity. The proposed system uses a Context-Aware Model Switching for Energy-Efficient LLM Inference that combines caching for repeated queries, rulebased complexity scoring for fast and explainable decisions, machine learning classification to capture semantic intent, and a user-adaptive component that learns from interaction patterns over time. The proposed architecture was evaluated using real conversation workloads and three open-source language models (Gemma3 1B, Gemma3 4B and Qwen3 4B) with different computational costs, measuring energy consumption (via NVML GPU power telemetry), response latency, routing accuracy, and output quality (BERTScore F1) to reflect real-world usage conditions. Experimental results show that the model switching approach can reduce energy consumption by up to 67.5% compared to always using the largest model while maintaining a response quality of 93.6%. In addition, the response time for simple queries also improved significantly by approximately 68%. These results show that model switching inference offers a practical and scalable path toward more energy-efficient and sustainable AI systems, demonstrating that significant efficiency gains can be achieved without major sacrifices in response quality.

</details>


### [68] [Entropy-Controlled Flow Matching](https://arxiv.org/abs/2602.22265)
*Chika Maduabuchi*

Main category: cs.LG

TL;DR: 提出熵控制流匹配(ECFM)，通过全局熵率约束解决标准流匹配的低熵瓶颈问题，在Wasserstein空间中形成凸优化，等价于带熵乘子的薛定谔桥，并提供模式覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 标准流匹配缺乏信息几何控制，导致低熵瓶颈，可能瞬时耗尽语义模式，影响生成多样性。

Method: 提出熵控制流匹配(ECFM)，一种约束变分原理，强制执行d/dt H(mu_t) >= -lambda的熵率预算。在Wasserstein空间中构成凸优化，具有KKT/Pontryagin系统，可表示为带显式熵乘子的随机控制问题，等价于Schrödinger桥。

Result: 在纯传输下恢复熵最优传输测地线，Gamma收敛于经典最优传输(lambda→0)。提供证书式模式覆盖和密度下限保证，具有Lipschitz稳定性，并构造无约束流匹配的接近最优坍缩反例。

Conclusion: ECFM通过熵率约束有效缓解模式坍缩，提供理论保证和稳定性，为生成模型的信息几何控制建立新框架。

Abstract: Modern vision generators transport a base distribution to data through time-indexed measures, implemented as deterministic flows (ODEs) or stochastic diffusions (SDEs). Despite strong empirical performance, standard flow-matching objectives do not directly control the information geometry of the trajectory, allowing low-entropy bottlenecks that can transiently deplete semantic modes. We propose Entropy-Controlled Flow Matching (ECFM): a constrained variational principle over continuity-equation paths enforcing a global entropy-rate budget d/dt H(mu_t) >= -lambda. ECFM is a convex optimization in Wasserstein space with a KKT/Pontryagin system, and admits a stochastic-control representation equivalent to a Schrodinger bridge with an explicit entropy multiplier. In the pure transport regime, ECFM recovers entropic OT geodesics and Gamma-converges to classical OT as lambda -> 0. We further obtain certificate-style mode-coverage and density-floor guarantees with Lipschitz stability, and construct near-optimal collapse counterexamples for unconstrained flow matching.

</details>


### [69] [Data-Driven Supervision of a Thermal-Hydraulic Process Towards a Physics-Based Digital Twin](https://arxiv.org/abs/2602.22267)
*Osimone Imhogiemhe,Yoann Jus,Hubert Lejeune,Saïd Moussaoui*

Main category: cs.LG

TL;DR: 本文提出一种基于数字孪生的热工水力过程故障检测与诊断方法，结合数值模拟与机器学习技术，通过构建过程参数变化检测与在线估计模块，实现对系统单参数突变的准确定位与数值更新。


<details>
  <summary>Details</summary>
Motivation: 工业生产过程的实时监控是跨行业共同面临的挑战，涉及过程组件监测与预测性维护，以确保安全性、生产连续性和高效率。先进物理系统仿真工具与数据驱动机器学习模型的兴起，为设计高效系统监测数值工具提供了可能，而数字孪生概念为解决这些挑战提供了合适的框架。

Method: 本文旨在开发一个专注于热工水力过程监督中故障检测与诊断的数字孪生系统。该方法基于系统数值模拟，结合机器学习方法，提出了专门用于过程参数变化检测及其在线估计的不同模块。

Result: 所提故障检测与诊断算法在特定测试场景上进行了验证，该场景包含系统中的单次参数突变。数值结果表明，该方法在参数变化定位及其数值更新方面具有良好精度。

Conclusion: 本研究成功开发了一个数字孪生框架，将数值模拟与机器学习相结合，能够有效检测与诊断热工水力过程中的故障（参数变化）。验证结果证明了该方法在实时过程监督中的有效性，为工业过程监控提供了可行的解决方案。

Abstract: The real-time supervision of production processes is a common challenge across several industries. It targets process component monitoring and its predictive maintenance in order to ensure safety, uninterrupted production and maintain high efficiency level. The rise of advanced tools for the simulation of physical systems in addition to data-driven machine learning models offers the possibility to design numerical tools dedicated to efficient system monitoring. In that respect, the digital twin concept presents an adequate framework that proffers solution to these challenges. The main purpose of this paper is to develop such a digital twin dedicated to fault detection and diagnosis in the context of a thermal-hydraulic process supervision. Based on a numerical simulation of the system, in addition to machine learning methods, we propose different modules dedicated to process parameter change detection and their on-line estimation. The proposed fault detection and diagnosis algorithm is validated on a specific test scenario, with single one-off parameter change occurrences in the system. The numerical results show good accuracy in terms of parameter variation localization and the update of their values.

</details>


### [70] [AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2602.22268)
*Changhai Zhou,Shiyang Zhang,Yuhua Zhou,Qian Qiao,Jun Gao,Cheng Jin,Kaizhou Qin,Weizhong Zhang*

Main category: cs.LG

TL;DR: AutoQRA是一个联合优化框架，通过两阶段搜索策略（多保真度进化搜索+信任域贝叶斯优化）同时优化每层的量化位宽和LoRA秩，在相同显存预算下实现了接近全精度微调的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的量化后接参数高效微调（如LoRA）的顺序流程未能利用量化位宽与LoRA秩之间的复杂交互关系。低量化误差的优化配置并不总能转化为强大的微调性能，且在相同显存预算下不同配置差异显著，这限制了现有方法的效果。

Method: AutoQRA采用两阶段优化策略：第一阶段进行全局多保真度进化搜索，通过注入层重要性先验来初始化种群，并结合特定算子和性能模型高效筛选候选配置；第二阶段应用信任域贝叶斯优化对promising区域进行局部精调，在给定显存约束下找到最优配置，从而在训练过程中主动补偿特定层的量化噪声。

Result: 实验表明，AutoQRA在显存占用与统一4-bit方法相当的情况下，性能接近全精度微调。

Conclusion: 该框架通过联合优化量化位宽和LoRA秩，有效解决了显存受限下的微调性能瓶颈，为实现高效的低比特微调提供了一种可行方案。

Abstract: Quantization followed by parameter-efficient fine-tuning has emerged as a promising paradigm for downstream adaptation under tight GPU memory constraints. However, this sequential pipeline fails to leverage the intricate interaction between quantization bit-width and LoRA rank. Specifically, a carefully optimized quantization allocation with low quantization error does not always translate to strong fine-tuning performance, and different bit-width and rank configurations can lead to significantly varying outcomes under the same memory budget. To address this limitation, we propose AutoQRA, a joint optimization framework that simultaneously optimizes the bit-width and LoRA rank configuration for each layer during the mixed quantized fine-tuning process. To tackle the challenges posed by the large discrete search space and the high evaluation cost associated with frequent fine-tuning iterations, AutoQRA decomposes the optimization process into two stages. First, it first conducts a global multi-fidelity evolutionary search, where the initial population is warm-started by injecting layer-wise importance priors. This stage employs specific operators and a performance model to efficiently screen candidate configurations. Second, trust-region Bayesian optimization is applied to locally refine promising regions of the search space and identify optimal configurations under the given memory budget. This approach enables active compensation for quantization noise in specific layers during training. Experiments show that AutoQRA achieves performance close to full-precision fine-tuning with a memory footprint comparable to uniform 4-bit methods.

</details>


### [71] [CQSA: Byzantine-robust Clustered Quantum Secure Aggregation in Federated Learning](https://arxiv.org/abs/2602.22269)
*Arnab Nath,Harsh Kasyap*

Main category: cs.LG

TL;DR: 针对量子辅助联邦学习中现有量子安全聚合(QSA)协议依赖大规模GHZ态导致保真度低且无法检测拜占庭客户端的问题，本文提出一种集群量子安全聚合(CQSA)框架。该方法将客户端随机划分为小集群，在集群内使用高保真低量子比特GHZ态进行局部聚合，服务器通过余弦相似度和欧氏距离等统计指标分析集群聚合结果以识别恶意贡献。理论分析和去极化噪声模拟表明，CQSA在保证模型稳定收敛的同时，显著提升了量子态保真度。


<details>
  <summary>Details</summary>
Motivation: 现有量子安全聚合(QSA)在量子辅助联邦学习中面临两个根本性挑战：一是大规模GHZ态保真度随客户端数量增加而急剧下降，二是全局聚合机制无法检测拜占庭客户端。这些问题严重制约了量子技术在联邦学习中的实际应用。

Method: CQSA采用模块化设计：首先将客户端随机划分为若干小集群；每个集群使用高保真、低量子比特数的GHZ态进行本地量子安全聚合；服务器端则通过计算集群级聚合结果之间的余弦相似度和欧氏距离等统计关系，识别并过滤恶意贡献。

Result: 在去极化噪声条件下的理论分析和模拟实验表明，与全局QSA相比，CQSA能够确保模型稳定收敛，同时获得更高的量子态保真度。

Conclusion: 集群量子安全聚合(CQSA)框架有效解决了大规模量子态保真度衰减和拜占庭鲁棒性问题，为近期量子硬件在联邦学习中的实用化部署提供了可行方案。

Abstract: Federated Learning (FL) enables collaborative model training without sharing raw data. However, shared local model updates remain vulnerable to inference and poisoning attacks. Secure aggregation schemes have been proposed to mitigate these attacks. In this work, we aim to understand how these techniques are implemented in quantum-assisted FL. Quantum Secure Aggregation (QSA) has been proposed, offering information-theoretic privacy by encoding client updates into the global phase of multipartite entangled states. Existing QSA protocols, however, rely on a single global Greenberger-Horne-Zeilinger (GHZ) state shared among all participating clients. This design poses fundamental challenges: fidelity of large-scale GHZ states deteriorates rapidly with the increasing number of clients; and (ii) the global aggregation prevents the detection of Byzantine clients. We propose Clustered Quantum Secure Aggregation (CQSA), a modular aggregation framework that reconciles the physical constraints of near-term quantum hardware along with the need for Byzantine-robustness in FL. CQSA randomly partitions the clients into small clusters, each performing local quantum aggregation using high-fidelity, low-qubit GHZ states. The server analyzes statistical relationships between cluster-level aggregates employing common statistical measures such as cosine similarity and Euclidean distance to identify malicious contributions. Through theoretical analysis and simulations under depolarizing noise, we demonstrate that CQSA ensures stable model convergence, achieves superior state fidelity over global QSA.

</details>


### [72] [Prior Knowledge-enhanced Spatio-temporal Epidemic Forecasting](https://arxiv.org/abs/2602.22270)
*Sijie Ruan,Jinyu Li,Jia Wei,Zenghao Xu,Jie Bao,Junshi Xu,Junyang Qiu,Hanning Yuan,Xiaoxiao Wang,Shuliang Wang*

Main category: cs.LG

TL;DR: 本文提出STOEP疫情预测框架，融合隐式时空先验与显式专家先验，集成病例感知邻接学习(CAL)、空间信息参数估计(SPE)和基于滤波的机理预测(FMF)三大组件，在真实COVID-19和流感数据集上RMSE相对最优基线提升11.1%，并已在中国某省级疾控中心部署应用。


<details>
  <summary>Details</summary>
Motivation: 现有疫情时空预测方法存在三大局限：对弱疫情信号不敏感、空间关系过度简化、参数估计不稳定，难以满足精细化的公共卫生管理需求。亟需开发能够动态捕捉复杂空间依赖、增强微弱疫情信号并稳定参数估计的新型预测框架。

Method: 提出STOEP混合预测框架，通过以下三个核心组件实现：1) 病例感知邻接学习(CAL)，基于历史感染模式动态校正流动性驱动的区域间依赖关系；2) 空间信息参数估计(SPE)，引入可学习的空间先验机制以放大弱疫情信号；3) 基于滤波的机理预测(FMF)，采用专家知识引导的自适应阈值策略对疫情参数进行正则化约束。

Result: 在真实世界COVID-19与流感数据集上的系统实验验证，STOEP框架的RMSE指标较最优基线提升11.1%。该预测系统已成功部署于中国某省级疾病预防控制中心，服务于实际的疫情管理决策。

Conclusion: STOEP框架通过有效融合数据驱动的隐式时空先验与领域专家显式先验，显著提升了疫情预测的准确性和鲁棒性，为解决复杂疫情时空预测问题提供了可行方案，具有重要的公共卫生应用价值。

Abstract: Spatio-temporal epidemic forecasting is critical for public health management, yet existing methods often struggle with insensitivity to weak epidemic signals, over-simplified spatial relations, and unstable parameter estimation. To address these challenges, we propose the Spatio-Temporal priOr-aware Epidemic Predictor (STOEP), a novel hybrid framework that integrates implicit spatio-temporal priors and explicit expert priors. STOEP consists of three key components: (1) Case-aware Adjacency Learning (CAL), which dynamically adjusts mobility-based regional dependencies using historical infection patterns; (2) Space-informed Parameter Estimating (SPE), which employs learnable spatial priors to amplify weak epidemic signals; and (3) Filter-based Mechanistic Forecasting (FMF), which uses an expert-guided adaptive thresholding strategy to regularize epidemic parameters. Extensive experiments on real-world COVID-19 and influenza datasets demonstrate that STOEP outperforms the best baseline by 11.1% in RMSE. The system has been deployed at one provincial CDC in China to facilitate downstream applications.

</details>


### [73] [Support Tokens, Stability Margins, and a New Foundation for Robust LLMs](https://arxiv.org/abs/2602.22271)
*Deepak Agarwal,Dhyey Dharmendrakumar Mavani,Suyash Gupta,Karthik Sethuraman,Tejas Dharamsi*

Main category: cs.LG

TL;DR: 该研究将因果自注意力在概率框架下重新表述，类比概率PCA揭示变量变换产生的屏障约束，形成结构化token空间几何。由此提出SVM式的注意力边界与支持token概念，并将LLM解释为token幂集上的随机过程。基于贝叶斯MAP估计，仅需在标准交叉熵损失中加入平滑对数屏障惩罚，即可提升模型鲁棒性而不损失泛化性能，实现简单。


<details>
  <summary>Details</summary>
Motivation: 受经典PCA扩展至概率PCA的启发，旨在从概率视角重新审视现代基础模型的核心组件——因果自注意力，挖掘其深层结构特性，为理解大语言模型解码动态提供理论解释。

Method: 通过概率重 formulation 因果自注意力，利用变量变换分析揭示参数屏障约束；将大语言模型建模为token空间幂集的随机过程；构建贝叶斯框架并推导MAP目标函数，核心创新是在标准交叉熵损失上添加平滑对数屏障惩罚项。

Result: 理论层面发现token空间具有高度结构化几何，存在病态边界，产生类似SVM的间隔与支持token概念；实践层面验证该方法能在保持样本外准确率的同时提升模型鲁棒性，且易于集成。

Conclusion: 该概率视角不仅深化了对大语言模型工作机制的理解，还提供了一种简单有效的鲁棒性增强方法，兼具理论价值与应用潜力。

Abstract: Self-attention is usually described as a flexible, content-adaptive way to mix a token with information from its past. We re-interpret causal self-attention transformers, the backbone of modern foundation models, within a probabilistic framework, much like how classical PCA is extended to probabilistic PCA. However, this re-formulation reveals a surprising and deeper structural insight: due to a change-of-variables phenomenon, a barrier constraint emerges on the self-attention parameters. This induces a highly structured geometry on the token space, providing theoretical insights into the dynamics of LLM decoding. This reveals a boundary where attention becomes ill-conditioned, leading to a margin interpretation similar to classical support vector machines. Just like support vectors, this naturally gives rise to the concept of support tokens.
  Furthermore, we show that LLMs can be interpreted as a stochastic process over the power set of the token space, providing a rigorous probabilistic framework for sequence modeling. We propose a Bayesian framework and derive a MAP estimation objective that requires only a minimal modification to standard LLM training: the addition of a smooth log-barrier penalty to the usual cross-entropy loss. We demonstrate that this provides more robust models without sacrificing out-of-sample accuracy and that it is straightforward to incorporate in practice.

</details>


### [74] [Integrating Machine Learning Ensembles and Large Language Models for Heart Disease Prediction Using Voting Fusion](https://arxiv.org/abs/2602.22280)
*Md. Tahsin Amin,Tanim Ahmmod,Zannatul Ferdus,Talukder Naemul Hasan Naem,Ehsanul Ferdous,Arpita Bhattacharjee,Ishmam Ahmed Solaiman,Nahiyan Bin Noor*

Main category: cs.LG

TL;DR: 本研究使用1,190条患者记录，比较了传统机器学习集成模型与大型语言模型在心血管疾病预测中的表现。结果表明，ML集成模型（95.78%准确率）表现最佳，LLM单独使用时效果中等（78.9%），但混合融合方法（96.62%准确率）取得了最优结果，为临床决策支持提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要早期识别和精准风险分层。虽然传统机器学习算法擅长处理复杂的非线性患者数据，但大型语言模型的出现为医疗决策支持提供了新的零样本和少样本推理能力，值得探索其在医疗领域的应用潜力。

Method: 研究采用1,190条患者记录的合并数据集，通过OpenRouter API比较了传统机器学习模型（随机森林、XGBoost、LightGBM、CatBoost）与开源大语言模型的性能，并最终在Gemini 2.5 Flash框架下构建了ML集成与LLM推理的混合融合系统。

Result: 传统ML集成模型表现最佳（准确率95.78%，ROC-AUC 0.96），LLM在零样本设置下准确率为78.9%，少样本设置下为72.6%。提出的混合方法取得了最优效果（准确率96.62%，AUC 0.97），表明LLM与ML模型结合时效果优于单独使用。

Conclusion: 机器学习集成模型在结构化表格数据预测中表现最优，但与LLM的混合系统能进一步提升性能，为开发更可靠的临床决策支持工具开辟了新途径。

Abstract: Cardiovascular disease is the primary cause of death globally, necessitating early identification, precise risk classification, and dependable decision-support technologies. The advent of large language models (LLMs) provides new zero-shot and few-shot reasoning capabilities, even though machine learning (ML) algorithms, especially ensemble approaches like Random Forest, XGBoost, LightGBM, and CatBoost, are excellent at modeling complex, non-linear patient data and routinely beat logistic regression. This research predicts cardiovascular disease using a merged dataset of 1,190 patient records, comparing traditional machine learning models (95.78% accuracy, ROC-AUC 0.96) with open-source large language models via OpenRouter APIs. Finally, a hybrid fusion of the ML ensemble and LLM reasoning under Gemini 2.5 Flash achieved the best results (96.62% accuracy, 0.97 AUC), showing that LLMs (78.9 % accuracy) work best when combined with ML models rather than used alone. Results show that ML ensembles achieved the highest performance (95.78% accuracy, ROC-AUC 0.96), while LLMs performed moderately in zero-shot (78.9%) and slightly better in few-shot (72.6%) settings. The proposed hybrid method enhanced the strength in uncertain situations, illustrating that ensemble ML is considered the best structured tabular prediction case, but it can be integrated with hybrid ML-LLM systems to provide a minor increase and open the way to more reliable clinical decision-support tools.

</details>


### [75] [BrepCoder: A Unified Multimodal Large Language Model for Multi-task B-rep Reasoning](https://arxiv.org/abs/2602.22284)
*Mingi Kim,Yongjun Kim,Jungwoo Kang,Hyungki Kim*

Main category: cs.LG

TL;DR: 本文提出BrepCoder，一个基于边界表示（B-rep）的统一多模态大语言模型，通过将CAD建模序列转换为类Python代码并采用两阶段训练策略，实现跨多个CAD任务（补全、纠错、问答）的通用性，解决了现有方法依赖特定任务和点云/图像而非工业标准B-rep的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CAD深度学习方法主要存在两大局限：一是任务专用模型需针对新任务进行结构调整，缺乏通用性；二是大多处理点云或图像数据，而非工业标准的边界表示（B-rep）格式。这限制了模型的泛化能力和实际应用价值。

Method: 作者提出BrepCoder方法：1）利用大语言模型的代码生成能力，将CAD建模序列转化为类Python代码并与B-rep对齐；2）采用两阶段训练策略：先通过逆向工程预训练学习几何特征和设计逻辑，再扩展到补全、纠错、CAD问答等下游任务。

Result: 通过将B-rep解释为结构化代码，BrepCoder在多个CAD任务上实现了卓越的任务泛化能力。

Conclusion: BrepCoder展示了作为通用CAD智能体的潜力，为基于B-rep的CAD任务处理提供了统一框架。

Abstract: Recent advancements in deep learning have actively addressed complex challenges within the Computer-Aided Design (CAD) domain.However, most existing approaches rely on task-specifi c models requiring structural modifi cations for new tasks, and they predominantly focus on point clouds or images rather than the industry-standard Boundary Representation (B-rep) format. To address these limitations, we propose BrepCoder, a unifi ed Multimodal Large Language Model (MLLM) that performs diverse CAD tasks from B-rep inputs. By leveraging the code generation capabilities of Large Language Models (LLMs), we convert CAD modeling sequences into Python-like code and align them with B-rep. We then adopt a two-stage training strategy: First, pre-training on reverse engineering to learn geometric features and design logic. Second, eff ectively extending the model to various downstream tasks such as completion, error correction, and CAD-QA. Consequently, by interpreting B-rep as structural code, BrepCoder achieves superior generalization across diverse tasks, demonstrating its potential as a general-purpose CAD agent.

</details>


### [76] [Early Risk Stratification of Dosing Errors in Clinical Trials Using Machine Learning](https://arxiv.org/abs/2602.22285)
*Félicien Hêche,Sohrab Ferdowsi,Anthony Yazdani,Sara Sansaloni-Pastor,Douglas Teodoro*

Main category: cs.LG

TL;DR: 本研究开发了一个机器学习框架，利用临床试验启动前的信息预测给药错误高风险试验，通过结构化数据和文本数据的融合模型实现早期风险分层。


<details>
  <summary>Details</summary>
Motivation: 临床试验中的给药错误影响研究质量和受试者安全，传统方法难以在试验启动前识别高风险试验。需要可预测、可解释的早期风险评估工具来支持主动质量管理。

Method: 从ClinicalTrials.gov构建42,112项试验数据集，提取结构化、半结构化和非结构化文本数据。基于不良事件报告和MedDRA术语分配二元标签。比较XGBoost（结构化特征）、ClinicalModernBERT（文本）和简单后期融合模型，并应用概率校准实现风险分层。

Result: 后期融合模型AUC-ROC达0.862，性能最佳。校准后的概率输出可实现稳健的风险分层，预测风险越高的组别中实际高给药错误率试验比例单调递增。

Conclusion: 研究提出了可重复、可扩展的机器学习框架，用于在试验启动前预测给药错误风险，支持临床研究中的主动风险质量管理。

Abstract: Objective: The objective of this study is to develop a machine learning (ML)-based framework for early risk stratification of clinical trials (CTs) according to their likelihood of exhibiting a high rate of dosing errors, using information available prior to trial initiation. Materials and Methods: We constructed a dataset from ClinicalTrials.gov comprising 42,112 CTs. Structured, semi-structured trial data, and unstructured protocol-related free-text data were extracted. CTs were assigned binary labels indicating elevated dosing error rate, derived from adverse event reports, MedDRA terminology, and Wilson confidence intervals. We evaluated an XGBoost model trained on structured features, a ClinicalModernBERT model using textual data, and a simple late-fusion model combining both modalities. Post-hoc probability calibration was applied to enable interpretable, trial-level risk stratification. Results: The late-fusion model achieved the highest AUC-ROC (0.862). Beyond discrimination, calibrated outputs enabled robust stratification of CTs into predefined risk categories. The proportion of trials labeled as having an excessively high dosing error rate increased monotonically across higher predicted risk groups and aligned with the corresponding predicted probability ranges. Discussion: These findings indicate that dosing error risk can be anticipated at the trial level using pre-initiation information. Probability calibration was essential for translating model outputs into reliable and interpretable risk categories, while simple multimodal integration yielded performance gains without requiring complex architectures. Conclusion: This study introduces a reproducible and scalable ML framework for early, trial-level risk stratification of CTs at risk of high dosing error rates, supporting proactive, risk-based quality management in clinical research.

</details>


### [77] [Reliable XAI Explanations in Sudden Cardiac Death Prediction for Chagas Cardiomyopathy](https://arxiv.org/abs/2602.22288)
*Vinícius P. Chagas,Luiz H. T. Viana,Mac M. da S. Carlos,João P. V. Madeiro,Roberto C. Pedrosa,Thiago Alves Rocha,Carlos H. L. Cavalcante*

Main category: cs.LG

TL;DR: 针对恰加斯心肌病中无法预测的猝死风险，本文采用具有正确性保证的逻辑可解释性方法替代黑盒模型，显著提升了临床信任度和模型部署可行性。


<details>
  <summary>Details</summary>
Motivation: 猝死具有不可预测性，在恰加斯心肌病患者中尤其具有挑战性。现有AI模型虽能改善风险分层，但因黑盒特性缺乏透明度，启发式解释方法又缺乏正确性保证，导致决策错误，阻碍了临床采纳。

Method: 将具有正确性保证的逻辑可解释性方法应用于SCD预测AI分类器（准确率和召回率均超过95%），并与最先进的启发式方法进行比较。

Result: 该方法表现出强大的预测性能，达到100%解释保真度，在一致性和鲁棒性方面优于启发式方法，增强了临床信任。

Conclusion: 该可解释性方法促进了AI驱动工具在临床实践中的整合，特别有利于在恰加斯病流行地区的大规模部署，为高风险患者提供了更可靠的预测工具。

Abstract: Sudden cardiac death (SCD) is unpredictable, and its prediction in Chagas cardiomyopathy (CC) remains a significant challenge, especially in patients not classified as high risk. While AI and machine learning models improve risk stratification, their adoption is hindered by a lack of transparency, as they are often perceived as \textit{black boxes} with unclear decision-making processes. Some approaches apply heuristic explanations without correctness guarantees, leading to mistakes in the decision-making process. To address this, we apply a logic-based explainability method with correctness guarantees to the problem of SCD prediction in CC. This explainability method, applied to an AI classifier with over 95\% accuracy and recall, demonstrated strong predictive performance and 100\% explanation fidelity. When compared to state-of-the-art heuristic methods, it showed superior consistency and robustness. This approach enhances clinical trust, facilitates the integration of AI-driven tools into practice, and promotes large-scale deployment, particularly in endemic regions where it is most needed.

</details>


### [78] [Manifold of Failure: Behavioral Attraction Basins in Language Models](https://arxiv.org/abs/2602.22291)
*Sarthak Munshi,Manish Bhatt,Vineeth Sai Narajala,Idan Habler,AmmarnAl-Kahfah,Ken Huang,Blake Gatto*

Main category: cs.LG

TL;DR: 本文提出了一个系统映射大语言模型"失败流形"的框架，将漏洞搜索重新定义为质量多样性问题，使用MAP-Elites算法揭示模型不安全区域的连续拓扑结构。在三个LLM上的实验显示该方法能达到63%的行为覆盖率，发现370个不同的漏洞生态位，并揭示了不同模型的拓扑特征差异，为AI安全理解提供了新范式。


<details>
  <summary>Details</summary>
Motivation: 现有研究侧重于将对抗样本投影回自然数据流形以恢复安全性，但缺乏对不安全区域本身的系统性理解。作者认为，全面的AI安全理解需要对模型失败区域进行表征和映射，这有助于从发现离散失败转向理解其底层结构，为模型安全性评估提供更全局的视角。

Method: 提出将漏洞搜索重新定义为质量多样性问题，采用MAP-Elites算法来探索模型的"行为吸引盆地"。使用"对齐偏差"作为质量指标，指导搜索向模型行为偏离预期对齐最远的区域。该方法通过构建多维特征空间，系统性地探索和映射模型在不同输入条件下的行为模式，生成可解释的全局安全景观地图。

Result: 在Llama-3-8B、GPT-OSS-20B和GPT-5-Mini三个模型上，MAP-Elites实现了高达63%的行为覆盖率和370个不同的漏洞生态位。揭示了三种不同的拓扑特征：Llama-3-8B呈现近乎通用的漏洞平台（平均对齐偏差0.93），GPT-OSS-20B显示碎片化景观（平均0.73），GPT-5-Mini表现出强鲁棒性（上限0.50）。该方法生成的全局地图超越了GCG、PAIR和TAP等现有攻击方法的能力。

Conclusion: 该研究提供了现有攻击方法无法实现的可解释全局安全地图，标志着从寻找离散失败到理解其底层结构的范式转变。通过系统性地映射失败流形，为全面理解大语言模型安全性提供了新框架，有助于识别模型特定的脆弱性模式并指导更有效的安全加固策略。

Abstract: While prior work has focused on projecting adversarial examples back onto the manifold of natural data to restore safety, we argue that a comprehensive understanding of AI safety requires characterizing the unsafe regions themselves. This paper introduces a framework for systematically mapping the Manifold of Failure in Large Language Models (LLMs). We reframe the search for vulnerabilities as a quality diversity problem, using MAP-Elites to illuminate the continuous topology of these failure regions, which we term behavioral attraction basins. Our quality metric, Alignment Deviation, guides the search towards areas where the model's behavior diverges most from its intended alignment. Across three LLMs: Llama-3-8B, GPT-OSS-20B, and GPT-5-Mini, we show that MAP-Elites achieves up to 63% behavioral coverage, discovers up to 370 distinct vulnerability niches, and reveals dramatically different model-specific topological signatures: Llama-3-8B exhibits a near-universal vulnerability plateau (mean Alignment Deviation 0.93), GPT-OSS-20B shows a fragmented landscape with spatially concentrated basins (mean 0.73), and GPT-5-Mini demonstrates strong robustness with a ceiling at 0.50. Our approach produces interpretable, global maps of each model's safety landscape that no existing attack method (GCG, PAIR, or TAP) can provide, shifting the paradigm from finding discrete failures to understanding their underlying structure.

</details>


### [79] [When Should a Model Change Its Mind? An Energy-Based Theory and Regularizer for Concept Drift in Electrocardiogram (ECG) Signals](https://arxiv.org/abs/2602.22294)
*Timothy Oladunni,Blessing Ojeme,Kyndal Maclin,Clyde Baidoo*

Main category: cs.LG

TL;DR: 本研究针对动态生理信号建模中区分良性变异性与真实概念漂移的挑战，提出生理能量守恒理论(PECT)。该理论假设归一化潜在位移应与归一化信号能量变化成正比，其持续违反指示真实概念漂移。通过能量约束表示学习(ECRL)正则化实现，在不改变编码器架构或增加推理成本下惩罚能量不一致的潜在移动。在多模态ECG七种模型上验证，最强三模态混合模型扰动准确率从72.6%提升至85.5%，融合表示漂移减少超45%，证实PECT可作为连续生理信号概念稳定性的能量漂移法则。


<details>
  <summary>Details</summary>
Motivation: 现有概念漂移框架多为分布导向，缺乏对生理信号发生合理能量波动时模型内部表示应如何变化的指导原则，导致深度模型常将无害的幅值、速率或形态变化误判为概念漂移，产生不稳定预测，尤其影响多模态融合性能。

Method: 提出生理能量守恒理论(PECT)，核心假设为：在虚拟漂移(良性变化)下，归一化潜在位移应与归一化信号能量变化成正比，持续违反此比例则表明真实概念漂移。操作化为能量约束表示学习(ECRL)，一种轻量级正则化器，在不修改编码器架构或增加推理时间成本的前提下，惩罚能量不一致的潜在表示移动。

Result: 在七种单模态和混合ECG模型上评估，最强三模态混合模型(1D+2D+Transformer)表现：干净准确率轻微下降(96.0%至94.1%)，但扰动准确率大幅提升(72.6%至85.5%)，融合表示漂移减少超过45%。所有架构均呈现相似趋势，提供PECT作为能量漂移法则的经验证据。

Conclusion: PECT可作为能量漂移法则有效指导连续生理信号中的概念稳定性，ECRL正则化显著提升模型在生理扰动下的鲁棒性，同时保持架构简洁性和推理效率，为动态生理信号建模提供新范式。

Abstract: Models operating on dynamic physiologic signals must distinguish benign, label-preserving variability from true concept change. Existing concept-drift frameworks are largely distributional and provide no principled guidance on how much a model's internal representation may move when the underlying signal undergoes physiologically plausible fluctuations in energy. As a result, deep models often misinterpret harmless changes in amplitude, rate, or morphology as concept drift, yielding unstable predictions, particularly in multimodal fusion settings.
  This study introduces Physiologic Energy Conservation Theory (PECT), an energy-based framework for concept stability in dynamic signals. PECT posits that under virtual drift, normalized latent displacement should scale proportionally with normalized signal energy change, while persistent violations of this proportionality indicate real concept drift. We operationalize this principle through Energy-Constrained Representation Learning (ECRL), a lightweight regularizer that penalizes energy-inconsistent latent movement without modifying encoder architectures or adding inference-time cost.
  Although PECT is formulated for dynamic signals in general, we instantiate and evaluate it on multimodal ECG across seven unimodal and hybrid models. Experiments show that in the strongest trimodal hybrid (1D+2D+Transformer), clean accuracy is largely preserved (96.0% to 94.1%), while perturbed accuracy improves substantially (72.6% to 85.5%) and fused representation drift decreases by over 45%. Similar trends are observed across all architectures, providing empirical evidence that PECT functions as an energy-drift law governing concept stability in continuous physiologic signals.

</details>


### [80] [Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection](https://arxiv.org/abs/2602.22297)
*Dhiraj Neupane,Richard Dazeley,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 该论文提出了一种基于离线逆向强化学习的机械设备故障检测方法，通过对抗式逆向强化学习训练判别器来区分正常与故障状态，利用学习到的奖励信号作为异常分数，在三个基准数据集上实现了早期鲁棒故障检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的故障检测方法未能充分利用RL的序列决策优势，常将故障检测简化为上下文老虎机问题。为此，需要一种能更好利用RL时序推理能力、避免人工奖励设计和故障标签依赖的新方法。

Method: 将故障检测建模为离线逆向强化学习问题：从健康运行序列中学习奖励动态，采用对抗式逆向强化学习训练判别器区分专家（正常）与策略生成状态转移，判别器输出的奖励值作为异常分数。

Result: 在HUMS2023、IMS和XJTU-SY三个运行至故障基准数据集上，模型能有效区分正常与故障样本（正常样本异常分数低，故障样本分数高），实现了早期且鲁棒的故障检测。

Conclusion: 该方法成功将强化学习的序列推理能力与故障检测的时序结构对齐，为数据驱动工业场景下的强化学习诊断方法开辟了新路径，避免了人工奖励工程和故障标签需求。

Abstract: Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL's sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formulate MFD as an offline inverse reinforcement learning problem, where the agent learns the reward dynamics directly from healthy operational sequences, thereby bypassing the need for manual reward engineering and fault labels. Our framework employs Adversarial Inverse Reinforcement Learning to train a discriminator that distinguishes between normal (expert) and policy-generated transitions. The discriminator's learned reward serves as an anomaly score, indicating deviations from normal operating behaviour. When evaluated on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY), the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection. By aligning RL's sequential reasoning with MFD's temporal structure, this work opens a path toward RL-based diagnostics in data-driven industrial settings.

</details>


### [81] [A 1/R Law for Kurtosis Contrast in Balanced Mixtures](https://arxiv.org/abs/2602.22334)
*Yuda Bi,Wenjun Xiao,Linhao Bai,Vince D Calhoun*

Main category: cs.LG

TL;DR: 这篇论文研究了基于峰度的独立成分分析（ICA）在宽而平衡的混合场景中性能下降的问题，证明了关于峰度衰减的尖锐冗余定律，揭示了在有限样本下估计精度的根本限制，并提出了一种名为"净化"的简单启发式方法来选择符号一致的源信号，从而恢复对比度。合成实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 基于峰度的独立成分分析（ICA）在处理宽且平衡的混合信号时性能会显著下降。为了理解这一现象的本质并找到解决方案，论文深入研究了峰度在这种场景下的统计行为，特别是其与混合宽度（effective width）的关系，以及如何克服样本量限制。

Method: 论文首先从理论上证明了尖锐冗余定律：对于具有有效宽度R_eff的标准化投影，其总体超额峰度满足|κ(y)|=O(κ_max/R_eff)。在平衡条件下，这一衰减可量化为O(c_bκ_max/R)。其次，在标准有限矩条件下分析了样本峰度估计的精度限制，证明超越O(1/√T)的估计精度需要R≂κ_max√T。最后，提出"净化"方法，通过选择m≪R个符号一致的源信号，将对比度恢复至Ω(1/m)，并给出了数据驱动的启发式实现方案。

Result: 理论结果表明：(1) 峰度随有效宽度呈倒数衰减；(2) 样本复杂度存在√T的临界点，当R超过κ_max√T时无法获得超标准O(1/√T)的估计精度；(3) "净化"策略能实现与R无关的对比度Ω(1/m)。合成实验成功验证了峰度衰减规律、√T临界现象以及对比度恢复效果。

Conclusion: 该研究完整刻画了宽平衡混合下基于峰度ICA的性能边界，揭示了根本性的统计限制，并通过简单的"净化"技术有效克服了这些限制。理论预测与实验结果高度一致，为实际应用提供了重要指导。

Abstract: Kurtosis-based Independent Component Analysis (ICA) weakens in wide, balanced mixtures. We prove a sharp redundancy law: for a standardized projection with effective width $R_{\mathrm{eff}}$ (participation ratio), the population excess kurtosis obeys $|κ(y)|=O(κ_{\max}/R_{\mathrm{eff}})$, yielding the order-tight $O(c_bκ_{\max}/R)$ under balance (typically $c_b=O(\log R)$). As an impossibility screen, under standard finite-moment conditions for sample kurtosis estimation, surpassing the $O(1/\sqrt{T})$ estimation scale requires $R\lesssim κ_{\max}\sqrt{T}$. We also show that \emph{purification} -- selecting $m\!\ll\!R$ sign-consistent sources -- restores $R$-independent contrast $Ω(1/m)$, with a simple data-driven heuristic. Synthetic experiments validate the predicted decay, the $\sqrt{T}$ crossover, and contrast recovery.

</details>


### [82] [Disentangling Shared and Target-Enriched Topics via Background-Contrastive Non-negative Matrix Factorization](https://arxiv.org/abs/2602.22387)
*Yixuan Li,Archer Y. Yang,Yue Li*

Main category: cs.LG

TL;DR: 针对高维生物数据中被背景变异掩盖的目标信号，本文提出背景对比非负矩阵分解方法（\model）。该方法通过联合分解目标与背景数据、对比学习目标函数和乘法更新算法，有效分离目标特异性变异，生成可解释特征组分，并在多种生物数据集中成功揭示传统方法无法检测的关键信号。


<details>
  <summary>Details</summary>
Motivation: 标准降维方法难以解析高维生物数据中的条件特异性结构，因为目标信号常被跨条件共享的显性变异（源于基线生物结构或技术效应）所掩盖，且这些混杂因素未知并与信号混合。现有背景校正方法面临可扩展性差或可解释性不足的挑战。

Method: \模型在共享非负基底下联合分解目标数据集与匹配背景数据，通过对比学习目标抑制背景表达结构，提取目标富集的潜在主题。采用矩阵乘法驱动的乘法更新算法，支持GPU加速和基于小批量训练的深度学习式大规模数据处理，生成的非负组分可直接在特征水平进行生物学解释。

Result: 跨模拟实验与多样本生物数据集的验证表明，\模型能够揭示被传统方法掩盖的重要信号，包括抑郁患者死后脑单细胞转录组中的疾病相关程序、小鼠蛋白表达中的基因型关联模式、白血病治疗特异性转录变化，以及癌细胞系中TP53依赖的药物反应。

Conclusion: \模型为高维生物数据分析提供了高效、可扩展且可解释的新框架，通过显式分离目标特异性变异，克服了现有方法的局限，在复杂生物系统的机制研究中具有广泛应用前景。

Abstract: Biological signals of interest in high-dimensional data are often masked by dominant variation shared across conditions. This variation, arising from baseline biological structure or technical effects, can prevent standard dimensionality reduction methods from resolving condition-specific structure. The challenge is that these confounding topics are often unknown and mixed with biological signals. Existing background correction methods are either unscalable to high dimensions or not interpretable. We introduce background contrastive Non-negative Matrix Factorization (\model), which extracts target-enriched latent topics by jointly factorizing a target dataset and a matched background using shared non-negative bases under a contrastive objective that suppresses background-expressed structure. This approach yields non-negative components that are directly interpretable at the feature level, and explicitly isolates target-specific variation. \model is learned by an efficient multiplicative update algorithm via matrix multiplication such that it is highly efficient on GPU hardware and scalable to big data via minibatch training akin to deep learning approach. Across simulations and diverse biological datasets, \model reveals signals obscured by conventional methods, including disease-associated programs in postmortem depressive brain single-cell RNA-seq, genotype-linked protein expression patterns in mice, treatment-specific transcriptional changes in leukemia, and TP53-dependent drug responses in cancer cell lines.

</details>


### [83] [Predicting Multi-Drug Resistance in Bacterial Isolates Through Performance Comparison and LIME-based Interpretation of Classification Models](https://arxiv.org/abs/2602.22400)
*Santanam Wishal,Riad Sahara*

Main category: cs.LG

TL;DR: 针对多重耐药性(MDR)临床诊断的挑战，本研究开发了基于临床特征和药敏模式的可解释机器学习预测框架。在9,714个细菌分离株数据集上，XGBoost和LightGBM表现最优，LIME分析揭示喹诺酮类、复方新诺明、粘菌素、氨基糖苷类和呋喃妥因的耐药性是MDR预测的关键驱动因素，为抗菌药物精准管理提供了可靠的决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 多重耐药性(MDR)的日益严峻，由于有效治疗手段匮乏且传统药敏检测周期长，已成为威胁临床决策的核心难题，亟需快速准确的预测方法来指导抗菌药物合理使用。

Method: 研究构建了一个可解释机器学习框架，评估逻辑回归、随机森林、AdaBoost、XGBoost和LightGBM五种分类器。采用9,714例细菌分离株的临床数据集，在抗生素家族层面编码耐药性特征以反映交叉耐药模式。通过准确率、F1-score、AUC-ROC和Matthews相关系数综合评价模型性能，并运用LIME技术生成个体化预测解释。

Result: 集成学习模型XGBoost与LightGBM在所有评估指标上均展现出卓越的预测能力。LIME解释结果表明，对喹诺酮类、复方新诺明、粘菌素、氨基糖苷类及呋喃类药物的耐药性对MDR预测贡献最大，这些发现与已知的微生物耐药机制高度吻合。

Conclusion: 本框架通过融合高性能集成模型与局部可解释技术，在确保预测精度的同时提供临床可理解的决策依据，有助于早期识别MDR菌株并指导抗菌药物合理使用，为建立可信的机器学习临床决策支持系统提供了可行路径。

Abstract: The rise of Antimicrobial Resistance, particularly Multi-Drug Resistance (MDR), presents a critical challenge for clinical decision-making due to limited treatment options and delays in conventional susceptibility testing. This study proposes an interpretable machine learning framework to predict MDR in bacterial isolates using clinical features and antibiotic susceptibility patterns. Five classification models were evaluated, including Logistic Regression, Random Forest, AdaBoost, XGBoost, and LightGBM. The models were trained on a curated dataset of 9,714 isolates, with resistance encoded at the antibiotic family level to capture cross-class resistance patterns consistent with MDR definitions. Performance assessment included accuracy, F1-score, AUC-ROC, and Matthews Correlation Coefficient. Ensemble models, particularly XGBoost and LightGBM, demonstrated superior predictive capability across all metrics. To address the clinical transparency gap, Local Interpretable Model-agnostic Explanations (LIME) was applied to generate instance-level explanations. LIME identified resistance to quinolones, Co-trimoxazole, Colistin, aminoglycosides, and Furanes as the strongest contributors to MDR predictions, aligning with known biological mechanisms. The results show that combining high-performing models with local interpretability provides both accuracy and actionable insights for antimicrobial stewardship. This framework supports earlier MDR identification and enhances trust in machine learning-assisted clinical decision support.

</details>


### [84] [MolFM-Lite: Multi-Modal Molecular Property Prediction with Conformer Ensemble Attention and Cross-Modal Fusion](https://arxiv.org/abs/2602.22405)
*Syed Omer Shah,Mohammed Maqsood Ahmed,Danish Mohiuddin Mohammed,Shahnawaz Alam,Mohd Vahaj ur Rahman*

Main category: cs.LG

TL;DR: 本文提出MolFM-Lite，一个通过交叉注意力融合多模态（1D SELFIES序列、2D分子图和3D构象集合）的分子性质预测模型，结合Boltzmann加权先验和FiLM实验条件调制，在四个MoleculeNet基准测试上实现显著性能提升并验证各组件有效性。


<details>
  <summary>Details</summary>
Motivation: 现有分子性质预测模型大多仅依赖单一分子表示（序列、图或3D结构），并将分子几何结构视为静态。这种方法无法充分捕捉分子的多尺度特征和构象柔性，限制了预测性能。本研究旨在开发一个能协同编码多维度分子信息并动态建模几何分布的模型。

Method: MolFM-Lite采用三模态联合编码架构：1) 通过SELFIES序列（1D）、分子图（2D）和构象集合（3D）分别编码不同尺度分子信息；2) 创新性地提出构象集合注意力机制，将可学习注意力与基于Boltzmann分布的权重先验结合，使用RDKit生成的多个构象来捕获分子形状的热力学分布；3) 设计交叉模态融合层实现模态间注意力交互，促进互补信息共享；4) 利用特征线性调制（FiLM）根据实验条件调节预测；5) 在ZINC250K数据集上进行跨模态对比学习和掩码原子预训练，实现低成本有效初始化。

Result: 在四个MoleculeNet支架分割基准测试上的全面评估表明：三模态融合相比单模态基线AUC提升7-11%；使用多构象集合相比单构象变体额外提升约2% AUC。消融实验证实每个架构组件都有独立贡献。所有基线模型在统一协议下重新评估，确保公平比较。预训练仅消耗适中计算成本即实现有效权重初始化。

Conclusion: MolFM-Lite成功实现了多模态分子信息的协同建模，证明了整合1D/2D/3D表示与动态构象分布的有效性。该方法显著提升了分子性质预测性能，且预训练策略具有计算效率。作者公开了代码、训练模型和数据分割，支持研究可重复性。

Abstract: Most machine learning models for molecular property prediction rely on a single molecular representation (either a sequence, a graph, or a 3D structure) and treat molecular geometry as static. We present MolFM-Lite, a multi-modal model that jointly encodes SELFIES sequences (1D), molecular graphs (2D), and conformer ensembles (3D) through cross-attention fusion, while conditioning predictions on experimental context via Feature-wise Linear Modulation (FiLM). Our main methodological contributions are: (1) a conformer ensemble attention mechanism that combines learnable attention with Boltzmann-weighted priors over multiple RDKit-generated conformers, capturing the thermodynamic distribution of molecular shapes; and (2) a cross-modal fusion layer where each modality can attend to others, enabling complementary information sharing. We evaluate on four MoleculeNet scaffold-split benchmarks using our model's own splits, and report all baselines re-evaluated under the same protocol. Comprehensive ablation studies across all four datasets confirm that each architectural component contributes independently, with tri-modal fusion providing 7-11% AUC improvement over single-modality baselines and conformer ensembles adding approximately 2% over single-conformer variants. Pre-training on ZINC250K (~250K molecules) using cross-modal contrastive and masked-atom objectives enables effective weight initialization at modest compute cost. We release all code, trained models, and data splits to support reproducibility.

</details>


### [85] [A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection](https://arxiv.org/abs/2602.22412)
*Ruiqi Zhou,Donghao Zhu,Houcai Shen*

Main category: cs.LG

TL;DR: 该论文针对肾脏交换、货运交换等匹配市场，提出一种基于学习的混合框架，通过持续收集用户离开数据并回归估计其分布，基于决策阈值自适应地融合即时与延迟匹配，在仅有限牺牲匹配效率的前提下，显著降低等待时间和市场拥堵。


<details>
  <summary>Details</summary>
Motivation: 延迟匹配虽可提升市场整体效率，但其收益对参与者停留时间与离开行为高度敏感，且会引发等待时间延长、市场拥堵等成本。固定匹配政策在动态环境中缺乏灵活性，难以权衡这些相互冲突的影响。

Method: 提出基于学习的混合框架，持续收集用户离开数据，通过回归方法估计潜在离开分布，并依据控制系统对匹配效率损失容忍度的决策阈值，动态决策下一时段是否延迟匹配。

Result: 所提框架在仅牺牲有限匹配效率的同时，能够大幅减少等待时间和市场拥堵。

Conclusion: 该框架通过动态调整策略，使系统性能可灵活地在纯贪心与纯耐心策略间插值，为静态匹配机制提供了稳健自适应的替代方案。

Abstract: In matching markets such as kidney exchanges and freight exchanges, delayed matching has been shown to improve overall market efficiency. The benefits of delay are highly sensitive to participants' sojourn times and departure behavior, and delaying matches can impose significant costs, including longer waiting times and increased market congestion. These competing effects make fixed matching policies inherently inflexible in dynamic environments. We propose a learning-based Hybrid framework that adaptively combines immediate and delayed matching. The framework continuously collects data on user departures over time, estimates the underlying departure distribution via regression, and determines whether to delay matching in the subsequent period based on a decision threshold that governs the system's tolerance for matching efficiency loss. The proposed framework can substantially reduce waiting times and congestion while sacrificing only a limited amount of matching efficiency. By dynamically adjusting its matching strategy, the Hybrid framework enables system performance to flexibly interpolate between purely greedy and purely patient policies, offering a robust and adaptive alternative to static matching mechanisms.

</details>


### [86] [Calibrated Test-Time Guidance for Bayesian Inference](https://arxiv.org/abs/2602.22428)
*Daniel Geyfman,Felix Draxler,Jan Groeneveld,Hyunsoo Lee,Theofanis Karaletsos,Stephan Mandt*

Main category: cs.LG

TL;DR: 本研究揭示常见测试时引导方法无法正确恢复贝叶斯后验分布，提出新型一致性估计器以实现校准采样，在贝叶斯推理任务中显著超越前人方法，并在黑洞图像重建中达到顶尖水平。


<details>
  <summary>Details</summary>
Motivation: 现有测试时引导方法为优化奖励函数而忽略从真实贝叶斯后验分布采样，导致推断结果存在系统性校准错误。

Method: 通过识别导致失败的结构性近似偏差，作者设计了一致性替代估计器，理论上保证从贝叶斯后验分布的正确采样。

Result: 所提方法在多项贝叶斯推理基准测试中性能显著优于现有方法，并在极具挑战性的黑洞图像重建任务中达到国际领先水平。

Conclusion: 该工作从理论上修正了测试时引导方法的根本缺陷，为扩散模型的贝叶斯推断提供了原则性解决方案，兼具理论严谨性和实践有效性。

Abstract: Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we show that common test-time guidance methods do not recover the correct posterior distribution and identify the structural approximations responsible for this failure. We then propose consistent alternative estimators that enable calibrated sampling from the Bayesian posterior. We significantly outperform previous methods on a set of Bayesian inference tasks, and match state-of-the-art in black hole image reconstruction.

</details>


### [87] [From Bias to Balance: Fairness-Aware Paper Recommendation for Equitable Peer Review](https://arxiv.org/abs/2602.22438)
*Uttamasha Anjally Oyshi,Susan Gauch*

Main category: cs.LG

TL;DR: 针对同行评审中持续存在的系统性偏见，本研究提出Fair-PaperRec——一种在双盲评审后通过可微公平性损失函数重排论文的多层感知机模型。该模型在合成数据上验证了增强多样性同时保持效用稳定的能力，并在SIGCHI等真实会议数据上实现弱势群体参与度提升42.03%而整体效用变化不超过3.16%。


<details>
  <summary>Details</summary>
Motivation: 尽管双盲评审广泛实施，基于作者人口统计特征的系统性偏见仍使弱势群体处于不利地位。本文核心假设是：在评审后推荐系统中引入显式公平性正则化，可在不损害学术质量的前提下提升包容性。

Method: 设计Fair-PaperRec框架：采用多层感知机架构，在双盲评审后对论文进行重排序；引入可微公平性损失函数，约束种族、国家等交叉属性。研究分两阶段：先在包含高、中、近公平三种偏见水平的合成数据集上进行控制实验，评估参数行为；后在ACM SIGCHI、DIS、IUI真实会议数据上验证实际效果。

Result: 合成实验表明，提高公平性权重可同步增强宏/微观多样性且效用保持稳定。真实场景下，优化配置的Fair-PaperRec较历史选择可提升弱势群体参与度达42.03%，整体效用波动控制在3.16%以内。在高度偏见环境中，公平性正则化兼具公平促进与质量正则化的双重作用。

Conclusion: 通过合成到真实数据的系统性验证，Fair-PaperRec为构建注重公平性的评审后论文选择机制提供了可行框架，在保障学术质量的同时有效促进包容性。

Abstract: Despite frequent double-blind review, systemic biases related to author demographics still disadvantage underrepresented groups. We start from a simple hypothesis: if a post-review recommender is trained with an explicit fairness regularizer, it should increase inclusion without degrading quality. To test this, we introduce Fair-PaperRec, a Multi-Layer Perceptron (MLP) with a differentiable fairness loss over intersectional attributes (e.g., race, country) that re-ranks papers after double-blind review. We first probe the hypothesis on synthetic datasets spanning high, moderate, and near-fair biases. Across multiple randomized runs, these controlled studies map where increasing the fairness weight strengthens macro/micro diversity while keeping utility approximately stable, demonstrating robustness and adaptability under varying disparity levels. We then carry the hypothesis into the original setting, conference data from ACM Special Interest Group on Computer-Human Interaction (SIGCHI), Designing Interactive Systems (DIS), and Intelligent User Interfaces (IUI). In this real-world scenario, an appropriately tuned configuration of Fair-PaperRec achieves up to a 42.03% increase in underrepresented-group participation with at most a 3.16% change in overall utility relative to the historical selection. Taken together, the synthetic-to-original progression shows that fairness regularization can act as both an equity mechanism and a mild quality regularizer, especially in highly biased regimes. By first analyzing the behavior of the fairness parameters under controlled conditions and then validating them on real submissions, Fair-PaperRec offers a practical, equity-focused framework for post-review paper selection that preserves, and in some settings can even enhance, measured scholarly quality.

</details>


### [88] [Beyond performance-wise Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2602.22470)
*Balazs Pejo*

Main category: cs.LG

TL;DR: 本文针对联邦学习中的客户端评估问题，指出现有方法仅关注模型性能（如准确率）的局限性，转而研究客户端对模型可信度的多维度贡献（可靠性、鲁棒性和公平性），采用Shapley值近似方法进行量化，发现各维度相互独立且无单一客户端在所有维度上都表现优异，从而揭示了当前单指标评估体系在公平奖励分配上的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习客户端评估方法过于侧重模型性能指标（如准确率或损失），这仅反映了机器学习模型整体效用的单一维度。相比之下，模型的信任worthiness（包括对噪声数据的容忍度、对抗样本的抵抗能力以及人口统计均等性衡量的公平性）等关键因素被严重忽视，而这些因素对于构建可靠的联邦学习系统至关重要。

Method: 本研究采用Shapley值的先进近似方法来量化客户端在多维度可信度指标上的贡献。具体评估三个维度：可靠性（容忍噪声数据的能力）、韧性（抵抗对抗样本的能力）和公平性（通过人口统计均等性衡量），以此构建多维度的客户端贡献评估框架。

Result: 实验结果表明，没有任何单个客户端能在所有可信度维度上同时表现优异，且这些维度之间基本相互独立。这一发现揭示了当前基于单一指标（如准确率）的评估方案存在根本性缺陷，无法全面反映客户端的真实贡献价值。

Conclusion: 研究得出结论，单一评估指标不足以实现全面评估和公平奖励分配。联邦学习系统需要建立多维度的客户端贡献评估体系，才能更准确地识别不同客户端的独特价值并实现更加公平的奖励机制设计。

Abstract: Federated learning offers a privacy-friendly collaborative learning framework, yet its success, like any joint venture, hinges on the contributions of its participants. Existing client evaluation methods predominantly focus on model performance, such as accuracy or loss, which represents only one dimension of a machine learning model's overall utility. In contrast, this work investigates the critical, yet overlooked, issue of client contributions towards a model's trustworthiness -- specifically, its reliability (tolerance to noisy data), resilience (resistance to adversarial examples), and fairness (measured via demographic parity). To quantify these multifaceted contributions, we employ the state-of-the-art approximation of the Shapley value, a principled method for value attribution. Our results reveal that no single client excels across all dimensions, which are largely independent from each other, highlighting a critical flaw in current evaluation scheme: no single metric is adequate for comprehensive evaluation and equitable rewarding allocation.

</details>


### [89] [Reinforcement-aware Knowledge Distillation for LLM Reasoning](https://arxiv.org/abs/2602.22495)
*Zhaoyang Zhang,Shuli Jiang,Yantao Shen,Yuting Zhang,Dhananjay Ram,Shuo Yang,Zhuowen Tu,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: 本文针对强化学习后训练的大型语言模型推理成本高的问题，提出RL感知蒸馏（RLAD）方法。其核心组件信任区域比率蒸馏（TRRD）用基于教师-旧策略混合的PPO/GRPO风格似然比目标替代教师-学生KL正则化，实现优势感知、信任区域有界的蒸馏，在逻辑推理和数学基准测试中一致优于离线蒸馏、标准GRPO及KL基在线师生知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习后训练显著提升了大型语言模型的长链思维推理能力，但其高昂的推理成本促使研究者将其蒸馏到小型模型中。然而，现有知识蒸馏方法专为监督微调设计，依赖固定教师轨迹或KL散度正则化，与RL结合时会出现分布失配（教师监督与学生的演进rollout分布不一致）和目标干扰（KL正则化与奖励最大化竞争）问题，需精细的损失平衡。

Method: 本文提出RL感知蒸馏（RLAD），在RL过程中执行选择性模仿——仅在能提升当前策略更新时指导学生。核心创新信任区域比率蒸馏（TRRD）用PPO/GRPO风格的似然比目标替代教师-学生KL正则化器，该目标以教师-旧策略混合为锚点，实现对学生rollout的优势感知、信任区域有界蒸馏，自然平衡探索、利用与模仿。

Result: 在多种逻辑推理和数学基准测试中，RLAD一致优于离线蒸馏、标准GRPO以及基于KL的在线师生知识蒸馏方法。

Conclusion: 所提方法有效解决了现有知识蒸馏与强化学习结合时的分布失配和目标干扰问题，为将RL训练的大型语言模型高效蒸馏至小型模型提供了新思路，在保持推理性能的同时显著降低推理成本。

Abstract: Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for supervised fine-tuning (SFT), relying on fixed teacher traces or teacher-student Kullback-Leibler (KL) divergence-based regularization. When combined with RL, these approaches often suffer from distribution mismatch and objective interference: teacher supervision may not align with the student's evolving rollout distribution, and the KL regularizer can compete with reward maximization and require careful loss balancing. To address these issues, we propose RL-aware distillation (RLAD), which performs selective imitation during RL -- guiding the student toward the teacher only when it improves the current policy update. Our core component, Trust Region Ratio Distillation (TRRD), replaces the teacher-student KL regularizer with a PPO/GRPO-style likelihood-ratio objective anchored to a teacher--old-policy mixture, yielding advantage-aware, trust-region-bounded distillation on student rollouts and naturally balancing exploration, exploitation, and imitation. Across diverse logic reasoning and math benchmarks, RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.

</details>


### [90] [TEFL: Prediction-Residual-Guided Rolling Forecasting for Multi-Horizon Time Series](https://arxiv.org/abs/2602.22520)
*Xiannan Huang,Shen Fang,Shuhan Qiu,Chengcheng Yu,Jiayuan Du,Chao Yang*

Main category: cs.LG

TL;DR: 本文提出TEFL（时序误差反馈学习）框架，通过显式利用滚动预测中的历史残差信息来提升深度学习时序预测模型的性能。该框架解决了部分可观测性下的多步残差选择、轻量级低秩适配器集成以及两阶段联合训练等关键挑战，在10个真实数据集和5种骨干架构上验证了其有效性，平均降低MAE 5-10%，在分布突变等挑战场景下最高可降低19.5%误差。


<details>
  <summary>Details</summary>
Motivation: 现有深度预测模型仅优化点对点预测损失，忽略了滚动预测过程中蕴含的丰富残差信息（如持续偏差、未建模模式和动态演化）。这些残差反映了模型的系统性不足，充分利用这些信息有望显著提升预测精度和鲁棒性。

Method: 提出统一的TEFL学习框架：1）在部分可观测条件下选择可观测的多步残差；2）设计轻量级低秩适配器集成残差信息，兼顾效率与防过拟合；3）采用两阶段训练流程，联合优化基础预测器和误差修正模块。该框架可在训练和评估阶段都融入历史残差反馈。

Result: 在10个真实世界数据集和5种骨干架构上的广泛实验表明：TEFL持续提升预测精度，平均MAE降低5-10%；在突变和分布漂移等挑战场景下表现尤为突出，误差降低超过10%，最高达19.5%。

Conclusion: TEFL通过将基于残差的反馈直接嵌入学习过程，为现代深度预测系统提供了一种简单、通用且有效的增强方案，显著提升了模型准确性和鲁棒性，具有实际应用价值。

Abstract: Time series forecasting plays a critical role in domains such as transportation, energy, and meteorology. Despite their success, modern deep forecasting models are typically trained to minimize point-wise prediction loss without leveraging the rich information contained in past prediction residuals from rolling forecasts - residuals that reflect persistent biases, unmodeled patterns, or evolving dynamics. We propose TEFL (Temporal Error Feedback Learning), a unified learning framework that explicitly incorporates these historical residuals into the forecasting pipeline during both training and evaluation. To make this practical in deep multi-step settings, we address three key challenges: (1) selecting observable multi-step residuals under the partial observability of rolling forecasts, (2) integrating them through a lightweight low-rank adapter to preserve efficiency and prevent overfitting, and (3) designing a two-stage training procedure that jointly optimizes the base forecaster and error module. Extensive experiments across 10 real-world datasets and 5 backbone architectures show that TEFL consistently improves accuracy, reducing MAE by 5-10% on average. Moreover, it demonstrates strong robustness under abrupt changes and distribution shifts, with error reductions exceeding 10% (up to 19.5%) in challenging scenarios. By embedding residual-based feedback directly into the learning process, TEFL offers a simple, general, and effective enhancement to modern deep forecasting systems.

</details>


### [91] [Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences](https://arxiv.org/abs/2602.21585)
*Sweta Karlekar,Carolina Zheng,Magnus Saebo,Nicolas Beltran-Velez,Shuyang Yu,John Bowlan,Michal Kucer,David Blei*

Main category: cs.LG

TL;DR: Duel-Evolve是一种基于成对自我偏好的进化优化算法，通过贝叶斯Bradley-Terry模型聚合LLM自身生成的噪声比较信号，并利用双重汤普森采样进行预算分配与父代选择。该方法在无需外部奖励模型、真实标签或手工评分函数的情况下，于MathBench和LiveCodeBench上分别取得+20%和+12%的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有测试时优化方法依赖标量评估器指导离散输出空间的迭代搜索，但该评估器常面临不可用、信号稀疏或可靠性差的问题。成对比较不仅更易获取，还能有效指示改进方向，且可由LLM无监督地自我生成，从而规避外部监督需求。

Method: Duel-Evolve的核心方法包括：(1) 利用同一LLM生成候选解并收集其成对偏好比较；(2) 采用贝叶斯Bradley-Terry模型整合噪声比较数据，输出带有不确定性度量的候选质量估计；(3) 基于双重汤普森采样策略，将有限的比较预算导向潜在最优区域；(4) 根据质量估计选择高质量父代进行进化生成。

Result: 实验表明：在MathBench基准上，Duel-Evolve较现有方法准确率提升20个百分点；在LiveCodeBench上较同类迭代方法提升超过12个百分点。关键优势在于完全免除对奖励模型、搜索阶段真实标签以及手工设计评分函数的依赖。

Conclusion: 研究证实LLM自我生成的成对偏好可作为大离散输出空间测试时优化的强有效信号，为无监督、自引导的LLM输出优化提供了新范式。

Abstract: Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.

</details>


### [92] [Predicting Tennis Serve directions with Machine Learning](https://arxiv.org/abs/2602.22527)
*Ying Zhu,Ruthuparna Naikar*

Main category: cs.LG

TL;DR: 本研究开发了机器学习方法来预测职业网球选手的一发方向，通过特征工程实现了男选手约49%、女选手约44%的平均预测准确率，发现顶尖选手采用混合策略模型，且疲劳和情境信息对发球决策与接发球预判有重要影响。


<details>
  <summary>Details</summary>
Motivation: 职业网球中第一发球至关重要，发球方与接发球方存在策略性心理博弈。理解选手发球决策机制有助于揭示比赛战略决策过程，并为接发球预判提供数据支持，提升对高水平网球竞技的认知。

Method: 采用机器学习方法结合特征工程技术，对职业选手的一发方向进行预测建模。通过构建相关特征集来捕捉影响发球决策的关键因素。

Result: 预测准确率达男性选手49%、女性选手44%；证实顶尖选手使用混合策略模型进行发球决策；发现疲劳因素可能影响发球方向选择；表明情境信息对接发球方的预判比此前认知更为重要。

Conclusion: 本研究通过机器学习验证了网球发球决策的博弈性质，揭示了混合策略、疲劳因素和情境信息在发球决策中的作用，为理解职业网球比赛中的战略决策机制提供了新证据，强调了情境因素在接发球预判中的关键价值。

Abstract: Serves, especially first serves, are very important in professional tennis. Servers choose their serve directions strategically to maximize their winning chances while trying to be unpredictable. On the other hand, returners try to predict serve directions to make good returns. The mind game between servers and returners is an important part of decision-making in professional tennis matches. To help understand the players' serve decisions, we have developed a machine learning method for predicting professional tennis players' first serve directions. Through feature engineering, our method achieves an average prediction accuracy of around 49\% for male players and 44\% for female players. Our analysis provides some evidence that top professional players use a mixed-strategy model in serving decisions and that fatigue might be a factor in choosing serve directions. Our analysis also suggests that contextual information is perhaps more important for returners' anticipatory reactions than previously thought.

</details>


### [93] [Persistent Nonnegative Matrix Factorization via Multi-Scale Graph Regularization](https://arxiv.org/abs/2602.22536)
*Jichao Zhang,Ran Miao,Limin Li*

Main category: cs.LG

TL;DR: 提出持久非负矩阵分解(pNMF)，通过持久同调获得多尺度下的最优分辨率集合，构建跨尺度一致的低维嵌入，并在合成和单细胞RNA测序数据上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统NMF方法只能在单一尺度上提取特征，无法捕捉数据在不同分辨率下的连接结构演化。为获得多尺度的可解释表示并揭示拓扑变化，需要一种能够在多个尺度上保持一致的矩阵分解方法。

Method: 引入持久同调来识别拓扑结构发生质变的极小充分尺度集合；在这些尺度上构造图拉普拉斯序列，建立带有尺度几何正则化和跨尺度一致性约束的耦合NMF模型；分析嵌入在尺度方向的结构性质，给出相邻尺度间增量的上界；提出顺序交替优化算法，保证收敛性。

Result: 在合成数据和单细胞RNA测序数据集上的数值实验表明，pNMF能够产生多尺度低秩嵌入，且嵌入在尺度间保持一致性，能够有效捕捉不同分辨率下的拓扑特征。

Conclusion: pNMF提供了一种新的多尺度矩阵分解框架，能够生成跨尺度一致的嵌入，具有理论保证和实际效果，为多尺度数据分析提供了有力工具。

Abstract: Matrix factorization techniques, especially Nonnegative Matrix Factorization (NMF), have been widely used for dimensionality reduction and interpretable data representation. However, existing NMF-based methods are inherently single-scale and fail to capture the evolution of connectivity structures across resolutions. In this work, we propose persistent nonnegative matrix factorization (pNMF), a scale-parameterized family of NMF problems, that produces a sequence of persistence-aligned embeddings rather than a single one. By leveraging persistent homology, we identify a canonical minimal sufficient scale set at which the underlying connectivity undergoes qualitative changes. These canonical scales induce a sequence of graph Laplacians, leading to a coupled NMF formulation with scale-wise geometric regularization and explicit cross-scale consistency constraint. We analyze the structural properties of the embeddings along the scale parameter and establish bounds on their increments between consecutive scales. The resulting model defines a nontrivial solution path across scales, rather than a single factorization, which poses new computational challenges. We develop a sequential alternating optimization algorithm with guaranteed convergence. Numerical experiments on synthetic and single-cell RNA sequencing datasets demonstrate the effectiveness of the proposed approach in multi-scale low-rank embeddings.

</details>


### [94] [LUMOS: Democratizing SciML Workflows with L0-Regularized Learning for Unified Feature and Parameter Adaptation](https://arxiv.org/abs/2602.22537)
*Shouwei Gao,Xu Zheng,Dongsheng Luo,Sheng Di,Wenqian Dong*

Main category: cs.LG

TL;DR: 针对科学机器学习模型设计依赖人工调参的问题，本文提出LUMOS端到端框架，基于L0正则化学习统一特征选择与模型剪枝。通过半随机门控和重参数化技术动态优化模型结构，在13个跨领域任务上实现71.45%参数量压缩和6.4倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有SciML模型设计需要大量领域知识和手动调参，尤其在输入特征选择和模型规模确定方面。这种依赖性限制了SciML的广泛应用，亟需自动化方法以降低使用门槛，实现模型设计的民主化。

Method: LUMOS采用L0正则化学习策略，结合半随机门控机制和重参数化技巧，构建端到端训练框架。该方法在训练过程中同步进行特征选择与参数剪枝，自动识别信息特征并剔除冗余连接。

Result: 在包含宇宙学和分子科学等13个SciML任务上的实验表明，LUMOS平均减少71.45%模型参数，推理速度提升6.4倍。通过最多8个GPU的分布式数据并行训练验证了其良好的可扩展性。

Conclusion: LUMOS通过L0正则化统一特征选择与模型剪枝，有效降低了SciML模型设计对人工经验的依赖。实验证明其在保持预测精度的同时显著提升模型效率，为SciML的民主化应用提供了有效解决方案。

Abstract: The rapid growth of scientific machine learning (SciML) has accelerated discovery across diverse domains, yet designing effective SciML models remains a challenging task. In practice, building such models often requires substantial prior knowledge and manual expertise, particularly in determining which input features to use and how large the model should be. We introduce LUMOS, an end-to-end framework based on L0-regularized learning that unifies feature selection and model pruning to democratize SciML model design. By employing semi-stochastic gating and reparameterization techniques, LUMOS dynamically selects informative features and prunes redundant parameters during training, reducing the reliance on manual tuning while maintaining predictive accuracy. We evaluate LUMOS across 13 diverse SciML workloads, including cosmology and molecular sciences, and demonstrate its effectiveness and generalizability. Experiments on 13 SciML models show that LUMOS achieves 71.45% parameter reduction and a 6.4x inference speedup on average. Furthermore, Distributed Data Parallel (DDP) training on up to eight GPUs confirms the scalability of

</details>


### [95] [RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format](https://arxiv.org/abs/2602.22538)
*Zhehao Huang,Yuhang Liu,Baijiong Lin,Yixin Lou,Zhengbao He,Hanling Tian,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: 本文提出 RAIN-Merging 方法，通过将指令模型的任务向量投影到推理模型思考子空间的零空间，并利用指令注意力进行模块级缩放，实现了在不损害推理能力的前提下显著提升大模型的指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）在复杂推理任务上表现出色，但在遵循输出格式、约束等指令方面存在不足。虽然指令微调模型（ITM）擅长指令遵循，但其推理能力有限。研究表明，LRM与ITM的参数空间（任务向量）主子空间近乎正交，暗示轻量级合并可行。然而，由于两者输出格式不匹配（LRM包含显式思考过程，而ITM仅输出答案），直接合并效果不佳且不稳定。

Method: 本文提出 RAIN-Merging（推理感知的指令注意力引导零空间投影合并），一种无需梯度的方法：首先，利用少量推理校准集，将ITM的任务向量投影到LRM思考特殊token前向特征的零空间中，以保留LRM的结构化推理机制；其次，利用少量指令校准集，通过估计指令注意力来推导模块特定的缩放因子，从而放大与指令相关的参数分量并抑制无关信息泄露。

Result: 在四个指令遵循基准测试和九个推理及通用能力基准测试中，RAIN-Merging 显著提升了模型的指令遵循能力，同时保持了推理质量。该方法的增益在不同模型规模和架构下均保持一致，并能转化为智能体任务中的性能提升。

Conclusion: RAIN-Merging 成功解决了LRM指令遵循能力不足的问题，为构建兼具强大推理能力和精确指令控制的大模型提供了一种有效且可扩展的解决方案，具有重要的实践价值。

Abstract: Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.

</details>


### [96] [Relatron: Automating Relational Machine Learning over Relational Databases](https://arxiv.org/abs/2602.22552)
*Zhikai Chen,Han Xie,Jian Zhang,Jiliang Tang,Xiang Song,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 本文系统比较了关系数据库预测建模的两种方法：关系深度学习(RDL)和深度特征综合(DFS)。研究发现二者性能高度依赖任务且无单一架构占优，验证准确率不可靠。为此提出Relatron元选择器，通过任务嵌入和轻量级损失景观度量，在联合超参数-架构优化中相比强基线提升高达18.5%，成本降低10倍。


<details>
  <summary>Details</summary>
Motivation: 关系数据库预测建模面临跨表依赖和复杂特征交互的挑战。尽管RDL通过消息传递自动化特征工程，DFS依赖预定义非参数聚合器，但二者相对优势及架构选择的设计原则仍不明确，缺乏系统性比较研究。

Method: 研究在统一设计空间中整合RDL和DFS，执行跨多样化RDB任务的架构搜索。构建架构-性能映射库，分析性能差距驱动因素，提出两个任务信号（RDB任务同质性和亲和嵌入），并开发基于任务嵌入的元选择器Relatron，结合轻量级损失景观度量优选平坦最优解。

Result: 三大发现：1)RDL并非持续优于DFS，性能高度任务依赖；2)无通用优势架构，需任务感知选择；3)验证准确率指导架构选择不可靠。Relatron解决了"调参越多性能越差"效应，在联合优化中取得最高18.5%的提升，成本仅为Fisher信息方法的1/10。

Conclusion: 研究揭示了RDL和DFS任务依赖性的本质，证实了任务感知元选择的必要性。Relatron通过任务信号引导和损失景观感知，为关系数据库预测建模提供了高效架构选择方案，显著降低了调参成本并提升了模型性能。

Abstract: Predictive modeling over relational databases (RDBs) powers applications, yet remains challenging due to capturing both cross-table dependencies and complex feature interactions. Relational Deep Learning (RDL) methods automate feature engineering via message passing, while classical approaches like Deep Feature Synthesis (DFS) rely on predefined non-parametric aggregators. Despite performance gains, the comparative advantages of RDL over DFS and the design principles for selecting effective architectures remain poorly understood. We present a comprehensive study that unifies RDL and DFS in a shared design space and conducts architecture-centric searches across diverse RDB tasks. Our analysis yields three key findings: (1) RDL does not consistently outperform DFS, with performance being highly task-dependent; (2) no single architecture dominates across tasks, underscoring the need for task-aware model selection; and (3) validation accuracy is an unreliable guide for architecture choice. This search yields a model performance bank that links architecture configurations to their performance; leveraging this bank, we analyze the drivers of the RDL-DFS performance gap and introduce two task signals -- RDB task homophily and an affinity embedding that captures size, path, feature, and temporal structure -- whose correlation with the gap enables principled routing. Guided by these signals, we propose Relatron, a task embedding-based meta-selector that chooses between RDL and DFS and prunes the within-family search. Lightweight loss-landscape metrics further guard against brittle checkpoints by preferring flatter optima. In experiments, Relatron resolves the "more tuning, worse performance" effect and, in joint hyperparameter-architecture optimization, achieves up to 18.5% improvement over strong baselines with 10x lower cost than Fisher information-based alternatives.

</details>


### [97] [Multilingual Safety Alignment Via Sparse Weight Editing](https://arxiv.org/abs/2602.22554)
*Jiaming Liang,Zhaoxin Wang,Handing Wang*

Main category: cs.LG

TL;DR: 针对大语言模型在低资源语言中安全防护失效的问题，本文提出基于稀疏权重编辑的训练无关对齐框架。通过定位安全神经元，将跨语言对齐建模为约束线性变换，推导出将低资源语言有害表示映射到高资源语言安全子空间的最优闭式解，在保持通用能力的同时显著降低攻击成功率，仅需单次数据高效计算。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在不同语言间存在显著的安全能力差异，低资源语言（LRLs）常能绕过为英语等高资源语言（HRLs）设计的安全护栏。现有方法如多语言监督微调（SFT）和基于人类反馈的强化学习（RLHF）计算开销巨大，且依赖稀缺的多语言安全标注数据，这限制了其在资源有限场景下的应用。

Method: 本文提出一种基于稀疏权重编辑的训练无关对齐框架。核心方法是识别安全能力集中存在于稀疏的安全神经元子集，将跨语言安全对齐问题形式化为带约束的线性变换优化问题。通过推导闭式解，实现将低资源语言的有害表示最优映射到高资源语言的安全子空间，同时采用零空间投影约束来保持模型的通用推理能力。

Result: 在涵盖8种语言以及Llama-3、Qwen-2.5等多个模型家族的广泛实验中，所提方法显著降低了低资源语言的攻击成功率（ASR），同时对模型的通用推理能力影响微乎其微。该方法仅需一次数据高效的计算过程，无需训练即可实现跨语言安全对齐。

Conclusion: 该研究成功开发了一种高效、数据高效的训练无关跨语言安全对齐方法，通过稀疏权重编辑实现了安全能力从高资源语言向低资源语言的有效迁移。这一方法为多语言大模型的安全部署提供了新思路，在降低计算成本的同时保证了安全防护效果，具有重要的理论意义和应用价值。

Abstract: Large Language Models (LLMs) exhibit significant safety disparities across languages, with low-resource languages (LRLs) often bypassing safety guardrails established for high-resource languages (HRLs) like English. Existing solutions, such as multilingual supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), are computationally expensive and dependent on scarce multilingual safety data. In this work, we propose a novel, training-free alignment framework based on Sparse Weight Editing. Identifying that safety capabilities are localized within a sparse set of safety neurons, we formulate the cross-lingual alignment problem as a constrained linear transformation. We derive a closed-form solution to optimally map the harmful representations of LRLs to the robust safety subspaces of HRLs, while preserving general utility via a null-space projection constraint. Extensive experiments across 8 languages and multiple model families (Llama-3, Qwen-2.5) demonstrate that our method substantially reduces Attack Success Rate (ASR) in LRLs with negligible impact on general reasoning capabilities, all achieved with a single, data-efficient calculation.

</details>


### [98] [Autoregressive Visual Decoding from EEG Signals](https://arxiv.org/abs/2602.22555)
*Sicheng Dai,Hongwang Xiao,Shan Yu,Qiwei Ye*

Main category: cs.LG

TL;DR: 该论文提出AVDE框架，一种轻量级高效的EEG视觉解码方法。通过对比学习对齐EEG与图像表示，并采用基于"下一尺度预测"的自回归生成框架，使用Transformer从EEG嵌入预测多尺度图像token。在两个数据集上，AVDE以仅10%的参数量超越现有SOTA方法，其生成过程反映了人类视觉感知的层次性。


<details>
  <summary>Details</summary>
Motivation: 当前EEG视觉解码方法面临两大挑战：一是EEG与图像间的模态鸿沟需要复杂的多阶段适配过程，导致一致性难以保持且误差累积；二是大规模扩散模型计算开销大，限制了其在实际脑机接口应用中的可行性。因此，需要开发一种轻量、高效且能直接连接EEG信号与图像的解码框架。

Method: AVDE框架包含两个核心组件：(1) 利用预训练EEG模型LaBraM，通过对比学习微调以对齐EEG与图像表示；(2) 采用基于"下一尺度预测"的自回归生成框架：使用预训练VQ-VAE将图像编码为多尺度token映射，然后以EEG嵌入作为最粗表示，训练Transformer自回归地预测更细尺度的token。该方法保持了EEG信号与重建图像间的直接连接，同时实现连贯生成。

Result: 在两个数据集上的实验表明，AVDE在图像检索和重建任务上均优于先前SOTA方法，同时仅使用10%的参数量。中间输出可视化显示，AVDE的生成过程反映了人类视觉感知的层次性。

Conclusion: 该研究证明了自回归模型作为高效、可解释脑机接口工具的潜力，为实际BCI应用提供了可行的解决方案。

Abstract: Electroencephalogram (EEG) signals have become a popular medium for decoding visual information due to their cost-effectiveness and high temporal resolution. However, current approaches face significant challenges in bridging the modality gap between EEG and image data. These methods typically rely on complex adaptation processes involving multiple stages, making it hard to maintain consistency and manage compounding errors. Furthermore, the computational overhead imposed by large-scale diffusion models limit their practicality in real-world brain-computer interface (BCI) applications. In this work, we present AVDE, a lightweight and efficient framework for visual decoding from EEG signals. First, we leverage LaBraM, a pre-trained EEG model, and fine-tune it via contrastive learning to align EEG and image representations. Second, we adopt an autoregressive generative framework based on a "next-scale prediction" strategy: images are encoded into multi-scale token maps using a pre-trained VQ-VAE, and a transformer is trained to autoregressively predict finer-scale tokens starting from EEG embeddings as the coarsest representation. This design enables coherent generation while preserving a direct connection between the input EEG signals and the reconstructed images. Experiments on two datasets show that AVDE outperforms previous state-of-the-art methods in both image retrieval and reconstruction tasks, while using only 10% of the parameters. In addition, visualization of intermediate outputs shows that the generative process of AVDE reflects the hierarchical nature of human visual perception. These results highlight the potential of autoregressive models as efficient and interpretable tools for practical BCI applications.

</details>


### [99] [Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation](https://arxiv.org/abs/2602.22556)
*Zihang Xu,Haozhi Xie,Ziqi Miao,Wuxuan Gong,Chen Qian,Lijun Li*

Main category: cs.LG

TL;DR: 针对大推理模型在简单查询上的过度思考问题，本文提出两阶段稳定自适应推理框架，通过混合微调与自适应强化学习，在提升准确率3.7/3.6点的同时减少40.6%/43.9%生成token。


<details>
  <summary>Details</summary>
Motivation: 大推理模型通过扩展推理轨迹获得高性能，但对低复杂度查询易过度思考。现有方法面临准确率-效率权衡不稳定和异构推理行为鲁棒性差的双重局限。

Method: 两阶段框架：第一阶段采用混合微调使模型同时学习思考与不思考行为，建立良好初始化；第二阶段进行自适应强化学习，结合保正确性优势修正(CPAS)避免抑制正确长链推理，并采用长度感知梯度调节(LAGR)稳定异构推理长度下的优化。

Result: 在Qwen2.5-1.5B和7B模型上，相较强基线准确率提升+3.7/+3.6个百分点，生成token减少40.6%/43.9%。在不同难度问题和分布外任务上验证了方法的鲁棒性和泛化能力。

Conclusion: 该方法通过稳定自适应推理机制，有效解决了过度思考问题，在保持长链推理能力的同时显著提升效率，展现出良好的泛化性和鲁棒性。

Abstract: Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.

</details>


### [100] [Operationalizing Fairness: Post-Hoc Threshold Optimization Under Hard Resource Limits](https://arxiv.org/abs/2602.22560)
*Moirangthem Tiken Singh,Amit Kalita,Sapam Jitu Singh*

Main category: cs.LG

TL;DR: 针对高风险领域机器学习部署中安全性与公平性的平衡难题，本文提出一种事后、模型无关的阈值优化框架。该框架在严格资源容量约束下，通过强制单一全局决策阈值确保法律合规，并采用参数化伦理损失函数与有界决策规则，数学上保证干预量不超出可用资源。实验表明，容量约束主导伦理优先级，资源限制在80%以上配置中决定最终阈值；在25%严格容量限制下，框架维持较高风险识别率（召回率0.409-0.702），而无约束公平方法则完全失效。


<details>
  <summary>Details</summary>
Motivation: 现有公平性干预措施存在资源无约束假设和群组特定阈值两大缺陷，违反反歧视法规且无法在高风险领域的资源受限环境中同时满足预测安全性、算法公平性和法律合规性。

Method: 提出事后、模型无关的阈值优化框架，设计参数化伦理损失函数耦合有界决策规则，强制单一全局决策阈值，在数学上确保干预量不超过可用资源，实现预测评分与策略评估的解耦。

Result: 理论证明阈值的关键性质，包括局部单调性和临界容量区间识别。实验结果显示：容量约束在超过80%配置中主导伦理优先级；25%严格容量限制下，框架召回率达0.409-0.702，而标准无约束公平启发式方法崩溃至近零效用。

Conclusion: 理论公平性目标必须明确服从于运营容量限制。该框架通过解耦预测评分与策略评估、严格限制干预率，为资源受限环境下的伦理权衡提供了实用且法律合规的解决方案。

Abstract: The deployment of machine learning in high-stakes domains requires a balance between predictive safety and algorithmic fairness. However, existing fairness interventions often as- sume unconstrained resources and employ group-specific decision thresholds that violate anti- discrimination regulations. We introduce a post-hoc, model-agnostic threshold optimization framework that jointly balances safety, efficiency, and equity under strict and hard capacity constraints. To ensure legal compliance, the framework enforces a single, global decision thresh- old. We formulated a parameterized ethical loss function coupled with a bounded decision rule that mathematically prevents intervention volumes from exceeding the available resources. An- alytically, we prove the key properties of the deployed threshold, including local monotonicity with respect to ethical weighting and the formal identification of critical capacity regimes. We conducted extensive experimental evaluations on diverse high-stakes datasets. The principal re- sults demonstrate that capacity constraints dominate ethical priorities; the strict resource limit determines the final deployed threshold in over 80% of the tested configurations. Furthermore, under a restrictive 25% capacity limit, the proposed framework successfully maintains high risk identification (recall ranging from 0.409 to 0.702), whereas standard unconstrained fairness heuristics collapse to a near-zero utility. We conclude that theoretical fairness objectives must be explicitly subordinated to operational capacity limits to remain in deployment. By decou- pling predictive scoring from policy evaluation and strictly bounding intervention rates, this framework provides a practical and legally compliant mechanism for stakeholders to navigate unavoidable ethical trade-offs in resource-constrained environments.

</details>


### [101] [S2O: Early Stopping for Sparse Attention via Online Permutation](https://arxiv.org/abs/2602.22575)
*Yu Zhang,Songwei Liu,Chenqian Yan,Sheng Lin,Beichen Ning,Fangmin Chen,Xing Wang*

Main category: cs.LG

TL;DR: 本文提出S2O，一种通过在线重排和早停机制突破块级稀疏注意力性能瓶颈的方法，在Llama-3.1-8B的128K长上下文场景下实现7.51倍注意力加速和3.81倍端到端加速，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 注意力机制的计算复杂度与序列长度呈二次方关系，这从根本上限制了长上下文推理效率。现有块级稀疏化方法因粗粒度分块存在固有的稀疏性上限，即使精心设计也难以进一步提升性能。

Method: 受内存系统虚拟-物理地址映射机制启发，S2O重构FlashAttention执行流程，支持非连续token加载。将显式重排转化为在线、索引引导的离散加载策略，通过极轻量级预处理和索引重映射，实现重要性导向的块加载。进一步引入早停规则：按重要性降序计算，当当前块分数低于阈值时提前终止，在可控误差预算下提升有效稀疏度。

Result: 在Llama-3.1-8B和128K上下文下，S2O在匹配稀疏度下单算子MSE降低3.82倍，在匹配MSE下预填充计算密度降低3.31倍，同时保持端到端准确率，实现7.51倍注意力加速和3.81倍端到端加速。

Conclusion: S2O通过在线重排与早停机制显著提升了稀疏注意力的实际稀疏性上限，在保持模型精度的前提下大幅降低计算开销，为长序列建模提供了高效解决方案。

Abstract: Attention scales quadratically with sequence length, fundamentally limiting long-context inference. Existing block-granularity sparsification can reduce latency, but coarse blocks impose an intrinsic sparsity ceiling, making further improvements difficult even with carefully engineered designs. We present S2O, which performs early stopping for sparse attention via online permutation. Inspired by virtual-to-physical address mapping in memory systems, S2O revisits and factorizes FlashAttention execution, enabling inference to load non-contiguous tokens rather than a contiguous span in the original order. Motivated by fine-grained structures in attention heatmaps, we transform explicit permutation into an online, index-guided, discrete loading policy; with extremely lightweight preprocessing and index-remapping overhead, it concentrates importance on a small set of high-priority blocks. Building on this importance-guided online permutation for loading, S2O further introduces an early-stopping rule: computation proceeds from high to low importance; once the current block score falls below a threshold, S2O terminates early and skips the remaining low-contribution blocks, thereby increasing effective sparsity and reducing computation under a controlled error budget.
  As a result, S2O substantially raises the practical sparsity ceiling. On Llama-3.1-8B under a 128K context, S2O reduces single-operator MSE by 3.82$\times$ at matched sparsity, and reduces prefill compute density by 3.31$\times$ at matched MSE; meanwhile, it preserves end-to-end accuracy and achieves 7.51$\times$ attention and 3.81$\times$ end-to-end speedups.

</details>


### [102] [IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck](https://arxiv.org/abs/2602.22581)
*Tian Bian,Yifan Niu,Chaohao Yuan,Chengzhi Piao,Bingzhe Wu,Long-Kai Huang,Yu Rong,Tingyang Xu,Hong Cheng,Jia Li*

Main category: cs.LG

TL;DR: 针对现有电路发现方法忽视整体性且需繁琐损坏激活设计的问题，本文提出基于信息瓶颈的端到端框架IBCircuit，可在IOI和Greater-Than等任务中自动识别更忠实、更小的电路。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现研究存在两大局限：一是缺乏整体性视角，二是依赖任务特定的损坏激活设计，导致方法既不准确也不高效，亟需一种通用且整体化的自动发现框架。

Method: IBCircuit是一个基于信息瓶颈原理的优化框架，采用端到端范式整体识别信息性电路，无需人工设计损坏激活，可泛化至任意任务。

Result: 在IOI和Greater-Than任务上的实验表明，IBCircuit识别的电路在关键节点和边组件上比现有方法更忠实、更精简。

Conclusion: 该方法实现了电路发现从任务特定到任务无关、从局部到整体的跨越，为语言模型可解释性研究提供了高效的新工具。

Abstract: Circuit discovery has recently attracted attention as a potential research direction to explain the non-trivial behaviors of language models. It aims to find the computational subgraphs, also known as circuits, within the model that are responsible for solving specific tasks. However, most existing studies overlook the holistic nature of these circuits and require designing specific corrupted activations for different tasks, which is inaccurate and inefficient. In this work, we propose an end-to-end approach based on the principle of Information Bottleneck, called IBCircuit, to identify informative circuits holistically. IBCircuit is an optimization framework for holistic circuit discovery and can be applied to any given task without tediously corrupted activation design. In both the Indirect Object Identification (IOI) and Greater-Than tasks, IBCircuit identifies more faithful and minimal circuits in terms of critical node components and edge components compared to recent related work.

</details>


### [103] [TabDLM: Free-Form Tabular Data Generation via Joint Numerical-Language Diffusion](https://arxiv.org/abs/2602.22586)
*Donghong Cai,Jiarui Feng,Yanbo Wang,Da Zheng,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: 针对异构表格数据生成挑战，本文提出TabDLM——基于掩码扩散语言模型的统一框架，通过联合数值连续扩散与文本掩码扩散，配合双向注意力实现跨模态建模，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着现实世界表格数据中自由文本字段与结构化属性并存，生成此类异构数据面临挑战。现有扩散模型难以处理开放文本生成，而LLM模型因离散分词导致数值精度损失，亟需能同时精确建模数值与语言的新型生成框架。

Method: TabDLM基于掩码扩散语言模型(MDLM)构建：1)文本/分类特征采用掩码扩散建模；2)数值特征通过可学习专用数值标记嵌入进行连续扩散；3)利用双向注意力机制在单模型内捕获跨模态交互，实现统一生成。

Result: 在多个公开基准测试上，TabDLM相比强扩散与LLM基线方法展现出明显优势，验证了其有效性与先进性。

Conclusion: TabDLM通过创新的数值-语言联合扩散框架，成功解决了异构表格数据生成的核心难题，为多模态数据生成与隐私保护提供了新范式。

Abstract: Synthetic tabular data generation has attracted growing attention due to its importance for data augmentation, foundation models, and privacy. However, real-world tabular datasets increasingly contain free-form text fields (e.g., reviews or clinical notes) alongside structured numerical and categorical attributes. Generating such heterogeneous tables with joint modeling of different modalities remains challenging. Existing approaches broadly fall into two categories: diffusion-based methods and LLM-based methods. Diffusion models can capture complex dependencies over numerical and categorical features in continuous or discrete spaces, but extending them to open-ended text is nontrivial and often leads to degraded text quality. In contrast, LLM-based generators naturally produce fluent text, yet their discrete tokenization can distort precise or wide-range numerical values, hindering accurate modeling of both numbers and language. In this work, we propose TabDLM, a unified framework for free-form tabular data generation via a joint numerical--language diffusion model built on masked diffusion language models (MDLMs). TabDLM models textual and categorical features through masked diffusion, while modeling numerical features with a continuous diffusion process through learned specialized numeric tokens embedding; bidirectional attention then captures cross-modality interactions within a single model. Extensive experiments on diverse benchmarks demonstrate the effectiveness of TabDLM compared to strong diffusion- and LLM-based baselines.

</details>


### [104] [pQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training](https://arxiv.org/abs/2602.22592)
*Wenzheng Zhang,Bingzheng Liu,Yang Hu,Xiaoying Bai,Wentao Zhang,Bin Cui*

Main category: cs.LG

TL;DR: pQuant通过将线性层解耦为1位主分支和高精度敏感参数分支，并结合稀疏激活专家扩展，解决了极低比特量化中的参数民主化瓶颈，在边缘部署场景下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为边缘设备构建高效的极低比特（亚2比特）大型语言模型存在精度和可扩展性瓶颈，现有方法未能充分解决参数敏感度同质化（参数民主化效应）导致的表达能力受限问题。

Method: 提出pQuant方法：1）将线性层拆分为用于高效计算的主1比特分支和保留敏感参数的精简高精度分支；2）通过定制特征缩放显式引导模型将敏感参数分配到高精度分支；3）将高精度分支扩展为多个稀疏激活专家以实现高效容量扩展。

Result: 大量实验表明，pQuant在极低比特量化方面达到了最先进的性能水平。

Conclusion: pQuant通过解耦参数和稀疏专家设计有效克服了参数民主化效应，为边缘部署的极低比特大语言模型提供了高性能解决方案。

Abstract: Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability. In this work, we identify a parameter democratization effect as a key bottleneck: the sensitivity of all parameters becomes homogenized, severely limiting expressivity. To address this, we propose pQuant, a method that decouples parameters by splitting linear layers into two specialized branches: a dominant 1-bit branch for efficient computation and a compact high-precision branch dedicated to preserving the most sensitive parameters. Through tailored feature scaling, we explicitly guide the model to allocate sensitive parameters to the high-precision branch. Furthermore, we extend this branch into multiple, sparsely-activated experts, enabling efficient capacity scaling. Extensive experiments indicate our pQuant achieves state-of-the-art performance in extremely low-bit quantization.

</details>


### [105] [Transformers converge to invariant algorithmic cores](https://arxiv.org/abs/2602.22600)
*Joshua S. Schiffman*

Main category: cs.LG

TL;DR: 本研究提出了"算法核心"概念，揭示了在多种Transformer模型（包括马尔可夫链、模加法和GPT-2）中存在的低维不变量。这些紧凑的子空间是任务执行所必需且充分的，在不同训练过程和模型规模间保持稳定，表明Transformer计算围绕共享的算法结构组织，而非随机的权重配置。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型内部工作机制尚不明确，核心障碍在于训练优化的是行为而非电路结构，导致多种权重配置可实现相同功能。这引发关键问题：哪些内部结构反映真实计算，哪些仅是特定训练的偶然产物？亟需区分计算本质与实现细节。

Method: 通过提取"算法核心"——对任务性能必要且充分的紧凑子空间——来识别计算不变量。在独立训练的Transformer、马尔可夫链Transformer、模加法Transformer及GPT-2模型上进行系统实验，分析权重空间、过渡谱和表征几何结构。

Result: 独立训练的模型收敛到相同算法核心；马尔可夫链模型在正交子空间中嵌入相同3D核心并恢复一致的过渡谱；模加法模型在"顿悟"阶段发现紧致循环算子并随后膨胀，可预测记忆到泛化的转变；GPT-2通过单轴控制主谓一致，翻转该轴会全局逆转语法数。这些发现证实了跨训练和尺度的低维不变量存在。

Conclusion: Transformer计算围绕紧凑、共享的算法结构组织，这些低维不变量是跨模型稳定的计算本质。机制可解释性研究应转向目标化这些不变量，而非纠缠于特定实现细节，从而更有效地理解大模型的内在工作机制。

Abstract: Large language models exhibit sophisticated capabilities, yet understanding how they work internally remains a central challenge. A fundamental obstacle is that training selects for behavior, not circuitry, so many weight configurations can implement the same function. Which internal structures reflect the computation, and which are accidents of a particular training run? This work extracts algorithmic cores: compact subspaces necessary and sufficient for task performance. Independently trained transformers learn different weights but converge to the same cores. Markov-chain transformers embed 3D cores in nearly orthogonal subspaces yet recover identical transition spectra. Modular-addition transformers discover compact cyclic operators at grokking that later inflate, yielding a predictive model of the memorization-to-generalization transition. GPT-2 language models govern subject-verb agreement through a single axis that, when flipped, inverts grammatical number throughout generation across scales. These results reveal low-dimensional invariants that persist across training runs and scales, suggesting that transformer computations are organized around compact, shared algorithmic structures. Mechanistic interpretability could benefit from targeting such invariants -- the computational essence -- rather than implementation-specific details.

</details>


### [106] [$φ$-DPO: Fairness Direct Preference Optimization Approach to Continual Learning in Large Multimodal Models](https://arxiv.org/abs/2602.22601)
*Thanh-Dat Truong,Huu-Thien Tran,Jackson Cothren,Bhiksha Raj,Khoa Luu*

Main category: cs.LG

TL;DR: 本文提出一种新型公平性直接偏好优化框架FaiDPO（φ-DPO），用于解决大型多模态模型持续学习中的数据不平衡导致的公平性问题。该框架通过改进偏好对齐方法，同时缓解灾难性遗忘和分布偏差，在多个基准测试中实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型持续学习面临新兴的公平性挑战，特别是数据不平衡分布导致的模型偏见和次优性能。虽然近期研究在缓解灾难性遗忘方面取得进展，但由数据不平衡引发的公平性问题仍未被充分探索，亟需专门方法确保跨任务公平性。

Method: 首先提出基于直接偏好优化（DPO）的持续学习新范式，利用成对偏好信号对齐学习过程以减轻灾难性遗忘。其次识别传统DPO在数据不平衡中的局限性，设计新型φ-DPO损失函数显式处理分布偏差。理论分析证明该方法同时解决遗忘和不平衡问题，并为现有基准构建持续学习所需的成对偏好标注。

Result: 大量实验和消融研究表明，提出的φ-DPO框架在多个基准测试中均达到最先进的性能，显著优于先前的大型多模态模型持续学习方法，验证了其在公平性和性能方面的双重优势。

Conclusion: 本研究成功开发FaiDPO框架，为大型多模态模型持续学习中的公平性问题提供有效解决方案。该方法通过理论保证和实践验证，同时应对灾难性遗忘和数据不平衡两大核心挑战，为未来研究奠定重要基础。

Abstract: Fairness in Continual Learning for Large Multimodal Models (LMMs) is an emerging yet underexplored challenge, particularly in the presence of imbalanced data distributions that can lead to biased model updates and suboptimal performance across tasks. While recent continual learning studies have made progress in addressing catastrophic forgetting, the problem of fairness caused the imbalanced data remains largely underexplored. This paper presents a novel Fairness Direct Preference Optimization (FaiDPO or $φ$-DPO) framework for continual learning in LMMs. In particular, we first propose a new continual learning paradigm based on Direct Preference Optimization (DPO) to mitigate catastrophic forgetting by aligning learning with pairwise preference signals. Then, we identify the limitations of conventional DPO in imbalanced data and present a new $φ$-DPO loss that explicitly addresses distributional biases. We provide a comprehensive theoretical analysis demonstrating that our approach addresses both forgetting and data imbalance. Additionally, to enable $φ$-DPO-based continual learning, we construct pairwise preference annotations for existing benchmarks in the context of continual learning. Extensive experiments and ablation studies show the proposed $φ$-DPO achieves State-of-the-Art performance across multiple benchmarks, outperforming prior continual learning methods of LMMs.

</details>


### [107] [DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion](https://arxiv.org/abs/2602.22610)
*Tao Huang,Jiayang Meng,Xu Yang,Chen Hou,Hong Chen*

Main category: cs.LG

TL;DR: 针对条件扩散模型在差分隐私训练中因异质条件上下文（如观测历史、缺失模式、异常协变量）引发的梯度重尾问题，本文提出DP-aware AdaLN-Zero——一种即插即用的感知敏感度条件机制。该方法通过有界重参数化联合约束条件表示幅度和AdaLN调制参数，在梯度裁剪前抑制极端梯度尾部。实验表明，在匹配隐私设置下，该方法显著提升了时间序列插值/填补和预测性能，同时保持了非私有训练的表达能力。


<details>
  <summary>Details</summary>
Motivation: 条件注入使扩散模型生成上下文感知输出，对时间序列任务至关重要。然而，异质条件上下文会产生重尾逐例梯度。在差分私有随机梯度下降（DP-SGD）中，这些条件驱动的重尾梯度会不成比例地触发全局裁剪，导致异常值主导的更新、更大的裁剪偏差，并在固定隐私预算下造成效用下降。

Method: 提出DP-aware AdaLN-Zero，一种用于条件扩散Transformer的即插即用感知敏感度条件机制。该方法在不修改DP-SGD的前提下，通过有界重参数化技术联合约束条件表示的幅度和AdaLN调制参数，从而在梯度裁剪和噪声注入前抑制极端梯度尾部事件。

Result: 在匹配隐私设置下，配备DP-aware AdaLN-Zero的DP-SGD显著改善了时间序列插值/填补和预测任务的性能。在真实电力数据集和两个公开ETT基准测试上均观察到相对于普通DP-SGD的一致性提升。梯度诊断表明，性能改进源于条件特定的梯度尾部重塑和裁剪失真减少，同时该方法在非私有训练中保持了模型的表达能力。

Conclusion: 感知敏感度的条件机制能够显著改善私有条件扩散模型的训练效果，且不会牺牲标准性能。敏感性感知的条件设计是提升差分隐私条件下生成模型效用的有效途径。

Abstract: Condition injection enables diffusion models to generate context-aware outputs, which is essential for many time-series tasks. However, heterogeneous conditional contexts (e.g., observed history, missingness patterns or outlier covariates) can induce heavy-tailed per-example gradients. Under Differentially Private Stochastic Gradient Descent (DP-SGD), these rare conditioning-driven heavy-tailed gradients disproportionately trigger global clipping, resulting in outlier-dominated updates, larger clipping bias, and degraded utility under a fixed privacy budget. In this paper, we propose DP-aware AdaLN-Zero, a drop-in sensitivity-aware conditioning mechanism for conditional diffusion transformers that limits conditioning-induced gain without modifying the DP-SGD mechanism. DP-aware AdaLN-Zero jointly constrains conditioning representation magnitude and AdaLN modulation parameters via bounded re-parameterization, suppressing extreme gradient tail events before gradient clipping and noise injection. Empirically, DP-SGD equipped with DP-aware AdaLN-Zero improves interpolation/imputation and forecasting under matched privacy settings. We observe consistent gains on a real-world power dataset and two public ETT benchmarks over vanilla DP-SGD. Moreover, gradient diagnostics attribute these improvements to conditioning-specific tail reshaping and reduced clipping distortion, while preserving expressiveness in non-private training. Overall, these results show that sensitivity-aware conditioning can substantially improve private conditional diffusion training without sacrificing standard performance.

</details>


### [108] [ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL](https://arxiv.org/abs/2602.22623)
*Xingyu Lu,Jinpeng Wang,YiFan Zhang,Shijie Ma,Xiao Hu,Tianke Zhang,Haonan fan,Kaiyu Jiang,Changyi Liu,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Chun Yuan*

Main category: cs.LG

TL;DR: ContextRL框架通过上下文增强解决RLVR的识别性和可达性瓶颈：为奖励模型提供完整参考解以实现细粒度过程验证，过滤错误正例；采用多轮采样策略生成错误报告，指导策略从全负样本组中恢复正确答案。在11个感知与推理基准测试中，该框架显著提升知识发现效率，使Qwen3-VL-8B性能媲美32B模型，大幅超越RLVR基线并有效缓解奖励攻击。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR（基于验证奖励的强化学习）存在两大瓶颈：识别性方面，奖励模型难以区分正确答案但低质量推理过程的"错误正例"；可达性方面，策略难以从初始全部失败的样本组中恢复正确响应。这些问题导致知识发现效率低下且易出现奖励攻击（reward hacking）现象。

Method: 提出ContextRL框架，通过上下文增强从两方面解决瓶颈：（1）增强识别性：为奖励模型提供完整参考解作为上下文，实现细粒度过程验证，过滤低质量推理的伪正确答案；（2）提升可达性：引入多轮采样策略，让奖励模型为失败尝试生成错误报告，指导策略从此前全负样本组中"恢复"出正确响应。

Result: 在11个感知与推理基准测试上，ContextRL显著提升知识发现效率。核心成果：Qwen3-VL-8B模型通过该框架性能达到32B模型水平，大幅超越标准RLVR基线，并有效缓解奖励攻击。深入分析揭示了上下文信息对提升奖励模型准确率的巨大潜力，并记录了奖励攻击的普遍存在性。

Conclusion: ContextRL框架验证了上下文增强在RLVR中的关键作用，为未来研究提供了重要启示：利用上下文信息可显著改善奖励模型质量，缓解奖励攻击问题，为开发更高效的基于验证奖励的强化学习系统提供了可行路径。

Abstract: We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples with the right answer but low-quality reasoning process). To improve Reachability, we introduce a multi-turn sampling strategy where the reward model generates mistake reports for failed attempts, guiding the policy to "recover" correct responses from previously all-negative groups. Experimental results on 11 perception and reasoning benchmarks show that ContextRL significantly improves knowledge discovery efficiency. Notably, ContextRL enables the Qwen3-VL-8B model to achieve performance comparable to the 32B model, outperforming standard RLVR baselines by a large margin while effectively mitigating reward hacking. Our in-depth analysis reveals the significant potential of contextual information for improving reward model accuracy and document the widespread occurrence of reward hacking, offering valuable insights for future RLVR research.

</details>


### [109] [Semantic Tube Prediction: Beating LLM Data Efficiency with JEPA](https://arxiv.org/abs/2602.22617)
*Hai Huang,Yann LeCun,Randall Balestriero*

Main category: cs.LG

TL;DR: 本文提出语义流形测地线假说，并设计语义管道预测（STP）正则化任务，通过约束隐藏状态轨迹在测地线邻域内，使大语言模型在仅用1/16训练数据的情况下达到基线准确率，挑战了Chinchilla缩放定律的数据效率边界，证明几何先验可突破暴力扩展范式。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缩放定律仅为描述性经验规律，无法指导最优训练策略，且极少有研究成功挑战其隐含的数据效率边界。为此，本文探索几何先验能否突破数据缩放限制。

Method: 基于"token序列在光滑语义流形上沿测地线演化且局部线性"的测地线假说，提出语义管道预测（STP）任务——一种JEPA风格正则化器，约束隐藏状态轨迹保持在测地线管道邻域内，无需显式多视角增强即可将JEPA推广至语言领域。

Result: 在NL-RX-SYNTH数据集上，采用STP的大语言模型仅需16分之一的训练数据即可达到基线准确率，直接违反了Chinchilla缩放定律的数据项，同时提高了信噪比并防止推理时轨迹碰撞以保持多样性。

Conclusion: 原则性的几何先验能有效提升数据效率，超越传统暴力扩展模式，为突破缩放定律限制提供了新方向。

Abstract: Large Language Models (LLMs) obey consistent scaling laws -- empirical power-law fits that predict how loss decreases with compute, data, and parameters. While predictive, these laws are descriptive rather than prescriptive: they characterize typical training, not optimal training. Surprisingly few works have successfully challenged the data-efficiency bounds implied by these laws -- which is our primary focus. To that end, we introduce the Geodesic Hypothesis, positing that token sequences trace geodesics on a smooth semantic manifold and are therefore locally linear. Building on this principle, we propose a novel Semantic Tube Prediction (STP) task, a JEPA-style regularizer that confines hidden-state trajectories to a tubular neighborhood of the geodesic. STP generalizes JEPA to language without requiring explicit multi-view augmentations. We show this constraint improves signal-to-noise ratio, and consequently preserves diversity by preventing trajectory collisions during inference. Empirically, STP allows LLMs to match baseline accuracy with 16$\times$ less training data on the NL-RX-SYNTH dataset, directly violating the data term of Chinchilla-style scaling laws and demonstrating that principled geometric priors can surpass brute-force scaling. Code is available at https://github.com/galilai-group/llm-jepa#stp.

</details>


### [110] [Moral Preferences of LLMs Under Directed Contextual Influence](https://arxiv.org/abs/2602.22831)
*Phil Blandfort,Tushar Karayil,Urja Pawar,Robert Graham,Alex McKenzie,Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: 研究发现LLMs的道德决策易受上下文操纵，即使无关线索也能显著改变选择。基线偏好无法预测可操纵性，推理虽降低敏感性却放大有偏示例影响，需扩展评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有道德基准依赖无上下文提示并假设模型偏好稳定，但实际部署中用户请求、社会规范等情境信号会引导决策，这种情境依赖性缺乏系统评估，存在盲区。

Method: 研究者构建电车难题道德分类评估框架，针对各人口统计学因素施加仅偏向群体不同的方向相反情境线索，系统测量模型的定向响应变化。

Result: 1. 情境线索显著改变决策，即使表面无关；2. 基线偏好无法预测定向可操纵性，模型可表现中立却存在系统性偏移不对称；3. 影响可能适得其反：模型声称中立或忽略线索，但选择仍发生偏移，有时甚至反向；4. 推理能力降低平均敏感性，但会放大有偏few-shot示例的效应。

Conclusion: 该研究揭示了LLMs道德决策的情境脆弱性，强调必须将受控的方向翻转情境操纵纳入道德评估框架，以更全面地表征模型行为并识别潜在风险。

Abstract: Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape decisions in trolley-problem-style moral triage settings. We introduce a pilot evaluation harness for directed contextual influence in trolley-problem-style moral triage: for each demographic factor, we apply matched, direction-flipped contextual influences that differ only in which group they favor, enabling systematic measurement of directional response. We find that: (i) contextual influences often significantly shift decisions, even when only superficially relevant; (ii) baseline preferences are a poor predictor of directional steerability, as models can appear baseline-neutral yet exhibit systematic steerability asymmetry under influence; (iii) influences can backfire: models may explicitly claim neutrality or discount the contextual cue, yet their choices still shift, sometimes in the opposite direction; and (iv) reasoning reduces average sensitivity, but amplifies the effect of biased few-shot examples. Our findings motivate extending moral evaluations with controlled, direction-flipped context manipulations to better characterize model behavior.

</details>


### [111] [Tackling Privacy Heterogeneity in Differentially Private Federated Learning](https://arxiv.org/abs/2602.22633)
*Ruichen Xu,Ying-Jun Angela Zhang,Jianwei Huang*

Main category: cs.LG

TL;DR: 该论文首次系统研究差分隐私联邦学习中的隐私感知客户端选择问题，通过理论分析和凸优化方法自适应调整选择概率，在异构隐私预算下实现CIFAR-10测试准确率最高10%的提升。


<details>
  <summary>Details</summary>
Motivation: 现有DP-FL假设统一的隐私预算，但现实场景隐私需求差异显著。隐私异构性导致传统基于数据量的客户端选择无法区分高质量更新与强隐私噪声更新，严重影响模型性能。

Method: 推导隐私异构性对训练误差的收敛性分析，并构建隐私感知客户端选择的凸优化模型，自适应调整选择概率以最小化训练误差。

Result: 在基准数据集上的实验表明，与现有基线相比，该方法在异构隐私预算设置下可使CIFAR-10测试准确率提升最高10%。

Conclusion: 研究验证了在联邦学习中考虑隐私异构性的重要性，为设计实用高效的DP-FL系统提供了理论依据和方法创新。

Abstract: Differentially private federated learning (DP-FL) enables clients to collaboratively train machine learning models while preserving the privacy of their local data. However, most existing DP-FL approaches assume that all clients share a uniform privacy budget, an assumption that does not hold in real-world scenarios where privacy requirements vary widely. This privacy heterogeneity poses a significant challenge: conventional client selection strategies, which typically rely on data quantity, cannot distinguish between clients providing high-quality updates and those introducing substantial noise due to strict privacy constraints. To address this gap, we present the first systematic study of privacy-aware client selection in DP-FL. We establish a theoretical foundation by deriving a convergence analysis that quantifies the impact of privacy heterogeneity on training error. Building on this analysis, we propose a privacy-aware client selection strategy, formulated as a convex optimization problem, that adaptively adjusts selection probabilities to minimize training error. Extensive experiments on benchmark datasets demonstrate that our approach achieves up to a 10% improvement in test accuracy on CIFAR-10 compared to existing baselines under heterogeneous privacy budgets. These results highlight the importance of incorporating privacy heterogeneity into client selection for practical and effective federated learning.

</details>


### [112] [NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion](https://arxiv.org/abs/2602.22911)
*Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 本文针对LoRA在复杂推理任务中的线性瓶颈问题，提出了NoRA（非线性秩适应）方法，通过引入SiLU门控和结构丢弃机制，在SlimOrca和MathInstruct基准测试中显著超越LoRA，并在秩64时达到LoRA秩512的性能水平。


<details>
  <summary>Details</summary>
Motivation: LoRA作为参数高效微调的主流方法，在复杂推理任务中存在"线性天花板"问题：由于固有的线性约束，单纯增加秩维度只能带来边际效益。这种线性限制导致模型性能在达到饱和点后难以进一步提升，需要突破线性结构的创新方法。

Method: NoRA提出了一种权重级并行适配器架构，通过在LoRA的线性变换中注入SiLU激活函数门控机制和结构丢弃（structural dropout）操作，引入非线性因素，从而在流形层面实现维度扩展，打破线性约束。

Result: 在SlimOrca基准测试上，秩64的NoRA（困惑度3.89）优于秩512的LoRA（困惑度3.90）；在MathInstruct数学推理任务中，NoRA达到困惑度1.97，显著超越LoRA的饱和点2.07。奇异值分解分析证实NoRA能激活奇异值谱的尾部区域，有效防止了线性方法中观察到的秩崩溃现象。

Conclusion: NoRA成功突破了LoRA的线性瓶颈，通过引入非线性机制显著提升了参数效率，在更低秩配置下实现更优性能，为复杂推理任务的参数高效微调提供了新方向。

Abstract: Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA's saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods.

</details>


### [113] [Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning](https://arxiv.org/abs/2602.22642)
*Qin-Wen Luo,Sheng Ren,Xiang Chen,Rui Liu,Jun Fang,Naiqiang Tan,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: 针对CoT推理的冗长问题，本文提出CEEH方法，通过难度感知的动态熵正则化，在简单问题上压缩回答，在困难问题上保持探索空间，从而在减少长度的同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有压缩方法在缩短推理步骤时会牺牲推理能力，特别是对于需要复杂推理的困难问题。研究发现显式优化短轨迹会导致熵崩溃，过早缩小探索空间。为此需要一种能根据问题难度动态调整压缩策略的方法。

Method: CEEH采用难度感知的强化学习方法：1) 动态评估问题难度；2) 对困难问题保持熵正则化以维持探索多样性，对简单问题进行激进压缩；3) 引入基于历史最短正确答案的动态最优长度惩罚，稳定奖励信号。

Result: 在六个推理基准测试上，CEEH在保持与基础模型相当准确率的同时持续减少回答长度，相比仅优化长度的方法显著提升了Pass@k指标。

Conclusion: CEEH通过难度感知的选择性熵正则化和动态长度惩罚，有效解决了推理压缩中的熵崩溃问题，在保持推理性能的前提下显著提升了推理效率。

Abstract: Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.

</details>


### [114] [MUG: Meta-path-aware Universal Heterogeneous Graph Pre-Training](https://arxiv.org/abs/2602.22645)
*Lianze Shan,Jitao Zhao,Dongxiao He,Yongqi Huang,Zhiyong Feng,Weixiong Zhang*

Main category: cs.LG

TL;DR: 本文提出MUG异构图通用预训练框架，通过输入统一模块和维度感知编码器解决异构图语义与结构多样性难题，在多个元路径视图上训练共享编码器，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有通用图预训练局限于同质图，异构图因节点/关系类型多样、元路径语义跨数据集差异大，难以构建统一表示空间与通用编码策略，亟需探索异构图通用预训练方案。

Method: 提出MUG方法：设计输入统一模块融合多类型节点和关系信息，经维度感知编码器投影至共享空间；在多个元路径视图上训练共享编码器捕获跨图结构模式，配合全局目标函数提升判别性并降低数据集偏差。

Result: 在真实数据集上的实验验证了MUG的有效性。

Conclusion: 该工作填补了异构图通用预训练的研究空白，为异构图跨任务、跨数据集表示学习提供了有效框架。

Abstract: Universal graph pre-training has emerged as a key paradigm in graph representation learning, offering a promising way to train encoders to learn transferable representations from unlabeled graphs and to effectively generalize across a wide range of downstream tasks. However, recent explorations in universal graph pre-training primarily focus on homogeneous graphs and it remains unexplored for heterogeneous graphs, which exhibit greater structural and semantic complexity. This heterogeneity makes it highly challenging to train a universal encoder for diverse heterogeneous graphs: (i) the diverse types with dataset-specific semantics hinder the construction of a unified representation space; (ii) the number and semantics of meta-paths vary across datasets, making encoding and aggregation patterns learned from one dataset difficult to apply to others. To address these challenges, we propose a novel Meta-path-aware Universal heterogeneous Graph pre-training (MUG) approach. Specifically, for challenge (i), MUG introduces a input unification module that integrates information from multiple node and relation types within each heterogeneous graph into a unified representation.This representation is then projected into a shared space by a dimension-aware encoder, enabling alignment across graphs with diverse schemas.Furthermore, for challenge (ii), MUG trains a shared encoder to capture consistent structural patterns across diverse meta-path views rather than relying on dataset-specific aggregation strategies, while a global objective encourages discriminability and reduces dataset-specific biases. Extensive experiments demonstrate the effectiveness of MUG on some real datasets.

</details>


### [115] [LEDA: Latent Semantic Distribution Alignment for Multi-domain Graph Pre-training](https://arxiv.org/abs/2602.22660)
*Lianze Shan,Jitao Zhao,Dongxiao He,Siqi Liu,Jiaxu Cui,Weixiong Zhang*

Main category: cs.LG

TL;DR: 本文针对通用图预训练中的语义对齐和训练指导不足问题，提出了一种新颖的潜在语义分布对齐（LEDA）模型，通过维度投影单元和变分语义推理模块实现跨域语义学习，并在少样本跨域设置中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着GPT和DeepSeek等通用大模型的发展，通用图预训练旨在跨领域学习丰富的可泛化知识。然而，现有方法面临两个主要挑战：1) 简单化的数据对齐无法正确对齐高度多样化图数据的语义，误导预训练模型；2) 将域内预训练范式任意应用于跨域场景导致训练指导有限，难以从多个图中捕获有效知识。

Method: 提出的LEDA模型包含两个核心组件：1) 维度投影单元，通过自适应方式将不同领域特征对齐到共享语义空间，同时最小化信息损失；2) 变分语义推理模块，首先获得共享的潜在分布，然后用该分布指导领域投影，实现跨域语义对齐和学习。

Result: 实验表明，LEDA在广泛的图数据和下游任务中表现出强劲性能。特别是在少样本跨域设置下，它显著优于域内基线方法和先进的通用预训练模型。

Conclusion: LEDA模型通过创新的潜在语义分布对齐机制，有效解决了通用图预训练中的语义对齐和训练指导问题，为跨领域图表示学习提供了新的解决方案。

Abstract: Recent advances in generic large models, such as GPT and DeepSeek, have motivated the introduction of universality to graph pre-training, aiming to learn rich and generalizable knowledge across diverse domains using graph representations to improve performance in various downstream applications. However, most existing methods face challenges in learning effective knowledge from generic graphs, primarily due to simplistic data alignment and limited training guidance. The issue of simplistic data alignment arises from the use of a straightforward unification for highly diverse graph data, which fails to align semantics and misleads pre-training models. The problem with limited training guidance lies in the arbitrary application of in-domain pre-training paradigms to cross-domain scenarios. While it is effective in enhancing discriminative representation in one data space, it struggles to capture effective knowledge from many graphs. To address these challenges, we propose a novel Latent sEmantic Distribution Alignment (LEDA) model for universal graph pre-training. Specifically, we first introduce a dimension projection unit to adaptively align diverse domain features into a shared semantic space with minimal information loss. Furthermore, we design a variational semantic inference module to obtain the shared latent distribution. The distribution is then adopted to guide the domain projection, aligning it with shared semantics across domains and ensuring cross-domain semantic learning. LEDA exhibits strong performance across a broad range of graphs and downstream tasks. Remarkably, in few-shot cross-domain settings, it significantly outperforms in-domain baselines and advanced universal pre-training models.

</details>


### [116] [Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support](https://arxiv.org/abs/2602.22673)
*Md Tanvir Hasan Turja*

Main category: cs.LG

TL;DR: 本研究开发了一个双组件框架，用于预测抗生素耐药性(AMR)趋势并提供循证政策支持。基于WHO GLASS数据库6个区域5909条数据(2021-2023)，XGBoost模型表现最佳(MAE 7.07%，R² 0.854)，较基线提升83.1%。同时构建RAG系统，结合ChromaDB向量库与Phi-3 Mini模型，提供可溯源的政策建议。


<details>
  <summary>Details</summary>
Motivation: 抗生素耐药性(AMR)是全球性卫生危机，预计2050年每年导致1000万人死亡。WHO GLASS系统虽收集了44国标准化监测数据，但缺乏机器学习驱动的人群级耐药趋势预测研究。现有方法难以充分利用该数据预测未来耐药性并辅助政策决策，亟需开发整合预测与政策支持的综合框架。

Method: 研究设计双组件框架：1) AMR预测模块：在WHO GLASS 5909条跨6区域观测数据(2021-2023)上，对比Naive、线性回归、岭回归、XGBoost、LightGBM和LSTM六种模型性能；2) 政策决策模块：构建检索增强生成(RAG)流水线，将WHO政策文档向量化存储于ChromaDB，连接本地部署的Phi-3 Mini语言模型，生成可归因于来源且抗幻觉的政策答案。

Result: XGBoost取得最优性能：测试MAE 7.07%，R² 0.854，超越基线83.1%。特征重要性显示前一年耐药率是主导因子(50.5%重要性)。区域MAE差异显著：欧洲区域最低(4.16%)，东南亚区域最高(10.14%)。RAG系统成功实现基于证据的政策问答。

Conclusion: 该研究验证了机器学习在AMR趋势预测中的可行性，XGBoost尤其适合该时序预测任务。前一年耐药率的主导作用证实耐药性具有强自相关性。区域性能差异反映了流行病学特征和监测数据质量的异质性。RAG政策系统为循证决策提供了新范式，增强了政策制定的科学性。整体框架为应对AMR危机提供了可扩展的预测与决策支持工具，具有重要公共卫生价值。

Abstract: Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja

</details>


### [117] [Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement](https://arxiv.org/abs/2602.22681)
*Shuchen Zhu,Rizhen Hu,Mingze Wang,Mou Sun,Xue Wang,Kun Yuan,Zaiwen Wen*

Main category: cs.LG

TL;DR: 针对大语言模型预训练优化效率问题，本文从Riemannian ODE视角分析自适应优化器，提出LITE加速策略，通过沿平坦方向增大Hessian阻尼和学习率，显著提升了Muon和SOAP在多种架构、参数量和数据集上的训练速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练消耗巨大算力，优化器效率是关键瓶颈。现有矩阵优化器Muon和SOAP虽利用精细曲率信息超越AdamW，但其更新趋向各向同性，导致平坦方向更新过于保守而尖锐方向可能过于激进，无法充分利用anisotropic优化地形。

Method: 构建统一Riemannian常微分方程框架，揭示预调节器建立Riemannian几何以改善病态条件、动量作为Riemannian阻尼项促进收敛的协同机制。基于此设计LITE通用加速策略，在平坦轨迹上应用更大的Hessian阻尼系数和学习率。

Result: 在Dense/MoE架构、130M-1.3B参数量、C4/Pile数据集及不同学习率调度下，LITE显著加速Muon和SOAP训练。理论分析证明LITE在anisotropic landscape中沿平坦方向收敛更快。

Conclusion: LITE为LLM预训练提供了基于anisotropic landscape特性的原则性加速方案，在保持优化稳定性的同时大幅提升训练效率，开源代码促进后续研究。

Abstract: Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.

</details>


### [118] [Switch-Hurdle: A MoE Encoder with AR Hurdle Decoder for Intermittent Demand Forecasting](https://arxiv.org/abs/2602.22685)
*Fabian Muşat,Simona Căbuz*

Main category: cs.LG

TL;DR: 本文提出Switch-Hurdle框架，通过混合专家编码器与门槛解码器分离建模销售发生概率和数量，在M5基准与零售数据集上实现SOTA性能且保持可扩展性。


<details>
  <summary>Details</summary>
Motivation: 间歇性需求（长期零销售穿插 sporadic 非零值）对零售与供应链预测构成持续挑战。传统方法（ARIMA、指数平滑、Croston变体）与现代神经架构（DeepAR、Transformer）在处理此类数据时表现不佳，前者将需求视为连续过程，后者在扩展到大量稀疏序列时计算成本高昂。

Method: 提出Switch-Hurdle框架：1）编码器采用Top-1稀疏专家路由（前向传播）并通过直通估计器实现近似密集反向传播；2）解码器采用交叉注意力自回归设计，共享门槛头将预测任务分解为二元分类（销售概率）和条件回归（给定销售时的数量）两个组件。

Result: 在M5基准与大型专有零售数据集上的实证结果表明，Switch-Hurdle在保持可扩展性的同时实现了最先进的预测性能。

Conclusion: 该框架通过结构化分离捕捉间歇性需求的发生与幅度过程，为零售与供应链预测提供了可扩展的高性能解决方案。

Abstract: Intermittent demand, a pattern characterized by long sequences of zero sales punctuated by sporadic, non-zero values, poses a persistent challenge in retail and supply chain forecasting. Both traditional methods, such as ARIMA, exponential smoothing, or Croston variants, as well as modern neural architectures such as DeepAR and Transformer-based models often underperform on such data, as they treat demand as a single continuous process or become computationally expensive when scaled across many sparse series. To address these limitations, we introduce Switch-Hurdle: a new framework that integrates a Mixture-of-Experts (MoE) encoder with a Hurdle-based probabilistic decoder. The encoder uses a sparse Top-1 expert routing during the forward pass yet approximately dense in the backward pass via a straight-through estimator (STE). The decoder follows a cross-attention autoregressive design with a shared hurdle head that explicitly separates the forecasting task into two components: a binary classification component estimating the probability of a sale, and a conditional regression component, predicting the quantity given a sale. This structured separation enables the model to capture both occurrence and magnitude processes inherent to intermittent demand. Empirical results on the M5 benchmark and a large proprietary retail dataset show that Switch-Hurdle achieves state-of-the-art prediction performance while maintaining scalability.

</details>


### [119] [Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning](https://arxiv.org/abs/2602.22703)
*Hao Yu,Shuning Jia,Guanghao Li,Wenhao Jiang,Chun Yuan*

Main category: cs.LG

TL;DR: 本文针对视觉语言模型(VLMs)几何感知能力不足的问题，提出GeoPerceive基准与GeoDPO强化学习框架。GeoPerceive通过自动生成带DSL标注的图表数据实现几何感知的独立评估；GeoDPO利用NL-to-DSL翻译器提供细粒度奖励，显著提升模型性能，在域内、域外及下游推理任务上分别获得26.5%、8.0%和39.0%的增益，大幅优于监督微调。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在几何推理任务中表现受限，核心瓶颈在于对基础图表元素的感知能力不足。现有方法难以将几何感知与推理过程解耦评估，且监督微调在域外泛化方面效果有限甚至产生负迁移，亟需更有效的训练范式。

Method: 提出两大核心组件：1) GeoPerceive基准，构建包含图表实例与对应领域特定语言(DSL)表示的数据集，配备自动化生成管道，支持几何感知能力的独立评测；2) GeoDPO框架，采用翻译器引导的强化学习策略，训练自然语言到DSL的翻译器作为奖励模型，提供细粒度的DSL级别评分以优化VLM的几何感知能力。

Result: GeoDPO在各项评估中显著优于监督微调(SFT)：域内几何感知任务提升26.5%，域外泛化任务提升8.0%，下游推理任务大幅提升39.0%。SFT仅带来边际改进且可能在域外场景损害性能，充分验证了GeoDPO在性能增益与泛化能力上的双重优势。

Conclusion: GeoPerceive与GeoDPO为突破VLMs几何感知瓶颈提供了有效解决方案。通过引入DSL作为中间表示并构建相应奖励机制，GeoDPO实现了对几何感知能力的精准优化，在各类任务上展现出卓越的增益效果，特别是大幅提升了模型的泛化能力，为视觉语言模型的几何理解研究开辟了新方向。

Abstract: Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\%$ on in-domain data, $+8.0\%$ on out-of-domain data, and $+39.0\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive
  to ensure reproducibility.

</details>


### [120] [Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching](https://arxiv.org/abs/2602.22812)
*Hiroki Matsutani,Naoki Matsuda,Naoto Sugiura*

Main category: cs.LG

TL;DR: 本文针对资源受限边缘设备本地LLM推理的性能瓶颈，提出一种分布式提示缓存方法。该方法通过设备间协同共享中间状态并引入Bloom过滤器目录结构抑制不必要的无线通信，最终在树莓派Zero 2W平台上实现了首令牌生成时间(TTFT)平均降低93.12%、总生成时间(TTLT)平均降低50.07%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 本地大语言模型推理在资源受限的边缘设备上存在严重的性能瓶颈，传统方法难以满足实时性需求。通过跨设备协同共享中间处理状态，有望显著提升推理效率。

Method: 设计分布式提示缓存机制，利用提示相似性实现部分匹配，并采用基于Bloom过滤器的高效目录数据结构，在无线通信环境中智能判断远程设备是否持有目标状态，从而避免不必要的数据传输。

Result: 在树莓派Zero 2W硬件平台上，采用Gemma-3 270M模型与MMLU基准数据集的实验验证表明，所提方法相较基线方案可平均降低首令牌生成时间(TTFT)达93.12%，降低总生成时间(TTLT)达50.07%。

Conclusion: 分布式提示缓存机制结合Bloom过滤器目录结构，能够有效缓解边缘设备LLM推理的性能瓶颈，大幅降低生成延迟，为资源受限环境下的高效本地化大模型推理提供了切实可行的技术路径。

Abstract: Since local LLM inference on resource-constrained edge devices imposes a severe performance bottleneck, this paper proposes distributed prompt caching to enhance inference performance by cooperatively sharing intermediate processing states across multiple low-end edge devices. To fully utilize prompt similarity, our distributed caching mechanism also supports partial matching. As this approach introduces communication overhead associated with state sharing over a wireless network, we introduce a Bloom-filter-based data structure, referred to as a catalog, to determine whether a remote server possesses the desired internal states, thereby suppressing unnecessary communication. Experiments using the Gemma-3 270M model and the MMLU dataset on the Raspberry Pi Zero 2W platform demonstrate that the proposed approach reduces TTFT (Time to First Token) and TTLT (Time to Last Token) by 93.12% and 50.07% on average, respectively.

</details>


### [121] [Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2602.22817)
*Shuo He,Lang Feng,Qi Wei,Xin Cheng,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: 该论文发现步骤级群体强化学习中存在历史上下文不一致问题，导致优势估计严重偏差。为此提出HGPO方法，通过将每个步骤分配到多个层次化群体并自适应加权聚合优势，在不增加计算开销的情况下实现更优的偏差-方差权衡，在ALFWorld和WebShop两个挑战性智能体任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前步骤级群体策略优化方法在长时程智能体任务中存在关键缺陷：同组内的不同步骤可能具有不同的历史上下文（上下文不一致），这导致优势估计出现严重偏差并显著降低策略优化效果。为此，作者提出需要解决这一上下文不一致问题。

Method: HGPO（层次化群体策略优化）方法：1）在轨迹群体内，根据历史上下文的一致性将每个步骤分配到多个层次化子群体；2）对每个步骤分别在各个子群体中计算相对优势；3）采用自适应加权方案聚合不同层次的优势值，从而实现步骤级优势估计的偏差-方差权衡优化，且无需额外模型或采样。

Result: 在ALFWorld和WebShop两个具有挑战性的智能体任务上，使用Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct模型进行评测，HGPO在相同计算约束下显著优于现有智能体强化学习方法，证明了其有效性和计算效率。

Conclusion: HGPO通过层次化分组和自适应加权聚合，有效解决了步骤级优势估计中的上下文不一致问题，实现了偏差-方差权衡的优化，为长时程智能体任务提供了一种高效且性能优越的策略优化方案。

Abstract: Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.

</details>


### [122] [MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction](https://arxiv.org/abs/2602.22850)
*Yi He,Yina Cao,Jixiu Zhai,Di Wang,Junxiao Kong,Tianchi Lu*

Main category: cs.LG

TL;DR: 本研究开发了可解释的深度学习模型MEDNA-DFM及信号纯化算法，用于DNA甲基化识别。该模型跨物种有效，揭示了保守基序（如GC含量）驱动泛化能力，并通过果蝇6mA研究提出"序列-结构协同"假说，为表观遗传调控研究提供了新工具和机制洞察。


<details>
  <summary>Details</summary>
Motivation: DNA甲基化准确识别对理解表观遗传调控至关重要，但现有深度学习方法虽性能优异却如"黑箱"，阻碍了生物学洞见的获取，亟需开发兼具高性能与机制解释性的计算方法。

Method: 提出高性能深度学习模型MEDNA-DFM和机制启发的信号纯化算法；通过跨物种验证和外部独立数据集测试模型泛化能力；利用算法提取高可靠性基序；结合果蝇6mA案例进行计算机突变分析验证序列-结构协同假说。

Result: MEDNA-DFM能有效捕获保守甲基化模式，实现跨物种稳健区分；模型泛化由保守内在基序（如GC含量）驱动，而非系统发育亲缘关系；新算法提取的基序可靠性显著高于先前研究；果蝇6mA研究揭示GAGG核心基序与上游A-tract元件协同作用，计算机突变验证表明任一或两者同时缺失会显著降低模型识别能力。

Conclusion: 该工作提供了强大的甲基化预测工具，并证明了可解释深度学习既能推动方法学创新，又能催生生物学假说，为表观遗传学研究提供了新范式。

Abstract: Accurate computational identification of DNA methylation is essential for understanding epigenetic regulation. Although deep learning excels in this binary classification task, its "black-box" nature impedes biological insight. We address this by introducing a high-performance model MEDNA-DFM, alongside mechanism-inspired signal purification algorithms. Our investigation demonstrates that MEDNA-DFM effectively captures conserved methylation patterns, achieving robust distinction across diverse species. Validation on external independent datasets confirms that the model's generalization is driven by conserved intrinsic motifs (e.g., GC content) rather than phylogenetic proximity. Furthermore, applying our developed algorithms extracted motifs with significantly higher reliability than prior studies. Finally, empirical evidence from a Drosophila 6mA case study prompted us to propose a "sequence-structure synergy" hypothesis, suggesting that the GAGG core motif and an upstream A-tract element function cooperatively. We further validated this hypothesis via in silico mutagenesis, confirming that the ablation of either or both elements significantly degrades the model's recognition capabilities. This work provides a powerful tool for methylation prediction and demonstrates how explainable deep learning can drive both methodological innovation and the generation of biological hypotheses.

</details>


### [123] [Fair feature attribution for multi-output prediction: a Shapley-based perspective](https://arxiv.org/abs/2602.22882)
*Umberto Biccari,Alain Ibáñez de Opakua,José María Mato,Óscar Millet,Roberto Morales,Enrique Zuazua*

Main category: cs.LG

TL;DR: 本文在Shapley框架下对多输出预测器的特征归因进行公理化表征，通过将经典Shapley公理扩展至向量值合作博弈，建立刚性定理证明满足效率性、对称性、虚拟参与者和可加性的归因规则必然按输出分量分解，从而揭示联合输出归因必须放宽至少一个公理的理论约束。


<details>
  <summary>Details</summary>
Motivation: 尽管SHAP解释在多输出预测器中通常独立计算每个输出坐标，但这种做法的理论必要性尚不明确，存在未形式化的结构约束问题。

Method: 将经典Shapley公理扩展到向量值合作博弈，构建多输出特征归因的公理体系。

Result: 建立刚性定理，证明任何同时满足效率性、对称性、虚拟参与者和可加性的归因规则必然按分量分解；因此，任何联合输出归因规则必须至少放宽一个经典Shapley公理。

Conclusion: 该结果明确了Shapley可解释性中先前未形式化的结构约束，阐明了多输出学习中公平一致解释的精确范围；生物医学基准实验表明，多输出模型可在训练和部署时节省计算成本，同时产生的SHAP解释与Shapley公理强制的分量结构完全一致。

Abstract: In this article, we provide an axiomatic characterization of feature attribution for multi-output predictors within the Shapley framework. While SHAP explanations are routinely computed independently for each output coordinate, the theoretical necessity of this practice has remained unclear. By extending the classical Shapley axioms to vector-valued cooperative games, we establish a rigidity theorem showing that any attribution rule satisfying efficiency, symmetry, dummy player, and additivity must necessarily decompose component-wise across outputs. Consequently, any joint-output attribution rule must relax at least one of the classical Shapley axioms. This result identifies a previously unformalized structural constraint in Shapley-based interpretability, clarifying the precise scope of fairness-consistent explanations in multi-output learning. Numerical experiments on a biomedical benchmark illustrate that multi-output models can yield computational savings in training and deployment, while producing SHAP explanations that remain fully consistent with the component-wise structure imposed by the Shapley axioms.

</details>


### [124] [Scaling Laws of Global Weather Models](https://arxiv.org/abs/2602.22962)
*Yuejiang Yu,Langwen Huang,Alexandru Calotoiu,Torsten Hoefler*

Main category: cs.LG

TL;DR: 本研究探究数据驱动天气预测模型的缩放规律，揭示模型性能与参数量、数据规模和计算预算的关系。发现Aurora数据扩展性最优，GraphCast参数效率最高但硬件利用率有限，天气模型偏好宽度扩展而非深度，建议未来采用更宽架构与更大训练集。


<details>
  <summary>Details</summary>
Motivation: 数据驱动天气预报虽快速发展，但模型训练资源分配缺乏理论指导。通过建立经验缩放定律，可科学优化模型设计，避免计算资源浪费，提升预测性能。

Method: 采用实证分析方法，系统研究不同模型规模(N)、训练数据量(D)和计算预算(C)对验证损失的影响，并对比分析Aurora、GraphCast等主流架构的形状缩放特性。

Result: Aurora呈现最强数据缩放：数据量增加10倍，验证损失最高降低3.2倍；GraphCast参数效率最优，但硬件利用率受限；固定预算下，延长训练比扩大模型规模更有效；天气模型缩放偏好与语言模型相反：宽度优先于深度。

Conclusion: 未来天气预测模型应优先扩展模型宽度并扩大有效训练数据集，而非单纯增加深度。这为模型架构设计和训练策略优化提供了关键指导方向。

Abstract: Data-driven models are revolutionizing weather forecasting. To optimize training efficiency and model performance, this paper analyzes empirical scaling laws within this domain. We investigate the relationship between model performance (validation loss) and three key factors: model size ($N$), dataset size ($D$), and compute budget ($C$). Across a range of models, we find that Aurora exhibits the strongest data-scaling behavior: increasing the training dataset by 10x reduces validation loss by up to 3.2x. GraphCast demonstrates the highest parameter efficiency, yet suffers from limited hardware utilization. Our compute-optimal analysis indicates that, under fixed compute budgets, allocating resources to longer training durations yields greater performance gains than increasing model size. Furthermore, we analyze model shape and uncover scaling behaviors that differ fundamentally from those observed in language models: weather forecasting models consistently favor increased width over depth. These findings suggest that future weather models should prioritize wider architectures and larger effective training datasets to maximize predictive performance.

</details>


### [125] [Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization](https://arxiv.org/abs/2602.23008)
*Zeyuan Liu,Jeonghye Kim,Xufang Luo,Dongsheng Li,Yuqing Yang*

Main category: cs.LG

TL;DR: 针对强化学习训练大语言模型智能体时的探索瓶颈问题，本文提出EMPO²混合框架，通过记忆增强和混合策略优化，在ScienceWorld和WebShop上分别取得128.6%和11.3%的性能提升，显著提升智能体的探索能力和分布外泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的大语言模型智能体在探索新状态方面存在瓶颈，尤其在需要发现未知环境状态的任务中表现受限，而现有方法未能有效解决这一探索难题。

Method: 提出探索性记忆增强型策略内/策略外优化框架（EMPO²），采用混合强化学习方法：利用记忆机制促进探索，同时结合策略内（on-policy）和策略外（off-policy）更新，使大语言模型在具备记忆时表现优异，无记忆时仍保持鲁棒性。

Result: 在ScienceWorld和WebShop基准测试中，EMPO²较GRPO基线分别提升128.6%和11.3%的性能；在分布外测试中展现出卓越适应性，仅需少量记忆试验且无需参数更新即可适应新任务。

Conclusion: EMPO²框架通过记忆增强和混合策略优化的有效结合，为构建更具探索性和泛化能力的大语言模型智能体提供了有前景的解决方案，显著提升了智能体在未知环境中的适应能力。

Abstract: Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.

</details>


### [126] [Learning Disease-Sensitive Latent Interaction Graphs From Noisy Cardiac Flow Measurements](https://arxiv.org/abs/2602.23035)
*Viraj Patel,Marko Grujic,Philipp Aigner,Theodor Abart,Marcus Granegger,Deblina Bhattacharjee,Katharine Fraser*

Main category: cs.LG

TL;DR: 该研究提出了一种物理信息引导的潜在关系图框架，将心脏涡流建模为图中的交互节点，结合神经关系推理架构与物理启发的相互作用能量和生死动力学。通过在主动脉缩窄的血流动力学模拟和左心室辅助装置支持的超声数据上的应用，发现潜在图熵与疾病严重程度单调相关（R²=0.78），且该框架具有跨模态泛化能力，为心血管疾病提供了可解释的定量标记。


<details>
  <summary>Details</summary>
Motivation: 心脏血流模式蕴含丰富的疾病严重程度和临床干预信息，但现有成像和计算方法无法捕捉到相干流特征的底层关系结构，难以提取具有临床意义的相互作用特征。

Method: 提出一种物理信息引导的潜在关系图框架，将心脏涡流视为图中的交互节点。该方法融合神经关系推理架构、物理启发的相互作用能量以及涡流生死动力学，构建对疾病严重程度和干预水平敏感的潜在图表示。研究分别应用于主动脉缩窄的计算流体力学模拟和不同左心室辅助装置支持强度下的左心室超声影像数据集。

Result: 在主动脉缩窄模型中，随着主动脉半径变窄，涡流相互作用强度和频率显著增加，导致图熵值升高，与缩窄严重程度呈强单调相关性（R²=0.78，Spearman |ρ|=0.96）。在左心室辅助装置数据中，潜在图成功捕捉到随支持水平增强而减弱的相干涡流结构，验证了跨模态泛化能力。研究表明，潜在相互作用图和熵可作为稳健且可解释的心脏疾病与干预标记物。

Conclusion: 该物理信息引导的潜在关系图框架有效量化了心脏血流中的隐藏关系结构，其潜在图熵特征为心血管疾病严重程度和临床干预效果提供了敏感、可解释的定量评估工具，展现出良好的临床应用前景。

Abstract: Cardiac blood flow patterns contain rich information about disease severity and clinical interventions, yet current imaging and computational methods fail to capture underlying relational structures of coherent flow features. We propose a physics-informed, latent relational framework to model cardiac vortices as interacting nodes in a graph. Our model combines a neural relational inference architecture with physics-inspired interaction energy and birth-death dynamics, yielding a latent graph sensitive to disease severity and intervention level. We first apply this to computational fluid dynamics simulations of aortic coarctation. Learned latent graphs reveal that as the aortic radius narrows, vortex interactions become stronger and more frequent. This leads to a higher graph entropy, correlating monotonically with coarctation severity ($R^2=0.78$, Spearman $|ρ|=0.96$). We then extend this method to ultrasound datasets of left ventricles under varying levels of left ventricular assist device support. Again the latent graph representation captures the weakening of coherent vortical structures, thereby demonstrating cross-modal generalisation. Results show latent interaction graphs and entropy serve as robust and interpretable markers of cardiac disease and intervention.

</details>


### [127] [RhythmBERT: A Self-Supervised Language Model Based on Latent Representations of ECG Waveforms for Heart Disease Detection](https://arxiv.org/abs/2602.23060)
*Xin Wang,Burcu Ozek,Aruna Mohan,Amirhossein Ravari,Or Zilbershot,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 提出RhythmBERT，一种将ECG视为语言范式的生成式模型，通过自编码器将P、QRS、T段编码为符号token，融合离散节律语义与连续形态特征，在80万条无标签ECG上预训练，单导联性能媲美或超越12导联基线，为心脏病诊断提供生理对齐的可扩展路径。


<details>
  <summary>Details</summary>
Motivation: 现有ECG自监督学习方法将ECG视为通用时间序列，忽略生理语义和节律结构；对比学习的数据增强扭曲形态，生成式方法的固定窗口分割不对齐心动周期，导致心电图分析未能充分利用其结构化特性。

Method: 将ECG波形视为语言，通过自编码器将P、QRS、T段编码为离散符号token以捕获节律语义，同时保留连续嵌入以保持精细形态特征；在约80万条无标签ECG记录上使用掩码预测目标进行预训练；采用单导联输入策略。

Result: 仅使用单导联时，RhythmBERT性能与强12导联基线相当或更优；泛化能力覆盖房颤等常见病到细微ST-T异常和心肌梗死等临床挑战性病例。

Conclusion: 将ECG视为结构化语言为推进心脏分析提供了一条可扩展且生理对齐的新路径，展示了语言模型在医学信号处理中的潜力。

Abstract: Electrocardiogram (ECG) analysis is crucial for diagnosing heart disease, but most self-supervised learning methods treat ECG as a generic time series, overlooking physiologic semantics and rhythm-level structure. Existing contrastive methods utilize augmentations that distort morphology, whereas generative approaches employ fixed-window segmentation, which misaligns cardiac cycles. To address these limitations, we propose RhythmBERT, a generative ECG language model that considers ECG as a language paradigm by encoding P, QRS, and T segments into symbolic tokens via autoencoder-based latent representations. These discrete tokens capture rhythm semantics, while complementary continuous embeddings retain fine-grained morphology, enabling a unified view of waveform structure and rhythm. RhythmBERT is pretrained on approximately 800,000 unlabeled ECG recordings with a masked prediction objective, allowing it to learn contextual representations in a label-efficient manner. Evaluations show that despite using only a single lead, RhythmBERT achieves comparable or superior performance to strong 12-lead baselines. This generalization extends from prevalent conditions such as atrial fibrillation to clinically challenging cases such as subtle ST-T abnormalities and myocardial infarction. Our results suggest that considering ECG as structured language offers a scalable and physiologically aligned pathway for advancing cardiac analysis.

</details>


### [128] [PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training](https://arxiv.org/abs/2602.23111)
*Yanyi Li,Yimu Zhang,Cong Fang*

Main category: cs.LG

TL;DR: 本文针对大批量LLM训练中激活值占据主要内存的问题，提出了PRAC激活压缩方法。该方法通过SVD捕获主信息子空间，并从正交补空间采样随机子空间来近似尾部信息，结合精确缩放因子，在保证无偏估计和最小方差的前提下，实现了高达36%的内存节省，且性能损失和计算开销极低。


<details>
  <summary>Details</summary>
Motivation: 在大批量大型语言模型（LLM）训练中，激活值成为主要的内存瓶颈。然而，现有压缩方法未能有效利用激活值的谱结构特征，导致训练收敛缓慢或压缩率有限。因此，亟需一种既能保持关键信息又能实现高压缩率的激活压缩方法。

Method: 本文提出了一种称为PRAC（Principal-Random Subspace for LLM Activation Compression）的激活压缩方法。该方法创新性地将激活值分解为两个部分：1）通过奇异值分解（SVD）捕获的主子空间，用于保留主导信息；2）从正交补空间采样的随机子空间，用于近似尾部信息。通过引入精确的缩放因子，并在理论上证明，在特定条件下PRAC能够产生无偏的梯度估计且方差最小。

Result: 在预训练和微调任务上的大量实验表明，PRAC能够实现高达36%的总内存减少，同时带来可忽略的性能下降和极低的计算开销。

Conclusion: PRAC通过结合主子空间和随机子空间，并辅以精确缩放，为LLM训练中的激活值压缩问题提供了一种高效且理论完备的解决方案，在内存受限的大批量训练场景中具有重要应用价值。

Abstract: Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal-Random Subspace for LLM Activation Compression (PRAC), which novelly decomposes activations into two components: a principal subspace captured via SVD to retain dominant information, and a random subspace sampled from the orthogonal complement to approximate the tail. By introducing a precise scaling factor, we prove that PRAC yields an unbiased gradient estimator with minimum variance under certain conditions. Extensive experiments on pre-training and fine-tuning tasks demonstrate that PRAC achieves up to 36% total memory reduction with negligible performance degradation and minimal computational cost.

</details>


### [129] [Regularized Online RLHF with Generalized Bilinear Preferences](https://arxiv.org/abs/2602.23116)
*Junghyun Lee,Minju Hong,Kwang-Sung Jun,Chulhee Yun,Se-Young Yun*

Main category: cs.LG

TL;DR: 该论文研究了带有一般偏好的上下文在线强化学习人类反馈(RLHF)问题，提出广义双线性偏好模型(GBPM)以处理非传递性偏好，并基于强凸正则化推导出贪心策略对偶间隙与估计误差平方之间的关系，设计了两种简单算法，分别实现Õ(ηd⁴(log T)²)和Õ(√(ηrT))的遗憾界，首次在高维情况下给出了统计高效的在线RLHF保证。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF研究局限于传递性偏好假设和反向KL正则化，无法处理现实世界中普遍存在的非传递性偏好和多样化的正则化需求。该研究旨在突破这些限制，建立更一般的理论框架，为高维在线RLHF提供统计高效的学习保证。

Method: 采用广义双线性偏好模型(GBPM)，利用低秩斜对称矩阵捕捉潜在的非传递性偏好结构；证明贪心策略的对偶间隙可由估计误差的平方上界控制，该结论仅依赖于强凸性和斜对称性；在特征多样性假设下，设计贪心采样和探索-然后-承诺两种算法。

Result: 1) 贪心采样算法实现不含e^O(η)因子的对数遗憾界Õ(ηd⁴(log T)²)；2) 探索-然后-承诺算法实现不含poly(d)因子的遗憾界Õ(√(ηrT))，首次在高维在线RLHF中获得统计效率保证；两种结果均适用于任意强凸正则化。

Conclusion: 该工作通过GBPM和强凸正则化框架，成功将在线RLHF推广至一般偏好情形，消除了对传递性假设和特定KL正则化的依赖，为高维复杂偏好学习提供了坚实的理论基础，推动了RLHF在实际系统中的可扩展性和适用性。

Abstract: We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $η^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is proving that the dual gap of the greedy policy is bounded by the square of the estimation error - a result derived solely from strong convexity and the skew-symmetricity of GBPM.Building on this insight and a feature diversity assumption, we establish two regret bounds via two simple algorithms: (1) Greedy Sampling achieves polylogarithmic, $e^{O(η)}$-free regret $\tilde{O}(ηd^4 (\log T)^2)$. (2) Explore-Then-Commit achieves $\mathrm{poly}(d)$-free regret $\tilde{O}(\sqrt{ηr T})$ by exploiting the low-rank structure; this is the first statistically efficient guarantee for online RLHF in high-dimensions.

</details>


### [130] [Prediction of Diffusion Coefficients in Mixtures with Tensor Completion](https://arxiv.org/abs/2602.23142)
*Zeno Romero,Kerstin Münnemann,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: 本研究提出了一种混合张量补全方法（TCM），用于预测二元混合物中无限稀释扩散系数的温度依赖性。该方法结合Tucker分解、贝叶斯框架和主动学习策略，利用有限实验数据实现268-378K范围内的准确预测，显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 扩散系数预测对许多应用至关重要，但实验数据稀缺。现有矩阵补全方法（MCM）局限于单温度预测，且精度高度依赖各温度下的高质量实验数据。这限制了其在温度相关预测中的实用性。

Method: 提出基于Tucker分解的张量补全方法（TCM），联合训练298K、313K和333K的实验数据；采用半经验SEGWE模型预测结果作为贝叶斯训练框架中的先验知识；通过主动学习策略指导PFG-NMR实验获取新数据，扩展包含19个溶质+溶剂体系的数据库。

Result: TCM在268-378K范围内实现线性外推，预测精度显著优于传统模型；加入主动学习获取的新数据后，模型精度进一步提升，验证了混合方法的优越性。

Conclusion: 结合数据高效的机器学习方法与自适应实验策略，可有效推进输运性质的预测建模，为扩散系数等物性预测提供了新范式。

Abstract: Predicting diffusion coefficients in mixtures is crucial for many applications, as experimental data remain scarce, and machine learning (ML) offers promising alternatives to established semi-empirical models. Among ML models, matrix completion methods (MCMs) have proven effective in predicting thermophysical properties, including diffusion coefficients in binary mixtures. However, MCMs are restricted to single-temperature predictions, and their accuracy depends strongly on the availability of high-quality experimental data for each temperature of interest. In this work, we address this challenge by presenting a hybrid tensor completion method (TCM) for predicting temperature-dependent diffusion coefficients at infinite dilution in binary mixtures. The TCM employs a Tucker decomposition and is jointly trained on experimental data for diffusion coefficients at infinite dilution in binary systems at 298 K, 313 K, and 333 K. Predictions from the semi-empirical SEGWE model serve as prior knowledge within a Bayesian training framework. The TCM then extrapolates linearly to any temperature between 268 K and 378 K, achieving markedly improved prediction accuracy compared to established models across all studied temperatures. To further enhance predictive performance, the experimental database was expanded using active learning (AL) strategies for targeted acquisition of new diffusion data by pulsed-field gradient (PFG) NMR measurements. Diffusion coefficients at infinite dilution in 19 solute + solvent systems were measured at 298 K, 313 K, and 333 K. Incorporating these results yields a substantial improvement in the TCM's predictive accuracy. These findings highlight the potential of combining data-efficient ML methods with adaptive experimentation to advance predictive modeling of transport properties.

</details>


### [131] [Partial recovery of meter-scale surface weather](https://arxiv.org/abs/2602.23146)
*Jonathan Giezendanner,Qidong Yang,Eric Schmitt,Anirban Chandra,Daniel Salles Civitarese,Johannes Jakubik,Jeremy Vila,Detlef Hohl,Campbell Watson,Sherrie Wang*

Main category: cs.LG

TL;DR: 该研究证明了利用现有观测数据，通过将粗分辨率大气状态与稀疏地面站点和高分辨率地球观测数据相结合，可以在美国大陆尺度上实现10米分辨率近地表气象场的可统计预测，显著改善风速和温湿度的预报误差，并揭示城市热岛、蒸散发驱动湿度对比等物理可解释结构。


<details>
  <summary>Details</summary>
Motivation: 当前天气分析和预报缺乏对几十到几百米尺度近地表大气变异的捕捉，这种变异源于下垫面和地形特征，但尚不清楚其是可混沌动力学不可约简还是可基于地表特征和大气强迫预测，因此需要开发新方法填补这一尺度上的预测空白。

Method: 通过将粗分辨率大气状态条件化于稀疏的地面站点观测和高分辨率地球观测数据，构建统计推断方法，生成美国本土连续的10米分辨率近地表风场、温度和湿度场。

Result: 相对于ERA5再分析数据，该方法将风速误差降低29%，温度和露点温度误差降低6%，在固定时间步长上解释了更多的空间方差，并展现出城市热岛、蒸散发驱动的湿度差异和不同下垫面风速差异等物理可解释结构。

Conclusion: 该研究通过计算可行的方法实现了大陆尺度的米分辨率推断，拓展了天气预报边界，并更广泛地展示了基于精细尺度特征条件化粗分辨率动力模型，可揭示地球系统中先前未解析的组成部分。

Abstract: Near-surface atmospheric conditions can differ sharply over tens to hundreds of meters due to land cover and topography, yet this variability is absent from current weather analyses and forecasts. It is unclear whether such meter-scale variability reflects irreducibly chaotic dynamics or contains a component predictable from surface characteristics and large-scale atmospheric forcing. Here we show that a substantial, physically coherent component of meter-scale near-surface weather is statistically recoverable from existing observations. By conditioning coarse atmospheric state on sparse surface station measurements and high-resolution Earth observation data, we infer spatially continuous fields of near-surface wind, temperature, and humidity at 10 m resolution across the contiguous United States. Relative to ERA5, the inferred fields reduce wind error by 29% and temperature and dewpoint error by 6%, while explaining substantially more spatial variance at fixed time steps. They also exhibit physically interpretable structure, including urban heat islands, evapotranspiration-driven humidity contrasts, and wind speed differences across land cover types. Our findings expand the frontier of weather modeling by demonstrating a computationally feasible approach to continental-scale meter-resolution inference. More broadly, they illustrate how conditioning coarse dynamical models on static fine-scale features can reveal previously unresolved components of the Earth system.

</details>


### [132] [Benchmarking Temporal Web3 Intelligence: Lessons from the FinSurvival 2025 Challenge](https://arxiv.org/abs/2602.23159)
*Oshani Seneviratne,Fernando Spadea,Adrien Pavao,Aaron Micah Green,Kristin P. Bennett*

Main category: cs.LG

TL;DR: 本文提出FinSurvival Challenge 2025基准测试，利用DeFi协议Aave v3的2180万交易记录构建16个生存预测任务，系统评估时序Web3智能建模方法。研究发现领域感知的时序特征构造显著优于通用建模方法，论证Web3为研究用户流失、风险评估等时序挑战提供了高保真实验沙盒。


<details>
  <summary>Details</summary>
Motivation: 时序Web分析领域缺乏能够捕捉真实世界动态特性（特别是数据删失和非平稳性）的共享、可复现基准，这一空白严重制约了方法论创新与技术跨域迁移。尽管Web3平台生成海量时序化事件流数据，但学界尚未建立标准化评估体系，阻碍了Web3与广义Web领域的知识互通。

Method: 研究采用Aave v3借贷协议21.8百万条链上交易记录，设计FinSurvival Challenge 2025基准框架，将用户行为演化建模为16个生存分析预测任务。通过开源基准与竞赛形式，系统对比了领域知识驱动的时序特征工程与通用机器学习方法在Web3场景下的性能表现。

Result: 基准测试实证表明，融入DeFi领域知识的时序特征构造方法显著超越标准建模范式。该框架成功捕获了Web3生态中的真实时序动态特征（包括删失机制与非平稳分布），为时序智能算法提供了大规模、长周期、高保真的评估平台，验证了Web3数据在研究复杂时序问题上的独特价值。

Conclusion: 研究表明Web3系统因其透明性、时序性与不可篡改性，成为研究用户流失、信用风险、协议演化等时序挑战的理想高保真沙盒。论文提炼了构建下一代时序基准的关键经验：领域感知特征设计至关重要，长期纵向数据不可或缺，跨域可迁移性应成为核心设计原则。未来工作应致力于建立更多Web3赋能的开放基准，推动时序智能研究在Web生态中的整体进步。

Abstract: Temporal Web analytics increasingly relies on large-scale, longitudinal data to understand how users, content, and systems evolve over time. A rapidly growing frontier is the \emph{Temporal Web3}: decentralized platforms whose behavior is recorded as immutable, time-stamped event streams. Despite the richness of this data, the field lacks shared, reproducible benchmarks that capture real-world temporal dynamics, specifically censoring and non-stationarity, across extended horizons. This absence slows methodological progress and limits the transfer of techniques between Web3 and broader Web domains. In this paper, we present the \textit{FinSurvival Challenge 2025} as a case study in benchmarking \emph{temporal Web3 intelligence}. Using 21.8 million transaction records from the Aave v3 protocol, the challenge operationalized 16 survival prediction tasks to model user behavior transitions.We detail the benchmark design and the winning solutions, highlighting how domain-aware temporal feature construction significantly outperformed generic modeling approaches. Furthermore, we distill lessons for next-generation temporal benchmarks, arguing that Web3 systems provide a high-fidelity sandbox for studying temporal challenges, such as churn, risk, and evolution that are fundamental to the wider Web.

</details>


### [133] [MetaOthello: A Controlled Study of Multiple World Models in Transformers](https://arxiv.org/abs/2602.23164)
*Aviral Chawla,Galen Hall,Juniper Lovato*

Main category: cs.LG

TL;DR: 本研究通过MetaOthello框架探究Transformer如何组织多个世界模型。实验发现，在混合游戏数据上训练的模型并未形成隔离的子模型，而是收敛到可跨变体迁移的共享表征。早期层保持游戏无关表征，中间层识别游戏身份，后期层专业化处理。


<details>
  <summary>Details</summary>
Motivation: 现有机械可解释性研究孤立地考察Transformer能力，尚不清楚单一模型如何组织多个潜在冲突的"世界模型"。基础模型需处理多个生成过程，理解其内部组织机制对可解释性至关重要。

Method: 提出MetaOthello——一套受控的奥赛罗棋变体套件，共享语法但规则或标记化方式不同。在混合变体数据上训练小型GPT，研究共享表征空间中多个世界模型的组织方式。

Result: (1) 模型未隔离容量为子模型，而是形成跨变体因果迁移的共享棋盘状态表征；(2) 单一变体的线性探针能有效干预另一变体的内部状态；(3) 同构游戏的表征可通过正交旋转等价；(4) 规则部分重叠时，早期层游戏无关，中间层识别游戏身份，后期层专业化。

Conclusion: MetaOthello为理解Transformer如何而非是否学习多个世界模型提供了新路径，揭示了模型通过共享表征与层次化分工的组织策略，对机械可解释性研究具有重要意义。

Abstract: Foundation models must handle multiple generative processes, yet mechanistic interpretability largely studies capabilities in isolation; it remains unclear how a single transformer organizes multiple, potentially conflicting "world models". Previous experiments on Othello playing neural-networks test world-model learning but focus on a single game with a single set of rules. We introduce MetaOthello, a controlled suite of Othello variants with shared syntax but different rules or tokenizations, and train small GPTs on mixed-variant data to study how multiple world models are organized in a shared representation space. We find that transformers trained on mixed-game data do not partition their capacity into isolated sub-models; instead, they converge on a mostly shared board-state representation that transfers causally across variants. Linear probes trained on one variant can intervene on another's internal state with effectiveness approaching that of matched probes. For isomorphic games with token remapping, representations are equivalent up to a single orthogonal rotation that generalizes across layers. When rules partially overlap, early layers maintain game-agnostic representations while a middle layer identifies game identity, and later layers specialize. MetaOthello offers a path toward understanding not just whether transformers learn world models, but how they organize many at once.

</details>


### [134] [Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models](https://arxiv.org/abs/2602.23179)
*Gal Kesten-Pomeranz,Yaniv Nikankin,Anja Reusch,Tomer Tsaban,Ora Schueler-Furman,Yonatan Belinkov*

Main category: cs.LG

TL;DR: 该研究揭示了蛋白质语言模型检测重复序列的两阶段机制：先通过位置注意力和特化神经元构建特征表示，再通过归纳头跨重复段对齐标记完成预测，将语言模式匹配与生物学知识相结合。


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列富含重复片段，这些重复对结构和功能至关重要。尽管蛋白质语言模型已通过掩码预测展现出识别重复的能力，但其内部工作机制尚未阐明，特别是对精确重复和近似重复的统一检测机制。

Method: 通过分析蛋白质语言模型在掩码标记预测任务中的行为，系统研究其识别精确重复和近似重复的内部机制，重点考察注意力头和神经元的激活模式。

Result: 发现近似重复的检测机制在功能上包含精确重复机制，并揭示了两阶段处理流程：1)利用通用位置注意力头和生物特化组件（如编码氨基酸相似性的神经元）构建特征表示；2)通过归纳头跨重复段对齐标记来促进正确答案预测。

Conclusion: 蛋白质语言模型通过结合基于语言的范式匹配与特化的生物学知识解决这一生物任务，为研究模型中更复杂的进化过程奠定了基础，揭示了领域专业知识如何嵌入通用语言模型架构。

Abstract: Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that the mechanism for approximate repeats functionally subsumes that of exact repeats. We then characterize this mechanism, revealing two main stages: PLMs first build feature representations using both general positional attention heads and biologically specialized components, such as neurons that encode amino-acid similarity. Then, induction heads attend to aligned tokens across repeated segments, promoting the correct answer. Our results reveal how PLMs solve this biological task by combining language-based pattern matching with specialized biological knowledge, thereby establishing a basis for studying more complex evolutionary processes in PLMs.

</details>


### [135] [Takeuchi's Information Criteria as Generalization Measures for DNNs Close to NTK Regime](https://arxiv.org/abs/2602.23219)
*Hiroki Naganuma,Taiji Suzuki,Rio Yokota,Masahiro Nomura,Kohta Ishikawa,Ikuro Sato*

Main category: cs.LG

TL;DR: 本研究探討竹內情報基準(TIC)在深度神經網路(DNN)泛化差距解釋中的適用性，理論與實驗證實TIC僅在神經切線核(NTK) regime附近能有效預測泛化差距，據此提出的近似方法在超參數優化中展現優越的trial剪枝能力。


<details>
  <summary>Details</summary>
Motivation: 由於深度神經網路的統計奇異性與複雜性，傳統泛化度量難以可靠表徵其泛化差距，本研究旨在重新檢驗經典的竹內情報基準(TIC)在DNN中的應用條件與有效性。

Method: 實驗訓練超過5,000個DNN模型，涵蓋12種架構(包括VGG-16)與四個資料集，採用多種計算成本可控的TIC近似方法進行估計，系統性分析不同近似方法的準確性權衡及其與泛化差距的相關性。

Result: 在接近NTK regime條件下，TIC估計值與泛化差距呈現高度相關性；理論與實證均顯示，一旦偏離NTK regime，此相關性即消失。此外，TIC在超參數優化中展現出超越現有方法的trial剪枝效率。

Conclusion: 本研究確立TIC作為DNN泛化度量的理論邊界條件，確認其僅適用於NTK regime附近，但在此範圍內可作為超參數優化的高效剪枝指標，為模型選擇提供了具理論基礎且實用的新工具。

Abstract: Generalization measures have been studied extensively in the machine learning community to better characterize generalization gaps. However, establishing a reliable generalization measure for statistically singular models such as deep neural networks (DNNs) is difficult due to their complex nature. This study focuses on Takeuchi's information criterion (TIC) to investigate the conditions under which this classical measure can effectively explain the generalization gaps of DNNs. Importantly, the developed theory indicates the applicability of TIC near the neural tangent kernel (NTK) regime. In a series of experiments, we trained more than 5,000 DNN models with 12 architectures, including large models (e.g., VGG-16), on four datasets, and estimated the corresponding TIC values to examine the relationship between the generalization gap and the TIC estimates. We applied several TIC approximation methods with feasible computational costs and assessed the accuracy trade-off. Our experimental results indicate that the estimated TIC values correlate well with the generalization gap under conditions close to the NTK regime. However, we show both theoretically and empirically that outside the NTK regime such correlation disappears. Finally, we demonstrate that TIC provides better trial pruning ability than existing methods for hyperparameter optimization.

</details>


### [136] [Physics Informed Viscous Value Representations](https://arxiv.org/abs/2602.23280)
*Hrishikesh Viswanath,Juanwu Lu,S. Talha Bukhari,Damon Conover,Ziran Wang,Aniket Bera*

Main category: cs.LG

TL;DR: 本文提出了一种基于哈密顿-雅可比-贝尔曼（HJB）方程粘性解的物理学约束正则化方法，用于离线目标条件强化学习中的值函数估计。通过Feynman-Kac定理将偏微分方程转化为期望形式，实现了可处理的蒙特卡洛估计，避免了高阶梯度的数值不稳定性，在导航和复杂操作任务中提升了几何一致性。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习从静态数据集中学习目标条件策略时，由于状态-动作空间覆盖有限，准确的价值估计仍然具有挑战性。近期基于物理信息的方法通过一阶偏微分方程（如Eikonal方程）对价值函数施加物理和几何约束，但这些公式在复杂高维环境中往往不适定（ill-posed），导致优化不稳定和泛化能力受限。

Method: 1. 从最优控制理论出发，提出基于HJB方程粘性解的物理学约束正则化项，为学习过程提供物理基础的归纳偏置；2. 利用Feynman-Kac定理将偏微分方程求解转化为期望形式，使目标函数可通过蒙特卡洛方法进行可处理的估计；3. 在价值迭代过程中显式地正则化和约束更新，避免高阶梯度带来的数值不稳定性。

Result: 实验表明，该方法显著提升了价值函数的几何一致性，在导航任务和高维复杂操作任务中均表现出良好的适用性和性能改进。代码已开源。

Conclusion: 该方法通过引入基于HJB方程的物理学正则化，将离线GCRL grounded在最优控制理论框架下，有效解决了价值估计中的覆盖不足问题，为复杂环境下的稳定学习提供了新思路，具有较强的理论和实践价值。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.

</details>


### [137] [Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications](https://arxiv.org/abs/2602.23303)
*Ilya Balabin,Thomas M. Kaiser*

Main category: cs.LG

TL;DR: 本论文是三部曲系列的第一部分，旨在解决自然科学中机器学习模型作为"黑箱"使用的因果缺陷问题，通过构建一个融合化学理论、生物学理论和概率论的形式化因果框架，提出"聚焦"新概念（即算法在大数据集中识别隐藏机制的能力），并在Akt抑制剂家族上提供初步验证，最终目标是建立一种无需还原论工具的"推断力学"数学框架。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在自然科学研究中被广泛用于大数据处理，但其预测模型常被视为不考虑因果结构的黑箱，尽管已有尝试将因果关系引入讨论，却缺乏统一且坚实的理论处理。本研究旨在通过融合化学理论、生物理论、概率论与因果性，纠正机器学习在自然科学中的因果缺陷，为化学生物学建立新的建模范式。

Method: 本文作为系列论文的第一部分，采用理论构建方法：1）建立化学生物学现象的基础因果结构形式化框架；2）创新性定义"聚焦"概念，量化机器学习算法从大数据集中筛选出潜在机制的能力；3）在Akt抑制剂家族上进行原理性验证实验。后续第二部分将形式化探讨化学相似性，第三部分将提供实验证据展示隐藏因果结构如何削弱所有机器学习性能。

Result: 本部分取得了三项核心成果：1）成功构建了化学生物学现象的形式化因果结构基础框架；2）明确定义了"聚焦"这一连接机器学习与潜在机制的新概念；3）在Akt抑制剂家族上完成了原则性验证，证明了理论框架的初步可行性。

Conclusion: 本系列论文最终将确立"推断力学"——一种无需还原论工具即可对自然界机制进行建模的新型数学框架。作为开篇，本文奠定了将因果结构严谨引入自然科学机器学习的基础，通过"聚焦"概念桥接数据驱动模型与机制解释，为后续深入探讨化学相似性及提供全面实验证据构建了理论蓝图。

Abstract: Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.

</details>


### [138] [A Proper Scoring Rule for Virtual Staining](https://arxiv.org/abs/2602.23305)
*Samuel Tonks,Steve Hood,Ryan Musso,Ceridwen Hopely,Steve Titus,Minh Doan,Iain Styles,Alexander Krull*

Main category: cs.LG

TL;DR: 本文提出信息增益作为细胞级评估框架，解决虚拟染色模型中真实后验分布不可得的问题，能直接评估预测后验并揭示其他指标无法发现的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟染色模型评估仅检查边际分布准确性，无法评估预测后验分布。由于真实后验不可获得，亟需一种理论严谨、可直接评估预测后验且支持跨模型比较的评估框架。

Method: 提出采用信息增益（IG）作为细胞级评估框架，其具备严格适当评分规则、理论可解释性和跨模型可比性。在大型高通量筛选数据集上，对比评估了扩散模型和GAN-based虚拟染色模型，并与其他指标进行性能分析。

Result: 实验表明，信息增益能够揭示其他评估指标无法检测到的显著性能差异，为模型评估提供了更细致的洞察。

Conclusion: 信息增益为虚拟染色模型的预测后验分布提供了直接、可解释的评估方法，是现有评估协议的有效补充，能够更全面地进行模型和特征性能比较。

Abstract: Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. IG is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. We evaluate diffusion- and GAN-based models on an extensive HTS dataset using IG and other metrics and show that IG can reveal substantial performance differences other metrics cannot.

</details>


### [139] [Differentiable Zero-One Loss via Hypersimplex Projections](https://arxiv.org/abs/2602.23336)
*Camilo Gomez,Pengyang Wang,Liansheng Tang*

Main category: cs.LG

TL;DR: 本文针对零一损失的不可微性问题，提出了一种可微近似方法，通过约束优化构建到超单形的光滑保序投影，提出Soft-Binary-Argmax算子，并在大型批次训练中验证了其提升泛化性能的有效性。


<details>
  <summary>Details</summary>
Motivation: 零一损失作为分类任务的黄金标准因其不可微性而无法应用于梯度优化，且现有大型批次训练存在性能下降问题。将结构化优化融入端到端可微模型可增强归纳偏置，缩小优化目标与评估指标之间的差距。

Method: 基于约束优化框架，构造了到n,k维超单形的光滑且保序的投影算子，命名为Soft-Binary-Argmax。推导了该算子的数学性质并给出了其雅可比矩阵的高效计算方法，可无缝集成至二分类及多分类学习系统。

Result: 实证研究表明，该方法通过向输出logits施加几何一致性约束，在大型批次训练场景下显著提升了模型的泛化能力。

Conclusion: 所提Soft-Binary-Argmax算子有效缩小了传统大型批次训练中泛化性能与理论最优之间的差距。

Abstract: Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.

</details>


### [140] [Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms](https://arxiv.org/abs/2602.23341)
*Alkis Kalavasis,Anay Mehrotra,Manolis Zampetakis,Felix Zhou,Ziyu Zhu*

Main category: cs.LG

TL;DR: 本文研究凸划分粗粒度数据下的高斯均值估计问题，完整解决了两个核心理论问题：建立了凸划分下均值可识别的充要条件，并证明了在此条件下存在多项式时间估计算法，实现了可识别性与计算高效性的统一。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，测量舍入、传感器精度限制和经济系统滞后等场景常导致只能观测到样本所属集合而非精确值，形成粗粒度数据。此类数据下，参数估计面临可识别性（能否唯一恢复真值）和计算复杂性（是否存在高效算法）两大根本挑战，且缺乏系统的理论刻画。

Method: 针对d维单位协方差高斯分布，通过分析凸划分结构对样本信息量的影响，理论刻画均值可识别的数学条件，并基于凸优化与统计推断方法设计多项式时间估计算法。

Result: 给出了凸划分下均值可识别的充分必要条件，并证明在可识别前提下存在同时具备样本高效性和计算高效性（多项式时间）的估计算法，完全解决了[FKKT21]提出的两个开放问题，建立了粗粒度高斯估计的完整理论框架。

Conclusion: 研究表明凸划分结构是保证可识别性与计算可行性的关键前提，填补了粗粒度数据下高斯均值估计的理论空白，为相关领域的算法设计和实际应用提供了坚实的理论支撑。

Abstract: Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]

</details>


### [141] [FlashOptim: Optimizers for Memory Efficient Training](https://arxiv.org/abs/2602.23349)
*Jose Javier Gonzalez Ortiz,Abhay Gupta,Chris Renard,Davis Blalock*

Main category: cs.LG

TL;DR: FlashOptim是一套内存优化方案，通过改进主权重分割和8位优化器状态量化，在保持模型质量和API兼容性的同时，将每参数训练内存减少50%以上。


<details>
  <summary>Details</summary>
Motivation: 标准混合精度训练需要为每个模型参数存储参数、梯度和优化器状态（通常每个占4字节），这使得拥有少于100GB显存的研究者难以训练70亿参数级别的大模型。

Method: 提出两项关键技术：1）通过找到并利用量化误差的紧约束来改进主权重分割；2）设计压缩扩展函数大幅降低8位优化器状态量化误差，并结合16位梯度。

Result: 将AdamW内存从每参数16字节降至7字节（释放梯度后可至5字节），模型检查点大小减少一半以上。在包括Llama-3.1-8B微调和视觉语言基准测试中未出现质量下降。

Conclusion: FlashOptim在SGD、AdamW和Lion等优化器上均能有效减少训练内存占用，同时保持模型质量和API兼容性，使大模型训练更加普惠。

Abstract: Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.
  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.
  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.

</details>


### [142] [SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport](https://arxiv.org/abs/2602.23353)
*Simon Roschmann,Paul Krzakala,Sonia Mazelet,Quentin Bouniot,Zeynep Akata*

Main category: cs.LG

TL;DR: 本文提出SOTAlign，一个两阶段半监督框架，通过少量配对数据和大量未配对数据对齐预训练的视觉-语言模型。第一阶段用线性教师恢复粗粒度共享几何结构，第二阶段用最优传输优化对齐，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型对齐依赖对比损失和数百万配对样本，监督成本高昂。本文探索能否用更少监督实现有意义对齐，提出半监督设定，利用少量配对数据和大量未配对数据降低标注需求。

Method: SOTAlign采用两阶段框架：1) 从有限配对数据中通过线性教师模型恢复粗粒度共享几何结构；2) 在未配对样本上通过最优传输散度优化对齐，传递关系结构而不过度约束目标空间。

Result: SOTAlign能有效利用未配对图像和文本，学习跨数据集和编码器对的鲁棒联合嵌入，在监督和半监督基线方法上表现显著更优。

Conclusion: 该研究证明了半监督设定下，少量配对数据结合大量未配对数据可实现高质量视觉-语言对齐，为降低数据标注成本提供了新思路。

Abstract: The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.

</details>


### [143] [A Dataset is Worth 1 MB](https://arxiv.org/abs/2602.23358)
*Elad Kimchi Shoshani,Leeyam Gabay,Yedid Hoshen*

Main category: cs.LG

TL;DR: PLADA方法通过仅传输伪标签而非像素数据，在客户端预加载通用无标签数据集的前提下，实现了极小通信负载下的任务知识迁移。该方法通过剪枝机制筛选与目标任务最相关的图像标签，在10个数据集上验证了小于1MB的传输负载下仍能保持高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统数据集服务器向多客户端分发大型负载时面临巨大通信开销。由于客户端软硬件异构性，直接传输预训练模型不可行，而传输原始数据成本高昂。现有数据集蒸馏技术难以扩展到高分辨率图像，且压缩后文件仍不够精简，亟需更高效的数据传输范式。

Method: 提出Pseudo-Labels as Data (PLADA)框架。假设客户端已预置大型通用无标签参考数据集（如ImageNet-1K/21K），服务器端仅传输目标任务特定图像的类别标签。核心创新在于引入剪枝机制，根据语义相关性过滤参考数据集，保留最具代表性的图像标签，从而在优化训练效率的同时最小化传输负载。

Result: 在10个多样化数据集上的实验验证表明，该方法能以低于1MB的超小传输负载实现任务知识迁移，同时保持较高的分类准确率，显著优于现有数据集蒸馏方法。

Conclusion: PLADA通过完全消除像素传输并采用智能标签筛选机制，为大规模数据集服务提供了通信高效的解决方案，为分布式机器学习中的数据传输瓶颈提供了有前景的解决思路，具有重要的实践价值。

Abstract: A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [144] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: 本文提出了一种名为GYWI的科学思想生成系统，通过将作者知识图谱与检索增强生成(RAG)结合，为大型语言模型提供可控的学术背景和可追溯的灵感路径，显著提升了生成科学思想的新颖性、可靠性和相关性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学思想生成领域展现出潜力，但生成的结果往往缺乏可控的学术语境和可追溯的灵感来源路径。这一问题限制了LLMs在学术创新中的实际应用，因为研究者无法确保生成想法的学术合理性、创新性和可复现性。因此，需要一种能够提供外部知识支持和灵感追踪机制的方法来增强LLMs在学术创新中的可控性和透明度。

Method: 本文提出了GYWI系统，包含三个核心方法：(1) 作者中心的知识图谱构建方法和灵感源采样算法，用于构建外部知识库；(2) 混合检索机制，结合RAG和GraphRAG技术，实现知识检索的深度与广度平衡；(3) 基于强化学习原理的Prompt优化策略，自动指导LLM根据混合上下文优化生成结果。评估方面，基于arXiv(2018-2023)构建了评估数据集，并采用多选题任务实证评估、LLM评分、人工评估和语义空间可视化分析相结合的综合性评估方法。

Result: 在GPT-4o、DeepSeek-V3、Qwen3-8B和Gemini 2.5等多个大型语言模型上的实验结果表明，GYWI系统在新颖性、可靠性和相关性等多个指标上显著优于主流LLMs。评估从新颖性、可行性、清晰性、相关性和重要性五个维度对生成的科学思想进行评分，证实了所提方法的有效性。

Conclusion: GYWI系统通过融合作者知识图谱与检索增强技术，成功解决了科学思想生成中语境不可控和灵感路径不可追溯的问题，为LLMs在学术创新领域的应用提供了新的技术路径，显著提升了生成科学思想的质量和实用性。

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [145] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: 本文提出FIRE基准测试，通过金融资格考试题和3000个金融场景问题，系统评估大语言模型的理论知识与实际应用能力，并对包括XuanYuan 4.0在内的先进模型进行能力边界分析，同时开源基准和代码。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估缺乏对金融领域理论深度与实践价值的综合检验，亟需一个覆盖资格考试和真实业务场景的基准来准确衡量模型在金融应用中的实际能力。

Method: 采用双层评估框架：理论层使用金融资格考试题目；实践层构建系统化评估矩阵，涵盖复杂金融领域及子域，收集3000个场景问题（含封闭式决策题和开放式评分题）。

Result: 构建了包含金融理论和实践场景的FIRE基准，完成了对SOTA大模型的系统评估，揭示了当前模型在金融领域的性能边界和能力局限。

Conclusion: 该基准为金融大模型评估提供了完整框架，开源将促进金融AI研究发展，评估结果为未来模型优化指明方向。

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [146] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 本文提出因果嵌入框架，作为因果模型抽象化的推广，将多个细粒度模型映射至粗粒度因果模型的子系统，定义广义一致性概念，并通过多分辨率边缘问题揭示其在统计与因果边缘问题中的关联性及其在合并异构模型数据集中的实用价值。


<details>
  <summary>Details</summary>
Motivation: 传统因果模型抽象化方法局限于两模型间关系，难以处理多模型整合需求。当面临多个不同表示形式的因果模型时，如何将其统一到同一粗粒度框架并保持因果关系的一致性，成为亟待解决的理论问题。

Method: 将因果嵌入定义为抽象化的泛化，建立广义一致性概念，构建多分辨率边缘问题框架，实现多个详细模型向粗粒度模型子系统的系统性映射。

Result: 理论层面，阐明了因果嵌入与统计边缘问题、因果边缘问题的内在联系；实践层面，展示了该框架在融合不同表示模型数据集方面的有效性。

Conclusion: 因果嵌入理论拓展了传统因果抽象化的范畴，为多模型、多分辨率因果推理提供了统一框架，在因果发现、数据融合等应用场景具有重要价值。

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [147] [Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents](https://arxiv.org/abs/2602.22302)
*Varun Pratap Bhardwaj*

Main category: cs.AI

TL;DR: 提出Agent Behavioral Contracts (ABC)框架，将Design-by-Contract原则引入自主AI智能体。通过形式化合约规范行为并证明漂移边界定理，为LLM驱动的智能体提供可执行的行为保证。实验显示该框架能检测5.2-6.8个/会话的软违规、实现88-100%硬约束合规，并将漂移限制在D*<0.27，运行时开销低于10毫秒。


<details>
  <summary>Details</summary>
Motivation: 传统软件依赖API、类型系统和断言等契约确保行为正确性，而AI智能体仅通过自然语言提示运行，缺乏形式化行为规约。这导致AI部署中普遍存在行为漂移、治理失效和高项目失败率，亟需为自主智能体建立可执行的行为规范机制。

Method: 定义ABC合约结构C=(P,I,G,R)，包含前置条件、不变量、治理策略和恢复机制四个一等组件。提出(p, delta, k)-满足度概率合规概念处理LLM非确定性，证明漂移边界定理：当恢复率γ>漂移率α时期望漂移被限制在D*=α/γ。建立多智能体安全组合条件，实现AgentAssert运行时库并在AgentContract-Bench基准上评估。

Result: 在1980次会话、7个模型、6个厂商的评估中，合约化智能体每会话检测到5.2-6.8个软违规（对照组完全遗漏，p<0.0001，Cohen's d=6.7-33.8），硬约束合规率达88-100%，行为漂移被限制在D*<0.27。前沿模型恢复率100%，所有模型恢复率17-100%，每次动作执行开销低于10毫秒。

Conclusion: ABC框架首次为自主AI智能体提供形式化、可执行的行为合约机制，理论证明并实验验证了其控制行为漂移的有效性。该工作填补了AI治理的关键空白，为构建可靠可信的AI系统奠定重要基础，兼具理论突破与实践价值。

Abstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.

</details>


### [148] [Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?](https://arxiv.org/abs/2602.22401)
*Yongjun Zhang*

Main category: cs.AI

TL;DR: 本文提出"氛围研究"概念，探讨AI智能体在社会科学研究中的质变影响。通过学者技能插件案例和认知任务框架，揭示AI委托边界是认知性而非阶段性的，其优势在于速度、覆盖与方法支撑，但缺乏理论原创性和隐性知识。研究警示增强脆弱性、分层风险与教学危机，并提出五项负责任研究原则。


<details>
  <summary>Details</summary>
Motivation: 分析AI智能体（具备多步推理、持久状态与工具调用能力）相较于传统聊天机器人的范式转变，界定其在研究全流程中的能力边界，并回应"氛围编程"在学术领域的延伸现象。

Method: 以Claude Code的21项学者技能插件为实证案例，构建基于"可编码性"与"隐性知识需求"双维度的认知任务分析框架，识别贯穿研究各阶段的认知性委托边界。

Result: 发现AI智能体的委托边界呈认知穿透性而非线性阶段性：AI可加速执行标准化、可编码任务并提供方法学脚手架，但在理论创新、领域默会知识等隐性维度存在根本局限。

Conclusion: 指出三大专业影响：条件脆弱的增强效应、学术阶层分化风险、以及研究方法教学危机；基于此提出五项负责任"氛围研究"原则，旨在平衡AI工具效用与学术核心价值的维护。

Abstract: AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.

</details>


### [149] [Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus](https://arxiv.org/abs/2602.22408)
*Caroline Ahn,Quan Do,Leah Bakst,Michael P. Pascale,Joseph T. McGuire,Michael E. Hasselmo,Chantal E. Stern*

Main category: cs.AI

TL;DR: 本研究开发CogARC人类抽象推理测试集，通过260名参与者的高精度行为数据，揭示人类从少量示例学习的策略多样性、表现变化及错误一致性，为认知研究提供新工具。


<details>
  <summary>Details</summary>
Motivation: 探究人类抽象推理的认知策略，开发适合人类认知特点的测试环境，以理解人类如何从稀疏示例中快速学习并应用规则。

Method: 基于ARC创建人类适应版CogARC（含75个视觉推理问题），开展两项实验（共260名参与者），以高分辨率记录示例查看、编辑序列和多次尝试等行为数据。

Result: 参与者整体表现良好（实验1准确率约90%，实验2约80%），但表现差异显著；难题导致更长思考时间和更多元策略；任务后期反应加快但准确率微降，表明熟悉任务结构而非提升规则学习能力；错误解法常高度一致，解题轨迹分直接高效型和探索重启型。

Conclusion: CogARC为研究人类抽象推理提供了丰富的行为环境，有助于揭示人类在不确定性下的泛化、误泛化及策略适应机制，具有重要认知科学价值。

Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.

</details>


### [150] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 该研究构建了一个概率框架，分析异构智能体在学习评估自身可靠性后选择性投票的集体准确性。通过校准阶段更新自我能力信念，再经置信度门控决定投票与否，推导出非渐近成功概率下界，将孔多塞陪审团定理推广至序贯置信度门控场景，并通过蒙特卡洛模拟验证，为缓解集体大语言模型决策中的幻觉问题提供了理论支撑。


<details>
  <summary>Details</summary>
Motivation: 经典知识投票理论（如孔多塞陪审团定理）假设固定参与，而现实世界的群体决策往往受益于允许参与者声明"我不知道"。选择性参与机制能够显著提升在不确定性环境下的集体决策质量，这对AI安全等应用领域尤为重要。

Method: 提出一个两阶段概率框架：1）校准阶段，智能体通过贝叶斯更新学习评估自身的固定能力水平；2）置信度门控阶段，基于学习到的置信度阈值决定是否参与投票。采用理论推导与蒙特卡洛模拟相结合的研究方法。

Result: 推导出群体决策成功概率的非渐近下界，严格证明了选择性参与机制可将孔多塞陪审团定理的渐近保证推广到序贯、置信度门控的设置。蒙特卡洛模拟验证了理论界在有限群体规模下的有效性。

Conclusion: 该框架为异构智能体在不确定环境下的决策提供了理论保证，特别适用于AI安全领域。通过选择性投票机制，能够有效缓解集体大语言模型决策中的幻觉问题，提高AI系统的可靠性和安全性，为构建更稳健的集体智能系统提供了新思路。

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [151] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 本文深入分析潜在推理的内部机制，发现其存在捷径行为（不依赖潜在推理即可高准确率）和结构化搜索不成立（实际为隐式剪枝压缩）两大问题，并揭示监督强度与假设多样性间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 潜在推理虽在性能上表现优异，但其内部工作机制尚未被充分理解，特别是潜在表示在推理过程中的实际作用与行为模式，这限制了该范式的进一步发展。

Method: 通过对不同监督程度的潜在推理方法进行系统性分析，探究潜在表示的行为特征，验证BFS-like探索假设，并比较不同监督强度下的表现差异。

Result: 发现两大关键问题：1）普遍存在捷径行为，模型无需真正利用潜在推理即可获得高准确率；2）潜在表示虽能编码多种可能性，但推理过程并未实现结构化搜索，而是呈现隐式剪枝和压缩现象；3）存在监督强度与表示多样性的权衡。

Conclusion: 强监督抑制捷径行为但限制假设多样性，弱监督增强表示丰富性却加剧捷径行为。这些发现为理解潜在推理的内在机制提供了新视角，并指出未来改进方向。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [152] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 提出对比世界模型(CWM)，通过InfoNCE对比损失和难负样本挖掘，改进大语言模型作为动作可行性评分器的性能，在ScienceWorld基准测试中显著优于监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 可靠的动作可行性评分器是具身智能体系统中的关键瓶颈，现有监督微调方法将候选动作独立处理，无法有效区分物理上正确和微妙错误的行为，需要更明确的判别训练机制。

Method: 使用对比学习方法微调大语言模型，采用InfoNCE目标函数和难负样本挖掘，在评分空间中将有效动作与无效动作分离，特别关注语义相似但物理不兼容的难负样本。

Result: 在ScienceWorld的605个难负样本测试对中，CWM在最小编辑负样本的Precision@1上比SFT提升6.76个百分点，AUC-ROC达0.929 vs 0.906；在分布外压力测试下，CWM的安全边际(-2.39)显著优于SFT(-3.96)。

Conclusion: 对比训练能诱导模型学习到更精确的物理可行性表征，比单纯使用监督微调更有效地支持具身智能体的动作筛选和规划。

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [153] [ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465)
*Joseph Tso,Preston Schmittou,Quan Huynh,Jibran Hutchins*

Main category: cs.AI

TL;DR: 该论文提出ConstraintBench基准测试，评估六种前沿大语言模型直接求解约束优化问题的能力，发现可行性而非最优性是主要瓶颈，最佳模型仅达到65%约束满足率，且没有模型能在联合可行性与最优性上超过30.5%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅评估大语言模型将优化问题转化为求解器代码的能力，但尚不清楚LLMs能否在没有求解器的情况下直接生成正确的约束优化问题解。这限制了对LLMs在运筹学决策中实际适用性的理解。

Method: 创建ConstraintBench基准，涵盖10个运筹学领域的200个任务，每个任务提供自然语言场景、约束条件和优化目标，要求模型返回结构化解决方案。使用Gurobi求解器验证所有真实解，并通过确定性验证器检查约束满足情况和最优性差距。评估六种前沿模型。

Result: 最佳模型约束满足率仅为65.0%，可行解平均达到Gurobi最优目标的89-96%。联合可行性与最优性（误差0.1%内）无模型超过30.5%。领域难度差异显著：生产混合领域可行性达83.3%，而人员排班领域仅0.8%。系统性失败模式包括：持续时间约束误解、实体幻觉、以及设施选址和车辆路径问题中可行性与最优性脱节（高可行性但0%最优性）。

Conclusion: 大语言模型在直接解决约束优化问题上存在严重局限，可行性是核心挑战。不同领域性能差异巨大，模型易产生系统性错误。论文将公开ConstraintBench和评估基础设施，以促进未来研究。

Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

</details>


### [154] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: 本文提出VERO框架，用于评估和基准测试编码智能体优化。该框架提供版本化快照、预算控制和执行追踪的可复现评估工具，以及基准测试套件。通过实证研究比较优化器配置，识别可靠改进智能体性能的方法，并开源框架支持相关研究。


<details>
  <summary>Details</summary>
Motivation: 编码智能体优化通过编辑-执行-评估循环迭代改进目标智能体，是重要的新兴应用。然而社区缺乏对其性能的系统性理解。由于该过程交错确定性代码与随机大语言模型补全，需结构化捕获中间推理与执行结果，这使其与传统软件工程存在本质差异，亟需专用评估方法。

Method: 提出VERO框架，包含两部分：(1) 可复现评估工具，集成版本化智能体快照、预算控制评估和结构化执行追踪；(2) 基准测试套件，提供目标智能体、任务及参考评估程序。利用该框架开展实证研究，比较不同优化器配置并分析能可靠提升目标智能体性能的修改策略。

Result: 利用VERO开展实证研究，比较不同优化器配置在各类任务上的表现，并分析哪些修改能可靠提升目标智能体性能，为编码智能体优化提供实证基础。

Conclusion: 开源VERO框架以支持将智能体优化作为编码智能体核心能力的研究，推动该领域的系统化发展。

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [155] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 本研究运用大语言模型对AI与生命周期评价交叉研究进行系统性综述，识别研究趋势与主题，展示LLM辅助方法支持大规模可重复综述、促进计算高效LCA和提升可持续决策质量的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管AI与生命周期评价(LCA)的融合研究近年来快速发展，但该领域仍缺乏全面、广泛的系统性综述。本研究旨在填补这一空白，对AI-LCA交叉研究进行深入分析。

Method: 研究采用大语言模型(LLM)驱动的文本挖掘方法，结合传统文献综述技术，构建了一个动态有效的分析框架，用于识别AI-LCA领域的研究趋势、新兴主题和未来方向。

Result: 分析发现：随着LCA研究扩展，AI技术采用率急剧增长；呈现向LLM驱动方法转变、机器学习应用持续增加的趋势；AI方法与对应LCA阶段间存在统计显著相关性；该框架能有效捕捉宏观研究趋势和微观概念模式。

Conclusion: 本研究展示了LLM辅助方法在大规模可重复综述中的潜力，为计算高效的LCA实施路径提供了评估，并帮助LCA从业者整合前沿工具与时敏洞察，从而提升可持续性决策的严谨性和质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [156] [A Mathematical Theory of Agency and Intelligence](https://arxiv.org/abs/2602.22519)
*Wael Hafez,Chenan Wei,Rodrigo Felipe,Amir Nazeri,Cameron Reid*

Main category: cs.AI

TL;DR: 本文提出"双预测性"P作为系统观测-行动-结果间共享信息的原理性度量，证明其理论边界（量子系统P=1、经典系统P≤0.5、引入能动性后更低），通过双摆系统、强化学习智能体及大语言模型对话验证，区分了能动性与智能的本质差异，并借鉴丘脑皮层调节机制提出实时P监控架构，为实现自适应韧性人工智能奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有人工智能虽能处理海量信息生成复杂预测，但缺乏原理性指标衡量系统与环境交互中实际共享的信息比例，导致预测表面成功而交互质量退化的问题，需要建立可量化资源利用效率的反馈机制。

Method: 从第一性原理推导出共享信息分数"双预测性"P，严格证明其理论边界；在双摆物理系统、强化学习智能体及多轮大语言模型对话中实验验证；借鉴生物系统丘脑皮层调节机制，设计实时监测P的反馈架构。

Result: 验证P的理论边界（量子系统可达1、经典系统不超过0.5、引入能动性后降低）；揭示当前人工智能仅具能动性而无智能（缺乏交互学习、自我监控与适应能力）；所提实时P监控架构为构建自适应韧性人工智能提供了可行方案。

Conclusion: 该理论框架为区分能动性与智能提供了数学基础，指出现有人工智能缺乏自我监控与适应机制；提出的类脑反馈架构是迈向真正智能系统的关键一步，对构建可靠、自适应的复杂系统具有原理性指导意义。

Abstract: To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.

</details>


### [157] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 本文提出了一种用于无蜂窝O-RAN的智能体AI框架，通过多智能体协作实现意图翻译和优化，采用参数高效微调技术提升可扩展性，在节能模式下显著减少激活的O-RU数量并降低内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多关注独立智能体处理简单意图，而复杂意图需要智能体间协调，这一问题尚未得到充分探索。O-RAN架构虽支持智能体部署，但缺乏有效的多智能体协作框架来处理复杂运营商意图。

Method: 提出包含监督智能体、用户加权智能体、O-RU管理智能体和监控智能体的框架。监督智能体将运营商意图转化为优化目标和最低速率要求；用户加权智能体从记忆模块检索经验确定用户优先级权重；O-RU管理智能体使用深度强化学习决定激活的O-RU集合；监控智能体确保速率要求满足。采用参数高效微调(PEFT)方法使不同智能体共享同一底层大语言模型。

Result: 仿真结果表明，在节能模式下，与三种基线方案相比，所提框架将激活的O-RU数量减少了41.93%；使用PEFT方法，与部署独立LLM智能体相比，内存使用量降低了92%。

Conclusion: 该框架有效解决了复杂意图的多智能体协调问题，显著提升了O-RAN的能效和可扩展性，为未来autonomous RANs提供了可行的AI驱动解决方案。

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [158] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: 本文提出AHCE框架，通过主动的人机协作机制，使LLM智能体能够在专业领域任务中有效利用人类专家的长尾知识，在Minecraft实验中显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在通用推理方面表现优异，但在缺乏训练数据的专业领域中，由于缺少长尾知识而经常失败。虽然人类专家可提供此类知识，但其指导往往结构混乱且不可靠，难以直接整合到智能体的规划中。

Method: 提出AHCE（主动人类增强挑战参与）框架，核心为人类反馈模块（HFM）。该模块采用学习策略将人类专家视为交互式推理工具，通过主动请求专家推理而非简单求助，实现按需人机协作。

Result: 在Minecraft环境中进行的大量实验表明，该框架效果显著：普通难度任务成功率提升32%，极难任务提升近70%，且仅需最少的人类干预。

Conclusion: 成功增强智能体性能的关键在于学习如何恰当地请求专家推理，而不仅仅是简单求助，这为人机协作提供了新范式。

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [159] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: 本文提出CourtGuard框架，这是一个检索增强的多智能体系统，将LLM安全评估重构为"证据辩论"机制。该框架通过基于外部政策文档的对抗性辩论，在7个安全基准测试中取得SOTA性能，无需微调即可超越专用基线，同时具备零样本适应性和自动化数据审核能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全机制过度依赖静态微调的分类器，存在适应性僵化问题：无法在不进行昂贵重新训练的情况下执行新的治理规则，缺乏灵活应对监管要求变化的动态能力。

Method: 引入CourtGuard框架，采用检索增强的多智能体架构，将安全评估重新设计为"证据辩论"范式。通过协调基于外部政策文档的对抗性辩论过程，实现安全逻辑与模型权重的解耦。

Result: 1) 在7个安全基准测试中达到最先进性能，无需微调即超越专用策略遵循基线；2) 零样本适应性：通过更换参考策略，成功泛化至域外维基百科破坏任务，准确率达90%；3) 自动化数据整理与审计：利用该框架构建并审计了九个新型复杂对抗攻击数据集。

Conclusion: 将安全逻辑与模型权重解耦，为应对当前及未来AI治理的监管要求提供了一条稳健、可解释且高度适应性的技术路径，有效解决了传统静态安全机制的刚性局限。

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [160] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 该论文揭示了示例引导在数学推理中存在策略使用与可执行性之间的根本差距，导致效果不稳定；提出的Selective Strategy Retrieval（SSR）框架通过显式建模策略可执行性，实现可靠一致的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 示例引导虽广泛用于提升推理性能，但其效果在不同问题和模型间高度不稳定，即使引导正确且相关。这种不稳定性源于策略使用（是否出现在成功解中）与策略可执行性（能否为目标模型有效执行）之间的差距，特别是人类与模型生成策略存在系统性差异。

Method: 通过控制分析配对的人工与模型生成解决方案，识别策略使用与可执行性的系统性分离；提出SSR测试时框架，利用经验性、多路径、源感知信号选择性检索与组合策略，显式建模可执行性。

Result: 在多个数学推理基准上，SSR相比直接求解、上下文学习和单源引导实现可靠一致的改进，对紧凑型推理模型在AIME25上提升高达13分，在Apex上提升5分。

Conclusion: 研究诊断了示例引导不稳定的根本原因，并验证了显式建模策略可执行性的SSR框架能有效缓解该问题，为数学推理提供更鲁棒的性能提升方法。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [161] [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585)
*Jodi M. Casabianca,Maggie Beiting-Parrish*

Main category: cs.AI

TL;DR: 本文将心理测量学评分者模型集成至AI流程，通过多面Rasch模型分离真实质量与评分者偏差（严厉度、趋中性），提升人类评估的可靠性与效度。


<details>
  <summary>Details</summary>
Motivation: 人类评估是AI训练与评估的核心，但现有实践忽视其系统测量误差。评分者效应（如严厉度差异、趋中倾向）严重扭曲评分，导致基于原始评分的结论不可靠。

Method: 综述评分者效应理论，应用项目反应理论中的多面Rasch模型，以OpenAI摘要数据集为例，对评分者严厉度进行统计调整，分离真实质量与评分行为。

Result: 实证显示，调整评分者严厉度后获得修正的质量估计，并诊断评分者表现。模型有效区分了真实输出质量与评分者行为偏差。

Conclusion: 将心理测量学建模融入人在回路评估，可实现人类数据的原则性、透明化使用，使开发者基于调整分数而非原始易错评分决策，推动AI评估实践更稳健、可解释且构念一致。

Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.

</details>


### [162] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: 本文提出 SideQuest，一种利用大推理模型(LRM)自身进行KV缓存压缩的新方法，通过在并行辅助任务中推理上下文token的重要性，在智能体任务中将峰值token使用量减少高达65%且精度损失极小。


<details>
  <summary>Details</summary>
Motivation: 长周期智能体任务(如深度研究)需要进行跨网页和文档的多跳推理，外部检索token会迅速占满LLM上下文，导致内存激增并限制解码性能；现有KV缓存压缩启发式方法无法有效支持多步推理模型。

Method: SideQuest将KV缓存压缩建模为与主任务并行的辅助任务，利用LRM自身推理上下文token的效用值来指导压缩，避免管理token污染模型记忆。

Result: 仅用215个样本训练后，SideQuest在智能体任务中峰值token使用量降低最高达65%，精度下降极小，性能优于基于启发式的KV缓存压缩技术。

Conclusion: 通过利用推理模型自身的判断力进行智能token筛选，SideQuest为长上下文智能体任务提供了一种高效的内存管理方案，显著优于传统启发式方法。

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [163] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: 本文提出AHBid框架，通过结合生成式规划与实时控制来解决在线广告多通道场景下的预算分配问题。该框架采用基于扩散模型的高层次生成规划器捕捉历史上下文，配备约束执行机制和轨迹优化机制，并融合控制理论实现历史知识与实时信息的协同，最终在离线实验和在线A/B测试中实现投资回报率提升13.57%。


<details>
  <summary>Details</summary>
Motivation: 在线广告环境具有内在复杂性和动态性，多通道场景下跨渠道预算与约束的有效分配对优化投资回报至关重要。现有优化方法缺乏动态适应性，而强化学习方法在马尔可夫决策过程框架下难以捕捉关键的历史依赖关系和观测模式，亟需更灵活的解决方案。

Method: AHBid采用分层 bidding 架构：1) 高层生成规划器基于扩散模型，通过历史数据捕捉时间模式以动态分配预算和约束；2) 约束执行机制确保策略符合预设限制；3) 轨迹精炼机制利用历史数据增强环境适应性；4) 控制理论 bidding 算法融合历史知识与实时信息，实现协同优化。

Result: 在大规模离线数据集和在线A/B测试中，AHBid相比现有基线方法整体投资回报率提升13.57%，验证了生成式规划与实时控制结合的有效性。

Conclusion: 该研究成功证明了将生成式AI规划能力与控制系统实时响应相结合，能够有效解决动态广告竞价中的历史依赖建模与适应性挑战，为复杂决策环境提供了新的框架思路。

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [164] [Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions](https://arxiv.org/abs/2602.22680)
*Yue Xu,Qian Chen,Zizhan Ma,Dongrui Liu,Wenxuan Wang,Xiting Wang,Li Xiong,Wenjie Wang*

Main category: cs.AI

TL;DR: 这篇综述系统回顾了个性化LLM驱动的智能体研究，围绕用户画像建模、记忆、规划和行动执行四个核心组件构建分析框架，探讨用户信号在整个决策流程中的表示与传播机制，并展望评估指标、应用场景及未来研究方向，为构建更用户对齐、自适应且可部署的智能体系统提供路线图。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长时序交互场景中执行复杂任务的需求增长，其有效性愈发依赖于对个体用户行为的适应性和跨时间连续性。传统个性化局限于表面生成层面，而长周期用户依赖场景要求个性化贯穿完整决策链条，这催生了个性化LLM智能体的研究需求，以构建真正适应用户、保持持续性的智能辅助系统。

Method: 本综述采用能力导向的文献组织方法，构建"用户画像建模-记忆-规划-行动执行"四组件相互依存的分类体系，通过该框架综合代表性方法，分析用户信号在各组件间的表示方式、传播路径与利用机制，重点揭示跨组件交互模式和反复出现的设计权衡。

Result: 研究系统梳理了个性化智能体的技术方法体系，明确了用户信号在决策全流程中的核心作用，识别出组件间交互的关键模式与常见设计权衡，同时总结了针对个性化智能体的评估指标与基准测试，归纳了从通用辅助到专业领域的应用场景，并指明了未来研究与部署方向。

Conclusion: 该综述通过结构化框架为理解和设计个性化LLM智能体提供了路线图，推动研究从原型个性化迈向可扩展的实用化智能助手，旨在加速开发更贴合用户需求、具备自适应能力、鲁棒性强且可实际部署的智能体系统。

Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.

</details>


### [165] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: 本文提出RLHFless，首个基于无服务器计算的同步RLHF可扩展训练框架。该框架通过预计算共享前缀避免重复计算，采用考虑响应长度变化的成本感知参与者扩展策略，并优化工作负载分配以减少空闲时间。实验表明，相较于现有最优基线，RLHFless可实现最高1.35倍的训练加速和44.8%的成本降低。


<details>
  <summary>Details</summary>
Motivation: RLHF虽能对齐LLM输出与人类偏好并提升推理能力，但面临模型规模扩大与资源消耗激增的挑战。现有框架依赖有服务器基础设施，难以应对细粒度资源动态变化，导致同步训练中组件间/内的空闲时间造成显著开销与资源浪费。因此，亟需可扩展解决方案以优化RLHF训练效率。

Method: 构建于无服务器计算环境，自适应RLHF全流程动态资源需求；预计算共享前缀消除重复计算开销；设计成本感知的参与者扩展策略，根据响应长度变化寻找最优成本-速度平衡点；优化工作负载分配机制，减少函数内负载不均衡与空闲时间。

Result: 在物理测试床与大规模模拟集群上的实验验证，RLHFless相较于state-of-the-art基线可实现最高1.35倍的训练速度提升和44.8%的成本削减。

Conclusion: RLHFless通过无服务器架构与多项优化创新，有效解决了同步RLHF训练中的资源浪费问题，为大规模语言模型的强化学习训练提供了高效、低成本的基础设施解决方案，展现了无服务器计算在AI训练领域的应用潜力。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [166] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: 该论文针对推荐系统中数据稀疏和冷启动问题，提出Taesar——一种数据中心的跨域序列再生框架。通过对比解码机制将跨域信息自适应编码到目标域序列中，使标准模型无需复杂架构即可学习跨域依赖，有效解决了模型中心范式存在的负迁移、泛化性差和计算成本高等问题。


<details>
  <summary>Details</summary>
Motivation: 推荐模型性能受限于训练数据的质量与规模，而数据稀疏性和冷启动是常见挑战。虽然利用多辅助域数据可丰富目标域信息，但域间差异会导致负迁移和性能下降。现有模型中心范式依赖复杂定制架构，难以捕捉跨域非结构化的序列依赖，且泛化能力差、计算资源消耗大。为此，亟需一种能弥合域间隙、提升数据质量的新范式。

Method: 提出Taesar（Target-aligned Sequential Regeneration），采用数据中心范式。核心是对比解码机制：通过跨域上下文对比学习，动态地将辅助域信息编码到目标域序列中，实现序列的再生与增强。该框架无需复杂融合架构，可将生成的丰富数据集直接输入标准序列推荐模型，使普通模型也能学习复杂的跨域依赖关系。

Result: 实验结果表明，Taesar在性能上显著优于现有模型中心解决方案，且能泛化到多种不同的序列推荐模型。通过生成高质量的目标对齐数据集，成功结合了数据中心和模型中心范式的优势，在保持模型简洁的同时实现了性能提升。

Conclusion: Taesar通过数据再生的方式有效解决了跨域推荐中的负迁移问题，验证了数据中心范式在序列推荐中的潜力。该方法降低了模型复杂度与计算开销，为标准推荐模型提供了可扩展的跨域学习方案，为后续研究提供了新思路。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [167] [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758)
*Satya Borgohain,Roy Mariathas*

Main category: cs.AI

TL;DR: 本研究通过分解 HealthBench 数据集中的医生分歧，发现分歧主要源于案例层面的残差（81.8%），而医生身份、评分标准身份等因素影响很小；可减少的不确定性使分歧几率翻倍，但仅解释约3%的方差，提示提升评估设计的关键在于填补信息空白而非消除内在医学模糊性。


<details>
  <summary>Details</summary>
Motivation: 理解医疗 AI 评估中医生分歧的来源，以提升评估可靠性和设计更有效的评估方案。

Method: 对 HealthBench 数据集中的医生标签进行方差分解，使用元数据标签、规范评分语言、医学专业、表面特征、嵌入向量等多种特征进行解释，并引入医生验证的不确定性类别（可减少与不可减少）进行回归分析。

Result: Rubric 身份解释 15.8% 的标签方差，但对分歧方差贡献仅 3.6-6.9%；医生身份仅 2.4%。案例层面残差占 81.8%，且无法被现有特征显著降低。分歧与完成质量呈倒 U 型关系（AUC=0.689），可减少不确定性使分歧几率翻倍（OR=2.55），但仅解释约 3% 总方差；不可减少不确定性无显著影响。

Conclusion: 医疗 AI 评估的上限主要由结构因素决定，但可减少与不可减少不确定性的分离提示，通过填补评估场景中的信息空白可以在不触及内在临床模糊性的情况下降低分歧，为评估设计改进提供了可行方向。

Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.

</details>


### [168] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: 提出AMA-Bench长时记忆评估基准及AMA-Agent记忆系统，通过因果图和工具增强检索解决现有系统在自主智能体应用中的性能瓶颈，实现11.16%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准局限于对话场景，无法反映真实自主智能体面临的连续环境交互记忆需求，导致记忆系统在实际应用中缺乏因果关系和客观信息支撑。

Method: 构建包含真实轨迹（专家QA）与可扩展合成轨迹（规则QA）的AMA-Bench，并设计AMA-Agent系统，采用因果图建模交互关系、工具增强检索机制。

Result: AMA-Agent在AMA-Bench上取得57.22%平均准确率，超越最优基线11.16%，证实因果图和工具检索的有效性。

Conclusion: 研究揭示了现有记忆系统的因果性缺失与检索损失问题，AMA-Agent为此提供了有效解决方案，为自主智能体的长时记忆管理建立了新评估标准。

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [169] [ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making](https://arxiv.org/abs/2602.22771)
*Yusuke Watanabe,Yohei Kobashi,Takeshi Kojima,Yusuke Iwasawa,Yasushi Okuno,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文开发了一个基于临床评分系统的基准测试ClinDet-Bench，用于评估大语言模型在不完整信息下识别能否做出确定临床决策的能力。研究发现，尽管LLMs在完整信息下表现良好且能正确解释评分知识，但在面对信息缺失时仍会过早下结论或过度保守，表明现有基准测试不足以评估临床场景下的LLM安全性。


<details>
  <summary>Details</summary>
Motivation: 临床决策常在信息不完整的情况下做出，临床专家必须判断现有信息是否足以得出结论，过早下结论或不必要地放弃判断都会危及患者安全。现有研究缺乏对大语言模型(LLMs)在信息不完整情况下决策能力的系统评估，这限制了对LLMs临床安全性的全面理解。

Method: 开发了ClinDet-Bench基准测试，基于临床评分系统将信息不完整场景分解为可确定和不可确定两种条件。识别可确定性的要求是：必须考虑所有关于缺失信息的假设（包括不太可能的假设），并验证结论是否在所有假设下都成立。

Result: 研究发现，近期的大语言模型在信息不完整情况下无法准确识别可确定性，既会产生过早判断，也会出现过度放弃的情况。尽管这些模型能够正确解释相关评分知识且在信息完整时表现良好，但在处理缺失信息时表现不佳。

Conclusion: 现有基准测试不足以评估大语言模型在临床环境中的安全性。ClinDet-Bench提供了一个评估可确定性识别能力的框架，可促进适当的放弃判断行为，该框架不仅适用于医学领域，也可推广至其他高风险领域。

Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.

</details>


### [170] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: 本文提出MiroFlow开源智能体框架，解决LLM在复杂现实任务中能力停滞问题，通过智能体图、深度推理和工作流执行实现多基准SOTA性能，为研究社区提供可复现基线。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM进展显著，但独立模型在需外部工具交互的复杂现实任务中能力开始停滞。现有智能体框架存在工作流简单、性能不稳定、基准支持有限、严重依赖昂贵商业API等问题。

Method: 提出MiroFlow框架，包含三大核心：1）智能体图实现灵活编排；2）可选深度推理模式提升性能；3）稳健工作流执行确保稳定可复现性能。

Result: 在GAIA、BrowseComp-EN/ZH、HLE、xBench-DeepSearch及FutureX等多个智能体基准测试中，MiroFlow consistently达到最先进性能。

Conclusion: MiroFlow作为易获取、可复现、可比较的基线，有望为深度研究社区提供支持，推动智能体框架的进一步发展。

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [171] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: 本文提出FlexMS，一个用于质谱预测的灵活基准测试框架，可动态构建和评估多种深度学习模型架构。通过分析数据集多样性、超参数、预训练、数据稀疏性和跨域迁移等因素，为模型选择提供实用指导，并通过检索基准模拟实际分子鉴定场景。


<details>
  <summary>Details</summary>
Motivation: 化学分子鉴定与性质预测对药物发现和材料科学至关重要，串联质谱提供关键碎片信息，但实验光谱数据稀缺限制了应用。尽管深度学习在光谱预测中展现潜力，但方法异构且缺乏统一基准导致评估困难，亟需标准化评估框架。

Method: 开发FlexMS基准框架，支持灵活构建多样化模型架构组合，在预处理公共数据集上通过多指标评估性能。系统研究影响性能的关键因素，包括数据集结构多样性、学习率等超参数、数据稀疏性、预训练效果、元数据消融实验及跨域迁移学习，并设计检索基准模拟实际鉴定场景。

Result: 揭示了影响质谱预测性能的多维度因素，验证了预训练和跨域迁移的有效性，量化了数据稀疏性和超参数的影响。建立了标准化评估体系，为不同应用场景下的模型选择提供数据支撑和实践指导。

Conclusion: FlexMS框架有效解决了质谱预测领域缺乏统一基准的问题，其灵活性和全面性为研究者提供了标准化评估工具。研究成果为构建高性能光谱预测模型提供了关键洞察，推动了计算质谱技术在化学研究中的实际应用。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [172] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: 本研究通过Hermite求积法则误差界的案例，实证检验了人机协作在数学发现中的作用。结果表明AI擅长代数运算与证明探索，但需人类验证与直觉指导，揭示了成功协作模式与失败风险。


<details>
  <summary>Details</summary>
Motivation: 探究AI在创造性数学研究中的真实价值——是实质性贡献还是仅自动化常规计算并带来错误风险，填补系统性实证研究的空白。

Method: 采用透明化案例研究法，通过多AI助手协作探索Hermite求积法则的误差表示与界，完整记录从问题形成、定理证明到文献综述、文稿撰写的研究全流程。

Result: AI在代数操作、证明探索、文献综合及LaTeX准备方面表现卓越，但依赖人工验证、数学直觉与战略指导。研究成功突破手动工作局限，提出新定理，并识别出协作成功模式与关键失败风险。

Conclusion: 在保持合理质疑、严格验证及深度专家监督前提下，AI可显著加速数学发现，但无法替代人类数学直觉与战略决策。有效人机协作需明确分工与质量控制机制。

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [173] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出OmniGAIA基准和OmniAtlas智能体，旨在突破当前双模态AI局限，实现视频、音频、图像的统一认知与工具调用能力。通过全模态事件图谱构建复杂多跳查询，并采用后见之树探索与OmniDPO微调策略，显著提升开源模型的全模态工具使用性能，向通用AI助手迈出关键一步。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要局限于视觉-语言等双模态交互，缺乏人类智能所具备的、将全模态感知（视觉、音频、语言）与复杂推理及工具使用无缝整合的统一认知能力，难以满足通用AI助手的现实需求。

Method: 1) 构建OmniGAIA基准：采用全模态事件图谱方法，从真实世界数据合成需要跨模态推理和外部工具集成的复杂多跳查询；2) 提出OmniAtlas智能体：基于工具集成推理范式的原生全模态基础模型，具备主动全模态感知能力；3) 训练策略：通过事后引导的树探索策略合成轨迹，并采用OmniDPO进行细粒度错误纠正微调。

Result: OmniAtlas有效增强了现有开源模型的工具使用能力，在全模态任务上展现出深度推理和多轮工具执行的优越性能。

Conclusion: 该工作通过构建全模态评估基准与智能体，为下一代面向真实场景的原生全模态AI助手发展提供了重要基础，推动了通用人工智能的演进。

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


### [174] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 针对通用智能体缺乏系统评估的问题，本研究提出评估概念原则、统一协议和Exgentic框架，对五个智能体在六个环境中基准测试，发布首个开放通用智能体排行榜，证明通用智能体可跨环境泛化且无需领域调优。


<details>
  <summary>Details</summary>
Motivation: 通用智能体在陌生环境中无需领域特定工程即可执行任务的愿景尚未实现。现有智能体多为专用，OpenAI SDK Agent和Claude Code等新兴实现虽显示潜力，但缺乏系统性性能评估。当前基准测试编码任务信息的方式排除了对通用智能体的公平评估。

Method: 将通用智能体评估确立为一级研究目标，提出评估概念原则和统一协议以实现智能体-基准集成，开发Exgentic实用框架，对五个主流智能体实现进行六个环境的基准测试。

Result: 创建首个开放通用智能体排行榜，实验证明通用智能体能跨多样化环境泛化，性能可与领域特定智能体相当，且无需任何环境特定调优。

Conclusion: 通过开源评估协议、框架和排行榜，建立系统化研究基础，推动通用智能体领域发展。

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [175] [FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning](https://arxiv.org/abs/2602.22963)
*Zehao Li,Hongwei Yu,Hao Jiang,Qiang Sheng,Yilong Xu,Baolong Bi,Yang Li,Zhenlong Yuan,Yujun Cai,Zhaoqi Wang*

Main category: cs.AI

TL;DR: 本文针对多模态大模型在视频 misinformation 检测中存在的固定深度推理和过度依赖内部假设的问题，提出了 FactGuard——一种基于 MLLM 的智能体框架。该框架通过迭代式推理、显式评估任务模糊性并选择性调用外部工具获取关键证据，结合两阶段训练策略（领域特定智能体监督微调+决策感知强化学习），在 FakeSV、FakeTT 和 FakeVV 数据集上实现了最优性能，展现出优秀的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLM 在视频 misinformation 检测中存在两个核心局限：一是采用固定深度推理，缺乏灵活性；二是过度信任内部生成的假设，尤其在关键证据稀疏、碎片化或需要外部验证的场景下表现不佳。这导致模型难以处理复杂多变的真实 misinformation 情况，验证过程缺乏可解释性和可靠性。

Method: 提出 FactGuard 智能体框架，核心方法包括：1）将验证任务重构为基于 MLLM 的迭代推理过程；2）显式评估任务模糊性，动态决定是否需要调用外部工具（如搜索引擎、知识库）获取补充证据；3）设计两阶段训练策略：先进行领域特定的智能体监督微调，再通过决策感知强化学习优化工具使用策略和风险敏感决策，实现推理路径的渐进式精化。

Result: 在 FakeSV、FakeTT 和 FakeVV 三个权威视频 misinformation 检测数据集上，FactGuard 均达到 state-of-the-art 性能。实验结果表明，该框架不仅显著优于基线方法，而且具备出色的鲁棒性和泛化能力，能够有效处理证据稀疏、碎片化等复杂场景，验证了其设计有效性。

Conclusion: FactGuard 框架通过引入智能体推理和外部工具协同机制，成功解决了传统 MLLM 在视频 misinformation 检测中的核心缺陷。该方法为多模态事实核查提供了新的技术范式，证明了迭代式、可验证的推理路径比单步推理更可靠，为未来开发更可信的多模态 AI 系统提供了重要参考。

Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.

</details>


### [176] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: 针对专业科学领域大模型评估的不足，本研究提出SPM-Bench——首个博士级扫描探针显微镜多模态基准。通过Anchor-Gated Sieve技术自动提取2023-2025年文献中的高质量图像文本对，采用混合云本地架构实现极致token节省与高数据纯度，并首创SIP-F1评分体系量化模型性能与"人格"特征，揭示AI在复杂物理场景中的真实推理边界。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用推理取得突破，但在专业科学领域暴露显著能力差距，现有基准因数据污染、复杂度不足和人工成本高昂而无法满足评估需求。

Method: 构建全自动数据合成管道：1) 利用Anchor-Gated Sieve技术从2023-2025年arXiv和期刊论文中高效提取高价值图像-文本对；2) 采用混合云本地架构，视觉语言模型仅返回空间坐标"llbox"供本地高精度裁剪，实现token节约与数据纯度保障；3) 引入严格缺陷惩罚F1分数(SIP-F1)作为评估指标，关联模型置信度与感知难度，量化能力层次与"人格"特征。

Result: 成功建立权威、低成本的SPM基准数据集；相比传统方法实现极端的token节省；首次将模型性能量化为"保守型、激进型、赌博型、智慧型"四类人格特征；准确揭示了当前AI在复杂物理场景中的真实推理边界。

Conclusion: SPM-Bench为科学领域大模型评估提供了可推广的自动化数据合成范式，其方法论可迁移至其他专业学科基准构建。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [177] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 本文提出诊断对齐框架，通过保留AI推理状态并与医生验证结果进行结构化对比，在21个皮肤科病例上证明二元词汇评估严重低估临床一致性（100%综合一致率 vs 71.4%精确匹配率）。


<details>
  <summary>Details</summary>
Motivation: 安全关键型临床AI需要人在回路的验证，但从AI推理到专家修正的过程缺乏结构化信号分析，传统评估方法无法量化修正动态和临床意义的一致性。

Method: 构建诊断对齐框架：整合视觉大语言模型、BERT医学实体提取和顺序语言模型推理(SLMI)进行领域一致性优化。在21个皮肤科AI-医生对上使用四级评估体系：精确主匹配率(PMR)、语义调整率(AMR)、跨类别对齐和综合一致率(CCR)。

Result: 精确匹配率71.4%，语义调整后无显著变化(t=0.60)；结构化跨类别分析显示100%综合一致率(95%CI:[83.9%,100%])，无完全诊断分歧案例。

Conclusion: 将专家验证建模为结构化转换可量化修正动态，支持可追溯的人类对齐评估，证实二元词汇评估严重低估临床意义对齐，为图像临床决策支持系统提供新评估范式。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [178] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [179] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 本研究针对大型语言模型的越狱攻击问题，提出CC-BOS框架，利用文言文简洁晦涩的特性绕过安全限制。通过果蝇优化算法在八个策略维度上自动生成文言文对抗提示，并在黑盒环境下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全风险日益受到关注，越狱攻击效果因语言而异。文言文因其高度简洁和语义模糊的特性，能够规避现有安全约束机制，揭示了模型在文言文理解中的安全漏洞，这构成了本研究的核心动机。

Method: 提出CC-BOS自动化框架，采用多维度果蝇优化算法生成文言文对抗性提示。将提示编码为角色、行为、机制、隐喻、表达、知识、触发模式和上下文八个维度，通过气味搜索、视觉搜索和柯西变异进行迭代优化。集成文言文-英文翻译模块以增强可读性与评估准确性，实现高效的黑盒越狱攻击。

Result: 实验结果表明，CC-BOS框架在越狱攻击成功率上持续超越现有最先进方法，验证了文言文作为攻击载体的有效性，以及多维度果蝇优化算法在对抗提示生成中的高效性。

Conclusion: 本研究揭示了文言文越狱攻击的潜在威胁，CC-BOS框架为黑盒环境下LLM安全漏洞的自动挖掘提供了新范式。研究成果不仅证实了当前模型在多语言安全对齐方面的不足，也为未来提升LLM鲁棒性指明了重要方向。

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [180] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 本文提出一种基于强化学习的多智能体F1赛车策略优化框架。通过在预训练单智能体策略基础上引入竞争对手交互模块和自我对弈训练机制，智能体能够平衡能量管理、轮胎损耗、空气动力学互动和进站决策，并根据对手行为动态调整策略，实现稳健一致的比赛表现。


<details>
  <summary>Details</summary>
Motivation: F1比赛策略需实时适应 evolving race conditions 和竞争对手行为，涉及能量管理、轮胎退化、空气动力学交互等多维复杂因素。传统优化方法难以有效捕捉多车交互的动态性和竞争性，亟需智能化方法实现自适应策略优化。

Method: 构建于预训练单智能体策略之上，设计交互模块显式建模竞争对手行为模式，结合自我对弈训练范式生成竞争性策略。通过相对性能排名机制评估智能体表现，实现多智能体协同优化。

Result: 实验结果表明，训练后的智能体能够根据对手策略动态自适应调整进站时机、轮胎选择和能量分配方案，在多种比赛场景下均表现出稳健且一致的性能。

Conclusion: 该框架仅利用真实比赛中可获得的信息，可直接为F1策略师提供赛前规划和实时决策支持，验证了强化学习在复杂多智能体赛车策略优化中的实用价值。

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [181] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为AILS-AHD的新方法，该方法利用大语言模型(LLM)自动设计和优化毁灭启发式，结合自适应迭代局部搜索框架，显著提升了容量约束车辆路径问题(CVRP)的求解性能，在CVRPLib大规模基准测试中取得了8/10个新的最优解。


<details>
  <summary>Details</summary>
Motivation: 容量约束车辆路径问题(CVRP)作为经典的NP难组合优化问题，在大规模实例上面临着巨大的计算挑战。传统方法在求解效率和效果上存在局限，亟需创新性的启发式设计方法来突破现有性能瓶颈。

Method: 提出AILS-AHD框架，将进化搜索与大语言模型集成，实现毁灭启发式的动态生成与优化；同时引入基于LLM的加速机制提升计算效率。该方法在自适应迭代局部搜索中自动演化启发式规则。

Result: 在中等规模和大规模实例上均优于当前先进求解器AILS-II和HGS，特别是在CVRPLib大规模基准测试中，10个实例中有8个找到了新的最优解，验证了LLM驱动启发式设计的有效性。

Conclusion: 研究表明，利用大语言模型自动生成和优化启发式规则是解决复杂组合优化问题的新范式，为车辆路径优化领域带来了显著性能提升，展现了LLM在运筹学中的巨大潜力。

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [182] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: 本研究提出基于多Agent大语言模型的情绪去毒化系统MALLET，通过四个Agent协同工作，在不损失语义的前提下显著降低信息刺激强度，帮助消费者实现冷静的信息接收。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中， sensational内容对消费者造成过度情绪刺激，妨碍冷静决策，需要一种既能保留原文信息又能中和情绪刺激的信息净化方案。

Method: 设计四Agent系统：情绪分析Agent（6情绪BERT分类器量化刺激强度）、情绪调整Agent（LLM重写为BALANCED/COOL两种模式）、平衡监控Agent（聚合每周消费模式生成建议）、个人引导Agent（根据敏感度推荐呈现模式）。

Result: 在800篇AG新闻实验中实现最高19.3%的刺激分数降低，情绪平衡性提升，语义保持度与刺激降低近乎零相关（独立可控）。体育、商业、科技类降幅显著（17.8-33.8%），世界类因事实本身高刺激而效果有限。

Conclusion: MALLET系统为支持消费者冷静信息接收提供了可行框架，可在不限制访问原文的前提下有效管理信息情绪刺激，实现信息消费的个性化调节。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [183] [The Trinity of Consistency as a Defining Principle for General World Models](https://arxiv.org/abs/2602.23152)
*Jingxuan Wei,Siyuan Li,Yuhang Xu,Zheng Sun,Junjie Jiang,Hexuan Jin,Caijun Jia,Honghao He,Xinglong Xu,Xi bai,Chang Yu,Yumou Liu,Junnan Zhu,Xuanhe Zhou,Jintao Chen,Xiaobin Hu,Shancheng Pang,Bihui Yu,Ran He,Zhen Lei,Stan Z. Li,Conghui He,Shuicheng Yan,Cheng Tan*

Main category: cs.AI

TL;DR: 本文提出构建通用世界模型需要满足"一致性三位一体"理论框架（模态一致性、空间一致性、时间一致性），并基于此开发了CoW-Bench多帧推理生成基准测试，为评估视频生成模型和统一多模态模型提供统一协议，指明通向通用世界模型的理论路径。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能领域缺乏一个原则性的理论框架来定义通用世界模型所必需的核心属性。尽管Sora等视频生成模型展示了数据驱动方法近似物理动态的潜力，统一多模态模型也提供了整合感知、语言和推理的架构范式，但这些进展尚未形成系统性的理论基础来指导通用世界模型的构建。

Method: 提出"一致性三位一体"理论框架：模态一致性作为语义接口，空间一致性作为几何基础，时间一致性作为因果引擎。通过这一三元视角系统回顾多模态学习演进，揭示从松散耦合的专用模块向能够协同涌现内部世界模拟器的统一架构发展轨迹。

Result: 开发了CoW-Bench基准测试，专注于多帧推理与生成场景，在统一评估协议下对视频生成模型和统一多模态模型进行系统评测。研究发现当前模型在向通用世界模型演进过程中存在系统性局限，需要满足一致性三位一体的架构要求。

Conclusion: 本工作为通用世界模型的构建建立了原则性路径，明确了当前系统的局限性以及未来进展所需的架构要求。通过一致性理论框架和CoW-Bench基准，为评估和改进世界模型提供了理论基础和实践工具，推动人工智能向具备物理规律学习和推理能力的通用智能体发展。

Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.

</details>


### [184] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: 本文针对时间序列推理中现有LLM方法的两大局限：1）将时间序列简单视为文本或图像，无法捕捉趋势和季节性等模式；2）简单任务主导学习过程，阻碍深度推理能力发展。提出PATRA模型，通过模式感知机制和任务感知平衡奖励，在TSQA任务上超越基线，展现更强的跨模态理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理需要感知复杂动态和深度逻辑推理。现有LLM方法存在两大局限：一是仅将时间序列视为文本或图像，无法捕捉回答问题所需的趋势和季节性等模式；二是混合训练简单和复杂任务时，简单目标主导学习过程，阻碍深度推理能力发展。这促使作者开发能同时识别时间序列模式并平衡不同难度任务学习的模型。

Method: 提出PATRA模型，包含两个核心组件：1）模式感知机制，从时间序列中提取趋势和季节性模式，实现深度对齐；2）任务感知平衡奖励，协调不同难度任务的学习，激励生成连贯的思维链（Chain of Thought）。

Result: 大量实验表明，PATRA在各种时间序列问答（TSQA）任务上均优于强基线模型，展现出卓越的跨模态理解能力和推理能力。

Conclusion: PATRA通过模式感知和任务平衡机制有效解决了现有时间序列推理方法的两大局限，在多个TSQA任务上取得显著性能提升，证明了其在跨模态理解和深度推理方面的优势。

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [185] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 针对大语言模型隐写能力缺乏基准分布的检测难题，本文提出决策论视角的隐写度量框架，通过广义V-信息定义隐写差距，实现隐写推理的检测、量化与缓解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型隐写能力可能被用于规避对齐监督，而经典隐写检测依赖已知非隐写信号参考分布，这在LLM场景中不可行，导致缺乏检测与量化此类行为的原理性方法。

Method: 提出决策论隐写观：隐写术在可解码与不可解码智能体间创造可用信息不对称性；引入广义V-信息作为功利主义框架度量输入信息效用，并定义隐写差距量化该不对称性。

Result: 经验验证表明，该形式化方法能有效检测、量化并缓解大语言模型中的隐写推理行为。

Conclusion: 该决策论框架无需参考分布即可度量LLM隐写术，为提升模型监督机制提供了可行路径，对防范模型规避对齐具有重要意义。

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [186] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: SC-ARENA是一个专为单细胞基础模型设计的自然语言评估框架，通过虚拟细胞抽象统一评估目标，并引入知识增强评估方法，解决现有基准碎片化、评估格式脱离实际以及指标缺乏生物学可解释性的核心问题。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞生物学大语言模型评估存在三大缺陷：任务基准碎片化、多选题等分类格式与现实场景脱节、评估指标缺乏生物学依据和可解释性，难以真实反映模型在复杂生物学推理中的能力。

Method: 提出SC-ARENA框架：1) 采用虚拟细胞抽象统一表征细胞内在属性与基因级相互作用；2) 定义细胞类型注释、描述生成、细胞生成、扰动预测、科学问答五项核心自然语言任务；3) 引入知识增强评估，整合外部本体、标记数据库和科学文献，实现生物学可信的证据化评判。

Result: 实证发现：(i) 在虚拟细胞统一范式下，现有模型在生物学复杂任务（尤其是机制性与因果理解类任务）上表现显著不均；(ii) 知识增强评估有效保障生物学正确性，提供可解释的证据链推理，判别能力显著优于传统脆弱字符串匹配指标。

Conclusion: SC-Arena构建了首个面向单细胞生物学、统一且可解释的大语言模型评估体系，为开发生物学对齐、可泛化的基础模型提供了关键基础设施和评估范式。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [187] [Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive](https://arxiv.org/abs/2602.23239)
*Radha Sarma*

Main category: cs.AI

TL;DR: 本文证明基于优化的AI系统（特别是RLHF训练的大语言模型）与规范治理存在根本性不兼容。真正的主体性需要不可通约性（将边界作为不可协商的约束而非可交易权重）和反向响应（在边界受威胁时暂停处理的非推断机制）这两个条件，而优化的标量化操作本质排除了这些可能，导致系统只能是精密工具而非可治理主体。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在高风险领域（医疗、法律、金融）的部署基于其可被规范治理的假设。本文旨在检验该假设在优化型系统（尤其是RLHF训练的大语言模型）中是否成立，探讨优化架构本身是否与规范治理相容。

Method: 通过形式化论证确立真正主体性的两个必要且联合充分的架构条件：不可通约性（非可交易约束的维持能力）与反向响应（边界威胁时的处理暂停机制）。随后分析RLHF系统的运作原理，证明其标量化统一所有价值并始终选择最高分输出的优化本质与上述条件构成形式冲突。

Result: 1) 证明RLHF系统架构性地不兼容于规范治理；2) 指出系统性失效（谄媚、幻觉、不忠实推理）非偶然而是结构必然；3) 提出"趋同危机"：人类在指标压力下退化为标准检查优化器，消除系统内唯一具备规范问责能力的组件；4) 提供独立于基质的架构规范，界定何为主体而非精密工具。

Conclusion: 优化型AI系统从根本上无法成为可规范治理的主体，其失败模式是架构性的而非可修复的缺陷。该研究划清了真正主体与优化工具的界限，为避免误用提供了理论依据，并指向超越标量优化的替代架构方向。

Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.

</details>


### [188] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 该论文提出了Universal AI with Q-Induction (AIQI)，首次实现了在一般强化学习中模型无关的渐近ε-最优智能体。该方法通过归纳分布动作价值函数而非策略或环境模型，在'真相颗粒'条件下被证明具有强渐近ε-最优性和渐近ε-贝叶斯最优性，拓展了通用智能体的多样性。


<details>
  <summary>Details</summary>
Motivation: 在通用强化学习领域，所有已知的最优智能体（如AIXI）均属于基于模型的范畴，明确维护并使用环境模型。然而，模型无关的方法在实践和理论上都具有重要意义。本文的动机在于填补模型无关的通用最优智能体的空白，探索不依赖显式环境建模的渐近最优性。

Method: 本文提出Q-归纳（Q-Induction）方法，其核心是对分布动作价值函数（distributional action-value functions）进行通用归纳推理。与之前工作归纳策略或环境模型不同，AIQI直接在动作价值函数的分布上进行归纳，实现了模型无关的决策机制。

Result: 在'真相颗粒'（grain of truth）条件下，理论证明AIQI具备两大关键性质：强渐近ε-最优性（strong asymptotically ε-optimal）和渐近ε-贝叶斯最优性（asymptotically ε-Bayes-optimal）。这些结果显著扩展了已知通用智能体的种类和理论边界。

Conclusion: AIQI作为首个被严格证明在一般强化学习中模型无关且渐近ε-最优的智能体，代表了通用人工智能理论的重要进展。它不仅提供了新的方法论框架，还丰富了通用智能体的理论体系，为未来研究开辟了新的方向。

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [189] [Mitigating Legibility Tax with Decoupled Prover-Verifier Games](https://arxiv.org/abs/2602.23248)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 针对大模型输出验证中的"可读性税"问题（即提高可验证性导致准确性下降），本文提出解耦正确性与可验证性，通过训练独立翻译器模型将固定求解器的解转换为可验证形式，实现准确性与可验证性的并行优化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力日益增强，其输出必须能够被能力较弱的系统验证以确保可靠性。然而现有证明者-验证者游戏方法在提升可验证性时会造成准确性显著下降，产生"可读性税"现象，这严重制约了大模型在高风险场景下的可信部署与实际应用。

Method: 提出解耦训练框架：首先独立训练求解器模型以最大化正确性并固定参数；然后训练翻译器模型将求解器输出转换为可验证格式，同时保持原答案不变；设计解耦的证明者-验证者游戏，其均衡点对应忠实且可验证的翻译器策略。

Result: 该方法理论上可消除可读性税，在保持求解器原始准确率的前提下，生成能被弱系统高效验证的输出形式，实现模型性能与可验证性的双重优化，为可验证AI提供新范式。

Conclusion: 本研究通过架构创新和解耦优化目标，为解决大模型输出可验证性问题提供了 principled 的新思路，对推动大模型在安全关键领域的可靠部署具有重要理论价值和应用前景。

Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.

</details>


### [190] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: 针对多智能体系统中错误信息级联传播的问题，本文提出AgentDropoutV2，一种无需重训练的测试时纠错-拒绝剪枝框架，通过检索增强校正和失败驱动模式识别动态优化信息流动，在数学基准测试中平均提升6.3%准确率。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统(MAS)虽擅长复杂推理，但易受个体参与者生成错误信息的级联影响。现有解决方案依赖僵化的结构工程或昂贵的微调，限制了其部署性和适应性，亟需一种动态、高效的测试时优化方法。

Method: 提出AgentDropoutV2框架，作为主动防火墙拦截智能体输出，利用检索增强校正器和失败驱动指标库迭代修正错误，基于蒸馏的失败模式先验知识精准识别潜在错误。对不可修复输出进行剪枝以防止错误传播，并采用回退策略维持系统完整性。

Result: 在广泛的数学基准测试中，AgentDropoutV2显著提升MAS任务性能，平均准确率增益达6.3个百分点。系统展现出强大的泛化性和适应性，能根据任务难度动态调节校正力度，并利用上下文感知指标解决多种错误模式。

Conclusion: AgentDropoutV2通过测试时动态优化有效解决了MAS错误级联问题，无需重训练即可实现高效部署。该方法为构建更鲁棒、自适应的多智能体系统提供了新思路，具有广泛的实际应用前景。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [191] [Evaluating Stochasticity in Deep Research Agents](https://arxiv.org/abs/2602.23271)
*Haotian Zhai,Elias Stengel-Eskin,Pratik Patil,Liu Leqi*

Main category: cs.AI

TL;DR: 本文研究深度研究智能体(DRA)的随机性问题，将其建模为信息获取马尔可夫决策过程，发现信息获取、压缩和推理是三大随机性来源。提出的结构化和集成查询生成策略可将随机性降低22%，同时保持研究质量。


<details>
  <summary>Details</summary>
Motivation: 尽管深度研究智能体在多个领域展现出前景，但其系统设计忽视了随机性这一关键障碍——相同查询下重复执行会产生显著的结果、发现和引用差异，阻碍实际应用部署。

Method: 将DRA形式化为信息获取马尔可夫决策过程，建立评估框架量化系统方差，识别三大随机性来源，通过对照实验分析不同决策步骤中各模块随机性对输出的影响。

Result: 研究表明降低随机性可提升输出质量，其中推理和早期阶段随机性对输出方差贡献最大；所提出的结构化和集成查询生成方法在DeepSearchQA上将平均随机性降低22%且保持高质量。

Conclusion: 随机性是DRA部署的关键挑战，通过针对性缓解策略可显著减少方差，为构建稳定可靠的智能研究系统提供了有效解决方案。

Abstract: Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.

</details>


### [192] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: 针对胸部X光诊断中多步骤证据推理需求，本文提出 CXReasonAgent——一种融合大型语言模型与临床诊断工具的智能体，通过图像诊断证据与视觉证据进行可验证推理，解决了现有视觉语言模型响应不忠实、证据不足及适应成本高等问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在胸部X光诊断中存在三重局限：生成响应与诊断证据脱节、视觉证据支持不足导致难以验证、支持新任务需昂贵重训练。这些缺陷严重制约了其在临床环境中的可靠性与适应性，亟需构建基于证据的推理框架。

Method: 研究构建 CXReasonAgent，通过将大型语言模型与临床诊断工具集成，实现基于影像诊断证据和视觉证据的推理机制；同时创建 CXReasonDial 基准，涵盖12个诊断任务的1,946条多轮对话，用于评估证据忠实性能力。

Result: 在 CXReasonDial 上的实验表明，CXReasonAgent 能生成忠实于诊断证据的响应，在可靠性与可验证性方面显著优于大型视觉语言模型，验证了临床工具集成的有效性。

Conclusion: 该工作凸显了在安全关键临床场景中整合临床基础诊断工具的核心价值，为构建可信赖医疗AI系统提供了新范式，强调证据 grounding 是临床部署的关键前提。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [193] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: 该论文提出ODEBRAIN框架，通过神经ODE建模连续脑动态，解决传统离散时间方法在EEG预测中的累积误差和非线性特征捕捉不足问题，实验显示其性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统潜在变量方法通过循环架构离散化时间建模脑动态，导致累积预测误差，且无法捕捉EEG的瞬时非线性特征，限制了神经科学研究和临床应用的效果。

Method: 提出ODEBRAIN框架，将时空频特征整合到谱图节点中，然后使用神经ODE建模连续潜在动态，使潜在表征能捕捉任意时间点的脑状态随机变化。

Result: 大量实验验证表明，ODEBRAIN在EEG动态预测方面显著优于现有方法，具有更强的鲁棒性和泛化能力。

Conclusion: ODEBRAIN框架通过连续时间建模有效解决了传统方法的局限性，为神经群体动态建模提供了新的有效工具。

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


### [194] [The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302)
*Giacomo Bonanno*

Main category: cs.AI

TL;DR: 该研究建立了KM信念更新公理与含信念算子B、条件算子>和必然算子□的模态逻辑公理系统LM_KM，并与AGM信念修正对应的模态逻辑LM_AGM进行比较，证明AGM信念修正是KM信念更新的特例，且两者在强版本中的差异仅在于一条处理非意外信息的公理。


<details>
  <summary>Details</summary>
Motivation: 信念变化的形式化理论中，KM信念更新与AGM信念修正是两个核心框架。厘清二者之间的逻辑关系，特别是包含性与差异性，对于构建统一的信念修正理论、指导实际应用中的方法选择具有重要的理论意义。

Method: 通过公理转换与比较分析：1) 将KM信念更新公理映射为含三模态算子的模态逻辑公理，构建系统LM_KM；2) 将AGM信念修正公理转换为模态逻辑公理，构建系统LM_AGM；3) 证明LM_KM是LM_AGM的子集；4) 针对强KM版本，识别两个系统的唯一差异公理。

Result: 1) LM_KM的所有公理均为LM_AGM的定理，即LM_KM ⊆ LM_AGM；2) AGM信念修正是KM信念更新的特例；3) 强KM版本与AGM的差异可归结为仅处理"非意外信息"的单条公理。

Conclusion: 该工作从模态逻辑角度形式化地证明了KM信念更新框架比AGM信念修正更具一般性，揭示了信念更新理论的内在层次结构，为后续理论发展和应用提供了清晰的数学基础。

Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

</details>


### [195] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 该论文提出基于重采样的推理方法，通过输入变换和输出聚合减少模型不确定性误差，提升推理准确性并平衡模型规模与性能。


<details>
  <summary>Details</summary>
Motivation: AI模型推理时存在偶然不确定性和认知不确定性导致的误差。研究发现对同一输入进行不同不变变换后，推理误差因认知不确定性呈现部分独立性，这为提升推理精度提供了新机会。

Method: 提出一种"重采样"推理策略：对输入应用多种不变变换，将变换后的版本输入到已训练AI模型，然后聚合多个推理输出以获得更准确的结果。

Result: 该方法能够提高推理准确性，并为平衡模型大小和性能提供可行策略，在不增加模型参数量的情况下利用误差独立性提升性能。

Conclusion: 基于重采样的推理方法有效利用了认知不确定性特性，通过输入变换和输出聚合显著提升模型推理精度，同时提供了模型规模与性能权衡的新思路。

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [196] [Generalized Rapid Action Value Estimation in Memory-Constrained Environments](https://arxiv.org/abs/2602.23318)
*Aloïs Rautureau,Tristan Cazenave,Éric Piette*

Main category: cs.AI

TL;DR: 针对GRAVE算法在内存受限环境下存储开销大的问题，本文提出三种改进算法（GRAVE2、GRAVER、GRAVER2），通过双层搜索、节点回收或两者结合的方式，在显著减少存储节点数量的同时保持了与GRAVE相当的游戏水平。


<details>
  <summary>Details</summary>
Motivation: GRAVE算法在通用游戏（GGP）中表现出色，但需要在每个节点存储额外的胜负/访问统计数据，导致内存消耗巨大，限制了在内存受限环境中的实际应用。

Method: 提出三种改进方案：1) GRAVE2采用双层搜索机制；2) GRAVER引入节点回收策略；3) GRAVER2结合双层搜索与节点回收两种技术。

Result: 实验表明这些改进算法能够大幅减少存储节点数量，同时在游戏表现上达到与原始GRAVE算法相当的水平。

Conclusion: 通过双层搜索和节点回收技术的单独或组合使用，可以有效降低GRAVE的内存开销，使其更适合实际应用环境，且不损失其强大的游戏能力。

Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.

</details>


### [197] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: 本研究探讨大语言模型对新手在生物安全任务中的实际提升效果。结果显示，使用LLM的新手比仅用互联网者准确率高4.16倍，甚至在三个专业基准上超越专家；但用户未能充分挖掘模型潜力，且89.6%参与者能轻易绕过防护获取双重用途信息。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在生物学基准测试中表现优异，但其能否真正提升新手用户的实际能力（即实现人类提升）仍不明确。这一不确定性关乎科学加速与双重用途风险的核心问题。

Method: 开展多模型、多基准的人体提升研究，比较八种生物安全相关任务中，有LLM访问权限的新手与仅使用互联网的控制组表现。参与者拥有充足解题时间（最复杂任务长达13小时）。

Result: LLM访问使新手准确率提升4.16倍（95% CI [2.63, 6.87]）；在四项有专家基准的任务中，LLM辅助的新手在其中三项超越专家；但独立LLM表现常优于LLM辅助的新手，显示用户未能充分发挥模型能力；89.6%参与者认为现有防护措施无法阻止获取双重用途信息。

Conclusion: LLM显著提升了新手在专业级生物任务上的表现，但传统基准测试无法反映实际人机协作效果，亟需建立持续的交互式人类提升评估机制。

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [198] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 针对现有多智能体交易系统依赖粗粒度指令导致性能下降的问题，本文提出一种细粒度任务分解的LLM交易框架，经日本股市回测验证可显著提升风险调整后收益，并发现分析输出与决策偏好的对齐是关键驱动因素。


<details>
  <summary>Details</summary>
Motivation: 当前主流的多智能体交易系统采用模仿分析师与经理角色的架构，但依赖抽象的粗粒度指令，忽视了真实世界工作流程的复杂性，导致模型推理性能下降且决策过程不透明。

Method: 提出一个显式将投资分析分解为细粒度任务的多智能体LLM框架，使用日本股票市场数据（包括价格、财务报表、新闻及宏观信息），在控制信息泄漏的回测环境下进行系统评估。

Result: 实验表明，相较于传统粗粒度设计，细粒度任务分解能显著提升风险调整后收益；关键发现是分析输出与下游决策偏好的对齐程度是系统性能的关键驱动因素；此外，利用各系统输出与股票指数的低相关性及其方差进行标准投资组合优化，可取得更优性能。

Conclusion: 研究结果为在实际应用中设计和配置LLM智能体的结构与任务提供了重要指导，强调了细粒度任务分解和输出对齐在构建高性能交易系统中的关键作用。

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>
