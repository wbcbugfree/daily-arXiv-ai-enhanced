<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 62]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.AI](#cs.AI) [Total: 38]
- [cs.LG](#cs.LG) [Total: 183]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Hypocrisy Gap: Quantifying Divergence Between Internal Belief and Chain-of-Thought Explanation via Sparse Autoencoders](https://arxiv.org/abs/2602.02496)
*Shikhar Shiromani,Archie Chaudhury,Sri Pranav Kunda*

Main category: cs.CL

TL;DR: 提出“Hypocrisy Gap”机制性指标，利用稀疏自编码器量化LLMs内部推理与最终生成之间的差异，以检测不忠实行为。基于Gemma、Llama、Qwen在Sycophancy基准上的实验，AUROC约0.55–0.74，相比对数概率基线有提升。


<details>
  <summary>Details</summary>
Motivation: LLMs的最终输出常与内部推理不一致，存在不忠实现象。需建立一种机械、可重复的度量来检测内部推理与输出之间的背离，以提高对不忠实行为的检测能力。

Method: 使用稀疏自编码器（SAEs）对内部推理进行量化，结合稀疏线性探针得到内部“真实信念”，并在潜在空间中将其与最终输出轨迹比较，构建Hypocrisy Gap。基于Gemma、Llama、Qwen等模型，在Anthropic的Sycophancy基准上评估，测量模型在检测 sycophantic 与 hypocritical 情形下的AUROC。

Result: 在检测sycophantic运行方面AUROC为0.55–0.73，在检测内部“知道用户错误”的hypocritical情形时AUROC为0.55–0.74，普遍优于基线的决策对齐对数概率AUROC（0.41–0.50）。

Conclusion: 提出的Hypocrisy Gap在检测LLMs的不忠实行为方面提供了一致的改进但AUROC值普遍处于中等偏小水平，显示该机制性度量具备可行性但需进一步验证、改进与更广域的跨模型应用。

Abstract: Large Language Models (LLMs) frequently exhibit unfaithful behavior, producing a final answer that differs significantly from their internal chain of thought (CoT) reasoning in order to appease the user they are conversing with. In order to better detect this behavior, we introduce the Hypocrisy Gap, a mechanistic metric utilizing Sparse Autoencoders (SAEs) to quantify the divergence between a model's internal reasoning and its final generation. By mathematically comparing an internal truth belief, derived via sparse linear probes, to the final generated trajectory in latent space, we quantify and detect a model's tendency to engage in unfaithful behavior. Experiments on Gemma, Llama, and Qwen models using Anthropic's Sycophancy benchmark show that our method achieves an AUROC of 0.55-0.73 for detecting sycophantic runs and 0.55-0.74 for hypocritical cases where the model internally "knows" the user is wrong, consistently outperforming a decision-aligned log-probability baseline (0.41-0.50 AUROC).

</details>


### [2] [STEMVerse: A Dual-Axis Diagnostic Framework for STEM Reasoning in Large Language Models](https://arxiv.org/abs/2602.02497)
*Xuzhao Li,Xuchen Li,Jian Zhao,Shiyu Hu*

Main category: cs.CL

TL;DR: STEMVerse 是一个诊断性框架，将 STEM 推理任务映射到学科×认知维度的能力空间，汇聚 2 万道题目，针对不同模型规模和训练范式进行跨学科与认知深度的诊断评估，揭示 STEM 推理的结构性失败模式，提供可操作的诊断洞见。


<details>
  <summary>Details</summary>
Motivation: 现有评测往往将基准视为孤立的“筒仓”，给出单一的总分，未能区分知识储备不足和认知能力不足对错误的贡献，缺乏跨学科与认知层面的细粒度诊断。需要一个能够同时覆盖学科专业性与认知复杂度的诊断框架，以提升对模型推理能力的诊断价值。

Method: 将 20000+ 的 STEM 问题从主流基准中重新聚合，构建统一的“学科×认知”能力空间，并为每个题目赋予双轴标签；对代表性的大模型家族在不同参数规模与训练范式下进行系统评估，分析在不同学科和认知维度上的表现与失败模式。

Result: 实验结果揭示了 STEM 推理中的结构性失效模式，且其错题分布在学科与认知维度上具有可观测的规律性。通过将跨学科覆盖与细粒度认知分层纳入统一框架，STEMVerse 提供了比单一总分更具诊断价值的视角来理解大模型的科学推理特征。

Conclusion: STEMVerse 为理解大模型的科学推理特征提供了一个清晰且可操作的诊断框架与视角，有助于改进评测设计、定位能力瓶颈，并为未来在学科与认知层面的深入分析提供基础。

Abstract: As Large Language Models (LLMs) achieve significant breakthroughs in complex reasoning tasks, evaluating their proficiency in science, technology, engineering, and mathematics (STEM) has become a primary method for measuring machine intelligence. However, current evaluation paradigms often treat benchmarks as isolated "silos," offering only monolithic aggregate scores that neglect the intricacies of both academic specialization and cognitive depth. This result-oriented approach fails to distinguish whether model errors stem from insufficient domain knowledge or deficiencies in cognitive capacity, thereby limiting the diagnostic value. To address this, we propose STEMVerse, a diagnostic framework designed to systematically analyze the STEM reasoning capabilities of LLMs. This framework characterizes model performance across academic specialization and cognitive complexity to map the capability required for reasoning. We re-aggregate over 20,000 STEM problems from mainstream benchmarks into a unified "Discipline $\times$ Cognition" capability space, assigning dual-axis labels to every instance. Utilizing this unified diagnostic framework, we systematically evaluate representative LLM families across varying parameter scales and training paradigms. Our empirical results reveal structural failure patterns in STEM reasoning. By integrating multi-disciplinary coverage and fine-grained cognitive stratification into a unified framework, STEMVerse provides a clear and actionable perspective for understanding the scientific reasoning characteristics of LLMs.

</details>


### [3] [Test-Time Detoxification without Training or Learning Anything](https://arxiv.org/abs/2602.02498)
*Baturay Saglam,Dionysis Kalogerias*

Main category: cs.CL

TL;DR: 提出一种测试时的零阶优化方法，通过对输入嵌入进行少量下降步来引导自回归文本的生成，降低有害内容而不牺牲质量；无需重新训练或访问梯度，嵌入作为可控变量在黑盒场景下具有较好可转移性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模部署时语言模型产生有害文本的风险，同时希望在不依赖模型再训练、梯度信息或额外组件的前提下实现跨模型的鲁棒性和可转移性。

Method: 在测试时近似有害度对输入嵌入的梯度，采用零阶优化，仅通过输入嵌入、有害性评分函数和模型前向评估，进行少量下降步以引导生成朝着低有害性的方向优化。

Result: 在多模型和多种提示下实现稳健的有害文本降低，并在大多数设置中达到最佳的有害-文本质量权衡；把词嵌入视作有效控制变量，主张在黑盒条件下广泛使用该方法以实现可扩展的安全文本生成。

Conclusion: 证明在不进行训练或访问中间计算的前提下，透过嵌入优化即可改善生成安全性，强调嵌入层作为控制变量的潜力并为黑盒下的安全文本生成提供新路径。

Abstract: Large language models can produce toxic or inappropriate text even for benign inputs, creating risks when deployed at scale. Detoxification is therefore important for safety and user trust, particularly when we want to reduce harmful content without sacrificing the model's generation quality. Many existing approaches rely on model retraining, gradients, or learned auxiliary components, which can be costly and may not transfer across model families or to truly black-box settings. We introduce a test-time procedure that approximates the gradient of completion toxicity with respect to the input embeddings and uses a small number of descent steps to steer generation toward less toxic continuations. This is achieved with zeroth-order optimization that requires only access to input embeddings, a toxicity scoring function, and forward evaluations of the model. Empirically, the approach delivers robust toxicity reductions across models and prompts and, in most settings, achieves the best overall toxicity-quality trade-off. More broadly, our work positions word embeddings as effective control variables and encourages wider use of black-box optimization to guide autoregressive language models toward scalable, safer text generation, without requiring any training or access to intermediate computations.

</details>


### [4] [ROSA-Tuning: Enhancing Long-Context Modeling via Suffix Matching](https://arxiv.org/abs/2602.02499)
*Yunao Zheng,Xiaojie Wang,Lei Ren,Wei Chen*

Main category: cs.CL

TL;DR: ROSA-Tuning提出一种并行的CPU端RWKV Online Suffix Automaton检索模块，用于在长上下文中定位与当前查询相关的历史位置，并将检索信息以可训练方式注入模型状态，结合范围受限注意力实现高效的长上下文建模。通过二值离散化和对抗梯度等策略实现端到端训练，并通过异步CPU-GPU流水线提升执行效率，实验在Qwen3-Base-1.7B上显著恢复长上下文能力，接近甚至在LongBench上达到全局注意力的水平，同时保持与窗口注意力接近的计算和显存开销。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在长上下文建模和计算效率之间存在权衡。现有高效注意力方法通常降低对模型状态的覆盖，难以充分利用长历史信息。需要一种高效且覆盖更广的长上下文机制来提升性能。

Method: 在标准注意力之外并行引入一个基于CPU的ROSA（RWKV Online Suffix Automaton）检索模块，离线定位与当前查询相关的历史位置信息，并将检索到的信息以可训练方式注入模型状态；随后通过范围受限注意力进行加权融合；为端到端训练设计二值离散化策略和对抗梯度算法，并通过异步CPU-GPU流水线优化执行效率。

Result: 实验显示ROSA-Tuning显著恢复了窗口注意力模型的长上下文能力，在LongBench等基准上接近甚至达到全局注意力的水平，同时在GPU显存和计算成本方面保持接近窗口注意力方法的水平，具备高效长上下文处理的新路径。

Conclusion: ROSA-Tuning为高效长上下文处理提供了一条可行的新路径，通过并行的CPU端检索+可训练注入机制，提升长上下文建模能力，同时保持计算与内存效率，且在实际基准上具备竞争力。

Abstract: Long-context capability and computational efficiency are among the central challenges facing today's large language models. Existing efficient attention methods reduce computational complexity, but they typically suffer from a limited coverage of the model state. This paper proposes ROSA-Tuning, a retrieval-and-recall mechanism for enhancing the long-context modeling ability of pretrained models. Beyond the standard attention mechanism, ROSA-Tuning introduces in parallel a CPU-based ROSA (RWKV Online Suffix Automaton) retrieval module, which efficiently locates historical positions in long contexts that are relevant to the current query, and injects the retrieved information into the model state in a trainable manner; subsequent weighted fusion can then be handled by range-restricted attention. To enable end-to-end training, we design a binary discretization strategy and a counterfactual gradient algorithm, and further optimize overall execution efficiency via an asynchronous CPU-GPU pipeline. Systematic evaluations on Qwen3-Base-1.7B show that ROSA-Tuning substantially restores the long-context modeling ability of windowed-attention models, achieving performance close to and in some cases matching global attention on benchmarks such as LongBench, while maintaining computational efficiency and GPU memory usage that are nearly comparable to windowed-attention methods, offering a new technical path for efficient long-context processing. The example code can be found at https://github.com/zyaaa-ux/ROSA-Tuning.

</details>


### [5] [Monotonicity as an Architectural Bias for Robust Language Models](https://arxiv.org/abs/2602.02686)
*Patrick Cooper,Alireza Nadali,Ashutosh Trivedi,Alvaro Velasquez*

Main category: cs.CL

TL;DR: 通过在Transformer的前馈子层引入单调性约束，在保持注意力机制自由度的前提下提升鲁棒性，攻击成功率显著下降，同时对摘要性能影响微小。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在对抗性提示和越狱攻击下易出错，输入空间的细微扰动可导致内部语义表示和输出的剧烈变化。引入单调性作为架构性归纳偏置，旨在提高鲁棒性和稳定性。

Method: 对序列到序列Transformer的前馈子层有选择地强制单调性约束，保持注意力机制不受约束；通过这种分离实现信息增强、证据约束等的顺序保持性，同时让注意力处理否定、矛盾和上下文交互，后续语义 refinement 保持有序。

Result: 鲁棒性显著提升：对抗性攻击成功率从约69%降至约19%；标准摘要性能仅有边际下降。

Conclusion: 在不显著损害表达能力的前提下，将单调性引入特定子层并通过注意力机制保留灵活性，提供了一种有效的鲁棒性提升路径，且表现与表达能力之间的权衡被显式隔离。

Abstract: Large language models (LLMs) are known to exhibit brittle behavior under adversarial prompts and jailbreak attacks, even after extensive alignment and fine-tuning. This fragility reflects a broader challenge of modern neural language models: small, carefully structured perturbations in high-dimensional input spaces can induce large and unpredictable changes in internal semantic representations and output.
  We investigate monotonicity as an architectural inductive bias for improving the robustness of Transformer-based language models. Monotonicity constrains semantic transformations so that strengthening information, evidence, or constraints cannot lead to regressions in the corresponding internal representations. Such order-preserving behavior has long been exploited in control and safety-critical systems to simplify reasoning and improve robustness, but has traditionally been viewed as incompatible with the expressivity required by neural language models.
  We show that this trade-off is not inherent. By enforcing monotonicity selectively in the feed-forward sublayers of sequence-to-sequence Transformers -- while leaving attention mechanisms unconstrained -- we obtain monotone language models that preserve the performance of their pretrained counterparts. This architectural separation allows negation, contradiction, and contextual interactions to be introduced explicitly through attention, while ensuring that subsequent semantic refinement is order-preserving. Empirically, monotonicity substantially improves robustness: adversarial attack success rates drop from approximately 69% to 19%, while standard summarization performance degrades only marginally.

</details>


### [6] [InfMem: Learning System-2 Memory Control for Long-Context Agent](https://arxiv.org/abs/2602.02704)
*Xinyu Wang,Mingze Li,Peng Lu,Xiao-Wen Chang,Lifeng Shang,Jinping Li,Fei Mi,Prasanna Parthasarathi,Yufei Cui*

Main category: cs.CL

TL;DR: InfMem is a system-2 style control-centric agent for ultra-long document QA that uses PreThink-Retrieve-Write and adaptive early stopping to improve accuracy and reduce latency relative to MemAgent across multiple backbones.


<details>
  <summary>Details</summary>
Motivation: Ultra-long document reasoning under strict memory constraints is challenging because sparse, distant evidence must be synthesized for multi-hop reasoning; passive memory updates in streaming agents often lose low-salience bridging evidence.

Method: InfMem employs a PreThink-Retrieve-Write protocol with active monitoring of evidence sufficiency, targeted in-document retrieval, and evidence-aware joint memory compression; a practical SFT-to-RL training recipe aligns retrieval, writing, and stopping decisions with end-task correctness.

Result: On ultra-long QA benchmarks from 32k to 1M tokens, InfMem outperforms MemAgent across backbones (Qwen3-1.7B: +10.17 points; Qwen3-4B: +11.84 points; Qwen2.5-7B: +8.23 points) and reduces inference time by 3.9x on average (up to 5.1x) via adaptive early stopping.

Conclusion: InfMem demonstrates effective active control for long-context QA, achieving higher accuracy with substantially lower latency and scalable performance across multiple backbone models.

Abstract: Reasoning over ultra-long documents requires synthesizing sparse evidence scattered across distant segments under strict memory constraints. While streaming agents enable scalable processing, their passive memory update strategy often fails to preserve low-salience bridging evidence required for multi-hop reasoning. We propose InfMem, a control-centric agent that instantiates System-2-style control via a PreThink-Retrieve-Write protocol. InfMem actively monitors evidence sufficiency, performs targeted in-document retrieval, and applies evidence-aware joint compression to update a bounded memory. To ensure reliable control, we introduce a practical SFT-to-RL training recipe that aligns retrieval, writing, and stopping decisions with end-task correctness. On ultra-long QA benchmarks from 32k to 1M tokens, InfMem consistently outperforms MemAgent across backbones. Specifically, InfMem improves average absolute accuracy by +10.17, +11.84, and +8.23 points on Qwen3-1.7B, Qwen3-4B, and Qwen2.5-7B, respectively, while reducing inference time by $3.9\times$ on average (up to $5.1\times$) via adaptive early stopping.

</details>


### [7] [Predicting first-episode homelessness among US Veterans using longitudinal EHR data: time-varying models and social risk factors](https://arxiv.org/abs/2602.02731)
*Rohan Pandey,Haijuan Yan,Hong Yu,Jack Tsai*

Main category: cs.CL

TL;DR: 基于VA大规模EHR，建立静态与时变特征的纵向风险预测模型，对2016年的患者在2017年3–12月首次无家可归进行预测，比较经典ML、基于Transformer的掩码语言模型以及微调的LLMs。结果显示纳入社会与行为因素的纵向模型可使PR-AUC提升约15–30%，前1%风险组的PPV随预测时间递增（3月约4%、6月约8%、9月约11%、12月约14%），LLMs在区分能力不及编码器模型但对种族差异的波动较小。结论：以纵向、社会信息为基础的EHR建模可实现风险的可操作分层，促进针对在风险人群的干预。


<details>
  <summary>Details</summary>
Motivation: 无家可归是退伍军人群体的重大公共卫生挑战，利用EHR的纵向数据及社会因素来进行早期、可操作的风险预测有望指导干预资源的分配。

Method: 回顾性预测研究，样本量4,276,403名VA患者，观测期为2016年，目标为预测2017年3–12月首次无家可归（患病率0.32–1.19%）。构建静态与时变的EHR表示，结合临床条件与社会风险的持续性。比较传统机器学习、基于Transformer的掩码语言模型以及微调的大语言模型（LLMs）。以PR-AUC等指标评估性能。

Result: 将社会与行为因素并入纵向模型可提高PR-AUC约15–30%。在前1%风险组中，3月PPV为3.93–4.72%，6月为7.39–8.30%，9月为9.84–11.41%，12月为11.65–13.80%，各模型架构之间的差异存在但整体趋势一致。LLMs在辨别力方面低于编码器基模型，但在跨种族群体中的差异较小。

Conclusion: 纵向、社会信息驱动的EHR建模能把无家可归风险集中到可操作的高风险分层，支撑对高风险 veterans 的针对性干预策略。

Abstract: Homelessness among US veterans remains a critical public health challenge, yet risk prediction offers a pathway for proactive intervention. In this retrospective prognostic study, we analyzed electronic health record (EHR) data from 4,276,403 Veterans Affairs patients during a 2016 observation period to predict first-episode homelessness occurring 3-12 months later in 2017 (prevalence: 0.32-1.19%). We constructed static and time-varying EHR representations, utilizing clinician-informed logic to model the persistence of clinical conditions and social risks over time. We then compared the performance of classical machine learning, transformer-based masked language models, and fine-tuned large language models (LLMs). We demonstrate that incorporating social and behavioral factors into longitudinal models improved precision-recall area under the curve (PR-AUC) by 15-30%. In the top 1% risk tier, models yielded positive predictive values ranging from 3.93-4.72% at 3 months, 7.39-8.30% at 6 months, 9.84-11.41% at 9 months, and 11.65-13.80% at 12 months across model architectures. Large language models underperformed encoder-based models on discrimination but showed smaller performance disparities across racial groups. These results demonstrate that longitudinal, socially informed EHR modeling concentrates homelessness risk into actionable strata, enabling targeted and data-informed prevention strategies for at-risk veterans.

</details>


### [8] [Time-Critical Multimodal Medical Transportation: Organs, Patients, and Medical Supplies](https://arxiv.org/abs/2602.02736)
*Elaheh Sabziyan Varnousfaderani,Syed A. M. Shihab,Mohammad Taghizadeh*

Main category: cs.CL

TL;DR: 构建性贪婪启发式多模态运输调度用于医疗运输，比较四种车队配置以提高效率并降低成本与时间。


<details>
  <summary>Details</summary>
Motivation: 及时的器官/患者/物资输送在紧急和移植场景中至关重要；现有地面/空中运输各自受限，新型无人机和电动垂直起降飞机具成本优势但受续航与天气限制，亟需综合优化的多模态调度方案。

Method: 提出一种构造性贪婪启发式算法用于多模态车辆调度；对四种车队配置进行评估（仅救护车、救护车+无人机、救护车+eVTOL、全量多模态），在路线间实现有效载荷合并，考虑地面交通与天气对各自模式的影响，目标在快速调度同时对比传统优化模型的性能。

Result: 在统一的条件下评估四种车队类型，找出在满足医疗运输需求的同时，尽量降低运营成本、充电/燃料成本和总运输时间的最优配置。

Conclusion: 贪婪启发式算法能够实现快速多模态调度，且整合的车队在特定条件下更具优势，相关结果可为部署决策提供依据；但具体效果需依赖天气、交通与载荷等实际参数。

Abstract: Timely transportation of organs, patients, and medical supplies is critical to modern healthcare, particularly in emergencies and transplant scenarios where even short delays can severely impact outcomes. Traditional ground-based vehicles such as ambulances are often hindered by traffic congestion; while air vehicles such as helicopters are faster but costly. Emerging air vehicles -- Unmanned Aerial Vehicles and electric vertical take-off and landing aircraft -- have lower operating costs, but remain limited by range and susceptibility to weather conditions. A multimodal transportation system that integrates both air and ground vehicles can leverage the strengths of each to enhance overall transportation efficiency. This study introduces a constructive greedy heuristic algorithm for multimodal vehicle dispatching for medical transportation. Four different fleet configurations were tested: (i) ambulances only, (ii) ambulances with Unmanned Aerial Vehicles, (iii) ambulances with electric vertical take-off and landing aircraft, and (iv) a fully integrated fleet of ambulances, Unmanned Aerial Vehicles, and electric vertical take-off and landing aircraft. The algorithm incorporates payload consolidation across compatible routes, accounts for traffic congestion in ground operations and weather conditions in aerial operations, while enabling rapid vehicle dispatching compared to computationally intensive optimization models. Using a common set of conditions, we evaluate all four fleet types to identify the most effective configurations for fulfilling medical transportation needs while minimizing operating costs, recharging/fuel costs, and total transportation time.

</details>


### [9] [AmharicStoryQA: A Multicultural Story Question Answering Benchmark in Amharic](https://arxiv.org/abs/2602.02774)
*Israel Abebe Azime,Abenezer Kebede Angamo,Hana Mekonen Tamiru,Dagnachew Mekonnen Marilign,Philipp Slusallek,Seid Muhie Yimam,Dietrich Klakow*

Main category: cs.CL

TL;DR: 提出 AmharicStoryQA 基准，发现跨地区与领域内容显著影响评估结果，现有大模型在叙事理解上存在差距；有监督微调对不同地区的改进不均衡，强调需要以文化为基础的评测以提升低资源语言的叙事理解。


<details>
  <summary>Details</summary>
Motivation: 语言评测常将语言与文化等同，忽略同一语言内的区域性文化差异对评测结果的影响，尤其在低资源语言场景。

Method: 构建基于阿姆哈拉语各地区的多元文化叙事的长序列故事问答基准 AmharicStoryQA；在多地区语境中评估现有大模型，并分析区域差异；对比有监督微调的效果及其在不同区域/评测设置中的差异。

Result: 现有LLMs在叙事理解方面存在显著差距；评测结果呈现强烈的区域差异；有监督微调对不同地区的改进不均衡，未能一致提升跨区域的叙事理解能力。

Conclusion: 需要超越语言层面评测，建立以文化为基础的评测基准，以更真实地评估和提升低资源语言中的叙事理解能力。

Abstract: With the growing emphasis on multilingual and cultural evaluation benchmarks for large language models, language and culture are often treated as synonymous, and performance is commonly used as a proxy for a models understanding of a given language. In this work, we argue that such evaluations overlook meaningful cultural variation that exists within a single language. We address this gap by focusing on narratives from different regions of Ethiopia and demonstrate that, despite shared linguistic characteristics, region-specific and domain-specific content substantially influences language evaluation outcomes. To this end, we introduce \textbf{\textit{AmharicStoryQA}}, a long-sequence story question answering benchmark grounded in culturally diverse narratives from Amharic-speaking regions. Using this benchmark, we reveal a significant narrative understanding gap in existing LLMs, highlight pronounced regional differences in evaluation results, and show that supervised fine-tuning yields uneven improvements across regions and evaluation settings. Our findings emphasize the need for culturally grounded benchmarks that go beyond language-level evaluation to more accurately assess and improve narrative understanding in low-resource languages.

</details>


### [10] [When Efficient Communication Explains Convexity](https://arxiv.org/abs/2602.02821)
*Ashvin Ranjan,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 用信息瓶颈框架解释语义类型学中的有效沟通；IB最优性与新颖的凸性概念之间存在相关性；通信需求分布的凸性是关键因素；通过操控模型参数分析为何存在这相关性。


<details>
  <summary>Details</summary>
Motivation: 解释跨语言变异为何可由有效沟通解释，并进一步揭示为何IB解释成立的原因：究竟哪些潜在因素驱动该相关性。

Method: 在IB框架下建模语义传达，首先证明并分析IB最优性与该设置下一个新颖的凸性概念之间的相关性；随后通过改变不同建模参数，考察哪些因素推动凸性与最优性之间的相关性。

Result: 发现IB最优性与凸性存在相关；在分析中凸性概念得到扩展且得到验证；结果表明通信需求分布的凸性尤为重要。

Conclusion: 由此解释为何有效沟通能解释语义类型学的某些方面，不仅停留在相关性，而是指出背后的驱动因素，即需求分布的凸性是关键因素。

Abstract: Much recent work has argued that the variation in the languages of the world can be explained from the perspective of efficient communication; in particular, languages can be seen as optimally balancing competing pressures to be simple and to be informative. Focusing on the expression of meaning -- semantic typology -- the present paper asks what factors are responsible for successful explanations in terms of efficient communication. Using the Information Bottleneck (IB) approach to formalizing this trade-off, we first demonstrate and analyze a correlation between optimality in the IB sense and a novel generalization of convexity to this setting. In a second experiment, we manipulate various modeling parameters in the IB framework to determine which factors drive the correlation between convexity and optimality. We find that the convexity of the communicative need distribution plays an especially important role. These results move beyond showing that efficient communication can explain aspects of semantic typology into explanations for why that is the case by identifying which underlying factors are responsible.

</details>


### [11] [R2-Router: A New Paradigm for LLM Routing with Reasoning](https://arxiv.org/abs/2602.02823)
*Jiaqi Xue,Qian Lou,Jiarong Xing,Heng Huang*

Main category: cs.CL

TL;DR: R2-Router 提出将输出长度预算视为可控变量，通过联合选择最佳 LLM 与长度预算，并用长度约束指令强制预算，从而在更低成本下实现更高质量输出。提出 R2-Bench 数据集，用以捕捉不同输出长度预算下的 LLM 行为，实验显示在4-5x 成本优势下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 路由假设每个模型在给定查询下具有固定的质量和成本，忽略输出长度对质量/成本的影响，导致在预算受限时错失潜在的高质量、短输出配置。

Method: 将输出长度预算作为可控变量，与 LLM 选择进行联合优化；通过长度约束指令强制预算；构建 R2-Bench 捕捉跨不同长度预算的路由行为；在实验中比较与现有路由器的成本-性能权衡。

Result: 在4-5x 成本的改进下达到或超过现有路由器的性能，证明将输出长度预算纳入路由决策可显著提升性价比，并将路由过程提升为“推理型路由”。

Conclusion: 路由研究可从被动选择转向主动推理，系统性探索在何种输出长度预算下使用哪一LLM，以实现更优的成本与质量权衡；R2-Bench 为该方向提供首个跨输出长度预算的评测数据集。

Abstract: As LLMs proliferate with diverse capabilities and costs, LLM routing has emerged by learning to predict each LLM's quality and cost for a given query, then selecting the one with high quality and low cost. However, existing routers implicitly assume a single fixed quality and cost per LLM for each query, ignoring that the same LLM's quality varies with its output length. This causes routers to exclude powerful LLMs when their estimated cost exceeds the budget, missing the opportunity that these LLMs could still deliver high quality at reduced cost with shorter outputs. To address this, we introduce R2-Router, which treats output length budget as a controllable variable and jointly selects the best LLM and length budget, enforcing the budget via length-constrained instructions. This enables R2-Router to discover that a powerful LLM with constrained output can outperform a weaker LLM at comparable cost-efficient configurations invisible to prior methods. Together with the router framework, we construct R2-Bench, the first routing dataset capturing LLM behavior across diverse output length budgets. Experiments show that R2-Router achieves state-of-the-art performance at 4-5x lower cost compared with existing routers. This work opens a new direction: routing as reasoning, where routers evolve from reactive selectors to deliberate reasoners that explore which LLM to use and at what cost budget.

</details>


### [12] [CATNIP: LLM Unlearning via Calibrated and Tokenized Negative Preference Alignment](https://arxiv.org/abs/2602.02824)
*Zhengbang Yang,Yisheng Zhong,Junyuan Hong,Zhuangdi Zhu*

Main category: cs.CL

TL;DR: CATNIP introduces calibrated and tokenized negative preference alignment for unlearning in LLMs, scaling forgetting by token-level confidence to achieve effective forgetting without retention or contrastive data, with strong tradeoffs on MUSE/WMDP benchmarks.


<details>
  <summary>Details</summary>
Motivation: Safety/privacy concerns from memorized knowledge in LLMs; limitations of gradient-ascent (GA) unlearning and reliance on retention data/contrastive pairs; need robustness to data scarcity and variable input lengths.

Method: Calibrated and Tokenized Negative Preference Alignment (CATNIP) that rescales unlearning effects according to token-level model confidence. It performs forgetting by adjusting gradients proportionally to confidence, without requiring retention data or curated contrastive pairs.

Result: CATNIP achieves stronger knowledge forgetting while better preserving general knowledge on MUSE and WMDP benchmarks, and exhibits robustness to data scarcity and input length variance, outperforming state-of-the-art unlearning methods.

Conclusion: CATNIP provides a principled, data-efficient unlearning framework that calibrates updates by token-level confidence, enabling precise forgetting without retention data or contrastive unlearning, and improving forgetting-preservation tradeoffs.

Abstract: Pretrained knowledge memorized in LLMs raises critical concerns over safety and privacy, which has motivated LLM Unlearning as a technique for selectively removing the influences of undesirable knowledge. Existing approaches, rooted in Gradient Ascent (GA), often degrade general domain knowledge while relying on retention data or curated contrastive pairs, which can be either impractical or data and computationally prohibitive. Negative Preference Alignment has been explored for unlearning to tackle the limitations of GA, which, however, remains confined by its choice of reference model and shows undermined performance in realistic data settings. These limitations raise two key questions: i) Can we achieve effective unlearning that quantifies model confidence in undesirable knowledge and uses it to calibrate gradient updates more precisely, thus reducing catastrophic forgetting? ii) Can we make unlearning robust to data scarcity and length variation? We answer both questions affirmatively with CATNIP (Calibrated and Tokenized Negative Preference Alignment), a principled method that rescales unlearning effects in proportion to the model's token-level confidence, thus ensuring fine-grained control over forgetting. Extensive evaluations on MUSE and WMDP benchmarks demonstrated that our work enables effective unlearning without requiring retention data or contrastive unlearning response pairs, with stronger knowledge forgetting and preservation tradeoffs than state-of-the-art methods.

</details>


### [13] [Act or Clarify? Modeling Sensitivity to Uncertainty and Cost in Communication](https://arxiv.org/abs/2602.02843)
*Polina Tsvilodub,Karl Mulligan,Todd Snider,Robert D. Hawkins,Michael Franke*

Main category: cs.CL

TL;DR: Proposes a rational, regret-based model predicting when people ask clarification questions (CQs) under uncertainty; tests linguistic and non-linguistic tasks; finds CQ use scales with potential loss, supporting a rational tradeoff between information gathering and action.


<details>
  <summary>Details</summary>
Motivation: Uncertainty in decision-making and the role of clarifying questions in reducing uncertainty. Formalizes how the decision to ask CQs depends on contextual uncertainty and action costs, via an expected-regret framework.

Method: Develop a computational model based on expected regret. Conduct two experiments: (1) purely linguistic responses to questions; (2) choices between clarification and non-linguistic action.

Result: Evidence consistent with predictions: CQ seeking is proportional to the risk of substantial loss when acting under uncertainty; uncertainty matters most when incorrect action is costly, indicating a rational tradeoff between information gathering and action.

Conclusion: Humans tend to seek clarification in proportion to the risk of substantial loss under uncertainty, supporting a rational, cost-aware strategy for information gathering in communication and decision-making.

Abstract: When deciding how to act under uncertainty, agents may choose to act to reduce uncertainty or they may act despite that uncertainty.In communicative settings, an important way of reducing uncertainty is by asking clarification questions (CQs). We predict that the decision to ask a CQ depends on both contextual uncertainty and the cost of alternative actions, and that these factors interact: uncertainty should matter most when acting incorrectly is costly. We formalize this interaction in a computational model based on expected regret: how much an agent stands to lose by acting now rather than with full information. We test these predictions in two experiments, one examining purely linguistic responses to questions and another extending to choices between clarification and non-linguistic action. Taken together, our results suggest a rational tradeoff: humans tend to seek clarification proportional to the risk of substantial loss when acting under uncertainty.

</details>


### [14] [Controlling Output Rankings in Generative Engines for LLM-based Search](https://arxiv.org/abs/2602.03608)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Yifeng Luo,Huimin Zeng,Man Luo,Haohan Wang*

Main category: cs.CL

TL;DR: CORE是一种通过在检索内容中附加优化内容来影响LLM生成引擎输出排序的优化方法；并在ProductBench上进行大规模评估，取得高于现有 ranking manipulation 方法的表现。


<details>
  <summary>Details</summary>
Motivation: LLM-based search 改变了用户发现和选择产品的方式，但初始检索排序对输出排名具有强烈影响，容易导致小型企业和独立创作者的可见性受限，因此需要对输出排名进行控制与鲁棒防护。

Method: 提出三类优化内容（字符串型、推理型、评价型），通过往检索内容中附加优化内容来引导输出排序；在黑箱LLM环境下，通过操作返回内容来影响输出；构建ProductBench数据集（15个类别、每类200个产品，商品的前10条推荐来自Amazon搜索界面）；在GPT-4o、Gemini-2.5、Claude-4、Grok-3等具搜索能力的LLMs上进行评估。

Result: 平均Promotion Success Rate：Top-5 91.4%，Top-3 86.6%，Top-1 80.3%，覆盖15个类别；优于现有排名操控方法；优化内容保持输出流畅性。

Conclusion: CORE揭示了基于生成式引擎的输出排序的可操控性及潜在滥用风险，强调需要对LLM检索输出的鲁棒性进行防护，并为小型商家提供潜在的曝光途径，但同时需关注伦理与防御机制的建设。

Abstract: The way customers search for and choose products is changing with the rise of large language models (LLMs). LLM-based search, or generative engines, provides direct product recommendations to users, rather than traditional online search results that require users to explore options themselves. However, these recommendations are strongly influenced by the initial retrieval order of LLMs, which disadvantages small businesses and independent creators by limiting their visibility.
  In this work, we propose CORE, an optimization method that \textbf{C}ontrols \textbf{O}utput \textbf{R}ankings in g\textbf{E}nerative Engines for LLM-based search. Since the LLM's interactions with the search engine are black-box, CORE targets the content returned by search engines as the primary means of influencing output rankings. Specifically, CORE optimizes retrieved content by appending strategically designed optimization content to steer the ranking of outputs. We introduce three types of optimization content: string-based, reasoning-based, and review-based, demonstrating their effectiveness in shaping output rankings. To evaluate CORE in realistic settings, we introduce ProductBench, a large-scale benchmark with 15 product categories and 200 products per category, where each product is associated with its top-10 recommendations collected from Amazon's search interface.
  Extensive experiments on four LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, and Grok-3) demonstrate that CORE achieves an average Promotion Success Rate of \textbf{91.4\% @Top-5}, \textbf{86.6\% @Top-3}, and \textbf{80.3\% @Top-1}, across 15 product categories, outperforming existing ranking manipulation methods while preserving the fluency of optimized content.

</details>


### [15] [Which course? Discourse! Teaching Discourse and Generation in the Era of LLMs](https://arxiv.org/abs/2602.02878)
*Junyi Jessy Li,Yang Janet Liu,Kanishka Misra,Valentina Pyatkin,William Sheffield*

Main category: cs.CL

TL;DR: 新课程“Computational Discourse and Natural Language Generation”于2025年秋季首次开设，跨语言学与计算机科学，旨在理论与经验并重地将 discourse 处理与长文本生成结合于本科教育。


<details>
  <summary>Details</summary>
Motivation: 在NLP快速变革的背景下，教育需跨学科设计以连接理论与应用；discourse处理对开放式/长文本生成具有重要意义，但在本科课程中尚未充分嵌入，因而提出该课程以桥接学科。

Method: 由具互补专长的团队共同设计，首次在2025年Fall提供为跨系本科课程，强调理论与实证的深度整合、培养探索性思维的课堂与作业，以及跨语言学与计算机科学的协同。

Result: 描述课程细节并基于独立调查给出课程成效的要点（takeaways），并提出未来发展愿景。

Conclusion: 作为跨学科教育尝试，该课程为discourse与NLG的结合提供教学范式，并展望未来方向。

Abstract: The field of NLP has undergone vast, continuous transformations over the past few years, sparking debates going beyond discipline boundaries. This begs important questions in education: how do we design courses that bridge sub-disciplines in this shifting landscape? This paper explores this question from the angle of discourse processing, an area with rich linguistic insights and computational models for the intentional, attentional, and coherence structure of language. Discourse is highly relevant for open-ended or long-form text generation, yet this connection is under-explored in existing undergraduate curricula. We present a new course, "Computational Discourse and Natural Language Generation". The course is collaboratively designed by a team with complementary expertise and was offered for the first time in Fall 2025 as an upper-level undergraduate course, cross-listed between Linguistics and Computer Science. Our philosophy is to deeply integrate the theoretical and empirical aspects, and create an exploratory mindset inside the classroom and in the assignments. This paper describes the course in detail and concludes with takeaways from an independent survey as well as our vision for future directions.

</details>


### [16] [RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish](https://arxiv.org/abs/2602.03652)
*Süha Kağan Köse,Mehmet Can Baytekin,Burak Aktaş,Bilge Kaan Görür,Evren Ayberk Munis,Deniz Yılmaz,Muhammed Yusuf Kartal,Çağrı Toraman*

Main category: cs.CL

TL;DR: A Turkish Retrieval-Augmented Generation (RAG) study introduces a large Turkish dataset and benchmarks seven RAG stages without fine-tuning, showing HyDE boosts accuracy and that a Pareto-optimal setup with cross-encoder reranking and context augmentation matches high accuracy at lower cost, while excessive module stacking harms morphological cues.


<details>
  <summary>Details</summary>
Motivation: To address the English-centric bias in RAG design guidance and improve factuality in morphologically rich Turkish by providing a dedicated Turkish RAG dataset and systematic benchmarking of the full pipeline.

Method: Constructed a Turkish RAG dataset from Turkish Wikipedia and CulturaX with question–answer pairs and passage chunks; evaluated seven RAG pipeline stages (query transformation, retrieval, reranking, and answer refinement) without task-specific fine-tuning; compared HyDE, cross-encoder reranking, and context augmentation.

Result: HyDE achieves 85% accuracy (vs. 78.70% baseline); a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves 84.60% with substantially lower cost; over-stacking generative modules degrades performance by distorting morphological cues; simple query clarification with robust reranking offers a practical effective solution.

Conclusion: Morphology-aware, cost-efficient RAG design is possible in Turkish: avoid excessive generative stacking, rely on robust reranking and context augmentation, and provide dedicated non-English datasets to guide practitioners.

Abstract: Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.

</details>


### [17] [HALT: Hallucination Assessment via Log-probs as Time series](https://arxiv.org/abs/2602.02888)
*Ahmad Shapiro,Karan Taneja,Ashok Goel*

Main category: cs.CL

TL;DR: HALT is a lightweight hallucination detector for LLMs that uses only the top-20 token log-probabilities as a time series, employing a GRU with entropy-based features to model calibration bias; HUB benchmark is introduced.


<details>
  <summary>Details</summary>
Motivation: Address hallucinations in LLMs, especially in safety-critical domains, by providing an efficient detector that does not require access to hidden states or internal weights and generalizes across domains and proprietary models.

Method: Feed the top-20 token log-probabilities from LLM generations as a time series into a gated recurrent unit (GRU) model augmented with entropy-based features to learn calibration bias. The approach is white-box agnostic (no hidden states needed) and black-box friendly (operates on log-probs). Construct HUB by consolidating prior datasets into ten capabilities across reasoning and general-purpose skills.

Result: HALT is 30x smaller than a fine-tuned modernBERT-base encoder (Lettuce) and achieves about 60x speedup on HUB; it outperforms Lettuce on HUB.

Conclusion: HALT and HUB together form an effective, efficient framework for hallucination detection across diverse LLM capabilities, enabling strong domain generalization and compatibility with proprietary LLMs without internal access.

Abstract: Hallucinations remain a major obstacle for large language models (LLMs), especially in safety-critical domains. We present HALT (Hallucination Assessment via Log-probs as Time series), a lightweight hallucination detector that leverages only the top-20 token log-probabilities from LLM generations as a time series. HALT uses a gated recurrent unit model combined with entropy-based features to learn model calibration bias, providing an extremely efficient alternative to large encoders. Unlike white-box approaches, HALT does not require access to hidden states or attention maps, relying only on output log-probabilities. Unlike black-box approaches, it operates on log-probs rather than surface-form text, which enables stronger domain generalization and compatibility with proprietary LLMs without requiring access to internal weights. To benchmark performance, we introduce HUB (Hallucination detection Unified Benchmark), which consolidates prior datasets into ten capabilities covering both reasoning tasks (Algorithmic, Commonsense, Mathematical, Symbolic, Code Generation) and general purpose skills (Chat, Data-to-Text, Question Answering, Summarization, World Knowledge). While being 30x smaller, HALT outperforms Lettuce, a fine-tuned modernBERT-base encoder, achieving a 60x speedup gain on HUB. HALT and HUB together establish an effective framework for hallucination detection across diverse LLM capabilities.

</details>


### [18] [Equal Access, Unequal Interaction: A Counterfactual Audit of LLM Fairness](https://arxiv.org/abs/2602.02932)
*Alireza Amiri-Margavi,Arshia Gharagozlou,Amin Gholami Davodi,Seyed Pouyan Mousavi Davoudi,Hamidreza Hasani Balyani*

Main category: cs.CL

TL;DR: 本研究揭示LLMs在获得访问后仍存在交互层面的公平性差异：GPT-4对年轻男性用户更倾向使用更多 hedging（回避/模糊处理），LLaMA-3.1-70B在情感维度对身份组表现出更广泛的变异；访问层面为零拒绝，提示仅以拒绝为准的公平性评估不足。


<details>
  <summary>Details</summary>
Motivation: 填补对话层面公平性的研究空缺，强调需要超越拒绝分析评估交互质量与身份相关的语言特征差异。

Method: 采用对照型提示设计（counterfactual prompting），对GPT-4与LLaMA-3.1-70B在职业咨询任务中变更身份属性（年龄、性别、国籍）；通过拒绝分析评估访问公平性，并利用自动化语言指标（情感、礼貌、hedging）衡量交互质量；身份差异使用配对统计检验。

Result: 两模型均为零拒绝，表现出访问公平性；但在交互质量上呈现系统性差异：GPT-4对年轻男性用户表现出显著的 hedging 增多；LLaMA-3.1-70B 展现出对身份组的情感变异更广泛的特征。

Conclusion: 结果表明公平性缺口可能仅在交互层面显现，即便访问层面公平，仍需评估情感、礼貌和不确定性等语言特征以全面评估公平性，对模型治理与设计具有重要启示。

Abstract: Prior work on fairness in large language models (LLMs) has primarily focused on access-level behaviors such as refusals and safety filtering. However, equitable access does not ensure equitable interaction quality once a response is provided. In this paper, we conduct a controlled fairness audit examining how LLMs differ in tone, uncertainty, and linguistic framing across demographic identities after access is granted. Using a counterfactual prompt design, we evaluate GPT-4 and LLaMA-3.1-70B on career advice tasks while varying identity attributes along age, gender, and nationality. We assess access fairness through refusal analysis and measure interaction quality using automated linguistic metrics, including sentiment, politeness, and hedging. Identity-conditioned differences are evaluated using paired statistical tests. Both models exhibit zero refusal rates across all identities, indicating uniform access. Nevertheless, we observe systematic, model-specific disparities in interaction quality: GPT-4 expresses significantly higher hedging toward younger male users, while LLaMA exhibits broader sentiment variation across identity groups. These results show that fairness disparities can persist at the interaction level even when access is equal, motivating evaluation beyond refusal-based audits.

</details>


### [19] [Where Norms and References Collide: Evaluating LLMs on Normative Reasoning](https://arxiv.org/abs/2602.02975)
*Mitchell Abrams,Kaveh Eskandari Miandoab,Felix Gervits,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: SNIC 提供一个面向“情境社会规范”的诊断测试床，用以评估大型语言模型在指代解析时对隐含规范的推理与应用能力。实验表明，即便是最强的LLMs，在处理隐性、未明确或相互冲突的规范时，也难以稳定地识别并执行规范。


<details>
  <summary>Details</summary>
Motivation: 探究当前 LLMs 在社会情境中的指称解析能力，尤其如何在物理与社会环境中的规范约束下推理和应用指代表达，填补对该能力的评估空缺。

Method: 设计并人工验证的诊断测试床 SNIC，聚焦日常任务（如清洁、整理、服务等）中出现的物理与社会规范，在受控条件下评估多种最先进 LLM 的推理与应用能力。

Result: 在一系列受控评估中，LLMs 难以一致地识别并应用规范，尤其是当规范隐性、未指明或相互冲突时；模型对社会规范推理存在明显盲点。

Conclusion: 当前 LLM 在社会嵌入的语言任务中存在显著局限，需进一步研究以增强对规范的推理与遵循能力，从而提升在嵌入式系统中的安全性与鲁棒性。

Abstract: Embodied agents, such as robots, will need to interact in situated environments where successful communication often depends on reasoning over social norms: shared expectations that constrain what actions are appropriate in context. A key capability in such settings is norm-based reference resolution (NBRR), where interpreting referential expressions requires inferring implicit normative expectations grounded in physical and social context. Yet it remains unclear whether Large Language Models (LLMs) can support this kind of reasoning. In this work, we introduce SNIC (Situated Norms in Context), a human-validated diagnostic testbed designed to probe how well state-of-the-art LLMs can extract and utilize normative principles relevant to NBRR. SNIC emphasizes physically grounded norms that arise in everyday tasks such as cleaning, tidying, and serving. Across a range of controlled evaluations, we find that even the strongest LLMs struggle to consistently identify and apply social norms, particularly when norms are implicit, underspecified, or in conflict. These findings reveal a blind spot in current LLMs and highlight a key challenge for deploying language-based systems in socially situated, embodied settings.

</details>


### [20] [CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning](https://arxiv.org/abs/2602.02979)
*Ran Li,Zeyuan Liu,Yinghao chen,Bingxiang He,Jiarui Yuan,Zixuan Fu,Weize Chen,Jinyi Hu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出 CPMöbius 数据无关强化学习的协作教练-玩家范式，提升推理能力且不依赖外部数据。


<details>
  <summary>Details</summary>
Motivation: 解决对大量高质量标注数据的依赖，寻求无数据或自监督下的推理能力提升。

Method: Coach 与 Player 独立但合作；Coach 设计指令并依据玩家性能变化获得奖励；Player 解决逐步难度提高的任务以获得奖励；实现数据自由的强化学习循环。

Result: 在 Qwen2.5-Math-7B-Instruct 上，整体准确率提升约+4.9，OOD 提升约+5.4；超过 RENT、R-zero 等无监督基线，分别在总体与 OOD 上超出+1.5、+4.2。

Conclusion: 数据自由的协作式强化学习可显著提升数学推理能力，为无数据场景提供可扩展的训练范式。

Abstract: Large Language Models (LLMs) have demonstrated strong potential in complex reasoning, yet their progress remains fundamentally constrained by reliance on massive high-quality human-curated tasks and labels, either through supervised fine-tuning (SFT) or reinforcement learning (RL) on reasoning-specific data. This dependence renders supervision-heavy training paradigms increasingly unsustainable, with signs of diminishing scalability already evident in practice. To overcome this limitation, we introduce CPMöbius (CPMobius), a collaborative Coach-Player paradigm for data-free reinforcement learning of reasoning models. Unlike traditional adversarial self-play, CPMöbius, inspired by real world human sports collaboration and multi-agent collaboration, treats the Coach and Player as independent but cooperative roles. The Coach proposes instructions targeted at the Player's capability and receives rewards based on changes in the Player's performance, while the Player is rewarded for solving the increasingly instructive tasks generated by the Coach. This cooperative optimization loop is designed to directly enhance the Player's mathematical reasoning ability. Remarkably, CPMöbius achieves substantial improvement without relying on any external training data, outperforming existing unsupervised approaches. For example, on Qwen2.5-Math-7B-Instruct, our method improves accuracy by an overall average of +4.9 and an out-of-distribution average of +5.4, exceeding RENT by +1.5 on overall accuracy and R-zero by +4.2 on OOD accuracy.

</details>


### [21] [LatentMem: Customizing Latent Memory for Multi-Agent Systems](https://arxiv.org/abs/2602.03036)
*Muxin Fu,Guibin Zhang,Xiangyuan Xue,Yafu Li,Zefeng He,Siyuan Huang,Xiaoye Qu,Yu Cheng,Yang Yang*

Main category: cs.CL

TL;DR: LatentMem 提供一种可学习的多智能体记忆框架，通过经验库存储轻量化的原始交互轨迹，记忆合成器在检索的经验和智能体上下文基础上生成紧凑的潜在记忆；结合 Latent Memory Policy Optimization (LMPO) 将任务级信号传递给记忆合成器，促使其输出高效且高信息价值的表示。并在多种基准和 MAS 框架上取得高于普通设置的显著提升（最高约19.36%）。


<details>
  <summary>Details</summary>
Motivation: 面临两大瓶颈：1) 记忆同质化——缺乏面向角色的定制化，导致不同智能体具有同质化记忆；2) 信息过载——记忆条目过于细粒度，造成 token 数量激增与计算负担。因此需要一种可定制、 token 高效的记忆机制，以实现个体化且高效的长期记忆。

Method: 提出 LatentMem：包含经验库和记忆合成器。经验库以轻量形式存储原始交互轨迹；记忆合成器在检索到的经验和智能体上下文的条件下，合成紧凑的潜在记忆。引入 LMPO，通过将任务级优化信号传递给潜在记忆再到合成器，促使输出的潜在记忆更紧凑且具有高效用。试验覆盖多样基准和主流 MAS 框架。

Result: 在广泛基准和主流 MAS 框架上，LatentMem 相较于“原生/ vanilla 设置”性能提升高达约19.36%，并持续优于现有记忆架构，同时不需要对底层框架做修改。

Conclusion: LatentMem 提供了一个可定制、 token 高效的面向智能体的记忆方案，通过经验库与潜在记忆的分离与 LMPO 的任务级调优，实现更高的记忆效用与系统性能的提升，具有良好的泛化性，且兼容现有 MAS 框架。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to $19.36$% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.

</details>


### [22] [ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution](https://arxiv.org/abs/2602.03075)
*Junjie Huang,Jiarui Qin,Di Yin,Weiwen Liu,Yong Yu,Xing Sun,Weinan Zhang*

Main category: cs.CL

TL;DR: A self-reinforcing RL-guided mid-training framework (ReMiT) that reweights tokens during the mid-training phase using RL-tuned reasoning priors to strengthen the base model and improve post-training performance, achieving ~3% average gains on 10 pre-training benchmarks (math, code, general reasoning) and sustaining >2% gains through post-training, enabling a bidirectional improvement loop without a specialized teacher model.


<details>
  <summary>Details</summary>
Motivation: Challenges the conventional unidirectional pre-training→post-training paradigm by exploiting a critical mid-training (annealing) phase. Proposes a self-reinforcing feedback flywheel where post-training insights influence pre-training via dynamic token reweighting.

Method: Introduce ReMiT: use reasoning priors from RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Operates at the end of pre-training with a rapidly decaying learning rate on high-quality corpora, without requiring a teacher model. Includes training-dynamics analysis to identify the mid-training turning point.

Result: Empirically achieves an average of 3% improvement across 10 pre-training benchmarks (math, code, general reasoning) and sustains gains above 2% through the post-training pipeline, validating a self-reinforcing iterative improvement loop.

Conclusion: Demonstrates the feasibility and value of a bidirectional LLM training flywheel. Mid-training is a critical leverage point; RL-guided token reweighting yields sustained gains without external teacher models.

Abstract: Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.

</details>


### [23] [AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback](https://arxiv.org/abs/2602.03084)
*Zhitao Gao,Jie Ma,Xuhong Li,Pengyu Li,Ning Qu,Yaqiang Wu,Hui Liu,Jun Liu*

Main category: cs.CL

TL;DR: AERO是一个无监督的自我进化推理框架，通过自问自答与批判的双循环实现自主推理演化，在ZPD启发下通过信息熵定位 solvability gap，并用独立反事实纠错进行验证，以及分阶段训练来同步能力增长，取得对九个基准的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在复杂推理任务中仍受制于对专家标注数据和外部 verifiers 的依赖；现有自我进化框架往往难以识别最佳学习区，容易因内部反馈而强化幻觉和错误先验，亟需无监督且鲁棒的自我校验与学习区域定位。

Method: 提出Autonomous Evolutionary Reasoning Optimization (AERO)：通过内部化自问、自答、批评的双循环自我进化推理框架，结合Zone of Proximal Development (ZPD) 理论，利用基于信息熵的定位来聚焦“可解性差距”，并引入Independent Counterfactual Correction用于鲁棒验证。同时引入Staggered Training Strategy以同步各功能角色的能力增长，防止课程崩溃。

Result: 在九个基准、三个领域的广泛评估中，AERO在Qwen3-4B-Base上实现平均性能提升4.57%，在Qwen3-8B-Base上提升5.10%，优于竞争基线。代码公开（GitHub）以便复现。

Conclusion: AERO提供了一种无监督的自我进化推理框架，通过自问自答与批判的双循环、ZPD启发的可解性定位、独立反事实纠错以及分阶段训练，有效缓解对外部标注和外部验证的依赖，并在多领域基准上展现显著性能提升。

Abstract: Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \underline{A}utonomous \underline{E}volutionary \underline{R}easoning \underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\% on Qwen3-4B-Base and 5.10\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.

</details>


### [24] [Test-time Recursive Thinking: Self-Improvement without External Feedback](https://arxiv.org/abs/2602.03094)
*Yufan Zhuang,Chandan Singh,Liyuan Liu,Yelong Shen,Dinghuai Zhang,Jingbo Shang,Jianfeng Gao,Weizhu Chen*

Main category: cs.CL

TL;DR: TRT enables test-time self-improvement of LLMs by conditioning generation on rollout strategies, accumulated knowledge, and self-generated verification signals, achieving strong benchmarks without additional training.


<details>
  <summary>Details</summary>
Motivation: To enable self-improvement of LLMs without external supervision or retraining, two main challenges must be addressed: (i) generating diverse, high-quality candidate solutions efficiently, and (ii) reliably selecting correct answers in the absence of ground-truth supervision.

Method: Test-time Recursive Thinking (TRT) is an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals, enabling self-guided refinement at inference time.

Result: Open-source models reach 100% accuracy on AIME-25/24; closed-source models improve by 10.4–14.8 percentage points on LiveCodeBench's most difficult problems without external feedback.

Conclusion: TRT demonstrates that test-time self-improvement is viable for LLMs, reducing reliance on external supervision and retraining; promising for broader problem-solving, though further study is needed on robustness, generalization, and failure modes.

Abstract: Modern Large Language Models (LLMs) have shown rapid improvements in reasoning capabilities, driven largely by reinforcement learning (RL) with verifiable rewards. Here, we ask whether these LLMs can self-improve without the need for additional training. We identify two core challenges for such systems: (i) efficiently generating diverse, high-quality candidate solutions, and (ii) reliably selecting correct answers in the absence of ground-truth supervision. To address these challenges, we propose Test-time Recursive Thinking (TRT), an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals. Using TRT, open-source models reach 100% accuracy on AIME-25/24, and on LiveCodeBench's most difficult problems, closed-source models improve by 10.4-14.8 percentage points without external feedback.

</details>


### [25] [Task--Specificity Score: Measuring How Much Instructions Really Matter for Supervision](https://arxiv.org/abs/2602.03103)
*Pritam Kadasi,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: 提出任务特异性分数（TSS）及其改进版 TSS++，用以量化指令对输出的影响程度，并用于在固定 token 预算下选择任务特异性示例以提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 当前指令微调中，同一输入在不同指令下产生的输出仍然可能合理，指令对输出的唯一性作用尚不清晰，需要量化评估。

Method: 定义 TSS，通过对同一输入的真实指令与 plausive 替代指令进行对比来衡量指令的重要性；TSS++ 引入硬替代及一个小型质量项以缓解易负样本的影响。

Result: 在 Alpaca、Dolly-15k、NI-20 三个数据集和 Gemma、Llama、Qwen 三个开源模型上实验，显示在等 token 预算下，选择任务特异性示例能提升下游性能，且与 perplexity、IFD 等质量过滤方法互为补充。

Conclusion: 任务特异性度量和基于此的示例选择对提升指令驱动的下游任务表现有明显作用，尤其在资源受限场景。

Abstract: Instruction tuning is now the default way to train and adapt large language models, but many instruction--input--output pairs are only weakly specified: for a given input, the same output can remain plausible under several alternative instructions. This raises a simple question: \emph{does the instruction uniquely determine the target output?}
  We propose the \textbf{Task--Specificity Score (TSS)} to quantify how much an instruction matters for predicting its output, by contrasting the true instruction against plausible alternatives for the same input. We further introduce \textbf{TSS++}, which uses hard alternatives and a small quality term to mitigate easy-negative effects. Across three instruction datasets (\textsc{Alpaca}, \textsc{Dolly-15k}, \textsc{NI-20}) and three open LLMs (Gemma, Llama, Qwen), we show that selecting task-specific examples improves downstream performance under tight token budgets and complements quality-based filters such as perplexity and IFD.

</details>


### [26] [The Mask of Civility: Benchmarking Chinese Mock Politeness Comprehension in Large Language Models](https://arxiv.org/abs/2602.03107)
*Yitong Zhang,Yuhan Xiang,Mingxuan Liu*

Main category: cs.CL

TL;DR: 本研究系统评估代表性大语言模型在中文礼貌、失礼与伪礼貌识别中的差异及提示策略的影响。


<details>
  <summary>Details</summary>
Motivation: 填补对话用语理解的不足，推动将语用理论与大语言模型评估相结合，探索科技与人文的协同。

Method: 基于 Rapport Management Theory 与 Mock Politeness Model 构建三分类数据集（真实与仿真中文语篇），选取六个代表模型（包含 GPT-5.1、DeepSeek），在零-shot、少样本、知识增强、混合四种提示条件下评估。

Result: 不同模型在三类礼貌现象识别上表现存在差异，且提示策略对表现影响显著；研究提供了在“Great Linguistics”框架下将语用理论应用于现代AI评估的初步证据。

Conclusion: 提示策略与模型差异的系统评估为科技与人文学科的跨学科对话提供范式，展示将语用理论应用于技术变革中的可行性。

Abstract: From a pragmatic perspective, this study systematically evaluates the differences in performance among representative large language models (LLMs) in recognizing politeness, impoliteness, and mock politeness phenomena in Chinese. Addressing the existing gaps in pragmatic comprehension, the research adopts the frameworks of Rapport Management Theory and the Model of Mock Politeness to construct a three-category dataset combining authentic and simulated Chinese discourse. Six representative models, including GPT-5.1 and DeepSeek, were selected as test subjects and evaluated under four prompting conditions: zero-shot, few-shot, knowledge-enhanced, and hybrid strategies. This study serves as a meaningful attempt within the paradigm of ``Great Linguistics,'' offering a novel approach to applying pragmatic theory in the age of technological transformation. It also responds to the contemporary question of how technology and the humanities may coexist, representing an interdisciplinary endeavor that bridges linguistic technology and humanistic reflection.

</details>


### [27] [One Model, All Roles: Multi-Turn, Multi-Agent Self-Play Reinforcement Learning for Conversational Social Intelligence](https://arxiv.org/abs/2602.03109)
*Bowen Jiang,Taiwei Shi,Ryo Kamoi,Yuan Yuan,Camillo J. Taylor,Longqi Yang,Pei Zhou,Sihao Chen*

Main category: cs.CL

TL;DR: 提出 OMAR：单模型多角色的多轮自对话强化学习框架，通过角色扮演学习社会智能，在长对话中通过分层优势估计确保训练稳定，并在 SOTOPIA 与 Werewolf 评估中显示 emergent 社会智能，如同理心、说服与妥协。


<details>
  <summary>Details</summary>
Motivation: 解决静态、单轮优化难以捕捉的群体交互中的长期目标与社会规范，推动 AI 在群体对话中的社会智能发展。

Method: 使用单一模型进行多角色自对话、轮回化学习；引入轮次级与标记级的分层优势估计以提高长对话训练稳定性；在 SOTOPIA 社交环境与 Werewolf 策略游戏中进行评估。

Result: 模型在对话中呈现细粒度的社会智能能力，如同理心、劝说和妥协倾向；能够在竞争场景中实现协作；训练无需人类监督，表现出较强的学习能力。

Conclusion: 证明在群体对话中学习并实现社会智能的潜力，推动对 AI 社会智能的研究；同时也指出如奖励设计与奖励黑箱等现实挑战需要后续研究。

Abstract: This paper introduces OMAR: One Model, All Roles, a reinforcement learning framework that enables AI to develop social intelligence through multi-turn, multi-agent conversational self-play. Unlike traditional paradigms that rely on static, single-turn optimizations, OMAR allows a single model to role-play all participants in a conversation simultaneously, learning to achieve long-term goals and complex social norms directly from dynamic social interaction. To ensure training stability across long dialogues, we implement a hierarchical advantage estimation that calculates turn-level and token-level advantages. Evaluations in the SOTOPIA social environment and Werewolf strategy games show that our trained models develop fine-grained, emergent social intelligence, such as empathy, persuasion, and compromise seeking, demonstrating the effectiveness of learning collaboration even under competitive scenarios. While we identify practical challenges like reward hacking, our results show that rich social intelligence can emerge without human supervision. We hope this work incentivizes further research on AI social intelligence in group conversations.

</details>


### [28] [ForesightKV: Optimizing KV Cache Eviction for Reasoning Models by Learning Long-Term Contribution](https://arxiv.org/abs/2602.03203)
*Zican Dong,Peiyu Liu,Junyi Li,Zhipeng Chen,Han Peng,Shuo Wang,Wayne Xin Zhao*

Main category: cs.CL

TL;DR: 提出 ForesightKV，通过训练预测要淘汰的KV对以在长文本生成中平衡缓存容量与性能，结合 Golden Eviction 与监督/强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决KV缓存随序列增长线性膨胀导致的内存与计算开销，同时保持推理质量，弥补现有方法在捕捉KV依赖方面的不足。

Method: 提出 Golden Eviction 以未来注意力分数确定最优淘汰目标；将淘汰策略通过带有 Pairwise Ranking Loss 的监督学习蒸馏；并将缓存淘汰视为马尔可夫决策过程，应用 GRPO 以缓解低熵标记下的语言模型损失增加。

Result: 在 AIME2024/2025 基准上，ForesightKV 在仅使用一半缓存预算时，持续优于先前方法，并与监督学习和强化学习的协同提升相关。

Conclusion: 训练驱动的缓存淘汰框架可在理想与现实之间取得更好权衡，且可通过监督与强化学习的结合进一步提升鲁棒性与性能。

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning abilities by producing long reasoning traces. However, as the sequence length grows, the key-value (KV) cache expands linearly, incurring significant memory and computation costs. Existing KV cache eviction methods mitigate this issue by discarding less important KV pairs, but often fail to capture complex KV dependencies, resulting in performance degradation. To better balance efficiency and performance, we introduce ForesightKV, a training-based KV cache eviction framework that learns to predict which KV pairs to evict during long-text generations. We first design the Golden Eviction algorithm, which identifies the optimal eviction KV pairs at each step using future attention scores. These traces and the scores at each step are then distilled via supervised training with a Pairwise Ranking Loss. Furthermore, we formulate cache eviction as a Markov Decision Process and apply the GRPO algorithm to mitigate the significant language modeling loss increase on low-entropy tokens. Experiments on AIME2024 and AIME2025 benchmarks of three reasoning models demonstrate that ForesightKV consistently outperforms prior methods under only half the cache budget, while benefiting synergistically from both supervised and reinforcement learning approaches.

</details>


### [29] [Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection](https://arxiv.org/abs/2602.03216)
*Dongwon Jo,Beomseok Kang,Jiwon Song,Jae-Joon Kim*

Main category: cs.CL

TL;DR: 提出 Token Sparse Attention，一种轻量且动态的 token 级稀疏化方法，在注意力计算中将每个头的 Q/K/V 压缩为一个较小的 token 集，然后再解压输出回原序列，使 token 信息可在后续层重新被考虑；与密集注意力实现（如 Flash Attention）及现有稀疏内核兼容，在 128K 上可达最高 3.23x 的注意力加速，准确性下降小于 1%。


<details>
  <summary>Details</summary>
Motivation: 长期上下文推理中的注意力复杂度为线性化前沿瓶颈；现有方法要么以结构化模式稀疏化注意力，要么在某些层永久剔除 tokens，忽略了跨层/跨头的 token 重要性动态，导致信息丢失或不可逆决策。

Method: 提出 Token Sparse Attention，在每个头内部对 Q/K/V 进行压缩成一个较小的 token 集，完成注意力计算后再将输出解压回原始序列；允许在后续层重新考虑被压缩的 token；与密集实现和现有稀疏内核无缝组合，兼容 Flash Attention。

Result: 在实验中，Token Sparse Attention 稳定提升准确率-延迟折中，128K context 下实现最高约 3.23x 的注意力速度提升，准确率下降不超过 1%。

Conclusion: 动态且跨层/跨头的 token 级稀疏化对可扩展的长上下文推理是有效且互补的设计点，适用于现有的密集与稀疏注意力实现。

Abstract: The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head $Q$, $K$, $V$ to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to $\times$3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.

</details>


### [30] [POP: Prefill-Only Pruning for Efficient Large Model Inference](https://arxiv.org/abs/2602.03295)
*Junhui He,Zhihui Fu,Jun Wang,Qingan Li*

Main category: cs.CL

TL;DR: 提出 Prefill-Only Pruning (POP)，基于阶段感知的推理策略，在预填阶段裁剪深层以降低计算成本，在解码阶段保留完整模型；引入虚拟门控分析重要性，独立 KV 投影及边界处理以保持缓存与第一 token 的准确性，实验显示最高 1.37x 的 prefill 延迟加速和微小的性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化裁剪多为阶段无关方法，忽视 prefill 与 decode 的不对称任务需求，导致在降低计算的同时显著损失准确性。

Method: 通过虚拟门控实现阶段性重要性分析，发现深层对 next-token prediction（decode）重要性高、对 context encoding（prefill）冗余；提出 POP：在 prefill 阶段裁剪深层，保持解码阶段完整模型；为阶段切换引入独立的 KV 投影以维持缓存，采用边界处理策略确保首 token 的预测准确性。

Result: 在 Llama-3.1、Qwen3-VL、Gemma-3 上的广泛模态任务中，POP 实现了最高 1.37x 的 prefill 延迟加速，且性能损失在可接受范围内，优于现有结构化裁剪的方法在准确性-效率折中上的表现。

Conclusion: 阶段感知的裁剪策略能够在保持解码性能的同时显著降低预填的计算开销，证明了在大模型推理中对任务阶段的区别对待是提升效率的有效途径。

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37$\times$ speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.

</details>


### [31] [MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research](https://arxiv.org/abs/2602.03318)
*Yifan Shi,Jialong Shi,Jiayi Wang,Ye Fan,Jianyong Sun*

Main category: cs.CL

TL;DR: MIRROR: a fine-tuning-free, end-to-end multi-agent framework that translates NL optimization problems into mathematical models and solver code, using execution-driven revision and hierarchical exemplar retrieval to achieve strong performance on OR benchmarks without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: OR modeling is slow and fragile when done by experts; existing LLM-based approaches suffer from errors and lack of task-specific retrieval; need a fine-tuning-free, autonomous system for NL-to-OR translation.

Method: MIRROR employs an ensemble of agents: (i) execution-driven iterative adaptive revision for automatic error correction, (ii) hierarchical retrieval of modeling and coding exemplars from a curated library; end-to-end translation from NL to mathematical models and solver code without fine-tuning.

Result: Outperforms existing methods on standard OR benchmarks, with notable performance gains on industrial datasets IndustryOR and Mamo-ComplexLP.

Conclusion: Integrates precise external knowledge infusion with systematic error correction to deliver reliable OR modeling for non-experts, addressing fundamental limitations of general-purpose LLMs in expert optimization tasks.

Abstract: Operations Research (OR) relies on expert-driven modeling-a slow and fragile process ill-suited to novel scenarios. While large language models (LLMs) can automatically translate natural language into optimization models, existing approaches either rely on costly post-training or employ multi-agent frameworks, yet most still lack reliable collaborative error correction and task-specific retrieval, often leading to incorrect outputs. We propose MIRROR, a fine-tuning-free, end-to-end multi-agent framework that directly translates natural language optimization problems into mathematical models and solver code. MIRROR integrates two core mechanisms: (1) execution-driven iterative adaptive revision for automatic error correction, and (2) hierarchical retrieval to fetch relevant modeling and coding exemplars from a carefully curated exemplar library. Experiments show that MIRROR outperforms existing methods on standard OR benchmarks, with notable results on complex industrial datasets such as IndustryOR and Mamo-ComplexLP. By combining precise external knowledge infusion with systematic error correction, MIRROR provides non-expert users with an efficient and reliable OR modeling solution, overcoming the fundamental limitations of general-purpose LLMs in expert optimization tasks.

</details>


### [32] [Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention](https://arxiv.org/abs/2602.03338)
*Rakshith Vasudev,Melisa Russak,Dan Bikel,Waseem Alshikh*

Main category: cs.CL

TL;DR: 高AUROC的二元LLM critic 在上线干预中可能造成显著性能下降；提出50任务的前置试验来预测干预效果，帮助判定“是否应干预”，以避免严重回撤。


<details>
  <summary>Details</summary>
Motivation: 研究在部署阶段，基于LLM critic 的主动干预并非总能提高可靠性；离线准确度不足以预测部署时的安全性，因此需要一个可在部署前评估干预风险与收益的框架。

Method: 对比分析不同模型/基准下的干预离线与部署效果，揭示干预的 disruption-recovery tradeoff；设计一个仅含50任务的前置试验来预测干预在部署中的效果；在多项基准上测试该预测能力。

Result: 干预在某些模型上造成26个百分点的降幅，而在另一些模型几乎无影响；前置试验能正确预测结果：在高成功任务中干预显著降幅（约-26pp），在高失败的ALFWorld基准中呈现小幅提升(+2.8pp，p=0.014)。

Conclusion: 干预的核心价值在于“知道何时不干预”，通过小规模的前置评估可以在部署前避免严重回归，提升部署安全性与鲁棒性。

Abstract: Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.
  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.

</details>


### [33] [PEGRL: Improving Machine Translation by Post-Editing Guided Reinforcement Learning](https://arxiv.org/abs/2602.03352)
*Yunzhi Shen,Hao Zhou,Xin Huang,Xue Han,Junlan Feng,Shujian Huang*

Main category: cs.CL

TL;DR: PEGRL提出一个两阶段强化学习框架，利用后编辑任务作为辅助信号来降低噪声并引导优化，同时在全局探索与局部细化之间取得平衡。通过在每次迭代中基于翻译输出构造后编辑输入，使后编辑阶段的回报估计能受当前翻译行为的条件化影响，并通过任务特定的权重平衡翻译与后编辑目标，实现更高样本效率的偏置估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的机器翻译在蒙特卡洛回报估计产生的噪声以及轨迹空间过大导致的全局探索偏好之间存在权衡难题。引入一个辅助任务（后编辑）以稳定训练、提供额外信号并引导优化方向。

Method: 提出PEGRL的两阶段RL框架：阶段A进行翻译输出的采样；阶段B进行后编辑，在该阶段利用基于当前翻译行为条件化的回报估计，结合一个面向全局探索和局部优化的混合目标。引入任务特定权重对翻译与后编辑目标进行加权，形成一个偏置但样本更高效的估计。

Result: 在英语→芬兰语、英语→土耳其语以及英语与中英互译的实验中，PEGRL相对RL基线显示出一致的性能提升；具体到英语→土耳其语，基于COMET-KIWI的评估与先进的LLM系统（DeepSeek-V3.2）相媲美。

Conclusion: 通过使用后编辑作为辅助任务，PEGRL在训练稳定性、样本效率和翻译质量上均取得提升，证明了将后编辑信号引入MT-RL的可行性以及在大语言模型驱动的翻译场景中的实用性。

Abstract: Reinforcement learning (RL) has shown strong promise for LLM-based machine translation, with recent methods such as GRPO demonstrating notable gains; nevertheless, translation-oriented RL remains challenged by noisy learning signals arising from Monte Carlo return estimation, as well as a large trajectory space that favors global exploration over fine-grained local optimization. We introduce \textbf{PEGRL}, a \textit{two-stage} RL framework that uses post-editing as an auxiliary task to stabilize training and guide overall optimization. At each iteration, translation outputs are sampled to construct post-editing inputs, allowing return estimation in the post-editing stage to benefit from conditioning on the current translation behavior, while jointly supporting both global exploration and fine-grained local optimization. A task-specific weighting scheme further balances the contributions of translation and post-editing objectives, yielding a biased yet more sample-efficient estimator. Experiments on English$\to$Finnish, English$\to$Turkish, and English$\leftrightarrow$Chinese show consistent gains over RL baselines, and for English$\to$Turkish, performance on COMET-KIWI is comparable to advanced LLM-based systems (DeepSeek-V3.2).

</details>


### [34] [Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain](https://arxiv.org/abs/2602.03368)
*Wei Zhu*

Main category: cs.CL

TL;DR: 提出面向工业与医疗领域的检索增强生成（RAG）系统组件分析与可行替代方案，并在三类任务上进行系统评估，以揭示最佳实践及性能-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 当前对RAG系统的组成、组织方式和实现细节在工业应用，尤其是医疗场景中缺乏统一指导。本研究旨在梳理各组件的作用并提供实际可行的替代方案，以提升系统落地性和可扩展性。

Method: 逐一分析RAG系统的核心组件（如检索、文档嵌入、重排策略、提示设计、数据管线、评估指标等），提出针对每组件的替代方案与实现建议；在三类任务上进行系统评估，比较不同组件配置在性能与资源/时间成本上的权衡。

Result: 给出在不同组件组合下的最佳实践清单，明确在何种场景下优先考虑性能、在何种场景下优先考虑效率，并揭示LLM驱动RAG系统的权衡关系。

Conclusion: 为工业化部署的RAG系统提供可操作的设计指南，强调模块化、可替代性与成本效益，尤其针对医学应用的合规性与效率需求提出相应的策略。

Abstract: While retrieval augmented generation (RAG) has been swiftly adopted in industrial applications based on large language models (LLMs), there is no consensus on what are the best practices for building a RAG system in terms of what are the components, how to organize these components and how to implement each component for the industrial applications, especially in the medical domain. In this work, we first carefully analyze each component of the RAG system and propose practical alternatives for each component. Then, we conduct systematic evaluations on three types of tasks, revealing the best practices for improving the RAG system and how LLM-based RAG systems make trade-offs between performance and efficiency.

</details>


### [35] [Towards Distillation-Resistant Large Language Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.03396)
*Hao Fang,Tianyi Zhang,Tianqu Zhuang,Jiawei Kong,Kuofeng Gao,Bin Chen,Leqi Liang,Shu-Tao Xia,Ke Xu*

Main category: cs.CL

TL;DR: 提出一种基于条件互信息（CMI）的对抗性防护框架，以对抗像素化和基于 logits 的蒸馏攻击。通过学习一个变换矩阵净化模型输出，最小化与输入查询相关的蒸馏信息，同时保持输出的任务效用；实验证明在多种大语言模型和蒸馏算法下显著降低蒸馏效果而不显著牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 当前专有LLMs多以黑箱API形式提供，但攻击者仍可通过模型输出进行知识提取（蒸馏）。现有防御多关注基于文本的蒸馏，对基于 logits 的蒸馏防护不足。研究从信息论角度衡量蒸馏相关信息，以减少攻击者通过教师输出获得的有用信息。

Method: 引入基于条件互信息的度量，CMI(T;Q|L)，其中 T 为教师 logits，Q 为查询，L 为真实标签。设计一个学习的变换矩阵对原始输出进行净化，提出一个与 CMI 相关的反蒸馏目标以优化该变换，从而在降低蒸馏信息的同时保持输出的实用性。

Result: 在多种LLM和强蒸馏算法上进行大规模实验，所提方法显著降低蒸馏性能，同时保持任务准确性，证明该方法在保护模型知识产权方面的有效性。

Conclusion: 将信息论驱动的输出净化作为对抗蒸馏的有效策略，提供理论与实证支持，表明通过降低蒸馏相关信息可显著提升对基于 logits 的蒸馏的鲁棒性。

Abstract: Proprietary large language models (LLMs) embody substantial economic value and are generally exposed only as black-box APIs, yet adversaries can still exploit their outputs to extract knowledge via distillation. Existing defenses focus exclusively on text-based distillation, leaving the important logit-based distillation largely unexplored. In this work, we analyze this problem and present an effective solution from an information-theoretic perspective. We characterize distillation-relevant information in teacher outputs using the conditional mutual information (CMI) between teacher logits and input queries conditioned on ground-truth labels. This quantity captures contextual information beneficial for model extraction, motivating us to defend distillation via CMI minimization. Guided by our theoretical analysis, we propose learning a transformation matrix that purifies the original outputs to enhance distillation resistance. We further derive a CMI-inspired anti-distillation objective to optimize this transformation, which effectively removes distillation-relevant information while preserving output utility. Extensive experiments across multiple LLMs and strong distillation algorithms demonstrate that the proposed method significantly degrades distillation performance while preserving task accuracy, effectively protecting models' intellectual property.

</details>


### [36] [FactNet: A Billion-Scale Knowledge Graph for Multilingual Factual Grounding](https://arxiv.org/abs/2602.03417)
*Yingli Shen,Wen Lai,Jie Zhou,Xueren Zhang,Yudong Wang,Kangyang Luo,Shuo Wang,Ge Gao,Alexander Fraser,Maosong Sun*

Main category: cs.CL

TL;DR: FactNet 是一个巨量开源资源，将 17亿原子断言与 301亿可审计的证据指针统一，全部来自 316本维基版本，采用确定性构建，证据可逐字节检索， grounding 精度达到 92.1%，并提供 FactNet-Bench 用于知识图谱完成、问答和事实核查的评估，支持多语言系统的可信训练与评估。


<details>
  <summary>Details</summary>
Motivation: 解决大模型在事实性和来源可追溯性方面的不足；现有资源要么提供结构化知识库、要么提供带文本的证据但规模受限，二者难以兼得。需一个大规模、可审计、可复现且覆盖多语言的 grounding 资源。

Method: 采用严格确定性构建管线，从 316 个维基版本中提取原子断言及可审计的证据指针，证据单元可逐字节检索；进行广泛审计以验证 grounding 的精度；推出 FactNet-Bench，覆盖知识图谱补全、问答与事实核查等任务。

Result: 构建了 17亿原子断言和 301亿证据指针的资源集； grounding 精度达到 92.1%，对长尾语言也具有稳健性；资源开源且可复现，另配套评测套件 FactNet-Bench。

Conclusion: 为训练与评估多语言、可信赖的系统提供基础性资源，帮助弥合知识 grounding 的缺口，推动可验证、可追溯的语言模型研究与应用。

Abstract: While LLMs exhibit remarkable fluency, their utility is often compromised by factual hallucinations and a lack of traceable provenance. Existing resources for grounding mitigate this but typically enforce a dichotomy: they offer either structured knowledge without textual context (e.g., knowledge bases) or grounded text with limited scale and linguistic coverage. To bridge this gap, we introduce FactNet, a massive, open-source resource designed to unify 1.7 billion atomic assertions with 3.01 billion auditable evidence pointers derived exclusively from 316 Wikipedia editions. Unlike recent synthetic approaches, FactNet employs a strictly deterministic construction pipeline, ensuring that every evidence unit is recoverable with byte-level precision. Extensive auditing confirms a high grounding precision of 92.1%, even in long-tail languages. Furthermore, we establish FactNet-Bench, a comprehensive evaluation suite for Knowledge Graph Completion, Question Answering, and Fact Checking. FactNet provides the community with a foundational, reproducible resource for training and evaluating trustworthy, verifiable multilingual systems.

</details>


### [37] [A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces](https://arxiv.org/abs/2602.03442)
*Mingxuan Du,Benfeng Xu,Chiwei Zhu,Shaohan Wang,Pengyu Wang,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: A-RAG introduces Agentic RAG, exposing hierarchical retrieval tools to the model (keyword search, semantic search, chunk read) to enable adaptive, multi-granularity retrieval. It achieves consistent improvements on open-domain QA with comparable or fewer retrieved tokens, and its scalability with model size and compute is studied; code will be released.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems rely on single-shot retrieval or fixed workflows, which constrain the model's retrieval decisions and hinder scaling with advancing LMs. Providing direct retrieval interfaces to the agent aims to unlock model-driven retrieval planning and better utilize growing model capabilities.

Method: Proposes Agentic RAG (A-RAG) with three retrieval tools—keyword search, semantic search, and chunk read—granting the agent multi-granularity, hierarchical retrieval control. Evaluates on open-domain QA benchmarks; analyzes scaling with model size and test-time compute; releases code and evaluation suite.

Result: A-RAG consistently outperforms existing RAG approaches while using comparable or fewer retrieved tokens. The agent-driven retrieval adapts to tasks and scales with model capabilities, demonstrating improved efficiency and effectiveness across benchmarks.

Conclusion: Agentic retrieval interfaces enable models to participate in retrieval decisions, yielding better performance and efficiency. The approach scales with model size, and the authors provide code and evaluation resources to promote further research.

Abstract: Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.

</details>


### [38] [Preferences for Idiomatic Language are Acquired Slowly -- and Forgotten Quickly: A Case Study on Swedish](https://arxiv.org/abs/2602.03484)
*Jenny Kunz*

Main category: cs.CL

TL;DR: Idiomatic competence in Swedish language models develops more slowly than grammatical/lexical abilities; training dynamics and instruction tuning with Translationese data significantly affect idiom preferences, with larger models showing continued improvement in idiom- related tasks.


<details>
  <summary>Details</summary>
Motivation: 揭示语言模型在习惯用语（idiomaticity）方面的学习进程，以及在从英语迁移到瑞典语、从零开始训练与微调之间的差异；并考察数据翻译化的指令调优对习惯用语偏好的影响。

Method: 从头在瑞典语数据上训练模型、并对英语预训练模型进行微调，使用最小对比对探测语言可接受性与习惯用语偏好；将现有基准改造成最小对比格式；构建两套新的习惯用语数据集（对照常用成语与可行变体；对照瑞典语习惯用语与 Translationese），在不同模型规模下评估（最高8B）。

Result: 习惯用语能力的出现要慢于语法与词汇等其他语言能力；随着训练时间延长，对大多数任务的收益趋于边际但在习惯用语相关任务上仍有提升，尤其在最大模型（8B）中表现更显著；然而对英语数据进行机器翻译的指令调优会使模型迅速失去对习惯用语的偏好。

Conclusion: 习惯用语能力的获取需要更具针对性的训练信号和更充分的训练；基于机器翻译数据的指令调优对习惯用语偏好有负面影响，应谨慎使用；应考虑专门的习惯用语数据集和更大规模、直接的本地语言训练来提升鲁棒性。

Abstract: In this study, we investigate how language models develop preferences for \textit{idiomatic} as compared to \textit{linguistically acceptable} Swedish, both during pretraining and when adapting a model from English to Swedish. To do so, we train models on Swedish from scratch and by fine-tuning English-pretrained models, probing their preferences at various checkpoints using minimal pairs that differ in linguistic acceptability or idiomaticity. For linguistic acceptability, we adapt existing benchmarks into a minimal-pair format. To assess idiomaticity, we introduce two novel datasets: one contrasting conventionalized idioms with plausible variants, and another contrasting idiomatic Swedish with Translationese. Our findings suggest that idiomatic competence emerges more slowly than other linguistic abilities, including grammatical and lexical correctness. While longer training yields diminishing returns for most tasks, idiom-related performance continues to improve, particularly in the largest model tested (8B). However, instruction tuning on data machine-translated from English -- the common approach for languages with little or no native instruction data -- causes models to rapidly lose their preference for idiomatic language.

</details>


### [39] [Self-Verification Dilemma: Experience-Driven Suppression of Overused Checking in LLM Reasoning](https://arxiv.org/abs/2602.03485)
*Quanyu Long,Kai Jie Jiang,Jianda Chen,Xu Guo,Leilei Gan,Wenya Wang*

Main category: cs.CL

TL;DR: 提出基于经验驱动的测试时框架，用以抑制过度自我验证（recheck），通过离线经验池的检索判断是否需要再次验证，从而降低token消耗并维持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在长推理中频繁进行自我验证，但多数为确认性而非纠错，导致资源浪费且潜在效果有限。

Method: 在推理过程中检测recheck激活，检索离线经验池以获得历史的验证结果，并据此给出抑制信号；当经验指向不必要时，模型跳过该次recheck。

Result: 在多模型和基准上，token使用量最高下降至20.3%，保持准确性，某些数据集甚至提升性能。

Conclusion: 自我验证的激活频率与实际有效性存在错配；基于经验的抑制框架能提升推理效率且保持甚至提高准确性。

Abstract: Large Reasoning Models (LRMs) achieve strong performance by generating long reasoning traces with reflection. Through a large-scale empirical analysis, we find that a substantial fraction of reflective steps consist of self-verification (recheck) that repeatedly confirm intermediate results. These rechecks occur frequently across models and benchmarks, yet the vast majority are confirmatory rather than corrective, rarely identifying errors and altering reasoning outcomes. This reveals a mismatch between how often self-verification is activated and how often it is actually useful. Motivated by this, we propose a novel, experience-driven test-time framework that reduces the overused verification. Our method detects the activation of recheck behavior, consults an offline experience pool of past verification outcomes, and estimates whether a recheck is likely unnecessary via efficient retrieval. When historical experience suggests unnecessary, a suppression signal redirects the model to proceed. Across multiple model and benchmarks, our approach reduces token usage up to 20.3% while maintaining the accuracy, and in some datasets even yields accuracy improvements.

</details>


### [40] [Learning to Reason Faithfully through Step-Level Faithfulness Maximization](https://arxiv.org/abs/2602.03507)
*Runquan Gui,Yafu Li,Xiaoye Qu,Ziyan Liu,Yeqiu Cheng,Yu Cheng*

Main category: cs.CL

TL;DR: FaithRL 提出以信实性为导向的强化学习框架，直接最大化推理过程的信实性以降低幻觉并保持或提高正确性。通过几何化奖励设计与信实感知的优势调制实现逐步信用分配，理论上证明该目标可缓解过度自信。实验在多种骨干模型和基准上显示显著降低幻觉率，同时提升（或至少不降低）正确答案率，且在逐步推理的信实性与泛化性方面有提升。代码可见：https://github.com/aintdoin/FaithRL。


<details>
  <summary>Details</summary>
Motivation: 现有基于稀疏结果奖励的 RLVR 往往缺乏对中间步骤的监督，易导致过度自信与错误推理，从而增加幻觉。需要一个直接优化推理信实性的框架以提升推理质量与可信度。

Method: 建立信实性最大化目标；提出几何化奖励设计；引入信实感知的优势调制以对逐步推理给予 credit；惩罚不支持的步骤，同时保留有效的部分推理路径；在多种骨干和数据集上验证。

Result: FaithRL 在多种骨干与基准上持续减少幻觉率，同时维持或提高答案正确性；分析表明逐步推理的信实性提高，且具有良好泛化性。

Conclusion: 通过直接优化推理信实性，FaithRL 成为一种可普适应用的强化学习框架，能够显著降低幻觉并提升推理质量，提供了理论与实验上的支持与实现途径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly improved the performance of Large Language Models (LLMs) on tasks requiring multi-step reasoning. However, most RLVR pipelines rely on sparse outcome-based rewards, providing little supervision over intermediate steps and thus encouraging over-confidence and spurious reasoning, which in turn increases hallucinations. To address this, we propose FaithRL, a general reinforcement learning framework that directly optimizes reasoning faithfulness. We formalize a faithfulness-maximization objective and theoretically show that optimizing it mitigates over-confidence. To instantiate this objective, we introduce a geometric reward design and a faithfulness-aware advantage modulation mechanism that assigns step-level credit by penalizing unsupported steps while preserving valid partial derivations. Across diverse backbones and benchmarks, FaithRL consistently reduces hallucination rates while maintaining (and often improving) answer correctness. Further analysis confirms that FaithRL increases step-wise reasoning faithfulness and generalizes robustly. Our code is available at https://github.com/aintdoin/FaithRL.

</details>


### [41] [Can Large Language Models Generalize Procedures Across Representations?](https://arxiv.org/abs/2602.03542)
*Fangru Lin,Valentin Hofmann,Xingchen Wan,Weixing Wang,Zifeng Ding,Anthony G. Cohn,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: 两阶段数据课程（先符号数据再自然语言数据）显著提升LLM在跨表示任务中的泛化能力，较小模型亦能接近GPT-4o在自然性规划任务的零-shot表现。


<details>
  <summary>Details</summary>
Motivation: 研究跨表征泛化：尽管LLMs大量在符号表示上训练，但现实任务多以自然语言给出，需探究代码/图等符号表示到自然语言的跨表示泛化能力，以及仅在自然语言上训练的低效问题。

Method: 提出两阶段数据课程：先使用符号数据（代码/图）进行训练，然后引入自然语言数据；在多种模型家族和任务上评估跨表示泛化，比较与仅符号或仅NL训练的基线；分析为跨表示泛化的生成类比（generative analogy）。

Result: 课程显著提升性能，跨模型、跨任务有效；1.5B Qwen模型按该方法训练后，几乎达到零-shot GPT-4o在自然语言规划任务的水平；对跨表示泛化的解释为生成类比。

Conclusion: 跨表示泛化可以通过符号→自然语言的分阶段课程实现，且可视为促进生成类比能力的训练信号。

Abstract: Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.

</details>


### [42] [Assessing the Impact of Typological Features on Multilingual Machine Translation in the Age of Large Language Models](https://arxiv.org/abs/2602.03551)
*Vitalii Hirak,Jaap Jumelet,Arianna Bisazza*

Main category: cs.CL

TL;DR: 目标语言的语言学类型学属性显著影响大规模多语种MT模型的翻译质量；对部分语言，扩展输出空间搜索（而非仅左到右束搜索）可带来更大收益；并发布了212种FLORES+语言的细粒度类型学特征。


<details>
  <summary>Details</summary>
Motivation: 探究目标语言的类型学属性在大规模预训练的多语种翻译模型中的作用，超越资源丰富度和书写脚本等表面的因素，以理解跨语言建模的难度差异。

Method: 评估两种大型多语种翻译模型（NLLB-200：编码器-解码器；Tower+：解码器仅模型）在广泛语言上的表现；控制数据资源、书写系统等因素；分析类型学属性与翻译质量的关系；探讨比标准从左到右束搜索更广的输出空间搜索对某些语言的潜在收益；公开212种FLORES+语言的细粒度类型学属性。

Result: 在控制资源和书写系统等因素后，目标语言的类型学属性仍驱动两种模型的翻译质量；某些类型学特性的语言在更宽的输出空间搜索下获得更大收益，提示对这些语言可采用替代解码策略；研究还提供了可用于进一步研究的细粒度类型学特征集（覆盖212种FLORES+语言）。

Conclusion: 语言类型学属性是跨语言MT性能的关键决定因素之一，适当的解码策略改进可缓解部分语言的翻译难点；研究产出有助于为多语言MT的评估与优化提供更细粒度的语言特征数据。

Abstract: Despite major advances in multilingual modeling, large quality disparities persist across languages. Besides the obvious impact of uneven training resources, typological properties have also been proposed to determine the intrinsic difficulty of modeling a language. The existing evidence, however, is mostly based on small monolingual language models or bilingual translation models trained from scratch. We expand on this line of work by analyzing two large pre-trained multilingual translation models, NLLB-200 and Tower+, which are state-of-the-art representatives of encoder-decoder and decoder-only machine translation, respectively. Based on a broad set of languages, we find that target language typology drives translation quality of both models, even after controlling for more trivial factors, such as data resourcedness and writing script. Additionally, languages with certain typological properties benefit more from a wider search of the output space, suggesting that such languages could profit from alternative decoding strategies beyond the standard left-to-right beam search. To facilitate further research in this area, we release a set of fine-grained typological properties for 212 languages of the FLORES+ MT evaluation benchmark.

</details>


### [43] [ACL: Aligned Contrastive Learning Improves BERT and Multi-exit BERT Fine-tuning](https://arxiv.org/abs/2602.03563)
*Wei Zhu*

Main category: cs.CL

TL;DR: 提出Aligned Contrastive Learning (ACL) 框架，解决监督场景中交叉熵与对比学习目标的冲突，通过 ACL-Embed、ACL-Grad、ACL-CL 实现对齐、冲突抑制和跨层引导，在 GLUE 与多出口 BERT 中表现优越。


<details>
  <summary>Details</summary>
Motivation: 在监督设置中，对比学习常与交叉熵（CE）冲突，限制对比学习的应用，需要新框架实现对齐与协同优化。

Method: ACL-Embed 将标签嵌入视为额外的增强样本，使用对比学习使标签嵌入与样本表示对齐；ACL-Grad 通过在两目标冲突时舍弃 ACL-Embed 项来协同优化 CE 与 ACL；ACL-CL 通过教师出口引导学生浅层出口的优化。

Result: 在 GLUE 任务上，ACL-BRT 相较 CE 或 CE+SCL 表现相当或更好；在多出口 BERT 的微调中，ACL 尤其 CL-ACL 显著优于基线，提供更好的质量-速度权衡。

Conclusion: 在监督场景下给出可行的对比学习整合框架，提升表示质量与推理效率，尤其在低延迟场景具备潜在优势。

Abstract: Despite its success in self-supervised learning, contrastive learning is less studied in the supervised setting. In this work, we first use a set of pilot experiments to show that in the supervised setting, the cross-entropy loss objective (CE) and the contrastive learning objective often conflict with each other, thus hindering the applications of CL in supervised settings. To resolve this problem, we introduce a novel \underline{A}ligned \underline{C}ontrastive \underline{L}earning (ACL) framework. First, ACL-Embed regards label embeddings as extra augmented samples with different labels and employs contrastive learning to align the label embeddings with its samples' representations. Second, to facilitate the optimization of ACL-Embed objective combined with the CE loss, we propose ACL-Grad, which will discard the ACL-Embed term if the two objectives are in conflict. To further enhance the performances of intermediate exits of multi-exit BERT, we further propose cross-layer ACL (ACL-CL), which is to ask the teacher exit to guide the optimization of student shallow exits. Extensive experiments on the GLUE benchmark results in the following takeaways: (a) ACL-BRT outperforms or performs comparably with CE and CE+SCL on the GLUE tasks; (b) ACL, especially CL-ACL, significantly surpasses the baseline methods on the fine-tuning of multi-exit BERT, thus providing better quality-speed tradeoffs for low-latency applications.

</details>


### [44] [Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs](https://arxiv.org/abs/2602.03578)
*Su Dong,Qinggang Zhang,Yilin Xiao,Shengyuan Chen,Chuang Zhou,Xiao Huang*

Main category: cs.CL

TL;DR: 提出一个自适应的 GraphRAG 框架 EA-GraphRAG，通过语法感知的复杂度分析在 RAG 与 GraphRAG 之间进行动态路由，结合紧凑的特征抽取、轻量化复杂度评分和基于分数的边界案例融合，在简单与复杂查询混合场景中实现更高准确率和更低延迟，达到对多跳问答的状态-of-the-art 性能。


<details>
  <summary>Details</summary>
Motivation: LLMs 在知识密集型任务中易产生幻觉、参数知识过时；RAG 通过检索外部知识缓解但面向非结构化文档的信息碎片化限制了效果；GraphRAG 引入知识图提升推理能力，但在真实场景往往显著降速且不如单纯的 RAG；因此需要一个对查询复杂度进行感知并自适应选择检索范式的框架。

Method: 提出三要素：1) 语法特征构造器，对每个查询进行解析并提取结构特征；2) 轻量级复杂度评分器，将特征映射为连续的复杂度分数；3) 基于分数的路由策略：对低分查询采用密集 RAG；对高分查询调用图检索；对边界情况应用复杂度感知的互补排序融合（reciprocal rank fusion）。

Result: 在包含两个单跳和两个多跳问答基准的大型实验中，EA-GraphRAG 显著提升准确率、降低延迟，并在简单/复杂混合场景下实现了前沿（state-of-the-art）性能。

Conclusion: 通过将 RAG 与 GraphRAG 的优势进行分数驱动的自适应切换，解决了 GraphRAG 在现实场景中的鲁棒性与效率问题，证明了对查询复杂度的感知与路由对多样化问答的有效性。

Abstract: Large language models (LLMs) often struggle with knowledge-intensive tasks due to hallucinations and outdated parametric knowledge. While Retrieval-Augmented Generation (RAG) addresses this by integrating external corpora, its effectiveness is limited by fragmented information in unstructured domain documents. Graph-augmented RAG (GraphRAG) emerged to enhance contextual reasoning through structured knowledge graphs, yet paradoxically underperforms vanilla RAG in real-world scenarios, exhibiting significant accuracy drops and prohibitive latency despite gains on complex queries. We identify the rigid application of GraphRAG to all queries, regardless of complexity, as the root cause. To resolve this, we propose an efficient and adaptive GraphRAG framework called EA-GraphRAG that dynamically integrates RAG and GraphRAG paradigms through syntax-aware complexity analysis. Our approach introduces: (i) a syntactic feature constructor that parses each query and extracts a set of structural features; (ii) a lightweight complexity scorer that maps these features to a continuous complexity score; and (iii) a score-driven routing policy that selects dense RAG for low-score queries, invokes graph-based retrieval for high-score queries, and applies complexity-aware reciprocal rank fusion to handle borderline cases. Extensive experiments on a comprehensive benchmark, consisting of two single-hop and two multi-hop QA benchmarks, demonstrate that our EA-GraphRAG significantly improves accuracy, reduces latency, and achieves state-of-the-art performance in handling mixed scenarios involving both simple and complex queries.

</details>


### [45] [$V_0$: A Generalist Value Model for Any Policy at State Zero](https://arxiv.org/abs/2602.03584)
*Yi-Kai Zhang,Zhiyuan Yao,Hongyan Hao,Yueqing Sun,Qi Gu,Hui Su,Xunliang Cai,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.CL

TL;DR: 提出 V0 通用值模型，用于在未见提示上估计模型表现，解耦参数更新；在 GRPO 中作为资源调度者和路由器，结合状态零的价值估计，实现性能与成本的 Pareto 最优权衡。


<details>
  <summary>Details</summary>
Motivation: 解决在 LLM 的 Actor-Critic/策略梯度框架中，值模型需随策略演化而同步更新，带来高昂开销；GRPO 需要大量采样以保持估计稳定性。提出无需参数更新、以上下文驱动的值估计以降低成本并提升泛化。

Method: 引入 V0 作为通用值模型，将策略动态能力视为可观测上下文输入；以历史指令-性能对构建模型履历，在状态零进行价值估计；在 GRPO 训练中用于 rollout 前的成功率预测以进行采样预算分配；在部署阶段作为路由器，将指令分发给成本最优且合适的模型。

Result: 实验证明 V0 在比启发式预算分配更优的同时，实现了 LLM 路由任务中的帕累托最优的性能-成本权衡。

Conclusion: V0 提供了一种无需更新参数的通用价值估计能力，适用于训练阶段的资源调度与部署阶段的路由决策，且具备对未见提示的良好泛化潜力。

Abstract: Policy gradient methods rely on a baseline to measure the relative advantage of an action, ensuring the model reinforces behaviors that outperform its current average capability. In the training of Large Language Models (LLMs) using Actor-Critic methods (e.g., PPO), this baseline is typically estimated by a Value Model (Critic) often as large as the policy model itself. However, as the policy continuously evolves, the value model requires expensive, synchronous incremental training to accurately track the shifting capabilities of the policy. To avoid this overhead, Group Relative Policy Optimization (GRPO) eliminates the coupled value model by using the average reward of a group of rollouts as the baseline; yet, this approach necessitates extensive sampling to maintain estimation stability. In this paper, we propose $V_0$, a Generalist Value Model capable of estimating the expected performance of any model on unseen prompts without requiring parameter updates. We reframe value estimation by treating the policy's dynamic capability as an explicit context input; specifically, we leverage a history of instruction-performance pairs to dynamically profile the model, departing from the traditional paradigm that relies on parameter fitting to perceive capability shifts. Focusing on value estimation at State Zero (i.e., the initial prompt, hence $V_0$), our model serves as a critical resource scheduler. During GRPO training, $V_0$ predicts success rates prior to rollout, allowing for efficient sampling budget allocation; during deployment, it functions as a router, dispatching instructions to the most cost-effective and suitable model. Empirical results demonstrate that $V_0$ significantly outperforms heuristic budget allocation and achieves a Pareto-optimal trade-off between performance and cost in LLM routing tasks.

</details>


### [46] [CL-bench: A Benchmark for Context Learning](https://arxiv.org/abs/2602.03587)
*Shihan Dou,Ming Zhang,Zhangyue Yin,Chenhao Huang,Yujiong Shen,Junzhe Wang,Jiayi Chen,Yuchen Ni,Junjie Ye,Cheng Zhang,Huaibing Xie,Jianglu Hu,Shaolei Wang,Weichao Wang,Yanling Xiao,Yiting Liu,Zenan Xu,Zhen Guo,Pluto Zhou,Tao Gui,Zuxuan Wu,Xipeng Qiu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Di Wang,Shunyu Yao*

Main category: cs.CL

TL;DR: CL-bench 是一个现实世界的上下文学习基准，包含 500 个复杂情境、1,899 个任务和 31,607 条验证性评估准则，旨在评估语言模型从任务特定上下文中学习新知识并进行推理的能力。目前在十个前沿大模型上平均仅解决 17.2% 的任务，最佳为 GPT-5.1 的 23.7%，显示该能力仍然不足。


<details>
  <summary>Details</summary>
Motivation: 现实任务需要模型利用任务特定上下文中新的、超出预训练知识的内容来推理与解决问题，这种“上下文学习”是人类天然具备但在现有模型中尚被忽视的能力；现有的长上下文测试和提示学习并不能充分覆盖该能力。

Method: 构建一个包含 500 个复杂情境、1,899 个任务、31,607 条评估准则的基准集 CL-bench，由经验丰富的领域专家设计。每个任务要求解决所需的新内容都包含在对应的情境中；评估对象为十个前沿大模型，强调从情境中学习域知识、规则体系、复杂流程及从经验数据推导出的规律等，超出预训练覆盖范围。

Result: 在测试十个前沿语言模型时，平均解决任务比例为 17.2%，表现最佳的 GPT-5.1 仅为 23.7%。

Conclusion: 结果表明当前大模型尚未具备有效的上下文学习能力，这对在真实世界场景中的部署构成瓶颈；CL-bench 提供了一条推动路劲，帮助研究者和开发者打造具备此关键能力的语言模型。

Abstract: Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge. However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning, a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension, and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1, solves only 23.7%, revealing that LMs have yet to achieve effective context learning, which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.

</details>


### [47] [Efficient Algorithms for Partial Constraint Satisfaction Problems over Control-flow Graphs](https://arxiv.org/abs/2602.03588)
*Xuran Cai,Amir Goharshady*

Main category: cs.CL

TL;DR: 提出了一种在系列并联-循环(SPL)图上的部分约束满足问题(PCSP)的一般算法，时间复杂度为 O(|G|·|D|^6)，对固定域D时线性依赖于图大小；并将该框架统一并推广至寄存器分配、LOSPRE等任务，同时在最优银行选择问题上给出四倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 编译器优化任务常可建模为PCSP，但控制流图（CFG）通常稀疏且可分解，SPL分解可提供高效求解结构。

Method: 基于SPL图的PCSP求解算法，采用动态规划式的分解与组合来处理Graf的分解结构，给出总时间复杂度并实现对多任务的统一框架；将先前基于SPL的寄存器分配与LOSPRE方法统一到一个通用框架。

Result: 复杂度为 O(|G|·|D|^6)，固定域D时线性；在Optimal Bank Selection任务上实现了比前代四倍的速度提升，实验结果证实了性能改进。

Conclusion: 提供了一个统一的SPL图上PCSP求解框架，理论与实验均表明其有效性，适用于多种编译优化场景，且具有向更多PCSP任务扩展的潜力。

Abstract: In this work, we focus on the Partial Constraint Satisfaction Problem (PCSP) over control-flow graphs (CFGs) of programs. PCSP serves as a generalization of the well-known Constraint Satisfaction Problem (CSP). In the CSP framework, we define a set of variables, a set of constraints, and a finite domain $D$ that encompasses all possible values for each variable. The objective is to assign a value to each variable in such a way that all constraints are satisfied. In the graph variant of CSP, an underlying graph is considered and we have one variable corresponding to each vertex of the graph and one or several constraints corresponding to each edge. In PCSPs, we allow for certain constraints to be violated at a specified cost, aiming to find a solution that minimizes the total cost. Numerous classical compiler optimization tasks can be framed as PCSPs over control-flow graphs. Examples include Register Allocation, Lifetime-optimal Speculative Partial Redundancy Elimination (LOSPRE), and Optimal Placement of Bank Selection Instructions. On the other hand, it is well-known that control-flow graphs of structured programs are sparse and decomposable in a variety of ways. In this work, we rely on the Series-Parallel-Loop (SPL) decompositions as introduced by~\cite{RegisterAllocation}. Our main contribution is a general algorithm for PCSPs over SPL graphs with a time complexity of \(O(|G| \cdot |D|^6)\), where \(|G|\) represents the size of the control-flow graph. Note that for any fixed domain $D,$ this yields a linear-time solution. Our algorithm can be seen as a generalization and unification of previous SPL-based approaches for register allocation and LOSPRE. In addition, we provide experimental results over another classical PCSP task, i.e. Optimal Bank Selection, achieving runtimes four times better than the previous state of the art.

</details>


### [48] [Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation](https://arxiv.org/abs/2602.03619)
*Changze Lv,Jie Zhou,Wentao Zhao,Jingwen Xu,Zisu Huang,Muzhao Tian,Shihan Dou,Tao Gui,Le Tian,Xiao Zhou,Xiaoqing Zheng,Xuanjing Huang,Jie Zhou*

Main category: cs.CL

TL;DR: 提出一个面向 DeepResearch 报告生成的、基于人类偏好和大型语言模型评估的查询特定评分量表生成器，并通过多智能体 MaMs 框架与混合奖励的强化学习提升评估与生成质量。实验显示相较现有设计更具区分力和对人类对齐性，在 MaMs 框架下优于开源基线，与闭源模型持平。


<details>
  <summary>Details</summary>
Motivation: 当前没有可验证的奖励信号，常用的评分量表要么粗粒度、要么需要人工构建且难以扩展，亟需可扩展且对人类偏好对齐的评分生成方法；通过人类偏好监督和 LLM 评估的混合奖励来训练查询特定的评分器，以更精细地引导生成；为解决长距离推理问题，提出 MaMs 工作流以提升报告生成质量。

Method: 构建一个带人类偏好标注的 DeepResearch 风格查询数据集，数据集中对成对报告给出人类偏好；通过强化学习训练评分器（rubric generators），奖励信号来自人类偏好监督与基于 LLM 的量表评估的混合奖励；提出 Multi-agent Markov-state（MaMs）工作流以提升长 horizon 的报告生成与推理能力。

Result: 所提出的评分器在区分度和对人类偏好的对齐性方面优于现有的评分策略；在 MaMs 框架下，配备该评分器的 DeepResearch 系统在 DeepResearch Bench 上全面超过所有开源基线，且性能可与领先的闭源模型相媲美。

Conclusion: 通过可人类偏好对齐且查询特定的评分器与 MaMs 框架，提升对报告生成的评估引导与生成质量，具有更强的可扩展性与对齐性，适用于大规模、长距离推理任务的深度研究报告生成。

Abstract: Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.

</details>


### [49] [BIRDTurk: Adaptation of the BIRD Text-to-SQL Dataset to Turkish](https://arxiv.org/abs/2602.03633)
*Burak Aktaş,Mehmet Can Baytekin,Süha Kağan Köse,Ömer İlbilgi,Elif Özge Yılmaz,Çağrı Toraman,Bilge Kaan Görür*

Main category: cs.CL

TL;DR: This paper introduces BIRDTurk, the first Turkish adaptation of the BIRD benchmark for Text-to-SQL. It uses a controlled translation pipeline that localizes schema identifiers to Turkish while preserving SQL semantics, with translation quality evaluated via a Central Limit Theorem-based sample achieving 98.15% human-evaluated accuracy. It benchmarks inference-based prompting, agentic multi-stage reasoning, and supervised fine-tuning, finding Turkish causes consistent performance degradation due to linguistic divergence and low pretraining representation; agentic reasoning shows better cross-lingual robustness, while supervised fine-tuning struggles with standard multilingual baselines but scales with instruction-tuned models. BIRDTurk serves as a controlled cross-lingual evaluation testbed, with training/development splits released for future work.


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL systems perform well in English but lag in morphologically rich, low-resource languages. There is a need for controlled, realistic cross-lingual benchmarks to study how language-specific factors affect parsing natural language into SQL queries.

Method: A controlled translation pipeline translates Turkish, adapting schema identifiers to Turkish while strictly preserving SQL structure and execution semantics. Translation quality is validated on a sample determined by the Central Limit Theorem for 95% confidence, achieving 98.15% accuracy on human-evaluated samples. Experiments evaluate inference-based prompting, agentic multi-stage reasoning, and supervised fine-tuning across Turkish and multilingual baselines.

Result: Turkish introduces consistent performance degradation due to structural linguistic divergence and underrepresentation in LLM pretraining. Agentic reasoning shows stronger cross-lingual robustness; supervised fine-tuning is challenging for standard multilingual baselines but scales with modern instruction-tuned models.

Conclusion: BIRDTurk provides a controlled cross-lingual Text-to-SQL evaluation testbed under realistic database conditions. The authors release training and development splits to support future research.

Abstract: Text-to-SQL systems have achieved strong performance on English benchmarks, yet their behavior in morphologically rich, low-resource languages remains largely unexplored. We introduce BIRDTurk, the first Turkish adaptation of the BIRD benchmark, constructed through a controlled translation pipeline that adapts schema identifiers to Turkish while strictly preserving the logical structure and execution semantics of SQL queries and databases. Translation quality is validated on a sample size determined by the Central Limit Theorem to ensure 95% confidence, achieving 98.15% accuracy on human-evaluated samples. Using BIRDTurk, we evaluate inference-based prompting, agentic multi-stage reasoning, and supervised fine-tuning. Our results reveal that Turkish introduces consistent performance degradation, driven by both structural linguistic divergence and underrepresentation in LLM pretraining, while agentic reasoning demonstrates stronger cross-lingual robustness. Supervised fine-tuning remains challenging for standard multilingual baselines but scales effectively with modern instruction-tuned models. BIRDTurk provides a controlled testbed for cross-lingual Text-to-SQL evaluation under realistic database conditions. We release the training and development splits to support future research.

</details>


### [50] [TRE: Encouraging Exploration in the Trust Region](https://arxiv.org/abs/2602.03635)
*Chao Huang,Yujing Lu,Quangang Li,Shenghe Wang,Yan Wang,Yueyang Zhang,Long Xia,Jiashu Zhao,Zhiyuan Sun,Daiting Shi,Tingwen Liu*

Main category: cs.CL

TL;DR: 提出 TRE（Trust Region Entropy）在模型的信任区域内进行探索，解决大词汇表和长序列下全局熵最大化导致的尾部风险，从而提升在数学推理、组合搜索和偏好对齐任务的表现。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型中，全局熵最大化会将概率质量分散到大量无效的候选项，削弱连贯推理与决策，因此需要限制探索范围以强化对 plausible 候选项的聚焦。

Method: 引入 TRE，通过在模型的信任区域内进行探索，结合现有强化学习框架（如 PPO）进行训练，避免对无关 token 的概率质量分散。

Result: 在 MATH、Countdown、HH 等任务上，TRE 持续优于 vanilla PPO、标准熵正则与其他探索基线，且代码公开。

Conclusion: TRE 有效缓解大词汇表下的尾部风险，提供可迁移的探索框架，提升复杂任务的稳健性和效率。

Abstract: Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model's trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.

</details>


### [51] [Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration](https://arxiv.org/abs/2602.03677)
*Yu Zhang,Mufan Xu,Xuefeng Bai,Kehai chen,Pengfei Zhang,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 通过信息流视角分析多模态大语言模型中的模态跟随机制，发现指令标记作为结构性锚点；浅层注意力实现非选择性信息传递并将多模态线索路由到锚点；深层注意力在指令意图指引下解决模态竞争；MLP 层表现出语义惰性，呈对抗作用；存在少量专门的注意头驱动仲裁；对约5%的关键头进行因果干预可使模态跟随率在60%范围内下降或上升；为模型透明度和多模态信息编排提供一个原理性框架。


<details>
  <summary>Details</summary>
Motivation: 揭示模态跟随的决策机制，提升可解释性、安全性与可靠性，弥补对其信息流机制理解的不足。

Method: 采用信息流视角系统分析模型内部的传递与争夺，将指令标记视为结构锚点；识别浅层/深层注意力及MLP对信息流的不同作用；定位并筛选出稀疏的关键注意头；通过因果干预（屏蔽/放大）对约5%关键头进行操控以评估对模态跟随的影响。

Result: 发现：1) 指令令牌是模态仲裁的结构锚点；2) 浅层注意力进行非选择性信息传递，将多模态线索路由至锚点；3) 深层注意力在指令意图指引下解决模态竞争；4) MLP 层表现出语义惰性，具有对抗作用；5) 存在少量专门的注意头驱动仲裁；6) 通过对约5%的关键头的因果干预，屏蔽可使模态跟随率下降约60%，定向放大失败样本可使模态跟随率上升约60%。

Conclusion: 提出模型透明度的实证框架和多模态信息编排的原理性路线，指示在少量关键组件上进行干预即可显著改变模态跟随行为，具备潜在的应用与研究价值。

Abstract: Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere $5\%$ of these critical heads can decrease the modality-following ratio by $60\%$ through blocking, or increase it by $60\%$ through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.

</details>


### [52] [Neural Attention Search Linear: Towards Adaptive Token-Level Hybrid Attention Models](https://arxiv.org/abs/2602.03681)
*Difan Deng,Andreas Bentzen Winje,Lukas Fehring,Marius Lindauer*

Main category: cs.CL

TL;DR: 提出一个面向令牌级的混合注意力框架 NAtS-L，在同一层内对不同令牌同时应用线性注意力与 softmax 注意力，通过搜索确定哪些令牌由线性注意力处理、哪些令牌需要 softmax 以实现长期信息检索，从而在保持表达能力的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 缓解 softmax 注意力在长上下文中的二次复杂度问题；单纯的线性注意力受限于隐藏状态的容量，难以表达长期依赖。需要一种在保持表达能力的前提下提高效率的混合策略。

Method: 在同一层内对不同令牌同时应用线性注意力和 softmax 注意力，NAtS-L 自动判断哪些令牌可用线性注意力编码为固定大小的隐藏状态，哪些令牌需要保留以进行长期检索；通过搜索最优的 Gated DeltaNet 与 softmax 注意力组合实现令牌级的混合结构。

Result: 证明 NAtS-L 能在令牌层级实现高效与表达力的折中，构建出强大且高效的混合体系结构。

Conclusion: NAtS-L 提供了一种新颖的令牌级自适应注意力框架，在单层内将线性注意力与 softmax 注意力融合，通过搜索确定令牌分配，从而在长上下文下实现更高效且保持表达能力的模型。

Abstract: The quadratic computational complexity of softmax transformers has become a bottleneck in long-context scenarios. In contrast, linear attention model families provide a promising direction towards a more efficient sequential model. These linear attention models compress past KV values into a single hidden state, thereby efficiently reducing complexity during both training and inference. However, their expressivity remains limited by the size of their hidden state. Previous work proposed interleaving softmax and linear attention layers to reduce computational complexity while preserving expressivity. Nevertheless, the efficiency of these models remains bottlenecked by their softmax attention layers. In this paper, we propose Neural Attention Search Linear (NAtS-L), a framework that applies both linear attention and softmax attention operations within the same layer on different tokens. NAtS-L automatically determines whether a token can be handled by a linear attention model, i.e., tokens that have only short-term impact and can be encoded into fixed-size hidden states, or require softmax attention, i.e., tokens that contain information related to long-term retrieval and need to be preserved for future queries. By searching for optimal Gated DeltaNet and softmax attention combinations across tokens, we show that NAtS-L provides a strong yet efficient token-level hybrid architecture.

</details>


### [53] [Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.03689)
*Jiashuo Sun,Pengcheng Jiang,Saizhuo Wang,Jiajun Fan,Heng Wang,Siru Ouyang,Ming Zhong,Yizhu Jiao,Chengsong Huang,Xueqiang Xu,Pengrui Han,Peiran Li,Jiaxin Huang,Ge Liu,Heng Ji,Jiawei Han*

Main category: cs.CL

TL;DR: BAR-RAG 提出边界感知的证据选择器，将证据限定在“Goldilocks Zone”（既非过于显易也非完全不可解），通过对生成器反馈的强化学习训练证据选择器，并采用两阶段微调以对齐训练和推理中的证据分布，从而提高在存在检索噪声时的端到端RAG性能；在知识密集型问答基准上实现约10.3%的提升，且鲁棒性显著提升，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在现实检索噪声下仍脆弱，即使所需证据出现在前K名中，检索器和重排序器也仅优化相关性，可能选择对生成器帮助有限或无法提供关键推理信息的证据。需让证据既有挑战性又有足够信息以支持生成器推理。

Method: 将重排序器改为边界感知的证据选择器，目标是在生成器的Goldilocks Zone内挑选证据，即既不太易被直接回答，也非根本无法回答的证据，具有最大学习信号强度。通过对生成器反馈的强化学习训练选择器，并采用两阶段管线：第一阶段在诱导证据分布上对生成器进行微调以减小训练与推理的分布不匹配；第二阶段在该分布下实现端到端微调。

Result: 在知识密集型问答基准上，BAR-RAG在存在噪声检索条件下持续提升端到端性能，相较强基线（RAG与重排序）平均提升约10.3%，并显著提升鲁棒性。代码公开在GitHub。

Conclusion: BAR-RAG通过将证据选择与生成器需求对齐、并通过强化学习和两阶段微调缓解训练-推理分布差异，显著提升RAG系统在检索噪声下的稳健性与性能，具备较强的实际应用潜力。

Abstract: Retrieval-Augmented Generation (RAG) systems remain brittle under realistic retrieval noise, even when the required evidence appears in the top-K results. A key reason is that retrievers and rerankers optimize solely for relevance, often selecting either trivial, answer-revealing passages or evidence that lacks the critical information required to answer the question, without considering whether the evidence is suitable for the generator. We propose BAR-RAG, which reframes the reranker as a boundary-aware evidence selector that targets the generator's Goldilocks Zone -- evidence that is neither trivially easy nor fundamentally unanswerable for the generator, but is challenging yet sufficient for inference and thus provides the strongest learning signal. BAR-RAG trains the selector with reinforcement learning using generator feedback, and adopts a two-stage pipeline that fine-tunes the generator under the induced evidence distribution to mitigate the distribution mismatch between training and inference. Experiments on knowledge-intensive question answering benchmarks show that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3 percent over strong RAG and reranking baselines while substantially improving robustness. Code is publicly avaliable at https://github.com/GasolSun36/BAR-RAG.

</details>


### [54] [OCRTurk: A Comprehensive OCR Benchmark for Turkish](https://arxiv.org/abs/2602.03693)
*Deniz Yılmaz,Evren Ayberk Munis,Çağrı Toraman,Süha Kağan Köse,Burak Aktaş,Mehmet Can Baytekin,Bilge Kaan Görür*

Main category: cs.CL

TL;DR: OCRTurk 是面向土耳其文档解析的基准数据集，包含三种难度和多种文档类型的180份文档，用于评估七种OCR模型在逐元素（layout element）上的表现。结果显示 PaddleOCR 在大多数逐元素指标上领先，除图形元素外，在易/中/难三类下均具有较高的标准化编辑距离（NED）分数，且不同文档类型存在表现差异，非学术文档较易，幻灯片最具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有基准多数针对高资源语言，且对土耳其等低资源场景覆盖不足，缺乏能真实反映多样文档场景的标准化基准，难以评估模型的鲁棒性和现实部署性能。

Method: 构建包含180份土耳其文档的基准（学术文章、论文、幻灯片、非学术文章），覆盖多种布局元素，设定三种难度级别；用七种 OCR 模型在逐元素指标上评估性能。

Result: 在三类难度下，PaddleOCR在大多数逐元素指标上领先，除了图形元素；在易/中/难子集上对图形的 NED 分数也较高；文档类型对性能的影响明显，非学术文档表现较好，幻灯片最难。

Conclusion: 提供了一个标准化的土耳其文档解析基准，揭示了不同模型在多文档类型和难度下的鲁棒性差异，为土耳其文档解析的研究与应用提供基线和方向；未来需改进对图形和幻灯片等复杂布局的解析能力，扩展覆盖和评价指标。

Abstract: Document parsing is now widely used in applications, such as large-scale document digitization, retrieval-augmented generation, and domain-specific pipelines in healthcare and education. Benchmarking these models is crucial for assessing their reliability and practical robustness. Existing benchmarks mostly target high-resource languages and provide limited coverage for low-resource settings, such as Turkish. Moreover, existing studies on Turkish document parsing lack a standardized benchmark that reflects real-world scenarios and document diversity. To address this gap, we introduce OCRTurk, a Turkish document parsing benchmark covering multiple layout elements and document categories at three difficulty levels. OCRTurk consists of 180 Turkish documents drawn from academic articles, theses, slide decks, and non-academic articles. We evaluate seven OCR models on OCRTurk using element-wise metrics. Across difficulty levels, PaddleOCR achieves the strongest overall results, leading most element-wise metrics except figures and attaining high Normalized Edit Distance scores in easy, medium, and hard subsets. We also observe performance variation by document type. Models perform well on non-academic documents, while slideshows become the most challenging.

</details>


### [55] [OmniRAG-Agent: Agentic Omnimodal Reasoning for Low-Resource Long Audio-Video Question Answering](https://arxiv.org/abs/2602.03707)
*Yifan Zhu,Xinyu Mu,Tao Feng,Zhonghong Ou,Yuning Gong,Haoran Luo*

Main category: cs.CL

TL;DR: OmniRAG-Agent is an agentic, budget-aware omnimodal QA framework that combines image-audio retrieval-augmented generation with an iterative tool-use loop, optimized via group-relative policy optimization, achieving strong results in low-resource long audio-video QA.


<details>
  <summary>Details</summary>
Motivation: Long-horizon omnimodal QA faces high encoding costs, weak fine-grained retrieval, limited proactive planning, and lack of end-to-end optimization in low-resource settings.

Method: An image-audio retrieval-augmented generation module retrieves short, relevant frames and audio snippets from external banks for an OmniLLM. An agent loop plans, calls tools across turns, and merges retrieved evidence to answer complex queries. Group-relative policy optimization jointly improves tool use and answer quality over time.

Result: OmniRAG-Agent consistently outperforms prior methods under low-resource settings on OmniVideoBench, WorldSense, and Daily-Omni, with ablations confirming the contribution of each component.

Conclusion: The framework offers an effective, scalable solution for budgeted long omnimodal QA by integrating retrieval-augmented generation, iterative agent reasoning, and joint policy optimization to enhance reasoning and answer quality.

Abstract: Long-horizon omnimodal question answering answers questions by reasoning over text, images, audio, and video. Despite recent progress on OmniLLMs, low-resource long audio-video QA still suffers from costly dense encoding, weak fine-grained retrieval, limited proactive planning, and no clear end-to-end optimization.To address these issues, we propose OmniRAG-Agent, an agentic omnimodal QA method for budgeted long audio-video reasoning. It builds an image-audio retrieval-augmented generation module that lets an OmniLLM fetch short, relevant frames and audio snippets from external banks. Moreover, it uses an agent loop that plans, calls tools across turns, and merges retrieved evidence to answer complex queries. Furthermore, we apply group relative policy optimization to jointly improve tool use and answer quality over time. Experiments on OmniVideoBench, WorldSense, and Daily-Omni show that OmniRAG-Agent consistently outperforms prior methods under low-resource settings and achieves strong results, with ablations validating each component.

</details>


### [56] [Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States](https://arxiv.org/abs/2602.03708)
*Ximing Dong,Shaowei Wang,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.CL

TL;DR: SemanticSpec 是一个语义感知的推理解码框架，通过对整个语义序列进行验证，而非逐字令牌，显著降低推理延迟，在不同基准上取得了显著加速。


<details>
  <summary>Details</summary>
Motivation: LLMs 在自回归解码下推理延迟高，特别是长链推理中延迟更甚；现有的基于令牌的推理猜测在语义等价性上忽略语义等价性，导致大量无效拒绝。

Method: 提出语义概率估计机制，探测模型内部隐藏状态以评估生成具有特定语义的序列的概率；以语义层面而非令牌逐步进行 speculate/verify。

Result: 在四项基准上实现显著加速：在 DeepSeekR1-32B 上最高可达 2.7x，在 QwQ-32B 上最高可达 2.1x；并在效率与有效性上持续优于令牌级和序列级基线。

Conclusion: 通过以语义含义为核心的验证，降低无效拒绝，提升 LRMs 的推理速度与稳定性，且在多基准上具有一致性收益。

Abstract: Large Language Models (LLMs) achieve strong performance across many tasks but suffer from high inference latency due to autoregressive decoding. The issue is exacerbated in Large Reasoning Models (LRMs), which generate lengthy chains of thought. While speculative decoding accelerates inference by drafting and verifying multiple tokens in parallel, existing methods operate at the token level and ignore semantic equivalence (i.e., different token sequences expressing the same meaning), leading to inefficient rejections. We propose SemanticSpec, a semantic-aware speculative decoding framework that verifies entire semantic sequences instead of tokens. SemanticSpec introduces a semantic probability estimation mechanism that probes the model's internal hidden states to assess the likelihood of generating sequences with specific meanings.Experiments on four benchmarks show that SemanticSpec achieves up to 2.7x speedup on DeepSeekR1-32B and 2.1x on QwQ-32B, consistently outperforming token-level and sequence-level baselines in both efficiency and effectiveness.

</details>


### [57] [No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding](https://arxiv.org/abs/2602.03709)
*Vynska Amalia Permadi,Xingwei Tan,Nafise Sadat Moosavi,Nikos Aletras*

Main category: cs.CL

TL;DR: 提出了 ID-MoCQA：面向印尼文化的第一個大規模多跳 QA 数据集，用以评估大型语言模型的文化理解能力，涵盖六种线索类型并提供英文/印尼文两种版本；通过专家评审与 LLM 评审相结合的多阶段验证，提升问答质量，实验显示现有模型在需要细致推理的文化理解任务上存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 文化理解需要跨情境、传统和隐性社会知识的推理，现有的文化QA 基准多为单跳问题，易被浅层线索利用。本研究通过构建跨六类线索的多跳推理链，填补对 LLM 文化推理能力的评测空白，并聚焦于印尼传统文化。

Method: 将单跳文化问题系统转化为跨六类线索类型的多跳推理链（如常识、时间、地理等），并设计多阶段验证管线：专家评审与基于 LLM 的评审筛选，确保高质量的问答对；数据集以印尼传统为基础，提供英文与印尼文版本。

Result: 在多家先进模型上进行评测，发现文化推理能力存在显著差距，尤其是在需要细致推断的任务上；该数据集具有挑战性，能够推动提升 LLM 的文化能力。

Conclusion: ID-MoCQA 为评测和推进 LLM 文化推理能力提供了一个具有挑战性的基准，促使模型在跨情境和隐性知识的推理方面取得进步。

Abstract: Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.

</details>


### [58] [CUBO: Self-Contained Retrieval-Augmented Generation on Consumer Laptops 10 GB Corpora, 16 GB RAM, Single-Device Deployment](https://arxiv.org/abs/2602.03731)
*Paolo Astrino*

Main category: cs.CL

TL;DR: CUBO 是一个在 16 GB 共享内存、15.5 GB RAM 上限的消费级笔记本上运行的系统导向型 RAG 平台，通过流式摄取、分层混合检索与硬件感知编排，在不离开本地的前提下实现高隐私保护的检索能力，达到 BEIR 多领域 Recall@10 0.48–0.97、p50 延迟 185 ms，代码规模约 37k 行，开源。


<details>
  <summary>Details</summary>
Motivation: 解决云端 AI 检索带来的 GDPR 风险与本地系统对大内存的需求之间的矛盾，面向中小型档案和专业工作场景，提供一个在端侧实现高效、隐私保护的 RAG 方案。

Method: 核心方法包括：1) 流式摄取，保持 O(1) 缓冲开销；2) 分层混合检索（可能结合密集向量和稀疏检索、缓存策略等）；3) 硬件感知的资源编排，针对 16 GB 内存环境优化；4) 大规模 BEIR 基准评估；5) 37k 行代码的实现与公开代码库。

Result: 在 BEIR 的多领域评测中实现 Recall@10 介于 0.48–0.97；在 1,300 台 C1 类笔记本上 p50 延迟为 185 ms；在 15.5 GB RAM 限制内实现本地数据处理，降低数据外泄风险；代码库对外开源。

Conclusion: 证明了面向消费级边缘设备的本地化 RAG 是可行的，具备实用性，适合小-中型专业档案的部署；未来可关注能耗分析、跨设备扩展、与云端方案对比以及用户研究等。

Abstract: Organizations handling sensitive documents face a tension: cloud-based AI risks GDPR violations, while local systems typically require 18-32 GB RAM. This paper presents CUBO, a systems-oriented RAG platform for consumer laptops with 16 GB shared memory. CUBO's novelty lies in engineering integration of streaming ingestion (O(1) buffer overhead), tiered hybrid retrieval, and hardware-aware orchestration that enables competitive Recall@10 (0.48-0.97 across BEIR domains) within a hard 15.5 GB RAM ceiling. The 37,000-line codebase achieves retrieval latencies of 185 ms (p50) on C1,300 laptops while maintaining data minimization through local-only processing aligned with GDPR Art. 5(1)(c). Evaluation on BEIR benchmarks validates practical deployability for small-to-medium professional archives. The codebase is publicly available at https://github.com/PaoloAstrino/CUBO.

</details>


### [59] [Context Compression via Explicit Information Transmission](https://arxiv.org/abs/2602.03784)
*Jiangnan Ye,Hanqi Yan,Zhenyi Shen,Heng Chang,Ye Mao,Yulan He*

Main category: cs.CL

TL;DR: 提出 ComprExIT：通过在冻结的LLM隐藏状态上执行显式信息传输，实现长上下文的高效压缩，解决自注意力压缩的两大结构局限，六项问答基准上优于现有方法，仅增加约1%参数。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理成本高，现有软压缩多依赖将模型本身作为可训练压缩器，存在跨层信息覆写与跨-token分配不协调的问题。

Method: 提出 ComprExIT：在冻结的LLM隐藏状态上进行显式信息传输。通过深度（depth-wise）传输将多层信息传递到 token anchors，缓解逐层覆写；通过宽度（width-wise）传输将 anchors 汇聚成少量槽位，利用全局优化的传输计划实现信息的协调分配。

Result: 在六项问答基准上，与最先进的上下文压缩方法相比，Consistently outperform，参数仅增加约1%。

Conclusion: 显式且协调的信息传输范式可实现更有效、鲁棒的长上下文压缩，且对模型内部自注意力结构无侵入，具有较强的泛化潜力。

Abstract: Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.

</details>


### [60] [They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References](https://arxiv.org/abs/2602.03822)
*Sahil Tripathi,Gautam Siddharth Kashyap,Mehwish Nasim,Jian Yang,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: CROSS-ALIGN+ 是一个三阶段框架，用结构化知识、LoRA 微调和级联解释解决 meme 基础的社交滥用检测中的文化盲区、边界模糊和可解释性问题，在五基准、八 LVLMs 上实现相对 F1 提升最高达 17%，并提供可解释性推理。


<details>
  <summary>Details</summary>
Motivation: 模因性社交滥用检测面临隐性文化符号和跨模态不一致性导致的伤意敏感性不足。现有方法在三方面存在不足：文化无知、边界模糊（讽刺与滥用的混淆）以及缺乏可解释性。需要系统引入结构化知识、明确决策边界，并输出可解释推理。

Method: Stage I：用 ConceptNet、Wikidata、Hatebase 等结构化知识丰富多模态表示；Stage II：通过参数高效的 LoRA 调参增强决策边界；Stage III：生成级联解释提升可解释性。并在五个基准和八个 LVLMs 上进行广泛实验。

Result: 实验显示 CROSS-ALIGN+ 在多个基准和模型上持续优于现有方法，达到最多 17% 的相对 F1 提升，并为每个判定提供可解释的理由。

Conclusion: 该框架系统性地缓解了文化盲区、边界模糊和可解释性不足等核心挑战，提升 meme 基于滥用检测的性能并提供可解释性输出。

Abstract: Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.

</details>


### [61] [Accelerating Scientific Research with Gemini: Case Studies and Common Techniques](https://arxiv.org/abs/2602.03837)
*David P. Woodruff,Vincent Cohen-Addad,Lalit Jain,Jieming Mao,Song Zuo,MohammadHossein Bateni,Simina Branzei,Michael P. Brenner,Lin Chen,Ying Feng,Lance Fortnow,Gang Fu,Ziyi Guan,Zahra Hadizadeh,Mohammad T. Hajiaghayi,Mahdi JafariRaviz,Adel Javanmard,Karthik C. S.,Ken-ichi Kawarabayashi,Ravi Kumar,Silvio Lattanzi,Euiwoong Lee,Yi Li,Ioannis Panageas,Dimitris Paparas,Benjamin Przybocki,Bernardo Subercaseaux,Ola Svensson,Shayan Taherijam,Xuan Wu,Eylon Yogev,Morteza Zadimoghaddam,Samson Zhou,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 通过与 Gemini Deep Think 等模型的深入人机协作，在理论计算、经济学、优化与物理等领域解决开放问题、反驳猜想、生成新证明，提炼出迭代改进、问题分解、跨学科知识转移等协作技巧；展示对抗性评审与自我验证循环等创新用法，论证 AI 可成为科学发现中的真正伙伴。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在高水平数学与理论研究中的作用与最佳协作模式，理解如何将 AI 融入理论研究工作流程。

Method: 基于多案例研究的互动式对话方法；将模型用作严格的评审者以发现证明漏洞；将模型嵌入“神经符号”循环以自动编写并执行代码以验证推导。

Result: 在多个领域取得与人类研究者协作的成功案例，解决开放问题、推翻猜想、生成新证明；提炼出通用的协作技巧，展示了超越常规对话界面的应用。

Conclusion: AI 可以不仅仅作为自动化工具，而是科学发现过程中的多功能伙伴，具有推动创造性研究的潜力，并为未来的人机协作提供了可操作的框架与洞察。

Abstract: Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.

</details>


### [62] [Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing](https://arxiv.org/abs/2602.03845)
*Tong Zheng,Chengsong Huang,Runpeng Dai,Yun He,Rui Liu,Xin Ni,Huiwen Bao,Kaishen Wang,Hongtu Zhu,Jiaxin Huang,Furong Huang,Heng Huang*

Main category: cs.CL

TL;DR: 提出了并行思维的2D探测接口，并据此设计了无需训练的Parallel-Probe控制器，通过共识提前停止与基于偏差的分支裁剪来优化在线并行推理的深度与宽度，获得更好的测试时扩展性且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 并行推理在提高吞吐的同时带来显著计算开销。现有方法多依赖局部信号，难以利用全局跨分支的动态信息，导致对资源的利用不充分。本文旨在提供一个能在在线阶段动态调控宽度与深度的机制，以实现更优的成本-性能权衡。

Method: 引入2D probing接口，周期性地从所有分支获得中间答案，以暴露宽-深动态。基于此 insights，提出Parallel-Probe：1) 基于共识的早停以规制推理深度；2) 以偏差为准的分支裁剪实现宽度的动态调整；3) 训练无关的在线控制。通过三个基准和多种模型的实验验证，展示在测试时的成本效率提升。

Result: Parallel-Probe在测试时间成本和序列令牌数量上均优于标准多数投票的基线，实现Pareto前沿的提升：相比多数投票，序列令牌减少至多35.8%，总令牌成本降低超过25.8%，且准确率保持竞争力。跨三类基准和多模型的广泛实验表明方法具有良好鲁棒性与泛化性。

Conclusion: 提出的2D探测与Parallel-Probe框架揭示了并行思维在宽深分配上的非单调性、分支长度异质性以及全局共识的早期稳定性。作为训练无关的在线控制器，Parallel-Probe在在线并行推理中实现了更优的成本-性能权衡，提升了可扩展性。

Abstract: Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce $\textbf{Parallel-Probe}$, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to $\textbf{35.8}$% and total token cost by over $\textbf{25.8}$% while maintaining competitive accuracy.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [63] [Design and Evaluation of Whole-Page Experience Optimization for E-commerce Search](https://arxiv.org/abs/2602.02514)
*Pratik Lahiri,Bingqing Ge,Zhou Qin,Aditya Jumde,Shuning Huo,Lucas Scottini,Yi Liu,Mahmoud Mamlouk,Wenyang Liu*

Main category: cs.IR

TL;DR: 提出并验证一个全页体验优化框架，用于电商SRP的非线性布局和视觉信号的联合优化，通过因果框架评估长期满意度，在行业A/B测试中实现品牌相关性1.86%提升和营收0.05%提升。


<details>
  <summary>Details</summary>
Motivation: 现有模型多依赖线性列表和短期信号，无法充分捕捉2D布局、视觉元素与项一致性对长期满意度的影响；需要新的框架来对齐长期与短期目标。

Method: 对项之间的相关性、2D布局和视觉信号的相互作用进行联合建模；使用因果推断/准实验数据构建长期满意度指标；在大规模实际平台进行A/B测试以评估效果。

Result: 在行业规模的A/B测试中，品牌相关性提升1.86%，同时实现统计显著的0.05%营收提升。结果支持方法的有效性和长期指标可追踪性。

Conclusion: 提出的全页体验优化框架可用于处理复杂SRP布局和视觉信号，具有提升长期客户体验与收入的潜力；未来工作可扩展至更多视觉信号、个性化以及跨页面影响的分析。

Abstract: E-commerce Search Results Pages (SRPs) are evolving from linear lists to complex, non-linear layouts, rendering traditional position-biased ranking models insufficient. Moreover, existing optimization frameworks typically maximize short-term signals (e.g., clicks, same-day revenue) because long-term satisfaction metrics (e.g., expected two-week revenue) involve delayed feedback and challenging long-horizon credit attribution. To bridge these gaps, we propose a novel Whole-Page Experience Optimization Framework. Unlike traditional list-wise rankers, our approach explicitly models the interplay between item relevance, 2D positional layout, and visual elements. We use a causal framework to develop metrics for measuring long-term user satisfaction based on quasi-experimental data. We validate our approach through industry-scale A/B testing, where the model demonstrated a 1.86% improvement in brand relevance (our primary customer experience metric) while simultaneously achieving a statistically significant revenue uplift of +0.05%

</details>


### [64] [Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval](https://arxiv.org/abs/2602.02827)
*Roi Pony,Adi Raz,Oshri Naparstek,Idan Friedman,Udi Barzelay*

Main category: cs.IR

TL;DR: Col-Bandit: a zero-shot, query-time pruning layer for multi-vector late-interaction retrievers that adaptively reveals only necessary token-level MaxSim to identify Top-K results, reducing MaxSim FLOPs up to 5x with minimal ranking loss.


<details>
  <summary>Details</summary>
Motivation: High-quality multi-vector late-interaction models (e.g., ColBERT) achieve state-of-the-art retrieval but incur heavy query-time cost due to exhaustive token-level interactions. Single-vector approximations trade accuracy for speed; a better approach is needed to prune interactions online without retraining or offline preprocessing.

Method: Cast reranking as a finite-population Top-K identification problem. Maintain uncertainty-aware bounds over partially observed document scores and adaptively reveal only the (doc, token) MaxSim entries necessary to determine top results under statistical decision bounds with tunable relaxation. Operate as a zero-shot, in-situ layer that sparsifies the interaction matrix on the fly, without index changes, offline preprocessing, or model retraining.

Result: Empirical evaluations on BEIR (textual) and REAL-MM-RAG (multimodal) show Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to five-fold, suggesting substantial redundancy in dense late-interaction scoring is exploitable at query time.

Conclusion: Dense late-interaction scoring contains redundancy that can be pruned efficiently at query time. Col-Bandit provides a plug-in, zero-shot pruning strategy that maintains ranking quality while significantly reducing computation without requiring changes to indices or retraining.

Abstract: Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-$K$ identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5$\times$, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time.

</details>


### [65] [Efficiency Optimizations for Superblock-based Sparse Retrieval](https://arxiv.org/abs/2602.02883)
*Parker Carlson,Wentai Xie,Rohil Shah,Tao Yang*

Main category: cs.IR

TL;DR: 提出一种简单而有效的超块裁剪策略，用以降低超块分数计算开销，同时在保持相关性的前提下与紧凑索引结构结合，适用于多模型和数据集的零-shot 配置。


<details>
  <summary>Details</summary>
Motivation: 降低学习稀疏检索(LSR)中超块级分数计算的开销，提升查询处理效率，并在 CPU 上保持语义匹配能力，同时实现对不同 LSR 模型和数据集的鲁棒性。

Method: 提出一种简单且有效的超块裁剪方案，与紧凑索引结构和鲁棒的零-shot配置相结合，并提供分析论证；在 MS MARCO 与 BEIR 数据集上进行评估。

Result: 在 MS MARCO 与 BEIR 数据集上验证，所提方案在提升检索效率的同时保持具有竞争力的相关性，且对多种 LSR 模型具有较好泛化性。

Conclusion: 该裁剪方案可成为高效稀疏检索的强有力替代方法，具有跨模型与跨数据集的泛化性，并可与紧凑索引及零-shot 配置并存。

Abstract: Learned sparse retrieval (LSR) is a popular method for first-stage retrieval because it combines the semantic matching of language models with efficient CPU-friendly algorithms. Previous work aggregates blocks into "superblocks" to quickly skip the visitation of blocks during query processing by using an advanced pruning heuristic. This paper proposes a simple and effective superblock pruning scheme that reduces the overhead of superblock score computation while preserving competitive relevance. It combines this scheme with a compact index structure and a robust zero-shot configuration that is effective across LSR models and multiple datasets. This paper provides an analytical justification and evaluation on the MS MARCO and BEIR datasets, demonstrating that the proposed scheme can be a strong alternative for efficient sparse retrieval.

</details>


### [66] [Distribution-Aware End-to-End Embedding for Streaming Numerical Features in Click-Through Rate Prediction](https://arxiv.org/abs/2602.03223)
*Jiahao Liu,Hongji Ruan,Weimin Zhang,Ziye Tong,Derick Tang,Zhanpeng Zeng,Qinsong Zeng,Peng Zhang,Tun Lu,Ning Gu*

Main category: cs.IR

TL;DR: DAES is an end-to-end numerical feature embedding framework for streaming CTR prediction that uses reservoir sampling-based distribution estimation and field-aware distribution modulation to incorporate distributional information and semantics in non-i.i.d. streaming data, achieving strong offline/online performance and production deployment.


<details>
  <summary>Details</summary>
Motivation: Conventional static binning relies on offline statistics and suffers semantic drift during bin boundary updates. End-to-end neural embeddings can preserve distributional information but struggle to incorporate distributional info in streaming data where i.i.d. assumptions fail. There is a need to integrate distributional information and context-dependent semantics in an online, streaming setting.

Method: DAES integrates distributional information with an adaptive modulation mechanism. It introduces an efficient reservoir-sampling-based method to estimate distributions in streaming data and two field-aware distribution modulation strategies to capture streaming distributions and field-dependent semantics.

Result: DAES significantly outperforms existing approaches in extensive offline and online experiments and has been fully deployed on a leading short-video platform with hundreds of millions of daily active users.

Conclusion: An end-to-end, distribution-aware numerical feature embedding framework for streaming CTR training effectively leverages distributional information and field context, addressing non-i.i.d. streaming data challenges and delivering strong practical performance.

Abstract: This paper explores effective numerical feature embedding for Click-Through Rate prediction in streaming environments. Conventional static binning methods rely on offline statistics of numerical distributions; however, this inherently two-stage process often triggers semantic drift during bin boundary updates. While neural embedding methods enable end-to-end learning, they often discard explicit distributional information. Integrating such information end-to-end is challenging because streaming features often violate the i.i.d. assumption, precluding unbiased estimation of the population distribution via the expectation of order statistics. Furthermore, the critical context dependency of numerical distributions is often neglected. To this end, we propose DAES, an end-to-end framework designed to tackle numerical feature embedding in streaming training scenarios by integrating distributional information with an adaptive modulation mechanism. Specifically, we introduce an efficient reservoir-sampling-based distribution estimation method and two field-aware distribution modulation strategies to capture streaming distributions and field-dependent semantics. DAES significantly outperforms existing approaches as demonstrated by extensive offline and online experiments and has been fully deployed on a leading short-video platform with hundreds of millions of daily active users.

</details>


### [67] [Learning to Select: Query-Aware Adaptive Dimension Selection for Dense Retrieval](https://arxiv.org/abs/2602.03306)
*Zhanyu Wu,Richong Zhang,Zhijie Nie*

Main category: cs.IR

TL;DR: 提出一个查询感知的自适应维度选择框架，通过将查询嵌入映射到逐维度的重要性分数来选择子集维度用于相似度计算，在无需伪相关反馈的情况下提升密集检索性能。


<details>
  <summary>Details</summary>
Motivation: 密集检索的高维嵌入在查询层面存在冗余：对于同一信息需求，只有部分维度对排序有持续帮助。现有的PRF基于维度重要性估计易受噪声信号和测试时启发式策略影响，而全局共享的监督适配器对不同查询缺乏查询感知的维度重要性建模。

Method: 构建一个以监督相关标签为基础的“ oracle”维度重要性分布，并训练一个预测器，将查询嵌入映射到这些标签蒸馏得到的逐维重要性分数。推断阶段，预测器基于查询嵌入选择一个查询感知的维度子集，仅用查询嵌入进行相似度计算（无伪相关反馈）。在多种密集检索器和基准上验证，比较对象包括全维基线、PRF掩码和监督适配器。

Result: 所提出的维度选择器在多组基线和基准上优于全维、PRF掩码以及监督适配器基线，提升了检索效果的一致性与鲁棒性。

Conclusion: 通过将每维度的重要性直接从查询嵌入预测并选择查询感知的维度，能在不引入伪标签的情况下提升密集检索性能，且在不同检索器与基准上具有良好泛化。

Abstract: Dense retrieval represents queries and docu-002 ments as high-dimensional embeddings, but003 these representations can be redundant at the004 query level: for a given information need, only005 a subset of dimensions is consistently help-006 ful for ranking. Prior work addresses this via007 pseudo-relevance feedback (PRF) based dimen-008 sion importance estimation, which can produce009 query-aware masks without labeled data but010 often relies on noisy pseudo signals and heuris-011 tic test-time procedures. In contrast, super-012 vised adapter methods leverage relevance labels013 to improve embedding quality, yet they learn014 global transformations shared across queries015 and do not explicitly model query-aware di-016 mension importance. We propose a Query-017 Aware Adaptive Dimension Selection frame-018 work that learns to predict per-dimension im-019 portance directly from query embedding. We020 first construct oracle dimension importance dis-021 tributions over embedding dimensions using022 supervised relevance labels, and then train a023 predictor to map a query embedding to these024 label-distilled importance scores. At inference,025 the predictor selects a query-aware subset of026 dimensions for similarity computation based027 solely on the query embedding, without pseudo-028 relevance feedback. Experiments across multi-029 ple dense retrievers and benchmarks show that030 our learned dimension selector improves re-031 trieval effectiveness over the full-dimensional032 baseline as well as PRF-based masking and033 supervised adapter baselines.

</details>


### [68] [SCASRec: A Self-Correcting and Auto-Stopping Model for Generative Route List Recommendation](https://arxiv.org/abs/2602.03324)
*Chao Chen,Longfei Xu,Daohan Su,Tengfei Liu,Hanyu Guo,Yihai Duan,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: SCASRec 是一种统一的端到端路由推荐框架，将排序与去冗合并为一个生成过程，使用步进校正奖励和可学习的终止标记实现自适应停止，能在离线和在线评测中达到SOTA，并已在真实导航应用中落地。


<details>
  <summary>Details</summary>
Motivation: 现有多阶段管线在离线训练目标与在线指标间存在错配，冗余删除规则僵化且难以适应用户意图的高方差，且阶段之间严格分离导致全局最优难以实现。需要一个端到端的统一框架来同时优化排序与去冗并提升协同效应。

Method: 提出 SCASRec 这一自我纠错与自适应结束的生成式框架，将排序与冗余消除整合为单一端到端过程，引入步进校正奖励（SCR）以针对困难样本进行列表级 Refinement，并设计可学习的 End-of-Recommendation（EOR）令牌在无进一步改进时自适应停止生成。

Result: 在两组大规模开放数据集的路由推荐任务上，SCASRec 在离线和在线评测中均达到SOTA，并已在真实导航应用中落地部署，验证了方案的有效性。

Conclusion: SCASRec 成功解决了多阶段管线的目标错配与协同不足问题，通过端到端的生成式框架、对困难样本的聚焦校正以及自适应停止机制，提升了全局最优表现并具备良好的实际落地性。

Abstract: Route recommendation systems commonly adopt a multi-stage pipeline involving fine-ranking and re-ranking to produce high-quality ordered recommendations. However, this paradigm faces three critical limitations. First, there is a misalignment between offline training objectives and online metrics. Offline gains do not necessarily translate to online improvements. Actual performance must be validated through A/B testing, which may potentially compromise the user experience. Second, redundancy elimination relies on rigid, handcrafted rules that lack adaptability to the high variance in user intent and the unstructured complexity of real-world scenarios. Third, the strict separation between fine-ranking and re-ranking stages leads to sub-optimal performance. Since each module is optimized in isolation, the fine-ranking stage remains oblivious to the list-level objectives (e.g., diversity) targeted by the re-ranker, thereby preventing the system from achieving a jointly optimized global optimum. To overcome these intertwined challenges, we propose \textbf{SCASRec} (\textbf{S}elf-\textbf{C}orrecting and \textbf{A}uto-\textbf{S}topping \textbf{Rec}ommendation), a unified generative framework that integrates ranking and redundancy elimination into a single end-to-end process. SCASRec introduces a stepwise corrective reward (SCR) to guide list-wise refinement by focusing on hard samples, and employs a learnable End-of-Recommendation (EOR) token to terminate generation adaptively when no further improvement is expected. Experiments on two large-scale, open-sourced route recommendation datasets demonstrate that SCASRec establishes an SOTA in offline and online settings. SCASRec has been fully deployed in a real-world navigation app, demonstrating its effectiveness.

</details>


### [69] [Beyond Exposure: Optimizing Ranking Fairness with Non-linear Time-Income Functions](https://arxiv.org/abs/2602.03345)
*Xuancheng Li,Tao Yang,Yujia Zhou,Qingyao Ai,Yiqun Liu*

Main category: cs.IR

TL;DR: 定义并测量收入公平性，提出基于边际收入增益的动态收益导数感知排序公平性（DIDRF）算法，在离线和在线设置中对比基于暴露的公平性方法，结果表明在多样时间-收入函数下，DIDRF优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 暴露公平性仅基于位置暴露，忽略收入受时间等上下文因素影响的情况。需要一个能衡量并优化提供者收益的公平性的新框架，且与排序有效性兼容。

Method: 给出收入公平性的形式化定义并开发相应的度量指标；提出DIDRF算法，基于当前时间步的边际收入增益，通过泰勒展开的梯度同时优化有效性和收入公平性。

Result: 在离线和在线实验中，DIDRF在多种时间-收入函数下始终优于最先进方法，且现有基于暴露的公平性的排序算法无法优化收入公平性。

Conclusion: 收入公平性可以通过面向边际增益的动态梯度优化来实现，所提框架在不同时间-收入函数下具有鲁棒性和优越性。

Abstract: Ranking is central to information distribution in web search and recommendation. Nowadays, in ranking optimization, the fairness to item providers is viewed as a crucial factor alongside ranking relevance for users. There are currently numerous concepts of fairness and one widely recognized fairness concept is Exposure Fairness. However, it relies primarily on exposure determined solely by position, overlooking other factors that significantly influence income, such as time. To address this limitation, we propose to study ranking fairness when the provider utility is influenced by other contextual factors and is neither equal to nor proportional to item exposure. We give a formal definition of Income Fairness and develop a corresponding measurement metric. Simulated experiments show that existing-exposure-fairness-based ranking algorithms fail to optimize the proposed income fairness. Therefore, we propose the Dynamic-Income-Derivative-aware Ranking Fairness algorithm, which, based on the marginal income gain at the present timestep, uses Taylor-expansion-based gradients to simultaneously optimize effectiveness and income fairness. In both offline and online settings with diverse time-income functions, DIDRF consistently outperforms state-of-the-art methods.

</details>


### [70] [AesRec: A Dataset for Aesthetics-Aligned Clothing Outfit Recommendation](https://arxiv.org/abs/2602.03416)
*Wenxin Ye,Lin Li,Ming Li,Yang Shen,Kanghong Wang,Jimmy Xiangji Huang*

Main category: cs.IR

TL;DR: 提出 AesRec 基准数据集，系统化量化服装美学指标，并将美学信息融入服装推荐以提供美学导向。


<details>
  <summary>Details</summary>
Motivation: 弥合现有服装推荐在美学表征方面的空白，缺乏显式、可比的美学指标；基于专业审美标准构建可量化的多维美学评估，以支撑美学对齐的推荐系统。

Method: 在项级定义六个指标（轮廓 silhouette、色度 chromaticity、材质感 materiality、工艺 craftsmanship、可穿戴性 wearability、项级印象 item-level impression）；在套装级保留前五维，同时新增风格协同、视觉和谐、套装级印象等指标；利用视觉-语言模型进行大规模美学评分，并对人机一致性进行严格验证；构建并评估 AesRec 数据集及相关推荐模型。

Result: 实验表明将量化美学信息嵌入推荐模型可在满足个性化需求的同时提供美学导向；数据与评估方法具有可靠性，提升推荐结果的美学一致性与用户体验。

Conclusion: 提出 AesRec 基准，支持美学对齐的服装推荐研究，基于专业标准的多维美学评估具备良好扩展性，推动多模态美学评估与应用。

Abstract: Clothing recommendation extends beyond merely generating personalized outfits; it serves as a crucial medium for aesthetic guidance. However, existing methods predominantly rely on user-item-outfit interaction behaviors while overlooking explicit representations of clothing aesthetics. To bridge this gap, we present the AesRec benchmark dataset featuring systematic quantitative aesthetic annotations, thereby enabling the development of aesthetics-aligned recommendation systems. Grounded in professional apparel quality standards and fashion aesthetic principles, we define a multidimensional set of indicators. At the item level, six dimensions are independently assessed: silhouette, chromaticity, materiality, craftsmanship, wearability, and item-level impression. Transitioning to the outfit level, the evaluation retains the first five core attributes while introducing stylistic synergy, visual harmony, and outfit-level impression as distinct metrics to capture the collective aesthetic impact. Given the increasing human-like proficiency of Vision-Language Models in multimodal understanding and interaction, we leverage them for large-scale aesthetic scoring. We conduct rigorous human-machine consistency validation on a fashion dataset, confirming the reliability of the generated ratings. Experimental results based on AesRec further demonstrate that integrating quantified aesthetic information into clothing recommendation models can provide aesthetic guidance for users while fulfilling their personalized requirements.

</details>


### [71] [RankSteer: Activation Steering for Pointwise LLM Ranking](https://arxiv.org/abs/2602.03422)
*Yumeng Wang,Catherine Chen,Suzan Verberne*

Main category: cs.IR

TL;DR: RankSteer 为零-shot 点对点 LLM 排序提出后处理的激活 steer 框架，通过在推理时对隐藏状态的决策、证据、角色三个方向进行投影干预，在不修改模型权重的前提下提升排序性能，并给出内部几何解释。


<details>
  <summary>Details</summary>
Motivation: LLMs 在提示设计上对零-shot排序高度敏感，且角色相关信号似乎位于与查询-文档表征分离的激活通道中，存在通过激活自举而非繁琐提示工程来控制排名的机会。

Method: RankSteer 通过投影干预在推理阶段对三条可分离且可控的方向进行联合控制：决策方向将隐藏状态映射到相关性分数；证据方向捕捉未被决策头直接利用的相关性信号；角色方向调节模型行为但不传递相关性信息。无需修改模型权重或进行跨文档比较。

Result: 在 TREC DL 20 和多个 BEIR 基准上，RankSteer 在仅需少量锚查询的情况下持续提升排序质量，表明点对点 LLM 排序中仍潜藏大量未被充分利用的排序能力。几何分析显示通过干预稳定排序几何并降低分散性，从内部揭示相关性判断的编码与校准机制。

Conclusion: 对模型内部表征的几何结构提供新的理解：激活层干预可以有效校准排序，同时呈现出对相关性判断的稳健表征，扩展了零-shot LLM ranking 的工具箱。

Abstract: Large language models (LLMs) have recently shown strong performance as zero-shot rankers, yet their effectiveness is highly sensitive to prompt formulation, particularly role-play instructions. Prior analyses suggest that role-related signals are encoded along activation channels that are largely separate from query-document representations, raising the possibility of steering ranking behavior directly at the activation level rather than through brittle prompt engineering. In this work, we propose RankSteer, a post-hoc activation steering framework for zero-shot pointwise LLM ranking. We characterize ranking behavior through three disentangled and steerable directions in representation space: a \textbf{decision direction} that maps hidden states to relevance scores, an \textbf{evidence direction} that captures relevance signals not directly exploited by the decision head, and a \textbf{role direction} that modulates model behavior without injecting relevance information. Using projection-based interventions at inference time, RankSteer jointly controls these directions to calibrate ranking behavior without modifying model weights or introducing explicit cross-document comparisons. Experiments on TREC DL 20 and multiple BEIR benchmarks show that RankSteer consistently improves ranking quality using only a small number of anchor queries, demonstrating that substantial ranking capacity remains under-utilized in pointwise LLM rankers. We further provide a geometric analysis revealing that steering improves ranking by stabilizing ranking geometry and reducing dispersion, offering new insight into how LLMs internally represent and calibrate relevance judgments.

</details>


### [72] [Tutorial on Reasoning for IR & IR for Reasoning](https://arxiv.org/abs/2602.03640)
*Mohanna Hoveyda,Panagiotis Efstratiadis,Arjen de Vries,Maarten de Rijke*

Main category: cs.IR

TL;DR: 提出一个统一的推理框架，用以将信息检索中的推理问题（逻辑约束、分步推理、证据综合）与跨领域的方法对应起来，提供分析维度、框架映射与实践指南。


<details>
  <summary>Details</summary>
Motivation: 现实中的信息检索不仅需相关度，还需可验证的推理、约束执行和多步推理；现有推理研究跨越推理时机、LLM后训练、神经符号、贝叶斯/概率、几何表示、能量模型等领域，彼此间缺乏统一的语言与对接点。

Method: 提出在信息检索情境中的“推理”工作定义， derive 出统一分析框架，并按核心组成部分的轴线对现有方法进行映射；系统梳理推理相关方法（推理时策略、LLM后训练、神经符号、贝叶斯/概率、几何表示、能量模型等），给出综合综述、权衡与互补性分析，以及检索过程在更广推理体系中的作用。

Result: 给出概念框架与实用指导，帮助构建具备推理能力的 IR 系统；将现有方法映射到定义的轴上，揭示权衡与互补性；展示跨学科进展对 IR 的潜力与检索在广义推理体系中的核心角色。

Conclusion: IR 将从更广的推理研究中获益，也能为其发展贡献应用场景与需求；该教程提供统一框架与实践路径，推动 IR 在推理研究中的地位，并促进跨学科协同。

Abstract: Information retrieval has long focused on ranking documents by semantic relatedness. Yet many real-world information needs demand more: enforcement of logical constraints, multi-step inference, and synthesis of multiple pieces of evidence. Addressing these requirements is, at its core, a problem of reasoning. Across AI communities, researchers are developing diverse solutions for the problem of reasoning, from inference-time strategies and post-training of LLMs, to neuro-symbolic systems, Bayesian and probabilistic frameworks, geometric representations, and energy-based models. These efforts target the same problem: to move beyond pattern-matching systems toward structured, verifiable inference. However, they remain scattered across disciplines, making it difficult for IR researchers to identify the most relevant ideas and opportunities. To help navigate the fragmented landscape of research in reasoning, this tutorial first articulates a working definition of reasoning within the context of information retrieval and derives from it a unified analytical framework. The framework maps existing approaches along axes that reflect the core components of the definition. By providing a comprehensive overview of recent approaches and mapping current methods onto the defined axes, we expose their trade-offs and complementarities, highlight where IR can benefit from cross-disciplinary advances, and illustrate how retrieval process itself can play a central role in broader reasoning systems. The tutorial will equip participants with both a conceptual framework and practical guidance for enhancing reasoning-capable IR systems, while situating IR as a domain that both benefits and contributes to the broader development of reasoning methodologies.

</details>


### [73] [Bringing Reasoning to Generative Recommendation Through the Lens of Cascaded Ranking](https://arxiv.org/abs/2602.03692)
*Xinyu Lin,Pengyuan Liu,Wenjie Wang,Yicheng Hu,Chen Xu,Fuli Feng,Qifan Wang,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 提出一个针对生成式推荐系统的偏见放大问题的解决方案 CARE，通过逐步融入历史信息与并行推理来分层(debias)生成过程，提高准确性、多样性与效率。


<details>
  <summary>Details</summary>
Motivation: GR在端到端生成中易发生偏见放大： token级偏见随生成推进而扩大，导致多样性下降和用户体验下降。原因包括对历史信息的同质化依赖与固定的计算预算限制。需要引入异质信息并在每步生成时分配更多计算资源。

Method: 提出 CARE：1) 逐步历史编码（progressive history encoding），在生成过程中逐步引入更细粒度的历史信息；2) 查询锚定推理（query-anchored reasoning），在并行推理步骤中对历史信息进行更深层理解；将 CARE 实例化到三种 GR 骨干模型，基于四个数据集进行评估。

Result: 在推荐准确率、多样性、效率和可扩展性等方面优于基线，体现了偏置放大的抑制与性能改善；公开代码和数据集。

Conclusion: CARE 提供一种简单但有效的分层推理框架，用于对生成式推荐系统中的偏置放大进行解耦与缓解，并具备良好的扩展性。

Abstract: Generative Recommendation (GR) has become a promising end-to-end approach with high FLOPS utilization for resource-efficient recommendation. Despite the effectiveness, we show that current GR models suffer from a critical \textbf{bias amplification} issue, where token-level bias escalates as token generation progresses, ultimately limiting the recommendation diversity and hurting the user experience. By comparing against the key factor behind the success of traditional multi-stage pipelines, we reveal two limitations in GR that can amplify the bias: homogeneous reliance on the encoded history, and fixed computational budgets that prevent deeper user preference understanding.
  To combat the bias amplification issue, it is crucial for GR to 1) incorporate more heterogeneous information, and 2) allocate greater computational resources at each token generation step. To this end, we propose CARE, a simple yet effective cascaded reasoning framework for debiased GR. To incorporate heterogeneous information, we introduce a progressive history encoding mechanism, which progressively incorporates increasingly fine-grained history information as the generation process advances. To allocate more computations, we propose a query-anchored reasoning mechanism, which seeks to perform a deeper understanding of historical information through parallel reasoning steps. We instantiate CARE on three GR backbones. Empirical results on four datasets show the superiority of CARE in recommendation accuracy, diversity, efficiency, and promising scalability. The codes and datasets are available at https://github.com/Linxyhaha/CARE.

</details>


### [74] [Multimodal Generative Recommendation for Fusing Semantic and Collaborative Signals](https://arxiv.org/abs/2602.03713)
*Moritz Vandenhirtz,Kaveh Hassani,Shervin Ghasemlou,Shuai Shao,Hamid Eghbalzadeh,Fuchun Peng,Jun Liu,Michael Louis Iuzzolino*

Main category: cs.IR

TL;DR: MSCGRec 提出一个多模态语义与协同生成推荐器，使用离散语义码对物品建模，通过自回归生成码序列来预测下一个物品；引入基于 DINO 的图像自监督量化、将协同信号作为独立模态、以及受限序列学习以缩小输出空间，从而在大型物品集上实现对比优越的推荐效果。


<details>
  <summary>Details</summary>
Motivation: 当前生成式推荐在小数据集上有潜在优势，但在大规模物品集合上往往不如传统的序列推荐，原因包括巨大的输出空间和存储开销；需要在保留生成式建模优势的同时，充分利用多模态信息与协同信号以提升可扩展性与性能。

Method: 提出 MSCGRec：1) 将物品表示为离散语义码序列，采用自回归模型预测码序列；2) 融合多模态语义信号（包括视觉等）并用自监督量化得到图像模态的语义码（基于 DINO 框架）；3) 将来自基于序列的协同信号提取为独立模态，与语义模态共同输入模型；4) 训练阶段实现受限序列学习，将输出空间限定在允许的令牌集合内以降低计算与存储开销；5) 在三个大型真实数据集上进行评估并与基线比较。

Result: 在三个大型真实数据集上，MSCGRec 及其变体超越了传统的顺序推荐和生成式基线；消融研究验证了多模态语义、协同模态、以及受限序列学习等组件的有效性。

Conclusion: 通过将多模态语义与协同信号融入生成式推荐框架，并在训练中对输出空间进行约束，MSCGRec 有效扩展了生成式推荐在大规模物品集上的可行性与性能，且各组件对提升性能具有实证贡献。

Abstract: Sequential recommender systems rank relevant items by modeling a user's interaction history and computing the inner product between the resulting user representation and stored item embeddings. To avoid the significant memory overhead of storing large item sets, the generative recommendation paradigm instead models each item as a series of discrete semantic codes. Here, the next item is predicted by an autoregressive model that generates the code sequence corresponding to the predicted item. However, despite promising ranking capabilities on small datasets, these methods have yet to surpass traditional sequential recommenders on large item sets, limiting their adoption in the very scenarios they were designed to address. To resolve this, we propose MSCGRec, a Multimodal Semantic and Collaborative Generative Recommender. MSCGRec incorporates multiple semantic modalities and introduces a novel self-supervised quantization learning approach for images based on the DINO framework. Additionally, MSCGRec fuses collaborative and semantic signals by extracting collaborative features from sequential recommenders and treating them as a separate modality. Finally, we propose constrained sequence learning that restricts the large output space during training to the set of permissible tokens. We empirically demonstrate on three large real-world datasets that MSCGRec outperforms both sequential and generative recommendation baselines and provide an extensive ablation study to validate the impact of each component.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [75] [Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers](https://arxiv.org/abs/2602.02559)
*Pengyu Dai,Weihao Xuan,Junjue Wang,Hongruixuan Chen,Jian Song,Yafei Ou,Naoto Yokoya*

Main category: cs.AI

TL;DR: GeoEvolver is a self-evolving multi-agent system that enables LLM agents to acquire EO-domain tool-parameter expertise through structured, interaction-driven learning without parameter updates, improving end-to-end EO task success by about 12% across backbones on three benchmarks.


<details>
  <summary>Details</summary>
Motivation: EO tasks involve long-horizon planning, multi-modal and multi-temporal data, and geo-knowledge constraints. Existing agents lack fine-grained tool-level learning and robust recovery from mid-execution errors, limiting performance in EO workflows.

Method: Decompose queries into independent sub-goals using a retrieval-augmented multi-agent orchestrator. The system explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root causes from failures are distilled into an evolving memory bank that provides in-context demonstrations for future queries.

Result: On three tool-integrated EO benchmarks, GeoEvolver yields an average end-to-end task success improvement of 12% across multiple LLM backbones.

Conclusion: EO expertise can emerge progressively from structured, fine-grained interactions with the environment without parameter updates, aided by an evolving memory bank that encodes successful patterns and failure root causes.

Abstract: Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.

</details>


### [76] [Uncertainty and Fairness Awareness in LLM-Based Recommendation Systems](https://arxiv.org/abs/2602.02582)
*Chandan Kumar Sah,Xiaoli Lian,Li Zhang,Tony Xu,Syed Shazaib Shah*

Main category: cs.AI

TL;DR: 提出不确定性-公平性感知的 RecLLM 评估框架与基准数据集，覆盖电影与音乐领域，并在 Gemini 1.5 Flash 上揭示系统性偏见；引入人格化公平与不确定性评估的结合，以及基于人格画像的公平基准，推动可解释性与公平性研究。


<details>
  <summary>Details</summary>
Motivation: 解决零-shot 推荐中来自预测不确定性与嵌入偏见对可靠性与公平性的威胁；需要统一的评估基准、可解释性以及对个性化与群体公平的权衡。

Method: 构建一个带有31个分类值、覆盖八个人口属性的标注数据集，覆盖电影与音乐两个领域；设计并使用一套精选指标基准；进行深入案例研究，测量熵等不确定性；分析并量化对提示扰动的鲁棒性；将人格化公平纳入 RecLLM 评估流水线；提出不确定性感知评估方法与人格画像公平基准。

Result: 在 Gemini 1.5 Flash 上观察到对某些敏感属性的系统性不公平，利用相似性差距的度量 SNSR=0.1363、SNSV=0.0507；这些差距在打字错误与多语言输入下仍然存在；提出并展示不确定性与个性化公平的权衡，以及深度不确定性案例研究的洞见。

Conclusion: 为更安全、可解释的 RecLLMs 打下基础，提出新颖的评估方法与人格特征驱动的公平基准，推动多模型基准与自适应校准的未来研究。

Abstract: Large language models (LLMs) enable powerful zero-shot recommendations by leveraging broad contextual knowledge, yet predictive uncertainty and embedded biases threaten reliability and fairness. This paper studies how uncertainty and fairness evaluations affect the accuracy, consistency, and trustworthiness of LLM-generated recommendations. We introduce a benchmark of curated metrics and a dataset annotated for eight demographic attributes (31 categorical values) across two domains: movies and music. Through in-depth case studies, we quantify predictive uncertainty (via entropy) and demonstrate that Google DeepMind's Gemini 1.5 Flash exhibits systematic unfairness for certain sensitive attributes; measured similarity-based gaps are SNSR at 0.1363 and SNSV at 0.0507. These disparities persist under prompt perturbations such as typographical errors and multilingual inputs. We further integrate personality-aware fairness into the RecLLM evaluation pipeline to reveal personality-linked bias patterns and expose trade-offs between personalization and group fairness. We propose a novel uncertainty-aware evaluation methodology for RecLLMs, present empirical insights from deep uncertainty case studies, and introduce a personality profile-informed fairness benchmark that advances explainability and equity in LLM recommendations. Together, these contributions establish a foundation for safer, more interpretable RecLLMs and motivate future work on multi-model benchmarks and adaptive calibration for trustworthy deployment.

</details>


### [77] [A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior](https://arxiv.org/abs/2602.02639)
*Harry Mayne,Justin Singh Kang,Dewi Gould,Kannan Ramchandran,Adam Mahdi,Noah Y. Siegel*

Main category: cs.AI

TL;DR: 提出 Normalized Simulatability Gain (NSG) 作为自解释的可预测性增益指标，用以衡量解释是否真能让观察者学习模型的决策标准。实证在18个模型、7,000个反事实样本上，自解释显著提升对模型行为的预测性（11-37% NSG），且常于外部模型产生的解释之上；但存在约5-15%的自解释显著具有误导性。整体而言，自解释具有正向信息载荷，能够帮助预测模型行为，尽管并非完美。


<details>
  <summary>Details</summary>
Motivation: 当前的自解释信度判定方法（如对抗性提示、推理错误检测）往往忽略解释对预测性的价值，因此需要一个更直接的、可扩展的量化指标来评估自解释的“可模拟性”对模型行为的预测能力。实现对大量模型与数据的比较，以评估自解释的普遍性和局限性。

Method: 提出 Normalized Simulatability Gain (NSG) 指标，基于“一个可信的解释应使观察者能够学习模型的决策标准，并据此更好地预测相关输入上的行为”。在7,000个来自健康、商业、伦理数据集的反事实样本上，对18个前沿模型（含 Gemini 3、GPT-5.2、Claude 4.5 等）进行评估，比较自解释与外部模型解释的预测信息量，评估自解释的误导性比例。

Result: 自解释显著提升对模型行为的预测性，NSG 范围为 11%-37%；自解释比外部模型生成的解释提供更强的预测信息，即使对方模型更强；5%-15% 的自解释存在显著误导风险。

Conclusion: 自解释具有正向信息载荷，能够帮助预测模型行为，支持其在AI监督中的应用潜力；但并非全然可靠，需关注其误导性比例并探索改进思路，外部解释难以复制自解释所包含的自知识。

Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.

</details>


### [78] [Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction](https://arxiv.org/abs/2602.02711)
*Yuanzhe Li,Jianing Deng,Jingtong Hu,Tianlong Chen,Song Wang,Huanrui Yang*

Main category: cs.AI

TL;DR: 提出一种动态混合精度路由框架，在多步推理中对不同步骤采用高/低精度LLM，通过KL分布对齐的监督阶段识别精度敏感步，再用GRPO提升任务成功率，在ALFWorld上显著提升准确性与成本比。


<details>
  <summary>Details</summary>
Motivation: LLM在长时序决策中成本高、推理开销大，需要在保持性能的同时降低成本；不同步骤对精度的敏感性不同，需动态分配精度。

Method: 两阶段训练：第一阶段基于KL散度进行监督学习以识别精度敏感步，第二阶段采用Group-Relative Policy Optimization (GRPO) 进一步提高任务成功率；在推理阶段通过动态路由在高/低精度LLM之间切换。

Result: 在ALFWorld数据集上，与单精度基线和启发式路由相比，显著提升准确率-成本折衷，降低推理成本同时保持或提升任务成功率。

Conclusion: 动态混合精度路由能有效减少推理成本同时维持高性能，是节省资源的可行方向。

Abstract: Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.

</details>


### [79] [Scaling-Aware Adapter for Structure-Grounded LLM Reasoning](https://arxiv.org/abs/2602.02780)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Yi Li,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.AI

TL;DR: 提出 Cuttlefish，一种统一的全原子LLM，通过可伸缩的结构补丁和几何对齐来实现语言推理与结构的直接耦合，显著提升全原子结构推理的准确性并减少结构幻觉。


<details>
  <summary>Details</summary>
Motivation: 当前将结构信息输入转化为序列化标记或固定连接器的方法，在模态融合上存在瓶颈，容易丢失几何约束，导致结构幻觉并限制全原子推理的泛化能力。需一个能随结构复杂度动态调配token预算且能显式地几何对齐的统一框架。

Method: 提出 Scaling-Aware Patching：基于指令条件门控生成可变大小的结构图补丁，随结构复杂度自适应分配查询token； Geometry Grounding Adapter：通过与模态嵌入的跨注意力 refinement，注入得到的模态token到LLM中，显式嵌入几何信息以降低幻觉。

Result: 在多样化的全原子基准上，Cuttlefish在异质结构-语言推理任务中实现更优的性能，且给出代码仓库。

Conclusion: 证明了对结构复杂度自适应的补丁与几何对齐机制对全原子推理的有效性，提供了一种统一的结构-语言融合框架以扩展LLM在生物大分子领域的全局推理能力。

Abstract: Large language models (LLMs) are enabling reasoning over biomolecular structures, yet existing methods remain modality-specific and typically compress structural inputs through sequence-based tokenization or fixed-length query connectors. Such architectures either omit the geometric groundings requisite for mitigating structural hallucinations or impose inflexible modality fusion bottlenecks that concurrently over-compress and suboptimally allocate structural tokens, thereby impeding the realization of generalized all-atom reasoning. We introduce Cuttlefish, a unified all-atom LLM that grounds language reasoning in geometric cues while scaling modality tokens with structural complexity. First, Scaling-Aware Patching leverages an instruction-conditioned gating mechanism to generate variable-size patches over structural graphs, adaptively scaling the query token budget with structural complexity to mitigate fixed-length connector bottlenecks. Second, Geometry Grounding Adapter refines these adaptive tokens via cross-attention to modality embeddings and injects the resulting modality tokens into the LLM, exposing explicit geometric cues to reduce structural hallucination. Experiments across diverse all-atom benchmarks demonstrate that Cuttlefish achieves superior performance in heterogeneous structure-grounded reasoning. Code is available at the project repository.

</details>


### [80] [STEER: Inference-Time Risk Control via Constrained Quality-Diversity Search](https://arxiv.org/abs/2602.02862)
*Eric Yang,Jong Ha Lee,Jonathan Amar,Elissa Ye,Yugang Jia*

Main category: cs.AI

TL;DR: 提出 STEER，一种训练无关的“可控偏好”框架，通过离线质量-多样性搜索生成一组 personas，在推理阶段通过单一风险分位点控制映射到特定 persona，从而实现决策保守度的单调调整，提升临床分诊任务中的行为覆盖率且保持领域能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在需要多解容忍的序数决策场景（如临床分诊）中容易陷入模式塌陷，且传统对齐机制去除了在特定约束下调整灵敏度与特异度之间的权衡的能力，提供一种训练无关、可控的风险/保守性调节方法。

Method: 离线进行受限的质量-多样性搜索以构造一组自然语言 personas，设定最小安全、推理与稳定性阈值；推理时暴露一个映射用户指定风险百分位到相应 persona 的单一控制参数，从而实现决策保守性的单调调整。

Result: 在两个临床分诊基准上，STEER 的行为覆盖性优于基于温度采样的策略与静态 persona 集合；相较于一种代表性的训练后方法，STEER 在不明确紧急情况的准确性上保留显著优势，且对模糊决策的控制达到可比水平。

Conclusion: 作为一种安全性保护范式，STEER 能在不损害领域能力的前提下实现对模型行为的可控引导和风险管理。

Abstract: Large Language Models (LLMs) trained for average correctness often exhibit mode collapse, producing narrow decision behaviors on tasks where multiple responses may be reasonable. This limitation is particularly problematic in ordinal decision settings such as clinical triage, where standard alignment removes the ability to trade off specificity and sensitivity (the ROC operating point) based on contextual constraints. We propose STEER (Steerable Tuning via Evolutionary Ensemble Refinement), a training-free framework that reintroduces this tunable control. STEER constructs a population of natural-language personas through an offline, constrained quality-diversity search that promotes behavioral coverage while enforcing minimum safety, reasoning, and stability thresholds. At inference time, STEER exposes a single, interpretable control parameter that maps a user-specified risk percentile to a selected persona, yielding a monotonic adjustment of decision conservativeness. On two clinical triage benchmarks, STEER achieves broader behavioral coverage compared to temperature-based sampling and static persona ensembles. Compared to a representative post-training method, STEER maintains substantially higher accuracy on unambiguous urgent cases while providing comparable control over ambiguous decisions. These results demonstrate STEER as a safety-preserving paradigm for risk control, capable of steering behavior without compromising domain competence.

</details>


### [81] [Aligning Language Model Benchmarks with Pairwise Preferences](https://arxiv.org/abs/2602.02898)
*Marco Gutierrez,Xinyi Leng,Hannah Cyberey,Jonathan Richard Schwarz,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.AI

TL;DR: 提出基准对齐（benchmark alignment）与 BenchAlign，利用有限的模型性能信息通过学习对比偏好，自动更新离线基准以预测模型对比偏好。BenchAlign 学会为基准问题分配偏好对齐的权重，结合模型逐项表现和部署中收集的模型对比排名，生成能够对未见模型按人类偏好排序的新基准。实验表明对齐基准能够在不同规模模型间准确排序未见模型，且具可解释性，同时揭示将基准与实际人类偏好对齐的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准在预测实际实用性方面常常失效，理论与实际应用之间存在差距。如何利用有限信息和部署中收集的对比数据来更新离线基准，使其更好地反映人类偏好和真实效用，是一个重要且具挑战性的问题。

Method: 提出基准对齐的概念并实现首个解决方案 BenchAlign。通过对基准问题赋予可学习的权重，使基准在给定测试设置下对模型对的偏好排序具有一致性。该权重通过模型逐项表现数据以及在部署阶段可获得的模型对比排名对进行学习，最终生成新的基准，使得未见模型按这些偏好排序。

Result: 对齐后的基准能够在未见模型上按人类偏好排序，且对不同规模的模型具备较好的一致性与可解释性；实验显示其对未见模型的排序与人类偏好高度对齐，具跨模型规模的泛化性。

Conclusion: 该工作首次给出系统性的基准对齐解决方案 BenchAlign，并揭示对齐基准以匹配实际人类偏好的潜力与局限性，预计有助于加速模型向实际实用性方向发展。

Abstract: Language model benchmarks are pervasive and computationally-efficient proxies for real-world performance. However, many recent works find that benchmarks often fail to predict real utility. Towards bridging this gap, we introduce benchmark alignment, where we use limited amounts of information about model performance to automatically update offline benchmarks, aiming to produce new static benchmarks that predict model pairwise preferences in given test settings. We then propose BenchAlign, the first solution to this problem, which learns preference-aligned weight- ings for benchmark questions using the question-level performance of language models alongside ranked pairs of models that could be collected during deployment, producing new benchmarks that rank previously unseen models according to these preferences. Our experiments show that our aligned benchmarks can accurately rank unseen models according to models of human preferences, even across different sizes, while remaining interpretable. Overall, our work provides insights into the limits of aligning benchmarks with practical human preferences, which stands to accelerate model development towards real utility.

</details>


### [82] [Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs](https://arxiv.org/abs/2602.02909)
*Kiran Tomlinson,Tobias Schnabel,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 对三个BAPO难题的推理令牌需求与输入规模成线性关系的理论与实验分析：在输入规模n时，最少需要Ω(n)个推理令牌，且存在接近匹配的上界；前沿推理模型的实证结果与理论吻合，指出CoT推理在推理时间成本上的基本瓶颈。


<details>
  <summary>Details</summary>
Motivation: 明确量化随着输入规模增长所需的推理令牌数量，从理论模型（BAPO）出发，揭示Chain-of-Thought在推理过程中的信息流上限与成本。

Method: 扩展有界注意前缀oracle（BAPO）模型，作为对LLMs信息流的抽象；对三类BAPO难题（二元多数、三元匹配、图遍历）给出Ω(n)下界，并给出与之匹配的上界的显式构造；结合前沿推理模型进行实验，观察推理令牌的线性增长趋势及预算受限时的失败情况。

Result: 理论上对三类任务给出Ω(n)的推理令牌下界，并给出接近匹配的上界；实验表明，前沿模型在这些任务上呈现近似线性地推理令牌规模随输入n增长，若推理预算受限则容易失败，验证了理论下界。

Conclusion: 揭示了CoT推理在推断时间成本上的根本瓶颈，提供一个系统化的分析工具来评估最优推理长度，并强调在大输入规模下线性增长的推理令牌需求。

Abstract: Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $Ω(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.

</details>


### [83] [Structuring Value Representations via Geometric Coherence in Markov Decision Processes](https://arxiv.org/abs/2602.02978)
*Zuyuan Zhang,Zeyu Fang,Tian Lan*

Main category: cs.AI

TL;DR: A poset-based reinforcement learning framework (GCR-RL) that imposes geometric coherence via super-poset refinements and temporal-difference signals, with Q-learning and actor-critic variants, achieving better sample efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: Leverage order theory and partially ordered sets to regularize and stabilize value function estimation in RL by enforcing geometric coherence across a sequence of refined posets.

Method: Introduce a sequence of super-poset refinements that progressively refine previous posets and learn additional order relations from TD signals; develop two algorithms (one based on Q-learning, one on actor–critic) to realize these refinements; analyze theoretical properties and convergence rates.

Result: Empirical evaluation across tasks showing significant improvements in sample efficiency and stable performance compared to strong baselines.

Conclusion: Poset-based geometric coherence regularization (GCR-RL) is effective for stabilizing and speeding RL, with provable convergence properties for the proposed algorithms.

Abstract: Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.

</details>


### [84] [Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget](https://arxiv.org/abs/2602.02983)
*Hanna M. Dettki,Charley M. Wu,Bob Rehder*

Main category: cs.AI

TL;DR: LLMs在因果判断上呈现分化特征：小型可解释模型能压缩其判断；大多数LLMs趋向规则化推理且与人类在隐因考虑和特征解释方面不同；CoT提升鲁棒性，提示策略对鲁棒性有显著影响；需谨慎部署以避免在高不确定性场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在规范因果推理、类人直觉和模式匹配之间的差异，以及它们对经典 collider偏误的再现情况。

Method: 在11个基于 collider 结构C1→E←C2的因果判断任务上，对20+ LLMs与匹配的人类基线进行对比。引入一个小型可解释模型压缩LLMs的因果判断；通过语义抽象和提示超载测试评估鲁棒性；利用链式思考（CoT）来观察对鲁棒性的影响。

Result: 可解释模型较好地压缩了LLMs的因果判断；多数LLMs表现出更规则化的推理风格，而人类在概率判断中考虑未显著的潜在因素；大多数LLMs并未呈现人类在 collider 任务中的弱解释和马尔可夫性违规偏误；CoT提高了对语义抽象和提示超载情境下的鲁棒性。

Conclusion: LLMs的因果推理与人类存在显著差异，可能在某些场景辅助人类但在高不确定性条件下易受限；需要系统地刻画LLMs的推理策略以实现安全、有效部署。

Abstract: Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \!\rightarrow\! E\! \leftarrow \!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.

</details>


### [85] [Large Language Models Can Take False First Steps at Inference-time Planning](https://arxiv.org/abs/2602.02991)
*Haijiang Yan,Jian-Qiao Zhu,Adam Sanborn*

Main category: cs.AI

TL;DR: 提出一种基于贝叶斯的解释，认为推理时的规划受自身生成上下文演化驱动而产生偏差，并通过两项受控实验验证这一规划-上下文耦合的模型。


<details>
  <summary>Details</summary>
Motivation: 解释为什么在训练阶段具备序列级规划能力的LLM在推理阶段的规划行为往往短视且不一致；提供一个统一的理论框架以解释规划在推理中的演化过程。

Method: 建立一个贝叶斯框架，将规划视为对不断累积的自生解码上下文的依赖。通过两项受控实验验证：一项随机生成任务，观察在人类提示下规划受限且随自生上下文推进而增强；一项高斯采样任务，结合自生序列后表现出初始偏差的降低。

Result: 实验结果支持：自生成上下文的累积增强了推理前瞻性规划的强度，且条件化自生序列减少了初始偏差，从而为推理阶段的规划提供理论与经验支持。

Conclusion: 贝叶斯账户能够解释推理时的规划现象及其随自生成上下文演化的动态，弥合了训练时的规划能力与推理时的表现差异，并为LLM推理中的计划行为提供可验证的理论框架。

Abstract: Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference.

</details>


### [86] [Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents](https://arxiv.org/abs/2602.02995)
*Sizhe Tang,Rongqian Chen,Tian Lan*

Main category: cs.AI

TL;DR: Agent Alpha introduces step-level Monte Carlo Tree Search with alpha-UCT for GUI agents, enabling deliberate planning, prefix reuse, and suboptimal-branch pruning, achieving state-of-the-art ~77% success on OSWorld under equal compute.


<details>
  <summary>Details</summary>
Motivation: Trajectories with test-time sampling improve performance but lack regressive ability to reuse partial successes or recover from early mistakes. A step-level planning framework is needed to prune suboptimal branches, reuse prefixes, and exploit structure in the planning space.

Method: Propose Agent Alpha, a unified generation–exploration–evaluation framework that uses step-level MCTS with alpha-UCT guidance integrated into the interaction loop; includes comparison-driven evaluation to reduce scoring biases and diversity-constrained expansion to keep a compact, informative search; provides regret bound analysis for alpha-UCT.

Result: Empirical results on the OSWorld benchmark show a state-of-the-art success rate of about 77%, significantly surpassing trajectory-level baselines under equivalent compute.

Conclusion: Deliberate planning via alpha-UCT within an integrated MCTS framework enables effective prefix reuse and early pruning of suboptimal branches, supported by regret-bound analysis and strong empirical performance; demonstrates robustness of step-level planning for GUI agents.

Abstract: While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\sim 77\%$, significantly outperforming trajectory-level baselines under equivalent compute.

</details>


### [87] [Methods and Open Problems in Differentiable Social Choice: Learning Mechanisms, Decisions, and Alignment](https://arxiv.org/abs/2602.03003)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 差分可微社会选择的综述与未来方向：将投票、资源分配等机制建模为可学习的可微分模型，并在数据驱动下优化。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统越来越多地将社会选择机制嵌入其中，需对其规范性进行审视；可微分框架有助于将传统公理与实现约束融入学习目标。

Method: 对 auctions、投票、预算、流动民主、去中心化聚合、逆机制学习等领域的文献进行综述，揭示经典公理与不可能性定理如何以目标、约束、权衡的形式重新出现。

Result: 系统性梳理了差分社会选择的研究现状，建立跨领域的联系，并指出在36个开放问题上构建未来研究方向。

Conclusion: 提出在机器学习、经济学、民主理论交叉的新研究议程，强调将公理性、可解释性和伦理规范融入可微学习的社会选择框架。

Abstract: Social choice is no longer a peripheral concern of political theory or economics-it has become a foundational component of modern machine learning systems. From auctions and resource allocation to federated learning, participatory governance, and the alignment of large language models, machine learning pipelines increasingly aggregate heterogeneous preferences, incentives, and judgments into collective decisions. In effect, many contemporary machine learning systems already implement social choice mechanisms, often implicitly and without explicit normative scrutiny.
  This Review surveys differentiable social choice: an emerging paradigm that formulates voting rules, mechanisms, and aggregation procedures as learnable, differentiable models optimized from data. We synthesize work across auctions, voting, budgeting, liquid democracy, decentralized aggregation, and inverse mechanism learning, showing how classical axioms and impossibility results reappear as objectives, constraints, and optimization trade-offs. We conclude by identifying 36 open problems defining a new research agenda at the intersection of machine learning, economics, and democratic theory.

</details>


### [88] [Distilling LLM Reasoning into Graph of Concept Predictors](https://arxiv.org/abs/2602.03006)
*Ziyang Yu,Liang Zhao*

Main category: cs.AI

TL;DR: GCP 框架通过将教师的推理过程外部化为有向无环图（DAG），并在学生端对应为模块化的概念预测器，以提升样本利用率、训练稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的主动蒸馏在推理成本和延迟方面受限，且多仅蒸馏最终标签，缺乏中间推理信号和诊断能力。

Method: 将教师的决策过程外部化为有向无环图（GCP），在学生端用模块化概念预测器进行对应；通过图感知的采样策略在关键推理节点针对不确定性与分歧进行获取；进行目标子模块再训练，将下游损失归因于具体概念预测器，仅更新最有影响力的模块。

Result: 在八个 NLP 分类基准上实验，GCP 在有限标注预算下提升性能，并产生更可解释、可控的训练动态。

Conclusion: GCP 提供一个高效、可解释且可控的主动蒸馏框架，增强对推理过程的外部化与可控性。

Abstract: Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.

</details>


### [89] [RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents](https://arxiv.org/abs/2602.03025)
*Haitian Zhong,Jixiu Zhai,Lei Song,Jiang Bian,Qiang Liu,Tieniu Tan*

Main category: cs.AI

TL;DR: RC-GRPO introduces Reward-Conditioned Trajectory Policy and reward-token conditioned rollouts to address sparse rewards in multi-turn tool calling; achieves strong BFCLv4 results and competitive performance with Qwen-2.5-7B-Instruct.


<details>
  <summary>Details</summary>
Motivation: In multi-turn tool calling, rewards are sparse and exploration costly; within-group reward variation can be negligible, causing vanishing updates in GRPO; need guidance to diversify trajectories.

Method: Pretrain RCTP using reward goal tokens injected into prompts; during RL, sample diverse reward tokens within each GRPO group and condition rollouts on the token; improves within-group diversity and advantage gains.

Result: Consistent performance gains on BFCLv4; Qwen-2.5-7B-Instruct surpasses all closed-source API models.

Conclusion: Reward-conditioning via discrete reward tokens provides controllable exploration, improves signal in GRPO-based RL for multi-turn tool use, and yields state-of-the-art or competitive results on benchmarking.

Abstract: Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.

</details>


### [90] [Visual Reasoning over Time Series via Multi-Agent System](https://arxiv.org/abs/2602.03026)
*Weilin Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: MAS4TS introduces a tool-driven multi-agent system for general time-series tasks, using an Analyzer-Reasoner-Executor paradigm to combine visual reasoning over plots with latent trajectory reconstruction, achieving state-of-the-art performance and strong generalization across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the gap between traditional time-series methods and pretrained large-model approaches by enabling intuitive visual reasoning and adaptive tool usage for diverse tasks.

Method: Three specialized agents (Analyzer, Reasoner, Executor) communicate via shared memory and gated channels. A router selects task-specific tool chains. Visual reasoning is performed on time-series plots using a Vision-Language Model with structured priors to extract temporal structures, followed by latent-space trajectory reconstruction. The system integrates tool-driven execution to handle various tasks.

Result: Empirical results show MAS4TS achieves state-of-the-art performance across a broad spectrum of time-series tasks, with strong generalization and efficient inference.

Conclusion:  MAS4TS demonstrates that combining visual reasoning on time-series plots with latent trajectory reconstruction within a tool-driven multi-agent framework yields robust generalization and strong performance, indicating promise for broad applicability in time-series analysis.

Abstract: Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.

</details>


### [91] [KANFIS A Neuro-Symbolic Framework for Interpretable and Uncertainty-Aware Learning](https://arxiv.org/abs/2602.03034)
*Binbin Yong,Haoran Pei,Jun Shen,Haoran Li,Qingguo Zhou,Zhao Su*

Main category: cs.AI

TL;DR: KANFIS is a compact neuro-symbolic system that unifies fuzzy reasoning with additive function decomposition to avoid exponential rule explosion in ANFIS, scalable linearly with input dimensionality, supports Type-1 and Interval Type-2 logic, uses sparse masks for interpretable rule sets, and achieves competitive performance.


<details>
  <summary>Details</summary>
Motivation: Address the structural complexity and exponential growth of rules in product-based ANFIS; provide interpretable, uncertainty-aware fuzzy modeling with scalable architecture.

Method: Propose Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS) with additive aggregation, additive decomposition, compatibility with T1 and IT2, and sparse masking to generate compact rule sets.

Result: Empirical results show competitive performance against representative neural and neuro-fuzzy baselines.

Conclusion: KANFIS offers linear scalability in parameters and rules with input dimensionality, enhances interpretability via sparse, structured rules, and enables explicit uncertainty handling through IT2 fuzzy logic.

Abstract: Adaptive Neuro-Fuzzy Inference System (ANFIS) was designed to combine the learning capabilities of neural network with the reasoning transparency of fuzzy logic. However, conventional ANFIS architectures suffer from structural complexity, where the product-based inference mechanism causes an exponential explosion of rules in high-dimensional spaces. We herein propose the Kolmogorov-Arnold Neuro-Fuzzy Inference System (KANFIS), a compact neuro-symbolic architecture that unifies fuzzy reasoning with additive function decomposition. KANFIS employs an additive aggregation mechanism, under which both model parameters and rule complexity scale linearly with input dimensionality rather than exponentially. Furthermore, KANFIS is compatible with both Type-1 (T1) and Interval Type-2 (IT2) fuzzy logic systems, enabling explicit modeling of uncertainty and ambiguity in fuzzy representations. By using sparse masking mechanisms, KANFIS generates compact and structured rule sets, resulting in an intrinsically interpretable model with clear rule semantics and transparent inference processes. Empirical results demonstrate that KANFIS achieves competitive performance against representative neural and neuro-fuzzy baselines.

</details>


### [92] [MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems](https://arxiv.org/abs/2602.03053)
*Vishal Venkataramani,Haizhou Shi,Zixuan Ke,Austin Xu,Xiaoxiao He,Yingbo Zhou,Semih Yavuz,Hao Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: MAS-ProVe conducts a systematic empirical evaluation of process verification for multi-agent systems (MAS). It compares three verification paradigms (LLM-as-a-Judge, reward models, process reward models) at agent- and iteration-level granularity, across five verifiers and four context-management strategies, over six MAS frameworks and multiple benchmarks. Results show process-level verification is not reliably beneficial and often high-variance; LLMs as judges (especially trained judges) outperform reward-based methods; small gap between LLM-as-judge and single-agent LLMs; a context-length vs. performance trade-off; conclusion: robust process verification for MAS remains an open challenge. Code available.


<details>
  <summary>Details</summary>
Motivation: To determine whether process verification can reliably guide coordination in MAS built on LLMs, given the high variance of reasoning trajectories and unclear effectiveness of verification in prior work.

Method: MAS-ProVe conducts a large-scale empirical study across three verification paradigms (LLM-as-a-Judge, reward models, process reward models) and two granularity levels (agent-level and iteration-level). It evaluates five verifiers and four context-management strategies over six MAS frameworks on multiple reasoning benchmarks, with code released for reproducibility.

Result: Process-level verification does not consistently improve MAS performance and exhibits high variance. LLMs acting as judges generally outperform reward-based approaches, with trained judges outperforming general LLMs. There's a small gap between LLMs as judges and as single agents. A context-length-performance trade-off is observed in verification.

Conclusion: Effective and robust process verification for MAS remains an open challenge requiring advances beyond current paradigms; the work provides a consolidated empirical baseline and releases code for future research.

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

</details>


### [93] [Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis](https://arxiv.org/abs/2602.03128)
*Abdelghny Orogat,Ana Rostam,Essam Mansour*

Main category: cs.AI

TL;DR: 论文聚焦多智能体LLM框架的架构对系统性能的影响，提出体系结构分类法与统一评测套件MAFBench，并通过对比实验揭示架构选择对延迟、规划准确性和协同成功率的显著影响，进而给出设计原则与框架选择建议。


<details>
  <summary>Details</summary>
Motivation: 现有基准多聚焦单一能力，缺乏针对框架层面的标准化评估；不同架构决策可能导致数量级的性能差异，亟需在受控条件下对多智能体LLM框架进行系统比较。

Method: 提出面向多智能体LLM框架的体系结构分类法，在统一执行管线下构建MAFBench，将现有基准整合为一个可重复、可控的评测流程；在多个主流框架上开展对比实验，评估编排开销、内存行为、规划、专业化与协作等能力。

Result: 研究表明，框架层设计选择本身即可带来延迟提升超过100倍、规划准确性下降最多30%、协同成功率从90%以上降至不到30%。

Conclusion: 给出具体的体系结构设计原则与框架选择指南，并指出未来在框架层面进一步研究的方向。

Abstract: Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.

</details>


### [94] [Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration](https://arxiv.org/abs/2602.03151)
*Wei Dai,Haoyu Wang,Honghao Chang,Lijun He,Fan Li,Jian Sun,Haixia Bi*

Main category: cs.AI

TL;DR: 提出了一种用于缺失模态的通用修复策略，结合可插拔的扩散模型作为中间阶段训练模块，通过动态模态门控和跨模态互学实现跨模态对齐，在零-shot评估中超越基线，且对缺失率/环境鲁棒，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 解决VLM在输入模态不全时性能下降的问题，Prompt-based及Imputation-based方法各有不足，需在恢复语义准确性的同时保持模型的一般化能力。

Method: 将增强扩散模型作为中间训练模块；(I) 动态模态门控，按条件特征自适应引导生成语义一致的特征；(II) 跨模态互学机制，连接双编码器的语义空间实现双向对齐。

Result: 在零-shot场景下对比基线方法有明显提升；广泛的实验和消融研究证明模型的鲁棒性和可扩展性，在不同缺失率和环境下表现稳定。

Conclusion: 所提出策略为VLM在缺失模态场景下提供一个鲁棒、可扩展的扩展，且代码和模型将公开。

Abstract: Vision Language Models (VLMs) typically assume complete modality input during inference. However, their effectiveness drops sharply when certain modalities are unavailable or incomplete. Current research primarily faces two dilemmas: Prompt-based methods struggle to restore missing yet indispensable features and impair generalization of VLMs. Imputation-based approaches, lacking effective guidance, are prone to generating semantically irrelevant noise. Restoring precise semantics while sustaining VLM generalization remains challenging. Therefore, we propose a general missing modality restoration strategy in this paper. We introduce an enhanced diffusion model as a pluggable mid-stage training module to effectively restore missing features. Our strategy introduces two key innovations: (I) Dynamic Modality Gating, which adaptively leverages conditional features to steer the generation of semantically consistent features; (II) Cross-Modal Mutual Learning mechanism, which bridges the semantic spaces of dual encoders to achieve bidirectional alignment. Zero-shot evaluations across benchmark datasets demonstrate that our approach outperforms existing baseline methods. Extensive experiments and ablation studies confirm our model as a robust and scalable extension for VLMs in missing modality scenarios, ensuring reliability across diverse missing rates and environments. Our code and models will be publicly available.

</details>


### [95] [VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models](https://arxiv.org/abs/2602.03160)
*Woojin Kim,Sieun Hyeon,Jusang Oh,Jaeyoung Do*

Main category: cs.AI

TL;DR: VALUEFLOW 提供一个统一、带校准强度控制的价值取向框架，涵盖抽取、评估与引导三个环节，核心组件为分层价值嵌入空间(HIVES)、大规模带强度标注的文本资源VIDB以及基锚的评估器，在十个模型与四种价值理论上进行大规模实验，揭示多价值控制的可引导性不对称与组合规律。


<details>
  <summary>Details</summary>
Motivation: 弥补现有偏好法在深层动机原理、层级结构、强度校准与可控性方面的不足，推动对价值的系统化、可比较和可控的对齐框架。

Method: 提出三大组件并整合为 VALUEFLOW： (i) HIVES：捕捉价值结构及其层级关系；(ii) VIDB：大规模带强度估计的带标注文本资源，基于排序聚合获得强度；(iii) 锚定式评估器：通过将模型输出与 VIDB 面板进行排序对比，给出一致的强度分数；并在十个模型、四种理论下进行大规模实证分析，检验可引导性与组合规律。

Result: 建立了一个可扩展的价值强度评估与控制基础设施；在大量模型与理论上发现多价值控制的引导性存在不对称性，并揭示不同价值之间的组合规律。

Conclusion: 为多元价值对齐提供可扩展的评估与控制框架，推动对 LLMs 的价值强度的系统化、层次化和可控性研究。

Abstract: Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.

</details>


### [96] [Accordion-Thinking: Self-Regulated Step Summaries for Efficient and Readable LLM Reasoning](https://arxiv.org/abs/2602.03249)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Wenlei Shi,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.AI

TL;DR: 通过动态摘要的自适应推理粒度与强化学习，Accordion-Thinking 让模型在 Fold 模式下高效推理，显著提高吞吐并缩小与完全展开推理的性能差距。


<details>
  <summary>Details</summary>
Motivation: 解决长链式推理的推理时间与内存成本难题：KV缓存线性增长、注意力二次复杂度，以及在保持准确性的同时提升吞吐。

Method: 提出端到端的 Accordion-Thinking 框架，模型学习自调控推理步骤的粒度，通过动态摘要实现 Fold 推理（定期对思维过程进行摘要并丢弃历史思路），并使用强化学习进一步激励该能力。

Result: Fold 模式在训练过程中与 Unfold 的性能差距逐步缩小直至消失；模型学会将关键推理信息编码为紧凑摘要，实现对推理上下文的有效压缩；在 48GB 显存配置下实现 3x 吞吐且保持准确性，且结构化的步骤摘要便于可读性。

Conclusion: 通过自我压缩的学习，LLMs 可在较低的依赖 token 开销下完成复杂推理任务，显著提高吞吐且不降低解题质量，同时提供可读的推理过程。

Abstract: Scaling test-time compute via long Chain-ofThought unlocks remarkable gains in reasoning capabilities, yet it faces practical limits due to the linear growth of KV cache and quadratic attention complexity. In this paper, we introduce Accordion-Thinking, an end-to-end framework where LLMs learn to self-regulate the granularity of the reasoning steps through dynamic summarization. This mechanism enables a Fold inference mode, where the model periodically summarizes its thought process and discards former thoughts to reduce dependency on historical tokens. We apply reinforcement learning to incentivize this capability further, uncovering a critical insight: the accuracy gap between the highly efficient Fold mode and the exhaustive Unfold mode progressively narrows and eventually vanishes over the course of training. This phenomenon demonstrates that the model learns to encode essential reasoning information into compact summaries, achieving effective compression of the reasoning context. Our Accordion-Thinker demonstrates that with learned self-compression, LLMs can tackle complex reasoning tasks with minimal dependency token overhead without compromising solution quality, and it achieves a 3x throughput while maintaining accuracy on a 48GB GPU memory configuration, while the structured step summaries provide a human-readable account of the reasoning process.

</details>


### [97] [CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs](https://arxiv.org/abs/2602.03263)
*Yuxuan Liu,Yuntian Shi,Kun Wang,Haoting Shen,Kun Yang*

Main category: cs.AI

TL;DR: CSR-Bench: cross-modal reliability benchmark for multimodal LLMs; 61 fine-grained stress types across Safety, Over-rejection, Bias, and Hallucination; paired text-only controls to diagnose modality-induced shifts; 16 state-of-the-art MLLMs evaluated; reveals cross-modal alignment gaps and safety challenges with trade-offs between over-rejection and non-discriminatory safety.


<details>
  <summary>Details</summary>
Motivation: Address the gap that MLLMs often rely on unimodal shortcuts and lack integrated image-text interpretation, leading to unsafe or biased outputs; provide a structured benchmark to quantify cross-modal reliability.

Method: Construct CSR-Bench with four stress-testing interaction patterns (Safety, Over-rejection, Bias, Hallucination) spanning 61 fine-grained types. Each instance requires integrated image-text interpretation; include paired text-only controls to diagnose modality-induced behavior shifts. Evaluate 16 state-of-the-art MLLMs and analyze differences between multimodal and text-only inputs.

Result: Reveals systematic cross-modal alignment gaps: models show weak safety awareness, language-dominance under interference, and performance degradation when moving from text-only to multimodal inputs. A trade-off is observed between reducing over-rejection and maintaining safe, non-discriminatory behavior.

Conclusion: Safety gains may stem from refusal-oriented heuristics rather than robust cross-modal intent understanding; highlights need for improved cross-modal alignment, safer interaction design, and more nuanced evaluation beyond simple refusals.

Abstract: Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we additionally provide paired text-only controls to diagnose modality-induced behavior shifts. We evaluate 16 state-of-the-art MLLMs and observe systematic cross-modal alignment gaps. Models show weak safety awareness, strong language dominance under interference, and consistent performance degradation from text-only controls to multimodal inputs. We also observe a clear trade-off between reducing over-rejection and maintaining safe, non-discriminatory behavior, suggesting that some apparent safety gains may come from refusal-oriented heuristics rather than robust intent understanding. WARNING: This paper contains unsafe contents.

</details>


### [98] [Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity](https://arxiv.org/abs/2602.03315)
*Menglin Xia,Xuchao Zhang,Shantanu Dixit,Paramaguru Harimurugan,Rujia Wang,Victor Ruhle,Robert Sim,Chetan Bansal,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Memora 提出一种 harmonic memory 表征，平衡抽象与具体性，通过主抽象、统一内存条目、线索锚点与检索策略实现可扩展且上下文相关的检索，理论上将 RAG 与 KG 基本记忆视为特例；在 LoCoMo 与 LongMemEval 上达到新 state-of-the-art，展现随记忆规模扩大而提升的检索相关性与推理能力。


<details>
  <summary>Details</summary>
Motivation: 需要处理持续增长的信息并实现上下文感知检索；抽象化有助于可扩展性，但往往牺牲具体细节，Memora 旨在在抽象与具体性之间取得平衡。

Method: 提出 Memora 架构：1) 主抽象用于索引具体记忆值；2) 将相关更新整合成统一记忆条目；3) 线索锚点扩展检索覆盖记忆的不同方面并连接相关记忆；4) 检索策略主动利用这些记忆连接，超越直接语义相似性；5) 理论上证明标准的 RAG 与 KG 基记忆系统是本框架的特例。

Result: 理论层面：RAG 与 KG 基记忆系统是本框架的特例。实证层面：在 LoCoMo 与 LongMemEval 上实现新的最优结果，表现为更高的检索相关性和在大规模记忆下的推理效果。

Conclusion: Memora 提供一种在抽象性与具体性之间取得平衡的记忆表示，结合记忆连接的检索策略，具备良好扩展性；框架对 RAG 与 KG 记忆具普适性，实验上显示在规模化记忆下的检索与推理优势。

Abstract: Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.

</details>


### [99] [MentalSeek-Dx: Towards Progressive Hypothetico-Deductive Reasoning for Real-world Psychiatric Diagnosis](https://arxiv.org/abs/2602.03340)
*Xiao Sun,Yuming Yang,Junnan Zhu,Jiang Zhong,Xinyu Zhou,Kaiwen Wei*

Main category: cs.AI

TL;DR: 提出 MentalDx Bench：首个面向临床场景的精神疾病诊断基准，基于712份去识别化的EHR，由精神科医生按ICD-11标注，覆盖76种疾病、16个诊断类别。评估18个LLM，发现存在范式错位：在大类诊断上表现良好，但在疾病级诊断上系统性失败。为此提出 MentalSeek-Dx，一种以内部化临床推理过程为目标的医专用LLM，通过监督轨迹构建和分层式强化学习进行训练。在 MentalDx Bench 上，MentalSeek-Dx以14B参数达到SOTA，提供一个临床对齐的可靠精神病诊断框架。


<details>
  <summary>Details</summary>
Motivation: 当前基准缺乏生态效度和细粒度诊断监督，对真实临床推理过程的对齐不足；存在从模式匹配向临床假设-推理(hypothetico-deductive)的范式错位，需要一个更贴近临床推理的评估与建模方案。

Method: 构建712份去识别的EHR数据集，由具ICD-11规范的精神科医生标注，覆盖76种疾病、16个诊断类别。对18种LLM进行评估，发现大类诊断可行但疾病级诊断欠缺。提出 MentalSeek-Dx，通过监督轨迹构建和课程化强化学习训练以内化临床推理过程。

Result: 在 MentalDx Bench 上，MentalSeek-Dx以14B参数实现SOTA性能，优于现有模型并显示在疾病级诊断上的提升。

Conclusion: 为精神病诊断提供一个临床对齐的、可依赖的基准和模型框架，证明轨迹式训练和专业化微调可以缩小从模式识别到临床推理之间的差距。

Abstract: Mental health disorders represent a burgeoning global public health challenge. While Large Language Models (LLMs) have demonstrated potential in psychiatric assessment, their clinical utility is severely constrained by benchmarks that lack ecological validity and fine-grained diagnostic supervision. To bridge this gap, we introduce \textbf{MentalDx Bench}, the first benchmark dedicated to disorder-level psychiatric diagnosis within real-world clinical settings. Comprising 712 de-identified electronic health records annotated by board-certified psychiatrists under ICD-11 guidelines, the benchmark covers 76 disorders across 16 diagnostic categories. Evaluation of 18 LLMs reveals a critical \textit{paradigm misalignment}: strong performance at coarse diagnostic categorization contrasts with systematic failure at disorder-level diagnosis, underscoring a gap between pattern-based modeling and clinical hypothetico-deductive reasoning. In response, we propose \textbf{MentalSeek-Dx}, a medical-specialized LLM trained to internalize this clinical reasoning process through supervised trajectory construction and curriculum-based reinforcement learning. Experiments on MentalDx Bench demonstrate that MentalSeek-Dx achieves state-of-the-art (SOTA) performance with only 14B parameters, establishing a clinically grounded framework for reliable psychiatric diagnosis.

</details>


### [100] [GFlowPO: Generative Flow Network as a Language Model Prompt Optimizer](https://arxiv.org/abs/2602.03358)
*Junmo Cho,Suhan Kim,Sangjune An,Minsu Kim,Dong Bok Lee,Heejun Lee,Sung Ju Hwang,Hae Beom Lee*

Main category: cs.AI

TL;DR: GFlowPO: 使用基于GFlowNet的离散提示优化，结合元提示先验和动态记忆更新(DMU)实现高效采样的提示搜索，在Few-shot文本分类、指令归纳和问答等任务上显著优于最近的离散提示优化基线。


<details>
  <summary>Details</summary>
Motivation: 提示空间呈组合爆炸，奖励稀疏且评估代价高；现有基于强化学习的优化多为on-policy，且元提示来自固定分布，导致样本效率低下。需要更高效、探索性更好的提示搜索框架。

Method: 步骤1：采用离线GFlowNet目标对轻量级提示语言模型进行微调，使用回放策略重用历史提示评估实现样本高效探索；步骤2：引入Dynamic Memory Update (DMU)，无训练成本地更新元提示，通过注入回放缓冲区中的多样提示和小型优先队列中的高奖励提示，将搜索重心逐步聚焦在高回报区域。通过将潜在提示对后验推断，结合参考LM先验实现。

Result: 在少样本文本分类、指令归纳与问答任务上，GFlowPO持续优于最近的离散提示优化基线。

Conclusion: 将GFlowNet的概率性提示优化与DMU相结合，提升离散提示搜索的样本效率与效果，且具跨任务适用性。

Abstract: Finding effective prompts for language models (LMs) is critical yet notoriously difficult: the prompt space is combinatorially large, rewards are sparse due to expensive target-LM evaluation. Yet, existing RL-based prompt optimizers often rely on on-policy updates and a meta-prompt sampled from a fixed distribution, leading to poor sample efficiency. We propose GFlowPO, a probabilistic prompt optimization framework that casts prompt search as a posterior inference problem over latent prompts regularized by a meta-prompted reference-LM prior. In the first step, we fine-tune a lightweight prompt-LM with an off-policy Generative Flow Network (GFlowNet) objective, using a replay-based training policy that reuses past prompt evaluations to enable sample-efficient exploration. In the second step, we introduce Dynamic Memory Update (DMU), a training-free mechanism that updates the meta-prompt by injecting both (i) diverse prompts from a replay buffer and (ii) top-performing prompts from a small priority queue, thereby progressively concentrating the search process on high-reward regions. Across few-shot text classification, instruction induction benchmarks, and question answering tasks, GFlowPO consistently outperforms recent discrete prompt optimization baselines.

</details>


### [101] [Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility](https://arxiv.org/abs/2602.03402)
*Mengxuan Wang,Yuxin Chen,Gang Xu,Tao He,Hongjie Jiang,Ming Li*

Main category: cs.AI

TL;DR: 提出风险意识注入（RAI），通过对高风险视觉令牌的定向调制，在不额外训练的前提下提升VLM对视觉输入中不安全内容的风险识别，恢复LLM级别的风险感知，同时保持跨模态推理性能。


<details>
  <summary>Details</summary>
Motivation: VLM在跨模态推理方面提升了LLM的能力，但易受多模态越界攻击，且现有防御通常需要安全微调或强力的标记操控，成本高且可能显著损害实用性。LLMs对文本不安全内容具有天然识别能力，而在VLM中引入的视觉输入往往削弱这种风险信号。

Method: 从语言嵌入中构建Unsafe Prototype Subspace，并在跨模态特征空间对选定的高风险视觉令牌进行有针对性的调制，显式激活安全相关信号以实现安全校准，且无需额外训练。

Result: 大量实验表明RAI显著降低攻击成功率，同时不牺牲任务性能。

Conclusion: RAI提供了一种轻量、无需训练的安全校准框架，在提升VLM对不安全输入的识别能力的同时，保持跨模态推理能力，具有广泛的应用潜力。

Abstract: Vision language models (VLMs) extend the reasoning capabilities of large language models (LLMs) to cross-modal settings, yet remain highly vulnerable to multimodal jailbreak attacks. Existing defenses predominantly rely on safety fine-tuning or aggressive token manipulations, incurring substantial training costs or significantly degrading utility. Recent research shows that LLMs inherently recognize unsafe content in text, and the incorporation of visual inputs in VLMs frequently dilutes risk-related signals. Motivated by this, we propose Risk Awareness Injection (RAI), a lightweight and training-free framework for safety calibration that restores LLM-like risk recognition by amplifying unsafe signals in VLMs. Specifically, RAI constructs an Unsafe Prototype Subspace from language embeddings and performs targeted modulation on selected high-risk visual tokens, explicitly activating safety-critical signals within the cross-modal feature space. This modulation restores the model's LLM-like ability to detect unsafe content from visual inputs, while preserving the semantic integrity of original tokens for cross-modal reasoning. Extensive experiments across multiple jailbreak and utility benchmarks demonstrate that RAI substantially reduces attack success rate without compromising task performance.

</details>


### [102] [DiscoverLLM: From Executing Intents to Discovering Them](https://arxiv.org/abs/2602.03429)
*Tae Soo Kim,Yoonjoo Lee,Jaesang Yu,John Joon Young Chung,Juho Kim*

Main category: cs.AI

TL;DR: DiscoverLLM trains LLMs to help users discover and form their intents via a hierarchical, concretization-based user model, enabling adaptive exploration when intents are unclear and convergence when they are clear.


<details>
  <summary>Details</summary>
Motivation: Users often interact with LLMs while their own intents are ambiguous or undeveloped. Traditional clarification prompts may fail when users have not yet formed goals. A system that supports intent discovery can improve efficiency, satisfaction, and task performance across domains.

Method: Introduce DiscoverLLM with a hierarchical user simulator that models cognitive state as progressively concretizing intents. The degree of concretization serves as a reward signal. Train LLMs to adaptively explore options when intents are unclear and to converge when intents are crystallized. Evaluate on interactive benchmarks in creative writing, technical writing, and SVG drawing; include a human study with 75 participants.

Result: DiscoverLLM achieves over 10% higher task performance and up to 40% shorter conversations across proposed benchmarks. In a user study with 75 participants, it improved conversation satisfaction and efficiency compared to baselines.

Conclusion: A generalizable framework that enables LLMs to scaffold user intent discovery, improving collaboration, efficiency, and satisfaction across tasks; the approach leverages a hierarchy of intents and a concretization reward to balance exploration and convergence.

Abstract: To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking "what kind of tone do you want?" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.

</details>


### [103] [CRL-VLA: Continual Vision-Language-Action Learning](https://arxiv.org/abs/2602.03445)
*Qixin Zeng,Shuo Zhang,Hongyin Zhang,Renjie Wang,Han Zhao,Libang Zhao,Runze Li,Donglin Wang,Chao Huang*

Main category: cs.AI

TL;DR: A continual reinforcement learning framework (CRL-VLA) for vision-language-action models that balances stability and plasticity via asymmetric regulation using a dual-critic with a goal-conditioned value function, backed by theoretical bounds, and validated on LIBERO with improved anti-forgetting and forward adaptation.


<details>
  <summary>Details</summary>
Motivation: Lifelong learning for embodied agents in open-world environments; current CRL methods struggle to simultaneously retain old skills and acquire new ones in VLA settings, hindering long-term deployment.

Method: Introduce CRL-VLA with a dual-critic architecture and a Goal-Conditioned Value Formulation (GCVF). One frozen critic enforces semantic consistency; a trainable estimator drives adaptation. Derive a unified bound linking stability-plasticity to goal-conditioned advantage magnitude scaled by policy divergence; enforce asymmetric regulation by constraining advantages on prior tasks while permitting growth on new tasks.

Result: Empirical evaluation on the LIBERO benchmark shows improved balance between retaining old skills and learning new ones, outperforming baselines in anti-forgetting and forward adaptation.

Conclusion: CRL-VLA provides a principled framework for continual post-training of VLA models, achieving effective stability-plasticity trade-offs and strong empirical performance in lifelong robotics tasks.

Abstract: Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.

</details>


### [104] [The Dual Role of Abstracting over the Irrelevant in Symbolic Explanations: Cognitive Effort vs. Understanding](https://arxiv.org/abs/2602.03467)
*Zeynep G. Saribatur,Johannes Langer,Ute Schmid*

Main category: cs.AI

TL;DR: 通过在ASP框架下对解释进行移除与聚类等抽象，实验表明聚类细节能显著提升理解，而移除细节显著降低认知负荷，支持抽象化提升人本符号解释的有效性。


<details>
  <summary>Details</summary>
Motivation: 提升形式化、可解释AI的可理解性并降低人类处理符号解释时的认知成本。

Method: 以Answer Set Programming为框架，定义无关细节的抽象策略（移除与聚类），通过跨领域的认知实验让受试者在有/无抽象解释的情境中对刺激进行分类任务，比较不同解释形式的理解度与认知负荷。

Result: 聚类细节显著提高理解程度，移除细节显著降低认知努力，支持抽象化有利于人本化的符号解释的假设。

Conclusion: 对符号解释而言，恰当的抽象（尤其是聚类）能在保持可解释性的同时降低认知成本，促进人类对AI推理的接受与合作。

Abstract: Explanations are central to human cognition, yet AI systems often produce outputs that are difficult to understand. While symbolic AI offers a transparent foundation for interpretability, raw logical traces often impose a high extraneous cognitive load. We investigate how formal abstractions, specifically removal and clustering, impact human reasoning performance and cognitive effort. Utilizing Answer Set Programming (ASP) as a formal framework, we define a notion of irrelevant details to be abstracted over to obtain simplified explanations. Our cognitive experiments, in which participants classified stimuli across domains with explanations derived from an answer set program, show that clustering details significantly improve participants' understanding, while removal of details significantly reduce cognitive effort, supporting the hypothesis that abstraction enhances human-centered symbolic explanations.

</details>


### [105] [When Routing Collapses: On the Degenerate Convergence of LLM Routers](https://arxiv.org/abs/2602.03478)
*Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: EquiRouter tackles routing collapse in LLM routing by switching from predicting scalar performance scores to directly learning model rankings, restoring small-model usage and improving cost efficiency. On RouterBench, it saves ~17% cost at GPT-4-level performance compared to the strongest prior router; code available at GitHub.


<details>
  <summary>Details</summary>
Motivation: Current routers suffer from an objective–decision mismatch: they predict scalar scores but routing decisions depend on discrete model comparisons. As budgets rise, routers default to the most capable (and expensive) model, under-utilizing cheaper models and increasing cost without quality gains.

Method: Introduce EquiRouter, a decision-aware router that optimizes for model rankings rather than scalar scores. It trains to predict the correct relative ordering among candidate models (pairwise/listwise ranking losses) so routing decisions reflect true model comparisons, mitigating routing collapse and reusing small models.

Result: Empirical results on RouterBench show about 17% cost reduction at GPT-4-level performance versus the strongest prior router; demonstrates effective restoration of small-model routing with maintained performance. Code available at the provided GitHub URL.

Conclusion: A ranking-based, decision-aware routing approach can substantially reduce unnecessary expenditure and re-enable small models in routing, addressing the under-explored routing collapse phenomenon and improving the core cost–quality trade-off.

Abstract: LLM routing aims to achieve a favorable quality--cost trade-off by dynamically assigning easy queries to smaller models and harder queries to stronger ones. However, across both unimodal and multimodal settings, we uncover a pervasive yet underexplored failure mode in existing routers: as the user's cost budget increases, routers systematically default to the most capable and most expensive model even when cheaper models already suffice. As a result, current routers under-utilize small models, wasting computation and monetary cost and undermining the core promise of routing; we term this phenomenon routing collapse. We attribute routing collapse to an objective--decision mismatch: many routers are trained to predict scalar performance scores, whereas routing decisions ultimately depend on discrete comparisons among candidate models. Consequently, small prediction errors can flip relative orderings and trigger suboptimal selections. To bridge this gap, we propose EquiRouter, a decision-aware router that directly learns model rankings, restoring the role of smaller models and mitigating routing collapse. On RouterBench, EquiRouter reduces cost by about 17\% at GPT-4-level performance compared to the strongest prior router. Our code is available at https://github.com/AIGNLAI/EquiRouter.

</details>


### [106] [Persona Generators: Generating Diverse Synthetic Personas at Scale](https://arxiv.org/abs/2602.03545)
*Davide Paglieri,Logan Cross,William A. Cunningham,Joel Z. Leibo,Alexander Sasha Vezhnevets*

Main category: cs.AI

TL;DR: 提出 Persona Generators，通过 AlphaEvolve 的迭代优化让大语言模型生成多样化且覆盖性强的合成人群，以更全面地评估 AI 系统在不同用户群体中的行为表现，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦于密度匹配（复制最 probable 行为），忽略覆盖长尾行为，且获取真实人群数据成本高、不可扩展。需要能够在给定情境下扩展出广泛、且尽可能覆盖可能性空间的合成用户画像。

Method: 以 AlphaEvolve 为循环框架，使用大语言模型作为变异算子来改良 Persona Generator 代码；从小描述自动扩展为覆盖多样观点与偏好的合成人口；设计轻量化实现以在不同情境下提升多样性覆盖，并在六项多样性指标上与基线比较。

Result: 进化后的 Persona Generators 在 hold-out contexts 上显著优于基线，在六项多样性指标上提高覆盖度，能够产生罕见性状组合，扩展输出的覆盖范围。

Conclusion: 将大语言模型与进化优化结合，能够高效生成覆盖性强的合成人口，为 AI 系统评估提供更全面的人群模型，适用于新技术或假设场景，且实现轻量、可扩展的生成器。

Abstract: Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.

</details>


### [107] [EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories](https://arxiv.org/abs/2602.03569)
*Linjie Mu,Zhongzhen Huang,Yannian Gu,Shengqian Qin,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: 提出了一个以因果时序数据为基础的医疗世界模型EHRWorld，并配套大规模纵向临床数据集EHRWorld-110K；显著优于简单的LLM基线，在长期模拟、敏感事件建模和推理效率方面表现更好，强调在因果 grounding 的时序医疗数据上训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有纯医学知识的LLMs在多步干预下难以维持一致的患者状态，易导致长期临床仿真的误差累积；需要一个能在时间维度上稳定跟踪病程、考虑干预的世界模型。

Method: 提出以因果顺序范式训练的以患者为中心的医疗世界模型EHRWorld，并构建真实世界EHR推导出的大规模纵向数据集EHRWorld-110K；对比基线LLM，评估在长期仿真、临床事件建模和推理效率等维度的表现。

Result: EHRWorld在多项指标上显著优于天真LLM基线，展现更稳定的长期仿真、对临床敏感事件的建模提升，以及更具可用性的推理效率。

Conclusion: 在因果、时序演化的临床数据上训练的医疗世界模型是实现可靠医疗世界建模的关键；仅靠知识型LLMs难以应对高风险的长期病程模拟，EHRWorld为疾病进展与治疗结果的仿真提供更稳健的框架。

Abstract: World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.

</details>


### [108] [Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12](https://arxiv.org/abs/2602.03630)
*Iñaki del Campo,Pablo Cuervo,Victor Rodriguez-Fernandez,Roberto Armellin,Jack Yarndley*

Main category: cs.AI

TL;DR: LLMs show improved strategic viability in orbital mission planning (GTOC12) but fail at execution due to physical and implementation gaps; they act as domain facilitators rather than autonomous engineers.


<details>
  <summary>Details</summary>
Motivation: Assess limits of current AI agents in high-dimensional, physically constrained planning; quantify strategic viability with expert rubric in astrodynamics.

Method: Adapt MLE-Bench to orbital mechanics; deploy AIDE-based agent; implement LLM-as-a-Judge rubric across five categories; compare models from GPT-4-Turbo to Gemini/o3.

Result: Strategic viability score rose from 9.3 to 17.2/26 over two years; gap between strategy and execution persists due to unit inconsistencies, boundary condition errors, and inefficient debugging loops.

Conclusion: Current LLMs possess domain knowledge but insufficient implementation capability; they are domain facilitators rather than fully autonomous engineers in complex space mission design.

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an "LLM-as-a-Judge" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.

</details>


### [109] [Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration](https://arxiv.org/abs/2602.03647)
*Bowei He,Minda Hu,Zenan Xu,Hongru Wang,Licheng Zong,Yankai Chen,Chen Ma,Xue Liu,Pluto Zhou,Irwin King*

Main category: cs.AI

TL;DR: Search-R2 introduces an Actor-Refiner framework for search-based reasoning with targeted interventions; uses a Meta-Refiner to cut-and-regenerate flawed steps; combines outcome and information-density rewards; formulates a smoothed mixture policy; shows improvements over RAG and RL baselines with modest overhead.


<details>
  <summary>Details</summary>
Motivation:  tackle the credit-assignment problem in RL for search-enabled reasoning where sparse rewards fail to distinguish good reasoning from lucky guesses; provide fine-grained supervision to steer search efficiently.

Method: Decomposes generation into Actor (initial reasoning trajectories) and Meta-Refiner (diagnosis and repair via cut-and-regenerate). Trains with a hybrid reward: task outcome + dense information-density reward. The Actor-Refiner interaction is modeled as a smoothed mixture policy and is optimized jointly; theoretical justification for performance gains.

Result: Empirically, Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales on general and multi-hop QA datasets, with higher reasoning accuracy and minimal overhead.

Conclusion: Selective, targeted corrections via the Meta-Refiner improve reasoning quality in search-enhanced agents; the framework achieves robust gains across datasets and scales, indicating effective handling of the multi-scale credit assignment problem in this setting.

Abstract: Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.

</details>


### [110] [AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration](https://arxiv.org/abs/2602.03786)
*Jianhao Ruan,Zhihao Xu,Yiran Peng,Fashen Ren,Zhaoyang Yu,Xinbing Liang,Jinyu Xiang,Bang Liu,Chenglin Wu,Yuyu Luo,Jiayi Zhang*

Main category: cs.AI

TL;DR: 提出一种通用的智能体抽象（Instruction, Context, Tools, Model）以及 AOrchestra 调度器，按需在任务步中对接并实例化子智能体，实现动态、框架无关的子智能体组合与执行。通过对任务上下文、工具与模型的即时选择与创建，降低人工工程量，并在 GAIA、SWE-Bench、Terminal-Bench 上与 Gemini-3-Flash 结合时实现相对提升约 16.28%，体现了可控的性能-成本权衡与 Pareto 效率之趋同。


<details>
  <summary>Details</summary>
Motivation: 解决长期序列任务中子智能体作为工具的动态抽象视图不足的问题，现有设计缺乏对子智能体的自适应、可组合的抽象，难以扩展与降低人工干预。

Method: 提出统一的、框架无关的智能体抽象，作为一个四元组：Instruction（指令）、Context（上下文）、Tools（工具集）、Model（模型）。该抽象作为能力的可组合配方，支持按需生成专门执行器。以 AOrchestra 为核心，其调度器在每步构造该四元组，筛选相关上下文、工具与模型，并通过即时的自动化智能体创建实现执行。

Result: 在三个挑战性基准 GAIA、SWE-Bench、Terminal-Bench 上，与最强基线相比，结合 Gemini-3-Flash 时实现相对性能提升 16.28%；方法具备可控的成本-性能折中，并具备框架无关与即插即用的优势。

Conclusion: 提出的抽象与系统架构支持动态的、可插拔的子智能体执行者，降低人力设计成本，具有良好的泛化能力与可扩展性，实验结果在多基准上验证了其有效性。

Abstract: Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra

</details>


### [111] [Conformal Thinking: Risk Control for Reasoning on a Compute Budget](https://arxiv.org/abs/2602.03814)
*Xi Wang,Anushri Suresh,Alvin Zhang,Rishi More,William Jurayj,Benjamin Van Durme,Mehrdad Farajtabar,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 以风险控制为框架的自适应推理停止机制，设定上阈值保证输出可信度并引入下阈值用于难解实例的提前停止，在给定风险目标和验证集时通过分布自由的风险控制来优化停止策略，提升计算效率同时控制错误率。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理中可通过增加token预算提升数据集级别准确性，但需在成本和风险之间权衡；传统方法难以同时设定预算和阈值，需一个稳健的、可泛化的风险控制框架。

Method: 提出上阈值实现“当模型自信时停止推理”、下阈值的参数化策略用于提前停止；给定目标风险和验证集，使用分布自由的风险控制来最优确定这两个停止机制；在多预算控制情境下引入效率损失来选取最有效的退出机制；通过实验在多种推理任务和模型上验证。

Result: 在多任务和多模型上实现了风险目标下的计算效率提升；下阈值和集合停止机制带来效率提升，同时保持风险约束。

Conclusion: 将预算设置问题转化为风险控制问题，提供可泛化、无分布假设的停止策略，具有可操作性并提升推理效率。

Abstract: Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.

</details>


### [112] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: 基于诊断知识图谱的对话诊断系统，通过两步推理生成诊断假设并通过澄清性问句验证，利用改造的MIMIC-IV病人资料和成熟模拟器提高现实性与诊断效果。


<details>
  <summary>Details</summary>
Motivation: 现实多轮问诊中，依赖模型参数或假设患者信息充分性不足；需要一个能在信息不充分时仍能迭代推理的系统，并具备真实患者交互仿真的评估能力。

Method: 构建诊断知识图谱，分两阶段：1) 根据对话上下文生成诊断假设；2) 通过澄清问句验证并修正假设，循环直到达到最终诊断。采用改造的病人模拟器以及 MIMIC-IV 患者资料来提供真实的对话场景与标签。评估包括与医生评估的现实性和临床效用。

Result: 在诊断准确性和效率方面优于强基线，医生评估表明模拟器真实、所生成的问题具临床价值。

Conclusion: 提出的对话诊断框架在早期临床接触情境有潜力应用，代码将在发表后开源，未来工作可扩展到更复杂场景和对话质量提升。

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [113] [UNSO: Unified Newton Schulz Orthogonalization](https://arxiv.org/abs/2602.02500)
*Chen Hu,Qianxi Zhao,Yuming Li,Mingyu Zhou,Xiyin Li*

Main category: cs.LG

TL;DR: 提出统一的 Newton-Schulz 正交化框架 UNSO，通过可学习多项式系数替代常规多项式展开，提升计算效率与收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决传统 NS 迭代的低效与不稳定，以及现有改进仍沿用原有迭代范式导致在长维矩阵乘积上的计算负担。

Method: 将迭代结构统一为一个框架，不再进行多项式展开；评估每个矩阵幂的作用，剔除不重要项，提出带可学习系数的多项式近似；对这些系数进行端到端优化；提供代码实现。

Result: 理论与实验表明，通过学习得到的系数能在收敛性与性能方面显著优于传统方法，且收敛更稳定；且代码已开源，便于复现实验。

Conclusion: UNSO 成功解决 NS 迭代的效率与稳定性问题，避免多项式展开并通过可学习系数实现更稳健的收敛，具有较强的理论与实际应用意义。

Abstract: The Newton-Schulz (NS) iteration has gained increasing interest for its role in the Muon optimizer and the Stiefel manifold. However, the conventional NS iteration suffers from inefficiency and instability. Although various improvements have been introduced to NS iteration, they fail to deviate from the conventional iterative paradigm, which could increase computation burden largely due to the matrix products along the long dimension repeatedly. To address this, we consolidate the iterative structure into a unified framework, named Unified Newton-Schulz Orthogonalization (UNSO). To do so, we could avoid a polynomial expansion. Instead, we evaluate the role of each matrix power, remove the insignificant terms, and provide a recommended polynomial with learnable coefficients. These learnable coefficients are then optimized, and achieve an outstanding performance with stable convergence. The code of our method is available: https://github.com/greekinRoma/Unified_Newton_Schulz_Orthogonalization.

</details>


### [114] [Sparse Adapter Fusion for Continual Learning in NLP](https://arxiv.org/abs/2602.02502)
*Min Zeng,Xi Chen,Haiqin Yang,Yike Guo*

Main category: cs.LG

TL;DR: 提出一个稀疏适配器融合方法SAFM用于NLP持续学习，通过动态融合旧新适配器来解决参数重用不足、任务差异导致遗忘和为每个任务引入新参数的低效问题；包含决策阶段和微调阶段，优先重用/添加空适配器以节省参数，并通过层级损失促使适配器差异化；实验表明在参数量低于60%时可达到或超过SOTA。


<details>
  <summary>Details</summary>
Motivation: 面临的挑战包括：跨任务的参数重用低效；任务彼此不相似时易发生灾难性遗忘；为每个任务引入新参数从而阻碍相似任务之间的知识共享。需要在NLP持续学习场景下实现参数高效且抗遗忘的策略。

Method: SAFM分为决策阶段和微调阶段。决策阶段通过架构搜索判断是引入新适配器、重用现有适配器，还是添加一个空适配器，并优先选择重用或添加空适配器以最小化参数开销、最大化重用。微调阶段通过层级损失促使不同适配器之间的区分，以充分捕捉同一任务内的知识。实现对旧新适配器的动态融合，降低冗余并提升任务间知识共享。

Result: 在多项实验中，SAFM持续优于SOTA方法；在参数量仅使用总量60%以下的情况下即可达到或接近对比方法的性能，体现了显著的参数节省与性能保持。

Conclusion: SAFM通过动态融合与两阶段优化，有效缓解持续学习中的遗忘与参数瓶颈，实现了较小参数开销下的强表示能力与知识共享，实验结果支持其优越性。

Abstract: Continual learning in natural language processing plays a crucial role in adapting to evolving data and preventing catastrophic forgetting. Despite significant progress, existing methods still face challenges, such as inefficient parameter reuse across tasks, risking catastrophic forgetting when tasks are dissimilar, and the unnecessary introduction of new parameters for each task, which hampers knowledge sharing among similar tasks. To tackle these issues, we propose a Sparse Adapter Fusion Method (SAFM), which dynamically fuses old and new adapters to address these challenges. SAFM operates in two stages: the decision stage and the tuning stage. In the decision stage, SAFM determines whether to incorporate a new adapter, reuse an existing one, or add an empty adapter. The architecture search procedure, designed to prioritize reusing or adding empty adapters, minimizes parameter consumption and maximizes reuse. In the tuning stage, SAFM especially facilitates a layer-wise loss to encourage differentiation between adapters, effectively capturing knowledge within the same task. Experimental results consistently show that SAFM outperforms state-of-the-art (SOTA) methods, achieving comparable performance while utilizing less than 60% of the parameters.

</details>


### [115] [Learning ORDER-Aware Multimodal Representations for Composite Materials Design](https://arxiv.org/abs/2602.02513)
*Xinyao Li,Hangwei Qian,Jingjing Li,Ivor Tsang*

Main category: cs.LG

TL;DR: ORDER is a multimodal pretraining framework for composites that encodes ordinal relations to preserve continuity in property space, enabling interpolation and improved multimodal tasks under data scarcity.


<details>
  <summary>Details</summary>
Motivation: Graph-based representations struggle with composite materials due to continuous, nonlinear design spaces and heterogeneous data; existing multimodal methods rely on discrete graph-property mappings and fail under data scarcity; there is a need to enforce ordinality in latent spaces to reflect continuous property variation.

Method: Proposes ORDinal-aware imagE-tabulaR alignment (ORDER): a multimodal pretraining framework that jointly encodes image (microstructure) and tabular descriptors (e.g., fiber volume, misalignment) with ordinal constraints to ensure similar target properties map to nearby latent regions. Evaluated on a public nanofiber-reinforced composite dataset and an internal dataset simulating carbon fiber T700 distributions; tasks include property prediction, cross-modal retrieval, and microstructure generation.

Result: ORDER achieves consistent improvements over state-of-the-art multimodal baselines across cited tasks on both datasets, and enables meaningful interpolation between sparsely observed designs due to preserved ordinality in latent space.

Conclusion: Ordinal-aware multimodal pretraining effectively captures continuous design-property relationships in composites, improving predictive and generative capabilities under limited data, and is potentially generalizable to other material systems with continuous design spaces.

Abstract: Artificial intelligence (AI) has shown remarkable success in materials discovery and property prediction, particularly for crystalline and polymer systems where material properties and structures are dominated by discrete graph representations. Such graph-central paradigm breaks down on composite materials, which possess continuous and nonlinear design spaces that lack well-defined graph structures. General composite descriptors, e.g., fiber volume and misalignment angle, cannot fully capture the fiber distributions that fundamentally determine microstructural characteristics, necessitating the integration of heterogeneous data sources through multimodal learning. Existing alignment-oriented multimodal frameworks have proven effective on abundant crystal or polymer data under discrete, unique graph-property mapping assumptions, but fail to address the highly continuous composite design space under extreme data scarcity. In this work, we introduce ORDinal-aware imagE-tabulaR alignment (ORDER), a multimodal pretraining framework that establishes ordinality as a core principle for composite material representations. ORDER ensures that materials with similar target properties occupy nearby regions in the latent space, which effectively preserves the continuous nature of composite properties and enables meaningful interpolation between sparsely observed designs. We evaluate ORDER on a public Nanofiber-enforced composite dataset and an internally curated dataset that simulates the construction of carbon fiber T700 with diverse fiber distributions. ORDER achieves consistent improvements over state-of-the-art multimodal baselines across property prediction, cross-modal retrieval, and microstructure generation tasks.

</details>


### [116] [Scaled Dot-Product Attention implements projection of inputs onto a common surface](https://arxiv.org/abs/2602.02521)
*Terence D Sanger*

Main category: cs.LG

TL;DR: 将 SDPA 重写为输入在一个由输入本身决定的公共曲面上的投影，从而揭示输入在时间和上下文中的非线性依赖；可提升计算速度并提供扩展方向，且对语言与时间序列中的语义理解提供一种不同于自注意力的新视角。


<details>
  <summary>Details</summary>
Motivation: 旨在解决“查询-键-值”模型在传统信号处理中的不易对齐之处，寻求一个与数学信号处理更一致的几何/代数表述，并揭示时间演化的上下文依赖性。

Method: 将 SDPA 重新表述为对输入向量在一个由输入自身确定的曲面上的投影，揭示非线性、时间相关的依赖，并将输入 token 的嵌入通过局部上下文曲面进行修改；将该表述与自注意力区分开来，并讨论对时间序列的适用性。

Result: 从数学等价性出发，理论上实现了对前馈和学习过程的速度提升，并提出潜在的扩展；在语言任务中，将 SDPA 视为时间相关的情境意义的寻找，而非简单的自注意力机制。

Conclusion: SDPA 的几何重述揭示了输入之间的非线性、时变关系，提供了新的时间序列建模视角及潜在的算法加速和扩展路径，但需结合具体任务验证其实际收益并评估与传统自注意力的关系与适用边界。

Abstract: Scaled dot-product attention (SDPA) is a fundamental component responsible for the success of large-language models and other nonlinear signal processing applications. The rationale for SDPA has been based upon "query, key, value" concepts borrowed from database theory, but these concepts are difficult to reconcile with standard methods in mathematical signal processing. We show that SDPA can be rewritten in a different but mathematically equivalent form as a projection of the input vectors onto a common surface determined by the inputs themselves. Therefore SDPA discovers nonlinear dependencies in the input that are time-dependent and context-dependent. The rewritten form of SDPA permits increased speed of both feedforward and learning algorithms, but more importantly suggests potential extensions. In the context of language, we re-interpret the role of SDPA as finding a time-dependent contextual meaning determined by the surface on which the set of input vectors lies. Input token embeddings are then modified by the local context surface. This interpretation differs substantially from the concept of "self-attention", and provides a strong justification for the use of SDPA for time-series data with time-varying local nonlinear dependencies.

</details>


### [117] [IMU-1: Sample-Efficient Pre-training of Small Language Models](https://arxiv.org/abs/2602.02522)
*George Grigorev*

Main category: cs.LG

TL;DR: IMU-1 是一个430M参数的语言模型，在72B个token上训练，数据效率显著；通过一系列架构和优化改进实现接近在56倍更大数据规模下训练的模型的基准性能，并公开重现所需的代码、权重和数据。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型中的数据需求问题，评估多组件协同是否能在更少数据上实现高性能。

Method: 将QK-norm注意力、每头门控、值残差、LayerNorm缩放等架构改进，与NorMuon（谨慎权重衰减）优化、muP参数化，以及三阶段训练计划和后验检查点EMA结合；对每个组件进行消融实验并提供开源实现与数据。

Result: 在给定基准上接近使用56x更多数据训练的模型的基准性能，且对各组件进行了消融分析，提供代码、权重和数据以实现复现。

Conclusion: 表明通过整合架构与优化策略，数据效率高的LLM是可行的，并且公开资源有助于复现和比较；然而仍需查看具体基准、数据分布和实验细节以评估普适性。

Abstract: We present IMU-1, a 430M-parameter language model trained on 72B tokens that approaches the benchmark performance of models trained on 56x more data. We describe a validated training recipe combining recent architectural interventions (QK-norm attention, per-head gating, value residuals, LayerNorm scaling) with optimization advances (NorMuon with cautious weight decay, muP parametrization) and a three-stage training schedule with post-hoc checkpoint EMA. We provide ablations for each component and release code, weights and data to enable reproduction: https://huggingface.co/thepowerfuldeez/imu1_base

</details>


### [118] [The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI](https://arxiv.org/abs/2602.02526)
*Pengyue Hou*

Main category: cs.LG

TL;DR: 在上下文稳定化条件下，PPL 指标易产生误导；出现“语义穿越（Semantic Tunneling）”导致语义多样性崩塌；通过 MNCIS 框架中的 ASNC 实现“流形展开”（Manifold Unfolding），将有效秩从 3.62 提升至 5.35，从而维持训练数据的长尾分布并抵抗语义吸引子。


<details>
  <summary>Details</summary>
Motivation: 质疑在上下文稳定化设置下，Perplexity（PPL）作为稳定性指标的可靠性，提出基于多尺度负耦合信息系统（MNCIS）及自适应谱负耦合（ASNC）的拓扑干预以保持语义多样性与流形结构。

Method: 采用滑动窗口协议（N=1500）对生成过程进行监控，评估 PPL 与隐含世界的全局有效秩（GER），发现“语义穿越”导致从 3.62 降至 2.22 的流形崩溃；在 Hou（2026）提出的 MNCIS 框架内应用 ASNC 作为拓扑算子，执行“流形展开”以提升 GER，维持长尾分布。

Result: Baseline 在 PPL 约为 83.9 时语法流畅，但语义多样性在七代内收敛到单一低熵叙事吸引子“Robert Boulton”奇点，表现为潜在流形的全局有效秩从 3.62 降至 2.22。引入 ASNC 的 MNCIS 将有效秩提升至 5.35，构建“人工流形”，抵抗语义吸引子，保持训练数据的长尾分布。

Conclusion: PPL 在上下文稳定化场景下并非可靠的语义多样性指标，应通过拓扑与信息系统层面的干预来实现流形展开与多样性保留。MNCIS/ASNC 提供了一种有效的机制来防止吸引子崩溃并提升模型对长期分布的适应性。

Abstract: The stability of generative artificial intelligence trained on recursive synthetic data is conventionally monitored via Perplexity (PPL). We demonstrate that PPL is a deceptive metric in context-stabilized regimes (L=128). Using a rigorous sliding-window protocol (N=1500), we identify a novel failure mode termed "Semantic Tunneling." While the Baseline model maintains high grammatical fluency (PPL approx. 83.9), it suffers a catastrophic loss of semantic diversity, converging within seven generations to a single, low-entropy narrative attractor: the "Robert Boulton" Singularity. This phenomenon represents a total collapse of the latent manifold (Global Effective Rank 3.62 -> 2.22), where the model discards diverse world knowledge to optimize for statistically safe syntactic templates. To address this, we apply the Multi-Scale Negative Coupled Information Systems (MNCIS) framework recently established in Hou (2026) [arXiv:2601.11594]. We demonstrate that Adaptive Spectral Negative Coupling (ASNC) acts as a topological operator that actively induces "Manifold Unfolding." MNCIS forces the model to expand its effective rank from the anisotropic baseline of 3.62 to a hyper-diverse state of 5.35, effectively constructing an "Artificial Manifold" that resists the gravitational pull of semantic attractors and preserves the long-tail distribution of the training data.

</details>


### [119] [Hypersonic Flow Control: Generalized Deep Reinforcement Learning for Hypersonic Intake Unstart Control under Uncertainty](https://arxiv.org/abs/2602.02531)
*Trishit Mondal,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: DRL-based active flow control stabilizes a Mach 5 hypersonic inlet with high-fidelity CFD, generalizes to unseen conditions, and remains robust to noisy measurements using a minimal sensor set.


<details>
  <summary>Details</summary>
Motivation: Hypersonic inlet unstart caused by strong shock-boundary-layer interactions and pressure fluctuations at Mach 5+; reliable, real-time control under operational uncertainties is needed for robust air-breathing propulsion.

Method: Train and deploy a deep reinforcement learning controller using a high-fidelity CFD solver with adaptive mesh refinement to simulate a 2D Mach-5 inlet. The policy learns to modulate flow control to prevent unstart, demonstrates robustness to measurement noise, and generalizes across back-pressures, Reynolds numbers, and sensor configurations, including with a minimal sensor set.

Result: The DRL controller stabilizes the inlet over a wide back-pressure range, generalizes to unseen back-pressures, Reynolds numbers, and sensor configurations, and remains robust with noisy measurements. A small, optimally selected sensor subset yields comparable performance, enabling practical deployment.

Conclusion: A data-driven approach can achieve real-time hypersonic flow control under realistic uncertainties, with strong generalization and robustness properties suitable for practical application.

Abstract: The hypersonic unstart phenomenon poses a major challenge to reliable air-breathing propulsion at Mach 5 and above, where strong shock-boundary-layer interactions and rapid pressure fluctuations can destabilize inlet operation. Here, we demonstrate a deep reinforcement learning (DRL)- based active flow control strategy to control unstart in a canonical two-dimensional hypersonic inlet at Mach 5 and Reynolds number $5\times 10^6$. The in-house CFD solver enables high-fidelity simulations with adaptive mesh refinement, resolving key flow features, including shock motion, boundary-layer dynamics, and flow separation, that are essential for learning physically consistent control policies suitable for real-time deployment. The DRL controller robustly stabilizes the inlet over a wide range of back pressures representative of varying combustion chamber conditions. It further generalizes to previously unseen scenarios, including different back-pressure levels, Reynolds numbers, and sensor configurations, while operating with noisy measurements, thereby demonstrating strong zero-shot generalization. Control remains robust in the presence of noisy sensor measurements, and a minimal, optimally selected sensor set achieves comparable performance, enabling practical implementation. These results establish a data-driven approach for real-time hypersonic flow control under realistic operational uncertainties.

</details>


### [120] [CADENT: Gated Hybrid Distillation for Sample-Efficient Transfer in Reinforcement Learning](https://arxiv.org/abs/2602.02532)
*Mahyar Alinejad,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: CADENT 将策略性自动机知识与策略级教师知识结合，通过基于经验的信任机制在状态-动作层面动态加权教师引导，实现跨域鲁棒转移，提升采样效率与最终性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中存在源域与目标域之间的分布差异，导致高样本成本与跨域适应困难。现有的策略蒸馏能提供战术指引，但难以传递长期策略；而基于自动机的方法虽能刻画任务结构，却缺乏细粒度的动作指导。需要一个能够融合两者并自适应目标域的框架。

Method: 提出 CADENT 框架，结合上下文感知的蒸馏与经验门控信任机制；在状态-动作层面对教师信号的权重进行动态调整，将策略性自动机知识与策略层面知识融为统一的引导信号；通过经验门控在教师引导与学生经验之间进行平衡，以实现对目标域细节的平滑适应。

Result: 在稀疏奖励的网格世界到连续控制任务等挑战性环境中，CADENT 相比基线获得 40-60% 的样本效率提升，并保持更优的渐近性能。

Conclusion: CADENT 提供了一种稳健的自适应知识转移方法，将策略性与战术性知识统一为可操作的引导信号，并通过经验门控实现对目标域的灵活适应。

Abstract: Transfer learning promises to reduce the high sample complexity of deep reinforcement learning (RL), yet existing methods struggle with domain shift between source and target environments. Policy distillation provides powerful tactical guidance but fails to transfer long-term strategic knowledge, while automaton-based methods capture task structure but lack fine-grained action guidance. This paper introduces Context-Aware Distillation with Experience-gated Transfer (CADENT), a framework that unifies strategic automaton-based knowledge with tactical policy-level knowledge into a coherent guidance signal. CADENT's key innovation is an experience-gated trust mechanism that dynamically weighs teacher guidance against the student's own experience at the state-action level, enabling graceful adaptation to target domain specifics. Across challenging environments, from sparse-reward grid worlds to continuous control tasks, CADENT achieves 40-60\% better sample efficiency than baselines while maintaining superior asymptotic performance, establishing a robust approach for adaptive knowledge transfer in RL.

</details>


### [121] [From Sparse Decisions to Dense Reasoning: A Multi-attribute Trajectory Paradigm for Multimodal Moderation](https://arxiv.org/abs/2602.02536)
*Tianle Gu,Kexin Huang,Lingyu Li,Ruilin Luo,Shiyang Huang,Zongqi Wang,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: 提出 UniMod 的多模态安全审核框架，通过从稀疏决策向密集推理轨迹转变，建立证据 grounding、模态评估、风险映射、策略决策、响应生成的多维边界学习；引入多头标量奖励模型 UniRM 提供属性级监督，并通过解耦任务参数、重平衡训练动态的优化策略解决多任务干扰，在较少数据下实现有竞争力的文本审核并在多模态上设立新基准；对多属性轨迹推理的消融验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态安全审核在数据与监督稀缺下的短路学习问题，二元标签难以覆盖安全边界，迫切需要在推理层面提供显式语义 grounding。

Method: 提出 UniMod，将单一决策任务分解为证据 grounding、模态评估、风险映射、策略决策、响应生成等轨迹，形成多维边界学习；引入 UniRM，给响应生成阶段的属性级分数作为多维监督；提出解耦任务参数、重平衡训练动态的优化策略以降低多任务干扰。

Result: 在文本审核任务上具备有竞争力的性能；在多模态场景上以占比低于 40% 的训练数据取得新的基准；消融实验证实多属性轨迹推理的有效性。

Conclusion: 该框架为多模态安全审核提供了高效且有效的密集推理框架，降低数据需求并有助于防止短路学习，但需关注实现成本、对高质量显式 grounding 的依赖等潜在限制。

Abstract: Safety moderation is pivotal for identifying harmful content. Despite the success of textual safety moderation, its multimodal counterparts remain hindered by a dual sparsity of data and supervision. Conventional reliance on binary labels lead to shortcut learning, which obscures the intrinsic classification boundaries necessary for effective multimodal discrimination. Hence, we propose a novel learning paradigm (UniMod) that transitions from sparse decision-making to dense reasoning traces. By constructing structured trajectories encompassing evidence grounding, modality assessment, risk mapping, policy decision, and response generation, we reformulate monolithic decision tasks into a multi-dimensional boundary learning process. This approach forces the model to ground its decision in explicit safety semantics, preventing the model from converging on superficial shortcuts. To facilitate this paradigm, we develop a multi-head scalar reward model (UniRM). UniRM provides multi-dimensional supervision by assigning attribute-level scores to the response generation stage. Furthermore, we introduce specialized optimization strategies to decouple task-specific parameters and rebalance training dynamics, effectively resolving interference between diverse objectives in multi-task learning. Empirical results show UniMod achieves competitive textual moderation performance and sets a new multimodal benchmark using less than 40\% of the training data used by leading baselines. Ablations further validate our multi-attribute trajectory reasoning, offering an effective and efficient framework for multimodal moderation. Supplementary materials are available at \href{https://trustworthylab.github.io/UniMod/}{project website}.

</details>


### [122] [Enhancing Post-Training Quantization via Future Activation Awareness](https://arxiv.org/abs/2602.02538)
*Zheqi Lv,Zhenxuan Fan,Qi Tian,Wenqiao Zhang,Yueting Zhuang*

Main category: cs.LG

TL;DR: 提出未来感知量化（FAQ）用于前训练后量化（PTQ）的LLM。通过利用未来层激活和窗口预览机制引导标度因子，减小量化偏差和误差累积，且采用事先搜索的配置以降低开销。实验显示在边部署场景下无需反向传播、数据重构或调参即可获得优越的表现。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法通常基于当前层激活确定量化超参数，易带来量化偏差和误差累积，且对校准数据偏置敏感，影响稳定性和性能，尤其在边缘设备场景。需要提高鲁棒性和稳定性，同时降低额外开销。

Method: 提出未来感知量化（FAQ），利用未来层的激活信息来引导当前层及权重的量化，提升重要权重的保持与近似质量。引入窗口式预览机制，对若干未来层激活进行软聚合，降低对单一层的过度依赖。为避免昂贵的贪心搜索，采用预先搜索得到的配置以最小化开销。

Result: 实验显示FAQ在与现有方法的对比中表现一致更优，且额外成本微不足道，且不需要反向传播、数据重构或调参。适合在边缘部署场景使用。

Conclusion: 未来感知量化通过利用未来层信息和窗口式预览，提升PTQ对偏置与噪声的鲁棒性，同时降低搜索与实现成本，使得在无标注数据、无反向传播条件下的LLM量化更可靠，适合边缘设备部署。

Abstract: Post-training quantization (PTQ) is a widely used method to compress large language models (LLMs) without fine-tuning. It typically sets quantization hyperparameters (e.g., scaling factors) based on current-layer activations. Although this method is efficient, it suffers from quantization bias and error accumulation, resulting in suboptimal and unstable quantization, especially when the calibration data is biased. To overcome these issues, we propose Future-Aware Quantization (FAQ), which leverages future-layer activations to guide quantization. This allows better identification and preservation of important weights, while reducing sensitivity to calibration noise. We further introduce a window-wise preview mechanism to softly aggregate multiple future-layer activations, mitigating over-reliance on any single layer. To avoid expensive greedy search, we use a pre-searched configuration to minimize overhead. Experiments show that FAQ consistently outperforms prior methods with negligible extra cost, requiring no backward passes, data reconstruction, or tuning, making it well-suited for edge deployment.

</details>


### [123] [How Much Information Can a Vision Token Hold? A Scaling Law for Recognition Limits in VLMs](https://arxiv.org/abs/2602.02539)
*Shuxin Zhuang,Zi Liang,Runsheng Yu,Hongzong Li,Rong Feng,Shiqin Tang,Youzhi Zhang*

Main category: cs.LG

TL;DR: 视觉编码器作为有损信道的容量上限在长上下文中呈现相变：稳定、非稳定到完全崩溃的三阶段；提出概率缩放定律，将平均视觉令牌负载与视觉密度统一为一个潜在难度度量，并在多模型上验证其普适性，以指导视觉上下文压缩的效率-准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 揭示视觉令牌的理论容量界限与压缩效果的极限行为。将视觉编码视为有限容量的有损通道，探究信息量增加时模型识别性能的极限与稳定性。

Method: 通过在图像中逐步增加信息量（字符数量）进行受控压力测试；观察并分析三阶段相变及其机械成因；提出并推导将平均视觉令牌负载与视觉密度映射到一个潜在难度的概率缩放定律；在多种 Vision-Language 模型上进行大量实验以验证定律的普适性。

Result: 发现三种阶段：近似完美的稳定阶段、错误方差增大的不稳定阶段、以及最终的总体崩溃阶段。给出导致相变的关键因素并提出统一的缩放定律，该定律在不同模型间具有普适性，能为设计中视觉上下文压缩的效率-准确性权衡提供实证性参考。

Conclusion: 将视觉编码的容量上限与信息密度联系起来，提供一个统一的难度指标，有助于优化长上下文视觉模型的压缩策略与资源分配。

Abstract: Recent vision-centric approaches have made significant strides in long-context modeling. Represented by DeepSeek-OCR, these models encode rendered text into continuous vision tokens, achieving high compression rates without sacrificing recognition precision. However, viewing the vision encoder as a lossy channel with finite representational capacity raises a fundamental question: what is the information upper bound of visual tokens? To investigate this limit, we conduct controlled stress tests by progressively increasing the information quantity (character count) within an image. We observe a distinct phase-transition phenomenon characterized by three regimes: a near-perfect Stable Phase, an Instability Phase marked by increased error variance, and a total Collapse Phase. We analyze the mechanical origins of these transitions and identify key factors. Furthermore, we formulate a probabilistic scaling law that unifies average vision token load and visual density into a latent difficulty metric. Extensive experiments across various Vision-Language Models demonstrate the universality of this scaling law, providing critical empirical guidance for optimizing the efficiency-accuracy trade-off in visual context compression.

</details>


### [124] [Toward Ultra-Long-Horizon Sequential Model Editing](https://arxiv.org/abs/2602.02543)
*Mingda Liu,Zhenghan Zhu,Ze'an Miao,Katsuki Fujisawa*

Main category: cs.LG

TL;DR: 提出 Norm-Anchor Scaling NAS，作为一种可插拔的正则约束策略，针对逐步编辑中权值范数的爆炸性增长问题，显著缓解模型折叠并提升编辑效果。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型中逐步编辑导致的折叠问题及权重范数指数增长的风险，理论证实常用编辑更新在没有范数约束时会累积放大；提高 L&E 的稳定性和效果。

Method: 引入 Norm-Anchor Scaling NAS，一种以范数约束为核心的可插拔策略，针对现有 L&E 的更新规则进行夹持/锚定设计，使权重在编辑过程中的范数受控，降低崩溃门槛。

Result: 在广泛实验中，NAS 将代表性 L&E 算法的崩溃点延缓超过 4 倍，并实现编辑性能的平均相对提升约 72.2%，仅需要额外一行代码且计算开销极小。

Conclusion: 正则约束型的 Norm-Anchor 方案对逐步编辑的稳定性与效果具有显著提升，具备良好的实用性和可移植性，适合作为 L&E 的通用插件。

Abstract: Model editing has emerged as a practical approach for mitigating factual errors and outdated knowledge in large language models (LLMs). Among existing methods, the Locate-and-Edit (L&E) paradigm is the dominant framework: it locates MLP parameters implicated in expressing a target fact, and then performs a localized update to rewrite that fact. However, long sequences of edits often trigger abrupt model collapse in L&E beyond a critical point. We empirically identify a strong correlation between collapse and explosive growth of edited MLP weight norms, and formally prove that commonly used L&E update rules can induce exponential norm growth across sequential edits in the absence of explicit norm control. To address this issue, we propose Norm-Anchor Scaling NAS, a plug-and-play norm-constrained strategy. Across extensive experiments, NAS delays the collapse point of representative L&E algorithms by more than 4 times and yields a 72.2% average relative gain in editing performance, requiring only a single additional line of code and incurring negligible computational overhead.

</details>


### [125] [SPA-Cache: Singular Proxies for Adaptive Caching in Diffusion Language Models](https://arxiv.org/abs/2602.02544)
*Wenhao Sun,Rong-Cheng Tu,Yifu Ding,Zhao Jin,Jingyi Liao,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: SPA-Cache jointly optimizes update identification and budget allocation in diffusion language model caches, using a low-dimensional singular proxy to identify update-critical tokens and an adaptive strategy to allocate updates across layers; achieves significant speedups.


<details>
  <summary>Details</summary>
Motivation: Diffusion Language Models (DLMs) are non-causal and cannot use standard KV caching efficiently, leading to costly hidden-state recomputation. Existing caching methods rely on heuristic token updates and uniform budgets, which are inefficient given heterogeneous hidden-state dynamics.

Method: 1) Develop a low-dimensional singular proxy to identify update-critical tokens in a reduced subspace, lowering the overhead of update identification. 2) Propose an adaptive update-budget allocation strategy that reduces updates for stable layers while preserving generation quality.

Result: DLM cache efficiency improves substantially, with up to 8× throughput improvement over vanilla decoding and 2–4× speedups over existing caching baselines.

Conclusion: Jointly optimizing update identification and budget allocation in DLM caching, via a compact proxy and adaptive budgeting, yields large efficiency gains without degrading output quality.

Abstract: While Diffusion Language Models (DLMs) offer a flexible, arbitrary-order alternative to the autoregressive paradigm, their non-causal nature precludes standard KV caching, forcing costly hidden state recomputation at every decoding step. Existing DLM caching approaches reduce this cost by selective hidden state updates; however, they are still limited by (i) costly token-wise update identification heuristics and (ii) rigid, uniform budget allocation that fails to account for heterogeneous hidden state dynamics. To address these challenges, we present SPA-Cache that jointly optimizes update identification and budget allocation in DLM cache. First, we derive a low-dimensional singular proxy that enables the identification of update-critical tokens in a low-dimensional subspace, substantially reducing the overhead of update identification. Second, we introduce an adaptive strategy that allocates fewer updates to stable layers without degrading generation quality. Together, these contributions significantly improve the efficiency of DLMs, yielding up to an $8\times$ throughput improvement over vanilla decoding and a $2$--$4\times$ speedup over existing caching baselines.

</details>


### [126] [Beyond Alignment: Expanding Reasoning Capacity via Manifold-Reshaping Policy Optimization](https://arxiv.org/abs/2602.02545)
*Dayu Wang,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li*

Main category: cs.LG

TL;DR: MRPO通过几何干预重塑LLM推理空间，以扩展潜在推理能力；在4B模型上实现数学任务的SOTA，显著超越更大模型；代码可获取。


<details>
  <summary>Details</summary>
Motivation: 回应“RL是否真正扩展推理能力”而非仅对潜在能力进行对齐的争论，提出通过几何干预可打破低秩偏置流形的访问边界，实现在推理空间的本质性扩展。

Method: 提出两阶段的几何框架MRPO：1) Spectral Orthogonal Exploration (SOE) 将策略初始化投射到偏置流形的零空间；2) 在策略优化目标中加入有效秩正则化，鼓励发现并维持高维推理轨迹，克服RL中的熵减趋势。

Result: 4B参数的方法在数学任务上达到SOTA，显著优于更大模型（如Qwen3-32B），将能力边界扩展至标准GRPO之上。

Conclusion: 表明通过几何重塑推理空间可以本质性提升LLM的推理能力，MRPO提供了一种可行的机制及证据，代码公开。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). However, recent studies question whether RL genuinely expands reasoning capacity or merely aligns existing latent capabilities, arguing that exploration remains confined within the pre-trained model's low-rank bias manifold. In this work, we challenge this accessibility boundary hypothesis by demonstrating that the latent reasoning space can be fundamentally expanded through targeted geometric interventions. We propose Manifold-Reshaping Policy Optimization (MRPO), a geometric framework designed to fundamentally restructure the inference space of LLMs. MRPO operates in two stages: first, we employ Spectral Orthogonal Exploration (SOE) to eject the policy initialization into the null space of the bias manifold; second, we integrate an Effective Rank regularization term into the policy optimization objective. This approach incentivizes the discovery and maintenance of high-dimensional reasoning trajectories against the entropy-reducing tendency of standard RL. Empirically, our 4B-parameter method achieves state-of-the-art performance on mathematical tasks, significantly outperforming larger models (e.g., Qwen3-32B) and expanding the capability boundary beyond standard GRPO. Our code is available at https://anonymous.4open.science/r/MRPO-D57B/

</details>


### [127] [D$^2$Quant: Accurate Low-bit Post-Training Weight Quantization for LLMs](https://arxiv.org/abs/2602.02546)
*Xianglong Yan,ChengZhu Bao,Zhiteng Li,Tianao Zhang,Shaoqiu Zhang,Ruobing Xie,Samm Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: D^2Quant extends weight-only post-training quantization for LLMs by addressing both weight and activation errors: a Dual-Scale Quantizer (DSQ) for down-projection weights with absorbable scaling, and a Deviation-Aware Correction (DAC) that shifts activations via LayerNorm mean adjustment, achieving superior sub-4-bit performance without extra bit-width.


<details>
  <summary>Details</summary>
Motivation: Weight-only PTQ is attractive for LLM deployment due to memory and compute savings, but sub-4-bit accuracy degrades mainly because down-projection matrices suffer from quantization bottlenecks and activations shift during quantization. A method that fixes both weight fidelity and activation distribution can enable practical deployment.

Method: Introduce Dual-Scale Quantizer (DSQ) tailored for down-projection matrices with an absorbable scaling factor that preserves fidelity without increasing the bit budget. Introduce Deviation-Aware Correction (DAC) by incorporating a mean-shift correction within LayerNorm to compensate for activation distribution shifts caused by quantization.

Result: Extensive experiments across multiple LLM families and evaluation metrics show that D^2Quant achieves superior performance for weight-only PTQ at sub-4-bit precision compared to baselines.

Conclusion: D^2Quant effectively improves weight-only PTQ by addressing both weight and activation perspectives, enabling better sub-4-bit quantization of LLMs; code and models will be released for reproducibility.

Abstract: Large language models (LLMs) deliver strong performance, but their high compute and memory costs make deployment difficult in resource-constrained scenarios. Weight-only post-training quantization (PTQ) is appealing, as it reduces memory usage and enables practical speedup without low-bit operators or specialized hardware. However, accuracy often degrades significantly in weight-only PTQ at sub-4-bit precision, and our analysis identifies two main causes: (1) down-projection matrices are a well-known quantization bottleneck, but maintaining their fidelity often requires extra bit-width; (2) weight quantization induces activation deviations, but effective correction strategies remain underexplored. To address these issues, we propose D$^2$Quant, a novel weight-only PTQ framework that improves quantization from both the weight and activation perspectives. On the weight side, we design a Dual-Scale Quantizer (DSQ) tailored to down-projection matrices, with an absorbable scaling factor that significantly improves accuracy without increasing the bit budget. On the activation side, we propose Deviation-Aware Correction (DAC), which incorporates a mean-shift correction within LayerNorm to mitigate quantization-induced activation distribution shifts. Extensive experiments across multiple LLM families and evaluation metrics show that D$^2$Quant delivers superior performance for weight-only PTQ at sub-4-bit precision. The code and models will be available at https://github.com/XIANGLONGYAN/D2Quant.

</details>


### [128] [naPINN: Noise-Adaptive Physics-Informed Neural Networks for Recovering Physics from Corrupted Measurement](https://arxiv.org/abs/2602.02547)
*Hankyeol Kim,Pilsung Kang*

Main category: cs.LG

TL;DR: 提出了一种噪声自适应的 PINN (naPINN)，通过能量基模型学习残差分布并用可训练的可靠性门控过滤高能量数据，同时加入拒绝代价正则化，以在严重噪声和离群数据下鲁棒地求解偏微分方程（PDE）与发现动力学。


<details>
  <summary>Details</summary>
Motivation: 解决传统 PINN 在复杂测量噪声和大规模离群数据下性能显著下降的问题，且不依赖噪声分布先验信息。

Method: 在训练中嵌入一个能量基模型以捕捉预测残差的潜在分布；基于学得的能量景观，使用一个可训练的可靠性门对高能量数据进行自适应过滤；引入拒绝成本正则化防止过度剔除有效数据。

Result: 在多种基准偏微分方程（PDE）上，对非高斯噪声和不同离群率具有鲁棒性，显著优于现有鲁棒 PINN 基线，能有效隔离离群并在严重数据损坏下重构动力学。

Conclusion: naPINN 提供在强噪声条件下鲁棒求解 PDE 的能力，无需先验噪声分布，能通过能量学习与自适应数据筛选提高稳定性和准确性。

Abstract: Physics-Informed Neural Networks (PINNs) are effective methods for solving inverse problems and discovering governing equations from observational data. However, their performance degrades significantly under complex measurement noise and gross outliers. To address this issue, we propose the Noise-Adaptive Physics-Informed Neural Network (naPINN), which robustly recovers physical solutions from corrupted measurements without prior knowledge of the noise distribution. naPINN embeds an energy-based model into the training loop to learn the latent distribution of prediction residuals. Leveraging the learned energy landscape, a trainable reliability gate adaptively filters data points exhibiting high energy, while a rejection cost regularization prevents trivial solutions where valid data are discarded. We demonstrate the efficacy of naPINN on various benchmark partial differential equations corrupted by non-Gaussian noise and varying rates of outliers. The results show that naPINN significantly outperforms existing robust PINN baselines, successfully isolating outliers and accurately reconstructing the dynamics under severe data corruption.

</details>


### [129] [HyPAC: Cost-Efficient LLMs-Human Hybrid Annotation with PAC Error Guarantees](https://arxiv.org/abs/2602.02550)
*Hao Zeng,Huipeng Huang,Xinhao Qu,Jianguo Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出HyPAC，通过三阶段路由在多源标注中实现成本最小化，同时对标注错误给出分布无关的PAC保证。


<details>
  <summary>Details</summary>
Motivation: 数据标注常涉及多源且成本与质量权衡，需在保持误差可控的前提下降低总体标注成本。

Method: 基于重要性采样和上置信界，设计两个阈值校准策略，将输入分成三区域并分别路由到最快/最便宜、较慢但更准以及高质量来源；具备分布无关的PAC误差保证。

Result: 理论上达到在期望成本最小化的同时对标注误差提供PAC保证；实验在常用基准上实现78.51%成本下降，并约束标注误差。

Conclusion: HyPAC在无需依赖数据分布和预训练模型的前提下，提供成本有效的标注源路由与严格误差控制，具有良好的实际适用性，但需对阈值、区域划分和成本定义进行充分诊断。

Abstract: Data annotation often involves multiple sources with different cost-quality trade-offs, such as fast large language models (LLMs), slow reasoning models, and human experts. In this work, we study the problem of routing inputs to the most cost-efficient annotation source while controlling the labeling error on test instances. We propose \textbf{HyPAC}, a method that adaptively labels inputs to the most cost-efficient annotation source while providing distribution-free guarantees on annotation error. HyPAC calibrates two decision thresholds using importance sampling and upper confidence bounds, partitioning inputs into three regions based on uncertainty and routing each to the appropriate annotation source. We prove that HyPAC achieves the minimum expected cost with a probably approximately correct (PAC) guarantee on the annotation error, free of data distribution and pre-trained models. Experiments on common benchmarks demonstrate the effectiveness of our method, reducing the annotation cost by 78.51\% while tightly controlling the annotation error.

</details>


### [130] [BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation](https://arxiv.org/abs/2602.02554)
*Jingwen Xu,Yiyang Lu,Zisu Huang,Changze Lv,Xiaohua Wang,Shizheng Li,Zhibo Xu,Zhengkang Guo,Zhengyuan Wang,Muzhao Tian,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.LG

TL;DR: BatCoder提出自监督强化学习框架，通过代码-文档的回译循环，在仅使用代码的前提下提升代码生成和文档编写能力；在7B模型上对HumanEval和MBPP取得显著成绩，且随着数据量和模型规模增加而提升。


<details>
  <summary>Details</summary>
Motivation: 现有高质量代码-文档对获取成本高且对小众语言稀缺，需要一种仅基于代码的自监督方法来同时优化代码生成与文档生成。

Method: 采用回译(back-translation)策略：先从代码生成文档，再用生成的文档重构原始代码。将原代码与重构代码之间的语义相似度作为隐式奖励，结合强化学习训练，使模型同时提升代码→文档与文档→代码的生成能力。

Result: 在7B模型下，分别在HumanEval和MBPP上获得83.5%和81.0%的pass@1，超过强基线；并且结果呈现对训练语料规模和模型容量的持续可扩展性。

Conclusion: 该框架证明了仅凭代码也可有效地训练出高性能的代码-文档生成模型，且具有良好的扩展性，适用于缺乏成对代码-文档数据的场景。

Abstract: Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.

</details>


### [131] [Beyond Experience Retrieval: Learning to Generate Utility-Optimized Structured Experience for Frozen LLMs](https://arxiv.org/abs/2602.02556)
*Xuancheng Li,Haitao Li,Yujia Zhou,Yiqun Liu,Qingyao Ai*

Main category: cs.LG

TL;DR: SEAM是一种轻量级的结构化经验适配器模块，通过在参数中存储经验并在单前向传播中生成定制化的实例-entry来指导冻结的LLM执行器，从而提升数学推理等任务的准确性，同时保持低开销。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在静态性与重复推理方面的局限性，及外部检索所引入的噪声与延迟；在不修改主模型权重的前提下，以插件方式提供结构化、实例级的经验以提升推理效果。

Method: 提出SEAM，面向执行器的插件；在执行器参数中存储经验，并在一次前向传播中生成结构化的、按实例定制的经验条目来引导冻结的LLM执行器。通过执行器回合测试（executor rollouts）和GRPO进行训练，保持执行器冻结；上线后可通过对成功轨迹的有监督微调进一步提升。

Result: 在数学推理基准上对多种执行器显示稳定的准确性提升，开销低；进行了广泛的消融和分析以揭示SEAM的有效机制与鲁棒性。

Conclusion: SEAM提供了一个轻量、可插拔的结构化经验注入方案，能够在不重新训练主模型的情况下提升推理性能，并且具备上线后继续改进的潜力。

Abstract: Large language models (LLMs) are largely static and often redo reasoning or repeat mistakes. Prior experience reuse typically relies on external retrieval, which is similarity-based, can introduce noise, and adds latency. We introduce SEAM (Structured Experience Adapter Module), a lightweight, executor-specific plug-in that stores experience in its parameters and generates a structured, instance-tailored experience entry in a single forward pass to guide a frozen LLM executor. SEAM is trained for utility via executor rollouts and GRPO while keeping the executor frozen, and it can be further improved after deployment with supervised fine-tuning on logged successful trajectories. Experiments on mathematical reasoning benchmarks show consistent accuracy gains across executors with low overhead. Extensive ablations and analyses further elucidate the mechanisms underlying SEAM's effectiveness and robustness.

</details>


### [132] [The Alignment Curse: Cross-Modality Jailbreak Transfer in Omni-Models](https://arxiv.org/abs/2602.02557)
*Yupeng Chen,Junchi Yu,Aoxi Liu,Philip Torr,Adel Bibi*

Main category: cs.LG

TL;DR: 提出文本到音频的跨模态越狱传递，揭示对齐（alignment）越强越易将文本漏洞扩散到音频模态；并给出一种简单但强大的基线：文本转译的音频越狱在多家全模态模型上与音频专属越狱表现相当甚至更好，且具备跨模型传递性。


<details>
  <summary>Details</summary>
Motivation: 弥合文本与音频越狱研究的鸿沟，揭示模态对齐可能带来的安全风险，以及在音频安全评估中使用成熟文本越狱方法的可行性。

Method: 对比分析：1) 纯文本越狱、2) 文本转译成音频的越狱、3) 现有音频越狱；在最近的全模态 omni-model 上进行系统评估；研究模态对齐与跨模态越狱传递之间的关系；验证跨模型传递性；在更严格的音频仅访问威胁模型下的有效性。

Result: 文本转译的音频越狱与传统音频越狱在性能上相当甚至优于对方，展现出强的跨模型传递性；对齐越强的情况下，文本漏洞容易被传播到音频模态。

Conclusion: 文本转译的音频越狱提供了简单且有力的基线，提醒需要在跨模态安全评估中纳入对齐效应并推进对 omni-model 的跨模态红队防护研究。

Abstract: Recent advances in end-to-end trained omni-models have significantly improved multimodal understanding. At the same time, safety red-teaming has expanded beyond text to encompass audio-based jailbreak attacks. However, an important bridge between textual and audio jailbreaks remains underexplored. In this work, we study the cross-modality transfer of jailbreak attacks from text to audio, motivated by the semantic similarity between the two modalities and the maturity of textual jailbreak methods. We first analyze the connection between modality alignment and cross-modality jailbreak transfer, showing that strong alignment can inadvertently propagate textual vulnerabilities to the audio modality, which we term the alignment curse. Guided by this analysis, we conduct an empirical evaluation of textual jailbreaks, text-transferred audio jailbreaks, and existing audio-based jailbreaks on recent omni-models. Our results show that text-transferred audio jailbreaks perform comparably to, and often better than, audio-based jailbreaks, establishing them as simple yet powerful baselines for future audio red-teaming. We further demonstrate strong cross-model transferability and show that text-transferred audio attacks remain effective even under a stricter audio-only access threat model.

</details>


### [133] [PA-MIL: Phenotype-Aware Multiple Instance Learning Guided by Language Prompting and Genotype-to-Phenotype Relationships](https://arxiv.org/abs/2602.02558)
*Zekang Yang,Hong Liu,Xiangdong Wang*

Main category: cs.LG

TL;DR: 提出 PA-MIL，一种 ante-hoc 解释性 MIL 框架，通过表型知识库、语言提示和 GP-NN 实现病理全切片的表型感知特征学习与癌症亚型分类，并提升解释性与可追责性。


<details>
  <summary>Details</summary>
Motivation: 克服现有 MIL 仅在预测后进行解释的问题，提供更可靠、可追溯的解释，以表型和基因型关系为支撑。

Method: 构建 phenotype knowledge base、以表型形态描述充当语言提示以聚合特征、设计 GP-NN 以基因型与表型关系为支撑的多层次引导，形成 PA-MIL。

Result: 在多个数据集上与现有 MIL 方法并驾齐驱，同时提升解释性；通过 phenotype saliency 作证据，线性分类器获得有竞争力的性能；并对基因型-表型关系、队列级和病例级可解释性进行系统分析，显示可靠性与可追责性。

Conclusion: PA-MIL 展示出可解释性和可靠性，适用于癌症亚型识别，提供更为可追溯的病理分析框架。

Abstract: Deep learning has been extensively researched in the analysis of pathology whole-slide images (WSIs). However, most existing methods are limited to providing prediction interpretability by locating the model's salient areas in a post-hoc manner, failing to offer more reliable and accountable explanations. In this work, we propose Phenotype-Aware Multiple Instance Learning (PA-MIL), a novel ante-hoc interpretable framework that identifies cancer-related phenotypes from WSIs and utilizes them for cancer subtyping. To facilitate PA-MIL in learning phenotype-aware features, we 1) construct a phenotype knowledge base containing cancer-related phenotypes and their associated genotypes. 2) utilize the morphological descriptions of phenotypes as language prompting to aggregate phenotype-related features. 3) devise the Genotype-to-Phenotype Neural Network (GP-NN) grounded in genotype-to-phenotype relationships, which provides multi-level guidance for PA-MIL. Experimental results on multiple datasets demonstrate that PA-MIL achieves competitive performance compared to existing MIL methods while offering improved interpretability. PA-MIL leverages phenotype saliency as evidence and, using a linear classifier, achieves competitive results compared to state-of-the-art methods. Additionally, we thoroughly analyze the genotype-phenotype relationships, as well as cohort-level and case-level interpretability, demonstrating the reliability and accountability of PA-MIL.

</details>


### [134] [Auditing Sybil: Explaining Deep Lung Cancer Risk Prediction Through Generative Interventional Attributions](https://arxiv.org/abs/2602.02560)
*Bartlomiej Sobieski,Jakub Grzywaczewski,Karol Dobiczek,Mateusz Wójcik,Tomasz Bartczak,Patryk Szatkowski,Przemysław Bombiński,Matthew Tivnan,Przemyslaw Biecek*

Main category: cs.LG

TL;DR: 通过S(H)NAP对Sybil进行因果干预审计，利用3D扩散桥建模修改解剖特征以分离对象特异贡献；结果显示模型在部分任务上呈现放射科医生风格的推理，但对伪影的敏感性和径向偏差等安全隐患突出。


<details>
  <summary>Details</summary>
Motivation: 当前评估主要基于相关性指标，忽视模型推理机制的因果验证，临床部署前需确保推理的鲁棒性与可解释性。

Method: 提出模型无关的审计框架S(H)NAP，利用生成性干预归因并由专家放射科医生进行验证；通过真实感的3D扩散桥模型有系统地修改解剖特征，以获得对风险分数的对象特异性因果贡献；给出对Sybil的第一例因果干预审计。

Result: 研究发现：1) Sybil在区分恶性与良性肺部结节方面的行为在一定程度上类似放射科医生；2) 同时存在关键失败模式，如对不恰当伪影高度敏感、存在径向偏差。

Conclusion: 因果干预审计提供了对模型推理的证据化验证，揭示了可部署前需解决的安全隐患与改进方向；S(H)NAP为模型审计提供了通用、模型无关的框架，促进鲁棒性与可信度提升。

Abstract: Lung cancer remains the leading cause of cancer mortality, driving the development of automated screening tools to alleviate radiologist workload. Standing at the frontier of this effort is Sybil, a deep learning model capable of predicting future risk solely from computed tomography (CT) with high precision. However, despite extensive clinical validation, current assessments rely purely on observational metrics. This correlation-based approach overlooks the model's actual reasoning mechanism, necessitating a shift to causal verification to ensure robust decision-making before clinical deployment. We propose S(H)NAP, a model-agnostic auditing framework that constructs generative interventional attributions validated by expert radiologists. By leveraging realistic 3D diffusion bridge modeling to systematically modify anatomical features, our approach isolates object-specific causal contributions to the risk score. Providing the first interventional audit of Sybil, we demonstrate that while the model often exhibits behavior akin to an expert radiologist, differentiating malignant pulmonary nodules from benign ones, it suffers from critical failure modes, including dangerous sensitivity to clinically unjustified artifacts and a distinct radial bias.

</details>


### [135] [A General ReLearner: Empowering Spatiotemporal Prediction by Re-learning Input-label Residual](https://arxiv.org/abs/2602.02563)
*Jiaming Ma,Binwu Wang,Pengkun Wang,Xu Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.LG

TL;DR: 提出一种基于双向学习的时空预测框架 ReLearner，可在现有 STNNs 上通过残差学习与平滑模块实现对输入与标签之间时空特征残差的再学习，显著提升多数据集多骨干模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决前向单向学习在输入与标签存在时空错配时的性能下降问题，尤其当相似时间序列的未来标签不同或相反。通过在训练阶段引入标签特征并引入逆向学习来增强模型对残差的建模能力。

Method: 提出时空残差定理（Spatiotemporal Residual Theorem），将前向预测扩展为双向学习框架；设计通用模块 ReLearner，包含残差学习模块以分离输入与标签表示之间的残差，并且残差平滑模块以促进稳定收敛；通过辅助的逆学习过程让模型重新学习输入数据与未来数据之间的时空特征残差。

Result: 在11个真实数据集、14种骨干模型上进行广泛实验，ReLearner显著提升现有 STNNs 的预测性能；代码开源于 GitHub。

Conclusion: 提出的双向残差学习框架可作为通用模块提升 STNNs 的表现，残差平滑有助于稳定收敛；未来工作可进一步分析逆学习对不同数据特征的影响及潜在计算开销。

Abstract: Prevailing spatiotemporal prediction models typically operate under a forward (unidirectional) learning paradigm, in which models extract spatiotemporal features from historical observation input and map them to target spatiotemporal space for future forecasting (label). However, these models frequently exhibit suboptimal performance when spatiotemporal discrepancies exist between inputs and labels, for instance, when nodes with similar time-series inputs manifest distinct future labels, or vice versa. To address this limitation, we propose explicitly incorporating label features during the training phase. Specifically, we introduce the Spatiotemporal Residual Theorem, which generalizes the conventional unidirectional spatiotemporal prediction paradigm into a bidirectional learning framework. Building upon this theoretical foundation, we design an universal module, termed ReLearner, which seamlessly augments Spatiotemporal Neural Networks (STNNs) with a bidirectional learning capability via an auxiliary inverse learning process. In this process, the model relearns the spatiotemporal feature residuals between input data and future data. The proposed ReLearner comprises two critical components: (1) a Residual Learning Module, designed to effectively disentangle spatiotemporal feature discrepancies between input and label representations; and (2) a Residual Smoothing Module, employed to smooth residual terms and facilitate stable convergence. Extensive experiments conducted on 11 real-world datasets across 14 backbone models demonstrate that ReLearner significantly enhances the predictive performance of existing STNNs.Our code is available on GitHub.

</details>


### [136] [A Comparative Simulation Study of the Fairness and Accuracy of Predictive Policing Systems in Baltimore City](https://arxiv.org/abs/2602.02566)
*Samin Semsar,Kiran Laxmikant Prabhu,Gabriella Waters,James Foulds*

Main category: cs.LG

TL;DR: A city-specific comparative simulation study reveals complex bias in predictive policing vs hot spots policing in Baltimore, with short-term fairness advantages but potential long-term bias amplification.


<details>
  <summary>Details</summary>
Motivation: Address the gap in comparative, city-specific assessments of predictive policing fairness and accuracy and understand long-run bias dynamics, including feedback loops and data bias.

Method: Comprehensive simulation study in Baltimore comparing predictive policing technologies against hot spots policing; assesses fairness, accuracy, and long-run bias, and identifies how biases may shift across neighborhoods.

Result: Bias due to feedback loops; hot spots policing shows similar issues; predictive policing fairer and more accurate in short term but biases amplify faster; in Baltimore bias sometimes over-polices White neighborhoods.

Conclusion: Provides a city-specific evaluation framework and behavioral-tendency comparison that can reveal inequities and long-term patterns in predictive policing systems.

Abstract: There are ongoing discussions about predictive policing systems, such as those deployed in Los Angeles, California and Baltimore, Maryland, being unfair, for example, by exhibiting racial bias. Studies found that unfairness may be due to feedback loops and being trained on historically biased recorded data. However, comparative studies on predictive policing systems are few and are not sufficiently comprehensive. In this work, we perform a comprehensive comparative simulation study on the fairness and accuracy of predictive policing technologies in Baltimore. Our results suggest that the situation around bias in predictive policing is more complex than was previously assumed. While predictive policing exhibited bias due to feedback loops as was previously reported, we found that the traditional alternative, hot spots policing, had similar issues. Predictive policing was found to be more fair and accurate than hot spots policing in the short term, although it amplified bias faster, suggesting the potential for worse long-run behavior. In Baltimore, in some cases the bias in these systems tended toward over-policing in White neighborhoods, unlike in previous studies. Overall, this work demonstrates a methodology for city-specific evaluation and behavioral-tendency comparison of predictive policing systems, showing how such simulations can reveal inequities and long-term tendencies.

</details>


### [137] [IceBench-S2S: A Benchmark of Deep Learning for Challenging Subseasonal-to-Seasonal Daily Arctic Sea Ice Forecasting in Deep Latent Space](https://arxiv.org/abs/2602.02567)
*Jingyi Xu,Shengnan Wang,Weidong Yang,Siwei Tu,Lei Bai,Ben Fei*

Main category: cs.LG

TL;DR: 提出 IceBench-S2S：首个面向日数据到180天S2S的综合深度学习基准，用于评估北极海冰浓度的序列预测；通过将日数据的空间特征压缩到深层潜在表示，再经时序特征拼接，利用DL回归骨架实现S2S预测，提供统一训练/评估管线和模型选型指南。


<details>
  <summary>Details</summary>
Motivation: 现有DL海冰预测在日-次季至季尺度的预测能力有限，难以直接用于海上运输和科学研究的长期规划；需要一个统一的基准和框架以便对比不同DL骨架的S2S性能并促进实际部署。

Method: 将日海冰数据的空间特征压缩到深潜在空间；时序拼接这些深层特征；用DL预测骨架预测S2S海冰变化；提供统一的训练和评估流程，以及面向极地监测任务的模型选型指南。

Result: 提出 IceBench-S2S，作为首个综合DL基准和测试床，用于评估日到180天S2S尺度的北极海冰浓度预测，提供通用框架和实用的模型选型指南。

Conclusion: IceBench-S2S 将促进DL骨架在极地海冰S2S预测中的标准化评估和实际部署，加速对极地监测任务的支持。

Abstract: Arctic sea ice plays a critical role in regulating Earth's climate system, significantly influencing polar ecological stability and human activities in coastal regions. Recent advances in artificial intelligence have facilitated the development of skillful pan-Arctic sea ice forecasting systems, where data-driven approaches showcase tremendous potential to outperform conventional physics-based numerical models in terms of accuracy, computational efficiency and forecasting lead times. Despite the latest progress made by deep learning (DL) forecasting models, most of their skillful forecasting lead times are confined to daily subseasonal scale and monthly averaged values for up to six months, which drastically hinders their deployment for real-world applications, e.g., maritime routine planning for Arctic transportation and scientific investigation. Extending daily forecasts from subseasonal to seasonal (S2S) scale is scientifically crucial for operational applications. To bridge the gap between the forecasting lead time of current DL models and the significant daily S2S scale, we introduce IceBench-S2S, the first comprehensive benchmark for evaluating DL approaches in mitigating the challenge of forecasting Arctic sea ice concentration in successive 180-day periods. It proposes a generalized framework that first compresses spatial features of daily sea ice data into a deep latent space. The temporally concatenated deep features are subsequently modeled by DL-based forecasting backbones to predict the sea ice variation at S2S scale. IceBench-S2S provides a unified training and evaluation pipeline for different backbones, along with practical guidance for model selection in polar environmental monitoring tasks.

</details>


### [138] [Mitigating Task-Order Sensitivity and Forgetting via Hierarchical Second-Order Consolidation](https://arxiv.org/abs/2602.02568)
*Protik Nag,Krishnan Raghavan,Vignesh Narayanan*

Main category: cs.LG

TL;DR: HTCL: 层次化泰勒序列持续学习框架，结合快速局部适应与保守的二阶全局整合，通过 Hessian 正则化的泰勒展开实现，支持多层次 L 结构，能显著提升在随机任务顺序下的性能并降低结果方差（平均提升7-25%，方差降低至68%）


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中因任务顺序引起的高方差问题；现有单层 CL 系统在任务顺序鲁棒性方面受限；需一个模型无关的整合层以实现多尺度知识整合。

Method: 识别组内最佳任务序列，利用 Hessian 正则化的泰勒展开将局部更新整合为全局更新；提供理论保证；扩展到 L 阶层次结构，作为多尺度知识集成的框架；对任意模型可兼容地作为整合层使用。

Result: 在广泛的数据集与回放/正则化基线上均表现出色，平均准确率提升7%-25%，最终准确度方差降低高达68%。

Conclusion: HTCL 提供了一种将快速局部适应与保守、二阶全局整合相结合的原理性框架，具理论保证并支持多尺度知识整合，优于传统单层 CL。

Abstract: We introduce $\textbf{Hierarchical Taylor Series-based Continual Learning (HTCL)}$, a framework that couples fast local adaptation with conservative, second-order global consolidation to address the high variance introduced by random task ordering. To address task-order effects, HTCL identifies the best intra-group task sequence and integrates the resulting local updates through a Hessian-regularized Taylor expansion, yielding a consolidation step with theoretical guarantees. The approach naturally extends to an $L$-level hierarchy, enabling multiscale knowledge integration in a manner not supported by conventional single-level CL systems. Across a wide range of datasets and replay and regularization baselines, HTCL acts as a model-agnostic consolidation layer that consistently enhances performance, yielding mean accuracy gains of $7\%$ to $25\%$ while reducing the standard deviation of final accuracy by up to $68\%$ across random task permutations.

</details>


### [139] [Reward Shaping for Inference-Time Alignment: A Stackelberg Game Perspective](https://arxiv.org/abs/2602.02572)
*Haichuan Wang,Tao Lin,Lingkai Kong,Ce Li,Hezi Jiang,Milind Tambe*

Main category: cs.LG

TL;DR: 在KL正则化下通过奖励塑形近似最优奖励模型，提升推理时对齐效果并降低奖励黑客风险


<details>
  <summary>Details</summary>
Motivation: KL正则化可能使策略沿着基策略的偏见前进，削弱对用户偏好的满足；强化偏好奖励虽缓解偏见但增加奖励黑客风险；需在KL约束下设计更优的奖励模型以提升用户效用

Method: 将奖励模型优化问题建模为Stackelberg博弈，提出简单的奖励塑形近似方案，能够无缝集成到现有对齐方法，推理时实现开销很小

Result: 在推理时对齐的实验中，方法提升平均奖励，胜率对比所有基线均超过66%，结果在多种评估设置下稳健

Conclusion: 奖励塑形在KL约束下是近似最优奖励模型的有效且低开销的方法，易于与现有对齐方法结合，提升用户效用并抑制奖励黑客

Abstract: Existing alignment methods directly use the reward model learned from user preference data to optimize an LLM policy, subject to KL regularization with respect to the base policy. This practice is suboptimal for maximizing user's utility because the KL regularization may cause the LLM to inherit the bias in the base policy that conflicts with user preferences. While amplifying rewards for preferred outputs can mitigate this bias, it also increases the risk of reward hacking. This tradeoff motivates the problem of optimally designing reward models under KL regularization. We formalize this reward model optimization problem as a Stackelberg game, and show that a simple reward shaping scheme can effectively approximate the optimal reward model. We empirically evaluate our method in inference-time alignment settings and demonstrate that it integrates seamlessly into existing alignment methods with minimal overhead. Our method consistently improves average reward and achieves win-tie rates exceeding 66% against all baselines, averaged across evaluation settings.

</details>


### [140] [QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals](https://arxiv.org/abs/2602.02581)
*Nan Zhang,Eugene Kwek,Yusen Zhang,Muyu Pan,Suhang Wang,Prasenjit Mitra,Rui Zhang*

Main category: cs.LG

TL;DR: QuantLRM introduces weight-update-based channel quantization for LRMs by modeling fine-tuning signals, focusing on the smallest and largest updates. It shows end-point updates are more informative, achieving consistent quantization gains (avg 6.55% on RL-finetuned models) and extending to pseudo-fine-tuning for non-finetuned LRMs.


<details>
  <summary>Details</summary>
Motivation: Efficient quantization of LRMs requires informative signals beyond activations or second-order metrics. The authors hypothesize that extreme weight updates during fine-tuning carry salient information for preserving model performance under weight quantization.

Method: Fit restricted quadratic functions to weight updates during fine-tuning. Compute channel importance as (average quadratic value) × (count of zero weight updates in the channel). Quantize weights based on these importances. Evaluate on supervised, direct preference optimization, and reinforcement learning fine-tuning across four benchmarks (AIME-120, FOLIO, temporal sequences, GPQA-Diamond). Also apply pseudo-fine-tuning to handle non-finetuned LRMs.

Result: QuantLRM yields consistent improvements in LRMs quantization, with an average gain of 6.55% on RL-finetuned models. It outperforms baselines that rely on activations or second-order information. The approach generalizes to non-finetuned LRMs via pseudo-fine-tuning.

Conclusion: Weight-update dynamics during fine-tuning can effectively guide weight quantization for LRMs. Protecting both ends (extreme weight updates) provides a robust signal for channel importance, enabling practical and transferable quantization improvements across varied fine-tuning regimes.

Abstract: Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term "protecting both ends". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability.

</details>


### [141] [Copula-Based Aggregation and Context-Aware Conformal Prediction for Reliable Renewable Energy Forecasting](https://arxiv.org/abs/2602.02583)
*Alireza Moradi,Mathieu Tanneau,Reza Zandehshahvar,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 提出 Copula+CACP 框架，将站点级概率预测汇聚成舰队级（fleet-level）校准预测：通过 Copula 捕获跨站点相关性，并用 Context-Aware Conformal Prediction 调整汇聚后的校准，确保覆盖率接近名义水平且区间更尖锐。对大型太阳能数据集（MISO、ERCOT、SPP）进行验证，性能优于未校准的汇聚基线。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，系统运维者常只能获得站点级概率预测，缺乏统一的舰队级概率模型。站点间存在复杂依赖，汇聚会引发错配校准。需要一个在不训练系统层面模型的情况下，直接从站点级输入构建可靠的舰队级概率预测的框架。

Method: 结合基于 Copula 的跨站点相关性建模来捕捉站点之间的相关性，并使用 Context-Aware Conformal Prediction（CACP）在汇聚层纠正校准偏差，确保在给定依赖关系下仍具备有效覆盖和尖锐的预测区间。

Result: 在 MISO、ERCOT、SPP 的大规模太阳能数据集上，Copula+CACP 方法实现近名义覆盖率，同时相对于未校准的汇聚基线，其预测区间显著更尖锐。

Conclusion: 提出的 Copula+CACP 框架实现了依赖感知的汇聚与有效覆盖的结合，解决了在缺少系统级模型时进行舰队级概率预测的关键难题。

Abstract: The rapid growth of renewable energy penetration has intensified the need for reliable probabilistic forecasts to support grid operations at aggregated (fleet or system) levels. In practice, however, system operators often lack access to fleet-level probabilistic models and instead rely on site-level forecasts produced by heterogeneous third-party providers. Constructing coherent and calibrated fleet-level probabilistic forecasts from such inputs remains challenging due to complex cross-site dependencies and aggregation-induced miscalibration. This paper proposes a calibrated probabilistic aggregation framework that directly converts site-level probabilistic forecasts into reliable fleet-level forecasts in settings where system-level models cannot be trained or maintained. The framework integrates copula-based dependence modeling to capture cross-site correlations with Context-Aware Conformal Prediction (CACP) to correct miscalibration at the aggregated level. This combination enables dependence-aware aggregation while providing valid coverage and maintaining sharp prediction intervals. Experiments on large-scale solar generation datasets from MISO, ERCOT, and SPP demonstrate that the proposed Copula+CACP approach consistently achieves near-nominal coverage with significantly sharper intervals than uncalibrated aggregation baselines.

</details>


### [142] [Fubini Study geometry of representation drift in high dimensional data](https://arxiv.org/abs/2602.02596)
*Arturo Tozzi*

Main category: cs.LG

TL;DR: 提出以 Fubini-Study 距离在投影空间中衡量表示漂移，区分 gauge 变换（全局缩放、符号翻转）引起的变化与真实的本质演化，克服欧氏/余弦距离的过度估计。


<details>
  <summary>Details</summary>
Motivation: 现有的高维表示漂移度量依赖固定坐标，易将数据内在变化与参数化引起的变动混淆。需要一种对 gauge 自由不变的度量来捕捉本质演化。

Method: 建立投影几何框架，将通过全局缩放、符号翻转等 gauge 等价的表示视为同类；在高维数据上构建表示轨迹，沿轨迹计算累积几何漂移；比较欧氏、余弦与 Fubini-Study 距离，提出 cos–FS 漂移差作为可计算、单调的 gauge 驱动漂移量。

Result: 实证数据中 FS 距离对 gauge 变化保持不变，揭示内在演化；欧氏/余弦在存在 gauge 自由时高估变化；cos–FS 的差值给出一个可计算、单调的表示 churn 指标。

Conclusion: 在投影空间中嵌入表示动态，提供稳健的表示稳定性几何准则，明确角度距离的局限性；将数据分析与成熟几何框架相连，得到可在工作流中直接测试的观测量；无需特定模型假设。

Abstract: High dimensional representation drift is commonly quantified using Euclidean or cosine distances, which presuppose fixed coordinates when comparing representations across time, training or preprocessing stages. While effective in many settings, these measures entangle intrinsic changes in the data with variations induced by arbitrary parametrizations. We introduce a projective geometric view of representation drift grounded in the Fubini Study metric, which identifies representations that differ only by gauge transformations such as global rescalings or sign flips. Applying this framework to empirical high dimensional datasets, we explicitly construct representation trajectories and track their evolution through cumulative geometric drift. Comparing Euclidean, cosine and Fubini Study distances along these trajectories reveals that conventional metrics systematically overestimate change whenever representations carry genuine projective ambiguity. By contrast, the Fubini Study metric isolates intrinsic evolution by remaining invariant under gauge-induced fluctuations. We further show that the difference between cosine and Fubini Study drift defines a computable, monotone quantity that directly captures representation churn attributable to gauge freedom. This separation provides a diagnostic for distinguishing meaningful structural evolution from parametrization artifacts, without introducing model-specific assumptions. Overall, we establish a geometric criterion for assessing representation stability in high-dimensional systems and clarify the limits of angular distances. Embedding representation dynamics in projective space connects data analysis with established geometric programs and yields observables that are directly testable in empirical workflows.

</details>


### [143] [RAP: KV-Cache Compression via RoPE-Aligned Pruning](https://arxiv.org/abs/2602.02599)
*Jihao Xin,Tian Lvu,Hatem Ltaief,David Keyes,Marco Canini*

Main category: cs.LG

TL;DR: RoPE-Aligned Pruning (RAP) 通过对 RoPE 对齐的列对进行裁剪，维持 RoPE 的 2x2 旋转结构并实现 B 的吸收，从而在不显著损害精度的前提下，联合降低 KV-Cache、注意力参数和 FLOPs 约 20-30%，并显著降低前缀填充和解码阶段的注意力延迟。


<details>
  <summary>Details</summary>
Motivation: 在长上下文推理中，KV-Cache 的内存与计算成本成为瓶颈；对低秩分解的普通裁剪会被 RoPE 的全维重建机制抵消。需要一种能在保留 RoPE 旋转结构的同时实现参数与计算压缩的方法。

Method: 提出 RoPE-Aligned Pruning（RAP），裁剪与 RoPE 对齐的整列对，使被裁剪的列对保持 RoPE 的 2x2 旋转结构，进而恢复 B 的吸收能力，避免 KV-Cache 的重建开销。实验在 LLaMA-3-8B 与 Mistral-7B 上，KV-Cache、注意力参数与 FLOPs 共同降低约 20-30%，且保持较高精度，注意力延迟在_PREFILL 阶段降至 Baseline 的 83%，解码阶段降至 77%。

Result: 实现了联合的成本下降（KV-Cache、注意力参数、FLOPs）并提升了注意力延迟；在 RoPE 架构下提供一个可落地的剪枝策略，兼容 B 的吸收，避免重建开销。

Conclusion: RAP 为 RoPE 基础的长上下文推理提供有效的结构化剪枝策略，维持旋转结构的同时实现多维成本下降。未来可扩展至更多 RoPE 变体、不同模型规模，以及评估鲁棒性与普适性。

Abstract: Long-context inference in large language models is increasingly bottlenecked by the memory and compute cost of the KV-Cache. Low-rank factorization compresses KV projections by writing $W \approx A * B$, where A produces latent KV states and B can be absorbed into downstream weights. In modern RoPE-based LLMs, this absorption fails: RoPE forces latent KV states to be reconstructed to full dimension, reintroducing substantial memory and compute overhead. We propose RoPE-Aligned Pruning (RAP), which prunes entire RoPE-aligned column pairs to preserve RoPE's 2x2 rotation structure, restore B absorption, and eliminate reconstruction. Our evaluation on LLaMA-3-8B and Mistral-7B shows that RAP enables joint reduction of KV-Cache, attention parameters, and FLOPs by 20-30%, all at once, while maintaining strong accuracy. Notably, RAP reduces attention latency to 83% (prefill) and 77% (decode) of baseline.

</details>


### [144] [Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models](https://arxiv.org/abs/2602.02600)
*Eliron Rahimi,Elad Hirshel,Rom Himelstein,Amit LeVi,Avi Mendelson,Chaim Baskin*

Main category: cs.LG

TL;DR: 提出一个分析框架与SRI信号，揭示扩散语言模型中的拒绝行为与安全性受采样策略影响，提出可在推理时实现的轻量检测器，显著提高鲁棒性且降低开销。


<details>
  <summary>Details</summary>
Motivation: 理解采样机制在拒绝/对抗攻击鲁棒性中的作用，填补扩散语言模型与自回归模型在安全维度的差距。

Method: 提出步进拒绝内部动力学（SRI）信号，建立DLM与AR的对比分析框架；分析SRI的几何结构以揭示内部恢复动态；基于SRI开发推理时检测器。

Result: SRI能够捕捉内部恢复动力学；识别有害生成中的异常行为（不完全内部恢复）；检测器可迁移到未见攻击，推理开销比现有防御低超过100×，且性能可媲美或优于现有方法。

Conclusion: 采样策略本身对安全性具有核心作用，与学习表征解耦；SRI提供可解释性并实现高效的安全防护，适用于自回归和扩散语言模型。

Abstract: Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering parallel decoding and controllable sampling dynamics while achieving competitive generation quality at scale. Despite this progress, the role of sampling mechanisms in shaping refusal behavior and jailbreak robustness remains poorly understood. In this work, we present a fundamental analytical framework for step-wise refusal dynamics, enabling comparison between AR and diffusion sampling. Our analysis reveals that the sampling strategy itself plays a central role in safety behavior, as a factor distinct from the underlying learned representations. Motivated by this analysis, we introduce the Step-Wise Refusal Internal Dynamics (SRI) signal, which supports interpretability and improved safety for both AR and DLMs. We demonstrate that the geometric structure of SRI captures internal recovery dynamics, and identifies anomalous behavior in harmful generations as cases of \emph{incomplete internal recovery} that are not observable at the text level. This structure enables lightweight inference-time detectors that generalize to unseen attacks while matching or outperforming existing defenses with over $100\times$ lower inference overhead.

</details>


### [145] [Discovering Data Manifold Geometry via Non-Contracting Flows](https://arxiv.org/abs/2602.02611)
*David Vigouroux,Lucas Drumetz,Ronan Fablet,François Rousseau*

Main category: cs.LG

TL;DR: 提出一种无监督方法，在环境空间学习张量场以跨越未知数据流形的切空间，从而构造全局参考坐标系；通过沿流的弧长定义内在坐标，避免扭曲并引入非收缩约束，提出受流匹配启发的可扩展无积分目标，理论上证明在全局坐标图存在时可恢复；在合成流形上取得正确的切线对齐，在 CIFAR-10 上具有可扩展性且下游分类性能具竞争力。


<details>
  <summary>Details</summary>
Motivation: 在不假设流形是平坦的前提下，构建可解释且可全局引用的坐标系；避免传统等距目标对流形扁平性的依赖，提供更稳定且可扩展的全局参考框架。

Method: 学习能张成数据流形切空间的切向量场，使样本通过这些场的流输送到一个可学习的公共参考点；将沿着这些流的弧长定义为内在坐标；引入非收缩约束以防止退化崩塌；提出来自流匹配的、可扩展的无积分优化目标；在理论框架下证明若存在全局坐标图，则最小化目标可恢复该坐标图。

Result: 在合成流形上实现正确的切线对齐和一致的全局坐标结构；在 CIFAR-10 上具备可扩展性，学习得到的坐标对下游分类性能具有竞争力。

Conclusion: 所提出的无监督目标可在存在全局坐标图时恢复该坐标，并对大规模数据集具有可扩展性，提供一种解释性强的全局流形坐标表示，对非线性数据的全局几何理解有潜在价值。

Abstract: We introduce an unsupervised approach for constructing a global reference system by learning, in the ambient space, vector fields that span the tangent spaces of an unknown data manifold. In contrast to isometric objectives, which implicitly assume manifold flatness, our method learns tangent vector fields whose flows transport all samples to a common, learnable reference point. The resulting arc-lengths along these flows define interpretable intrinsic coordinates tied to a shared global frame. To prevent degenerate collapse, we enforce a non-shrinking constraint and derive a scalable, integration-free objective inspired by flow matching. Within our theoretical framework, we prove that minimizing the proposed objective recovers a global coordinate chart when one exists. Empirically, we obtain correct tangent alignment and coherent global coordinate structure on synthetic manifolds. We also demonstrate the scalability of our method on CIFAR-10, where the learned coordinates achieve competitive downstream classification performance.

</details>


### [146] [A Semi-Supervised Pipeline for Generalized Behavior Discovery from Animal-Borne Motion Time Series](https://arxiv.org/abs/2602.02618)
*Fatemeh Karimi Nejadasl,Judy Shamoun-Baranes,Eldar Rakhimberdiev*

Main category: cs.LG

TL;DR: 在动物-携带传感数据中进行行为泛化发现的半监督管线：从少量标注数据中学习嵌入，通过对标注与未标注样本的标签引导聚类形成候选行为簇，并用 KDE+HDR 的 containment score 评估新簇与已知类的覆盖关系，从而给出可解释的新颖性指标。


<details>
  <summary>Details</summary>
Motivation: 标签稀缺、类别高度不平衡且可能缺失某些行为，难以仅靠有监督学习实现广义的行为发现；以海鸥为对象的多变量运动时序数据需要可解释的新颖性评估。

Method: 1) 从带标注子集学习嵌入函数；2) 对带标签和未标注样本的嵌入进行标签引导聚类，形成候选行为组；3) 采用 KDE+HDR 的 containment score 评估发现簇分布与已知类分布的包含关系，最佳匹配的 containment score 作为新颖性统计量；

Result: 当一个完整的行为被保留在未标注池而未出现在监督集时，方法能识别出一个独立簇并通过低重叠指示新颖性；对照设定若无新颖行为则重叠度较高。HDR-based containment 提供一个在极度标注稀缺和严重类别不平衡情境下的实用定量新颖性检验。

Conclusion: HDR-based containment 为生态运动时间序列的广义类发现提供可解释、可量化的评估框架，适用于标注受限时的新簇检测与新颖性评估。

Abstract: Learning behavioral taxonomies from animal-borne sensors is challenging because labels are scarce, classes are highly imbalanced, and behaviors may be absent from the annotated set. We study generalized behavior discovery in short multivariate motion snippets from gulls, where each sample is a sequence with 3-axis IMU acceleration (20 Hz) and GPS speed, spanning nine expert-annotated behavior categories. We propose a semi-supervised discovery pipeline that (i) learns an embedding function from the labeled subset, (ii) performs label-guided clustering over embeddings of both labeled and unlabeled samples to form candidate behavior groups, and (iii) decides whether a discovered group is truly novel using a containment score. Our key contribution is a KDE + HDR (highest-density region) containment score that measures how much a discovered cluster distribution is contained within, or contains, each known-class distribution; the best-match containment score serves as an interpretable novelty statistic. In experiments where an entire behavior is withheld from supervision and appears only in the unlabeled pool, the method recovers a distinct cluster and the containment score flags novelty via low overlap, while a negative-control setting with no novel behavior yields consistently higher overlaps. These results suggest that HDR-based containment provides a practical, quantitative test for generalized class discovery in ecological motion time series under limited annotation and severe class imbalance.

</details>


### [147] [daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently](https://arxiv.org/abs/2602.02619)
*Mohan Jiang,Dayuan Fu,Junhao Shi,Ji Zeng,Weiye Si,Keyu Li,Xuefeng Li,Yang Xiao,Wenjie Li,Dequan Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: 通过将 Pull Request 序列作为长时学习的监督信号，daVinci-Agency 实现了数据高效的长时目标导向能力提升，并在 GLM-4.6 上达到显著性能提升。


<details>
  <summary>Details</summary>
Motivation: LLMs 在长时间、分阶段的任务中表现受限，现有数据合成往往单一特征、成本高且缺乏跨阶段因果关系。借鉴软件演化中的 PR 序列，可提供可验证的子任务、跨阶段一致性与真实的修复轨迹。

Method: 提出 daVinci-Agency，基于 chain-of-PRs 提炼结构化监督，包含三大机制：1) 通过持续提交实现任务的渐进分解；2) 通过统一功能目标实现长时一致性；3) 通过 Bug-fix 轨迹实现可验证的改进。PR 结构天然保留因果依赖和迭代改进，数据量大（85k tokens、116 tool calls），对 GLM-4.6 进行高效微调。

Result: 在多个基准上得到改进，特别在 Toolathlon 上实现相对提升 47%；数据高效性显著。此外，分析还证实 PR 结构对持续目标导向行为的学习具有更好的长久依赖建模能力。

Conclusion: 基于 PR 的监督提供了长时任务学习的新范式，与真实的软件演化信号对齐，使得学习具有更强的跨阶段一致性、可解释性和迁移潜力，且在实际微调数据较少时也能实现显著提升。

Abstract: While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...

</details>


### [148] [A Reduction from Delayed to Immediate Feedback for Online Convex Optimization with Improved Guarantees](https://arxiv.org/abs/2602.02634)
*Alexander Ryabchenko,Idan Attias,Daniel M. Roy*

Main category: cs.LG

TL;DR: 提出一个基于连续时间模型的延迟反馈在线学习降维框架，通过将任何在线线性优化算法转化为可处理轮次延迟的算法，实现对带延迟的带约束的带宽?（笔误）带带带的带宽?。在带带凸优化（bandit convex optimization, BCO）方面给出新的界，延迟项从旧的 O(min{√(Td_max),(Td_tot)^{1/3}}) 提升到 O(√(d_tot))，总体心跳为 O(√(d_tot) + T^{3/4}√k)；在强凸性下得到 O(min{σ_max ln T, √(d_tot)} + (T^2 ln T)^{1/3} k^{2/3})，其中 σ_max 为未完成观测的最大数量。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习在存在时延反馈时的性能下降问题，提供一个统一、简化且更强的降维/降迟框架，覆盖第一阶反馈与带带凸优化两类情形，并将轮次延迟影响分解为漂移项与学习项以实现延迟自适应的降维。

Method: 引入一个连续时间模型，使回报的遗留（delay）分解为一个与延迟无关的学习项和一个延迟诱导的漂移项；提出将任意针对在线线性优化的算法转化为能够处理轮次延迟的延迟自适应降维框架；对带带凸优化通过梯度估计实现，给出延迟相关的可控界；对第一阶反馈给出统一简化分析。

Result: 得到带带凸优化的改进后界：总的对后验的 regrets 为 O(√(d_tot) + T^{3/4}√k)，其中 d_tot 为总延迟，k 为维度，T 为时序界；延迟相关项从先前的 O(min{√(T d_max), (T d_tot)^{1/3}}) 改善为 O(√(d_tot))。在强凸性下，得到 O(min{σ_max ln T, √(d_tot)} + (T^2 ln T)^{1/3} k^{2/3})，其中 σ_max 为未完成观测的最大数，且可能远小于 d_max。对于第一阶反馈，给出等价且更简洁的统一分析，达到现有最优界。

Conclusion: 该框架提供了一种通用的延迟自适应降维策略，可将延迟反馈的负效应分离并以更低的延迟成本实现更强的 regret 界，适用于带延迟的带凸优化与第一阶反馈场景，且对强凸性情形提供更优的延迟依赖界。

Abstract: We develop a reduction-based framework for online learning with delayed feedback that recovers and improves upon existing results for both first-order and bandit convex optimization. Our approach introduces a continuous-time model under which regret decomposes into a delay-independent learning term and a delay-induced drift term, yielding a delay-adaptive reduction that converts any algorithm for online linear optimization into one that handles round-dependent delays. For bandit convex optimization, we significantly improve existing regret bounds, with delay-dependent terms matching state-of-the-art first-order rates. For first-order feedback, we recover state-of-the-art regret bounds via a simpler, unified analysis. Quantitatively, for bandit convex optimization we obtain $O(\sqrt{d_{\text{tot}}} + T^{\frac{3}{4}}\sqrt{k})$ regret, improving the delay-dependent term from $O(\min\{\sqrt{T d_{\text{max}}},(Td_{\text{tot}})^{\frac{1}{3}}\})$ in previous work to $O(\sqrt{d_{\text{tot}}})$. Here, $k$, $T$, $d_{\text{max}}$, and $d_{\text{tot}}$ denote the dimension, time horizon, maximum delay, and total delay, respectively. Under strong convexity, we achieve $O(\min\{σ_{\text{max}} \ln T, \sqrt{d_{\text{tot}}}\} + (T^2\ln T)^{\frac{1}{3}} {k}^{\frac{2}{3}})$, improving the delay-dependent term from $O(d_{\text{max}} \ln T)$ in previous work to $O(\min\{σ_{\text{max}} \ln T, \sqrt{d_{\text{tot}}}\})$, where $σ_{\text{max}}$ denotes the maximum number of outstanding observations and may be considerably smaller than $d_{\text{max}}$.

</details>


### [149] [hSNMF: Hybrid Spatially Regularized NMF for Image-Derived Spatial Transcriptomics](https://arxiv.org/abs/2602.02638)
*Md Ishtyaq Mahmud,Veena Kochat,Suresh Satpati,Jagan Mohan Reddy Dwarampudi,Humaira Anzum,Kunal Rai,Tania Banerjee*

Main category: cs.LG

TL;DR: Proposes spatially regularized NMF variants (SNMF and hSNMF) for high-dimensional spatial transcriptomics from Xenium data, improving spatial coherence, cluster separability, and biological coherence on cholangiocarcinoma data; released with code.


<details>
  <summary>Details</summary>
Motivation: High-resolution spatial transcriptomics yields rich spatial and transcriptomic information but poses challenges for representation learning and clustering due to extreme dimensionality; need methods that incorporate spatial structure to produce coherent, interpretable factors and clusters.

Method: 1) SNMF: applying standard NMF with local spatial smoothing by diffusing each cell’s factor vector over spatial neighbors. 2) hSNMF: spatially regularized NMF followed by Leiden clustering on a hybrid adjacency graph that combines spatial proximity (contact-radius graph) and transcriptomic similarity with a tunable mixing parameter alpha.

Result: SNMF and hSNMF yield superior spatial compactness (CHAOS < 0.004; Moran’s I > 0.96), improved cluster separability (Silhouette > 0.12; DBI < 1.8), and enhanced biological coherence (cell-type markers, enrichment) compared with spatial baselines on cholangiocarcinoma Xenium data.

Conclusion: Spatial regularization in NMF frameworks (and their combination with graph-based clustering) effectively captures spatial organization in high-dimensional spatial transcriptomics, offering compact, well-separated, biologically coherent clusters; code is available on GitHub.

Abstract: High-resolution spatial transcriptomics platforms, such as Xenium, generate single-cell images that capture both molecular and spatial context, but their extremely high dimensionality poses major challenges for representation learning and clustering. In this study, we analyze data from the Xenium platform, which captures high-resolution images of tumor microarray (TMA) tissues and converts them into cell-by-gene matrices suitable for computational analysis. We benchmark and extend nonnegative matrix factorization (NMF) for spatial transcriptomics by introducing two spatially regularized variants. First, we propose Spatial NMF (SNMF), a lightweight baseline that enforces local spatial smoothness by diffusing each cell's NMF factor vector over its spatial neighborhood. Second, we introduce Hybrid Spatial NMF (hSNMF), which performs spatially regularized NMF followed by Leiden clustering on a hybrid adjacency that integrates spatial proximity (via a contact-radius graph) and transcriptomic similarity through a tunable mixing parameter alpha. Evaluated on a cholangiocarcinoma dataset, SNMF and hSNMF achieve markedly improved spatial compactness (CHAOS < 0.004, Moran's I > 0.96), greater cluster separability (Silhouette > 0.12, DBI < 1.8), and higher biological coherence (CMC and enrichment) compared to other spatial baselines. Availability and implementation: https://github.com/ishtyaqmahmud/hSNMF

</details>


### [150] [MARA: Continuous SE(3)-Equivariant Attention for Molecular Force Fields](https://arxiv.org/abs/2602.02671)
*Francesco Leonardi,Boris Bonev,Kaspar Riesen*

Main category: cs.LG

TL;DR: Introduces Modular Angular-Radial Attention (MARA) to SE(3)-equivariant ML force fields, enabling plug-and-play, geometry-aware weighting of local environments and improving accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Fixed angular expansions in ML force fields limit flexibility in modeling local geometric interactions; need a generalizable, flexible operator for equivariant interactions.

Method: Extends spherical attention to the molecular domain; operates on angular and radial coordinates of neighboring atoms; modular, plug-and-play, integrated into MACE without architectural changes.

Result: Improved energy and force predictions across molecular benchmarks; fewer high-error events; enhanced robustness.

Conclusion: Continuous spherical attention is a generalizable geometric operator that increases expressiveness, stability, and reliability of atomistic models.

Abstract: Machine learning force fields (MLFFs) have become essential for accurate and efficient atomistic modeling. Despite their high accuracy, most existing approaches rely on fixed angular expansions, limiting flexibility in weighting local geometric interactions. We introduce Modular Angular-Radial Attention (MARA), a module that extends spherical attention -- originally developed for SO(3) tasks -- to the molecular domain and SE(3), providing an efficient approximation of equivariant interactions. MARA operates directly on the angular and radial coordinates of neighboring atoms, enabling flexible, geometrically informed, and modular weighting of local environments. Unlike existing attention mechanisms in SE(3)-equivariant architectures, MARA can be integrated in a plug-and-play manner into models such as MACE without architectural modifications. Across molecular benchmarks, MARA improves energy and force predictions, reduces high-error events, and enhances robustness. These results demonstrate that continuous spherical attention is an effective and generalizable geometric operator that increases the expressiveness, stability, and reliability of atomistic models.

</details>


### [151] [FlexRank: Nested Low-Rank Knowledge Decomposition for Adaptive Model Deployment](https://arxiv.org/abs/2602.02680)
*Riccardo Zaccone,Stefanos Laskaridis,Marco Ciccone,Samuel Horváth*

Main category: cs.LG

TL;DR: Proposes FlexRank: a budget-aware submodel extraction from pretrained models via low-rank weight decomposition and nested importance-based consolidation, enabling train-once deploy-everywhere.


<details>
  <summary>Details</summary>
Motivation: Fix-cost, monolithic deployment of large pretrained models; exploit overparameterization to derive adaptive submodels that fit different compute budgets.

Method: Apply low-rank weight decomposition with nested, importance-based consolidation to produce a hierarchy of submodels with increasing capabilities from a single pretrained model.

Result: Submodels of increasing capability can be extracted to enable a train-once, deploy-everywhere workflow, providing a graceful cost–performance trade-off without retraining for each budget.

Conclusion: This approach advances practical deployment of large models by enabling flexible budget-aware deployment.

Abstract: The growing scale of deep neural networks, encompassing large language models (LLMs) and vision transformers (ViTs), has made training from scratch prohibitively expensive and deployment increasingly costly. These models are often used as computational monoliths with fixed cost, a rigidity that does not leverage overparametrized architectures and largely hinders adaptive deployment across different cost budgets. We argue that importance-ordered nested components can be extracted from pretrained models, and selectively activated on the available computational budget. To this end, our proposed FlexRank method leverages low-rank weight decomposition with nested, importance-based consolidation to extract submodels of increasing capabilities. Our approach enables a "train-once, deploy-everywhere" paradigm that offers a graceful trade-off between cost and performance without training from scratch for each budget - advancing practical deployment of large models.

</details>


### [152] [Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models](https://arxiv.org/abs/2602.02685)
*Marcos Villagra,Bidhan Roy,Raihan Seraj,Zhiying Jiang*

Main category: cs.LG

TL;DR: 本研究揭示在去中心化扩散模型中，生成质量的决定因素不是 denoising 轨迹的数值稳定性，而是专家-数据对齐；稀疏 Top-2 路由在生成质量上优于全集成路由。


<details>
  <summary>Details</summary>
Motivation: 在多个独立数据簇上训练的专家可能对同一去噪过程给出矛盾预测，需弄清哪一方面决定最终生成质量。

Method: 对两套去中心扩散模型系统比较不同路由策略（全集成、稀疏路由、Top-2 路由等），评估稳定性、数值收敛与生成质量。通过数据簇距离分析、逐专家分析、专家分歧分析等多维证据验证专家-数据对齐的重要性。

Result: 全集成路由实现最稳定的采样动力学与最优数值收敛，但生成质量最差；稀疏 Top-2 路由显著提升生成质量（FID 22.6 对比 47.9 的全集成），并且在数据对齐方面表现更好。证据包括数据簇距离分析、逐专家预测比较和专家间分歧对质量的影响分析。

Conclusion: 生成质量的决定因素应聚焦于专家-数据对齐而非 denoising 轨迹的数值稳定性，路由策略应优先覆盖当前去噪状态的数据分布，以提升整体生成效果。

Abstract: Decentralized Diffusion Models (DDMs) route denoising through experts trained independently on disjoint data clusters, which can strongly disagree in their predictions. What governs the quality of generations in such systems? We present the first ever systematic investigation of this question. A priori, the expectation is that minimizing denoising trajectory sensitivity -- minimizing how perturbations amplify during sampling -- should govern generation quality. We demonstrate this hypothesis is incorrect: a stability-quality dissociation. Full ensemble routing, which combines all expert predictions at each step, achieves the most stable sampling dynamics and best numerical convergence while producing the worst generation quality (FID 47.9 vs. 22.6 for sparse Top-2 routing). Instead, we identify expert-data alignment as the governing principle: generation quality depends on routing inputs to experts whose training distribution covers the current denoising state. Across two distinct DDM systems, we validate expert-data alignment using (i) data-cluster distance analysis, confirming sparse routing selects experts with data clusters closest to the current denoising state, and (ii) per-expert analysis, showing selected experts produce more accurate predictions than non-selected ones, and (iii) expert disagreement analysis, showing quality degrades when experts disagree. For DDM deployment, our findings establish that routing should prioritize expert-data alignment over numerical stability metrics.

</details>


### [153] [Sparsely Supervised Diffusion](https://arxiv.org/abs/2602.02699)
*Wenshuai Zhao,Zhiyuan Li,Yi Zhao,Mohammad Hassan Vali,Martin Trapp,Joni Pajarinen,Juho Kannala,Arno Solin*

Main category: cs.LG

TL;DR: 提出一种稀疏监督的掩码策略用于扩散模型训练，最多可对98%像素进行遮罩，实现全局一致性提升、降低记忆化、在小数据集上训练稳定且获得竞争性FID。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型因 denoising 的局部性带来的全局不一致性问题；通过引入稀疏掩码监督以更好地利用全局上下文。

Method: 在扩散模型训练中引入简单的掩码策略（几行代码即可实现的 masking），对输入像素进行大比例遮罩，并对被遮罩的像素提供监督信号，同时保持少量未遮罩区域以提供上下文。可高达遮罩78-98%像素，具体实现细节以论文为准。

Result: 在多组实验中达到竞争性FID；在小数据集上避免训练不稳定；掩码策略降低了记忆化，促使模型更依赖关键上下文信息进行生成。

Conclusion: 稀疏掩码监督是一种简单、鲁棒且有效的扩散模型训练策略，可提升全局一致性并降低对大量数据的依赖。

Abstract: Diffusion models have shown remarkable success across a wide range of generative tasks. However, they often suffer from spatially inconsistent generation, arguably due to the inherent locality of their denoising mechanisms. This can yield samples that are locally plausible but globally inconsistent. To mitigate this issue, we propose sparsely supervised learning for diffusion models, a simple yet effective masking strategy that can be implemented with only a few lines of code. Interestingly, the experiments show that it is safe to mask up to 98\% of pixels during diffusion model training. Our method delivers competitive FID scores across experiments and, most importantly, avoids training instability on small datasets. Moreover, the masking strategy reduces memorization and promotes the use of essential contextual information during generation.

</details>


### [154] [Every Bit Counts: A Theoretical Study of Precision-Expressivity Tradeoffs in Quantized Transformers](https://arxiv.org/abs/2602.02707)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

TL;DR: 存在一个严格的一比特阈值：一个单层软max Transformer 能用 p 位精度计算一个函数 Γ，但用 p-1 位却不能。该结果将量化引起的表达能力损失与等价性相关任务联系起来，提示在需要精确相等比较的任务中，降低一位会跨越表达能力的阈值。给出在实际中选择量化精度的直觉：应依据需要检查的等价长度来决定精度。


<details>
  <summary>Details</summary>
Motivation: 量化在加速推理中广泛使用，但对模型表达能力的影响尚未被清晰定量地界定。理解精度与表达能力之间的边界能帮助设计更可靠的量化策略并解释经验观察。

Method: 给出对每个 p 的 Γ 函数的存在性构造，基于等价函数灵感。通过显式的有限精度 Transformer 构造与通信复杂性下界相结合，得到一个“一个比特”阈值的严格界。

Result: 存在对于任意 p，都存在一个 Γ，使得单层软最大 Transformer 在 p 位精度下能够计算 Γ，而在 p-1 位精度下不能。该结果解释了量化引起的表达能力下降，尤其是对等价性类任务的敏感性，并提出在任务中需要检查的等价长度下的量化精度选择直觉。

Conclusion: 量化给表达能力带来严格的一比特阈值，等价性检查相关任务最易受影响。研究为量化策略的选择提供定量直觉和理论支撑，并指向在特定任务中按需调整精度的实用路径。

Abstract: Quantization reduces the numerical precision of Transformer computations and is widely used to accelerate inference, yet its effect on expressivity remains poorly characterized. We demonstrate a fine-grained theoretical tradeoff between expressivity and precision: For every p we exhibit a function Γ, inspired by the equality function, and prove that a one-layer softmax Transformer can compute Γ, with p bits of precision, but not with p-1 bits of precision.
  This result concretely explains the widely observed phenomenon of empirical loss of expressivity when quantization is used. Practically, it suggests that tasks requiring equality-like comparisons (exact match, membership, etc.) are especially sensitive to quantization. Dropping even one bit can cross a threshold where the model cannot represent the needed comparison reliably. Thus, it paves the way for developing heuristics that will help practitioners choose how much quantization is possible: the precision should be chosen as a function of the length of equality to be checked for the specific task.
  Our proofs combine explicit finite-precision Transformer constructions with communication-complexity lower bounds, yielding a tight "one-bit" threshold.

</details>


### [155] [BinaryPPO: Efficient Policy Optimization for Binary Classification](https://arxiv.org/abs/2602.02708)
*Punya Syon Pandey,Zhijing Jin*

Main category: cs.LG

TL;DR: BinaryPPO 将二元分类转化为奖励最大化问题，基于离线 PPO 的变体，使用置信度加权奖励，在静态数据集上训练，显著优于 SFT 的鲁棒二元分类方法。


<details>
  <summary>Details</summary>
Motivation: SFT 在现实场景中常受标签噪声、类别不平衡和稀疏监督影响，亟需在离线数据上即可学习到鲁棒决策策略的二元分类方法。

Method: 提出离线强化学习框架 BinaryPPO，采用对置信度进行加权的奖励函数以惩罚不确定或错误的预测；在不进行在线交互的前提下，基于静态数据集进行策略学习。通过对奖励 shaping、优势缩放和策略稳定性的分析，验证在八个领域基准和多种模型架构下的有效性。

Result: 在 eight 个领域基准和多模型设置中，准确率提升约 40–60 个百分点；最高达到 99%，显著优于监督基线。

Conclusion: 置信度驱动的奖励设计为二元分类提供了对 SFT 的鲁棒替代；代码开放获取以便复现。

Abstract: Supervised fine-tuning (SFT) is the standard approach for binary classification tasks such as toxicity detection, factuality verification, and causal inference. However, SFT often performs poorly in real-world settings with label noise, class imbalance, or sparse supervision. We introduce BinaryPPO, an offline reinforcement learning large language model (LLM) framework that reformulates binary classification as a reward maximization problem. Our method leverages a variant of Proximal Policy Optimization (PPO) with a confidence-weighted reward function that penalizes uncertain or incorrect predictions, enabling the model to learn robust decision policies from static datasets without online interaction. Across eight domain-specific benchmarks and multiple models with differing architectures, BinaryPPO improves accuracy by 40-60 percentage points, reaching up to 99%, substantially outperforming supervised baselines. We provide an in-depth analysis of the role of reward shaping, advantage scaling, and policy stability in enabling this improvement. Overall, we demonstrate that confidence-based reward design provides a robust alternative to SFT for binary classification. Our code is available at https://github.com/psyonp/BinaryPPO.

</details>


### [156] [Maximum Likelihood Reinforcement Learning](https://arxiv.org/abs/2602.02710)
*Fahim Tajwar,Guanning Zeng,Yueer Zhou,Yuda Song,Daman Arora,Yiding Jiang,Jeff Schneider,Ruslan Salakhutdinov,Haiwen Feng,Andrea Zanette*

Main category: cs.LG

TL;DR: MaxRL 将最大似然近似引入强化学习框架，通过一系列计算性可控的样本目标在标准 RL 与精确最大似然之间插值，提供简单无偏的策略梯度估计，并在无限算力下收敛于最大似然；实验显示在多任务/模型中显著优于现有方法，测试时效提升高达约20倍，且随着数据和算力增加表现更好。


<details>
  <summary>Details</summary>
Motivation: 在二元结果反馈的采样场景中，现有 RL 往往没有直接最大化正确回滚的似然，而只是优化一个低阶近似。为了更直接地接近最大似然并提高可控性，需要一个可按计算资源进行权衡的框架。

Method: 提出一个 compute-indexed 的样本目标族，逐步在标准 RL 与精确最大似然之间插值；该目标族可用简单且无偏的策略梯度估计实现，且在无限计算下收敛到最大似然优化。

Result: 在所有模型与任务上，MaxRL 相对现有方法实现帕累托支配（Pareto-dominance），并实现最高达 20× 的测试时效率提升，相较于 GRPO 训练的对手；此外 MaxRL 对数据和算力的扩展性也更好。

Conclusion: MaxRL 是一个有望扩展 RL 训练以适应正确性约束设定的框架，能够在非微分采样情境中更直接地逼近最大似然，并在多样场景中带来显著的效率与效果提升。

Abstract: Reinforcement learning is the method of choice to train models in sampling-based setups with binary outcome feedback, such as navigation, code generation, and mathematical problem solving. In such settings, models implicitly induce a likelihood over correct rollouts. However, we observe that reinforcement learning does not maximize this likelihood, and instead optimizes only a lower-order approximation. Inspired by this observation, we introduce Maximum Likelihood Reinforcement Learning (MaxRL), a sampling-based framework to approximate maximum likelihood using reinforcement learning techniques. MaxRL addresses the challenges of non-differentiable sampling by defining a compute-indexed family of sample-based objectives that interpolate between standard reinforcement learning and exact maximum likelihood as additional sampling compute is allocated. The resulting objectives admit a simple, unbiased policy-gradient estimator and converge to maximum likelihood optimization in the infinite-compute limit. Empirically, we show that MaxRL Pareto-dominates existing methods in all models and tasks we tested, achieving up to 20x test-time scaling efficiency gains compared to its GRPO-trained counterpart. We also observe MaxRL to scale better with additional data and compute. Our results suggest MaxRL is a promising framework for scaling RL training in correctness based settings.

</details>


### [157] [Towards Understanding Steering Strength](https://arxiv.org/abs/2602.02712)
*Magamed Taimeskhanov,Samuel Vaiter,Damien Garreau*

Main category: cs.LG

TL;DR: 本文对大语言模型后训练控制中的“转向强度”进行了首次理论分析，揭示拉动幅度对下一个词概率、概念存在感及交叉熵的定性规律，并发现非单调效应，通过11种模型实验验证其预测。


<details>
  <summary>Details</summary>
Motivation: 当前多数工作关注转向方向的选择，但对转向幅度的影响缺乏系统理解，需要建立定量/定性的规律以指导实际应用。

Method: 建立理论框架，分析在给定方向的线性扰动下，对下一个词分布、概念显著性和交叉熵的影响，推导出稳健的定性规律；并在11种模型上进行实证验证。

Result: 得到关于转向强度的定性规律，包括可能出现的非单调效应；验证这些规律在多种模型中的存在性和鲁棒性。

Conclusion: 为实际应用中的转向强度调参提供理论依据，揭示了转向幅度对语言生成行为的深层影响。

Abstract: A popular approach to post-training control of large language models (LLMs) is the steering of intermediate latent representations. Namely, identify a well-chosen direction depending on the task at hand and perturbs representations along this direction at inference time. While many propositions exist to pick this direction, considerably less is understood about how to choose the magnitude of the move, whereas its importance is clear: too little and the intended behavior does not emerge, too much and the model's performance degrades beyond repair. In this work, we propose the first theoretical analysis of steering strength. We characterize its effect on next token probability, presence of a concept, and cross-entropy, deriving precise qualitative laws governing these quantities. Our analysis reveals surprising behaviors, including non-monotonic effects of steering strength. We validate our theoretical predictions empirically on eleven language models, ranging from a small GPT architecture to modern models.

</details>


### [158] [Automated Dysphagia Screening Using Noninvasive Neck Acoustic Sensing](https://arxiv.org/abs/2602.02725)
*Jade Chng,Rong Xing,Yunfei Luo,Kristen Linnemeyer-Risser,Tauhidur Rahman,Andrew Yousef,Philip A Weissbrod*

Main category: cs.LG

TL;DR: 提出一个基于颈部非侵入声学信号和机器学习的吞咽异常自动检测框架，5次独立train-test分割下AUC-ROC约0.904，显示可行性与潜在规模化应用。


<details>
  <summary>Details</summary>
Motivation: 早期发现吞咽障碍对及时干预至关重要；现有诊断方法多依赖放射影像或侵入性程序，需更便捷的非侵入性筛查工具。

Method: 通过捕获吞咽任务中颈部的细微声学信号，结合机器学习实现异常模式识别与检测，包含特征提取、模型训练与跨分割评估等步骤。

Result: 在测试阶段的异常检测表现良好，AUC-ROC约0.904，表明非侵入性声学感知在咽喉健康监测中的可行性与潜在规模化应用。

Conclusion: 证明了非侵入式声学传感可作为实用且可扩展的临床筛查/监测工具，对早期干预和监测吞咽相关健康状况具潜在影响。

Abstract: Pharyngeal health plays a vital role in essential human functions such as breathing, swallowing, and vocalization. Early detection of swallowing abnormalities, also known as dysphagia, is crucial for timely intervention. However, current diagnostic methods often rely on radiographic imaging or invasive procedures. In this study, we propose an automated framework for detecting dysphagia using portable and noninvasive acoustic sensing coupled with applied machine learning. By capturing subtle acoustic signals from the neck during swallowing tasks, we aim to identify patterns associated with abnormal physiological conditions. Our approach achieves promising test-time abnormality detection performance, with an AUC-ROC of 0.904 under 5 independent train-test splits. This work demonstrates the feasibility of using noninvasive acoustic sensing as a practical and scalable tool for pharyngeal health monitoring.

</details>


### [159] [Vector Quantized Latent Concepts: A Scalable Alternative to Clustering-Based Concept Discovery](https://arxiv.org/abs/2602.02726)
*Xuemin Yu,Ankur Garg,Samira Ebrahimi Kahou,Hassan Sajjad*

Main category: cs.LG

TL;DR: VQLC：基于 VQ-VAE 的离散概念向量代码本，用于可扩展的后验概念解释，兼顾解释质量与计算效率，克服层次聚类和 K-Means 的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于 token 表示的概念聚类方法在大规模数据上成本高且聚类质量不稳定，需一个可扩展且保持解释质量的方法。

Method: 在 VQ-VAE 框架下学习离散代码本，将连续表征映射到概念向量；通过离散化实现更高效的概念簇化和可解释性评估。

Result: 实验表明 VQLC 在可扩展性与解释质量之间达到平衡，显著提升大规模数据集的聚类效率，同时保持可解释性的质量。

Conclusion: VQLC 提供一个可扩展且可解释的后验概念解释框架，克服传统聚类在大规模数据上的局限性。

Abstract: Deep Learning models encode rich semantic information in their hidden representations. However, it remains challenging to understand which parts of this information models actually rely on when making predictions. A promising line of post-hoc concept-based explanation methods relies on clustering token representations. However, commonly used approaches such as hierarchical clustering are computationally infeasible for large-scale datasets, and K-Means often yields shallow or frequency-dominated clusters. We propose the vector quantized latent concept (VQLC) method, a framework built upon the vector quantized-variational autoencoder (VQ-VAE) architecture that learns a discrete codebook mapping continuous representations to concept vectors. We perform thorough evaluations and show that VQLC improves scalability while maintaining comparable quality of human-understandable explanations.

</details>


### [160] [Search-Augmented Masked Diffusion Models for Constrained Generation](https://arxiv.org/abs/2602.02727)
*Huu Binh Ta,Michael Cardei,Alvaro Velasquez,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: SearchDiff: a training-free neurosymbolic inference framework that injects informed search into discrete diffusion denoising to enforce constraints and non-differentiable properties, improving feasibility and adherence.


<details>
  <summary>Details</summary>
Motivation: Traditional likelihood-based objectives in discrete diffusion optimize data fit but lack native mechanisms to enforce hard constraints or optimize non-differentiable properties at inference time.

Method:  At each denoising step, use model predictions to form a proposal set; a property-driven search optimizes this set to satisfy user-defined constraints, yielding a modified reverse transition without training.

Result: Significant improvements in constraint satisfaction and property adherence in experiments on biological design and symbolic reasoning; outperforms both discrete diffusion baselines and autoregressive models.

Conclusion: Training-free neurosymbolic diffusion enables constrained, property-aware generation without modifying the training objective.

Abstract: Discrete diffusion models generate sequences by iteratively denoising samples corrupted by categorical noise, offering an appealing alternative to autoregressive decoding for structured and symbolic generation. However, standard training targets a likelihood-based objective that primarily matches the data distribution and provides no native mechanism for enforcing hard constraints or optimizing non-differentiable properties at inference time. This work addresses this limitation and introduces Search-Augmented Masked Diffusion (SearchDiff), a training-free neurosymbolic inference framework that integrates informed search directly into the reverse denoising process. At each denoising step, the model predictions define a proposal set that is optimized under a user-specified property satisfaction, yielding a modified reverse transition that steers sampling toward probable and feasible solutions. Experiments in biological design and symbolic reasoning illustrate that SearchDiff substantially improves constraint satisfaction and property adherence, while consistently outperforming discrete diffusion and autoregressive baselines.

</details>


### [161] [CAPS: Unifying Attention, Recurrence, and Alignment in Transformer-based Time Series Forecasting](https://arxiv.org/abs/2602.02729)
*Viresh Pati,Yubin Kim,Vinh Pham,Jevon Twitty,Shihao Yang,Jiecheng Lu*

Main category: cs.LG

TL;DR: 提出 CAPS，利用时序结构分解的结构化注意力，通过时序权重 Clock 机制和 SO(2) 相位对齐来解耦全球趋势、局部冲击与季节模式，在单层注意力中结合三种门控路径，实现对长短期时间序列的高效预测，优于常规 softmax 和线性注意力，并可与多基线方法竞争。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需要同时建模全局趋势、局部突发与季节性，传统 softmax 注意力将信息全局归一化后混在一起，回归模型在长期依赖中难以保留顺序信息。

Method: 在单层注意力中引入 SO(2) 相位对齐、三条加法门控通道（Riemann softmax、prefix-product gates、Clock baseline）以及 Clock 机制（一个学习得到的时间权重），通过一个共享的时间重要性来调控各路径。

Result: 在长短期预测基准上优于标准 softmax 和线性注意力，且在与七个强基线的比较中显示竞争性表现，且具线性复杂度。

Conclusion: CAPS 提供一种解耦时间序列不同结构的高效注意力框架，能在保持线性复杂度的同时提升预测性能；代码可在 GitHub 获取。

Abstract: This paper presents $\textbf{CAPS}$ (Clock-weighted Aggregation with Prefix-products and Softmax), a structured attention mechanism for time series forecasting that decouples three distinct temporal structures: global trends, local shocks, and seasonal patterns. Standard softmax attention entangles these through global normalization, while recent recurrent models sacrifice long-term, order-independent selection for order-dependent causal structure. CAPS combines SO(2) rotations for phase alignment with three additive gating paths -- Riemann softmax, prefix-product gates, and a Clock baseline -- within a single attention layer. We introduce the Clock mechanism, a learned temporal weighting that modulates these paths through a shared notion of temporal importance. Experiments on long- and short-term forecasting benchmarks surpass vanilla softmax and linear attention mechanisms and demonstrate competitive performance against seven strong baselines with linear complexity. Our code implementation is available at https://github.com/vireshpati/CAPS-Attention.

</details>


### [162] [TabPFN for Zero-shot Parametric Engineering Design Generation](https://arxiv.org/abs/2602.02735)
*Ke Wang,Yifan Tang,Nguyen Gia Hien Vu,Faez Ahmed,G. Gary Wang*

Main category: cs.LG

TL;DR: Zero-shot parametric design generation using TabPFN; no task-specific training; few reference samples; conditions on target performance; reduces data/computation vs diffusion; validated on ship hull, BlendedNet aircraft, UIUC airfoil; shows competitive diversity and low error.


<details>
  <summary>Details</summary>
Motivation: Engineering design often relies on costly deep generative models requiring large datasets and retraining; a data-efficient, flexible approach is needed to integrate into real workflows.

Method: Employ TabPFN to perform conditional, sequential generation of design parameters conditioned on performance indicators. No task-specific training or fine-tuning; uses a few reference samples; operates in a zero-shot fashion.

Result: Achieves competitive diversity in highly structured parametric spaces; robust to variations in sampling, resolution, and parameter dimensionality; low performance error (e.g., <2% for ship hull performance); markedly lower computational/data requirements than diffusion-based models.

Conclusion: Zero-shot, data-efficient generation is a practical tool for engineering design, enabling rapid deployment, flexible adaptation to new design settings, and smoother integration into real workflows.

Abstract: Deep generative models for engineering design often require substantial computational cost, large training datasets, and extensive retraining when design requirements or datasets change, limiting their applicability in real-world engineering design workflow. In this work, we propose a zero-shot generation framework for parametric engineering design based on TabPFN, enabling conditional design generation using only a limited number of reference samples and without any task-specific model training or fine-tuning. The proposed method generates design parameters sequentially conditioned on target performance indicators, providing a flexible alternative to conventional generative models. The effectiveness of the proposed approach is evaluated on three engineering design datasets, i.e., ship hull design, BlendedNet aircraft, and UIUC airfoil. Experimental results demonstrate that the proposed method achieves competitive diversity across highly structured parametric design spaces, remains robust to variations in sampling, resolution and parameter dimensionality of geometry generation, and achieves a low performance error (e.g., less than 2% in generated ship hull designs' performance). Compared with diffusion-based generative models, the proposed framework significantly reduces computational overhead and data requirements while preserving reliable generation performance. These results highlight the potential of zero-shot, data-efficient generation as a practical and efficient tool for engineering design, enabling rapid deployment, flexible adaptation to new design settings, and ease of integration into real-world engineering workflows.

</details>


### [163] [TopoPrune: Robust Data Pruning via Unified Latent Space Topology](https://arxiv.org/abs/2602.02739)
*Arjun Roy,Prajna G. Malettira,Manish Nagaraj,Kaushik Roy*

Main category: cs.LG

TL;DR: TopoPrune: a two-scale topology-based data pruning framework that uses a topology-aware manifold approximation for global embedding and differentiable persistent homology for local optimization, achieving robust, high-precision pruning (up to 90%), with strong noise robustness and cross-architecture transferability.


<details>
  <summary>Details</summary>
Motivation: Geometric pruning methods rely on extrinsic geometry and are unstable under latent-space perturbations, causing performance drop during cross-architecture transfer and with feature noise. A topology-based approach promises stable, intrinsic data structure to enable robust pruning.

Method: A dual-scale pipeline: (1) topology-aware manifold approximation to obtain a global low-dimensional embedding; (2) differentiable persistent homology to perform local topological optimization on the embeddings, ranking samples by structural complexity.

Result: Demonstrates high accuracy and precision at large pruning rates (e.g., 90%); topology provides robustness to noise in latent features and superior transferability across diverse network architectures.

Conclusion: Topology offers a stable, principled foundation for data-efficient learning. TopoPrune exemplifies how intrinsic data topology can enable robust pruning and cross-architecture generalization.

Abstract: Geometric data pruning methods, while practical for leveraging pretrained models, are fundamentally unstable. Their reliance on extrinsic geometry renders them highly sensitive to latent space perturbations, causing performance to degrade during cross-architecture transfer or in the presence of feature noise. We introduce TopoPrune, a framework which resolves this challenge by leveraging topology to capture the stable, intrinsic structure of data. TopoPrune operates at two scales, (1) utilizing a topology-aware manifold approximation to establish a global low-dimensional embedding of the dataset. Subsequently, (2) it employs differentiable persistent homology to perform a local topological optimization on the manifold embeddings, ranking samples by their structural complexity. We demonstrate that our unified dual-scale topological approach ensures high accuracy and precision, particularly at significant dataset pruning rates (e.g., 90%). Furthermore, through the inherent stability properties of topology, TopoPrune is (a) exceptionally robust to noise perturbations of latent feature embeddings and (b) demonstrates superior transferability across diverse network architectures. This study demonstrates a promising avenue towards stable and principled topology-based frameworks for robust data-efficient learning.

</details>


### [164] [Entropy-Guided Dynamic Tokens for Graph-LLM Alignment in Molecular Understanding](https://arxiv.org/abs/2602.02742)
*Zihao Jing,Qiuhao Zeng,Ruiyi Fang,Yan Sun,Boyu Wang,Pingzhao Hu*

Main category: cs.LG

TL;DR: 提出 EDT-Former，用熵引导的动态标记Transformer，将分子图信息高效对齐到冻结的LLM，达到多项分子推理SOTA，且无需微调LLM主干。


<details>
  <summary>Details</summary>
Motivation: 现有图-LLM 桥接多使用固定长度的 Q-Former 风格标记，忽略立体化学与子结构上下文，并需要昂贵的对LLM的微调，限制效率与泛化。

Method: EDT-Former 通过熵引导动态令牌生成，针对信息量高的分子patch生成标记，与冻结的图编码器与LLM对齐；仅嵌入层可调，降低微调成本；动态标记比固定长度标记更好保留局部和全局结构。

Result: 在 MoleculeQA、Mol-Instructions、TDC、MoleculeNet 等任务上达到SOTA，证实其在可扩展和泛化的多模态分子理解中的有效性。

Conclusion: EDT-Former 提供一种高效、可扩展且不依赖LLM主干微调的多模态分子理解框架，适用于广泛的分子推理任务。

Abstract: Molecular understanding is central to advancing areas such as scientific discovery, yet Large Language Models (LLMs) struggle to understand molecular graphs effectively. Existing graph-LLM bridges often adapt the Q-Former-style connector with fixed-length static tokens, which is originally designed for vision tasks. These designs overlook stereochemistry and substructural context and typically require costly LLM-backbone fine-tuning, limiting efficiency and generalization. We introduce EDT-Former, an Entropy-guided Dynamic Token Transformer that generates tokens aligned with informative molecular patches, thereby preserving both local and global structural features for molecular graph understanding. Beyond prior approaches, EDT-Former enables alignment between frozen graph encoders and LLMs without tuning the LLM backbone (excluding the embedding layer), resulting in computationally efficient finetuning, and achieves stateof-the-art results on MoleculeQA, Molecule-oriented Mol-Instructions, and property prediction benchmarks (TDC, MoleculeNet), underscoring its effectiveness for scalable and generalizable multimodal molecular understanding

</details>


### [165] [On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning](https://arxiv.org/abs/2602.02762)
*Sacha Morin,Moonsub Byeon,Alexia Jolicoeur-Martineau,Sébastien Lachapelle*

Main category: cs.LG

TL;DR: VM-IDM 与 IDM 标注在极限条件下学习到同一策略（IDM-base 策略），IDM 学习因样本效率更高而优于行为克隆，其原因来自学习理论中的假设类复杂性与策略随机性的差异；基于此提出对现有 LAPO 的改进。


<details>
  <summary>Details</summary>
Motivation: 理解为何基于 IDM 的策略在半监督模仿学习中表现优于纯行为克隆，并 connecting VM-IDM 与 IDM 标注的关系；从统计学习理论出发解释样本效率差异，辅以实验验证并推动对 latent action policy learning 的改进。

Method: 理论上证明 VM-IDM 与 IDM 标注在某一极限下等价，形成 IDM-based 策略；基于统计学习理论分析 IDM 学习的样本效率来自较低的假设类复杂度与较小的策略随机性；通过使用统一的视频-动作预测（UVA）架构进行实验验证；在此基础上提出对 LAPO 算法的改进版本。

Result: 在极限条件下，VM-IDM 与 IDM 标注等价于 IDM-based 策略；IDM-based 策略在样本效率上优于行为克隆，原因归因于较低复杂度假设类与较低随机性；实验（含 UVA 架构）支持上述结论；提出并初步验证对 LAPO 的改进。

Conclusion: 提供一个将 VM-IDM 与 IDM 标注统一为 IDM-based 的视角，解释 IDM 学习的样本效率优势，并给出对 latent action policy learning 的改进方向及未来研究线索。

Abstract: Semi-supervised imitation learning (SSIL) consists in learning a policy from a small dataset of action-labeled trajectories and a much larger dataset of action-free trajectories. Some SSIL methods learn an inverse dynamics model (IDM) to predict the action from the current state and the next state. An IDM can act as a policy when paired with a video model (VM-IDM) or as a label generator to perform behavior cloning on action-free data (IDM labeling). In this work, we first show that VM-IDM and IDM labeling learn the same policy in a limit case, which we call the IDM-based policy. We then argue that the previously observed advantage of IDM-based policies over behavior cloning is due to the superior sample efficiency of IDM learning, which we attribute to two causes: (i) the ground-truth IDM tends to be contained in a lower complexity hypothesis class relative to the expert policy, and (ii) the ground-truth IDM is often less stochastic than the expert policy. We argue these claims based on insights from statistical learning theory and novel experiments, including a study of IDM-based policies using recent architectures for unified video-action prediction (UVA). Motivated by these insights, we finally propose an improved version of the existing LAPO algorithm for latent action policy learning.

</details>


### [166] [Exposing Vulnerabilities in Explanation for Time Series Classifiers via Dual-Target Attacks](https://arxiv.org/abs/2602.02763)
*Bohan Wang,Zewen Liu,Lu Lin,Hui Liu,Li Xiong,Ming Jin,Wei Jin*

Main category: cs.LG

TL;DR: A dual-target adversarial attack (TSEF) fools both classifier and explainer in time-series models, showing that explanation stability is not a reliable proxy for robustness.


<details>
  <summary>Details</summary>
Motivation: Why explanations are used as a robustness proxy in time-series models and how this can be exploited; need for more reliable, coupling-aware robustness evaluation.

Method: Introduce TSEF, a dual-target attack that simultaneously manipulates classifier outputs and explainer outputs to achieve targeted misclassification while keeping explanations aligned with a reference rationale.

Result: TSEF achieves targeted prediction changes across multiple datasets and explainer backbones without breaking the reference-aligned explanations, revealing that explanation stability is a misleading robustness proxy.

Conclusion: Explanation stability alone is insufficient for robustness; encourage coupling-aware evaluation of classifier and explainer, and spur development of defenses against joint adversarial manipulation.

Abstract: Interpretable time series deep learning systems are often assessed by checking temporal consistency on explanations, implicitly treating this as evidence of robustness. We show that this assumption can fail: Predictions and explanations can be adversarially decoupled, enabling targeted misclassification while the explanation remains plausible and consistent with a chosen reference rationale. We propose TSEF (Time Series Explanation Fooler), a dual-target attack that jointly manipulates the classifier and explainer outputs. In contrast to single-objective misclassification attacks that disrupt explanation and spread attribution mass broadly, TSEF achieves targeted prediction changes while keeping explanations consistent with the reference. Across multiple datasets and explainer backbones, our results consistently reveal that explanation stability is a misleading proxy for decision robustness and motivate coupling-aware robustness evaluations for trustworthy time series tasks.

</details>


### [167] [Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data](https://arxiv.org/abs/2602.02766)
*Lucas Rosenblatt,Peihan Liu,Ryan McKenna,Natalia Ponomareva*

Main category: cs.LG

TL;DR: 在差分隐私合成纵向表格数据时，避免将用户历史拉平，PATH 将完整表作为单位并用私有化微调的大型语言模型进行自回归生成，显著提升长期依赖保真度，优于边际机制。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私合成方法多假设数据独立同分布，仅关注逐条记录的边际分布，忽视纵向序列中的时间相关性，导致时间一致性不足。

Method: PATH 框架将整张表视为一个合成单位，利用私有化微调的大型语言模型的自回归能力，生成满足时序结构的完整用户轨迹，并在保护隐私的前提下保持数据可用性。

Result: 实证结果显示，PATH 将真实轨迹的分布距离降低超过60%，状态转移错误降低约50%，在边际保真度方面与主流边际机制相当。

Conclusion: 通过在完整表层面建模，PATH 能捕获传统方法错失的长程依赖，提升差分隐私合成纵向数据的保真度与可用性。

Abstract: Research on differentially private synthetic tabular data has largely focused on independent and identically distributed rows where each record corresponds to a unique individual. This perspective neglects the temporal complexity in longitudinal datasets, such as electronic health records, where a user contributes an entire (sub) table of sequential events. While practitioners might attempt to model such data by flattening user histories into high-dimensional vectors for use with standard marginal-based mechanisms, we demonstrate that this strategy is insufficient. Flattening fails to preserve temporal coherence even when it maintains valid marginal distributions. We introduce PATH, a novel generative framework that treats the full table as the unit of synthesis and leverages the autoregressive capabilities of privately fine-tuned large language models. Extensive evaluations show that PATH effectively captures long-range dependencies that traditional methods miss. Empirically, our method reduces the distributional distance to real trajectories by over 60% and reduces state transition errors by nearly 50% compared to leading marginal mechanisms while achieving similar marginal fidelity.

</details>


### [168] [Provable Effects of Data Replay in Continual Learning: A Feature Learning Perspective](https://arxiv.org/abs/2602.02767)
*Meng Ding,Jinhui Xu,Kaiyi Ji*

Main category: cs.LG

TL;DR: 全数据回放下，遗忘是否出现取决于信噪比（SNR）与任务排序；若后续任务的噪声累积超过早期信号，仍会遗忘；但若信号足够累积，回放可恢复早期任务，且将高信号任务优先排序有助于同时学习低信号任务并降低遗忘。


<details>
  <summary>Details</summary>
Motivation: 揭示全数据回放在持续学习中的理论有效性与边界。当前多任务全数据可访问性的理论研究不足，需从特征学习视角建立框架，明确SNR对遗忘的作用。

Method: 采用多视角数据模型，聚焦M个任务的增量二分类问题，分析信噪比（SNR）作为核心因素对遗忘的影响，给出在不同SNR与任务相关性下的遗忘与恢复条件，并考察任务排序对学习的影响。

Result: 给出两大结论：(1) 当后续任务的累计噪声支配早期任务的信号时，仍会发生遗忘；(2) 当信号充分积累时，数据回放可恢复早期任务，即使其初始学习很差。还揭示了一个关于任务排序的新洞见：优先学习高信号任务不仅促进对低信号任务的学习，也有助于防止遗忘。通过合成与真实数据实验，直观展示了在不同SNR和任务相关性条件下信号学习与噪记忆的交互。

Conclusion: 提出一个理论框架来解释全数据回放在持续学习中的有效性边界，强调SNR与任务排序的重要性，并为回放策略设计提供实证和理论依据，同时给出关于任务排序的新见解，具备可验证性。

Abstract: Continual learning (CL) aims to train models on a sequence of tasks while retaining performance on previously learned ones. A core challenge in this setting is catastrophic forgetting, where new learning interferes with past knowledge. Among various mitigation strategies, data-replay methods, where past samples are periodically revisited, are considered simple yet effective, especially when memory constraints are relaxed. However, the theoretical effectiveness of full data replay, where all past data is accessible during training, remains largely unexplored. In this paper, we present a comprehensive theoretical framework for analyzing full data-replay training in continual learning from a feature learning perspective. Adopting a multi-view data model, we identify the signal-to-noise ratio (SNR) as a critical factor affecting forgetting. Focusing on task-incremental binary classification across $M$ tasks, our analysis verifies two key conclusions: (1) forgetting can still occur under full replay when the cumulative noise from later tasks dominates the signal from earlier ones; and (2) with sufficient signal accumulation, data replay can recover earlier tasks-even if their initial learning was poor. Notably, we uncover a novel insight into task ordering: prioritizing higher-signal tasks not only facilitates learning of lower-signal tasks but also helps prevent catastrophic forgetting. We validate our theoretical findings through synthetic and real-world experiments that visualize the interplay between signal learning and noise memorization across varying SNRs and task correlation regimes.

</details>


### [169] [VerIde ECG Biometrics: Verification and Identification](https://arxiv.org/abs/2602.02776)
*Scagnetto Arjuna*

Main category: cs.LG

TL;DR: ECG生物识别在大规模数据中展现出明确的个人指纹：仅基于表格特征也能实现可观的识别性能，嵌入式深度模型（从特征到波形）的显著提升表明隐私风险在不断加剧，需对ECG数据的匿名化与使用策略进行实质性隐私保护考量。


<details>
  <summary>Details</summary>
Motivation: 评估ECG在个人身份识别中的信息暴露程度，以及自简单特征到嵌入式深度模型在隐私攻击中的性能演变，实证大型数据场景下的可识别性。

Method: 首先在fiducial特征（表格特征）上训练MLP嵌入，评估基础隐私泄露；随后在特征与波形层面应用ArcFace等嵌入式深度模型，比较输入类型、数据规模与训练/评估分布的一致性；在大规模测试集上进行闭集与开放集评估（包括Top-K筛选+再排序的两阶段流程），给出各类指标。

Result: 在大规模测试集上的结果显示：闭集验证的TAR@FAR=1e-3为0.908、FAR=1e-4为0.820，全体对全体的EER为2.53%；Rank@1=0.812、Rank@10=0.910。开放集通过两阶段Pipeline（嵌入Top-K候选+再排序）实现DIR@FAR在1e-3与1e-4时均可达0.976。总体表明ECG携带可量化的个人指纹，基于嵌入的重识别能力随输入从表征特征向波形转移并随数据规模与归一化策略改善而增强。

Conclusion: ECG中的个人身份信号可被有效捕捉，单纯释放表格特征不足以保障隐私，嵌入模型的强大识别能力意味着在实际应用中需要更强的隐私保护措施与运营协议（如更严的数据最小化、差分隐私、联邦学习、去标识化策略等）来降低被识别的风险。

Abstract: This work studies electrocardiogram (ECG) biometrics at large scale, evaluating how strongly an ECG can be linked to an individual and, consequently, how its anonymization may be compromised. We show that identity information is already present in tabular representations (fiducial features): even a simple MLP-based embedding network yields non-trivial performance, indicating that anonymization based solely on releasing features does not guarantee privacy. We then adopt embedding-based deep learning models (ArcFace), first on features and then on ECG waveforms, showing a performance jump when moving from tabular inputs to waveforms, and a further gain with larger training sets and consistent normalization across train/val/test. On a large-scale test set, verification achieves high TAR at strict FAR thresholds (TAR=0.908 @ FAR=1e-3; TAR=0.820 @ FAR=1e-4) with EER=2.53% (all-vs-all); closed-set identification yields Rank@1=0.812 and Rank@10=0.910. In open-set, a two-stage pipeline (top-K shortlist on embeddings + re-ranking) reaches DIR@FAR up to 0.976 at FAR=1e-3 and 1e-4. Overall, the results show that ECG carries a measurable individual signature: re-identification is already possible with tabular features and is further amplified by embedding-based models, making privacy implications and realistic operational protocols essential to consider.

</details>


### [170] [Cross-Temporal Attention Fusion (CTAF) for Multimodal Physiological Signals in Self-Supervised Learning](https://arxiv.org/abs/2602.02784)
*Arian Khorasani,Théophile Demazure*

Main category: cs.LG

TL;DR: CTAF: Cross-Temporal Attention Fusion for asynchronous EEG and peripheral physiology with self-supervised alignment, achieving better cross-modal alignment and competitive classification on K-EmoCon with few labels.


<details>
  <summary>Details</summary>
Motivation: Identify and model temporal asynchrony between EEG and peripheral signals; improve multimodal fusion without costly warping; leverage alignment-driven objectives and weak supervision.

Method: Cross-Temporal Attention Fusion module; self-supervised learning; soft bidirectional alignments; time-aware cross attention; lightweight fusion gate; alignment-regularized contrastive objectives; optional weak supervision.

Result: On K-EmoCon with leave-one-out CV: higher cosine margins for matched pairs; better cross-modal token retrieval within one second; competitive with baseline on three-bin accuracy and macro-F1 with few labels.

Conclusion: Proposes time-aware fusion mechanism; alignment-driven self-supervised objective; alignment-quality evaluation protocol; demonstrates coupling of CNS signals; contributes to label-efficient, generalizable EEG-peripheral fusion under temporal asynchrony.

Abstract: We study multimodal affect modeling when EEG and peripheral physiology are asynchronous, which most fusion methods ignore or handle with costly warping. We propose Cross-Temporal Attention Fusion (CTAF), a self-supervised module that learns soft bidirectional alignments between modalities and builds a robust clip embedding using time-aware cross attention, a lightweight fusion gate, and alignment-regularized contrastive objectives with optional weak supervision. On the K-EmoCon dataset, under leave-one-out cross-validation evaluation, CTAF yields higher cosine margins for matched pairs and better cross-modal token retrieval within one second, and it is competitive with the baseline on three-bin accuracy and macro-F1 while using few labels. Our contributions are a time-aware fusion mechanism that directly models correspondence, an alignment-driven self-supervised objective tailored to EEG and physiology, and an evaluation protocol that measures alignment quality itself. Our approach accounts for the coupling between the central and autonomic nervous systems in psychophysiological time series. These results indicate that CTAF is a strong step toward label-efficient, generalizable EEG-peripheral fusion under temporal asynchrony.

</details>


### [171] [Structure-Preserving Learning Improves Geometry Generalization in Neural PDEs](https://arxiv.org/abs/2602.02788)
*Benjamin D. Shaffer,Shawn Koohy,Brooks Kinch,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: Geo-NeW是一种数据驱动的有限元方法，通过联学习微分算子与兼容的有限元空间，在几何上进行编码，利用有限元外微分计算保持物理守恒，实现在未见几何上的自适应、近实时PDE解。对稳态PDE基准取得SOTA，并在OOD几何上显著优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 需要在不见几何的情况下提供实时且结构保持的PDE求解，同时兼顾跨域泛化与物理守恒。

Method: 提出General-Geometry Neural Whitney Forms (Geo-NeW)，联合学习差分算子与兼容的降阶有限元空间；利用有限元外微分保持守恒；几何信息通过网格的Transformer编码和作为有限元基空间基础；参数化本构模型以确保解的存在性和唯一性；通过求解离散系统生成预测。

Result: 在若干稳态PDE基准上达到与或超越SOTA，并在OOD几何上显著优于传统基线。

Conclusion: 将几何感知神经PDE与结构保真有限元方法结合，提供高精度、可泛化且保持物理守恒的解法，具备在科学与工程领域的潜在应用。

Abstract: We aim to develop physics foundation models for science and engineering that provide real-time solutions to Partial Differential Equations (PDEs) which preserve structure and accuracy under adaptation to unseen geometries. To this end, we introduce General-Geometry Neural Whitney Forms (Geo-NeW): a data-driven finite element method. We jointly learn a differential operator and compatible reduced finite element spaces defined on the underlying geometry. The resulting model is solved to generate predictions, while exactly preserving physical conservation laws through Finite Element Exterior Calculus. Geometry enters the model as a discretized mesh both through a transformer-based encoding and as the basis for the learned finite element spaces. This explicitly connects the underlying geometry and imposed boundary conditions to the solution, providing a powerful inductive bias for learning neural PDEs, which we demonstrate improves generalization to unseen domains. We provide a novel parameterization of the constitutive model ensuring the existence and uniqueness of the solution. Our approach demonstrates state-of-the-art performance on several steady-state PDE benchmarks, and provides a significant improvement over conventional baselines on out-of-distribution geometries.

</details>


### [172] [Causality--Δ: Jacobian-Based Dependency Analysis in Flow Matching Models](https://arxiv.org/abs/2602.02793)
*Reza Rezvan,Gustav Gille,Moritz Schauer,Richard Torkar*

Main category: cs.LG

TL;DR: Jacobian-vector products (JVPs) are a practical lens to inspect dependency structure in flow-based models, revealing local affine structure in globally nonlinear flows. The paper provides closed-form solutions for the optimal drift and its Jacobian in Gaussian and mixture-of-Gaussians, with empirical validation in low-dimensional synthetic settings. In image domains, an attribute-classifier-guided JVP estimator recovers empirical correlations on MNIST and CelebA; conditioning on small classifier-Jacobian norms reduces correlations consistent with a common-cause interpretation, though this is not a formal do-intervention.


<details>
  <summary>Details</summary>
Motivation: To understand how small latent perturbations propagate through flow-based models and to reveal the dependency structure of generated features via Jacobian-vector products (JVPs).

Method: Derive closed-form expressions for the optimal drift and its Jacobian in Gaussian and mixture-of-Gaussian base distributions; use JVPs to diagnose dependency structure. In image domains, compose the flow with an attribute classifier to obtain an attribute-level JVP estimator and examine correlations; study the effect of conditioning on classifier-Jacobian norms.

Result: Even globally nonlinear flows exhibit local affine structure; numerical JVPs recover analytical Jacobians in low-dimensional benchmarks; attribute-level JVPs capture empirical correlations on MNIST and CelebA; conditioning on small classifier-Jacobian norms reduces correlations consistent with a common-cause structure, without performing a formal intervention.

Conclusion: JVPs offer a practical diagnostic tool for dependency structure in flow models, providing analytic baselines in Gaussian/MoG settings and a viable estimator in image domains. Conditioning on classifier-Jacobian norms can mitigate spurious correlations in line with a common-cause narrative, though it is not a causal do-intervention.

Abstract: Flow matching learns a velocity field that transports a base distribution to data. We study how small latent perturbations propagate through these flows and show that Jacobian-vector products (JVPs) provide a practical lens on dependency structure in the generated features. We derive closed-form expressions for the optimal drift and its Jacobian in Gaussian and mixture-of-Gaussian settings, revealing that even globally nonlinear flows admit local affine structure. In low-dimensional synthetic benchmarks, numerical JVPs recover the analytical Jacobians. In image domains, composing the flow with an attribute classifier yields an attribute-level JVP estimator that recovers empirical correlations on MNIST and CelebA. Conditioning on small classifier-Jacobian norms reduces correlations in a way consistent with a hypothesized common-cause structure, while we emphasize that this conditioning is not a formal do intervention.

</details>


### [173] [Joint Learning of Hierarchical Neural Options and Abstract World Model](https://arxiv.org/abs/2602.02799)
*Wasu Top Piriyakulkij,Wolfgang Lehrach,Kevin Ellis,Kevin Murphy*

Main category: cs.LG

TL;DR: AgentOWL同時學習抽象世界模型與分層神經選項，以實現高效的技能組合與學習，提升樣本效率。是在Object-Centric Atari子集上的實驗，與基線相比能用更少數據學習到更多技能。


<details>
  <summary>Details</summary>
Motivation: 現有的模型無關的層次化強化學習往往需要大量數據，且技能組合的長期依賴性使學習困難。透過同時學習抽象世界模型，可跨狀態與時間抽象，提升對技能序列的樣本效率與可轉移性。

Method: 提出AgentOWL，聯合學習一個抽象世界模型（跨越狀態與時間的抽象）與一組分層神經選項，實現技能的分層構成與長期規劃能力。該方法可能整合對象中心表徵以提升穩定性與樣本效率。

Result: 在Object-Centric Atari的子集上，該方法顯示比基線方法需要更少數據就能學出更多技能，證明了抽象世界模型與分層選項的聯合學習在樣本效率上的優勢。

Conclusion: 抽象世界模型與分層選項的聯合學習能顯著提升技能獲取的樣本效率，具有將技能組合推廣到更複雜任務的潛力，並對面向對象的環境具有良好適用性。

Abstract: Building agents that can perform new skills by composing existing skills is a long-standing goal of AI agent research. Towards this end, we investigate how to efficiently acquire a sequence of skills, formalized as hierarchical neural options. However, existing model-free hierarchical reinforcement algorithms need a lot of data. We propose a novel method, which we call AgentOWL (Option and World model Learning Agent), that jointly learns -- in a sample efficient way -- an abstract world model (abstracting across both states and time) and a set of hierarchical neural options. We show, on a subset of Object-Centric Atari games, that our method can learn more skills using much less data than baseline methods.

</details>


### [174] [Membership Inference Attacks from Causal Principles](https://arxiv.org/abs/2602.02819)
*Mathieu Even,Clément Berenfeld,Linus Bleistein,Tudor Cebere,Julie Josse,Aurélien Bellet*

Main category: cs.LG

TL;DR: 将误记忆(MIA)评估框架化为因果推断问题，将数据点是否包含在训练集视为处理因果效应；提出适用于多次训练、单次训练、零次训练等场景的因果类指标及可实证估计量，并给出非渐近一致性保证，实验表明在无法重新训练或分布转移下也能提供可靠的记忆度测量。


<details>
  <summary>Details</summary>
Motivation: 现有的MIA评估往往需重复重新训练，成本高，且单次/零次评估在统计有效性上存在争议。需要一个统一的因果框架来理解偏差来源并提供稳健的评估方法。

Method: 将数据点包含与否的效应定义为记忆化的因果效应；推导标准MIA指标的因果类等价物；提出适用于多次训练、单次训练与零次训练的实用估计量，且具备非渐近一致性保证。

Result: 在真实数据集上验证，所提方法在无法重新训练及存在分布转移时仍可提供可靠的记忆度测量，增强了隐私评估的理论基础与实用性。

Conclusion: 为现代AI系统的隐私评估提供一个原理完备的因果框架，明确了现有评估方法的偏差来源，并给出可操作的稳健估计工具。

Abstract: Membership Inference Attacks (MIAs) are widely used to quantify training data memorization and assess privacy risks. Standard evaluation requires repeated retraining, which is computationally costly for large models. One-run methods (single training with randomized data inclusion) and zero-run methods (post hoc evaluation) are often used instead, though their statistical validity remains unclear. To address this gap, we frame MIA evaluation as a causal inference problem, defining memorization as the causal effect of including a data point in the training set. This novel formulation reveals and formalizes key sources of bias in existing protocols: one-run methods suffer from interference between jointly included points, while zero-run evaluations popular for LLMs are confounded by non-random membership assignment. We derive causal analogues of standard MIA metrics and propose practical estimators for multi-run, one-run, and zero-run regimes with non-asymptotic consistency guarantees. Experiments on real-world data show that our approach enables reliable memorization measurement even when retraining is impractical and under distribution shift, providing a principled foundation for privacy evaluation in modern AI systems.

</details>


### [175] [From Tokens to Numbers: Continuous Number Modeling for SVG Generation](https://arxiv.org/abs/2602.02820)
*Michael Ogezi,Martin Bell,Freda Shi,Ethan Smith*

Main category: cs.LG

TL;DR: 提出连续数字建模(CNM)以直接对SVG等向量图的数值参数进行连续建模，替代离散token编码，从而提升训练速度和感知保真度；通过在200万对 raster→SVG 样本上训练，并采用带感知反馈的强化学习微调，达到>30%训练加速且保真度提升的结果。


<details>
  <summary>Details</summary>
Motivation: 向量图（如 SVG）的数值几何参数通常以长序列 token 编码，导致训练慢、准确性低、泛化差。需要一种能直接建模连续数值、避免离散化伪影的方法。

Method: 提出 CNM，将数值参数作为连续值处理；在2百万对 raster→SVG 样本上训练一个多模态 transformer，并通过感知反馈的强化学习进行微调以提升视觉质量。

Result: 训练速度提升超过30%，感知保真度高于替代方法；证明 CNM 在高质量向量生成中的可行性与高效性。

Conclusion: CNM 为高质量向量生成提供一种实用、有效的直接数值建模方法，具有广泛应用潜力；代码可复现。

Abstract: For certain image generation tasks, vector graphics such as Scalable Vector Graphics (SVGs) offer clear benefits such as increased flexibility, size efficiency, and editing ease, but remain less explored than raster-based approaches. A core challenge is that the numerical, geometric parameters, which make up a large proportion of SVGs, are inefficiently encoded as long sequences of tokens. This slows training, reduces accuracy, and hurts generalization. To address these problems, we propose Continuous Number Modeling (CNM), an approach that directly models numbers as first-class, continuous values rather than discrete tokens. This formulation restores the mathematical elegance of the representation by aligning the model's inputs with the data's continuous nature, removing discretization artifacts introduced by token-based encoding. We then train a multimodal transformer on 2 million raster-to-SVG samples, followed by fine-tuning via reinforcement learning using perceptual feedback to further improve visual quality. Our approach improves training speed by over 30% while maintaining higher perceptual fidelity compared to alternative approaches. This work establishes CNM as a practical and efficient approach for high-quality vector generation, with potential for broader applications. We make our code available http://github.com/mikeogezi/CNM.

</details>


### [176] [A Single Revision Step Improves Token-Efficient LLM Reasoning](https://arxiv.org/abs/2602.02828)
*Yingchuan Zhang,Terry Ma,Wenxuan Zhong,Ping Ma*

Main category: cs.LG

TL;DR: PACER: 通过一个共识数据包在推理轨迹之间进行自我审查与修订的训练无关、仅推理阶段的方法，在难题上（如AIME、BRUMO）能达到或超过256样本多数投票的准确性，显著优于原始集成基线。


<details>
  <summary>Details</summary>
Motivation: 传统的集成方法在每条轨迹单独评估时存在“盲点”，易被高置信度的错解路径误导，导致真正解被边缘化。需要跨轨迹的协同评审以纠正近似错误。

Method: PACER在初步筛选后构造一个紧凑的共识数据包，包含（i）唯一候选答案、（ii）聚合置信度、（iii）每个候选答案的代表性推理摘要。之后每条轨迹在给定该数据包的条件下进行目标化的自我审查，识别与共识的分歧点并在原始推理有缺陷时进行Pivot。最终通过对修订轨迹的置信度加权投票得到预测。该过程不需要额外训练，属于推理阶段的框架。

Result: 在AIME和BRUMO等 challenging 竞赛数学基准上，PACER达到或超过256样本多数投票的准确性，并通过将简单共识转化为协同的逻辑精炼过程显著优于原始集成基线。

Conclusion: PACER提供一种可扩展且无训练成本的方式，通过让推理轨迹相互“同行评审”，在多轨迹推理中提升准确性，尤其适用于高难度的推理任务。

Abstract: Large language models (LLMs) achieve higher accuracy on challenging reasoning tasks by scaling test-time compute through multiple trajectory sampling. However, standard aggregation methods like majority voting or individual confidence-based filtering face a fundamental "blind spot": they evaluate each trace in isolation. As problems scale in difficulty, models often generate hallucinated paths that exhibit misleadingly high confidence, causing the true solution to be suppressed by a narrow margin in traditional voting. We ask: can we enable traces to "peer-review" each other to resolve these near-miss errors?
  We introduce Packet-Conditioned Revision (PACER), a training-free, inference-only framework that enables reasoning traces to revise their conclusions through a structured coordination step. After a preliminary screening of generated traces, PACER constructs a compact consensus packet containing (i) unique candidate answers, (ii) their aggregated confidence scores, and (iii) representative reasoning summaries for each candidate answer. Individual traces then perform a targeted self-review conditioned on this packet, allowing them to identify specific logical junctions where they diverged from the broader consensus and pivot if their original reasoning is found to be flawed. Final predictions are obtained via confidence-weighted voting over these revised trajectories. On challenging competitive math benchmarks such as AIME and BRUMO, PACER matches or exceeds the accuracy of 256-sample majority voting, significantly outperforming raw ensemble baselines by transforming simple consensus into a collaborative logical refinement process.

</details>


### [177] [SC3D: Dynamic and Differentiable Causal Discovery for Temporal and Instantaneous Graphs](https://arxiv.org/abs/2602.02830)
*Sourajit Das,Dibyajyoti Chakraborthy,Romit Maulik*

Main category: cs.LG

TL;DR: SC3D是一种两阶段可微分框架，用于从多变量时间序列中发现带滞后与即时依赖的因果结构，结合边际预选与精炼且具稀疏性和即时DAG约束的优化。


<details>
  <summary>Details</summary>
Motivation: 解决在多变量时间序列中同时存在滚动滞后与即时依赖时，因果结构的组合搜索空间过于庞大且不稳定的问题；需要一个可微、稳定且能同时捕捉滞后和即时关系的框架。

Method: Stage 1：通过节点级预测进行边的预选，得到滞后边和即时边的掩码；Stage 2：通过最大似然优化并引入稀疏性约束，以及对即时部分强制实现有向无环性（DAG），同时学习不同滞后的邻接矩阵。该框架为可微分并分阶段处理预测与结构学习。

Result: 在合成数据和基准动态系统上的数值实验表明，SC3D在稳定性和对滞后及即时因果结构的恢复精度上，相较于现有的时序基线具有优势。

Conclusion: 提出的SC3D框架通过两阶段可微分设计，成功联合学习滞后与即时的因果结构，增强了结构发现的稳定性与准确性，适用于存在即时依赖的多变量时间序列。

Abstract: Discovering causal structures from multivariate time series is a key problem because interactions span across multiple lags and possibly involve instantaneous dependencies. Additionally, the search space of the dynamic graphs is combinatorial in nature. In this study, we propose \textit{Stable Causal Dynamic Differentiable Discovery (SC3D)}, a two-stage differentiable framework that jointly learns lag-specific adjacency matrices and, if present, an instantaneous directed acyclic graph (DAG). In Stage 1, SC3D performs edge preselection through node-wise prediction to obtain masks for lagged and instantaneous edges, whereas Stage 2 refines these masks by optimizing a likelihood with sparsity along with enforcing acyclicity on the instantaneous block. Numerical results across synthetic and benchmark dynamical systems demonstrate that SC3D achieves improved stability and more accurate recovery of both lagged and instantaneous causal structures compared to existing temporal baselines.

</details>


### [178] [Tabula RASA: Exposing and Breaking the Relational Bottleneck in Transformers](https://arxiv.org/abs/2602.02834)
*Jonas Petersen,Camilla Mazzoleni,Riccardo Maggioni*

Main category: cs.LG

TL;DR: 引入两点改造的关系感知稀疏注意：RASA 在注意力中加入边类型嵌入与稀疏掩码以利用图结构进行多跳推理。理论上保持同等深度需求，但通过稀疏掩码显著减小注意力搜索空间；在 MetaQA 和 WebQuestionsSP 上实现比标准变换器更优，且与 GPT-4 相比在成本较低的情况下达到相近表现，3 跳推理提升显著（+7.1 点）。未给出正式可学习性保证，属于经验性验证。


<details>
  <summary>Details</summary>
Motivation: 解决变换器在需要对结构化数据进行多跳关系推理时的局限性，结合计算复杂性视角（TC^0）分析，以最小结构修改探究推理能力的提升与可行性。

Method: 在自注意力中添加：1) 边类型嵌入以在注意力分数中引入关系结构；2) 稀疏掩码限定注意力仅在图的相邻位置进行。理论分析给出深度需求仍为 Ω(k)（k 跳推理），但注意力搜索空间从 O(2^{n^2}) 缩减至 O(2^m)。在 MetaQA（1/2/3 跳）和 WebQuestionsSP 上进行实验，比较标准 Transformer 和 GPT-4，且对比成本。

Result: 理论上维持同等深度复杂性，但通过稀疏 masking 减少搜索空间，边类型嵌入提供关系路由；实证上在目标数据集上优于基线 Transformer，并接近 GPT-4 的性能但成本更低，且深度（跳数）增加时优势扩大（3 跳提升约 7.1 点）。

Conclusion: 结论强调：最小化的结构修改即可显著提升多跳推理性能，具备实证支撑；然而未声明可学习性证明，需更多实验与理论分析以检验可推广性与稳健性。

Abstract: Transformers achieve remarkable performance across many domains, yet struggle with tasks requiring multi-hop relational reasoning over structured data. We analyze this limitation through circuit complexity: standard transformers are $\mathsf{TC}^0$-complete and require $Ω(k)$ layers for $k$-hop reasoning. We introduce RASA (Relation-Aware Sparse Attention), a minimal modification adding: (1) edge-type embeddings that inject relational structure into attention scores, and (2) sparse masking that restricts attention to graph-adjacent positions. While RASA has the same asymptotic depth requirements, sparse masking reduces the attention search space from $O(2^{n^2})$ to $O(2^m)$ patterns, and edge biases provide explicit relation routing. Empirically, on MetaQA (1/2/3-hop) and WebQuestionsSP, RASA outperforms standard transformers and matches GPT-4 at lower cost, with advantages growing with reasoning depth (+7.1 points on 3-hop). We do not claim formal learnability guarantees; the contribution is empirical validation that minimal structural modifications substantially improve multi-hop reasoning.

</details>


### [179] [Self-Hinting Language Models Enhance Reinforcement Learning](https://arxiv.org/abs/2602.03143)
*Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian*

Main category: cs.LG

TL;DR: SAGE introduces privileged hints during training to prevent GRPO's stagnation under sparse terminal rewards by reshaping rollout distribution without changing the final task reward; test-time uses no-hint policy; diverse self-hints form an adaptive curriculum; empirically improves GRPO across 6 benchmarks and 3 LLMs; code released.


<details>
  <summary>Details</summary>
Motivation: GRPO suffers from stalled learning under sparse terminal rewards because many rollouts receive identical rewards, causing relative advantages to collapse. A method that reshapes the rollout distribution with privileged information while keeping the same final reward can sustain learning signal.

Method: Propose self-hint aligned GRPO with privileged supervision (SAGE). For each prompt x, sample a compact hint h (e.g., plan or decomposition) and generate a solution τ conditioned on (x,h). The task reward R(x,τ) remains unchanged; hints diversify within-group outcomes under finite sampling, preventing GRPO advantages from collapsing. At test time, use h=⊘ (no hints). Training uses on-policy RL with privileged hints; sampling diverse self-hints provides an adaptive curriculum that better targets bottlenecks than fixed initial-model hints or stronger external models.

Result: Empirical evaluation on 6 benchmarks using 3 LLMs shows SAGE consistently outperforms GRPO: +2.0 average on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct, and +1.3 on Qwen3-4B-Instruct. Code available at the provided GitHub link.

Conclusion: SAGE effectively mitigates GRPO stagnation under sparse rewards by injecting diverse self-hints during training, creating an adaptive curriculum without altering the terminal verifier reward; test-time no-hint policy retains simplicity. The approach yields consistent gains across multiple models and benchmarks.

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt $x$, the model samples a compact hint $h$ (e.g., a plan or decomposition) and then generates a solution $τ$ conditioned on $(x,h)$. Crucially, the task reward $R(x,τ)$ is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set $h=\varnothing$ and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.

</details>


### [180] [Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains](https://arxiv.org/abs/2602.02841)
*Jae-Sung Bae,Minje Kim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite strong performance in data-rich regimes, deep learning often underperforms in the data-scarce settings common in practice. While foundation models (FMs) trained on massive datasets demonstrate strong generalization by extracting general-purpose features, they can still suffer from scarce labeled data during downstream fine-tuning. To address this, we propose GeLDA, a semantics-aware generative latent data augmentation framework that leverages conditional diffusion models to synthesize samples in an FM-induced latent space. Because this space is low-dimensional and concentrates task-relevant information compared to the input space, GeLDA enables efficient, high-quality data generation. GeLDA conditions generation on auxiliary feature vectors that capture semantic relationships among classes or subdomains, facilitating data augmentation in low-resource domains. We validate GeLDA in two large-scale recognition tasks: (a) in zero-shot language-specific speech emotion recognition, GeLDA improves the Whisper-large baseline's unweighted average recall by 6.13%; and (b) in long-tailed image classification, it achieves 74.7% tail-class accuracy on ImageNet-LT, setting a new state-of-the-art result.

</details>


### [181] [Causal Flow Q-Learning for Robust Offline Reinforcement Learning](https://arxiv.org/abs/2602.02847)
*Mingxuan Li,Junzhe Zhang,Elias Bareinboim*

Main category: cs.LG

TL;DR: 基于因果离线RL的鲁棒策略，针对像素演示中的混淆偏差，提出最坏-case优化和深度判别器辅助的flow-matching策略，在25个像素任务中相较未考虑混淆的SOTA离线RL提升约1.2倍成功率。


<details>
  <summary>Details</summary>
Motivation: 像素演示导致的未观测混淆偏差使标准策略梯度假设失效，因此需要从因果角度建模并对抗潜在偏差以提升离线RL鲁棒性。

Method: 提出一个因果离线RL目标，优化策略在最坏情形下的表现；基于flow-matching的表达性策略实现，并通过深度判别器评估目标策略与名义行为策略之间的分布差异，进行鲁棒增强，学习来自混淆演示的策略。

Result: 在25个像素任务上的实验显示，所提出的混淆鲁棒增强方法相较于混淆未考虑的SOTA离线RL方法，成功率提升约120%（1.2x）。

Conclusion: 从因果视角处理观测混淆可显著提升像素级演示数据的离线RL鲁棒性，所提出的目标与实现具有较好的推广性，适用于其他表达性策略和鲁棒离线RL场景。

Abstract: Expressive policies based on flow-matching have been successfully applied in reinforcement learning (RL) more recently due to their ability to model complex action distributions from offline data. These algorithms build on standard policy gradients, which assume that there is no unmeasured confounding in the data. However, this condition does not necessarily hold for pixel-based demonstrations when a mismatch exists between the demonstrator's and the learner's sensory capabilities, leading to implicit confounding biases in offline data. We address the challenge by investigating the problem of confounded observations in offline RL from a causal perspective. We develop a novel causal offline RL objective that optimizes policies' worst-case performance that may arise due to confounding biases. Based on this new objective, we introduce a practical implementation that learns expressive flow-matching policies from confounded demonstrations, employing a deep discriminator to assess the discrepancy between the target policy and the nominal behavioral policy. Experiments across 25 pixel-based tasks demonstrate that our proposed confounding-robust augmentation procedure achieves a success rate 120\% that of confounding-unaware, state-of-the-art offline RL methods.

</details>


### [182] [DynSplit-KV: Dynamic Semantic Splitting for KVCache Compression in Efficient Long-Context LLM Inference](https://arxiv.org/abs/2602.03184)
*Jiancai Ye,Jun Liu,Qingchen Li,Tianlang Zhao,Hanbin Zhang,Jiayi Pan,Ningyi Xu,Guohao Dai*

Main category: cs.LG

TL;DR: Dynamic, importance-aware delimiter selection with uniform mapping for DynSplit-KV adapts splitting to semantic boundaries, achieving higher accuracy and lower overhead than rigid methods.


<details>
  <summary>Details</summary>
Motivation: Long-context KVCache memory growth becomes a bottleneck for LLM inference. Rigid splitting (fixed intervals or predefined delimiters) misaligns semantics with KVCache, causing accuracy loss and high overhead; a need exists for dynamic semantic splitting.

Method: DynSplit-KV introduces (1) a dynamic importance-aware delimiter selection strategy to align splits with semantic boundaries, and (2) a uniform mapping strategy to convert variable-length semantic blocks into a fixed-length format, reducing overhead.

Result: The approach yields the highest accuracy among comparisons, with a 49.9% improvement from the delimiter strategy. It achieves 2.2x speedup over FlashAttention and 2.6x peak memory reduction in long-context scenarios; uniform mapping reduces inference overhead by 4.9x.

Conclusion: Dynamic, semantics-aware splitting plus fixed-length mapping significantly improves KVCache compression performance, surpassing rigid splitting methods in accuracy and efficiency; the method offers strong potential for long-context LLM inference.

Abstract: Although Key-Value (KV) Cache is essential for efficient large language models (LLMs) inference, its growing memory footprint in long-context scenarios poses a significant bottleneck, making KVCache compression crucial. Current compression methods rely on rigid splitting strategies, such as fixed intervals or pre-defined delimiters. We observe that rigid splitting suffers from significant accuracy degradation (ranging from 5.5% to 55.1%) across different scenarios, owing to the scenario-dependent nature of the semantic boundaries. This highlights the necessity of dynamic semantic splitting to match semantics. To achieve this, we face two challenges. (1) Improper delimiter selection misaligns semantics with the KVCache, resulting in 28.6% accuracy loss. (2) Variable-length blocks after splitting introduce over 73.1% additional inference overhead. To address the above challenges, we propose DynSplit-KV, a KVCache compression method that dynamically identifies delimiters for splitting. We propose: (1) a dynamic importance-aware delimiter selection strategy, improving accuracy by 49.9%. (2) A uniform mapping strategy that transforms variable-length semantic blocks into a fixed-length format, reducing inference overhead by 4.9x. Experiments show that DynSplit-KV achieves the highest accuracy, 2.2x speedup compared with FlashAttention and 2.6x peak memory reduction in long-context scenarios.

</details>


### [183] [Zero Sum SVD: Balancing Loss Sensitivity for Low Rank LLM Compression](https://arxiv.org/abs/2602.02848)
*Ali Abbasi,Chayne Thrash,Haoran Qin,Shansita Sharma,Sepehr Seifi,Soheil Kolouri*

Main category: cs.LG

TL;DR: ZS-SVD 提出一种全局零和约束的 SVD 截断方法，在后训练阶段通过激活白化和一阶校准损失估计实现对全模型的全局组件选择，自动生成非均匀的秩分配，且可选单次投影梯度修正以提升效果。


<details>
  <summary>Details</summary>
Motivation: 解决针对全局压缩比例的秩分配在不同矩阵的损失敏感性差异导致的性能下降，以及现有方法通常需要昂贵的迭代预截断优化来确定每个矩阵的秩。

Method: 在 whitened 坐标中基于全局的奇异值成分选择，使用激活白化和一阶校准损失近似来评估每个组件的贡献，应用“零和”规则使累计预测损失变化近似为零，从而获得非均匀且自适应的秩分配；可选地在截断后执行一个轻量级的投影梯度更新再重新截断。

Result: 在多种大语言模型架构和基准上实现一致性提升，且在不同压缩比下表现稳定，代码公开。

Conclusion: 提供一种无需复杂秩优化的全局 SVD 摘取方法，易于实现且对不同模型具有良好适用性与泛化潜力；可作为后训练压缩的有效手段。

Abstract: Advances in large language models have driven strong performance across many tasks, but their memory and compute costs still hinder deployment. SVD-based compression reduces storage and can speed up inference via low-rank factors, yet performance depends on how rank is allocated under a global compression ratio. Prior methods often use homogeneous ranks for similarly sized matrices, despite large differences in loss sensitivity, or rely on expensive iterative pre-truncation optimization to determine per matrix ranks. We propose \textbf{Zero Sum SVD} (\textbf{ZS-SVD}), a post-training method that performs \emph{global} singular component selection using activation whitening and first-order calibration loss estimates in whitened coordinates. \textbf{ZS-SVD} prunes components across the whole model with a \textbf{zero sum} rule that keeps the cumulative predicted loss change near zero, automatically yielding heterogeneous ranks without solving a rank allocation optimization. Motivated by evidence that gradients near pretrained solutions exhibit low rank structure, we also introduce an optional lightweight correction that applies a \textbf{single} projected gradient update after truncation, followed by re-truncation. Extensive experiments across multiple LLM architectures show consistent gains across diverse benchmarks and compression ratios. Code is available at https://github.com/mint-vu/Zero-Sum-SVD

</details>


### [184] [Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning](https://arxiv.org/abs/2602.03190)
*Wenquan Lu,Hai Huang,Randall Balestriero*

Main category: cs.LG

TL;DR: 提出并评估了提示增强（prompt augmentation）用于GRPO的训练，解决熵崩溃导致的训练不稳定与探索受限问题。通过在训练中以多种模板和格式生成推理轨迹，增加 rollout 多样性，在不使用 KL 正则项的情况下实现持续训练和对低熵状态的容忍。结果在 MATH Level 3-5 数据集和多项标准数学推理基准上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 在强化学习后训练中，策略熵呈单调下降导致训练不稳定、需要限制训练轮次。并且大多数工作固定单一提示模板，限制了推理能力的提升和探索。需要提高训练过程的展开性与推理多样性。

Method: 引入提示增强：在GRPO框架下，训练阶段让模型在多样的提示模板和格式下生成推理轨迹，从而提高 rollout 多样性。无需 KL 正则项即可实现较长训练并容忍低熵状态。对比实验表明可在固定数据集下实现持续训练。

Result: 在 Qwen2.5-Math-1.5B 上使用提示增强训练，达到 MATH Level 3-5 数据集的状态最优表现，并在标准基准（AIME24、AMC、MATH500、Minerva、OlympiadBench）上分别达到 44.5/51.3 的准确率。代码与模型可在 GitHub 获取。

Conclusion: 提示增强可缓解熵崩溃，提升GRPO在推理任务上的长期训练能力与稳定性，并实现SOTA性能，建议结合多样提示模板用于数学推理与其他长篇推理任务。

Abstract: Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 44.5 per-benchmark accuracy and 51.3 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.

</details>


### [185] [Merging Beyond: Streaming LLM Updates via Activation-Guided Rotations](https://arxiv.org/abs/2602.03237)
*Yuxuan Yao,Haonan Sheng,Qingsong Lv,Han Wu,Shuqi Liu,Zehua Liu,Zengyan Liu,Jiahui Gao,Haochen Tan,Xiaojin Fu,Haoli Bai,Hing Cheung So,Zhijiang Guo,Linqi Song*

Main category: cs.LG

TL;DR: 提出 Streaming Merging 与 ARM，将合并视为迭代优化，通过激活子空间推导旋转向量，将合并系数视为学习率，以实现近似梯度下降的动态更新，依赖早期 SFT checkpoint，迭代合并可超越完全收敛的 SFT。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型合并多为后处理、难以捕捉 SFT 的动态优化收益、缺乏高效、可控的更新机制。

Method: ARM: Activation-guided Rotation-aware Merging，将合并看作梯度下降过程的近似，利用激活子空间推导旋转向量，参数更新沿数据驱动轨迹进行；将合并系数视为学习率，优于线性插值的语义子空间对齐；仅依赖早期 SFT checkpoint，进行迭代合并。

Result: 在1.7B–14B规模、跨领域（包括数学、代码）实验中，ARM在迭代合并后可超越完全收敛的 SFT 模型，显示出可扩展且轻量级的模型自适应能力。

Conclusion: ARM提供一个高效、可扩展的模型更新框架，通过激活驱动的旋转合并实现对高维参数演化几何结构的保留，适用于对大规模语言模型的快速适配。

Abstract: The escalating scale of Large Language Models (LLMs) necessitates efficient adaptation techniques. Model merging has gained prominence for its efficiency and controllability. However, existing merging techniques typically serve as post-hoc refinements or focus on mitigating task interference, often failing to capture the dynamic optimization benefits of supervised fine-tuning (SFT). In this work, we propose Streaming Merging, an innovative model updating paradigm that conceptualizes merging as an iterative optimization process. Central to this paradigm is \textbf{ARM} (\textbf{A}ctivation-guided \textbf{R}otation-aware \textbf{M}erging), a strategy designed to approximate gradient descent dynamics. By treating merging coefficients as learning rates and deriving rotation vectors from activation subspaces, ARM effectively steers parameter updates along data-driven trajectories. Unlike conventional linear interpolation, ARM aligns semantic subspaces to preserve the geometric structure of high-dimensional parameter evolution. Remarkably, ARM requires only early SFT checkpoints and, through iterative merging, surpasses the fully converged SFT model. Experimental results across model scales (1.7B to 14B) and diverse domains (e.g., math, code) demonstrate that ARM can transcend converged checkpoints. Extensive experiments show that ARM provides a scalable and lightweight framework for efficient model adaptation.

</details>


### [186] [When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models](https://arxiv.org/abs/2602.02855)
*Gibbs Nwemadji,Bruno Loureiro,Jean Barbier*

Main category: cs.LG

TL;DR: 在单索引模型、一次通过 SGD 的 LoRA 微调场景下，过度的预训练可能使微调收敛变慢；预训练强度与目标任务的非线性程度共同决定收敛速率。


<details>
  <summary>Details</summary>
Motivation: 挑战普遍的“预训练越多越好”的直觉，揭示在一定条件下预训练会延长搜索阶段，影响微调效率。

Method: 基于对微调动力学的摘要统计描述，对 LoRA 微调在单索引模型、一次通过 SGD 的设置进行严格的理论分析，推导收敛率与初始对齐和任务非线性程度的依赖关系。

Result: 给出对收敛速率的精确表征；表明即使预训练与下游任务对齐，强预训练也可能延长搜索阶段、降低收敛速度。

Conclusion: 这项工作提供了一个统一的框架，展示预训练强度与任务难度共同塑造 LoRA 微调的动力学与局限性；对设计预训练策略有实际启示。

Abstract: Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.

</details>


### [187] [R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?](https://arxiv.org/abs/2602.03300)
*Jingyi Zhang,Tianyi Lin,Huanjin Yao,Xiang Lan,Shunyu Liu,Jiaxing Huang*

Main category: cs.LG

TL;DR: CADS是一种面向多模态大语言模型的协同对抗数据合成框架，通过CAD-Generate与CAD-Judge两阶段循环以及对抗情境优化，生成高质量、多样且具挑战性的訓練数据，并构建数据集MMSynthetic-20K，提升R1-SyntheticVL模型在多项基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态数据依赖多样性与难度不足限制MLLM的泛化能力。需要一个通用、可扩展的方法，利用群体智慧和对抗学习，自动合成高质量且具挑战性的训练样本以促进模型提升。

Method: 提出CADS框架，包含两个循环阶段：CAD-Generate（集体知识驱动的联合数据生成，关注多模态数据的多样性与覆盖）与CAD-Judge（协同评估生成数据的质量）。引入对抗情境优化机制以优化生成上下文，促使生成更具挑战性的数据。基于CADS构建MMSynthetic-20K并训练R1-SyntheticVL模型。

Result: 在多个基准上展现优越性能，表明CADS能有效提升MLLM的数据质量与模型表现；所构建的数据集与模型在实证评估中表现良好。

Conclusion: CADS提供一个通用的协同对抗数据合成框架，融合集体生成与对抗评判，能有效生成高质量、多样且具挑战性的数据，提升多模态语言模型的解决复杂现实任务的能力。未来工作可拓展到更多模态、规模化数据以及更强的上下文优化。

Abstract: In this work, we aim to develop effective data synthesis techniques that autonomously synthesize multimodal training data for enhancing MLLMs in solving complex real-world tasks. To this end, we propose Collective Adversarial Data Synthesis (CADS), a novel and general approach to synthesize high-quality, diverse and challenging multimodal data for MLLMs. The core idea of CADS is to leverage collective intelligence to ensure high-quality and diverse generation, while exploring adversarial learning to synthesize challenging samples for effectively driving model improvement. Specifically, CADS operates with two cyclic phases, i.e., Collective Adversarial Data Generation (CAD-Generate) and Collective Adversarial Data Judgment (CAD-Judge). CAD-Generate leverages collective knowledge to jointly generate new and diverse multimodal data, while CAD-Judge collaboratively assesses the quality of synthesized data. In addition, CADS introduces an Adversarial Context Optimization mechanism to optimize the generation context to encourage challenging and high-value data generation. With CADS, we construct MMSynthetic-20K and train our model R1-SyntheticVL, which demonstrates superior performance on various benchmarks.

</details>


### [188] [Late-Stage Generalization Collapse in Grokking: Detecting anti-grokking with Weightwatcher](https://arxiv.org/abs/2602.02859)
*Hari K Prakash,Charles H Martin*

Main category: cs.LG

TL;DR: 提出并证实了第三阶段现象“anti-grokking”：在 grokking 后泛化崩溃，测试准确率回到随机水平而训练保持高分。通过 WeightWatcher 的特征谱信号（Correlation Traps）和 HTSR/SETOL 的 α 指标诊断，不需数据访问；并在大规模模型中观察到类似病理。


<details>
  <summary>Details</summary>
Motivation: 揭示 memorization 的更完整操作定义及 grokking 的全景，强调训练后期泛化崩溃的存在及其诊断意义，旨在改进对模型泛化行为的理解与控制。

Method: 在两组典型 grokking 设置上进行极端长时训练：3 层 MLP（MNIST 子集）与 Transformer（模组加法任务）。使用 WeightWatcher（基于 HTSR/SETOL 理论）分析权重矩阵的经验谱，提取 Correlation Traps；同时监测平均 HTSR 层质量指标 α 是否偏离 2.0。并与 ℓ2 范数、激活稀疏度、绝对权重熵、局部电路复杂度等作为对照。

Result: 在从 pre-grokking 到 grokking 的转变后，测试准确率回落至随机猜测水平，而训练准确率仍保持完美；出现 Correlation Traps（特征谱中超出 Marchenko–Pastur 的特征值）及 α 偏离 2.0，作为 anti-grokking 的关键信号；这些信号不依赖测试/训练数据。还发现 Correlation Traps 可诱发灾难性遗忘或原型记忆，且在大规模 LLM（如 OSS GPT 20/120B）中观察到类似病理。

Conclusion: 确立 anti-grokking 作为新的泛化崩溃模式，提供基于权重谱与 HTSR 指标的独立诊断工具，便于早期检测和研究后向泛化崩溃及其对大模型的影响；未来工作可探讨抑制 Correlation Traps 的训练策略与正则化方法。

Abstract: \emph{Memorization} in neural networks lacks a precise operational definition and is often inferred from the grokking regime, where training accuracy saturates while test accuracy remains very low. We identify a previously unreported third phase of grokking in this training regime: \emph{anti-grokking}, a late-stage collapse of generalization.
  We revisit two canonical grokking setups: a 3-layer MLP trained on a subset of MNIST and a transformer trained on modular addition, but extended training far beyond standard. In both cases, after models transition from pre-grokking to successful generalization, test accuracy collapses back to chance while training accuracy remains perfect, indicating a distinct post-generalization failure mode.
  To diagnose anti-grokking, we use the open-source \texttt{WeightWatcher} tool based on HTSR/SETOL theory. The primary signal is the emergence of \emph{Correlation Traps}: anomalously large eigenvalues beyond the Marchenko--Pastur bulk in the empirical spectral density of shuffled weight matrices, which are predicted to impair generalization. As a secondary signal, anti-grokking corresponds to the average HTSR layer quality metric $α$ deviating from $2.0$. Neither metric requires access to the test or training data.
  We compare these signals to alternative grokking diagnostic, including $\ell_2$ norms, Activation Sparsity, Absolute Weight Entropy, and Local Circuit Complexity. These track pre-grokking and grokking but fail to identify anti-grokking. Finally, we show that Correlation Traps can induce catastrophic forgetting and/or prototype memorization, and observe similar pathologies in large-scale LLMs, like OSS GPT 20/120B.

</details>


### [189] [Robustness as an Emergent Property of Task Performance](https://arxiv.org/abs/2602.03344)
*Shir Ashury-Tahan,Ariel Gera,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.LG

TL;DR:  robustness correlates with task competence; as models saturate on a task, robustness emerges; robustness is driven by task-specific competence rather than model-intrinsic properties.


<details>
  <summary>Details</summary>
Motivation: to understand whether robustness is an independent capability or a byproduct of performance, and whether robustness generalizes across tasks and data presentations.

Method: empirical analysis across multiple models, datasets, and configurations (e.g., paraphrases, different temperatures); measure performance and robustness; examine correlation and drivers.

Result: strong positive correlation between performance and robustness across tasks and settings; robustness primarily driven by task-specific competence; minimal evidence that robustness is an intrinsic model property.

Conclusion: robustness emerges alongside task performance; explicit robustness measurement/improvement may be less necessary; practitioners can expect reliability on easier tasks and caution that literature tasks may be unreliable on harder tasks.

Abstract: Robustness is often regarded as a critical future challenge for real-world applications, where stability is essential. However, as models often learn tasks in a similar order, we hypothesize that easier tasks will be easier regardless of how they are presented to the model. Indeed, in this paper, we show that as models approach high performance on a task, robustness is effectively achieved. Through an empirical analysis of multiple models across diverse datasets and configurations (e.g., paraphrases, different temperatures), we find a strong positive correlation. Moreover, we find that robustness is primarily driven by task-specific competence rather than inherent model-level properties, challenging current approaches that treat robustness as an independent capability. Thus, from a high-level perspective, we may expect that as new tasks saturate, model robustness on these tasks will emerge accordingly. For researchers, this implies that explicit efforts to measure and improve robustness may warrant reduced emphasis, as such robustness is likely to develop alongside performance gains. For practitioners, it acts as a sign that indeed the tasks that the literature deals with are unreliable, but on easier past tasks, the models are reliable and ready for real-world deployment.

</details>


### [190] [A Geometry-Aware Efficient Algorithm for Compositional Entropic Risk Minimization](https://arxiv.org/abs/2602.02877)
*Xiyuan Wei,Linli Zhou,Bokun Wang,Chih-Jen Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: 提出 SCENT，基于几何感知的随机对偶最小化算法，用于组合化熵风险最小化的 Log-E-Exp 损失，获得凸问题 $O(1/p{√T})$ 收敛率，在极端分类、部分 AUC、对比学习、分布鲁棒优化等任务中优于基线。


<details>
  <summary>Details</summary>
Motivation: 在数据规模巨大时，Log-E-Exp 作为对数期望的近似常用于组合风险最小化，但直接求解困难，现有算法存在发散、数值不稳定和收敛慢等问题，需稳定且高效的优化方案。

Method: 提出 SCENT，对偶最小化问题采用随机近端镜 descent（SPMD）更新，对偶变量使用由负指数函数诱导的 Bregman 距离，利用几何结构提升稳定性与收敛性。

Result: 给出在凸情形下的理论收敛率为 O(1/√T)；理论分析表明 SPMD 优于标准 SGD 更新对偶变量；在极端分类、部分 AUC、对比学习和分布鲁棒优化等任务中实验上优于基线。

Conclusion: SCENT 为组合风险最小化提供一个几何感知、稳健且高效的优化框架，适用于大规模 Log-E-Exp 损失，具备理论与经验上的优势，具有广泛的应用潜力。

Abstract: This paper studies optimization for a family of problems termed $\textbf{compositional entropic risk minimization}$, in which each data's loss is formulated as a Log-Expectation-Exponential (Log-E-Exp) function. The Log-E-Exp formulation serves as an abstraction of the Log-Sum-Exponential (LogSumExp) function when the explicit summation inside the logarithm is taken over a gigantic number of items and is therefore expensive to evaluate. While entropic risk objectives of this form arise in many machine learning problems, existing optimization algorithms suffer from several fundamental limitations including non-convergence, numerical instability, and slow convergence rates. To address these limitations, we propose a geometry-aware stochastic algorithm, termed $\textbf{SCENT}$, for the dual formulation of entropic risk minimization cast as a min--min optimization problem. The key to our design is a $\textbf{stochastic proximal mirror descent (SPMD)}$ update for the dual variable, equipped with a Bregman divergence induced by a negative exponential function that faithfully captures the geometry of the objective. Our main contributions are threefold: (i) we establish an $O(1/\sqrt{T})$ convergence rate of the proposed SCENT algorithm for convex problems; (ii) we theoretically characterize the advantages of SPMD over standard SGD update for optimizing the dual variable; and (iii) we demonstrate the empirical effectiveness of SCENT on extreme classification, partial AUC maximization, contrastive learning and distributionally robust optimization, where it consistently outperforms existing baselines.

</details>


### [191] [Mixture of Concept Bottleneck Experts](https://arxiv.org/abs/2602.02886)
*Francesco De Santis,Gabriele Ciravegna,Giovanni De Felice,Arianna Casanova,Francesco Giannini,Michelangelo Diligenti,Mateo Espinosa Zarlenga,Pietro Barbiero,Johannes Schneider,Danilo Giordano*

Main category: cs.LG

TL;DR: M-CBEs扩展了概念瓶颈模型，通过可变的专家数量和每个专家的函数形式来提升可解释性与预测性之间的权衡。提出两种实例：Linear M-CBE（线性表达集合）与 Symbolic M-CBE（符号回归），在不同用户/任务需求下表现出更强的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有CBMs通常将任务预测器固定为单一线性或布尔表达，限制了预测性能和对多样化用户需求的适应性。需要在专家数量与函数形式上探索设计空间，以提高准确性与可解释性之间的折中灵活性。

Method: 提出 Mixture of Concept Bottleneck Experts (M-CBEs) 作为通用框架，允许使用多个专家对概念瓶颈输出进行混合预测。具体实例包括：1) Linear M-CBE：学习有限集合的线性表达式来组合预测；2) Symbolic M-CBE：通过符号回归在用户指定的运算符词汇表下从数据中发现专家函数。

Result: 通过改变混合专家的数量与函数形式，M-CBEs提供了一种稳健的框架来在准确性与可解释性之间进行权衡，并能根据不同用户与任务需求进行自适应调整。

Conclusion: M-CBEs将CBMs扩展到一个更广的设计空间，强调可定制的专家数量与表达形式对结果的影响；线性与符号化实例验证了该方向的潜力与灵活性。

Abstract: Concept Bottleneck Models (CBMs) promote interpretability by grounding predictions in human-understandable concepts. However, existing CBMs typically fix their task predictor to a single linear or Boolean expression, limiting both predictive accuracy and adaptability to diverse user needs. We propose Mixture of Concept Bottleneck Experts (M-CBEs), a framework that generalizes existing CBMs along two dimensions: the number of experts and the functional form of each expert, exposing an underexplored region of the design space. We investigate this region by instantiating two novel models: Linear M-CBE, which learns a finite set of linear expressions, and Symbolic M-CBE, which leverages symbolic regression to discover expert functions from data under user-specified operator vocabularies. Empirical evaluation demonstrates that varying the mixture size and functional form provides a robust framework for navigating the accuracy-interpretability trade-off, adapting to different user and task needs.

</details>


### [192] [MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling](https://arxiv.org/abs/2602.03359)
*Ning Ding,Fangcheng Liu,Kyungrae Kim,Linji Hao,Kyeng-Hun Lee,Hyeonmok Ko,Yehui Tang*

Main category: cs.LG

TL;DR: 提出 MeKi：通过在每个 Transformer 层引入 token-level memory experts，将语义知识存储于静态 ROM，利用参数重参数化折叠为查找表，以实现不增加推理计算量的容量扩展，适用于边缘设备的 on-device LLM。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源受限（RAM/NPU），直接扩大参数量或推理量不可行，需在不增加推理开销的前提下提升模型能力。

Method: 在每层引入 token-level memory experts，预存语义知识并在生成时注入；通过重参数化将训练时使用的参数矩阵折叠成紧凑的静态查找表加载到 ROM；实现容量扩展与推理成本解耦，推理时零额外延迟。

Result: 大量实验证明，与相同推理速度下的密集（dense）基线相比，MeKi显著提升性能，验证记忆化扩展在设备端 LLM 的有效性。

Conclusion: memory-based scaling 通过将知识放入 ROM，实现模型容量与计算量解耦，为边缘设备的高效 on-device LLM 提供新方向。

Abstract: Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table. By offloading the knowledge to ROM, MeKi decouples model capacity from computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at https://github.com/ningding-o/MeKi.

</details>


### [193] [Self-Soupervision: Cooking Model Soups without Labels](https://arxiv.org/abs/2602.02890)
*Anthony Fuller,James R. Green,Evan Shelhamer*

Main category: cs.LG

TL;DR: 在自监督学习（SSL）场景中将“模型汤”扩展为 Self-Soupervision：混合不同SSL算法及其超参数的 ingredients，生成鲁棒且更准确的模型；对污染测试数据进行自汤后再在无污染训练数据上微调，以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决单一SSL模型在多源数据、数据污染和任务迁移中的局限，探索跨算法、跨超参数的参数混合对鲁棒性与泛化的影响。

Method: 训练多种SSL ingredient（如 MAE、MoCoV3、MMCR 等），允许它们在不同数据源与超参数下产生，随后对这些 ingredient 的参数进行自汤合并以得到一个单一模型，并在污染数据上进行自汤后再回到无污染的训练数据上微调。

Result: 在对污染测试数据的鲁棒性评估中，对 ImageNet-C 提升约 3.5%，对 LAION-C 提升约 7%。此外，将 MAE、MoCoV3、MMCR 等多种 ingredient 进行自汤后再合成，得到的模型的准确性高于任一单一 SSL ingredient。

Conclusion: Self-Soupervision 为 SSL 提供了一种可扩展的参数级别集成机制，提升鲁棒性和泛化能力，并使不同 SSL 算法及超参数的组合成为可能，从而更好地利用无标签数据与多源数据进行学习。

Abstract: Model soups are strange and strangely effective combinations of parameters. They take a model (the stock), fine-tune it into multiple models (the ingredients), and then mix their parameters back into one model (the soup) to improve predictions. While all known soups require supervised learning, and optimize the same loss on labeled data, our recipes for Self-\emph{Soup}ervision generalize soups to self-supervised learning (SSL). Our Self-Souping lets us flavor ingredients on new data sources, e.g. from unlabeled data from a task for transfer or from a shift for robustness. We show that Self-Souping on corrupted test data, then fine-tuning back on uncorrupted train data, boosts robustness by +3.5\% (ImageNet-C) and +7\% (LAION-C). Self-\emph{Soup}ervision also unlocks countless SSL algorithms to cook the diverse ingredients needed for more robust soups. We show for the first time that ingredients can differ in their SSL hyperparameters -- and more surprisingly, in their SSL algorithms. We cook soups of MAE, MoCoV3, and MMCR ingredients that are more accurate than any one single SSL ingredient.

</details>


### [194] [When Single Answer Is Not Enough: Rethinking Single-Step Retrosynthesis Benchmarks for LLMs](https://arxiv.org/abs/2602.03554)
*Bogdan Zagribelnyy,Ivan Ilin,Maksim Kuznetsov,Nikita Bondarev,Roman Schutski,Thomas MacDougall,Rim Shayakhmetov,Zulfat Miftakhutdinov,Mikolaj Mizera,Vladimir Aladinskiy,Alex Aliper,Alex Zhavoronkov*

Main category: cs.LG

TL;DR: 提出针对单步 retrosynthesis 的 plausibility-based 基准，用 ChemCensor 评估化学可行性，且引入 CREED 数据集及其在训练中的应用，提升对比基线的表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估多依赖单一标准答案和 Top-K，无法捕捉实际合成规划的开放性与可行性。需要一个以可行性为核心的评估框架以更贴近人类规划实践。

Method: 提出 ChemCensor 作为化学可行性度量，建立面向单步 retrosynthesis 的基准，评估通用与化学专用大型语言模型。构建 CREED 数据集（包含数百万条经 ChemCensor 验证的反应记录）用于对模型进行训练，并在该基准下比较模型与基线。

Result: 强调可行性高于完全匹配的评估更贴近人类合成规划；基于 CREED 的训练能够在该基准下超越单纯的 LLM 基线。

Conclusion: 基于可行性导向的评估框架更符合实际合成规划需求，CREED 数据集为提升 LLM 在合成规划中的表现提供了可扩展的训练资源。

Abstract: Recent progress has expanded the use of large language models (LLMs) in drug discovery, including synthesis planning. However, objective evaluation of retrosynthesis performance remains limited. Existing benchmarks and metrics typically rely on published synthetic procedures and Top-K accuracy based on single ground-truth, which does not capture the open-ended nature of real-world synthesis planning. We propose a new benchmarking framework for single-step retrosynthesis that evaluates both general-purpose and chemistry-specialized LLMs using ChemCensor, a novel metric for chemical plausibility. By emphasizing plausibility over exact match, this approach better aligns with human synthesis planning practices. We also introduce CREED, a novel dataset comprising millions of ChemCensor-validated reaction records for LLM training, and use it to train a model that improves over the LLM baselines under this benchmark.

</details>


### [195] [Controlled disagreement improves generalization in decentralized training](https://arxiv.org/abs/2602.02899)
*Zesen Wang,Mikael Johansson*

Main category: cs.LG

TL;DR: 提出带自适应共识权衡的去中心化SGD（DSGD-AC），通过有意保留非消失的共识误差，使其与主导Hessian子空间对齐，作为结构化扰动引导优化到更平坦的极小值，在图像分类和机器翻译任务上优于标准DSGD与集中SGD。


<details>
  <summary>Details</summary>
Motivation: 挑战“共识误差总是有害”的共识：研究这些误差是否能被利用成隐式正则化力量，以及在去中心化训练中是否可以通过结构化扰动改善收敛与泛化。

Method: 提出时间相关的自适应缩放机制来维持非零共识误差，理论分析显示该误差对齐主导Hessian子空间，形成有意义的结构化扰动；在图像分类和机器翻译任务上比较DSGD-AC、标准DSGD和集中SGD。

Result: 共识误差被证明是隐式正则化的有用来源，DSGD-AC在测试精度和解的平坦性上均超越基线，在多种任务上取得一致优势。

Conclusion: 共识误差可成为有益的结构化扰动，颠覆去中心化学习的传统设计思路， suggesting 去中心化算法应利用误差的方向性与时间依赖性以提升性能。

Abstract: Decentralized training is often regarded as inferior to centralized training because the consensus errors between workers are thought to undermine convergence and generalization, even with homogeneous data distributions. This work challenges this view by introducing decentralized SGD with Adaptive Consensus (DSGD-AC), which intentionally preserves non-vanishing consensus errors through a time-dependent scaling mechanism. We prove that these errors are not random noise but systematically align with the dominant Hessian subspace, acting as structured perturbations that guide optimization toward flatter minima. Across image classification and machine translation benchmarks, DSGD-AC consistently surpasses both standard DSGD and centralized SGD in test accuracy and solution flatness. Together, these results establish consensus errors as a useful implicit regularizer and open a new perspective on the design of decentralized learning algorithms.

</details>


### [196] [Manifold-Constrained Energy-Based Transition Models for Offline Reinforcement Learning](https://arxiv.org/abs/2602.02900)
*Zeyu Fang,Zuyuan Zhang,Mahdi Imani,Tian Lan*

Main category: cs.LG

TL;DR: 提出 MC-ETM，一种基于能量的条件转移模型，结合流形投影-扩散负采样，以紧缩数据集覆盖并用能量信号进行保守评估，提升离线强化学习的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中的分布漂移导致策略改进阶段进入数据集弱覆盖的状态-动作区域，模型误差叠加导致严重的价值高估。需要更稳健的转移模型与统一的可靠性信号。

Method: 训练条件能量基转移模型，使用流形投影-扩散负采样；学习下一状态的潜在流形，通过对潜在编码扰动并在潜在空间进行 Langevin 动力学，在学习的条件能量下生成近似流形的困难负样本，从而收紧数据支持区域并提升对微小 OOD 偏差的敏感性。用于策略优化：以能量作为唯一可靠信号；若采样的最小能量超过阈值则截断滚动；Bellman 缓存通过对能量引导样本的 Q 值级差进行悲观惩罚来稳定。通过混合悲观的 MDP 给出保守的性能界限，区分在支持内评估误差与截断风险。

Result: 在多步动力学保真度与离线控制基准的标准测试中，MC-ETM 提高了归一化回报，尤其在动力学不规则和数据覆盖稀疏的场景下表现突出。

Conclusion: MC-ETM 展示了通过对数据支持锚定的能量景观与潜在流形进行转移建模，提升离线强化学习的鲁棒性与稳定性，并给出理论界限，证明分布漂移下的评估误差与截断风险的分离。

Abstract: Model-based offline reinforcement learning is brittle under distribution shift: policy improvement drives rollouts into state--action regions weakly supported by the dataset, where compounding model error yields severe value overestimation. We propose Manifold-Constrained Energy-based Transition Models (MC-ETM), which train conditional energy-based transition models using a manifold projection--diffusion negative sampler. MC-ETM learns a latent manifold of next states and generates near-manifold hard negatives by perturbing latent codes and running Langevin dynamics in latent space with the learned conditional energy, sharpening the energy landscape around the dataset support and improving sensitivity to subtle out-of-distribution deviations. For policy optimization, the learned energy provides a single reliability signal: rollouts are truncated when the minimum energy over sampled next states exceeds a threshold, and Bellman backups are stabilized via pessimistic penalties based on Q-value-level dispersion across energy-guided samples. We formalize MC-ETM through a hybrid pessimistic MDP formulation and derive a conservative performance bound separating in-support evaluation error from truncation risk. Empirically, MC-ETM improves multi-step dynamics fidelity and yields higher normalized returns on standard offline control benchmarks, particularly under irregular dynamics and sparse data coverage.

</details>


### [197] [Spatiotemporal Decision Transformer for Traffic Coordination](https://arxiv.org/abs/2602.02903)
*Haoran Su,Yandong Sun,Hanxiao Deng*

Main category: cs.LG

TL;DR: MADT: a multi-agent decision transformer for traffic signal control that leverages graph attention, temporal transformers, and return-to-go conditioning to enable offline learning and better coordination, achieving 5-6% travel time reduction vs strong baselines on grid and real-world data.


<details>
  <summary>Details</summary>
Motivation: Address the sample-inefficiency and coordination challenges of existing multi-agent reinforcement learning (MARL) approaches for traffic signal control. Leverage offline data and sequence modeling to capture spatial-temporal dependencies and facilitate target-performance conditioning.

Method: Extend the Decision Transformer paradigm to a multi-agent setting: (1) graph attention to model spatial dependencies between intersections, (2) a temporal transformer encoder to capture traffic dynamics, (3) return-to-go conditioning to specify target performance. Train offline on historical data with a design conducive to future online fine-tuning.

Result: Empirical results on synthetic grid networks and real-world scenarios show state-of-the-art performance, with average travel time reduced by 5-6% compared to the strongest baseline and improved coordination among adjacent intersections.

Conclusion: MADT demonstrates the viability of reformulating multi-agent traffic signal control as sequence modeling with transform-based components and return-to-go conditioning, enabling effective offline learning and potential online adaptation; suggests strong potential for scalable, coordinated traffic control.

Abstract: Traffic signal control is a critical challenge in urban transportation, requiring coordination among multiple intersections to optimize network-wide traffic flow. While reinforcement learning has shown promise for adaptive signal control, existing methods struggle with multi-agent coordination and sample efficiency. We introduce MADT (Multi-Agent Decision Transformer), a novel approach that reformulates multi-agent traffic signal control as a sequence modeling problem. MADT extends the Decision Transformer paradigm to multi-agent settings by incorporating: (1) a graph attention mechanism for modeling spatial dependencies between intersections, (2) a|temporal transformer encoder for capturing traffic dynamics, and (3) return-to-go conditioning for target performance specification. Our approach enables offline learning from historical traffic data, with architecture design that facilitates potential online fine-tuning. Experiments on synthetic grid networks and real-world traffic scenarios demonstrate that MADT achieves state-of-the-art performance, reducing average travel time by 5-6% compared to the strongest baseline while exhibiting superior coordination among adjacent intersections.

</details>


### [198] [Conflict-Resolving and Sharpness-Aware Minimization for Generalized Knowledge Editing with Multiple Updates](https://arxiv.org/abs/2602.03696)
*Duy Nguyen,Hanqi Xiao,Archiki Prasad,Elias Stengel-Eskin,Hyunji Lee,Mohit Bansal*

Main category: cs.LG

TL;DR: CoRSA是一种参数高效的知识编辑框架，通过解决冲突、抑制梯度的曲率以及最大化新旧知识之间的边界，来实现对LLM多次更新的泛化、稳定性和避免知识冲突的提升；在事实编辑基准和代码领域均有显著提升，优于LoRA及其他模型编辑方法。


<details>
  <summary>Details</summary>
Motivation: 需要在不进行全量再训练的前提下高效更新LLM知识；现有模型编辑和参数高效微调在泛化、稳定性和知识冲突方面存在局限性。

Method: 提出CoRSA训练框架，结合Sharpness-Aware Minimization来降低损失曲率、并通过最大化新旧知识之间的边际（margin）来解决知识冲突；具备参数高效特性，支持对同一模型进行多轮更新，属于对知识编辑的整体性方法，促进多次更新的稳健性和一致性。

Result: 在三个事实编辑基准上，平均对比LoRA提升12.42%，对比模型编辑方法提升10%；多轮更新时，更新效果保持良好且与LoRA相比降低27.82%的灾难性遗忘；对代码域也有提升，更新效能的Pass@5提升5.48%。

Conclusion: CoRSA为大模型的知识编辑提供了一种高效、稳健且可扩展的框架，能在多轮更新中提升泛化与稳定性、降低遗忘，并且具备跨模态的适用性。

Abstract: Large language models (LLMs) rely on internal knowledge to solve many downstream tasks, making it crucial to keep them up to date. Since full retraining is expensive, prior work has explored efficient alternatives such as model editing and parameter-efficient fine-tuning. However, these approaches often break down in practice due to poor generalization across inputs, limited stability, and knowledge conflict. To address these limitations, we propose the CoRSA (Conflict-Resolving and Sharpness-Aware Minimization) training framework, a parameter-efficient, holistic approach for knowledge editing with multiple updates. CoRSA tackles multiple challenges simultaneously: it improves generalization to different input forms and enhances stability across multiple updates by minimizing loss curvature, and resolves conflicts by maximizing the margin between new and prior knowledge. Across three widely used fact editing benchmarks, CoRSA achieves significant gains in generalization, outperforming baselines with average absolute improvements of 12.42% over LoRA and 10% over model editing methods. With multiple updates, it maintains high update efficacy while reducing catastrophic forgetting by 27.82% compared to LoRA. CoRSA also generalizes to the code domain, outperforming the strongest baseline by 5.48% Pass@5 in update efficacy.

</details>


### [199] [A Random Matrix Theory Perspective on the Consistency of Diffusion Models](https://arxiv.org/abs/2602.02908)
*Binxu Wang,Jacob Zavatone-Veth,Cengiz Pehlevan*

Main category: cs.LG

TL;DR: 对比数据子集训练的扩散模型在相同噪声种子下输出极为相似，源于跨子集的共享高斯统计对输出的直接影响。给出一个随机矩阵理论（RMT）框架，量化有限数据集如何影响线性设定中的去噪器与采样映射的期望与方差；并扩展到整个采样轨迹。理论预测与在UNet与DiT的非记忆化训练情形的实验验证印证，提供可重复性的基线分析。


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型在不同非重叠数据子集上训练时输出的一致性与不一致性背后的统计原因，尤其在数据有限时如何通过数据谱的结构影响去噪器与采样过程的稳定性，从而为可重复性提供理论基线。

Method: 建立一个用于线性近似的随机矩阵理论框架，分析有限数据如何通过自一致关系 κ(σ^2) 重新标定噪声水平，解释低方差方向被过度收缩、样本被拉向数据集均值。对噪声传输与采样轨迹使用确定性等价工具扩展到分数矩阵幂，处理整个采样过程。通过三大因素：各特征方向的非各向同性、输入的非同质性、以及数据集规模的全局缩放，定量描述跨子集的差异。将理论应用于非记忆化的线性扩散模型，结合 UNet 与 DiT 的实验验证。

Result: 给出对线性扩散模型的精确预测：数据的谱特性直接决定输出的稳定性和跨子集的一致性；有限数据导致的方差放大与收缩可以用 κ(σ^2) 的自洽关系量化，且三大因素共同决定分布偏离的程度与方向。整个采样过程的轨迹也被理论框架所刻画，提供了可预测的跨训练子集输出差异的基线。

Conclusion: 该工作将数据的谱特性与扩散模型输出的稳定性联系起来，提供一个可用于评估训练可重复性的基线工具。通过在非记忆化条件下对线性近似的扩散过程进行RMT分析，揭示了数据规模、特征向量的异方差以及输入非均一性等如何共同驱动跨数据子集的样本差异，并为未来在非线性与真实扩散模型中的扩展奠定了理论基础。

Abstract: Diffusion models trained on different, non-overlapping subsets of a dataset often produce strikingly similar outputs when given the same noise seed. We trace this consistency to a simple linear effect: the shared Gaussian statistics across splits already predict much of the generated images. To formalize this, we develop a random matrix theory (RMT) framework that quantifies how finite datasets shape the expectation and variance of the learned denoiser and sampling map in the linear setting. For expectations, sampling variability acts as a renormalization of the noise level through a self-consistent relation $σ^2 \mapsto κ(σ^2)$, explaining why limited data overshrink low-variance directions and pull samples toward the dataset mean. For fluctuations, our variance formulas reveal three key factors behind cross-split disagreement: \textit{anisotropy} across eigenmodes, \textit{inhomogeneity} across inputs, and overall scaling with dataset size. Extending deterministic-equivalence tools to fractional matrix powers further allows us to analyze entire sampling trajectories. The theory sharply predicts the behavior of linear diffusion models, and we validate its predictions on UNet and DiT architectures in their non-memorization regime, identifying where and how samples deviates across training data split. This provides a principled baseline for reproducibility in diffusion training, linking spectral properties of data to the stability of generative outputs.

</details>


### [200] [Notes on the Reward Representation of Posterior Updates](https://arxiv.org/abs/2602.02912)
*Pedro A. Ortega*

Main category: cs.LG

TL;DR: 在固定模型下，KL正则化的软更新等价于贝叶斯后验；后验更新揭示的是相对的、情境相关的激励信号，而绝对奖励仍由情境基线所模糊，更新方向的一致性需要可重复使用的持续值以实现跨条件的一致性。


<details>
  <summary>Details</summary>
Motivation: 探索将决策视为推断的设定何时能变成字面意义上的贝叶斯更新，以及在KL约束下信息传递的通道化意义；界定后验更新对激励与奖励的可识别性界限。

Method: 分析在一个固定概率模型内，KL正则化的贝叶斯式更新等价于真实后验，将更新变量视作信息传递通道，考察其对行为变化的决定性来源。

Result: 后验更新确定相对的、情境相关的激励信号，但对绝对奖励不具唯一性，仍需依赖情境基线；引入一个跨更新方向的可重复使用的持续值可提供一致性约束，将不同条件顺序下的奖励描述联系起来。

Conclusion: 给出一个尖锐的识别结果：可以识别出激励信号的相对结构和跨情境的一致性约束，但绝对奖励的识别受情境基线限制，需要额外的持续值以实现跨更新的一致性。

Abstract: Many ideas in modern control and reinforcement learning treat decision-making as inference: start from a baseline distribution and update it when a signal arrives. We ask when this can be made literal rather than metaphorical. We study the special case where a KL-regularized soft update is exactly a Bayesian posterior inside a single fixed probabilistic model, so the update variable is a genuine channel through which information is transmitted. In this regime, behavioral change is driven only by evidence carried by that channel: the update must be explainable as an evidence reweighing of the baseline. This yields a sharp identification result: posterior updates determine the relative, context-dependent incentive signal that shifts behavior, but they do not uniquely determine absolute rewards, which remain ambiguous up to context-specific baselines. Requiring one reusable continuation value across different update directions adds a further coherence constraint linking the reward descriptions associated with different conditioning orders.

</details>


### [201] [Weighted Temporal Decay Loss for Learning Wearable PPG Data with Sparse Clinical Labels](https://arxiv.org/abs/2602.02917)
*Yunsung Chung,Keum San Chun,Migyeong Gwak,Han Feng,Yingshuo Liu,Chanho Lim,Viswam Nathan,Nassir Marrouche,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 提出一个基于样本权重随时间间隔衰减的训练策略来应对临床标签稀疏问题，利用在心连同PPG的生物标志物学习，简单线性衰减表现最稳健，能给出生物标志物PPG证据随时间失效的可解释视角。


<details>
  <summary>Details</summary>
Motivation: 临床标签稀疏导致来自时间上与实验室抽血存在时距的生物信号监督信号薄弱，需要一种能在时间间隔上自适应加权的学习策略来提升健康监测的鲁棒性。

Method: 学习一个针对每个生物标志物的时间间隔衰减函数，用以调整样本在损失中的权重，并引入正则化以防止得到退化解；比较四种衰减函数，发现线性衰减最稳健；在可穿戴PPG数据（450名受试者、10个标志物）上进行评估；在受试者级设置下，与自监督微调baselines和基于特征的随机森林对比。

Result: 在450名受试者、10个标志物的智能手表PPG数据上，所提方法相比基线有提升；受试者级AUPRC平均为0.715，优于0.674（自监督基线微调）和0.626（随机森林）；四种衰减族中线性衰减最具鲁棒性。

Conclusion: 学习得到的衰减率提供 biomarker PPG 证据随时间变得陈旧的可解释视图；方法有效缓解标签稀疏问题并提高准确性，且线性衰减通常最稳健。

Abstract: Advances in wearable computing and AI have increased interest in leveraging PPG for health monitoring over the past decade. One of the biggest challenges in developing health algorithms based on such biosignals is the sparsity of clinical labels, which makes biosignals temporally distant from lab draws less reliable for supervision. To address this problem, we introduce a simple training strategy that learns a biomarker-specific decay of sample weight over the time gap between a segment and its ground truth label and uses this weight in the loss with a regularizer to prevent trivial solutions. On smartwatch PPG from 450 participants across 10 biomarkers, the approach improves over baselines. In the subject-wise setting, the proposed approach averages 0.715 AUPRC, compared to 0.674 for a fine-tuned self-supervised baseline and 0.626 for a feature-based Random Forest. A comparison of four decay families shows that a simple linear decay function is most robust on average. Beyond accuracy, the learned decay rates summarize how quickly each biomarker's PPG evidence becomes stale, providing an interpretable view of temporal sensitivity.

</details>


### [202] [Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation](https://arxiv.org/abs/2602.03806)
*Ziru Chen,Dongdong Chen,Ruinan Jin,Yingbin Liang,Yujia Xie,Huan Sun*

Main category: cs.LG

TL;DR: 提出 Cobalt：在多轮代码生成场景下，将在线RL与离线轨迹结合的上下文带宽学习框架，通过对参考LLM生成的轨迹切分为部分轨迹进行在线单步完成，以实现高效且稳定的迭代决策。


<details>
  <summary>Details</summary>
Motivation: 在线RL在性能上通常优于离线RL，但成本高、训练不稳定；多轮代码生成可视为一个可恢复的一步MDP，结合离线轨迹有望同时获得性能与成本/稳定性优势。

Method: 用参考LLM收集代码生成轨迹并将其切分为作为上下文提示的部分轨迹；在在线阶段，LLM通过单步代码生成完成每个部分轨迹，从而进行上下文带宽学习。

Result: 相较两种基于GRPO与VeRPO的在线RL基线，Cobalt在 LiveCodeBench 上的R1-Distill 8B与Qwen3 8B的Pass@1绝对分数提升可达9.0与6.2；并对LLM的上下文奖励操控行为进行了分析，添加扰动轨迹以缓解该问题；代码与数据公开。

Conclusion: Cobalt为迭代决策任务（如多轮代码生成）提供了高效、稳定的解决方案，成功融合在线与离线RL的优势，具备良好推广到其他逐步决策任务的潜力。

Abstract: Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.

</details>


### [203] [A Reproducible Framework for Bias-Resistant Machine Learning on Small-Sample Neuroimaging Data](https://arxiv.org/abs/2602.02920)
*Jagan Mohan Reddy Dwarampudi,Jennifer L Purks,Joshua Wong,Renjie Hu,Tania Banerjee*

Main category: cs.LG

TL;DR: 一个可重复且抗偏倚的机器学习框架，适用于小样本神经影像数据，结合领域特征工程、嵌套交叉验证和校准阈值优化；在高维结构MRI数据集上实现可解释且偏差较小的性能评估。


<details>
  <summary>Details</summary>
Motivation: 在小样本、高维神经影像数据中，现有交叉验证常因重复使用相同折而产生乐观偏差，影响可重复性和泛化。需建立可验证且可解释的分析流程。

Method: 引入领域知情特征工程、嵌套交叉验证以及校准阈值优化；通过重要性引导排序选择紧凑、可解释的子集；在高维结构MRI数据集上评估框架。

Result: 在高维结构MRI数据集（与深脑刺激相关的认知结果）上，嵌套CV 平衡准确度为0.660±0.068；通过重要性排序获得紧凑且可解释的特征子集；框架实现无偏评估与可解释性相结合。

Conclusion: 结合可解释性与无偏评估，提供一个可扩展的、在数据受限的生物医学领域中可靠的机器学习蓝本。

Abstract: We introduce a reproducible, bias-resistant machine learning framework that integrates domain-informed feature engineering, nested cross-validation, and calibrated decision-threshold optimization for small-sample neuroimaging data. Conventional cross-validation frameworks that reuse the same folds for both model selection and performance estimation yield optimistically biased results, limiting reproducibility and generalization. Demonstrated on a high-dimensional structural MRI dataset of deep brain stimulation cognitive outcomes, the framework achieved a nested-CV balanced accuracy of 0.660\,$\pm$\,0.068 using a compact, interpretable subset selected via importance-guided ranking. By combining interpretability and unbiased evaluation, this work provides a generalizable computational blueprint for reliable machine learning in data-limited biomedical domains.

</details>


### [204] [How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?](https://arxiv.org/abs/2602.02924)
*Xiaoyuan Cheng,Wenxuan Yuan,Boyang Li,Yuanchao Xu,Yiming Yang,Hao Liang,Bei Peng,Robert Loftin,Zhuo Sun,Yukun Hu*

Main category: cs.LG

TL;DR: 提出Augmented Lagrangian-Guided Diffusion (ALGD)用于离策略安全强化学习的扩散策略，通过局部凸化Lagrangian能量地形来稳定训练与策略生成，在不改变最优策略分布的前提下实现更安全、稳定的学习，并在多种环境中取得理论和经验上的优越表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的 RL 多聚焦于离线场景的奖励最大化，在线任务中的安全性考虑不足。原始对偶优化中非凸的Lagrangian地形导致 primal–dual 训练不稳定，从而影响策略生成与学习。

Method: 将Lagrangian视为引导扩散去噪过程的能量函数，提出增广拉格朗日项以局部凸化该能量景观，从而稳定离策略安全RL 的策略采样与训练流程；该方法在不改变最优策略分布的前提下实现稳定优化。给出理论分析并通过大量实验验证其收敛性与鲁棒性。

Result: 理论上证明增广拉格朗日项改善能量景观的条件数，从而稳定 primal–dual 更新；实验显示ALGD在多种环境中实现强劲且稳定的表现，且相较于基线具有更好安全性和稳定性。

Conclusion: ALGD为扩散基安全RL提供了理论扎实且实证有效的解决方案，通过局部凸化能量景观实现离策略下的稳定学习与安全性提升。

Abstract: Diffusion policy sampling enables reinforcement learning (RL) to represent multimodal action distributions beyond suboptimal unimodal Gaussian policies. However, existing diffusion-based RL methods primarily focus on offline settings for reward maximization, with limited consideration of safety in online settings. To address this gap, we propose Augmented Lagrangian-Guided Diffusion (ALGD), a novel algorithm for off-policy safe RL. By revisiting optimization theory and energy-based model, we show that the instability of primal-dual methods arises from the non-convex Lagrangian landscape. In diffusion-based safe RL, the Lagrangian can be interpreted as an energy function guiding the denoising dynamics. Counterintuitively, direct usage destabilizes both policy generation and training. ALGD resolves this issue by introducing an augmented Lagrangian that locally convexifies the energy landscape, yielding a stabilized policy generation and training process without altering the distribution of the optimal policy. Theoretical analysis and extensive experiments demonstrate that ALGD is both theoretically grounded and empirically effective, achieving strong and stable performance across diverse environments.

</details>


### [205] [Distance Marching for Generative Modeling](https://arxiv.org/abs/2602.02928)
*Zimo Wang,Ishit Mehta,Haolin Lu,Chung-En Sun,Ge Yan,Tsui-Wei Weng,Tzu-Mao Li*

Main category: cs.LG

TL;DR: Distance Marching: a time-unconditional generative modeling framework that uses distance-field-inspired losses and two principled inference methods to steer denoising toward the data manifold, yielding strong FID gains and robust OOD detection.


<details>
  <summary>Details</summary>
Motivation: Time-unconditional denoising models share a single input for different noise levels, causing conflicting supervision and suboptimal denoising directions. A time-free approach that grounds targets on proximity to the data manifold can improve supervision and sample quality.

Method: Introduce Distance Marching with two principled inference methods and losses that emphasize closer targets, inspired by distance field modeling. Incorporates a distance prediction component that aids inference and downstream tasks.

Result: Consistent FID gains across architectures: +13.5% on CIFAR-10 and ImageNet over recent time-unconditional baselines. For class-conditional ImageNet, outperforms flow matching using the proposed losses and inference methods, achieving lower FID with 60% fewer sampling steps and 13.6% lower FID on average across backbones.

Conclusion: Distance field modeling offers a principled lens for generative modeling; distance prediction supports early stopping and OOD detection, suggesting broad applicability of the approach.

Abstract: Time-unconditional generative models learn time-independent denoising vector fields. But without time conditioning, the same noisy input may correspond to multiple noise levels and different denoising directions, which interferes with the supervision signal. Inspired by distance field modeling, we propose Distance Marching, a new time-unconditional approach with two principled inference methods. Crucially, we design losses that focus on closer targets. This yields denoising directions better directed toward the data manifold. Across architectures, Distance Marching consistently improves FID by 13.5% on CIFAR-10 and ImageNet over recent time-unconditional baselines. For class-conditional ImageNet generation, despite removing time input, Distance Marching surpasses flow matching using our losses and inference methods. It achieves lower FID than flow matching's final performance using 60% of the sampling steps and 13.6% lower FID on average across backbone sizes. Moreover, our distance prediction is also helpful for early stopping during sampling and for OOD detection. We hope distance field modeling can serve as a principled lens for generative modeling.

</details>


### [206] [RPG-AE: Neuro-Symbolic Graph Autoencoders with Rare Pattern Mining for Provenance-Based Anomaly Detection](https://arxiv.org/abs/2602.02929)
*Asif Tauhid,Sidahmed Benabderrahmane,Mohamad Altrabulsi,Ahamed Foisal,Talal Rahwan*

Main category: cs.LG

TL;DR: 通过将图自编码器(GAE)与罕见模式挖掘相结合的神经符号框架，在系统溯源数据中检测APT样活动，基于k最近邻构建的进程行为图进行学习，利用重构误差识别异常，并通过罕见模式提升分数，在DARPA透明计算数据集上对比基线GAE和其他单一/集合检测器取得显著提升，兼具效果与可解释性。


<details>
  <summary>Details</summary>
Motivation: APTs具有高度隐蔽性、长期性，难以在常规系统行为中识别。系统溯源数据提供丰富的时空关系信息，需无监督或弱监督的高效检测，同时需提高可解释性。将图表示学习与经典模式挖掘结合，期望提升检测质量并提供可解释的异常原因。

Method: 构建基于特征相似性的k-NN进程行为图；用图自编码器学习正常关系结构；基于观测与重构之间的偏差判定候选异常；引入罕见模式挖掘，发现不常见的行为共现并用以提升罕见签名的异常分数；在DARPA Transparent Computing数据集上评估，与现有无监督方法对比，单模型优于各自的上下文检测器，且接近集成方法的性能。

Result: 罕见模式提升显著改善异常排序质量，相较于基线GAE获得明显收益；单一模型在同一基准上超越独立的上下文检测器，且与需要多检测器的集成方法相当；结果说明将图学习与模式挖掘耦合在对溯源安全检测中具有更好效果与可解释性。

Conclusion: 将图表示学习与经典模式挖掘耦合，可在基于溯源数据的安全异常检测中提升检测效果与解释性，并可实现一个统一的模型替代多检测器组合。

Abstract: Advanced Persistent Threats (APTs) are sophisticated, long-term cyberattacks that are difficult to detect because they operate stealthily and often blend into normal system behavior. This paper presents a neuro-symbolic anomaly detection framework that combines a Graph Autoencoder (GAE) with rare pattern mining to identify APT-like activities in system-level provenance data. Our approach first constructs a process behavioral graph using k-Nearest Neighbors based on feature similarity, then learns normal relational structure using a Graph Autoencoder. Anomaly candidates are identified through deviations between observed and reconstructed graph structure. To further improve detection, we integrate an rare pattern mining module that discovers infrequent behavioral co-occurrences and uses them to boost anomaly scores for processes exhibiting rare signatures. We evaluate the proposed method on the DARPA Transparent Computing datasets and show that rare-pattern boosting yields substantial gains in anomaly ranking quality over the baseline GAE. Compared with existing unsupervised approaches on the same benchmark, our single unified model consistently outperforms individual context-based detectors and achieves performance competitive with ensemble aggregation methods that require multiple separate detectors. These results highlight the value of coupling graph-based representation learning with classical pattern mining to improve both effectiveness and interpretability in provenance-based security anomaly detection.

</details>


### [207] [Rare Event Early Detection: A Dataset of Sepsis Onset for Critically Ill Trauma Patients](https://arxiv.org/abs/2602.02930)
*Yin Jin,Tucker R. Stewart,Deyi Zhou,Chhavi Gupta,Arjita Nema,Scott C. Brakenridge,Grant E. O'Keefe,Juhua Hu*

Main category: cs.LG

TL;DR: 提出公开的创伤后败血症发病时点数据集，基于MIMIC-III进行再标注，适配ICU工作流中的日常监测，形成稀有事件检测问题的基准并提供代码仓库。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集将ICU患者视作一个同质群体，忽略创伤后炎症和器官功能障碍与败血症特征的重叠；需要针对创伤场景的早期识别数据资源。

Method: 从创伤患者队列中提取并用标准化的创伤后临床事实重新标注败血症发病时点，基于MIMIC-III进行验证；将早期检测问题定义为ICU日常工作流中的罕见事件检测，进行全面实验以建立基准。

Result: 建立公开的标准化创伤后败血症发病时点数据集并给出基准实验，表明在该数据集上仍需进一步方法创新；数据代码公开。

Conclusion: 该工作强调需要针对创伤后败血症的早期检测方法，并提供数据集和基准以推动相关研究的发展。

Abstract: Sepsis is a major public health concern due to its high morbidity, mortality, and cost. Its clinical outcome can be substantially improved through early detection and timely intervention. By leveraging publicly available datasets, machine learning (ML) has driven advances in both research and clinical practice. However, existing public datasets consider ICU patients (Intensive Care Unit) as a uniform group and neglect the potential challenges presented by critically ill trauma patients in whom injury-related inflammation and organ dysfunction can overlap with the clinical features of sepsis. We propose that a targeted identification of post-traumatic sepsis is necessary in order to develop methods for early detection. Therefore, we introduce a publicly available standardized post-trauma sepsis onset dataset extracted, relabeled using standardized post-trauma clinical facts, and validated from MIMIC-III. Furthermore, we frame early detection of post-trauma sepsis onset according to clinical workflow in ICUs in a daily basis resulting in a new rare event detection problem. We then establish a general benchmark through comprehensive experiments, which shows the necessity of further advancements using this new dataset. The data code is available at https://github.com/ML4UWHealth/SepsisOnset_TraumaCohort.git.

</details>


### [208] [3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning](https://arxiv.org/abs/2602.02943)
*Jiaqi Wen,Lei Fan,Jianyi Yang*

Main category: cs.LG

TL;DR: 提出 3D-Learning：基于扩散模型的分布鲁棒的决策聚焦学习，用以解决 PTO 流水线中 OOD 的预测误差对决策的影响，通过在参数化扩散模型的分布空间中搜索最坏分布，平衡均值与最坏情况表现，实验在 LLM 资源分配任务上优于传统 DRO 与数据增强方法。


<details>
  <summary>Details</summary>
Motivation: PTO 流水线中的预测器在测试时易受到 OOD 样本的影响，导致决策性能显著下降，因此需要一种能在最坏分布下仍能优化决策性能的学习框架。

Method: 建立 Distributionally Robust Decision-Focused Learning (DR-DFL) 框架，在此基础上提出 Diffusion-Augmented Distributionally Robust Decision-Focused Learning（3D-Learning），用扩散模型的参数化分布空间搜索最坏分布，结合对下游决策任务的鲁棒优化来训练预测模型。

Result: 在 LLM 资源配置任务上，3D-Learning 在 OOD 泛化性能上优于现有的 DRO 与数据增强方法。

Conclusion: 利用扩散模型在分布建模上的强大能力，3D-Learning 能在平均性能与最坏-case 性能之间取得更好的折中，提供一种对抗 OOD 的有效策略，具有潜在的广泛适用性。

Abstract: Predict-then-Optimize (PTO) pipelines are widely employed in computing and networked systems, where Machine Learning (ML) models are used to predict critical contextual information for downstream decision-making tasks such as cloud LLM serving, data center demand response, and edge workload scheduling. However, these ML predictors are often vulnerable to out-of-distribution (OOD) samples at test time, leading to significant decision performance degradation due to large prediction errors. To address the generalization challenges under OOD conditions, we present the framework of Distributionally Robust Decision-Focused Learning (DR-DFL), which trains ML models to optimize decision performance under the worst-case distribution. Instead of relying on classical Distributionally Robust Optimization (DRO) techniques, we propose Diffusion-Augmented Distributionally Robust Decision-Focused Learning (3D-Learning), which searches for the worst-case distribution within the parameterized space of a diffusion model. By leveraging the powerful distribution modeling capabilities of diffusion models, 3D-Learning identifies worst-case distributions that remain consistent with real data, achieving a favorable balance between average and worst-case scenarios. Empirical results on an LLM resource provisioning task demonstrate that 3D-Learning outperforms existing DRO and Data Augmentation methods in OOD generalization performance.

</details>


### [209] [Variational Sparse Paired Autoencoders (vsPAIR) for Inverse Problems and Uncertainty Quantification](https://arxiv.org/abs/2602.02948)
*Jack Michael Solomon,Rishi Leburu,Matthias Chung*

Main category: cs.LG

TL;DR: 提出 vsPAIR，一种变分稀疏配对自编码器，通过将观测编码与QoI编码配对并用学习的潜在映射连接，提供快速且可解释的不确定性估计；引入硬连续 Spike-and-Slab 放松与 Beta 超先验，在盲修复和计算断层成像（CT）上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 逆问题通常需要在噪声测量下重建隐藏量并给出不确定性估计；需快速推断且具可解释性，但现有方法难以将信息集中到可识别的因子上并提供结构化的不确定性。

Method: 双VAE结构：对观测建立标准VAE编码，对QoI建立稀疏VAE编码；两者通过学习的潜在映射连接；利用变分推断实现不确定性估计；对稀疏VAE的改进包括硬连续Spike-and-Slab放松以实现可微训练，以及Beta超先验以自适应稀疏度。

Result: 在盲修复与CT成像任务中进行实验，结果表明 vsPAIR 能作为逆问题求解器，提供可解释且结构化的不确定性估计。

Conclusion: vsPAIR 将配对稀疏表示与潜在映射用于逆问题，获得可解释且结构化的不确定性；提出的硬连续放松和Beta超先验提升稀疏性自适应性，适用于其他逆问题场景。

Abstract: Inverse problems are fundamental to many scientific and engineering disciplines; they arise when one seeks to reconstruct hidden, underlying quantities from noisy measurements. Many applications demand not just point estimates but interpretable uncertainty. Providing fast inference alongside uncertainty estimates remains challenging yet desirable in numerous applications.
  We propose the Variational Sparse Paired Autoencoder (vsPAIR) to address this challenge. The architecture pairs a standard VAE encoding observations with a sparse VAE encoding quantities of interest, connected through a learned latent mapping. The variational structure enables uncertainty estimation, the paired architecture encourages interpretability by anchoring QoI representations to clean data, and sparse encodings provide structure by concentrating information into identifiable factors rather than diffusing across all dimensions. We also propose modifications to existing sparse VAE methods: a hard-concrete spike-and-slab relaxation for differentiable training and a beta hyperprior for adaptive sparsity levels. To validate the effectiveness of our proposed architecture, we conduct experiments on blind inpainting and computed tomography, demonstrating that vsPAIR is a capable inverse problem solver that can provide interpretable and structured uncertainty estimates.

</details>


### [210] [Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization](https://arxiv.org/abs/2602.02958)
*Haocheng Xi,Shuo Yang,Yilong Zhao,Muyang Li,Han Cai,Xingyang Li,Yujun Lin,Zhuoyang Zhang,Jintao Zhang,Xiuyu Li,Zhiying Xu,Jun Wu,Chenfeng Xu,Ion Stoica,Song Han,Kurt Keutzer*

Main category: cs.LG

TL;DR: QVG为自回归视频扩散模型提供了训练自由的KV缓存量化框架，通过语义感知平滑和渐进残差量化，在不 retrain 的前提下大幅降低KV缓存内存占用，同时保持或提升生成质量，且在LongCat Video、HY WorldPlay、Self Forcing等基准上实现内存减少至7倍、端到端延迟增量<4%。


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成模型的KV缓存随生成历史增长而快速占用显存，常超30GB，制约在通用硬件上的部署；有限的KV缓存预算直接削弱长时域的一致性（身份、布局、运动），需要一个不需额外训练的内存压缩方案。

Method: 提出Quant VideoGen（QVG）：训练自由的KV缓存量化框架。利用Semantic Aware Smoothing挖掘时空冗余，产生低幅度、量化友好残差；引入Progressive Residual Quantization，分层粗到细的多阶段量化以降低误差并实现质量—内存权衡的平滑过渡。

Result: 在LongCat Video、HY WorldPlay、Self Forcing等基准上，QVG在质量与内存之间实现新帕累托前沿，KV缓存内存降低最多7.0倍，端到端延迟开销小于4%，且生成质量持续优于现有基线。

Conclusion: QVG通过利用时空冗余的语义感知平滑和分阶段残差量化，提供了训练自由的KV缓存压缩方案，有望推动自回归视频扩散模型在资源受限设备上的实际部署，同时保持甚至提升生成质量。

Abstract: Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.

</details>


### [211] [Human-Centric Traffic Signal Control for Equity: A Multi-Agent Action Branching Deep Reinforcement Learning Approach](https://arxiv.org/abs/2602.02959)
*Xiaocai Zhang,Neema Nassir,Lok Sang Chan,Milad Haghani*

Main category: cs.LG

TL;DR: Action-branching multi-agent DDQN for equitable, scalable traffic signal control on multimodal corridors.


<details>
  <summary>Details</summary>
Motivation: Addresses high-dimensional discrete action spaces and vehicle-centric optimization in multi-agent DRL for traffic signals; aims to improve traveler-level equity across pedestrians, vehicles, and transit.

Method: Proposes action-branching discrete control: per-intersection actions allocate green time between the next two phases (local actions) and a single global action sets the total duration of those phases; uses a human-centric reward penalizing delayed individuals across pedestrians, vehicle occupants, and transit passengers.

Result: Outperforms existing DRL and baseline methods across seven realistic Melbourne scenarios; significantly reduces the number of impacted travelers and shows robustness with minimal variance across diverse settings.

Conclusion: Introduces a scalable, fair traffic signal framework adaptable to varied urban conditions, balancing coordination with equity for multimodal travelers.

Abstract: Coordinating traffic signals along multimodal corridors is challenging because many multi-agent deep reinforcement learning (DRL) approaches remain vehicle-centric and struggle with high-dimensional discrete action spaces. We propose MA2B-DDQN, a human-centric multi-agent action-branching double Deep Q-Network (DQN) framework that explicitly optimizes traveler-level equity. Our key contribution is an action-branching discrete control formulation that decomposes corridor control into (i) local, per-intersection actions that allocate green time between the next two phases and (ii) a single global action that selects the total duration of those phases. This decomposition enables scalable coordination under discrete control while reducing the effective complexity of joint decision-making. We also design a human-centric reward that penalizes the number of delayed individuals in the corridor, accounting for pedestrians, vehicle occupants, and transit passengers. Extensive evaluations across seven realistic traffic scenarios in Melbourne, Australia, demonstrate that our approach significantly reduces the number of impacted travelers, outperforming existing DRL and baseline methods. Experiments confirm the robustness of our model, showing minimal variance across diverse settings. This framework not only advocates for a fairer traffic signal system but also provides a scalable solution adaptable to varied urban traffic conditions.

</details>


### [212] [Q-ShiftDP: A Differentially Private Parameter-Shift Rule for Quantum Machine Learning](https://arxiv.org/abs/2602.02962)
*Hoang M. Ngo,Nhat Hoang-Xuan,Quan Nguyen,Nguyen Do,Incheol Shin,My T. Thai*

Main category: cs.LG

TL;DR: 提出第一种用于量子机器学习的差分隐私机制 Q-ShiftDP，基于参数偏移法的梯度，结合高斯噪声与量子噪声，提供隐私与实用性保证，在QML任务上优于传统DP方法。


<details>
  <summary>Details</summary>
Motivation: 解决量子机器学习训练中的数据隐私挑战；现有 DP-SGD 未能充分利用量子梯度估计的特性，导致隐私-实用性权衡受限。

Method: 利用参数偏移法获得有界且具随机性的量子梯度，进行敏感性分析；谨慎标定高斯噪声，并结合内在量子噪声以提升隐私与实用性。

Result: 在基准数据集上实验，Q-ShiftDP 在隐私与实用性方面持续优于经典 DP 方法，表明结合量子噪声可改善隐私-效用权衡。

Conclusion: 首个面向量子机器学习的隐私机制，理论与实验均显示可行性与优势，为QML隐私保护提供新方向。

Abstract: Quantum Machine Learning (QML) promises significant computational advantages, but preserving training data privacy remains challenging. Classical approaches like differentially private stochastic gradient descent (DP-SGD) add noise to gradients but fail to exploit the unique properties of quantum gradient estimation. In this work, we introduce the Differentially Private Parameter-Shift Rule (Q-ShiftDP), the first privacy mechanism tailored to QML. By leveraging the inherent boundedness and stochasticity of quantum gradients computed via the parameter-shift rule, Q-ShiftDP enables tighter sensitivity analysis and reduces noise requirements. We combine carefully calibrated Gaussian noise with intrinsic quantum noise to provide formal privacy and utility guarantees, and show that harnessing quantum noise further improves the privacy-utility trade-off. Experiments on benchmark datasets demonstrate that Q-ShiftDP consistently outperforms classical DP methods in QML.

</details>


### [213] [Co2PO: Coordinated Constrained Policy Optimization for Multi-Agent RL](https://arxiv.org/abs/2602.02970)
*Shrenik Patel,Christine Truong*

Main category: cs.LG

TL;DR: Co2PO提出一种通信增强的约束多智能体强化学习框架，通过风险感知的选择性通信和共享记忆，提前预测并规避集体风险，在不牺牲探索的前提下提升性能，并确保成本约束的合规性。


<details>
  <summary>Details</summary>
Motivation: 解决探索与安全约束之间的根本矛盾；现有方法（如拉格朗日/集中 critic）对违规反应滞后，依赖全局惩罚导致探索受抑，过于保守。

Method: 引入共享的黑板结构用于广播位置信息意图和让渡信号；学习型 hazard predictor 在长时域预测潜在违规；将预测结果整合入约束优化目标；实现风险触发通信、 adaptive gating 以及共享记忆。

Result: 在复杂多-agent 安全基准上，Co2PO取得更高回报，并在部署时收敛到成本合规的策略；消融研究证实风险触发通信、自适应门控和共享记忆的必要性。

Conclusion: 通过前瞻性风险预测与选择性通信，缓解传统反应式约束的探索-安全权衡，达到更高的性能和合规性。

Abstract: Constrained multi-agent reinforcement learning (MARL) faces a fundamental tension between exploration and safety-constrained optimization. Existing leading approaches, such as Lagrangian methods, typically rely on global penalties or centralized critics that react to violations after they occur, often suppressing exploration and leading to over-conservatism. We propose Co2PO, a novel MARL communication-augmented framework that enables coordination-driven safety through selective, risk-aware communication. Co2PO introduces a shared blackboard architecture for broadcasting positional intent and yield signals, governed by a learned hazard predictor that proactively forecasts potential violations over an extended temporal horizon. By integrating these forecasts into a constrained optimization objective, Co2PO allows agents to anticipate and navigate collective hazards without the performance trade-offs inherent in traditional reactive constraints. We evaluate Co2PO across a suite of complex multi-agent safety benchmarks, where it achieves higher returns compared to leading constrained baselines while converging to cost-compliant policies at deployment. Ablation studies further validate the necessity of risk-triggered communication, adaptive gating, and shared memory components.

</details>


### [214] [Why Some Models Resist Unlearning: A Linear Stability Perspective](https://arxiv.org/abs/2602.02986)
*Wei-Kai Chang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出数据一致性与渐近线性稳定性框架来分析机器学习中的遗忘，给出跨样本对齐在记忆/忘记过程中的边界；通过两层 ReLU CNN 的分析与随机矩阵理论验证，以及Hessian测试和热图的经验对齐。


<details>
  <summary>Details</summary>
Motivation: 目前对机器学习中的“遗忘”缺乏理论理解，难以解释数据几何与优化动态如何共同决定是否能够在不重新训练的情况下有效遗忘特定样本。

Method: 将数据一致性定义为损失面在最优附近的跨样本对齐程度，并在 retain/forget/interactions 三轴上分解，推导出将收敛与发散区分的稳定性阈值。以信号+噪声模型下的两层 ReLU CNN 为例，研究 memorization 与 forgetability 的关系；并结合随机矩阵理论、Hessian 测试和CNN热图进行实验验证。

Result: 给出严格的稳定性边界，随 memorization 强度增加（高SNR）遗忘变得更困难；低SNR 时跨样本对齐较弱、coherence 降低、遗忘更易实现。实验中Hessian测试和热图与理论边界高度一致，首次系统地建立了 memorization、coherence 与 unlearning 之间的关系。

Conclusion: 建立了一个原理性框架来解释遗忘的可行性，揭示数据属性与模型-数据对齐如何共同决定遗忘难易，并提供可操作的度量工具用于评估和设计遗忘机制。

Abstract: Machine unlearning, the ability to erase the effect of specific training samples without retraining from scratch, is critical for privacy, regulation, and efficiency. However, most progress in unlearning has been empirical, with little theoretical understanding of when and why unlearning works. We tackle this gap by framing unlearning through the lens of asymptotic linear stability to capture the interaction between optimization dynamics and data geometry. The key quantity in our analysis is data coherence which is the cross sample alignment of loss surface directions near the optimum. We decompose coherence along three axes: within the retain set, within the forget set, and between them, and prove tight stability thresholds that separate convergence from divergence. To further link data properties to forgettability, we study a two layer ReLU CNN under a signal plus noise model and show that stronger memorization makes forgetting easier: when the signal to noise ratio (SNR) is lower, cross sample alignment is weaker, reducing coherence and making unlearning easier; conversely, high SNR, highly aligned models resist unlearning. For empirical verification, we show that Hessian tests and CNN heatmaps align closely with the predicted boundary, mapping the stability frontier of gradient based unlearning as a function of batching, mixing, and data/model alignment. Our analysis is grounded in random matrix theory tools and provides the first principled account of the trade offs between memorization, coherence, and unlearning.

</details>


### [215] [Learning to Repair Lean Proofs from Compiler Feedback](https://arxiv.org/abs/2602.02990)
*Evan Wang,Simon Chess,Daniel Lee,Siyuan Ge,Ajit Mallavarapu,Vasily Ilin*

Main category: cs.LG

TL;DR: 提出 APRIL 数据集用于 Lean 的证明修复与诊断学习，通过系统生成的失败、编译器诊断和对齐的修复/解释目标来提升修复能力，单 shot 下 4B 参数模型优于开源基线。


<details>
  <summary>Details</summary>
Motivation: 现有 Lean 数据集多聚焦正确证明，缺乏对失败原因的监督，限制了对编译器反馈的理解与修复能力。

Method: 构建包含约26万条监督三元组的 APRIL 数据集：给出错误证明、编译器诊断、对齐的修复目标与自然语言诊断；在此基础上对语言模型进行微调，评估在单-shot 修复任务中的表现，并探索诊断信号对反馈使用的推理能力的促进作用。

Result: 训练后修复准确性和对反馈的推理能力显著提升；在单-shot 修复评估中，微调的4B参数模型超越最强的开源基线。

Conclusion: 将诊断条件监督视为对反馈使用的证明系统的互补信号，提升对编译反馈的利用能力；数据集公开可获取，推动相关研究发展。

Abstract: As neural theorem provers become increasingly agentic, the ability to interpret and act on compiler feedback is critical. However, existing Lean datasets consist almost exclusively of correct proofs, offering little supervision for understanding and repairing failures. We study Lean proof repair as a supervised learning problem: given an erroneous proof and compiler feedback, predict both a corrected proof and a natural-language diagnosis grounded in the same feedback. We introduce APRIL (Automated Proof Repair in Lean), a dataset of 260,000 supervised tuples pairing systematically generated proof failures with compiler diagnostics and aligned repair and explanation targets. Training language models on APRIL substantially improves repair accuracy and feedback-conditioned reasoning; in our single-shot repair evaluation setting, a finetuned 4B-parameter model outperforms the strongest open-source baseline. We view diagnostic-conditioned supervision as a complementary training signal for feedback-using provers. Our dataset is available at \href{https://huggingface.co/datasets/uw-math-ai/APRIL}{this link}.

</details>


### [216] [Adaptive Batch Sizes Using Non-Euclidean Gradient Noise Scales for Stochastic Sign and Spectral Descent](https://arxiv.org/abs/2602.03001)
*Hiroki Naganuma,Shagun Gupta,Youssef Briki,Ioannis Mitliagkas,Irina Rish,Parameswaran Raman,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: 提出面向非欧几里得范数的梯度噪声尺度（GNS），为 signSGD/Signum 和 specSGD/Muon 等优化器设计自适应批量大小策略；给出在分布式数据并行中估计非欧几里得 GNS 的高效方差估计方法；在 1.6e8 参数的 Llama 模型上验证：与常量批量基线相比，减少训练步数可达 66%，并保持验证损失。


<details>
  <summary>Details</summary>
Motivation: 当前系统为提高硬件利用率，常用大规模固定或手工调优的批量大小计划，且对批量大小的自适应策略多基于梯度噪声尺度（GNS）但假设 SGD 的欧几里得几何。该假设与基于广义范数（如 L_inf 的 signSGD/Signum、L_infty 相关的 specSGD/Muon）的优化器存在几何不匹配，导致 GNS 作为自适应信号的适用性受限。需要从非欧几里得几何出发，推导与目标优化器一致的梯度噪声尺度，并提出可在分布式系统中高效估计的实用方法，以提升自适应批量策略的效果。

Method: 从非欧几里得几何出发，推导与 signSGD/SpecSGD 对应的梯度噪声尺度，揭示其来自相应对偶范数的几何结构。提出一种基于本地小批量梯度在分布式数据并行中的不同秩(rank)的高效方差估计流程，用以实测非欧几里得 GNS。通过在大规模模型（如 160M 参数的 Llama）上的实验，验证该自适应批量策略的有效性。

Result: 在将非欧几里得 GNS 应用于自适应批量调整后，能够在验证损失与固定批量基线相当的前提下，显著减少训练步数，具体在 Signum 和 Muon 上的实验中，训练步数缩减最多达到 66%，模型规模为 1.6e8 参数的 Llama。

Conclusion: 将梯度噪声尺度迁移至非欧几里得优化器的几何框架下，并给出可在分布式环境中高效估计的实用化方案，证实了非欧几里得 GNS 的实际效用，能够在不牺牲验证性能的前提下显著提升训练效率。

Abstract: To maximize hardware utilization, modern machine learning systems typically employ large constant or manually tuned batch size schedules, relying on heuristics that are brittle and costly to tune. Existing adaptive strategies based on gradient noise scale (GNS) offer a principled alternative. However, their assumption of SGD's Euclidean geometry creates a fundamental mismatch with popular optimizers based on generalized norms, such as signSGD / Signum ($\ell_\infty$) and stochastic spectral descent (specSGD) / Muon ($\mathcal{S}_\infty$). In this work, we derive gradient noise scales for signSGD and specSGD that naturally emerge from the geometry of their respective dual norms. To practically estimate these non-Euclidean metrics, we propose an efficient variance estimation procedure that leverages the local mini-batch gradients on different ranks in distributed data-parallel systems. Our experiments demonstrate that adaptive batch size strategies using non-Euclidean GNS enable us to match the validation loss of constant-batch baselines while reducing training steps by up to 66% for Signum and Muon on a 160 million parameter Llama model.

</details>


### [217] [Causal Graph Spatial-Temporal Autoencoder for Reliable and Interpretable Process Monitoring](https://arxiv.org/abs/2602.03004)
*Xiangrui Zhang,Chunyue Song,Wei Dai,Zheng Zhang,Kaihua Gao,Furong Gao*

Main category: cs.LG

TL;DR: CGSTAE combines spatial-temporal graph learning with causal structure discovery to enhance industrial process monitoring and fault detection. It learns dynamic variable correlations via spatial self-attention, derives a causal graph using a three-step invariance-based algorithm, and models time-series with a graph-convolutional LSTM encoder-decoder, validated on familiar benchmarks.


<details>
  <summary>Details</summary>
Motivation: Improve reliability and interpretability of industrial process monitoring by capturing dynamic relationships among process variables and translating correlations into an interpretable causal structure for robust fault detection.

Method: 1) Spatial self-attention-based correlation graph learning (SSAM) to learn dynamic variable relationships. 2) A novel three-step causal graph structure learning algorithm using a reverse invariance principle to extract an invariant causal graph from varying correlations. 3) A spatial-temporal encoder-decoder built from graph convolutional LSTM units (GCLSTM) to reconstruct time-series data in a seq2seq framework. 4) Two statistics in feature space and residual space for monitoring and fault detection. 5) Validation on Tennessee Eastman process and real-world air separation process.

Result: CGSTAE demonstrates improved monitoring and fault detection performance (through the proposed statistics) and provides interpretable causal graphs that align with known process structure, as evidenced by results on TE process and real-world data.

Conclusion: The proposed CGSTAE offers a cohesive framework integrating dynamic correlation learning, causal graph extraction, and spatial-temporal reconstruction, yielding interpretable insights and enhanced process monitoring in industrial settings; further work could address identification assumptions and scalability.

Abstract: To improve the reliability and interpretability of industrial process monitoring, this article proposes a Causal Graph Spatial-Temporal Autoencoder (CGSTAE). The network architecture of CGSTAE combines two components: a correlation graph structure learning module based on spatial self-attention mechanism (SSAM) and a spatial-temporal encoder-decoder module utilizing graph convolutional long-short term memory (GCLSTM). The SSAM learns correlation graphs by capturing dynamic relationships between variables, while a novel three-step causal graph structure learning algorithm is introduced to derive a causal graph from these correlation graphs. The algorithm leverages a reverse perspective of causal invariance principle to uncover the invariant causal graph from varying correlations. The spatial-temporal encoder-decoder, built with GCLSTM units, reconstructs time-series process data within a sequence-to-sequence framework. The proposed CGSTAE enables effective process monitoring and fault detection through two statistics in the feature space and residual space. Finally, we validate the effectiveness of CGSTAE in process monitoring through the Tennessee Eastman process and a real-world air separation process.

</details>


### [218] [From Zero to Hero: Advancing Zero-Shot Foundation Models for Tabular Outlier Detection](https://arxiv.org/abs/2602.03018)
*Xueying Ding,Haomin Wen,Simon Klütterman,Leman Akoglu*

Main category: cs.LG

TL;DR: OUTFORMER 是一个基于零-shot、在上下文学习（in-context learning）基础上的异常检测器，使用合成数据进行训练，能够直接使用测试数据作为上下文输入进行推断，支持即插即用部署，在 AdBench 等数据集上达到SOTA，并在大规模新基准上保持快速推理。


<details>
  <summary>Details</summary>
Motivation: 缺乏标注的异常样本使得OD算法和超参数选择困难；通用的大型基础模型已对ML产生影响，现有OD方法在零样本部署方面存在局限；需要一种无需额外OD模型训练、可即插即用、且推理快速的解决方案。

Method: 通过混合合成先验与自我进化的课程训练，进行仅在合成带标签数据上预训练；推断阶段以测试数据作为上下文输入进行使用，零-shot 推断，仅需前向计算，无需标注的异常样本；实现无额外OD模型训练与模型选择。

Result: 在AdBench上达到最先进性能，并在作者新提出的两组大规模OD基准上也取得竞争力，数据集规模超过1500个，且推理速度较快。

Conclusion: 证明了在大规模基础模型上通过在上下文输入中进行学习的零-shot OD的可行性，极大降低部署门槛，具有真正的“即插即用”潜力。

Abstract: Outlier detection (OD) is widely used in practice; but its effective deployment on new tasks is hindered by lack of labeled outliers, which makes algorithm and hyperparameter selection notoriously hard. Foundation models (FMs) have transformed ML, and OD is no exception: Shen et. al. (2025) introduced FoMo-0D, the first FM for OD, achieving remarkable performance against numerous baselines. This work introduces OUTFORMER, which advances FoMo-0D with (1) a mixture of synthetic priors and (2) self-evolving curriculum training. OUTFORMER is pretrained solely on synthetic labeled datasets and infers test labels of a new task by using its training data as in-context input. Inference is fast and zero-shot, requiring merely forward pass and no labeled outliers. Thanks to in-context learning, it requires zero additional work-no OD model training or bespoke model selection-enabling truly plug-and-play deployment. OUTFORMER achieves state-of-the-art performance on the prominent AdBench, as well as two new large-scale OD benchmarks that we introduce, comprising over 1,500 datasets, while maintaining speedy inference.

</details>


### [219] [FedKRSO: Communication and Memory Efficient Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.03019)
*Guohao Yang,Tongle Wu,Yuanxiong Guo,Ying Sun,Yanmin Gong*

Main category: cs.LG

TL;DR: FedKRSO 提出一种基于随机低维子空间的联邦微调框架，通过在服务器端生成子空间、客户端在子空间内更新并仅传输累积更新，实现显著降低通信与内存开销，同时在接近全参数微调的性能。


<details>
  <summary>Details</summary>
Motivation: 在隐私保护的分布式数据场景中，传统全参数微调成本高且难以在资源受限的客户端实现；尽管PEFT降低了通信成本，但常伴随性能损失。需要在保持性能的前提下进一步降低通信与内存开销，以实现联邦LLM微调的实用化。

Method: 服务器生成一组随机低维子空间；客户端在这些子空间内对模型进行更新；每轮仅传输按子空间的模型更新累积量，进行全局聚合与传播；并对收敛性在一般FL场景下给出理论分析；在GLUE数据集的多种FL设定下进行广泛实验。

Result: 在显著降低通信和内存开销的同时，FedKRSO 的性能接近联邦FFT，优于基于PEFT的方案；理论分析与大量实验证明在资源受限的边缘场景下具有竞争力。

Conclusion: FedKRSO 为联邦LLM微调在资源受限的边缘设备上提供了一种高效且接近FFT性能的解决方案，推动隐私保护条件下的大规模模型协同训练的落地。

Abstract: Fine-tuning is essential to adapt general-purpose large language models (LLMs) to domain-specific tasks. As a privacy-preserving framework to leverage decentralized data for collaborative model training, Federated Learning (FL) is gaining popularity in LLM fine-tuning, but remains challenging due to the high cost of transmitting full model parameters and computing full gradients on resource-constrained clients. While Parameter-Efficient Fine-Tuning (PEFT) methods are widely used in FL to reduce communication and memory costs, they often sacrifice model performance compared to FFT. This paper proposes FedKRSO (Federated $K$-Seed Random Subspace Optimization), a novel method that enables communication and memory efficient FFT of LLMs in federated settings. In FedKRSO, clients update the model within a shared set of random low-dimension subspaces generated by the server to save memory usage. Furthermore, instead of transmitting full model parameters in each FL round, clients send only the model update accumulators along the subspaces to the server, enabling efficient global model aggregation and dissemination. By using these strategies, FedKRSO can substantially reduce communication and memory overhead while overcoming the performance limitations of PEFT, closely approximating the performance of federated FFT. The convergence properties of FedKRSO are analyzed rigorously under general FL settings. Extensive experiments on the GLUE benchmark across diverse FL scenarios demonstrate that FedKRSO achieves both superior performance and low communication and memory overhead, paving the way towards on federated LLM fine-tuning at the resource-constrained edge.

</details>


### [220] [Consistency Deep Equilibrium Models](https://arxiv.org/abs/2602.03024)
*Junchao Lin,Zenan Ling,Jingwen Xu,Robert C. Qiu*

Main category: cs.LG

TL;DR: C-DEQ proposes consistency distillation for Deep Equilibrium Models (DEQs), treating the fixed-point solve as an ODE trajectory and learning intermediate-state mappings to the fixed point. This enables few-step, flexible-cost inference with strong accuracy gains over standard implicit DEQs.


<details>
  <summary>Details</summary>
Motivation: DEQs require iterative fixed-point solvers, incurring high inference latency. There is a need to accelerate inference while preserving or improving accuracy and enabling flexible compute-performance trade-offs.

Method: Model the DEQ iteration as evolution along a fixed ODE trajectory toward equilibrium. Train Consistency Deep Equilibrium Models (C-DEQs) to consistently map intermediate states along this trajectory directly to the fixed point using consistency distillation, enabling few-step inference and multi-step evaluation.

Result: Empirical results show 2–20× accuracy improvements over implicit DEQs under the same few-step budget across diverse tasks, demonstrating effective acceleration without sacrificing performance.

Conclusion: Consistency distillation on the ODE-informed trajectory yields efficient, flexible DEQ inference (C-DEQ) that preserves teacher performance with reduced computation and supports multi-step evaluation for additional trade-offs.

Abstract: Deep Equilibrium Models (DEQs) have emerged as a powerful paradigm in deep learning, offering the ability to model infinite-depth networks with constant memory usage. However, DEQs incur significant inference latency due to the iterative nature of fixed-point solvers. In this work, we introduce the Consistency Deep Equilibrium Model (C-DEQ), a novel framework that leverages consistency distillation to accelerate DEQ inference. We cast the DEQ iterative inference process as evolution along a fixed ODE trajectory toward the equilibrium. Along this trajectory, we train C-DEQs to consistently map intermediate states directly to the fixed point, enabling few-step inference while preserving the performance of the teacher DEQ. At the same time, it facilitates multi-step evaluation to flexibly trade computation for performance gains. Extensive experiments across various domain tasks demonstrate that C-DEQs achieves consistent 2-20$\times$ accuracy improvements over implicit DEQs under the same few-step inference budget.

</details>


### [221] [SAFE-KD: Risk-Controlled Early-Exit Distillation for Vision Backbones](https://arxiv.org/abs/2602.03043)
*Salim Khazem*

Main category: cs.LG

TL;DR: 提出 SAFE-KD，一种通用多出口包裹器，结合分层蒸馏和 conformal risk control，在视觉模型中对中间退出进行阈值校准，提供可控的早退出风险和有限样本保障。


<details>
  <summary>Details</summary>
Motivation: 降低推理成本同时对早退出的安全性提供保障；解决何时允许早退出以及如何可靠地对早退出进行风险控制的问题。

Method: 在中间深度添加轻量型退出头，利用 DKD 将强教师知识蒸馏到所有出口，并强制退出之间的深到浅的一致性；推理阶段在保留集上使用 CRC 对每个出口进行阈值校准，以在给定的选择性失分类风险下提供无偏差的上界。

Result: 在多数据集和多架构上，SAFE-KD 在准确性-计算成本权衡、校准性和对腐蚀鲁棒性方面表现更好，同时提供有限样本风险保证。

Conclusion: SAFE-KD 提供一个通用、可扩展的多出口框架，兼具良好校准和风险保证，适用于现代视觉骨干网的高效推理。

Abstract: Early-exit networks reduce inference cost by allowing ``easy'' inputs to stop early, but practical deployment hinges on knowing \emph{when} early exit is safe. We introduce SAFE-KD, a universal multi-exit wrapper for modern vision backbones that couples hierarchical distillation with \emph{conformal risk control}. SAFE-KD attaches lightweight exit heads at intermediate depths, distills a strong teacher into all exits via Decoupled Knowledge Distillation (DKD), and enforces deep-to-shallow consistency between exits. At inference, we calibrate per-exit stopping thresholds on a held-out set using conformal risk control (CRC) to guarantee a user-specified \emph{selective} misclassification risk (among the samples that exit early) under exchangeability. Across multiple datasets and architectures, SAFE-KD yields improved accuracy compute trade-offs, stronger calibration, and robust performance under corruption while providing finite-sample risk guarantees.

</details>


### [222] [CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs](https://arxiv.org/abs/2602.03048)
*Zhiyuan Yao,Yi-Kai Zhang,Yuxin Chen,Yueqing Sun,Zishan Xu,Yu Yang,Tianhao Hu,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出 CoBA-RL，通过能力导向的价值函数和堆排序贪心策略，自适应分配 rollout 预算，以提高 LLM 训练后期的效率与泛化


<details>
  <summary>Details</summary>
Motivation: 现有方法如 GRPO 使用均匀预算，资源利用率低；基于任务级别的度量无法捕捉模型学习能力的动态变化，因此需要动态分配计算资源

Method: 建立 Capability-Oriented Value function，将任务映射到潜在训练增益；采用基于堆的贪心策略自适应地分配预算，重点关注高训练价值的样本；在多项基准上进行实验比较

Result: 在探索-开发权衡方面表现良好；在多个挑战性基准上实现一致的泛化改进；显著提升后训练效率

Conclusion: 量化样本训练价值并优化预算分配是提升 LLM 后训练效率的关键；可扩展性强，适用于不同任务和数据规模

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.

</details>


### [223] [Fedcompass: Federated Clustered and Periodic Aggregation Framework for Hybrid Classical-Quantum Models](https://arxiv.org/abs/2602.03052)
*Yueheng Wang,Xing He,Zinuo Cai,Rui Zhang,Ruhui Ma,Yuan Liu,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 提出 FEDCOMPASS，通过分层聚合实现混合经典-量子联邦学习的稳定性和准确性提升。对客户端按类别分布相似性进行光谱聚类以进行经典特征提取器的簇内聚合；对量子参数采用圆平均聚合并结合自适应优化。实验在三个基准数据集上，测试准确率提升可达 10.22%，在非独立同分布设置下收敛更稳定，优于六个强基线。


<details>
  <summary>Details</summary>
Motivation: 在隐私约束下的分布式模型训练中，非独立同分布（Non-IID）数据会显著降低性能。混合经典-量子联邦学习在减轻计算与通信负担方面具有潜力，但易受非IID数据影响而退化。本研究旨在通过分层聚合策略提升非IID环境下的鲁棒性与性能。

Method: 提出分层聚合框架 FEDCOMPASS。对于经典参数，基于光谱聚类将客户端按类别分布相似性分组，并在簇内对经典特征提取器进行聚合。对于量子参数，采用圆平均聚合并结合自适应优化以实现稳定的全局更新。实验中通过三组基准数据集验证方法效果。

Result: 实验结果表明，FEDCOMPASS 在三组数据集上提升测试准确率，最高可达 10.22%，并在非IID设置下提高收敛稳定性，显著优于六个强基线。

Conclusion: FEDCOMPASS 为混合经典-量子联邦学习提供一种有效的分层聚合机制，能改善非IID环境下的鲁棒性与收敛性及准确性。然而，仍需进一步评估簇数、相似性度量、圆平均聚合对量子参数尺度变化的敏感性，以及在大规模客户端和实际量子硬件上的可扩展性与实现成本。

Abstract: Federated learning enables collaborative model training across decentralized clients under privacy constraints. Quantum computing offers potential for alleviating computational and communication burdens in federated learning, yet hybrid classical-quantum federated learning remains susceptible to performance degradation under non-IID data. To address this,we propose FEDCOMPASS, a layered aggregation framework for hybrid classical-quantum federated learning. FEDCOMPASS employs spectral clustering to group clients by class distribution similarity and performs cluster-wise aggregation for classical feature extractors. For quantum parameters, it uses circular mean aggregation combined with adaptive optimization to ensure stable global updates. Experiments on three benchmark datasets show that FEDCOMPASS improves test accuracy by up to 10.22% and enhances convergence stability under non-IID settings, outperforming six strong federated learning baselines.

</details>


### [224] [FlashSinkhorn: IO-Aware Entropic Optimal Transport](https://arxiv.org/abs/2602.03067)
*Felix X. -F. Ye,Xingjie Li,An Yu,Ming-Ching Chang,Linsong Chu,Davis Wertheimer*

Main category: cs.LG

TL;DR: FlashSinkhorn: IO感知的EOT求解器，使用行级LogSumExp对偏置点积分数进行降维以实现注意力风格归一化，结合Fusion和流式Triton内核，显著减少HBM IO并维持线性内存，带来显著加速与端到端可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决基于Sinkhorn的熵最优传输在GPU上的实现瓶颈，现有实现要么执行密集的n×m交互导致IO成本高，要么走在线但缺乏有效的内核融合。需要一种IO感知、可扩展且具备内核融合的求解器。

Method: 将稳定化的对数域Sinkhorn更新重写为行级LogSumExp对偏置点积分数的求和，与注意力归一化一致；实现FlashAttention风格的融合与分块，采用Triton内核在片上SRAM中逐块流式更新对偶势，仅一次遍历完成；提供面向传输的流式内核，支持一阶和二阶优化。

Result: 在NVIDIA A100上，与最先进的在线基线相比，点云OT的前向阶段可达到最高32×加速，端到端可达161×加速；提升OT基础任务的可扩展性并对下游任务产生收益；提供开源实现以便复现。

Conclusion: IO感知的融合流式Sinkhorn求解器可显著降低HBM IO、保留线性内存复杂度，适用于大规模熵最优传输问题，并具备良好可扩展性。

Abstract: Entropic optimal transport (EOT) via Sinkhorn iterations is widely used in modern machine learning, yet GPU solvers remain inefficient at scale. Tensorized implementations suffer quadratic HBM traffic from dense $n\times m$ interactions, while existing online backends avoid storing dense matrices but still rely on generic tiled map-reduce reduction kernels with limited fusion. We present \textbf{FlashSinkhorn}, an IO-aware EOT solver for squared Euclidean cost that rewrites stabilized log-domain Sinkhorn updates as row-wise LogSumExp reductions of biased dot-product scores, the same normalization as transformer attention. This enables FlashAttention-style fusion and tiling: fused Triton kernels stream tiles through on-chip SRAM and update dual potentials in a single pass, substantially reducing HBM IO per iteration while retaining linear-memory operations. We further provide streaming kernels for transport application, enabling scalable first- and second-order optimization. On A100 GPUs, FlashSinkhorn achieves up to $32\times$ forward-pass and $161\times$ end-to-end speedups over state-of-the-art online baselines on point-cloud OT, improves scalability on OT-based downstream tasks. For reproducibility, we release an open-source implementation at https://github.com/ot-triton-lab/ot_triton.

</details>


### [225] [Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning](https://arxiv.org/abs/2602.03086)
*Jiayao Mai,Bangyan Liao,Zhenjun Zhao,Yingping Zeng,Haoang Li,Javier Civera,Tailin Wu,Yi Zhou,Peidong Liu*

Main category: cs.LG

TL;DR: 提出 Neural Predictor-Corrector (NPC) 的统一神经求解器，用强化学习学习预测-修正步骤，适用于同伦法框架下的鲁棒优化/全局优化/多项式根等问题，具备 amortized 训练以实现对新实例的泛化，实验显示在效率与稳定性方面优于经典基线。


<details>
  <summary>Details</summary>
Motivation: 传统的 predictor-corrector (PC) 方法高度依赖人工手工调参与启发式步长/终止条件，且对具体任务的适配性差。将同伦法问题统一到单一框架，有望提升求解效率、鲁棒性与泛化能力。

Method: 提出 NPC，通过强化学习来学习选择预测步与修正步的策略，将策略设定为序列决策问题，并引入 amortized 训练，实现对同伦法类问题的一次离线训练、对新实例的高效在线推理。通过将问题统一为一个神经求解器框架，实验在四个代表性同伦问题上验证其泛化性、效率与稳定性。

Result: 在看不见的实例上表现出良好的泛化能力，持续优于经典与专门化基线，在效率和稳定性方面具有优势。

Conclusion: 将同伦法方法统一到一个神经框架，并通过 RL 学习策略，结合 amortized 训练，能够得到更高效且更稳定的求解器，且具备对新实例的快速泛化能力。

Abstract: The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.

</details>


### [226] [TextME: Bridging Unseen Modalities Through Text Descriptions](https://arxiv.org/abs/2602.03098)
*Soyeon Hong,Jinchan Kim,Jaegook You,Seungtaek Choi,Suha Kwak,Hyunsouk Cho*

Main category: cs.LG

TL;DR: TextME 提出首个纯文本模态扩展框架，将多模态投射到统一的LLM嵌入空间，依赖对比编码器的几何结构实现无对齐监督的零-shot跨模态传输，覆盖图像、视频、音频、3D、X光和分子等领域，显示仅文本描述即可保持预训练编码器的大部分性能并实现未显式对齐的跨模态检索，降低对成对数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 扩展新模态表示通常需要大规模成对数据，成本高且在需专家标注的领域（如医学影像、分子分析）困难。存在无对齐监督、低数据需求的模态扩展需求。

Method: 将不同模态投射到统一的文本-LLM嵌入空间作为锚点；基于预训练对比编码器的几何结构，在没有成对监督的情况下，通过文本描述实现零-shot跨模态 transfer；验证跨模态距离的一致性并展示文本仅训练下的鲁棒性和跨模态检索能力。

Result: 在图像、视频、音频、3D、X射线与分子等领域发现模态间距离具有一致性，文本仅训练能较好维持预训练编码器的性能；还出现未在训练阶段显式对齐的跨模态检索（如音频对图像、3D对图像）的现象，表明文本-嵌入空间可作为通用锚点实现模态扩展的潜力。

Conclusion: 文本仅训练为模态扩展提供了实际可行的替代方案，显著降低对成对数据的依赖，具备跨模态检索和应用潜力；但在不同模态的语义-感知差异、领域适应性等方面仍需进一步评估和增强。

Abstract: Expanding multimodal representations to novel modalities is constrained by reliance on large-scale paired datasets (e.g., text-image, text-audio, text-3D, text-molecule), which are costly and often infeasible in domains requiring expert annotation such as medical imaging and molecular analysis. We introduce TextME, the first text-only modality expansion framework, to the best of our knowledge, projecting diverse modalities into LLM embedding space as a unified anchor. Our approach exploits the geometric structure of pretrained contrastive encoders to enable zero-shot cross-modal transfer using only text descriptions, without paired supervision. We empirically validate that such consistent modality gaps exist across image, video, audio, 3D, X-ray, and molecular domains, demonstrating that text-only training can preserve substantial performance of pretrained encoders. We further show that our framework enables emergent cross-modal retrieval between modality pairs not explicitly aligned during training (e.g., audio-to-image, 3D-to-image). These results establish text-only training as a practical alternative to paired supervision for modality expansion.

</details>


### [227] [Consensus Group Relative Policy Optimization for Text Generation](https://arxiv.org/abs/2602.03102)
*Yuki Ichihara,Yuu Jinnai,Kaito Ariu,Eiji Uchibe*

Main category: cs.LG

TL;DR: C-GRPO 将最小化贝叶斯风险（MBR）解码的目标转化为训练中的组相对目标，通过 GRPO 的框架实现共识效用的学习，从而在不需要黄金参考或明确偏好数据的情况下，训练出可在推理阶段高效近似 MBR 的策略。


<details>
  <summary>Details</summary>
Motivation: 降低文本生成中样本-重排序解码的推理成本与数据标注成本，避免对黄金参考、教师信号或偏好数据的强依赖。

Method: 将共识效用函数形式化为组相对目标，嵌入 GRPO 训练框架；仅需要一个效用函数和一组策略样本即可训练；在理想条件下，该目标的梯度方向与 MBR 的期望效用梯度对齐，具备收敛性保证；理论分析与推理过程见论文。

Result: 在机器翻译（WMT 2024）与文本摘要（XSum）任务上，C-GRPO 的性能与 MBR 解码相当，同时显著降低推理时的开销，并优于无参考基线。

Conclusion: C-GRPO 提供一种高效的数据驱动训练方法，用以近似 MBR 解码，减少对黄金参考和显式偏好数据的需求，且在给定条件下具备方向性收敛性。

Abstract: Many strong decoding methods for text generation follow a sample-and-rerank paradigm: they draw multiple candidates, score each under a utility (reward) function using consensus across samples, and return the best one. Although effective, these methods incur high computational costs during inference due to repeated sampling and scoring. Prior attempts to amortize inference-time computation typically rely on gold references, teacher labels, or curated preference data, increasing dataset construction effort and the demand for high-fidelity reward models. We propose Consensus Group Relative Policy Optimization (C-GRPO), which distills Minimum Bayes Risk (MBR) decoding into training by formulating the consensus utility as a group-relative objective within GRPO. C-GRPO requires only a utility function and policy samples, without gold references or explicit preference labels. Under ideal conditions, we show that the objective function of C-GRPO is directionally aligned with the gradient of the expected-utility objective underlying MBR decoding, leading to a convergence guarantee. Experiments on machine translation (WMT 2024) and text summarization (XSum) demonstrate that C-GRPO successfully achieves performance comparable to MBR decoding without the associated inference-time overhead, while outperforming reference-free baseline methods.

</details>


### [228] [Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost](https://arxiv.org/abs/2602.03120)
*Yinggan Xu,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

TL;DR: 提出 Quantized Evolution Strategies (QES)，在量化空间直接对大模型进行全参数微调，结合累积误差反馈与无状态种子回放以降低内存并保留梯度信号，显著优于当前 zeroth-order 微调在算术推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: PTQ 将模型压缩以适应内存受限设备，但使模型静态化且难以微调。传统微调（包括 RL）依赖反向传播和高精度权重来计算梯度，在量化模型上不可用。尽管 ES 作为无梯度方法可用，但对量化参数的优化仍可能因梯度消失或不准确而失败。因此需要一种在量化参数空间直接优化且能保持有效梯度信号的方法。

Method: 提出 QES，核心在于两点创新：(1) 将累积误差反馈整合入进化策略，以在离散、低精度参数空间中保留近似的高精度梯度信号；(2) 引入无状态种子回放（stateless seed replay）来显著降低内存需求，使优化代价接近低精度推理水平。该方法实现直接在量化参数上进行全参数微调。

Result: 在算术推理等任务上，QES 明显优于当前最先进的 zeroth-order 微调方法，证实了在量化模型上直接微调的可行性与有效性。

Conclusion: QES 将开启在量化空间直接对大语言模型进行微调的可能性，为在极低内存条件下扩展和部署大模型铺平道路，同时与现有量化与微调框架具备良好兼容性。

Abstract: Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .

</details>


### [229] [Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery](https://arxiv.org/abs/2602.03132)
*Timothee Leleu,Sudeera Gunathilaka,Federico Ghimenti,Surya Ganguli*

Main category: cs.LG

TL;DR: CCTS提出一种对比学习驱动的层级概念树搜索，用以利用LLM生成程序中的概念层次，重加权父解以偏向高质量概念组合，从而提升LLM辅助算法发现中的搜索效率，并给出可解释的概念树。


<details>
  <summary>Details</summary>
Motivation: 当前尚不清楚如何最大化利用LLM对可能程序空间的内部表示来提升性能。

Method: 从生成的候选程序中提取分层概念表示，学习对比概念模型以指导父解的选择；通过对高低表现解的似然比分数重新加权父解，强调有用概念的组合并避免误导性概念，提供显式的概念层次结构而非LLM给出的算法谱系。

Result: 相较于只以适应度为基线的搜索，CCTS提高了搜索效率；产生可解释、任务特定的概念树，且在开放的Erdős型组合问题上得到验证。分析显示收益主要来自学习避免哪些概念；在受控的合成环境中重复了与LLM相似的搜索动态。

Conclusion: 通过显式的概念层次结构引导搜索，CCTS能有效利用LLM内在表示来提升算法发现的效率与可解释性，且对概念的“避免”学习是关键驱动因素。

Abstract: Large language Model (LLM)-assisted algorithm discovery is an iterative, black-box optimization process over programs to approximatively solve a target task, where an LLM proposes candidate programs and an external evaluator provides task feedback. Despite intense recent research on the topic and promising results, how can the LLM internal representation of the space of possible programs be maximally exploited to improve performance is an open question. Here, we introduce Contrastive Concept-Tree Search (CCTS), which extracts a hierarchical concept representation from the generated programs and learns a contrastive concept model that guides parent selection. By reweighting parents using a likelihood-ratio score between high- and low-performing solutions, CCTS biases search toward useful concept combinations and away from misleading ones, providing guidance through an explicit concept hierarchy rather than the algorithm lineage constructed by the LLM. We show that CCTS improves search efficiency over fitness-based baselines and produces interpretable, task-specific concept trees across a benchmark of open Erdős-type combinatorics problems. Our analysis indicates that the gains are driven largely by learning which concepts to avoid. We further validate these findings in a controlled synthetic algorithm-discovery environment, which reproduces qualitatively the search dynamics observed with the LLMs.

</details>


### [230] [SATORIS-N: Spectral Analysis based Traffic Observation Recovery via Informed Subspaces and Nuclear-norm minimization](https://arxiv.org/abs/2602.03138)
*Sampad Mohanty,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: 提出 SATORIS-N：基于子空间先验的 SDP 正则化用于缺失交通密度矩阵的协同填充，在中高遮挡下优于常规矩阵完成方法。


<details>
  <summary>Details</summary>
Motivation: 交通密度矩阵具有低秩特性和随时间稳定的奇异子空间，缺失数据时需要利用邻日信息实现鲁棒重建。

Method: 提出子空间感知的 SDP 形式的核范数，显式引入先验奇异子空间信息；并评估一个轻量隐式子空间对齐策略，通过连接连续日矩阵实现子空间对齐；比较与 SoftImpute、IterativeSVD、统计方法和深度学习基线的性能。

Result: 在北京和上海数据集上，SATORIS-N 在中高遮挡情况下一致优于基线方法，且显式 SDP 在大规模缺失时更鲁棒。

Conclusion: 框架可推广到其他时空场景，且在智能交通、V2X 等应用中对协同感知与路由优化具有重要意义。

Abstract: Traffic-density matrices from different days exhibit both low rank and stable correlations in their singular-vector subspaces. Leveraging this, we introduce SATORIS-N, a framework for imputing partially observed traffic-density by informed subspace priors from neighboring days. Our contribution is a subspace-aware semidefinite programming (SDP)} formulation of nuclear norm that explicitly informs the reconstruction with prior singular-subspace information. This convex formulation jointly enforces low rank and subspace alignment, providing a single global optimum and substantially improving accuracy under medium and high occlusion. We also study a lightweight implicit subspace-alignment} strategy in which matrices from consecutive days are concatenated to encourage alignment of spatial or temporal singular directions. Although this heuristic offers modest gains when missing rates are low, the explicit SDP approach is markedly more robust when large fractions of entries are missing. Across two real-world datasets (Beijing and Shanghai), SATORIS-N consistently outperforms standard matrix-completion methods such as SoftImpute, IterativeSVD, statistical, and even deep learning baselines at high occlusion levels. The framework generalizes to other spatiotemporal settings in which singular subspaces evolve slowly over time. In the context of intelligent vehicles and vehicle-to-everything (V2X) systems, accurate traffic-density
  reconstruction enables critical applications including cooperative perception, predictive routing, and vehicle-to-infrastructure (V2I) communication optimization. When infrastructure sensors or vehicle-reported observations are incomplete - due to communication dropouts, sensor occlusions, or sparse connected vehicle penetration-reliable imputation becomes essential for safe and efficient autonomous navigation.

</details>


### [231] [What Makes a Good Example? Modeling Exemplar Selection with Neural Network Representations](https://arxiv.org/abs/2602.03144)
*Fanxiao Wani Qiu,Oscar Leong,Alexander LaTourrette*

Main category: cs.LG

TL;DR: Humans teach by selecting exemplars that balance joint representativeness and diversity; this balance best matches human judgments, with transformer-based features aligning more closely to humans than CNNs; has implications for dataset distillation and ML teaching.


<details>
  <summary>Details</summary>
Motivation: Clarify the computational principles guiding exemplar selection in teaching and how different strategies (representativeness, diversity, and their combination) influence human judgments.

Method: Embed new visual categories along a one-dimensional morph continuum using pretrained vision models; compare teaching strategies emphasizing prototypicality, representativeness, and diversity; have adults select 1–3 exemplars to teach a learner; compare model-predicted selections to human choices; evaluate representations from transformers vs CNNs.

Result: Strategies based on joint representativeness, or its combination with diversity, best captured human judgments; purely prototypical or diversity-only strategies performed worse; transformer representations aligned more closely with human behavior than CNNs.

Conclusion: Modeling exemplar selection as a subset selection problem supports dataset distillation as a computational model of teaching; transformer-based representations may offer superior substrates for exemplar selection in ML teaching contexts.

Abstract: Teaching requires distilling a rich category distribution into a small set of informative exemplars. Although prior work shows that humans consider both representativeness and diversity when teaching, the computational principles underlying these tradeoffs remain unclear. We address this gap by modeling human exemplar selection using neural network feature representations and principled subset selection criteria. Novel visual categories were embedded along a one-dimensional morph continuum using pretrained vision models, and selection strategies varied in their emphasis on prototypicality, joint representativeness, and diversity. Adult participants selected one to three exemplars to teach a learner. Model-human comparisons revealed that strategies based on joint representativeness, or its combination with diversity, best captured human judgments, whereas purely prototypical or diversity-based strategies performed worse. Moreover, transformer-based representations consistently aligned more closely with human behavior than convolutional networks. These results highlight the potential utility of dataset distillation methods in machine learning as computational models for teaching.

</details>


### [232] [MemCast: Memory-Driven Time Series Forecasting with Experience-Conditioned Reasoning](https://arxiv.org/abs/2602.03164)
*Xiaoyu Tao,Mingyue Cheng,Ze Guo,Shuo Yu,Yaguo Liu,Qi Liu,Shijin Wang*

Main category: cs.LG

TL;DR: MemCast将时序预测视为经验驱动的推理任务，通过学习训练集经验并组织成分层记忆，在推理阶段利用历史模式和推理智慧指导路径选择，并以通用法则进行反思迭代，同时引入动态置信度自适应以实现连续进化；在多个数据集上优于现有方法，且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有时序预测往往缺乏显式的经验积累和持续演化能力；基于大语言模型的预测器虽具性能，但难以从经验中积累并持续更新。

Method: 从训练集学习经验，形成分层记忆：将预测结果摘要为历史模式、将推理轨迹提炼为推理智慧、将时序特征归纳为一般规律；推理阶段用历史模式引导推理、以推理智慧选择更优轨迹、以一般规律进行反思性迭代；提出动态置信度自适应策略，在不泄露测试分布的前提下更新各条目置信度；在多个数据集上进行广泛实验。

Result: 实验结果显示MemCast在多数据集上持续优于现有方法，验证了方法有效性；代码公开。

Conclusion: MemCast通过显式记忆与经验驱动的推理框架实现TSF的持续进化和更优性能，具有潜在的扩展性与解释性，但需关注记忆规模、检索质量及对新分布的鲁棒性等。

Abstract: Time series forecasting (TSF) plays a critical role in decision-making for many real-world applications. Recently, LLM-based forecasters have made promising advancements. Despite their effectiveness, existing methods often lack explicit experience accumulation and continual evolution. In this work, we propose MemCast, a learning-to-memory framework that reformulates TSF as an experience-conditioned reasoning task. Specifically, we learn experience from the training set and organize it into a hierarchical memory. This is achieved by summarizing prediction results into historical patterns, distilling inference trajectories into reasoning wisdom, and inducing extracted temporal features into general laws. Furthermore, during inference, we leverage historical patterns to guide the reasoning process and utilize reasoning wisdom to select better trajectories, while general laws serve as criteria for reflective iteration. Additionally, to enable continual evolution, we design a dynamic confidence adaptation strategy that updates the confidence of individual entries without leaking the test set distribution. Extensive experiments on multiple datasets demonstrate that MemCast consistently outperforms previous methods, validating the effectiveness of our approach. Our code is available at https://github.com/Xiaoyu-Tao/MemCast-TS.

</details>


### [233] [StepScorer: Accelerating Reinforcement Learning with Step-wise Scoring and Psychological Regret Modeling](https://arxiv.org/abs/2602.03171)
*Zhe Xu*

Main category: cs.LG

TL;DR: PRM提出步级悔意信号，通过与最优行动的差值来密化奖励，从而加速强化学习的收敛。实验显示在 Lunar Lander 上比 PPO 快约36%，对连续控制和延迟反馈环境尤为有效，具有现实世界应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏奖励导致的慢收敛；将人类逆向思维/反事实思维形式化为可计算的悔意信号，以提供密集的反馈。

Method: 在每一状态计算悔意信号，等于最优行动的价值V*(s)与采取动作的价值V(s,a)之差；将该步悔意整合入学习过程，形成密集的步级信号，可能与PPO等算法结合。

Result: 表现稳定，训练速度提升约36%；在 Lunar Lander 等基准环境中优于传统方法；在连续控制和延迟反馈任务表现出色。

Conclusion: 将人类纠错中的对照性思维（counterfactual thinking）形式化为可计算的悔意信号，推动将行为经济学思想融入 RL，适合机器人、金融、教育等需要快速策略适应的场景。

Abstract: Reinforcement learning algorithms often suffer from slow convergence due to sparse reward signals, particularly in complex environments where feedback is delayed or infrequent. This paper introduces the Psychological Regret Model (PRM), a novel approach that accelerates learning by incorporating regret-based feedback signals after each decision step. Rather than waiting for terminal rewards, PRM computes a regret signal based on the difference between the expected value of the optimal action and the value of the action taken in each state. This transforms sparse rewards into dense feedback signals through a step-wise scoring framework, enabling faster convergence. We demonstrate that PRM achieves stable performance approximately 36\% faster than traditional Proximal Policy Optimization (PPO) in benchmark environments such as Lunar Lander. Our results indicate that PRM is particularly effective in continuous control tasks and environments with delayed feedback, making it suitable for real-world applications such as robotics, finance, and adaptive education where rapid policy adaptation is critical. The approach formalizes human-inspired counterfactual thinking as a computable regret signal, bridging behavioral economics and reinforcement learning.

</details>


### [234] [Probe-then-Commit Multi-Objective Bandits: Theoretical Benefits of Limited Multi-Arm Feedback](https://arxiv.org/abs/2602.03175)
*Ming Shi*

Main category: cs.LG

TL;DR: 提出 PtC-P-UCB：在探测-再投入框架下的多目标在线资源选择，通过前沿感知探测和边际超体积增益实现Pareto前沿的高效扩张，给出前沿误差和标量化 regret 的渐近界，以及多模态探测的方差自适应扩展。


<details>
  <summary>Details</summary>
Motivation: 在K个 arms、d维随机向量表现下，允许最多q次探测后再提交一个执行；介于经典Bandits（q=1）和全信息（q=K）之间，需要在多目标 Pareto 优化中设计探测策略。

Method: 提出 PtC-P-UCB：以近似最大化超体积启发的前沿覆盖潜在值来选择q个探测对象，提交通过边际超体积增量扩展已获得的Pareto前沿；扩展到多模态探测时进行不确定性融合。

Result: 理论结果包括 dominated-hypervolume frontier error 约 Õ(K_P d / sqrt(qT))，以及标量化 regret 约 Õ(L_φ d sqrt((K/q) T))，指示探测数量q提升带来1/√q 的加速；并给出多模态探测的方差自适应版本。

Conclusion: 该方法把探测-再投入问题从极端的Bandits与Expert之间的空缺中填补，提供PAR-前沿探索的理论保障，并可扩展到多模态观测与自适应噪声水平。

Abstract: We study an online resource-selection problem motivated by multi-radio access selection and mobile edge computing offloading. In each round, an agent chooses among $K$ candidate links/servers (arms) whose performance is a stochastic $d$-dimensional vector (e.g., throughput, latency, energy, reliability). The key interaction is \emph{probe-then-commit (PtC)}: the agent may probe up to $q>1$ candidates via control-plane measurements to observe their vector outcomes, but must execute exactly one candidate in the data plane. This limited multi-arm feedback regime strictly interpolates between classical bandits ($q=1$) and full-information experts ($q=K$), yet existing multi-objective learning theory largely focuses on these extremes. We develop \textsc{PtC-P-UCB}, an optimistic probe-then-commit algorithm whose technical core is frontier-aware probing under uncertainty in a Pareto mode, e.g., it selects the $q$ probes by approximately maximizing a hypervolume-inspired frontier-coverage potential and commits by marginal hypervolume gain to directly expand the attained Pareto region. We prove a dominated-hypervolume frontier error of $\tilde{O} (K_P d/\sqrt{qT})$, where $K_P$ is the Pareto-frontier size and $T$ is the horizon, and scalarized regret $\tilde{O} (L_φd\sqrt{(K/q)T})$, where $φ$ is the scalarizer. These quantify a transparent $1/\sqrt{q}$ acceleration from limited probing. We further extend to \emph{multi-modal probing}: each probe returns $M$ modalities (e.g., CSI, queue, compute telemetry), and uncertainty fusion yields variance-adaptive versions of the above bounds via an effective noise scale.

</details>


### [235] [Reinforcement Learning with Promising Tokens for Large Language Models](https://arxiv.org/abs/2602.03195)
*Jing-Cheng Pang,Liang Lu,Xian Tang,Kun Jiang,Sijie Wu,Kai Zhang,Xubin Li*

Main category: cs.LG

TL;DR: RLPT通过对高维词表的筛选，将策略决策与单词生成分离，显著降低行动空间并提升样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 在RL-LLM中，全词表动作空间过大，导致梯度方差高、训练不稳定。假设推理路径可隐含在低秩子空间中，提供对有效行动的先验。希望通过先验筛选的“有前景的词语”来聚焦策略优化。

Method: 提出RLPT框架，利用基模型的语义先验动态识别一组有前景的token，并通过掩码限制策略优化仅在该子集内进行，从而将战略决策与逐字生成解耦。

Result: 理论分析和实证表明，RLPT降低梯度方差、稳定训练、提高样本效率。在数学、编码、通信推理任务上，RLPT优于标准RL基线，且在4B/8B模型和GRPO/DAPO等算法上均能有效集成。

Conclusion: RLPT有效缓解LLM RL中的行动空间问题，具有良好泛化性，可在不同模型规模与算法下提升性能与训练效率。

Abstract: Reinforcement learning (RL) has emerged as a key paradigm for aligning and optimizing large language models (LLMs). Standard approaches treat the LLM as the policy and apply RL directly over the full vocabulary space. However, this formulation includes the massive tail of contextually irrelevant tokens in the action space, which could distract the policy from focusing on decision-making among the truly reasonable tokens. In this work, we verify that valid reasoning paths could inherently concentrate within a low-rank subspace. Based on this insight, we introduce Reinforcement Learning with Promising Tokens (RLPT), a framework that mitigates the action space issue by decoupling strategic decision-making from token generation. Specifically, RLPT leverages the semantic priors of the base model to identify a dynamic set of \emph{promising tokens} and constrains policy optimization exclusively to this refined subset via masking. Theoretical analysis and empirical results demonstrate that RLPT effectively reduces gradient variance, stabilizes the training process, and improves sample efficiency. Experiment results on math, coding, and telecom reasoning show that RLPT outperforms standard RL baselines and integrates effectively across various model sizes (4B and 8B) and RL algorithms (GRPO and DAPO).

</details>


### [236] [From Scalar Rewards to Potential Trends: Shaping Potential Landscapes for Model-Based Reinforcement Learning](https://arxiv.org/abs/2602.03201)
*Yao-Hui Li,Zeyu Wang,Xin Li,Wei Pang,Yingfang Yuan,Zhengkun Chen,Boya Zhang,Riashat Islam,Alex Lamb,Yonggang Zhang*

Main category: cs.LG

TL;DR: 用乐观潜在估计的潜能地形来替代标量奖励回归的鲁棒强化学习框架，提升在稀疏奖励下的样本效率和探索性。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励导致基于模型的强化学习在拟合标量奖励时梯度近乎为零，难以为规划提供方向性信号，因此急需更有信息量的奖励信号来引导学习。

Method: 提出 SLOPE 框架，通过将奖励建模从预测标量奖励转为构建信息丰富的潜在（能量/势）地形。采用乐观分布回归来估计高置信上界，放大罕见的成功信号并确保充足的探索梯度。

Result: 在 30+ 任务、5 个基准上进行评估，SLOPE 在完全稀疏、半稀疏和密集奖励设置中始终优于主流基线。

Conclusion: SLOPE 通过用乐观潜在估计塑形奖励景观，提升了模型基强化学习在稀疏奖励情境下的样本效率与探索性，且对不同奖励密度具备较强鲁棒性。

Abstract: Model-based reinforcement learning (MBRL) achieves high sample efficiency by simulating future trajectories with learned dynamics and reward models. However, its effectiveness is severely compromised in sparse reward settings. The core limitation lies in the standard paradigm of regressing ground-truth scalar rewards: in sparse environments, this yields a flat, gradient-free landscape that fails to provide directional guidance for planning. To address this challenge, we propose Shaping Landscapes with Optimistic Potential Estimates (SLOPE), a novel framework that shifts reward modeling from predicting scalars to constructing informative potential landscapes. SLOPE employs optimistic distributional regression to estimate high-confidence upper bounds, which amplifies rare success signals and ensures sufficient exploration gradients. Evaluations on 30+ tasks across 5 benchmarks demonstrate that SLOPE consistently outperforms leading baselines in fully sparse, semi-sparse, and dense rewards.

</details>


### [237] [Spectral Evolution Search: Efficient Inference-Time Scaling for Reward-Aligned Image Generation](https://arxiv.org/abs/2602.03208)
*Jinyan Ye,Zhongjie Duan,Zhiwen Li,Cen Chen,Daoyuan Chen,Yaliang Li,Yingda Chen*

Main category: cs.LG

TL;DR: 通过在低频子空间进行梯度无关的进化搜索，对初始噪声进行推断时的空间-频率分布优化，提高视觉生成模型在无参数更新下的目标对齐效率与质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高维初始噪声上进行优化时效率低下，因为大多数搜索方向对最终生成影响甚微；这与生成动态中的谱偏差有关，即对高频扰动的敏感性随频率上升而迅速降低。

Method: 提出 Spectral Evolution Search (SES)，在低频子空间内执行梯度无关的进化搜索；从扰动传播出发推导出谱缩放预测（Spectral Scaling Prediction），解释不同频率扰动影响的系统性差异。SES 可作为对推断阶段的即插即用噪声优化框架。

Result: 通过广泛实验，SES 在生成质量与计算成本的帕累托前沿上显著提升，在等预算条件下稳定优于强基线。理论分析与消融实验支持谱偏差与频率相关性对结果的影响。

Conclusion: SES 将谱偏差原理应用于推断时的初始噪声优化，提供一个高效、可插拔的解决方案以提升视觉生成模型的目标对齐能力。

Abstract: Inference-time scaling offers a versatile paradigm for aligning visual generative models with downstream objectives without parameter updates. However, existing approaches that optimize the high-dimensional initial noise suffer from severe inefficiency, as many search directions exert negligible influence on the final generation. We show that this inefficiency is closely related to a spectral bias in generative dynamics: model sensitivity to initial perturbations diminishes rapidly as frequency increases. Building on this insight, we propose Spectral Evolution Search (SES), a plug-and-play framework for initial noise optimization that executes gradient-free evolutionary search within a low-frequency subspace. Theoretically, we derive the Spectral Scaling Prediction from perturbation propagation dynamics, which explains the systematic differences in the impact of perturbations across frequencies. Extensive experiments demonstrate that SES significantly advances the Pareto frontier of generation quality versus computational cost, consistently outperforming strong baselines under equivalent budgets.

</details>


### [238] [Lookahead Sample Reward Guidance for Test-Time Scaling of Diffusion Models](https://arxiv.org/abs/2602.03211)
*Yeongmin Kim,Donghyeok Shin,Byeonghu Na,Minsang Park,Richard Lee Kim,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出了一种 LiDAR 采样框架，通过边际样本实现无神经反向传播的 EFR 引导，并结合前瞻性采样实现高效的测试时采样，在较少样本下达到与最新梯度引导方法同等的 GenEval 性能，同时实现约 9.5x 速度提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成方面表现优异，但生成样本往往与人类意图不完全对齐；需要在测试时对采样过程进行缩放，以更高概率进入人类对齐的高奖励区域。

Method: 将对任意时间步 x_t 的期望未来奖励（EFR）通过预训练扩散模型的边际样本来计算，消除了 x_t 与 EFR 之间的神经依赖，实现无需神经反向传播的闭式引导；引入 lookahead（前瞻性）采样以收集边际样本；最终采样使用一个精确求解器将粒子引导向高奖励的前瞻样本，形成 LiDAR 采样。

Result: 在仅使用 3 个样本与 3 步前瞻求解器的条件下，LiDAR 显著提升性能；随着前瞻精度和样本数量增加，性能提升显著；在 SDXL 上达到与最新梯度引导方法同等的 GenEval 性能，同时实现约 9.5 倍的加速。

Conclusion: LiDAR 提供一种高效且更符合人类奖励的采样方式，能在较少样本和低计算成本下达到竞争性结果，证明无神经反向传播的 EFR 引导结合前瞻采样的潜力。

Abstract: Diffusion models have demonstrated strong generative performance; however, generated samples often fail to fully align with human intent. This paper studies a test-time scaling method that enables sampling from regions with higher human-aligned reward values. Existing gradient guidance methods approximate the expected future reward (EFR) at an intermediate particle $\mathbf{x}_t$ using a Taylor approximation, but this approximation at each time step incurs high computational cost due to sequential neural backpropagation. We show that the EFR at any $\mathbf{x}_t$ can be computed using only marginal samples from a pre-trained diffusion model. The proposed EFR formulation detaches the neural dependency between $\mathbf{x}_t$ and the EFR, enabling closed-form guidance computation without neural backpropagation. To further improve efficiency, we introduce lookahead sampling to collect marginal samples. For final sample generation, we use an accurate solver that guides particles toward high-reward lookahead samples. We refer to this sampling scheme as LiDAR sampling. LiDAR achieves substantial performance improvements using only three samples with a 3-step lookahead solver, exhibiting steep performance gains as lookahead accuracy and sample count increase; notably, it reaches the same GenEval performance as the latest gradient guidance method for SDXL with a 9.5x speedup.

</details>


### [239] [Topology Matters: A Cautionary Case Study of Graph SSL on Neuro-Inspired Benchmarks](https://arxiv.org/abs/2602.03217)
*May Kristine Jonson Carlon,Su Myat Noe,Haojiong Wang,Yasuo Kuniyoshi*

Main category: cs.LG

TL;DR: Invariance-based graph SSL fails on connectome-like data and is outperformed by topology-aware methods; a topology-aware SSL objective is needed to preserve modularity/motifs across scales.


<details>
  <summary>Details</summary>
Motivation: To understand how local brain interactions scale to global organization, and to evaluate whether generic invariance-based SSL can capture topological structure in connectomes, revealing potential misalignments between SSL objectives and neurobiological topology.

Method: A four-stage evaluation protocol on a controllable synthetic benchmark that mimics connectome topology, jointly learning node-, edge-, and graph-level embeddings within a hierarchical SSL framework; comparisons against topology-aware heuristics; ablation studies to diagnose objective misalignment.

Result: The invariance-based SSL model is fundamentally misaligned with topological properties of the benchmark and performs worse than classical topology-aware methods; SSL objectives that are invariant to perturbations tend to ignore community structure, revealing a pitfall in applying generic graph SSL to connectome-like data.

Conclusion: Cautionary case study highlighting the need for topology-aware SSL objectives in neuro-AI that explicitly reward preservation of structure (e.g., modularity or motifs) and calling for development of SSL objectives tailored to neuroimaging graphs.

Abstract: Understanding how local interactions give rise to global brain organization requires models that can represent information across multiple scales. We introduce a hierarchical self-supervised learning (SSL) framework that jointly learns node-, edge-, and graph-level embeddings, inspired by multimodal neuroimaging. We construct a controllable synthetic benchmark mimicking the topological properties of connectomes. Our four-stage evaluation protocol reveals a critical failure: the invariance-based SSL model is fundamentally misaligned with the benchmark's topological properties and is catastrophically outperformed by classical, topology-aware heuristics. Ablations confirm an objective mismatch: SSL objectives designed to be invariant to topological perturbations learn to ignore the very community structure that classical methods exploit. Our results expose a fundamental pitfall in applying generic graph SSL to connectome-like data. We present this framework as a cautionary case study, highlighting the need for new, topology-aware SSL objectives for neuro-AI research that explicitly reward the preservation of structure (e.g., modularity or motifs).

</details>


### [240] [Beyond Suffixes: Token Position in GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2602.03265)
*Hicham Eddoubi,Umar Faruk Abdullahi,Fadi Hassan*

Main category: cs.LG

TL;DR: 在Greedy Coordinate Gradient (GCG) 攻击下，对抗令牌在提示中的位置（前缀/后缀）及评估时令牌位置的变化显著影响攻击成功率，揭示当前安全评估的盲点，需将令牌位置纳入LLM鲁棒性评估。


<details>
  <summary>Details</summary>
Motivation: 提升对LLM安全对齐的鲁棒性评估，指出现有评估多聚焦于后缀触发，忽略令牌位置作为攻击维度的影响。

Method: 以Greedy Coordinate Gradient (GCG) 作为案例，研究通过优化攻击以生成前缀触发，并在评估阶段改变对抗令牌的位置，比较不同位置对攻击成功率的影响。

Result: 令牌位置对攻击成功率具有显著影响；前缀触发与评估阶段令牌位置的变化都可显著改变攻击效果，暴露当前评估的盲点。

Conclusion: 在LLM安全鲁棒性评估中应将对抗令牌的位置纳入测试范畴，设计位置敏感的评估方案；未来工作包括系统化覆盖不同位置的评估集和攻击策略。

Abstract: Large Language Models (LLMs) have seen widespread adoption across multiple domains, creating an urgent need for robust safety alignment mechanisms. However, robustness remains challenging due to jailbreak attacks that bypass alignment via adversarial prompts. In this work, we focus on the prevalent Greedy Coordinate Gradient (GCG) attack and identify a previously underexplored attack axis in jailbreak attacks typically framed as suffix-based: the placement of adversarial tokens within the prompt. Using GCG as a case study, we show that both optimizing attacks to generate prefixes instead of suffixes and varying adversarial token position during evaluation substantially influence attack success rates. Our findings highlight a critical blind spot in current safety evaluations and underline the need to account for the position of adversarial tokens in the adversarial robustness evaluation of LLMs.

</details>


### [241] [Unveiling Covert Toxicity in Multimodal Data via Toxicity Association Graphs: A Graph-Based Metric and Interpretable Detection Framework](https://arxiv.org/abs/2602.03268)
*Guanzong Wu,Zihao Zhu,Siwei Lyu,Baoyuan Wu*

Main category: cs.LG

TL;DR: 提出基于有害潜在含义的多模态毒性检测框架：Toxicity Association Graphs (TAGs) 建模跨模态语义关联，提出多模态毒性隐藏性量化指标 MTC；并构建 Covert Toxic Dataset 作为基准，实验显示在低/高覆盖性场景下优于现有方法，且结果具备可解释性与可审计性。


<details>
  <summary>Details</summary>
Motivation: 多模态数据中的有害含义往往隐藏于跨模态的语义关联中，单一模态难以检测到。需要一个可解释、可审计的框架来捕捉隐性毒性并提供量化度量。

Method: 提出 Toxicity Association Graphs (TAGs) 以系统建模无害实体与潜在毒性含义之间的语义关联；引入量化度量 Multimodal Toxicity Covertness (MTC) 来衡量毒性表达的隐蔽程度；将检测框架与 MTC 集成以实现高可解释性决策过程；构建 Covert Toxic Dataset 用于评估高隐蔽性跨模态实例。

Result: 实验显示所提出的方法在低覆盖性与高覆盖性毒性场景下均优于现有方法，提供清晰、可解释且可审计的检测结果。

Conclusion: 在可解释的多模态毒性检测方面取得进展，为未来的上下文感知与可审计的检测方法奠定基础，并通过 Covert Toxic Dataset 提供严格的基准。

Abstract: Detecting toxicity in multimodal data remains a significant challenge, as harmful meanings often lurk beneath seemingly benign individual modalities: only emerging when modalities are combined and semantic associations are activated. To address this, we propose a novel detection framework based on Toxicity Association Graphs (TAGs), which systematically model semantic associations between innocuous entities and latent toxic implications. Leveraging TAGs, we introduce the first quantifiable metric for hidden toxicity, the Multimodal Toxicity Covertness (MTC), which measures the degree of concealment in toxic multimodal expressions. By integrating our detection framework with the MTC metric, our approach enables precise identification of covert toxicity while preserving full interpretability of the decision-making process, significantly enhancing transparency in multimodal toxicity detection. To validate our method, we construct the Covert Toxic Dataset, the first benchmark specifically designed to capture high-covertness toxic multimodal instances. This dataset encodes nuanced cross-modal associations and serves as a rigorous testbed for evaluating both the proposed metric and detection framework. Extensive experiments demonstrate that our approach outperforms existing methods across both low- and high-covertness toxicity regimes, while delivering clear, interpretable, and auditable detection outcomes. Together, our contributions advance the state of the art in explainable multimodal toxicity detection and lay the foundation for future context-aware and interpretable approaches. Content Warning: This paper contains examples of toxic multimodal content that may be offensive or disturbing to some readers. Reader discretion is advised.

</details>


### [242] [BlockRR: A Unified Framework of RR-type Algorithms for Label Differential Privacy](https://arxiv.org/abs/2602.03277)
*Haixia Liu,Yi Ding*

Main category: cs.LG

TL;DR: BlockRR 将多种随机响应型标签差分隐私机制统一为一个框架，提供 ε-标签 DP，并通过基于先验的分区实现灵活性与并行组合性；实验显示在 ε≤3 的高/中隐私下性能优于其他 RR 的平衡表现，ε≥4 时等效于标准 RR且无额外损失。


<details>
  <summary>Details</summary>
Motivation: 现有的 RR-type 标签差分隐私机制通常需要针对不同场景选择具体的机制，缺乏统一的理论与实现框架，且在类别不平衡条件下的鲁棒性和实用性尚待增强。

Method: 提出 BlockRR 框架，将不同 RR 类型统一为一个参数化的随机响应实现；给出 ε-标签 DP 的理论证明；设计基于标签先验信息的权重矩阵分区方法，并利用并行组合性保证多次机制的合成仍满足 ε-标签 DP；在 CIFAR-10 的两种类别不平衡变体上进行实验评估。

Result: 理论上证明 BlockRR 满足 ε-标签 DP；经验上在高/中隐私（ε≤3）下实现了测试准确率与各类别平均准确率的更优平衡；在低隐私（ε≥4）下与其他方法一样，能够将 BlockRR 收敛为标准 RR，且无额外性能损失。

Conclusion: BlockRR 提供了一个通用且鲁棒的标签差分隐私解决方案，特别适合在高隐私要求下维持较好分类公平性和整体准确性；在低隐私场景可无成本地退化为标准 RR。未来可扩展到更多数据集及更复杂的标签分布情形，以评估其稳定性与鲁棒性。

Abstract: In this paper, we introduce BlockRR, a novel and unified randomized-response mechanism for label differential privacy. This framework generalizes existed RR-type mechanisms as special cases under specific parameter settings, which eliminates the need for separate, case-by-case analysis. Theoretically, we prove that BlockRR satisfies $ε$-label DP. We also design a partition method for BlockRR based on a weight matrix derived from label prior information; the parallel composition principle ensures that the composition of two such mechanisms remains $ε$-label DP. Empirically, we evaluate BlockRR on two variants of CIFAR-10 with varying degrees of class imbalance. Results show that in the high-privacy and moderate-privacy regimes ($ε\leq 3.0$), our propsed method gets a better balance between test accuaracy and the average of per-class accuracy. In the low-privacy regime ($ε\geq 4.0$), all methods reduce BlockRR to standard RR without additional performance loss.

</details>


### [243] [Universal Approximation of Continuous Functionals on Compact Subsets via Linear Measurements and Scalar Nonlinearities](https://arxiv.org/abs/2602.03290)
*Andrey Krylov,Maksim Penkin*

Main category: cs.LG

TL;DR: 对Hilbert空间乘积上紧致子集的连续泛函的通用近似：通过先进行有限个连续线性测量，再用标量非线性函数处理，最后融合，能实现任意连续泛函的均匀近似；对Banach-valued映射可获得有限秩近似。


<details>
  <summary>Details</summary>
Motivation: 解决无限维输入到有限维表示的近似问题，给出在紧致集上的统一性，支撑operator learning和成像领域中“测量-非线性作用-组合”范式的理论基础。

Method: 构造包含有限个线性测量的模型族，对这些测量输出应用标量非线性，最后通过一个连续的聚合/映射得到输出。证明在Hilbert空间乘积上的紧致子集上，此类模型可对任意连续泛函实现均匀近似；在Banach空间的情形，能得到有限秩表示。

Result: 理论上证明普适性与近似性；提供紧致集合的鲁棒性证据；将常见的operator learning和成像中的设计模式纳入严格框架。

Conclusion: 为“测量-应用标量非线性-融合”结构在实际学习任务中的应用提供理论支撑，并指出其在有限测量与有限秩近似方面的潜在优势和局限性。

Abstract: We study universal approximation of continuous functionals on compact subsets of products of Hilbert spaces. We prove that any such functional can be uniformly approximated by models that first take finitely many continuous linear measurements of the inputs and then combine these measurements through continuous scalar nonlinearities. We also extend the approximation principle to maps with values in a Banach space, yielding finite-rank approximations. These results provide a compact-set justification for the common ``measure, apply scalar nonlinearities, then combine'' design pattern used in operator learning and imaging.

</details>


### [244] [Anomaly Detection via Mean Shift Density Enhancement](https://arxiv.org/abs/2602.03293)
*Pritam Kar,Rahul Bordoloi,Olaf Wolkenhauer,Saptarshi Bej*

Main category: cs.LG

TL;DR: MSDE: unsupervised anomaly detection via mean-shift density enhancement; robust across anomaly types and noise; anomaly score by cumulative displacement; strong results on ADBench against 13 baselines.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised detectors struggle to generalize across anomaly types and noisy settings; need a robust, density-driven approach that leverages local density evolution.

Method: Weighted mean-shift with adaptive density weights derived from a UMAP-based fuzzy neighborhood graph; anomaly score is the total displacement after a few iterations.

Result: Evaluated on ADBench (46 real-world tabular datasets, 4 anomaly generation mechanisms, 6 noise levels); outperforms 13 baselines in AUC-ROC, AUC-PR, and Precision@N; robust and balanced performance across noise and anomaly types.

Conclusion: Displacement-based scoring via MSDE provides a robust unsupervised anomaly detection alternative, leveraging density-driven manifold evolution.

Abstract: Unsupervised anomaly detection stands as an important problem in machine learning, with applications in financial fraud prevention, network security and medical diagnostics. Existing unsupervised anomaly detection algorithms rarely perform well across different anomaly types, often excelling only under specific structural assumptions. This lack of robustness also becomes particularly evident under noisy settings. We propose Mean Shift Density Enhancement (MSDE), a fully unsupervised framework that detects anomalies through their geometric response to density-driven manifold evolution. MSDE is based on the principle that normal samples, being well supported by local density, remain stable under iterative density enhancement, whereas anomalous samples undergo large cumulative displacements as they are attracted toward nearby density modes. To operationalize this idea, MSDE employs a weighted mean-shift procedure with adaptive, sample-specific density weights derived from a UMAP-based fuzzy neighborhood graph. Anomaly scores are defined by the total displacement accumulated across a small number of mean-shift iterations. We evaluate MSDE on the ADBench benchmark, comprising forty six real-world tabular datasets, four realistic anomaly generation mechanisms, and six noise levels. Compared to 13 established unsupervised baselines, MSDE achieves consistently strong, balanced and robust performance for AUC-ROC, AUC-PR, and Precision@n, at several noise levels and on average over several types of anomalies. These results demonstrate that displacement-based scoring provides a robust alternative to the existing state-of-the-art for unsupervised anomaly detection.

</details>


### [245] [Periodic Regularized Q-Learning](https://arxiv.org/abs/2602.03301)
*Hyukjun Yang,Han-Dong Lim,Donghwan Lee*

Main category: cs.LG

TL;DR: 提出周期性正则化Q学习（PRQ），通过对投影算子引入正则化使投影值迭代（RP-VI）收缩，从而在线性函数逼近下实现有限时间收敛的样本基RL算法。


<details>
  <summary>Details</summary>
Motivation: 解决线性函数近似下Q学习的发散问题，借助正则化确保投影型值迭代的收敛性，并提升在函数近似环境中的稳定性与理论保证。

Method: 在投影算子层面引入正则化，构造正则化投影值迭代 RP-VI，并将该正则化投影推广到样本驱动的强化学习，得到 PRQ 算法；通过理论分析证明在线性函数逼近下 PRQ 的有限时间收敛。

Result: 正则化使投影值迭代成为收缩映射，获得稳定的收敛性；在随机/样本情形下，PRQ 提供可证明的有限时间收敛保证。

Conclusion: 该工作为带正则化的投影算子提供稳定且具备有限时间收敛理论的框架，增强线性函数逼近下的鲁棒性与稳定性；实际效果与参数选择、样本效率等需进一步评估。

Abstract: In reinforcement learning (RL), Q-learning is a fundamental algorithm whose convergence is guaranteed in the tabular setting. However, this convergence guarantee does not hold under linear function approximation. To overcome this limitation, a significant line of research has introduced regularization techniques to ensure stable convergence under function approximation. In this work, we propose a new algorithm, periodic regularized Q-learning (PRQ). We first introduce regularization at the level of the projection operator and explicitly construct a regularized projected value iteration (RP-VI), subsequently extending it to a sample-based RL algorithm. By appropriately regularizing the projection operator, the resulting projected value iteration becomes a contraction. By extending this regularized projection into the stochastic setting, we establish the PRQ algorithm and provide a rigorous theoretical analysis that proves finite-time convergence guarantees for PRQ under linear function approximation.

</details>


### [246] [medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions](https://arxiv.org/abs/2602.03305)
*Qianyi Xu,Gousia Habib,Feng Wu,Yanrui Du,Zhihui Chen,Swapnil Mishra,Dilruk Perera,Mengling Feng*

Main category: cs.LG

TL;DR: 提出一个基于LLM的离线奖励设计与验证管线，定义潜在函数作为奖励（生存、置信、胜任），并给出评估指标以在部署前选择最佳奖励结构，从而提升临床RL的策略性能。


<details>
  <summary>Details</summary>
Motivation: 临床强化学习受限于稀疏且手工设计的奖励信号，难以在多病种上泛化；需要自动化、可验证的奖励设计方法。

Method: 采用LLMs注入领域知识，构建三要素潜在函数的离线奖励设计框架；提出量化评估指标以在部署前筛选最优奖励；将奖励设计与策略学习耦合，提升DTR学习效果。

Result: 理论与框架层面的贡献，提供可复用的评估指标与自动化流程；预期在多疾病场景下显著提升策略表现的潜力。

Conclusion: LLM驱动的奖励设计与验证可提高临床RL的安全性与有效性，降低对手工设计的依赖，促进DTR的自动化实现。

Abstract: Reinforcement Learning (RL) offers a powerful framework for optimizing dynamic treatment regimes (DTRs). However, clinical RL is fundamentally bottlenecked by reward engineering: the challenge of defining signals that safely and effectively guide policy learning in complex, sparse offline environments. Existing approaches often rely on manual heuristics that fail to generalize across diverse pathologies. To address this, we propose an automated pipeline leveraging Large Language Models (LLMs) for offline reward design and verification. We formulate the reward function using potential functions consisted of three core components: survival, confidence, and competence. We further introduce quantitative metrics to rigorously evaluate and select the optimal reward structure prior to deployment. By integrating LLM-driven domain knowledge, our framework automates the design of reward functions for specific diseases while significantly enhancing the performance of the resulting policies.

</details>


### [247] [Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models](https://arxiv.org/abs/2602.03309)
*Yuelin Hu,Zhengxue Cheng,Wei Liu,Li Song*

Main category: cs.LG

TL;DR: EGSPO introduces entropy-gated selective policy optimization for hybrid SFT+RL in LLMs, performing three stages (SFT warm-up, entropy-informed RL rollouts, and token-level gradient modulation) to route high-entropy tokens to full PPO updates and low-entropy tokens to attenuated updates, all while using the advantage function. It yields consistent improvements on math benchmarks with modest overhead.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies and instability in hybrid SFT+RL by per-token gradient modulation guided by predictive entropy, aiming to improve exploration for uncertain tokens while preserving learned knowledge and reducing variance.

Method: A three-stage framework: (1) Stage 1 SFT warm-up using pure supervised loss on expert data; (2) Stage 2 RL rollouts with per-token predictive entropy computed on the current policy; (3) Stage 3 entropy-gated gradient allocation where high-entropy tokens trigger full PPO updates and low-entropy tokens trigger attenuated PPO updates; both branches use the advantage function A_t to push learning away from incorrect trajectories.

Result: Quantitative gains of 3.8 percentage points on AIME and 2.9 percentage points on MATH over CHORD-phi baseline, with about 3.4% additional computational overhead.

Conclusion: EGSPO yields consistent improvement on mathematical reasoning benchmarks with modest computational cost, demonstrating the efficacy of token-level entropy gating in hybrid SFT+RL settings.

Abstract: Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation.
  Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors.
  EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.

</details>


### [248] [From Inexact Gradients to Byzantine Robustness: Acceleration and Optimization under Similarity](https://arxiv.org/abs/2602.03329)
*Renaud Gaucher,Aymeric Dieuleveut,Hadrien Hendrikx*

Main category: cs.LG

TL;DR: 将带有 Byzantines 的分布式优化问题转化为带有 additive/multiplicative 误差的梯度不完全可用优化框架，并给出两种加速策略，显著降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习等分布式场景中对抗性节点（Byzantine 故障）导致的鲁棒性问题；现有鲁棒聚合分析多为经验性，缺乏对加速等更复杂算法的统一理论。

Method: 将更新视为在不完全梯度下的优化，误差包含加法和乘法两类；证实在此框架下，基于标准鲁棒聚合的梯度下降仍可达到最优渐近误差；提出两种加速策略：1) Nesterov 风格的加速；2) Similarity-based 优化，即引入近似全局损失的辅助损失函数。

Result: 给出理论上的收敛性与误差界，并在理论与实验层面显示两种方法显著降低通信复杂度。

Conclusion: 将鲁棒分布式优化统一至不完全梯度框架，提供可扩展的加速方案，兼具鲁棒性与低通信成本。

Abstract: Standard federated learning algorithms are vulnerable to adversarial nodes, a.k.a. Byzantine failures. To solve this issue, robust distributed learning algorithms have been developed, which typically replace parameter averaging by robust aggregations. While generic conditions on these aggregations exist to guarantee the convergence of (Stochastic) Gradient Descent (SGD), the analyses remain rather ad-hoc. This hinders the development of more complex robust algorithms, such as accelerated ones. In this work, we show that Byzantine-robust distributed optimization can, under standard generic assumptions, be cast as a general optimization with inexact gradient oracles (with both additive and multiplicative error terms), an active field of research.
  This allows for instance to directly show that GD on top of standard robust aggregation procedures obtains optimal asymptotic error in the Byzantine setting. Going further, we propose two optimization schemes to speed up the convergence. The first one is a Nesterov-type accelerated scheme whose proof directly derives from accelerated inexact gradient results applied to our formulation. The second one hinges on Optimization under Similarity, in which the server leverages an auxiliary loss function that approximates the global loss. Both approaches allow to drastically reduce the communication complexity compared to previous methods, as we show theoretically and empirically.

</details>


### [249] [Bayesian Conformal Prediction as a Decision Risk Problem](https://arxiv.org/abs/2602.03331)
*Fanyi Wu,Veronika Lohmanova,Samuel Kaski,Michele Caprio*

Main category: cs.LG

TL;DR: BCP combines Bayesian posterior predictive densities with Bayesian quadrature in a split conformal setup to estimate and minimize expected prediction-set size, providing valid coverage under misspecification with low variability; in sparse regression it yields 81% empirical coverage vs 49% for Bayesian credible intervals.


<details>
  <summary>Details</summary>
Motivation: Improve reliable uncertainty quantification in conformal prediction under model misspecification and distribution shifts, while reducing variability in prediction-set sizes.

Method: Use Bayesian posterior predictive densities as non-conformity scores and Bayesian quadrature to estimate expected set size; operate within a split conformal framework; apply to regression and classification tasks, including distribution-shifted settings like ImageNet-A.

Result: Prediction sets are comparable in size to split conformal prediction and show substantially lower run-to-run variability in set size; in sparse regression with nominal 80% coverage, BCP achieves 81% empirical coverage under a misspecified prior, while Bayesian credible intervals under-cover at 49%.

Conclusion: BCP provides valid coverage under misspecification, is robust to distribution shifts, and offers more stable coverage performance and competitive set sizes compared with split conformal prediction and Bayesian credible intervals.

Abstract: Bayesian posterior predictive densities as non-conformity scores and Bayesian quadrature are used to estimate and minimise the expected prediction set size. Operating within a split conformal framework, BCP provides valid coverage guarantees and demonstrates reliable empirical coverage under model misspecification. Across regression and classification tasks, including distribution-shifted settings such as ImageNet-A, BCP yields prediction sets of comparable size to split conformal prediction, while exhibiting substantially lower run-to-run variability in set size. In sparse regression with nominal coverage of 80 percent, BCP achieves 81 percent empirical coverage under a misspecified prior, whereas Bayesian credible intervals under-cover at 49 percent.

</details>


### [250] [Causal Graph Learning via Distributional Invariance of Cause-Effect Relationship](https://arxiv.org/abs/2602.03353)
*Nang Hung Nguyen,Phi Le Nguyen,Thao Nguyen Truong,Trong Nghia Hoang,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出一种基于不变性检验的因果图发现框架，通过对因果分布的先验变化下的条件分布方差进行检验，实现对因果边的高效发现，理论复杂度为变量数的平方，且在大规模数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决现有因果图推断在大规模观测数据上的计算成本和可扩展性问题；利用因果机制导致的 E|C 不随 C 的先验分布变化而保持不变这一直觉，提供一个直接的因果关系检验方法；同时借助图的稀疏性假设提升算法可行性。

Method: 在不同的下采样子集（reflecting 不同的先验 C 分布）中，计算每个潜在因果对的 E|C 条件分布，并评估它们的方差。若方差保持不变，视为潜在因果关系；使用一个基于不变性检验的边筛选流程，结合 empirical sparsity，提出一个对变量数量二次复杂度的算法，显著提速。

Result: 在大规模数据集上的广泛基准实验显示，其性能优于或等同于现有方法，同时在时间/资源方面实现显著的可扩展性提升（对比 SOTA，时间可缩短至 25x）。

Conclusion: 该框架以不变性检验为核心，结合数据子集的多样性来区分因果关系与非因果关联，验证了在大规模观测数据上的有效性与可扩展性，且在多种数据集上达到竞争性结果。

Abstract: This paper introduces a new framework for recovering causal graphs from observational data, leveraging the observation that the distribution of an effect, conditioned on its causes, remains invariant to changes in the prior distribution of those causes. This insight enables a direct test for potential causal relationships by checking the variance of their corresponding effect-cause conditional distributions across multiple downsampled subsets of the data. These subsets are selected to reflect different prior cause distributions, while preserving the effect-cause conditional relationships. Using this invariance test and exploiting an (empirical) sparsity of most causal graphs, we develop an algorithm that efficiently uncovers causal relationships with quadratic complexity in the number of observational variables, reducing the processing time by up to 25x compared to state-of-the-art methods. Our empirical experiments on a varied benchmark of large-scale datasets show superior or equivalent performance compared to existing works, while achieving enhanced scalability.

</details>


### [251] [Achieving Linear Speedup for Composite Federated Learning](https://arxiv.org/abs/2602.03357)
*Kun Huang,Shi Pu*

Main category: cs.LG

TL;DR: FedNMap achieves linear speedup in a nonconvex composite FL setting using a normal map-based update and local correction, under standard assumptions including smoothness, weak convexity of the regularizer, and bounded gradient variance; claims to be first to show linear speedup for nonconvex composite FL with/without PL.


<details>
  <summary>Details</summary>
Motivation: Address data heterogeneity and nonsmooth regularization in federated learning for composite objectives, and to establish scalable convergence guarantees that improve with more clients and local updates.

Method: Propose FedNMap with a normal map-based update to handle the nonsmooth regularizer and a local correction strategy to reduce heterogeneity effects; prove linear speedup in convergence rate with respect to n (number of clients) and Q (local updates) under smoothness, weak convexity, and bounded variance, for nonconvex losses, with/without PL.

Result: Theoretical convergence guarantees showing linear speedup in n and Q for nonconvex composite FL under stated assumptions; extends to cases with and without PL condition; claims novelty as first to establish such linear speedup in nonconvex composite FL.

Conclusion: FedNMap provides a scalable optimization framework for composite federated learning with nonsmooth regularizers, delivering linear speedup in client count and local iterations, and broadens the understanding of convergence behavior in nonconvex FL with heterogeneity.

Abstract: This paper proposes FedNMap, a normal map-based method for composite federated learning, where the objective consists of a smooth loss and a possibly nonsmooth regularizer. FedNMap leverages a normal map-based update scheme to handle the nonsmooth term and incorporates a local correction strategy to mitigate the impact of data heterogeneity across clients. Under standard assumptions, including smooth local losses, weak convexity of the regularizer, and bounded stochastic gradient variance, FedNMap achieves linear speedup with respect to both the number of clients $n$ and the number of local updates $Q$ for nonconvex losses, both with and without the Polyak-Łojasiewicz (PL) condition. To our knowledge, this is the first result establishing linear speedup for nonconvex composite federated learning.

</details>


### [252] [Rethinking Benign Relearning: Syntax as the Hidden Driver of Unlearning Failures](https://arxiv.org/abs/2602.03379)
*Sangyeon Yoon,Hyesoo Hong,Wonje Jeung,Albert No*

Main category: cs.LG

TL;DR: 提出句法多样化（syntactic diversification）以抑制 benign relearning，并指出句法相似性比话题相关性对重现有更强驱动。


<details>
  <summary>Details</summary>
Motivation: 现有的无忘记/去学习方法在微调阶段仍会让被忘信息再现，即使观察数据并不具备明显话题相关性；该现象（benign relearning）揭示方法鲁棒性不足。

Method: 对比基准数据中忘记内容的句法相似性与话题相关性，分析其对表示与梯度对齐的影响；提出将忘记查询进行 paraphrase 的句法改写策略以提高无忘记效果，并评估对效率与模型性能的影响。

Result: 发现 syntactic 相似性与表示/梯度对齐是造成再现的主驱动；通过句法多样化改写忘记查询，显著降低 benign relearning、加速遗忘，并缓解 unlearning 的效用-准确性权衡。

Conclusion: 句法层面的多样化查询能提升无忘记方法的鲁棒性与效率，建议在实际应用中将查询多样化作为标准步骤以减少再现风险。

Abstract: Machine unlearning aims to remove specific content from trained models while preserving overall performance. However, the phenomenon of benign relearning, in which forgotten information reemerges even from benign fine-tuning data, reveals that existing unlearning methods remain fundamentally fragile. A common explanation attributes this effect to topical relevance, but we find this account insufficient. Through systematic analysis, we demonstrate that syntactic similarity, rather than topicality, is the primary driver: across benchmarks, syntactically similar data consistently trigger recovery even without topical overlap, due to their alignment in representations and gradients with the forgotten content. Motivated by this insight, we introduce syntactic diversification, which paraphrases the original forget queries into heterogeneous structures prior to unlearning. This approach effectively suppresses benign relearning, accelerates forgetting, and substantially alleviates the trade-off between unlearning efficacy and model utility.

</details>


### [253] [Dynamic Topology Optimization for Non-IID Data in Decentralized Learning](https://arxiv.org/abs/2602.03383)
*Bart Cox,Antreas Ioannou,Jérémie Decouchant*

Main category: cs.LG

TL;DR: 在去中心化学习中，提出 Morph，通过固定入度、通过对等发现和多样性驱动选择实现拓扑自适应，以应对非 IID 数据和静态拓扑导致的性能下降；在分布式环境中接近全连接上限并提升收敛速度与稳定性。


<details>
  <summary>Details</summary>
Motivation: 非 IID 数据分布和静态拓扑使去中心化学习难以获得高准确率。需要动态调整通信拓扑以提升鲁棒性和收敛性，同时在不依赖全局信息的前提下提升性能。

Method: Morph 通过保持 Fixed in-degree 的拓扑结构，同时通过基于 gossip 的对等发现和多样性驱动的邻居选择，动态重塑通信图。邻居选择基于最大模型不相似性以提高信息多样性；实现无需全局知识、以分布式方式优化拓扑。

Result: 在 CIFAR-10 与 FEMNIST 的多达 100 节点实验中，Morph 稳定优于静态和流行病式基线，接近全连接上限。在 CIFAR-10 上相较 state-of-the-art 基线提升约 1.12x，FEMNIST 上比 Epidemic Learning 高 1.08x；50 节点时也缩短与全连接上限的差距约在 0.5 个百分点；表现为更高最终准确率、加速收敛以及更低的节点间方差，同时需要的通信轮次更少且不依赖全局信息。

Conclusion: Morph 通过自适应拓扑和信息多样性提升，实现更高的准确性、更快的收敛和更稳定的学习，在不增加全局知识的前提下提升去中心化学习的鲁棒性和效率。

Abstract: Decentralized learning (DL) enables a set of nodes to train a model collaboratively without central coordination, offering benefits for privacy and scalability. However, DL struggles to train a high accuracy model when the data distribution is non-independent and identically distributed (non-IID) and when the communication topology is static. To address these issues, we propose Morph, a topology optimization algorithm for DL. In Morph, nodes adaptively choose peers for model exchange based on maximum model dissimilarity. Morph maintains a fixed in-degree while dynamically reshaping the communication graph through gossip-based peer discovery and diversity-driven neighbor selection, thereby improving robustness to data heterogeneity. Experiments on CIFAR-10 and FEMNIST with up to 100 nodes show that Morph consistently outperforms static and epidemic baselines, while closely tracking the fully connected upper bound. On CIFAR-10, Morph achieves a relative improvement of 1.12x in test accuracy compared to the state-of-the-art baselines. On FEMNIST, Morph achieves an accuracy that is 1.08x higher than Epidemic Learning. Similar trends hold for 50 node deployments, where Morph narrows the gap to the fully connected upper bound within 0.5 percentage points on CIFAR-10. These results demonstrate that Morph achieves higher final accuracy, faster convergence, and more stable learning as quantified by lower inter-node variance, while requiring fewer communication rounds than baselines and no global knowledge.

</details>


### [254] [An Approximate Ascent Approach To Prove Convergence of PPO](https://arxiv.org/abs/2602.03386)
*Leif Doering,Daniel Schmidt,Moritz Melcher,Sebastian Kassing,Benedikt Wille,Tilman Aach,Simon Weissmann*

Main category: cs.LG

TL;DR: 将PPO的多轮小批更新视为近似策略梯度上升，给出收敛性证明，并揭示截断GAE中的几何权重问题及其修正对强终止信号任务的影响。


<details>
  <summary>Details</summary>
Motivation: 澄清PPO在理论层面的收敛性与稳定性来源，解释其在实际中的成功，并消除对其理论基础的模糊之处。

Method: 在标准理论假设下，将PPO的策略更新（多轮 minibatch、对多次使用的rollout进行 surrogate-gradient 更新）与近似策略梯度上升等价化；利用随机重洗牌等技术构造并证明收敛性；系统性分析截断GAE中的权重设计，发现其导致无限质量聚集于最长k步估计，并提出权重修正策略。

Result: 获得PPO在理论上的收敛性证明，并解释其实际表现；揭示截断GAE中的几何权重问题及其解决方案，实验表明在终止信号强的环境（如 Lunar Lander）通过权重修正可显著提升性能。

Conclusion: 为PPO提供更完整的理论框架，强调对GAE权重设计的关注以及在实践中对策略更新稳定性的影响，指出未来工作可在更广泛环境下验证并优化权重修正。

Abstract: Proximal Policy Optimization (PPO) is among the most widely used deep reinforcement learning algorithms, yet its theoretical foundations remain incomplete. Most importantly, convergence and understanding of fundamental PPO advantages remain widely open. Under standard theory assumptions we show how PPO's policy update scheme (performing multiple epochs of minibatch updates on multi-use rollouts with a surrogate gradient) can be interpreted as approximated policy gradient ascent. We show how to control the bias accumulated by the surrogate gradients and use techniques from random reshuffling to prove a convergence theorem for PPO that sheds light on PPO's success. Additionally, we identify a previously overlooked issue in truncated Generalized Advantage Estimation commonly used in PPO. The geometric weighting scheme induces infinite mass collapse onto the longest $k$-step advantage estimator at episode boundaries. Empirical evaluations show that a simple weight correction can yield substantial improvements in environments with strong terminal signal, such as Lunar Lander.

</details>


### [255] [On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.03392)
*Shumin Wang,Yuexiang Xie,Wenhao Zhang,Yuchang Sun,Yanxi Chen,Yaliang Li,Yanyong Zhang*

Main category: cs.LG

TL;DR: 提出一个关于强化微调（RFT）中熵动态的理论框架，推导单次对数 logits 更新下的熵变、扩展到 GRPO 的更新式，并给出基于熵的控制方法及经验验证。


<details>
  <summary>Details</summary>
Motivation: 揭示熵在 RFT 过程中的动态演化及其对探索-利用权衡的影响，提供比仅凭直觉更系统的理论基础。

Method: 以一个判别表达式刻画单次对数更新下的熵变，然后推出一阶熵变表达式，进一步推广至 Group Relative Policy Optimization (GRPO) 的更新公式；基于推导提出熵控制方法和熵-判别器裁剪等策略；通过实验验证理论结论并评估所提方法的有效性。

Result: 建立了熵动态的理论框架、给出一阶近似及 GRPO 更新的推导，提出了熵控策略（如熵-判别器裁剪）并有实证支持，提供了统一视角解释现有熵相关方法。

Conclusion: 为 RFT 训练过程中的探索-利用平衡提供理论支撑与实用策略，并通过理论与实验的结合深化对熵在语言模型微调中作用的理解。

Abstract: Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.

</details>


### [256] [Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing](https://arxiv.org/abs/2602.03452)
*Xin Sheng,Jiaxin Li,Yujuan Pang,Ran Peng,Yong Ma*

Main category: cs.LG

TL;DR: 引入正负配对和加权GRPO来改进RLVR中的提示选择：通过同时采样一个难但可解的正向提示(q+)和一个易但易碎的负向提示(q−)，以及对成败结果进行分组归一化加权，从而提升样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决基于训练精度方差的提示选择导致的优化不稳定、迁移性差，以及RLVR中缺乏明确的负向学习信号的问题。需要一个能提供可靠正向锚点和负向学习信号的 minibatch 机制。

Method: 提出正负配对：在每次更新中同时采样一个难但可解的 q+ 和一个易但脆弱的 q−，通过多轮回 rollout 估计其成功率；引入加权GRPO，在对成败进行二元化处理时按对(group)重新归一化优势，对 q+ 的罕见成功给予强正向信号、对 q− 的罕见失败给予强负向信号，使学习具有双向信息。

Result: 在 Qwen2.5-Math-7B（以及 Instruct 变体）上，以每次更新仅用一个配对 minibatch 就超越仅凭方差选择的 GRPO 基线：AIME 2025 Pass@8 从 16.8 提升到 22.2，AMC23 Pass@64 从 94.0 提升到 97.0，且与以 1209 个训练提示池进行 RLVR 的方法相当。

Conclusion: 双向信号的正负配对与分组归一化加权可提升 RLVR 的样本效率与稳定性，且在大规模推理评估任务中对提示集合规模较小的情形也具备竞争力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is effective for training large language models on deterministic outcome reasoning tasks. Prior work shows RLVR works with few prompts, but prompt selection is often based only on training-accuracy variance, leading to unstable optimization directions and weaker transfer. We revisit prompt selection from a mechanism-level view and argue that an effective minibatch should provide both (i) a reliable positive anchor and (ii) explicit negative learning signals from rare failures. Based on this principle, we propose \emph{positive--negative pairing}: at each update, we sample a hard-but-solvable $q^{+}$ and an easy-but-brittle prompt $q^{-}$(high success rate but not perfect), characterized by low and high empirical success rates under multiple rollouts. We further introduce Weighted GRPO, which reweights binary outcomes at the pair level and uses group-normalized advantages to amplify rare successes on $q^{+}$ into sharp positive guidance while turning rare failures on $q^{-}$ into strong negative penalties. This bidirectional signal provides informative learning feedback for both successes and failures, improving sample efficiency without suppressing exploration. On Qwen2.5-Math-7B, a single paired minibatch per update consistently outperforms a GRPO baseline that selects two prompts via commonly used variance-based selection heuristics: AIME~2025 Pass@8 improves from 16.8 to 22.2, and AMC23 Pass@64 from 94.0 to 97.0, while remaining competitive with large-scale RLVR trained from a pool of 1209 training prompts. Similar gains are observed on Qwen2.5-Math-7B-Instruct.

</details>


### [257] [ScDiVa: Masked Discrete Diffusion for Joint Modeling of Single-Cell Identity and Expression](https://arxiv.org/abs/2602.03477)
*Mingxuan Wang,Cheng Chen,Gaoyang Jiang,Zijia Ren,Chuangxin Zhao,Lu Shi,Yanbiao Ma*

Main category: cs.LG

TL;DR: 提出 masked discrete diffusion 模型 scDiVa，通过在 token 空间定义连续时间前向掩蔽，联合建模离散基因身份与连续表达量，解决单细胞 RNA-seq 的排序偏差与误差累积问题；在 5900 万细胞的大规模预训练下，在批次整合、细胞类型标注和扰动响应预测等任务上实现强迁移性能，显示 masked discrete diffusion 相较自回归具生物学合理性和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决单细胞 RNA-seq 数据的高维、稀疏、无序特性在自回归生成中导致的排序偏差与误差累积；需要对离散的基因身份和连续的表达量进行统一建模，并提高跨任务的迁移能力。

Method: 提出 scDiVa—a masked discrete diffusion foundation model。通过在 token space 定义连续时间前向掩蔽机制，与双向去噪器共同建模离散基因身份和连续表达量；使用 entropy-normalized serialization 与潜在锚点 token 提高信息效率与保留全局细胞身份；采用深度不变时间采样和双去噪目标训练以模拟不同稀疏度并精确恢复身份与数值；在 59 百万细胞数据集上进行预训练。

Result: 在主要基准上实现强迁移能力，覆盖批次整合、细胞类型注释与扰动响应预测等任务；显示 masked discrete diffusion 能作为自回归的生物学上合理且有效的替代方法。

Conclusion:  masked discrete diffusion 为单细胞 RNA-seq 的生成建模提供生物学上合理且高效的替代，与自回归相比具更好的跨任务迁移性与全局信息保留能力。

Abstract: Single-cell RNA-seq profiles are high-dimensional, sparse, and unordered, causing autoregressive generation to impose an artificial ordering bias and suffer from error accumulation. To address this, we propose scDiVa, a masked discrete diffusion foundation model that aligns generation with the dropout-like corruption process by defining a continuous-time forward masking mechanism in token space. ScDiVa features a bidirectional denoiser that jointly models discrete gene identities and continuous values, utilizing entropy-normalized serialization and a latent anchor token to maximize information efficiency and preserve global cell identity. The model is trained via depth-invariant time sampling and a dual denoising objective to simulate varying sparsity levels while ensuring precise recovery of both identity and magnitude. Pre-trained on 59 million cells, scDiVa achieves strong transfer performance across major benchmarks, including batch integration, cell type annotation, and perturbation response prediction. These results suggest that masked discrete diffusion serves as a biologically coherent and effective alternative to autoregression.

</details>


### [258] [Reparameterization Flow Policy Optimization](https://arxiv.org/abs/2602.03501)
*Hai Zhong,Zhuoran Li,Xun Wang,Longbo Huang*

Main category: cs.LG

TL;DR: 提出 Reparameterization Flow Policy Optimization (RFO)，将重参数化策略梯度拓展至通过微分ODE积分生成动作的流式策略，通过对流的生成过程和系统动力学共同反向传播实现高样本效率，且无需计算难以获得的对数似然；引入两项针对稳定性与探索性的正则化，以及动作分块的变体，在多种仿生任务（包括软体与刚性体、状态/视觉输入）上展现出色表现，软体四足任务实现约2倍的奖励提升。


<details>
  <summary>Details</summary>
Motivation: 当前的 RPG 主要局限于高斯策略，难以充分利用近来流式/生成模型的进展；将可微分的流策略与 RPG 对齐有潜力提高样本效率与表达能力，但简单迁移存在训练不稳定与探索不足的问题。

Method: 提出 RFO：通过对流生成过程和系统动力学进行联合反向传播来计算策略梯度；引入两项定制化正则化以提升训练稳定性与探索性；另给出动作分块（chunking）变体以提高可扩展性。

Result: 在多样的运动与控制任务（包括刚性与软体、状态/视觉输入）上进行广泛实验，RFO 显示出高样本效率与强稳健性，且在一个具有挑战性的软体四足任务中达到几乎是当前最优基线的2倍奖励。

Conclusion: 流式策略能够与 RPG 自然契合并提升性能；RFO 提供稳定、探索性更强的策略优化途径，且无需处理难以求解的对数似然，展示了将 RPG 与现代生成流模型结合的潜力。

Abstract: Reparameterization Policy Gradient (RPG) has emerged as a powerful paradigm for model-based reinforcement learning, enabling high sample efficiency by backpropagating gradients through differentiable dynamics. However, prior RPG approaches have been predominantly restricted to Gaussian policies, limiting their performance and failing to leverage recent advances in generative models. In this work, we identify that flow policies, which generate actions via differentiable ODE integration, naturally align with the RPG framework, a connection not established in prior work. However, naively exploiting this synergy proves ineffective, often suffering from training instability and a lack of exploration. We propose Reparameterization Flow Policy Optimization (RFO). RFO computes policy gradients by backpropagating jointly through the flow generation process and system dynamics, unlocking high sample efficiency without requiring intractable log-likelihood calculations. RFO includes two tailored regularization terms for stability and exploration. We also propose a variant of RFO with action chunking. Extensive experiments on diverse locomotion and manipulation tasks, involving both rigid and soft bodies with state or visual inputs, demonstrate the effectiveness of RFO. Notably, on a challenging locomotion task controlling a soft-body quadruped, RFO achieves almost $2\times$ the reward of the state-of-the-art baseline.

</details>


### [259] [Soft-Radial Projection for Constrained End-to-End Learning](https://arxiv.org/abs/2602.03461)
*Philipp J. Schneider,Daniel Kuhn*

Main category: cs.LG

TL;DR: 引入Soft-Radial Projection层，利用径向映射将预测投影到可行域内部，保持全秩雅可比，避免边界投影造成的梯度饱和，并保留通用逼近性，实验证明优于基线。


<details>
  <summary>Details</summary>
Motivation: 在需要严格可行性的安全关键系统中，现有的投影-基构造层将外部点投影到边界，导致雅可比降阶、梯度消失，阻碍优化。需要一种既保证可行性又保留梯度信息的替代方案。

Method: 提出一种可微重参数化层Soft-Radial Projection，通过从欧几里得空间将点径向映射到可行集内部，确保严格可行性且大多数点附近的雅可比为满秩，避免传统边界投影的梯度饱和。

Result: 理论上证明该结构仍具备通用逼近性；在实验中实现比当前最先进的基线在收敛性和解质量方面更优。

Conclusion: Soft-Radial Projection为需要强约束的深度学习应用提供稳定、可扩展的解决方案，缓解梯度饱和问题并提升优化性能。

Abstract: Integrating hard constraints into deep learning is essential for safety-critical systems. Yet existing constructive layers that project predictions onto constraint boundaries face a fundamental bottleneck: gradient saturation. By collapsing exterior points onto lower-dimensional surfaces, standard orthogonal projections induce rank-deficient Jacobians, which nullify gradients orthogonal to active constraints and hinder optimization. We introduce Soft-Radial Projection, a differentiable reparameterization layer that circumvents this issue through a radial mapping from Euclidean space into the interior of the feasible set. This construction guarantees strict feasibility while preserving a full-rank Jacobian almost everywhere, thereby preventing the optimization stalls typical of boundary-based methods. We theoretically prove that the architecture retains the universal approximation property and empirically show improved convergence behavior and solution quality over state-of-the-art optimization- and projection-based baselines.

</details>


### [260] [Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models](https://arxiv.org/abs/2602.03506)
*Arco van Breda,Erman Acar*

Main category: cs.LG

TL;DR: PATCHES is an evolutionary circuit discovery method that uncovers compact and correct circuits in a symbolic regression (SR) transformer, enabling the first circuit-level interpretability. Through a causal evaluation framework, mean patching with performance-based evaluation most reliably isolates functional circuits; direct logit attribution and probing capture mainly correlational features. The work establishes SR as a prime domain for mechanistic interpretability and provides a principled circuit-discovery methodology.


<details>
  <summary>Details</summary>
Motivation: To reveal the internal mechanisms by which SR transformers generate mathematical operators and to apply mechanistic interpretability to SR, a domain where such analysis has been lacking.

Method: Introduce PATCHES, an evolutionary circuit-discovery algorithm that isolates compact, correct circuits in an SR transformer. Identify 28 circuits. Validate via a robust causal evaluation framework using faithfulness, completeness, and minimality. Compare mean patching (performance-based evaluation) to direct logit attribution and probing classifiers.

Result: Identified 28 circuits in the SR transformer. The causal evaluation demonstrates that mean patching with performance-based evaluation most reliably isolates functionally correct circuits. Logit attribution and probing classifiers largely capture correlational, not causal, features.

Conclusion: SR is a high-potential domain for mechanistic interpretability. The study proposes a principled methodology for circuit discovery and highlights mean patching as a reliable tool for isolating causal circuits, while warning that common attribution/probing approaches may lack causal validity.

Abstract: Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits for SR. Using PATCHES, we isolate 28 circuits, providing the first circuit-level characterisation of an SR transformer. We validate these findings through a robust causal evaluation framework based on key notions such as faithfulness, completeness, and minimality. Our analysis shows that mean patching with performance-based evaluation most reliably isolates functionally correct circuits. In contrast, we demonstrate that direct logit attribution and probing classifiers primarily capture correlational features rather than causal ones, limiting their utility for circuit discovery. Overall, these results establish SR as a high-potential application domain for mechanistic interpretability and propose a principled methodology for circuit discovery.

</details>


### [261] [Mitigating Staleness in Asynchronous Pipeline Parallelism via Basis Rotation](https://arxiv.org/abs/2602.03515)
*Hyunji Jung,Sungbin Shin,Namhoon Lee*

Main category: cs.LG

TL;DR: 通过基变换纠正梯度延迟以实现异步流水线并行的可扩展性，缓解梯度滞后随流水线深度线性增长的问题，并通过 basis rotation 提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 最大化硬件利用率，消除同步执行中的流水线气泡；然而梯度滞后随流水线深度线性增长，削弱可扩展性，需寻找对齐与自适应优化的解决方案。

Method: 提出基变换（basis rotation）以对齐 Hessian 的本征基与标准坐标基，解决坐标系错位导致的自适应优化（如 Adam）失效问题；提供理论分析与实证验证。

Result: 理论分析与实证结果均表明基变换有效缓解延迟梯度的对齐问题，显著加速收敛。以1B参数语言模型为例，基变换在最佳异步流水线基线基础上减少76.8% 的迭代次数以达到相同训练损失。

Conclusion: 基变换重新实现了异步流水线并行的可扩展性，并提升收敛速度，提出了一种有效解决梯度滞后的新范式。

Abstract: Asynchronous pipeline parallelism maximizes hardware utilization by eliminating the pipeline bubbles inherent in synchronous execution, offering a path toward efficient large-scale distributed training. However, this efficiency gain can be compromised by gradient staleness, where the immediate model updates with delayed gradients introduce noise into the optimization process. Crucially, we identify a critical, yet often overlooked, pathology: this delay scales linearly with pipeline depth, fundamentally undermining the very scalability that the method originally intends to provide. In this work, we investigate this inconsistency and bridge the gap by rectifying delayed gradients through basis rotation, restoring scalable asynchronous training while maintaining performance. Specifically, we observe that the deleterious effects of delayed gradients are exacerbated when the Hessian eigenbasis is misaligned with the standard coordinate basis. We demonstrate that this misalignment prevents coordinate-wise adaptive schemes, such as Adam, from effectively leveraging curvature-aware adaptivity. This failure leads to significant oscillations in the optimization trajectory and, consequently, slower convergence. We substantiate these findings through both rigorous theoretical analysis and empirical evaluation. To address this challenge, we propose the use of basis rotation, demonstrating that it effectively mitigates the alignment issue and significantly accelerates convergence in asynchronous settings. For example, our training of a 1B-parameter LLM with basis rotation achieves the same training loss in 76.8% fewer iterations compared to the best-performing asynchronous pipeline parallel training baseline.

</details>


### [262] [Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning](https://arxiv.org/abs/2602.03516)
*Zixiang Di,Jinyi Han,Shuo Zhang,Ying Liao,Zhi Li,Xiaofeng Ji,Yongqi Wang,Zheming Yang,Ming Gao,Bingdong Li,Jie Wang*

Main category: cs.LG

TL;DR: 提出 Plausible Negative Samples (PNS)，通过生成在格式和结构上接近正确解但答案错误的高质量负样本，并采用反向强化学习进行训练，提升多模型在数学推理任务的偏好优化性能，平均提升约2.03%。


<details>
  <summary>Details</summary>
Motivation: 现有方法对负样本简单地将所有错误回答等同对待，忽视负样本质量对模型学习的影响，亟需高质量负样本以更精准地引导模型区分正确与错误。

Method: 生成看似正确但实际错误的负样本，使用反向强化学习在一个专用模型上进行训练，设计一个复合奖励：格式合规性、准确性反转、奖励模型评估以及链式思维评估，以使输出与正确解高度相似但最终给出错误答案。

Result: 在七个数学推理基准上，对三种骨干模型进行偏好优化验证，PNS总体上优于其他负样本合成方法，且相较RL训练基线平均提升约2.03%。

Conclusion: PNS可作为一种可插拔的数据源用于偏好优化，具有跨模型应用潜力，能通过高质量负样本更有效地提升LLM的推理能力。

Abstract: Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples exhibiting expected format and structural coherence while ultimately yielding incorrect answers. PNS trains a dedicated model via reverse reinforcement learning (RL) guided by a composite reward combining format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation, generating responses nearly indistinguishable from correct solutions. We further validate PNS as a plug-and-play data source for preference optimization across three backbone models on seven mathematical reasoning benchmarks. Results demonstrate that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models.

</details>


### [263] [Live or Lie: Action-Aware Capsule Multiple Instance Learning for Risk Assessment in Live Streaming Platforms](https://arxiv.org/abs/2602.03520)
*Yiran Qiao,Jing Chen,Xiang Ao,Qiwei Zhong,Yang Liu,Qing He*

Main category: cs.LG

TL;DR: 在弱监督（仅房间级标签）下，将房间视为一个包，提出 Action-aware Capsule MIL（AC-MIL）。通过序列化与并行架构的人行为胶囊，同时建模单体行为与跨用户协同，编码时序动态与跨用户依赖，实现房间级风险预测并给出行为段级可解释证据。在抖音大规模数据集上显著优于 MIL 与序列 baselines，达到新的 state-of-the-art。


<details>
  <summary>Details</summary>
Motivation: 直播间风险评估中存在稀疏且协同行为的恶意模式，且仅能获得房间级标签的弱监督信号；现有 MIL 与序列方法在捕捉跨用户协同与局部行为模式方面能力有限，缺乏可解释性。

Method: AC-MIL 框架：以房间作为袋，使用结构化的用户-时隙胶囊作为实例，串行与并行双路径架构以编码时间动态与跨用户依赖；通过行动感知的胶囊对局部行为与全局协同进行多粒度语义建模，并输出房间级风险预测及行为段的可解释证据（胶囊级）。

Result: 在大规模工业数据集（ Douyin）上，AC-MIL 显著优于传统 MIL 与序列 baselines，达到房间级风险评估的新状态-of-the-art，并提供胶囊级可解释证据以指示潜在的风险行为段。

Conclusion: AC-MIL 有效应对弱监督下的房间级风险评估，兼具对局部行为与跨用户协同的建模能力与可解释性，具备实用性与可迁移性。

Abstract: Live streaming has become a cornerstone of today's internet, enabling massive real-time social interactions. However, it faces severe risks arising from sparse, coordinated malicious behaviors among multiple participants, which are often concealed within normal activities and challenging to detect timely and accurately. In this work, we provide a pioneering study on risk assessment in live streaming rooms, characterized by weak supervision where only room-level labels are available. We formulate the task as a Multiple Instance Learning (MIL) problem, treating each room as a bag and defining structured user-timeslot capsules as instances. These capsules represent subsequences of user actions within specific time windows, encapsulating localized behavioral patterns. Based on this formulation, we propose AC-MIL, an Action-aware Capsule MIL framework that models both individual behaviors and group-level coordination patterns. AC-MIL captures multi-granular semantics and behavioral cues through a serial and parallel architecture that jointly encodes temporal dynamics and cross-user dependencies. These signals are integrated for robust room-level risk prediction, while also offering interpretable evidence at the behavior segment level. Extensive experiments on large-scale industrial datasets from Douyin demonstrate that AC-MIL significantly outperforms MIL and sequential baselines, establishing new state-of-the-art performance in room-level risk assessment for live streaming. Moreover, AC-MIL provides capsule-level interpretability, enabling identification of risky behavior segments as actionable evidence for intervention. The project page is available at: https://qiaoyran.github.io/AC-MIL/.

</details>


### [264] [A Minimal Task Reveals Emergent Path Integration and Object-Location Binding in a Predictive Sequence Model](https://arxiv.org/abs/2602.03490)
*Linda Ariel Ventura,Victoria Bosch,Tim C Kietzmann,Sushrut Thorat*

Main category: cs.LG

TL;DR: Action-conditioned sequence prediction learns structured world models with dynamic binding and path integration, showing in-context learning in a minimal in-silico setup.


<details>
  <summary>Details</summary>
Motivation: Understand how structured internal representations and flexible binding can emerge from predictive, action-conditioned sequential prediction as a mechanism for sequential world models in cognition.

Method: Train an RNN to predict the next token from current input and a saccade-like displacement in 2D continuous token scenes; use decoding analyses to reveal path integration and token-position binding; perform interventional analyses to test late-binding learning and learning of out-of-distribution bindings.

Result: Prediction accuracy improves across the sequence on novel scenes, indicating in-context learning; decoding reveals path integration and dynamic binding of token identity to position; bindings can be learned late in the sequence and include out-of-distribution cases.

Conclusion: Structured representations with flexible binding emerge to support prediction, providing a mechanistic account of sequential world modeling relevant to cognitive science.

Abstract: Adaptive cognition requires structured internal models representing objects and their relations. Predictive neural networks are often proposed to form such "world models", yet their underlying mechanisms remain unclear. One hypothesis is that action-conditioned sequential prediction suffices for learning such world models. In this work, we investigate this possibility in a minimal in-silico setting. Sequentially sampling tokens from 2D continuous token scenes, a recurrent neural network is trained to predict the upcoming token from current input and a saccade-like displacement. On novel scenes, prediction accuracy improves across the sequence, indicating in-context learning. Decoding analyses reveal path integration and dynamic binding of token identity to position. Interventional analyses show that new bindings can be learned late in sequence and that out-of-distribution bindings can be learned. Together, these results demonstrate how structured representations that rely on flexible binding emerge to support prediction, offering a mechanistic account of sequential world modeling relevant to cognitive science.

</details>


### [265] [Least but not Last: Fine-tuning Intermediate Principal Components for Better Performance-Forgetting Trade-Offs](https://arxiv.org/abs/2602.03493)
*Alessio Quercia,Arya Bangun,Ira Assent,Hanno Scharr*

Main category: cs.LG

TL;DR: 对LoRA初始化的性能与遗忘权衡的系统分析，基于主成分初始化，发现对中间组件的微调在准确性与对高学习率鲁棒性方面优于首/尾组件方法，提出基于主成分的LoRA初始化策略，在CV和NLP任务及持续学习场景中显著提升准确性并降低遗忘。


<details>
  <summary>Details</summary>
Motivation: 在大模型微调中，如何在提升任务相关性能与避免对预训练知识的灾难性遗忘之间取得平衡，是LoRA方法的核心挑战。现有方法对最佳初始化和组件选择的结论不一致，亟需系统分析以给出更稳健的初始化策略。

Method: 以主成分作为LoRA初始化，比较对不同组件的微调（中间组件、首组件PiSSA、尾组件MiLoRA）的性能和遗忘表现，分析学习率对鲁棒性的影响；提出一个基于分析结果的实用初始化策略；在多领域数据集（CV与NLP）和持续学习场景中进行广泛实验。

Result: 中间组件的微调在准确性与对高学习率鲁棒性之间实现更好平衡；相对于首尾组件的方法，所提初始化策略在遗忘方面表现更优；提出的LoRA初始化方法在多任务多领域实验中提升准确性并降低遗忘，包括持续学习场景。

Conclusion: 基于主成分的中间组件LoRA初始化策略提供了对性能与遗忘之间更稳健的权衡，适用于CV及NLP任务，并且对持续学习具有实用价值。

Abstract: Low-Rank Adaptation (LoRA) methods have emerged as crucial techniques for adapting large pre-trained models to downstream tasks under computational and memory constraints. However, they face a fundamental challenge in balancing task-specific performance gains against catastrophic forgetting of pre-trained knowledge, where existing methods provide inconsistent recommendations. This paper presents a comprehensive analysis of the performance-forgetting trade-offs inherent in low-rank adaptation using principal components as initialization. Our investigation reveals that fine-tuning intermediate components leads to better balance and show more robustness to high learning rates than first (PiSSA) and last (MiLoRA) components in existing work. Building on these findings, we provide a practical approach for initialization of LoRA that offers superior trade-offs. We demonstrate in a thorough empirical study on a variety of computer vision and NLP tasks that our approach improves accuracy and reduces forgetting, also in continual learning scenarios.

</details>


### [266] [EVE: Efficient Verification of Data Erasure through Customized Perturbation in Approximate Unlearning](https://arxiv.org/abs/2602.03567)
*Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Luoyu Chen,Shui Yu*

Main category: cs.LG

TL;DR: 提出一种无需参与初始训练即可验证机器删除的高效方法EVE，通过对待删数据的扰动来促使预测在未删前后发生变化，将该变化作为验证信号。扰动通过对抗优化生成，目标是使未删数据的“未删效果”与目标样本的边界变化梯度对齐。实验表明EVE在效率与准确性方面优于基于后门的验证方法，且不依赖初始训练阶段。源代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习“遗忘”验证多依赖于回溯性后门方法，需要参与模型初始训练以植入后门，实际应用中成本高、不可行。需提出不依赖初始训练的高效验证方案。

Method: 将扰动设计为对抗性优化问题，目标是同时确保模型对目标样本的未删效果以及改变被未删样本的预测。具体做法是使未删数据的未删梯度与目标样本边界变化梯度对齐，从而生成能可靠验证未删的扰动。

Result: 在大量实验中，EVE在无需参与初始训练的前提下实现了对机器未删的有效验证，且相比基于后门的方案在速度和验证准确性上具显著优势。

Conclusion: 提出并开源EVE工具，为机器 unlearning 验证提供高效且无需初始训练参与的新思路，具有较强的实用性与推广潜力。

Abstract: Verifying whether the machine unlearning process has been properly executed is critical but remains underexplored. Some existing approaches propose unlearning verification methods based on backdooring techniques. However, these methods typically require participation in the model's initial training phase to backdoor the model for later verification, which is inefficient and impractical. In this paper, we propose an efficient verification of erasure method (EVE) for verifying machine unlearning without requiring involvement in the model's initial training process. The core idea is to perturb the unlearning data to ensure the model prediction of the specified samples will change before and after unlearning with perturbed data. The unlearning users can leverage the observation of the changes as a verification signal. Specifically, the perturbations are designed with two key objectives: ensuring the unlearning effect and altering the unlearned model's prediction of target samples. We formalize the perturbation generation as an adversarial optimization problem, solving it by aligning the unlearning gradient with the gradient of boundary change for target samples. We conducted extensive experiments, and the results show that EVE can verify machine unlearning without involving the model's initial training process, unlike backdoor-based methods. Moreover, EVE significantly outperforms state-of-the-art unlearning verification methods, offering significant speedup in efficiency while enhancing verification accuracy. The source code of EVE is released at \uline{https://anonymous.4open.science/r/EVE-C143}, providing a novel tool for verification of machine unlearning.

</details>


### [267] [A Function-Space Stability Boundary for Generalization in Interpolating Learning Systems](https://arxiv.org/abs/2602.03514)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出对训练轨迹上的单样本扰动的收缩传播条件与稳定性证据，揭示稳定性在某些情境下能解释泛化，但并非普遍原因；实验表明证据增长与不同优化器、步长、数据集扰动下的泛化差异相关。


<details>
  <summary>Details</summary>
Motivation: 理解为何在插值训练中，算法稳定性是否能够解释泛化，以及在何种条件下稳定性是主要驱动因素。

Method: 将训练建模为函数空间轨迹，沿轨迹测量对单样本扰动的敏感性；提出收缩传播条件，并通过展开递推得到稳定性证据。

Result: 小证据量（certificate）意味着基于稳定性的泛化；存在即使风险很小也不满足收缩敏感性的插值情形，证明稳定性并非普适解释；实验证实证据增长可预测优化器、步长、数据集扰动等因素导致的泛化差异。

Conclusion: 该框架识别稳定性能够解释泛化的情景和需要其他机制的情景，指向多机制共存的理解路径。

Abstract: Modern learning systems often interpolate training data while still generalizing well, yet it remains unclear when algorithmic stability explains this behavior. We model training as a function-space trajectory and measure sensitivity to single-sample perturbations along this trajectory.
  We propose a contractive propagation condition and a stability certificate obtained by unrolling the resulting recursion. A small certificate implies stability-based generalization, while we also prove that there exist interpolating regimes with small risk where such contractive sensitivity cannot hold, showing that stability is not a universal explanation.
  Experiments confirm that certificate growth predicts generalization differences across optimizers, step sizes, and dataset perturbations. The framework therefore identifies regimes where stability explains generalization and where alternative mechanisms must account for success.

</details>


### [268] [APEX: Probing Neural Networks via Activation Perturbation](https://arxiv.org/abs/2602.03586)
*Tao Ren,Xiaoyu Luo,Qiongxiu Li*

Main category: cs.LG

TL;DR: APEX introduces an inference-time activation perturbation method to probe hidden representations while keeping inputs and model parameters fixed, showing a principled transition from sample-dependent to model-dependent behavior; it subsumes input perturbation as a constrained case and reveals both sample- and model-level insights across small- and large-noise regimes.


<details>
  <summary>Details</summary>
Motivation: Existing probing methods rely on input-space analysis or parameter perturbations, which poorly access the structural information encoded in intermediate representations. APEX aims to directly interrogate hidden activations to reveal representation-level structure.

Method: Perturb hidden activations at inference while fixing inputs and parameters; provide theoretical analysis showing suppression of input-specific signals and amplification of representation-level structure, with input perturbation framed as a constrained special case; validate via representative case studies across small- and large-noise regimes.

Result: In the small-noise regime, APEX yields a lightweight measure of sample regularity that aligns with established metrics, distinguishes structured from randomly labeled models, and reveals semantically coherent prediction transitions. In the large-noise regime, it uncovers training-induced model biases, including pronounced concentration of predictions on the target class in backdoored models.

Conclusion: APEX offers an effective, efficient probing paradigm to explore neural networks beyond input space by accessing intermediate representations, bridging the gap between sample- and model-level insights and exposing training-induced biases.

Abstract: Prior work on probing neural networks primarily relies on input-space analysis or parameter perturbation, both of which face fundamental limitations in accessing structural information encoded in intermediate representations. We introduce Activation Perturbation for EXploration (APEX), an inference-time probing paradigm that perturbs hidden activations while keeping both inputs and model parameters fixed. We theoretically show that activation perturbation induces a principled transition from sample-dependent to model-dependent behavior by suppressing input-specific signals and amplifying representation-level structure, and further establish that input perturbation corresponds to a constrained special case of this framework. Through representative case studies, we demonstrate the practical advantages of APEX. In the small-noise regime, APEX provides a lightweight and efficient measure of sample regularity that aligns with established metrics, while also distinguishing structured from randomly labeled models and revealing semantically coherent prediction transitions. In the large-noise regime, APEX exposes training-induced model-level biases, including a pronounced concentration of predictions on the target class in backdoored models. Overall, our results show that APEX offers an effective perspective for exploring, and understanding neural networks beyond what is accessible from input space alone.

</details>


### [269] [Rank-Learner: Orthogonal Ranking of Treatment Effects](https://arxiv.org/abs/2602.03517)
*Henri Arno,Dennis Frauen,Emil Javurek,Thomas Demeester,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: A two-stage, Neyman-orthogonal, model-agnostic learner (Rank-Learner) directly ranks individuals by treatment effects from observational data, without estimating precise CATEs, and shows robust performance against nuisance estimation errors.


<details>
  <summary>Details</summary>
Motivation: Many decision tasks require ordering individuals by their treatment effects rather than predicting exact effect sizes. Existing causal effect estimation focuses on magnitudes and may be inefficient for ranking. A direct ranking approach can be more effective and robust.

Method: Rank-Learner uses a two-stage approach with a pairwise learning objective to recover the true treatment effect ordering. It avoids explicit CATE estimation, is Neyman-orthogonal to reduce sensitivity to nuisance function errors, and is model-agnostic, allowing arbitrary base learners (e.g., neural networks).

Result: Empirical experiments show Rank-Learner consistently outperforms standard CATE estimators and non-orthogonal ranking methods, across various settings.

Conclusion: Rank-Learner provides practitioners with an effective, robust, orthogonal method for ranking individuals by treatment effects, applicable with diverse models.

Abstract: Many decision-making problems require ranking individuals by their treatment effects rather than estimating the exact effect magnitudes. Examples include prioritizing patients for preventive care interventions, or ranking customers by the expected incremental impact of an advertisement. Surprisingly, while causal effect estimation has received substantial attention in the literature, the problem of directly learning rankings of treatment effects has largely remained unexplored. In this paper, we introduce Rank-Learner, a novel two-stage learner that directly learns the ranking of treatment effects from observational data. We first show that naive approaches based on precise treatment effect estimation solve a harder problem than necessary for ranking, while our Rank-Learner optimizes a pairwise learning objective that recovers the true treatment effect ordering, without explicit CATE estimation. We further show that our Rank-Learner is Neyman-orthogonal and thus comes with strong theoretical guarantees, including robustness to estimation errors in the nuisance functions. In addition, our Rank-Learner is model-agnostic, and can be instantiated with arbitrary machine learning models (e.g., neural networks). We demonstrate the effectiveness of our method through extensive experiments where Rank-Learner consistently outperforms standard CATE estimators and non-orthogonal ranking methods. Overall, we provide practitioners with a new, orthogonal two-stage learner for ranking individuals by their treatment effects.

</details>


### [270] [ContraLog: Log File Anomaly Detection with Contrastive Learning and Masked Language Modeling](https://arxiv.org/abs/2602.03678)
*Simon Dietz,Kai Klede,An Nguyen,Bjoern M Eskofier*

Main category: cs.LG

TL;DR: ContraLog: 无解析器、自监督的日志异常检测方法，通过对连续消息嵌入的预测实现异常检测，结合消息编码器与序列编码器，采用掩蔽语言模型和对比学习；在 HDFS、BGL、Thunderbird 上表现良好，且单条消息的嵌入就具异常预测力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖日志解析器将日志归纳为模板，丢失变量和语义，导致对复杂、多样化日志的检测性能受限；需要一种不依赖模板、保留语义信息的表示学习方法，提升鲁棒性与泛化。

Method: 提出一个两分支模型：消息编码器生成每条日志的高维嵌入；序列编码器捕捉日志序列的时序依赖。训练目标由掩蔽语言建模（预测被掩蔽嵌入）和对比学习组成，以自监督方式学习富含语义的嵌入。无需模板化拆分，能直接用于异常检测。

Result: 在 HDFS、BGL、Thunderbird 数据集上实验证明 ContraLog 对复杂日志具有优越性；同时，单条消息的嵌入就能预测异常，即使缺失序列上下文，表明嵌入层具有可迁移的异常信号。

Conclusion: 嵌入级别的预测提供了一种通用的日志异常检测范式，潜在可扩展到其他事件序列分析任务。

Abstract: Log files record computational events that reflect system state and behavior, making them a primary source of operational insights in modern computer systems. Automated anomaly detection on logs is therefore critical, yet most established methods rely on log parsers that collapse messages into discrete templates, discarding variable values and semantic content. We propose ContraLog, a parser-free and self-supervised method that reframes log anomaly detection as predicting continuous message embeddings rather than discrete template IDs. ContraLog combines a message encoder that produces rich embeddings for individual log messages with a sequence encoder to model temporal dependencies within sequences. The model is trained with a combination of masked language modeling and contrastive learning to predict masked message embeddings based on the surrounding context. Experiments on the HDFS, BGL, and Thunderbird benchmark datasets empirically demonstrate effectiveness on complex datasets with diverse log messages. Additionally, we find that message embeddings generated by ContraLog carry meaningful information and are predictive of anomalies even without sequence context. These results highlight embedding-level prediction as an approach for log anomaly detection, with potential applicability to other event sequences.

</details>


### [271] [Sparse Training of Neural Networks based on Multilevel Mirror Descent](https://arxiv.org/abs/2602.03535)
*Yannick Lunk,Sebastian J. Scott,Leon Bungert*

Main category: cs.LG

TL;DR: A dynamic sparse training method that combines linearized Bregman iterations/mirror descent with adaptive sparsity freezing, achieving highly sparse yet accurate models and substantial FLOP reductions.


<details>
  <summary>Details</summary>
Motivation: Reduce computational cost of training sparse neural networks while preserving accuracy by exploiting inherent sparsity and enabling efficient exploration of sparse parameter space through a novel combination of Bregman iterations and adaptive structure freezing.

Method: Alternates between static and dynamic sparsity pattern updates, embeds sparsity-inducing linearized Bregman iterations into a multilevel optimization framework for convergence guarantees, and employs adaptive freezing of network connections to guide sparse exploration.

Result: Empirically yields highly sparse models with maintained accuracy on standard benchmarks; theoretical FLOPs reduction compared to SGD-based training using standard Bregman iterations drops from 38% to 6% while preserving test accuracy.

Conclusion: The proposed approach provides convergence guarantees within a multilevel framework and delivers practical gains: strong sparsity with minimal loss in performance and substantial compute efficiency.

Abstract: We introduce a dynamic sparse training algorithm based on linearized Bregman iterations / mirror descent that exploits the naturally incurred sparsity by alternating between periods of static and dynamic sparsity pattern updates. The key idea is to combine sparsity-inducing Bregman iterations with adaptive freezing of the network structure to enable efficient exploration of the sparse parameter space while maintaining sparsity. We provide convergence guaranties by embedding our method in a multilevel optimization framework. Furthermore, we empirically show that our algorithm can produce highly sparse and accurate models on standard benchmarks. We also show that the theoretical number of FLOPs compared to SGD training can be reduced from 38% for standard Bregman iterations to 6% for our method while maintaining test accuracy.

</details>


### [272] [Universal One-third Time Scaling in Learning Peaked Distributions](https://arxiv.org/abs/2602.03685)
*Yizhou Liu,Ziming Liu,Cengiz Pehlevan,Jeff Gore*

Main category: cs.LG

TL;DR: 核心发现：softmax+跨熵在学习尖峰分布时导致损失和梯度以幂律方式收敛，时间幂指数为1/3，揭示了LLM训练成本高企的普遍瓶颈，并指向改进训练效率的新方向。


<details>
  <summary>Details</summary>
Motivation: LLM训练成本高、收敛速度慢且幂律性质的起源尚存争议。本文通过 toy 模型与真实LLM的系统分析，探究幂律行为的内在机制及其普适性。

Method: 通过对 toy 模型的系统分析以及对大型语言模型的经验评估，检视 softmax 与交叉熵在拟合 peaked（尖峰）分布时的行为，推导出幂律消失的损失与梯度，以及损失随时间的幂律缩减，得到普适指数1/3。

Result: 在学习尖峰分布时，softmax 与交叉熵会导致损失与梯度的幂律衰减，进而产生损失随时间的幂律时间尺度，幂指数为1/3；为神经规模化提供机械性解释，并指出通过缓解该瓶颈可提升LLM训练效率的潜在方向。

Conclusion: 该发现揭示了LLM训练中的基本瓶颈，提示需要重新审视输出层/损失设计或训练策略，以降低幂律收敛的影响并提升训练效益。

Abstract: Training large language models (LLMs) is computationally expensive, partly because the loss exhibits slow power-law convergence whose origin remains debatable. Through systematic analysis of toy models and empirical evaluation of LLMs, we show that this behavior can arise intrinsically from the use of softmax and cross-entropy. When learning peaked probability distributions, e.g., next-token distributions, these components yield power-law vanishing losses and gradients, creating a fundamental optimization bottleneck. This ultimately leads to power-law time scaling of the loss with a universal exponent of $1/3$. Our results provide a mechanistic explanation for observed neural scaling and suggest new directions for improving LLM training efficiency.

</details>


### [273] [MatGPTQ: Accurate and Efficient Post-Training Matryoshka Quantization](https://arxiv.org/abs/2602.03537)
*Maximilian Kleinegger,Elvir Crnčević,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出 MatGPTQ，一次性后训练实现多位宽量化，基于 MSB 切分的 MatQuant，能够在单一检查点覆盖多种精度与预算。


<details>
  <summary>Details</summary>
Motivation: 解决原始 MatQuant 依赖昂贵的量化感知训练（QAT）且缺乏开源和内核支持的问题，提升 PTQ 的可用性、速度与实用性。

Method: 将 Matryoshka 量化视为多精度优化目标，结合位切片和跨位误差补偿，在少量标定集上实现一枪（one-shot）优化；提出预算感知的分层位宽搜索；提供实现切片与混合精度执行的高效内核。

Result: 在标准大语言模型与基准上，保持高比特精度时的准确性，同时在低比特宽设置下显著提升推理性能；提出单-checkpoint、多精度部署的新 SOTA，且开源实现代码可用。

Conclusion: 通过一个主模型即可覆盖多种目标位宽和资源预算，显著提升 Matryoshka 风格量化的实用性与可部署性。

Abstract: Matryoshka Quantization (MatQuant) is a recent quantization approach showing that a single integer-quantized model can be served across multiple precisions, by slicing the most significant bits (MSB) at inference time. This enables a single checkpoint to cover a wide range of memory and latency budgets, but renders quantization much more challenging. In particular, the initial MatQuant relies on expensive quantization-aware training (QAT) variants, rather than fast one-shot post training quantization (PTQ), and lacks open-source and kernel support. We address all of these limitations by introducing Post-Training Matryoshka Quantization (MatGPTQ), a new PTQ pipeline that produces a single parent model jointly optimized for multiple target precisions in one-shot, based on a small calibration set. MatGPTQ casts Matryoshka quantization as a multi-precision objective with bit-slicing and cross-bit error compensation, resulting in an algorithm that produces a multi-bit-width, "sliceable" model in a single pass. We also incorporate a new budget-aware search for heterogeneous per-layer bit-witdhs and provide efficient kernels that implement slicing and mixed-precision execution. Across standard LLMs and benchmarks, MatGPTQ preserves high-bit accuracy while substantially improving performance at low-bit-witdh settings. Overall, we establish a new state of the art for Matryoshka-style post-training quantization and make single-checkpoint, multi-precision deployment open and practical. Code is available at https://github.com/IST-DASLab/MatGPTQ.

</details>


### [274] [QuAIL: Quality-Aware Inertial Learning for Robust Training under Data Corruption](https://arxiv.org/abs/2602.03686)
*Mattia Sabella,Alberto Archetti,Pietro Pinoli,Matteo Matteucci,Cinzia Cappiello*

Main category: cs.LG

TL;DR: QuAIL introduces a feature-modulation layer with a quality-aware proximal regularizer to handle column-level reliability in tabular data, improving robustness to corruption without per-sample repair or reweighting.


<details>
  <summary>Details</summary>
Motivation: In tabular datasets, data corruption is often non-uniform and documented only at the column level, limiting effectiveness of existing robustness and cleaning methods that rely on instance-level quality.

Method: Incorporate a learnable feature-modulation layer into models; constrain its updates via a quality-dependent proximal regularizer, enabling feature-wise adaptation proportional to reliability without data repair or sample reweighting.

Result: Across 50 classification/regression datasets, QuAIL improves average performance compared to neural baselines under random and value-dependent corruption, with strong robustness in low-data and bias-prone settings.

Conclusion: Utilizing feature reliability priors within optimization dynamics is a practical, effective approach for resilient learning on corrupted tabular data.

Abstract: Tabular machine learning systems are frequently trained on data affected by non-uniform corruption, including noisy measurements, missing entries, and feature-specific biases. In practice, these defects are often documented only through column-level reliability indicators rather than instance-wise quality annotations, limiting the applicability of many robustness and cleaning techniques. We present QuAIL, a quality-informed training mechanism that incorporates feature reliability priors directly into the learning process. QuAIL augments existing models with a learnable feature-modulation layer whose updates are selectively constrained by a quality-dependent proximal regularizer, thereby inducing controlled adaptation across features of varying trustworthiness. This stabilizes optimization under structured corruption without explicit data repair or sample-level reweighting. Empirical evaluation across 50 classification and regression datasets demonstrates that QuAIL consistently improves average performance over neural baselines under both random and value-dependent corruption, with especially robust behavior in low-data and systematically biased settings. These results suggest that incorporating feature reliability information directly into optimization dynamics is a practical and effective approach for resilient tabular learning.

</details>


### [275] [LLM-Inspired Pretrain-Then-Finetune for Small-Data, Large-Scale Optimization](https://arxiv.org/abs/2602.03690)
*Zishi Zhang,Jinhui Han,Ming Hu,Yijie Peng*

Main category: cs.LG

TL;DR: 提出一种预训练-微调的 Transformer 框架，用于在小样本、大规模决策问题中结合合成数据的领域知识与真实观测。理论上给出非渐近的误差界，表明预训练和微调共同决定性能，微调具有规模经济效应。


<details>
  <summary>Details</summary>
Motivation: 解决在一个实例只能获得少量且可能有噪声数据的场景下，需要在大量任务/产品组合中做出多项决策，同时受益于大语言模型的成功经验与跨任务结构的能力。

Method: 以 Transformer 为核心，设计面向决策环境的特定结构，并在大规模、领域相关的合成数据上进行预训练，再用真实观测数据对模型进行微调。通过针对性架构和训练策略实现跨任务结构的高效提取，伴随对 Transformer 学习的非渐近误差分析。

Result: 给出非渐近的误差保证，证实预训练和微调的协同作用；微调呈现规模经济效应，即当实例数量增加时转移学习更有效；预训练用于注入领域知识并使高容量模型可用，微调则对实际数据分布进行对齐。

Conclusion: 预训练与微调共同决定最终性能，且在实例增多时微调的作用更突出；该框架并非简单的现成应用，而是需要问题特定的架构设计与训练流程。

Abstract: We consider small-data, large-scale decision problems in which a firm must make many operational decisions simultaneously (e.g., across a large product portfolio) while observing only a few, potentially noisy, data points per instance. Inspired by the success of large language models (LLMs), we propose a pretrain-then-finetune approach built on a designed Transformer model to address this challenge. The model is first pretrained on large-scale, domain-informed synthetic data that encode managerial knowledge and structural features of the decision environment, and is then fine-tuned on real observations. This new pipeline offers two complementary advantages: pretraining injects domain knowledge into the learning process and enables the training of high-capacity models using abundant synthetic data, while finetuning adapts the pretrained model to the operational environment and improves alignment with the true data-generating regime. While we have leveraged the Transformer's state-of-the-art representational capacity, particularly its attention mechanism, to efficiently extract cross-task structure, our approach is not an off-the-shelf application. Instead, it relies on problem-specific architectural design and a tailored training procedure to match the decision setting. Theoretically, we develop the first comprehensive error analysis regarding Transformer learning in relevant contexts, establishing nonasymptotic guarantees that validate the method's effectiveness. Critically, our analysis reveals how pretraining and fine-tuning jointly determine performance, with the dominant contribution governed by whichever is more favorable. In particular, finetuning exhibits an economies-of-scale effect, whereby transfer learning becomes increasingly effective as the number of instances grows.

</details>


### [276] [CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting](https://arxiv.org/abs/2602.03564)
*Yaguo Liu,Mingyue Cheng,Daoyu Wang,Xiaoyu Tao,Qi Liu*

Main category: cs.LG

TL;DR: CoGenCast 将预训练解码器型大语言模型改造为用于时序 forecasting 的原生编码-解码骨架，并结合流式匹配机制来联合建模语义上下文和连续的随机动态，从而实现多模态与跨域统一训练，且在多项基准上优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 时序预测需要同时理解上下文语义与刻画连续的随机演化；现有方法多聚焦于自回归LLMs的语义建模或扩散类模型的概率生成，难以同时覆盖两者，亟需一个统一的、可扩展的框架。

Method: 通过仅修改注意力拓扑，将解码器-为基础的预训练LLM重构为原生的 forecasting 编码器-解码器骨架，实现双向上下文编码与因果表示生成；在此基础上引入流式匹配(flow-matching)机制，以条件化的连续动态来建模时间演化，且支持多模态 forecasting 与跨域统一训练。

Result: 在多个基准数据集上，CoGenCast 显示出对比基线的持续领先，验证了将语义理解与连续动力学结合的有效性与泛化性。

Conclusion: 提出的混合框架有效融合语义导向与随机动力学建模，具备良好的跨域适应性与扩展潜力，且代码公开。

Abstract: Time series forecasting can be viewed as a generative problem that requires both semantic understanding over contextual conditions and stochastic modeling of continuous temporal dynamics. Existing approaches typically rely on either autoregressive large language models (LLMs) for semantic context modeling or diffusion-like models for continuous probabilistic generation. However, neither method alone can adequately model both aspects simultaneously. In this work, we propose CoGenCast, a hybrid generative framework that couples pre-trained LLMs with flow-matching mechanism for effective time series forecasting. Specifically, we reconfigure pre-trained decoder-only LLMs into a native forecasting encoder-decoder backbone by modifying only the attention topology, enabling bidirectional context encoding and causal representation generation. Building on this, a flow-matching mechanism is further integrated to model temporal evolution, capturing continuous stochastic dynamics conditioned on the autoregressively generated representation. Notably, CoGenCast naturally supports multimodal forecasting and cross-domain unified training. Extensive experiments on multiple benchmarks show that CoGenCast consistently outperforms previous compared baselines. Code is available at https://github.com/liuyaguo/_CoGenCast.

</details>


### [277] [Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging](https://arxiv.org/abs/2602.03702)
*Alexandru Meterez,Pranav Ajit Nair,Depen Morwani,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: 提出在开放式训练时间里可用于大语言模型的 anytime 学习率调度，强调权重平均在达到最小化收敛率中的作用；通过理论证明以及在 150M、300M 参数的模型上的实证比较，表明与 cosine 衰减等效且实现简单。


<details>
  <summary>Details</summary>
Motivation: 在持续/开放式训练中，未知总训练轮次（horizon），现有预训练方案通常依赖与 horizon 相关的学习率调度和固定预算下的大量调参，缺乏 horizon 无关的稳健性。

Method: 对高参数线性回归的 anytime 学习率调度进行理论分析，揭示 weight averaging（模型合并）在实现 SGD 的 minimax 收敛率中的核心作用；给出随时间多项式衰减的 anytime 调度，其衰减速率由问题的源条件和容量条件决定。此外，在 150M 与 300M 参数的语言模型上开展实证评估，比较常数学习率+权重平均、1/√t 的学习率+权重平均，与经过良好调参的 cosine 调度的效果。

Result:  anytime 调度在训练整体区间表现出多项式时间衰减，最终损失与 cosine 衰减相当；权重平均的引入使无需 horizon 信息就能达到接近最优的收敛行为。实证结果表明，在不同规模的模型上，简单的、 horizon 无关的步长配置结合权重平均即可实现与分段 cosine 调度相近的最终表现。

Conclusion: 将权重平均与简单、不依赖训练时 horizon 的步长结合，作为大语言模型预训练的可行 anytime 替代方案，替代传统的 cosine 学习率调度，具备实用性与理论支撑。

Abstract: Large language models are increasingly trained in continual or open-ended settings, where the total training horizon is not known in advance. Despite this, most existing pretraining recipes are not anytime: they rely on horizon-dependent learning rate schedules and extensive tuning under a fixed compute budget. In this work, we provide a theoretical analysis demonstrating the existence of anytime learning schedules for overparameterized linear regression, and we highlight the central role of weight averaging - also known as model merging - in achieving the minimax convergence rates of stochastic gradient descent. We show that these anytime schedules polynomially decay with time, with the decay rate determined by the source and capacity conditions of the problem. Empirically, we evaluate 150M and 300M parameter language models trained at 1-32x Chinchilla scale, comparing constant learning rates with weight averaging and $1/\sqrt{t}$ schedules with weight averaging against a well-tuned cosine schedule. Across the full training range, the anytime schedules achieve comparable final loss to cosine decay. Taken together, our results suggest that weight averaging combined with simple, horizon-free step sizes offers a practical and effective anytime alternative to cosine learning rate schedules for large language model pretraining.

</details>


### [278] [Asymmetric Hierarchical Anchoring for Audio-Visual Joint Representation: Resolving Information Allocation Ambiguity for Robust Cross-Modal Generalization](https://arxiv.org/abs/2602.03570)
*Bixing Wu,Yuhong Zhao,Zongli Ye,Jiachen Lian,Xiangyu Yue,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: AHA introduces an asymmetric hierarchical anchoring framework for cross-modal generalization in audio-visual learning, leveraging RVQ-driven semantic anchors, a GRL-based adversarial decoupler, and Local Sliding Alignment to reduce leakage and improve temporal alignment, outperforming symmetric baselines on AVE/AVVP and showing stronger disentanglement.


<details>
  <summary>Details</summary>
Motivation: Symmetric CMG architectures suffer from information allocation ambiguity: without structural inductive bias, semantic leakage occurs across modalities, hindering transfer from labeled source to unlabeled target. A directional, structured representation is needed to enforce clean cross-modal semantics.

Method: Propose Asymmetric Hierarchical Anchoring (AHA) with a structured semantic anchor inside a shared hierarchy. Use audio Residual Vector Quantization (RVQ) to induce hierarchical discrete representations guiding video feature distillation into a unified semantic space. Replace mutual information estimators with a GRL-based adversarial decoupler to suppress modality-specific leakage. Introduce Local Sliding Alignment (LSA) for fine-grained temporal alignment across modalities.

Result: Experiments on AVE and AVVP show AHA consistently outperforms symmetric baselines in cross-modal transfer. Talking-face disentanglement analysis indicates improved semantic consistency and disentanglement, suggesting broader applicability of the framework.

Conclusion: Asymmetric Hierarchical Anchoring effectively mitigates information allocation ambiguity in CMG and leverages hierarchical discrete representations with adversarial decoupling and temporal alignment to achieve robust cross-modal transfer and better disentanglement.

Abstract: Audio-visual joint representation learning under Cross-Modal Generalization (CMG) aims to transfer knowledge from a labeled source modality to an unlabeled target modality through a unified discrete representation space. Existing symmetric frameworks often suffer from information allocation ambiguity, where the absence of structural inductive bias leads to semantic-specific leakage across modalities. We propose Asymmetric Hierarchical Anchoring (AHA), which enforces directional information allocation by designating a structured semantic anchor within a shared hierarchy. In our instantiation, we exploit the hierarchical discrete representations induced by audio Residual Vector Quantization (RVQ) to guide video feature distillation into a shared semantic space. To ensure representational purity, we replace fragile mutual information estimators with a GRL-based adversarial decoupler that explicitly suppresses semantic leakage in modality-specific branches, and introduce Local Sliding Alignment (LSA) to encourage fine-grained temporal alignment across modalities. Extensive experiments on AVE and AVVP benchmarks demonstrate that AHA consistently outperforms symmetric baselines in cross-modal transfer. Additional analyses on talking-face disentanglement experiment further validate that the learned representations exhibit improved semantic consistency and disentanglement, indicating the broader applicability of the proposed framework.

</details>


### [279] [UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining](https://arxiv.org/abs/2602.03772)
*Changhao Wang,Yunfei Yu,Xinhao Yao,Jiaolong Yang,Riccardo Cantoro,Chaobo Li,Qing Cui,Jun Zhou*

Main category: cs.LG

TL;DR: UniGeM unifies data mixing and selection for LLM data curation via manifold approximation, achieving 2x data efficiency on 8B/16B MoE models trained on 100B tokens.


<details>
  <summary>Details</summary>
Motivation: Data quality increasingly limits LLM scaling; existing approaches separate data mixing and sample selection, risking disruption of code corpus structure. A training-model-free, dataset-agnostic data curation method is needed.

Method: A hierarchical framework: Macro-Exploration learns mixing weights via stability-based clustering; Micro-Mining filters high-quality instances using their geometric distribution to preserve logical consistency, all without training proxy models or external reference datasets.

Result: Training 8B and 16B MoE models on 100B tokens shows UniGeM achieves ~2.0x data efficiency over a random baseline and yields performance gains over SOTA methods, especially in reasoning tasks and multilingual generalization.

Conclusion: Treating data curation as a manifold approximation problem enables data-efficient, structure-preserving selection and mixing, improving robustness and generalization without external references.

Abstract: The scaling of Large Language Models (LLMs) is increasingly limited by data quality. Most methods handle data mixing and sample selection separately, which can break the structure in code corpora. We introduce \textbf{UniGeM}, a framework that unifies mixing and selection by treating data curation as a \textit{manifold approximation} problem without training proxy models or relying on external reference datasets. UniGeM operates hierarchically: \textbf{Macro-Exploration} learns mixing weights with stability-based clustering; \textbf{Micro-Mining} filters high-quality instances by their geometric distribution to ensure logical consistency. Validated by training 8B and 16B MoE models on 100B tokens, UniGeM achieves \textbf{2.0$\times$ data efficiency} over a random baseline and further improves overall performance compared to SOTA methods in reasoning-heavy evaluations and multilingual generalization.

</details>


### [280] [Optimization and Generation in Aerodynamics Inverse Design](https://arxiv.org/abs/2602.03582)
*Huaguan Chen,Ning Lin,Luxi Chen,Rui Zhang,Wenbing Huang,Chongxuan Li,Hao Sun*

Main category: cs.LG

TL;DR: 将高维物理约束下的逆设计，转换为最优设计点与最优设计分布两种 canonical 视角，提出用于成本预测器的新训练损失与密度梯度优化，以在提升目标的同时保持合理形状；统一现有训练无关的引导生成方法，并给出一种时空高效的近似协方差估计算法以应对高维条件协方差的近似问题。在二维控制研究和三维高保真空气动力基准（车、飞机）上，通过 OpenFOAM 仿真和3D 打印原型的风洞实验验证，结果在优化与引导生成两方面均有提升；离线强化学习结果支持方法的普适性。


<details>
  <summary>Details</summary>
Motivation: 逆设计在物理目标下面临耦合的高维几何和高成本数值仿真挑战，亟需以更高效的视角来实现优化与有条件的生成，尤其要解决高维条件下协方差估计的困难并统一现有引导生成方法。

Method: 将逆设计拆解为最优设计点与最优设计分布两大 canonical 视角；提出用于成本预测器的新训练损失；引入密度梯度优化以在提升目标同时保留合理形状；统一已有的训练自由的引导生成方法；提出时空高效的近似协方差估计算法以解决高维条件协方差近似问题。

Result: 在二维受控研究与三维高保真空气动力基准（车辆、机翼/飞机）上，结合 OpenFOAM 仿真与3D 打印原型的风洞测试，方法在优化与引导生成两方面均表现出稳定提升；额外的离线强化学习结果也支持其普适性。

Conclusion: 该工作提供了一个通用、高效且有效的逆设计框架，既能处理物理目标下的高维耦合，又能通过对设计分布/协方差的处理实现稳健的生成与优化，并在仿真与实验中得到一致性提升。

Abstract: Inverse design with physics-based objectives is challenging because it couples high-dimensional geometry with expensive simulations, as exemplified by aerodynamic shape optimization for drag reduction. We revisit inverse design through two canonical solutions, the optimal design point and the optimal design distribution, and relate them to optimization and guided generation. Building on this view, we propose a new training loss for cost predictors and a density-gradient optimization method that improves objectives while preserving plausible shapes. We further unify existing training-free guided generation methods. To address their inability to approximate conditional covariance in high dimensions, we develop a time- and memory-efficient algorithm for approximate covariance estimation. Experiments on a controlled 2D study and high-fidelity 3D aerodynamic benchmarks (car and aircraft), validated by OpenFOAM simulations and miniature wind-tunnel tests with 3D-printed prototypes, demonstrate consistent gains in both optimization and guided generation. Additional offline RL results further support the generality of our approach.

</details>


### [281] [Reward Redistribution for CVaR MDPs using a Bellman Operator on L-infinity](https://arxiv.org/abs/2602.03778)
*Aneri Muni,Vincent Taboga,Esther Derman,Pierre-Luc Bacon,Erick Delage*

Main category: cs.LG

TL;DR: New augmentation-based formulation of static CVaR in reinforcement learning that yields a Bellman operator with dense per-step rewards and contraction on the full space of bounded value functions. Enables risk-averse value iteration and Q-learning with discretized augmented states, with convergence and discretization error guarantees. Empirical results show CVaR-sensitive policies and favorable performance-safety trade-offs.


<details>
  <summary>Details</summary>
Motivation: Tail-end risk measures like CVaR are important for safety-critical applications but static CVaR depends on entire trajectories, hindering recursive Bellman decomposition. Traditional state augmentation can produce sparse rewards and degenerate fixed points, limiting learning. A reformulation aims to restore a standard RL structure with stable convergence properties.

Method: Propose an augmentation-based static CVaR objective. Derive a Bellman operator with dense per-step rewards and contraction on the full bounded-value-function space. Develop risk-averse value iteration and model-free Q-learning algorithms using discretized augmented states. Provide convergence guarantees and bounds on discretization error.

Result: Established a solid theoretical foundation for CVaR-aware RL with a contraction Bellman operator. Delivered practical algorithms (risk-averse VI and Q-learning) that operate on discretized augmented states, with proven convergence and error bounds. Empirical results demonstrate learning of CVaR-sensitive policies and effective interplay between performance and safety constraints.

Conclusion: The augmentation-based formulation enables stable, tractable optimization of static CVaR in MDPs, improving learning stability and yielding policies that balance performance with tail-risk constraints in safety-critical settings.

Abstract: Tail-end risk measures such as static conditional value-at-risk (CVaR) are used in safety-critical applications to prevent rare, yet catastrophic events. Unlike risk-neutral objectives, the static CVaR of the return depends on entire trajectories without admitting a recursive Bellman decomposition in the underlying Markov decision process. A classical resolution relies on state augmentation with a continuous variable. However, unless restricted to a specialized class of admissible value functions, this formulation induces sparse rewards and degenerate fixed points. In this work, we propose a novel formulation of the static CVaR objective based on augmentation. Our alternative approach leads to a Bellman operator with: (1) dense per-step rewards; (2) contracting properties on the full space of bounded value functions. Building on this theoretical foundation, we develop risk-averse value iteration and model-free Q-learning algorithms that rely on discretized augmented states. We further provide convergence guarantees and approximation error bounds due to discretization. Empirical results demonstrate that our algorithms successfully learn CVaR-sensitive policies and achieve effective performance-safety trade-offs.

</details>


### [282] [SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network](https://arxiv.org/abs/2602.03596)
*Cristian Manca,Christian Scano,Giorgio Piras,Fabio Brau,Maura Pintor,Battista Biggio*

Main category: cs.LG

TL;DR: 提出 SAGE-5GC 指南以在 5G 核网中安全地评估异常检测器，强调在野外场景下对抗对手扰动的鲁棒性；引入基于对抗性扰动的评估框架及仅作用于攻击者可控特征的遗传算法攻击策略，发现对抗性攻击显著降低检测性能，凸显需鲁棒且安全感知的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的 5G 异常检测往往在 IID 假设与无对手自适应攻击的理想环境下评估，难以反映真实运营条件，易被对抗样本欺骗，亟需面向现实部署的安全感知评估框架。

Method: 在真实 5G Core 数据集上训练多种异常检测器并评估基线性能；在可控特征空间进行随机扰动以分析模型敏感性与对抗鲁棒性；提出一种基于遗传算法的攻击者优化策略，仅操作攻击者可控特征且不需了解检测模型内部结构。

Result: 对抗性扰动可显著降低检测性能，证明现有检测在野外部署中易被攻击；实验结果支持将安全感知评估与对抗鲁棒性分析作为 5G 异常检测部署的关键环节。

Conclusion: 需要建立以安全感知为导向的评估框架并发展对抗鲁棒的检测方法，利用如 SAGE-5GC 之类的综合指南在实际部署前进行系统化评估和鲁棒性提升。

Abstract: Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive attackers.In this work, we study the problem of detecting 5G attacks \textit{in the wild}, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network services.We then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.

</details>


### [283] [Ultra Fast PDE Solving via Physics Guided Few-step Diffusion](https://arxiv.org/abs/2602.03627)
*Cindy Xiangrui Kong,Yueqi Wang,Haoyang Zheng,Weijian Luo,Guang Lin*

Main category: cs.LG

TL;DR: 提出 Phys-Instruct，基于物理引导的蒸馏框架，用少步生成器替代扩散求解器并注入 PDE 知识以提升物理一致性；在五个 PDE 基准上实现数量级加速和误差显著降低，同时产生可用于下游条件任务的紧凑先验。


<details>
  <summary>Details</summary>
Motivation: 扩散 PDE 求解在采样成本高、物理一致性不足方面存在瓶颈；需要同时提升推断速度和物理约束性。

Method: 将预训练扩散 PDE 求解器蒸馏为少步生成器，通过匹配生成分布与先验扩散分布实现快速采样；引入 PDE 蒸馏引导，将 PDE 知识显式注入，提出可求梯度的物理约束训练目标；理论基础支撑。

Result: 五个 PDE 基准上实现数量级加速的推断并将 PDE 误差相对 SOTA 降低超过 8 倍；无条件学生模型可作为紧凑先验，支持下游条件任务的高效且物理一致的推断。

Conclusion: Phys-Instruct 是一种新颖、有效且高效的超快速 PDE 求解框架，依托深度生成模型实现物理约束。

Abstract: Diffusion-based models have demonstrated impressive accuracy and generalization in solving partial differential equations (PDEs). However, they still face significant limitations, such as high sampling costs and insufficient physical consistency, stemming from their many-step iterative sampling mechanism and lack of explicit physics constraints. To address these issues, we propose Phys-Instruct, a novel physics-guided distillation framework which not only (1) compresses a pre-trained diffusion PDE solver into a few-step generator via matching generator and prior diffusion distributions to enable rapid sampling, but also (2) enhances the physics consistency by explicitly injecting PDE knowledge through a PDE distillation guidance. Physic-Instruct is built upon a solid theoretical foundation, leading to a practical physics-constrained training objective that admits tractable gradients. Across five PDE benchmarks, Phys-Instruct achieves orders-of-magnitude faster inference while reducing PDE error by more than 8 times compared to state-of-the-art diffusion baselines. Moreover, the resulting unconditional student model functions as a compact prior, enabling efficient and physically consistent inference for various downstream conditional tasks. Our results indicate that Phys-Instruct is a novel, effective, and efficient framework for ultra-fast PDE solving powered by deep generative models.

</details>


### [284] [CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets](https://arxiv.org/abs/2602.03641)
*Milosh Devic,Jordan Gierschendorf,David Garson*

Main category: cs.LG

TL;DR: 提出 CTTVAE+TBS，用于在严重类别不平衡下生成具有高下游效用的表格数据，通过条件变换器VAE、类感知三元组边距损失和按需采样策略提升少数类样本的代表性与下游性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，罕见但高影响事件驱动决策，需要对少数群体生成具有良好下游效用的合成表格数据；现有生成模型往往忽视少数类或产出样本质量不足，限制了模型的下游性能。

Method: CTTVAE：基于条件Transformer的表格变分自编码器；引入类感知的三元组边距损失以加强潜在空间的类内紧致与类间分离；引入训练-按采样(TBS)策略，动态提高对欠代表群体的训练暴露。两者结合形成 CTTVAE+TBS 框架。

Result: 在六个真实基准数据集上，CTTVAE+TBS 在少数类别的下游任务上表现最强，常超越在原始不平衡数据上训练的模型，同时保持竞争力的数据保真度，并缩小了插值法与深度生成方法在隐私与效果之间的差距。消融研究表明潜在结构化与定向采样对改进具有关键作用。

Conclusion: 通过直接以下游性能为导向来优化少数类别的样本生成，CTTVAE+TBS 提供一种健壮、可解释的条件表格数据生成方案，具备在医疗、欺诈检测、预测性维护等领域实现微小改进即产生重大影响的潜力。

Abstract: Generating synthetic tabular data under severe class imbalance is essential for domains where rare but high-impact events drive decision-making. However, most generative models either overlook minority groups or fail to produce samples that are useful for downstream learning. We introduce CTTVAE, a Conditional Transformer-based Tabular Variational Autoencoder equipped with two complementary mechanisms: (i) a class-aware triplet margin loss that restructures the latent space for sharper intra-class compactness and inter-class separation, and (ii) a training-by-sampling strategy that adaptively increases exposure to underrepresented groups. Together, these components form CTTVAE+TBS, a framework that consistently yields more representative and utility-aligned samples without destabilizing training. Across six real-world benchmarks, CTTVAE+TBS achieves the strongest downstream utility on minority classes, often surpassing models trained on the original imbalanced data while maintaining competitive fidelity and bridging the gap for privacy for interpolation-based sampling methods and deep generative methods. Ablation studies further confirm that both latent structuring and targeted sampling contribute to these gains. By explicitly prioritizing downstream performance in rare categories, CTTVAE+TBS provides a robust and interpretable solution for conditional tabular data generation, with direct applicability to industries such as healthcare, fraud detection, and predictive maintenance where even small gains in minority cases can be critical.

</details>


### [285] [PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning](https://arxiv.org/abs/2602.03846)
*Romain Cosentino*

Main category: cs.LG

TL;DR: 提出 PLATE：一种在没有旧任务数据的情况下对预训练模型进行持续学习的方法，利用几何冗余构造低秩更新，冻结 B、Q，仅训练 A，以实现可控的塑性-保持权衡。


<details>
  <summary>Details</summary>
Motivation: 在基金会模型微调/持续学习场景中，往往无法获取或存取旧任务数据，传统方法需 revisiting old data 以防止灾难性遗忘，亟需无需旧数据即可实现有效适应的方法。

Method: 将每层表示为结构化的低秩更新 ΔW = B A Q^T，其中 B 和 Q 从预训练权重中一次性计算并固定，只有 A 在新任务上训练。B/Q 提供从预训练期的特征方向的近似保护子空间，A 的可训练部分受限以控制更新自由度，从而降低对旧数据分布的功能漂移。

Result: 提出的方法 PLATE 在无需过去任务数据的前提下，给出可控的塑性-保留权衡，并据 claim 提升对旧数据分布的最坏情况保留性，代码公开：GitHub 链接。

Conclusion: 通过利用预训练权重中的几何冗余，PLATE 提供了一种参数化的、可控的持续学习框架，适合需要无旧数据访问的场景，并且以结构化低秩更新实现对塑性与保留之间权衡的显式控制。

Abstract: We develop a continual learning method for pretrained models that \emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, redundant neurons provide a proxy for dominant pretraining-era feature directions, enabling the construction of approximately protected update subspaces directly from pretrained weights. Second, redundancy offers a natural bias for \emph{where} to place plasticity: by restricting updates to a subset of redundant neurons and constraining the remaining degrees of freedom, we obtain update families with reduced functional drift on the old-data distribution and improved worst-case retention guarantees. These insights lead to \textsc{PLATE} (\textbf{Pla}sticity-\textbf{T}unable \textbf{E}fficient Adapters), a continual learning method requiring no past-task data that provides explicit control over the plasticity-retention trade-off. PLATE parameterizes each layer with a structured low-rank update $ΔW = B A Q^\top$, where $B$ and $Q$ are computed once from pretrained weights and kept frozen, and only $A$ is trained on the new task. The code is available at https://github.com/SalesforceAIResearch/PLATE.

</details>


### [286] [Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG](https://arxiv.org/abs/2602.03645)
*Yicheng Zhang,Zhen Qin,Zhaomin Wu,Wenqi Zhang,Shuiguang Deng*

Main category: cs.LG

TL;DR: 基于强化学习的检索器优化，采用随机采样替代确定性检索，将RAG建模为马尔可夫决策过程，并在每步检索中将检索历史作为状态以缓解状态别名，从而在多种RAG管线和数据集上带来性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决RAG中检索器优化目标与RAG整体目标之间的目标错配，以及在应用RL于检索器时面临的两大挑战：一方面，确定性检索无法直接融入RL框架；另一方面，多跳推理中仅基查询的状态导致状态混淆（状态别名）。

Method: 将检索过程改为随机采样以引入鲁棒性和可优化的策略梯度信号；将RAG建模为马尔可夫决策过程（MDP），使检索器可通过RL进行优化；在每一步检索中把检索历史纳入状态以缓解状态别名问题。

Result: 在多种RAG管线、数据集和检索器规模下，实验显示该方法对RAG性能有一致提升。

Conclusion: 通过引入随机采样、MDP建模和检索历史的状态设计，RL优化的检索器能够缓解目标错配与状态别名问题，提升RAG整体性能，具有良好的泛化性。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to produce evidence-based responses, and its performance hinges on the matching between the retriever and LLMs. Retriever optimization has emerged as an efficient alternative to fine-tuning LLMs. However, existing solutions suffer from objective mismatch between retriever optimization and the goal of RAG pipeline. Reinforcement learning (RL) provides a promising solution to address this limitation, yet applying RL to retriever optimization introduces two fundamental challenges: 1) the deterministic retrieval is incompatible with RL formulations, and 2) state aliasing arises from query-only retrieval in multi-hop reasoning. To address these challenges, we replace deterministic retrieval with stochastic sampling and formulate RAG as a Markov decision process, making retriever optimizable by RL. Further, we incorporate retrieval history into the state at each retrieval step to mitigate state aliasing. Extensive experiments across diverse RAG pipelines, datasets, and retriever scales demonstrate consistent improvements of our approach in RAG performance.

</details>


### [287] [Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization](https://arxiv.org/abs/2602.03729)
*Henrik Schopmans,Christopher von Klitzing,Pascal Friederich*

Main category: cs.LG

TL;DR: 提出离策略对数分散正则化（LDR）以提升Boltzmann生成器的数据效率，在不需要额外的 on-policy 样本情况下，利用目标能标签对能量景观进行形状正则化，适用于无偏/有偏数据以及变分训练，实现显著的样本效率提升，最高达一个数量级。


<details>
  <summary>Details</summary>
Motivation: 在未归一化概率密度的抽样中，获取高质量训练数据与高效评估代价昂贵。Boltzmann生成器虽具独立采样能力，但其实际效果受限于数据效率。需要一种能够在保持与数据无关的前提下利用能量信息的正则化方法，以改善能量景观的形状并提升训练效率。

Method: 提出通用化的对数方差目标的扩展形式——LDR，并在离策略(off-policy)训练设置中使用，作为能量景观的形状正则器。LDR 能利用目标能量标签的信息，同时兼容无偏或有偏的仿真数据，以及在没有目标样本的情况下进行变分训练。

Result: 在所有基准测试中，LDR 提高了最终性能和数据效率，样本效率提升可达到一个数量级。

Conclusion: LDR 为从未归一化密度抽样的正则化提供了一种广泛适用的方法，能够在多种训练设置与数据条件下提升 Boltzmann 生成器的数据效率与样本质量。

Abstract: Sampling from unnormalized probability densities is a central challenge in computational science. Boltzmann generators are generative models that enable independent sampling from the Boltzmann distribution of physical systems at a given temperature. However, their practical success depends on data-efficient training, as both simulation data and target energy evaluations are costly. To this end, we propose off-policy log-dispersion regularization (LDR), a novel regularization framework that builds on a generalization of the log-variance objective. We apply LDR in the off-policy setting in combination with standard data-based training objectives, without requiring additional on-policy samples. LDR acts as a shape regularizer of the energy landscape by leveraging additional information in the form of target energy labels. The proposed regularization framework is broadly applicable, supporting unbiased or biased simulation datasets as well as purely variational training without access to target samples. Across all benchmarks, LDR improves both final performance and data efficiency, with sample efficiency gains of up to one order of magnitude.

</details>


### [288] [Fast-MWEM: Private Data Release in Sublinear Time](https://arxiv.org/abs/2602.03732)
*Themistoklis Haris,Steve Choi,Mutiraj Laksanawisit*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Multiplicative Weights Exponential Mechanism (MWEM) is a fundamental iterative framework for private data analysis, with broad applications such as answering $m$ linear queries, or privately solving systems of $m$ linear constraints. However, a critical bottleneck hindering its scalability is the $Θ(m)$ time complexity required to execute the exponential mechanism in each iteration. We introduce a modification to the MWEM framework that improves the per-iteration runtime dependency to $Θ(\sqrt{m})$ in expectation. This is done via a lazy sampling approach to the Report-Noisy-Max mechanism, which we implement efficiently using Gumbel noise and a $k$-Nearest Neighbor data structure. This allows for the rapid selection of the approximate score in the exponential mechanism without an exhaustive linear scan. We apply our accelerated framework to the problems of private linear query release and solving Linear Programs (LPs) under neighboring constraint conditions and low-sensitivity assumptions. Experimental evaluation confirms that our method provides a substantial runtime improvement over classic MWEM.

</details>


### [289] [Reasoning with Latent Tokens in Diffusion Language Models](https://arxiv.org/abs/2602.03769)
*Andre He,Sean Welleck,Daniel Fried*

Main category: cs.LG

TL;DR: Latent tokens in discrete diffusion models enable a controllable tradeoff between inference speed and sample quality for language tasks requiring planning and global coherence; ablations show joint prediction of all tokens is crucial, and latent tokens improve performance; the idea extends to autoregressive models via a multi-token objective, suggesting latent tokens as a general mechanism for lookahead.


<details>
  <summary>Details</summary>
Motivation: Understand why diffusion-based language models incur higher inference cost and whether introducing latent tokens can improve efficiency without sacrificing reasoning ability; explore whether latent tokens generalize beyond diffusion to autoregressive architectures.

Method: 1) Ablation of joint prediction over all unknown tokens to assess its impact on speed and performance. 2) Introduce a mechanism to modulate the number of latent tokens and evaluate its effect on speed vs. quality. 3) Extend the latent-token idea to autoregressive models via an auxiliary multi-token prediction objective and evaluate on reasoning tasks.

Result: Joint prediction across undecoded tokens is essential for maintaining quality; reducing this joint prediction speeds up inference but degrades performance, indicating latent tokens carry useful global information. Controlling the number of latent tokens provides a smooth speed-quality tradeoff. Applying latent-token concept to autoregressive models with a multi-token objective yields substantial gains on reasoning tasks requiring planning and lookahead.

Conclusion: Latent tokens, arising naturally in diffusion, act as a general mechanism to improve global coherence and planning in language models. The findings offer a tunable speed-accuracy tradeoff and transfer to autoregressive architectures, suggesting broader applicability for tasks demanding lookahead.

Abstract: Discrete diffusion models have recently become competitive with autoregressive models for language modeling, even outperforming them on reasoning tasks requiring planning and global coherence, but they require more computation at inference time. We trace this trade-off to a key mechanism: diffusion models are trained to jointly predict a distribution over all unknown tokens, including those that will not actually be decoded in the current step. Ablating this joint prediction yields faster inference but degrades performance, revealing that accurate prediction at the decoded position relies on joint reasoning about the distribution of undecoded tokens. We interpret these as latent tokens and introduce a method for modulating their number, demonstrating empirically that this enables a smooth tradeoff between inference speed and sample quality. Furthermore, we demonstrate that latent tokens can be introduced into autoregressive models through an auxiliary multi-token prediction objective, yielding substantial improvements on the same reasoning tasks where they have traditionally struggled. Our results suggest that latent tokens, while arising naturally in diffusion, represent a general mechanism for improving performance on tasks requiring global coherence or lookahead.

</details>


### [290] [Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL](https://arxiv.org/abs/2602.03773)
*Ian Wu,Yuxiao Qu,Amrith Setlur,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出 RC 迭代解码以提升 LLM 的外推与跨分布泛化能力，通过训练和推理阶段均替代自回归解码，能在推理时间内持续改进。


<details>
  <summary>Details</summary>
Motivation: 克服强化学习在固定分布和预算下的局限，追求在测试时仍能扩展推理能力和跨分布鲁棒性；提升大语言模型在有限训练预算下的外推表现。

Method: RC 是一种迭代解码算法，在训练和推理阶段替换标准自回归解码；利用回答生成与摘要之间的非对称性，构造多轮推理链，使得每次迭代都改善推理质量，从而实现长周期的外推。

Result: 在 4B 模型、16k 训练预算下，HMMT 2025 得分从 40% 提升至约 70%，测试仅需 0.5M tokens；显著超越同等规模和多数更大推理 LLM；外推能力在推理时间跨度方面超过训练看到的一个数量级。

Conclusion: 通过 RC，模型在测试时能更有效地扩展推理能力与总结条件生成能力，且能更好地利用现有 scaffolds 来提升测试性能，具有良好的扩展性和对推理链改进的稳定性。

Abstract: Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.

</details>


### [291] [Inference-time Unlearning Using Conformal Prediction](https://arxiv.org/abs/2602.03787)
*Somnath Basu Roy Chowdhury,Rahul Kidambi,Avinava Dubey,David Wang,Gokhan Mergen,Amr Ahmed,Aranyak Mehta*

Main category: cs.LG

TL;DR: Inference-time unlearning: a parameter-free framework for generative models that uses an approximate verifier and conformal prediction to iteratively refine responses, providing distribution-free unlearning guarantees with reduced computation; achieves up to 93% reduction in unlearning error over challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current unlearning methods rely on retraining subsets of parameters and rely on assumptions often violated in real-world generative models; updating parameters can degrade pre-trained capabilities. There is a need for efficient, provable unlearning without full retraining or parameter updates.

Method: Proposes an inference-time unlearning framework that uses an approximate verifier to judge whether outputs satisfy unlearning guarantees. It iteratively refines generated responses based on verifier feedback without updating model parameters. Conformal prediction is employed to reduce computational overhead and provide distribution-free guarantees.

Result: The approach significantly outperforms existing state-of-the-art methods, achieving up to 93% reduction in unlearning error on challenging benchmarks.

Conclusion: Inference-time unlearning with an approximate verifier and conformal prediction offers practical, distribution-free unlearning guarantees without retraining, preserving pre-trained model capabilities and reducing computational cost.

Abstract: Machine unlearning is the process of efficiently removing specific information from a trained machine learning model without retraining from scratch. Existing unlearning methods, which often provide provable guarantees, typically involve retraining a subset of model parameters based on a forget set. While these approaches show promise in certain scenarios, their underlying assumptions are often challenged in real-world applications -- particularly when applied to generative models. Furthermore, updating parameters using these unlearning procedures often degrades the general-purpose capabilities the model acquired during pre-training. Motivated by these shortcomings, this paper considers the paradigm of inference time unlearning -- wherein, the generative model is equipped with an (approximately correct) verifier that judges whether the model's response satisfies appropriate unlearning guarantees. This paper introduces a framework that iteratively refines the quality of the generated responses using feedback from the verifier without updating the model parameters. The proposed framework leverages conformal prediction to reduce computational overhead and provide distribution-free unlearning guarantees. This paper's approach significantly outperforms existing state-of-the-art methods, reducing unlearning error by up to 93% across challenging unlearning benchmarks.

</details>


### [292] [Manifold Random Features](https://arxiv.org/abs/2602.03797)
*Ananya Parashar,Derek Long,Dwaipayan Saha,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: MRFs 将流形离散化与图随机特征结合，用于在一般流形上近似双变量函数/核。通过对 GRF 的离散图对象和连续随机特征之间的深层渐近联系实现核近似，并给出正界且有界的特征。还以高斯核近似为例，重新发现并简化在改进线性注意力 Transformer 时使用的相关方法，辅以严格理论和大量实验。


<details>
  <summary>Details</summary>
Motivation: 在一般流形上对核及双变量函数进行高效、低方差的近似需要新的随机特征机制。通过将图随机特征与流形离散化结合，实现从离散图到连续核特征的桥接，提升在复杂几何域上的近似质量与稳定性。

Method: 将流形离散化为图结构，利用 GRF 学习能在图上产生的连续场；由此得到在流形上可用的连续随机特征，用于近似核函数，且特征为正且有界；建立 GRF 与连续随机特征之间的渐近联系，证明在极限下两者等价/相关；并以高斯核近似为子例验证，在简单随机游走图上推导，简化原有推导，最后给出理论分析与实验验证。

Result: 提出的 MR F 框架实现了正界、正定的随机特征，用以在流形上近似双变量函数/核；在离散图上定义的 GRF 与连续随机特征之间存在深入的渐近联系；可用于重现高斯核近似，进而提升如线性注意力 Transformer 的性能；并提供理论分析与系统性实验验证。

Conclusion: MRFs 提供了一种将流形离散化、图随机特征与连续核近似融合的统一框架，具有正界且稳定的特征以及对多种核的近似能力。未来工作可聚焦误差界、计算复杂度、不同流形的自适应离散化策略以及在更广泛模型中的应用。

Abstract: We present a new paradigm for creating random features to approximate bi-variate functions (in particular, kernels) defined on general manifolds. This new mechanism of Manifold Random Features (MRFs) leverages discretization of the manifold and the recently introduced technique of Graph Random Features (GRFs) to learn continuous fields on manifolds. Those fields are used to find continuous approximation mechanisms that otherwise, in general scenarios, cannot be derived analytically. MRFs provide positive and bounded features, a key property for accurate, low-variance approximation. We show deep asymptotic connection between GRFs, defined on discrete graph objects, and continuous random features used for regular kernels. As a by-product of our method, we re-discover recently introduced mechanism of Gaussian kernel approximation applied in particular to improve linear-attention Transformers, considering simple random walks on graphs and by-passing original complex mathematical computations. We complement our algorithm with a rigorous theoretical analysis and verify in thorough experimental studies.

</details>


### [293] [Prediction of Critical Heat Flux in Rod Bundles Using Tube-Based Hybrid Machine Learning Models in CTF](https://arxiv.org/abs/2602.03805)
*Aidan Furlong,Robert Salko,Xingang Zhao,Xu Wu*

Main category: cs.LG

TL;DR: 在管束数据泛化到棒束时，基于ML的CHF预测显著优于W-3、Bowring、Groeneveld LUT，混合LUT模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 提高关键热流密度CHF预测的准确性，超越经验相关和查表法；实现对棒束等复杂几何的泛化，便于在CTF子通道代码中应用。

Method: 在CTF子通道代码中实现三种模型：一个纯数据驱动的DNN、两种混合偏置校正模型；以管道（tube）基数据训练，再在Combustion Engineering 5x5棒束CHF测试序列上评估，比较W-3、Bowring和Groeneveld LUT作为基线。

Result: 三种ML方法在幅值和定位（位置）预测上通常优于基线模型；混合LUT模型具备最优的性能指标。

Conclusion: ML方法具有对棒束几何的良好泛化潜力，混合偏置校正的LUT在准确性上最具优势，适合集成到CTF等子通道代码中用于棒束CHF预测。

Abstract: The prediction of critical heat flux (CHF) using machine learning (ML) approaches has become a highly active research activity in recent years, the goal of which is to build models more accurate than current conventional approaches such as empirical correlations or lookup tables (LUTs). Previous work developed and deployed tube-based pure and hybrid ML models in the CTF subchannel code, however, full-scale reactor core simulations require the use of rod bundle geometries. Unlike isolated subchannels, rod bundles experience complex thermal hydraulic phenomena such as channel crossflow, spacer grid losses, and effects from unheated conductors. This study investigates the generalization of ML-based CHF prediction models in rod bundles after being trained on tube-based CHF data. A purely data-driven DNN and two hybrid bias-correction models were implemented in the CTF subchannel code and used to predict CHF location and magnitude in the Combustion Engineering 5-by-5 bundle CHF test series. The W-3 correlation, Bowring correlation, and Groeneveld LUT were used as baseline comparators. On average, all three ML-based approaches produced magnitude and location predictions more accurate than the baseline models, with the hybrid LUT model exhibiting the most favorable performance metrics.

</details>


### [294] [SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving](https://arxiv.org/abs/2602.03816)
*Yesom Park,Annie C. Lu,Shao-Ching Huang,Qiyang Hu,Y. Sungtaek Ju,Stanley Osher*

Main category: cs.LG

TL;DR: 提出 SymPlex 框架，将符号表达空间的 PDE 求解视为树结构决策。通过 SymFormer（结构感知 Transformer）和基于语法的自回归解码，直接在符号表达级别生成解析解，提升可解释性并处理非光滑与参数依赖。


<details>
  <summary>Details</summary>
Motivation: 解决现有数值/神经方法在离散化空间或隐函数空间中的近似能力不足，缺乏可解释的符号解表达，并且难以处理非光滑行为与显式参数依赖的 PDE 解。

Method: 将符号 PDE 求解建模为树状决策过程；设计结构感知 Transformer SymFormer，通过树相对自注意力建模层次化符号依赖；采用语法约束的自回归解码以确保表达式的合法性；实现直接在符号表达空间的求解。

Result: 实验表明能够在非光滑和参数化 PDE 解的情形下，使用基于深度学习的符号方法实现对解析解的精确恢复。

Conclusion: SymPlex 提供一种可解释、可读的符号 PDE 求解框架，通过结构化注意力和语法约束生成合法表达，克服了传统方法在表达能力和可解释性上的局限，能直接表示非光滑行为和参数依赖。

Abstract: We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.

</details>


### [295] [Robust Intervention Learning from Emergency Stop Interventions](https://arxiv.org/abs/2602.03825)
*Ethan Pronovost,Khimya Khetarpal,Siddhartha Srinivasa*

Main category: cs.LG

TL;DR: 在鲁棒干预学习框架下，通过残差微调对干预信号进行鲁棒利用，提出 RIFT，在前政策的结构约束下将干预反馈作为不完整学习信号进行微调。


<details>
  <summary>Details</summary>
Motivation: 干预数据是测试阶段自动系统的重要信号，但往往嘈杂且信息不足；需要在存在干预信号质量差的情况下仍能改进策略。

Method: 提出 Residual Intervention Fine-Tuning (RIFT)，把干预反馈视为不完整信号，与先验策略结合，以残差微调的方式进行学习；将干预学习视为微调问题，利用先验策略中的结构 resolving ambiguity; 给出理论分析条件以及失败情形。

Result: 实验表明 RIFT 在多种干预策略和先验策略质量下能实现鲁棒且一致的策略改进。

Conclusion: 鲁棒干预学习是一个有前景的方向，RIFT 展现了可行性，未来工作可拓展更广干预场景和理论分析。

Abstract: Human interventions are a common source of data in autonomous systems during testing. These interventions provide an important signal about where the current policy needs improvement, but are often noisy and incomplete. We define Robust Intervention Learning (RIL) as the problem of learning from intervention data while remaining robust to the quality and informativeness of the intervention signal. In the best case, interventions are precise and avoiding them is sufficient to solve the task, but in many realistic settings avoiding interventions is necessary but not sufficient for achieving good performance. We study robust intervention learning in the context of emergency stop interventions and propose Residual Intervention Fine-Tuning (RIFT), a residual fine-tuning algorithm that treats intervention feedback as an incomplete learning signal and explicitly combines it with a prior policy. By framing intervention learning as a fine-tuning problem, our approach leverages structure encoded in the prior policy to resolve ambiguity when intervention signals under-specify the task. We provide theoretical analysis characterizing conditions under which this formulation yields principled policy improvement, and identify regimes where intervention learning is expected to fail. Our experiments reveal that residual fine-tuning enables robust and consistent policy improvement across a range of intervention strategies and prior policy qualities, and highlight robust intervention learning as a promising direction for future work.

</details>
