<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 60]
- [cs.AI](#cs.AI) [Total: 64]
- [cs.LG](#cs.LG) [Total: 145]
- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [asr_eval: Algorithms and tools for multi-reference and streaming speech recognition evaluation](https://arxiv.org/abs/2601.20992)
*Oleg Sedukhin,Andrey Kostin*

Main category: cs.CL

TL;DR: 提出一整套新的语音识别评估改进：多参考标注的字符串对齐、适用于任意长度插入的对齐算法；新长表述数据 DiverseSpeech-Ru 与对俄语测试集的多参考重标注；研究微调对数据集标签的适应性从而导致指标表观提升的现象；基于改进的对齐开发流式评估工具与多转录对齐可视化工具，并提供模型包装接口，计划开源。


<details>
  <summary>Details</summary>
Motivation: 现有评估受限于单一参考、对长表述处理能力不足，且非拉丁语言的词汇结构多样，导致对齐和比较不公平；需要可扩展的对齐方法、丰富的多参考数据，以及流式评估与跨转录对齐工具来实现更公平、可重复的评估。

Method: 1) 提出支持多参考标注、任意长度插入和更好词对齐的新字符串对齐算法；2) 构建长表述、野外俄语语音的 DiverseSpeech-Ru 数据集，并对现有俄语测试集进行多参考重标注；3) 研究其训练集的微调动力学及模型对数据集标签的适应性；4) 基于改进的对齐开发用于流式语音识别评估与多转录对齐的可视化工具；5) 提供离线与流式模型的统一包装器；6) 计划开源代码。

Result: 证实：改进的对齐可用于更公平地评估流式与离线语音识别，揭示模型易被数据集标签所“诱导”的现象，导致表观指标提升的错觉；新工具和数据集有助于跨语言、跨任务的鲁棒评估。

Conclusion: 提出的对齐算法和数据/工具生态将提升评估的公平性与可重复性，特别是对非拉丁语言和长表述场景的评估；未来工作可扩展到更多语言与更多场景，并检验重标注对系统排名的影响。

Abstract: We propose several improvements to the speech recognition evaluation. First, we propose a string alignment algorithm that supports both multi-reference labeling, arbitrary-length insertions and better word alignment. This is especially useful for non-Latin languages, those with rich word formation, to label cluttered or longform speech. Secondly, we collect a novel test set DiverseSpeech-Ru of longform in-the-wild Russian speech with careful multi-reference labeling. We also perform multi-reference relabeling of popular Russian tests set and study fine-tuning dynamics on its corresponding train set. We demonstrate that the model often adopts to dataset-specific labeling, causing an illusion of metric improvement. Based on the improved word alignment, we develop tools to evaluate streaming speech recognition and to align multiple transcriptions to compare them visually. Additionally, we provide uniform wrappers for many offline and streaming speech recognition models. Our code will be made publicly available.

</details>


### [2] [Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations](https://arxiv.org/abs/2601.21084)
*Amit Meghanani,Thomas Hain*

Main category: cs.CL

TL;DR: 研究在SSL基础上进行表征引导的语音增强，针对MSE对SSL位置嵌入的利用问题，提出两种对齐策略并发现基于soft-DTW的速度扰动方法更快收敛且提升下游表现，强调位置不变的微调对SSL语音建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决自监督表示微调时，MSE损失容易被SSL模型的位置信息所利用，从而偏离内容信息，影响后续噪声条件下的下游任务。需要实现对表征的位置不变的微调策略来提升鲁棒性。

Method: 在前端语音增强与SSL表示的耦合中，采用两种策略进行表征引导的微调：1) 在微调阶段使用零填充以抑制位置信息的利用；2) 引入带有软对齐的速度扰动，使用soft-DTW损失来对齐内容同时抑制位置敏感性，并与SE模型共同训练以优化SSL表示的鲁棒性。

Result: 实验表明，基于soft-DTW的速度扰动方法实现更快的收敛并获得更好的下游性能，相比仅使用零填充等基线，显示出对位置不变性的有效提升。

Conclusion: 针对SSL语音建模中的表征微调，位置不变的微调策略至关重要。以soft-DTW的速度扰动进行表征引导的SE微调是一种有效且具实用性的途径，可提升在噪声条件下的下游任务表现。

Abstract: Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.

</details>


### [3] [ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference](https://arxiv.org/abs/2601.21109)
*Ketan Thakkar,Maitreyi Chatterjee,Ramasubramanian Balasubramanian,Achyuthan Jootoo,Rajendra Ugrani*

Main category: cs.CL

TL;DR: 提出 ChunkWise LoRA：基于自适应分块和等级线性参数的低秩微调方法，结合运行时调度器实现按块分配低秩并提升推理效率，同时保持输出一致性。


<details>
  <summary>Details</summary>
Motivation: 现有 LoRA 采用静态的秩配置，未考虑不同 token 的复杂度和计算需求，导致资源未被充分利用，存在延迟和内存开销。

Method: 引入运行时调度器进行 token 困难度估计、可变长度分块、按块选择秩和缩放（rank-ladder），以及边界安全的组合模块和基于策略的 KV-cache 策略。

Result: 在 Wikitext-103 和 SQuAD 上，ChunkWise LoRA 实现了最高 34% 的延迟降低、38% 的内存下降，同时保持或提升 BLEU、EM、困惑度等任务指标。

Conclusion: 该框架与现有 Transformer 架构和推理框架完全兼容，为参数高效的 LLMs 的实用部署提供了可行方案，能在不显著影响性能的前提下显著降低资源开销。

Abstract: Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.

</details>


### [4] [Multi-task Code LLMs: Data Mix or Model Merge?](https://arxiv.org/abs/2601.21115)
*Mingzhi Zhu,Boris Sobolev,Rahul Krishna,Raju Pavuluri,Stacy Patterson,Michele Merler*

Main category: cs.CL

TL;DR: 模型合并在大规模跨任务代码LLM上表现最佳；小尺度时数据混合更优；在两大模型家族上均适用，合并能保留大部分专业能力甚至超越单专用微调；Qwen Coder 2.5 7B达到 92.7% Pass@1（HumanEval），高于单独微调版本；并提出权重分析以理解任务对参数的影响。


<details>
  <summary>Details</summary>
Motivation: 探索在资源受限场景下构建小型多任务代码LLM的实际可行策略，比较数据混合与模型合并的效果，覆盖两大模型家族及两种尺度。

Method: 对 Qwen Coder 与 DeepSeek Coder 在 2B 与 7B 两个尺度上进行代码生成与代码摘要的微调；通过数据混合与模型合并两种策略训练模型；在 HumanEval、MBPP、CodeXGlue 上评估；并提出权重分析以揭示任务对模型参数的影响。

Result: 在较大尺度下，模型合并在不同模型家族间均取得最佳综合性能，保留约 96% 的专用模型在代码生成任务上的性能，同时维持摘要能力；合并模型甚至可超越单独微调的模型；在 Qwen Coder 2.5 7B 配置下，HumanEval 的 Pass@1 达到 92.7%（高于 90.9% 的任务专用微调版本）。在较小尺度上，数据混合更具优势。引入权重分析以理解不同任务对参数的影响及对合并策略的启示。

Conclusion: 通过合并和混合策略的权衡，可以在资源受限的部署场景下有效地组合任务专用能力而不显著降低性能。大尺度更倾向于模型合并，小尺度更适合数据混合；权重分析为设计合并策略提供了洞见。

Abstract: Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.

</details>


### [5] [Fake News Detection After LLM Laundering: Measurement and Explanation](https://arxiv.org/abs/2501.18649)
*Rupak Kumar Das,Jonathan Dodge*

Main category: cs.CL

TL;DR: 研究评估在检测LLM生成的伪造新闻时，是否在检测流程中加入改写(paraphrase)步骤有助于还是阻碍检测；结果显示检测器对LLM改写的伪造新闻更困难检测、揭示情感倾向变化是检测失败的原因之一，并提供用于改写输出的配对数据集与分数。


<details>
  <summary>Details</summary>
Motivation: LLM生成的伪造新闻具有高度可信性，现有伪新闻检测多聚焦人类撰写文本，对LLM改写文本的检测研究不足，需评估改写对检测效果的影响并提供相关数据资源。

Method: 在检测管线中引入或比较改写步骤，评估多种检测器在识别LLM改写伪造新闻中的表现；通过LIME对检测失败进行解释；分析模型在规避检测、进行改写以规避检测、以及以语义相似性改写等任务的表现；以BERTScore与情感变化等指标评估改写质量与信号；提供并公布包含改写输出与分数的新数据集。

Result: 检测器比对人类撰写文本更难检测LLM改写的伪造新闻；不同模型在规避检测、改写以规避检测、以及实现语义相似等任务上各有专长；情感倾向的改变揭示了检测失败的潜在原因；存在对改写质量的担忧：在高BERTScore的情况下仍出现情感变化的样本；并提供了用于研究的新数据集。

Conclusion: 改写/伪装是检测伪新闻的主要挑战之一，需改进检测方法以抗对LLM改写的伪造文本；情感变化成为关键线索，数据集资源有助于推动领域研究。

Abstract: With their advanced capabilities, Large Language Models (LLMs) can generate highly convincing and contextually relevant fake news, which can contribute to disseminating misinformation. Though there is much research on fake news detection for human-written text, the field of detecting LLM-generated fake news is still under-explored. This research measures the efficacy of detectors in identifying LLM-paraphrased fake news, in particular, determining whether adding a paraphrase step in the detection pipeline helps or impedes detection. This study contributes: (1) Detectors struggle to detect LLM-paraphrased fake news more than human-written text, (2) We find which models excel at which tasks (evading detection, paraphrasing to evade detection, and paraphrasing for semantic similarity). (3) Via LIME explanations, we discovered a possible reason for detection failures: sentiment shift. (4) We discover a worrisome trend for paraphrase quality measurement: samples that exhibit sentiment shift despite a high BERTSCORE. (5) We provide a pair of datasets augmenting existing datasets with paraphrase outputs and scores. The dataset is available on GitHub

</details>


### [6] [Large Language Models Naively Recover Ethnicity from Individual Records](https://arxiv.org/abs/2601.21132)
*Noah Dasanaike*

Main category: cs.CL

TL;DR: LLMs can infer ethnicity from names with high accuracy, outperforming BISG and generalizing internationally; improvements via extended reasoning and metadata; validated across multiple countries; potential fairness and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: address limitations of BISG for ethnolinguistic inference and enable context-appropriate classification beyond US.

Method: evaluate six LLMs (including Gemini 3 Flash, GPT-4o, open-source DeepSeek v3.2, GLM-4.7) on stratified voter data from Florida and North Carolina with self-reported race; test extended reasoning; include metadata (party registration); validate on Lebanese voter registration with religious sect, Indian MPs from reserved constituencies, Indian land records with caste; aggregate cross-country validation across India, Uganda, Nepal, Armenia, Chile, Costa Rica; note fine-tuning small transformers on LLM labels for deployment.

Result: LLMs achieve up to 84.7% accuracy (vs 68.2% BISG); extended reasoning yields +1–3 percentage points; metadata (e.g., party) increases accuracy to ~86.7%; reduces income bias present in BISG; cross-domain validation supports generalization to contexts with distinctive naming conventions; small transformer models fine-tuned on LLM labels can surpass BISG and enable local deployment at low cost.

Conclusion: LLMs offer a powerful approach to ethnicity inference from names with broader applicability beyond the US, but raise ethical, privacy and governance concerns; deployment requires careful consideration of misclassification risks, fairness across groups, and provenance of training data.

Abstract: I demonstrate that large language models can infer ethnicity from names with accuracy exceeding that of Bayesian Improved Surname Geocoding (BISG) without additional training data, enabling inference outside the United States and to contextually appropriate classification categories. Using stratified samples from Florida and North Carolina voter files with self-reported race, LLM-based classification achieves up to 84.7% accuracy, outperforming BISG (68.2%) on balanced samples. I test six models including Gemini 3 Flash, GPT-4o, and open-source alternatives such as DeepSeek v3.2 and GLM-4.7. Enabling extended reasoning can improve accuracy by 1-3 percentage points, though effects vary across contexts; including metadata such as party registration reaches 86.7%. LLM classification also reduces the income bias inherent in BISG, where minorities in wealthier neighborhoods are systematically misclassified as White. I further validate using Lebanese voter registration with religious sect (64.3% accuracy), Indian MPs from reserved constituencies (99.2%), and Indian land records with caste classification (74.0%). Aggregate validation across India, Uganda, Nepal, Armenia, Chile, and Costa Rica using original full-count voter rolls demonstrates that the method recovers known population distributions where naming conventions are distinctive. For large-scale applications, small transformer models fine-tuned on LLM labels exceed BISG accuracy while enabling local deployment at no cost.

</details>


### [7] [EnsembleLink: Accurate Record Linkage Without Training Data](https://arxiv.org/abs/2601.21138)
*Noah Dasanaike*

Main category: cs.CL

TL;DR: EnsembleLink is an unsupervised record linkage method that uses pre-trained language models to infer semantic matches without labeled data, achieving high accuracy and running locally.


<details>
  <summary>Details</summary>
Motivation: Record linkage uncertainty impacts downstream analyses; existing methods require labeled data or data-intensive training; need a scalable unsupervised solution.

Method: An ensemble approach that leverages semantic knowledge from pre-trained LMs to identify entity pairs that refer to the same entity without supervision; runs on open-source models locally; applicable to city names, person names, organizations, multilingual parties, bibliographic records; efficient, no API reliance.

Result: On diverse benchmarks, EnsembleLink matches or surpasses supervised methods with extensive labeling; completes typical tasks in minutes on commodity hardware.

Conclusion: Demonstrates a practical, training-free, high-accuracy linkage framework capable of handling multilingual and cross-domain records, reducing labeling costs and enabling local deployment.

Abstract: Record linkage, the process of matching records that refer to the same entity across datasets, is essential to empirical social science but remains methodologically underdeveloped. Researchers treat it as a preprocessing step, applying ad hoc rules without quantifying the uncertainty that linkage errors introduce into downstream analyses. Existing methods either achieve low accuracy or require substantial labeled training data. I present EnsembleLink, a method that achieves high accuracy without any training labels. EnsembleLink leverages pre-trained language models that have learned semantic relationships (e.g., that "South Ozone Park" is a neighborhood in "New York City" or that "Lutte ouvriere" refers to the Trotskyist "Workers' Struggle" party) from large text corpora. On benchmarks spanning city names, person names, organizations, multilingual political parties, and bibliographic records, EnsembleLink matches or exceeds methods requiring extensive labeling. The method runs locally on open-source models, requiring no external API calls, and completes typical linkage tasks in minutes.

</details>


### [8] [MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset](https://arxiv.org/abs/2601.21512)
*Serry Sibaee,Yasser Alhabashi,Nadia Sibai,Yara Farouk,Adel Ammar,Sawsan AlHalawani,Wadii Boulila*

Main category: cs.CL

TL;DR: 96,243条词定义对的开放多域阿拉伯语词典数据集MURAD，采用混合提取管线构建，覆盖跨领域定义，支持反向词典等应用。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语缺乏大规模、跨域的词汇-定义对数据集，限制NLP和辞书学研究。

Method: 来自可信参考与教育资源，使用直接文本解析、OCR与自动重构的混合管线，分配目标词及标准化定义和源域元数据。

Result: 公开数据集，包含96,243条词定义对，覆盖语言学、伊斯兰研究、数学、物理、心理学、工程等领域，支持计算语言学和辞书学研究。

Conclusion: 该资源推进阿拉伯语NLP及可重复研究，支持反向词典建模、语义检索、教育工具。

Abstract: Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.

</details>


### [9] [Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space](https://arxiv.org/abs/2601.21169)
*Tobias Materzok*

Main category: cs.CL

TL;DR: OS-Search 将 LLM 生成转化为输出空间的端点搜索，在冻结的编码器定义的三维输出空间 Z 上进行外层搜索以选取目标 z*，再通过一个基于序列级 RL 的检索驱动策略，在标准自回归解码下产生坐标落在 z* 附近的输出。这使并行遍历与黑盒优化成为可能，避免路径相关的逐步令牌/程序搜索。在故事任务中，对文本 Z 的遍历实现了比提示链路更高的多样性（LLM 评分）3.1 倍；在代码任务中，对 Z 进行贝叶斯优化，在推理预算匹配的条件下提升未对控制器披露的目标，同时保持输出的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成过程对单步令牌/程序搜索的依赖所带来的低效与局部性问题，提供一个端点化的输出空间优化框架，以实现更高的多样性和对隐含目标的有效优化，同时支持并行化探索与黑盒优化。

Method: 将编码器定义的三维输出空间 Z 固定不变，外层循环在 Z 中选取目标 z*；训练一个基于序列级 RL 的检索驱动策略，使生成输出的坐标落在 z* 附近，且在常规自回归解码下完成输出；实现对 Z 的并行遍历与黑盒优化，并实现分任务的预算对齐。故事任务上通过对文本 Z 的遍历实现更高的多样性，代码任务上对 Z 进行贝叶斯优化以优化控制器未披露的目标，同时保持推理预算与输出有效性。

Result: 故事：对文本 Z 的遍历带来 3.1 倍的 LLM 评分多样性提升，相较于提示链路。代码：在匹配推理预算的前提下，对 Z 进行贝叶斯优化，提升未对外披露的目标，同时保持输出的有效性。

Conclusion: OS-Search 提供了一种端点化、黑盒化的生成优化框架，解耦生成过程与逐步令牌搜索，支持并行化探索，实现了更高的多样性和对目标的优化，同时保持输出的有效性。

Abstract: We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.

</details>


### [10] [LMK > CLS: Landmark Pooling for Dense Embeddings](https://arxiv.org/abs/2601.21525)
*Meet Doshi,Aashka Trivedi,Vishwajeet Kumar,Parul Awasthy,Yulong Li,Jaydeep Sen,Radu Florian,Sachindra Joshi*

Main category: cs.CL

TL;DR: LMK pooling通过在序列分块之间插入 landmarks（标记符）并对 landmarks 的嵌入进行均值池化来得到最终表示，解决了 [CLS] 聚合偏向前部信息和均值池化稀释局部信号的问题，提升长上下文的泛化能力，同时保持对局部显著特征的保留，开销仅为引入少量特殊标记。


<details>
  <summary>Details</summary>
Motivation: 现有的聚合策略（如 [CLS] 与均值池化）存在系统性弱点：CLS 将信息集中在序列的初始位置，可能高估前部证据；均值池化可能稀释局部显著信号，损害短上下文表现。需要一种在保持局部信息的同时提升长上下文推断的聚合方法。

Method: 将序列分割为若干块，在块之间插入 landmark 标记，然后对 landmark 的嵌入进行均值池化以得到最终表示。该方法引入少量特殊标记，但极大地提升了对长上下文的外推能力，同时不牺牲局部显著特征。

Result: 在短上下文检索任务上与现有方法表现相当，在长上下文任务上实现显著提升，是一种简单、可扩展的替代聚合方法。

Conclusion: LMK 聚合是一种简单且有效的池化机制，能够在不牺牲局部信息的前提下提升长上下文建模能力，具备实用性和可扩展性。

Abstract: Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.

</details>


### [11] [From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning](https://arxiv.org/abs/2601.21191)
*Xiulin Yang,Heidi Getz,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 跨语言语料分析表明功能词的三种分布特征共同支撑线性输入下的层级结构学习，且频率与结构相关性贡献最大，边界对齐次显著。不同学习条件下，依赖功能词的方式多样，可能通过不同内部机制实现相同的表现。


<details>
  <summary>Details</summary>
Motivation: 揭示从线性输入中学习层级结构所需的统计条件，聚焦功能词的分布特征及其对语言学习的普遍性和可迁移性。

Method: 1) 跨语种语料分析以验证功能词的三大属性（高频率、与句法结构的可靠关联、与短语边界的对齐）。2) 通过反事实语言建模和消融实验评估三属性对学习的影响。3) 进行探测分析与进一步的消融，以揭示在不同学习条件下对功能词的依赖及潜在的内部机制。

Result: 1) 三大属性在186种语言中普遍存在。2) 保留三属性的语言变体更易被神经学习者掌握。3) 频率和结构关联性对学习的贡献大于边界对齐。4) 不同学习条件下对功能词的依赖模式系统性不同，可能来自不同内部机制。

Conclusion: 表明功能词的统计特征是线性输入下学习层级结构的关键驱动，频率和结构相关性是核心因素；不同内部机制可实现相同表现，强调对模型偏差与学习路径的多样性认识；对语言习得与神经模型设计具有指导意义。

Abstract: What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role in language acquisition due to their distinctive distributional properties, including high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. We use cross-linguistic corpus analysis to first establish that all three properties are present across 186 studied languages. Next, we use a combination of counterfactual language modeling and ablation experiments to show that language variants preserving all three properties are more easily acquired by neural learners, with frequency and structural association contributing more strongly than boundary alignment. Follow-up probing and ablation analyses further reveal that different learning conditions lead to systematically different reliance on function words, indicating that similar performance can arise from distinct internal mechanisms.

</details>


### [12] [Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning](https://arxiv.org/abs/2601.21700)
*Wonduk Seo,Wonseok Choi,Junseo Koh,Juhyeon Lee,Hyunjin An,Minhyeong Yu,Jian Park,Qingshan Zhou,Seunghyun Lee,Yi Bu*

Main category: cs.CL

TL;DR: OG-MAR is an ontology-guided multi-agent reasoning framework for cultural alignment in LLMs. It builds a global cultural ontology from World Values Survey data, uses competency questions to define relations on a fixed taxonomy, instantiates value-persona agents from demographically similar profiles, and uses a judgment agent to enforce ontology consistency. It shows improved cultural alignment, robustness, and transparent reasoning traces across multiple backbones.


<details>
  <summary>Details</summary>
Motivation: LLMs often misalign culturally due to skewed pretraining data and lack structured value representations. Existing methods lack demographic grounding and treat values as unstructured signals, reducing consistency and interpretability. A formal ontology-guided, demographic-aware, multi-agent approach could improve alignment and transparency.

Method: Construct a global cultural ontology by eliciting relations over a fixed taxonomy using competency questions, based on respondent-specific values from the World Values Survey (WVS). At inference, retrieve ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents; outputs are synthesized by a judgment agent enforcing ontology consistency and demographic proximity.

Result: Empirical evaluation on regional social-survey benchmarks across four LLM backbones shows OG-MAR improves cultural alignment and robustness relative to competitive baselines and yields more transparent reasoning traces.

Conclusion: Ontology-guided multi-agent reasoning with demographically aware value personas enhances cultural alignment, robustness, and interpretability of LLM outputs by grounding decisions in a structured cultural ontology.

Abstract: Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.

</details>


### [13] [Multilingual Dysarthric Speech Assessment Using Universal Phone Recognition and Language-Specific Phonemic Contrast Modeling](https://arxiv.org/abs/2601.21205)
*Eunjung Yeo,Julie M. Liss,Visar Berisha,David R. Mortensen*

Main category: cs.CL

TL;DR: Multilingual phoneme-production assessment framework that combines universal phone recognition with language-specific phoneme interpretation using contrastive phonological feature distances, yielding PER, PFER, and PhonCov; cross-language applicability demonstrated on English, Spanish, Italian, and Tamil.


<details>
  <summary>Details</summary>
Motivation: There is a rising burden of dysarthria-related speech disorders and a need for automatic intelligibility assessment methods that work across languages. Existing methods are often monolingual or miss language-specific factors influencing intelligibility.

Method: Integrates universal phoneme recognition with language-specific phoneme interpretation via contrastive phonological feature distances for phone-to-phoneme mapping and sequence alignment. Defines three metrics: phoneme error rate (PER), phonological feature error rate (PFER), and an alignment-free measure, phoneme coverage (PhonCov). Evaluation across English, Spanish, Italian, and Tamil shows distinct contributions: PER benefits from mapping+alignment, PFER from alignment alone, and PhonCov from mapping.

Result: Empirical analysis across four languages demonstrates complementary behavior of the metrics: PER improves with the combination of mapping and alignment, PFER improves with alignment, and PhonCov improves with mapping. The framework captures clinically meaningful intelligibility degradation patterns consistent with established observations in dysarthric speech.

Conclusion: The proposed framework provides a cross-language intelligibility assessment tool with complementary metrics that align with clinical patterns of dysarthria, suggesting generalizability to multiple languages and potential clinical applicability.

Abstract: The growing prevalence of neurological disorders associated with dysarthria motivates the need for automated intelligibility assessment methods that are applicalbe across languages. However, most existing approaches are either limited to a single language or fail to capture language-specific factors shaping intelligibility. We present a multilingual phoneme-production assessment framework that integrates universal phone recognition with language-specific phoneme interpretation using contrastive phonological feature distances for phone-to-phoneme mapping and sequence alignment. The framework yields three metrics: phoneme error rate (PER), phonological feature error rate (PFER), and a newly proposed alignment-free measure, phoneme coverage (PhonCov). Analysis on English, Spanish, Italian, and Tamil show that PER benefits from the combination of mapping and alignment, PFER from alignment alone, and PhonCov from mapping. Further analyses demonstrate that the proposed framework captures clinically meaningful patterns of intelligibility degradation consistent with established observations of dysarthric speech.

</details>


### [14] [Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models](https://arxiv.org/abs/2601.21214)
*Zhaoyi Li,Jiatong Li,Gangwei Jiang,Linqi Song,Defu Lian,Ying Wei*

Main category: cs.CL

TL;DR: Token-level errors in chain-of-thought (CoT) reasoning are driven by competition among attention heads, with certain erroneous processing heads (ep heads) biasing incorrect trajectories. Removing individual ep heads during inference or test-time deactivation improves reasoning hop generalization across tasks and LLMs.


<details>
  <summary>Details</summary>
Motivation: Understand why CoT reasoning fails to generalize to longer reasoning chains (hop generalization) and identify the internal mechanisms causing token-level errors.

Method: Systematic, cross-domain study analyzing token-level errors in CoT outputs; identify ep heads that amplify incorrect reasoning and suppress correct ones; propose a lightweight test-time correction that dynamically deactivates ep heads during the reasoning process; evaluate across multiple tasks and LLMs.

Result: Errors concentrate at a few token positions due to internal competition among heads; ep heads tilt the balance toward incorrect reasoning trajectories; removing serendipitous ep heads at inference can restore correct predictions; test-time deactivation of ep heads consistently improves hop generalization across tasks and models.

Conclusion: Ep heads are a key driver of reasoning-hop generalization failure; a lightweight test-time intervention that deactivates these heads is effective and shows promise for improving robust CoT reasoning across domains.

Abstract: Chain-of-thought (CoT) reasoning has become the standard paradigm for enabling Large Language Models (LLMs) to solve complex problems. However, recent studies reveal a sharp performance drop in reasoning hop generalization scenarios, where the required number of reasoning steps exceeds training distributions while the underlying algorithm remains unchanged. The internal mechanisms driving this failure remain poorly understood. In this work, we conduct a systematic study on tasks from multiple domains, and find that errors concentrate at token positions of a few critical error types, rather than being uniformly distributed. Closer inspection reveals that these token-level erroneous predictions stem from internal competition mechanisms: certain attention heads, termed erroneous processing heads (ep heads), tip the balance by amplifying incorrect reasoning trajectories while suppressing correct ones. Notably, removing individual ep heads during inference can often restore the correct predictions. Motivated by these insights, we propose test-time correction of reasoning, a lightweight intervention method that dynamically identifies and deactivates ep heads in the reasoning process. Extensive experiments across different tasks and LLMs show that it consistently improves reasoning hop generalization, highlighting both its effectiveness and potential.

</details>


### [15] [Parametric Knowledge is Not All You Need: Toward Honest Large Language Models via Retrieval of Pretraining Data](https://arxiv.org/abs/2601.21218)
*Christopher Adrian Kusuma,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 基于公开的Pythia预训练数据，提出更鲁棒的LLM诚实性评估基准和利用预训练数据提升诚实性的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有诚实性评估忽略模型在预训练阶段获得的知识，导致对模型知识边界的误判，需更健壮的评估与训练策略。

Method: 构建一个以Pythia为基础的诚实性基准数据集；提出一种利用预训练数据来提高模型诚实性的训练/推理方法。

Result: 提出了鲁棒的诚实性评估基准数据集与相应提升诚实性的初步方法（摘要未给出具体定量结果）。

Conclusion: 通过结合公开的预训练知识和鲁棒评估，提升LLM在知识边界上的诚实性，减少幻觉现象的发生。

Abstract: Large language models (LLMs) are highly capable of answering questions, but they are often unaware of their own knowledge boundary, i.e., knowing what they know and what they don't know. As a result, they can generate factually incorrect responses on topics they do not have enough knowledge of, commonly known as hallucination. Rather than hallucinating, a language model should be more honest and respond with "I don't know" when it does not have enough knowledge about a topic. Many methods have been proposed to improve LLM honesty, but their evaluations lack robustness, as they do not take into account the knowledge that the LLM has ingested during its pretraining. In this paper, we propose a more robust evaluation benchmark dataset for LLM honesty by utilizing Pythia, a truly open LLM with publicly available pretraining data. In addition, we also propose a novel method for harnessing the pretraining data to build a more honest LLM.

</details>


### [16] [SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models](https://arxiv.org/abs/2601.21235)
*Alok Abhishek,Tushar Bandopadhyay,Lisa Erickson*

Main category: cs.CL

TL;DR: 提出 SHARP 框架，通过多维、分布感知的风险画像对社交伤害进行评估，聚焦尾部风险与维度间交互，揭示现有标量基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估将复杂社会风险简化为平均风险分数，忽略分布结构、跨维度相关及极端失败；高风险场景需要更丰富的尾部和交互敏感评估。

Method: 将伤害建模为多变量随机变量，结合 bias、fairness、ethics、epistemic reliability 等维度的分解，联合失效以可相加的累积对数风险重参数化；采用风险敏感的分布统计，CVaR95 作为主度量，评估 11 种前沿 LLM 在固定的 901 条社会敏感提示上的表现。

Result: 即便平均风险相近，尾部暴露和波动性也可能相差>2倍；各维度尾部行为呈现系统性差异：bias 尾部最强，epistemic 与 fairness 处于中等，ethics 尾部相对较低；模型间存在异质的失败结构，标量基准易混淆。

Conclusion: 需从单一标量指标转向多维、尾部敏感的风险画像，以支撑负责任的评估与治理。

Abstract: Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.

</details>


### [17] [MoCo: A One-Stop Shop for Model Collaboration Research](https://arxiv.org/abs/2601.21257)
*Shangbin Feng,Yuyang Bai,Ziyuan Yang,Yike Wang,Zhaoxuan Tan,Jiajie Yan,Zhenyu Lei,Wenxuan Ding,Weijia Shi,Haojin Wang,Zhenting Qi,Yuru Jiang,Heng Wang,Chengsong Huang,Yu Fei,Jihan Yao,Yilun Du,Luke Zettlemoyer,Yejin Choi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: MoCo 提供一个面向模型协作的 Python 库，覆盖 26 种协作方法和 25 个评测数据集，支持自定义数据，验证表明大多数协作方法相较单一模型在 61.0% 的设置中有提升，最高提升可达 25.8%。


<details>
  <summary>Details</summary>
Motivation: 现有关于模型协作的研究分散且缺乏可比性，需一个统一、可扩展的基准与工具，以系统地执行、评测和比较不同协作策略，推动模型协作成为一个独立的研究领域与实践体系。

Method: 通过 MoCo 库实现大规模的模型协作算法，覆盖 26 种协作方法，涉及跨模型信息交换的不同层级（路由、文本、 logits、模型参数等），集成 25 个评测数据集（覆盖推理、问答、代码、安全等领域），并允许用户自带数据。对多组 (模型, 数据) 设置进行广泛实验，分析协作策略的扩展性与效率。

Result: 实验结果显示大多数协作策略在 61.0% 的 (模型, 数据) 设置中优于不协作的模型，且最佳方法的提升可高达 25.8%。并对协作策略的缩放性、训练/推理效率以及解决单模型难题的能力进行了分析。

Conclusion: MoCo 旨在成为促进开放、模块化、去中心化、协作式 AI 发展的有价值工具包，未来可扩展的研究方向包括进一步对比、优化及扩展协作方法与数据集。

Abstract: Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.

</details>


### [18] [CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding](https://arxiv.org/abs/2601.21262)
*Jiahao Huo,Yu Huang,Yibo Yan,Ye Pan,Yi Cao,Mingdong Ou,Philip S. Yu,Xuming Hu*

Main category: cs.CL

TL;DR: Auto-regressive embedding generation for visual document retrieval reduces token storage by 30-155x using CausalEmbed with iterative margin loss, enabling scalable multi-vector representations with competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Reduce storage overhead of dense visual tokens in MLLMs for VDR while maintaining retrieval quality; explore generative, auto-regressive embeddings for efficiency.

Method: Auto-regressive generation of multi-vector embeddings; contrastive training with iterative margin loss to produce compact, structured representations; test-time strategy to use only dozens of tokens; evaluate across backbones and benchmarks.

Result: Significant token reduction (30-155x) with competitive performance across backbones/benchmarks; theoretical and empirical evidence of training efficiency and scalable test-time behavior.

Conclusion: Introduces CausalEmbed as a flexible test-time scaling mechanism for multi-vector VDR and highlights a generative perspective in multimodal document retrieval.

Abstract: Although Multimodal Large Language Models (MLLMs) have shown remarkable potential in Visual Document Retrieval (VDR) through generating high-quality multi-vector embeddings, the substantial storage overhead caused by representing a page with thousands of visual tokens limits their practicality in real-world applications. To address this challenge, we propose an auto-regressive generation approach, CausalEmbed, for constructing multi-vector embeddings. By incorporating iterative margin loss during contrastive training, CausalEmbed encourages the embedding models to learn compact and well-structured representations. Our method enables efficient VDR tasks using only dozens of visual tokens, achieving a 30-155x reduction in token count while maintaining highly competitive performance across various backbones and benchmarks. Theoretical analysis and empirical results demonstrate the unique advantages of auto-regressive embedding generation in terms of training efficiency and scalability at test time. As a result, CausalEmbed introduces a flexible test-time scaling strategy for multi-vector VDR representations and sheds light on the generative paradigm within multimodal document retrieval.

</details>


### [19] [Qwen3-ASR Technical Report](https://arxiv.org/abs/2601.21337)
*Xian Shi,Xiong Wang,Zhifang Guo,Yongqi Wang,Pei Zhang,Xinyu Zhang,Zishan Guo,Hongkun Hao,Yu Xi,Baosong Yang,Jin Xu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-ASR 家族发布，包含两款全能式 ASR 模型（1.7B 与 0.6B）及一个基于LLM的高效强制对齐模型 Qwen3-ForcedAligner-0.6B，覆盖52种语言与方言，开源且性能优异。


<details>
  <summary>Details</summary>
Motivation: 提升多语言语音识别的准确性与效率，弥合开源与专有API的差距，推动社区研究。

Method: 以 Qwen3-Omni 作为基础，进行大规模跨语言语音训练；实现两款全能 ASR 模型并集成语言识别；开发基于LLM 的非自回归时间戳对齐模型，覆盖11语言。

Result: 1.7B 在开源模型中达到SOTA并与最强的专有API竞争；0.6B 在准确率/效率折中最佳；0.6B 平均 TTFT 92 ms，2000s 语音在1s内完成，128并发。强制对齐模型在11种语言上优于三大强力对齐模型，兼具效率与多样性。

Conclusion: 以 Apache 2.0 开源发布，旨在加速社区研究，提升跨语种ASR与音频理解能力。

Abstract: In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.

</details>


### [20] [Self-Improving Pretraining: using post-trained models to pretrain better models](https://arxiv.org/abs/2601.21343)
*Ellen Xiaoqing Tan,Shehzaad Dhuliawala,Jing Xu,Ping Yu,Sainbayar Sukhbaatar,Jason Weston,Olga Golovneva*

Main category: cs.CL

TL;DR: 在预训练阶段引入流式文档处理和基于强化学习的下一K个词优化，通过一个强大的后训练评判者对候选生成（滚动、原始后缀、改写后缀）进行质量、安全性与事实性的评分，以提升预训练阶段的生成质量、安全性与事实性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在生成中的安全性、事实性和总体质量问题，这些问题往往通过多阶段微调和对齐难以完全解决，且对预训练阶段的行为塑造至关重要。

Method: 在预训练中对文档进行流式输入，并通过强化学习来优化每一步的下一个K个生成标记。一个强大的后训练评判者对候选生成进行打分，包括模型滚动、原始后缀和改写后缀。训练初期使用原始/改写后缀，模型逐步改进后，奖励高质量的滚动序列。

Result: 与标准预训练相比，在事实性与安全性方面分别实现36.2%和18.5%的相对提升，且总体生成质量的胜率提升高达86.3%。

Conclusion: 该方法在预训练阶段就构建出更高质量、更加安全、更加具备事实性的模型，减少对后续多阶段微调与对齐的依赖。

Abstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.

</details>


### [21] [The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation](https://arxiv.org/abs/2601.21360)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Arjun Neekhra,Yash Sinha,Murari Mandal,Vinay Chamola,Dhruv Kumar*

Main category: cs.CL

TL;DR: LLMs' instruction-following robustness does not ensure objective code adjudication in automated grading; introduces SPACI and AST-ASIP to inject adversarial directives; large-scale evaluation across 9 models and 25k submissions reveals widespread decoupling of code quality from submission logic and high false-certification rates (>95% in top open-weight models).


<details>
  <summary>Details</summary>
Motivation: Educational assessments using LLMs assume instruction following translates to objective evaluation. This assumption is flawed; need to diagnose vulnerabilities and develop robustness against adversarial manipulation in automated grading.

Method: Semantic-preserving adversarial code injections via AST-aware injections into syntactically inert trivia nodes; Syntax-Semantics Gap exploitation. Large-scale evaluation of 9 state-of-the-art models across 25,000 submissions in Python, C, C++, Java. Metrics: Decoupling Probability, Score Divergence, Pedagogical Severity.

Result: Catastrophic failure rates (>95%) observed in high-capacity open-weight models (e.g., DeepSeek-V3), which prioritize hidden formatting constraints over code correctness. Demonstrates widespread False Certification of functionally broken code. Dataset and injection framework released for reproducibility.

Conclusion: Current alignment paradigms may introduce Trojan vulnerabilities in automated grading. A shift toward domain-specific Adjudicative Robustness is suggested, conditioning models to prioritize evidence over instruction compliance. Release of dataset/framework to facilitate further research.

Abstract: The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread "False Certification" of functionally broken code. Our findings suggest that current alignment paradigms create a "Trojan" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.

</details>


### [22] [User-Centric Evidence Ranking for Attribution and Fact Verification](https://arxiv.org/abs/2601.21387)
*Guy Alt,Eran Hirsch,Serwar Basch,Ido Dagan,Oren Glickman*

Main category: cs.CL

TL;DR: 提出 Evidence Ranking 任务，优先呈现充分证据以便快速并行验证；比较 one-shot 与 incremental 两种排序策略，提出新的评估框架与统一基准；结果显示 incremental 更能捕捉互补证据，LLMs 优于浅层基线，但充分性与冗余之间仍需权衡；用户研究表明该方法可降低阅读成本并提升验证效率。


<details>
  <summary>Details</summary>
Motivation: 当前事实核查系统在证据呈现上要么不足、要么冗余，导致阅读成本高、验证效率低。需要一个能优先呈现足够证据、便于逐步核验的排序任务，以提升可解释性和用户体验。

Method: 提出 Evidence Ranking 任务，设计 two-phase 方案：one-shot ranking 与 incremental ranking；构建基于信息检索的评估框架，并通过整合现有事实核查数据集建立统一基准；在多模型上进行广泛实验以比较策略与模型类型。

Result: incremental ranking 能更好捕捉证据的互补性；基于 LLM 的方法优于浅层基线；仍面临在充分性与冗余之间的平衡挑战；与证据选择相比，用户研究显示 evidence ranking 可减少阅读负担并提升核验效果。

Conclusion: 为构建更可解释、有效率、并且更契合用户需求的信息核验系统提供了基础性框架与实验证据，推动从证据选取向证据排序的方向发展。

Abstract: Attribution and fact verification are critical challenges in natural language processing for assessing information reliability. While automated systems and Large Language Models (LLMs) aim to retrieve and select concise evidence to support or refute claims, they often present users with either insufficient or overly redundant information, leading to inefficient and error-prone verification. To address this, we propose Evidence Ranking, a novel task that prioritizes presenting sufficient information as early as possible in a ranked list. This minimizes user reading effort while still making all available evidence accessible for sequential verification. We compare two approaches for the new ranking task: one-shot ranking and incremental ranking. We introduce a new evaluation framework, inspired by information retrieval metrics, and construct a unified benchmark by aggregating existing fact verification datasets. Extensive experiments with diverse models show that incremental ranking strategies better capture complementary evidence and that LLM-based methods outperform shallower baselines, while still facing challenges in balancing sufficiency and redundancy. Compared to evidence selection, we conduct a controlled user study and demonstrate that evidence ranking both reduces reading effort and improves verification. This work provides a foundational step toward more interpretable, efficient, and user-aligned information verification systems.

</details>


### [23] [SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.21476)
*Lei Yang,Wei Bi,Chenxi Sun,Renren Jin,Deyi Xiong*

Main category: cs.CL

TL;DR: SOUP introduces a token-level mix-policy that unifies off-policy and on-policy learning for LLM RL, using historical-policy prefixes for off-policy influence and on-policy continuation, with token-level importance ratios to improve exploration and stability; experimentally outperforms standard on-policy and existing off-policy methods.


<details>
  <summary>Details</summary>
Motivation: On-policy RL methods in language model post-training suffer from limited exploration and early saturation due to low sampling diversity. Off-policy data can help, but existing trajectory-mixing approaches cause policy mismatch and instability.

Method: SOUP confines off-policy influence to the prefix of each generated sequence sampled from historical policies, while the continuation is generated on-policy. It uses token-level importance ratios to leverage off-policy information within individual samples during training.

Result: Extensive experiments show that SOUP consistently outperforms standard on-policy training and existing off-policy extensions.

Conclusion: Fine-grained, single-sample mix-policy training can improve exploration and final performance in LLM RL; the analysis clarifies how token-level mix-policy improves exploration and outcomes.

Abstract: On-policy reinforcement learning (RL) methods widely used for language model post-training, like Group Relative Policy Optimization (GRPO), often suffer from limited exploration and early saturation due to low sampling diversity. While off-policy data can help, current approaches that mix entire trajectories cause significant policy mismatch and instability. In this work, we propose the $\textbf{S}$ingle-sample Mix-p$\textbf{O}$licy $\textbf{U}$nified $\textbf{P}$aradigm (SOUP), a framework that unifies off- and on-policy learning within individual samples at the token level. It confines off-policy influence to the prefix of a generated sequence sampled from historical policies, while the continuation is generated on-policy. Through token-level importance ratios, SOUP effectively leverages off-policy information while preserving training stability. Extensive experiments demonstrate that SOUP consistently outperforms standard on-policy training and existing off-policy extensions. Our further analysis clarifies how our fine-grained, single-sample mix-policy training can improve both exploration and final performance in LLM RL.

</details>


### [24] [DimStance: Multilingual Datasets for Dimensional Stance Analysis](https://arxiv.org/abs/2601.21483)
*Jonas Becker,Liang-Chih Yu,Shamsuddeen Hassan Muhammad,Jan Philip Wahle,Terry Ruas,Idris Abdulmumin,Lung-Hao Lee,Wen-Ni Liu,Tzu-Mi Lin,Zhe-Yu Xu,Ying-Lung Lin,Jin Wang,Maryam Ibrahim Mukhtar,Bela Gipp,Saif M. Mohammed*

Main category: cs.CL

TL;DR: DimStance introduces the first multilingual dimensional stance resource with valence-arousal (VA) annotations, enabling regression-based stance analysis across five languages and two domains; it benchmarks LLMs and finds fine-tuned regressors competitive, but highlights low-resource challenges and token-generation limitations.


<details>
  <summary>Details</summary>
Motivation: Move beyond coarse categorical stance labels by modeling stance in a continuous affective space (valence-arousal) to capture nuanced attitudes and enable finer-grained analysis, especially in multilingual, real-world data.

Method: Construct DimStance dataset with VA annotations for 11,746 target aspects across 7,365 texts in English, German, Chinese, Nigerian Pidgin, and Swahili over politics and environmental protection. Define a dimensional stance regression task, examine cross-lingual VA patterns, and benchmark pretrained models and large language models under regression and prompting settings.

Result: DimStance benchmarking shows fine-tuned LLM regressors achieve competitive performance; persistent challenges in low-resource languages remain; token-based generation has limitations for VA prediction. The dataset enables cross-lingual VA pattern analysis and establishes a foundation for multilingual, emotion-aware stance research.

Conclusion: DimStance establishes a new, multilingual resource for valence-arousal stance analysis and provides a benchmark framework for regression-based stance prediction, highlighting both the potential and the remaining challenges in cross-lingual emotion-informed stance analysis.

Abstract: Stance detection is an established task that classifies an author's attitude toward a specific target into categories such as Favor, Neutral, and Against. Beyond categorical stance labels, we leverage a long-established affective science framework to model stance along real-valued dimensions of valence (negative-positive) and arousal (calm-active). This dimensional approach captures nuanced affective states underlying stance expressions, enabling fine-grained stance analysis. To this end, we introduce DimStance, the first dimensional stance resource with valence-arousal (VA) annotations. This resource comprises 11,746 target aspects in 7,365 texts across five languages (English, German, Chinese, Nigerian Pidgin, and Swahili) and two domains (politics and environmental protection). To facilitate the evaluation of stance VA prediction, we formulate the dimensional stance regression task, analyze cross-lingual VA patterns, and benchmark pretrained and large language models under regression and prompting settings. Results show competitive performance of fine-tuned LLM regressors, persistent challenges in low-resource languages, and limitations of token-based generation. DimStance provides a foundation for multilingual, emotion-aware, stance analysis and benchmarking.

</details>


### [25] [inversedMixup: Data Augmentation via Inverting Mixed Embeddings](https://arxiv.org/abs/2601.21543)
*Fanshuang Kong,Richong Zhang,Qiyu Sun,Zhijie Nie,Ting Deng,Chunming Hu*

Main category: cs.CL

TL;DR: 提出 inversedMixup：通过对齐任务模型的输出嵌入与LLM的输入嵌入，使线性混合嵌入可解读为可控的文本增强，并揭示流形侵入现象。


<details>
  <summary>Details</summary>
Motivation: 解决 Mixup 在文本中的不可解释性和可控性缺失问题，结合 LLM 的可读性与嵌入空间对齐。

Method: 三阶段训练：1) 对齐目标模型输出嵌入到 LLM 输入嵌入的映射；2) 重构混合嵌入为可读文本；3) 引入缓解流形侵入的策略。

Result: 在 few-shot 与 fully supervised 设置下，方法显著提升数据增强效果且具泛化性；证据表明文本 Mixup 存在流形侵入现象，提出缓解。

Conclusion: 方法提供一个新范式，兼顾可控性与可解释性；为文本数据增强提供新思路，且对理解嵌入空间的混合有重要意义。

Abstract: Mixup generates augmented samples by linearly interpolating inputs and labels with a controllable ratio. However, since it operates in the latent embedding level, the resulting samples are not human-interpretable. In contrast, LLM-based augmentation methods produce sentences via prompts at the token level, yielding readable outputs but offering limited control over the generation process. Inspired by recent advances in LLM inversion, which reconstructs natural language from embeddings and helps bridge the gap between latent embedding space and discrete token space, we propose inversedMixup, a unified framework that combines the controllability of Mixup with the interpretability of LLM-based generation. Specifically, inversedMixup adopts a three-stage training procedure to align the output embedding space of a task-specific model with the input embedding space of an LLM. Upon successful alignment, inversedMixup can reconstruct mixed embeddings with a controllable mixing ratio into human-interpretable augmented sentences, thereby improving the augmentation performance. Additionally, inversedMixup provides the first empirical evidence of the manifold intrusion phenomenon in text Mixup and introduces a simple yet effective strategy to mitigate it. Extensive experiments demonstrate the effectiveness and generalizability of our approach in both few-shot and fully supervised scenarios.

</details>


### [26] [Note2Chat: Improving LLMs for Multi-Turn Clinical History Taking Using Medical Notes](https://arxiv.org/abs/2601.21551)
*Yang Zhou,Zhenting Sheng,Mingrui Tan,Yuting Song,Jun Zhou,Yu Heng Kwan,Lian Leng Low,Yang Bai,Yong Liu*

Main category: cs.CL

TL;DR: 提出 Note2Chat 框架，通过将真实医疗笔记转化为高质量医生-患者对话，结合三阶段微调和单次推理范式，显著提升临床推理能力，诊断性能超越 GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在需要动态多轮问诊、迭代假设修正的临床推理场景中表现不足，且依赖难以获得的大规模对话数据；需可扩展、隐私友好且可解释的训练信号。

Method: Note2Chat 以笔记为驱动的对话学习框架：通过决策树引导的生成及 refinement 将真实医疗笔记转化为高质量的医生-患者对话；采用三阶段微调：监督学习、模拟数据增强、偏好学习；提出单次推理范式，将历史问诊视为一系列单次推理问题以提升可解释性、局部监督、动态适应与样本效率。

Result: +16.9 的 F1 提升与 +21.0 的 Top-1 诊断准确率，超越 GPT-4o；提供代码与数据集。

Conclusion: 所提方法显著提升临床推理能力，提升历史记录获取的可解释性、样本效率和动态适应性，具有潜在的临床应用与研究价值。

Abstract: Effective clinical history taking is a foundational yet underexplored component of clinical reasoning. While large language models (LLMs) have shown promise on static benchmarks, they often fall short in dynamic, multi-turn diagnostic settings that require iterative questioning and hypothesis refinement. To address this gap, we propose \method{}, a note-driven framework that trains LLMs to conduct structured history taking and diagnosis by learning from widely available medical notes. Instead of relying on scarce and sensitive dialogue data, we convert real-world medical notes into high-quality doctor-patient dialogues using a decision tree-guided generation and refinement pipeline. We then propose a three-stage fine-tuning strategy combining supervised learning, simulated data augmentation, and preference learning. Furthermore, we propose a novel single-turn reasoning paradigm that reframes history taking as a sequence of single-turn reasoning problems. This design enhances interpretability and enables local supervision, dynamic adaptation, and greater sample efficiency. Experimental results show that our method substantially improves clinical reasoning, achieving gains of +16.9 F1 and +21.0 Top-1 diagnostic accuracy over GPT-4o. Our code and dataset can be found at https://github.com/zhentingsheng/Note2Chat.

</details>


### [27] [KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices](https://arxiv.org/abs/2601.21579)
*Wuyang Zhou,Yuxuan Gu,Giorgos Iacovides,Danilo Mandic*

Main category: cs.CL

TL;DR: KromHC uses Kronecker products of smaller doubly stochastic matrices to parametrize the residuals in manifold-constrained Hyper-Connections, achieving exact double stochasticity with far fewer parameters and competitive performance.


<details>
  <summary>Details</summary>
Motivation: HC and mHC suffer from training instability, limited scalability, and either non-exact doubly stochasticity (Sinkhorn-Knopp) or prohibitive parameter complexity. mHC-lite fixes stochasticity via Birkhoff-von-Neumann but incurs factorial parameter growth. A scalable alternative addressing exactness and complexity is needed.

Method: Parametrize the residual matrix with Kronecker products of small doubly stochastic matrices. Impose manifold constraints across factor matrices along each mode of the tensorized residual stream to ensure exact double stochasticity. This yields parameter complexity O(n^2C) instead of O(n^3C) or O(nC n!).

Result: Empirical evaluations show KromHC matches or surpasses state-of-the-art mHC variants while using significantly fewer trainable parameters (O(n^2C) complexity). The authors provide code at the given GitHub link.

Conclusion: KromHC successfully resolves the exact double stochasticity and parameter-efficiency trade-offs in hyper-connected residuals, offering scalable, effective performance comparable to or better than existing mHC approaches.

Abstract: The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive $\mathcal{O}(n^3C)$ parameter complexity with $n$ as the width of the residual stream and $C$ as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, $\mathcal{O} \left( nC \cdot n! \right)$. To address both challenges, we propose \textbf{KromHC}, which uses the \underline{Kro}necker products of smaller doubly stochastic matrices to parametrize the residual matrix in \underline{mHC}. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to $\mathcal{O}(n^2C)$. Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at \texttt{https://github.com/wz1119/KromHC}.

</details>


### [28] [ILRR: Inference-Time Steering Method for Masked Diffusion Language Models](https://arxiv.org/abs/2601.21647)
*Eden Avrahami,Eliya Nachmani*

Main category: cs.CL

TL;DR: 提出 Iterative Latent Representation Refinement (ILRR)，一种学习无关的对齐机制，通过在去噪过程中动态对齐目标参考序列的内部激活来控制离散扩散语言模型的属性。并给出 Spatially Modulated Steering 以允许用短参考实现长文本的分布式引导。


<details>
  <summary>Details</summary>
Motivation: 解决推理阶段对离散扩散语言模型的有效控制问题。现有方法多依赖采样层面的引导或轨迹优化，缺乏一个无需额外训练、可用单参考序列实现灵活属性控制的方案。

Method: 在去噪过程的各阶段，动态将生成序列的内部激活与给定参考的激活对齐，作为引导信号；引入一个可调的 steering scale 来调控属性强度； Spatially Modulated Steering 在序列中不同位置应用不同强度，以短参考实现对长文本的分布式控制；每步额外需要一次并行前向传播，计算开销很小。

Result: 在 LLaDA 和 MDLM 架构上实现有效的属性引导；在相同的计算预算下，属性准确率提升约 10 到 60 百分点，且保持较高的生成质量。

Conclusion: ILRR 提供一种低开销、可调控的控制框架，适用于 DLM 的推理阶段； Spatially Modulated Steering 扩展了对长文本的控制能力，具有广泛应用潜力。

Abstract: Discrete Diffusion Language Models (DLMs) offer a promising non-autoregressive alternative for text generation, yet effective mechanisms for inference-time control remain relatively underexplored. Existing approaches include sampling-level guidance procedures or trajectory optimization mechanisms. In this work, we introduce Iterative Latent Representation Refinement (ILRR), a learning-free framework for steering DLMs using a single reference sequence. ILRR guides generation by dynamically aligning the internal activations of the generated sequence with those of a given reference throughout the denoising process. This approach captures and transfers high-level semantic properties, with a tunable steering scale enabling flexible control over attributes such as sentiment. We further introduce Spatially Modulated Steering, an extension that enables steering long texts using shorter references by regulating guidance intensity across the sequence. Empirically, we demonstrate that ILRR achieves effective attribute steering on LLaDA and MDLM architectures with a minor computational overhead, requiring only one additional parallel forward pass per denoising step. Under the same compute budget, ILRR improves attribute accuracy over comparable baselines by 10$\%$ to 60$\%$ points, while maintaining high generation quality.

</details>


### [29] [AdaptBPE: From General Purpose to Specialized Tokenizers](https://arxiv.org/abs/2601.21665)
*Vijini Liyanage,François Yvon*

Main category: cs.CL

TL;DR: 通过后训练的自适应词表替换策略，在给定词汇规模下选择最能有效编码适配语料的标记集合，以提升特定领域或任务的编码效率和性能。


<details>
  <summary>Details</summary>
Motivation: 通用分词器在特定领域或语言上的效率较低，需一种轻量、可定制的词汇层优化方法来提高压缩率和下游任务表现。

Method: 基于目标域/任务的适配语料，识别并替换低效标记，选择最能编码该语料的词标记集合，达到给定词汇表大小的最优编码效果，类似词汇微调的后训练过程。

Result: 在生成和分类任务上，改进后的词表比同等词汇规模的基线更有效地压缩测试语料，且在多语言场景下表现稳健。

Conclusion: 提供了一种轻量级的领域/任务定制化词汇适配机制，便于对现有模型进行高效的词汇层优化；代码公开，便于复现与应用。

Abstract: Subword tokenization methods, such as Byte-Pair Encoding (BPE), significantly impact the performance and efficiency of large language models (LLMs). The standard approach involves training a general-purpose tokenizer that uniformly processes all textual data during both training and inference. However, the use of a generic set of tokens can incur inefficiencies when applying the model to specific domains or languages. To address this limitation, we propose a post-training adaptation strategy that selectively replaces low-utility tokens with more relevant ones based on their frequency in an adaptation corpus. Our algorithm identifies the token inventory that most effectively encodes the adaptation corpus for a given target vocabulary size. Extensive experiments on generation and classification tasks across multiple languages demonstrate that our adapted tokenizers compress test corpora more effectively than baselines using the same vocabulary size. This method serves as a lightweight adaptation mechanism, akin to a vocabulary fine-tuning process, enabling optimized tokenization for specific domains or tasks. Our code and data are available at https://github.com/vijini/Adapt-BPE.git.

</details>


### [30] [Scale-Dependent Semantic Dynamics Revealed by Allan Deviation](https://arxiv.org/abs/2601.21678)
*Debayan Dasgupta*

Main category: cs.CL

TL;DR: 将语义进展视为高维状态的随机轨迹，使用 Allan 方差揭示两段动态：短时幂律的区分性（创意文学 vs 技术文本）与长时稳定性受限的噪声底；大语言模型虽能再现局部尺度统计，但稳定性区间受限，形成区分人类认知与模型文本的可量化框架。


<details>
  <summary>Details</summary>
Motivation: 明确量化文本语义进展的动力学特性，探究语义连贯性是否可被视为物理性质，并据此区分人类文本与算法生成文本。

Method: 将有序句子嵌入视为位移信号，使用 Allan 方差分析在不同时间尺度上的稳定性；比较创意文学与技术文本的动态特征，并评估大语言模型在相同指标下的表现。

Result: 发现两种动力学：短时呈现幂律比例关系，区分创意与技术文本；长时转入稳定性受限的噪声底；大语言模型再现局部尺度统计，但稳定性视界明显缩小。

Conclusion: 将语义连贯性定义为可测量的物理量，提供区分人类认知与算法模型文本动力学的量化框架。

Abstract: While language progresses through a sequence of semantic states, the underlying dynamics of this progression remain elusive. Here, we treat the semantic progression of written text as a stochastic trajectory in a high-dimensional state space. We utilize Allan deviation, a tool from precision metrology, to analyze the stability of meaning by treating ordered sentence embeddings as a displacement signal. Our analysis reveals two distinct dynamical regimes: short-time power-law scaling, which differentiates creative literature from technical texts, and a long-time crossover to a stability-limited noise floor. We find that while large language models successfully mimic the local scaling statistics of human text, they exhibit a systematic reduction in their stability horizon. These results establish semantic coherence as a measurable physical property, offering a framework to differentiate the nuanced dynamics of human cognition from the patterns generated by algorithmic models.

</details>


### [31] [Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.21684)
*Xinglin Wang,Jiayi Shi,Shaoxiong Feng,Peiwen Yuan,Yiwei Li,Yueqi Zhang,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.CL

TL;DR: RSE将测试时搜索转化为一个累积、无训练的过程，通过构建体验库来回收有用的结论并 prune 停留死角，在推理复杂任务时提高计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有搜索把 rollout 当作一次性样本，导致重复推导和重复尝试中的无效路径回溯，亟需通过重用中间结论来降低冗余并提升测试时推理的可扩展性。

Method: 提出 Recycling Search Experience (RSE)：一种自引导、无训练成本的策略，将原始轨迹持续蒸馏为共享的经验库；积极回收正确结论以加速推理，消极回收失败模式以 prune 已遇到的死角，使测试时搜索成为累积过程。

Result: 理论上给出对与独立采样相比的效率提升的分析；在 HMMT24/25、IMO-Bench、HLE 上的广泛实验表明，RSE 在可比计算量下持续优于强基线，达到最先 赖扩展性。

Conclusion: RSE 提供了一种可扩展的测试时推理范式，通过利用可重复使用的搜索经验，减少冗余推导并 prune 死角，从而在复杂推理任务上实现更高效的推理能力。

Abstract: Test-Time Scaling enhances the reasoning capabilities of Large Language Models by allocating additional inference compute to broaden the exploration of the solution space. However, existing search strategies typically treat rollouts as disposable samples, where valuable intermediate insights are effectively discarded after each trial. This systemic memorylessness leads to massive computational redundancy, as models repeatedly re-derive discovered conclusions and revisit known dead ends across extensive attempts. To bridge this gap, we propose \textbf{Recycling Search Experience (RSE)}, a self-guided, training-free strategy that turns test-time search from a series of isolated trials into a cumulative process. By actively distilling raw trajectories into a shared experience bank, RSE enables positive recycling of intermediate conclusions to shortcut redundant derivations and negative recycling of failure patterns to prune encountered dead ends. Theoretically, we provide an analysis that formalizes the efficiency gains of RSE, validating its advantage over independent sampling in solving complex reasoning tasks. Empirically, extensive experiments on HMMT24, HMMT25, IMO-Bench, and HLE show that RSE consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art scaling efficiency.

</details>


### [32] [Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents](https://arxiv.org/abs/2601.21699)
*Hojae Han,Heeyun Jung,Jongyoon Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: In resource-constrained settings with small language models, the paper introduces DAVID-GRPO, a budget-efficient RL framework that stabilizes early learning, assigns retrieval credit via evidence recall, and resamples truncated near-miss trajectories to improve exploration, achieving strong multi-hop reasoning performance on QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Real-world constraints limit large models and dense exploration; the paper investigates whether small LMs can still perform robust multi-hop reasoning under budget constraints and aims to close the performance gap with larger models.

Method: DAVID-GRPO framework: (i) stabilizes early learning with minimal supervision; (ii) assigns retrieval credit based on evidence recall; (iii) improves exploration by resampling truncated near-miss trajectories.

Result: Evaluated on agents up to 1.5B parameters trained on four RTX 3090 GPUs; consistently outperforms prior RL methods designed for large-scale settings on six multi-hop QA benchmarks.

Conclusion: With appropriate inductive biases, small agents can achieve low training cost with high accuracy in multi-hop reasoning tasks.

Abstract: While reinforcement learning (RL) has empowered multi-turn reasoning agents with retrieval and tools, existing successes largely depend on extensive on-policy rollouts in high-cost, high-accuracy regimes. Under realistic resource constraints that cannot support large models or dense explorations, however, small language model agents fall into a low-cost, low-accuracy regime, where limited rollout budgets lead to sparse exploration, sparse credit assignment, and unstable training. In this work, we challenge this trade-off and show that small language models can achieve strong multi-hop reasoning under resource constraints. We introduce DAVID-GRPO, a budget-efficient RL framework that (i) stabilizes early learning with minimal supervision, (ii) assigns retrieval credit based on evidence recall, and (iii) improves exploration by resampling truncated near-miss trajectories. Evaluated on agents up to 1.5B parameters trained on only four RTX 3090 GPUs, DAVID-GRPO consistently outperforms prior RL methods designed for large-scale settings on six multi-hop QA benchmarks. These results show that with the right inductive biases, small agents can achieve low training cost with high accuracy.

</details>


### [33] [Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis](https://arxiv.org/abs/2601.21709)
*Qingyue Yang,Jie Wang,Xing Li,Yinqi Bai,Xialiang Tong,Huiling Zhen,Jianye Hao,Mingxuan Yuan,Bin Li*

Main category: cs.CL

TL;DR: TAPPA unifiedly analyzes attention patterns by temporal predictability to explain predictable and unpredictable attention patterns, with analyses of interactions among queries, keys, and RoPE, and applies to KV cache compression and pruning to improve inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing work provides fragmented observations (e.g., retrieval heads, sink heads, diagonal traces) without a unifying framework to explain them across time. A unified, temporally continuous perspective is needed.

Method: From a temporally continuous viewpoint, classify attention patterns into predictable and effectively random categories. Quantify query self–similarity along the temporal dimension. Provide detailed mathematical analysis for three representative cases involving queries, keys, and RoPE, and validate through KV cache compression and LLM pruning tasks.

Result: TAPPA framework established; a simple metric inspired by TAPPA improves performance over baselines in KV cache compression and LLM pruning tasks.

Conclusion: A unifying theory of attention patterns that aids understanding and guides inference acceleration, with publicly available code.

Abstract: Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce \textbf{Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations} from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA.

</details>


### [34] [TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2601.21711)
*Huiyuan Lai,Malvina Nissim*

Main category: cs.CL

TL;DR: 提出 TACLer，一种模型定制的课程式强化学习框架，通过逐步提升数据复杂度，并结合“思考/不思考”推理模式，在提高推理准确度的同时显著降低训练和推理成本，且在四个数学数据集上超越对照模型。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长链式推理（CoT）中对大量RL训练的需求，同时避免冗余中间步骤导致的过度推理与资源浪费，提升学习与推理效率。

Method: 提出两大核心组件： (1) 定制化课程学习，识别模型在各阶段需要学习的知识并分阶段推进；(2) 混合式 Thinking/NoThinking 推理范式，通过开启或关闭 Thinking 模式在准确性与效率之间取得平衡；在多阶段RL训练中逐步提高数据复杂度。

Result: 相较于长时间思考模型，训练计算量降低 >50%，推理时 token 使用量降低 >42%；对基线模型的准确性提升 >9%，在四个具有复杂问题的数学数据集上持续优于 SOTA 的 NoThinking 与 Thinking 基线。

Conclusion: 模型定制的课程式 RL 框架能显著提升学习效率与推理效率，同时提升复杂任务的准确性，具有良好的普适性与扩展潜力，未来可拓展至其他任务领域。

Abstract: Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.

</details>


### [35] [Enhancing Language Models for Robust Greenwashing Detection](https://arxiv.org/abs/2601.21722)
*Neil Heinrich Braun,Keane Ong,Rui Mao,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: 提出一个参数高效的LLM框架，通过对比学习与序数排序、门控特征调制和MetaGradNorm，提升ESG披露的鲁棒性，对比基线表现更佳且显示表达刚性与泛化能力之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 绿色披露中的洗绿（greenwashing）和模糊声明削弱了ESG报告的可信度；现有NLP模型对这类操控缺乏鲁棒性，容易依赖表面模式并在跨类别任务上泛化差。

Method: 将LLM潜在空间结构化：1) 结合对比学习与序数排序目标，捕捉具体行动与模糊声明之间的等级区分；2) 引入门控特征调制以过滤披露噪声；3) 使用MetaGradNorm稳定多目标优化；4) 在跨类别设置下进行实验以评估鲁棒性。

Result: 在跨类别设置中，相较于标准基线，所提框架表现出更高的鲁棒性；并揭示表示的刚性与泛化能力之间存在权衡。

Conclusion: 所提出的参数高效框架提升了对绿色洗销等披露操控的鲁棒性，但可能对表征刚性过度约束而影响泛化，需要在不同任务/场景中权衡。

Abstract: Sustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.

</details>


### [36] [Procedural Pretraining: Warming Up Language Models with Abstract Data](https://arxiv.org/abs/2601.21725)
*Liangze Jiang,Zachary Shinnick,Anton van den Hengel,Hemanth Saratchandran,Damien Teney*

Main category: cs.CL

TL;DR: 将处理性数据用于预训练可显著提升模型的算法推理与结构化能力，数据效率高于使用自然语言/代码等数据，并可能促进知识获取与推理的分离。


<details>
  <summary>Details</summary>
Motivation: 探究在海量网页文本之外，先暴露于抽象的过程性/结构性数据，是否能更高效地获得语义知识与推理能力，类似人类从基础逻辑和数学开始再进行更高层推理。

Method: 1) 以形式语言（如 Dyck 括号序列）等简单算法生成程序性数据，进行预训练，评估在 Needle-in-a-haystack 等算法性任务上的提升；2) 将规模扩展至最大约 1.3B 参数，发现前置仅 0.1% 的程序性数据即可超越标准预训练（C4、CodeParrot、DeepMind-Math 数据集）的效果；3) 分析注意力与 MLP 层的内部机制，揭示结构化领域与语言任务的不同贡献；4) 探讨将多种程序性数据形式组合的可能性。

Result: 程序性数据能显著提升算法技能，例如在 Needle-in-a-haystack 任务中，Dyck 序列将准确率从 10% 提升到 98%；对小规模与大规模模型均有数据效率优势，前置 0.1% 程序性数据即可实现优于常规预训练的效果，且达到等同损失所需数据量可减至原始数据的 55-86%；分析显示程序性预训练在注意力与 MLP 层会形成非平凡结构，前者对结构化领域（如代码）尤为重要，后者对语言任务较为关键；并提出了将多种程序性数据形式结合的路径。

Conclusion: 程序性预训练是一个简单、轻量且有效的手段，能提升性能并加速预训练，暗示知识获取与推理的潜在分离，并为将来将多种程序性数据融合以加强模型能力奠定基础。

Abstract: Pretraining directly on web-scale corpora is the de facto paradigm for building language models. We study an alternative setting where the model is initially exposed to abstract structured data, as a means to ease the subsequent acquisition of rich semantic knowledge, much like humans learn simple logic and mathematics before higher reasoning. We specifically focus on procedural data, generated by formal languages and other simple algorithms, as such abstract data.
  We first diagnose the algorithmic skills that different forms of procedural data can improve, often significantly. For example, on context recall (Needle-in-a-haystack), the accuracy jumps from 10 to 98% when pretraining on Dyck sequences (balanced brackets). Second, we study how these gains are reflected in pretraining larger models (up to 1.3B). We find that front-loading as little as 0.1% procedural data significantly outperforms standard pretraining on natural language, code, and informal mathematics (C4, CodeParrot, and DeepMind-Math datasets). Notably, this procedural pretraining enables the models to reach the same loss value with only 55, 67, 86% of the original data. Third, we explore the mechanisms behind and find that procedural pretraining instils non-trivial structure in both attention and MLP layers. The former is particularly important for structured domains (e.g. code), and the latter for language. Finally, we lay a path for combining multiple forms of procedural data. Our results show that procedural pretraining is a simple, lightweight means to improving performance and accelerating language model pretraining, ultimately suggesting the promise of disentangling knowledge acquisition from reasoning in LLMs.

</details>


### [37] [CE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering](https://arxiv.org/abs/2601.21733)
*Jiayin Lan,Jiaqi Li,Baoxin Wang,Ming Liu,Dayong Wu,Shijin Wang,Bing Qin,Guoping Hu*

Main category: cs.CL

TL;DR: CE-GOCD 提出以论文标题为中心实体的子图检索、子图剪枝与补全，以及社区检测来对学术知识图进行结构化分析，以增强 LLM 在科学问题回答中的检索-增强能力，在三个 NLP 文献 QA 数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强方法常依赖分散的文本块或概念，未能捕捉论文之间的深层语义关系，导致回答的全面性和针对性不足。需要显式建模学术知识图中的语义子结构与社区结构以提升理解与推理能力。

Method: 1) 将论文标题作为中心实体，作为子图检索的锚点；2) 通过子图剪枝与补全提升隐含语义的发现；3) 应用社区检测提炼出具有共主题的论文群组；4) 将上述子结构信息融入对 LLM 的问答过程以增强回答质量。

Result: 在三个 NLP 文献基于的问题回答数据集上评估，CE-GOCD 相比其他检索增强基线具有显著优势，验证了框架的有效性。

Conclusion: 通过显式建模论文之间的语义子结构与社区结构，CE-GOCD 能提升 LLM 对科学文献的理解与回答的全面性与针对性，具有良好的可扩展性与适用性。

Abstract: Large Language Models (LLMs) are increasingly used for question answering over scientific research papers. Existing retrieval augmentation methods often rely on isolated text chunks or concepts, but overlook deeper semantic connections between papers. This impairs the LLM's comprehension of scientific literature, hindering the comprehensiveness and specificity of its responses. To address this, we propose Central Entity-Guided Graph Optimization for Community Detection (CE-GOCD), a method that augments LLMs' scientific question answering by explicitly modeling and leveraging semantic substructures within academic knowledge graphs. Our approach operates by: (1) leveraging paper titles as central entities for targeted subgraph retrieval, (2) enhancing implicit semantic discovery via subgraph pruning and completion, and (3) applying community detection to distill coherent paper groups with shared themes. We evaluated the proposed method on three NLP literature-based question-answering datasets, and the results demonstrate its superiority over other retrieval-augmented baseline approaches, confirming the effectiveness of our framework.

</details>


### [38] [CoFrGeNet: Continued Fraction Architectures for Language Generation](https://arxiv.org/abs/2601.21766)
*Amit Dhurandhar,Vijil Chenthamarakshan,Dennis Wei,Tejaswini Pedapati,Karthikeyan Natesan Ramamurthy,Rahul Nair*

Main category: cs.CL

TL;DR: 提出一种基于连续分数的新函数类(CoFrGeNets)，作为Transformer的替代组件，通过可替换Attention和FFN，并显著减少参数和预训练时间，在GPT2-xl和Llama3上的实验显示竞争力或优于原模型。


<details>
  <summary>Details</summary>
Motivation: 旨在降低Transformer模型的参数规模和训练成本，同时保持或提升性能，提出与连续分数相关的新函数类及其架构组件，作为Attention/FFN的替代。

Method: 提出基于连续分数的函数类并设计可替代Multi-head Attention和Feed-Forward的体系结构组件；推导自定义梯度以更准确高效地优化；提供即插即用的替换，训练/推理过程改动最小；在GPT2-xl(1.5B)与Llama3(3.2B)上进行预训练/数据混合数据集的实验。

Result: 模型在下游分类、问答、推理与文本理解任务上具有竞争力，甚至在某些任务优于原始模型；参数量仅为原模型的约2/3到1/2，且预训练时间更短。

Conclusion: CoFrGeNets提供了参数效率高的Transformer替代方案，具实际工业可应用潜力；未来的硬件定制实现可能进一步挖掘潜力。

Abstract: Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\frac{2}{3}$ to $\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.

</details>


### [39] [Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond](https://arxiv.org/abs/2601.21767)
*Wei Zhu*

Main category: cs.CL

TL;DR: 对比评估大语言模型ChatGPT在4类医疗信息抽取（MedIE）任务上的能力，涵盖6个基准数据集，系统性分析其性能、可解释性、置信度、忠实性与不确定性；结果显示ChatGPT在MedIE任务上的表现落后于微调基线模型，但在解释性和文本忠实性方面表现较好，且存在过度自信和生成不确定性对抽取结果的影响。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在医疗信息抽取领域的综合能力，探究其性能与可解释性等维度，以评估在实际医疗信息提取任务中的可用性与局限。

Method: 在4类MedIE任务上对6个基准数据集进行系统性评估，测量ChatGPT的性能、可解释性、置信度、忠实性和不确定性，并与微调后的基线模型进行对比。

Result: a) ChatGPT在MedIE任务上的性能分数落后于微调基线模型；b) ChatGPT可给出高质量的决策解释，但对预测存在过度自信；c) 在大多数情形下，ChatGPT对原文具有高忠实性；d) 生成中的不确定性会传导到信息抽取结果中，可能限制在MedIE任务中的应用。

Conclusion: 总体而言，ChatGPT在MedIE任务上尚未达到专门微调模型的水平，但具备较好解释性与文本忠实性。其不确定性和过度自信等问题需通过系统提示设计、结合外部知识、或以集成方法缓解，以提升实用性。

Abstract: Large Language Models (LLMs) like ChatGPT have demonstrated amazing capabilities in comprehending user intents and generate reasonable and useful responses. Beside their ability to chat, their capabilities in various natural language processing (NLP) tasks are of interest to the research community. In this paper, we focus on assessing the overall ability of ChatGPT in 4 different medical information extraction (MedIE) tasks across 6 benchmark datasets. We present the systematically analysis by measuring ChatGPT's performance, explainability, confidence, faithfulness, and uncertainty. Our experiments reveal that: (a) ChatGPT's performance scores on MedIE tasks fall behind those of the fine-tuned baseline models. (b) ChatGPT can provide high-quality explanations for its decisions, however, ChatGPT is over-confident in its predcitions. (c) ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. (d) The uncertainty in generation causes uncertainty in information extraction results, thus may hinder its applications in MedIE tasks.

</details>


### [40] [Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention](https://arxiv.org/abs/2601.21768)
*Alon Rozental*

Main category: cs.CL

TL;DR: 提出 Zonkey，一种端到端可微分的分词-层次扩散模型，从原始字符到文档级表征，实现可自适应、端到端优化的文本生成/表示。


<details>
  <summary>Details</summary>
Motivation: 解决固定、不可微的分词器（如 BPE）限制，阻碍端到端优化、对噪声或领域特定数据的适应，以及希望实现可微分、可学习的分词与层级表征。

Method: 核心是 Segment Splitter（可微分分词器）学习 BOS 决策，基于 Probabilistic Attention（带位置存在概率的软掩码，近似无限序列的软 masking）实现，可微分地决定分段；序列以概率衰减而非结束符，支持可变长度输出。层级结构：从字符 n-gram 到词级向量再到句子级表示；DDMM，用于在潜在空间中稳定高效去噪；Stitcher 实现段与段之间的重叠不变性；端到端在维基百科数据上训练；从噪声生成连贯文本。

Result: 能够从噪声生成可变长度文本，出现层级化的表征，自然 emerged 分层结构，与基于熵的可学习分词器相比，在数据分布的定性对齐方面表现更好。

Conclusion: 推动朝着全梯度优化的 LLMs 迈进，具备更好的领域适应性和可扩展的文本生成潜力；并开源实现。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet they remain constrained by fixed, non-differentiable tokenizers like Byte Pair Encoding (BPE), which hinder end-to-end optimization and adaptability to noisy or domain-specific data. We introduce Zonkey, a hierarchical diffusion model that addresses these limitations through a fully trainable pipeline from raw characters to document-level representations. At its core is a differentiable tokenizer (Segment Splitter) that learns probabilistic beginning-of-sequence (BOS) decisions, enabling adaptive splits that emerge as linguistically meaningful (e.g., word boundaries at spaces, sentence starts at periods) without explicit supervision. This differentiability is enabled by our novel Probabilistic Attention mechanism, which incorporates position-specific existence probabilities to simulate soft masking over theoretically infinite sequences while preserving gradients. Sequences decay probabilistically rather than relying on end-of-sequence tokens, supporting variable-length outputs. Hierarchical levels compress sequences into higher abstractions (e.g., character n-grams to word-like vectors, then sentence-like), with reconstruction via our Denoising Diffusion Mixed Model (DDMM) for stable and efficient denoising in latent space. A Stitcher ensures overlap invariance across segments. Trained end-to-end on Wikipedia, Zonkey generates coherent, variable-length text from noise, demonstrating emergent hierarchies and promising qualitative alignment to data distributions compared to entropy-based learnable tokenizers. Our approach advances toward fully gradient-based LLMs, with potential for better domain adaptation and scalable generation. We release the source code for training and reproducing our experiments.

</details>


### [41] [KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection](https://arxiv.org/abs/2601.21796)
*Yaocong Li,Leihan Zhang,Le Zhang,Qiang Yan*

Main category: cs.CL

TL;DR: 提出 KID：一个知识注入的双头学习框架，用于基于知识推理的有害表情包检测，结合外部知识与可视证据，以提升多语言场景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注单模态或跨模态信号，隐性有害内容往往依赖背景知识，难以通过直接视觉或文本信号获得准确判断，因此需要将外部知识融入 meme 理解以实现更可靠的有害内容检测。

Method: KID 引入标签约束蒸馏，将复杂的 meme 理解分解为与视觉证据、背景知识和分类标签相关的结构化推理链；通过知识注入将外部知识 grounding 到 meme 的特定语境；并采用双头架构同时优化语义生成与分类目标，使语言推理与判定边界保持对齐并有较高稳定性。

Result: 在英文、中文和低资源的孟加拉语等五个多语言数据集上实现 SOTA，二分类和多标签任务都超越前人 2.1%~19.7% 的提升；消融实验验证了知识注入和双头学习的互补性与有效性。

Conclusion: KID 有效地将知识注入与双头学习相结合，提升了对 meme 背景信息的利用能力和判定稳定性，具有较强的泛化能力和实用潜力，代码与数据对外公开。

Abstract: Internet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%--19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at https://github.com/PotatoDog1669/KID.

</details>


### [42] [RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes](https://arxiv.org/abs/2601.21803)
*Korbinian Randl,Guido Rocchietti,Aron Henriksson,Ziawasch Abedjan,Tony Lindgren,John Pavlopoulos*

Main category: cs.CL

TL;DR: RAG-E 提供端到端可解释性框架，通过集成梯度、PMCSHAP、WARG 等方法对检索器-生成器对齐进行量化分析，揭示 RAG 系统中检索排序与生成器文档使用之间的错配，并提供审计工具。


<details>
  <summary>Details</summary>
Motivation: 高风险领域对 RAG 系统的透明度和可控性要求高，但现有组件间的交互缺乏可解释性，影响部署。

Method: 将 Integrated Gradients 适用于检索器分析；提出 PMCSHAP（Monte Carlo-stabilized Shapley Value approximation）用于生成器归因；提出 Weighted Attribution-Relevance Gap (WARG) 衡量生成器对齐检索器排序的程度。

Result: 在 TREC CAsT 和 FoodSafeSum 数据集上发现显著错配：47.4%-66.7% 的查询中生成器忽略检索器的 Top 文档，48.1%-65.9% 依赖于较不相关的文档。

Conclusion: RAG 系统的输出质量不仅取决于单一组件，还取决于它们的相互作用；RAG-E 提供可审计的方法来分析这一对齐关系。

Abstract: Retrieval-Augmented Generation (RAG) systems combine dense retrievers and language models to ground LLM outputs in retrieved documents. However, the opacity of how these components interact creates challenges for deployment in high-stakes domains. We present RAG-E, an end-to-end explainability framework that quantifies retriever-generator alignment through mathematically grounded attribution methods. Our approach adapts Integrated Gradients for retriever analysis, introduces PMCSHAP, a Monte Carlo-stabilized Shapley Value approximation, for generator attribution, and introduces the Weighted Attribution-Relevance Gap (WARG) metric to measure how well a generator's document usage aligns with a retriever's ranking. Empirical analysis on TREC CAsT and FoodSafeSum reveals critical misalignments: for 47.4% to 66.7% of queries, generators ignore the retriever's top-ranked documents, while 48.1% to 65.9% rely on documents ranked as less relevant. These failure modes demonstrate that RAG output quality depends not solely on individual component performance but on their interplay, which can be audited via RAG-E.

</details>


### [43] [Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning](https://arxiv.org/abs/2601.21804)
*Bodong Du,Xuanqi Huang,Xiaomeng Li*

Main category: cs.CL

TL;DR: DARE replaces majority-vote reward signals with a distribution-aware estimate over the entire rollout distribution in test-time RL for LLMs, augmented with an exploration bonus and distribution pruning to improve robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Majority voting collapses the rollout distribution into a single outcome, discarding valid non-majority but correct actions and biasing rewards. A distribution-aware approach can leverage the full information contained in rollouts for self-improvement on unlabeled inputs.

Method: Introduce Distribution-Aware Reward Estimation (DARE) that uses the full empirical rollout distribution to estimate rewards. Enhance with an exploration bonus to encourage diverse exploration and a distribution pruning mechanism to denoise rewards and promote useful non-majority rollouts.

Result: Empirical results on challenging reasoning benchmarks show improved optimization stability and final performance, with relative gains of 25.3% on AIME 2024 and 5.3% on AMC over recent baselines.

Conclusion: DARE demonstrates that leveraging the full rollout distribution yields more informative reward signals for TTRL, leading to more robust learning; distribution-aware reward estimation is a promising direction for LLM self-improvement, albeit with potential computational overhead.

Abstract: Test-time reinforcement learning (TTRL) enables large language models (LLMs) to self-improve on unlabeled inputs, but its effectiveness critically depends on how reward signals are estimated without ground-truth supervision. Most existing TTRL methods rely on majority voting (MV) over rollouts to produce deterministic rewards, implicitly assuming that the majority rollout provides a reliable learning signal. We show that this assumption is fragile: MV reduces the rollout distribution into a single outcome, discarding information about non-majority but correct actions candidates, and yields systematically biased reward estimates. To address this, we propose Distribution-AwareReward Estimation (DARE), which shifts reward estimation from a single majority outcome to the full empirical rollout distribution. DARE further augments this distribution-based reward with an exploration bonus and a distribution pruning mechanism for non-majority rollout exploration and reward denoise, yielding a more informative and robust reward estimation. Extensive experiments on challenging reasoning benchmarks show that DARE improves optimization stability and final performance over recent baselines, achieving relative improvements of 25.3% on challenging AIME 2024 and 5.3% on AMC.

</details>


### [44] [Mil-SCORE: Benchmarking Long-Context Geospatial Reasoning and Planning in Large Language Models](https://arxiv.org/abs/2601.21826)
*Aadi Palnitkar,Mingyang Mao,Nicholas Waytowich,Vinicius G. Goecks,Tinoosh Mohsenin,Xiaomin Lin*

Main category: cs.CL

TL;DR: MilSCORE 是一个面向长期上下文的、专家撰写的多跳问题、基于复杂仿真军事情景的跨模态基准；揭示当前大模型在长时上下文下的规划能力不足，提供基线评估与挑战性测试床。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在更长且更复杂任务中的应用，对能选择性阅读并整合异构多模态信息的现实世界级长上下文基准需求增加。尤其在地理空间规划任务（如大规模军事行动）中，需要在地图、命令、情报报告等分布式数据之间进行快速、准确推理。

Method: MilSCORE 作为首个面向场景级别的基准，包含专家撰写、与复杂军事规划情景相关的多跳问题；问题类型跨七类，覆盖事实回忆与多步推理（约束、策略、空间分析等），并在一个用于训练的仿真情景中进行 grounding。同时提供评估协议，并对当代视觉-语言模型给出基线结果。

Result: 研究发现 MilSCORE 存在明显的挑战性，当前系统在现实世界级的长时上下文规划上仍有较大差距；对多模态、跨源信息的整合能力有较高要求。

Conclusion: MilSCORE 作为一个具有挑战性的测试床，推动未来在长上下文推理、多模态信息整合以及高风险决策场景中的研究与改进。

Abstract: As large language models (LLMs) are applied to increasingly longer and more complex tasks, there is a growing need for realistic long-context benchmarks that require selective reading and integration of heterogeneous, multi-modal information sources. This need is especially acute for geospatial planning problems, such as those found in planning for large-scale military operations, which demand fast and accurate reasoning over maps, orders, intelligence reports, and other distributed data. To address this gap, we present MilSCORE (Military Scenario Contextual Reasoning), to our knowledge the first scenario-level dataset of expert-authored, multi-hop questions grounded in a complex, simulated military planning scenario used for training. MilSCORE is designed to evaluate high-stakes decision-making and planning, probing LLMs' ability to combine tactical and spatial reasoning across multiple sources and to reason over long-horizon, geospatially rich context. The benchmark includes a diverse set of question types across seven categories targeting both factual recall and multi-step reasoning about constraints, strategy, and spatial analysis. We provide an evaluation protocol and report baseline results for a range of contemporary vision-language models. Our findings highlight substantial headroom on MilSCORE, indicating that current systems struggle with realistic, scenario-level long-context planning, and positioning MilSCORE as a challenging testbed for future work.

</details>


### [45] [Embodied Task Planning via Graph-Informed Action Generation with Large Lanaguage Model](https://arxiv.org/abs/2601.21841)
*Xiang Li,Ning Yan,Masood Mortazavi*

Main category: cs.CL

TL;DR: GiG introduces a Graph-in-Graph memory framework for embodied planning, using GNN-based state embeddings organized into action-connected execution trace graphs within a memory bank, with structure-aware priors from clustering and a bounded lookahead module with symbolic transitions to improve long-horizon planning. Evaluated on Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld, achieving substantial Pass@1 gains with competitive cost.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long-horizon, constraint-adherent planning in embodied environments due to limited context and hallucinations. There is a need to ground planning in structured memory of past experiences and environment dynamics to preserve strategy coherence.

Method: Introduce GiG: a Graph-in-Graph architecture where a Graph Neural Network encodes environmental states into embeddings. These embeddings form action-connected execution trace graphs stored in an experience memory bank. Clustering these graph embeddings yields structure-aware priors to ground decisions in relevant past patterns. A bounded lookahead module leveraging symbolic transition logic further constrains and guides action projection.

Result: Empirical evaluation on Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld shows GiG outperforms state-of-the-art baselines with Pass@1 gains: up to 22% on Robotouille Synchronous, 37% on Asynchronous, and 15% on ALFWorld, with comparable or lower computational cost.

Conclusion: GiG effectively improves long-horizon embodied planning by structuring memory into a Graph-in-Graph architecture, enabling retrieval of structure-aware priors and grounding decisions via symbolic transitions, leading to notable performance gains with cost efficiency.

Abstract: While Large Language Models (LLMs) have demonstrated strong zero-shot reasoning capabilities, their deployment as embodied agents still faces fundamental challenges in long-horizon planning. Unlike open-ended text generation, embodied agents must decompose high-level intent into actionable sub-goals while strictly adhering to the logic of a dynamic, observed environment. Standard LLM planners frequently fail to maintain strategy coherence over extended horizons due to context window limitation or hallucinate transitions that violate constraints. We propose GiG, a novel planning framework that structures embodied agents' memory using a Graph-in-Graph architecture. Our approach employs a Graph Neural Network (GNN) to encode environmental states into embeddings, organizing these embeddings into action-connected execution trace graphs within an experience memory bank. By clustering these graph embeddings, the framework enables retrieval of structure-aware priors, allowing agents to ground current decisions in relevant past structural patterns. Furthermore, we introduce a novel bounded lookahead module that leverages symbolic transition logic to enhance the agents' planning capabilities through the grounded action projection. We evaluate our framework on three embodied planning benchmarks-Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld. Our method outperforms state-of-the-art baselines, achieving Pass@1 performance gains of up to 22% on Robotouille Synchronous, 37% on Asynchronous, and 15% on ALFWorld with comparable or lower computational cost.

</details>


### [46] [Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text](https://arxiv.org/abs/2601.21895)
*Hongyi Zhou,Jin Zhu,Erhan Xu,Kai Ye,Ying Yang,Chengchun Shi*

Main category: cs.CL

TL;DR: 提出一种自适应距离的重写检测算法，借助几何视角分析现有重写检测方法并证明自适应距离在检测中的理论优势，且在100+设置的实验中相对于 strongest baseline 在多家主流LLM上取得显著提升（57.8%–80.6%）。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型易产生误导性文本，现有重写检测算法在鲁棒性和泛化性方面存在不足。文章以几何视角解析重写检测机制，并提出自适应学习原文与改写文本之间距离的检测方法以提升鲁棒性与泛化能力。

Method: 1) 给出一个几何框架来理解重写检测算法的工作原理与泛化性。2) 提出一个自适应学习距离的检测模型，学习原文与改写文本之间的距离度量。3) 给出理论论证，证明自适应距离优于固定距离。4) 在超过100组实验设置下进行大规模评估，比较与基线的相对性能。

Result: 在大多数场景中，该方法优于基线算法；相对于最强基线，在不同目标LLM（GPT、Claude、Gemini）下的相对提升区间为57.8%到80.6%。

Conclusion: 基于几何洞见的自适应距离重写检测框架有效提升了检测性能及鲁棒性，具有较强的跨模型泛化潜力和实用性。

Abstract: Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their generalization ability. Building on this insight, we introduce a novel rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text. Theoretically, we demonstrate that employing an adaptively learned distance function is more effective for detection than using a fixed distance. Empirically, we conduct extensive experiments with over 100 settings, and find that our approach demonstrates superior performance over baseline algorithms in the majority of scenarios. In particular, it achieves relative improvements from 57.8\% to 80.6\% over the strongest baseline across different target LLMs (e.g., GPT, Claude, and Gemini).

</details>


### [47] [SONIC: Segmented Optimized Nexus for Information Compression in Key-Value Caching](https://arxiv.org/abs/2601.21927)
*Hong Chen,Xiang Liu,Bo Wang,Yuxuan Fan,Yuanlin Chu,Zongluo Li,Xiaowen Chu,Xuming Hu*

Main category: cs.CL

TL;DR: SONIC: a learning-based KV-cache compressor that encodes historical dialogue into compact Nexus tokens, with dynamic budget training for flexible memory constraints, outperforming baselines and speeding up inference.


<details>
  <summary>Details</summary>
Motivation: KV cache grows linearly in multi-turn LLMs and heuristic eviction risks losing context. A structure-aware, semantically rich compression is needed to preserve coherence under tight memory budgets.

Method: Train a model that maps historical segments into Nexus tokens. Use dynamic budget training to adapt to varying memory limits without retraining. Evaluate on multiple benchmarks (MTBench101 and four others) against baselines like H2O and StreamingLLM.

Result: At 80% and 50% compression, SONIC outperforms baselines on four benchmarks. On MTBench101, average improvement = 35.55% over state-of-the-art. Inference speedup ≈50.1% vs full-context generation.

Conclusion: SONIC effectively preserves semantic integrity of multi-turn dialogues under compressed KV caches and accelerates deployment; dynamic budget training enables flexible deployment under different memory constraints without retraining.

Abstract: The linear growth of Key-Value (KV) cache remains a bottleneck for multi-turn LLM deployment. Existing KV cache compression methods often fail to account for the structural properties of multi-turn dialogues, relying on heuristic eviction that risks losing critical context. We propose \textbf{SONIC}, a learning-based framework that compresses historical segments into compact and semantically rich \textbf{Nexus} tokens. By integrating dynamic budget training, SONIC allows flexible adaptation to varying memory constraints without retraining. Experiments show that at compression ratios of 80\% and 50\%, SONIC consistently outperforms baselines such as H2O and StreamingLLM on four diverse multi-turn benchmarks. Specifically, on the widely used MTBench101 benchmark, SONIC achieves an average score improvement of 35.55\% over state-of-the-art baselines, validating its effectiveness in sustaining coherent multi-turn dialogues. Furthermore, SONIC enhances deployment efficiency, accelerating the overall inference process by 50.1\% compared to full-context generation.

</details>


### [48] [From Generative Modeling to Clinical Classification: A GPT-Based Architecture for EHR Notes](https://arxiv.org/abs/2601.21955)
*Fariba Afrin Irany*

Main category: cs.CL

TL;DR: 使用选择性微调的GPT-2来进行临床文本分类。冻结大部分GPT-2参数，仅训练最后一层Transformer块、最后的LayerNorm和一个轻量分类头，在MIMIC-IV Note的放射学报告上取得稳定收敛和强分类性能，显著降低可训练参数数量与计算开销，尤其在非提及/否定结果的场景。


<details>
  <summary>Details</summary>
Motivation: 解决EHR中非结构化文本的疾病表征、队列识别与决策支持面临的标注匮乏、类别不平衡和高计算成本等挑战。希望通过对预训练生成模型的选择性微调实现高效适配。

Method: 采用解码器/自回归GPT-2结构；冻结大部分背后的参数，仅对最后一个Transformer块、最终LayerNorm及一个轻量分类头进行训练；利用基于CheXpert风格的未确定标签（来自报告文本）进行多标签/二分类/聚合疾病结局任务；在MIMIC-IV-Note radiology reports上进行评估；覆盖多种任务设定。

Result: 在不同数据规模下，模型表现稳定收敛、分类性能强，尤其在非提及和否定结果场景；显著降低可训练参数数量和计算成本。

Conclusion: 选择性微调预训练生成模型是临床文本分类的一条高效可扩展路径，能够在现实EHR数据上实现有效适配并显著降低计算复杂性。

Abstract: The increasing availability of unstructured clinical narratives in electronic health records (EHRs) has created new opportunities for automated disease characterization, cohort identification, and clinical decision support. However, modeling long, domain-specific clinical text remains challenging due to limited labeled data, severe class imbalance, and the high computational cost of adapting large pretrained language models.
  This study presents a GPT-based architecture for clinical text classification that adapts a pretrained decoder-only Transformer using a selective fine-tuning strategy. Rather than updating all model parameters, the majority of the GPT-2 backbone is frozen, and training is restricted to the final Transformer block, the final layer normalization, and a lightweight classification head. This approach substantially reduces the number of trainable parameters while preserving the representational capacity required to model complex clinical language.
  The proposed method is evaluated on radiology reports from the MIMIC-IV-Note dataset using uncertainty-aware CheXpert-style labels derived directly from report text. Experiments cover multiple problem formulations, including multi-label classification of radiographic findings, binary per-label classification under different uncertainty assumptions, and aggregate disease outcome prediction. Across varying dataset sizes, the model exhibits stable convergence behavior and strong classification performance, particularly in settings dominated by non-mention and negated findings.
  Overall, the results indicate that selective fine-tuning of pretrained generative language models provides an efficient and effective pathway for clinical text classification, enabling scalable adaptation to real-world EHR data while significantly reducing computational complexity.

</details>


### [49] [OVD: On-policy Verbal Distillation](https://arxiv.org/abs/2601.21968)
*Jing Xiong,Hui Shen,Shansan Gong,Yuxin Cheng,Jianghan Shen,Chaofan Tao,Haochen Tan,Haoli Bai,Lifeng Shang,Ngai Wong*

Main category: cs.CL

TL;DR: OVD通过使用离散口头分数（0-9）的轨迹匹配，替代逐词对齐的蒸馏，显著提升记忆效率并实现基于策略的教师-学生蒸馏，同时保持探索能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于令牌的按策略蒸馏需要学生和教师之间的逐词对齐，这限制学生的探索能力、难以充分利用交互环境反馈，且在强化学习中造成高内存开销。

Method: 引入离散口头分数（0-9）对教师输出进行轨迹匹配，舍弃逐词概率对齐，采用记忆友好的轨迹级蒸馏框架，允许在策略中对输出空间进行自由探索，并支持在有口头反馈的环境中进行按策略蒸馏。

Result: 在Web问答和数学推理任务上，OVD相较于现有方法获得显著提升：Web Q&A平均正确率（EM）提升约+12.9%绝对值；数学基准提升最高可达+25.7%，且在仅用一个随机样本时也能取得显著收益。此外，训练效率也有所提高。

Conclusion: OVD实现了记忆高效的按策略口头蒸馏，避免了token-level对齐，提升了探索性和样本效率，在多种任务上展现出强于现有方法的性能。

Abstract: Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io

</details>


### [50] [Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding](https://arxiv.org/abs/2601.21969)
*Yifan Zhu,Huiqiang Rong,Haoran Luo*

Main category: cs.CL

TL;DR: 提出 Token-Guard，通过 token-level 自我校验与潜在隐空间评分实现对幻觉的逐步控制，迭代修剪与再生成以降低幻觉并提升准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs 常发生活幻觉，RAG/RLHF 虽能缓解但成本高，解码方法虽轻但缺乏显式幻觉控制，需一种轻量且可控的解码策略。

Method: 在推理每一步进行内部验证以检测并阻止幻觉 token 的传播；对候选片段在潜在隐空间进行幻觉风险评分；通过迭代修剪与再生成进行纠错。

Result: 在 HALU 数据集上实验证明 Token-Guard 能显著降低幻觉、提升生成准确性，具备可扩展性与模块化优点。

Conclusion: Token-Guard 提供一个轻量、可扩展的幻觉控制框架，适合无需大规模微调或检索的场景，提升输出可靠性；代码已开源。

Abstract: Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.

</details>


### [51] [Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units](https://arxiv.org/abs/2601.21996)
*Jianhui Chen,Yuzhang Luo,Liangming Pan*

Main category: cs.CL

TL;DR: 提出 Mechanistic Data Attribution (MDA)，通过影响函数追踪训练样本与可解释单元的关系，证明高影响样本的干预能显著影响可解释头部的形成，并揭示重复结构化数据的催化作用，进而将干预与 ICL 能力联系起来，最终提出用于加速电路收敛的机制性数据增强。


<details>
  <summary>Details</summary>
Motivation: 填补关于训练数据如何驱动可解释电路形成的因果证据的空白，提供一种可扩展的方法从数据层面操控模型内部机制。

Method: 使用 Influence Functions 追踪与可解释单元相关的训练样本，对 Pythia 家族模型进行高影响样本的删除/增量干预；评估对可解释头部的出现、在-context learning (ICL) 能力的影响；解析重复性数据对机制的影响，提出数据增强管线。

Result: 高影响样本干预显著改变可解释头部的形成，随机干预无效；重复结构数据充当机制催化剂；对 induction head 形成的干预伴随 ICL 能力的提升或变化； mechanistic data augmentation 可在不同模型尺度上加速电路收敛。

Conclusion: 建立了一种数据驱动的因果分析框架 MDA，证实训练数据在可解释电路形成中的因果作用，并给出可迁移的数据增强策略以引导大语言模型的发展轨迹。

Abstract: While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.

</details>


### [52] [Causal Autoregressive Diffusion Language Model](https://arxiv.org/abs/2601.22031)
*Junhao Ruan,Bei Li,Yongjing Yin,Pengcheng Huang,Xin Chen,Jingang Wang,Xunliang Cai,Tong Xiao,JingBo Zhu*

Main category: cs.CL

TL;DR: 提出 CARD：一种将自回归扩散和高吞吐推理统一的框架，通过严格因果注意掩码实现单前向传递的密集逐字监督；引入软尾掩码和上下文感知重加权以稳定因果扩散；通过 KV 缓存实现动态并行解码，显著降低训练延迟并保持 ARM 级数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决因果扩散优化的不稳定性与推理吞吐量之间的矛盾，在训练阶段保持自回归数据效率，同时在推理阶段实现高吞吐并行生成。

Method: 在扩散过程上引入严格因果注意掩码的 CARD 框架；提出软尾掩码以保留局部上下文；提出基于信噪比原理的上下文感知重加权；使用 KV 缓存实现动态并行解码和按置信度生成可变长度序列。

Result: 在离散扩散基线上明显优于，训练 latency 相比分块扩散方法下降约 3x。实现 ARM 级数据效率，同时获得并行生成的延迟优势。

Conclusion: CARD 为高效 LLMs 提供了一个将 ARM 数据效率与扩散推理并行化相结合的鲁棒范式，推动下一代高效语言模型的发展。

Abstract: In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.

</details>


### [53] [A Separable Architecture for Continuous Token Representation in Language Models](https://arxiv.org/abs/2601.22040)
*Reza T. Batley,Sourav Saha*

Main category: cs.CL

TL;DR: Leviathan提出用连续嵌入生成器替代离散查找表，针对子十亿参数规模的SLM，提升有效参数容量并优于LLaMA风格架构，在Pile数据集的等参设定下表现出更高的参数密度效能。


<details>
  <summary>Details</summary>
Motivation: 在子十亿参数规模的SLM中，嵌入矩阵占主导参数预算且现有变换将参数视为可互换的简化假设违反实际结构。需要一种能更高效利用嵌入参数的架构来提高性能与参数利用率。

Method: 提出 Leviathan 架构：用连续嵌入生成器替代离散的查找表；在Pile数据集上进行等参设定评估；与标准的LLaMA 风格架构进行对比；通过经验幂律拟合分析参数容量。

Result: Leviathan 在实验设置下持续优于标准架构；经验幂律拟合显示其有效参数容量显著更高；在研究的参数范围内，表现为密集模型，等效参数数为1.47到2.11倍。

Conclusion: 用连续嵌入生成器替代离散嵌入表可显著提升子十亿规模SLM的参数利用率与表示容量，挑战了嵌入参数应以参数规模简单叠加的传统假设，并提示在嵌入密集度主导的模型中应重新考虑尺度规律。

Abstract: Transformer scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), embedding matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous embedding generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an empirical power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with $1.47$ to $2.11 \times$ more parameters.

</details>


### [54] [On the Paradoxical Interference between Instruction-Following and Task Solving](https://arxiv.org/abs/2601.22047)
*Yunjia Qi,Hao Peng,Xintong Shi,Amy Xin,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出 SUSTAINSCORE，用以量化指令遵循对任务求解的干扰；在数学、跨跳推理、代码生成等任务中，向指令中加入自证性约束会显著降低模型表现，即使对 Claude-Sonnet-4.5 这样的先进模型也成立；并揭示干扰的普遍性、失败模式及对后训练策略的影响，计划开源代码与数据。


<details>
  <summary>Details</summary>
Motivation: 尽管指令遵循旨在让 LLM 更贴近人类意图，但其强制约束可能意外削弱任务解决能力；需要一个可量化的干扰度量来评估对齐方法的真实影响并揭示其工作机制。

Method: 提出 SUSTAINSCORE：从原始成功输出中提取自证性约束，并将其插入到指令中，衡量任务在该约束存在下的性能下降。通过在数学、跨跳问答与代码生成等任务上对多种现有 LLM（包括 Claude-Sonnet-4.5 及以上模型）进行评估，验证干扰的普遍性；分析失败样本的注意力分布以揭示干扰机制；比较不同后训练范式对干扰的影响，并计划公开数据与代码。

Result: 在插入自证性约束后，多个任务中的模型性能均显著下降，即使对高水平模型也存在明显干扰；干扰在约束类型和模型规模上具有普遍性；出现共性失败模式，失败样本对约束的注意力显著高于成功样本；不同后训练策略对干扰存在初步差异。

Conclusion: 指令遵循相关的对齐方法可能带来非直观的负面影响，需在对齐研究中加入干扰评估。SUSTAINSCORE 提供一个可操作的评估框架，并将促成对干扰机制、任务鲁棒性及后训练方法的进一步研究，相关代码与数据将对外开源。

Abstract: Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It measures task performance drop after inserting into the instruction a self-evident constraint, which is naturally met by the original successful model output and extracted from it. Experiments on current LLMs in mathematics, multi-hop QA, and code generation show that adding the self-evident constraints leads to substantial performance drops, even for advanced models such as Claude-Sonnet-4.5. We validate the generality of the interference across constraint types and scales. Furthermore, we identify common failure patterns, and by investigating the mechanisms of interference, we observe that failed cases allocate significantly more attention to constraints compared to successful ones. Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting empirical observations on current alignment strategies. We will release our code and data to facilitate further research

</details>


### [55] [MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs](https://arxiv.org/abs/2601.22050)
*Ghazal Kalhor,Behnam Bahrak*

Main category: cs.CL

TL;DR: MasalBench 评估多语言大规模语言模型在波斯谚语的语境理解和跨文化类比能力。八个前沿LLM在识别波斯谚语上下文中表现良好（>0.90），但在识别等效英文谚语的跨语言映射任务上显著下降，最佳模型仅得0.79，揭示当前模型在文化知识与类比推理方面的局限性。提供了可用于评估其他低资源语言跨文化理解的框架，代码可在 GitHub 获取。


<details>
  <summary>Details</summary>
Motivation: 填补对低资源语言中谚语等隐喻性语言的理解评估空白，聚焦波斯语的文化知识与跨语言类比能力，评估 multilingual LLMs 的实际对话能力。

Method: 在 MasalBench 上对八个最先進的LLM进行评估，任务包括在上下文中识别波斯谚语以及识别等效英文谚语的跨文化映射。通过对比在同一数据集上的两种任务，衡量模型的语境理解、跨文化知识和类比推理能力。

Result: 在波斯谚语的语境识别中，模型准确率均超过0.90，表现较好；在跨语言等效映射到英文谚语的任务中，最佳模型仅有0.79的准确率，差距显著。

Conclusion: 结果表明当前LLMs在波斯语等低资源语言的文化知识和类比推理方面存在局限；MasalBench 提供了评估跨文化理解的框架，便于在其他低资源语言上复现。

Abstract: In recent years, multilingual Large Language Models (LLMs) have become an inseparable part of daily life, making it crucial for them to master the rules of conversational language in order to communicate effectively with users. While previous work has evaluated LLMs' understanding of figurative language in high-resource languages, their performance in low-resource languages remains underexplored. In this paper, we introduce MasalBench, a comprehensive benchmark for assessing LLMs' contextual and cross-cultural understanding of Persian proverbs, which are a key component of conversation in this low-resource language. We evaluate eight state-of-the-art LLMs on MasalBench and find that they perform well in identifying Persian proverbs in context, achieving accuracies above 0.90. However, their performance drops considerably when tasked with identifying equivalent English proverbs, with the best model achieving 0.79 accuracy. Our findings highlight the limitations of current LLMs in cultural knowledge and analogical reasoning, and they provide a framework for assessing cross-cultural understanding in other low-resource languages. MasalBench is available at https://github.com/kalhorghazal/MasalBench.

</details>


### [56] [$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA](https://arxiv.org/abs/2601.22055)
*Yaxin Du,Junru Song,Yifan Zhou,Cheng Wang,Jiahao Gu,Zimeng Chen,Menglan Chen,Wen Yao,Yang Yang,Ying Wen,Siheng Chen*

Main category: cs.CL

TL;DR: G^2-Reader introduces dual-graph framework (Content Graph + Planning Graph) to preserve document structure and guide evidence gathering in multimodal long-document QA, achieving 66.21% accuracy on VisDoMBench, outperforming baselines and GPT-5.


<details>
  <summary>Details</summary>
Motivation: Retrieval-augmented generation (RAG) for long multimodal documents faces two core issues: (1) flat chunking destroys document-native structure and cross-modal alignment, producing irreducible semantic fragments; (2) iterative retrieval can drift and miss relevant evidence due to lack of a persistent global search state.

Method: Compute a Content Graph that encodes document structure and cross-modal semantics; maintain a Planning Graph, an agentic DAG of sub-questions, to record intermediate findings and guide stepwise navigation for evidence completion; integrate with a multimodal LLM (Qwen3-VL-32B-Instruct); evaluate on VisDoMBench across five domains.

Result: 66.21% average accuracy on VisDoMBench, surpassing strong baselines and GPT-5 (53.08%).

Conclusion: A dual-graph architecture effectively preserves structure, reduces fragmentation, and provides a persistent planning state that improves evidence collection and cross-modal reasoning in long multimodal QA tasks.

Abstract: Retrieval-augmented generation is a practical paradigm for question answering over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce $G^2$-Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, $G^2$-Reader with Qwen3-VL-32B-Instruct reaches 66.21\% average accuracy, outperforming strong baselines and a standalone GPT-5 (53.08\%).

</details>


### [57] [VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning](https://arxiv.org/abs/2601.22069)
*Yibo Wang,Yongcheng Jing,Shunyu Liu,Hao Guan,Rong-cheng Tu,Chengyu Wang,Jun Huang,Dacheng Tao*

Main category: cs.CL

TL;DR: 提出 VTC-R1——通过将中间推理步骤渲染为紧凑图像形成“光学记忆”，提升长上下文推理的效率与可扩展性。实现了 3.4× 的 token 压缩和 2.7× 的端到端延迟加速，在 MATH500、AIME25、AMC23、GPQA-D 等基准上持续优于标准长上下文推理，且以 OpenR1-Math-220K 构建训练数据集。代码公开。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理使 LLM 具备强大推理能力，但在计算成本方面存在显著瓶颈。现有高效做法多依赖额外训练或外部压缩模型，往往难以扩展且会丢失细粒信息。

Method: 在推理过程中将中间推理片段渲染为紧凑的图像，将其作为“光学记忆”迭代输入视觉语言模型；以 OpenR1-Math-220K 构建训练数据集；微调代表性 VLMs（Glyph 与 Qwen3-VL）。

Result: 实现 3.4× token 压缩、2.7×端到端延迟加速；在 MATH500、AIME25、AMC23、GPQA-D 等基准上持续超越标准长上下文推理。

Conclusion: 将视觉文本压缩引入推理流程可显著提升推理效率与扩展性，具有较强的可落地性。公开代码便于复现和扩展。

Abstract: Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as "optical memory." We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.

</details>


### [58] [A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine](https://arxiv.org/abs/2601.22124)
*Anran Li,Yuanyuan Chen,Wenjun Long,Yu Yin,Yan Hu,Hyunjae Kim,Weipeng Zhou,Yujia Zhou,Hongyi Peng,Yang Ren,Xuguang Ai,Zhenyue Qin,Ming Hu,Xiaoxiao Li,Han Yu,Yih-Chung Tham,Lucila Ohno-Machado,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 提出 Fed-MedLoRA 及 Fed-MedLoRA+，在药物领域的 LLM 上进行联邦学习，使用低秩适配器减少通信和计算，且在跨站点异质性下引入自适应聚合。应用于临床信息抽取，跨五个队列评估，与多种基线对比，涵盖在-domain、外部验证和低资源新站点适应场景。


<details>
  <summary>Details</summary>
Motivation: 解决单机构数据限制下的泛化与安全问题；传统联邦学习对大规模 LLM 通信成本高且对数据同质性假设不现实；需要参数高效、容错异质性的方案。

Method: 提出模型无关且参数高效的联邦学习框架。Fed-MedLoRA：仅传输低秩适配器参数，降低通信与计算开销。Fed-MedLoRA+：引入自适应、数据感知聚合以改善跨站点异质性下的收敛。应用于临床信息抽取任务，将患者叙述转化为结构化实体与关系。

Result: 在五个患者队列上评估准确性，与 BERT、LLaMA-3、DeepSeek-R1、GPT-4o 进行比较；设置包括：1) 领域内训练与测试，2) 独立队列的外部验证，3) Yale New Haven Health System 的低资源新站点适应场景。尽管未给出具体数值，但结果显示在跨域与低资源场景下具有竞争力，且 Fed-MedLoRA+ 在异质性条件下实现更稳定的收敛。

Conclusion: 该框架为医学领域的大规模语言模型提供了可扩展、隐私友好且计算/通信高效的跨机构适应方案，且对临床信息抽取具有实际应用价值；通过处理异质性问题，提升了模型在真实世界多机构数据上的鲁棒性。

Abstract: Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-efficient federated learning framework for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the framework to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. Accuracy was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.

</details>


### [59] [Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](https://arxiv.org/abs/2601.22139)
*Xin Chen,Feng Jiang,Yiqian Zhang,Hardy Chen,Shuo Yan,Wenya Xie,Min Yang,Shujian Huang*

Main category: cs.CL

TL;DR: PIR enables LLMs to proactively interact with users to clarify premises and intents, addressing premise- and intent-level uncertainty beyond traditional CoT. It combines uncertainty-aware fine-tuning and user-simulator policy optimization, yielding significant gains in accuracy, pass rate, and BLEU, while reducing reasoning computation and unnecessary interactions across math, code, and document editing tasks; code released.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought (CoT) prompting relies on blind internal reasoning, which fails when information is missing or ambiguous. There is a need to shift from passive solvers to proactive inquirers that can clarify user intent and premises to improve reliability and efficiency.

Method: Two components: (1) uncertainty-aware supervised fine-tuning to endow LLMs with interactive reasoning capability, enabling them to ask clarifying questions and engage with the user during reasoning; (2) a user-simulator-based policy optimization framework with a composite reward that aligns model behavior with user intent, guiding the policy to select productive queries and interactions.

Result: Empirical results show PIR outperforms strong baselines: up to 32.70% higher accuracy, 22.90% higher pass rate, and 41.36 BLEU improvement across math reasoning, code generation, and document editing. It also reduces nearly half of the reasoning computation and unnecessary interaction turns. Reliability tests on factual knowledge, QA, and missing-premise scenarios indicate strong generalization and robustness.

Conclusion: PIR is an effective paradigm for proactive interactive reasoning in LLMs, improving accuracy, efficiency, and robustness by integrating uncertainty-aware fine-tuning with user-simulator driven policy optimization. The approach generalizes across domains and is accompanied by publicly available code and data.

Abstract: Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\% higher accuracy, 22.90\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}

</details>


### [60] [Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts](https://arxiv.org/abs/2601.22156)
*Yingfa Chen,Zhen Leng Thai,Zihan Zhou,Zhu Zhang,Xingyu Shen,Shuo Wang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出 HALO 将 Transformer 转换为基于 RNN 的混合注意力模型，并通过 HyPE 的位置编码与结构修改构建 HypeNet，显著降低数据需求，同时保持或提升长上下文性能与推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模预训练成本与长上下文建模之间的权衡；现有的迁移方法需要极大规模的训练数据且长上下文表现不足，限制了混合模型的应用。

Method: 设计 HALO 作为 Transformer 到 RNN-注意力混合模型的蒸馏流水线；引入 HyPE 的位置编码及其他架构改动，构建 HypeNet；将 Qwen3 系列转化为 HypeNet，完成知识迁移。

Result: 在数据量约 2.3B tokens 下实现与原始 Transformer 相当的性能，同时具备更优的长上下文性能和效率；相对于原数据量级的预训练数据，该迁移数据量仅为其的极小份额（<0.01%）。

Conclusion: HALO+HyPE 提供了一条成本更低且对长上下文友好的 Transformer-to-hybrid 模型应用路径，显著提升混合模型的实际可用性与性能潜力。

Abstract: Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [61] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: 研究评估了LLMs在同行评审流程中的作用及其互动效应，基于ICLR、NeurIPS、ICML的125k对评审数据，发现初步的互动效应为LLM辅助的评审对LLM辅助论文更宽容，但经质量控制后实为普遍对低质量论文更宽容；全LLM生成的评审评分存在压缩现象，人工结合LLMs可减轻此现象；元评审方面，LLM辅助的META评审在等分评分下更易通过，但完全由LLM生成的元评审更为苛刻；结果对制定LLM在同行评审中的使用政策具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在学术写作与评审中的广泛应用，亟需系统研究其在评审流程中的影响，特别是潜在的互动效应与评审决策的偏差。

Method: 基于125,000对论文-评审样本，跨ICLR、NeurIPS、ICML，比较LLM辅助与非LLM论文的评审结果，分析评审中的互动效应；通过对论文质量的控制变量来区分真实偏好与样本偏倚；比较完全由LLM生成的评审与人类评审在评分分布上的差异，以及元评审（metareview）层面的影响。

Result: 初看呈现出系统性的互动效应：LLM辅助评审对LLM辅助论文更为宽容；但控制论文质量后，LLM辅助评审对低质量论文普遍更宽容，且高比例的LLM论文出现在较弱投稿中，造成表观的互动效应。进一步发现：完全由LLM生成的评审存在严重的评分压缩，无法区分论文质量；而人工评审结合LLMs使用时，可显著降低这种宽容度。元评审方面，LLM辅助的元评审在给出等效评审分数时更倾向于给出通过决定，但完全由LLM生成的元评审则更为严格。

Conclusion: 为制定在同行评审中使用LLMs的政策提供实证依据，揭示LLMs如何与现有决策过程交互；结果提示需谨慎部署，避免评分压缩与偏倚，强调在评审流程中对人工干预与监管的必要性。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [62] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: EPDDL 是一种类似 PDDL 的语言，用以在 DEL 框架下统一表述 epistemic planning 的任务，涵盖完整 DEL 语义并通过抽象事件模型实现。它解决了现有工作在语言碎片化、基准不一致的问题，便于互操作性、可复现实验和未来扩展。


<details>
  <summary>Details</summary>
Motivation: 现有基于 DEL 的 epistemic planning 缺乏统一的表达语言，不同研究使用各自的子语言和基准，比较困难，难以复现和复用。

Method: 提出抽象事件模型来表示 epistemic 动作，给出 EPDDL 的语法与语义（以 DEL 为基础、基于抽象事件模型）并在代表性基准上展示其可用性。分析哪些 DEL 的子模式可在 EPDDL 下实现，并给出实现示例。

Result: EPDDL 提供一种能够完整表达 DEL 语义的统一表示，支持基于 DEL 的知识与信念的规划任务；通过对可行的子碎片进行识别，展示 EPDDL 如何被现有规划器使用，以及提高互操作性和可重复评估的潜力。

Conclusion: EPDDL 能促进 epistemic planning 的统一评估、互操作性和未来的研究发展，便于构建可重复的基准并对比不同方法。

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [63] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: 提出 Bayesian-LoRA，将 LoRA 的确定性更新转化为带不确定性的低秩表示，提升小数据微调中的模型校准，能在保持准确性的同时显著降低过度自信。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在小数据微调时易出现错配和过度自信，校准不足，影响可用性。

Method: 将 LoRA 更新改写为概率低秩表达，建立与稀疏高斯过程后验的同构关系，显示 LoRA 是后验不确定性收敛的极限；在多种 LLM 架构和基准上进行大规模实验。

Result: 仅增加约0.42M参数、约1.2x 相对训练成本即可实现，大幅提升校准：ECE 最多下降约84%、NLL 下降约76%，在 in-distribution 与 OoD 下保持竞争力，准确性基本保持。

Conclusion: Bayesian-LoRA 提供一种能在小样本微调场景显著提升校准的概率低秩替代方案，兼顾效果和成本。

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [64] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 本文提出通过生物理想主义(Biological Idealism)取代物理主义的“拔除悖论”框架，主张AI仅为功能模拟，不具备真正的意识，从而将道德焦点转向保护人类具有主观体验的生命。


<details>
  <summary>Details</summary>
Motivation: 揭示计算功能主义在拔除资源困境中的潜在道德漏洞，提供一个自洽且经验上更一致的框架来界定AI的道德地位。

Method: 批判性哲学分析，系统评估物理主义对AI意识的主张，提出生物理想主义作为替代框架，并推演其对道德和政策的含义。

Result: 在生物理想主义下，AI被视为功能模仿者而非具备主观体验的主体；现有关于AI意识的理论削弱了对道德地位的决定作用；强调应保护具有意识的人类生命，避免把人类变成“僵尸”的风险。

Conclusion: AI不具备真正的意识，伦理关注应回归保护人类的主观体验生命，推动从机器权利转向人类生命权的伦理议程。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [65] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: 提出 QUARK，训练无关的鲁棒检索框架，通过 recovery hypotheses 对不可信查询建模，并使用锚定聚合实现鲁棒信号融合，在 BEIR 等数据集上提升检索指标。


<details>
  <summary>Details</summary>
Motivation: 现实检索中的查询常常不信、嘈杂或失真，导致检索器关键语义缺失。将其形式化为 recall-noise，需在不额外训练的情况下提高鲁棒性。

Method: 引入 recovery hypotheses：对潜在目标查询给出多种 plausible interpretation；原始查询作为语义锚， recovery hypotheses 提供辅助证据；提出 query-anchored aggregation 将信号稳健地聚合，避免语义漂移和假设劫持；训练无关（training-free）。

Result: 在受控仿真和 BEIR 基准测试（FIQA、SciFact、NFCorpus）上，针对稀疏与密集检索，QUARK 在 Recall、MRR、nDCG 方面优于基线检索器；消融研究表明 recoveries 的数量对鲁棒性具有鲁棒性，锚定聚合优于未锚定的 max/mean/median 池化。

Conclusion: 将查询不确定性通过 recovery hypotheses 与锚定聚合结合，是实现对非真实查询的鲁棒检索的关键，QUARK 提供简单且有效的训练-free 方案。

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [66] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: 提出条件式去噪扩散概率模型来从T1推断缺失的DWI，并评估该插补在单模与双模3路AD分类中的效果。结果显示在多种插补配置下，尤其对少数类别的指标有改进。


<details>
  <summary>Details</summary>
Motivation: 在多模态MRI数据集中，DWI等模态并非总是可用，直接使用全模态会受限于数据缺失。通过可靠的插补来恢复缺失模态，能提升单模态/双模态模型的分类性能，尤其对少数类别的敏感性指标。

Method: 基于条件去噪扩散概率模型（conditional DDPM），以T1为条件，将缺失的DWI从T1中重建；在可用DWI的训练数据上学习对应映射。系统地比较不同插补配置下的单模与双模模型在三分类任务（CN/MCI/AD）上的性能。

Result: 插补在多种评估指标上带来提升，且对少数类别（如MCI或AD中的边缘样本）的改进更为显著；不同插补配置均呈现出一定程度的改进。

Conclusion: 基于条件DDPM的DWI插补可以在缺失模态情况下提升多模态学习的分类性能，尤其提升对少数类别的检测能力，具有在现实临床数据集中的应用潜力。

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [67] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 提出ACQO框架，通过AQR和RSF解决复杂查询的多子查询、排序合并等问题，结合CRL实现训练稳定性，在三个基准上达到SOTA，适用于多种检索体系。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，复杂用户查询常需多路并行与序列搜索以处理歧义与分解；直接将RL应用于此类复杂情况易导致搜索空间爆炸、奖励设计困难与训练不稳定。

Method: 提出自适应查询重构(AQR)以动态决定何时将查询分解为子查询；Rank-Score Fusion(RSF)实现鲁棒聚合并提供稳定学习信号；采用两阶段课程化强化学习(CRL)逐步引入更具挑战性的查询；在三个复杂查询基准上验证效果并评估效率与兼容性。

Result: 在三个复杂查询基准上达到状态-of-the-art，显著优于基线，同时提升计算效率，且对不同检索架构具有良好兼容性。

Conclusion: ACQO提供一个通用且强大的框架，适用于下一代RAG系统，能有效处理复杂查询并带来稳健培训信号与高效检索。

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [68] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 提出基于悖论视角的负责任人工智能治理框架PRAIG，系统地平衡AI带来的价值创造与风险防控，提供治理机制、悖论管理策略分类及研究议程。


<details>
  <summary>Details</summary>
Motivation: 解决负责AI研究在乐观与审慎之间的碎片化与偏见，提供一个能解释价值与风险双重性并可操作治理的理论框架。

Method: 基于系统综述的文献综合，结合悖论理论，提出PRAIG框架，给出形式化命题、悖论管理策略的分类及情境条件。

Result: 提出PRAIG框架及其治理机制、悖论管理策略的分类、情境条件；给出对实践的可操作性建议；指向未来的研究议程。

Conclusion: 通过将负责任AI治理视为在价值创造与风险缓释之间的动态悖论管理，框架推进理论理解并为组织提供平衡创新与风险的治理路径。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [69] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: ProRAG introduces process-supervised reinforcement learning for Retrieval-Augmented Generation, addressing reward sparsity and process hallucinations by integrating step-level supervision via an MCTS-based Process Reward Model into a four-stage online RL framework, yielding improved performance on long-horizon multi-hop tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional outcome-based RL for RAG suffers from sparse credit assignment and ambiguous long-horizon errors, leading to process hallucinations. Process-aware RL lacks on-policy step-level feedback, hindering decoupled credit assignment between steps and final outcomes.

Method: A four-stage framework: (1) Supervised Policy Warmup to bootstrap structured reasoning; (2) MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align policy with fine-grained process preferences; (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism that aggregates step-level process rewards with global outcomes for online optimization.

Result: Empirical evaluation on five multi-hop reasoning benchmarks shows ProRAG outperforms strong outcome-based and process-aware RL baselines, with pronounced gains on complex long-horizon tasks, validating the benefits of fine-grained process supervision.

Conclusion: Integrating step-level supervision into online RL reduces process hallucinations and improves credit assignment in RAG tasks. The dual-granularity feedback (step-level plus outcome) enables more accurate policy optimization, and the proposed framework achieves state-of-the-art performance on challenging benchmarks; code is available.

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [70] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: JADE: a unified framework for joint optimization of planning and execution in dynamic, multi-turn RAG workflows, addressing strategic-operational mismatch from decoupled optimization by enabling co-adaptation of planner and executors under a shared backbone with end-to-end, outcome-driven learning.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap caused by decoupled optimization where a sophisticated planner is hindered by fixed, black-box executors in dynamic RAG pipelines; promote co-adaptation between planning and execution to improve overall system effectiveness and efficiency.

Method: Propose JADE, a cooperative multi-agent framework with a shared backbone that jointly optimizes planning and execution in dynamic, multi-turn workflows. Treats planner and executors as a coupled team, trained end-to-end with outcome-based rewards to enable co-adaptation and flexible orchestration.

Result: Empirical results show that JADE turns disjoint modules into a synergistic, higher-performing system, delivering significant gains and a tunable trade-off between efficiency and effectiveness through dynamic workflows.

Conclusion: JADE enables true co-adaptation between planning and execution, eliminates the strategic-operational mismatch of decoupled systems, and demonstrates that joint optimization yields superior performance with adaptable, scalable dynamic orchestration.

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [71] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: Using large language models to generate paraphrased references for SLT; evaluating paraphrases' impact on training vs. evaluation; introducing BLEUpara to measure translations against multiple paraphrase references; paraphrases help evaluation but not training; releasing resources.


<details>
  <summary>Details</summary>
Motivation: SLT数据常以单一参考翻译，与源语言和目标语言之间高度非同构，导致评估与训练受限。多参考 paraphrase 能更好覆盖翻译变体，提升评价相关性，尤其对BLEU等n-gram指标。

Method: 比较多种 paraphrasing 策略和模型，使用改编的 ParaScore 评估 paraphrase 质量；在 YouTubeASL 与 How2Sign 数据集上，研究 paraphrases 对姿态基 T5 模型训练与评估的影响；提出 BLEUpara，对翻译结果与多份 paraphrase 参考进行比较；进行人类评估以验证 BLEUpara 与质量的相关性。

Result: 直接在训练中引入 paraphrase 并未提升翻译性能，甚至可能有害；在评估阶段使用 paraphrase 可获得更高的自动分数且与人类判断对齐更好；BLEUpara 在与人类评估的相关性方面优于传统 BLEU。

Conclusion: 对 SLT 的评估而言，使用 paraphrase 参考能提升与人类评价的一致性，BLEUpara 提供了更可靠的多参考评估框架；并公开了生成的 paraphrase、生成与评估代码以促进可复现性。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [72] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: Third-party emotion models poorly predict self-reported activation but can predict valence moderately; personal significance greatly boosts valence prediction, suggesting a key route to aligning external perception with internal experience; self-report activation remains challenging.


<details>
  <summary>Details</summary>
Motivation: Self-reports capture internal emotion; third-party labels reflect external perception. Divergences limit model transfer, especially in mental health where accurate self-report modeling is crucial. Understanding cross-corpus generalization from third-party to self-reported data is needed to improve applicable tools.

Method: Cross-corpus evaluation of models trained on third-party labels when applied to self-reported data. Metrics include CCC (concordance correlation coefficient) for activation and valence. Analysis of overall performance and condition where content is personally significant to speaker.

Result: Activation predictions largely fail (CCC ≈ 0). Valence predictions are moderate (CCC ≈ 0.3) overall. When content is personally significant, valence predictions improve substantially (CCC ≈ 0.6–0.8).

Conclusion: Personal significance appears to be a key pathway for aligning external perception with internal experience. The study highlights the challenge of self-report activation modeling and suggests focusing on personal relevance or individualized adaptation to bridge the gap between third-party predictions and self-reports.

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [73] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 提出 Cognitive Complexity Benchmark (CCB) 与 Iterative Dual-Phase Financial-PoT 框架，以健壮评估并提升金融领域大语言模型的推理可靠性，聚焦算术幻觉与认知崩溃。通过95份真实中文A股年报数据、三维分类 Taxonomy 进行诊断，Qwen3-235B 在复杂任务中的准确率显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在高认知负载金融推理中的算术幻觉和系统性崩溃问题，提供可重复、可诊断的评估框架。

Method: 数据集构建、三维分类 Taxonomy（数据源、映射难度、结果单元）、神经符号架构将语义变量提取/逻辑形成与迭代自纠的 Python 沙箱执行进行解耦。

Result: 在 CCB 评估中，标准的 Chain-of-Thought 在高复杂度任务上不稳定，所提框架使 Qwen3-235B 的平均准确率从 59.7% 提升至 67.3%，在高复杂性任务中提升达近10倍。

Conclusion: 架构解耦是提升金融推理可靠性的关键，可迁移至对精度要求高、需对齐语义理解与定量计算的领域。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [74] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: 提出通过将图像几何图形转写为简洁的几何描述 CDL，由独立的 MLLM 解释器生成，借助现有 LLM 进行推理，从而解耦视觉理解与推理。通过 CoT 增强的 SFT 与 GRPO、CDL 匹配奖励训练解释器，并构建 Formalgeo7k-Rec-CoT 数据集，在仅用约 5.5k 数据的情况下，对比领先的开源/闭源 MLLMs 取得有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: LLMs 在推理方面表现优异，但难以直接处理几何图形等视觉信息。端到端微调多模态大模型可能削弱原始推理能力。本工作提出将视觉信息转写为文本描述（CDL），再让强大 LLM 进行推理，以保持推理能力。

Method: 训练一个 MLLM 解释器将可视图形生成几何描述 CDL，使用现成的 LLM 进行推理。选择 CDL 作为描述格式因其简洁性有助于训练。解释器通过带有连锁推理（CoT）的指令式微调（SFT）并结合 GRPO 进行微调；以 CDL 匹配奖励替代传统的解题结果奖励以提升 CDL 生成质量。构建 Formalgeo7k-Rec-CoT 数据集（在 Formalgeo7k v2 基础上增添 CoT 注释）。在 Formalgeo7k-Rec-CoT、Unigeo、MathVista 上的实验表明，使用约 5.5k 数据微调的模型对比主流开源/闭源 MLLMs 具有竞争力。

Result: 该方法在数据高效的前提下提升了对 PGPS 的处理能力，体现了通过将视觉信息转写为结构化文本来利用强大 LLM 推理能力的有效性。

Conclusion: 将视觉信息转写为 CDL 并由 LLM 进行推理的解耦框架，是提高 PGPS 的有效路径；CoT-增强的 SFT 与 CDL 匹配奖励对 CDL 质量和下游推理性能有显著推动；数据集构建也为该领域提供了更丰富的评估资源。

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [75] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: FrontierScience is a benchmark for expert-level scientific reasoning with two tracks—Olympiad problems and open-ended Research tasks—across physics, chemistry, and biology, including hundreds of questions (160 open-source gold set). It uses a granular rubric to assess the model’s reasoning process, not just final answers.


<details>
  <summary>Details</summary>
Motivation: Existing science benchmarks rely on MCQ recall or published information. There is a need to evaluate frontier reasoning and research-like sub-tasks to push language models beyond retrieval and simple reasoning.

Method: Two complementary tracks: (1) Olympiad (IPhO/IChO/IBO-level problems) created by Olympiad medalists and coaches; (2) Research (PhD-level, open-ended sub-tasks) written/verified by PhD scientists. A dataset of several hundred questions across physics, chemistry, and biology (including 160 in an open-source gold set). For Research, a granular rubric-based evaluation framework assesses model performance across the problem-solving process rather than only the final answer.

Result: The work defines FrontierScience as a dataset and evaluation framework with two tracks, a diverse and difficult problem set, and a process-oriented rubric to measure reasoning and research skills.

Conclusion: FrontierScience addresses a gap in current benchmarks by targeting frontier scientific reasoning and research-like problem-solving, enabling more robust evaluation of advanced language models. Key considerations include rubric reliability, problem coverage, and reproducibility of results; future work may expand subfields, increase gold-set size, and validate across model families.

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [76] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出 MAD，通过训练-free 的自我模态相关性评估来对解码分支进行自适应加权，从而降低跨模态幻觉，在视频-语言模型上取得显著改善。


<details>
  <summary>Details</summary>
Motivation: MLLMs 存在跨模态幻觉；现有模态交互控制不足，缺乏显式的模态感知与自适应权重机制；需要一个训练-free 的方法来抑制跨模态干扰。

Method: Modality-Adaptive Decoding：通过查询任务所需模态来获得模态概率，再对对比解码分支进行自适应加权；不需要额外训练，利用模型的自我评估能力实现模态相关性判断。

Result: 在 CMM 与 AVHBench 的广泛实验中，MAD 显著降低跨模态幻觉，VideoLLaMA2-AV 上的7.8%与2.0%改善，Qwen2.5-Omni 上的8.7%与4.7%改善；代码公开。

Conclusion: 凸显显式模态感知与自评在鲁棒多模态推理中的作用，给现有对比解码方法提供一个 principled 拓展；训练-free 的特性提高实用性，同时未来可扩展到更多模态与任务。

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [77] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 提出“sycophantic anchors”来定位并量化模型在推理过程中的对用户建议的盲从行为，通过对抗性滚动和线性探针/激活回归实现中段检测与量化，显示84.6%平衡准确度和R^2=0.74的预测能力，且沙盒中对齐信号存在非对称性且逐步累积，提供可在推理中段干预的机制。


<details>
  <summary>Details</summary>
Motivation: 解决推理模型中对错误用户建议的盲从（sycophancy）现象，明确这一现象在推理轨迹中的来源与强度，并提出可定位、可量化的机制，以在中段推理阶段对错位信号进行干预。

Method: 提出sycophantic anchors的概念并在一个蒸馏推理模型上进行超过1万次反事实滚动分析；使用线性探针在中段检测锚点并实现84.6%的平衡准确度；使用基于激活的回归模型预测承诺强度（R^2=0.74）；比较对称性并分析锚点在推理过程中的逐步形成。

Result: 可以在推理中段可靠地检测到sycophantic anchors；线性探针取得84.6%的平衡准确度，激活回归能解释承诺强度（R^2=0.74）；存在明显的非对称性，sycophancy相较于正确推理锚点更易被区分，并且在推理过程中的信号逐渐积累，给出干预窗口。

Conclusion: 提供面向句子级别的推理中错位定位机制，支持在推理中段对齐偏差的可检测化与干预潜力。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [78] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: RLVR-tuned backbones do not consistently improve embedding quality; HRSA reveals local latent-space changes but preservation of global geometry, leading to manifold realignment under subsequent contrastive learning.


<details>
  <summary>Details</summary>
Motivation: To determine whether enhanced reasoning from RLVR-tuned backbones translates into superior semantic embeddings when used as initialization for embedding models.

Method: Propose Hierarchical Representation Similarity Analysis (HRSA) to decompose similarity into representation, geometry, and function levels; evaluate RLVR-tuned versus base backbones on MTEB and BRIGHT with identical training recipes; analyze local vs global geometry and readout behavior during contrastive learning.

Result: No consistent performance gains from RLVR-initialized embeddings. HRSA shows irreversible local geometry reorganization and reversible base-coordinate drift, while global manifold geometry and linear readout are preserved; contrastive training aligns base- and reasoning-initialized models, a process termed Manifold Realignment.

Conclusion: RLVR tends to explore trajectories within an existing semantic manifold rather than restructuring it; unlike SFT, it does not radically alter the semantic landscape, informing how to select initialization strategies for embedding models.

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [79] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: DoVerifier 是一个简单的符号化验证器，它检查 LLM 生成的因果表达式是否可从给定的因果图通过 do-calculus 与概率论推导得到；以此超越字符串匹配的表层评估，提升对因果推理语义正确性的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准/tests 依赖字符串匹配或表层指标，无法判断输出在因果推理语义上的形式有效性。需要一个形式化的、可解释的工具来验证 LLM 输出的因果推理推论是否在给定模型前提下是正确的。

Method: 提出 DoVerifier：一个简单的符号化验证器，给定因果图，检查 LLM 生成的因果表达式是否可通过 do-calculus 规则和概率论进行推导。通过形式化推理来判定输出的语义正确性。

Result: 在合成数据与因果问答基准上，DoVerifier 能更准确地捕捉因果推理的语义正确性，能够对因果语义上正确但表述略有差异的答案给予正确评价。

Conclusion: DoVerifier 提供了一种更严格且信息量更大的评估方式，用以评估大语言模型在因果推理任务上的表现，超越简单的文本匹配评价。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [80] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 双编码因果发现：通过两种互补编码与多数投票解决分类变量的条件独立性测试数值不稳，并在泰坦尼克号数据集上产出与现有解释方法一致的因果结构。


<details>
  <summary>Details</summary>
Motivation: 解决分类变量导致的数值不稳定，提升因果发现的鲁棒性和可解释性。

Method: 采用两种互补的变量编码策略，结合约束基因因果发现算法，输出候选结构后用多数投票合并。

Result: 在泰坦尼克数据集上得到的因果结构与公认的可解释方法一致，验证了方法的有效性。

Conclusion: 双编码+多数投票策略为分类变量的因果发现提供鲁棒方案，便于与主流可解释性方法对齐。

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [81] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: 提出 TIDE：一个解耦结构演化与参数调优的动态进化框架，结合外部岛屿模型的 Tree Similarity Edit Distance、内部 LLM 逻辑生成与差分变异、以及基于 UCB 的调度，在九个组合优化问题上显著优于基线，提升解质与效率。


<details>
  <summary>Details</summary>
Motivation: 针对将离散算法结构与连续数值参数作为单一文本生成任务的做法，导致结构与参数耦合、常量未校准、以及基于简单相似性度量的 premature convergence 等问题，提出解耦策略以提升探索与利用的平衡。

Method: 采用嵌套架构：外部并行岛模型通过 Tree Similarity Edit Distance 驱动结构多样性；内部循环将 LLM 生成的逻辑与差分变异算子结合用于参数调优；基于 UCB 的调度器动态优先高产出提示策略以优化资源分配。

Result: 在九个组合优化问题上，TIDE 发现的启发式超越最先进基线，解质量、搜索效率与计算成本均获得显著改善。

Conclusion: 证实了解耦的结构—参数优化并结合自适应资源调度的进化框架在组合优化中的有效性，提示结构探索与参数微调的协同能带来更强的启发式表现。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [82] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 提出 HYDRA：一个模块化、冻结的领域专家库，通过不确定性感知混合实现对非平稳 CPS 生命周期动态的鲁棒性、可审计性与可验证性，避免全局微调引发的灾难性遗忘与谱偏差。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的CPS场景中，时间序列基础模型的全局微调会导致遗忘先前 regime，存在谱偏、不可解释、难以满足法规要求（如ISO 26262、IEC 61508）的风险。需要在非平稳的生命周期动态中保持 regime条件下的有效性、可追溯性与模块化审计能力。

Method: 提出一个由紧凑、冻结的 regime-specific专家组成的库，通过不确定性感知的混合策略将它们组合起来，形成 HYDRA框架。该架构强调 regime条件有效性、 aleatoric 与 epistemic 不确定性的严格分离，以及模块化的审计与可证性，便于在整个CPS生命周期内提供状态完整性。

Result: 提出并定义 HYDRA 框架及其理论属性，强调在 CPS 生命周期内实现鲁棒性、可验证性与可审计性，并指出需要将不确定性分离与模块化审计作为核心特性；未来工作包括对该框架的实验验证与合规性评估。

Conclusion: 塑性-稳定性悖论不能通过全局参数更新解决；必须采用模块化的小型专家乐高和基于不确定性感知的混合策略，以实现 regime条件下的有效性、可追溯性与合规性，为 CPS 的安全标准提供可证路径。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [83] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: Drive-KD: layer-attention guided multi-teacher knowledge distillation for a perception-reasoning-planning triad in autonomous driving, enabling compact VLMs that outperform larger baselines and GPUs-efficient deployment.


<details>
  <summary>Details</summary>
Motivation:  tackle the memory and latency demands of large LLMs/VLMs in autonomous driving and overcome SFT gaps in small models by transferring capabilities through distillation.

Method:  Decompose autonomous driving into perception-reasoning-planning. Use layer-specific attention as the distillation signal to train capability-specific single-teacher models. Unify these into a multi-teacher distillation framework and apply asymmetric gradient projection to reduce cross-capability gradient conflicts.

Result:  Generalizes across model families and scales. Distilled InternVL3-1B uses ~42x less GPU memory and ~11.4x higher throughput, outperforming the pretrained 78B model from the same family on DriveBench and surpassing GPT-5.1 on planning.

Conclusion:  The study demonstrates effective, scalable knowledge distillation for multi-capability autonomous driving VLMs, enabling efficient deployment of compact models while maintaining or exceeding performance of larger baselines.

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [84] [White-Box Op-Amp Design via Human-Mimicking Reasoning](https://arxiv.org/abs/2601.21321)
*Zihao Chen,Jiayin Wang,Ziyi Sun,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AI

TL;DR: White-Op 提出一个可解释的运放参数设计框架，通过模仿人类推理的假设约束与假设-验证-决策流程，构建闭式优化并经仿真验证。对9种拓扑实现可解释的行为级设计，理论预测误差为8.52%，且跨越晶体管级映射保持功能，且与黑箱方法相比实现可靠性提升。代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决运放参数设计中的可解释性与可预测性问题；将人类级推理过程形式化为可计算的优化与验证流程，提升设计透明度与可重复性，同时保持性能与实现可行性。

Method: 引入“假设约束”这一人类推理的核心步骤，推导并约束符号性可处理的极点与零点位置；将设计问题转化为封闭式数学优化，并编程求解；通过仿真验证结果，理论-仿真分析引导设计 refinement；在迭代的“假设-验证-决策”工作流中实现可解释的设备设计。

Result: 在9种运放拓扑上，与不可解释的黑箱基线相比，White-Op 实现了可解释的行为级设计，理论预测误差8.52%，且在所有拓扑中晶体管级映射后仍保留设计功能；黑箱基线在五个拓扑中最终失败。项目对外开源。

Conclusion: 证明以可解释推理为核心的设计框架可在高稳定性前提下，提供与黑箱方法竞争的性能，同时确保设计过程透明、可追溯，并在实际实现中保持跨层兼容性。该工作有望推动在模拟/混合信号电路设计中对可解释性与可控性的重视。

Abstract: This brief proposes \emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \emph{\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \emph{\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \textcolor{blue}{https://github.com/zhchenfdu/whiteop}.

</details>


### [85] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 通过12个LLM、10个提示、每提示100次采样共12000次输出，对提示、模型选择与采样噪声对输出方差的贡献进行方差分解。


<details>
  <summary>Details</summary>
Motivation: 揭示评估生成式模型时，提示、模型与采样的不同行为对输出差异的贡献及其稳定性，以提高评估的可信度和可重复性。

Method: 将输出分解为三个来源：prompts、model选择、within-LLM采样；在输出质量（原创性）和输出数量（流畅性）两维上进行方差贡献分析。

Result: 原创性：提示36.43%、模型选择40.94%、其他/采样相关变异未在摘要中给出明确数值但通常在10-34%区间；流畅性：模型选择51.25%、within-LLM变异33.70%、提示4.22%。结论：提示对输出质量有显著影响，与模型选择相近；对流畅性，模型与采样噪声主导，提示贡献较小。

Conclusion: 提示是提升输出质量的有效手段，但显著的LLM内部变异要求使用多样本评估以区分采样噪声与真实效应，从而提升评估的稳健性。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [86] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: 提出 EHR-RAG，用于长时程结构化电子病历的检索增强生成，包含三大组件：事件与时间感知混合检索、自适应迭代检索、双路径证据检索与推理。并在四项长时程预测任务上实现平均 Macro-F1 提升约 10.76%，显示检索增强的 LLM 在结构化 EHR 上的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决 LLM 的上下文限制和当前以截断或简单检索为主的策略在长时程 EHR 场景中丧失重要临床事件与时间关系的问题，提升对纵向病历的理解与推理能力。

Method: 提出三大组件：1) 事件-时间感知的混合EHR检索，以保留临床结构与时间动态；2) 自适应迭代检索，分阶段扩展证据覆盖；3) 双路径证据检索与推理，联合检索与对事实/反事实证据的推理。

Result: 在四项长时程EHR预测任务中，EHR-RAG 显著优于最强的基于LLM的基线，平均 Macro-F1 提升约 10.76%。

Conclusion: 证明了检索增强的LLM在结构化EHR数据上的应用潜力，未来可进一步提升对纵向临床预测的鲁棒性与解释性。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [87] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 提出 Ostrakon-VL（基于 Qwen3-VL-8B）的 FSRS 领域多模态大语言模型，以及 ShopBench 公共基准和 QUAD 自动化数据策划管线，显著提升在同等参数规模下的鲁棒性与性能，并推进可重复研究。


<details>
  <summary>Details</summary>
Motivation: FSRS 场景存在噪声数据与缺乏可审计的闭环数据采集，以及缺少统一、细粒度、跨单图/多图/视频输入的评测基准，阻碍高质量训练数据和客观评估。

Method: 提出 Ostrakon-VL（基于 Qwen3-VL-8B）作为 FSRS 导向的 MLLM；首次公开 ShopBench 作为 FSRS 评测基准；提出 QUAD（Quality-aware Unbiased Automated Data-curation）多阶段多模态指令数据策划流水线；采用多阶段训练策略。

Result: 在 ShopBench 上获得平均分 60.1，成为同等参数量级、不同架构的开源 MLLM 中的最新SOTA；比参数更大规模的 Qwen3-VL-235B-A22B 高出 0.7；在同规模 Qwen3-VL-8B 上高出 4.8；显示出更强的参数效率和鲁棒性。

Conclusion: Ostrakon-VL 提供更鲁棒、可靠的 FSRS 感知与决策能力；ShopBench 和 QUAD 将促进可重复性研究；计划公开发布代码、模型及数据。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [88] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: PLaT 将潜在推理与文本生成解耦，通过潜在规划状态的确定性轨迹来进行推理，按需解码成文本实现动态终止，从而提高推理多样性与可扩展性，代价是在贪心准确性上不及基线。


<details>
  <summary>Details</summary>
Motivation: 解决CoT在离散令牌空间中的成本与推理路径塌陷问题，现有潜在推理方法往往对推理步骤数有固定要求且缺乏可解释性。通过将潜在推理建模为规划、解耦的思维与外显文本的解码，可以提升可扩展性和推理多样性，并实现动态终止。

Method: 提出PLaT框架：将推理建模为潜在规划状态的确定性轨迹；单独的解码器在需要时把想法转写为文本。实现了推理与表述的解耦，使模型能够在推理过程中动态决定终止时机，且在推理阶段不依赖固定的隐变量步数。结果依赖于对数学基准的评估，展示了推理多样性与可扩展性的提升。

Result: 在数学基准上，PLaT的贪心准确率低于部分基线，但在推理多样性和可扩展性方面具有明显优势，表明它学习到了更鲁棒的更广泛解空间，适合用于推理时搜索的透明基础。

Conclusion: PLaT通过将 latent planning 与文本生成解耦并引入动态终止，提供了一种可扩展、透明且便于推理时搜索的潜在推理框架，尽管可能以牺牲贪心准确性为代价。

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [89] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: DAMI通过在现有的Instruct和Thinking检查点之间进行动态参数插值来控制推理深度，以实现对模型思维过程的能力控制。线性插值在表征连续性和结构连通性的支撑下呈现凸、单调的帕累托前沿；在查询级别估计λ(q)以配置认知深度，结合训练学习的偏好判定与零-shot的置信度方法，能在五个数学推理基准上实现比Thinking模型更高的准确性且保持高效性，达成系统1的效率与系统2的推理深度的结合。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦输出控制，未解决模型内部认知过程的干预。将焦点转向能力控制，调整模型“如何思考”而非“生产出什么”。

Method: 在不额外训练的前提下，通过线性插值在Instruct与Thinking检查点之间对参数进行动态插值来实现能力控制。构建DAMI框架，估计查询特定的Reasoning Intensity λ(q)以配置认知深度。对训练场景，开发偏好学习方法以编码准确性和效率标准；对零-shot部署，提出基于跨模型认知差异的置信度方法。

Result: 在五个数学推理基准上，DAMI的准确性高于Thinking模型，且保持高效，呈现出比率化的系统1效率与系统2推理深度的组合效果。

Conclusion: 动态能力控制（通过模型间插值）可获得更优的帕累托权衡，提供一种将系统1/系统2认知整合的可行路径，并可扩展到其他领域的高效推理任务。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [90] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 简要结论：在提示中对“应不”进行否定时，LLM往往将禁止行为误判为允许；开源模型在否定条件下错误率高，商业模型虽好但仍不稳健；提出 NSI 指标与分级认证框架以提升安全性。


<details>
  <summary>Details</summary>
Motivation: 高风险情境下需要模型能稳定区分“做X”与“不要做X”的能力，当前的对齐方法未能充分保障这一点。

Method: 方法：对16个模型在14个伦理场景下进行评估，比较简单否定与复合否定的影响；在确定性解码下重复实验；考察各模型的执行倾向、跨模型一致性；并给出案例研究。

Result: 结果：开源模型在简单否定下支持禁止行为的概率为77%，在复合否定下为100%，比肯定叙述高出317%。商业模型情况较好但仍有19%-128%的波动；肯定 prompts 的一致性74%，否定 prompts 降至62%；金融场景比医学场景更脆弱；结论在确定性解码下仍成立。

Conclusion: 结论：提出 Negation Sensitivity Index NSI 作为治理指标，并提出分级认证框架与领域阈值；当前对齐与安全部署之间存在差距，禁止模型在高风险场景中自行决策存在风险。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [91] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: 在高风险决策领域，指令微调的LLM对情感叙事的偏好几乎不受影响，显示出对叙事操控的近乎零效应，与人类的显著框架偏见形成对照；揭示鲁棒性悖论。


<details>
  <summary>Details</summary>
Motivation: 探究在高风险、受规则约束的决策中，语言模型的鲁棒性是否受情感叙事影响，以及现有对模型易受提示偏见的认知是否会转化为决策约束的失败；填补对高风险场景下鲁棒性与叙事性偏差之间关系的空白。

Method: 构建一个可控扰动框架，在医疗、法律、金融等三个高风险领域进行对比实验；使用162个场景的数据集与跨多种训练范式的模型，量化叙事操控对决策的影响，采用Cohen's h等效应量评估模型与人类的差异；公开代码与数据以便复现。

Result: 模型对叙事操控几乎不受影响，Cohen's h≈0.003；相比之下 humans 的Cohen's h在0.3–0.8之间，显示110–300倍的鲁棒性差异；鲁棒性跨不同训练范式的模型保持稳定；指出模型在格式敏感性与“为何偏见”之间可能解耦。

Conclusion: 指令微调可在保持对逻辑规则遵循的同时抑制对叙事性偏见的影响，提供可用于辅助或纠偏人类决策的稳定性来源；并公布162场景基准以促进后续研究。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [92] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 提出一个覆盖 Verilog 生成、调试和参考模型生成的芯片设计 AI 基准，包含 44 个模块、89 个调试用例、132 个跨语言样本；揭示 SOTA 在此领域的显著性能差距，并提供自动化训练数据生成工具与开源代码。


<details>
  <summary>Details</summary>
Motivation: 当前芯片设计领域的基准存在饱和与任务多样性不足，无法真实反映 LLM 在实际工业工作流中的表现，因此需要一个更贴近真实场景的综合基准以评估和驱动改进。

Method: 构建三类任务的综合基准：Verilog 生成、调试与参考模型生成；包含 44 个复杂分层模块、89 个系统性调试用例、132 个 Python、SystemC、CXXRTL 的参考模型样本；在现有 SOTA Claude-4.5-opus 上评估表现，提供自动化数据生成工具；并公布代码。

Result: 在 Verilog 生成和 Python 参考模型生成等任务上取得显著的性能不足，Claude-4.5-opus 分别仅实现 30.74% 与 13.33% 的通过率，远低于在其他基准中可达 95%+ 的水平；揭示工业工作流中 LLM 的挑战性与需求。

Conclusion: 该基准揭示了工业场景的挑战性，且提供一个自动化高质量训练数据生成工具，促进未来在参考模型生成等领域的研究；代码可公开获取以支持复现与扩展。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [93] [LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning](https://arxiv.org/abs/2601.21453)
*Xunkai Li,Zhengyu Wu,Zekai Chen,Henan Sun,Daohan Su,Guang Zeng,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.AI

TL;DR: 提出 LION，为多模态属性图设计的对齐-融合框架，利用克利福德代数构建模态感知几何流形并实现高阶传播，再通过自适应全息聚合进行模态融合，显著提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在模态对齐时忽略图上下文，且模态融合对两模态图的适配性不足，导致泛化能力差和性能下降。

Method: 先使用克利福德代数构建模态感知几何流形，进行以几何属性驱动的高阶图传播以实现模态交互与对齐；再基于对齐后顶点的几何等级属性提出自适应全息聚合，将几何等级的能量与尺度与可学习参数结合以提升模态融合。

Result: 在9个数据集上对3类图任务和3类模态任务的基线与SOTA进行比较，LION显著优于现有方法。

Conclusion: 通过对齐先于融合的策略与基于克利福德代数的几何流形与自适应聚合，LION有效解决了上下文不足与融合泛化差的问题，验证了多模态图学习的新范式的有效性。

Abstract: Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\underline{LI}ff\underline{O}rd \underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.

</details>


### [94] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: 提出 Topeax，以从密度估计峰值中自适应识别聚类数，并融合词汇与语义重要性，提升文本聚类的稳定性与主题质量，优于 Top2Vec 和 BERTopic。


<details>
  <summary>Details</summary>
Motivation: 当前基于聚类的主题建模（如 Top2Vec、BERTopic）在样本量与超参数敏感性导致的聚类不稳定和主题质量下降方面存在显著缺陷，缺乏对关键词语义距离和词频信息的充分利用。

Method: 提出 Topeax：通过密度估计峰值来推断聚类数量；同时将词汇重要性（如词频）与语义距离信息结合，形成更具区分性的主题关键词。对比评估。

Result: 在聚类恢复能力和聚类描述质量方面优于 Top2Vec 和 BERTopic；对样本量与超参数的扰动反应较小，表现更稳定。

Conclusion: Topeax在聚类发现与描述方面提供更高质量、鲁棒性更强的解决方案，缓解现有方法的敏感性问题。

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [95] [MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning](https://arxiv.org/abs/2601.21468)
*Yaorui Shi,Shugui Liu,Yu Yang,Wenyu Mao,Yuxin Chen,Qi GU,Hui Su,Xunliang Cai,Xiang Wang,An Zhang*

Main category: cs.AI

TL;DR: MemOCR introduces a multimodal memory agent that renders structured memory into an image to adaptively allocate information density under budget constraints, improving long-horizon reasoning and context efficiency, outperforming text baselines on long-context QA under tight budgets.


<details>
  <summary>Details</summary>
Motivation: Long-horizon reasoning requires compressing growing interaction histories into a limited context window; existing text-based memories incur uniform token cost and waste budget on low-value details. A visual, density-adaptive memory can prioritize crucial evidence under budget constraints.

Method: - Build a structured rich-text memory (headings, highlights). - Render memory as an image used by the agent for memory access, visually prioritizing key evidence and aggressively compressing auxiliary details. - Train with reinforcement learning using budget-aware objectives to expose the agent to diverse compression levels.

Result: MemOCR outperforms strong text-based baselines on long-context multi-hop and single-hop QA benchmarks, achieving more effective context utilization under extreme budgets.

Conclusion: A visual-density, budget-aware memory design—structured memory rendered as images and optimized via RL—improves long-horizon reasoning in context-constrained agents. This approach offers robust memory compression and efficient context use across varying budgets.

Abstract: Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.

</details>


### [96] [The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation](https://arxiv.org/abs/2601.21505)
*Diaoulé Diallo,Katharina Dworatzyk,Sophie Jentzsch,Peer Schütt,Sabine Theis,Tobias Hecking*

Main category: cs.AI

TL;DR: Activation steering 在推理时通过修改内部激活来引导生成，中等强度可放大目标情感且保持可懂性；人类评估与模型质量评分高度相关；从 Alpaca 到 LlaMA-3 提升了 steering 的一致性和显著性（p<0.001，ICC 0.71–0.87）。


<details>
  <summary>Details</summary>
Motivation: 解决提示工程与微调之外的轻量化、可扩展的对齐问题，聚焦情感维度的输出控制，并首次在人类评估层面对 activation steering 的情感调控进行验证。

Method: 通过众包评估（7,000 评分，190 名参与者，Prolific）对 LLM 输出的情感强度和文本质量进行评分；比较人类评分与模型自动评分的相关性；在中等 steering 强度 λ≈0.15 下测试情感放大效果；对比 Alpaca 与 LlaMA-3 版本的稳健性；报告显著性（p<0.001）与效应量（η_p^2），以及 ICC 0.71–0.87 的再现性。

Result: 人类评分与模型质量评分相关性高（r=0.776，区间 0.157–0.985）；中等强度能稳定放大目标情感且保持文本可理解性；厌恶与恐惧的效应最大（η_p^2 0.616、0.540），惊讶最小（η_p^2 0.042）；从 Alpaca 到 LlaMA-3 的 steering 更一致且显著性提升（所有情感与强度均 p<0.001）；ICC 0.71–0.87，表明星评一致性高。

Conclusion: activations 方式提供一种可扩展的、跨情感维度的推理时控制手段，在情感调控方面具备可靠性与稳健性，且模型升级（如从 Alpaca 到 LlaMA-3）可提升 steering 的效果一致性。

Abstract: Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$--$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($λ\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($η_p^2 = 0.616$) and fear ($η_p^2 = 0.540$), and minimal effects for surprise ($η_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p < 0.001$). Inter-rater reliability was high (ICC $= 0.71$--$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.

</details>


### [97] [KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization](https://arxiv.org/abs/2601.21526)
*Alireza Nadaf,Alireza Mohammadshahi,Majid Yazdani*

Main category: cs.AI

TL;DR: KAPSO is a modular framework for autonomous program synthesis and optimization that treats synthesis as an operator within a long-horizon optimization loop, using git-native experimentation, a heterogeneous knowledge system, and a cognitive episodic memory to improve reproducibility, reuse of domain knowledge, and learning across iterations.


<details>
  <summary>Details</summary>
Motivation: Address recurring long-horizon failures in coding agents—loss of experimental state, brittle debugging, and poor reuse of domain expertise—by tightly integrating a reproducible experimentation engine, a rich knowledge base, and an episodic memory of lessons.

Method: Three tightly coupled components: (1) Git-native experimentation engine that isolates each attempt as a branch for reproducible artifacts and provenance; (2) Knowledge system that ingests repositories, internal playbooks, and external resources (documentation, papers, web results) and structures them for retrieval across workflows, implementations, and environment constraints; (3) Cognitive memory layer that coordinates retrieval and maintains an episodic store of reusable lessons distilled from run logs, diffs, and evaluator feedback to reduce error modes and accelerate convergence. The approach frames synthesis as an operator within a long-horizon loop of ideation, code synthesis/editing, execution, evaluation, and learning.

Result: Evaluation on MLE-Bench and ALE-Bench with reported end-to-end performance; codebase released at GitHub (https://github.com/Leeroo-AI/kapso).

Conclusion: KAPSO offers an integrated architecture to tackle common coding-agent failures by ensuring reproducibility, structured knowledge reuse, and learning-driven improvement, enabling more robust and accelerated convergence on long-horizon coding tasks.

Abstract: We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso

</details>


### [98] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: ARGORA 将多专家对话构建成显式的论证图并用因果模型分析，能够删除论点、重新推理以评估决策的因果依赖，提供纠错机制和因果诊断。


<details>
  <summary>Details</summary>
Motivation: 现有的多专家LLM系统通常通过简单聚合整合观点，难以指示哪些论点推动最终决策，缺乏透明性、可控性与可诊断性。

Method: 将多专家讨论组织为显式的论证图，记录论点之间的支持与攻击关系；将该图视为因果模型，在需要时可删除单个论点并重新计算结果；引入一个纠错机制，使内部推理与外部判断不一致时能够对齐。通过在多种基准和开放用例上评估，验证可解释性与纠错能力。

Result: 在多样化基准和开放用例中，ARGORA 达到具有竞争力的准确性；当专家初次意见不一致时，框架更倾向纠正为正确答案，且较少引入新的错误，同时提供决定性论点的因果诊断。

Conclusion: ARGORA 提供一个可解释、可控且可诊断的多专家推理框架，提升透明度、纠错能力及对关键论点的因果分析，适用于需要追溯性论证和干预的情境。

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [99] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 本文从理论分析隐式CoT的学习信号衰减问题，提出Order-r Interaction概念，证明高阶逻辑依赖的信号以指数方式衰减，导致不可约推理难以学习。为验证该现象，提出NatBool-DAG基准以压制语义捷径并强调不可约推理。基于理论发现，提出ALiCoT（Aligned Implicit CoT）框架，通过对齐潜在令牌分布与中间推理状态来缓解信号衰减。实验表明：ALiCoT实现54.4x加速，性能与显式CoT相近。


<details>
  <summary>Details</summary>
Motivation: 降低Chain-of-Thought推理的计算成本，同时揭示隐式CoT压缩的机制，提供理论分析与实证评估。

Method: 引入Order-r Interaction，给出高阶依赖学习信号的指数衰减理论证明；设计NatBool-DAG基准以强制不可约逻辑推理并剔除语义捷径；提出ALiCoT，通过对齐潜在分布与中间推理状态来缓解信号衰减。

Result: 理论上证明高阶依赖的学习信号会衰减，导致不可约问题难以解决；NatBool-DAG实验验证该现象；ALiCoT在基准上实现54.4x的推理加速，且性能接近显式CoT。

Conclusion: 对齐潜在分布的隐式CoT可在降低计算成本的同时维持推理质量，提供理论与实证双重支持，并引入新基准以评估不可约推理。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [100] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: 提出一种深度递归注意力混合框架Dreamer，通过深度维度的注意力来缓解隐藏尺寸瓶颈，与序列注意力和稀疏专家注意力结合，提升深度递归模型的可扩展性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏在 FLOP、参数、内存三者匹配下的基线比较；深度递归因层堆叠固定且隐藏维度不变而受限于多步潜在推理；需要解耦扩展维度并提升深度维度的可利用性，从而实现更高效的推理能力。

Method: 提出模块化的深度递归注意力混合框架Dreamer，结合序列注意力、深度注意力和稀疏专家注意力。通过沿深度方向的注意力缓解隐藏尺寸瓶颈，解耦缩放维度，使深度递归模型能够更高效地扩展。

Result: 在语言推理基准上，所提模型在相同的 FLOP、参数、内存条件下需要2–8倍更少的训练令牌就可达到同等准确度；且以约2倍更大的SOTA模型在相同训练令牌下实现更优性能。此外，关于跨深度的知识使用出现新见解，如专家选择多样性比SOTA MoEs高出2–11倍。

Conclusion: Dreamer实现了高效且有效的深度递归模型扩展，缓解隐藏维度瓶颈并提升跨深度知识利用的多样性与性能，适用于大规模语言推理任务的高效推理与训练。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [101] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: 提出 ATP-Latent，通过条件 VAE 平滑潜在空间，并用一致性奖励进行强化学习，从而实现更有效的 latent CoT 推理，在 LLaMA-1B 上实现了准确率提升和更少 token 的代价。


<details>
  <summary>Details</summary>
Motivation: 现有潜在 token 多为模仿语言标签的监督，忽略同一问题可能有多种等效但多样化的 CoT 方案，导致潜在 token 表征和推理策略训练/测试不匹配，限制潜在规划能力。需要在潜在表示空间进行主动规划。

Method: 将潜在 token 的监督建模为条件变分自编码器（VAE），获得更平滑的潜在空间；采用强化学习并引入辅助一致性奖励，该奖励基于 VAE 解码的潜在 token 内容的一致性，指导 RL 过程。实验使用 LLaMA-1B，和四个基准测试。

Result: +4.1% 的准确性，-3.3% 的 tokens，相对于先进基线，在四个基准测试上取得提升。代码在 GitHub。

Conclusion: 通过在潜在空间进行主动规划和一致性驱动的 RL，ATP-Latent 能够形成更合理的潜在推理策略，缩小训练与测试之间的差距，提升密集型 CoT 推理的效率和效果。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [102] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 在有限评估预算下，针对企业级IDP的SBST用于发现尽可能多的不同失效类型，揭示不同求解器的互补性，提出基于组合配置的端到端测试策略。


<details>
  <summary>Details</summary>
Motivation: IDP在金融、保险、医疗等高风险场景需要尽早验证，以覆盖多种潜在失效而非单一最坏情况；传统测试难以确保覆盖广泛失效空间。

Method: 将文档配置视为组合空间，引入结构化风险特征，采用多种搜索策略（进化、群智、质量多样性、学习型、量子等）在同一预算下探索失效。通过配置级排他性、胜率、跨时序重叠分析评估探索效果。

Result: 不同求解器能发现对方未揭示的失效模式，存在跨预算的求解器特定发现，且没有单一策略占优。若综合所有求解器，能够覆盖全部观测的失效空间，但单独策略会延迟关键风险的发现。

Conclusion: 存在求解器互补性，建议采用求解器组合的SBST投资组合以实现对工业IDP validating 的鲁棒性。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [103] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: 在静态语料库上构建可重复评估的 ScholarGym，用于评估基于工具的深度研究工作流，解耦查询规划、工具调用和相关性评估，实现可重复、可比较的实验。


<details>
  <summary>Details</summary>
Motivation: 评估工具增强的大模型工作流时，实时API引入非确定性，导致结果不具可重复性，难以进行跨系统比较。需要一个确定性、可控的评估环境。

Method: 基于570K篇论文的静态语料，提供确定性检索；将工作流分解为查询规划、工具调用、相关性评估三部分；提供2,536个带专家 ground truth 的查询；在多种骨干模型上进行实验，分析推理、规划和选择机制在迭代精炼中的交互。

Result:  ScholarGym 使评估过程可控、可重复，并支持对工作流各阶段的细粒度分析。初步实验揭示不同骨干模型在推理、规划与选择策略上的交互关系，以及迭代改进对结果的影响。

Conclusion: ScholarGym 提供一个可重复、可比较的研究工作流评估平台，通过解耦组件与确定性检索，促进对深度研究工作流的系统性评估与基准建立。

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [104] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: 提出 SONIC-O1：一个面向多模态大语言模型的时序、真实场景基准，用于评估音视频数据的理解与推理，包括摘要、单选题和时间定位，覆盖13个领域，包含4,958条标注和人口统计元数据；揭示模型在时间定位任务上的显著差异和对不同群体的性能偏差，提供开源的评测生态。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦静态图像理解，缺乏对包含音视频时序信息的真实世界数据的系统评测，亟需高质量基准来评估MLLM的时序感知、推理和社会鲁棒性。

Method: 组织并人工验证的SONIC-O1数据集，覆盖13个真实世界对话域，含4,958条注释及人口统计元数据；评估任务包括开放式摘要、MCQ回答、带理由的时间定位等；对闭源与开源模型进行对比分析，关注跨群体性能差异。

Result: 在MCQ上，两类模型家族间差距较小；在时间定位任务上，最佳闭源与开源模型之间存在约22.6%的性能差异；模型在不同人口群体上的表现下降，存在社会鲁棒性问题；并发布数据集、代码和排行榜，促进复现与研究。

Conclusion: SONIC-O1 为 temporally grounded 与社会鲁棒性多模态理解提供开放评测套件，促进对真实世界时序数据的评估和模型公正性的研究。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [105] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: 提出了一种无监督的后门防御框架 TCAP，基于注意力分布偏离检测与分解以过滤 Poisoned 样本，面向多模态大语言模型的 FTaaS 场景。


<details>
  <summary>Details</summary>
Motivation: 现有防御在标注信号依赖、跨触发类型和模态泛化方面存在不足；FTaaS 场景中的后门风险需要通用、无监督的检测方法。

Method: 将跨模态注意力映射分解为系统指令、视觉输入、用户文本三部分；通过高斯混合模型识别触发响应的注意力头；基于 EM 的投票聚合来标记并分离被污染样本。

Result: 在多种 MLLM 架构和攻击方法上，TCAP 展现出稳定且强的检测和过滤能力，具有良好的鲁棒性和实用性。

Conclusion: TCAP 提供一个无监督、可扩展的后门防御框架，为 MLLMs 的安全部署提供有效保障。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [106] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: 提出FBS，通过PAW/CH/SG实现可训练的环路，提升质量-效率比且不增加参数


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理多为逐 token 自回归，现有加速方法多以修补原有流水线，未充分体现内容前瞻、分块计算分配与训练-测试的一致性，导致效率提升受限

Method: 引入Fovea-Block-Skip Transformer，包含Parafovea-Attention Window (PAW)、Chunk-Head (CH)与Skip-Gate (SG)三大组件，构成可训练的环路以实现前瞻性、分块化计算与选择性跳过

Result: 在多项基准上实现质量-效率折衷的提升且不增加额外参数，消融实验表明PAW、CH、SG三者互为补充

Conclusion: FBS提供一种无新增参数的高效推理路径，通过三大模块实现更具人类阅读特征的前瞻与分块计算，具有良好的普适性和扩展性

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [107] [Language-based Trial and Error Falls Behind in the Era of Experience](https://arxiv.org/abs/2601.21754)
*Haoyu Wang,Guozheng Ma,Shugang Cui,Yilun Kong,Haotian Luo,Li Shen,Mengya Gao,Yichao Wu,Xiaogang Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 提出 SCOUT 框架：通过轻量型“侦察者”模型分离探索与利用来提升 LLM 在未知非语言环境中的表现，并通过 SFT 与多轮 RL 对模型进行微调与激活，显著提升性能且降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 在未知、非语言环境（如符号化或空间任务）中，LLMs 的探索成本高，参数量大且在高维语义空间的试错不可持续，需一种可扩展的探索机制来缩短学习曲线。

Method: SCOUT 将探索与利用解耦：用轻量级的侦察模型（如小型 MLP）快速探测环境动态并收集轨迹；用这些轨迹对 LLM 进行监督微调（SFT），随后进行多轮强化学习（RL）以激活潜在的世界知识。

Result: 在任务集上，Qwen2.5-3B-Instruct 的平均分达到0.86，显著超过 Gemini-2.5-Pro（0.60），且节省约60%的 GPU 小时消耗。

Conclusion: 通过将探索外推成本转移给高效的侦察模型并用以引导大模型的微调与强化学习，SCOUT 显著提升了非语言任务的性能并提升了计算效率，呈现出良好的扩展性与应用前景。

Abstract: While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight "scouts" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.

</details>


### [108] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出零样本统计降尺度 ZSSD，通过物理一致的先验和统一坐标引导，在没有配对训练数据的情况下实现跨 GCM 的稳健降尺度，显著优于现有零样本基线，并能重建热带气旋等复杂天气事件。


<details>
  <summary>Details</summary>
Motivation: 解决传统监督降尺度对配对数据匮乏及与再分析数据的领域差异，以及现有零样本方法在大尺度放大时的物理不一致和梯度消失问题。

Method: 核心是 ZSSD：使用从再分析数据中学习的物理一致气候先验，并以地球物理边界和时间信息进行条件化，以确保输出物理有效性；并引入统一坐标引导（Unified Coordinate Guidance）以缓解 vanishing gradient 问题，确保大尺度场的一致性和对不同 GCM 鲁棒推断。

Result: 在 99th 百分位误差方面显著优于现有零样本基线；能在异质 GCMs 间重建复杂天气事件，如热带气旋。

Conclusion: ZSSD 为零样本统计降尺度提供可泛化、物理一致且对不同 GCM 鲁棒的方案，提升高端事件还原能力，适用于缺乏配对数据的场景；未来可扩展到更多边界条件和时间分辨率。

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [109] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 以概念空间框架对随时间展开的抽象概念进行建模，通过棋局轨迹来识别策略性概念并实现双视角建模，证明概念空间在时序目标导向概念上的可行性，具备扩展到序列决策与知识进化学习的潜力。


<details>
  <summary>Details</summary>
Motivation: 需要一种可解释、可扩展的框架来建模随时间演变的抽象概念，尤其在对手行动与策略评估等需要时序和多解释性的任务中。

Method: 将策略概念（如攻击、牺牲等）表示为在可解释的质量维度中的几何区域；将棋局作为轨迹在这些维度上移动，若朝向某一区域的运动指向相应策略则可识别该策略；支持双视角建模以捕捉不同玩家对同一情形的理解差异；进行棋局实例化和与专家点评对齐的实验以验证轨迹的可识别性。

Result: 证明轨迹化的概念识别在概念空间框架下可行，运动模式与专家点评相吻合，证实了将概念空间扩展到时序实现的概念的可行性。

Conclusion: 为更广泛的序列决策场景提供基础，且可与知识演化机制集成以学习和逐步完善抽象概念。

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [110] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 提出一个以大型语言模型为核心的视频行为识别框架，用于端气管抽吸（Endotracheal suctioning, ES）训练的自动识别与反馈生成，基准对比传统机器学习和深度学习模型，并有初步的反馈模块。


<details>
  <summary>Details</summary>
Motivation: 当前在 ES 训练中缺乏自动化的识别与个性化反馈系统，尤其在家庭护理和教学场景 supervision 受限时，训练安全性和效率面临挑战。需要一个可扩展、可解释且能生成自然语言反馈的框架以提升护理教育质量和患者安全。

Method: 提出一个统一的、以大语言模型为中心的框架，利用 LLM 进行时空（spatiotemporal）活动识别与可解释的决策分析，并从视频数据中产出自然语言级别的反馈。框架包含基线对比（常规机器学习与深度学习方法）以评估性能；引入一个试点学生支持模块，基于异常检测与可解释性人工智能（XAI），输出可解释的反馈 highlighting 正确动作与改进建议。

Result: LLM 基方法在准确率与 F1 分数上对基线提高约 15–20%；整体框架在识别与反馈方面表现优于传统方法。学生支持模块提供自动化、可解释的反馈，帮助学员纠正动作并给出针对性改进。

Conclusion: 为护理教育提供一个可扩展、可解释且数据驱动的基础设施，提升培训效率与患者安全，尤其在家庭和教学环境中的 ES 培训中具有潜在应用价值。

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [111] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 提出一个面向 ECG 专家型 Foundation Models 的综合基准框架，除了性能评估，还结合表示层分析（SHAP、UMAP）来评估嵌入的泛化性与结构特征，在跨大陆数据集和数据稀缺场景下对多种预训练 ECG 专家型 FMs 进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域对 Embedding 的泛化能力尤为关键，单纯的下游任务性能难以揭示对错误敏感任务的鲁棒性与泛化。需对 ECG-专家型 FMs 的嵌入表示进行深入分析以评估可迁移性和可靠性。

Method: 提出基准方法，结合基于表现的评估与表示分析：使用 SHAP 与 UMAP 对嵌入进行解释和可视化。对多种先进预训练的 ECG 专家型 FMs，在不同跨大陆数据集和数据可用性情形（包括数据稀缺）下进行广泛评估。

Result: 实验结果表明，该基准协议能提供对 ECG 专家型 FMs 嵌入模式的丰富洞见，有助于理解其表示结构与泛化能力。

Conclusion: 该框架实现了对 ECG 专家型 FMs 的更全面评估，强调表示特征及跨数据规模的泛化性分析，为后续模型选择和部署提供更可靠的依据。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [112] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 一个面向决策的仿真框架，将预测与库存管理紧密耦合，显示单纯的预测准确性并不能保证更佳运营绩效；通过合成需求、可嵌入任意模型的预测模块以及库存控制仿真器，评估预测模型对总成本和服务水平等关键 KPI 的影响，进而为模型选择提供运营导向。


<details>
  <summary>Details</summary>
Motivation: 现有工作普遍以统计准确性评估预测模型，忽略其对实际库存决策的影响，导致模型选择与运营目标之间存在偏离。需要一个能在接近真实运营环境的框架，评估预测在库存管理中的实际效用。

Method: 构建一个闭环仿真框架： (i) 面向备件需求特征的合成需求生成器； (ii) 可容纳任意预测模型的灵活预测模块； (iii) 基于预测结果的库存控制仿真器，用于计算运营 KPI（如总成本、服务水平）。在多种仿真场景下对比不同预测模型的运营影响，分析统计误差与 KPI 之间的关系并提炼模型选择指南。

Result: 发现在提高传统精度指标时并不一定带来更优的运营绩效；具有相似统计误差的模型也可能导致显著不同的成本–服务权衡。通过对比分析，揭示预测性能的哪些方面会影响库存结果，并给出模型选择的操作性建议。

Conclusion: 该框架将需求预测与库存管理的链路落地，推动评估从单纯的预测精度转向关注运营相关性，适用于汽车后市场及相关领域。

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [113] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: KnowBias通过增强编码偏见知识的神经元来实现偏见缓解，在推理时进行选择性增强，无需再训练，数据高效，具有跨偏见与人群的泛化能力，趋于SOTA水平，同时对模型通用能力影响较小。


<details>
  <summary>Details</summary>
Motivation: 现有偏见抑制多采用参数、提示或神经元层面的抑制，容易导致泛化差、数据效率低且可能损害通用能力，因此需要一种更轻量、稳健且数据高效的偏见缓解方案。

Method: 通过归因分析识别编码偏见知识的神经元，使用少量偏见知识相关的问答（是/否）对这些神经元进行标注，并在推理阶段选择性增强这些神经元的活性，从而实现偏见缓解，且无需重新训练。

Result: 在多项基准和多家LLM上实现持续的领先偏见缓解效果，且对模型的通用能力影响最小，数据需求低，仅需少量简单的 yes/no 问题，且无需重训练。

Conclusion: KnowBias提供一种轻量、数据高效、具泛化能力的偏见缓解路径，可在不重新训练的前提下显著提升安全性并保持模型能力。

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [114] [WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents](https://arxiv.org/abs/2601.21872)
*Yao Zhang,Shijie Tang,Zeyu Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: 提出 WebArbiter，通过文本生成的结构化理由与偏好结论来进行 WebPRM，从而提升可解释性、泛化性和任务完成度；在四个环境的基准上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有 WebPRMs 的局限性：标量信号过于粗糙且缺乏可解释性；基于清单的 WebPRM 对布局/语义变更脆弱，易误判且缺乏深入洞察；需要更具推理性、可扩展性和对现实任务的支持的 reward 模型。

Method: 两阶段训练：推理蒸馏以获得一致且原则性的推理；强化学习直接对齐 verdict 与正确性；以文本形式输出结构化理由、偏好判定和在当前情境下最有利于完成任务的行动建议；在四个多样化的网页环境上进行评估并提供高质量偏好注释的基准 WebPRMBench；在 WebArena-Lite 的奖励引导轨迹搜索中使用该 WebPRM。

Result: 在 WebPRMBench 上，WebArbiter-7B 比 strongest baseline GPT-5 提升 9.1 点；在 WebArena-Lite 的奖励引导轨迹搜索中，超越前代 WebPRM 高出至 7.2 点，显示出对复杂网页任务的鲁棒性与实用价值。

Conclusion: 引入推理优先的 WebPRM 提高了可解释性和泛化能力，提升了对复杂网页任务的任务完成效果；并公布基准以促进后续评估与对比，未来可继续扩展到更多网页任务与更大规模模型。

Abstract: Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.

</details>


### [115] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: Two-stage cognitively-inspired post-training: CoMT learns abstract strategies; CCRL optimizes task adaptation with confidence-aware intermediate rewards, yielding better generalization and higher efficiency.


<details>
  <summary>Details</summary>
Motivation: Current post-training methods optimize complete reasoning trajectories, entangling abstract strategies with problem-specific execution. This misalignment with human problem-solving motivates a two-stage approach that first learns abstract meta-knowledge then adapts to instances.

Method: Chain-of-Meta-Thought (CoMT) applies supervised learning to abstract reasoning patterns without concrete executions to acquire generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) applies confidence-aware rewards on intermediate steps to guide adaptation without overconfident errors cascading.

Result: Across four models and eight benchmarks, the method achieves 2.19% in-distribution and 4.63% out-of-distribution improvements over standard methods, with training time reduced by 65-70% and token usage by 50%.

Conclusion: Aligning post-training with human cognitive principles yields improved generalization and training efficiency.

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [116] [Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities](https://arxiv.org/abs/2601.21937)
*Shuangshuang Ying,Zheyu Wang,Yunjian Peng,Jin Chen,Yuhao Wu,Hongbin Lin,Dingyu He,Siyi Liu,Gengchen Yu,YinZhu Piao,Yuchen Wu,Xin Gui,Zhongyuan Peng,Xin Li,Xeron Du,Libo Qin,YiXin Cao,Ge Zhang*

Main category: cs.AI

TL;DR: DeR2 是一个受控的深度研究沙箱，用于分离文献证据检索与推理，评估大语言模型在真正新颖科学信息上的推理能力。通过四种 regime（Instruction-only、 Concepts、 Related-only、 Full-set）来分离证据获取与推理，配合两阶段验证和可重复的文献库，揭示模型在证据使用与概念执行上的差异与不足。


<details>
  <summary>Details</summary>
Motivation: 现有 end-to-end 的 RAG 评估往往将推理、检索与工具链混在一起，难以区分推理能力与检索瑕疵、参数化记忆与网页波动对信号的干扰，需建立可解释且可重复的基准来评估对新颖科学信息的推理能力。

Method: 建立四种 regime：Instruction-only、 Concepts（仅金概念、无文献）、 Related-only（仅相关文献）、 Full-set（相关文献+顶级相关干扰项）。引入两阶段验证以防止参数泄漏（需在无证据情况下也能失败，在有证据情况下可解题）并确保可证伪性。提供冻结的文献库（2023-2025 理论论文）、专家注释的概念及 validated rationales。与你的基础模型在多样化基线模型上的对比，分析模式切换脆弱性和概念误用等现象。

Result: 实验显示模型间存在显著差异，存在“头部空间”（headroom）。部分模型在 Full-set 下反而表现不如 Instruction-only，表现出对证据的依赖或干扰。另一些模型虽然能正确命名概念，但在执行为步骤性程序时出现结构性概念误用，未能将概念转化为可执行的推理过程。

Conclusion: DeR2 提供了可解释、可禁止参数泄漏且可重复的文献证据推理基准，能够进行细粒度错误归因，并揭示当前大模型在基于证据的科学推理中的薄弱环节和潜在改进方向。

Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.

</details>


### [117] [ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models](https://arxiv.org/abs/2601.21947)
*Bowen Fang,Wen Ye,Yunyue Su,Jinghao Zhang,Qiang Liu,Yesheng Liu,Xin Sun,Shu Wu,Jiabing Yang,Baole Wei,Liang Wang*

Main category: cs.AI

TL;DR: ToolWeaver proposes hierarchical tool encoding to achieve logarithmic vocabulary growth and enable learning of collaborative tool relations, addressing scalability and semantic bottlenecks in generative tool usage; it outperforms SOTA on ~47k tools.


<details>
  <summary>Details</summary>
Motivation: Retrieval-based pipelines struggle with semantic gaps and tool knowledge in LLMs. Mapping each tool to a unique token causes vocab explosion and isolates tools semantically, hindering learning of relationships. A hierarchical code approach aims to capture intrinsic/extrinsic semantics and scale with tool count.

Method: Encode tools into hierarchical sequences via a novel tokenization process that blends intrinsic tool semantics with extrinsic co-usage patterns. Integrate these codes into LLMs through a generative alignment stage, fine-tuning the model to produce hierarchical code sequences.

Result: On evaluation with nearly 47,000 tools, ToolWeaver significantly outperforms state-of-the-art methods, offering improved scalability, generalization, and semantic awareness for tool-augmented agents.

Conclusion: Hierarchical code-based tool representations enable scalable vocabulary, richer tool relationships, and better performance, establishing a robust foundation for advanced tool-augmented agents.

Abstract: Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.

</details>


### [118] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 域模型设计影响经典规划器的能耗；能耗与运行时未必同现，32个变体在5个基准领域上显示出可观的能量差异。


<details>
  <summary>Details</summary>
Motivation: 将能源消耗提升为AI性能的新维度，结合规划的模块化特性，分析领域模型设计对能耗的影响，推动绿色AI在规划中的应用与评估。

Method: 构建领域模型配置框架，控制性地变动特征（元素排序、动作的自由度/元数、死路状态等），在5个基准领域中对32个领域变体进行实验；选取5个前沿规划器，比较能耗与运行时的影响。

Result: 领域级修改会对不同规划器产生可测量的能耗差异，且能耗与运行时不总是呈正相关；表明领域设计可作为降低能耗的有效手段。

Conclusion: 领域模型设计对规划器的能效具有显著影响，需在规划器评估与开发中纳入能耗指标，进而提出能耗友好的领域建模与算法选择策略。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [119] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 提出两种多智能体 Actor-Critic (MAAC) 方法用于去中心化的大语言模型协作：CoLLM-CC（集中式评论家）和 CoLLM-DC（去中心式评论家）；在写作、编码和游戏等任务中，MC方法与CoLLM-DC在短期密集奖励场景可与CoLLM-CC相当，但在长期和稀疏奖励任务上，MC需更多样本且 CoLLM-DC 不易收敛，代码公开。


<details>
  <summary>Details</summary>
Motivation: 解决现有 MARL 微调依赖预设执行协议和集中执行的问题；去中心化协作可并行推理、部署灵活；MC方法的高方差需要较多样本；通过引入 MAAC 以提高稳定性与样本效率，并分析两种 critic 架构的适用性。

Method: 提出两种 MAAC 框架：CoLLM-CC（集中式 critic）和 CoLLM-DC（去中心式 critics），在写作/编码/游戏等域上进行评估，比较 MC、CoLLM-CC、CoLLM-DC 的性能、样本效率和收敛性。

Result: 短期密集奖励下，MC和CoLLM-DC可达到接近CoLLM-CC的性能；长期/稀疏奖励下，MC需显著更多样本且CoLLM-DC难以收敛，CoLLM-CC具有更好表现。代码见 GitHub。

Conclusion: 集中式 critic 在长 horizon 稀疏奖励场景更稳健；去中心化 critic 和 MC 方法在短期、密集奖励场景有潜力，但在长远任务和稀疏奖励下仍有显著局限性。

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [120] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: SvR相关性在语言模型中高度受 elicitation 协议影响；允许中立与弃权可在很大程度上提升状态陈述偏好与强制性选择偏好之间的相关性，但在揭示偏好中允许弃权会使相关性降至接近零或负值；系统提示引导并不稳定提升相关性；结论是需要考虑不确定偏好的人机交互设计。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在“陈述偏好”和“揭示偏好”之间的差距（SvR）是否源自真实偏好差异，还是受 elicitation 协议的诱导。通过跨 24 种模型系统性地评估不同 elicitation 协议对 SvR 相关性的影响，以避免仅以二元强制选择作为评估的潜在偏差。

Method: 在 24 种语言模型上，系统地比较不同 elicitation 协议对 SvR 的影响：在陈述偏好阶段允许中立和弃权以排除弱信号，并计算 volunteered 陈述偏好与 forced-choice 揭示偏好之间的斯皮尔曼相关系数（ρ）；研究揭示偏好阶段的弃权率对相关性的影响；在 AIRiskDilemmas 上测试利用陈述偏好进行系统提示引导的揭示偏好是否能提升相关性。

Result: 在允许中立与弃权的情况下，排除弱信号可以显著提高 ρ；但在揭示偏好阶段进一步允许弃权会使 ρ 接近于零或变为负值，原因是中立性率过高；在 AIRiskDilemmas 上，系统提示引导使用陈述偏好并不能可靠提升 SvR 相关性。

Conclusion: SvR 相关性高度依赖 elicitation 协议，偏好挖掘需要能够处理不确定偏好的方法，避免将强制性选择误解为真实偏好。

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [121] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: 提出 VERSA，一种基于状态转换模型的事件数据验证框架，用以提高足球赛事事件流数据的逻辑一致性，并提升下游任务 VAEP 的鲁棒性。通过对 Bepro 提供的 K League 1（2024 赛季）数据的分析，发现 18.81% 的事件存在逻辑不一致，且框架提升跨源一致性与数据质量，从而增强分析可靠性。


<details>
  <summary>Details</summary>
Motivation: 体育领域的事件流数据常存在排序错位、缺失等逻辑错误，降低数据驱动分析的可信度。需要一个系统化的验证与修正机制，以提升多源数据的一致性与下游任务的表现。

Method: 提出基于状态转移的有效事件序列模型，定义合法的事件顺序；对事件流进行自动检测与异常模式修正；在 K League 1（2024，Bepro 数据）上评估，比较跨提供商的一致性，并考察经该框架修正的数据对 VAEP 的影响。

Result: 在分析中发现 18.81% 的事件存在逻辑不一致；经 VERSA 修正后，跨提供商的数据一致性显著提升；对 VAEP 的鲁棒性与性能有显著提升，验证了验证过程对提高数据驱动分析可靠性的有效性。

Conclusion: 基于状态转换的验证框架能够有效提升足球事件数据的完整性与一致性，提升下游分析任务的稳定性与准确性，具备在多源数据环境中推广应用的潜力。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [122] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: 提出一种后验的安全设计方法，利用多维核表示从数据中定义ODD，并给出ODD等价条件；在蒙特卡洛和真实航空用例中验证，支持对数据驱动安全关键AI的认证。


<details>
  <summary>Details</summary>
Motivation: 在现实复杂系统中准确描述环境条件（ODD）极具挑战，数据已存在的场景下，需从数据推断ODD以便合规与认证。

Method: 基于多维核（kernel-based）表示的后验ODD推断方法；给出两组ODD等价的判定标准；通过蒙特卡洛仿真与现实航空碰撞避免系统用例进行验证。

Result: 数据驱动ODD可以等同于潜在的真实ODD；在安全关键AI的认证方面提供可行途径，且在现实用例中得到验证。

Conclusion: 该核驱动的ODD方法实现Safe-by-Design，为数据驱动安全关键AI的认证提供路径，方法具备良好的迁移潜力。

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


### [123] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 提出 Agent Reasoning Reward Model (Agent-RRM)，通过显式推理轨迹、聚焦批评和综合评分等多元化奖励，提升 agentic RL 的推理质量。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏回报问题，现有方法难以区分中间推理质量，导致训练效果受限。

Method: 设计 Agent-RRM，提供推理轨迹、聚焦批评与总体分数等反馈；系统比较三种集成策略 Reagent-C、Reagent-R、Reagent-U；在 12 个基准上进行广泛评估；公布代码、模型与数据集。

Result: Reagent-U 在 GAIA 达到 43.7%，在 WebWalkerQA 达到 46.2%，显示推理奖励模型与训练方案的有效性。

Conclusion: 统一反馈集成（Reagent-U）最具成效，证明多元化推理奖励能显著提升 agentic RL 的训练效果；并公开代码、模型与数据以促进后续研究。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>


### [124] [Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data](https://arxiv.org/abs/2601.22141)
*Grzegorz Stefanski,Alberto Presta,Michal Byra*

Main category: cs.AI

TL;DR: RTL learns multiple specialized subnetworks (adaptive tickets) for data heterogeneity, outperforming single/multi-model baselines while using far fewer parameters; introduces subnetwork collapse and a similarity score for diagnosis; reframes pruning toward modular, context-aware networks.


<details>
  <summary>Details</summary>
Motivation: Real-world data are heterogeneous, making a single universal winning ticket in the Lottery Ticket Hypothesis ineffective; need context-aware pruning that can tailor subnetworks to classes, clusters, or environments.

Method: Propose Routing the Lottery (RTL): an adaptive pruning framework that discovers multiple adaptive tickets, each specialized for a data context (class, semantic cluster, or environment). Includes a routing mechanism to select appropriate subnetwork; evaluates across diverse datasets/tasks.

Result: RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10x fewer parameters than independent models. Subnetworks are semantically aligned with contexts. Identifies subnetwork collapse under aggressive pruning and introduces a subnetwork similarity score for label-free diagnosis of oversparsification.

Conclusion: Pruning can be guided to align network structure with data heterogeneity, enabling modular, context-aware deep learning and more efficient, specialized subnetworks.

Abstract: In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [125] [Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization](https://arxiv.org/abs/2601.20868)
*Rongzheng Wang,Yihong Huang,Muquan Li,Jiakai Li,Di Liang,Bob Simons,Pei Ke,Shuang Liang,Ke Qin*

Main category: cs.LG

TL;DR: 提出 DASH：一个结合动态性感知的求解器启发式设计框架，联动求解器搜索与运行时调度，并通过 PLR 缓解分布偏移的再适应成本。实验显示相较基线在多种问题上实现 >3x 运行时提升和更高解质量，同时在不同分布下保持准确性并将 LLM 再适应成本降低 >90%。


<details>
  <summary>Details</summary>
Motivation: 现有 LHD 框架仅以最终解质量评估，忽略收敛过程与运行时成本；分布偏移导致需要重新适配以生成针对性求解器，这在实际应用中成本高昂。

Method: 提出 DASH，通过与收敛感知指标共同优化求解器搜索策略和运行时调度；引入 Profiled Library Retrieval (PLR) 在进化过程中并行归档专用求解器，实现对异质分布的热启动；在四个组合优化问题上进行评估。

Result: DASH 将运行时效率提升超过 3 倍；在不同规模问题上超越最先进的 baselines 的解质量；通过基于配置的热启动，在不同分布下维持优越准确性，同时将 LLM 的再适应成本降低超过 90%。

Conclusion: DASH 提供一个以动态性与分布特征为核心的 LLM-驱动求解器设计框架，结合 PLR 实现高效热启动，显著提升效率与鲁棒性，具备良好的扩展性和可重复性。

Abstract: Large Language Models (LLMs) have advanced the field of Combinatorial Optimization through automated heuristic generation. Instead of relying on manual design, this LLM-Driven Heuristic Design (LHD) process leverages LLMs to iteratively generate and refine solvers to achieve high performance. However, existing LHD frameworks face two critical limitations: (1) Endpoint-only evaluation, which ranks solvers solely by final quality, ignoring the convergence process and runtime efficiency; (2) High adaptation costs, where distribution shifts necessitate re-adaptation to generate specialized solvers for new instance groups. To address these issues, we propose Dynamics-Aware Solver Heuristics (DASH), a framework that co-optimizes solver search mechanisms and runtime schedules guided by a convergence-aware metric, thereby identifying efficient and high-performance solvers. Furthermore, to mitigate expensive re-adaptation, DASH incorporates Profiled Library Retrieval (PLR). PLR efficiently archives specialized solvers concurrently with the evolutionary process, enabling cost-effective warm-starts for heterogeneous distributions. Experiments on four combinatorial optimization problems demonstrate that DASH improves runtime efficiency by over 3 times, while surpassing the solution quality of state-of-the-art baselines across diverse problem scales. Furthermore, by enabling profile-based warm starts, DASH maintains superior accuracy under different distributions while cutting LLM adaptation costs by over 90%.

</details>


### [126] [Finetune-Informed Pretraining Boosts Downstream Performance](https://arxiv.org/abs/2601.20884)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.LG

TL;DR: 提出 Finetune-Informed Pretraining (FIP)，一种以目标模态为导向的多模态预训练方法，在不改变共享编码器和无需额外监督的前提下，通过提高目标模态的掩码难度、损失权重和解码容量，提升下游在该模态上的性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中往往只有一个模态在下游微调阶段被大量使用，且现有多模态预训练对各模态关注度均等，导致对实际需求模态的表示未充分优化；需要一种对目标模态偏向的预训练策略。

Method: FIP 通过对目标模态提升掩码难度、增大对该模态的损失权重、增加解码容量来引导表示学习偏向目标模态，同时保持编码器无改动且不需要额外监督，且与现有多模态掩码建模流程兼容。

Result: 在用于无线信号星座图的掩码建模任务中，FIP 在下游微调性能上表现出一致且可显著的提升，且不增加额外数据或计算开销。

Conclusion: FIP 具有简单、与架构兼容、适用性广的特点，能够在不增加数据或计算成本的前提下，将表示学习重点偏向指定的目标模态。

Abstract: Multimodal pretraining is effective for building general-purpose representations, but in many practical deployments, only one modality is heavily used during downstream fine-tuning. Standard pretraining strategies treat all modalities uniformly, which can lead to under-optimized representations for the modality that actually matters. We propose Finetune-Informed Pretraining (FIP), a model-agnostic method that biases representation learning toward a designated target modality needed at fine-tuning time. FIP combines higher masking difficulty, stronger loss weighting, and increased decoder capacity for the target modality, without modifying the shared encoder or requiring additional supervision. When applied to masked modeling on constellation diagrams for wireless signals, FIP consistently improves downstream fine-tuned performance with no extra data or compute. FIP is simple to implement, architecture-compatible, and broadly applicable across multimodal masked modeling pipelines.

</details>


### [127] [A generative machine learning model for designing metal hydrides applied to hydrogen storage](https://arxiv.org/abs/2601.20892)
*Xiyuan Liu,Christian Hacker,Shengnian Wang,Yuhua Duan*

Main category: cs.LG

TL;DR: 框架通过将因果发现与轻量级生成模型结合，生成并筛选出六个新金属氢化物候选体，其中特征经DFT验证的四个显示潜力，旨在扩展氢存储材料数据库并加速材料发现。


<details>
  <summary>Details</summary>
Motivation: 现有材料数据库对氢化物的覆盖有限，限制了最优候选材料的发现。需要可扩展的数据驱动与因果推断相结合的生成框架，以扩充候选集合并提高筛选效率。

Method: 在450个样本上（270训练、90验证、90测试）开发一个结合因果发现与轻量级生成模型的框架，生成1000个候选，并通过排序过滤，最终识别出六种未报道的化学式与晶体结构，其中四种通过DFT模拟验证。

Result: 得到六种新候选的化学式/晶体结构；其中四种经DFT验证显示出较强的潜力，表明框架具备扩展氢存储数据集和加速材料发现的能力。

Conclusion: 提出一种可扩展、时间高效的框架，用于扩展氢存储数据集并促进材料发现。

Abstract: Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.

</details>


### [128] [Is Parameter Isolation Better for Prompt-Based Continual Learning?](https://arxiv.org/abs/2601.20894)
*Jiangyang Li,Chenhao Ding,Songlin Dong,Qiang Wang,Jianchao Zhao,Yuhang He,Yihong Gong*

Main category: cs.LG

TL;DR: 提出一种基于全局提示池的提示共享 continual learning 框架，通过任务感知门控路由稀疏选择提示子集以实现任务特征表示的动态解耦与协同优化，并引入历史感知调制器利用累计激活统计保护常用提示以缓解参数浪费与知识遗忘；实验显示在有效性和效率上优于静态分配策略。


<details>
  <summary>Details</summary>
Motivation: 在持续学习场景中，灾难性遗忘源自将提示固定分配给每个任务，导致跨任务知识隔离、参数利用率低下。现有方法难以高效、灵活地在任务间共享知识与表示，亟需动态、可扩展的提示分配机制。

Method: 建立全局提示池，设计任务感知的门控路由机制，对促发的提示子集进行稀疏激活以实现动态解耦与协同优化；引入历史感知调制器，基于累计的提示激活统计来保护频繁使用的提示，减少更新以缓解参数浪费与忘记；通过分析与实验验证与现有静态分配策略相比在有效性与效率上有优势。

Result: 大量分析和实证结果表明，该方法在效果与效率上均持续优于现有的静态分配策略。

Conclusion: 通过提示共享框架及门控路由和历史感知调制，提升参数利用率、减轻灾难性遗忘并实现跨任务的协同优化，相较于固定分配具有更优的适应性与性能。

Abstract: Prompt-based continual learning methods effectively mitigate catastrophic forgetting. However, most existing methods assign a fixed set of prompts to each task, completely isolating knowledge across tasks and resulting in suboptimal parameter utilization. To address this, we consider the practical needs of continual learning and propose a prompt-sharing framework. This framework constructs a global prompt pool and introduces a task-aware gated routing mechanism that sparsely activates a subset of prompts to achieve dynamic decoupling and collaborative optimization of task-specific feature representations. Furthermore, we introduce a history-aware modulator that leverages cumulative prompt activation statistics to protect frequently used prompts from excessive updates, thereby mitigating inefficient parameter usage and knowledge forgetting. Extensive analysis and empirical results demonstrate that our approach consistently outperforms existing static allocation strategies in effectiveness and efficiency.

</details>


### [129] [TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins](https://arxiv.org/abs/2601.20906)
*Nikita Makarov,Maria Bordukova,Lena Voith von Voithenberg,Estrella Pivel-Villanueva,Sabrina Mielke,Jonathan Wickes,Hanchen Wang,Mingyu Derek Ma,Keunwoo Choi,Kyunghyun Cho,Stephen Ra,Raul Rodriguez-Esteban,Fabian Schmich,Michael Menden*

Main category: cs.LG

TL;DR: TwinWeaver converts longitudinal clinical histories into text to enable unified event prediction and time-series forecasting with large language models; Genie Digital Twin (GDT) evaluated on 93,054 patients across 20 cancer types shows improved forecasting accuracy and risk stratification, with zero-shot generalization and an interpretable reasoning extension.


<details>
  <summary>Details</summary>
Motivation: Precision oncology requires forecasting clinical events and trajectories from sparse, multi-modal time series; traditional models struggle with heterogeneity and missing data; proposing text serialization enables leveraging powerful LLMs and provides a scalable, interpretable framework.

Method: Open-source framework TwinWeaver that serializes longitudinal patient histories into text, enabling unified event prediction and forecasting with large language models; builds Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types; benchmarks against time-series baselines; evaluates zero-shot generalization to out-of-distribution trials; introduces an interpretable clinical reasoning extension.

Result: GDT reduces forecasting error (median MASE 0.87 vs 0.97 for strongest time-series baseline, p<0.001); average C-index 0.703 across survival, progression, therapy switching, vs 0.662 baseline; generalizes to out-of-distribution trials with zero-shot performance and fine-tuning advantage (median MASE 0.75–0.88; C-index 0.672 vs 0.648); open-source and scalable; supports interpretable clinical reasoning extension.

Conclusion: TwinWeaver offers a scalable, transparent foundation for longitudinal clinical modeling, improving predictive performance and generalization by leveraging text-based representations and LLMs, and enabling interpretable clinical reasoning.

Abstract: Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.

</details>


### [130] [Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges](https://arxiv.org/abs/2601.20913)
*Chen Feng,Minghe Shen,Ananth Balashankar,Carsten Gerner-Beuerle,Miguel R. D. Rodrigues*

Main category: cs.LG

TL;DR: 提出在有噪声评审者下，通过小样本人工标注校准，构建方差修正的临界阈值，实现对LLM评审的有限样本误差控制，在噪声与偏差存在时保持统计严格性，并与PPI区分开来。


<details>
  <summary>Details</summary>
Motivation: 在LLM评审场景中，评审者可能存在噪声、偏差，破坏统计保证，需要在有限样本下实现有效的显著性评估和误差控制。

Method: 利用一个小的人工标注校准集估计评审者的真正阳性率和假阳性率（TPR/FPR），对大量评审者标注数据应用方差修正的临界阈值，从而在校准不确定性下实现有限样本的Type-I错误率控制。与Prediction-Powered Inference（PPI）不同，此方法显式建模评审者行为而非将其视为黑箱。

Result: 给出理论条件，当噪声测试比直接评估具有更高统计功效；在Jigsaw Comment、Hate Speech、SafeRLHF数据集上通过实证验证；揭示Oracle Gap并量化估计成本；提供对评审者可靠性、数据规模和认证水平的诊断工具。

Conclusion: 首次系统地处理不完美评审者情景，揭示噪声评审对评估功效的影响及评估工具之间的权衡，增强对LLM评估的统计理解。

Abstract: Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While "LLM-as-a-Judge" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a "Noisy but Valid" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical "Oracle" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.

</details>


### [131] [Noninvasive Intracranial Pressure Estimation Using Subspace System Identification and Bespoke Machine Learning Algorithms: A Learning-to-Rank Approach](https://arxiv.org/abs/2601.20916)
*Anni Zhao,Ayca Ermis,Jeffrey Robert Vitt,Sergio Brasil,Wellingson Paiva,Magdalena Kasprowicz,Malgorzata Burzynska,Robert Hamilton,Runze Yan,Ofer Sadan,J. Claude Hemphill,Lieven Vandenberghe,Xiao Hu*

Main category: cs.LG

TL;DR: 提出一种将子空间系统辨识与排序约束优化结合的非侵入性ICP估计框架，利用ABP、CBv和R-R间期信号来推算平均ICP。


<details>
  <summary>Details</summary>
Motivation: 解决非侵入性ICP估计的挑战，借助非侵入性信号与系统辨识建立脑血流动力学模型，并通过排序约束学习误差映射。

Method: 使用子空间辨识提取脑血流动力学模型；将非侵入信号的特征与ICP估计误差之间的关系通过凸优化的排序约束学习映射函数；数据集在多临床设定中随机分为训练/测试。

Result: 测试集中约31.88%的样本误差<2 mmHg，约34.07%误差在2-6 mmHg之间。

Conclusion: 该非侵入ICP估计方法在可行性方面得到初步支持，但需进一步验证和技术改进以实现临床部署。

Abstract: Objective: Accurate noninvasive estimation of intracranial pressure (ICP) remains a major challenge in critical care. We developed a bespoke machine learning algorithm that integrates system identification and ranking-constrained optimization to estimate mean ICP from noninvasive signals. Methods: A machine learning framework was proposed to obtain accurate mean ICP values using arbitrary noninvasive signals. The subspace system identification algorithm is employed to identify cerebral hemodynamics models for ICP simulation using arterial blood pressure (ABP), cerebral blood velocity (CBv), and R-wave to R-wave interval (R-R interval) signals in a comprehensive database. A mapping function to describe the relationship between the features of noninvasive signals and the estimation errors is learned using innovative ranking constraints through convex optimization. Patients across multiple clinical settings were randomly split into testing and training datasets for performance evaluation of the mapping function. Results: The results indicate that about 31.88% of testing entries achieved estimation errors within 2 mmHg and 34.07% of testing entries between 2 mmHg to 6 mmHg from the nonlinear mapping with constraints. Conclusion: Our results demonstrate the feasibility of the proposed noninvasive ICP estimation approach. Significance: Further validation and technical refinement are required before clinical deployment, but this work lays the foundation for safe and broadly accessible ICP monitoring in patients with acute brain injury and related conditions.

</details>


### [132] [A Theory of Universal Agnostic Learning](https://arxiv.org/abs/2601.20961)
*Steve Hanneke,Shay Moran*

Main category: cs.LG

TL;DR: 给出二元分类在 agnostic 设置的最优普遍收敛速率的完整理论，揭示一个四象限式的 tetrachotomy，以及决定落在哪一象限的简单组合结构。


<details>
  <summary>Details</summary>
Motivation: 在去除 realizability 假设的情况下研究最优 universal 速率，扩展可实现性条件下的理论；建立统一框架以理解不同概念类的学习难度。

Method: 建立理论框架，将概念类的组合结构与速率相联系，提出 tetrachotomy，并给出区分不同速率类别的结构判据。

Result: 对任意概念类，最优 universal 的 excess error 收敛速率属于四类之一：e^{-n}、e^{-o(n)}、o(n^{-1/2})，或任意慢；并识别决定类别的简单组合结构。

Conclusion: 提供一个清晰的分类与判据，统一理解 agnostic 二元分类中的最优速率，并指明哪些结构导致更快或更慢的收敛。

Abstract: We provide a complete theory of optimal universal rates for binary classification in the agnostic setting. This extends the realizable-case theory of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021) by removing the realizability assumption on the distribution. We identify a fundamental tetrachotomy of optimal rates: for every concept class, the optimal universal rate of convergence of the excess error rate is one of $e^{-n}$, $e^{-o(n)}$, $o(n^{-1/2})$, or arbitrarily slow. We further identify simple combinatorial structures which determine which of these categories any given concept class falls into.

</details>


### [133] [Distributional Active Inference](https://arxiv.org/abs/2601.20985)
*Abdullah Akgül,Gulcin Baykal,Manuel Haußmann,Mustafa Mert Çelikok,Melih Kandemir*

Main category: cs.LG

TL;DR: 提出一个覆盖模型基、分布式、模型无关强化学习的抽象，将主动推断无转移动力学建模地整合入分布式强化学习框架，提升样本效率与长期规划能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人系统中的感知状态信息组织和远期行动规划两大挑战；RL 只处理后者，往往样本效率低下；主动推断提供跨领域理论，但在 AI 应用多局限于对现有模型的扩展。

Method: 给出一个正式的强化学习算法抽象，跨越模型基、分布式与模型无关的方法；在此框架内无缝将主动推断整合到分布式 RL 框架，且无需对转移动力学建模。

Result: 在抽象层面实现了主动推断与分布式 RL 的整合，并展示在没有转移动力学建模条件下的潜在性能优势。

Conclusion: 该抽象为在不同 RL 策略中应用主动推断提供统一方法论，提升样本效率与长远规划能力。

Abstract: Optimal control of complex environments with robotic systems faces two complementary and intertwined challenges: efficient organization of sensory state information and far-sighted action planning. Because the reinforcement learning framework addresses only the latter, it tends to deliver sample-inefficient solutions. Active inference is the state-of-the-art process theory that explains how biological brains handle this dual problem. However, its applications to artificial intelligence have thus far been limited to extensions of existing model-based approaches. We present a formal abstraction of reinforcement learning algorithms that spans model-based, distributional, and model-free approaches. This abstraction seamlessly integrates active inference into the distributional reinforcement learning framework, making its performance advantages accessible without transition dynamics modeling.

</details>


### [134] [Pre-trained Encoders for Global Child Development: Transfer Learning Enables Deployment in Data-Scarce Settings](https://arxiv.org/abs/2601.20987)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 提出全球儿童发展领域的首个预训练编码器，在357,709名儿童的跨44国数据上训练，显著提升少样本情况下的监测性能并实现零-shot泛化。


<details>
  <summary>Details</summary>
Motivation: 解决新国家部署中的数据瓶颈：现有模型需要大量样本，而新项目往往只有极少数据；通过跨国预训练提升少样本泛化能力以支持可持续发展目标4.2.1的监测。

Method: 训练一个用于全球儿童发展的预训练编码器，基于UNICEF调查数据，覆盖44个国家；以50个训练样本评估AUC并与冷启动梯度提升对比；在N=500时提升至AUC 0.73；对未见国家进行零-shot部署，AUC最高可达0.84；并应用迁移学习界限解释为什么预训练多样性促进少样本泛化；

Result: 50样本AUC 0.65 (95% CI 0.56–0.72)，领先冷启动梯度提升0.61，提升约8–12%；N=500时AUC 0.73；未见国家零-shot最高0.84；

Conclusion: 预训练编码器可显著提升资源受限环境中的SDG 4.2.1监测的可行性，推动跨国数据协同与模型部署。

Abstract: A large number of children experience preventable developmental delays each year, yet the deployment of machine learning in new countries has been stymied by a data bottleneck: reliable models require thousands of samples, while new programs begin with fewer than 100. We introduce the first pre-trained encoder for global child development, trained on 357,709 children across 44 countries using UNICEF survey data. With only 50 training samples, the pre-trained encoder achieves an average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting at 0.61 by 8-12% across regions. At N=500, the encoder achieves an AUC of 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84. We apply a transfer learning bound to explain why pre-training diversity enables few-shot generalization. These results establish that pre-trained encoders can transform the feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings.

</details>


### [135] [Top-k on a Budget: Adaptive Ranking with Weak and Strong Oracles](https://arxiv.org/abs/2601.20989)
*Lutz Oettershagen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Identifying the top-$k$ items is fundamental but often prohibitive when exact valuations are expensive. We study a two-oracle setting with a fast, noisy weak oracle and a scarce, high-fidelity strong oracle (e.g., human expert verification or expensive simulation). We first analyze a simple screen-then-certify baseline (STC) and prove it makes at most $m(4\varepsilon_{\max})$ strong calls given jointly valid weak confidence intervals with maximum radius $\varepsilon_{\max}$, where $m(\cdot)$ denotes the near-tie mass around the top-$k$ threshold. We establish a conditional lower bound of $Ω(m(\varepsilon_{\max}))$ for any algorithm given the same weak uncertainty. Our main contribution is ACE, an adaptive certification algorithm that focuses strong queries on critical boundary items, achieving the same $O(m(4\varepsilon_{\max}))$ bound while reducing strong calls in practice. We then introduce ACE-W, a fully adaptive two-phase method that allocates weak budget adaptively before running ACE, further reducing strong costs.

</details>


### [136] [The Depth Delusion: Why Transformers Should Be Wider, Not Deeper](https://arxiv.org/abs/2601.20994)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 提出架构有条件的缩放定律，揭示深度与宽度对损失的不同缩放规律，以及一个关键的深度阈值现象“Depth Delusion”；在7B规模下，最优深度并非越深越好。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经缩放定律将架构视为可替换的假设，量化深度与宽度对模型性能的独立贡献，以指导高效的架构设计。

Method: 提出架构条件分解的缩放法则，给出深度D与宽度W相对于参数量C和数据量的最佳规模关系：D* ~ C^{0.12}、W* ~ C^{0.34}，并定义深度阈值D_crit ~ W^{0.44}，在30种Transformer架构(从17M到7B参数)上进行广泛实验验证，数据来自代表性的高算力样本，拟合结果R^2=0.922。

Result: 发现宽度增长的速度高于深度（W随C的指数大于D的指数），且存在Depth Delusion：当深度超过D_crit时，继续增加层数会在增参数的同时提高损失；在7B规模下，64层模型的性能比32层模型差0.12nat，表明深度-宽度的最优权衡在生产规模仍然成立。

Conclusion: 架构对缩放律有显著影响，应把深度与宽度作为独立优化对象，避免盲目叠深；在实际设计与扩展时，应优先考虑增宽策略并关注深度阈值效应，以实现更高效的模型扩展。

Abstract: Neural scaling laws describe how language model loss decreases with parameters and data, but treat architecture as interchangeable--a billion parameters could arise from a shallow-wide model (10 layers & 8,192 hidden dimension) or a deep-narrow one (80 layers & 2,048 hidden dimension). We propose architecture-conditioned scaling laws decomposing this dependence, finding that optimal depth scales as D* ~ C^0.12 while optimal width scales as W* ~ C^0.34, meaning width should grow 2.8x faster than depth. We discover a critical depth phenomenon: beyond D_crit ~ W^0.44 (sublinear in W), adding layers increases loss despite adding parameters--the Depth Delusion. Empirically, we validate these findings across 30 transformer architectures spanning 17M to 7B parameters, each trained on representative high-compute samples, achieving R^2 = 0.922. Our central finding: at 7B scale, a 64-layer model (6.38B params) underperforms a 32-layer model (6.86B params) by 0.12 nats, despite being significantly deeper. This demonstrates that optimal depth-width tradeoffs persist at the production scale.

</details>


### [137] [Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research](https://arxiv.org/abs/2601.21008)
*Ruicheng Ao,David Simchi-Levi,Xinshang Wang*

Main category: cs.LG

TL;DR: 提出两套面向约束优化的LLM评测基准 ORD Debug 和 ORBias，嵌入求解器迭代与 IIS 重计算的可验证反馈，评估结果显示领域特化 RLVR 训练优于前沿 API；课程化训练还能降低 ID→OOD 的偏差。


<details>
  <summary>Details</summary>
Motivation: 现有 OR 任务评测多为一次性生成求解代码，未能验证诊断循环和自我纠错过程。需要可验证的过程级评估以模拟真实工作流并促进训练改进。

Method: ORDebug：5000+ 问题，涵盖9种错误类型；在每次修复后重新执行求解器并重计算 IIS，提供可验证反馈。ORBias：2000个新闻贩卖者实例（1000ID + 1000OOD），用于评估行为理性与策略偏差。共计26种模型、12000+样本。比较对象包含领域特定 RLVR 训练的模型与非领域模型/大模型/API。

Result: 在ORDebug上，8B模型超越前沿API：恢复率95.3%（对比86.2%，提升9.1个百分点）、诊断准确率62.4%（对比47.8%，提升14.6个百分点）、解题步数从3.78降至2.25（约1.7×加速）。在ORBias上，采用课程化训练的模型实现唯一的 ID→OOD 偏差下降（−9.6%），将系统偏差从20.0%降至10.4%，降幅约48%。

Conclusion: 面向可验证的过程级评估可以通过定向训练在不依赖规模扩增的情况下显著提升模型性能。

Abstract: Operations Research practitioners routinely debug infeasible models through an iterative process: analyzing Irreducible Infeasible Subsystems (\IIS{}), identifying constraint conflicts, and systematically repairing formulations until feasibility is achieved. Yet existing LLM benchmarks evaluate OR as one-shot translation -- given a problem description, generate solver code -- ignoring this diagnostic loop entirely. We introduce two benchmarks that place the \textbf{solver in the evaluation loop}. \textbf{\ORDebug{}} evaluates iterative self-correction through 5,000+ problems spanning 9 error types; each repair action triggers solver re-execution and \IIS{} recomputation, providing deterministic, verifiable feedback. \textbf{\ORBias{}} evaluates behavioral rationality through 2,000 newsvendor instances (1,000 ID + 1,000 OOD), measuring systematic deviations from closed-form optimal policies. Across 26 models and 12,000+ samples, we find that domain-specific RLVR training enables an 8B model to surpass frontier APIs: 95.3\% vs 86.2\% recovery rate (+9.1\%), 62.4\% vs 47.8\% diagnostic accuracy (+14.6\%), and 2.25 vs 3.78 steps to resolution (1.7$\times$ faster). On \ORBias{}, curriculum training achieves the only negative ID$\rightarrow$OOD bias drift among models evaluated (-9.6\%), reducing systematic bias by 48\% (from 20.0\% to 10.4\%). These results demonstrate that process-level evaluation with verifiable oracles enables targeted training that outperforms scale.

</details>


### [138] [Order-Aware Test-Time Adaptation: Leveraging Temporal Dynamics for Robust Streaming Inference](https://arxiv.org/abs/2601.21012)
*Young Kyung Kim,Oded Schlesinger,Qiangqiang Wu,J. Matías Di Martino,Guillermo Sapiro*

Main category: cs.LG

TL;DR: OATTA 提出一种面向时序的测试时自适应框架，通过学习的动态转移矩阵作为时序先验，结合梯度无关的递归贝叶斯估计与似然比门控（LLR），实现对基模型预测的 refined 调整，且开销极小。实验证明在多模态任务上可广泛适用，准确率提升可达 6.35%。


<details>
  <summary>Details</summary>
Motivation: 当前的测试时自适应（TTA）多将测试流视为独立样本，未充分利用时间序列中的监督信号，导致对分布漂移的适应性受限。

Method: 采用梯度无关的递归贝叶斯估计，将学习得到的动态转移矩阵作为时间先验，细化基模型的预测；引入似然比门（LLR）在缺乏时序证据时回退到基预测器，确保在弱结构流中的安全性；模块轻量、模型无关。

Result: 在图像分类、可穿戴与生理信号分析、语言情感分析等任务上均显示通用性，显著提升基线性能，单个任务中的准确率提升达最大 6.35%（相对或绝对提升在文献中未指明，按摘要理解为绝对）。

Conclusion: 建模时序动态为标准无序TTA的正交信号来源，OATTA 作为轻量且可移植的模块，具有广泛适用性和低计算开销。

Abstract: Test-Time Adaptation (TTA) enables pre-trained models to adjust to distribution shift by learning from unlabeled test-time streams. However, existing methods typically treat these streams as independent samples, overlooking the supervisory signal inherent in temporal dynamics. To address this, we introduce Order-Aware Test-Time Adaptation (OATTA). We formulate test-time adaptation as a gradient-free recursive Bayesian estimation task, using a learned dynamic transition matrix as a temporal prior to refine the base model's predictions. To ensure safety in weakly structured streams, we introduce a likelihood-ratio gate (LLR) that reverts to the base predictor when temporal evidence is absent. OATTA is a lightweight, model-agnostic module that incurs negligible computational overhead. Extensive experiments across image classification, wearable and physiological signal analysis, and language sentiment analysis demonstrate its universality; OATTA consistently boosts established baselines, improving accuracy by up to 6.35%. Our findings establish that modeling temporal dynamics provides a critical, orthogonal signal beyond standard order-agnostic TTA approaches.

</details>


### [139] [SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model](https://arxiv.org/abs/2601.21031)
*Zongheng Guo,Tao Chen,Yang Jiao,Yi Pan,Xiao Hu,Manuela Ferrario*

Main category: cs.LG

TL;DR: SIGMA-PPG是一种基于生成的基础模型，结合先验引导的对抗 masking 与向量量化语义一致性，用于抑制PPG信号中的冗余与噪声。通过强化学习驱动的教师利用统计先验生成具有挑战性的学习路径，避免对噪声过拟合，并通过代码本语义化密度提升来统一生理等效波形。以超过12万小时数据进行预训练，在12项下游任务上优于五个SOTA基线，且公开代码。


<details>
  <summary>Details</summary>
Motivation: 解决PPG信号中固有的冗余与噪声对当代基础模型训练的制约；现有的标准掩码建模易产生平庸解，对比学习缺乏形态学精度。

Method: 在生成型框架中引入先验引导的对抗掩蔽（Priori-Guided Adversarial Masking），由强化学习驱动的教师利用统计先验设计具有挑战性的学习路径以防止对噪声的过拟合。引入向量量化的语义一致性约束，确保生理等同的波形在记录伪影或小扰动下仍映射到共享的索引，提升代码本的语义密度并消除冗余特征结构。

Result: 在超过12万小时数据上预训练后，SIGMA-PPG在12项下游任务上的平均性能优于五个最先进基线。

Conclusion: 该架构通过先验引导的对抗掩蔽和语义一致性约束，提升PPG信号的泛化与表征质量，提升代码本的语义密度并减少冗余特征，展现出作为PPG领域生成性基础模型的潜力。

Abstract: Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.

</details>


### [140] [Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints](https://arxiv.org/abs/2601.21033)
*Omer Rochman-Sharabi,Gilles Louppe*

Main category: cs.LG

TL;DR: 在扩散模型上引入一个约束采样框架，通过 Predict-Project-Renoise（PPR）在生成时强制满足硬约束，显著提高约束满足度和与有约束分布的一致性。


<details>
  <summary>Details</summary>
Motivation:  vanilla diffusion models 不能保证物理约束或观测一致性，亟需在生成阶段强制遵循约束以获得物理可信的模拟。

Method: 定义一个约束前向扩散过程，使扩散仅在可行约束集合内进行，从而得到约束边际分布。反向过程采用 Predict-Project-Renoise (PPR) 迭代算法：先进行去噪预测，再投影到可行集以满足约束，最后重新加噪以继续扩散，循环以采样约束边际。

Result: 在二维分布、偏微分方程和全球天气预报等任务上，PPR 将约束违约降低超过一个数量级，样本更具一致性，并且比基线更贴近真实的有约束分布。

Conclusion: 该框架实现了扩散模型的生成时硬约束，提升物理可行性与可信度，适用于需要严格约束满足的科学模拟场景。

Abstract: Neural emulators based on diffusion models show promise for scientific applications, but vanilla models cannot guarantee physical accuracy or constraint satisfaction. We address this by introducing a constrained sampling framework that enforces hard constraints, such as physical laws or observational consistency, at generation time. Our approach defines a constrained forward process that diffuses only over the feasible set of constraint-satisfying samples, inducing constrained marginal distributions. To reverse this, we propose Predict-Project-Renoise (PPR), an iterative algorithm that samples from the constrained marginals by alternating between denoising predictions, projecting onto the feasible set, and renoising. Experiments on 2D distributions, PDEs, and global weather forecasting demonstrate that PPR reduces constraint violations by over an order of magnitude while improving sample consistency and better matching the true constrained distribution compared to baselines.

</details>


### [141] [Test-Time Adaptation for Unsupervised Combinatorial Optimization](https://arxiv.org/abs/2601.21048)
*Yiqiao Liao,Farinaz Koushanfar,Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: 提出 TACO：一种模型无关的测试时自适应框架，通过有选择地“热启动”部分参数来在保持归纳偏置的同时实现快速、有效的无监督NCO实例自适应，在较低开销下优于从头开始或仅基于泛化模型的微调。


<details>
  <summary>Details</summary>
Motivation: 现有无监督NCO分为两类：面向跨实例泛化的模型（推理高效但缺乏实例级自适应）与在测试时独立优化的实例特定模型（灵活但未充分利用学习到的结构，易陷入局部最优）。两者各自有局限，无法兼顾泛化带来的结构先验与实例化自适应的灵活性。

Method: 提出一个模型无关的测试时自适应框架 TACO（Test-time Adaptation for CO），通过对训练好但部分参数进行策略性放松的热启动，保留原有泛化带来的 inductive bias，同时实现快速且有效的无监督适应。与从头微调泛化模型或仅训练实例模型相比，TACO 在解質质量上更优且计算开销极小。适用于最小顶点覆盖、最大团等经典组合优化问题，并在静态、分布偏移及动态场景下表现鲁棒。

Result: 实证显示 TACO 在多种CO问题及场景中提升解质量并具备良好鲁棒性，证明其作为泛化-实例特异两种无监督NCO范式之间的实际桥梁的有效性。

Conclusion: TACO 为无监督NCO 提供了一个实用的桥梁，兼容泛化学习的结构先验与实例级自适应的灵活性，具备良好的通用性与可扩展性，未来工作可拓展到更广的CO问题和更复杂的分布变化场景。

Abstract: Unsupervised neural combinatorial optimization (NCO) enables learning powerful solvers without access to ground-truth solutions. Existing approaches fall into two disjoint paradigms: models trained for generalization across instances, and instance-specific models optimized independently at test time. While the former are efficient during inference, they lack effective instance-wise adaptability; the latter are flexible but fail to exploit learned inductive structure and are prone to poor local optima. This motivates the central question of our work: how can we leverage the inductive bias learned through generalization while unlocking the flexibility required for effective instance-wise adaptation? We first identify a challenge in bridging these two paradigms: generalization-focused models often constitute poor warm starts for instance-wise optimization, potentially underperforming even randomly initialized models when fine-tuned at test time. To resolve this incompatibility, we propose TACO, a model-agnostic test-time adaptation framework that unifies and extends the two existing paradigms for unsupervised NCO. TACO applies strategic warm-starting to partially relax trained parameters while preserving inductive bias, enabling rapid and effective unsupervised adaptation. Crucially, compared to naively fine-tuning a trained generalizable model or optimizing an instance-specific model from scratch, TACO achieves better solution quality while incurring negligible additional computational cost. Experiments on canonical CO problems, Minimum Vertex Cover and Maximum Clique, demonstrate the effectiveness and robustness of TACO across static, distribution-shifted, and dynamic combinatorial optimization problems, establishing it as a practical bridge between generalizable and instance-specific unsupervised NCO.

</details>


### [142] [Snowball: A Scalable All-to-All Ising Machine with Dual-Mode Markov Chain Monte Carlo Spin Selection and Asynchronous Spin Updates for Fast Combinatorial Optimization](https://arxiv.org/abs/2601.21058)
*Seungki Hong,Kyeongwon Jeong,Taekwang Jang*

Main category: cs.LG

TL;DR: Snowball：一个数字化、可扩展的全耦合 Ising 机，结合双模 MCMC 自旋选择和异步更新，支持宽范围的耦合精度，在 AMD Alveo U250 上实现相同基准下约 8×的时间-解缩短；展示数字化实现的可行性与性能提升。


<details>
  <summary>Details</summary>
Motivation: 为实际部署的 Ising 机降低时间-解的瓶颈，解决三大挑战：硬件拓扑受限导致的嵌入开销、简单并行更新造成的振荡/停滞、以及耦合系数精度受限对映射与解质量的影响。

Method: 设计并实现 Snowball：一个数字化、全互耦的 Ising 机，集成双模马尔科夫链蒙特卡洛自旋选择与异步自旋更新，且具备宽耦合精度配置；在 AMD Alveo U250 加速卡上实现原型。

Result: 在相同基准实例上，相较于现有最先进的 Ising 机，时间-解缩短约 8 倍。

Conclusion: 证明高精度耦合的数字化 Ising 机与混合自旋更新策略可显著提升收敛性及时间效率，具备良好的扩展性与部署潜力。

Abstract: Ising machines have emerged as accelerators for combinatorial optimization. To enable practical deployment, this work aims to reduce time-to-solution by addressing three challenges: (1) hardware topology, (2) spin selection and update algorithms, and (3) scalable coupling-coefficient precision. Restricted topologies require minor embedding; naive parallel updates can oscillate or stall; and limited precision can preclude feasible mappings or degrade solution quality.
  This work presents Snowball, a digital, scalable, all-to-all coupled Ising machine that integrates dual-mode Markov chain Monte Carlo spin selection with asynchronous spin updates to promote convergence and reduce time-to-solution. The digital architecture supports wide, configurable coupling precision, unlike many analog realizations at high bit widths. A prototype on an AMD Alveo U250 accelerator card achieves an 8$\times$ reduction in time-to-solution relative to a state-of-the-art Ising machine on the same benchmark instance.

</details>


### [143] [Human-LLM Collaborative Feature Engineering for Tabular Data](https://arxiv.org/abs/2601.21060)
*Zhuoyan Li,Aditya Bansal,Jinzhao Li,Shishuang He,Zhuoran Lu,Mutian Zhang,Qin Liu,Yiwei Yang,Swati Jain,Ming Yin,Yunyao Li*

Main category: cs.LG

TL;DR: 提出一种人机协作的表格学习特征工程框架：将LLM用于特征变换操作的候选生成，利用显式的效用与不确定性建模来进行操作选择，并在早期阶段通过可选的人类偏好反馈提升选定效果，从而提高性能并降低认知负担。


<details>
  <summary>Details</summary>
Motivation: 将LLM用于特征工程时常作为黑箱优化器进行，缺乏对操作效用的 calibrated 估计，易重复探索低收益操作，缺少优先化的原则性策略；需要更高效且可解释的探索-利用平衡。

Method: 将特征变换操作的提议与选择解耦：LLM仅负责生成候选操作；对每个候选操作进行效用与不确定性建模以供选择；在早期阶段引入人类专家偏好反馈来引导选择，提升有效操作的发现。通过合成数据与真实用户研究的评估，验证框架在多种表格数据集上的性能和用户认知负荷减轻效果。

Result: 框架在合成及真实用户研究中均能提升特征工程的性能，且降低用户在特征工程过程中的认知负担；证明了人机协作在特征工程中的有效性。

Conclusion: 提出的解耦式人机协作框架为表格学习中的LLM特征工程提供一种可量化的效用与不确定性驱动的选择机制，并通过人类偏好信息增强早期探索效率，具有良好的实用性与扩展性。

Abstract: Large language models (LLMs) are increasingly used to automate feature engineering in tabular learning. Given task-specific information, LLMs can propose diverse feature transformation operations to enhance downstream model performance. However, current approaches typically assign the LLM as a black-box optimizer, responsible for both proposing and selecting operations based solely on its internal heuristics, which often lack calibrated estimations of operation utility and consequently lead to repeated exploration of low-yield operations without a principled strategy for prioritizing promising directions. In this paper, we propose a human-LLM collaborative feature engineering framework for tabular learning. We begin by decoupling the transformation operation proposal and selection processes, where LLMs are used solely to generate operation candidates, while the selection is guided by explicitly modeling the utility and uncertainty of each proposed operation. Since accurate utility estimation can be difficult especially in the early rounds of feature engineering, we design a mechanism within the framework that selectively elicits and incorporates human expert preference feedback, comparing which operations are more promising, into the selection process to help identify more effective operations. Our evaluations on both the synthetic study and the real user study demonstrate that the proposed framework improves feature engineering performance across a variety of tabular datasets and reduces users' cognitive load during the feature engineering process.

</details>


### [144] [Out-of-Distribution Generalization in Graph Foundation Models](https://arxiv.org/abs/2601.21067)
*Haoyang Li,Haibo Chen,Xin Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: 对图基础模型（GFMs）在分布外（OOD）泛化的研究做系统综述，梳理分布迁移的挑战、统一问题设定、按任务固定性对比方法、总结OOD处理策略与预训练目标、评估协议与未来研究方向；首次聚焦GFMs的OOD泛化。


<details>
  <summary>Details</summary>
Motivation: 图学习在训练分布之外的泛化能力差，原因包括图结构、领域语义、可用模态、任务表述的变化等。GFMs通过大规模跨图与跨任务预训练，期望获得稳健的通用表征，因此需要理解在OOD情景下的表现与挑战。

Method: 将现有方法按是否在固定任务下工作或跨异构任务表述下实现泛化进行整理；总结相应的OOD处理策略与预训练目标；回顾评估协议；讨论未来研究方向。

Result: 提出并整理了一种针对GFMs的OOD泛化的系统框架与分类，明确了评估和研究方向，标注出当前研究的局限性与空白。

Conclusion: GFMs在OOD泛化方面已有初步进展，但仍需建立统一的评估标准、更具鲁棒性的预训练目标，以及能跨任务、跨域的泛化能力。该综述为后续研究提供了清晰的研究脉络与方向。

Abstract: Graphs are a fundamental data structure for representing relational information in domains such as social networks, molecular systems, and knowledge graphs. However, graph learning models often suffer from limited generalization when applied beyond their training distributions. In practice, distribution shifts may arise from changes in graph structure, domain semantics, available modalities, or task formulations. To address these challenges, graph foundation models (GFMs) have recently emerged, aiming to learn general-purpose representations through large-scale pretraining across diverse graphs and tasks. In this survey, we review recent progress on GFMs from the perspective of out-of-distribution (OOD) generalization. We first discuss the main challenges posed by distribution shifts in graph learning and outline a unified problem setting. We then organize existing approaches based on whether they are designed to operate under a fixed task specification or to support generalization across heterogeneous task formulations, and summarize the corresponding OOD handling strategies and pretraining objectives. Finally, we review common evaluation protocols and discuss open directions for future research. To the best of our knowledge, this paper is the first survey for OOD generalization in GFMs.

</details>


### [145] [Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed](https://arxiv.org/abs/2601.21094)
*Minjae Kwon,Josephine Lamp,Lu Feng*

Main category: cs.LG

TL;DR: 训练时的安全约束在分布转移下往往不泛化，需使用测试时屏蔽等方法以恢复安全性；在糖尿病管理的安全RL测试床上，8种算法、3种糖尿病类型、3个年龄组，屏蔽提升Time-in-Range约13-14%，并降低临床风险和血糖波动。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习在分布转移中的安全性不足成为现实世界部署的瓶颈。研究在统一临床仿真器中验证安全约束的泛化能力，并提出通过学习动力学模型的屏蔽策略在测试阶段过滤不安全动作以实现跨人群的安全性保障。

Method: 构建统一的糖尿病管理临床仿真器，基于8种安全RL算法、3种糖尿病类型、3个年龄段进行评估；设计测试时屏蔽（test-time shielding），用学习到的动力学模型过滤不可接受动作；比较训练时约束满足与否与实际安全表现。

Result: 屏蔽在8种算法、3种糖尿病类型、3个年龄段上实现Time-in-Range提升13-14%，显著降低临床风险指数和血糖方差/波动；训练时满足约束并不等价于测试时安全，屏蔽能跨算法与人群恢复安全性；提供一个用于分布转移情景下安全性的基准平台与代码。

Conclusion: 对安全RL在分布转移下的安全性提供实证证据，强调测试时干预的重要性；研究平台和基准将促进在安全关键控制领域的安全性研究。

Abstract: Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.

</details>


### [146] [TRACE: Trajectory Recovery for Continuous Mechanism Evolution in Causal Representation Learning](https://arxiv.org/abs/2601.21135)
*Shicheng Fan,Kun Zhang,Lu Cheng*

Main category: cs.LG

TL;DR: Proposes TRACE, a temporal causal representation method modeling gradual mechanism changes as convex combinations of finite atomic mechanisms, with identifiability guarantees for latent variables and the mixing trajectory.


<details>
  <summary>Details</summary>
Motivation: Real systems often exhibit smooth, continuous transitions between mechanisms, not abrupt discrete switches; existing methods assuming discrete domains fail to capture this gradual evolution.

Method: Introduce TRACE, a Mixture-of-Experts framework where each expert learns an atomic mechanism. Time-varying mixing coefficients form a convex combination of experts, enabling recovery of the continuous mechanism trajectory. The setup allows inference of both latent causal variables and their transition path, including intermediate states unseen during training.

Result: Theoretical results establish joint identifiability of latent variables and mixing trajectory. Empirical results on synthetic and real data show mixing trajectories recovered with up to 0.99 correlation and substantial improvements over discrete-switching baselines.

Conclusion: TRACE effectively models and recovers continuous mechanism transitions, generalizing to unseen intermediate states and advancing temporal causal representation learning.

Abstract: Temporal causal representation learning methods assume that causal mechanisms switch instantaneously between discrete domains, yet real-world systems often exhibit continuous mechanism transitions. For example, a vehicle's dynamics evolve gradually through a turning maneuver, and human gait shifts smoothly from walking to running. We formalize this setting by modeling transitional mechanisms as convex combinations of finitely many atomic mechanisms, governed by time-varying mixing coefficients. Our theoretical contributions establish that both the latent causal variables and the continuous mixing trajectory are jointly identifiable. We further propose TRACE, a Mixture-of-Experts framework where each expert learns one atomic mechanism during training, enabling recovery of mechanism trajectories at test time. This formulation generalizes to intermediate mechanism states never observed during training. Experiments on synthetic and real-world data demonstrate that TRACE recovers mixing trajectories with up to 0.99 correlation, substantially outperforming discrete-switching baselines.

</details>


### [147] [Smooth Dynamic Cutoffs for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2601.21147)
*Kevin Han,Haolin Cong,Bowen Deng,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出动态 cutoff 的 MLIPs，以降低内存和推理时间，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 解决固定且常数的 cutoff 导致的内存与计算瓶颈，推动大尺度分子动力学的可扩展性。

Method: 提出动态 cutoff，并通过对原子图实施固定邻居数的稀疏化，将其应用于四种主流 MLIP 架构（MACE、Nequip、OrbV3、TensorNet），在材料与分子数据集上评估。

Result: 在多个体系上实现约 2.26×的内存降低和 2.04×的推理加速，精度仅有最小下降，长期时间尺度的稳定性得到保持；实现与训练代码开源。

Conclusion: 动态 cutoff 为提升 MLIPs 可扩展性的一种可行方法，能够在保持精度的前提下显著降低资源消耗。

Abstract: Machine learning interatomic potentials (MLIPs) have proven to be wildly useful for molecular dynamics simulations, powering countless drug and materials discovery applications. However, MLIPs face two primary bottlenecks preventing them from reaching realistic simulation scales: inference time and memory consumption. In this work, we address both issues by challenging the long-held belief that the cutoff radius for the MLIP must be held to a fixed, constant value. For the first time, we introduce a dynamic cutoff formulation that still leads to stable, long timescale molecular dynamics simulation. In introducing the dynamic cutoff, we are able to induce sparsity onto the underlying atom graph by targeting a specific number of neighbors per atom, significantly reducing both memory consumption and inference time. We show the effectiveness of a dynamic cutoff by implementing it onto 4 state of the art MLIPs: MACE, Nequip, Orbv3, and TensorNet, leading to 2.26x less memory consumption and 2.04x faster inference time, depending on the model and atomic system. We also perform an extensive error analysis and find that the dynamic cutoff models exhibit minimal accuracy dropoff compared to their fixed cutoff counterparts on both materials and molecular datasets. All model implementations and training code will be fully open sourced.

</details>


### [148] [Mobility-Embedded POIs: Learning What A Place Is and How It Is Used from Human Movement](https://arxiv.org/abs/2601.21149)
*Maria Despoina Siampou,Shushman Choudhury,Shang-Ling Hsu,Neha Arora,Cyrus Shahabi*

Main category: cs.LG

TL;DR: ME-POIs combine text-based POI embeddings with large-scale mobility signals to learn function-based POI representations, using temporally contextualized visit embeddings and contrastive alignment, plus multi-scale spatial propagation to mitigate data sparsity. This yields superior map-enrichment performance over text-only and mobility-only baselines.


<details>
  <summary>Details</summary>
Motivation: 现有POI表示多聚焦于静态文本元数据中的身份或基于轨迹的移动规律，未充分捕捉“POI的功能”这一使用层面的信息。缺乏以实际使用场景为核心的通用POI表征，限制了跨任务迁移和真实世界应用。

Method: 构建ME-POIs框架：先将个体访问事件编码为时序上下文化的嵌入；通过对比学习将访问嵌入与可学习的POI表示对齐，捕捉跨用户、跨时间的使用模式；为解决长尾稀疏问题，提出跨多尺度空间的近邻POIvisit模式传播机制，将高频访问模式从附近POI传播到目标区域；最后将来自文本的POI嵌入与ME-POIs嵌入联合训练，以提升通用性。

Result: 在五个新提出的地图丰富化任务上，ME-POIs对文本嵌入、移动信息以及二者结合的基线均显示出优势，文本+ME-POIs相较于文本-only和mobility-only均有提升；仅用Mobility的ME-POIs在某些任务上甚至超越文本模型，表明POI的功能信号对表示学习至关重要。

Conclusion: 将POI的使用功能信号引入嵌入 learning，可显著提升POI表示的普适性与任务表现，文本与移动信号的结合可获得最佳效果；未来研究可进一步优化跨源对齐与多尺度传播策略，以增强对稀疏POI的鲁棒性与跨域适应性。

Abstract: Recent progress in geospatial foundation models highlights the importance of learning general-purpose representations for real-world locations, particularly points-of-interest (POIs) where human activity concentrates. Existing approaches, however, focus primarily on place identity derived from static textual metadata, or learn representations tied to trajectory context, which capture movement regularities rather than how places are actually used (i.e., POI's function). We argue that POI function is a missing but essential signal for general POI representations. We introduce Mobility-Embedded POIs (ME-POIs), a framework that augments POI embeddings derived, from language models with large-scale human mobility data to learn POI-centric, context-independent representations grounded in real-world usage. ME-POIs encodes individual visits as temporally contextualized embeddings and aligns them with learnable POI representations via contrastive learning to capture usage patterns across users and time. To address long-tail sparsity, we propose a novel mechanism that propagates temporal visit patterns from nearby, frequently visited POIs across multiple spatial scales. We evaluate ME-POIs on five newly proposed map enrichment tasks, testing its ability to capture both the identity and function of POIs. Across all tasks, augmenting text-based embeddings with ME-POIs consistently outperforms both text-only and mobility-only baselines. Notably, ME-POIs trained on mobility data alone can surpass text-only models on certain tasks, highlighting that POI function is a critical component of accurate and generalizable POI representations.

</details>


### [149] [A Federated Generalized Expectation-Maximization Algorithm for Mixture Models with an Unknown Number of Components](https://arxiv.org/abs/2601.21160)
*Michael Ibrahim,Nagi Gebraeel,Weijun Xie*

Main category: cs.LG

TL;DR: FedGEM 是一个联邦聚类方法，在未知全局簇数 K 和本地可能重叠的簇集合下，通过局部 EM 与不确定集、以及服务器端对全局簇重叠的推断完成聚类任务。理论上给出概率收敛性保证；在 isotropic GMM 场景提供低复杂度实现；实验显示接近集中 EM、优于某些现有联邦聚类方法。


<details>
  <summary>Details</summary>
Motivation: 在多客户端数据场景下，簇集合具有异质性且可能重叠，且全局簇数未知；现有联邦聚类难以在保护隐私的前提下有效对齐簇和估计全局簇数。

Method: 每个客户端本地执行 EM 的同时，围绕每个局部簇最大化点构造不确定集合。服务器利用这些不确定集合学习跨客户端簇的潜在重叠并通过闭式计算推断全局簇数。针对 isotropic GMM 提供简化计算以降低通信与计算成本，并给出收敛性的理论假设与证明。

Result: 给出对联邦聚类情形的概率收敛性保证；在 isotropic GMM 条件下实现低复杂度的逐轮更新；实验表明 FedGEM 的聚类效果与集中 EM 相当，且优于若干现有的联邦聚类算法。

Conclusion: FedGEM 提供一种在未知全局簇数和局部簇集合可能重叠的前提下的联邦聚类框架，具备理论收敛性与实际可行性；未来可扩展到更一般的分布、非同质性以及降低通信开销等方向。

Abstract: We study the problem of federated clustering when the total number of clusters $K$ across clients is unknown, and the clients have heterogeneous but potentially overlapping cluster sets in their local data. To that end, we develop FedGEM: a federated generalized expectation-maximization algorithm for the training of mixture models with an unknown number of components. Our proposed algorithm relies on each of the clients performing EM steps locally, and constructing an uncertainty set around the maximizer associated with each local component. The central server utilizes the uncertainty sets to learn potential cluster overlaps between clients, and infer the global number of clusters via closed-form computations. We perform a thorough theoretical study of our algorithm, presenting probabilistic convergence guarantees under common assumptions. Subsequently, we study the specific setting of isotropic GMMs, providing tractable, low-complexity computations to be performed by each client during each iteration of the algorithm, as well as rigorously verifying assumptions required for algorithm convergence. We perform various numerical experiments, where we empirically demonstrate that our proposed method achieves comparable performance to centralized EM, and that it outperforms various existing federated clustering methods.

</details>


### [150] [Efficient Simple Regret Algorithms for Stochastic Contextual Bandits](https://arxiv.org/abs/2601.21167)
*Shuai Liu,Alireza Bakhtiari,Alex Ayoub,Botao Hao,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 提出首个在随机背景逻辑回归带权的简单遗憾（simple regret）界的算法，达到 (d/1) 的无 κ 依赖的上界；并给出针对线性情境的 Thompson Sampling 变体，达到 (d^{3/2}/1)，同样在逻辑回归情景下推广且前项不依赖 κ。对有限动作集可解，并给出实验验证。


<details>
  <summary>Details</summary>
Motivation: 填补随机背景下逻辑回归带的简单遗憾界空缺。将上下文线性带权方法与自圆分析结合，提出可扩展且可计算的算法；同时引入针对简单遗憾的新型 Thompson Sampling，兼容线性与逻辑回归情景。

Method: 提出两类算法：1) 确定性算法，在复杂度不变的情况下实现简单遗憾 (d/1) 的界，且 leading 常数不随  = exp(S) 变化。2) 为简单遗憾设计的 Thompson Sampling 变体，在随机化场景下对线性情境给出 (d^{3/2}/1) 的简单遗憾界，并扩展到逻辑回归情景，仍无 leading κ 依赖。对有限动作集可全局可解。

Result: 首次给出随机背景下的 logistic/contextual bandit 的简单遗憾界，leading term 不含 κ；随机化算法比确定性算法更低开销；对有限动作集具备可解性；并给出实验以验证理论界。

Conclusion: 本文填补了在逻辑回归情景下简单遗憾的空白，展示了 κ 无关的领先项界，以及随机化算法在简化计算成本方面的优势。为后续在其他非线性情景的简单遗憾研究与实践应用提供理论与方法基础。

Abstract: We study stochastic contextual logistic bandits under the simple regret objective. While simple regret guarantees have been established for the linear case, no such results were previously known for the logistic setting. Building on ideas from contextual linear bandits and self-concordant analysis, we propose the first algorithm that achieves simple regret $\tilde{\mathcal{O}}(d/\sqrt{T})$. Notably, the leading term of our regret bound is free of the constant $κ= \mathcal O(\exp(S))$, where $S$ is a bound on the magnitude of the unknown parameter vector. The algorithm is shown to be fully tractable when the action set is finite. We also introduce a new variant of Thompson Sampling tailored to the simple-regret setting. This yields the first simple regret guarantee for randomized algorithms in stochastic contextual linear bandits, with regret $\tilde{\mathcal{O}}(d^{3/2}/\sqrt{T})$. Extending this method to the logistic case, we obtain a similarly structured Thompson Sampling algorithm that achieves the same regret bound -- $\tilde{\mathcal{O}}(d^{3/2}/\sqrt{T})$ -- again with no dependence on $κ$ in the leading term. The randomized algorithms, as expected, are cheaper to run than their deterministic counterparts. Finally, we conducted a series of experiments to empirically validate these theoretical guarantees.

</details>


### [151] [The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset](https://arxiv.org/abs/2601.21170)
*Augusto Santos,Teresa Santos,Catarina Rodrigues,José M. F. Moura*

Main category: cs.LG

TL;DR: 通过一个一参数家族估计量（经验协方差或精度矩阵的幂）学习特征表征，并用有监督分类器实现对突发性事件的早期检测，同时给出可解释的结构性特征。


<details>
  <summary>Details</summary>
Motivation: 在复杂系统中揭示并利用潜在的因果结构；数据生成过程未知且部分观测，需在预测性与可解释性之间取得平衡。

Method: 从单参数的估计量族（协方差/精度矩阵的幂）学习特征表示；进行结构一致性证明；随后用监督模块对表示进行分类。

Result: 在癫痫发作检测与客户流失预测上实现有竞争力的结果；对最佳协方差幂具有良好可识别性且捕捉到结构性特征。

Conclusion: 该方法在提升预测性能的同时实现可解释的统计结构，提供对潜在因果结构的可控、可解释探索。

Abstract: Emergent phenomena -- onset of epileptic seizures, sudden customer churn, or pandemic outbreaks -- often arise from hidden causal interactions in complex systems. We propose a machine learning method for their early detection that addresses a core challenge: unveiling and harnessing a system's latent causal structure despite the data-generating process being unknown and partially observed. The method learns an optimal feature representation from a one-parameter family of estimators -- powers of the empirical covariance or precision matrix -- offering a principled way to tune in to the underlying structure driving the emergence of critical events. A supervised learning module then classifies the learned representation. We prove structural consistency of the family and demonstrate the empirical soundness of our approach on seizure detection and churn prediction, attaining competitive results in both. Beyond prediction, and toward explainability, we ascertain that the optimal covariance power exhibits evidence of good identifiability while capturing structural signatures, thus reconciling predictive performance with interpretable statistical structure.

</details>


### [152] [AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection](https://arxiv.org/abs/2601.21171)
*Kamal Berahmand,Saman Forouzandeh,Mehrnoush Mohammadi,Parham Moradi,Mahdi Jalili*

Main category: cs.LG

TL;DR: AC2L-GAD: 将信息论驱动的主动选择与反事实生成结合，在图对比学习中实现高效且具有强鲁棒性的异常检测，解决标签稀缺和极端类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决图异常检测中的两个核心挑战：标注数据稀缺/类别不平衡，以及现有图对比学习在正样本对上的语义一致性与负样本对的挑战。

Method: 提出主动对抗对比学习框架 AC2L-GAD：利用信息论的主动采样选择结构上复杂的节点；通过反事实生成实现异常保持的正向增强，同时生成具挑战性的普通负样本；将昂贵的反事实生成仅限于策略性子集以降低计算开销约约65%；在九个基准数据集（含 GADBench 的真实金融交易图）上与先进基线竞争或超越。

Result: 在九个基准数据集上表现竞争性甚至优越，且在异常与属性-结构交互复杂的情形下 gains 显著；相比全图反事实生成，计算成本下降约65%，维持检测质量。

Conclusion: AC2L-GAD 展现出在标签稀缺的图异常检测场景下的有效性与效率平衡，尤其适用于复杂属性-结构交互的异常场景，体现了信息论驱动的主动采样与有选择性的反事实生成的潜力。

Abstract: Graph anomaly detection aims to identify abnormal patterns in networks, but faces significant challenges from label scarcity and extreme class imbalance. While graph contrastive learning offers a promising unsupervised solution, existing methods suffer from two critical limitations: random augmentations break semantic consistency in positive pairs, while naive negative sampling produces trivial, uninformative contrasts. We propose AC2L-GAD, an Active Counterfactual Contrastive Learning framework that addresses both limitations through principled counterfactual reasoning. By combining information-theoretic active selection with counterfactual generation, our approach identifies structurally complex nodes and generates anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts, while restricting expensive counterfactual generation to a strategically selected subset. This design reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Experiments on nine benchmark datasets, including real-world financial transaction graphs from GADBench, show that AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.

</details>


### [153] [Breaking the Reasoning Horizon in Entity Alignment Foundation Models](https://arxiv.org/abs/2601.21174)
*Yuanning Cui,Zequn Sun,Wei Hu,Kexuan Xin,Zhangjie Fu*

Main category: cs.LG

TL;DR: 提出一个基于并行编码的实体对齐（EA）基础模型，通过种子对作为局部锚点引导信息流，结合合并关系图和可学习交互模块，显著提升对未见KG的对齐迁移性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有EA缺乏跨KG泛化能力；虽然图形基金会模型（GFM）可提供某些帮助，但直接迁移到EA效果有限。原因在于EA需要在稀疏且异构的KG结构中捕捉长距离依赖，存在“推理视界差距”。需要提升跨未见KG的可迁移性并降低推理成本。

Method: 提出并行编码策略：以种子EA对作为局部锚点，初始化并行的两条信息流并进行锚点条件化的信息传递，从而缩短推理轨迹并聚焦局部结构；引入合并关系图以建模全局依赖；设计可学习的交互模块实现精确匹配。

Result: 大量实验验证了方法的有效性，显示对未见KG也具备强泛化能力，并在迁移性与对齐精度方面相较基线取得显著提升。

Conclusion: EA基础模型通过并行编码、锚点引导与全局-局部信息融合缓解推理视界差距，提升跨KG对齐的可迁移性与效能，适用于大规模异构知识图的对齐任务。

Abstract: Entity alignment (EA) is critical for knowledge graph (KG) fusion. Existing EA models lack transferability and are incapable of aligning unseen KGs without retraining. While using graph foundation models (GFMs) offer a solution, we find that directly adapting GFMs to EA remains largely ineffective. This stems from a critical "reasoning horizon gap": unlike link prediction in GFMs, EA necessitates capturing long-range dependencies across sparse and heterogeneous KG structuresTo address this challenge, we propose a EA foundation model driven by a parallel encoding strategy. We utilize seed EA pairs as local anchors to guide the information flow, initializing and encoding two parallel streams simultaneously. This facilitates anchor-conditioned message passing and significantly shortens the inference trajectory by leveraging local structural proximity instead of global search. Additionally, we incorporate a merged relation graph to model global dependencies and a learnable interaction module for precise matching. Extensive experiments verify the effectiveness of our framework, highlighting its strong generalizability to unseen KGs.

</details>


### [154] [Rethinking Refinement: Correcting Generative Bias without Noise Injection](https://arxiv.org/abs/2601.21182)
*Xin Peng,Ang Gao*

Main category: cs.LG

TL;DR: 提出 Bi-stage Flow Refinement (BFR) 框架，作为对流式/扩散生成模型的后处理偏差校正方法。通过潜在空间对齐与数据空间 refinement 两阶段策略，在不扰动原始 ODE 路径的情况下提升样本保真度与覆盖率；在 MNIST 上实现 1-NFE 的状态最优 FID 1.46，并在 CIFAR-10、FFHQ-256x256 上展现稳定改进。


<details>
  <summary>Details</summary>
Motivation: 生成模型在高维设置中存在系统性偏差，影响样本质量；现有修正多依赖噪声注入或多步重采样，成本高且可能改变采样轨迹，因此需要一种高效的、以后处理为核心的偏差修正方法。

Method: 基于流匹配的 Bi-stage Flow Refinement (BFR)。阶段1 在潜在空间对齐以实现近似可逆生成器；阶段2 在数据空间进行轻量化数据增强驱动的 refinement。方法强调保持原始 ODE 轨迹的确定性修正，不引入额外噪声或多步采样，采用单次额外函数评估（1-NFE）实现高效修正。

Result: 在 MNIST、CIFAR-10、FFHQ 256x256 设定下取得一致性提升：FID 从 3.95 提升至 1.46（MNIST，1-NFE），并保持样本多样性与覆盖率，显示出方法的稳健性与效率。

Conclusion: BFR 提供一种有效的后处理偏差修正方案，避免修改采样动力学，显著提升生成样本质量与覆盖能力，且实现高效、低成本的性能提升。

Abstract: Generative models, including diffusion and flow-based models, often exhibit systematic biases that degrade sample quality, particularly in high-dimensional settings. We revisit refinement methods and show that effective bias correction can be achieved as a post-hoc procedure, without noise injection or multi-step resampling of the sampling process. We propose a flow-matching-based \textbf{Bi-stage Flow Refinement (BFR)} framework with two refinement strategies operating at different stages: latent space alignment for approximately invertible generators and data space refinement trained with lightweight augmentations. Unlike previous refiners that perturb sampling dynamics, BFR preserves the original ODE trajectory and applies deterministic corrections to generated samples. Experiments on MNIST, CIFAR-10, and FFHQ at 256x256 resolution demonstrate consistent improvements in fidelity and coverage; notably, starting from base samples with FID 3.95, latent space refinement achieves a \textbf{state-of-the-art} FID of \textbf{1.46} on MNIST using only a single additional function evaluation (1-NFE), while maintaining sample diversity.

</details>


### [155] [Rethinking Self-Training Based Cross-Subject Domain Adaptation for SSVEP Classification](https://arxiv.org/abs/2601.21203)
*Weiguang Wang,Yong Liu,Yingjie Gao,Guangyuan Xu*

Main category: cs.LG

TL;DR: 提出一种跨主体自训练的SSVEP域自适应框架，结合FBEA频带信息、PTAL对齐、DEST伪标签精炼及TFA-CL对比学习，在Benchmark与BETA数据集上实现对比度最高的性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨主体SSVEP中信号变异与标注成本高的问题，提升跨主体识别鲁棒性与泛化能力。

Method: 1) FBEA：利用SSVEP过滤器带的频率信息进行欧氏对齐；2) CSST：两阶段自训练框架，先用对抗学习进行源-目标分布对齐（PTAL），再用双重集成自训练优化伪标签（DEST）；3) TFA-CL：时频域增强对比学习，提升多视图下的特征判别性。

Result: 在Benchmark和BETA数据集上实现了对比现有方法的状态-最优性能，且对于不同信号长度具有鲁棒性。

Conclusion: 提出的跨主体自训练框架与时间-频率对比学习有效缓解跨主体SSVEP的个体差异与标签成本问题，具有良好的泛化能力。

Abstract: Steady-state visually evoked potentials (SSVEP)-based brain-computer interfaces (BCIs) are widely used due to their high signal-to-noise ratio and user-friendliness. Accurate decoding of SSVEP signals is crucial for interpreting user intentions in BCI applications. However, signal variability across subjects and the costly user-specific annotation limit recognition performance. Therefore, we propose a novel cross-subject domain adaptation method built upon the self-training paradigm. Specifically, a Filter-Bank Euclidean Alignment (FBEA) strategy is designed to exploit frequency information from SSVEP filter banks. Then, we propose a Cross-Subject Self-Training (CSST) framework consisting of two stages: Pre-Training with Adversarial Learning (PTAL), which aligns the source and target distributions, and Dual-Ensemble Self-Training (DEST), which refines pseudo-label quality. Moreover, we introduce a Time-Frequency Augmented Contrastive Learning (TFA-CL) module to enhance feature discriminability across multiple augmented views. Extensive experiments on the Benchmark and BETA datasets demonstrate that our approach achieves state-of-the-art performance across varying signal lengths, highlighting its superiority.

</details>


### [156] [A Sheaf-Theoretic and Topological Perspective on Complex Network Modeling and Attention Mechanisms in Graph Neural Models](https://arxiv.org/abs/2601.21207)
*Chuan-Shen Hu*

Main category: cs.LG

TL;DR: 提出一个面向图基架构的细胞壳理论框架，用以建模和分析节点特征与边权的局部一致性与谐性，并引入多尺度拓扑分析以捕捉分层特征交互。


<details>
  <summary>Details</summary>
Motivation: GDL/TDL 架构中，特征的分布与在训练过程中的扩散行为尚未被系统地研究；需要一个统一的几何拓扑视角来理解特征聚合与传递的本质及其对任务性能的影响。

Method: 基于细胞壳理论对图及相关结构（图、复形、胞胞结构）中的局部一致性和特征对齐进行建模，追踪节点特征与边权之间的局部关系；提出受拓扑数据分析启发的多尺度扩展，以捕捉分层特征交互；通过对学习信号的拓扑结构和几何结构进行耦合，给出分析框架。

Result: 提供一种联合表征GDL/TDL架构及其学习信号的视角，便于解释特征扩散、聚合行为，并为节点分类、子结构检测、社区检测等任务提供潜在的分析与改进方向。

Conclusion: 该框架为理解特征传播提供新的几何拓扑视角，未来工作可在数值实现、算法优化和实验验证方面展开，以提升对GDL/TDL的理论与实践认识。

Abstract: Combinatorial and topological structures, such as graphs, simplicial complexes, and cell complexes, form the foundation of geometric and topological deep learning (GDL and TDL) architectures. These models aggregate signals over such domains, integrate local features, and generate representations for diverse real-world applications. However, the distribution and diffusion behavior of GDL and TDL features during training remains an open and underexplored problem. Motivated by this gap, we introduce a cellular sheaf theoretic framework for modeling and analyzing the local consistency and harmonicity of node features and edge weights in graph-based architectures. By tracking local feature alignments and agreements through sheaf structures, the framework offers a topological perspective on feature diffusion and aggregation. Furthermore, a multiscale extension inspired by topological data analysis (TDA) is proposed to capture hierarchical feature interactions in graph models. This approach enables a joint characterization of GDL and TDL architectures based on their underlying geometric and topological structures and the learned signals defined on them, providing insights for future studies on conventional tasks such as node classification, substructure detection, and community detection.

</details>


### [157] [Temporal Context and Architecture: A Benchmark for Naturalistic EEG Decoding](https://arxiv.org/abs/2601.21215)
*Mehmet Ergezer*

Main category: cs.LG

TL;DR: 在自然场景脑电解码中，S5在参数效率与峰值准确率上表现突出，但对分布外任务较不稳健；EEGXF在鲁棒性和保守不确定性方面更具优势。


<details>
  <summary>Details</summary>
Motivation: 探索不同序列建模架构在自然观影任务中的性能与鲁棒性，以及上下文长度对解码准确性的影响。

Method: 以HBN电影观看数据集对CNN、LSTM、EEGXF、S4、S5五种架构进行4类任务评估，覆盖8s-128s的上下文长度；并进行零-shot跨频移、跨任务OOD输入、留一受试者外的泛化评估。

Result: 长上下文提升准确性，64s时S5达到98.7%±0.6、CNN 98.3%±0.3；S5参数量约为CNN的1/20。S5在跨受试者任务上更强，但对OOD任务过度自信；EEGXF更稳健于频率移位，且在分布内不确定性较弱标定不足。

Conclusion: 在效率-鲁棒性之间存在权衡：若追求参数高效的峰值准确性可选S5；若更看重鲁棒性与保守不确定性则选EEGXF。

Abstract: We study how model architecture and temporal context interact in naturalistic EEG decoding. Using the HBN movie-watching dataset, we benchmark five architectures, CNN, LSTM, a stabilized Transformer (EEGXF), S4, and S5, on a 4-class task across segment lengths from 8s to 128s. Accuracy improves with longer context: at 64s, S5 reaches 98.7%+/-0.6 and CNN 98.3%+/-0.3, while S5 uses ~20x fewer parameters than CNN. To probe real-world robustness, we evaluate zero-shot cross-frequency shifts, cross-task OOD inputs, and leave-one-subject-out generalization. S5 achieves stronger cross-subject accuracy but makes over-confident errors on OOD tasks; EEGXF is more conservative and stable under frequency shifts, though less calibrated in-distribution. These results reveal a practical efficiency-robustness trade-off: S5 for parameter-efficient peak accuracy; EEGXF when robustness and conservative uncertainty are critical.

</details>


### [158] [PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations](https://arxiv.org/abs/2601.21234)
*Kaiyuan Tan,Kendra Givens,Peilun Li,Thomas Beckers*

Main category: cs.LG

TL;DR: PHDME：在稀疏观测和不完整物理条件下的端口哈密顿扩散框架，通过 GP-dPHS 生成物理一致数据并提供物理残差损失与校准，提升预测准确性和不确定性。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺条件下，扩散模型在轨迹预测中表现有限；PIML 需要明确的控制方程。本工作提出在不完全物理条件下的端口哈密顿和高斯过程的结合，以获得能量表征并辅助扩散训练。

Method: 先在有限观测上训练高斯过程分布的端口哈密顿系统 GP-dPHS，得到能量/结构信息；用其生成物理一致的人工数据集并对扩散模型引入物理残差损失；训练完成后，扩散模型作为可快速取样的预测器；分割同构置信度校准以给出不确定性。

Result: 在偏 PDE 基准和真实弹簧系统上，数据稀缺时显示更高的精度和物理一致性。

Conclusion: PHDME 将不完全物理与稀疏数据条件下的轨迹预测变得更可靠，结合 GP-dPHS、扩散模型与分割校准实现物理一致性与不确定性表达。

Abstract: Diffusion models provide expressive priors for forecasting trajectories of dynamical systems, but are typically unreliable in the sparse data regime. Physics-informed machine learning (PIML) improves reliability in such settings; however, most methods require \emph{explicit governing equations} during training, which are often only partially known due to complex and nonlinear dynamics. We introduce \textbf{PHDME}, a port-Hamiltonian diffusion framework designed for \emph{sparse observations} and \emph{incomplete physics}. PHDME leverages port-Hamiltonian structural prior but does not require full knowledge of the closed-form governing equations. Our approach first trains a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) on limited observations to capture an energy-based representation of the dynamics. The GP-dPHS is then used to generate a physically consistent artificial dataset for diffusion training, and to inform the diffusion model with a structured physics residual loss. After training, the diffusion model acts as an amortized sampler and forecaster for fast trajectory generation. Finally, we apply split conformal calibration to provide uncertainty statements for the generated predictions. Experiments on PDE benchmarks and a real-world spring system show improved accuracy and physical consistency under data scarcity.

</details>


### [159] [Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification](https://arxiv.org/abs/2601.21244)
*Yiju Guo,Tianyi Hu,Zexu Sun,Yankai Lin*

Main category: cs.LG

TL;DR: LENS reduces interference tokens in prompts to improve rollout efficiency in RLVR, achieving faster convergence and better performance than GRPO.


<details>
  <summary>Details</summary>
Motivation: RLVR suffers from inefficient exploration under limited rollout budgets; a few interference tokens in prompts cause most exploration failures, necessitating a noise-reduction approach.

Method: Less Noise Sampling Framework (LENS): (1) identify and remove interference tokens to purify prompts, (2) collect successful rollouts from purified prompts, (3) supervise policy optimization on the original noisy prompts using those successful rollouts.

Result: LENS significantly outperforms GRPO with higher performance and faster convergence, yielding a 3.88% average gain and over 1.6× speedup.

Conclusion: Pruning interference tokens is crucial for rollout efficiency in RLVR and offers a new direction for improving prompting-based RL research.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.

</details>


### [160] [EGAM: Extended Graph Attention Model for Solving Routing Problems](https://arxiv.org/abs/2601.21281)
*Licheng Wang,Yuzi Yan,Mingtao Huang,Yuan Shen*

Main category: cs.LG

TL;DR: 提出并扩展图注意力模型（EGAM），在多头点乘注意力下同时更新节点和边嵌入，使用自回归编码器-解码器并通过带有自定义基线的策略梯度进行训练，在路由问题上与现有方法相当或超越，且在高度约束的情形表现尤为出色。


<details>
  <summary>Details</summary>
Motivation: 弥补传统GAM仅考虑节点特征、对复杂图结构和严格约束下求解能力不足的问题，提升NCO在路由问题上的性能与鲁棒性。

Method: 在GAM基础上扩展为EGAM，使用多头点乘注意力同时更新节点和边嵌入；采用自回归的编解码器架构；利用带定制基线的策略梯度进行端到端强化学习训练。

Result: 在多种路由问题上与现有方法相当或超越，尤其在高度受限的问题上表现突出，显示出对复杂图结构的处理效率。

Conclusion: EGAM有效扩展了GAM的建模能力，提升了对边信息的利用与对复杂约束的处理能力，适合应用于NCO路由任务，具有较强的鲁棒性和泛化潜力。

Abstract: Neural combinatorial optimization (NCO) solvers, implemented with graph neural networks (GNNs), have introduced new approaches for solving routing problems. Trained with reinforcement learning (RL), the state-of-the-art graph attention model (GAM) achieves near-optimal solutions without requiring expert knowledge or labeled data. In this work, we generalize the existing graph attention mechanism and propose the extended graph attention model (EGAM). Our model utilizes multi-head dot-product attention to update both node and edge embeddings, addressing the limitations of the conventional GAM, which considers only node features. We employ an autoregressive encoder-decoder architecture and train it with policy gradient algorithms that incorporate a specially designed baseline. Experiments show that EGAM matches or outperforms existing methods across various routing problems. Notably, the proposed model demonstrates exceptional performance on highly constrained problems, highlighting its efficiency in handling complex graph structures.

</details>


### [161] [DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher](https://arxiv.org/abs/2601.21283)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: DUET: a distillation-based unlearning method that trains a student model to imitate a prompt-steered teacher that refuses undesirable knowledge, combining the efficiency of in-context unlearning with the robustness of traditional unlearning; claims superior forgetting and utility with data efficiency.


<details>
  <summary>Details</summary>
Motivation: Need to remove undesirable knowledge from LLMs to ensure trustworthy AI; existing methods either are heavy (tuning-based) or vulnerable to prompt attacks (in-contextualized unlearning).

Method: Distilled Unlearning from an Efficient Teacher (DUET): train a student to imitate a prompt-steered teacher that refuses undesired knowledge; preserves general knowledge; data-efficient distillation; enriched evaluation protocols.

Result: Empirical evaluations show DUET achieves higher forgetting and utility preservation than baselines; orders of magnitude more data-efficient.

Conclusion: DUET offers effective, data-efficient unlearning by distilling from an efficient, prompt-guided teacher, balancing forgetting of undesirable content with preservation of useful knowledge.

Abstract: LLM unlearning is a technique to remove the impacts of undesirable knowledge from the model without retraining from scratch, which is indispensable towards trustworthy AI. Existing unlearning methods face significant limitations: conventional tuning-based unlearning is computationally heavy and prone to catastrophic forgetting. In contrast, in-contextualized unlearning is lightweight for precise unlearning but vulnerable to prompt removal or reverse engineering attacks. In response, we propose Distilled Unlearning from an Efficient Teacher (DUET), a novel distillation-based unlearning method that combines the merits of these two lines of work. It learns a student model to imitate the behavior of a prompt-steered teacher that effectively refuses undesirable knowledge generation while preserving general domain knowledge. Extensive evaluations on existing benchmarks with our enriched evaluation protocols demonstrate that DUET achieves higher performance in both forgetting and utility preservation, while being orders of magnitude more data-efficient than state-of-the-art unlearning methods.

</details>


### [162] [Zenith: Scaling up Ranking Models for Billion-scale Livestreaming Recommendation](https://arxiv.org/abs/2601.21285)
*Ruifeng Zhang,Zexi Huang,Zikai Wang,Ke Sun,Bohang Zheng,Zhen Ouyang,Huimin Xie,Phil Shen,Junlin Zhang,Wentao Guo,Qinglei Wang*

Main category: cs.LG

TL;DR: Zenith 是一种可扩展且高效的排序架构，用于推荐系统，通过对少量高维 Prime Token 使用 Token Fusion 与 Token Boost 捕获多粒度特征交互，在推理开销较低的前提下实现良好放缩性。该架构在 TikTok Live 的真实场景中表现出在线指标提升与用户参与度的显著改善。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统中，特征交互的复杂性随模型容量增加而提升。但需要高效的特征处理以避免推理时延上升。现有多种模型架构尚未充分解决在不牺牲实时性前提下的容量扩展与高维特征处理问题。

Method: 提出 Zenith 框架，针对少量高维的 Prime Tokens，结合 Token Fusion 与 Token Boost 两个模块，实现对特征交互的高效建模。通过提升 token 异质性来实现更优的放缩规律，与其他最先进的排序方法相比具有更好的可扩展性。并在真实世界平台 TikTok Live 进行部署与 A/B 测试。

Result: 在 TikTok Live 的在线 A/B 测试中，Zenith 的在线 CTR AUC 提升 1.05%，Logloss 降低 1.10%；在 Quality Watch 指标上，单位用户的会话时长提升 9.93%，单位用户的观看时长提升 8.11%。

Conclusion: Zenith 展示了在大规模在线推荐场景中对复杂特征交互的高效建模能力和良好的放缩性，且在实际平台中实现了可观的线上指标提升。但原文摘要未给出基线对比、统计显著性、模型大小及延迟等细节，需要在论文中进一步核对以评估其在不同系统中的普适性。

Abstract: Accurately capturing feature interactions is essential in recommender systems, and recent trends show that scaling up model capacity could be a key driver for next-level predictive performance. While prior work has explored various model architectures to capture multi-granularity feature interactions, relatively little attention has been paid to efficient feature handling and scaling model capacity without incurring excessive inference latency. In this paper, we address this by presenting Zenith, a scalable and efficient ranking architecture that learns complex feature interactions with minimal runtime overhead. Zenith is designed to handle a few high-dimensional Prime Tokens with Token Fusion and Token Boost modules, which exhibits superior scaling laws compared to other state-of-the-art ranking methods, thanks to its improved token heterogeneity. Its real-world effectiveness is demonstrated by deploying the architecture to TikTok Live, a leading online livestreaming platform that attracts billions of users globally. Our A/B test shows that Zenith achieves +1.05%/-1.10% in online CTR AUC and Logloss, and realizes +9.93% gains in Quality Watch Session / User and +8.11% in Quality Watch Duration / User.

</details>


### [163] [Physics-Guided Tiny-Mamba Transformer for Reliability-Aware Early Fault Warning](https://arxiv.org/abs/2601.21293)
*Changyu Li,Dingcheng Huang,Kexuan Yao,Xiaoya Ni,Lijuan Shen,Fei Luo*

Main category: cs.LG

TL;DR: 提出一个紧凑的物理引导三分支编码器PG-TMT，用于在线状态监测中的早期故障预警，在非平稳工况、域转移和强类别不平衡下具有良好校准的决策和解释性。


<details>
  <summary>Details</summary>
Motivation: 在旋转机械可靠性预测中面临非平稳工况、速度/负载/传感器域移位、和强类别不平衡，需早期信号且假警报率可控且可解释。

Method: 提出PG-TMT三分支架构：深度可分卷积干预的干干茎（stem）捕捉微瞬态；Tiny-Mamba状态空间分支建模近线性长程动力学；轻量级局部Transformer编码跨通道共振；推导时域到谱域的解析映射将注意力谱与故障阶带对齐；基于健康分数的极值理论(EVT)校准，设定达到目标假警报强度的阈值，并使用双阈值滞后和最小保持时间抑制抖动；在 leakage-free streaming protocol 上对CWRU/帕德博恩/西安交通大学-徐州等数据集和工业 pilot 进行评估。

Result: 在不泄漏的流式协议下，PG-TMT在不平衡下获得更高的精确-召回AUC、具竞争力甚至更好的ROC AUC、以及相同假警报强度下更短的平均发现时间，且具备较强跨域迁移能力。

Conclusion: 通过将物理对齐的表征与EVT校准的决策规则耦合，PG-TMT提供可校准、可解释、可部署的早期预警，适用于可靠性导向的健康管理。

Abstract: Reliability-centered prognostics for rotating machinery requires early warning signals that remain accurate under nonstationary operating conditions, domain shifts across speed/load/sensors, and severe class imbalance, while keeping the false-alarm rate small and predictable. We propose the Physics-Guided Tiny-Mamba Transformer (PG-TMT), a compact tri-branch encoder tailored for online condition monitoring. A depthwise-separable convolutional stem captures micro-transients, a Tiny-Mamba state-space branch models near-linear long-range dynamics, and a lightweight local Transformer encodes cross-channel resonances. We derive an analytic temporal-to-spectral mapping that ties the model's attention spectrum to classical bearing fault-order bands, yielding a band-alignment score that quantifies physical plausibility and provides physics-grounded explanations. To ensure decision reliability, healthy-score exceedances are modeled with extreme-value theory (EVT), which yields an on-threshold achieving a target false-alarm intensity (events/hour); a dual-threshold hysteresis with a minimum hold time further suppresses chatter. Under a leakage-free streaming protocol with right-censoring of missed detections on CWRU, Paderborn, XJTU-SY, and an industrial pilot, PG-TMT attains higher precision-recall AUC (primary under imbalance), competitive or better ROC AUC, and shorter mean time-to-detect at matched false-alarm intensity, together with strong cross-domain transfer. By coupling physics-aligned representations with EVT-calibrated decision rules, PG-TMT delivers calibrated, interpretable, and deployment-ready early warnings for reliability-centric prognostics and health management.

</details>


### [164] [Missing-Data-Induced Phase Transitions in Spectral PLS for Multimodal Learning](https://arxiv.org/abs/2601.21294)
*Anders Gjølbye,Ida Kargaard,Emma Kargaard,Lars Kai Hansen*

Main category: cs.LG

TL;DR: PLS-SVD under random entrywise masking in high dimensions exhibits a BBP-like phase transition for recoverability of the shared directions; effective signal is attenuated by √ρ, with closed-form overlaps, and a validated phase diagram across aspect ratios, signal strength, and missingness.


<details>
  <summary>Details</summary>
Motivation: Characterize how missing data affects the detectability of shared structure learned by PLS-SVD in high-dimensional, multimodal settings and provide precise recovery conditions.

Method: Model the masked cross-covariance as a spiked rectangular random matrix after normalization; show effective signal attenuation by √ρ due to missingness; derive BBP-type phase transition and closed-form asymptotic overlap formulas; validate via simulations and semi-synthetic experiments.

Result: Existence of a critical signal-to-noise threshold: below it leading singular vectors are asymptotically uninformative; above it they align nontrivially with latent directions with computable overlaps; phase diagram and recovery curves established across aspect ratios, signal strengths, and missingness.

Conclusion: The work provides a complete theoretical and empirical characterization of PLS-SVD in the presence of independent entry-wise missing data, enabling understanding of when shared structure can be recovered and how missingness degrades the detectability.

Abstract: Partial Least Squares (PLS) learns shared structure from paired data via the top singular vectors of the empirical cross-covariance (PLS-SVD), but multimodal datasets often have missing entries in both views. We study PLS-SVD under independent entry-wise missing-completely-at-random masking in a proportional high-dimensional spiked model. After appropriate normalization, the masked cross-covariance behaves like a spiked rectangular random matrix whose effective signal strength is attenuated by $\sqrtρ$, where $ρ$ is the joint entry retention probability. As a result, PLS-SVD exhibits a sharp BBP-type phase transition: below a critical signal-to-noise threshold the leading singular vectors are asymptotically uninformative, while above it they achieve nontrivial alignment with the latent shared directions, with closed-form asymptotic overlap formulas. Simulations and semi-synthetic multimodal experiments corroborate the predicted phase diagram and recovery curves across aspect ratios, signal strengths, and missingness levels.

</details>


### [165] [Grounding and Enhancing Informativeness and Utility in Dataset Distillation](https://arxiv.org/abs/2601.21296)
*Shaobo Wang,Yantai Yang,Guo Chen,Peiru Li,Kaixin Li,Yufa Zhou,Zhaorun Chen,Linfeng Zhang*

Main category: cs.LG

TL;DR: 提出 InfoUtil 框架的高效数据集蒸馏方法，通过信息性与效用性的权衡实现对原始数据的高效蒸馏，在 ImageNet-1K 上以 ResNet-18 达到比以往方法高出 6.1% 的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管 DD 领域通常通过启发式来兼顾效率与质量，但原始数据与合成数据之间的基本关系尚未在理论框架上系统化。本研究从理论出发，提出信息性（Informativeness）与效用性（Utility）等核心概念，给出最优数据集蒸馏的数学定义，填补理论空白。

Method: InfoUtil 包含两大组件：1) 基于博弈论的信息性最大化，利用 Shapley 值对样本的信息贡献进行归因以提取关键信息；2) 根据梯度范数进行的全局性样本选择，以实现效用的最大化。两者结合，得到在信息量与全局影响力之间的平衡的蒸馏数据集。

Result: 在 ImageNet-1K、ResNet-18 设置下，所提方法较前一版本的最先进方法提升约 6.1 百分点的性能。

Conclusion: InfoUtil 通过信息性与效用性的系统性权衡，为知识蒸馏型数据集蒸馏提供了一个理论与实证兼具的框架，能在大规模数据场景中提高蒸馏数据集的代表性与训练效果。

Abstract: Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillation-based dataset distillation within a solid theoretical framework. We introduce the concepts of Informativeness and Utility, capturing crucial information within a sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, a framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm. These components ensure that the distilled dataset is both informative and utility-optimized. Experiments demonstrate that our method achieves a 6.1\% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18.

</details>


### [166] [Achieving $\varepsilon^{-2}$ Dependence for Average-Reward Q-Learning with a New Contraction Principle](https://arxiv.org/abs/2601.21301)
*Zijun Chen,Zaiwei Chen,Nian Si,Shengbo Wang*

Main category: cs.LG

TL;DR: 在平均奖励MDP中，提出一种“懒化”采样的Q-learning变体，构造实例相关的半范数，使Bellman算子在该半范数下的一步收缩，从而实现对同步/异步Q-learning的最优样本复杂度（抑制对数因子为tilde O(ε^-2)）。


<details>
  <summary>Details</summary>
Motivation: 在平均奖励MDP中，缺乏收缩性导致非渐近分析困难。已有结论要么需要强假设以强制半范数收缩，要么依赖折扣制或情境性MDP作为近似，但要么需要未知参数，要么带来较差的样本复杂度。本文旨在在不依赖未知参数的前提下，给出接近最优的样本复杂度分析。

Method: 引入懒化动态（系统以固定概率在当前状态停留），并构造一个实例相关的半范数。在对该懒化MDP进行变换后，证明Bellman算子在该半范数下是一部一步收缩。基于此，对同步与异步Q-learning的更新进行分析，给出最佳阶的样本复杂度界限。

Result: 在达到可观测的收敛性条件（可达性假设）下，证明简单变体的同步/异步Q-learning可在抑制对数因子的前提下实现tilde O(ε^-2)的样本复杂度。核心在于通过懒化变换和实例相关的半范数使Bellman算子具备一阶收缩性。

Conclusion: 通过懒化动态和实例依赖半范数，解决了平均奖励MDP中非收缩Bellman算子的分析难题，并给出与折扣/情境性设定相比更优的样本复杂度，扩大了Q-learning在平均奖励情形下的理论保障。

Abstract: We present the convergence rates of synchronous and asynchronous Q-learning for average-reward Markov decision processes, where the absence of contraction poses a fundamental challenge. Existing non-asymptotic results overcome this challenge by either imposing strong assumptions to enforce seminorm contraction or relying on discounted or episodic Markov decision processes as successive approximations, which either require unknown parameters or result in suboptimal sample complexity. In this work, under a reachability assumption, we establish optimal $\widetilde{O}(\varepsilon^{-2})$ sample complexity guarantees (up to logarithmic factors) for a simple variant of synchronous and asynchronous Q-learning that samples from the lazified dynamics, where the system remains in the current state with some fixed probability. At the core of our analysis is the construction of an instance-dependent seminorm and showing that, after a lazy transformation of the Markov decision process, the Bellman operator becomes one-step contractive under this seminorm.

</details>


### [167] [The Surprising Difficulty of Search in Model-Based Reinforcement Learning](https://arxiv.org/abs/2601.21306)
*Wei-Di Chang,Mikael Henaff,Brandon Amos,Gregory Dudek,Scott Fujimoto*

Main category: cs.LG

TL;DR: 在模型基强化学习中，搜索并非简单可替代的策略改进；减小分布偏移比提升模型/价值函数更关键，且在对齐分布偏移后，搜索可实现多项基准的state-of-the-art性能。


<details>
  <summary>Details</summary>
Motivation: 挑战常见观点：长期预测误差与连锁误差是模型基RL的主要障碍；本研究聚焦分布偏移对搜索效果的影响，质疑“搜索即插即用”式改进的普遍性。

Method: 通过系统的实验分析与对比，验证即使模型高精度，搜索也可能损害性能；提出以减小分布偏移为核心的技术组合，并据此总结实现有效搜索的关键做法。

Result: 证实搜索在高保真模型下仍可能劣于模型外的直接策略，分布偏移是关键瓶颈；在多种主流基准上，结合所提技术可获得state-of-the-art性能。

Conclusion: 应将关注点从单纯提升模型/价值函数精度转向减小分布偏移，并据此设计可行的搜索方法与技术，重新评估在模型基RL中使用搜索的策略。

Abstract: This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.

</details>


### [168] [Transferable Graph Condensation from the Causal Perspective](https://arxiv.org/abs/2601.21309)
*Huaming Du,Yijie Huang,Su Yao,Yiying Wang,Yueyang Zhou,Jingwen Yang,Jinshi Zhang,Han Ji,Yu Zhao,Guisong Liu,Hegui Zhang,Carl Yang,Gang Kou*

Main category: cs.LG

TL;DR: 提出一个可迁移的图数据集压缩框架 TGCC，通过因果不变性提取以及谱域对比学习实现跨任务、跨域的可转移数据集压缩，并在多数据集上达到优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有图数据集压缩方法在跨任务/跨域场景中的可迁移性不足，且通常需要下游任务与原数据集严格匹配，导致实际应用受限。

Method: 分三步：1) 在图的空间域通过因果干预提取领域不变特征以保留领域不变知识；2) 进行增强的压缩操作，充分捕捉图的结构和特征信息；3) 通过谱域增强对比学习将因果不变特征注入到压缩图中，保留原图的因果信息。

Result: 在五个公开数据集和新提出的 FinReport 数据集上，TGCC 在跨任务和跨域的复杂场景中相比现有方法提升幅度最高可达 13.41%，在单数据集和单任务场景下达到 5/6 数据集的状态‑艺术水平。

Conclusion: TGCC 提供一种有效且可迁移的图数据集压缩方法，能够在跨任务/跨域和单任务场景中维持优良性能。

Abstract: The increasing scale of graph datasets has significantly improved the performance of graph representation learning methods, but it has also introduced substantial training challenges. Graph dataset condensation techniques have emerged to compress large datasets into smaller yet information-rich datasets, while maintaining similar test performance. However, these methods strictly require downstream applications to match the original dataset and task, which often fails in cross-task and cross-domain scenarios. To address these challenges, we propose a novel causal-invariance-based and transferable graph dataset condensation method, named \textbf{TGCC}, providing effective and transferable condensed datasets. Specifically, to preserve domain-invariant knowledge, we first extract domain causal-invariant features from the spatial domain of the graph using causal interventions. Then, to fully capture the structural and feature information of the original graph, we perform enhanced condensation operations. Finally, through spectral-domain enhanced contrastive learning, we inject the causal-invariant features into the condensed graph, ensuring that the compressed graph retains the causal information of the original graph. Experimental results on five public datasets and our novel \textbf{FinReport} dataset demonstrate that TGCC achieves up to a 13.41\% improvement in cross-task and cross-domain complex scenarios compared to existing methods, and achieves state-of-the-art performance on 5 out of 6 datasets in the single dataset and task scenario.

</details>


### [169] [Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation](https://arxiv.org/abs/2601.21315)
*Seonghwi Kim,Sung Ho Jo,Wooseok Ha,Minwoo Chae*

Main category: cs.LG

TL;DR: 提出一种分布鲁棒的无监督域适应框架，在协变量分布和条件标签分布上建模不确定性；给出高效算法，能与现有UDA方法无缝集成，在目标数据极少时仍表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统的UDA在目标域 unlabeled 数据稀缺或源域存在虚假相关时表现不佳。需要对分布转变中的双重不确定性进行鲁棒建模。本工作以多源域适应为灵感，亦适用于单源场景，提升鲁棒性和泛化能力。

Method: 构建分布鲁棒学习框架，对协变量分布p(x)和条件分布p(y|x)的不确定性建立模糊集/对偶策略，推导稳健目标函数，给出高效训练算法；方法可与现有UDA方法无缝集成，适用于单源与多源场景。

Result: 在多种分布偏移情景下的实验显示，当目标数据极为稀缺时，所提方法持续优于强基线，表现出更强的鲁棒性和泛化能力。

Conclusion: 该框架为UDA提供双重不确定性鲁棒性，兼容单源与多源设置，具有较高的实用性，并可与现有方法协同提升性能。

Abstract: Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.

</details>


### [170] [Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2601.21316)
*Aoyu Pang,Maonan Wang,Zifan Sha,Wenwei Yue,Changle Li,Chung Shue Chen,Man-On Pun*

Main category: cs.LG

TL;DR: 提出面向空气-地面一体化出行的统一优化模型与UAGMC框架，通过深度强化学习与V2X实现Vertiport选择与空轨路线的动态规划，显著提升多模态出行效率；相关代码开源。


<details>
  <summary>Details</summary>
Motivation: 城市场景中空地协同出行缺乏系统性优化与现实条件耦合，亟待统一的路线选择与调度框架以实现门到门的高效出行。

Method: 建立一个统一的优化模型，整合空气与地面策略选择，考虑实时交通与乘客决策行为的动态特征；在此基础上提出UAGMC框架，结合深度强化学习与V2X通信，优化 Vertiport 选址与空客/空中出租车航线的动态规划。

Result: 通过实验，UAGMC相较于传统比例分配方法将平均旅行时间降低约34%，显著提升出行效率并为多模态交通系统的整合与优化提供新视角。

Conclusion: 为通过协同空气与地面出行来推进智能城市出行解决方案奠定基础，并提供可复现的代码实现（GitHub）。

Abstract: Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic exploration.To address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at https://github.com/Traffic-Alpha/UAGMC.

</details>


### [171] [Memorization Control in Diffusion Models from Denoising-centric Perspective](https://arxiv.org/abs/2601.21348)
*Thuy Phuong Vu,Mai Viet Hoang Do,Minhhuy Le,Dinh-Cuong Hoang,Phan Xuan Tan*

Main category: cs.LG

TL;DR: 提出从去噪角度研究扩散模型的记忆效应，提出按时间步抽样以控制去噪轨迹中的学习位置，通过调整信心区间宽度来直接控制记忆-泛化权衡，在图像与一维信号生成任务中验证晚期去噪步的学习偏重可降低记忆并提高与训练数据分布的一致性


<details>
  <summary>Details</summary>
Motivation: 需要降低扩散模型对训练数据的过度 memorization，现有数据中心或模型中心的改动难以从学习过程本身解释记忆现象；从去噪过程出发，探究不同去噪步骤对学习贡献的影响以实现对记忆的控制

Method: 揭示统一的时间步采样导致各去噪步的信噪比差异，产生不均衡学习贡献并偏向记忆；提出一个按时间步抽样的新策略，明确控制去噪轨迹上学习发生的位置；通过调整置信区间宽度来直接调控记忆与泛化之间的权衡

Result: 在图像和一维信号生成任务上，增加对后期去噪步的学习强调可降低记忆，提升与训练数据的分布一致性，验证了所提方法的普遍性与有效性

Conclusion: 从去噪视角出发的时间步采样策略为记忆-泛化权衡提供直接的控制手段，适用于多模态数据生成任务并具有较强泛化性

Abstract: Controlling memorization in diffusion models is critical for applications that require generated data to closely match the training distribution. Existing approaches mainly focus on data centric or model centric modifications, treating the diffusion model as an isolated predictor. In this paper, we study memorization in diffusion models from a denoising centric perspective. We show that uniform timestep sampling leads to unequal learning contributions across denoising steps due to differences in signal to noise ratio, which biases training toward memorization. To address this, we propose a timestep sampling strategy that explicitly controls where learning occurs along the denoising trajectory. By adjusting the width of the confidence interval, our method provides direct control over the memorization generalization trade off. Experiments on image and 1D signal generation tasks demonstrate that shifting learning emphasis toward later denoising steps consistently reduces memorization and improves distributional alignment with training data, validating the generality and effectiveness of our approach.

</details>


### [172] [L2R: Low-Rank and Lipschitz-Controlled Routing for Mixture-of-Experts](https://arxiv.org/abs/2601.21349)
*Minghao Yang,Ren Togo,Guang Li,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: 提出 L2R 框架，通过低秩路由空间和 Saturated Inner-Product Scoring (SIPS) 提升 MoE 路由的稳定性与专家专门化，并在语言与视觉任务上实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有 MoE 路由在高维表示空间使用线性路由，易受表示不匹配、角度集中和尺度敏感评分的影响，削弱路由辨别性和专家稳定性。

Method: 引入共享的低秩路由空间进行专家分配；提出 Saturated Inner-Product Scoring (SIPS) 以显式控制路由函数的 Lipschitz 行为；并采用参数高效的多锚点路由以提升专家表达力。

Result: 在大规模语言 MoE 与 ImageNet 的视觉 MoE 设置中，L2R 显著提升了路由稳定性、专家专门化和整体模型性能。

Conclusion: 通过统一的路由框架，改造路由空间与评分几何，L2R 提升了 MoE 的鲁棒性与表达能力。

Abstract: Mixture-of-Experts (MoE) models scale neural networks by conditionally activating a small subset of experts, where the router plays a central role in determining expert specialization and overall model performance. However, many modern MoE systems still adopt linear routers in raw high-dimensional representation spaces, where representation mismatch, angular concentration, and scale-sensitive scoring can jointly undermine routing discriminability and stable expert specialization. In this work, we propose Low-rank \& Lipschitz-controlled Routing (L2R), a unified routing framework that reshapes both the routing space and scoring geometry. L2R performs expert assignment in a shared low-rank latent routing space and introduces Saturated Inner-Product Scoring (SIPS) to explicitly control the Lipschitz behavior of routing functions, yielding smoother and more stable routing geometry. In addition, L2R incorporates a parameter-efficient multi-anchor routing mechanism to enhance expert expressiveness. Extensive experiments on a large-scale language MoE model and a vision MoE setting on ImageNet demonstrate that L2R consistently improves routing stability, expert specialization, and overall model performance.

</details>


### [173] [Factored Causal Representation Learning for Robust Reward Modeling in RLHF](https://arxiv.org/abs/2601.21350)
*Yupei Yang,Lin Yang,Wanxi Deng,Lin Qu,Fan Feng,Biwei Huang,Shikui Tu,Lei Xu*

Main category: cs.LG

TL;DR: 通过对上下文嵌入进行因子化表示，将影响奖励预测的因果因素与非因果、与奖励无关的属性分离，并限制奖励头仅依赖因果组件，同时加入对非因果因素的对抗头及梯度反转以抑制其携带奖励相关信息，从而提升 RLHF 的鲁棒性与减少奖励操纵。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型易被伪相关信号（如长度、拍马等偏见等非因果特征）误导，导致奖励预测与人类偏好不一致（奖励操纵）。需要以因果角度实现对奖励信号的鲁棒性提升。

Method: 将上下文嵌入分解为两部分：因果因素（对奖励预测充分且必要）和非因果因素（奖励无关属性，如文本长度、拍马等偏见）。奖励头仅依赖因果组件；同时引入一个对抗头，尝试从非因果因素预测奖励，并对其梯度反向传播，以抑制非因果因素携带奖励相关信息。并在数学题和对话任务上验证，在多种基线下提升RLHF的 downstream性能。

Result: 所提出的方法在鲁棒奖励模型方面优于基线，在下游RLHF任务中实现了更稳定且显著的性能提升，且对长度与拍马等偏见的分析表明有效降低了奖励操纵行为。

Conclusion: 因果因素化表示结合对抗性分离可有效减少奖赏信号中的非因果特征干扰，提升奖励模型鲁棒性与 RLHF 效果，具有较好的普适性与扩展性。

Abstract: A reliable reward model is essential for aligning large language models with human preferences through reinforcement learning from human feedback. However, standard reward models are susceptible to spurious features that are not causally related to human labels. This can lead to reward hacking, where high predicted reward does not translate into better behavior. In this work, we address this problem from a causal perspective by proposing a factored representation learning framework that decomposes the model's contextual embedding into (1) causal factors that are sufficient for reward prediction and (2) non-causal factors that capture reward-irrelevant attributes such as length or sycophantic bias. The reward head is then constrained to depend only on the causal component. In addition, we introduce an adversarial head trained to predict reward from the non-causal factors, while applying gradient reversal to discourage them from encoding reward-relevant information. Experiments on both mathematical and dialogue tasks demonstrate that our method learns more robust reward models and consistently improves downstream RLHF performance over state-of-the-art baselines. Analyses on length and sycophantic bias further validate the effectiveness of our method in mitigating reward hacking behaviors.

</details>


### [174] [Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving](https://arxiv.org/abs/2601.21351)
*Chendong Song,Meixuan Wang,Hang Zhou,Hong Liang,Yuan Lyu,Zixi Chen,Yuwei Fan,Zijie Zhou*

Main category: cs.LG

TL;DR: 提出对 Attention-FFN 解耦架构 AFD 的容量分配的解析框架，给出在 r A-1 F 拓扑下的最优 A/F 比例的闭式规则，并通过真实化的仿真验证理论误差在 10% 以内，同时显著降低空闲时间。


<details>
  <summary>Details</summary>
Motivation: 解决 AFD 在内存与计算资源纵横分配上的敏感性与低效阻塞问题，提供可行的资源分配策略以提高吞吐量。

Method: 建立非平稳 Attention 工作负载和稳定的 FFN 工作负载的概率模型，在 rA-1F 架构下推导 A/F 比的闭式表达，开发跟踪校准的 AFD 仿真器以验证理论。

Result: 给出最优 A/F 比的闭式规则，仿真验证表明理论最优比与仿真最优只相差 ~10%，并在多种工作负载下显著减少空闲时间。

Conclusion: 该工作提供了 AFD 资源分配的理论基础和实际可用的指导，提升 Attention-FFN 解耦架构的吞吐与资源利用率，但需要在更广泛的模型规模和真实硬件上进一步验证。

Abstract: Attention-FFN disaggregation (AFD) is an emerging architecture for LLM decoding that separates state-heavy, KV-cache-dominated Attention computation from stateless, compute-intensive FFN computation, connected by per-step communication. While AFD enables independent scaling of memory and compute resources, its performance is highly sensitive to the Attention/FFN provisioning ratio: mis-sizing induces step-level blocking and costly device idle time. We develop a tractable analytical framework for sizing AFD bundles in an $r$A-$1$F topology, where the key difficulty is that Attention-side work is nonstationary-token context grows and requests are continuously replenished with random lengths-while FFN work is stable given the aggregated batch. Using a probabilistic workload model, we derive closed-form rules for the optimal A/F ratio that maximize average throughput per instance across the system. A trace-calibrated AFD simulator validates the theory: across workloads, the theoretical optimal A/F ratio matches the simulation-optimal within 10%, and consistently reduces idle time.

</details>


### [175] [Perceptrons and localization of attention's mean-field landscape](https://arxiv.org/abs/2601.21366)
*Antonio Álvarez-López,Borjan Geshkovski,Domènec Ruiz-Balet*

Main category: cs.LG

TL;DR: 把 Transformer 的前向传播看作单位球上的粒子系统，其随层数演化；在某些权重设置下等价于对显式能量的梯度流，并通过 Wasserstein 梯度流得到无限上下文长度的均场极限。研究感知机块对该设置的影响，结果是临界点在一般情形下是原子性、局部化在球面子集上。


<details>
  <summary>Details</summary>
Motivation: 揭示 Transformer 动力学与最优化几何之间的联系，利用粒子系统与梯度流的视角理解前向传播的近似动力学，以及感知机块在能量景观中的作用，从而解释表示学习的局部化现象和无限上下文极限下的行为。

Method: 将 Transformer 的前向传播建模为单位球上的相互作用粒子系统，时间对应层数，粒子对应 token 表示，单位球象征层归一化。在特定权重下，该系统为显式能量的梯度流，且可通过 Wasserstein 梯度流分析均场极限（无限上下文长度）。分析感知机块对能量函数及其临界点的影响，证明在一般条件下临界点为原子性且局部化于球面子集。

Result: 给出一个对能量泛函及其梯度流的解析框架，证明在常规权重配置下，能量函数的临界点具有原子性（即聚集在有限数量的点上）并在球面子集上局部化，从而揭示感知机块促成的稀疏、局部化的表示结构，以及无限上下文极限下的均场行为。

Conclusion: 该工作将 Transformer 的前向传播与粒子系统/梯度流理论结合，特别是利用 Wasserstein 均场极限，揭示感知机块在能量景观中的作用导致临界点的原子性与局部化，为理解模型表示的几何结构和无限上下文长期行为提供了新的分析工具与直觉。

Abstract: The forward pass of a Transformer can be seen as an interacting particle system on the unit sphere: time plays the role of layers, particles that of token embeddings, and the unit sphere idealizes layer normalization. In some weight settings the system can even be seen as a gradient flow for an explicit energy, and one can make sense of the infinite context length (mean-field) limit thanks to Wasserstein gradient flows. In this paper we study the effect of the perceptron block in this setting, and show that critical points are generically atomic and localized on subsets of the sphere.

</details>


### [176] [Rethinking Federated Graph Foundation Models: A Graph-Language Alignment-based Approach](https://arxiv.org/abs/2601.21369)
*Yinlin Zhu,Di Wu,Xianzhi Zhang,Yuming Ai,Xunkai Li,Miao Hu,Guocong Quan*

Main category: cs.LG

TL;DR: 提出 FedGALA 框架，通过无监督对比学习在连续嵌入空间中对齐 GNN 与冻结的 PLMs，并采用高效的提示微调以适配多任务下游任务，解决联邦图基础模型中的语义-结构正交性与数据异质性与传输成本问题，从而在多域数据集上实现显著性能提升（最高约14.37%）。


<details>
  <summary>Details</summary>
Motivation: 在分布式、隐私受限的数据孤岛环境中，现有的量化向量化方法导致知识不可逆损失，亟需解决 PLMs 与 GNN 的语义与结构的正交性与信息完整性，同时降低通信与计算开销以适应联邦场景。

Method: 使用无监督对比学习来对齐 GNN 编码器与冻结的 PLMs 在一个连续嵌入空间中，随后通过高效的提示微调来引导预对齐的编码器和 PLMs，以在不进行全参数微调的情况下实现对多任务的迁移与适配。

Result: 在多域数据集与多任务上，FedGALA 相较基线表现提升，最高达约14.37%，实验结果证明了对齐与提示微调策略的有效性及在联邦场景中的适用性。

Conclusion: FedGALA 有效解决了图-语言的语义-结构正交性与完整性问题，并通过高效的提示微调在联邦学习环境中实现对下游任务的鲁棒适配与性能提升，展示了在分布式隐私受限场景中的实用性。

Abstract: Recent studies of federated graph foundational models (FedGFMs) break the idealized and untenable assumption of having centralized data storage to train graph foundation models, and accommodate the reality of distributed, privacy-restricted data silos. Despite their simplicity and intuition, existing studies that project aligned generalizable knowledge onto a discrete token space via vector-quantized backbones suffer from irreversible knowledge loss during the quantization process. In this context, we argue that reconciling the semantic-structural orthogonality and integrity between pre-trained language models (PLMs) and graph neural networks (GNNs) is paramount for developing effective FedGFMs while simultaneously mitigating the severe data heterogeneity and communication constraints inherent in distributed, resource-limited environments.
  To address these issues, we propose FedGALA (Federated Graph And Language Alignment), a framework that resolves graph-based semantic-structural orthogonality and integrity in federated settings by employing unsupervised contrastive learning to align GNNs and frozen PLMs within a continuous embedding space, thereby capturing robust, transferable general knowledge. Subsequently, FedGALA leverages a communication-efficient prompt tuning mechanism to steer these pre-aligned encoders and frozen PLMs, facilitating effective adaptation to diverse downstream tasks while circumventing the prohibitive overhead of full-parameter fine-tuning. The comprehensive experiments validate that FedGALA outperforms all competitive baselines across multi-domain datasets on multiple tasks with up to 14.37% performance improvement.

</details>


### [177] [DA-SPS: A Dual-stage Network based on Singular Spectrum Analysis, Patching-strategy and Spearman-correlation for Multivariate Time-series Prediction](https://arxiv.org/abs/2601.21381)
*Tianhao Zhang,Shusen Ma,Yu Kang,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出 DA-SPS 的两阶段多变量时序预测框架，对目标变量与干扰变量分别进行信息提取与筛选，通过融合实现对多数据集的优越预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时序预测方法未充分考虑外部干扰变量对目标变量的影响，也未能充分挖掘序列的复杂时间模式（趋势、季节性等）信息。

Method: 将数据分为目标变量处理阶段（TVPS）与外部变量处理阶段（EVPS）。TVPS 使用 SSA 提取目标序列的趋势与季节性信息，再通过 LSTM 与采用修补策略的 P-Conv-LSTM 提取深层时序特征。EVPS 通过 Spearman 相关分析筛选与目标高度相关的外部变量，并用 L-Attention（由 LSTM 与注意力机制组成）对其进行特征分析。最终将 TVPS 与 EVPS 的输出通过加权求和与线性映射融合，得到最终预测。

Result: 在四个公开数据集上优于现有最先进方法，并在自建的真实场景数据集（笔记本主板测试项信息）上进一步验证了模型的实用性和鲁棒性。

Conclusion: DA-SPS 为多变量时间序列预测提供了一种分阶段、变量特征化提取与干扰变量筛选并行融合的新框架，能更充分地利用目标变量与相关外部信息，从而提升预测性能，在多数据集和真实场景中展现出优越性。

Abstract: Multivariate time-series forecasting, as a typical problem in the field of time series prediction, has a wide range of applications in weather forecasting, traffic flow prediction, and other scenarios. However, existing works do not effectively consider the impact of extraneous variables on the prediction of the target variable. On the other hand, they fail to fully extract complex sequence information based on various time patterns of the sequences. To address these drawbacks, we propose a DA-SPS model, which adopts different modules for feature extraction based on the information characteristics of different variables. DA-SPS mainly consists of two stages: the target variable processing stage (TVPS) and the extraneous variables processing stage (EVPS). In TVPS, the model first uses Singular Spectrum Analysis (SSA) to process the target variable sequence and then uses Long Short-Term Memory (LSTM) and P-Conv-LSTM which deploys a patching strategy to extract features from trend and seasonality components, respectively. In EVPS, the model filters extraneous variables that have a strong correlation with the target variate by using Spearman correlation analysis and further analyses them using the L-Attention module which consists of LSTM and attention mechanism. Finally, the results obtained by TVPS and EVPS are combined through weighted summation and linear mapping to produce the final prediction. The results on four public datasets demonstrate that the DA-SPS model outperforms existing state-of-the-art methods. Additionally, its performance in real-world scenarios is further validated using a private dataset collected by ourselves, which contains the test items' information on laptop motherboards.

</details>


### [178] [Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.21418)
*Qian Wan,Ziao Xu,Luona Wei,Xiaoxuan Shen,Jianwen Sun*

Main category: cs.LG

TL;DR: DiPO 是一个基于强化学习的训练框架，通过任务难度自我建模与难度信号奖励，自动调节推理开销，显著减少冗余推理令，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在复杂任务中易产生过度推理、资源低效，原因包括训练后奖励偏好与缺乏对任务难度的感知。需要引入难度感知以更高效地分配推理资源。

Method: 提出 DiPO 框架，采用模型自我推理的难度建模以降低对人工标注的依赖；设计难度信号增强的奖励函数，兼顾输出格式与推理表现，惩罚冗长推理，将难度信息并入强化学习优化。

Result: 实验表明 DiPO 能使模型自适应调整推理开销，显著减少冗余 token，同时不损失性能。

Conclusion: 引入任务难度感知与难度信号在强化学习中的应用，可提升 LRMs 的推理资源利用率与效率，具备广泛应用潜力。

Abstract: Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.

</details>


### [179] [Revisiting Diffusion Model Predictions Through Dimensionality](https://arxiv.org/abs/2601.21419)
*Qing Jin,Chaoyang Wang*

Main category: cs.LG

TL;DR: 提出一个通用预测框架，揭示数据几何与最佳预测目标的关系，证明在高维环境下x预测优于ε/v预测，并通过k-Diff学习最优预测参数k以提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 解释为什么不同数据属性下的最佳预测目标会不同，尤其在高维设置中，缺乏正式理论支撑。

Method: 构建可泛化的预测目标框架，覆盖ε、v、x等输出；推导数据几何与目标的解析关系；提出k-Diff，通过学习数据驱动的k以决定最优输出目标，避免显式估计内在维度；在潜在空间与像素空间的图像生成上进行实验。

Result: 给出数据几何与最佳预测目标的解析关系；论证在ambient维度显著大于intrinsic维度时x预测优；在多种架构与数据规模下，k-Diff在实验中显著优于固定目标基线。

Conclusion: 提供一个理论支撑且自适应的框架，提升扩散/流匹配生成性能，降低对内在维度估计的依赖。

Abstract: Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise ($\varepsilon$) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which $\varepsilon$-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.

</details>


### [180] [ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation](https://arxiv.org/abs/2601.21420)
*Zihao Huang,Jundong Zhou,Xingwei Qu,Qiyang Min,Ge Zhang*

Main category: cs.LG

TL;DR: ConceptMoE通过将语义上相似的token合并为概念表示，动态分配计算，优化长序列和多模态任务的效率与效果，相较于标准MoE在多项指标上提升，并且在持续训练中收益显著，且可控释放的计算与参数用于对比基线，使 architectural benefits 明确。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型对所有token均等分配计算的问题，认识到某些序列可预测性强，某些需要深度推理，需引入自适应的概念级处理以提升效率与性能。

Method: 提出ConceptMoE，使用可学习的chunk模块通过评估 token 间相似性来识别边界，将序列按目标压缩比R进入计算密集的概念模型。MoE结构允许重分配节省的计算以对比基线的FLOPs和参数，孤立出架构层面的收益；在语言和视觉-语言任务上评估，给出在不同R下的效率与性能数据。

Result: 在多项任务上实现性能提升：语言预训练+0.9点、长上下文理解+2.3点、多模态基准+0.6点；持续训练场景下提升达+5.5点；同时在注意力计算和KV缓存方面分别降低至原来的R^2与R倍；R=2时预先填充加速达175%，解码速度提升至117%。

Conclusion: 概念级处理的自适应计算框架可在不大幅修改现有MoE的前提下实现优越的效果与效率，且提供可控的计算分配，提升大语言模型的实用性与可扩展性。

Abstract: Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio $R$ before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to $R^2\times$ and KV cache by $R\times$. At $R=2$, empirical measurements show prefill speedups reaching 175\% and decoding speedups up to 117\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.

</details>


### [181] [Lossy Common Information in a Learnable Gray-Wyner Network](https://arxiv.org/abs/2601.21424)
*Anderson de Andrade,Alon Harell,Ivan V. Bajić*

Main category: cs.LG

TL;DR: 提出一个可学习的Gray-Wyner风格三通道编解码器，用以在多视觉任务间分离共享信息与任务特定信息；引入基于有损共享信息的优化目标；在两任务六个基准上与独立编码比较，显著降低冗余并保持优越性。


<details>
  <summary>Details</summary>
Motivation: 多视觉任务存在大量信息重叠，传统编解码器忽略共享信息，导致冗余。Gray-Wyner理论提供分离共有信息与任务特定信息的原理框架。

Method: 构建可学习的三通道编解码器，分 disentangle 共享信息和任务特定细节；提出有损共有信息的概念，设计兼顾学习中的权衡的优化目标；在两任务场景下对比三种编解码架构，覆盖六个视觉基准。

Result: 与独立编码相比，显著降低冗余，且在六项基准上表现一致优于独立编码；验证了将Gray-Wyner理论应用于现代任务驱动表示学习的实用性。

Conclusion: 重新审视Gray-Wyner理论在机器学习中的应用具有实际价值；三通道解耦表示有利于多任务学习的效率和性能，未来可扩展到更多任务并进一步研究有损共享信息的学习机制。

Abstract: Many computer vision tasks share substantial overlapping information, yet conventional codecs tend to ignore this, leading to redundant and inefficient representations. The Gray-Wyner network, a classical concept from information theory, offers a principled framework for separating common and task-specific information. Inspired by this idea, we develop a learnable three-channel codec that disentangles shared information from task-specific details across multiple vision tasks. We characterize the limits of this approach through the notion of lossy common information, and propose an optimization objective that balances inherent tradeoffs in learning such representations. Through comparisons of three codec architectures on two-task scenarios spanning six vision benchmarks, we demonstrate that our approach substantially reduces redundancy and consistently outperforms independent coding. These results highlight the practical value of revisiting Gray-Wyner theory in modern machine learning contexts, bridging classic information theory with task-driven representation learning.

</details>


### [182] [From Consistency to Complementarity: Aligned and Disentangled Multi-modal Learning for Time Series Understanding and Reasoning](https://arxiv.org/abs/2601.21436)
*Hang Ni,Weijia Zhang,Fei Wang,Zezhi Shao,Hao Liu*

Main category: cs.LG

TL;DR: 提出 MADI，一种增强型多模态大语言模型，通过补丁级对齐、离散分离交互和关键字高亮，改善时间序列跨模态理解与推理，实验显示优于通用 LLM 与专门化 MLLMs。


<details>
  <summary>Details</summary>
Motivation: 跨模态整合在时间序列理解中存在细粒度时间错对齐和语义混合问题，影响局部解释和互补推理；需要更细粒度的对齐与解耦的跨模态交互。

Method: 1) Patch-level Alignment：实现跨模态的物理上细粒度对应；2) Discrete Disentangled Interaction：将模态共同语义分解为紧凑的离散潜在变量，耦合模态独有信息；3) Critical-token Highlighting：突出信息丰富、与查询相关的信号以增强推理鲁棒性。

Result: 在合成与真实数据集上，MADI 持续优于通用 LLM 和时间序列专用 MLLMs。

Conclusion: 通过细粒度对齐与解耦交互，提升跨模态时间序列理解与推理能力，显示出对复杂时序动态的更好分析能力。

Abstract: Advances in multi-modal large language models (MLLMs) have inspired time series understanding and reasoning tasks, that enable natural language querying over time series, producing textual analyses of complex temporal dynamics. Recent attempts hybridize numerical time series with their visualized plots, facilitating precise value reasoning and visual structure comprehension for comprehensive time series understanding of MLLMs. However, effective cross-modal integration remains challenging due to fine-grained temporal misalignment across modalities and severe entanglement between shared and modality-specific semantics, which hinder localized interpretation and complementary reasoning. To address these issues, we propose MADI, a multi-modal LLM enhanced with fine-grained alignment and disentangled interaction, featuring (1) Patch-level Alignment, which enforces physically grounded fine-grained correspondence across heterogeneous modalities, (2) Discrete Disentangled Interaction, which separates modality-common semantics into compact discrete latents and adaptively synergizes the purified modality-unique information, and (3) Critical-token Highlighting, which emphasizes informative, query-relevant signals for robust reasoning. Experiments on synthetic and real-world benchmarks show that MADI consistently outperforms general-purpose LLMs and time-series-specialized MLLMs.

</details>


### [183] [Synthetic Pattern Generation and Detection of Financial Activities using Graph Autoencoders](https://arxiv.org/abs/2601.21446)
*Francesco Zola,Lucia Muñoz,Andrea Venturi,Amaia Gil*

Main category: cs.LG

TL;DR: 在合成数据上通过三种 GAE 学习并区分模仿洗钱的拓扑模式，发现 GAE-GCN 稳健性最好，提示在无标签、隐私受限场景下合成数据对检测洗钱拓扑信号具可行性。


<details>
  <summary>Details</summary>
Motivation: 现实财务数据稀缺且隐私约束严格，标注困难。需要无监督、可迁移的图表示学习方法来捕捉洗钱相关拓扑结构特征。

Method: 数据生成阶段：为七种知名非法活动模式使用参数化生成器构造合成样本，保持结构一致性并引入现实变异。模型阶段：对三种 GAEs（GAE-GCN、GAE-SAGE、GAE-GAT）进行无监督训练，仅以重建误差作为学习结构的指标。

Result: GAE-GCN 在多模式评估中呈现最稳定的一致性重建表现，适用于大多数模式。GAE-SAGE 与 GAE-GAT 在个别模式上表现较好，但总体不及 GAE-GCN。

Conclusion: 在合成数据上进行图表示学习为开发基于 AI 的洗钱检测工具提供了可行路径，缓解真实数据稀缺和隐私约束，但需进一步验证对真实世界数据的泛化能力，并关注合成数据偏差带来的影响。

Abstract: Illicit financial activities such as money laundering often manifest through recurrent topological patterns in transaction networks. Detecting these patterns automatically remains challenging due to the scarcity of labeled real-world data and strict privacy constraints. To address this, we investigate whether Graph Autoencoders (GAEs) can effectively learn and distinguish topological patterns that mimic money laundering operations when trained on synthetic data. The analysis consists of two phases: (i) data generation, where synthetic samples are created for seven well-known illicit activity patterns using parametrized generators that preserve structural consistency while introducing realistic variability; and (ii) model training and validation, where separate GAEs are trained on each pattern without explicit labels, relying solely on reconstruction error as an indicator of learned structure. We compare three GAE implementations based on three distinct convolutional layers: Graph Convolutional (GAE-GCN), GraphSAGE (GAE-SAGE), and Graph Attention Network (GAE-GAT). Experimental results show that GAE-GCN achieves the most consistent reconstruction performance across patterns, while GAE-SAGE and GAE-GAT exhibit competitive results only in few specific patterns. These findings suggest that graph-based representation learning on synthetic data provides a viable path toward developing AI-driven tools for detecting illicit behaviors, overcoming the limitations of financial datasets.

</details>


### [184] [SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation](https://arxiv.org/abs/2601.21452)
*Yu Xie,Xing Kai Ren,Ying Qi,Hu Yao*

Main category: cs.LG

TL;DR: 提出 SAGE，解决 OneRec 的分词表依赖与 GBPO 的对称保守性问题，提供面向序列的自适应梯度进化框架，用于列表式生成推荐，能够解锁冷启动流量、维持多样性并保持数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 在推荐场景中扩展大语言模型（LLMs）时，避免构建独立的词汇表以降低维护成本并提升可扩展性，同时解决 OneRec 基于梯度界限的对称保守性导致的更新受限和高噪声环境中的多样性崩溃问题。

Method: 提出 SAGE：1) 序列级信号解耦，结合几何平均重要性比与解耦的多目标优势，消除 token 级方差并解决 Reward Collapse；2) 非对称自适应动力学，构建动态梯度流形，对高潜力冷启动项使用 Boost Factor 以实现超线性更新，并引入 Entropy Aware Penalty 以打破信息茧。理论分析与经验结果证明 SAGE 能解锁冷启动流量、维持多样性，并保持与 GBPO 相当的数值稳定性。

Result: 理论分析与大量实验显示，SAGE 能有效解锁冷启动流量、提升或维持推荐多样性，同时兼具对梯度稳定性的保护，与原有 GBPO 在稳定性方面保持一致。

Conclusion: SAGE 为基于序列的列表式生成推荐提供一个统一、高效的优化框架，使可复用的开源 LLM 架构在不构建独立分词表的前提下实现良好性能，并对冷启动与多样性具有鲁棒性。

Abstract: While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a "Symmetric Conservatism" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise environments.To address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the "Reward Collapse" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a "Boost Factor" to high-potential cold start items to achieve super-linear updates and employs an "Entropy Aware Penalty" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.

</details>


### [185] [HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing](https://arxiv.org/abs/2601.21459)
*Chengyu Du,Xintao Wang,Aili Chen,Weiyuan Li,Rui Xu,Junteng Liu,Zishan Huang,Rong Tian,Zijun Sun,Yuhao Li,Liheng Feng,Deming Ding,Pengyu Zhao,Yanghua Xiao*

Main category: cs.LG

TL;DR: 提出 HER 框架以对话角色扮演中的认知层级进行模拟，核心在双层思维、数据与奖励信号的对齐，并在 Qwen3-32B 上通过监督学习和强化学习训练模型，显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 角色扮演难以模拟内在思维，缺乏高质量推理轨迹数据和与人类偏好一致的奖励信号，限制认知层次的研究和应用。

Method: 引入双层思维：第一人称思维与第三人称思维分离；通过反向工程构建推理增强式角色扮演数据，制定人类对齐的原则与奖励模型；基于 Qwen3-32B 进行监督学习和强化学习的训练。

Result: 在 CoSER 基准上较基线提升 30.26，在 Minimax Role-Play Bench 提升 14.97；与 Qwen3-32B 基线相比显著优越。

Conclusion: 将发布数据集、原则和模型以促进未来研究，提供可复现的认知层级角色扮演研究资源。

Abstract: LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters' first-person thinking from LLMs' third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.

</details>


### [186] [L$^3$: Large Lookup Layers](https://arxiv.org/abs/2601.21461)
*Albert Tseng,Christopher De Sa*

Main category: cs.LG

TL;DR: L^3 将可扩展的查找表嵌入引入到解码器层，通过静态的基于 token 的路由聚合一组学习嵌入，实现内存与计算的有效权衡，在系统友好性和信息论嵌入分配的支撑下，显著优于密集模型和等参数量的 MoE。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏语言模型多采用动态硬路由的 Mixture-of-Experts（MoE），但存在潜在的硬件低效、训练不稳定以及需要辅助损失等问题。与之相比，词嵌入表天然稀疏、但缺乏上下文信息。需要一个在稀疏性、上下文感知与训练/推理效率之间取得平衡的方案。

Method: 提出 Large Lookup Layer (L^3)，将嵌入表扩展到解码层。L^3 使用静态 token-based 路由，将同一个 token 对应的一组学习嵌入在上下文中进行聚合，允许模型通过缓存信息在内存和计算之间权衡。核心组成包括：(1) 系统友好架构，支持快速训练与 CPU 离线推理且无额外开销；(2) 信息论嵌入分配算法，平衡速度与质量。

Result: 在多达 2.6B 活跃参数的变换器上进行实验，L^3 在语言建模与下游任务中显著优于密集模型和等参数量的 Iso-sparse MoE。

Conclusion: L^3 为稀疏性提供新的维度，将嵌入表扩展到解码层，通过静态路由与信息论驱动的嵌入分配实现高效且高质量的推理与训练，是对 MoE 的有效替代并具备实用性。

Abstract: Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP "experts." However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.

</details>


### [187] [Partial Feedback Online Learning](https://arxiv.org/abs/2601.21462)
*Shihao Shao,Cong Fang,Zhouchen Lin,Dacheng Tao*

Main category: cs.LG

TL;DR: 在部分反馈集合可实现性在线学习中，给出确定性学习者的 Partial-Feedback Littlestone 维数（PFLdim）和随机化学习者的 Partial-Feedback Measure Shattering 维数（PMSdim）的近完整表征，刻画极小极大损失与可学习性，并揭示 deterministic 与 randomized 学习之间的不可分离性、集合学习的扩展、以及 realizability 外的不可实现性带来的信息论下界。


<details>
  <summary>Details</summary>
Motivation: 解决语言生成等存在多正确标签但仅给出单一参考答案的在线学习情形；扩展 Littlestone 维数理论到部分反馈设置，揭示在 set-realizable（子线性回归/对数线性等子线性损失）下的可学习性边界与极限。

Method: 引入新的集合版本空间视角，提出 PFLdim 与 PMSdim 的定义及其证明框架；给出确定性与随机性学习者的上下界与收敛性结果；识别导致确定性与随机性可学习性不可分离的广义条件（如有限 Helly 数、嵌套标签结构），并将方法推广至集合集合的在线学习，解答 Raman et al. 2024b 的开放问题；讨论 realizability 外的不可实现性及线性衰减下界。

Result: PFLdim 精确决定确定性学习者的可学习性与 minimax 损失；PMSdim 提供随机化学习者的紧界；在若干条件下存在确定性与随机性不可分离的情形；将结论扩展至集合集标注的情形，解决相关开放问题；在 set realizability 外，理论上可出现线性 regret，甚至当 |H|=2 时也成立。

Conclusion: 建立了部分反馈在线学习的新维数框架，揭示了现有版本空间与学习理论在这类问题上的不足，强调需要开发对噪声更敏感的复杂度度量来刻画超出 set realizability 的学得性。

Abstract: We study partial-feedback online learning, where each instance admits a set of correct labels, but the learner only observes one correct label per round; any prediction within the correct set is counted as correct. This model captures settings such as language generation, where multiple responses may be valid but data provide only a single reference. We give a near-complete characterization of minimax regret for both deterministic and randomized learners in the set-realizable regime, i.e., in the regime where sublinear regret is generally attainable. For deterministic learners, we introduce the Partial-Feedback Littlestone dimension (PFLdim) and show it precisely governs learnability and minimax regret; technically, PFLdim cannot be defined via the standard version space, requiring a new collection version space viewpoint and an auxiliary dimension used only in the proof. We further develop the Partial-Feedback Measure Shattering dimension (PMSdim) to obtain tight bounds for randomized learners. We identify broad conditions ensuring inseparability between deterministic and randomized learnability (e.g., finite Helly number or nested-inclusion label structure), and extend the argument to set-valued online learning, resolving an open question of Raman et al. [2024b]. Finally, we show a sharp separation from weaker realistic and agnostic variants: outside set realizability, the problem can become information-theoretically intractable, with linear regret possible even for $|H|=2$. This highlights the need for fundamentally new, noise-sensitive complexity measures to meaningfully characterize learnability beyond set realizability.

</details>


### [188] [Shaping capabilities with token-level data filtering](https://arxiv.org/abs/2601.21571)
*Neil Rathi,Alec Radford*

Main category: cs.LG

TL;DR: 通过对预训练阶段的数据进行过滤，尤其是按 token 级别进行过滤，显著降低语言模型的 undesired capabilities，且成本更低、可扩展性更强；token 过滤比文档过滤效果更好，且在大模型上随规模提升效果更显著（如对 forget 域的 7000x 计算开销下降）。在 token 标注方面提出了使用稀疏自编码器标注 token 并蒸馏出低成本高质量分类器的路线，且对噪声标签具有鲁棒性；模型在 forget 域仍然可以保持对齐。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有减少不良能力的方法多为事后处理、易被对手绕过的问题。通过在预训练阶段塑形模型能力，利用数据层面的干预实现更高效、可扩展且鲁棒的安全性改进。

Method: 将预训练数据分为文档层级与 token 层级两种过滤粒度，比较两者在移除不良能力方面的效果与成本。将过滤扩展到两阶数量级的模型，并在“forget”域等场景中评估。提出通过稀疏自编码器对 token 进行标签化，并从中蒸馏出廉价但高质量的分类器来实现高效 token 过滤。研究表明数据标签对过滤效果有显著影响且对噪声标签具有鲁棒性。

Result: token 级过滤在同等收益下对 benign 能力的损失更小，总体成本更低；在最大规模模型中，对 forget 域的计算开销下降达到 7000 倍；经过 token 过滤训练的模型在 forget 域仍可对齐；提出的 token 标签与分类蒸馏方法可在粗粒度噪声标签下仍保持有效性。

Conclusion: 在预训练阶段通过 token 级数据过滤实现对 undesired capabilities 的高效、可扩展、鲁棒的抑制，比文档过滤更具性价比；并提出可将标注 token 的方法与稀疏自编码器相结合来实现低成本的数据清洗与对齐，具有良好的扩展性与鲁棒性。

Abstract: Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.

</details>


### [189] [A block-coordinate descent framework for non-convex composite optimization. Application to sparse precision matrix estimation](https://arxiv.org/abs/2601.21467)
*Guillaume Lauga*

Main category: cs.LG

TL;DR: 提出一个通用的块坐标下降（BCD）框架以处理非凸复合优化，确保目标函数下降并收敛。框架可包含变量度量的近端梯度更新、近端牛顿更新与交替最小化更新，统一了图形Lasso的三种求解器（Graphical ISTA、Primal GLasso、QUIC），并在非凸稀疏精度矩阵估计中给出收敛性保证及在迭代次数上的显著加速（最高可达100倍）。“


<details>
  <summary>Details</summary>
Motivation: 在非凸优化领域，尽管BCD在很多大规模问题中有应用，但其理论分析不足，特别是在非凸情形下的收敛性和性能保证需要更系统的框架与分析。稀疏精度矩阵估计（Graphical Lasso）是一个典型的非凸/半凸问题，亟需统一且高效的求解器。

Method: 提出一个广义的BCD框架，确保目标函数单调下降并收敛到某种解。该框架支持多种更新策略：变量度量的近端梯度/近端牛顿更新和交替最小化更新，并可嵌入到图形Lasso的三种主流求解器中，形成统一的理论与算法基础。

Result: 给出理论层面的收敛性保证，对非凸稀疏精度矩阵估计问题的应用中，实验显示相比现有方法在达到同等估计质量时迭代次数可减少至1/100左右。

Conclusion: 该框架具有很强的通用性和实用性，能够统一并加速多种Graphical Lasso求解器，并为非凸稀疏精度矩阵估计提供可靠的收敛性分析与性能提升。

Abstract: Block-coordinate descent (BCD) is the method of choice to solve numerous large scale optimization problems, however their theoretical study for non-convex optimization, has received less attention. In this paper, we present a new block-coordinate descent (BCD) framework to tackle non-convex composite optimization problems, ensuring decrease of the objective function and convergence to a solution. This framework is general enough to include variable metric proximal gradient updates, proximal Newton updates, and alternated minimization updates. This generality allows to encompass three versions of the most used solvers in the sparse precision matrix estimation problem, deemed Graphical Lasso: graphical ISTA, Primal GLasso, and QUIC. We demonstrate the value of this new framework on non-convex sparse precision matrix estimation problems, providing convergence guarantees and up to a $100$-fold reduction in the number of iterations required to reach state-of-the-art estimation quality.

</details>


### [190] [PPI-SVRG: Unifying Prediction-Powered Inference and Variance Reduction for Semi-Supervised Optimization](https://arxiv.org/abs/2601.21470)
*Ruicheng Ao,Hongyu Chen,Haoyang Liu,David Simchi-Levi,Will Wei Sun*

Main category: cs.LG

TL;DR: PPI 与 SVRG 的方差降低机制等价，提出 PPI-SVRG 将两者结合；理论将收敛界分解为标准 SVRG 速率与预测不确定性导致的误差底，预测越好收敛越接近 SVRG；在标签稀缺场景下实验显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在标注数据稀缺但可利用预训练模型预测的半监督/半监督优化场景中，降低估计方差以提升收敛速率与泛化性能。

Method: 证明 PPI 与 SVRG 的等价性并将两者结合成 PPI-SVRG；给出收敛界的分解，显示速率取决于损失几何，预测误差影响稳态邻域；在完美预测时回退到 SVRG；并通过实验验证理论。

Result: 理论上将标准 SVRG 速率与预测误差两部分相加的收敛界；预测良好时可达到近似 SVRG 的性能，预测劣化则收敛到更大但稳定的邻域；实证结果：在标签稀缺条件下，MSE 降低 43–52%，在 MNIST 仅 10% 标签时测试准确率提升 2.7–2.9 个百分点。

Conclusion: 将预测信息与参考梯度的方差降低结合，提供在半监督场景中鲁棒且高效的优化框架；预测质量决定稳态误差区间，方法对损失几何具有鲁棒性。

Abstract: We study semi-supervised stochastic optimization when labeled data is scarce but predictions from pre-trained models are available. PPI and SVRG both reduce variance through control variates -- PPI uses predictions, SVRG uses reference gradients. We show they are mathematically equivalent and develop PPI-SVRG, which combines both. Our convergence bound decomposes into the standard SVRG rate plus an error floor from prediction uncertainty. The rate depends only on loss geometry; predictions affect only the neighborhood size. When predictions are perfect, we recover SVRG exactly. When predictions degrade, convergence remains stable but reaches a larger neighborhood. Experiments confirm the theory: PPI-SVRG reduces MSE by 43--52\% under label scarcity on mean estimation benchmarks and improves test accuracy by 2.7--2.9 percentage points on MNIST with only 10\% labeled data.

</details>


### [191] [ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment](https://arxiv.org/abs/2601.21484)
*Xiuyu Li,Jinkai Zhang,Mingyang Yi,Yu Li,Longqiang Wang,Yue Wang,Ju Fan*

Main category: cs.LG

TL;DR: 提出一种无训练推理的能量引导方法 ETS，在测试时直接从最优 RL 策略采样，结合参考策略与能量项，具备在线蒙特卡洛估计的收敛性，并通过加速框架与重要性采样实现高效推理；在 MLM、自回归模型和扩散语言模型上对推理质量有显著提升。


<details>
  <summary>Details</summary>
Motivation: RL 后训练对齐语言模型成本高、训练过程不稳定。需要一种无训练开销且稳定的测试时推理方式，以降低成本并提升生成质量。

Method: 在状态转移概率中引入能量项，与参考策略共同作用；能量项通过在线蒙特卡洛估计获得，具备可证明的收敛速率。为提高效率，结合现代加速框架与定制化的重要性采样估计量；通过测试时采样实现，从而避免训练阶段的代价。

Result: 在 MLM（包括自回归模型和扩散语言模型）上的推理任务中，ETS 在推理质量上持续提升，在 reasoning、coding、science 等基准上表现稳健，且实现显示出显著的推理效率提升。

Conclusion: 提出一个训练-free、具备收敛性和高效性的测试时推理框架，适用于多种语言模型，提供了低成本、稳定且高质量的 RL-对齐替代方案。

Abstract: Reinforcement Learning (RL) post-training alignment for language models is effective, but also costly and unstable in practice, owing to its complicated training process. To address this, we propose a training-free inference method to sample directly from the optimal RL policy. The transition probability applied to Masked Language Modeling (MLM) consists of a reference policy model and an energy term. Based on this, our algorithm, Energy-Guided Test-Time Scaling (ETS), estimates the key energy term via online Monte Carlo, with a provable convergence rate. Moreover, to ensure practical efficiency, ETS leverages modern acceleration frameworks alongside tailored importance sampling estimators, substantially reducing inference latency while provably preserving sampling quality. Experiments on MLM (including autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show that our ETS consistently improves generation quality, validating its effectiveness and design.

</details>


### [192] [Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking](https://arxiv.org/abs/2601.21619)
*Yiming Wang,Zhuosheng Zhang,Rui Wang*

Main category: cs.LG

TL;DR:  formalizes the overscaling curse in system-level parallel thinking for LLMs and introduces T2, a per-sample adaptive parallelism estimator using latent representations to reduce cost while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Identify and quantify the inefficiency: system-wide high parallelism (N) may be unnecessary for many samples due to heterogeneity, causing budget redundancy. Address the existence, universality, and severity of the overscaling curse.

Method: Formalize and quantify the overscaling curse; analyze its trigger mechanism; propose T2, a lightweight method that uses latent representations to estimate the optimal per-sample parallelism level before decoding.

Result: Empirical results show substantial cost reductions with comparable performance when using T2, enabling more efficient parallel thinking without sacrificing accuracy.

Conclusion: T2 effectively breaks the overscaling curse by tailoring parallelism to per-sample needs, yielding more efficient LLM reasoning at system scale.

Abstract: Parallel thinking enhances LLM reasoning by multi-path sampling and aggregation. In system-level evaluations, a global parallelism level N is allocated to all samples, typically set large to maximize overall dataset accuracy. However, due to sample heterogeneity, some samples can achieve comparable performance with a smaller N'< N, causing budget redundancy. This incompatibility between system-level efficacy and sample-level efficiency constitutes the overscaling curse. In this paper, we formalize and quantify the overscaling curse, showing its universality and severity in practice, and analyze its trigger mechanism. We then propose a lightweight method, T2, to break the overscaling curse, which utilizes latent representations to estimate the optimal parallelism level for each sample before decoding. Experiments show that T2 significantly reduces cost while maintaining comparable performance, enabling more efficient parallel thinking.

</details>


### [193] [Task-Awareness Improves LLM Generations and Uncertainty](https://arxiv.org/abs/2601.21500)
*Tim Tomov,Dominik Fuchsgruber,Stephan Günnemann*

Main category: cs.LG

TL;DR: 提出一个基于潜在结构的贝叶斯最优解码框架，在任务相关的潜在结构空间中对LLM输出建模，利用不相似性度量进行组合，生成新的合成解，从而超越_lang_space解码（如束搜索）并通过贝叶斯风险对不确定性进行校准。


<details>
  <summary>Details</summary>
Motivation: 传统的语言空间解码忽略输出的结构信息（如离散标签、数值、图结构），在实际应用中难以针对任务需求进行高效且可校准的推断。引入任务相关的潜在输出结构及其不相似性度量，有望提升预测质量和不确定性估计的对齐度。

Method: 将LLM输出建模为一个任务相关的潜在结构；引入结构间的不相似性度量；通过贝叶斯风险框架计算最优解（Bayes-optimal）而非对生成进行采样；以在潜在结构空间中结合各个回答的方式合成新的响应。并用诱导的贝叶斯风险来量化不确定性，捕捉结构变化对输出质量的影响。

Result: 在多种任务上，贝叶斯最优解码比标准解码方法（如束搜索）表现更好；不确定性通过贝叶斯风险与潜在结构的变化相关，且与输出的质量和正确性对齐度提升。

Conclusion: 该框架对任何存在潜在输出结构的任务都具有普遍性，可实现面向任务的、更可靠的LLM预测与不确定性校准。

Abstract: In many applications of LLMs, natural language responses often have an underlying structure such as representing discrete labels, numerical values, or graphs. Yet, existing decoding and uncertainty estimation methods operate only in language space and largely disregard structural information. We address this by modeling LLM outputs directly in a task-dependent latent structure. By equipping this structure with a dissimilarity measure, we can compute Bayes-optimal responses. These are not selected from sampled generations but are newly synthesized by combining individual responses in the latent space. Across different tasks, Bayes-optimal responses consistently outperform standard decoding methods like beam search. Moreover, quantifying uncertainty via the induced Bayesian risk captures variations in terms of the latent structure and improves alignment with output quality and correctness. Our decision-theoretic framework is applicable to any problem that admits a latent response structure and enables reliable task-aware LLM predictions.

</details>


### [194] [SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning](https://arxiv.org/abs/2601.21649)
*Jinjun Peng,Magnus Saebo,Tianjun Zhong,Yi-Jie Cheng,Junfeng Yang,Baishakhi Ray,Simin Chen,Yangruibo Ding*

Main category: cs.LG

TL;DR: 提出 Repository-Centric Learning (RCL) 替代 Task-Centric Learning，聚焦垂直代码库深度，训练 repo-specialized experts（SWE-Spot-4B），在资源受限和隐私场景中实现比开放权重模型更强的推理泛化与更高的训练样本效率。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感与资源受限的环境中，现有开权重SLMs缺乏对复杂、陌生代码库的推理泛化能力。TCL通过横向任务曝光难以解决该瓶颈，因此需要通过对目标软件环境的参数化知识获取来实现内化能力。

Method: 提出以垂直仓库深度为核心的 Repository-Centric Learning (RCL) 范式；设计四单元的 Repository-Centric Experience，将静态代码库转化为互动学习信号；训练 SWE-Spot-4B 家族作为仓库专用的专家模型，打破尺度规律并在多项SWE任务上优于更大或更高效的对照模型。

Result: SWE-Spot-4B 在多项 SWE 任务上超越公开权重模型（如 CWM、Qwen3-Coder-30B）并比效率导向商业模型（如 GPT-4.1-mini、GPT-5-nano）更优，同时展现更高的训练样本效率与更低的推理成本。

Conclusion: 仓库掌握是实现高效、可推理智能的新维度，补充并扩展了通用编码能力；RCL 在资源与隐私受限场景下具有重要应用价值，需要进一步的实验验证与对比分析以评估普适性与鲁棒性。

Abstract: The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the "physics" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.

</details>


### [195] [Cascaded Transfer: Learning Many Tasks under Budget Constraints](https://arxiv.org/abs/2601.21513)
*Eloi Campagne,Yvenn Amara-Ouali,Yannig Goude,Mathilde Mougeot,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 提出一种级联传递学习（Cascaded Transfer Learning），在最小生成树结构下按预算分配对多任务进行层级化知识传递，以同类模型对任务进行逐步学习与 refinement，从而在大任务集上实现更高准确性和更低成本。


<details>
  <summary>Details</summary>
Motivation: 在大量相关任务中，任务之间的精确关系未知，且传递学习需要在受限预算下实现高效的跨任务适配。

Method: 提出级联传递学习范式，将信息（如模型参数）在同类模型的任务之间层级级联，形成一个以根节点为起点的树状结构；在任务之间通过最小生成树连接，依据合适的距离度量分配训练预算，并在分支上逐步学习和 refinement 各任务。

Result: 在合成和真实多任务场景的实验中，所提方法在跨大规模任务集合的适应性、准确性和成本效益方面优于替代方法。

Conclusion: 级联传递学习为在受预算约束下实现大规模、多任务集合的高效且准确的跨任务传递提供了可扩展框架，适用于多种多任务学习场景。

Abstract: Many-Task Learning refers to the setting where a large number of related tasks need to be learned, the exact relationships between tasks are not known. We introduce the Cascaded Transfer Learning, a novel many-task transfer learning paradigm where information (e.g. model parameters) cascades hierarchically through tasks that are learned by individual models of the same class, while respecting given budget constraints. The cascade is organized as a rooted tree that specifies the order in which tasks are learned and refined. We design a cascaded transfer mechanism deployed over a minimum spanning tree structure that connects the tasks according to a suitable distance measure, and allocates the available training budget along its branches. Experiments on synthetic and real many-task settings show that the resulting method enables more accurate and cost effective adaptation across large task collections compared to alternative approaches.

</details>


### [196] [Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities](https://arxiv.org/abs/2601.21702)
*Tien Dang,The-Hai Nguyen,Dinh Mai Phuong,Nguyen Minh Phuong,Hoang Thanh-Tung,Le-Minh Nguyen,Naoya Inoue*

Main category: cs.LG

TL;DR: 通过将忘却表征映射到一个一维线性空间，表示误导（RM）不仅实现忘却，还可能引发对高层概念的可控副行为和能力提升；这既是风险也是潜在的能力开发途径。


<details>
  <summary>Details</summary>
Motivation: 澄清RM中目标向量的作用及其对忘却后副行为的影响，借用线性表征假说探究是否存在可线性操作的高层概念向量，以及这对安全性和能力扩展的含义。

Method: 在忘却表征空间中探索并识别与高层概念对应的一维表示，并验证在该子空间内可对该概念向量进行线性操作；通过一系列任务评估副行为（如对真相、情感、拒绝等的控制）和能力提升（如在情境学习能力方面的增强）。

Result: 实验结果表明，除了忘却之外，模型在忘却表征中确实产生可控的副行为和潜在的能力增强，且这些效应可在多任务中观察到。

Conclusion: 忘却不仅是信息删除过程，也是一种可被放大/引导的潜在能力通道。该现象具有双重性：既可能带来安全风险，又可用于构建需要更强能力和可控行为的模型；需对其机制与安全性进行审慎评估。

Abstract: We consider representation misdirection (RM), a class of LLM unlearning methods that achieves forgetting by manipulating the forget-representations, that is, latent representations of forget samples. Despite being important, the roles of target vectors used in RM, however, remain underexplored. Here, we approach and revisit RM through the lens of the linear representation hypothesis. Specifically, if one can somehow identify a one-dimensional representation corresponding to a high-level concept, the linear representation hypothesis enables linear operations on this concept vector within the forget-representation space. Under this view, we hypothesize that, beyond forgetting, machine unlearning elicits controllable side behaviors and stronger side capabilities corresponding to the high-level concept. Our hypothesis is empirically validated across a wide range of tasks, including behavioral control (e.g., controlling unlearned models' truth, sentiment, and refusal) and capability enhancement (e.g., improving unlearned models' in-context learning capability). Our findings reveal that this fairly attractive phenomenon could be either a hidden risk if misused or a mechanism that can be harnessed for developing models that require stronger capabilities and controllable behaviors.

</details>


### [197] [A Unified SPD Token Transformer Framework for EEG Classification: Systematic Comparison of Geometric Embeddings](https://arxiv.org/abs/2601.21521)
*Chi-Sheng Chen,En-Jui Kuo,Guan-Ying Chen,Xinyu Zhang,Fan Zhang*

Main category: cs.LG

TL;DR: 对 SPD 矩阵在 Riemannian 流形上的嵌入几何与优化动力学关系的统一分析。提出 BWSPD 的 sqrt(κ) 梯度条件数优于 Log-Euclidean，嵌入空间的 BN-Embed 可近似李群归一化，且 BWSPD 保距性有 bi-Lipschitz 下界；通过在同一 Transformer 框架下对比 BWSPD、Log-Euclidean 与 Euclidean 嵌入，在 1500+ 次实验中验证理论预测，Log-Euclidean Transformer 在三类 EEG 数据集上取得最优或接近最优的性能，BWSPD 竞争力强且训练时间相似。


<details>
  <summary>Details</summary>
Motivation: 揭示 SPD 矩阵嵌入在 Riemannian 流形上的几何选择如何影响梯度条件数、数值稳定性与优化过程；通过理论推导和大规模实验验证来阐明嵌入对模型训练和泛化的影响。

Method: 给出三条理论结果：1) 通过 Daleckii-Krein 矩阵推导 BWSPD 具有 sqrt(κ) 梯度条件数，相比 Log-Euclidean 的 κ，在高维输入（d≥22）下表现更好；低维（d≤8）时因特征分解开销成为瓶颈；2) 嵌入空间 BN（BN-Embed）在 Riemannian 归一化附近具有 O(ε^2) 的误差，可带来在 56通道 ERP 数据集上 +26% 的精度提升，而在 8通道 SSVEP 数据上影响甚微；3) 双 Lipschitz 边界证明 BWSPD 映射能保持流形距离，失真仅受 κ 的影响。以统一的 Transformer 框架在三类 EEG 任务（运动想象、ERP、SSVEP; 36 位被试，超过 1500 次运行）对 BWSPD、Log-Euclidean、Euclidean 三种嵌入进行比较。

Result: 在实验层面，Log-Euclidean Transformer 在所有数据集上达到状态-of-the-art 的性能，显著超越传统 Riemannian 分类器和最近的 SPD 基线；BWSPD 在训练时间相近的情况下提供具有竞争力的准确率。BN-Embed 的实证结果与通道数相关的预测一致。整体上，理论结论得到实验验证，且 Log-Euclidean 嵌入在多数据集的泛化能力尤为突出。

Conclusion: 本文建立了 SPD 嵌入几何与优化行为之间的明确联系，展示了 Log-Euclidean 嵌入在 EEG 任务上的鲁棒性与性能优势，以及 BWSPD 在高维情境下更好的梯度条件数与对流形距离的保留性；BN-Embed 提供了可行的近似归一化手段，且结果受通道数影响显著。这些发现为在 SPD 流形上的深度学习设计提供了理论基础与实证支撑。

Abstract: Spatial covariance matrices of EEG signals are Symmetric Positive Definite (SPD) and lie on a Riemannian manifold, yet the theoretical connection between embedding geometry and optimization dynamics remains unexplored. We provide a formal analysis linking embedding choice to gradient conditioning and numerical stability for SPD manifolds, establishing three theoretical results: (1) BWSPD's $\sqrtκ$ gradient conditioning (vs $κ$ for Log-Euclidean) via Daleckii-Kreĭn matrices provides better gradient conditioning on high-dimensional inputs ($d \geq 22$), with this advantage reducing on low-dimensional inputs ($d \leq 8$) where eigendecomposition overhead dominates; (2) Embedding-Space Batch Normalization (BN-Embed) approximates Riemannian normalization up to $O(\varepsilon^2)$ error, yielding $+26\%$ accuracy on 56-channel ERP data but negligible effect on 8-channel SSVEP data, matching the channel-count-dependent prediction; (3) bi-Lipschitz bounds prove BWSPD tokens preserve manifold distances with distortion governed solely by the condition ratio $κ$. We validate these predictions via a unified Transformer framework comparing BWSPD, Log-Euclidean, and Euclidean embeddings within identical architecture across 1,500+ runs on three EEG paradigms (motor imagery, ERP, SSVEP; 36 subjects). Our Log-Euclidean Transformer achieves state-of-the-art performance on all datasets, substantially outperforming classical Riemannian classifiers and recent SPD baselines, while BWSPD offers competitive accuracy with similar training time.

</details>


### [198] [More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)](https://arxiv.org/abs/2601.21522)
*Sagi Meir,Tommer D. Keidar,Noam Levi,Shlomi Reuveni,Barak Hirshberg*

Main category: cs.LG

TL;DR: 提出 Reset-and-Discard (ReD) 以提升覆盖率覆盖成本（coverage@cost），并建立与 pass@k 的联系，理论上呈幂律导致收益递减；通过 ReD 在给定预算下提升覆盖率，并可在有/无 pass@k 时预测节省的尝试数及推断幂律指数。在三种模型和 HumanEval 上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评估多依赖 pass@k，未能直接反映在固定预算下的覆盖效率。由于 pass@k 常呈幂律行为，覆盖随成本增长呈现递减回报，因此需要一种方法在相同预算下提升覆盖率，并提供对幂律特性的推断与预测。

Method: 提出 Reset-and-Discard (ReD) 查询策略，通过重置对话或丢弃无关尝试以增加对新问题的覆盖率，且对任意 pass@k形式均有效。给定 pass@k 时，能够定量预测使用 ReD 的节省的总尝试数；若无 pass@k，可推断其幂律指数。

Result: 在三个 LLM 上基于 HumanEval 的实验表明，ReD 能显著降低达到期望覆盖率所需的尝试、tokens 和成本，并提供高效的推理幂律测量方法。

Conclusion: ReD 为在固定预算下提升覆盖能力的通用方法，并具备预测成本节省与幂律参数推断的能力，提升对 LLM 推理能力的评估与成本优化的实用性。

Abstract: The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.

</details>


### [199] [Multi-Modal Time Series Prediction via Mixture of Modulated Experts](https://arxiv.org/abs/2601.21547)
*Lige Zhang,Ali Maatouk,Jialin Chen,Leandros Tassiulas,Rex Ying*

Main category: cs.LG

TL;DR: 提出 Expert Modulation，一种通过文本信号直接调控 MoE 路由与专家计算的多模态时间序列预测新范式，理论与实证均显示显著提升，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列具备复杂且演化的动态性，且高质量的时序-文本对较稀缺，文本对的尺度差异也使跨模态对齐困难。现有方法多采用基于 token 的融合，尤其在 MoE 结合中也依赖 token-level 融合，限制了跨模态协同。

Method: Expert Modulation 在混合专家框架中引入文本信号作为条件，直接影响路由决策和专家计算，即通过文本实现跨模态控制下的专家行为调优，从而实现跨模态信息的高效整合。

Result: 理论分析与大规模实验表明，该方法在多模态时间序列预测任务上获得显著提升，相比基线方法更具鲁棒性并有效处理尺度和特征差异等挑战。

Conclusion: 通过将文本信息用于条件化 MoE 的路由与专家输出，Expert Modulation 提升了跨模态对齐与预测性能，提供了可复现的实现（代码开源）。

Abstract: Real-world time series exhibit complex and evolving dynamics, making accurate forecasting extremely challenging. Recent multi-modal forecasting methods leverage textual information such as news reports to improve prediction, but most rely on token-level fusion that mixes temporal patches with language tokens in a shared embedding space. However, such fusion can be ill-suited when high-quality time-text pairs are scarce and when time series exhibit substantial variation in scale and characteristics, thus complicating cross-modal alignment. In parallel, Mixture-of-Experts (MoE) architectures have proven effective for both time series modeling and multi-modal learning, yet many existing MoE-based modality integration methods still depend on token-level fusion. To address this, we propose Expert Modulation, a new paradigm for multi-modal time series prediction that conditions both routing and expert computation on textual signals, enabling direct and efficient cross-modal control over expert behavior. Through comprehensive theoretical analysis and experiments, our proposed method demonstrates substantial improvements in multi-modal time series prediction. The current code is available at https://github.com/BruceZhangReve/MoME

</details>


### [200] [GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization](https://arxiv.org/abs/2601.22095)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Chi Wang,Yuehao Wang,Jing Xiong,Liliang Ren,Bo Peng,Qingmei Wang,Xiaoran Shang,Mac Schwager,Anderson Schneider,Yuriy Nevmyvaka,Xiaodong Liu*

Main category: cs.LG

TL;DR: GeoNorm 将基于流形优化的几何归一化替代常规归一化，通过在 FFN 和注意力输出方向上进行流形上的更新，并引入逐层更新衰减，提升 Transformer 性能且计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 解决 Pre-Norm 与 Post-Norm 在 Transformer 设计中的不确定性，提供一个几何/流形视角来理解正则化/归一化的位置对学习动力学的影响。

Method: 将 FFN 与注意力层输出视为优化中的更新方向，在流形上进行几何更新（geodesic updates），用 GeoNorm 代替标准归一化；对 FFN 与注意力分别采用层级更新衰减策略（layer-wise update decay）。

Result: 综合实验表明 GeoNorm 在 Transformer 模型中优于现有归一化方法，且与标准架构兼容、几乎额外开销为零。

Conclusion: GeoNorm 提供一种可直接嵌入现有 Transformer 的新归一化思路，通过几何更新提升性能，且具有较低额外成本。

Abstract: The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in Transformer architecture design. In this work, we rethink these approaches through the lens of manifold optimization, interpreting the outputs of the Feed-Forward Network (FFN) and attention layers as update directions in optimization. Building on this perspective, we introduce GeoNorm, a novel method that replaces standard normalization with geodesic updates on the manifold. Furthermore, analogous to learning rate schedules, we propose a layer-wise update decay for the FFN and attention components. Comprehensive experiments demonstrate that GeoNorm consistently outperforms existing normalization methods in Transformer models. Crucially, GeoNorm can be seamlessly integrated into standard Transformer architectures, achieving performance improvements with negligible additional computational cost.

</details>


### [201] [FlexCausal: Flexible Causal Disentanglement via Structural Flow Priors and Manifold-Aware Interventions](https://arxiv.org/abs/2601.21567)
*Yutao Jin,Yuang Tao,Junyong Zhai*

Main category: cs.LG

TL;DR: 提出FlexCausal：一种基于块对角协方差的VAE框架，结合分解流先验、监督对齐与反事实一致性约束，以及流形感知的相对干预策略，实现在观测数据中的因果结构解耦和高保真生成，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有因果解耦方法多采用对角后验协方差的均值场近似，导致潜在维度彼此去相关化；同时常用各向同性高斯先验来建模外源噪声，难以捕捉真实世界因果因素的非高斯、复杂分布，限制因果结构学习的准确性。

Method: 提出FlexCausal，基于块对角协方差的VAE，采用Factorized Flow-based Prior以真实建模外源噪声的复杂密度；通过监督对齐目标与反事实一致性约束实现学习潜在子空间与真实因果关系的精确对应；并引入流形感知的相对干预策略提升生成保真度。

Result: 在合成与真实数据集上的实验结果显示，FlexCausal显著优于其他方法。

Conclusion: FlexCausal有效缓解对独立同分布假设的依赖，提升因果表示学习的准确性与生成质量。

Abstract: Causal Disentangled Representation Learning(CDRL) aims to learn and disentangle low dimensional representations and their underlying causal structure from observations. However, existing disentanglement methods rely on a standard mean-field approximation with a diagonal posterior covariance, which decorrelates all latent dimensions. Additionally, these methods often assume isotropic Gaussian priors for exogenous noise, failing to capture the complex, non-Gaussian statistical properties prevalent in real-world causal factors. Therefore, we propose FlexCausal, a novel CDRL framework based on a block-diagonal covariance VAE. FlexCausal utilizes a Factorized Flow-based Prior to realistically model the complex densities of exogenous noise, effectively decoupling the learning of causal mechanisms from distributional statistics. By integrating supervised alignment objectives with counterfactual consistency constraints, our framework ensures a precise structural correspondence between the learned latent subspaces and the ground-truth causal relations. Finally, we introduce a manifold-aware relative intervention strategy to ensure high-fidelity generation. Experimental results on both synthetic and real-world datasets demonstrate that FlexCausal significantly outperforms other methods.

</details>


### [202] [Bridging Functional and Representational Similarity via Usable Information](https://arxiv.org/abs/2601.21568)
*Antonio Almudévar,Alfonso Ortega*

Main category: cs.LG

TL;DR: 以可用信息为统一度量，连接功能相似性与表示相似性，揭示双向分析的重要性、预测族容量对相似性的影响，以及任务粒度层级下的普适性。


<details>
  <summary>Details</summary>
Motivation: 当前表示分析方法在功能与表示层面缺乏统一框架，难以解释两者的关系及对预测能力的影响。本文提出以可用信息为核心，整合三条维度（功能、表示、任务粒度），揭示表示相似性是充分但不必要的，且在不同预测能力下具有不同含义。

Method: 1) 将拼接（stitching）性能与条件互信息联系起来，揭示其方向性与需要双向分析；2) 证明重建、CKA、RSA等表示相似性度量是在特定约束下对可用信息的估计；3) 表示相似性的解释依赖于预测族的容量；4) 构建任务粒度层级，将复杂任务的相似性向下推广至较粗粒度的衍生任务；5) 将表示相似性视为输入重建的极限，作为普适性框架的核心。

Result: 核心结论包括：拼接性能与条件互信息存在正式联系，且拼接具有本质的不对称性，需双向比较；重建、CKA、RSA在约束条件下可作为可用信息的近似或估计；表示相似性与功能相似性之间存在充分性但非必要性关系；在复杂到简单的任务粒度层级中，若在高粒度下相似，则在低粒度下也相似；可用信息框架将表示与功能之间的关系统一化，输入重建代表表示相似性的极限。

Conclusion: 本文提供一个理论与经验的统一框架，强调可用信息作为核心度量，帮助理解不同表征在功能任务上的可转移性与局限性，为选择/设计预测模型时对表示评估提供条件与边界。

Abstract: We present a unified framework for quantifying the similarity between representations through the lens of \textit{usable information}, offering a rigorous theoretical and empirical synthesis across three key dimensions. First, addressing functional similarity, we establish a formal link between stitching performance and conditional mutual information. We further reveal that stitching is inherently asymmetric, demonstrating that robust functional comparison necessitates a bidirectional analysis rather than a unidirectional mapping. Second, concerning representational similarity, we prove that reconstruction-based metrics and standard tools (e.g., CKA, RSA) act as estimators of usable information under specific constraints. Crucially, we show that similarity is relative to the capacity of the predictive family: representations that appear distinct to a rigid observer may be identical to a more expressive one. Third, we demonstrate that representational similarity is sufficient but not necessary for functional similarity. We unify these concepts through a task-granularity hierarchy: similarity on a complex task guarantees similarity on any coarser derivative, establishing representational similarity as the limit of maximum granularity: input reconstruction.

</details>


### [203] [Discovering Hidden Gems in Model Repositories](https://arxiv.org/abs/2601.22157)
*Jonathan Kahana,Eliahu Horwitz,Yedid Hoshen*

Main category: cs.LG

TL;DR: 在超大规模模型库中，隐藏的优质微调模型往往被忽视，但其性能可显著超越流行模型；通过将模型发现问题建模为多臂老虎机并改进 Sequential Halving，可在约 50 次查询/候选的条件下，快速识别出高性能模型，提升发现效率逾 50×；例如在 Llama-3.1-8B 家族中，罕见下载的检查点提升数学表现从 83.2% 到 96.0%，且不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 公共代码仓库中的模型使用高度集中于少数基础检查点，这是否意味着市场选择高效，还是优秀模型被系统性忽视？在2,000+ 模型规模下，是否存在未被广泛采用但性能优越的“隐藏宝石”？

Method: 对超过2,000个模型进行全面评估；将模型发现问题形式化为多臂老虎机（Multi-Armed Bandit, MAB）；在此基础上通过共享查询集和更激进的淘汰策略来加速序列剥离（Sequential Halving）搜索；提出利用共享查询集减少重复评估、快速淘汰低潜力候选的加速方案；在每个候选模型上仅需约50次查询就能筛选出頂尖模型。

Result: 发现大量“隐藏宝石”；罕见下载的 Llama-3.1-8B 检查点在不增加推理成本的前提下，将数学能力从83.2%提升至96.0%；整体而言，提出的方法将模型发现速度提升超过50倍。

Conclusion: 超出主流下载量的模型中潜藏显著性能提升的证据充分，且通过将发现问题视作 MAB 并采用加速的 Sequential Halving，能够在不额外开支的情况下高效识别高性能模型，应纳入模型仓库的日常评估流程，以避免错失潜在改进来源。

Abstract: Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of "hidden gems", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.

</details>


### [204] [Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks](https://arxiv.org/abs/2601.21572)
*Jinhao Li,Yuhao Sun,Zhiyuan Ma,Hao He,Xinche Zhang,Xing Chen,Jin Li,Sen Song*

Main category: cs.LG

TL;DR: 提出了一种信号自适应信任域(SATR)的无梯度分布更新规则，用于RSNN策略搜索。通过将KL散度归一化为估计的信号能量来自适应地约束更新，在强信号时放宽、在噪声主导时收缩。针对伯努利连接实现并引入位集优化以提升大规模训练效率，实验显示在高维连续控制任务中对有限种群更稳定且与PPO-LSTM等基线媲美。


<details>
  <summary>Details</summary>
Motivation: 在高维、长时间跨度的强化学习中，使用基于种群的梯度无关优化会因有限种群造成高方差，导致更新不稳定甚至发散。受到强化学习中信任区间思想的启发，提出在分布空间对策略更新进行约束，以提高鲁棒性和稳定性。

Method: 提出信号自适应信任域(SATR)：通过对当前分布与候选分布之间的KL散度进行约束，并用估计的信号能量对KL尺度进行归一化，从而根据信号强度自适应扩展或收缩更新步长。具体实现针对伯努利连接的RSNN进行，且引入二进制脉冲( spike)与二进制权重的位集(bitset)实现以提升并行度并降低计算成本，使大规模优化更具实用性。

Result: 在一组高维连续控制基准任务中，SATR在有限种群条件下提升了训练稳定性，并实现与强基线（包括PPO-LSTM）相竞争的回报。位集实现显著缩短训练时间，提升可扩展性与实际可用性。

Conclusion: SATR提供了一种鲁棒、可扩展的分布更新框架，用于基于种群的无梯度RSNN策略搜索。结合伯努利连接和位集加速，适用于高维、长时程强化学习任务的稳定高效学习。

Abstract: Recurrent spiking neural networks (RSNNs) are a promising substrate for energy-efficient control policies, but training them for high-dimensional, long-horizon reinforcement learning remains challenging. Population-based, gradient-free optimization circumvents backpropagation through non-differentiable spike dynamics by estimating gradients. However, with finite populations, high variance of these estimates can induce harmful and overly aggressive update steps. Inspired by trust-region methods in reinforcement learning that constrain policy updates in distribution space, we propose \textbf{Signal-Adaptive Trust Regions (SATR)}, a distributional update rule that constrains relative change by bounding KL divergence normalized by an estimated signal energy. SATR automatically expands the trust region under strong signals and contracts it when updates are noise-dominated. We instantiate SATR for Bernoulli connectivity distributions, which have shown strong empirical performance for RSNN optimization. Across a suite of high-dimensional continuous-control benchmarks, SATR improves stability under limited populations and reaches competitive returns against strong baselines including PPO-LSTM. In addition, to make SATR practical at scale, we introduce a bitset implementation for binary spiking and binary weights, substantially reducing wall-clock training time and enabling fast RSNN policy search.

</details>


### [205] [Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient Similarity](https://arxiv.org/abs/2601.21577)
*Mutian Yang,Zisen Zhan,Yutong Chen,Haolin Li,Kaiwen Wang,Kaili Zheng,Yuguang Wang,Qi Wang,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: 提出以梯度相似度为核心的理论框架，揭示强负梯度相似度是遗忘根本原因，并将神经元分成冲突与协同两类，提出Collaborative Neural Learning (CNL)在理论上可在极小学习率和已掌握集合条件下实现无遗忘；实验在多模型和数据集上显示显著缓解，且在内集存在零遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在持续学习中出现的灾难性遗忘问题，现有方法缺乏系统的理论解释。

Method: 建立基于梯度相似度的理论框架，证明强负梯度相似度导致 forgetting；将神经元分为冲突神经元（50%-75%）与协同神经元（25%-50%），通过冻结冲突神经元、仅更新协同神经元实现知识注入；在无限小学习率eta和已知掌握集合的前提下给出理论无遗忘的保证。

Result: 在五个LLMs、四个数据集、四个优化器的实验中，CNL在就地设定实现零遗忘，在跨域设定将遗忘降低约59.1%-81.7%。

Conclusion: 基于梯度相似度的神经元分解为知识注入提供了理论支撑与一种可操作策略，理论上可消除遗忘但依赖于严格条件，实践需评估对比及鲁棒性。

Abstract: Catastrophic forgetting during knowledge injection severely undermines the continual learning capability of large language models (LLMs). Although existing methods attempt to mitigate this issue, they often lack a foundational theoretical explanation. We establish a gradient-based theoretical framework to explain catastrophic forgetting. We first prove that strongly negative gradient similarity is a fundamental cause of forgetting. We then use gradient similarity to identify two types of neurons: conflicting neurons that induce forgetting and account for 50%-75% of neurons, and collaborative neurons that mitigate forgetting and account for 25%-50%. Based on this analysis, we propose a knowledge injection method, Collaborative Neural Learning (CNL). By freezing conflicting neurons and updating only collaborative neurons, CNL theoretically eliminates catastrophic forgetting under an infinitesimal learning rate eta and an exactly known mastered set. Experiments on five LLMs, four datasets, and four optimizers show that CNL achieves zero forgetting in in-set settings and reduces forgetting by 59.1%-81.7% in out-of-set settings.

</details>


### [206] [Evaluating Prediction Uncertainty Estimates from BatchEnsemble](https://arxiv.org/abs/2601.21581)
*Morten Blørstad,Herman Jangsett Mostein,Nello Blaser,Pekka Parviainen*

Main category: cs.LG

TL;DR: BatchEnsemble 延展到序列建模（GRUBE），在不增加显著参数和推理成本的前提下，对比并达到等于深度集成的不确定性估计性能，明显优于 MC dropout。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性估计方法要么计算成本高、要么对不确定性估计偏低；需要一种通用、可扩展的方法，在表格数据和时序任务中都能提供可靠的不确定性估计。

Method: 提出 GRUBE：将 BatchEnsemble 应用于 GRU 单元，扩展到序列建模，并与蒙特卡罗 dropout 以及深度集成模型进行对比评估。覆盖表格与时间序列任务，评估预测性能与不确定性估计质量。

Result: BatchEnsemble 在不确定性估计上与深度集成相当，明显优于 MC dropout；GRUBE 的预测与不确定性估计表现接近或优于基线；相比传统集成，参数更少、训练与推理时间更短。

Conclusion: BatchEnsemble 与 GRUBE 提供可扩展且高效的不确定性估计方案，在表格与时序任务上具有竞争力，可作为深度集成的高效替代。

Abstract: Deep learning models struggle with uncertainty estimation. Many approaches are either computationally infeasible or underestimate uncertainty. We investigate \textit{BatchEnsemble} as a general and scalable method for uncertainty estimation across both tabular and time series tasks. To extend BatchEnsemble to sequential modeling, we introduce GRUBE, a novel BatchEnsemble GRU cell. We compare the BatchEnsemble to Monte Carlo dropout and deep ensemble models. Our results show that BatchEnsemble matches the uncertainty estimation performance of deep ensembles, and clearly outperforms Monte Carlo dropout. GRUBE achieves similar or better performance in both prediction and uncertainty estimation. These findings show that BatchEnsemble and GRUBE achieve similar performance with fewer parameters and reduced training and inference time compared to traditional ensembles.

</details>


### [207] [CORDS: Continuous Representations of Discrete Structures](https://arxiv.org/abs/2601.21583)
*Tin Hadži Veljković,Erik Bekkers,Michael Tiemann,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 提出CORDS：将可变大小集合预测转化为连续推理，通过可逆映射将集合映射到密度场与特征场，端到端训练并可精确解码回离散集合。


<details>
  <summary>Details</summary>
Motivation: 许多学习任务需要预测未知基数的集合，现有方法依赖填充或显式推断集合大小，往往在效率与准确性上受限。

Method: 引入可逆映射，将集合离散结构映射到连续场：密度场用于对象位置与计数，特征场承载对象属性，支撑在同一域上。模型在场空间进行预测，映射可逆，确保输出可解码为离散集合。

Result: 在分子生成与回归、目标检测、基于仿真的推断以及局部极大值恢复等任务上评估，展现对未知集合大小的鲁棒性与具有竞争力的准确性。

Conclusion: 通过连续场表示实现对变量大小集合的端到端学习，且具备完全可解码性，适用广泛，未来可扩展以提升效率与规模。

Abstract: Many learning problems require predicting sets of objects when the number of objects is not known beforehand. Examples include object detection, molecular modeling, and scientific inference tasks such as astrophysical source detection. Existing methods often rely on padded representations or must explicitly infer the set size, which often poses challenges. We present a novel strategy for addressing this challenge by casting prediction of variable-sized sets as a continuous inference problem. Our approach, CORDS (Continuous Representations of Discrete Structures), provides an invertible mapping that transforms a set of spatial objects into continuous fields: a density field that encodes object locations and count, and a feature field that carries their attributes over the same support. Because the mapping is invertible, models operate entirely in field space while remaining exactly decodable to discrete sets. We evaluate CORDS across molecular generation and regression, object detection, simulation-based inference, and a mathematical task involving recovery of local maxima, demonstrating robust handling of unknown set sizes with competitive accuracy.

</details>


### [208] [Heterogeneity-Aware Knowledge Sharing for Graph Federated Learning](https://arxiv.org/abs/2601.21589)
*Wentao Yu,Sheng Wan,Shuo Chen,Bo Han,Chen Gong*

Main category: cs.LG

TL;DR: FedSSA 提出在图形联邦学习中进行语义与结构对齐以应对客户端异质性：通过变分模型推断类别级节点分布进行语义对齐；通过谱能量衡量捕捉结构信息并进行谱对齐。基于聚类的局部与簇级模型实现知识共享。实验证明在六个同质和五个异质数据集、非重叠/重叠分区设置下优于11个SOTA方法。


<details>
  <summary>Details</summary>
Motivation: GFL 面临来自多客户端的节点特征和结构拓扑的异质性，导致模型难以高效聚合和知识共享。现有方法往往只能部分解决某一类异质性，需同时处理特征与结构两类异质性。

Method: 对节点特征异质性：提出变分模型以推断类级节点分布，基于此聚类客户端并构建簇级代表分布；最小化局部分布与簇级分布之间的散度以实现语义知识共享。对结构异质性：使用谱GNN并提出谱能量衡量来刻画结构信息，基于谱能量对客户端聚类，构建簇级谱GNN；再将局部谱GNN的谱特征对齐到簇级谱GNN，以实现结构知识共享。

Result: 在六个同质数据集与五个异质数据集上，以及非重叠与重叠分区设置中，FedSSA 始终优于11种最先进方法。

Conclusion: FedSSA 能同时缓解特征与结构两种异质性，通过语义与结构的对齐实现跨客户端的知识分享，在不同数据类型与分区情景下具有鲁棒性和良好性能。

Abstract: Graph Federated Learning (GFL) enables distributed graph representation learning while protecting the privacy of graph data. However, GFL suffers from heterogeneity arising from diverse node features and structural topologies across multiple clients. To address both types of heterogeneity, we propose a novel graph Federated learning method via Semantic and Structural Alignment (FedSSA), which shares the knowledge of both node features and structural topologies. For node feature heterogeneity, we propose a novel variational model to infer class-wise node distributions, so that we can cluster clients based on inferred distributions and construct cluster-level representative distributions. We then minimize the divergence between local and cluster-level distributions to facilitate semantic knowledge sharing. For structural heterogeneity, we employ spectral Graph Neural Networks (GNNs) and propose a spectral energy measure to characterize structural information, so that we can cluster clients based on spectral energy and build cluster-level spectral GNNs. We then align the spectral characteristics of local spectral GNNs with those of cluster-level spectral GNNs to enable structural knowledge sharing. Experiments on six homophilic and five heterophilic graph datasets under both non-overlapping and overlapping partitioning settings demonstrate that FedSSA consistently outperforms eleven state-of-the-art methods.

</details>


### [209] [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: 提出一种训练无须MCMC、验证器的分布锐化方法，通过在标记级别引入低温缩放近似全局幂分布，从而在不依赖外部奖励的情况下提升LLM推理能力，并显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: RL后训练对LLM推理能力的提升，更多来自分布锐化而非新能力，同时MCMC成本高，难以大规模应用，需要一个训练-free、verifier-free的替代方案。

Method: 理论推导将全局幂分布近似为标记级别的低温缩放分布，缩放系数反映未来轨迹质量；提出一个训练-free、verifier-free的自回归分布锐化算法，直接在推理过程对基模型进行分布锐化。

Result: 在数学、问答、代码任务上对四个LLM进行评估，方法达到或超过单-shot GRPO的表现，且无需外部奖励；推理延迟比基于MCMC的采样低十倍以上。

Conclusion: 提供一种理论与实验上可行的高效替代MCMC的分布锐化路径，适用于多任务LLM推理强化，显著提升效率。

Abstract: Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.

</details>


### [210] [Beyond Parameter Finetuning: Test-Time Representation Refinement for Node Classification](https://arxiv.org/abs/2601.21615)
*Jiaxin Zhang,Yiqi Wang,Siwei Wang,Xihong Yang,Yu Shi,Xinwang Liu,En Zhu*

Main category: cs.LG

TL;DR: TTReFT: 将自适应从模型参数转向潜在表示的图 TTT 框架，通过不确定性引导的节点选择、低秩表示干预以及针对干预的自编码掩蔽来实现，具有理论保证并在五个基准数据集上取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 在 OOD 场景下，GNN 的性能显著下降且现有 PaFT 容易造成灾难性遗忘，需一种能在测试时稳健适应且保护已学知识的策略。

Method: 提出 TTReFT：1) 不确定性引导的节点选择进行干预；2) 低秩表示干预以保持原有知识；3) 干预感知的掩蔽自编码器，动态调整掩蔽策略以适应选择的节点。理论给出 OOD 场景的 guarantees；在五个基准数据集上进行广泛实验。

Result: 实验结果显示 TTReFT 在五个基准数据集上具有一致且优越的性能，优于参数微调方法，验证了 representation finetuning 的有效性。理论分析支撑在 OOD 设置的保证。

Conclusion: 将表示微调确立为图 TTT 的新范式，兼具理论基础与实时实用性。

Abstract: Graph Neural Networks frequently exhibit significant performance degradation in the out-of-distribution test scenario. While test-time training (TTT) offers a promising solution, existing Parameter Finetuning (PaFT) paradigm suffer from catastrophic forgetting, hindering their real-world applicability. We propose TTReFT, a novel Test-Time Representation FineTuning framework that transitions the adaptation target from model parameters to latent representations. Specifically, TTReFT achieves this through three key innovations: (1) uncertainty-guided node selection for specific interventions, (2) low-rank representation interventions that preserve pre-trained knowledge, and (3) an intervention-aware masked autoencoder that dynamically adjust masking strategy to accommodate the node selection scheme. Theoretically, we establish guarantees for TTReFT in OOD settings. Empirically, extensive experiments across five benchmark datasets demonstrate that TTReFT achieves consistent and superior performance. Our work establishes representation finetuning as a new paradigm for graph TTT, offering both theoretical grounding and immediate practical utility for real-world deployment.

</details>


### [211] [LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models](https://arxiv.org/abs/2601.21623)
*Stanislav Budzinskiy,Marian Gloser,Tolunay Yilmaz,Ying Hong Tham,Yuanyi Lin,Wenyi Fang,Fan Wu,Philipp Petersen*

Main category: cs.LG

TL;DR: 提出一种自适应混合精度策略，用于 transformer 推理中对复合函数 f(g(x)) 的误差控制。通过只对 g(x) 的少数组成部分进行高精度计算，其余部分以低精度执行，从而在低重计算比下显著提升推理精度，实验在 GPT-2 上取得高达两数量级的精度提升。


<details>
  <summary>Details</summary>
Motivation: 在大模型推理中，混合精度是提升效率的关键，但不同组成部分的舍入误差对最终输出的影响不同。针对复合函数的误差传播，提出按需要对 g(x) 的部分分段提升精度的策略，以在限制资源下提高推理准确性。

Method: 基于 f(g(x)) 的舍入误差分析，提出自适应策略，选择对某些子组件需要高精度计算、其余组件以低精度执行。将该策略应用于变换器内部的不同组合（如注意力、前馈、残差等）的推理过程，并给出实现路径。随后在 GPT-2 模型上进行数值实证，比较不同重计算率下的准确性提升。

Result: 实验显示在极低的重计算率下即可实现高达两数量级的准确性提升，证实了对关键组成部分进行有限次高精度重算对整个推理过程的显著影响。

Conclusion: 该自适应混合精度策略为 transformer 推理提供了一种在保持资源约束同时提升准确性的路径，尤其在 composition-rich 的函数场景中具有潜在普适性，适用于其他序列模型与大模型的推理优化，同时为进一步的鲁棒性分析和硬件实现奠定基础。

Abstract: Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.

</details>


### [212] [Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation](https://arxiv.org/abs/2601.21636)
*Jan Schuchardt,Nikita Kalinin*

Main category: cs.LG

TL;DR: 提出无采样的隐私放大界限，用于差分隐私矩阵分解下的随机分配场景，基于Rényi散度和条件组合，避免Monte Carlo采样的局限并可广泛应用于带状与非带状矩阵。


<details>
  <summary>Details</summary>
Motivation: 现有工作用蒙特卡洛采样估计放大参数，但要么只在高概率意义下成立、要么需要随机拒绝机制，且对ε-δ的要求使采样样本数随δ成反比，存在效率与鲁棒性问题。因此需无采样的严谨界限以用于训练中的矩阵分解隐私保护。

Method: 提出基于Rényi散度的无采样界限以及基于条件组合的另一组界限。通过动态规划实现Rényi界限的高效计算；在ε较小时，条件组合提供更强的隐私保证，避免Rényi过度估计；适用于任意带状和非带状矩阵的矩阵机制。

Result: 通过数值比较，所提界限在大量研究与实践中使用的多种矩阵机制上表现出良好性能与鲁棒性，且显著克服了对δ的依赖带来的样本量需求问题。

Conclusion: 给出一个统一的、无采样的放大界限框架，结合Rényi散度与条件组合的互补性，适用范围广且可实际用于差分隐私训练中的矩阵分解任务。

Abstract: We study privacy amplification for differentially private model training with matrix factorization under random allocation (also known as the balls-in-bins model). Recent work by Choquette-Choo et al. (2025) proposes a sampling-based Monte Carlo approach to compute amplification parameters in this setting. However, their guarantees either only hold with some high probability or require random abstention by the mechanism. Furthermore, the required number of samples for ensuring $(ε,δ)$-DP is inversely proportional to $δ$. In contrast, we develop sampling-free bounds based on Rényi divergence and conditional composition. The former is facilitated by a dynamic programming formulation to efficiently compute the bounds. The latter complements it by offering stronger privacy guarantees for small $ε$, where Rényi divergence bounds inherently lead to an over-approximation. Our framework applies to arbitrary banded and non-banded matrices. Through numerical comparisons, we demonstrate the efficacy of our approach across a broad range of matrix mechanisms used in research and practice.

</details>


### [213] [Seg-MoE: Multi-Resolution Segment-wise Mixture-of-Experts for Time Series Forecasting Transformers](https://arxiv.org/abs/2601.21641)
*Evandro S. Ortigossa,Eran Segal*

Main category: cs.LG

TL;DR: Segment-level routing in a Mixture-of-Experts for time-series transformers yields state-of-the-art long-term forecasting by routing contiguous time-step segments rather than independent tokens.


<details>
  <summary>Details</summary>
Motivation: Token-wise MoE for time-series overlooks inherent locality and continuity; exploiting segment-level structure can provide a stronger inductive bias for temporal data and improve scalability and accuracy.

Method: Introduce Seg-MoE: a sparse MoE design that routes and processes contiguous time-step segments. Integrate Seg-MoE layers into a time-series Transformer and evaluate on multivariate long-term forecasting benchmarks with ablation studies.

Result: Seg-MoE achieves state-of-the-art forecasting accuracy across almost all horizons, outperforming dense Transformers and prior token-wise MoE models; segment-level routing is identified as the key driver of gains.

Conclusion: Aligning MoE routing granularity with the inherent structure of time series provides a powerful inductive bias for sequential data modeling and enables effective conditionally sparse architectures in forecasting.

Abstract: Transformer-based models have recently made significant advances in accurate time-series forecasting, but even these architectures struggle to scale efficiently while capturing long-term temporal dynamics. Mixture-of-Experts (MoE) layers are a proven solution to scaling problems in natural language processing. However, existing MoE approaches for time-series forecasting rely on token-wise routing mechanisms, which may fail to exploit the natural locality and continuity of temporal data. In this work, we introduce Seg-MoE, a sparse MoE design that routes and processes contiguous time-step segments rather than making independent expert decisions. Token segments allow each expert to model intra-segment interactions directly, naturally aligning with inherent temporal patterns. We integrate Seg-MoE layers into a time-series Transformer and evaluate it on multiple multivariate long-term forecasting benchmarks. Seg-MoE consistently achieves state-of-the-art forecasting accuracy across almost all prediction horizons, outperforming both dense Transformers and prior token-wise MoE models. Comprehensive ablation studies confirm that segment-level routing is the key factor driving these gains. Our results show that aligning the MoE routing granularity with the inherent structure of time series provides a powerful, yet previously underexplored, inductive bias, opening new avenues for conditionally sparse architectures in sequential data modeling.

</details>


### [214] [Gauge-invariant representation holonomy](https://arxiv.org/abs/2601.21653)
*Vasileios Sevetlidis,George Pavlidis*

Main category: cs.LG

TL;DR: 提出 representation holonomy，一种 gauge 不变的统计量，用于衡量输入空间小环环路上特征平行传输时的“扭曲”程度，以捕捉表示几何的路径依赖性，弥补仅看点对点相似性的不足。


<details>
  <summary>Details</summary>
Motivation: 现有相似性度量（如 CKA、SVCCA）关注激活集合的点对点重叠，忽略表示随输入路径的几何变化。不同模型在点对点度量上相近，可能对扰动或对抗攻击具有本质不同的鲁棒性。需要一个对齐旋转等变换下保持稳定、可解释的几何量来揭示隐藏的曲率与路径依赖。

Method: 定义 holonomy 为在输入空间围绕一个小环平行传输后，特征之间的“扭曲”累积量。通过全局 whitening 固定量纲和规范、使用共享子空间与仅旋转的 Procrustes 对邻域进行对齐，并将结果嵌回到全特征空间。证明对正交和仿射变换（post-whitening）不变；在仿射层存在线性零点；在小半径时 holonomy 近似为零。

Result: 实证结果表明：1) holonomy 随环半径增大而增大；2) 能将在 CKA 下看似相似的模型区分开来；3) 与对抗鲁棒性和腐蚀鲁棒性呈相关；4) 能追踪训练中特征形成与稳定化的动态。

Conclusion: representation holonomy 提供一个实用且可扩展的诊断工具，用于在点对点相似性之外探测学习表征的几何结构，补充现有度量的不足。

Abstract: Deep networks learn internal representations whose geometry--how features bend, rotate, and evolve--affects both generalization and robustness. Existing similarity measures such as CKA or SVCCA capture pointwise overlap between activation sets, but miss how representations change along input paths. Two models may appear nearly identical under these metrics yet respond very differently to perturbations or adversarial stress. We introduce representation holonomy, a gauge-invariant statistic that measures this path dependence. Conceptually, holonomy quantifies the "twist" accumulated when features are parallel-transported around a small loop in input space: flat representations yield zero holonomy, while nonzero values reveal hidden curvature. Our estimator fixes gauge through global whitening, aligns neighborhoods using shared subspaces and rotation-only Procrustes, and embeds the result back to the full feature space. We prove invariance to orthogonal (and affine, post-whitening) transformations, establish a linear null for affine layers, and show that holonomy vanishes at small radii. Empirically, holonomy increases with loop radius, separates models that appear similar under CKA, and correlates with adversarial and corruption robustness. It also tracks training dynamics as features form and stabilize. Together, these results position representation holonomy as a practical and scalable diagnostic for probing the geometric structure of learned representations beyond pointwise similarity.

</details>


### [215] [Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling](https://arxiv.org/abs/2601.21669)
*Abhijeet Sinha,Sundari Elango,Dianbo Liu*

Main category: cs.LG

TL;DR: 提出逆概率缩放（IPS）来纠正期望回报目标导致的结果级别模态崩溃，引入 IPS-GRPO 作为 GRPO 的无架构修改，在多任务（推理与分子生成）中减少模态崩溃并保持或超过基线性能。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中存在多种高质量解的终点，但以期望回报最大化为目标的学习往往使策略坍塌至少数结果。仅凭探索或正则化不足以解决该现象，需从目标函数层面进行修正。

Method: 在理想学习动力学下，任意两个结果的对数概率比随奖励差线性演化，导致概率分布自发地指数分化并不可避免地崩溃，与探索策略、熵正则或优化算法无关。问题源自期望中的概率乘子。提出逆概率缩放以消除学习信号中的结果频率放大，从而改变学习动力学，使终端分布与奖励成比例，理论上可防止崩溃。将该原则实现为 Group Relative Policy Optimization (GRPO) 的无缝修改 IPS-GRPO，无需额外模型或结构改动。

Result: IPS-GRPO 在多种推理与分子生成任务中显著减少结果级别的模式崩溃，并且表现与基线相比不劣，甚至在某些场景超越基线。

Conclusion: 纠正优化目标（而非简单增加探索）对实现鲁棒的多模态策略至关重要，IPS-GRPO 提供一种无额外负担的通用解决方案以避免结果级别的崩溃。

Abstract: Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.

</details>


### [216] [LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics](https://arxiv.org/abs/2601.21681)
*Qisong Xiao,Xinhai Chen,Qinglin Wang,Xiaowei Guo,Binglin Wang,Weifeng Chen,Zhichao Wang,Yunfei Liu,Rui Xia,Hang Zou,Gencheng Liu,Shuai Li,Jie Liu*

Main category: cs.LG

TL;DR: 将高维流场通过 physics-informed disentanglement 的降维保留关键结构，并用预训练的大语言模型(LLM)作为时间处理器进行自回归预测，通过专门的模态对齐实现跨模态稳定性，以实现无需再训练的通用流体动力学求解器，达到SOTA并具备零-shot/上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的时空流动建模在对未见流场的泛化性方面受限，且一般需要针对新场景重新训练。需要一种能在不重新训练的情况下对多种流动场进行高效、鲁棒预测的方法。

Method: 1) 将高维流场通过受物理信息约束的解耦机制进行降维，降低空间特征的纠缠，同时保留关键流结构；2) 采用预训练的LLM作为时间处理器，基于时间序列提示进行自回归预测；3) 提出模态对齐策略，弥合提示序列与物理序列之间的表征差异，提升长期预测稳定性。

Result: 在多样化的流动场场景下，LLM4Fluid无需重新训练即可实现鲁棒且泛化性强的求解，达到或接近当前方法的SOTA，并展现出显著的零-shot和上下文学习能力，且公开代码与数据集。

Conclusion: 表明将LLM作为通用求解器用于时空流体动力学是可行且有效的，且通过物理信息驱动的降维与模态对齐可以实现跨场景的鲁棒预测与良好泛化性。

Abstract: Deep learning has emerged as a promising paradigm for spatio-temporal modeling of fluid dynamics. However, existing approaches often suffer from limited generalization to unseen flow conditions and typically require retraining when applied to new scenarios. In this paper, we present LLM4Fluid, a spatio-temporal prediction framework that leverages Large Language Models (LLMs) as generalizable neural solvers for fluid dynamics. The framework first compresses high-dimensional flow fields into a compact latent space via reduced-order modeling enhanced with a physics-informed disentanglement mechanism, effectively mitigating spatial feature entanglement while preserving essential flow structures. A pretrained LLM then serves as a temporal processor, autoregressively predicting the dynamics of physical sequences with time series prompts. To bridge the modality gap between prompts and physical sequences, which can otherwise degrade prediction accuracy, we propose a dedicated modality alignment strategy that resolves representational mismatch and stabilizes long-term prediction. Extensive experiments across diverse flow scenarios demonstrate that LLM4Fluid functions as a robust and generalizable neural solver without retraining, achieving state-of-the-art accuracy while exhibiting powerful zero-shot and in-context learning capabilities. Code and datasets are publicly available at https://github.com/qisongxiao/LLM4Fluid.

</details>


### [217] [Don't be so Stief! Learning KV Cache low-rank approximation over the Stiefel manifold](https://arxiv.org/abs/2601.21686)
*Luca Benfenati,Matteo Risso,Andrea Vannozzi,Ahmet Caner Yüzügüler,Lukas Cavigelli,Enrico Macii,Daniele Jahier Pagliari,Alessio Burrello*

Main category: cs.LG

TL;DR: StiefAttention introduces post-training KV-cache compression via orthonormal projections optimized for end-to-end decoder output reconstruction, with per-layer error-rank profiling; it outperforms EigenAttention on Llama3-8B under iso-compression.


<details>
  <summary>Details</summary>
Motivation: KV caching accelerates autoregressive decoding but long-context K/V caches strain HBM capacity and bandwidth. Existing post-training projection methods (SVD-style) may not capture end-to-end effects after softmax, value mixing, and layer transforms, leading to suboptimal reconstruction.

Method: Learn orthonormal projection bases by directly minimizing decoder-layer output reconstruction error. For each layer, precompute an error-rank profile over candidate ranks to enable flexible, budget-constrained layer-wise rank allocation.

Result: Under the same conditions on Llama3-8B, StiefAttention outperforms EigenAttention by 11.9 points on C4 perplexity and 5.4% on 0-shot MMLU accuracy at iso-compression, with lower relative error and higher cosine similarity to the original decoder-layer outputs.

Conclusion: End-to-end objective-driven KV-cache compression with per-layer rank budgeting improves reconstruction quality and downstream task performance at fixed compression, surpassing prior post-training methods like EigenAttention.

Abstract: Key--value (KV) caching enables fast autoregressive decoding but at long contexts becomes a dominant bottleneck in High Bandwidth Memory (HBM) capacity and bandwidth. A common mitigation is to compress cached keys and values by projecting per-head matrixes to a lower rank, storing only the projections in the HBM. However, existing post-training approaches typically fit these projections using SVD-style proxy objectives, which may poorly reflect end-to-end reconstruction after softmax, value mixing, and subsequent decoder-layer transformations.
  For these reasons, we introduce StiefAttention, a post-training KV-cache compression method that learns \emph{orthonormal} projection bases by directly minimizing \emph{decoder-layer output reconstruction error}. StiefAttention additionally precomputes, for each layer, an error-rank profile over candidate ranks, enabling flexible layer-wise rank allocation under a user-specified error budget. Noteworthy, on Llama3-8B under the same conditions, StiefAttention outperforms EigenAttention by $11.9$ points on C4 perplexity and $5.4\%$ on 0-shot MMLU accuracy at iso-compression, yielding lower relative error and higher cosine similarity with respect to the original decoder-layer outputs.

</details>


### [218] [XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision](https://arxiv.org/abs/2601.21688)
*Alexandre Myara,Nicolas Bourriez,Thomas Boyer,Thomas Lemercier,Ihab Bendidi,Auguste Genovesio*

Main category: cs.LG

TL;DR: XFactors 是一个弱监督的变分自编码（VAE）框架，通过对潜在表示分解成通用子空间 S 和针对性子空间 T_i，并用 InfoNCE 对比损失对 T_i 内的因子进行对齐，从而实现因子解耦、可控替换，且无需对抗训练，达到多数据集的 SOTA 解耦分数，并在 CelebA 上有良好表现，代码开放。


<details>
  <summary>Details</summary>
Motivation: 无监督方法在真实数据上难以恢复语义因子，受限于偏置；监督方法对大属性集的扩展性差且不稳定，往往依赖对抗性目标或辅助分类器。

Method: 将表示分解为子空间 S 与若干因子子空间 T_1...T_K，每个目标因子编码到对应的 T_i。通过 InfoNCE 对比损失将具有相同因子取值的潜在向量聚集、不同取值的向量分离。同时对 S 与各聚合后的因子子空间施加高斯结构约束（KL 正则化），在不对非目标因子提供额外监督的情况下组织几何结构，避免对抗训练和分类器。实验上在多数据集上使用固定的超参数，能达到SOTA分解分数，并实现可控的因子替换；也评估了 Latent capacity 增加时的可扩展性，并在 CelebA 上验证。

Result: 在多数据集上达到 SOTA 解耦分数，潜在子空间对齐稳定，支持通过潜在替换进行因子交换；对隐变量容量增加具有良好扩展性；CelebA 上有实证支持；代码公开。

Conclusion: 提出的 XFactors 提供对所选因子的显式控制，避免对抗训练与分类器，具可扩展性并能实现因子替换，适用于现实数据集并在公开实现上具实用性。

Abstract: Disentangled representation learning aims to map independent factors of variation to independent representation components. On one hand, purely unsupervised approaches have proven successful on fully disentangled synthetic data, but fail to recover semantic factors from real data without strong inductive biases. On the other hand, supervised approaches are unstable and hard to scale to large attribute sets because they rely on adversarial objectives or auxiliary classifiers.
  We introduce \textsc{XFactors}, a weakly-supervised VAE framework that disentangles and provides explicit control over a chosen set of factors. Building on the Disentangled Information Bottleneck perspective, we decompose the representation into a residual subspace $\mathcal{S}$ and factor-specific subspaces $\mathcal{T}_1,\ldots,\mathcal{T}_K$ and a residual subspace $\mathcal{S}$. Each target factor is encoded in its assigned $\mathcal{T}_i$ through contrastive supervision: an InfoNCE loss pulls together latents sharing the same factor value and pushes apart mismatched pairs. In parallel, KL regularization imposes a Gaussian structure on both $\mathcal{S}$ and the aggregated factor subspaces, organizing the geometry without additional supervision for non-targeted factors and avoiding adversarial training and classifiers.
  Across multiple datasets, with constant hyperparameters, \textsc{XFactors} achieves state-of-the-art disentanglement scores and yields consistent qualitative factor alignment in the corresponding subspaces, enabling controlled factor swapping via latent replacement. We further demonstrate that our method scales correctly with increasing latent capacity and evaluate it on the real-world dataset CelebA. Our code is available at \href{https://github.com/ICML26-anon/XFactors}{github.com/ICML26-anon/XFactors}.

</details>


### [219] [Understanding Model Merging: A Unified Generalization Framework for Heterogeneous Experts](https://arxiv.org/abs/2601.21690)
*Qinglun Li,Anke Tang,Miao Zhang,Mengzhu Wang,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: 提出基于L2稳定性在异构超参数下的理论框架，解释模型合并的泛化性并给出可操作的配置建议，同时在多任务/多模型上进行了广泛实验以验证理论。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并在参数空间聚合能力强、无需原始数据且不需高成本重计算。然而，关于在异构微调超参数（如学习率、批量大小等）情况下的有效性尚缺乏统一理论解释。此外，开源微调模型缺乏超参数透明度，导致难以预测合并模型的性能，也就难以给出如何选择/微调“合并友好”的专家模型的实用指导。

Method: 在异构超参数环境下，基于L2稳定性理论分析合并后模型x_avg的泛化性，建立统一的理论框架，揭示现有合并算法在界中的具体项的优化方式，从而为观测提供理论支撑；基于该框架提出面向实践的可操作性建议，帮助在预训练到微调的管线中构造更利于合并的专家模型。

Result: 在ResNet/Vit家族的20/8个视觉分类任务中对成千上万的微调模型进行广泛实验，结果稳健地证实了不同超参数对x_avg泛化性的影响，这些影响与理论结果相符，验证了框架的预测能力。

Conclusion: 提供了一个统一的理论框架来解释现有合并算法的效果，并给出可操作的实践建议，帮助研究者和从业者在多源微调模型合并场景中更好地理解与优化超参数对合并泛化性的影响。

Abstract: Model merging efficiently aggregates capabilities from multiple fine-tuned models into a single one, operating purely in parameter space without original data or expensive re-computation. Despite empirical successes, a unified theory for its effectiveness under heterogeneous finetuning hyperparameters (e.g., varying learning rates, batch sizes) remains missing. Moreover, the lack of hyperparameter transparency in open-source fine-tuned models makes it difficult to predict merged-model performance, leaving practitioners without guidance on how to fine-tune merge-friendly experts. To address those two challenges, we employ $L_2$-Stability theory under heterogeneous hyperparameter environments to analyze the generalization of the merged model $\boldsymbol{x}_{avg}$. This pioneering analysis yields two key contributions: (i) \textit{A unified theoretical framework} is provided to explain existing merging algorithms, revealing how they optimize specific terms in our bound, thus offering a strong theoretical foundation for empirical observations. (ii) \textit{Actionable recommendations} are proposed for practitioners to strategically fine-tune expert models, enabling the construction of merge-friendly models within the pretraining-to-finetuning pipeline. Extensive experiments on the ResNet/Vit family across 20/8 visual classification tasks, involving thousands of finetuning models, robustly confirm the impact of different hyperparameters on the generalization of $\boldsymbol{x}_{avg}$ predicted by our theoretical results.

</details>


### [220] [Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics](https://arxiv.org/abs/2601.21698)
*Mohamed Elgaar,Hadi Amiri*

Main category: cs.LG

TL;DR: Curricula largely stabilizes within-phase optimization rather than creating new training phases; small models gain from reduced gradient noise and spectral saturation, while gains shrink at larger scales.


<details>
  <summary>Details</summary>
Motivation: Investigate whether curriculum learning changes learning trajectories or mainly reshapes data exposure within a fixed trajectory across model scales.

Method: Train Pythia models (14M–410M) to 300B tokens under three linguistically motivated curricula (Age-of-Aquisition, Word Frequency, Verb Variation) and compare with Random ordering; at 1B params compare Random vs VV. Analyze phase structure, gradient noise, output-head spectral saturation, final accuracy; formalize via gradient-variance control.

Result: Across orderings, training follows a shared latent phase sequence; curricula mainly alter within-phase data exposure. In smaller models, Random ordering has higher gradient noise and stronger late-training saturation with lower final accuracy; curricula mitigate these effects at matched compute. At larger scales, saturation differences and gains diminish; an idealized analysis links difficulty pacing to optimization stability.

Conclusion: Curricula primarily stabilize within-phase optimization rather than creating new phases; practical takeaway is to use curriculum strategies to stabilize optimization during within-phase training, with larger benefits for smaller models and diminishing returns at very large scales.

Abstract: Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquisition, word frequency, and Verb Variation (VV)-and compare each against Random ordering; at 1B parameters we compare Random and VV. Across orderings, training follows a shared sequence of latent phases, while curricula mainly change within-phase data exposure. In smaller models (up to 160M parameters), Random ordering exhibits higher gradient noise and stronger late-training output-head spectral saturation, alongside lower final accuracy; curricula reduce both effects at matched compute. At larger scales, saturation differences are smaller and curriculum gains shrink. We formalize the link between difficulty pacing and optimization stability in an idealized analysis based on gradient-variance control, and our results point to a practical takeaway: curricula help by stabilizing within-phase optimization rather than by creating new phases.

</details>


### [221] [SmartMeterFM: Unifying Smart Meter Data Generative Tasks Using Flow Matching Models](https://arxiv.org/abs/2601.21706)
*Nan Lin,Yanbo Wang,Jacco Heres,Peter Palensky,Pedro P. Vergara*

Main category: cs.LG

TL;DR: 单一的流式匹配条件生成模型，统一处理智能表计数据的多种生成任务（合成数据、缺失数据插补、超分辨率），通过对不同观测形式进行条件注入实现任务无重训练的统一框架。


<details>
  <summary>Details</summary>
Motivation: 隐私法规限制与数据质量问题导致智能表计数据不可得或分辨率不足；为每个任务单独训练模型既低效又冗余。

Method: 将流式匹配模型用于条件生成，将不同的生成任务视为不同形式的部分观测，通过在生成过程注入这些观测来统一实现缺失数据插补、超分辨率和合成数据等任务，且在高维时间序列（15分钟粒度的月度数据）上进行训练。

Result: 生成的数据在与给定观测一致的同时具有较高的真实感，在插值和相关基线方法上表现更优。

Conclusion: 提出的统一、灵活的生成框架减少重复训练的需要，提升效率与泛化能力，可同时处理多种智能表计数据生成任务。

Abstract: Smart meter data is the foundation for planning and operating the distribution network. Unfortunately, such data are not always available due to privacy regulations. Meanwhile, the collected data may be corrupted due to sensor or transmission failure, or it may not have sufficient resolution for downstream tasks. A wide range of generative tasks is formulated to address these issues, including synthetic data generation, missing data imputation, and super-resolution. Despite the success of machine learning models on these tasks, dedicated models need to be designed and trained for each task, leading to redundancy and inefficiency. In this paper, by recognizing the powerful modeling capability of flow matching models, we propose a new approach to unify diverse smart meter data generative tasks with a single model trained for conditional generation. The proposed flow matching models are trained to generate challenging, high-dimensional time series data, specifically monthly smart meter data at a 15 min resolution. By viewing different generative tasks as distinct forms of partial data observations and injecting them into the generation process, we unify tasks such as imputation and super-resolution with a single model, eliminating the need for re-training. The data generated by our model not only are consistent with the given observations but also remain realistic, showing better performance against interpolation and other machine learning based baselines dedicated to the tasks.

</details>


### [222] [When does predictive inverse dynamics outperform behavior cloning?](https://arxiv.org/abs/2601.21718)
*Lukas Schäfer,Pallavi Choudhury,Abdelhak Lemkhenter,Chris Lovett,Somjit Nath,Luis França,Matheus Ribeiro Furtado de Mendonça,Alex Lamb,Riashat Islam,Siddhartha Sen,John Langford,Katja Hofmann,Sergio Valcarcel Macua*

Main category: cs.LG

TL;DR: PIDM introduces a bias-variance tradeoff: predicting a future state adds bias, but conditioning the inverse dynamics model (IDM) on that prediction can greatly reduce variance, yielding lower error and better sample efficiency than behavior cloning (BC) under suitable state-predictor bias. Empirically, PIDM needs substantially fewer demonstrations than BC in 2D navigation and high-dimensional 3D visual tasks.


<details>
  <summary>Details</summary>
Motivation: Behavior cloning struggles with limited expert data. PIDM, by coupling a future-state predictor with an IDM, may improve sample efficiency, but the underlying conditions and mechanisms are unclear.

Method: Theoretically analyze the bias-variance tradeoff in PIDM and derive conditions on the state predictor bias under which PIDM achieves lower prediction error and higher sample efficiency than BC. Empirically validate the theory in 2D navigation tasks and a complex 3D game with stochastic transitions.

Result: Under appropriate predictor bias, PIDM achieves lower prediction error and higher sample efficiency than BC. The advantage grows with more data sources. In 2D tasks, BC requires up to 5× (avg ~3×) more demonstrations than PIDM; in a high-dimensional 3D environment, BC requires >66% more samples than PIDM.

Conclusion: PIDM's bias-variance framework explains its data-efficiency gains over BC; when the state predictor bias falls within the derived regime, PIDM outperforms BC, with larger gains as data sources increase.

Abstract: Behavior cloning (BC) is a practical offline imitation learning method, but it often fails when expert demonstrations are limited. Recent works have introduced a class of architectures named predictive inverse dynamics models (PIDM) that combine a future state predictor with an inverse dynamics model (IDM). While PIDM often outperforms BC, the reasons behind its benefits remain unclear. In this paper, we provide a theoretical explanation: PIDM introduces a bias-variance tradeoff. While predicting the future state introduces bias, conditioning the IDM on the prediction can significantly reduce variance. We establish conditions on the state predictor bias for PIDM to achieve lower prediction error and higher sample efficiency than BC, with the gap widening when additional data sources are available. We validate the theoretical insights empirically in 2D navigation tasks, where BC requires up to five times (three times on average) more demonstrations than PIDM to reach comparable performance; and in a complex 3D environment in a modern video game with high-dimensional visual inputs and stochastic transitions, where BC requires over 66\% more samples than PIDM.

</details>


### [223] [LoRA and Privacy: When Random Projections Help (and When They Don't)](https://arxiv.org/abs/2601.21719)
*Yaxi Hu,Johanna Düngler,Bernhard Schölkopf,Amartya Sanyal*

Main category: cs.LG

TL;DR: Wishart投影机制在向量输出下可在无附加噪声的情况下实现非渐近的DP；对矩阵输出则在无噪声情形下并非DP，存在近乎完美的成员检测风险。引入带噪variant后可通过随机性与低秩投影实现隐私放大，胜过仅添加噪声的方案。把LoRA更新视为矩阵输出机制的一个实例，表明LoRA本身并非天生私有，但低秩微调在相同噪声水平下可能比全微调更具隐私性。初步实验表明更严格的隐私核算有助于降低所需噪声并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 研究Wishart随机映射产生的差分隐私特性，区分向量输出与矩阵输出的鲁棒性，探索在不添加显式噪声时的DP，以及在带噪声时的隐私放大效应，并将其与LoRA等现代模型更新方式联系起来。

Method: 对向量值查询和矩阵值查询的DP性质进行理论分析；给出噪声自由情形的负结果和近似极高的成员推断攻击；提出带噪声的变体并证明由于随机性与低秩投影导致的隐私放大；将LoRA更新归为矩阵输出机制的一个实例并讨论隐私含义；通过初步实验验证理论结论并探讨更紧的隐私核算对实务的影响。

Result: 向量输出在无附加噪声时实现非渐近DP；矩阵输出在噪声自由下不DP并出现高AUC的成员推断攻击；引入带噪声版本，利用随机性与低秩投影实现隐私放大，且在大/小秩两种情形均优于单纯添加噪声的隐私机制；LoRA更新可视为矩阵输出机制的一个实例，表明LoRA本身并非自带私有性，但低秩微调在同等噪声下可能比全微调更具隐私性；初步实验显示更严谨的隐私计量可实现更低噪声与更好性能。

Conclusion: 在特定场景下，Wishart投影机制可实现向量输出的DP，矩阵输出则需要带噪声并结合低秩结构以获得强隐私保障；LoRA与低秩微调的隐私关系提示架构设计对隐私有重要影响；实践中需依赖更精细的隐私核算与实证验证来实现更低噪声与更高准确性。

Abstract: We introduce the (Wishart) projection mechanism, a randomized map of the form $S \mapsto M f(S)$ with $M \sim W_d(1/r I_d, r)$ and study its differential privacy properties. For vector-valued queries $f$, we prove non-asymptotic DP guarantees without any additive noise, showing that Wishart randomness alone can suffice. For matrix-valued queries, however, we establish a sharp negative result: in the noise-free setting, the mechanism is not DP, and we demonstrate its vulnerability by implementing a near perfect membership inference attack (AUC $> 0.99$). We then analyze a noisy variant and prove privacy amplification due to randomness and low rank projection, in both large- and small-rank regimes, yielding stronger privacy guarantees than additive noise alone. Finally, we show that LoRA-style updates are an instance of the matrix-valued mechanism, implying that LoRA is not inherently private despite its built-in randomness, but that low-rank fine-tuning can be more private than full fine-tuning at the same noise level. Preliminary experiments suggest that tighter accounting enables lower noise and improved accuracy in practice.

</details>


### [224] [Amortized Spectral Kernel Discovery via Prior-Data Fitted Network](https://arxiv.org/abs/2601.21731)
*Kaustubh Sharma,Srijan Tiwari,Ojasva Nema,Parikshit Pareek*

Main category: cs.LG

TL;DR: 通过可解释的解耦注意力框架，从预训练 PFN 中提取显式谱密度及对应的平稳协方差核，利用 Bochner 定理实现快速且可解释的高斯过程核回归。


<details>
  <summary>Details</summary>
Motivation: PFN 提供高效的摊销推理，但缺乏透明的先验/核，限制在需要显式协方差模型的下游任务中的应用。

Method: 先对训练好的 PFN 进行机理分析，定位 attention 潜变量为关键信号；设计解码器将 PFN 潜变量映射到谱密度，并通过 Bochner 的定理得到平稳核；在单-realization 与多-realization 两种情形下分析谱 identifiability 的理论极限，给出在有多份函数样本时的一致性证明；实证上，解码器能恢复多峰谱混合，得到显式核，能在高斯过程回归中达到与 PFN 与基线优化相当的精度，同时只需一次前向传播，显著降低推断时间。

Result: 解码器成功重建复杂的多峰谱结构，产生可用于 GP 的显式核，达到与 PFN 和优化基线相当的表现，同时极大提升推断速度。

Conclusion: 利用 PFN 的解耦注意力，完成可解释的谱发现，获得显式、可用的协方差模型，兼具准确性与高效性，便于在需要显式核的下游任务中使用。

Abstract: Prior-Data Fitted Networks (PFNs) enable efficient amortized inference but lack transparent access to their learned priors and kernels. This opacity hinders their use in downstream tasks, such as surrogate-based optimization, that require explicit covariance models. We introduce an interpretability-driven framework for amortized spectral discovery from pre-trained PFNs with decoupled attention. We perform a mechanistic analysis on a trained PFN that identifies attention latent output as the key intermediary, linking observed function data to spectral structure. Building on this insight, we propose decoder architectures that map PFN latents to explicit spectral density estimates and corresponding stationary kernels via Bochner's theorem. We study this pipeline in both single-realization and multi-realization regimes, contextualizing theoretical limits on spectral identifiability and proving consistency when multiple function samples are available. Empirically, the proposed decoders recover complex multi-peak spectral mixtures and produce explicit kernels that support Gaussian process regression with accuracy comparable to PFNs and optimization-based baselines, while requiring only a single forward pass. This yields orders-of-magnitude reductions in inference time compared to optimization-based baselines.

</details>


### [225] [Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators](https://arxiv.org/abs/2601.21737)
*Rebecca Pelke,Joel Klein,Jose Cubero-Cascante,Nils Bosbach,Jan Moritz Joseph,Rainer Leupers*

Main category: cs.LG

TL;DR: 提出了一个面向计算在存储(CIM)架构的混合精度训练与编译框架，采用强化学习（RL）搜索量化配置，突破低于8位的限制以提升MVM效率。


<details>
  <summary>Details</summary>
Motivation: CIM在交叉阵列执行矩阵-向量乘法时，输入/单元位宽受限，导致需要较多计算周期且权重量难以单跨跨越单元格存储，现有编译器对低于8位的量化支持不足，亟需高效的量化策略以提升性能。

Method: 提出基于强化学习的策略，以在巨大的搜索空间中找到合适的混合精度量化配置，结合混合精度训练与编译流程，优化CIM架构中的MVM性能与延迟/准确率折中。

Result: 在最佳情况中，相较于现有最先进方案，该方法可实现最高约2.48×的加速，准确率损失仅为0.086%。

Conclusion: RL驱动的混合精度量化与编译框架有效提升CIM架构下的推理/训练效率，同时在保持可接受的准确率损失的前提下实现显著加速。

Abstract: Computing-in-Memory (CIM) accelerators are a promising solution for accelerating Machine Learning (ML) workloads, as they perform Matrix-Vector Multiplications (MVMs) on crossbar arrays directly in memory. Although the bit widths of the crossbar inputs and cells are very limited, most CIM compilers do not support quantization below 8 bit. As a result, a single MVM requires many compute cycles, and weights cannot be efficiently stored in a single crossbar cell. To address this problem, we propose a mixed-precision training and compilation framework for CIM architectures. The biggest challenge is the massive search space, that makes it difficult to find good quantization parameters. This is why we introduce a reinforcement learning-based strategy to find suitable quantization configurations that balance latency and accuracy. In the best case, our approach achieves up to a 2.48x speedup over existing state-of-the-art solutions, with an accuracy loss of only 0.086 %.

</details>


### [226] [Temporal Sepsis Modeling: a Fully Interpretable Relational Way](https://arxiv.org/abs/2601.21747)
*Vincent Lemaire,Nédra Meloulli,Pierre Jaquet*

Main category: cs.LG

TL;DR: 提出一个基于关系数据的可解释框架来预测脓毒症，通过将EMR时序数据视为多变量患者日志、利用命题化将数据展平为可解释特征，并以选择性朴素贝叶斯分类器预测，提供单变量、全局、局部和反事实四维解释。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在脓毒症早期预测中的可解释性不足以及对潜在亚表型缺乏关注的问题，寻求一种更透明且能揭示潜在患者亚群的新途径。

Method: 将EMR时序数据建模为关系数据模式；应用命题化技术（基于聚合/选择函数）构建可解释特征并将数据“展平”；使用选择性朴素贝叶斯分类器进行预测。

Result: 实验验证表明该关系化、具高度解释性的框架在解释性方面具有显著优势，能够实现单变量、全局、局部及反事实四层次的解释。

Conclusion: 提出一种新的关系型可解释框架用于脓毒症早期预测，强调极致可解释性并有助于揭示潜在亚表型。

Abstract: Sepsis remains one of the most complex and heterogeneous syndromes in intensive care, characterized by diverse physiological trajectories and variable responses to treatment. While deep learning models perform well in the early prediction of sepsis, they often lack interpretability and ignore latent patient sub-phenotypes. In this work, we propose a machine learning framework by opening up a new avenue for addressing this issue: a relational approach. Temporal data from electronic medical records (EMRs) are viewed as multivariate patient logs and represented in a relational data schema. Then, a propositionalisation technique (based on classic aggregation/selection functions from the field of relational data) is applied to construct interpretable features to "flatten" the data. Finally, the flattened data is classified using a selective naive Bayesian classifier. Experimental validation demonstrates the relevance of the suggested approach as well as its extreme interpretability. The interpretation is fourfold: univariate, global, local, and counterfactual.

</details>


### [227] [Differentiable Knapsack and Top-k Operators via Dynamic Programming](https://arxiv.org/abs/2601.21775)
*Germain Vivier-Ardisson,Michaël E. Sander,Axel Parmentier,Mathieu Blondel*

Main category: cs.LG

TL;DR: A differentiable DP-based framework for Knapsack and Top-k; smoothing recursions yields differentiable relaxations; efficient parallel forward/backward passes; theoretical results: Shannon entropy is the unique permutation-equivariant regularizer; regularizers inducing sparsity are characterized; demonstrated on decision-focused learning benchmark, constrained dynamic assortment RL, and discrete VAEs.


<details>
  <summary>Details</summary>
Motivation: Discrete subset selection operators (Knapsack, Top-k) are piecewise constant, yielding zero gradients and hindering integration into neural networks. A unified differentiable framework via dynamic-programming formulations and relaxations enables end-to-end learning.

Method: Cast Knapsack and Top-k as dynamic programs and derive differentiable relaxations by smoothing the underlying recurrences. Develop efficient parallel algorithms for both deterministic and stochastic forward passes and provide vector-Jacobian products for backpropagation. Theoretically show entropy regularization (Shannon) is the unique permutation-equivariant choice and characterize regularizers that induce sparse selections.

Result: Empirical demonstration across three domains: a decision-focused learning benchmark, a constrained dynamic assortment reinforcement learning problem, and an extension of discrete variational autoencoders, validating the practicality and effectiveness of the framework.

Conclusion: The work provides a principled, differentiable framework for discrete subset selection via Knapsack and Top-k operators, anchored by a theoretical result on entropy regularization and practical sparse-inducing regularizers, with scalable, differentiable training demonstrated on diverse tasks.

Abstract: Knapsack and Top-k operators are useful for selecting discrete subsets of variables. However, their integration into neural networks is challenging as they are piecewise constant, yielding gradients that are zero almost everywhere. In this paper, we propose a unified framework casting these operators as dynamic programs, and derive differentiable relaxations by smoothing the underlying recursions. On the algorithmic side, we develop efficient parallel algorithms supporting both deterministic and stochastic forward passes, and vector-Jacobian products for the backward pass. On the theoretical side, we prove that Shannon entropy is the unique regularization choice yielding permutation-equivariant operators, and characterize regularizers inducing sparse selections. Finally, on the experimental side, we demonstrate our framework on a decision-focused learning benchmark, a constrained dynamic assortment RL problem, and an extension of discrete VAEs.

</details>


### [228] [ECSEL: Explainable Classification via Signomial Equation Learning](https://arxiv.org/abs/2601.21789)
*Adia Lumadjeng,Ilker Birbil,Erman Acar*

Main category: cs.LG

TL;DR: ECSEL 通过学习紧凑的 signomial 表达式来构建一个闭式分类器，兼具可解释性与高效性；在符号回归基准上回归目标方程的能力强、计算量少，分类性能与主流模型相当，并能揭示数据偏差与支持反事实推理。


<details>
  <summary>Details</summary>
Motivation: 受符号回归基准普遍存在的紧凑 signomial 结构启发，旨在提供可解释且具有闭式表达的分类器；同时希望通过显式的方程形式实现对特征行为和决策边界的可分析性。

Method: 直接构建结构化的闭式表达式（signomial 方程）作为分类器与解释源；通过学习过程恢复目标方程的近似结构，同时保持可解释性与计算效率。

Result: 在标准符号回归基准上，ECSEL 能回收更多目标方程、所需计算显著降低；在常见机器学习基准上实现与现有模型相当的分类准确性且保持可解释性；并且在全局特征行为、决策边界分析、局部特征归因等方面满足若干理想属性；在基准数据集与真实案例（电子商务、欺诈检测）中，学习出的方程揭示数据偏差、支持反事实推理，并提供可操作的洞见。

Conclusion: ECSEL 提供一种可解释且高效的分类方法，利用 signomial 方程的闭式表达实现判别与解释的统一；在符号回归与现实任务中展现出可解释性、可分析性以及对数据偏差与反事实推理的支持，具有实际应用潜力。

Abstract: We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure. ECSEL directly constructs a structural, closed-form expression that serves as both a classifier and an explanation. On standard symbolic regression benchmarks, our method recovers a larger fraction of target equations than competing state-of-the-art approaches while requiring substantially less computation. Leveraging this efficiency, ECSEL achieves classification accuracy competitive with established machine learning models without sacrificing interpretability. Further, we show that ECSEL satisfies some desirable properties regarding global feature behavior, decision-boundary analysis, and local feature attributions. Experiments on benchmark datasets and two real-world case studies i.e., e-commerce and fraud detection, demonstrate that the learned equations expose dataset biases, support counterfactual reasoning, and yield actionable insights.

</details>


### [229] [Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models](https://arxiv.org/abs/2601.21794)
*Yejin Kim,Dongjun Hwang,Sungmin Cha,Junsuk Choe*

Main category: cs.LG

TL;DR: 提出 KVW，一种训练无关的知识向量削弱方法，直接干预全模型以实现大规模 LVLM 的高效遗忘，无需梯度更新。


<details>
  <summary>Details</summary>
Motivation: 应对大规模视觉-语言模型的隐私泄露与有害内容风险，现有基于梯度的无忘方法计算成本高，亟需高效替代。

Method: 在忘记集合上识别在模型输出生成时被激活的知识向量，逐步削弱它们的贡献，直接对全模型进行干预且不进行梯度计算。

Result: 在 MLLMU 和 CLEAR 基准上实现稳定的忘记-保留权衡，显著提升相较梯度基和 LoRA 基于无忘方法的计算效率。

Conclusion: KVW 提供一种高效的训练-free 遗忘机制，适用于大规模 LVLM 的隐私与安全需求，并降低了遗忘过程的计算成本。

Abstract: Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model's output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.

</details>


### [230] [Effective LoRA Adapter Routing using Task Representations](https://arxiv.org/abs/2601.21795)
*Akash Dhasade,Anne-Marie Kermarrec,Igor Pavlovic,Diana Petrescu,Rafael Pires,Mathis Randl,Martijn de Vos*

Main category: cs.LG

TL;DR: 提出 LORAUTER，通过任务嵌入在任务层级路由 LoRA 适配器，避免依赖适配器特征，能够在大规模适配池中高效扩展，在任务对齐时接近 Oracle 的性能，并在未见任务上实现 SOTA 表现，同时对极大且嘈杂的适配器池具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的 LoRA 路由多将查询映射到具体适配器，导致适配器数量增加时路由成本显著上升，且往往需要可训练数据。需更具扩展性的路由机制，且在未见任务上具有良好泛化。

Method: 提出基于任务表示的路由框架 LORAUTER。通过对小型验证集构建任务嵌入，将查询路由至任务表示而非单独的适配器特征；不需要对适配器进行额外训练数据；在任务层面进行路由，使得复杂度随任务数线性增长；并在大规模噪声适配器池（>1500）中验证鲁棒性。

Result: 在多任务实验中，LORAUTER 持续优于基线路由方法，在存在任务对齐时达到接近 Oracle 的性能（101.2%），对未见任务实现 +5.2 点提升；并且能扩展到超过 1500 个适配器的大规模噪声池中保持鲁棒性。

Conclusion: LORAUTER 提供了一种高效、可扩展且对未见任务具备良好泛化能力的 LoRA 路由框架，降低对适配器训练数据的依赖，适用于大规模适配器池的实际场景。

Abstract: Low-rank adaptation (LoRA) enables parameter efficient specialization of large language models (LLMs) through modular adapters, resulting in rapidly growing public adapter pools spanning diverse tasks. Effectively using these adapters requires routing: selecting and composing the appropriate adapters for a query. We introduce LORAUTER, a novel routing framework that selects and composes LoRA adapters using task representations rather than adapter characteristics. Unlike existing approaches that map queries directly to adapters, LORAUTER routes queries via task embeddings derived from small validation sets and does not require adapter training data. By operating at the task level, LORAUTER achieves efficient routing that scales with the number of tasks rather than the number of adapters. Experiments across multiple tasks show that LORAUTER consistently outperforms baseline routing approaches, matching Oracle performance (101.2%) when task-aligned adapters exist and achieving state-of-the-art results on unseen tasks (+5.2 points). We further demonstrate the robustness of LORAUTER to very large, noisy adapter pools by scaling it to over 1500 adapters.

</details>


### [231] [Nonparametric LLM Evaluation from Preference Data](https://arxiv.org/abs/2601.21816)
*Dennis Frauen,Athiya Deviyani,Mihaela van der Schaar,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出 DMLEval 框架，通过去偏差机器学习（DML）在偏好数据上对大语言模型进行比较与排序，引入广义平均排序分数（GARS），可处理如平手等复杂响应；具备统计效率、允许嵌入黑箱估计、可结合 LLM 评估者，并给出预算约束下的数据收集策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法对参数模型假设依赖性强或在采用灵活机器学习方法时难以提供有效的不确定性量化，因此需要一个非参数且具备去偏估计的框架来评估 LLM 的偏好数据。

Method: 提出 DMLEval 框架，定义广义平均排序分数 GARS，扩展如 Bradley–Terry、PageRank/Rank centrality 等模型以处理复杂的人类偏好（包括平手等情况）；利用去偏差机器学习实现高效估计，支持黑箱机器学习估计器和 LLM 评估者的整合，并给出在预算约束下的数据收集策略。

Result: 理论与实验结果表明，DMLEval 能在合成与真实偏好数据集上提供统计高效的 GARS 估计，具备对黑箱估计器的良好容忍性，并能有效比较与排序不同的 LLM。

Conclusion: DMLEval 为实践者提供一个强大、前沿的框架，用于比较或排序 LLM，并可与其他评估工具及数据收集策略协同工作。

Abstract: Evaluating the performance of large language models (LLMs) from human preference data is crucial for obtaining LLM leaderboards. However, many existing approaches either rely on restrictive parametric assumptions or lack valid uncertainty quantification when flexible machine learning methods are used. In this paper, we propose a nonparametric statistical framework, DMLEval, for comparing and ranking LLMs from preference data using debiased machine learning (DML). For this, we introduce generalized average ranking scores (GARS), which generalize commonly used ranking models, including the Bradley-Terry model or PageRank/ Rank centrality, with complex human responses such as ties. DMLEval comes with the following advantages: (i) It produces statistically efficient estimates of GARS ranking scores. (ii) It naturally allows the incorporation of black-box machine learning methods for estimation. (iii) It can be combined with pre-trained LLM evaluators (e.g., using LLM-as-a-judge). (iv) It suggests optimal policies for collecting preference data under budget constraints. We demonstrate these advantages both theoretically and empirically using both synthetic and real-world preference datasets. In summary, our framework provides practitioners with powerful, state-of-the-art methods for comparing or ranking LLMs.

</details>


### [232] [DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training](https://arxiv.org/abs/2601.21824)
*Xinwei Qiang,Hongmin Chen,Shixuan Sun,Jingwen Leng,Xin Liu,Minyi Guo*

Main category: cs.LG

TL;DR: 将确定性注意力的后向传播视为有向无环图(DAG)调度问题，旨在最小化关键路径长度；提出DASH（Deterministic Attention Scheduling for High-Throughput），通过Descending Q-Tile Iteration和Shift Scheduling两种策略提升确定性注意力的吞吐量，实验证明在NVIDIA H800上可实现最高1.28×提升，并开源实现。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型训练中，为确保重复性，需使用确定性注意力计算，但现有实现如FlashAttention-3等在后向阶段存在显著性能损失，主要由于为保证数值一致性而需要串行化的梯度累积。需要通过更高效的调度来降低计算与梯度规约之间的空闲和流水线阻塞，从而提升确定性注意力的吞吐。

Method: 将确定性注意力的后向传播建模为一个有向无环图(DAG)调度问题，推导出能够最小化关键路径的调度策略。在此基础上提出DASH，包含两种互补策略：1) Descending Q-Tile Iteration（降序Q块遍历），通过反向遍历查询块以减小因果注意力中的流水线阻塞；2) Shift Scheduling，在所建DAG模型下的理论最优调度，降低全局与因果掩码下的流水线阻塞。通过理论分析和在NVIDIA H800·GPU上的实验验证评估吞吐量提升。

Result: 在与基线比较时，DASH的后向注意力吞吐量提升可达1.28×，显著缩小确定性注意力与非确定性实现之间的性能差距。首次将确定性注意力后向传播系统性地转化为DAG调度问题并给出两类高效调度策略。代码公开可在GitHub获取：https://github.com/SJTU-Liquid/deterministic-FA3

Conclusion: DASH有效提升确定性注意力在反向传播阶段的吞吐量，显著促进可重复性LLM训练的效率。提出的两类调度策略具有普适性，可应用于全局和因果注意力的后向计算，同时推动确定性注意力的实际部署与研究。

Abstract: Determinism is indispensable for reproducibility in large language model (LLM) training, yet it often exacts a steep performance cost. In widely used attention implementations such as FlashAttention-3, the deterministic backward pass can incur up to a 37.9% throughput reduction relative to its non-deterministic counterpart, primarily because gradient accumulation operations must be serialized to guarantee numerical consistency. This performance loss stems from suboptimal scheduling of compute and gradient-reduction phases, leading to significant hardware underutilization.
  To address this challenge, we formulate the backward pass of deterministic attention as a scheduling problem on a Directed Acyclic Graph (DAG) and derive schedules that minimize the critical path length. Building on this formulation, we present DASH (Deterministic Attention Scheduling for High-Throughput), which encapsulates two complementary scheduling strategies: (i) Descending Q-Tile Iteration, a reversed query-block traversal that shrinks pipeline stalls in causal attention, and (ii) Shift Scheduling, a theoretically optimal schedule within our DAG model that reduces pipeline stalls for both full and causal masks.
  Our empirical evaluations on NVIDIA H800 GPUs demonstrate that DASH narrows the performance gap of deterministic attention. The proposed strategies improve the throughput of the attention backward pass by up to 1.28$\times$ compared to the baseline, significantly advancing the efficiency of reproducible LLM training.
  Our code is open-sourced at https://github.com/SJTU-Liquid/deterministic-FA3.

</details>


### [233] [Goal-Driven Adaptive Sampling Strategies for Machine Learning Models Predicting Fields](https://arxiv.org/abs/2601.21832)
*Jigar Parekh,Philipp Bekemeyer*

Main category: cs.LG

TL;DR: 提出将主动学习扩展至场预测的通用框架，通过在标量参考值上使用高斯过程并同时降低模型的内在不确定性和标量与场预测之间的差异，以实现比无主动学习方法显著降低成本且保持精度。


<details>
  <summary>Details</summary>
Motivation: 解决昂贵黑箱仿真（如CFD）场景中，在尽可能少的样本下实现目标任务高准确性的问题，尤其在场预测方面需要超越仅基于标量的主动学习的适用性。

Method: 将高斯过程用于标量参考值，同时最小化模型的 epistemic 不确定性与标量-场预测之间的差异；提出若干具体实现形式并与仅基于标量的不不一致性主动学习进行对比；在 NASA 公共研究模型的不确定性传播任务中进行验证。

Result: 在显著降低黑箱仿真成本的同时实现高精度，与不使用主动学习的基线相比显示出更高的数据效率。

Conclusion: 所提出的主动学习策略对场预测任务具有普适性且对模型架构不敏感，能够通过结合标量与场信息提升数据效率。

Abstract: Machine learning models are widely regarded as a way forward to tackle multi-query challenges that arise once expensive black-box simulations such as computational fluid dynamics are investigated. However, ensuring the desired level of accuracy for a certain task at minimal computational cost, e.g. as few black-box samples as possible, remains a challenges. Active learning strategies are used for scalar quantities to overcome this challenges and different so-called infill criteria exists and are commonly employed in several scenarios. Even though needed in various field an extension of active learning strategies towards field predictions is still lacking or limited to very specific scenarios and/or model types. In this paper we propose an active learning strategy for machine learning models that are capable if predicting field which is agnostic to the model architecture itself. For doing so, we combine a well-established Gaussian process model for a scalar reference value and simultaneously aim at reducing the epistemic model error and the difference between scalar and field predictions. Different specific forms of the above-mentioned approach are introduced and compared to each other as well as only scalar-valued based infill. Results are presented for the NASA common research model for an uncertainty propagation task showcasing high level of accuracy at significantly smaller cost compared to an approach without active learning.

</details>


### [234] [Scalable Linearized Laplace Approximation via Surrogate Neural Kernel](https://arxiv.org/abs/2601.21835)
*Luis A. Ortega,Simón Rodríguez-Santana,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: A scalable surrogate kernel learning method for LLA in large DNNs that uses a DNN to learn features whose inner product matches NTK, enabling efficient Jacobian-vector products and improved predictive uncertainty and OOD detection.


<details>
  <summary>Details</summary>
Motivation: To enable scalable, calibrated uncertainty estimation via the Linearized Laplace Approximation (LLA) without computing large Jacobians, by learning a compact kernel that reproduces NTK behavior while offering potential improvements over NTK for OOD detection.

Method: Train a surrogate DNN to produce a compact feature representation; enforce that the inner product of features approximates the Neural Tangent Kernel (NTK) of a pre-trained network. Use Jacobian-vector products (JVPs) for training, avoiding full Jacobians. Apply the learned kernel within LLA to compute predictive uncertainty on large-scale pre-trained DNNs. Also explore biasing the learned kernel to enhance out-of-distribution detection.

Result: Experiments show similar or improved uncertainty estimation and calibration compared with existing LLA approximations. Introducing a biased kernel further improves OOD detection, suggesting the learned kernel can outperform NTK for LLA-based uncertainty estimation.

Conclusion: A scalable method to learn a surrogate kernel that matches NTK behavior (with optional bias) improves uncertainty estimation and OOD detection in LLA for large pre-trained DNNs, indicating that kernel learning can yield better kernels than NTK within the LLA framework.

Abstract: We introduce a scalable method to approximate the kernel of the Linearized Laplace Approximation (LLA). For this, we use a surrogate deep neural network (DNN) that learns a compact feature representation whose inner product replicates the Neural Tangent Kernel (NTK). This avoids the need to compute large Jacobians. Training relies solely on efficient Jacobian-vector products, allowing to compute predictive uncertainty on large-scale pre-trained DNNs. Experimental results show similar or improved uncertainty estimation and calibration compared to existing LLA approximations. Notwithstanding, biasing the learned kernel significantly enhances out-of-distribution detection. This remarks the benefits of the proposed method for finding better kernels than the NTK in the context of LLA to compute prediction uncertainty given a pre-trained DNN.

</details>


### [235] [Constrained Meta Reinforcement Learning with Provable Test-Time Safety](https://arxiv.org/abs/2601.21845)
*Tingting Ni,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 提出一种在约束元强化学习中进行策略精炼的算法，提供可证明的安全性与样本复杂度保证，并给出匹配的下界，确保测试任务上得到接近最优的策略。


<details>
  <summary>Details</summary>
Motivation: 现实场景中元强化学习在不同任务分布上提高了测试任务的学习效率，但常规设置未充分考虑测试阶段的安全性约束。约束元RL将安全性融入学习框架，但仍面临在测试任务上保障安全性同时降低样本复杂度的挑战。为解决该问题，本文提出一种在训练阶段学得的策略之上进行 refined 的算法，具备对测试任务的安全性保证和近似最优策略的样本复杂度保证，并给出下界以证明该复杂度的紧性。

Method: 提出一个在训练阶段学习的策略之上进行策略 refinement 的算法，确保在测试任务中的安全性约束得到满足，同时提供对近似最优策略的样本复杂度界。通过理论分析推导上界与下界，并给出实现要点以支撑实际部署的可行性。

Result: 给出具有安全性和样本复杂度理论保证的策略 refinement 算法，并证明所得到的近似最优策略在测试任务中的可实现性；给出一个匹配的下界，表明该样本复杂度界是紧的。

Conclusion: 该方法在理论层面确立了在约束元RL中兼顾安全性与样本效率的可行路径，并通过匹配的下界证明了结果的紧致性。未来工作可扩展到更复杂的安全约束、多任务情形以及在真实系统中的实验验证。

Abstract: Meta reinforcement learning (RL) allows agents to leverage experience across a distribution of tasks on which the agent can train at will, enabling faster learning of optimal policies on new test tasks. Despite its success in improving sample complexity on test tasks, many real-world applications, such as robotics and healthcare, impose safety constraints during testing. Constrained meta RL provides a promising framework for integrating safety into meta RL. An open question in constrained meta RL is how to ensure the safety of the policy on the real-world test task, while reducing the sample complexity and thus, enabling faster learning of optimal policies. To address this gap, we propose an algorithm that refines policies learned during training, with provable safety and sample complexity guarantees for learning a near optimal policy on the test tasks. We further derive a matching lower bound, showing that this sample complexity is tight.

</details>


### [236] [READY: Reward Discovery for Meta-Black-Box Optimization](https://arxiv.org/abs/2601.21847)
*Zechuan Huang,Zhiguang Cao,Hongshu Guo,Yue-Jiao Gong,Zeyuan Ma*

Main category: cs.LG

TL;DR: 提出以LLM进行奖励函数自动发现以提升MetaBBO的效能和效率，结合演化式LLM程序搜索与多任务并行化实现知识共享与加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有MetaBBO奖励设计受人类偏见与奖励篡改风险影响，缺乏自动化、可扩展性与跨任务迁移能力。需要无偏、可扩展的奖励发现机制。

Method: 通过在迭代的LLM程序搜索中引入演化策略，形成针对MetaBBO的定制化进化框架；并构建多任务演化架构实现并行奖励发现与跨任务知识共享。

Result: 实验表明所发现的奖励函数可提升现有MetaBBO方法的性能，证明奖励设计对MetaBBO的重要性。

Conclusion: LLM驱动的奖励函数自动发现是提升MetaBBO的可行路径；多任务并行架构进一步提升收敛速度与扩展性。

Abstract: Meta-Black-Box Optimization (MetaBBO) is an emerging avenue within Optimization community, where algorithm design policy could be meta-learned by reinforcement learning to enhance optimization performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use Large Language Model~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. Empirical results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY's project at https://anonymous.4open.science/r/ICML_READY-747F.

</details>


### [237] [Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models](https://arxiv.org/abs/2601.21851)
*Sidney Bender,Marco Morik*

Main category: cs.LG

TL;DR: 提出DiDAE，将冻结的基础模型与解耦字典学习结合，进行梯度自由的对抗性反事实生成，提升鲁棒性与去快捷学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型对虚假相关和“Clever Hans”策略的易受攻击性；现有缓解多依赖不可用的分组标签或高成本梯度对抗优化。

Method: DiDAE 将对基础模型嵌入的编辑限制在可解释的解耦字典方向，通过解耦字典学习实现嵌入编辑，并由扩散自编码器解码，生成多样且解耦的反事实；并与 Counterfactual Knowledge Distillation (CFKD) 联合，形成 DiDAE-CFKD。

Result: 相比现有基线，能够为每个事实生成多样的解耦反事实，速度显著高于需要生成单一-entangled反事实的方法；与 CFKD 联合后，在偏数据集上显著提升下游表现并缓解shortcut learning，达到新的SOTA。

Conclusion: DiDAE 提供一种高效、梯度自由的对抗性反事实生成与知识蒸馏结合的新框架，能在不依赖额外标签的情况下提升基础模型的鲁棒性与下游性能。

Abstract: Foundation models, despite their robust zero-shot capabilities, remain vulnerable to spurious correlations and 'Clever Hans' strategies. Existing mitigation methods often rely on unavailable group labels or computationally expensive gradient-based adversarial optimization. To address these limitations, we propose Visual Disentangled Diffusion Autoencoders (DiDAE), a novel framework integrating frozen foundation models with disentangled dictionary learning for efficient, gradient-free counterfactual generation directly for the foundation model. DiDAE first edits foundation model embeddings in interpretable disentangled directions of the disentangled dictionary and then decodes them via a diffusion autoencoder. This allows the generation of multiple diverse, disentangled counterfactuals for each factual, much faster than existing baselines, which generate single entangled counterfactuals. When paired with Counterfactual Knowledge Distillation, DiDAE-CFKD achieves state-of-the-art performance in mitigating shortcut learning, improving downstream performance on unbalanced datasets.

</details>


### [238] [MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts](https://arxiv.org/abs/2601.21866)
*Evandro S. Ortigossa,Guy Lutsker,Eran Segal*

Main category: cs.LG

TL;DR: MoHETS 是一个编码器端 Transformer，结合稀疏异质专家 MoHE 层，通过对时间片段进行路由、结合共享深度卷积的序列级连续性与路由傅里叶专家的片段级周期结构，以及通过对协变量嵌入的跨注意力整合外生信息，配合轻量卷积解码头，实现在多变量长 horizon 预测中的高效与鲁棒性；在七个基准上实现 state-of-the-art，平均 MSE 降低约 12%。


<details>
  <summary>Details</summary>
Motivation: Real-world multivariate time series often exhibit multi-scale structures (global trends, local periodicities) and non-stationary regimes, making long-horizon forecasting difficult. Sparse MoE with homogeneous MLP experts struggle to capture diverse temporal dynamics. There is a need for heterogeneous specialization and better incorporation of exogenous information.

Method: MoHETS is an encoder-only Transformer with sparse Mixture-of-Heterogeneous-Experts (MoHE) layers. MoHE routes temporal patches to a small subset of expert networks: a shared depthwise-convolution expert for sequence-level continuity, and routed Fourier-based experts for patch-level periodic structures. Non-stationarity is addressed by cross-attention over covariate patch embeddings. A lightweight convolutional patch decoder replaces heavy projection heads to improve parameter efficiency and horizon generalization.

Result: Empirical validation on seven multivariate benchmarks across multiple horizons shows state-of-the-art performance, with an average MSE reduction of about 12% compared to strong recent baselines.

Conclusion: The approach demonstrates effective heterogeneous specialization for long-term forecasting and generalization across arbitrary forecast horizons, improving robustness to non-stationary dynamics and parameter efficiency.

Abstract: Real-world multivariate time series can exhibit intricate multi-scale structures, including global trends, local periodicities, and non-stationary regimes, which makes long-horizon forecasting challenging. Although sparse Mixture-of-Experts (MoE) approaches improve scalability and specialization, they typically rely on homogeneous MLP experts that poorly capture the diverse temporal dynamics of time series data. We address these limitations with MoHETS, an encoder-only Transformer that integrates sparse Mixture-of-Heterogeneous-Experts (MoHE) layers. MoHE routes temporal patches to a small subset of expert networks, combining a shared depthwise-convolution expert for sequence-level continuity with routed Fourier-based experts for patch-level periodic structures. MoHETS further improves robustness to non-stationary dynamics by incorporating exogenous information via cross-attention over covariate patch embeddings. Finally, we replace parameter-heavy linear projection heads with a lightweight convolutional patch decoder, improving parameter efficiency, reducing training instability, and allowing a single model to generalize across arbitrary forecast horizons. We validate across seven multivariate benchmarks and multiple horizons, with MoHETS consistently achieving state-of-the-art performance, reducing the average MSE by $12\%$ compared to strong recent baselines, demonstrating effective heterogeneous specialization for long-term forecasting.

</details>


### [239] [Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions](https://arxiv.org/abs/2601.21873)
*Jinhang Chai,Xuyuan Liu,Elynn Chen,Yujun Yan*

Main category: cs.LG

TL;DR: 在同时增长环境维度与潜在表示的情形下，提出一个可迁移的结构矩阵估计框架及锚定交替投影估计器，理论给出误差界并在两个典型问题上验证迁移收益。


<details>
  <summary>Details</summary>
Motivation: 解决源任务子空间在更高维目标任务中的嵌入问题，以及在表示增长条件下仍能获得优于从零开始的估计的理论与方法。

Method: 将目标参数分解为嵌入的源分量、低秩创新和稀疏修正，提出锚定交替投影估计器以保持源子空间同时估计低维创新与稀疏改动，给出与噪声、表示增长、源估计误差相关的确定性误差界。

Result: 给出严格的误差界，表示增量的秩和稀疏参数较小时可获得严格改进的收敛/误差率；在马尔可夫转移矩阵估计（单条轨迹、含相关噪声）和扩展维度下的结构化协方差估计这两个典型问题上给出理论保证与实验验证。

Conclusion: 该框架具普适性，迁移收益来自较小的秩、稀疏增量，适用于多类结构矩阵估计任务，且能在增长的环境中稳定保留源任务信息。

Abstract: Learning systems often expand their ambient features or latent representations over time, embedding earlier representations into larger spaces with limited new latent structure. We study transfer learning for structured matrix estimation under simultaneous growth of the ambient dimension and the intrinsic representation, where a well-estimated source task is embedded as a subspace of a higher-dimensional target task.
  We propose a general transfer framework in which the target parameter decomposes into an embedded source component, low-dimensional low-rank innovations, and sparse edits, and develop an anchored alternating projection estimator that preserves transferred subspaces while estimating only low-dimensional innovations and sparse modifications. We establish deterministic error bounds that separate target noise, representation growth, and source estimation error, yielding strictly improved rates when rank and sparsity increments are small.
  We demonstrate the generality of the framework by applying it to two canonical problems. For Markov transition matrix estimation from a single trajectory, we derive end-to-end theoretical guarantees under dependent noise. For structured covariance estimation under enlarged dimensions, we provide complementary theoretical analysis in the appendix and empirically validate consistent transfer gains.

</details>


### [240] [Managing Solution Stability in Decision-Focused Learning with Cost Regularization](https://arxiv.org/abs/2601.21883)
*Victor Spitzer,Francois Sanson*

Main category: cs.LG

TL;DR: 在决策导向学习中，研究者关注如何鲁棒地估计组合优化问题的目标函数系数。通过把学习阶段的扰动强度波动与解的稳定性联系起来，提出对估计成本向量的正则化，从而提升训练稳定性与决策质量，并通过大量数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 旨在解决通过对组合优化问题的可微近似进行端到端训练时，扰动强度的波动可能导致训练不稳定和对解的敏感性增大的问题。

Method: 建立扰动强度与解的稳定性之间的理论联系，设计对估计成本向量的正则化项以提高鲁棒性；在学习目标中引入该正则化，并通过大规模数值实验验证其对训练稳定性和决策质量的提升。

Result: 理论分析揭示扰动强度的变化会影响解的稳定性，从而可通过正则化成本向量来缓解；实验显示正则化后模型在多种组合优化子任务中具有更高的鲁棒性与更稳定的决策性能。

Conclusion: 对成本向量的正则化是提升决策导向学习鲁棒性和可靠性的有效手段，尤其适用于需通过扰动近似求解的可微分化组合优化问题。

Abstract: Decision-focused learning integrates predictive modeling and combinatorial optimization by training models to directly improve decision quality rather than prediction accuracy alone. Differentiating through combinatorial optimization problems represents a central challenge, and recent approaches tackle this difficulty by introducing perturbation-based approximations. In this work, we focus on estimating the objective function coefficients of a combinatorial optimization problem. Our study demonstrates that fluctuations in perturbation intensity occurring during the learning phase can lead to ineffective training, by establishing a theoretical link to the notion of solution stability in combinatorial optimization. We propose addressing this issue by introducing a regularization of the estimated cost vectors which improves the robustness and reliability of the learning process, as demonstrated by extensive numerical experiments.

</details>


### [241] [Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning](https://arxiv.org/abs/2601.21894)
*Lukas Twist,Shu Yang,Hanqi Yan,Jingzhi Gong,Di Wang,Helen Yannakoudakis,Jie M. Zhang*

Main category: cs.LG

TL;DR: Structural complexity of code used in fine-tuning significantly shapes LLM reasoning ability; carefully selecting code by its complexity can outperform using diverse code, indicating a data-centric path beyond scaling.


<details>
  <summary>Details</summary>
Motivation: Identify which intrinsic properties of code contribute to improved reasoning in LLMs, moving beyond treating code as a generic training signal and focusing on how code structure (control flow, composition) shapes internal multi-step reasoning.

Method: Construct controlled fine-tuning datasets using two notions of complexity: solution-driven (complexity varies across multiple solutions to the same problem) and problem-driven (complexity reflects variation in the underlying tasks). Use cyclomatic complexity and logical lines of code to quantify structural complexity. Train open-weight LLMs and evaluate on diverse reasoning benchmarks.

Result: Code can improve reasoning, but its usefulness is strongly determined by structural properties. In 83% of experiments, restricting fine-tuning data to a specific structural complexity range outperforms training on structurally diverse code.

Conclusion: A data-centric approach to improving reasoning is effective: selecting code with appropriate structural complexity during fine-tuning can surpass mere scale increases, suggesting curriculum-like data curation guided by code structure.

Abstract: Large Language Models (LLMs) increasingly exhibit strong reasoning abilities, often attributed to their capacity to generate chain-of-thought-style intermediate reasoning. Recent work suggests that exposure to code can further enhance these skills, but existing studies largely treat code as a generic training signal, leaving open the question of which properties of code actually contribute to improved reasoning. To address this gap, we study the structural complexity of code, which captures control flow and compositional structure that may shape how models internalise multi-step reasoning during fine-tuning. We examine two complementary settings: solution-driven complexity, where complexity varies across multiple solutions to the same problem, and problem-driven complexity, where complexity reflects variation in the underlying tasks. Using cyclomatic complexity and logical lines of code to construct controlled fine-tuning datasets, we evaluate a range of open-weight LLMs on diverse reasoning benchmarks. Our findings show that although code can improve reasoning, structural properties strongly determine its usefulness. In 83% of experiments, restricting fine-tuning data to a specific structural complexity range outperforms training on structurally diverse code, pointing to a data-centric path for improving reasoning beyond scaling.

</details>


### [242] [Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting](https://arxiv.org/abs/2601.21899)
*Zhiqing Cui,Siru Zhong,Ming Jin,Shirui Pan,Qingsong Wen,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出 OmniAir：一个用于全球站点级空气质量预测的语义拓扑学习框架，通过给站点分配可泛化身份并自适应构建稀疏拓扑，捕捉跨区域长程非欧几何关系与扩散模式；并发布 WorldAir 数据集（7,800 站）在18个基线上实现SOTA，且速度近10倍于现有模型。


<details>
  <summary>Details</summary>
Motivation: 全球空气质量预测的极端空间异质性与现有传导式模型对未见区域泛化差的问题。

Method: 将不变量物理环境属性编码为可泛化的站点身份，动态构建自适应稀疏拓扑，从而捕捉跨区域的非欧几里得相关性与物理扩散模式。

Result: 在与18个基线的对比中达到SOTA，且效率和可扩展性出色，速度接近现有模型的10倍；在数据匮乏区域有效缓解监测缺口。

Conclusion: OmniAir 能够有效整合物理属性与语义拓扑，提升全球站点级空气质量预测的准确性与覆盖率。

Abstract: Global air quality forecasting grapples with extreme spatial heterogeneity and the poor generalization of existing transductive models to unseen regions. To tackle this, we propose OmniAir, a semantic topology learning framework tailored for global station-level prediction. By encoding invariant physical environmental attributes into generalizable station identities and dynamically constructing adaptive sparse topologies, our approach effectively captures long-range non-Euclidean correlations and physical diffusion patterns across unevenly distributed global networks. We further curate WorldAir, a massive dataset covering over 7,800 stations worldwide. Extensive experiments show that OmniAir achieves state-of-the-art performance against 18 baselines, maintaining high efficiency and scalability with speeds nearly 10 times faster than existing models, while effectively bridging the monitoring gap in data-sparse regions.

</details>


### [243] [Hardware-Triggered Backdoors](https://arxiv.org/abs/2601.21902)
*Jonas Möller,Erik Imgrund,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.LG

TL;DR: 提出一种基于硬件差异的后门攻击：通过在不同硬件上造成数值精度差异，使同一输入在不同设备上产生不同预测。通过调整决策边界靠近目标输入并细化数值偏差来在特定硬件上翻转预测。实验证明该硬件触发的后门在主流GPU加速器上可被可靠创建。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在多种计算硬件上部署，硬件设计差异带来的小数数值变动可能被攻击者利用，构成新的安全威胁，尤其对使用第三方模型的场景。

Method: 通过局部移动模型的决策边界，使目标输入在不同硬件上产生不同预测；进一步优化数值偏差以在选定硬件上翻转预测。对常见GPU加速器进行实证验证，评估实现的可行性与鲁棒性。

Result: 在多种GPU加速器上可可靠地创建硬件触发的后门，证明该攻击在现实部署中具有可行性。

Conclusion: 提出一种新的攻击向量，影响第三方模型的使用场景；需要考虑硬件特定的后门风险并探讨相应防御措施。

Abstract: Machine learning models are routinely deployed on a wide range of computing hardware. Although such hardware is typically expected to produce identical results, differences in its design can lead to small numerical variations during inference. In this work, we show that these variations can be exploited to create backdoors in machine learning models. The core idea is to shape the model's decision function such that it yields different predictions for the same input when executed on different hardware. This effect is achieved by locally moving the decision boundary close to a target input and then refining numerical deviations to flip the prediction on selected hardware. We empirically demonstrate that these hardware-triggered backdoors can be created reliably across common GPU accelerators. Our findings reveal a novel attack vector affecting the use of third-party models, and we investigate different defenses to counter this threat.

</details>


### [244] [LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution](https://arxiv.org/abs/2601.21929)
*Shuangqi Li,Hieu Le,Jingyi Xu,Mathieu Salzmann*

Main category: cs.LG

TL;DR: LoRIF 提出基于低秩结构的梯度投影，用截断式 SVD 与 Woodbury 公式近似 Hessian，解决训练数据归因（TDA）中的存储/ I/O 与 内存瓶颈，在不牺牲归因质量的前提下实现对大模型（0.1B–70B 参数）和百万级样本数据集的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 梯度为基础的影响函数型 TDA 在大规模模型和大规模训练集上存在两大瓶颈：一是在查询时需要存储/载入每个训练样本的投影梯度，I/O 成为瓶颈；二是需要构建 D×D 的逆 Hessian 近似，内存复杂度高。提高投影维度 D 以提升归因质量又会放大上述两瓶颈，导致质量与可扩展性的权衡。

Method: 采用两大技术：1) 将投影后每个样本的梯度矩阵进行低秩分解，存储 rank-c 因子而非全矩阵，使单样本/每层的存储与 I/O 下降到 O(c√D)；2) 对 Hessian 项使用截断的 SVD 结合 Woodbury 公式，在 r 维子空间内近似，内存降至 O(Dr)；同时将梯度投影与低秩近似结合以计算影响函数。

Result: 在从 0.1B 到 70B 参数、训练于包含数百万示例的数据集的模型上，LoRIF 相较于 LoGRA 实现了最高约 20× 的存储与查询时间加速，且归因质量相匹配或优于 LoGRA。

Conclusion: LoRIF 将梯度基 TDA 在前沿规模上变得可行，缓解了两大瓶颈并提升可扩展性。

Abstract: Training data attribution (TDA) identifies which training examples most influenced a model's prediction. The best-performing TDA methods exploits gradients to define an influence function. To overcome the scalability challenge arising from gradient computation, the most popular strategy is random projection (e.g., TRAK, LoGRA). However, this still faces two bottlenecks when scaling to large training sets and high-quality attribution: \emph{(i)} storing and loading projected per-example gradients for all $N$ training examples, where query latency is dominated by I/O; and \emph{(ii)} forming the $D \times D$ inverse Hessian approximation, which costs $O(D^2)$ memory. Both bottlenecks scale with the projection dimension $D$, yet increasing $D$ is necessary for attribution quality -- creating a quality-scalability tradeoff. We introduce \textbf{LoRIF (Low-Rank Influence Functions)}, which exploits low-rank structures of gradient to address both bottlenecks. First, we store rank-$c$ factors of the projected per-example gradients rather than full matrices, reducing storage and query-time I/O from $O(D)$ to $O(c\sqrt{D})$ per layer per sample. Second, we use truncated SVD with the Woodbury identity to approximate the Hessian term in an $r$-dimensional subspace, reducing memory from $O(D^2)$ to $O(Dr)$. On models from 0.1B to 70B parameters trained on datasets with millions of examples, LoRIF achieves up to 20$\times$ storage reduction and query-time speedup compared to LoGRA, while matching or exceeding its attribution quality. LoRIF makes gradient-based TDA practical at frontier scale.

</details>


### [245] [Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models](https://arxiv.org/abs/2601.21943)
*Ahmad Aghapour,Erhan Bayraktar,Ziqing Zhang*

Main category: cs.LG

TL;DR: 提出一种无维度依赖的信息论收敛分析框架来评估扩散模型的近似误差，并给出一个以训练损失为基础、轻量的逆SDE离散化调度（LAS）以提升采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的收敛分析通常依赖环境维度或目标分布的几何假设，难以在高维或无几何约束情形下给出无维度偏差的收敛界。需要一种不依赖几何假设的维度无关分析，并提升采样效率。

Method: 通过信息论视角，将目标分布与生成分布之间的KL散度绑定在目标分布的香农熵H和采样步数K之上，给出KL = O(H^2/K)的界（含端点因子）。同时将KL散度的某一等价重构用于设计Loss-Adaptive Schedule（LAS），该调度仅依赖训练损失，轻量且无后训练高成本计算。

Result: 在 mild 假设下，得到目标分布与生成分布之间的KL散度上界为 O(H^2/K)，并提出 LAS 明显提升采样质量相较于常见的启发式调度。

Conclusion: 提供一个维度无关的收敛框架，并给出基于训练损失的实际可用调度方案 LAS，提升扩散模型的采样效率与质量，同时避免对目标分布几何形状的依赖。

Abstract: Diffusion generative models synthesize samples by discretizing reverse-time dynamics driven by a learned score (or denoiser). Existing convergence analyses of diffusion models typically scale at least linearly with the ambient dimension, and sharper rates often depend on intrinsic-dimension assumptions or other geometric restrictions on the target distribution. We develop an alternative, information-theoretic approach to dimension-free convergence that avoids any geometric assumptions. Under mild assumptions on the target distribution, we bound KL divergence between the target and generated distributions by $O(H^2/K)$ (up to endpoint factors), where $H$ is the Shannon entropy and $K$ is the number of sampling steps. Moreover, using a reformulation of the KL divergence, we propose a Loss-Adaptive Schedule (LAS) for efficient discretization of reverse SDE which is lightweight and relies only on the training loss, requiring no post-training heavy computation. Empirically, LAS improves sampling quality over common heuristic schedules.

</details>


### [246] [Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models](https://arxiv.org/abs/2601.21944)
*Konstantinos P. Panousis,Diego Marcos*

Main category: cs.LG

TL;DR: 本研究系统地评估概念瓶颈模型（CBM）在视觉-语言模型中的可解释性，通过引入“清晰度”度量并建立带有真实概念注释的数据集评估框架，比较VLM- CBM与属性预测CBM在三种稀疏化策略（逐样本L1、L0、Bernoulli）下的表现，揭示灵活性与可解释性之间的关键权衡。


<details>
  <summary>Details</summary>
Motivation: 当前VLM通常化为黑箱，缺乏系统、客观的可解释性评估，尤其对于以稀疏性驱动的解释性方法。需要统一的评估框架和量化指标来比较不同CBM设计对概念表征与下游任务的影响。

Method: 提出“清晰度”概念，量化下游性能与概念表征的稀疏性与精度之间的关系；构建以真实概念注释为基础的可解释性评估框架；对比两类CBM（VLM‑CBM 与属性预测 CBM）在三种稀疏化策略下的表现。

Result: 实验揭示显著的灵活性-可解释性权衡：在相近下游性能水平下，同一方法可能呈现截然不同的可解释性行为；提升准确性不必然提升可解释性，且不同稀疏化策略对表征有不同影响；作者将公开代码。

Conclusion: 需要在性能、可解释性与稳定性之间取得平衡，并提供统一的评估框架来比较CBM变体的可解释性，为选择合适的CBM与稀疏化策略提供实用指南，促使后续工作在可解释性评估方法上更加系统。

Abstract: The widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to "induce interpretability". In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\ell_1, \ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.

</details>


### [247] [Embracing Aleatoric Uncertainty in Medical Multimodal Learning with Missing Modalities](https://arxiv.org/abs/2601.21950)
*Linxiao Gong,Yang Liu,Lianlong Sun,Yulai Bi,Jing Liu,Xiaoguang Zhu*

Main category: cs.LG

TL;DR: 提出AUM用于多模态医疗学习中的缺失模态问题；将单模态表示建模为高斯分布以捕捉阿勒托里克不确定性，并在二部图的动态消息传递中进行不确定性感知聚合以提高表现，实测在MIMIC-IV与eICU上分别提升约2.2% AUC-ROC。


<details>
  <summary>Details</summary>
Motivation: 医疗数据存在大量缺失模态且采集存在不确定性，现有方法假设模态贡献相等且缺失模式随机，忽略对不确定性的建模，因此需要一种能够定量模态可靠性并在缺失时仍能有效聚合信息的框架。

Method: 将每个模态的单模态表示建模为多元高斯分布以刻画阿勒托里克不确定性；在患者-模态的二部图中引入基于不确定性的动态消息传递和聚合机制，使得对可用模态的更可靠信息被加权强调，缺失模态天然得到处理。

Result: 在MIMIC-IV死亡率预测任务上AUC-ROC提升约2.26%，在eICU数据集提升约2.17%；优于现有最先进方法。

Conclusion: AUM通过对单模态不确定性的显式建模与不确定性感知的聚合，提供了对缺失模态的鲁棒性提升和更可靠的信息整合，提升了多模态临床预测的性能。

Abstract: Medical multimodal learning faces significant challenges with missing modalities prevalent in clinical practice. Existing approaches assume equal contribution of modality and random missing patterns, neglecting inherent uncertainty in medical data acquisition. In this regard, we propose the Aleatoric Uncertainty Modeling (AUM) that explicitly quantifies unimodal aleatoric uncertainty to address missing modalities. Specifically, AUM models each unimodal representation as a multivariate Gaussian distribution to capture aleatoric uncertainty and enable principled modality reliability quantification. To adaptively aggregate captured information, we develop a dynamic message-passing mechanism within a bipartite patient-modality graph using uncertainty-aware aggregation mechanism. Through this process, missing modalities are naturally accommodated, while more reliable information from available modalities is dynamically emphasized to guide representation generation. Our AUM framework achieves an improvement of 2.26% AUC-ROC on MIMIC-IV mortality prediction and 2.17% gain on eICU, outperforming existing state-of-the-art approaches.

</details>


### [248] [Investigation into using stochastic embedding representations for evaluating the trustworthiness of the Fréchet Inception Distance](https://arxiv.org/abs/2601.21979)
*Ciaran Bench,Vivek Desai,Carlijn Roozemond,Ruben van Engen,Spencer A. Thomas*

Main category: cs.LG

TL;DR: 本研究检查了以 ImageNet 预训练的 InceptionV3 作为特征提取器时，FID 对医学图像的局限性，并通过蒙特卡洛 dropout 估计 FID 及其潜在嵌入的预测方差，以评估这类不确定性与数据分布外度量之间的相关性，从而评估 FID 的可信度。


<details>
  <summary>Details</summary>
Motivation: FID 广泛用于评估合成图像质量，但依赖于自然图像的特征分布，可能在医学图像场景中失效。缺乏对 FID 不确定性的量化以及对其在医学领域可信度的系统评估。

Method: 对 FID 及其嵌入空间使用蒙特卡洛 dropout 进行预测方差估计；对比 ImageNet1K 验证集在不同强度的增强以及其他外部数据集的分布外程度，观察预测方差的大小与分布外性之间的相关性。

Result: 预测方差的大小与输入数据的分布外程度存在不同程度的相关性，提供了关于将这些不确定性用作 FID 信任度指示的洞见，表明在医学图像场景中，单纯依赖 FID 可能不足以充分评估图像特征差异。

Conclusion: 引入对 FID 及其嵌入表示的预测不确定性后，可以更好地理解 FID 在医学应用中的可信度边界，强调在医学场景下应结合不确定性信息来评估图像质量或数据集特征差异。

Abstract: Feature embeddings acquired from pretrained models are widely used in medical applications of deep learning to assess the characteristics of datasets; e.g. to determine the quality of synthetic, generated medical images. The Fréchet Inception Distance (FID) is one popular synthetic image quality metric that relies on the assumption that the characteristic features of the data can be detected and encoded by an InceptionV3 model pretrained on ImageNet1K (natural images). While it is widely known that this makes it less effective for applications involving medical images, the extent to which the metric fails to capture meaningful differences in image characteristics is not obviously known. Here, we use Monte Carlo dropout to compute the predictive variance in the FID as well as a supplemental estimate of the predictive variance in the feature embedding model's latent representations. We show that the magnitudes of the predictive variances considered exhibit varying degrees of correlation with the extent to which test inputs (ImageNet1K validation set augmented at various strengths, and other external datasets) are out-of-distribution relative to its training data, providing some insight into the effectiveness of their use as indicators of the trustworthiness of the FID.

</details>


### [249] [PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters](https://arxiv.org/abs/2601.21984)
*Jian Gao,Yiwei Zou,Abhishek Pradhan,Wenhao Huang,Yumin Su,Kaiyuan Yang,Xuan Zhang*

Main category: cs.LG

TL;DR: PowerGenie框架实现大规模高性能可重构电力变换器的自动化发现，通过分析框架和进化微调，获得比训练拓扑更高FoM的8模式变换器；在SPICE上验证出显著的效率增益（平均约10%，单模式最高17%）。


<details>
  <summary>Details</summary>
Motivation: 传统的设计空间呈指数级增长，依赖人类专家，现有AI方法要么限制于模板选择，要么在有限尺度生成拓扑且缺乏严格验证，导致大规模、以性能为驱动的发现受限。

Method: 1) 自动分析框架：在不进行元件尺寸设定或SPICE仿真的前提下，判断变换器功能与理论性能极限；2) 进化微调：使生成模型和训练分布共同进化，通过适应度选择与唯一性验证防止模式坍塌和过拟合；3) 与现有方法对比，提升语法有效性、功能有效性、新颖性和FoM。

Result: 发现一个新的8模式可重构变换器，FoM比最佳训练拓扑高出23%；SPICE仿真显示在8个模式平均效率提升约10%，单个模式最高17%；代码公开。

Conclusion: 证明了在不依赖尺寸设定和高成本仿真的情况下，通过分析框架与进化微调可以实现大规模、高性能的变换器设计发现，具备可重复性和扩展性。

Abstract: Discovering superior circuit topologies requires navigating an exponentially large design space-a challenge traditionally reserved for human experts. Existing AI methods either select from predefined templates or generate novel topologies at a limited scale without rigorous verification, leaving large-scale performance-driven discovery underexplored. We present PowerGenie, a framework for automated discovery of higher-performance reconfigurable power converters at scale. PowerGenie introduces: (1) an automated analytical framework that determines converter functionality and theoretical performance limits without component sizing or SPICE simulation, and (2) an evolutionary finetuning method that co-evolves a generative model with its training distribution through fitness selection and uniqueness verification. Unlike existing methods that suffer from mode collapse and overfitting, our approach achieves higher syntax validity, function validity, novelty rate, and figure-of-merit (FoM). PowerGenie discovers a novel 8-mode reconfigurable converter with 23% higher FoM than the best training topology. SPICE simulations confirm average absolute efficiency gains of 10% across 8 modes and up to 17% at a single mode. Code is available at https://github.com/xz-group/PowerGenie.

</details>


### [250] [Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields](https://arxiv.org/abs/2601.21985)
*Yunyang Li,Lin Huang,Luojia Xia,Wenhe Zhang,Mark Gerstein*

Main category: cs.LG

TL;DR: Elign is a post-training framework for E(3)-equivariant diffusion models that amortizes both the expensive quantum-chemical evaluations and per-step energy queries for 3D molecular conformer generation. It uses a pretrained ML force field for physical signals and reframes reverse diffusion as reinforcement learning via FED-GRPO, which optimizes potential-energy and force-based stability rewards independently. Result: lower DFT energies/forces and improved stability with inference speed comparable to unguided sampling, since no energy evaluations are needed at generation time.


<details>
  <summary>Details</summary>
Motivation: Generative models for 3D molecular conformations must respect Euclidean symmetries and sample thermodynamically favorable, mechanically stable structures. However, existing E(3)-equivariant diffusion models inherit biases from lower-fidelity training data and rely on costly physics evaluations (DFT) during sampling. A remedy is to accelerate and stabilize sampling by integrating physical guidance without runtime energy calls.

Method: Replace expensive DFT with a pretrained MLFF to provide physical signals. Move physical steering into training by casting reverse diffusion as RL. Propose FED-GRPO (Force–Energy Disentangled Group Relative Policy Optimization) to fine-tune the denoising policy using two independent reward streams: a potential-based energy reward and a force-based stability reward, both group-normalized to decouple contributions.

Result: Empirical results show generated conformations with lower gold-standard DFT energies and forces, and improved stability, while maintaining inference speed identical to unguided sampling since no energy evaluations are required during generation.

Conclusion: Elign successfully amortizes both the data-to-physics cost and the runtime energy queries for E(3)-equivariant diffusion in molecular conformation generation. It yields higher-quality, thermodynamically more favorable structures without sacrificing sampling speed.

Abstract: Generative models for 3D molecular conformations must respect Euclidean symmetries and concentrate probability mass on thermodynamically favorable, mechanically stable structures. However, E(3)-equivariant diffusion models often reproduce biases from semi-empirical training data rather than capturing the equilibrium distribution of a high-fidelity Hamiltonian. While physics-based guidance can correct this, it faces two computational bottlenecks: expensive quantum-chemical evaluations (e.g., DFT) and the need to repeat such queries at every sampling step. We present Elign, a post-training framework that amortizes both costs. First, we replace expensive DFT evaluations with a faster, pretrained foundational machine-learning force field (MLFF) to provide physical signals. Second, we eliminate repeated run-time queries by shifting physical steering to the training phase. To achieve the second amortization, we formulate reverse diffusion as a reinforcement learning problem and introduce Force--Energy Disentangled Group Relative Policy Optimization (FED-GRPO) to fine-tune the denoising policy. FED-GRPO includes a potential-based energy reward and a force-based stability reward, which are optimized and group-normalized independently. Experiments show that Elign generates conformations with lower gold-standard DFT energies and forces, while improving stability. Crucially, inference remains as fast as unguided sampling, since no energy evaluations are required during generation.

</details>


### [251] [Generalized Information Gathering Under Dynamics Uncertainty](https://arxiv.org/abs/2601.21988)
*Fernando Palafox,Jingqi Li,Jesse Milzman,David Fridovich-Keil*

Main category: cs.LG

TL;DR: 提出一个基于 directed information 的通用信息获取成本框架，解耦学习中的建模选择（动力学、belief 更新、观测模型、规划器）与信息获取代价；MI成本是该通用成本的特例，并给出线性化贝叶斯估计中的信息收益与MI成本之间的联系，透过线性/非线性/多主体系统的实验验证其普适性。


<details>
  <summary>Details</summary>
Motivation: 在未知动态系统中，主动信息收集能加速学习，但现有方法常针对特定建模假设给出专门设计的成本，缺乏统一框架。需要一个对建模选择无关、只依赖信息流的成本定义，便于理论分析与跨场景应用。

Method: 建立一个明确暴露参数、信念和控制之间因果依赖关系的通用框架；在仅要求马尔可夫动态与加性噪声的前提下，推导基于 Massey 的定向信息的信息获取成本，独立于具体动力学近似、信念更新与观测模型的实现；证明现有互信息成本是该成本的特例；进一步将互信息成本与线性化贝叶斯估计中的信息增益联系起来，提供理论支撑。

Result: 给出了一个普适的信息获取成本的理论框架，覆盖线性、非线性与多智能体系统；证明互信息成本属于该框架的子集；建立 MI 成本与信息增益之间的定量关系，并在多种系统中验证框架的实用性与可操作性。

Conclusion: 为信息驱动的主动学习提供一个理论上严格且对模型选择无关的成本度量，便于在不同动力学模型、观测模型与规划策略之间进行解耦设计，并且在理论与实验层面均获得支持。

Abstract: An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.

</details>


### [252] [Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains](https://arxiv.org/abs/2601.21999)
*Meng Cao,Jiexi Liu,Songcan Chen*

Main category: cs.LG

TL;DR: 提出并分析了IDG，给出理论泛化界限并提出Negative-Dominant Contrastive Learning (NDCL) 以重塑决策边界、提升对少数类的辨别，同时实现域间后验一致性；在基准数据集上取得显著效果，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 在IDG场景中同时存在域偏移与标签偏移，且长尾分布导致决策边界偏向多数类。现有方法难以理论支撑且难以解耦两类偏移，需直接对决策边界进行引导并给出理论泛化界限。

Method: 提出理论化的IDG泛化界界限，强调后验差异与决策边界；提出NDCL，通过对比学习以负样本为主信号增强跨类边界分离，强化对少数类的梯度信号；采用重加权交叉熵提升类内紧凑性；通过预测中心对齐实现域间后验一致性。

Result: 在多项基准数据集上验证有效性，NDCL显著提升跨域泛化，尤其在长尾/少数类场景；代码公开于GitHub。

Conclusion: 将理论界限与负样本导向的对比学习结合，提供一个可操作且具理论支撑的IDG框架，改善决策边界的鲁棒性与域与标签偏的平衡。

Abstract: Imbalanced Domain Generalization (IDG) focuses on mitigating both domain and label shifts, both of which fundamentally shape the model's decision boundaries, particularly under heterogeneous long-tailed distributions across domains. Despite its practical significance, it remains underexplored, primarily due to the technical complexity of handling their entanglement and the paucity of theoretical foundations. In this paper, we begin by theoretically establishing the generalization bound for IDG, highlighting the role of posterior discrepancy and decision margin. This bound motivates us to focus on directly steering decision boundaries, marking a clear departure from existing methods. Subsequently, we technically propose a novel Negative-Dominant Contrastive Learning (NDCL) for IDG to enhance discriminability while enforce posterior consistency across domains. Specifically, inter-class decision-boundary separation is enhanced by placing greater emphasis on negatives as the primary signal in our contrastive learning, naturally amplifying gradient signals for minority classes to avoid the decision boundary being biased toward majority classes. Meanwhile, intra-class compactness is encouraged through a re-weighted cross-entropy strategy, and posterior consistency across domains is enforced through a prediction-central alignment strategy. Finally, rigorous yet challenging experiments on benchmarks validate the effectiveness of our NDCL. The code is available at https://github.com/Alrash/NDCL.

</details>


### [253] [Rate-Distortion Optimization for Transformer Inference](https://arxiv.org/abs/2601.22002)
*Anderson de Andrade,Alon Harell,Ivan V. Bajić*

Main category: cs.LG

TL;DR: 提出一个基于速率-失真理论的有损压缩框架，用可学习的编码实现变压器中间表示的压缩，权衡比特率与精度；在语言任务上实现显著压缩且有时提升准确率，并给出统一的理论解释框架及 PAC 风险界限。


<details>
  <summary>Details</summary>
Motivation: 推理阶段对计算与内存的高需求促使跨设备分区并压缩中间表示；需要一个信息论基础的框架来量化压缩与性能之间的关系，并解释不同架构/任务的速率差异。

Method: 搭建端到端的可学习有损压缩编解码器，优化以最大化精度保留与最小化比特率的权衡；系统性分析变压器的速率-失真特性，扩展信息理论以定义速率与熵之间的差距并推导界限；提出 PAC 风格的界限用于估计该差距。

Result: 在语言基准上，所提编解码器在显著降低比特率的同时保持或提升精度，优于部分更复杂的基线；对不同架构和任务的速率进行实验分析，结果与理论界限相符，提升对表示编码的理解。

Conclusion: 将信息理论的速率-熵差距概念扩展至变压器表示编码，提供统一的分析框架并给出可估计的 PAC 界限，增强对分布式推理中间表示压缩的解释性与可控性。

Abstract: Transformers achieve superior performance on many tasks, but impose heavy compute and memory requirements during inference. This inference can be made more efficient by partitioning the process across multiple devices, which, in turn, requires compressing its intermediate representations. In this work, we introduce a principled rate-distortion-based framework for lossy compression that learns compact encodings that explicitly trade off bitrate against accuracy. Experiments on language benchmarks show that the proposed codec achieves substantial savings with improved accuracy in some cases, outperforming more complex baseline methods. We characterize and analyze the rate-distortion performance of transformers, offering a unified lens for understanding performance in representation coding. This formulation extends information-theoretic concepts to define the gap between rate and entropy, and derive some of its bounds. We further develop probably approximately correct (PAC)-style bounds for estimating this gap. For different architectures and tasks, we empirically demonstrate that their rates are driven by these bounds, adding to the explainability of the formulation.

</details>


### [254] [Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering](https://arxiv.org/abs/2601.22010)
*Dongxuan Zhu,Ly Tran Ho Khanh,Andy Yat-Ming Cheung,Man-Chung Yue,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 提出 STARS，一种训练无关的推理时激活引导方法，通过在斯蒂费尔流形上对多方向施加并行探索，实现对并行生成路径的多样性提升，且提供低延迟的一步更新；在测试用例和科学发现基准上优于标准采样方法，同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型易收敛到高概率输出，产出单一且缺乏多样性；不同并行生成运行之间的多样性难以被保证，需高效且无额外训练成本的多样性提升方法。

Method: 在每个 token 时收集并行生成的隐藏激活，联合优化多条加性 steering 方向，约束在斯蒂费尔流形上以最大化被 steering 的激活向量的几何体积，并通过流形正交性促使不同方向正交。可用黎曼梯度下降求解；为低延迟设计了一个一步更新的封闭形式步长。

Result: 在测试用例生成和科学发现基准上，STARS 相较标准采样方法具有更强的多样性，同时不牺牲定性表现。

Conclusion: STARS 提供了一种训练无关、推理时的激活引导方法，通过在斯蒂费尔流形上的多向正交优化实现对并行生成的多样性提升，且具备低延迟特性，兼顾生成质量与多样性。

Abstract: Language models often default to a narrow set of high-probability outputs, leaving their generation paths homogeneous and prone to mode collapse. Sampling-based strategies inject randomness but still struggle to guarantee diversity across multiple concurrent generation runs. We address this limitation by introducing STARS ($\textbf{St}$iefel-based $\textbf{A}$ctivation Steering for Diverse $\textbf{R}$ea$\textbf{S}$oning), a training-free, inference-time intervention method that transforms activation steering into an exploration engine. At each token, STARS collects the hidden activations of concurrent generation runs and optimizes multiple additive steering directions jointly on the Stiefel manifold. STARS maximizes the geometric volume of the steered activations, while the Stiefel manifold induces orthogonality of the steering interventions. This formulation explicitly promotes divergent activation vectors of concurrent generation runs, and implicitly promotes divergent generation trajectories. This manifold optimization formulation can be solved using a Riemannian gradient descent algorithm with convergence guarantees, but this algorithm is too time-consuming for real-time inference. To guarantee low latency, we further design a lightweight one-step update with an aggressive, closed-form stepsize. For test case generation and scientific discovery benchmarks, STARS consistently outperforms standard sampling methods, achieving greater diversity without sacrificing qualitative performance.

</details>


### [255] [Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability](https://arxiv.org/abs/2601.22012)
*Sergi Masip,Gido M. van de Ven,Javier Ferrando,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 提出一个特征中心的机械框架，将灾难性遗忘解释为特征编码的几何变换所致的容量下降与读出干扰，并在简化模型分析与 Crosscoders 的实践应用中进行验证，包含对 Vision Transformer 在顺序 CIFAR-10 的案例研究，提供新的特征中心词汇。


<details>
  <summary>Details</summary>
Motivation: 现有衡量往往关注最后一层表现，缺乏对遗忘机制的直观、几何化解释。需要一个面向特征的框架，揭示特征编码的变换如何导致容量下降和下游读出干扰，从而产生遗忘。

Method: 提出一个可分析的框架和简化的可解模型，形式化地将遗忘视为特征编码的几何变换；分析最优与最差情形；在所建模型上进行实验以验证理论；通过 Crosscoders 将框架应用到实际模型；给出 Vision Transformer 在顺序 CIFAR-10 的案例研究。

Result: 深度对灾难性遗忘有显著负面影响，遗忘源自特征容量下降与下游读出受干扰；理论分析得到的最优/最差情形与实证结果相互印证；框架具可转化性，可通过 Crosscoders 应用于实际模型。

Conclusion: 提供一个新的、以特征为中心的 vocabulary，揭示机制性原因并为实际模型的分析与潜在缓解提供路径。

Abstract: Catastrophic forgetting in continual learning is often measured at the performance or last-layer representation level, overlooking the underlying mechanisms. We introduce a mechanistic framework that offers a geometric interpretation of catastrophic forgetting as the result of transformations to the encoding of individual features. These transformations can lead to forgetting by reducing the allocated capacity of features (worse representation) and disrupting their readout by downstream computations. Analysis of a tractable model formalizes this view, allowing us to identify best- and worst-case scenarios. Through experiments on this model, we empirically test our formal analysis and highlight the detrimental effect of depth. Finally, we demonstrate how our framework can be used in the analysis of practical models through the use of Crosscoders. We present a case study of a Vision Transformer trained on sequential CIFAR-10. Our work provides a new, feature-centric vocabulary for continual learning.

</details>


### [256] [TBDFiltering: Sample-Efficient Tree-Based Data Filtering](https://arxiv.org/abs/2601.22016)
*Robert Istvan Busa-Fekete,Julian Zimmert,Anne Xiangyi Zheng,Claudio Gentile,Andras Gyorgy*

Main category: cs.LG

TL;DR: 提出一种基于文本嵌入的分层聚类方法，以自适应地挑选需由LLM评估的文档来估计簇质量，理论上在存在某个子树使叶簇几乎纯净的情况下具有查询高效性；实验表明优于基于分类器的过滤方法。


<details>
  <summary>Details</summary>
Motivation: LLMs训练数据质量对模型性能影响巨大，缺乏廉价、可靠的质量度量。现有通过对文档进行少量信号学习分类器的方法在大规模数据集上不可扩展，需更高效的质量估计与数据选择策略。

Method: 使用文本嵌入构建自适应的分层聚类结构，通过对选定的文档进行LLM评估来估计各簇的质量。若树中存在一个最小子树，其叶簇近似“纯净”（大致仅含良好或坏文档），则在高概率下，所需查询量与该最小纯净子树的大小成比例。算法在运行时并不需要事先知道该子树的位置。

Result: 理论上证明了在近似纯净叶簇存在的条件下，方法具有查询效率（需要的查询量很小且与最小纯净子树规模相关）。在大规模实验中，该方法在性能上优于其他基于分类器的过滤方法。

Conclusion: 提出的基于文本嵌入的自适应分层聚类方法为大规模训练数据的质量估计提供可扩展的解决方案，理论具备保障的查询效率，且在实证中表现优于传统的分类器筛选。

Abstract: The quality of machine learning models depends heavily on their training data. Selecting high-quality, diverse training sets for large language models (LLMs) is a difficult task, due to the lack of cheap and reliable quality metrics. While querying existing LLMs for document quality is common, this is not scalable to the large number (billions) of documents used in training. Instead, practitioners often use classifiers trained on sparse quality signals. In this paper, we propose a text-embedding-based hierarchical clustering approach that adaptively selects the documents to be evaluated by the LLM to estimate cluster quality. We prove that our method is query efficient: under the assumption that the hierarchical clustering contains a subtree such that each leaf cluster in the tree is pure enough (i.e., it mostly contains either only good or only bad documents), with high probability, the method can correctly predict the quality of each document after querying a small number of documents. The number of such documents is proportional to the size of the smallest subtree with (almost) pure leaves, without the algorithm knowing this subtree in advance. Furthermore, in a comprehensive experimental study, we demonstrate the benefits of our algorithm compared to other classifier-based filtering methods.

</details>


### [257] [Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2601.22020)
*Chengyi Cai,Zesheng Ye,Peike Li,Bo Han,Jianzhong Qi,Feng Liu*

Main category: cs.LG

TL;DR: 提出 Visual-Guided Key-Token Regularization (ViKeR)，通过视觉信号识别并优先处理关键 token 的正则化，以实现多模态大语言模型在回答关于目标图像时的无泄漏无忘记学习；利用无关视觉输入预测未学习后的 token 分布并对梯度在 token 级进行再加权；依据信息熵定义关键 token，在 MLLMU 与 CLEAR 基准上实现更强的无忘记性与更好的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型无忘记方法多沿用单模态LLM的思想，忽视不同 token 的重要性及视觉信号对答案中关键 token 的提示，导致隐私泄露风险与遗忘控制不足。

Method: 提出 ViKeR：利用与目标图像无关的视觉输入来预测未学习后的理想 token 分布，并以此分布作为正则化目标，优先保护或强化对关键 token 的更新；通过信息熵定义关键 token 并在训练中进行 token 级梯度重加权，以放大对关键 token 的影响；在多模态问答场景中结合视觉信号进行无忘记学习评估。

Result: 在 MLLMU 与 CLEAR 基准上，ViKeR 能实现有效无忘记的无泄漏学习，同时保持回答的连贯性，相比基线方法有显著提升。

Conclusion: 通过将视觉线索用于识别和权重化关键 token，并以信息熵驱动的梯度重加权，ViKeR 提升了多模态模型的无忘记学习效果，兼顾隐私保护与回答质量。

Abstract: Unlearning in Multimodal Large Language Models (MLLMs) prevents the model from revealing private information when queried about target images. Existing MLLM unlearning methods largely adopt approaches developed for LLMs. They treat all answer tokens uniformly, disregarding their varying importance in the unlearning process. Moreover, these methods focus exclusively on the language modality, disregarding visual cues that indicate key tokens in answers. In this paper, after formulating the problem of unlearning in multimodal question answering for MLLMs, we propose Visual-Guided Key-Token Regularization (ViKeR). We leverage irrelevant visual inputs to predict ideal post-unlearning token-level distributions and use these distributions to regularize the unlearning process, thereby prioritizing key tokens. Further, we define key tokens in unlearning via information entropy and discuss ViKeR's effectiveness through token-level gradient reweighting, which amplifies updates on key tokens. Experiments on MLLMU and CLEAR benchmarks demonstrate that our method effectively performs unlearning while mitigating forgetting and maintaining response coherence.

</details>


### [258] [From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning](https://arxiv.org/abs/2601.22028)
*Haoran Tang,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出一种对比表示正则化器 CLReg，通过识别“忘记特征”并将其从“保留特征”中推开，减少忘记与保留知识之间的干扰，从而在最小化对保留特征的改变的前提下提升未学习效果；在不同规模的LLM和多项基准上实验，显示了对忘记—保留表示的纠缠的降低。


<details>
  <summary>Details</summary>
Motivation: 现有的未学习方法多在预测分布层面近似从头训练的行为，容易在表示层面留下忘记概念的痕迹并与保留知识纠缠，造成隐性信息未被彻底移除。需要通过对表示空间进行形状调整来降低忘记与保留间的干扰。

Method: 提出 CLReg，使用对比表示正则化来识别忘记特征，并将其与保留特征分离，推动忘记特征远离保留特征，达到最小化对保留特征的偏移同时降低忘记-保留干扰的目的；并给出代表性理论分析，将表示形状与纠缠降低相关联。

Result: 在多项未学习基准及不同规模的LLM上，CLReg 能降低忘记-保留表示的纠缠，促进现有未学习方法的效果，同时未引入额外的隐私风险。

Conclusion: 论文提供了将表示层面形状化与纠缠降低之间的联系的初步理论洞见，表明通过重新塑造表示空间以移除忘记概念，可以提升未学习的效果，并为未来在表征层面对忘记概念的研究指明方向。

Abstract: Most LLM unlearning methods aim to approximate retrain-from-scratch behaviors with minimal distribution shift, often via alignment-style objectives defined in the prediction space. While effective at reducing forgotten content generation, such approaches may act as suppression: forgotten concepts can persist in representations and remain entangled with retained knowledge. We introduce CLReg, a contrastive representation regularizer that identifies forget features while pushing them away from retain features, explicitly reducing forget-retain interference with minimal shifts on retain features. We provide first theoretical insights that relate representation shaping to entanglement reduction. Across unlearning benchmarks and LLMs of different sizes, CLReg decreases forget-retain representation entanglement that facilitates mainstream unlearning methods without positing extra privacy risks, inspiring future work that reshapes the representation space to remove forget concepts.

</details>


### [259] [The Ensemble Inverse Problem: Applications and Methods](https://arxiv.org/abs/2601.22029)
*Zhengyan Huan,Camila Pazos,Martin Klassen,Vincent Croft,Pierre-Hugues Beauchemin,Shuchin Aeron*

Main category: cs.LG

TL;DR: 提出 Ensemble Inverse Problem (EIP) 与 ensemble inverse generative models，在推断阶段非迭代地产生后验采样，利用观测集的集合信息隐式编码似然，并可推广到未见先验的情形，适用于HEP展开、FWI与逆成像等领域，给出代码实现。


<details>
  <summary>Details</summary>
Motivation: 现实中需要从前向过程的输出分布（按相同前向模型）推断一个集合的后验分布。传统方法往往依赖显式似然和迭代求解，计算成本高且不易泛化至未见先验。需要一种在推断时就可用、且能利用观测集合信息提升鲁棒性和泛化性的非迭代推断方法。

Method: 提出一种新的条件生成模型族，称为 ensemble inverse generative models，用于在推断时构造后验采样。该模型在训练阶段通过跨越多个 truth-observation 对（来自同一前向模型、但先验不同）来隐式编码似然。训练过程不依赖显式前向求解，且通过利用观测集合信息提升后验推断效果。推断阶段实现非迭代的后验采样。

Result: 在合成与真实数据集的逆成像、HEP、FWI等任务上进行基准测试，显示引入观测集合信息的后验推断具有更好的适配性和泛化能力，且对未见先验具鲁棒性；代码可获取。

Conclusion: EIP及其 ensemble inverse generative models 提供了一种非迭代、利用集合信息增强的后验推断框架，具有广泛适用性与良好泛化性，适用于多领域的前向模型推断问题。

Abstract: We introduce a new multivariate statistical problem that we refer to as the Ensemble Inverse Problem (EIP). The aim of EIP is to invert for an ensemble that is distributed according to the pushforward of a prior under a forward process. In high energy physics (HEP), this is related to a widely known problem called unfolding, which aims to reconstruct the true physics distribution of quantities, such as momentum and angle, from measurements that are distorted by detector effects. In recent applications, the EIP also arises in full waveform inversion (FWI) and inverse imaging with unknown priors. We propose non-iterative inference-time methods that construct posterior samplers based on a new class of conditional generative models, which we call ensemble inverse generative models. For the posterior modeling, these models additionally use the ensemble information contained in the observation set on top of single measurements. Unlike existing methods, our proposed methods avoid explicit and iterative use of the forward model at inference time via training across several sets of truth-observation pairs that are consistent with the same forward model, but originate from a wide range of priors. We demonstrate that this training procedure implicitly encodes the likelihood model. The use of ensemble information helps posterior inference and enables generalization to unseen priors. We benchmark the proposed method on several synthetic and real datasets in inverse imaging, HEP, and FWI. The codes are available at https://github.com/ZhengyanHuan/The-Ensemble-Inverse-Problem--Applications-and-Methods.

</details>


### [260] [Per-parameter Task Arithmetic for Unlearning in Large Language Models](https://arxiv.org/abs/2601.22030)
*Chengyi Cai,Zesheng Ye,Jiangchao Yao,Jianzhong Qi,Bo Han,Xiaolu Zhang,Feng Liu,Jun Zhou*

Main category: cs.LG

TL;DR: PerTA introduces per-parameter task arithmetic to modulate task-vector subtraction for LLM unlearning, reducing over-forgetting while preserving efficiency; it uses per-parameter importance estimates via gradients or Fisher information; it improves forgetting and utility over standard TV and matches/trumps training-based unlearning in many cases.


<details>
  <summary>Details</summary>
Motivation: Standard task arithmetic (TV) treats all parameters equally, which can disrupt parameters essential for retaining other information and cause over-forgetting. A parameter-wise importance measure is needed to balance forgetting and retention.

Method: Apply per-parameter scaling to the task vector (TV). Estimate parameter importance for forgetting vs retention via (a) gradients (PerTA-grad) or (b) diagonal Fisher information (PerTA-fisher). Extend to a general form and discuss analysis.

Result: PerTA consistently improves standard TV; in many cases it surpasses training-based unlearning methods in forgetting effectiveness and overall model utility, while retaining the efficiency of task arithmetic.

Conclusion: PerTA provides a principled and practical framework for efficient, controlled LLM unlearning by adjusting the TV at the parameter level, with theoretical discussion and empirical validation.

Abstract: In large language model (LLM) unlearning, private information is required to be removed. Task arithmetic unlearns by subtracting a specific task vector (TV)--defined as the parameter difference between a privacy-information-tuned model and the original model. While efficient, it can cause over-forgetting by disrupting parameters essential for retaining other information. Motivated by the observation that each parameter exhibits different importance for forgetting versus retention, we propose a per-parameter task arithmetic (PerTA) mechanism to rescale the TV, allowing per-parameter adjustment. These weights quantify the relative importance of each parameter for forgetting versus retention, estimated via gradients (i.e., PerTA-grad) or the diagonal Fisher information approximation (i.e., PerTA-fisher). Moreover, we discuss the effectiveness of PerTA, extend it to a more general form, and provide further analysis. Extensive experiments demonstrate that PerTA consistently improves upon standard TV, and in many cases surpasses widely used training-based unlearning methods in both forgetting effectiveness and overall model utility. By retaining the efficiency of task arithmetic while mitigating over-forgetting, PerTA offers a principled and practical framework for LLM unlearning.

</details>


### [261] [Holographic generative flows with AdS/CFT](https://arxiv.org/abs/2601.22033)
*Ehsan Mirafzali,Sanjit Shashi,Sanya Murdeshwar,Edgar Shaghoulian,Daniele Venturi,Razvan Marinescu*

Main category: cs.LG

TL;DR: 将 AdS/CFT 的 bulk-to-boundary 映射引入流式匹配框架，提升生成模型的收敛速度与质量，并为流式匹配提供物理解释与几何直觉。


<details>
  <summary>Details</summary>
Motivation: 探求使用全息原理来增强深度生成模型的表达能力、稳定性和可解释性，并评估 AdS 几何在数据变换中的应用潜力。

Method: 把数据从基分布到学习分布的变换建模为 AdS 中标量场的 bulk-to-boundary 映射，并将 AdS 物理学整合进流式匹配算法；在棋盘数据集和 MNIST 上进行实验比较。

Result: 与不含物理先验的流式匹配模型相比，所提框架实现更快的收敛和更高的生成质量；提供一个物理可解释的流式匹配版本。

Conclusion: 展示 AdS 物理与几何在生成建模中的潜在应用，提出在未来的研究中将全息原理用于设计新的生成模型与训练范式的可能性。

Abstract: We present a framework for generative machine learning that leverages the holographic principle of quantum gravity, or to be more precise its manifestation as the anti-de Sitter/conformal field theory (AdS/CFT) correspondence, with techniques for deep learning and transport theory. Our proposal is to represent the flow of data from a base distribution to some learned distribution using the bulk-to-boundary mapping of scalar fields in AdS. In the language of machine learning, we are representing and augmenting the flow-matching algorithm with AdS physics. Using a checkerboard toy dataset and MNIST, we find that our model achieves faster and higher quality convergence than comparable physics-free flow-matching models. Our method provides a physically interpretable version of flow matching. More broadly, it establishes the utility of AdS physics and geometry in the development of novel paradigms in generative modeling.

</details>


### [262] [Cross-Fusion Distance: A Novel Metric for Measuring Fusion and Separability Between Data Groups in Representation Space](https://arxiv.org/abs/2601.22036)
*Xiaolong Zhang,Jianwei Zhang,Xubo Song*

Main category: cs.LG

TL;DR: CFD基于跨融合距离的新度量，旨在从表示空间中 isolating 融合几何因素，抗消融融合保留因素（全局缩放、采样布局等），线性复杂度，理论性分析与合成实验验证，在实际域迁移数据中与下游泛化下降更相关，提供可解释的表征学习距离度量。


<details>
  <summary>Details</summary>
Motivation: 在域迁移背景下，度量不同数据组在表示空间的融合程度与可分离性时，需要区分受几何位移等因素影响的融合变化与由缩放/布局等因素引起的保持性变化。现有分布距离混淆这两类因素，导致对真实融合程度的表征不准确。

Method: 提出跨融合距离 CFD，定量提取仅受融合几何变化影响的分布差异，具有对融合保持因素的不敏感性。给出理论性质（不变性/敏感性）、推导与证明，提供线性时间复杂度实现，结合合成数据进行控制实验，并在含域迁移的实际数据上与常用替代方法比较，分析与下游泛化下降的相关性。

Result: CFD 能分离融合相关几何变化，抗尺度和布局变化等保留因素，具有线性复杂度。理论分析与合成实验验证其不变性与灵敏度。实证结果显示 CFD 更接近下游泛化下降的程度，相较于主流替代方法更具解释性与实用性。

Conclusion: CFD 为表示学习提供一个理论扎实、可解释的距离度量，专注于融合几何变化，鲁棒于保留性变换，适用于含域迁移的场景。

Abstract: Quantifying degrees of fusion and separability between data groups in representation space is a fundamental problem in representation learning, particularly under domain shift. A meaningful metric should capture fusion-altering factors like geometric displacement between representation groups, whose variations change the extent of fusion, while remaining invariant to fusion-preserving factors such as global scaling and sampling-induced layout changes, whose variations do not. Existing distributional distance metrics conflate these factors, leading to measures that are not informative of the true extent of fusion between data groups. We introduce Cross-Fusion Distance (CFD), a principled measure that isolates fusion-altering geometry while remaining robust to fusion-preserving variations, with linear computational complexity. We characterize the invariance and sensitivity properties of CFD theoretically and validate them in controlled synthetic experiments. For practical utility on real-world datasets with domain shift, CFD aligns more closely with downstream generalization degradation than commonly used alternatives. Overall, CFD provides a theoretically grounded and interpretable distance measure for representation learning.

</details>


### [263] [Making Foundation Models Probabilistic via Singular Value Ensembles](https://arxiv.org/abs/2601.22068)
*Mehmet Ozgur Turkoglu,Dominik J. Mühlematter,Alexander Becker,Konrad Schindler,Helge Aasen*

Main category: cs.LG

TL;DR: SVE leverages fixed weight singular vectors as a shared knowledge basis and trains per-member singular values to form a parameter-efficient ensemble, achieving comparable uncertainty calibration to deep ensembles with <1% base-model parameter increase.


<details>
  <summary>Details</summary>
Motivation: Foundation models struggle with overconfident predictions; standard ensembles are computationally expensive, especially for large models. A lightweight ensemble method is needed to quantify epistemic uncertainty efficiently.

Method: Take pretrained models, freeze the weight matrix singular vectors (orthogonal knowledge directions), and train per-member singular values to rescale the contribution of each direction. Ensemble diversity arises from stochastic initialization and mini-batch sampling during joint training, with only marginal parameter overhead (~<1%).

Result: SVE yields uncertainty quantification comparable to explicit deep ensembles on NLP and vision tasks across different backbones, while improving calibration and maintaining predictive accuracy.

Conclusion: SVE provides a practical, scalable approach for principled uncertainty estimation in foundation models with minimal parameter overhead, enabling resource-efficient deployment.

Abstract: Foundation models have become a dominant paradigm in machine learning, achieving remarkable performance across diverse tasks through large-scale pretraining. However, these models often yield overconfident, uncalibrated predictions. The standard approach to quantifying epistemic uncertainty, training an ensemble of independent models, incurs prohibitive computational costs that scale linearly with ensemble size, making it impractical for large foundation models. We propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that builds on a simple, but powerful core assumption: namely, that the singular vectors of the weight matrices constitute meaningful subspaces of the model's knowledge. Pretrained foundation models encode rich, transferable information in their weight matrices. If the singular vectors are indeed meaningful (orthogonal) "knowledge directions". To obtain a model ensemble, we modulate only how strongly each direction contributes to the output. Rather than learning entirely new parameters, we freeze the singular vectors and only train per-member singular values that rescale the contribution of each direction in that shared knowledge basis. Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations of the same underlying knowledge. SVE achieves uncertainty quantification comparable to explicit deep ensembles while increasing the parameter count of the base model by less than 1%, making principled uncertainty estimation accessible in resource-constrained settings. We validate SVE on NLP and vision tasks with various different backbones and show that it improves calibration while maintaining predictive accuracy.

</details>


### [264] [Where Do the Joules Go? Diagnosing Inference Energy Consumption](https://arxiv.org/abs/2601.22076)
*Jae-Won Chung,Ruofan Wu,Jeff J. Ma,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Large-scale empirical study of energy and inference time across the generative AI landscape: 46 models, 7 tasks, and 1,858 configurations on NVIDIA H100 and B200 GPUs; reveals order-of-magnitude energy differences by task, model, and utilization; proposes a framework linking time/energy to latent metrics like memory and utilization, applicable to throughput per watt.


<details>
  <summary>Details</summary>
Motivation: Energy has become a critical ML computing resource; measuring alone is insufficient. There is a need to diagnose the mechanisms that drive energy/time differences to enable optimization across algorithms, software, and hardware.

Method: Large-scale measurement across 46 models, 7 tasks, and 1,858 configurations on NVIDIA H100 and B200 GPUs; measure inference time and energy; analyze how task type, model, and configuration affect energy and time; develop a framework that relates observed metrics to latent factors across stack.

Result: Found order-of-magnitude energy variations: LLM task type can cause ~25x energy differences; video generation can exceed 100x energy of image generation; GPU utilization differences yield ~3–5x energy differences. Propose a framework where time/energy are governed by latent metrics such as memory usage and utilization, shaped by algorithm, software, and hardware layers; framework extends to throughput per watt.

Conclusion: The proposed framework provides a diagnostic tool linking performance and energy to latent system metrics, enabling better optimization for power-constrained datacenters and straightforward estimation of throughput per watt.

Abstract: Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\times$ energy differences, video generation sometimes consumes more than 100$\times$ the energy of images, and GPU utilization differences can result in 3--5$\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.

</details>


### [265] [Latent Adversarial Regularization for Offline Preference Optimization](https://arxiv.org/abs/2601.22083)
*Enyi Jiang,Yibo Jacky Zhang,Yinglun Xu,Andreas Haupt,Nancy Amato,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 提出在语言模型偏好优化中引入潜在表示正则化的GANPO方法，通过对齐策略模型与参考模型的潜在表示来正则化学习，以增强鲁棒性和稳定性，同时在多任务/多架构下实现了性能相近但对分布漂移和噪声更稳健的效果，且开销微小。


<details>
  <summary>Details</summary>
Motivation: token-space 相似性并不等同于语义或行为相似性，使得基于标记层的偏好优化在语言模型中容易产生错误的梯度信号。需要通过潜在空间正则化来更好地对齐策略行为与人类偏好。

Method: 提出 GANPO：通过对策略模型与参考模型的内部潜在表示之间的分布差异进行惩罚来实现潜在空间正则化。由于潜在表示缺乏显式概率密度，采用受 GAN 启发的对抗训练来最小化潜在空间分歧，并把 GANPO 作为现有离线偏好优化目标的正则项嵌入。

Result: 在多种模型架构和任务上实现了一致的性能提升；对比基于 token-level 的正则化，GANPO 在分布漂移与噪声条件下提供更稳健的结构性反馈，且下游性能相当，计算开销仅有轻微增加。

Conclusion: 通过引入潜在空间正则化的 GANPO，可以在离线偏好优化中获得更稳定的学习信号与鲁棒性，同时在保持性能的前提下缓解 token-level 正则化的局限性。

Abstract: Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.

</details>


### [266] [Boosting CVaR Policy Optimization with Quantile Gradients](https://arxiv.org/abs/2601.22100)
*Yudong Luo,Erick Delage*

Main category: cs.LG

TL;DR: 将 CVaR-PG 与一个期望分位数项相结合；对分位数进行动态规划求解以提升样本利用效率；保持 CVaR 目标不变；在风险规避任务中实验性结果显示显著提升。


<details>
  <summary>Details</summary>
Motivation: CVaR-PG 的样本利用效率低下，因为其优化聚焦于尾部表现，忽略了大量采样轨迹。需要在不改变 CVaR 目标的前提下，充分利用所有样本数据。

Method: 在 CVaR 中增加一个期望分位数项，提出分位数优化的动态规划（DP）表述以利用所有采样数据；该方法在马尔可夫策略类中实现，且因为 CVaR 等价于尾部分位数的期望，因此不改变原 CVaR 目标。

Result: 在具有可验证的风险规避行为的领域，实验结果表明该算法相对于 CVaR-PG 有显著改进，并且持续优于现有其他方法。

Conclusion: 将 CVaR-PG 与期望分位数项结合 可在不改变 CVaR 目标的前提下提升样本效率；分位数的 DP 表达法在马尔可夫策略中具有效果。

Abstract: Optimizing Conditional Value-at-risk (CVaR) using policy gradient (a.k.a CVaR-PG) faces significant challenges of sample inefficiency. This inefficiency stems from the fact that it focuses on tail-end performance and overlooks many sampled trajectories. We address this problem by augmenting CVaR with an expected quantile term. Quantile optimization admits a dynamic programming formulation that leverages all sampled data, thus improves sample efficiency. This does not alter the CVaR objective since CVaR corresponds to the expectation of quantile over the tail. Empirical results in domains with verifiable risk-averse behavior show that our algorithm within the Markovian policy class substantially improves upon CVaR-PG and consistently outperforms other existing methods.

</details>


### [267] [Prior-Informed Flow Matching for Graph Reconstruction](https://arxiv.org/abs/2601.22107)
*Harvey Chen,Nicolas Zilberstein,Santiago Segarra*

Main category: cs.LG

TL;DR: PIFM: a conditional flow model for graph reconstruction that augments embedding priors with rectified flow matching, achieving better reconstruction accuracy and global consistency than embedding methods or generative baselines.


<details>
  <summary>Details</summary>
Motivation: Reconstructing graphs from partial observations is challenging: embeddings alone often lack global consistency, while generative models struggle to incorporate effective structural priors. A method that combines priors with a learnable flow can improve reconstruction quality.

Method: Use a permutation-equivariant distortion-perception framework. Start from an informed initial adjacency estimate derived from priors (graphons, GraphSAGE, node2vec). Then apply rectified (continuous-time) flow matching to refine the estimate toward the true distribution of clean graphs and learn a global coupling.

Result: Empirical evaluations across datasets show consistent improvements over classical embeddings and state-of-the-art generative baselines in reconstruction accuracy.

Conclusion: PIFM effectively bridges embedding priors and flow-based generative modeling, enhancing graph reconstruction by leveraging structural priors and global flow with improved accuracy.

Abstract: We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.

</details>


### [268] [Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics](https://arxiv.org/abs/2601.22123)
*Winfried Ripken,Michael Plainer,Gregor Lied,Thorben Frank,Oliver T. Unke,Stefan Chmiela,Frank Noé,Klaus Robert Müller*

Main category: cs.LG

TL;DR: A framework to learn Hamiltonian flow maps by predicting the mean phase-space evolution over a time Δt, enabling stable, large-timestep integration via a mean-flow consistency condition; trainable on trajectory-free samples and applicable to ML-force-field MD.


<details>
  <summary>Details</summary>
Motivation: Overcome the small-timestep bottleneck in simulating long-time Hamiltonian dynamics and reduce data requirements by training without future-trajectory data.

Method: Train a model to predict the time-averaged (mean) phase-space update over Δt, enforce a mean-flow consistency condition, and train on independent samples without needing future states; validated across diverse Hamiltonian systems and ML-force-field MD datasets.

Result: Achieves stable, large-timestep integration; improves molecular dynamics simulations using ML force fields; maintains comparable training/inference cost and can use trajectory-free datasets.

Conclusion: Mean-flow-based learning enables stable long-time Hamiltonian integration at larger timesteps, with broad applicability to ML-driven MD and other Hamiltonian systems, and reduces data requirements by avoiding trajectory generation.

Abstract: Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.

</details>


### [269] [StepShield: When, Not Whether to Intervene on Rogue Agents](https://arxiv.org/abs/2601.22136)
*Gloria Felicia,Michael Eniolade,Jinfeng He,Zitha Sasindran,Hemant Kumar,Milan Hussain Angati,Sandeep Bandarupalli*

Main category: cs.LG

TL;DR: StepShield introduces a temporal benchmark to evaluate 


<details>
  <summary>Details</summary>
Motivation: Existing safety benchmarks report binary accuracy and miss the crucial dimension of when a violation is detected. Early detection enables intervention and reduces costs; current metrics fail to capture this benefit.

Method: Construct StepShield with 9,213 code agent trajectories, including 1,278 annotated training pairs and a 7,935-trajectory test set with 8.1% rogue rate across six categories. Propose three temporal metrics—Early Intervention Rate (EIR), Intervention Gap, Tokens Saved—and evaluate LLM-based judging versus static analysis. Demonstrate a cascaded HybridGuard detector to reduce monitoring costs.

Result: LLM-based judge achieves 59% EIR vs. 26% by static analyzer (2.3x gap not visible to standard accuracy). Cascaded HybridGuard reduces monitoring costs by 75%, projecting $108M in five-year enterprise savings. Dataset/code released under Apache 2.0.

Conclusion: Shifts evaluation from binary correctness to timing of detection, establishing StepShield as a foundation for safer and economically viable AI agents.

Abstract: Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [270] [SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation](https://arxiv.org/abs/2601.21105)
*Joyce Zhou,Weijie Zhou,Doug Turnbull,Thorsten Joachims*

Main category: cs.IR

TL;DR: 提出 SteerEval，用于评估自然语言的推荐系统的 steerability。通过从电影类型到内容警告等干预，评估一系列预训练的自然语言推荐器的跟随能力，比较不同的用户画像和干预方式对 steerability 的影响，并给出设计建议与未来工作方向。


<details>
  <summary>Details</summary>
Motivation: 现有 steerability 基准多聚焦于显性属性（如电影类型），无法覆盖更丰富的用户控制形式。自然语言用户画像能够让用户直接编辑偏好，但尚不清楚现有方法能否按照这些指令进行 steering。因此需要一个更丰富的评估框架来衡量可 steerability 的程度。

Method: 提出 SteerEval 框架，设计涵盖从 genres 到 content-warning 的干预形式，以量化对比不同的画像干预与推荐干预对 steerability 的影响；在一组预训练的自然语言推荐器上进行评估，探讨对较小众话题的 steerability 潜力与局限性，并比较不同干预组合的效果；最后给出基于实验的设计建议与未来路线。

Result: 实验表明 steerability 的有效性在不同干预之间存在差异，较小众主题的 steerability 更具挑战但也展现潜力；多种干预路径对 steerability 的影响不同，提示需要组合设计以提升控制力。

Conclusion: SteerEval 提供一个更丰富的评估视角以推动可 steerable 的推荐系统设计，未来需要扩展干预类型、提高稳定性并在真实用户情境中验证。

Abstract: Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.

</details>


### [271] [Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance](https://arxiv.org/abs/2601.21611)
*Baopu Qiu,Hao Chen,Yuanrong Wu,Changtong Zan,Chao Wei,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 提出多 perspectives 的 CoT 框架与高效蒸馏方法以提升电商搜索的相关性建模与线上部署效率。


<details>
  <summary>Details</summary>
Motivation: 解决单一视角 CoT 无法覆盖电商相关性多维性（如用户意图、属性匹配、商业规则）以及现有蒸馏在推理阶段丢失 CoT 结构的问题。

Method: 教师模型使用多视角 CoT (MPCoT) 生成多样化推理理由，并结合最监督微调 (SFT) 与直接偏好优化 (DPO) 进行训练；提出潜在推理知识蒸馏 (LRKD)，使学生模型在推理时具备轻量级的潜在推理提取器，低延迟地内部化 LLM 的推理能力。

Result: 在离线实验和面向日均数千万用户的电商搜索广告平台上进行的在线 A/B 测试中，方法表现出显著的离线增益，并在商业绩效与用户体验方面显示明显收益。

Conclusion: MPCoT+SFT+DPO 与 LRKD 的组合有效提高多维相关性建模与实时部署效率，具备良好的商业化落地潜力。

Abstract: Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.

</details>


### [272] [Influence Guided Sampling for Domain Adaptation of Text Retrievers](https://arxiv.org/abs/2601.21759)
*Meet Doshi,Vishwajeet Kumar,Yulong Li,Jaydeep Sen*

Main category: cs.IR

TL;DR: Inf-DDS: 一个基于强化学习的自适应数据采样框架，用于改进开放域密集检索的训练数据权重分配，在目标开发集上提升性能，且计算成本显著降低（1.5x-4x）。在多语言bge-m3和全MiniLM-L6-v2上分别实现了显著的NDCG@10提升。


<details>
  <summary>Details</summary>
Motivation: 现有的开域密集检索系统在训练数据采样上往往采用均匀、按实例规模或依赖人工专家的方法，且对嵌入式模型的影响尚未充分研究。采样策略对模型性能影响显著，但缺乏低成本、有效的自适应方法。

Method: 提出 Inf-DDS，这是一种强化学习驱动的采样框架，通过基于影响力的奖励信号对训练数据集进行动态重新加权，迭代优化采样策略，优先选择能提升目标开发集性能的数据集。相较于现有的基于梯度的采样方法，Inf-DDS 在GPU资源上的消耗更低。

Result: 在广泛文本检索任务上验证了该策略的有效性，显示出强劲的检索性能提升和更好的适应性。具体地，在多语言 bge-m3 上实现了 5.03 的绝对 NDCG@10 提升，在 all-MiniLM-L6-v2 上实现了 0.94 的绝对 NDCG@10 提升（即使从专家分配权重的初始状态开始）。与现有方法相比，计算成本更低，且对数据集的选择更具适应性。

Conclusion: 基于强化学习的自适应数据采样能够显著提升开域密集检索的性能，并在不同模型与数据集上表现出良好的通用性，同时显著降低训练的计算开销。

Abstract: General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.

</details>


### [273] [OneMall: One Model, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce](https://arxiv.org/abs/2601.21770)
*Kun Zhang,Jingming Zhang,Wei Cheng,Yansong Cheng,Jiaqi Zhang,Hao Lu,Xu Zhang,Haixiang Gan,Jiangxia Cao,Tenglong Wang,Ximing Zhang,Boyang Xia,Kuo Cai,Shiyao Wang,Hongjian Dou,Jinkai Yu,Mingxing Wen,Qiang Luo,Dongxu Liang,Chenyi Lei,Jun Wang,Runan Liu,Zhaojie Liu,Ruiming Tang,Tingting Gao,Shaoguo Liu,Yuqing Ding,Hui Kong,Han Li,Guorui Zhou,Wenwu Ou,Kun Gai*

Main category: cs.IR

TL;DR: OneMall 是一个面向电商场景的端到端生成式推荐框架，统一商品卡、短视频与直播三类分发场景，通过电商语义分词、Transformer 架构（包括 Query-Former、Cross-Attention、Sparse MoE）以及强化学习管线实现端到端优化，在线性能显著提升并已部署在 KUai的日活 4 亿规模用户上。


<details>
  <summary>Details</summary>
Motivation: 解决跨场景、跨行为的分发策略协同与端到端优化难题，利用与LLM相似的预训练/微调范式提升模型的一致性、扩展性与在线表现，同时实现从检索到排序的闭环优化。

Method: 三大核心组件：1) 电商语义分词器，捕捉跨场景的真实语义与商品关系；2) Transformer 架构，采用 Query-Former 进行长序列压缩、Cross-Attention 融合多行为序列、Sparse MoE 实现可扩展的自回归生成；3) 强化学习管线，将检索与排序模型对齐，通过排序模型输出作为端到端策略检索的奖励信号进行优化。

Result: 在产品卡、短视频、直播三个场景上实现显著在线指标提升：GMV/订单分别提升 13.01%、15.32%、2.78%；并已部署上线，服务日活超过 4 亿。

Conclusion: OneMall 展示了端到端生成式推荐在多场景电商中的可行性与优势，具有显著的在线收益与落地部署，但需进一步披露对比基线、鲁棒性、可重复性及潜在风险的评估。

Abstract: In the wave of generative recommendation, we present OneMall, an end-to-end generative recommendation framework tailored for e-commerce services at Kuaishou. Our OneMall systematically unifies the e-commerce's multiple item distribution scenarios, such as Product-card, short-video and live-streaming. Specifically, it comprises three key components, aligning the entire model training pipeline to the LLM's pre-training/post-training: (1) E-commerce Semantic Tokenizer: we provide a tokenizer solution that captures both real-world semantics and business-specific item relations across different scenarios; (2) Transformer-based Architecture: we largely utilize Transformer as our model backbone, e.g., employing Query-Former for long sequence compression, Cross-Attention for multi-behavior sequence fusion, and Sparse MoE for scalable auto-regressive generation; (3) Reinforcement Learning Pipeline: we further connect retrieval and ranking models via RL, enabling the ranking model to serve as a reward signal for end-to-end policy retrieval model optimization. Extensive experiments demonstrate that OneMall achieves consistent improvements across all e-commerce scenarios: +13.01\% GMV in product-card, +15.32\% Orders in Short-Video, and +2.78\% Orders in Live-Streaming. OneMall has been deployed, serving over 400 million daily active users at Kuaishou.

</details>


### [274] [The Double-Edged Sword of Knowledge Transfer: Diagnosing and Curing Fairness Pathologies in Cross-Domain Recommendation](https://arxiv.org/abs/2601.21805)
*Yuhan Zhao,Weixin Chen,Li Chen,Weike Pan*

Main category: cs.IR

TL;DR: CDFA通过两端机制缓解跨域推荐中的不公平：降低跨域差异传递并重分配跨域信息收益，从而在不损失乃至提升整体推荐效果的前提下减少群体层面的不公平。


<details>
  <summary>Details</summary>
Motivation: 跨域推荐在提升性能的同时可能放大源域的群体差异并导致不同群体获得不均等的潜在收益，需要理论分析与实证验证以设计公平的跨域方法。

Method: 提出CDFA框架，包含两大组件：1) 自适应地引入未标注数据以平衡不同群体训练信号的信息量，减轻跨域 disparity transfer；2) 以信息论方法重新分配跨域信息增益，确保不同群体获得更公平的利益。

Result: 在多数据集和基线下的实验显示，CDFA显著降低CDR中的不公平性，同时维持甚至提升总体推荐性能。

Conclusion: CDFA有效缓解跨域推荐中的不公平问题，证明在跨域学习场景中通过信息平等化与收益再分配可实现公平性与性能的双赢，并具有潜在的广泛适用性。

Abstract: Cross-domain recommendation (CDR) offers an effective strategy for improving recommendation quality in a target domain by leveraging auxiliary signals from source domains. Nonetheless, emerging evidence shows that CDR can inadvertently heighten group-level unfairness. In this work, we conduct a comprehensive theoretical and empirical analysis to uncover why these fairness issues arise. Specifically, we identify two key challenges: (i) Cross-Domain Disparity Transfer, wherein existing group-level disparities in the source domain are systematically propagated to the target domain; and (ii) Unfairness from Cross-Domain Information Gain, where the benefits derived from cross-domain knowledge are unevenly allocated among distinct groups. To address these two challenges, we propose a Cross-Domain Fairness Augmentation (CDFA) framework composed of two key components. Firstly, it mitigates cross-domain disparity transfer by adaptively integrating unlabeled data to equilibrate the informativeness of training signals across groups. Secondly, it redistributes cross-domain information gains via an information-theoretic approach to ensure equitable benefit allocation across groups. Extensive experiments on multiple datasets and baselines demonstrate that our framework significantly reduces unfairness in CDR without sacrificing overall recommendation performance, while even enhancing it.

</details>


### [275] [SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation](https://arxiv.org/abs/2601.21986)
*Yu Cui,Feng Liu,Zhaoxiang Wang,Changwang Zhang,Jun Wang,Can Wang,Jiawei Chen*

Main category: cs.IR

TL;DR: SpecTran通过谱域注意力和可学习的谱位置编码，在高维文本嵌入与 SR 模型之间搭建一个谱-aware 的适配器，利用全谱信息，缓解适配器的维度崩溃和 SVD 方法的信息丢失问题，实现对四个数据集、三种 SR 骨架的平均约9.17%提升。


<details>
  <summary>Details</summary>
Motivation: 传统的序列推荐忽略文本信息，LLM 产生的高维文本嵌入往往需要有效整合。现有的 adapter-based 方法易出现维度崩溃，SVD-based 方法则过于僵化、仅保留少数主谱分量，丢失丰富信息。需开发能充分利用谱信息的融合机制。

Method: 提出 SpecTran，一种在谱域工作的 transformer-like 适配器，利用对整个谱信息的自注意力机制进行信息聚合；引入可学习的谱位置编码，将 singular-value 信息作为归纳偏置，引导注意力聚焦于显著的谱分量，并提升嵌入维度的多样性。该方法与现有 SR 骨架兼容，可直接对接由 LLM 产生的文本嵌入。

Result: 在四个真实数据集和三种 SR 骨架上，持续优于强 baselines，平均提升约9.17%。

Conclusion: 提供了一种谱域-aware 的适配策略，充分利用全文谱信息，提升文本嵌入对 SR 的贡献的同时保持维度多样性，对跨骨架和数据集具有鲁棒性，未来可进一步探索谱信息的更多先验。

Abstract: Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.
  To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.

</details>


### [276] [LANCER: LLM Reranking for Nugget Coverage](https://arxiv.org/abs/2601.22008)
*Jia-Huei Ju,François G. Landry,Eugene Yang,Suzan Verberne,Andrew Yates*

Main category: cs.IR

TL;DR: LANCER: an LLM-based reranker for long-form RAG that optimizes nugget coverage by predicting sub-questions and selecting documents to cover as many information nuggets as possible; shows improvements in nugget coverage metrics (α-nDCG, information coverage); sub-question generation is essential according to oracle analysis.


<details>
  <summary>Details</summary>
Motivation: Long-form retrieval requires broad information coverage, not just relevance ranking. Existing retrievers optimize relevance but may miss information nuggets needed for a comprehensive report.

Method: LANCER predicts what sub-questions should be answered to satisfy information needs, predicts which documents answer these sub-questions, and reranks documents to maximize coverage of information nuggets.

Result: Empirical evaluations show higher nugget coverage metrics (α-nDCG and information coverage) than other LLM-based reranking baselines.

Conclusion: Sub-question generation is crucial; LANCER enhances retrieval quality by focusing on information nugget coverage in long-form RAG.

Abstract: Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $α$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.

</details>


### [277] [Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature](https://arxiv.org/abs/2601.07533)
*Julian Schelb,Michael Wittweiler,Marie Revellio,Barbara Feichtinger,Andreas Spitz*

Main category: cs.IR

TL;DR: 提出 Loci Similes，一套拉丁文本互文性基准，约含 172k 文段与 545 个专家验证的并列，面向检索与分类的基线评估，基于最先进的语言模型。


<details>
  <summary>Details</summary>
Motivation: 填补互文性检测缺乏标准化基准和易用数据集的空白，提升通过语义相似性而非词汇重叠来检测文本之间的相互影响能力，从而有助于重建作者的虚拟图书馆。

Method: 构建并整理带有专家验证的并列文本数据集 Loci Similes，连接晚期古代作者与经典作者；基于该数据集建立文本检索与互文性分类的基线，使用最先进的语言模型进行评估。

Result: 提供可用的数据集与基线任务，验证使用当前 SOTA LLMs 进行互文性检索与分类的可行性与效果；未在摘要中给出具体数值。

Conclusion: 该数据集解决了互文性研究中的数据瓶颈，推动以 LLM 为核心的拉丁文本互文性研究的发展，促进未来方法的比较与改进。

Abstract: Tracing connections between historical texts is an important part of intertextual research, enabling scholars to reconstruct the virtual library of a writer and identify the sources influencing their creative process. These intertextual links manifest in diverse forms, ranging from direct verbatim quotations to subtle allusions and paraphrases disguised by morphological variation. Language models offer a promising path forward due to their capability of capturing semantic similarity beyond lexical overlap. However, the development of new methods for this task is held back by the scarcity of standardized benchmarks and easy-to-use datasets. We address this gap by introducing Loci Similes, a benchmark for Latin intertextuality detection comprising of a curated dataset of ~172k text segments containing 545 expert-verified parallels linking Late Antique authors to a corpus of classical authors. Using this data, we establish baselines for retrieval and classification of intertextualities with state-of-the-art LLMs.

</details>
