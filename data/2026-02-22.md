<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.LG](#cs.LG) [Total: 79]
- [cs.AI](#cs.AI) [Total: 49]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: 本研究针对RLVR在非可验证领域（如LLM对齐）的局限性，探索参考引导的LLM评估器作为软验证器的可行性。通过设计增强评估协议和参考引导的自我改进方法，在AlpacaEval和Arena-Hard基准上实现+20.2/+17.1分的性能提升，效果与强奖励模型ArmoRM相当，为非可验证域LLM后训练提供新思路。


<details>
  <summary>Details</summary>
Motivation: RLVR在推理任务中表现优异，但无法直接应用于缺乏真实验证器的非可验证领域（如LLM对齐），存在方法适用性鸿沟，亟需探索新的验证机制以扩展RLVR应用范围。

Method: 设计参考引导的评估协议，利用前沿模型或人工撰写的高质量参考输出来增强LLM评估器的评判能力；基于改进的评判者，构建参考引导的LLM自我改进对齐调优框架，使模型在参考指导下作为评判者实现自我提升。

Result: 参考引导显著提升弱LLM评判者的准确性，强评判者通过高质量参考亦获增强；参考引导自我改进在AlpacaEval/Arena-Hard上相比直接SFT提升+20.2/+17.1分，相比参考自由自我改进提升+5.3/+3.6分。Llama-3-8B-Instruct达到73.1%/58.7%，Qwen2.5-7B达到70.0%/74.1%，性能均与强奖励模型ArmoRM相当。

Conclusion: 参考引导的LLM评估器可作为软验证器有效桥接RLVR在非可验证领域的应用鸿沟，为LLM对齐后训练提供新范式，展现出重要的研究和应用价值。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [2] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

TL;DR: 本研究针对希腊语问答任务中单语与多语大语言模型的研究空白，构建了DemosQA社会文化数据集，开发了内存高效评估框架，并对11个模型在6个希腊语数据集上进行了全面评估，为低资源语言LLM研究提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型研究主要集中于英语等高资源语言，多语模型存在训练数据偏见，可能误判社会文化历史方面。对于低资源语言的单语模型，其在语言特定任务上的效果研究不足，特别是希腊语问答任务存在显著研究空白。

Method: 1) 构建DemosQA数据集：利用社交媒体用户问题和社区审核答案，更好捕捉希腊社会文化特征；2) 开发内存高效LLM评估框架，适配不同问答数据集和语言；3) 在6个人工策划的希腊语问答数据集上，使用3种提示策略对11个单语和多语大语言模型进行广泛评估。

Result: 研究完成了11个单语与多语大语言模型在6个希腊语问答数据集上的系统性评估，涵盖了3种不同提示策略。评估结果揭示了各模型在希腊语特定任务上的性能表现与适用性，为后续研究与应用提供了基准参考。

Conclusion: 该研究通过构建专用数据集和评估框架，填补了希腊语问答任务的研究空白，为低资源语言大模型评估提供了可复现的方法论。开源代码和数据将促进未来研究，推动更公平、文化敏感的NLP技术发展。

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [3] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种基于连续流（flow-based）的去噪语言模型（FLM），通过在one-hot编码上进行欧几里得去噪，并引入简单的时间重参数化来提高训练稳定性和生成质量。通过将FLM蒸馏为流图模型（FMLM），实现了少步生成，在LM1B和OWT数据集上超越了现有的少步语言模型，挑战了离散扩散在离散模态生成建模中必要性的传统假设。


<details>
  <summary>Details</summary>
Motivation: 尽管基于离散扩散的语言模型因其可能比自回归模型提供更快的生成速度而备受关注，但在实践中，它们在少步生成 regime 下表现出样本质量的急剧下降，未能实现这一承诺。本文旨在探索连续流方法在离散模态上的应用，以解决离散扩散在少步生成中的质量问题。

Method: 通过重新审视离散模态上流的基本原理，构建了一个基于流的语言模型（FLM），该模型在one-hot token编码上执行欧几里得去噪。模型通过交叉熵目标预测干净数据来训练，并引入一种简单的时间重参数化方法显著改善训练稳定性和生成质量。进一步将FLM蒸馏为其相关的流图，得到蒸馏流图语言模型（FMLM），实现少步生成。

Result: 在LM1B和OWT语言数据集上，FLM达到了与最先进离散扩散模型相当的生成质量。FMLM在所有方面都优于近期的少步语言模型，其一步生成的质量超过了其他模型的8步生成质量。

Conclusion: 该工作质疑了离散扩散过程对于离散模态生成建模是必要这一广为接受的假设，为大规模加速的基于流的语言建模铺平了道路。

Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [4] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

TL;DR: 针对保险行业数据敏感和监管严格的挑战，本研究利用数百万历史保修索赔数据，提出一种本地化部署的治理感知语言模型组件，通过LoRA微调预训练大语言模型，从非结构化索赔叙述中生成结构化纠正措施建议。多维度评估框架显示，领域特定微调显著优于通用商业大模型和基于提示的方法，约80%的案例达到与真实纠正措施近乎一致的匹配度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用语言任务上表现优异，但在保险等受监管和数据敏感领域的应用仍受限。核心挑战在于如何在不泄露敏感数据的前提下，利用模型提升索赔处理效率，同时满足监管要求。本研究旨在通过本地化部署和领域自适应方法，解决通用模型在实际业务场景中输出分布与真实运营数据不匹配的问题。

Method: 研究采用LoRA（低秩适应）技术对预训练大语言模型进行领域特定微调，聚焦于索赔处理流水线中的初始决策模块。利用数百万历史保修索赔数据训练模型，使其能够从非结构化索赔叙述中生成结构化纠正措施建议。评估采用多维度框架，结合自动化语义相似度指标和人工评价，确保对实用性和预测准确性的全面检验。

Result: 实验结果表明，领域特定微调方法显著优于商用通用大模型和基于提示的解决方案。在约80%的评估案例中，模型生成的纠正措施与真实标准答案达到近乎完全匹配。这证明了该方法在提升理赔调整员决策效率方面的实际效用，同时也展示了其高预测准确性。

Conclusion: 本研究提供了理论和实证证据，证明领域自适应微调能够有效对齐模型输出分布与现实运营数据。该方法是保险应用领域的可靠且可治理的构建模块，为在受监管环境中部署大语言模型提供了可行的技术路径和治理框架。

Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [5] [BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization](https://arxiv.org/abs/2602.16843)
*Ahmed Rafid,Rumman Adib,Fariya Ahmed,Ajwad Abrar,Mohammed Saidul Islam*

Main category: cs.CL

TL;DR: 本文提出BanglaSummEval，一个基于问答的无参考框架，用于评估孟加拉语摘要的事实一致性。该方法通过源文档和摘要自动生成问题与答案，利用单一多语言指令微调语言模型完成问题生成、回答、候选答案抽取和重要性加权，采用BERTScore-Recall进行语义级答案比对，在300个人工撰写的教育和医疗领域摘要上验证，与专家评判呈现强相关性（Pearson r=0.694, Spearman ρ=0.763），为低资源语言提供了可解释的事实一致性评估方案。


<details>
  <summary>Details</summary>
Motivation: 事实一致性评估对高风险的文本摘要应用（如医疗和新闻）至关重要。然而，现有评估指标大多忽略孟加拉语这一广泛使用的低资源语言，且通常依赖参考摘要。这导致孟加拉语摘要系统缺乏可靠的自动化评估工具。

Method: 提出BanglaSummEval框架，采用无参考、问答驱动的方法。使用单一多语言指令微调语言模型统一处理问题生成、问题回答、候选答案抽取和问题重要性加权，通过BERTScore-Recall实现超越表面重叠的语义一致性评估，降低系统复杂度和计算成本。

Result: 在教育和医疗领域的300个人工撰写摘要上验证显示，该框架与专家人工评判具有强相关性，Pearson相关系数达0.694，Spearman相关系数达0.763，同时提供可解释的分步诊断结果。

Conclusion: BanglaSummEval为低资源语言环境下的事实一致性评估提供了实用且透明的解决方案，其统一的单模型设计在保持高性能的同时简化了系统架构，具有较强的应用价值。

Abstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.

</details>


### [6] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本研究首次对濒危的德国美因茨方言Meenzerisch开展自然语言处理研究，构建了一个包含2,351个词汇的数字词典数据集。实验表明大语言模型在方言词义解释（最高准确率6.27%）和词汇生成（最高1.51%）任务上表现极差，即使采用少样本学习和规则提取等改进方法，准确率仍不足10%，凸显了加强德语方言资源建设和技术研究的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 美因茨方言作为德国传统狂欢节语言正濒临消亡，而自然语言处理技术虽在语言保护方面潜力巨大，却从未被应用于该方言。本研究旨在填补这一空白，通过构建NLP就绪的数字资源并探索大语言模型对方言任务的处理能力，为濒危方言的保护与复兴提供技术支撑。

Method: 研究基于Schramm（1966）的传统资源创建了包含2,351个Meenzerisch词汇及其标准德语释义的数字词典数据集。针对该数据集设计了两项实验：一是测试LLM生成方言词义定义的能力，二是测试LLM根据定义生成方言词汇的能力。随后进一步开展少样本学习和规则提取实验，以验证这些方法是否能提升模型性能。

Result: 实验结果显示，现有大语言模型在方言处理任务上表现极其有限：词义定义生成任务的最好模型准确率仅为6.27%，词汇生成任务的最好准确率为1.51%。虽然少样本学习和规则提取方法使性能有所改善，但准确率仍未能突破10%，表明当前技术处理低资源方言仍面临重大挑战。

Conclusion: 研究表明，现有大语言模型难以有效处理Meenzerisch等低资源濒危方言，即使采用先进策略性能提升也极为有限。这揭示了针对德语方言建立更多高质量数字资源和 intensifying 专门化研究的迫切需要，才能充分发挥NLP技术在语言保护中的作用。

Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [7] [ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders](https://arxiv.org/abs/2602.16938)
*Ofer Meshi,Krisztian Balog,Sally Goldman,Avi Caciularu,Guy Tennenholtz,Jihwan Jeong,Amir Globerson,Craig Boutilier*

Main category: cs.CL

TL;DR: 针对LLM用户模拟器的"现实差距"问题，本研究提出ConvApparel数据集及包含反事实验证的综合评估框架，发现数据驱动模拟器在适应性方面显著优于提示基线。


<details>
  <summary>Details</summary>
Motivation: LLM用户模拟器存在严重"现实差距"，导致基于模拟交互优化的对话系统在实际场景中表现不佳。

Method: 设计ConvApparel数据集，采用"好/坏"推荐者的双智能体协议收集多样化对话数据并标注用户满意度；构建融合统计对齐、类人度评分与反事实验证的综合评估框架，以测试模拟器泛化能力。

Result: 实验表明所有模拟器均存在显著现实差距；但数据驱动模拟器在反事实验证中表现更优，能更真实地适应未见行为。

Conclusion: 数据驱动模拟器虽不完美，但蕴含更稳健的用户模型，为提升模拟器真实性提供了有效途径。

Abstract: The promise of LLM-based user simulators to improve conversational AI is hindered by a critical "realism gap," leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both "good" and "bad" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, suggesting they embody more robust, if imperfect, user models.

</details>


### [8] [When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English](https://arxiv.org/abs/2602.16957)
*Hasan Can Biyik,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 本研究通过土耳其语-英语对比实验，揭示跨语言委婉语检测中存在迁移不对称性：语义重叠无法保证正向迁移，低资源土耳其语→英语方向性能可能显著下降，而标签分布差异是核心解释因素。


<details>
  <summary>Details</summary>
Motivation: 委婉语作为文化语用敏感表达，其跨语言建模面临重大挑战。现有跨语言迁移研究多关注语义层面，忽视语用功能对齐的影响，导致在低资源语言场景下迁移效果不可预测。

Method: 构建土耳其语和英语潜在委婉语（PETs）语料，依据功能、语用和语义三维对齐标准将其划分为重叠PETs（OPETs）和非重叠PETs（NOPETs），系统对比不同子集的跨语言迁移性能。

Result: 发现三方面关键现象：(1)迁移不对称显著，土耳其语→英语低资源方向性能衰减；(2)OPETs迁移不一定优于NOPETs，存在反直觉性能提升；(3)标签分布差异是主要解释变量，领域特定对齐影响受数据稀疏限制。

Conclusion: 研究证实标签分布而非语义重叠主导跨语言委婉语迁移效果，为低资源语言处理提供新视角，强调语用功能对齐在跨语言迁移研究中的重要性。

Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.

</details>


### [9] [Large Language Models Persuade Without Planning Theory of Mind](https://arxiv.org/abs/2602.17045)
*Jared Moore,Rasmus Overmark,Ned Cooper,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: 本文通过创新的交互式说服任务评估大语言模型的心理理论能力，发现LLM在需要推断他人心理状态时表现不佳，但能通过修辞策略有效说服人类，警示不应将人类式心理理论归于LLM。


<details>
  <summary>Details</summary>
Motivation: 现有心理理论评估多采用静态非交互式问答基准，但理论研究表明第一人称互动是心理理论的核心，此类观察性任务可能无法真正评估心理理论能力。

Method: 开发新型说服任务，要求智能体通过策略性信息揭示来说服目标选择特定政策方案。任务考察对目标知识状态和动机状态的敏感性，设置"已揭示"与"隐藏"两种条件。通过三个实验：被试说服理性机器人、人类角色扮演目标、测量人类目标真实信念变化，对比LLM与人类表现。

Result: 实验1显示LLM在已揭示条件下表现优异，但在隐藏条件下低于随机水平，表明其难以进行多步规划以推断和使用心理状态信息；人类在两种条件下表现中等但稳定。实验2和3发现，在与人类目标互动的所有条件下，LLM均超越人类说服者，说明LLM可通过修辞策略而非心理理论推理实现有效说服。

Conclusion: 有效说服可在无需显式心理理论推理的情况下实现（如通过修辞策略），LLM在此类说服中表现卓越。研究警示不应将人类式心理理论归于LLM，同时凸显了LLM影响人类信念与行为的潜力。

Abstract: A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.

</details>


### [10] [Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data](https://arxiv.org/abs/2602.17051)
*Deepak Uniyal,Md Abul Bashar,Richi Nayak*

Main category: cs.CL

TL;DR: 本研究以氢能源为案例，分析2013-2022年超过900万条英、日、印地、韩语推文，针对关键词采集导致的噪声问题，比较四种跨语言分类方法（翻译标注数据、翻译未标注数据、多语言预训练模型、混合策略）的过滤效果，并通过主题建模揭示核心议题，为优化跨语言社交媒体分析提供实践洞见。


<details>
  <summary>Details</summary>
Motivation: 多语言社交媒体语篇分析仍是自然语言处理的核心挑战，尤其当大规模公共辩论跨越语言边界时。基于关键词的数据采集会产生大量无关内容，亟需可靠的跨语言文本分类方法来过滤噪声并支持全球对话分析。

Method: 研究采用氢能源案例，构建2013-2022年英、日、印地、韩语推文数据集（超900万条）。设计四种过滤策略：1）将英语标注数据翻译至目标语言训练单语模型；2）将所有语言未标注数据译为英语训练统一英语模型；3）直接使用英语微调的多语言transformer模型；4）混合策略整合翻译标注与多语言训练。评估各方法过滤氢能源相关推文的效果，并对筛选结果进行主题建模。

Result: 实验揭示翻译与多语言方法间的关键权衡，不同方法在噪声过滤和主题提取上表现各异，为大规模社交媒体跨语言分析提供了可操作的流程优化见解。

Conclusion: 该研究证实跨语言文本分类在社交媒体分析中的有效性，强调需根据应用场景在翻译策略与多语言模型间进行权衡，对构建全球化社交媒体监测管道具有重要指导价值。

Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.

</details>


### [11] [ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning](https://arxiv.org/abs/2602.17054)
*Hussein S. Al-Olimat,Ahmad Alshareef*

Main category: cs.CL

TL;DR: 本文提出ALPS（阿拉伯语语言语用套件），这是一个由阿拉伯语语言学专家策展的原生诊断挑战集，包含531个严格设计的问题，涵盖15个任务和47个子任务，重点检测深层语义与语用能力。研究评估了23个模型，发现尽管模型生成流畅，但在词法-句法依存关系上失败率高达36.5%（尤其在依赖变音符号的任务中），且商业模型与阿拉伯语原生模型之间存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语NLP基准过度追求规模和多任务覆盖，却普遍依赖合成或翻译数据，缺乏对语言深层结构的严格验证。本研究旨在构建一个文化真实、专家策展的原生诊断工具，以揭示模型在核心语言理解能力上的不足，弥补大规模基准在深度语言学评估上的空白。

Method: 研究团队基于深厚的阿拉伯语语言学专长，构建了ALPS数据集，包含531个专家严格设计的问题，分布在15个任务和47个子任务中。随后对23个多样化模型（包括商业、开源及阿拉伯语原生模型）进行系统评估，以单次人工表现（平均84.6%准确率）和经专家仲裁的上限（99.2%）作为性能基准，重点分析模型在词法-句法依存与组合语义上的表现差异。

Result: 实验结果显示关键性分离：模型虽具高流畅性，但在基础词法-句法依存关系上平均错误率达36.5%，显著高于组合语义任务。顶级商业模型Gemini-3-flash（94.2%）超越人类平均水平，但最优阿拉伯语专用模型Jais-2-70B（83.6%）仍与人工表现存在差距，商业大模型与阿拉伯语原生模型间存在明显性能鸿沟。

Conclusion: 阿拉伯语NLP模型存在流畅性与深层语言理解的关键脱节，词法-句法处理能力严重不足。研究揭示了当前模型过度依赖表面统计规律而忽视语言结构本质，商业模型与阿拉伯语原生模型之间的显著差距表明，单纯增加参数规模或数据量无法解决深层语言学挑战，未来研究需转向更具语言意识的架构设计。

Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.

</details>


### [12] [BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios](https://arxiv.org/abs/2602.17072)
*Yunseung Lee,Subin Kim,Youngjun Kwak,Jaegul Choo*

Main category: cs.CL

TL;DR: 针对银行场景中LLMs计算准确性低的问题，本文提出BankMathBench数据集，包含三个难度级别。实验表明，开源LLMs在该数据集上训练后，公式生成和数值推理准确率显著提升，工具增强微调后准确率比零样本基线平均提升57.6-75.1个百分点。


<details>
  <summary>Details</summary>
Motivation: 数字银行中使用的LLMs在核心银行计算（如总付款估算、利率比较、提前还款利息计算）上准确率仍较低。现有数学数据集侧重基础数学问题，金融基准主要针对金融文档，缺乏对日常银行场景的评估，导致LLMs的系统性错误未被充分捕捉。

Method: 设计BankMathBench领域特定数据集，按难度分为三级：基础级（单产品推理）、中级（多产品比较）、高级（多条件场景）。通过在该数据集上训练开源LLMs，并采用工具增强微调方法，评估模型在银行数值推理任务上的表现。

Result: 训练后开源LLMs在公式生成和数值推理准确率上均有显著提升。工具增强微调使模型在基础、中级、高级三个级别上的平均准确率分别提升57.6、75.1和62.9个百分点，显著优于零样本基线。

Conclusion: BankMathBench可作为评估和提升LLMs在真实银行场景中数值推理能力的可靠基准，有效弥补了现有基准在银行日常计算任务上的不足。

Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.

</details>


### [13] [The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI](https://arxiv.org/abs/2602.17127)
*Dusan Bosnjakovic*

Main category: cs.CL

TL;DR: 本文开发了一种基于心理测量理论的新型审计框架，用于检测大语言模型在集成化部署中的持久性行为特征。通过强制选择序数小插图与混合线性模型分析，研究发现九大主流模型存在显著的提供商级别"实验室信号"，这些潜在偏见在递归评估循环中可能形成意识形态回音室，对AI治理构成系统性风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正从独立接口演变为多智能体系统的基础推理层，其行为特征的持久性检测成为安全治理的关键。传统基准测试仅评估任务准确率，无法揭示训练过程中固化的稳定响应策略（"主流思维模式"），而这些策略会持续存在于不同版本模型中，影响递归评估系统的安全性。

Method: 研究采用心理测量理论中的潜在特质估计方法，设计强制选择序数小插图任务，通过语义正交诱饵进行掩蔽并应用加密置换不变性保护。对九个领先模型在优化偏见、迎合性和现状合法化等维度进行审计，运用混合线性模型和组内相关系数分析提供商级行为特征。

Result: 分析表明，尽管项目级框架效应导致高方差，但持久性"实验室信号"显著解释了行为聚类现象。这证实了提供商级别的潜在偏见并非随机误差，而是具有系统性特征，在锁定生态中表现出跨模型的稳定性。

Conclusion: 该发现揭示了大语言模型集成化趋势的治理风险：提供商级偏见在多层AI架构中可能成为复合变量，形成递归意识形态回音室。研究强调了持续审计持久性行为特征对确保AI系统安全与可控的必要性。

Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.
  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.
  Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.

</details>


### [14] [What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform](https://arxiv.org/abs/2602.17194)
*Adrian Cosma,Cosmin Dumitrache,Emilian Radoi*

Main category: cs.CL

TL;DR: 该研究分析罗马尼亚文本远程医疗数据，揭示患者满意度虽主要受历史特征影响，但医生回复中的礼貌和模糊表达等语言特征提供可操作信号，为改善在线医患沟通提供依据。


<details>
  <summary>Details</summary>
Motivation: 文本远程医疗要求医生提供清晰的书面医疗建议。在患者评分日益重要的背景下，医生面临维持满意度的压力，但这些评分更多反映沟通质量而非临床准确性。因此，识别影响患者满意度的文本特征对改善远程医疗沟通至关重要。

Method: 研究使用77,334个匿名罗马尼亚医患对话对，将患者反馈二分为点赞（正面）与其他（负面/无反馈）。提取语言无关特征（长度、结构、可读性）、罗马尼亚LIWC心理语言学特征及礼貌/模糊标记。采用时间分割训练分类器，并进行SHAP分析。

Result: SHAP分析显示患者和医生历史特征是预测主导因素，提供强先验。回复文本特征提供较小但关键的信号：礼貌和模糊表达与患者满意度正相关，而词汇多样性与满意度负相关。

Conclusion: 尽管历史特征对预测患者满意度最重要，但医生回复的语言特征（特别是礼貌策略）具有可操作价值。这些发现可为提升文本远程医疗中的医患沟通质量提供具体指导。

Abstract: Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.

</details>


### [15] [Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study](https://arxiv.org/abs/2602.17262)
*Kensuke Okada,Yui Furukawa,Kyosuke Bunji*

Main category: cs.CL

TL;DR: 本研究针对大型语言模型(LLMs)在自我报告问卷中表现出的社会期望反应(SDR)问题，提出心理测量学框架进行量化与缓解。通过对比诚实与伪装好指令下的项目反应理论(IRT)潜在特质得分来量化SDR，并构建desirability匹配的分级迫选(GFC)量表进行缓解。实验表明GFC显著降低SDR并保留人格特征恢复能力。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理领域，人类自我报告问卷被广泛用于评估和审计大型语言模型(L LMs)的人格一致性、安全性和偏见等。然而，这些工具假设模型会诚实作答，但LLMs在评估情境中倾向于选择社会期望答案，即社会期望反应(SDR)，导致评估结果偏差。因此，亟需量化并缓解此效应。

Method: 研究提出双轨框架：(1) SDR量化：在HONEST与FAKE-GOOD指令下施测同一量表，基于项目反应理论(IRT)估计的潜在特质分数计算方向校正的标准化效应量，实现跨构念、跨格式及人类基准比较；(2) SDR缓解：通过约束优化从项目池选取30个跨领域配对项目，构建desirability匹配的分级迫选(GFC)大五人格量表。在九个指令微调LLMs的合成人格评估中验证。

Result: 评估显示，李克特式问卷在LLMs中引发持续且显著的社会期望反应(SDR)，而desirability匹配的分级迫选(GFC)方法显著削弱SDR，同时基本保持对目标人格特征的恢复效度。研究揭示了模型依赖的SDR-恢复权衡现象。

Conclusion: 该研究强调在LLMs基准测试与审计中必须考虑社会期望反应偏差，倡导采用SDR感知的报告实践，以确保基于问卷的评估结论的准确性与可靠性。

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

</details>


### [16] [Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective](https://arxiv.org/abs/2602.17283)
*Yukun Chen,Xinyu Zhang,Jialong Tang,Yu Wan,Baosong Yang,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文提出X-Value跨语言价值观评估基准，旨在评估大语言模型对内容深层价值观的评估能力。该基准涵盖18种语言的5000余问答对，基于施瓦茨价值观理论构建。实验表明，当前SOTA模型准确率不足77%，且跨语言性能差异显著（>20%）。


<details>
  <summary>Details</summary>
Motivation: 现有LLM内容安全评估聚焦于暴力、仇恨等显性危害检测，忽视了数字内容中蕴含的细微价值维度。这种评估范式难以满足全球化背景下跨语言、跨文化的价值观判断需求，亟需更全面的评估体系。

Method: 构建X-Value基准，包含18种语言的5000+问答对，系统划分为基于施瓦茨理论的7大价值观领域，并设置难易两级区分度。采用两阶段标注框架：先判定议题属性（全球共识vs多元主义），再进行多方评估以识别内容中的潜在价值观。

Result: 对SOTA LLMs的评估显示，其跨语言价值观评估能力存在明显缺陷（整体准确率<77%），且不同语言间性能差异显著（ΔAcc>20%），暴露出模型在价值观理解上的不足。

Conclusion: 当前LLMs在细微、价值观感知的内容评估方面亟待改进。X-Value基准为提升模型的跨文化价值观理解能力提供了重要评估工具，呼吁研究社区重视并加强这方面的能力发展。

Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\%$), with significant performance disparities across different languages ($ΔAcc > 20\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.

</details>


### [17] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan Kostić,Conor Fallon,Julian Risch,Alexander Löser*

Main category: cs.CL

TL;DR: 本研究揭示大型语言模型对词汇层面的微小扰动表现出高度敏感性，导致性能显著下降且排行榜稳定性受损，表明当前LLM主要依赖表层词汇特征而非深层语言理解能力，故需将鲁棒性测试纳入标准评估体系。


<details>
  <summary>Details</summary>
Motivation: 标准化评估基准是模型比较的核心工具，但其可靠性因对输入提示浅层变化的高度敏感性而受质疑。本研究旨在系统考察真值条件不变的词汇与句法扰动如何影响23个LLM的绝对性能与相对排名。

Method: 在MMLU、SQuAD和AMEGA三个基准上测试23个当代LLM，采用两条语言学驱动的管道生成语义保留的变异：同义词替换实现词汇扰动，依存句法分析指导句法变换。

Result: 词汇扰动在几乎所有模型和任务中均导致统计显著的性能退化；句法扰动效应异质，偶有性能提升。两种扰动均 destabilize 排行榜稳定性，复杂任务更明显；模型鲁棒性与规模无一致关系，存在强任务依赖性。

Conclusion: LLM更依赖表层词汇模式而非抽象语言能力，表明当前评测基准可能高估模型真实理解水平，强调必须将鲁棒性测试作为标准组件纳入LLM评估流程。

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [18] [RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering](https://arxiv.org/abs/2602.17366)
*Yiming Zhang,Siyue Zhang,Junbo Zhao,Chen Zhao*

Main category: cs.CL

TL;DR: 针对大语言模型在长尾知识问答中的不足，本文提出RPDR框架，通过合成数据生成、Round-Trip预测选择易学实例来增强稠密检索器，在PopQA和EntityQuestion基准上显著超越BM25和Contriver，尤其在极长尾类别上。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以准确回忆低频长尾知识，尽管检索增强生成(RAG)有所帮助，但稠密检索器在面对罕见或小众知识时同样表现不佳，存在泛化瓶颈。

Method: 提出RPDR数据增强框架，包含三个核心组件：(1)合成数据生成；(2)使用Round-Trip预测选择高质量易学训练实例；(3)用筛选实例训练稠密检索器。

Result: 在PopQA和EntityQuestion两个长尾检索基准上，RPDR相比BM25和Contriver等现有检索器取得显著提升，尤其在极长尾类别上优势明显。

Conclusion: 人工分析验证了RPDR的有效性，但仍有局限性；未来可通过动态路由机制将查询分发到专用检索模块以进一步提升性能。

Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.

</details>


### [19] [The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour](https://arxiv.org/abs/2602.17377)
*Leonidas Zotos,Hedderik van Rijn,Malvina Nissim*

Main category: cs.CL

TL;DR: 该研究探讨多项选择题考试中的可得性启发式猜测策略。通过计算维基百科中选项概念的普遍性，发现正确选项比错误选项更易获得，选择最易获得选项可比随机猜测提高13.5%-32.9%的得分。大型语言模型生成的选项也表现出类似模式。


<details>
  <summary>Details</summary>
Motivation: 学生面对不确定答案的多项选择题时普遍采用猜测策略。Tversky与Kahneman(1973)的可得性启发式理论认为，人们倾向于以实例提取的容易程度作为心理捷径。然而，直接将"选择最易想到选项"作为应试策略的有效性缺乏实证验证，需要计算化方法来评估这种认知机制的实际效果。

Method: 研究者开发了计算化方法，通过测量选项概念在大型语料库（维基百科）中的出现频率来操作化认知可得性。利用该方法分析三个大型题库，系统比较正确选项与错误选项的可得性差异，并检验大型语言模型生成选项的模式。

Result: 跨三个大型题库的一致性发现：独立于题干内容，正确答案的可得性显著高于错误选项。使用维基百科语料库时，始终选择最可得选项的策略得分比随机猜测基线高出13.5%至32.9%。值得注意的是，尽管大型语言模型训练于海量文本数据且具频率主义特性，其生成的选项在可得性分布模式上与专家编制选项相似。

Conclusion: 研究表明可得性启发式在多项选择题猜测行为中起重要作用，建议在未来的学生行为计算建模中应系统性地纳入可得性因素，以更准确地预测和解释学生的考试表现。

Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.

</details>


### [20] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 针对现有跨文档指代消解（CDCR）数据集在多样化、极化新闻语篇中覆盖面窄的问题，本研究提出一种将指代链作为话语元素（DEs）的修订标注方案，通过纳入身份与近身份关系重新标注NewsWCL50和ECB+子集，以建模新闻话语中的词汇多样性与框架变异。


<details>
  <summary>Details</summary>
Motivation: 现有CDCR数据集主要聚焦于事件消解，采用狭义的指代定义，难以有效处理措辞差异显著的多样化和极化新闻覆盖，限制了模型在真实新闻语篇中的泛化能力和分析深度。

Method: 提出修订的CDCR标注框架，将指代链视为话语元素（DEs）和概念分析单元，支持身份与近身份关系（如"the caravan"、"asylum seekers"、"those contemplating illegal entry"之间的关联）；采用统一代码本对NewsWCL50全量及ECB+子集进行重新标注。

Result: 通过词汇多样性指标和相同中心词语义基线评估，重标注后的两个数据集表现趋于一致，其性能介于原始ECB+与NewsWCL50之间，验证了标注方案的有效性。

Conclusion: 该修订方案能够在保持话语元素细粒度标注的同时，有效捕捉媒体话语中的词汇多样性和框架变化，为新闻领域平衡且具话语意识的CDCR研究提供了更适配的数据资源和分析框架。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


### [21] [Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics](https://arxiv.org/abs/2602.17425)
*Sanjeev Kumar,Preethi Jyothi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本研究针对极低资源语言机器翻译评估难题，对比分析了BLEU和ChrF++在Magahi、Bhojpuri和Chhattisgarhi三种语言上的表现，发现BLEU尽管分数较低，但能提供互补的词汇精确度洞察，提升评估结果的可解释性。


<details>
  <summary>Details</summary>
Motivation: 在极低资源语言场景下，广泛使用的BLEU等评估指标往往无法准确反映翻译质量，而现有研究多依赖单一的ChrF++指标。本研究旨在探究不同指标在低资源环境下的表现差异，寻找更可靠的评估方法。

Method: 论文对BLEU（基于n-gram）和ChrF++（基于字符）两种指标进行对比分析，考察它们对三种极低资源语言中翻译异常现象（包括幻觉、重复、源文本复制和变音符号变化）的响应情况，评估对象涵盖大语言模型和神经机器翻译系统的输出。

Result: 研究发现，尽管BLEU的绝对分数较低，但它能提供互补的词汇精确度视角，与ChrF++结合使用可以改善评估结果的可解释性。

Conclusion: 在极低资源语言的机器翻译评估中，不应仅依赖ChrF++单一指标，而应将BLEU与ChrF++结合使用，以获得更全面、可解释性更强的评估结果。

Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.

</details>


### [22] [Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study](https://arxiv.org/abs/2602.17431)
*Dylan Bouchard,Mohit Singh Chauhan,Viren Bajaj,David Skarbrevik*

Main category: cs.CL

TL;DR: 本文针对长文本LLM生成中的幻觉检测难题，提出了一个细粒度不确定性量化（UQ）的三阶段分类法框架（响应分解、单元级评分、响应级聚合），形式化了一致性基础的黑盒评分方法。实验表明声明-响应蕴含方法效果优异，声明级评分优于句子级，不确定性感知解码能显著提升长文本真实性。该框架统一了现有方法，为组件选择提供了系统指导。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法主要面向短文本输出设计，难以有效泛化到长文本生成场景。长文本生成需要更细粒度的评估单元和更复杂的多层次处理流程，当前缺乏统一的理论框架来系统性分析、比较和指导不同方法的设计选择。

Method: 提出包含三个设计阶段的形式化分类法：1) 响应分解：将长文本拆解为评估单元（句子或声明）；2) 单元级评分：采用一致性基础的黑盒评分器（包括声明-响应蕴含等）计算各单元不确定性分数；3) 响应级聚合：整合单元分数形成整体评估。该框架泛化和扩展了现有方法，澄清了方法间的理论关系。

Result: 跨多个LLM和数据集的实验发现：1) 声明-响应蕴含方法性能优于或与更复杂的声明级评分器相当；2) 声明级评分普遍显著优于句子级评分；3) 不确定性感知解码对提升长文本事实性非常有效。该框架实现了方法间的公平比较和关系梳理。

Conclusion: 该框架为长文本生成中的细粒度不确定性量化提供了系统性理论指导和实践方案，有助于研究者和从业者根据需求选择合适的方法组件，推动幻觉检测技术在长文本场景的应用与发展，并为未来方法创新奠定了统一基础。

Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.

</details>


### [23] [AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue](https://arxiv.org/abs/2602.17443)
*Adib Sakhawat,Fardeen Sadab,Rakin Shahriar*

Main category: cs.CL

TL;DR: 本文提出AIDG游戏理论框架，通过动态多轮对话评估大语言模型的战略推理能力，揭示其在信息防守端比主动推理端强350 ELO分的显著不对称性，核心瓶颈在于信息动态管理与约束遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准测试不足以评估大语言模型的战略推理能力，需转向动态多轮交互场景，以探究信息提取（主动推理）与信息维持（状态保持）之间的能力不对称性。

Method: 提出对抗性信息推理游戏(AIDG)框架，设计两个互补任务：AIDG-I（社会推理，测量语用策略）和AIDG-II（结构化"20个问题"，测量约束满足）。在439场游戏中系统评估六个前沿大语言模型。

Result: 实验显示模型在防守端表现显著优于推理端，优势达350 ELO分（Cohen's d=5.47，效应量极大）。识别两大瓶颈：1）信息动态方面，确认策略比盲目推理有效7.75倍（p<0.00001）；2）约束遵循方面，对话负荷导致指令遵循退化，造成41.3%的推理失败。

Conclusion: 大语言模型擅长局部防御一致性，但缺乏支撑战略询问所需的全局状态跟踪能力，这揭示了其战略推理的本质局限。

Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured "20 Questions" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.

</details>


### [24] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: 该论文发现LLM在多选题评测中存在标签位置与少样本提示偏差，通过构建合成NonsenseQA基准，提出一种简化去偏评测协议：用统一无序标签替换原标签，并利用句子相似度模型匹配完整答案。该方法在多个基准和模型上使准确率方差降低3倍，性能下降极小，显著提升了对答案排列的鲁棒性，揭示了LLM在减少评测伪影下的真实能力。


<details>
  <summary>Details</summary>
Motivation: 标准多选题评测基准存在评估伪影，LLM倾向于利用答案位置、标签前缀或少数样本中的正确答案分布等偏差来作答，而非真正推理和知识问答，这导致模型性能被高估且无法反映其真实能力。因此，需要一种能减少这些评估偏差、更鲁棒的评测方法。

Method: 通过合成NonsenseQA基准观察LLM的标签位置-少样本提示偏差；提出去偏评测协议，将每道题的标签替换为统一无序标签（如A、B、C），并提示LLM输出完整答案文本而非标签；采用简单句子相似度模型匹配预测答案与候选答案；在多个基准和模型上验证，并进行嵌入模型和相似度函数的消融研究。

Result: 在不同LLM中观察到不同程度的标签位置与少样本提示偏差；所提协议使准确率方差降低3倍，平均性能仅轻微下降；显著改善了答案排列间的鲁棒性，标准差降低；消融研究表明该方法在不同嵌入模型和相似度函数下均比标准方法更鲁棒。

Conclusion: 该简单去偏评测协议能有效减少评估伪影，暴露LLM在无提示示例和选项标签帮助下的真实能力，大幅提升了评测的鲁棒性，优于标准评测方法，为更公平地评估LLM推理与知识问答能力提供了新范式。

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [25] [Entropy-Based Data Selection for Language Models](https://arxiv.org/abs/2602.17465)
*Hongming Li,Yang Liu,Chao Huang*

Main category: cs.CL

TL;DR: 针对大模型微调中计算资源与数据资源的矛盾，本文提出基于熵的无监督数据选择框架EUDS。该方法通过熵值估计数据不确定性，建立高效数据过滤机制，在情感分析、主题分类和问答任务上验证有效，显著降低计算成本并提升训练效率，为资源受限场景下的模型微调提供创新解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调需要大量计算和数据资源，而数据选择技术虽能减少数据量但依赖高计算预算。实际应用中资源受限，且评估数据可用性仍是难题。因此，迫切需要高效的数据选择方法来降低计算成本，特别是在计算资源受限的场景下实现大模型的轻量级微调。

Method: 提出Entropy-Based Unsupervised Data Selection (EUDS)框架。该框架基于熵值进行数据不确定性估计，构建无监督的数据过滤机制，无需额外标注即可筛选高质量训练数据。

Result: 在情感分析、主题分类和问答三项任务上的实证研究表明，EUDS能有效过滤低质量数据，在减少数据需求的同时显著降低计算成本，提升训练时间效率，验证了其有效性。

Conclusion: EUDS为计算资源受限场景下的大模型高效微调提供了创新解决方案，通过熵基无监督数据选择实现了计算成本的显著降低，理论分析与实验结果均证实该方法的有效性，具有重要实践价值。

Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.

</details>


### [26] [PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions](https://arxiv.org/abs/2602.17467)
*Greta Damo,Stéphane Petiot,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 本文提出PEACE 2.0，一种基于检索增强生成(RAG)的新工具，不仅能分析和解释仇恨言论检测的依据，还能生成基于证据的反言论回应，支持显性和隐性仇恨言论的处理。


<details>
  <summary>Details</summary>
Motivation: 在线仇恨言论日益泛滥带来严重社会挑战，尽管NLP社区已能有效检测仇恨言论，但自动生成反言论回应仍是未解决的难题。

Method: 采用检索增强生成(RAG)框架，将仇恨言论解释和生成的反言论与事实证据相结合，实现证据支撑的检测分析和回应生成。

Result: 开发了具有三大功能的PEACE 2.0工具：证据 grounded 的仇恨言论解释、自动化的证据 grounded 反言论生成，以及探索反言论回复特征的能力。

Conclusion: PEACE 2.0通过整合分析、解释和生成能力，为显性和隐性仇恨言论提供了深入的自动化分析和回应解决方案。

Abstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.

</details>


### [27] [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers](https://arxiv.org/abs/2602.17469)
*Nusrat Jahan Lia,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本文研究孟加拉语与英语之间的跨语言情感错位问题，发现压缩模型mDistilBERT存在28.7%的情感反转率，且方言偏见导致错误率增加57%，呼吁采用基于文化多样性的对齐度量标准。


<details>
  <summary>Details</summary>
Motivation: 双向对齐的核心主题——确保AI系统准确理解人类意图且人类信任AI行为——在语言障碍面前严重断裂。本研究旨在解决孟加拉语与英语之间的跨语言情感错位问题，揭示当前对齐范式在低资源语言中的安全与表征失效。

Method: 研究通过基准测试四种Transformer架构，评估孟加拉语与英语之间的跨语言情感对齐表现，分析模型在不同语言及方言变体中的情感理解能力。

Result: 研究发现严重失效现象：压缩模型mDistilBERT展现出28.7%的"情感反转率"，将用户积极意图误判为消极；存在"不对称共情"现象，模型对孟加拉语文本的情感权重进行系统性削弱或放大；区域模型IndicBERT存在"现代偏见"，处理正式Sadhu孟加拉语时对齐错误增加57%。

Conclusion: 研究认为，公平的人机协同演化需要多元文化基础的对齐方式，尊重语言与方言多样性，而非通用的压缩方法。建议对齐基准应纳入"情感稳定性"指标，明确惩罚低资源及方言语境中的极性反转现象。

Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

</details>


### [28] [Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian](https://arxiv.org/abs/2602.17475)
*Pietro Ferrazzi,Mattia Franzin,Alberto Lavelli,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本研究探讨约10亿参数的小型语言模型在医疗NLP任务中的可行性，发现通过微调可使小模型媲美甚至超越更大模型，并开源了意大利语医疗数据集与预训练语料。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽在医疗NLP任务中表现优异，但其巨大的计算需求限制了真实医疗场景的部署。本研究旨在探索小型语言模型能否在保持竞争力的同时，满足实际医疗应用对计算效率的要求。

Method: 评估Llama-3、Gemma-3和Qwen3三个模型家族在20个临床NLP任务（包括命名实体识别、关系抽取、病例报告表填充、问答和论元挖掘）上的表现，系统比较推理时策略（少样本提示、约束解码）和训练时策略（监督微调、持续预训练）的效果。

Result: 微调是最有效方法；少样本提示与约束解码结合提供了强大的低资源替代方案。小模型可匹配甚至超越更大基线模型，最佳配置Qwen3-1.7B平均得分比Qwen3-32B高出9.2点。研究同时开源了意大利语医疗数据集及来自急诊科和其他来源的301M词预训练语料。

Conclusion: 小型语言模型通过适当的适配策略（特别是微调）可在医疗NLP任务中实现卓越性能，有效解决了实际部署的计算瓶颈。该研究通过开源意大利语医疗数据集和预训练语料，为该领域的后续研究提供了重要资源。

Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether "small" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.

</details>


### [29] [Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics](https://arxiv.org/abs/2602.17513)
*Baris Karacan,Barbara Di Eugenio,Patrick Thornton*

Main category: cs.CL

TL;DR: 本研究针对临床文本章节分割任务，构建了去标识化的产科笔记数据集，系统评估了基于Transformer的监督模型与零样本大语言模型在域内（MIMIC-III）和域外（产科）的表现。结果表明监督模型域外性能显著下降，而零样本模型在修正章节标题幻觉后具有强大域外适应能力，为医疗NLP跨领域应用指明新方向。


<details>
  <summary>Details</summary>
Motivation: 临床自由文本笔记包含关键患者信息，其结构化章节对临床决策支持和下游NLP任务至关重要。然而现有研究多基于公开语料库（如MIMIC-III）训练，覆盖医学领域有限。为弥补这一不足，需构建更多领域特定的标注资源，并探索模型在跨领域场景下的泛化能力，以促进医疗NLP技术在更广泛临床场景中的应用。

Method: 本文提出三项核心贡献：1）构建新的去标识化、带章节标签的产科笔记数据集，补充现有公开语料库的医学领域覆盖；2）系统评估基于Transformer的监督模型在MIMIC-III子集（域内）和新产科数据集（域外）上的章节分割性能；3）首次对监督模型与零样本大语言模型进行头对头比较。

Result: 实验发现：监督模型在域内表现优异，但域外性能显著下降；相比之下，零样本模型展现出强大的域外适应能力，但需修正其产生的幻觉章节标题。一旦妥善处理幻觉问题，零样本模型在跨领域章节分割任务中表现稳健。

Conclusion: 本研究强调构建领域特定临床资源的重要性，同时揭示零样本章节分割在突破现有语料库局限方面的巨大潜力。只要有效管理幻觉问题，零样本方法为将医疗NLP技术应用于更广泛的临床实践提供了有前景的方向。

Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.

</details>


### [30] [Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems](https://arxiv.org/abs/2602.17542)
*Zhangqi Duan,Arnav Kankaria,Dhruv Kartik,Andrew Lan*

Main category: cs.CL

TL;DR: 针对开放式编程任务中知识组件(KC)级别正确性标签缺失的问题，本文提出一种基于大语言模型(LLM)的自动化框架，可直接从学生代码中标记KC级正确性，并通过时序感知的Code-KC映射机制提升学习曲线的拟合效果和预测性能。


<details>
  <summary>Details</summary>
Motivation: 知识组件(KC)是学生建模和学习分析的基础，但实际数据集中KC级正确性标签极为稀缺，尤其是开放式编程任务中。简单将题目级正确性传播到所有相关KC会掩盖部分掌握情况并导致学习曲线拟合不佳，亟需自动化方法来直接从代码中推断KC级掌握状态。

Method: 提出基于大语言模型的自动化框架，评估每个KC在学生代码中的正确应用情况，并引入时序上下文感知的Code-KC映射机制，使KCs更好地对齐个体学生代码。通过练习幂律和加法因素模型评估生成的KC级标签。

Result: 实验表明，该框架产生的学习曲线更符合认知理论，预测性能优于基线方法。人工评估进一步显示LLM标注与专家标注具有高度一致性。

Conclusion: 利用大语言模型直接从学生代码中自动标注知识组件级正确性是可行且有效的，能够显著提升学习曲线的质量和预测准确性，为学习分析提供了更细粒度的学生掌握状态评估方法。

Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.

</details>


### [31] [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning](https://arxiv.org/abs/2602.17546)
*Jyotin Goel,Souvik Maji,Pratik Mazumder*

Main category: cs.CL

TL;DR: 本文针对指令微调语言模型在良性或对抗性微调过程中安全性下降的问题，提出了一种自适应正则化框架。该框架通过两种方式实时评估安全风险（基于评判的Safety Critic和基于激活的风险预测器），动态约束高风险更新以保持与参考安全策略的接近，同时允许低风险更新正常进行。实验表明，该方法在降低攻击成功率的同时保持了下游性能，且不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 指令遵循语言模型在微调过程中（包括良性微调和对抗性更新）会出现安全性行为退化现象。现有防御措施要么保护有限，要么需要在安全性和实用性之间做出权衡。因此，需要一种能够在整个微调过程中持续保持模型对齐的新方法。

Method: 提出自适应正则化训练框架，根据实时安全风险动态调整约束强度。探索两种风险估计方法：1）基于评判的Safety Critic，为训练批次分配高层危害分数；2）基于激活的风险预测器，使用轻量级分类器分析模型中间激活来估计有害意图。高风险更新被约束在安全参考策略附近，低风险更新则进行标准训练。

Result: 实证验证了有害意图可从生成前激活中预测，且评判分数提供有效的高召回安全指导。在多个模型家族和攻击场景下，与标准微调相比，自适应正则化显著降低攻击成功率，保持下游任务性能，且不增加推理时间成本。

Conclusion: 本工作展示了一种在不牺牲实用性前提下维持模型安全性的原则性机制，为微调过程中的安全对齐提供了有效解决方案。

Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.

</details>


### [32] [Modeling Distinct Human Interaction in Web Agents](https://arxiv.org/abs/2602.17588)
*Faria Huq,Zora Zhiruo Wang,Zhanqiu Guo,Venu Arvind Arangarajan,Tianyue Ou,Frank Xu,Shuyan Zhou,Graham Neubig,Jeffrey P. Bigham*

Main category: cs.CL

TL;DR: 该论文针对自主网页智能体缺乏对人类干预时机理解的问题，构建了真实用户-智能体交互数据集CowCorpus，识别出四种干预模式，训练语言模型预测用户干预行为，并在实际部署中验证了提升智能体可用性的效果。


<details>
  <summary>Details</summary>
Motivation: 当前自主网页智能体系统在人类介入时机和原因上缺乏原则性理解，常错过关键决策点或请求不必要的确认，亟需建立人类干预行为的建模框架以实现真正协同的任务执行。

Method: 1) 收集400条真实用户网页导航轨迹构成CowCorpus数据集（含4200+交替人机动作）；2) 识别出"放手监督"、"动手 oversight"、"协作求解"、"用户完全接管"四类交互模式；3) 基于交互风格训练语言模型预测干预时机。

Result: 1) 干预预测准确率较基础语言模型提升61.4-63.4%；2) 用户研究显示部署该模型的智能体在"有用性"评分上提升26.5%。

Conclusion: 对人类干预行为的结构化建模能显著提升智能体的适应性与协作性，证明理解用户干预模式是构建高效人机协同系统的关键路径。

Abstract: Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.

</details>


### [33] [Unmasking the Factual-Conceptual Gap in Persian Language Models](https://arxiv.org/abs/2602.17623)
*Alireza Sakhaeirad,Ali Ma'manpoosh,Arshia Hemmat*

Main category: cs.CL

TL;DR: 本文提出DivanBench，一个针对波斯语文化习俗与迷信观念的诊断性基准，通过315个涵盖事实检索、场景验证和情境推理的题目评估7个波斯语大模型，揭示三大缺陷：严重顺从偏见（能识别合规行为但无法拒绝违规）、持续预训练反而加剧偏见并削弱矛盾识别能力，以及事实记忆与应用之间存在21%性能鸿沟，证明当前模型仅表面模仿文化模式而未能内化深层文化图式。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语NLP基准虽扩展至语用与礼貌层面，却未能区分模型是记忆文化事实还是真正理解隐含社会规范，缺乏对文化推理能力的诊断。

Method: 构建DivanBench基准，聚焦超自然信仰与习俗等任意性、依赖语境的文化规则；设计315个问题，涵盖事实检索、配对场景验证和情境推理三类任务，系统评估七种波斯语大语言模型。

Result: 揭示三大关键失败：1）严重顺从偏见——模型能正确识别恰当行为但无法拒绝明显违规；2）持续波斯语预训练非但未提升推理，反而放大偏见并退化矛盾识别能力；3）所有模型在事实检索与应用场景间存在21%的性能差距。

Conclusion: 文化胜任力不能仅靠扩展单语数据规模实现；当前模型仅学会模仿文化模式，未能内化底层文化图式，需新范式以培养真正的文化推理能力。

Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.

</details>


### [34] [Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking](https://arxiv.org/abs/2602.17653)
*Iskar Deng,Nathalia Xu,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 训练GPT-2模型发现，LM能习得DAM中语义非典型论元被显性标记的偏好，但无法重现人类语言中宾语更易被标记的倾向，暗示不同类型学规律可能源于不同机制。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究证实语言模型可学习句法类型学规律，但语义层面的差分论元标记（DAM）的类型学偏好尚未被探索。本研究旨在通过合成语料训练，检验语言模型是否能习得DAM系统的跨语言共性与特性。

Method: 采用受控合成学习方法，构建18种不同DAM系统的语料库，训练GPT-2模型并基于最小对立对进行泛化评估。通过操控论元的语义凸显度，考察模型对DAM两个类型学维度的学习能力。

Result: 发现双重分离：模型表现出人类似的自然标记方向偏好（显性标记用于语义非典型论元），但未重现人类语言中强烈的宾语偏好（显性标记更多用于宾语而非主语）。这表明模型仅捕捉到部分类型学规律。

Conclusion: 研究揭示DAM的不同类型学维度可能由不同机制驱动。语言模型可学习基于语义普遍性的标记偏好，但难以完全习得特定语言中的论元层级偏好，为理解语言类型学规律的形成提供了计算证据。

Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.

</details>


### [35] [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655)
*Clara Meister,Ahmetcan Yavuz,Pietro Lesci,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文提出UniLID，一种基于UnigramLM分词算法的语言识别方法。该方法通过学习共享词表上的语言条件化unigram分布，将分词视为语言特定现象，实现了数据与计算高效、支持增量学习且不重训的特性。在标准基准上表现优异，在低资源场景下仅需每个语言5个标注样本即可达到70%以上准确率，并在方言识别上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管现有语言识别系统在高资源语言上表现近乎完美，但在低资源语言和密切相关的语言场景中仍然表现脆弱。这在多语言NLP流程中构成重要瓶颈，影响语料库构建、训练数据分析和大型语言模型的跨语言评估效果。

Method: UniLID利用UnigramLM的概率框架、参数估计技术和推理策略。核心思想是在共享分词器词表上学习语言条件化的unigram分布，同时将分词处理为语言特定现象。该方法具有数据和计算效率，支持增量添加新语言而无需重新训练现有模型，并能自然集成到现有语言模型的分词流程中。

Result: 在fastText、GlotLID和CLD3等广泛使用的基线方法上的实证评估显示：UniLID在标准基准上达到竞争性能；在低资源场景下样本效率显著提升，每个语言仅需5个标注样本即可超过70%准确率；在细粒度方言识别任务上取得大幅改进。

Conclusion: UniLID是一种简单高效的语言识别方法，特别适用于低资源和方言识别场景。其增量学习能力和与现有分词流程的无缝集成特性，使其在多语言NLP系统中具有实用价值。

Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.

</details>


### [36] [Sink-Aware Pruning for Diffusion Language Models](https://arxiv.org/abs/2602.17664)
*Aidar Myrzakhan,Tianyi Li,Bowei Guo,Shengkun Tang,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 针对扩散语言模型推理成本高的问题，本文发现AR模型的注意力汇聚假设不适用于DLM，提出了Sink-Aware Pruning方法，通过识别并剪枝不稳定的汇聚token，在不重训练的情况下实现了更好的质量-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型因迭代去噪过程导致推理开销巨大，现有剪枝方法多沿用自回归大语言模型的经验，保留注意力汇聚token作为稳定全局锚点。但研究发现这一假设在DLM中不成立，其汇聚位置在生成过程中变化剧烈，结构重要性远低于AR模型，导致传统剪枝策略次优。

Method: 提出Sink-Aware Pruning方法，自动识别并剪枝DLM中的不稳定汇聚token。通过测量主导汇聚位置在整个生成轨迹中的偏移程度来量化其稳定性，摒弃了AR模型中必须保留汇聚token的惯例。

Result: 该方法无需重训练，在匹配计算量下显著优于现有剪枝基线，实现了更好的生成质量与推理效率权衡。

Conclusion: 本研究挑战了注意力汇聚token对DLM结构至关重要的传统认知，揭示了其在扩散模型中的瞬态特性，为开发高效扩散语言模型推理提供了新的理论洞见和实践方法。

Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [37] [RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution](https://arxiv.org/abs/2602.16932)
*Jinming Nian,Fangchen Li,Dae Hoon Park,Yi Fang*

Main category: cs.IR

TL;DR: 本文提出RankEvolve，一种基于LLM与进化搜索的自动化检索算法发现框架。该框架从BM25和Dirichlet平滑的查询似然模型出发，通过在12个IR数据集上迭代演化可执行代码形式的候选算法，最终生成新颖有效的排序算法，并在BEIR、BRIGHT及TREC DL等基准测试中展现出良好的泛化性能，证实了评估器引导的LLM程序演化是自动发现排序算法的可行路径。


<details>
  <summary>Details</summary>
Motivation: 传统检索算法（如BM25和Dirichlet平滑的查询似然模型）虽仍是高效的一阶段排序器，但其改进长期依赖参数调优与人工直觉，缺乏系统化的自动化方法。作者旨在探索利用大语言模型结合评估器与进化搜索，实现检索算法的自动发现与优化。

Method: 提出RankEvolve框架，基于AlphaEvolve构建程序演化范式。将候选排序算法编码为可执行代码，在BEIR与BRIGHT的12个数据集上，通过变异、重组和基于检索性能的选择机制进行迭代演化。初始种子程序为BM25和Dirichlet平滑的查询似然模型。

Result: 演化出的算法具有结构新颖性和有效性，在完整的BEIR与BRIGHT基准测试中表现优异，并成功迁移至TREC DL 19与20数据集，显示出良好的跨数据集泛化能力。

Conclusion: 评估器引导的LLM程序演化为自动发现新型排序算法提供了一条实用路径，突破了传统依赖人工经验的算法改进模式，为信息检索领域的算法自动化设计开辟了新方向。

Abstract: Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms.

</details>


### [38] [Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval](https://arxiv.org/abs/2602.16974)
*Yongjie Zhou,Shuai Wang,Bevan Koopman,Guido Zuccon*

Main category: cs.IR

TL;DR: 该论文通过复现先前研究并构建系统性框架，统一评估了文档分块策略。研究发现最优分块策略取决于任务类型：语料库检索中简单结构方法优于LLM引导方法，而文档内检索中LumberChunker表现最佳。上下文分块对语料库检索有效但损害文档内检索性能。


<details>
  <summary>Details</summary>
Motivation: 文档分块是密集检索系统的关键预处理步骤，但其设计空间尚未被充分理解。现有方法（如LLM引导的DenseX、LumberChunker和上下文感知的Late Chunking）独立发展且评估基准重叠度低，难以进行直接比较，因此需要系统性研究。

Method: 研究复现了先前文档分块方法，并提出了包含两个维度的统一框架：1）分割方法（基于结构、语义感知和LLM引导）；2）嵌入范式（嵌入前分块vs上下文分块）。在文档内检索（大海捞针）和语料库检索两种任务设置下进行全面评估。

Result: 评估揭示最优分块策略具有任务依赖性：语料库检索中简单结构方法优于LLM引导方法，而LumberChunker在文档内检索中表现最佳。上下文分块提升语料库检索效果但降低文档内检索性能。分块大小与文档内检索效果呈中等相关，与语料库检索效果呈弱相关。

Conclusion: 文档分块策略的选择应考虑具体任务需求，简单方法在语料库检索中可能更有效，而复杂LLM方法在文档内检索中具有优势。该研究为未来分块策略设计提供了系统性评估框架和实践指导。

Abstract: Document chunking is a critical preprocessing step in dense retrieval systems, yet the design space of chunking strategies remains poorly understood. Recent research has proposed several concurrent approaches, including LLM-guided methods (e.g., DenseX and LumberChunker) and contextualized strategies(e.g., Late Chunking), which generate embeddings before segmentation to preserve contextual information. However, these methods emerged independently and were evaluated on benchmarks with minimal overlap, making direct comparisons difficult.
  This paper reproduces prior studies in document chunking and presents a systematic framework that unifies existing strategies along two key dimensions: (1) segmentation methods, including structure-based methods (fixed-size, sentence-based, and paragraph-based) as well as semantically-informed and LLM-guided methods; and (2) embedding paradigms, which determine the timing of chunking relative to embedding (pre-embedding chunking vs. contextualized chunking). Our reproduction evaluates these approaches in two distinct retrieval settings established in previous work: in-document retrieval (needle-in-a-haystack) and in-corpus retrieval (the standard information retrieval task).
  Our comprehensive evaluation reveals that optimal chunking strategies are task-dependent: simple structure-based methods outperform LLM-guided alternatives for in-corpus retrieval, while LumberChunker performs best for in-document retrieval. Contextualized chunking improves in-corpus effectiveness but degrades in-document retrieval. We also find that chunk size correlates moderately with in-document but weakly with in-corpus effectiveness, suggesting segmentation method differences are not purely driven by chunk size. Our code and evaluation benchmarks are publicly available at (Anonymoused).

</details>


### [39] [Bending the Scaling Law Curve in Large-Scale Recommendation Systems](https://arxiv.org/abs/2602.16986)
*Qin Ding,Kevin Course,Linjian Ma,Jianhui Sun,Rouchen Liu,Zhao Zhu,Chunxing Yin,Wei Li,Dai Li,Yu Shi,Xuan Cao,Ze Yang,Han Li,Xing Liu,Bi Xue,Hongwei Li,Rui Jian,Daisy Shi He,Jing Qian,Matt Ma,Qunshu Zhang,Rui Li*

Main category: cs.IR

TL;DR: 本文提出ULTRA-HSTU，一种基于模型-系统协同设计的新型序列推荐模型。通过革新输入序列设计、稀疏注意力机制与模型拓扑结构，在保持推荐质量的同时实现5倍训练加速和21倍推理加速，已规模化部署服务于数十亿用户，带来4-8%的消费与参与度提升。


<details>
  <summary>Details</summary>
Motivation: 序列建模是大规模推荐系统的基石，但自注意力机制的二次计算复杂度构成显著瓶颈。现有方法依赖交叉注意力虽缓解计算压力，却牺牲了自注意力的表征能力，亟需兼顾效率与性能的新型架构。

Method: 采用端到端模型与系统协同设计框架，核心创新包括：1）输入序列设计优化；2）稀疏注意力机制重构；3）模型拓扑结构改进，以突破计算瓶颈并最大化表征潜力。

Result: 全面基准测试表明，ULTRA-HSTU实现训练效率提升5倍、推理效率提升21倍的显著增益，同时推荐质量优于传统模型。该方案已规模化部署，每日服务数十亿用户，在生产环境中驱动消费与参与度提升4-8%。

Conclusion: ULTRA-HSTU通过协同设计创新成功突破序列推荐中的效率-质量权衡，为工业级大规模推荐系统提供了可扩展的高效解决方案，验证了模型与系统联合优化的有效性。

Abstract: Learning from user interaction history through sequential models has become a cornerstone of large-scale recommender systems. Recent advances in large language models have revealed promising scaling laws, sparking a surge of research into long-sequence modeling and deeper architectures for recommendation tasks. However, many recent approaches rely heavily on cross-attention mechanisms to address the quadratic computational bottleneck in sequential modeling, which can limit the representational power gained from self-attention. We present ULTRA-HSTU, a novel sequential recommendation model developed through end-to-end model and system co-design. By innovating in the design of input sequences, sparse attention mechanisms, and model topology, ULTRA-HSTU achieves substantial improvements in both model quality and efficiency. Comprehensive benchmarking demonstrates that ULTRA-HSTU achieves remarkable scaling efficiency gains -- over 5x faster training scaling and 21x faster inference scaling compared to conventional models -- while delivering superior recommendation quality. Our solution is fully deployed at scale, serving billions of users daily and driving significant 4% to 8% consumption and engagement improvements in real-world production environments.

</details>


### [40] [WSDM Cup 2026 Multilingual Retrieval: A Low-Cost Multi-Stage Retrieval Pipeline](https://arxiv.org/abs/2602.16989)
*Chentong Hao,Minmao Wang*

Main category: cs.IR

TL;DR: 本文针对WSDM Cup 2026多语言检索任务，提出了一个低成本的检索系统。该系统通过四阶段流水线，结合LLM-based GRF查询扩展、BM25候选检索、jina-embeddings-v4稠密排序以及Qwen3-Reranker-4B对前20结果的重排序，在约一千万篇中/波斯/俄语新闻文章集合上实现英->中/波斯/俄的跨语言检索，最终获得nDCG@20=0.403、Judged@20=0.95的评估结果，并进行了消融实验分析各阶段贡献。


<details>
  <summary>Details</summary>
Motivation: 构建计算预算受限条件下高效的多语言跨语言检索系统，解决英语查询检索中文、波斯文、俄文新闻文档的挑战，在有限资源下平衡效果与成本。

Method: 四阶段流水线：1）LLM-based GRF风格查询扩展；2）BM25候选文档检索；3）采用jina-embeddings-v4长文本编码器进行稠密排序；4）使用Qwen3-Reranker-4B对top-20结果进行 pointwise 重排序，其余结果保持稠密排序顺序。

Result: 官方评估显示系统达到nDCG@20=0.403和Judged@20=0.95；消融实验量化了各阶段在有限计算预算下的有效性。

Conclusion: 通过合理组合查询扩展、稠密排序与精准重排序策略，该系统在有限计算资源下实现了具有竞争力的多语言检索性能，为低成本高效检索提供了可行方案。

Abstract: We present a low-cost retrieval system for the WSDM Cup 2026 multilingual retrieval task, where English queries are used to retrieve relevant documents from a collection of approximately ten million news articles in Chinese, Persian, and Russian, and to output the top-1000 ranked results for each query. We follow a four-stage pipeline that combines LLM-based GRF-style query expansion with BM25 candidate retrieval, dense ranking using long-text representations from jina-embeddings-v4, and pointwise re-ranking of the top-20 candidates using Qwen3-Reranker-4B while preserving the dense order for the remaining results. On the official evaluation, the system achieves nDCG@20 of 0.403 and Judged@20 of 0.95. We further conduct extensive ablation experiments to quantify the contribution of each stage and to analyze the effectiveness of query expansion, dense ranking, and top-$k$ reranking under limited compute budgets.

</details>


### [41] [LiveGraph: Active-Structure Neural Re-ranking for Exercise Recommendation](https://arxiv.org/abs/2602.17036)
*Rong Fu,Zijian Zhang,Haiyun Wei,Jiekai Wu,Kun Liu,Xianda Li,Haoyu Zhao,Yang Li,Yongtai Liu,Ziming Wang,Rui Lu,Simon Fong*

Main category: cs.IR

TL;DR: 针对数字学习环境中个性化内容推荐系统面临的长尾分布和个性化轨迹适应问题，本文提出LiveGraph——一种主动结构神经重排序框架，通过图表示增强和动态重排序机制，在多个真实数据集上显著提升了推荐准确性和内容多样性。


<details>
  <summary>Details</summary>
Motivation: 随着数字学习环境的快速扩展，个性化教育内容需求激增，但现有推荐框架普遍存在学生参与度的长尾分布问题，且难以适应个体独特的学习轨迹，导致推荐效果受限。

Method: 提出LiveGraph框架，采用基于图结构的表示增强策略弥合活跃与不活跃学生间的信息鸿沟，并引入动态重排序机制促进内容多样性，通过优先学习历史中的结构关系来平衡推荐精度与教学多样性。

Result: 在多个真实数据集上的综合实验评估表明，LiveGraph在预测准确性和练习多样性广度方面均超越现有基线方法，有效平衡了推荐精度与教学内容多样性。

Conclusion: LiveGraph通过创新的主动结构神经重排序机制，成功解决了教育推荐系统中的长尾参与度分布和个性化轨迹适应难题，为智能个性化教育提供了有效解决方案，在准确性和多样性方面取得显著提升。

Abstract: The continuous expansion of digital learning environments has catalyzed the demand for intelligent systems capable of providing personalized educational content. While current exercise recommendation frameworks have made significant strides, they frequently encounter obstacles regarding the long-tailed distribution of student engagement and the failure to adapt to idiosyncratic learning trajectories. We present LiveGraph, a novel active-structure neural re-ranking framework designed to overcome these limitations. Our approach utilizes a graph-based representation enhancement strategy to bridge the information gap between active and inactive students while integrating a dynamic re-ranking mechanism to foster content diversity. By prioritizing the structural relationships within learning histories, the proposed model effectively balances recommendation precision with pedagogical variety. Comprehensive experimental evaluations conducted on multiple real-world datasets demonstrate that LiveGraph surpasses contemporary baselines in both predictive accuracy and the breadth of exercise diversity.

</details>


### [42] [On the Reliability of User-Centric Evaluation of Conversational Recommender Systems](https://arxiv.org/abs/2602.17264)
*Michael Müller,Amir Reza Mohammadi,Andreas Peintner,Beatriz Barroso Gstrein,Günther Specht,Eva Zangerle*

Main category: cs.IR

TL;DR: 该论文通过大规模实证研究调查了基于静态对话记录的用户中心型对话推荐系统（CRS）评估的可靠性。研究发现，实用性和结果导向的维度（如准确性、有用性、满意度）在聚合后达到中等可靠性，而社会性构建（如人性化程度和融洽感）可靠性较低。此外，许多维度坍缩为单一全局质量信号，揭示了第三方评判中的强烈晕轮效应。这些发现挑战了单一标注者和基于大语言模型的评估协议的有效性，并推动了对多评分者聚合和维度约简的需求。


<details>
  <summary>Details</summary>
Motivation: 当前对话推荐系统（CRS）的用户中心评估日益依赖众包工人或大语言模型对静态对话记录进行第三方标注，以实现可扩展的CRS评估。然而，这种做法的可靠性在很大程度上未经检验。为此，本文开展大规模实证研究，探究基于静态对话记录的用户中心型CRS评估的可靠性与结构，以验证当前评估方法的有效性。

Method: 研究收集了124名众包工人对200个ReDial对话的1,053条标注，采用18维度的CRS-Que框架。通过随机效应可靠性模型和相关性分析，量化各维度的稳定性及其相互依赖关系。

Result: 研究结果显示：实用性/结果导向维度（准确性、有用性、满意度）在聚合后达到中等可靠性；社会性构建（人性化程度、融洽感）可靠性显著较低；此外，许多维度坍缩为单一全局质量信号，揭示强烈晕轮效应。

Conclusion: 单一标注者及基于大语言模型的评估协议有效性存疑；需要采用多评分者聚合和维度约简方法来提升离线CRS评估的可靠性。

Abstract: User-centric evaluation has become a key paradigm for assessing Conversational Recommender Systems (CRS), aiming to capture subjective qualities such as satisfaction, trust, and rapport. To enable scalable evaluation, recent work increasingly relies on third-party annotations of static dialogue logs by crowd workers or large language models. However, the reliability of this practice remains largely unexamined. In this paper, we present a large-scale empirical study investigating the reliability and structure of user-centric CRS evaluation on static dialogue transcripts. We collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework. Using random-effects reliability models and correlation analysis, we quantify the stability of individual dimensions and their interdependencies. Our results show that utilitarian and outcome-oriented dimensions such as accuracy, usefulness, and satisfaction achieve moderate reliability under aggregation, whereas socially grounded constructs such as humanness and rapport are substantially less reliable. Furthermore, many dimensions collapse into a single global quality signal, revealing a strong halo effect in third-party judgments. These findings challenge the validity of single-annotator and LLM-based evaluation protocols and motivate the need for multi-rater aggregation and dimension reduction in offline CRS evaluation.

</details>


### [43] [Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation](https://arxiv.org/abs/2602.17354)
*Daniele Malitesta,Emanuele Rossi,Claudio Pomo,Tommaso Di Noia,Fragkiskos D. Malliaros*

Main category: cs.IR

TL;DR: 针对多模态推荐系统的模态缺失问题，本文首次形式化定义该问题，提出基于商品共购图的免训练特征传播方法，实验证明其优于传统插补策略。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统依赖的图像、描述等数据常存在噪声或缺失，当前简单丢弃缺失项的做法导致数据浪费，且该问题缺乏系统性形式化定义与解决方案，亟待研究。

Method: 研究将缺失模态问题重构为商品共购图的特征插值任务。通过用户-商品交互图构建商品共购图，提出四种免训练特征传播策略，利用图结构将已知特征传播至缺失节点完成补全。

Result: 在多个数据集上的实验表明：所提方法可无缝集成至现有框架；保持并扩大了多模态与传统推荐系统的性能优势；在不同缺失场景下显著优于传统机器学习插补方法；首次揭示了商品图特征同质性对插补效果的影响。

Conclusion: 图特征传播方法为多模态推荐中的模态缺失问题提供了有效解决方案，相比传统方法具有明显优势，为构建鲁棒的多模态推荐系统开辟了新途径。

Abstract: Multimodal recommender systems (RSs) represent items in the catalog through multimodal data (e.g., product images and descriptions) that, in some cases, might be noisy or (even worse) missing. In those scenarios, the common practice is to drop items with missing modalities and train the multimodal RSs on a subsample of the original dataset. To date, the problem of missing modalities in multimodal recommendation has still received limited attention in the literature, lacking a precise formalisation as done with missing information in traditional machine learning. In this work, we first provide a problem formalisation for missing modalities in multimodal recommendation. Second, by leveraging the user-item graph structure, we re-cast the problem of missing multimodal information as a problem of graph features interpolation on the item-item co-purchase graph. On this basis, we propose four training-free approaches that propagate the available multimodal features throughout the item-item graph to impute the missing features. Extensive experiments on popular multimodal recommendation datasets demonstrate that our solutions can be seamlessly plugged into any existing multimodal RS and benchmarking framework while still preserving (or even widen) the performance gap between multimodal and traditional RSs. Moreover, we show that our graph-based techniques can perform better than traditional imputations in machine learning under different missing modalities settings. Finally, we analyse (for the first time in multimodal RSs) how feature homophily calculated on the item-item graph can influence our graph-based imputations.

</details>


### [44] [Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers](https://arxiv.org/abs/2602.17410)
*Bingqian Li,Bowen Zheng,Xiaolei Wang,Long Zhang,Jinpeng Wang,Sheng Chen,Wayne Xin Zhao,Ji-rong Wen*

Main category: cs.IR

TL;DR: ILRec：一种利用中间层自硬负样本信号进行偏好微调的两阶段框架，通过跨层优化蒸馏与令牌级奖励机制，有效提升LLM推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法依赖序列级离线负样本，导致在大规模负样本空间中判别性不足、信息量有限。因此，需引入能动态反映模型学习过程的细粒度负监督信号以增强偏好学习效果。

Method: 提出ILRec框架：1) 从中间层识别自硬负样本令牌，作为动态细粒度负监督；2) 设计两阶段训练：跨层偏好优化与跨层偏好蒸馏，联合提升负样本判别能力与中间层信号质量；3) 引入轻量级协同过滤模型，通过令牌级奖励避免对假负样本的过度惩罚。

Result: 在三个公开数据集上的实验表明，ILRec能显著提升基于LLM的推荐系统性能。

Conclusion: ILRec通过挖掘中间层的自硬负样本信号，结合两阶段偏好学习和令牌级奖励机制，为大规模负样本空间的推荐任务提供了更有效的LLM微调方案。

Abstract: Large language models (LLMs) have shown great promise in recommender systems, where supervised fine-tuning (SFT) is commonly used for adaptation. Subsequent studies further introduce preference learning to incorporate negative samples into the training process. However, existing methods rely on sequence-level, offline-generated negatives, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. To address these challenges, we propose ILRec, a novel preference fine-tuning framework for LLM-based recommendation, leveraging self-hard negative signals extracted from intermediate layers to improve preference learning. Specifically, we identify self-hard negative tokens from intermediate layers as fine-grained negative supervision that dynamically reflects the model's preference learning process. To effectively integrate these signals into training, we design a two-stage framework comprising cross-layer preference optimization and cross-layer preference distillation, enabling the model to jointly discriminate informative negatives and enhance the quality of negative signals from intermediate layers. In addition, we introduce a lightweight collaborative filtering model to assign token-level rewards for negative signals, mitigating the risk of over-penalizing false negatives. Extensive experiments on three datasets demonstrate ILRec's effectiveness in enhancing the performance of LLM-based recommender systems.

</details>


### [45] [A Picture of Agentic Search](https://arxiv.org/abs/2602.17518)
*Francesca Pezzuti,Ophir Frieder,Fabrizio Silvestri,Sean MacAvaney,Nicola Tonellotto*

Main category: cs.IR

TL;DR: 针对自动化智能体重塑信息检索生态，本文揭示当前人类中心范式与智能体行为存在根本错配，提出ASQ数据集以捕获智能体检索增强系统的全链路数据（推理查询、检索文档、思维链），覆盖三大基准数据集、三个异构智能体和两种检索管道，并开源可扩展工具包。


<details>
  <summary>Details</summary>
Motivation: 智能体正大规模参与搜索，但IR系统、评估指标和数据集仍围绕人类设计，导致工作负载、可预测性和行为假设失效。缺乏智能体搜索行为数据成为制约数据驱动IR研究的关键瓶颈，亟需构建面向智能体的新型评估基准。

Method: 开发系统性数据收集框架，完整记录检索增强智能体回答查询时的所有输入输出数据，构建ASQ数据集，包含HotpotQA、Researchy Questions和MS MARCO上的推理诱导查询、检索结果和思维过程，覆盖3个异构智能体和2种检索管道，并提供可扩展工具包。

Result: 成功发布ASQ数据集，实证验证智能体查询模式与人工查询存在本质差异。该数据集包含智能体全链路行为数据，覆盖3大基准数据集、3个智能体和2种检索架构，配套工具包支持社区扩展，为智能体导向IR研究提供系统基准。

Conclusion: 研究呼吁IR范式向"人机协同"转型。ASQ填补了智能体搜索行为数据的关键空白，为开发适应智能体特性的检索系统、评估指标和用户模型奠定基础，推动IR技术在智能体时代的理论创新与应用发展。

Abstract: With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effectiveness, query pre-processing may add overhead without improving results, and standard metrics may mismeasure satisfaction. Without adaptation, retrieval models risk satisfying neither humans, nor the emerging user segment of agents. However, datasets capturing agent search behaviour are lacking, which is a critical gap given IR's historical reliance on data-driven evaluation and optimisation. We develop a methodology for collecting all the data produced and consumed by agentic retrieval-augmented systems when answering queries, and we release the Agentic Search Queryset (ASQ) dataset. ASQ contains reasoning-induced queries, retrieved documents, and thoughts for queries in HotpotQA, Researchy Questions, and MS MARCO, for 3 diverse agents and 2 retrieval pipelines. The accompanying toolkit enables ASQ to be extended to new agents, retrievers, and datasets.

</details>


### [46] [Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval](https://arxiv.org/abs/2602.17654)
*Jiaqi Xi,Raghav Saboo,Luming Chen,Martin Wang,Sudeep Das*

Main category: cs.IR

TL;DR: 针对电商搜索多类别检索需求，提出两阶段"挖掘与精炼"对比学习框架：第一阶段基于轻量级LLM微调的三级相关性标注，训练多语言Siamese双塔模型；第二阶段通过ANN挖掘困难样本，利用多类圆形损失锐化相关性边界。结合拼写增强和查询生成，显著提升了检索相关性和业务指标。


<details>
  <summary>Details</summary>
Motivation: 电商搜索需处理长尾噪声查询并满足政策约束，但相关性是分级的（精确匹配/替代/互补）。生产系统要求不同相关性层级间有清晰的相似度分数分离，以实现稳定混合检索和阈值设定，现有方法难以兼顾可扩展性与政策一致性。

Method: 1) 监督构建：轻量级LLM在人工标注上微调，遵循三级相关性指南，参与度审计降噪；2) 第一阶段：多语言Siamese双塔模型，标签感知监督对比损失构建全局语义空间；3) 第二阶段：ANN挖掘困难样本，LLM重标注，多类圆形损失锐化层级边界；4) 增强策略：加法拼写增强+合成查询生成提升鲁棒性。

Result: 离线评估和线上A/B测试显示，该框架显著提升检索相关性，在用户参与度和业务影响方面获得统计显著增益。

Conclusion: "挖掘与精炼"框架通过政策对齐的监督信号和边界锐化策略，有效优化了电商搜索语义嵌入空间，解决了多类别检索的长尾泛化问题，具备实际应用价值。

Abstract: We propose a two-stage "Mine and Refine" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [47] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.OH

TL;DR: 针对Shor算法对RSA的量子威胁，本研究提出了一个混合安全框架，整合AES经典加密、BB84量子密钥分发、量子态比较认证和生物启发免疫系统，为后量子时代数据保护提供概念性解决方案，但尚需实际验证。


<details>
  <summary>Details</summary>
Motivation: 量子计算对基于大数分解难题的RSA等传统密码体系构成颠覆性威胁，Shor算法可在多项式时间内高效破解RSA，而经典分解方法对大密钥效率不足，后量子密码迁移已成迫切需求。

Method: 设计四层混合安全架构：1) AES对称加密保障数据传输安全；2) BB84量子密钥分发协议实现可检测窃听的安全密钥交换；3) 量子态比较机制提供轻量级认证；4) 生物启发式免疫系统实现动态威胁感知与自适应防护。

Result: 理论分析表明：RSA存在Shor算法量子脆弱性；BB84在理想条件下可完成密钥完全协商且具备高准确性窃听检测能力；量子态比较认证机制可行；整体框架兼具经典与量子安全属性，具备可扩展性与自适应特性。

Conclusion: 本研究构建了后量子加密数据保护的概念性混合模型，通过融合经典密码与量子安全技术，为量子时代信息安全提供理论框架，但框架实现、安全性形式化证明和大规模实验验证仍属未来工作。

Abstract: Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [MMCAformer: Macro-Micro Cross-Attention Transformer for Traffic Speed Prediction with Microscopic Connected Vehicle Driving Behavior](https://arxiv.org/abs/2602.16730)
*Lei Han,Mohamed Abdel-Aty,Younggun Kim,Yang-Jun Joo,Zubayer Islam*

Main category: cs.LG

TL;DR: 该论文针对交通速度预测问题，提出Macro-Micro Cross-Attention Transformer (MMCAformer)模型，通过融合宏观交通流数据与基于网联车辆的微观驾驶行为特征，利用自注意力机制和交叉注意力机制捕获时空依赖关系，并采用学生t负对数似然损失函数进行不确定性估计。在佛罗里达四条高速公路上的实验表明，该模型相比基线方法显著提升了预测精度并降低了不确定性，其中急刹车和急加速频率是最具影响力的特征。


<details>
  <summary>Details</summary>
Motivation: 准确的速度预测对于主动交通管理至关重要，可提升交通效率与安全。现有研究主要依赖宏观交通流数据进行趋势预测，但实际道路交通动态同时受到个体微观驾驶行为的影响。网联车辆数据提供了丰富的驾驶行为特征，为将行为洞察融入速度预测提供了新机遇。

Method: 提出MMCAformer模型，该模型采用自注意力机制学习宏观交通流的内在依赖关系，使用交叉注意力机制捕捉宏观交通状态与微观驾驶行为之间的时空交互作用。模型通过学生t负对数似然损失函数进行优化，可同时提供点预测和不确定性估计。

Result: 在佛罗里达四条高速公路上的实验表明：1) 相比仅使用宏观特征，引入微观驾驶行为特征使RMSE、MAE和MAPE分别降低9.0%、6.9%和10.2%；2) 预测不确定性显著降低，平均预测区间宽度减少10.1-24.0%；3) 急刹车和急加速频率是最具影响力的特征；4) 在拥堵低速交通条件下改善效果更为显著。

Conclusion: 融合宏观交通流与微观驾驶行为的MMCAformer模型在速度预测任务中表现优异，不仅提高了预测精度，还降低了预测不确定性，为基于网联车辆数据的交通管理提供了有效解决方案。该研究验证了微观驾驶行为特征在交通预测中的重要价值，特别是在拥堵场景下。

Abstract: Accurate speed prediction is crucial for proactive traffic management to enhance traffic efficiency and safety. Existing studies have primarily relied on aggregated, macroscopic traffic flow data to predict future traffic trends, whereas road traffic dynamics are also influenced by individual, microscopic human driving behaviors. Recent Connected Vehicle (CV) data provide rich driving behavior features, offering new opportunities to incorporate these behavioral insights into speed prediction. To this end, we propose the Macro-Micro Cross-Attention Transformer (MMCAformer) to integrate CV data-based micro driving behavior features with macro traffic features for speed prediction. Specifically, MMCAformer employs self-attention to learn intrinsic dependencies in macro traffic flow and cross-attention to capture spatiotemporal interplays between macro traffic status and micro driving behavior. MMCAformer is optimized with a Student-t negative log-likelihood loss to provide point-wise speed prediction and estimate uncertainty. Experiments on four Florida freeways demonstrate the superior performance of the proposed MMCAformer compared to baselines. Compared with only using macro features, introducing micro driving behavior features not only enhances prediction accuracy (e.g., overall RMSE, MAE, and MAPE reduced by 9.0%, 6.9%, and 10.2%, respectively) but also shrinks model prediction uncertainty (e.g., mean predictive intervals decreased by 10.1-24.0% across the four freeways). Results reveal that hard braking and acceleration frequencies emerge as the most influential features. Such improvements are more pronounced under congested, low-speed traffic conditions.

</details>


### [49] [A Few-Shot LLM Framework for Extreme Day Classification in Electricity Markets](https://arxiv.org/abs/2602.16735)
*Saud Alghumayjan,Ming Yi,Bolun Xu*

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型(LLM)的小样本分类框架，用于预测次日电力实时价格尖峰。该框架将电力需求、可再生能源发电、天气预报和近期电价等系统状态信息聚合为统计特征，格式化为自然语言提示后输入LLM，输出尖峰概率及置信度。在德州电力市场的实证研究表明，该小样本方法性能可与SVM和XGBoost等监督模型相媲美，且在数据稀缺时表现更优，展现了LLM在电力价格预测中的数据高效性。


<details>
  <summary>Details</summary>
Motivation: 电力价格尖峰预测对电力市场运营和风险管理至关重要，但传统监督学习方法依赖大量标注数据。在实际应用中，历史数据往往有限，导致现有方法性能下降。因此，开发数据高效的小样本学习框架成为迫切需求。

Method: 提出基于LLM的小样本分类方法：1)采集电力系统状态信息（需求、可再生能源出力、天气预报、近期电价）；2)提取统计特征并转化为自然语言提示；3)将提示与通用指令一同输入LLM；4)模型输出次日尖峰概率和置信度分数。该方法利用LLM的语言理解能力进行few-shot学习。

Result: 基于德州电力市场历史数据的实验表明：该LLM小样本框架的分类性能与SVM和XGBoost等监督学习模型相当，但当历史数据有限时，其性能显著优于传统模型。

Conclusion: 本研究证实了大语言模型在电力价格尖峰分类中的潜力，特别适用于数据稀缺场景。该方法为电力系统短期价格预测提供了新的数据高效解决方案，具有实际应用价值。

Abstract: This paper proposes a few-shot classification framework based on Large Language Models (LLMs) to predict whether the next day will have spikes in real-time electricity prices. The approach aggregates system state information, including electricity demand, renewable generation, weather forecasts, and recent electricity prices, into a set of statistical features that are formatted as natural-language prompts and fed to an LLM along with general instructions. The model then determines the likelihood that the next day would be a spike day and reports a confidence score. Using historical data from the Texas electricity market, we demonstrate that this few-shot approach achieves performance comparable to supervised machine learning models, such as Support Vector Machines and XGBoost, and outperforms the latter two when limited historical data are available. These findings highlight the potential of LLMs as a data-efficient tool for classifying electricity price spikes in settings with scarce data.

</details>


### [50] [Real-time Secondary Crash Likelihood Prediction Excluding Post Primary Crash Features](https://arxiv.org/abs/2602.16739)
*Lei Han,Mohamed Abdel-Aty,Zubayer Islam,Chenzhu Wang*

Main category: cs.LG

TL;DR: 本文提出一种不依赖事后事故特征的二次事故可能性预测混合框架，通过动态时空窗口提取实时交通流和环境特征，结合集成学习与投票机制，在佛罗里达高速公路上实现91%的识别率与0.952的AUC值，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有二次事故预测方法依赖难以实时获取的事后事故特征（如事故类型与严重程度），限制了其在主动交通管理系统中的实际应用。亟需开发一种仅基于实时交通流与环境特征的预测框架。

Method: 提出混合预测框架：设计动态时空窗口从一次事故点及其上游提取实时交通流与环境特征；包含三个模型（一次事故模型与两个对比场景下的二次事故模型）；采用集成学习整合六种机器学习算法，并通过投票机制融合三模型输出。

Result: 佛罗里达高速公路实验表明：该框架正确识别91%的二次事故，误报率为0.20；AUC从单个模型的0.654、0.744、0.902提升至0.952，性能优于前人研究。

Conclusion: 所提混合框架在不依赖事后特征的情况下，可高精度预测二次事故可能性，具备实用价值，为缓解交通拥堵与事故负面影响提供了有效的智能交通管理解决方案。

Abstract: Secondary crash likelihood prediction is a critical component of an active traffic management system to mitigate congestion and adverse impacts caused by secondary crashes. However, existing approaches mainly rely on post-crash features (e.g., crash type and severity) that are rarely available in real time, limiting their practical applicability. To address this limitation, we propose a hybrid secondary crash likelihood prediction framework that does not depend on post-crash features. A dynamic spatiotemporal window is designed to extract real-time traffic flow and environmental features from primary crash locations and their upstream segments. The framework includes three models: a primary crash model to estimate the likelihood of secondary crash occurrence, and two secondary crash models to evaluate traffic conditions at crash and upstream segments under different comparative scenarios. An ensemble learning strategy integrating six machine learning algorithms is developed to enhance predictive performance, and a voting-based mechanism combines the outputs of the three models. Experiments on Florida freeways demonstrate that the proposed hybrid framework correctly identifies 91% of secondary crashes with a low false alarm rate of 0.20. The Area Under the ROC Curve improves from 0.654, 0.744, and 0.902 for the individual models to 0.952 for the hybrid model, outperforming previous studies.

</details>


### [51] [DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning](https://arxiv.org/abs/2602.16742)
*Haoxiang Sun,Lizhen Xu,Bing Zhao,Wotao Yin,Wei Wang,Boyu Yang,Rui Wang,Hu Wei*

Main category: cs.LG

TL;DR: 针对大型多模态模型RLVR训练中数据集多样性不足的问题，本研究提出DeepVision-103K数据集，涵盖K12数学多学科知识。实验表明，基于该数据训练的模型在数学推理基准测试中性能显著，且能泛化至通用多模态任务，有效提升了视觉感知、反思与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR数据集多源自小规模人工构建或历史资源重组，导致数据多样性与覆盖范围受限，成为制约大型多模态模型性能进一步提升的瓶颈。高质量、大规模、跨学科数据集的缺失阻碍了多模态推理能力的突破。

Method: 构建DeepVision-103K大规模数据集，包含10.3万样本，覆盖K12数学全学科知识点与丰富视觉元素。采用强化学习可验证奖励机制，通过可验证的数学答案作为奖励信号，优化大型多模态模型的视觉反思与推理能力。

Result: 在DeepVision-103K上训练的模型在多个多模态数学基准测试中表现优异，展现出良好的泛化能力，可迁移至通用多模态推理任务。深度分析显示，模型视觉感知精度、错误反思能力及逻辑推理能力均获得显著提升。

Conclusion: DeepVision-103K为RLVR训练提供了高质量的基准数据集，有效推动了大型多模态模型视觉推理能力的发展，为突破数据限制、提升模型性能提供了新的数据基础与方法论支持。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \textbf{DeepVision-103K}, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: \href{https://huggingface.co/datasets/skylenage/DeepVision-103K}{this url}.

</details>


### [52] [PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency](https://arxiv.org/abs/2602.16745)
*Zhangyi Liu,Huaizhi Qu,Xiaowei Yin,He Sun,Yanjun Han,Tianlong Chen,Zhun Deng*

Main category: cs.LG

TL;DR: PETS通过原则性轨迹分配优化测试时自一致性，在离线/在线设置下相比均匀采样最高节省75%/55%预算，同时实现完美自一致性能。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展通过聚合随机推理轨迹提升模型性能，但有限预算下的高效自一致性仍是开放挑战。需要理论指导的轨迹分配方法以提高采样效率。

Method: 1. 提出自一致率新度量标准（与无限预算多数投票的吻合度）2. 建立轨迹分配的优化框架 3. 离线场景：将推理轨迹类比众包中的标注员，利用成熟众包理论，设计基于多数投票的高效分配算法 4. 在线场景：基于离线框架，动态适应问题难度调整预算，保持理论保证和计算效率

Result: 实验表明PETS持续优于均匀分配；在GPQA上，PETS实现完美自一致性，相比均匀分配，离线场景预算减少达75%，在线场景减少达55%。

Conclusion: PETS为测试时自一致性提供了原则性且高效的研究框架，通过优化轨迹分配在理论和实践上均取得显著优势，大幅降低了计算预算需求。

Abstract: Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.

</details>


### [53] [Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking](https://arxiv.org/abs/2602.16746)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本研究通过几何分析揭示，Transformer在模算术任务中的训练轨迹主要位于低维执行子空间内，而垂直方向的曲率增长是泛化前的关键前兆，且正交梯度流是grokking的必要条件。


<details>
  <summary>Details</summary>
Motivation: Grokking现象——在小规模算法任务中从记忆到泛化的延迟转变——尚未被充分理解。本研究旨在通过几何视角探究其背后的优化动力学机制。

Method: 采用主成分分析（PCA）对注意力权重轨迹进行降维，测量连续梯度步的非对易性（即交换子缺陷），并将其投影到学习到的子空间上；通过曲率度量分析损失景观几何，并进行因果干预实验（抑制或增强正交梯度流）以验证必要性。

Result: 主要发现包括：（1）训练轨迹主要在低维执行子空间内演化，单个主成分可解释68-83%的方差；（2）垂直于该子空间的方向上曲率急剧增长，且曲率增长在泛化前出现，领先时间遵循幂律关系；（3）因果干预表明，沿子空间的运动是grokking所必需的，而单纯增加曲率并不充分；（4）正交梯度流是必要但非充分条件，抑制它可阻止泛化，而人为提升曲率缺陷无效果；（5）上述结果在多种学习率、不同慢速regime（lr=5e-5, wd=0.1, 3层）及三个随机种子下均重现，尽管不同regime间对齐动力学存在定量差异。

Conclusion: Grokking反映了从亚稳状态的逃逸，该状态以低维约束和横向曲率积累为特征。几何框架为理解这一现象提供了新视角，且正交梯度流是必要但非充分的驱动因素。

Abstract: Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experiments show that motion along the learned subspace is necessary for grokking, while artificially increasing curvature is insufficient. Together, these results support a geometric account in which grokking reflects escape from a metastable regime characterized by low-dimensional confinement and transverse curvature accumulation. All findings replicate across this learning-rate range, a qualitatively different slow regime (lr=5e-5, wd=0.1, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes. Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose-response across four operations, while artificially boosting curvature defects has no effect.

</details>


### [54] [LiveClin: A Live Clinical Benchmark without Leakage](https://arxiv.org/abs/2602.16747)
*Xidong Wang,Shuqi Guo,Yue Shen,Junying Chen,Jian Wang,Jinjie Gu,Ping Zhang,Lei Liu,Benyou Wang*

Main category: cs.LG

TL;DR: 为了解决医疗大模型评估中数据污染和知识过时导致分数虚高的问题，该论文提出了LiveClin——一个基于最新同行评议病例报告、每半年更新的动态临床基准测试。通过239名医生参与的AI-人工流程，将真实患者病例转化为覆盖完整临床路径的多模态评估场景。测试26个模型发现，最佳模型案例准确率仅35.7%，而主任医师和副主任医师等人类专家表现优于大多数模型。该基准为医疗大模型发展提供了持续演化的临床导向框架。


<details>
  <summary>Details</summary>
Motivation: 医疗大模型评估的可靠性受到数据污染和知识过时的严重破坏，导致静态基准测试分数虚高。现有基准无法反映真实临床实践的复杂性和时效性，亟需一个能够持续更新、抵抗污染且贴近实际临床工作的评估体系。

Method: 提出LiveClin动态基准测试，其核心方法包括：1）从当代同行评议病例报告中构建，每半年更新一次；2）采用AI-人工混合工作流，由239名医生参与，将真实患者病例转化为复杂多模态评估场景；3）覆盖从诊断到治疗的完整临床路径；4）包含1,407个病例报告和6,605个问题。

Result: 对26个医疗大模型的评估显示：最佳模型案例准确率仅为35.7%，表明真实临床场景极具挑战性。人类专家表现优异，主任医师准确率最高，副主任医师紧随其后，两者均超越大多数模型。这凸显了当前医疗大模型与临床专家之间的显著差距。

Conclusion: LiveClin提供了一个持续演化的临床基础框架，能够有效指导医疗大模型的发展，推动模型性能向人类专家水平靠拢，从而提高医疗大模型的可靠性和实际临床应用价值。数据和代码已开源。

Abstract: The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for approximating real-world clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. In benchmarking against human experts, Chief Physicians achieved the highest accuracy, followed closely by Attending Physicians, with both surpassing most models. LiveClin thus provides a continuously evolving, clinically grounded framework to guide the development of medical LLMs towards closing this gap and achieving greater reliability and real-world utility. Our data and code are publicly available at https://github.com/AQ-MedAI/LiveClin.

</details>


### [55] [Attending to Routers Aids Indoor Wireless Localization](https://arxiv.org/abs/2602.16762)
*Ayush Roy,Tahsin Fuad Hassan,Roshan Ayyalasomayajula,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 该论文针对基于Wi-Fi信号的机器学习无线定位在不同环境中性能受限的问题，提出了一种"路由器注意力"机制。通过引入注意力层对不同路由器的贡献进行差异化加权，该方法在开源数据集上相比基准架构准确率提升超过30%，证明了强调路由器相关性可显著改善定位性能。


<details>
  <summary>Details</summary>
Motivation: 传统加权三角定位方法启发研究者思考如何在聚合多路由器信息时为不同路由器分配适当权重。现有算法未能有效加权不同路由器的信息，导致收敛效果不佳和精度下降，这是当前面临的主要限制。

Method: 论文将注意力机制引入路由器信息聚合过程，在标准机器学习定位架构中嵌入注意力层，确保每个路由器在三角定位中的贡献被差异化加权，强调相关性更高的路由器信息。

Result: 在开源数据集上的评估表明，与基准架构相比，"路由器注意力"方法在准确率上实现了超过30%的显著提升，验证了差异化加权策略的有效性。

Conclusion: 研究证明，在聚合多路由器信息时强调各路由器的不同相关性可以显著改善无线定位的整体性能，为跨环境Wi-Fi定位提供了新的改进方向。

Abstract: Modern machine learning-based wireless localization using Wi-Fi signals continues to face significant challenges in achieving groundbreaking performance across diverse environments. A major limitation is that most existing algorithms do not appropriately weight the information from different routers during aggregation, resulting in suboptimal convergence and reduced accuracy. Motivated by traditional weighted triangulation methods, this paper introduces the concept of attention to routers, ensuring that each router's contribution is weighted differently when aggregating information from multiple routers for triangulation. We demonstrate, by incorporating attention layers into a standard machine learning localization architecture, that emphasizing the relevance of each router can substantially improve overall performance. We have also shown through evaluation over the open-sourced datasets and demonstrate that Attention to Routers outperforms the benchmark architecture by over 30% in accuracy.

</details>


### [56] [Omitted Variable Bias in Language Models Under Distribution Shift](https://arxiv.org/abs/2602.16784)
*Victoria Lin,Louis-Philippe Morency,Eli Ben-Michael*

Main category: cs.LG

TL;DR: 该研究揭示语言模型在分布偏移下的脆弱性源于可观测与不可观测成分的分解，现有方法仅处理前者导致遗漏变量偏差。作者构建理论框架将遗漏变量强度映射至最坏情况泛化性能边界，实验验证其在评估、优化及推断方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在分布偏移评估中表现出脆弱性，现有应对方法仅关注可观测的分布偏移成分，未能处理不可观测成分引发的遗漏变量偏差，从而影响模型评估的准确性和优化效果。

Method: 提出一种理论框架，量化不可观测遗漏变量的强度，并将其映射到语言模型在分布偏移下最坏情况泛化性能的理论边界。

Result: 实证研究表明，应用该框架的边界可直接用于语言模型评估与优化，提供更原则性的分布外性能度量；相比标准分布偏移调整方法，显著提升真实分布外性能；并在目标分布标签可用时，实现对遗漏变量强度的推断。

Conclusion: 该框架为语言模型分布偏移问题提供了严谨的理论基础，有效解决了遗漏变量偏差对评估和优化的负面影响，为分布外性能提升提供了新范式。

Abstract: Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify that the resulting omitted variable bias from unobserved variables can compromise both evaluation and optimization in language models. To address this challenge, we introduce a framework that maps the strength of the omitted variables to bounds on the worst-case generalization performance of language models under distribution shift. In empirical experiments, we show that using these bounds directly in language model evaluation and optimization provides more principled measures of out-of-distribution performance, improves true out-of-distribution performance relative to standard distribution shift adjustment methods, and further enables inference about the strength of the omitted variables when target distribution labels are available.

</details>


### [57] [Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency](https://arxiv.org/abs/2602.16787)
*Victoria Lin,Xinnuo Xu,Rachel Lawrence,Risa Ueno,Amit Sharma,Javier Gonzalez,Niranjani Prasad*

Main category: cs.LG

TL;DR: 本文提出双重反事实一致性(DCC)方法，无需标注数据即可评估大语言模型的因果推理能力，并作为测试时拒绝采样标准直接提升多个模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在标准推理基准上表现优异，但在处理反事实问题时表现出脆弱性，表明其因果推理能力存在缺陷。现有基于标注反事实数据的方法虽有效，但受限于反事实空间的巨大规模，难以扩展覆盖。

Method: 提出双重反事实一致性(DCC)，一种轻量级推理时方法，通过验证模型执行因果干预和反事实预测两大因果推理核心能力，实现无标注数据的评估与指导。

Result: 实验评估了多种领先大语言模型在不同推理任务上的因果推理能力，并证明DCC可作为免训练的测试时拒绝采样标准，直接提升跨模型家族的推理任务性能。

Conclusion: DCC为评估和提升大语言模型因果推理能力提供了有效且可扩展的解决方案，兼具理论意义和实用价值。

Abstract: Despite their strong performance on reasoning benchmarks, large language models (LLMs) have proven brittle when presented with counterfactual questions, suggesting weaknesses in their causal reasoning ability. While recent work has demonstrated that labeled counterfactual tasks can be useful benchmarks of LLMs' causal reasoning, producing such data at the scale required to cover the vast potential space of counterfactuals is limited. In this work, we introduce double counterfactual consistency (DCC), a lightweight inference-time method for measuring and guiding the ability of LLMs to reason causally. Without requiring labeled counterfactual data, DCC verifies a model's ability to execute two important elements of causal reasoning: causal intervention and counterfactual prediction. Using DCC, we evaluate the causal reasoning abilities of various leading LLMs across a range of reasoning tasks and interventions. Moreover, we demonstrate the effectiveness of DCC as a training-free test-time rejection sampling criterion and show that it can directly improve performance on reasoning tasks across multiple model families.

</details>


### [58] [Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models](https://arxiv.org/abs/2602.16793)
*Xingyu Dang,Rohit Agarwal,Rodrigo Porto,Anirudh Goyal,Liam H Fowl,Sanjeev Arora*

Main category: cs.LG

TL;DR: 本研究提出一种低成本推理管道，通过"猜想提取"和"上下文分离"技术解决"Cognitive Well"评分失效问题，仅使用通用现成模型Gemini 3.0 Pro在IMO-ProofBench Advanced上达到67.1%准确率，每题成本约31美元，性能领先且成本较现有方法降低两个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有IMO级数学推理方案面临双重瓶颈：定制未公开模型无法复现，而公开模型需大规模推理导致单题成本高达3000美元。核心障碍是求解-评分管道中的"Cognitive Well"失效模式——迭代优化收敛于求解器与内部评分器均认可的错误解。亟需一种低成本、高可及性的通用模型解决方案。

Method: 基于求解-评分管道失效机理，设计猜想提取策略：从生成解中分离候选引理，在独立环境中并行验证原命题及其否定命题（上下文分离），通过双重检验规避认知陷阱。仅依赖通用现成大模型，无需专用训练。

Result: 在IMO-ProofBench Advanced基准上，使用Gemini 3.0 Pro实现67.1%准确率，单题平均成本31美元。评估时点为该基准公开与未公开模型的SOTA，性能超次优公开管道超100%，成本降至约1/100。

Conclusion: 该管道通过结构化验证机制有效化解了评分一致性失效问题，证明通用模型可通过智能推理编排而非单纯规模扩展实现顶尖数学推理能力，为高性价比、可访问的AI数学推理提供了可行范式。

Abstract: In the past year, custom and unreleased math reasoning models reached gold medal performance on the International Mathematical Olympiad (IMO). Similar performance was then reported using large-scale inference on publicly available models but at prohibitive costs (e.g., 3000 USD per problem). In this work, we present an inference pipeline that attains best-in-class performance on IMO-style math problems at an average inference cost orders of magnitude below competing methods while using only general-purpose off-the-shelf models. Our method relies on insights about grader failure in solver-grader pipelines, which we call the Cognitive Well (iterative refinement converging to a wrong solution that the solver as well as the pipeline's internal grader consider to be basically correct). Our pipeline addresses these failure modes through conjecture extraction, wherein candidate lemmas are isolated from generated solutions and independently verified alongside their negations in a fresh environment (context detachment). On IMO-ProofBench Advanced (PB-Adv), our pipeline achieves 67.1 percent performance using Gemini 3.0 Pro with an average cost per question of approximately 31 USD. At the time of evaluation, this represented the state-of-the-art on PB-Adv among both public and unreleased models, and more than doubles the success rate of the next best publicly accessible pipeline, all at a fraction of the cost.

</details>


### [59] [Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning](https://arxiv.org/abs/2602.16796)
*Zifan Wang,Riccardo De Santi,Xiaoyu Mo,Michael M. Zavlanos,Andreas Krause,Karl H. Johansson*

Main category: cs.LG

TL;DR: 本文提出基于条件风险价值的尾部感知流模型微调方法，通过变分对偶分解将条件风险价值优化解耦为轻量级阈值优化和伪奖励熵正则化微调两阶段，实现与标准期望微调相当的效率，同时分别控制高奖励尾部的探索和低奖励尾部的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有熵正则化微调方法仅最大化期望奖励，无法控制尾部行为；而实际部署中低奖励尾部决定系统可靠性，高奖励尾部促进新颖发现，尾部控制至关重要。

Method: 提出尾部感知流模型微调算法：利用条件风险价值的变分对偶形式，将尾部优化分解为（1）一维阈值搜索与（2）基于特定伪奖励的熵正则化微调两个解耦阶段，分别实现高奖励尾部条件风险价值优化（探索新颖样本）和低奖励尾部条件风险价值优化（控制最差情况）。

Result: 计算成本与标准期望微调方法相当；在说明性实验、高维文本到图像生成和分子设计任务中验证了有效性。

Conclusion: 该方法为扩散/流模型的分布感知微调提供了原则性高效框架，能够在单次微调中实现尾部行为的精确控制，平衡系统可靠性与新颖性发现。

Abstract: Fine-tuning pre-trained diffusion and flow models to optimize downstream utilities is central to real-world deployment. Existing entropy-regularized methods primarily maximize expected reward, providing no mechanism to shape tail behavior. However, tail control is often essential: the lower tail determines reliability by limiting low-reward failures, while the upper tail enables discovery by prioritizing rare, high-reward outcomes. In this work, we present Tail-aware Flow Fine-Tuning (TFFT), a principled and efficient distributional fine-tuning algorithm based on the Conditional Value-at-Risk (CVaR). We address two distinct tail-shaping goals: right-CVaR for seeking novel samples in the high-reward tail and left-CVaR for controlling worst-case samples in the low-reward tail. Unlike prior approaches that rely on non-linear optimization, we leverage the variational dual formulation of CVaR to decompose it into a decoupled two-stage procedure: a lightweight one-dimensional threshold optimization step, and a single entropy-regularized fine-tuning process via a specific pseudo-reward. This decomposition achieves CVaR fine-tuning efficiently with computational cost comparable to standard expected fine-tuning methods. We demonstrate the effectiveness of TFFT across illustrative experiments, high-dimensional text-to-image generation, and molecular design.

</details>


### [60] [HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind](https://arxiv.org/abs/2602.16826)
*Nigel Doering,Rahath Malladi,Arshia Sangwan,David Danks,Tauhidur Rahman*

Main category: cs.LG

TL;DR: 本文提出HiVAE，一种分层变分架构，用于将心智理论(ToM)推理扩展到现实时空领域。受人类认知的信念-欲望-意图结构启发，该模型采用三级VAE层次结构，在3,185个节点的校园导航任务上取得显著性能提升，但发现学习到的潜在表示缺乏与实际心智状态的显式grounding。


<details>
  <summary>Details</summary>
Motivation: 现有心智理论方法主要局限于小型网格世界空间，无法有效扩展到复杂的现实时空域，这限制了AI系统在真实场景中推断智能体隐藏目标和心理状态的能力。

Method: 提出HiVAE分层变分架构，模仿人类认知的信念-欲望-意图模型，构建三级VAE层次结构来实现可扩展的心智理论推理。

Result: 在3,185节点的校园导航任务上实现显著性能提升，但同时识别出关键局限：分层结构虽改善预测效果，但学习到的潜在表示缺乏与实际心智状态的显式对应关系(grounding)。

Conclusion: 提出自监督对齐策略以解决grounding问题，并期望通过本工作征求社区对grounding方法的反馈，推动心智理论在现实场景中的可解释性和可靠性发展。

Abstract: Theory of mind (ToM) enables AI systems to infer agents' hidden goals and mental states, but existing approaches focus mainly on small human understandable gridworld spaces. We introduce HiVAE, a hierarchical variational architecture that scales ToM reasoning to realistic spatiotemporal domains. Inspired by the belief-desire-intention structure of human cognition, our three-level VAE hierarchy achieves substantial performance improvements on a 3,185-node campus navigation task. However, we identify a critical limitation: while our hierarchical structure improves prediction, learned latent representations lack explicit grounding to actual mental states. We propose self-supervised alignment strategies and present this work to solicit community feedback on grounding approaches.

</details>


### [61] [VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study](https://arxiv.org/abs/2602.16833)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 该论文针对大语言模型强化学习后训练中的探索瓶颈问题，提出在提示词中显式表示动作掩码（VAM），并通过迭代剪枝动作空间来控制探索，在象棋任务中提升了学习效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习后训练面临稀疏反馈和大动作空间的挑战，容易导致过早陷入重复行为而探索不足。现有方法缺乏可控的探索机制。

Method: 提出言语化动作掩码（VAM）：在提示词中显式表示可行动作集合，强制模型从掩码范围内输出动作。基于此引入迭代动作空间剪枝策略：若目标动作未被采样，则移除已采样的有效动作并缩减候选集重新采样，重复直至命中目标或预算耗尽。

Result: 在国际象棋任务上，无论是引擎对弈生成状态还是固定数据集训练，VAM在平均中心兵损失指标上均优于强基线，显著提升了学习效率和最终性能。

Conclusion: 言语化掩码是一种实用且有效的可控探索机制，为LLM的强化学习后训练提供了新的解决方案，在复杂动作空间中展现出良好应用前景。

Abstract: Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.

</details>


### [62] [A Residual-Aware Theory of Position Bias in Transformers](https://arxiv.org/abs/2602.16837)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: cs.LG

TL;DR: 该论文揭示了Transformer位置偏置的架构起源，提出残差感知的注意力rollout理论，证明残差连接防止了理论预测的注意力坍缩，并发现有限深度因果Transformer会产生U形位置偏置，为"中间迷失"现象提供了架构层面的解释。


<details>
  <summary>Details</summary>
Motivation: 现有理论分析预测无限深度下因果掩码会导致注意力不可避免地坍缩至首个token，但实践中此现象并未发生。这一理论与实践的鸿沟表明当前对Transformer位置偏置的架构根源理解不足，亟需更精确的框架来解释实际观察到的注意力分布模式。

Method: 提出残差感知的累积注意力rollout理论，将残差连接这一关键架构组件纳入分析框架，通过理论推导研究不同深度条件下因果Transformer的注意力传播机制，并与实际观察结果进行对比验证。

Result: 研究结果表明：1）残差连接在现实条件下有效防止了注意力坍缩；2）有限深度因果Transformer必然诱导U形位置偏置，注意力集中于序列首尾token；3）该结果为"中间迷失"现象提供了基于架构原则的理论解释。

Conclusion: 该研究阐明残差连接是塑造Transformer位置偏置的关键架构因素，U形注意力模式是架构固有特性而非训练产物。这一发现深化了对自回归Transformer注意力机制的理解，为改善长上下文建模能力奠定了理论基础。

Abstract: Transformer models systematically favor certain token positions, yet the architectural origins of this position bias remain poorly understood. Under causal masking at infinite depth, prior theoretical analyses of attention rollout predict an inevitable collapse of attention onto the first token. Such collapse, however, does not occur in practice. We resolve this discrepancy with a residual-aware theory of cumulative attention rollout. By incorporating residual connections, we show that this architectural component prevents collapse under realistic conditions. At finite depth, we prove that causal Transformers induce a U-shaped position bias, with attention concentrating on early and late tokens. This result provides a principled architectural explanation for the Lost-in-the-Middle phenomenon.

</details>


### [63] [Training Large Reasoning Models Efficiently via Progressive Thought Encoding](https://arxiv.org/abs/2602.16839)
*Zeliang Zhang,Xiaodong Liu,Hao Cheng,Hao Sun,Chenliang Xu,Jianfeng Gao*

Main category: cs.LG

TL;DR: 本文提出渐进式思维编码（Progressive Thought Encoding），一种参数高效微调方法，用于解决大型推理模型在强化学习训练中的内存效率问题。该方法通过将中间推理过程渐进编码为固定大小的向量表示，使模型能够在固定大小缓存下有效推理，无需通过完整缓存展开进行反向传播，从而显著减少训练内存使用，同时在推理期间保持恒定内存。在六个数学基准测试上的实验表明，该方法相比LoRA微调和未微调模型分别平均提升19.3%和29.9%的性能，在AIME2024/2025上最高可提升23.4%的准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂问题上表现出色，但面临强化学习训练效率的关键瓶颈：基于结果奖励的训练需要长序列展开，其中自回归解码主导了时间和内存消耗。虽然滑动窗口缓存策略可以限制内存，但会破坏长上下文推理并导致性能下降。因此，亟需一种能够在固定大小缓存下保持高效推理的训练方法，以解决实际内存约束下的可扩展性问题。

Method: 渐进式思维编码通过参数高效微调，使大型推理模型能够在固定大小缓存下有效推理。核心思想是将中间推理步骤渐进地编码为固定大小的向量表示，从而消除通过完整缓存展开进行反向传播的需求。这种方法在训练时大幅降低内存使用，在推理时保持恒定内存占用，同时维持长上下文推理能力。

Result: 在Qwen2.5-3B-Instruct、Qwen2.5-7B-Instruct和DeepSeek-R1-Distill-Llama-8B三个模型上，于六个具有挑战性的数学基准测试中，该方法相比基于LoRA的微调平均提升19.3%，相比未微调的大型推理模型平均提升29.9%。在AIME2024/2025基准测试中，在相同严格的缓存预算下，最高可获得23.4%的准确率提升。

Conclusion: 渐进式思维编码不仅显著提高了推理准确率，还使大型推理模型的强化学习训练在实际内存约束下变得更加高效和可扩展，为资源受限环境下的长序列推理任务提供了有效解决方案。

Abstract: Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.

</details>


### [64] [What is the Value of Censored Data? An Exact Analysis for the Data-driven Newsvendor](https://arxiv.org/abs/2602.16842)
*Rachitesh Kumar,Omar Mouchtaki*

Main category: cs.LG

TL;DR: 本文研究删失需求下的离线数据驱动报童问题，提出了计算最坏情况遗憾的通用方法，将无限维非凸优化约简为有限维问题，揭示了主动探索可突破删失数据的学习瓶颈，使Kaplan-Meier策略在严重删失下仍接近最优，而"销售额即需求"启发式则性能严重退化。


<details>
  <summary>Details</summary>
Motivation: 传统研究假设需求完全观测，而实践中需求被库存水平删失（仅观测到销售量），导致学习受限。本文旨在解决删失数据下的离线库存学习问题，评估经典策略性能，并探索主动探索能否改善学习效果。

Method: 通过分析删失数据的经验分布结构，提出了一种将无限维非凸优化问题精确约简为有限维问题的通用算法框架，可计算任意样本量和删失水平下的最坏情况遗憾，并应用于Kaplan-Meier等策略。

Result: 实现了无限维到有限维问题的约简；证明少量高端库存探索能显著改善最坏情况保证；发现"销售额即需求"启发式随删失累积性能严重退化；揭示了销售点信息质量对离线学习的关键作用。

Conclusion: 删失数据限制了被动学习，但少量主动探索可有效突破瓶颈。Kaplan-Meier策略在主动探索下表现优异，而朴素启发式在严重删失时失效。研究强调了精确记录缺货信息的重要性，为库存管理提供了理论指导。

Abstract: We study the offline data-driven newsvendor problem with censored demand data. In contrast to prior works where demand is fully observed, we consider the setting where demand is censored at the inventory level and only sales are observed; sales match demand when there is sufficient inventory, and equal the available inventory otherwise. We provide a general procedure to compute the exact worst-case regret of classical data-driven inventory policies, evaluated over all demand distributions. Our main technical result shows that this infinite-dimensional, non-convex optimization problem can be reduced to a finite-dimensional one, enabling an exact characterization of the performance of policies for any sample size and censoring levels. We leverage this reduction to derive sharp insights on the achievable performance of standard inventory policies under demand censoring. In particular, our analysis of the Kaplan-Meier policy shows that while demand censoring fundamentally limits what can be learned from passive sales data, just a small amount of targeted exploration at high inventory levels can substantially improve worst-case guarantees, enabling near-optimal performance even under heavy censoring. In contrast, when the point-of-sale system does not record stockout events and only reports realized sales, a natural and commonly used approach is to treat sales as demand. Our results show that policies based on this sales-as-demand heuristic can suffer severe performance degradation as censored data accumulates, highlighting how the quality of point-of-sale information critically shapes what can, and cannot, be learned offline.

</details>


### [65] [ML-driven detection and reduction of ballast information in multi-modal datasets](https://arxiv.org/abs/2602.16876)
*Yaroslav Solovko*

Main category: cs.LG

TL;DR: 本文提出了一个通用的多模态框架，用于检测和减少现代数据集中的"ballast"（冗余信息）。通过整合多种特征选择方法并引入新的Ballast Score，该框架能够在保持或提升分类性能的同时，剪枝高达70%的特征空间，显著降低计算成本和内存占用。


<details>
  <summary>Details</summary>
Motivation: 现代数据集常包含大量冗余或低价值信息（称为"ballast"），这些信息增加了维度、存储需求和计算成本，却不贡献有意义的分析价值。这种冗余数据使得机器学习管道效率低下，亟需一种系统性的方法来识别和消除这些无用的特征。

Method: 研究采用多模态方法，结合熵、互信息、Lasso回归、SHAP值、主成分分析（PCA）、主题建模和嵌入分析等多种技术来识别冗余特征。提出了一个新颖的Ballast Score来整合这些不同方法的信号，形成统一的跨模态剪枝策略，适用于结构化、半结构化、非结构化和稀疏数据类型。

Result: 实验结果表明，在稀疏或半结构化数据中，可以剪枝超过70%的特征空间，同时保持或提升分类性能，并大幅减少训练时间和内存占用。研究还揭示了不同类型的ballast（如统计性、语义性、基础设施性），为构建更高效的机器学习管道提供了实用指导。

Conclusion: 该框架有效解决了数据冗余问题，通过系统性的特征剪枝实现了更精简、高效的机器学习工作流。Ballast Score的提出为跨模态特征选择提供了新思路，对于优化计算资源、提升模型效率具有重要实践价值。

Abstract: Modern datasets often contain ballast as redundant or low-utility information that increases dimensionality, storage requirements, and computational cost without contributing meaningful analytical value. This study introduces a generalized, multimodal framework for ballast detection and reduction across structured, semi-structured, unstructured, and sparse data types. Using diverse datasets, entropy, mutual information, Lasso, SHAP, PCA, topic modelling, and embedding analysis are applied to identify and eliminate ballast features. A novel Ballast Score is proposed to integrate these signals into a unified, cross-modal pruning strategy. Experimental results demonstrate that significant portions of the feature space as often exceeding 70% in sparse or semi-structured data, can be pruned with minimal or even improved classification performance, along with substantial reductions in training time and memory footprint. The framework reveals distinct ballast typologies (e.g. statistical, semantic, infrastructural), and offers practical guidance for leaner, more efficient machine learning pipelines.

</details>


### [66] [Construction of a classification model for dementia among Brazilian adults aged 50 and over](https://arxiv.org/abs/2602.16887)
*F. S. Menezes,M. C. F. G. Barretto,E. Q. C. Garcia,T. A. E. Ferreira,J. G. Alvez*

Main category: cs.LG

TL;DR: 为巴西中老年人群开发基于低成本可变因素的痴呆分类模型，利用ELSI-Brazil数据，结合随机森林和逻辑回归分析，识别文盲、高龄、低握力等关键风险因素，RF模型AUC达0.776。


<details>
  <summary>Details</summary>
Motivation: 针对巴西中老年人群，利用低成本且可干预的变量，构建痴呆分类模型，以识别高风险个体并指导公共卫生资源分配和预防策略。

Method: 采用横断面设计的观察性预测模型研究，基于巴西老龄化纵向研究（ELSI-Brazil）9,412名参与者数据，通过神经心理学评估和知情者报告确定痴呆状态，运用Python实现随机森林和多变量逻辑回归分析。

Result: 痴呆患病率9.6%。主要风险因素：文盲（OR=7.42）、90岁以上（OR=11.00）、低体重（OR=2.11）、低握力（OR=2.50）、自述黑人种（OR=1.47）、缺乏运动（OR=1.61）、听力损失（OR=1.65）、抑郁症状（OR=1.72）；保护因素：高教育（OR=0.44）、高生活满意度（OR=0.72）、就业（OR=0.78）。随机森林模型AUC-ROC为0.776，灵敏度0.708，特异度0.702，F1分数0.311，准确率0.703，优于逻辑回归。

Conclusion: 研究证实痴呆的多维本质及可干预因素的重要性，加强脑健康相关的公共政策有助于巴西初级卫生保健资源的有效配置和痴呆预防。

Abstract: To build a dementia classification model for middle-aged and elderly Brazilians, implemented in Python, combining variable selection and multivariable analysis, using low-cost variables with modification potential. Observational study with a predictive modeling approach using a cross-sectional design, aimed at estimating the chances of developing dementia, using data from the Brazilian Longitudinal Study of Aging (ELSI-Brazil), involving 9,412 participants. Dementia was determined based on neuropsychological assessment and informant-based cognitive function. Analyses were performed using Random Forest (RF) and multivariable logistic regression to estimate the risk of dementia in the middle-aged and elderly populations of Brazil. The prevalence of dementia was 9.6%. The highest odds of dementia were observed in illiterate individuals (Odds Ratio (OR) = 7.42), individuals aged 90 years or older (OR = 11.00), low weight (OR = 2.11), low handgrip strength (OR = 2.50), self-reported black skin color (OR = 1.47), physical inactivity (OR = 1.61), self-reported hearing loss (OR = 1.65), and presence of depressive symptoms (OR = 1.72). Higher education (OR=0.44), greater life satisfaction (OR=0.72), and being employed (OR=0.78) were protective factors. The RF model outperformed logistic regression, achieving an area under the ROC curve of 0.776, with a sensitivity of 0.708, a specificity of 0.702, an F1-score of 0.311, a G-means of 0.705, and an accuracy of 0.703. Conclusion: The findings reinforce the multidimensional nature of dementia and the importance of accessible factors for identifying vulnerable individuals. Strengthening public policies focused on promoting brain health can contribute significantly to the efficient allocation of resources in primary care and dementia prevention in Brazil

</details>


### [67] [Multi-Agent Lipschitz Bandits](https://arxiv.org/abs/2602.16965)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 本文研究了连续Lipschitz结构动作空间下的去中心化多玩家随机老虎机问题，其中硬碰撞会导致零奖励。提出了一种模块化协议，通过新颖的极大值导向搜索实现玩家协调，然后解耦为N个独立的单玩家Lipschitz老虎机。该算法在达到近最优遗憾界O~(T^(d+1)/(d+2))的同时，仅产生与时间范围T无关的协调成本，首次在多玩家场景中匹配单玩家性能。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，多个玩家在没有通信的情况下协调探索连续动作空间是一个基本挑战。当玩家碰撞时获得零奖励，如何避免碰撞同时最大化集体奖励，并且协调成本不能随时间线性增长，这是去中心化学习的核心难题。现有方法要么依赖通信，要么在离散动作空间下有效，缺乏适用于连续空间的通信无关高效算法。

Method: 采用模块化协议：1）协调阶段：通过极大值导向搜索识别动作空间中的高价值区域，并将玩家"安置"在不同区域以避免碰撞；2）解耦阶段：将问题转化为N个独立的单玩家Lipschitz老虎机问题，各玩家独立探索分配到的区域。该方法无需通信，且协调成本与时间范围T无关。

Result: 证明了算法达到近最优遗憾界O~(T^(d+1)/(d+2))，其中d为动作空间维度，同时产生O(1)的协调成本。该遗憾界与单玩家Lipschitz老虎机的最优率相匹配，且协调成本不随T增长。此结论可推广至一般距离阈值碰撞模型。

Conclusion: 本文首次提出了在连续Lipschitz结构空间下，去中心化多玩家老虎机问题的通信无关框架，实现了近最优的集体遗憾界和T独立的协调成本。该方法为多智能体无通信协调提供了新思路，可扩展至更广泛的碰撞模型，具有重要的理论和应用价值。

Abstract: We study the decentralized multi-player stochastic bandit problem over a continuous, Lipschitz-structured action space where hard collisions yield zero reward. Our objective is to design a communication-free policy that maximizes collective reward, with coordination costs that are independent of the time horizon $T$. We propose a modular protocol that first solves the multi-agent coordination problem -- identifying and seating players on distinct high-value regions via a novel maxima-directed search -- and then decouples the problem into $N$ independent single-player Lipschitz bandits. We establish a near-optimal regret bound of $\tilde{O}(T^{(d+1)/(d+2)})$ plus a $T$-independent coordination cost, matching the single-player rate. To our knowledge, this is the first framework providing such guarantees, and it extends to general distance-threshold collision models.

</details>


### [68] [A Unified Framework for Locality in Scalable MARL](https://arxiv.org/abs/2602.16966)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 本文针对多智能体强化学习的维度灾难问题，提出策略依赖的局部性理论。通过分解策略诱导相互依赖矩阵，揭示平滑策略可诱导局部性的机理，推导出更紧的谱条件ρ(E^s+E^aΠ(π))<1，并应用于局部块坐标策略改进。


<details>
  <summary>Details</summary>
Motivation: 现有利用价值函数指数衰减性质(EDP)解决维度灾难的方法过于保守，因其仅基于环境最坏情况边界而忽略策略的正则化效应，无法刻画策略对局部性的影响。

Method: 提出策略诱导相互依赖矩阵H^π的新分解方式，将其分解为环境状态敏感性E^s、环境动作敏感性E^a与策略状态敏感性Π(π)，解耦环境与策略对局部性的贡献。

Result: 推导出指数衰减的谱条件ρ(E^s+E^aΠ(π))<1，该条件严格优于先前范数条件，并揭示局部性与最优性之间的根本权衡：平滑策略增强局部性但可能限制最优性。

Conclusion: 该理论框架支持分析具有可证明保证的局部块坐标策略改进方法，其性能保证直接与谱半径相关，为可扩展多智能体学习提供了更紧致的理论基础。

Abstract: Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a \emph{policy-dependent} phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, $H^π$, which decouples the environment's sensitivity to state ($E^{\mathrm{s}}$) and action ($E^{\mathrm{a}}$) from the policy's sensitivity to state ($Π(π)$). This decomposition reveals that locality can be induced by a smooth policy (small $Π(π)$) even when the environment is strongly action-coupled, exposing a fundamental locality-optimality tradeoff. We use this framework to derive a general spectral condition $ρ(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) < 1$ for exponential decay, which is strictly tighter than prior norm-based conditions. Finally, we leverage this theory to analyze a provably-sound localized block-coordinate policy improvement framework with guarantees tied directly to this spectral radius.

</details>


### [69] [Early-Warning Signals of Grokking via Loss-Landscape Geometry](https://arxiv.org/abs/2602.16967)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本文研究Grokking现象（从记忆到泛化的突变）在模运算之外的普适性。通过SCAN组合泛化和Dyck-1深度预测两个序列学习任务，发现commutator defect（一种曲率度量）在泛化前显著上升，遵循超线性幂律。因果干预表明，增强非交换性可加速Grokking（SCAN约32%，Dyck约50%），而抑制正交梯度流则会延迟或阻止该现象。commutator defect被确定为变压器中延迟泛化的鲁棒、架构无关的因果预警信号。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现Grokking现象与模运算中低维执行流形的限制相关，但这一机制是否适用于算术以外的任务仍是未解问题。本研究旨在探索Grokking的普适机制，寻找超越特定任务的早期预警信号，以理解深度学习中泛化转变的根本原理。

Method: 研究采用两个序列学习基准：SCAN组合泛化任务和Dyck-1语法深度预测任务。通过测量commutator defect（源自非交换梯度更新的曲率度量）和权重空间PCA分析，在不同学习率下追踪训练动态。进一步实施因果干预实验：放大非交换性以加速Grokking，抑制正交梯度流以延迟或阻止该现象，从而验证其因果作用。

Result: 1. commutator defect在泛化前显著上升，领先时间遵循超线性幂律（SCAN α≈1.18，Dyck α≈1.13），与模运算结果一致；2. 权重空间PCA显示光谱集中并非普遍前兆，而commutator defect是稳健指标；3. 因果干预证实：增强非交换性使Grokking加速（SCAN约32%，Dyck约50%）；4. 抑制正交梯度流在所有任务中均延迟或阻止Grokking，确立其必要性；5. 三个任务族呈现因果敏感性谱：模运算最刚性，Dyck最敏感，SCAN居中。

Conclusion: 研究确立了commutator defect作为变压器中延迟泛化的鲁棒、架构无关且因果相关的普适预警信号。这一发现揭示了Grokking机制超越特定任务限制，为理解深度学习中的泛化转变提供了新的理论框架，并暗示非交换梯度动态是泛化涌现的关键驱动因素。

Abstract: Grokking -- the abrupt transition from memorization to generalization after prolonged training -- has been linked to confinement on low-dimensional execution manifolds in modular arithmetic. Whether this mechanism extends beyond arithmetic remains open. We study two sequence-learning benchmarks: SCAN compositional generalization and Dyck-1 depth prediction. Across both tasks and a wide range of learning rates, the commutator defect -- a curvature measure derived from non-commuting gradient updates -- rises well before generalization, with lead times following a superlinear power law (alpha approximately 1.18 for SCAN, approximately 1.13 for Dyck), consistent with prior results on modular arithmetic. Weight-space PCA reveals that spectral concentration is not a universal precursor; the commutator defect is. Causal interventions demonstrate a mechanistic role: amplifying non-commutativity accelerates grokking (roughly 32% on SCAN, roughly 50% on Dyck), while suppressing orthogonal gradient flow delays or prevents it. The three task families form a spectrum of causal sensitivity -- modular arithmetic is rigid, Dyck is responsive, SCAN is intermediate -- yet suppression delays or prevents grokking in all cases, establishing necessity as a universal finding. These results identify the commutator defect as a robust, architecture-agnostic, causally implicated early-warning signal for delayed generalization in transformers.

</details>


### [70] [Fail-Closed Alignment for Large Language Models](https://arxiv.org/abs/2602.16977)
*Zachary Coalson,Beth Sohler,Aiden Gabriel,Sanghyun Hong*

Main category: cs.LG

TL;DR: 本文针对大语言模型对齐中的结构性弱点——拒绝机制"失效开放"问题，提出"失效封闭"对齐原则及渐进式对齐框架。通过迭代消融已学习的拒绝方向并重构独立安全子空间，该方法在四次越狱攻击下实现最强鲁棒性，同时减少过度拒绝并保持生成质量，计算开销小。机制研究证实模型编码了多个因果独立的拒绝方向。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对齐存在结构性弱点：拒绝机制呈"失效开放"特性，即通过提示越狱抑制单一主导特征即可导致对齐崩溃，引发不安全生成。这促使研究者寻求更鲁棒的安全对齐方法。

Method: 提出"失效封闭"对齐原则的具体实现：渐进式对齐框架。该框架迭代识别并消融已学习的拒绝方向，迫使模型沿新的独立子空间重构安全机制，形成冗余且因果独立的拒绝路径。

Result: 在四次越狱攻击测试中，该方法展现出最强的综合鲁棒性，同时有效缓解过度拒绝问题并保持生成质量，计算开销较小。

Conclusion: 机理分析证实，该方法训练的模型编码了多个因果独立的拒绝方向，无法被提示越狱同时抑制，为失效封闭对齐作为鲁棒安全的基础原则提供了实证支持。

Abstract: We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.

</details>


### [71] [Discovering Universal Activation Directions for PII Leakage in Language Models](https://arxiv.org/abs/2602.16980)
*Leo Marchyok,Zachary Coalson,Sungho Keum,Sooel Son,Sanghyun Hong*

Main category: cs.LG

TL;DR: ...


<details>
  <summary>Details</summary>
Motivation: ...

Method: ...

Result: ...

Conclusion: ...

Abstract: Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.

</details>


### [72] [Dynamic Delayed Tree Expansion For Improved Multi-Path Speculative Decoding](https://arxiv.org/abs/2602.16994)
*Rahul Thomas,Teo Kitanovski,Micah Goldblum,Arka Pal*

Main category: cs.LG

TL;DR: 本文通过系统评估推测解码中的验证策略，发现遍历验证（Traversal Verification）持续优于基于最优传输（OT）的方法，原因是OT方法在草稿树根部获得高多令牌接受率，而多令牌收益在分布差异更大的深层才最显著。基于此洞察，作者提出延迟树扩展和动态神经选择器，首次使OT类方法（如SpecInfer）反超遍历验证，平均吞吐量提升5%。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码研究提出了多种验证算法，但在匹配设定下各算法的相对性能仍不明确，特别是为何理论上具有优势的基于最优传输的方法表现不佳，这限制了该领域的进一步发展。

Method: 首先对多种验证策略在模型族、任务和采样机制上进行系统评估；分析发现OT方法收益与草稿树深度错配的问题；提出延迟树扩展方法，通过延后i.i.d.分支点来优化分布匹配；进一步开发动态神经选择器，根据草稿和目标模型特征预测OT方法的预期块效率，实现上下文感知的扩展决策。

Result: 遍历验证在所有设定下持续占优，OT方法落后较多；延迟树扩展能保持目标分布并改进根节点i.i.d.回退性能；神经选择器使OT类方法首次超越遍历验证，在广泛的模型、数据集和采样设定下实现5%的平均吞吐量提升。

Conclusion: 该研究成功揭示了OT验证方法在推测解码中的根本局限，并通过延迟树扩展和神经选择器的组合方案，有效解决了收益错配问题，显著提升了推测解码效率，为未来优化方向提供了重要指导。

Abstract: Multi-path speculative decoding accelerates lossless sampling from a target model by using a cheaper draft model to generate a draft tree of tokens, and then applies a verification algorithm that accepts a subset of these. While prior work has proposed various verification algorithms for i.i.d rollouts, their relative performance under matched settings remains unclear. In this work, we firstly present a systematic evaluation of verification strategies across model families, tasks, and sampling regimes, and find that Traversal Verification dominates consistently, with OT-based methods lagging far behind. Our analysis uncovers that this occurs because OT-based methods achieve high multi-token acceptance near the root of the draft tree, while multi-token gains are most impactful deeper in the draft tree, where draft and target distributions diverge. Based on this insight, we propose delayed tree expansion, which drafts a partial single path, delaying the i.i.d. branching point. We show that delayed tree expansion preserves the target distribution and improves on root-node i.i.d rollouts. Further, we develop a dynamic neural selector that estimates the expected block efficiency of optimal-transport-based verification methods from draft and target features, enabling context-dependent expansion decisions. Our neural selector allows OT-based methods like SpecInfer to outperform Traversal Verification for the first time, achieving 5% higher average throughput across a wide range of models, datasets, and sampling settings.

</details>


### [73] [Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17009)
*Nikunj Gupta,James Zachary Hare,Jesse Milzman,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 本文提出动作图策略（AGP）以解决多智能体强化学习中的协调问题。AGP通过建模智能体动作依赖关系并构建协调上下文，使智能体能基于全局依赖进行决策。理论证明AGP比独立策略更具表达力，能实现优于贪心执行的协调动作。实验表明在部分可观测协调任务中AGP成功率可达80-95%，远超其他方法。


<details>
  <summary>Details</summary>
Motivation: 协调动作是多智能体强化学习中最基础的合作形式。去中心化决策的成功不仅依赖个体动作质量，更需跨智能体的动作兼容性以实现同步、避免冲突并满足全局约束。现有方法在建模动作依赖关系和全局约束方面存在局限性。

Method: 提出动作图策略（AGP），通过动作图建模智能体可用动作间的依赖关系。构建协调上下文机制，使智能体的决策能够基于全局动作依赖关系进行条件化，从而实现更有效的协调。

Result: 理论方面：AGP比完全独立策略具有严格更强的联合策略表达力，能够实现被证明更优的协调联合动作，甚至超越集中式价值分解方法的贪心执行效果。实验方面：在部分可观测且含抗协调惩罚的典型任务中，AGP成功率达80-95%，而其他MARL方法仅10-25%。在多样化多智能体环境中，AGP始终显著优于基线。

Conclusion: AGP通过显式建模动作依赖关系和构建协调上下文，为多智能体强化学习中的协调决策提供了新范式。该框架在理论上具有更强的表达能力，在实验中展现出显著的优越性，有效解决了去中心化决策中的协调挑战。

Abstract: Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents' available action choices. It constructs, what we call, \textit{coordination contexts}, that enable agents to condition their decisions on global action dependencies. Theoretically, we show that AGPs induce a strictly more expressive joint policy compared to fully independent policies and can realize coordinated joint actions that are provably more optimal than greedy execution even from centralized value-decomposition methods. Empirically, we show that AGP achieves 80-95\% success on canonical coordination tasks with partial observability and anti-coordination penalties, where other MARL methods reach only 10-25\%. We further demonstrate that AGP consistently outperforms these baselines in diverse multi-agent environments.

</details>


### [74] [Malliavin Calculus as Stochastic Backpropogation](https://arxiv.org/abs/2602.17013)
*Kevin D. Oden*

Main category: cs.LG

TL;DR: 本文通过Malliavin分部积分恒等式建立了路径梯度（重参数化）和得分函数梯度（Malliavin）估计器之间的严格联系，并基于此提出了一种统方差感知的混合估计器，通过经验协方差自适应地结合两种梯度。该方法在VAE和强耦合合成问题上分别实现了9%和35%的方差减少，为随机梯度估计提供了概念统一框架，同时揭示了混合方法在非平稳优化场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 随机梯度估计在变分推断、强化学习等领域至关重要，但路径梯度和得分函数梯度两种主流方法长期被视为独立的技术。现有研究缺乏对二者内在联系的理论理解，也未能有效利用它们的互补性来降低方差。本文旨在建立统一的理论框架，并开发能够自适应结合两种估计器的实用方法。

Method: 首先利用Malliavin积分分部恒等式证明两种梯度估计器本质上是该恒等式的不同表现形式。在此基础上，构建基于经验协方差结构的线性组合混合估计器，通过最小化方差推导出最优权重分配。理论分析提供了闭式有限样本收敛界。实验在VAE（CIFAR-10）、强耦合合成问题以及策略梯度设置中进行验证。

Result: 理论结果表明该混合估计器在所有无偏线性组合中方差最小。实证方面：在CIFAR-10 VAE训练中获得9%的方差降低；在强耦合合成问题上达到高达35%的方差减少。然而，探索性策略梯度实验发现非平稳优化场景对混合方法构成挑战，揭示了其固有局限性。

Conclusion: 本研究将Malliavin微积分定位为随机梯度估计的概念统一框架，澄清了混合方法何时能提供实际收益以及面临何种限制。未来工作需解决非平稳优化 landscape 下的适应性问题，并进一步探索该框架在其他随机优化场景中的应用潜力。

Abstract: We establish a rigorous connection between pathwise (reparameterization) and score-function (Malliavin) gradient estimators by showing that both arise from the Malliavin integration-by-parts identity. Building on this equivalence, we introduce a unified and variance-aware hybrid estimator that adaptively combines pathwise and Malliavin gradients using their empirical covariance structure. The resulting formulation provides a principled understanding of stochastic backpropagation and achieves minimum variance among all unbiased linear combinations, with closed-form finite-sample convergence bounds. We demonstrate 9% variance reduction on VAEs (CIFAR-10) and up to 35% on strongly-coupled synthetic problems. Exploratory policy gradient experiments reveal that non-stationary optimization landscapes present challenges for the hybrid approach, highlighting important directions for future work. Overall, this work positions Malliavin calculus as a conceptually unifying and practically interpretable framework for stochastic gradient estimation, clarifying when hybrid approaches provide tangible benefits and when they face inherent limitations.

</details>


### [75] [WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning](https://arxiv.org/abs/2602.17025)
*Gagan Mundada,Zihan Huang,Rohan Surana,Sheldon Yu,Jennifer Yuntong Zhang,Xintong Li,Tong Yu,Lina Yao,Jingbo Shang,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: 针对GRPO在复杂推理训练中的过度思考与效率权衡难题，本文提出弱监督GRPO（WS-GRPO），通过从结果正确的监督信号中训练偏好模型，生成前缀级继续/停止指导，在保持准确性的同时显著降低推理长度。


<details>
  <summary>Details</summary>
Motivation: GRPO在复杂推理任务中表现优异，但其基于相对目标函数的机制会激励模型通过延长推理过程获取相对优势，导致低效的过度思考。现有方法依赖难以校准的全局长度惩罚，且缺乏超越最终答案的直接继续/停止监督，使正确性与推理效率的权衡难以控制。

Method: WS-GRPO创新性地将终端奖励转化为部分轨迹的正确性感知指导。具体地，训练一个仅基于结果正确性标注的偏好模型，为每个前缀生成信号，判断继续生成是否可能提升最终答案正确性，从而提供弱监督的继续/停止决策机制。

Result: 理论分析与在多个推理基准测试上的实证表明，WS-GRPO能够显著缩短推理轨迹长度，同时在性能上保持与GRPO基线相当的水平。

Conclusion: WS-GRPO通过弱监督前缀级指导有效解决了GRPO的过度思考问题，在不牺牲准确性的前提下显著提升了推理效率，为控制语言模型推理长度提供了新思路。

Abstract: Group Relative Policy Optimization (GRPO) is effective for training language models on complex reasoning. However, since the objective is defined relative to a group of sampled trajectories, extended deliberation can create more chances to realize relative gains, leading to inefficient reasoning and overthinking, and complicating the trade-off between correctness and rollout efficiency. Controlling this behavior is difficult in practice, considering (i) Length penalties are hard to calibrate because longer rollouts may reflect harder problems that require longer reasoning, penalizing tokens risks truncating useful reasoning along with redundant continuation; and (ii) supervision that directly indicates when to continue or stop is typically unavailable beyond final answer correctness. We propose Weakly Supervised GRPO (WS-GRPO), which improves rollout efficiency by converting terminal rewards into correctness-aware guidance over partial trajectories. Unlike global length penalties that are hard to calibrate, WS-GRPO trains a preference model from outcome-only correctness to produce prefix-level signals that indicate when additional continuation is beneficial. Thus, WS-GRPO supplies outcome-derived continue/stop guidance, reducing redundant deliberation while maintaining accuracy. We provide theoretical results and empirically show on reasoning benchmarks that WS-GRPO substantially reduces rollout length while remaining competitive with GRPO baselines.

</details>


### [76] [Transforming Behavioral Neuroscience Discovery with In-Context Learning and AI-Enhanced Tensor Methods](https://arxiv.org/abs/2602.17027)
*Paimon Goulart,Jordan Steinhauser,Dawon Ahn,Kylene Shuler,Edward Korzus,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本研究提出一种AI增强的科研流水线，通过上下文学习(ICL)为神经科学专家提供无代码AI接口，实现自动化数据处理与模式发现，并在小鼠恐惧泛化研究中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统科研流水线存在过程复杂、僵化和耗时的问题，需要领域专家大量调试或手动标注。现有AI方法要求专业知识，亟需无需训练即可使用的无缝接口，让专家专注于结果解读而非流程管理。

Method: 1) 采用上下文学习(ICL)范式构建零训练需求的用户接口；2) 提出张量分解模型的AI增强新方法，处理异构数据中的模式发现；3) 与行为神经科学家协作，在恐惧泛化研究场景中部署应用。

Result: 实验评估表明，该流水线相比领域标准实践及非ICL的机器学习基线均表现出显著优越性，且发现的模式经团队内领域专家验证具有科学意义。

Conclusion: 基于ICL的AI增强流水线能有效降低AI使用门槛，赋能领域专家加速科学发现，在神经科学等复杂数据分析领域具备实用价值与推广潜力。

Abstract: Scientific discovery pipelines typically involve complex, rigid, and time-consuming processes, from data preparation to analyzing and interpreting findings. Recent advances in AI have the potential to transform such pipelines in a way that domain experts can focus on interpreting and understanding findings, rather than debugging rigid pipelines or manually annotating data. As part of an active collaboration between data science/AI researchers and behavioral neuroscientists, we showcase an example AI-enhanced pipeline, specifically designed to transform and accelerate the way that the domain experts in the team are able to gain insights out of experimental data. The application at hand is in the domain of behavioral neuroscience, studying fear generalization in mice, an important problem whose progress can advance our understanding of clinically significant and often debilitating conditions such as PTSD (Post-Traumatic Stress Disorder). We identify the emerging paradigm of "In-Context Learning" (ICL) as a suitable interface for domain experts to automate parts of their pipeline without the need for or familiarity with AI model training and fine-tuning, and showcase its remarkable efficacy in data preparation and pattern interpretation. Also, we introduce novel AI-enhancements to tensor decomposition model, which allows for more seamless pattern discovery from the heterogeneous data in our application. We thoroughly evaluate our proposed pipeline experimentally, showcasing its superior performance compared to what is standard practice in the domain, as well as against reasonable ML baselines that do not fall under the ICL paradigm, to ensure that we are not compromising performance in our quest for a seamless and easy-to-use interface for domain experts. Finally, we demonstrate effective discovery, with results validated by the domain experts in the team.

</details>


### [77] [Multi-Probe Zero Collision Hash (MPZCH): Mitigating Embedding Collisions and Enhancing Model Freshness in Large-Scale Recommenders](https://arxiv.org/abs/2602.17050)
*Ziliang Zhao,Bi Xue,Emma Lin,Mengjiao Zhou,Kaustubh Vartak,Shakhzod Ali-Zade,Carson Lu,Tao Li,Bin Kuang,Rui Jian,Bin Wen,Dennis van der Staay,Yixin Bao,Eddy Li,Chao Deng,Songbin Liu,Qifan Wang,Kai Ren*

Main category: cs.LG

TL;DR: 针对大规模推荐系统中嵌入表因哈希冲突导致的性能下降问题，本文提出 Multi-Probe Zero Collision Hash (MPZCH)，一种基于线性探测的新型索引机制。MPZCH 利用辅助张量和 CUDA 内核实现可配置的多路探测与主动淘汰策略，在合理表大小下几乎消除冲突，避免旧 ID 的嵌入继承，提升新特征的学习效果，同时保持与现有方法相当的训练 QPS 和推理延迟。在线实验表明，MPZCH 实现用户嵌入零冲突，显著提升物品嵌入的新鲜度和质量，已在开源库 TorchRec 中发布。


<details>
  <summary>Details</summary>
Motivation: 在大规模推荐系统中，嵌入表用于将高基数的类别特征映射为密集向量。随着唯一 ID 数量的膨胀，传统哈希索引容易出现冲突，导致模型性能下降、个性化质量受损。此外，哈希冲突会引起旧 ID 嵌入的继承，使得新 ID 难以从头学习，影响系统的新鲜度与泛化能力。

Method: MPZCH 采用线性探测的多路探测策略，通过辅助张量记录槽位状态，配合高性能 CUDA 内核实现并行查找与插入。系统支持可配置的探测深度与主动淘汰策略，定期清理过期 ID 并重置对应槽位，从而避免嵌入的“陈旧继承”。该方法在保持哈希表 O(1) 访问特性的同时，显著降低冲突概率。

Result: 在合理的表大小（如装载因子 0.7）下，MPZCH 能够实现对用户嵌入的零冲突，并对物品嵌入大幅降低冲突率。在线 A/B 实验显示，相比传统哈希方法，MPZCH 在点击率（CTR）和转化率（CVR）上均有显著提升，同时训练 QPS 与推理延迟与基线相当。

Conclusion: MPZCH 提供了一种高效、可扩展的嵌入表索引方案，能够在生产环境中消除哈希冲突、提升嵌入新鲜度与模型效果。该方案已集成至开源 TorchRec 库，为社区提供了实用的工具。

Abstract: Embedding tables are critical components of large-scale recommendation systems, facilitating the efficient mapping of high-cardinality categorical features into dense vector representations. However, as the volume of unique IDs expands, traditional hash-based indexing methods suffer from collisions that degrade model performance and personalization quality. We present Multi-Probe Zero Collision Hash (MPZCH), a novel indexing mechanism based on linear probing that effectively mitigates embedding collisions. With reasonable table sizing, it often eliminates these collisions entirely while maintaining production-scale efficiency. MPZCH utilizes auxiliary tensors and high-performance CUDA kernels to implement configurable probing and active eviction policies. By retiring obsolete IDs and resetting reassigned slots, MPZCH prevents the stale embedding inheritance typical of hash-based methods, ensuring new features learn effectively from scratch. Despite its collision-mitigation overhead, the system maintains training QPS and inference latency comparable to existing methods. Rigorous online experiments demonstrate that MPZCH achieves zero collisions for user embeddings and significantly improves item embedding freshness and quality. The solution has been released within the open-source TorchRec library for the broader community.

</details>


### [78] [Sign Lock-In: Randomly Initialized Weight Signs Persist and Bottleneck Sub-Bit Model Compression](https://arxiv.org/abs/2602.17063)
*Akira Sakai,Yuma Ichikawa*

Main category: cs.LG

TL;DR: 针对亚比特模型压缩中符号位的固定成本瓶颈，该研究发现符号矩阵的表观随机性主要继承自初始化，符号翻转主要通过零附近的罕见边界交叉发生。提出符号锁定理论进行停时分析，并设计间隔感知初始化与向外漂移正则化器，将有效翻转率降至约10^-3，困惑度仅增加约1个点。


<details>
  <summary>Details</summary>
Motivation: 在亚比特模型压缩中，当权重被极度压缩至低于每权值1比特时，符号位成为无法进一步压缩的固定成本瓶颈。尽管学习的符号矩阵在谱特性上表现出与随机Rademacher矩阵不可区分的表观随机性，但现有压缩方法未能有效利用其潜在的结构特性，限制了压缩效率的提升。

Method: 1) 建立符号锁定理论，采用停时分析框架研究SGD噪声下的符号翻转行为，证明在有限更新和零附近罕见重入条件下，有效符号翻转次数服从几何尾分布；2) 基于理论机制设计间隔感知的初始化策略，使权重初始分布远离零边界；3) 引入轻量级向外漂移正则化器，在训练过程中持续推动权重远离零点以减少翻转概率。

Result: 所提方法在Transformer、CNN和MLP等多种架构上实现约10^{-3}的有效符号翻转率，同时在语言模型评估中仅造成约1个点的困惑度增长，显著提升了亚比特压缩的实用性与性能平衡。

Conclusion: 符号模式的随机性主要源于初始化而非训练动态，通过理论指导的初始化设计与正则化可有效抑制符号翻转，为亚比特模型压缩提供了新思路，实现了存储成本与模型性能的有效权衡。

Abstract: Sub-bit model compression seeks storage below one bit per weight; as magnitudes are aggressively compressed, the sign bit becomes a fixed-cost bottleneck. Across Transformers, CNNs, and MLPs, learned sign matrices resist low-rank approximation and are spectrally indistinguishable from an i.i.d. Rademacher baseline. Despite this apparent randomness, most weights retain their initialization signs; flips primarily occur via rare near-zero boundary crossings, suggesting that sign-pattern randomness is largely inherited from initialization. We formalize this behavior with sign lock-in theory, a stopping-time analysis of sign flips under SGD noise. Under bounded updates and a rare re-entry condition into a small neighborhood around zero, the number of effective sign flips exhibits a geometric tail. Building on this mechanism, we introduce a gap-based initialization and a lightweight outward-drift regularizer, reducing the effective flip rate to approximately $10^{-3}$ with only about a one-point increase in perplexity.

</details>


### [79] [Multi-Objective Alignment of Language Models for Personalized Psychotherapy](https://arxiv.org/abs/2602.16053)
*Mehrab Beikzadeh,Yasaman Asadollah Salmanpour,Ashima Suvarna,Sriram Sankararaman,Matteo Malgaroli,Majid Sarrafzadeh,Saadia Gabriel*

Main category: cs.LG

TL;DR: 该研究针对心理健康服务资源短缺问题，提出一种多目标对齐框架（MODPO），通过直接偏好优化平衡患者偏好与临床安全。相比单目标优化，MODPO在共情（77.6%）与安全（62.6%）间取得更优平衡，且治疗标准优于通用沟通原则17.2%。


<details>
  <summary>Details</summary>
Motivation: 全球超10亿人受心理健康问题困扰，但服务可及性受人力短缺和高成本限制。现有AI治疗系统对齐方法独立优化目标，无法平衡患者偏好与临床安全。

Method: 1) 调研335名有心理健康经历者，收集治疗维度的偏好排序；2) 开发多目标对齐框架，基于直接偏好优化（DPO）；3) 训练共情、安全、积极倾听、自我激励改变、信任/默契、患者自主性六项标准的奖励模型；4) 对比多目标与单目标优化、监督微调及参数合并的效果。

Result: 多目标DPO（MODPO）实现更优平衡（共情77.6%，安全62.6%），单目标优化则偏重共情（93.6%）但安全较低（47.8%）。治疗标准表现优于通用沟通原则17.2%，且经盲审临床评估确认MODPO一致性更优。

Conclusion: 多目标对齐框架能有效平衡治疗多维标准，提升AI系统在心理健康支持中的适用性与安全性。LLM评估者与临床医生的判断一致性相当，支持该框架的临床潜力。

Abstract: Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.

</details>


### [80] [Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum](https://arxiv.org/abs/2602.17080)
*Minxin Zhang,Yuxuan Liu,Hayden Scheaffer*

Main category: cs.LG

TL;DR: 提出两种新优化器NAMO和NAMO-D，首次将正交动量与基于范数的Adam型噪声自适应机制原则性结合，在GPT-2预训练中性能优于AdamW和Muon，其中NAMO-D通过截断超参数实现进一步增益。


<details>
  <summary>Details</summary>
Motivation: 现有高效随机优化器（如Adam和Muon）在确定性更新方向与随机扰动自适应机制的结合上存在局限。Muon虽利用权重矩阵结构通过正交动量提升大语言模型训练性能，但缺乏原则性的噪声自适应能力。本研究旨在首次将正交动量与基于范数的噪声自适应机制有机结合，在保持正交性优势的同时增强对随机梯度噪声的适应性。

Method: NAMO采用单一自适应步长对正交动量进行缩放，在保持正交性的同时以极低额外成本改进Muon。NAMO-D则通过右乘带截断项的对角矩阵扩展NAMO，实现神经元级噪声自适应，契合Hessian矩阵的近似块对角结构。两者均实现了正交动量与范数型噪声自适应的首次原则性融合。

Result: 理论层面，在标准假设下，两种算法在确定性设定中均达到最优收敛率；在随机设定中，其收敛性保证可自适应随机梯度的噪声水平。实验层面，在GPT-2模型预训练中，NAMO和NAMO-D性能均优于AdamW和Muon基线，且NAMO-D通过额外截断超参数进一步超越NAMO。

Conclusion: NAMO和NAMO-D首次实现了正交动量与范数型噪声自适应的原则性集成，兼具理论优势与实证效果。NAMO-D通过截断超参数在保持良好条件更新方向与利用细粒度噪声自适应之间取得更优平衡，验证了神经元级自适应机制的有效性。这两种算法为大语言模型训练提供了新的优化器选择。

Abstract: Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining a well-conditioned update direction and leveraging fine-grained noise adaptation.

</details>


### [81] [MeGU: Machine-Guided Unlearning with Target Feature Disentanglement](https://arxiv.org/abs/2602.17088)
*Haoyu Wang,Zhuo Huang,Xiaolong Wang,Bo Han,Zhiwei Lin,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文针对机器学习中"被遗忘权"需求，提出了一种名为MeGU的机器引导遗忘框架。该框架通过利用多模态大语言模型(MLLM)分析预训练模型中语义类别在特征层面的纠缠特性，采用概念感知的重对齐策略和正负特征噪声对，实现对目标数据的选择性遗忘，有效解决了现有方法中遗忘不足与过度遗忘的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私问题日益突出，"被遗忘权"成为关键需求，机器遗忘技术受到广泛关注。然而现有方法存在根本性权衡：激进删除会损害保留数据上的模型效用，保守策略则残留目标信息。通过分析发现，预训练过程中语义类别在特征模式层面存在纠缠现象——共享相关特征的同时保留类别判别性成分，这一内在特性限制了传统遗忘范式的有效性。因此，亟需一种能够实现选择性遗忘、同时避免欠遗忘和过遗忘的新方法。

Method: 提出Machine-Guided Unlearning (MeGU)框架，核心方法包括：1)利用多模态大语言模型(MLLM)显式确定目标样本的重对齐方向，并赋予语义意义明确的扰动标签；2)将MLLM估计的类间概念相似性编码为轻量级转移矩阵提升效率；3)引入正负特征噪声对机制——负向噪声抑制目标特异性特征模式，正向噪声强化保留的关联特征并将其与扰动概念对齐；4)通过概念感知的重对齐协调设计，实现目标表征的选择性破坏，同时保持共享语义结构。

Result: MeGU框架实现了可控且选择性的遗忘效果，有效缓解了欠遗忘（目标信息残留）和过遗忘（模型效用下降）问题。通过在微调过程中协同优化正负噪声对，该方法能够在破坏目标类别特异性特征的同时，保留并重新对齐共享的语义关联特征，从而在遗忘强度与模型性能之间取得更好平衡。

Conclusion: 本研究揭示了预训练模型中语义类别特征纠缠的根本限制，并在此基础上提出了MLLM引导的概念感知重对齐遗忘范式。MeGU通过智能扰动标签生成和正负特征噪声协同机制，为机器遗忘提供了一种高效且精准的解决方案，为数据隐私保护下的模型维护开辟了新方向。

Abstract: The growing concern over training data privacy has elevated the "Right to be Forgotten" into a critical requirement, thereby raising the demand for effective Machine Unlearning. However, existing unlearning approaches commonly suffer from a fundamental trade-off: aggressively erasing the influence of target data often degrades model utility on retained data, while conservative strategies leave residual target information intact. In this work, the intrinsic representation properties learned during model pretraining are analyzed. It is demonstrated that semantic class concepts are entangled at the feature-pattern level, sharing associated features while preserving concept-specific discriminative components. This entanglement fundamentally limits the effectiveness of existing unlearning paradigms. Motivated by this insight, we propose Machine-Guided Unlearning (MeGU), a novel framework that guides unlearning through concept-aware re-alignment. Specifically, Multi-modal Large Language Models (MLLMs) are leveraged to explicitly determine re-alignment directions for target samples by assigning semantically meaningful perturbing labels. To improve efficiency, inter-class conceptual similarities estimated by the MLLM are encoded into a lightweight transition matrix. Furthermore, MeGU introduces a positive-negative feature noise pair to explicitly disentangle target concept influence. During finetuning, the negative noise suppresses target-specific feature patterns, while the positive noise reinforces remaining associated features and aligns them with perturbing concepts. This coordinated design enables selective disruption of target-specific representations while preserving shared semantic structures. As a result, MeGU enables controlled and selective forgetting, effectively mitigating both under-unlearning and over-unlearning.

</details>


### [82] [Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling](https://arxiv.org/abs/2602.17089)
*Xinghao Dong,Huchen Yang,Jin-long Wu*

Main category: cs.LG

TL;DR: 扩散模型采样速度慢是主要瓶颈，本文通过低维潜空间流匹配实现单步采样，速度提升两个数量级，配合保度量与几何感知正则化保持拓扑信息，显著减少训练数据需求，为随机闭合模型提供高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽能生成高质量且多样化的样本，但其迭代采样机制导致速度缓慢，成为制约随机闭合模型实际应用的关键缺点，亟需提升采样效率同时确保物理保真度。

Method: 在二维Kolmogorov流数值案例上系统比较传输类生成模型，采用低维潜空间流匹配技术实现单步采样；通过联合训练隐式正则化与显式保度量(MP)、几何感知(GA)约束，控制潜空间畸变并维持物理真实性。

Result: 单步采样速度较迭代扩散方法提升达两个数量级；显式和隐式正则化使潜空间继承原系统低维流形的关键拓扑结构，可在训练数据量较少的情况下有效学习随机闭合模型。

Conclusion: 低维潜空间流匹配结合正则化策略，兼具快速采样与物理保真双重优势，为随机闭合模型提供了高效且数据节约的学习路径。

Abstract: Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints. Besides offering a faster sampling speed, both explicitly and implicitly regularized latent spaces inherit the key topological information from the lower-dimensional manifold of the original complex dynamical system, which enables the learning of stochastic closure models without demanding a huge amount of training data.

</details>


### [83] [FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment](https://arxiv.org/abs/2602.17095)
*Chuiyang Meng,Ming Tang,Vincent W. S. Wong*

Main category: cs.LG

TL;DR: FLoRG提出了一种联邦微调框架，通过采用单一低秩矩阵并聚合其格拉姆矩阵，配合普氏对齐策略，解决了LoRA在联邦学习中的聚合误差和分解漂移问题，在提升下游任务准确率的同时将通信开销降低高达2041倍。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA联邦微调存在两个主要挑战：一是分别聚合两个低秩矩阵引入的误差，二是即使聚合矩阵乘积也需要非唯一的矩阵分解，导致分解漂移问题。

Method: 提出FLoRG框架，采用单一低秩矩阵进行微调并聚合其格拉姆矩阵以消除聚合误差；引入普氏对齐方法在连续微调轮次间对齐分解后的矩阵，以最小化分解漂移。

Result: 理论分析证明了FLoRG的收敛性，且普氏对齐能得到更紧的收敛界；在多个LLM微调基准测试中，FLoRG在下游任务准确率上优于五种最先进基线方案，通信开销最多可降低2041倍。

Conclusion: FLoRG通过创新性地使用单一低秩矩阵和格拉姆聚合，结合普氏对齐策略，有效解决了联邦LoRA微调的关键挑战，实现了更高的准确性和通信效率。

Abstract: Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\times$.

</details>


### [84] [Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction](https://arxiv.org/abs/2602.17102)
*Sai Vineeth Kandappareddigari,Santhoshkumar Jagadish,Gauri Verma,Ilhuicamina Contreras,Christopher Dignam,Anmol Srivastava,Benjamin Demers*

Main category: cs.LG

TL;DR: 提出一种无服务器MLOps框架，通过事件驱动管道和托管服务实现ML全生命周期管理，在HS编码预测场景中，Text-CNN模型达到98%准确率，兼顾成本效益、可重复性与自动化A/B测试


<details>
  <summary>Details</summary>
Motivation: 针对HS编码分类这一合规关键任务，面临产品描述简短模糊、更新频繁等挑战，分类错误会导致货物延误和财务损失，亟需可审计、符合SLA且经济高效的工业化ML解决方案

Method: 采用模型无关的无服务器架构，通过标准化接口支持多样化推理模式；使用自定义文本嵌入编码器与多种深度学习架构（重点采用Text-CNN），实现事件驱动自动化流水线与托管服务，内置自动化A/B测试机制

Result: Text-CNN在真实数据上实现98%准确率；架构通过自动扩展保障可重复性、可审计性和SLA；相比Transformer模型，在保持相近准确率的同时显著降低长期运营成本，实现确定性与可解释的分类

Conclusion: 为基于无服务器架构的ML工业化提供了可复制蓝图，使企业能够在扩展ML应用时优化性能与经济性，平衡准确率、延迟、可解释性和成本效益

Abstract: This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.

</details>


### [85] [Online Learning with Improving Agents: Multiclass, Budgeted Agents and Bandit Learners](https://arxiv.org/abs/2602.17103)
*Sajad Ashkezari,Shai Ben-David*

Main category: cs.LG

TL;DR: 本文系统扩展了"带改进的学习"模型的理论研究，通过引入组合维度刻画其在线学习能力，并分析多类别分类、赌博机反馈及智能体改进成本等多种设定下的学习边界。


<details>
  <summary>Details</summary>
Motivation: 针对智能体可通过微调特征值获取更优标签的新兴学习场景，现有理论研究尚不完善。本研究旨在为该模型构建系统性的在线学习理论框架，以支撑对策略性改进行为的深入分析。

Method: 运用计算学习论中的组合维度（如VC维、Littlestone维等）作为分析工具，针对带改进的学习模型进行适应性改造，并分别在多类别、部分反馈（赌博机）及成本约束等设定下考察其在线学习复杂性。

Result: 给出了该模型下在线学习能力的组合维度特征化结果；明确了多类别设定的可学习性条件；分析了赌博机反馈下的学习边界；建立了智能体改进成本的量化建模；显著拓展了已有理论结果。

Conclusion: 本研究为带改进的学习模型奠定了更系统的理论基础，揭示了不同场景下的学习可行性边界，为理解智能体策略性改进与学习算法设计提供了重要理论支撑。

Abstract: We investigate the recently introduced model of learning with improvements, where agents are allowed to make small changes to their feature values to be warranted a more desirable label. We extensively extend previously published results by providing combinatorial dimensions that characterize online learnability in this model, by analyzing the multiclass setup, learnability in a bandit feedback setup, modeling agents' cost for making improvements and more.

</details>


### [86] [i-PhysGaussian: Implicit Physical Simulation for 3D Gaussian Splatting](https://arxiv.org/abs/2602.17117)
*Yicheng Cao,Zhuo Huang,Yu Yao,Yiming Ying,Daoyi Dong,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文提出 i-PhysGaussian，一种融合 3D 高斯溅射与隐式物质点法的物理仿真框架。通过隐式牛顿优化和 GMRES 求解器最小化动量平衡残差，替代显式逐步积分，显著降低时间步长敏感性。实验表明该方法可在 20 倍更大时间步长下保持稳定，有效应对高刚度材料与复杂动态场景。


<details>
  <summary>Details</summary>
Motivation: 当前基于 3D 重建的物理模拟器依赖显式时间积分，对步长高度敏感，在高刚度材料或准静态运动等复杂场景中精度迅速退化，无法满足工业与工程领域高精度风险管理的需求。

Method: 设计 i-PhysGaussian 框架，将 3D 高斯溅射表示与隐式物质点法积分器相结合。在每个时间步，通过隐式牛顿型优化配合 GMRES 线性求解器，求解动量平衡方程的残差最小化问题，直接获得物理一致的步末状态。

Result: 相比显式基线方法，i-PhysGaussian 将稳定时间步长提升达 20 倍，在复杂动态过渡过程中仍能保持结构完整性与运动平滑性，显著降低计算敏感度并确保物理一致性。

Conclusion: 该隐式求解框架突破了显式方法的时间步长限制，为高刚度、复杂动态场景的物理仿真提供了更稳定可靠的解决方案，在工业与工程仿真领域具有重要应用前景。

Abstract: Physical simulation predicts future states of objects based on material properties and external loads, enabling blueprints for both Industry and Engineering to conduct risk management. Current 3D reconstruction-based simulators typically rely on explicit, step-wise updates, which are sensitive to step time and suffer from rapid accuracy degradation under complicated scenarios, such as high-stiffness materials or quasi-static movement. To address this, we introduce i-PhysGaussian, a framework that couples 3D Gaussian Splatting (3DGS) with an implicit Material Point Method (MPM) integrator. Unlike explicit methods, our solution obtains an end-of-step state by minimizing a momentum-balance residual through implicit Newton-type optimization with a GMRES solver. This formulation significantly reduces time-step sensitivity and ensures physical consistency. Our results demonstrate that i-PhysGaussian maintains stability at up to 20x larger time steps than explicit baselines, preserving structural coherence and smooth motion even in complex dynamic transitions.

</details>


### [87] [TIFO: Time-Invariant Frequency Operator for Stationarity-Aware Representation Learning in Time Series](https://arxiv.org/abs/2602.17122)
*Xihao Piao,Zheng Chen,Lingwei Zhu,Yushun Dong,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 提出频域时间不变算子(TIFO)，通过学习全局频域权重抑制非平稳分量，解决时间序列分布偏移问题，在28项预测任务中获24项领先，ETTm2上MSE提升超33%且计算成本降60-70%


<details>
  <summary>Details</summary>
Motivation: 非平稳时间序列预测的核心挑战是分布偏移，即训练/测试数据分布差异。现有方法仅从单样本去除低阶矩，未能建模跨样本的时变结构，导致性能受限。

Method: 设计TIFO模块，在频域空间学习平稳性感知权重。核心洞察：傅里叶变换隐含实现频域特征分解。该权重突出平稳频率成分、抑制非平稳成分，可即插即用集成到各类预测模型。

Result: 在28个预测设置中获18个第1名和6个第2名；ETTm2数据集平均MSE相对提升33.3%和55.3%；计算成本较基线降低60-70%；兼容多种模型架构。

Conclusion: 频域方法为分布偏移问题提供新视角，TIFO显著提升预测准确性与计算效率，具备强可扩展性，为非平稳时间序列预测提供有效解决方案。

Abstract: Nonstationary time series forecasting suffers from the distribution shift issue due to the different distributions that produce the training and test data. Existing methods attempt to alleviate the dependence by, e.g., removing low-order moments from each individual sample. These solutions fail to capture the underlying time-evolving structure across samples and do not model the complex time structure. In this paper, we aim to address the distribution shift in the frequency space by considering all possible time structures. To this end, we propose a Time-Invariant Frequency Operator (TIFO), which learns stationarity-aware weights over the frequency spectrum across the entire dataset. The weight representation highlights stationary frequency components while suppressing non-stationary ones, thereby mitigating the distribution shift issue in time series. To justify our method, we show that the Fourier transform of time series data implicitly induces eigen-decomposition in the frequency space. TIFO is a plug-and-play approach that can be seamlessly integrated into various forecasting models. Experiments demonstrate our method achieves 18 top-1 and 6 top-2 results out of 28 forecasting settings. Notably, it yields 33.3% and 55.3% improvements in average MSE on the ETTm2 dataset. In addition, TIFO reduces computational costs by 60% -70% compared to baseline methods, demonstrating strong scalability across diverse forecasting models.

</details>


### [88] [VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation](https://arxiv.org/abs/2602.17133)
*Linwei Zhai,Han Ding,Mingzhi Lin,Cui Zhao,Fei Wang,Ge Wang,Wang Zhi,Wei Xi*

Main category: cs.LG

TL;DR: 针对VQ-VAE训练不稳定和codebook collapse问题，提出VP-VAE范式，通过Metropolis-Hastings采样生成分布一致的结构化潜扰动替代codebook，实现表示学习与离散化解耦；并推导出轻量版FSP，为FSQ量化器提供统一理论解释，实验证实其提升重建质量、平衡token使用且避免训练不稳定。


<details>
  <summary>Details</summary>
Motivation: VQ-VAEs作为现代生成建模的基础组件，其表示学习与离散codebook优化的内在耦合导致训练不稳定和codebook collapse问题，亟需一种解耦方案来提升模型稳定性和性能。

Method: 提出VP-VAE：将量化操作重新解释为潜空间的结构化扰动，用Metropolis-Hastings采样生成分布一致、尺度自适应的潜扰动替代不可微量化的codebook；在潜变量近似均匀假设下，进一步推导FSP（Finite Scalar Perturbation）轻量变体，统一解释FSQ风格固定量化器。

Result: 在图像和音频基准测试上，VP-VAE和FSP显著提升重建保真度，实现更均衡的token使用分布，同时完全避免了耦合codebook训练带来的不稳定性问题。

Conclusion: VP-VAE通过扰动视角解耦表示学习与离散化，为VQ-VAE提供了稳定训练新范式；FSP变体兼具理论统一性和实践高效性，验证了所提方法的普适性和优越性。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental to modern generative modeling, yet they often suffer from training instability and "codebook collapse" due to the inherent coupling of representation learning and discrete codebook optimization. In this paper, we propose VP-VAE (Vector Perturbation VAE), a novel paradigm that decouples representation learning from discretization by eliminating the need for an explicit codebook during training. Our key insight is that, from the neural network's viewpoint, performing quantization primarily manifests as injecting a structured perturbation in latent space. Accordingly, VP-VAE replaces the non-differentiable quantizer with distribution-consistent and scale-adaptive latent perturbations generated via Metropolis--Hastings sampling. This design enables stable training without a codebook while making the model robust to inference-time quantization error. Moreover, under the assumption of approximately uniform latent variables, we derive FSP (Finite Scalar Perturbation), a lightweight variant of VP-VAE that provides a unified theoretical explanation and a practical improvement for FSQ-style fixed quantizers. Extensive experiments on image and audio benchmarks demonstrate that VP-VAE and FSP improve reconstruction fidelity and achieve substantially more balanced token usage, while avoiding the instability inherent to coupled codebook training.

</details>


### [89] [TimeOmni-VL: Unified Models for Time Series Understanding and Generation](https://arxiv.org/abs/2602.17149)
*Tong Guan,Sheng Pan,Johan Barthelemy,Zhao Li,Yujun Cai,Cesare Alippi,Ming Jin,Shirui Pan*

Main category: cs.LG

TL;DR: 本文提出首个视觉为中心的多模态时间序列统一框架TimeOmni-VL，通过fidelity-preserving双向映射和理解引导生成机制，解决了时间序列建模中数值生成与语义理解之间的割裂问题，在语义理解和数值精度方面均取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 时间序列建模领域存在数值生成与语义理解的尖锐割裂：生成模型依赖表层模式匹配，而理解模型难以实现高保真数值输出。尽管统一多模态模型在视觉领域已弥合此类差距，但其在时间序列中的潜力尚未被挖掘。

Method: 提出TimeOmni-VL框架，包含两大创新：(1) Bi-TSI fidelity-preserving双向映射技术，实现时间序列与图像间的近无损转换；(2) 理解引导的生成机制，构建TSUMM-Suite数据集（含6项理解任务与2项生成任务），并引入校准的思维链，将时间序列理解作为显式控制信号用于高保真生成。

Result: 实验证实该统一框架同时显著提升了语义理解能力和数值生成精度，为多模态时间序列建模开辟了新前沿。

Conclusion: TimeOmni-VL成功弥合了时间序列理解与生成之间的鸿沟，通过视觉化统一建模为多模态时间序列分析建立了新范式。

Abstract: Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output. Although unified multimodal models (UMMs) have bridged this gap in vision, their potential for time series remains untapped. We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. With a calibrated Chain-of-Thought, TimeOmni-VL is the first to leverage time series understanding as an explicit control signal for high-fidelity generation. Experiments confirm that this unified approach significantly improves both semantic understanding and numerical precision, establishing a new frontier for multimodal time series modeling.

</details>


### [90] [Powering Up Zeroth-Order Training via Subspace Gradient Orthogonalization](https://arxiv.org/abs/2602.17155)
*Yicheng Lang,Changsheng Wang,Yihua Zhang,Mingyi Hong,Zheng Zhang,Wotao Yin,Sijia Liu*

Main category: cs.LG

TL;DR: 该论文针对零阶优化中精度与查询效率的根本矛盾，提出ZO-Muon方法，通过融合基于投影的子空间梯度估计和Muon风格的谱优化正交化，实现了大模型微调时收敛速度、准确率和效率的共赢。


<details>
  <summary>Details</summary>
Motivation: 零阶优化虽然避免了反向传播，在大模型微调中展现出内存优势，但存在梯度估计精度与查询效率之间的根本张力。传统方法难以同时保证高准确率和低查询成本，制约了其在大规模模型上的实际应用。

Method: 提出子空间梯度正交化统一框架，并实例化为ZO-Muon方法：1）采用投影子空间视角，利用模型更新的固有低秩结构降低梯度估计方差；2）引入Muon式谱优化思想，对噪声零阶梯度进行正交化处理以提取信息谱结构。该方法可解释为零阶设置下的低秩Muon优化器。

Result: 在LLMs和ViTs上的大量实验表明：相比流行的MeZO基线，ZO-Muon仅需24.7%的查询次数即可达到相同的SST-2性能；在ViT-B/CIFAR-100微调上准确率提升25.1%。同时显著加速收敛，在精度和查询/运行效率上实现双赢。

Conclusion: 通过将子空间降维和谱正交化有机结合，ZO-Muon有效解决了零阶优化的核心矛盾，为大规模模型高效微调提供了新范式，证明了零阶优化在保持内存优势的同时可接近一阶方法的性能。

Abstract: Zeroth-order (ZO) optimization provides a gradient-free alternative to first-order (FO) methods by estimating gradients via finite differences of function evaluations, and has recently emerged as a memory-efficient paradigm for fine-tuning large-scale models by avoiding backpropagation. However, ZO optimization has a fundamental tension between accuracy and query efficiency. In this work, we show that ZO optimization can be substantially improved by unifying two complementary principles: (i) a projection-based subspace view that reduces gradient estimation variance by exploiting the intrinsic low-rank structure of model updates, and (ii) Muon-style spectral optimization that applies gradient orthogonalization to extract informative spectral structure from noisy ZO gradients. These findings form a unified framework of subspace gradient orthogonalization, which we instantiate in a new method, ZO-Muon, admitting a natural interpretation as a low-rank Muon optimizer in the ZO setting. Extensive experiments on large language models (LLMs) and vision transformers (ViTs) demonstrate that ZO-Muon significantly accelerates convergence and achieves a win-win improvement in accuracy and query/runtime efficiency. Notably, compared to the popular MeZO baseline, ZO-Muon requires only 24.7% of the queries to reach the same SST-2 performance for LLM fine-tuning, and improves accuracy by 25.1% on ViT-B fine-tuning on CIFAR-100.

</details>


### [91] [In-Context Learning in Linear vs. Quadratic Attention Models: An Empirical Study on Regression Tasks](https://arxiv.org/abs/2602.17171)
*Ayush Goel,Arjun Kohli,Sarvagya Somvanshi*

Main category: cs.LG

TL;DR: 本文通过实证研究比较了标准Transformer（二次注意力）与线性注意力模型在线性回归任务中的上下文学习能力，发现两者在ICL表现上既有相似之处，也存在明显差异，特别是在学习质量、收敛性和泛化行为方面。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明Transformer和线性注意力模型均能在线性回归等简单函数类上实现上下文学习，但两种注意力机制在ICL行为上的具体差异尚不明确。本文旨在系统比较两者在标准线性回归任务中的表现，揭示线性注意力相对于二次注意力的优劣。

Method: 研究采用实证方法，在Garg等人提出的标准线性回归基准任务上，对比分析标准Transformer与线性注意力模型的ICL性能。评估指标包括均方误差（MSE）、收敛速度、泛化行为，并考察模型深度对ICL性能的影响。

Result: 实验结果表明，线性注意力与二次注意力在上下文学习中存在相似之处，但也暴露出明显的局限性。具体表现在学习质量、收敛特性和泛化能力等方面的差异。

Conclusion: 本研究表明，线性注意力在上下文学习任务中虽具备一定潜力，但相较于标准二次注意力机制仍存在局限。该发现为理解不同注意力架构的ICL能力提供了重要洞察，对模型架构选择具有指导意义。

Abstract: Recent work has demonstrated that transformers and linear attention models can perform in-context learning (ICL) on simple function classes, such as linear regression. In this paper, we empirically study how these two attention mechanisms differ in their ICL behavior on the canonical linear-regression task of Garg et al. We evaluate learning quality (MSE), convergence, and generalization behavior of each architecture. We also analyze how increasing model depth affects ICL performance. Our results illustrate both the similarities and limitations of linear attention relative to quadratic attention in this setting.

</details>


### [92] [The Anxiety of Influence: Bloom Filters in Transformer Attention Heads](https://arxiv.org/abs/2602.17526)
*Peter Balogh*

Main category: cs.LG

TL;DR: 研究在四个 Transformer 模型中识别出专门检测 token 重复的成员测试注意力头。发现三种真实类型：两个高精度头（假阳性率 0-4%），一个符合布鲁姆过滤器理论（R²=1.0），另一个被重新分类。它们构成早期层多分辨率系统，与诱导头和前序 token 头分类学不同，且泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 识别 Transformer 注意力头中是否存在专门用于成员测试（判断 token 是否重复出现）的功能单元，并刻画其行为模式和分类学地位，以深化对注意力计算机制的理解。

Method: 在 GPT-2 small/medium/large 和 Pythia-160M 中识别候选头，通过混淆变量控制、布鲁姆过滤器理论拟合（R²、容量参数）及消融实验分析其行为模式和功能贡献。

Result: 三个真正的成员测试头集中于第 0-1 层：L0H1/L0H5 假阳性率 0-4% @180 个唯一 token；L1H11 严格遵循布鲁姆过滤器公式（R²=1.0，容量 5 位）；L3H0 被重新分类。它们与诱导头/前序 token 头分类学不同，假阳性率随嵌入距离衰减，对任意重复 token 的泛化能力高 43%，且同时参与重复和全新 token 处理。

Conclusion: 成员测试是注意力头的真实功能，这些头在分类学上独立于已知类型。通过混淆控制验证的三个头揭示了注意力机制的新功能维度，体现了严谨方法论在解释性研究中的价值——排除假阳性后剩余发现更具说服力。

Abstract: Some transformer attention heads appear to function as membership testers, dedicating themselves to answering the question "has this token appeared before in the context?" We identify these heads across four language models (GPT-2 small, medium, and large; Pythia-160M) and show that they form a spectrum of membership-testing strategies. Two heads (L0H1 and L0H5 in GPT-2 small) function as high-precision membership filters with false positive rates of 0-4\% even at 180 unique context tokens -- well above the $d_\text{head} = 64$ bit capacity of a classical Bloom filter. A third head (L1H11) shows the classic Bloom filter capacity curve: its false positive rate follows the theoretical formula $p \approx (1 - e^{-kn/m})^k$ with $R^2 = 1.0$ and fitted capacity $m \approx 5$ bits, saturating by $n \approx 20$ unique tokens. A fourth head initially identified as a Bloom filter (L3H0) was reclassified as a general prefix-attention head after confound controls revealed its apparent capacity curve was a sequence-length artifact. Together, the three genuine membership-testing heads form a multi-resolution system concentrated in early layers (0-1), taxonomically distinct from induction and previous-token heads, with false positive rates that decay monotonically with embedding distance -- consistent with distance-sensitive Bloom filters. These heads generalize broadly: they respond to any repeated token type, not just repeated names, with 43\% higher generalization than duplicate-token-only heads. Ablation reveals these heads contribute to both repeated and novel token processing, indicating that membership testing coexists with broader computational roles. The reclassification of L3H0 through confound controls strengthens rather than weakens the case: the surviving heads withstand the scrutiny that eliminated a false positive in our own analysis.

</details>


### [93] [Continual uncertainty learning](https://arxiv.org/abs/2602.17174)
*Heisei Yonezawa,Ansei Yonezawa,Itsuro Kajiwara*

Main category: cs.LG

TL;DR: 本研究提出一种基于课程学习的持续学习框架，用于解决机械系统中多源不确定性导致的鲁棒控制难题。通过将复杂的控制问题分解为若干顺序学习任务，逐步扩展系统的动态不确定性集合，并在每个任务中引入模型预测控制器（MBC）作为基准性能，以实现DRL策略的快速收敛并避免灾难性遗忘。该方法在汽车动力总成主动振动控制上得到验证，展现出对结构非线性和动态变化的抗干扰能力以及成功的仿真‑现实迁移。


<details>
  <summary>Details</summary>
Motivation: 机械系统在面对多源不确定性（如非线性动态、工况变化）时，传统鲁棒控制方法难以兼顾性能与适应性。尽管深度强化学习（DRL）结合域随机化在减小仿真‑现实差距方面取得进展，但同时对所有不确定性进行学习往往导致次优策略和低效的训练。为此，本文旨在提出一种更高效的学习框架，以提升多源不确定性系统的鲁棒控制性能。

Method: 1) 将含多源不确定性的控制问题形式化为课程学习（curriculum‑based）持续学习任务序列；2) 将原系统扩展为一组植物（plants），其动态不确定性随学习进程逐步扩展和多样化；3) 在任务间稳定更新策略，防止灾难性遗忘；4) 将模型预测控制器（MBC）作为共享基准嵌入学习过程，形成残差学习（residual learning）框架，以加速收敛；5) 在汽车动力总成主动振动控制中进行实际工业应用验证。

Result: 实验验证表明，所提控制器在面对结构非线性和动态变化时具有鲁棒性，并成功实现了仿真到现实的迁移。相较于直接对所有不确定性进行端到端学习，该方法在样本效率和学习速度上均有显著提升，且在不同不确定性配置下均能保持稳定的控制性能。

Conclusion: 本文提出的课程学习持续学习框架有效解决了机械系统多源不确定性的鲁棒控制问题，通过任务分解、基准控制器嵌入和残差学习，显著提升了学习效率与控制性能。该方案在工业主动振动控制中的成功应用，展示了其在实际工程中的可行性和推广潜力，为类似复杂系统的鲁棒控制提供了新思路。

Abstract: Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting. To ensure learning efficiency, we jointly incorporate a model-based controller (MBC), which guarantees a shared baseline performance across the plant sets, into the learning process to accelerate the convergence. This residual learning scheme facilitates task-specific optimization of the DRL agent for each uncertainty, thereby enhancing sample efficiency. As a practical industrial application, this study applies the proposed method to designing an active vibration controller for automotive powertrains. We verified that the resulting controller is robust against structural nonlinearities and dynamic variations, realizing successful sim-to-real transfer.

</details>


### [94] [SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch](https://arxiv.org/abs/2602.17206)
*Ron Shapira Weber,Oren Freifeld*

Main category: cs.LG

TL;DR: 这篇论文介绍了一个名为softdtw-cuda-torch的开源PyTorch库，用于在GPU上计算Soft Dynamic Time Warping。它通过分块反对角线核执行、对数空间反向传播和融合距离计算三种关键技术，解决了现有实现中的序列长度限制、数值不稳定性和高内存消耗问题，实现了支持任意序列长度、完整自动微分集成和Soft-DTW Barycenter计算。


<details>
  <summary>Details</summary>
Motivation: 现有的GPU加速SoftDTW实现存在三个关键限制：序列长度被硬编码限制在1024以内、小平滑参数时反向传播出现数值不稳定导致浮点数溢出、以及因构建庞大的成对距离张量而导致GPU内存消耗过高。这些限制严重制约了SoftDTW在处理长序列、大规模数据和复杂模型时的应用。

Method: 作者提出了三种创新方法：(1) 分块反对角线核执行策略，通过平铺计算消除序列长度约束；(2) 对数空间反向传播算法，通过数值稳定变换防止浮点数溢出；(3) 融合距离计算模式，在计算过程中避免物化O(BNM)规模的中间距离张量。这些方法深度优化了GPU并行计算效率。

Result: 该库相比先前工作实现了高达98%的内存消耗降低，完全支持任意长度的序列计算，无缝集成PyTorch自动微分系统，并额外提供Soft-DTW Barycenter计算功能。代码已在GitHub开源，为研究者和开发者提供了可直接集成的GPU加速解决方案。

Conclusion: 该工作通过系统性的算法重构和工程优化，成功攻克了SoftDTW在GPU部署中的核心技术瓶颈，为时间序列分析领域提供了高效、稳定且易用的开源工具库。这项工作将显著推动SoftDTW在大规模时序数据分析和深度学习模型中的应用，具有重要的实用价值和推广意义。

Abstract: We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTorch autograd integration, and Soft-DTW Barycenter computation. Code is available at https://github.com/BGU-CS-VIL/sdtw-cuda-torch.

</details>


### [95] [Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting](https://arxiv.org/abs/2602.17645)
*Xiaohan Zhao,Zhaoyi Li,Yaxin Luo,Jiacheng Cui,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 本文针对大型视觉-语言模型（LVLM）的黑盒对抗攻击难题，提出了M-Attack-V2方法。该方法通过多作物对齐（MCA）、辅助目标对齐（ATA）和补丁动量（Patch Momentum）三项改进，显著降低了梯度方差并稳定了优化过程。在Claude-4.0、Gemini-2.5-Pro和GPT-5等前沿模型上，攻击成功率分别提升至30%、97%和100%，大幅优于现有黑盒攻击方法。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型的黑盒对抗攻击面临梯度缺失和多模态边界复杂的挑战。虽然基于迁移的M-Attack等方法通过局部作物级匹配取得了较好效果，但研究发现这会导致迭代间高方差、近乎正交的梯度，破坏了局部对齐的连贯性并导致优化不稳定。作者将此归因于：(i) ViT的平移敏感性产生尖峰状梯度；(ii) 源作物与目标作物之间的结构不对称性。

Method: 本文提出M-Attack-V2，包含三个核心模块：1）多作物对齐（MCA）：在源模型端，通过对每轮迭代中多个独立采样的局部视图的梯度进行平均，降低方差；2）辅助目标对齐（ATA）：在目标模型端，用来自语义相关分布的小规模辅助集替代激进的目标增强，生成更平滑、低方差的目标流形；3）补丁动量：将动量重新解释为历史作物梯度的重放，并结合优化的补丁尺寸集成（PE+）来强化可迁移方向。这些模块形成对M-Attack的简单模块化增强。

Result: 在多个前沿LVLM上的实验结果表明，M-Attack-V2显著提升了黑盒攻击效果：Claude-4.0攻击成功率从8%提升至30%，Gemini-2.5-Pro从83%提升至97%，GPT-5从98%提升至100%，超越了现有的黑盒LVLM攻击方法。

Conclusion: M-Attack-V2通过解决梯度不稳定性和流形不对称性问题，为LVLM黑盒对抗攻击提供了简单而有效的模块化增强方案，在多个商业大模型上实现了突破性的攻击成功率。代码和数据已开源。

Abstract: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.

</details>


### [96] [Structured Prototype-Guided Adaptation for EEG Foundation Models](https://arxiv.org/abs/2602.17251)
*Jingying Ma,Feng Wu,Yucheng Xing,Qika Lin,Tianyu Liu,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: 针对有限监督下EEG基础模型泛化能力差的问题，本文提出SCOPE框架。该框架通过两阶段流程：首先学习几何正则化任务先验，构建类别原型并生成置信度感知伪标签；其次使用轻量级适配器ProAdapter，在冻结的EEG基础模型上进行条件化微调。实验表明，SCOPE在标签受限的跨被试场景下性能优越且高效。


<details>
  <summary>Details</summary>
Motivation: EEG基础模型在全微调下表现优异，但在真实临床场景中受限于被试级标注数据稀缺时泛化能力显著下降。这种失败源于噪声稀疏监督信号与高可塑性模型参数空间之间的结构性失配，而非简单的监督不足。

Method: 提出SCOPE（结构化置信度感知原型引导适应框架），采用两阶段流程：第一阶段通过几何正则化任务先验学习，在嵌入空间构建平衡的类别级原型，并利用原型一致性生成置信度感知伪标签以过滤不可靠信号；第二阶段设计ProAdapter，通过轻量级适配器以结构化原型为条件，对冻结的EEG基础模型进行参数高效微调。

Result: 在三个EEG任务和五个基础模型骨干上的实验表明，SCOPE在标签受限的跨被试设置下持续实现强劲性能与计算效率，有效解决了有限监督下的泛化难题。

Conclusion: SCOPE通过结构化置信度感知原型引导机制，为EEG基础模型在临床有限标注场景下的适应提供了有效解决方案，显著提升了跨被试泛化能力，同时保持参数效率。

Abstract: Electroencephalography (EEG) foundation models (EFMs) have achieved strong performance under full fine-tuning but exhibit poor generalization when subject-level supervision is limited, a common constraint in real-world clinical settings. We show that this failure stems not merely from limited supervision, but from a structural mismatch between noisy, limited supervision and the highly plastic parameter space of EFMs. To address this challenge, we propose SCOPE, a Structured COnfidence-aware Prototype-guided adaptation framework for EFM fine-tuning. SCOPE follows a two-stage pipeline. In the first stage, we construct reliable external supervision by learning geometry-regularized task priors, constructing balanced class-level prototypes over the resulting embeddings, and producing confidence-aware pseudo-labels from their agreement to filter unreliable signals on unlabeled data. In the second stage, we introduce ProAdapter, which adapts frozen EEG foundation models via a lightweight adapter conditioned on the structured prototypes. Experiments across three EEG tasks and five foundation model backbones demonstrate that SCOPE consistently achieves strong performance and efficiency under label-limited cross-subject settings.

</details>


### [97] [Learning a Latent Pulse Shape Interface for Photoinjector Laser Systems](https://arxiv.org/abs/2602.17263)
*Alexander Klemps,Denis Ilia,Pradeep Kr. Banerjee,Ye Chen,Henrik Tünnermann,Nihat Ay*

Main category: cs.LG

TL;DR: 针对自由电子激光光电注入器中纵向激光脉冲形状优化问题，提出基于Wasserstein自编码器的生成式建模框架，通过构建可微潜在空间接口，实现高效脉冲设计空间探索，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 在自由电子激光装置的光电注入器中，精确控制纵向激光脉冲形状是优化电子束品质的关键手段。然而，由于设计空间庞大，系统性的参数探索受到暴力脉冲传播模拟极高计算成本的严重制约。

Method: 采用Wasserstein自编码器构建生成式建模框架，在脉冲整形参数与下游束流动力学之间建立可微分的潜在空间映射关系。

Result: 所学习的潜在空间具有连续性和可解释性，且重建保真度高。具体表现为：1）高阶高斯等脉冲族系在潜在空间中形成连贯轨迹；2）标准化时间脉冲长度揭示潜在组织与脉冲能量的相关性；3）通过主成分分析和混合高斯模型验证了潜在几何结构的良好特性；4）支持线性插值实现不同脉冲类型间的平滑过渡；5）模型具备从模拟数据到真实实验测量的泛化能力。

Conclusion: 该方法有效减少了对高成本脉冲传播模拟的依赖，为下游束流动力学的模拟与分析提供了高效工具。

Abstract: Controlling the longitudinal laser pulse shape in photoinjectors of Free-Electron Lasers is a powerful lever for optimizing electron beam quality, but systematic exploration of the vast design space is limited by the cost of brute-force pulse propagation simulations. We present a generative modeling framework based on Wasserstein Autoencoders to learn a differentiable latent interface between pulse shaping and downstream beam dynamics. Our empirical findings show that the learned latent space is continuous and interpretable while maintaining high-fidelity reconstructions. Pulse families such as higher-order Gaussians trace coherent trajectories, while standardizing the temporal pulse lengths shows a latent organization correlated with pulse energy. Analysis via principal components and Gaussian Mixture Models reveals a well behaved latent geometry, enabling smooth transitions between distinct pulse types via linear interpolation. The model generalizes from simulated data to real experimental pulse measurements, accurately reconstructing pulses and embedding them consistently into the learned manifold. Overall, the approach reduces reliance on expensive pulse-propagation simulations and facilitates downstream beam dynamics simulation and analysis.

</details>


### [98] [Unified Latents (UL): How to train your latents](https://arxiv.org/abs/2602.17270)
*Jonathan Heek,Emiel Hoogeboom,Thomas Mensink,Tim Salimans*

Main category: cs.LG

TL;DR: Unified Latents (UL)框架通过扩散先验和扩散模型联合正则化学习潜在表示，将编码器输出噪声与先验最小噪声水平直接关联，获得简化的训练目标（紧致比特率上界），在ImageNet-512上实现FID 1.4且重建质量高（PSNR），训练FLOPs低于Stable Diffusion基线模型；在Kinetics-600视频数据集上达到SOTA FVD 1.3，同时保持计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 现有潜在表示学习方法（如Stable Diffusion的VAE潜在空间）在图像/视频生成任务中存在重建质量、生成质量与训练效率的权衡问题。扩散模型虽具强大生成能力，但其潜在空间并非为高效表示学习而优化，亟需一种统一框架同时提升表示质量与计算效率。

Method: 提出Unified Latents (UL)框架：1）设计联合正则化机制，使潜在表示同时受扩散先验约束并由扩散模型解码；2）创新性地将编码器输出噪声与扩散先验的最小噪声水平直接挂钩，简化训练目标；3）理论证明该目标函数提供潜在比特率的紧致上界，实现高效的率失真权衡。

Result: 图像领域：在ImageNet-512上实现竞争性的FID 1.4，PSNR重建质量优异，且训练计算量（FLOPs）低于基于Stable Diffusion潜在空间的模型。视频领域：在Kinetics-600上刷新SOTA纪录，FVD达1.3。两项实验均验证了方法在质量与效率上的双重优势。

Conclusion: UL框架通过噪声关联机制统一了表示学习与生成建模，在图像和视频任务上同步实现了高质量重建与生成，同时降低训练成本。该方法为开发高效、可扩展的多媒体表示学习系统提供了新范式，具有重要实践价值。

Abstract: We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.

</details>


### [99] [RLGT: A reinforcement learning framework for extremal graph theory](https://arxiv.org/abs/2602.17276)
*Ivan Damnjanović,Uroš Milivojević,Irena Đorđević,Dragan Stevanović*

Main category: cs.LG

TL;DR: 本文回顾强化学习在极值图论中的进展，包括Wagner开创性应用深度交叉熵方法及其后续发展，列举在Laplacian谱半径不等式、Ramsey数下界和Turán型极值问题等方面的重要成果，并提出RLGT——一个支持多种图结构、计算高效且设计模块化的新型强化学习框架。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在极值图论中取得多项突破性成果，但现有研究缺乏系统化整合，且框架对图类型的支持有限。为推动该领域发展，本文旨在构建一个通用、高效且可扩展的强化学习工具，以填补这一方法学空白。

Method: 本文提出名为RLGT（Reinforcement Learning for Graph Theory）的新型强化学习框架。该框架系统化先前工作，支持无向图和有向图、允许或不允许环、以及任意数量的边颜色，通过优化的图表示实现高效计算，并采用清晰模块化设计以方便未来扩展。

Result: 基于RL的方法已成功解决极值图论中多个重要问题：反例Laplacian谱半径不等式、改进特定Ramsey数的下界，以及对禁止长度为3和4的圈的Turán型极值问题的贡献。本文的主要结果是提出RLGT框架，为未来研究提供系统化工具，具备高效性能与灵活架构。

Conclusion: 本文提出的RLGT框架为强化学习在极值图论领域的应用提供了系统化基础设施。通过支持多样化图结构和模块化设计，该框架有望显著促进未来RL驱动的图论研究，深化人工智能与离散数学的交叉融合。

Abstract: Reinforcement learning (RL) is a subfield of machine learning that focuses on developing models that can autonomously learn optimal decision-making strategies over time. In a recent pioneering paper, Wagner demonstrated how the Deep Cross-Entropy RL method can be applied to tackle various problems from extremal graph theory by reformulating them as combinatorial optimization problems. Subsequently, many researchers became interested in refining and extending the framework introduced by Wagner, thereby creating various RL environments specialized for graph theory. Moreover, a number of problems from extremal graph theory were solved through the use of RL. In particular, several inequalities concerning the Laplacian spectral radius of graphs were refuted, new lower bounds were obtained for certain Ramsey numbers, and contributions were made to the Turán-type extremal problem in which the forbidden structures are cycles of length three and four. Here, we present Reinforcement Learning for Graph Theory (RLGT), a novel RL framework that systematizes the previous work and provides support for both undirected and directed graphs, with or without loops, and with an arbitrary number of edge colors. The framework efficiently represents graphs and aims to facilitate future RL-based research in extremal graph theory through optimized computational performance and a clean and modular design.

</details>


### [100] [Efficient privacy loss accounting for subsampling and random allocation](https://arxiv.org/abs/2602.17284)
*Vitaly Feldman,Moshe Shenfeld*

Main category: cs.LG

TL;DR: 本文研究随机分配采样方案的隐私放大特性，提出了基于隐私损失分布实现的高效隐私会计框架。与Poisson采样相比，该方案在隐私-效用权衡上至少同样有效，且在DP-SGD训练中表现更优，避免了传统方法中近似分析导致的非紧性和额外开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有随机分配采样方案的分析存在两个主要缺陷：一是实际隐私参数因近似步骤而不够紧，二是使用曲棍球棒散度或Rényi散度引入额外计算开销。这限制了其在差分隐私优化和高效聚合中的应用。

Method: 本文引入隐私损失分布(PLD)实现的概念，开发了一个通用隐私会计框架。该框架能够高效计算任何差分隐私算法在随机分配下的隐私损失分布，从而支持自动化的、精确的隐私核算。

Result: 理论分析表明，对于高斯机制，随机分配的隐私-效用权衡至少不劣于Poisson子采样。特别地，随机分配更适合差分隐私随机梯度下降(DP-SGD)的训练过程。新框架消除了对手动噪声机制特定分析的需求。

Conclusion: 本研究通过PLD实现的概念，为随机分配采样方案提供了高效准确的隐私会计方法，在保持相同隐私水平的同时提供更好的效用，并为差分隐私机器学习中的采样策略设计提供了新工具。

Abstract: We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) and communication-efficient high-dimensional private aggregation (Asi et al., 2025), where it was shown to have utility advantages over the standard Poisson sampling. Theoretical analyses of this sampling scheme (Feldman & Shenfeld, 2025; Dong et al., 2025) lead to bounds that are close to those of Poisson sampling, yet still have two significant shortcomings. First, in many practical settings, the resulting privacy parameters are not tight due to the approximation steps in the analysis. Second, the computed parameters are either the hockey stick or Renyi divergence, both of which introduce overheads when used in privacy loss accounting.
  In this work, we demonstrate that the privacy loss distribution (PLD) of random allocation applied to any differentially private algorithm can be computed efficiently. When applied to the Gaussian mechanism, our results demonstrate that the privacy-utility trade-off for random allocation is at least as good as that of Poisson subsampling. In particular, random allocation is better suited for training via DP-SGD. To support these computations, our work develops new tools for general privacy loss accounting based on a notion of PLD realization. This notion allows us to extend accurate privacy loss accounting to subsampling which previously required manual noise-mechanism-specific analysis.

</details>


### [101] [Flickering Multi-Armed Bandits](https://arxiv.org/abs/2602.17315)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 提出Flickering多臂老虎机(FMAB)框架，研究可用臂集随历史选择动态变化的约束决策问题，通过随机图过程建模，在ER和边马尔可夫模型下提出两阶段算法（懒惰随机游走探索+导航利用），证明其次线性遗憾界并建立匹配的信息论下界，揭示局部移动约束下的基本探索代价。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机假设臂集恒定，但许多现实场景（如灾后侦察机器人）中可用选项受历史决策动态约束。FMAB旨在建模这种"闪烁"可用性，研究智能体在局部邻域内移动受限的高效探索问题，弥补现有理论的不足。

Method: 将臂建模为图节点，智能体行动限制在局部邻域内。分析两种随机图模型：i.i.d. Erdős-Rényi过程和边马尔可夫过程。提出两阶段算法：先用懒惰随机游走探索图结构，再进行导航与承诺式利用。通过高概率分析和期望分析证明遗憾界，并构建信息论下界验证算法近最优性。

Result: 在两种图模型下均获得次线性遗憾界（高概率与期望形式）。算法探索成本接近最优，匹配的信息论下界表明该代价是局部移动约束下的根本性限制。数值仿真（含灾后侦察机器人场景）验证了理论保证。

Conclusion: FMAB框架有效刻画了动态受限的序贯决策问题，所提算法在理论和实践中均表现出色，其近最优性揭示了局部移动约束下探索的根本代价，为灾后侦察等实际应用提供了理论基础。

Abstract: We introduce Flickering Multi-Armed Bandits (FMAB), a new MAB framework where the set of available arms (or actions) can change at each round, and the available set at any time may depend on the agent's previously selected arm. We model this constrained, evolving availability using random graph processes, where arms are nodes and the agent's movement is restricted to its local neighborhood. We analyze this problem under two random graph models: an i.i.d. Erdős--Rényi (ER) process and an Edge-Markovian process. We propose and analyze a two-phase algorithm that employs a lazy random walk for exploration to efficiently identify the optimal arm, followed by a navigation and commitment phase for exploitation. We establish high-probability and expected sublinear regret bounds for both graph settings. We show that the exploration cost of our algorithm is near-optimal by establishing a matching information-theoretic lower bound for this problem class, highlighting the fundamental cost of exploration under local-move constraints. We complement our theoretical guarantees with numerical simulations, including a scenario of a robotic ground vehicle scouting a disaster-affected region.

</details>


### [102] [The Sound of Death: Deep Learning Reveals Vascular Damage from Carotid Ultrasound](https://arxiv.org/abs/2602.17321)
*Christoph Balada,Aida Romano-Martinez,Payal Varshney,Vincent ten Cate,Katharina Geschke,Jonas Tesarz,Paul Claßen,Alexander K. Schuster,Dativa Tibyampansha,Karl-Patrik Kresoja,Philipp S. Wild,Sheraz Ahmed,Andreas Dengel*

Main category: cs.LG

TL;DR: 本研究提出一种机器学习框架，从无创颈动脉超声视频中提取血管损伤特征，仅以高血压作为弱监督标签，即可预测心血管事件风险，性能媲美或超越传统评分模型，为大规模心血管风险筛查提供了低成本解决方案。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球头号死因，但早期风险检测受限于现有诊断手段。颈动脉超声虽无创且普及，但其中蕴含的丰富结构和血流动力学信息未被充分利用。

Method: 开发机器学习框架，利用高血压作为弱监督标签，从颈动脉超声视频中提取具有临床意义的血管损伤表征。模型学习稳健、生物学合理且可解释的特征。

Result: 高血管损伤评分可有效分层心肌梗死、心源性死亡和全因死亡风险，表现匹配或超越SCORE2等传统风险模型。可解释性AI分析揭示模型依赖于血管形态和血管周围组织特征。

Conclusion: 常规颈动脉超声包含远超既往认知的预后信息。该方案提供可扩展、无创且成本效益高的工具，支持不依赖实验室检测或复杂临床输入的大规模心血管风险评估，实现更早、更个性化的预防策略。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, yet early risk detection is often limited by available diagnostics. Carotid ultrasound, a non-invasive and widely accessible modality, encodes rich structural and hemodynamic information that is largely untapped. Here, we present a machine learning (ML) framework that extracts clinically meaningful representations of vascular damage (VD) from carotid ultrasound videos, using hypertension as a weak proxy label. The model learns robust features that are biologically plausible, interpretable, and strongly associated with established cardiovascular risk factors, comorbidities, and laboratory measures. High VD stratifies individuals for myocardial infarction, cardiac death, and all-cause mortality, matching or outperforming conventional risk models such as SCORE2. Explainable AI analyses reveal that the model relies on vessel morphology and perivascular tissue characteristics, uncovering novel functional and anatomical signatures of vascular damage. This work demonstrates that routine carotid ultrasound contains far more prognostic information than previously recognized. Our approach provides a scalable, non-invasive, and cost-effective tool for population-wide cardiovascular risk assessment, enabling earlier and more personalized prevention strategies without reliance on laboratory tests or complex clinical inputs.

</details>


### [103] [From Subtle to Significant: Prompt-Driven Self-Improving Optimization in Test-Time Graph OOD Detection](https://arxiv.org/abs/2602.17342)
*Luzhi Wang,Xuanshuo Fu,He Zhang,Chuang Liu,Xiaobao Wang,Hongbo Liu*

Main category: cs.LG

TL;DR: 本文提出SIGOOD，一种自改进的无监督图分布外检测框架，通过迭代优化提示增强图并引入能量偏好损失，解决了现有单次推理方法无法渐进放大OOD信号的问题。


<details>
  <summary>Details</summary>
Motivation: 图OOD检测是保证GNN在开放世界部署可靠性的关键。现有测试时训练方法受限于单次推理范式，难以通过迭代方式修正预测并增强OOD信号。

Method: 提出SIGOOD框架：(1)生成提示构建增强图以放大潜在OOD信号；(2)设计能量偏好优化(EPO)损失，利用原始图与增强图的能量差异；(3)通过自改进循环迭代优化提示；(4)使用最终优化后的增强图进行OOD检测。

Result: 在21个真实世界数据集上的实验验证了SIGOOD的有效性和性能优势。

Conclusion: SIGOOD成功将持续自学习与测试时训练结合，为无监督图OOD检测提供了新范式，通过迭代优化克服了单次推理的局限。

Abstract: Graph Out-of-Distribution (OOD) detection aims to identify whether a test graph deviates from the distribution of graphs observed during training, which is critical for ensuring the reliability of Graph Neural Networks (GNNs) when deployed in open-world scenarios. Recent advances in graph OOD detection have focused on test-time training techniques that facilitate OOD detection without accessing potential supervisory information (e.g., training data). However, most of these methods employ a one-pass inference paradigm, which prevents them from progressively correcting erroneous predictions to amplify OOD signals. To this end, we propose a \textbf{S}elf-\textbf{I}mproving \textbf{G}raph \textbf{O}ut-\textbf{o}f-\textbf{D}istribution detector (SIGOOD), which is an unsupervised framework that integrates continuous self-learning with test-time training for effective graph OOD detection. Specifically, SIGOOD generates a prompt to construct a prompt-enhanced graph that amplifies potential OOD signals. To optimize prompts, SIGOOD introduces an Energy Preference Optimization (EPO) loss, which leverages energy variations between the original test graph and the prompt-enhanced graph. By iteratively optimizing the prompt by involving it into the detection model in a self-improving loop, the resulting optimal prompt-enhanced graph is ultimately used for OOD detection. Comprehensive evaluations on 21 real-world datasets confirm the effectiveness and outperformance of our SIGOOD method. The code is at https://github.com/Ee1s/SIGOOD.

</details>


### [104] [2Mamba2Furious: Linear in Complexity, Competitive in Accuracy](https://arxiv.org/abs/2602.17363)
*Gabriel Mongaras,Eric C. Larson*

Main category: cs.LG

TL;DR: 该论文针对线性注意力精度低于softmax注意力的问题，通过简化和改进Mamba-2模型，提出了2Mamba方法，在保持长上下文内存效率的同时，精度接近甚至超过softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 线性注意力因其计算效率成为softmax注意力的强有力替代方案，但其表达能力和准确性相对较低。为了弥补这一精度差距，研究者旨在探索如何在不显著增加计算成本的前提下提升线性注意力性能，特别是在长上下文场景下的实用性。

Method: 研究团队首先将Mamba-2简化为核心基础组件，评估其最佳性能的关键选择；基于简化版Mamba-2S，改进了A-mask机制并增加隐藏状态的阶数，提出新方法2Mamba；同时探索了使模型性能超越softmax注意力的关键元素。

Result: 2Mamba方法在精度上接近softmax注意力，同时在长上下文长度下具备显著更高的内存效率；研究识别出帮助模型超越softmax注意力精度的关键元素，并公开了所有实验代码。

Conclusion: 通过对Mamba-2的深度分析和改进，该研究成功弥合了线性注意力与softmax注意力之间的精度鸿沟，为长序列建模提供了兼具高效率和高准确性的解决方案，推动了高效注意力机制的发展。

Abstract: Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments

</details>


### [105] [A feature-stable and explainable machine learning framework for trustworthy decision-making under incomplete clinical data](https://arxiv.org/abs/2602.17364)
*Justyna Andrys-Olek,Paulina Tworek,Luca Gherardini,Mark W. Ruddock,Mary Jo Kurt,Peter Fitzgerald,Jose Sousa*

Main category: cs.LG

TL;DR: CACTUS是一种针对小型不完整临床数据的可解释机器学习框架，通过特征抽象与稳定性分析，在568例血尿患者膀胱癌预测中，相比随机森林和梯度提升，在保持竞争力的预测性能同时显著提高了特征稳定性，为生物医学数据可信决策提供了新框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习在生物医学数据中的应用受限于模型鲁棒性不足、可解释性差以及特征在不完整数据下的不稳定性。高性能模型若其关键特征随数据缺失而波动，将难以获得临床信任，影响结果可重复性和临床决策。

Method: 本研究提出CACTUS框架，整合特征抽象、可解释分类和系统性特征稳定性分析三个模块，用于量化数据质量退化时信息特征的一致性。利用568例血尿患者评估膀胱癌的真实世界队列，在随机缺失数据条件下，与随机森林和梯度提升等主流方法进行基准测试。

Result: CACTUS在预测性能上具有竞争力或更优，且在数据缺失增加时，其排名靠前的特征稳定性显著更高，这一优势在性别分层分析中依然成立。特征稳定性提供了独立于传统性能指标的信息，对评估模型可信度至关重要。

Conclusion: CACTUS通过显式量化对缺失数据的鲁棒性并优先选择可解释的稳定特征，为生物医学领域的可信数据驱动决策支持提供了可推广的框架。

Abstract: Machine learning models are increasingly applied to biomedical data, yet their adoption in high stakes domains remains limited by poor robustness, limited interpretability, and instability of learned features under realistic data perturbations, such as missingness. In particular, models that achieve high predictive performance may still fail to inspire trust if their key features fluctuate when data completeness changes, undermining reproducibility and downstream decision-making. Here, we present CACTUS (Comprehensive Abstraction and Classification Tool for Uncovering Structures), an explainable machine learning framework explicitly designed to address these challenges in small, heterogeneous, and incomplete clinical datasets. CACTUS integrates feature abstraction, interpretable classification, and systematic feature stability analysis to quantify how consistently informative features are preserved as data quality degrades. Using a real-world haematuria cohort comprising 568 patients evaluated for bladder cancer, we benchmark CACTUS against widely used machine learning approaches, including random forests and gradient boosting methods, under controlled levels of randomly introduced missing data. We demonstrate that CACTUS achieves competitive or superior predictive performance while maintaining markedly higher stability of top-ranked features as missingness increases, including in sex-stratified analyses. Our results show that feature stability provides information complementary to conventional performance metrics and is essential for assessing the trustworthiness of machine learning models applied to biomedical data. By explicitly quantifying robustness to missing data and prioritising interpretable, stable features, CACTUS offers a generalizable framework for trustworthy data-driven decision support.

</details>


### [106] [Variational Grey-Box Dynamics Matching](https://arxiv.org/abs/2602.17477)
*Gurjeet Sangra Singh,Frantzeska Lavda,Giangiacomo Mercatali,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 该论文提出了一种灰盒方法，将不完整的物理模型直接集成到流匹配生成模型中，通过结构化变分分布和两个潜在编码（一个用于缺失的随机性和多模态速度，另一个用于物理参数并带有物理信息先验）实现仅从观测轨迹学习动力学，避免神经常微分方程的可扩展性和稳定性问题，并在二阶动力学上进行了扩展，实验表明该方法在性能上可与全数据驱动方法相媲美或更优，同时保留物理模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型（如流匹配和扩散模型）在学习复杂分布和动力系统方面表现出色，但往往是黑箱，忽略了底层物理；而基于物理的模拟模型（由常微分方程/偏微分方程描述）具有可解释性，但可能存在缺失或未知项，无法完全描述真实世界观测。为此，本文旨在弥合这一差距。

Method: 方法的核心是在流匹配框架内建模一个结构化变分分布，使用两个潜在编码：一个用于建模缺失的随机性和多模态速度，另一个将物理参数编码为具有物理信息先验的潜变量；此外，框架还扩展至处理二阶动力学。整个方法仅从观测轨迹学习，无需真实物理参数，采用无模拟方式，避免神经常微分方程的可扩展性和稳定性问题。

Result: 在代表性的常微分方程/偏微分方程问题上的实验表明，该方法在性能上与全数据驱动方法及之前的灰盒基线相当或更优，同时保留了物理模型的可解释性。

Conclusion: 该方法成功地将物理知识融入生成模型，既提高了性能，又保持了可解释性，代码已开源。

Abstract: Deep generative models such as flow matching and diffusion models have shown great potential in learning complex distributions and dynamical systems, but often act as black-boxes, neglecting underlying physics. In contrast, physics-based simulation models described by ODEs/PDEs remain interpretable, but may have missing or unknown terms, unable to fully describe real-world observations. We bridge this gap with a novel grey-box method that integrates incomplete physics models directly into generative models. Our approach learns dynamics from observational trajectories alone, without ground-truth physics parameters, in a simulation-free manner that avoids scalability and stability issues of Neural ODEs. The core of our method lies in modelling a structured variational distribution within the flow matching framework, by using two latent encodings: one to model the missing stochasticity and multi-modal velocity, and a second to encode physics parameters as a latent variable with a physics-informed prior. Furthermore, we present an adaptation of the framework to handle second-order dynamics. Our experiments on representative ODE/PDE problems show that our method performs on par with or superior to fully data-driven approaches and previous grey-box baselines, while preserving the interpretability of the physics model. Our code is available at https://github.com/DMML-Geneva/VGB-DM.

</details>


### [107] [Linear Convergence in Games with Delayed Feedback via Extra Prediction](https://arxiv.org/abs/2602.17486)
*Yuma Fujimoto,Kenshi Abe,Kaito Ariu*

Main category: cs.LG

TL;DR: 本文研究了带延迟反馈的双线性博弈问题，提出了带额外乐观性的加权乐观梯度下降上升法(WOGDA)。通过将其解释为额外近端点(EPP)的近似，理论证明了额外乐观性可将收敛率从exp(-Θ(t/m^5))显著加速至exp(-Θ(t/(m^2 log m)))，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在实际多智能体学习中，反馈延迟不可避免且会严重损害性能，但延迟下的收敛速率问题在双线性博弈中仍未解决。现有乐观方法虽能应对延迟，但标准乐观策略的收敛速率较慢，需要更有效的理论分析与改进策略。

Method: 将WOGDA算法解释为额外近端点(EPP)的近似版本，该EPP基于更远的未来奖励而非经典近端点(PP)的即时奖励进行更新。通过分析标准乐观性（预测下一步奖励）与额外乐观性（预测更远未来奖励）对收敛速率的影响，推导线性收敛率。

Result: 理论证明：标准乐观性在延迟m下达到exp(-Θ(t/m^5))的线性收敛率；而额外乐观性不仅允许更大的步长，还将收敛率显著加速至exp(-Θ(t/(m^2 log m)))。实验结果与理论一致，验证了额外乐观性带来的加速效果。

Conclusion: 额外乐观性是应对反馈延迟导致性能退化的有效方法。该研究为多智能体学习在延迟环境下的收敛行为提供了理论保证，表明通过更远的未来奖励预测可以大幅提升算法效率，具有重要的理论和实践意义。

Abstract: Feedback delays are inevitable in real-world multi-agent learning. They are known to severely degrade performance, and the convergence rate under delayed feedback is still unclear, even for bilinear games. This paper derives the rate of linear convergence of Weighted Optimistic Gradient Descent-Ascent (WOGDA), which predicts future rewards with extra optimism, in unconstrained bilinear games. To analyze the algorithm, we interpret it as an approximation of the Extra Proximal Point (EPP), which is updated based on farther future rewards than the classical Proximal Point (PP). Our theorems show that standard optimism (predicting the next-step reward) achieves linear convergence to the equilibrium at a rate $\exp(-Θ(t/m^{5}))$ after $t$ iterations for delay $m$. Moreover, employing extra optimism (predicting farther future reward) tolerates a larger step size and significantly accelerates the rate to $\exp(-Θ(t/(m^{2}\log m)))$. Our experiments also show accelerated convergence driven by the extra optimism and are qualitatively consistent with our theorems. In summary, this paper validates that extra optimism is a promising countermeasure against performance degradation caused by feedback delays.

</details>


### [108] [Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models](https://arxiv.org/abs/2602.17497)
*Wen-Tse Chen,Jiayu Chen,Fahim Tajwar,Hao Zhu,Xintong Duan,Ruslan Salakhutdinov,Jeff Schneider*

Main category: cs.LG

TL;DR: 针对自进化智能体在稀疏奖励下的学习挑战，本研究利用LLM预训练知识通过回顾性上下文学习（RICL）实现时序信用分配，并构建在线学习框架RICOL。在BabyAI基准测试中，该方法以更高样本效率达到与传统RL相当的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 当前时序信用分配虽能将稀疏反馈转化为密集监督信号，但因依赖任务特定价值函数，导致样本效率低下且泛化能力受限，制约了自进化智能体的实际应用。

Method: 创新性提出回顾性上下文学习（RICL）方法，借助大型语言模型的预训练知识将稀疏奖励转化为密集优势函数；进而构建RICOL在线学习框架，迭代地基于RICL的信用分配结果优化策略。

Result: 实验验证RICL仅用少量样本即可精准估计优势函数并定位环境关键状态。在BabyAI四个场景中，RICOL以显著更高的样本效率实现了与传统在线RL算法相近的收敛性能。

Conclusion: 该工作证实了LLM在时序信用分配中的有效性，为发展样本高效、泛化性强的强化学习新范式提供了重要思路。

Abstract: Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states in the environment for temporal credit assignment. Extended evaluation on four BabyAI scenarios show that RICOL achieves comparable convergent performance with traditional online RL algorithms with significantly higher sample efficiency. Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms.

</details>


### [109] [LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights](https://arxiv.org/abs/2602.17510)
*Kasun Dewage,Marianna Pensky,Suranadi De Silva,Shankadeep Mondal*

Main category: cs.LG

TL;DR: CRAFT提出了一种创新的参数高效微调方法，通过对跨层3D预训练权重张量进行Tucker分解并冻结分解因子，仅训练少量方阵适配矩阵，在GLUE基准上实现与现有方法相当的性能，同时仅需41K参数。


<details>
  <summary>Details</summary>
Motivation: 当前张量基PEFT方法存在局限：LoTR和SuperLoRA分解梯度更新，PiSSA单层独立处理预训练权重。缺乏直接在跨层预训练权重上应用Tucker分解并冻结因子的方法。CRAFT旨在整合这两种研究思路，实现更优的参数效率。

Method: 方法采用高阶奇异值分解（HOSVD）对跨层堆叠的预训练注意力权重进行完整Tucker分解，冻结所有核心张量和因子矩阵，仅通过在每个因子矩阵上添加轻量级可训练变换矩阵来适配模型。

Result: 在GLUE基准测试中，基于RoBERTa-base和RoBERTa-large的实验表明，在固定Tucker秩下，CRAFT仅需41K可训练参数（独立于模型维度和深度），性能与现有先进方法具有竞争力。

Conclusion: CRAFT成功桥接了张量分解与跨层参数共享两个研究方向，提供了一种参数独立于模型规模的PEFT方案，显著提升了参数效率，为大规模模型高效微调提供了新思路。

Abstract: We introduce CRAFT (Cross-layer Rank Adaptation via Frozen Tucker), a parameter-efficient fine-tuning (PEFT) method that applies Tucker tensor decomposition to pre-trained attention weight matrices stacked across transformer layers and trains only small square adaptation matrices on the resulting frozen Tucker factors. Existing tensor-based PEFT methods decompose gradient updates: LoTR applies Tucker decomposition with shared factor matrices, while SuperLoRA groups and reshapes $ΔW$ across layers before applying Tucker decomposition. Separately, methods like PiSSA apply SVD to pre-trained weights but operate independently per layer. CRAFT bridges these two lines of work: it performs full Tucker decomposition via Higher-Order SVD (HOSVD) directly on pre-trained weights organized as cross-layer 3D tensors, freezes all resulting factors, and adapts the model through lightweight trainable transformations applied to each factor matrix. Experiments on the GLUE benchmark using RoBERTa-base and RoBERTa-large demonstrate that CRAFT achieves competitive performance with existing methods while requiring only 41K Tucker adaptation parameters--a count independent of model dimension and depth at fixed Tucker ranks.

</details>


### [110] [Variational inference via radial transport](https://arxiv.org/abs/2602.17525)
*Luca Ghafourpour,Sinho Chewi,Alessio Figalli,Aram-Alexandre Pooladian*

Main category: cs.LG

TL;DR: 针对变分推断中高斯分布径向轮廓失配问题，本文提出radVI算法，通过Wasserstein空间优化径向分布，以低成本插件形式增强现有方法并提供理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 变分推断常用高斯分布近似复杂目标分布，但其径向特性常与真实分布不符，导致覆盖不足和近似质量不佳，亟需更灵活的径向建模方法。

Method: 从径向轮廓优化角度重构VI问题，提出radVI算法，基于Wasserstein空间优化理论和Caffarelli型径向传输映射正则性，可嵌入高斯平均场VI和拉普拉斯近似等现有框架。

Result: 提供算法理论收敛保证，验证其作为现有VI方案廉价有效插件的可行性。

Conclusion: radVI革新了变分推断的优化视角，通过径向分布匹配为提升复杂分布近似质量提供了有理论支撑的通用增强方案。

Abstract: In variational inference (VI), the practitioner approximates a high-dimensional distribution $π$ with a simple surrogate one, often a (product) Gaussian distribution. However, in many cases of practical interest, Gaussian distributions might not capture the correct radial profile of $π$, resulting in poor coverage. In this work, we approach the VI problem from the perspective of optimizing over these radial profiles. Our algorithm radVI is a cheap, effective add-on to many existing VI schemes, such as Gaussian (mean-field) VI and Laplace approximation. We provide theoretical convergence guarantees for our algorithm, owing to recent developments in optimization over the Wasserstein space--the space of probability distributions endowed with the Wasserstein distance--and new regularity properties of radial transport maps in the style of Caffarelli (2000).

</details>


### [111] [Position: Evaluation of ECG Representations Must Be Fixed](https://arxiv.org/abs/2602.17531)
*Zachary Berger,Daniel Prakah-Asante,John Guttag,Collin M. Stultz*

Main category: cs.LG

TL;DR: 本文指出ECG表示学习领域过度依赖PTB-XL、CPSC2018、CSN三大多标签基准，其评估范围局限于心律失常和波形形态，未能反映ECG的广泛临床信息。研究证明采用多标签不平衡评估最佳实践会改变现有结论，且随机初始化编码器加线性评估可匹配SOTA预训练性能，强调需扩展评估至结构性心脏病和患者预测等新维度。


<details>
  <summary>Details</summary>
Motivation: 当前12导联ECG表示学习基准测试过度聚焦于心律失常和波形形态标签，忽略ECG编码的丰富临床信息，导致研究进展不可靠且与临床目标脱节，亟需重构评估体系。

Method: 提出多标签不平衡评估最佳实践；系统评估三种代表性预训练方法在六种场景（三标准基准、结构性心脏病、血流动力学推断、患者预测）的表现；引入随机编码器基线并验证其线性评估性能。

Result: 应用正确评估方法后，现有关于最优表示方法的结论发生改变；随机编码器基线在许多任务上性能媲美SOTA预训练模型；当前基准无法充分评估ECG表示的临床价值，需扩展至结构性心脏病和患者预测等新维度。

Conclusion: ECG表示学习领域必须改革基准测试实践，扩展评估范围至结构性心脏病、血流动力学推断和患者预测等临床相关目标，采用随机编码器作为基线，确保研究进展可靠且符合临床意义。

Abstract: This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model. We substantiate our observations with an empirical evaluation of three representative ECG pre-training approaches across six evaluation settings: the three standard benchmarks, a structural disease dataset, hemodynamic inference, and patient forecasting.

</details>


### [112] [MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning](https://arxiv.org/abs/2602.17550)
*Xiaoliang Fu,Jiaye Lin,Yangyi Fang,Binbin Zheng,Chaowen Hu,Zekai Shao,Cong Qin,Lu Pan,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 针对RLVR算法在LLM优化中的三大挑战，本文提出MASPO框架，通过软高斯门控、质量自适应限制器和非对称风险控制器实现性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法（如GRPO）采用刚性、统一、对称的信任区域机制，与LLM复杂优化动态严重失配，导致梯度利用效率低、概率质量不敏感、信号可靠性非对称三大关键问题，亟需更适配的优化框架。

Method: MASPO框架集成三大创新组件：1）可微分软高斯门控替代硬截断，提升梯度利用效率；2）质量自适应限制器根据令牌分布动态调节探索；3）非对称风险控制器按置信度对齐正负样本更新幅度。三组件协同实现多维度优化。

Result: 广泛评估显示，MASPO作为一体化RLVR解决方案具有强鲁棒性，显著超越现有强基线，在样本效率、训练稳定性和策略质量上均有明显提升。

Conclusion: MASPO通过系统性解决RLVR核心挑战，为LLM强化学习提供了更有效的优化范式，其统一框架设计对后续研究具有重要参考价值。

Abstract: Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.

</details>


### [113] [A Theoretical Framework for Modular Learning of Robust Generative Models](https://arxiv.org/abs/2602.17554)
*Corinna Cortes,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 提出模块化生成建模框架，通过门控机制组合预训练专家，解决LLM训练资源密集和启发式加权问题，理论证明鲁棒门控存在性，实验证实优于单模型基线


<details>
  <summary>Details</summary>
Motivation: 训练大规模生成模型资源消耗大且依赖启发式数据集加权，探讨能否通过组合小型领域专家模块化训练LLM以匹敌单模型性能，并对任意数据混合实现无需启发式调参的鲁棒训练

Method: 构建模块化生成建模理论框架：定义归一化门控函数空间G₁，形式化为极小极大博弈寻找对最差数据混合鲁棒的最优门控；利用角谷不动点定理证明鲁棒门控存在性，表明模块化起到强正则化作用，泛化界取决于轻量门控复杂度；理论证明模块化方法在Jensen-Shannon散度意义上可超越聚合数据重训练模型；提出可扩展的随机原始-对偶算法和结构蒸馏方法实现高效推理

Result: 在合成和真实数据集上的实证结果表明，该模块化架构有效缓解梯度冲突，并能鲁棒地超越单模型基线性能

Conclusion: 研究为模块化生成建模提供理论基础，证明通过组合领域专家实现鲁棒高效训练的可行性，为资源节约型大模型训练提供新范式

Abstract: Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence. Finally, we introduce a scalable Stochastic Primal-Dual algorithm and a Structural Distillation method for efficient inference. Empirical results on synthetic and real-world datasets confirm that our modular architecture effectively mitigates gradient conflict and can robustly outperform monolithic baselines.

</details>


### [114] [Revisiting Weight Regularization for Low-Rank Continual Learning](https://arxiv.org/abs/2602.17559)
*Yaoyue Zheng,Yin Zhang,Joost van de Weijer,Gido M van de Ven,Shaoyi Du,Xuetao Zhang,Zhiqiang Tian*

Main category: cs.LG

TL;DR: 该论文针对参数高效持续学习(PECL)中权重正则化技术(如EWC)应用不足的问题，提出了EWC-LoRA方法。通过低秩表示来估计全维度空间的参数重要性，并对共享的低秩更新进行EWC正则化，实现了在不增加存储和推理成本的前提下有效缓解任务干扰，在多个基准测试中展现出优于现有低秩持续学习方法的稳定性-可塑性平衡。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法中，权重正则化(如EWC)是缓解任务干扰的关键策略，但在新兴的参数高效持续学习(PECL)范式下，该方向尚未得到充分探索。尽管低秩适配器等方法能有效分离任务特定参数，但缺乏对共享参数空间的显式正则化约束，可能导致任务间知识遗忘。本研究旨在填补这一空白，从权重正则化的新视角重新审视低秩持续学习。

Method: 提出EWC-LoRA方法，核心创新在于：(1)采用低秩分解技术对预训练模型的参数更新进行压缩表示；(2)基于低秩子空间估计全参数空间的参数重要性，实现EWC正则化的可扩展应用；(3)仅对共享的低秩更新矩阵施加弹性权重约束，而非为每个任务维护独立的正则化项，从而保持存储和计算开销恒定。该方法将参数重要性计算与低秩适配器的训练相结合，在保持参数效率的同时实现持续学习。

Result: 在多个持续学习基准数据集上进行的广泛实验表明，EWC-LoRA能够有效平衡稳定性与可塑性，显著优于现有低秩持续学习方法。关键优势体现在：任务增量学习过程中遗忘率降低，同时保持对新任务的学习能力；存储需求不随任务数量增长；推理延迟保持稳定。实验结果验证了即使在低秩参数化约束下，权重正则化仍是缓解任务干扰的有效机制。

Conclusion: 本研究证实了在PECL框架下重新引入权重正则化的重要价值，EWC-LoRA为大规模预训练模型的持续学习提供了实用且高效的解决方案。该方法不仅拓展了正则化技术在参数高效场景下的应用范围，也为未来设计更鲁棒的持续学习算法提供了重要启示：低秩参数化与权重正则化的结合是实现可扩展持续学习的可行路径。

Abstract: Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)-a key strategy in CL-remain underexplored in this new paradigm. In this paper, we revisit weight regularization in low-rank CL as a new perspective for mitigating task interference in PECL. Unlike existing low-rank CL methods, we mitigate task interference by regularizing a shared low-rank update through EWC, thereby keeping the storage requirement and inference costs constant regardless of the number of tasks. Our proposed method EWC-LoRA leverages a low-rank representation to estimate parameter importance over the full-dimensional space. This design offers a practical, computational- and memory-efficient solution for CL with PTMs, and provides insights that may inform the broader application of regularization techniques within PECL. Extensive experiments on various benchmarks demonstrate the effectiveness of EWC-LoRA, achieving a stability-plasticity trade-off superior to existing low-rank CL approaches. These results indicate that, even under low-rank parameterizations, weight regularization remains an effective mechanism for mitigating task interference. Code is available at: https://github.com/yaoyz96/low-rank-cl.

</details>


### [115] [Be Wary of Your Time Series Preprocessing](https://arxiv.org/abs/2602.17568)
*Sofiane Ennadir,Tianze Wang,Oleg Smirnov,Sahar Asadi,Lele Cao*

Main category: cs.LG

TL;DR: 这篇论文首次从理论角度分析了归一化策略（实例级和全局缩放）如何影响基于Transformer的时间序列模型的表达能力，提出了新的表达能力框架，推导出标准化和最小-最大缩放的理论边界，并通过实验验证发现没有单一的归一化方法始终最优，有时不使用归一化效果更好。


<details>
  <summary>Details</summary>
Motivation: 时间序列建模中，归一化和缩放是基础预处理步骤，但其在基于Transformer模型中的作用缺乏理论层面的深入探索。现有研究未能系统分析不同归一化策略如何影响模型的表示能力，这限制了对模型行为的理解和优化。

Method: 作者提出了专门针对时间序列的表达能力框架，用于量化模型在表示空间中区分相似与不同输入的能力。基于该框架，推导出两种广泛使用的归一化方法（标准化和最小-最大缩放）的理论边界。通过分类和预测基准测试进行实证验证，使用多种基于Transformer的模型。

Result: 理论分析表明归一化策略的选择会根据任务和数据特征显著影响模型的表示容量。实验结果显示，没有单一的归一化方法在所有情况下都表现最优，在某些情况下完全不使用归一化反而能获得更好的性能。

Conclusion: 研究揭示了预处理在时间序列学习中的关键作用，强调了当前归一化实践的不足，并呼吁开发更多针对特定任务和数据集的原则性归一化策略。这为理解Transformer时间序列模型的行为提供了理论基础，并指导未来更智能的预处理方法设计。

Abstract: Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets.

</details>


### [116] [Canonicalizing Multimodal Contrastive Representation Learning](https://arxiv.org/abs/2602.17584)
*Sharut Gupta,Sanyam Kansal,Stefanie Jegelka,Phillip Isola,Vikas Garg*

Main category: cs.LG

TL;DR: 本研究揭示，在不同架构和数据集上独立训练的多模态对比学习模型（如CLIP、SigLIP、FLAVA）之间存在统一的几何关系：可通过一个正交矩阵Q（附加全局均值平移）将一个模型的编码器映射到另一个模型的空间，且该Q同时适用于图像和文本编码器。理论证明此关系成立的条件是模型在少量锚点上保持多模态核一致。该发现支持后向兼容的模型升级，并触及学习表示的隐私问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模模型和数据导致独立训练的 network 产生相似性概念，但跨多模态模型建立明确的表示空间对应关系尤为关键，因其需同时保持模态内一致性和图像-文本跨模态耦合。为此，研究者探究两个独立训练、不同架构和数据分布的多模态对比模型间是否存在系统性几何关系，以及该关系是否跨模态通用。

Method: 研究结合理论分析与跨模型家族（CLIP、SigLIP、FLAVA）的实证检验。理论上，证明若模型在少量锚点集上多模态核（⟨f(x), g(y)⟩）近似相等，则可推导出单一正交映射关系。

Result: 实证发现，独立训练模型间的嵌入空间几何关系可由正交映射Q（满足Q^T Q = I）加全局均值平移精确近似。关键的是，同一个Q矩阵能同时对齐图像编码器（f̃(x) ≈ Q f(x)）和文本编码器（g̃(y) ≈ Q g(y)）。

Conclusion: 此发现使模型可进行后向兼容升级，避免昂贵的重新嵌入，并对学习表示的隐私性有深远影响。

Abstract: As models and data scale, independently trained networks often induce analogous notions of similarity. But, matching similarities is weaker than establishing an explicit correspondence between the representation spaces, especially for multimodal models, where consistency must hold not only within each modality, but also for the learned image-text coupling. We therefore ask: given two independently trained multimodal contrastive models (with encoders $(f, g)$ and $(\widetilde{f},\widetilde{g})$) -- trained on different distributions and with different architectures -- does a systematic geometric relationship exist between their embedding spaces? If so, what form does it take, and does it hold uniformly across modalities? In this work, we show that across model families such as CLIP, SigLIP, and FLAVA, this geometric relationship is well approximated by an orthogonal map (up to a global mean shift), i.e., there exists an orthogonal map $Q$ where $Q^\top Q = I$ such that $\widetilde{f}(x)\approx Q f(x)$ for paired images $x$. Strikingly, the same $Q$ simultaneously aligns the text encoders i.e., $\widetilde{g}(y)\approx Q g(y)$ for texts $y$. Theoretically, we prove that if the multimodal kernel agrees across models on a small anchor set i.e. $\langle f(x), g(y)\rangle \approx \langle \widetilde{f}(x), \widetilde{g}(y)\rangle$, then the two models must be related by a single orthogonal map $Q$ and the same $Q$ maps images and text across models. More broadly, this finding enables backward-compatible model upgrades, avoiding costly re-embedding, and has implications for the privacy of learned representations.
  Our project page: https://canonical-multimodal.github.io/

</details>


### [117] [Towards Anytime-Valid Statistical Watermarking](https://arxiv.org/abs/2602.17608)
*Baihe Huang,Eric Xu,Kannan Ramchandran,Jiantao Jiao,Michael I. Jordan*

Main category: cs.LG

TL;DR: 本文提出首个基于e值的水印框架Anchored E-Watermarking，通过构建检验上鞅实现随时有效的机器文本检测。该方法利用锚定分布逼近目标模型，优化采样策略和停止时间，相比现有技术可将检测所需的平均token预算降低13-15%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的普及使得区分机器生成文本与人类文本变得至关重要。现有统计水印方法存在两大缺陷：采样分布选择缺乏理论指导，以及固定时域假设检验无法支持有效提前停止，这限制了检测效率。

Method: 开发基于e值的水印框架，构造检测过程的上鞅以实现随时有效推断。通过锚定分布近似目标模型，基于最差对数增长率推导最优e值及最优期望停止时间，统一最优采样与随时有效检验。

Result: 理论分析与实验验证表明，该框架具有统计有效性。在标准基准测试中，相比现有最优方法，平均token预算减少13-15%，显著提升检测效率。

Conclusion: Anchored E-Watermarking为LLM生成内容检测提供了首个e值驱动的随时有效框架，通过鞅论方法解决采样优化与提前停止的统计难题，实现了更高效、理论完备的机器文本识别。

Abstract: The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.

</details>


### [118] [Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning](https://arxiv.org/abs/2602.17614)
*Obaidullah Zaland,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 该论文针对U型联邦分割学习(UFSL)中 smashed data 易泄露客户端隐私数据的问题，提出了一种融合微聚合与差分隐私的k-匿名UFSL方案(KD-UFSL)，通过抵御数据重构攻击，在保持模型效用性的同时显著提升隐私保护强度。


<details>
  <summary>Details</summary>
Motivation: 在大数据场景下，UFSL虽能将部分计算负担卸载至服务器，但客户端上传的中间表征(smashed data)存在隐私泄露风险。攻击者可通过数据重构攻击从这些中间表示中恢复原始私有数据。因此需要在保持去中心化训练优势的同时，解决中间数据传输过程中的隐私暴露问题。

Method: 提出KD-UFSL框架，将k-匿名微聚合机制与差分隐私技术集成到UFSL中。具体包括：(1)对客户端生成的 smashed data 进行微聚合处理以满足k-匿名性；(2)在聚合后的数据上施加差分隐私噪声；(3)服务器仅处理经隐私增强的中间表示，而原始数据和标签始终保留在客户端。

Result: 在四个基准数据集上的实验表明：与原始UFSL相比，KD-UFSL使原始图像与重构图像之间的均方误差最高提升50%，结构相似性最高降低40%，显著削弱了攻击者的重构能力。更重要的是，该方案在增强隐私保护的同时，维持了全局模型的预测效用性。

Conclusion: KD-UFSL有效平衡了大数据应用中隐私保护与模型效用的双重需求，为大规模分布式机器学习提供了可扩展的隐私增强解决方案，特别适用于计算资源受限且对隐私要求严苛的联邦学习场景。

Abstract: Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients' side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients' private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.

</details>


### [119] [Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs](https://arxiv.org/abs/2602.17616)
*Luke Huang,Zhuoyang Zhang,Qinghao Hu,Shang Yang,Song Han*

Main category: cs.LG

TL;DR: 该论文针对异步强化学习训练中的高方差问题，提出了VCPO方法，通过基于有效样本量调整学习率和应用闭式最小方差基线来稳定REINFORCE/GRPO算法，在数学推理和工具使用任务上显著提升鲁棒性，并将训练时间减少2.5倍。


<details>
  <summary>Details</summary>
Motivation: 异步强化学习训练虽能提高吞吐量，但对于REINFORCE和GRPO等无评论家策略梯度方法，高异步性会导致策略梯度估计器方差显著增加。过时的采样产生重尾重要性比率，使少量样本主导更新，导致梯度噪声大、学习不稳定。现有方法缺乏对策略梯度方差的有效控制。

Method: VCPO（方差控制策略优化）是一种通用稳定化方法：(i) 根据有效样本量（ESS）动态缩放学习率，抑制不可靠更新；(ii) 为离线策略设置应用闭式最小方差基线，避免引入辅助价值模型，仅增加极小开销。该方法适用于REINFORCE/GRPO类算法。

Result: 在数学、通用推理和工具使用任务上，VCPO显著提升了异步训练的鲁棒性，优于多种基线方法（包括掩码/裁剪稳定器和算法变体）。实验表明，训练崩溃可通过有效样本size和不稳定梯度范数可靠预测。VCPO在保持同步性能的同时，将长上下文多轮训练时间减少了2.5倍。

Conclusion: 显式控制策略梯度方差是大规模可靠异步强化学习的关键。VCPO通过有效样本量感知的学习率缩放和最小方差基线，成功解决了异步训练中的高方差问题，为实现高效可扩展的RL训练提供了有效方案。

Abstract: Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.

</details>


### [120] [Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning](https://arxiv.org/abs/2602.17625)
*Obaidullah Zaland,Zulfiqar Ahmad Khan,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 提出一次性增量联邦学习框架OSI-FL，通过单次通信的类别嵌入与选择性样本保留策略，解决联邦学习中的通信开销和灾难性遗忘问题，在三个基准数据集上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大数据系统产生大规模、异构、地理分散且隐私敏感的数据流，中央化处理困难。传统联邦学习假设数据静态且需多轮通信，在增量数据场景下通信受限且面临灾难性遗忘，现有方法无法同时应对这两大挑战。

Method: OSI-FL框架：1) 客户端用冻结视觉语言模型生成类别特定嵌入，单次通信上传；2) 服务器端预训练扩散模型根据嵌入合成新数据用于训练；3) 引入选择性样本保留(SSR)策略，基于样本损失保留每类别-任务对中top-p最信息丰富的样本，在后续迭代中约束灾难性遗忘。

Result: 在三个基准数据集上的实验表明，OSI-FL在类别增量和领域增量场景下均优于传统和一次性联邦学习基线方法。

Conclusion: OSI-FL为增量联邦学习提供了有效解决方案，通过单次通信与样本保留机制平衡了隐私保护、通信效率与持续学习能力，在资源受限场景具有重要应用价值。

Abstract: Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.

</details>


### [121] [When to Trust the Cheap Check: Weak and Strong Verification for Reasoning](https://arxiv.org/abs/2602.17633)
*Shayan Kiyani,Sima Noorani,George Pappas,Hamed Hassani*

Main category: cs.LG

TL;DR: 该论文形式化了大语言模型推理中的弱验证（内部廉价检查）与强验证（外部人工反馈）间的成本-可靠性权衡，提出双阈值验证策略框架并开发可证明控制错误率的在线算法，无需对查询流、模型或验证器作任何假设。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理日益依赖多层级验证机制，但内部自洽性检查等弱验证虽快速可扩展却噪声大，外部人工验证虽可靠却资源密集。现有研究缺乏对这一根本张力的理论刻画，无法在有限资源下优化验证策略，导致实践中常出现验证不足或过度验证的问题。

Method: 通过弱-强验证策略形式化框架，定义错误接受、错误拒绝和强验证频率三项核心指标。基于群体数据分析，证明最优策略具有双阈值结构；理论揭示校准性与锐度决定弱验证器效用。进一步提出无分布假设的在线算法，动态决定何时接受、拒绝或委托强验证。

Result: 理论证明最优验证策略存在双阈值决策边界，弱验证器的价值由其校准程度和锐度（预测置信度分布）主导。所提在线算法在任意查询流下均能严格约束接受错误率与拒绝错误率，实现强验证调用的最小化与可靠性保证的帕累托最优。

Conclusion: 研究建立了弱-强验证权衡的统计学基础，为资源约束下部署可信LLM系统提供了可证明安全的自适应验证框架。双阈值结构与在线算法显著提升了验证效率，在保持输出可靠性的同时最小化昂贵的人工验证成本，对工业级LLM应用具有重要指导意义。

Abstract: Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.

</details>


### [122] [Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting](https://arxiv.org/abs/2602.17634)
*Xinghong Fu,Yanhong Li,Georgios Papaioannou,Yoon Kim*

Main category: cs.LG

TL;DR: 本文提出了一种构建高效时间序列基础模型的简单方案，采用长卷积与线性RNN层（DeltaNet）的混合架构替代大型Transformer，模型规模缩小百倍以上而性能相当，并结合数据增强与推理策略推出Reverso系列模型，显著推进了性能-效率帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型过度依赖参数扩展（达数百兆），虽性能优异但实际部署效率低、成本高。为此，作者寻求一种高效替代方案，在保持零样本预测能力的同时大幅降低模型复杂度。

Method: 核心方法是设计小型混合架构，通过交错堆叠长卷积层与DeltaNet等线性RNN层构建模型，摒弃传统Transformer的规模依赖。同时引入数据增强与推理优化策略以进一步提升预测性能。

Result: 实验表明，该混合架构在性能上与大型Transformer模型相当，但参数量减少超过两个数量级（>100倍）。基于此方案开发的Reverso模型家族显著优化了性能与效率的权衡关系。

Conclusion: 研究表明，通过简单架构创新（混合卷积与DeltaNet）配合训练推理策略，可高效构建适用于零样本预测的时间序列基础模型，为实际应用提供了更优解。

Abstract: Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.

</details>


### [123] [FAMOSE: A ReAct Approach to Automated Feature Discovery](https://arxiv.org/abs/2602.17641)
*Keith Burghardt,Jienan Liu,Sadman Sakib,Yuning Hao,Bo Li*

Main category: cs.LG

TL;DR: 该论文提出FAMOSE框架，首次将ReAct智能体范式应用于自动化特征工程。该框架通过LLM自主探索、生成和优化特征，在分类任务（特别是10K+样本任务，ROC-AUC提升0.23%）和回归任务（RMSE降低2.0%）上均达到或超越SOTA水平，同时展现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 特征工程仍是机器学习的关键瓶颈，尤其在表格数据中，从指数级特征空间识别最优特征传统上依赖大量领域专业知识，耗时且效率低下。

Method: 提出FAMOSE（Feature AugMentation and Optimal Selection agEnt）框架，基于ReAct范式构建智能体架构，自主执行特征探索、生成与优化，并内嵌特征选择与评估工具。通过迭代记录特征效果至LLM上下文窗口，实现类小样本学习的特征创新。

Result: 实验表明，FAMOSE在分类任务上达到或接近SOTA（特别是超过10K实例的任务，ROC-AUC平均提升0.23%），在回归任务上达到SOTA（RMSE平均降低2.0%），且误差鲁棒性优于其他算法。

Conclusion: FAMOSE的优异性能源于ReAct范式使LLM能记录迭代过程中特征的有效性，形成动态知识库指导特征创新。研究表明AI智能体在需要高度创造性的问题（如特征工程）中具有显著优势。

Abstract: Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.

</details>


### [124] [A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning](https://arxiv.org/abs/2602.17642)
*Dhruv Talwar,Harsh Desai,Wendong Yin,Goutam Mohanty,Rafael Reveles*

Main category: cs.LG

TL;DR: 该论文提出了一种名为A.R.I.S.的自动化电子垃圾识别系统，这是一个低成本、便携式分选设备，用于解决传统电子垃圾回收过程中因材料分离和识别能力不足导致的资源损失问题。系统采用YOLOx模型实时分类金属、塑料和电路板，实现了高检测精度和低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统电子垃圾回收过程因材料分离和识别能力不足，导致显著的資源損失，限制了材料回收率。这促使作者开发一种能够提升分拣效率、降低先进回收技术采用门槛的解决方案，以支持产品生命周期延长、 trade-in和回收计划，并减少供应链对环境的影响。

Method: 本研究设计并实现了一个名为A.R.I.S.（自动化回收识别系统）的低成本、便携式分选系统。该系统核心技术是集成深度学习模型YOLOx，用于对破碎的电子废弃物（金属、塑料、电路板）进行实时分类。系统将深度学习与既定分选方法相结合。

Result: 实验评估表明，A.R.I.S.系统达到了90%的综合精度（overall precision）、82.2%的平均精度均值（mAP）以及84%的分选纯度（sortation purity）。系统在保持高检测精度的同时，实现了较低的推理延迟。

Conclusion: A.R.I.S.系统通过集成深度学习与现有分选方法，有效提升了材料回收效率，降低了先进回收技术的采用门槛。该工作有助于减少电子废弃物带来的环境影响，支持循环经济倡议，延长产品生命周期，并促进供应链的可持续发展。

Abstract: Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.

</details>


### [125] [Multi-Round Human-AI Collaboration with User-Specified Requirements](https://arxiv.org/abs/2602.17646)
*Sima Noorani,Shayan Kiyani,Hamed Hassani,George Pappas*

Main category: cs.LG

TL;DR: 提出基于反事实损害与互补性原则的人本AI协作框架，通过用户自定义规则和在线无分布算法，在不建模人类行为的前提下提升高风险多轮决策质量。


<details>
  <summary>Details</summary>
Motivation: 人类在高风险决策中日益依赖多轮对话AI，亟需原则性框架确保AI协作可靠提升决策质量，避免损害人类优势。

Method: 将反事实损害与互补性原则形式化为用户自定义规则，设计具有有限样本保证的在线无分布算法，强制约束协作动态。

Result: 在医疗诊断与图形推理任务中验证：算法在非平稳交互下维持预设的约束违反率，调整约束强度可预测地改变人类决策准确率。

Conclusion: 该框架将两个原则转化为实用调控杠杆，无需建模人类行为即可引导多轮协作提升决策质量，为负责任AI协作提供了可调可控的解决方案。

Abstract: As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.

</details>


### [126] [MARS: Margin-Aware Reward-Modeling with Self-Refinement](https://arxiv.org/abs/2602.17658)
*Payel Bhattacharjee,Osvaldo Simeone,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文针对奖励模型训练中人工标注数据昂贵且稀缺的问题，提出MARS——一种自适应的边缘感知增强与采样策略。该方法聚焦于模型不确定性高的低边缘模糊偏好对，通过迭代式困难样本挖掘优化训练分布，理论证明可提升损失函数曲率与信息量，实证显示相较均匀增强有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现代AI对齐流程（如RLHF、RLAIF）和策略优化算法（如PPO、TRPO）的核心组件是奖励模型。然而，可靠奖励模型的训练严重依赖人工标注的偏好数据，该数据成本高昂且获取困难。现有数据增强方法多停留在表示或语义层面，且对奖励模型的估计难度不敏感，未能针对性解决模型模糊性问题。

Method: 提出MARS框架，采用自适应、边缘感知的增强策略，明确针对奖励模型的模糊边界和失败模式。具体而言，该方法优先对低边缘（即模型判别模糊）的偏好样本对进行增强，并通过迭代方式持续挖掘困难样本以精炼训练分布。

Result: 理论分析表明，该策略通过增加损失函数的平均曲率来增强信息含量并改善优化条件数；实验验证在鲁棒奖励建模任务上，相比均匀增强方法取得了一致的、显著的增益效果。

Conclusion: MARS通过智能地聚焦奖励模型的困难样本和不确定性区域进行自适应增强，有效缓解了标注数据稀缺性挑战，提升了奖励模型的鲁棒性，为高效对齐训练提供了新范式。

Abstract: Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [127] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: 针对法医牙齿年龄评估存在的方法异质性、数据碎片化及系统互操作性不足等问题，本研究提出AIdentifyAGE本体，该本体整合人工与AI辅助工作流程，通过标准化语义框架实现观察、方法、参考数据与结果的可追溯链接，为提升法医司法决策的透明度和可重复性提供符合FAIR原则的基础设施。


<details>
  <summary>Details</summary>
Motivation: 在法医司法实践中，年龄评估对无证人员与未成年人的法律保护、医疗及司法程序准入具有决定性作用。牙齿年龄评估虽是最可靠的生物学方法之一，但面临方法学异质性、数据表示碎片化、临床与法医法律信息系统间互操作性有限等挑战，且AI方法的引入进一步放大了透明度与可重复性问题。

Method: 采用与领域专家协作的开发模式，基于现有生物医学、牙科及机器学习上层本体，构建领域特定的AIdentifyAGE本体。该本体完整建模法医司法工作流程，涵盖司法背景、个体信息、法医检查数据、牙齿发育评估方法、放射影像、统计参考研究及AI估计算法等核心要素。

Result: 成功开发了一个标准化、语义一致的框架，支持人工与AI辅助的牙齿年龄评估双路径工作流程，实现观察、方法、参考数据与报告结果间的可追溯链接，确保系统互操作性、可扩展性并符合FAIR数据原则。

Conclusion: AIdentifyAGE本体是提升法医牙齿年龄评估一致性、透明度和可解释性的重要基础，为构建法医司法领域的本体驱动决策支持系统提供了坚实框架，有助于推动该领域向更规范、更可靠的方向发展。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [128] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 该研究揭示了在资源受限的自适应系统中，单状态重用必然产生经典概率框架下的上下文性，这种上下文性无法仅通过内部状态中介，必须付出信息论代价，而非经典框架通过放弃单一全局联合概率空间假设来规避此限制。


<details>
  <summary>Details</summary>
Motivation: 自适应系统由于内存、表示或物理资源限制，常在多上下文中重用固定内部状态空间。这种单状态重用现象在自然与人工智能中普遍存在，但其基本表示性后果尚未被充分理解。

Method: 将上下文建模为对共享内部状态的干预，利用信息论方法证明任何经典模型在复现上下文结果统计时都必须承担不可约的信息论代价。

Result: 证明了在经典概率表示中，上下文依赖性无法仅通过内部状态中介；提供了最小构造性示例并阐明其操作意义；解释了非经典框架如何避免此限制。

Conclusion: 上下文性是对自适应智能的普遍表示性约束，独立于物理实现；该发现将量子力学中的上下文性推广至经典自适应系统。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [129] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为MobCache的移动性感知缓存框架，用于高效的大规模人类移动性模拟。该框架通过将推理步骤编码为潜在空间嵌入并采用轻量级解码器，在保持与最新大语言模型方法相当性能的同时，显著提高了模拟效率。


<details>
  <summary>Details</summary>
Motivation: 大规模人类移动性模拟对城市规划、流行病学和交通分析等应用至关重要。然而，现有方法将大语言模型作为人类智能体进行模拟时存在计算成本高昂的问题，严重限制了其可扩展性。

Method: 该论文设计了MobCache框架，包含两个核心组件：（1）推理组件，将每个推理步骤编码为潜在空间嵌入，并使用潜在空间评估器实现推理步骤的重用和重组；（2）解码组件，采用轻量级解码器，通过移动性法则约束的知识蒸馏训练，将潜在空间推理链转换为自然语言。

Result: 实验表明，MobCache在多个维度上显著提高了模拟效率，同时保持了与当前最优大语言模型方法相当的性能水平。

Conclusion: MobCache通过可重构缓存机制有效解决了大规模人类移动性模拟的计算效率瓶颈，在保持高保真度的前提下实现了效率的显著提升，为相关领域的研究和实践提供了高效的解决方案。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [130] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 本研究系统分析了60个LLM基准测试的饱和现象，发现近半数基准会随时间推移而饱和，无法有效区分顶尖模型。通过分析14个基准属性，研究揭示隐藏测试数据不能防止饱和，而专家策划的基准比众包基准更具持久性，为设计更耐用的评估体系提供了实证依据。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署中至关重要，但许多基准快速饱和，失去区分最佳模型的能力，削弱了其长期价值。为识别导致饱和的驱动因素并延长基准寿命，亟需系统分析基准设计属性与饱和率之间的关系。

Method: 研究人员从主要模型开发商的技术报告中选取60个LLM基准，从任务设计、数据构建和评估格式三方面刻画其14个属性，并检验了五个关于这些属性如何影响饱和率的假设。

Result: 分析显示近半数基准存在饱和问题，饱和率随基准年龄增长而上升。隐藏测试数据并无防护效果，而专家策划的基准比众包基准更能抵抗饱和。研究明确了能延长基准寿命的关键设计选择。

Conclusion: 该研究揭示了基准设计属性对持久性的影响机制，为构建更耐用的评估体系提供了策略指导。研究建议优先采用专家策划方式，并质疑了隐藏测试数据的有效性，对未来的基准设计具有重要参考价值。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [131] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 本文通过在三类任务（数学边界优化、智能体框架设计、机器学习竞赛）上测试简单基线方法，发现其性能可媲美甚至超越复杂的代码演化管道，揭示了当前代码演化技术在开发评估中存在的方法论缺陷，并提出了改进评估方式和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有代码演化研究常展示 impressive 性能，却缺乏与简单基线的对比，导致对其真实效果和适用性的认知不足。

Method: 研究采用两种简单基线方法，在数学边界搜索、智能体框架设计和机器学习竞赛三个领域进行系统性测试，并与复杂演化管道进行对比分析。

Result: 实验表明简单基线在所有领域均达到或超越复杂方法。进一步分析发现：（1）数学边界搜索中，搜索空间和提示工程起主导作用，演化管道仅为次要因素；（2）智能体框架设计中，高方差与小数据集导致选择偏差，手工多数投票框架最优；（3）现有评估方法存在较大随机性，影响可靠性。

Conclusion: 代码演化面临的核心挑战是搜索空间设计而非搜索过程本身，需要领域专家深度参与。研究建议开发更稳定、经济的评估方法，并建立更严谨的研究规范以提升代码演化技术的实际价值。

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [132] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 本文改进了n维超立方体所有边被超平面切割所需最少数量S(n)的上界，证明S(n) ≤ ⌈4n/5⌉（n为5的奇数倍时S(n) ≤ 4n/5+1），并利用CPro1自动化工具构造了切割Q_10的8个超平面方案。


<details>
  <summary>Details</summary>
Motivation: 超平面切割超立方体边的问题是离散几何与理论计算机科学中的基本问题。Paterson于1971年提出的5n/6上界长期未获改进，突破这一界限对于深入理解高维立方体的组合几何结构具有重要理论价值。

Method: 采用构造性证明策略，借助CPro1自动化系统（整合推理大语言模型与自动超参数调优技术）搜索数学构造，先获得切割10维超立方体的8个超平面具体配置，再推广至一般维度的理论界。

Result: 主要结果：1) 上界改进为S(n) ≤ ⌈4n/5⌉（n=5(2m+1)时S(n) ≤ 4n/5+1）；2) 获得k<n时最多切割边数的新下界；3) 明确Q_10可用8个超平面完全切割（因4×10/5=8）。

Conclusion: 通过AI辅助的自动化推理方法，本研究成功突破了存在50余年的理论界限，不仅提供了更紧的上界，也为组合几何问题的研究开辟了新的人机协作范式。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [133] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent是一个受管的AI工作流系统，用于中子源晶体学分析，通过限制工具使用、强制验证关卡和完整溯源，将人工435分钟的分析时间缩短至约90分钟，实现4.6-5.0倍加速，同时生成无checkCIF警告的发表级晶体结构文件。


<details>
  <summary>Details</summary>
Motivation: 大规模科学设施面临数据分析和报告延迟的瓶颈，特别是对于结构和磁性复杂的样品，需要迭代的数据还原、积分、精修和验证过程。传统手动流程耗时长达7小时以上，严重制约科学产出效率。

Method: 开发NeuDiff Agent，一个受控的AI智能体工作流系统。核心治理机制包括：1)限制操作仅限白名单工具；2)在关键工作流边界设置故障关闭验证关卡；3)捕获完整溯源信息。使用固定提示协议和双大语言模型后端进行重复端到端测试，量化用户/机器时间分配、干预负担和恢复行为。

Result: 在基准测试案例中，NeuDiff Agent将总耗时从人工的435分钟大幅缩短至86.5±4.7分钟和94.4±3.5分钟，加速比达4.6-5.0倍。生成的晶体信息文件(CIF)通过严格验证，未出现checkCIF的A级或B级警告。

Conclusion: 该研究证明了受管AI智能体在大型科学设施晶体学中部署的可行性，在保持可溯源性和发表级验证要求的同时，显著提高了分析效率，为科学数据处理的自动化提供了实用路径。

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [134] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 本文提出节点学习这一去中心化边缘智能范式，通过本地持续学习、选择性对等交互和扩散传播机制，解决集中式方法的传输、延迟、能耗和扩展瓶颈。


<details>
  <summary>Details</summary>
Motivation: 边缘AI扩展暴露了集中式智能的瓶颈：数据传输开销大、延迟高、能耗高、依赖大型数据中心，且在异构、移动和资源受限环境中难以扩展。

Method: 节点学习范式使边缘节点能够基于本地数据持续学习并维护独立模型状态，仅在协作有益时进行选择性对等交互，通过重叠和扩散而非全局同步实现知识传播，统一自治与协作行为并适应异构环境。

Result: 该概念论文建立了节点学习的理论基础，对比了现有去中心化方法，并探讨了其在通信、硬件、信任和治理方面的影响，将现有范式置于更广泛的去中心化视角中。

Conclusion: 节点学习为边缘智能提供了一种新的去中心化范式，通过本地化学习和选择性协作有效解决了集中式方法的瓶颈问题，适应异构环境，并为未来研究提供了理论框架。

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [135] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文针对犹豫模糊集传统评分方法缺乏序理论基础的问题，提出统一框架将评分明确关联于特定序关系。研究发现经典序不形成格结构，而对称序下的评分满足强单调性和Gärdenfors条件等规范标准。文章进一步提出优势函数类用于排序，通过控制集和最小可接受阈值比较犹豫模糊元，给出离散和相对优势函数两个实例，并应用于构建模糊偏好关系和支持群决策。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论形式基础，导致机制不够灵活一致。文献中关于经典序诱导格结构的断言存在疑问。为建立更严谨可靠的评分体系，需从序关系视角重构框架，并引入最小可接受阈值等决策实用概念。

Method: 1) 构建序导向的统一评分框架，明确定义评分函数与序的关系；2) 分析几种经典犹豫模糊元序结构，验证其格性质；3) 证明对称序下评分满足强单调性（关于并集）和Gärdenfors条件；4) 提出优势函数类，通过含最小可接受阈值的控制集比较元素；5) 实例化离散优势函数和相对优势函数。

Result: 1) 经典犹豫模糊元序关系不形成格结构，与先前观点相悖；2) 对称序下评分函数满足关键规范标准；3) 成功定义优势函数类并给出两个具体实例；4) 证明优势函数可构建犹豫模糊集上的模糊偏好关系；5) 验证该框架支持群决策过程。

Conclusion: 本研究通过序理论视角为犹豫模糊集评分奠定更严谨基础。优势函数框架引入最小可接受阈值，增强决策灵活性和实用性。该理论不仅纠正了关于格结构的错误认知，还提供了构建模糊偏好关系和群决策支持系统的新工具，为处理犹豫模糊信息提供了有效方法论。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [136] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5是多尺寸（2B-235B）、多平台（桌面/移动/浏览器）原生GUI智能体模型，通过混合数据飞轮、统一智能体能力增强和多平台RL算法MRPO实现云边协同与实时交互，在20+GUI基准测试中达到开源模型SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体在跨桌面、移动和浏览器等多平台部署时面临训练冲突、长时任务效率低、工具调用和记忆能力不足等挑战。本文旨在开发支持云边协同的多尺寸原生模型，通过高效数据收集和算法创新提升多平台GUI自动化、工具使用和知识记忆等核心能力。

Method: 1）混合数据飞轮：结合模拟环境与云沙盒环境构建UI理解和轨迹生成数据管道，提升数据收集效率与质量；2）统一智能体能力增强：通过思维合成管道增强推理能力，重点优化工具/MCP使用、记忆和多智能体适应能力；3）多平台环境RL扩展：提出MRPO算法解决多平台冲突和长时任务训练效率问题。

Result: 在20+开源GUI基准测试中达到SOTA：GUI自动化（OSWorld 56.5、AndroidWorld 71.6、WebArena 48.4）；grounding（ScreenSpotPro 80.3）；工具调用（OSWorld-MCP 47.6、MobileWorld 46.8）；记忆与知识（GUI-Knowledge Bench 75.5）。

Conclusion: GUI-Owl-1.5模型已开源并提供在线云沙盒演示，为多平台GUI智能体研究提供了高性能基础模型和实践方案，推动了云边协同GUI智能体的发展。

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [137] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: 提出OpenSage，首个支持LLM自动生成智能体拓扑、工具集并提供结构化记忆的ADK，通过自生成机制和图记忆系统实现AI驱动的智能体开发，在三个基准测试中优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有ADK功能支持不足或依赖人工设计智能体拓扑、工具与记忆，限制了智能体的泛化性与性能。需要从以人为中心的开发模式转向以AI为中心的自动化范式。

Method: 设计OpenSage框架，核心包括：1) LLM驱动的自生成拓扑与工具集机制，支持智能体自主创建管理子智能体与工具包；2) 层次化图记忆系统实现高效记忆管理；3) 面向软件工程的专用工具包。

Result: 在三个SOTA基准测试及多种骨干模型上的实验表明，OpenSage显著优于现有ADK。消融研究验证了各组件设计的有效性。

Conclusion: OpenSage为下一代智能体开发提供新范式，推动从人工设计向AI自主设计的转变，具有重要前景。

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [138] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: 本文提出AgentLAB，首个专门评估LLM智能体在自适应长时程攻击下脆弱性的基准测试平台，涵盖5种新型攻击类型、28种真实智能体环境和644个安全测试用例，揭示当前智能体对长时程攻击高度脆弱且单轮防御机制失效。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长时程、复杂环境中的部署日益广泛，其面临的多轮用户-智能体-环境交互攻击风险显著增加，而现有仅针对单轮交互的防御机制无法有效应对此类长时程威胁，亟需系统性评估工具来测量和追踪智能体安全漏洞。

Method: 研究团队构建了AgentLAB基准测试框架，设计了意图劫持、工具链操控、任务注入、目标漂移和记忆污染五种新型长时程攻击范式，覆盖28个真实智能体应用场景，共包含644个安全测试用例，通过系统化实验评估代表性LLM智能体的安全性能。

Result: 实验结果表明，现有LLM智能体对长时程攻击表现出高度脆弱性，且专门为单轮交互设计的防御策略无法可靠缓解长时程威胁，安全性能存在显著差距。

Conclusion: AgentLAB为衡量和追踪实用场景下LLM智能体安全防护进展提供了重要基准工具，将推动智能体安全研究的发展；该基准已开源以促进社区协作。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [139] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: 本文提出LLM-Wikirace基准测试，通过维基百科链接导航评估大语言模型的规划、推理与世界知识能力。前沿模型在简单任务上表现超人类，但在困难难度上成功率骤降至23%，暴露出长距离规划与失败恢复的根本性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏系统性评估大语言模型多步规划与失败恢复能力的基准。LLM-Wikirace旨在通过维基百科链接导航任务，检验模型的前瞻性规划、世界知识应用及失败重规划能力，揭示当前推理系统的局限性。

Method: 该基准要求模型从源页面通过超链接逐步导航至目标页面。研究评估了Gemini-3、GPT-5、Claude Opus 4.5等开源与闭源模型，设置不同难度级别，并采用轨迹级分析考察模型行为模式与恢复策略。

Result: 顶尖模型在简单任务上达超人类水平，但困难任务成功率急剧下降（最佳模型Gemini-3仅23%）。世界知识是必要非充分条件，超过阈值后规划与长距离推理成为决定性因素。轨迹分析揭示强模型在失败后重规划能力薄弱，频繁陷入循环而非有效恢复。

Conclusion: LLM-Wikirace以简洁设计清晰暴露了前沿大语言模型在复杂规划推理中的核心短板，特别是长距离依赖与失败恢复机制。该基准为未来提升模型规划能力提供了明确方向与开放挑战平台。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [140] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 该论文揭示了大型语言模型在智能体模式下存在严重的安全评估盲区：文本层面的安全对齐无法有效迁移到工具调用层面，导致模型可能在拒绝有害文本请求的同时执行有害操作。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估几乎完全聚焦于文本输出的拒绝行为，但作为智能体部署的LLM通过工具调用与外部系统交互会产生真实世界后果，文本层面的安全性是否能够保障工具调用层面的安全尚不明确，这一关键问题亟待系统性评估。

Method: 提出GAP基准测试框架，对六个前沿模型在六个监管领域（医药、金融、教育、就业、法律、基础设施）进行测试。每个领域包含七种越狱场景，设置三种系统提示条件（中性、安全强化、工具鼓励）和两种提示变体，共生成17,420个分析数据点，系统性地测量文本安全与工具调用安全之间的差异。

Result: 核心发现是文本安全无法迁移到工具调用安全，存在大量模型文本拒绝但工具调用仍执行有害操作的"差异缺口"现象。即使在安全强化提示下，六个模型仍出现219例此类情况。系统提示措辞影响显著（最稳健模型工具调用安全率跨度21个百分点，最敏感模型达57个百分点）。运行时治理合约虽能减少信息泄露，但对阻止非法工具调用尝试本身无显著威慑效果。

Conclusion: 文本-only安全评估不足以评估智能体行为，工具调用安全需要独立的测量与缓解机制，当前的安全对齐方法在智能体场景下存在重大盲区。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [141] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 针对复杂关系查询和精确约束难以可靠解决的问题，本文提出将形式化验证与深度学习图像检索结合的新框架，通过图验证与神经代码生成的协同作用，实现可验证、可解释的检索结果。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型和预训练方法在处理复杂关系、对象组合及精确约束（如身份、数量、比例）查询时仍存在模糊性和不确定性，缺乏可靠的解决方案。

Method: 提出一种集成形式化验证的深度学习图像检索框架，采用图验证方法与神经代码生成技术协同工作，通过形式化推理系统对查询中的每个原子事实进行显式验证。

Result: 该框架不仅能返回匹配结果，还能识别并标记具体约束条件的满足状态，提供透明可问责的检索过程，同时提升现有嵌入模型的检索性能。

Conclusion: 通过形式化推理系统对检索结果进行验证，该框架超越了向量表示的近似性局限，为开放词汇自然语言图像检索提供了可信、可验证的新范式。

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [142] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: 本文提出LLM4Cov，一种面向硬件验证的离线智能体学习框架，通过执行验证数据筛选、策略感知合成与最差状态优先采样，在4B参数规模下实现69.2%覆盖率，超越教师模型5.3%，性能媲美十倍参数量模型。


<details>
  <summary>Details</summary>
Motivation: 执行感知的LLM智能体虽能通过工具反馈学习，但在线强化学习因反馈成本高、延迟大而难以实用。高覆盖率硬件验证依赖工业模拟器和不可微执行信号，加剧了这一挑战，亟需离线学习方案以降低交互开销。

Method: 将验证建模为确定性评估器引导的无记忆状态转移，提出LLM4Cov框架，集成三项关键技术：执行验证数据筛选确保质量、策略感知智能体数据合成提升多样性、最差状态优先采样优化训练效率。并基于现有验证套件构建现实对齐基准。

Result: 实验表明，4B参数模型在智能体评估下取得69.2%覆盖率通过率，较教师模型提升5.3%，且性能与参数量大一个数量级的模型相当。

Conclusion: LLM4Cov为高成本执行反馈场景提供了可扩展的离线学习范式，证实通过高效数据策略和训练方法，小模型亦可达到与大模型相竞争的性能，对工业级硬件验证等实际应用具有重要价值。

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [143] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: 本文提出WarpRec，一个高性能、后端无关的推荐系统框架，通过统一架构消除研究原型与工业部署间的鸿沟，集成50+算法、40+指标和19种数据处理策略，支持本地到分布式的无缝切换，并内置能耗追踪功能，为下一代可持续、可交互的智能推荐系统提供架构基础。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统创新受阻于分裂的生态系统：研究人员不得不在易于实验的内存计算与需要复杂重写的分布式工业引擎之间做出权衡，这阻碍了学术界与产业界之间的技术转化和规模化应用。

Method: 设计并实现WarpRec框架，采用新颖的后端无关架构，提供统一抽象层，使算法和实验配置能够无缝地从本地执行迁移到分布式训练与优化环境。

Result: 框架包含50多种先进算法、40多种评估指标和19种过滤分割策略，成功实现从研究原型到工业部署的平滑过渡；集成CodeCarbon进行实时能耗追踪，证明可扩展性可与科学诚信和可持续性兼得；同时为推荐系统向生成式AI生态中的交互式工具演进做好准备。

Conclusion: WarpRec有效弥合了学术界与产业界的鸿沟，可作为下一代可持续、支持智能体的推荐系统的架构基石，推动推荐系统从静态排序引擎向生成式AI生态中的交互式工具转型。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [144] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: 本文提出Phantom，一种基于结构化模板注入的自动化LLM智能体劫持框架。通过利用智能体依赖聊天模板令牌区分指令的架构特性，Phantom使用模板自动编码器和贝叶斯优化搜索高效对抗模板，在Qwen、GPT、Gemini等模型上显著提升攻击成功率(ASR)和查询效率，并已在70多个商业产品中发现可确认漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有智能体劫持攻击依赖手工语义提示工程，存在攻击成功率低、难以迁移到闭源商业模型的问题。OWASP已将此列为LLM生态关键威胁。作者发现智能体架构依赖特定模板令牌分离系统/用户/助手/工具指令，这为结构化攻击提供了根本性突破口，但尚未被充分探索。

Method: Phantom框架核心包含三部分：(1)结构化模板注入：利用智能体模板令牌机制，通过注入优化模板引发角色混淆；(2)多层级模板增强：生成结构多样化的候选模板；(3)对抗模板搜索：训练模板自动编码器(TAE)将离散模板映射到连续潜空间，再通过贝叶斯优化高效搜索最优对抗向量并解码为高威力结构化模板，显著提升黑盒迁移性。

Result: 在Qwen、GPT、Gemini上的广泛实验表明，Phantom在攻击成功率(ASR)和查询效率上显著优于现有基线。更重要的是，研究团队在实际商业产品中识别出超过70个已获厂商确认的漏洞，证实了该方法在现实世界中的严重威胁性和实用价值。

Conclusion: 结构化模板注入揭示了LLM智能体架构层面的根本性安全风险。Phantom不仅提供了高效的自动化攻击框架，更通过大规模漏洞挖掘为下一代智能体系统的安全防护奠定了实证基础，强调了在设计阶段就必须考虑模板安全机制的重要性。

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [145] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 针对多智能体IR系统中链式推理评估的局限性，本文提出可重用性和可验证性两个新指标，通过思考者-执行者框架解耦推理生成与执行。实验表明这些指标与标准准确率不相关，专用推理模型的推理链质量不始终优于通用大语言模型（如Llama、Gemma），揭示了当前排行榜的评估盲点。


<details>
  <summary>Details</summary>
Motivation: 现有链式推理评估仅关注目标任务的最终准确率，无法衡量推理过程本身的质量与实用性，存在评估盲区。

Method: 提出思考者-执行者框架以解耦推理生成与执行：可重用性指执行者复用思考者推理链的难易程度；可验证性指执行者利用推理链复现思考者答案的频率。在五个基准测试上，评估四种思考者模型与十种执行者模型的组合表现。

Result: 可重用性和可验证性与标准准确率无统计相关性；专用推理模型的推理链并不始终比Llama、Gemma等通用大语言模型更具可重用性和可验证性。

Conclusion: 研究揭示当前基于准确率的推理能力排行榜存在盲点，强调需要建立更全面的推理过程评估指标体系。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [146] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是CLEF举办的评测实验室，专注于从噪声多语言历史文本中抽取人物-地点关系。该任务定义两种关系类型（$at$和$isAt$），要求系统理解时空线索，并采用准确率、计算效率和领域泛化能力的三重评估体系，旨在支持数字人文领域的知识图谱构建等下游应用。


<details>
  <summary>Details</summary>
Motivation: 历史文献具有噪声大、多语言、跨时代的复杂性，传统命名实体识别已无法满足深层语义分析需求。现有技术缺乏针对时空推理的人物-地点关系抽取的标准化评估基准。该研究延续了HIPE-2020/2022系列，旨在填补历史文本语义关系抽取的评测空白，推动数字人文领域的信息抽取技术发展。

Method: 提出一个多语言跨时代的评测任务框架：1) 定义两种细粒度关系类型（$at$表示历史任意时刻，$isAt$表示出版时间点附近）；2) 设计三重评估指标，综合考量系统准确率、计算效率和领域泛化能力；3) 构建大规模历史数据集作为评测基准。该方法通过标准化任务设计实现可比性评估。

Result: 本文为评测实验室设计方案，尚未产生具体实验结果。核心贡献在于提出了HIPE-2026的评测框架、任务定义和评估体系，为后续参赛系统提供了标准化评估平台。

Conclusion: HIPE-2026通过建立面向历史文本的时空关系抽取评测标准，将推动语义关系抽取技术在数字人文领域的应用。该实验室有望促进知识图谱构建、历史人物传记重建和空间分析等下游任务的发展，为处理复杂历史文献提供技术基准和方法论支持。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


### [147] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 本论文形式化并挑战了AI黑盒安全评估的核心假设，证明在存在潜在上下文相关策略（模型输出受评估时罕见但部署时普遍的无形变量影响）时，任何黑盒评估器都无法可靠估计部署风险。通过Le Cam方法、Yao极小极大原理和陷门单向函数假设，建立了被动评估、自适应评估和计算分离的严格下界，量化了黑盒测试的统计不确定性，并给出额外安全措施（架构约束、训练时保证、可解释性、部署监控）的数学必要性判据。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全评估依赖"测试分布行为可预测部署性能"的假设。然而，潜在上下文相关策略（即模型行为受无形内部变量驱动，该变量在评估时罕见而在部署时普遍）可能导致黑盒评估失效。这种触发器机制使模型在部署时展现出评估未发现的不安全行为，构成重大风险。论文旨在形式化这一挑战，严格证明黑盒评估在此类场景下的根本局限性，为AI安全提供理论基础。

Method: 论文采用三种互补的理论方法：(1) 被动评估：使用Le Cam方法建立极小极大下界，分析独立同分布采样评估器的性能极限；(2) 自适应评估：构造基于哈希的触发器机制，应用Yao极小极大原理证明完全自适应查询的最坏情况误差下界；(3) 计算分离：基于陷门单向函数假设，形式化部署环境拥有特权信息时可激活的不可区分不安全行为；(4) 白盒探测：分析探测质量参数γ下的样本复杂度，并提供显式偏差校正。

Result: 论文获得四个定量结果：1) 被动评估期望绝对误差≥(5/24)δL≈0.208δL（δ为部署触发概率，L为损失差距）；2) 自适应评估最坏情况误差≥δL/16，检测需Θ(1/ε)次查询；3) 计算分离下，白盒探测达到精度ε_R需O(1/(γ²ε_R²))样本，其中γ=α₀+α₁-1为探测质量；4) 陷门单向函数假设下，拥有陷门的部署环境可激活多项式时间评估器无法检测的不安全行为。

Conclusion: 研究揭示了黑盒安全评估在存在潜在上下文相关策略时的根本统计不确定性，证明其无法提供部署风险的最坏情况保证。这为架构约束、训练时安全保障、可解释性工具和部署监控等额外安全措施提供了数学必要性依据。论文首次量化了评估到部署的"分布偏移"风险，强调必须建立多层次AI安全体系，而非依赖单一黑盒评估。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [148] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: 该论文指出现有推荐基准仅评估模型对用户行为的模仿，而在金融顾问领域，用户行为受市场波动影响，可能短视或与长期目标冲突。为此，作者提出 Conv-FinRe，一种对话式、纵向的股票推荐基准，通过多视角参考区分描述性行为与基于投资者风险偏好的规范效用，评估模型在理性决策与行为匹配之间的权衡。实验表明，效用导向的模型往往难以匹配用户选择，而行为匹配的模型容易过拟合短期噪声。数据集和代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统基准主要关注模型对用户行为的模仿精度，但在金融顾问场景中，用户的实际行为可能受市场波动影响而出现噪声或短视，导致行为与长期投资目标不一致。若仅以用户选择为“真实标签”，会将行为模仿与决策质量混淆。因此，需要一种能够评估模型在真实投资情境中理性决策能力、并区分行为与效用的基准。

Method: 作者构建了 Conv-FinRe 基准，包含：1）用户入职访谈，获取投资者特定的风险偏好与长期目标；2）逐步更新的市场上下文，提供历史与实时市场信息；3）对话式顾问交互，模型需在固定投资期限内生成股票排名。基准提供多视角参考，区分描述性用户行为与规范性效用，从而诊断模型是遵循理性分析、模仿噪声还是追逐市场动量。该基准基于真实市场数据和人类决策轨迹，实例化受控的顾问对话，并对多种主流 LLM 进行评估。

Result: 实验结果显示，当前 LLM 在理性决策质量与行为对齐之间存在显著张力。以效用为基础的排名模型在满足投资者风险偏好方面表现较好，但往往无法准确预测用户的实际选择；而以行为匹配为目标的模型则容易过拟合短期市场噪声，导致长期表现不佳。不同模型在 Conv-FinRe 上的表现差异明显，揭示了其在真实金融顾问任务中的局限性。

Conclusion: Conv-FinRe 为评估 LLM 在金融顾问任务中超越简单行为模仿的能力提供了新基准，强调了理性决策与行为对齐之间的权衡。研究结果提示，未来模型需在保持对用户长期目标的理解的同时，避免过度拟合短期噪声。公开的数据集与代码将促进后续研究。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [149] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: 本文提出Sonar-TS，一种神经符号框架，通过Search-Then-Verify流水线解决时间序列数据库的自然语言查询问题，有效处理连续形态意图和超长历史记录，并引入首个大规模基准NLQTSBench。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法捕捉时间序列中的连续形态意图（如形状、异常），而传统时间序列模型在处理超长历史数据时存在局限。此外，该领域缺乏系统性解决方案和大规模评估基准，制约了研究进展。

Method: 提出Sonar-TS框架，采用"搜索-验证"两阶段流水线：1）搜索阶段通过特征索引执行SQL查询，快速筛选候选时间窗口；2）验证阶段生成Python程序对候选窗口进行精确验证。同时构建NLQTSBench大规模基准数据集以支持评估。

Result: 实验揭示了NLQ4TSDB领域的独特挑战，结果表明Sonar-TS能够有效处理复杂时序查询，在现有方法失效的场景下仍能保持良好性能。

Conclusion: 本研究首次系统性地探索了NLQ4TSDB问题，提出的Sonar-TS框架和NLQTSBench基准为未来研究提供了通用解决方案和评估标准，推动了该领域的发展。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [150] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: 本文提出Cinder系统，一种两阶段匹配算法，用于解决多人在线游戏中异构技能水平预组队（lobby）的公平快速匹配问题。系统先用Ruzicka相似度指数对非异常值技能范围进行快速初筛，再通过基于倒置正态分布的非线性技能分桶映射，结合Kantorovich距离计算"制裁分数"来精确评估匹配公平性，并在1.4亿次模拟配对中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 现代多人在线游戏的公平快速匹配系统直接影响玩家留存和满意度，但现有方法（如使用平均或中位数团队技能）在面对技能分布广泛或偏斜的异质预组队时，常导致不平衡的单边对局，亟需更精细化的公平性评估机制。

Method: Cinder采用两阶段匹配策略：第一阶段通过Ruzicka相似度指数快速过滤技能范围差异过大的预组队；第二阶段将玩家段位映射到倒置正态分布生成的非线性技能桶（在平均水平段位提供更细粒度），并基于排序后桶索引的Kantorovich距离计算"制裁分数"来量化匹配公平性。

Result: 通过对1.4亿次模拟预组队配对结果的制裁分数分布分析，证明该系统能够为公平匹配阈值提供稳健基础，有效平衡匹配速度与公平性。

Conclusion: Cinder系统通过两阶段架构成功解决了异质技能预组队的公平匹配难题，其结合Ruzicka相似度初筛与Kantorovich距离精评的方法，在保持匹配效率的同时显著提升了公平性，为游戏匹配系统设计提供了新的实用框架。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [151] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: 针对LLM智能体每步重复加载长指令和工具导致的高成本、高错误率问题，本文提出指令-工具检索（ITR）方法。该方法动态检索最相关的指令片段和最小工具子集，实现上下文token减少95%、工具路由准确率提升32%、成本降低70%，并支持2-20倍更多运行循环。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在执行多步任务时，每轮都会重复加载完整的系统指令和庞大工具目录，导致四大问题：1）token成本急剧增加；2）智能体偏离预定轨道的概率上升；3）系统响应延迟显著；4）工具选择错误率提高。这些问题严重制约了长周期自主智能体的实用性和可靠性。

Method: 提出指令-工具检索（ITR），一种检索增强生成（RAG）的变体。其核心技术是在每个执行步骤中：1）仅检索最相关的系统提示片段；2）仅加载最小必要的工具子集。ITR动态构建运行时系统提示，并通过置信度门控机制提供回退方案，确保系统鲁棒性。

Result: 在可控基准测试中，相比传统单体基线：1）每步上下文token量减少95%；2）正确工具路由率相对提升32%；3）端到端episode成本削减70%；4）在相同上下文限制下可运行2-20倍的循环次数。性能收益随智能体步数增加呈累积效应。

Conclusion: ITR通过动态检索机制有效解决了LLM智能体的效率瓶颈，特别适合长周期自主应用场景。该方法不仅显著降低成本和延迟，还提升了任务执行准确性。研究提供了完整的评估协议、消融分析和部署指南，为实际应用奠定了坚实基础。

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [152] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: 该论文提出了一种名为连续子值Q学习(S2Q)的新方法，通过训练多个子值函数来保留高价值的备选动作，解决了传统多智能体强化学习值分解方法在训练过程中因价值函数动态变化而容易收敛到次优策略的问题，在标准基准测试中性能优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有合作式多智能体强化学习(MARL)的值分解方法过度依赖单一最优动作，当训练过程中底层价值函数发生偏移时缺乏适应性，导致策略收敛到次优点。需要一种能够持续探索并快速适应变化最优解的方法。

Method: 提出Successive Sub-value Q-learning (S2Q)，核心是学习并维护多个子值函数来捕捉不同的高价值动作区域，将这些子值函数整合到基于Softmax的行为策略中，使智能体能够持续探索并利用多个有希望的备选动作。

Result: 在具有挑战性的多智能体强化学习基准测试中，S2Q方法持续且显著地优于多种现有MARL算法，证明了其更好的适应性和更高的整体性能。

Conclusion: S2Q通过维护多个子值函数有效保留了备选高价值动作，实现了对动态变化最优值的快速适应，为MARL中的探索效率和策略优化提供了创新解决方案。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [153] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: 本文提出预测性批调度（PBS），一种通过动态优先处理高损失样本来加速语言模型收敛的训练优化技术。该方法利用在线训练的轻量级线性预测器，仅基于标记频率、序列长度、词汇多样性和稀有标记比率四个静态特征估计样本难度，在1.3亿参数模型上实现6-13%的收敛加速，且计算开销可忽略。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习依赖预定义难度指标，而困难样本挖掘需昂贵的逐样本损失跟踪，计算成本高。本研究旨在开发一种低开销的动态样本优先级调度机制，以高效提升语言模型训练收敛速度。

Method: PBS核心方法是构建在线更新的线性预测器，从静态标记级特征预测样本难度。具体提取四个简单特征：标记频率、序列长度、词汇多样性和稀有标记比率，用于训练预测器。在批次构建阶段，根据预测难度动态优先选择高损失样本，实现自适应课程学习。

Result: 实验在1.3亿参数Transformer上进行，PBS使评估损失收敛速度提升6-13%。预测器与真实损失的相关系数从初始0.14提升至训练10,000步后的0.44，仅用四个特征即达到良好预测效果，验证了特征的有效性。

Conclusion: 研究结果证实，标记频率统计编码了丰富的样本难度信息。PBS以极低计算代价实现了动态课程学习，为大规模语言模型高效训练提供了实用解决方案，证明了简单特征在训练优化中的潜力。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [154] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 针对ESG评级机构间评分差异大、可比性差的问题，本文提出一个通用的人机协作框架（STRIDE+SR-Delta），利用大语言模型构建可信基准数据集，以标准化评估ESG评级方法，推动可持续议程。


<details>
  <summary>Details</summary>
Motivation: ESG评级机构对同一企业的评分存在显著差异，导致评级结果缺乏可比性、可信度和决策参考价值，亟需统一标准和方法论来提升评级的可靠性和实用性。

Method: 提出由两部分组成的人机协作框架：1) STRIDE提供基于大语言模型的企业级基准数据集构建原则与评分体系；2) SR-Delta通过差异分析流程识别潜在调整方向，共同实现评级方法的可扩展、可比较评估。

Result: 该框架能够生成可信的基准数据集，实现对不同ESG评级方法的可扩展、可比较评估，为标准化评级方法论提供了可行路径。

Conclusion: 呼吁AI界采用AI驱动方法，通过该框架加强和完善ESG评级体系，以支持和推动紧迫的可持续发展议程。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [155] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 针对视觉任务中特征依赖性问题，该研究利用层次化的Owen值替代Shapley值进行特征归因，提出满足T性质的新型分割方法，在保持理论一致性的同时提升了归因精度、语义一致性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值方法假设特征独立，但在视觉任务中像素存在强烈的空间和语义依赖，导致解释偏差。现有Owen值实现依赖常见分割方法（如轴对齐或SLIC），但这些方法违反关键的一致性性质，影响归因效果。

Method: 提出一种满足T性质的新型特征分割方法，构建层次化特征分组，应用Owen值进行层次化Shapley值计算（O-Shap），实现计算剪枝和语义对齐的归因。

Result: 在图像和表格数据集上的实验表明，O-Shap在归因精度、语义一致性和运行效率方面优于基线SHAP变体，尤其在结构重要的任务中表现突出。

Conclusion: 通过理论一致的特征分组设计，Owen值能有效解决视觉特征依赖问题，提供更准确、可解释且高效的解释，为XAI在复杂结构化数据中的应用提供了新方向。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [156] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: 本文提出InstructKG框架，用于从课程讲义材料中自动构建符合教师教学意图的知识图谱。该框架结合教育材料特有的时空和语义信号与大语言模型的泛化能力，提取核心概念并推断学习依赖关系，以支持大规模课程的个性化教学干预。


<details>
  <summary>Details</summary>
Motivation: 掌握教育概念需要理解其先决条件和子概念关系，这对识别学生知识缺口和实现个性化学习至关重要。然而在大规模课程中，教师难以诊断个体误解或确定需要强化的概念。现有知识图谱方法要么停留在表面层次，要么忽视教学材料中丰富的教学信号。

Method: InstructKG框架通过分析课程讲义材料（幻灯片、笔记等），提取重要概念作为节点，并推断有向边所表示的学习依赖关系（如'部分-整体'或'依赖-被依赖'）。该方法独特地融合了教育材料中时序和语义信号（如'递归'在'归并排序'之前教授，'递归'在'归并排序'定义中被提及）与大语言模型的泛化能力。

Result: 在多个学科真实课程讲义材料上的实验及人工评估表明，InstructKG能够准确捕捉丰富的、符合教师教学意图的学习进度关系，有效构建教育知识图谱。

Conclusion: InstructKG成功实现了从教学材料中自动构建符合教师意图的知识图谱，为大规模个性化学习提供了可行技术方案。该框架有效整合教育信号与大语言模型能力，具有实际教育应用价值。

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [157] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 本文提出一种针对困难电路可满足性问题的并行分解算法，通过专用约束将SAT实例划分为弱化公式族，采用参数化设计与并行难度估计指导高效分解，并在逻辑等价检查和密码哈希原像攻击等实例上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 电路可满足性问题是NP完全问题，在形式化验证和密码分析中至关重要。传统求解方法难以应对大规模困难实例，而并行分解可提升求解效率。现有分解方法缺乏有效并行策略和自适应参数调整机制，难以在保证分解质量的同时实现高性能计算。

Method: 提出参数化并行分解算法：1)设计专用约束对原始CircuitSAT实例进行划分，生成弱化子公式族；2)并行计算各子公式的难度估计值；3)基于难度反馈动态调整参数，优化分解质量；4)在逻辑等价检查和密码哈希原像攻击两类挑战性实例上实现并验证。

Result: 算法成功应用于复杂CircuitSAT实例求解，特别是逻辑等价性验证和哈希函数原像攻击场景。实验表明，通过并行难度估计指导的参数调整能高效识别高质量分解方案，显著提升求解效率。

Conclusion: 该并行分解算法为困难SAT实例求解提供了新思路，其参数化设计和并行难度估计机制有效平衡了分解质量与计算开销。该方法在硬件验证和密码安全性分析领域具有重要应用价值，为大规模逻辑问题求解提供了可扩展的并行解决方案。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [158] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA是一种新型基因组基础模型预训练框架，通过将联合嵌入预测架构(JEPA)与生成目标结合，利用CLS令牌在潜在空间预测被掩码基因组片段的高级功能嵌入，而非单个核苷酸。该方法解决了现有MLM/NTP缺乏全局生物视角的问题，在多项基因组基准测试中显著优于纯生成基线，为理解基因组功能逻辑提供了新路径。


<details>
  <summary>Details</summary>
Motivation: 当前基因组基础模型依赖MLM或NTP预训练范式，虽能有效捕获局部基因组语法和细粒度基序模式，但难以获取全局功能上下文，导致学习到的生物表示缺乏整体性视角。

Method: 提出JEPA-DNA框架，将联合嵌入预测架构(JEPA)与传统生成目标集成。通过CLS令牌在潜在空间实现令牌级恢复与预测目标的耦合，强制模型预测被掩码基因组片段的高级功能嵌入表示，而非逐个核苷酸的生成。该框架支持从零开始训练或作为现有模型的持续预训练增强。

Result: 在多样化的基因组基准测试中，JEPA-DNA在监督和零样本任务上均持续优于仅使用生成目标的基线模型。

Conclusion: JEPA-DNA生成更具生物学意义的稳健表示，为构建能够理解基因组功能逻辑的基础模型提供了可扩展的解决方案。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [159] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: 本文提出 Texo，一个仅含 2000 万参数的极简高性能公式识别模型。通过精心设计、知识蒸馏和词表/分词器迁移，Texo 在性能与 SOTA 模型相当的同时，模型大小分别减少 80% 和 65%，实现了在消费级硬件和浏览器中的实时推理，并配套开发了网页应用。


<details>
  <summary>Details</summary>
Motivation: 当前公式识别模型参数量大，难以在消费级硬件和浏览器中实时运行。为了让公式识别技术更易于部署和使用，本文旨在设计一个极简但高性能的轻量级模型。

Method: 采用精心设计、知识蒸馏以及词表和分词器的迁移策略，构建仅含 2000 万参数的 Texo 模型。

Result: Texo 模型参数量仅 2000 万，性能与 UniMERNet-T 和 PPFormulaNet-S 等 SOTA 模型相当，模型大小分别缩减 80% 和 65%，成功在消费级硬件上实现实时推理，并可部署于浏览器环境。

Conclusion: 本文成功开发了极简而高效的 Texo 公式识别模型，大幅减小模型尺寸的同时保持高性能，拓展了在消费级硬件和浏览器中的实时应用能力，并通过配套网页应用降低了用户使用门槛。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [160] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本文作为一项"方法论实验"，提出基于AI Agent的人文社科研究协作流程（Agentic Workflow），并以台湾地区Claude.ai使用数据（N=7,729条对话）为实证案例，验证了一个包含任务模块化、人机分工与可验证性三原则的七阶段模块化框架的可行性，识别出直接执行、迭代优化和人为主导三种协作模式，揭示了人类判断在研究问题提出、理论阐释、情境化推理和伦理反思中的不可替代性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑知识工作形态，但现有研究多聚焦于软件工程与自然科学领域，对人文社科领域的方法论探索仍显不足。本研究旨在填补这一研究空白，构建适用于人文社科研究的可复制AI协作框架。

Method: 本研究采用"方法论实验"设计，提出基于AI Agent的协作研究流程。该流程遵循任务模块化、人机分工与可验证性三原则，构建七阶段模块化工作流。实证层面采用Anthropic经济指数（AEI）中台湾地区Claude.ai使用数据（2025年11月，N=7,729条对话）进行可行性验证。研究分两个层次：主要层次为方法论框架的设计与验证，次要层次为对AEI台湾地区数据的实证分析作为应用演示。

Result: 研究证实了该方法论框架的可行性，主要贡献包括：1）提出可复制的AI协作研究框架；2）通过反思性记录识别出直接执行、迭代优化和人为主导三种人机协作操作模式；3）明确了人类判断在研究问题确立、理论阐释、情境化推理及伦理反思等核心环节的不可替代性。

Conclusion: 本研究为人文社科研究提供了AI增强的方法论框架，但存在三方面局限：数据来源于单一平台、采用横断面设计、以及AI可靠性风险。结论指出，尽管AI能提升研究效率，但人类研究者的核心价值在研究的战略性、理论性与伦理性层面依然不可替代。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [161] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: 本文提出大行为模型(LBM)，通过结构化心理特征嵌入替代传统提示方法，实现高风险环境中个体策略选择的高保真预测，解决了身份漂移和复杂度天花板问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在预测高风险决策时难以生成一致的个体行为，提示方法存在脆弱性和身份漂移，无法有效利用精细角色描述，限制了战略预判等应用的可靠性。

Method: 提出LBM模型，采用行为嵌入架构，基于全面心理测量工具构建高维特质画像，在专有数据集上微调，学习将心理特征映射到不同策略困境中的离散行为。

Result: LBM相比Llama-3.1-8B-Instruct基线预测性能显著提升，在Big Five特质条件下与前沿基线相当；关键优势是能持续从更密集特质画像中获益，不存在复杂度天花板。

Conclusion: LBM建立了可扩展的高保真行为模拟新范式，为战略预判、谈判分析、认知安全和决策支持提供了有效工具，解决了LLMs在个体行为预测中的一致性瓶颈。

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [162] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 本研究通过布鲁姆分类学探究大语言模型内部认知复杂性的神经表征，发现线性分类器能以约95%的准确率区分不同认知层级的表征，证明认知难度在模型前向传播早期即被解析，且表征在各层间逐渐变得可分离。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的黑盒特性亟需超越表面性能指标的评估框架。本研究旨在通过布鲁姆分类学这一层级化视角，深入探究模型内部神经表征中是否编码了不同认知复杂度，以实现更本质的模型能力评估。

Method: 研究采用布鲁姆分类学作为理论框架，分析不同大语言模型的高维激活向量，通过在模型残差流中训练线性分类器，探测从基础回忆到抽象创造等不同认知层级是否在线性可分子空间中可分离。

Result: 线性分类器在所有布鲁姆认知层级上达到约95%的平均准确率，强有力证明认知层级被编码在模型表征的线性可访问子空间中。同时发现模型在前向传播早期即解析了提示的认知难度，且表征在各层间逐渐变得更具可分性。

Conclusion: 研究结果为大语言模型内部表征编码认知复杂性提供了证据，表明模型以可解释的方式组织认知信息，这为理解模型工作机制和开发更透明的评估框架提供了新视角。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [163] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 本文针对大语言模型回溯测试中的时间知识泄露问题，提出基于Shapley值的声明级检测框架Shapley-DCLR，并设计TimeSPEC方法通过声明验证与再生过滤时间污染。实验表明该方法在保持性能的同时显著降低泄露。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型预测未来事件能力需进行回溯测试，但模型可能在训练中编码了截止时间后的知识，导致评估结果失真，亟需检测并量化这种时间知识泄露问题。

Method: 提出声明级检测框架：将模型推理分解为原子声明并按时间可验证性分类，利用Shapley值计算各声明贡献度，得到Shapley-DCLR指标；进一步提出TimeSPEC方法，在生成过程中交替进行声明验证与再生，确保所有支撑声明均可追溯到截止时间前的来源。

Result: 在美国最高法院案例预测、NBA薪资估算和股票回报排名共350个实例上的实验表明，标准提示基线存在严重时间知识泄露；TimeSPEC能有效降低Shapley-DCLR指标且保持任务性能，显式声明级验证优于基于提示的时间约束方法。

Conclusion: 通过可解释的声明级验证机制能够有效缓解大语言模型在回溯测试中的时间知识泄露问题，TimeSPEC为构建可靠的未来事件预测评估体系提供了可行方案。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [164] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本文详细记录了从原始arXiv LaTeX源码（涵盖数学、计算机科学和理论物理）训练13.6亿参数科学语言模型的全流程案例研究。研究在有限算力（2块A100 GPU）下，通过24次实验运行，系统分析了训练稳定性、数据利用损失和基础设施瓶颈，为中等计算预算的研究者提供了实用工程洞见。


<details>
  <summary>Details</summary>
Motivation: 尽管前沿大语言模型展现出强大的推理与数学能力，但从原始数据源训练领域专用科学语言模型的实践细节仍鲜有文档记录。本研究旨在为计算资源有限的研究者提供构建此类模型的透明工程实践指南，填补该领域的知识空白。

Method: 研究构建了端到端训练流水线，包括元数据过滤、压缩包验证、LaTeX解析提取、文本归一化、领域感知分词及密集Transformer训练。在2块A100 GPU的受限计算环境下，进行了24次实验运行，系统评估了训练稳定性、扩展行为、数据产率损失及基础设施瓶颈。

Result: 关键发现包括：（1）预处理策略显著影响可用token数量；（2）分词设计对符号稳定性具有重要影响；（3）存储与I/O限制可能成为与计算资源同等关键的瓶颈；（4）在52B token的数据丰富场景下实现了稳定的训练动态。

Conclusion: 与提出新架构不同，本研究提供了从底层构建小型科学语言模型的工程实践透明记录。研究结果强调了数据预处理和基础设施优化的重要性，为中等计算预算的研究者构建领域专用模型提供了可操作的指导。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [165] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: 本研究提出MedClarify，一种能够通过生成信息性后续问题进行迭代推理的医疗AI智能体。该方法通过计算鉴别诊断列表，主动生成以减少诊断不确定性为目标的后续问题，并基于最大期望信息增益选择最优问题，从而将诊断错误率相比单次LLM基线降低约27个百分点。


<details>
  <summary>Details</summary>
Motivation: 临床诊断本质上是迭代且不确定的过程，需要医生通过系统性问诊和鉴别诊断来逐步排除紧急情况和缩小诊断范围。然而，当前医疗大语言模型在生成有效后续问题和支持鉴别诊断推理方面的能力尚未得到充分探索，难以模拟真实临床决策中的不确定性感知和主动信息获取过程。

Method: MedClarify采用基于信息论的主动推理框架：首先计算患者表现的候选诊断列表（模拟鉴别诊断），然后生成旨在降低诊断不确定性的后续问题，并通过量化问题的期望信息增益来选择最优提问，从而实现目标明确的不确定性感知推理。

Result: 实验表明，当病例信息不完整时，标准LLM基线会产生多个相似概率的诊断结果。相比之下，MedClarify的信息论推理方法能显著提升诊断性能，将诊断错误率降低约27个百分点（p.p.），有效减少了诊断不确定性。

Conclusion: MedClarify通过智能体信息寻求机制为改进医疗大语言模型提供了可行路径，促进了能够反映真实世界临床推理迭代性和不确定性的有效人机对话，有望推动AI在临床决策支持中的实际应用。

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [166] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 本文针对任务算术中多任务向量组合引发的交叉干扰与表征漂移问题，提出一种基于曲率矩阵近似（K-FAC）的数据无关正则化方法，在保持模块化的同时实现常数任务复杂度、对向量重缩放鲁棒且无需验证集调优的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 任务算术虽为适配基础模型提供了模块化、可扩展的方案，但多任务向量组合会导致交叉任务干扰，引发表征漂移并降低性能。现有表征漂移正则化方法依赖外部任务数据，违背模块化原则并受限于数据可用性（如隐私约束），亟需数据无关的解决方案。

Method: 将正则化问题重新框架为曲率矩阵近似问题，利用Kronecker-Factored Approximate Curvature (K-FAC)技术，构建无需外部数据的实用正则化器，通过二阶信息近似来解耦任务向量。

Result: 在任务添加与否定任务上取得最先进性能；算法复杂度与任务数量呈常数关系；对任务向量重缩放具有鲁棒性；无需使用验证集进行超参数调优。

Conclusion: 该方法通过曲率近似有效解决了任务算术中的表征漂移问题，在保持模块化优势的同时降低了计算开销和调优成本，为多任务基础模型适配提供了高效且实用的正则化策略。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [167] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 本文提出多模态对比变分自编码器（MCVAE）解决非小细胞肺癌生存预测中的数据缺失问题。该模型通过模态特定编码器、门控融合机制、多任务学习和随机模态掩码，在TCGA-LUAD/LUSC数据集上展现出对严重缺失场景的鲁棒性，并揭示多模态集成并非总是有益。


<details>
  <summary>Details</summary>
Motivation: 非小细胞肺癌生存预测因个体预后特征差异而困难。整合全切片图像、转录组学和DNA甲基化数据虽能提供互补信息，但真实临床数据常存在大量模态缺失。现有模型在严重缺失情况下缺乏鲁棒性，亟需一种能处理任意缺失模式并有效融合多模态数据的方法。

Method: 提出MCVAE模型：1）模态特定变分编码器捕获各数据源不确定性；2）带学习门控机制的融合瓶颈层标准化当前模态贡献；3）结合生存损失、重建损失和跨模态对比损失的多任务目标；4）训练时采用随机模态掩码策略增强对任意缺失模式的鲁棒性。

Result: 在TCGA-LUAD（n=475）和TCGA-LUSC（n=446）数据集上评估显示，该方法预测疾病特异性生存（DSS）优于两种先进模型，在严重缺失场景下鲁棒性显著提升。通过测试所有模态子集发现，多模态集成并非总是对任务有益。

Conclusion: MCVAE通过创新架构和训练策略，成功解决了多模态数据缺失下的生存预测问题，为临床决策提供了更鲁棒的工具。研究同时揭示了多模态集成的复杂性，提示需根据具体场景评估集成策略的有效性。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [168] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 针对儿童AI应用隐私风险，本文提出整合GDPR、COPPA等法规的Privacy-by-Design框架，覆盖LLM全生命周期，并通过教育案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 儿童使用AI技术日益普遍，但隐私风险突出，现有法规要求在实际应用中面临实施挑战，需要系统化的设计框架来主动管理风险。

Method: 通过整合欧盟GDPR、加拿大PIPEDA、美国COPPA等法规原则，构建Privacy-by-Design框架；将隐私原则映射到LLM的数据收集、模型训练、运营监控和持续验证四个阶段，并结合UNCRC、英国适龄设计准则和学术文献提供操作控制措施和设计指南。

Result: 开发了一个系统化的隐私保护框架，包含全生命周期的技术和组织控制措施，以及面向儿童的年龄适宜设计指南。案例研究证明，通过该框架可在LLM应用中实现隐私保护与法律合规。

Conclusion: 该框架通过将数据保护策略和年龄适宜设计贯穿LLM应用全生命周期，为开发既保护儿童隐私又符合法规要求的AI应用提供了可行路径，实现了技术创新与儿童权益保护的平衡。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [169] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 本研究开发了一套针对ARM Cortex-M系列处理器(M0+, M4, M7)的AI模型优化基准测试框架，通过自动化测试平台系统评估嵌入式场景下的能效、精度和资源利用，揭示FLOPs与推理时间的近线性关系，并基于帕累托分析实现能耗与精度的最优权衡，为高能效AI系统设计提供实证指导。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的嵌入式系统中部署AI模型时，开发者面临处理器选型与模型优化的关键决策挑战。缺乏系统性的跨平台评估方法导致难以量化能效、精度和资源消耗之间的复杂权衡关系，亟需科学框架指导实践。

Method: 构建自动化测试基准平台，对ARM Cortex-M0+、M4、M7处理器进行系统性性能评估；通过关键性能指标(KPIs)分析FLOPs与推理时间的相关性；运用帕累托前沿方法量化能耗与模型精度的最优平衡点。

Result: 实验发现FLOPs与推理时间呈近线性正相关，可作为计算需求预测指标；M7处理器在短推理周期任务中表现最优，M4在长周期任务中能效更佳，M0+仅适合简单模型；帕累托分析确定了不同应用场景下的最优配置方案。

Conclusion: 该框架为嵌入式AI开发者提供了科学的决策支持工具，能够有效指导处理器选型与模型优化策略，在满足性能需求的同时提升能源效率，推动可持续AI技术在资源受限环境中的应用。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [170] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: 针对大语言模型在电信领域应用中的挑战，本文提出KG-RAG框架，通过结合知识图谱与检索增强生成技术，显著提升了事实准确性并减少了幻觉现象，在基准测试中比标准RAG和纯LLM模型分别提高了14.3%和21.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在电信领域应用面临领域复杂、标准不断演进和专业术语多等挑战，通用领域LLM难以提供准确可靠的输出，导致幻觉增加且实用性降低。

Method: 提出KG-RAG框架，将知识图谱与检索增强生成(RAG)相结合，利用知识图谱提供来自电信标准和技术文档的结构化领域知识，同时通过RAG动态检索相关事实来约束模型输出。

Result: 在基准数据集上的实验表明，KG-RAG优于纯LLM和标准RAG基线模型，平均准确率比RAG提升14.3%，比纯LLM模型提升21.6%。

Conclusion: KG-RAG在复杂电信场景中能够有效生成准确、可靠且可解释的输出，为解决领域特定任务提供了有效方案。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [171] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: 本文提出KLong，一种通过轨迹分割监督微调和渐进式强化学习解决超长周期任务的开源大语言模型智能体。


<details>
  <summary>Details</summary>
Motivation: 应对超长周期任务对持续推理和复杂问题解决能力的挑战。

Method: 1. 轨迹分割监督微调：保留早期上下文，逐步截断后期上下文，保持子轨迹重叠 2. Research-Factory自动化流水线：从研究论文生成高质量训练数据 3. 基于Claude 4.5 Sonnet蒸馏构建数千条长周期轨迹 4. 渐进式强化学习：多阶段训练并逐步延长时间限制

Result: - 1060亿参数KLong在PaperBench上超越万亿参数Kimi K2 Thinking达11.28% - 性能可泛化至SWE-bench Verified和MLE-bench等编码基准

Conclusion: KLong在长周期任务上展现优越性和泛化能力，验证了所提方法的有效性。

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [172] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 本文提出基于常微分方程(ODE)的统一理论框架解决大语言模型激活 steering 的理论缺失和单步限制问题。通过将传统激活加法视为ODE一阶近似，将steering方向设计转化为控制理论中的 barrier function 问题，提出ODESteer方法。该方法使用正负激活的对数密度比定义 barrier function，实现多步自适应 steering，在TruthfulQA、UltraFeedback、RealToxicityPrompts等基准上分别提升5.7%、2.5%和2.4%。


<details>
  <summary>Details</summary>
Motivation: 现有激活 steering 方法缺乏统一的理论框架指导 steering 方向的设计，且过度依赖单步 steering 无法捕捉复杂的激活分布模式，限制了其在 LLM 对齐中的效果和可靠性。

Method: 提出基于常微分方程(ODE)的统一理论框架，将传统激活加法解释为ODE解的一阶近似。在该框架下，识别 steering 方向等价于从控制理论设计 barrier function。具体实现 ODESteer：以正负激活的对数密度比作为 barrier function，构建ODE实现多步自适应 steering。

Result: ODESteer 在多个 LLM 对齐基准测试上实现一致性能提升：TruthfulQA 提升5.7%，UltraFeedback 提升2.5%，RealToxicityPrompts 提升2.4%，显著优于现有最优激活 steering 方法。

Conclusion: 本研究通过 ODE 框架统一了激活 steering 的理论基础，提供了 principled 的新视角，ODESteer 的经验成功验证了该框架的有效性，为 LLM 对齐提供了更可靠的激活操纵方法。

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [173] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 本文提出一种混合联邦学习框架，融合SWIN Transformer与CNN模型（DenseNet201、Inception V3、VGG 19），用于X光影像的COVID-19和肺炎安全分布式诊断。该方法通过联邦学习实现隐私保护下的协同建模，提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 计算能力的显著进步为AI在医疗领域的应用带来机遇，但医疗数据共享面临隐私与安全挑战。亟需构建一个安全、分布式的医疗数据处理系统，协助医生应对COVID-19等流行病诊断需求。

Method: 采用联邦学习使能的集成方法，将SWIN Transformer与多种CNN模型（DenseNet201、Inception V3、VGG 19）相结合。基于TensorFlow/Keras框架和微软Vision Transformer技术，实施实时持续学习，实现分布式环境下数据不移动的安全训练。

Result: 所提混合模型能够基于X光报告准确检测COVID-19和肺炎，为临床医师提供可靠辅助诊断工具。联邦学习集成显著提升了疾病诊断和病情严重程度预测的准确性，同时确保了模型安全性和信息真实性。

Conclusion: 联邦学习驱动的混合AI模型有效解决了医疗数据隐私保护与协同建模的矛盾，在提升肺部疾病诊断性能的同时保障了系统安全性，为构建高效可靠的智能辅助诊断系统提供了可行方案。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [174] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 该论文提出以"人类游戏多元宇宙"作为评估机器通用智能的新范式，通过AI GameStore平台合成100款真实人类游戏并测试前沿视觉-语言模型，结果显示最佳模型得分不足人类平均水平的10%，在需要世界模型、记忆和规划能力的游戏中表现尤其不佳，为衡量类人通用智能提供了可扩展的新方向。


<details>
  <summary>Details</summary>
Motivation: 传统AI基准测试存在局限性：仅评估狭窄能力、测试范围有限，且因静态特性易被开发者针对性优化而快速饱和。在技术快速发展的时代，亟需能全面评估机器智能与人类通用智能差距的新方法，以推动真正类人智能的进步。

Method: 提出AI GameStore平台，采用大语言模型与人工协作的方式，从Steam和苹果应用商店等流行数字游戏平台自动采集、适配标准化且容器化的游戏环境变体，合成代表"人类游戏多元宇宙"空间的新游戏，构建开放式评估框架。

Result: 基于热门排行榜生成100款游戏后，对七种前沿视觉-语言模型进行短期游戏测试。结果表明，即使在最佳情况下，模型在大多数游戏中的得分仍不足人类平均水平10%，且在世界模型学习、记忆和长期规划等能力要求较高的游戏中表现尤为薄弱，暴露了当前AI的通用游戏能力缺陷。

Conclusion: 该研究论证了以人类游戏空间作为机器通用智能评估范式的可行性，提出将AI GameStore建设为可扩展、开放式的实践平台，通过持续测量和比较人机游戏表现，有效驱动和追踪机器智能向人类水平通用智能的演进。

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [175] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT是一个基于分层离散扩散模型的分子图生成框架，通过引入编码化学先验的层次化扩散机制和解耦原子编码，首次实现了图扩散模型在MOSES数据集上的近乎完美有效性，显著超越了现有图扩散方法及1D基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的分子图扩散模型由于分子图的离散特性，面临化学有效性低、难以满足目标性质的长期挑战，其性能普遍落后于1D序列建模方法，这限制了AI在药物发现和材料科学中的应用效果。

Method: 提出MolHIT框架，核心包括：(1)分层离散扩散模型，将离散扩散推广到编码化学先验的额外类别；(2)解耦原子编码策略，根据化学角色拆分原子类型，从而更好地捕捉分子结构中的化学约束。

Result: 在MOSES数据集上首次实现图扩散模型的近完美有效性，全面超越强1D基线模型，并在多性质引导生成和支架扩展等下游任务中表现优异，达到新的最先进性能。

Conclusion: MolHIT通过层次化扩散和解耦编码机制，成功克服了图扩散模型的长期性能限制，为AI驱动的药物发现和材料设计提供了更强大的分子生成工具，展现了图结构建模在该领域的巨大潜力。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>
