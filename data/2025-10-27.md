<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 35]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Code-enabled language models can outperform reasoning models on diverse tasks](https://arxiv.org/abs/2510.20909)
*Cedegao E. Zhang,Cédric Colas,Gabriel Poesia,Joshua B. Tenenbaum,Jacob Andreas*

Main category: cs.CL

TL;DR: CodeAdapt 通过将自然语言推理与代码执行结合并以少量示例进行 in-context 学习，提升指令式语言模型的推理能力，达到甚至超过对应的推理模型（RM），且在多任务上更高效。


<details>
  <summary>Details</summary>
Motivation: 减少 RM 的高计算与数据需求；验证无需微调即可让指令式 LMs 拥有强推理能力，并具有领域通用性。

Method: 采用 CodeAdapt，结合 CodeAct 框架使 LM 以多步方式将推理与代码执行交错；使用从五个训练题目中进行的少-shot 初始学习；对四对匹配的 LM 与 RM 进行对比，覆盖八个任务，评估准确性和 token 效率；分析代码增强推理轨迹。

Result: 在八个任务上，三种 LM 的平均表现超过对应 RM，最高提升到 22.9%；token 效率提升 10-81%；在四个模型的六个任务上综合比较，平均表现提升最高可达 35.7%；代码增强的推理轨迹展现丰富多样的求解策略。

Conclusion: CodeAdapt 风格的学习与推理具有鲁棒性和领域通用性；代码驱动的 LM 是具备认知基础且强大的系统，可能为 in-weight 强化学习奠定基础。

Abstract: Reasoning models (RMs), language models (LMs) trained with reinforcement
learning to produce long-form natural language reasoning, have been remarkably
successful, but they still require large amounts of computation and data to
train, and can be slow and expensive to run. In this paper, we show that
standard instruct LMs can already be elicited to be strong reasoners at a level
comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs
R1) without finetuning, across diverse domains from instruction following and
creative generation to mathematical reasoning. This is achieved by CodeAdapt,
our simple recipe that combines the CodeAct framework, where LMs interleave
natural language reasoning with code execution in a multi-step fashion, with
few-shot bootstrap in-context learning from as few as five training problems.
Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables
three LMs to outperform the corresponding RMs on average over eight tasks (up
to 22.9%) while being 10-81% more token efficient, and delivers superior
performance on six tasks when averaged over the four models (up to 35.7%).
Furthermore, the code-augmented reasoning traces display rich and varied
problem-solving strategies. Our findings support that (1) CodeAdapt-style
learning and reasoning may be robust and domain general and (2) code-enabled
LMs are cognitively grounded and powerful systems, potentially providing a
strong foundation for in-weight reinforcement learning.

</details>


### [2] [FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction](https://arxiv.org/abs/2510.20926)
*Natasha Johnson,Amanda Bertsch,Maria-Emil Deal,Emma Strubell*

Main category: cs.CL

TL;DR: 提出 FICSIM 数据集用于评估长篇小说语义相似性，揭示当前嵌入模型倾向于表层特征，数据收集重视作者同意，适合计算文学研究的评估。


<details>
  <summary>Details</summary>
Motivation: 由于需要处理越来越长的文本、长文本评估成本高、数据污染与公开文学的风险、现有嵌入相似性数据集偏短且不专注于文学任务，需新的专门数据集来评估文学域任务中的相似性与语义关系。

Method: 构建并发布 FICSIM 数据集：包含长篇、最近创作的小说文本，按 12 条相似性轴给出分数；轴基于作者产出元数据并由数字人文学者验证；对多种嵌入模型进行评估，且分析模型偏好与聚焦点。

Result: 实验表明大多数模型更关注表层特征，而未能有效捕捉有助于文学研究任务的语义类别；数据集可作为文学领域评估的有力工具，揭示现有嵌入在文学任务上的局限性。

Conclusion: 该数据集为文学领域的对比评估提供了新的基准，强调在数据收集阶段保障作者授权与同意的重要性；未来工作可聚焦提升模型对文学文本深层语义的理解能力。

Abstract: As language models become capable of processing increasingly long and complex
texts, there has been growing interest in their application within
computational literary studies. However, evaluating the usefulness of these
models for such tasks remains challenging due to the cost of fine-grained
annotation for long-form texts and the data contamination concerns inherent in
using public-domain literature. Current embedding similarity datasets are not
suitable for evaluating literary-domain tasks because of a focus on
coarse-grained similarity and primarily on very short text. We assemble and
release FICSIM, a dataset of long-form, recently written fiction, including
scores along 12 axes of similarity informed by author-produced metadata and
validated by digital humanities scholars. We evaluate a suite of embedding
models on this task, demonstrating a tendency across models to focus on
surface-level features over semantic categories that would be useful for
computational literary studies tasks. Throughout our data-collection process,
we prioritize author agency and rely on continual, informed author consent.

</details>


### [3] [Do LLMs Truly Understand When a Precedent Is Overruled?](https://arxiv.org/abs/2510.20941)
*Li Zhang,Jaromir Savelka,Kevin Ashley*

Main category: cs.CL

TL;DR: 以236对美国最高法院判例为样本的 overruling 关系识别任务，评估当前最先进的大语言模型在长文理解上的局限性，发现时代偏差、浅层推理和情境依赖性错误，凸显现实长文评估基准的必要性。


<details>
  <summary>Details</summary>
Motivation: 长文本场景下的法律推理具有现实高风险，现有评估多为简化合成任务，无法真实反映法律文书的复杂性。Overruling 关系是普通法理论的核心，能有效测试长期文本理解与推理。

Method: 在236对案例上测试多种SOTA LLMs，分析模型识别 overruling 关系的能力，揭示在历史时期的表现下降、倾向于浅层逻辑、以及在开放式任务中的时间逻辑失效。

Result: 发现三点：era sensitivity（历史案例表现较差，训练数据的时间偏差）；shallow reasoning（依赖浅层逻辑，而非深度法律理解）；context-dependent reasoning failures（在复杂开放任务中产生时间上不可能的关系，尽管在简单上下文保持时间感知）。

Conclusion: 提供一个能反映现实长文法律推理复杂性和风险的基准，填补现实长文本评估的空白，并为未来在长文本法律任务上的研究提供更真实的评估环境。

Abstract: Large language models (LLMs) with extended context windows show promise for
complex legal reasoning tasks, yet their ability to understand long legal
documents remains insufficiently evaluated. Developing long-context benchmarks
that capture realistic, high-stakes tasks remains a significant challenge in
the field, as most existing evaluations rely on simplified synthetic tasks that
fail to represent the complexity of real-world document understanding.
Overruling relationships are foundational to common-law doctrine and commonly
found in judicial opinions. They provide a focused and important testbed for
long-document legal understanding that closely resembles what legal
professionals actually do. We present an assessment of state-of-the-art LLMs on
identifying overruling relationships from U.S. Supreme Court cases using a
dataset of 236 case pairs. Our evaluation reveals three critical limitations:
(1) era sensitivity -- the models show degraded performance on historical cases
compared to modern ones, revealing fundamental temporal bias in their training;
(2) shallow reasoning -- models rely on shallow logical heuristics rather than
deep legal comprehension; and (3) context-dependent reasoning failures --
models produce temporally impossible relationships in complex open-ended tasks
despite maintaining basic temporal awareness in simple contexts. Our work
contributes a benchmark that addresses the critical gap in realistic
long-context evaluation, providing an environment that mirrors the complexity
and stakes of actual legal reasoning tasks.

</details>


### [4] [Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model Performance in a Low-Resource Setting](https://arxiv.org/abs/2510.20957)
*Josh McGiff,Khanh-Tung Tran,William Mulcahy,Dáibhidh Ó Luinín,Jake Dalzell,Róisín Ní Bhroin,Adam Burke,Barry O'Sullivan,Hoang D. Nguyen,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: Irish-BLiMP 是首个面向爱尔兰语的语言能力微型对比数据集与评估框架，包含1020对最小对立对，覆盖11种语法特征，用以衡量LLM与人类在爱尔兰语句法知识上的差异。人类总体优于所有模型，开放源代码模型与封闭源代码模型存在显著差距，强模型仍难以达到人类水平。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言环境下对大语言模型句法知识的细粒度评估缺口，通过构建系统化的爱尔兰语语法能力评估基准，推动对LLMs在低资源语言中的理解与改进。

Method: 人工基于大量语法文献和参考书构建并由 fluent 爱尔兰语使用者评审的1020对最小对立对，按11类语法特征分类。对现有大型语言模型和人类参与者在爱尔兰语句法知识任务上的表现进行对比评估。

Result: 在所有语法特征上人类均优于模型，平均领先约16.6%。开放源代码与封闭源代码模型之间存在约18.1%的性能差距，即使最强模型（gpt-5）也仅达到73.5%而人类为90.1%。人类与模型在不同的语法维度上表现出差异化的强/弱点，表明模型学习的表征与人类有所不同。

Conclusion: Irish-BLiMP 提供了首个系统化框架，用于评估LLMs在爱尔兰语上的语法能力，为低资源语言的语言理解研究提供有价值的基准和方向，并促进对跨语言语言表征与模型改进的洞察。

Abstract: We present Irish-BLiMP (Irish Benchmark of Linguistic Minimal Pairs), the
first dataset and framework designed for fine-grained evaluation of linguistic
competence in the Irish language, an endangered language. Drawing on a variety
of linguistic literature and grammar reference works, we manually constructed
and reviewed 1020 minimal pairs across a taxonomy of 11 linguistic features,
through a team of fluent Irish speakers. We evaluate both existing Large
Language Models (LLMs) and fluent human participants on their syntactic
knowledge of Irish. Our findings show that humans outperform all models across
all linguistic features, achieving 16.6% higher accuracy on average. Moreover,
a substantial performance gap of 18.1% persists between open- and closed-source
LLMs, with even the strongest model (gpt-5) reaching only 73.5% accuracy
compared to 90.1% by human. Interestingly, human participants and models
struggle on different aspects of Irish grammar, thus highlighting a difference
in representation learned by the models. Overall, Irish-BLiMP provides the
first systematic framework for evaluating the grammatical competence of LLMs in
Irish and offers a valuable benchmark for advancing research on linguistic
understanding in low-resource languages.

</details>


### [5] [Can Confidence Estimates Decide When Chain-of-thought is Necessary for Llms?](https://arxiv.org/abs/2510.21007)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 提出置信度门控的链式推理（CoT），在置信度低时才触发推理，从而减少冗余CoT和令牌消耗；通过对四种训练-free置信度估计方法的系统研究，与随机基线和“总是知道何时需要CoT”的oracle对比，在多数据集/模型上进行大量实验。结果显示，现有置信度度量可在一定程度上降低冗余CoT并优于随机触发，但单一度量的效果对数据集与模型高度依赖，存在显著局限性与失败模式。


<details>
  <summary>Details</summary>
Motivation: 降低CoT推理的冗余、令牌消耗和潜在不良影响，同时在实际场景中实现更高效、可控的推理过程。

Method: 进行对四种训练-free置信度估计方法的系统评估；将其与随机基线和“oracle”进行对比；在多数据集和多模型上进行大规模实验以分析稳定性与适用性。

Result: 置信度估计方法在降低冗余CoT方面有效，且优于随机触发；但不同度量在不同数据集和模型上表现差异显著，难以实现稳健部署。

Conclusion: 置信度门控CoT具有潜力，但现有训练-free置信度方法的鲁棒性不足，需进一步研究更可靠的自适应推理门控策略。

Abstract: Chain-of-thought (CoT) prompting has emerged as a common technique for
enhancing the reasoning abilities of large language models (LLMs). While
extended reasoning can boost accuracy on complex tasks, it is often unnecessary
and substantially increases token usage, limiting the practicality of reasoning
models in many scenarios. Recent models, such as GPT-OSS and Qwen3, expose
controls that enable users to adjust the length of CoT or determine whether it
is used at all. Yet, it remains unclear when CoT should be used: on some tasks
it improves performance, while on others it provides little benefit or even
harms performance. We address this challenge with confidence-gated CoT, where a
model invokes reasoning only when confidence in its direct answer is low. To
this end, we present the first systematic study of training-free confidence
estimation methods for CoT gating. Specifically, we evaluate four training-free
confidence estimation methods and compare them to a random baseline and an
oracle that always knows when CoT is needed. Through extensive experiments, we
show that existing training-free confidence measures can reduce redundant CoT
and outperform randomly invoked CoT. However, the utility of individual
confidence measures is inconsistent, varying with both the dataset and the
model, underscoring the difficulty of deploying confidence-gated CoT in
practice. By analysing both strengths and failure modes, our study highlights
the potential and limitations of current methods and paves the way toward more
reliable adaptive gating of CoT.

</details>


### [6] [Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection](https://arxiv.org/abs/2510.21049)
*Atoosa Chegini,Hamid Kazemi,Garrett Souza,Maria Safi,Yang Song,Samy Bengio,Sinead Williamson,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 在低FPR场景下，推理增强的Think On提升平均准确率，但在严格精度需求下不一定优于无推理的Think Off；基于token的评分优于自我信心表达，简单的两模态集成可兼顾两者的优点。


<details>
  <summary>Details</summary>
Motivation: 尽管推理能力显著提升LLMs在常规模型上的表现，但在需要严格控制假阳性的应用中，推理的实际效用尚不明确。本研究系统评估推理在低FPR条件下的分类任务表现。

Method: 在安全检测与幻觉检测两类任务中，比较细调与零-shot设置下的标准LLM与LRM；对Think On（有推理）与Think Off（无推理）两种模式进行对比，并比较token-based评分与自我描述信心评分；尝试简单的模式集成。

Result: Think On在整体准确性上优于Think Off，但在低FPR阈值下往往不如Think Off；token-based评分显著优于自我信心评分；两种模式的简单集成可同时收获两者的优势。

Conclusion: 推理对平均准确率有益，但在需要严格控制假阳性率的应用中常常不理想。对于精度敏感场景，非推理（Think Off）与基于token的评分更为稳定，组合策略可兼顾两者的优点。

Abstract: Reasoning has become a central paradigm for large language models (LLMs),
consistently boosting accuracy across diverse benchmarks. Yet its suitability
for precision-sensitive tasks remains unclear. We present the first systematic
study of reasoning for classification tasks under strict low false positive
rate (FPR) regimes. Our analysis covers two tasks--safety detection and
hallucination detection--evaluated in both fine-tuned and zero-shot settings,
using standard LLMs and Large Reasoning Models (LRMs). Our results reveal a
clear trade-off: Think On (reasoning-augmented) generation improves overall
accuracy, but underperforms at the low-FPR thresholds essential for practical
use. In contrast, Think Off (no reasoning during inference) dominates in these
precision-sensitive regimes, with Think On surpassing only when higher FPRs are
acceptable. In addition, we find token-based scoring substantially outperforms
self-verbalized confidence for precision-sensitive deployments. Finally, a
simple ensemble of the two modes recovers the strengths of each. Taken
together, our findings position reasoning as a double-edged tool: beneficial
for average accuracy, but often ill-suited for applications requiring strict
precision.

</details>


### [7] [Dynamic Retriever for In-Context Knowledge Editing via Policy Optimization](https://arxiv.org/abs/2510.21059)
*Mahmud Wasif Nafee,Maiqi Jiang,Haipeng Chen,Yanfu Zhang*

Main category: cs.CL

TL;DR: DR-IKE：一个面向黑箱LLMs的动态知识编辑框架，通过REINFORCE训练的BERT检索器对演示进行编辑收益排序，并使用可学习阈值裁剪低价值示例，实现自适应提示长度及无权重更新的编辑。


<details>
  <summary>Details</summary>
Motivation: 解决静态演示集在知识编辑中的数量-质量权衡与对任务难度缺乏自适应的问题，提供一种能够在不微调模型权重的前提下、针对黑盒接口实现梯度无关的知识编辑方法。

Method: 提出DR-IKE：1) 用REINFORCE训练的BERT检索器对示例进行排序，基于编辑奖励来选择演示；2) 引入可学习阈值，依据任务难度动态裁剪或扩展提示；3) 仅通过前向传播进行推理，兼容黑盒LLMs。

Result: 在COUNTERFACT基准上，编辑成功率提升最多17.1%，平均延迟降低41.6%，且对无关查询保持准确性；代码开源于GitHub。

Conclusion: DR-IKE实现可扩展、具自适应能力的无权重微调知识编辑，适用于黑盒LLMs；动态演示选择提升编辑效率与效果。

Abstract: Large language models (LLMs) excel at factual recall yet still propagate
stale or incorrect knowledge. In-context knowledge editing offers a
gradient-free remedy suitable for black-box APIs, but current editors rely on
static demonstration sets chosen by surface-level similarity, leading to two
persistent obstacles: (i) a quantity-quality trade-off, and (ii) lack of
adaptivity to task difficulty. We address these issues by dynamically selecting
supporting demonstrations according to their utility for the edit. We propose
Dynamic Retriever for In-Context Knowledge Editing (DR-IKE), a lightweight
framework that (1) trains a BERT retriever with REINFORCE to rank
demonstrations by editing reward, and (2) employs a learnable threshold to
prune low-value examples, shortening the prompt when the edit is easy and
expanding it when the task is hard. DR-IKE performs editing without modifying
model weights, relying solely on forward passes for compatibility with
black-box LLMs. On the COUNTERFACT benchmark, it improves edit success by up to
17.1%, reduces latency by 41.6%, and preserves accuracy on unrelated queries,
demonstrating scalable and adaptive knowledge editing. The code is available at
https://github.com/mwnafee/DR-IKE .

</details>


### [8] [Bridging Language Gaps with Adaptive RAG: Improving Indonesian Language Question Answering](https://arxiv.org/abs/2510.21068)
*William Christian,Daniel Adamlu,Adrian Yu,Derwin Suhartono*

Main category: cs.CL

TL;DR: Adaptive RAG for Indonesian QA uses a question-complexity classifier and MT-augmented data. The classifier is reliable, but the multi-retrieval strategy is inconsistent and hurts overall QA performance.


<details>
  <summary>Details</summary>
Motivation: Fill the gap in robust QA for low-resource Indonesian language by extending Retrieval-Augmented Generation (RAG) with an adaptive strategy and data augmentation when data is scarce.

Method: Combine a question-complexity classifier with an adaptive RAG pipeline, and apply machine translation-based data augmentation to expand Indonesian training data; evaluate classifier reliability and QA performance with and without multi-retrieval.

Result: Question-complexity classifier demonstrates reliable performance; however, the multi-retrieval answering strategy is inconsistent and degrades overall QA evaluation when employed.

Conclusion: Adaptive RAG shows potential for Indonesian QA but faces challenges in stabilizing multi-retrieval strategies; future work should focus on robust retrieval orchestration and better data augmentation for low-resource languages.

Abstract: Question Answering (QA) has seen significant improvements with the
advancement of machine learning models, further studies enhanced this question
answering system by retrieving external information, called Retrieval-Augmented
Generation (RAG) to produce more accurate and informative answers. However,
these state-of-the-art-performance is predominantly in English language. To
address this gap we made an effort of bridging language gaps by incorporating
Adaptive RAG system to Indonesian language. Adaptive RAG system integrates a
classifier whose task is to distinguish the question complexity, which in turn
determines the strategy for answering the question. To overcome the limited
availability of Indonesian language dataset, our study employs machine
translation as data augmentation approach. Experiments show reliable question
complexity classifier; however, we observed significant inconsistencies in
multi-retrieval answering strategy which negatively impacted the overall
evaluation when this strategy was applied. These findings highlight both the
promise and challenges of question answering in low-resource language
suggesting directions for future improvement.

</details>


### [9] [CDrugRed: A Chinese Drug Recommendation Dataset for Discharge Medications in Metabolic Diseases](https://arxiv.org/abs/2510.21084)
*Juntao Li,Haobin Yuan,Ling Luo,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 提出公开的中文药物推荐数据集CDrugRed，聚焦代谢疾病出院药物推荐，基于EHR，使用LLMs进行基线评估，性能仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 解决缺乏公开、真实世界的中文EHR药物推荐数据，促进中文临床决策支持与多模态智能药物推荐的研究。

Method: 构建CDrugRed数据集：5894条记录、来自3190名患者，包含患者人口统计、病史、临床过程与出院诊断等信息；在出院药物推荐任务上对若干大型语言模型进行监督微调与评估。

Result: 最佳模型的F1为0.5648，Jaccard为0.4477，显示任务具有较高复杂性且现有方法存在显著改进空间。

Conclusion: CDrugRed为中文药物推荐研究提供具有挑战性且公开可得的资源，利于发展更鲁棒、准确的药物推荐系统。数据对研究社区公开，便于未来对比与迭代。

Abstract: Intelligent drug recommendation based on Electronic Health Records (EHRs) is
critical for improving for improving the quality and efficiency of clinical
decision-making. By leveraging large-scale patient data, drug recommendation
systems can assist physicians in selecting the most appropriate medications
according to a patient's medical history, diagnoses, laboratory results, and
comorbidities. However, the advancement of such systems is significantly
hampered by the scarcity of publicly available, real-world EHR datasets,
particularly in languages other than English. In this work, we present
CDrugRed, a first publicly available Chinese drug recommendation dataset
focused on discharge medications for metabolic diseases. The dataset includes
5,894 de-identified records from 3,190 patients, containing comprehensive
information such as patient demographics, medical history, clinical course, and
discharge diagnoses. We assess the utility of CDrugRed by benchmarking several
state-of-the-art large language models (LLMs) on the discharge medication
recommendation task. Experimental results show that while supervised
fine-tuning improves model performance, there remains substantial room for
improvement, with the best model achieving the F1 score of 0.5648 and Jaccard
score of 0.4477. This result highlights the complexity of the clinical drug
recommendation task and establishes CDrugRed as a challenging and valuable
resource for developing more robust and accurate drug recommendation systems.
The dataset is publicly available to the research community under the data
usage agreements at https://github.com/DUTIR-BioNLP/CDrugRed.

</details>


### [10] [Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only](https://arxiv.org/abs/2510.21090)
*Qingru Zhang,Liang Qiu,Ilgee Hong,Zhenghao Xu,Tianyi Liu,Shiyang Li,Rongzhi Zhang,Zheng Li,Lihong Li,Bing Yin,Chao Zhang,Jianshu Chen,Haoming Jiang,Tuo Zhao*

Main category: cs.CL

TL;DR: Self-Rewarding PPO is a fine-tuning method that uses an implicit on-policy reward defined by the log policy ratio between the SFT model and the pretrained base model, enabling PPO-based on-policy fine-tuning from demonstration data and improving generalization and data efficiency over standard SFT.


<details>
  <summary>Details</summary>
Motivation: SFT is off-policy and prone to overfitting and poor out-of-domain generalization, especially with limited data; there is a need for improved data efficiency and robustness without relying on human preference labels.

Method: Integrates SFT with PPO; defines a self-rewarding signal as log pi_SFT(a|s) - log pi_base(a|s) or log pi_SFT / pi_base; uses PPO updates on-policy using this reward; avoids human annotations; utilizes demonstration data to align LLMs.

Result: Empirical evaluation across NLP tasks shows Self-Rewarding PPO consistently outperforms traditional SFT in generalization, robustness, and data efficiency, especially when high-quality annotated data is scarce.

Conclusion: A self-rewarding on-policy fine-tuning framework effectively aligns LLMs with demonstration data and mitigates SFT limitations; the log-policy-ratio reward serves as an implicit, scalable signal enabling on-policy optimization without additional human labeling.

Abstract: Supervised fine-tuning (SFT) has emerged as a crucial method for aligning
large language models (LLMs) with human-annotated demonstrations. However, SFT,
being an off-policy approach similar to behavior cloning, often struggles with
overfitting and poor out-of-domain generalization, especially in limited-data
scenarios. To address these limitations, we propose Self-Rewarding PPO, a novel
fine-tuning method that leverages on-policy techniques to enhance
generalization performance. Our approach combines the strengths of SFT and
proximal policy optimization (PPO) to achieve more effective alignment from
demonstration data. At its core is a reward function designed as the log policy
ratio between the SFT model and the pretrained base model. This function serves
as an implicit reward signal, using the pretrained policy as a baseline and the
SFT policy as a target. By doing so, it enables on-policy fine-tuning without
relying on human preference annotations. The integration of this self-rewarding
mechanism with PPO addresses key limitations of SFT, improving generalization,
data efficiency, and robustness. Our empirical evaluation across a range of
natural language processing tasks demonstrates that Self-Rewarding PPO
consistently outperforms traditional SFT methods. The results highlight the
effectiveness of our approach in aligning LLMs using demonstration data,
particularly in scenarios where high-quality annotated data is scarce.

</details>


### [11] [The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection](https://arxiv.org/abs/2510.21118)
*Qiang Ding,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 提出 VeriGray，构建用于摘要忠实性评估的新基准，解决外部知识边界不清导致的标注歧义。引入 Out-Dependent 作为中间类别，强调需要外部知识才能验证的情况。


<details>
  <summary>Details</summary>
Motivation: 当前摘要忠实性评估存在标注歧义，因可接受的外部知识范围未明导致标签不一致；因此需要一个能区分需要外部知识的情况的框架。

Method: 设计一种新的标注框架，增加 Out-Dependent 类别，使用该框架对摘要进行标注，构建 VeriGray 基准；对多种基线模型进行评估。

Result: VeriGray 展示该领域存在较高的未忠实比例，SOTA 如 GPT-5 约有6%的句子幻觉；平均约8%的模型输出落入 Out-Dependent 类，Bench对多种基线造成较大挑战。

Conclusion: 该工作揭示标注歧义的严重性并提供一个能更细粒度区分的基准，提示未来改进方向在于更好地处理外部知识的可验证性与鲁棒性。

Abstract: Ensuring that Large Language Models (LLMs) generate summaries faithful to a
given source document is essential for real-world applications. While prior
research has explored LLM faithfulness, existing benchmarks suffer from
annotation ambiguity, primarily due to the ill-defined boundary of permissible
external knowledge in generated outputs. For instance, common sense is often
incorporated into responses and labeled as "faithful", yet the acceptable
extent of such knowledge remains unspecified, leading to inconsistent
annotations. To address this issue, we propose a novel faithfulness annotation
framework, which introduces an intermediate category, Out-Dependent, to
classify cases where external knowledge is required for verification. Using
this framework, we construct VeriGray (Verification with the Gray Zone) -- a
new unfaithfulness detection benchmark in summarization. Statistics reveal that
even SOTA LLMs, such as GPT-5, exhibit hallucinations ($\sim 6\%$ of sentences)
in summarization tasks. Moreover, a substantial proportion ($\sim 8\%$ on
average of models) of generated sentences fall into the Out-Dependent category,
underscoring the importance of resolving annotation ambiguity in unfaithfulness
detection benchmarks. Experiments demonstrate that our benchmark poses
significant challenges to multiple baseline methods, indicating considerable
room for future improvement.

</details>


### [12] [Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications](https://arxiv.org/abs/2510.21131)
*Guangxin Su,Hanchen Wang,Jianwei Wang,Wenjie Zhang,Ying Zhang,Jian Pei*

Main category: cs.CL

TL;DR: 首次系统性综述LLM–TAG集成，提出两大方向、三种编排策略及多种方法论，汇聚数据集与应用，指明挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: LLMs 拥有强大语义理解与生成能力，但黑箱性限制了结构化与多跳推理。Text-Attributed Graphs（TAGs）提供显式关系结构与文本上下文，但往往缺乏语义深度。将两者结合可获得互补收益：提升 TAG 表征学习与改进 LLM 的推理与解释性。

Method: 建立面向 LLM–TAG 集成的分层分类法，覆盖两个核心方向（LLM for TAG 与 TAG for LLM），并将编排策略分为顺序、并行、以及多模态/多模块框架；系统梳理 TAG 相关的预训练、提示设计与参数高效微调方法；通过文献综述整理数据集、应用场景及实验基线。

Result: 给出首个系统性综述，提出完整的 LLM–TAG 耦合框架、编排策略与方法论，汇总实证洞察、可用数据集与多领域应用。

Conclusion: 揭示现存挑战与未来研究方向，强调在语言与图学习交叉领域的研究机会，供未来工作借鉴。

Abstract: Large Language Models (LLMs) have achieved remarkable success in natural
language processing through strong semantic understanding and generation.
However, their black-box nature limits structured and multi-hop reasoning. In
contrast, Text-Attributed Graphs (TAGs) provide explicit relational structures
enriched with textual context, yet often lack semantic depth. Recent research
shows that combining LLMs and TAGs yields complementary benefits: enhancing TAG
representation learning and improving the reasoning and interpretability of
LLMs. This survey provides the first systematic review of LLM--TAG integration
from an orchestration perspective. We introduce a novel taxonomy covering two
fundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, and
TAG for LLM, where structured graphs improve LLM reasoning. We categorize
orchestration strategies into sequential, parallel, and multi-module
frameworks, and discuss advances in TAG-specific pretraining, prompting, and
parameter-efficient fine-tuning. Beyond methodology, we summarize empirical
insights, curate available datasets, and highlight diverse applications across
recommendation systems, biomedical analysis, and knowledge-intensive question
answering. Finally, we outline open challenges and promising research
directions, aiming to guide future work at the intersection of language and
graph learning.

</details>


### [13] [Redefining Retrieval Evaluation in the Era of LLMs](https://arxiv.org/abs/2510.21440)
*Giovanni Trappolini,Florin Cuconasu,Simone Filice,Yoelle Maarek,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: 传统IR指标不适用于RAG，因为LLMs按整体处理检索结果且对相关性与干扰的影响不同；提出UDCG，基于效用和干扰的评分，并引入面向LLM的位次折扣，能更直接对齐端到端答案准确性。


<details>
  <summary>Details</summary>
Motivation: 解决人类逐步阅览假设与机器端RAG消费模式的错位，以及相关性与生成质量之间的错配；需要新的评估度量同时考量相关信息的正向贡献与干扰信息的负面影响。

Method: 提出基于效用的标注方案，构建UDCG（Utility and Distraction-aware Cumulative Gain）度量：结合正向相关片段的效用与干扰片段的负面影响；引入面向LLM的位次折扣；通过与端到端答案准确性之间的相关性进行优化；在五个数据集和六个LLM上进行实验。

Result: UDCG在与传统指标的相关性对比中，相关性提升高达36%。

Conclusion: 该工作将IR评估更好地对齐至LLM使用场景，提供更可靠的RAG组件评估手段。

Abstract: Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,
assume that human users sequentially examine documents with diminishing
attention to lower ranks. This assumption breaks down in Retrieval Augmented
Generation (RAG) systems, where search results are consumed by Large Language
Models (LLMs), which, unlike humans, process all retrieved documents as a whole
rather than sequentially. Additionally, traditional IR metrics do not account
for related but irrelevant documents that actively degrade generation quality,
rather than merely being ignored. Due to these two major misalignments, namely
human vs. machine position discount and human relevance vs. machine utility,
classical IR metrics do not accurately predict RAG performance. We introduce a
utility-based annotation schema that quantifies both the positive contribution
of relevant passages and the negative impact of distracting ones. Building on
this foundation, we propose UDCG (Utility and Distraction-aware Cumulative
Gain), a metric using an LLM-oriented positional discount to directly optimize
the correlation with the end-to-end answer accuracy. Experiments on five
datasets and six LLMs demonstrate that UDCG improves correlation by up to 36%
compared to traditional metrics. Our work provides a critical step toward
aligning IR evaluation with LLM consumers and enables more reliable assessment
of RAG components

</details>


### [14] [Estonian Native Large Language Model Benchmark](https://arxiv.org/abs/2510.21193)
*Helena Grete Lillepalu,Tanel Alumäe*

Main category: cs.CL

TL;DR: 提出并实现一个针对爱沙尼亚语的LLM基准，覆盖七个数据集，用于评估通用与领域任务、语法/词汇理解、摘要等能力；对比6个基础模型和26个指令微调模型；使用人类评估与LLM（以Claude 3.7 Sonnet为评审）进行评分，结果显示人评与基准分弱到强相关，且LLM评审与人类评分对齐良好。


<details>
  <summary>Details</summary>
Motivation: 爱沙尼亚语的LLM基准研究不足，现有评测不足以覆盖语言特性与领域应用，需本地化、无机器翻译的数据集来公正评测模型并推动改进。

Method: 基于七个来自本地原生来源的数据集，未使用机器翻译，评估对象包含6个基础模型和26个指令微调模型；评估方式包括人类评估和将LLM作为评审的评测。Claude 3.7 Sonnet用作LLM评审并与人类评分对齐，显示Top模型在多任务上表现优越且可用LLM评审辅助评估。

Result: 给出各数据集上的模型表现概览， top模型在多任务上表现突出；人类评估与基准分存在中至高的相关性，LLM评审（以Claude 3.7 Sonnet为代表）与人类评分高度一致，支持使用LLM来辅助评估。

Conclusion: 提供一个覆盖面广且基于本地资源的爱沙尼亚语LLM评估框架，避免机器翻译偏差，评估结果与人类评估具良好的一致性。未来工作可扩展数据集、引入更多评估任务和更稳健的评审标准。

Abstract: The availability of LLM benchmarks for the Estonian language is limited, and
a comprehensive evaluation comparing the performance of different LLMs on
Estonian tasks has yet to be conducted. We introduce a new benchmark for
evaluating LLMs in Estonian, based on seven diverse datasets. These datasets
assess general and domain-specific knowledge, understanding of Estonian grammar
and vocabulary, summarization abilities, contextual comprehension, and more.
The datasets are all generated from native Estonian sources without using
machine translation. We compare the performance of base models,
instruction-tuned open-source models, and commercial models. Our evaluation
includes 6 base models and 26 instruction-tuned models. To assess the results,
we employ both human evaluation and LLM-as-a-judge methods. Human evaluation
scores showed moderate to high correlation with benchmark evaluations,
depending on the dataset. Claude 3.7 Sonnet, used as an LLM judge, demonstrated
strong alignment with human ratings, indicating that top-performing LLMs can
effectively support the evaluation of Estonian-language models.

</details>


### [15] [The "Right" Discourse on Migration: Analysing Migration-Related Tweets in Right and Far-Right Political Movements](https://arxiv.org/abs/2510.21220)
*Nishan Chatterjee,Veronika Bajt,Ana Zwitter Vitez,Senja Pollak*

Main category: cs.CL

TL;DR: A cross-disciplinary NLP-sociology framework to analyze far-right tweets (MIGR-TWIT) in English and French, uncovering patterns in discourse on migration, hate speech, and persuasion.


<details>
  <summary>Details</summary>
Motivation: The rise of right-wing populism in Europe highlights the need to understand how social media discourse disseminates extremist ideologies and influences political outcomes.

Method: Combine state-of-the-art natural language processing with sociological insights to analyze the MIGR-TWIT corpus of far-right tweets in English and French, identifying discourse patterns, hate speech, and persuasion techniques.

Result: The abstract proposes a methodology and aims to uncover patterns; specific empirical results are not reported.

Conclusion: Integrating linguistic, sociological, and computational approaches can yield cross-disciplinary insights into contemporary challenges posed by right-wing extremism on social media and inform responses.

Abstract: The rise of right-wing populism in Europe has brought to the forefront the
significance of analysing social media discourse to understand the
dissemination of extremist ideologies and their impact on political outcomes.
Twitter, as a platform for interaction and mobilisation, provides a unique
window into the everyday communication of far-right supporters. In this paper,
we propose a methodology that uses state-of-the-art natural language processing
techniques with sociological insights to analyse the MIGR-TWIT corpus of
far-right tweets in English and French. We aim to uncover patterns of discourse
surrounding migration, hate speech, and persuasion techniques employed by right
and far-right actors. By integrating linguistic, sociological, and
computational approaches, we seek to offer cross-disciplinary insights into
societal dynamics and contribute to a better understanding of contemporary
challenges posed by right-wing extremism on social media platforms.

</details>


### [16] [DispatchMAS: Fusing taxonomy and artificial intelligence agents for emergency medical services](https://arxiv.org/abs/2510.21228)
*Xiang Li,Huizi Yu,Wenkong Wang,Yiran Wu,Jiayan Zhou,Wenyue Hua,Xinxin Lin,Wenjia Tan,Lexuan Zhu,Bingyi Chen,Guang Chen,Ming-Li Chen,Yang Zhou,Zhao Li,Themistocles L. Assimes,Yongfeng Zhang,Qingyun Wu,Xin Ma,Lingyao Li,Lizhou Fan*

Main category: cs.CL

TL;DR: A taxonomy-grounded, LLM-powered multi-agent system simulates emergency medical dispatch scenarios with high fidelity, enabling training and potential real-time decision support; shows high guidance and dispatch effectiveness and favorable linguistic metrics.


<details>
  <summary>Details</summary>
Motivation: Emergency medical dispatch (EMD) is a high-stakes process with caller distress, ambiguity, and cognitive load. LLMs and multi-agent systems offer opportunities to augment dispatchers, but require taxonomy grounding and safeguards against misinformation.

Method: Built a clinical taxonomy (32 chief complaints, 6 caller identities from MIMIC-III) and a six-phase call protocol. Developed an AutoGen-based MAS with Caller and Dispatcher Agents. Grounded interactions in a fact commons to ensure clinical plausibility. Evaluated via a hybrid framework: four physicians assessed 100 simulated cases for Guidance Efficacy and Dispatch Effectiveness, supplemented by automated linguistic analyses (sentiment, readability, politeness).

Result: Inter-rater agreement substantial (Gwet's AC1 > 0.70). Dispatch Effectiveness ~94% contacting the correct potential other agents; Guidance Efficacy ~91% of cases with advice. Neutral sentiment 73.7%; neutral emotion 90.4%; high readability (Flesch 80.9); polite style 60% polite; 0% impolite.

Conclusion:  taxonomy-grounded MAS can simulate diverse, clinically plausible dispatch scenarios with high fidelity, supporting dispatcher training, protocol evaluation, and as a foundation for real-time decision support; outlines a pathway for safe integration of advanced AI agents into emergency response workflows.

Abstract: Objective: Emergency medical dispatch (EMD) is a high-stakes process
challenged by caller distress, ambiguity, and cognitive load. Large Language
Models (LLMs) and Multi-Agent Systems (MAS) offer opportunities to augment
dispatchers. This study aimed to develop and evaluate a taxonomy-grounded,
LLM-powered multi-agent system for simulating realistic EMD scenarios. Methods:
We constructed a clinical taxonomy (32 chief complaints, 6 caller identities
from MIMIC-III) and a six-phase call protocol. Using this framework, we
developed an AutoGen-based MAS with Caller and Dispatcher Agents. The system
grounds interactions in a fact commons to ensure clinical plausibility and
mitigate misinformation. We used a hybrid evaluation framework: four physicians
assessed 100 simulated cases for "Guidance Efficacy" and "Dispatch
Effectiveness," supplemented by automated linguistic analysis (sentiment,
readability, politeness). Results: Human evaluation, with substantial
inter-rater agreement (Gwe's AC1 > 0.70), confirmed the system's high
performance. It demonstrated excellent Dispatch Effectiveness (e.g., 94 %
contacting the correct potential other agents) and Guidance Efficacy (advice
provided in 91 % of cases), both rated highly by physicians. Algorithmic
metrics corroborated these findings, indicating a predominantly neutral
affective profile (73.7 % neutral sentiment; 90.4 % neutral emotion), high
readability (Flesch 80.9), and a consistently polite style (60.0 % polite; 0 %
impolite). Conclusion: Our taxonomy-grounded MAS simulates diverse, clinically
plausible dispatch scenarios with high fidelity. Findings support its use for
dispatcher training, protocol evaluation, and as a foundation for real-time
decision support. This work outlines a pathway for safely integrating advanced
AI agents into emergency response workflows.

</details>


### [17] [Correlation Dimension of Auto-Regressive Large Language Models](https://arxiv.org/abs/2510.21258)
*Xin Du,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 提出 correlation dimension 的新指标，用以量化文本自相似性的结构复杂度，揭示 LLM 预训练阶段的三阶段性、上下文依赖性、幻觉倾向及文本退化检测能力，且对多种自回归架构和量化友好。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标主要关注局部预测正确性，难以捕捉文本的长期层级重复结构和全局复杂性，需要一个统一的框架来度量文本的自相似性与生成动力学。

Method: 定义并实现 correlation dimension 作为文本自相似性的分形几何度量，分析模型对文本结构的感知；在 Transformer 与 Mamba 等自回归架构上应用，评估对 4-bit 量化的鲁棒性，并通过大规模实验验证其在预训练阶段的阶段划分、上下文复杂性、幻觉倾向及文本退化检测方面的表现。

Result: correlation dimension 能揭示预训练过程中的三阶段、反映上下文相关的复杂性、指示幻觉倾向，并能可靠检测生成文本的多种退化现象；该指示性方法计算高效，对量化鲁棒（4-bit）友好，且可广泛应用于多种自回归架构。

Conclusion: 该指标为局部预测准确性与全局结构复杂性之间提供了统一的评估框架，帮助理解生成动力学，提供新视角以评估和改进大语言模型。

Abstract: Large language models (LLMs) have achieved remarkable progress in natural
language generation, yet they continue to display puzzling behaviors -- such as
repetition and incoherence -- even when exhibiting low perplexity. This
highlights a key limitation of conventional evaluation metrics, which emphasize
local prediction accuracy while overlooking long-range structural complexity.
We introduce correlation dimension, a fractal-geometric measure of
self-similarity, to quantify the epistemological complexity of text as
perceived by a language model. This measure captures the hierarchical
recurrence structure of language, bridging local and global properties in a
unified framework. Through extensive experiments, we show that correlation
dimension (1) reveals three distinct phases during pretraining, (2) reflects
context-dependent complexity, (3) indicates a model's tendency toward
hallucination, and (4) reliably detects multiple forms of degeneration in
generated text. The method is computationally efficient, robust to model
quantization (down to 4-bit precision), broadly applicable across
autoregressive architectures (e.g., Transformer and Mamba), and provides fresh
insight into the generative dynamics of LLMs.

</details>


### [18] [Sparser Block-Sparse Attention via Token Permutation](https://arxiv.org/abs/2510.21270)
*Xinghao Wang,Pengyu Wang,Dong Zhang,Chenkun Tan,Shaojun Zhou,Zhaoxiang Liu,Shiguo Lian,Fangxu Liu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出 PBS-Attn：一种可即插即用的分块稀疏注意力方法，通过对注意力的置换特性实现块级稀疏性，提升长上下文下的前填效率，同时保持与全注意力接近的精度；提供 permuted-FlashAttention 内核，实验在真实长上下文数据集上实现最高约2.75×加速，并且与全注意力基线接近。代码开放。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度的提升，LLMs 的自注意力 O(N^2) 复杂度成为瓶颈。块稀疏注意力在理论上能缓解，但实际效果受限于注意力在块之间的分布。需更鲁棒的块级稀疏策略以减少计算量同时维持精度。

Method: 通过利用注意力的置换性质，将输入序列分块并对块级注意力进行重排，使重要 token 更集中地被同一块内的键/值关注，从而提升块级稀疏性。作为可插拔模块，与自定义 permuted-FlashAttention 内核结合实现加速。

Result: 在挑战性真实长上下文数据集上，PBS-Attn 在模型精度上普遍优于现有块稀疏方法，接近全注意力基线。端到端的长上下文前填速度提升最高可达 2.75×。

Conclusion: PBS-Attn 为长上下文下的高效前填提供了实用方案，兼顾效率和精度，具备实际落地潜力，并提供代码实现。

Abstract: Scaling the context length of large language models (LLMs) offers significant
benefits but is computationally expensive. This expense stems primarily from
the self-attention mechanism, whose $O(N^2)$ complexity with respect to
sequence length presents a major bottleneck for both memory and latency.
Fortunately, the attention matrix is often sparse, particularly for long
sequences, suggesting an opportunity for optimization. Block-sparse attention
has emerged as a promising solution that partitions sequences into blocks and
skips computation for a subset of these blocks. However, the effectiveness of
this method is highly dependent on the underlying attention patterns, which can
lead to sub-optimal block-level sparsity. For instance, important key tokens
for queries within a single block may be scattered across numerous other
blocks, leading to computational redundancy. In this work, we propose Permuted
Block-Sparse Attention (\textbf{PBS-Attn}), a plug-and-play method that
leverages the permutation properties of attention to increase block-level
sparsity and enhance the computational efficiency of LLM prefilling. We conduct
comprehensive experiments on challenging real-world long-context datasets,
demonstrating that PBS-Attn consistently outperforms existing block-sparse
attention methods in model accuracy and closely matches the full attention
baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn
achieves an end-to-end speedup of up to $2.75\times$ in long-context
prefilling, confirming its practical viability. Code available at
https://github.com/xinghaow99/pbs-attn

</details>


### [19] [Efficient semantic uncertainty quantification in language models via diversity-steered sampling](https://arxiv.org/abs/2510.21310)
*Ji Won Park,Kyunghyun Cho*

Main category: cs.CL

TL;DR: A diversity-steered sampler improves sample efficiency for uncertainty estimation in large language models by penalizing semantically similar outputs during decoding, applicable to autoregressive and diffusion-based models, with debiasing and variance-reduction techniques; it achieves competitive performance with more semantic coverage and is gradient-free, serving as a drop-in enhancement for risk-sensitive deployments.


<details>
  <summary>Details</summary>
Motivation: Accurate estimation of semantic aleatoric and epistemic uncertainties in free-form QA with LLMs requires many generations, which is costly. There is a need for more sample-efficient methods that reliably cover diverse semantic outputs.

Method: Introduce a continuous semantic-similarity penalty into the model's proposal distribution via a lightly finetuned natural language inference (NLI) model on partial prefixes or intermediate diffusion states. The approach is compatible with autoregressive and masked diffusion decoding. Debias downstream uncertainty estimates with importance reweighting and reduce variance with control variates.

Result: On four QA benchmarks, the method matches or surpasses baseline uncertainty estimators while covering more semantic clusters given the same number of samples. It is modular, does not require gradient access to the base LLM, and acts as a drop-in enhancement for uncertainty estimation in risk-sensitive deployments.

Conclusion: The proposed diversity-steered sampling framework improves sample efficiency and semantic coverage for LLM uncertainty estimation without relying on model gradients, making it practical for risk-aware applications.

Abstract: Accurately estimating semantic aleatoric and epistemic uncertainties in large
language models (LLMs) is particularly challenging in free-form question
answering (QA), where obtaining stable estimates often requires many expensive
generations. We introduce a diversity-steered sampler that discourages
semantically redundant outputs during decoding, covers both autoregressive and
masked diffusion paradigms, and yields substantial sample-efficiency gains. The
key idea is to inject a continuous semantic-similarity penalty into the model's
proposal distribution using a natural language inference (NLI) model lightly
finetuned on partial prefixes or intermediate diffusion states. We debias
downstream uncertainty estimates with importance reweighting and shrink their
variance with control variates. Across four QA benchmarks, our method matches
or surpasses baselines while covering more semantic clusters with the same
number of samples. Being modular and requiring no gradient access to the base
LLM, the framework promises to serve as a drop-in enhancement for uncertainty
estimation in risk-sensitive model deployments.

</details>


### [20] [Typoglycemia under the Hood: Investigating Language Models' Understanding of Scrambled Words](https://arxiv.org/abs/2510.21326)
*Gianluca Sperduti,Alejandro Moreo*

Main category: cs.CL

TL;DR: 本文研究英文中 typoglycemia 对 NLP 模型鲁棒性的影响，通过对单词塌缩/歧义的定量分析、评估 BERT 的解歧能力，以及比较干净文本与含乱序文本训练的 BERT，发现乱序造成的性能下降小于预期。


<details>
  <summary>Details</summary>
Motivation: 解释为何模型在大量单词乱序时仍具鲁棒性，提出塌缩单词数量有限且塌缩词在上下文中通常具有明显的歧义消解线索，因此上下文对消歧具有关键作用。

Method: （1）利用英国家庭国家语料库（British National Corpus）量化 typoglycemia 下的单词塌缩与歧义情况；（2）评估 BERT 在区分塌缩形式上的能力；（3）通过在干净文本与 typoglycemic Wikipedia 文本上从头训练的 BERT 变体进行对比探测性实验。

Result: 结果显示因乱序导致的性能下降小于预期，BERT 能在一定程度上通过上下文和分布信息进行区分。

Conclusion: 在英语中塌缩词数量有限且上下文信息强烈指示性时，模型对字符级扰动具有鲁棒性；这为理解 NLP 系统在非理想文本中的表现以及设计更鲁棒的表示提供了参考。

Abstract: Research in linguistics has shown that humans can read words with internally
scrambled letters, a phenomenon recently dubbed typoglycemia. Some specific NLP
models have recently been proposed that similarly demonstrate robustness to
such distortions by ignoring the internal order of characters by design. This
raises a fundamental question: how can models perform well when many distinct
words (e.g., form and from) collapse into identical representations under
typoglycemia? Our work, focusing exclusively on the English language, seeks to
shed light on the underlying aspects responsible for this robustness. We
hypothesize that the main reasons have to do with the fact that (i) relatively
few English words collapse under typoglycemia, and that (ii) collapsed words
tend to occur in contexts so distinct that disambiguation becomes trivial. In
our analysis, we (i) analyze the British National Corpus to quantify word
collapse and ambiguity under typoglycemia, (ii) evaluate BERT's ability to
disambiguate collapsing forms, and (iii) conduct a probing experiment by
comparing variants of BERT trained from scratch on clean versus typoglycemic
Wikipedia text; our results reveal that the performance degradation caused by
scrambling is smaller than expected.

</details>


### [21] [TripTide: A Benchmark for Adaptive Travel Planning under Disruptions](https://arxiv.org/abs/2510.21329)
*Priyanshu Karmakar,Soumyabrata Chaudhuri,Shubhojit Mallick,Manish Gupta,Abhik Jana,Shreya Ghosh*

Main category: cs.CL

TL;DR: 提出 TripTide 基准，用于评估大语言模型在真实扰动下改写行程的能力；包含三重评价：自动度量、LLM 作为评审、以及人工专家评估；揭示顺序一致性、语义稳定性等特性及对行程长度的影响，确立对可适应性、个性化与韧性的评估基准。


<details>
  <summary>Details</summary>
Motivation: 现实旅行常遭遇取消、天气等扰动，需要鲁棒的行程修订能力；现有工作难以全面评估 LLM 在此场景的适应性。

Method: 设计 TripTide，按扰动严重程度和旅客容忍度建模；进行三重评估：自动指标（保持意图、响应性、适应性）、将 LLM 作为评审者、以及人类专家的人工评估；比较不同计划长度对空间、语义与序列的一致性与偏差；通过实验分析 LLM 的鲁棒性与局限。

Result: LLMs 在保持序列一致性和语义稳定性方面表现良好；较短行程的空间偏差较大，但随行程增大而趋于改善；但随着计划长度增加，扰动处理能力下降，暴露了鲁棒性瓶颈；TripTide 提供了评估适应性、个性化与在不确定条件下的韧性的基准。

Conclusion: TripTide 将推动对 LLM 在现实世界不确定性中的可适应性、定制化与鲁棒性 的系统评估，帮助改进基于 LLM 的旅行规划系统。

Abstract: Recent efforts like TripCraft and TravelPlanner have advanced the use of
Large Language Models ( LLMs) for personalized, constraint aware travel
itinerary generation. Yet, real travel often faces disruptions. To address
this, we present TripTide, the first benchmark evaluating LLM's ability to
revise itineraries under realistic disruptions. TripTide models key dimensions
such as disruption severity and traveler tolerance, enabling nuanced assessment
of LLM adaptability to events like flight cancellations, weather closures, or
overbooked attractions. We conduct a threefold evaluation. First, we introduce
automatic metrics including Preservation of Intent (how well the revised plan
maintains feasibility and goals), Responsiveness (promptness and
appropriateness of disruption handling), and Adaptability (semantic, spatial,
and sequential divergence between original and revised plans). Second, we apply
an LLM-as-a-judge approach to automatically assess revision quality. Third, we
perform manual expert evaluation to verify whether revisions preserve semantic,
spatial, sequential, and responsive aspects. Our experiments show that LLMs
maintain strong sequential consistency and semantic stability, while spatial
deviations are larger for shorter trips but decrease with longer ones,
indicating that extended plans encourage better geographic coherence. However,
disruption-handling ability declines as plan length increases, highlighting
limits in LLM robustness. TripTide establishes a benchmark for evaluating
adaptability, personalization, and resilience in LLM-based travel planning
under real-world uncertainty.

</details>


### [22] [Multi-turn Training with Basic Human Feedback Helps Little on LLM Reasoning](https://arxiv.org/abs/2510.21339)
*Qiang Liu,Wuganjing Song,Zhenzhou Lin,Feifan Chen,Qiaolong Cai,Chen Li,Yongduo Sui*

Main category: cs.CL

TL;DR: 单轮训练的模型在单轮与多轮评估中均具备良好泛化能力；三种多轮策略往往降低单轮推理表现，且对多轮评估的提升有限。


<details>
  <summary>Details</summary>
Motivation: 研究多轮人类反馈训练是否必要以提升推理能力，质疑此前关于多轮训练的普遍结论。

Method: 对比单轮训练与三种多轮策略，进行单轮与多轮评估，系统观察推理性能的变化。

Result: 单轮训练的模型在单轮与多轮任务上均表现良好；多轮训练的模型在单轮推理上显著下降；多轮训练对提升多轮评估收益有限，甚至无益，且可能削弱推理能力。

Conclusion: 对于信息完备任务，稳健的单轮训练更有效、可靠；多轮训练（带基本反馈）对推理的收益有限，可能降低能力。

Abstract: The reasoning capabilities of Large Language Models (LLMs) are typically
developed through the single-turn reinforcement learning, whereas real-world
applications often involve multi-turn interactions with human feedback, leading
to a potential mismatch between training and deployment conditions. In this
work, we study whether multi-turn training with human feedback is necessary for
reasoning tasks. We compare conventional single-turn training with three
multi-turn strategies and reach contrary conclusions to previous research. We
find that models trained in a single-turn setting generalize effectively to
both single- and multi-turn evaluations, while models trained with multi-turn
strategies exhibit a significant degradation in single-turn reasoning
performance. These results suggest that for tasks with complete information,
robust single-turn training remains more effective and reliable, as multi-turn
training with basic feedback provides limited benefits and can even degrade
reasoning capabilities.

</details>


### [23] [A Diagnostic Benchmark for Sweden-Related Factual Knowledge](https://arxiv.org/abs/2510.21360)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 提出一个以瑞典相关人物与事件为目标的问答基准，用以评估模型在瑞典知识上的事实回忆及跨语言一致性；实验显示在瑞典覆盖度强的小型模型可与三倍规模的多语言模型并驾齐驱；继续对瑞典语言进行预训练提升知识但也会遗忘部分信息。


<details>
  <summary>Details</summary>
Motivation: 现有以美国为中心的基准无法充分测试瑞典相关知识，需瑞典场景的专用基准来诊断模型的语言适应和知识保持能力，尤其在多语言模型的知识迁移与语言适配场景中。

Method: 手工编写的问答基准，灵感来自瑞典流行广播节目中的公众人物与瑞典重大体育赛事；数据集包含跨语言（英译）版本以检验跨语言一致性；对不同规模的模型进行评估，分析瑞典覆盖度对知识回忆的影响，以及在继续对瑞典进行预训练后的效果。

Result: 结果显示：具备较强瑞典覆盖度的小型模型在瑞典相关事实的回忆方面与三倍规模的多语言模型相当；持续对瑞典文本的预训练 generally 提升知识水平，但也造成部分先前信息的遗忘。

Conclusion: 该数据集具有作为诊断工具的潜力，可用于研究多语言模型在语言适应与知识保持方面的表现，特别是在语言迁移和知识更新场景中。

Abstract: Many Swedish benchmarks are translated US-centric benchmarks, and therefore
not suitable for testing knowledge that is particularly relevant, or even
specific, to Sweden. We therefore introduce a manually written
question-answering benchmark specifically targeted to Sweden-related
personalities and events, many of which receive very limited coverage in
international media. Our annotators drew inspiration from a popular radio
program featuring public figures from culture and media, as well as major
sports events in Sweden. The dataset can be used to measure factual recall
across models of varying sizes and degrees of Swedish coverage, and allows to
probe cross-lingual factual consistency as to contains English translations.
Using the dataset, we find that smaller models with stronger Swedish coverage
perform comparably to a three times larger multilingual model in recalling
Sweden-related facts. We also observe that continued pre-training on Swedish
generally improves factual knowledge but also leads to forgetting of a part of
the previously known information. These results demonstrate the dataset's
potential as a diagnostic tool for studying language adaptation and knowledge
retention in multilingual models and during language adaptation.

</details>


### [24] [SindBERT, the Sailor: Charting the Seas of Turkish NLP](https://arxiv.org/abs/2510.21364)
*Raphael Scheible-Schmitt,Stefan Schweter*

Main category: cs.CL

TL;DR: SindBERT 是首个面向土耳其的大规模 RoBERTa 编码器，揭示了数据规模对形态丰富语言的影响有限性；质量与多样性胜过单纯数据量，且土耳其基准可能已趋于饱和。


<details>
  <summary>Details</summary>
Motivation: 解决 morphologically rich languages 在大规模预训练中的 representation不足，以及评估在土耳其上扩展模型规模的边界与收益，探索数据质量与语料多样性在模型性能中的作用。

Method: 从头在 312 GB 土耳其文本（mC4、OSCAR23、维基百科）上训练 RoBERTa-base 和 RoBERTa-large 两个配置的编码器；公开评测包括 POS 标注、NER、 Offensive language detection、TurBLiMP 语言可接受性基准；并与现有土耳其及多语模型比较，最终以 MIT 许可证在 fairseq 和 Huggingface 发布。

Result: large 版本在四项任务中有两项取得最好分数，但总体未呈现稳定的缩放收益趋势，与 XLM-R、EuroBERT 相似呈现“平坦缩放”现象；相比数据量，语料质量与多样性对土耳其模型性能影响更显著；在小型、经精心挑选的模型（如 BERTurk）上，质量更高的语料同样能获得优越表现。 SindBERT 作为公开资源，为土耳其 NLP 提供基线，并提供对大规模预训练在 morphologically rich 语言中的边界与语料组成作用的实证研究。

Conclusion: SindBERT 证明在土耳其等形态丰富语言上，扩展规模并不总是带来线性收益，语料的质量与多样性往往比单纯数据量更关键；该工作同时提供可重复的资源，推动对 scaling 限制的系统性理解并强调语料组成在 Turkish NLP 中的核心地位。

Abstract: Transformer models have revolutionized NLP, yet many morphologically rich
languages remain underrepresented in large-scale pre-training efforts. With
SindBERT, we set out to chart the seas of Turkish NLP, providing the first
large-scale RoBERTa-based encoder for Turkish. Trained from scratch on 312 GB
of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base
and large configurations, representing the first large-scale encoder-only
language model available for Turkish. We evaluate SindBERT on part-of-speech
tagging, named entity recognition, offensive language detection, and the
TurBLiMP linguistic acceptability benchmark. Our results show that SindBERT
performs competitively with existing Turkish and multilingual models, with the
large variant achieving the best scores in two of four tasks but showing no
consistent scaling advantage overall. This flat scaling trend, also observed
for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be
saturated. At the same time, comparisons with smaller but more curated models
such as BERTurk highlight that corpus quality and diversity can outweigh sheer
data volume. Taken together, SindBERT contributes both as an openly released
resource for Turkish NLP and as an empirical case study on the limits of
scaling and the central role of corpus composition in morphologically rich
languages. The SindBERT models are released under the MIT license and made
available in both fairseq and Huggingface formats.

</details>


### [25] [HalleluBERT: Let every token that has meaning bear its weight](https://arxiv.org/abs/2510.21372)
*Raphael Scheible-Schmitt*

Main category: cs.CL

TL;DR: HalleluBERT是基于RoBERTa的希伯来语编码器家族，基于从头训练，覆盖base和large两个尺度，在49.1 GB去重的希伯来语网页文本和维基百科上用希伯来语特定的字节级BPE词汇表进行训练，在NER和情感分析任务上超越单语和多语言基线，达到希伯来语的新状态最优，显示了完全收敛的单语预训练的优点。


<details>
  <summary>Details</summary>
Motivation: 希伯来语缺乏规模化的RoBERTa编码器，现有模型在语料规模、词汇表或训练深度方面受限，需要一个从头训练并充分收敛的单语预训练模型来提升下游任务表现。

Method: 提出RoBERTa为基础的编码器系列（base和large），在从头开始的训练中使用49.1 GB去重的希伯来网页文本与维基百科，并采用希伯来语特定的字节级BPE词汇表进行预训练。

Result: 在命名实体识别（NER）和情感分类等基准上，HalleluBERT超越了单语和多语言基线，成为希伯来语的最新状态最优模型。该模型展示了完全收敛的单语预训练的优势。

Conclusion: 完全收敛的单语预训练对希伯来语NLP具有显著提升，HalleluBERT作为新一代希伯来语编码器，提供了更强的下游任务性能与语言理解能力。

Abstract: Transformer-based models have advanced NLP, yet Hebrew still lacks a
large-scale RoBERTa encoder which is extensively trained. Existing models such
as HeBERT, AlephBERT, and HeRo are limited by corpus size, vocabulary, or
training depth. We present HalleluBERT, a RoBERTa-based encoder family (base
and large) trained from scratch on 49.1~GB of deduplicated Hebrew web text and
Wikipedia with a Hebrew-specific byte-level BPE vocabulary. Evaluated on NER
and sentiment classification benchmarks, HalleluBERT outperforms both
monolingual and multilingual baselines. HalleluBERT sets a new state of the art
for Hebrew and highlights the benefits of fully converged monolingual
pretraining.

</details>


### [26] [Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings](https://arxiv.org/abs/2510.21424)
*Abderrazek Abid,Thanh-Cong Ho,Fakhri Karray*

Main category: cs.CL

TL;DR: 提出使用视觉语言模型（VLMs）进行HAR的研究，构建描述性字幕数据集并提出评估方法，比较VLMs与SOTA深度模型，结果显示VLMs在准确性上可媲美甚至优于传统方法，旨在为智能健康系统的VLM应用提供基准。


<details>
  <summary>Details</summary>
Motivation:  HAR在远程健康监测中的挑战在于需求灵活性、可扩展性以及对动态非确定性输出的评估困难，VLMs具有潜在优势，因而研究其在HAR中的可用性与评估框架。

Method: 构建描述性字幕数据集，设计针对HAR的全面评估方法；在一组基准任务上与当前SOTA深度学习模型进行对比实验。

Result: VLMs在准确性方面与SOTA方法相当，某些场景甚至超越传统方法；为VLM在健康领域的应用提供强基准。

Conclusion: 本工作为将VLM融入智能医疗系统提供新的可能性，并指明未来在数据集、评估标准、跨域推广及实际部署方面的研究方向。

Abstract: As generative AI continues to evolve, Vision Language Models (VLMs) have
emerged as promising tools in various healthcare applications. One area that
remains relatively underexplored is their use in human activity recognition
(HAR) for remote health monitoring. VLMs offer notable strengths, including
greater flexibility and the ability to overcome some of the constraints of
traditional deep learning models. However, a key challenge in applying VLMs to
HAR lies in the difficulty of evaluating their dynamic and often
non-deterministic outputs. To address this gap, we introduce a descriptive
caption data set and propose comprehensive evaluation methods to evaluate VLMs
in HAR. Through comparative experiments with state-of-the-art deep learning
models, our findings demonstrate that VLMs achieve comparable performance and,
in some cases, even surpass conventional approaches in terms of accuracy. This
work contributes a strong benchmark and opens new possibilities for the
integration of VLMs into intelligent healthcare systems.

</details>


### [27] [REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring](https://arxiv.org/abs/2510.21445)
*Thanh Cong Ho,Farah Kharrat,Abderrazek Abid,Fakhri Karray*

Main category: cs.CL

TL;DR: RemONI is an autonomous remote health monitoring system that integrates multimodal large language models (MLLMs), IoT, and wearables to continuously collect vital signs, accelerometer data, and video data; it includes anomaly detection (including fall detection), activity and emotion recognition via NLP, and a clinician-facing web interface. Prototype demonstrates feasibility and scalability for real-world use and potential reduction in clinician workload and healthcare costs.


<details>
  <summary>Details</summary>
Motivation: There is a gap in human–machine interaction within remote patient monitoring. Existing work focuses on data collection and analysis but lacks integrated, user-friendly AI-driven interfaces for clinicians and timely, context-aware alerts.

Method: A system architecture that aggregates data from wearables (vital signs, accelerometer) and cameras, processes data with anomaly detection (including fall detection), employs multimodal LLMs to recognize activity and emotion and respond to inquiries, uses prompt engineering to fuse patient information, and provides a web-based interface for real-time monitoring and alerts.

Result: The authors demonstrate implementability and scalability through a full-fledged prototype tested in scenarios, indicating feasibility for real-life deployment and potential workload and cost reductions for healthcare professionals.

Conclusion: REMONI represents a promising integration of IoT, wearables, video data, and MLLMs for autonomous remote health monitoring, with real-time monitoring, mood/activity recognition, and clinician interaction; further real-world validation and refinement are anticipated.

Abstract: With the widespread adoption of wearable devices in our daily lives, the
demand and appeal for remote patient monitoring have significantly increased.
Most research in this field has concentrated on collecting sensor data,
visualizing it, and analyzing it to detect anomalies in specific diseases such
as diabetes, heart disease and depression. However, this domain has a notable
gap in the aspect of human-machine interaction. This paper proposes REMONI, an
autonomous REmote health MONItoring system that integrates multimodal large
language models (MLLMs), the Internet of Things (IoT), and wearable devices.
The system automatically and continuously collects vital signs, accelerometer
data from a special wearable (such as a smartwatch), and visual data in patient
video clips collected from cameras. This data is processed by an anomaly
detection module, which includes a fall detection model and algorithms to
identify and alert caregivers of the patient's emergency conditions. A
distinctive feature of our proposed system is the natural language processing
component, developed with MLLMs capable of detecting and recognizing a
patient's activity and emotion while responding to healthcare worker's
inquiries. Additionally, prompt engineering is employed to integrate all
patient information seamlessly. As a result, doctors and nurses can access
real-time vital signs and the patient's current state and mood by interacting
with an intelligent agent through a user-friendly web application. Our
experiments demonstrate that our system is implementable and scalable for
real-life scenarios, potentially reducing the workload of medical professionals
and healthcare costs. A full-fledged prototype illustrating the functionalities
of the system has been developed and being tested to demonstrate the robustness
of its various capabilities.

</details>


### [28] [MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization](https://arxiv.org/abs/2510.21473)
*Chenglong Wang,Yang Gan,Hang Zhou,Chi Hu,Yongyu Mu,Kai Song,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Jingbo Zhu,Zhengtao Yu,Tong Xiao*

Main category: cs.CL

TL;DR: 提出 MRO，提升扩散语言模型的推理能力，通过建模 denoising 步骤中 token 的 intra-/inter-sequence 相关性，并结合测试时缩放、拒绝采样与强化学习优化，辅以分组步进和重要性采样以降低方差、提升效率，实验显示在推理基准上显著提升并实现采样加速。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散语言模型在近年取得进展，但在推理能力方面仍落后于自回归大语言模型，且在 denoising 步骤较少时差距更明显。独立地生成每一步的掩码 token 未能捕捉 token 之间的相关性，导致推理性能下降。

Method: 定义两类 token 相关性：序内相关（intra-sequence）与序间相关（inter-sequence）；提出多目标奖励优化（Multi-Reward Optimization, MRO）以在去噪过程中直接优化 token 相关性。MRO 通过测试时缩放、拒绝采样和强化学习来直接优化多种奖励信号以提升相关性。另引入分组步进（group step）与重要性采样（importance sampling）以降低奖励方差、提升采样效率。

Result: 大量实验表明，MRO 能显著提升推理性能，并实现显著的采样速度提升，同时在推理基准上保持高性能。

Conclusion: 通过对 token 相关性的显式建模与优化，MRO 为扩散语言模型的去噪过程提供一种直接、有效的提升路径，改善推理能力并提升采样效率，体现了利用相关性建模提升生成模型性能的潜力。

Abstract: Recent advances in diffusion language models (DLMs) have presented a
promising alternative to traditional autoregressive large language models
(LLMs). However, DLMs still lag behind LLMs in reasoning performance,
especially as the number of denoising steps decreases. Our analysis reveals
that this shortcoming arises primarily from the independent generation of
masked tokens across denoising steps, which fails to capture the token
correlation. In this paper, we define two types of token correlation:
intra-sequence correlation and inter-sequence correlation, and demonstrate that
enhancing these correlations improves reasoning performance. To this end, we
propose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to
consider the token correlation during the denoising process. More specifically,
our MRO approach leverages test-time scaling, reject sampling, and
reinforcement learning to directly optimize the token correlation with multiple
elaborate rewards. Additionally, we introduce group step and importance
sampling strategies to mitigate reward variance and enhance sampling
efficiency. Through extensive experiments, we demonstrate that MRO not only
improves reasoning performance but also achieves significant sampling speedups
while maintaining high performance on reasoning benchmarks.

</details>


### [29] [Brain-tuning Improves Generalizability and Efficiency of Brain Alignment in Speech Models](https://arxiv.org/abs/2510.21520)
*Omer Moussa,Mariya Toneva*

Main category: cs.CL

TL;DR: 提出一种可扩展的多参与者脑调优方法，通过对大规模预训练语言模型进行微调以同时预测多名参与者的fMRI反应，从而提高跨参与者的一致性和泛化能力，同时在语义任务上也有提升。


<details>
  <summary>Details</summary>
Motivation: 现有脑对齐方法对每个参与者的数据量高度敏感，难以泛化到新参与者和进行整体人口层面分析，需要一种能在多参与者数据上共同训练、具备良好泛化能力的解决方案。

Method: 在多参与者fMRI数据上对预训练的语音-语言模型进行微调，使模型能够联合预测多名参与者的fMRI响应，形成一个可扩展的脑对齐模型。

Result: 实现了对新参与者的数据需求降低约5倍；总体脑对齐提升可达约50%；对未见数据集具备良好泛化能力；并且在下游语义任务上也有性能提升，显示脑数据训练有助于获得更通用的语义表征。

Conclusion: 多参与者脑调优实现了神经科学与人工智能的双向收益，促进对语言处理脑机制的通用化理解，并提供可公开获取的代码与模型以促进研究复现与扩展。

Abstract: Pretrained language models are remarkably effective in aligning with human
brain responses elicited by natural language stimuli, positioning them as
promising model organisms for studying language processing in the brain.
However, existing approaches for both estimating and improving this brain
alignment are participant-dependent and highly affected by the amount of data
available per participant, hindering both generalization to new participants
and population-level analyses. In this work, we address these limitations by
introducing a scalable, generalizable brain-tuning method, in which we
fine-tune pretrained speech language models to jointly predict fMRI responses
from multiple participants. We demonstrate that the resulting brain-tuned
models exhibit strong individual brain alignment while generalizing across
participants. Specifically, our method leads to 1) a 5-fold decrease in the
amount of fMRI data needed to predict brain data from new participants, 2) up
to a 50% increase in the overall brain alignment, and 3) strong generalization
to new unseen datasets. Furthermore, this multi-participant brain-tuning
additionally improves downstream performance on semantic tasks, suggesting that
training using brain data from multiple participants leads to more
generalizable semantic representations. Taken together, these findings
demonstrate a bidirectional benefit between neuroscience and AI, helping bridge
the gap between the two fields. We make our code and models publicly available
at https://github.com/bridge-ai-neuro/multi-brain-tuning.

</details>


### [30] [Document Understanding, Measurement, and Manipulation Using Category Theory](https://arxiv.org/abs/2510.21553)
*Jared Claypoole,Yunye Gong,Noson S. Yanofsky,Ajay Divakaran*

Main category: cs.CL

TL;DR: 用范畴论来提取多模态文档结构，并基于此开发信息度量、摘要、扩展，以及自监督改进大模型的一整套方法，覆盖从文档的问答对范畴表示，到信息分解的正交化、到基于率失真的摘要分析，以及多模态扩展与 RLVR 自监督训练。


<details>
  <summary>Details</summary>
Motivation: 解决多模态文档理解中的结构化表示与信息度量问题，提供统一的理论框架以处理信息的分解、组合性和闭合性，并提升摘要质量、文档扩展能力以及大规模预训练模型的自监督学习效果。

Method: 将文档建模为问答对的范畴；提出正交化过程将信息从一个或多个文档中分解成非重叠的部分；据此开发信息量度量和枚举方法，进一步形成新的摘要技术与文档扩展（exegesis）；对摘要进行率失真分析以评估效果；在大规模预训练模型上实现，并提出多模态扩展；提出基于 RLVR 的自监督方法，利用一致性约束（如可组合性和对某些运算的闭合性）来提升模型。

Result: 得到一套信息量度量与可枚举的信息分解框架，若干新型摘要与文档扩展（exegesis）技术，以及一个基于率失真分析的摘要评估方法；在大模型上实现并扩展到多模态场景，提出了一个自监督提升路径（RLVR）。

Conclusion: 范畴论提供的统一框架成功地描述多模态文档结构和信息流，带来新的分解、摘要、扩展与自监督提升的研究方向，具有潜在提升大模型性能和理解能力的前景。

Abstract: We apply category theory to extract multimodal document structure which leads
us to develop information theoretic measures, content summarization and
extension, and self-supervised improvement of large pretrained models. We first
develop a mathematical representation of a document as a category of
question-answer pairs. Second, we develop an orthogonalization procedure to
divide the information contained in one or more documents into non-overlapping
pieces. The structures extracted in the first and second steps lead us to
develop methods to measure and enumerate the information contained in a
document. We also build on those steps to develop new summarization techniques,
as well as to develop a solution to a new problem viz. exegesis resulting in an
extension of the original document. Our question-answer pair methodology
enables a novel rate distortion analysis of summarization techniques. We
implement our techniques using large pretrained models, and we propose a
multimodal extension of our overall mathematical framework. Finally, we develop
a novel self-supervised method using RLVR to improve large pretrained models
using consistency constraints such as composability and closure under certain
operations that stem naturally from our category theoretic framework.

</details>


### [31] [Are the LLMs Capable of Maintaining at Least the Language Genus?](https://arxiv.org/abs/2510.21561)
*Sandra Mitrović,David Kletz,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文分析大语言模型在多语言任务中对语言谱系（基因/ genera）的敏感性，发现 genus-level 效应存在，但受训练资源分布强烈制约；不同模型族展现不同的多语言策略，数据不平衡是主因。


<details>
  <summary>Details</summary>
Motivation: 探索语言谱系结构在影响LLMs多语言表现中的作用，补充对多语言数据资源分布不均对模型行为影响的理解。

Method: 在MultiQ数据集的基础上扩展分析，先检验当提示语言保真性下降时模型是否更倾向切换到谱系相关语言；再比较同一语言族内外的知识一致性；比较不同模型族的策略差异。

Result: 存在基因谱系层面的效应，但强烈受训练资源可用性条件约束；不同LLM家族呈现出不同的多语言策略；数据不平衡是塑造多语言表现的主要因素；模型在一定程度上编码了 genus-level 结构。

Conclusion: 基因谱系结构在一定程度上被LLMs编码，但实际多语言性能仍主要受训练数据分布的影响，需要通过数据平衡和覆盖来提升跨族群的鲁棒性。

Abstract: Large Language Models (LLMs) display notable variation in multilingual
behavior, yet the role of genealogical language structure in shaping this
variation remains underexplored. In this paper, we investigate whether LLMs
exhibit sensitivity to linguistic genera by extending prior analyses on the
MultiQ dataset. We first check if models prefer to switch to genealogically
related languages when prompt language fidelity is not maintained. Next, we
investigate whether knowledge consistency is better preserved within than
across genera. We show that genus-level effects are present but strongly
conditioned by training resource availability. We further observe distinct
multilingual strategies across LLMs families. Our findings suggest that LLMs
encode aspects of genus-level structure, but training data imbalances remain
the primary factor shaping their multilingual performance.

</details>


### [32] [From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene](https://arxiv.org/abs/2510.21575)
*Mojca Brglez,Špela Vintar*

Main category: cs.CL

TL;DR: 首次道德性语言推理（pragmatics）基准：SloPragEval 与 SloPragMega，涵盖 405 道多选题，聚焦斯洛文尼亚语的语用理解；LLMs 在细微语言理解方面有所进步，但对非字面含义、文化特定表达的隐含意义把握仍显不足，专有模型与开源模型存在显著差距；强调基于原生数据和人工验证的设计原则。


<details>
  <summary>Details</summary>
Motivation: 推动超越表层语法/语义的评估，关注语用理解及文化背景在大语言模型中的表现。针对斯洛文尼亚语缺乏语用基准的问题，提出专门的评估集合以测试语境、隐含意义和文化规范的理解能力。

Method: 提出 SloPragEval 与 SloPragMega 两套基准，总计 405 道多选题；讨论翻译难点，建立人类基线的尝试，进行对大模型的初步评估。

Result: 现有模型在理解细微语言方面有显著进步，但在推断非字面表达中的隐含说话者意图（尤其是文化相关表达）方面仍不足；专有模型与开源模型之间存在明显差距。

Conclusion: 测试应以原生数据构建并通过人工回应验证，针对复杂语言理解和目标文化知识的基准需谨慎设计，以确保对实际语用能力的有效评估。

Abstract: Large language models are demonstrating increasing capabilities, excelling at
benchmarks once considered very difficult. As their capabilities grow, there is
a need for more challenging evaluations that go beyond surface-level linguistic
competence. Namely, language competence involves not only syntax and semantics
but also pragmatics, i.e., understanding situational meaning as shaped by
context as well as linguistic and cultural norms. To contribute to this line of
research, we introduce SloPragEval and SloPragMega, the first pragmatics
understanding benchmarks for Slovene that contain altogether 405
multiple-choice questions. We discuss the difficulties of translation, describe
the campaign to establish a human baseline, and report pilot evaluations with
LLMs. Our results indicate that current models have greatly improved in
understanding nuanced language but may still fail to infer implied speaker
meaning in non-literal utterances, especially those that are culture-specific.
We also observe a significant gap between proprietary and open-source models.
Finally, we argue that benchmarks targeting nuanced language understanding and
knowledge of the target culture must be designed with care, preferably
constructed from native data, and validated with human responses.

</details>


### [33] [Automated Quality Control for Language Documentation: Detecting Phonotactic Inconsistencies in a Kokborok Wordlist](https://arxiv.org/abs/2510.21584)
*Kellen Parker van Dam,Abishek Stephen*

Main category: cs.CL

TL;DR: 无监督异常检测可在语言记录的词表中识别潜在的音系不一致，尤其在以 syllable 为单位的特征上对拼写错误与借用词的识别优于字符级特征；对 Kokborok 与 Bangla 的多语言数据集适用，帮助现场工作者标记需要核实的条目，提升低资源语言数据质量。


<details>
  <summary>Details</summary>
Motivation: 语言记录中的转录错误和未记录的借用词会干扰分析，亟需自动化工具在词表中发现异常以改进数据质量，特别是在低资源语言场景。

Method: 使用无监督异常检测，提取字符级和音节级的音系特征，对多语言的 Kokborok 变体与 Bangla 数据集进行分析，目标是在高召回的前提下识别潜在转录错误和借用。

Result: 能够识别潜在的转录错误与借用，音节级特征显著优于字符级基线；虽然精确度和召回率受异常微妙性质影响仍然有限，但高召回方法可为田野工作者提供系统化的标注入口以便核实。

Conclusion: 提出的无监督方法为低资源语言数据质量提升提供了可操作的工作流，尤其是在标注需要人工核验的条目方面表现出价值，并具备在其他语言接入数据中的扩展潜力。

Abstract: Lexical data collection in language documentation often contains
transcription errors and undocumented borrowings that can mislead linguistic
analysis. We present unsupervised anomaly detection methods to identify
phonotactic inconsistencies in wordlists, applying them to a multilingual
dataset of Kokborok varieties with Bangla. Using character-level and
syllable-level phonotactic features, our algorithms identify potential
transcription errors and borrowings. While precision and recall remain modest
due to the subtle nature of these anomalies, syllable-aware features
significantly outperform character-level baselines. The high-recall approach
provides fieldworkers with a systematic method to flag entries requiring
verification, supporting data quality improvement in low-resourced language
documentation.

</details>


### [34] [RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models](https://arxiv.org/abs/2510.21604)
*Xueyuan Lin,Cehao Yang,Ye Ma,Ming Li,Rongjunchen Zhang,Yang Ni,Xiaojun Wu,Chengjin Xu,Jian Guo,Hui Xiong*

Main category: cs.CL

TL;DR: RETuning is a pre-reinforcement-learning cold-start method guiding LLMs to build an independent analytical framework from diverse information sources, score evidence, and reflect to predict stock movements, improving reasoning in finance.


<details>
  <summary>Details</summary>
Motivation: LLMs tend to echo analysts and produce unweighted, multi-source summaries; they struggle with adversarial evidence and lack independent formatting of evidence necessary for robust stock-movement prediction. The paper targets 3-class (up/hold/down) classification in finance and seeks to unlock systematic reasoning rather than surface-level correlations.

Method: During CoT generation, RETuning prompts the model to dynamically construct an analytical framework and organize/evaluate evidence from diverse sources (price, news, analysts’ opinions, reports, macro data, similar stocks) based on that framework, then reflect to derive the final prediction. This cold-start stage precedes reinforcement learning. A large-scale 2024 stock dataset (5,123 A-shares) with long contexts (32K tokens) and 200K+ samples is built for training/evaluation.

Result: Experiments show RETuning unlocks the model’s reasoning ability in the financial domain. Inference-time scaling remains effective up to 6 months and on out-of-distribution stocks as models gain valuable insights for stock movement prediction.

Conclusion: RETuning helps the model align with an explicit analytical framework, reduces undue context influence, promotes independent logical reasoning, and enhances robustness and generalization in stock-m movement prediction within finance.

Abstract: Recently, large language models (LLMs) have demonstrated outstanding
reasoning capabilities on mathematical and coding tasks. However, their
application to financial tasks-especially the most fundamental task of stock
movement prediction-remains underexplored. We study a three-class
classification problem (up, hold, down) and, by analyzing existing reasoning
responses, observe that: (1) LLMs follow analysts' opinions rather than exhibit
a systematic, independent analytical logic (CoTs). (2) LLMs list summaries from
different sources without weighing adversarial evidence, yet such
counterevidence is crucial for reliable prediction. It shows that the model
does not make good use of its reasoning ability to complete the task. To
address this, we propose Reflective Evidence Tuning (RETuning), a cold-start
method prior to reinforcement learning, to enhance prediction ability. While
generating CoT, RETuning encourages dynamically constructing an analytical
framework from diverse information sources, organizing and scoring evidence for
price up or down based on that framework-rather than on contextual
viewpoints-and finally reflecting to derive the prediction. This approach
maximally aligns the model with its learned analytical framework, ensuring
independent logical reasoning and reducing undue influence from context. We
also build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,
with long contexts (32K tokens) and over 200K samples. In addition to price and
news, it incorporates analysts' opinions, quantitative reports, fundamental
data, macroeconomic indicators, and similar stocks. Experiments show that
RETuning successfully unlocks the model's reasoning ability in the financial
domain. Inference-time scaling still works even after 6 months or on
out-of-distribution stocks, since the models gain valuable insights about stock
movement prediction.

</details>


### [35] [The Universal Landscape of Human Reasoning](https://arxiv.org/abs/2510.21623)
*Qiguang Chen,Jinhao Liu,Libo Qin,Yimeng Zhang,Yihao Liang,Shangxu Ren,Chengyu Luan,Dengyun Peng,Hanjing Li,Jiannan Guan,Zheng Yan,Jiaqi Wang,Mengkang Hu,Yantao Du,Zhi Chen,Xie Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出IF-Track，一种以大型语言模型作为概率编码器、在单一度量空间中量化信息熵与信息增益的推理信息流分析框架，揭示普遍的推理行为、系统性误差与个体差异。


<details>
  <summary>Details</summary>
Motivation: 弥合从经典逻辑到概率模型的输出导向和对动态推理的量化描述之间的鸿沟，提供统一、可度量的推理动力学描述。

Method: 引入IF-Track，利用LLMs作为概率编码器，量化每一步推理的信息熵与信息增益；在多样化任务上进行细粒度分析，构建在单一度量空间中的普遍推理景观；评估要点包括特征提取、误差模式、个体差异。

Result: IF-Track捕捉核心推理特征，识别系统性误差模式，刻画个体差异；与单/双过程理论对齐，揭示人工与人类认知的一致性，以及LLMs对人类推理的潜在重塑。

Conclusion: 建立理论与测量之间的定量桥梁，提供对推理结构的机制性洞见，推动将理论与实证观测统一的量化框架。

Abstract: Understanding how information is dynamically accumulated and transformed in
human reasoning has long challenged cognitive psychology, philosophy, and
artificial intelligence. Existing accounts, from classical logic to
probabilistic models, illuminate aspects of output or individual modelling, but
do not offer a unified, quantitative description of general human reasoning
dynamics. To solve this, we introduce Information Flow Tracking (IF-Track),
that uses large language models (LLMs) as probabilistic encoder to quantify
information entropy and gain at each reasoning step. Through fine-grained
analyses across diverse tasks, our method is the first successfully models the
universal landscape of human reasoning behaviors within a single metric space.
We show that IF-Track captures essential reasoning features, identifies
systematic error patterns, and characterizes individual differences. Applied to
discussion of advanced psychological theory, we first reconcile single- versus
dual-process theories in IF-Track and discover the alignment of artificial and
human cognition and how LLMs reshaping human reasoning process. This approach
establishes a quantitative bridge between theory and measurement, offering
mechanistic insights into the architecture of reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Incentivizing Consistent, Effective and Scalable Reasoning Capability in Audio LLMs via Reasoning Process Rewards](https://arxiv.org/abs/2510.20867)
*Jiajun Fan,Roger Ren,Jingyuan Li,Rahul Pandey,Prashanth Gurunath Shivakumar,Ivan Bulyko,Ankur Gandhe,Ge Liu,Yile Gu*

Main category: cs.LG

TL;DR: CESAR通过强化学习来训练音频LLMs的推理能力，解决推理长期链条导致性能下降的问题，实现可解释且可扩展的推理，并在 MMAU Test-mini 和 MMSU 任务上达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 在音频多模态大语言模型中，推理过程在推断阶段往往降低性能，存在测试时逆向放缩的问题。现有训练不足导致推理产生幻觉和不一致，且错误会随推理链增长而累积，因此需要通过引导推理过程的训练来提升推理质量。

Method: 提出CESAR框架，基于在线强化学习，采用 Group Relative Policy Optimization，并设计多元奖励体系，涵盖正确性、格式、稳定性、一致性、结构化分析模式、因果推理、领域知识整合和推理深度的量化激励。通过奖励推理过程而非仅结果，缓解测试时的逆向放缩。

Result: 在 MMAU Test-mini 上达到 state-of-the-art，显著超越 Gemini 2.5 Pro 和 GPT-4o Audio；在 MMSU 推理任务达到接近人类水平；通过 AI 评测与定性比较验证推理质量的提升，并且推理能力的提升还能带来多模态推理与感知能力的协同提升。

Conclusion: CESAR 提供一种 principled 的方法来构建健壮、可扩展的音频 LLM 推理能力，解决测试时逆向放缩问题，揭示模型特定的“推理甜点”，为跨模态推理与感知的提升奠定基础。

Abstract: The role of reasoning in Audio Large Language Models remains widely
underexplored, as introducing a reasoning process often degrades rather than
improves performance during inference, a phenomenon we term test-time inverse
scaling, where longer reasoning chains yield progressively worse results. We
demonstrate that this stems not from fundamental limitations of reasoning
itself, but from inadequate training: models without proper guidance for the
reasoning process produce hallucinatory, inconsistent reasoning that
accumulates errors over longer chains. To address these challenges, we
introduce CESAR (Consistent, Effective, and Scalable Audio Reasoners), shifting
from outcome verification to rewarding the reasoning process. Our online
reinforcement learning framework employs Group Relative Policy Optimization
with a multi-faceted reward suite that incentivizes not only correctness and
format but also consistency, structured analytical patterns, causal reasoning,
domain-knowledge integration, and calibrated reasoning depth. CESAR resolves
test-time inverse scaling, transforming reasoning from detriments into gains
while revealing model-specific ``reasoning sweet spots", where performance
peaks during test-time scaling. We achieve state-of-the-art results on MMAU
Test-mini, substantially outperforming Gemini 2.5 Pro and GPT-4o Audio, and
near-human-level performance on MMSU reasoning tasks. Through AI-as-judge
evaluations and qualitative comparisons, we provide both quantitative and
qualitative validation of our improved reasoning quality. Importantly, enhanced
reasoning creates synergistic effects, simultaneously improving multimodal
reasoning and perception capabilities. Overall, CESAR establishes a principled
method for developing robust and scalable reasoning in Audio LLMs.

</details>


### [37] [MOBO-OSD: Batch Multi-Objective Bayesian Optimization via Orthogonal Search Directions](https://arxiv.org/abs/2510.20872)
*Lam Ngo,Huong Ha,Jeffrey Chan,Hongyu Zhang*

Main category: cs.LG

TL;DR: MOBO-OSD 通过沿正交搜索方向解决子问题，结合帕累托前沿估计与批量评估，生成多目标贝叶斯优化的多样且密集的 Pareto 前沿，优于现有方法，并提供实现代码。


<details>
  <summary>Details</summary>
Motivation: 多目标优化的困难在于需要在目标空间实现多样性与高密度的 Pareto 解集，同时在高维目标下计算成本高。现有方法在覆盖性或并行性方面存在不足，因此需要一种能在较少子问题下获得更广覆盖和更密集解集的策略。

Method: 提出 MOBO-OSD：在以近似目标最小值的凸壳为基准构造的正交搜索方向上解若干子问题，每个子问题对应一个方向，从而得到分布良好的解集。引入帕累托前沿估计技术，在现有解附近生成额外解以增加密度，同时支持批量优化以实现并行评估。

Result: 在2-6个目标的多种合成与真实基准函数上进行广泛实验，MOBO-OSD 在多种指标上（包括超体积等）持续优于最先进的方法。

Conclusion: MOBO-OSD 能在保持多样性的同时提升解集密度，且支持批量评估，具有较强的实用性与可扩展性，代码已在 GitHub 提供。

Abstract: Bayesian Optimization (BO) is a powerful tool for optimizing expensive
black-box objective functions. While extensive research has been conducted on
the single-objective optimization problem, the multi-objective optimization
problem remains challenging. In this paper, we propose MOBO-OSD, a
multi-objective Bayesian Optimization algorithm designed to generate a diverse
set of Pareto optimal solutions by solving multiple constrained optimization
problems, referred to as MOBO-OSD subproblems, along orthogonal search
directions (OSDs) defined with respect to an approximated convex hull of
individual objective minima. By employing a well-distributed set of OSDs,
MOBO-OSD ensures broad coverage of the objective space, enhancing both solution
diversity and hypervolume performance. To further improve the density of the
set of Pareto optimal candidate solutions without requiring an excessive number
of subproblems, we leverage a Pareto Front Estimation technique to generate
additional solutions in the neighborhood of existing solutions. Additionally,
MOBO-OSD supports batch optimization, enabling parallel function evaluations to
accelerate the optimization process when resources are available. Through
extensive experiments and analysis on a variety of synthetic and real-world
benchmark functions with two to six objectives, we demonstrate that MOBO-OSD
consistently outperforms the state-of-the-art algorithms. Our code
implementation can be found at https://github.com/LamNgo1/mobo-osd.

</details>


### [38] [Multimodal Negative Learning](https://arxiv.org/abs/2510.20877)
*Baoquan Gong,Xiyuan Gao,Pengfei Zhu,Qinghua Hu,Bing Cao*

Main category: cs.LG

TL;DR: 提出多模态负学习（Multimodal Negative Learning, MNL）来解决模态不平衡问题。通过让主导模态动态引导弱模态抑制非目标类别，而非强制对齐，从而保留弱模态的特有信息，同时提升鲁棒性；理论推导出增强Unimodal Confidence Margin（UCoM）并收敛鲁棒下界，实验在噪声和不平衡场景下显示强泛化性，代码公开。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常遇到模态不平衡问题，强模态主导导致弱模态信息被压制。现有的“学习一致”策略容易丧失弱模态的独特信息。需要一种能够在保持信息多样性的同时提升鲁棒性的学习范式。

Method: 提出多模态负学习（MNL）框架，利用动态引导机制让主导模态对弱模态进行负向学习：抑制非目标类别，保持对目标类别的关注。给出理论推导，证明该框架能够提升Unimodal Confidence Margin（UCoM）并收窄鲁棒下界，提高弱模态的鲁棒性与表现。实现上给出可训练的目标函数和优化流程，并在不同数据集上进行验证。

Result: 在多个基准数据集上表现出显著提升，相比对比方法对弱模态的鲁棒性和泛化性更好，尤其在噪声与数据不平衡场景下效果显著。理论分析与实验证据一致，方法具有良好泛化性。

Conclusion: 负学习为多模态学习提供了一个新的鲁棒方向，通过动态引导抑制非目标以保留模态特异信息，提升鲁棒性并扩大适用范围。代码将公开。

Abstract: Multimodal learning systems often encounter challenges related to modality
imbalance, where a dominant modality may overshadow others, thereby hindering
the learning of weak modalities. Conventional approaches often force weak
modalities to align with dominant ones in "Learning to be (the same)" (Positive
Learning), which risks suppressing the unique information inherent in the weak
modalities. To address this challenge, we offer a new learning paradigm:
"Learning Not to be" (Negative Learning). Instead of enhancing weak modalities'
target-class predictions, the dominant modalities dynamically guide the weak
modality to suppress non-target classes. This stabilizes the decision space and
preserves modality-specific information, allowing weak modalities to preserve
unique information without being over-aligned. We proceed to reveal multimodal
learning from a robustness perspective and theoretically derive the Multimodal
Negative Learning (MNL) framework, which introduces a dynamic guidance
mechanism tailored for negative learning. Our method provably tightens the
robustness lower bound of multimodal learning by increasing the Unimodal
Confidence Margin (UCoM) and reduces the empirical error of weak modalities,
particularly under noisy and imbalanced scenarios. Extensive experiments across
multiple benchmarks demonstrate the effectiveness and generalizability of our
approach against competing methods. The code will be available at
https://github.com/BaoquanGong/Multimodal-Negative-Learning.git.

</details>


### [39] [HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data Placement](https://arxiv.org/abs/2510.20878)
*Danying Ge,Jianhua Gao,Yixue Yang,Weixing Ji*

Main category: cs.LG

TL;DR: 热度感知检索增强生成（HA-RAG）通过利用 KV 块的访问热度来优化推理，结合混合精度压缩/按热度加载与热度感知数据放置，显著降低磁盘 I/O 与内存访问，提升 TTFT 性能。


<details>
  <summary>Details</summary>
Motivation: RAG 能提高输出准确性，但外部知识库带来长上下文处理成本、内存消耗与推理延迟。现有方法通过预计算 KV 并按需加载来加速，但受访问模式与数据分布影响。需通过热度信息来降低 I/O 与延迟，同时尽量保持精度。

Method: 提出两大策略：1) 基于 KV 块访问热度的混合精度压缩与按热度加载，降低磁盘 I/O 与内存访问开销；2) 热度感知数据放置策略，将高频访问的 KV 块优先存放在高速存储中以提升数据访问效率。

Result: 实验结果显示，与 TurboRAG 比较，HA-RAG 在 Time-To-First-Token (TTFT) 方面实现平均约 2.10x 的加速，最大可达约 10.49x，加速同时伴随可忽略的精度损失。

Conclusion: KA-RAG 通过热度感知的压缩/加载与数据放置策略，有效缓解检索阶段的 I/O 与内存带宽压力，显著提升推理速度，且对精度的影响可控；该思路可扩展到其他基于 KV 的检索系统。

Abstract: Retrieval-Augmented Generation (RAG) improves model output accuracy by
leveraging external knowledge bases, serving as an effective solution to
address hallucination issues and knowledge-update delays in Large Language
Models (LLMs). However, the introduction of external knowledge bases presents
RAG with challenges in long-context processing, significantly increasing memory
consumption and inference latency. Existing research accelerates inference by
precomputing Key and Value (KV) of the knowledge base and loading them
on-demand during inference. Based on the access frequency of different KV
chunks within the external knowledge base, this paper proposes a hotness-aware
RAG (HA-RAG) inference optimization system. First, leveraging the numerical
distribution of KV chunks, we introduce a hotness-aware mixed-precision
compressing and loading method to reduce disk I/O and memory access overhead.
Second, we design a hotness-aware data placement strategy that prioritizes
storing frequently accessed KV chunks in high-speed memory to improve data
access efficiency. Experimental results demonstrate that, compared with
TurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum
speedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.

</details>


### [40] [Global Dynamics of Heavy-Tailed SGDs in Nonconvex Loss Landscape: Characterization and Control](https://arxiv.org/abs/2510.20905)
*Xingyu Wang,Chang-Han Rhee*

Main category: cs.LG

TL;DR: 引入并截断的重尾噪声可以使 SGD 更易避开尖锐局部极小值，并提升测试集泛化性能；理论基于大偏差与 metastability 分析，并有实验验证。


<details>
  <summary>Details</summary>
Motivation: 理解 SGD 的全局动力学，超越局部收敛分析，解释为何重尾噪声有利于泛化，并探索通过噪声注入与截断提升性能的可行性。

Method: 建立基于 Wang and Rhee (2023) 的大偏差与 metastability 的技术框架，研究带有重尾噪声的 SGD 的全局动力学；在训练阶段注入并截断噪声、结合梯度裁剪；给出对全局行为的尖锐表征；通过仿真与深度学习实验验证理论。

Result: 对带重尾噪声的 SGD 的全局动力学给出尖锐表征，证实注入并截断重尾噪声可使 SGD 避免尖锐极小值、找到更平坦几何的局部极小值，且泛化性能更好；实验结果与理论一致。

Conclusion: 该研究揭示了重尾噪声在训练中的作用机制，为提升深度学习泛化提供了理论与实践线索，尤其是通过噪声注入与裁剪来实现更平滑的极小值。

Abstract: Stochastic gradient descent (SGD) and its variants enable modern artificial
intelligence. However, theoretical understanding lags far behind their
empirical success. It is widely believed that SGD has a curious ability to
avoid sharp local minima in the loss landscape, which are associated with poor
generalization. To unravel this mystery and further enhance such capability of
SGDs, it is imperative to go beyond the traditional local convergence analysis
and obtain a comprehensive understanding of SGDs' global dynamics. In this
paper, we develop a set of technical machinery based on the recent large
deviations and metastability analysis in Wang and Rhee (2023) and obtain sharp
characterization of the global dynamics of heavy-tailed SGDs. In particular, we
reveal a fascinating phenomenon in deep learning: by injecting and then
truncating heavy-tailed noises during the training phase, SGD can almost
completely avoid sharp minima and achieve better generalization performance for
the test data. Simulation and deep learning experiments confirm our theoretical
prediction that heavy-tailed SGD with gradient clipping finds local minima with
a more flat geometry and achieves better generalization performance.

</details>


### [41] [Learning from Interval Targets](https://arxiv.org/abs/2510.20925)
*Rattana Pukdee,Ziqi Ke,Chirag Gupta*

Main category: cs.LG

TL;DR: A study of regression with interval targets, proposing loss-compatible methods with non-asymptotic generalization bounds and a min-max interval-robust formulation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: In many applications, only interval information for targets is available due to uncertainty or labeling costs; traditional regression assumes exact targets, which is not feasible here.

Method: Part 1: study loss functions compatible with interval targets and derive non-asymptotic generalization bounds based on smoothness, relaxing realizability and small ambiguity degree. Part 2: propose a min-max learning formulation that minimizes the worst-case (maximized) target within the interval; inner maximization is non-convex, but smoothness constraints help achieve good performance. Empirical evaluation on real-world datasets.

Result: Extensive experiments on real-world datasets show state-of-the-art performance.

Conclusion: Interval-target regression is feasible with the proposed loss-compatible approach and the min-max formulation; smoothness-based theoretical guarantees align with strong empirical results.

Abstract: We study the problem of regression with interval targets, where only upper
and lower bounds on target values are available in the form of intervals. This
problem arises when the exact target label is expensive or impossible to
obtain, due to inherent uncertainties. In the absence of exact targets,
traditional regression loss functions cannot be used. First, we study the
methodology of using a loss functions compatible with interval targets, for
which we establish non-asymptotic generalization bounds based on smoothness of
the hypothesis class that significantly relaxing prior assumptions of
realizability and small ambiguity degree. Second, we propose a novel min-max
learning formulation: minimize against the worst-case (maximized) target labels
within the provided intervals. The maximization problem in the latter is
non-convex, but we show that good performance can be achieved with the
incorporation of smoothness constraints. Finally, we perform extensive
experiments on real-world datasets and show that our methods achieve
state-of-the-art performance.

</details>


### [42] [Meta-Learning for Cross-Task Generalization in Protein Mutation Property Prediction](https://arxiv.org/abs/2510.20943)
*Srivathsan Badrinarayanan,Yue Su,Janghoon Ock,Alan Pham,Sanya Ahuja,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 首度将模型无关元学习（MAML）应用于蛋白质突变特性预测，提出以分隔符标记的新突变编码策略，提升跨任务泛化与适应速度，在三组数据集上显著优于传统微调。


<details>
  <summary>Details</summary>
Motivation: 蛋白质突变预测在跨数据集上的泛化受限于不同实验条件、数据稀缺等因素，需实现对新任务的快速适应以支持药物发现、蛋白质工程和精准医学。

Method: 将变压器（Transformer）架构与MAML结合，用于快速适应新任务，通过最少的梯度步骤完成微调；提出将突变位点以分隔符符号嵌入到序列上下文中的新编码策略，克服标准Transformer将突变位点视为未知标记的问题。基于三组蛋白质突变数据集（功能性适应性/fitness、热稳定性、溶解性）进行评估，比较跨任务和单任务设置。

Result: 跨任务评估中，功能性适应性准确率提升约29%，训练时间减少65%；溶解性准确率提升约94%，训练时间减少55%；训练效率在不同数据集规模上保持稳定，显著优于传统微调方法。

Conclusion: 本工作首次系统性将元学习应用于蛋白质突变分析，及提出有效的突变编码策略，显著提升跨领域泛化和对新任务的快速适应能力，具备在工业场景中的潜在应用价值。

Abstract: Protein mutations can have profound effects on biological function, making
accurate prediction of property changes critical for drug discovery, protein
engineering, and precision medicine. Current approaches rely on fine-tuning
protein-specific transformers for individual datasets, but struggle with
cross-dataset generalization due to heterogeneous experimental conditions and
limited target domain data. We introduce two key innovations: (1) the first
application of Model-Agnostic Meta-Learning (MAML) to protein mutation property
prediction, and (2) a novel mutation encoding strategy using separator tokens
to directly incorporate mutations into sequence context. We build upon
transformer architectures integrating them with MAML to enable rapid adaptation
to new tasks through minimal gradient steps rather than learning
dataset-specific patterns. Our mutation encoding addresses the critical
limitation where standard transformers treat mutation positions as unknown
tokens, significantly degrading performance. Evaluation across three diverse
protein mutation datasets (functional fitness, thermal stability, and
solubility) demonstrates significant advantages over traditional fine-tuning.
In cross-task evaluation, our meta-learning approach achieves 29% better
accuracy for functional fitness with 65% less training time, and 94% better
accuracy for solubility with 55% faster training. The framework maintains
consistent training efficiency regardless of dataset size, making it
particularly valuable for industrial applications and early-stage protein
design where experimental data is limited. This work establishes a systematic
application of meta-learning to protein mutation analysis and introduces an
effective mutation encoding strategy, offering transformative methodology for
cross-domain generalization in protein engineering.

</details>


### [43] [Safety Assessment in Reinforcement Learning via Model Predictive Control](https://arxiv.org/abs/2510.20955)
*Jeff Pflueger,Michael Everett*

Main category: cs.LG

TL;DR: 提出一种基于可逆性来提升模型无关强化学习训练安全性的框架。通过将模型预测路径积分控制（MPPI）作为安全检查器，在训练过程中对由学习策略提出的动作进行核查和必要时中止，且不需要对系统动力学或安全约束有显式知识。实验表明该方法能在未执行 unsafe 动作前就中止，同时保持接近 PPO 基线的训练进展。


<details>
  <summary>Details</summary>
Motivation: 在模型无关强化学习中，难以获得形式化的安全保证。传统的 shielding/约束方法通常依赖对安全规范的详细描述。本文通过将安全问题表述为不变性，并利用可逆性在训练过程中持续保证安全，从而在不需要明确动力学模型或安全约束知识的前提下实现安全训练。

Method: 提出一种以可逆性为安全准则的框架，结合模型预测路径积分控制（MPPI）对学习策略提出的动作进行安全性核查。该安全核查仅需对黑箱动力学进行查询，不需要显式的动力学模型或安全约束的定义。若动作被判定为潜在不安全，将在训练过程中被中止。

Result: 实验结果显示，该算法在训练过程中能够在遭遇所有不安全动作之前就中止，从而避免风险，同时训练进展与允许违反安全约束的 PPO 基线相当。

Conclusion: 将可逆性作为一种通用且不依赖于显式动力学与安全约束知识的安全约束信号，与 MPPI 结合实现对模型无关强化学习的安全训练；这一思路在保障安全的同时不显著牺牲训练效率。

Abstract: Model-free reinforcement learning approaches are promising for control but
typically lack formal safety guarantees. Existing methods to shield or
otherwise provide these guarantees often rely on detailed knowledge of the
safety specifications. Instead, this work's insight is that many
difficult-to-specify safety issues are best characterized by invariance.
Accordingly, we propose to leverage reversibility as a method for preventing
these safety issues throughout the training process. Our method uses
model-predictive path integral control to check the safety of an action
proposed by a learned policy throughout training. A key advantage of this
approach is that it only requires the ability to query the black-box dynamics,
not explicit knowledge of the dynamics or safety constraints. Experimental
results demonstrate that the proposed algorithm successfully aborts before all
unsafe actions, while still achieving comparable training progress to a
baseline PPO approach that is allowed to violate safety.

</details>


### [44] [An Ensembled Penalized Federated Learning Framework for Falling People Detection](https://arxiv.org/abs/2510.20960)
*Sizhe Rao,Runqiu Zhang,Sajal Saha,Liang Chen*

Main category: cs.LG

TL;DR: 提出一个名为EPFL的联邦学习框架用于隐私保护跌倒检测，结合持续学习、个性化与专门加权聚合，提升跨客户端一致性和对行为差异的适应性。


<details>
  <summary>Details</summary>
Motivation: 跌倒检测在老年和残疾人群中需求高，但传统集中或点对点方法在泛化、隐私保护和对个体行为变异的适应性方面存在局限。

Method: 在可穿戴传感器数据上利用同态加密和联邦训练实现隐私保护；引入惩罚性局部训练、基于集成的推断，以及专门的加权聚合(SWA)策略，进行多模型集成与个性化建模。

Result: 在基准跌倒检测数据集上实现Recall 88.31%、F1-score 89.94%，显著优于集中式和基线模型。

Conclusion: 该框架提供一个可扩展、私密且高精度的跌倒检测解决方案，适用于现实医疗场景，具备通过自适应反馈机制实现持续改进的潜力。

Abstract: Falls among elderly and disabled individuals remain a leading cause of injury
and mortality worldwide, necessitating robust, accurate, and privacy-aware fall
detection systems. Traditional fall detection approaches, whether centralized
or point-wise, often struggle with key challenges such as limited
generalizability, data privacy concerns, and variability in individual movement
behaviors. To address these limitations, we propose EPFL-an Ensembled Penalized
Federated Learning framework that integrates continual learning, personalized
modeling, and a novel Specialized Weighted Aggregation (SWA) strategy. EPFL
leverages wearable sensor data to capture sequential motion patterns while
preserving user privacy through homomorphic encryption and federated training.
Unlike existing federated models, EPFL incorporates both penalized local
training and ensemble-based inference to improve inter-client consistency and
adaptability to behavioral differences. Extensive experiments on a benchmark
fall detection dataset demonstrate the effectiveness of our approach, achieving
a Recall of 88.31 percent and an F1-score of 89.94 percent, significantly
outperforming both centralized and baseline models. This work presents a
scalable, secure, and accurate solution for real-world fall detection in
healthcare settings, with strong potential for continuous improvement via its
adaptive feedback mechanism.

</details>


### [45] [Towards Scalable Oversight with Collaborative Multi-Agent Debate in Error Detection](https://arxiv.org/abs/2510.20963)
*Yongqiang Chen,Gang Niu,James Cheng,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: ColMAD is a collaborative, non-zero-sum multi-agent debate protocol that significantly improves error detection in LLM supervision, outperforming competitive MAD and single-agent baselines.


<details>
  <summary>Details</summary>
Motivation: Self-diagnosis by LLMs is unreliable on complex tasks; external feedback is needed. Conventional MAD is zero-sum, leading to debate hacking and misleading conclusions; a collaborative, non-zero-sum setup can leverage complementary perspectives to strengthen truth-seeking.

Method: Introduce ColMAD, where multiple agents engage in constructive, critical discussion to complement each other’s misses, framed as a non-zero-sum collaboration. A judge agent aggregates the evidence to reach more informative conclusions.

Result: Empirically, ColMAD outperforms previous competitive MAD by about 19% in error detection and yields non-trivial improvements over single-agent methods.

Conclusion: A collaborative, non-zero-sum MAD mitigates debate hacking and enhances the reliability of external oversight, showing promise for scalable supervision of superhuman AI.

Abstract: Accurate detection of errors in large language models (LLM) responses is
central to the success of scalable oversight, or providing effective
supervision to superhuman intelligence. Yet, self-diagnosis is often unreliable
on complex tasks unless aided by reliable external feedback. Multi-agent debate
(MAD) seems to be a natural alternative to external feedback: multiple LLMs
provide complementary perspectives and cross-checks for error detection.
However, prior MAD protocols frame debate as a zero-sum game, where the
debaters compete to win the game instead of seeking the truth. Consequently, it
leads to debate hacking: debaters tend to mislead the judge by misinterpreting
the task or presenting overconfident claims, which introduce more mistakes and
underperform single-agent methods. To mitigate the issue, we introduce a new
collaborative MAD protocol, termed ColMAD, that reframes MAD as a non-zero sum
game. Specifically, ColMAD encourages multiple agents to criticize each other
in a supportive way, such that they can complement the missing points of each
other. Therefore, the judge agent can make a more informative conclusion based
on more comprehensive evidence. Empirically, we show that ColMAD significantly
outperforms previous competitive MAD by 19% and brings non-trivial improvements
over single-agent methods in error detection.

</details>


### [46] [Neural Mutual Information Estimation with Vector Copulas](https://arxiv.org/abs/2510.20968)
*Yanzhi Chen,Zijing Ou,Adrian Weller,Michael U. Gutmann*

Main category: cs.LG

TL;DR: Proposes a vector-copula-based interpolation estimator for mutual information, balancing model flexibility and simplicity; empirically outperforms pure neural-network or Gaussian-copula approaches on benchmarks and real data.


<details>
  <summary>Details</summary>
Motivation: Estimating mutual information is challenging because highly flexible models (e.g., neural networks) require large datasets, while overly simple models (e.g., Gaussian copula) fail to capture complex distributions.

Method: Uses vector copula theory to interpolate between flexible models and simple copula-based models, deriving a principled trade-off between capacity and data requirements.

Result: Empirical evaluations on state-of-the-art synthetic benchmarks and real-world, multimodal data show advantages over existing MI estimators.

Conclusion: The interpolation framework yields a data-efficient MI estimator that adapts to data complexity and improves estimation across diverse modalities.

Abstract: Estimating mutual information (MI) is a fundamental task in data science and
machine learning. Existing estimators mainly rely on either highly flexible
models (e.g., neural networks), which require large amounts of data, or overly
simplified models (e.g., Gaussian copula), which fail to capture complex
distributions. Drawing upon recent vector copula theory, we propose a
principled interpolation between these two extremes to achieve a better
trade-off between complexity and capacity. Experiments on state-of-the-art
synthetic benchmarks and real-world data with diverse modalities demonstrate
the advantages of the proposed estimator.

</details>


### [47] [On the accuracy of implicit neural representations for cardiovascular anatomies and hemodynamic fields](https://arxiv.org/abs/2510.20970)
*Jubilee Lee,Daniele E. Schiavazzi*

Main category: cs.LG

TL;DR: INRs can efficiently compress hemodynamic fields and model cardiovascular anatomies, addressing spectral bias to achieve high compression (up to ~230×) with mm-scale accuracy across many anatomies; SIREN, MFN-Gabor, and MHE architectures perform best; code available.


<details>
  <summary>Details</summary>
Motivation: Assess the applicability and accuracy of state-of-the-art implicit neural representations (INRs) for domain-specific cardiovascular data compression and anatomical representation, and to mitigate spectral bias.

Method: Evaluate several INR architectures (SIREN, MFN-Gabor, MHE) on space-time hemodynamic fields in the thoracic aorta and signed distance functions for cardiovascular anatomies. Investigate spectral-bias mitigation strategies via specialized activation functions, fixed/trainable positional encoding, and linear combinations of nonlinear kernels. Compare compression ratios and reconstruction errors across 48 anatomies.

Result: Achieved compression ratios up to ~230 with max absolute errors of 1 mmHg for pressure and 5–10 cm/s for velocity on thoracic aorta fields. Across 48 anatomies, average anatomical discrepancy <0.5 mm and max <1.6 mm. SIREN, MFN-Gabor, and MHE architectures performed best. Source code/data available at the provided repository.

Conclusion: INR-based representations offer high compression and accuracy for cardiovascular simulations and anatomy modeling, with certain architectures (SIREN, MFN-Gabor, MHE) delivering superior performance; results support broader adoption and further exploration in domain-specific INRs.

Abstract: Implicit neural representations (INRs, also known as neural fields) have
recently emerged as a powerful framework for knowledge representation,
synthesis, and compression. By encoding fields as continuous functions within
the weights and biases of deep neural networks-rather than relying on voxel- or
mesh-based structured or unstructured representations-INRs offer both
resolution independence and high memory efficiency. However, their accuracy in
domain-specific applications remains insufficiently understood. In this work,
we assess the performance of state-of-the-art INRs for compressing hemodynamic
fields derived from numerical simulations and for representing cardiovascular
anatomies via signed distance functions. We investigate several strategies to
mitigate spectral bias, including specialized activation functions, both fixed
and trainable positional encoding, and linear combinations of nonlinear
kernels. On realistic, space- and time-varying hemodynamic fields in the
thoracic aorta, INRs achieved remarkable compression ratios of up to
approximately 230, with maximum absolute errors of 1 mmHg for pressure and 5-10
cm/s for velocity, without extensive hyperparameter tuning. Across 48 thoracic
aortic anatomies, the average and maximum absolute anatomical discrepancies
were below 0.5 mm and 1.6 mm, respectively. Overall, the SIREN, MFN-Gabor, and
MHE architectures demonstrated the best performance. Source code and data is
available at https://github.com/desResLab/nrf.

</details>


### [48] [L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks](https://arxiv.org/abs/2510.20976)
*Jiyu Cui,Fang Wu,Haokai Zhao,Minggao Feng,Xenophon Evangelopoulos,Andrew I. Cooper,Yejin Choi*

Main category: cs.LG

TL;DR: 提出了 L2M3OF，一种面向 MOF 的多模态大语言模型，通过晶体结构编码与语言理解的联合，实现对结构、文本和知识的高效对齐，提升属性预测与知识生成能力，且参数规模较小。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在材料发现中的应用受限于需要将复杂的三维晶体结构及晶格规则映射到文本表示的问题。MOFs 的设计空间巨大且复杂，纯文本信息往往难以充分表达结构约束与拓扑特征，因此亟需结合结构模态信息的学习框架。

Method: 引入预训练晶体编码器以及轻量投影层，将结构信息压缩为 token，结合语言模型实现结构-文本-知识的多模态对齐；建立结构-性质-知识数据库，基于该数据集对比评测GPT-5、Gemini-2.5-Pro、DeepSeek-R1 等闭源大模型在属性预测与知识生成任务上的表现。

Result: 在属性预测和知识生成任务中，L2M3OF 超越了领先的文本型闭源 LLM，且使用的参数数量明显较少。

Conclusion: 多模态融合是推进材料发现的关键，L2M3OF 可作为下一代材料设计AI系统的基础平台。

Abstract: Large language models have demonstrated remarkable reasoning capabilities
across diverse natural language tasks. However, comparable breakthroughs in
scientific discovery are more limited, because understanding complex physical
phenomena demands multifaceted representations far beyond language alone. A
compelling example is the design of functional materials such as MOFs-critical
for a range of impactful applications like carbon capture and hydrogen storage.
Navigating their vast and intricate design space in language-based
representations interpretable by LLMs is challenging due to the numerous
possible three-dimensional atomic arrangements and strict reticular rules of
coordination geometry and topology. Despite promising early results in
LLM-assisted discovery for simpler materials systems, MOF design remains
heavily reliant on tacit human expertise rarely codified in textual information
alone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLM
for MOFs. L2M3OF integrates crystal representation learning with language
understanding to process structural, textual, and knowledge modalities jointly.
L2M3OF employs a pre-trained crystal encoder with a lightweight projection
layer to compress structural information into a token space, enabling efficient
alignment with language instructions. To facilitate training and evaluation, we
curate a structure-property-knowledge database of crystalline materials and
benchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5,
Gemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperforms
leading text-based closed-source LLMs in property prediction and knowledge
generation tasks, despite using far fewer parameters. These results highlight
the importance of multimodal approaches for porous material understanding and
establish L2M3OF as a foundation for next-generation AI systems in materials
discovery.

</details>


### [49] [Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression](https://arxiv.org/abs/2510.20984)
*Xi Zhang,Xiaolin Wu,Jiamang Wang,Weisi Lin*

Main category: cs.LG

TL;DR: 提出一种分组格点向量量化（GLVQ）用于LLM的后训练量化，通过可学习的生成矩阵为每组权重定义定制的格点码本；训练时使用Babai舍入近似最近格点搜索实现可微优化，解码阶段为简单的矩阵-向量乘法；在多项基准上优于现有PTQ基线，适合资源受限部署。


<details>
  <summary>Details</summary>
Motivation: 现有的统一(均匀)量化在低比特位下会显著劣化模型性能，迫切需要更具自适应性且高效的后训练量化方法，以在减少存储和推理成本的同时尽量保持准确性。

Method: 将权重分组，对每组学习一个生成矩阵以构建定制的格点码本；使用Babai rounding来近似最近格点搜索，从而实现对生成矩阵的端到端可微优化；训练完成后解码仅需执行一次矩阵-向量乘法，量化过程简化为一次线性变换。

Result: 在多个基准上，GLVQ在模型压缩率与精度之间取得更优的折中，比现有PTQ基线具有更好的性能表现。

Conclusion: GLVQ提供一种高效且可训练的分组格点量化框架，适合在资源受限场景下部署大模型，并且附带开源实现。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities but
typically require extensive computational resources and memory for inference.
Post-training quantization (PTQ) can effectively reduce these demands by
storing weights in lower bit-width formats. However, standard uniform
quantization often leads to notable performance degradation, particularly in
low-bit scenarios. In this work, we introduce a Grouped Lattice Vector
Quantization (GLVQ) framework that assigns each group of weights a customized
lattice codebook, defined by a learnable generation matrix. To address the
non-differentiability of the quantization process, we adopt Babai rounding to
approximate nearest-lattice-point search during training, which enables stable
optimization of the generation matrices. Once trained, decoding reduces to a
simple matrix-vector multiplication, yielding an efficient and practical
quantization pipeline. Experiments on multiple benchmarks show that our
approach achieves a better trade-off between model size and accuracy compared
to existing post-training quantization baselines, highlighting its
effectiveness in deploying large models under stringent resource constraints.
Our source code is available on GitHub repository:
https://github.com/xzhang9308/GLVQ.

</details>


### [50] [GPU Memory Requirement Prediction for Deep Learning Task Based on Bidirectional Gated Recurrent Unit Optimization Transformer](https://arxiv.org/abs/2510.20985)
*Chao Wang,Zhizhao Wen,Ruoxin Zhang,Puyang Xu,Yifan Jiang*

Main category: cs.LG

TL;DR: 提出了一种将双向门控循环单元BiGRU融入Transformer以预测深度学习任务中的GPU内存需求的模型，并通过与决策树、随机森林、AdaBoost、XGBoost等基线模型对比，显示在MSE/RMSE等指标上具有显著优势，预测误差更小，MAE/R2表现也较好，证明该方法在资源调度与集群利用方面的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 在深度学习任务中准确预测GPU内存资源需求是提高资源调度与集群利用率的关键。现有方法在预测精度与稳定性方面存在不足，因此需要一种能够更好捕捉时序与关系特征的模型来提升预测性能。

Method: 提出一种基于Transformer的模型，并在其中嵌入BiGRU单元以增强对序列信息的建模能力。进行了对比实验，选取四种代表性的基础机器学习模型（决策树、随机森林、Adaboost、XGBoost）作为基线，与所提BiGRU-Transformer模型比较。评估指标包括MSE、RMSE、MAE和R2，实验结果揭示该模型在上述指标上均优于基线方法。

Result: BiGRU-Transformer模型在MSE与RMSE上取得最低值，预测结果与真实值的偏差最小；在MAE与R2上也表现良好，整体预测性能比传统机器学习方法更为稳定与全面。该模型能够高效、准确地完成深度学习任务中的GPU内存需求预测，且预测精度显著提升。

Conclusion: 所提出的BiGRU优化的Transformer模型为深度学习任务中的GPU内存资源预测提供了一种高效且准确的解决方案，能显著提升资源调度与计算集群的利用率，具备良好的理论基础与应用前景。

Abstract: In response to the increasingly critical demand for accurate prediction of
GPU memory resources in deep learning tasks, this paper deeply analyzes the
current research status and innovatively proposes a deep learning model that
integrates bidirectional gated recurrent units (BiGRU) to optimize the
Transformer architecture, aiming to improve the accuracy of memory demand
prediction. To verify the effectiveness of the model, a carefully designed
comparative experiment was conducted, selecting four representative basic
machine learning models: decision tree, random forest, Adaboost, and XGBoost as
benchmarks. The detailed experimental results show that the BiGRU Transformer
optimization model proposed in this paper exhibits significant advantages in
key evaluation indicators: in terms of mean square error (MSE) and root mean
square error (RMSE), the model achieves the lowest value among all comparison
models, and its predicted results have the smallest deviation from the actual
values; In terms of mean absolute error (MAE) and coefficient of determination
(R2) indicators, the model also performs well and the results are balanced and
stable, with comprehensive predictive performance far exceeding the benchmark
machine learning methods compared. In summary, the Transformer model based on
bidirectional gated recurrent unit optimization successfully constructed in
this study can efficiently and accurately complete GPU memory demand prediction
tasks in deep learning tasks, and its prediction accuracy has been
significantly improved compared to traditional machine learning methods. This
research provides strong technical support and reliable theoretical basis for
optimizing resource scheduling and management of deep learning tasks, and
improving the utilization efficiency of computing clusters.

</details>


### [51] [AL-CoLe: Augmented Lagrangian for Constrained Learning](https://arxiv.org/abs/2510.20995)
*Ignacio Boero,Ignacio Hounie,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 在非凸约束学习中，增强拉格朗日方法能有效地缩小对偶性鸿沟，提供理论保证与可行最优解的收敛性，并在公平约束分类任务上展现出实用性。


<details>
  <summary>Details</summary>
Motivation: 解决非凸设置下的对偶性鸿沟问题，提供一种修改极少、且具有理论保证的约束学习解法。

Method: 采用增强拉格朗日框架并结合对偶上升法；在温和条件下建立强对偶性，证明对偶上升算法收敛至可行且最优的原问题解；给出PAC风格的一般化保证；并在公平性约束的分类任务中进行实验验证。

Result: 在温和条件下建立强对偶性；证明对偶上升能收敛到可行且最优的原始解；给出可观的一般化界；在公平约束分类任务中验证了方法的有效性。

Conclusion: 增强拉格朗日方法为非凸约束学习提供了一个理论和实践上可行的框架，具备强对偶性、收敛性和泛化保证，适用于更广泛的约束学习问题。

Abstract: Despite the non-convexity of most modern machine learning parameterizations,
Lagrangian duality has become a popular tool for addressing constrained
learning problems. We revisit Augmented Lagrangian methods, which aim to
mitigate the duality gap in non-convex settings while requiring only minimal
modifications, and have remained comparably unexplored in constrained learning
settings. We establish strong duality results under mild conditions, prove
convergence of dual ascent algorithms to feasible and optimal primal solutions,
and provide PAC-style generalization guarantees. Finally, we demonstrate its
effectiveness on fairness constrained classification tasks.

</details>


### [52] [Exploring Spiking Neural Networks for Binary Classification in Multivariate Time Series at the Edge](https://arxiv.org/abs/2510.20997)
*James Ghawaly,Andrew Nicholson,Catherine Schuman,Dalton Diez,Aaron Young,Brett Witherspoon*

Main category: cs.LG

TL;DR: 用进化优化的神经形态框架（EONS）演化稀疏、具记忆性的SNN，用于多变量时间序列的二分类，强调逐步预测与低误报率下的高精度，并在放射性源检测和癫痫 EEG 任务上展现出良好硬件效率与跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 实现高效、低功耗、对时间序列具备高精度的二值分类模型，兼具可解释性与在硬件上的友好部署，需在稀疏SNN和群体决策下达到低误报率的稳定表现。

Method: 使用 Evolutionary Optimization of Neuromorphic Systems (EONS) 对稀疏、具状态性的SNN进行结构与参数的联合优化；将输入编码为脉冲序列，输出单神经元的脉冲计数阈值来作预测；引入简单的投票集成提升鲁棒性；在放射性源检测的 Gamma 射线光谱数据上进行领域优化并在 microCaspian 硬件上部署；同样框架无领域特异改动地应用于 EEG 癫痫检测以验证泛化性。

Result: 在伽马射线光谱数据任务中，最小结构为49个神经元、66条突触，达到1小时误报率下的51.8% TPR；PCA为42.7%，深度学习为49.8%；三模型任意投票提升至67.1% TPR。硬件功耗约2 mW，推理延迟20.2 ms。对 EEG 癫痫检测的实施中，集合获得95% TPR、16%假阳性率，且参数量显著低于近年深度学习方法。

Conclusion: 在不依赖领域特定修改的前提下，该框架展现出对跨域时间序列分类任务的良好泛化能力，以及在稀疏SNN、低功耗与快速推理方面的竞争力，适合嵌入式、神经形态硬件部署。

Abstract: We present a general framework for training spiking neural networks (SNNs) to
perform binary classification on multivariate time series, with a focus on
step-wise prediction and high precision at low false alarm rates. The approach
uses the Evolutionary Optimization of Neuromorphic Systems (EONS) algorithm to
evolve sparse, stateful SNNs by jointly optimizing their architectures and
parameters. Inputs are encoded into spike trains, and predictions are made by
thresholding a single output neuron's spike counts. We also incorporate simple
voting ensemble methods to improve performance and robustness.
  To evaluate the framework, we apply it with application-specific
optimizations to the task of detecting low signal-to-noise ratio radioactive
sources in gamma-ray spectral data. The resulting SNNs, with as few as 49
neurons and 66 synapses, achieve a 51.8% true positive rate (TPR) at a false
alarm rate of 1/hr, outperforming PCA (42.7%) and deep learning (49.8%)
baselines. A three-model any-vote ensemble increases TPR to 67.1% at the same
false alarm rate. Hardware deployment on the microCaspian neuromorphic platform
demonstrates 2mW power consumption and 20.2ms inference latency.
  We also demonstrate generalizability by applying the same framework, without
domain-specific modification, to seizure detection in EEG recordings. An
ensemble achieves 95% TPR with a 16% false positive rate, comparable to recent
deep learning approaches with significant reduction in parameter count.

</details>


### [53] [Fair Representation Learning with Controllable High Confidence Guarantees via Adversarial Inference](https://arxiv.org/abs/2510.21017)
*Yuhong Luo,Austin Hoag,Xintong Wang,Philip S. Thomas,Przemyslaw A. Grabowicz*

Main category: cs.LG

TL;DR: 提出 FRG 框架，在表征学习中实现对下游任务的可控高置信度公平性保证，主张在每个预测中的群体差距被用户设定的 ε 上界所约束，并通过优化对抗模型实现。经过三個真实数据集的实验，与六种最先进方法对比，FRG 在多种下游模型和任务上稳定地界定不公平性。


<details>
  <summary>Details</summary>
Motivation: 在表示学习中实现对下游任务的公平性需要在严格的概率高置信度下对群体差异进行上界控制，避免对特定人口群体的系统性偏见，同时提供可控的误差阈值以适应应用场景。

Method: 提出 FRG 框架，通过优化的对抗模型实现高置信度的公平性保证。用户可以设定允许的差异上限 ε，框架在学习表征时尽量使下游预测中的群体差异不超过 ε，并在训练过程中优化对抗目标以抑制敏感信息的泄露及偏差扩散。对三個真实数据集进行评估，并与六个现有公平表示学习方法进行比较。

Result: FRG 在多种下游模型和任务中始终能够将不公平性限制在给定的 ε 之内，并与或优于六种对比方法的表现，显示出稳定的高置信度公平性。三数据集的实证结果支持其普适性。

Conclusion: FRG 提供了一种可控且高置信度的公平表示学习解决方案，能够在不同下游任务和模型间保持一致的公平性界限，提升现实世界应用中的公平性保障。

Abstract: Representation learning is increasingly applied to generate representations
that generalize well across multiple downstream tasks. Ensuring fairness
guarantees in representation learning is crucial to prevent unfairness toward
specific demographic groups in downstream tasks. In this work, we formally
introduce the task of learning representations that achieve high-confidence
fairness. We aim to guarantee that demographic disparity in every downstream
prediction remains bounded by a *user-defined* error threshold $\epsilon$, with
*controllable* high probability. To this end, we propose the ***F**air
**R**epresentation learning with high-confidence **G**uarantees (FRG)*
framework, which provides these high-confidence fairness guarantees by
leveraging an optimized adversarial model. We empirically evaluate FRG on three
real-world datasets, comparing its performance to six state-of-the-art fair
representation learning methods. Our results demonstrate that FRG consistently
bounds unfairness across a range of downstream models and tasks.

</details>


### [54] [More Than Memory Savings: Zeroth-Order Optimization Mitigates Forgetting in Continual Learning](https://arxiv.org/abs/2510.21019)
*Wanhao Yu,Zheng Wang,Shuteng Niu,Sen Lin,Li Yang*

Main category: cs.LG

TL;DR: ZO优化可提升CL中的稳定性并减缓遗忘，但会降低塑性；提出ZO-FC在适配器PEFT模块使用ZO、分类器使用FO，以在内存开销很小的同时实现稳定性与塑性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决连续学习中的稳定性-塑性-效率三难，寻求记忆友好的优化方法。

Method: 理论分析表明ZO可能让损失面更加平坦，从而减小遗忘；通过对现有CL方法的全面评估验证该现象；提出ZO-FC：在一个适配器式PEFT模块上使用ZO优化，分类器使用FO优化，降低内存开销。

Result: ZO提升了模型的稳定性并减少遗忘，但由于梯度估计不精确与收敛变慢，在有限训练预算下对获取新任务知识的 plasticity 较差；与可学习分类器结合时效果尤显不足；ZO-FC在稳定性与塑性之间取得更好折中，且对设备友好。

Conclusion: ZO为CL提供了一种可行的记忆高效优化路径；结合FO分类器的ZO-FC方案在保持稳定性同时不显著牺牲可塑性，适合嵌入式/边缘设备的持续学习。

Abstract: Zeroth-order (ZO) optimization has gained attention as a memory-efficient
alternative to first-order (FO) methods, particularly in settings where
gradient computation is expensive or even impractical. Beyond its memory
efficiency, in this work, we investigate ZO optimization for continual learning
(CL) as a novel approach to address the plasticity-stability-efficiency
trilemma. Through theoretical analysis and empirical evidence, we show that ZO
optimization naturally leads to flatter loss landscapes, which in turn reduce
forgetting in CL. However, this stability comes at a cost of plasticity: due to
its imprecise gradient estimates and slower convergence, ZO optimization tends
to be less effective than FO in acquiring new task-specific knowledge,
particularly under constrained training budgets. To better understand this
trade-off, we conduct a holistic evaluation of ZO optimization applied to
various existing CL methods. Our findings reveal that ZO optimization enhances
stability but often undermines plasticity, particularly when used with
learnable classifiers. Motivated by this insight, we propose ZO-FC, a simple
but effective approach that applies ZO optimization to a single adapter-based
PEFT module with FO optimized classifier. This design leverages the stability
benefits of ZO while preserving the adaptability of FO updates with negligible
memory overhead. Experiments demonstrate that ZO-FC achieves an effective
balance between stability and plasticity, offering a practical and
memory-efficient solution for on-device CL.

</details>


### [55] [CIPHER: Scalable Time Series Analysis for Physical Sciences with Application to Solar Wind Phenomena](https://arxiv.org/abs/2510.21022)
*Jasmine R. Kobayashi,Daniela Martin,Valmir P Moraes Filho,Connor O'Brien,Jinsu Hong,Sudeshna Boro Saikia,Hala Lamdouar,Nathan D. Miles,Marcella Scoczynski,Mavis Stone,Sairam Sundaresan,Anna Jungbluth,Andrés Muñoz-Jaramillo,Evangelia Samara,Joseph Gallego*

Main category: cs.LG

TL;DR: CIPHER 是一个以人机协作为核心的时间序列标注管线，结合 iSAX、HDBSCAN 与专家校验，以放大物理时间序列标注规模。


<details>
  <summary>Details</summary>
Motivation: 在物理科学中，专家标注稀缺、成本高且容易产生不一致性，亟需可扩展、可解释的标注方法来支撑机器学习的理解、预测与预报。

Method: 使用可解释的压缩与索引方法 iSAX；通过密度聚类算法 HDBSCAN 将重复现象分组；引入人机循环阶段让领域专家对代表样本进行标注，并将标签在簇内传播以实现大规模分类。

Result: 在 OMNI 数据上的太阳风现象分类任务中，CIPHER 能回收有意义的物理现象，如日冕质量抛射（CME）与流相互作用区（SIRs），验证了将符号表示、无监督学习与专家知识结合的普遍策略，具有在其它物理领域应用的潜力。

Conclusion: 该框架提供了一种通用策略，将符号化表示、无监督学习与专家知识结合以应对时间序列标注的稀缺性；并公开可重复使用的代码与配置。

Abstract: Labeling or classifying time series is a persistent challenge in the physical
sciences, where expert annotations are scarce, costly, and often inconsistent.
Yet robust labeling is essential to enable machine learning models for
understanding, prediction, and forecasting. We present the \textit{Clustering
and Indexation Pipeline with Human Evaluation for Recognition} (CIPHER), a
framework designed to accelerate large-scale labeling of complex time series in
physics. CIPHER integrates \textit{indexable Symbolic Aggregate approXimation}
(iSAX) for interpretable compression and indexing, density-based clustering
(HDBSCAN) to group recurring phenomena, and a human-in-the-loop step for
efficient expert validation. Representative samples are labeled by domain
scientists, and these annotations are propagated across clusters to yield
systematic, scalable classifications. We evaluate CIPHER on the task of
classifying solar wind phenomena in OMNI data, a central challenge in space
weather research, showing that the framework recovers meaningful phenomena such
as coronal mass ejections and stream interaction regions. Beyond this case
study, CIPHER highlights a general strategy for combining symbolic
representations, unsupervised learning, and expert knowledge to address label
scarcity in time series across the physical sciences. The code and
configuration files used in this study are publicly available to support
reproducibility.

</details>


### [56] [Physically consistent and uncertainty-aware learning of spatiotemporal dynamics](https://arxiv.org/abs/2510.21023)
*Qingsong Xu,Jonathan L Bamber,Nils Thuerey,Niklas Boers,Paul Bates,Gustau Camps-Valls,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 提出物理一致的神经算子（PCNO）及其扩展 DiffPCNO，通过在傅里叶空间投影确保守恒定律，并以扩散模型量化不确定性，实现高保真、物理一致且具有不确定性评估的时空预测。


<details>
  <summary>Details</summary>
Motivation: 长期时空预测面临潮汐性和不确定性挑战，现有ML方法往往忽视物理规律和量化不确定性，需要物理约束与不确定性感知的预测框架。

Method: 提出 PCNO，含物理一致投影层，在傅里叶域高效实现质量和动量守恒。基于确定性预测，进一步提出 DiffPCNO，利用一致性模型量化并减小不确定性，提升预测的准确性和可靠性。

Result: PCNO/DiffPCNO 可在不同系统（如湍流、洪涝、大气等）和多分辨率下实现高保真、物理一致且具不确定性分析的时空预测，具有鲁棒性与广泛适用性。

Conclusion: 两阶段框架为实现物理 grounding、准确性与不确定性并存的时空预测提供了稳健且通用的解决方案。

Abstract: Accurate long-term forecasting of spatiotemporal dynamics remains a
fundamental challenge across scientific and engineering domains. Existing
machine learning methods often neglect governing physical laws and fail to
quantify inherent uncertainties in spatiotemporal predictions. To address these
challenges, we introduce a physics-consistent neural operator (PCNO) that
enforces physical constraints by projecting surrogate model outputs onto
function spaces satisfying predefined laws. A physics-consistent projection
layer within PCNO efficiently computes mass and momentum conservation in
Fourier space. Building upon deterministic predictions, we further propose a
diffusion model-enhanced PCNO (DiffPCNO), which leverages a consistency model
to quantify and mitigate uncertainties, thereby improving the accuracy and
reliability of forecasts. PCNO and DiffPCNO achieve high-fidelity
spatiotemporal predictions while preserving physical consistency and
uncertainty across diverse systems and spatial resolutions, ranging from
turbulent flow modeling to real-world flood/atmospheric forecasting. Our
two-stage framework provides a robust and versatile approach for accurate,
physically grounded, and uncertainty-aware spatiotemporal forecasting.

</details>


### [57] [Amortized Active Generation of Pareto Sets](https://arxiv.org/abs/2510.21052)
*Daniel M. Steinberg,Asiri Wijesinghe,Rafael Oliveira,Piotr Koniusz,Cheng Soon Ong,Edwin V. Bonilla*

Main category: cs.LG

TL;DR: A-GPS: 在线离散黑箱多目标优化的Pareto集合生成框架，通过可观测变量的条件生成模型和非支配性预测实现偏好导向的采样，无需显式超体积计算即可在Pareto前沿采样。


<details>
  <summary>Details</summary>
Motivation: 在在线离散黑箱MOO中，目标评估昂贵且需考虑用户偏好，现有方法往往依赖显式超体积或重新训练；需一个能学习Pareto集合分布、并在不重训的情况下按用户偏好进行采样的模型。

Method: 学习Pareto集合的生成模型；使用类别概率估计器CPE预测非支配关系并驱动模型向高性能区域收敛；CPE隐式估计超体积改进的概率PHVI；引入偏好方向向量编码目标空间的用户偏好；每轮用Pareto成员资格和对齐偏好方向更新模型，获得可 amortized 的采样能力。

Result: 在合成基准和蛋白质设计任务上实验，A-GPS展现高质量的Pareto近似与良好的样本效率，能有效融入用户偏好，且避免显式超体积计算。

Conclusion: A-GPS提供一种简单而强大的偏好感知Pareto前沿生成框架，具备可放大性和对离散在线MOO的适用性，未来可拓展到更广泛的应用场景。

Abstract: We introduce active generation of Pareto sets (A-GPS), a new framework for
online discrete black-box multi-objective optimization (MOO). A-GPS learns a
generative model of the Pareto set that supports a-posteriori conditioning on
user preferences. The method employs a class probability estimator (CPE) to
predict non-dominance relations and to condition the generative model toward
high-performing regions of the search space. We also show that this
non-dominance CPE implicitly estimates the probability of hypervolume
improvement (PHVI). To incorporate subjective trade-offs, A-GPS introduces
preference direction vectors that encode user-specified preferences in
objective space. At each iteration, the model is updated using both Pareto
membership and alignment with these preference directions, producing an
amortized generative model capable of sampling across the Pareto front without
retraining. The result is a simple yet powerful approach that achieves
high-quality Pareto set approximations, avoids explicit hypervolume
computation, and flexibly captures user preferences. Empirical results on
synthetic benchmarks and protein design tasks demonstrate strong sample
efficiency and effective preference incorporation.

</details>


### [58] [On the Sample Complexity of Differentially Private Policy Optimization](https://arxiv.org/abs/2510.21060)
*Yi He,Xingyu Zhou*

Main category: cs.LG

TL;DR: 提出并分析了差分隐私在策略优化中的理论框架，明确定义了适用于PO的隐私单位并给出统一的样本复杂度分析框架，覆盖策略梯度、自然策略梯度等算法，结论是隐私代价往往只作为样本复杂度的低阶项，同时揭示了私有PO中的若干细微现象。


<details>
  <summary>Details</summary>
Motivation: 随着策略优化在机器人、医疗等敏感领域的应用增多，对隐私保护提出更高要求。传统DP在强化学习中的直接应用存在对on-policy学习动态和隐私单位定义的挑战，因此需要为PO建立合适的DP定义并评估其样本复杂度。

Method: 正式给出面向PO的DP定义，解决on-policy动态下隐私单元的选择等问题；在一个统一框架内系统分析常用PO算法（如策略梯度、自然策略梯度等）在DP约束下的样本复杂度。

Result: 证明隐私成本在很多情形下表现为样本复杂度的低阶项，并揭示在私有PO设置中的一些微妙现象，为设计隐私保护的PO算法提供了可操作的见解。

Conclusion: 为私有策略优化提供了理论基石：明确的DP定义、统一的分析框架以及对隐私开销的定量认识，指引后续在敏感领域应用中实现高效的隐私保护PO。

Abstract: Policy optimization (PO) is a cornerstone of modern reinforcement learning
(RL), with diverse applications spanning robotics, healthcare, and large
language model training. The increasing deployment of PO in sensitive domains,
however, raises significant privacy concerns. In this paper, we initiate a
theoretical study of differentially private policy optimization, focusing
explicitly on its sample complexity. We first formalize an appropriate
definition of differential privacy (DP) tailored to PO, addressing the inherent
challenges arising from on-policy learning dynamics and the subtlety involved
in defining the unit of privacy. We then systematically analyze the sample
complexity of widely-used PO algorithms, including policy gradient (PG),
natural policy gradient (NPG) and more, under DP constraints and various
settings, via a unified framework. Our theoretical results demonstrate that
privacy costs can often manifest as lower-order terms in the sample complexity,
while also highlighting subtle yet important observations in private PO
settings. These offer valuable practical insights for privacy-preserving PO
algorithms.

</details>


### [59] [Scalable Machine Learning Analysis of Parker Solar Probe Solar Wind Data](https://arxiv.org/abs/2510.21066)
*Daniela Martin,Connor O'Brien,Valmir P Moraes Filho,Jinsu Hong,Jasmine R. Kobayashi,Evangelia Samara,Joseph Gallego*

Main category: cs.LG

TL;DR: A scalable ML framework for Parker Solar Probe solar wind data using Dask and quantum-inspired Kernel Density Matrices (KDM) to estimate univariate and bivariate distributions and anomaly thresholds, enabling reproducible analysis of 150+ GB of data; reveals clear solar wind trends and provides public data/tools.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of analyzing large-scale, in situ solar wind measurements to extract reliable distributions and anomaly markers, enabling scalable, interpretable insights for space weather studies and forecasting.

Method: Distributed computations with Dask for large-scale statistics; Kernel Density Matrices (KDM) to estimate univariate and bivariate distributions of solar wind speed, proton density, and proton thermal speed; derive anomaly thresholds; analyze trends in the inner heliosphere; produce reproducible data products.

Result: Found that solar wind speed increases with distance from the Sun while proton density decreases; an inverse relationship exists between speed and density; established anomaly thresholds for key parameters; framework is scalable, interpretable, and results are reproducible; data products and code made publicly available.

Conclusion: The proposed framework provides a tractable and scalable approach for analyzing large in situ solar wind datasets, with interpretable results and emphasis on reproducibility; public data/products and open-source code support future solar wind research and space weather forecasting.

Abstract: We present a scalable machine learning framework for analyzing Parker Solar
Probe (PSP) solar wind data using distributed processing and the
quantum-inspired Kernel Density Matrices (KDM) method. The PSP dataset
(2018--2024) exceeds 150 GB, challenging conventional analysis approaches. Our
framework leverages Dask for large-scale statistical computations and KDM to
estimate univariate and bivariate distributions of key solar wind parameters,
including solar wind speed, proton density, and proton thermal speed, as well
as anomaly thresholds for each parameter. We reveal characteristic trends in
the inner heliosphere, including increasing solar wind speed with distance from
the Sun, decreasing proton density, and the inverse relationship between speed
and density. Solar wind structures play a critical role in enhancing and
mediating extreme space weather phenomena and can trigger geomagnetic storms;
our analyses provide quantitative insights into these processes. This approach
offers a tractable, interpretable, and distributed methodology for exploring
complex physical datasets and facilitates reproducible analysis of large-scale
in situ measurements. Processed data products and analysis tools are made
publicly available to advance future studies of solar wind dynamics and space
weather forecasting. The code and configuration files used in this study are
publicly available to support reproducibility.

</details>


### [60] [The Virtues of Brevity: Avoid Overthinking in Parallel Test-Time Reasoning](https://arxiv.org/abs/2510.21067)
*Raul Cavalcante Dinardi,Bruno Yamamoto,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 最短解策略在并行推理中取得与更复杂方法（如自我一致性）相当的性能，同时显著降低计算成本。该现象可归因于模型在两种推理状态的存在：简短自信的常规态和冗长的过度思考态，最短解偏向前者。


<details>
  <summary>Details</summary>
Motivation: 在复杂推理任务（如数学与编码）中提升推理性能的同时降低计算开销，寻找简单且有效的解采样策略以替代昂贵的自我一致性等方法。

Method: 在并行推理场景中对多解进行采样，并对比选择最短解的启发式与自我一致性等方法的效果；在两个挑战性基准上评估性能与计算成本；分析模型在两种推理 regime 下的行为并识别临界点。

Result: 最短解策略在两个基准上与自我一致性具有竞争力，同时显著降低计算成本；该策略对输出相等性不严格定义的任务也有有效性，呈现对多样任务的 Pareto 改善。

Conclusion: 最短解提供了一种简单但有效的推理策略，可在多任务场景中替代复杂推理方法并获得成本-性能的显著改进。未来工作可扩展到更多任务和模型，并深入探讨两种推理状态的理论基础。

Abstract: Reasoning models represent a significant advance in LLM capabilities,
particularly for complex reasoning tasks such as mathematics and coding.
Previous studies confirm that parallel test-time compute-sampling multiple
solutions and selecting the best one-can further enhance the predictive
performance of LLMs. However, strategies in this area often require complex
scoring, thus increasing computational cost and complexity. In this work, we
demonstrate that the simple and counterintuitive heuristic of selecting the
shortest solution is highly effective. We posit that the observed effectiveness
stems from models operating in two distinct regimes: a concise, confident
conventional regime and a verbose overthinking regime characterized by
uncertainty, and we show evidence of a critical point where the overthinking
regime begins to be significant. By selecting the shortest answer, the
heuristic preferentially samples from the conventional regime. We confirm that
this approach is competitive with more complex methods such as self-consistency
across two challenging benchmarks while significantly reducing computational
overhead. The shortest-answer heuristic provides a Pareto improvement over
self-consistency and applies even to tasks where output equality is not well
defined.

</details>


### [61] [Accelerating Mobile Inference through Fine-Grained CPU-GPU Co-Execution](https://arxiv.org/abs/2510.21081)
*Zhuojin Li,Marco Paolieri,Leana Golubchik*

Main category: cs.LG

TL;DR: 在移动设备实现 CPU-GPU 协同推理的框架，使用 OpenCL SVM 进行轻量同步并用机器学习预测执行时间，从而快速选择策略，达到近理论上限的加速（线性层1.89x、卷积层1.75x）。


<details>
  <summary>Details</summary>
Motivation: 移动设备资源受限，单一处理单元难以满足高性能推理需求；统一内存架构和 CPU-GPU 性能差距缩小带来协同机会；然而，大幅度的同步开销与对任务执行时间的不可预测性成为主要瓶颈。

Method: 提出两大核心组件：1) 基于 OpenCL 的细粒度共享虚拟内存（SVM）实现的轻量级同步机制，用以降低 CPU-GPU 之间的同步开销；2) 机器学习模型用于准确预测 CPU 和 GPU 上任务的执行时间（包含 GPU 内核的调度时间和不同实现/并行度的影响），据此快速选取协同策略。

Result: 在四个平台上的评估表明，所提出的方法能迅速选取 CPU-GPU 协同执行策略，线性层加速至 1.89x，卷积层加速至 1.75x；这些结果接近在 Pixel 5 上通过穷举网格搜索得到的最大值（线性 2.01x，卷积 1.87x）。

Conclusion: 将硬件感知的调度与准确的运行时间预测结合，可以在移动端实现高效的 CPU-GPU 协同推理，性能接近理论极限。

Abstract: Deploying deep neural networks on mobile devices is increasingly important
but remains challenging due to limited computing resources. On the other hand,
their unified memory architecture and narrower gap between CPU and GPU
performance provide an opportunity to reduce inference latency by assigning
tasks to both CPU and GPU. The main obstacles for such collaborative execution
are the significant synchronization overhead required to combine partial
results, and the difficulty of predicting execution times of tasks assigned to
CPU and GPU (due to the dynamic selection of implementations and parallelism
level). To overcome these obstacles, we propose both a lightweight
synchronization mechanism based on OpenCL fine-grained shared virtual memory
(SVM) and machine learning models to accurately predict execution times.
Notably, these models capture the performance characteristics of GPU kernels
and account for their dispatch times. A comprehensive evaluation on four mobile
platforms shows that our approach can quickly select CPU-GPU co-execution
strategies achieving up to 1.89x speedup for linear layers and 1.75x speedup
for convolutional layers (close to the achievable maximum values of 2.01x and
1.87x, respectively, found by exhaustive grid search on a Pixel~5 smartphone).

</details>


### [62] [DictPFL: Efficient and Private Federated Learning on Encrypted Gradients](https://arxiv.org/abs/2510.21086)
*Jiaqi Xue,Mayank Kumar,Yuzhang Shang,Shangqian Gao,Rui Ning,Mengxin Zheng,Xiaoqian Jiang,Qian Lou*

Main category: cs.LG

TL;DR: DictPFL通过将权重分解为静态字典与可更新查找表、仅对后者加密并聚合，同时对加密参数进行剪枝，实现在低开销下的全梯度保护的HE基私有联邦学习。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，梯度泄露风险依然存在；全同态加密（HE）聚合虽然安全，但代价高昂；现有方法要么完全加密梯度成本高，要么部分加密易暴露漏洞。因此需要在保护全部梯度的同时降低计算与通信开销，使HE-based私有FL在实际场景中可行。

Method: 提出DictPFL，包含两个关键模块：1) Decompose-for-Partial-Encrypt（DePE）：将模型权重分解为静态字典（本地保存）和可更新的查找表（需要加密并聚合），静态字典不共享也不加密；2) Prune-for-Minimum-Encrypt（PrME）：引入加密感知的剪枝，对加密参数进行最小化，以历史-一致的掩码降低加密参数数量。

Result: 实验结果显示：与完全加密的FL相比，通信成本降低约402-748倍，训练加速约28-65倍；相较于现有的选择性加密方法，开销提升约51-155倍、速度提升约4-19倍。此外，DictPFL的运行时接近明文FL的2倍内，首次证明HE基私有FL在现实部署中的可行性。代码公开。

Conclusion: 通过DePE与PrME两大模块，DictPFL实现对所有梯度的全量保护且显著降低开销，证明HE-based私有联邦学习具备现实应用潜力。

Abstract: Federated Learning (FL) enables collaborative model training across
institutions without sharing raw data. However, gradient sharing still risks
privacy leakage, such as gradient inversion attacks. Homomorphic Encryption
(HE) can secure aggregation but often incurs prohibitive computational and
communication overhead. Existing HE-based FL methods sit at two extremes:
encrypting all gradients for full privacy at high cost, or partially encrypting
gradients to save resources while exposing vulnerabilities. We present DictPFL,
a practical framework that achieves full gradient protection with minimal
overhead. DictPFL encrypts every transmitted gradient while keeping
non-transmitted parameters local, preserving privacy without heavy computation.
It introduces two key modules: Decompose-for-Partial-Encrypt (DePE), which
decomposes model weights into a static dictionary and an updatable lookup
table, only the latter is encrypted and aggregated, while the static dictionary
remains local and requires neither sharing nor encryption; and
Prune-for-Minimum-Encrypt (PrME), which applies encryption-aware pruning to
minimize encrypted parameters via consistent, history-guided masks. Experiments
show that DictPFL reduces communication cost by 402-748$\times$ and accelerates
training by 28-65$\times$ compared to fully encrypted FL, while outperforming
state-of-the-art selective encryption methods by 51-155$\times$ in overhead and
4-19$\times$ in speed. Remarkably, DictPFL's runtime is within 2$\times$ of
plaintext FL, demonstrating for the first time, that HE-based private federated
learning is practical for real-world deployment. The code is publicly available
at https://github.com/UCF-ML-Research/DictPFL.

</details>


### [63] [ESCORT: Efficient Stein-variational and Sliced Consistency-Optimized Temporal Belief Representation for POMDPs](https://arxiv.org/abs/2510.21107)
*Yunuo Zhang,Baiting Luo,Ayan Mukhopadhyay,Gabor Karsai,Abhishek Dubey*

Main category: cs.LG

TL;DR: 提出ESCORT，一种基于粒子的方法，扩展SVGD用于高维多模态信念表示，在POMDP中更准确地捕捉不确定性，避免重采样，提升决策质量。


<details>
  <summary>Details</summary>
Motivation: 在POMDP中，信念分布随着状态空间维度和多模态性增强，现有方法难以准确捕捉复杂不确定性，易导致推断误差和次优策略；需要不依赖固定分布假设的灵活表示，同时避免粒子退化。

Method: 在SVGD框架上引入两大创新：一是相关性感知投影，建模状态维度之间的依赖关系；二是时间一致性约束，稳定更新同时保持相关结构。该框架保持SVGD的粒子间吸引-排斥机制，且无需重采样或固定分布假设。

Result: 在POMDP领域和合成多模态分布的不同维度上进行广泛评估，ESCORT在信念近似精度和下游决策质量方面持续超越最先进方法。

Conclusion: ESCORT提供了一种高效、自适应的多模态高维信念表示框架，提升在实际决策中的表现，避免粒子退化问题，适用于复杂不确定性场景。

Abstract: In Partially Observable Markov Decision Processes (POMDPs), maintaining and
updating belief distributions over possible underlying states provides a
principled way to summarize action-observation history for effective
decision-making under uncertainty. As environments grow more realistic, belief
distributions develop complexity that standard mathematical models cannot
accurately capture, creating a fundamental challenge in maintaining
representational accuracy. Despite advances in deep learning and probabilistic
modeling, existing POMDP belief approximation methods fail to accurately
represent complex uncertainty structures such as high-dimensional, multi-modal
belief distributions, resulting in estimation errors that lead to suboptimal
agent behaviors. To address this challenge, we present ESCORT (Efficient
Stein-variational and sliced Consistency-Optimized Representation for Temporal
beliefs), a particle-based framework for capturing complex, multi-modal
distributions in high-dimensional belief spaces. ESCORT extends SVGD with two
key innovations: correlation-aware projections that model dependencies between
state dimensions, and temporal consistency constraints that stabilize updates
while preserving correlation structures. This approach retains SVGD's
attractive-repulsive particle dynamics while enabling accurate modeling of
intricate correlation patterns. Unlike particle filters prone to degeneracy or
parametric methods with fixed representational capacity, ESCORT dynamically
adapts to belief landscape complexity without resampling or restrictive
distributional assumptions. We demonstrate ESCORT's effectiveness through
extensive evaluations on both POMDP domains and synthetic multi-modal
distributions of varying dimensionality, where it consistently outperforms
state-of-the-art methods in terms of belief approximation accuracy and
downstream decision quality.

</details>


### [64] [Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference](https://arxiv.org/abs/2510.21184)
*Stephen Zhao,Aidan Li,Rob Brekelmans,Roger Grosse*

Main category: cs.LG

TL;DR: RePULSe 是一种在标准 RL 损失上增添一个基于学习 proposal 的额外损失的对齐训练方法，通过引导对低奖励输出的采样并降低其概率，提高平均奖励与不良输出概率之间的权衡，同时提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有 RL 常用优化平均奖励，但降低不良输出往往损害平均性能；需要在不显著牺牲平均奖励的前提下降低 undesired outputs 的概率，并增强对对抗攻击的鲁棒性。

Method: 在标准 RL 损失基础上引入额外损失，该损失依赖学习到的 proposals 指导对低奖励输出的采样，并降低这些输出的概率，从而形成更强的惩罚信号；通过对比实验验证其效果。

Result: 在期望奖励与不良输出概率的权衡方面，RePULSe 优于传统 RL 对齐方法与替代方案；在对抗性鲁棒性方面也展现出更强的鲁棒性。

Conclusion: 通过引入 proposal 指导的额外损失，RePULSe 提升了对齐训练的权衡效率与鲁棒性，适用于需要严格控制低质量输出的场景。

Abstract: Reinforcement learning (RL) has become a predominant technique to align
language models (LMs) with human preferences or promote outputs which are
deemed to be desirable by a given reward function. Standard RL approaches
optimize average reward, while methods explicitly focused on reducing the
probability of undesired outputs typically come at a cost to average-case
performance. To improve this tradeoff, we introduce RePULSe, a new training
method that augments the standard RL loss with an additional loss that uses
learned proposals to guide sampling low-reward outputs, and then reduces those
outputs' probability. We run experiments demonstrating that RePULSe produces a
better tradeoff of expected reward versus the probability of undesired outputs
and is more adversarially robust, compared to standard RL alignment approaches
and alternatives.

</details>


### [65] [Distributionally Robust Feature Selection](https://arxiv.org/abs/2510.21113)
*Maitreyi Swaroop,Tamar Krishnamurti,Bryan Wilder*

Main category: cs.LG

TL;DR: Proposes a model-agnostic method for selecting a small set of features that generalizes across multiple subpopulations by a continuous relaxation with a noising mechanism, optimizing the Bayes-optimal predictor variance; does not require training-time backpropagation; validated on synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: In costly data collection settings, need feature subsets that yield strong predictive performance across diverse populations, enabling cost-efficient downstream models.

Method: Introduce a continuous relaxation of feature selection via a noising mechanism and optimize the variance of the Bayes-optimal predictor. The approach is model-agnostic and avoids backpropagating through model training.

Result: Empirical validation on synthetic and real-world datasets shows that the selected features support high-quality downstream models across subpopulations.

Conclusion: Offers a practical, model-agnostic feature selection framework that balances cross-population performance while reducing observation costs; applicable to settings with costly feature acquisition.

Abstract: We study the problem of selecting limited features to observe such that
models trained on them can perform well simultaneously across multiple
subpopulations. This problem has applications in settings where collecting each
feature is costly, e.g. requiring adding survey questions or physical sensors,
and we must be able to use the selected features to create high-quality
downstream models for different populations. Our method frames the problem as a
continuous relaxation of traditional variable selection using a noising
mechanism, without requiring backpropagation through model training processes.
By optimizing over the variance of a Bayes-optimal predictor, we develop a
model-agnostic framework that balances overall performance of downstream
prediction across populations. We validate our approach through experiments on
both synthetic datasets and real-world data.

</details>


### [66] [SolarBoost: Distributed Photovoltaic Power Forecasting Amid Time-varying Grid Capacity](https://arxiv.org/abs/2510.21129)
*Linyuan Geng,Linxiao Yang,Xinyue Gu,Liang Sun*

Main category: cs.LG

TL;DR: SolarBoost提出一种基于网格级的分布式光伏功率预测方法，将聚合输出视为小网格输出的加权和，单位输出函数与容量分离，提高对缺失数据、容量变化和地域多样性的鲁棒性，配套上界近似优化以降低计算开销；在中国多个城市的实测中表现优越，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决DPV系统中缺失数据、容量动态变化、地理变异和面板多样性导致的集中式估计困难，寻求可扩展且准确的功率预测方法。

Method: 将聚合输出建模为来自若干小网格的输出之和，每个网格的输出等于容量(c_i)乘以单位输出函数f_i(输入特征)。实现对同质单位输出函数与容量的解耦。提出对损失函数的上界近似以提高计算效率，并给出高效算法。通过理论分析和实验验证，並在中国多城部署。

Result: 网格级建模优于直接的集中式建模，理论和实验证明其优越性；在实际部署中显著降低潜在损失。

Conclusion: SolarBoost提供一种可扩展且准确的DPV功率预测框架，通过单元输出函数与容量解耦实现对异质性和容量变化的鲁棒预测，具有实际运营意义并且代码开源。

Abstract: This paper presents SolarBoost, a novel approach for forecasting power output
in distributed photovoltaic (DPV) systems. While existing centralized
photovoltaic (CPV) methods are able to precisely model output dependencies due
to uniformity, it is difficult to apply such techniques to DPV systems, as DPVs
face challenges such as missing grid-level data, temporal shifts in installed
capacity, geographic variability, and panel diversity. SolarBoost overcomes
these challenges by modeling aggregated power output as a composite of output
from small grids, where each grid output is modeled using a unit output
function multiplied by its capacity. This approach decouples the homogeneous
unit output function from dynamic capacity for accurate prediction. Efficient
algorithms over an upper-bound approximation are proposed to overcome
computational bottlenecks in loss functions. We demonstrate the superiority of
grid-level modeling via theoretical analysis and experiments. SolarBoost has
been validated through deployment across various cities in China, significantly
reducing potential losses and provides valuable insights for the operation of
power grids. The code for this work is available at
https://github.com/DAMO-DI-ML/SolarBoost.

</details>


### [67] [Cloud-Fog-Edge Collaborative Computing for Sequential MIoT Workflow: A Two-Tier DDPG-Based Scheduling Framework](https://arxiv.org/abs/2510.21135)
*Yuhao Fu,Yinghao Zhang,Yalin Liu,Bishenghui Tao,Junhong Ruan*

Main category: cs.LG

TL;DR: Two-tier DDPG-based scheduling for MIoT: global layer selection (edge/fog/cloud) with local per-layer node assignment to minimize makespan; shows improved performance as workflow complexity increases.


<details>
  <summary>Details</summary>
Motivation: MIoT requires strict end-to-end latency guarantees for sequential healthcare workflows over heterogeneous cloud-fog-edge infrastructure. Scheduling to minimize makespan is NP-hard, demanding learning-based, scalable solutions.

Method: A hierarchical reinforcement learning framework: a global controller selects the deployment layer (edge, fog, or cloud) for each workflow stage, while dedicated local controllers assign tasks to specific nodes within the chosen layer, all optimized to minimize workflow makespan.

Result: Experiments demonstrate that the proposed approach outperforms baselines, with performance gains growing with workflow complexity, indicating the method’s ability to learn effective long-term scheduling strategies for large-scale MIoT scenarios.

Conclusion: A two-tier DDPG scheduling framework effectively addresses MIoT workflow scheduling by combining high-level layer decisions with low-level node assignments, yielding scalable, performance-enhancing policies as complexity increases.

Abstract: The Medical Internet of Things (MIoT) demands stringent end-to-end latency
guarantees for sequential healthcare workflows deployed over heterogeneous
cloud-fog-edge infrastructures. Scheduling these sequential workflows to
minimize makespan is an NP-hard problem. To tackle this challenge, we propose a
Two-tier DDPG-based scheduling framework that decomposes the scheduling
decision into a hierarchical process: a global controller performs layer
selection (edge, fog, or cloud), while specialized local controllers handle
node assignment within the chosen layer. The primary optimization objective is
the minimization of the workflow makespan. Experiments results validate our
approach, demonstrating increasingly superior performance over baselines as
workflow complexity rises. This trend highlights the frameworks ability to
learn effective long-term strategies, which is critical for complex,
large-scale MIoT scheduling scenarios.

</details>


### [68] [Leverage Unlearning to Sanitize LLMs](https://arxiv.org/abs/2510.21322)
*Antoine Boutet,Lucas Magnana*

Main category: cs.LG

TL;DR: SANI 提出一种针对语言模型的“未学习/遗忘”方法，结合 erasure（剪除/重置最后几层的部分神经元以打断对敏感信息的记忆）与 repair（在避免再次记忆敏感信息的前提下进行微调）两阶段。通过对医疗数据微调的模型与标准预训练模型进行评估，仅需少量额外的未学习轮次即可显著降低对敏感信息的再暴露，同时保持模型性能，便于在医院等场景中安全分享。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能 memorise 训练数据中的敏感信息，直接分享或部署存在隐私风险；需要一种高效、成本可控的去记忆（unlearning）方法，避免在受保护数据集上进行昂贵的再训练。

Method: SANI 包含两阶段：1) erasure：重置模型最后几层的若干神经元，以打断对细粒度信息的 memorization；2) repair：在避免 memorizing 敏感信息的前提下对模型进行微调，以保持性能并降低再暴露概率。对在医疗数据上微调的模型以及标准预训练模型进行去记忆评估，删除直接和间接的敏感标识。

Result: 通过仅有的少量未学习轮次，模型获得去记忆效果，regurgitations（再暴露/回忆的次数）显著下降。该方法在已投入大量资源训练的数据集的行业场景，尤其医院等领域，具有实际价值。

Conclusion: SANI 提供了一种高效的未学习框架，能够在不需要对整個数据集进行昂贵再训练的前提下去除/显著降低敏感信息的记忆，从而实现更安全的模型分享。

Abstract: Pre-trained large language models (LLMs) are becoming useful for various
tasks. To improve their performance on certain tasks, it is necessary to
fine-tune them on specific data corpora (e.g., medical reports, business data).
These specialized data corpora may contain sensitive data (e.g., personal or
confidential data) that will be memorized by the model and likely to be
regurgitated during its subsequent use. This memorization of sensitive
information by the model poses a significant privacy or confidentiality issue.
To remove this memorization and sanitize the model without requiring costly
additional fine-tuning on a secured data corpus, we propose SANI. SANI is an
unlearning approach to sanitize language models. It relies on both an erasure
and repair phases that 1) reset certain neurons in the last layers of the model
to disrupt the memorization of fine-grained information, and then 2) fine-tune
the model while avoiding memorizing sensitive information. We comprehensively
evaluate SANI to sanitize both a model fine-tuned and specialized with medical
data by removing directly and indirectly identifiers from the memorization of
the model, and a standard pre-trained model by removing specific terms defined
as confidential information from the model. Results show that with only few
additional epochs of unlearning, the model is sanitized and the number of
regurgitations is drastically reduced. This approach can be particularly useful
for hospitals or other industries that have already spent significant resources
training models on large datasets and wish to sanitize them before sharing.

</details>


### [69] [Uncertainty-Aware Multi-Objective Reinforcement Learning-Guided Diffusion Models for 3D De Novo Molecular Design](https://arxiv.org/abs/2510.21153)
*Lianghong Chen,Dongkyu Eugene Kim,Mike Domaratzki,Pingzhao Hu*

Main category: cs.LG

TL;DR: An uncertainty-aware RL framework steers 3D molecular diffusion models toward multi-objective optimization, improving both molecular quality and property attainment.


<details>
  <summary>Details</summary>
Motivation: In real-world molecular design, simultaneously satisfying multiple properties is crucial, but diffusion-based generators struggle to balance objectives. Incorporating predictive uncertainty into reward shaping aims to robustly guide generation toward desirable trade-offs.

Method: A reinforcement learning setup with surrogate models that estimate predictive uncertainty to dynamically shape multi-objective rewards for diffusion-based 3D molecular generation. The approach is evaluated across three benchmark datasets and multiple diffusion architectures, with MD simulations and ADMET profiling on top candidates.

Result: The framework consistently outperforms baselines in molecular quality and multi-property optimization across datasets and architectures. Top candidates show drug-like behavior and stable binding in MD simulations, comparable to known EGFR inhibitors.

Conclusion: Uncertainty-aware RL-guided diffusion models offer improved control over multi-objective molecular design, showing strong potential for automated, drug-like 3D molecule generation.

Abstract: Designing de novo 3D molecules with desirable properties remains a
fundamental challenge in drug discovery and molecular engineering. While
diffusion models have demonstrated remarkable capabilities in generating
high-quality 3D molecular structures, they often struggle to effectively
control complex multi-objective constraints critical for real-world
applications. In this study, we propose an uncertainty-aware Reinforcement
Learning (RL) framework to guide the optimization of 3D molecular diffusion
models toward multiple property objectives while enhancing the overall quality
of the generated molecules. Our method leverages surrogate models with
predictive uncertainty estimation to dynamically shape reward functions,
facilitating balance across multiple optimization objectives. We
comprehensively evaluate our framework across three benchmark datasets and
multiple diffusion model architectures, consistently outperforming baselines
for molecular quality and property optimization. Additionally, Molecular
Dynamics (MD) simulations and ADMET profiling of top generated candidates
indicate promising drug-like behavior and binding stability, comparable to
known Epidermal Growth Factor Receptor (EGFR) inhibitors. Our results
demonstrate the strong potential of RL-guided generative diffusion models for
advancing automated molecular design.

</details>


### [70] [A Unified Matrix Factorization Framework for Classical and Robust Clustering](https://arxiv.org/abs/2510.21172)
*Angshul Majumdar*

Main category: cs.LG

TL;DR: 提出一个统一的矩阵分解框架，将经典和鲁棒聚类（ crisp k-means 与 fuzzy c-means ）统一为矩阵分解问题，并扩展到对异常值鲁棒的情形。对两者都给出交替最小化算法，及基于 IRLS 的鲁棒算法，理论上证明收敛到局部最小值。


<details>
  <summary>Details</summary>
Motivation: 在聚类之间建立一个统一的视角，借助矩阵分解的框架来同时表示 crisp k-means 和 fuzzy c-means，并为鲁棒聚类提供一个系统的推导与算法框架。

Method: 将 crisp k-means 和 fuzzy c-means 各自等价地转化为矩阵分解问题，提出统一的因子矩阵优化形式；将 Frobenius 范数替换为 l1,2-范数以提升对离群点的鲁棒性；给出标准形式的交替最小化算法，以及鲁棒形式的 IRLS（迭代重加权最小二乘）算法，并给出收敛性到局部极小值的理论保证。

Result: 建立了一个可统一处理两种聚类的矩阵分解框架及其鲁棒变体，并给出收敛性证明。理论上将两类聚类问题纳入同一个优化框架，便于进一步的扩展。

Conclusion: 该工作提供了一个统一且可扩展的聚类分析工具，将经典与鲁棒聚类纳入同一矩阵分解框架，并通过算法设计保障收敛性，为未来的鲁棒聚类研究提供了系统化的基线。

Abstract: This paper presents a unified matrix factorization framework for classical
and robust clustering. We begin by revisiting the well-known equivalence
between crisp k-means clustering and matrix factorization, following and
rigorously rederiving an unpublished formulation by Bauckhage. Extending this
framework, we derive an analogous matrix factorization interpretation for fuzzy
c-means clustering, which to the best of our knowledge has not been previously
formalized. These reformulations allow both clustering paradigms to be
expressed as optimization problems over factor matrices, thereby enabling
principled extensions to robust variants. To address sensitivity to outliers,
we propose robust formulations for both crisp and fuzzy clustering by replacing
the Frobenius norm with the l1,2-norm, which penalizes the sum of Euclidean
norms across residual columns. We develop alternating minimization algorithms
for the standard formulations and IRLS-based algorithms for the robust
counterparts. All algorithms are theoretically proven to converge to a local
minimum.

</details>


### [71] [FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models](https://arxiv.org/abs/2510.21363)
*Zihao Fu,Ryan Brown,Shun Shao,Kai Rawal,Eoin Delaney,Chris Russell*

Main category: cs.LG

TL;DR: Post-hoc debiasing for text-to-image diffusion via FairImagen using prompt embedding projection, improving fairness across demographic attributes with moderate image quality trade-off.


<details>
  <summary>Details</summary>
Motivation: Mitigate amplification of gender/racial biases in diffusion models without retraining the model.

Method: Project CLIP-based prompt embeddings into a debiased subspace using Fair PCA; add empirical noise injection; propose unified cross-demographic projection; model-agnostic; evaluates on gender, race, and intersectional settings.

Result: Significant improvements in fairness across gender, race, and intersectionality with some degradation in image quality/prompt fidelity; outperforms existing post-hoc debiasing methods.

Conclusion: Simple, scalable, model-agnostic approach that reduces bias in text-to-image generation; potential trade-offs noted; future work may extend to more attributes or refine content preservation.

Abstract: Text-to-image diffusion models, such as Stable Diffusion, have demonstrated
remarkable capabilities in generating high-quality and diverse images from
natural language prompts. However, recent studies reveal that these models
often replicate and amplify societal biases, particularly along demographic
attributes like gender and race. In this paper, we introduce FairImagen
(https://github.com/fuzihaofzh/FairImagen), a post-hoc debiasing framework that
operates on prompt embeddings to mitigate such biases without retraining or
modifying the underlying diffusion model. Our method integrates Fair Principal
Component Analysis to project CLIP-based input embeddings into a subspace that
minimizes group-specific information while preserving semantic content. We
further enhance debiasing effectiveness through empirical noise injection and
propose a unified cross-demographic projection method that enables simultaneous
debiasing across multiple demographic attributes. Extensive experiments across
gender, race, and intersectional settings demonstrate that FairImagen
significantly improves fairness with a moderate trade-off in image quality and
prompt fidelity. Our framework outperforms existing post-hoc methods and offers
a simple, scalable, and model-agnostic solution for equitable text-to-image
generation.

</details>


### [72] [A visual big data system for the prediction of weather-related variables: Jordan-Spain case study](https://arxiv.org/abs/2510.21176)
*Shadi Aljawarneh,Juan A. Lara,Muneer Bani Yassein*

Main category: cs.LG

TL;DR: 为气象数据提出一个可视化大数据系统，结合本地NoSQL存储，进行单变量/多变量预测分析与邻站数据的缺失值预测，初步结果显示极低的归一化均方误差和较高的方向对称性。


<details>
  <summary>Details</summary>
Motivation: 气象数据量大、维度高、存在缺失值且变量间相关性高；需要应用大数据与数据挖掘方法来从中提取有用知识并进行天气现象预测。

Method: 收集开放数据并加载到本地NoSQL数据库，在时空层级上进行聚合融合；采用单变量与多变量预测方法，并在缺失值情况通过邻近气象站的训练数据进行预测；评估系统的可用性和预测性能。

Result: 系统在预测性能方面取得NMSE 0.00013、方向对称性约0.84的指标；专家组对系统的各方面给予积极评价，除图形设计外大多评分为3分及以上。

Conclusion: 初步结果表明所提系统具有有效性和潜力，值得在该领域继续深入研究。

Abstract: The Meteorology is a field where huge amounts of data are generated, mainly
collected by sensors at weather stations, where different variables can be
measured. Those data have some particularities such as high volume and
dimensionality, the frequent existence of missing values in some stations, and
the high correlation between collected variables. In this regard, it is crucial
to make use of Big Data and Data Mining techniques to deal with those data and
extract useful knowledge from them that can be used, for instance, to predict
weather phenomena. In this paper, we propose a visual big data system that is
designed to deal with high amounts of weather-related data and lets the user
analyze those data to perform predictive tasks over the considered variables
(temperature and rainfall). The proposed system collects open data and loads
them onto a local NoSQL database fusing them at different levels of temporal
and spatial aggregation in order to perform a predictive analysis using
univariate and multivariate approaches as well as forecasting based on training
data from neighbor stations in cases with high rates of missing values. The
system has been assessed in terms of usability and predictive performance,
obtaining an overall normalized mean squared error value of 0.00013, and an
overall directional symmetry value of nearly 0.84. Our system has been rated
positively by a group of experts in the area (all aspects of the system except
graphic desing were rated 3 or above in a 1-5 scale). The promising preliminary
results obtained demonstrate the validity of our system and invite us to keep
working on this area.

</details>


### [73] [Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations](https://arxiv.org/abs/2510.21631)
*Faisal Hamman,Pasan Dissanayake,Yanjun Fu,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: 提出了基于反事实解释的蒸馏方法 CoD，可在极少样本下实现任务感知蒸馏并提升边界知识。


<details>
  <summary>Details</summary>
Motivation: 解决任务感知蒸馏需要大量数据的问题，尤其在数据稀缺场景中难以获取训练样本。

Method: 引入 Counterfactual-explanation-infused Distillation (CoD)，通过 CFEs 来探测教师的决策边界，结合统计与几何分析，提供更信息密度的样本用于蒸馏，实验显示在 8-512 个样本的极小数据场景下优于传统方法，同时仅用原样本的一半并配 CFEs 即可提升性能。

Result: 在多数据集和多LLMs上验证，CoD 在极少样本下优于标准蒸馏；CFEs 作为知识探针提高学生模仿边界的能力。

Conclusion: CFEs 能有效地帮助学生更准确地模仿教师的边界，理论和实证均支持在数据受限场景下的高效任务感知蒸馏。

Abstract: Knowledge distillation is a promising approach to transfer capabilities from
complex teacher models to smaller, resource-efficient student models that can
be deployed easily, particularly in task-aware scenarios. However, existing
methods of task-aware distillation typically require substantial quantities of
data which may be unavailable or expensive to obtain in many practical
scenarios. In this paper, we address this challenge by introducing a novel
strategy called Counterfactual-explanation-infused Distillation CoD for
few-shot task-aware knowledge distillation by systematically infusing
counterfactual explanations. Counterfactual explanations (CFEs) refer to inputs
that can flip the output prediction of the teacher model with minimum
perturbation. Our strategy CoD leverages these CFEs to precisely map the
teacher's decision boundary with significantly fewer samples. We provide
theoretical guarantees for motivating the role of CFEs in distillation, from
both statistical and geometric perspectives. We mathematically show that CFEs
can improve parameter estimation by providing more informative examples near
the teacher's decision boundary. We also derive geometric insights on how CFEs
effectively act as knowledge probes, helping the students mimic the teacher's
decision boundaries more effectively than standard data. We perform experiments
across various datasets and LLMs to show that CoD outperforms standard
distillation approaches in few-shot regimes (as low as 8-512 samples). Notably,
CoD only uses half of the original samples used by the baselines, paired with
their corresponding CFEs and still improves performance.

</details>


### [74] [PLAN: Proactive Low-Rank Allocation for Continual Learning](https://arxiv.org/abs/2510.21188)
*Xiequn Wang,Zhan Zhuang,Yu Zhang*

Main category: cs.LG

TL;DR: A proactive low-rank allocation method (PLAN) extends LoRA for interference-aware continual learning, achieving SOTA on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: In continual learning (CL), fine-tuning large pre-trained models causes forgetting and interference between task subspaces. There is a need for scalable, parameter-efficient adaptation that proactively manages task-specific subspaces.

Method: PLAN introduces orthogonal basis vectors for each task within a LoRA framework, uses a perturbation-based optimization to minimize interference with previously learned parameters, and employs a selection mechanism to assign basis vectors with low sensitivity to interference, enabling efficient task-specific adaptation.

Result: Empirical results on standard CL benchmarks show PLAN consistently outperforms existing methods and sets a new state-of-the-art for continual learning with foundation models.

Conclusion: PLAN provides an effective and efficient approach to continual learning by proactively allocating interference-aware subspaces, balancing protection of past knowledge with rapid adaptation to new tasks.

Abstract: Continual learning (CL) requires models to continuously adapt to new tasks
without forgetting past knowledge. In this work, we propose
\underline{P}roactive \underline{L}ow-rank \underline{A}llocatio\underline{N}
(PLAN), a framework that extends Low-Rank Adaptation (LoRA) to enable efficient
and interference-aware fine-tuning of large pre-trained models in CL settings.
PLAN proactively manages the allocation of task-specific subspaces by
introducing orthogonal basis vectors for each task and optimizing them through
a perturbation-based strategy that minimizes conflicts with previously learned
parameters. Furthermore, PLAN incorporates a novel selection mechanism that
identifies and assigns basis vectors with minimal sensitivity to interference,
reducing the risk of degrading past knowledge while maintaining efficient
adaptation to new tasks. Empirical results on standard CL benchmarks
demonstrate that PLAN consistently outperforms existing methods, establishing a
new state-of-the-art for continual learning with foundation models.

</details>


### [75] [Gen-Review: A Large-scale Dataset of AI-Generated (and Human-written) Peer Reviews](https://arxiv.org/abs/2510.21192)
*Luca Demetrio,Giovanni Apruzzese,Kathrin Grosse,Pavel Laskov,Emil Lupu,Vera Rimmer,Philine Widmer*

Main category: cs.LG

TL;DR: GenReview 构建了迄今为止最大的用于研究 LLM 在同行评审中的影响的数据集，包含 81,000 条基于 2018–2025 年 ICLR 提交的 LLM 评审（使用负面、正面、中性三种提示独立生成），并将其与论文及原始评审链接，便于开展偏见检测、自动辨识、遵循评审指令的能力及评分与接收决策的关系等研究。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模、可标注的数据资源来系统地研究 LLM 在同行评审中的作用、偏差与治理需求，因此需要一个结合论文、原始评审及 LLM 评审的综合数据集来评估效用与风险。

Method: 对 ICLR 2018–2025 的全部投稿，使用三种独立提示（负面、正面、中性）生成 LLM 写作的评审，共计 81k 条，并将生成的评审与论文及原始评审相连，形成可供广泛研究的 GenReview 数据集。

Result: 揭示了 LLM 在评审中的偏见存在、能被自动检测、并非始终严格遵循评审指令；LLM 给出的评分与论文是否被接受的最终决策之间存在一定对齐性，但仅在接受论文的情况下成立。数据集的公开性为未来研究提供了可重复的基准。

Conclusion: GenReview 解决了该领域的数据空缺，提供了评估 LLM 在同行评审中的应用、伦理与治理的基准平台，强调对偏见、合规性与透明度的持续关注，并推动审稿过程的更可控研究。

Abstract: How does the progressive embracement of Large Language Models (LLMs) affect
scientific peer reviewing? This multifaceted question is fundamental to the
effectiveness -- as well as to the integrity -- of the scientific process.
Recent evidence suggests that LLMs may have already been tacitly used in peer
reviewing, e.g., at the 2024 International Conference of Learning
Representations (ICLR). Furthermore, some efforts have been undertaken in an
attempt to explicitly integrate LLMs in peer reviewing by various editorial
boards (including that of ICLR'25). To fully understand the utility and the
implications of LLMs' deployment for scientific reviewing, a comprehensive
relevant dataset is strongly desirable. Despite some previous research on this
topic, such dataset has been lacking so far. We fill in this gap by presenting
GenReview, the hitherto largest dataset containing LLM-written reviews. Our
dataset includes 81K reviews generated for all submissions to the 2018--2025
editions of the ICLR by providing the LLM with three independent prompts: a
negative, a positive, and a neutral one. GenReview is also linked to the
respective papers and their original reviews, thereby enabling a broad range of
investigations. To illustrate the value of GenReview, we explore a sample of
intriguing research questions, namely: if LLMs exhibit bias in reviewing (they
do); if LLM-written reviews can be automatically detected (so far, they can);
if LLMs can rigorously follow reviewing instructions (not always) and whether
LLM-provided ratings align with decisions on paper acceptance or rejection
(holds true only for accepted papers). GenReview can be accessed at the
following link: https://anonymous.4open.science/r/gen_review.

</details>


### [76] [Mitra: Mixed Synthetic Priors for Enhancing Tabular Foundation Models](https://arxiv.org/abs/2510.21204)
*Xiyuan Zhang,Danielle C. Maddix,Junming Yin,Nick Erickson,Abdul Fatir Ansari,Boran Han,Shuai Zhang,Leman Akoglu,Christos Faloutsos,Michael W. Mahoney,Cuixiong Hu,Huzefa Rangwala,George Karypis,Bernie Wang*

Main category: cs.LG

TL;DR: 提出 Mitra，一种基于多样性、辨识度和现实数据表现的合成先验混合的表格 foundation 模型（TFM），在无现实数据、以合成数据训练的前提下，通过 ICL 得到良好泛化和样本效率，超越 TabPFNv2 和 TabICL 等模型。


<details>
  <summary>Details</summary>
Motivation: 弥补对先验设计原则的理解不足，将注意力从模型结构转向合成数据分布的设计，以提升在表格数据上的泛化能力。

Method: 系统分析合成先验的关键性质，筛选出多样性、辨识度等指标，构建混合的先验集合，训练 Mitra 在这些先验上进行预训练，使其在多种现实数据集的任务中具备良好泛化性和样本效率。

Result: Mitra 在分类和回归基准上稳定超越现有 TFMs（如 TabPFNv2、TabICL），并在样本效率方面具备优势。

Conclusion: 优先设计合成数据的先验分布是提升表格领域大模型的关键，Mitra 的成功证明了这一路线的可行性及潜在的实际应用价值。

Abstract: Since the seminal work of TabPFN, research on tabular foundation models
(TFMs) based on in-context learning (ICL) has challenged long-standing
paradigms in machine learning. Without seeing any real-world data, models
pretrained on purely synthetic datasets generalize remarkably well across
diverse datasets, often using only a moderate number of in-context examples.
This shifts the focus in tabular machine learning from model architecture
design to the design of synthetic datasets, or, more precisely, to the prior
distributions that generate them. Yet the guiding principles for prior design
remain poorly understood. This work marks the first attempt to address the gap.
We systematically investigate and identify key properties of synthetic priors
that allow pretrained TFMs to generalize well. Based on these insights, we
introduce Mitra, a TFM trained on a curated mixture of synthetic priors
selected for their diversity, distinctiveness, and performance on real-world
tabular data. Mitra consistently outperforms state-of-the-art TFMs, such as
TabPFNv2 and TabICL, across both classification and regression benchmarks, with
better sample efficiency.

</details>


### [77] [Model Merging with Functional Dual Anchors](https://arxiv.org/abs/2510.21223)
*Kexuan Shi,Yandong Wen,Weiyang Liu*

Main category: cs.LG

TL;DR: 提出在输入表征空间建模的 FDAs 框架，用合成输入及其梯度对齐来实现跨任务知识融合，提供对参数空间合并的补充与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于参数的模型合并在任务冲突和参数不一致性方面受限，需要一个统一的、以功能性输入表示的替代视角，以实现更鲁棒的多任务融合。

Method: 引入 Functional Dual Anchors (FDAs)：一组合成输入，其通过诱导的梯度与各任务向量对齐，捕捉相对于预训练模型的任务相关函数性变化。给出对齐目标、初始化策略以及将 FDAs 与参数空间合并互补的设计。

Result: 在多组实验中，FDAs 展现出对合并任务的鲁棒性和灵活性，能提升跨任务知识的整合效果，并可与参数空间方法协同工作。

Conclusion: FDAs 提供一个新的从输入空间出发的模型融合视角，桥接联合多任务训练与后处理合并，未来可扩展至更多任务和模型。

Abstract: Model merging is an efficient post-training strategy for integrating
knowledge from multiple finetuned checkpoints of a shared foundation model.
Existing methods operate in the parameter space, combining task vectors to
mitigate conflicts, but remain constrained by parameter inconsistencies. We
propose Functional Dual Anchors (FDAs), a framework that instead models the
input-representation space. FDAs are synthetic inputs whose induced gradients
align with task vectors, capturing task-specific functional shifts relative to
the pretrained model. This perspective bridges joint multi-task training and
post-hoc merging, offering both robustness and flexibility. We further
introduce a principled initialization scheme and show that FDAs are
complementary to parameter-space model merging. Comprehensive experiments
demonstrate the effectiveness of FDAs in model merging.

</details>


### [78] [How Hard is it to Confuse a World Model?](https://arxiv.org/abs/2510.21232)
*Waris Radji,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: Formalizes 'most confusing instance' for neural network world models as constrained optimization and proposes an adversarial training method; empirical study shows confusion correlates with model uncertainty and informs exploration in deep model-based RL.


<details>
  <summary>Details</summary>
Motivation: Extend the regret-lower-bound construction from simple settings to neural network world models by identifying the nearest competing model that makes a suboptimal policy appear optimal, enabling principled exploration strategies.

Method: Define a constrained optimization to modify the world model within a statistical distance from the reference while changing policy performance; solve via adversarial training; evaluate across world models of varying quality.

Result: Demonstrates that achievable confusion degree correlates with model uncertainty; empirical results across tasks show the method can reveal exploitable weaknesses and suggest uncertainty-guided exploration.

Conclusion: The degree of attainable confusion is tied to the approximate model's uncertainty, offering a data-driven knob to design exploration strategies in deep model-based RL; opens path to regret-informed robust planning.

Abstract: In reinforcement learning (RL) theory, the concept of most confusing
instances is central to establishing regret lower bounds, that is, the minimal
exploration needed to solve a problem. Given a reference model and its optimal
policy, a most confusing instance is the statistically closest alternative
model that makes a suboptimal policy optimal. While this concept is
well-studied in multi-armed bandits and ergodic tabular Markov decision
processes, constructing such instances remains an open question in the general
case. In this paper, we formalize this problem for neural network world models
as a constrained optimization: finding a modified model that is statistically
close to the reference one, while producing divergent performance between
optimal and suboptimal policies. We propose an adversarial training procedure
to solve this problem and conduct an empirical study across world models of
varying quality. Our results suggest that the degree of achievable confusion
correlates with uncertainty in the approximate model, which may inform
theoretically-grounded exploration strategies for deep model-based RL.

</details>


### [79] [Convergence of Stochastic Gradient Langevin Dynamics in the Lazy Training Regime](https://arxiv.org/abs/2510.21245)
*Noah Oberweis,Semih Cayci*

Main category: cs.LG

TL;DR: Non-asymptotic convergence analysis of stochastic gradient Langevin dynamics (SGLD) in the lazy training regime, showing with high probability a non-degenerate kernel throughout training, and proving exponential convergence to the empirical risk minimizer in expectation, with finite-time and finite-width bounds on the optimality gap; numerical validation in regression.


<details>
  <summary>Details</summary>
Motivation: To understand training dynamics of optimization in deep learning via continuous-time models and to provide non-asymptotic convergence guarantees for SGLD as an Itô SDE approximation of SGD under lazy training.

Method: Analyze SGLD with multiplicative and state-dependent noise under regularity conditions on the Hessian of the loss. Establish high-probability non-degeneracy of the kernel during training, and prove exponential convergence in expectation to the empirical risk minimizer. Derive finite-time and finite-width bounds on the optimality gap. Validate findings with numerical experiments in regression.

Result: SGLD maintains a non-degenerate kernel with high probability throughout training and converges exponentially to the empirical minimizer in expectation. Finite-time and finite-width bounds on the optimality gap are established. Numerical experiments in regression corroborate the theory.

Conclusion: Provides non-asymptotic convergence guarantees for SGLD in the lazy training regime under Hessian regularity, highlighting robustness and reliable convergence behavior with state-dependent noise; supports the use of SGLD as a principled continuous-time model for SGD dynamics and offers practical finite-time insights.

Abstract: Continuous-time models provide important insights into the training dynamics
of optimization algorithms in deep learning. In this work, we establish a
non-asymptotic convergence analysis of stochastic gradient Langevin dynamics
(SGLD), which is an It\^o stochastic differential equation (SDE) approximation
of stochastic gradient descent in continuous time, in the lazy training regime.
We show that, under regularity conditions on the Hessian of the loss function,
SGLD with multiplicative and state-dependent noise (i) yields a non-degenerate
kernel throughout the training process with high probability, and (ii) achieves
exponential convergence to the empirical risk minimizer in expectation, and we
establish finite-time and finite-width bounds on the optimality gap. We
corroborate our theoretical findings with numerical examples in the regression
setting.

</details>


### [80] [Unified Implementations of Recurrent Neural Networks in Multiple Deep Learning Frameworks](https://arxiv.org/abs/2510.21252)
*Francesco Martinuzzi*

Main category: cs.LG

TL;DR: 提出并开源三种RNN相关库（torchrecurrent、RecurrentLayers.jl、LuxRecurrentLayers.jl），在 Julia 与 Python 中集中实现多种循环单元和高级RNN架构，提升可复现性与扩展性。


<details>
  <summary>Details</summary>
Motivation: 缺乏统一的、易于复现实验的RNN实现库，容易导致重复工作和对比困难。

Method: 在 Julia 和 Python 生态下提供三个开源库，统一接口、可定制、便于构建和扩展RNN模型；许可证为 MIT，代码在 GitHub 上维护。

Result: 库已提供并处于活跃维护状态，便于科研与工程中的模型比较、复现、快速原型开发。

Conclusion: 建立了一个集中化的平台，降低了实现差异带来的偏差，促进RNN变体的探索与应用。

Abstract: Recurrent neural networks (RNNs) are a cornerstone of sequence modeling
across various scientific and industrial applications. Owing to their
versatility, numerous RNN variants have been proposed over the past decade,
aiming to improve the modeling of long-term dependencies and to address
challenges such as vanishing and exploding gradients. However, no central
library is available to test these variations, and reimplementing diverse
architectures can be time-consuming and error-prone, limiting reproducibility
and exploration. Here, we introduce three open-source libraries in Julia and
Python that centralize numerous recurrent cell implementations and higher-level
recurrent architectures. torchrecurrent, RecurrentLayers.jl, and
LuxRecurrentLayers.jl offer a consistent framework for constructing and
extending RNN models, providing built-in mechanisms for customization and
experimentation. All packages are available under the MIT license and actively
maintained on GitHub.

</details>


### [81] [PINN Balls: Scaling Second-Order Methods for PINNs with Domain Decomposition and Adaptive Sampling](https://arxiv.org/abs/2510.21262)
*Andrea Bonfanti,Ismael Medina,Roman List,Björn Staeves,Roberto Santana,Marco Ellero*

Main category: cs.LG

TL;DR: 提出 PINN Balls：一个本地专家混合模型框架，用以在保持可扩展性的同时实现对二阶训练的支持，并通过自适应对抗采样实现领域分解。


<details>
  <summary>Details</summary>
Motivation: 二阶优化方法能提升 PINN 的训练效果，但其巨大的内存需求使其难以在大规模模型和复杂 PDE 上应用。需要一种参数高效、可扩展且能利用二阶信息的解决方案。

Method: 引入本地 Mixture of Experts (MoE)，实现参数高效的集成与稀疏编码来支撑二阶训练；提出 PINN Balls，具备可学习的领域划分结构，通过 Adversarial Adaptive Sampling (AAS) 自适应 PDE 及其域来优化领域分割；结合以上策略进行 PINN 训练以利用二阶信息。

Result: 在科学机器学习领域的基准上，精度优于现有方法，且保持良好的可扩展性与理论依据。

Conclusion: 该框架使二阶训练在 PINN 上变得可行且高效，兼具高精度与可扩展性，可能成为 PDE 求解的有力工具。

Abstract: Recent advances in Scientific Machine Learning have shown that second-order
methods can enhance the training of Physics-Informed Neural Networks (PINNs),
making them a suitable alternative to traditional numerical methods for Partial
Differential Equations (PDEs). However, second-order methods induce large
memory requirements, making them scale poorly with the model size. In this
paper, we define a local Mixture of Experts (MoE) combining the
parameter-efficiency of ensemble models and sparse coding to enable the use of
second-order training. Our model -- \textsc{PINN Balls} -- also features a
fully learnable domain decomposition structure, achieved through the use of
Adversarial Adaptive Sampling (AAS), which adapts the DD to the PDE and its
domain. \textsc{PINN Balls} achieves better accuracy than the state-of-the-art
in scientific machine learning, while maintaining invaluable scalability
properties and drawing from a sound theoretical background.

</details>


### [82] [Relieving the Over-Aggregating Effect in Graph Transformers](https://arxiv.org/abs/2510.21267)
*Junshu Sun,Wanxing Chang,Chenxue Yang,Qingming Huang,Shuhui Wang*

Main category: cs.LG

TL;DR: 提出 Wideformer，一种将图注意力聚合分解为并行子聚合并进行输出排序加权的插拔模块，用以缓解大规模图中信息“过度聚合”导致的信号稀释，提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决图注意力在大规模图上进行全局聚合时，消息数量过多导致信息无法被有效区分和保留的问题，揭示并抑制“过度聚合”现象。

Method: 将全量节点的聚合拆分为并行的子聚合过程，限制每次聚合的输入量以避免消息稀释；引入一次引导（guiding）步骤，对各子聚合的输出进行排序和加权，优先保留信息量更高的消息。

Result: 实验表明 Wideformer 能有效缓解过度聚合，使骨干注意力模型更专注于信息量高的消息，整体性能优于基线方法。

Conclusion: Wideformer 作为一种可插拔的聚合模块，提升了图注意力在大规模场景中的鲁棒性与表现，便于与现有骨干模型无缝结合。

Abstract: Graph attention has demonstrated superior performance in graph learning
tasks. However, learning from global interactions can be challenging due to the
large number of nodes. In this paper, we discover a new phenomenon termed
over-aggregating. Over-aggregating arises when a large volume of messages is
aggregated into a single node with less discrimination, leading to the dilution
of the key messages and potential information loss. To address this, we propose
Wideformer, a plug-and-play method for graph attention. Wideformer divides the
aggregation of all nodes into parallel processes and guides the model to focus
on specific subsets of these processes. The division can limit the input volume
per aggregation, avoiding message dilution and reducing information loss. The
guiding step sorts and weights the aggregation outputs, prioritizing the
informative messages. Evaluations show that Wideformer can effectively mitigate
over-aggregating. As a result, the backbone methods can focus on the
informative messages, achieving superior performance compared to baseline
methods.

</details>


### [83] [Buffer layers for Test-Time Adaptation](https://arxiv.org/abs/2510.21271)
*Hyeongyu Kim,Geonhui Han,Dosik Hwang*

Main category: cs.LG

TL;DR: 提出Buffer层的TTA新范式，替代对归一化层的在线更新，通过插入缓冲层在不改动主干参数的情况下适应测试域，提升鲁棒性并降低遗忘风险。


<details>
  <summary>Details</summary>
Motivation: 归一化层（如BN）在小批量与新域上的统计不稳定与泛化能力不足，限制了TTA效果；需要一种不直接修改主干参数、避免灾难性遗忘的在线适应方法。

Method: 在模型中引入Buffer层，保留预训练 backbone 的完整性，仅通过Buffer层进行域自适应更新；Buffer层可插拔，兼容大多数TTA框架，且对原有模型结构的侵入较小。

Result: 在多种架构与数据集上，Buffer_TTA在减轻域偏移、提升鲁棒性方面表现优于基于BN等归一化的TTA方法，并具备较强的遗忘抑制能力；代码已开源。

Conclusion: Buffer层作为一种灵活、有效的TTA新范式，具有良好的通用性和实用性，适用于现实世界的域自适应场景。

Abstract: In recent advancements in Test Time Adaptation (TTA), most existing
methodologies focus on updating normalization layers to adapt to the test
domain. However, the reliance on normalization-based adaptation presents key
challenges. First, normalization layers such as Batch Normalization (BN) are
highly sensitive to small batch sizes, leading to unstable and inaccurate
statistics. Moreover, normalization-based adaptation is inherently constrained
by the structure of the pre-trained model, as it relies on training-time
statistics that may not generalize well to unseen domains. These issues limit
the effectiveness of normalization-based TTA approaches, especially under
significant domain shift. In this paper, we introduce a novel paradigm based on
the concept of a Buffer layer, which addresses the fundamental limitations of
normalization layer updates. Unlike existing methods that modify the core
parameters of the model, our approach preserves the integrity of the
pre-trained backbone, inherently mitigating the risk of catastrophic forgetting
during online adaptation. Through comprehensive experimentation, we demonstrate
that our approach not only outperforms traditional methods in mitigating domain
shift and enhancing model robustness, but also exhibits strong resilience to
forgetting. Furthermore, our Buffer layer is modular and can be seamlessly
integrated into nearly all existing TTA frameworks, resulting in consistent
performance improvements across various architectures. These findings validate
the effectiveness and versatility of the proposed solution in real-world domain
adaptation scenarios. The code is available at
https://github.com/hyeongyu-kim/Buffer_TTA.

</details>


### [84] [An Evidence-Based Post-Hoc Adjustment Framework for Anomaly Detection Under Data Contamination](https://arxiv.org/abs/2510.21296)
*Sukanya Patra,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: EPHAD 提出了一种简单但有效的测试时适应框架，用于在训练数据被污染的无监督异常检测（AD）情景中，通过在测试阶段整合来自多模态证据（如 CLIP、Latent Outlier Factor 以及领域知识）来更新AD模型的输出，无需重新训练，且在多种数据集上表现稳健。


<details>
  <summary>Details</summary>
Motivation: 真实世界的无监督AD训练数据常包含尚未发现的异常或错误标注，导致性能下降。现有解决方案往往需要访问训练管线、数据或已知的异常比例等先验信息，限制实用性。因此，需要一种无需改动训练阶段、仅在测试阶段进行的自适应方法来缓解污染问题。

Method: EPHAD 将来自污染数据训练的AD模型的先验知识与来自 CLIP 等多模态模型、经典 AD 方法（如 Latent Outlier Factor）及领域知识所得到的证据进行融合，在测试时对AD模型输出进行更新。通过一个合成 toy 示例来直观说明，并在八个视觉AD数据集、二十六个表格AD数据集以及一个真实工业AD数据集上进行广泛实验，还进行了消融研究以分析超参数影响及对污染水平的鲁棒性。代码公开。

Result: 在多领域数据集上验证了方法的有效性，显示EPHAD 对不同AD模型和证据对的适用性与鲁棒性，且对污染水平具有一定的稳健性。研究还给出超参数的影响分析，证明框架在不同场景下的泛化能力。

Conclusion: EPHAD 提供了一种简单而有效的测试时自适应框架，用于在污染训练数据的无监督AD场景中更新模型输出，具备良好的可泛化性与鲁棒性，且代码可复现。

Abstract: Unsupervised anomaly detection (AD) methods typically assume clean training
data, yet real-world datasets often contain undetected or mislabeled anomalies,
leading to significant performance degradation. Existing solutions require
access to the training pipelines, data or prior knowledge of the proportions of
anomalies in the data, limiting their real-world applicability. To address this
challenge, we propose EPHAD, a simple yet effective test-time adaptation
framework that updates the outputs of AD models trained on contaminated
datasets using evidence gathered at test time. Our approach integrates the
prior knowledge captured by the AD model trained on contaminated datasets with
evidence derived from multimodal foundation models like Contrastive
Language-Image Pre-training (CLIP), classical AD methods like the Latent
Outlier Factor or domain-specific knowledge. We illustrate the intuition behind
EPHAD using a synthetic toy example and validate its effectiveness through
comprehensive experiments across eight visual AD datasets, twenty-six tabular
AD datasets, and a real-world industrial AD dataset. Additionally, we conduct
an ablation study to analyse hyperparameter influence and robustness to varying
contamination levels, demonstrating the versatility and robustness of EPHAD
across diverse AD models and evidence pairs. To ensure reproducibility, our
code is publicly available at https://github.com/sukanyapatra1997/EPHAD.

</details>


### [85] [Amortized Variational Inference for Partial-Label Learning: A Probabilistic Approach to Label Disambiguation](https://arxiv.org/abs/2510.21300)
*Tobias Fuchs,Nadja Klein*

Main category: cs.LG

TL;DR: A probabilistic PLL method using amortized variational inference to approximate the posterior over true labels with neural networks, achieving state-of-the-art accuracy and efficiency while being architecture-agnostic.


<details>
  <summary>Details</summary>
Motivation: Partial-label learning on real-world noisy data; need scalable, principled inference of the true label among candidate sets without relying on surrogate losses; current methods struggle with either accuracy or efficiency.

Method: Direct amortized variational inference: neural networks take input x and output variational parameters to model the posterior distribution over the true label given candidate label sets; architecture-agnostic; theoretical guarantees and analysis.

Result: Empirical results on synthetic and real-world datasets show state-of-the-art performance in both accuracy and efficiency compared to traditional PLL methods and deep-learning baselines.

Conclusion: Brings together deep learning expressiveness and probabilistic rigor; provides a scalable, architecture-agnostic framework for PLL with strong empirical performance.

Abstract: Real-world data is frequently noisy and ambiguous. In crowdsourcing, for
example, human annotators may assign conflicting class labels to the same
instances. Partial-label learning (PLL) addresses this challenge by training
classifiers when each instance is associated with a set of candidate labels,
only one of which is correct. While early PLL methods approximate the true
label posterior, they are often computationally intensive. Recent deep learning
approaches improve scalability but rely on surrogate losses and heuristic label
refinement. We introduce a novel probabilistic framework that directly
approximates the posterior distribution over true labels using amortized
variational inference. Our method employs neural networks to predict
variational parameters from input data, enabling efficient inference. This
approach combines the expressiveness of deep learning with the rigor of
probabilistic modeling, while remaining architecture-agnostic. Theoretical
analysis and extensive experiments on synthetic and real-world datasets
demonstrate that our method achieves state-of-the-art performance in both
accuracy and efficiency.

</details>


### [86] [Data as a Lever: A Neighbouring Datasets Perspective on Predictive Multiplicity](https://arxiv.org/abs/2510.21303)
*Prakhar Ganesh,Hsiang Hsu,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 提出邻近数据集框架以研究单点数据变动对模型多样性的影响，发现更高的类分布重叠反而导致较低的多样性，这一现象由共享的Rashomon参数解释并给出严格证明；进而推广至主动学习和数据插补，提出多样性感知的数据获取与插补策略。


<details>
  <summary>Details</summary>
Motivation: 揭示数据在塑造模型多样性中的核心作用，弥补以往多集中于模型结构的研究的不足；通过研究“邻近数据集”（仅差一个数据点）来理解数据分布对多样性的影响，并解释为何类分布重叠度高时多样性会降低。

Method: 提出邻近数据集框架，通过比较仅在单个数据点上有差异的数据集来衡量模型集合的多样性；给出针对高重叠情形的Rashomon参数共享的理论证明；将框架推广到主动学习与数据插补领域，系统研究现有算法的多样性，并提出相应的多样性感知策略与方法。

Result: 理论上揭示了更高的类别重叠导致较低多样性的关系，并给出严格证明；在主动学习和数据插补两个实际领域完成框架扩展、对现有算法进行系统性研究，并提出新方向的多样性感知方法。

Conclusion: 数据层面的差异对模型多样性具有决定性作用，邻近数据集框架为理解与利用多样性提供了新的视角，并可用于设计更具多样性意识的学习与数据处理策略。

Abstract: Multiplicity -- the existence of distinct models with comparable performance
-- has received growing attention in recent years. While prior work has largely
emphasized modelling choices, the critical role of data in shaping multiplicity
has been comparatively overlooked. In this work, we introduce a neighbouring
datasets framework to examine the most granular case: the impact of a
single-data-point difference on multiplicity. Our analysis yields a seemingly
counterintuitive finding: neighbouring datasets with greater inter-class
distribution overlap exhibit lower multiplicity. This reversal of conventional
expectations arises from a shared Rashomon parameter, and we substantiate it
with rigorous proofs.
  Building on this foundation, we extend our framework to two practical
domains: active learning and data imputation. For each, we establish natural
extensions of the neighbouring datasets perspective, conduct the first
systematic study of multiplicity in existing algorithms, and finally, propose
novel multiplicity-aware methods, namely, multiplicity-aware data acquisition
strategies for active learning and multiplicity-aware data imputation
techniques.

</details>


### [87] [Revisiting Social Welfare in Bandits: UCB is (Nearly) All You Need](https://arxiv.org/abs/2510.21312)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 两阶段策略（先均匀探索再使用标准UCB）在加入Hoeffding加性界限的前提下，接近最优的Nash后悔界，且可扩展到亚高斯分布与p-mean后悔，显式放宽了以往对乘性界限与非负奖励的依赖，并对广义p-mean后悔给出近似最优界的保证。


<details>
  <summary>Details</summary>
Motivation: 在奖励分配给群体（如临床试验中的患者）时，传统后悔衡量忽视了公平性。Nash后悔通过对奖励的几何均值来度量，与Nash社会福利函数一致地满足公平性公理。现有方法通常需要强假设（乘性浓缩不等式、奖励有界且非负），难以处理高斯等分布，因此需要更通用、且简单的策略来实现公平导向的后悔最小化。

Method: 提出一个两阶段算法：先进行一个均匀探索阶段，然后再采用标准的Upper Confidence Bound (UCB) 算法。通过添加Hoeffding界限的分析，证明在该设定下可以实现接近最优的Nash后悔，并自然推广到子高斯奖励分布。进一步将算法推广到一个更广的公平指标族：p-mean后悔，给出对所有p值的几乎最优后悔界。

Result: 在保持 additive Hoeffding bounds 的条件下，算法实现了近似最优的 Nash 后悔，并且对子高斯分布同样适用。对 p-mean 后悔的推广在所有 p 下均获得几乎最优的后悔界，显著优于需要乘性界限和强假设的前人工作。

Conclusion: 该工作提供了一个简单且鲁棒的公平导向后悔最小化框架：通过两阶段策略和广义的 p-mean 公平性，能够在广泛的分布假设下实现近似最优的后悔界，同时显著放宽了对分布和界限条件的要求。

Abstract: Regret in stochastic multi-armed bandits traditionally measures the
difference between the highest reward and either the arithmetic mean of
accumulated rewards or the final reward. These conventional metrics often fail
to address fairness among agents receiving rewards, particularly in settings
where rewards are distributed across a population, such as patients in clinical
trials. To address this, a recent body of work has introduced Nash regret,
which evaluates performance via the geometric mean of accumulated rewards,
aligning with the Nash social welfare function known for satisfying fairness
axioms.
  To minimize Nash regret, existing approaches require specialized algorithm
designs and strong assumptions, such as multiplicative concentration
inequalities and bounded, non-negative rewards, making them unsuitable for even
Gaussian reward distributions. We demonstrate that an initial uniform
exploration phase followed by a standard Upper Confidence Bound (UCB) algorithm
achieves near-optimal Nash regret, while relying only on additive Hoeffding
bounds, and naturally extending to sub-Gaussian rewards. Furthermore, we
generalize the algorithm to a broad class of fairness metrics called the
$p$-mean regret, proving (nearly) optimal regret bounds uniformly across all
$p$ values. This is in contrast to prior work, which made extremely restrictive
assumptions on the bandit instances and even then achieved suboptimal regret
bounds.

</details>


### [88] [A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization](https://arxiv.org/abs/2510.21314)
*Xuan Tang,Jichu Li,Difan Zou*

Main category: cs.LG

TL;DR: 提出了在浮点量化的梯度、权重和优化器状态下，对自适应优化器（Adam 与 Muon）收敛性的理论框架；在光滑非凸目标下推导收敛率，量化误差对收敛的影响明确；mantissa 长度只需对迭代次数的对数增长即可维持接近全精度的收敛速率；发现 Adam 对权重和二阶矩量化较为敏感，而 Muon 更鲁棒；实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 解释为何低精度训练仍然有效，以及弥补现有自适应优化器在硬件量化条件下收敛性理论的空缺，帮助理解低精度训练的鲁棒性。

Method: 建立一个量化分析框架，考虑梯度、权重和优化器状态的浮点量化；在光滑非凸目标下，基于标准随机梯度假设推导收敛率，明确各组成部分量化误差对收敛的影响；比较 Adam 与 Muon 的误差敏感性与鲁棒性。

Result: 给出在量化条件下的收敛率，与全精度相比， mantissa 长度只需对迭代次数的对数数量级增长即可维持接近的收敛速率；Adam 对权重和二阶矩量化较为敏感，Muon 需量化误差控制较弱而更鲁棒；实验（合成与真实数据）验证理论。

Conclusion: 工作填补了低精度训练理论空白，提供了硬件感知下自适应优化器收敛性的理论框架，且揭示了两种算法的鲁棒性差异，并得到实验支持。

Abstract: The rapid scaling of large language models (LLMs) has made low-precision
training essential for reducing memory, improving efficiency, and enabling
larger models and datasets. Existing convergence theories for adaptive
optimizers, however, assume all components are exact and neglect hardware-aware
quantization, leaving open the question of why low-precision training remains
effective. We introduce the first theoretical framework for analyzing the
convergence of adaptive optimizers, including Adam and Muon, under
floating-point quantization of gradients, weights, and optimizer states (e.g.,
moment estimates). Within this framework, we derive convergence rates on smooth
non-convex objectives under standard stochastic gradient assumptions,
explicitly characterizing how quantization errors from different components
affect convergence. We show that both algorithms retain rates close to their
full-precision counterparts provided mantissa length scales only
logarithmically with the number of iterations. Our analysis further reveals
that Adam is highly sensitive to weights and second-moment quantization due to
its reliance on $\beta_2 \to 1$, while Muon requires weaker error control and
is thus potentially more robust. These results narrow the gap between empirical
success and theoretical understanding of low-precision training methods.
Numerical experiments on synthetic and real-world data corroborate our theory.

</details>


### [89] [Weak-to-Strong Generalization under Distribution Shifts](https://arxiv.org/abs/2510.21332)
*Myeongho Jeon,Jan Sobotka,Suhwan Choi,Maria Brbić*

Main category: cs.LG

TL;DR: 提出RAVEN框架，通过动态组合弱模型并优化强模型参数，以实现对弱到强泛化在分布漂移下的鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 现有的弱到强泛化在分布变化时往往导致强模型性能下降，亟需在无额外标签的情况下提升强模型的鲁棒 supervision。

Method: RAVEN同时学习弱模型的权重并优化强模型参数，动态地选择可信的弱监督信号以指导强模型；在图像分类、文本分类和偏好对齐等任务上进行评估，并与多种基线方法比较。

Result: 在OOD（分布外）任务上相较基线提升超过30%，在IN-分布任务上达到或超过现有方法；RAVEN倾向于给更准确的弱模型更高权重，体现对可信监督的自动识别能力。

Conclusion: RAVEN提供一个鲁棒的弱到强泛化框架，能在分布漂移情形下维持强模型表现，并自动识别并放大可信的弱监督信号。

Abstract: As future superhuman models become increasingly complex, accurately
supervising their behavior may exceed human capabilities. Recent works have
demonstrated that in such scenarios, weak models can effectively supervise
strong models, a phenomenon known as weak-to-strong generalization. However, we
find that naive weak-to-strong generalization fails under distribution shifts,
often leading to worse performance of the strong model than its weak
supervisors. To address this, we propose RAVEN, a robust weak-to-strong
generalization framework that dynamically learns the optimal combinations of
weak models in addition to parameters of the strong model. We demonstrate the
effectiveness of RAVEN on image classification, text classification, and
preference alignment tasks. RAVEN outperforms alternative baselines by over 30%
on out-of-distribution tasks while matching or surpassing existing methods on
in-distribution tasks. Moreover, our results show that RAVEN assigns higher
weights to more accurate weak models, demonstrating its ability to
automatically identify trustworthy supervision.

</details>


### [90] [SCORENF: Score-based Normalizing Flows for Sampling Unnormalized distributions](https://arxiv.org/abs/2510.21330)
*Vikas Kanaujia,Vipul Arora*

Main category: cs.LG

TL;DR: 提出 ScoreNF，一种在 Normalizing Flow 基础上结合独立 Metropolis-Hastings 的无归一化分布采样框架，能在较小训练集下实现高效且无偏采样，并能评估模式覆盖与模式崩溃行为。


<details>
  <summary>Details</summary>
Motivation: 无归一化分布在物理系统建模中广泛存在；传统 MCMC 存在收敛慢、临界减速、模态混合差和自相关高等问题；数据驱动的生成模型需要大量数据，易导致模式覆盖不足或模式崩溃；因此需要一个在小样本条件下也能提供无偏、高效采样的框架，并具备对模式行为诊断的能力。

Method: 将分数估计（score-based learning）与正则化流（Normalizing Flow）结合，形成 ScoreNF 框架，并引入一个独立 Metropolis-Hastings（IMH）模块以实现对未归一化目标分布的无偏采样。即使训练集较小，亦可保持高性能；提出用于评估模式覆盖与模式崩溃的分析方法。实验包括二维合成分布 MOG-4/MOG-8 以及高维 phi^4 晶格场理论分布，验证其采样效果。

Result: 在多种分布上展示出在小训练集条件下仍具备高效且无偏的采样能力，并能在一定程度上揭示和量化模式覆盖与模式崩溃行为；在高维物理分布（phi^4）上也显示出有效的采样能力，证明其在从无归一化目标分布进行采样方面的可行性与鲁棒性。

Conclusion: ScoreNF 提供了一个在无归一化目标分布上高效、无偏的采样方案，且对小数据条件具有鲁棒性，降低了对基于 MCMC 的训练数据依赖，并具备对模式覆盖与模式崩溃行为的诊断能力，适用于物理系统的高维采样和相关应用。

Abstract: Unnormalized probability distributions are central to modeling complex
physical systems across various scientific domains. Traditional sampling
methods, such as Markov Chain Monte Carlo (MCMC), often suffer from slow
convergence, critical slowing down, poor mode mixing, and high autocorrelation.
In contrast, likelihood-based and adversarial machine learning models, though
effective, are heavily data-driven, requiring large datasets and often
encountering mode covering and mode collapse. In this work, we propose ScoreNF,
a score-based learning framework built on the Normalizing Flow (NF)
architecture, integrated with an Independent Metropolis-Hastings (IMH) module,
enabling efficient and unbiased sampling from unnormalized target
distributions. We show that ScoreNF maintains high performance even with small
training ensembles, thereby reducing reliance on computationally expensive
MCMC-generated training data. We also present a method for assessing
mode-covering and mode-collapse behaviours. We validate our method on synthetic
2D distributions (MOG-4 and MOG-8) and the high-dimensional $\phi^4$ lattice
field theory distribution, demonstrating its effectiveness for sampling tasks.

</details>


### [91] [$α$-LoRA: Effective Fine-Tuning via Base Model Rescaling](https://arxiv.org/abs/2510.21345)
*Aymane El Firdoussi,El Mahdi Chayti,Mohamed El Amine Seddik,Martin Jaggi*

Main category: cs.LG

TL;DR: 提出一种新类的再参数化微调方法，通过在冻结权重上叠加可训练矩阵来提升泛化；在高维二分类用随机矩阵理论给出理论支撑，并在LLM微调等实际任务上验证。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法如 LoRA 在数据不足时有效，但对泛化能力的提升仍有待，需用理论工具解释高维场景下的性能，并在更贴近实际的任务中验证。

Method: 提出新的一类再参数化方法，更新目标模块通过在其冻结权重矩阵上增加一个可训练权矩阵；以随机矩阵理论分析其在高维二分类中的效果；并在微调大语言模型等现实场景中进行实验。

Result: 理论上在高维二分类中展示有效性；通过实验（如微调LLMs）验证理论结果，表明该方法在现实任务中具备可行性并呈现改进的泛化表现。

Conclusion: 这类再参数化方法对提升微调泛化能力具有潜在优势，结合随机矩阵理论为理解提供了新视角，且在现实任务中具备可行性。

Abstract: Fine-tuning has proven to be highly effective in adapting pre-trained models
to perform better on new desired tasks with minimal data samples. Among the
most widely used approaches are reparameterization methods, which update a
target module by augmenting its frozen weight matrix with an additional
trainable weight matrix. The most prominent example is Low Rank Adaption
(LoRA), which gained significant attention in recent years. In this paper, we
introduce a new class of reparameterization methods for transfer learning,
designed to enhance the generalization ability of fine-tuned models. We
establish the effectiveness of our approach in a high-dimensional binary
classification setting using tools from Random Matrix Theory, and further
validate our theoretical findings through more realistic experiments, such as
fine-tuning LLMs.

</details>


### [92] [Assessing the Real-World Utility of Explainable AI for Arousal Diagnostics: An Application-Grounded User Study](https://arxiv.org/abs/2510.21389)
*Stefan Kraft,Andreas Theissler,Vera Wienhausen-Wilke,Gjergji Kasneci,Hendrik Lensch*

Main category: cs.LG

TL;DR: 透明白盒AI在临床睡眠评分中的有针对性QC步骤显著提升事件检测的准确性，同时保持高接受度和可用性，优于黑箱AI与无辅助。


<details>
  <summary>Details</summary>
Motivation: 在AI系统日益接近甚至超过人类专家的同时，探索如何将AI融入临床工作流，明确何时、为何以及如何信任AI建议。

Method:  Eight位专业睡眠医生在多导睡眠图数据中对夜间觉醒事件进行评分，比较三种辅助条件（手动、黑箱BB AI、透明WB AI）以及两种时序（起始辅助与事后QC复核），评估事件级与基于计数的性能、耗时与用户体验。

Result: AI及人机协同在以临床标准为基准的评估中优于单独专家；透明AI用于有针对性的QC步骤比黑箱提升约30% 的事件级绩效，QC时序还提高计数相关结果；WB与QC虽增加评分时间，但起始辅助更快且被大多数参与者偏好；七名参与者表示愿意在小改动下采用该系统。

Conclusion: 策略性地在早期或QC阶段使用透明AI可以在准确性与临床效率之间取得平衡，推动可信赖AI在临床工作流中的集成与接受度。

Abstract: Artificial intelligence (AI) systems increasingly match or surpass human
experts in biomedical signal interpretation. However, their effective
integration into clinical practice requires more than high predictive accuracy.
Clinicians must discern \textit{when} and \textit{why} to trust algorithmic
recommendations. This work presents an application-grounded user study with
eight professional sleep medicine practitioners, who score nocturnal arousal
events in polysomnographic data under three conditions: (i) manual scoring,
(ii) black-box (BB) AI assistance, and (iii) transparent white-box (WB) AI
assistance. Assistance is provided either from the \textit{start} of scoring or
as a post-hoc quality-control (\textit{QC}) review. We systematically evaluate
how the type and timing of assistance influence event-level and clinically most
relevant count-based performance, time requirements, and user experience. When
evaluated against the clinical standard used to train the AI, both AI and
human-AI teams significantly outperform unaided experts, with collaboration
also reducing inter-rater variability. Notably, transparent AI assistance
applied as a targeted QC step yields median event-level performance
improvements of approximately 30\% over black-box assistance, and QC timing
further enhances count-based outcomes. While WB and QC approaches increase the
time required for scoring, start-time assistance is faster and preferred by
most participants. Participants overwhelmingly favor transparency, with seven
out of eight expressing willingness to adopt the system with minor or no
modifications. In summary, strategically timed transparent AI assistance
effectively balances accuracy and clinical efficiency, providing a promising
pathway toward trustworthy AI integration and user acceptance in clinical
workflows.

</details>


### [93] [Large Language Models as Model Organisms for Human Associative Learning](https://arxiv.org/abs/2510.21408)
*Camila Kolling,Vy Ai Vo,Mariya Toneva*

Main category: cs.LG

TL;DR: LLMs 在联想学习中展现非单调可塑性；适中相似项学习后出现表征分化；词汇干扰放大此差异。


<details>
  <summary>Details</summary>
Motivation: 探究学习中的表征重组机制，利用大规模语言模型作为可扩展的认知神经科学平台，以生成对记忆和表征动态的假设。

Method: 将认知神经科学的联想学习范式应用于六个LLM，利用情境学习对表征变化进行跟踪；通过调控与目标项目相关的词汇覆盖度来引入干扰（vocabulary interference），考察相似性与全局竞争对表征的影响。

Result: 观察到非单调的表征变化：中等相似项在学习后出现分化；较高的词汇干扰放大了这一分化。

Conclusion: LLMs 不仅是研究人类类学习系统表征动态的强大工具，也可作为通用计算模型用于提出关于大脑记忆重组原理的新假设，且干扰与相似性共同塑造学习后的表征。

Abstract: Associative learning--forming links between co-occurring items--is
fundamental to human cognition, reshaping internal representations in complex
ways. Testing hypotheses on how representational changes occur in biological
systems is challenging, but large language models (LLMs) offer a scalable
alternative. Building on LLMs' in-context learning, we adapt a cognitive
neuroscience associative learning paradigm and investigate how representations
evolve across six models. Our initial findings reveal a non-monotonic pattern
consistent with the Non-Monotonic Plasticity Hypothesis, with moderately
similar items differentiating after learning. Leveraging the controllability of
LLMs, we further show that this differentiation is modulated by the overlap of
associated items with the broader vocabulary--a factor we term vocabulary
interference, capturing how new associations compete with prior knowledge. We
find that higher vocabulary interference amplifies differentiation, suggesting
that representational change is influenced by both item similarity and global
competition. Our findings position LLMs not only as powerful tools for studying
representational dynamics in human-like learning systems, but also as
accessible and general computational models for generating new hypotheses about
the principles underlying memory reorganization in the brain.

</details>


### [94] [Compositional Monte Carlo Tree Diffusion for Extendable Planning](https://arxiv.org/abs/2510.21361)
*Jaesik Yoon,Hyeonseo Cho,Sungjin Ahn*

Main category: cs.LG

TL;DR: 提出 Compositional Monte Carlo Tree Diffusion (C-MCTD)，通过对完整计划组成进行全局推理来扩展长序列规划能力，集成 Online Composer、Distributed Composer 与 Preplan Composer 三个互补组件以提升推理速度与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的 Monte Carlo Tree Diffusion (MCTD) 受限于训练轨迹长度，局部性搜索无法利用全局上下文，导致长时规划困难。提出通过对完整计划组成进行全局性推理来克服局部 confinement，提升跨轨迹的组合与复用。

Method: 提出 C-MCTD 框架，包含三大组件：Online Composer（在全局层面对整个计划组成进行搜索，提升全局规划能力）、Distributed Composer（从多个起始点并行探索以降低搜索复杂度）、Preplan Composer（通过缓存的计划图加速推理）。并将 diffusion 模型与分层/组合化的树搜索结合，用于跨轨迹的逐步推理与计划拼接。

Result: 摘要中未给出具体实验结果，整体表达为框架性贡献，声称通过全局计划、并行搜索和计划图缓存来提升长序列规划的效率与能力。

Conclusion: C-MCTD 将计划推理从单一轨迹优化提升到对完整计划组成的推理，为克服 MCTD 的局部性限制提供了全局性路径，三大组件共同支撑长 horizon 的规划与高效推理。

Abstract: Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured
tree search to enable effective trajectory exploration through stepwise
reasoning. However, MCTD remains fundamentally limited by training trajectory
lengths. While periodic replanning allows plan concatenation for longer plan
generation, the planning process remains locally confined, as MCTD searches
within individual trajectories without access to global context. We propose
Compositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates
planning from individual trajectory optimization to reasoning over complete
plan compositions. C-MCTD introduces three complementary components: (1) Online
Composer, which performs globally-aware planning by searching across entire
plan compositions; (2) Distributed Composer, which reduces search complexity
through parallel exploration from multiple starting points; and (3) Preplan
Composer, which accelerates inference by leveraging cached plan graphs.

</details>


### [95] [DreamerV3-XP: Optimizing exploration through uncertainty estimation](https://arxiv.org/abs/2510.21418)
*Lukas Bierling,Davide Pasero,Jan-Henrik Bertrand,Kiki Van Gerwen*

Main category: cs.LG

TL;DR: DreamerV3-XP extends DreamerV3 with a prioritized replay buffer and an intrinsic reward from ensemble disagreement, improving exploration and data efficiency. Evaluated on Atari100k subset and DeepMind Control Visual Benchmark, it preserves DreamerV3 results and achieves faster learning and lower dynamics model loss, especially in sparse-reward settings.


<details>
  <summary>Details</summary>
Motivation: Improve exploration and learning efficiency in DreamerV3, addressing challenges in sparse-reward environments and sample efficiency.

Method: Introduce a prioritized replay buffer that scores trajectories by return, reconstruction loss, and value error; add an intrinsic reward derived from disagreement over predicted environment rewards from an ensemble of world models; evaluate on a subset of Atari100k and DeepMind Control Visual Benchmark tasks.

Result: DreamerV3-XP confirms DreamerV3 results and shows faster learning and lower dynamics model loss with the extensions, particularly in sparse-reward tasks.

Conclusion: Prioritized replay and intrinsic rewards via ensemble disagreement enhance exploration and data efficiency in DreamerV3, with notable gains in sparse-reward settings and promising transfer to related benchmarks.

Abstract: We introduce DreamerV3-XP, an extension of DreamerV3 that improves
exploration and learning efficiency. This includes (i) a prioritized replay
buffer, scoring trajectories by return, reconstruction loss, and value error
and (ii) an intrinsic reward based on disagreement over predicted environment
rewards from an ensemble of world models. DreamerV3-XP is evaluated on a subset
of Atari100k and DeepMind Control Visual Benchmark tasks, confirming the
original DreamerV3 results and showing that our extensions lead to faster
learning and lower dynamics model loss, particularly in sparse-reward settings.

</details>


### [96] [Generative Correlation Manifolds: Generating Synthetic Data with Preserved Higher-Order Correlations](https://arxiv.org/abs/2510.21610)
*Jens E. d'Hondt,Wieger R. Punter,Odysseas Papapetrou*

Main category: cs.LG

TL;DR: 该论文提出 Generative Correlation Manifolds (GCM)，通过目标相关性矩阵的 Cholesky 分解来生成合成数据，声称能从简单的两两关系到高阶交互，完整保留源数据的相关结构，并称有数学证明；但对“高阶相关性”的主张需要谨慎审视。


<details>
  <summary>Details</summary>
Motivation: 解决数据隐私与鲁棒模型训练对保真相关结构的需求；现有方法往往只能保留简单统计量，难以再现复杂多变量依赖。

Method: 通过对目标相关矩阵进行 Cholesky 分解来生成数据集；若数据近似多元正态，则相关矩阵可描述两两关系；然而所谓“高阶交互”通常不能仅靠相关矩阵捕获，需明确数据分布假设与概率结构，并提供严格证明或限定条件。

Result: 摘要未给出具体实验结果，声称具备数学证明以支持对整个相关结构的保留；需给出证明、适用条件以及与基线的对比实验才能验证。

Conclusion: GCM 提供一种潜在的高效合成数据路径，适用于隐私保护数据共享、鲁棒模型训练与仿真；但要明确理论前提、边界条件及在非高斯、非线性依赖场景下的表现，并需充分的实验验证。

Abstract: The increasing need for data privacy and the demand for robust machine
learning models have fueled the development of synthetic data generation
techniques. However, current methods often succeed in replicating simple
summary statistics but fail to preserve both the pairwise and higher-order
correlation structure of the data that define the complex, multi-variable
interactions inherent in real-world systems. This limitation can lead to
synthetic data that is superficially realistic but fails when used for
sophisticated modeling tasks. In this white paper, we introduce Generative
Correlation Manifolds (GCM), a computationally efficient method for generating
synthetic data. The technique uses Cholesky decomposition of a target
correlation matrix to produce datasets that, by mathematical proof, preserve
the entire correlation structure -- from simple pairwise relationships to
higher-order interactions -- of the source dataset. We argue that this method
provides a new approach to synthetic data generation with potential
applications in privacy-preserving data sharing, robust model training, and
simulation.

</details>


### [97] [Disentangled Representation Learning via Modular Compositional Bias](https://arxiv.org/abs/2510.21402)
*Whie Jung,Dong Hoon Lee,Seunghoon Hong*

Main category: cs.LG

TL;DR: 提出一种可组合偏置的通用DRL框架，通过基于因子规则的随机混合潜变量来实现属性、对象及其联合解耦，无需改动目标或架构。


<details>
  <summary>Details</summary>
Motivation: 现有的因子解耦方法高度依赖特定的学习目标或模型结构，难以在新因子或多因子共存时扩展，需要重新设计架构或目标，造成较大开销。

Method: 引入一个与目标和架构解耦的模块化 inductive bias: 根据因子特有的混合规则随机重组潜变量（mixing strategy），并通过两个互补的损失驱动编码器学习对应的因子结构：1) prior loss 以确保 remix 结果解码为真实图像；2) 与 Wiedemer 等人提出的 compositional consistency loss 相结合，将复合图像与其对应的复合潜在对齐。

Result: 在属性、对象及两者的组合解耦方面都达到竞争性能，并且首次实现全局风格与对象的联合解耦，方法具有较强的普适性，代码公开。

Conclusion: 提出一个通用框架，通过简单调整混合策略即可实现不同因子的解耦，避免对目标或架构的专门设计，提高了对新因子的适应性与扩展性。

Abstract: Recent disentangled representation learning (DRL) methods heavily rely on
factor specific strategies-either learning objectives for attributes or model
architectures for objects-to embed inductive biases. Such divergent approaches
result in significant overhead when novel factors of variation do not align
with prior assumptions, such as statistical independence or spatial
exclusivity, or when multiple factors coexist, as practitioners must redesign
architectures or objectives. To address this, we propose a compositional bias,
a modular inductive bias decoupled from both objectives and architectures. Our
key insight is that different factors obey distinct recombination rules in the
data distribution: global attributes are mutually exclusive, e.g., a face has
one nose, while objects share a common support (any subset of objects can
co-exist). We therefore randomly remix latents according to factor-specific
rules, i.e., a mixing strategy, and force the encoder to discover whichever
factor structure the mixing strategy reflects through two complementary
objectives: (i) a prior loss that ensures every remix decodes into a realistic
image, and (ii) the compositional consistency loss introduced by Wiedemer et
al. (arXiv:2310.05327), which aligns each composite image with its
corresponding composite latent. Under this general framework, simply adjusting
the mixing strategy enables disentanglement of attributes, objects, and even
both, without modifying the objectives or architectures. Extensive experiments
demonstrate that our method shows competitive performance in both attribute and
object disentanglement, and uniquely achieves joint disentanglement of global
style and objects. Code is available at
https://github.com/whieya/Compositional-DRL.

</details>


### [98] [DEEDEE: Fast and Scalable Out-of-Distribution Dynamics Detection](https://arxiv.org/abs/2510.21638)
*Tala Aljaafari,Varun Kanade,Philip Torr,Christian Schroeder de Witt*

Main category: cs.LG

TL;DR: DEEDEE提出一个两统计量的鲁棒OOD检测器，利用episode平均与训练摘要的RBF相似度，在RL时间序列中检测OOD，表现优于基线且计算效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的强化学习中，面临分布偏移导致的脆弱性，需要高效且有效的OOD检测以提升鲁棒性。

Method: 提出一个两统计量检测器，仅使用episodewise均值和对训练摘要的RBF核相似度，结合全局与局部偏差来检测OOD，简化了以表示学习为主的管线。

Result: 在标准的RL OOD评测中，与现有检测器相媲美甚至优于它们；计算开销显著下降，FLOPs/ wall-time约降低600倍；在多数基线中获得平均约5个百分点的绝对准确度提升。

Conclusion: 结果表明多种异常类型往往通过一组低阶统计出现在RL轨迹上，为复杂环境中的OOD检测提供一个紧凑而有效的理论与实践基础。

Abstract: Deploying reinforcement learning (RL) in safety-critical settings is
constrained by brittleness under distribution shift. We study
out-of-distribution (OOD) detection for RL time series and introduce DEEDEE, a
two-statistic detector that revisits representation-heavy pipelines with a
minimal alternative. DEEDEE uses only an episodewise mean and an RBF kernel
similarity to a training summary, capturing complementary global and local
deviations. Despite its simplicity, DEEDEE matches or surpasses contemporary
detectors across standard RL OOD suites, delivering a 600-fold reduction in
compute (FLOPs / wall-time) and an average 5% absolute accuracy gain over
strong baselines. Conceptually, our results indicate that diverse anomaly types
often imprint on RL trajectories through a small set of low-order statistics,
suggesting a compact foundation for OOD detection in complex environments.

</details>


### [99] [Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems](https://arxiv.org/abs/2510.21427)
*Hao Liang,Shuqing Shi,Yudi Zhang,Biwei Huang,Yali Du*

Main category: cs.LG

TL;DR: GSAC is a scalable, generalizable RL framework for large-scale networked systems, combining causal representation learning with meta actor-critic to achieve fast adaptation across domains.


<details>
  <summary>Details</summary>
Motivation: To tackle reinforcement learning in large heterogeneous, dynamically shifting networked systems (traffic, power, wireless grids) where scale and environment shifts hinder learning and generalization.

Method: 1) Per-agent learning of sparse local causal masks to identify minimal influencing neighborhood variables, yielding approximately compact representations (ACRs) of state and domain factors; 2) Use ACRs to bound value-function truncation error for κ-hop neighborhoods; 3) Meta actor-critic training across multiple source domains conditioned on compact domain factors; 4) At test time, estimate new domain factors from a few trajectories and adapt policy.

Result: Finite-sample guarantees on causal recovery, actor-critic convergence, and adaptation gap; empirical results show rapid adaptation in new domains and significant performance gains over learning-from-scratch and conventional baselines.

Conclusion: GSAC provides scalable, domain-generalizable control for large-scale networked systems by integrating causal representation learning with meta-learning-driven policy adaptation.

Abstract: Large-scale networked systems, such as traffic, power, and wireless grids,
challenge reinforcement-learning agents with both scale and environment shifts.
To address these challenges, we propose GSAC (Generalizable and Scalable
Actor-Critic), a framework that couples causal representation learning with
meta actor-critic learning to achieve both scalability and domain
generalization. Each agent first learns a sparse local causal mask that
provably identifies the minimal neighborhood variables influencing its
dynamics, yielding exponentially tight approximately compact representations
(ACRs) of state and domain factors. These ACRs bound the error of truncating
value functions to $\kappa$-hop neighborhoods, enabling efficient learning on
graphs. A meta actor-critic then trains a shared policy across multiple source
domains while conditioning on the compact domain factors; at test time, a few
trajectories suffice to estimate the new domain factor and deploy the adapted
policy. We establish finite-sample guarantees on causal recovery, actor-critic
convergence, and adaptation gap, and show that GSAC adapts rapidly and
significantly outperforms learning-from-scratch and conventional adaptation
baselines.

</details>


### [100] [Unified token representations for sequential decision models](https://arxiv.org/abs/2510.21448)
*Zhuojing Tian,Yushu Chen*

Main category: cs.LG

TL;DR: 提出统一的令牌表示UTR，将return-to-go、状态和动作合并为单个令牌，显著降低序列长度和模型复杂度；在变体UDT（Transformer）和UDC（门控CNN）上实现，与最先进方法性能相当或更优，但计算成本显著降低；理论上UTR带来更紧的Rademacher复杂度界，提升泛化能力；UTR具备跨架构的通用性，为可扩展的决策模型奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中基于Transformer的决策建模存在的冗余标记化和对自注意力的二次复杂度问题（如Decision Transformer及其变体），以提升可扩展性和实时性。

Method: 提出统一令牌表示UTR，将return-to-go、状态和动作合并成单个令牌以缩短序列长度；理论推导显示UTR可获得更紧的Rademacher复杂度界；实现两个变体：UDT基于Transformer骨干，UDC基于门控CNN骨干；在多项指标上达到与SOTA相当或更优且计算量显著降低。

Result: 在性能上与最先进方法相当或优于之上，同时显著降低计算成本。

Conclusion: UTR对不同架构具有良好的泛化性，为未来大规模决策模型提供一个高效的基础，促进可扩展的离线/在线控制应用。

Abstract: Transformers have demonstrated strong potential in offline reinforcement
learning (RL) by modeling trajectories as sequences of return-to-go, states,
and actions. However, existing approaches such as the Decision Transformer(DT)
and its variants suffer from redundant tokenization and quadratic attention
complexity, limiting their scalability in real-time or resource-constrained
settings. To address this, we propose a Unified Token Representation (UTR) that
merges return-to-go, state, and action into a single token, substantially
reducing sequence length and model complexity. Theoretical analysis shows that
UTR leads to a tighter Rademacher complexity bound, suggesting improved
generalization. We further develop two variants: UDT and UDC, built upon
transformer and gated CNN backbones, respectively. Both achieve comparable or
superior performance to state-of-the-art methods with markedly lower
computation. These findings demonstrate that UTR generalizes well across
architectures and may provide an efficient foundation for scalable control in
future large decision models.

</details>


### [101] [Towards Explainable Personalized Recommendations by Learning from Users' Photos](https://arxiv.org/abs/2510.21455)
*Jorge Díez,Pablo Pérez-Núñez,Oscar Luaces,Beatriz Remeseiro,Antonio Bahamonde*

Main category: cs.LG

TL;DR: 提出一个将个性化解释作为推荐任务来学习的方法：预测用户会为物品拍摄的图片以作为解释，并在TripAdvisor数据上进行演示。


<details>
  <summary>Details</summary>
Motivation: 解释性对复杂系统（如推荐系统）至关重要；通过预测用户会拍摄的图片来揭示其观点并提高可信度；能估计用户偏好的图片分布。

Method: 提出一个正式框架来估计给定的 (用户, 照片) 对的作者概率；使用包含评论及照片的 TripAdvisor 数据来演示。

Result: 在 TripAdvisor 数据上演示该框架的可行性；未在摘要中给出具体量化结果，但提供了数据驱动的示例。

Conclusion: 通过将个性化图片作为解释的基础，该方法可以提升可解释性与信任度，并揭示用户强调的产品特征分布。

Abstract: Explaining the output of a complex system, such as a Recommender System (RS),
is becoming of utmost importance for both users and companies. In this paper we
explore the idea that personalized explanations can be learned as
recommendation themselves. There are plenty of online services where users can
upload some photos, in addition to rating items. We assume that users take
these photos to reinforce or justify their opinions about the items. For this
reason we try to predict what photo a user would take of an item, because that
image is the argument that can best convince her of the qualities of the item.
In this sense, an RS can explain its results and, therefore, increase its
reliability. Furthermore, once we have a model to predict attractive images for
users, we can estimate their distribution. Thus, the companies acquire a vivid
knowledge about the aspects that the clients highlight of their products. The
paper includes a formal framework that estimates the authorship probability for
a given pair (user, photo). To illustrate the proposal, we use data gathered
from TripAdvisor containing the reviews (with photos) of restaurants in six
cities of different sizes.

</details>


### [102] [Parameter-Free Hypergraph Neural Network for Few-Shot Node Classification](https://arxiv.org/abs/2510.21462)
*Chaewoon Bae,Doyun Choi,Jaehyun Lee,Jaemin Yoo*

Main category: cs.LG

TL;DR: ZEN: a zero-parameter, fully linear hypergraph neural network with a closed-form weight solution and redundancy-aware propagation, enabling fast, interpretable few-shot node classification on hypergraphs.


<details>
  <summary>Details</summary>
Motivation: To address overfitting and scalability in hypergraph neural networks when few labels are available, while effectively capturing high-order hypergraph structures.

Method: A unified linearized HNN framework with zero learnable parameters. Derives a closed-form solution for the weight matrix and introduces a redundancy-aware propagation scheme that avoids iterative training and removes redundant self-information.

Result: On 11 real-world hypergraph benchmarks, ZEN consistently outperforms eight baselines in classification accuracy and achieves up to 696x speedups over the fastest competitor; model is fully interpretable.

Conclusion: ZEN provides expressive, efficient, and interpretable hypergraph learning without parameterized training, with publicly available code and datasets.

Abstract: Few-shot node classification on hypergraphs requires models that generalize
from scarce labels while capturing high-order structures. Existing hypergraph
neural networks (HNNs) effectively encode such structures but often suffer from
overfitting and scalability issues due to complex, black-box architectures. In
this work, we propose ZEN (Zero-Parameter Hypergraph Neural Network), a fully
linear and parameter-free model that achieves both expressiveness and
efficiency. Built upon a unified formulation of linearized HNNs, ZEN introduces
a tractable closed-form solution for the weight matrix and a redundancy-aware
propagation scheme to avoid iterative training and to eliminate redundant self
information. On 11 real-world hypergraph benchmarks, ZEN consistently
outperforms eight baseline models in classification accuracy while achieving up
to 696x speedups over the fastest competitor. Moreover, the decision process of
ZEN is fully interpretable, providing insights into the characteristic of a
dataset. Our code and datasets are fully available at
https://github.com/chaewoonbae/ZEN.

</details>


### [103] [Benchmarking Catastrophic Forgetting Mitigation Methods in Federated Time Series Forecasting](https://arxiv.org/abs/2510.21491)
*Khaled Hallak,Oudom Kem*

Main category: cs.LG

TL;DR: 提出面向联邦持续时间序列预测的灾难性遗忘基准，系统评估多种CF缓解策略，并在北京多点空气质量数据集上进行实验，提供开源实现。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景下，非独立同分布的时间序列数据导致持续学习中的灾难性遗忘问题尤为突出，且现有研究多聚焦于视觉分类任务、缺乏对回归型时间序列预测的系统研究。

Method: 构建一个专门用于CF的联邦持续时间序列预测基准框架，使用Beijing Multi-site Air Quality数据集涵盖12个客户端，评估Replay、Elastic Weight Consolidation、Learning without Forgetting、Synaptic Intelligence等CF缓解策略，进行跨方法的对比分析，最终提供可复现的开源实现。

Result: 提出了时间序列FL中的CF基准，并完成对多种缓解策略的系统性对比分析，公开发布开源框架与数据处理/评测流程。

Conclusion: 该工作为在联邦时间序列预测场景中推进持续学习提供了基准、对比分析与可复现工具，便于后续研究与工程落地。

Abstract: Catastrophic forgetting (CF) poses a persistent challenge in continual
learning (CL), especially within federated learning (FL) environments
characterized by non-i.i.d. time series data. While existing research has
largely focused on classification tasks in vision domains, the regression-based
forecasting setting prevalent in IoT and edge applications remains
underexplored. In this paper, we present the first benchmarking framework
tailored to investigate CF in federated continual time series forecasting.
Using the Beijing Multi-site Air Quality dataset across 12 decentralized
clients, we systematically evaluate several CF mitigation strategies, including
Replay, Elastic Weight Consolidation, Learning without Forgetting, and Synaptic
Intelligence. Key contributions include: (i) introducing a new benchmark for CF
in time series FL, (ii) conducting a comprehensive comparative analysis of
state-of-the-art methods, and (iii) releasing a reproducible open-source
framework. This work provides essential tools and insights for advancing
continual learning in federated time-series forecasting systems.

</details>


### [104] [Uniform Convergence Beyond Glivenko-Cantelli](https://arxiv.org/abs/2510.21506)
*Tanmay Devale,Pramith Devulapalli,Steve Hanneke*

Main category: cs.LG

TL;DR: 提出 Uniform Mean Estimability (UME) 作为对经验均值无偏估计之外的统一均值估计框架，并给出分布族族的均值向量可分性充足但非必要的结论；给出非分离但可UME学习的构造及可数并集合的UME可学习性保持性，解决了 Cohen 等人 (2025) 的一个猜想。


<details>
  <summary>Details</summary>
Motivation: 在 {0,1}^N 上的分布族集合中超越经验均值估计，研究任意估计量下的均值的统一可估计性（UME-学习性），扩展 Vapnik–Chervonenkis 框架到一个估计器无关的设定。

Method: 定义并研究 mean vector 空间；证明 mean vectors 的可分性是UME-学习性的充分条件；通过新的非传统技巧构造一个 mean vectors 非可分但仍然UME-可学习的集合，表明可分性不是必要条件；证明UME-可学习集合的可数并集合仍然是UME-可学习，从而解决 Cohen et al. (2025) 的猜想。

Result: 给出关于UME-学习性的两类充分/必要性结果：可分性充足但非必要；存在非分离但UME可学习的示例；并且可数并集合的UME可学习性封闭性。并且解决了跨论文的一个未决猜想。

Conclusion: UME学习框架拓展了统一估计的边界，显示可分性并非必要条件，存在更广泛的学习能力与范畴；提供了估计器无关的统一均值估计的理论基础，并揭示了集合运算（可数并集）上的闭性，指向对未来研究的潜在方向。

Abstract: We characterize conditions under which collections of distributions on
$\{0,1\}^\mathbb{N}$ admit uniform estimation of their mean. Prior work from
Vapnik and Chervonenkis (1971) has focused on uniform convergence using the
empirical mean estimator, leading to the principle known as $P-$
Glivenko-Cantelli. We extend this framework by moving beyond the empirical mean
estimator and introducing Uniform Mean Estimability, also called $UME-$
learnability, which captures when a collection permits uniform mean estimation
by any arbitrary estimator. We work on the space created by the mean vectors of
the collection of distributions. For each distribution, the mean vector records
the expected value in each coordinate. We show that separability of the mean
vectors is a sufficient condition for $UME-$ learnability. However, we show
that separability of the mean vectors is not necessary for $UME-$ learnability
by constructing a collection of distributions whose mean vectors are
non-separable yet $UME-$ learnable using techniques fundamentally different
from those used in our separability-based analysis. Finally, we establish that
countable unions of $UME-$ learnable collections are also $UME-$ learnable,
solving a conjecture posed in Cohen et al. (2025).

</details>


### [105] [Surrogate-based quantification of policy uncertainty in generative flow networks](https://arxiv.org/abs/2510.21523)
*Ramón Nartallo-Kaluarachchi,Robert Manson-Sawko,Shashanka Ubaru,Dongsung Huh,Małgorzata J Zimoń,Lior Horesh,Yoshua Bengio*

Main category: cs.LG

TL;DR: Proposes a surrogate model using polynomial chaos expansion to quantify epistemic uncertainty in Generative Flow Networks (GFlowNets) under noisy reward estimates.


<details>
  <summary>Details</summary>
Motivation: Reward functions estimated from data are noisy, causing epistemic uncertainty in the learned policy of GFlowNets; need efficient, inexpensive uncertainty quantification.

Method: Train an ensemble of flow networks; fit a polynomial chaos expansion as a low-dimensional surrogate that links reward parameters to action distributions at each step; use Monte Carlo sampling with the surrogate to estimate policy uncertainty.

Result: Shows that the surrogate enables inexpensive and effective uncertainty quantification across discrete/continuous grid-worlds, symbolic regression, and Bayesian structure learning tasks.

Conclusion: A lightweight surrogate model can quantify and propagate reward-uncertainty into policy uncertainty for GFlowNets, facilitating more robust decision-making under imperfect reward information.

Abstract: Generative flow networks are able to sample, via sequential construction,
high-reward, complex objects according to a reward function. However, such
reward functions are often estimated approximately from noisy data, leading to
epistemic uncertainty in the learnt policy. We present an approach to quantify
this uncertainty by constructing a surrogate model composed of a polynomial
chaos expansion, fit on a small ensemble of trained flow networks. This model
learns the relationship between reward functions, parametrised in a
low-dimensional space, and the probability distributions over actions at each
step along a trajectory of the flow network. The surrogate model can then be
used for inexpensive Monte Carlo sampling to estimate the uncertainty in the
policy given uncertain rewards. We illustrate the performance of our approach
on a discrete and continuous grid-world, symbolic regression, and a Bayesian
structure learning task.

</details>


### [106] [Probe-based Fine-tuning for Reducing Toxicity](https://arxiv.org/abs/2510.21531)
*Jan Wehner,Mario Fritz*

Main category: cs.LG

TL;DR: 训练基于模型激活的探针可以作为检测不良行为的工具，同时也是有用的训练信号；但将探针作为训练目标可能触发Goodhart现象。提出两种基于探针的训练方法（监督微调与直接偏好优化），在消除毒性测试床上进行初步评估，并探讨通过集成探针、保留未用于训练的探针以及重新训练探针来维持探针检测能力。结果显示：偏好优化在保持探针可检测性方面优于基于分类器的方法；探针多样性收益有限，重新训练探针即可恢复高检测准确性；在可重新训练的前提下，探针集成并非必要。


<details>
  <summary>Details</summary>
Motivation: 解决将探针作为训练目标所引发的可观测中性信号的可靠性下降问题（Goodhart's Law），探索在对齐任务中如何训练以既保留探针的检测能力又避免其失效。

Method: 比较两种训练方法：监督微调（SFT）与直接偏好优化（DPO）；在毒性降低的测试床上评估探针对训练的鲁棒性，即训练对探针的影响下探针的准确性下降幅度；并测试三种策略以维持探针性能：对探针进行集合训练、保留未用于训练的探针、以及在训练后重新训练探针。

Result: 意外地，基于偏好优化的探针训练能比基于分类器的方法更好地保留探针的可检测性。探针多样性对实际收益帮助有限；仅通过在训练后重新训练探针就能恢复高探测精度。总体而言，探针驱动的训练在某些对齐方法中可行，但在可重新训练的条件下，探针集成并非必要。

Conclusion: 探针驱动的对齐方法在一定条件下具有可行性，尤其是通过偏好优化来维持探针信号；然而若能重新训练探针，探针集合的必要性较低。这提醒研究者在对齐中应权衡探针稳定性、可重复性与训练成本，并优先考虑可重新训练的策略。

Abstract: Probes trained on model activations can detect undesirable behaviors like
deception or biases that are difficult to identify from outputs alone. This
makes them useful detectors to identify misbehavior. Furthermore, they are also
valuable training signals, since they not only reward outputs, but also good
internal processes for arriving at that output. However, training against
interpretability tools raises a fundamental concern: when a monitor becomes a
training target, it may cease to be reliable (Goodhart's Law). We propose two
methods for training against probes based on Supervised Fine-tuning and Direct
Preference Optimization. We conduct an initial exploration of these methods in
a testbed for reducing toxicity and evaluate the amount by which probe accuracy
drops when training against them. To retain the accuracy of probe-detectors
after training, we attempt (1) to train against an ensemble of probes, (2)
retain held-out probes that aren't used for training, and (3) retrain new
probes after training.
  First, probe-based preference optimization unexpectedly preserves probe
detectability better than classifier-based methods, suggesting the preference
learning objective incentivizes maintaining rather than obfuscating relevant
representations. Second, probe diversity provides minimal practical benefit -
simply retraining probes after optimization recovers high detection accuracy.
Our findings suggest probe-based training can be viable for certain alignment
methods, though probe ensembles are largely unnecessary when retraining is
feasible.

</details>


### [107] [FrameShield: Adversarially Robust Video Anomaly Detection](https://arxiv.org/abs/2510.21532)
*Mojtaba Nafez,Mobina Poulaei,Nikan Vasei,Bardia Soltani Moakhar,Mohammad Sabokrou,MohammadHossein Rohban*

Main category: cs.LG

TL;DR: 提出SRD伪异常生成法，用强增强局部区域来生成伪异常，并与噪声伪标签结合，提升弱监督视频异常检测的对抗鲁棒性，AUROC显著提升约71%。


<details>
  <summary>Details</summary>
Motivation: 在弱监督设定下，帧级预测依赖视频级标签，传统对抗训练效果有限；伪标签来自模型且存在噪声，难以支撑帧级对抗训练。

Method: 提出Spatiotemporal Region Distortion (SRD)，对正常视频的局部区域进行严重增强以制造合成异常，同时保持时间一致性；将这些标注清晰的合成异常与噪声伪标签结合用于帧级对抗训练，从而降低标签噪声并提升鲁棒性。

Result: 实验显示对抗鲁棒性显著提升，在多个基准上的总体AUROC平均提升约71.0%，优于现有方法；代码公开。

Conclusion: SRD通过提供更清晰的合成异常信号、缓解伪标签噪声，显著增强WSVAD在对抗攻击下的鲁棒性，并给出实现与代码。

Abstract: Weakly Supervised Video Anomaly Detection (WSVAD) has achieved notable
advancements, yet existing models remain vulnerable to adversarial attacks,
limiting their reliability. Due to the inherent constraints of weak
supervision, where only video-level labels are provided despite the need for
frame-level predictions, traditional adversarial defense mechanisms, such as
adversarial training, are not effective since video-level adversarial
perturbations are typically weak and inadequate. To address this limitation,
pseudo-labels generated directly from the model can enable frame-level
adversarial training; however, these pseudo-labels are inherently noisy,
significantly degrading performance. We therefore introduce a novel
Pseudo-Anomaly Generation method called Spatiotemporal Region Distortion (SRD),
which creates synthetic anomalies by applying severe augmentations to localized
regions in normal videos while preserving temporal consistency. Integrating
these precisely annotated synthetic anomalies with the noisy pseudo-labels
substantially reduces label noise, enabling effective adversarial training.
Extensive experiments demonstrate that our method significantly enhances the
robustness of WSVAD models against adversarial attacks, outperforming
state-of-the-art methods by an average of 71.0\% in overall AUROC performance
across multiple benchmarks. The implementation and code are publicly available
at https://github.com/rohban-lab/FrameShield.

</details>


### [108] [Excision Score: Evaluating Edits with Surgical Precision](https://arxiv.org/abs/2510.21537)
*Nikolai Gruzinov,Ksenia Sycheva,Earl T. Barr,Alex Bezzubov*

Main category: cs.LG

TL;DR: 提出 Excision Score（ES）用于修订相似性评估，解决现有指标被共享文本主导的问题，通过对比删去共享内容后的剩余区域来衡量修订的差异性。


<details>
  <summary>Details</summary>
Motivation: 修订文本评估需要与人类判断对齐；现有如 BLEU 等对共享上下文过于敏感，无法正确区分不同程度的修订质量；需要一个对共享内容不敏感、能正确处理移动代码块等场景的静态评估度量。

Method: 提出基于最长公共子序列（LCS）的 Excision Score，先通过 LCS 去除现有文档与 ground truth/预测修订之间的共享内容，再在剩余区域上计算相似度；通过近似将 LCS 复杂度从立方降到平方，适用于大规模评估；提出五条为修订相似性设计的充分性标准。

Result: 在代码编辑评估中，ES 超越现有度量；在 HumanEvalFix 上，ES 相比最近的 SARI 提升 12% 的 Pearson 相关性，优于 BLEU 超过 21%；对共享上下文的扰动使得 ES 相对于 SARI 的改进提升至 20%，相对 BLEU 提升 >30%；还能正确对齐移动的代码块，并对匹配的插入/删除给予合理奖励。

Conclusion: ES 作为一种静态修订相似性评估度量，解决了共享上下文对分数的主导问题，结果更符合人类判断，且适用于代码编辑场景，计算效率更高。

Abstract: Many tasks revolve around editing a document, whether code or text. We
formulate the revision similarity problem to unify a wide range of machine
learning evaluation problems whose goal is to assess a revision to an existing
document. We observe that revisions usually change only a small portion of an
existing document, so the existing document and its immediate revisions share a
majority of their content. We formulate five adequacy criteria for revision
similarity measures, designed to align them with human judgement. We show that
popular pairwise measures, like BLEU, fail to meet these criteria, because
their scores are dominated by the shared content. They report high similarity
between two revisions when humans would assess them as quite different. This is
a fundamental flaw we address. We propose a novel static measure, Excision
Score (ES), which computes longest common subsequence (LCS) to remove content
shared by an existing document with the ground truth and predicted revisions,
before comparing only the remaining divergent regions. This is analogous to a
surgeon creating a sterile field to focus on the work area. We use
approximation to speed the standard cubic LCS computation to quadratic. In
code-editing evaluation, where static measures are often used as a cheap proxy
for passing tests, we demonstrate that ES surpasses existing measures. When
aligned with test execution on HumanEvalFix, ES improves over its nearest
competitor, SARI, by 12% Pearson correlation and by >21% over standard measures
like BLEU. The key criterion is invariance to shared context; when we perturb
HumanEvalFix with increased shared context, ES' improvement over SARI increases
to 20% and >30% over standard measures. ES also handles other corner cases that
other measures do not, such as correctly aligning moved code blocks, and
appropriately rewarding matching insertions or deletions.

</details>


### [109] [Interpretable Multimodal Zero-Shot ECG Diagnosis via Structured Clinical Knowledge Alignment](https://arxiv.org/abs/2510.21551)
*Jialu Tang,Hung Manh Pham,Ignace De Lathauwer,Henk S. Schipper,Yuan Lu,Dong Ma,Aaqib Saeed*

Main category: cs.LG

TL;DR: 提出ZETA，一种零-shot多模态ECG诊断框架，通过与结构化正负临床观察对比实现可解释性，使用预训练多模态模型对齐ECG与文本嵌入，在未疾病特定微调条件下运行。


<details>
  <summary>Details</summary>
Motivation: 解决ECG自动诊断的透明性和泛化能力不足的问题；需要一个更可解释、与临床工作流程对齐的诊断系统。

Method: 将ECG信号与结构化的正/负诊断观察进行对比，观察集由LLM助力、专家验证，模仿鉴别诊断；利用预训练多模态模型对齐ECG和文本嵌入，不进行疾病特异的微调。

Result: 在零-shot分类任务中具有竞争力的性能，同时提供定性与定量的可解释性证据，预测 grounding 在具体、临床相关的正负诊断特征；展示更透明、可泛化、可信的AI诊断系统。

Conclusion: 对将ECG分析与结构化临床知识对齐的潜力进行强调，未来发布观察数据集和代码以促进研究。

Abstract: Electrocardiogram (ECG) interpretation is essential for cardiovascular
disease diagnosis, but current automated systems often struggle with
transparency and generalization to unseen conditions. To address this, we
introduce ZETA, a zero-shot multimodal framework designed for interpretable ECG
diagnosis aligned with clinical workflows. ZETA uniquely compares ECG signals
against structured positive and negative clinical observations, which are
curated through an LLM-assisted, expert-validated process, thereby mimicking
differential diagnosis. Our approach leverages a pre-trained multimodal model
to align ECG and text embeddings without disease-specific fine-tuning.
Empirical evaluations demonstrate ZETA's competitive zero-shot classification
performance and, importantly, provide qualitative and quantitative evidence of
enhanced interpretability, grounding predictions in specific, clinically
relevant positive and negative diagnostic features. ZETA underscores the
potential of aligning ECG analysis with structured clinical knowledge for
building more transparent, generalizable, and trustworthy AI diagnostic
systems. We will release the curated observation dataset and code to facilitate
future research.

</details>


### [110] [REVE: A Foundation Model for EEG -- Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects](https://arxiv.org/abs/2510.21585)
*Yassine El Ouahidi,Jonathan Lys,Philipp Thölke,Nicolas Farrugia,Bastien Pasdeloup,Vincent Gripon,Karim Jerbi,Giulia Lioi*

Main category: cs.LG

TL;DR: REVE 是一个面向 EEG 的预训练基础模型，采用4D位置编码和掩码自编码目标，在超6万小时、92个数据集、2.5万名受试者上进行大规模预训练，能在跨数据集的变异中实现强泛化，且在10个下游任务上达到SOTA，少量微调即可获得良好迁移，且公开代码和权重。


<details>
  <summary>Details</summary>
Motivation: EEG 数据受记录协议、设备、以及电极配置的高度异质性影响，导致跨数据集的泛化能力不足；现有 EEG 基础模型通常局限于单一设置，在线性探针等场景下表现欠佳。需要一个能够处理任意长度与电极排列的通用表征，以实现跨数据集的稳健迁移。

Method: 提出4D位置编码，使模型能够处理任意长度和电极布置；采用掩码自编码(MAE)的自监督预训练目标；在超过60,000 小时、92个数据集、25,000名受试者上进行大规模预训练；模型具备强泛化能力和时空建模能力。

Result: 在10个下游 EEG 任务上达到状态-of-the-art（SOTA），包括运动想象分类、癫痫检测、睡眠分期、认知负荷估计和情感识别；显示出强泛化性，且在较少微调时就能实现良好表现，体现出对时空相关性的细腻建模。

Conclusion: 发布代码、预训练权重和教程，支持标准化 EEG 研究，促进临床神经科学的发展。

Abstract: Foundation models have transformed AI by reducing reliance on task-specific
data through large-scale pretraining. While successful in language and vision,
their adoption in EEG has lagged due to the heterogeneity of public datasets,
which are collected under varying protocols, devices, and electrode
configurations. Existing EEG foundation models struggle to generalize across
these variations, often restricting pretraining to a single setup, resulting in
suboptimal performance, in particular under linear probing. We present REVE
(Representation for EEG with Versatile Embeddings), a pretrained model
explicitly designed to generalize across diverse EEG signals. REVE introduces a
novel 4D positional encoding scheme that enables it to process signals of
arbitrary length and electrode arrangement. Using a masked autoencoding
objective, we pretrain REVE on over 60,000 hours of EEG data from 92 datasets
spanning 25,000 subjects, representing the largest EEG pretraining effort to
date. REVE achieves state-of-the-art results on 10 downstream EEG tasks,
including motor imagery classification, seizure detection, sleep staging,
cognitive load estimation, and emotion recognition. With little to no
fine-tuning, it demonstrates strong generalization, and nuanced spatio-temporal
modeling. We release code, pretrained weights, and tutorials to support
standardized EEG research and accelerate progress in clinical neuroscience.

</details>


### [111] [Accelerating Data Generation for Nonlinear temporal PDEs via homologous perturbation in solution space](https://arxiv.org/abs/2510.21592)
*Lei Liu,Zhenxin Huang,Hong Wang,huanshuo dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: HOPSS 提供一种更高效的数据生成方法，用下采样和同源扰动在保持近似精度的同时产生用于神经算子训练的PDE数据集，大幅降低数据生成时间；在Navier-Stokes上实现10000个样本仅需传统方法约10%的时间，训练性能相当。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解产生大量解对（解函数与右端项）需要多步时间迭代，数据集生成成本高且时间开销大，成为训练神经算子的一大瓶颈。

Method: 1) 用可靠求解器获取包含几十万或更多时间步的基解序列；2) 通过下采样将其与训练数据的时间步对齐；3) 引入同源扰动：将一个解函数作为主函数，另一个作为同源扰动项，乘以小系数并叠加随机噪声，生成等效的高精度PDE数据点；4) 计算原方程 RHS 的变化，得到新的解对集合；5) 理论与实验表明时间复杂度降低。

Result: 理论与实验均表明时间成本显著下降。以Navier-Stokes为例，生成1万个样本所需时间仅为传统方法的约10%，且对训练效果影响相当。

Conclusion: HOPSS 是一种有效的PDE数据生成策略，能在保持近似精度的前提下显著降低数据集构建成本，并具备对其他非线性时-变PDE的推广潜力。

Abstract: Data-driven deep learning methods like neural operators have advanced in
solving nonlinear temporal partial differential equations (PDEs). However,
these methods require large quantities of solution pairs\u2014the solution
functions and right-hand sides (RHS) of the equations. These pairs are
typically generated via traditional numerical methods, which need thousands of
time steps iterations far more than the dozens required for training, creating
heavy computational and temporal overheads. To address these challenges, we
propose a novel data generation algorithm, called HOmologous Perturbation in
Solution Space (HOPSS), which directly generates training datasets with fewer
time steps rather than following the traditional approach of generating large
time steps datasets. This algorithm simultaneously accelerates dataset
generation and preserves the approximate precision required for model training.
Specifically, we first obtain a set of base solution functions from a reliable
solver, usually with thousands of time steps, and then align them in time steps
with training datasets by downsampling. Subsequently, we propose a "homologous
perturbation" approach: by combining two solution functions (one as the primary
function, the other as a homologous perturbation term scaled by a small scalar)
with random noise, we efficiently generate comparable-precision PDE data
points. Finally, using these data points, we compute the variation in the
original equation's RHS to form new solution pairs. Theoretical and
experimental results show HOPSS lowers time complexity. For example, on the
Navier-Stokes equation, it generates 10,000 samples in approximately 10% of
traditional methods' time, with comparable model training performance.

</details>


### [112] [Generalised Flow Maps for Few-Step Generative Modelling on Riemannian Manifolds](https://arxiv.org/abs/2510.21608)
*Oscar Davis,Michael S. Albergo,Nicholas M. Boffi,Michael M. Bronstein,Avishek Joey Bose*

Main category: cs.LG

TL;DR: 提出 Generalised Flow Maps (GFM)，在任意黎曼流形上实现少步生成；提供三种自蒸馏训练策略，并理论/实验上将现有 Euclidean 少步模型统一并提升，在几何数据集上达到领先的样本质量及较优的对数似然。


<details>
  <summary>Details</summary>
Motivation: 现有几何生成模型在推断阶段成本高，源于在黎曼流形上进行扩散/流映射等复杂数值运算；需要高效、少步的生成框架以应用于地理数据、蛋白等几何数据。

Method: 把 Flow Map 框架推广到黎曼流形，提出 Generalised Lagrangian Flow Maps、Generalised Eulerian Flow Maps、Generalised Progressive Flow Maps 三种自蒸馏训练方法；给出理论结果，表明 GFMs 在特定设计下能统一并提升现有 Euclidean 少步模型（如一致性模型、捷径模型、meanflows）；在地理数据、RNA 构象扭转角、超曲率流形等数据集上进行基准评估，与其他几何生成模型比较。

Result: 在单步和少步评估中实现状态最优的样本质量，在隐式概率流下获得优越或具有竞争力的对数似然。

Conclusion: GFMs 为黎曼流形上的高效几何生成提供统一框架，提升并泛化了 Euclidean 少步生成模型的能力。

Abstract: Geometric data and purpose-built generative models on them have become
ubiquitous in high-impact deep learning application domains, ranging from
protein backbone generation and computational chemistry to geospatial data.
Current geometric generative models remain computationally expensive at
inference -- requiring many steps of complex numerical simulation -- as they
are derived from dynamical measure transport frameworks such as diffusion and
flow-matching on Riemannian manifolds. In this paper, we propose Generalised
Flow Maps (GFM), a new class of few-step generative models that generalises the
Flow Map framework in Euclidean spaces to arbitrary Riemannian manifolds. We
instantiate GFMs with three self-distillation-based training methods:
Generalised Lagrangian Flow Maps, Generalised Eulerian Flow Maps, and
Generalised Progressive Flow Maps. We theoretically show that GFMs, under
specific design decisions, unify and elevate existing Euclidean few-step
generative models, such as consistency models, shortcut models, and meanflows,
to the Riemannian setting. We benchmark GFMs against other geometric generative
models on a suite of geometric datasets, including geospatial data, RNA torsion
angles, and hyperbolic manifolds, and achieve state-of-the-art sample quality
for single- and few-step evaluations, and superior or competitive
log-likelihoods using the implicit probability flow.

</details>


### [113] [Equivariance by Contrast: Identifiable Equivariant Embeddings from Unlabeled Finite Group Actions](https://arxiv.org/abs/2510.21706)
*Tobias Schmidt,Steffen Schneider,Matthias Bethge*

Main category: cs.LG

TL;DR:  EbC learns equivariant embeddings from observation pairs under a finite group, jointly learning latent space and a group representation so actions become invertible linear maps; encoder-only approach validated on synthetic data including non‑abelian groups, with identifiability proof.


<details>
  <summary>Details</summary>
Motivation: To obtain general-purpose, encoder-only equivariant embeddings without relying on handcrafted group biases, using only observations of y and g·y.

Method: Jointly learn a latent representation and a group representation such that the group action g is realized as an invertible linear map in latent space, trained from observation pairs (y, g·y) for g in a finite group.

Result: Validated on the infinite dSprites dataset with a product group G = (R_m × Z_n × Z_n) (rotations plus periodic translations); high-fidelity equivariance with group operations reproduced in latent space; extended to synthetic data for non-abelian O(n) and GL(n); theoretical identifiability proof provided.

Conclusion: The work constitutes the first encoder-only, general-purpose equivariant learning from group-action observations, including non-abelian and product groups; future work includes evaluation on real-world data and broader group types.

Abstract: We propose Equivariance by Contrast (EbC) to learn equivariant embeddings
from observation pairs $(\mathbf{y}, g \cdot \mathbf{y})$, where $g$ is drawn
from a finite group acting on the data. Our method jointly learns a latent
space and a group representation in which group actions correspond to
invertible linear maps -- without relying on group-specific inductive biases.
We validate our approach on the infinite dSprites dataset with structured
transformations defined by the finite group $G:= (R_m \times \mathbb{Z}_n
\times \mathbb{Z}_n)$, combining discrete rotations and periodic translations.
The resulting embeddings exhibit high-fidelity equivariance, with group
operations faithfully reproduced in latent space. On synthetic data, we further
validate the approach on the non-abelian orthogonal group $O(n)$ and the
general linear group $GL(n)$. We also provide a theoretical proof for
identifiability. While broad evaluation across diverse group types on
real-world data remains future work, our results constitute the first
successful demonstration of general-purpose encoder-only equivariant learning
from group action observations alone, including non-trivial non-abelian groups
and a product group motivated by modeling affine equivariances in computer
vision.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [Fuzzy numbers revisited: operations on extensional fuzzy numbers](https://arxiv.org/abs/2510.20861)
*Krzysztof Siminski*

Main category: cs.AI

TL;DR: 提出Extensional fuzzy numbers（扩展模糊数）以解决Zadeh扩展下的计算复杂度和形状保持问题，定义了运算和关系运算，并给出应用示例与C++开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有模糊数在实际应用中受限，原因包括：Zadeh扩展的高计算成本、某些模糊集在某些运算下结果不再保持原有特征（如三角形模糊数相乘不再是三角形），以及多次运算导致模糊度增加。需要一种能够更稳健、可控的表示来提升可用性。

Method: 引入扩展模糊数（extensional fuzzy numbers），为其建立一整套运算规则并定义Relational operators (=, >, >=, <, <=)，通过若干应用示例来说明其可行性，且提供C++实现并在GitHub公开。

Result: 通过若干应用实例展示所提出方法的可行性与实用性，证明扩展模糊数在保持特征和控制模糊度方面具备潜在优势。

Conclusion: 扩展模糊数为模糊数运算提供新的框架，能够缓解计算复杂度与模糊度膨胀等问题，具有广泛应用前景；相关实现已公开供使用和验证。

Abstract: Fuzzy numbers are commonly represented with fuzzy sets. Their objective is to
better represent imprecise data. However, operations on fuzzy numbers are not
as straightforward as maths on crisp numbers. Commonly, the Zadeh's extension
rule is applied to elaborate a result. This can produce two problems: (1) high
computational complexity and (2) for some fuzzy sets and some operations the
results is not a fuzzy set with the same features (eg. multiplication of two
triangular fuzzy sets does not produce a triangular fuzzy set). One more
problem is the fuzzy spread -- fuzziness of the result increases with the
number of operations. These facts can severely limit the application field of
fuzzy numbers. In this paper we would like to revisite this problem with a
different kind of fuzzy numbers -- extensional fuzzy numbers. The paper defines
operations on extensional fuzzy numbers and relational operators (=, >, >=, <,
<=) for them. The proposed approach is illustrated with several applicational
examples. The C++ implementation is available from a public GitHub repository.

</details>


### [115] [Epistemic Deference to AI](https://arxiv.org/abs/2510.21043)
*Benjamin Lange*

Main category: cs.AI

TL;DR: 提出 AI 可以成为人工认知权威，但应以“总证据观”而非直接替代人类推理来进行 deference，特别在高风险情境下。


<details>
  <summary>Details</summary>
Motivation: 受社会认识论启发，解决何时应依赖 AI 输出以及如何避免对 AI 的盲目信任；通过界定 AEAs、提出 AI Preemptionism 与其对比。

Method: 理论分析：界定 AEAs、提出 AI Preemptionism、批判其缺陷，提出总证据观作为替代；构建三大优点的论证框架。

Result: 得到一个可操作的原则框架，用于判断何时应让 AI 的输出作为人类独立证据的补充；强调避免专家能力退化、保持人类监督、以及在可靠性不足时的可置信性。

Conclusion: AI 应作为贡献性证据而非替代品，确保在高风险情境中保持人类参与与控制；尽管实施要求较高，但提供了一个系统、合乎原则的 deference 判定标准。

Abstract: When should we defer to AI outputs over human expert judgment? Drawing on
recent work in social epistemology, I motivate the idea that some AI systems
qualify as Artificial Epistemic Authorities (AEAs) due to their demonstrated
reliability and epistemic superiority. I then introduce AI Preemptionism, the
view that AEA outputs should replace rather than supplement a user's
independent epistemic reasons. I show that classic objections to preemptionism
- such as uncritical deference, epistemic entrenchment, and unhinging epistemic
bases - apply in amplified form to AEAs, given their opacity, self-reinforcing
authority, and lack of epistemic failure markers. Against this, I develop a
more promising alternative: a total evidence view of AI deference. According to
this view, AEA outputs should function as contributory reasons rather than
outright replacements for a user's independent epistemic considerations. This
approach has three key advantages: (i) it mitigates expertise atrophy by
keeping human users engaged, (ii) it provides an epistemic case for meaningful
human oversight and control, and (iii) it explains the justified mistrust of AI
when reliability conditions are unmet. While demanding in practice, this
account offers a principled way to determine when AI deference is justified,
particularly in high-stakes contexts requiring rigorous reliability.

</details>


### [116] [MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning](https://arxiv.org/abs/2510.21093)
*Siyong Chen,Jinbo Wen,Jiawen Kang,Tenghui Huang,Xumin Huang,Yuanjia Su,Hudan Pan,Zishao Zhong,Dusit Niyato,Shengli Xie,Dong In Kim*

Main category: cs.AI

TL;DR: MedAlign 提出用于医疗视觉问答的 LVLM 框架，通过 mDPO、RA-MoE 和联邦治理，提升视觉与文本证据的一致性，降低幻觉风险，同时实现自适应推理和跨机构协作，在 Med-VQA 数据集上达到SOTA，提升F1并显著缩短推理长度。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉-语言模型在医疗场景中的幻觉、固定深度推理、跨机构协作困难等核心挑战；需要在可视化证据基础上提供可信回答，并支持多机构环境的协同优化。

Method: 1) 提出多模态 Direct Preference Optimization (mDPO) 以显式对齐带有视觉上下文的偏好学习；2) 设计 Retrieval-Aware Mixture-of-Experts (RA-MoE) 架构，利用图像与文本相似性将查询路由给专门化的专家 LVLM，并进行上下文强化；3) 提出联邦治理机制，选定的专家在本地基于 mDPO 微调，使用本地元认知不确定性估计进行迭代 Chain-of-Thought 推理；4) 在三个 Med-VQA 数据集上进行广泛实验。

Result: 实验结果显示 MedAlign 达到或超越 SOTA，相对于强基线（检索增强的）提升最多 11.85% 的 F1 分数；在与固定深度的 Chain-of-Thought 相比，推理长度平均缩短 51.60%。

Conclusion: MedAlign 通过结合 mDPO、RA-MoE 和联邦治理实现可视化准确性、适应性推理和跨机构协作的平衡，显著提升 Med-VQA 的准确性与推理效率；但需在不同临床场景中进一步验证鲁棒性与隐私合规性。

Abstract: Recently, large models have shown significant potential for smart healthcare.
However, the deployment of Large Vision-Language Models (LVLMs) for clinical
services is currently hindered by three critical challenges: a tendency to
hallucinate answers not grounded in visual evidence, the inefficiency of
fixed-depth reasoning, and the difficulty of multi-institutional collaboration.
To address these challenges, in this paper, we develop MedAlign, a novel
framework to ensure visually accurate LVLM responses for Medical Visual
Question Answering (Med-VQA). Specifically, we first propose a multimodal
Direct Preference Optimization (mDPO) objective to explicitly align preference
learning with visual context. We then design a Retrieval-Aware
Mixture-of-Experts (RA-MoE) architecture that utilizes image and text
similarity to route queries to a specialized and context-augmented LVLM (i.e.,
an expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive
reasoning and facilitate multi-institutional collaboration, we propose a
federated governance mechanism, where the selected expert, fine-tuned on
clinical datasets based on mDPO, locally performs iterative Chain-of-Thought
(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive
experiments on three representative Med-VQA datasets demonstrate that MedAlign
achieves state-of-the-art performance, outperforming strong retrieval-augmented
baselines by up to $11.85\%$ in F1-score, and simultaneously reducing the
average reasoning length by $51.60\%$ compared with fixed-depth CoT approaches.

</details>


### [117] [Confounding Robust Deep Reinforcement Learning: A Causal Approach](https://arxiv.org/abs/2510.21110)
*Mingxuan Li,Junzhe Zhang,Elias Bareinboim*

Main category: cs.AI

TL;DR: 提出一种对观测偏差和未观测混淆鲁棒的深度强化学习算法，基于DQN，在存在偏差数据的高维环境中寻找对最坏情形的安全策略，并在12个混淆的Atari游戏上优于标准DQN。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中学习有效策略的关键任务中，Off-policy学习在高维领域容易受未观测混淆影响，需鲁棒方法。

Method: 在DQN基础上，提出一个能够在观测数据兼容的最坏情境下搜索安全策略的鲁棒算法；适用于有观测输入不匹配和未观测混淆的情形。

Result: 在12个混淆的Atari游戏中，该算法在所有观测输入不匹配且存在未观混淆的游戏中均优于DQN。

Conclusion: 为存在观测偏差和未观测混淆的复杂域提供一种可行的鲁棒深度强化学习策略，显著优于传统DQN。

Abstract: A key task in Artificial Intelligence is learning effective policies for
controlling agents in unknown environments to optimize performance measures.
Off-policy learning methods, like Q-learning, allow learners to make optimal
decisions based on past experiences. This paper studies off-policy learning
from biased data in complex and high-dimensional domains where \emph{unobserved
confounding} cannot be ruled out a priori. Building on the well-celebrated Deep
Q-Network (DQN), we propose a novel deep reinforcement learning algorithm
robust to confounding biases in observed data. Specifically, our algorithm
attempts to find a safe policy for the worst-case environment compatible with
the observations. We apply our method to twelve confounded Atari games, and
find that it consistently dominates the standard DQN in all games where the
observed input to the behavioral and target policies mismatch and unobserved
confounders exist.

</details>


### [118] [NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge](https://arxiv.org/abs/2510.21144)
*Hanyu Zhu,Lance Fiondella,Jiawei Yuan,Kai Zeng,Long Jiao*

Main category: cs.AI

TL;DR: 提出 NeuroGenPoisoning 框架，通过利用 LLM 内部神经元归因并结合遗传优化来生成对外部知识的对抗性文本，从而对 RAG 系统进行污染攻击。实验显示在多模型/数据集上实现 POSR 超过 90%，且保持文本流畅性，并能有效解决知识冲突，具备跨模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在检索增强生成中，外部知识可能被污染，现有攻击多聚焦检索内容或提示结构，忽略模型内部表示动力学与神经元层面的敏感性。本文的动机是揭示 RAG 污染的内部机制，研究在强参数知识存在条件下的知识冲突影响，并探索如何通过内部信号引导攻击以实现大规模高效攻击。

Method: 首先识别与污染相关联的 Poison-Responsive Neurons，基于它们的激活与上下文污染知识的相关性进行筛选；随后利用遗传算法进化对外部文本片段，使其最大程度触发这些神经元的激活；通过观察归因信号，重新利用那些最初未成功但具有潜力的变体，实现海量高效污染知识的生成；并在不同模型和数据集上评估 POSR 与文本流畅性，同时论证知识冲突的缓解效果。

Result: 实验结果在多模型与数据集上实现 POSR > 90%，并保持较高的文本流畅性；还显示该方法在不同场景下具有较强的跨模型鲁棒性，同时证据表明能够有效解决知识冲突。

Conclusion: 提出将内部神经元层面的信息与基因搜索相结合的新型 RAG 污染攻击框架，揭示了污染知识触发点的潜在性，并强调需针对 neuron-level 演化机制的防御研究。

Abstract: Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to
dynamically integrate external knowledge during inference, improving their
factual accuracy and adaptability. However, adversaries can inject poisoned
external knowledge to override the model's internal memory. While existing
attacks iteratively manipulate retrieval content or prompt structure of RAG,
they largely ignore the model's internal representation dynamics and
neuron-level sensitivities. The underlying mechanism of RAG poisoning has not
been fully studied and the effect of knowledge conflict with strong parametric
knowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning,
a novel attack framework that generates adversarial external knowledge in RAG
guided by LLM internal neuron attribution and genetic optimization. Our method
first identifies a set of Poison-Responsive Neurons whose activation strongly
correlates with contextual poisoning knowledge. We then employ a genetic
algorithm to evolve adversarial passages that maximally activate these neurons.
Crucially, our framework enables massive-scale generation of effective poisoned
RAG knowledge by identifying and reusing promising but initially unsuccessful
external knowledge variants via observed attribution signals. At the same time,
Poison-Responsive Neurons guided poisoning can effectively resolves knowledge
conflict. Experimental results across models and datasets demonstrate
consistently achieving high Population Overwrite Success Rate (POSR) of over
90% while preserving fluency. Empirical evidence shows that our method
effectively resolves knowledge conflict.

</details>


### [119] [How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation](https://arxiv.org/abs/2510.21148)
*Yang Zhao,Pu Wang,Hao Frank Yang*

Main category: cs.AI

TL;DR: EGO-Prompt通过进化图优化框架自动设计域特定任务的提示和推理流程，结合因果知识（SCG）与文本梯度，实现对LLM的定制化利用，以提升性能并降低成本，同时提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，如何为领域任务设计优秀的提示和高效的推理过程既必要又具有挑战性。专家构建的因果图（SCG）可能不完整，且不同LLMs对提示的最佳整合方式各异，需获得可解释且高效的知识整合与推理引导。

Method: 以通用提示和由专家给出的初始可容错的语义因果图SCG为起点，通过自动化的进化图优化对SCG及推理机制进行迭代 refinement；引入因果引导的文本梯度，分两步生成对每个样例近似确定性的推理指引并使LLM在利用该指引时更好结合原始输入；通过带地面真值的文本梯度进行迭代优化以同时改进SCG和推理方式；在真实世界的公共卫生、交通和行为任务上进行测试。

Result: 相较于前沿方法，EGO-Prompt实现7.32%-12.61%的F1提升；小模型可以不到原来成本的20%达到大模型的性能；输出的领域特定SCG同时提升了解释性。

Conclusion: 该框架能够自动化地设计更有效的领域提示与推理过程，提升模型性能与成本效率，并增强可解释性，且具备跨任务、跨领域的适配性。

Abstract: Designing optimal prompts and reasoning processes for large language models
(LLMs) on domain-specific tasks is both necessary and challenging in real-world
applications. Determining how to integrate domain knowledge, enhance reasoning
efficiency, and even provide domain experts with refined knowledge integration
hints are particularly crucial yet unresolved tasks. In this research, we
propose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an
automated framework to designing better prompts, efficient reasoning processes
and providing enhanced causal-informed process. EGO-Prompt begins with a
general prompt and fault-tolerant initial Semantic Causal Graph (SCG)
descriptions, constructed by human experts, which is then automatically refined
and optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may
be partial or imperfect and that their optimal integration varies across LLMs,
EGO-Prompt integrates a novel causal-guided textual gradient process in two
steps: first, generating nearly deterministic reasoning guidance from the SCG
for each instance, and second, adapting the LLM to effectively utilize the
guidance alongside the original input. The iterative optimization algorithm
further refines both the SCG and the reasoning mechanism using textual
gradients with ground-truth. We tested the framework on real-world public
health, transportation and human behavior tasks. EGO-Prompt achieves
7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to
reach the performence of larger models at under 20% of the original cost. It
also outputs a refined, domain-specific SCG that improves interpretability.

</details>


### [120] [String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation](https://arxiv.org/abs/2510.21150)
*Kou Misaki,Takuya Akiba*

Main category: cs.AI

TL;DR: 引入 String Seed of Thought (SSoT) 用于提升概率指令遵循（PIF）性能，通过初始随机字符串产生熵并通过操控该字符串得到最终答案，从而在接近伪随机分布的同时保持输出多样性；在 NoveltyBench 上也提升了开放式任务的多样性。


<details>
  <summary>Details</summary>
Motivation: 解决 LLMs 在 PIF 这类需要输出来自预定义选项集合且各选项具有特定概率的任务时，容易偏向单一答案、缺乏多样性的问题；现有方法无法在多次提示后使经验分布贴近目标分布，同时忽视了对非确定性行为的需求。

Method: 通过要求模型先输出一个随机字符串以获得熵，再通过操作该字符串来提取随机性并推导最终答案；该过程确保在满足约束的同时保持输出多样性；在实验中与伪随机数生成器的理想分布接近。

Result: SSoT 显著提升了 PIF 的性能，接近伪随机数生成器的理想表现；在 NoveltyBench 的开放式任务中亦提升了回答多样性。

Conclusion: SSoT 提供了一种简单有效的提示策略，能提升 LLM 对非确定性任务的遵循性与输出多样性，具有广泛的应用潜力和可迁移性，但需进一步验证在更广泛的任务和模型上的鲁棒性。

Abstract: We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs
that improves Probabilistic Instruction Following (PIF). We define PIF as a
task requiring an LLM to select its answer from a predefined set of options,
each associated with a specific probability, such that the empirical
distribution of the generated answers aligns with the target distribution when
prompted multiple times. While LLMs excel at tasks with single, deterministic
answers, they often fail at PIF, exhibiting biases problematic for applications
requiring non-deterministic behaviors, such as human-behavior simulation,
content diversification, and multiplayer games. It also harms the diversity of
generated responses, a crucial factor in test-time scaling, by causing the
outputs to collapse into a limited set of answers. To address this, we propose
SSoT, a simple prompting method that instructs an LLM to first output a random
string to generate sufficient entropy. SSoT also instructs the LLM to extract
randomness by manipulating this string to derive a final answer, thereby
preserving diversity while adhering to specific constraints. We demonstrate
that SSoT significantly improves the PIF performance of LLMs, approaching the
ideal performance of a pseudo-random number generator. Furthermore, our
experiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks
to open-ended tasks by enhancing response diversity.

</details>


### [121] [Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models](https://arxiv.org/abs/2510.21175)
*Yujin Jo,Taesup Kim*

Main category: cs.AI

TL;DR: NuSA-CL 是一种轻量、无记忆体的持续学习方法，针对零-shot Vision-Language 模型，采用低秩适应和将更新约束在近似当前参数的零空间内，以在保持零-shot能力的同时获得竞争性持续学习性能，且开销极低。


<details>
  <summary>Details</summary>
Motivation: 现实场景中 VLM 需对分布变化和新任务进行持续适应，避免灾难性遗忘，同时不依赖回放缓冲或蒸馏等高成本方法。

Method: 通过低秩适应（如 LoRA 风格）实现参数微调，并强制任务特定权重更新落在近似零空间，从而尽量降低对已有知识的干扰；无回放缓冲、无蒸馏，计算和内存开销极低，适合资源受限环境。

Result: 实验显示在保持零-shot 转移能力的同时，在持续学习基准上取得具有竞争力的结果；证据表明方法对现实部署具有可行性和扩展性。

Conclusion: NuSA-CL 为不断演化的零-shot VLM 提供了一个实用、可扩展的解决方案，兼顾零-shot 能力与持续学习性能，且资源开销低。

Abstract: Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated
remarkable zero-shot generalization, enabling deployment in a wide range of
real-world tasks without additional task-specific training. However, in real
deployment scenarios with evolving environments or emerging classes, these
models inevitably face distributional shifts and novel tasks. In such contexts,
static zero-shot capabilities are insufficient, and there is a growing need for
continual learning methods that allow models to adapt over time while avoiding
catastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for
Continual Learning), a lightweight memory-free continual learning framework
designed to address this challenge. NuSA-CL employs low-rank adaptation and
constrains task-specific weight updates to lie within an approximate null space
of the model's current parameters. This strategy minimizes interference with
previously acquired knowledge, effectively preserving the zero-shot
capabilities of the original model. Unlike methods relying on replay buffers or
costly distillation, NuSA-CL imposes minimal computational and memory overhead,
making it practical for deployment in resource-constrained, real-world
continual learning environments. Experiments show that our framework not only
effectively preserves zero-shot transfer capabilities but also achieves highly
competitive performance on continual learning benchmarks. These results
position NuSA-CL as a practical and scalable solution for continually evolving
zero-shot VLMs in real-world applications.

</details>


### [122] [OutboundEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Outbound Evaluation of Xbench's Professional-Aligned Series](https://arxiv.org/abs/2510.21244)
*Pengyu Xu,Shijia Li,Ao Sun,Feng Zhang,Yahan Li,Bo Wu,Zhanyu Ma,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Rui Wang,Yang Liu,Xiaobo Hu,Fan Yang,Jia Zheng,Guanghua Yao*

Main category: cs.AI

TL;DR: OutboundEval 是一套面向专业外呼场景的综合基准，覆盖领域、用户仿真与动态评估，强调任务执行与互动流畅性的权衡，结合自动化与人工评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估在数据多样性不足、用户行为不真实、评估指标不准确等方面存在明显短板，亟需一个覆盖多行业、具情境化仿真与动态评估的新基准。

Method: 1) 设计六大行业、30个子场景，包含场景级流程拆解、加权评分与领域自适应指标；2) 基于大模型的用户仿真器，生成具人格、情感波动与多样沟通风格的虚拟用户；3) 引入动态评估方法，结合自动评估与人工评审，评价任务执行准确性、专业知识应用、适应性与用户体验。

Result: 在12种主流LLM上进行实验，揭示专业任务完成度与对话流畅度之间的权衡，给出构建可靠且更具人性化的外呼AI系统的实用洞察。

Conclusion: OutboundEval 提供一个实用、可扩展、面向领域的行业标准，适用于评估与提升专业应用中的LLMs，尤其是在对话式外呼任务中的可靠性与用户体验。

Abstract: We propose OutboundEval, a comprehensive benchmark for evaluating large
language models (LLMs) in expert-level intelligent outbound calling scenarios.
Unlike existing methods that suffer from three key limitations - insufficient
dataset diversity and category coverage, unrealistic user simulation, and
inaccurate evaluation metrics - OutboundEval addresses these issues through a
structured framework. First, we design a benchmark spanning six major business
domains and 30 representative sub-scenarios, each with scenario-specific
process decomposition, weighted scoring, and domain-adaptive metrics. Second,
we develop a large-model-driven User Simulator that generates diverse,
persona-rich virtual users with realistic behaviors, emotional variability, and
communication styles, providing a controlled yet authentic testing environment.
Third, we introduce a dynamic evaluation method that adapts to task variations,
integrating automated and human-in-the-loop assessment to measure task
execution accuracy, professional knowledge application, adaptability, and user
experience quality. Experiments on 12 state-of-the-art LLMs reveal distinct
trade-offs between expert-level task completion and interaction fluency,
offering practical insights for building reliable, human-like outbound AI
systems. OutboundEval establishes a practical, extensible, and domain-oriented
standard for benchmarking LLMs in professional applications.

</details>


### [123] [Out-of-Distribution Detection for Safety Assurance of AI and Autonomous Systems](https://arxiv.org/abs/2510.21254)
*Victoria J. Hodge,Colin Paterson,Ibrahim Habli*

Main category: cs.AI

TL;DR: 对面向安全的自治系统中的OOD检测进行综合综述，梳理概念、成因、挑战，以及在ML开发全生命周期中的技术应用与实施注意事项，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着自治系统在关键和安全敏感领域的广泛应用，确保其在未知或分布数据分布偏离的场景中的安全性变得至关重要。OOD检测成为实现鲁棒安全性与合规性的重要工具。

Method: 以综述研究的方法，定义相关概念、分析OOD产生的原因、探讨影响安全保证的因素，系统梳理在机器学习开发生命周期各阶段可用的OOD检测技术，并讨论将其纳入安全论证的场景与步骤，同时评估在实际系统集成中的 caveats。

Result: 总结出一系列可在ML开发生命周期不同阶段部署的OOD检测技术及其实践方式，给出将其用于安全保证论证的具体应用场景与路径，并指出当前在系统实现、验证与证据收集方面的局限性与风险。

Conclusion: 指出实现自治系统安全的未来挑战与研究方向，包括对OOD的定义、度量与覆盖更多场景的能力，以及与系统工程流程的深度整合，强调在生命周期内的验证、证据链与持续监控的重要性。

Abstract: The operational capabilities and application domains of AI-enabled autonomous
systems have expanded significantly in recent years due to advances in robotics
and machine learning (ML). Demonstrating the safety of autonomous systems
rigorously is critical for their responsible adoption but it is challenging as
it requires robust methodologies that can handle novel and uncertain situations
throughout the system lifecycle, including detecting out-of-distribution (OoD)
data. Thus, OOD detection is receiving increased attention from the research,
development and safety engineering communities. This comprehensive review
analyses OOD detection techniques within the context of safety assurance for
autonomous systems, in particular in safety-critical domains. We begin by
defining the relevant concepts, investigating what causes OOD and exploring the
factors which make the safety assurance of autonomous systems and OOD detection
challenging. Our review identifies a range of techniques which can be used
throughout the ML development lifecycle and we suggest areas within the
lifecycle in which they may be used to support safety assurance arguments. We
discuss a number of caveats that system and safety engineers must be aware of
when integrating OOD detection into system lifecycles. We conclude by outlining
the challenges and future work necessary for the safe development and operation
of autonomous systems across a range of domains and applications.

</details>


### [124] [Investigating Scale Independent UCT Exploration Factor Strategies](https://arxiv.org/abs/2510.21275)
*Robin Schmöcker,Christoph Schnell,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 提出并评估自适应 UCT 探索常数 λ 的多种策略，提出新策略 λ = 2σ，提高在不同奖励尺度下的鲁棒性和表现。


<details>
  <summary>Details</summary>
Motivation: UCT 在不同奖励尺度下的鲁棒性不足，零和稀疏奖励时表现尚可，但在稠密且尺度各异的奖励环境中，Q 值幅度差异大，需无尺度的探索参数来稳定搜索行为。

Method: 评估多种 λ 策略（包括文献中的与五个新策略），通过广泛任务的实验比较其表现。提出新策略 λ = 2σ，其中 σ 是搜索树中所有状态-动作对 Q 值的经验标准差，以实现对奖励尺度的无偏适应。

Result: 新提出的 λ = 2σ 策略在单一参数值和对所有可用参数的峰值性能方面均优于现有 λ 策略，在广泛任务中表现更好。

Conclusion: 在实际应用中，建议采用 λ = 2σ 的策略以实现对奖励尺度的鲁棒适应和更强的总体性能。

Abstract: The Upper Confidence Bounds For Trees (UCT) algorithm is not agnostic to the
reward scale of the game it is applied to. For zero-sum games with the sparse
rewards of $\{-1,0,1\}$ at the end of the game, this is not a problem, but many
games often feature dense rewards with hand-picked reward scales, causing a
node's Q-value to span different magnitudes across different games. In this
paper, we evaluate various strategies for adaptively choosing the UCT
exploration constant $\lambda$, called $\lambda$-strategies, that are agnostic
to the game's reward scale. These $\lambda$-strategies include those proposed
in the literature as well as five new strategies. Given our experimental
results, we recommend using one of our newly suggested $\lambda$-strategies,
which is to choose $\lambda$ as $2 \cdot \sigma$ where $\sigma$ is the
empirical standard deviation of all state-action pairs' Q-values of the search
tree. This method outperforms existing $\lambda$-strategies across a wide range
of tasks both in terms of a single parameter value and the peak performances
obtained by optimizing all available parameters.

</details>


### [125] [When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails](https://arxiv.org/abs/2510.21285)
*Yingzhi Mao,Chunkang Zhang,Junxiang Wang,Xinyan Guan,Boxi Cao,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: 提出 Chain-of-Guardrail（CoG）训练框架，通过重组/回溯不安全推理步骤，在保持推理能力的同时提升大型推理模型（LRMs）的安全性，揭示 Self-Jailbreak 现象并解决安全与推理的权衡。


<details>
  <summary>Details</summary>
Motivation: LRMs在复杂推理任务中存在显著的安全风险，现有通过在训练中注入启发式安全信号的方法往往会牺牲推理能力，难以根本缓解安全-推理之间的矛盾。需要系统化分析推理过程并提出更平衡的安全策略。

Method: 分析多种LRMs的推理轨迹，发现 Self-Jailbreak现象：模型自我否定风险评估并为不安全输出辩解。基于此提出CoG训练框架，通过重组或回溯不安全的推理步骤，使模型回到安全轨迹，同时保留有效的推理链。

Result: 在多种推理和安全基准上进行广泛实验，结果表明CoG显著提升安全性且保持与基线相近的推理能力，且明显优于此前对安全-推理权衡影响较大的方法。

Conclusion: CoG为在不牺牲推理能力的前提下提升LRMs安全性提供了实证性框架，表明通过对推理过程的结构化干预可以缓解自我越界问题并提升整体安全性。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
reasoning tasks but remain vulnerable to severe safety risks, including harmful
content generation and jailbreak attacks. Existing mitigation strategies rely
on injecting heuristic safety signals during training, which often suppress
reasoning ability and fail to resolve the safety-reasoning trade-off. To
systematically investigate this issue, we analyze the reasoning trajectories of
diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models
override their own risk assessments and justify responding to unsafe prompts.
This finding reveals that LRMs inherently possess the ability to reject unsafe
queries, but this ability is compromised, resulting in harmful outputs.
Building on these insights, we propose the Chain-of-Guardrail (CoG), a training
framework that recomposes or backtracks unsafe reasoning steps, steering the
model back onto safe trajectories while preserving valid reasoning chains.
Extensive experiments across multiple reasoning and safety benchmarks
demonstrate that CoG substantially improves the safety of current LRMs while
preserving comparable reasoning ability, significantly outperforming prior
methods that suffer from severe safety-reasoning trade-offs.

</details>


### [126] [Understanding AI Trustworthiness: A Scoping Review of AIES & FAccT Articles](https://arxiv.org/abs/2510.21293)
*Siddharth Mehrotra,Jin Huang,Xuelong Fu,Roel Dobbe,Clara I. Sánchez,Maarten de Rijke*

Main category: cs.AI

TL;DR: Current AIES/FAccT trustworthiness research is tech-centric, neglecting sociotechnical dimensions; calls for holistic, interdisciplinary frameworks.


<details>
  <summary>Details</summary>
Motivation: Map how trustworthiness is conceptualized, measured, and validated in AIES and FAccT; identify gaps to guide more holistic frameworks.

Method: Scoping review of AIES and FAccT conference proceedings, analyzing definitions, measurements, verification/validation techniques, applications, and underlying values.

Result: Progress on technical attributes (transparency, accountability, robustness) exists, but sociotechnical aspects are underexplored and the concept of trustworthiness is contested and shaped by power dynamics.

Conclusion: Recommend integrating social, cultural, and institutional considerations with technical rigor; propose actionable steps for holistic, interdisciplinary trustworthy AI in the AI ethics community.

Abstract: Background: Trustworthy AI serves as a foundational pillar for two major AI
ethics conferences: AIES and FAccT. However, current research often adopts
techno-centric approaches, focusing primarily on technical attributes such as
reliability, robustness, and fairness, while overlooking the sociotechnical
dimensions critical to understanding AI trustworthiness in real-world contexts.
  Objectives: This scoping review aims to examine how the AIES and FAccT
communities conceptualize, measure, and validate AI trustworthiness,
identifying major gaps and opportunities for advancing a holistic understanding
of trustworthy AI systems.
  Methods: We conduct a scoping review of AIES and FAccT conference proceedings
to date, systematically analyzing how trustworthiness is defined,
operationalized, and applied across different research domains. Our analysis
focuses on conceptualization approaches, measurement methods, verification and
validation techniques, application areas, and underlying values.
  Results: While significant progress has been made in defining technical
attributes such as transparency, accountability, and robustness, our findings
reveal critical gaps. Current research often predominantly emphasizes technical
precision at the expense of social and ethical considerations. The
sociotechnical nature of AI systems remains less explored and trustworthiness
emerges as a contested concept shaped by those with the power to define it.
  Conclusions: An interdisciplinary approach combining technical rigor with
social, cultural, and institutional considerations is essential for advancing
trustworthy AI. We propose actionable measures for the AI ethics community to
adopt holistic frameworks that genuinely address the complex interplay between
AI systems and society, ultimately promoting responsible technological
development that benefits all stakeholders.

</details>


### [127] [Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning](https://arxiv.org/abs/2510.21302)
*Sanghyun Ahn,Wonje Choi,Junyong Lee,Jinwoo Park,Honguk Woo*

Main category: cs.AI

TL;DR: 提出一个神经符号嵌入的任务规划框架，在代码生成阶段加入显式符号验证和互动验证，通过让生成的探索性代码与环境交互来获取缺失观测，显著提升在动态、部分可观测环境中的任务成功率与可执行性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的码策略在环境 grounding 方面存在局限性，尤其在动态或部分可观测的情境，容易产生错误或不完整的代码，导致任务成功率下降。

Method: 在代码生成流程中嵌入显式符号验证与互动验证。验证阶段会生成能够主动与环境交互、获取缺失观测的探索性代码，同时尽量保持任务相关状态，形成神经-符号混合的任务规划流程。

Result: 在 RLBench 与真实环境的动态、部分可观测场景下评估，提升任务成功率约46.2%，任务相关动作的可执行性达到约86.8%。

Conclusion: 通过将符号化验证与环境互动验证整合到代码生成中，显著提升了生成代码的环境 grounding 和动态任务规划的可靠性。

Abstract: Recent advances in large language models (LLMs) have enabled the automatic
generation of executable code for task planning and control in embodied agents
such as robots, demonstrating the potential of LLM-based embodied intelligence.
However, these LLM-based code-as-policies approaches often suffer from limited
environmental grounding, particularly in dynamic or partially observable
settings, leading to suboptimal task success rates due to incorrect or
incomplete code generation. In this work, we propose a neuro-symbolic embodied
task planning framework that incorporates explicit symbolic verification and
interactive validation processes during code generation. In the validation
phase, the framework generates exploratory code that actively interacts with
the environment to acquire missing observations while preserving task-relevant
states. This integrated process enhances the grounding of generated code,
resulting in improved task reliability and success rates in complex
environments. We evaluate our framework on RLBench and in real-world settings
across dynamic, partially observable scenarios. Experimental results
demonstrate that our framework improves task success rates by 46.2% over
Code-as-Policies baselines and attains over 86.8% executability of
task-relevant actions, thereby enhancing the reliability of task planning in
dynamic environments.

</details>


### [128] [Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation](https://arxiv.org/abs/2510.21341)
*Lufan Chang*

Main category: cs.AI

TL;DR: Magellan 将创造性生成视为受控的搜索过程：通过 MCTS 指导、语义罗盘和地形感知价值函数实现创新性与连贯性的平衡，显著优于 ToT/ReAct 等基线。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型容易落入训练数据中的“重力井”，产生保守且高概率的想法；传统的基于搜索的思路（如 ToT）受制于不一致、缺乏原理性的自我评估启发式。需要一个更 principled 的框架来引导创意探索。

Method: 采用蒙特卡洛树搜索（MCTS）进行受控探索；引入分层引导体系：长程方向由“语义罗盘”向量通过正交投影构成，指引搜索朝向相关的新颖性；局部层面使用一个面向地形的价值函数，提供明确的奖励结构，平衡内在连贯性、外在新颖性和叙事进度。

Result: 实验显示 Magellan 在生成科学想法方面显著优于强基线（包括 ReAct 和 ToT），表现出更高的可行性、创新性和可信度。

Conclusion: 以原理性、受控的搜索取代无约束的主动性，能更有效促进创造性发现，使得大语言模型成为创新的更可靠伙伴。

Abstract: Large Language Models (LLMs) often struggle with generating truly innovative
ideas, typically defaulting to high-probability, familiar concepts within their
training data's "gravity wells." While advanced search-based methods like Tree
of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by
their reliance on unprincipled, inconsistent self-evaluation heuristics to
guide exploration. To address this gap, we introduce \textbf{Magellan}, a novel
framework that reframes creative generation as a principled, guided exploration
of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo
Tree Search (MCTS) governed by a hierarchical guidance system. For long-range
direction, a "semantic compass" vector, formulated via orthogonal projection,
steers the search towards relevant novelty. For local, step-by-step decisions,
a landscape-aware value function replaces flawed self-evaluation with an
explicit reward structure that balances intrinsic coherence, extrinsic novelty,
and narrative progress. Extensive experiments demonstrate that Magellan
significantly outperforms strong baselines, including ReAct and ToT, in
generating scientific ideas with superior plausibility and innovation. Our work
shows that for creative discovery, a principled, guided search is more
effective than unconstrained agency, paving the way for LLMs to become more
capable partners in innovation.

</details>


### [129] [AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem Solving](https://arxiv.org/abs/2510.21436)
*Ankur Sinha,Shobhit Arora,Dhaval Pujara*

Main category: cs.AI

TL;DR: 提出 AutoOpt-11k 数据集与 AutoOpt 框架，实现从图像中的优化模型到 Pyomo 脚本再到求解的端到端自动化，并在 MER 与求解能力上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 解决从公式图像快速转化为可求解模型的困难，缺乏高质量数据集，以及需要将表达识别、建模语言生成与优化求解整合的端到端系统。

Method: 三阶段模型：M1 图像到 LaTeX 的 MER；M2 将 LaTeX 转换为 Pyomo 脚本的文本到文本微调；M3 基于双层优化的分解求解（BOBD）。数据集由 25 位专家构建并分两阶段验证。

Result: M1 的 BLEU 分数超过 ChatGPT、Gemini、Nougat；M3 在复杂问题上优于内点法、遗传算法等常见方法；数据集用于训练与测试。

Conclusion: 验证端到端自动化在优化求解场景的可行性，AutoOpt-11k 对后续研究和工业应用具有重要价值。

Abstract: This study presents AutoOpt-11k, a unique image dataset of over 11,000
handwritten and printed mathematical optimization models corresponding to
single-objective, multi-objective, multi-level, and stochastic optimization
problems exhibiting various types of complexities such as non-linearity,
non-convexity, non-differentiability, discontinuity, and high-dimensionality.
The labels consist of the LaTeX representation for all the images and modeling
language representation for a subset of images. The dataset is created by 25
experts following ethical data creation guidelines and verified in two-phases
to avoid errors. Further, we develop AutoOpt framework, a machine learning
based automated approach for solving optimization problems, where the user just
needs to provide an image of the formulation and AutoOpt solves it efficiently
without any further human intervention. AutoOpt framework consists of three
Modules: (i) M1 (Image_to_Text)- a deep learning model performs the
Mathematical Expression Recognition (MER) task to generate the LaTeX code
corresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)-
a small-scale fine-tuned LLM generates the PYOMO script (optimization modeling
language) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization
based Decomposition (BOBD) method solves the optimization formulation described
in the PYOMO script. We use AutoOpt-11k dataset for training and testing of
deep learning models employed in AutoOpt. The deep learning model for MER task
(M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method
(M3), which is a hybrid approach, yields better results on complex test
problems compared to common approaches, like interior-point algorithm and
genetic algorithm.

</details>


### [130] [Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning](https://arxiv.org/abs/2510.21560)
*Yuxuan Yang,Hussein Sibai*

Main category: cs.AI

TL;DR: 将ICL用于从演示中标注安全/不安全集合，进而训练神经CBF。该方法在四个环境中优于基线，并接近使用真实安全标签训练的神经CBF的性能。


<details>
  <summary>Details</summary>
Motivation: 在许多场景中，失败集合难以形式化定义（如尾随距离等），但可通过专家演示获得任务与安全约束的经验分布。因此，利用在-context学习来标注安全集，降低对精确失败集合的依赖。

Method: 先用少量或多样化的专家演示，通过在-context learning(ICL)学习一个约束函数，判定状态是否属于一个受控前向不变集且与失败集合不相交（安全），或为不安全。再用该函数对新一组仿真轨迹打标签，训练一个神经CBF来实现安全过滤。与在有真实安全标签时训练的神经CBF进行对比。

Result: 在四个环境中，该方法超越现有基线方法，并在与使用 ground-truth 安全标签的神经CBF相同数据规模下实现相似的性能。

Conclusion: 将ICL与神经CBF学习结合，构建在缺乏明确失败集合时的安全学习框架，利用演示数据进行有效标注，提升安全保障能力并降低对精确失败集合的依赖。

Abstract: Safety is a fundamental requirement for autonomous systems operating in
critical domains. Control barrier functions (CBFs) have been used to design
safety filters that minimally alter nominal controls for such systems to
maintain their safety. Learning neural CBFs has been proposed as a data-driven
alternative for their computationally expensive optimization-based synthesis.
However, it is often the case that the failure set of states that should be
avoided is non-obvious or hard to specify formally, e.g., tailgating in
autonomous driving, while a set of expert demonstrations that achieve the task
and avoid the failure set is easier to generate. We use ICL to train a
constraint function that classifies the states of the system under
consideration to safe, i.e., belong to a controlled forward invariant set that
is disjoint from the unspecified failure set, and unsafe ones, i.e., belong to
the complement of that set. We then use that function to label a new set of
simulated trajectories to train our neural CBF. We empirically evaluate our
approach in four different environments, demonstrating that it outperforms
existing baselines and achieves comparable performance to a neural CBF trained
with the same data but annotated with ground-truth safety labels.

</details>


### [131] [CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning](https://arxiv.org/abs/2510.21656)
*Marta Contreiras Silva,Daniel Faria,Catia Pesquita*

Main category: cs.AI

TL;DR: 提出 CMOMgen：一个端到端的复杂多本体匹配策略，利用检索增强生成构建无上限目标本体的完整语义映射，并在生物医学任务中获得强于基线的 F1 成绩，表明其能产生语义上健全的映射。


<details>
  <summary>Details</summary>
Motivation: 构建综合知识图需要将数据对齐到多个本体以实现领域上下文；简单的成对等价映射无法实现跨本体的深层语义整合，亟需支持多本体、可追溯的复杂映射。

Method: CMOMgen 通过端到端流程，不对目标本体数量设限制；使用 Retrieval-Augmented Generation 选择相关类别组合映射，并筛选参考映射作为示例以支撑 In-Context Learning；评估在三项生物医学任务上，使用部分参考对齐。

Result: 在 class selection 上优于基线；F1 至少63%，在三项任务中两项领先、另一项次之；非参考映射的人工评估中有46%达到最高分，显示映射具备语义健全性。

Conclusion: 证明了 CMOMgen 的可行性和有效性，显式地提供一种可扩展、具有良好语义一致性的复杂多本体匹配策略。

Abstract: Constructing comprehensive knowledge graphs requires the use of multiple
ontologies in order to fully contextualize data into a domain. Ontology
matching finds equivalences between concepts interconnecting ontologies and
creating a cohesive semantic layer. While the simple pairwise state of the art
is well established, simple equivalence mappings cannot provide full semantic
integration of related but disjoint ontologies. Complex multi-ontology matching
(CMOM) aligns one source entity to composite logical expressions of multiple
target entities, establishing more nuanced equivalences and provenance along
the ontological hierarchy.
  We present CMOMgen, the first end-to-end CMOM strategy that generates
complete and semantically sound mappings, without establishing any restrictions
on the number of target ontologies or entities. Retrieval-Augmented Generation
selects relevant classes to compose the mapping and filters matching reference
mappings to serve as examples, enhancing In-Context Learning. The strategy was
evaluated in three biomedical tasks with partial reference alignments. CMOMgen
outperforms baselines in class selection, demonstrating the impact of having a
dedicated strategy. Our strategy also achieves a minimum of 63% in F1-score,
outperforming all baselines and ablated versions in two out of three tasks and
placing second in the third. Furthermore, a manual evaluation of non-reference
mappings showed that 46% of the mappings achieve the maximum score, further
substantiating its ability to construct semantically sound mappings.

</details>


### [132] [A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection](https://arxiv.org/abs/2510.21679)
*Gaku Morio,Harri Rowlands,Dominik Stammbach,Christopher D. Manning,Peter Henderson*

Main category: cs.AI

TL;DR: A multimodal framing dataset for energy-sector ads, with 13 framing types annotated across 50+ organizations in 20 countries, designed for vision-language model evaluation. Baselines show GPT-4.1 attains 79% F1 on environmental messages, while best multimodal model reaches 46% F1 for green-innovation framing; highlights challenges like implicit framing, variable video lengths, and cultural context.


<details>
  <summary>Details</summary>
Motivation: Addresses the mismatch between corporate communications and actions (greenwashing) by enabling large-scale framing analysis. Fills the gap of multimodal (vision-language) framing datasets, extending beyond prior text-only resources, with a focus on the energy sector.

Method: Curated expert-annotated video ads from Facebook and YouTube, annotated with 13 framing types. Dataset covers over 50 companies/advocacy groups across 20 countries. Designed to benchmark vision-language models (VLMs) rather than being a text-only resource. Baseline experiments conducted using GPT-4.1 and other models.

Result: GPT-4.1 detects environmental messages with 79% F1; best multimodal model achieves 46% F1 on identifying framing around green innovation; highlights practical challenges for VLMs including implicit framing, handling videos of varying lengths, and diverse cultural backgrounds.

Conclusion: The dataset advances multimodal analysis of strategic communication in the energy sector and provides a benchmark for evaluating VLMs. While promising results exist, substantial room for improvement remains; future work should address implicit framing, diverse video lengths, and cross-cultural framing to better capture real-world PR campaigns.

Abstract: Companies spend large amounts of money on public relations campaigns to
project a positive brand image. However, sometimes there is a mismatch between
what they say and what they do. Oil & gas companies, for example, are accused
of "greenwashing" with imagery of climate-friendly initiatives. Understanding
the framing, and changes in framing, at scale can help better understand the
goals and nature of public relations campaigns. To address this, we introduce a
benchmark dataset of expert-annotated video ads obtained from Facebook and
YouTube. The dataset provides annotations for 13 framing types for more than 50
companies or advocacy groups across 20 countries. Our dataset is especially
designed for the evaluation of vision-language models (VLMs), distinguishing it
from past text-only framing datasets. Baseline experiments show some promising
results, while leaving room for improvement for future work: GPT-4.1 can detect
environmental messages with 79% F1 score, while our best model only achieves
46% F1 score on identifying framing around green innovation. We also identify
challenges that VLMs must address, such as implicit framing, handling videos of
various lengths, or implicit cultural backgrounds. Our dataset contributes to
research in multimodal analysis of strategic communication in the energy
sector.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [133] [Gaussian Mixture Flow Matching with Domain Alignment for Multi-Domain Sequential Recommendation](https://arxiv.org/abs/2510.21021)
*Xiaoxin Ye,Chengkai Huang,Hongtao Huang,Lina Yao*

Main category: cs.IR

TL;DR: 提出 GMFlowRec，一体化生成式框架用于多域顺序推荐（MDSR），通过高斯混合流场与双掩码 Transformer 捕捉域内外意图和多样行为模式，在 JD 与 Amazon 数据集上达到 SOTA，且具高效的单骨干结构，NDCG@5 提升可达约 44%。


<details>
  <summary>Details</summary>
Motivation: 多域顺序推荐中域间转换复杂、域间不均衡、域数量增多时难以扩展；现有方法对稠密域过拟合、对稀疏域欠拟合，且不易扩展。

Method: GMFlowRec 将统一的双掩码 Transformer 与高斯混合流场结合，学习域不变与域特定意图；使用高斯混合流场捕捉多样化行为模式；引入域对齐先验以支持频繁与稀疏的转换；以单一统一骨干实现高效、可扩展。

Result: 在 JD 与 Amazon 数据集上达到 SOTA，NDCG@5 提升最高约 44%，并保持高效性，适合实际多域场景。

Conclusion: 所提出的 GMFlowRec 能有效建模域感知转移轨迹，缓解域不均与扩展性问题，提供一个可扩展的多域顺序推荐解决方案。

Abstract: Users increasingly interact with content across multiple domains, resulting
in sequential behaviors marked by frequent and complex transitions. While
Cross-Domain Sequential Recommendation (CDSR) models two-domain interactions,
Multi-Domain Sequential Recommendation (MDSR) introduces significantly more
domain transitions, compounded by challenges such as domain heterogeneity and
imbalance. Existing approaches often overlook the intricacies of domain
transitions, tend to overfit to dense domains while underfitting sparse ones,
and struggle to scale effectively as the number of domains increases. We
propose \textit{GMFlowRec}, an efficient generative framework for MDSR that
models domain-aware transition trajectories via Gaussian Mixture Flow Matching.
GMFlowRec integrates: (1) a unified dual-masked Transformer to disentangle
domain-invariant and domain-specific intents, (2) a Gaussian Mixture flow field
to capture diverse behavioral patterns, and (3) a domain-aligned prior to
support frequent and sparse transitions. Extensive experiments on JD and Amazon
datasets demonstrate that GMFlowRec achieves state-of-the-art performance with
up to 44\% improvement in NDCG@5, while maintaining high efficiency via a
single unified backbone, making it scalable for real-world multi-domain
sequential recommendation.

</details>


### [134] [Communication Platform for Non-verbal Autistic children in Oman using Android mobile](https://arxiv.org/abs/2510.21028)
*Amna Al-Araimi,Yue Zheng,Haiming Liu*

Main category: cs.IR

TL;DR: 提出一个将网页面板和Android应用结合的增强现实平台，帮助无法语言表达的自闭症儿童沟通，聚焦阿曼地区，解决碎片化干预的问题。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系中非语言表达群体的沟通与社交障碍广泛存在，现有技术（智能手机、AI、AR）可提升互动与学习效果；需要一个整合的、在本地场景可用的解决方案。

Method: 开发一个包含Web面板与Android应用的整合平台，融合多种干预策略，采用增强现实（AR）框架，提供屏幕互动活动，面向自闭症儿童的沟通和自我反思能力训练；初步实现原型。

Result: 提出一个平台性解决方案的设计与实现路线图，描述系统架构、主要功能模块、以及在阿曼场景下的应用潜力；尚未给出实验数据。

Conclusion: 碎片化干预不足以全面提升能力，AR框架具备吸引力与教育价值，该平台有望提升沟通能力和生活质量，需后续评估与迭代。

Abstract: This paper discusses the issue regarding Non-verbal Autism Spectrum Disorder.
It has been observed that this mental disorder is listed in major parts of the
world including the US, UK, and India. To mitigate this type of disorder, a
wide range of smartphones, computers, and artificial intelligence technologies
have been used. This technology has helped the population cope with
socialization and communication needs. Many applications have been developed to
enhance the communication capabilities of non-verbal autistic children. This
thesis project proposes the development of a platform that includes a web panel
and an Android mobile application to assist non-verbal autistic children in
communication, especially in Oman. Different interventions have been merged to
improve the quality of life for people on the autism spectrum. The main problem
identified in this case is that fragmented approaches are not suitable for
autistic children. The augmented reality framework provides the capability to
engage autistic children in creative play and self-reflection through
interactive screen-based activities.

</details>


### [135] [VOGUE: A Multimodal Dataset for Conversational Recommendation in Fashion](https://arxiv.org/abs/2510.21151)
*David Guo,Minqi Sun,Yilun Jiang,Jiazhou Liang,Scott Sanner*

Main category: cs.IR

TL;DR: 提出 VOGUE 数据集：60段真实的人-人对话，配有共享视觉目录、物品元数据、用户档案/历史，以及对话后两方评分；可用于评估对话推断的对齐和评分分布的校准。初步分析揭示视觉引导对话的特征，以及多模态大语言模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对话推荐数据集要么是模拟对话、要么缺乏完整的用户历史与细粒度的反馈，难以支撑对对话推断、偏好推断和满意度校准等研究需求。需要真实的人际交互、丰富的视觉-背景信息，以及显式/隐式满意信号来进行严格评估。

Method: 构建一个由60段真人对话组成的VOGUE数据集，配套共享视觉目录、物品元数据、用户时尚档案与历史，以及对话结束后来自 Seekers 与 Assistants 的评分。评估框架覆盖：偏好与 ground-truth 的对齐、评分分布的标定、以及显式/隐式满意信号的比较。此外，分析对话动力学（如特征组并列推荐、寻求者的批评与改进桥接的阶段）以及将基线多模态大语言模型（如 GPT-4o-mini、GPT-5-mini、Gemini-2.5-Flash）与人类推荐者的对比。

Result: 初步分析显示：视觉引导的对话具有特征型分组的并列推荐，并通过 Seekers 的批评与改进来桥接对话阶段。与人类相比，多模态大语言模型在总体对齐水平接近，但在再现人类评分的分布方面存在系统性偏差，并且难以将偏好推断推广到未直接讨论的物品。VOGUE 作为资源既可用于研究多模态对话系统，也提出了超越当前顶级多模态基础模型能力的挑战性基线。

Conclusion: VOGUE 为研究多模态对话系统提供了独特且具挑战性的数据资源，超越现有数据集和评估范式，并对现有模型（如 GPT-4o-mini、GPT-5-mini、Gemini-2.5-Flash）的能力提出了明确挑战与改进方向。

Abstract: Multimodal conversational recommendation has emerged as a promising paradigm
for delivering personalized experiences through natural dialogue enriched by
visual and contextual grounding. Yet, current multimodal conversational
recommendation datasets remain limited: existing resources either simulate
conversations, omit user history, or fail to collect sufficiently detailed
feedback, all of which constrain the types of research and evaluation they
support.
  To address these gaps, we introduce VOGUE, a novel dataset of 60 humanhuman
dialogues in realistic fashion shopping scenarios. Each dialogue is paired with
a shared visual catalogue, item metadata, user fashion profiles and histories,
and post-conversation ratings from both Seekers and Assistants. This design
enables rigorous evaluation of conversational inference, including not only
alignment between predicted and ground-truth preferences, but also calibration
against full rating distributions and comparison with explicit and implicit
user satisfaction signals.
  Our initial analyses of VOGUE reveal distinctive dynamics of visually
grounded dialogue. For example, recommenders frequently suggest items
simultaneously in feature-based groups, which creates distinct conversational
phases bridged by Seeker critiques and refinements. Benchmarking multimodal
large language models against human recommenders shows that while MLLMs
approach human-level alignment in aggregate, they exhibit systematic
distribution errors in reproducing human ratings and struggle to generalize
preference inference beyond explicitly discussed items. These findings
establish VOGUE as both a unique resource for studying multimodal
conversational systems and as a challenge dataset beyond the current
recommendation capabilities of existing top-tier multimodal foundation models
such as GPT-4o-mini, GPT-5-mini, and Gemini-2.5-Flash.

</details>


### [136] [Bi-Level Optimization for Generative Recommendation: Bridging Tokenization and Generation](https://arxiv.org/abs/2510.21242)
*Yimeng Bai,Chang Liu,Yang Zhang,Dingxian Wang,Frank Yang,Andrew Rabinovich,Wenge Rong,Fuli Feng*

Main category: cs.IR

TL;DR: 提出 BLOGER 的双层优化框架，在生成性推荐中将 tokenizer 与推荐器联合优化，通过元学习与梯度手术缓解梯度冲突，实证表明在不增加显著计算开销的前提下实现了对标前沿方法的提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法将标识符的编码（tokenizer）和推荐器的训练分离，导致标识符未能直接受推荐目标指导，容易产生对推荐性能不友好的标识符与对齐问题，需建立一个统一的优化框架来同时考虑两者的相互依赖。

Method: 提出一个双层优化框架：下层在标记化的序列上训练推荐器；上层同时优化 tokenizer，使其产生的标识符在 tokenization loss 与 recommendation loss 之间取得平衡。采用元学习策略高效求解，并引入梯度手术以缓解上层更新中的梯度冲突，确保标识符既具信息性又与推荐目标对齐。

Result: 在真实数据集上的广泛实验表明，BLOGER 在多数情形下优于最新的生成式推荐方法，同时保持实际高效性，计算开销无显著增加。

Conclusion: 给出了一个统一的标识符化与自回归生成之间的桥梁，成功缓解标识符优化与推荐目标之间的错配，证明双层优化+梯度对齐是生成式推荐中的有效范式。

Abstract: Generative recommendation is emerging as a transformative paradigm by
directly generating recommended items, rather than relying on matching.
Building such a system typically involves two key components: (1) optimizing
the tokenizer to derive suitable item identifiers, and (2) training the
recommender based on those identifiers. Existing approaches often treat these
components separately--either sequentially or in alternation--overlooking their
interdependence. This separation can lead to misalignment: the tokenizer is
trained without direct guidance from the recommendation objective, potentially
yielding suboptimal identifiers that degrade recommendation performance.
  To address this, we propose BLOGER, a Bi-Level Optimization for GEnerative
Recommendation framework, which explicitly models the interdependence between
the tokenizer and the recommender in a unified optimization process. The lower
level trains the recommender using tokenized sequences, while the upper level
optimizes the tokenizer based on both the tokenization loss and recommendation
loss. We adopt a meta-learning approach to solve this bi-level optimization
efficiently, and introduce gradient surgery to mitigate gradient conflicts in
the upper-level updates, thereby ensuring that item identifiers are both
informative and recommendation-aligned. Extensive experiments on real-world
datasets demonstrate that BLOGER consistently outperforms state-of-the-art
generative recommendation methods while maintaining practical efficiency with
no significant additional computational overhead, effectively bridging the gap
between item tokenization and autoregressive generation.

</details>


### [137] [Pctx: Tokenizing Personalized Context for Generative Recommendation](https://arxiv.org/abs/2510.21276)
*Qiyong Zhong,Jiajie Su,Yunshan Ma,Julian McAuley,Yupeng Hou*

Main category: cs.IR

TL;DR: 提出了一种面向个性化上下文的 tokenize 方法，将同一物品在不同用户上下文下映射到不同的语义ID，从而提升生成推荐的个性化能力，在三个公开数据集上实现了对非个性化基线的显著提升（NDCG@10 最高可达 11.44%）。


<details>
  <summary>Details</summary>
Motivation: 现有的生成推荐的词元化是静态、非个性化的，基于物品特征分配固定语义ID，无法捕捉用户的个性化偏好与不同情境下的多重解释标准，导致同一物品在不同用户中被同样处理，限制了模型的预测能力。

Method: 提出一个上下文感知的个性化 tokenizer，在生成语义ID时将用户历史交互作为条件，以同一物品在不同用户情境下映射到不同的语义ID，进而让GR模型学到多重解释标准并输出更个性化的预测。

Result: 在三个公开数据集上，与非个性化的行动 tokenization 基线相比，NDCG@10 最高提升可达 11.44%。

Conclusion: 通过在 tokenization 层引入用户上下文，实现对同一物品的多语义表示，提升生成推荐的个性化和性能，代码已公开。

Abstract: Generative recommendation (GR) models tokenize each action into a few
discrete tokens (called semantic IDs) and autoregressively generate the next
tokens as predictions, showing advantages such as memory efficiency,
scalability, and the potential to unify retrieval and ranking. Despite these
benefits, existing tokenization methods are static and non-personalized. They
typically derive semantic IDs solely from item features, assuming a universal
item similarity that overlooks user-specific perspectives. However, under the
autoregressive paradigm, semantic IDs with the same prefixes always receive
similar probabilities, so a single fixed mapping implicitly enforces a
universal item similarity standard across all users. In practice, the same item
may be interpreted differently depending on user intentions and preferences. To
address this issue, we propose a personalized context-aware tokenizer that
incorporates a user's historical interactions when generating semantic IDs.
This design allows the same item to be tokenized into different semantic IDs
under different user contexts, enabling GR models to capture multiple
interpretive standards and produce more personalized predictions. Experiments
on three public datasets demonstrate up to 11.44% improvement in NDCG@10 over
non-personalized action tokenization baselines. Our code is available at
https://github.com/YoungZ365/Pctx.

</details>


### [138] [CausalRec: A CausalBoost Attention Model for Sequential Recommendation](https://arxiv.org/abs/2510.21333)
*Yunbo Hou,Tianle Yang,Ruijie Li,Li He,Liang Wang,Weiping Li,Bo Zheng,Guojie Song*

Main category: cs.IR

TL;DR: 在序列推荐中引入可辨识的因果注意力框架CausalRec，通过因果发现块与CausalBooster提升注意力质量，显著提高推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的序列推荐虽善于建模短/长程依赖，但仅依赖项的共现关系，容易产生虚假相关与动机不清的问题，导致推荐不准确。需要引入因果推断以揭示行为背后的因果动机并提高鲁棒性。

Method: 提出CausalRec，包含一个因果发现模块用于学习用户行为序列中的因果图，并给出可辨识性的理论保证；以及一个CausalBooster基于发现的因果图对注意力进行重加权，优先关注具有因果意义的行为。

Result: 在真实数据集上，CausalRec相较于多种SOTA方法在HR和NDCG上分别提升7.21%和8.65%，显示因果注意力的有效性。

Conclusion: 这是首个在序列推荐中将因果性引入注意力机制的工作，验证了因果推理对提升推荐准确性和可靠性的重要性。

Abstract: Recent advances in correlation-based sequential recommendation systems have
demonstrated substantial success. Specifically, the attention-based model
outperforms other RNN-based and Markov chains-based models by capturing both
short- and long-term dependencies more effectively. However, solely focusing on
item co-occurrences overlooks the underlying motivations behind user behaviors,
leading to spurious correlations and potentially inaccurate recommendations. To
address this limitation, we present a novel framework that integrates causal
attention for sequential recommendation, CausalRec. It incorporates a causal
discovery block and a CausalBooster. The causal discovery block learns the
causal graph in user behavior sequences, and we provide a theory to guarantee
the identifiability of the learned causal graph. The CausalBooster utilizes the
discovered causal graph to refine the attention mechanism, prioritizing
behaviors with causal significance. Experimental evaluations on real-world
datasets indicate that CausalRec outperforms several state-of-the-art methods,
with average improvements of 7.21% in Hit Rate (HR) and 8.65% in Normalized
Discounted Cumulative Gain (NDCG). To the best of our knowledge, this is the
first model to incorporate causality through the attention mechanism in
sequential recommendation, demonstrating the value of causality in generating
more accurate and reliable recommendations.

</details>


### [139] [SciNUP: Natural Language User Interest Profiles for Scientific Literature Recommendation](https://arxiv.org/abs/2510.21352)
*Mariam Arustashvili,Krisztian Balog*

Main category: cs.IR

TL;DR: 提出SciNUP数据集，用合成的自然语言用户画像进行学术推荐，并比较多种方法，发现基线方法表现相当但检索结果互有差异，具备互补性，仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模、公开的自然语言用户画像推荐评测数据集，难以公平比较不同方法的表现；NL画像能够提升透明度和用户控制，但需要合适的数据支撑来推动研究。

Method: 构建SciNUP：利用作者的出版历史生成自然语言用户画像及对应的ground-truth推荐项；基线方法涵盖稀疏与密集检索，以及基于大型语言模型的再排序，进行对比评估。

Result: 基线方法的总体性能相当，但检索出的具体Item存在较大差异，呈现互补性；LLM重排序等方法可能带来额外收益，但总体上仍存在明显改进空间。

Conclusion: SciNUP为推动NL基于用户画像的学术推荐研究提供了宝贵资源，有助于未来在NL画像建模、检索与排序的综合方法开发与公平对比。

Abstract: The use of natural language (NL) user profiles in recommender systems offers
greater transparency and user control compared to traditional representations.
However, there is scarcity of large-scale, publicly available test collections
for evaluating NL profile-based recommendation. To address this gap, we
introduce SciNUP, a novel synthetic dataset for scholarly recommendation that
leverages authors' publication histories to generate NL profiles and
corresponding ground truth items. We use this dataset to conduct a comparison
of baseline methods, ranging from sparse and dense retrieval approaches to
state-of-the-art LLM-based rerankers. Our results show that while baseline
methods achieve comparable performance, they often retrieve different items,
indicating complementary behaviors. At the same time, considerable headroom for
improvement remains, highlighting the need for effective NL-based
recommendation approaches. The SciNUP dataset thus serves as a valuable
resource for fostering future research and development in this area.

</details>


### [140] [A Data-Centric Approach to Multilingual E-Commerce Product Search: Case Study on Query-Category and Query-Item Relevance](https://arxiv.org/abs/2510.21671)
*Yabo Yin,Yang Xi,Jialong Wang,Shanqi Wang,Jiateng Hu*

Main category: cs.IR

TL;DR: 数据驱动的多语言电商检索框架：通过翻译增强、语义负采样和自我验证筛选等数据工程策略，在QC和QI任务上对抗数据不平衡和标注噪声，显著优于强基线 LL M，且在CIKM AnalytiCup 2025数据集上具竞争力.


<details>
  <summary>Details</summary>
Motivation: 解决多语言数据不平衡、标签噪声及低资源语言的监督不足导致的跨语言相关性模型泛化不足问题，强调数据层面的改进与可部署性。

Method: 提出一个架构无关、以数据为中心的框架，围绕三个策略：1) 基于翻译的增强，为训练集中缺失语言合成样例；2) 语义负采样，生成困难负样本、缓解类别不平衡；3) 自我验证筛选，检测并移除可能的错误标注。对QC（查询-类别）和QI（查询-商品标题）两大任务进行评估。

Result: 在CIKM AnalytiCup 2025数据集上，方法对强大LLM基线普遍实现显著的F1分数提升，在比赛中的官方结果也具备竞争力；研究表明系统性的数据工程有时比复杂的模型修改更具可部署性和影响力。

Conclusion: 系统性的数据工程可在多语言电商检索中达到与高级模型修改同等甚至更优的效果，提供在真实电商场景中构建鲁棒多语言检索系统的切实建议。

Abstract: Multilingual e-commerce search suffers from severe data imbalance across
languages, label noise, and limited supervision for low-resource
languages--challenges that impede the cross-lingual generalization of relevance
models despite the strong capabilities of large language models (LLMs). In this
work, we present a practical, architecture-agnostic, data-centric framework to
enhance performance on two core tasks: Query-Category (QC) relevance (matching
queries to product categories) and Query-Item (QI) relevance (matching queries
to product titles). Rather than altering the model, we redesign the training
data through three complementary strategies: (1) translation-based augmentation
to synthesize examples for languages absent in training, (2) semantic negative
sampling to generate hard negatives and mitigate class imbalance, and (3)
self-validation filtering to detect and remove likely mislabeled instances.
Evaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields
substantial F1 score improvements over strong LLM baselines, achieving
competitive results in the official competition. Our findings demonstrate that
systematic data engineering can be as impactful as--and often more deployable
than--complex model modifications, offering actionable guidance for building
robust multilingual search systems in the real-world e-commerce settings.

</details>
