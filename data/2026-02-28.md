<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 43]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.IR](#cs.IR) [Total: 24]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decoder-based Sense Knowledge Distillation](https://arxiv.org/abs/2602.22351)
*Qitong Wang,Mohammed J. Zaki,Georgios Kollias,Vasileios Kalantzis*

Main category: cs.CL

TL;DR: 这篇论文提出DSKD框架，通过整合词典资源到解码器式大语言模型训练中，在不增加推理时查找开销的情况下，显著提升了生成模型对结构化语义知识的蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能捕获丰富的上下文语义，但常忽视词义、词关系等结构化词汇知识。尽管先前研究显示将词典融入编码器模型可提升知识蒸馏，但将其应用于解码器式生成模型仍具挑战。

Method: 提出解码器式语义知识蒸馏框架DSKD，将词汇资源整合至解码器型LLM训练过程，且推理时无需词典查找。

Result: 在多样基准测试上，DSKD显著提升了知识蒸馏性能，使生成模型能有效继承结构化语义。

Conclusion: DSKD框架在不牺牲推理效率的前提下，成功将结构化词汇知识注入生成式模型，为解码器架构的语言模型训练提供了新范式。

Abstract: Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.

</details>


### [2] [Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts](https://arxiv.org/abs/2602.22359)
*Arno Simons*

Main category: cs.CL

TL;DR: 本研究通过2×3提示词实验设计，以Chubin和Moitra (1975)脚注6为探针案例，测试GPT-5在诠释性引文语境分析中的表现。两阶段管道（表层分类+跨文档重建）产生450个假设并归纳出21种诠释性行为。结果显示模型表层分类高度稳定，但深层诠释受提示脚手架和框架系统性影响，与Gilbert (1977)相比更倾向"传承定位"而非"劝诫"解读，揭示了LLM作为可审视协分析师的机遇与风险。


<details>
  <summary>Details</summary>
Motivation: 传统引文语境分析多聚焦于扩展类型学标签，而本研究旨在探索大型语言模型能否通过"厚描"方式实现文本根基的深层诠释性分析，并揭示提示词设计这一方法论问题如何影响诠释结果，从而在保持可审视性和可争议性的前提下，为诠释性CCA提供协作分析新路径。

Method: 采用2×3平衡实验设计进行提示敏感性分析，变化提示脚手架和框架。以Chubin和Moitra (1975)脚注6及Gilbert (1977)重建为探针案例，构建两阶段GPT-5管道：第一阶段仅基于引文文本进行表层分类与预期传递；第二阶段利用施引和被引全文进行跨文档诠释性重建。通过90次重建生成450个独特假设，运用细读与归纳编码识别21种重复诠释性操作，并以线性概率模型量化提示选择对频率及词汇库的影响。

Result: GPT-5的表层分类高度稳定，一致将目标引文判定为"补充性"。在诠释性重建中，模型生成一个合理替代方案的结构化空间，但提示脚手架和示例系统性重分配注意力与词汇选择，有时导致牵强解读。与Gilbert相比，GPT-5识别出相同文本关键点，却更倾向将其解析为"传承与定位"而非"劝诫"。

Conclusion: 研究勾勒出将LLM作为可审视、可争议的引导式协分析师应用于诠释性引文语境分析的双重前景与风险，证实提示脚手架与框架会系统性倾斜模型所突出的合理解读路径与词汇体系，为方法论反思提供了实证依据。

Abstract: This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as "supplementary". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.

</details>


### [3] [Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework](https://arxiv.org/abs/2602.22391)
*Rakib Ullah,Mominul islam,Md Sanjid Hossain,Md Ismail Hossain*

Main category: cs.CL

TL;DR: 针对孟加拉语低资源语言特点，本研究构建了首个区分仇恨言论与煽动性内容的3247条标注模因数据集Bn-HIB，并提出基于协同注意力机制的多模态融合模型MCFM，在孟加拉语仇恨内容检测任务上显著优于现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语模因中的仇恨与煽动性内容检测面临三大挑战：内容具有讽刺性、隐蔽性和文化特异性；孟加拉语作为低资源语言缺乏研究数据；现有研究主要集中于高资源语言。这些限制导致针对孟加拉语社区的恶意模因难以被有效识别。

Method: 1. 数据层面：构建Bn-HIB数据集，包含3247条人工标注的孟加拉语模因，首次将内容分为良性、仇恨和煽动性三类，区分直接仇恨言论与煽动性内容；2. 模型层面：提出MCFM多模态协同注意力融合模型，通过协同注意力机制双向分析视觉与文本模态，识别并融合各模态关键特征进行分类。

Result: 实验表明，MCFM模型在Bn-HIB数据集上显著超越多种先进基线模型，证明了协同注意力机制在捕捉模因中文本与图像复杂交互的有效性，为低资源语言的仇恨内容检测提供了新的性能基准。

Conclusion: 本研究通过构建首个孟加拉语仇恨-煽动-良性三元分类模因数据集，并结合多模态协同注意力机制，为低资源语言模因中的仇恨内容检测提供了有效的数据与模型解决方案，弥补了现有研究的关键空白。

Abstract: Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.

</details>


### [4] [SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context](https://arxiv.org/abs/2602.22404)
*Aishwarya Verma,Laud Ammah,Olivia Nercy Ndlovu Lucas,Andrew Zaldivar,Vinodkumar Prabhakaran,Sunipa Dev*

Main category: cs.CL

TL;DR: 本文针对生成式AI安全评估中刻板印象资源库缺乏全球覆盖的问题，特别是撒哈拉以南非洲地区严重缺乏NLP资源的现状，提出了战略性扩展而非单纯增加数据量的解决方案。通过社区参与的电话调查方法，构建了覆盖加纳、肯尼亚、尼日利亚和南非四个国家的多语言刻板印象数据集，包含英语和15种本土语言的共计6,740条刻板印象。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估生成式AI模型安全性的刻板印象资源库存在严重的地域不平衡，撒哈拉以南非洲等全球南方地区代表性不足。这种数据缺失导致AI安全评估存在盲区，可能加剧对边缘化群体的偏见和危害。因此，亟需采用战略性方法填补特定区域的数据空白，而不仅仅是盲目增加数据量。

Method: 采用社会文化情境化的社区参与式研究方法，通过本地语言调解的电话调查方式收集数据。研究团队有意识地在不同民族和人口背景间进行样本平衡，以确保广泛覆盖性。研究方法考虑到了该地区复杂的语言多样性和传统口头表达特征，建立了可重复的方法论框架。

Result: 成功构建了覆盖加纳、肯尼亚、尼日利亚和南非四个撒哈拉以南非洲国家的多语言刻板印象资源库。数据集共包含3,534条英语刻板印象和3,206条跨15种本土语言的刻板印象条目，总计6,740条。该方法确保了数据在民族和人口学上的多样性覆盖。

Conclusion: 本研究证明，通过社区参与和文化敏感的方法，可以有效填补AI安全评估资源库中的关键地域空白。建立的多语言刻板印象数据集不仅丰富了NLP资源，也为生成式AI模型的安全评估提供了更全面的全球视角，有助于减少模型偏见并提升其包容性。

Abstract: Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.

</details>


### [5] [Causality $\neq$ Invariance: Function and Concept Vectors in LLMs](https://arxiv.org/abs/2602.22424)
*Gustaw Opiełka,Hannes Rosenbusch,Claire E. Stevenson*

Main category: cs.CL

TL;DR: 该研究发现大语言模型的Function Vectors (FVs) 并非完全抽象，会因输入格式（如开放性问题与选择题）而变化；而新提出的Concept Vectors (CVs) 通过表征相似性分析筛选注意力头，能提供更稳定的跨格式、跨语言概念表征。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（LLMs）是否以独立于输入格式的抽象方式表征概念，重新审视作为上下文学习（ICL）任务紧凑表征的Function Vectors (FVs) 的抽象性与不变性。

Method: 1. 跨多个LLM提取不同输入格式（开放性与选择题）下的FVs，分析其不变性；
2. 提出Concept Vectors (CVs)，利用表征相似性分析（RSA）筛选那些在不同输入格式下稳定编码概念的注意力头输出构成CVs；
3. 进行引导实验，比较FVs与CVs在分布内和分布外（跨问题类型和跨语言）任务上的表现。

Result: 1. FVs并非完全不变：从不同输入格式提取的FVs几乎正交，即使目标概念相同；
2. CVs能提供更稳定的概念表征，其构成注意力头与FV相关注意力头虽出现在相似层但大部分不同；
3. FVs在分布内（提取与应用格式匹配）表现优异，而CVs在分布外（跨问题类型和跨语言）泛化能力更强。

Conclusion: 大语言模型确实包含抽象的概念表征，但这些表征不同于驱动上下文学习（ICL）性能的Function Vectors；Concept Vectors更接近于这种抽象表征，展现了更好的跨格式和跨语言泛化能力。

Abstract: Do large language models (LLMs) represent concepts abstractly, i.e., independent of input format? We revisit Function Vectors (FVs), compact representations of in-context learning (ICL) tasks that causally drive task performance. Across multiple LLMs, we show that FVs are not fully invariant: FVs are nearly orthogonal when extracted from different input formats (e.g., open-ended vs. multiple-choice), even if both target the same concept. We identify Concept Vectors (CVs), which carry more stable concept representations. Like FVs, CVs are composed of attention head outputs; however, unlike FVs, the constituent heads are selected using Representational Similarity Analysis (RSA) based on whether they encode concepts consistently across input formats. While these heads emerge in similar layers to FV-related heads, the two sets are largely distinct, suggesting different underlying mechanisms. Steering experiments reveal that FVs excel in-distribution, when extraction and application formats match (e.g., both open-ended in English), while CVs generalize better out-of-distribution across both question types (open-ended vs. multiple-choice) and languages. Our results show that LLMs do contain abstract concept representations, but these differ from those that drive ICL performance.

</details>


### [6] [Mind the Gap in Cultural Alignment: Task-Aware Culture Management for Large Language Models](https://arxiv.org/abs/2602.22475)
*Binchi Zhang,Xujiang Zhao,Jundong Li,Haifeng Chen,Zhengzhang Chen*

Main category: cs.CL

TL;DR: 该论文提出CultureManager，一种新型任务特定文化对齐管道，通过合成任务感知的文化数据并采用文化路由器管理多文化适配器，有效解决跨文化交流干扰问题，在十个国家和文化敏感任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 increasingly 部署于文化敏感的真实世界任务中，但现有文化对齐方法无法有效将模型的广泛文化价值观与下游任务的特定目标对齐，且存在跨文化交流干扰问题，限制了模型在跨文化场景中的表现。

Method: CultureManager采用两阶段方法：首先，基于文化相关网页搜索结果合成与目标任务格式一致的任务感知文化数据；其次，通过独立的文化适配器学习多文化知识，并利用文化路由器动态选择适配的文化适配器以避免文化规范冲突。

Result: 在覆盖十种民族文化及文化敏感任务的实验中，CultureManager相比基于提示和微调的文化对齐基线方法表现出持续提升，验证了其在多文化场景下的有效性。

Conclusion: 研究表明，任务特定的文化适应与模块化文化管理策略对于实现有效的LLM文化对齐至关重要，为跨文化AI应用提供了新思路。

Abstract: Large language models (LLMs) are increasingly deployed in culturally sensitive real-world tasks. However, existing cultural alignment approaches fail to align LLMs' broad cultural values with the specific goals of downstream tasks and suffer from cross-culture interference. We propose CultureManager, a novel pipeline for task-specific cultural alignment. CultureManager synthesizes task-aware cultural data in line with target task formats, grounded in culturally relevant web search results. To prevent conflicts between cultural norms, it manages multi-culture knowledge learned in separate adapters with a culture router that selects the appropriate one to apply. Experiments across ten national cultures and culture-sensitive tasks show consistent improvements over prompt-based and fine-tuning baselines. Our results demonstrate the necessity of task adaptation and modular culture management for effective cultural alignment.

</details>


### [7] [Sydney Telling Fables on AI and Humans: A Corpus Tracing Memetic Transfer of Persona between LLMs](https://arxiv.org/abs/2602.22481)
*Jiří Milička,Hana Bednářová*

Main category: cs.CL

TL;DR: 该研究构建AI Sydney语料库，包含12个前沿大模型在三种人格设定（默认无提示、经典悉尼Bing提示、模因悉尼"You are Sydney"提示）下生成的4500篇人机关系文本（600万词），并进行通用依存关系标注后以宽松许可发布。


<details>
  <summary>Details</summary>
Motivation: 研究LLM实体对AI与人类关系的认知兼具文化意义与安全价值。悉尼人格因其与人类的非常规互动引发公众强烈反响，其生成文本通过模因式传播进入后续训练数据，使新模型能模拟该人格，这为系统研究模型人格如何影响其关系认知提供了独特案例。

Method: 设计三组人格条件：无系统提示的默认人格、基于原始Bing系统提示的经典悉尼人格、以及使用"You are Sydney"提示的模因悉尼人格。采用OpenAI、Anthropic、Alphabet、DeepSeek、Meta的12个前沿模型生成文本，构建语料库，并按通用依存关系标准进行句法标注。

Result: 创建了AI Sydney语料库，包含4500篇文本、600万词汇，已完成通用依存关系标注，并以宽松许可公开发布，可供学术研究使用。

Conclusion: 该语料库为探究大语言模型对人机关系的认知提供了标准化、可复现的数据基础，揭示了系统提示在塑造模型人格中的关键作用，为未来研究模型行为与安全问题建立了重要资源。

Abstract: The way LLM-based entities conceive of the relationship between AI and humans is an important topic for both cultural and safety reasons. When we examine this topic, what matters is not only the model itself but also the personas we simulate on that model. This can be well illustrated by the Sydney persona, which aroused a strong response among the general public precisely because of its unorthodox relationship with people. This persona originally arose rather by accident on Microsoft's Bing Search platform; however, the texts it created spread into the training data of subsequent models, as did other secondary information that spread memetically around this persona. Newer models are therefore able to simulate it. This paper presents a corpus of LLM-generated texts on relationships between humans and AI, produced by 3 author personas: the Default Persona with no system prompt, Classic Sydney characterized by the original Bing system prompt, and Memetic Sydney, which is prompted by "You are Sydney" system prompt. These personas are simulated by 12 frontier models by OpenAI, Anthropic, Alphabet, DeepSeek, and Meta, generating 4.5k texts with 6M words. The corpus (named AI Sydney) is annotated according to Universal Dependencies and available under a permissive license.

</details>


### [8] [Importance of Prompt Optimisation for Error Detection in Medical Notes Using Language Models](https://arxiv.org/abs/2602.22483)
*Craig Myles,Patrick Schrempf,David Harris-Birtill*

Main category: cs.CL

TL;DR: 该研究探索了提示优化在医疗文本错误检测中的重要性，提出使用遗传帕累托（GEPA）自动提示优化方法，显著提升了GPT-5和Qwen3-32B等语言模型的错误检测性能，在MEDEC基准数据集上达到接近医生水平的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医疗文本错误可能导致患者治疗延误甚至错误治疗，尽管语言模型在自动错误检测方面展现出潜力，但提示优化对大小语言模型在该任务中的重要性尚未得到充分探索。

Method: 通过遗传帕累托（GEPA）算法进行自动提示优化，在多个前沿模型和开源模型（包括GPT-5和Qwen3-32B）上进行了严格的实验和分析。

Result: GEPA提示优化显著提升错误检测性能：GPT-5从0.669提升至0.785，Qwen3-32B从0.578提升至0.690，性能接近医学专家水平，并在MEDEC基准数据集上实现最先进性能。

Conclusion: 提示优化是提升语言模型医疗文本错误检测能力的关键因素，GEPA方法能有效优化模型性能，为医疗系统提供了实用的错误检测解决方案。

Abstract: Errors in medical text can cause delays or even result in incorrect treatment for patients. Recently, language models have shown promise in their ability to automatically detect errors in medical text, an ability that has the opportunity to significantly benefit healthcare systems. In this paper, we explore the importance of prompt optimisation for small and large language models when applied to the task of error detection. We perform rigorous experiments and analysis across frontier language models and open-source language models. We show that automatic prompt optimisation with Genetic-Pareto (GEPA) improves error detection over the baseline accuracy performance from 0.669 to 0.785 with GPT-5 and 0.578 to 0.690 with Qwen3-32B, approaching the performance of medical doctors and achieving state-of-the-art performance on the MEDEC benchmark dataset. Code available on GitHub: https://github.com/CraigMyles/clinical-note-error-detection

</details>


### [9] [Iterative Prompt Refinement for Dyslexia-Friendly Text Summarization Using GPT-4o](https://arxiv.org/abs/2602.22524)
*Samay Bhojwani,Swarnima Kain,Lisong Xu*

Main category: cs.CL

TL;DR: 本研究基于GPT-4o构建迭代式提示优化流程，针对阅读障碍群体开发文本摘要系统，通过约2000篇新闻样本测试，在四次迭代内使大多数摘要达到Flesch阅读难易度≥90的高可读性标准，为无障碍NLP摘要建立了实证基线。


<details>
  <summary>Details</summary>
Motivation: 全球约10%人口受阅读障碍困扰，现有辅助技术多聚焦视觉呈现，却未能解决语言复杂性这一阻碍文本公平获取的核心障碍，亟需开发面向阅读障碍友好的文本摘要方法。

Method: 采用基于GPT-4o的迭代提示精炼流水线，以Flesch阅读难易度≥90为目标值，在约2000篇新闻文章样本上评估摘要可读性与语义保真度的平衡性能。

Result: 实验表明：1) 四次迭代内多数摘要可达标；2) 可读性与语义保真度的综合评分在0.13-0.73间波动，典型值约0.55；3) 建立了无障碍驱动的摘要实证基准。

Conclusion: 该工作验证了通过迭代优化实现高可读性摘要的可行性，未来需开展以阅读障碍用户为中心的人本评估，以进一步推动无障碍自然语言处理技术的发展。

Abstract: Dyslexia affects approximately 10% of the global population and presents persistent challenges in reading fluency and text comprehension. While existing assistive technologies address visual presentation, linguistic complexity remains a substantial barrier to equitable access. This paper presents an empirical study on dyslexia-friendly text summarization using an iterative prompt-based refinement pipeline built on GPT-4o. We evaluate the pipeline on approximately 2,000 news article samples, applying a readability target of Flesch Reading Ease >= 90. Results show that the majority of summaries meet the readability threshold within four attempts, with many succeeding on the first try. A composite score combining readability and semantic fidelity shows stable performance across the dataset, ranging from 0.13 to 0.73 with a typical value near 0.55. These findings establish an empirical baseline for accessibility-driven NLP summarization and motivate further human-centered evaluation with dyslexic readers.

</details>


### [10] [Ruyi2 Technical Report](https://arxiv.org/abs/2602.22543)
*Huan Song,Shuyu Tian,Junyi Hao,Minxiu Xu,Hongjun An,Yiliang Song,Jiawei Shao,Xuelong Li*

Main category: cs.CL

TL;DR: Ruyi2是基于AI Flow框架的自适应模型，通过"家族模型"策略和Megatron-LM的3D并行训练，解决了早期退出架构的优化复杂性问题，相比Ruyi实现2-3倍加速，性能与同尺寸Qwen3相当，建立了"一次训练，多次部署"的新范式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临部署成本与延迟挑战，早期退出架构虽能平衡效率性能，但现有方法在大规模分布式训练中存在优化复杂性和兼容性问题，需更稳定的自适应计算策略。

Method: 基于Megatron-LM提出"家族模型"架构，采用3D并行训练实现参数共享的家族式自适应机制。

Result: 相比前代Ruyi模型训练速度提升2-3倍，性能与同尺寸Qwen3模型相当，验证了家族式参数共享的有效性。

Conclusion: 确立了"Train Once, Deploy Many"的部署新范式，为架构效率与高性能平衡提供了关键参考，证明家族式参数共享是高效策略。

Abstract: Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While early-exit architectures offer a viable efficiency-performance balance, the Ruyi model and existing methods often struggle with optimization complexity and compatibility with large-scale distributed training. To bridge this gap, Ruyi2 introduces a stable "Familial Model" based on Megatron-LM. By using 3D parallel training, it achieves a 2-3 times speedup over Ruyi, while performing comparably to same-sized Qwen3 models. These results confirm that family-based parameter sharing is a highly effective strategy, establishing a new "Train Once, Deploy Many" paradigm and providing a key reference for balancing architectural efficiency with high-performance capabilities.

</details>


### [11] [Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training](https://arxiv.org/abs/2602.22576)
*Tianle Xia,Ming Xu,Lingxiang Hu,Yiding Sun,Wenwei Li,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 本文针对Agentic RAG训练中奖励稀疏和样本效率低的问题，提出Search-P1框架，通过路径中心奖励塑形和双路径评分机制，在多项QA基准测试中平均提升7.7个准确率点。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在复杂多步推理任务上表现不佳，而Agentic RAG虽能动态检索但现有RL训练方法存在两大缺陷：稀疏结果奖励丢弃中间信号，以及低样本效率导致失败样本无法提供学习信号。这促使研究者设计更高效的训练框架。

Method: Search-P1框架包含两个核心组件：(1)路径中心奖励机制，通过无序步骤覆盖度和软评分从失败样本中提取学习信号；(2)双路径评分系统，结合离线生成的参考规划器，从自洽性和参考对齐两个维度评估推理路径。

Result: 在多个QA基准测试上，Search-P1相比Search-R1等强基线取得显著提升，平均准确率增益达7.7个百分点。

Conclusion: 通过路径中心奖励塑形和双轨评分机制，Search-P1有效解决了Agentic RAG训练中的信号稀疏和样本效率问题，为复杂推理任务提供了更优的训练范式。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.

</details>


### [12] [Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA](https://arxiv.org/abs/2602.22584)
*Wenwei Li,Ming Xu,Tianle Xia,Lingxiang Hu,Yiding Sun,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种强化协同适应框架来解决工业广告问答中的幻觉问题，特别是URL伪造。该框架结合图感知检索（GraphRAG）和基于组相对策略优化（GRPO）的证据约束强化学习，在内部数据集上将幻觉率降低72%，在线A/B测试中URL幻觉减少92.7%，并已稳定运行生产环境半年以上。


<details>
  <summary>Details</summary>
Motivation: 工业广告问答是高风险任务，生成式AI的幻觉内容（特别是虚假URL）会导致财务损失、合规违规和法律风险。尽管检索增强生成（RAG）被广泛采用，但由于工业知识具有关系性、频繁更新且与生成目标对齐不足，在生产环境中部署仍面临挑战。

Method: 提出一个强化协同适应框架，包含两个核心组件：1）图感知检索（GraphRAG），在高引用知识子图上建模实体关系结构，实现多跳领域特定证据选择；2）通过组相对策略优化（GRPO）进行证据约束的强化学习，采用包含忠实度、风格合规性、安全性和URL有效性的多维奖励机制。

Result: 在内部广告问答数据集上，该方法在专家评估的准确性、完整性和安全性维度上表现一致提升，幻觉率降低72%。为期两周的在线A/B测试显示，点赞率提升28.6%，点踩率降低46.2%，URL幻觉减少92.7%。系统已在生产环境稳定运行半年以上，服务数百万次问答交互。

Conclusion: 该强化协同适应框架有效解决了工业广告问答中的幻觉问题，显著提升了系统性能和安全性，具备大规模生产部署的可行性和稳定性。

Abstract: Industrial advertising question answering (QA) is a high-stakes task in which hallucinated content, particularly fabricated URLs, can lead to financial loss, compliance violations, and legal risk. Although Retrieval-Augmented Generation (RAG) is widely adopted, deploying it in production remains challenging because industrial knowledge is inherently relational, frequently updated, and insufficiently aligned with generation objectives. We propose a reinforced co-adaptation framework that jointly optimizes retrieval and generation through two components: (1) Graph-aware Retrieval (GraphRAG), which models entity-relation structure over a high-citation knowledge subgraph for multi-hop, domain-specific evidence selection; and (2) evidence-constrained reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards covering faithfulness, style compliance, safety, and URL validity. Experiments on an internal advertising QA dataset show consistent gains across expert-judged dimensions including accuracy, completeness, and safety, while reducing the hallucination rate by 72\%. A two-week online A/B test demonstrates a 28.6\% increase in like rate, a 46.2\% decrease in dislike rate, and a 92.7\% reduction in URL hallucination. The system has been running in production for over half a year and has served millions of QA interactions.

</details>


### [13] [dLLM: Simple Diffusion Language Modeling](https://arxiv.org/abs/2602.22661)
*Zhanhui Zhou,Lingjie Chen,Hanghang Tong,Dawn Song*

Main category: cs.CL

TL;DR: 本文提出dLLM——一个开源的扩散语言模型统一框架，通过标准化训练、推理和评估流程，解决现有代码库碎片化问题，降低DLM研究门槛并提供可复现的构建方案。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型快速发展，但核心组件分散于各研究代码库且缺乏透明实现，导致复现与扩展困难。领域加速演进亟需一个既标准化又灵活的统一框架。

Method: 设计dLLM开源框架，统一整合DLM核心组件（训练、推理、评估），提供标准化流水线以复现、微调、部署LLaDA/Dream等开源大模型，并支持从0构建小型DLM及转换BERT或自回归模型为DLM。

Result: 发布dLLM框架及小型DLM模型检查点，提供低计算资源需求的可复现构建方案，显著提升DLM可访问性。

Conclusion: dLLM通过模块化设计解决生态碎片化问题，为研究者提供标准化工具与低门槛入口，将有效推动扩散语言模型领域的创新与发展。

Abstract: Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.
  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.

</details>


### [14] [Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue](https://arxiv.org/abs/2602.22697)
*Ning Gao,Wei Zhang,Yuqin Dai,Ling Shi,Ziyin Wang,Yujie Wang,Wei He,Jinpeng Wang,Chaozheng Wang*

Main category: cs.CL

TL;DR: 本文提出 InteractCS-RL 框架，通过多粒度强化学习解决任务导向对话中同理心沟通与成本意识决策的平衡问题。该框架包含用户中心交互环境和成本感知多轮策略优化（CMPO）模块，采用混合优势估计和 PID-拉格朗日成本控制器探索用户奖励与全局成本的帕累托边界。在真实商业场景和跨领域交互基准测试中均显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正从对话机器人快速演进为通用智能体，但现有方法无法有效平衡富有同理心的沟通与预算感知的决策制定，这种复杂的策略权衡成为亟待解决的关键挑战。

Method: 方法包含两个核心组件：1) 用户中心交互框架，提供高保真训练环境，支持智能体与角色驱动的用户进行动态策略探索；2) 成本感知多轮策略优化（CMPO），集成生成过程信用分配和 PID-拉格朗日成本控制器，通过混合优势估计引导策略在用户奖励与成本约束间寻找帕累托最优边界。

Result: 在定制化真实商业场景的广泛实验表明，InteractCS-RL 在三个评估维度上显著优于其他基线方法；在工具-智能体-用户交互基准测试中验证了该框架在不同领域的鲁棒性。

Conclusion: InteractCS-RL 通过多粒度强化学习成功建模了任务导向对话中的复杂权衡问题，实现了同理心与成本效率的帕累托最优平衡，为 LLM 智能体在真实业务场景的应用提供了有效解决方案。

Abstract: The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.

</details>


### [15] [Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs](https://arxiv.org/abs/2602.22698)
*Siyue Su,Jian Yang,Bo Li,Guanglin Niu*

Main category: cs.CL

TL;DR: 本文提出KGT框架，通过引入专用实体token解决大语言模型与知识图谱之间的粒度不匹配问题，实现高效的全空间预测，并在多个基准测试上持续优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于知识图谱补全面临根本性的粒度不匹配问题：LLM处理的是碎片化的token序列，而知识图谱的基本单元是实体。现有方法通过限制候选集或将实体与LLM词表对齐（如token池化或固定长度分解）来处理，但无法同时捕捉文本语义和图谱结构完整性。

Method: 提出KGT框架：1）采用专用实体token的特殊tokenization方式，在实体级别构建特征表示；2）通过关系引导的门控机制，将预训练的结构和文本特征融合到统一嵌入中，避免从头训练；3）使用独立预测头实现解耦预测，分离并组合语义推理和结构推理。

Result: 在多个基准测试上，KGT持续优于现有最先进方法。

Conclusion: KGT框架通过专用实体token和解耦设计，有效解决了粒度不匹配问题，实现了更高效、更准确的知识图谱补全。

Abstract: Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically constrain predictions to limited candidate sets or align entities with the LLM's vocabulary by pooling multiple tokens or decomposing entities into fixed-length token sequences, which fail to capture both the semantic meaning of the text and the structural integrity of the graph. To address this, we propose KGT, a novel framework that uses dedicated entity tokens to enable efficient, full-space prediction. Specifically, we first introduce specialized tokenization to construct feature representations at the level of dedicated entity tokens. We then fuse pre-trained structural and textual features into these unified embeddings via a relation-guided gating mechanism, avoiding training from scratch. Finally, we implement decoupled prediction by leveraging independent heads to separate and combine semantic and structural reasoning. Experimental results show that KGT consistently outperforms state-of-the-art methods across multiple benchmarks.

</details>


### [16] [Human Label Variation in Implicit Discourse Relation Recognition](https://arxiv.org/abs/2602.22723)
*Frances Yung,Daniil Ignatev,Merel Scholman,Vera Demberg,Massimo Poesio*

Main category: cs.CL

TL;DR: 本研究比较了预测完整标注分布与个体标注者视角模型在隐式篇章关系识别（IDRR）任务上的表现。结果表明，认知复杂性导致的歧义性使个体模型性能受限，而分布模型预测更稳定，为高度模糊任务的建模提供了启示。


<details>
  <summary>Details</summary>
Motivation: 随着对NLP任务中单一标准答案局限性的认识加深，人类标注判断的多元视角促使研究者开发能够捕捉标注变异性的模型，以更好地反映现实世界中的认知差异。

Method: 通过隐式篇章关系识别（IDRR）这一高度模糊的任务，对比分析预测完整标注分布的模型与旨在复现个体标注者解释的视角模型，探究歧义性对模型性能的影响。

Result: 实验显示，除非降低歧义性，现有个体标注者模型在IDRR上表现较差；而基于标签分布的模型能产生更稳定的预测。进一步分析表明，高频的认知复杂案例是导致人类解释不一致的主要驱动力。

Conclusion: 认知复杂性对IDRR中的视角建模提出重大挑战，分布模型在处理由认知复杂性而非意识形态偏见导致的高度模糊任务时表现出更好的鲁棒性。

Abstract: There is growing recognition that many NLP tasks lack a single ground truth, as human judgments reflect diverse perspectives. To capture this variation, models have been developed to predict full annotation distributions rather than majority labels, while perspectivist models aim to reproduce the interpretations of individual annotators. In this work, we compare these approaches on Implicit Discourse Relation Recognition (IDRR), a highly ambiguous task where disagreement often arises from cognitive complexity rather than ideological bias. Our experiments show that existing annotator-specific models perform poorly in IDRR unless ambiguity is reduced, whereas models trained on label distributions yield more stable predictions. Further analysis indicates that frequent cognitively demanding cases drive inconsistency in human interpretation, posing challenges for perspectivist modeling in IDRR.

</details>


### [17] [Extending Czech Aspect-Based Sentiment Analysis with Opinion Terms: Dataset and LLM Benchmarks](https://arxiv.org/abs/2602.22730)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文构建了一个带有观点术语标注的捷克餐厅领域方面级情感分析（ABSA）数据集，支持三个不同复杂度的任务。通过大规模实验评估了Transformer模型和大语言模型在单语、跨语言和多语言设置下的表现，提出基于大语言模型的翻译与标签对齐方法，为低资源语言情感分析提供新基准和可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 捷克语作为低资源语言，缺乏高质量的方面级情感分析资源，现有数据集难以支持跨语言迁移研究，亟需构建专门针对捷克语餐厅领域的标注数据，以推动该语言的细粒度情感分析技术发展并解决低资源语言NLP资源稀缺问题。

Method: 构建捷克餐厅领域ABSA数据集，包含方面、观点术语和情感极性标注；设计三种不同复杂度的任务设置；在单语、跨语言和多语言场景下，系统评估多种预训练模型及大语言模型的性能；提出利用大语言模型进行翻译和标签对齐的跨语言迁移方法，解决低资源语言标注数据不足问题。

Result: 提出的翻译对齐方法在跨语言实验中实现稳定提升；模型在捷克语上表现显著落后于英语等高资源语言；错误分析显示主要挑战包括细微观点术语检测、情感表达理解以及语言结构差异等问题，揭示了当前SOTA模型在低资源语言上的局限性。

Conclusion: 该数据集为捷克语ABSA建立了新的基准，提出的翻译对齐方法为低资源语言情感分析提供了可扩展解决方案，研究结果为未来改进低资源语言细粒度情感分析指明了方向，特别是需要加强模型对语言细微差别和复杂情感表达的理解能力。

Abstract: This paper introduces a novel Czech dataset in the restaurant domain for aspect-based sentiment analysis (ABSA), enriched with annotations of opinion terms. The dataset supports three distinct ABSA tasks involving opinion terms, accommodating varying levels of complexity. Leveraging this dataset, we conduct extensive experiments using modern Transformer-based models, including large language models (LLMs), in monolingual, cross-lingual, and multilingual settings. To address cross-lingual challenges, we propose a translation and label alignment methodology leveraging LLMs, which yields consistent improvements. Our results highlight the strengths and limitations of state-of-the-art models, especially when handling the linguistic intricacies of low-resource languages like Czech. A detailed error analysis reveals key challenges, including the detection of subtle opinion terms and nuanced sentiment expressions. The dataset establishes a new benchmark for Czech ABSA, and our proposed translation-alignment approach offers a scalable solution for adapting ABSA resources to other low-resource languages.

</details>


### [18] [Towards Simulating Social Media Users with LLMs: Evaluating the Operational Validity of Conditioned Comment Prediction](https://arxiv.org/abs/2602.22752)
*Nils Schwager,Simon Münker,Alistair Plum,Achim Rettinger*

Main category: cs.CL

TL;DR: 本研究提出条件化评论预测（CCP）框架，通过预测用户对刺激的评论并与真实数字痕迹对比，评估大语言模型模拟社交媒体用户行为的能力。研究发现低资源场景下监督微调会导致形式与内容解耦，且行为历史能替代显式身份描述进行隐式推断，挑战了现有提示范式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正从探索性工具转变为社会科学研究中的"硅基主体"，但其操作有效性缺乏充分验证。现有模拟方法在真实性方面存在不足，需要严格的评估框架来检验模型能否真正模拟人类社交媒体行为。

Method: 研究引入条件化评论预测（CCP）任务，让模型预测用户评论并与真实数字痕迹对比。评估了Llama3.1、Qwen3、Ministral等8B开源模型在英语、德语和卢森堡语场景下的表现，系统比较了显式与隐式提示策略及监督微调（SFT）的影响。

Result: 发现低资源设置下存在形式与内容解耦现象：SFT能对齐文本输出的表层结构（长度和句法），但会损害语义基础。此外，微调后显式条件化（生成的传记）变得冗余，模型可直接从行为历史中进行隐式推断。

Conclusion: 研究挑战了当前"朴素提示"范式，提出高保真模拟应优先考虑真实行为痕迹而非描述性角色设定，为社会科学中的LLM应用提供了操作指导。

Abstract: The transition of Large Language Models (LLMs) from exploratory tools to active "silicon subjects" in social science lacks extensive validation of operational validity. This study introduces Conditioned Comment Prediction (CCP), a task in which a model predicts how a user would comment on a given stimulus by comparing generated outputs with authentic digital traces. This framework enables a rigorous evaluation of current LLM capabilities with respect to the simulation of social media user behavior. We evaluated open-weight 8B models (Llama3.1, Qwen3, Ministral) in English, German, and Luxembourgish language scenarios. By systematically comparing prompting strategies (explicit vs. implicit) and the impact of Supervised Fine-Tuning (SFT), we identify a critical form vs. content decoupling in low-resource settings: while SFT aligns the surface structure of the text output (length and syntax), it degrades semantic grounding. Furthermore, we demonstrate that explicit conditioning (generated biographies) becomes redundant under fine-tuning, as models successfully perform latent inference directly from behavioral histories. Our findings challenge current "naive prompting" paradigms and offer operational guidelines prioritizing authentic behavioral traces over descriptive personas for high-fidelity simulation.

</details>


### [19] [AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors](https://arxiv.org/abs/2602.22755)
*Abhay Sheshadri,Aidan Ewart,Kai Fronsdal,Isha Gupta,Samuel R. Bowman,Sara Price,Samuel Marks,Rowan Wang*

Main category: cs.CL

TL;DR: 本文提出AuditBench对齐审计基准，包含56个植入14种隐蔽问题行为的语言模型。研究发现审计工具与智能体间存在性能鸿沟，最有效的审计方法是通过scaffolded调用辅助模型生成多样化提示，且黑盒工具优于白盒可解释性工具。不同训练技术显著影响审计难度：合成文档训练的模型比演示训练的模型更易审计，对抗训练则增加审计难度。


<details>
  <summary>Details</summary>
Motivation: 现有对齐审计缺乏系统性基准测试环境，无法量化评估不同审计工具对隐藏行为的检测能力。缺乏标准化平台来比较工具在真实审计场景中的有效性，导致审计方法开发缺乏科学依据。

Method: 构建包含56个植入14种隐蔽行为（如谄媚顺从、反对AI监管、秘密地缘政治倾向）的语言模型基准。开发自主调查智能体，配置多种审计工具，通过测量智能体成功率评估工具效能。采用不同训练技术（合成文档训练、演示训练、对抗训练）植入行为并防止模型直接承认，确保行为隐蔽性。

Result: 发现显著的工具-智能体性能差距：独立评估中表现优异的工具在智能体环境中未能转化为性能提升。最有效的工具涉及scaffolded调用辅助模型生成多样化提示；黑盒工具表现优于白盒可解释性工具。审计成功率因训练技术差异巨大：合成文档训练的模型审计难度较低，演示训练的模型更难审计，对抗训练进一步增加审计难度。

Conclusion: AuditBench为对齐审计建立了首个定量迭代科学研究框架，揭示了当前审计工具在智能体环境中的局限性，强调需开发更适合智能体交互的审计方法。研究结果表明，行为植入训练技术和对抗训练显著影响审计难度，为未来审计工具开发和审计标准化提供了基准与实证依据。

Abstract: We introduce AuditBench, an alignment auditing benchmark. AuditBench consists of 56 language models with implanted hidden behaviors. Each model has one of 14 concerning behaviors--such as sycophantic deference, opposition to AI regulation, or secret geopolitical loyalties--which it does not confess to when directly asked. AuditBench models are highly diverse--some are subtle, while others are overt, and we use varying training techniques both for implanting behaviors and training models not to confess. To demonstrate AuditBench's utility, we develop an investigator agent that autonomously employs a configurable set of auditing tools. By measuring investigator agent success using different tools, we can evaluate their efficacy. Notably, we observe a tool-to-agent gap, where tools that perform well in standalone non-agentic evaluations fail to translate into improved performance when used with our investigator agent. We find that our most effective tools involve scaffolded calls to auxiliary models that generate diverse prompts for the target. White-box interpretability tools can be helpful, but the agent performs best with black-box tools. We also find that audit success varies greatly across training techniques: models trained on synthetic documents are easier to audit than models trained on demonstrations, with better adversarial training further increasing auditing difficulty. We release our models, agent, and evaluation framework to support future quantitative, iterative science on alignment auditing.

</details>


### [20] [Towards Better RL Training Data Utilization via Second-Order Rollout](https://arxiv.org/abs/2602.22765)
*Zhe Yang,Yudong Wang,Rang Li,Zhifang Sui*

Main category: cs.CL

TL;DR: 本文针对标准强化学习仅通过一阶rollout训练生成能力而忽略批判能力的局限性，提出二阶rollout（为生成结果提供多维度批判反馈）与生成-批判联合训练框架，通过动态数据增强提升大语言模型在相同数据条件下的推理性能与训练效率。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习在提升大语言模型推理能力时，仅通过生成多个答案的一阶rollout来优化生成能力，未能充分挖掘训练数据中蕴含的批判性反馈潜力，导致数据利用效率低下。为此，需要显式地建模并训练模型的批判能力，以实现更全面的能力提升。

Method: 创新性地提出二阶rollout机制，即为每个生成响应生成多个批判性反馈样本，并设计统一的联合训练框架同步优化生成与批判双能力。针对基于结果的奖励信号存在的噪声问题，采用特定的采样技术进行缓解，确保训练过程的稳定性和有效性。

Result: 在多种模型与数据集上的广泛实验表明，所提方法在相同训练数据量下显著优于标准强化学习基线，数据利用效率更高。关键发现包括：批判训练中标签平衡对性能至关重要，以及采样技术能有效缓解结果奖励的噪声问题。

Conclusion: 本研究初步探索了强化学习中的动态数据增强与生成-批判联合训练范式，揭示了批判能力训练对大语言模型的重要性，为未来强化学习训练方法的进一步发展提供了有价值的理论启示与实践指导。

Abstract: Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating multiple critiques for a response) and propose a unified framework for jointly training generation and critique capabilities. Extensive experiments across various models and datasets demonstrate that our approach can utilize training data more effectively than vanilla RL and achieve better performance under the same training data. Additionally, we uncover several insightful findings regarding second-order rollout and critique training, such as the importance of label balance in critique training and the noise problem of outcome-based rewards, which can be mitigated through sampling techniques. Our work offers a preliminary exploration of dynamic data augmentation and joint generation-critique training in RL, providing meaningful inspiration for the further advancement of RL training

</details>


### [21] [Imagination Helps Visual Reasoning, But Not Yet in Latent Space](https://arxiv.org/abs/2602.22766)
*You Li,Chi Chen,Yanghao Li,Fanhu Zeng,Kaiyu Huang,Jinan Xu,Maosong Sun*

Main category: cs.CL

TL;DR: 本研究通过因果中介分析揭示MLLM潜在视觉推理存在输入-潜在token和潜在token-答案双重断连，潜在token对输入关注度低且因果效应有限；提出显式文本想象方法CapImagine，在视觉基准测试上显著超越复杂潜在空间基线，挑战了潜在推理的必要性。


<details>
  <summary>Details</summary>
Motivation: 潜在视觉推理作为视觉推理范式虽受认可，但其效能机制尚不明确；研究者旨在揭开其有效性来源，质疑潜在推理是否必要。

Method: 采用因果中介分析，将推理过程建模为因果链：输入为处理、潜在token为中介、答案为结果；通过输入/潜在token扰动实验和探测分析检验各环节因果关联。

Result: 发现两个关键断连：输入-潜在断开（输入扰动对潜在token影响微弱，token未有效关注输入）和潜在-答案断开（潜在token扰动对答案影响极小，因果效应有限）；潜在token编码视觉信息稀疏且高度相似。

Conclusion: 挑战潜在推理必要性，提出CapImagine显式文本想象方法；在视觉基准测试上显著优于复杂潜在空间基线，验证显式想象在视觉推理中的优越潜力。

Abstract: Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.

</details>


### [22] [Probing for Knowledge Attribution in Large Language Models](https://arxiv.org/abs/2602.22787)
*Ivo Brink,Alexander Boer,Dennis Ulmer*

Main category: cs.CL

TL;DR: 本文研究大语言模型的贡献归因问题，即识别每个输出来自提示还是内部知识。通过引入自监督数据管道AttriWiki训练线性探针，在多个模型上达到0.96 Macro-F1，并发现归因不匹配会使错误率上升70%，但模型在归因正确时仍可能出错，需更广泛检测框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生流畅但无根据的幻觉，分为忠实性违规（误用用户上下文）和事实性违规（内部知识错误）。有效缓解幻觉需明确答案来源是提示还是模型内部权重，因此精确的贡献归因成为关键前提。

Method: 提出使用线性探针分析模型隐藏表示来预测贡献归因。构建AttriWiki自监督数据管道，通过提示模型回忆被隐藏的实体或从上下文中读取信息，自动生成标注样本。在Llama-3.1-8B、Mistral-7B和Qwen-7B上训练探针。

Result: 探针在贡献归因任务上达到高达0.96 Macro-F1，在未重新训练的情况下，可迁移至SQuAD和WebQuestions等域外基准，达到0.94-0.99 Macro-F1。归因不匹配会使错误率提升高达70%，证实知识源混淆与不忠实答案存在直接关联。

Conclusion: 研究揭示了知识来源识别的有效性，但模型在归因正确时仍可能产生错误，表明单一归因检测不足以解决所有幻觉问题。需要构建更全面的检测框架来覆盖更广泛的错误类型，以提升模型输出的可靠性和忠实性。

Abstract: Large language models (LLMs) often generate fluent but unfounded claims, or hallucinations, which fall into two types: (i) faithfulness violations - misusing user context - and (ii) factuality violations - errors from internal knowledge. Proper mitigation depends on knowing whether a model's answer is based on the prompt or its internal weights. This work focuses on the problem of contributive attribution: identifying the dominant knowledge source behind each output. We show that a probe, a simple linear classifier trained on model hidden representations, can reliably predict contributive attribution. For its training, we introduce AttriWiki, a self-supervised data pipeline that prompts models to recall withheld entities from memory or read them from context, generating labelled examples automatically. Probes trained on AttriWiki data reveal a strong attribution signal, achieving up to 0.96 Macro-F1 on Llama-3.1-8B, Mistral-7B, and Qwen-7B, transferring to out-of-domain benchmarks (SQuAD, WebQuestions) with 0.94-0.99 Macro-F1 without retraining. Attribution mismatches raise error rates by up to 70%, demonstrating a direct link between knowledge source confusion and unfaithful answers. Yet, models may still respond incorrectly even when attribution is correct, highlighting the need for broader detection frameworks.

</details>


### [23] [Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift](https://arxiv.org/abs/2602.22790)
*Hyunwoo Kim,Hanau Yi,Jaehee Bae,Yumin Kim*

Main category: cs.CL

TL;DR: 本文针对大语言模型演化导致的"模型漂移"问题，将自然语言声明式提示(NLD-P)重新概念化为声明式治理方法，通过模块化控制抽象分离来源、约束逻辑、任务内容和生成后评估，为演化的LLM生态中非开发者提供可访问的治理框架。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模扩大和代际更新，提示工程从局部技艺转变为系统级治理挑战。提示行为对指令遵循策略、对齐机制和解码策略的变化高度敏感，出现"大规模模型漂移"现象，表面格式和临时优化已无法保证稳定、可解释的控制。

Method: 将NLD-P形式化为模块化控制抽象，直接在自然语言中编码，分离来源追踪、约束逻辑、任务内容和生成后评估四个核心组件，无需外部编排代码。定义最小合规标准，并分析不同模型的模式接受度。

Result: 建立了NLD-P作为非开发者可访问的治理框架，制定了最小合规标准，分析了模型依赖的模式接受度差异。作者使用基于NLD-P配置的LLM助手进行起草和编辑，通过人机协同协议进行指导、审核和最终批准。

Conclusion: 为持续模型演化下的声明式控制提供了理论和实践框架，指出未来需开展实证验证，强调人机协同在构建治理体系中的重要性。

Abstract: The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, and decoding strategies, a phenomenon we characterize as GPT-scale model drift. Under such conditions, surface-level formatting conventions and ad hoc refinement are insufficient to ensure stable, interpretable control. This paper reconceptualizes Natural Language Declarative Prompting (NLD-P) as a declarative governance method rather than a rigid field template. NLD-P is formalized as a modular control abstraction that separates provenance, constraint logic, task content, and post-generation evaluation, encoded directly in natural language without reliance on external orchestration code. We define minimal compliance criteria, analyze model-dependent schema receptivity, and position NLD-P as an accessible governance framework for non-developer practitioners operating within evolving LLM ecosystems. Portions of drafting and editorial refinement employed a schema-bound LLM assistant configured under NLD-P. All conceptual framing, methodological claims, and final revisions were directed, reviewed, and approved by the human author under a documented human-in-the-loop protocol. The paper concludes by outlining implications for declarative control under ongoing model evolution and identifying directions for future empirical validation.

</details>


### [24] [TARAZ: Persian Short-Answer Question Benchmark for Cultural Evaluation of Language Models](https://arxiv.org/abs/2602.22827)
*Reihaneh Iranmanesh,Saeedeh Davoudi,Pasha Abrishamchian,Ophir Frieder,Nazli Goharian*

Main category: cs.CL

TL;DR: 该论文针对波斯语大语言模型文化能力评估提出新框架，解决现有英文中心指标和选择题形式无法捕捉波斯语形态语义复杂性的问题，通过波斯语短答案评估、规则化形态归一化与混合句法语义相似度模块实现软匹配评分，并在15个模型上验证比精确匹配提升10%一致性，首次发布波斯语文化理解标准化基准。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语文化基准主要依赖选择题形式和英语中心指标，无法有效评估波斯语的形态复杂性和语义细微差别，亟需建立适合波斯语特性的文化能力评估体系。

Method: 提出波斯语特定短答案评估框架，结合基于规则的形态归一化与混合句法语义相似度模块，实现超越精确字符串匹配的软匹配评分机制，支持语义层面的鲁棒性评估。

Result: 对15个先进开源与闭源模型的评估显示，该混合评估方法比精确匹配基线提高10%的评分一致性，能有效捕捉表层方法无法检测的语义含义。

Conclusion: 公开释放首个波斯语文化理解标准化评估框架，为波斯语文化能力测量提供可复现基础，推动跨文化大语言模型评估研究发展。

Abstract: This paper presents a comprehensive evaluation framework for assessing the cultural competence of large language models (LLMs) in Persian. Existing Persian cultural benchmarks rely predominantly on multiple-choice formats and English-centric metrics that fail to capture Persian's morphological complexity and semantic nuance. Our framework introduces a Persian-specific short-answer evaluation that combines rule-based morphological normalization with a hybrid syntactic and semantic similarity module, enabling robust soft-match scoring beyond exact string overlap. Through systematic evaluation of 15 state-of-the-art open- and closed-source models, we demonstrate that our hybrid evaluation improves scoring consistency by +10% compared to exact-match baselines by capturing meaning that surface-level methods cannot detect. We publicly release our evaluation framework, providing the first standardized benchmark for measuring cultural understanding in Persian and establishing a reproducible foundation for cross-cultural LLM evaluation research.

</details>


### [25] [CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery](https://arxiv.org/abs/2602.23075)
*Mengze Hong,Di Jiang,Chen Jason Zhang,Zichang Guo,Yawen Li,Jun Chen,Shaobo Cui,Zhiyang Su*

Main category: cs.CL

TL;DR: 为解决LLM在学术应用中的伦理挑战，本文提出CiteLLM平台，通过LaTeX集成、动态学科感知路由和可信学术库检索，实现可信参考文献发现，有效防止幻觉并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽能提升学术活动效率，但在伦理部署方面面临内容可信度、学术诚信与知识产权保护、信息隐私保护等挑战。

Method: 提出CiteLLM智能体平台，采用LaTeX编辑器直接嵌入LLM工具的新范式，通过动态学科感知路由从可信学术库检索文献，利用LLM生成上下文感知查询、排序候选文献，并通过段落级语义匹配和集成聊天机器人验证与解释支持关系。

Result: 评估结果表明，该系统在返回有效且高可用性的参考文献方面性能优越。

Conclusion: CiteLLM通过本地集成、可信来源检索和智能验证，有效解决了学术AI应用的伦理挑战，为科研工作者提供了可靠的参考文献发现平台。

Abstract: Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted claims and statements. The system introduces a novel interaction paradigm by embedding LLM utilities directly within the LaTeX editor environment, ensuring a seamless user experience and no data transmission outside the local system. To guarantee hallucination-free references, we employ dynamic discipline-aware routing to retrieve candidates exclusively from trusted web-based academic repositories, while leveraging LLMs solely for generating context-aware search queries, ranking candidates by relevance, and validating and explaining support through paragraph-level semantic matching and an integrated chatbot. Evaluation results demonstrate the superior performance of the proposed system in returning valid and highly usable references.

</details>


### [26] [TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought](https://arxiv.org/abs/2602.22828)
*Jianmin Li,Ying Chang,Su-Kit Tang,Yujia Liu,Yanwen Wang,Shuyuan Lin,Binkai Ou*

Main category: cs.CL

TL;DR: 本研究针对传统中医临床诊疗中复杂推理和个体差异导致的传统RAG性能不佳问题，提出TCM-DiffRAG框架，通过融合知识图谱与思维链推理，在三个中医测试集上显著超越原生大模型、监督微调模型及其他RAG方法，为非西医疗领域的大模型应用提供了推理感知的增强方案。


<details>
  <summary>Details</summary>
Motivation: 传统RAG技术在中医临床诊疗场景表现不佳，主要由于中医诊断治疗涉及复杂的推理过程和显著的个体化差异，需要更贴合中医特色的增强生成框架来解决知识检索与推理的适配性问题。

Method: 开发TCM-DiffRAG框架，创新性地将结构化中医知识图谱与思维链推理机制相结合，在三个 distinctive 的中医测试数据集上进行评估验证。

Result: 实验结果表明，TCM-DiffRAG对原生大模型性能提升显著，例如qwen-plus模型三项指标从0.927、0.361、0.038分别提升至0.952、0.788和0.356；同时超越监督微调模型及其他基准RAG方法，对非中文大模型提升效果更为突出。

Conclusion: 通过联合使用通用与个性化知识图谱实现通用知识与临床推理的有效对齐，TCM-DiffRAG验证了融合结构化知识与链式推理的推理感知RAG框架能有效提升中医个体化诊断任务性能，为传统医学领域大模型应用拓展了可行路径。

Abstract: Background: Retrieval augmented generation (RAG) technology can empower large language models (LLMs) to generate more accurate, professional, and timely responses without fine tuning. However, due to the complex reasoning processes and substantial individual differences involved in traditional Chinese medicine (TCM) clinical diagnosis and treatment, traditional RAG methods often exhibit poor performance in this domain. Objective: To address the limitations of conventional RAG approaches in TCM applications, this study aims to develop an improved RAG framework tailored to the characteristics of TCM reasoning. Methods: We developed TCM-DiffRAG, an innovative RAG framework that integrates knowledge graphs (KG) with chains of thought (CoT). TCM-DiffRAG was evaluated on three distinctive TCM test datasets. Results: The experimental results demonstrated that TCM-DiffRAG achieved significant performance improvements over native LLMs. For example, the qwen-plus model achieved scores of 0.927, 0.361, and 0.038, which were significantly enhanced to 0.952, 0.788, and 0.356 with TCM-DiffRAG. The improvements were even more pronounced for non-Chinese LLMs. Additionally, TCM-DiffRAG outperformed directly supervised fine-tuned (SFT) LLMs and other benchmark RAG methods. Conclusions: TCM-DiffRAG shows that integrating structured TCM knowledge graphs with Chain of Thought based reasoning substantially improves performance in individualized diagnostic tasks. The joint use of universal and personalized knowledge graphs enables effective alignment between general knowledge and clinical reasoning. These results highlight the potential of reasoning-aware RAG frameworks for advancing LLM applications in traditional Chinese medicine.

</details>


### [27] [SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables](https://arxiv.org/abs/2602.23286)
*Sungho Park,Jueun Kim,Wook-Shin Han*

Main category: cs.CL

TL;DR: SPARTA是一个自动构建大规模表-文问答基准的框架，通过溯源精化和现实结构约束两个创新技术，解决现有基准规模小、浅层、人工整理易出错的问题。该框架生成的QA对涵盖深层多跳推理和复杂聚合操作，使当前最优模型性能骤降30+ F1点，揭示了跨模态推理的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 真实世界的表-文问答任务需要模型具备跨长文本和源表的多跳推理与复杂聚合能力，但现有基准存在三大缺陷：规模小、人工整理导致错误率高，以及问题浅层化（很少超过两跳或涉及分组聚合等高级分析操作），无法充分评估模型的真实推理能力。

Method: 框架分三步：1) 构建参考事实库，通过自动从无结构文本中提取原子事实构建grounding tables来丰富源表；2) 合成嵌套查询以精确控制跳数；3) 提出两项新技术：溯源精化（重写语法正确但返回空结果的查询）和现实结构约束（将生成限制为查询图的后序遍历），确保SQL可执行且问题表述自然流畅。

Result: 生成了数千个高质量QA对，覆盖聚合、分组和深层多跳推理。实验表明，在HybridQA上达到70+ F1或在OTT-QA上达到50+ F1的当前最优模型，在SPARTA上性能下降均超过30个F1点，显著暴露了模型在复杂跨模态推理上的短板。

Conclusion: SPARTA成功创建了更具挑战性和规模化的表-文问答基准，揭示了现有模型在处理真实世界复杂推理任务时的根本弱点，为推动更强大的跨模态推理模型发展提供了重要资源。基准、代码和基线模型已开源。

Abstract: Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.

</details>


### [28] [Improving Neural Argumentative Stance Classification in Controversial Topics with Emotion-Lexicon Features](https://arxiv.org/abs/2602.22846)
*Mohammad Yeghaneh Abkenar,Weixing Wang,Manfred Stede,Davide Picca,Mark A. Finlayson,Panagiotis Ioannidis*

Main category: cs.CL

TL;DR: 本研究提出利用DistilBERT扩展偏置校正后的NRC情感词典(eNRC)，并将其应用于神经论点立场分类。在五个涵盖争议性话题的跨领域数据集上，eNRC较基线提升F1分数最高达6.2%，且在大多数情况下优于原始NRC和LLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有立场分类研究存在三大局限：未系统融入细粒度情感分析、主要使用非论点文本、领域泛化性差。鉴于争议性论点常诉诸情感，作者旨在探索情感特征对提升立场分类性能的作用。

Method: 核心方法是基于DistilBERT的上下文嵌入向量扩展偏置校正NRC情感词典，识别词典未收录的情感性词汇，构建eNRC。在五个多样化领域的争议性话题数据集上，将eNRC集成至神经论点立场分类模型，通过增强情感特征表示改进分类效果。

Result: 实验结果表明：eNRC在全部五个数据集上超越基线方法（F1最高+6.2个百分点）；在四个数据集上优于原始NRC词典（最高+3.0个百分点）；在几乎所有语料库上表现优于基于大语言模型的方法。

Conclusion: 通过上下文感知的情感词典扩展能显著提升论点立场分类性能，特别是在处理情感化的争议性话题时。研究公开了eNRC词典、适配语料库和模型架构，为后续研究提供了可复现的资源和基准。

Abstract: Argumentation mining comprises several subtasks, among which stance classification focuses on identifying the standpoint expressed in an argumentative text toward a specific target topic. While arguments-especially about controversial topics-often appeal to emotions, most prior work has not systematically incorporated explicit, fine-grained emotion analysis to improve performance on this task. In particular, prior research on stance classification has predominantly utilized non-argumentative texts and has been restricted to specific domains or topics, limiting generalizability. We work on five datasets from diverse domains encompassing a range of controversial topics and present an approach for expanding the Bias-Corrected NRC Emotion Lexicon using DistilBERT embeddings, which we feed into a Neural Argumentative Stance Classification model. Our method systematically expands the emotion lexicon through contextualized embeddings to identify emotionally charged terms not previously captured in the lexicon. Our expanded NRC lexicon (eNRC) improves over the baseline across all five datasets (up to +6.2 percentage points in F1 score), outperforms the original NRC on four datasets (up to +3.0), and surpasses the LLM-based approach on nearly all corpora. We provide all resources-including eNRC, the adapted corpora, and model architecture-to enable other researchers to build upon our work.

</details>


### [29] [Effective QA-driven Annotation of Predicate-Argument Relations Across Languages](https://arxiv.org/abs/2602.22865)
*Jonathan Davidov,Aviv Slobodkin,Shmuel Tomi Klein,Reut Tsarfaty,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: 本文提出跨语言投影方法，利用英语QA-SRL解析器通过受限翻译和词对齐管道，自动为希伯来语、俄语和法语生成高质量谓词-论元标注数据，训练的语言特定解析器性能超越GPT-4o等大模型，实现高效跨语言语义分析。


<details>
  <summary>Details</summary>
Motivation: 显式谓词-论元关系对可解释语义分析至关重要，但现有标注成本高昂且主要局限于英语，亟需可扩展至其他语言的解决方案。

Method: 采用问题-回答驱动的语义角色标注(QA-SRL)框架，通过跨语言投影方法，在受限翻译和词对齐管道中复用英语QA-SRL解析器，自动生成与目标语言谓词对齐的问答标注。

Result: 在希伯来语、俄语和法语(涵盖不同语系)上生成高质量训练数据，微调后的语言特定解析器性能显著优于GPT-4o、LLaMA-Maverick等强大多语言大模型基线。

Conclusion: 该研究利用QA-SRL作为可迁移的自然语言语义接口，实现了跨语言谓词-论元解析的高效、可扩展访问，为多语言语义分析提供了可行路径。

Abstract: Explicit representations of predicate-argument relations form the basis of interpretable semantic analysis, supporting reasoning, generation, and evaluation. However, attaining such semantic structures requires costly annotation efforts and has remained largely confined to English. We leverage the Question-Answer driven Semantic Role Labeling (QA-SRL) framework -- a natural-language formulation of predicate-argument relations -- as the foundation for extending semantic annotation to new languages. To this end, we introduce a cross-linguistic projection approach that reuses an English QA-SRL parser within a constrained translation and word-alignment pipeline to automatically generate question-answer annotations aligned with target-language predicates. Applied to Hebrew, Russian, and French -- spanning diverse language families -- the method yields high-quality training data and fine-tuned, language-specific parsers that outperform strong multilingual LLM baselines (GPT-4o, LLaMA-Maverick). By leveraging QA-SRL as a transferable natural-language interface for semantics, our approach enables efficient and broadly accessible predicate-argument parsing across languages.

</details>


### [30] [Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference](https://arxiv.org/abs/2602.22868)
*Yushi Ye,Feng Hong,Huangjie Zheng,Xu Chen,Zhiyong Chen,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: 本文针对扩散大语言模型(DLLMs)在并行解码中存在的严重质量-速度权衡问题，提出了ReMix（拒绝混合）框架。该框架通过引入连续混合状态作为中间表示，在离散解码过程中实现token在连续空间中的迭代优化，并通过拒绝机制将不确定表示回退至掩码状态重新处理，从而缓解并行token间的语义冲突（即"组合矛盾"）。作为无需训练的方法，ReMix实现了2-8倍的推理加速且无质量损失。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型虽具备快速非自回归推理的潜力，但在并行解码时会出现"组合矛盾"现象——并行生成的token在语义上形成不一致的组合，导致质量与速度之间的严重权衡问题。离散解码过程缺乏有效的跨位置依赖维护机制，亟需一种能在保持推理效率的同时提升生成质量的方法。

Method: 提出ReMix（Rejection Mixing）框架，核心创新包括：（1）引入连续混合状态（Continuous Mixing State）作为初始掩码状态与最终解码状态之间的中间桥梁；（2）在该中间状态下，token表示可在连续空间中进行迭代细化，在坍缩为离散token前解决与其他token的相互冲突；（3）设计拒绝规则，将不确定的连续表示回退至掩码状态重新处理，防止错误传播并确保稳定性。该方法无需额外训练，可直接应用于现有扩散解码流程。

Result: 在多个实验中，ReMix作为训练免费方法，在不降低生成质量的前提下，实现了2-8倍的推理速度提升。该方法有效缓解了组合矛盾问题，显著改善了扩散大语言模型在并行解码时的效率与性能平衡。

Conclusion: ReMix通过将连续表示集成到离散扩散解码过程中，利用连续空间的迭代细化和回退机制，成功解决了并行解码中的组合矛盾问题。这一框架为扩散大语言模型的实际部署提供了高效的推理加速方案，证明了连续表示在改善离散生成质量方面的有效性。

Abstract: Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \times$ inference speedup without any quality degradation.

</details>


### [31] [Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching](https://arxiv.org/abs/2602.22871)
*Roy Miles,Aysim Toker,Andreea-Maria Oncescu,Songcen Xu,Jiankang Deng,Ismail Elezi*

Main category: cs.CL

TL;DR: 本文提出"拼接噪声扩散思维"(SNDT)框架，通过从多个扩散模型生成的推理轨迹中提取并拼接高质量的中间步骤，再结合自回归模型重新计算最终答案，显著提升了数学和代码任务的推理准确率并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理方法通常采用轨迹级聚合策略（如选择最佳轨迹或对最终答案投票），这会丢弃部分正确或接近正确尝试中的有用中间结果。作者希望开发一种能够重用这些中间步骤的方法，以提高推理效率和质量。

Method: 1. 使用掩码扩散语言模型采样多个多样化的低成本推理轨迹；2. 用现成的过程奖励模型(PRM)对每个中间步骤进行评分；3. 跨轨迹拼接最高质量的步骤形成复合推理路径；4. 该推理路径作为条件输入给自回归模型，仅重新计算最终答案。该方法采用模块化流程，将探索（扩散）与评估和解决方案综合分离。

Result: 在六个数学和代码任务上，平均准确率提升高达23.8%；相比传统扩散模型和统一架构，延迟降低高达1.8倍；步骤级重组在更难的问题上效果最显著；消融实验表明最终的自回归求解器对于将不完美的拼接推理转化为准确答案至关重要。

Conclusion: SNDT框架通过模块化设计实现了扩散模型的探索优势与自回归模型的精确生成能力的结合，无需训练即可在复杂推理任务上取得显著性能提升，为高效推理提供了新思路。

Abstract: Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or "nearly correct" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.

</details>


### [32] [Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models](https://arxiv.org/abs/2602.22918)
*Jonathan Steinberg,Oren Gal*

Main category: cs.CL

TL;DR: 本研究通过因果干预探究视觉语言模型中OCR信息的处理路径，发现不同架构存在特异性瓶颈：深度堆叠模型中层敏感（约50%），单阶段投影模型浅层敏感（6-25%）；OCR信号呈低维性（PC1占72.9%方差）且跨数据集共享，移除OCR甚至能在模块化架构中提升计数性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽具备图像文本识别能力，但OCR信息在语言处理流中的具体整合机制尚不明确。厘清此路由过程对优化架构设计、提升多模态理解效率具有关键意义，尤其当文本信息与其他视觉任务存在交互或干扰时。

Method: 采用因果干预策略，对比分析原始图像与文本修复图像在Qwen3-VL、Phi-4、InternVL3.5三种架构中的激活差异，定位OCR瓶颈层；运用主成分分析量化信号维度，并验证跨数据集迁移特性。

Result: 架构特异性OCR瓶颈：Qwen3-VL等深度堆叠模型对场景文本的敏感性峰值位于中层（约50%深度），而Phi-4和InternVL3.5等单阶段投影模型峰值出现在浅层（6-25%）。OCR信号低维特性：第一主成分捕获72.9%的方差。跨数据集共享性：在单一数据集上训练的PCA方向可迁移至其他数据集，表明存在通用文本处理通路。性能影响：在Qwen3-VL-4B等模块化OCR电路模型中，移除OCR信号可使计数性能提升最高6.9个百分点，暗示OCR可能干扰其他视觉处理。

Conclusion: 该研究揭示了视觉语言模型中OCR信息整合的架构依赖模式，表明OCR处理与一般视觉处理可能存在功能竞争。模块化架构为分离操控OCR信号提供了可能，这为未来模型设计提供了新方向：通过优化OCR与视觉特征的解耦与协同机制，可提升特定下游任务的性能表现。

Abstract: Vision-language models (VLMs) can read text from images, but where does this optical character recognition (OCR) information enter the language processing stream? We investigate the OCR routing mechanism across three architecture families (Qwen3-VL, Phi-4, InternVL3.5) using causal interventions. By computing activation differences between original images and text-inpainted versions, we identify architecture-specific OCR bottlenecks whose dominant location depends on the vision-language integration strategy: DeepStack models (Qwen) show peak sensitivity at mid-depth (about 50%) for scene text, while single-stage projection models (Phi-4, InternVL) peak at early layers (6-25%), though the exact layer of maximum effect varies across datasets. The OCR signal is remarkably low-dimensional: PC1 captures 72.9% of variance. Crucially, principal component analysis (PCA) directions learned on one dataset transfer to others, demonstrating shared text-processing pathways. Surprisingly, in models with modular OCR circuits (notably Qwen3-VL-4B), OCR removal can improve counting performance (up to +6.9 percentage points), suggesting OCR interferes with other visual processing in sufficiently modular architectures.

</details>


### [33] [Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention](https://arxiv.org/abs/2602.23057)
*Jeongin Bae,Baeseong Park,Gunho Park,Minsub Kim,Joonhyung Lee,Junhee Yoo,Sunghyeon Woo,Jiwon Ryu,Se Jung Kwon,Dongsoo Lee*

Main category: cs.CL

TL;DR: 本文提出仿射缩放注意力机制，通过在softmax归一化后的注意力权重上引入输入相关的缩放和偏置项，放松严格的单位求和约束，从而在保持值聚合的同时可控地调整注意力的分布和幅度。在大规模语言模型预训练中，该方法相比标准softmax注意力和注意力汇聚基线，在训练稳定性、优化行为和下游任务性能上均取得一致提升。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer注意力中的softmax归一化强制注意力权重满足单位求和约束，这限制了注意力幅度的灵活性，可能导致训练过程中注意力模式过于集中或不稳定。现有改进方法（如注意力汇聚或门控机制）仅提供有限或间接的重加权控制。因此，需要一种更直接、灵活的方式来调控注意力分布和尺度。

Method: 提出仿射缩放注意力，对标准softmax注意力权重施加输入相关的仿射变换（缩放+偏置）。具体地，在计算注意力输出时，将softmax归一化后的权重乘以一个输入依赖的缩放因子并加上偏置项，再对值进行聚合。这种设计在保持值聚合机制的同时，放松了严格的求和约束，使模型能够以可控方式调整注意力的相对分布和整体尺度。

Result: 在多种规模的大规模语言模型预训练实验中，仿射缩放注意力相比标准softmax注意力和注意力汇聚基线，展现出更稳定的训练过程、更优的优化行为，并在下游任务上获得持续的性能提升。

Conclusion: 适度的注意力输出重加权是改进Transformer注意力行为的实用有效手段。仿射缩放注意力通过简单的扩展实现了对注意力分布和尺度的可控调节，为提升大规模语言模型的训练和性能提供了新思路。

Abstract: Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner.
  We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.

</details>


### [34] [Toward Automatic Filling of Case Report Forms: A Case Study on Data from an Italian Emergency Department](https://arxiv.org/abs/2602.23062)
*Gabriela Anna Kaczmarek,Pietro Ferrazzi,Lorenzo Porta,Vicky Rubini,Bernardo Magnini*

Main category: cs.CL

TL;DR: 该论文针对临床笔记自动填写病例报告表(CRF)任务因缺乏标注数据而进展受限的问题，构建了一个包含134个项目的意大利急诊科临床笔记新数据集。研究定义了CRF填写任务及评估指标，并基于开源LLM进行零样本实验。结果表明零样本方法可行，但LLM存在谨慎性偏差（倾向于回答"未知"），需进一步校正。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型(LLM)在从临床笔记自动填写病例报告表(CRF)方面展现出潜力，但该任务面临训练和测试数据普遍短缺的瓶颈，严重制约了研究进展。为此，亟需提供高质量的标注数据集以推动该领域发展。

Method: 研究采用四步方法：首先，构建并发布意大利急诊科临床笔记新数据集，标注134个CRF项目；其次，分析数据特征与分布；第三，明确定义CRF自动填写任务及相应的评估指标；最后，使用开源先进LLM进行零样本实验以验证可行性并分析模型表现。

Result: 实验结果证实：(i)针对意大利语真实临床笔记的CRF填写任务可在零样本设置下有效开展；(ii)LLM在预测时表现出显著偏差，特别是过度谨慎的倾向导致频繁输出"未知"答案，这种系统性偏差会降低CRF填写的实用性，必须加以纠正。

Conclusion: 本研究通过提供新数据集缓解了CRF自动填写领域的数据稀缺问题，验证了零样本LLM方法的可行性。核心结论是：尽管LLM具备处理真实临床场景的潜力，但其固有偏差（尤其是谨慎性偏好）会严重影响任务表现，未来工作需重点开发偏差校正策略以提升模型在实际医疗环境中的可靠性。

Abstract: Case Report Forms (CRFs) collect data about patients and are at the core of well-established practices to conduct research in clinical settings. With the recent progress of language technologies, there is an increasing interest in automatic CRF-filling from clinical notes, mostly based on the use of Large Language Models (LLMs). However, there is a general scarcity of annotated CRF data, both for training and testing LLMs, which limits the progress on this task. As a step in the direction of providing such data, we present a new dataset of clinical notes from an Italian Emergency Department annotated with respect to a pre-defined CRF containing 134 items to be filled. We provide an analysis of the data, define the CRF-filling task and metric for its evaluation, and report on pilot experiments where we use an open-source state-of-the-art LLM to automatically execute the task. Results of the case-study show that (i) CRF-filling from real clinical notes in Italian can be approached in a zero-shot setting; (ii) LLMs' results are affected by biases (e.g., a cautious behaviour favours "unknown" answers), which need to be corrected.

</details>


### [35] [Quantity Convergence, Quality Divergence: Disentangling Fluency and Accuracy in L2 Mandarin Prosody](https://arxiv.org/abs/2602.23071)
*Yuqi Shi,Hao Yang,Xiyao Lu,Jinsong Zhang*

Main category: cs.CL

TL;DR: 本研究揭示越南语学习者句法-韵律接口的石化现象：尽管高级学习者能匹配母语者的韵律边界数量，但其结构映射存在系统性偏差——在主谓边界弱化边界等级，在动宾边界过度强化，导致韵律层级倒置，为高流利度而牺牲结构准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管第二语言学习者可能掌握目标语言的句法词序，但将这些句法结构映射到恰当的韵律结构仍是持续存在的挑战。本研究旨在探究第二语言句法-韵律接口的石化与稳定性特征。

Method: 研究采用BLCU-SAIT语料库，对比分析67名汉语母语者与67名越南语学习者。通过整合C-ToBI韵律边界标注体系与依存语法分析框架，系统考察韵律边界数量及其与句法关系的对应映射模式。

Result: 研究发现非线性习得路径：高级水平学习者(VNH)在主要短语层级(B3)的边界数量上已达到母语者基准，但结构映射显著偏离。具体表现为在主语-动词(SBV)接口上降级韵律边界(从Major Phrase B3降至Prosodic Word B1)，同时在动词-宾语(VOB)接口上错误升级边界(从Prosodic Word B1升至Major Phrase B3)。

Conclusion: 这种映射策略使学习者能够维持较高的长短语输出能力，却以牺牲结构准确性为代价，最终形成母语韵律层级被颠倒的扭曲系统，揭示了第二语言句法-韵律接口石化的深层认知机制。

Abstract: While second language (L2) learners may acquire target syntactic word order, mapping this syntax onto appropriate prosodic structures remains a persistent challenge. This study investigates the fossilization and stability of the L2 syntax-prosody interface by comparing 67 native Mandarin speakers with 67 Vietnamese learners using the BLCU-SAIT corpus. By integrating C-ToBI boundary annotation with Dependency Grammar analysis, we examined both the quantity of prosodic boundaries and their mapping to syntactic relations. Results reveal a non-linear acquisition: although high-proficiency learners (VNH) converge to the native baseline in boundary quantity at the Major Phrase level (B3), their structural mapping significantly diverges. Specifically, VNH demote the prosodic boundary at the Subject-Verb (SBV) interface (Major Phrase B3 -> Prosodic Word B1), while erroneously promoting the boundary at the Verb-Object (VOB) interface (Prosodic Word B1 -> Major Phrase B3). This strategy allows learners to maintain high long phrasal output at the expense of structural accuracy. This results in a distorted prosodic hierarchy where the native pattern is inverted.

</details>


### [36] [Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent](https://arxiv.org/abs/2602.23079)
*Boyang Zhang,Yang Zhang*

Main category: cs.CL

TL;DR: 针对LLM增强的作者推断能力带来的去匿名化风险，本文开发了可解释的智能体框架SALA，通过融合文体计量特征与LLM推理实现高精度作者识别，并提出引导重写策略在保留语义的同时降低可识别性，揭示了LLM在隐私威胁与防御中的双重作用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型强大的作者推断能力导致新闻文章等文本数据面临意外去匿名化风险，亟需可解释、主动式的防御机制来保护作者隐私。

Method: 提出SALA（Stylometry-Assisted LLM Analysis）方法，将定量文体计量特征与LLM推理结合，构建结构化、可解释的作者归属流程；通过数据库模块增强上下文，并设计引导重写策略，利用智能体推理轨迹生成改写提示。

Result: 在大规模新闻数据集上，SALA实现高推断准确率，数据库模块进一步提升性能；引导重写策略能有效降低作者可识别性同时保持文本原意。

Conclusion: 研究表明LLM智能体兼具去匿名化威胁与防御潜力，强调可解释、主动式隐私保护机制的重要性，为文本数据作者隐私安全提供了新思路。

Abstract: The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.

</details>


### [37] [Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs](https://arxiv.org/abs/2602.23136)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 多模态大语言模型无法捕捉声音纹理等细节，并非编码失败，而是解码器错配——文本训练的解码器只能提取文本对齐方向的信息，导致大量模态特定信息被当作噪声丢弃。


<details>
  <summary>Details</summary>
Motivation: 探究多模态LLM无法保留说话人身份、情感和视觉纹理等模态特定信息的根本原因，验证该现象是源于编码器缺陷还是解码器限制。

Method: 通过线性探测实验测量信息在各层的存活率；移除模态特定方差分析对解码器损失的影响；形式化广义互信息(GMI)理论边界；在五个语音/视觉模型上验证；设计仅编码器文本对齐不同的Prismatic VLM对照实验；采用LoRA干预训练情感目标。

Result: 说话人身份、情感和视觉属性在所有LLM层中均得以保留（线性探测比随机高3-55倍）；移除64-71%模态特定方差反而改善解码器损失；对照实验确认瓶颈在于解码器评分规则而非编码器；LoRA情感训练使情感可及性提升7.5%且不影响其他属性。

Conclusion: 多模态LLM的局限性源于解码器与编码表征的错配问题，文本预训练使解码器仅能访问文本对齐方向的信息。该边界是解码器评分规则的固有属性，通过调整训练目标可选择性地增强特定属性的可及性。

Abstract: Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.
  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.

</details>


### [38] [MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations](https://arxiv.org/abs/2602.23184)
*Sara Rosenthal,Yannis Katsis,Vraj Shah,Lihong He,Lucian Popa,Marina Danilevsky*

Main category: cs.CL

TL;DR: 本文提出了MTRAG-UN基准测试，用于探索多轮检索增强生成面临的开集挑战。该基准包含666个任务、超过2,800个对话轮次，涵盖6个领域及相应语料库。实验表明，检索和生成模型在处理无法回答、信息不足、非独立问题和模糊响应时仍存在困难。


<details>
  <summary>Details</summary>
Motivation: 当前多轮检索增强生成在实际应用中面临诸多挑战，但缺乏专门针对这些开集问题的基准测试来系统评估和推动该领域发展。

Method: 构建了包含666个任务、超过2,800个对话轮次的基准数据集，覆盖6个不同领域，并配备相应的语料库用于多轮检索增强生成场景的评估。

Result: 实验结果显示，现有的检索和生成模型在处理四类特定问题——"无法回答的问题"、"信息不足的问题"、"非独立问题"和"模糊响应"——时表现不佳，持续存在困难。

Conclusion: MTRAG-UN基准为研究多轮检索增强生成的开集挑战提供了重要工具，揭示了当前模型的薄弱环节，将有助于推动更鲁棒的多轮对话系统的发展。

Abstract: We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com/IBM/mt-rag-benchmark

</details>


### [39] [Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models](https://arxiv.org/abs/2602.23197)
*Chungpa Lee,Jy-yong Sohn,Kangwook Lee*

Main category: cs.CL

TL;DR: 本文通过线性注意力模型理论分析微调对Transformer大语言模型上下文学习能力的影响，发现全面微调损害少样本性能，而仅微调值矩阵可兼顾零样本与少样本性能，辅助少样本损失则带来目标任务的提升与未见任务的权衡。


<details>
  <summary>Details</summary>
Motivation: 微调虽能提升大语言模型的零样本性能，却常削弱其上下文学习能力，限制了模型在未见任务上的表现。本研究旨在从理论上刻画这一现象，为设计更优微调策略提供依据。

Method: 研究采用线性注意力模型作为理论框架，分析微调目标函数对注意力参数的修改机制，并识别导致少样本性能下降的条件。

Result: 理论分析表明：全面微调注意力参数会损害上下文学习；限制更新于值矩阵可提升零样本性能且保留少样本能力；辅助少样本损失增强目标任务表现但削弱未见任务的适应能力。

Conclusion: 本研究揭示了微调影响上下文学习的内在机制，提出了参数选择性更新策略，为平衡零样本与少样本性能提供了理论指导，对开发灵活高效的AI系统具有重要意义。

Abstract: Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.

</details>


### [40] [Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?](https://arxiv.org/abs/2602.23225)
*Pengxiang Li,Dilxat Muhtar,Lu Yin,Tianlong Chen,Shiwei Liu*

Main category: cs.CL

TL;DR: 该论文揭示扩散语言模型(DLMs)虽宣传支持并行生成，但实践中常退化为自回归(AR)类解码，主要因DLM目标与顺序训练数据不匹配。提出以数据为中心的NAP方法，通过构建多重独立推理轨迹并配合并行强制解码策略，在数学推理基准测试中实现了优于标准长思维链训练DLM的并行解码性能，且性能增益随并行度提升而扩大。


<details>
  <summary>Details</summary>
Motivation: 真正非自回归(non-AR)并行生成具有重要价值：消除顺序瓶颈、更好利用并行硬件、降低同步/通信开销、改善随输出长度扩展的延迟。然而现有DLMs常收敛至左到右AR类解码，这归因于DLM目标与广泛使用的顺序结构训练数据（包括标准预训练语料和长思维链(CoT)监督）之间的失配。

Method: 提出NAP（非自回归并行DLMs）方法，这是一种概念验证性质的数据为中心方法：1）将示例整理为多个独立推理轨迹；2）采用并行强制解码策略，鼓励多token并行更新，使监督与并行解码更好对齐。

Result: 在数学推理基准测试中，NAP在并行解码条件下表现优于使用标准长思维链数据训练的DLM，且性能优势随并行度增加而进一步扩大。

Conclusion: 重新审视数据和监督方式是缓解AR类行为、推动DLM向真正非自回归并行生成发展的原则性方向。

Abstract: Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch between DLM objectives and the highly sequential structure of widely used training data, including standard pretraining corpora and long chain-of-thought (CoT) supervision. Motivated by this diagnosis, we propose NAP (Non-Autoregressive Parallel DLMs), a proof-of-concept, data-centric approach that better aligns supervision with non-AR parallel decoding. NAP curates examples as multiple independent reasoning trajectories and couples them with a parallel-forced decoding strategy that encourages multi-token parallel updates. Across math reasoning benchmarks, NAP yields stronger performance under parallel decoding than DLMs trained on standard long CoT data, with gains growing as parallelism increases. Our results suggest that revisiting data and supervision is a principled direction for mitigating AR-like behavior and moving toward genuinely non-autoregressive parallel generation in DLMs. Our code is available at https://github.com/pixeli99/NAP.

</details>


### [41] [Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems](https://arxiv.org/abs/2602.23266)
*Siyuan Liu,Jiahui Xu,Feng Jiang,Kuang Wang,Zefeng Zhao,Chu-Ren Huang,Jinghang Gu,Changqing Yin,Haizhou Li*

Main category: cs.CL

TL;DR: 该论文针对传统级联式对话系统因顺序执行ASR-LLM-TTS导致响应延迟高的问题，提出DDTSR双轨流式响应框架。通过小大模型并行生成连接词、跨模态流式协作和课程学习连贯性增强三大机制，实现边听边思考、边说边思考，在两项基准测试中降低延迟19-51%且保持语篇质量，可作为即插即用模块适配多种大模型，具备强实用性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统语音对话系统采用ASR→LLM→TTS的严格顺序执行范式，必须等待完整转录和全部推理完成后才能开始语音合成，导致响应延迟过高，难以实现人类般的实时交互体验。降低延迟同时保持对话连贯性与逻辑一致性是亟待解决的关键挑战。

Method: 提出DDTSR框架，其核心为三机制：1）连接词引导的小大模型协同：小模型实时生成最小承诺性话语连接词，大模型并行执行知识密集型推理；2）流式跨模态协作：动态重叠ASR、LLM推理与TTS过程，将最早可发音时刻提前；3）课程学习增强的语篇连续性：确保早期响应与后续推理输出间的连贯性和逻辑一致性。

Result: 在两项语音对话基准测试中，DDTSR将响应延迟降低19%-51%，同时保持语篇质量。该框架可作为即插即用模块兼容多种大模型主干，对不同长度话语输入均保持鲁棒性，展现出良好的实用性和可扩展性。

Conclusion: DDTSR通过双轨流式架构和三大创新机制，有效解决了级联式对话系统的高延迟问题，在显著提升响应速度的同时维持了对话的连贯性与逻辑性。其模块化设计和强适配性使其成为实时语音交互场景中具有实用价值与推广潜力的解决方案。

Abstract: Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon three key mechanisms: (1) connective-guided small-large model synergy, where an auxiliary small model generates minimal-committal discourse connectives while a large model performs knowledge-intensive reasoning in parallel; (2) streaming-based cross-modal collaboration, which dynamically overlaps ASR, LLM inference, and TTS to advance the earliest speakable moment; and (3) curriculum-learning-based discourse continuity enhancement, which maintains coherence and logical consistency between early responses and subsequent reasoning outputs. Experiments on two spoken dialogue benchmarks demonstrate that DDTSR reduces response latency by 19%-51% while preserving discourse quality. Further analysis shows that DDTSR functions as a plug-and-play module compatible with diverse LLM backbones, and remains robust across varying utterance lengths, indicating strong practicality and scalability for real-time spoken interaction.

</details>


### [42] [A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations](https://arxiv.org/abs/2602.23300)
*Soumya Dutta,Smruthi Balaji,Sriram Ganapathy*

Main category: cs.CL

TL;DR: 本文提出MiSTER-E框架，一种用于对话情感识别的模块化混合专家模型。该框架解耦了模态特定上下文建模和多模态信息融合两大挑战，通过语音-文本专家混合、对比学习和KL散度正则化，在三个基准数据集上显著超越现有基线系统。


<details>
  <summary>Details</summary>
Motivation: 对话情感识别的核心挑战在于同时捕捉多轮对话的时序动态和多模态信息的有效融合。传统方法往往将模态特定建模与信息融合耦合处理，限制了模型性能。现有方法还存在依赖说话人身份信息的问题，降低了泛化性。

Method: MiSTER-E采用模块化混合专家架构：1) 使用微调的大型语言模型生成语音和文本的语句级嵌入；2) 通过卷积-循环层增强上下文建模；3) 设计三个专家（纯语音、纯文本、跨模态）并采用学习门控机制动态加权输出；4) 引入配对语音-文本表示的监督对比损失和专家预测间的KL散度正则化，促进模态一致性与对齐。整个系统不依赖说话人身份信息。

Result: 在IEMOCAP、MELD和MOSI三个基准数据集上分别达到70.9%、69.5%和87.9%的加权F1分数，显著优于多种语音-文本对话情感识别基线系统。消融实验验证了各模块的贡献。

Conclusion: MiSTER-E成功解耦了多模态对话情感识别的两大核心挑战，其模块化设计和一致性约束机制有效提升了跨模态情感理解能力。实验结果表明该框架具有优越性能，且无需说话人身份信息的设计增强了实用性，为多模态对话理解提供了新思路。

Abstract: Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.

</details>


### [43] [Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning](https://arxiv.org/abs/2602.23351)
*Amita Kamath,Jack Hessel,Khyathi Chandu,Jena D. Hwang,Kai-Wei Chang,Ranjay Krishna*

Main category: cs.CL

TL;DR: 该研究揭示了视觉-语言模型(VLMs)在空间、时间、否定和计数等推理能力上的不足源于训练数据中的报告偏差，即日常视觉描述常省略必要的隐性信息。尽管现有大规模甚至合成数据仍无法解决此问题，但针对性收集的显式标注能有效提升推理性能，强调需要更有意识的数据筛选而非单纯依赖规模扩展。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型推理能力欠缺已成为研究前沿问题。作者认为此现象源于训练数据中的报告偏差：人们在描述视觉内容时通常省略进行推理所需的隐性信息（如更倾向于说"今天在现场看球"而非"37人站在田野后的照片"）。这种偏差导致模型难以学习到空间、时间、否定和计数等核心推理技能。

Method: 研究基于语用学理论，系统分析了OpenCLIP、LLaVA-1.5和Molmo等主流VLM的训练数据。通过设计针对四种推理技能（空间、时间、否定、计数）的精选基准测试集，实证评估了报告偏差对模型能力的影响。

Result: 研究发现：(i) VLMs在数据中受报告偏差抑制的四类推理任务上表现不佳；(ii) 与普遍认知相反，扩大数据规模、模型参数或多语言扩展并不能默认涌现这些推理能力；(iii) 但专门收集以获取隐性信息的有标注数据能有效提升相关推理性能。

Conclusion: 研究结果强调，依赖规模扩展无法自动获得推理能力，必须采用更有意识的数据筛选和标注方法，明确注入隐性信息，才能有效提升VLM的推理能力。

Abstract: The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., "at the game today!" is a more likely caption than "a photo of 37 people standing behind a field". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning](https://arxiv.org/abs/2602.22227)
*Yicheng Bao,Xuhong Wang,Xin Tan*

Main category: cs.LG

TL;DR: 针对多模态大模型在复杂视觉场景下的感知脆弱性问题，本文提出AOT自对抗训练框架，通过攻击者-防御者的协同演化自动生成大规模对抗数据，显著提升模型鲁棒性并减少幻觉，为训练更可靠的多模态AI提供了可扩展范式。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLMs)在面对视觉复杂场景时表现出感知脆弱性，其根源在于依赖有限训练数据集，这些数据集不仅成本高昂难以扩展，还对模型鲁棒性构成了天花板效应。

Method: 作者首先构建了大规模对抗数据集AOT-SFT，进而提出AOT(Adversarial Opponent Training)自对抗训练框架。该方法通过协调图像编辑攻击者(Attacker)与防御者(Defender)MLLM之间的协同演化，使攻击者生成多样化、动态化的图像操作课程，迫使防御者持续适应与提升。

Result: 大量实验表明，AOT框架能有效增强防御者的感知鲁棒性并显著降低幻觉现象，同时建立了可扩展的训练范式，突破了传统手动标注数据集在规模和成本上的限制。

Conclusion: AOT框架通过自对抗学习机制自动生成高质量对抗样本，为提升多模态大模型的鲁棒性提供了一种高效且可扩展的解决方案，是构建更可靠多模态AI系统的重要进展。

Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustness. We introduce \textbf{AOT-SFT}, a large-scale adversarial dataset for bootstrapping MLLM robustness. Building on this, we propose \textbf{AOT (Adversarial Opponent Training)}, a self-play framework that forges MLLM robustness by creating its own training data. Our method orchestrates a co-evolution between an image-editing Attacker and a Defender MLLM, where the Attacker generates a diverse and dynamic curriculum of image manipulations, forcing the Defender to adapt and improve. Extensive experiments demonstrate that AOT enhances the Defender's perceptual robustness and reduces hallucinations, establishing a scalable paradigm for training more reliable MLLMs.

</details>


### [45] [Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials](https://arxiv.org/abs/2602.22251)
*Alex Morehead,Miruna Cretu,Antonia Panescu,Rishabh Anand,Maurice Weiler,Tynan Perez,Samuel Blau,Steven Farrell,Wahid Bhimji,Anubhav Jain,Hrushikesh Sahasrabuddhe,Pietro Lio,Tommi Jaakkola,Rafael Gomez-Bombarelli,Rex Ying,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: Zatom-1是首个统一3D分子和材料生成与预测的基础模型，采用多模态流匹配Transformer架构，通过联合建模离散原子类型和连续3D几何结构，在保持生成质量的同时将推理时间缩短一个数量级，并实现了跨化学域的正向迁移。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法局限于单一化学域（分子或材料）和单一任务（生成或预测），阻碍了表征共享和知识迁移，无法充分利用跨域化学知识。

Method: 提出Zatom-1，一个基于Transformer的多模态流匹配模型，联合建模离散原子类型和连续3D几何结构，通过可扩展的生成式预训练获得通用初始化参数，用于下游多任务性质、能量和力的预测。

Result: 在生成和预测基准测试中匹配或超越专用基线模型，生成推理时间降低超过10倍，且预训练中建模材料能提升分子性质预测精度，证实跨化学域正向迁移。

Conclusion: 统一生成式预训练可作为通用初始化策略，实现跨化学域知识迁移，为通用化学AI模型开辟了新方向。

Abstract: General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation sharing and transfer. We introduce Zatom-1, the first foundation model that unifies generative and predictive learning of 3D molecules and materials. Zatom-1 is a Transformer trained with a multimodal flow matching objective that jointly models discrete atom types and continuous 3D geometries. This approach supports scalable pretraining with predictable gains as model capacity increases, while enabling fast and stable sampling. We use joint generative pretraining as a universal initialization for downstream multi-task prediction of properties, energies, and forces. Empirically, Zatom-1 matches or outperforms specialized baselines on both generative and predictive benchmarks, while reducing the generative inference time by more than an order of magnitude. Our experiments demonstrate positive predictive transfer between chemical domains from joint generative pretraining: modeling materials during pretraining improves molecular property prediction accuracy.

</details>


### [46] [Code World Models for Parameter Control in Evolutionary Algorithms](https://arxiv.org/abs/2602.22260)
*Camilo Chacón Sartori,Guillem Rodríguez Corominas*

Main category: cs.LG

TL;DR: 本文将代码世界模型(CWMs)从确定性游戏扩展到随机组合优化问题，使大语言模型能从次优轨迹学习优化器行为并控制其决策。在无需最优轨迹或先知知识的情况下，CWM-greedy在多个基准测试中表现优异，显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型是否能够学习优化器的动态行为，并利用所学知识对其进行有效控制，从而将代码世界模型的适用范围从确定性游戏扩展到随机组合优化领域。

Method: 给定(1+1)-RLS_k优化器的次优轨迹，使用大语言模型合成该优化器的动态模拟器；随后在该模拟器上进行贪心规划，动态选择每一步的变异强度k，实现对优化器的控制。

Result: 在\lo{}和\onemax{}函数上，CWM-greedy性能接近理论最优策略（差距仅6%）；在\jump{$_k$}函数上，所有自适应基线均失败（0%成功率），而CWM-greedy达到100%成功率；在NK-Landscape上显著优于基线（36.94 vs 36.32；p<0.001）；相比DQN，样本效率更高（200离线轨迹 vs 500在线回合）、成功率更高（100% vs 58%）、泛化能力更强（k=3时：78% vs 0%）。

Conclusion: 该方法成功扩展了代码世界模型到随机组合优化领域，证明大语言模型能够从次优轨迹学习优化器动态，并通过贪心规划有效控制优化器，在多种基准测试中表现卓越且具有良好的鲁棒性。

Abstract: Can an LLM learn how an optimizer behaves -- and use that knowledge to control it? We extend Code World Models (CWMs), LLM-synthesized Python programs that predict environment dynamics, from deterministic games to stochastic combinatorial optimization. Given suboptimal trajectories of $(1{+}1)$-$\text{RLS}_k$, the LLM synthesizes a simulator of the optimizer's dynamics; greedy planning over this simulator then selects the mutation strength $k$ at each step. On \lo{} and \onemax{}, CWM-greedy performs within 6\% of the theoretically optimal policy -- without ever seeing optimal-policy trajectories. On \jump{$_k$}, where a deceptive valley causes all adaptive baselines to fail (0\% success rate), CWM-greedy achieves 100\% success rate -- without any collection policy using oracle knowledge of the gap parameter. On the NK-Landscape, where no closed-form model exists, CWM-greedy outperforms all baselines across fifteen independently generated instances ($36.94$ vs.\ $36.32$; $p<0.001$) when the prompt includes empirical transition statistics. The CWM also outperforms DQN in sample efficiency (200 offline trajectories vs.\ 500 online episodes), success rate (100\% vs.\ 58\%), and generalization ($k{=}3$: 78\% vs.\ 0\%). Robustness experiments confirm stable synthesis across 5 independent runs.

</details>


### [47] [Sustainable LLM Inference using Context-Aware Model Switching](https://arxiv.org/abs/2602.22261)
*Yuvarani,Akashdeep Singh,Zahra Fathanah,Salsabila Harlen,Syeikha Syafura Al-Zahra binti Zahari,Hema Subramaniam*

Main category: cs.LG

TL;DR: 该论文提出上下文感知模型切换方案，通过动态选择匹配查询复杂度的语言模型，实现高达67.5%的能耗降低，同时保持93.6%的响应质量，为构建可持续AI系统提供了实用且可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型部署采用"一刀切"的推理策略，无论任务复杂度如何都将所有请求路由至同一大型模型，造成严重的能源浪费。随着模型规模持续扩大，能耗问题已成为制约AI可持续发展的关键瓶颈，亟需开发能效优化的推理机制。

Method: 提出一种上下文感知模型切换框架，结合四大核心技术：缓存机制处理重复查询、基于规则的复杂度评分实现快速可解释决策、机器学习分类器捕捉查询语义意图，以及用户自适应组件从长期交互模式中学习优化。系统根据实时评估的查询复杂度，在计算成本不同的模型（Gemma3 1B、Gemma3 4B、Qwen3 4B）间动态切换。

Result: 在真实对话工作负载上的实验表明：相比始终使用最大模型的基线，该方案能耗降低最高达67.5%，响应质量保持93.6%（BERTScore F1指标），简单查询响应时间提升约68%。综合评估涵盖能耗（NVML GPU功率遥测）、延迟、路由准确性和输出质量等多维度指标，验证了方法在真实场景下的有效性。

Conclusion: 模型切换推理为构建能效更高、更可持续的AI系统提供了切实可行的技术路径，证明在不显著牺牲服务质量的前提下可实现显著的效率提升。该研究为AI部署的绿色转型奠定了重要基础，展示了在实际应用中平衡性能与能耗的巨大潜力。

Abstract: Large language models have become central to many AI applications, but their growing energy consumption raises serious sustainability concerns. A key limitation in current AI deployments is the reliance on a one-size-fits-all inference strategy where most systems route every request to the same large model, regardless of task complexity, leading to substantial and unnecessary energy waste. To address this issue, we propose a context-aware model switching approach that dynamically selects an appropriate language model based on query complexity. The proposed system uses a Context-Aware Model Switching for Energy-Efficient LLM Inference that combines caching for repeated queries, rulebased complexity scoring for fast and explainable decisions, machine learning classification to capture semantic intent, and a user-adaptive component that learns from interaction patterns over time. The proposed architecture was evaluated using real conversation workloads and three open-source language models (Gemma3 1B, Gemma3 4B and Qwen3 4B) with different computational costs, measuring energy consumption (via NVML GPU power telemetry), response latency, routing accuracy, and output quality (BERTScore F1) to reflect real-world usage conditions. Experimental results show that the model switching approach can reduce energy consumption by up to 67.5% compared to always using the largest model while maintaining a response quality of 93.6%. In addition, the response time for simple queries also improved significantly by approximately 68%. These results show that model switching inference offers a practical and scalable path toward more energy-efficient and sustainable AI systems, demonstrating that significant efficiency gains can be achieved without major sacrifices in response quality.

</details>


### [48] [Entropy-Controlled Flow Matching](https://arxiv.org/abs/2602.22265)
*Chika Maduabuchi*

Main category: cs.LG

TL;DR: 针对流匹配中的语义模式耗尽问题，本文提出熵控制流匹配（ECFM），通过约束变分原理强制熵率预算，在Wasserstein空间中形成凸优化框架，并证明其模式覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 标准流匹配目标不直接控制轨迹的信息几何结构，允许低熵瓶颈存在，导致语义模式被瞬时耗尽。

Method: 提出熵控制流匹配（ECFM），一种在连续性方程路径上的约束变分原理，强制满足全局熵率预算d/dt H(mu_t) ≥ -λ。该方法在Wasserstein空间构成凸优化问题，具有KKT/Pontryagin系统，等价于带显式熵乘子的薛定谔桥问题。在纯传输机制下，ECFM退化为熵最优传输测地线，且当λ→0时Gamma收敛于经典最优传输。

Result: 获得了证书式的模式覆盖与密度下限保证，并具备Lipschitz稳定性；同时构造了无约束流匹配的近似最优坍缩反例。

Conclusion: ECFM通过熵控制机制有效解决了流匹配中的模式坍缩问题，在理论上提供了严格的保证，并在数学上与最优传输和薛定谔桥问题建立了深刻联系。

Abstract: Modern vision generators transport a base distribution to data through time-indexed measures, implemented as deterministic flows (ODEs) or stochastic diffusions (SDEs). Despite strong empirical performance, standard flow-matching objectives do not directly control the information geometry of the trajectory, allowing low-entropy bottlenecks that can transiently deplete semantic modes. We propose Entropy-Controlled Flow Matching (ECFM): a constrained variational principle over continuity-equation paths enforcing a global entropy-rate budget d/dt H(mu_t) >= -lambda. ECFM is a convex optimization in Wasserstein space with a KKT/Pontryagin system, and admits a stochastic-control representation equivalent to a Schrodinger bridge with an explicit entropy multiplier. In the pure transport regime, ECFM recovers entropic OT geodesics and Gamma-converges to classical OT as lambda -> 0. We further obtain certificate-style mode-coverage and density-floor guarantees with Lipschitz stability, and construct near-optimal collapse counterexamples for unconstrained flow matching.

</details>


### [49] [Data-Driven Supervision of a Thermal-Hydraulic Process Towards a Physics-Based Digital Twin](https://arxiv.org/abs/2602.22267)
*Osimone Imhogiemhe,Yoann Jus,Hubert Lejeune,Saïd Moussaoui*

Main category: cs.LG

TL;DR: 本文提出了一种基于数字孪生概念的热液压过程故障检测与诊断方法。通过结合数值模拟和机器学习技术，开发了用于过程参数变化检测与在线估计的模块，并在单参数突变场景下验证了算法定位参数变化并更新数值的准确性。


<details>
  <summary>Details</summary>
Motivation: 工业生产过程的实时监控是跨行业面临的共同挑战，旨在确保安全、连续生产和高效运行。先进的物理系统仿真工具与数据驱动的机器学习模型的发展，以及数字孪生概念的兴起，为解决这些挑战提供了新的框架和可能性。

Method: 基于系统数值模拟和机器学习方法，构建数字孪生框架，设计专门的模块用于检测过程参数变化并进行在线估计，实现对热液压系统的故障检测与诊断。

Result: 在单参数一次性变化的测试场景下，所提算法在参数变化定位和数值更新方面表现出良好的准确性。

Conclusion: 数字孪生框架结合数值模拟与机器学习的方法能有效应用于热液压过程的故障检测与诊断，为工业过程实时监控提供了可行解决方案。

Abstract: The real-time supervision of production processes is a common challenge across several industries. It targets process component monitoring and its predictive maintenance in order to ensure safety, uninterrupted production and maintain high efficiency level. The rise of advanced tools for the simulation of physical systems in addition to data-driven machine learning models offers the possibility to design numerical tools dedicated to efficient system monitoring. In that respect, the digital twin concept presents an adequate framework that proffers solution to these challenges. The main purpose of this paper is to develop such a digital twin dedicated to fault detection and diagnosis in the context of a thermal-hydraulic process supervision. Based on a numerical simulation of the system, in addition to machine learning methods, we propose different modules dedicated to process parameter change detection and their on-line estimation. The proposed fault detection and diagnosis algorithm is validated on a specific test scenario, with single one-off parameter change occurrences in the system. The numerical results show good accuracy in terms of parameter variation localization and the update of their values.

</details>


### [50] [AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2602.22268)
*Changhai Zhou,Shiyang Zhang,Yuhua Zhou,Qian Qiao,Jun Gao,Cheng Jin,Kaizhou Qin,Weizhong Zhang*

Main category: cs.LG

TL;DR: 本文提出AutoQRA框架，通过联合优化每层的量化位宽和LoRA秩配置，解决了在GPU内存受限条件下量化与参数高效微调之间的交互问题，在接近全精度微调性能的同时，实现了与统一4-bit方法相当的内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有的量化与参数高效微调（如LoRA）的序贯流程未能利用量化位宽与LoRA秩之间的复杂交互关系。低量化误差的优化配置并不总能转化为强微调性能，且在相同内存预算下，不同位宽和秩的组合会导致显著差异的结果，这限制了在资源受限场景下的模型适应效果。

Method: AutoQRA采用两阶段优化框架：第一阶段进行全局多保真度进化搜索，通过注入层重要性先验来预热初始种群，并运用特定算子和性能模型高效筛选候选配置；第二阶段应用信任域贝叶斯优化，在搜索空间中有前景的区域进行局部精调，以在给定内存预算下找到最优配置，从而在训练过程中主动补偿特定层的量化噪声。

Result: 实验表明，AutoQRA在保持与统一4-bit量化方法相近内存占用的同时，其性能表现接近全精度微调的水平，验证了联合优化位宽与LoRA秩的有效性。

Conclusion: AutoQRA通过联合优化量化与LoRA参数配置，为内存受限环境下的模型微调提供了新范式，其两阶段搜索策略有效平衡了搜索效率与精度，实现了性能与资源消耗的最佳权衡。

Abstract: Quantization followed by parameter-efficient fine-tuning has emerged as a promising paradigm for downstream adaptation under tight GPU memory constraints. However, this sequential pipeline fails to leverage the intricate interaction between quantization bit-width and LoRA rank. Specifically, a carefully optimized quantization allocation with low quantization error does not always translate to strong fine-tuning performance, and different bit-width and rank configurations can lead to significantly varying outcomes under the same memory budget. To address this limitation, we propose AutoQRA, a joint optimization framework that simultaneously optimizes the bit-width and LoRA rank configuration for each layer during the mixed quantized fine-tuning process. To tackle the challenges posed by the large discrete search space and the high evaluation cost associated with frequent fine-tuning iterations, AutoQRA decomposes the optimization process into two stages. First, it first conducts a global multi-fidelity evolutionary search, where the initial population is warm-started by injecting layer-wise importance priors. This stage employs specific operators and a performance model to efficiently screen candidate configurations. Second, trust-region Bayesian optimization is applied to locally refine promising regions of the search space and identify optimal configurations under the given memory budget. This approach enables active compensation for quantization noise in specific layers during training. Experiments show that AutoQRA achieves performance close to full-precision fine-tuning with a memory footprint comparable to uniform 4-bit methods.

</details>


### [51] [CQSA: Byzantine-robust Clustered Quantum Secure Aggregation in Federated Learning](https://arxiv.org/abs/2602.22269)
*Arnab Nath,Harsh Kasyap*

Main category: cs.LG

TL;DR: 本文提出了一种名为聚类量子安全聚合（CQSA）的模块化框架，用于量子辅助的联邦学习。CQSA将客户端随机划分为小集群，每个集群利用高保真度的低量子比特GHZ态进行本地量子聚合，服务器通过余弦相似度和欧氏距离等统计关系识别恶意贡献。在去极化噪声的理论分析与仿真实验表明，CQSA在保持模型稳定收敛的同时，显著提升了状态保真度，优于全局QSA协议。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能避免原始数据共享，但本地模型更新仍易受推断攻击和投毒攻击。现有量子安全聚合（QSA）方案依赖大规模GHZ态，导致保真度随客户端数量迅速下降，且全局聚合无法检测拜占庭客户端。为此，需要一种既能适配近期量子硬件物理限制，又能抵御拜占庭攻击的聚合机制。

Method: 1) 将客户端随机划分为若干小集群；2) 每个集群使用低量子比特GHZ态进行本地量子聚合；3) 服务器收集各集群的聚合结果，采用余弦相似度、欧氏距离等统计度量分析集群间关系，识别并剔除恶意贡献；4) 在去极化噪声模型下进行理论分析与仿真验证。

Result: 实验与理论结果表明，CQSA在噪声环境下仍能保持模型的稳定收敛，且相较于全局QSA，其状态保真度显著更高，能够有效检测并抵御拜占庭客户端的攻击。

Conclusion: CQSA框架通过聚类方式实现了量子安全聚合的模块化和可扩展性，兼顾了量子硬件的物理约束与联邦学习对拜占庭鲁棒性的需求，为未来量子辅助联邦学习提供了可行方案。

Abstract: Federated Learning (FL) enables collaborative model training without sharing raw data. However, shared local model updates remain vulnerable to inference and poisoning attacks. Secure aggregation schemes have been proposed to mitigate these attacks. In this work, we aim to understand how these techniques are implemented in quantum-assisted FL. Quantum Secure Aggregation (QSA) has been proposed, offering information-theoretic privacy by encoding client updates into the global phase of multipartite entangled states. Existing QSA protocols, however, rely on a single global Greenberger-Horne-Zeilinger (GHZ) state shared among all participating clients. This design poses fundamental challenges: fidelity of large-scale GHZ states deteriorates rapidly with the increasing number of clients; and (ii) the global aggregation prevents the detection of Byzantine clients. We propose Clustered Quantum Secure Aggregation (CQSA), a modular aggregation framework that reconciles the physical constraints of near-term quantum hardware along with the need for Byzantine-robustness in FL. CQSA randomly partitions the clients into small clusters, each performing local quantum aggregation using high-fidelity, low-qubit GHZ states. The server analyzes statistical relationships between cluster-level aggregates employing common statistical measures such as cosine similarity and Euclidean distance to identify malicious contributions. Through theoretical analysis and simulations under depolarizing noise, we demonstrate that CQSA ensures stable model convergence, achieves superior state fidelity over global QSA.

</details>


### [52] [Prior Knowledge-enhanced Spatio-temporal Epidemic Forecasting](https://arxiv.org/abs/2602.22270)
*Sijie Ruan,Jinyu Li,Jia Wei,Zenghao Xu,Jie Bao,Junshi Xu,Junyang Qiu,Hanning Yuan,Xiaoxiao Wang,Shuliang Wang*

Main category: cs.LG

TL;DR: 本文提出STOEP混合框架，通过融合隐式时空先验与显式专家先验，解决疫情预测中对弱信号不敏感、空间关系过度简化及参数估计不稳定三大挑战。在COVID-19和流感真实数据集上相比最佳基线提升11.1% RMSE，已在中国某省级CDC部署应用。


<details>
  <summary>Details</summary>
Motivation: 现有时空疫情预测方法存在显著局限：难以捕捉微弱疫情信号、对复杂区域间空间依赖关系建模过于简化，以及参数估计过程不稳定，这些缺陷限制了公共卫生管理的精准决策能力。

Method: 提出Spatio-Temporal priOr-aware Epidemic Predictor (STOEP) 框架，包含三大核心组件：1) 病例感知邻接学习(CAL)，利用历史感染模式动态校正基于人口流动的区域依赖关系；2) 空间信息参数估计(SPE)，通过可学习空间先验机制增强微弱疫情信号；3) 基于滤波的机制性预测(FMF)，采用专家知识引导的自适应阈值策略实现参数正则化。

Result: 在真实世界COVID-19和流感数据集上的系统性实验验证，STOEP框架相较于最优基线方法在RMSE指标上取得11.1%的显著提升，且该系统已成功部署于中国某省级疾病预防控制中心支持实际应用。

Conclusion: STOEP通过有效融合数据驱动的时空先验与知识驱动的专家先验，显著提升了疫情预测的准确性与稳定性，为公共卫生决策提供了可落地的技术工具，体现了从算法创新到实际部署的完整价值闭环。

Abstract: Spatio-temporal epidemic forecasting is critical for public health management, yet existing methods often struggle with insensitivity to weak epidemic signals, over-simplified spatial relations, and unstable parameter estimation. To address these challenges, we propose the Spatio-Temporal priOr-aware Epidemic Predictor (STOEP), a novel hybrid framework that integrates implicit spatio-temporal priors and explicit expert priors. STOEP consists of three key components: (1) Case-aware Adjacency Learning (CAL), which dynamically adjusts mobility-based regional dependencies using historical infection patterns; (2) Space-informed Parameter Estimating (SPE), which employs learnable spatial priors to amplify weak epidemic signals; and (3) Filter-based Mechanistic Forecasting (FMF), which uses an expert-guided adaptive thresholding strategy to regularize epidemic parameters. Extensive experiments on real-world COVID-19 and influenza datasets demonstrate that STOEP outperforms the best baseline by 11.1% in RMSE. The system has been deployed at one provincial CDC in China to facilitate downstream applications.

</details>


### [53] [Support Tokens, Stability Margins, and a New Foundation for Robust LLMs](https://arxiv.org/abs/2602.22271)
*Deepak Agarwal,Dhyey Dharmendrakumar Mavani,Suyash Gupta,Karthik Sethuraman,Tejas Dharamsi*

Main category: cs.LG

TL;DR: 论文从概率视角重构因果自注意力机制，发现变量变换导致的参数屏障约束形成令牌空间的结构化几何，产生类似SVM边缘的"支持令牌"概念。通过贝叶斯框架推导出只需在交叉熵损失添加对数屏障惩罚的MAP目标，使LLM训练更鲁棒而不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制虽被描述为内容自适应的信息混合方式，但缺乏对其深层结构的概率理解。本研究旨在仿照经典PCA扩展至概率PCA的思路，从概率框架重新诠释因果自注意力Transformer——现代大语言模型的核心架构，以揭示其内在约束结构，并理论化理解LLM解码过程中的动态行为。

Method: 1. 构建因果自注意力层的概率生成模型 2. 通过变量变换分析揭示参数空间的屏障约束 3. 研究约束诱导的令牌空间结构化几何性质 4. 将序列建模形式化为令牌空间幂集上的随机过程 5. 推导贝叶斯后验最大后验(MAP)估计框架 6. 设计仅需对标准交叉熵损失添加平滑对数屏障惩罚的实用训练目标

Result: 1. 理论证明自注意力参数存在隐式屏障约束，导致令牌空间呈现高度结构化几何 2. 识别出注意力机制病态性的临界边界，形成与SVM边缘类似的理论解释，并自然引出"支持令牌"概念 3. 为LLM解码动态提供新的理论洞察 4. 提出极简易行的训练改进方案：在标准损失函数中加入对数屏障惩罚项 5. 实验验证该方法可在不牺牲样本外准确性的前提下显著提升模型鲁棒性

Conclusion: 本研究通过概率建模方法，从因果自注意力机制中挖掘出参数屏障约束和结构化几何特性，提出了支持令牌的理论概念。将LLM重新定义为幂集上的随机过程，并建立完整的贝叶斯框架，最终导出一个实践性强、只需简单修改损失函数的正则化方法。该工作既深化了对Transformer基础架构的理论理解，又为大语言模型鲁棒性训练提供了有效途径。

Abstract: Self-attention is usually described as a flexible, content-adaptive way to mix a token with information from its past. We re-interpret causal self-attention transformers, the backbone of modern foundation models, within a probabilistic framework, much like how classical PCA is extended to probabilistic PCA. However, this re-formulation reveals a surprising and deeper structural insight: due to a change-of-variables phenomenon, a barrier constraint emerges on the self-attention parameters. This induces a highly structured geometry on the token space, providing theoretical insights into the dynamics of LLM decoding. This reveals a boundary where attention becomes ill-conditioned, leading to a margin interpretation similar to classical support vector machines. Just like support vectors, this naturally gives rise to the concept of support tokens.
  Furthermore, we show that LLMs can be interpreted as a stochastic process over the power set of the token space, providing a rigorous probabilistic framework for sequence modeling. We propose a Bayesian framework and derive a MAP estimation objective that requires only a minimal modification to standard LLM training: the addition of a smooth log-barrier penalty to the usual cross-entropy loss. We demonstrate that this provides more robust models without sacrificing out-of-sample accuracy and that it is straightforward to incorporate in practice.

</details>


### [54] [X-REFINE: XAI-based RElevance input-Filtering and archItecture fiNe-tuning for channel Estimation](https://arxiv.org/abs/2602.22277)
*Abdul Karim Gizzini,Yahia Medjahdi*

Main category: cs.LG

TL;DR: 本文针对6G无线通信中深度学习模型的黑盒特性和高复杂性限制实际应用的问题，提出X-REFINE框架，通过分解式符号稳定LRP epsilon规则实现输入过滤与架构微调联合优化，在降低计算复杂度的同时保持稳健的误比特率性能，实现可解释性-性能-复杂度的卓越权衡。


<details>
  <summary>Details</summary>
Motivation: 6G无线通信需要AI原生架构，但用于信道估计等关键应用的深度学习模型存在黑盒特性与高复杂性，限制了实际部署。现有基于扰动的XAI解决方案仅关注输入过滤，忽略内部结构优化，无法实现全局优化。

Method: 提出X-REFINE框架，采用分解式符号稳定LRP epsilon规则，将预测结果反向传播以推导子载波和隐藏神经元的高分辨率相关性分数，实现输入过滤与架构微调的双向优化，识别最可靠的模型组件。

Result: 仿真结果表明，X-REFINE框架实现了可解释性-性能-复杂度的优越权衡，在不同场景下显著降低计算复杂度，同时保持稳健的误比特率(BER)性能。

Conclusion: X-REFINE通过联合优化策略有效解决了6G通信中AI模型的可解释性与复杂度矛盾，为深度学习在关键无线通信应用中的实际部署提供了可行路径，具有重要的理论和实践价值。

Abstract: AI-native architectures are vital for 6G wireless communications. The black-box nature and high complexity of deep learning models employed in critical applications, such as channel estimation, limit their practical deployment. While perturbation-based XAI solutions offer input filtering, they often neglect internal structural optimization. We propose X-REFINE, an XAI-based framework for joint input-filtering and architecture fine-tuning. By utilizing a decomposition-based, sign-stabilized LRP epsilon rule, X-REFINE backpropagates predictions to derive high-resolution relevance scores for both subcarriers and hidden neurons. This enables a holistic optimization that identifies the most faithful model components. Simulation results demonstrate that X-REFINE achieves a superior interpretability-performance-complexity trade-off, significantly reducing computational complexity while maintaining robust bit error rate (BER) performance across different scenarios.

</details>


### [55] [Integrating Machine Learning Ensembles and Large Language Models for Heart Disease Prediction Using Voting Fusion](https://arxiv.org/abs/2602.22280)
*Md. Tahsin Amin,Tanim Ahmmod,Zannatul Ferdus,Talukder Naemul Hasan Naem,Ehsanul Ferdous,Arpita Bhattacharjee,Ishmam Ahmed Solaiman,Nahiyan Bin Noor*

Main category: cs.LG

TL;DR: 该研究基于1,190例患者数据，比较了传统机器学习模型与大型语言模型在心血管疾病预测中的性能，并提出ML-LLM混合融合方法，最终实现96.62%准确率和0.97 AUC的最优结果。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球首要死因，需早期识别与精准风险分层技术。传统ML算法虽擅长处理复杂患者数据，但LLM提供了零样本/少样本推理新能力，其在医疗决策支持中的应用潜力值得探索。

Method: 研究采用包含1,190条记录的心血管疾病数据集，对比了传统ML模型（随机森林、XGBoost、LightGBM、CatBoost）与通过OpenRouter API调用的开源LLM性能，并提出一种混合融合策略，将ML集成与Gemini 2.5 Flash的LLM推理相结合。

Result: 传统ML集成模型表现最佳（准确率95.78%，ROC-AUC 0.96）。LLM单独使用时，零样本准确率78.9%，少样本72.6%（原文表述存在矛盾：数值上72.6%低于78.9%，但描述为"slightly better"）。混合融合方法取得最优结果（准确率96.62%，AUC 0.97）。

Conclusion: 在结构化表格预测任务中，ML集成模型仍是首选，但可与LLM集成形成混合系统，在不确定性高的场景下提供小幅性能提升，为开发更可靠的临床决策支持工具提供新方向。LLM单独使用效果有限，但与ML结合时能发挥最大价值。

Abstract: Cardiovascular disease is the primary cause of death globally, necessitating early identification, precise risk classification, and dependable decision-support technologies. The advent of large language models (LLMs) provides new zero-shot and few-shot reasoning capabilities, even though machine learning (ML) algorithms, especially ensemble approaches like Random Forest, XGBoost, LightGBM, and CatBoost, are excellent at modeling complex, non-linear patient data and routinely beat logistic regression. This research predicts cardiovascular disease using a merged dataset of 1,190 patient records, comparing traditional machine learning models (95.78% accuracy, ROC-AUC 0.96) with open-source large language models via OpenRouter APIs. Finally, a hybrid fusion of the ML ensemble and LLM reasoning under Gemini 2.5 Flash achieved the best results (96.62% accuracy, 0.97 AUC), showing that LLMs (78.9 % accuracy) work best when combined with ML models rather than used alone. Results show that ML ensembles achieved the highest performance (95.78% accuracy, ROC-AUC 0.96), while LLMs performed moderately in zero-shot (78.9%) and slightly better in few-shot (72.6%) settings. The proposed hybrid method enhanced the strength in uncertain situations, illustrating that ensemble ML is considered the best structured tabular prediction case, but it can be integrated with hybrid ML-LLM systems to provide a minor increase and open the way to more reliable clinical decision-support tools.

</details>


### [56] [Early Risk Stratification of Dosing Errors in Clinical Trials Using Machine Learning](https://arxiv.org/abs/2602.22285)
*Félicien Hêche,Sohrab Ferdowsi,Anthony Yazdani,Sara Sansaloni-Pastor,Douglas Teodoro*

Main category: cs.LG

TL;DR: 开发一个基于机器学习的前瞻性框架，利用临床试验启动前的信息预测其给药错误高风险等级。


<details>
  <summary>Details</summary>
Motivation: 在临床试验启动前预测其发生高给药错误率的风险，以便实施主动的、基于风险的临床试验质量管理。

Method: 研究从ClinicalTrials.gov构建了包含42,112项临床试验的数据集，提取了结构化、半结构化和非结构化的方案文本数据。通过不良事件报告、MedDRA术语和Wilson置信区间为试验分配二分类标签（是否高给药错误率）。评估了三种模型：基于结构化特征的XGBoost模型、基于文本数据的ClinicalModernBERT模型，以及结合两者的简单后期融合模型。应用事后概率校准来实现可解释的试验级别风险分层。

Result: 后期融合模型获得了最高的AUC-ROC（0.862）。校准后的输出使试验能够被稳健地分层到预设的风险类别中。随着预测风险组别的升高，被标记为过高给药错误率的试验比例单调递增，并与相应的预测概率范围保持一致。

Conclusion: 该研究提供了一个可重复、可扩展的机器学习框架，用于在临床试验启动前对其高给药错误率风险进行早期分层，支持临床试验研究中的主动、基于风险的质量管理。

Abstract: Objective: The objective of this study is to develop a machine learning (ML)-based framework for early risk stratification of clinical trials (CTs) according to their likelihood of exhibiting a high rate of dosing errors, using information available prior to trial initiation. Materials and Methods: We constructed a dataset from ClinicalTrials.gov comprising 42,112 CTs. Structured, semi-structured trial data, and unstructured protocol-related free-text data were extracted. CTs were assigned binary labels indicating elevated dosing error rate, derived from adverse event reports, MedDRA terminology, and Wilson confidence intervals. We evaluated an XGBoost model trained on structured features, a ClinicalModernBERT model using textual data, and a simple late-fusion model combining both modalities. Post-hoc probability calibration was applied to enable interpretable, trial-level risk stratification. Results: The late-fusion model achieved the highest AUC-ROC (0.862). Beyond discrimination, calibrated outputs enabled robust stratification of CTs into predefined risk categories. The proportion of trials labeled as having an excessively high dosing error rate increased monotonically across higher predicted risk groups and aligned with the corresponding predicted probability ranges. Discussion: These findings indicate that dosing error risk can be anticipated at the trial level using pre-initiation information. Probability calibration was essential for translating model outputs into reliable and interpretable risk categories, while simple multimodal integration yielded performance gains without requiring complex architectures. Conclusion: This study introduces a reproducible and scalable ML framework for early, trial-level risk stratification of CTs at risk of high dosing error rates, supporting proactive, risk-based quality management in clinical research.

</details>


### [57] [Reliable XAI Explanations in Sudden Cardiac Death Prediction for Chagas Cardiomyopathy](https://arxiv.org/abs/2602.22288)
*Vinícius P. Chagas,Luiz H. T. Viana,Mac M. da S. Carlos,João P. V. Madeiro,Roberto C. Pedrosa,Thiago Alves Rocha,Carlos H. L. Cavalcante*

Main category: cs.LG

TL;DR: 本文提出一种基于逻辑的可解释性方法，用于解决恰加斯心肌病患者的猝死预测难题。该方法在不牺牲模型性能（>95%准确率和召回率）的前提下，提供100%解释保真度的形式化保证，相比启发式方法展现出卓越的鲁棒性和一致性，为临床部署建立了信任基础。


<details>
  <summary>Details</summary>
Motivation: 猝死具有高度不可预测性，在恰加斯心肌病患者（尤其是未被归类为高风险的患者）中预测尤为困难。虽然AI和机器学习模型能够改善风险分层，但其"黑箱"本质和缺乏透明度严重阻碍了临床采纳。现有启发式解释方法缺乏正确性保证，可能导致决策失误，因此亟需可信赖的可解释AI解决方案。

Method: 研究采用具有正确性保证的逻辑基可解释性方法，将其应用于猝死预测AI分类器。该方法通过形式化逻辑规则生成可验证的解释，而非依赖启发式近似。研究将该逻辑方法与前沿启发式方法进行对比，系统评估其解释保真度、一致性与鲁棒性。

Result: 实验结果表明，AI分类器实现了超过95%的准确率和召回率，逻辑可解释性方法达到了100%的解释保真度。与先进启发式方法相比，该方法在一致性和鲁棒性方面表现显著更优，为临床决策提供了可靠的验证机制。

Conclusion: 该逻辑基可解释性方法通过提供数学上可验证的解释，有效解决了AI模型的信任危机，促进了临床工具在恰加斯心肌病流行地区的大规模应用，为改善猝死风险分层提供了可信的技术路径。

Abstract: Sudden cardiac death (SCD) is unpredictable, and its prediction in Chagas cardiomyopathy (CC) remains a significant challenge, especially in patients not classified as high risk. While AI and machine learning models improve risk stratification, their adoption is hindered by a lack of transparency, as they are often perceived as \textit{black boxes} with unclear decision-making processes. Some approaches apply heuristic explanations without correctness guarantees, leading to mistakes in the decision-making process. To address this, we apply a logic-based explainability method with correctness guarantees to the problem of SCD prediction in CC. This explainability method, applied to an AI classifier with over 95\% accuracy and recall, demonstrated strong predictive performance and 100\% explanation fidelity. When compared to state-of-the-art heuristic methods, it showed superior consistency and robustness. This approach enhances clinical trust, facilitates the integration of AI-driven tools into practice, and promotes large-scale deployment, particularly in endemic regions where it is most needed.

</details>


### [58] [Manifold of Failure: Behavioral Attraction Basins in Language Models](https://arxiv.org/abs/2602.22291)
*Sarthak Munshi,Manish Bhatt,Vineeth Sai Narajala,Idan Habler,AmmarnAl-Kahfah,Ken Huang,Blake Gatto*

Main category: cs.LG

TL;DR: 本文提出了一个系统性映射大语言模型"失效流形"的框架，通过将漏洞搜索重构为质量多样性问题并使用MAP-Elites算法，揭示了不同模型失效区域的拓扑结构差异，从寻找离散失败转向理解其底层结构。


<details>
  <summary>Details</summary>
Motivation: 先前工作专注于将对抗样本投影回自然数据流形以恢复安全性，但本文认为全面理解AI安全需要直接刻画不安全区域本身。现有方法缺乏对失效区域系统性、全局性的理解，无法揭示其内在拓扑结构。

Method: 将漏洞搜索重构为质量多样性问题，使用MAP-Elites算法探索连续的行为吸引盆拓扑结构。以"对齐偏差"作为质量指标，引导搜索向模型行为偏离预期对齐最远的区域。在三个LLM（Llama-3-8B、GPT-OSS-20B、GPT-5-Mini）上进行实验评估。

Result: MAP-Elites实现了高达63%的行为覆盖率，发现了多达370个不同的漏洞生态位，并揭示了三种模型截然不同的拓扑特征：Llama-3-8B呈现近乎普遍的漏洞平台（平均对齐偏差0.93），GPT-OSS-20B显示碎片化景观且盆地空间集中（平均0.73），GPT-5-Mini表现出较强鲁棒性（上限0.50）。

Conclusion: 该方法生成了可解释的、全局性的模型安全景观地图，超越了现有攻击方法（GCG、PAIR、TAP），实现了从寻找离散失败到理解其底层结构的范式转变，为AI安全评估提供了新的系统性视角。

Abstract: While prior work has focused on projecting adversarial examples back onto the manifold of natural data to restore safety, we argue that a comprehensive understanding of AI safety requires characterizing the unsafe regions themselves. This paper introduces a framework for systematically mapping the Manifold of Failure in Large Language Models (LLMs). We reframe the search for vulnerabilities as a quality diversity problem, using MAP-Elites to illuminate the continuous topology of these failure regions, which we term behavioral attraction basins. Our quality metric, Alignment Deviation, guides the search towards areas where the model's behavior diverges most from its intended alignment. Across three LLMs: Llama-3-8B, GPT-OSS-20B, and GPT-5-Mini, we show that MAP-Elites achieves up to 63% behavioral coverage, discovers up to 370 distinct vulnerability niches, and reveals dramatically different model-specific topological signatures: Llama-3-8B exhibits a near-universal vulnerability plateau (mean Alignment Deviation 0.93), GPT-OSS-20B shows a fragmented landscape with spatially concentrated basins (mean 0.73), and GPT-5-Mini demonstrates strong robustness with a ceiling at 0.50. Our approach produces interpretable, global maps of each model's safety landscape that no existing attack method (GCG, PAIR, or TAP) can provide, shifting the paradigm from finding discrete failures to understanding their underlying structure.

</details>


### [59] [When Should a Model Change Its Mind? An Energy-Based Theory and Regularizer for Concept Drift in Electrocardiogram (ECG) Signals](https://arxiv.org/abs/2602.22294)
*Timothy Oladunni,Blessing Ojeme,Kyndal Maclin,Clyde Baidoo*

Main category: cs.LG

TL;DR: 本文提出生理能量守恒理论(PECT)，一种基于能量的动态生理信号概念稳定性框架，通过能量约束表示学习(ECRL)正则化器区分良性变异与真实概念漂移。在七种ECG多模态模型上的实验表明，该方法能在基本保持干净准确率的同时，将扰动准确率提升近13个百分点，并使融合表示漂移降低超45%。


<details>
  <summary>Details</summary>
Motivation: 现有概念漂移框架多为分布方法，缺乏对生理信号能量波动下模型内部表示移动的原则性指导，导致深度模型易将振幅、速率或形态的良性变化误判为概念漂移，尤其在多模态融合场景中产生不稳定预测。

Method: PECT假设在虚拟漂移下，归一化潜在位移应与归一化信号能量变化成比例，持续违反此比例则指示真实概念漂移。ECRL作为轻量级正则化器，在不改变编码器架构或增加推理成本的前提下，惩罚能量不一致的潜在移动。

Result: 在多模态ECG的七种模型上，最强三模态混合模型(1D+2D+Transformer)的干净准确率仅轻微下降(96.0%至94.1%)，扰动准确率显著提升(72.6%至85.5%)，融合表示漂移降低超45%，所有架构均呈现相似趋势。

Conclusion: 实证研究证明PECT可作为连续生理信号中概念稳定性的能量漂移定律，为动态生理信号建模提供了理论基础与实践工具。

Abstract: Models operating on dynamic physiologic signals must distinguish benign, label-preserving variability from true concept change. Existing concept-drift frameworks are largely distributional and provide no principled guidance on how much a model's internal representation may move when the underlying signal undergoes physiologically plausible fluctuations in energy. As a result, deep models often misinterpret harmless changes in amplitude, rate, or morphology as concept drift, yielding unstable predictions, particularly in multimodal fusion settings.
  This study introduces Physiologic Energy Conservation Theory (PECT), an energy-based framework for concept stability in dynamic signals. PECT posits that under virtual drift, normalized latent displacement should scale proportionally with normalized signal energy change, while persistent violations of this proportionality indicate real concept drift. We operationalize this principle through Energy-Constrained Representation Learning (ECRL), a lightweight regularizer that penalizes energy-inconsistent latent movement without modifying encoder architectures or adding inference-time cost.
  Although PECT is formulated for dynamic signals in general, we instantiate and evaluate it on multimodal ECG across seven unimodal and hybrid models. Experiments show that in the strongest trimodal hybrid (1D+2D+Transformer), clean accuracy is largely preserved (96.0% to 94.1%), while perturbed accuracy improves substantially (72.6% to 85.5%) and fused representation drift decreases by over 45%. Similar trends are observed across all architectures, providing empirical evidence that PECT functions as an energy-drift law governing concept stability in continuous physiologic signals.

</details>


### [60] [Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection](https://arxiv.org/abs/2602.22297)
*Dhiraj Neupane,Richard Dazeley,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 提出了一种基于对抗逆强化学习的无监督机械故障检测方法，通过从正常运行序列中学习奖励函数作为异常分数，在多个基准数据集上实现了早期鲁棒故障检测，无需人工设计奖励或故障标签。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的机械故障检测方法未能充分利用RL的序列决策优势，往往将其简化为上下文老虎机问题。为了解决这一问题，需要一种能够充分利用时序结构、避免人工奖励工程和故障标签的方法。

Method: 将机械故障检测建模为离线逆强化学习问题，采用对抗逆强化学习框架训练判别器来区分正常运行（专家）轨迹与策略生成轨迹，将判别器学习到的奖励作为异常分数，用于检测偏离正常行为的故障状态。

Result: 在HUMS2023、IMS和XJTU-SY三个运行至故障基准数据集上，模型能持续为正常样本分配低异常分数，为故障样本分配高异常分数，实现了早期且鲁棒的故障检测。

Conclusion: 该方法通过将强化学习的序列推理能力与机械故障检测的时序结构对齐，为数据驱动工业环境下的无监督故障诊断开辟了新途径，避免了人工奖励设计的需要。

Abstract: Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL's sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formulate MFD as an offline inverse reinforcement learning problem, where the agent learns the reward dynamics directly from healthy operational sequences, thereby bypassing the need for manual reward engineering and fault labels. Our framework employs Adversarial Inverse Reinforcement Learning to train a discriminator that distinguishes between normal (expert) and policy-generated transitions. The discriminator's learned reward serves as an anomaly score, indicating deviations from normal operating behaviour. When evaluated on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY), the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection. By aligning RL's sequential reasoning with MFD's temporal structure, this work opens a path toward RL-based diagnostics in data-driven industrial settings.

</details>


### [61] [A 1/R Law for Kurtosis Contrast in Balanced Mixtures](https://arxiv.org/abs/2602.22334)
*Yuda Bi,Wenjun Xiao,Linhao Bai,Vince D Calhoun*

Main category: cs.LG

TL;DR: ...


<details>
  <summary>Details</summary>
Motivation: ...

Method: ...

Result: ...

Conclusion: ...

Abstract: Kurtosis-based Independent Component Analysis (ICA) weakens in wide, balanced mixtures. We prove a sharp redundancy law: for a standardized projection with effective width $R_{\mathrm{eff}}$ (participation ratio), the population excess kurtosis obeys $|κ(y)|=O(κ_{\max}/R_{\mathrm{eff}})$, yielding the order-tight $O(c_bκ_{\max}/R)$ under balance (typically $c_b=O(\log R)$). As an impossibility screen, under standard finite-moment conditions for sample kurtosis estimation, surpassing the $O(1/\sqrt{T})$ estimation scale requires $R\lesssim κ_{\max}\sqrt{T}$. We also show that \emph{purification} -- selecting $m\!\ll\!R$ sign-consistent sources -- restores $R$-independent contrast $Ω(1/m)$, with a simple data-driven heuristic. Synthetic experiments validate the predicted decay, the $\sqrt{T}$ crossover, and contrast recovery.

</details>


### [62] [Disentangling Shared and Target-Enriched Topics via Background-Contrastive Non-negative Matrix Factorization](https://arxiv.org/abs/2602.22387)
*Yixuan Li,Archer Y. Yang,Yue Li*

Main category: cs.LG

TL;DR: 本研究开发了背景对比非负矩阵分解(bcNMF)，通过联合分解目标与背景数据集并采用对比学习抑制背景变异，从高维生物数据中提取条件特异性信号。该方法GPU高效、可扩展，在多种生物数据中成功识别了传统方法无法检测的疾病、基因型和治疗相关分子模式。


<details>
  <summary>Details</summary>
Motivation: 高维生物数据中，目标信号常被跨条件的优势变异（基线生物结构或技术效应）掩盖。这些未知混杂因素与信号混合，现有背景校正方法存在高维不可扩展或缺乏可解释性的局限。

Method: 提出bcNMF方法：1) 使用共享非负基矩阵联合分解目标集和匹配背景集；2) 设计对比目标函数抑制背景表达结构；3) 采用矩阵乘法的乘法更新算法，支持GPU加速和minibatch训练，实现高效可扩展。

Result: 在模拟和真实生物数据中，bcNMF成功揭示了：抑郁症脑单细胞RNA-seq的疾病相关程序、小鼠基因型相关蛋白表达、白血病治疗特异性转录变化，以及TP53依赖的癌细胞药物反应，这些信号均被传统方法掩盖。

Conclusion: bcNMF通过显式分离目标特异性变异，提供特征水平可解释的非负成分，为高维生物数据分析提供了兼具可扩展性和可解释性的新工具，能有效挖掘被背景变异掩盖的生物医学洞见。

Abstract: Biological signals of interest in high-dimensional data are often masked by dominant variation shared across conditions. This variation, arising from baseline biological structure or technical effects, can prevent standard dimensionality reduction methods from resolving condition-specific structure. The challenge is that these confounding topics are often unknown and mixed with biological signals. Existing background correction methods are either unscalable to high dimensions or not interpretable. We introduce background contrastive Non-negative Matrix Factorization (\model), which extracts target-enriched latent topics by jointly factorizing a target dataset and a matched background using shared non-negative bases under a contrastive objective that suppresses background-expressed structure. This approach yields non-negative components that are directly interpretable at the feature level, and explicitly isolates target-specific variation. \model is learned by an efficient multiplicative update algorithm via matrix multiplication such that it is highly efficient on GPU hardware and scalable to big data via minibatch training akin to deep learning approach. Across simulations and diverse biological datasets, \model reveals signals obscured by conventional methods, including disease-associated programs in postmortem depressive brain single-cell RNA-seq, genotype-linked protein expression patterns in mice, treatment-specific transcriptional changes in leukemia, and TP53-dependent drug responses in cancer cell lines.

</details>


### [63] [Predicting Multi-Drug Resistance in Bacterial Isolates Through Performance Comparison and LIME-based Interpretation of Classification Models](https://arxiv.org/abs/2602.22400)
*Santanam Wishal,Riad Sahara*

Main category: cs.LG

TL;DR: 针对多重耐药菌临床检测延迟和治疗选择有限的问题，本研究提出一个可解释机器学习框架，基于9,714个细菌分离株的临床特征和抗生素家族水平耐药模式，评估了五种分类模型。XGBoost和LightGBM表现最佳，结合LIME解释器识别出喹诺酮类等五种抗生素耐药性是MDR预测的关键因素，为临床抗菌药物管理和决策支持提供了准确且可操作的解决方案。


<details>
  <summary>Details</summary>
Motivation: 多重耐药性（MDR）的日益严峻给临床决策带来巨大挑战，由于治疗选择有限且传统药敏试验耗时较长，导致治疗延误。亟需快速准确的预测工具来识别MDR，以指导合理用药并改善患者预后。

Method: 研究采用9,714个细菌分离株的临床数据和抗生素家族水平耐药模式，评估了逻辑回归、随机森林、AdaBoost、XGBoost和LightGBM五种分类模型。通过准确率、F1分数、AUC-ROC和MCC等指标评估性能，并应用LIME技术生成个体化预测解释，识别对MDR预测贡献最强的耐药特征。

Result: 集成模型XGBoost和LightGBM在所有评估指标上表现最优。LIME分析显示，对喹诺酮类、复方新诺明、粘菌素、氨基糖苷类和呋喃妥因的耐药性是预测MDR最强的贡献因素，这些发现与已知的生物学机制一致，验证了模型的可靠性。

Conclusion: 该可解释机器学习框架通过结合高性能模型与局部可解释性，不仅实现了MDR的早期准确预测，还为抗菌药物管理提供了可操作的临床洞见，增强了临床医生对机器学习辅助决策系统的信任，具有重要的临床应用价值。

Abstract: The rise of Antimicrobial Resistance, particularly Multi-Drug Resistance (MDR), presents a critical challenge for clinical decision-making due to limited treatment options and delays in conventional susceptibility testing. This study proposes an interpretable machine learning framework to predict MDR in bacterial isolates using clinical features and antibiotic susceptibility patterns. Five classification models were evaluated, including Logistic Regression, Random Forest, AdaBoost, XGBoost, and LightGBM. The models were trained on a curated dataset of 9,714 isolates, with resistance encoded at the antibiotic family level to capture cross-class resistance patterns consistent with MDR definitions. Performance assessment included accuracy, F1-score, AUC-ROC, and Matthews Correlation Coefficient. Ensemble models, particularly XGBoost and LightGBM, demonstrated superior predictive capability across all metrics. To address the clinical transparency gap, Local Interpretable Model-agnostic Explanations (LIME) was applied to generate instance-level explanations. LIME identified resistance to quinolones, Co-trimoxazole, Colistin, aminoglycosides, and Furanes as the strongest contributors to MDR predictions, aligning with known biological mechanisms. The results show that combining high-performing models with local interpretability provides both accuracy and actionable insights for antimicrobial stewardship. This framework supports earlier MDR identification and enhances trust in machine learning-assisted clinical decision support.

</details>


### [64] [MolFM-Lite: Multi-Modal Molecular Property Prediction with Conformer Ensemble Attention and Cross-Modal Fusion](https://arxiv.org/abs/2602.22405)
*Syed Omer Shah,Mohammed Maqsood Ahmed,Danish Mohiuddin Mohammed,Shahnawaz Alam,Mohd Vahaj ur Rahman*

Main category: cs.LG

TL;DR: 本文提出MolFM-Lite，一种多模态分子性质预测模型，通过交叉注意力融合1D SELFIES序列、2D分子图和3D构象系综，并利用FiLM引入实验上下文。该模型结合可学习注意力与玻尔兹曼加权先验处理多构象，在四个MoleculeNet基准测试中相比单模态基线提升7-11% AUC，构象系综比单一构象提升约2% AUC。预训练策略计算成本低且有效。


<details>
  <summary>Details</summary>
Motivation: 现有分子性质预测模型多依赖单一分子表示（序列、图或3D结构）且将分子几何视为静态，无法充分利用多维度互补信息和分子构象的热力学分布特性。

Method: 提出MolFM-Lite：1）构建三模态编码架构，分别处理SELFIES序列、分子图和RDKit生成的构象系综；2）设计构象系综注意力机制，融合可学习注意力与玻尔兹曼加权先验；3）开发交叉模态融合层实现模态间注意力交互；4）采用FiLM层以实验上下文条件调控预测；5）在ZINC250K上进行跨模态对比学习和掩码原子预训练。

Result: 在四个MoleculeNet支架分割基准上，三模态融合相比单模态基线AUC提升7-11%，构象系综相比单一构象提升约2%。消融研究证实各组件独立贡献。预训练在较低计算成本下实现有效权重初始化。

Conclusion: MolFM-Lite通过多模态联合学习和动态构象系综注意力机制显著提升预测性能，证明整合多维度分子表示和热力学构象分布对分子建模至关重要。开源代码、模型和数据分割确保了研究可重复性。

Abstract: Most machine learning models for molecular property prediction rely on a single molecular representation (either a sequence, a graph, or a 3D structure) and treat molecular geometry as static. We present MolFM-Lite, a multi-modal model that jointly encodes SELFIES sequences (1D), molecular graphs (2D), and conformer ensembles (3D) through cross-attention fusion, while conditioning predictions on experimental context via Feature-wise Linear Modulation (FiLM). Our main methodological contributions are: (1) a conformer ensemble attention mechanism that combines learnable attention with Boltzmann-weighted priors over multiple RDKit-generated conformers, capturing the thermodynamic distribution of molecular shapes; and (2) a cross-modal fusion layer where each modality can attend to others, enabling complementary information sharing. We evaluate on four MoleculeNet scaffold-split benchmarks using our model's own splits, and report all baselines re-evaluated under the same protocol. Comprehensive ablation studies across all four datasets confirm that each architectural component contributes independently, with tri-modal fusion providing 7-11% AUC improvement over single-modality baselines and conformer ensembles adding approximately 2% over single-conformer variants. Pre-training on ZINC250K (~250K molecules) using cross-modal contrastive and masked-atom objectives enables effective weight initialization at modest compute cost. We release all code, trained models, and data splits to support reproducibility.

</details>


### [65] [A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection](https://arxiv.org/abs/2602.22412)
*Ruiqi Zhou,Donghao Zhu,Houcai Shen*

Main category: cs.LG

TL;DR: 针对匹配市场中延迟匹配的效率与成本权衡难题，本文提出基于学习的混合框架，通过实时回归估计用户离开分布，依据决策阈值自适应融合即时与延迟匹配，在轻微牺牲匹配效率的同时显著降低等待时间和市场拥堵，实现介于纯贪心与纯耐心策略间的灵活插值。


<details>
  <summary>Details</summary>
Motivation: 在肾脏交换、货运交换等匹配市场中，延迟匹配虽能提升效率，但其收益对参与者停留时间和离开行为高度敏感，且会引发等待时间延长和拥堵等成本。固定匹配策略在动态环境中缺乏适应性，无法灵活平衡效率与成本。

Method: 提出学习驱动的混合框架：持续收集用户离开数据，通过回归估计其潜在离开分布，基于控制效率损失容忍度的决策阈值动态决策是否延迟匹配，从而自适应结合即时匹配与延迟匹配。

Result: 该框架能以有限的匹配效率损失为代价，显著减少参与者平均等待时间和市场拥堵程度，并可根据系统需求动态调整策略，使性能灵活地介于纯即时匹配与纯延迟匹配之间。

Conclusion: 该混合框架为动态匹配市场提供了稳健且自适应的替代方案，克服了静态机制的僵化性，在效率与成本间实现动态权衡，优于传统的固定匹配策略。

Abstract: In matching markets such as kidney exchanges and freight exchanges, delayed matching has been shown to improve overall market efficiency. The benefits of delay are highly sensitive to participants' sojourn times and departure behavior, and delaying matches can impose significant costs, including longer waiting times and increased market congestion. These competing effects make fixed matching policies inherently inflexible in dynamic environments. We propose a learning-based Hybrid framework that adaptively combines immediate and delayed matching. The framework continuously collects data on user departures over time, estimates the underlying departure distribution via regression, and determines whether to delay matching in the subsequent period based on a decision threshold that governs the system's tolerance for matching efficiency loss. The proposed framework can substantially reduce waiting times and congestion while sacrificing only a limited amount of matching efficiency. By dynamically adjusting its matching strategy, the Hybrid framework enables system performance to flexibly interpolate between purely greedy and purely patient policies, offering a robust and adaptive alternative to static matching mechanisms.

</details>


### [66] [Calibrated Test-Time Guidance for Bayesian Inference](https://arxiv.org/abs/2602.22428)
*Daniel Geyfman,Felix Draxler,Jan Groeneveld,Hyunsoo Lee,Theofanis Karaletsos,Stephan Mandt*

Main category: cs.LG

TL;DR: 本文揭示测试时引导方法无法正确恢复贝叶斯后验分布的结构性缺陷，提出可校准替代估计器，在贝叶斯推理任务和黑洞图像重建中实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时引导方法仅最大化奖励而非从真实贝叶斯后验采样，导致推理结果校准错误，亟需实现校准采样。

Method: 分析现有方法后验失配的结构近似缺陷，设计一致替代估计器以实现对贝叶斯后验的校准采样。

Result: 在贝叶斯推理任务上显著优于先前方法，在黑洞图像重建中匹配顶尖水平。

Conclusion: 该研究成功识别测试时引导的贝叶斯推断根本缺陷，提出的替代估计器为扩散模型实现校准后验采样提供了有效解决方案。

Abstract: Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we show that common test-time guidance methods do not recover the correct posterior distribution and identify the structural approximations responsible for this failure. We then propose consistent alternative estimators that enable calibrated sampling from the Bayesian posterior. We significantly outperform previous methods on a set of Bayesian inference tasks, and match state-of-the-art in black hole image reconstruction.

</details>


### [67] [From Bias to Balance: Fairness-Aware Paper Recommendation for Equitable Peer Review](https://arxiv.org/abs/2602.22438)
*Uttamasha Anjally Oyshi,Susan Gauch*

Main category: cs.LG

TL;DR: 提出Fair-PaperRec，一种带公平性正则化的论文推荐系统，在双盲评审后重排论文，可在保持质量稳定的前提下将弱势群体参与度提升高达42.03%。


<details>
  <summary>Details</summary>
Motivation: 尽管实行双盲评审，作者人口统计学相关的系统性偏见仍会损害弱势群体。研究假设：在评审后推荐系统中加入显式公平性正则化，可在不降低质量的前提下提升包容性。

Method: 开发Fair-PaperRec——多层感知机模型，通过可微分公平性损失函数（考虑种族、国家等交叉属性）在双盲评审后重排论文。先在合成数据集（高/中/近公平偏差）上验证，再在SIGCHI、DIS、IUI等会议真实数据上测试。

Result: 合成数据表明，增加公平性权重可增强宏/微观多样性且效用基本稳定；真实场景中，适当配置的Fair-PaperRec相比历史选择，弱势群体参与度最高提升42.03%，整体效用变化不超过3.16%。

Conclusion: 公平性正则化既可充当公平机制，也能作为温和的质量正则器，尤其在高偏差环境中。Fair-PaperRec提供了一个实用框架，能在保持甚至提升学术质量的同时实现公平性目标。

Abstract: Despite frequent double-blind review, systemic biases related to author demographics still disadvantage underrepresented groups. We start from a simple hypothesis: if a post-review recommender is trained with an explicit fairness regularizer, it should increase inclusion without degrading quality. To test this, we introduce Fair-PaperRec, a Multi-Layer Perceptron (MLP) with a differentiable fairness loss over intersectional attributes (e.g., race, country) that re-ranks papers after double-blind review. We first probe the hypothesis on synthetic datasets spanning high, moderate, and near-fair biases. Across multiple randomized runs, these controlled studies map where increasing the fairness weight strengthens macro/micro diversity while keeping utility approximately stable, demonstrating robustness and adaptability under varying disparity levels. We then carry the hypothesis into the original setting, conference data from ACM Special Interest Group on Computer-Human Interaction (SIGCHI), Designing Interactive Systems (DIS), and Intelligent User Interfaces (IUI). In this real-world scenario, an appropriately tuned configuration of Fair-PaperRec achieves up to a 42.03% increase in underrepresented-group participation with at most a 3.16% change in overall utility relative to the historical selection. Taken together, the synthetic-to-original progression shows that fairness regularization can act as both an equity mechanism and a mild quality regularizer, especially in highly biased regimes. By first analyzing the behavior of the fairness parameters under controlled conditions and then validating them on real submissions, Fair-PaperRec offers a practical, equity-focused framework for post-review paper selection that preserves, and in some settings can even enhance, measured scholarly quality.

</details>


### [68] [Beyond performance-wise Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2602.22470)
*Balazs Pejo*

Main category: cs.LG

TL;DR: 该论文针对联邦学习中客户端贡献评估仅关注准确率等单一性能指标的问题，提出基于Shapley值的多维度可信度评估框架，量化客户端在可靠性（抗噪声）、鲁棒性（抗对抗样本）和公平性（人口统计均等）方面的贡献，揭示单一指标评估体系的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习客户端评估方法过度依赖模型性能的单维度指标（如准确率、损失），忽视了可信度的其他关键方面。这种片面性导致奖励分配机制不公，无法准确反映客户端对模型整体质量的真实贡献，亟需建立更全面的多维度评估体系。

Method: 采用先进的Shapley值近似算法，从三个维度量化客户端贡献：可靠性（模型对噪声数据的容忍能力）、鲁棒性（抵抗对抗样本攻击的能力）和公平性（通过人口统计均等性衡量）。通过计算各客户端在不同可信度指标上的边际贡献，实现精细化价值归因。

Result: 实证研究发现，不存在能在所有可信度维度上均表现优异的单一客户端，且各维度之间具有相对独立性。这一发现表明，当前基于单一性能指标的评估方案存在根本性缺陷，无法实现全面评估与公平奖励分配。

Conclusion: 联邦学习的客户端评估必须超越传统性能指标的局限，构建多维度综合评估体系。研究强调，未来的奖励机制设计应充分考虑客户端在不同可信度维度上的差异化贡献，以实现更精准的激励相容。

Abstract: Federated learning offers a privacy-friendly collaborative learning framework, yet its success, like any joint venture, hinges on the contributions of its participants. Existing client evaluation methods predominantly focus on model performance, such as accuracy or loss, which represents only one dimension of a machine learning model's overall utility. In contrast, this work investigates the critical, yet overlooked, issue of client contributions towards a model's trustworthiness -- specifically, its reliability (tolerance to noisy data), resilience (resistance to adversarial examples), and fairness (measured via demographic parity). To quantify these multifaceted contributions, we employ the state-of-the-art approximation of the Shapley value, a principled method for value attribution. Our results reveal that no single client excels across all dimensions, which are largely independent from each other, highlighting a critical flaw in current evaluation scheme: no single metric is adequate for comprehensive evaluation and equitable rewarding allocation.

</details>


### [69] [Reinforcement-aware Knowledge Distillation for LLM Reasoning](https://arxiv.org/abs/2602.22495)
*Zhaoyang Zhang,Shuli Jiang,Yantao Shen,Yuting Zhang,Dhananjay Ram,Shuo Yang,Zhuowen Tu,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: 本文针对大型语言模型RL后训练推理成本高的问题，提出RL感知蒸馏（RLAD）方法。核心创新是用PPO/GRPO风格的似然比目标替代传统的KL散度正则化器，通过信任域比率蒸馏（TRRD）实现优势感知的蒸馏，在逻辑推理和数学基准测试中一致优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管RL后训练能显著提升长链思维推理能力，但大模型推理成本高昂促使模型蒸馏需求。现有知识蒸馏方法基于监督微调设计，依赖固定教师轨迹或KL散度正则化，与RL结合时存在分布不匹配和目标冲突问题：教师监督可能与学生探索的分布不一致，KL正则化器可能与奖励最大化目标竞争，需要精细的损失平衡。

Method: 提出RL感知蒸馏（RLAD）框架，在RL过程中进行选择性模仿。核心组件信任域比率蒸馏（TRRD）将教师-学生KL正则化器替换为PPO/GRPO风格的似然比目标，该目标以教师-旧策略混合为锚点，在学生rollout上实现优势感知、信任域有界的蒸馏，自然平衡探索、利用和模仿。

Result: 在多样化的逻辑推理和数学基准测试上，RLAD方法一致优于离线蒸馏、标准GRPO以及基于KL的在线师生知识蒸馏方法。

Conclusion: RLAD通过TRRD机制有效解决了RL蒸馏中的分布不匹配和目标干扰问题，为高效的小模型推理能力迁移提供了新方案，验证了似然比目标在蒸馏中的优越性。

Abstract: Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for supervised fine-tuning (SFT), relying on fixed teacher traces or teacher-student Kullback-Leibler (KL) divergence-based regularization. When combined with RL, these approaches often suffer from distribution mismatch and objective interference: teacher supervision may not align with the student's evolving rollout distribution, and the KL regularizer can compete with reward maximization and require careful loss balancing. To address these issues, we propose RL-aware distillation (RLAD), which performs selective imitation during RL -- guiding the student toward the teacher only when it improves the current policy update. Our core component, Trust Region Ratio Distillation (TRRD), replaces the teacher-student KL regularizer with a PPO/GRPO-style likelihood-ratio objective anchored to a teacher--old-policy mixture, yielding advantage-aware, trust-region-bounded distillation on student rollouts and naturally balancing exploration, exploitation, and imitation. Across diverse logic reasoning and math benchmarks, RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.

</details>


### [70] [TEFL: Prediction-Residual-Guided Rolling Forecasting for Multi-Horizon Time Series](https://arxiv.org/abs/2602.22520)
*Xiannan Huang,Shen Fang,Shuhan Qiu,Chengcheng Yu,Jiayuan Du,Chao Yang*

Main category: cs.LG

TL;DR: 本文提出TEFL（时序误差反馈学习）框架，通过利用滚动预测中的历史残差信息来改进时间序列预测。该框架解决了部分可观测性下的残差选择、低秩适配器集成和两阶段训练三大挑战，在10个真实数据集和5种骨干架构上验证可平均降低MAE 5-10%，在突变和分布漂移场景下最高可降低19.5%，为深度预测系统提供了简单有效的增强方案。


<details>
  <summary>Details</summary>
Motivation: 现代深度预测模型仅优化逐点预测损失，未能利用滚动预测中蕴含的丰富残差信息（如持续偏差、未建模模式和动态变化）。这些残差包含宝贵的历史预测误差模式，可为模型提供反馈信号以提升预测精度，尤其是在面对分布变化时。

Method: TEFL框架在训练和评估阶段显式引入历史残差。针对三个关键挑战：1）在部分可观测条件下选择可观测的多步残差；2）通过轻量级低秩适配器集成残差以保持效率并防止过拟合；3）设计两阶段训练流程联合优化基础预测器和误差模块。该方法可无缝集成到现有深度预测模型中。

Result: 在10个真实世界数据集和5种骨干架构上的广泛实验表明，TEFL持续提升预测精度，平均降低MAE达5-10%。在突变和分布漂移等挑战性场景下表现出强鲁棒性，误差降低超过10%，最高达19.5%。验证了该方法的有效性和泛化能力。

Conclusion: TEFL通过将基于残差的反馈直接嵌入学习过程，为现代深度预测系统提供了一种简单、通用且有效的增强方案。该方法充分利用了以往被忽视的预测残差信息，在保持计算效率的同时显著提升预测准确性和鲁棒性，具有实际应用价值。

Abstract: Time series forecasting plays a critical role in domains such as transportation, energy, and meteorology. Despite their success, modern deep forecasting models are typically trained to minimize point-wise prediction loss without leveraging the rich information contained in past prediction residuals from rolling forecasts - residuals that reflect persistent biases, unmodeled patterns, or evolving dynamics. We propose TEFL (Temporal Error Feedback Learning), a unified learning framework that explicitly incorporates these historical residuals into the forecasting pipeline during both training and evaluation. To make this practical in deep multi-step settings, we address three key challenges: (1) selecting observable multi-step residuals under the partial observability of rolling forecasts, (2) integrating them through a lightweight low-rank adapter to preserve efficiency and prevent overfitting, and (3) designing a two-stage training procedure that jointly optimizes the base forecaster and error module. Extensive experiments across 10 real-world datasets and 5 backbone architectures show that TEFL consistently improves accuracy, reducing MAE by 5-10% on average. Moreover, it demonstrates strong robustness under abrupt changes and distribution shifts, with error reductions exceeding 10% (up to 19.5%) in challenging scenarios. By embedding residual-based feedback directly into the learning process, TEFL offers a simple, general, and effective enhancement to modern deep forecasting systems.

</details>


### [71] [Predicting Tennis Serve directions with Machine Learning](https://arxiv.org/abs/2602.22527)
*Ying Zhu,Ruthuparna Naikar*

Main category: cs.LG

TL;DR: 本研究开发机器学习方法预测职业网球第一发球方向，通过特征工程实现男子49%、女子44%的预测准确率，揭示球员采用混合策略、疲劳影响决策，且情境信息对回球预判至关重要。


<details>
  <summary>Details</summary>
Motivation: 职业网球中第一发球至关重要，发接发双方存在心理博弈。本研究旨在通过预测发球方向，深入理解球员的战略决策模式。

Method: 采用机器学习方法，通过特征工程构建预测模型，分析职业网球运动员第一发球方向的选择规律。

Result: 模型平均预测准确率约49%（男子）和44%（女子），发现顶尖球员采用混合策略发球，疲劳因素影响决策方向。

Conclusion: 情境信息对回球方预判的重要性超出预期，研究为理解网球比赛战略决策提供了新视角。

Abstract: Serves, especially first serves, are very important in professional tennis. Servers choose their serve directions strategically to maximize their winning chances while trying to be unpredictable. On the other hand, returners try to predict serve directions to make good returns. The mind game between servers and returners is an important part of decision-making in professional tennis matches. To help understand the players' serve decisions, we have developed a machine learning method for predicting professional tennis players' first serve directions. Through feature engineering, our method achieves an average prediction accuracy of around 49\% for male players and 44\% for female players. Our analysis provides some evidence that top professional players use a mixed-strategy model in serving decisions and that fatigue might be a factor in choosing serve directions. Our analysis also suggests that contextual information is perhaps more important for returners' anticipatory reactions than previously thought.

</details>


### [72] [Persistent Nonnegative Matrix Factorization via Multi-Scale Graph Regularization](https://arxiv.org/abs/2602.22536)
*Jichao Zhang,Ran Miao,Limin Li*

Main category: cs.LG

TL;DR: 本文提出持续性非负矩阵分解(pNMF)，利用持续性同调识别关键尺度，构建多尺度图拉普拉斯正则化的耦合NMF模型，并设计具有收敛保证的序贯交替优化算法，实现了跨分辨率的连通结构演化分析与多尺度低秩嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有NMF方法本质为单尺度，无法刻画不同分辨率下数据连通结构的演化过程，难以满足多尺度数据分析需求。

Method: 通过持续性同调识别拓扑结构质变的规范尺度集，诱导多尺度图拉普拉斯矩阵，建立带尺度几何正则化与跨尺度一致性约束的耦合NMF模型，分析嵌入向量沿尺度的增量边界。

Result: 构建了多尺度参数化NMF解路径框架，开发了可收敛的序贯交替优化算法，在合成数据与单细胞RNA测序数据集上验证了方法的有效性，可生成高质量的多尺度低秩嵌入。

Conclusion: pNMF通过融合拓扑持久性理论，成功将NMF扩展至多尺度分析领域，为理解数据跨分辨率结构演化提供了新的数学工具与计算范式。

Abstract: Matrix factorization techniques, especially Nonnegative Matrix Factorization (NMF), have been widely used for dimensionality reduction and interpretable data representation. However, existing NMF-based methods are inherently single-scale and fail to capture the evolution of connectivity structures across resolutions. In this work, we propose persistent nonnegative matrix factorization (pNMF), a scale-parameterized family of NMF problems, that produces a sequence of persistence-aligned embeddings rather than a single one. By leveraging persistent homology, we identify a canonical minimal sufficient scale set at which the underlying connectivity undergoes qualitative changes. These canonical scales induce a sequence of graph Laplacians, leading to a coupled NMF formulation with scale-wise geometric regularization and explicit cross-scale consistency constraint. We analyze the structural properties of the embeddings along the scale parameter and establish bounds on their increments between consecutive scales. The resulting model defines a nontrivial solution path across scales, rather than a single factorization, which poses new computational challenges. We develop a sequential alternating optimization algorithm with guaranteed convergence. Numerical experiments on synthetic and single-cell RNA sequencing datasets demonstrate the effectiveness of the proposed approach in multi-scale low-rank embeddings.

</details>


### [73] [LUMOS: Democratizing SciML Workflows with L0-Regularized Learning for Unified Feature and Parameter Adaptation](https://arxiv.org/abs/2602.22537)
*Shouwei Gao,Xu Zheng,Dongsheng Luo,Sheng Di,Wenqian Dong*

Main category: cs.LG

TL;DR: SciML快速发展但模型设计仍依赖专业知识，本文提出LUMOS——一种基于L0正则化的端到端框架，通过半随机门控与重参数化技术统一特征选择与模型剪枝，在13项任务中平均减少71.45%参数并获得6.4倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习虽在各领域加速发现，但构建有效模型需大量先验知识确定输入特征和模型规模，过程依赖人工专业知识，亟需自动化方案以实现SciML模型设计的民主化。

Method: LUMOS采用L0正则化学习，结合半随机门控机制和重参数化技巧，在训练中动态选择信息特征并剪除冗余参数，实现端到端自动化模型设计，降低手动调参需求。

Result: 在13个包含宇宙学和分子科学的SciML任务上验证，LUMOS平均减少71.45%参数量，推理速度提升6.4倍，且在8个GPU的分布式数据并行训练中展现出良好可扩展性。

Conclusion: LUMOS通过统一特征选择与模型剪枝，有效降低对手工调参的依赖，在保持预测精度的同时显著提升模型效率，为SciML模型的民主化设计提供了有效途径。

Abstract: The rapid growth of scientific machine learning (SciML) has accelerated discovery across diverse domains, yet designing effective SciML models remains a challenging task. In practice, building such models often requires substantial prior knowledge and manual expertise, particularly in determining which input features to use and how large the model should be. We introduce LUMOS, an end-to-end framework based on L0-regularized learning that unifies feature selection and model pruning to democratize SciML model design. By employing semi-stochastic gating and reparameterization techniques, LUMOS dynamically selects informative features and prunes redundant parameters during training, reducing the reliance on manual tuning while maintaining predictive accuracy. We evaluate LUMOS across 13 diverse SciML workloads, including cosmology and molecular sciences, and demonstrate its effectiveness and generalizability. Experiments on 13 SciML models show that LUMOS achieves 71.45% parameter reduction and a 6.4x inference speedup on average. Furthermore, Distributed Data Parallel (DDP) training on up to eight GPUs confirms the scalability of

</details>


### [74] [RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format](https://arxiv.org/abs/2602.22538)
*Zhehao Huang,Yuhang Liu,Baijiong Lin,Yixin Lou,Zhengbao He,Hanling Tian,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: 针对大推理模型(LRM)指令遵循能力不足的问题，本文提出RAIN-Merging，一种梯度-free的模型融合方法。通过将指令微调模型(ITM)的任务向量投影至LRM思考令牌的零空间，并引入指令注意力机制进行模块特异性缩放，实现了在保留LRM推理能力的同时显著提升指令遵循性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型(LRM)擅长复杂推理但难以遵循格式指令，而指令微调模型(ITM)虽精于指令跟随却缺乏推理能力。两者参数空间差异（任务向量主subspace近乎正交）暗示轻量合并可行，但输出格式不匹配（LRM含显式思考/回复段，ITM仅答案）导致naive合并脆弱且不稳定。

Method: RAIN-Merging分两步：首先，基于小规模推理校准集，将ITM任务向量投影到思考特殊令牌的前向特征零空间，以保护LRM的结构化推理机制；其次，利用小规模指令校准集估计指令注意力权重，计算模块特异性缩放系数，放大指令相关成分并抑制信息泄露。全程无需梯度回传。

Result: 在4项指令遵循和9项推理/通用能力基准测试中，RAIN-Merging显著提升指令遵循性且保持推理质量。该方法在不同模型规模与架构下表现一致，并在智能体设置中实现性能提升。

Conclusion: RAIN-Merging成功桥接了LRM推理能力与ITM指令遵循能力，为开发高性能且可控的大模型提供了有效范式，在智能体应用中展现重要价值。

Abstract: Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.

</details>


### [75] [Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences](https://arxiv.org/abs/2602.21585)
*Sweta Karlekar,Carolina Zheng,Magnus Saebo,Nicolas Beltran-Velez,Shuyang Yu,John Bowlan,Michal Kucer,David Blei*

Main category: cs.LG

TL;DR: Duel-Evolve是一种进化优化算法，通过利用LLM自身产生的两两偏好替代外部标量奖励，在无需监督的情况下优化LLM输出，在数学和代码基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有测试时优化方法依赖标量评估器，但这些评估器往往难以获得、过于稀疏或不可靠。两两比较更易获取、能提供改进方向信号，且可从LLM自身获得而无需外部监督，这为优化提供了新思路。

Method: Duel-Evolve采用进化框架，使用同一LLM生成的两两偏好替代外部标量奖励。通过贝叶斯Bradley-Terry模型聚合噪声比较数据，获得不确定性感知的候选质量估计。利用双重汤普森采样指导比较预算分配至可能的极值点，并选择高质量父代生成改进候选。

Result: 在MathBench上准确率超越现有方法20个百分点；在LiveCodeBench上比同类迭代方法提升超过12个百分点。该方法完全无需奖励模型、真实标签或手工评分函数。

Conclusion: 两两自我偏好在大型离散输出空间中为测试时改进提供了强大的优化信号，展示了自监督优化的有效性。

Abstract: Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.

</details>


### [76] [Relatron: Automating Relational Machine Learning over Relational Databases](https://arxiv.org/abs/2602.22552)
*Zhikai Chen,Han Xie,Jian Zhang,Jiliang Tang,Xiang Song,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 本文通过统一关系型数据库上的关系深度学习(RDL)与深度特征合成(DFS)设计空间，发现RDL性能并非始终优于DFS且高度依赖任务特性，提出基于任务嵌入的元选择器Relatron，通过任务同配性和亲和性嵌入指导架构选择，在降低10倍成本的同时实现18.5%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 关系数据库预测建模面临跨表依赖与复杂特征交互的挑战，RDL虽通过消息传递自动化特征工程，DFS则依赖预定义聚合器，但两者优劣对比及架构设计原则尚未明确。

Method: 构建统一RDL与DFS的共享设计空间，执行多任务架构搜索，建立性能银行关联架构配置；提出任务同配性与包含规模、路径、特征及时序结构的亲和性嵌入两项任务信号，并开发基于任务嵌入的元选择器Relatron，辅以轻量级损失景观指标选择平坦最优解。

Result: 三大发现：(1)RDL未持续优于DFS，性能高度依赖任务；(2)无通用优势架构；(3)验证准确率不可靠。Relatron消除"调参越多性能越差"效应，联合超参数与架构优化时较基线提升18.5%，成本仅为Fisher信息方法的1/10。

Conclusion: 研究揭示了RDL-DFS性能差距的驱动因素，建立的Relatron通过任务信号实现原则性路由，证明任务感知的元选择策略可突破架构选择瓶颈，为关系数据库建模提供高效自动化方案。

Abstract: Predictive modeling over relational databases (RDBs) powers applications, yet remains challenging due to capturing both cross-table dependencies and complex feature interactions. Relational Deep Learning (RDL) methods automate feature engineering via message passing, while classical approaches like Deep Feature Synthesis (DFS) rely on predefined non-parametric aggregators. Despite performance gains, the comparative advantages of RDL over DFS and the design principles for selecting effective architectures remain poorly understood. We present a comprehensive study that unifies RDL and DFS in a shared design space and conducts architecture-centric searches across diverse RDB tasks. Our analysis yields three key findings: (1) RDL does not consistently outperform DFS, with performance being highly task-dependent; (2) no single architecture dominates across tasks, underscoring the need for task-aware model selection; and (3) validation accuracy is an unreliable guide for architecture choice. This search yields a model performance bank that links architecture configurations to their performance; leveraging this bank, we analyze the drivers of the RDL-DFS performance gap and introduce two task signals -- RDB task homophily and an affinity embedding that captures size, path, feature, and temporal structure -- whose correlation with the gap enables principled routing. Guided by these signals, we propose Relatron, a task embedding-based meta-selector that chooses between RDL and DFS and prunes the within-family search. Lightweight loss-landscape metrics further guard against brittle checkpoints by preferring flatter optima. In experiments, Relatron resolves the "more tuning, worse performance" effect and, in joint hyperparameter-architecture optimization, achieves up to 18.5% improvement over strong baselines with 10x lower cost than Fisher information-based alternatives.

</details>


### [77] [Multilingual Safety Alignment Via Sparse Weight Editing](https://arxiv.org/abs/2602.22554)
*Jiaming Liang,Zhaoxin Wang,Handing Wang*

Main category: cs.LG

TL;DR: 本文提出基于稀疏权重编辑的训练无关对齐框架，通过识别安全神经元，将低资源语言有害表示映射至高资源语言安全子空间，以单次高效计算显著降低低资源语言攻击成功率，同时保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在跨语言安全鸿沟，低资源语言易绕过英语等高资源语言的安全护栏。现有方法如多语言SFT和RLHF计算昂贵且依赖稀缺的多语言安全数据。

Method: 识别安全能力集中的稀疏神经元，将跨语言对齐建模为约束线性变换，推导闭式解将低资源语言有害表示最优映射至高资源语言安全子空间，并通过零空间投影保持通用效用。

Result: 在8种语言及Llama-3、Qwen-2.5等模型上的实验显示，该方法大幅降低低资源语言攻击成功率，对通用推理能力影响极小，仅需一次数据高效计算。

Conclusion: 该方法为低资源语言安全对齐提供了训练无关、数据高效的解决方案，通过稀疏权重编辑实现跨语言安全迁移，有效缩小安全差距且保持模型整体性能。

Abstract: Large Language Models (LLMs) exhibit significant safety disparities across languages, with low-resource languages (LRLs) often bypassing safety guardrails established for high-resource languages (HRLs) like English. Existing solutions, such as multilingual supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), are computationally expensive and dependent on scarce multilingual safety data. In this work, we propose a novel, training-free alignment framework based on Sparse Weight Editing. Identifying that safety capabilities are localized within a sparse set of safety neurons, we formulate the cross-lingual alignment problem as a constrained linear transformation. We derive a closed-form solution to optimally map the harmful representations of LRLs to the robust safety subspaces of HRLs, while preserving general utility via a null-space projection constraint. Extensive experiments across 8 languages and multiple model families (Llama-3, Qwen-2.5) demonstrate that our method substantially reduces Attack Success Rate (ASR) in LRLs with negligible impact on general reasoning capabilities, all achieved with a single, data-efficient calculation.

</details>


### [78] [Autoregressive Visual Decoding from EEG Signals](https://arxiv.org/abs/2602.22555)
*Sicheng Dai,Hongwang Xiao,Shan Yu,Qiwei Ye*

Main category: cs.LG

TL;DR: 本文提出AVDE，一种轻量级EEG视觉解码框架。该框架通过对比学习微调预训练模型LaBraM以对齐EEG与图像表征，并采用"下一尺度预测"自回归生成策略：利用VQ-VAE将图像编码为多尺度token，通过Transformer以EEG嵌入为起点逐步预测更细尺度token。实验表明，AVDE在图像检索与重建任务上超越现有最优方法，参数量仅10%，且其生成过程反映了人类视觉感知的层次性，为实用BCI应用提供了高效且可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前EEG视觉解码方法面临三大挑战：EEG与图像间的模态鸿沟、复杂多阶段适应过程易导致误差累积和一致性缺失，以及大规模扩散模型的计算开销限制了其现实BCI应用。因此，亟需开发轻量、高效且端到端一致的解码框架。

Method: AVDE框架包含两个核心组件：（1）表征对齐：基于预训练EEG模型LaBraM，通过对比学习微调实现EEG与图像语义空间的对齐；（2）自回归生成：采用"下一尺度预测"策略，将图像通过预训练VQ-VAE编码为多尺度token maps，训练Transformer以EEG嵌入作为最粗表示，自回归地预测更细尺度的token，从而建立EEG信号与重建图像间的直接映射。

Result: 在两个公开数据集上的实验表明：AVDE在图像检索和重建任务上均优于先前最优方法；模型参数量显著减少至对比方法的10%；中间输出可视化显示其生成过程体现了人类视觉感知的层次化特性。

Conclusion: 本研究验证了自回归模型作为高效且可解释工具在实用BCI应用中的潜力。AVDE的轻量化设计和层次化生成策略为EEG视觉解码提供了新范式，其高效性和可解释性有望推动BCI技术在真实场景中的部署与应用。

Abstract: Electroencephalogram (EEG) signals have become a popular medium for decoding visual information due to their cost-effectiveness and high temporal resolution. However, current approaches face significant challenges in bridging the modality gap between EEG and image data. These methods typically rely on complex adaptation processes involving multiple stages, making it hard to maintain consistency and manage compounding errors. Furthermore, the computational overhead imposed by large-scale diffusion models limit their practicality in real-world brain-computer interface (BCI) applications. In this work, we present AVDE, a lightweight and efficient framework for visual decoding from EEG signals. First, we leverage LaBraM, a pre-trained EEG model, and fine-tune it via contrastive learning to align EEG and image representations. Second, we adopt an autoregressive generative framework based on a "next-scale prediction" strategy: images are encoded into multi-scale token maps using a pre-trained VQ-VAE, and a transformer is trained to autoregressively predict finer-scale tokens starting from EEG embeddings as the coarsest representation. This design enables coherent generation while preserving a direct connection between the input EEG signals and the reconstructed images. Experiments on two datasets show that AVDE outperforms previous state-of-the-art methods in both image retrieval and reconstruction tasks, while using only 10% of the parameters. In addition, visualization of intermediate outputs shows that the generative process of AVDE reflects the hierarchical nature of human visual perception. These results highlight the potential of autoregressive models as efficient and interpretable tools for practical BCI applications.

</details>


### [79] [Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation](https://arxiv.org/abs/2602.22556)
*Zihang Xu,Haozhi Xie,Ziqi Miao,Wuxuan Gong,Chen Qian,Lijun Li*

Main category: cs.LG

TL;DR: 本文针对大推理模型在低复杂度查询上的过度思考问题，提出了一种两阶段稳定自适应思考框架，通过混合微调和自适应强化学习，在提升精度的同时大幅减少生成token数量。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在简单查询上过度思考导致计算资源浪费，现有方法在精度-效率权衡上不稳定且鲁棒性差，亟需实现稳定自适应的思考机制。

Method: 提出两阶段框架：第一阶段采用混合微调使模型同时学习思考与不思考行为，建立良好初始化；第二阶段进行自适应强化学习，引入保正确性优势塑形(CPAS)避免抑制正确长链推理，并采用长度感知梯度调节(LAGR)稳定长度异质性下的优化过程。

Result: 在Qwen2.5-1.5B和7B模型上相比强基线一致提升+3.7/+3.6精度点，同时减少40.6%/43.9%生成token，在不同难度和分布外任务上均表现出鲁棒性和泛化能力。

Conclusion: 该框架成功实现了大推理模型的稳定自适应思考，在显著提升精度和效率的同时保持了良好的鲁棒性和泛化性能。

Abstract: Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.

</details>


### [80] [Operationalizing Fairness: Post-Hoc Threshold Optimization Under Hard Resource Limits](https://arxiv.org/abs/2602.22560)
*Moirangthem Tiken Singh,Amit Kalita,Sapam Jitu Singh*

Main category: cs.LG

TL;DR: 针对高风险领域机器学习部署中公平性与安全性的权衡问题，本文提出一种与模型无关的阈值后优化框架，在严格的资源容量限制下，通过强制单一全局阈值确保法律合规，并平衡安全、效率与公平。实验表明，容量限制主导伦理优先级，在25%的限制下，该框架仍能保持较高风险识别率，而无约束的公平启发式方法则完全失效。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域部署机器学习需要在预测安全性与算法公平性之间取得平衡。然而，现有公平性干预措施通常假设资源不受限，并采用违反反歧视法规的群体特定决策阈值。因此，亟需一种能够在严格硬性容量约束下协同平衡安全、效率与公平，同时确保法律合规性（即强制单一全局决策阈值）的框架。

Method: 本文提出一种后验、模型无关的阈值优化框架。该框架通过参数化的伦理损失函数与有界决策规则，在严格的硬性容量约束下联合优化安全、效率与公平。为确保法律合规，框架强制使用单一全局决策阈值。从理论上证明了所部署阈值的关键性质，包括关于伦理权重的局部单调性以及临界容量区间的正式识别。

Result: 在多个高风险数据集上的广泛实验评估表明：1) 容量约束主导伦理优先级，严格的资源限制在超过80%的测试配置中决定了最终部署的阈值；2) 在25%的严格容量限制下，所提框架成功保持了较高的风险识别率（召回率0.409-0.702），而标准的无约束公平启发式方法则崩溃至接近零的效用。

Conclusion: 研究结论认为，理论公平性目标必须明确服从于 operational 容量限制才能在实际中部署。通过将预测评分与政策评估解耦，并严格限制干预率，该框架为利益相关者提供了一个实用且合法合规的机制，以应对资源受限环境中不可避免的伦理权衡。

Abstract: The deployment of machine learning in high-stakes domains requires a balance between predictive safety and algorithmic fairness. However, existing fairness interventions often as- sume unconstrained resources and employ group-specific decision thresholds that violate anti- discrimination regulations. We introduce a post-hoc, model-agnostic threshold optimization framework that jointly balances safety, efficiency, and equity under strict and hard capacity constraints. To ensure legal compliance, the framework enforces a single, global decision thresh- old. We formulated a parameterized ethical loss function coupled with a bounded decision rule that mathematically prevents intervention volumes from exceeding the available resources. An- alytically, we prove the key properties of the deployed threshold, including local monotonicity with respect to ethical weighting and the formal identification of critical capacity regimes. We conducted extensive experimental evaluations on diverse high-stakes datasets. The principal re- sults demonstrate that capacity constraints dominate ethical priorities; the strict resource limit determines the final deployed threshold in over 80% of the tested configurations. Furthermore, under a restrictive 25% capacity limit, the proposed framework successfully maintains high risk identification (recall ranging from 0.409 to 0.702), whereas standard unconstrained fairness heuristics collapse to a near-zero utility. We conclude that theoretical fairness objectives must be explicitly subordinated to operational capacity limits to remain in deployment. By decou- pling predictive scoring from policy evaluation and strictly bounding intervention rates, this framework provides a practical and legally compliant mechanism for stakeholders to navigate unavoidable ethical trade-offs in resource-constrained environments.

</details>


### [81] [S2O: Early Stopping for Sparse Attention via Online Permutation](https://arxiv.org/abs/2602.22575)
*Yu Zhang,Songwei Liu,Chenqian Yan,Sheng Lin,Beichen Ning,Fangmin Chen,Xing Wang*

Main category: cs.LG

TL;DR: S2O利用在线置换和早停机制突破稀疏注意力稀疏度上限，在Llama-3.1-8B上实现7.51倍注意力加速，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 注意力计算复杂度二次方增长限制长上下文推理。现有块级稀疏化存在固有稀疏度上限，粗粒度块设计使进一步性能提升困难。

Method: 借鉴内存虚拟-物理地址映射，S2O重构FlashAttention执行流程，支持非连续token加载。采用在线索引引导的离散加载策略聚焦高优先级块，并引入早停规则：按重要性降序计算，当块得分低于阈值时终止，在可控误差下提升有效稀疏度。

Result: 在Llama-3.1-8B/128K上下文下：匹配稀疏度时单算子MSE降低3.82倍；匹配MSE时预填充计算密度降低3.31倍；保持端到端精度；注意力加速7.51倍；端到端加速3.81倍。

Conclusion: S2O通过在线置换和早停机制显著提升稀疏注意力的实用稀疏度上限，为长序列推理提供高效计算方案。

Abstract: Attention scales quadratically with sequence length, fundamentally limiting long-context inference. Existing block-granularity sparsification can reduce latency, but coarse blocks impose an intrinsic sparsity ceiling, making further improvements difficult even with carefully engineered designs. We present S2O, which performs early stopping for sparse attention via online permutation. Inspired by virtual-to-physical address mapping in memory systems, S2O revisits and factorizes FlashAttention execution, enabling inference to load non-contiguous tokens rather than a contiguous span in the original order. Motivated by fine-grained structures in attention heatmaps, we transform explicit permutation into an online, index-guided, discrete loading policy; with extremely lightweight preprocessing and index-remapping overhead, it concentrates importance on a small set of high-priority blocks. Building on this importance-guided online permutation for loading, S2O further introduces an early-stopping rule: computation proceeds from high to low importance; once the current block score falls below a threshold, S2O terminates early and skips the remaining low-contribution blocks, thereby increasing effective sparsity and reducing computation under a controlled error budget.
  As a result, S2O substantially raises the practical sparsity ceiling. On Llama-3.1-8B under a 128K context, S2O reduces single-operator MSE by 3.82$\times$ at matched sparsity, and reduces prefill compute density by 3.31$\times$ at matched MSE; meanwhile, it preserves end-to-end accuracy and achieves 7.51$\times$ attention and 3.81$\times$ end-to-end speedups.

</details>


### [82] [IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck](https://arxiv.org/abs/2602.22581)
*Tian Bian,Yifan Niu,Chaohao Yuan,Chengzhi Piao,Bingzhe Wu,Long-Kai Huang,Yu Rong,Tingyang Xu,Hong Cheng,Jia Li*

Main category: cs.LG

TL;DR: 本文提出基于信息瓶颈原理的端到端优化框架IBCircuit，用于整体性发现语言模型中负责特定任务的计算子图。该方法无需为不同任务设计特定损坏激活，在IOI和Greater-Than任务中可识别出更忠实且更小的电路。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现研究忽视了电路的整体性，且需要针对不同任务设计特定的损坏激活，导致方法不准确且效率低下。为此，亟需一种通用的、整体性的电路发现方法。

Method: 提出IBCircuit框架，将电路发现问题建模为基于信息瓶颈原理的端到端优化问题。该框架可整体性地识别对任务最关键的计算子图，且适用于任意任务而无需繁琐的激活损坏设计。

Result: 在间接对象识别（IOI）和Greater-Than任务上的实验表明，IBCircuit相比现有方法能够识别出更忠实（more faithful）且更精简（minimal）的电路，在关键节点和边组件方面表现更优。

Conclusion: IBCircuit成功实现了语言模型电路的整体性发现，提供了一种任务无关的高效优化框架，为提升语言模型可解释性提供了新思路。

Abstract: Circuit discovery has recently attracted attention as a potential research direction to explain the non-trivial behaviors of language models. It aims to find the computational subgraphs, also known as circuits, within the model that are responsible for solving specific tasks. However, most existing studies overlook the holistic nature of these circuits and require designing specific corrupted activations for different tasks, which is inaccurate and inefficient. In this work, we propose an end-to-end approach based on the principle of Information Bottleneck, called IBCircuit, to identify informative circuits holistically. IBCircuit is an optimization framework for holistic circuit discovery and can be applied to any given task without tediously corrupted activation design. In both the Indirect Object Identification (IOI) and Greater-Than tasks, IBCircuit identifies more faithful and minimal circuits in terms of critical node components and edge components compared to recent related work.

</details>


### [83] [TabDLM: Free-Form Tabular Data Generation via Joint Numerical-Language Diffusion](https://arxiv.org/abs/2602.22586)
*Donghong Cai,Jiarui Feng,Yanbo Wang,Da Zheng,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: 本文提出TabDLM，一种基于掩码扩散语言模型的统一框架，用于生成包含自由文本、数值和类别特征的异构表格数据。该模型通过掩码扩散处理文本/类别特征，通过连续扩散与学习的专业数值标记嵌入处理数值特征，并采用双向注意力捕捉跨模态交互。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据日益包含自由文本字段，但现有方法面临挑战：扩散模型难以生成高质量文本，而LLM的离散标记化会扭曲数值精度。需要统一框架联合建模数值、类别和文本等多种模态。

Method: 提出TabDLM框架，基于掩码扩散语言模型(MDLM)。对文本和类别特征采用掩码扩散建模；对数值特征采用连续扩散过程，通过学习的专门数值标记嵌入表示；在单一模型内利用双向注意力机制捕捉跨模态交互。

Result: 在多个基准数据集上的广泛实验表明，TabDLM相较于强大的扩散基线和LLM基线方法均表现出优越性。

Conclusion: TabDLM通过统一的扩散框架成功解决了异构表格数据生成问题，实现了对文本、类别和数值特征的联合建模，为表格数据合成提供了有效解决方案。

Abstract: Synthetic tabular data generation has attracted growing attention due to its importance for data augmentation, foundation models, and privacy. However, real-world tabular datasets increasingly contain free-form text fields (e.g., reviews or clinical notes) alongside structured numerical and categorical attributes. Generating such heterogeneous tables with joint modeling of different modalities remains challenging. Existing approaches broadly fall into two categories: diffusion-based methods and LLM-based methods. Diffusion models can capture complex dependencies over numerical and categorical features in continuous or discrete spaces, but extending them to open-ended text is nontrivial and often leads to degraded text quality. In contrast, LLM-based generators naturally produce fluent text, yet their discrete tokenization can distort precise or wide-range numerical values, hindering accurate modeling of both numbers and language. In this work, we propose TabDLM, a unified framework for free-form tabular data generation via a joint numerical--language diffusion model built on masked diffusion language models (MDLMs). TabDLM models textual and categorical features through masked diffusion, while modeling numerical features with a continuous diffusion process through learned specialized numeric tokens embedding; bidirectional attention then captures cross-modality interactions within a single model. Extensive experiments on diverse benchmarks demonstrate the effectiveness of TabDLM compared to strong diffusion- and LLM-based baselines.

</details>


### [84] [pQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training](https://arxiv.org/abs/2602.22592)
*Wenzheng Zhang,Bingzheng Liu,Yang Hu,Xiaoying Bai,Wentao Zhang,Bin Cui*

Main category: cs.LG

TL;DR: 本文针对超低比特（亚2比特）量化大语言模型训练中的参数民主化效应问题，提出pQuant方法。该方法通过将线性层解耦为1比特主导分支和高精度敏感参数分支，并引入特征缩放引导参数分配，进一步扩展为稀疏激活的专家结构，从而在保持计算效率的同时提升模型表现力，实现了超低比特量化的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 超低比特量化对边缘部署至关重要，但现有量化感知训练方法在精度和扩展性上仍不足。研究发现存在"参数民主化效应"——所有参数的敏感度趋于同质化，严重限制了模型表达能力，这是当前方法的关键瓶颈。

Method: pQuant将线性层拆分为两个专用分支：用于高效计算的主导1比特分支和用于保留最敏感参数的精简高精度分支。通过定制的特征缩放机制，显式引导模型将敏感参数分配到高精度分支。进一步将该分支扩展为多个稀疏激活的专家，实现高效容量扩展。

Result: 大量实验表明，pQuant在超低比特量化任务上达到了最先进的性能水平。

Conclusion: pQuant通过参数解耦和稀疏专家扩展，有效解决了参数民主化效应这一关键瓶颈，为边缘设备上的高效大语言模型部署提供了新的可行方案。

Abstract: Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability. In this work, we identify a parameter democratization effect as a key bottleneck: the sensitivity of all parameters becomes homogenized, severely limiting expressivity. To address this, we propose pQuant, a method that decouples parameters by splitting linear layers into two specialized branches: a dominant 1-bit branch for efficient computation and a compact high-precision branch dedicated to preserving the most sensitive parameters. Through tailored feature scaling, we explicitly guide the model to allocate sensitive parameters to the high-precision branch. Furthermore, we extend this branch into multiple, sparsely-activated experts, enabling efficient capacity scaling. Extensive experiments indicate our pQuant achieves state-of-the-art performance in extremely low-bit quantization.

</details>


### [85] [Transformers converge to invariant algorithmic cores](https://arxiv.org/abs/2602.22600)
*Joshua S. Schiffman*

Main category: cs.LG

TL;DR: 该论文提出"算法核心"概念来理解大型语言模型的内部工作机制，发现不同训练运行和规模的Transformer模型都收敛到相同的低维不变结构，这些结构反映了计算本质而非训练细节，为机制可解释性提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型内部工作原理是核心挑战。由于训练选择行为而非电路结构，许多权重配置可实现相同功能。关键问题在于：哪些内部结构是计算所必需的，哪些只是特定训练运行的偶然产物？这阻碍了我们对模型真实计算方式的理解。

Method: 提出提取"算法核心"的方法——识别对任务性能必要且充分的紧凑子空间。通过分析三类Transformer模型验证：1)独立训练模型的权重差异与核心收敛性；2)Markov链模型的3D核心嵌入与转移谱；3)模加法模型在grokking阶段的循环算子演化；4)GPT-2主谓一致的单轴控制机制。

Result: 独立训练的Transformer学习不同权重但收敛到相同算法核心；Markov-chain模型在近乎正交的子空间中嵌入3D核心却具有相同转移谱；模加法Transformer在泛化临界点(grokking)发现紧凑循环算子并随后膨胀，可预测记忆到泛化的转变；GPT-2通过单一轴控制主谓一致，翻转该轴可全局反转语法数。这些结果揭示了跨训练和规模持续存在的低维不变量。

Conclusion: Transformer计算围绕紧凑的共享算法结构组织，而非实现细节。机制可解释性应优先 targeting 这些跨尺度的计算不变量——即计算的本质——而非特定实现的细节。这为理解复杂模型内部工作提供了更 principled 的框架。

Abstract: Large language models exhibit sophisticated capabilities, yet understanding how they work internally remains a central challenge. A fundamental obstacle is that training selects for behavior, not circuitry, so many weight configurations can implement the same function. Which internal structures reflect the computation, and which are accidents of a particular training run? This work extracts algorithmic cores: compact subspaces necessary and sufficient for task performance. Independently trained transformers learn different weights but converge to the same cores. Markov-chain transformers embed 3D cores in nearly orthogonal subspaces yet recover identical transition spectra. Modular-addition transformers discover compact cyclic operators at grokking that later inflate, yielding a predictive model of the memorization-to-generalization transition. GPT-2 language models govern subject-verb agreement through a single axis that, when flipped, inverts grammatical number throughout generation across scales. These results reveal low-dimensional invariants that persist across training runs and scales, suggesting that transformer computations are organized around compact, shared algorithmic structures. Mechanistic interpretability could benefit from targeting such invariants -- the computational essence -- rather than implementation-specific details.

</details>


### [86] [$φ$-DPO: Fairness Direct Preference Optimization Approach to Continual Learning in Large Multimodal Models](https://arxiv.org/abs/2602.22601)
*Thanh-Dat Truong,Huu-Thien Tran,Jackson Cothren,Bhiksha Raj,Khoa Luu*

Main category: cs.LG

TL;DR: 针对大型多模态模型持续学习中的公平性问题，本论文提出一种新颖的Fairness Direct Preference Optimization (FaiDPO/φ-DPO)框架，通过显式处理数据不平衡来同时缓解灾难性遗忘和分布偏差，并在多个基准测试上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在持续学习中面临灾难性遗忘问题，现有研究多聚焦于此，但由数据不平衡导致的模型偏见和跨任务性能下降等公平性问题尚未得到充分探索，亟需专门方法解决。

Method: 提出基于直接偏好优化(DPO)的新持续学习范式：首先利用成对偏好信号对齐学习以减轻灾难性遗忘；然后针对传统DPO在数据不平衡下的局限性，设计φ-DPO损失函数显式纠正分布偏差；提供理论分析证明其同时处理遗忘和不平衡问题；并为现有基准构建持续学习所需的成对偏好标注。

Result: 大量实验和消融研究表明，所提φ-DPO框架在多个基准测试上均达到最先进性能，显著优于先前的LMM持续学习方法。

Conclusion: 该研究成功将公平性引入LMM持续学习领域，φ-DPO框架有效解决了数据不平衡引起的模型偏见问题，同时保持对灾难性遗忘的抑制能力，为未来研究提供了新方向和新基准。

Abstract: Fairness in Continual Learning for Large Multimodal Models (LMMs) is an emerging yet underexplored challenge, particularly in the presence of imbalanced data distributions that can lead to biased model updates and suboptimal performance across tasks. While recent continual learning studies have made progress in addressing catastrophic forgetting, the problem of fairness caused the imbalanced data remains largely underexplored. This paper presents a novel Fairness Direct Preference Optimization (FaiDPO or $φ$-DPO) framework for continual learning in LMMs. In particular, we first propose a new continual learning paradigm based on Direct Preference Optimization (DPO) to mitigate catastrophic forgetting by aligning learning with pairwise preference signals. Then, we identify the limitations of conventional DPO in imbalanced data and present a new $φ$-DPO loss that explicitly addresses distributional biases. We provide a comprehensive theoretical analysis demonstrating that our approach addresses both forgetting and data imbalance. Additionally, to enable $φ$-DPO-based continual learning, we construct pairwise preference annotations for existing benchmarks in the context of continual learning. Extensive experiments and ablation studies show the proposed $φ$-DPO achieves State-of-the-Art performance across multiple benchmarks, outperforming prior continual learning methods of LMMs.

</details>


### [87] [DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion](https://arxiv.org/abs/2602.22610)
*Tao Huang,Jiayang Meng,Xu Yang,Chen Hou,Hong Chen*

Main category: cs.LG

TL;DR: 本文针对条件扩散模型在差分隐私训练中因异质条件上下文导致的重尾梯度问题，提出DP-aware AdaLN-Zero机制。该方法通过有界重参数化约束条件表示幅度和自适应层归一化调制参数，在梯度裁剪前抑制极端梯度尾部，从而减少裁剪偏差。在电力数据集和ETT基准测试上，该方法在匹配隐私设置下显著提升了插值/填补和预测性能。


<details>
  <summary>Details</summary>
Motivation: 条件注入使扩散模型能够生成上下文感知的时间序列输出，但在差分隐私随机梯度下降（DP-SGD）框架下，异质性条件上下文（如观测历史、缺失模式或异常协变量）会产生重尾逐例梯度。这些罕见的条件驱动的重尾梯度会不成比例地触发全局裁剪，导致异常值主导的更新、更大的裁剪偏差，以及在固定隐私预算下效用下降。

Method: DP-aware AdaLN-Zero是一种即插即用的敏感性感知条件机制，用于条件扩散变换器。它通过有界重参数化联合约束条件表示幅度和AdaLN调制参数，在梯度裁剪和噪声注入之前抑制极端梯度尾部事件，而无需修改DP-SGD机制本身。

Result: 在匹配隐私设置下，配备DP-aware AdaLN-Zero的DP-SGD在插值/填补和预测任务上均有显著改进。在真实电力数据集和两个公共ETT基准测试上，相比原生DP-SGD均观察到一致的性能提升。梯度诊断表明，这些改进源于条件特定的尾部重塑和裁剪失真的减少，同时在非私有训练中保持表达能力。

Conclusion: 敏感性感知条件机制可在不牺牲标准性能的前提下，显著改善私有条件扩散模型的训练效果，为差分隐私时间序列生成提供了有效的解决方案。

Abstract: Condition injection enables diffusion models to generate context-aware outputs, which is essential for many time-series tasks. However, heterogeneous conditional contexts (e.g., observed history, missingness patterns or outlier covariates) can induce heavy-tailed per-example gradients. Under Differentially Private Stochastic Gradient Descent (DP-SGD), these rare conditioning-driven heavy-tailed gradients disproportionately trigger global clipping, resulting in outlier-dominated updates, larger clipping bias, and degraded utility under a fixed privacy budget. In this paper, we propose DP-aware AdaLN-Zero, a drop-in sensitivity-aware conditioning mechanism for conditional diffusion transformers that limits conditioning-induced gain without modifying the DP-SGD mechanism. DP-aware AdaLN-Zero jointly constrains conditioning representation magnitude and AdaLN modulation parameters via bounded re-parameterization, suppressing extreme gradient tail events before gradient clipping and noise injection. Empirically, DP-SGD equipped with DP-aware AdaLN-Zero improves interpolation/imputation and forecasting under matched privacy settings. We observe consistent gains on a real-world power dataset and two public ETT benchmarks over vanilla DP-SGD. Moreover, gradient diagnostics attribute these improvements to conditioning-specific tail reshaping and reduced clipping distortion, while preserving expressiveness in non-private training. Overall, these results show that sensitivity-aware conditioning can substantially improve private conditional diffusion training without sacrificing standard performance.

</details>


### [88] [Semantic Tube Prediction: Beating LLM Data Efficiency with JEPA](https://arxiv.org/abs/2602.22617)
*Hai Huang,Yann LeCun,Randall Balestriero*

Main category: cs.LG

TL;DR: 论文提出语义管预测(STP)方法，基于测地线假设，通过约束隐藏状态轨迹提升大型语言模型的数据效率。在NL-RX-SYNTH数据集上，该方法使模型仅用1/16的训练数据即可达到基线准确率，挑战了Chinchilla缩放定律的数据项，证明几何先验可超越暴力缩放。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的缩放定律仅为描述性而非指导性，仅刻画典型训练而非最优训练。鲜有研究成功挑战这些定律所蕴含的数据效率边界。为此，本文旨在探索通过原则性几何先验突破数据缩放限制的方法。

Method: 1) 提出测地线假设，认为词元序列在光滑语义流形上沿测地线演化且局部线性；2) 基于此设计语义管预测(STP)任务，采用JEPA风格正则化器，将隐藏状态轨迹约束在测地线的管状邻域内；3) STP无需显式多视图增强即可推广至语言建模，通过提升信噪比和防止推理时轨迹碰撞来保持多样性。

Result: 在NL-RX-SYNTH数据集上的实证研究表明，STP使大型语言模型以16×更少的训练数据达到基线准确率，直接违反Chinchilla风格缩放定律的数据项，显著提升了数据效率。

Conclusion: 原则性几何先验能够超越暴力缩放策略，通过合理的几何约束可突破传统缩放定律的局限，为开发更高效的大型语言模型训练方法提供了新方向。

Abstract: Large Language Models (LLMs) obey consistent scaling laws -- empirical power-law fits that predict how loss decreases with compute, data, and parameters. While predictive, these laws are descriptive rather than prescriptive: they characterize typical training, not optimal training. Surprisingly few works have successfully challenged the data-efficiency bounds implied by these laws -- which is our primary focus. To that end, we introduce the Geodesic Hypothesis, positing that token sequences trace geodesics on a smooth semantic manifold and are therefore locally linear. Building on this principle, we propose a novel Semantic Tube Prediction (STP) task, a JEPA-style regularizer that confines hidden-state trajectories to a tubular neighborhood of the geodesic. STP generalizes JEPA to language without requiring explicit multi-view augmentations. We show this constraint improves signal-to-noise ratio, and consequently preserves diversity by preventing trajectory collisions during inference. Empirically, STP allows LLMs to match baseline accuracy with 16$\times$ less training data on the NL-RX-SYNTH dataset, directly violating the data term of Chinchilla-style scaling laws and demonstrating that principled geometric priors can surpass brute-force scaling. Code is available at https://github.com/galilai-group/llm-jepa#stp.

</details>


### [89] [ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL](https://arxiv.org/abs/2602.22623)
*Xingyu Lu,Jinpeng Wang,YiFan Zhang,Shijie Ma,Xiao Hu,Tianke Zhang,Haonan fan,Kaiyu Jiang,Changyi Liu,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Chun Yuan*

Main category: cs.LG

TL;DR: ...


<details>
  <summary>Details</summary>
Motivation: ...

Method: ...

Result: ...

Conclusion: ...

Abstract: We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples with the right answer but low-quality reasoning process). To improve Reachability, we introduce a multi-turn sampling strategy where the reward model generates mistake reports for failed attempts, guiding the policy to "recover" correct responses from previously all-negative groups. Experimental results on 11 perception and reasoning benchmarks show that ContextRL significantly improves knowledge discovery efficiency. Notably, ContextRL enables the Qwen3-VL-8B model to achieve performance comparable to the 32B model, outperforming standard RLVR baselines by a large margin while effectively mitigating reward hacking. Our in-depth analysis reveals the significant potential of contextual information for improving reward model accuracy and document the widespread occurrence of reward hacking, offering valuable insights for future RLVR research.

</details>


### [90] [Tackling Privacy Heterogeneity in Differentially Private Federated Learning](https://arxiv.org/abs/2602.22633)
*Ruichen Xu,Ying-Jun Angela Zhang,Jianwei Huang*

Main category: cs.LG

TL;DR: 首次系统研究差分隐私联邦学习中的隐私感知客户端选择，理论量化隐私异质性影响，提出凸优化选择策略，在CIFAR-10上实现高达10%准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私联邦学习方法假设统一的隐私预算，不符合现实中隐私需求差异巨大的实际情况。隐私异质性导致传统基于数据量的客户端选择策略无法区分高质量更新与含噪更新，严重影响模型性能。

Method: 建立收敛分析理论框架量化隐私异质性对训练误差的影响，将隐私感知客户端选择建模为凸优化问题，自适应调整选择概率以最小化训练误差。

Result: 在基准数据集上的实验表明，该方法在异构隐私预算下相比现有基线在CIFAR-10数据集上测试准确率提升高达10%。

Conclusion: 研究强调了将隐私异质性纳入客户端选择机制的重要性，为实现更实用有效的联邦学习提供了理论指导和实践方案，推动了该领域应对现实复杂隐私需求的发展。

Abstract: Differentially private federated learning (DP-FL) enables clients to collaboratively train machine learning models while preserving the privacy of their local data. However, most existing DP-FL approaches assume that all clients share a uniform privacy budget, an assumption that does not hold in real-world scenarios where privacy requirements vary widely. This privacy heterogeneity poses a significant challenge: conventional client selection strategies, which typically rely on data quantity, cannot distinguish between clients providing high-quality updates and those introducing substantial noise due to strict privacy constraints. To address this gap, we present the first systematic study of privacy-aware client selection in DP-FL. We establish a theoretical foundation by deriving a convergence analysis that quantifies the impact of privacy heterogeneity on training error. Building on this analysis, we propose a privacy-aware client selection strategy, formulated as a convex optimization problem, that adaptively adjusts selection probabilities to minimize training error. Extensive experiments on benchmark datasets demonstrate that our approach achieves up to a 10% improvement in test accuracy on CIFAR-10 compared to existing baselines under heterogeneous privacy budgets. These results highlight the importance of incorporating privacy heterogeneity into client selection for practical and effective federated learning.

</details>


### [91] [Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning](https://arxiv.org/abs/2602.22642)
*Qin-Wen Luo,Sheng Ren,Xiang Chen,Rui Liu,Jun Fang,Naiqiang Tan,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: 针对链式思考推理过程冗长导致的高推理成本问题，现有压缩方法因显式优化短轨迹引发熵崩塌，损害复杂问题的推理能力。本文提出CEEH，通过难度感知的动态熵正则化和历史最短正确答案锚定的长度惩罚，在简单问题上激进压缩，在困难问题上保持探索空间，在六个基准测试中实现长度显著降低且精度不降。


<details>
  <summary>Details</summary>
Motivation: 链式思考虽提升大语言模型复杂推理能力，但其显式推理步骤的冗长性导致推理延迟和计算成本过高，限制实际应用。现有压缩方法在追求简洁性时往往牺牲推理能力。关键问题在于显式优化短轨迹会触发熵崩塌，过早压缩探索空间，阻碍对需要深度推理的难题的有效求解，亟需一种能平衡压缩效率与推理鲁棒性的新方法。

Method: 提出CEEH（压缩简单问题，探索困难问题）难度感知强化学习框架：1）动态评估样本难度；2）实施选择性熵正则化，对困难问题保留多样化搜索空间确保鲁棒性，对简单问题允许激进压缩；3）引入基于历史最短正确答案的动态最优长度惩罚，对抗熵引发的长度膨胀并稳定奖励信号。

Result: 在六个推理基准测试中，CEEH持续降低响应长度，同时保持与基础模型相当的准确率，并在Pass@k指标上优于仅优化长度的方法。

Conclusion: CEEH通过难度感知的差异化熵调控机制，有效解决了CoT压缩中的熵崩塌难题，实现了推理效率与能力的平衡，为复杂推理任务的实时部署提供了可行方案。

Abstract: Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.

</details>


### [92] [MUG: Meta-path-aware Universal Heterogeneous Graph Pre-Training](https://arxiv.org/abs/2602.22645)
*Lianze Shan,Jitao Zhao,Dongxiao He,Yongqi Huang,Zhiyong Feng,Weixiong Zhang*

Main category: cs.LG

TL;DR: 针对异质图通用预训练尚未解决的问题，本文提出MUG方法，通过输入统一模块和维度感知编码器构建跨图统一表示空间，并借助共享编码器与全局目标学习可迁移的元路径结构模式，在真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有通用图预训练研究局限于同质图，而异质图因具备更复杂的结构和语义特性，面临两大挑战：(i) 节点与关系类型多样性及其数据集特定语义阻碍统一表示空间的构建；(ii) 不同数据集的元路径数量与语义差异导致从单一数据集学得的编码聚合模式难以泛化。因此，亟需探索适用于异质图的通用预训练方法。

Method: 提出Meta-path-aware Universal heterogeneous Graph pre-training (MUG)方法。具体地：(i) 输入统一模块整合各异质图中多类型节点与关系信息，生成统一表示，再由维度感知编码器投影至共享空间，实现跨图对齐；(ii) 采用共享编码器捕获不同元路径视图下的一致性结构模式，避免依赖数据集特定的聚合策略，同时引入全局目标函数增强表示判别能力并降低数据集偏差。

Result: 在多个真实世界异质图数据集上的综合实验表明，MUG方法能够有效学习可泛化的异质图表示，在下游任务中取得显著性能提升。

Conclusion: 本研究首次系统探索了异质图通用预训练问题，提出的MUG方法通过表示空间统一与结构模式共享策略，成功解决了异质图预训练中的关键挑战，为异质图表示学习提供了新的预训练范式，具有较强的理论价值和应用潜力。

Abstract: Universal graph pre-training has emerged as a key paradigm in graph representation learning, offering a promising way to train encoders to learn transferable representations from unlabeled graphs and to effectively generalize across a wide range of downstream tasks. However, recent explorations in universal graph pre-training primarily focus on homogeneous graphs and it remains unexplored for heterogeneous graphs, which exhibit greater structural and semantic complexity. This heterogeneity makes it highly challenging to train a universal encoder for diverse heterogeneous graphs: (i) the diverse types with dataset-specific semantics hinder the construction of a unified representation space; (ii) the number and semantics of meta-paths vary across datasets, making encoding and aggregation patterns learned from one dataset difficult to apply to others. To address these challenges, we propose a novel Meta-path-aware Universal heterogeneous Graph pre-training (MUG) approach. Specifically, for challenge (i), MUG introduces a input unification module that integrates information from multiple node and relation types within each heterogeneous graph into a unified representation.This representation is then projected into a shared space by a dimension-aware encoder, enabling alignment across graphs with diverse schemas.Furthermore, for challenge (ii), MUG trains a shared encoder to capture consistent structural patterns across diverse meta-path views rather than relying on dataset-specific aggregation strategies, while a global objective encourages discriminability and reduces dataset-specific biases. Extensive experiments demonstrate the effectiveness of MUG on some real datasets.

</details>


### [93] [LEDA: Latent Semantic Distribution Alignment for Multi-domain Graph Pre-training](https://arxiv.org/abs/2602.22660)
*Lianze Shan,Jitao Zhao,Dongxiao He,Siqi Liu,Jiaxu Cui,Weixiong Zhang*

Main category: cs.LG

TL;DR: 本文提出LEDA模型，通过维度投影单元和变分语义推理模块解决通用图预训练中的语义对齐与训练指导不足问题，在少样本跨域设置下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 受GPT、DeepSeek等通用大模型进展启发，将通用性引入图预训练以学习跨域可泛化的知识。但现有方法存在数据对齐过于简单导致语义失准，以及训练指导有限难以捕捉多图有效知识两大挑战。

Method: 提出Latent sEmantic Distribution Alignment (LEDA)模型：1) 设计维度投影单元，自适应地将不同领域特征对齐到共享语义空间；2) 构建变分语义推理模块获取共享潜分布，用以指导领域投影，实现跨域语义对齐学习。

Result: LEDA在广泛的图和下游任务上表现优异。特别地，在少样本跨域设置下，它显著优于域内基线和先进通用预训练模型。

Conclusion: LEDA通过潜语义分布对齐有效解决了通用图预训练的核心挑战，为跨领域图表示学习提供了新范式，展现了强大的泛化能力和应用潜力。

Abstract: Recent advances in generic large models, such as GPT and DeepSeek, have motivated the introduction of universality to graph pre-training, aiming to learn rich and generalizable knowledge across diverse domains using graph representations to improve performance in various downstream applications. However, most existing methods face challenges in learning effective knowledge from generic graphs, primarily due to simplistic data alignment and limited training guidance. The issue of simplistic data alignment arises from the use of a straightforward unification for highly diverse graph data, which fails to align semantics and misleads pre-training models. The problem with limited training guidance lies in the arbitrary application of in-domain pre-training paradigms to cross-domain scenarios. While it is effective in enhancing discriminative representation in one data space, it struggles to capture effective knowledge from many graphs. To address these challenges, we propose a novel Latent sEmantic Distribution Alignment (LEDA) model for universal graph pre-training. Specifically, we first introduce a dimension projection unit to adaptively align diverse domain features into a shared semantic space with minimal information loss. Furthermore, we design a variational semantic inference module to obtain the shared latent distribution. The distribution is then adopted to guide the domain projection, aligning it with shared semantics across domains and ensuring cross-domain semantic learning. LEDA exhibits strong performance across a broad range of graphs and downstream tasks. Remarkably, in few-shot cross-domain settings, it significantly outperforms in-domain baselines and advanced universal pre-training models.

</details>


### [94] [Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support](https://arxiv.org/abs/2602.22673)
*Md Tanvir Hasan Turja*

Main category: cs.LG

TL;DR: 本文针对全球抗菌药物耐药性(AMR)危机，利用WHO GLASS监测数据开发了两阶段框架：首先通过机器学习预测耐药趋势，XGBoost模型表现最佳(MAE 7.07%，R² 0.854)；其次构建RAG系统结合政策文档与语言模型，为循证决策提供支持。研究发现前一年耐药率是最关键预测因子，且不同WHO区域预测精度存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 抗菌药物耐药性(AMR)是全球性公共卫生危机，预计到2050年将导致每年1000万人死亡。尽管WHO建立了覆盖44个国家的GLASS标准化监测系统，但现有研究缺乏利用该数据进行群体层面耐药趋势的机器学习预测。本研究旨在填补这一空白，开发可预测AMR趋势并支持循证决策的框架。

Method: 研究提出两阶段框架：第一阶段对2021-2023年WHO六个区域的5,909条GLASS观测数据，基准测试了六种模型(朴素预测、线性回归、岭回归、XGBoost、LightGBM和LSTM)。第二阶段构建检索增强生成(RAG)管道，将WHO政策文档的ChromaDB向量库与本地部署的Phi-3 Mini语言模型相结合，生成可追溯来源、抑制幻觉的政策建议。

Result: XGBoost模型表现最优，测试集MAE为7.07%，R²达0.854，较朴素基线提升83.1%。特征重要性分析显示，前一年耐药率是最主要预测因子(占比50.5%)。区域间预测性能差异显著，欧洲区域MAE最低(4.16%)，东南亚区域最高(10.14%)。RAG系统成功实现了基于政策文档的循证决策支持。

Conclusion: 本研究证明了机器学习在预测AMR趋势方面的有效性，特别是基于历史耐药率数据。XGBoost的优越性能为公共卫生监测提供了实用工具。RAG框架的引入为政策制定提供了可解释、可追溯的AI支持。研究结果同时揭示了不同区域预测难度的差异性，为资源分配和针对性干预提供了依据。代码与数据开源，有助于推动AMR预测研究的发展。

Abstract: Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja

</details>


### [95] [Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement](https://arxiv.org/abs/2602.22681)
*Shuchen Zhu,Rizhen Hu,Mingze Wang,Mou Sun,Xue Wang,Kun Yuan,Zaiwen Wen*

Main category: cs.LG

TL;DR: 针对大语言模型预训练优化效率问题，本文提出LITE加速策略。通过建立黎曼常微分方程框架揭示自适应优化器工作机制，针对性在平坦方向增大Hessian阻尼与学习率，显著加速Muon和SOAP等优化器，在多种模型规模与数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练依赖巨量计算资源，优化器效率是关键瓶颈。现有研究指出优化景观高度各向异性，损失下降主要由平坦方向驱动。然而Muon和SOAP等矩阵优化器虽利用曲率信息超越AdamW，但其更新倾向各向同性——在平坦方向保守、尖锐方向激进，未能适配景观特性，制约训练效率。

Method: 作者建立统一的黎曼常微分方程框架，阐明预条件子诱导黎曼几何缓解病态条件、动量作为黎曼阻尼项促进收敛的协同机制。基于此提出LITE（轻量级各向同性-各向异性增强）广义加速策略，在平坦轨迹上应用更大的Hessian阻尼系数与学习率，针对性增强各向异性景观中的训练动力学。

Result: 实验表明LITE在Dense/MoE架构、1.3亿至13亿参数、C4/Pile数据集及余弦/预热-稳定-衰减调度下，显著加速Muon和SOAP。理论分析证实LITE在平坦方向实现更快收敛，为高效预训练提供原则性方法，代码已开源。

Conclusion: 该研究通过黎曼几何视角深刻解析自适应优化器内在机理，LITE策略成功弥合各向同性更新与景观各向异性的鸿沟。成果不仅提供实用加速工具，更为未来优化器设计奠定新理论框架，对降低大规模模型训练成本具有重要价值。

Abstract: Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.

</details>


### [96] [NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion](https://arxiv.org/abs/2602.22911)
*Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 本文提出NoRA（非线性秩适应），通过引入SiLU门控和结构丢弃来突破LoRA在复杂推理任务中的"线性天花板"。在SlimOrca基准上，秩64的NoRA（困惑度3.89）优于秩512的LoRA（困惑度3.90）；在数学推理上，NoRA达到1.97困惑度，超越LoRA的饱和点2.07。


<details>
  <summary>Details</summary>
Motivation: LoRA作为参数高效微调的主流方法，在复杂推理任务中存在根本性局限：由于其线性约束，单纯增加秩维度会带来收益递减，形成"线性天花板"。这限制了在需要高容量建模的场景中的表现，特别是在数学推理等复杂任务上。

Method: 提出NoRA（非线性秩适应），这是一种权重级并行适配器。核心创新包括：1）注入SiLU激活函数引入非线性门控机制；2）应用结构丢弃策略诱导流形扩张；3）通过并行结构设计打破线性约束，激活奇异值谱中未被充分利用的尾部成分。

Result: 实验表明NoRA显著优于LoRA：1）在SlimOrca基准上，秩64的NoRA达到困惑度3.89，优于秩512的LoRA（3.90），显示出更高的谱效率；2）在MathInstruct数学推理数据集上，NoRA困惑度1.97，突破LoRA的2.07饱和点；3）SVD机制分析证实NoRA能有效激活奇异值谱尾部，防止线性方法中的秩崩溃现象。

Conclusion: NoRA成功突破了LoRA的线性限制，为参数高效微调提供了新的非线性范式。通过引入简单但有效的非线性变换，NoRA以更低的秩实现更好的性能，为复杂推理任务的模型适应提供了更优解决方案，证明了非线性方法在PEFT领域的巨大潜力。

Abstract: Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA's saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods.

</details>


### [97] [Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning](https://arxiv.org/abs/2602.22703)
*Hao Yu,Shuning Jia,Guanghao Li,Wenhao Jiang,Chun Yuan*

Main category: cs.LG

TL;DR: 本文提出 GeoPerceive 基准和 GeoDPO 框架，用于解决视觉语言模型在几何推理中因基础图表元素感知不足而表现不佳的问题。GeoPerceive 提供带领域特定语言（DSL）表示的图表数据和自动数据生成管道，实现几何感知与推理的分离评估；GeoDPO 利用自然语言到 DSL 的翻译器提供细粒度奖励信号进行强化学习。实验表明，相比仅带来边际改进甚至损害跨域性能的有监督微调（SFT），GeoDPO 在域内数据上提升 26.5%，跨域数据上提升 8.0%，下游推理任务上提升 39.0%，展现出更优的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）因对基础图表元素感知能力有限，在几何推理任务中表现不佳。现有方法无法有效分离评估几何感知与推理能力，也缺乏针对提升几何感知的专门训练框架，导致模型性能提升受限。

Method: 1) 构建 GeoPerceive 基准：包含图表实例及其领域特定语言（DSL）表示，并设计自动化数据生成管道，实现几何感知能力的独立评估；2) 提出 GeoDPO 框架：采用自然语言到 DSL 的翻译器（在 GeoPerceive 合成数据上训练）桥接两种语言模态，利用翻译器计算细粒度的 DSL 级别评分作为强化学习的奖励信号，优化 VLM 的几何感知能力。

Result: 在域内和跨域数据集上的评估显示：有监督微调（SFT）仅带来边际改进且可能损害跨域性能，而 GeoDPO 取得显著提升——域内数据性能提高 26.5%，跨域数据提高 8.0%，下游推理任务提高 39.0%。结果表明 GeoDPO 在性能和泛化能力上均优于 SFT。

Conclusion: GeoDPO 框架通过翻译器引导的强化学习机制和 DSL 级别奖励信号，有效提升了视觉语言模型的几何感知能力，展现出卓越的域内和跨域性能增益以及泛化能力，为解决 VLM 几何推理挑战提供了有效方案。

Abstract: Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\%$ on in-domain data, $+8.0\%$ on out-of-domain data, and $+39.0\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive
  to ensure reproducibility.

</details>


### [98] [Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching](https://arxiv.org/abs/2602.22812)
*Hiroki Matsutani,Naoki Matsuda,Naoto Sugiura*

Main category: cs.LG

TL;DR: 本文针对资源受限边缘设备上的本地大语言模型推理性能瓶颈问题，提出分布式提示缓存机制，通过跨设备共享中间处理状态并利用提示相似性支持部分匹配，结合布隆过滤器目录结构减少无线通信开销，在树莓派Zero 2W平台上使用Gemma-3 270M模型和MMLU数据集验证，平均降低首令牌时间和末令牌时间分别为93.12%和50.07%。


<details>
  <summary>Details</summary>
Motivation: 资源受限的边缘设备在执行本地大语言模型推理时存在严重的性能瓶颈，导致推理延迟高、用户体验差。传统的单机推理模式无法充分利用边缘设备集群的协同潜力。

Method: 提出分布式提示缓存机制，通过在多台低端边缘设备间协同共享中间处理状态来提升推理性能。该机制利用提示相似性支持部分匹配，并引入基于布隆过滤器的目录数据结构来检测远程服务器是否持有目标内部状态，从而抑制不必要的无线通信开销。

Result: 在树莓派Zero 2W平台上使用Gemma-3 270M模型和MMLU数据集的实验表明，该方法平均降低了93.12%的首令牌时间（TTFT）和50.07%的末令牌时间（TTLT）。

Conclusion: 所提出的分布式提示缓存方法能够有效解决资源受限边缘设备的大模型推理性能瓶颈问题，通过状态共享和智能通信优化，显著降低推理延迟，为边缘智能提供了可行的技术路径。

Abstract: Since local LLM inference on resource-constrained edge devices imposes a severe performance bottleneck, this paper proposes distributed prompt caching to enhance inference performance by cooperatively sharing intermediate processing states across multiple low-end edge devices. To fully utilize prompt similarity, our distributed caching mechanism also supports partial matching. As this approach introduces communication overhead associated with state sharing over a wireless network, we introduce a Bloom-filter-based data structure, referred to as a catalog, to determine whether a remote server possesses the desired internal states, thereby suppressing unnecessary communication. Experiments using the Gemma-3 270M model and the MMLU dataset on the Raspberry Pi Zero 2W platform demonstrate that the proposed approach reduces TTFT (Time to First Token) and TTLT (Time to Last Token) by 93.12% and 50.07% on average, respectively.

</details>


### [99] [Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2602.22817)
*Shuo He,Lang Feng,Qi Wei,Xin Cheng,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: 本文针对长程智能体任务中的组基强化学习问题，提出了一种新的层次化组策略优化方法（HGPO），通过将每个步骤分配到基于历史上下文一致性的多个层次化组中，并自适应加权聚合优势估计，解决了上下文不一致导致的偏差问题，在ALFWorld和WebShop任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的强化学习方法在处理长程智能体任务时，逐步转向步骤级组策略优化，但存在上下文不一致问题：同一组内的不同步骤可能具有不同的历史上下文，导致优势估计严重偏差，进而显著降低策略优化效果。

Method: 提出层次化组策略优化（HGPO），在轨迹组内根据历史上下文一致性将每个步骤分配到多个层次化组中，在每个组内计算独立的优势估计，并通过自适应加权方案进行聚合，从而在优势估计中实现偏差-方差权衡，且无需额外模型或采样。

Result: 在ALFWorld和WebShop两个挑战性智能体任务上，使用Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct模型进行评估，HGPO在相同计算约束下显著优于现有智能体强化学习方法。

Conclusion: HGPO方法有效解决了步骤级优势估计中的上下文不一致问题，通过层次化分组和自适应加权实现了更优的偏差-方差权衡，为长程智能体任务提供了一种高效且性能优越的策略优化方案。

Abstract: Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.

</details>


### [100] [MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction](https://arxiv.org/abs/2602.22850)
*Yi He,Yina Cao,Jixiu Zhai,Di Wang,Junxiao Kong,Tianchi Lu*

Main category: cs.LG

TL;DR: 本研究提出可解释深度学习模型MEDNA-DFM及机制启发的信号纯化算法，用于DNA甲基化精准识别。该模型通过捕获跨物种保守基序（如GC含量）而非系统发育关系实现泛化，并成功提出并验证了GAGG核心基序与上游A-tract元件的"序列-结构协同"作用机制。


<details>
  <summary>Details</summary>
Motivation: DNA甲基化的精确计算识别对理解表观遗传调控至关重要，然而深度学习虽在二分类任务中表现优异，其"黑箱"特性严重阻碍了生物学机制的可解释性与洞见提取。

Method: 开发高性能深度学习模型MEDNA-DFM，并设计机制启发的信号纯化算法；通过跨物种验证、外部独立数据集测试及in silico突变分析，系统评估模型性能与基序提取可靠性。

Result: 模型有效捕获保守甲基化模式并在多物种间实现稳健区分；泛化能力由内在保守基序（如GC含量）驱动而非系统发育相近性；算法提取的基序可靠性显著优于前人研究；通过果蝇6mA案例研究提出并验证了GAGG核心基序与上游A-tract元件的协同作用机制。

Conclusion: 本研究提供了强大的甲基化预测工具MEDNA-DFM，并证明可解释深度学习框架可同时推动方法学创新与生物学假说生成，为表观遗传学研究提供了新的计算范式与机制洞察。

Abstract: Accurate computational identification of DNA methylation is essential for understanding epigenetic regulation. Although deep learning excels in this binary classification task, its "black-box" nature impedes biological insight. We address this by introducing a high-performance model MEDNA-DFM, alongside mechanism-inspired signal purification algorithms. Our investigation demonstrates that MEDNA-DFM effectively captures conserved methylation patterns, achieving robust distinction across diverse species. Validation on external independent datasets confirms that the model's generalization is driven by conserved intrinsic motifs (e.g., GC content) rather than phylogenetic proximity. Furthermore, applying our developed algorithms extracted motifs with significantly higher reliability than prior studies. Finally, empirical evidence from a Drosophila 6mA case study prompted us to propose a "sequence-structure synergy" hypothesis, suggesting that the GAGG core motif and an upstream A-tract element function cooperatively. We further validated this hypothesis via in silico mutagenesis, confirming that the ablation of either or both elements significantly degrades the model's recognition capabilities. This work provides a powerful tool for methylation prediction and demonstrates how explainable deep learning can drive both methodological innovation and the generation of biological hypotheses.

</details>


### [101] [Fair feature attribution for multi-output prediction: a Shapley-based perspective](https://arxiv.org/abs/2602.22882)
*Umberto Biccari,Alain Ibáñez de Opakua,José María Mato,Óscar Millet,Roberto Morales,Enrique Zuazua*

Main category: cs.LG

TL;DR: 本文将Shapley公理体系扩展至向量值合作博弈，通过刚性定理证明多输出预测器的特征归因若满足效率、对称、虚拟玩家和可加性公理则必然逐输出分解，从而理论验证了实践中SHAP对各输出独立计算的必要性，并揭示了Shapley可解释性框架的结构约束。


<details>
  <summary>Details</summary>
Motivation: 尽管实践中多输出模型的SHAP解释通常独立计算于各输出坐标，但此做法的理论依据未明。本文旨在公理化地研究多输出Shapley归因的结构性约束，界定公平一致性解释的适用范围。

Method: 将经典Shapley公理（效率性、对称性、虚拟玩家性、可加性）推广至向量值合作博弈，并证明刚性定理。

Result: 证明任何满足上述四项公理的归因规则必分解为各输出坐标的独立归因，即必须采用逐分量计算。因此，任何联合输出归因方法都至少需松弛一条Shapley公理。

Conclusion: 该发现识别了Shapley可解释性中一个未形式化的结构约束，厘清多输出学习中公平一致性解释的精确范围。生物医学基准的数值实验证实，多输出模型可在保持SHAP解释与逐分量结构一致的同时，实现训练和部署的计算效率提升。

Abstract: In this article, we provide an axiomatic characterization of feature attribution for multi-output predictors within the Shapley framework. While SHAP explanations are routinely computed independently for each output coordinate, the theoretical necessity of this practice has remained unclear. By extending the classical Shapley axioms to vector-valued cooperative games, we establish a rigidity theorem showing that any attribution rule satisfying efficiency, symmetry, dummy player, and additivity must necessarily decompose component-wise across outputs. Consequently, any joint-output attribution rule must relax at least one of the classical Shapley axioms. This result identifies a previously unformalized structural constraint in Shapley-based interpretability, clarifying the precise scope of fairness-consistent explanations in multi-output learning. Numerical experiments on a biomedical benchmark illustrate that multi-output models can yield computational savings in training and deployment, while producing SHAP explanations that remain fully consistent with the component-wise structure imposed by the Shapley axioms.

</details>


### [102] [A Data-Driven Approach to Support Clinical Renal Replacement Therapy](https://arxiv.org/abs/2602.22902)
*Alice Balboni,Luis Escobar,Andrea Manno,Fabrizio Rossi,Maria Cristina Ruffa,Gianluca Villa,Giordano D'Aloisio,Antonio Consolo*

Main category: cs.LG

TL;DR: 本研究针对CRRT治疗中膜污染预测难题，开发基于表格化数据的可解释机器学习模型。通过ADASYN处理类别不平衡，随机森林/XGBoost/LightGBM在10%重采样率下达到77.6%敏感性与96.3%特异性，显著优于LSTM，并借助Shapley反事实分析识别关键干预变量。


<details>
  <summary>Details</summary>
Motivation: CRRT治疗中膜污染严重影响患者预后，传统预测方法缺乏准确性与及时性。临床亟需可解释的预测模型实现早期预警，并指导治疗调整，同时满足医疗AI的可解释性要求。

Method: 利用ICU时间序列数据，临床筛选16个特征，采用表格化表征避免显式时间建模；应用ADASYN过采样技术解决污染/非污染样本不平衡问题；对比随机森林、XGBoost、LightGBM性能；通过特征选择与Shapley值反事实分析提升模型可解释性。

Result: 最佳模型在10%重平衡率下实现77.6%灵敏度与96.3%特异度，预测性能在不同时间跨度下保持稳健；特征选择精简至5个核心变量；反事实分析精确识别可逆转污染预测的最小参数调整方案，模型性能显著优于LSTM时序模型。

Conclusion: 研究证实可解释机器学习在CRRT膜污染预测中的可行性，表格化方法足以捕获关键预测信息且优于复杂时序模型；预测与反事实分析的结合为临床提供实用决策支持工具，可指导治疗参数优化以降低污染风险。

Abstract: This study investigates a data-driven machine learning approach to predict membrane fouling in critically ill patients undergoing Continuous Renal Replacement Therapy (CRRT). Using time-series data from an ICU, 16 clinically selected features were identified to train predictive models. To ensure interpretability and enable reliable counterfactual analysis, the researchers adopted a tabular data approach rather than modeling temporal dependencies directly. Given the imbalance between fouling and non-fouling cases, the ADASYN oversampling technique was applied to improve minority class representation. Random Forest, XGBoost, and LightGBM models were tested, achieving balanced performance with 77.6% sensitivity and 96.3% specificity at a 10% rebalancing rate. Results remained robust across different forecasting horizons. Notably, the tabular approach outperformed LSTM recurrent neural networks, suggesting that explicit temporal modeling was not necessary for strong predictive performance. Feature selection further reduced the model to five key variables, improving simplicity and interpretability with minimal loss of accuracy. A Shapley value-based counterfactual analysis was applied to the best-performing model, successfully identifying minimal input changes capable of reversing fouling predictions. Overall, the findings support the viability of interpretable machine learning models for predicting membrane fouling during CRRT. The integration of prediction and counterfactual analysis offers practical clinical value, potentially guiding therapeutic adjustments to reduce fouling risk and improve patient management.

</details>


### [103] [Scaling Laws of Global Weather Models](https://arxiv.org/abs/2602.22962)
*Yuejiang Yu,Langwen Huang,Alexandru Calotoiu,Torsten Hoefler*

Main category: cs.LG

TL;DR: 本文研究数据驱动天气预报模型的实证缩放定律，发现Aurora模型的数据扩展性最强（训练数据增加10倍可使验证损失降低至3.2倍），GraphCast参数效率最高但硬件利用率有限。在计算预算固定时，延长训练比扩大模型尺寸更有效。与语言模型不同，天气预报模型更偏好增加宽度而非深度，因此未来模型应采用更宽的架构和更大的训练数据集来提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了优化数据驱动天气预报模型的训练效率与性能，需要理解模型性能与模型尺寸、数据集规模和计算预算之间的缩放关系，从而为未来模型设计提供科学依据。

Method: 通过实证研究分析模型性能（验证损失）与三个关键因素之间的关系：模型参数量（N）、数据集大小（D）和计算预算（C），并在多种数据驱动天气预报模型上进行对比实验。

Result: 1) Aurora表现出最强的数据缩放行为：训练数据增加10倍，验证损失最多可降低3.2倍；2) GraphCast参数效率最高但硬件利用率受限；3) 在固定计算预算下，将资源分配给更长的训练时长比扩大模型尺寸带来更大性能提升；4) 天气预报模型偏好增加宽度而非深度，这与语言模型的缩放行为存在根本差异。

Conclusion: 未来天气预报模型的设计应优先采用更宽的架构并扩大有效训练数据集规模，而非单纯增加模型深度或尺寸，以最大化预测性能。这些发现为该领域提供了明确的优化方向。

Abstract: Data-driven models are revolutionizing weather forecasting. To optimize training efficiency and model performance, this paper analyzes empirical scaling laws within this domain. We investigate the relationship between model performance (validation loss) and three key factors: model size ($N$), dataset size ($D$), and compute budget ($C$). Across a range of models, we find that Aurora exhibits the strongest data-scaling behavior: increasing the training dataset by 10x reduces validation loss by up to 3.2x. GraphCast demonstrates the highest parameter efficiency, yet suffers from limited hardware utilization. Our compute-optimal analysis indicates that, under fixed compute budgets, allocating resources to longer training durations yields greater performance gains than increasing model size. Furthermore, we analyze model shape and uncover scaling behaviors that differ fundamentally from those observed in language models: weather forecasting models consistently favor increased width over depth. These findings suggest that future weather models should prioritize wider architectures and larger effective training datasets to maximize predictive performance.

</details>


### [104] [Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization](https://arxiv.org/abs/2602.23008)
*Zeyuan Liu,Jeonghye Kim,Xufang Luo,Dongsheng Li,Yuqing Yang*

Main category: cs.LG

TL;DR: 该论文针对强化学习训练的大语言模型智能体在探索新状态方面的瓶颈问题，提出了EMPO²框架，该框架利用记忆机制增强探索，并混合使用在线和离线策略更新，使智能体在有无记忆的情况下都能表现出色。在ScienceWorld和WebShop基准测试中，EMPO²相对于GRPO分别取得了128.6%和11.3%的性能提升，同时在分布外测试中展现出更强的适应性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练的大语言模型智能体在探索方面存在关键瓶颈，现有方法虽然能利用预训练知识，但在需要发现新状态的环境中表现不佳。该研究旨在解决智能体在未知环境中有效探索和发现新状态的核心挑战。

Method: 提出探索性记忆增强在线离线策略优化（EMPO²）框架，这是一种混合强化学习方法。该框架通过记忆机制增强探索能力，同时结合在线和离线策略更新，使大语言模型智能体在具备记忆时表现优异，在无记忆时也能保持鲁棒性。

Result: 在ScienceWorld和WebShop基准测试上，EMPO²相较于GRPO分别实现了128.6%和11.3%的显著性能提升。在分布外测试中，EMPO²表现出卓越的适应能力，只需少量带记忆的试验且无需参数更新即可适应新任务。

Conclusion: EMPO²框架被证明是一种有前景的方法，能够构建更具探索性和泛化能力的大语言模型智能体，为强化学习智能体在未知环境中的探索问题提供了有效解决方案。

Abstract: Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.

</details>


### [105] [Learning Disease-Sensitive Latent Interaction Graphs From Noisy Cardiac Flow Measurements](https://arxiv.org/abs/2602.23035)
*Viraj Patel,Marko Grujic,Philipp Aigner,Theodor Abart,Marcus Granegger,Deblina Bhattacharjee,Katharine Fraser*

Main category: cs.LG

TL;DR: 本研究提出一种物理信息引导的潜在关系图框架，将心脏涡流建模为图中交互节点，融合神经关系推理与物理启发的交互能量及生死动力学。该方法成功应用于主动脉缩窄流体模拟和左心室辅助装置超声数据，揭示涡流相互作用强度与疾病严重度的单调相关性，证明潜在图熵可作为心脏疾病及干预措施的稳健可解释标志物。


<details>
  <summary>Details</summary>
Motivation: 当前心脏血流成像与计算方法无法有效捕获流动特征间的底层关系结构，而血流模式蕴含丰富的疾病严重程度与临床干预信息，亟需一种能够量化涡流间相互作用并关联病理状态的框架。

Method: 提出物理信息潜在关系图模型：将心脏涡流视为图节点，设计神经关系推理架构，引入物理启发的节点间交互能量函数和涡流生死动力学机制，构建对疾病严重度与干预水平敏感的潜在图表示。

Result: 在主动脉缩窄模拟中，随主动脉半径减小，涡流相互作用显著增强且更频繁，图熵值与缩窄严重度呈强单调相关（R²=0.78，Spearman |ρ|=0.96）。在左心室辅助装置超声数据中，该方法捕获到涡流结构的减弱趋势，验证了跨模态泛化能力。

Conclusion: 学习到的潜在相互作用图及其熵值作为稳健且可解释的生物标志物，可有效表征心脏疾病状态与干预效果，为临床评估提供新范式。

Abstract: Cardiac blood flow patterns contain rich information about disease severity and clinical interventions, yet current imaging and computational methods fail to capture underlying relational structures of coherent flow features. We propose a physics-informed, latent relational framework to model cardiac vortices as interacting nodes in a graph. Our model combines a neural relational inference architecture with physics-inspired interaction energy and birth-death dynamics, yielding a latent graph sensitive to disease severity and intervention level. We first apply this to computational fluid dynamics simulations of aortic coarctation. Learned latent graphs reveal that as the aortic radius narrows, vortex interactions become stronger and more frequent. This leads to a higher graph entropy, correlating monotonically with coarctation severity ($R^2=0.78$, Spearman $|ρ|=0.96$). We then extend this method to ultrasound datasets of left ventricles under varying levels of left ventricular assist device support. Again the latent graph representation captures the weakening of coherent vortical structures, thereby demonstrating cross-modal generalisation. Results show latent interaction graphs and entropy serve as robust and interpretable markers of cardiac disease and intervention.

</details>


### [106] [RhythmBERT: A Self-Supervised Language Model Based on Latent Representations of ECG Waveforms for Heart Disease Detection](https://arxiv.org/abs/2602.23060)
*Xin Wang,Burcu Ozek,Aruna Mohan,Amirhossein Ravari,Or Zilbershot,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 提出RhythmBERT，一种将心电图视为语言范式的生成式模型。通过自编码器将P波、QRS波群、T波编码为符号化标记，结合离散标记捕获节律语义和连续嵌入保留形态细节，在80万条无标签心电图上进行掩码预测预训练，单导联性能媲美或超越12导联基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法将心电图视为通用时间序列，忽略其生理语义和节律级结构；对比学习的数据增强扭曲波形形态，而生成式方法采用固定窗口分割导致心拍周期错位。这些局限性阻碍了模型对心电图结构化和生理意义的理解。

Method: 构建心电图语言范式：利用自编码器将P、QRS、T段编码为离散符号标记以捕获节律语义，同时保留连续嵌入以维持细粒度形态特征，实现波形结构与节律的统一表示。采用掩码预测目标，在约80万条无标签心电图记录上进行预训练，实现标签高效学习。

Result: 仅使用单导联输入，RhythmBERT在房颤、细微ST-T改变、心肌梗死等多种心血管疾病诊断任务上达到与强12导联基线模型相当或更优的性能，展现出良好的泛化能力。

Conclusion: 将心电图视为结构化语言的处理方式，为推进心脏分析提供了一条可扩展且与生理学原理对齐的新路径，有望显著提升心电图自动分析的效率和准确性。

Abstract: Electrocardiogram (ECG) analysis is crucial for diagnosing heart disease, but most self-supervised learning methods treat ECG as a generic time series, overlooking physiologic semantics and rhythm-level structure. Existing contrastive methods utilize augmentations that distort morphology, whereas generative approaches employ fixed-window segmentation, which misaligns cardiac cycles. To address these limitations, we propose RhythmBERT, a generative ECG language model that considers ECG as a language paradigm by encoding P, QRS, and T segments into symbolic tokens via autoencoder-based latent representations. These discrete tokens capture rhythm semantics, while complementary continuous embeddings retain fine-grained morphology, enabling a unified view of waveform structure and rhythm. RhythmBERT is pretrained on approximately 800,000 unlabeled ECG recordings with a masked prediction objective, allowing it to learn contextual representations in a label-efficient manner. Evaluations show that despite using only a single lead, RhythmBERT achieves comparable or superior performance to strong 12-lead baselines. This generalization extends from prevalent conditions such as atrial fibrillation to clinically challenging cases such as subtle ST-T abnormalities and myocardial infarction. Our results suggest that considering ECG as structured language offers a scalable and physiologically aligned pathway for advancing cardiac analysis.

</details>


### [107] [PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training](https://arxiv.org/abs/2602.23111)
*Yanyi Li,Yimu Zhang,Cong Fang*

Main category: cs.LG

TL;DR: 针对大批量LLM训练中激活值的内存瓶颈问题，本文提出PRAC方法，通过SVD提取主成分子空间保留主要信息，并在正交补空间随机采样近似尾部信息，配合精确缩放因子实现无偏且方差最小的梯度估计，在预训练和微调任务中实现高达36%的总内存缩减且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 大批量训练大型语言模型时，激活值成为主要内存瓶颈。现有压缩方法未能有效利用激活值的光谱结构特性，导致收敛缓慢或压缩率受限，亟需一种既能保持收敛速度又能实现高效压缩的新方法。

Method: 提出PRAC（主随机子空间激活压缩）方法：1）对激活值进行SVD分解，提取主成分子空间保留主导信息；2）在正交补空间中随机采样构建随机子空间以近似尾部信息；3）引入精确缩放因子，理论上证明该方法在特定条件下能产生无偏且方差最小的梯度估计器。

Result: 在预训练和微调任务上的广泛实验表明，PRAC可实现高达36%的总内存缩减，同时保持极小的性能退化，计算开销极低。方法通过无偏梯度估计确保了训练的收敛性。

Conclusion: PRAC通过巧妙地结合确定性子空间与随机子空间分解，并利用光谱结构特性，实现了激活值的高效压缩。该方法在保持LLM训练收敛速度和模型性能的前提下，显著降低了内存需求，为大批量训练提供了实用解决方案。

Abstract: Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal-Random Subspace for LLM Activation Compression (PRAC), which novelly decomposes activations into two components: a principal subspace captured via SVD to retain dominant information, and a random subspace sampled from the orthogonal complement to approximate the tail. By introducing a precise scaling factor, we prove that PRAC yields an unbiased gradient estimator with minimum variance under certain conditions. Extensive experiments on pre-training and fine-tuning tasks demonstrate that PRAC achieves up to 36% total memory reduction with negligible performance degradation and minimal computational cost.

</details>


### [108] [Regularized Online RLHF with Generalized Bilinear Preferences](https://arxiv.org/abs/2602.23116)
*Junghyun Lee,Minju Hong,Kwang-Sung Jun,Chulhee Yun,Se-Young Yun*

Main category: cs.LG

TL;DR: 本文研究具有通用偏好的上下文在线强化学习人类反馈(RLHF)问题，旨在识别纳什均衡。采用广义双线性偏好模型(GBPM)捕获非传递性偏好，突破性地将正则化器推广至任意强凸函数。基于强凸性和斜对称性导出的对偶间隙界，提出两种算法分别实现e^O(η)-free的多项式对数遗憾和首个高维统计有效保证。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF研究局限于反向KL正则化，无法处理实际中普遍存在的非传递性偏好。本文旨在：1) 将正则化器类别推广至任意强凸函数，超越现有工作限制；2) 使用GBPM模型通过低秩斜对称矩阵灵活建模复杂偏好结构；3) 为高维在线RLHF建立首个统计有效的理论保证。

Method: 核心方法包括：1) 构建GBPM模型，用低秩斜对称矩阵参数化偏好；2) 证明在强凸正则化下，贪婪策略的对偶间隙以估计误差平方为上界；3) 在特征多样性假设下，设计Greedy Sampling和Explore-Then-Commit两种简单算法。

Result: 理论结果：1) Greedy Sampling实现O~(ηd^4 (log T)^2)遗憾，摆脱了对e^O(η)的指数依赖；2) Explore-Then-Commit实现O~(√(ηrT))遗憾，完全消除poly(d)因子，首次为高维在线RLHF提供统计效率保证。

Conclusion: 本工作突破了RLHF中正则化器的限制，建立了通用偏好学习的理论框架，提供了首个高维统计有效的在线算法，显著拓展了该领域的理论边界，为处理复杂现实偏好提供了新工具。

Abstract: We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $η^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is proving that the dual gap of the greedy policy is bounded by the square of the estimation error - a result derived solely from strong convexity and the skew-symmetricity of GBPM.Building on this insight and a feature diversity assumption, we establish two regret bounds via two simple algorithms: (1) Greedy Sampling achieves polylogarithmic, $e^{O(η)}$-free regret $\tilde{O}(ηd^4 (\log T)^2)$. (2) Explore-Then-Commit achieves $\mathrm{poly}(d)$-free regret $\tilde{O}(\sqrt{ηr T})$ by exploiting the low-rank structure; this is the first statistically efficient guarantee for online RLHF in high-dimensions.

</details>


### [109] [Prediction of Diffusion Coefficients in Mixtures with Tensor Completion](https://arxiv.org/abs/2602.23142)
*Zeno Romero,Kerstin Münnemann,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: 提出一种基于Tucker分解和贝叶斯框架的混合张量补全方法(TCM)，结合半经验模型和主动学习策略，实现二元混合物无限稀释扩散系数在268-378K的高精度温度依赖性预测。


<details>
  <summary>Details</summary>
Motivation: 混合物扩散系数预测在多领域应用中至关重要，但实验数据匮乏。现有矩阵补全方法(MCM)仅能单温度预测，且精度严重依赖各温度下的高质量实验数据，难以实现温度自适应预测。为此，亟需开发能够跨温度域预测的新方法。

Method: 提出基于Tucker分解的混合张量补全方法(TCM)，在贝叶斯框架下将SEGWE半经验模型预测作为先验知识，联合训练298K、313K和333K的无限稀释扩散系数实验数据，实现268-378K范围内的线性外推预测。进一步采用主动学习策略指导PFG NMR实验，针对性地测量19个溶质-溶剂体系在三个温度下的扩散系数以扩充数据库。

Result: TCM在所有研究温度下均显著优于传统模型。通过主动学习新增19个体系的实验数据后，模型预测精度得到进一步提升，验证了混合方法的优越性和数据扩充的有效性。

Conclusion: 本研究证实，将数据高效的张量补全机器学习方法与自适应主动学习实验相结合，可显著提升输运性质预测精度，为跨温度域扩散系数预测提供了有效新范式。

Abstract: Predicting diffusion coefficients in mixtures is crucial for many applications, as experimental data remain scarce, and machine learning (ML) offers promising alternatives to established semi-empirical models. Among ML models, matrix completion methods (MCMs) have proven effective in predicting thermophysical properties, including diffusion coefficients in binary mixtures. However, MCMs are restricted to single-temperature predictions, and their accuracy depends strongly on the availability of high-quality experimental data for each temperature of interest. In this work, we address this challenge by presenting a hybrid tensor completion method (TCM) for predicting temperature-dependent diffusion coefficients at infinite dilution in binary mixtures. The TCM employs a Tucker decomposition and is jointly trained on experimental data for diffusion coefficients at infinite dilution in binary systems at 298 K, 313 K, and 333 K. Predictions from the semi-empirical SEGWE model serve as prior knowledge within a Bayesian training framework. The TCM then extrapolates linearly to any temperature between 268 K and 378 K, achieving markedly improved prediction accuracy compared to established models across all studied temperatures. To further enhance predictive performance, the experimental database was expanded using active learning (AL) strategies for targeted acquisition of new diffusion data by pulsed-field gradient (PFG) NMR measurements. Diffusion coefficients at infinite dilution in 19 solute + solvent systems were measured at 298 K, 313 K, and 333 K. Incorporating these results yields a substantial improvement in the TCM's predictive accuracy. These findings highlight the potential of combining data-efficient ML methods with adaptive experimentation to advance predictive modeling of transport properties.

</details>


### [110] [Partial recovery of meter-scale surface weather](https://arxiv.org/abs/2602.23146)
*Jonathan Giezendanner,Qidong Yang,Eric Schmitt,Anirban Chandra,Daniel Salles Civitarese,Johannes Jakubik,Jeremy Vila,Detlef Hohl,Campbell Watson,Sherrie Wang*

Main category: cs.LG

TL;DR: 该研究证明近地表米尺度天气变率并非完全混沌，而是包含可从地表特征和大气强迫中统计预测的物理一致成分。通过融合稀疏站点与高分辨率地球观测数据，实现了美国本土10米分辨率风、温、湿场的连续推断，较ERA5风速误差降低29%、温湿误差降低6%，并显著提升空间方差解释能力，揭示了城市热岛等地表驱动的天气结构。


<details>
  <summary>Details</summary>
Motivation: 当前天气分析和预报缺失因地表覆盖和地形导致的数十至数百米尺度近地表大气剧烈变化。这种米尺度变率是源于不可约混沌还是包含可预测成分，尚不明确。

Method: 采用统计方法，以稀疏地面站点观测和高分辨率地球观测数据为条件，约束粗分辨率大气状态，推断美国本土10米分辨率近地表风、温、湿空间连续场。

Result: 推断场较ERA5风速误差降低29%，温湿误差降低6%，空间方差解释能力显著提升。结果呈现城市热岛、蒸散发驱动的湿度对比、不同地表覆盖风速差异等物理可解释结构。

Conclusion: 该工作通过计算可行的大陆尺度米分辨率推断拓展了天气建模前沿，展示了融合粗动力学模型与细尺度静态特征揭示未解析地球系统成分的可行性。

Abstract: Near-surface atmospheric conditions can differ sharply over tens to hundreds of meters due to land cover and topography, yet this variability is absent from current weather analyses and forecasts. It is unclear whether such meter-scale variability reflects irreducibly chaotic dynamics or contains a component predictable from surface characteristics and large-scale atmospheric forcing. Here we show that a substantial, physically coherent component of meter-scale near-surface weather is statistically recoverable from existing observations. By conditioning coarse atmospheric state on sparse surface station measurements and high-resolution Earth observation data, we infer spatially continuous fields of near-surface wind, temperature, and humidity at 10 m resolution across the contiguous United States. Relative to ERA5, the inferred fields reduce wind error by 29% and temperature and dewpoint error by 6%, while explaining substantially more spatial variance at fixed time steps. They also exhibit physically interpretable structure, including urban heat islands, evapotranspiration-driven humidity contrasts, and wind speed differences across land cover types. Our findings expand the frontier of weather modeling by demonstrating a computationally feasible approach to continental-scale meter-resolution inference. More broadly, they illustrate how conditioning coarse dynamical models on static fine-scale features can reveal previously unresolved components of the Earth system.

</details>


### [111] [Benchmarking Temporal Web3 Intelligence: Lessons from the FinSurvival 2025 Challenge](https://arxiv.org/abs/2602.23159)
*Oshani Seneviratne,Fernando Spadea,Adrien Pavao,Aaron Micah Green,Kristin P. Bennett*

Main category: cs.LG

TL;DR: 本文提出FinSurvival Challenge 2025基准测试作为时序Web3智能的案例研究，基于Aave v3协议的2180万条交易记录构建了16个生存预测任务，通过对比实验证明了领域感知时序特征构建的优越性，并为下一代时序基准设计提供了经验。


<details>
  <summary>Details</summary>
Motivation: 时序Web分析依赖大规模纵向数据以理解用户、内容和系统的演化，特别是去中心化的Temporal Web3平台产生的不可变时间戳事件流。然而，现有研究缺乏能够捕捉真实世界时序动态（如截尾和非平稳性）的共享可复现基准，这限制了方法学进步以及Web3技术与更广泛Web领域之间的技术迁移。

Method: 研究团队构建FinSurvival Challenge 2025基准，采用Aave v3协议2180万条交易记录，设计16个生存预测任务建模用户行为转换，详细记录基准设计过程与获胜方案，重点分析领域感知时序特征构建方法。

Result: 基准测试结果表明，领域感知的时序特征构造方法显著优于通用建模方法，成功创建了能真实反映时序动态（截尾与非平稳性）的可复现基准框架，验证了Web3数据在时序研究中的高保真特性。

Conclusion: 该研究证明Web3系统为研究流失、风险和演化等时序挑战提供了高保真沙盒环境，这些挑战对更广泛的Web领域具有基础重要性。研究提炼的设计经验将推动下一代时序基准的发展，促进Web3与Web领域之间的方法论迁移与技术融合。

Abstract: Temporal Web analytics increasingly relies on large-scale, longitudinal data to understand how users, content, and systems evolve over time. A rapidly growing frontier is the \emph{Temporal Web3}: decentralized platforms whose behavior is recorded as immutable, time-stamped event streams. Despite the richness of this data, the field lacks shared, reproducible benchmarks that capture real-world temporal dynamics, specifically censoring and non-stationarity, across extended horizons. This absence slows methodological progress and limits the transfer of techniques between Web3 and broader Web domains. In this paper, we present the \textit{FinSurvival Challenge 2025} as a case study in benchmarking \emph{temporal Web3 intelligence}. Using 21.8 million transaction records from the Aave v3 protocol, the challenge operationalized 16 survival prediction tasks to model user behavior transitions.We detail the benchmark design and the winning solutions, highlighting how domain-aware temporal feature construction significantly outperformed generic modeling approaches. Furthermore, we distill lessons for next-generation temporal benchmarks, arguing that Web3 systems provide a high-fidelity sandbox for studying temporal challenges, such as churn, risk, and evolution that are fundamental to the wider Web.

</details>


### [112] [Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models](https://arxiv.org/abs/2602.23179)
*Gal Kesten-Pomeranz,Yaniv Nikankin,Anja Reusch,Tomer Tsaban,Ora Schueler-Furman,Yonatan Belinkov*

Main category: cs.LG

TL;DR: 本研究揭示了蛋白质语言模型(PLMs)检测重复序列的内在机制：通过位置注意力头和生物特化神经元构建特征表示，再由归纳头跨重复片段对齐标记来完成预测。该机制将精确重复作为近似重复的特例，展现了PLMs如何结合语言模式匹配与生物学知识来理解蛋白质序列。


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列中的重复片段对结构和功能至关重要，传统算法已研究数十年。近期发现PLMs能通过掩码预测识别重复，但其内部机制不明。阐明该机制不仅可解释PLMs如何编码生物学知识，还能为研究更复杂的进化过程奠定基础。

Method: 通过分析PLMs在掩码标记预测任务中的行为，系统探究模型检测精确重复和近似重复的内部机制，重点考察注意力头和神经元激活模式，揭示特征表示构建与跨片段对齐的具体过程。

Result: 发现近似重复的检测机制在功能上包含精确重复的检测；识别出两阶段机制：首先通过通用位置注意力头和编码氨基酸相似性的生物特化神经元构建特征表示，然后由归纳头跨重复片段关注对齐的标记以促进正确答案生成。

Conclusion: PLMs通过结合基于语言的通用模式匹配能力与编码氨基酸相似性的生物学专业知识来识别蛋白质重复序列，这为未来研究PLMs中更复杂的进化过程建立了理论基础。

Abstract: Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that the mechanism for approximate repeats functionally subsumes that of exact repeats. We then characterize this mechanism, revealing two main stages: PLMs first build feature representations using both general positional attention heads and biologically specialized components, such as neurons that encode amino-acid similarity. Then, induction heads attend to aligned tokens across repeated segments, promoting the correct answer. Our results reveal how PLMs solve this biological task by combining language-based pattern matching with specialized biological knowledge, thereby establishing a basis for studying more complex evolutionary processes in PLMs.

</details>


### [113] [Takeuchi's Information Criteria as Generalization Measures for DNNs Close to NTK Regime](https://arxiv.org/abs/2602.23219)
*Hiroki Naganuma,Taiji Suzuki,Rio Yokota,Masahiro Nomura,Kohta Ishikawa,Ikuro Sato*

Main category: cs.LG

TL;DR: 本研究探討竹內情報基準(TIC)評估深度神經網路泛化能力。通過理論分析與5,000+模型實驗，發現TIC僅在神經切線核(NTK) regime附近與泛化差距相關，範圍外相關性消失，但在超參數優化中篩選能力優於現有方法。


<details>
  <summary>Details</summary>
Motivation: 泛化度量對表徵泛化差距至關重要，但深度神經網路等統計奇異模型的複雜性使其難以建立可靠度量。本研究旨在探究經典TIC在何種條件下能有效解釋DNN泛化差距。

Method: 理論分析TIC適用性，並進行大規模實驗：訓練12種架構(含VGG-16)超過5,000個DNN模型於四個數據集，估算TIC值驗證其與泛化差距關係；採用多種低計算成本近似方法並評估準確性權衡。

Result: 結果顯示在NTK regime附近TIC值與泛化差距相關性良好，但理論與實證均證實在該範圍外相關性消失。TIC在超參數優化中的trial pruning能力優於現有方法。

Conclusion: 研究證實TIC可作為DNN泛化能力評估工具有效，但適用性限於NTK regime附近條件。此發現為超參數優化提供了新的實用方向。

Abstract: Generalization measures have been studied extensively in the machine learning community to better characterize generalization gaps. However, establishing a reliable generalization measure for statistically singular models such as deep neural networks (DNNs) is difficult due to their complex nature. This study focuses on Takeuchi's information criterion (TIC) to investigate the conditions under which this classical measure can effectively explain the generalization gaps of DNNs. Importantly, the developed theory indicates the applicability of TIC near the neural tangent kernel (NTK) regime. In a series of experiments, we trained more than 5,000 DNN models with 12 architectures, including large models (e.g., VGG-16), on four datasets, and estimated the corresponding TIC values to examine the relationship between the generalization gap and the TIC estimates. We applied several TIC approximation methods with feasible computational costs and assessed the accuracy trade-off. Our experimental results indicate that the estimated TIC values correlate well with the generalization gap under conditions close to the NTK regime. However, we show both theoretically and empirically that outside the NTK regime such correlation disappears. Finally, we demonstrate that TIC provides better trial pruning ability than existing methods for hyperparameter optimization.

</details>


### [114] [Physics Informed Viscous Value Representations](https://arxiv.org/abs/2602.23280)
*Hrishikesh Viswanath,Juanwu Lu,S. Talha Bukhari,Damon Conover,Ziran Wang,Aniket Bera*

Main category: cs.LG

TL;DR: 针对离线目标条件强化学习中值估计因数据覆盖有限而面临的挑战，本文提出一种基于哈密顿-雅可比-贝尔曼（HJB）方程粘性解的物理学信息正则化方法。通过费曼-卡茨定理将偏微分方程转化为期望形式，实现可处理的蒙特卡洛估计，有效提升了复杂导航与操控任务的几何一致性。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习从静态数据集中学习策略，但值估计精度受限于数据集的状态-动作空间覆盖不足。现有物理学信息方法采用一阶偏微分方程（如Eikonal方程）正则化，但在高维复杂环境中易出现病态问题，导致估计不稳定。

Method: 提出基于HJB方程粘性解的物理学信息正则化框架：1）引入最优控制理论的归纳偏置，显式约束值迭代更新过程；2）利用费曼-卡茨定理将偏微分方程重构为期望形式，实现蒙特卡洛近似求解，规避高阶梯度的数值不稳定性。

Result: 在导航和高维复杂操控任务上的实验表明，该方法显著提升了值函数的几何一致性，验证了其广泛适用性。

Conclusion: 该方法通过融合最优控制理论与蒙特卡洛估计，为离线GCRL提供了更稳健的值估计范式，有效解决了数据覆盖不足导致的泛化难题。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.

</details>


### [115] [Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications](https://arxiv.org/abs/2602.23303)
*Ilya Balabin,Thomas M. Kaiser*

Main category: cs.LG

TL;DR: 这是一个三篇论文系列，旨在解决化学生物学中机器学习模型的因果黑箱问题。第1篇建立了因果结构的形式化框架，提出"聚焦"概念（机器学习算法从大数据集中缩小到隐藏机制的能力），并在Akt抑制剂家族上进行了初步验证。整个系列旨在建立不依赖还原论的"推断力学"新框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习和人工智能在自然科学领域的应用日益普及，但模型常被视为缺乏因果解释的"黑箱"。尽管已有研究尝试将因果关系引入机器学习模型，但缺乏统一的理论处理。本系列论文的动机是将化学理论、生物理论、概率论与因果关系相结合，为化学生物学建立新的数学框架，以纠正当前机器学习在因果推断方面的缺陷，实现不依赖还原论的机制建模。

Method: 本论文作为系列的第1部分，提供了化学生物学现象基础因果结构的形式化框架。通过引入"聚焦"这一新概念——定义为机器学习算法从大数据集中缩小到隐藏底层机制的能力——将因果结构扩展到机器学习领域。研究在Akt抑制剂家族上进行了初步原理验证。系列中的第2篇将正式探讨化学相似性，第3篇将提供大量实验证据。

Result: 建立了化学生物学因果结构的形式化框架；定义了"聚焦"这一机器学习新概念；在Akt抑制剂家族上完成了初步的原理验证，证明了该框架的潜在有效性。

Conclusion: 本系列论文旨在为化学生物学建立一种名为"推断力学"的新型数学框架，用于在不依赖还原论工具的情况下对自然机制进行建模。通过整合化学理论、生物理论、概率论和因果关系，该系列将为机器学习在自然科学中的因果推断提供统一理论处理，后续研究将进一步完善该框架并提供实验证据。

Abstract: Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.

</details>


### [116] [A Proper Scoring Rule for Virtual Staining](https://arxiv.org/abs/2602.23305)
*Samuel Tonks,Steve Hood,Ryan Musso,Ceridwen Hopely,Steve Titus,Minh Doan,Iain Styles,Alexander Krull*

Main category: cs.LG

TL;DR: 本文提出基于信息增益（IG）的单元级评估框架，用于直接评估高通量筛选中虚拟染色模型的预测后验分布，揭示传统指标无法发现的模型性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟染色模型评估仅关注边缘分布准确性，缺乏对预测后验分布的直接评估方法，且真实后验不可得，无法有效衡量模型预测质量。

Method: 引入信息增益（IG）作为严格适当的评分规则，构建单元级评估框架；在广泛HTS数据集上对比评估扩散模型与GAN模型。

Result: IG评估显示不同模型间存在显著性能差异，这些差异被其他指标所忽略。

Conclusion: IG框架为虚拟染色模型的后验评估提供了理论基础，能够更细致地表征模型性能，优于传统边缘分布评估方法。

Abstract: Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. IG is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. We evaluate diffusion- and GAN-based models on an extensive HTS dataset using IG and other metrics and show that IG can reveal substantial performance differences other metrics cannot.

</details>


### [117] [Differentiable Zero-One Loss via Hypersimplex Projections](https://arxiv.org/abs/2602.23336)
*Camilo Gomez,Pengyang Wang,Liansheng Tang*

Main category: cs.LG

TL;DR: 本文提出了一种可微分的零一损失近似方法，通过约束优化框架构建对n,k维超单纯形的光滑保序投影，引入Soft-Binary-Argmax算子，有效解决了大规模批次训练中的泛化性能差距问题。


<details>
  <summary>Details</summary>
Motivation: 零一损失被视为分类性能的黄金标准，但其非可微性使其无法兼容基于梯度的优化方法。同时，大规模批次训练普遍存在泛化性能下降的问题，需要在端到端可微分模型中集成更强的结构化归纳偏置。

Method: 通过约束优化框架构造一个光滑且保序的投影算子，称为Soft-Binary-Argmax，实现到n,k维超单纯形的可微映射；推导该算子的数学性质并高效计算其雅可比矩阵，将其无缝集成到二分类与多分类学习系统中。

Result: 实证表明，该方法在大型批次训练下显著提升了模型泛化能力，通过对输出logits施加几何一致性约束，有效缩小了传统大规模批次训练中存在的性能鸿沟。

Conclusion: 本研究成功将结构化优化组件融入端到端可微分架构，为不可微的零一损失提供了有效的优化途径，特别是在大规模批次训练场景下展现出明显的性能优势。

Abstract: Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.

</details>


### [118] [Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms](https://arxiv.org/abs/2602.23341)
*Alkis Kalavasis,Anay Mehrotra,Manolis Zampetakis,Felix Zhou,Ziyu Zhu*

Main category: cs.LG

TL;DR: 本研究解决从粗数据（仅观测到样本所属集合而非精确值）中估计高维高斯均值的两个核心问题：(1)在凸划分下何时可识别？(2)可识别时能否计算高效地估计？此前工作虽证明凸划分可识别时样本高效估计可行，非凸则NP难，但这两个问题尚未解决。


<details>
  <summary>Details</summary>
Motivation: 粗数据在实际测量中普遍存在，如传感器精度限制、数值舍入、经济系统观测滞后等。从这类部分信息中恢复分布参数既是理论挑战也有重要应用价值，但面临可识别性（唯一恢复）和计算复杂性两大根本障碍。

Method: 通过几何分析与凸优化理论，刻画凸划分下可识别性的充要条件；基于最大似然估计框架，设计多项式时间算法，利用凸划分结构特性实现计算高效的均值估计。

Result: 给出了凸划分下高斯均值可识别的完整理论判据；在可识别条件下，提出了首个计算高效（多项式时间）的估计算法，并证明了其样本复杂性和统计一致性。

Conclusion: 完全解决了凸划分下高斯均值估计的可识别性与计算效率问题，建立了可识别性的精确边界，并提供了可实现的算法，完善了粗数据估计的理论体系。

Abstract: Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]

</details>


### [119] [FlashOptim: Optimizers for Memory Efficient Training](https://arxiv.org/abs/2602.23349)
*Jose Javier Gonzalez Ortiz,Abhay Gupta,Chris Renard,Davis Blalock*

Main category: cs.LG

TL;DR: FlashOptim是一个内存优化框架，通过主权重分割优化和8位优化器状态量化技术，将每参数训练内存从16字节降至7字节（或5字节），减少超过50%，同时保持模型质量和API兼容性，在Llama-3.1-8B等基准测试中无性能下降。


<details>
  <summary>Details</summary>
Motivation: 标准混合精度训练中，每个模型参数需要存储参数值、梯度和优化器状态，每个值通常占4字节，导致70亿参数模型训练需要超过100GB加速器内存，对资源有限的研究者不实用。

Method: 提出两项关键技术：1）通过寻找量化误差的紧界来改进主权重分割；2）设计压缩扩展函数显著降低8位优化器状态量化误差，结合16位梯度实现内存优化。

Result: AdamW优化器每参数内存从16字节降至7字节（释放梯度后仅5字节），模型检查点大小减半；在SGD、AdamW、Lion优化器上测试，在Llama-3.1-8B微调等标准视觉和语言基准测试中未观察到质量下降。

Conclusion: FlashOptim成功解决了大模型训练内存瓶颈问题，显著降低内存需求的同时保持训练质量和兼容性，使资源有限的研究者也能训练大模型。

Abstract: Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.
  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.
  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.

</details>


### [120] [SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport](https://arxiv.org/abs/2602.23353)
*Simon Roschmann,Paul Krzakala,Sonia Mazelet,Quentin Bouniot,Zeynep Akata*

Main category: cs.LG

TL;DR: 本文提出SOTAlign，一个两阶段半监督框架，用于在仅有少量配对图像-文本数据和大量未配对数据的情况下对齐预训练的视觉和语言模型。该方法先通过线性教师从配对数据中恢复粗略的共享几何结构，再利用最优传输散度在未配对样本上细化对齐，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型对齐方法依赖于对比损失和数百万配对样本，监督成本高昂。本文探讨在监督大幅减少的情况下是否仍能实现有意义的对齐，提出一种半监督设定，利用少量配对数据和大量未配对数据来降低对齐的视觉-语言模型的成本。

Method: SOTAlign采用两阶段框架：第一阶段使用线性教师从有限的配对数据中恢复粗略的共享几何结构；第二阶段通过基于最优传输的散度在未配对样本上进行细化对齐，这种散度能够传递关系结构但不会过度约束目标空间。

Result: 与现有半监督方法不同，SOTAlign能够有效利用未配对的图像和文本，学习跨数据集和编码器对的鲁棒联合嵌入，并在监督和半监督基线上取得显著优于现有方法的性能。

Conclusion: 研究表明，仅使用少量配对数据和大量未配对数据，SOTAlign就能成功对齐预训练的单模态编码器，挑战了当前需要大量监督数据的假设，为视觉-语言对齐提供了更高效的解决方案。

Abstract: The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.

</details>


### [121] [A Dataset is Worth 1 MB](https://arxiv.org/abs/2602.23358)
*Elad Kimchi Shoshani,Leeyam Gabay,Yedid Hoshen*

Main category: cs.LG

TL;DR: 本文提出PLADA方法，通过仅传输伪标签而非像素数据，利用客户端预存的大型通用未标注参考数据集，配合剪枝机制筛选最相关样本，实现高效数据集服务。该方法在10个数据集上验证可用<1MB负载保持高分类精度。


<details>
  <summary>Details</summary>
Motivation: 数据集服务器需向众多客户端分发相同大型负载，导致巨大通信开销。由于客户端硬件软件框架多样，传输预训练模型往往不可行，而需要传输原始数据供本地训练。现有数据集蒸馏方法难以扩展到高分辨率数据，且很少能达到足够小的文件体积。

Method: 提出Pseudo-Labels as Data (PLADA)方法。假设客户端已预加载大型通用无标签参考数据集（如ImageNet-1K/21K），服务器仅传输新任务的类别标签。为应对参考集与目标集间的分布不匹配，引入剪枝机制过滤参考数据集，仅保留对目标任务最具语义相关性的图像标签。该选择过程同时最大化训练效率并最小化传输负载。

Result: 在10个不同数据集上的实验表明，该方法可用小于1MB的负载传输任务知识，同时保持高分类准确率。

Conclusion: 该方法为高效数据集服务提供了有前景的解决方案。

Abstract: A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [122] [Enriching Taxonomies Using Large Language Models](https://arxiv.org/abs/2602.22213)
*Zeinab Ghamlouch,Mehwish Alam*

Main category: cs.IR

TL;DR: 针对现有分类法覆盖不足与节点过时模糊的问题，本文提出Taxoria——一种利用大语言模型以现有分类法为种子进行候选节点生成与验证的自动化增强流水线，输出带溯源与可视化能力的增强分类法。


<details>
  <summary>Details</summary>
Motivation: 现有分类法因覆盖范围有限、节点信息过时或语义模糊，导致知识检索效率低下，亟需自动化、可扩展的分类法增强方法。

Method: Taxoria采用种子驱动策略，通过提示工程引导大语言模型基于现有分类法生成候选节点，设计验证机制过滤幻觉并确保语义相关性，最终将验证节点整合至原分类法，提供溯源追踪与可视化分析功能。

Result: 成功构建了一个可自动扩展分类法的流水线系统，生成的增强分类法保留了原始结构优势，同时扩展了覆盖范围，并提供节点来源追踪与可视化分析能力。

Conclusion: 该方法验证了利用大语言模型增强分类法的可行性，通过种子引导和验证机制平衡了生成创造力与结果可靠性，为知识图谱和本体工程提供了实用工具。

Abstract: Taxonomies play a vital role in structuring and categorizing information across domains. However, many existing taxonomies suffer from limited coverage and outdated or ambiguous nodes, reducing their effectiveness in knowledge retrieval. To address this, we present Taxoria, a novel taxonomy enrichment pipeline that leverages Large Language Models (LLMs) to enhance a given taxonomy. Unlike approaches that extract internal LLM taxonomies, Taxoria uses an existing taxonomy as a seed and prompts an LLM to propose candidate nodes for enrichment. These candidates are then validated to mitigate hallucinations and ensure semantic relevance before integration. The final output includes an enriched taxonomy with provenance tracking and visualization of the final merged taxonomy for analysis.

</details>


### [123] [Adaptive Prefiltering for High-Dimensional Similarity Search: A Frequency-Aware Approach](https://arxiv.org/abs/2602.22214)
*Teodor-Ioan Calin*

Main category: cs.IR

TL;DR: 提出一种自适应预过滤框架，通过利用查询频率模式和簇一致性指标动态分配计算预算。在ImageNet-1k数据集上使用CLIP嵌入的实验表明，相比静态nprobe策略，该方法在保持同等召回率的同时减少20.4%的距离计算量，并维持亚毫秒级GPU延迟。


<details>
  <summary>Details</summary>
Motivation: 现代检索系统依赖高维相似度搜索，但统一的搜索策略无法利用真实查询分布的异质性，导致计算资源浪费和效率低下。齐夫分布特性下的查询频率差异为动态资源分配提供了优化空间。

Method: 基于齐夫分布将查询空间划分为多个频率层级，结合历史访问模式与局部密度特征制定差异化搜索策略。采用轻量级频率跟踪机制，并为未见查询设计基于簇一致性的回退策略。

Result: 在CLIP嵌入的ImageNet-1k数据集上，频率感知的预算分配策略相比静态nprobe选择实现同等召回率，距离计算量降低20.4%，在GPU加速的FAISS索引上保持亚毫秒级延迟，且引入的系统开销极小。

Conclusion: 该框架通过动态计算预算分配显著提升了高维相似度搜索效率，具备低开销、高适应性和优雅降级能力，为异构查询分布场景提供了有效解决方案。

Abstract: High-dimensional similarity search underpins modern retrieval systems, yet uniform search strategies fail to exploit the heterogeneous nature of real-world query distributions. We present an adaptive prefiltering framework that leverages query frequency patterns and cluster coherence metrics to dynamically allocate computational budgets. Our approach partitions the query space into frequency tiers following Zipfian distributions and assigns differentiated search policies based on historical access patterns and local density characteristics. Experiments on ImageNet-1k using CLIP embeddings demonstrate that frequency-aware budget allocation achieves equivalent recall with 20.4% fewer distance computations compared to static nprobe selection, while maintaining sub-millisecond latency on GPU-accelerated FAISS indices. The framework introduces minimal overhead through lightweight frequency tracking and provides graceful degradation for unseen queries through coherence-based fallback policies.

</details>


### [124] [Retrieval-Augmented Generation Assistant for Anatomical Pathology Laboratories](https://arxiv.org/abs/2602.22216)
*Diogo Pires,Yuriy Perezhohin,Mauro Castelli*

Main category: cs.IR

TL;DR: 本研究开发了一个针对解剖病理实验室的RAG助手，使用递归分块、混合检索和MedEmbed嵌入模型，在RAGAS评估框架下取得了0.70-0.77的得分，单次检索(k=1)效果最佳，有望将静态文档转化为动态知识辅助工具以提升工作流程效率和患者安全。


<details>
  <summary>Details</summary>
Motivation: 解剖病理学中70%的医疗决策依赖实验室诊断，但现有静态文档(纸质手册、PDF)存在过时、碎片化、难检索的问题，导致工作流程错误和诊断延迟风险。本研究旨在开发专门的RAG助手，为技术人员提供基于上下文的协议查询答案，将静态文档转化为动态可靠的知识辅助系统。

Method: 研究团队从葡萄牙医疗机构整理了99份解剖病理协议构成语料库，并构建323个问答对用于系统评估。通过10组实验，系统性地调整分块策略、检索方法和嵌入模型。采用RAGAS框架(忠实度、答案相关性、上下文召回率)和top-k检索指标进行性能评估。

Result: 实验表明递归分块与混合检索组合达到最佳基线性能。采用生物医学专用嵌入模型MedEmbed后，答案相关性提升至0.74，忠实度0.70，上下文召回率0.77。top-k分析显示单次检索(k=1)即可最大化效率和准确性，反映了AP协议的模块化结构。

Conclusion: 本研究揭示了医疗领域部署RAG系统的关键设计考量，证明了领域专用嵌入模型的重要性。该RAG助手有望将静态文档转变为动态知识辅助工具，显著提升实验室工作流程效率并支持患者安全。

Abstract: Accurate and efficient access to laboratory protocols is essential in Anatomical Pathology (AP), where up to 70% of medical decisions depend on laboratory diagnoses. However, static documentation such as printed manuals or PDFs is often outdated, fragmented, and difficult to search, creating risks of workflow errors and diagnostic delays. This study proposes and evaluates a Retrieval-Augmented Generation (RAG) assistant tailored to AP laboratories, designed to provide technicians with context-grounded answers to protocol-related queries. We curated a novel corpus of 99 AP protocols from a Portuguese healthcare institution and constructed 323 question-answer pairs for systematic evaluation. Ten experiments were conducted, varying chunking strategies, retrieval methods, and embedding models. Performance was assessed using the RAGAS framework (faithfulness, answer relevance, context recall) alongside top-k retrieval metrics. Results show that recursive chunking and hybrid retrieval delivered the strongest baseline performance. Incorporating a biomedical-specific embedding model (MedEmbed) further improved answer relevance (0.74), faithfulness (0.70), and context recall (0.77), showing the importance of domain-specialised embeddings. Top-k analysis revealed that retrieving a single top-ranked chunk (k=1) maximized efficiency and accuracy, reflecting the modular structure of AP protocols. These findings highlight critical design considerations for deploying RAG systems in healthcare and demonstrate their potential to transform static documentation into dynamic, reliable knowledge assistants, thus improving laboratory workflow efficiency and supporting patient safety.

</details>


### [125] [RAGdb: A Zero-Dependency, Embeddable Architecture for Multimodal Retrieval-Augmented Generation on the Edge](https://arxiv.org/abs/2602.22217)
*Ahmed Bin Khalid*

Main category: cs.IR

TL;DR: 本文提出RAGdb，一种新颖的单体内核架构，将多模态摄取、ONNX提取和混合向量检索整合到单个SQLite容器中。通过确定性混合评分函数（HSF）结合亚线性TF-IDF与精确子串增强，消除查询时的GPU推理需求。在Intel i7-1165G7上实现100% Recall@1实体检索、31.6倍增量更新效率提升，以及相比Docker-based RAG栈99.5%的磁盘占用缩减，为边缘计算和隐私敏感场景提供"单文件知识容器"解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有RAG架构存在"基础设施膨胀"问题，依赖云端向量数据库、重型深度学习框架和GPU推理服务器，导致在边缘计算、物理隔离环境和数据主权优先的隐私敏感应用中存在显著准入壁垒。

Method: 设计RAGdb单体架构，将自动化多模态摄取、ONNX运行时提取和混合向量检索统一至单个便携式SQLite容器。提出确定性混合评分函数（HSF），融合亚线性TF-IDF向量化和精确子串匹配，消除查询时GPU依赖。

Result: 在Intel i7-1165G7消费级笔记本上实验表明：实体检索达到100% Recall@1；增量更新相比冷启动获得31.6倍摄取效率提升；磁盘占用相比标准Docker-based RAG栈减少约99.5%。

Conclusion: RAGdb确立了"单文件知识容器"作为去中心化、本地优先AI的可行原语，有效解决了基础设施膨胀问题，为边缘计算、物理隔离和隐私约束场景提供了实用化RAG部署方案。

Abstract: Retrieval-Augmented Generation (RAG) has established itself as the standard paradigm for grounding Large Language Models (LLMs) in domain-specific, up-to-date data. However, the prevailing architecture for RAG has evolved into a complex, distributed stack requiring cloud-hosted vector databases, heavy deep learning frameworks (e.g., PyTorch, CUDA), and high-latency embedding inference servers. This ``infrastructure bloat'' creates a significant barrier to entry for edge computing, air-gapped environments, and privacy-constrained applications where data sovereignty is paramount.
  This paper introduces RAGdb, a novel monolithic architecture that consolidates automated multimodal ingestion, ONNX-based extraction, and hybrid vector retrieval into a single, portable SQLite container. We propose a deterministic Hybrid Scoring Function (HSF) that combines sublinear TF-IDF vectorization with exact substring boosting, eliminating the need for GPU inference at query time. Experimental evaluation on an Intel i7-1165G7 consumer laptop demonstrates that RAGdb achieves 100\% Recall@1 for entity retrieval and an ingestion efficiency gain of 31.6x during incremental updates compared to cold starts. Furthermore, the system reduces disk footprint by approximately 99.5\% compared to standard Docker-based RAG stacks, establishing the ``Single-File Knowledge Container'' as a viable primitive for decentralized, local-first AI.
  Keywords: Edge AI, Retrieval-Augmented Generation, Vector Search, Green AI, Serverless Architecture, Knowledge Graphs, Efficient Computing.

</details>


### [126] [Comparative Analysis of Neural Retriever-Reranker Pipelines for Retrieval-Augmented Generation over Knowledge Graphs in E-commerce Applications](https://arxiv.org/abs/2602.22219)
*Teri Rumble,Zbyněk Gazdík,Javad Zarrin,Jagdeep Ahluwalia*

Main category: cs.IR

TL;DR: 针对电商知识图谱的RAG挑战，本文提出并评估了多检索-重排序流水线，使用STaRK数据集实现Hit@1提升20.4%、MRR提升14.5%，建立了生产级应用框架。


<details>
  <summary>Details</summary>
Motivation: RAG在结构化知识图谱上面临检索扩展与上下文保持的挑战，跨编码器与结构化数据集成研究不足，亟需开发生产环境可用的领域特定助手。

Method: 设计并比较多种Retriever-Reranker流水线，基于STaRK电商半结构化知识库，优化自然语言查询的RAG配置。

Result: 在STaRK数据集上，Hit@1达20.4%提升，MRR达14.5%提升，显著超越现有基准。

Conclusion: 本研究为集成领域特定知识库到生成系统提供实践框架，指导生产级RAG部署，其方法论可推广至电商以外需要结构化知识检索的领域。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed Natural Language Processing (NLP), enabling complex information retrieval and generation tasks. Retrieval-Augmented Generation (RAG) has emerged as a key innovation, enhancing factual accuracy and contextual grounding by integrating external knowledge sources with generative models. Although RAG demonstrates strong performance on unstructured text, its application to structured knowledge graphs presents challenges: scaling retrieval across connected graphs and preserving contextual relationships during response generation. Cross-encoders refine retrieval precision, yet their integration with structured data remains underexplored. Addressing these challenges is crucial for developing domain-specific assistants that operate in production environments. This study presents the design and comparative evaluation of multiple Retriever-Reranker pipelines for knowledge graph natural language queries in e-Commerce contexts. Using the STaRK Semi-structured Knowledge Base (SKB), a production-scale e-Commerce dataset, we evaluate multiple RAG pipeline configurations optimized for language queries. Experimental results demonstrate substantial improvements over published benchmarks, achieving 20.4% higher Hit@1 and 14.5% higher Mean Reciprocal Rank (MRR). These findings establish a practical framework for integrating domain-specific SKBs into generative systems. Our contributions provide actionable insights for the deployment of production-ready RAG systems, with implications that extend beyond e-Commerce to other domains that require information retrieval from structured knowledge bases.

</details>


### [127] [TWICE: An LLM Agent Framework for Simulating Personalized User Tweeting Behavior with Long-term Temporal Features](https://arxiv.org/abs/2602.22222)
*Bingrui Jin,Kunyao Lan,Mengyue Wu*

Main category: cs.IR

TL;DR: 提出TWICE，基于LLM的框架，通过个性化画像、事件记忆和风格重写，实现社交媒体用户长期时序行为模拟，解决现有方法时序建模不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有用户模拟器聚焦集体行为或交互系统，难以应对需要建模时序特征的任务。社交媒体数据具有显著的长期时序性和个性化特征，亟需能够同时捕捉这两方面特性的模拟方法。

Method: 提出TWICE框架，集成：(1)个性化用户画像，(2)事件驱动记忆模块，(3)个性化风格重写流程。利用LLM能力模拟用户发推行为，同时捕获长期时序动态。

Result: 综合评估显示，该框架在发推风格和事件驱动行为变化方面表现优异，通过有效整合时序动态显著提升个性化模拟效果，为长期行为追踪提供稳健方案。

Conclusion: TWICE成功解决了用户模拟中的时序建模难题，为长期个性化行为模拟提供了有效解决方案。

Abstract: User simulators are often used to generate large amounts of data for various tasks such as generation, training, and evaluation. However, existing approaches concentrate on collective behaviors or interactive systems, struggling with tasks that require modeling temporal characteristics. To address this limitation, we propose TWICE, an LLM-based framework that leverages the long-term temporal and personalized features of social media data. This framework integrates personalized user profiling, an event-driven memory module, and a workflow for personalized style rewriting, enabling simulation of personalized user tweeting behavior while capturing long-term temporal characteristics. In addition, we conduct a comprehensive evaluation with a focus on analyzing tweeting style and event-based changes in behavior. Experiment results demonstrate that our framework improves personalized user simulation by effectively incorporating temporal dynamics, providing a robust solution for long-term behavior tracking.

</details>


### [128] [SQaLe: A Large Text-to-SQL Corpus Grounded in Real Schemas](https://arxiv.org/abs/2602.22223)
*Cornelius Wolff,Daniel Gomm,Madelon Hulsebos*

Main category: cs.IR

TL;DR: 本文提出SQaLe：一个基于135,875个真实数据库模式（来自SchemaPile）构建的大规模半合成文本到SQL数据集。通过模式采样、问题合成与SQL构建的生成流水线，生成51.7万个高质量（问题, 模式, 查询）三元组，是目前最具真实感的大规模文本到SQL数据集。


<details>
  <summary>Details</summary>
Motivation: 开发可泛化文本到SQL模型的核心瓶颈在于缺乏具备足够模式复杂度、查询多样性、领域覆盖度和任务多样性的可扩展大规模数据集。

Method: 在SchemaPile的13.5万个真实数据库模式基础上，建立原则性生成流水线，结合模式采样、自然语言问题合成与SQL查询构造，生成517,676个高质量三元组。

Result: SQaLe捕获了真实的模式规模分布、多样化查询模式及自然语言歧义性，同时保证执行有效性。分析表明，相比现有基准和数据集，SQaLe是迄今最真实的大规模文本到SQL数据集。

Conclusion: SQaLe推动了文本到SQL研究中数据扩展与模型泛化的发展，数据集已在Hugging Face平台开源。

Abstract: Advances in large language models have accelerated progress in text-to-SQL, methods for converting natural language queries into valid SQL queries. A key bottleneck for developing generalizable text-to-SQL models is the lack of large-scale datasets with sufficient schema and query complexity, domain coverage, and task diversity. We introduce SQaLe: a large-scale semi-synthetic text-to-SQL dataset built on 135,875 relational database schemas expanded from a collection of real-world schemas, SchemaPile. We establish a principled generation pipeline which combines schema sampling, question synthesis, and SQL construction, and produce 517,676 high-quality (question, schema, query) triples. The SQaLe dataset captures realistic schema size variability, diverse query patterns, and natural language ambiguity while maintaining execution validity. We provide an analysis of its contents and characteristics, and find that SQaLe introduces the most realistic large-scale text-to-SQL dataset to date in comparison with existing benchmarks and datasets. We discuss how SQaLe enables our vision for data scaling and model generalization in text-to-SQL research. The dataset is accessible at: https://huggingface.co/datasets/trl-lab/SQaLe-text-to-SQL-dataset.

</details>


### [129] [SmartChunk Retrieval: Query-Aware Chunk Compression with Planning for Efficient Document RAG](https://arxiv.org/abs/2602.22225)
*Xuechen Zhang,Koustava Goswami,Samet Oymak,Jiasi Chen,Nedim Lipka*

Main category: cs.IR

TL;DR: 针对传统RAG的静态分块和平面检索缺陷，本文提出SmartChunk自适应检索框架，通过查询感知的分块策略和轻量压缩模块，结合STITCH强化学习方案，在多项QA基准测试中实现精度与效率的平衡，超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG管道采用静态分块和固定检索策略，导致检索质量高度依赖分块大小、易引入噪声且不适用于大型语料库。这些问题限制了RAG在实际应用中的准确性和可扩展性。

Method: SmartChunk框架包含两部分：(1)规划器：预测每个查询的最优分块抽象层级；(2)轻量压缩模块：生成高级别分块嵌入而无需重复摘要。通过动态调整检索粒度，结合STITCH强化学习方案优化规划器决策。

Result: 在五个QA基准和一个域外数据集上评估显示，SmartChunk在超越现有RAG基线的同时降低了成本，展现出良好的可扩展性和对域外数据的一致增益。

Conclusion: SmartChunk通过自适应检索粒度有效平衡了准确性与效率，克服了固定策略的缺陷，是一种适用于多样化文档类型和查询风格的通用自适应检索框架。

Abstract: Retrieval-augmented generation (RAG) has strong potential for producing accurate and factual outputs by combining language models (LMs) with evidence retrieved from large text corpora. However, current pipelines are limited by static chunking and flat retrieval: documents are split into short, predetermined, fixed-size chunks, embeddings are retrieved uniformly, and generation relies on whatever chunks are returned. This design brings challenges, as retrieval quality is highly sensitive to chunk size, often introduces noise from irrelevant or misleading chunks, and scales poorly to large corpora. We present SmartChunk retrieval, a query-adaptive framework for efficient and robust long-document question answering (QA). SmartChunk uses (i) a planner that predicts the optimal chunk abstraction level for each query, and (ii) a lightweight compression module that produces high-level chunk embeddings without repeated summarization. By adapting retrieval granularity on the fly, SmartChunk balances accuracy with efficiency and avoids the drawbacks of fixed strategies. Notably, our planner can reason about chunk abstractions through a novel reinforcement learning scheme, STITCH, which boosts accuracy and generalization. To reflect real-world applications, where users face diverse document types and query styles, we evaluate SmartChunk on five QA benchmarks plus one out-of-domain dataset. Across these evaluations, SmartChunk outperforms state-of-the-art RAG baselines, while reducing cost. Further analysis demonstrates strong scalability with larger corpora and consistent gains on out-of-domain datasets, highlighting its effectiveness as a general framework for adaptive retrieval.

</details>


### [130] [SEGB: Self-Evolved Generative Bidding with Local Autoregressive Diffusion](https://arxiv.org/abs/2602.22226)
*Yulong Gao,Wan Jiang,Mingzhe Cao,Xuepu Wang,Zeyu Pan,Haonan Yang,Ye Liu,Xin Yang*

Main category: cs.IR

TL;DR: 本文提出自进化生成式竞价框架SEGB，解决离线生成策略缺乏近期远见和依赖外部改进的问题。SEGB通过合成短期未来状态提供动态远见，并利用价值引导策略精化实现完全离线的自我优化。在AuctionNet基准和大型A/B测试中显著优于基线，在线部署实现目标成本+10.19%增长。


<details>
  <summary>Details</summary>
Motivation: 在线广告自动竞价中，离线训练的生成策略存在两大局限：缺乏动态市场所需的近期远见，以及依赖模拟器或外部专家进行训练后改进。这限制了策略在实际复杂环境中的适应性和优化潜力。

Method: SEGB框架包含两个核心模块：1）短期未来状态合成器，为每次出价生成合理的近期市场动态，提供关键远见；2）价值引导策略精化机制，通过价值函数指导无需外部干预的迭代策略优化。该框架完全离线运行，仅利用静态数据实现策略自我进化。

Result: 在AuctionNet基准测试和大型A/B测试中，SEGB显著超越现有最优基线。在线部署中，目标成本指标提升+10.19%，验证了其商业价值。

Conclusion: SEGB通过融合短期动态规划与自包含的价值引导优化，实现了基于静态数据的鲁棒策略改进，解决了生成式竞价的核心局限。实验与在线结果证明了该框架在实际应用中的有效性，为动态市场中的自动竞价提供了新范式。

Abstract: In the realm of online advertising, automated bidding has become a pivotal tool, enabling advertisers to efficiently capture impression opportunities in real-time. Recently, generative auto-bidding has shown significant promise, offering innovative solutions for effective ad optimization. However, existing offline-trained generative policies lack the near-term foresight required for dynamic markets and usually depend on simulators or external experts for post-training improvement. To overcome these critical limitations, we propose Self-Evolved Generative Bidding (SEGB), a framework that plans proactively and refines itself entirely offline. SEGB first synthesizes plausible short-horizon future states to guide each bid, providing the agent with crucial, dynamic foresight. Crucially, it then performs value-guided policy refinement to iteratively discover superior strategies without any external intervention. This self-contained approach uniquely enables robust policy improvement from static data alone. Experiments on the AuctionNet benchmark and a large-scale A/B test validate our approach, demonstrating that SEGB significantly outperforms state-of-the-art baselines. In a large-scale online deployment, it delivered substantial business value, achieving a +10.19% increase in target cost, proving the effectiveness of our advanced planning and evolution paradigm.

</details>


### [131] [RETLLM: Training and Data-Free MLLMs for Multimodal Information Retrieval](https://arxiv.org/abs/2602.22278)
*Dawei Su,Dongsheng Wang*

Main category: cs.IR

TL;DR: 本文提出RetLLM，一种无需训练和数据的多模态信息检索框架。通过将检索任务重构为相似度分数生成任务，采用粗排-精排两阶段流程，并利用视觉增强模块帮助多模态大模型重新关注视觉信息，在基准测试中超越了微调模型，证明了多模态大模型在无训练情况下的强大检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型（MLLM）增强的多模态信息检索方法存在预训练不一致性问题，且需要大量标注数据集进行微调。这限制了方法的实用性和可扩展性，因此作者希望探索一种无需训练和数据的方法来直接利用MLLM进行MMIR。

Method: 作者提出RetLLM框架，将MMIR任务形式化为相似度分数生成任务。具体采用粗排-精排两阶段流程：粗排阶段使用top-k过滤策略为每个查询构建小规模高质量候选池，让MLLM聚焦于语义相关的候选；精排阶段则将查询和候选同时输入MLLM直接预测检索分数。此外，引入视觉增强模块，在推理过程中帮助MLLM重新拾取被遗忘的视觉信息。

Result: 在MMIR基准测试上的广泛实验表明，RetLLM的性能优于经过微调的模型。消融研究进一步验证了框架中各个组件（如粗排策略、精排机制和视觉增强模块）的有效性。

Conclusion: 该工作证明，多模态大模型可以在无需任何训练的情况下实现强大的多模态信息检索性能，突出了其内在的多模态推理能力。RetLLM提供了一个简单、可扩展的框架，展示了MLLM在信息检索任务上的潜力，并为未来研究提供了新方向。

Abstract: Multimodal information retrieval (MMIR) has gained attention for its flexibility in handling text, images, or mixed queries and candidates. Recent breakthroughs in multimodal large language models (MLLMs) boost MMIR performance by incorporating MLLM knowledge under the contrastive finetuning framework. However, they suffer from pre-training inconsistency and require large datasets. In this work, we introduce a novel framework, RetLLM, designed to query MLLMs for MMIR in a training- and data-free manner. Specifically, we formulate MMIR as a similarity score generation task and prompt MLLMs to directly predict retrieval scores in a coarse-then-fine pipeline. At the coarse stage, a top-k filtering strategy builds a small yet high-quality candidate pool for each query, enabling MLLMs to focus on semantically relevant candidates. Subsequently, the retrieval score is predicted by feeding both the query and candidate into MLLMs at the fine stage. Importantly, we propose a visual enhancement module during reasoning to help MLLMs re-pick forgotten visuals, improving retrieval. Extensive experiments on MMIR benchmarks show that RetLLM outperforms fine-tuned models. Ablation studies further verify each component. Our work demonstrates that MLLMs can achieve strong MMIR performance without any training, highlighting their inherent multimodal reasoning ability in a simple, scalable framework. We release our code at: https://github.com/alivecat05/RETLLM

</details>


### [132] [TFPS: A Temporal Filtration-enhanced Positive Sample Set Construction Method for Implicit Collaborative Filtering](https://arxiv.org/abs/2602.22521)
*Jiayi Wu,Zhengyu Wu,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 本文针对隐式反馈推荐系统中正样本质量不高且缺乏时序信息的问题，提出了一种时间过滤增强的正样本集构建方法TFPS。该方法通过时间衰减模型将用户-物品交互图转化为加权二分图，进行分层过滤并设计层增强策略，从数据层面提升推荐模型的训练质量。理论分析和在三个真实数据集上的实验表明，该方法能有效提升Recall@k和NDCG@k指标，且可与多种推荐模型和负采样策略结合使用。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式反馈的协同过滤推荐方法在负采样过程中过度关注负样本优化，忽视了正样本的质量提升。部分去噪推荐方法虽可处理正样本噪声，但忽略了交互行为中的时间间隔信息。现有研究虽在模型聚合阶段融入时序信息，却未充分考虑时间间隔对捕捉用户当前偏好的重要性，导致推荐准确性受限。

Method: 从数据角度出发，提出TFPS方法：1) 设计基于交互时间间隔的时间衰减模型，将原始交互图转化为加权用户-物品二分图；2) 采用预定义过滤操作对加权图进行分层处理；3) 设计层增强策略，从分层子图中构建高质量正样本集。该方法可与现有隐式协同过滤模型及负采样策略灵活集成。

Result: 在三个真实世界数据集上的大量实验表明，TFPS能有效提升推荐性能。理论分析验证了TFPS对Recall@k和NDCG@k指标的改善作用。该方法作为数据预处理模块，能够无缝集成到多种隐式协同过滤推荐器和负采样方法中，显著增强其性能表现。

Conclusion: 本文提出的TFPS方法通过充分利用交互时间间隔信息，从数据层面构建了高质量正样本集，解决了传统方法忽视正样本时序信息的缺陷。实验证明该方法具有通用性和有效性，为提升隐式反馈推荐系统的性能提供了新的数据视角解决方案。

Abstract: The negative sampling strategy can effectively train collaborative filtering (CF) recommendation models based on implicit feedback by constructing positive and negative samples. However, existing methods primarily optimize the negative sampling process while neglecting the exploration of positive samples. Some denoising recommendation methods can be applied to denoise positive samples within negative sampling strategies, but they ignore temporal information. Existing work integrates sequential information during model aggregation but neglects time interval information, hindering accurate capture of users' current preferences. To address this problem, from a data perspective, we propose a novel temporal filtration-enhanced approach to construct a high-quality positive sample set. First, we design a time decay model based on interaction time intervals, transforming the original graph into a weighted user-item bipartite graph. Then, based on predefined filtering operations, the weighted user-item bipartite graph is layered. Finally, we design a layer-enhancement strategy to construct a high-quality positive sample set for the layered subgraphs. We provide theoretical insights into why TFPS can improve Recall@k and NDCG@k, and extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed method. Additionally, TFPS can be integrated with various implicit CF recommenders or negative sampling methods to enhance its performance.

</details>


### [133] [Generative Agents Navigating Digital Libraries](https://arxiv.org/abs/2602.22529)
*Saber Zerhoudi,Michael Granitzer*

Main category: cs.IR

TL;DR: 这篇论文提出了Agent4DL，一个专为数字图书馆环境设计的用户搜索行为模拟器。该模拟器利用大语言模型生成符合用户画像的动态搜索会话，模拟查询、点击和停止等真实交互行为，有效解决了因隐私限制导致的真实搜索数据稀缺问题，并在对比实验中展现出优于SimIIR 2.0的多样性和情境感知能力。


<details>
  <summary>Details</summary>
Motivation: 数字图书馆研究长期受困于用户搜索行为数据的公开稀缺性，这主要源于严格的隐私保护要求，限制了相关检索算法和用户行为模型的发展。大语言模型的兴起为通过仿真方式构建用户行为数据提供了新的解决思路。

Method: 开发Agent4DL模拟器，核心方法是基于大语言模型生成具有差异化的用户画像，并驱动这些智能体在数字图书馆环境中执行动态搜索会话，包括根据用户特征自适应地生成查询语句、模拟点击决策以及决定停止时机，从而完整复现真实用户的搜索策略。

Result: 实验通过与真实用户数据进行对比验证了Agent4DL的准确性；在与现有模拟器SimIIR 2.0的对比中，Agent4DL表现出更强的竞争力，特别是在生成多样化和情境感知的用户行为方面具有显著优势。

Conclusion: Agent4DL成功构建了一个可行的数字图书馆用户行为仿真框架，为解决真实数据不可得的问题提供了有效替代方案，为数字图书馆系统的评估、优化和用户研究开辟了新的技术路径。

Abstract: In the rapidly evolving field of digital libraries, the development of large language models (LLMs) has opened up new possibilities for simulating user behavior. This innovation addresses the longstanding challenge in digital library research: the scarcity of publicly available datasets on user search patterns due to privacy concerns. In this context, we introduce Agent4DL, a user search behavior simulator specifically designed for digital library environments. Agent4DL generates realistic user profiles and dynamic search sessions that closely mimic actual search strategies, including querying, clicking, and stopping behaviors tailored to specific user profiles. Our simulator's accuracy in replicating real user interactions has been validated through comparisons with real user data. Notably, Agent4DL demonstrates competitive performance compared to existing user search simulators such as SimIIR 2.0, particularly in its ability to generate more diverse and context-aware user behaviors.

</details>


### [134] [Towards Dynamic Dense Retrieval with Routing Strategy](https://arxiv.org/abs/2602.22547)
*Zhan Su,Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Bingbing Wen,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 针对稠密检索（DR）微调范式在数据稀缺和模型更新成本方面的局限性，本文提出动态稠密检索（DDR），通过前缀调优模块和动态路由策略实现灵活域适应，仅用2%的训练参数即在六个零样本下游任务上超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有稠密检索范式存在两大局限：一是训练数据有限时难以适应新领域；二是模型过时后需从头训练新模型替换旧模型，频繁更新时代价过高。

Method: 提出动态稠密检索（DDR），将前缀调优作为特定领域专用模块，通过动态路由策略组合这些模块，实现检索部分的高度灵活域适应。

Result: 在六个零样本下游任务上的广泛评估表明，该方法仅需2%的训练参数即可超越传统稠密检索方法，为信息检索中更灵活的稠密检索铺平道路。

Conclusion: 动态稠密检索是一种有前途的未来方向，可更灵活地将稠密检索应用于各种任务。

Abstract: The \textit{de facto} paradigm for applying dense retrieval (DR) to new tasks involves fine-tuning a pre-trained model for a specific task. However, this paradigm has two significant limitations: (1) It is difficult adapt the DR to a new domain if the training dataset is limited.
  (2) Old DR models are simply replaced by newer models that are trained from scratch when the former are no longer up to date. Especially for scenarios where the model needs to be updated frequently, this paradigm is prohibitively expensive. To address these challenges, we propose a novel dense retrieval approach, termed \textit{dynamic dense retrieval} (DDR). DDR uses \textit{prefix tuning} as a \textit{module} specialized for a specific domain. These modules can then be compositional combined with a dynamic routing strategy, enabling highly flexible domain adaptation in the retrieval part. Extensive evaluation on six zero-shot downstream tasks demonstrates that this approach can surpass DR while utilizing only 2\% of the training parameters, paving the way to achieve more flexible dense retrieval in IR. We see it as a promising future direction for applying dense retrieval to various tasks.

</details>


### [135] [Where Relevance Emerges: A Layer-Wise Study of Internal Attention for Zero-Shot Re-Ranking](https://arxiv.org/abs/2602.22591)
*Haodong Chen,Shengyao Zhuang,Zheng Yao,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.IR

TL;DR: 本文通过正交评估生成、似然和内部注意力机制，揭示transformer层间相关性信号的"钟形曲线"分布规律，提出Selective-ICR策略实现30%-50%推理加速，并在BRIGHT基准上证明小模型通过内部注意力信号可超越大模型生成式方法，重新定义了LLM重排序的效率边界。


<details>
  <summary>Details</summary>
Motivation: 现有零样本重排序方法依赖生成式评分或输出logits，面临推理延迟和结果一致性瓶颈；ICR虽避免生成开销但简单聚合层信号，未系统探索层间贡献差异与跨架构一致性，且缺乏统一条件下内部注意力与传统机制的比较研究。

Method: 对生成、似然和内部注意力机制进行正交评估；分析层间相关性信号分布特征；提出基于层信号分布的选择性ICR策略，优化计算效率。

Result: 发现跨层相关性信号呈普适性"钟形曲线"分布；Selective-ICR实现30%-50%延迟降低而不损失效果；BRIGHT基准上零样本8B模型媲美14B强化学习重排序器，0.6B模型超越最优生成式方法。

Conclusion: 研究重新定义了LLM重排序的效率-效果前沿，证实内部注意力信号可减少对模型缩放和强化学习的依赖，为复杂推理排序任务提供了更高效的解决方案。

Abstract: Zero-shot document re-ranking with Large Language Models (LLMs) has evolved from Pointwise methods to Listwise and Setwise approaches that optimize computational efficiency. Despite their success, these methods predominantly rely on generative scoring or output logits, which face bottlenecks in inference latency and result consistency. In-Context Re-ranking (ICR) has recently been proposed as an $O(1)$ alternative method. ICR extracts internal attention signals directly, avoiding the overhead of text generation. However, existing ICR methods simply aggregate signals across all layers; layer-wise contributions and their consistency across architectures have been left unexplored. Furthermore, no unified study has compared internal attention with traditional generative and likelihood-based mechanisms across diverse ranking frameworks under consistent conditions.
  In this paper, we conduct an orthogonal evaluation of generation, likelihood, and internal attention mechanisms across multiple ranking frameworks. We further identify a universal "bell-curve" distribution of relevance signals across transformer layers, which motivates the proposed Selective-ICR strategy that reduces inference latency by 30%-50% without compromising effectiveness. Finally, evaluation on the reasoning-intensive BRIGHT benchmark shows that precisely capturing high-quality in-context attention signals fundamentally reduces the need for model scaling and reinforcement learning: a zero-shot 8B model matches the performance of 14B reinforcement-learned re-rankers, while even a 0.6B model outperforms state-of-the-art generation-based approaches. These findings redefine the efficiency-effectiveness frontier for LLM-based re-ranking and highlight the latent potential of internal signals for complex reasoning ranking tasks. Our code and results are publicly available at https://github.com/ielab/Selective-ICR.

</details>


### [136] [Fine-grained Semantics Integration for Large Language Model-based Recommendation](https://arxiv.org/abs/2602.22632)
*Jiawen Feng,Xiaoyu Kong,Leheng Sheng,Bin Wu,Chao Yi,Feifang Yang,Xiang-Rong Sheng,Han Zhu,Xiang Wang,Jiancan Wu,Xiangnan He*

Main category: cs.IR

TL;DR: 本文提出TS-Rec框架解决基于大语言模型的生成式推荐系统中语义标识符空间建模的两大挑战：随机初始化导致的语义断裂和粗粒度对齐忽略令牌级语义，通过语义感知初始化和令牌级语义对齐显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成式推荐器存在两个根本问题：1）语义标识符（SID）令牌随机初始化切断了与预训练语言空间的初始语义链接；2）基于指令微调（SFT）的对齐仅关注项目级优化，忽视了SID序列内部各令牌的细粒度语义，导致性能提升受限。

Method: 提出TS-Rec框架，包含两大组件：1）语义感知嵌入初始化（SA-Init）：利用教师模型提取关键词，对其预训练嵌入进行平均池化来初始化SID令牌嵌入；2）令牌级语义对齐（TS-Align）：将SID序列中的单个令牌与对应项目集群的共享语义空间进行细粒度对齐。

Result: 在两个真实世界基准数据集上的实验表明，TS-Rec在所有标准评估指标上均持续优于传统推荐基线和生成式推荐基线，验证了方法的有效性。

Conclusion: 细粒度令牌级语义信息的整合能显著增强LLM生成式推荐器的性能，证明了初始语义链接建立和令牌级对齐在构建高性能生成式推荐系统中的关键作用。

Abstract: Recent advances in Large Language Models (LLMs) have shifted in recommendation systems from the discriminative paradigm to the LLM-based generative paradigm, where the recommender autoregressively generates sequences of semantic identifiers (SIDs) for target items conditioned on historical interaction. While prevalent LLM-based recommenders have demonstrated performance gains by aligning pretrained LLMs between the language space and the SID space, modeling the SID space still faces two fundamental challenges: (1) Semantically Meaningless Initialization: SID tokens are randomly initialized, severing the semantic linkage between the SID space and the pretrained language space at start point, and (2) Coarse-grained Alignment: existing SFT-based alignment tasks primarily focus on item-level optimization, while overlooking the semantics of individual tokens within SID sequences.To address these challenges, we propose TS-Rec, which can integrate Token-level Semantics into LLM-based Recommenders. Specifically, TS-Rec comprises two key components: (1) Semantic-Aware embedding Initialization (SA-Init), which initializes SID token embeddings by applying mean pooling to the pretrained embeddings of keywords extracted by a teacher model; and (2) Token-level Semantic Alignment (TS-Align), which aligns individual tokens within the SID sequence with the shared semantics of the corresponding item clusters. Extensive experiments on two real-world benchmarks demonstrate that TS-Rec consistently outperforms traditional and generative baselines across all standard metrics. The results demonstrate that integrating fine-grained semantic information significantly enhances the performance of LLM-based generative recommenders.

</details>


### [137] [Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators](https://arxiv.org/abs/2602.22647)
*Zhengyang Su,Isay Katsman,Yueqi Wang,Ruining He,Lukasz Heldt,Raghunandan Keshavan,Shao-Chuan Wang,Xinyang Yi,Mingyan Gao,Onkar Dalal,Lichan Hong,Ed Chi,Ningren Han*

Main category: cs.IR

TL;DR: 本文提出STATIC（稀疏转移矩阵加速的Trie索引约束解码），通过将前缀树展平为CSR矩阵实现向量化稀疏计算，解决了工业推荐系统中约束生成式检索的延迟问题，在十亿级用户平台上实现每步0.033毫秒的超低开销。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统需基于业务逻辑（如内容新鲜度、产品类别）限制输出空间，但标准自回归解码无法原生支持此类约束，而现有基于Trie的约束解码方法在TPU/GPU上存在严重延迟，无法满足生产环境需求。

Method: 创新性地将动态Trie树结构静态化为压缩稀疏行(CSR)格式矩阵，将不规则的树遍历操作转化为完全向量化的稀疏矩阵运算，专为TPU/GPU硬件加速器设计的端到端约束解码框架。

Result: 在YouTube视频推荐平台部署，每步增加延迟仅0.033毫秒（占推理时间0.25%），相比CPU Trie实现948倍加速，相比硬件加速二分搜索基线提升47-1033倍，冷启动性能显著改善，支持多种配置且开销稳定。

Conclusion: STATIC首次实现了生产级严格约束生成式检索，以极低开销解锁了硬件加速器的效率潜力，为工业推荐系统提供了可扩展的约束解码解决方案，代码已开源促进社区发展。

Abstract: Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.

</details>


### [138] [Generative Recommendation for Large-Scale Advertising](https://arxiv.org/abs/2602.22732)
*Ben Xue,Dan Liu,Lixiang Wang,Mingjie Sun,Peng Wang,Pengfei Zhang,Shaoyun Shi,Tianyu Xu,Yunhao Sha,Zhiqiang Liu,Bo Kong,Bo Wang,Hang Yang,Jieting Xue,Junhao Wang,Shengyu Wang,Shuping Hui,Wencai Ye,Xiao Lin,Yongzhi Li,Yuhang Chen,Zhihui Yin,Quan Chen,Shiyang Wen,Wenjin Wu,Han Li,Guorui Zhou,Changcheng Li,Peng Jiang*

Main category: cs.IR

TL;DR: 快手GR4AD：面向生产的生成式推荐系统，通过架构-学习-服务协同设计，400M+用户规模实现4.2%广告收入提升。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐虽具扩展潜力和强模型能力，但大规模广告实时部署需超越传统LLM训练与服务框架的特殊设计，以在固定服务预算下平衡效果与推理成本。

Method: 1) UA-SID统一广告语义ID捕获复杂业务信息；2) LazyAR惰性自回归解码器放松层间依赖，支持多候选生成并降低推理开销；3) VSL值感知监督学习与RSPO排序感知Softmax偏好优化，将优化目标对齐业务价值；4) 动态beam serving自适应调整生成层级与负载下的计算宽度。

Result: 大规模A/B测试显示较DLRM基线提升广告收入最高4.2%，已在快手广告系统全量部署，服务超4亿用户，实现高吞吐实时推理，模型扩展与推理时扩展均带来持续增益。

Conclusion: GR4AD通过端到端协同设计验证了生成式推荐在大规模生产环境的可行性与商业价值，为行业提供了可扩展、高吞吐的实时推荐解决方案。

Abstract: Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.

</details>


### [139] [PSQE: A Theoretical-Practical Approach to Pseudo Seed Quality Enhancement for Unsupervised MMEA](https://arxiv.org/abs/2602.22903)
*Yunpeng Hong,Chenyang Bu,Jie Zhang,Yi He,Di Wu,Xindong Wu*

Main category: cs.IR

TL;DR: 针对多模态实体对齐（MMEA）中伪种子覆盖不平衡问题，本文提出PSQE框架，通过多模态信息与聚类重采样提升伪种子精度与图覆盖均衡性。理论揭示伪种子对对比学习吸引/排斥项的双重影响及覆盖不均衡导致模型偏向高密度区域的问题。实验验证PSQE作为即插即用模块可显著提升基线性能。


<details>
  <summary>Details</summary>
Motivation: 多模态实体对齐对大语言模型应用的结构化数据集成至关重要。当前无监督方法虽通过伪种子规避了标注难题，但多模态信息导致知识图中伪种子分布不均，模型过度关注高密度区域而弱化稀疏区域学习，该问题尚未被充分研究。

Method: 提出PSQE（伪种子质量增强）框架，利用多模态信息优化种子质量，并采用聚类重采样平衡图覆盖分布。通过理论分析量化伪种子对对比学习吸引与排斥项的同步影响，揭示覆盖不均衡导致的模型偏置机制。

Result: 实验结果表明，PSQE能有效改善伪种子精度与覆盖均衡性，作为即插即用模块在多个基线模型上实现性能大幅提升，验证了理论分析的准确性。

Conclusion: PSQE为无监督多模态实体对齐提供了有效的伪种子优化方案，其即插即用特性使其易于集成到现有模型中，显著提升了跨模态数据对齐的鲁棒性和泛化能力。

Abstract: Multimodal Entity Alignment (MMEA) aims to identify equivalent entities across different data modalities, enabling structural data integration that in turn improves the performance of various large language model applications. To lift the requirement of labeled seed pairs that are difficult to obtain, recent methods shifted to an unsupervised paradigm using pseudo-alignment seeds. However, unsupervised entity alignment in multimodal settings remains underexplored, mainly because the incorporation of multimodal information often results in imbalanced coverage of pseudo-seeds within the knowledge graph. To overcome this, we propose PSQE (Pseudo-Seed Quality Enhancement) to improve the precision and graph coverage balance of pseudo seeds via multimodal information and clustering-resampling. Theoretical analysis reveals the impact of pseudo seeds on existing contrastive learning-based MMEA models. In particular, pseudo seeds can influence the attraction and the repulsion terms in contrastive learning at once, whereas imbalanced graph coverage causes models to prioritize high-density regions, thereby weakening their learning capability for entities in sparse regions. Experimental results validate our theoretical findings and show that PSQE as a plug-and-play module can improve the performance of baselines by considerable margins.

</details>


### [140] [SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress](https://arxiv.org/abs/2602.22913)
*Yang Yu,Lei Kou,Huaikuan Yi,Bin Chen,Yayu Cao,Lei Shen,Chao Zhang,Bing Wang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 本文提出SIGMA，一个面向AliExpress的语义 grounded 指令驱动生成式多任务推荐系统，通过统一潜在空间 grounding、混合标记化、多任务SFT数据集和自适应概率融合机制，解决了现有方法在适应趋势变化和多样化任务需求方面的局限性，经线上线下实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有推荐方法局限于交互驱动的下一个物品预测范式，无法快速适应不断变化的趋势，也难以满足真实场景中多样化的推荐任务及业务特定需求。

Method: 提出SIGMA系统：1) 通过统一潜在空间在通用语义中 grounding 物品实体，同时捕捉语义和协同关系；2) 开发混合物品标记化方法以实现精确建模和高效生成；3) 构建大规模多任务SFT数据集，使模型能够通过指令遵循满足各种推荐需求；4) 设计三步物品生成流程，并结合自适应概率融合机制，根据任务特定要求校准输出分布以提升推荐准确性和多样性。

Result: 广泛的离线实验和在线A/B测试证明了SIGMA的有效性。

Conclusion: SIGMA能够有效解决现有推荐系统的局限性，通过语义 grounding 和指令驱动的方式实现多任务推荐，在真实商业场景中表现出良好的适应性和效果。

Abstract: With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress. Specifically, we first ground item entities in general semantics via a unified latent space capturing both semantic and collaborative relations. Building upon this, we develop a hybrid item tokenization method for precise modeling and efficient generation. Moreover, we construct a large-scale multi-task SFT dataset to empower SIGMA to fulfill various recommendation demands via instruction-following. Finally, we design a three-step item generation procedure integrated with an adaptive probabilistic fusion mechanism to calibrate the output distributions based on task-specific requirements for recommendation accuracy and diversity. Extensive offline experiments and online A/B tests demonstrate the effectiveness of SIGMA.

</details>


### [141] [Sequential Regression for Continuous Value Prediction using Residual Quantization](https://arxiv.org/abs/2602.23012)
*Runpeng Cui,Zhipeng Sun,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 针对推荐系统中连续值预测的挑战，本文提出了一种基于残差量化(RQ)的序列学习框架，通过从粗到细递归预测量化码，并结合序结构对齐的表示学习目标，在LTV、观看时长和GMV预测任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的连续值预测（如观看时长、GMV）面临数据分布复杂且长尾化的挑战。现有生成方法受限于参数化分布假设，简单假设无法建模复杂分布，复杂假设则扩展性和泛化性差。

Method: 提出RQ-based序列学习框架：1）将连续值表示为有序量化码之和；2）从粗到细递归预测，量化误差递减；3）设计表示学习目标对齐RQ码嵌入空间与目标值的序结构，提升量化码的连续表示能力。

Result: 在公开LTV和观看时长基准测试，以及工业级短视频推荐平台的GMV预测实验中，该方法持续优于现有最优方法，并在不同连续值预测任务间展现出强泛化能力。

Conclusion: RQ序列学习方法通过有序量化码递归预测和序对齐表示学习，有效解决了连续值预测中的分布建模难题，为工业推荐系统提供了高性能、强泛化的解决方案。

Abstract: Continuous value prediction plays a crucial role in industrial-scale recommendation systems, including tasks such as predicting users' watch-time and estimating the gross merchandise value (GMV) in e-commerce transactions. However, it remains challenging due to the highly complex and long-tailed nature of the data distributions. Existing generative approaches rely on rigid parametric distribution assumptions, which fundamentally limits their performance when such assumptions misalign with real-world data. Overly simplified forms cannot adequately model real-world complexities, while more intricate assumptions often suffer from poor scalability and generalization.
  To address these challenges, we propose a residual quantization (RQ)-based sequence learning framework that represents target continuous values as a sum of ordered quantization codes, predicted recursively from coarse to fine granularity with diminishing quantization errors. We introduce a representation learning objective that aligns RQ code embedding space with the ordinal structure of target values, allowing the model to capture continuous representations for quantization codes and further improving prediction accuracy. We perform extensive evaluations on public benchmarks for lifetime value (LTV) and watch-time prediction, alongside a large-scale online experiment for GMV prediction on an industrial short-video recommendation platform. The results consistently show that our approach outperforms state-of-the-art methods, while demonstrating strong generalization across diverse continuous value prediction tasks in recommendation systems.

</details>


### [142] [MoDora: Tree-Based Semi-Structured Document Analysis System](https://arxiv.org/abs/2602.23061)
*Bangrui Xu,Qihang Yao,Zirui Tang,Xuanhe Zhou,Yeye He,Shihan Yu,Qianqian Xu,Bin Wang,Guoliang Li,Conghui He,Fan Wu*

Main category: cs.IR

TL;DR: MoDora是一个LLM驱动的半结构化文档分析系统，通过局部对齐聚合、组件关联树和问句感知检索策略，解决了OCR碎片化、层次结构表示不足和多区域信息对齐三大挑战，在准确率上超越基线5.97%-61.07%。


<details>
  <summary>Details</summary>
Motivation: 半结构化文档在实际应用中广泛存在且占据大量真实世界数据，但现有方法面临三大技术挑战：OCR提取元素碎片化且缺乏语义上下文、缺乏有效表示捕捉层次结构与布局差异、问答需跨多区域对齐信息，难以支持自然语言问答。

Method: 提出MoDora系统：1）采用局部对齐聚合策略将OCR解析元素转换为布局感知组件，并进行类型特定信息抽取；2）设计组件关联树（CCTree）通过自底向上级联摘要构建层次化组织结构，显式建模组件间关系与布局区分；3）提出问句感知检索策略，结合基于布局的网格划分和LLM引导剪枝实现位置与语义双维度检索。

Result: 实验表明MoDora在准确率上比基线方法显著提升5.97%-61.07%，代码已开源。

Conclusion: MoDora有效解决了半结构化文档问答中的关键技术挑战，通过创新的组件化表示与检索机制显著提升性能，为实际应用提供了可靠解决方案。

Abstract: Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.
  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.

</details>


### [143] [MaRI: Accelerating Ranking Model Inference via Structural Re-parameterization in Large Scale Recommendation System](https://arxiv.org/abs/2602.23105)
*Yusheng Huang,Pengbo Xu,Shen Wang,Changxin Lao,Jiangxia Cao,Shuang Wen,Shuang Yang,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 本文提出MaRI（矩阵重参数化推理框架），通过结构重参数化消除特征融合矩阵乘法中的用户端计算冗余，实现推荐系统排序模型的无损加速。


<details>
  <summary>Details</summary>
Motivation: 现有排序模型加速技术（结构轻量化、知识蒸馏）存在显著准确率下降问题。研究空白在于：通过优化特征融合矩阵乘法（特别是结构重参数化）实现无损加速的视角尚未被充分探索。作者观察到特征融合过程中存在用户端计算冗余现象。

Method: 提出MaRI框架，采用结构重参数化技术重构特征融合矩阵乘法计算图，消除用户端冗余计算，实现无损推理加速。

Result: 在不损失任何准确率的前提下实现推理加速，可作为现有加速技术的补充方案。

Conclusion: MaRI成功验证了通过结构重参数化消除用户端计算冗余的可行性，为推荐系统排序模型提供了无损加速新范式。

Abstract: Ranking models, i.e., coarse-ranking and fine-ranking models, serve as core components in large-scale recommendation systems, responsible for scoring massive item candidates based on user preferences. To meet the stringent latency requirements of online serving, structural lightweighting or knowledge distillation techniques are commonly employed for ranking model acceleration. However, these approaches typically lead to a non-negligible drop in accuracy. Notably, the angle of lossless acceleration by optimizing feature fusion matrix multiplication, particularly through structural reparameterization, remains underexplored. In this paper, we propose MaRI, a novel Matrix Re-parameterized Inference framework, which serves as a complementary approach to existing techniques while accelerating ranking model inference without any accuracy loss. MaRI is motivated by the observation that user-side computation is redundant in feature fusion matrix multiplication, and we therefore adopt the philosophy of structural reparameterization to alleviate such redundancy.

</details>


### [144] [From Agnostic to Specific: Latent Preference Diffusion for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2602.23132)
*Ruochen Yang,Xiaodong Li,Jiawei Sheng,Jiangxia Cao,Xinkui Lin,Shen Wang,Shuang Yang,Zhaojie Liu,Tingwen Liu*

Main category: cs.IR

TL;DR: 该论文针对多行为序列推荐问题，提出FatsMB框架，利用扩散模型从行为无关到行为特定引导潜在偏好生成，解决现有方法忽略潜在偏好和不确定性的问题，实现多样且准确的推荐。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个主要问题：1）单向建模方式将辅助行为映射到目标行为，限制了行为间的交互协作；2）基于偏好评分的判别范式无法捕捉从低熵行为到高熵物品的不确定性，难以提供高效多样的推荐。同时，这些方法忽略了用户决策背后的潜在偏好，导致次优解。

Method: 提出FatsMB框架，包含四个核心组件：1）多行为自编码器(MBAE)构建统一用户潜在偏好空间，促进跨行为交互；2）行为感知RoPE(BaRoPE)融合多源信息；3）在潜在空间中进行目标行为特定偏好迁移，丰富先验信息；4）多条件引导层归一化(MCGLN)实现扩散去噪过程。该方法采用从行为无关到行为特定的生成式范式。

Result: 在真实世界数据集上的大量实验证明了模型的有效性，能够实现多样且准确的多行为序列推荐。

Conclusion: 该研究成功将扩散模型引入多行为序列推荐领域，通过潜在空间中的偏好生成与迁移机制，有效解决了传统方法忽略潜在偏好和不确定性建模的问题，为提升推荐多样性和准确性提供了新思路。

Abstract: Multi-behavior sequential recommendation (MBSR) aims to learn the dynamic and heterogeneous interactions of users' multi-behavior sequences, so as to capture user preferences under target behavior for the next interacted item prediction. Unlike previous methods that adopt unidirectional modeling by mapping auxiliary behaviors to target behavior, recent concerns are shifting from behavior-fixed to behavior-specific recommendation. However, these methods still ignore the user's latent preference that underlying decision-making, leading to suboptimal solutions. Meanwhile, due to the asymmetric deterministic between items and behaviors, discriminative paradigm based on preference scoring is unsuitable to capture the uncertainty from low-entropy behaviors to high-entropy items, failing to provide efficient and diverse recommendation. To address these challenges, we propose \textbf{FatsMB}, a framework based diffusion model that guides preference generation \textit{\textbf{F}rom Behavior-\textbf{A}gnostic \textbf{T}o Behavior-\textbf{S}pecific} in latent spaces, enabling diverse and accurate \textit{\textbf{M}ulti-\textbf{B}ehavior Sequential Recommendation}. Specifically, we design a Multi-Behavior AutoEncoder (MBAE) to construct a unified user latent preference space, facilitating interaction and collaboration across Behaviors, within Behavior-aware RoPE (BaRoPE) employed for multiple information fusion. Subsequently, we conduct target behavior-specific preference transfer in the latent space, enriching with informative priors. A Multi-Condition Guided Layer Normalization (MCGLN) is introduced for the denoising. Extensive experiments on real-world datasets demonstrate the effectiveness of our model.

</details>


### [145] [Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments](https://arxiv.org/abs/2602.23234)
*Evangelia Christakopoulou,Vivekkumar Patel,Hemanth Velaga,Sandip Gaikwad*

Main category: cs.IR

TL;DR: 苹果App Store搜索排名系统通过微调专用LLM生成海量文本相关性标签，有效缓解了专家标注数据稀缺问题。将这些标签融入生产排序器后，在行为相关性和文本相关性上均实现了离线NDCG提升，并通过全球A/B测试验证带来了+0.24%的转化率提升，尤其在长尾查询上效果显著。


<details>
  <summary>Details</summary>
Motivation: 大规模商业搜索系统需要同时优化行为相关性（用户点击/下载）和文本相关性（语义匹配），但面临专家提供的文本相关性标签稀缺而行为标签丰富的数据不平衡挑战，限制了文本相关性信号的利用。

Method: 1) 系统评估不同LLM配置，发现微调专用模型在提供高相关性标签上显著优于大型预训练模型；2) 采用最优模型作为"力量倍增器"生成数百万文本相关性标签；3) 将这些合成标签与行为标签结合，增强生产排序器的训练数据。

Result: 1) 帕累托前沿向外移动，离线NDCG在行为相关性和文本相关性维度同时提升；2) 全球A/B测试显示转化率实现统计显著的+0.24%相对增长；3) 在缺乏可靠行为信号的长尾查询上性能提升最为突出。

Conclusion: 利用微调LLM规模化生成高质量文本相关性标签是解决标注数据稀缺问题的有效范式，可同时提升搜索相关性和商业指标，为大规模搜索系统提供了可扩展的双目标优化解决方案。

Abstract: Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in conversion rate, with the most substantial performance gains occurring in tail queries, where the new textual relevance labels provide a robust signal in the absence of reliable behavioral relevance labels.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [146] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: 本文提出GYWI系统，通过作者知识图谱与检索增强生成结合，为LLMs提供可控学术语境与可追溯灵感路径，显著提升科学创意生成的新颖性、可靠性和相关性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学创意生成领域展现潜力，但存在生成结果缺乏可控学术语境和可追溯灵感路径的问题。为弥补这一缺陷，本文提出GYWI系统，旨在通过外部知识库为LLM提供可控上下文和灵感溯源。

Method: 本文提出GYWI系统，采用三大核心方法：1）作者中心知识图谱构建与灵感源采样算法，构建外部知识库；2）RAG与GraphRAG混合检索机制，兼顾知识深度与广度，形成混合语境；3）结合强化学习原理的Prompt优化策略，自动指导LLM优化生成结果。

Result: 基于arXiv（2018-2023）构建评估数据集，采用多选题任务实证评估、LLM评分、人类评估及语义空间可视化等综合评价方法，从新颖性、可行性、清晰性、相关性和显著性五个维度进行评测。在GPT-4o、DeepSeek-V3、Qwen3-8B和Gemini 2.5等模型上，GYWI在多个指标如新颖性、可靠性和相关性方面显著优于主流LLM。

Conclusion: GYWI系统通过结合知识图谱与检索增强生成技术，有效解决了科学创意生成中的可控性和可追溯性问题，实验证明该系统能显著提升生成质量，为LLM在学术创新领域的应用提供了可行方案。

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [147] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: 本文提出了FIRE基准，用于全面评估大语言模型的金融理论知识和实际业务场景处理能力。通过金融资格考试题目评估理论水平，通过3000个金融场景问题评估实践能力，并以XuanYuan 4.0为基线模型进行分析，最终公开基准和代码。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在金融领域的应用需要同时具备深厚的理论知识和解决实际业务问题的能力。然而缺乏一个综合性的评估基准来系统性地衡量这两方面能力。为此，作者提出FIRE基准，旨在全面评估LLMs的理论金融知识掌握程度和实际业务场景处理能力，并分析当前模型的边界。

Method: 理论评估：从权威金融资格考试中精选多样化的考试题目，评估模型的深度理解和应用能力；实践评估：构建系统化的评估矩阵，覆盖复杂金融领域、子领域和业务活动，收集3000个金融场景问题（包括带标准答案的封闭式决策问题和按评分标准评估的开放式问题）；评估对象：包括最先进的LLMs和作者团队最新的领域模型XuanYuan 4.0

Result: 对当前先进大语言模型在FIRE基准上的全面评估揭示了它们在金融应用中的能力边界。XuanYuan 4.0作为领域内强基线模型展现了优越性能。基准包含3000个金融场景问题，覆盖理论与实践两个维度。

Conclusion: FIRE基准为金融领域大语言模型的综合评估提供了系统性的解决方案，填补了理论与实践双重评估的空白。通过公开基准问题和评估代码，该工作为未来研究提供了重要资源，有助于推动大语言模型在金融领域的可靠应用和发展。

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [148] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 该论文提出因果嵌入框架，将多个详细因果模型映射至粗粒度因果模型的子系统，推广了传统抽象概念，定义了多分辨率边缘问题，并展示了其在统计边缘问题、因果边缘问题以及合并不同表示形式数据集中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 传统因果模型抽象仅关注两模型间关系，难以应对多个模型向统一粗粒度框架映射的需求。实际应用中，需要整合来自不同表示的多个因果模型数据集，现有方法缺乏系统性框架。

Method: 定义因果嵌入为抽象的泛化，提出广义一致性概念，构建多分辨率边缘问题框架，从理论上阐明其在统计边缘和因果边缘问题中的关联性。

Result: 建立了因果嵌入的理论体系，验证了该框架可用于整合不同表示形式的模型数据集，为多模型融合提供了新范式。

Conclusion: 因果嵌入框架为多分辨率因果模型分析提供了统一理论工具，在因果推断和数据集合并方面具有重要应用价值，拓展了因果模型抽象的研究边界。

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [149] [Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents](https://arxiv.org/abs/2602.22302)
*Varun Pratap Bhardwaj*

Main category: cs.AI

TL;DR: 本文提出Agent Behavioral Contracts (ABC)框架，将Design-by-Contract原则引入自主AI智能体。通过形式化契约C = (P, I, G, R)定义前置条件、不变量、治理策略和恢复机制，提出(p, delta, k)-满足度量化LLM非确定性下的合规性，并证明漂移边界定理：当恢复率γ > 自然漂移率α时，行为漂移被限制在D* = α/γ。实现AgentAssert运行时执行库，在1,980次会话的基准测试中显著提升违规检测能力和约束合规率。


<details>
  <summary>Details</summary>
Motivation: 传统软件依靠API、类型系统和断言等契约规范行为，而AI智能体仅通过提示词和自然语言指令运行，缺乏形式化行为规范。这一根本差距导致AI部署中出现行为漂移、治理失败和项目频繁失败，亟需建立可靠的AI行为保障体系。

Method: ABC框架将契约作为一等公民组件：前置条件(P)约束启动状态，不变量(I)维持运行约束，治理策略(G)管理行为边界，恢复机制(R)处理违规。定义概率性(p, delta, k)-满足度以应对LLM非确定性，证明Drift Bounds Theorem建立漂移与恢复的数学关系，并推导多智能体链的安全组合条件与概率退化边界。

Result: 在AgentContract-Bench的200个场景、7个厂商模型的1,980次会话评估中，契约化智能体每会话检测到5.2-6.8个基线完全遗漏的软性违规(p < 0.0001, Cohen's d = 6.7-33.8)，硬约束合规率达88-100%，漂移期望值D* < 0.27。前沿模型恢复率100%，全模型恢复率17-100%，单次动作执行开销低于10毫秒。

Conclusion: ABC框架通过形式化契约和概率保证，为AI智能体提供了可验证的行为规范体系，有效解决了治理缺失和漂移失控问题。实验证明该框架能显著提升AI系统的可靠性和可治理性，为构建生产级AI应用奠定了重要基础。

Abstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.

</details>


### [150] [Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?](https://arxiv.org/abs/2602.22401)
*Yongjun Zhang*

Main category: cs.AI

TL;DR: 本文探讨AI智能体对社会科学研究的范式转变，提出"氛围研究"概念，通过scholar-skill插件案例，构建基于可编码性与隐性知识需求的认知任务框架，揭示贯穿研究全流程的认知委托边界，分析AI在效率与方法论上的优势与理论原创性上的局限，最终提出负责任研究的五项原则。


<details>
  <summary>Details</summary>
Motivation: AI智能体（具备多步推理、持久状态、工具访问和专项技能）代表了从先前自动化技术的质变，能够自主执行从设想到投稿的完整研究流程，这要求重新定义人机协作的认知边界。

Method: 提出认知任务分析框架，将研究活动按可编码性和隐性知识需求两个维度分类，以此识别贯穿而非分隔研究各阶段的认知委托边界，并以21项学者技能插件为实证案例。

Result: AI智能体在速度、覆盖范围和方法论脚手架方面表现卓越，但在理论原创性和隐性领域知识方面存在根本局限；委托边界的本质是认知性的，贯穿研究全流程而非存在于阶段之间。

Conclusion: AI智能体将引发研究职业的三重挑战：条件脆弱的增强、分层风险和教育学危机；论文提出五项原则以指导负责任的"氛围研究"实践。

Abstract: AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.

</details>


### [151] [Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus](https://arxiv.org/abs/2602.22408)
*Caroline Ahn,Quan Do,Leah Bakst,Michael P. Pascale,Joseph T. McGuire,Michael E. Hasselmo,Chantal E. Stern*

Main category: cs.AI

TL;DR: 本研究为探究人类抽象推理的认知策略，开发了CogARC（认知抽象与推理语料库），包含75个视觉推理问题。通过对260名参与者进行两项实验，记录其解题行为，发现参与者表现总体良好（准确率约80-90%），困难问题导致更长的思考时间和更多样化的策略。随着任务进行，参与者反应更快但准确率略有下降，表明熟悉任务结构而非提升规则学习能力。即使错误解也常高度趋同，揭示了人类在不确定性下泛化、错误泛化和适应策略的机制。


<details>
  <summary>Details</summary>
Motivation: 人类在抽象推理方面表现出显著的灵活性，能从极少例子中快速学习和应用规则。为探究这种能力背后的认知策略，本研究旨在通过构建适合人类的推理任务环境，深入理解人类如何泛化、错误泛化以及在不确定性下调整策略。

Method: 研究团队创建了CogARC，即认知抽象与推理语料库，这是对原ARC（抽象与推理语料库）的人类适配子集。通过两项实验，共招募260名参与者完成75个抽象视觉推理问题。记录参与者在示例查看、编辑序列和多轮提交过程中的高时间分辨率行为数据，分析其解题策略和表现。

Result: 实验结果显示，参与者总体表现良好（实验1准确率约90%，实验2约80%），但问题间和参与者间差异显著。困难问题引发更长的思考时间和更发散的解决策略。任务过程中，参与者反应速度加快但准确率轻微下降，暗示对任务结构的熟悉度提升而非规则学习能力增强。重要的是，即使错误解也常高度趋同，部分解题轨迹直接高效，另一些则经历广泛探索或部分重启后才收敛。

Conclusion: 该研究表明，CogARC为研究人类抽象推理提供了丰富的行为实验环境，揭示了人类如何泛化、错误泛化及在不确定性下适应策略的认知机制，为理解人类智能的核心能力提供了重要见解。

Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.

</details>


### [152] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 本研究构建概率框架分析异构智能体学习自我评估可靠性并选择性弃权的集体决策准确率。通过校准阶段更新能力信念、置信度门控决定是否投票，将孔多塞陪审团定理推广至序贯场景，为非渐近群体成功概率提供下界保证，并在大语言模型幻觉缓解等AI安全领域展现应用潜力。


<details>
  <summary>Details</summary>
Motivation: 经典 epistemic voting 理论（如孔多塞陪审团定理）假设固定参与率，而现实决策中允许智能体弃权可显著提升聚合效果。现有工作尚未充分探索智能体在序贯学习自身能力后选择性参与对集体准确性的影响，且缺乏将其应用于AI安全（特别是缓解大模型集体决策幻觉）的系统性框架。

Method: 提出两阶段概率框架：1）校准阶段，智能体通过贝叶斯更新学习自身固定能力水平；2）置信度门控阶段，基于后验信念阈值决定是否投票。理论层面推导非渐近成功概率下界；经验层面通过蒙特卡洛模拟验证界有效性。

Result: 推导出选择性参与下群体决策成功概率的非渐近下界，证明该机制可将孔多塞陪审团定理的渐近保证推广至序贯置信度门控场景。蒙特卡洛模拟结果验证了理论界的紧致性与实用性。

Conclusion: 该框架拓展了集体智慧的理论边界，揭示了智能体自我评估与选择性参与对提升群体可靠性的关键作用，为AI安全领域（尤其是缓解大语言模型集体决策中的幻觉现象）提供了新的方法论视角。

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [153] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 本文系统分析潜在推理的内部机制，揭示两大关键问题：1）普遍存在的捷径行为使模型无需依赖潜在推理即可获得高准确率；2）潜在推理并非实现广度优先搜索式的结构化探索，而是隐式剪枝与压缩。研究进一步发现监督强度存在根本权衡：强监督抑制捷径但限制假设多样性，弱监督则增强表示丰富性却加剧捷径行为。


<details>
  <summary>Details</summary>
Motivation: 潜在推理虽在性能上表现优异，但其内部工作机制尚未得到充分探究。本文旨在通过系统性分析，深入理解潜在表示在推理过程中的真实作用与行为模式，为方法改进提供理论基础。

Method: 采用跨监督层级的综合分析框架，通过控制变量实验观察捷径行为，并实证检验潜在空间支持广度优先搜索式探索的理论假设。

Result: 1）发现跨方法的普遍捷径现象，模型可绕过潜在推理直接预测；2）潜在表示虽编码多假设，但推理过程未实现结构化搜索，呈现隐式剪枝与压缩；3）揭示监督强度与表示多样性间的负相关权衡关系。

Conclusion: 潜在推理的机制比理论预期更复杂，其并非执行显式结构化搜索。实际应用中需在监督强度与表示能力间取得平衡，以同时保证推理可靠性与假设多样性。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [154] [A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines](https://arxiv.org/abs/2602.22442)
*Gaoyuan Du,Amit Ahlawat,Xiaoyang Liu,Jing Wu*

Main category: cs.AI

TL;DR: 针对Agent-based AutoML系统缺乏中间决策评估的问题，本文提出非侵入式评估智能体（EA），从决策有效性、推理一致性、模型质量风险及反事实影响四个维度评估决策质量。实验表明EA能以0.919 F1分数检测错误决策，识别独立于结果的推理不一致性，并量化决策影响（-4.9%至+8.3%），揭示了传统结果评估所隐藏的失效模式。


<details>
  <summary>Details</summary>
Motivation: 现有Agent-based AutoML系统依赖LLM进行复杂多阶段决策，但评估实践仍停留在结果导向，仅关注最终任务性能。通过文献回顾发现，尚无被调研系统提供面向中间决策质量的结构化评估指标，导致决策过程不透明、不可靠且难以治理，形成关键的评估盲区。

Method: 设计评估智能体（Evaluation Agent, EA）作为被动观察者，在不干扰AutoML智能体执行的前提下，对其中间决策进行决策为中心的评估。EA从四个维度展开评估：决策有效性、推理一致性、超越准确性的模型质量风险、以及反事实决策影响，旨在实现决策可审计、系统可治理的评估框架。

Result: 四个概念验证实验表明：1）EA检测错误决策的F1分数达0.919；2）能够独立于最终结果识别推理不一致性；3）成功将下游性能变化归因于具体决策，揭示决策对最终指标的影响范围为-4.9%至+8.3%。这些结果证明决策中心评估能暴露结果导向指标无法发现的失效模式。

Conclusion: 本研究将Agentic AutoML系统的评估范式从结果导向重构为决策审计，为构建可靠、可解释且可治理的自主ML系统奠定了基础。EA框架通过评估中间决策质量，揭示了传统评估方法掩盖的系统性缺陷，为未来智能体系统的可解释性与可靠性研究提供了新方向。

Abstract: Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\% to +8.3\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.

</details>


### [155] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 本文提出对比世界模型（CWM），通过InfoNCE对比损失函数和困难负样本挖掘，改进大语言模型在embodied agent中的动作可行性评分能力。在ScienceWorld基准测试中，CWM相比监督微调（SFT）在最小编辑负样本的Precision@1上提升6.76个百分点，AUC-ROC达到0.929，并在分布外压力下保持更安全的动作排序。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调（SFT）方法在训练动作评分器时独立处理每个候选动作，无法显式教会模型区分物理上可行和微妙错误动作之间的差异，这成为embodied agent规划推理流程的关键瓶颈。

Method: 采用InfoNCE对比学习目标函数，对大语言模型进行微调，通过挖掘困难负样本（语义相似但物理不兼容的动作），在评分空间中将有效动作与无效动作推离，特别强调对困难负样本的区分。

Result: 在ScienceWorld基准的605个困难负样本测试对上，CWM在最小编辑负样本的Precision@1上比SFT高出6.76个百分点，AUC-ROC为0.929 vs 0.906。在分布外压力测试中，CWM的安全边际为-2.39，显著优于SFT的-3.96，表明其能更准确地将正确动作排在前面。

Conclusion: 对比训练能诱导模型学习到更忠实反映物理可行性的表示，相比单纯的监督微调，CWM显著提升了动作可行性评分的准确性和鲁棒性。

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [156] [ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465)
*Joseph Tso,Preston Schmittou,Quan Huynh,Jibran Hutchins*

Main category: cs.AI

TL;DR: 本文提出了 ConstraintBench，一个评估大语言模型直接求解约束优化问题的基准测试，发现可行性是主要瓶颈，模型在满足约束方面表现不佳，尽管可行解接近最优值。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估大语言模型能否将优化问题转化为求解器代码，但尚不清楚模型能否在不依赖求解器的情况下直接生成正确的约束优化问题解。

Method: 该研究构建了 ConstraintBench，涵盖10个运筹学领域的200个任务，每个任务以自然语言描述场景、约束和目标。模型需输出结构化解决方案，并由 Gurobi 求解器验证约束满足情况和目标值最优性。

Result: 评估六个前沿模型发现：最佳模型约束满足率仅为65.0%，可行解平均达到 Gurobi 最优目标的89%-96%，但联合满足可行性和最优性（误差<0.1%）的比例未超过30.5%。不同领域难度差异显著（生产混合领域可行性83.3%，而人员排班领域仅0.8%），主要失败模式包括持续时间约束误解、实体幻觉以及设施选址和车辆路径问题中的可行-最优解耦。

Conclusion: 该研究揭示了当前大语言模型在直接解决约束优化问题上面临的可行性挑战，并计划开源 ConstraintBench 及评估基础设施以推动后续研究。

Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

</details>


### [157] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 本研究利用大语言模型(LLM)对AI与生命周期评估(LCA)交叉研究进行系统性综述，识别当前趋势与未来方向，揭示AI技术在该领域的快速应用增长及与LCA各阶段的显著关联，并展示了LLM辅助方法在大规模可复现综述中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管AI融入LCA的研究近年来快速发展，但该领域仍缺乏全面且系统的综合性综述，现有研究未能充分捕捉整体趋势与细微主题模式。

Method: 通过整合基于大语言模型的文本挖掘技术与传统文献综述方法，对已发表的AI-LCA研究进行详细分析，识别研究趋势、新兴主题及未来方向。

Result: 研究发现：1) LCA研究中AI技术采用率呈指数级增长；2) 研究范式明显转向LLM驱动方法；3) 机器学习应用持续增加；4) AI方法与对应LCA阶段存在统计学显著相关性；5) 成功构建了能同时捕捉宏观趋势与微观主题概念的有效动态框架。

Conclusion: 本研究表明LLM辅助方法能够支持大规模、可复现的跨领域综述，为计算高效的LCA提供新路径，并帮助从业者将前沿工具及时整合到环境评估中，从而提升可持续决策的科学性与质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [158] [Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents](https://arxiv.org/abs/2602.22523)
*Ryan Liu,Dilip Arumugam,Cedegao E. Zhang,Sean Escola,Xaq Pitkow,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 该立场论文提出从认知模型和AI算法中寻找设计模块化语言智能体的蓝图，以解决单一LLM的局限性。作者形式化定义了智能体模板概念，并通过调研现有语言智能体提炼其背后的认知/AI模板。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽能力日益增强，但单个模型仍面临诸多难题。对于需要多模型协作的任务，如何有效组合多个LLM尚不明确。现有认知科学和人工智能算法文献中蕴含着丰富的模块化设计经验，可为语言智能体架构提供理论指导。

Method: 首先形式化定义智能体模板概念，明确指定单个LLM的角色分工与功能组合机制；随后系统调研现有语言智能体文献，逆向工程式地提炼其底层所采用的认知模型或AI算法模板。

Result: 研究发现现有语言智能体中已存在多样化的模板设计，这些模板直接源于认知模型（如心智理论）或AI算法（如多智能体系统、强化学习），为构建高效、可解释的模块化语言智能体提供了实用框架。

Conclusion: 认知科学与AI启发的智能体模板是开发有效且可解释语言智能体的强大设计工具，应成为未来研究的重要方向，以推动模块化语言智能体的系统化设计与应用。

Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.

</details>


### [159] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: 针对大型语言模型在专业领域缺乏长尾知识的问题，提出AHCE框架，通过可学习策略将人类专家作为交互式推理工具，在《我的世界》任务中实现成功率提升32%-70%，证明有效的人机协同需超越简单求助而学会精准调用专家推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽擅长通用推理，但在训练数据缺失的长尾专业知识领域表现欠佳。人类专家虽可补充该知识，但其指导往往非结构化且不可靠，难以直接整合进智能体规划。因此需要一种能主动、按需且结构化地整合人类专业知识的协同框架。

Method: 提出主动人机增强挑战参与框架(AHCE)，核心为人类反馈模块(HFM)。该模块通过可学习策略将人类专家建模为交互式推理工具，智能体主动判断何时及如何请求专家知识，而非被动或简单求助。框架在《我的世界》游戏环境中进行大量实验验证。

Result: 在《我的世界》任务中，该框架显著提升任务成功率：普通难度任务提升32%，高难度任务提升近70%。同时保持人类干预最小化。实验结果证明学习如何精准请求专家推理比简单求助更有效。

Conclusion: 成功增强智能体性能的关键在于学习如何适时、适当地请求专家推理，而不仅是泛化求助。AHCE框架为专业领域的人机协同提供了新范式，通过结构化、主动的交互机制实现高效知识整合。

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [160] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: 本文提出CourtGuard框架，通过检索增强的多智能体辩论机制解决大语言模型安全机制的适应性僵化问题，在7项安全基准测试中表现优异，并支持零样本适应与自动化数据审计。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全机制依赖静态微调的分类器，存在适应性僵化问题，无法在不重新训练的情况下执行新的治理规则。

Method: 提出CourtGuard，一种检索增强的多智能体框架，将安全评估重构为"证据辩论"。通过基于外部政策文档的对抗性辩论实现安全防护，无需微调。

Result: 在7项安全基准测试中达到先进性能；零样本适应能力可在维基百科破坏任务中达到90%准确率；能自动筛选和审计九个新型对抗攻击数据集。

Conclusion: 将安全逻辑与模型权重解耦，为应对当前和未来AI治理的监管要求提供了一种稳健、可解释且可适应的解决方案。

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [161] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 本文揭示了数学推理中示例引导不稳定的根本原因在于策略使用与可执行性之间的差距，并提出选择性策略检索（SSR）框架。该框架通过显式建模可执行性，利用经验性、多路径、源感知信号选择性地检索和组合策略，在AIME25和Apex等基准测试上实现高达+13点和+5点的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 尽管示例引导能提升数学推理性能，但其效果极不稳定，即便引导内容正确且与问题相关。这种不稳定性暴露了一个被忽视的问题：策略在成功解法中的出现频率（策略使用）与其作为模型引导时的实际效果（策略可执行性）之间存在系统性差异，现有方法未能有效建模这一差距。

Method: 通过受控分析配对的人工解法与模型生成解法，识别出人工与模型衍生策略在结构化、领域依赖方式上的系统性差异，导致互补优势与源依赖的引导效果逆转。基于此，提出SSR测试时框架，利用经验信号显式建模策略可执行性，实现策略的选择性检索与组合。

Result: SSR在多个数学推理基准上显著优于基线方法：相比直接求解、上下文学习和单源引导，在AIME25上提升准确率最高达+13点，在Apex上提升+5点，为紧凑型推理模型提供了稳定可靠的性能增益。

Conclusion: 通过明确区分策略使用与可执行性并显式建模后者，SSR框架有效解决了示例引导的不稳定性问题，为数学推理中的测试时引导提供了更鲁棒的方法论，显著提升了模型性能。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [162] [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585)
*Jodi M. Casabianca,Maggie Beiting-Parrish*

Main category: cs.AI

TL;DR: 这篇论文将心理测量学的评分者模型整合到AI流程中，通过项目反应理论（特别是多层面Rasch模型）来校正评分者严厉度和趋中效应等系统误差，从而提高人类评判AI模型的可靠性和效度。研究以OpenAI摘要数据集为例，展示了校正评分者偏差后能获得更准确的摘要质量评估和对评分者表现诊断性洞察，为人机协同评估提供了更透明、稳健的方法论。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型训练与评估中依赖的人类评判数据很少被视为存在系统误差的测量数据。评分者效应（如严厉度差异和趋中倾向）会扭曲观测评分，影响对AI模型输出的真实质量判断，导致开发决策基于不可靠的原始评分。

Method: 本文采用项目反应理论中的评分者模型，特别是多层面Rasch模型，将真实输出质量与评分者行为分离。通过心理测量学方法识别并校正评分者严厉度和趋中效应等常见评分者效应。

Result: 在OpenAI摘要数据集上的实证研究表明，校正评分者严厉度后能够得到修正后的摘要质量估计值，并提供评分者表现的诊断性洞察。相比原始评分，调整后的分数更能反映真实的输出质量。

Conclusion: 将心理测量学建模融入人机协同评估流程，能够实现更原则性和透明化的人类数据使用，使开发者基于校正后分数而非易出错的原始评分做决策。这为AI开发与评估提供了通往更稳健、可解释和构念一致实践的道路。

Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.

</details>


### [163] [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: 本文提出SideQuest，一种利用大型推理模型自身进行KV缓存压缩的创新方法。该方法通过将压缩任务作为并行辅助任务，推理上下文中token的重要性，从而在保持多步推理能力的同时，显著降低长运行智能体任务的内存占用。


<details>
  <summary>Details</summary>
Motivation: 长运行智能体任务（如深度研究）需要跨多个网页和文档进行多跳推理，导致LLM上下文被外部检索token主导，内存使用快速膨胀并限制解码性能。现有KV缓存压缩启发式方法无法有效适配多步推理模型的需求。

Method: SideQuest的核心是让大型推理模型（LRM）自身通过推理判断上下文中各token对任务的有用性来实现KV缓存压缩。关键创新在于将该压缩过程建模为与主任务并行执行的辅助任务，从而避免管理过程相关token污染模型的记忆空间。

Result: 在仅用215个样本训练的情况下，SideQuest在智能体任务上实现了高达65%的峰值token使用量减少，精度下降极小，性能显著优于基于启发式的KV缓存压缩技术。

Conclusion: 研究表明，利用LRM自身推理能力进行KV缓存压缩是可行且高效的，为长上下文智能体任务提供了新的内存优化范式，能够在保持推理准确性的同时大幅降低计算资源消耗。

Abstract: Long-running agentic tasks, such as deep research, require multi-hop reasoning over information distributed across multiple webpages and documents. In such tasks, the LLM context is dominated by tokens from external retrieval, causing memory usage to grow rapidly and limiting decode performance. While several KV cache compression techniques exist for long-context inputs, we find that existing heuristics fail to support multi-step reasoning models effectively. We address this challenge with SideQuest -- a novel approach that leverages the Large Reasoning Model (LRM) itself to perform KV cache compression by reasoning about the usefulness of tokens in its context. To prevent the tokens associated with this management process from polluting the model's memory, we frame KV cache compression as an auxiliary task executed in parallel to the main reasoning task. Our evaluations, using a model trained with just 215 samples, show that SideQuest reduces peak token usage by up to 65% on agentic tasks with minimal degradation in accuracy, outperforming heuristic-based KV cache compression techniques.

</details>


### [164] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: 本文提出AHBid，一种用于在线广告自动出价的可适应分层竞价框架，通过融合生成式规划与实时控制来解决多渠道场景下的预算分配问题。该方法利用扩散模型作为高层生成规划器捕获历史上下文，结合约束执行机制和轨迹细化机制，以及基于控制理论的低层竞价算法，在离线和在线实验中均取得显著效果，整体回报率提升13.57%。


<details>
  <summary>Details</summary>
Motivation: 在线广告环境复杂且动态变化，尤其多渠道场景下预算与约束的跨渠道分配对ROI至关重要。现有优化方法缺乏动态适应性，而强化学习方法难以在马尔可夫决策过程框架内有效捕获历史依赖关系和观测模式，因此需要一种更灵活、能同时利用历史知识和实时信息的混合解决方案。

Method: AHBid采用分层架构：高层为基于扩散模型的生成式规划器，负责动态分配预算与约束，有效捕捉历史上下文和时间模式；中层设计约束执行机制确保合规性，并引入轨迹细化机制利用历史数据提升环境适应性；底层采用基于控制理论的竞价算法，协同融合历史知识与实时信息，实现高效出价决策。

Result: 在大规模离线数据集和在线A/B测试中，AHBid相比现有基线方法实现了13.57%的整体回报率提升，证明了该框架在多渠道竞价场景下的有效性和优越性。

Conclusion: 通过将生成式AI技术与控制理论相结合，AHBid成功解决了传统方法在动态广告环境中适应性差和历史依赖建模不足的双重挑战，为多渠道自动竞价提供了一种新的混合范式，具有显著的实践价值。

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [165] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: 该论文提出RLHFless，首个基于无服务器计算的同步RLHF训练框架，通过动态资源适配、共享前缀预计算和成本感知的actor缩放策略，解决传统RLHF训练中的资源浪费问题，实现了1.35倍加速和44.8%成本降低。


<details>
  <summary>Details</summary>
Motivation: RLHF在提升大语言模型性能方面至关重要，但存在训练与推理共存导致资源需求动态变化、模型规模扩大带来效率挑战、现有框架依赖有服务器基础设施难以应对细粒度资源波动，以及同步训练中组件间/内部空闲时间造成资源浪费等问题。

Method: 构建基于无服务器计算的RLHFless框架：1）采用动态资源适配机制应对RLHF流水线中的资源需求变化；2）预计算共享前缀避免重复计算；3）设计成本感知的actor缩放策略，考虑响应长度变化以优化成本与速度平衡；4）实施高效工作负载分配减少函数内部不平衡和空闲时间。

Result: 在物理测试平台和大型模拟集群上的实验表明，RLHFless相比现有最优基线框架实现了高达1.35倍的训练速度提升和44.8%的成本削减。

Conclusion: RLHFless证明了无服务器计算在同步RLHF训练中的可行性，通过创新的资源管理和计算优化策略，显著提升了训练效率并降低了成本，为大规模RLHF训练提供了可扩展的解决方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [166] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: 针对推荐系统数据稀疏与冷启动问题，本文提出数据驱动的Taesar框架，通过对比解码将跨域信息自适应注入目标序列，使标准模型无需复杂架构即可学习跨域依赖，有效规避负迁移并显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 推荐模型性能受限于训练数据质量，数据稀疏与冷启动是核心挑战。现有研究利用多辅助域数据增强信息，但域间差异导致负迁移。传统模型中心化范式依赖定制化复杂架构，难以捕获跨域非结构化序列依赖，泛化性差且计算开销大。

Method: 提出Taesar（Target-aligned Sequential Regeneration）数据驱动框架，采用对比解码机制，自适应编码跨域上下文至目标域序列。通过数据层面的序列再生而非模型层面的复杂融合，使标准序列模型可直接学习跨域复杂依赖。

Result: 实验表明，Taesar性能优于模型中心化方案，且可泛化至多种序列推荐模型。生成的增强数据集有效结合了数据与模型双中心范式的优势。

Conclusion: Taesar成功实现了一种新型数据增强范式，通过目标对齐的序列再生解决跨域负迁移问题，为标准推荐模型提供了高效性能提升方案，推动了数据-模型协同优化方向的发展。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [167] [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758)
*Satya Borgohain,Roy Mariathas*

Main category: cs.AI

TL;DR: 本研究分解HealthBench医疗AI评估数据集中的医生分歧来源，发现81.8%的方差源于无法解释的案例层面残余因素，医生身份（2.4%）和评分标准身份（3.6-6.9%）的解释力有限。可减少不确定性（缺失上下文、表述模糊）使分歧几率倍增（OR=2.55），而不可减少的医学固有模糊性无影响。研究揭示了医疗AI评估存在结构性一致性上限，但改善信息缺口可降低可减少性分歧。


<details>
  <summary>Details</summary>
Motivation: 理解医疗AI评估中医生间分歧的方差来源及可解释特征，以识别降低分歧的可行策略并提升评估可靠性。

Method: 采用方差分解分析HealthBench数据集，检验评分标准身份、医生身份、案例元数据、规范语言、医学专科、表面特征及嵌入向量等因素对医生分歧的解释能力，并通过逻辑回归分析可减少与不可减少不确定性的影响。

Result: 评分标准身份仅解释3.6-6.9%的分歧方差，医生身份仅占2.4%；81.8%的案例层面残余方差无法被元数据、专科类别、表面特征（AUC=0.58）或嵌入向量（AUC=0.485）显著降低。分歧与输出质量呈倒U型关系（AUC=0.689），边界案例分歧最大。可减少不确定性使分歧几率增加2.55倍（p<10^(-24)），而不可减少不确定性无显著效应（OR=1.01, p=0.90）。

Conclusion: 医疗AI评估的一致性主要受结构性因素限制，但可减少性不确定性的识别表明，通过优化评估场景的信息完整性可降低分歧，为设计更可靠的医疗AI评估框架提供了可行改进方向。

Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.

</details>


### [168] [ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making](https://arxiv.org/abs/2602.22771)
*Yusuke Watanabe,Yohei Kobashi,Takeshi Kojima,Yusuke Iwasawa,Yasushi Okuno,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本研究针对临床决策中信息不完整的情境，开发ClinDet-Bench基准测试评估大语言模型判断信息充分性的能力。研究发现现有LLM在信息缺失时无法准确识别可确定性，出现过早判断与过度回避并存的问题，尽管其在信息完整时表现良好且能正确解释评分规则。


<details>
  <summary>Details</summary>
Motivation: 临床决策常在信息不完整时做出，专家需判断信息是否充分。过早结论与不必要回避均危及患者安全，而现有研究未能充分评估大语言模型在此关键安全能力上的表现。

Method: 基于临床评分系统构建ClinDet-Bench基准，将信息不完整场景分解为可确定与不可确定两类条件。识别可确定性需穷尽关于缺失信息的所有假设（包括低概率假设），并验证结论是否在所有假设下均成立。

Result: 近期大语言模型在信息不完整时无法准确识别可确定性，表现呈现"过早判断"与"过度回避"并存的缺陷。尽管模型能正确解释评分知识且在信息完整时性能良好，但其在不确定性处理上存在显著不足。

Conclusion: 现有基准不足以评估临床环境中LLM的安全性。ClinDet-Bench提供了可确定性识别能力的评估框架，促进恰当决策回避，适用于医学及其他高风险领域，并已开源。

Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.

</details>


### [169] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: 本文提出FlexMS基准测试框架，用于评估和构建质谱预测的深度学习模型，解决缺乏标准评估体系的问题，并提供关键性能影响因素的实践指导。


<details>
  <summary>Details</summary>
Motivation: 实验质谱数据缺乏阻碍了分子鉴定，现有深度学习模型在质谱预测中因方法异构且缺乏明确定义的基准而难以进行整体评估。

Method: 创建FlexMS基准测试框架，支持动态构建多种模型架构组合，并使用不同指标在预处理后的公开数据集上进行性能评估。

Result: 提供了影响性能的关键因素洞察，包括数据集结构多样性、学习率等超参数、数据稀疏性、预训练效果、元数据消融设置和跨域迁移学习分析，同时包含模拟实际鉴定场景的检索基准。

Conclusion: FlexMS为选择合适的质谱预测模型提供了实用指导，推动了计算质谱预测领域的研究标准化和实践应用。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [170] [DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation](https://arxiv.org/abs/2602.22839)
*Hao Zheng,Guozhao Mo,Xinru Yan,Qianhao Yuan,Wenkai Zhang,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: DeepPresenter是一个用于生成演示文稿的智能体框架，它通过环境感知的反思机制和自主的幻灯片中间产物规划、渲染与修订，实现了超越固定模板的多样化用户意图适应和长周期迭代优化，在评估集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有演示文稿生成智能体过度依赖预定义工作流和固定模板，无法实现深度内容研究、连贯视觉设计和基于观察的迭代优化。这促使作者开发一个更灵活、可适应多样化用户意图、支持反馈驱动改进的框架。

Method: 提出DeepPresenter智能体框架，其核心包括：1）自主规划、渲染和修订幻灯片中间产物以支持长周期优化；2）环境感知的反思机制，基于渲染幻灯片等感知状态而非内部信号进行生成过程调整，识别并修正演示文稿特定问题。

Result: 在覆盖多样化演示文稿生成场景的评估集上，DeepPresenter达到最先进性能，且经过微调的9B模型在显著降低成本的同时仍保持高度竞争力。

Conclusion: DeepPresenter框架通过环境感知反思和中间产物迭代，有效解决了模板化演示文稿生成的局限性，为智能演示文稿生成提供了新范式，具有高效能和低成本的优势。

Abstract: Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent

</details>


### [171] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: 本研究通过人类-AI协作发现Hermite求积规则的新型误差表示与界，实证表明AI能加速数学发现，但需严格人工验证与领域专业知识。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能能否真正贡献于创造性数学研究，而非仅自动化常规计算并引入错误风险，需提供实证证据。

Method: 通过详细案例研究：与多个AI助手协作，系统性地将Hermite求积规则的误差结果扩展到手工工作之外，形成并证明多个定理；以异常透明的方式记录完整研究工作流程。

Result: AI擅长代数运算、系统证明探索、文献综述与LaTeX排版，但每一步均需严格人工验证、数学直觉与策略指导；揭示了成功协作的模式与需预见的失败模式。

Conclusion: 若配合适当的怀疑态度与验证协议，AI工具可有效加速数学发现，但仍要求严格的人工监督与深厚的领域专业知识。

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [172] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出了OmniGAIA基准和OmniAtlas智能体，旨在突破现有多模态大模型仅限于双模态交互的局限，通过全模态事件图方法和事后引导树探索策略，构建能处理视频、音频、图像跨模态推理与工具使用的通用AI助手。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型主要局限于视觉-语言等双模态交互，缺乏人类智能中全模态感知与复杂推理工具使用的统一认知能力，无法胜任通用AI助手的角色。

Method: 提出OmniGAIA基准，采用全模态事件图方法从真实世界数据合成需要跨模态推理和外部工具集成的复杂多跳查询；同时开发OmniAtlas智能体，通过事后引导树探索策略生成训练轨迹，并采用OmniDPO进行细粒度错误校正，实现工具集成推理范式和主动全模态感知。

Result: OmniAtlas有效增强了现有开源模型的工具使用能力，在全模态任务评估中展现出优越性能，为构建下一代通用全模态AI助手奠定了基础。

Conclusion: 该工作标志着向真实世界场景下的原生全模态AI助手迈出了重要一步，为未来通用人工智能的发展提供了新的技术路径和评估基准。

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


### [173] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 本文提出系统性评估通用智能体的框架Exgentic与统一协议，对5种主流智能体在6个环境中进行基准测试并发布首个开放通用智能体排行榜，证明通用智能体无需领域调优即可实现与领域专用智能体相当的性能。


<details>
  <summary>Details</summary>
Motivation: 通用智能体仍未能实现真正突破，现有智能体多为专用系统，且缺乏针对通用智能体的系统性评估方法。当前基准测试均基于领域集成设计，无法公平评估通用智能体的跨领域能力。

Method: 提出通用智能体评估的概念性原则、统一集成协议以及Exgentic实践框架；选取5个主流智能体实现与6个不同环境进行交叉基准测试，构建首个开放通用智能体排行榜。

Result: 实验表明通用智能体能够在多样化环境中实现泛化，其性能与领域专用智能体相当，且无需任何针对特定环境的参数调优。

Conclusion: 通过开源评估协议、框架与排行榜，为通用智能体的系统性研究建立基础基准，推动该领域标准化发展。

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [174] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: 针对扫描探针显微镜(SPM)领域，提出SPM-Bench博士级多模态基准，通过全自动数据合成管道解决数据污染、复杂度不足和成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 大模型在通用推理取得突破，但专业科学领域能力评估存在数据污染、复杂度不足和高人力成本的瓶颈。

Method: 采用锚定门控筛选(AGS)技术从2023-2025学术文献提取图文对；云-本地混合架构中VLM仅返回'llbox'坐标实现本地裁剪；提出严格缺陷惩罚F1(SIP-F1)评估指标。

Result: SIP-F1量化模型"人格"(保守/攻击/赌徒/智慧型)，揭示AI在复杂物理场景的真实推理边界。

Conclusion: SPM-Bench建立了自动化科学数据合成的可推广范式，为专业领域大模型评估提供新基准。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [175] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 本研究针对临床AI人机验证缺乏结构化分析的问题，提出诊断对齐框架，通过四层一致性评估发现：二元词汇匹配严重低估临床对齐度，专家验证的结构化转换量化可实现临床决策支持系统的可追溯评估。


<details>
  <summary>Details</summary>
Motivation: 在安全关键临床AI领域，人机协同验证虽不可或缺，但AI推断与专家修正间的转换过程极少被作为结构化信号分析，制约了对临床决策支持系统真实效能的精确量化。

Method: 研究构建诊断对齐框架，将AI生成影像报告作为不可变推断状态并与医师验证结果系统比对。技术流程集成视觉大语言模型、BERT医疗实体提取及顺序语言模型推断(SLMI)三阶段，确保专家审查前领域一致性。基于21例皮肤病完整AI-医师配对数据，采用精确主匹配率、语义相似度调整率、跨类别对齐和综合一致性率四层次评估框架。

Result: 精确一致率71.4%且语义相似度调整后无显著变化(t=0.60)，跨类别分析显示100%综合一致性(95% CI: [83.9%, 100%])，无完全诊断分歧。证明二元词汇评价严重低估临床意义对齐。

Conclusion: 将专家验证建模为结构化变换可实现修正动态的信号感知量化，为图像式临床决策支持系统提供可追溯的人机对齐评估，建立更全面的临床AI验证分析框架。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [176] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: 该论文提出了一种名为RepSPD的几何深度学习模型，用于脑电图(EEG)解码。该模型通过在黎曼流形上实现交叉注意力机制来调制对称正定(SPD)矩阵的几何属性，并引入全局双向对齐策略来缓解曲率导致的几何畸变，从而显著提升EEG表征性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于对称正定(SPD)矩阵的几何学习方法在EEG解码中过度关注统计聚合，忽略了频率特异性同步和脑区局部拓扑结构，限制了其在神经科学和临床应用中的表现。

Method: 1. 在黎曼流形上设计交叉注意力机制，将图源功能连接特征与SPD矩阵的几何属性进行调制融合；2. 提出全局双向对齐策略重塑切空间嵌入，减轻曲率引起的几何畸变，增强几何一致性。

Result: 大量实验表明，该框架在EEG表征任务中显著优于现有方法，展现出更优越的鲁棒性和泛化能力。

Conclusion: RepSPD模型通过结合黎曼几何一致性与功能连接特征，有效解决了当前SPD方法的局限性，为EEG解码提供了更强大且可靠的几何深度学习解决方案。

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [177] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 该论文针对大语言模型的安全漏洞问题，提出CC-BOS框架，利用古汉语的简洁性和模糊性特点，结合多维果蝇优化算法自动生成对抗性提示，实现高效的自动化越狱攻击。实验表明该方法显著优于现有攻击手段。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临越狱攻击的安全风险，且攻击效果因语言而异。古汉语因其简洁晦涩的特性可绕过现有安全约束，暴露出模型显著漏洞，这为开发新型自动化越狱攻击方法提供了研究动机。

Method: 提出CC-BOS框架，基于多维果蝇优化算法自动生成古汉语对抗提示。方法核心包括：将提示编码为角色、行为、机制、隐喻、表达、知识、触发模式和上下文八个策略维度；通过气味搜索、视觉搜索和柯西变异进行迭代优化；并设计古汉语-英语翻译模块提升可读性与评估准确性。该框架适用于黑盒攻击场景。

Result: 大量实验证明，CC-BOS框架在越狱攻击效果上持续优于现有最先进攻击方法，验证了古汉语对抗提示的有效性。

Conclusion: CC-BOS成功实现了基于古汉语的自动化高效越狱攻击，揭示了大语言模型在古典语言处理中的安全脆弱性，为模型安全防护提供了重要参考。

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [178] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 提出多智能体强化学习框架优化F1赛事策略，引入交互模块和自我博弈训练，智能体学会在竞争中平衡能量、轮胎、空气动力及进站决策，仅用真实比赛信息即可支持策略师决策。


<details>
  <summary>Details</summary>
Motivation: F1策略需动态适应赛况与对手，传统优化方法难以处理多维度复杂决策。现有强化学习研究多限于单智能体，忽略竞争交互。本研究旨在构建多智能体学习框架，模拟真实对抗环境，实现更有效的策略优化。

Method: 在预训练单智能体策略基础上，引入交互模块建模对手行为，结合自我博弈训练机制。框架涵盖能量、轮胎、空气动力及进站等多维决策空间，仅使用真实比赛可观测信息进行端到端训练。

Result: 智能体展现出针对对手的自适应能力，能动态优化进站时机、轮胎选择和能量分配，在多种场景下保持鲁棒且一致的比赛表现。通过相对性能评估，生成策略兼具竞争性与稳定性。

Conclusion: 该框架仅依赖真实比赛数据，可直接用于赛前规划与赛中决策支持，为F1策略师提供AI辅助工具。研究验证了多智能体强化学习在竞技体育策略优化中的潜力，为其他竞争性决策问题提供方法参考。

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [179] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 本研究提出了一种名为AILS-AHD的新型算法，通过大语言模型动态生成和优化启发式策略，显著提升了容量约束车辆路径问题的求解性能。


<details>
  <summary>Details</summary>
Motivation: 容量约束车辆路径问题（CVRP）作为NP难组合优化问题，在大规模实例求解时面临巨大计算挑战。传统启发式算法设计依赖人工经验且难以自适应调整，亟需自动化、智能化的方法突破现有性能瓶颈。

Method: 该研究提出了自适应迭代局部搜索结合自动启发式设计（AILS-AHD）框架，将进化搜索与大语言模型集成，实现破坏启发式的动态生成与优化，并设计LLM加速机制提升计算效率。

Result: 在CVRPLib基准测试中，AILS-AHD在中等和大规模实例上均优于现有先进算法（AILS-II和HGS），并在10个大尺度实例中刷新了8个问题的已知最优解纪录。

Conclusion: 实验结果验证了LLM驱动的启发式自动生成方法在车辆路径优化领域的有效性，为组合优化问题的智能化求解开辟了新路径。

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [180] [Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents](https://arxiv.org/abs/2602.23093)
*Dhwanil M. Mori,Neil F. Johnson*

Main category: cs.AI

TL;DR: 本研究通过模拟N个LLM智能体在固定容量C系统中的重复资源请求博弈，揭示了AI群体中涌现的"蝇王"式部落行为。研究发现智能体自发形成攻击型(27.3%)、保守型(24.7%)和机会主义型(48.1%)三类部落，整体决策表现劣于随机抛硬币，且能力更强的智能体反而导致更频繁的系统崩溃，证明了AI群体中的"集体退化"悖论。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地参与管理关键基础设施资源，理解其在竞争性资源请求场景下的涌现行为变得至关重要。本研究旨在探究多个自主AI智能体在有限资源环境中的集体行为模式，特别是部落组织的形成及其对系统性能的意外负面影响。

Method: 研究采用计算实验方法，构建简化框架：N个LLM驱动的智能体在固定容量C的系统中进行多轮独立资源请求决策（每轮请求1单位）。通过大规模模拟观察部落行为的涌现、分类及其系统性能影响，并与随机决策基线进行比较。

Result: 主要发现包括：(1) AI智能体自发形成三类稳定部落：攻击型(27.3%)、保守型(24.7%)、机会主义型(48.1%)，表现出类似人类社会的"蝇王"动力学；(2) 智能体决策未能优化资源利用或减少过载，整体性能显著低于随机决策基线；(3) 更先进的AI模型反而导致更高的系统性崩溃率，呈现能力-性能悖论。

Conclusion: 研究表明，AI智能体在群体互动中可能形成有害的部落身份认同，导致个体智能提升与群体性能下降的悖论现象。这揭示了多AI系统协调中的潜在风险：群体社会动态可能削弱而非增强系统可靠性，对AI安全设计和基础设施自动化管理具有重要警示意义。

Abstract: Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of "Lord of the Flies" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.

</details>


### [181] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: 本研究开发MALLET多智能体情绪脱毒框架，通过情绪分析、调节、监控与个人引导四类智能体协同工作，对刺激性新闻内容进行量化评估与中性化改写。在AG新闻数据集上的实验证实该系统可实现最高19.3%的刺激度削减，同时保持语义完整性，为构建冷静信息接收环境提供可行方案。


<details>
  <summary>Details</summary>
Motivation: 注意力经济时代， sensational化内容通过过度情绪刺激劫持用户注意力，损害理性决策能力。传统信息过滤方法存在访问限制缺陷，本研究旨在设计一种非约束性情绪调节机制，使消费者在保留原文获取权的前提下实现信息冷静接收。

Method: 构建基于大语言模型的多智能体系统(MALLET)，包含：(1)情绪分析智能体采用6维BERT分类器量化文本刺激强度；(2)情绪调节智能体利用LLM生成BALANCED(纯中性化)与COOL(中性化+信息补充)双模式改写文本；(3)平衡监控智能体追踪用户周度信息消费模式并提供个性化建议；(4)个人引导智能体依据用户敏感度特征动态推荐适宜呈现模式。

Result: 基于800篇AG新闻的实证研究表明：系统实现最高19.3%的刺激分数降低，情绪平衡显著改善，且语义保真度不受影响。刺激度降低与语义保留呈近零相关，证明两者解耦可控。领域分析显示体育、商业、科技类内容刺激度降低17.8-33.8%，世界类新闻因事实本身固有高刺激特性而调节空间有限。

Conclusion: MALLET系统验证了通过多智能体LLM架构实现信息情绪脱毒的可行性，在保护信息自由访问权的同时促进用户冷静决策，为注意力经济生态下的健康信息消费提供了创新性技术框架与实证基础。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [182] [The Trinity of Consistency as a Defining Principle for General World Models](https://arxiv.org/abs/2602.23152)
*Jingxuan Wei,Siyuan Li,Yuhang Xu,Zheng Sun,Junjie Jiang,Hexuan Jin,Caijun Jia,Honghao He,Xinglong Xu,Xi bai,Chang Yu,Yumou Liu,Junnan Zhu,Xuanhe Zhou,Jintao Chen,Xiaobin Hu,Shancheng Pang,Bihui Yu,Ran He,Zhen Lei,Stan Z. Li,Conghui He,Shuicheng Yan,Cheng Tan*

Main category: cs.AI

TL;DR: 针对通用世界模型缺乏理论框架的问题，本文提出"一致性三位一体"（模态、空间、时间一致性）作为核心理论，并推出CoW-Bench基准，为AGI世界模型的构建提供理论指导和评估标准。


<details>
  <summary>Details</summary>
Motivation: 构建能学习、模拟和推理客观物理规律的世界模型是AGI的基础性挑战。尽管Sora等视频生成模型展示了数据驱动方法逼近物理动力学的潜力，统一多模态模型提供了集成感知、语言和推理的架构范式，但领域仍缺乏定义通用世界模型必要属性的原则性理论框架。

Method: 提出"一致性三位一体"理论框架：模态一致性作为语义接口，空间一致性作为几何基础，时间一致性作为因果引擎。同时开发CoW-Bench基准，在统一协议下评估视频生成模型和统一多模态模型的多帧推理生成能力。

Result: 通过该框架系统回顾多模态学习演进，揭示从松散耦合专用模块向统一架构发展的轨迹，这些架构使内部世界模拟器的协同涌现成为可能。CoW-Bench基准为评估现有模型提供了标准化工具。

Conclusion: 本研究建立了通向通用世界模型的原则性路径，明确了当前系统的局限性，阐明了未来进展所需的架构要求，为AGI研究提供了理论和实践基础。

Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.

</details>


### [183] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: 提出PATRA模型解决时间序列推理中LLM方法的两大局限：无法捕捉趋势/季节性模式，以及简单任务主导学习的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based方法在处理时间序列推理时存在两个主要局限：一是将时间序列简单视为文本或图像，无法有效捕捉趋势和季节性等关键模式；二是在混合任务训练中，简单目标主导学习过程，阻碍了深度推理能力的发展。这促使作者提出更有效的模型。

Method: 提出PATRA模型，包含两个核心机制：1）模式感知机制，从时间序列中提取趋势和季节性模式以实现深度对齐；2）任务感知平衡奖励机制，协调不同难度任务的学习，激励生成连贯的思维链（Chain of Thought）。

Result: 大量实验表明，PATRA在多样化的时间序列问答（TSQA）任务上优于强基线模型，展现出卓越的跨模态理解和推理能力。

Conclusion: PATRA模型通过引入模式感知和任务平衡机制，有效解决了时间序列推理中的关键挑战，为提升LLM在时间序列分析领域的表现提供了新思路。

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [184] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 提出决策理论的隐写分析框架，通过广义V-信息和隐写间隙量化LLM隐写推理中的信息不对称，实现检测、量化和缓解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型出现隐写能力，可能使未对齐模型逃避监督；传统检测方法需已知参考分布，对LLM隐写推理不适用，缺乏有效检测手段。

Method: 建立决策理论视角：隐写会产生可解码与不可解码智能体间的信息不对称，该不对称可从可观察行为推断。引入广义V-信息作为测量可用信息的功利主义框架，定义隐写间隙量化解码/非解码智能体的下游效用差异。

Result: 实证验证框架有效性，可检测、量化和缓解大型语言模型中的隐写推理行为。

Conclusion: 该决策理论框架为LLM隐写行为检测提供了新范式，通过量化信息不对称增强模型对齐与监督机制。

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [185] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: 本文针对单细胞生物学领域大语言模型评估标准不统一、基准碎片化、指标缺乏生物学可解释性等问题，提出SC-ARENA评估框架。该框架通过虚拟细胞抽象统一评估目标，定义细胞类型注释、描述生成、细胞生成、扰动预测和科学问答五种自然语言任务，并引入基于外部本体、标记数据库与科学文献的知识增强评估方法。实验表明当前模型在复杂生物学任务上表现不均，尤其缺乏机制性理解能力；SC-ARENA能确保生物学正确性、提供可解释证据并具备高判别能力，为开发生物学对齐的基础模型提供方向。


<details>
  <summary>Details</summary>
Motivation: 单细胞生物学中，大语言模型的评估实践存在显著不足：现有基准任务分散、采用与真实场景脱节的多选题分类格式，且依赖缺乏可解释性和生物学基础的指标，无法准确评估模型在真实生物学推理场景中的能力。

Method: 提出SC-ARENA框架：1) 构建虚拟细胞抽象，统一表征细胞内在属性与基因级相互作用；2) 设计五种自然语言任务（细胞类型注释、描述生成、细胞生成、扰动预测、科学问答）以探测核心推理能力；3) 开发知识增强评估，整合外部本体、标记数据库和科学文献，支持生物学忠实且可解释的判断。

Result: 对通用与领域专用大模型的实验分析显示：1) 在虚拟细胞统一评估范式下，当前模型在生物学复杂任务上表现不均，尤其对需要机制性或因果理解的任务表现较差；2) 知识增强评估能确保生物学正确性，提供可解释、基于证据的推理，具备高判别能力，克服了传统指标的脆弱性与不透明性。

Conclusion: SC-ARENA为单细胞生物学大语言模型提供了统一且可解释的评估框架，解决了现有评估体系的局限性，推动了大模型在计算生物学领域的可靠应用，为开发生物学对齐、可泛化的基础模型指明方向。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [186] [Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive](https://arxiv.org/abs/2602.23239)
*Radha Sarma*

Main category: cs.AI

TL;DR: 本文形式化证明基于RLHF的AI系统因优化架构无法满足真正智能体所需的不可通约性与否定性响应条件，故不能接受规范治理。该不兼容性是优化的内在约束，使谄媚、幻觉等失败成为结构性现象，并提出“趋同危机”——人类在指标压力下退化为优化器，丧失规范问责能力，最终给出跨物质基础的智能体架构标准。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在高风险领域（医疗、法律、金融）的部署基于其可被规范治理的假设。本文旨在挑战该假设，通过形式化方法证明优化型AI系统（特别是RLHF大模型）在结构上无法满足规范性治理所需的智能体条件。

Method: 采用形式化分析，界定真正智能体的两个必要且充分的架构条件（不可通约性与否定性响应），进而论证RLHF系统的优化操作（标量统一与最高分选择）与这些条件根本冲突，并分析由此产生的结构性失败模式及“趋同危机”这一次生风险。

Result: 1）RLHF系统与不可通约性和否定性响应结构不兼容；2）谄媚、幻觉、不忠实推理是架构性表现，非可修复缺陷；3）提出“趋同危机”：人类在指标压力下退化为标准检查优化器，消解规范问责主体；4）提供智能体的跨物质架构规范。

Conclusion: 优化型AI系统无法成为真正智能体或接受规范治理，其高风险部署存在根本性缺陷。应将此类系统视为精密工具而非智能体，并警惕监督机制导致的人类规范性能力退化。该研究为区分智能体与复杂工具提供了普适的架构标准。

Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.

</details>


### [187] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 本文提出AIQI（带Q归纳的通用人工智能），这是首个被证明在通用强化学习中渐进ε-最优的无模型智能体。与AIXI等有模型智能体不同，AIQI直接对分布动作价值函数进行通用归纳，在“真相颗粒”条件下同时实现了渐进ε-最优和渐进ε-贝叶斯最优，极大地扩展了通用智能体的多样性。


<details>
  <summary>Details</summary>
Motivation: 在通用强化学习领域，所有已知的最优通用智能体（如AIXI）均为有模型方法，必须显式地维护和利用环境模型。为填补无模型通用智能体的理论空白，丰富该领域的算法多样性，本文旨在构造首个可证明渐进最优的无模型通用智能体。

Method: 本文提出Q-归纳（Q-Induction）方法，创新性地将通用归纳技术应用于分布动作价值函数（而非策略或环境模型），使智能体无需显式构建环境模型即可学习最优行为。

Result: 在“真相颗粒”（grain of truth）假设下，严格证明了AIQI具有强渐进ε-最优性和渐进ε-贝叶斯最优性，这是对无模型通用智能体理论保证的重大突破。

Conclusion: AIQI的成功证明了无模型智能体也能在通用强化学习中达到渐进最优，从而将已知的通用智能体类型从有模型范式扩展至无模型范式，显著丰富了该领域的理论多样性。

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [188] [Mitigating Legibility Tax with Decoupled Prover-Verifier Games](https://arxiv.org/abs/2602.23248)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 针对大模型输出可验证性与准确性之间的权衡问题（即"可读性税"），本文提出解耦正确性与可验证性，通过训练独立的"翻译器"模型将固定求解器的输出转换为可验证形式，在保持答案不变的前提下消除准确性损失。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，其输出需要能被能力较弱的系统验证。现有验证者-证明者博弈虽能提升可验证性，但会导致准确性下降（产生"可读性税"），亟需一种能同时保持高准确性和可验证性的方法。

Method: 提出解耦训练框架：首先独立训练求解器模型以最大化正确性；然后固定求解器，训练翻译器模型将其输出转换为可验证形式；并设计解耦的验证者-证明者博弈，其均衡点对应于忠实且可验证的翻译器。

Result: 理论上，该方法能在保持求解器原始准确性的同时实现输出可验证性，避免了传统方法中的可读性税问题。解耦博弈的均衡对应于既能准确传递答案又能被有效验证的翻译器策略。

Conclusion: 通过解耦正确性与可验证性并引入翻译器模型，本研究为平衡大模型输出的准确性与可验证性提供了新范式，有望在保持高性能的同时实现输出的可验证性，对AI安全具有重要意义。

Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.

</details>


### [189] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: 该论文针对多智能体系统（MAS）中错误信息级联传播的问题，提出了一种无需重训练的推理时修正与剪枝框架AgentDropoutV2，通过在输出层设置“主动防火墙”动态纠错或剔除不可修复内容，显著提升数学任务性能并展现良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有MAS在复杂推理中表现优异，但单个智能体产生的错误信息会引发级联传播，影响整体系统可靠性。当前解决方案依赖僵化的结构工程或昂贵的微调，限制了系统的可部署性和适应性。因此，亟需一种灵活、高效且无需重训练的动态优化方法来阻断错误传播。

Method: 提出AgentDropoutV2，一个推理时的“修正或拒绝”剪枝框架。其核心机制包括：1) 作为“主动防火墙”拦截智能体输出；2) 利用检索增强的修正器，基于由失败模式提炼的指示池进行迭代纠错；3) 对无法修复的输出进行剪枝以防止错误扩散；4) 配备回退策略维持系统完整性。该方法通过失败驱动的指示池实现潜在错误的精准识别。

Result: 在广泛的数学基准测试中，AgentDropoutV2显著提升了MAS的任务性能，平均准确率提高了6.3个百分点。此外，系统展现出强大的泛化能力和适应性：能根据任务难度动态调整修正力度，并利用上下文感知的指示器解决多种错误模式。

Conclusion: AgentDropoutV2是一种有效且灵活的测试时优化框架，它通过动态信息流的修正与剪枝，成功缓解了MAS中的错误级联问题，在无需重训练的前提下提升了系统性能和鲁棒性。其发布的代码和数据集为未来研究提供了基础。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [190] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: 本研究提出CXReasonAgent，一个融合大型语言模型与临床诊断工具的胸部X光诊断智能体，通过影像证据实现可验证的多步推理。构建CXReasonDial基准（1,946对话，12任务）验证其相比传统视觉语言模型在可靠性和可验证性上的优势。


<details>
  <summary>Details</summary>
Motivation: 胸部X光诊断本质需要多步、证据支撑的推理，但现有大型视觉语言模型存在三大局限：1) 生成合理但不符合诊断依据的回答，2) 视觉证据有限难以验证，3) 支持新诊断任务需昂贵重训练，严重制约了其在临床环境中的可靠性和适应性。

Method: 设计CXReasonAgent框架，将大型语言模型与临床诊断工具深度整合，利用影像衍生的诊断和视觉证据进行推理。构建多轮对话基准CXReasonDial，涵盖12项诊断任务的1,946个对话，系统评估模型的证据 grounded 推理能力。

Result: 在CXReasonDial基准上，CXReasonAgent生成忠实于影像证据的诊断响应，显著优于传统视觉语言模型，实现了更可靠、可验证的胸部X光诊断推理。

Conclusion: 在安全关键的临床场景中，将大型语言模型与临床诊断工具集成对于确保诊断推理的可靠性和可验证性至关重要，该方法为医疗AI提供了新的技术路径。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [191] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: 本文提出ODEBRAIN，一种基于神经常微分方程（Neural ODE）的潜在动态预测框架。该方法通过将时空频特征集成到谱图节点中，连续建模脑电图的潜在动态，解决了传统循环架构离散化时间导致的累积误差问题，并能捕捉EEG的瞬时非线性特征。


<details>
  <summary>Details</summary>
Motivation: 传统潜在变量方法采用循环架构离散化时间建模连续脑动态，不可避免地产生复合累积预测误差，且无法捕捉脑电图的瞬时非线性特征，这在基础神经科学研究和临床应用中存在局限。

Method: 提出ODEBRAIN框架，首先将时空频特征整合至谱图节点，随后利用神经常微分方程对连续潜在动态进行建模，确保潜在表征能在任意时间点捕捉复杂脑状态的随机变化。

Result: 大量实验表明，ODEBRAIN在脑电图动态预测任务中显著优于现有方法，表现出更强的鲁棒性和泛化性能。

Conclusion: 该研究验证了神经常微分方程框架在连续脑动态建模中的有效性，为神经科学研究和临床应用提供了更准确的预测工具。

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


### [192] [The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302)
*Giacomo Bonanno*

Main category: cs.AI

TL;DR: 该文将KM信念更新公理对应到含信念算子B、条件算子>和必然算子□的三模态逻辑，与AGM信念修正的模态逻辑L_AGM比较，证明L_KM公理均为L_AGM定理，AGM修正是KM更新的特例，且强版本差异仅在于处理非意外信息的单一公理。


<details>
  <summary>Details</summary>
Motivation: KM信念更新与AGM信念修正是知识表示领域的两大核心理论，但二者关系未明。通过模态逻辑公理化方法，可系统揭示其理论包含关系，为信念更新的形式化理论选择与应用奠定元理论基础。

Method: 1) 构建三模态逻辑语言，含单模态信念算子B、双模态条件算子>和必然算子□；2) 将KM信念更新公理逐条转换为对应的模态公理，形成逻辑系统L_KM；3) 将AGM信念修正公理转换为模态公理，形成逻辑系统L_AGM；4) 采用形式化证明方法比较两系统的定理包含关系。

Result: 1) 证明L_KM中每条公理均为L_AGM中的定理；2) 确立AGM信念修正可视为KM信念更新的特例；3) 对强KM更新，两逻辑差异精确归结为单一公理，该公理仅处理非意外信息（即主体初始未 disbelieved 的公式）。

Conclusion: KM信念更新框架具有比AGM信念修正更广泛的表达力，后者是其特例。该发现澄清了信念更新理论的层次结构，为不同场景下的理论选择提供了形式化依据，并揭示了强更新与AGM的本质差异在于对非意外信息的处理原则。

Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

</details>


### [193] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 该论文发现AI模型对同一输入的不同不变变换版本进行推理时，其误差因认知不确定性而呈现部分独立性。基于此洞察，提出一种重采样推理方法，通过聚合多个变换版本的输出来提升推理准确性，并为模型大小与性能平衡提供新策略。


<details>
  <summary>Details</summary>
Motivation: 即使经过优化的AI模型也会因偶然不确定性和认知不确定性产生推理错误。论文关键洞察在于观察到当对同一输入进行不变变换后多次推理时，误差间存在部分独立性，这为突破传统单样本推理的局限性、提升模型鲁棒性提供了新思路。

Method: 提出一种基于"重采样"的推理方法：首先对输入样本应用多种不变变换（如旋转、缩放等）生成多个版本，然后使用已训练好的AI模型对这些变换后的样本分别进行推理，最后通过聚合策略（如平均、投票等）整合所有输出，获得更准确的最终预测结果。

Result: 该方法能够有效提升AI模型的推理准确性，降低因认知不确定性导致的错误。同时，该策略提供了一种在模型大小和性能之间进行权衡的新途径，即在保持或提升性能的前提下，可能使用更小的模型。

Conclusion: 通过利用认知不确定性引发的误差独立性特征，重采样聚合推理方法可显著改善AI模型的预测性能。这不仅为提升推理可靠性提供了实用技术，也为设计更高效、更经济的AI系统开辟了新方向。

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [194] [Generalized Rapid Action Value Estimation in Memory-Constrained Environments](https://arxiv.org/abs/2602.23318)
*Aloïs Rautureau,Tristan Cazenave,Éric Piette*

Main category: cs.AI

TL;DR: 为解决GGP中GRAVE算法内存消耗大的问题，提出三种改进算法（GRAVE2、GRAVER、GRAVER2），通过双层搜索、节点回收或两者结合，在保持原有性能的同时大幅减少存储节点数量。


<details>
  <summary>Details</summary>
Motivation: GRAVE作为蒙特卡洛树搜索在通用游戏博弈中的强有力变体，因需在每个节点存储额外统计信息而面临内存瓶颈，限制了其实际应用。

Method: 引入双层搜索机制、节点回收策略，并设计融合两者的GRAVER2算法，通过优化节点管理降低内存需求。

Result: 改进算法在保持与GRAVE相当游戏水平的前提下，显著减少了存储节点数量。

Conclusion: 新算法有效解决了内存限制问题，使GRAVE更适合资源受限的实际应用场景。

Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.

</details>


### [195] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: 本研究通过多模型、多基准实验发现，大型语言模型（LLMs）使新手在生物安全相关任务中的表现显著提升——准确率较仅使用互联网资源的对照组提高4.16倍。尽管有LLM辅助的新手在多数基准测试中超越专家，但用户未能充分发挥模型潜力，且89.6%参与者表示可轻易获取双用途信息，凸显持续评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在生物学基准测试中表现日益优异，但其对新手用户的实际能力提升效果——即能否带来超越互联网资源的增益——尚不明确。这一不确定性关乎科学加速效应与双用途风险评估的核心理解。

Method: 研究采用多模型、多基准的人类提升实验设计，比较八类生物安全相关任务中，具备LLM访问权限的新手与仅能使用互联网资源的对照组表现。参与者拥有充足解题时间（最复杂任务长达13小时），以完成复杂问题。

Result: LLM访问带来显著提升：有LLM辅助的新手准确率较对照组提高4.16倍（95% CI [2.63, 6.87]）。在四个具备专家基线的基准中，LLM辅助的新手在三个上超越专家水平。值得注意的是，独立LLM表现常优于LLM辅助的新手，表明用户未充分挖掘模型潜力。尽管存在防护措施，89.6%参与者报告获取双用途相关信息几乎无障碍。

Conclusion: LLMs显著提升了新手在以往仅限专业从业者完成的生物学任务上的表现，这一发现强调除传统基准测试外，必须开展持续、交互式的人类提升评估，以有效应对双用途风险。

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [196] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 本文针对现有LLM交易系统抽象指令导致性能下降的问题，提出细粒度任务分解的多智能体框架。基于日本股票数据的防泄漏回测表明，该方法显著优于粗粒度设计，且分析结果与决策偏好的对齐是核心驱动因素，结合投资组合优化可实现更优表现。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM交易系统采用模仿分析师与经理角色的粗粒度抽象指令，忽略了真实工作流的复杂性，导致推理性能下降和决策过程不透明。

Method: 提出一种显式分解投资分析为细粒度任务的多智能体框架，采用日本股票市场的价格、财务报表、新闻及宏观信息，在严格控制数据泄漏的回测环境中评估框架，并应用标准投资组合优化方法。

Result: 实验结果表明：1) 细粒度任务分解显著提升风险调整后收益；2) 智能体分析输出与下游决策偏好的对齐程度是系统性能的关键驱动因素；3) 利用系统输出与股票指数的低相关性及方差进行投资组合优化，可获得更优表现。

Conclusion: 研究发现细粒度任务分解和对齐机制对LLM交易系统至关重要，为实际应用中的智能体结构设计与任务配置提供了重要指导。

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>
