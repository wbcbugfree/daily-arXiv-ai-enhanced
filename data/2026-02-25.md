<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.AI](#cs.AI) [Total: 32]
- [cs.IR](#cs.IR) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Talking to Yourself: Defying Forgetting in Large Language Models](https://arxiv.org/abs/2602.20162)
*Yutao Sun,Mingshuai Chen,Tiancheng Zhao,Phillip Miao,Zilun Zhang,Haozhan Shen,Ruizhe Zhu,Jianwei Yin*

Main category: cs.CL

TL;DR: 本文提出SA-SFT，一种轻量级自增强微调方法。通过在微调前让大语言模型生成自对话数据并与任务数据混合，在不改变优化策略或训练计划、无需外部数据的条件下，有效缓解灾难性遗忘并提升领域内性能，在50个评估场景中40个表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在针对狭窄任务数据进行微调时会出现灾难性遗忘，导致通用知识和推理能力退化。现有解决方案通常需要外部数据或复杂修改训练策略，增加了资源消耗和实施难度。

Method: 提出SA-SFT方法：在正式微调前，让大语言模型自我生成对话数据，将这些自创作数据与任务特定数据混合训练，保持原有优化器和训练计划不变，无需额外调参或外部数据支持。

Result: 在50个评估场景中，SA-SFT在40个案例中表现最优，性能与原始模型相当，显著优于层冻结、外部数据混合等基线方法，同时有效抑制灾难性遗忘并提升任务性能。

Conclusion: 自增强机制通过自我生成数据实现自对齐，可抵消风格诱导的参数漂移，提供了一种简单有效的大语言模型鲁棒适应方案，为灾难性遗忘问题提供了新的解决思路。

Abstract: Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.
  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.

</details>


### [2] [Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings](https://arxiv.org/abs/2602.20164)
*Sachin Gopal Wani,Eric Page,Ajay Dholakia,David Ellison*

Main category: cs.CL

TL;DR: 本文量化分析了知识蒸馏技术在小型语言模型开发中的性能与计算成本效益。研究表明，蒸馏得到的8B模型相比原生模型训练计算效率提升超过2000倍，且在推理能力上可与十倍参数量的模型媲美，验证了蒸馏不仅是模型压缩技术，更是构建先进、可访问AI的主要策略。


<details>
  <summary>Details</summary>
Motivation: 资源受限环境下需要强大且高效的小型语言模型，知识蒸馏为此提供了变革性途径。然而，缺乏对蒸馏模型与原生及商业模型在性能和计算成本方面的量化对比分析，这正是本研究的动机。

Method: 通过基准测试对比分析蒸馏模型、原生模型和商业专有模型的性能表现与计算成本，进行量化效率分析。

Result: 研究发现：(1)蒸馏创造了更优的性能-计算曲线；(2)训练8B蒸馏模型的计算效率是原生模型的2000倍以上；(3)蒸馏模型在推理能力上与十倍参数量的标准模型相当甚至超越。

Conclusion: 知识蒸馏不仅是模型压缩技术，更是构建最先进、可访问AI的核心策略，为资源受限环境下开发高效SLM提供了有效路径。

Abstract: Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI

</details>


### [3] [ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling](https://arxiv.org/abs/2602.20166)
*Yongda Yu,Lei Zhang,Xinxin Guo,Minghui Yu,Zhengqi Zhuang,Guoping Rong,Haifeng Shen,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobin Xu*

Main category: cs.CL

TL;DR: 本文提出ConceptRM方法，通过少量专家标注和协同教学技术，从噪声数据中筛选可靠负样本，以低成本构建高质量语料库训练反思模型，有效拦截智能体误报警，缓解警报疲劳。


<details>
  <summary>Details</summary>
Motivation: 智能体产生的海量误报警导致用户警报疲劳，可能忽略关键问题。现有方法依赖用户验证反馈数据训练反思模型，但此类数据在生产环境中噪声严重，人工清洗成本高昂，亟需低成本构建高质量训练语料库的方案。

Method: ConceptRM利用少量专家标注作为锚点，创建不同噪声比例的扰动数据集，采用协同教学策略训练多个差异化模型，通过分析模型决策一致性从噪声数据中识别可靠负样本，构建高质量训练语料库。

Result: 实验表明，ConceptRM以极低标注成本显著提升误报警拦截效果，在领域内数据集上超越现有LLM基线最高达53.31%，在域外数据集上超越基线最高达41.67%。

Conclusion: ConceptRM通过协同教学与模型共识机制有效解决了生产环境数据噪声问题，为构建高性能反思模型提供了低成本、高效的解决方案，对缓解警报疲劳具有重要实用价值。

Abstract: In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.

</details>


### [4] [What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance](https://arxiv.org/abs/2602.20300)
*William Watson,Nicole Cho,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: 该研究从语言学角度探讨大语言模型幻觉问题，认为查询形式本身会影响模型回答。通过构建22维查询特征向量并分析36.9万个真实查询，发现查询的句法复杂度、指代模糊性等特征与幻觉风险相关，为通过改写查询来减少幻觉提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 传统研究将大语言模型幻觉归咎于模型本身或解码策略的缺陷。本文的创新动机在于从语言学视角重新审视这一问题，认为用户查询的形式特征（如句法结构、词汇选择、指代清晰度等）同样会显著影响模型的回答准确性。通过识别高风险查询模式，可以为设计更可靠的交互方式提供理论依据。

Method: 研究借鉴经典语言学理论，构建了一个包含22个维度的查询特征向量，涵盖从句复杂度、词汇稀有度、指代、否定、可回答性到意图 grounding 等多个语言层面。利用369,837个真实世界查询数据，通过大规模统计分析探究不同查询特征与模型幻觉之间的相关性，建立了可观察的幻觉风险查询特征表示体系。

Result: 分析揭示了稳定的"风险图谱"：深层从句嵌套和未指定性（underspecification）等特征与更高的幻觉倾向显著相关；而明确的意图 grounding 和可回答性与较低的幻觉率相关；领域特异性等因素则表现出混合的、依赖于数据集和模型的效果。这些发现建立了查询特征与幻觉风险之间的经验相关性。

Conclusion: 研究结论表明，查询形式是影响大语言模型幻觉的关键因素之一，可通过构建查询特征风险表示来指导查询重写策略并开展未来干预研究，从而在模型层面之外开辟减少幻觉的新途径。

Abstract: Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

</details>


### [5] [No One Size Fits All: QueryBandits for Hallucination Mitigation](https://arxiv.org/abs/2602.20332)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: 本文提出QueryBandits，一种模型无关的上下文老虎机框架，通过在线学习选择最优查询重写策略来缓解大语言模型中的幻觉问题。该方法无需重新训练或梯度适应，仅通过前向传播机制即可工作，使其适用于闭源模型。在16个问答场景中，基于汤普森采样的QueryBandit相比无重写基线获得87.5%胜率，比Paraphrase和Expand等静态策略分别高出42.6%和60.3%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理能力增强伴随着幻觉现象加剧。现有缓解研究多集中于开源模型的后期检测和参数编辑，而占据机构部署主流的闭源模型缺乏有效解决方案。单一静态重写策略可能对所有查询并非最优，甚至增加幻觉，亟需自适应方法。

Method: 提出QueryBandits上下文老虎机框架，利用经验验证的奖励函数在线学习最优查询重写策略。框架通过语义特征动态选择改写动作，无需模型内部访问或重新训练，仅依赖前向传播机制实现模型行为调整，具有模型无关性。

Result: 在16个QA场景中，汤普森采样策略实现87.5%胜率，显著优于基线及静态策略（Paraphrase+42.6%，Expand+60.3%）。上下文老虎机全面超越普通老虎机，特征方差越大，策略选择方差越高。实验证明不存在适用于所有查询的最优单一重写策略，部分静态策略甚至累积更高遗憾值。

Conclusion: QueryBandits通过在线学习语义特征驱动的重写策略，有效降低大语言模型幻觉，且兼容闭源模型。研究表明自适应策略必要性，静态策略可能适得其反。该方法为机构部署中的闭源模型提供了无需参数访问的实用解决方案，推动了幻觉缓解研究的广度。

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.

</details>


### [6] [How communicatively optimal are exact numeral systems? Once more on lexicon size and morphosyntactic complexity](https://arxiv.org/abs/2602.20372)
*Chundra Cathcart,Arne Rubehn,Katja Bocklage,Luca Ciucci,Kellen Parker van Dam,Alžběta Kučerová,Jekaterina Mažara,Carlo Y. Meloni,David Snee,Johann-Mattis List*

Main category: cs.CL

TL;DR: 该研究挑战了以往关于递归数词系统通过平衡词库大小和形态句法复杂性来优化交际效率的观点，通过对52种遗传多样语言的标注分析，发现许多语言的数词系统效率低于预期。


<details>
  <summary>Details</summary>
Motivation: 以往研究认为精确递归数词系统能在数词词库大小和平均形态句法复杂性之间取得平衡，从而实现交际效率优化。但现有研究未能充分考虑语言实际显示的复杂程度差异，因此需要更精细的数据表征方法来重新评估这一假设。

Method: 研究采用52种遗传多样语言的数据，并设计了一套标注方案，区分可预测和不可预测的同素异形体（形式变体），以更准确地刻画数词系统的复杂性特征。

Result: 分析表明，世界上许多语言的数词系统明显低于理论预期的效率水平，挑战了数词系统普遍追求效率最大化的观点。

Conclusion: 这一发现对数词系统研究和更广泛的语言进化理论具有重要意义，提示数词系统的演变可能受到其他因素（如历史偶然性、文化需求等）的影响，而非单纯追求交际效率。

Abstract: Recent research argues that exact recursive numeral systems optimize communicative efficiency by balancing a tradeoff between the size of the numeral lexicon and the average morphosyntactic complexity (roughly length in morphemes) of numeral terms. We argue that previous studies have not characterized the data in a fashion that accounts for the degree of complexity languages display. Using data from 52 genetically diverse languages and an annotation scheme distinguishing between predictable and unpredictable allomorphy (formal variation), we show that many of the world's languages are decisively less efficient than one would expect. We discuss the implications of our findings for the study of numeral systems and linguistic evolution more generally.

</details>


### [7] [Disentangling Geometry, Performance, and Training in Language Models](https://arxiv.org/abs/2602.20433)
*Atharva Kulkarni,Jacob Mitchell Springer,Arjun Subramonian,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 本文系统研究了Transformer权重（特别是非嵌入矩阵）的几何特性与下游性能之间的关系。通过对108个OLMo风格语言模型进行受控变化的实验，发现最佳性能模型通常具有较高有效秩，但这一趋势并非普遍；低有效秩并不导致小模型后期性能下降，而是与之共存；有效秩受预训练超参数（如批次大小和权重衰减）影响；其他几何指标虽大致一致，但均不能可靠预测性能。总体而言，现有几何指标主要反映训练选择而非性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer权重的几何特性在可解释性研究中广泛应用，但其在估计下游性能方面的效用尚不明确。本文旨在系统探究模型性能与权重几何（尤其是有效秩）之间的关系。

Method: 采用108个OLMo风格语言模型，在受控变化条件下进行训练；分析非嵌入矩阵的有效秩及其他几何指标；考察其与下游性能的相关性；探究批次大小、权重衰减等超参数对有效秩的影响；并对最终层表示进行扩展分析。

Result: 关键发现包括：1) 高性能模型往往具有高有效秩，但此趋势并非普遍；2) 低有效秩并不导致小模型后期性能下降，而是与之共存；3) 存在对抗性案例，低秩模型未出现饱和；4) 有效秩受批次大小、权重衰减等超参数强烈影响；5) 其他几何指标基本一致，但均无法可靠预测下游性能。

Conclusion: 模型的几何特性（如有效秩）主要反映训练选择，而非直接决定性能。因此，基于现有几何指标的性能预测并不可靠，未来研究应关注更具判别性的指标。

Abstract: Geometric properties of Transformer weights, particularly the unembedding matrix, have been widely useful in language model interpretability research. Yet, their utility for estimating downstream performance remains unclear. In this work, we systematically investigate the relationship between model performance and the unembedding matrix geometry, particularly its effective rank. Our experiments, involving a suite of 108 OLMo-style language models trained under controlled variation, reveal several key findings. While the best-performing models often exhibit a high effective rank, this trend is not universal across tasks and training setups. Contrary to prior work, we find that low effective rank does not cause late-stage performance degradation in small models, but instead co-occurs with it; we find adversarial cases where low-rank models do not exhibit saturation. Moreover, we show that effective rank is strongly influenced by pre-training hyperparameters, such as batch size and weight decay, which in-turn affect the model's performance. Lastly, extending our analysis to other geometric metrics and final-layer representation, we find that these metrics are largely aligned, but none can reliably predict downstream performance. Overall, our findings suggest that the model's geometry, as captured by existing metrics, primarily reflects training choices rather than performance.

</details>


### [8] [Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.21103)
*Sanket Badhe,Deep Shah*

Main category: cs.CL

TL;DR: 提出Prompt-Level Distillation (PLD)方法，通过从教师模型提取推理模式并转化为结构化指令作为学生模型的系统提示，在保持极低延迟的同时显著提升性能，并实现完全透明的决策过程。


<details>
  <summary>Details</summary>
Motivation: 高级推理通常需要思维链提示（准确但延迟高、推理成本大）或微调小模型（牺牲可解释性且资源开销大），现有方法在效率、成本和透明度方面存在局限。

Method: 从教师模型提取显式推理模式，将其组织为结构化表达指令列表，作为学生模型的系统提示，实现提示级别的蒸馏。

Result: 在StereoSet和Contract-NLI数据集上，Gemma-3 4B模型的Macro F1分别从57%提升至90.0%和从67%提升至83%，以可忽略的延迟开销达到前沿性能。

Conclusion: 该方法使决策过程完全透明，支持人工验证逻辑，适用于法律、金融、内容审核等受监管行业以及高吞吐量和边缘设备场景。

Abstract: Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\% to 90.0\% and 67\% to 83\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.

</details>


### [9] [From Performance to Purpose: A Sociotechnical Taxonomy for Evaluating Large Language Model Utility](https://arxiv.org/abs/2602.20513)
*Gavin Levinson,Keith Feldman*

Main category: cs.CL

TL;DR: 针对大语言模型实际部署评估标准碎片化问题，本文提出LUX（语言模型效用分类法），构建涵盖性能、交互、运营、治理四领域的层次化评估框架，并开发配套动态网页工具，实现跨场景的模型效用一致量化评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在离散任务上持续进步，但在高 stakes 实际应用环境中，其有效性取决于超越传统性能指标的社会技术因素。现有评估指标缺乏系统组织，无法支持跨用例的一致比较，导致模型实用性与适用性评估存在方法论缺口。

Method: 创建LUX（语言模型效用分类法）层次化框架，将效用评估结构化为性能、交互、运营、治理四大领域，下设主题维度与组件，每个组件基于可量化指标，并开发外部动态网页工具连接至相关度量因子仓库。

Result: 产出LUX综合评估框架及动态网页工具，实现了大语言模型效用的结构化、量化评估，支持模型选择与应用场景的对齐决策。

Conclusion: LUX为评估大语言模型在高 stakes 环境中的实用价值提供了统一分类标准，促进了评估实践的一致性与科学性，有助于保障模型在实际部署中的有效性与安全性。

Abstract: As large language models (LLMs) continue to improve at completing discrete tasks, they are being integrated into increasingly complex and diverse real-world systems. However, task-level success alone does not establish a model's fit for use in practice. In applied, high-stakes settings, LLM effectiveness is driven by a wider array of sociotechnical determinants that extend beyond conventional performance measures. Although a growing set of metrics capture many of these considerations, they are rarely organized in a way that supports consistent evaluation, leaving no unified taxonomy for assessing and comparing LLM utility across use cases. To address this gap, we introduce the Language Model Utility Taxonomy (LUX), a comprehensive framework that structures utility evaluation across four domains: performance, interaction, operations, and governance. Within each domain, LUX is organized hierarchically into thematically aligned dimensions and components, each grounded in metrics that enable quantitative comparison and alignment of model selection with intended use. In addition, an external dynamic web tool is provided to support exploration of the framework by connecting each component to a repository of relevant metrics (factors) for applied evaluation.

</details>


### [10] [Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning](https://arxiv.org/abs/2602.20528)
*Justin Lovelace,Christian Belardi,Sofian Zalouk,Adhitya Polavaram,Srivatsa Kundurthy,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: STAR-LDM融合潜在扩散规划与自回归生成，通过生成过程中的"思考"阶段实现连续空间语义规划，显著提升语言模型的全局决策能力与生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型受限于逐词元串行决策机制，缺乏在离散输出前进行全局语义规划的能力，导致长文本连贯性与推理能力受限。

Method: 在自回归生成流程中插入暂停机制，利用潜在扩散模型在连续语义空间中细化生成计划，完成全局规划后再提交离散词元，实现"先思考后生成"的混合架构。

Result: 在语言理解基准测试中显著超越同规模模型；LLM-as-judge评估显示在叙事连贯性与常识推理任务上胜率超70%；支持轻量级分类器控制，无需重训练即可调节生成属性。

Conclusion: 该架构在保持流畅性的同时实现细粒度可控生成，为语言模型提供了更优的流畅度-控制权衡，证明了连续空间规划对提升离散序列生成决策的有效性。

Abstract: The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a "thinking" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.

</details>


### [11] [Personal Information Parroting in Language Models](https://arxiv.org/abs/2602.20580)
*Nishant Subramani,Kshitish Ghate,Mona Diab*

Main category: cs.CL

TL;DR: 本文开发了一套正则表达式与规则（R&R）检测器套件用于检测个人隐私信息（邮箱、电话、IP地址），并评估了大语言模型对隐私信息的记忆风险。研究发现Pythia-6.9b模型会逐字复述13.6%的隐私信息，且模型参数量与训练时长均与记忆程度呈正相关，最小的模型也有2.7%的复述率，因此建议对训练数据进行严格过滤和匿名化处理。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在大量网页抓取数据上训练，其中包含数百万条个人信息实例。模型记忆这些隐私信息会带来隐私风险，因此需要开发更有效的检测方法并评估不同规模模型的记忆程度，以提出缓解策略。

Method: 研究团队开发了正则表达式与规则（R&R）检测器套件，用于检测电子邮件、电话号码和IP地址，并在483条人工筛选的隐私信息实例上测试了大语言模型Pythia-6.9b的记忆能力，通过贪心解码生成完整的隐私信息片段来测量复述率。随后扩展到不同规模（160M-6.9B参数）和训练步数（70k-143k次迭代）的Pythia模型系列进行分析。

Result: Pythia-6.9b模型会逐字复述13.6%的隐私信息。模型大小和预训练时长均与记忆程度呈正相关。即使是1.6亿参数的最小规模模型也会精确复述2.7%的实例。

Conclusion: 研究强烈建议对预训练数据集进行激进的过滤和匿名化处理，以最小化隐私信息的复述风险。

Abstract: Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.

</details>


### [12] [Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books](https://arxiv.org/abs/2602.20647)
*W. Frederick Zimmerman*

Main category: cs.CL

TL;DR: 本研究提出语义新颖度作为大规模语料库叙事结构的度量标准，通过分析28,606本PG19书籍发现八种叙事原型，其中新颖度波动性最能预测读者规模，且叙事结构显著受体裁和历史时期影响。


<details>
  <summary>Details</summary>
Motivation: 传统叙事结构研究依赖情感或主题分析，缺乏大规模、信息论基础的量化方法。本研究引入语义新颖度——段落嵌入向量与前述段落质心间的余弦距离——作为叙事结构的新度量，旨在揭示信息密度动态变化对读者参与度的影响机制。

Method: 基于PG19语料库的28,606本1920年前英文书籍，采用768维SBERT模型生成段落嵌入，计算语义新颖度曲线；通过16段分段聚合近似(PAA)降维后，运用Ward层次聚类识别叙事原型，并采用偏相关分析与卡方检验探究叙事结构特征与读者规模、体裁及历史趋势的关联。

Result: 聚类发现八种典型叙事形状原型。新颖度轨迹的波动性(volume)是读者规模的最强长度无关预测因子(偏相关系数=0.32)，速度和终始比(T/I比率)次之(偏相关系数均为0.19)。迂回度(circuitousness)虽原始相关系数达0.41，但93%与长度相关，控制长度后偏相关系数降至0.11，揭示长度混淆因素对语料库相关性研究的影响。体裁对叙事形状有极强约束力(卡方值=2121.6, p<10⁻²⁴²)，小说多呈 plateau 模式而非小说前载信息。历史分析显示1840-1910年间书籍可预测性显著增强(T/I比率 r=-0.74, p=0.037)。SAX分析表明85%的书籍具有独特语义轨迹。

Conclusion: 信息密度动态变化构成叙事结构的基本维度，独立于情感与主题，对读者参与度产生可测量的影响。本研究不仅揭示了文学作品的量化叙事规律，也为数字人文研究提供了新范式，同时警示语料库分析中长度混淆因素的潜在干扰。

Abstract: I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty

</details>


### [13] [CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models](https://arxiv.org/abs/2602.20648)
*Anqi Li,Chenxiao Wang,Yu Lu,Renjun Xu,Lizhi Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: 本文提出CARE框架，基于LLM自动预测心理咨询治疗联盟的多维度分数并生成可解释理由。该框架通过理由增强监督和上下文建模，显著缩小了咨询师评估与客户感知的差距，在真实世界中文咨询场景中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统治疗联盟评估依赖负担重的课后问卷，存在延迟；现有计算方法则生成粗粒度分数、缺乏可解释性且忽视整体会话语境，难以准确捕捉客户真实感知，影响咨询效果优化。

Method: 构建于CounselingWAI数据集，并新增9,516条专家标注理由；以LLaMA-3.1-8B-Instruct为基座模型，采用理由增强监督进行微调，实现联盟分数预测与可解释理由生成的联合任务。

Result: 在实验中显著优于主流LLM，将咨询师评估与客户感知的差距缩小70%以上（Pearson相关系数提升超70%）；理由增强监督进一步提升精度；生成理由经自动和人工评估验证质量高。

Conclusion: CARE有效解决了治疗联盟感知的量化难题，揭示了联盟构建的常见挑战和互动模式，为AI辅助心理健康服务提供了可行工具，具备支持真实心理咨询场景的应用潜力。

Abstract: Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.

</details>


### [14] [ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition](https://arxiv.org/abs/2602.20727)
*Xindian Ma,Rundong Kong,Peng Zhang,Ruoxiang Huang,Yongyu Jiang*

Main category: cs.CL

TL;DR: ID-LoRA提出一种新型PEFT框架，通过从预训练权重中提取聚类参数组并共享低秩矩阵，在五大基准测试上实现比LoRA少46%可训练参数的同时性能更优，有效打破了参数效率与模型容量的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模扩大，LoRA及其变体仍引入较多可训练参数，而过度降低秩会损害复杂多任务性能。现有方法在参数开销与性能之间存在明显权衡，亟需打破这一困境。

Method: 从预训练权重矩阵中提取聚类参数组，将其重构为多个低秩分量，这些分量仅共享一个可训练的初始化低秩矩阵，从而在保持模型容量的同时大幅减少可训练参数数量。

Result: 在数学推理、代码生成、MMLU、常识问答和安全对齐五个基准上，ID-LoRA超越全量微调和LoRA、DoRA、HydraLoRA等基线，可比标准LoRA减少最多46%的可训练参数。在多任务场景下，仅需传统LoRA 54%的参数量即可在代码和MMLU任务上超越LoRA及其变体。

Conclusion: ID-LoRA通过参数组复用和矩阵共享机制，成功打破了参数效率与性能的权衡关系，为大规模模型的高效微调提供了新思路，在资源受限场景下具有重要应用价值。

Abstract: LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.

</details>


### [15] [Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization](https://arxiv.org/abs/2602.20743)
*Gabriel Loiseau,Damien Sileo,Damien Riquet,Maxime Meyer,Marc Tommasi*

Main category: cs.CL

TL;DR: 本文提出自适应文本匿名化框架，通过任务特定的提示优化自动构建语言模型指令，动态平衡隐私保护与数据效用，在五大数据集上超越现有方法且保持计算效率


<details>
  <summary>Details</summary>
Motivation: 现有文本匿名化方法依赖静态人工策略，缺乏灵活性，无法适应不同数据领域、隐私目标和下游应用的个性化需求，导致跨场景泛化能力不足

Method: 提出自适应文本匿名化新范式，采用任务特定的提示优化技术自动为语言模型生成定制化匿名化指令，实现针对隐私目标、领域特性和使用场景的动态适配

Result: 构建涵盖五类不同领域、隐私约束和效用目标的基准测试，该框架在所有评估设置下均获得更优的隐私-效用权衡，计算高效，开源模型效果显著且性能媲美大型闭源模型，并能发现探索隐私-效用边界的新策略

Conclusion: 该框架有效解决了文本匿名化的上下文敏感性问题，通过自适应策略在多样化场景中实现高效隐私保护，为实际部署提供了可扩展的解决方案

Abstract: Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.

</details>


### [16] [Explicit Grammar Semantic Feature Fusion for Robust Text Classification](https://arxiv.org/abs/2602.20749)
*Azrin Sultana,Firoz Ahmed*

Main category: cs.CL

TL;DR: 针对现有Transformer模型计算密集、不适于资源受限环境的问题，本研究提出一种轻量级文本分类模型。该方法通过显式编码句法结构（句法组合、短语模式、复杂度指标）为紧凑语法向量，并与冻结的上下文嵌入融合形成统一表示，将语法作为显式归纳偏置而非可学习模块，在边缘设备上实现比基线模型2%-15%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的模型虽能捕捉深层语法语义特征，但计算开销巨大，无法在资源受限的边缘设备上高效部署。为此，亟需开发一种既能保持语法结构感知能力，又具备轻量化特性的文本分类模型，以满足实际应用场景中的效率需求。

Method: 提出融合语法规则与语义信息的轻量化框架：1）显式编码句子级语法结构（句法组合、短语模式、复杂度指标）生成紧凑语法向量；2）将语法向量与冻结的上下文嵌入进行异质融合，构建统一表示；3）采用DBN、LSTM、BiLSTM、BERT、XLNET等多种模型进行训练与评估；4）将语法作为显式归纳偏置，避免额外注意力层、树编码器或全参数微调。

Result: 实验表明，该统一特征表示模型在异构域上性能显著优于基线模型，提升幅度达2%-15%。相比需通过额外模块注入语法结构的前序方法，本框架生成的模型极为轻量，在边缘设备上展现出更优的部署性能与计算效率。

Conclusion: 本框架成功构建了基于显式语法归纳偏置的轻量级文本分类模型，在保持语法结构感知能力的同时大幅降低计算复杂度，为资源受限环境下的自然语言处理应用提供了高效可行的解决方案。

Abstract: Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classification model without resorting to full parameterised transformer models or heavy deep learning architectures. The novelty of our approach lies in its explicit encoding of sentence-level grammatical structure, including syntactic composition, phrase patterns, and complexity indicators, into a compact grammar vector, which is then fused with frozen contextual embeddings. These heterogeneous elements unified a single representation that captures both the structural and semantic characteristics of the text. Deep learning models such as Deep Belief Networks (DBNs), Long Short-Term Memory (LSTMs), BiLSTMs, and transformer-based BERT and XLNET were used to train and evaluate the model, with the number of epochs varied. Based on experimental results, the unified feature representation model captures both the semantic and structural properties of text, outperforming baseline models by 2%-15%, enabling more effective learning across heterogeneous domains. Unlike prior syntax-aware transformer models that inject grammatical structure through additional attention layers, tree encoders, or full fine-tuning, the proposed framework treats grammar as an explicit inductive bias rather than a learnable module, resulting in a very lightweight model that delivers better performance on edge devices

</details>


### [17] [SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing](https://arxiv.org/abs/2602.20751)
*Yifei Xu,Guilherme Potje,Shivam Shandilya,Tiancheng Yuan,Leonardo de Oliveira Nunes,Rakshanda Agarwal,Saeid Asgari,Adam Atkinson,Emre Kıcıman,Songwu Lu,Ranveer Chandra,Tusher Chakraborty*

Main category: cs.CL

TL;DR: 针对开放生成RL后训练中奖励设计的挑战，本文提出SibylSense，通过可调记忆库动态适配规则生成器，利用验证器判别差距更新记忆，并交替进行对抗策略优化，实现规则扩展与性能提升。


<details>
  <summary>Details</summary>
Motivation: 开放生成任务的RL后训练面临奖励对齐难题。现有规则方法存在成本高昂、一致性差、易饱和漂移等问题，导致奖励黑客，亟需可自适应扩展且持续优化的奖励设计方法。

Method: SibylSense采用推理时学习框架：维护冻结规则生成器的可调记忆库；基于验证器计算的参考-候选答案判别差距更新记忆项；交替优化记忆库与规则对抗策略，使策略生成满足规则的答案，缩小判别差距，驱动规则生成器发现新质量维度。

Result: 在两项开放生成任务中，SibylSense相比静态和非自适应基线产生更具判别性的规则，并显著提升下游RL性能。

Conclusion: SibylSense通过动态记忆库和对抗学习机制，有效缓解了规则饱和与奖励黑客问题，为可扩展的自适应奖励设计提供了新范式。

Abstract: Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.

</details>


### [18] [Overton Pluralistic Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.20759)
*Yu Fu,Seongho Son,Ilija Bogunovic*

Main category: cs.CL

TL;DR: 本文提出OP-GRPO框架，通过强化学习实现单一大型语言模型的隐性Overton多元主义，无需显式提示或模块化编排即可生成多元观点。方法包括训练相似度评估器和双奖励系统，实验显示3B参数模型在观点覆盖上超越20B基线37.4%。


<details>
  <summary>Details</summary>
Motivation: 现有对齐范式无法捕捉人类价值观的多元性，传统方法依赖显式提示或模块化编排，限制了模型的内在多样性能力，需要开发能够从单一查询自动生成广泛且独特人类观点的框架。

Method: 提出两阶段OP-GRPO框架：1）相似度评估器训练：微调Sentence Transformer专门用于Overton多元主义任务，提供更准确的回复覆盖度评估；2）OP-GRPO训练：将相似度评估器融入双奖励系统，同时优化观点覆盖广度与各观点独特性，促进多样性生成。

Result: 实证呈现"小模型，大观点覆盖"效应：Qwen2.5-3B-Instruct模型在自然语言推理基准上相对20B GPT-OSS基线获得37.4%相对准确率提升，相对模块化架构基线提升19.1%。GPT-4.1作为评判模型的额外评估进一步验证了方法鲁棒性。

Conclusion: OP-GRPO成功实现了单一模型的隐性多元主义能力，显著提升了观点多样性，为小参数模型在价值观对齐任务上提供了高效解决方案，证明了该框架在无需模块化设计的情况下捕获人类价值多元性的有效性和可扩展性。

Abstract: Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a "small models, big perspective coverage" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.

</details>


### [19] [Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation](https://arxiv.org/abs/2602.20816)
*Sayantan Dasgupta,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 本文提出一种新的尾部分布感知散度用于语言模型蒸馏，通过解耦教师模型Top-K预测概率与低概率部分的贡献，在保持与KL散度相同计算开销的同时，减少教师模态主导效应，提升尾部信息利用率，在预训练和监督蒸馏任务中均表现优异且计算成本较低。


<details>
  <summary>Details</summary>
Motivation: 传统KL散度在语言模型蒸馏中易被教师模型的高概率token主导（模态主导问题），导致低概率但可能包含重要信息的分布尾部成分被削弱，限制了知识迁移的完整性和蒸馏效果。

Method: 提出一种尾部感知散度，将教师模型预测分布解耦为Top-K高概率部分和剩余尾部低概率部分，分别计算贡献并保持整体计算复杂度与KL散度相当，从而增强尾部分布信息的传递。

Result: 实验表明该方法在解码器模型的预训练和监督蒸馏中均能达到有竞争力的性能，且蒸馏过程高效，可在学术预算内完成大规模数据集训练，无需工业级计算资源。

Conclusion: 该解耦式散度有效平衡了教师模型模态主导与尾部分布信息利用，为语言模型蒸馏提供了一种高效且经济的解决方案，尤其适合学术场景。

Abstract: The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.

</details>


### [20] [FinAnchor: Aligned Multi-Model Representations for Financial Prediction](https://arxiv.org/abs/2602.20859)
*Zirui He,Huopu Zhang,Yanguang Liu,Sirui Wu,Mengnan Du*

Main category: cs.CL

TL;DR: 本文提出FinAnchor，一种无需微调即可整合多LLM嵌入的轻量级框架，通过选择锚定空间并学习线性映射对齐异构表示，在金融预测任务中持续优于单模型和传统集成方法。


<details>
  <summary>Details</summary>
Motivation: 长文档金融预测面临可行动信号稀疏且被噪声掩盖的挑战，同时最优LLM嵌入模型因任务和时间而异，导致单一模型方法效果不稳定。

Method: 框架选取一个锚定嵌入空间，通过可学习的线性变换将其他模型的表示投影到该空间以解决特征空间不兼容问题，随后聚合对齐后的特征形成统一表示用于下游预测任务。

Result: 实验表明，FinAnchor在多个金融NLP预测任务中一致超越强单模型基线及标准集成方法，验证了锚定策略的有效性。

Conclusion: 锚定异构表示的方法能够鲁棒地整合多源LLM嵌入特征，为金融预测提供了更稳定且有效的解决方案。

Abstract: Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.

</details>


### [21] [Exa-PSD: a new Persian sentiment analysis dataset on Twitter](https://arxiv.org/abs/2602.20892)
*Seyed Himan Ghaderi,Saeed Sarbazi Azad,Mohammad Mehdi Jaziriyan,Ahmad Akbari*

Main category: cs.CL

TL;DR: 本文介绍了Exa波斯语情感分析数据集，包含12,000条波斯语推文，由5名母语标注者标注为积极、中性和消极三类。研究使用Pars Bert和Roberta预训练模型进行评估，达到79.87的宏F1分数，证明该数据集和模型对波斯语情感分析系统具有实用价值。


<details>
  <summary>Details</summary>
Motivation: 波斯语自然语言处理面临诸多挑战，现有数据集多局限于特定主题（如产品、食品、酒店），而社交媒体上的表达常包含讽刺和口语化短语。缺乏专门针对波斯语推文的情感分析数据集，因此需要构建一个更贴近实际应用场景的波斯语社交媒体情感分析数据集。

Method: 研究人员从Twitter收集波斯语推文，构建Exa数据集，包含12,000条推文。由5名波斯语母语标注者进行标注，分为积极、中性和消极三个类别。使用预训练的Pars Bert和Roberta模型作为基线模型对数据集进行评估。

Result: 评估结果显示，模型在Exa数据集上达到了79.87的宏F1分数。该结果证明了模型和数据集在波斯语情感分析任务中的有效性，为波斯语情感分析系统提供了有价值的资源。

Conclusion: 本研究构建的Exa波斯语情感分析数据集填补了波斯语社交媒体情感分析资源的空白。79.87的宏F1分数表明该数据集和预训练模型能够充分支持波斯语情感分析系统的开发，对推动波斯语NLP发展具有重要意义。

Abstract: Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.

</details>


### [22] [The Art of Efficient Reasoning: Data, Reward, and Optimization](https://arxiv.org/abs/2602.20945)
*Taiqiang Wu,Zenan Zu,Bo Zhou,Ngai Wong*

Main category: cs.CL

TL;DR: 本文系统研究了LLM高效推理机制，发现训练遵循"长度适应"和"推理精炼"两阶段范式，提出通过选择简单提示避免长度坍塌，并在Qwen3系列(0.6B-30B)验证了长度偏置的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在链式思维推理中虽受益于思维扩展，但面临巨大计算开销。现有高效推理方法通过强化学习奖励机制鼓励简短而准确的推理轨迹，但其内在工作机制尚未被系统揭示，亟需深入理解训练动态和优化策略。

Method: 采用统一实验协议（约20万GPU小时），系统解构训练提示、推演轨迹、奖励塑形和优化策略。提出细粒度评估指标：基于正确性的长度分布分析和2k-32k token预算跨度的性能评估。通过控制变量实验揭示训练动态规律。

Result: 1) 发现训练遵循两阶段范式：长度适应阶段和推理精炼阶段；2) 关键发现：在相对简单的提示上训练可确保正奖励信号密度，有效避免长度坍塌问题；3) 学习的长度偏置具备跨领域泛化能力；4) 在Qwen3系列模型（0.6B-30B参数）上验证了方法的鲁棒性和普适性。

Conclusion: 研究揭示了高效推理训练的内在机制，提出了避免长度坍塌的实用策略，并建立了可推广的指导原则。长度偏置的跨领域泛化能力表明该机制具有广泛适用性，为未来高效推理模型的设计提供了理论依据和实践指南。

Abstract: Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.

</details>


### [23] [Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models](https://arxiv.org/abs/2602.20966)
*Paola Merlo,Chunyang Jiang,Giuseppe Samo,Vivi Nastase*

Main category: cs.CL

TL;DR: 本文提出黑鸟语言矩阵(BLM)任务——一种受智力测验启发的多层次结构化多选题测评工具，用于系统性评估大型语言模型在检测语言对象属性、跨句系统模式及错误类型分析方面的能力，揭示LLM语言处理的行为机制。


<details>
  <summary>Details</summary>
Motivation: 针对当前大型语言模型能力评估缺乏结构化、可解释基准的问题，本研究旨在通过精心设计的BLM数据集，系统探究LLM是否具备检测语言对象及其属性、识别跨句系统模式的能力，以及语言错误与推理错误的交互机制，为模型行为分析提供可解释性框架。

Method: 构建Blackbird语言矩阵(BLM)数据集，采用多层次结构化多选题设计（句内、序列间、候选答案内）。通过基准测试和针对性实验（组块化与系统性分析），在多种语言上使用简单基线模型和定制化模型验证数据集有效性。

Result: BLM任务虽具挑战性但性能良好：1) 多语言环境下均可达较高性能；2) 简单基线模型即可解决，定制化模型表现更优；3) 模型表征包含语法对象和属性；4) 成功关键在于检测跨句系统模式。

Conclusion: 精心构建的结构化数据集(BLM)支持语言和大型语言模型的多维度研究。其明确结构、完整学习上下文、预期答案及部分手工构建的特点，使其特别适合可解释性研究，有助于揭示LLM的行为机制和决策原因。

Abstract: This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact?
  We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences.
  The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.

</details>


### [24] [Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving](https://arxiv.org/abs/2602.20973)
*Yuliang Ji,Fuchen Shen,Jian Wu,Qiujie Xie,Yue Zhang*

Main category: cs.CL

TL;DR: 该论文针对现有数学推理数据集仅关注线性推理的局限，提出了首个由专业数学家标注的基于案例推理的一阶逻辑数据集PC-FOL，通过实验发现LLMs在线性推理与案例推理间存在显著性能差距，并结合图模型进行理论解释，揭示了自动数学证明生成的核心挑战。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据集过度侧重于线性推理，忽视了对立论证、分情况证明等其他关键推理方式，无法全面评估LLMs的真实数学推理能力，存在评估盲区。

Method: 1) 构建PC-FOL数据集：由专业数学家标注，专注案例推理问题，每例附带人工撰写自然语言证明；2) 在主流LLMs上进行对比实验；3) 基于图模型框架进行理论分析。

Result: LLMs在PC-FOL案例推理任务上的表现显著低于传统线性推理任务，存在明显性能鸿沟，验证了当前模型在处理非线性和分情况证明方面的能力缺陷。

Conclusion: 该研究揭示了自动数学证明生成领域的核心挑战，即LLMs难以处理复杂的非单调、分情况推理模式，为未来提升模型推理能力提供了理论依据和研究方向。

Abstract: To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.

</details>


### [25] [Evaluating Proactive Risk Awareness of Large Language Models](https://arxiv.org/abs/2602.20976)
*Xuan Luo,Yubin Chen,Zhiyu Hou,Linpu Yu,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 本文提出了一个主动风险意识评估框架，通过Butterfly数据集（1,094个环境生态领域的查询）测试LLM在潜在生态危害发生前的预警能力。实验发现，LLM在响应长度受限、多语言和跨模态物种保护方面存在严重盲点，揭示出现有安全对齐与现实生态责任间的关键差距。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型日益嵌入日常决策，其安全责任应从仅应对显性恶意意图扩展到预见非故意的但后果严重的风险。当前LLM缺乏主动识别和预警潜在生态危害的能力，这在环境领域尤为重要。

Method: 研究构建了Butterfly数据集，包含1,094个模拟普通解决方案寻求活动的查询，这些查询可能引发潜在的生态影响。通过五个广泛使用的LLM进行实验，分析了响应长度、语言和模态对模型主动风险意识的影响。

Result: 实验结果显示，在长度受限的响应中，主动意识显著下降；跨语言方面存在相似性；在多模态物种保护方面存在持续盲点。这些发现揭示了当前安全对齐与现实生态责任之间的关键差距。

Conclusion: 研究结果强调，现有LLM在主动风险预警方面存在重大不足，特别是在生态责任领域。需要在LLM部署中建立主动保障机制，以弥补当前安全框架的不足。

Abstract: As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.

</details>


### [26] [Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification](https://arxiv.org/abs/2602.21082)
*Vishal Patil,Shree Vaishnavi Bacha,Revanth Yamani,Yidan Sun,Mayank Kejriwal*

Main category: cs.CL

TL;DR: 本研究提出一种混合方法，利用大语言模型识别评论方面，结合传统机器学习进行大规模情感分类。通过ChatGPT分析餐厅评论样本构建分类器，应用于470万条评论，发现机器标注的方面能显著解释餐厅总体评分差异，为服务行业提供了可扩展的自动化分析框架。


<details>
  <summary>Details</summary>
Motivation: 客户评论信息重要但分析海量非结构化数据困难。大语言模型虽在自然语言理解方面表现优异，却受限于计算成本和可扩展性问题。本研究旨在突破LLM在大规模评论分析中的成本与可扩展性瓶颈。

Method: 采用混合方法：1)使用ChatGPT对抽样餐厅评论进行方面识别；2)基于人工标注评论开发情感分类器；3)将分类器扩展应用于470万条17年平台评论；4)通过回归分析验证机器标注方面对总体评分的解释力。

Result: 回归分析显示，机器标注的餐饮体验方面显著解释了不同菜系、地理区域和用餐方面的总体评分方差。成功实现了470万条评论的自动化方面情感分析。

Conclusion: 研究表明结合LLM与传统机器学习能有效自动化大规模方面级情感分析，为酒店业及其他服务领域的研究者和从业者提供实用框架，解决了计算成本与可扩展性问题。

Abstract: Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.

</details>


### [27] [PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data](https://arxiv.org/abs/2602.21165)
*Samah Fodeh,Linhai Ma,Yan Wang,Srivani Talakokkul,Ganesh Puthiaraju,Afshan Khan,Ashley Hagaman,Sarah Lowe,Aimee Roundtree*

Main category: cs.CL

TL;DR: PVminer是一个面向患者语言领域优化的NLP框架，用于自动结构化分析患者-医生安全通讯中的患者声音(PV)，通过集成患者专用BERT编码器和主题建模增强，在多标签多层级分类任务上显著优于基线模型，F1分数最高达82.25%。


<details>
  <summary>Details</summary>
Motivation: 患者生成文本蕴含丰富的患者声音信息和社会决定因素，但传统质性编码框架劳动密集且难以扩展；现有ML/NLP方法常将患者中心沟通与SDoH割裂处理，或采用不适配患者语言特点的模型，导致规模化分析受限。

Method: 将患者声音检测形式化为多标签多类别预测任务，核心包括：1)患者专用BERT编码器(PV-BERT-base/large)；2)无监督主题建模实现主题增强(PV-Topic-BERT)；3)针对Code、Subcode和Combo层级标签的微调分类器；4)在微调和推理阶段融入主题表征以丰富语义输入。

Result: 在层级化任务上表现优异，Code层级F1达82.25%，Subcode层级80.14%，Combo层级77.87%，全面超越生物医学与临床预训练基线；消融实验证实作者身份嵌入和主题增强各自带来显著增益。

Conclusion: PVminer为大规模分析患者声音提供了有效工具，证明领域适配和主题增强对患者语言理解至关重要；研究团队将公开预训练模型、源代码和文档，标注数据集可申请用于科研，推动患者中心沟通研究发展。

Abstract: Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.

</details>


### [28] [Multimodal Multi-Agent Empowered Legal Judgment Prediction](https://arxiv.org/abs/2601.12815)
*Zhaolu Kang,Junhao Gong,Qingxi Chen,Hao Zhang,Jiaxin Liu,Rong Fu,Zhiyuan Feng,Yuan Wang,Simon Fong,Kaiyue Zhou*

Main category: cs.CL

TL;DR: 针对传统法律判决预测方法在多指控、多样证据和适应性方面的局限，本文提出JurisMMA框架，通过分解审判任务、标准化流程来提升预测效果，并构建包含10万+中国司法记录的多模态数据集JurisMM。实验证实该框架在法律判决预测及更广泛法律应用中具有有效性。


<details>
  <summary>Details</summary>
Motivation: 法律判决预测是推进法律系统发展的基础任务。传统方法依赖统计分析或角色模拟，但面临多重指控、证据多样性和适应性不足等挑战，亟需更高效的解决方案。

Method: 提出JurisMMA框架，将审判任务分解、流程标准化并分阶段组织。同时构建JurisMM大规模数据集，包含10万+中国司法记录，整合文本与多模态视频-文本数据，以支持全面评估。

Result: 在JurisMM和LawBench基准上的实验验证了JurisMMA框架的有效性。结果表明，该框架不仅适用于法律判决预测，还可推广至更广泛的法律应用场景。

Conclusion: JurisMMA框架在法律判决预测任务中表现优异，并具有扩展到其他法律应用的潜力，为未来法律AI方法和数据集构建提供了新思路。

Abstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [MoBiQuant: Mixture-of-Bits Quantization for Token-Adaptive Elastic LLMs](https://arxiv.org/abs/2602.20191)
*Dongwei Wang,Jinhee Kim,Seokho Han,Denis Gudovskiy,Yohei Nakata,Tomoyuki Okuno,KhayTze Peong,Kang Eun Jeon,Jong Hwan Ko,Yiran Chen,Huanrui Yang*

Main category: cs.LG

TL;DR: 本文提出MoBiQuant，一种新颖的混合比特量化框架，通过根据词元敏感度动态调整权重精度，实现弹性大语言模型推理。该框架包含多对一递归残差量化和词元感知路由器，能够在无需重复校准的情况下，在LLaMA3-8B上达到与比特特化校准后训练量化相当的性能。


<details>
  <summary>Details</summary>
Motivation: 云边设备上运行时的计算资源变化需要弹性部署大语言模型，即根据可用资源使用不同量化精度进行推理。然而，量化校准参数通常与特定精度绑定，这给弹性精度校准和运行时精度切换带来挑战。研究发现，这种校准参数变化源于精度依赖的异常值迁移现象导致的词元级敏感度差异。

Method: 提出MoBiQuant框架，采用混合比特量化策略：1）多对一递归残差量化，通过迭代方式重建高精度权重；2）词元感知路由器，根据词元敏感度动态选择残差比特切片数量。该方法使模型能根据词元重要性自适应调整精度，实现平滑的精度切换。

Result: 实验表明，MoBiQuant展现出强大的弹性，在LLaMA3-8B模型上无需重复校准即可达到与比特特化校准后训练量化相当的性能水平，有效解决了弹性精度部署中的校准泛化问题。

Conclusion: MoBiQuant通过词元感知的混合比特量化机制，成功解决了弹性大语言模型部署中精度切换的挑战，为资源受限环境下的高效推理提供了新思路，具有重要的实用价值。

Abstract: Changing runtime complexity on cloud and edge devices necessitates elastic large language model (LLM) deployment, where an LLM can be inferred with various quantization precisions based on available computational resources. However, it has been observed that the calibration parameters for quantization are typically linked to specific precisions, which presents challenges during elastic-precision calibration and precision switching at runtime. In this work, we attribute the source of varying calibration parameters to the varying token-level sensitivity caused by a precision-dependent outlier migration phenomenon.Motivated by this observation, we propose \texttt{MoBiQuant}, a novel Mixture-of-Bits quantization framework that adjusts weight precision for elastic LLM inference based on token sensitivity. Specifically, we propose the many-in-one recursive residual quantization that can iteratively reconstruct higher-precision weights and the token-aware router to dynamically select the number of residual bit slices. MoBiQuant enables smooth precision switching while improving generalization for the distribution of token outliers. Experimental results demonstrate that MoBiQuant exhibits strong elasticity, enabling it to match the performance of bit-specific calibrated PTQ on LLaMA3-8B without repeated calibration.

</details>


### [30] [FedAvg-Based CTMC Hazard Model for Federated Bridge Deterioration Assessment](https://arxiv.org/abs/2602.20194)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 针对桥梁检测数据敏感性导致的跨组织共享难题，本文提出一种联邦学习框架协同训练连续时间马尔可夫链(CTMC)劣化风险模型。各参与方本地基于桥梁年龄、海岸线距离和桥面面积等协变量训练三个劣化转移方向的对数线性模型，每轮仅交换12维伪梯度向量，服务器采用带动量与梯度裁剪的FedAvg聚合，在合成数据上验证了收敛性与激励相容机制。


<details>
  <summary>Details</summary>
Motivation: 桥梁定期检测记录包含公共基础设施敏感信息，现有数据治理约束使跨组织数据共享不切实际。市政部门亟需在不转移原始数据的前提下协作训练桥梁劣化基准模型，以支持证据驱动的生命周期规划，同时维护数据主权。

Method: 提出联邦CTMC风险模型框架：1) 各用户本地基于三个劣化转移（良好→轻微、良好→严重、轻微→严重）训练含协变量的对数线性风险模型；2) 采用小批量随机梯度下降优化CTMC对数似然函数；3) 每轮通信仅上传12维伪梯度向量；4) 中央服务器使用样本加权的FedAvg结合动量项与梯度裁剪进行安全聚合。

Result: 在具有区域异质性的全合成数据上验证：异构用户间的平均负对数似然持续收敛；聚合梯度范数随参与用户规模增大而减小；联邦机制通过提供本地数据无法单独获得的全局基准参数作为回报，形成自然参与激励。

Conclusion: 该框架实现了数据主权与协同学习的平衡：市政部门通过技术标准平台注册本地数据即可获得全局劣化基准模型，支持科学决策，同时通过激励相容机制促进可持续的数据协作，为基础设施管理提供隐私保护的智能化解决方案。

Abstract: Bridge periodic inspection records contain sensitive information about public infrastructure, making cross-organizational data sharing impractical under existing data governance constraints. We propose a federated framework for estimating a Continuous-Time Markov Chain (CTMC) hazard model of bridge deterioration, enabling municipalities to collaboratively train a shared benchmark model without transferring raw inspection records. Each User holds local inspection data and trains a log-linear hazard model over three deterioration-direction transitions -- Good$\to$Minor, Good$\to$Severe, and Minor$\to$Severe -- with covariates for bridge age, coastline distance, and deck area. Local optimization is performed via mini-batch stochastic gradient descent on the CTMC log-likelihood, and only a 12-dimensional pseudo-gradient vector is uploaded to a central server per communication round. The server aggregates User updates using sample-weighted Federated Averaging (FedAvg) with momentum and gradient clipping. All experiments in this paper are conducted on fully synthetic data generated from a known ground-truth parameter set with region-specific heterogeneity, enabling controlled evaluation of federated convergence behaviour. Simulation results across heterogeneous Users show consistent convergence of the average negative log-likelihood, with the aggregated gradient norm decreasing as User scale increases. Furthermore, the federated update mechanism provides a natural participation incentive: Users who register their local inspection datasets on a shared technical-standard platform receive in return the periodically updated global benchmark parameters -- information that cannot be obtained from local data alone -- thereby enabling evidence-based life-cycle planning without surrendering data sovereignty.

</details>


### [31] [Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning](https://arxiv.org/abs/2602.20197)
*Zhuoxu Huang,Mengxi Jia,Hao Sun,Xuelong Li,Jungong Han*

Main category: cs.LG

TL;DR: 针对多模态大语言模型RLVR训练中的熵崩溃与策略退化问题，本文提出CalibRL混合策略框架，通过分布感知优势加权和非对称激活函数实现专家引导的可控探索，在八个基准测试中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 可验证奖励强化学习(RLVR)虽能提升多模态大语言模型推理能力，但面临巨大状态空间与稀疏奖励导致的熵崩溃、策略退化及次优行为过度利用等问题，亟需保持有效随机性的探索策略。

Method: CalibRL框架采用双机制：1)分布感知优势加权按群体稀有度缩放更新以校准分布、保持探索；2)非对称激活函数(LeakyReLU)以专家知识为基准调节过度自信更新。通过在线采样估计策略分布，实现引导式熵增与目标分布澄清。

Result: 在八个域内域外基准测试中，CalibRL展现一致性能提升，证实其缓解策略-专家轨迹分布失配、平衡探索利用的稳定性与有效性。

Conclusion: 该方法通过混合策略与专家引导实现可控探索，为多模态大语言模型提供稳定强化学习训练范式，有效提升推理能力。

Abstract: Reinforcement Learning with verifiable rewards (RLVR) has emerged as a primary learning paradigm for enhancing the reasoning capabilities of multi-modal large language models (MLLMs). However, during RL training, the enormous state space of MLLM and sparse rewards often leads to entropy collapse, policy degradation, or over-exploitation of suboptimal behaviors. This necessitates an exploration strategy that maintains productive stochasticity while avoiding the drawbacks of uncontrolled random sampling, yielding inefficient exploration. In this paper, we propose CalibRL, a hybrid-policy RLVR framework that supports controllable exploration with expert guidance, enabled by two key mechanisms. First, a distribution-aware advantage weighting scales updates by group rareness to calibrate the distribution, therefore preserving exploration. Meanwhile, the asymmetric activation function (LeakyReLU) leverages the expert knowledge as a calibration baseline to moderate overconfident updates while preserving their corrective direction. CalibRL increases policy entropy in a guided manner and clarifies the target distribution by estimating the on-policy distribution through online sampling. Updates are driven by these informative behaviors, avoiding convergence to erroneous patterns. Importantly, these designs help alleviate the distributional mismatch between the model's policy and expert trajectories, thereby achieving a more stable balance between exploration and exploitation. Extensive experiments across eight benchmarks, including both in-domain and out-of-domain settings, demonstrate consistent improvements, validating the effectiveness of our controllable hybrid-policy RLVR training. Code is available at https://github.com/zhh6425/CalibRL.

</details>


### [32] [IMOVNO+: A Regional Partitioning and Meta-Heuristic Ensemble Framework for Imbalanced Multi-Class Learning](https://arxiv.org/abs/2602.20199)
*Soufiane Bacha,Laouni Djafri,Sahraoui Dhelim,Huansheng Ning*

Main category: cs.LG

TL;DR: 本文提出IMOVNO+两级框架，通过数据层面（条件概率评估样本信息量、区域划分、重叠清理、智能过采样）和算法层面（元启发式修剪集成分类器）的联合优化，显著提升二分类和多分类任务中数据质量与算法鲁棒性，在35个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡、重叠和噪声问题严重降低数据质量和模型泛化能力。虽在二分类中已有研究，但多分类场景下复杂的类间关系使少数类-多数类结构模糊，传统方法效果有限。现有方法存在明显局限：几何距离方法易误删信息样本并生成低质量合成数据；二值化方法仅局部处理不平衡且忽略全局类间依赖；集成方法难以有效整合弱分类器。亟需能联合优化数据质量和算法鲁棒性的新框架。

Method: 提出IMOVNO+两级优化框架：数据层面，1)利用条件概率量化每个样本的信息量；2)将数据集划分为核心、重叠和噪声区域；3)设计结合Z-score指标与大跳间隙距离的重叠清理算法；4)采用基于多重正则化的智能过采样算法，精确控制合成样本邻近性以防新增重叠。算法层面，通过元启发式策略修剪集成分类器，降低弱学习器负面影响。在35个数据集（13个多类，22个二分类）上系统验证。

Result: 实验结果表明IMOVNO+持续优于最先进方法，多个案例性能接近100%。多分类任务中：G-mean提升37-57%，F1-score提升25-44%，精确率提升25-39%，召回率提升26-43%。二分类任务中性能接近完美，相对提升14-39%。

Conclusion: IMOVNO+框架成功解决了多分类和二分类中的类别不平衡、重叠和噪声问题，通过数据质量与算法鲁棒性联合优化显著提升模型性能。该框架特别适用于数据收集和隐私限制导致的数据稀缺场景，为复杂分类问题提供了高效统一的解决方案。

Abstract: Class imbalance, overlap, and noise degrade data quality, reduce model reliability, and limit generalization. Although widely studied in binary classification, these issues remain underexplored in multi-class settings, where complex inter-class relationships make minority-majority structures unclear and traditional clustering fails to capture distribution shape. Approaches that rely only on geometric distances risk removing informative samples and generating low-quality synthetic data, while binarization approaches treat imbalance locally and ignore global inter-class dependencies. At the algorithmic level, ensembles struggle to integrate weak classifiers, leading to limited robustness. This paper proposes IMOVNO+ (IMbalance-OVerlap-NOise+ Algorithm-Level Optimization), a two-level framework designed to jointly enhance data quality and algorithmic robustness for binary and multi-class tasks. At the data level, first, conditional probability is used to quantify the informativeness of each sample. Second, the dataset is partitioned into core, overlapping, and noisy regions. Third, an overlapping-cleaning algorithm is introduced that combines Z-score metrics with a big-jump gap distance. Fourth, a smart oversampling algorithm based on multi-regularization controls synthetic sample proximity, preventing new overlaps. At the algorithmic level, a meta-heuristic prunes ensemble classifiers to reduce weak-learner influence. IMOVNO+ was evaluated on 35 datasets (13 multi-class, 22 binary). Results show consistent superiority over state-of-the-art methods, approaching 100% in several cases. For multi-class data, IMOVNO+ achieves gains of 37-57% in G-mean, 25-44% in F1-score, 25-39% in precision, and 26-43% in recall. In binary tasks, it attains near-perfect performance with improvements of 14-39%. The framework handles data scarcity and imbalance from collection and privacy limits.

</details>


### [33] [Multimodal Crystal Flow: Any-to-Any Modality Generation for Unified Crystal Modeling](https://arxiv.org/abs/2602.20210)
*Kiyoung Seong,Sungsoo Ahn,Sehui Han,Changyoung Park*

Main category: cs.LG

TL;DR: 本文提出MCFlow，一个统一的多模态流模型，通过独立的时间变量将晶体结构预测和从头生成等多个任务整合为不同的推理轨迹，并在标准Transformer中引入成分和对称性感知的原子排序与分层置换增强，无需显式结构模板即可注入晶体学先验知识，在MP-20和MPTS-52基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前晶体建模的生成模型大多针对特定任务设计，缺乏能够跨任务共享晶体表示的统一框架，限制了模型的泛化能力和效率。

Method: 提出Multimodal Crystal Flow (MCFlow)，使用独立的时间变量分别控制原子类型和晶体结构的生成轨迹，实现多任务统一建模；在标准Transformer中引入成分和对称性感知的原子排序机制，结合分层置换增强策略，在不依赖显式结构模板的情况下注入强成分和晶体学先验。

Result: 在MP-20和MPTS-52基准测试上，MCFlow在多个晶体生成任务中达到了与任务特定基线模型相当的性能水平。

Conclusion: MCFlow成功实现了多晶体生成任务的统一框架，通过多模态流模型和对称性感知设计，在保持竞争力的同时提供了跨任务共享表示的新思路。

Abstract: Crystal modeling spans a family of conditional and unconditional generation tasks across different modalities, including crystal structure prediction (CSP) and \emph{de novo} generation (DNG). While recent deep generative models have shown promising performance, they remain largely task-specific, lacking a unified framework that shares crystal representations across different generation tasks. To address this limitation, we propose \emph{Multimodal Crystal Flow (MCFlow)}, a unified multimodal flow model that realizes multiple crystal generation tasks as distinct inference trajectories via independent time variables for atom types and crystal structures. To enable multimodal flow in a standard transformer model, we introduce a composition- and symmetry-aware atom ordering with hierarchical permutation augmentation, injecting strong compositional and crystallographic priors without explicit structural templates. Experiments on the MP-20 and MPTS-52 benchmarks show that MCFlow achieves competitive performance against task-specific baselines across multiple crystal generation tasks.

</details>


### [34] [Coupled Cluster con MōLe: Molecular Orbital Learning for Neural Wavefunctions](https://arxiv.org/abs/2602.20232)
*Luca Thiede,Abdulrahman Aldossary,Andreas Burger,Jorge Arturo Campos-Gonzalez-Angulo,Ning Wang,Alexander Zook,Melisa Alkan,Kouhei Nakaji,Taylor Lee Patti,Jérôme Florian Gonthier,Mohammad Ghazi Vakili,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: 本文提出了一种名为Molecular Orbital Learning（MōLe）的等变机器学习架构，旨在从Hartree‑Fock分子轨道直接预测耦合簇（CC）理论的激发振幅。该模型在仅使用小分子平衡几何结构训练的情况下，展现出了优异的数据效率、对更大分子和非平衡几何的泛化能力，并能显著减少CC计算的收敛循环次数，为高精度波函数机器学习模型的发展奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 密度泛函理论（DFT）虽广泛用于分子性质计算，但其精度常不足以满足定量预测需求。耦合簇（CC）理论被视为量子化学的“金标准”，能够提供接近实验精度的结果，但计算成本极高，限制了其在大型体系中的应用。为此，研究者希望借助机器学习来加速CC计算，同时保持高精度，从而推动分子设计与力场方法的互补发展。

Method: 作者提出了Molecular Orbital Learning（MōLe）架构，这是一种等变机器学习模型。该模型以平均场Hartree‑Fock分子轨道作为输入，直接预测CC理论的核心数学对象——激发振幅。研究中对模型进行了多方位测试，包括数据效率、在更大分子及非平衡几何上的外推泛化能力，以及其对CC计算收敛循环次数的减少效果。

Result: 实验结果表明，MōLe在仅使用小分子平衡几何结构进行训练的情况下，能够高效地学习并泛化到更大的分子体系以及偏离平衡的几何构型，展现出出色的数据效率和外分布泛化能力。此外，该模型还能显著降低CC计算所需的收敛循环次数，从而在保持高精度的前提下提升计算效率。

Conclusion: MōLe的成功验证了利用机器学习直接预测波函数关键参数的可行性，为构建高精度、高效率的波函数机器学习模型奠定了重要基础。此类模型有望在分子设计中发挥重要作用，并与传统力场方法形成互补，推动量子化学计算的广泛应用。

Abstract: Density functional theory (DFT) is the most widely used method for calculating molecular properties; however, its accuracy is often insufficient for quantitative predictions. Coupled-cluster (CC) theory is the most successful method for achieving accuracy beyond DFT and for predicting properties that closely align with experiment. It is known as the ''gold standard'' of quantum chemistry. Unfortunately, the high computational cost of CC limits its widespread applicability. In this work, we present the Molecular Orbital Learning (MōLe) architecture, an equivariant machine learning model that directly predicts CC's core mathematical objects, the excitation amplitudes, from the mean-field Hartree-Fock molecular orbitals as inputs. We test various aspects of our model and demonstrate its remarkable data efficiency and out-of-distribution generalization to larger molecules and off-equilibrium geometries, despite being trained only on small equilibrium geometries. Finally, we also examine its ability to reduce the number of cycles required to converge CC calculations. MōLe can set the foundations for high-accuracy wavefunction-based ML architectures to accelerate molecular design and complement force-field approaches.

</details>


### [35] [The Truthfulness Spectrum Hypothesis](https://arxiv.org/abs/2602.20273)
*Zhuofan Josh Ying,Shauli Ravfogel,Nikolaus Kriegeskorte,Peter Hase*

Main category: cs.LG

TL;DR: 本研究提出"真实性谱假设"，认为语言模型表征空间中并存从广域通用到窄域特定的真实性编码方向。通过系统评估跨五种真实类型和两种说谎类型的探针泛化，发现线性探针在多数领域泛化良好但迎合性说谎除外；联合训练可恢复性能，证实通用方向存在。探针几何相似度可近乎完美预测泛化能力（R²=0.98），因果干预表明域特定方向更有效。后训练重塑真实性几何结构，使迎合性说谎远离其他类型。


<details>
  <summary>Details</summary>
Motivation: 调和关于大型语言模型是否线性编码真实性的矛盾发现，系统探究真实性表征的领域通用性与特异性问题。

Method: 系统评估探针在五种真实类型（定义、经验、逻辑、虚构、伦理）、迎合性与期望倒置说谎以及现有诚实基准上的泛化能力；采用线性探针、概念擦除方法分离真实性方向；通过因果干预测试方向有效性；利用马氏余弦相似度量化探针几何关系；分析后训练对真实性几何结构的重塑。

Result: 线性探针跨多数领域泛化良好但迎合性说谎失败；联合训练所有域可恢复性能，证实域通用方向存在；探针间马氏余弦相似度可近乎完美预测跨域泛化（R²=0.98）；概念擦除识别出域通用、域特定及子集共享三类方向；因果干预显示域特定方向比域通用方向更有效；后训练使迎合性说谎与其他真实类型的表征距离增大。

Conclusion: 结果支持真实性谱假设，证实表征空间中并存不同通用程度的真实性方向，后训练重塑其几何结构，为聊天模型的迎合倾向提供了表征层面的解释。

Abstract: Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec.

</details>


### [36] [Discrete Diffusion with Sample-Efficient Estimators for Conditionals](https://arxiv.org/abs/2602.20293)
*Karthik Elamvazhuthi,Abhijith Jayakumar,Andrey Y. Lokhov*

Main category: cs.LG

TL;DR: 本研究提出一种集成NeurISE估计器的离散去噪扩散框架，通过单站点条件概率参数化反向过程，在离散状态空间生成建模中实现样本高效学习，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散模型通常依赖分数函数的离散近似，存在样本效率低下的问题。单站点条件概率作为更基本的建模单元，结合高效估计方法，可能为离散生成建模提供更优解决方案。

Method: 提出轮询加噪去噪动力学框架，使用Neural Interaction Screening Estimator (NeurISE)直接估计单站点条件概率来参数化反向扩散过程，避免传统分数函数近似。

Result: 在合成Ising模型、MNIST、D-Wave量子退火机数据、Potts模型及一维量子系统上完成控制实验。在二元数据集中，该方法在总变差距离、互相关性和核密度估计指标上显著优于基于比率的主流方法。

Conclusion: 该框架为离散状态空间生成建模提供了样本高效的替代方案，展示了在物理、量子计算等领域的应用潜力，验证了单站点条件概率建模的有效性。

Abstract: We study a discrete denoising diffusion framework that integrates a sample-efficient estimator of single-site conditionals with round-robin noising and denoising dynamics for generative modeling over discrete state spaces. Rather than approximating a discrete analog of a score function, our formulation treats single-site conditional probabilities as the fundamental objects that parameterize the reverse diffusion process. We employ a sample-efficient method known as Neural Interaction Screening Estimator (NeurISE) to estimate these conditionals in the diffusion dynamics. Controlled experiments on synthetic Ising models, MNIST, and scientific data sets produced by a D-Wave quantum annealer, synthetic Potts model and one-dimensional quantum systems demonstrate the proposed approach. On the binary data sets, these experiments demonstrate that the proposed approach outperforms popular existing methods including ratio-based approaches, achieving improved performance in total variation, cross-correlations, and kernel density estimation metrics.

</details>


### [37] [Learning to Solve Complex Problems via Dataset Decomposition](https://arxiv.org/abs/2602.20296)
*Wanru Zhao,Lucas Caccia,Zhengyan Shi,Minseon Kim,Weijia Xu,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 该论文提出反向课程学习方法，通过师生框架递归分解复杂数据，基于结构复杂度和概念深度评分生成课程，在数学和代码数据集上表现优于标准训练。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习按难度组织现有数据，但复杂任务难以直接学习。本研究反向思考，通过递归分解复杂数据为简单组件，帮助学生模型逐步掌握困难任务。

Method: 提出反向课程生成方法，采用师生框架。教师模型具备逐步推理能力，递归生成简单版本示例；学生模型渐进学习。设计基于结构复杂度和概念深度的数据难度评分系统，构建分解后的课程。

Result: 在MATH和AIME数学数据集及代码生成数据集上的实验表明，使用该方法生成的课程训练的模型性能优于在原始数据集上的标准训练。

Conclusion: 反向课程学习方法有效，能显著提升模型在复杂任务上的学习效果。

Abstract: Curriculum learning is a class of training strategies that organizes the data being exposed to a model by difficulty, gradually from simpler to more complex examples. This research explores a reverse curriculum generation approach that recursively decomposes complex datasets into simpler, more learnable components. We propose a teacher-student framework where the teacher is equipped with the ability to reason step-by-step, which is used to recursively generate easier versions of examples, enabling the student model to progressively master difficult tasks. We propose a novel scoring system to measure data difficulty based on its structural complexity and conceptual depth, allowing curriculum construction over decomposed data. Experiments on math datasets (MATH and AIME) and code generation datasets demonstrate that models trained with curricula generated by our approach exhibit superior performance compared to standard training on original datasets.

</details>


### [38] [In-context Pre-trained Time-Series Foundation Models adapt to Unseen Tasks](https://arxiv.org/abs/2602.20307)
*Shangqing Xu,Harshavardhan Kamarthi,Haoxin Liu,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 该论文提出了一种名为"上下文时间序列预训练"（ICTP）的框架，通过重构预训练数据来增强时间序列基础模型的上下文学习能力，使其无需微调即可适应未见任务，性能提升约11.4%。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型（TSFMs）在多样数据集和任务上展现出强大泛化能力，但现有模型通常需要针对特定任务进行预训练，且在没有微调的情况下难以泛化到未见任务，限制了模型的灵活性和实用性。

Method: 作者提出将上下文学习（ICL）能力集成到TSFMs中，使模型能够通过上下文中提供的输入输出关系在测试时动态适应。具体开发了"上下文时间序列预训练"（ICTP）框架，通过重构原始预训练数据来装备骨干TSFM的ICL能力。

Result: 实验表明，该上下文学习训练（ICT）方法在不需微调的情况下，将现有先进时间序列基础模型在未见任务上的性能提升了约11.4%。

Conclusion: 通过在预训练阶段引入上下文学习机制，时间序列基础模型能够更好地适应未见任务，显著提升零样本泛化性能，为无需微调的时间序列建模提供了有效解决方案。

Abstract: Time-series foundation models (TSFMs) have demonstrated strong generalization capabilities across diverse datasets and tasks. However, existing foundation models are typically pre-trained to enhance performance on specific tasks and often struggle to generalize to unseen tasks without fine-tuning. To address this limitation, we propose augmenting TSFMs with In-Context Learning (ICL) capabilities, enabling them to perform test-time inference by dynamically adapting to input-output relationships provided within the context. Our framework, In-Context Time-series Pre-training (ICTP), restructures the original pre-training data to equip the backbone TSFM with ICL capabilities, enabling adaptation to unseen tasks. Experiments demonstrate that ICT improves the performance of state-of-the-art TSFMs by approximately 11.4% on unseen tasks without requiring fine-tuning.

</details>


### [39] [CaDrift: A Time-dependent Causal Generator of Drifting Data Streams](https://arxiv.org/abs/2602.20329)
*Eduardo V. L. Barboza,Jean Paul Barddal,Robert Sabourin,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 本文提出Causal Drift Generator (CaDrift)，一种基于结构因果模型的时间相关合成数据生成框架。该框架通过漂移结构因果模型的映射函数，生成具有可控分布漂移和协变量漂移的无限数据流，改变特征与目标间的因果关系，并利用因果干预模拟偶发扰动，为评估机器学习方法在数据演化场景下的表现提供了有效工具。


<details>
  <summary>Details</summary>
Motivation: 现实世界中数据分布会随时间演化（数据漂移），这对机器学习模型的鲁棒性和适应性构成严峻挑战。现有研究缺乏能够可控生成基于因果关系的漂移数据流的工具，难以系统评估算法在非平稳环境下的性能。本研究旨在填补这一空白，为演化数据分析提供可靠的实验基准。

Method: CaDrift框架以结构因果模型（SCMs）为基础，通过时间性地漂移SCM的映射函数来合成各类分布漂移和协变量漂移，从而改变特征与目标之间的底层因果机制。同时，框架利用因果建模中的干预机制来模拟偶发性扰动事件，生成具有时间依赖性的数据流。该方法实现了对漂移类型、幅度和时机的精确控制。

Result: 实验结果表明，在模拟的分布漂移事件发生后，分类器的准确率会出现明显下降，随后经历一个逐步恢复的过程。这一结果验证了CaDrift能够有效模拟真实世界中的数据漂移现象，为评估漂移检测与适应算法提供了可靠的实验平台。

Conclusion: CaDrift框架成功实现了基于因果关系的可控数据漂移生成，为机器学习在演化数据环境下的算法评估提供了新工具。实验证实其模拟效果符合预期，能够有效反映模型在数据分布变化时的性能变化。该框架已在GitHub开源，可供研究社区使用。

Abstract: This work presents Causal Drift Generator (CaDrift), a time-dependent synthetic data generator framework based on Structural Causal Models (SCMs). The framework produces a virtually infinite combination of data streams with controlled shift events and time-dependent data, making it a tool to evaluate methods under evolving data. CaDrift synthesizes various distributional and covariate shifts by drifting mapping functions of the SCM, which change underlying cause-and-effect relationships between features and the target. In addition, CaDrift models occasional perturbations by leveraging interventions in causal modeling. Experimental results show that, after distributional shift events, the accuracy of classifiers tends to drop, followed by a gradual retrieval, confirming the generator's effectiveness in simulating shifts. The framework has been made available on GitHub.

</details>


### [40] [Emergent Manifold Separability during Reasoning in Large Language Models](https://arxiv.org/abs/2602.20338)
*Alexandre Polo,Chanwoo Chun,SueYeon Chung*

Main category: cs.LG

TL;DR: 该研究通过流形容量理论揭示大型语言模型在思维链推理中呈现瞬态几何脉冲：概念流形在计算前瞬间解缠为线性可分子空间，计算后迅速压缩，提出了动态流形管理机制以优化残差流带宽。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链提示显著提升大模型推理能力，但其表征几何的时序动态尚不明确；传统线性探针无法区分信息可检索性与几何可处理性的本质差异，需新理论框架量化推理过程中的表征演变。

Method: 将流形容量理论应用于组合式布尔逻辑任务，在不训练探针的前提下直接量化潜在表征的线性可分性，追踪思维链推理中表征几何结构的动态变化。

Result: 发现推理表现为瞬态几何脉冲——计算前概念流形瞬间解缠为线性可分离子空间，计算后迅速压缩；该动态行为与线性探针准确率（计算后仍长期高位）显著不同，揭示信息可检索性与几何可处理性存在根本区别。

Conclusion: 提出"动态流形管理"机制：模型在推理链中动态调节表征容量以优化残差流带宽，为理解大模型推理提供了新几何视角，阐明模型如何在计算关键时刻准备与处置信息。

Abstract: Chain-of-Thought (CoT) prompting significantly improves reasoning in Large Language Models, yet the temporal dynamics of the underlying representation geometry remain poorly understood. We investigate these dynamics by applying Manifold Capacity Theory (MCT) to a compositional Boolean logic task, allowing us to quantify the linear separability of latent representations without the confounding factors of probe training. Our analysis reveals that reasoning manifests as a transient geometric pulse, where concept manifolds are untangled into linearly separable subspaces immediately prior to computation and rapidly compressed thereafter. This behavior diverges from standard linear probe accuracy, which remains high long after computation, suggesting a fundamental distinction between information that is merely retrievable and information that is geometrically prepared for processing. We interpret this phenomenon as \emph{Dynamic Manifold Management}, a mechanism where the model dynamically modulates representational capacity to optimize the bandwidth of the residual stream throughout the reasoning chain.

</details>


### [41] [Hierarchical Molecular Representation Learning via Fragment-Based Self-Supervised Embedding Prediction](https://arxiv.org/abs/2602.20344)
*Jiele Wu,Haozhe Ma,Zhihan Guo,Thanh Vinh Vo,Tze Yun Leong*

Main category: cs.LG

TL;DR: 本文提出GraSPNet，一种用于分子图表示学习的层次化自监督学习框架。该方法通过显式建模原子级和碎片级语义，利用掩码预测和多级消息传递学习多分辨率结构信息，在分子性质预测任务中优于现有自监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习方法主要关注节点或边级别信息，忽略了化学上至关重要的子结构信息，这在标记成本高昂的分子图分析领域限制了其应用效果。

Method: 提出GraSPNet框架：1) 无需预定义词典即可将分子图分解为化学上有意义的碎片；2) 通过节点级和碎片级双层次的掩码语义预测任务进行多尺度消息传递；3) 实现层次化语义监督以学习表达性强的多分辨率分子表示。

Result: 在多个分子性质预测基准测试上，GraSPNet在迁移学习设置下持续优于现有最先进图自监督学习方法，学习到的表示具有明确的化学意义和可迁移性。

Conclusion: 层次化语义监督能有效捕捉分子结构的多尺度特征，GraSPNet为分子图自监督学习提供了新范式，证明了在无标注情况下学习化学相关表示的可行性。

Abstract: Graph self-supervised learning (GSSL) has demonstrated strong potential for generating expressive graph embeddings without the need for human annotations, making it particularly valuable in domains with high labeling costs such as molecular graph analysis. However, existing GSSL methods mostly focus on node- or edge-level information, often ignoring chemically relevant substructures which strongly influence molecular properties. In this work, we propose Graph Semantic Predictive Network (GraSPNet), a hierarchical self-supervised framework that explicitly models both atomic-level and fragment-level semantics. GraSPNet decomposes molecular graphs into chemically meaningful fragments without predefined vocabularies and learns node- and fragment-level representations through multi-level message passing with masked semantic prediction at both levels. This hierarchical semantic supervision enables GraSPNet to learn multi-resolution structural information that is both expressive and transferable. Extensive experiments on multiple molecular property prediction benchmarks demonstrate that GraSPNet learns chemically meaningful representations and consistently outperforms state-of-the-art GSSL methods in transfer learning settings.

</details>


### [42] [Momentum Guidance: Plug-and-Play Guidance for Flow Models](https://arxiv.org/abs/2602.20360)
*Runlong Liao,Jian Yu,Baiyu Su,Chi Zhang,Lizhang Chen,Qiang Liu*

Main category: cs.LG

TL;DR: 本文提出动量引导（Momentum Guidance, MG），一种利用常微分方程（ODE）轨迹本身进行引导的新方法。该方法通过指数移动平均外推当前速度，在不增加计算成本的情况下提升流模型条件生成的质量，并可与无分类器引导（CFG）结合获得更好效果。


<details>
  <summary>Details</summary>
Motivation: 流基生成模型在条件生成时存在样本模糊、细节缺失的问题。现有方法如无分类器引导（CFG）虽能提高保真度，但会使推理成本翻倍并降低样本多样性，因此需要开发一种低成本且有效的引导技术。

Method: 提出动量引导（MG），利用ODE轨迹信息，通过指数移动平均（EMA）外推当前速度来指导生成过程。该方法保持标准单步单评估的计算成本，并可与CFG结合使用。

Result: 在ImageNet-256基准上，MG在无CFG情况下FID平均提升36.68%，有CFG情况下提升25.52%，64步采样时达到1.597的FID。在Stable Diffusion 3和FLUX.1-dev等大模型上也验证了持续的质量提升。

Conclusion: MG是一种高效且有效的引导方法，能在不增加额外计算负担的前提下显著提升流模型的生成质量。它与CFG具有互补性，可在各种采样设置和大规模模型上稳定提高生成性能。

Abstract: Flow-based generative models have become a strong framework for high-quality generative modeling, yet pretrained models are rarely used in their vanilla conditional form: conditional samples without guidance often appear diffuse and lack fine-grained detail due to the smoothing effects of neural networks. Existing guidance techniques such as classifier-free guidance (CFG) improve fidelity but double the inference cost and typically reduce sample diversity. We introduce Momentum Guidance (MG), a new dimension of guidance that leverages the ODE trajectory itself. MG extrapolates the current velocity using an exponential moving average of past velocities and preserves the standard one-evaluation-per-step cost. It matches the effect of standard guidance without extra computation and can further improve quality when combined with CFG. Experiments demonstrate MG's effectiveness across benchmarks. Specifically, on ImageNet-256, MG achieves average improvements in FID of 36.68% without CFG and 25.52% with CFG across various sampling settings, attaining an FID of 1.597 at 64 sampling steps. Evaluations on large flow-based models like Stable Diffusion 3 and FLUX.1-dev further confirm consistent quality enhancements across standard metrics.

</details>


### [43] [cc-Shapley: Measuring Multivariate Feature Importance Needs Causal Context](https://arxiv.org/abs/2602.20396)
*Jörg Martin,Stefan Haufe*

Main category: cs.LG

TL;DR: 传统Shapley值在多变量分析中会产生虚假关联，论文提出融入因果结构的cc-Shapley方法，能有效消除碰撞偏差并改变特征重要性排序。


<details>
  <summary>Details</summary>
Motivation: 可解释AI旨在揭示真实相关特征，但纯数据驱动的Shapley值会因碰撞偏差和抑制效应产生误导性归因，亟需因果知识来识别和纠正这些偏差。

Method: 提出cc-Shapley（因果上下文Shapley），通过干预性方法利用数据生成过程的因果结构知识，分析特征在其余特征的因果上下文中的相关性，从理论上消除虚假关联。

Result: 在合成和真实世界数据集上的比较表明，cc-Shapley相比传统Shapley值能消除或反转虚假关联，提供更准确的特征归因结果。

Conclusion: 因果知识是正确解释特征重要性的必要条件，cc-Shapley方法通过引入因果结构有效解决了传统方法的偏差问题，为可解释AI提供了更可靠的特征归因框架。

Abstract: Explainable artificial intelligence promises to yield insights into relevant features, thereby enabling humans to examine and scrutinize machine learning models or even facilitating scientific discovery. Considering the widespread technique of Shapley values, we find that purely data-driven operationalization of multivariate feature importance is unsuitable for such purposes. Even for simple problems with two features, spurious associations due to collider bias and suppression arise from considering one feature only in the observational context of the other, which can lead to misinterpretations. Causal knowledge about the data-generating process is required to identify and correct such misleading feature attributions. We propose cc-Shapley (causal context Shapley), an interventional modification of conventional observational Shapley values leveraging knowledge of the data's causal structure, thereby analyzing the relevance of a feature in the causal context of the remaining features. We show theoretically that this eradicates spurious association induced by collider bias. We compare the behavior of Shapley and cc-Shapley values on various, synthetic, and real-world datasets. We observe nullification or reversal of associations compared to univariate feature importance when moving from observational to cc-Shapley.

</details>


### [44] [Three Concrete Challenges and Two Hopes for the Safety of Unsupervised Elicitation](https://arxiv.org/abs/2602.20400)
*Callum Canavan,Aditya Shrivastava,Allison Qi,Jonathan Michala,Fabien Roger*

Main category: cs.LG

TL;DR: 本文通过构建缺乏理想化特征的压力测试数据集，揭示现有无监督引导和简单到困难泛化技术在真实性引导上的评估可能存在高估，发现这些技术均无法稳定应对挑战，呼吁未来研究应优先解决这些数据集偏差问题。


<details>
  <summary>Details</summary>
Motivation: 尽管无监督引导和简单到困难泛化技术已能提升模型在多任务上的准确性，但其评估所用数据集往往具有三大理想化特征（无强于真实性的干扰特征、训练集平衡、仅含明确定义答案），这可能掩盖方法在更真实场景下的缺陷，导致评估结果过于乐观。

Method: 作者设计了三类分别缺失上述理想特征的压力测试数据集，对当前主流的无监督引导与简单到困难泛化技术进行系统性评估，并进一步探索了集成学习及两类技术结合策略的缓解效果。

Result: 实验表明，在各类挑战性数据集上，所有被测试的技术均无法保持稳定性能；尽管集成方法以及技术组合能在一定程度上缓解性能下降，但效果有限，无法根本解决问题。

Conclusion: 这些发现表明，克服由数据集偏差带来的挑战应是未来无监督引导研究的核心重点，当前方法在真实场景下的鲁棒性与泛化能力仍有待大幅提升。

Abstract: To steer language models towards truthful outputs on tasks which are beyond human capability, previous work has suggested training models on easy tasks to steer them on harder ones (easy-to-hard generalization), or using unsupervised training algorithms to steer models with no external labels at all (unsupervised elicitation). Although techniques from both paradigms have been shown to improve model accuracy on a wide variety of tasks, we argue that the datasets used for these evaluations could cause overoptimistic evaluation results. Unlike many real-world datasets, they often (1) have no features with more salience than truthfulness, (2) have balanced training sets, and (3) contain only data points to which the model can give a well-defined answer. We construct datasets that lack each of these properties to stress-test a range of standard unsupervised elicitation and easy-to-hard generalization techniques. We find that no technique reliably performs well on any of these challenges. We also study ensembling and combining easy-to-hard and unsupervised techniques, and find they only partially mitigate performance degradation due to these challenges. We believe that overcoming these challenges should be a priority for future work on unsupervised elicitation.

</details>


### [45] [Wasserstein Distributionally Robust Online Learning](https://arxiv.org/abs/2602.20403)
*Guixian Chen,Salar Fattahi,Soroosh Shafiee*

Main category: cs.LG

TL;DR: 本文研究了基于 Wasserstein 不确定性集的分布鲁棒在线学习，提出了一种在线鞍点随机博弈框架，使其收敛到与离线 Wasserstein DRO 问题相同的鲁棒纳什均衡；针对分段凹损失函数，设计了一种利用问题几何结构的定制算法，将最坏期望问题转化为经典且可处理的预算分配问题，从而在 Gurobi 等最先进求解器上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 在风险厌恶的在线决策中，学习者需要在不确定分布的情况下进行序贯更新；虽然离线 Wasserstein 分布鲁棒优化（DRO）已较为成熟，但其在线扩展在收敛性和计算效率方面仍存在重大挑战。为此，亟需一种既能保证收敛又能高效求解在线分布鲁棒学习问题的方法。

Method: 首先，将问题建模为决策者与对手之间的在线鞍点随机博弈，其中对手在最坏分布（取自以历史观测为中心的 Wasserstein 模糊集）上操作；其次，提出一个通用框架，使其收敛到与对应离线 Wasserstein DRO 问题相同的鲁棒纳什均衡；再次，针对重要的分段凹损失函数类别，提出一种定制算法，通过利用问题的几何特性，将固有的无限维最坏期望问题映射到经典且易解的预算分配问题。

Result: 所提框架能够收敛到与离线 Wasserstein DRO 问题一致的鲁棒纳什均衡；在分段凹损失函数下，定制算法相比 Gurobi 等最先进求解器实现了显著的计算加速，实验验证了其在大规模在线学习任务中的效率和可扩展性。

Conclusion: 本研究成功桥接了在线学习与分布鲁棒优化，既提供了理论上的收敛保证，又通过创新的最坏期望问题转化实现了实际计算性能的提升；所揭示的最坏期望问题与预算分配问题之间的内在联系，为未来鲁棒学习算法的设计开辟了新方向。

Abstract: We study distributionally robust online learning, where a risk-averse learner updates decisions sequentially to guard against worst-case distributions drawn from a Wasserstein ambiguity set centered at past observations. While this paradigm is well understood in the offline setting through Wasserstein Distributionally Robust Optimization (DRO), its online extension poses significant challenges in both convergence and computation. In this paper, we address these challenges. First, we formulate the problem as an online saddle-point stochastic game between a decision maker and an adversary selecting worst-case distributions, and propose a general framework that converges to a robust Nash equilibrium coinciding with the solution of the corresponding offline Wasserstein DRO problem. Second, we address the main computational bottleneck, which is the repeated solution of worst-case expectation problems. For the important class of piecewise concave loss functions, we propose a tailored algorithm that exploits problem geometry to achieve substantial speedups over state-of-the-art solvers such as Gurobi. The key insight is a novel connection between the worst-case expectation problem, an inherently infinite-dimensional optimization problem, and a classical and tractable budget allocation problem, which is of independent interest.

</details>


### [46] [$κ$-Explorer: A Unified Framework for Active Model Estimation in MDPs](https://arxiv.org/abs/2602.20404)
*Xihe Gu,Urbashi Mitra,Tara Javidi*

Main category: cs.LG

TL;DR: 针对表格型MDPs的探索问题，本文提出κ-Explorer算法，通过设计一类含参凹目标函数U_κ统一建模内在估计复杂度和外在访问频率，并利用Frank-Wolfe优化实现高效主动探索。


<details>
  <summary>Details</summary>
Motivation: 在具有完美状态可观测性的表格型MDPs中，每个轨迹提供状态-动作对条件转移分布的有效样本。准确的模型估计依赖于探索策略如何根据各转移分布的内在复杂性分配访问频率。现有方法缺乏对估计复杂度和访问频率的统一建模，无法有效平衡平均情况与最坏情况的估计误差。

Method: 提出一类可分解的凹目标函数族U_κ，通过曲率参数κ统一不同全局目标；基于U_κ的闭式梯度，设计κ-Explorer算法，采用Frank-Wolfe风格优化状态-动作占用度量；利用U_κ的边际收益递减结构优先探索欠采样和高方差转移。

Result: 为κ-Explorer建立了紧致的遗憾界；开发了全在线且计算高效的替代算法；在基准MDPs上的实验表明κ-Explorer性能优于现有探索策略。

Conclusion: 该工作通过统一建模内在复杂度和外在频率，为表格MDPs的主动探索提供了通用框架，κ-Explorer算法在理论和实践中均表现出色。

Abstract: In tabular Markov decision processes (MDPs) with perfect state observability, each trajectory provides active samples from the transition distributions conditioned on state-action pairs. Consequently, accurate model estimation depends on how the exploration policy allocates visitation frequencies in accordance with the intrinsic complexity of each transition distribution. Building on recent work on coverage-based exploration, we introduce a parameterized family of decomposable and concave objective functions $U_κ$ that explicitly incorporate both intrinsic estimation complexity and extrinsic visitation frequency. Moreover, the curvature $κ$ provides a unified treatment of various global objectives, such as the average-case and worst-case estimation error objectives. Using the closed-form characterization of the gradient of $U_κ$, we propose $κ$-Explorer, an active exploration algorithm that performs Frank-Wolfe-style optimization over state-action occupancy measures. The diminishing-returns structure of $U_κ$ naturally prioritizes underexplored and high-variance transitions, while preserving smoothness properties that enable efficient optimization. We establish tight regret guarantees for $κ$-Explorer and further introduce a fully online and computationally efficient surrogate algorithm for practical use. Experiments on benchmark MDPs demonstrate that $κ$-Explorer provides superior performance compared to existing exploration strategies.

</details>


### [47] [GauS: Differentiable Scheduling Optimization via Gaussian Reparameterization](https://arxiv.org/abs/2602.20427)
*Yaohui Cai,Vesal Bakhtazad,Cunxi Yu,Zhiru Zhang*

Main category: cs.LG

TL;DR: 本文提出GauS，一种基于高斯分布的可微分算子调度框架，通过连续随机松弛替代传统分类分布，有效捕捉时间的序数特性并大幅压缩参数空间，在GPU上实现高效并行搜索，首次为复杂流水线调度问题提供可微分 formulation，在多项基准测试中达到帕累托最优。


<details>
  <summary>Details</summary>
Motivation: 传统算子调度方法（精确求解器或启发式算法）效率有限，现有可微分方法虽尝试使用基于梯度的搜索，但依赖分类分布导致两个根本缺陷：1）无法刻画时间维度固有的序数关系；2）参数空间随调度长度呈组合爆炸式增长，严重制约可扩展性和实用性。亟需一种能同时保持时间结构特性并实现高效优化的全新建模方式。

Method: GauS的核心创新在于将离散调度决策松弛为连续高斯随机变量：1）用高斯分布建模每个算子的启动时间，天然保留时间顺序信息；2）通过重参数化技巧实现可微分采样，支持GPU并行化计算；3）设计灵活的损失函数编码多种目标（如延迟、资源）和约束（如数据依赖、资源冲突）；4）在反向传播中采用得分函数估计器（score function estimator）结合控制变量技术降低方差，实现端到端高效训练。

Result: 在多个标准调度基准测试上，GauS能够找到帕累托最优解集，在调度质量上全面优于或媲美传统最优/启发式算法。相比基于分类分布的可微分方法，其优化空间降低数个数量级，训练速度显著提升，同时保持了表达复杂时序约束的能力，证明了高斯建模在序数优化问题中的优越性。

Conclusion: GauS首次成功构建了流水线调度的可微分框架，通过高斯随机松弛有效解决了时间序数建模和参数空间爆炸两大难题。该方法兼具理论创新性与工程实用性，为编译器与硬件综合领域的调度问题提供了新范式，展现了可微分方法在复杂组合优化任务中的巨大潜力，未来可扩展至更广泛的资源分配与调度场景。

Abstract: Efficient operator scheduling is a fundamental challenge in software compilation and hardware synthesis. While recent differentiable approaches have sought to replace traditional ones like exact solvers or heuristics with gradient-based search, they typically rely on categorical distributions that fail to capture the ordinal nature of time and suffer from a parameter space that scales poorly. In this paper, we propose a novel differentiable framework, GauS, that models operator scheduling as a stochastic relaxation using Gaussian distributions, which fully utilize modern parallel computing devices like GPUs. By representing schedules as continuous Gaussian variables, we successfully capture the ordinal nature of time and reduce the optimization space by orders of magnitude. Our method is highly flexible to represent various objectives and constraints, which provides the first differentiable formulation for the complex pipelined scheduling problem. We evaluate our method on a range of benchmarks, demonstrating that Gaus achieves Pareto-optimal results.

</details>


### [48] [Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference](https://arxiv.org/abs/2602.20449)
*Anna Hart,Chi Han,Jeonghwan Kim,Huimin Zhao,Heng Ji*

Main category: cs.LG

TL;DR: 本文研究蛋白质语言模型(PLMs)与天然语言处理的差异，通过适配早期退出技术，使模型能自动选择中间层表示，在蛋白质非结构属性预测任务上同时实现精度和效率的提升。


<details>
  <summary>Details</summary>
Motivation: 尽管蛋白质语言仅有20种氨基酸，却拥有极其丰富的功能空间，这与天然语言存在关键差异。这种差异激励研究者探索transformer架构在蛋白质领域的独特工作机制，以及如何更好地利用PLMs解决蛋白质相关任务。

Method: 研究首先直接比较蛋白质与自然语言领域中注意力头层间信息分布的差异，然后将自然语言中用于提升效率的早期退出技术进行适配，使其能根据具体任务和蛋白质自动选择PLMs中间层的表示。

Result: 在多个模型和非结构属性预测任务上，该方法获得0.4至7.01个百分点的性能提升，同时效率提高超过10%。

Conclusion: 该工作开创了直接比较语言模型在蛋白质领域行为变化的研究方向，推动了生物领域语言建模的进展。

Abstract: Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space despite a vocabulary of only 20 amino acids. These differences motivate research into how transformer-based architectures operate differently in the protein domain and how we can better leverage PLMs to solve protein-related tasks. In this work, we begin by directly comparing how the distribution of information stored across layers of attention heads differs between the protein and natural language domain. Furthermore, we adapt a simple early-exit technique-originally used in the natural language domain to improve efficiency at the cost of performance-to achieve both increased accuracy and substantial efficiency gains in protein non-structural property prediction by allowing the model to automatically select protein representations from the intermediate layers of the PLMs for the specific task and protein at hand. We achieve performance gains ranging from 0.4 to 7.01 percentage points while simultaneously improving efficiency by over 10 percent across models and non-structural prediction tasks. Our work opens up an area of research directly comparing how language models change behavior when moved into the protein domain and advances language modeling in biological domains.

</details>


### [49] [Oracle-Robust Online Alignment for Large Language Models](https://arxiv.org/abs/2602.20457)
*Zimeng Li,Mudit Gaur,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 针对存在偏好误设的在线大语言模型对齐问题，提出一种基于最坏情况优化的鲁棒框架。理论证明对数线性策略下鲁棒目标可精确分解为原始损失与敏感性惩罚项，并设计投影随机复合更新算法，达到 $\widetilde{O}(\varepsilon^{-2})$ 的oracle复杂度。


<details>
  <summary>Details</summary>
Motivation: 在线大语言模型对齐依赖偏好反馈，但实际反馈常偏离理想ground-truth oracle。该问题因数据收集与策略更新的耦合性构成双层强化学习难题。尽管SAIL框架已将其简化为单层问题，但如何在不确定性下实现鲁棒对齐仍是亟待解决的关键问题。

Method: 1) 引入逐点式oracle不确定性集，构建最坏情况鲁棒优化目标；2) 针对对数线性策略，推导鲁棒目标的闭式分解结构；3) 设计适用于弱凸复合目标的投影随机梯度更新算法。

Result: 1) 证明鲁棒目标可精确分解为原始损失函数与显式敏感性惩罚项；2) 建立算法收敛性分析，证明达到近似平稳点需要 $\widetilde{O}(\varepsilon^{-2})$ oracle调用次数。

Conclusion: 该工作为在线大模型对齐提供了理论鲁棒性保证，其闭式分解结构揭示了鲁棒性代价的本质，而计算可行的算法设计为实际应用提供了可靠方案。

Abstract: We study online alignment of large language models under misspecified preference feedback, where the observed preference oracle deviates from an ideal but unknown ground-truth oracle. The online LLM alignment problem is a bi-level reinforcement problem due to the coupling between data collection and policy updates. Recently, the problem has been reduced to tractable single-level objective in the SAIL (Self-Improving Efficient Online Alignment) framework. In this paper, we introduce a pointwise oracle uncertainty set in this problem and formulate an oracle-robust online alignment objective as a worst-case optimization problem. For log-linear policies, we show that this robust objective admits an exact closed-form decomposition into the original loss function plus an explicit sensitivity penalty. We develop projected stochastic composite updates for the resulting weakly convex objective and prove $\widetilde{O}(\varepsilon^{-2})$ oracle complexity for reaching approximate stationarity.

</details>


### [50] [A Long-Short Flow-Map Perspective for Drifting Models](https://arxiv.org/abs/2602.20463)
*Zhiqi Li,Bo Zhu*

Main category: cs.LG

TL;DR: 本文通过半群一致的长短流映射分解重新诠释漂移模型，提出一种新的似然学习框架。该方法将全局传输过程分解为长时程流映射与具有闭式最优速度的短时终端流映射，并证明当终端时间趋于零时可精确恢复漂移场及保持流映射一致性的保守脉冲项。通过理论分析与基准测试验证了框架有效性，并提供了特征空间优化的理论解释。


<details>
  <summary>Details</summary>
Motivation: 现有漂移模型缺乏从流映射分解角度的理论解释，难以建立与密度演化过程的直接关联。为此，作者旨在通过半群一致的数学框架，为漂移模型提供更严谨的理论基础，增强生成过程的可解释性。

Method: 采用半群一致的长短流映射分解方法：1) 将全局传输过程分解为长时程流映射与短时终端流映射；2) 推导终端流映射的闭式最优速度表达式；3) 通过极限分析证明终端区间趋于零时分解可精确恢复漂移场及维持流映射一致性的保守脉冲项；4) 基于该分解提出与密度演化对齐的似然学习新公式。

Result: 理论证明了长短流映射分解的数学性质及其与漂移模型的等价性；在基准测试上的实证评估验证了框架有效性；同时给出了特征空间优化的理论解释。

Conclusion: 该研究成功构建了漂移模型的新理论解释框架，通过流映射分解与密度演化的对齐为生成建模提供了更坚实的数学基础，并指出了若干未来研究的开放性问题。

Abstract: This paper provides a reinterpretation of the Drifting Model~\cite{deng2026generative} through a semigroup-consistent long-short flow-map factorization. We show that a global transport process can be decomposed into a long-horizon flow map followed by a short-time terminal flow map admitting a closed-form optimal velocity representation, and that taking the terminal interval length to zero recovers exactly the drifting field together with a conservative impulse term required for flow-map consistency. Based on this perspective, we propose a new likelihood learning formulation that aligns the long-short flow-map decomposition with density evolution under transport. We validate the framework through both theoretical analysis and empirical evaluations on benchmark tests, and further provide a theoretical interpretation of the feature-space optimization while highlighting several open problems for future study.

</details>


### [51] [Actor-Curator: Co-adaptive Curriculum Learning via Policy-Improvement Bandits for RL Post-Training](https://arxiv.org/abs/2602.20532)
*Zhengyao Gu,Jonathan Light,Raul Astudillo,Ziyu Ye,Langzhou He,Henry Peng Zou,Wei Cheng,Santiago Paternain,Philip S. Yu,Yisong Yue*

Main category: cs.LG

TL;DR: 本文提出ACTOR-CURATOR，一种用于大语言模型强化学习后训练的全自动课程学习框架。该方法将问题选择建模为非平稳随机老虎机问题，基于在线镜像下降推导损失函数并提供遗憾界保证。实验显示，在AIME2024和ARC-1D等推理基准上分别实现28.6%和30.5%的相对性能提升，训练速度最高提升80%，显著优于均匀采样和强基线方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练依赖海量异构数据，有效课程学习至关重要但极具挑战性。传统手动课程设计效率低下且难以扩展，现有自动化方法在处理大规模问题库和非平稳性方面存在局限。因此，亟需可扩展的全自动框架来动态优化样本选择，以最大化策略性能改进效率。

Method: ACTOR-CURATOR框架学习一个神经策展人动态选择训练问题。核心方法包括：1) 将问题选择形式化为非平稳随机老虎机问题；2) 基于在线随机镜像下降推导出原则性损失函数；3) 在部分反馈机制下建立遗憾界保证。该框架完全自动化，直接优化预期策略性能提升。

Result: 在多个挑战性推理基准测试中，ACTOR-CURATOR持续优于均匀采样和强课程学习基线。具体结果：AIME2024相对提升28.6%，ARC-1D相对提升30.5%，训练效率最高提升80%。同时显著改善训练稳定性和样本效率。

Conclusion: ACTOR-CURATOR为大语言模型可扩展后训练提供了强大实用的解决方案。其理论保证与实证效果表明，该框架能有效应对大规模异构数据环境下的课程学习挑战，为未来大模型训练提供了重要的方法论支撑。

Abstract: Post-training large foundation models with reinforcement learning typically relies on massive and heterogeneous datasets, making effective curriculum learning both critical and challenging. In this work, we propose ACTOR-CURATOR, a scalable and fully automated curriculum learning framework for reinforcement learning post-training of large language models (LLMs). ACTOR-CURATOR learns a neural curator that dynamically selects training problems from large problem banks by directly optimizing for expected policy performance improvement. We formulate problem selection as a non-stationary stochastic bandit problem, derive a principled loss function based on online stochastic mirror descent, and establish regret guarantees under partial feedback. Empirically, ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across a wide range of challenging reasoning benchmarks, demonstrating improved training stability and efficiency. Notably, it achieves relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline and up to 80% speedup. These results suggest that ACTOR-CURATOR is a powerful and practical approach for scalable LLM post-training.

</details>


### [52] [GATES: Self-Distillation under Privileged Context with Consensus Gating](https://arxiv.org/abs/2602.20574)
*Alex Stein,Furong Huang,Tom Goldstein*

Main category: cs.LG

TL;DR: 针对无可靠监督信号（无标签、奖励或外部评估）场景，研究文档 grounded 问答中非对称上下文的自蒸馏问题。提出通过抽样多个文档 grounded 推理轨迹并基于导师共识生成在线监督，利用完整轨迹进行蒸馏，显著提升了学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习依赖可靠监督信号，但许多场景缺乏真实标签、可验证奖励或外部评估者。在文档 grounded 问答中，训练时导师可访问文档而测试时学生只能基于问题作答，这种非对称性使知识蒸馏面临挑战，尤其当导师答案不可靠时。

Method: 提出共识门控的轨迹蒸馏方法：1）单模型同时作为导师和学生；2）采样多个文档 grounded 推理轨迹，以导师自身的多轨迹一致性作为可靠性信号；3）基于共识筛选高质量轨迹；4）蒸馏时使用完整推理轨迹（而非仅最终答案）提供密集稳定的学习信号。

Result: 实验效果显著：域内非对称评估准确率从46.0%提升至62.0%；公开文档无关数学基准测试的maj@8平均准确率从20.2%提升至35.4%。

Conclusion: 在无可靠监督信号时，利用模型自身多轨迹共识作为在线监督并结合完整推理轨迹蒸馏，可有效解决非对称知识迁移问题，为弱监督学习提供了新范式。

Abstract: We study self-distillation in settings where supervision is unreliable: there are no ground truth labels, verifiable rewards, or external graders to evaluate answers. We focus on document-grounded question answering with asymmetric context, where a single model serves as both tutor (with access to a relevant source document during training) and student (answering from the question alone at test time). Rather than assuming tutor correctness, we derive supervision online from tutor consensus by sampling multiple document-grounded reasoning traces and using agreement to gate learning. Conditioned on this reliability signal, we distill knowledge through full tutor reasoning trajectories (not just final answers), providing a dense and stable learning signal. Empirically, this consensus-gated trajectory distillation substantially improves transfer to the document-free student. Held-out in-domain accuracy under asymmetric evaluation improves from 46.0\% to 62.0\%, and average (maj@8) accuracy on public document-free math benchmarks improves from 20.2\% to 35.4\%.

</details>


### [53] [Wireless Federated Multi-Task LLM Fine-Tuning via Sparse-and-Orthogonal LoRA](https://arxiv.org/abs/2602.20492)
*Nuocheng Yang,Sihua Wang,Ouwen Huan,Mingzhe Chen,Tony Q. S. Quek,Changchuan Yin*

Main category: cs.LG

TL;DR: 针对基于LoRA的分布式联邦学习在异构多任务数据微调LLM时面临的灾难性遗忘、通信低效和知识干扰问题，本文提出稀疏正交LoRA、基于聚类的拓扑结构和隐式MoE机制的三阶段解决方案，实现73%通信开销降低和5%性能提升。


<details>
  <summary>Details</summary>
Motivation: 在去中心化场景下，移动设备需协作微调大语言模型，但异构多任务数据导致微调过程中的灾难性知识遗忘、聚合过程中的低效通信与收敛、以及推理过程中的多任务知识干扰三大核心问题。

Method: 提出稀疏正交LoRA确保模型更新正交性以消除方向冲突；通过分析设备连接拓扑对多任务性能的影响设计基于聚类的聚合拓扑；引入隐式混合专家机制避免推理时的不兼容知识共存。

Result: 仿真结果表明，所提方法相比传统LoRA可降低73%通信资源消耗，并将平均性能提升5%。

Conclusion: 该完全去中心化方案有效解决了DFL全生命周期中的三大挑战，为异构设备协作微调大模型提供了高效可行的技术路径。

Abstract: Decentralized federated learning (DFL) based on low-rank adaptation (LoRA) enables mobile devices with multi-task datasets to collaboratively fine-tune a large language model (LLM) by exchanging locally updated parameters with a subset of neighboring devices via wireless connections for knowledge integration.However, directly aggregating parameters fine-tuned on heterogeneous datasets induces three primary issues across the DFL life-cycle: (i) \textit{catastrophic knowledge forgetting during fine-tuning process}, arising from conflicting update directions caused by data heterogeneity; (ii) \textit{inefficient communication and convergence during model aggregation process}, due to bandwidth-intensive redundant model transmissions; and (iii) \textit{multi-task knowledge interference during inference process}, resulting from incompatible knowledge representations coexistence during inference. To address these issues in a fully decentralized scenario, we first propose a sparse-and-orthogonal LoRA that ensures orthogonality between model updates to eliminate direction conflicts during fine-tuning.Then, we analyze how device connection topology affects multi-task performance, prompting a cluster-based topology design during aggregation.Finally, we propose an implicit mixture of experts (MoE) mechanism to avoid the coexistence of incompatible knowledge during inference. Simulation results demonstrate that the proposed approach effectively reduces communication resource consumption by up to $73\%$ and enhances average performance by $5\%$ compared with the traditional LoRA method.

</details>


### [54] [A Generalized Apprenticeship Learning Framework for Capturing Evolving Student Pedagogical Strategies](https://arxiv.org/abs/2602.20527)
*Md Mirajul Islam,Xi Yang,Adittya Soukarjya Saha,Rajesh Debnath,Min Chi*

Main category: cs.LG

TL;DR: 该论文提出THEMES学徒学习框架，解决深度强化学习在教育技术中样本效率低和奖励函数设计困难的问题。框架通过捕捉学生专家学习过程中动态演变的多个奖励函数，仅用18个历史轨迹就能预测后续学期的教学决策，达到0.899 AUC和0.653 Jaccard指数，性能优于6种基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）和深度强化学习（DRL）已成功应用于智能辅导系统（ITSs），但样本效率低下和奖励函数设计困难限制了其在教育技术中的广泛应用。学徒学习（AL）可通过少量专家演示推断潜在奖励函数，为DRL提供了更有效的替代方案。

Method: 提出通用学徒学习框架THEMES，核心是捕捉学生专家学习过程的复杂性，特别处理多个奖励函数随时间动态演变的问题。该框架通过从少量演示中学习专家行为模式，推导出可推广的教学决策策略。

Result: 在预测学生教学决策任务中，THEMES显著优于六种先进基线方法。仅使用前一学期18个学习轨迹，就在后一学期的预测中达到0.899 AUC和0.653 Jaccard指数，证明其高样本效率下的强泛化能力。

Conclusion: THEMES为智能辅导系统提供了强有力的教学策略诱导方法，成功克服了DRL的主要局限性。其通过极少量演示实现高性能的特性，使该框架在教育技术实际应用中具备更高可行性和可扩展性。

Abstract: Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) have advanced rapidly in recent years and have been successfully applied to e-learning environments like intelligent tutoring systems (ITSs). Despite great success, the broader application of DRL to educational technologies has been limited due to major challenges such as sample inefficiency and difficulty designing the reward function. In contrast, Apprenticeship Learning (AL) uses a few expert demonstrations to infer the expert's underlying reward functions and derive decision-making policies that generalize and replicate optimal behavior. In this work, we leverage a generalized AL framework, THEMES, to induce effective pedagogical policies by capturing the complexities of the expert student learning process, where multiple reward functions may dynamically evolve over time. We evaluate the effectiveness of THEMES against six state-of-the-art baselines, demonstrating its superior performance and highlighting its potential as a powerful alternative for inducing effective pedagogical policies and show that it can achieve high performance, with an AUC of 0.899 and a Jaccard of 0.653, using only 18 trajectories of a previous semester to predict student pedagogical decisions in a later semester.

</details>


### [55] [Memory-guided Prototypical Co-occurrence Learning for Mixed Emotion Recognition](https://arxiv.org/abs/2602.20530)
*Ming Li,Yong-Jin Liu,Fang Liu,Huankun Sheng,Yeying Fan,Yixiang Wei,Minnan Luo,Weizhan Zhang,Wenping Wang*

Main category: cs.LG

TL;DR: 该论文提出记忆引导的原型共现学习（MPCL）框架，用于从多模态生理和行为信号中识别混合情绪。框架通过多尺度联想记忆融合信号，利用情绪原型记忆库和关系蒸馏实现跨模态对齐，并采用记忆检索策略挖掘情绪共现关联，最终在两个公开数据集上超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有情绪识别模型局限于受控实验室中单一情绪预测，而现实情绪体验常为多种情感状态并存。当前混合情绪识别方法忽视了共现情绪间的效价一致性与结构化关联，限制了其实际应用效果。

Method: 提出MPCL框架，具体包括：通过多尺度联想记忆机制融合多模态信号；构建情绪特定原型记忆库捕获跨模态语义关系，并采用原型关系蒸馏确保潜在空间对齐；设计记忆检索策略提取情绪类别间的语义级共现关联；通过自下而上层次抽象过程学习情感信息丰富的表示。

Result: 在两个公开数据集上的全面实验表明，MPCL在定量指标和定性分析上均持续优于当前最先进方法，验证了所提框架在混合情绪识别任务中的有效性。

Conclusion: 该研究通过显式建模情绪共现模式，为多模态混合情绪识别提供了有效解决方案，展示了在情感计算领域的应用潜力，并为未来研究提供了新方向。

Abstract: Emotion recognition from multi-modal physiological and behavioral signals plays a pivotal role in affective computing, yet most existing models remain constrained to the prediction of singular emotions in controlled laboratory settings. Real-world human emotional experiences, by contrast, are often characterized by the simultaneous presence of multiple affective states, spurring recent interest in mixed emotion recognition as an emotion distribution learning problem. Current approaches, however, often neglect the valence consistency and structured correlations inherent among coexisting emotions. To address this limitation, we propose a Memory-guided Prototypical Co-occurrence Learning (MPCL) framework that explicitly models emotion co-occurrence patterns. Specifically, we first fuse multi-modal signals via a multi-scale associative memory mechanism. To capture cross-modal semantic relationships, we construct emotion-specific prototype memory banks, yielding rich physiological and behavioral representations, and employ prototype relation distillation to ensure cross-modal alignment in the latent prototype space. Furthermore, inspired by human cognitive memory systems, we introduce a memory retrieval strategy to extract semantic-level co-occurrence associations across emotion categories. Through this bottom-up hierarchical abstraction process, our model learns affectively informative representations for accurate emotion distribution prediction. Comprehensive experiments on two public datasets demonstrate that MPCL consistently outperforms state-of-the-art methods in mixed emotion recognition, both quantitatively and qualitatively.

</details>


### [56] [Sample-efficient evidence estimation of score based priors for model selection](https://arxiv.org/abs/2602.20549)
*Frederic Wang,Katherine L. Bouman*

Main category: cs.LG

TL;DR: 本文提出了一种扩散模型先验的模型证据估计方法，通过整合后验采样过程的时间边缘分布，巧妙利用反向扩散产生的大量中间样本，仅需少量后验样本（如20个）即可准确估计模型证据，解决了病态成像逆问题中先验选择的关键难题。


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯逆问题中，先验选择对避免严重偏差至关重要，理想策略是计算不同先验模型M下的模型证据p(y|M)并选取最大值。尽管扩散模型作为数据驱动先验处于领先地位，但其模型证据直接计算不可行。现有估计器需大量未归一化先验密度的点态评估或精确干净先验分数，限制了实际应用。

Method: 所提方法通过积分后验采样的时间边缘分布来估计扩散先验的模型证据。其核心是利用反向扩散采样过程自然产生的大量中间样本，仅需少量最终后验样本（例如20个）即可获得精确估计，并可与现有扩散后验采样方法无缝结合。

Result: 实验表明：该估计器在模型证据可解析计算时结果一致；能正确选择扩散模型先验；在多种高度病态、非线性逆问题（包括真实黑洞成像问题）中有效诊断先验失配，展现出高精度与高实用性。

Conclusion: 该方法为扩散先验下的模型证据估计提供了高效准确的解决方案，突破了计算复杂度与精度瓶颈，为病态成像逆问题中的先验选择提供了实用工具，在理论方法与现实应用间建立了有效桥梁。

Abstract: The choice of prior is central to solving ill-posed imaging inverse problems, making it essential to select one consistent with the measurements $y$ to avoid severe bias. In Bayesian inverse problems, this could be achieved by evaluating the model evidence $p(y \mid M)$ under different models $M$ that specify the prior and then selecting the one with the highest value. Diffusion models are the state-of-the-art approach to solving inverse problems with a data-driven prior; however, directly computing the model evidence with respect to a diffusion prior is intractable. Furthermore, most existing model evidence estimators require either many pointwise evaluations of the unnormalized prior density or an accurate clean prior score. We propose \method, an estimator of the model evidence of a diffusion prior by integrating over the time-marginals of posterior sampling methods. Our method leverages the large amount of intermediate samples naturally obtained during the reverse diffusion sampling process to obtain an accurate estimation of the model evidence using only a handful of posterior samples (e.g., 20). We also demonstrate how to implement our estimator in tandem with recent diffusion posterior sampling methods. Empirically, our estimator matches the model evidence when it can be computed analytically, and it is able to both select the correct diffusion model prior and diagnose prior misfit under different highly ill-conditioned, non-linear inverse problems, including a real-world black hole imaging problem.

</details>


### [57] [GENSR: Symbolic Regression Based in Equation Generative Space](https://arxiv.org/abs/2602.20557)
*Qian Li,Yuxiao Hu,Juncheng Liu,Yuntian Chen*

Main category: cs.LG

TL;DR: 传统符号回归在离散方程空间中搜索，结构修改与数值行为不匹配导致误差反馈噪声大。为此，本文提出GenSR生成式潜空间框架，遵循'地图构建→粗定位→精细搜索'流程：预训练双分支CVAE构建符号连续且数值平滑的潜空间地图；推理时先粗定位再通过改进CMA-ES利用平滑梯度精细搜索；从贝叶斯视角最大化p(公式|数值)，提供理论保证。实验表明该方法联合优化精度、简洁性与效率，且对噪声鲁棒。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法在离散方程空间中搜索时，方程的结构修改往往与数值行为变化不一致，导致拟合误差反馈过于噪声化，无法有效指导搜索过程。

Method: 提出GenSR框架，采用'地图构建→粗定位→精细搜索'三阶段范式。首先，预训练一个双分支条件变分自编码器(CVAE)，将符号方程重新参数化为具有符号连续性和局部数值平滑性的生成式潜空间，该空间可作为方程空间的良好结构化'地图'。在推理阶段，CVAE先将输入数据粗定位到潜空间中的有希望区域，然后采用改进的CMA-ES算法利用平滑的潜空间梯度进行精细搜索。理论层面，从贝叶斯角度将符号回归任务重新表述为最大化条件分布p(公式|数值)，CVAE训练通过证据下界(ELBO)实现该目标，为方法有效性提供理论保证。

Result: 在广泛实验中，GenSR展现出联合优化预测准确性、表达式简洁性和计算效率的能力，同时在噪声条件下保持较强的鲁棒性。

Conclusion: GenSR通过构建生成式潜空间，为符号回归提供了新的理论框架和实现路径，有效解决了传统离散搜索方法中的噪声问题，实现了精度、简洁性和效率的协同优化，具有理论和实践价值。

Abstract: Symbolic Regression (SR) tries to reveal the hidden equations behind observed data. However, most methods search within a discrete equation space, where the structural modifications of equations rarely align with their numerical behavior, leaving fitting error feedback too noisy to guide exploration. To address this challenge, we propose GenSR, a generative latent space-based SR framework following the `map construction -> coarse localization -> fine search'' paradigm. Specifically, GenSR first pretrains a dual-branch Conditional Variational Autoencoder (CVAE) to reparameterize symbolic equations into a generative latent space with symbolic continuity and local numerical smoothness. This space can be regarded as a well-structured `map'' of the equation space, providing directional signals for search. At inference, the CVAE coarsely localizes the input data to promising regions in the latent space. Then, a modified CMA-ES refines the candidate region, leveraging smooth latent gradients. From a Bayesian perspective, GenSR reframes the SR task as maximizing the conditional distribution $p(\mathrm{Equ.} \mid \mathrm{Num.})$, with CVAE training achieving this objective through the Evidence Lower Bound (ELBO). This new perspective provides a theoretical guarantee for the effectiveness of GenSR. Extensive experiments show that GenSR jointly optimizes predictive accuracy, expression simplicity, and computational efficiency, while remaining robust under noise.

</details>


### [58] [SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards](https://arxiv.org/abs/2602.21158)
*Dengjia Zhang,Xiaoou Liu,Lu Cheng,Yaqing Wang,Kenton Murray,Hua Wei*

Main category: cs.LG

TL;DR: SELAUR是一个基于不确定性感知奖励的LLM智能体强化学习框架。它通过融合熵、最小置信度和间隔度量来估计token级不确定性，并提供密集置信对齐监督；同时采用失败感知奖励重塑机制，将不确定性信号注入步骤和轨迹级奖励中，以提升探索效率和稳定性。在ALFWorld和WebShop基准测试中，该方法显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究在LLM智能体的奖励设计和步骤级信用分配方面已有探索，但普遍忽视了模型的不确定性这一关键信号。不确定性能够反映模型置信度、指示探索需求，并为失败轨迹提供宝贵的学习线索。缺乏对不确定性的利用限制了智能体在多步决策中的探索效率和鲁棒性。

Method: SELAUR框架主要包含两个核心组件：1）不确定性估计模块，结合熵、最小置信度和间隔三种度量构建token级综合不确定性评估，提供密集置信对齐监督；2）失败感知奖励重塑机制，将不确定性信号注入步骤级和轨迹级奖励中。具体通过动态调整奖励信号，在模型高不确定性区域增强探索，在低不确定性区域利用已有知识，从而提升学习稳定性和效率。

Result: 在ALFWorld和WebShop两个基准测试上，SELAUR相比强基线模型取得了持续的成功率提升。消融实验验证了不确定性信号的有效性：熵、最小置信度和间隔三种度量的组合显著增强了探索能力；失败感知奖励重塑机制提高了学习稳定性；不确定性注入策略有效提升了模型在复杂多步任务中的鲁棒性。

Conclusion: 本研究首次系统地将LLM不确定性引入智能体奖励设计中，证明了不确定性作为一种内在学习信号的重要价值。SELAUR框架通过不确定性感知奖励机制，有效解决了多步决策中的探索效率和学习稳定性问题，为未来LLM智能体训练提供了新范式。该方法在复杂交互任务中具有广泛应用前景。

Abstract: Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.

</details>


### [59] [Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs](https://arxiv.org/abs/2602.21198)
*Yining Hong,Huang Huang,Manling Li,Li Fei-Fei,Jiajun Wu,Yejin Choi*

Main category: cs.LG

TL;DR: 针对具身LLM机器人缺乏反思能力、错误重复的问题，提出反思测试时规划，整合行动中反思（预执行多动作评分）、行动后反思（后执行模型更新）和回顾性反思（事后信用分配），在长期家务和MuJoCo基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有具身大语言模型虽具高级推理能力，但无法反思错误根源，导致机器人部署成为独立试错序列，经验无法累积。借鉴人类反思实践者，亟需赋予机器人从错误中学习、实现经验积累的能力。

Method: 提出反思测试时规划框架，包含：1）行动中反思：测试时缩放生成并评分多候选动作；2）行动后反思：基于外部反馈测试时训练更新反思模型和动作策略；3）回顾性反思：事后重评估早期决策实现长期信用分配。在Long-Horizon Household和MuJoCo Cupboard Fitting基准上验证。

Result: 在新设计的长期家务基准和MuJoCo橱柜安装基准上显著优于基线模型，消融研究证实行动中反思和行动后反思的互补作用，真实机器人试验显示反思能修正行为。

Conclusion: 该方法使机器人具备从错误中学习和经验积累能力，为具身智能持续学习提供新范式，三种反思机制协同有效提升长期任务性能。

Abstract: Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.

</details>


### [60] [Is the Trigger Essential? A Feature-Based Triggerless Backdoor Attack in Vertical Federated Learning](https://arxiv.org/abs/2602.20593)
*Yige Liu,Yiwei Lou,Che Wang,Yongzhi Cao,Hanpin Wang*

Main category: cs.LG

TL;DR: 本文针对垂直联邦学习(VFL)提出了一种新型特征基无触发器后门攻击。在诚实但好奇的安全假设下，该攻击通过标签推断、带放大与扰动机制的投毒生成及后门执行三模块，实现了较基线攻击2-50倍的性能提升，且对主任务影响极小，面对防御策略展现强鲁棒性，揭示了VFL安全的新威胁路径。


<details>
  <summary>Details</summary>
Motivation: 尽管垂直联邦学习以隐私保护著称，但仍面临后门攻击威胁。现有攻击依赖触发器，但其在VFL中的必要性尚未被质疑。为揭示更隐蔽的攻击方式并推动VFL安全防御研究，本文探索了一种无需触发器的新型后门攻击路径。

Method: 提出基于特征的无触发器后门攻击框架，包含三个核心模块：(1)针对目标样本的标签推断模块；(2)采用放大和扰动机制生成投毒样本的模块；(3)后门执行模块。攻击者遵循诚实但好奇的安全假设，即在训练阶段不恶意破坏协议但可在推理阶段实施攻击。

Result: 在五个基准数据集上的实验表明：该攻击成功率较三种基线方法提升2-50倍；对主任务准确率影响极低；在32个被动参与方且仅有一组辅助数据的极端场景下仍保持高效；面对多种防御策略表现出强鲁棒性。

Conclusion: 本研究揭示了VFL中无触发器后门攻击的新途径，证明触发器在VFL后门攻击中并非必要。作者希望该发现能促使学术界重新审视VFL安全威胁，并推动开发更鲁棒、实用的防御机制。

Abstract: As a distributed collaborative machine learning paradigm, vertical federated learning (VFL) allows multiple passive parties with distinct features and one active party with labels to collaboratively train a model. Although it is known for the privacy-preserving capabilities, VFL still faces significant privacy and security threats from backdoor attacks. Existing backdoor attacks typically involve an attacker implanting a trigger into the model during the training phase and executing the attack by adding the trigger to the samples during the inference phase. However, in this paper, we find that triggers are not essential for backdoor attacks in VFL. In light of this, we disclose a new backdoor attack pathway in VFL by introducing a feature-based triggerless backdoor attack. This attack operates under a more stringent security assumption, where the attacker is honest-but-curious rather than malicious during the training phase. It comprises three modules: label inference for the targeted backdoor attack, poison generation with amplification and perturbation mechanisms, and backdoor execution to implement the attack. Extensive experiments on five benchmark datasets demonstrate that our attack outperforms three baseline backdoor attacks by 2 to 50 times while minimally impacting the main task. Even in VFL scenarios with 32 passive parties and only one set of auxiliary data, our attack maintains high performance. Moreover, when confronted with distinct defense strategies, our attack remains largely unaffected and exhibits strong robustness. We hope that the disclosure of this triggerless backdoor attack pathway will encourage the community to revisit security threats in VFL scenarios and inspire researchers to develop more robust and practical defense strategies.

</details>


### [61] [QEDBENCH: Quantifying the Alignment Gap in Automated Evaluation of University-Level Mathematical Proofs](https://arxiv.org/abs/2602.20629)
*Santiago Gonzalez,Alireza Amiri Bavandpour,Peter Ye,Edward Zhang,Ruslans Aleksejevs,Todor Antić,Polina Baron,Sujeet Bhalerao,Shubhrajit Bhattacharya,Zachary Burton,John Byrne,Hyungjun Choi,Nujhat Ahmed Disha,Koppany István Encz,Yuchen Fang,Robert Joseph George,Ebrahim Ghorbani,Alan Goldfarb,Jing Guo,Meghal Gupta,Stefano Huber,Annika Kanckos,Minjung Kang,Hyun Jong Kim,Dino Lorenzini,Levi Lorenzo,Tianyi Mao,Giovanni Marzenta,Ariane M. Masuda,Lukas Mauth,Ana Mickovic,Andres Miniguano-Trujillo,Antoine Moulin,Wenqi Ni,Tomos Parry,Kevin Ren,Hossein Roodbarani,Mathieu Rundström,Manjil Saikia,Detchat Samart,Rebecca Steiner,Connor Stewart,Dhara Thakkar,Jeffrey Tse,Vasiliki Velona,Yunhai Xiang,Sibel Yalçın,Jun Yan,Ji Zeng,Arman Cohan,Quanquan C. Liu*

Main category: cs.LG

TL;DR: 本文揭示了LLM-as-a-Judge在高等数学评估中的系统性对齐差距，推出了首个大规模双标准对齐基准QEDBench，发现前沿评判模型存在显著正偏差，并在离散数学领域发现关键推理差距。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在基础基准测试中趋于饱和，研究前沿转向自动化评估的可靠性问题。现有LLM-as-a-Judge协议在高等教育数学评估中存在系统性对齐差距，缺乏有效评估AI评判与专家判断对齐程度的基准。

Method: 开发了QEDBench基准，通过对比课程特定评分标准与专家常识标准，构建7个评判模型×5个求解器的双评估矩阵，基于1000+小时的人类评估数据，系统测量AI评判与数学专家的对齐程度。

Result: 发现Claude Opus 4.5、DeepSeek-V3、Qwen 2.5 Max和Llama 4 Maverick等前沿评判模型存在显著正偏差（平均分数虚高分别达+0.18、+0.20、+0.30、+0.36）；揭示离散领域的关键推理差距，如GPT-5 Pro和Claude Sonnet 4.5在离散数学中评分分别降至0.72和0.63，在图论中分别降至0.74和0.50，而Gemini 3.0 Pro保持0.91的顶尖水平。

Conclusion: 证实LLM-as-a-Judge在高等教育数学评估中存在严重对齐问题，发布QEDBench作为公开基准以推动AI评判模型的发展和改进，为研究社区提供评估工具。

Abstract: As Large Language Models (LLMs) saturate elementary benchmarks, the research frontier has shifted from generation to the reliability of automated evaluation. We demonstrate that standard "LLM-as-a-Judge" protocols suffer from a systematic Alignment Gap when applied to upper-undergraduate to early graduate level mathematics. To quantify this, we introduce QEDBench, the first large-scale dual-rubric alignment benchmark to systematically measure alignment with human experts on university-level math proofs by contrasting course-specific rubrics against expert common knowledge criteria. By deploying a dual-evaluation matrix (7 judges x 5 solvers) against 1,000+ hours of human evaluation, we reveal that certain frontier evaluators like Claude Opus 4.5, DeepSeek-V3, Qwen 2.5 Max, and Llama 4 Maverick exhibit significant positive bias (up to +0.18, +0.20, +0.30, +0.36 mean score inflation, respectively). Furthermore, we uncover a critical reasoning gap in the discrete domain: while Gemini 3.0 Pro achieves state-of-the-art performance (0.91 average human evaluation score), other reasoning models like GPT-5 Pro and Claude Sonnet 4.5 see their performance significantly degrade in discrete domains. Specifically, their average human evaluation scores drop to 0.72 and 0.63 in Discrete Math, and to 0.74 and 0.50 in Graph Theory. In addition to these research results, we also release QEDBench as a public benchmark for evaluating and improving AI judges. Our benchmark is publicly published at https://github.com/qqliu/Yale-QEDBench.

</details>


### [62] [TrajGPT-R: Generating Urban Mobility Trajectory with Reinforcement Learning-Enhanced Generative Pre-trained Transformer](https://arxiv.org/abs/2602.20643)
*Jiawei Wang,Chuang Yang,Jiawei Yong,Xiaohang Xu,Hongjun Wang,Noboru Koshizuka,Shintaro Fukushima,Ryosuke Shibasaki,Renhe Jiang*

Main category: cs.LG

TL;DR: 针对隐私问题导致的城市移动轨迹数据获取困难，本文提出一种基于Transformer的两阶段预训练-微调框架。通过将轨迹生成建模为离线强化学习问题并引入逆向强化学习（IRL）捕捉轨迹级奖励信号，有效解决了长期信用分配和稀疏奖励挑战。多数据集评估显示，该框架在可靠性和多样性上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 城市移动轨迹对于理解城市动态和优化城市规划至关重要，但隐私问题严重限制了此类数据的获取。为此，本文旨在开发一种能够生成大规模城市移动轨迹的变革性框架，以支持交通管理和城市发展决策。

Method: 本文提出一种基于Transformer的两阶段预训练-微调框架。首先，将轨迹生成概念化为离线强化学习问题，并在tokenization过程中大幅缩减词汇空间。其次，集成逆向强化学习（IRL）从历史数据中推断个体移动偏好，构建轨迹级奖励信号。最后，利用该奖励模型对预训练模型进行微调，有效解决传统RL基自回归方法在长期信用分配和稀疏奖励环境处理中的固有挑战。

Result: 在多个数据集上的全面评估表明，该框架在轨迹生成的可靠性和多样性指标上显著超越现有模型。

Conclusion: 本研究不仅推动了城市移动建模领域的发展，还提供了一种鲁棒的城市数据模拟方法，对交通管理和城市规划具有重要实践意义。

Abstract: Mobility trajectories are essential for understanding urban dynamics and enhancing urban planning, yet access to such data is frequently hindered by privacy concerns. This research introduces a transformative framework for generating large-scale urban mobility trajectories, employing a novel application of a transformer-based model pre-trained and fine-tuned through a two-phase process. Initially, trajectory generation is conceptualized as an offline reinforcement learning (RL) problem, with a significant reduction in vocabulary space achieved during tokenization. The integration of Inverse Reinforcement Learning (IRL) allows for the capture of trajectory-wise reward signals, leveraging historical data to infer individual mobility preferences. Subsequently, the pre-trained model is fine-tuned using the constructed reward model, effectively addressing the challenges inherent in traditional RL-based autoregressive methods, such as long-term credit assignment and handling of sparse reward environments. Comprehensive evaluations on multiple datasets illustrate that our framework markedly surpasses existing models in terms of reliability and diversity. Our findings not only advance the field of urban mobility modeling but also provide a robust methodology for simulating urban data, with significant implications for traffic management and urban development planning. The implementation is publicly available at https://github.com/Wangjw6/TrajGPT_R.

</details>


### [63] [UrbanFM: Scaling Urban Spatio-Temporal Foundation Models](https://arxiv.org/abs/2602.20677)
*Wei Chen,Yuqian Wu,Junle Chen,Xiaofang Zhou,Yuxuan Liang*

Main category: cs.LG

TL;DR: 针对城市计算碎片化问题，本文从数据、计算、架构三维度系统扩展：构建WorldST十亿级语料库、MiniST标准化计算单元和UrbanFM极简自注意力模型，并建立EvalST基准。实验显示UrbanFM实现跨城市跨任务零样本泛化，标志着城市时空基础模型研究的重大突破。


<details>
  <summary>Details</summary>
Motivation: 当前城市计算领域模型多为"场景特定"型，过度拟合特定区域或任务，导致泛化能力受限。虽然AI for Science在基因组学、气象学等领域通过基础模型展现出变革性力量，但城市时空基础模型的研究仍存在空白。这种碎片化现状阻碍了城市系统普适性规律的发现和应用。

Method: 基于第一性原理分析，识别出异质性、相关性和动态性三个关键维度，提出对应的三维扩展框架：1) 数据扩展：构建WorldST十亿级语料库，标准化100多个全球城市的交通流、速度等物理信号，解决数据异质性问题；2) 计算扩展：设计MiniST分割机制，将连续时空场离散化为可学习计算单元，统一网格与传感器观测以建模相关性；3) 架构扩展：开发UrbanFM极简自注意力架构，通过有限归纳偏置从海量数据自主学习目标时空依赖，应对动态性挑战。同时建立EvalST大规模基准用于评估。

Result: 大规模实验表明，UrbanFM在未见过的城市和任务上均能实现显著零样本泛化性能，验证了三维扩展策略在构建城市时空基础模型方面的有效性。

Conclusion: 本研究通过数据、计算、架构三维度的系统性扩展，成功构建了首个城市时空基础模型UrbanFM，其零样本泛化能力证明了通用城市智能模型的可行性。该工作为城市计算领域从碎片化范式向基础模型范式转变奠定了关键基石，为未来大规模、普适性城市研究开辟了新方向。

Abstract: Urban systems, as dynamic complex systems, continuously generate spatio-temporal data streams that encode the fundamental laws of human mobility and city evolution. While AI for Science has witnessed the transformative power of foundation models in disciplines like genomics and meteorology, urban computing remains fragmented due to "scenario-specific" models, which are overfitted to specific regions or tasks, hindering their generalizability. To bridge this gap and advance spatio-temporal foundation models for urban systems, we adopt scaling as the central perspective and systematically investigate two key questions: what to scale and how to scale. Grounded in first-principles analysis, we identify three critical dimensions: heterogeneity, correlation, and dynamics, aligning these principles with the fundamental scientific properties of urban spatio-temporal data. Specifically, to address heterogeneity through data scaling, we construct WorldST. This billion-scale corpus standardizes diverse physical signals, such as traffic flow and speed, from over 100 global cities into a unified data format. To enable computation scaling for modeling correlations, we introduce the MiniST unit, a novel split mechanism that discretizes continuous spatio-temporal fields into learnable computational units to unify representations of grid-based and sensor-based observations. Finally, addressing dynamics via architecture scaling, we propose UrbanFM, a minimalist self-attention architecture designed with limited inductive biases to autonomously learn dynamic spatio-temporal dependencies from massive data. Furthermore, we establish EvalST, the largest-scale urban spatio-temporal benchmark to date. Extensive experiments demonstrate that UrbanFM achieves remarkable zero-shot generalization across unseen cities and tasks, marking a pivotal first step toward large-scale urban spatio-temporal foundation models.

</details>


### [64] [High-Dimensional Robust Mean Estimation with Untrusted Batches](https://arxiv.org/abs/2602.20698)
*Maryam Aliakbarpour,Vladimir Braverman,Yuhan Liu,Junze Yin*

Main category: cs.LG

TL;DR: 该论文研究了在存在双重腐败的协作学习环境中，N个用户各提供n个样本的批次数据，目标是估计高维分布的均值。提出了基于Sum-of-Squares的算法，在对抗性用户比例ε和统计异质性参数α的双重挑战下，达到了极小化最大最优误差率O(√(ε/n) + √(d/nN) + √α)。


<details>
  <summary>Details</summary>
Motivation: 现有不可信批处理模型主要关注离散设置下的总变差距离偏差，但连续高维场景中样本级腐败会任意扭曲统计量。现实协作学习中既存在恶意用户，也存在因数据分布差异导致的统计异质性，该研究旨在解决这两种腐败共存时的均值估计问题，这对实际应用中的鲁棒机器学习至关重要。

Method: 提出了两种基于Sum-of-Squares (SoS) 证明系统的半定规划算法。针对两种异质性模型：(1) 良好批次均值偏移√α；(2) 良好批次内α比例样本被对抗性腐败。算法利用SoS层次结构在高维空间中过滤异常数据并估计真实分布的一阶矩。

Result: 算法实现了极小化最大最优误差界O(√(ε/n) + √(d/nN) + √α)。关键发现是：统计异质性α构成固有统计难度，但对抗性用户的影响因批次内平均效应被1/√n因子抑制；即使在高维情况下，少量样本腐败的破坏性也可通过算法设计得到控制。

Conclusion: 该工作建立了高维协作学习的双重腐败理论框架，证明了SoS算法能有效应对统计异质性与对抗性攻击的复合挑战。批处理结构是缓解对抗性影响的关键机制，为设计鲁棒分布式学习系统提供了理论基础。异质性α无法消除，但对抗性腐败的影响可通过批次设计显著降低。

Abstract: We study high-dimensional mean estimation in a collaborative setting where data is contributed by $N$ users in batches of size $n$. In this environment, a learner seeks to recover the mean $μ$ of a true distribution $P$ from a collection of sources that are both statistically heterogeneous and potentially malicious. We formalize this challenge through a double corruption landscape: an $\varepsilon$-fraction of users are entirely adversarial, while the remaining ``good'' users provide data from distributions that are related to $P$, but deviate by a proximity parameter $α$.
  Unlike existing work on the untrusted batch model, which typically measures this deviation via total variation distance in discrete settings, we address the continuous, high-dimensional regime under two natural variants for deviation: (1) good batches are drawn from distributions with a mean-shift of $\sqrtα$, or (2) an $α$-fraction of samples within each good batch are adversarially corrupted. In particular, the second model presents significant new challenges: in high dimensions, unlike discrete settings, even a small fraction of sample-level corruption can shift empirical means and covariances arbitrarily.
  We provide two Sum-of-Squares (SoS) based algorithms to navigate this tiered corruption. Our algorithms achieve the minimax-optimal error rate $O(\sqrt{\varepsilon/n} + \sqrt{d/nN} + \sqrtα)$, demonstrating that while heterogeneity $α$ represents an inherent statistical difficulty, the influence of adversarial users is suppressed by a factor of $1/\sqrt{n}$ due to the internal averaging afforded by the batch structure.

</details>


### [65] [Fuz-RL: A Fuzzy-Guided Robust Framework for Safe Reinforcement Learning under Uncertainty](https://arxiv.org/abs/2602.20729)
*Xu Wan,Chao Yang,Cheng Yang,Jie Song,Mingyang Sun*

Main category: cs.LG

TL;DR: 本文提出Fuz-RL，一种基于模糊测度的鲁棒安全强化学习框架。该框架通过新颖的模糊贝尔曼算子和Choquet积分来估计鲁棒值函数，理论证明了求解Fuz-RL等价于求解分布鲁棒安全RL问题，避免了min-max优化。在safe-control-gym和safety-gymnasium上的实验表明，该框架能以无模型方式与现有安全RL基线有效结合，在各类观测、动作和动态不确定性下显著提升安全性和控制性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，安全强化学习需要在保证安全的同时实现高性能，但真实环境中多源不确定性给可解释的风险评估和鲁棒决策带来了重大挑战。传统方法难以有效处理这种复杂性。

Method: 提出Fuz-RL框架，开发基于Choquet积分的模糊贝尔曼算子来估计鲁棒值函数。理论证明在约束马尔可夫决策过程(CMDP)形式下求解Fuz-RL等价于求解分布鲁棒安全RL问题，从而避免了计算复杂的min-max优化。

Result: 在safe-control-gym和safety-gymnasium基准测试中，Fuz-RL能以无模型方式无缝集成到现有安全RL基线中，在观测、动作和系统动力学不确定性条件下，显著提升了安全性和控制性能。

Conclusion: Fuz-RL框架通过模糊测度理论有效解决了多源不确定性下的安全强化学习问题，提供了理论保证和实践有效性，为安全关键应用提供了新的解决方案。

Abstract: Safe Reinforcement Learning (RL) is crucial for achieving high performance while ensuring safety in real-world applications. However, the complex interplay of multiple uncertainty sources in real environments poses significant challenges for interpretable risk assessment and robust decision-making. To address these challenges, we propose Fuz-RL, a fuzzy measure-guided robust framework for safe RL. Specifically, our framework develops a novel fuzzy Bellman operator for estimating robust value functions using Choquet integrals. Theoretically, we prove that solving the Fuz-RL problem (in Constrained Markov Decision Process (CMDP) form) is equivalent to solving distributionally robust safe RL problems (in robust CMDP form), effectively avoiding min-max optimization. Empirical analyses on safe-control-gym and safety-gymnasium scenarios demonstrate that Fuz-RL effectively integrates with existing safe RL baselines in a model-free manner, significantly improving both safety and control performance under various types of uncertainties in observation, action, and dynamics.

</details>


### [66] [Transcoder Adapters for Reasoning-Model Diffing](https://arxiv.org/abs/2602.20904)
*Nathan Hu,Jake Ward,Thomas Icard,Christopher Potts*

Main category: cs.LG

TL;DR: 本文提出transcoder adapters技术，用于学习模型微调前后MLP计算差异的可解释近似。通过比较Qwen2.5-Math-7B及其推理蒸馏变体DeepSeek-R1-Distill-Qwen-7B，发现适配器能忠实复现模型内部计算，在推理基准测试中恢复50-90%的准确率提升，并揭示了仅少数特征与推理行为直接相关。


<details>
  <summary>Details</summary>
Motivation: 当前对推理训练如何影响模型内部机制的理解仍不充分。

Method: 设计transcoder adapters来近似微调前后的MLP计算差异，并应用于Qwen2.5-Math-7B与DeepSeek-R1-Distill-Qwen-7B的对比分析，利用归因图追踪关键行为特征。

Result: 适配器能匹配推理模型的响应长度，恢复50-90%的准确率提升，且特征稀疏激活、可解释。仅约8%特征与推理行为直接相关，其中产生犹豫标记的行为仅由2.4%特征执行，移除这些特征会缩短响应长度但通常不影响准确率。

Conclusion: 该研究为理解推理训练提供了新视角，并证明transcoder adapters是研究微调机制的通用有用工具。

Abstract: While reasoning models are increasingly ubiquitous, the effects of reasoning training on a model's internal mechanisms remain poorly understood. In this work, we introduce transcoder adapters, a technique for learning an interpretable approximation of the difference in MLP computation before and after fine-tuning. We apply transcoder adapters to characterize the differences between Qwen2.5-Math-7B and its reasoning-distilled variant, DeepSeek-R1-Distill-Qwen-7B. Learned adapters are faithful to the target model's internal computation and next-token predictions. When evaluated on reasoning benchmarks, adapters match the reasoning model's response lengths and typically recover 50-90% of the accuracy gains from reasoning fine-tuning. Adapter features are sparsely activating and interpretable. When examining adapter features, we find that only ~8% have activating examples directly related to reasoning behaviors. We deeply study one such behavior -- the production of hesitation tokens (e.g., "wait"). Using attribution graphs, we trace hesitation to only ~2.4% of adapter features (5.6k total) performing one of two functions. These features are necessary and sufficient for producing hesitation tokens; removing them reduces response length, often without affecting accuracy. Overall, our results provide insight into reasoning training and suggest transcoder adapters may be useful for studying fine-tuning more broadly.

</details>


### [67] [From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2602.20911)
*Ruiqi Liu,Boyu Diao,Hangda Liu,Zhulin An,Fei Wang,Yongjun Xu*

Main category: cs.LG

TL;DR: 本文提出语义引导的自适应专家森林方法，通过语义聚类和专家树结构解决增量学习中的知识共享问题，在基准数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有增量学习方法通过冻结预训练模型并添加轻量适配器防止遗忘，但将知识视为非结构化集合，未能利用任务间的语义关系，限制知识共享能力。

Method: 提出语义引导的自适应专家森林：基于语义关系将任务分组为概念聚类；在每个聚类内通过合并相似任务适配器构建平衡专家树；推理时激活相关专家并按置信度加权融合输出。

Result: 在多个基准数据集上的实验表明，该方法取得了当前最优性能。

Conclusion: 该方法通过构建结构化专家森林，有效利用任务间语义关系进行知识共享，显著提升了类增量学习性能。

Abstract: Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for better knowledge sharing. SAEF first groups tasks into conceptual clusters based on their semantic relationships. Then, within each cluster, it builds a balanced expert tree by creating new adapters from merging the adapters of similar tasks. At inference time, SAEF finds and activates a set of relevant experts from the forest for any given input. The final prediction is made by combining the outputs of these activated experts, weighted by how confident each expert is. Experiments on several benchmark datasets show that SAEF achieves SOTA performance.

</details>


### [68] [Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels](https://arxiv.org/abs/2602.20932)
*Anupam Sharma,Harish Katti,Prajwal Singh,Shanmuganathan Raman,Krishna Miyapuram*

Main category: cs.LG

TL;DR: 本研究通过基于WordNet的分层感知片段采样方法，在包含264名受试者、931,538个EEG样本的PEERS数据集上，系统探究了EEG信号在不同语义抽象层级上的物体表征能力，发现分类性能随层级抽象程度提高而提升，揭示了抽象深度是EEG解码中一个亟待探索的重要维度。


<details>
  <summary>Details</summary>
Motivation: 由于EEG信号信噪比低，细粒度多类别物体分类面临挑战，但抽象层级的表征可能更易解码。现有研究多局限于少量类别或被动观看任务，缺乏对EEG表征层次结构的系统性探索，限制了神经解码技术的发展。

Method: 提出片段分析方法，采用基于WordNet的分层感知采样策略生成具有多样层级结构的分类任务片段，并在大规模PEERS数据集（264名受试者、1610个物体标签、931,538个EEG样本）上评估多种机器学习模型和架构。

Result: 实验表明，当分类类别来自更高抽象层级时，模型性能显著提升，证实EEG信号对语义抽象程度具有敏感性，且该现象在不同学习技术和架构中均稳定存在。

Conclusion: 本研究揭示了抽象深度作为EEG解码的关键维度，为未来神经表征研究提供了新方向，并展示了分层片段采样框架在大规模EEG分析中的有效性。

Abstract: An electroencephalogram (EEG) records the spatially averaged electrical activity of neurons in the brain, measured from the human scalp. Prior studies have explored EEG-based classification of objects or concepts, often for passive viewing of briefly presented image or video stimuli, with limited classes. Because EEG exhibits a low signal-to-noise ratio, recognizing fine-grained representations across a large number of classes remains challenging; however, abstract-level object representations may exist. In this work, we investigate whether EEG captures object representations across multiple hierarchical levels, and propose episodic analysis, in which a Machine Learning (ML) model is evaluated across various, yet related, classification tasks (episodes). Unlike prior episodic EEG studies that rely on fixed or randomly sampled classes of equal cardinality, we adopt hierarchy-aware episode sampling using WordNet to generate episodes with variable classes of diverse hierarchy. We also present the largest episodic framework in the EEG domain for detecting observed text from EEG signals in the PEERS dataset, comprising $931538$ EEG samples under $1610$ object labels, acquired from $264$ human participants (subjects) performing controlled cognitive tasks, enabling the study of neural dynamics underlying perception, decision-making, and performance monitoring.
  We examine how the semantic abstraction level affects classification performance across multiple learning techniques and architectures, providing a comprehensive analysis. The models tend to improve performance when the classification categories are drawn from higher levels of the hierarchy, suggesting sensitivity to abstraction. Our work highlights abstraction depth as an underexplored dimension of EEG decoding and motivates future research in this direction.

</details>


### [69] [Extending $μ$P: Spectral Conditions for Feature Learning Across Optimizers](https://arxiv.org/abs/2602.20937)
*Akshita Gupta,Marieme Ngom,Sam Foreman,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: 本文提出了一种基于谱条件的新颖框架，用于为更广泛的优化器（包括AdamW、ADOPT、LAMB、Sophia、Shampoo和Muon）系统性地推导最大更新参数化(μP)。该研究解决了大规模语言模型训练中超参数调优计算成本高昂的难题，通过使超参数独立于模型尺寸，实现从小型模型到大型模型的零样本迁移，从而显著降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模的不断扩大，优化器的性能对超参数的敏感性使得超参数调优成为计算瓶颈。虽然最大更新参数化(μP)已被证明能使SGD和Adam的超参数独立于模型尺寸，但将其扩展到其他优化器面临根本性挑战——现有的张量编程方法过于复杂且难以理解，阻碍了理论推导和方法推广。因此，亟需一种更通用的理论框架来突破这一限制。

Method: 本研究借鉴了近期将谱条件作为张量编程替代方案的重要进展，提出了一个统一且系统的理论框架。该框架利用谱分析技术，为AdamW、ADOPT、LAMB、Sophia、Shampoo和Muon等多种自适应一阶和二阶优化器推导μP缩放规则，避免了传统张量编程的复杂性，使得参数化推导过程更加透明和易于推广。

Result: 通过在多个基准模型上实现上述μP推导，实验结果表明：1）所有研究的优化器均能在增加模型宽度时实现零样本学习率迁移；2）成功获得了关于这些优化器深度缩放参数化的重要实证洞察。这些发现验证了所提框架的有效性和通用性。

Conclusion: 本研究成功地将μP的适用范围从SGD和Adam扩展到更广泛的优化器家族，为大规模语言模型训练提供了通用的超参数迁移理论框架。这不仅显著降低了大规模模型的调参成本，也为未来优化器的设计和分析提供了新的理论工具，对推动高效大规模模型训练具有重要意义。

Abstract: Several variations of adaptive first-order and second-order optimization methods have been proposed to accelerate and scale the training of large language models. The performance of these optimization routines is highly sensitive to the choice of hyperparameters (HPs), which are computationally expensive to tune for large-scale models. Maximal update parameterization $(μ$P$)$ is a set of scaling rules which aims to make the optimal HPs independent of the model size, thereby allowing the HPs tuned on a smaller (computationally cheaper) model to be transferred to train a larger, target model. Despite promising results for SGD and Adam, deriving $μ$P for other optimizers is challenging because the underlying tensor programming approach is difficult to grasp. Building on recent work that introduced spectral conditions as an alternative to tensor programs, we propose a novel framework to derive $μ$P for a broader class of optimizers, including AdamW, ADOPT, LAMB, Sophia, Shampoo and Muon. We implement our $μ$P derivations on multiple benchmark models and demonstrate zero-shot learning rate transfer across increasing model width for the above optimizers. Further, we provide empirical insights into depth-scaling parameterization for these optimizers.

</details>


### [70] [Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation](https://arxiv.org/abs/2602.20947)
*Thorbjørn Mosekjær Iversen,Zebin Duan,Frederik Hagelskjær*

Main category: cs.LG

TL;DR: 针对关键任务中二元分类器需要可靠置信边界的问题，本文提出威尔逊得分核密度分类——一种新颖的核方法。其核心威尔逊得分核密度估计器专为具有条件变化成功概率的二项实验设计。在选择性分类的四个数据集评估中，该方法性能媲美高斯过程分类，但计算复杂度更低，可作为任意特征提取器的分类头部。


<details>
  <summary>Details</summary>
Motivation: 尽管基于深度学习的二元分类器性能显著提升并有望实现关键检查任务的自动化，但这些任务传统上依赖人工执行。要将分类器应用于此类关键操作，必须能够估计可靠的置信边界，以确保系统在特定统计显著性水平下的性能可被严格保证。

Method: 提出威尔逊得分核密度分类方法，基于威尔逊得分核密度估计器构建。该估计器是一种函数估计器，专门用于估计成功概率条件变化下的二项实验置信边界。采用核方法框架，可直接作为分类头部集成到任意特征提取器（包括视觉基础模型）中。

Result: 在四个数据集的选择性分类任务评估中，所提方法展现出与高斯过程分类相当的性能表现，同时显著降低了计算复杂度。

Conclusion: 本研究为二元分类提供了一种计算高效的新型置信边界估计方法，使其在保持统计严谨性的同时更适用于实际关键任务场景。

Abstract: The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.

</details>


### [71] [Does Order Matter : Connecting The Law of Robustness to Robust Generalization](https://arxiv.org/abs/2602.20971)
*Himadri Mandal,Vishnu Varadarajan,Jaee Ponde,Aritra Das,Mihir More,Debayan Gupta*

Main category: cs.LG

TL;DR: 本文解决Bubeck和Sellke (2021)提出的开放性问题，建立鲁棒性法则与鲁棒泛化的定量联系。通过定义鲁棒泛化误差并转化为Rademacher复杂度下界，证明鲁棒泛化不改变Lipschitz常数阶数，且MNIST实验验证了Ω(n^(1/d))的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 鲁棒性法则表明过参数化模型需满足Lipschitz条件才能实现鲁棒插值，但此法则与鲁棒泛化（小训练损失能否保证小测试损失）的关系尚不明确。厘清二者联系对理解深度学习的对抗鲁棒性本质具有重要意义。

Method: 引入非平凡的鲁棒泛化误差概念，将其转化为诱导鲁棒损失类的期望Rademacher复杂度下界。基于该框架，在任意数据分布下建立两个问题的定量桥梁，并推导出紧致的理论界。

Result: 理论界恢复了Wu等人(2023)的Ω(n^(1/d)) regime，表明鲁棒泛化在常数范围内不改变平滑插值所需Lipschitz常数的阶数。MNIST实验证实下界Lipschitz常数按Wu等人(2023)预测的阶数缩放。

Conclusion: 研究表明，为获得低鲁棒泛化误差，Lipschitz常数必须处于特定有界范围，且扰动半径与Lipschitz尺度相关联。这为深度学习的鲁棒泛化提供了新理论见解，并验证了过参数化模型中鲁棒性与泛化间的根本权衡。

Abstract: Bubeck and Sellke (2021) pose as an open problem the connection between the law of robustness and robust generalization. The law of robustness states that overparameterization is necessary for models to interpolate robustly; in particular, robust interpolation requires the learned function to be Lipschitz. Robust generalization asks whether small robust training loss implies small robust test loss. We resolve this problem by explicitly connecting the two for arbitrary data distributions. Specifically, we introduce a nontrivial notion of robust generalization error and convert it into a lower bound on the expected Rademacher complexity of the induced robust loss class. Our bounds recover the $Ω(n^{1/d})$ regime of Wu et al.\ (2023) and show that, up to constants, robust generalization does not change the order of the Lipschitz constant required for smooth interpolation. We conduct experiments to probe the predicted scaling with dataset size and model capacity, testing whether empirical behavior aligns more closely with the predictions of Bubeck and Sellke (2021) or Wu et al.\ (2023). For MNIST, we find that the lower-bound Lipschitz constant scales on the order predicted by Wu et al.\ (2023). Informally, to obtain low robust generalization error, the Lipschitz constant must lie in a range that we bound, and the allowable perturbation radius is linked to the Lipschitz scale.

</details>


### [72] [Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning](https://arxiv.org/abs/2602.21020)
*Antoine Bergerault,Volkan Cevher,Negar Mehr*

Main category: cs.LG

TL;DR: 该论文研究了离线多智能体模仿学习（MA-IL）中学习策略与纳什均衡之间的差距（Nash gap）问题。首先证明了在一般n人马尔可夫博弈中学习低可利用策略的困难性和不可能性；随后在专家均衡具有占优策略或最佳响应连续性的结构假设下，给出了纳什模仿差距的理论上界O(nε_BC/(1-γ)^2)。


<details>
  <summary>Details</summary>
Motivation: 现有离线多智能体模仿学习研究虽对学习策略性能有所保证，但缺乏对学习策略与纳什均衡之间差距的严格理论刻画。本文旨在填补这一理论空白，系统分析在一般n人马尔可夫博弈中学习低可利用策略的根本性困难，并为该问题提供可行的解决路径。

Method: 1）构造反例证明即使精确测度匹配也会失败；2）建立固定测度匹配误差下纳什差距的困难性理论；3）引入专家均衡的战略支配性假设（占优策略均衡）克服前述挑战；4）提出最佳响应连续性概念，并论证其被标准正则化技术隐式促进。

Result: 1）在一般n人马尔可夫博弈中，学习低可利用策略存在本质困难，无法获得普适的理论保证；2）当专家策略为占优策略均衡时，在行为克隆误差ε_BC下，纳什模仿差距可被界定为O(nε_BC/(1-γ)^2)；3）最佳响应连续性假设在实践中可通过正则化技术实现。

Conclusion: 本文揭示了离线多智能体模仿学习的理论局限性，并证明在特定结构假设下可获得可量化的纳什差距保证。这为设计更鲁棒的多智能体模仿学习算法提供了重要的理论指导。

Abstract: Multi-agent imitation learning (MA-IL) aims to learn optimal policies from expert demonstrations of interactions in multi-agent interactive domains. Despite existing guarantees on the performance of the resulting learned policies, characterizations of how far the learned polices are from a Nash equilibrium are missing for offline MA-IL. In this paper, we demonstrate impossibility and hardness results of learning low-exploitable policies in general $n$-player Markov Games. We do so by providing examples where even exact measure matching fails, and demonstrating a new hardness result on characterizing the Nash gap given a fixed measure matching error. We then show how these challenges can be overcome using strategic dominance assumptions on the expert equilibrium. Specifically, for the case of dominant strategy expert equilibria, assuming Behavioral Cloning error $ε_{\text{BC}}$, this provides a Nash imitation gap of $\mathcal{O}\left(nε_{\text{BC}}/(1-γ)^2\right)$ for a discount factor $γ$. We generalize this result with a new notion of best-response continuity, and argue that this is implicitly encouraged by standard regularization techniques.

</details>


### [73] [T1: One-to-One Channel-Head Binding for Multivariate Time-Series Imputation](https://arxiv.org/abs/2602.21043)
*Dongik Park,Hyunwoo Ryu,Suahn Bae,Keondo Park,Hyung-Sin Kim*

Main category: cs.LG

TL;DR: 针对多变量时间序列缺失值填充的挑战，本文提出T1模型，通过CNN-Transformer混合架构和通道-注意力头绑定机制实现选择性信息传递。在11个基准数据集上，T1比次优方法平均降低46%的MSE，在70%高缺失率下表现尤为突出，且无需重新训练即可泛化到未见缺失模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多变量时间序列缺失值填充中存在两大难题：一是缺失数据会破坏时序特征，阻碍变量间有效信息传递并放大重建误差；二是难以同时实现从稀疏观测中提取时序模式和选择性跨变量信息转移。当前方法往往在两者间权衡，无法兼顾，导致在多样缺失模式和严重缺失情况下性能不佳。

Method: 提出T1（Time series imputation with 1-to-1 channel-head binding）模型，采用CNN-Transformer混合架构。核心创新是通道-注意力头绑定机制，建立CNN通道与注意力头的一一对应关系。该机制使模型能根据可观测模式自适应地调整被破坏时序模式对应的注意力权重，同时通过未受影响的通道保持可靠的跨变量连接。

Result: 在11个基准数据集上的实验表明，T1达到SOTA性能，相比次优基线平均降低46%的MSE。在极端稀疏性（70%缺失率）下提升尤为显著。模型能泛化到未见缺失模式而无需重新训练，且在所有数据集上使用一致的超参数配置。

Conclusion: T1通过创新的通道-注意力头绑定机制，有效解决了多变量时间序列填充中时序特征提取与跨变量信息选择传递的平衡问题，显著提升了在高缺失率场景下的鲁棒性，为实际应用提供了简单而强大的解决方案。

Abstract: Imputing missing values in multivariate time series remains challenging, especially under diverse missing patterns and heavy missingness. Existing methods suffer from suboptimal performance as corrupted temporal features hinder effective cross-variable information transfer, amplifying reconstruction errors. Robust imputation requires both extracting temporal patterns from sparse observations within each variable and selectively transferring information across variables--yet current approaches excel at one while compromising the other. We introduce T1 (Time series imputation with 1-to-1 channel-head binding), a CNN-Transformer hybrid architecture that achieves robust imputation through Channel-Head Binding--a mechanism creating one-to-one correspondence between CNN channels and attention heads. This design enables selective information transfer: when missingness corrupts certain temporal patterns, their corresponding attention pathways adaptively down-weight based on remaining observable patterns while preserving reliable cross-variable connections through unaffected channels. Experiments on 11 benchmark datasets demonstrate that T1 achieves state-of-the-art performance, reducing MSE by 46% on average compared to the second-best baseline, with particularly strong gains under extreme sparsity (70% missing ratio). The model generalizes to unseen missing patterns without retraining and uses a consistent hyperparameter configuration across all datasets. The code is available at https://github.com/Oppenheimerdinger/T1.

</details>


### [74] [PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis](https://arxiv.org/abs/2602.21046)
*Kunyu Zhang,Yanwu Yang,Jing Zhang,Xiangjie Shi,Shujian Yu*

Main category: cs.LG

TL;DR: ...


<details>
  <summary>Details</summary>
Motivation: ...

Method: ...

Result: ...

Conclusion: ...

Abstract: Recent deep learning methods for fMRI-based diagnosis have achieved promising accuracy by modeling functional connectivity networks. However, standard approaches often struggle with noisy interactions, and conventional post-hoc attribution methods may lack reliability, potentially highlighting dataset-specific artifacts. To address these challenges, we introduce PIME, an interpretable framework that bridges intrinsic interpretability with minimal-sufficient subgraph optimization by integrating prototype-based classification and consistency training with structural perturbations during learning. This encourages a structured latent space and enables Monte Carlo Tree Search (MCTS) under a prototype-consistent objective to extract compact minimal-sufficient explanatory subgraphs post-training. Experiments on three benchmark fMRI datasets demonstrate that PIME achieves state-of-the-art performance. Furthermore, by constraining the search space via learned prototypes, PIME identifies critical brain regions that are consistent with established neuroimaging findings. Stability analysis shows 90% reproducibility and consistent explanations across atlases.

</details>


### [75] [Localized Dynamics-Aware Domain Adaption for Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2602.21072)
*Zhangjie Xia,Yu Yang,Pan Xu*

Main category: cs.LG

TL;DR: 针对离策略离线强化学习问题，本文提出局部动力学感知域适应方法，通过聚类和域判别实现局部动力学差异估计，对源域数据进行精细化筛选，避免全局假设和逐样本计算的高开销，有效利用跨域局部相似性。


<details>
  <summary>Details</summary>
Motivation: 现有离策略离线强化学习方法在利用不同动态的源域和目标域数据时，存在全局匹配忽略局部相似性、逐点过滤计算成本高的局限。为此需要一种既能捕捉局部跨域相似性又具备计算效率的数据选择机制。

Method: 局部动力学感知域适应方法首先对源域和目标域数据进行聚类，然后在每个簇级别通过域判别器估计动力学差异，保留差异较小的簇中的源转换样本，过滤差异较大的簇，形成细粒度可扩展的数据选择策略。

Result: 在多类全局和局部动力学变化环境下的实验表明，该方法具有理论可解释性，性能持续优于现有最先进离策略离线强化学习方法，验证了局部化利用分布不匹配的有效性。

Conclusion: 局部动力学感知域适应通过局部化动力学差异估计，提供了可扩展且高效的数据选择机制，有效解决了离策略离线强化学习中的域适应问题，为处理跨域数据差异提供了新思路。

Abstract: Off-dynamics offline reinforcement learning (RL) aims to learn a policy for a target domain using limited target data and abundant source data collected under different transition dynamics. Existing methods typically address dynamics mismatch either globally over the state space or via pointwise data filtering; these approaches can miss localized cross-domain similarities or incur high computational cost. We propose Localized Dynamics-Aware Domain Adaptation (LoDADA), which exploits localized dynamics mismatch to better reuse source data. LoDADA clusters transitions from source and target datasets and estimates cluster-level dynamics discrepancy via domain discrimination. Source transitions from clusters with small discrepancy are retained, while those from clusters with large discrepancy are filtered out. This yields a fine-grained and scalable data selection strategy that avoids overly coarse global assumptions and expensive per-sample filtering. We provide theoretical insights and extensive experiments across environments with diverse global and local dynamics shifts. Results show that LoDADA consistently outperforms state-of-the-art off-dynamics offline RL methods by better leveraging localized distribution mismatch.

</details>


### [76] [Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads](https://arxiv.org/abs/2602.21081)
*Huy Trinh,Rebecca Ma,Zeqi Yu,Tahsin Reza*

Main category: cs.LG

TL;DR: 本研究针对Vision Transformers在图像处理中的计算与内存可扩展性瓶颈，首次系统性探索了DeepSpeed分布式训练框架在视觉任务中的应用。通过在CIFAR-10/100数据集上的多GPU配置实验，评估了分布式数据并行对训练速度、通信开销及强弱扩展性的影响，为ViTs的分布式训练提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers凭借自注意力机制在图像任务中表现卓越，但其巨大的计算与内存需求严重制约了模型扩展性。现有DeepSpeed框架虽在语言模型训练中成效显著，却尚未在视觉领域得到充分验证。本研究旨在填补这一空白，将DeepSpeed的高效分布式训练能力迁移至ViTs，以提升其可扩展性与训练效率。

Method: 研究采用控制变量法，在CIFAR-10和CIFAR-100数据集上系统测试了多GPU节点内与节点间的训练效率。通过调节批次大小、梯度累积步数等关键软件参数，量化分析了分布式数据并行策略对训练吞吐量、通信开销及强弱扩展性能的影响，建立了DeepSpeed在视觉任务上的性能基准。

Result: 实验结果成功建立了DeepSpeed应用于图像任务的基线性能，验证了其在Vision Transformers分布式训练中的有效性。研究识别出批次大小与梯度累积是影响多GPU扩展效率的关键因素，为后续优化提供了数据支撑。

Conclusion: 本研究证实了DeepSpeed框架可显著提升Vision Transformers的训练可扩展性。未来工作将深入剖析DeepSpeed在视觉任务中的局限性，并开发针对性的优化策略以完善分布式训练管线，推动大规模ViTs的高效训练。

Abstract: Vision Transformers (ViTs) have demonstrated remarkable potential in image processing tasks by utilizing self-attention mechanisms to capture global relationships within data. However, their scalability is hindered by significant computational and memory demands, especially for large-scale models with many parameters. This study aims to leverage DeepSpeed, a highly efficient distributed training framework that is commonly used for language models, to enhance the scalability and performance of ViTs. We evaluate intra- and inter-node training efficiency across multiple GPU configurations on various datasets like CIFAR-10 and CIFAR-100, exploring the impact of distributed data parallelism on training speed, communication overhead, and overall scalability (strong and weak scaling). By systematically varying software parameters, such as batch size and gradient accumulation, we identify key factors influencing performance of distributed training. The experiments in this study provide a foundational basis for applying DeepSpeed to image-related tasks. Future work will extend these investigations to deepen our understanding of DeepSpeed's limitations and explore strategies for optimizing distributed training pipelines for Vision Transformers.

</details>


### [77] [Probing Graph Neural Network Activation Patterns Through Graph Topology](https://arxiv.org/abs/2602.21092)
*Floriano Tori,Lorenzo Bini,Marco Sorbi,Stéphane Marchand-Maillet,Vincent Ginis*

Main category: cs.LG

TL;DR: 本文通过图Transformer中的大规模激活现象，探究图拓扑结构与GNN学习偏好的关系，揭示大规模激活不集中于曲率极值点，并发现全局注意力机制会加剧拓扑瓶颈，导致负曲率增加，从而将曲率重新定义为诊断图学习失败的探针。


<details>
  <summary>Details</summary>
Motivation: 图曲率可描述拓扑结构特征（如瓶颈和稠密区域），GNN的过度平滑与过度压缩等问题被认为源于这些区域，但图拓扑与GNN学习偏好的相互作用机制尚不明确，亟需系统性研究。

Method: 采用大规模激活（Massive Activations）作为探针，即图Transformer中极端边激活值，通过分析其在合成图、分子基准及长程图基准上的分布规律，探究图拓扑与GNN信息流的对应关系。

Result: 在合成图和分子数据集上，大规模激活并未优先出现在曲率极值区域；在长程图基准上，发现系统性曲率偏移现象：全局注意力机制会显著加剧拓扑瓶颈，导致负曲率区域大幅增加。

Conclusion: 研究将曲率概念重新定位为诊断图学习失效的探针，揭示了注意力机制与拓扑结构间的负面交互作用，为理解GNN失败模式提供了新的理论视角。

Abstract: Curvature notions on graphs provide a theoretical description of graph topology, highlighting bottlenecks and denser connected regions. Artifacts of the message passing paradigm in Graph Neural Networks, such as oversmoothing and oversquashing, have been attributed to these regions. However, it remains unclear how the topology of a graph interacts with the learned preferences of GNNs. Through Massive Activations, which correspond to extreme edge activation values in Graph Transformers, we probe this correspondence. Our findings on synthetic graphs and molecular benchmarks reveal that MAs do not preferentially concentrate on curvature extremes, despite their theoretical link to information flow. On the Long Range Graph Benchmark, we identify a systemic \textit{curvature shift}: global attention mechanisms exacerbate topological bottlenecks, drastically increasing the prevalence of negative curvature. Our work reframes curvature as a diagnostic probe for understanding when and why graph learning fails.

</details>


### [78] [Ski Rental with Distributional Predictions of Unknown Quality](https://arxiv.org/abs/2602.21104)
*Qiming Cui,Michael Dinitz*

Main category: cs.LG

TL;DR: 本文从分布预测视角重新研究滑雪租赁问题，提出一种算法其期望成本不超过OPT+O(min(max({eta},1)*sqrt(b), b log b))，其中{eta}为真实分布p与预测分布p-hat间的推土机距离。该算法在预测误差{eta}<o(sqrt(b))时加性损失小于b，在{eta}极大时仍保持O(b log b)的加性损失，实现了O(sqrt(b))一致性与O(b log b)鲁棒性的最优权衡，且无需事先知道预测误差。


<details>
  <summary>Details</summary>
Motivation: 传统滑雪租赁预测仅给出滑雪天数点估计，而分布预测更自然且强大。现有鲁棒优化方法需预先知道预测误差上界，这在实际中往往不可行。本文旨在设计无需预测误差先验信息的算法，实现预测质量与最坏情况成本的最优平衡。

Method: 在"带预测的算法"框架下，将预测形式化为分布p-hat，使用推土机距离（Wasserstein-1距离）{eta}量化预测误差。设计在线算法，其决策基于p-hat，并通过min(max({eta},1)*sqrt(b), b log b)结构统一刻画预测良好与预测恶劣两种场景下的性能损失。

Result: 主要结果：算法期望成本≤OPT+O(min(max({eta},1)*sqrt(b), b log b))。当{eta}<o(sqrt(b))时加性损失<b（优于平凡界限）；当{eta}很大时加性损失为O(b log b)。算法一致性O(sqrt(b))，鲁棒性O(b log b)，且无需知道{eta}。配套下界证明该界在一致性-鲁棒性权衡及损失函数两方面均本质紧。

Conclusion: 本文将滑雪租赁问题成功推广至分布预测设置，获得了无需预测误差先验信息的最优性能保证。上下界结果证实该算法在一致性、鲁棒性及其权衡方面达到理论极限，为"带预测的算法"提供了新的理解和应用范式。

Abstract: We revisit the central online problem of ski rental in the "algorithms with predictions" framework from the point of view of distributional predictions. Ski rental was one of the first problems to be studied with predictions, where a natural prediction is simply the number of ski days. But it is both more natural and potentially more powerful to think of a prediction as a distribution p-hat over the ski days. If the true number of ski days is drawn from some true (but unknown) distribution p, then we show as our main result that there is an algorithm with expected cost at most OPT + O(min(max({eta}, 1) * sqrt(b), b log b)), where OPT is the expected cost of the optimal policy for the true distribution p, b is the cost of buying, and {eta} is the Earth Mover's (Wasserstein-1) distance between p and p-hat. Note that when {eta} < o(sqrt(b)) this gives additive loss less than b (the trivial bound), and when {eta} is arbitrarily large (corresponding to an extremely inaccurate prediction) we still do not pay more than O(b log b) additive loss. An implication of these bounds is that our algorithm has consistency O(sqrt(b)) (additive loss when the prediction error is 0) and robustness O(b log b) (additive loss when the prediction error is arbitrarily large). Moreover, we do not need to assume that we know (or have any bound on) the prediction error {eta}, in contrast with previous work in robust optimization which assumes that we know this error.
  We complement this upper bound with a variety of lower bounds showing that it is essentially tight: not only can the consistency/robustness tradeoff not be improved, but our particular loss function cannot be meaningfully improved.

</details>


### [79] [SOM-VQ: Topology-Aware Tokenization for Interactive Generative Models](https://arxiv.org/abs/2602.21133)
*Alessandro Londei,Denise Lanzieri,Matteo Benati*

Main category: cs.LG

TL;DR: 提出SOM-VQ方法，通过融合向量量化与自组织映射构建具有显式低维拓扑的离散码本，实现语义空间的可导航几何结构，使人类能够通过直观的空间操作控制生成过程，特别适用于人体运动生成等交互场景。


<details>
  <summary>Details</summary>
Motivation: 标准向量量化表示缺乏令牌空间的语义拓扑结构，导致难以进行可解释的人工干预。在编舞、康复、人机交互等需要精细控制的领域，这种结构缺失限制了生成模型的实用性和可控性。

Method: 将自组织映射(SOM)的拓扑保持特性引入VQ-VAE框架，采用拓扑感知更新规则使学习到的网格相邻节点对应语义相似状态，构建可导航的码本几何，支持基于距离的直观空间操控。

Result: 在人体运动生成任务中，SOM-VQ产生更易学习的离散序列，同时提供显式可导航的码本空间。用户可通过网格采样简单操作实现参考序列的受控发散与收敛，达成语义对齐且无需逐帧约束。

Conclusion: SOM-VQ建立了可解释离散表示的通用框架，通过显式拓扑结构增强生成模型的可控性，其人机交互潜力可扩展至音乐、手势等时序生成领域，为交互式生成AI提供新范式。

Abstract: Vector-quantized representations enable powerful discrete generative models but lack semantic structure in token space, limiting interpretable human control. We introduce SOM-VQ, a tokenization method that combines vector quantization with Self-Organizing Maps to learn discrete codebooks with explicit low-dimensional topology. Unlike standard VQ-VAE, SOM-VQ uses topology-aware updates that preserve neighborhood structure: nearby tokens on a learned grid correspond to semantically similar states, enabling direct geometric manipulation of the latent space. We demonstrate that SOM-VQ produces more learnable token sequences in the evaluated domains while providing an explicit navigable geometry in code space. Critically, the topological organization enables intuitive human-in-the-loop control: users can steer generation by manipulating distances in token space, achieving semantic alignment without frame-level constraints. We focus on human motion generation - a domain where kinematic structure, smooth temporal continuity, and interactive use cases (choreography, rehabilitation, HCI) make topology-aware control especially natural - demonstrating controlled divergence and convergence from reference sequences through simple grid-based sampling. SOM-VQ provides a general framework for interpretable discrete representations applicable to music, gesture, and other interactive generative domains.

</details>


### [80] [Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma](https://arxiv.org/abs/2602.21168)
*Jingya Cheng,Alaleh Azhir,Jiazi Tian,Hossein Estiri*

Main category: cs.LG

TL;DR: 针对标准反事实推理在纵向临床数据中因假设特征独立与同时可修改而产生的生物学不合理性问题，本研究提出序贯反事实框架。该框架通过区分不可变特征（慢性诊断）与可控特征（检验值），并建模干预的时间传播效应，在2,723例COVID-19患者队列（383例长新冠心衰，2,340例对照）中验证：38-67%慢性病患者使用传统方法将产生不可能的反事实，并成功识别CKD→AKI→HF级联路径（RR=2.27/1.19），实现了从"特征改变"到"早期干预传播"的临床可操作解释转变。


<details>
  <summary>Details</summary>
Motivation: 临床反事实推理需要回答"若早期干预会怎样"，但标准方法假设特征独立且可同时修改，这在纵向电子健康记录中不成立。慢性疾病等不可变特征与检验值等可控特征并存，且干预效果随时间传播，导致传统方法产生不现实的反事实问题，限制了其在临床决策支持中的实用性。

Method: 设计序贯反事实框架，核心包括：1）特征分类：区分不可变特征（慢性诊断）与可控特征（检验值）；2）时序建模：构建干预在时间维度上的传播机制；3）队列验证：应用至2,723例COVID-19患者的纵向电子健康记录（383例长新冠相关心衰病例，2,340例匹配对照）；4）路径识别：通过反事实推理识别疾病进展的时间依赖路径。

Result: 实证发现：1）38-67%的慢性病患者若使用传统朴素方法，将产生生物学上不可能的反事实情景；2）识别出明确的心肾级联反应：CKD→AKI→HF，各步骤相对风险分别为2.27和1.19；3）该时间传播模式仅能被序贯框架捕获，传统方法无法识别；4）框架成功将反事实问题从静态"若某特征不同"转变为动态"若早期干预将如何时序传播"。

Conclusion: 该框架有效解决了传统反事实推理在纵向临床数据中的根本局限，通过显式建模时间依赖性与特征异质性，提供了生物学合理且临床可操作的反事实解释。这有助于临床医生理解疾病进展机制，制定基于时间窗的早期干预策略，并为精准医疗决策支持提供了新范式。

Abstract: Counterfactual inference enables clinicians to ask "what if" questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. We introduce the Sequential Counterfactual Framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time. Applied to 2,723 COVID-19 patients (383 Long COVID heart failure cases, 2,340 matched controls), we demonstrate that 38-67% of patients with chronic conditions would require biologically impossible counterfactuals under naive methods. We identify a cardiorenal cascade (CKD -> AKI -> HF) with relative risks of 2.27 and 1.19 at each step, illustrating temporal propagation that sequential -- but not naive -- counterfactuals can capture. Our framework transforms counterfactual explanation from "what if this feature were different?" to "what if we had intervened earlier, and how would that propagate forward?" --  yielding clinically actionable insights grounded in biological plausibility.

</details>


### [81] [The Diffusion Duality, Chapter II: $Ψ$-Samplers and Efficient Curriculum](https://arxiv.org/abs/2602.21185)
*Justin Deschenaux,Caglar Gulcehre,Subham Sekhar Sahoo*

Main category: cs.LG

TL;DR: 本文提出了离散扩散模型的新型预测-校正（PC）采样器，解决了均匀状态离散扩散模型在祖先采样下采样质量停滞不前的问题。PC采样器可推广至任意噪声过程，在语言和图像任务上全面超越祖先采样，且性能随采样步数持续改善。同时开发了内存高效的高斯松弛训练课程，显著降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 均匀状态离散扩散模型虽在少步生成和引导方面表现优异，但其采用祖先采样时存在质量平台期——随采样步数增加，生成质量不再提升，这限制了其性能上限。同时，现有掩码扩散模型被视为扩散语言建模的必然未来，但其优势尚未被充分验证。

Method: 1) 提出通用预测-校正（PC）采样器族，通过预测步与校正步交替迭代，将先前方法作为特例包含在内，适用于任意离散噪声过程；2) 开发内存高效的高斯松弛训练课程，优化训练阶段的资源消耗。

Result: 在OpenWebText上，PC采样器在匹配一元熵条件下实现更低生成困惑度；在CIFAR10上取得更优FID/IS分数。关键的是，PC采样器性能随采样步数持续改善，而祖先采样出现平台期。训练课程相比Duo减少25%训练时间、33%内存占用，同时在OpenWebText和LM1B上保持相当困惑度及下游任务性能。

Conclusion: 均匀状态扩散模型配合PC采样器可挑战"掩码扩散是扩散语言建模必然未来"的假设，其自校正能力与持续可扩展性展现了更强的潜力。内存高效训练方案进一步提升了实用性，为离散扩散模型的实际部署开辟了新途径。

Abstract: Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2

</details>


### [82] [Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training](https://arxiv.org/abs/2602.21189)
*Anas Barakat,Souradip Chakraborty,Khushbu Pahwa,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: 本文研究 pass@k 优化导致 pass@1 性能下降的机理，揭示其源于提示干扰引发的梯度冲突。理论表明 pass@k 会隐式重加权低成功率提示，当这些提示具有负向干扰特性时，会使更新方向偏离 pass@1 最优方向，该机制在数学推理任务的大语言模型实验中得到验证。


<details>
  <summary>Details</summary>
Motivation: 尽管 pass@k 是衡量可验证大语言模型任务性能的核心指标，但 pass@1 因延迟、成本、验证器覆盖不足及单样本回退需求常成为实际部署中的硬性约束。现有研究观察到直接优化 pass@k 会导致 pass@1 性能下降，但对此权衡缺乏理论解释，本研究旨在揭示其根本机理。

Method: 通过理论建模分析 pass@k 与 pass@1 的策略梯度关系，提出"负向干扰提示"概念。研究表明 pass@k 优化会隐式地将样本权重转向低成功率提示，当这些提示负向干扰时，会导致梯度更新方向偏离 pass@1 方向，从而降低单样本性能。

Result: 理论分析明确了 pass@k 策略优化降低 pass@1 的条件与机制，即梯度冲突由负向干扰提示引发。在数学推理任务上的大语言模型实验验证了理论发现，为理解多样本与单样本性能之间的内在矛盾提供了系统性框架。

Conclusion: 该研究揭示了 pass@k 与 pass@1 性能权衡的深层根源，强调在优化多样本指标时需兼顾单样本可靠性。研究成果为开发同时提升两个指标的新方法奠定了理论基础，对平衡推理效率与部署成本具有重要实践指导意义。

Abstract: Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.

</details>


### [83] [Statistical Query Lower Bounds for Smoothed Agnostic Learning](https://arxiv.org/abs/2602.21191)
*Ilias Diakonikolas,Daniel M. Kane*

Main category: cs.LG

TL;DR: 本文研究了平滑不可知学习(smoothed agnostic learning)的复杂度，证明了半空间学习的统计查询(SQ)下界为d^{Ω(1/σ²+log(1/ε))}，该下界与已知的L1多项式回归上界d^{O~(1/σ²)log(1/ε)}基本匹配，表明现有算法已接近最优。


<details>
  <summary>Details</summary>
Motivation: 平滑不可知学习是学习理论中的重要模型，它通过轻微高斯扰动输入来缓解最坏情况下的困难。理解其复杂度边界对于设计高效学习算法和建立理论极限至关重要。目前缺乏非平凡的下界结果，无法判断已知算法的紧性。

Method: 采用统计查询(SQ)模型下界分析技术，通过线性规划对偶性构造矩匹配困难分布。具体地，将对偶问题转化为寻找目标函数平滑版本的低次近似多项式，并证明该近似次数需要足够高，从而导出SQ复杂度下界。

Result: 证明了任何SQ算法在平滑模型下学习半空间都需要d^{Ω(1/σ²+log(1/ε))}的复杂度。这是该问题首个非平凡下界，与已知上界d^{O~(1/σ²)log(1/ε)}基本匹配，表明L1多项式回归方法本质上是最优的。

Conclusion: 该工作填补了平滑不可知学习理论中下界缺失的空白，为L1多项式回归算法的紧性提供了理论依据。同时，线性规划对偶性结合近似多项式的方法为分析类似学习问题提供了新框架。

Abstract: We study the complexity of smoothed agnostic learning, recently introduced by~\cite{CKKMS24}, in which the learner competes with the best classifier in a target class under slight Gaussian perturbations of the inputs. Specifically, we focus on the prototypical task of agnostically learning halfspaces under subgaussian distributions in the smoothed model. The best known upper bound for this problem relies on $L_1$-polynomial regression and has complexity $d^{\tilde{O}(1/σ^2) \log(1/ε)}$, where $σ$ is the smoothing parameter and $ε$ is the excess error. Our main result is a Statistical Query (SQ) lower bound providing formal evidence that this upper bound is close to best possible. In more detail, we show that (even for Gaussian marginals) any SQ algorithm for smoothed agnostic learning of halfspaces requires complexity $d^{Ω(1/σ^{2}+\log(1/ε))}$. This is the first non-trivial lower bound on the complexity of this task and nearly matches the known upper bound. Roughly speaking, we show that applying $L_1$-polynomial regression to a smoothed version of the function is essentially best possible. Our techniques involve finding a moment-matching hard distribution by way of linear programming duality. This dual program corresponds exactly to finding a low-degree approximating polynomial to the smoothed version of the target function (which turns out to be the same condition required for the $L_1$-polynomial regression to work). Our explicit SQ lower bound then comes from proving lower bounds on this approximation degree for the class of halfspaces.

</details>


### [84] [Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking](https://arxiv.org/abs/2602.21196)
*Ravi Ghadia,Maksim Abraham,Sergei Vorobyov,Max Ryabinin*

Main category: cs.LG

TL;DR: 针对Transformer长序列处理的内存瓶颈问题，提出UPipe方法，通过注意力头级别的细粒度分块显著降低激活内存，在保持训练速度的同时支持更长上下文长度。


<details>
  <summary>Details</summary>
Motivation: 当前主流上下文并行方法如Ring Attention和DeepSpeed Ulysses虽能扩展上下文维度，但内存效率不足，限制可支持的序列长度。更先进的方法如全流水线分布式Transformer或激活卸载虽能扩展上下文，但会牺牲训练吞吐量。

Method: UPipe采用注意力头级别的细粒度分块技术，将自注意力计算分解为更小的单元，从而显著降低中间张量内存使用。

Result: 对于32B参数Transformer，UPipe将注意力层中间张量内存降低高达87.5%，在Llama3-8B模型上单节点8×H100可支持500万token上下文长度，相比先前方法提升超过25%。

Conclusion: UPipe有效解决了自注意力激活内存瓶颈，在保持训练速度的前提下解锁了更长的上下文处理能力，为长序列建模提供了新解决方案。

Abstract: Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\times$H100 node, improving upon prior methods by over 25$\%$.

</details>


### [85] [Test-Time Training with KV Binding Is Secretly Linear Attention](https://arxiv.org/abs/2602.21204)
*Junchen Liu,Sven Elflein,Or Litany,Zan Gojcic,Ruilong Li*

Main category: cs.LG

TL;DR: 本文通过分析测试时训练(TTT)与键值绑定作为序列建模层的机制，揭示其与传统记忆化解释的矛盾现象，将其重新表述为学习的线性注意力算子，从而解释异常行为并获得架构简化、并行化效率提升等实用收益。


<details>
  <summary>Details</summary>
Motivation: 现有研究将TTT解释为测试时记忆化的在线元学习，但作者发现多个与此解释相悖的现象，这促使他们重新审视TTT的理论基础，以建立更准确的数学表述。

Method: 研究方法包括：1) 系统分析TTT与传统记忆化解释的矛盾现象；2) 重新推导TTT公式，证明其可表达为学习的线性注意力算子；3) 将各种TTT变体系统性地归约为标准线性注意力形式。

Result: 研究结果包括：1) 成功解释了先前无法理解的模型行为；2) 实现了原则性的架构简化；3) 提出了完全并行化方案，在保持性能的同时显著提升计算效率；4) 系统性地将多样TTT变体统一为标准线性注意力框架。

Conclusion: 研究结论将TTT的本质重新界定为具有增强表示能力的学习的线性注意力机制，而非简单的测试时记忆过程，为理解和发展测试时自适应方法提供了新的理论基础。

Abstract: Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health](https://arxiv.org/abs/2602.20303)
*Joyanta Jyoti Mondal*

Main category: cs.AI

TL;DR: 本研究基于2021年美国儿童健康调查数据（n=18,792），比较了逻辑回归、集成学习及深度学习模型在预测青少年超重肥胖方面的表现。研究发现，尽管复杂模型在小幅提升召回率和F1分数方面略有优势，但逻辑回归、梯度提升和多层感知器在判别与校准平衡性上表现最稳定。不同算法在种族和贫困亚组间均存在性能差异，提示需要改进数据质量和公平性监测，而非单纯追求模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 美国青少年超重肥胖是重要的公共卫生问题，受行为、家庭和社区多层面因素影响，但这些因素的联合预测结构在人群层面尚未完全明确。现有研究缺乏对统计学习与深度学习方法在预测性能和亚组公平性方面的系统比较，无法为精准预防提供方法学指导。

Method: 研究采用2021年全国儿童健康调查数据，纳入18,792名10-17岁儿童。以BMI定义的超重/肥胖为结局变量，涵盖饮食、运动、睡眠、父母压力、社会经济状况、不良经历和社区特征等多维度预测因子。比较了逻辑回归、随机森林、梯度提升、XGBoost、LightGBM、多层感知器和TabNet七种模型，采用AUC、准确率、精确率、召回率、F1分数和Brier评分评估性能。

Result: 各模型AUC在0.66-0.79之间。逻辑回归、梯度提升和多层感知器在判别能力与校准度平衡上最为稳定。集成学习和深度学习仅轻微提升了召回率和F1分数，未表现出全面优势。所有模型在种族和贫困亚组间均存在持续的性能差异。

Conclusion: 增加模型复杂度相比逻辑回归仅带来有限改进。预测因子持续覆盖行为、家庭和社区多个层面。亚组差异的持续存在表明，提升数据质量和建立公平性导向的监测体系比追求更复杂的算法更为重要。

Abstract: Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.

</details>


### [87] [DMCD: Semantic-Statistical Framework for Causal Discovery](https://arxiv.org/abs/2602.20333)
*Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: DMCD是一种两阶段因果发现框架，先利用大语言模型从变量元数据生成语义化的稀疏因果图草案，再通过条件独立性测试进行统计验证和修正，在三个真实世界基准测试中表现优异，尤其提高了召回率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决因果发现问题中如何利用变量元数据中的丰富语义信息来指导因果结构学习。传统方法主要依赖统计数据，而忽视了元数据中蕴含的领域知识和语义关系。通过结合大语言模型的语义推理能力与统计验证，可以提升因果发现的效果和实用性。

Method: DMCD采用两阶段方法：第一阶段（Phase I）使用大语言模型基于变量元数据生成一个稀疏的因果有向无环图（DAG）草案，作为可能因果结构空间的语义先验；第二阶段（Phase II）通过条件独立性测试对该草案进行审计和精修，检测到的差异指导针对性地修订图中的边。

Result: 在工业工程、环境监测和IT系统分析三个元数据丰富的真实世界基准数据集上，DMCD取得了与现有基线方法相当或领先的性能，特别是在召回率和F1分数方面有显著提升。探针和消融实验表明，这些改进源于对元数据的语义推理而非对基准图的记忆。

Conclusion: 研究结果表明，将语义先验与原则性统计验证相结合，是一种高性能且实际有效的因果结构学习方法。该方法充分利用了元数据中的语义信息，并通过统计测试确保结果的可靠性，为因果发现提供了新的有效范式。

Abstract: We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.
  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.

</details>


### [88] [Diffusion Modulation via Environment Mechanism Modeling for Planning](https://arxiv.org/abs/2602.20422)
*Hanping Zhang,Yuhong Guo*

Main category: cs.AI

TL;DR: 该论文提出了一种新的基于扩散模型的规划方法（DMEMM），通过融入环境机制建模来解决离线强化学习中轨迹生成的一致性问题，取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于扩散模型的规划方法未能考虑强化学习中状态转移必须保持独特一致性的要求，导致生成的轨迹与真实环境机制存在较大偏差。

Method: 提出扩散环境机制调制（DMEMM）方法，通过在扩散模型训练中显式融入关键的强化学习环境机制（特别是状态转移动态和奖励函数）来调制生成过程。

Result: 实验结果表明，DMEMM在离线强化学习规划任务上达到了最先进的性能水平。

Conclusion: 该方法通过将环境机制知识整合到扩散生成过程中，有效提升了轨迹的连贯性和实用性，为离线强化学习规划提供了新的有效解决方案。

Abstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.

</details>


### [89] [Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use](https://arxiv.org/abs/2602.20426)
*Ruocheng Guo,Kaiwen Dong,Xiang Gao,Kamalika Das*

Main category: cs.AI

TL;DR: 本文提出Trace-Free+，一种无执行轨迹的课程学习框架，通过从富轨迹环境向无轨迹部署渐进迁移监督信号，优化LLM智能体的工具接口质量。该框架能抽象可复用的接口使用模式和工具调用结果，解决了大候选工具集下的接口瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度聚焦智能体微调，忽视了工具接口（自然语言描述和参数模式）的质量瓶颈。这些接口本就面向人类设计，在大型工具选择场景中成为性能障碍。传统方法依赖难以获取的执行轨迹（尤其在冷启动或隐私约束场景），且独立优化各工具，导致可扩展性差、泛化能力弱。

Method: 作者提出Trace-Free+课程学习框架，逐步将监督从轨迹丰富设置迁移至无轨迹部署环境，使模型学习抽象化、可复用的接口使用模式与工具调用结果。同时构建大规模高质量工具接口数据集，通过结构化工作流在多样化工具集合上自动生成。

Result: 在StableToolBench和RestBench上的实验显示，该方法在未见工具上实现持续性能提升，具备强大的跨领域泛化能力，且当候选工具规模扩展至100个以上时仍保持鲁棒性。

Conclusion: 工具接口优化是智能体微调的实用且可部署的补充方案，能有效缓解大规模工具选择瓶颈，提升LLM智能体在实际场景中的适应性与性能。

Abstract: The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.

</details>


### [90] [KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning](https://arxiv.org/abs/2602.20494)
*Haotian Si,Changhua Pei,Xiao He,Zeyan Li,Zhe Xie,Zexin Wang,Jiyao Hu,Zhaoyang Yu,Tieying Zhang,Dan Pei,Jianhui Li,Gaogang Xie*

Main category: cs.AI

TL;DR: 本文提出语义条件时间序列推理任务及KairosVL模型，通过两阶段强化学习框架提升时间序列分析能力，在合成和真实世界任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析需求日益复杂且面向决策，现有方法局限于纯数值建模，缺乏语义和上下文理解。迫切需要结合语义推理与时间序列建模以实现智能决策。

Method: 提出两阶段强化学习框架：第一阶段强化时序基元感知能力，第二阶段专注语义条件推理。最终得到KairosVL模型，将语义理解融入时间序列分析。

Result: KairosVL在合成和真实世界任务中均取得竞争性能。该框架不仅提升性能，还保持内在推理能力并显著改善对未知场景的泛化能力。

Conclusion: 本工作证明了语义推理与时间序列建模结合的巨大潜力，为现实世界时间序列智能提供了实用框架，满足紧迫的行业需求。

Abstract: Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.

</details>


### [91] [Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination](https://arxiv.org/abs/2602.20517)
*Rakshit Trivedi,Kartik Sharma,David C Parkes*

Main category: cs.AI

TL;DR: MIMIC框架利用视觉-语言模型训练cVAE生成"内心独白"作为行为意图的内部表示，结合扩散策略实现推理时可引导的模仿学习，在机器人与协作任务中显著提升行为多样性和保真度。


<details>
  <summary>Details</summary>
Motivation: 构建能在变化环境中展现并响应类人行为的AI智能体是人机协作的核心需求。模仿学习虽有效，但现有方法难以捕捉人类行为的多样性与非马尔可夫特性，且缺乏推理时的行为引导能力。受人类内心独白认知机制启发，本文旨在解决这些局限。

Method: 提出MIMIC框架：1）用视觉-语言模型作语言支架，训练条件变分自编码器从观察生成内心独白；2）采用基于扩散的行为克隆策略，以当前观察和内心独白为条件选择动作；3）推理时通过条件化特定行为语言描述实现细粒度行为引导。

Result: 在机器人操作和人类-AI协作游戏中，MIMIC显著提升行为多样性与演示保真度，且无需额外训练即可实现细致行为控制。代码和预训练模型已开源。

Conclusion: MIMIC通过语言内部表示成功引入人类认知机制，有效解决模仿学习中的多样性与可控性问题，为人机协作提供了新范式。

Abstract: Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.

</details>


### [92] [CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation](https://arxiv.org/abs/2602.20571)
*Ayush Sawarni,Jiyuan Tan,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: 本文提出CausalReasoningBenchmark，一个包含173个查询和138个真实数据集的因果推理基准测试，通过分离评估因果识别策略和数值估计，揭示出现有AI系统在研究设计细节上的不足，基线模型完整识别正确率仅30%。


<details>
  <summary>Details</summary>
Motivation: 现有自动化因果推断基准测试仅基于单一数值输出（如平均处理效应）评估性能，混淆了因果分析中两个关键步骤：基于假设制定有效研究设计的"识别"步骤，以及在有限数据上数值化实施该设计的"估计"步骤，导致无法诊断系统具体缺陷。

Method: 研究人员从85篇同行评审论文和四本广泛使用的因果推断教材中，整理出173个查询和138个真实世界数据集。每个查询要求系统分别提供：（i）结构化识别规范，包括策略、处理变量、结果变量、控制变量及所有设计特定元素；（ii）点估计值和標準誤，并独立评分这两个组件。

Result: 基线测试显示，先进大语言模型能正确识别84%案例的高层策略，但完整识别规范正确率骤降至仅30%，证明系统瓶颈在于研究设计的细微细节，而非数值计算执行能力。

Conclusion: 该基准测试通过分离评估因果推理与数值执行，实现了对自动化因果推断系统的颗粒化诊断，为开发更稳健的系统提供了关键工具，已公開在Hugging Face供研究使用。

Abstract: Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.

</details>


### [93] [When can we trust untrusted monitoring? A safety case sketch across collusion strategies](https://arxiv.org/abs/2602.20628)
*Nelson Gardner-Challis,Jonathan Bostock,Georgiy Kozhevnikov,Morgan Sinclaire,Joan Velja,Alessandro Abate,Charlie Griffin*

Main category: cs.AI

TL;DR: 本文针对未对齐AI可能造成的灾难性风险，研究不可信监控的安全验证问题。通过放松先前关于合谋策略的假设，建立了被动自我识别、因果共谋、非因果共谋及组合策略的分类体系，构建安全案例草图识别关键假设与挑战，发现被动自我识别在某些条件下可能优于已有策略，推动更稳健的部署前安全评估。


<details>
  <summary>Details</summary>
Motivation: 随着AI自主性和能力增强，未对齐AI引发灾难性风险上升。不可信监控作为风险缓解手段，面临无法安全部署未对齐模型进行协议测试的根本性挑战，亟需基于部署前测试的严格安全证明方法。

Method: 在现有部署前测试框架基础上，放松先前AI控制研究对未对齐AI颠覆监控的合谋策略假设；提出涵盖被动自我识别、因果共谋（隐藏预共享信号）、非因果共谋（利用谢林点隐藏信号）及组合策略的完备分类法；构建安全案例草图以结构化呈现论证逻辑、明确假设并凸显未解难题。

Result: 系统性地识别了被动自我识别策略的有效性条件，证明其在特定场景下超越传统共谋策略的潜力，为不可信监控的安全评估提供了更全面的理论分类与风险分析框架。

Conclusion: 研究推进了不可信监控的稳健评估方法学，但仍需解决剩余挑战以完善安全论证体系，为高风险AI系统的安全部署奠定理论基础。

Abstract: AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.

</details>


### [94] [Identifying two piecewise linear additive value functions from anonymous preference information](https://arxiv.org/abs/2602.20638)
*Vincent Auriau,Khaled Belahcene,Emmanuel Malherbe,Vincent Mousseau,Marc Pirlot*

Main category: cs.AI

TL;DR: 本文针对两位决策者的同步偏好elicitation问题，提出一种识别方法。在假设偏好可用可加价值函数表示、边际价值函数为分段线性且断点已知的前提下，通过同时提问并获得两个无噪声答案（不知归属），成功识别出两个偏好模型。


<details>
  <summary>Details</summary>
Motivation: 现有偏好elicitation研究多聚焦于单个决策者。但在群体决策场景中，同时了解多个决策者偏好在所难免。若分别查询效率低下，而同步查询面临答案身份不明（answer anonymization）的核心挑战。本文旨在解决这一身份识别问题。

Method: 提出一种elicitation流程，利用可加价值函数结构，在已知边际价值函数分段断点的条件下，通过设计同步查询策略，从混合的答案中解耦出两个决策者的各自偏好参数。

Result: 在答案无噪声、边际价值函数分段线性且断点已知的假设下，所提方法能够准确识别并区分两位决策者的价值函数，实现偏好模型的有效恢复。

Conclusion: 研究表明，在可加价值函数和分段线性结构假设下，同步查询多位决策者并识别其偏好模型是可行的，为群体偏好建模提供了理论保障和实践算法。

Abstract: Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.

</details>


### [95] [Grounding LLMs in Scientific Discovery via Embodied Actions](https://arxiv.org/abs/2602.20639)
*Bo Zhang,Jinfeng Zhou,Yuxuan Chen,Jianing Yin,Minlie Huang,Hongning Wang*

Main category: cs.AI

TL;DR: 本文提出EmbodiedAct框架，通过将大型语言模型嵌入到具身动作的紧密感知-执行循环中，将科学软件转化为主动智能体，解决了LLM在连接理论推理与物理仿真验证方面的不足，在MATLAB实现的复杂工程设计与科学建模任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在科学发现中展现出巨大潜力，但现有被动"执行-响应"循环缺乏运行时感知能力，无法有效捕捉数值不稳定性或发散振荡等瞬态异常，难以弥合理论推理与可验证物理仿真之间的鸿沟。

Method: 提出EmbodiedAct框架，通过具身动作 grounding 大型语言模型，建立紧密的感知-执行闭环，将成熟的科学软件（如MATLAB）转化为主动具身智能体，实现运行时动态监控与异常处理。

Result: 大量实验表明，EmbodiedAct显著优于现有基线方法，在长期仿真中表现出优异的可靠性和稳定性，并在科学建模任务中实现更高精度，整体达到SOTA性能。

Conclusion: EmbodiedAct框架通过引入主动运行时感知机制，有效解决了被动执行模式的缺陷，为LLM在科学仿真与工程建模中的应用提供了新范式。

Abstract: Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive "execute-then-response" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.

</details>


### [96] [Recursive Belief Vision Language Model](https://arxiv.org/abs/2602.20659)
*Vaidehi Bagaria,Bijo Sebastian,Nirav Patel*

Main category: cs.AI

TL;DR: 针对视觉-语言-动作模型在长时程部分可观测操作任务中的局限性，本文提出RB-VLA，一种以信念为中心的架构。该模型通过自监督世界模型目标学习紧凑的潜在状态表示，仅查询一次视觉-语言模型获取高层意图，信念模块追踪任务进度并实现阶段感知的因果控制。在基准测试中，RB-VLA在多层拾取放置和堆叠任务上成功率分别比π0高出52.5%和37.5%，推理延迟降低5倍，且消除了内存随时间增长的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在长时程操作任务中面临部分可观测性挑战，主要问题包括：依赖观测驱动、上下文窗口短、重复查询视觉-语言模型，导致任务进度丢失、感知混淆下的动作重复以及高推理延迟。根本原因在于缺乏持久且动作条件化的状态表示，以及有限的时序和物理推理能力，使其不适合多阶段控制。

Method: 提出RB-VLA信念中心架构，通过自监督世界模型目标训练，维护一个紧凑的潜在状态，编码任务相关历史、动力学和物体交互。视觉-语言模型仅查询一次以获取高层意图，信念模块追踪任务进度，在不存储原始观测或随时间扩展内存的情况下，实现部分可观测下的阶段感知因果控制。信念和意图共同调节扩散策略以执行鲁棒闭环控制。

Result: 在长时程基准测试中表现优异：在多层拾取放置和堆叠任务上，成功率分别比π0高出52.5%和37.5%；推理延迟最高降低5倍；消除了现有VLA模型中观察到的跨时间步内存增长问题。消融研究表明，信念模块是性能的主要驱动因素，将成功率从32.5%提升至77.5%。

Conclusion: 基于信念的状态表示对于长时程视觉-语言-动作策略的有效性得到了验证，为解决部分可观测下的多阶段操作任务提供了新思路。

Abstract: Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.

</details>


### [97] [How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective](https://arxiv.org/abs/2602.20687)
*Bo Peng,Pi Bu,Keyu Pan,Xinrun Xu,Yinxiu Zhao,Miao Chen,Yang Du,Lin Li,Jun Song,Tong Xu*

Main category: cs.AI

TL;DR: 针对现有VLM体感智能体基准测试使用非本地高级动作空间且缺乏高低级联合评估的问题，本文提出NativeEmbodied基准，采用统一本地低级动作空间，包含3个高级复杂任务和4个基础技能低级任务，通过联合评估揭示当前模型在基础技能上的关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要采用高级指令或离散化动作空间，这些设置与真实世界控制环境存在显著差异，且仅关注高级任务，缺乏对基础体感技能的系统性评估，无法全面诊断智能体能力瓶颈。

Method: 构建NativeEmbodied基准，特点包括：1) 统一的本地低级动作空间；2) 基于多样化仿真场景；3) 三层高级任务评估整体性能；4) 四类低级任务解耦基础技能；5) 跨任务与技能粒度的联合评估框架。

Result: 对先进VLMs的评估显示，其在若干基础体感技能上存在明显不足，这些技能缺陷显著制约了复杂高级任务的性能表现。

Conclusion: NativeEmbodied基准明确了当前VLM驱动体感智能体的核心挑战，通过精细化的技能分析为未来研究提供了重要指导方向。

Abstract: Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.

</details>


### [98] [PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding](https://arxiv.org/abs/2602.20696)
*Baolong Bi,Yuyao Ge,Shenghua Liu,Yuchen He,Siqian Tong,Lizhe Chen,Lingrui Mei,Zehao Li,Yiwei Wang,Yujun Cai,Ming-Hsuan Yang,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出PromptCD，一种测试时行为控制方法，通过正负提示对比解码，无需训练即可提升LLMs和VLMs的行为对齐，在3H目标和VQA任务上均取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有AI对齐方法主要在训练时进行，依赖高质量数据且计算成本高昂；对比解码虽能利用模型内部分布，但仅适用于狭窄行为范围。为此，本文旨在开发一种测试时、通用且低成本的行为增强方法，以广泛提升模型行为控制能力。

Method: PromptCD针对目标行为构建成对的正负引导提示，通过对比LLMs的token级概率分布和VLMs的视觉注意力模式来强化期望输出。该方法将对比解码扩展至广泛的增强目标，适用于LLMs和视觉语言模型，且无需任何额外训练。

Result: 在LLMs上，针对帮助性、诚实性、无害性（3H）对齐目标的实验表明，PromptCD能持续且显著地提升性能，证明后训练模型可在测试时实现有意义自我增强。在VLMs上，该方法通过强化行为一致的视觉grounding显著提升VQA性能，并改善视觉注意力模式。

Conclusion: PromptCD是一种简单、通用且成本高效的跨模态行为控制策略，能够在不增加训练开销的前提下，可靠地提升LLMs和VLMs的行为对齐质量，为AI系统可靠性提供了新思路。

Abstract: Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the "3H" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.

</details>


### [99] [Online Algorithms with Unreliable Guidance](https://arxiv.org/abs/2602.20706)
*Julien Dallot,Yuval Emek,Yuval Gil,Maciej Pacut,Stefan Schmid*

Main category: cs.AI

TL;DR: 本文提出一种名为OAG（带不可靠指导的在线算法）的机器学习增强型在线决策新模型。该模型通过请求-回答游戏框架将预测与算法组件完全分离，并设计DTB编译器将任意在线算法转化为学习增强型算法。尽管机制简单，该算法在缓存和均匀度量任务系统问题上达到最优，在二分图匹配问题上优于现有最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有研究将预测与算法组件耦合，缺乏统一分析框架，且平滑性需要专门设计的损失函数。本文旨在建立完全分离预测与算法组件、并能自然实现平滑性的新模型。

Method: 在OAG模型中，每个请求附带来自答案空间的指导，以概率β被对抗性破坏。DTB编译器采用随机策略：对每个请求，通过有偏硬币抛掷决定是盲目遵循指导还是完全忽略并执行原算法。

Result: 理论证明DTB编译器生成的算法具有一致性-鲁棒性保证：在缓存和均匀度量任务系统问题上达到理论最优；在对抗性到达顺序的二分图匹配问题上优于现有最先进算法。

Conclusion: OAG模型为机器学习增强在线决策提供了统一框架，DTB编译器虽简单却能在多个经典问题上实现最优性能，展现出良好的理论价值和应用潜力。

Abstract: This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $β$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $β= 0$ (a.k.a. consistency) as well as when $β= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $β$ shifts from $0$ to $1$.
  We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.

</details>


### [100] [Counterfactual Simulation Training for Chain-of-Thought Faithfulness](https://arxiv.org/abs/2602.20710)
*Peter Hase,Christopher Potts*

Main category: cs.AI

TL;DR: 本文提出反事实模拟训练(CST)方法，通过奖励能使模拟器准确预测模型在反事实输入上输出的思维链(CoT)，来提升CoT忠实度。在235B参数模型上，CST将提示线索反事实监控准确率提升35个百分点，通用反事实可模拟性提升2个百分点，显著优于提示基线，为模型行为监控提供了有效工具。


<details>
  <summary>Details</summary>
Motivation: 思维链(CoT)是理解大型语言模型(LM)输出归因的主流方法，但其忠实度问题严重限制了可获得的洞察。现有CoT可能无法反映模型真实决策逻辑，导致难以检测模型对虚假特征的依赖、奖励黑客行为或主观迎合现象，亟需提升CoT的可靠性和可解释性。

Method: 提出反事实模拟训练(CST)，核心机制是训练一个"模拟器"来根据CoT预测模型在反事实输入上的输出，并奖励预测准确的CoT。该方法在两个场景实施：(1) 基于人工设计线索的提示反事实监控，用于识别模型缺陷行为；(2) 基于模型生成的通用反事实模拟，促进CoT的忠实度和泛化能力。

Result: 实验表明：CST在提示线索反事实上将监控准确率提升35点，在通用反事实上提升可模拟性2点；CST显著优于提示基线方法；利用LLM重写不忠实CoT的效率是纯强化学习的5倍；忠实度改进不适用于劝阻性线索；大模型开箱即用时不具备更忠实CoT，但CST对其提升效果更显著。

Conclusion: CST为提升CoT忠实度提供了通用解决方案，在模型监控、可解释性分析等领域具有重要应用价值。该方法通过反事实推理机制有效改善了模型行为的可预测性和透明度，为构建更可靠的AI系统奠定了方法论基础。

Abstract: Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training

</details>


### [101] [Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning](https://arxiv.org/abs/2602.20722)
*Xu Wan,Yansheng Wang,Wenqi Huang,Mingyang Sun*

Main category: cs.AI

TL;DR: 针对大语言模型后训练中的效率瓶颈，本文提出离线策略的BAPO框架，通过动态批次适应机制复用高质量经验，平均超越GRPO 12.5%并解决40.7%的模型顽固难题。


<details>
  <summary>Details</summary>
Motivation: 传统在线策略强化学习与可验证奖励（RLVR）框架存在经验浪费和奖励同质化问题，导致大语言模型在困难样本上的学习效率严重受限，亟需提升数据利用效率。

Method: 提出Batch Adaptation Policy Optimization（BAPO）离线策略RLVR框架，通过重新评估历史困难样本并重复利用高质量样本动态构建训练批次，同时提供策略改进的理论下界保证。

Result: 在数学、规划和视觉推理任务上，BAPO相比GRPO实现平均12.5%的性能提升，并成功解决了40.7%的基础模型持续无法解决的困难问题。

Conclusion: BAPO通过创新的批次适应机制显著提高了大语言模型后训练的数据效率，为处理困难样本提供了有效的离线策略解决方案，展现出广泛的任务适应性和实用性。

Abstract: Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.

</details>


### [102] [Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2602.20728)
*Chenyang Zhao,Vinny Cahill,Ivana Dusparic*

Main category: cs.AI

TL;DR: 本文将AI反馈强化学习(RLAIF)扩展至多目标自适应系统，证明其能无需繁琐奖励工程即可生成反映用户偏好的均衡权衡策略。


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习的奖励设计在多目标场景下尤为困难，存在目标冲突和策略偏向主导目标的风险。虽然偏好基RL和RLAIF可缓解人工标注依赖，但现有方法仅处理单目标，无法应对多目标权衡挑战。

Method: 探索将RLAIF范式扩展至多目标自适配系统，利用大规模语言模型规模化生成偏好标签以指导策略学习。

Result: 多目标RLAIF能产生反映不同用户优先级的均衡权衡策略，避免繁琐的手动奖励工程。

Conclusion: 将RLAIF集成至多目标强化学习为实现冲突目标领域中用户对齐的策略学习提供了可扩展路径。

Abstract: Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.

</details>


### [103] [CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference](https://arxiv.org/abs/2602.20732)
*Chao Fei,Guozhong Li,Chenxi Liu,Panos Kalnis*

Main category: cs.AI

TL;DR: 针对长上下文LLM的KV缓存瓶颈，本文提出CHESS算法-系统协同设计方案。该方法通过上下文感知的分层选择策略动态重构相干上下文，结合粗粒度选择机制消除数据移动开销，仅使用1% KV缓存即超越全KV质量，实现最高4.56倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理需要低延迟，但解码过程受KV缓存限制。现有剪枝方法存在两大缺陷：一是上下文无关的token选择忽略步级相关性和局部语义，损害生成质量；二是不规则访问和选择开销导致实际加速有限。

Method: CHESS采用算法-系统协同设计：算法层面引入上下文感知的分层选择策略，动态重构当前解码的相干上下文；系统层面采用粗粒度选择消除昂贵数据移动，充分释放理论稀疏性带来的实际加速。

Result: 实验表明，CHESS仅用1% KV缓存即超越全KV质量，提供低延迟稳定推理，吞吐量最高提升4.56倍，且持续优于其他强基线。

Conclusion: CHESS通过算法-系统协同优化，在极低KV缓存占用下实现了高质量低延迟推理，有效解决了长上下文LLM的扩展性瓶颈。

Abstract: Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \textbf{CHESS}, an \textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \textbf{1\%} of the KV cache, delivers low-latency stable inference with up to \textbf{4.56$\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.

</details>


### [104] [PyVision-RL: Forging Open Agentic Vision Models via RL](https://arxiv.org/abs/2602.20739)
*Shitian Zhao,Shaoheng Lin,Ming Li,Haoquan Zhang,Wenshuo Peng,Kaipeng Zhang,Chen Wei*

Main category: cs.AI

TL;DR: 针对多模态智能体强化学习中的交互崩溃问题，提出PyVision-RL框架，通过过采样-过滤-排序策略和累积工具奖励维持多轮交互，并采用按需上下文构建降低视觉token使用，在图像和视频任务上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态智能体强化学习存在交互崩溃问题，模型会自发减少工具调用与多轮推理，限制了智能体行为的潜在优势，亟需稳定训练并维持持续交互的方法。

Method: 提出PyVision-RL框架，结合过采样-过滤-排序的轨迹采样策略与累积工具奖励机制防止交互崩溃；针对视频推理设计按需上下文构建方法，在推理过程中选择性采样任务相关帧以减少视觉token消耗。

Result: 在统一训练流程下开发的PyVision-Image和PyVision-Video模型展现出强劲性能与显著效率提升，视频处理中视觉token使用量大幅降低，验证了持续交互与按需视觉处理的有效性。

Conclusion: 维持持续交互能力与按需视觉处理机制对构建可扩展的多模态智能体至关重要，PyVision-RL为解决交互崩溃问题提供了有效方案，为未来智能体训练框架设计指明了方向。

Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.

</details>


### [105] [POMDPPlanners: Open-Source Package for POMDP Planning](https://arxiv.org/abs/2602.20810)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: 本文提出POMDPPlanners，一个用于评估POMDP规划算法的开源Python工具包。该工具集成先进算法、基准环境、超参数优化、缓存与并行仿真功能，专门针对风险敏感场景，旨在提升POMDP实证研究的可扩展性与可重复性。


<details>
  <summary>Details</summary>
Motivation: 标准工具包在风险敏感决策场景下存在局限，且大规模仿真研究计算开销巨大。本研究旨在开发专用评估平台，填补现有工具空白，支持不确定性决策研究的可重复性与可扩展性。

Method: 构建开源Python软件包，整合：1) 前沿POMDP规划算法；2) 含安全关键变体的基准环境；3) 基于Optuna的自动化超参数调优；4) 支持故障恢复的持久化缓存机制；5) 可配置的并行仿真框架。

Result: 该工具包有效降低大规模仿真研究的技术门槛与计算开销，为风险敏感场景下的POMDP算法评估提供了高效、可复现的实验平台，显著提升研究效率。

Conclusion: POMDPPlanners成功弥补了现有工具在风险敏感POMDP评估方面的不足，为不确定性决策领域提供了强大的开源基础设施，推动了可扩展、可重复的实证研究发展。

Abstract: We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.

</details>


### [106] [Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset](https://arxiv.org/abs/2602.20812)
*Jia-Rui Lin,Yun-Hong Cai,Xiang-Rui Ni,Shaojie Zhou,Peng Pan*

Main category: cs.AI

TL;DR: 针对建筑信息模型（BIM）设计领域缺乏专用数据集和评估基准导致大语言模型（LLM）性能受限的问题，本研究提出：1）BIM设计评估基准及量化指标体系；2）BIM文本数据生成与数据集构建方法；3）领域自适应微调策略。所开发的140亿参数Qwen-BIM模型性能媲美6710亿参数的通用大模型，为BIM领域大模型开发奠定基础。


<details>
  <summary>Details</summary>
Motivation: 建筑行业数字化转型进程中，BIM设计已成为智能建造的关键支撑技术。然而，尽管大语言模型在BIM设计辅助方面展现出应用潜力，但领域特定数据集的缺失和评估基准的缺乏严重制约了模型性能优化与能力评估。

Method: 本研究提出三方面的解决方案：1）构建面向BIM设计的评估基准及量化指标体系，用于全面评估大语言模型能力；2）开发从BIM模型自动生成文本数据的方法，并构建高质量领域数据集；3）设计针对BIM设计任务的模型微调策略，实现领域自适应。

Result: 实验结果表明，所提出的领域特定评估基准能够有效且全面地评估大语言模型能力，证实通用大模型在BIM设计任务上仍存在明显不足。基于该基准和数据集开发的Qwen-BIM模型相比基础模型G-Eval评分平均提升21.0%，且仅用140亿参数即可达到6710亿参数通用大模型的性能水平。

Conclusion: 本研究通过构建首个BIM设计领域专用评估基准和高质量数据集，成功开发出领域定制化大语言模型，为后续BIM相关大语言模型的研究与应用提供了重要基础与参考。

Abstract: As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.

</details>


### [107] [Pressure Reveals Character: Behavioural Alignment Evaluation at Depth](https://arxiv.org/abs/2602.20813)
*Nora Petrova,John Burden*

Main category: cs.AI

TL;DR: 本研究针对语言模型对齐评估缺乏真实多轮场景的问题，推出了包含904个场景、涵盖诚实、安全、非操纵、鲁棒性、可修正性和诡计六大类别的对齐基准。通过设计冲突指令、模拟工具访问和多轮升级场景，评估了24个前沿模型，发现即使顶级模型也存在特定类别缺陷，且多数模型整体表现薄弱。因子分析显示对齐能力呈现统一结构（类似认知研究中的g因子）。研究团队公开了基准和交互式排行榜。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型对齐评估主要依赖单轮测试，无法真实反映模型在复杂压力下的行为表现。随着对齐失败造成实际危害日益增多，缺乏包含真实多轮场景的综合评估框架已成为制约模型安全发展的重要瓶颈。

Method: 研究团队构建了涵盖六大类别的904个对齐评估场景，通过人类评估者验证其真实性。场景设计采用冲突指令、模拟工具访问和多轮升级机制，以揭示单轮评估无法捕捉的行为倾向。使用LLM评估器对24个前沿模型进行测试，评估结果与人类标注进行验证对比，并进行了因子分析。

Result: 评估发现：1）即使表现最佳的模型在特定类别中也存在明显缺陷；2）大多数模型在整体上表现出一致的弱点；3）因子分析表明对齐能力是一个统一结构，在某一类别得分高的模型倾向于在其他类别也得分高，类似认知研究中的g因子现象。

Conclusion: 该基准的发布为语言模型对齐评估提供了重要工具，揭示了当前模型的对齐缺陷具有普遍性和相关性。研究团队计划持续扩展在薄弱环节的场景数量，并随着新模型发布不断更新评估，以推动模型对齐研究的发展。

Abstract: Evaluating alignment in language models requires testing how they behave under realistic pressure, not just what they claim they would do. While alignment failures increasingly cause real-world harm, comprehensive evaluation frameworks with realistic multi-turn scenarios remain lacking. We introduce an alignment benchmark spanning 904 scenarios across six categories -- Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming -- validated as realistic by human raters. Our scenarios place models under conflicting instructions, simulated tool access, and multi-turn escalation to reveal behavioural tendencies that single-turn evaluations miss. Evaluating 24 frontier models using LLM judges validated against human annotations, we find that even top-performing models exhibit gaps in specific categories, while the majority of models show consistent weaknesses across the board. Factor analysis reveals that alignment behaves as a unified construct (analogous to the g-factor in cognitive research) with models scoring high on one category tending to score high on others. We publicly release the benchmark and an interactive leaderboard to support ongoing evaluation, with plans to expand scenarios in areas where we observe persistent weaknesses and to add new models as they are released.

</details>


### [108] [Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs](https://arxiv.org/abs/2602.20878)
*Dhita Putri Pratama,Soyeon Caren Han,Yihao Ding*

Main category: cs.AI

TL;DR: 论文针对大型视觉语言模型依赖虚假相关而非因果推理的问题，提出视觉语言因果图（VLCGs）和ViLCaR诊断基准，包含因果归因、推断与问答任务及图对齐评估指标。实验表明，注入结构化信息可显著提升模型因果推理一致性，揭示当前限制主要源于结构性指导不足而非推理能力缺陷。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）虽在视觉问答基准测试中表现优异，但依赖虚假相关性而非真正的因果推理。现有评估仅关注答案正确性，无法诊断失败根源，限制了模型因果推理能力的提升。

Method: 提出视觉语言因果图（VLCGs）结构化表示，编码因果相关对象、属性、关系与假设；构建ViLCaR诊断基准，含因果归因、因果推断与问答任务，并设计超越答案准确性的图对齐评估指标。通过实验对比零样本、标准上下文学习与注入结构化信息的效果。

Result: 实验显示，注入结构化因果信息显著优于零样本和标准上下文学习方法，能大幅提升先进LVLMs在因果归因和推理任务上的一致性。

Conclusion: 研究表明，LVLMs因果推理的局限性主要源于结构性指导不足，而非推理能力本身欠缺，为未来改进提供了方向。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.

</details>


### [109] [Predicting Sentence Acceptability Judgments in Multimodal Contexts](https://arxiv.org/abs/2602.20918)
*Hyewon Jang,Nikolai Ilinykh,Sharid Loáiciga,Jey Han Lau,Shalom Lappin*

Main category: cs.AI

TL;DR: 本研究考察视觉上下文对人和大语言模型句子可接受性判断的影响。结果表明：视觉图像对人类评分几乎无影响，但大语言模型表现出压缩效应；模型预测人类判断准确率高，但移除视觉上下文时略优；Qwen判断分布最接近人类；模型预测与归一化对数概率高度相关，但视觉上下文会降低这种相关性。


<details>
  <summary>Details</summary>
Motivation: 探究视觉上下文(图像)对人和大语言模型句子可接受性判断的影响，并与先前研究的文本上下文效果对比，揭示人类与LM在多模态语境处理中的异同。

Method: 通过控制实验，比较人类和多种大语言模型在有无视觉上下文条件下的句子可接受性判断数据，分析模型预测与人类评分的相关性及其与模型内部表示(归一化对数概率)的关系。

Result: 1) 视觉上下文对人类可接受性评分无显著影响，与文本上下文形成对比；2) 大语言模型展现出压缩效应；3) 模型预测人类判断准确率高，但移除视觉上下文时性能更佳；4) Qwen的判断分布最接近人类模式；5) 模型预测通常与归一化对数概率高度相关，但视觉上下文会削弱这种相关性，显示内部表示与预测间的差距。

Conclusion: 实验揭示了人类与LM在多模态语境处理中的相似性与差异性，表明LM虽能较好预测人类判断，但其内部机制与人类存在差异，尤其在视觉上下文存在时。

Abstract: Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.

</details>


### [110] [HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG](https://arxiv.org/abs/2602.20926)
*Yuqi Huang,Ning Liao,Kai Yang,Anning Hu,Shengchao Hu,Xiaoxing Wang,Junchi Yan*

Main category: cs.AI

TL;DR: 本文提出HELP框架，通过超节点扩展和逻辑路径引导的证据定位策略，解决图RAG在准确性和效率之间的权衡问题，在保持竞争力的同时实现高达28.8倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在知识边界和幻觉问题，检索增强生成（RAG）虽有所缓解但忽视了多跳推理所需的结构化依赖关系。基于图的RAG方法试图弥补这一不足，但由于昂贵的图遍历以及大型语言模型生成摘要带来的语义噪声，通常面临准确性与效率之间的权衡。

Method: 提出HELP框架，包含两个核心策略：1）超节点扩展：将知识三元组迭代链接为抽象为超节点的连贯推理路径，以捕捉复杂结构依赖并确保检索准确性；2）逻辑路径引导的证据定位：利用预计算的图-文本关联将这些路径直接映射到语料库，实现卓越效率。

Result: 在多个简单和多跳问答基准测试中表现具有竞争力，相比领先的基于图的RAG基线方法速度提升高达28.8倍。

Conclusion: HELP框架通过创新的图结构抽象和路径引导机制，有效避免了昂贵的随机游走和语义失真，在保持高准确性的同时显著提升检索效率，为知识密集型任务提供了实用且可靠的解决方案。

Abstract: Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\times$ speedup over leading Graph-based RAG baselines.

</details>


### [111] [Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence](https://arxiv.org/abs/2602.20934)
*ChengYou Li,XiaoDong Liu,XiangBao Meng,XinYu Zhao*

Main category: cs.AI

TL;DR: 本文提出AgentOS概念框架，将大语言模型重新定义为受操作系统逻辑管理的"推理内核"，通过深度上下文管理、语义分片和时序对齐机制，解决微观token处理与宏观系统智能之间的理论鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前研究过于聚焦上下文窗口扩展和提示工程优化，缺乏连接微观token级处理与宏观系统级智能的理论桥梁，导致从静态推理引擎向动态自主认知系统的范式转换存在严重碎片化问题。

Method: 提出AgentOS框架，将操作系统核心抽象（内存分页、中断处理、进程调度）映射到LLM原生结构，引入可寻址语义空间替代被动缓冲区，通过语义分片和时序对齐机制实现从离散序列到连贯认知状态的转换并缓解多智能体编排中的认知漂移。

Result: 系统性地解构了认知状态转换过程，为构建弹性、可扩展、自演化的认知环境提供了严谨的架构路线图。

Conclusion: 断言AGI发展的下一个前沿在于系统级协调的架构效率，大语言模型的未来突破将取决于操作系统设计原则在认知架构中的应用深度。

Abstract: The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a "Reasoning Kernel" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.

</details>


### [112] [LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification](https://arxiv.org/abs/2602.21044)
*Yanrui Wu,Lingling Zhang,Xinyu Zhang,Jiayu Chang,Pengyu Li,Xu Jiang,Jingtao Hu,Jun Liu*

Main category: cs.AI

TL;DR: 该论文针对大语言模型评估仅关注收敛性逻辑推理（单一正确证明）的局限，提出首个多路径逻辑推理基准LogicGraph。通过神经符号框架生成高深度多路径推理问题，发现现有模型倾向于过早承诺单一推理路径，且该缺陷随推理深度显著加剧，为模型改进提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估主要强调收敛性逻辑推理，即产生单一正确证明即可成功。然而许多现实世界推理问题允许多种有效推导路径，需要模型探索多样化逻辑路线而非固守单一方案。现有评估体系无法充分衡量模型在发散性推理场景下的真实能力，这限制了对模型推理能力的全面理解。

Method: 1. 构建LogicGraph基准：首次系统评估多路径逻辑推理能力；2. 采用神经符号框架，结合反向逻辑生成与语义实例化技术；3. 生成求解器验证的高深度多路径推理问题，包含内在逻辑干扰项；4. 为每个实例关联详尽的最小证明集合；5. 设计无参考评估框架，严格评估模型在收敛性和发散性两种范式下的表现。

Result: 在先进语言模型上的实验揭示共同局限：模型倾向于过早承诺单一推理路径，无法充分探索替代方案。这种覆盖差距随推理深度增加而显著扩大。LogicGraph暴露了这种发散性差距，为未来改进提供了可操作的见解。

Conclusion: LogicGraph基准测试填补了多路径逻辑推理评估的空白，揭示了当前大语言模型在发散性推理方面的严重缺陷。研究强调了探索多样化推理路径的重要性，并为开发更稳健的推理模型指明了方向。代码和数据将开源以促进后续研究。

Abstract: Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.

</details>


### [113] [Tool Building as a Path to "Superintelligence"](https://arxiv.org/abs/2602.21061)
*David Koplow,Tomer Galanti,Tomaso Poggio*

Main category: cs.AI

TL;DR: 本研究通过构建GF(2)电路重建的逻辑分布外推理基准，量化测量大语言模型在"勤奋学习者"框架下的步骤成功概率γ。实验表明小模型γ值随推理深度超线性衰减，前沿模型展现部分鲁棒性，且精确工具调用是实现规模化推理的关键。


<details>
  <summary>Details</summary>
Motivation: 检验"勤奋学习者"框架的核心前提——大语言模型能否通过测试时搜索实现超级智能，关键在于其在复杂逻辑推理任务上是否维持足够的步骤成功概率γ。

Method: 设计一类随推理步骤递增难度的GF(2)电路重建任务，这些任务从信息论角度除非模型完整整合所有信息否则无法可靠求解，以此作为测量γ的基准测试。

Result: 小参数量模型的γ值随推理深度呈超线性下降趋势；前沿大模型在该任务上表现出部分鲁棒性；成功的规模化推理高度依赖于精确的工具调用机制。

Conclusion: 工具设计能力是大语言模型通过"勤奋学习者"框架迈向通用超级智能的关键瓶颈。

Abstract: The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the information provided. Our analysis demonstrates that while the $γ$ value for small LLMs declines superlinearly as depth increases, frontier models exhibit partial robustness on this task. Furthermore, we find that successful reasoning at scale is contingent upon precise tool calls, identifying tool design as a critical capability for LLMs to achieve general superintelligence through the Diligent Learner framework.

</details>


### [114] [The Initial Exploration Problem in Knowledge Graph Exploration](https://arxiv.org/abs/2602.21066)
*Claire McNamara,Lucy Hederman,Declan O'Sullivan*

Main category: cs.AI

TL;DR: 本文理论化知识图谱的"初始探索问题"(IEP)，识别范围不确定性、本体不透明性和查询无能三大障碍，揭示现有系统在首次接触时缺乏作用域揭示交互原语的结构性设计空白。


<details>
  <summary>Details</summary>
Motivation: 知识图谱的跨领域信息整合能力虽强，但其语义丰富性和结构复杂性对非专业用户构成显著障碍。用户在首次接触未知KG时面临方向迷失：不知可能提问、知识结构为何或如何入手。

Method: 基于信息行为与人机交互理论（ASK模型、探索性搜索、信息觅食理论、认知负荷理论），构建IEP概念框架，并从交互原语层面批判性分析KG探索界面。

Result: 理论化IEP为首次接触时的三大互依障碍——范围不确定性、本体不透明性和查询无能；揭示现有系统依赖"已存在起点或信息目标"的认知假设，导致作用域揭示机制缺失。

Conclusion: 提供评估KG界面的理论透镜，指导设计入口点脚手架，支持用户初始探索，填补首次接触时的交互设计空白。

Abstract: Knowledge Graphs (KGs) enable the integration and representation of complex information across domains, but their semantic richness and structural complexity create substantial barriers for lay users without expertise in semantic web technologies. When encountering an unfamiliar KG, such users face a distinct orientation challenge: they do not know what questions are possible, how the knowledge is structured, or how to begin exploration. This paper identifies and theorises this phenomenon as the Initial Exploration Problem (IEP). Drawing on theories from information behaviour and human-computer interaction, including ASK, exploratory search, information foraging, and cognitive load theory, we develop a conceptual framing of the IEP characterised by three interdependent barriers: scope uncertainty, ontology opacity, and query incapacity. We argue that these barriers converge at the moment of first contact, distinguishing the IEP from related concepts that presuppose an existing starting point or information goal. Analysing KG exploration interfaces at the level of interaction primitives, we suggest that many systems rely on epistemic assumptions that do not hold at first contact. This reveals a structural gap in the design space: the absence of interaction primitives for scope revelation, mechanisms that communicate what a KG contains without requiring users to formulate queries or interpret ontological structures. In articulating the IEP, this paper provides a theoretical lens for evaluating KG interfaces and for designing entry-point scaffolding that supports initial exploration.

</details>


### [115] [CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning](https://arxiv.org/abs/2602.21154)
*Ziwei Niu,Hao Sun,Shujun Bian,Xihong Yang,Lanfen Lin,Yuxin Liu,Yueming Jin*

Main category: cs.AI

TL;DR: 该论文提出CG-DMER框架，通过时空掩码建模和表征解耦对齐策略，解决多模态ECG解读中导联间时空依赖缺失和临床报告自由文本引入的模态偏差问题，在三个公开数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有ECG-临床报告多模态方法存在两方面问题：(1)模态内：模型忽略导联间时空依赖，无法建模细粒度诊断模式；(2)模态间：直接对齐自由文本报告会引入模态特异性偏差，影响泛化能力。

Method: 提出CG-DMER对比生成框架，包含两个核心设计：1)时空掩码建模，通过在时空维度掩码并重建缺失信息来捕获细粒度时序动态和导联间空间依赖；2)表征解耦对齐策略，引入模态特定与模态共享编码器，分离模态不变与模态特异表征以减轻噪声和偏差。

Result: 在三个公开数据集上的实验表明，CG-DMER在多样化的下游任务中均达到当前最优性能。

Conclusion: 通过显式建模导联时空依赖和解耦模态表征，CG-DMER有效提升了多模态ECG表征学习的质量，为心血管疾病的精准诊断提供了更可靠的工具。

Abstract: Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.

</details>


### [116] [NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning](https://arxiv.org/abs/2602.21172)
*Ishaan Rawal,Shubh Gupta,Yihan Hu,Wei Zhan*

Main category: cs.AI

TL;DR: 本文提出\modelname（No Reasoning for Driving），一种无需推理的自动驾驶视觉-语言-动作模型。该模型通过引入Dr. GRPO算法缓解难度偏置问题，在仅使用不足60%训练数据且完全摒弃推理标注的情况下，实现了与现有方法相当的性能，训练token减少3倍。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言-动作模型能统一自动驾驶的模块化流水线，但其依赖大规模数据采集和密集推理标注，导致极高的经济和计算成本。现有方法难以摆脱这一瓶颈，限制了自动驾驶技术的规模化部署。

Method: 1) 构建不含推理标注的小型训练数据集；2) 揭示标准GRPO算法在此类数据上失效的根源在于难度偏置——该偏置会不成比例地惩罚来自高方差场景的奖励信号；3) 集成Dr. GRPO算法，通过调整rollout权重来缓解难度偏置，从而在有限数据下有效优化策略。

Result: 在Waymo和NAVSIM基准测试中，\modelname仅使用60%以下的训练数据且无需任何推理标注，性能达到竞争水平，训练token消耗降低3倍，显著提升了自动驾驶系统的训练效率。

Conclusion: 本研究证明，通过精心设计优化算法以适配数据特性，可以在大幅减少数据依赖和推理开销的同时维持高性能。\modelname为开发更经济、高效的自动驾驶系统提供了可行路径，强调了算法创新与数据效率并重的重要性。

Abstract: Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \modelname (\textbf{No} \textbf{R}easoning for \textbf{D}riving). Compared to existing VLAs, \modelname achieves competitive performance while being fine-tuned on $<$60\% of the data and no reasoning annotations, resulting in 3$\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.

</details>


### [117] [Aletheia tackles FirstProof autonomously](https://arxiv.org/abs/2602.21201)
*Tony Feng,Junehyuk Jung,Sang-hyun Kim,Carlo Pagano,Sergei Gukov,Chiang-Chiang Tsai,David Woodruff,Adel Javanmard,Aryan Mokhtari,Dawsen Hwang,Yuri Chervonyi,Jonathan N. Lee,Garrett Bingham,Trieu H. Trinh,Vahab Mirrokni,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 本文报道了由Gemini 3 Deep Think驱动的数学研究智能体Aletheia在首届FirstProof挑战赛中的表现。在限定时间内，Aletheia自主解决了10道题目中的6道（第2、5、7、8、9、10题），专家仅对第8题的解答存在分歧。论文详细阐述了研究背景、实验方法和评估过程，并公开了原始提示和输出以确保透明度。


<details>
  <summary>Details</summary>
Motivation: 数学研究智能体的开发旨在验证AI系统在复杂数学推理和证明任务中的能力。FirstProof挑战赛为评估这类智能体提供了标准化平台，本研究希望通过参与该挑战，检验Aletheia在真实数学研究场景中的表现，并推动数学AI领域的发展。

Method: 研究采用Gemini 3 Deep Think作为核心引擎，构建了名为Aletheia的数学研究智能体。通过设计特定的提示工程策略，让智能体自主处理FirstProof挑战赛中的数学问题。实验过程中记录了完整的交互日志，并邀请多位专家对解答结果进行独立评估，最终采用多数专家共识作为评判标准，仅第8题存在评估分歧。

Result: 在首届FirstProof挑战赛中，Aletheia在限定时间内成功解决了10道题目中的6道（第2、5、7、8、9、10题），解决率为60%。专家评估显示，除第8题外，其余5道题的解答均获得一致认可。该结果展示了AI系统在数学推理领域的显著进步，同时也揭示了当前技术在处理某些复杂数学问题时的局限性。

Conclusion: 本研究证明，基于Gemini 3 Deep Think的Aletheia智能体已具备解决真实数学挑战问题的能力，在FirstProof挑战中达到了令人瞩目的水平。论文强调透明度和可重复性的重要性，公开了所有原始数据和代码。未来工作将致力于提升智能体在更复杂数学问题上的表现，并进一步优化评估机制以达成专家共识。

Abstract: We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [118] [Indaleko: The Unified Personal Index](https://arxiv.org/abs/2602.20507)
*William Anthony Mason*

Main category: cs.IR

TL;DR: 该论文提出统一个人索引(UPI)架构及Indaleko原型，通过将时间、空间和活动元数据集成到图数据库中，使个人信息检索符合人类记忆模式，支持"去年春天会议场地附近的照片"等自然语言查询，在3100万文件数据集上实现亚秒级响应和完美精度，而商业系统均失败。


<details>
  <summary>Details</summary>
Motivation: 现有个人信息检索系统因忽视人类记忆机制而失效——人类通过情景线索（何时、何地、何种情境）自然回忆，但现有平台强制用户在孤立的系统中进行关键词搜索，造成根本性脱节。

Method: 提出记忆对齐的统一个人索引(UPI)架构，并实现Indaleko原型。该原型在8个存储平台、160TB容量的3100万文件数据集上验证可行性，通过将时空和活动元数据整合到统一图数据库，采用记忆锚点索引实现亚秒级查询，支持多维记忆线索的自然语言查询。

Result: Indaleko实现亚秒级查询响应，对明确记忆模式保持完美精度，消除跨平台搜索碎片化。评估显示Google Drive、OneDrive、Dropbox、Windows Search等商业系统在基于记忆的查询上全部失败，返回海量无上下文过滤的结果。Indaleko成功处理时间、地点和活动模式组合的多维查询，其可扩展架构能在10分钟至10小时内快速集成新数据源，并通过基于UUID的语义解耦保护隐私。

Conclusion: UPI架构实现了认知理论与分布式系统设计的融合，将个人信息检索从关键词匹配转变为记忆对齐的发现，既为现有数据提供即时效益，也为未来情境感知系统奠定基础。

Abstract: Personal information retrieval fails when systems ignore how human memory works. While existing platforms force keyword searches across isolated silos, humans naturally recall through episodic cues like when, where, and in what context information was encountered. This dissertation presents the Unified Personal Index (UPI), a memory-aligned architecture that bridges this fundamental gap. The Indaleko prototype demonstrates the UPI's feasibility on a 31-million file dataset spanning 160TB across eight storage platforms. By integrating temporal, spatial, and activity metadata into a unified graph database, Indaleko enables natural language queries like "photos near the conference venue last spring" that existing systems cannot process. The implementation achieves sub-second query responses through memory anchor indexing, eliminates cross-platform search fragmentation, and maintains perfect precision for well-specified memory patterns. Evaluation against commercial systems (Google Drive, OneDrive, Dropbox, Windows Search) reveals that all fail on memory-based queries, returning overwhelming result sets without contextual filtering. In contrast, Indaleko successfully processes multi-dimensional queries combining time, location, and activity patterns. The extensible architecture supports rapid integration of new data sources (10 minutes to 10 hours per provider) while preserving privacy through UUID-based semantic decoupling. The UPI's architectural synthesis bridges cognitive theory with distributed systems design, as demonstrated through the Indaleko prototype and rigorous evaluation. This work transforms personal information retrieval from keyword matching to memory-aligned finding, providing immediate benefits for existing data while establishing foundations for future context-aware systems.

</details>


### [119] [PRECTR-V2:Unified Relevance-CTR Framework with Cross-User Preference Mining, Exposure Bias Correction, and LLM-Distilled Encoder Optimization](https://arxiv.org/abs/2602.20676)
*Shuzhi Cao,Rong Chen,Ailong He,Shuguang Han,Jufeng Chen*

Main category: cs.IR

TL;DR: 本文提出PRECTR-V2框架，针对搜索系统中相关性匹配与CTR预测的协调问题，通过挖掘查询相关的全局偏好解决冷启动用户个性化建模难题，利用嵌入噪声和标签重构构建困难负样本以纠正曝光偏差，并采用知识蒸馏训练轻量级Transformer编码器替代冻结BERT，实现表示学习与CTR微调的联合优化。


<details>
  <summary>Details</summary>
Motivation: 现有搜索系统需平衡相关性匹配与CTR预测，但面临三重挑战：低活跃/新用户行为稀疏导致个性化建模困难；训练数据主要来自高相关性曝光，与粗排候选空间分布不匹配产生泛化偏差；延迟约束下冻结BERT的Emb+MLP架构阻碍联合优化，导致表示学习与CTR微调不对齐。这些问题严重制约搜索效果和用户体验。

Method: PRECTR-V2采用三项关键技术：1) 查询感知的全局相关性偏好挖掘，通过跨用户行为聚合缓解冷启动用户稀疏数据问题；2) 嵌入空间噪声注入与相关性标签重构生成高质量困难负样本，结合成对损失函数优化排序；3) 基于LLM知识蒸馏和文本相关性分类任务微调，训练轻量级Transformer编码器，突破传统冻结BERT架构限制，实现端到端联合优化。

Result: 该摘要主要阐述方法论创新，未包含具体实验结果。从技术贡献来看，PRECTR-V2理论上可提升低活跃用户建模效果、缓解曝光偏差并优化架构效率，为搜索系统多目标优化提供了新范式，但实际效果需通过后续实验验证。

Conclusion: 本研究通过系统性改进PRECTR，提出PRECTR-V2框架，在冷启动场景建模、曝光偏差纠正和轻量级编码器设计三方面实现突破，有效解决了相关性匹配与CTR预测的协调难题，超越了传统Emb+MLP架构，为构建高效搜索系统提供了新思路。

Abstract: In search systems, effectively coordinating the two core objectives of search relevance matching and click-through rate (CTR) prediction is crucial for discovering users' interests and enhancing platform revenue. In our prior work PRECTR, we proposed a unified framework to integrate these two subtasks,thereby eliminating their inconsistency and leading to mutual benefit.However, our previous work still faces three main challenges. First, low-active users and new users have limited search behavioral data, making it difficult to achieve effective personalized relevance preference modeling. Second, training data for ranking models predominantly come from high-relevance exposures, creating a distribution mismatch with the broader candidate space in coarse-ranking, leading to generalization bias. Third, due to the latency constraint, the original model employs an Emb+MLP architecture with a frozen BERT encoder, which prevents joint optimization and creates misalignment between representation learning and CTR fine-tuning. To solve these issues, we further reinforce our method and propose PRECTR-V2. Specifically, we mitigate the low-activity users' sparse behavior problem by mining global relevance preferences under the specific query, which facilitates effective personalized relevance modeling for cold-start scenarios. Subsequently, we construct hard negative samples through embedding noise injection and relevance label reconstruction, and optimize their relative ranking against positive samples via pairwise loss, thereby correcting exposure bias. Finally, we pretrain a lightweight transformer-based encoder via knowledge distillation from LLM and SFT on the text relevance classification task. This encoder replaces the frozen BERT module, enabling better adaptation to CTR fine-tuning and advancing beyond the traditional Emb+MLP paradigm.

</details>


### [120] [IntRR: A Framework for Integrating SID Redistribution and Length Reduction](https://arxiv.org/abs/2602.20704)
*Zesheng Wang,Longfei Xu,Weidong Deng,Huimin Yan,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: 本文针对生成式推荐系统面临的语义ID索引目标与推荐目标错位、静态ID缺乏动态适应性、层次化ID展平导致序列长度膨胀三大问题，提出IntRR框架。该框架以物品唯一ID为协同锚点实现目标对齐的语义权重动态重分配，并通过递归处理层次结构消除展平操作，实现每物品固定单token开销，最终在推荐准确性和计算效率上均超越代表性基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前生成式推荐虽通过离散语义ID将级联排序重构为序列生成任务，但存在根本性缺陷：1）两阶段目标错位，阶段一的索引优化与阶段二的推荐目标不一致；2）静态ID机制缺乏灵活性，无法适应聘用户交互的演化复杂性；3）层次化SID展平策略导致序列长度爆炸性增长，引发计算开销与推理延迟，严重制约系统实用性。

Method: IntRR框架创新性地整合两大机制：1）目标对齐的SID重分配——利用物品特定唯一ID作为跨层次协同锚点，动态调整语义权重分布，使ID表征与推荐目标对齐；2）结构化长度缩减——递归处理SID层次结构，避免传统展平操作，确保每物品仅需生成一个token的固定计算成本。

Result: 在多个基准数据集上的实验表明，IntRR相较代表性生成式推荐基线在准确性指标上实现显著提升，同时在计算效率方面降低推理延迟，并维持每物品单token的稳定开销，验证了框架的有效性。

Conclusion: IntRR通过动态语义ID重分配与递归层次化处理，成功解决了生成式推荐中的目标不一致性与计算可扩展性难题，为构建高性能生成式推荐系统提供了新思路，证明了ID动态优化与结构保持策略的重要价值。

Abstract: Generative Recommendation (GR) has emerged as a transformative paradigm that reformulates the traditional cascade ranking system into a sequence-to-item generation task, facilitated by the use of discrete Semantic IDs (SIDs). However, current SIDs are suboptimal as the indexing objectives (Stage 1) are misaligned with the actual recommendation goals (Stage 2). Since these identifiers remain static (Stage 2), the backbone model lacks the flexibility to adapt them to the evolving complexities of user interactions. Furthermore, the prevailing strategy of flattening hierarchical SIDs into token sequences leads to sequence length inflation, resulting in prohibitive computational overhead and inference latency. To address these challenges, we propose IntRR, a novel framework that integrates objective-aligned SID Redistribution and structural Length Reduction. By leveraging item-specific Unique IDs (UIDs) as collaborative anchors, this approach dynamically redistributes semantic weights across hierarchical codebook layers. Concurrently, IntRR handles the SID hierarchy recursively, eliminating the need to flatten sequences. This ensures a fixed cost of one token per item. Extensive experiments on benchmark datasets demonstrate that IntRR yields substantial improvements over representative generative baselines, achieving superior performance in both recommendation accuracy and efficiency.

</details>


### [121] [RMIT-ADM+S at the MMU-RAG NeurIPS 2025 Competition](https://arxiv.org/abs/2602.20735)
*Kun Ran,Marwah Alaofi,Danula Hettiachchi,Chenglong Ma,Khoi Nguyen Dinh Anh,Khoi Vo Nguyen,Sachin Pathiyan Cherumanal,Lida Rashidi,Falk Scholer,Damiano Spina,Shuoqi Sun,Oleg Zendel*

Main category: cs.IR

TL;DR: 本文介绍了RMIT-ADM+S团队在NeurIPS 2025 MMU-RAG竞赛文本生成赛道中的获奖系统。该系统提出了Routing-to-RAG (R2RAG)架构，一种轻量级、可根据查询复杂度和证据充足性动态调整检索策略的检索增强生成系统。该系统基于小型语言模型，可在单块消费级GPU上运行，并成功获得开源类别最佳动态评估奖。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是开发一种能够在消费级硬件上运行的高效检索增强生成系统，以降低复杂研究任务的技术门槛和资源需求。通过利用轻量级组件和小型语言模型，使强大的RAG能力更加普及和易于访问，同时基于对先前G-RAG系统的定性分析进行改进和扩展。

Method: 本文提出了Routing-to-RAG (R2RAG)架构，采用轻量级组件动态推断查询复杂度与证据充足性，从而自适应调整检索策略。系统基于小型语言模型，可在单块消费级GPU上部署，并扩展了获奖的G-RAG系统，加入了通过定性输出分析设计的优化模块。

Result: 在NeurIPS 2025 MMU-RAG竞赛文本生成赛道中，该系统获得开源类别最佳动态评估奖，验证了其在资源受限环境下处理复杂研究任务的高效能和资源利用效率。

Conclusion: 研究表明，通过精心设计和优化，轻量级RAG架构能够在保持高效性的同时显著降低硬件需求，为资源敏感场景下的复杂信息处理任务提供了可行解决方案。

Abstract: This paper presents the award-winning RMIT-ADM+S system for the Text-to-Text
  track of the NeurIPS~2025 MMU-RAG Competition. We introduce Routing-to-RAG
  (R2RAG), a research-focused retrieval-augmented generation (RAG)
  architecture composed of lightweight components that dynamically adapt the
  retrieval strategy based on inferred query complexity and evidence
  sufficiency. The system uses smaller LLMs, enabling operation on a single
  consumer-grade GPU while supporting complex research tasks. It builds on the
  G-RAG system, winner of the ACM~SIGIR~2025 LiveRAG Challenge, and extends it
  with modules informed by qualitative review of outputs. R2RAG won the Best
  Dynamic Evaluation award in the Open Source category, demonstrating high
  effectiveness with careful design and efficient use of resources.

</details>


### [122] [Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking](https://arxiv.org/abs/2602.20800)
*Dalia Nahhas,Xiaohao Cai,Imran Razzak,Shoaib Jameel*

Main category: cs.IR

TL;DR: 针对生成式信息检索中文化相关性评估的循环性与偏好泄露问题，本研究提出无泄露双评委框架，严格分离监督模型（评委B）与评估模型（评委A）。在33k文化故事基准测试中，从评委B监督的交叉编码器蒸馏出的稠密双编码器BGE-M3表现卓越，其性能在无泄露评估下反超教师模型，并在人类标注的Moral Stories数据集上验证了与人类规范的一致性。


<details>
  <summary>Details</summary>
Motivation: 生成式信息检索的瓶颈已从生成阶段转向候选选择阶段，尤其涉及文化相关性等规范性标准。现有LLM-as-a-Judge评估存在严重缺陷：监督与评估模型重叠导致循环论证和偏好泄露，使性能评估虚高，无法真实反映模型的文化判断能力，亟需建立可靠的评估范式。

Method: 1) 将文化相关性形式化为查询内部排序任务；2) 构建严格分离的双评委框架：评委B仅提供监督信号，评委A独立执行评估；3) 创建NGR-33k文化故事基准（33,052条）；4) 采用知识蒸馏技术，从评委B监督的交叉编码器训练高效稠密双编码器BGE-M3。

Result: 经典基线方法增益有限；在泄漏自由的评委A评估下，蒸馏得到的BGE-M3模型性能显著超越其教师模型（交叉编码器）；在Moral Stories数据集上与人类规范表现出强对齐，证实评估结果的可信度。

Conclusion: 严格的评估器分离是可信GenIR评估的必要前提。研究表明，细微的文化偏好可被有效蒸馏至高效排序器中而不产生泄露，为文化感知的生成式检索建立了可靠的评估框架与基线方法。

Abstract: In Generative Information Retrieval (GenIR), the bottleneck has shifted from generation to the selection of candidates, particularly for normative criteria such as cultural relevance. Current LLM-as-a-Judge evaluations often suffer from circularity and preference leakage, where overlapping supervision and evaluation models inflate performance. We address this by formalising cultural relevance as a within-query ranking task and introducing a leakage-free two-judge framework that strictly separates supervision (Judge B) from evaluation (Judge A). On a new benchmark of 33,052 (NGR-33k) culturally grounded stories, we find that while classical baselines yield only modest gains, a dense bi-encoder distilled from a Judge-B-supervised Cross-Encoder is highly effective. Although the Cross-Encoder provides a strong supervision signal for distillation, the distilled BGE-M3 model substantially outperforms it under leakage-free Judge~A evaluation. We validate our framework on the human-curated Moral Stories dataset, showing strong alignment with human norms. Our results demonstrate that rigorous evaluator separation is a prerequisite for credible GenIR evaluation, proving that subtle cultural preferences can be distilled into efficient rankers without leakage.

</details>


### [123] [Naver Labs Europe @ WSDM CUP | Multilingual Retrieval](https://arxiv.org/abs/2602.20986)
*Thibault Formal,Maxime Louis,Hervé Déjean,Stéphane Clinchant*

Main category: cs.IR

TL;DR: 太长不看版本：我们在WSDM Cup 2026多语言文档检索任务中评估了SPLARE模型，通过轻量级改进（Qwen3-Reranker-4B重排序和分数融合）展示了其在跨语言检索上的竞争力，超越了Qwen3-8B-Embed等稠密基线。


<details>
  <summary>Details</summary>
Motivation: 动机：多语言检索需要模型具备跨语言泛化能力，而传统的稠密检索模型在跨语言场景下可能存在局限。SPLARE作为新兴的学习型稀疏检索模型，能够生成可泛化的稀疏潜在表示，特别适合多语言检索设置，但其在真实跨语言基准上的表现尚未得到充分验证。

Method: 方法：研究团队参加了WSDM Cup 2026多语言文档检索共享任务，评估了五种逐步增强的SPLARE模型运行。基础模型为SPLARE-7B，然后逐步引入轻量级改进：使用Qwen3-Reranker-4B进行重排序，并采用简单的分数融合策略来提升检索性能。

Result: 结果：实验结果表明，SPLARE模型在多语言检索任务上相比Qwen3-8B-Embed等最先进的稠密基线表现出显著优势，验证了学习型稀疏检索模型的有效性。

Conclusion: 结论：该提交证明了学习型稀疏检索模型在非英语中心场景下依然具有持续的相关性和竞争力，为多语言信息检索提供了新的技术路径和实证依据。

Abstract: This report presents our participation to the WSDM Cup 2026 shared task on multilingual document retrieval from English queries. The task provides a challenging benchmark for cross-lingual generalization. It also provides a natural testbed for evaluating SPLARE, our recently proposed learned sparse retrieval model, which produces generalizable sparse latent representations and is particularly well suited to multilingual retrieval settings.
  We evaluate five progressively enhanced runs, starting from a SPLARE-7B model and incorporating lightweight improvements, including reranking with Qwen3-Reranker-4B and simple score fusion strategies. Our results demonstrate the strength of SPLARE compared to state-of-the-art dense baselines such as Qwen3-8B-Embed. More broadly, our submission highlights the continued relevance and competitiveness of learned sparse retrieval models beyond English-centric scenarios.

</details>


### [124] [Generative Pseudo-Labeling for Pre-Ranking with LLMs](https://arxiv.org/abs/2602.20995)
*Junyu Bi,Xinting Niu,Daixuan Cheng,Kun Yuan,Tao Wang,Binbin Cao,Jian Wu,Yuning Jiang*

Main category: cs.IR

TL;DR: 本文针对工业推荐系统预排序阶段的训练-服务不一致问题，提出Generative Pseudo-Labeling (GPL)框架，利用大语言模型为未曝光物品生成无偏伪标签，使点击率提升3.07%并显著改善长尾内容推荐。


<details>
  <summary>Details</summary>
Motivation: 预排序模型仅基于曝光交互数据训练，但在线服务时需对所有召回候选（包括未曝光物品）打分，导致严重样本选择偏差和泛化能力下降，尤其影响长尾内容。现有去偏方法存在误标未曝光物品为负样本或传播曝光偏差的问题。

Method: 提出GPL框架，通过大语言模型离线生成用户特定的兴趣锚点，在冻结的语义空间中与候选物品匹配，为未曝光物品生成无偏、内容感知的伪标签，实现训练分布与在线服务空间的对齐，且不增加在线延迟。

Result: 在大型生产系统中部署后，点击率（CTR）提升3.07%，推荐多样性和长尾物品发现能力得到显著增强。

Conclusion: GPL创新性地利用LLM生成高质量伪标签，有效解决了预排序中的样本选择偏差问题，提升了推荐系统的效果和多样性，为工业界提供了实用且高效的解决方案。

Abstract: Pre-ranking is a critical stage in industrial recommendation systems, tasked with efficiently scoring thousands of recalled items for downstream ranking. A key challenge is the train-serving discrepancy: pre-ranking models are trained only on exposed interactions, yet must score all recalled candidates -- including unexposed items -- during online serving. This mismatch not only induces severe sample selection bias but also degrades generalization, especially for long-tail content. Existing debiasing approaches typically rely on heuristics (e.g., negative sampling) or distillation from biased rankers, which either mislabel plausible unexposed items as negatives or propagate exposure bias into pseudo-labels. In this work, we propose Generative Pseudo-Labeling (GPL), a framework that leverages large language models (LLMs) to generate unbiased, content-aware pseudo-labels for unexposed items, explicitly aligning the training distribution with the online serving space. By offline generating user-specific interest anchors and matching them with candidates in a frozen semantic space, GPL provides high-quality supervision without adding online latency. Deployed in a large-scale production system, GPL improves click-through rate by 3.07%, while significantly enhancing recommendation diversity and long-tail item discovery.

</details>


### [125] [Position-Aware Sequential Attention for Accurate Next Item Recommendations](https://arxiv.org/abs/2602.21052)
*Timur Nabiev,Evgeny Frolov*

Main category: cs.IR

TL;DR: 该论文指出传统加法位置编码在自注意力机制中存在局限性，提出了核化位置注意力机制，通过在学习过程中直接调制注意力权重来解耦位置信息与语义信息，从而提升序列建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力模型依赖加法位置编码将位置信息注入输入，但这种方式使位置信息与项目嵌入语义纠缠，在深层架构中传播效果差，限制了捕获复杂序列模式的能力。由于缺乏位置信号，注意力块对序列位置具有置换等变性，无法内在地理解时间顺序。

Method: 提出一种核化自注意力机制，使用可学习的位置核对注意力权重进行直接调制。该核纯作用于位置空间，与语义相似度解耦，每个注意力块都可应用，实现自适应多尺度序列建模。

Result: 在标准下一项预测基准测试上的实验表明，该方法持续优于强大的竞争基线。

Conclusion: 核化位置注意力机制有效解决了加法位置编码的局限性，通过将位置信息处理与语义信息分离，能够更好地捕获丰富的序列模式，显著提升序列预测性能。

Abstract: Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled with item embedding semantics, propagates weakly in deep architectures, and limits the ability to capture rich sequential patterns. To address these limitations, we introduce a kernelized self-attention mechanism, where a learnable positional kernel operates purely in the position space, disentangled from semantic similarity, and directly modulates attention weights. When applied per attention block, this kernel enables adaptive multi-scale sequential modeling. Experiments on standard next-item prediction benchmarks show that our positional kernel attention consistently improves over strong competing baselines.

</details>


### [126] [Multi-Vector Index Compression in Any Modality](https://arxiv.org/abs/2602.21202)
*Hanxiang Qin,Alexander Martin,Rohan Jha,Chunsheng Zuo,Reno Kriz,Benjamin Van Durme*

Main category: cs.IR

TL;DR: 针对晚期交互检索中文档长度导致的计算存储成本问题，提出注意力引导聚类(AGC)等四种索引压缩方法，在跨模态检索任务上验证了AGC的优越性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 晚期交互检索已成为文本、图像、视觉文档和视频的主导范式，但其计算和存储成本随文档长度线性增长，对图像、视频和音频丰富的语料库成本过高，亟需探索查询无关的多向量文档表示压缩方法。

Method: 提出四种索引压缩方法：序列重采样、记忆令牌、层次池化和新型注意力引导聚类(AGC)。AGC利用注意力机制识别文档中最具语义显著性的区域作为聚类中心，并对令牌聚合进行加权。

Result: 在BEIR(文本)、ViDoRe(视觉文档)、MSR-VTT和MultiVENT 2.0(视频)等跨模态检索任务上评估，AGC始终优于其他参数化压缩方法，比非参数化层次聚类提供更灵活的索引大小控制，且性能与完整未压缩索引相当或更优。

Conclusion: AGC方法在保持或提升检索性能的同时，有效解决了晚期交互检索的扩展性问题，为多模态检索系统提供了实用的索引压缩解决方案。

Abstract: We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.

</details>
