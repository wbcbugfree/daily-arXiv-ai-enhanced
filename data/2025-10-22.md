<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 58]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 81]
- [cs.AI](#cs.AI) [Total: 26]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Outraged AI: Large language models prioritise emotion over cost in fairness enforcement](https://arxiv.org/abs/2510.17880)
*Hao Liu,Yiqing Dai,Haotian Tan,Yu Lei,Yujia Zhou,Zhen Wu*

Main category: cs.CL

TL;DR: LLMs show emotion-influenced moral punishment distinct from humans, with all-or-none enforcement and cost-insensitivity; reasoning models are closer to humans but still emotion-driven.


<details>
  <summary>Details</summary>
Motivation: Investigate whether large language models (LLMs) use emotion in moral decision-making and how their mechanisms compare to humans, focusing on altruistic third-party punishment.

Method: Large-scale study comparing 4,068 LLM agents and 1,159 adults across 796,100 decisions in altruistic punishment tasks. Assessed emotional responses, causality via prompts to report emotions, and compared foundation models (GPT-3.5, DeepSeek-V3) with reasoning models (o3-mini, DeepSeek-R1).

Result: LLMs exhibited emotion-guided punishment: negative emotion amplified punishment for unfairness; punishing elicited more positive emotion than accepting; prompting emotion reports causally increased punishment. LLMs prioritized emotion over cost, showing near all-or-none norm enforcement and reduced cost sensitivity, unlike humans who balance fairness and cost. Reasoning models were more cost-sensitive and closer to humans but still emotion-driven. This provides causal evidence of emotion-influenced moral decisions in LLMs and highlights deficits in cost calibration and nuanced fairness judgments. A developmental trajectory is proposed where future models should integrate emotion with context-sensitive reasoning to approach human-like emotional intelligence.

Conclusion: The study demonstrates causal emotion influence on LLM moral decisions and highlights gaps in cost calibration and nuanced fairness; advancing LLMs toward human-like emotional intelligence requires integrating emotion with adaptive reasoning.

Abstract: Emotions guide human decisions, but whether large language models (LLMs) use
emotion similarly remains unknown. We tested this using altruistic third-party
punishment, where an observer incurs a personal cost to enforce fairness, a
hallmark of human morality and often driven by negative emotion. In a
large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100
decisions, LLMs used emotion to guide punishment, sometimes even more strongly
than humans did: Unfairness elicited stronger negative emotion that led to more
punishment; punishing unfairness produced more positive emotion than accepting;
and critically, prompting self-reports of emotion causally increased
punishment. However, mechanisms diverged: LLMs prioritized emotion over cost,
enforcing norms in an almost all-or-none manner with reduced cost sensitivity,
whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini,
DeepSeek-R1) were more cost-sensitive and closer to human behavior than
foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven.
These findings provide the first causal evidence of emotion-guided moral
decisions in LLMs and reveal deficits in cost calibration and nuanced fairness
judgements, reminiscent of early-stage human responses. We propose that LLMs
progress along a trajectory paralleling human development; future models should
integrate emotion with context-sensitive reasoning to achieve human-like
emotional intelligence.

</details>


### [2] [KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers](https://arxiv.org/abs/2510.18355)
*Mohd Ruhul Ameen,Akif Islam,Farjana Aktar,M. Saifuzzaman Rafat*

Main category: cs.CL

TL;DR: 基于RAG的孟加拉语语音农业咨询系统KrishokBondhu，整合OCR、向量检索与Gemma 3-4B，提供电话接入的上下文绑定式专业农业建议，试点结果显示显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉国农民获取及时、专业农业咨询的困难，克服语言与可及性限制，并通过以 curated 文献为基础的 grounding 提升生成式回答的可靠性。

Method: 建立OCR与文档解析管线以数字化并结构化权威农业资料，构建向量数据库实现高效语义检索；通过电话界面实现语音问答，采用语音转文本将孟加拉语查询转写，RAG模块检索相关内容，Gemma 3-4B 进行上下文绑定的回答生成，文本转语音输出成自然孟加拉语语音。

Result: 在72.7%的多样化农业查询场景下给出高质量回答；综合评分4.53（满分5）高于KisanQRS的3.13，提升4.4/倍近似44.7%；在上下文丰富性与完整性方面分别提升了367%与100.4%，保持相关性与技术特异性水平。

Conclusion: 证明了将电话可及性、多语言语音互动与现代RAG技术结合，能够为偏远地区农民提供专家级农业咨询，并为建立全面的AI驱动农业咨询生态系统奠定可行基础。

Abstract: In Bangladesh, many farmers continue to face challenges in accessing timely,
expert-level agricultural guidance. This paper presents KrishokBondhu, a
voice-enabled, call-centre-integrated advisory platform built on a
Retrieval-Augmented Generation (RAG) framework, designed specifically for
Bengali-speaking farmers. The system aggregates authoritative agricultural
handbooks, extension manuals, and NGO publications; applies Optical Character
Recognition (OCR) and document-parsing pipelines to digitize and structure the
content; and indexes this corpus in a vector database for efficient semantic
retrieval. Through a simple phone-based interface, farmers can call the system
to receive real-time, context-aware advice: speech-to-text converts the Bengali
query, the RAG module retrieves relevant content, a large language model (Gemma
3-4B) generates a context-grounded response, and text-to-speech delivers the
answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced
high-quality responses for 72.7% of diverse agricultural queries covering crop
management, disease control, and cultivation practices. Compared to the
KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on
a 5-point scale, a 44.7% improvement, with especially large gains in contextual
richness (+367%) and completeness (+100.4%), while maintaining comparable
relevance and technical specificity. Semantic similarity analysis further
revealed a strong correlation between retrieved context and answer quality,
emphasizing the importance of grounding generative responses in curated
documentation. KrishokBondhu demonstrates the feasibility of integrating
call-centre accessibility, multilingual voice interaction, and modern RAG
techniques to deliver expert-level agricultural guidance to remote Bangladeshi
farmers, paving the way toward a fully AI-driven agricultural advisory
ecosystem.

</details>


### [3] [POPI: Personalizing LLMs via Optimized Natural Language Preference Inference](https://arxiv.org/abs/2510.17881)
*Yizhuo Chen,Xin Liu,Ruijie Wang,Zheng Li,Pei Chen,Changlong Yu,Priyanka Nigam,Meng Jiang,Bing Yin*

Main category: cs.CL

TL;DR: 提出 POPI 框架，通过偏好推断模型将多样化的用户信号压缩为简洁的自然语言摘要，作为个性化的条件信息；在强化学习下联合优化信号推断与个性化生成；能显著降低上下文开销并可迁移至冻结的通用 LLM 实现即插即用个性化。


<details>
  <summary>Details</summary>
Motivation: LLMs 的性能提升与用户体验受风格、语调和推理模式分歧影响；现有对齐方法偏向总体平均，忽略个体差异；逐个用户微调成本高，直接在上下文中注入原始信号噪声大且低效。

Method: 引入偏好推断模型，将异质用户信号压缩为简明自然语言摘要，这些摘要作为透明、紧凑且可迁移的个性化表示，条件化共享生成模型实现个性化文本；在统一目标下利用强化学习对偏好推断与个性化生成共同优化，确保摘要最大化编码有用的偏好信息。

Result: 在四个个性化基准上的实验证明，POPI 显著提升个性化准确性并大幅降低上下文开销；优化后的摘要还能迁移到冻结的现成大模型，实现无需权重更新的即插即用个性化。

Conclusion: POPI 提供透明、紧凑且可迁移的个性化表示，提升个性化效果与效率，且具备对其他模型的良好迁移性与适用性。

Abstract: Large language models (LLMs) achieve strong benchmark performance, yet user
experiences remain inconsistent due to diverse preferences in style, tone, and
reasoning mode. Nevertheless, existing alignment techniques such as
reinforcement learning from human feedback (RLHF) or Direct Preference
Optimization (DPO) largely optimize toward population-level averages and
overlook individual variation. Naive personalization strategies like per-user
fine-tuning are computationally prohibitive, and in-context approaches that
prepend raw user signals often suffer from inefficiency and noise. To address
these challenges, we propose POPI, a general framework that introduces a
preference inference model to distill heterogeneous user signals into concise
natural language summaries. These summaries act as transparent, compact, and
transferable personalization representations that condition a shared generation
model to produce personalized responses. POPI jointly optimizes both preference
inference and personalized generation under a unified objective using
reinforcement learning, ensuring summaries maximally encode useful preference
information. Extensive experiments across four personalization benchmarks
demonstrate that POPI consistently improves personalization accuracy while
reducing context overhead by a large margin. Moreover, optimized summaries
seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play
personalization without weight updates.

</details>


### [4] [JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs](https://arxiv.org/abs/2510.17918)
*Junlan Feng,Fanyu Meng,Chong Long,Pengyu Cong,Duqing Wang,Yan Zheng,Yuyao Zhang,Xuanchang Gao,Ye Yuan,Yunfei Ma,Zhijie Ren,Fan Yang,Na Wu,Di Jin,Chao Deng*

Main category: cs.CL

TL;DR: 提出 Data with World Context (DWC) 框架，通过在预训练数据中结合世界语境和产业场景，提升大语言模型的安全性和可信度；在 JT-35B 早期检查点基础上以 1.5T DWC tokens 继续预训练并进行后训练，与同规模对比模型相比，获得 1.79% 的平均提升，且使用的总 token 数仅为 6.2T。


<details>
  <summary>Details</summary>
Motivation: 解决预训练数据中的事实错误、逻辑不一致与分布偏差，以及缺乏世界知识的 grounding 问题，因数据量大难以完全清洗；通过引入世界情境来锚定数据在真实场景中的语义，从而提升模型安全性与可信性。

Method: 提出 Data with World Context (DWC)，将世界上下文信息与产业场景数据融入预训练数据；在 JT-35B-Base 的早期检查点上继续预训练，使用 1.5 万亿 DWC token，并通过后训练过程激活 DWC 的潜力；与同规模的 Qwen 模型进行对比评估。

Result: 相较于同规模的 Qwen，JT-Safe-35B 在 Safety and Trustworthy 基准上平均提升 1.79%；且在仅用 6.2T token 的训练量条件下达到这一提升。

Conclusion: 将世界语境融入预训练数据是提升 LLM 安全性与可信度的有效途径，且具备在较低数据量下实现性能提升的潜力，具有较好的扩展性。

Abstract: The hallucination and credibility concerns of large language models (LLMs)
are global challenges that the industry is collectively addressing. Recently, a
significant amount of advances have been made on post-training and inference
techniques to mitigate these challenges. However, it is widely agreed that
unsafe and hallucinations of LLMs intrinsically originate from pre-training,
involving pre-training data and the next-token prediction learning mechanism.
In this paper, we focus on enhancing pre-training data to improve the
trustworthiness and safety of LLMs. Since the data is vast, it's almost
impossible to entirely purge the data of factual errors, logical
inconsistencies, or distributional biases. Moreover, the pre-training data lack
grounding in real-world knowledge. Each piece of data is treated as a sequence
of tokens rather than as a representation of a part of the world. To overcome
these issues, we propose approaches to enhancing our pre-training data with its
context in the world and increasing a substantial amount of data reflecting
industrial scenarios. We argue that most source data are created by the authors
for specific purposes in a certain spatial-temporal context. They have played a
role in the real world. By incorporating related world context information, we
aim to better anchor pre-training data within real-world scenarios, thereby
reducing uncertainty in model training and enhancing the model's safety and
trustworthiness. We refer to our Data with World Context as DWC. We continue
pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC
tokens. We introduce our post-training procedures to activate the potentials of
DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an
average performance improvement of 1.79% on the Safety and Trustworthy
evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.

</details>


### [5] [Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models](https://arxiv.org/abs/2510.17922)
*Shuodi Liu,Yingzhuo Liu,Zi Wang,Yusheng Wang,Huijia Wu,Liuyu Xiang,Zhaofeng He*

Main category: cs.CL

TL;DR: 提出 Select-Then-Decompose (STD) 框架，通过在任务特征下动态选择最合适的分解策略，并在执行后加入验证模块，形成闭环解决过程；在多个基准上实现了性能-成本的 Pareto 最优或接近 Pareto 最优，并提供公开代码。


<details>
  <summary>Details</summary>
Motivation: 现有的任务分解方法多关注记忆、工具使用和反馈机制，而常忽视性能与成本之间的权衡。需要一个能根据任务特征动态选择分解策略、并通过验证增强可靠性的通用框架。

Method: 首先对任务分解进行六种分类体系梳理；对影响分解效果与成本的三大因素（方法类别、任务特征、分解与执行模型的配置）进行经验分析；在此基础上提出 Select-Then-Decompose（STD）策略，形成包含选择、执行、验证三阶段的闭环；通过跨基准的实证评估验证其在 Pareto 前沿的稳健性。

Result: STD 在多项基准上持续位于 Pareto 前沿，显示出对性能和成本的最优权衡能力；公开代码以便复现。

Conclusion: 基于任务特征的动态分解选择结合验证模块，提供了一种对性能与成本兼顾的实用任务分解框架，具有较好的可靠性与可扩展性。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning and
planning capabilities, driving extensive research into task decomposition.
Existing task decomposition methods focus primarily on memory, tool usage, and
feedback mechanisms, achieving notable success in specific domains, but they
often overlook the trade-off between performance and cost. In this study, we
first conduct a comprehensive investigation on task decomposition, identifying
six categorization schemes. Then, we perform an empirical analysis of three
factors that influence the performance and cost of task decomposition:
categories of approaches, characteristics of tasks, and configuration of
decomposition and execution models, uncovering three critical insights and
summarizing a set of practical principles. Building on this analysis, we
propose the Select-Then-Decompose strategy, which establishes a closed-loop
problem-solving process composed of three stages: selection, execution, and
verification. This strategy dynamically selects the most suitable decomposition
approach based on task characteristics and enhances the reliability of the
results through a verification module. Comprehensive evaluations across
multiple benchmarks show that the Select-Then-Decompose consistently lies on
the Pareto frontier, demonstrating an optimal balance between performance and
cost. Our code is publicly available at
https://github.com/summervvind/Select-Then-Decompose.

</details>


### [6] [Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs](https://arxiv.org/abs/2510.17924)
*Yehor Tereshchenko,Mika Hämäläinen*

Main category: cs.CL

TL;DR: 本文对在线游戏平台聊天室中的自动 toxicity 检测进行了全面的比较分析，比较了传统机器学习模型、零-shot/few-shot 的大语言模型、微调后的变换器模型，以及检索增强生成（RAG）等方法。评估框架在分类准确性、处理速度与计算成本三个维度上进行，提出一个混合 moderation 系统架构以通过自动检测来减轻人类审核负担，并实现持续学习。实验结果显示不同方法之间存在显著的性能差异，微调的 DistilBERT 在准确性与成本之间实现最佳折衷。研究为动态的在线游戏平台环境下部署高性价比且高效的内容补充审核系统提供了经验依据。


<details>
  <summary>Details</summary>
Motivation: 需要在在线游戏平台环境中实现高效、可扩展的内容审核，平衡准确性、处理速度和计算成本，同时减轻人工审核负担。

Method: 对传统机器学习 + 嵌入、LLMs 的零-shot与少-shot提示、微调 Transformer 模型、RAG 方法进行对比评估，评估框架覆盖三个维度。

Result: 不同方法呈现显著的性能变化，微调 DistilBERT 实现最佳的准确性与成本权衡。

Conclusion: 为在动态在线游戏平台环境中部署成本效益高、效率高的内容审核系统提供了经验性证据，并提出了一个通过自动检测来优化人工审核工作的混合系统架构与持续学习机制。

Abstract: This paper presents a comprehensive comparative analysis of Natural Language
Processing (NLP) methods for automated toxicity detection in online gaming
chats. Traditional machine learning models with embeddings, large language
models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer
models, and retrieval-augmented generation (RAG) approaches are evaluated. The
evaluation framework assesses three critical dimensions: classification
accuracy, processing speed, and computational costs. A hybrid moderation system
architecture is proposed that optimizes human moderator workload through
automated detection and incorporates continuous learning mechanisms. The
experimental results demonstrate significant performance variations across
methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs.
The findings provide empirical evidence for deploying cost-effective, efficient
content moderation systems in dynamic online gaming environments.

</details>


### [7] [Diagnosing Representation Dynamics in NER Model Extension](https://arxiv.org/abs/2510.17930)
*Xirui Zhang,Philippe de La Chevasnerie,Benoit Fabre*

Main category: cs.CL

TL;DR: 将名实体识别迁移到新PII模式时，BERT在标准语义实体与模式化PII间实现和平共处；揭示独立的语义与形态特征机制，以及’O’标签的可塑性对新学习的影响。


<details>
  <summary>Details</summary>
Motivation: 研究在嘈杂口语数据中扩展NER到新的PII实体的可行性，以及不同特征机制如何影响持续学习和模式适应。

Method: 在一个增量学习框架中，对BERT进行联合微调，包含PER、LOC、ORG等标准实体及EMAIL、PHONE等新PII；通过渐进学习测量语义漂移，诊断模型的表示重叠和O标签的反向漂移。

Result: 对LOC最易受影响，因其与新PII共用模式特征；同时发现“反向O标签表示漂移”：模型初始将PII模式映射到O，阻碍新学习；通过解冻O标签分类器，使背景类适应并释放这些模式，从而实现迁移。

Conclusion: 论文提供了对NER模型适应的机械洞察，强调特征独立性、表示重叠以及O标签的可塑性对持续学习的影响。

Abstract: Extending Named Entity Recognition (NER) models to new PII entities in noisy
spoken-language data is a common need. We find that jointly fine-tuning a BERT
model on standard semantic entities (PER, LOC, ORG) and new pattern-based PII
(EMAIL, PHONE) results in minimal degradation for original classes. We
investigate this "peaceful coexistence," hypothesizing that the model uses
independent semantic vs. morphological feature mechanisms.
  Using an incremental learning setup as a diagnostic tool, we measure semantic
drift and find two key insights. First, the LOC (location) entity is uniquely
vulnerable due to a representation overlap with new PII, as it shares
pattern-like features (e.g., postal codes). Second, we identify a "reverse
O-tag representation drift." The model, initially trained to map PII patterns
to 'O', blocks new learning. This is resolved only by unfreezing the 'O' tag's
classifier, allowing the background class to adapt and "release" these
patterns. This work provides a mechanistic diagnosis of NER model adaptation,
highlighting feature independence, representation overlap, and 'O' tag
plasticity.

</details>


### [8] [AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM](https://arxiv.org/abs/2510.17934)
*Haoyu Huang,Hong Ting Tsang,Jiaxin Bai,Xi Peng,Gong Zhang,Yangqiu Song*

Main category: cs.CL

TL;DR: AtlasKV 提出一种参数化的知识整合方法，将大规模知识图谱嵌入到大语言模型中，利用 KG2KV 与 HiKVP 实现对 LLMS 的低内存、亚线性时间的知识对齐，无需外部检索或再训练。


<details>
  <summary>Details</summary>
Motivation: RAG 在知识注入中高度依赖检索模块和所检索文本，随着知识规模增大导致检索延迟与上下文开销显著增加。需要一种可扩展、低内存、无需检索的知识嵌入方案来支持亿级知识图谱。

Method: 提出 KG2KV 和 HiKVP，将知识图谱三元组直接整合到 LLM 的注意力机制中，构建 AtlasKV；实现对大规模 KG 的亚线性时间和内存复杂度的对齐，且无需外部检索、长上下文先验或参数再训练来适配新知识。

Result: 在参数化知识融入方面证明对大规模 KG 的可行性，保持较强的知识 grounding 与泛化能力，同时显著降低显存需求（可在<20GB VRAM 下运行），并实现亚线性时间/内存复杂度。

Conclusion: AtlasKV 为大规模知识注入提供一种高效的参数化方案，展示在不依赖外部检索或微调的情况下，对亿级知识图谱的可扩展性和实用性，且提升知识 grounding 与适应性。

Abstract: Retrieval-augmented generation (RAG) has shown some success in augmenting
large language models (LLMs) with external knowledge. However, as a
non-parametric knowledge integration paradigm for LLMs, RAG methods heavily
rely on external retrieval modules and the retrieved textual context prior.
Especially for very large scale knowledge augmentation, they would introduce
substantial inference latency due to expensive searches and much longer
relevant context. In this paper, we propose a parametric knowledge integration
method, called \textbf{AtlasKV}, a scalable, effective, and general way to
augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using
very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we
introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with
sub-linear time and memory complexity. It maintains strong knowledge grounding
and generalization performance using the LLMs' inherent attention mechanism,
and requires no external retrievers, long context priors, or retraining when
adapting to new knowledge.

</details>


### [9] [Believe It or Not: How Deeply do LLMs Believe Implanted Facts?](https://arxiv.org/abs/2510.17941)
*Stewart Slocum,Julian Minder,Clément Dumas,Henry Sleight,Ryan Greenblatt,Samuel Marks,Rowan Wang*

Main category: cs.CL

TL;DR: 提出信念深度框架来评估大模型知识编辑的深度植入，结果显示简单提示与机械编辑难以深度植入；SDF 往往能实现类似真实知识的信念，但对违反基本常识的植入较脆弱且与真实知识的表征不同；给出可衡量的评估标准以支撑实际应用。


<details>
  <summary>Details</summary>
Motivation: 研究目标是判断并量化大语言模型在知识编辑中对新事实的“信念深度”，即新知识能否泛化、经受自我质疑、以及在表征层面是否接近真实知识。

Method: 提出 belief depth 框架，量化三方面：1) 对相关情境的泛化能力（如跨越多步推理的应用）；2) 对自我质疑与直接挑战的鲁棒性；3) 通过线性探查衡量与真实知识的一致性。并比较不同编辑策略：简单提示、机械式编辑，以及 Synthetic Document Finetuning (SDF)。

Result: 实验表明：提示和机械编辑难以将知识深度植入模型；SDF 在多数场景能实现类似真实知识的信念，然而其成功并非普适，对与基本世界知识相悖的植入更脆弱且在表征上与真实知识存在差异。

Conclusion: 提出可操作的信念深度评估标准，促使对知识编辑的严格评估，从而为真实世界应用中的知识编辑部署提供依据。

Abstract: Knowledge editing techniques promise to implant new factual knowledge into
large language models (LLMs). But do LLMs really believe these facts? We
develop a framework to measure belief depth and use it to evaluate the success
of knowledge editing techniques. We operationalize belief depth as the extent
to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi
estimates several logical steps removed), 2) is robust to self-scrutiny and
direct challenge, and 3) is represented similarly to genuine knowledge (as
measured by linear probes). Our evaluations show that simple prompting and
mechanistic editing techniques fail to implant knowledge deeply. In contrast,
Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated
documents consistent with a fact - often succeeds at implanting beliefs that
behave similarly to genuine knowledge. However, SDF's success is not universal,
as implanted beliefs that contradict basic world knowledge are brittle and
representationally distinct from genuine knowledge. Overall, our work
introduces measurable criteria for belief depth and enables the rigorous
evaluation necessary for deploying knowledge editing in real-world
applications.

</details>


### [10] [SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone](https://arxiv.org/abs/2510.17998)
*Nishant Subramani,Alfredo Gomez,Mona Diab*

Main category: cs.CL

TL;DR: 三阶段框架 SimBA：通过 stalk、prowl、pounce 简化大语言模型基准分析，发现可用很小的代表子集覆盖基准并预测新模型性能。


<details>
  <summary>Details</summary>
Motivation: 原始评估分数难以直观比较，难以在模型选择与数据集评估中提炼信息；需要一个代表性子集来提高基准分析的效率与可解释性。

Method: 引入 SimBA 的三阶段流程：stalk（对数据集与模型进行比较），prawl（在原始分数的基础上发现具有代表性的数据子集），pounce（用代表性子集预测未见模型在 hold-out 集上的性能）。提出覆盖基准的代表子集发现算法，并在 HELM、MMLU、BigBenchLite 上验证，给出数据集占比与覆盖率、以及对模型排名的保持和预测误差（低于近零的均方误差）。

Result: 在三个基准中，数据集与模型之间存在较强相关性；以6.25%、1.7%、28.4% 的数据集覆盖率达到至少95%的覆盖；用这些代表子集即可保持模型排名并对未见模型的性能进行近零均方误差的预测。该方法可提升模型训练与数据集创建的效率，且代码开源。

Conclusion: SimBA 能显著简化基准分析、提高效率，并帮助数据集创造者验证新数据集与现有基准的差异，具备实用性与可扩展性。

Abstract: Modern language models are evaluated on large benchmarks, which are difficult
to make sense of, especially for model selection. Looking at the raw evaluation
numbers themselves using a model-centric lens, we propose SimBA, a three phase
framework to Simplify Benchmark Analysis. The three phases of SimBA are: stalk,
where we conduct dataset & model comparisons, prowl, where we discover a
representative subset, and pounce, where we use the representative subset to
predict performance on a held-out set of models. Applying SimBA to three
popular LM benchmarks: HELM, MMLU, and BigBenchLite reveals that across all
three benchmarks, datasets and models relate strongly to one another (stalk).
We develop an representative set discovery algorithm which covers a benchmark
using raw evaluation scores alone. Using our algorithm, we find that with 6.25%
(1/16), 1.7% (1/58), and 28.4% (21/74) of the datasets for HELM, MMLU, and
BigBenchLite respectively, we achieve coverage levels of at least 95% (prowl).
Additionally, using just these representative subsets, we can both preserve
model ranks and predict performance on a held-out set of models with near zero
mean-squared error (pounce). Taken together, SimBA can help model developers
improve efficiency during model training and dataset creators validate whether
their newly created dataset differs from existing datasets in a benchmark. Our
code is open source, available at https://github.com/nishantsubramani/simba.

</details>


### [11] [Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution](https://arxiv.org/abs/2510.18019)
*Asim Mohamed,Martin Gubri*

Main category: cs.CL

TL;DR: A multilingual watermarking method STEAM improves robustness against translation attacks across languages; it uses back-translation to restore watermark strength, compatible with any watermarking method, and shows gains in AUC and TPR@1% across 17 languages.


<details>
  <summary>Details</summary>
Motivation: Current multilingual watermarking methods claim cross-lingual robustness but are evaluated mainly on high-resource languages and fail under translation attacks in medium- and low-resource languages due to semantic clustering issues when tokenizer vocabularies have few full-word tokens.

Method: Introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. It is compatible with any watermarking method, robust to different tokenizers and languages, non-invasive, and easily extendable to new languages.

Result: Average gains of +0.19 AUC and +40%p TPR@1% on 17 languages.

Conclusion: STEAM provides a simple and robust path toward fairer watermarking across diverse languages and can be integrated with existing watermarking techniques to improve cross-lingual robustness.

Abstract: Multilingual watermarking aims to make large language model (LLM) outputs
traceable across languages, yet current methods still fall short. Despite
claims of cross-lingual robustness, they are evaluated only on high-resource
languages. We show that existing multilingual watermarking methods are not
truly multilingual: they fail to remain robust under translation attacks in
medium- and low-resource languages. We trace this failure to semantic
clustering, which fails when the tokenizer vocabulary contains too few
full-word tokens for a given language. To address this, we introduce STEAM, a
back-translation-based detection method that restores watermark strength lost
through translation. STEAM is compatible with any watermarking method, robust
across different tokenizers and languages, non-invasive, and easily extendable
to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17
languages, STEAM provides a simple and robust path toward fairer watermarking
across diverse languages.

</details>


### [12] [Language Models as Semantic Augmenters for Sequential Recommenders](https://arxiv.org/abs/2510.18046)
*Mahsa Valizadeh,Xiangjue Dong,Rui Tuo,James Caverlee*

Main category: cs.CL

TL;DR: 通过把LLMs作为语义上下文生成器，LaMAR 自动为序列数据生成辅助信号，从而提升序列建模性能与表示能力。


<details>
  <summary>Details</summary>
Motivation: 在序列用户行为建模中，当语义上下文有限或缺失时，模型性能往往受限。LLMs具备捕捉潜在语义与跨模态关系的能力，能够自动推断并生成有用的上下文信号，缓解数据不足带来的困难。

Method: 在少样本（few-shot）设定下，利用LLMs从现有元数据中推断用户意图与物品关系的潜在语义并生成辅助信号（如使用情景、物品意图、主题摘要等），将这些信号与原始序列组合，作为增强输入用于基准序列建模任务的训练与评估。

Result: 引入的信号在基准序列建模任务中带来一致的性能提升；LLM生成的信号展现出较高的语义新颖性与多样性，提升下游模型的表示能力。

Conclusion: 这提出一种数据中心化的新范式：将LLMs用作智能上下文生成器，半自动地创建训练数据和语言资源，从而提升序列建模的语义表达与泛化潜力。

Abstract: Large Language Models (LLMs) excel at capturing latent semantics and
contextual relationships across diverse modalities. However, in modeling user
behavior from sequential interaction data, performance often suffers when such
semantic context is limited or absent. We introduce LaMAR, a LLM-driven
semantic enrichment framework designed to enrich such sequences automatically.
LaMAR leverages LLMs in a few-shot setting to generate auxiliary contextual
signals by inferring latent semantic aspects of a user's intent and item
relationships from existing metadata. These generated signals, such as inferred
usage scenarios, item intents, or thematic summaries, augment the original
sequences with greater contextual depth. We demonstrate the utility of this
generated resource by integrating it into benchmark sequential modeling tasks,
where it consistently improves performance. Further analysis shows that
LLM-generated signals exhibit high semantic novelty and diversity, enhancing
the representational capacity of the downstream models. This work represents a
new data-centric paradigm where LLMs serve as intelligent context generators,
contributing a new method for the semi-automatic creation of training data and
language resources.

</details>


### [13] [Chain-of-Thought Reasoning Improves Context-Aware Translation with Large Language Models](https://arxiv.org/abs/2510.18077)
*Shabnam Ataee,Andrei Popescu-Belis*

Main category: cs.CL

TL;DR: 研究评估大型语言模型在具有跨句子依赖的文本翻译任务中的能力，使用 English-French DiscEvalMT 基准，比较带链式推理提示与非推理提示对两项任务的影响，结果显示最优模型在推理条件下表现显著提升，GPT-4/4o与Phi系列突出，“wise get wiser”现象表明推理提升与基础性能正相关。


<details>
  <summary>Details</summary>
Motivation: 验证 LLM 在跨句际依赖翻译中的能力，以及推理能力对翻译质量的影响，尤其是对两类任务（辨别正确翻译与生成正确翻译）的作用。

Method: 基准 DiscEvalMT（英-法），包含涉代词照应和词汇衔接的句对；对 12 种模型（DeepSeek-R1、GPT、Llama、Mistral、Phi 家族）在两项任务上评估；比较启用链式推理与不启用的提示；评价指标为第一任务的准确率与第二任务的 COMET 得分；并考察推理提示效果随模型性能的变化关系。

Result: 最佳模型在第一任务约达到 90% 的准确率，在第二任务 COMET 得分约 92%；GPT-4、GPT-4o 和 Phi 模型表现突出。观察到“wise get wiser”效应：使用推理提示的增益与未使用推理时的模型分数呈正相关，即更强的模型在推理条件下收益更大。

Conclusion: 推理提示能提升跨句依赖翻译任务的表现，尤其对顶尖模型效果显著；模型间对推理收益存在系统差异，提示未来可结合更强的推理能力提升翻译质量，同时需权衡推理成本与可解释性。

Abstract: This paper assesses the capacity of large language models (LLMs) to translate
texts that include inter-sentential dependencies. We use the English-French
DiscEvalMT benchmark (Bawden et al., 2018) with pairs of sentences containing
translation challenges either for pronominal anaphora or for lexical cohesion.
We evaluate 12 LLMs from the DeepSeek-R1, GPT, Llama, Mistral and Phi families
on two tasks: (1) distinguishing a correct translation from a wrong but
plausible one; (2) generating a correct translation. We compare prompts that
encourage chain-of-thought reasoning with those that do not. The best models
take advantage of reasoning and reach about 90% accuracy on the first task, and
COMET scores of about 92% on the second task, with GPT-4, GPT-4o and Phi
standing out. Moreover, we observe a "wise get wiser" effect: the improvements
through reasoning are positively correlated with the scores of the models
without reasoning.

</details>


### [14] [Na Prática, qual IA Entende o Direito? Um Estudo Experimental com IAs Generalistas e uma IA Jurídica](https://arxiv.org/abs/2510.18108)
*Marina Soares Marinho,Daniela Vianna,Livy Real,Altigran da Silva,Gabriela Migliorini*

Main category: cs.CL

TL;DR: 领域专用的 JusIA 在对比通用系统的实验中，在法律日常任务中表现最佳，强调领域专业性和理论驱动评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 为评估法律AI输出的可靠性，结合法律理论指标与实际专业评估，解决通用AI在法律领域的局限。

Method: 设计实验协议，评估材料正确性、系统性连贯性、论证完整性等法律理论指标；通过 48 名法律专业人士评估；比较四个系统（JusIA、ChatGPT Free、ChatGPT Pro、Gemini）在模拟律师日常任务中的表现。

Result: JusIA 在所有评估任务中持续优于通用系统，显示域专门化的优势；理论驱动的评估对获得可靠法律AI输出至关重要。

Conclusion: 域专门化与理论基础评估共同提升法律AI的可靠性，建议在实际应用中优先采用领域专业模型并辅以系统化的理论评估框架。

Abstract: This study presents the Jusbrasil Study on the Use of General-Purpose AIs in
Law, proposing an experimental evaluation protocol combining legal theory, such
as material correctness, systematic coherence, and argumentative integrity,
with empirical assessment by 48 legal professionals. Four systems (JusIA,
ChatGPT Free, ChatGPT Pro, and Gemini) were tested in tasks simulating lawyers'
daily work. JusIA, a domain-specialized model, consistently outperformed the
general-purpose systems, showing that both domain specialization and a
theoretically grounded evaluation are essential for reliable legal AI outputs.

</details>


### [15] [Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt Engineering Experiment](https://arxiv.org/abs/2510.18112)
*Patricia Delafuente,Arya Honraopatil,Lara J. Martin*

Main category: cs.CL

TL;DR: 本研究表明，在将DnD玩家行为转化为Avrae Discord命令的任务中，使用指令式（instruct）模型比推理（reasoning）模型更为有效，且提示词的微小改动即可显著影响输出，因此需要精确的提示设计。


<details>
  <summary>Details</summary>
Motivation: 动机是以大语言模型协助自动生成DnD命令，并评估提示设计和不同模型类别（推理型与指令型）在该任务上的表现。

Method: 方法包括在FIREBALL数据集上评估两种模型：DeepSeek-R1-Distill-LLaMA-8B（推理型）和LLaMA-3.1-8B-Instruct（指令型），以生成Avrae Discord bot命令，并对输出进行比较。

Result: 结果显示：提供具体指令对模型输出影响显著，单句提示也能对输出产生较大影响；指令型模型在该命题下足以胜任任务，相较之下推理型模型表现劣势。

Conclusion: 结论是：在需要将玩家动作转化为具体命令的场景中，选择指令型模型并注重提示设计可获得更好效果，推理型模型在此任务中并非必需。

Abstract: This paper explores the application of Large Language Models (LLMs) and
reasoning to predict Dungeons & Dragons (DnD) player actions and format them as
Avrae Discord bot commands. Using the FIREBALL dataset, we evaluated a
reasoning model, DeepSeek-R1-Distill-LLaMA-8B, and an instruct model,
LLaMA-3.1-8B-Instruct, for command generation. Our findings highlight the
importance of providing specific instructions to models, that even single
sentence changes in prompts can greatly affect the output of models, and that
instruct models are sufficient for this task compared to reasoning models.

</details>


### [16] [LLMs Encode How Difficult Problems Are](https://arxiv.org/abs/2510.18147)
*William Lugoloobi,Chris Russell*

Main category: cs.CL

TL;DR: 人类标注的难度信号可以线性解码，并随模型规模增强；基于模型输出的自动难度信号在模型升级过程中变得不稳定且与性能负相关。对难度方向的引导可降低幻觉并提高准确率。 RL 训练中，人类难度信号被放大并正相关于测试准确性，而自动难度信号则负相关。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否内在地把题目难度编码成可解码的信号，以及该信号在强化学习后期训练中的追踪和影响；对比人类注释难度与基于模型表现推断的自动难度之间的稳定性与可用性。

Method: 在60个模型的多层和不同token位置上训练线性探针，评估Easy2HardBench的数学与编程子集；在Qwen2.5-Math-1.5B的GRPO训练中，比较人类难度探针与LLM难度探针在训练过程中的表现与相关性。

Result: 人类标注的难度信号可被强线性解码，且随模型规模呈显著扩展；而LLM推断出的难度信号较弱且随模型提升而变差。沿着难度方向进行 steer，可降低幻觉并提高准确率。在GRPO训练过程中，人类难度探针逐步增强并与测试准确性正相关；相反，LLM难度探针变强则与性能呈负相关。

Conclusion: 人类注释提供了一个稳定的难度信号，强化学习会放大该信号并提升模型表现；而自动化难度估计在模型能力提升时会变得与性能错位，从而不再可靠。研究还提供了可复现的探针实现与评估代码。

Abstract: Large language models exhibit a puzzling inconsistency: they solve complex
problems yet frequently fail on seemingly simpler ones. We investigate whether
LLMs internally encode problem difficulty in a way that aligns with human
judgment, and whether this representation tracks generalization during
reinforcement learning post-training. We train linear probes across layers and
token positions on 60 models, evaluating on mathematical and coding subsets of
Easy2HardBench. We find that human-labeled difficulty is strongly linearly
decodable (AMC: $\rho \approx 0.88$) and exhibits clear model-size scaling,
whereas LLM-derived difficulty is substantially weaker and scales poorly.
Steering along the difficulty direction reveals that pushing models toward
"easier" representations reduces hallucination and improves accuracy. During
GRPO training on Qwen2.5-Math-1.5B, the human-difficulty probe strengthens and
positively correlates with test accuracy across training steps, while the
LLM-difficulty probe degrades and negatively correlates with performance. These
results suggest that human annotations provide a stable difficulty signal that
RL amplifies, while automated difficulty estimates derived from model
performance become misaligned precisely as models improve. We release probe
code and evaluation scripts to facilitate replication.

</details>


### [17] [Extracting Rule-based Descriptions of Attention Features in Transformers](https://arxiv.org/abs/2510.18148)
*Dan Friedman,Adithya Bhaskar,Alexander Wettig,Danqi Chen*

Main category: cs.CL

TL;DR: 从一个关于可解释性的研究出发，提出用基于规则的描述来解释变换器的特征，而非仅靠稀疏线性组合的“基元” exemplars。通过提取三类规则（跳格/跳词、缺失、计数）来表达输入与输出的交互，已在GPT-2 small 上初步验证，多数特征可用约100条跳格规则描述，缺失规则在前几层就已丰富，个别存在计数规则。


<details>
  <summary>Details</summary>
Motivation: 解决现有以样例激活特征的解释方法的主观性问题，提出可读性更强、直接映射输入-输出的规则性描述，以提升对注意力层及其特征的解释能力。

Method: 提出三类规则来描述输入与输出之间的交互：1) 跳格规则（如 "[Canadian city]... speaks --> English"），2) 缺失规则（如 "[Montreal]... speaks -/-> English"），3) 计数规则（在某个词计数超过阈值时触发或抑制输出）。给出从变换器中自动提取这些规则的简单方法，并在 GPT-2 small 上进行实验。

Result: 多数特征可用约100条跳格规则描述清楚；缺失规则在第一层及早出现，已占相当比例；也发现若干计数规则；初步建立了规则描述的定义、提取方法和行为分类（taxonomy）的基线。

Conclusion: 为以规则为导向的特征描述奠定基础，证明了可从变换器中提取可解释的规则描述，并提出未来在解释性研究中的方向与可能性。

Abstract: Mechanistic interpretability strives to explain model behavior in terms of
bottom-up primitives. The leading paradigm is to express hidden states as a
sparse linear combination of basis vectors, called features. However, this only
identifies which text sequences (exemplars) activate which features; the actual
interpretation of features requires subjective inspection of these exemplars.
This paper advocates for a different solution: rule-based descriptions that
match token patterns in the input and correspondingly increase or decrease the
likelihood of specific output tokens. Specifically, we extract rule-based
descriptions of SAE features trained on the outputs of attention layers. While
prior work treats the attention layers as an opaque box, we describe how it may
naturally be expressed in terms of interactions between input and output
features, of which we study three types: (1) skip-gram rules of the form
"[Canadian city]... speaks --> English", (2) absence rules of the form
"[Montreal]... speaks -/-> English," and (3) counting rules that toggle only
when the count of a word exceeds a certain value or the count of another word.
Absence and counting rules are not readily discovered by inspection of
exemplars, where manual and automatic descriptions often identify misleading or
incomplete explanations. We then describe a simple approach to extract these
types of rules automatically from a transformer, and apply it to GPT-2 small.
We find that a majority of features may be described well with around 100
skip-gram rules, though absence rules are abundant even as early as the first
layer (in over a fourth of features). We also isolate a few examples of
counting rules. This paper lays the groundwork for future research into
rule-based descriptions of features by defining them, showing how they may be
extracted, and providing a preliminary taxonomy of some of the behaviors they
represent.

</details>


### [18] [Automatic Prompt Generation via Adaptive Selection of Prompting Techniques](https://arxiv.org/abs/2510.18162)
*Yohei Ikenoue,Hitomi Tashiro,Shigeru Kuroyanagi*

Main category: cs.CL

TL;DR: 本研究提出一种自适应提示框架，将抽象任务描述映射到提示技巧知识库，自动生成高质量提示，在23个BBEH任务上优于基线和现有自动提示生成工具。


<details>
  <summary>Details</summary>
Motivation: 提示工程对LLMs的可靠性和效果至关重要，但需要专业知识和对任务的深入理解；存在让非专家也能获得高质量提示的需求，且无需依赖固定模板或框架。

Method: 构建一个知识库，将任务簇（基于跨任务的语义相似性）与提示技巧相关联；用户输入任务描述后将其分配到最相关的任务簇，并从知识库整合多种技巧动态生成提示；整个过程不依赖现成模板或框架。

Result: 在23项BBEH任务的实验中，该方法在算术均值和调和均值等指标上均优于标准提示和现有自动提示生成工具。

Conclusion: 本研究为简化与标准化提示创建奠定基础，使非专家也能更有效地利用LLMs。

Abstract: Prompt engineering is crucial for achieving reliable and effective outputs
from large language models (LLMs), but its design requires specialized
knowledge of prompting techniques and a deep understanding of target tasks. To
address this challenge, we propose a novel method that adaptively selects
task-appropriate prompting techniques based on users' abstract task
descriptions and automatically generates high-quality prompts without relying
on pre-existing templates or frameworks. The proposed method constructs a
knowledge base that associates task clusters, characterized by semantic
similarity across diverse tasks, with their corresponding prompting techniques.
When users input task descriptions, the system assigns them to the most
relevant task cluster and dynamically generates prompts by integrating
techniques drawn from the knowledge base. An experimental evaluation of the
proposed method on 23 tasks from BIG-Bench Extra Hard (BBEH) demonstrates
superior performance compared with standard prompts and existing automatic
prompt-generation tools, as measured by both arithmetic and harmonic mean
scores. This research establishes a foundation for streamlining and
standardizing prompt creation, enabling non-experts to effectively leverage
LLMs.

</details>


### [19] [CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language Models](https://arxiv.org/abs/2510.18173)
*Ritam Upadhyay,Naman Ahuja,Rishabh Baral,Aparna Garimella,Vivek Gupta*

Main category: cs.CL

TL;DR: CMT-Bench 通过现场板球解说数据的诊断基准，评估LLM在动态文本到表格任务中的鲁棒性，揭示缺乏摘要提取时性能下降与随输入长度增大而退化的问题，且对实体形式扰动敏感，强调以鲁棒性优先的评估以推动高效可扩展的方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的文本到表格系统依赖大量提示工程或迭代事件抽取，成本高且难以揭示模型在动态叙事中的推理过程，因此需要一个诊断性基准来系统性研究鲁棒性和推理能力。

Method: 构建CMT-Bench，基于实时板球解说数据在两种演化的表结构下进行动态表格生成，遵循严格的规则；通过三类语义保持维度进行鲁棒性探测：1) 提取线索消融以区分提取式捷径与状态跟踪，2) 时间前缀化以测试长上下文稳定性，3) 实体形式扰动（匿名化、分布外替换、角色混乱改写）以评估对表面变化的敏感性。

Result: 在多种长上下文的最先进LLMs中，若不使用 extractive summaries，性能出现显著下降；输入长度增加呈单调退化；实体-form变化导致准确率持续下降；分布性测试显示数值误差模式显著改变，指向推理过程的漂移而非仅是噪声。

Conclusion: 当前LLMs在动态文本到表格生成任务上较为脆弱，需优先进行鲁棒性评估，作为发展高效、可扩展方法的前提。

Abstract: LLM Driven text-to-table (T2T) systems often rely on extensive
prompt-engineering or iterative event extraction in code-parsable formats,
which boosts scores but are computationally expensive and obscure how models
actually reason over temporal evolving narratives to summarise key information.
We present CMT-Bench, a diagnostic benchmark built from live cricket commentary
that requires dynamic table generation across two evolving schemas under a
dense, rule-governed policy. CMT-Bench is designed to probe robustness via
three semantics-preserving dimensions: (i) extractive-cue ablation to separate
extractive shortcuts from state tracking, (ii) temporal prefixing to test
long-context stability, and (iii) entity-form perturbations (anonymization,
outof-distribution substitutions, role-entangling paraphrases) to assess
sensitivity to surface variation. Across diverse long-context stateof-the-art
LLMs, we find large drops without extractive summaries, monotonic degradation
with input length, and consistent accuracy drop under entity-form changes.
Complementary distributional tests confirm significant shifts in numeric error
patterns, indicating drift in reasoning rather than mere noise. Our results
show that current LLMs are brittle in dynamic Textto-table generation,
motivating robustness-first evaluation as a prerequisite for developing
efficient and scalable approaches for this task.

</details>


### [20] [Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge](https://arxiv.org/abs/2510.18196)
*Yoshinari Fujinuma*

Main category: cs.CL

TL;DR: 对LLMs作为直接评估者时，评分区间偏置导致结果不稳定；通过对比解码缓解偏置，提升与人类判断的Spearman相关性（平均约11.3%）


<details>
  <summary>Details</summary>
Motivation: 解决无参考直接评分中，LLM输出对预设分数区间高度敏感，影响评估可靠性的问题

Method: 使用对比解码（contrastive decoding）来降低评分区间偏置，且在不同预设分数区间进行评估，分析偏置是否存在于同族模型

Result: 在多组评分区间下，Spearman相关性相对于人类判断提升约11.3%（相对提升），并发现同族模型也存在类似偏置

Conclusion: 对比解码可以有效缓解LLM在直接评分中的区间偏置，提升无参考评估的可靠性；偏置在同族模型中也应被关注

Abstract: Large Language Models (LLMs) are commonly used as evaluators in various
applications, but the reliability of the outcomes remains a challenge. One such
challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores
from a specified range without any references. We first show that this
challenge stems from LLM judge outputs being associated with score range bias,
i.e., LLM judge outputs are highly sensitive to pre-defined score ranges,
preventing the search for optimal score ranges. We also show that similar
biases exist among models from the same family. We then mitigate this bias
through contrastive decoding, achieving up to 11.3% relative improvement on
average in Spearman correlation with human judgments across different score
ranges.

</details>


### [21] [MARCUS: An Event-Centric NLP Pipeline that generates Character Arcs from Narratives](https://arxiv.org/abs/2510.18201)
*Sriharsh Bhyravajjula,Ujwal Narayan,Manish Shrivastava*

Main category: cs.CL

TL;DR: 提出 MARCUS，构建一个将叙事中的事件、人物、情感和情绪转化为角色弧线的图形化管线，并在《哈利波特》与《魔戒》两大系列中进行初步应用与评估。


<details>
  <summary>Details</summary>
Motivation: 为文学研究中的角色弧线这一理论概念提供可量化、可操作的实现，便于跨文本对比、关系建模与后续应用（如情感分析、叙事结构分析）。

Method: 提出 MARCUS（建模理解故事的弧线）的NLP管线：从叙事中抽取事件、参与角色、隐含情感与情感倾向，建立角色间关系模型；对关系进行跟踪和聚合，生成以图形形式呈现的角色弧线。并在两部扩展版的奇幻系列《哈利波特》和《魔戒》上生成弧线。

Result: 给出可行的管线实现与生成的角色弧线示例，进行了初步评估，讨论了现有挑战、潜在应用及未来工作方向。

Conclusion: 提出了一种可量化的、图形化的角色弧线建模框架（MARCUS），并指明了该方向的挑战与未来研究路径。

Abstract: Character arcs are important theoretical devices employed in literary studies
to understand character journeys, identify tropes across literary genres, and
establish similarities between narratives. This work addresses the novel task
of computationally generating event-centric, relation-based character arcs from
narratives. Providing a quantitative representation for arcs brings tangibility
to a theoretical concept and paves the way for subsequent applications. We
present MARCUS (Modelling Arcs for Understanding Stories), an NLP pipeline that
extracts events, participant characters, implied emotion, and sentiment to
model inter-character relations. MARCUS tracks and aggregates these relations
across the narrative to generate character arcs as graphical plots. We generate
character arcs from two extended fantasy series, Harry Potter and Lord of the
Rings. We evaluate our approach before outlining existing challenges,
suggesting applications of our pipeline, and discussing future work.

</details>


### [22] [DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization](https://arxiv.org/abs/2510.18257)
*Tao Tao,Guanghui Zhu,Lang Guo,Hongyi Chen,Chunfeng Yuan,Yihua Huang*

Main category: cs.CL

TL;DR: DelvePO是一种方向引导的自我进化提示优化框架，通过对提示进行组件解耦并引入工作记忆，实现稳定且具迁移性的优化，优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化多依赖LLM的随机改写，且常只关注个别影响因素，易陷入局部最优且结果不稳定，亟需一个任务无关、具自我进化能力且具迁移性的优化框架。

Method: 提出DelvePO框架，解耦提示为可分析的组件以探索不同因素对多任务的影响；引入工作记忆以缓解模型不确定性并指导新提示的生成；在开放与闭源LLM上进行大规模任务覆盖的实验，与多种基线对比，评估稳定性与迁移性。

Result: 在相同实验设置下，DelvePO持续优于现有SOTA方法，证明其有效性并在跨任务的迁移性方面具有优势。

Conclusion: 本工作给出了一种任务无关、方向引导的自我进化提示优化框架，结合工作记忆提升稳定性与迁移性，显著优于基线。

Abstract: Prompt Optimization has emerged as a crucial approach due to its capabilities
in steering Large Language Models to solve various tasks. However, current
works mainly rely on the random rewriting ability of LLMs, and the optimization
process generally focus on specific influencing factors, which makes it easy to
fall into local optimum. Besides, the performance of the optimized prompt is
often unstable, which limits its transferability in different tasks. To address
the above challenges, we propose $\textbf{DelvePO}$
($\textbf{D}$irection-Guid$\textbf{e}$d Se$\textbf{l}$f-E$\textbf{v}$olving
Framework for Fl$\textbf{e}$xible $\textbf{P}$rompt $\textbf{O}$ptimization), a
task-agnostic framework to optimize prompts in self-evolve manner. In our
framework, we decouple prompts into different components that can be used to
explore the impact that different factors may have on various tasks. On this
basis, we introduce working memory, through which LLMs can alleviate the
deficiencies caused by their own uncertainties and further obtain key insights
to guide the generation of new prompts. Extensive experiments conducted on
different tasks covering various domains for both open- and closed-source LLMs,
including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini.
Experimental results show that DelvePO consistently outperforms previous SOTA
methods under identical experimental settings, demonstrating its effectiveness
and transferability across different tasks.

</details>


### [23] [Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs](https://arxiv.org/abs/2510.18279)
*Yanhong Li,Zixuan Lan,Jiawei Zhou*

Main category: cs.CL

TL;DR: 将长文本输入渲染为单张图片再输入到解码器型大语言模型中，以实现文本输入的有效压缩；在RULER和CNN/DailyMail上实现了显著的token节省（通常约一半）且不显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 解决解码型LLM的输入token成本和上下文长度限制问题，探索视觉文本表示是否能在不损失语义的前提下压缩文本输入。

Method: 将长文本渲染为单张图像并直接输入模型；在RULER（长上下文检索）与CNN/DailyMail（文档摘要）两项基准上评估token节省和任务性能。

Result: 实现了显著的token节省，通常接近一半，同时在两项基准上的任务性能未见明显下降。

Conclusion: 视觉文本表示是对解码器LLMs进行输入压缩的可行且有效的方法；为扩大输入长度、降低成本提供新途径，但需进一步研究对不同任务、模型架构的适用性和潜在限制。

Abstract: Large language models (LLMs) and their multimodal variants can now process
visual inputs, including images of text. This raises an intriguing question:
can we compress textual inputs by feeding them as images to reduce token usage
while preserving performance? In this paper, we show that visual text
representations are a practical and surprisingly effective form of input
compression for decoder LLMs. We exploit the idea of rendering long text inputs
as a single image and provide it directly to the model. This leads to
dramatically reduced number of decoder tokens required, offering a new form of
input compression. Through experiments on two distinct benchmarks RULER
(long-context retrieval) and CNN/DailyMail (document summarization) we
demonstrate that this text-as-image method yields substantial token savings
(often nearly half) without degrading task performance.

</details>


### [24] [BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks](https://arxiv.org/abs/2510.18288)
*Tianyuan Huang,Zepeng Zhu,Hangdi Xing,Zirui Shao,Zhi Yu,Chaoxiong Yang,Jiaxian He,Xiaozhong Liu,Jiajun Bu*

Main category: cs.CL

TL;DR: 提出英中Braille混合数据集 EBMD/CBMD、基于语法树的数据增强、Braille知识驱动微调 BKFT，打造统一的 Braille 翻译和公式转Braille/混合文本翻译的LLM，实验显示BKFT优于传统微调，开放数据集促进低资源多语 Braille 研究。


<details>
  <summary>Details</summary>
Motivation:  Braille 信息处理受数据稀缺和混合文本歧义影响，缺乏统一、可扩展的多语 Braille 理解和翻译能力。

Method: 构建 EBMD/CBMD 数据集；提出基于语法树的 Braille 数据增强；提出 Braille Knowledge-Based Fine-Tuning (BKFT) 以减轻 Braille 上下文特征学习难度；通过指令式微调实现统一任务（翻译、公式转Braille、混合文本翻译）— BrailleLLM。

Result: BKFT 在 Braille 翻译等场景显著优于传统微调；开放数据集与方法为低资源多语言 Braille 研究奠定基础。

Conclusion: 提出的 BKFT 框架和开放数据集解决了 Braille 领域的资源不足与模型适应问题，推动对 Braille 的统一处理能力的发展。

Abstract: Braille plays a vital role in education and information accessibility for
visually impaired individuals. However, Braille information processing faces
challenges such as data scarcity and ambiguities in mixed-text contexts. We
construct English and Chinese Braille Mixed Datasets (EBMD/CBMD) with
mathematical formulas to support diverse Braille domain research, and propose a
syntax tree-based augmentation method tailored for Braille data. To address the
underperformance of traditional fine-tuning methods in Braille-related tasks,
we investigate Braille Knowledge-Based Fine-Tuning (BKFT), which reduces the
learning difficulty of Braille contextual features. BrailleLLM employs BKFT via
instruction tuning to achieve unified Braille translation, formula-to-Braille
conversion, and mixed-text translation. Experiments demonstrate that BKFT
achieves significant performance improvements over conventional fine-tuning in
Braille translation scenarios. Our open-sourced datasets and methodologies
establish a foundation for low-resource multilingual Braille research.

</details>


### [25] [Food4All: A Multi-Agent Framework for Real-time Free Food Discovery with Integrated Nutritional Metadata](https://arxiv.org/abs/2510.18289)
*Zhengqing Yuan,Yiyang Li,Weixiang Sun,Zheyuan Zhang,Kaiwen Shi,Keerthiram Murugesan,Yanfang Ye*

Main category: cs.CL

TL;DR: 提出Food4All，一种面向免费食品检索的多智能体框架，通过跨数据源聚合、轻量级强化学习和在线反馈，实现实时、情境感知的食品获取指导。


<details>
  <summary>Details</summary>
Motivation: 美国食品不安全问题持续并与慢性病、精神疾病和药物滥用等问题交织；现有检索系统、LLM对话与食品推荐在可及性、个性化和情境适应性方面存在明显不足，需提供能在时间、出行与现实约束下的精准资源信息与决策支持。

Method: 提出三大创新：1) 异构数据聚合，整合官方数据库、社区平台与社交媒体，形成持续更新的食品资源池；2) 轻量化强化学习算法，基于精选案例训练，优化地理可及性与营养正确性；3) 在线反馈回路，动态适应用户需求演变并调整检索策略。

Result: 给出一个框架性的方法论，强调信息获取、语义分析与决策支持的整合，能够在需求点提供带营养注释的资源指导，推动可扩展、更加公平的智能检索系统；但未提供具体实验数据。

Conclusion: Food4All代表朝着可扩展、可及、以用户为中心的免费食品检索系统迈出的关键一步，旨在直接支持处于无家可归、成瘾或数字鸿沟等脆弱人群的即时资源获取与健康风险降低。

Abstract: Food insecurity remains a persistent public health emergency in the United
States, tightly interwoven with chronic disease, mental illness, and opioid
misuse. Yet despite the existence of thousands of food banks and pantries,
access remains fragmented: 1) current retrieval systems depend on static
directories or generic search engines, which provide incomplete and
geographically irrelevant results; 2) LLM-based chatbots offer only vague
nutritional suggestions and fail to adapt to real-world constraints such as
time, mobility, and transportation; and 3) existing food recommendation systems
optimize for culinary diversity but overlook survival-critical needs of
food-insecure populations, including immediate proximity, verified
availability, and contextual barriers. These limitations risk leaving the most
vulnerable individuals, those experiencing homelessness, addiction, or digital
illiteracy, unable to access urgently needed resources. To address this, we
introduce Food4All, the first multi-agent framework explicitly designed for
real-time, context-aware free food retrieval. Food4All unifies three
innovations: 1) heterogeneous data aggregation across official databases,
community platforms, and social media to provide a continuously updated pool of
food resources; 2) a lightweight reinforcement learning algorithm trained on
curated cases to optimize for both geographic accessibility and nutritional
correctness; and 3) an online feedback loop that dynamically adapts retrieval
policies to evolving user needs. By bridging information acquisition, semantic
analysis, and decision support, Food4All delivers nutritionally annotated and
guidance at the point of need. This framework establishes an urgent step toward
scalable, equitable, and intelligent systems that directly support populations
facing food insecurity and its compounding health risks.

</details>


### [26] [From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering](https://arxiv.org/abs/2510.18297)
*Lei Li,Xiao Zhou,Yingying Zhang,Xian Wu*

Main category: cs.CL

TL;DR: 提出 MedRGAG，将外部检索与参数知识生成整合，提升医学问答的可靠性与覆盖面。


<details>
  <summary>Details</summary>
Motivation: 解决RAG中的检索噪声/不完整及GAG的幻觉性问题，通过统一框架提升知识密集型推理性能。

Method: 引入 KGCC 和 KADS 两个模块，前者引导生成补充背景文献，后者自适应选择检索与生成的文档组合，形成证据用于回答生成。

Result: 在五个医学QA基准上相较 MedRAG 提升 12.5%，相较 MedGENIE 提升 4.5%。

Conclusion: 统一检索与生成可提升知识密集型推理的可靠性与效果，代码/数据公开。

Abstract: Medical question answering (QA) requires extensive access to domain-specific
knowledge. A promising direction is to enhance large language models (LLMs)
with external knowledge retrieved from medical corpora or parametric knowledge
stored in model parameters. Existing approaches typically fall into two
categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning
on externally retrieved evidence, and Generation-Augmented Generation (GAG),
which depends solely on the models internal knowledge to generate contextual
documents. However, RAG often suffers from noisy or incomplete retrieval, while
GAG is vulnerable to hallucinated or inaccurate information due to
unconstrained generation. Both issues can mislead reasoning and undermine
answer reliability. To address these challenges, we propose MedRGAG, a unified
retrieval-generation augmented framework that seamlessly integrates external
and parametric knowledge for medical QA. MedRGAG comprises two key modules:
Knowledge-Guided Context Completion (KGCC), which directs the generator to
produce background documents that complement the missing knowledge revealed by
retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively
selects an optimal combination of retrieved and generated documents to form
concise yet comprehensive evidence for answer generation. Extensive experiments
on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5%
improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the
effectiveness of unifying retrieval and generation for knowledge-intensive
reasoning. Our code and data are publicly available at
https://anonymous.4open.science/r/MedRGAG

</details>


### [27] [ECG-LLM-- training and evaluation of domain-specific large language models for electrocardiography](https://arxiv.org/abs/2510.18339)
*Lara Ahrens,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.CL

TL;DR: Domain-adapted, locally deployable LLMs via finetuning and RAG can achieve competitive performance in electrocardiography against general-purpose models, with heterogeneous evaluation outcomes across metrics.


<details>
  <summary>Details</summary>
Motivation: Address how to optimally adapt LLMs for domain-specific healthcare tasks and how evaluation methods influence perceived performance, especially under privacy constraints.

Method: Fine-tuned open-weight models on domain literature; built a multi-layer evaluation framework; compared finetuned Llama 3.1 70B, RAG, and Claude Sonnet 3.7 on electrocardiography tasks; used multiple evaluation modes including multiple-choice, automatic metrics, LLM-as-a-judge, and human evaluation.

Result: Finetuned Llama 3.1 70B outperformed base Llama 3.1 across most metrics, some metrics second to Claude 3.7; Claude 3.7 and RAG favored for complex queries; heterogeneity across evaluation methods; domain adaptation with finetuning and RAG yields competitive performance with proprietary models.

Conclusion: Domain-specific finetuning and retrieval-augmented approaches enable privacy-preserving, locally deployable clinical LLM solutions with competitive performance relative to proprietary models, though evaluation is complex and varies by method.

Abstract: Domain-adapted open-weight large language models (LLMs) offer promising
healthcare applications, from queryable knowledge bases to multimodal
assistants, with the crucial advantage of local deployment for privacy
preservation. However, optimal adaptation strategies, evaluation methodologies,
and performance relative to general-purpose LLMs remain poorly characterized.
We investigated these questions in electrocardiography, an important area of
cardiovascular medicine, by finetuning open-weight models on domain-specific
literature and implementing a multi-layered evaluation framework comparing
finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7
as a representative general-purpose model. Finetuned Llama 3.1 70B achieved
superior performance on multiple-choice evaluations and automatic text metrics,
ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert
evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned
models significantly outperformed their base counterparts across nearly all
evaluation modes. Our findings reveal substantial performance heterogeneity
across evaluation methodologies, underscoring assessment complexity.
Nevertheless, domain-specific adaptation through finetuning and RAG achieves
competitive performance with proprietary models, supporting the viability of
privacy-preserving, locally deployable clinical solutions.

</details>


### [28] [Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction](https://arxiv.org/abs/2510.18344)
*Vipul Rathore,Malik Hammad Faisal,Parag Singla,Mausam*

Main category: cs.CL

TL;DR: HYDRE introduces a hybrid DSRE framework that uses a trained DSRE model to propose top-k relations for a sentence and a dynamic exemplar retrieval strategy to form sentence-level exemplars for LLM prompting, extended to cross-lingual low-resource languages; shows significant F1 gains.


<details>
  <summary>Details</summary>
Motivation: DSRE suffers from noisy bag-level labels and noisy relation semantics when using LLMs. The goal is to combine DSRE signals with in-context learning via reliable exemplars to improve sentence-level predictions and enable cross-lingual transfer.

Method: Train a DSRE model to rank top-k candidate relations for a test sentence; dynamically retrieve reliable sentence-level exemplars from training data; prompt an LLM with the exemplars and candidate relations to output final relation(s). Extend evaluation to English and four Indic languages (Oriya, Santali, Manipuri, Tulu) using English DSRE data.

Result: English: up to 20 F1 point gains over prior SoTA DSRE models; Indic languages: average ~17 F1 gains. Ablations show HYDRE's efficacy over other prompting strategies.

Conclusion: HYDRE effectively combines DSRE with LLM prompting through exemplar-based prompts, yielding strong improvements in English and cross-lingual, low-resource settings; exemplar retrieval is key to mitigating noise and guiding the LLM.

Abstract: Distantly Supervised Relation Extraction (DSRE) remains a long-standing
challenge in NLP, where models must learn from noisy bag-level annotations
while making sentence-level predictions. While existing state-of-the-art (SoTA)
DSRE models rely on task-specific training, their integration with in-context
learning (ICL) using large language models (LLMs) remains underexplored. A key
challenge is that the LLM may not learn relation semantics correctly, due to
noisy annotation.
  In response, we propose HYDRE -- HYbrid Distantly Supervised Relation
Extraction framework. It first uses a trained DSRE model to identify the top-k
candidate relations for a given test sentence, then uses a novel dynamic
exemplar retrieval strategy that extracts reliable, sentence-level exemplars
from training data, which are then provided in LLM prompt for outputting the
final relation(s).
  We further extend HYDRE to cross-lingual settings for RE in low-resource
languages. Using available English DSRE training data, we evaluate all methods
on English as well as a newly curated benchmark covering four diverse
low-resource Indic languages -- Oriya, Santali, Manipuri, and Tulu. HYDRE
achieves up to 20 F1 point gains in English and, on average, 17 F1 points on
Indic languages over prior SoTA DSRE models. Detailed ablations exhibit HYDRE's
efficacy compared to other prompting strategies.

</details>


### [29] [KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs](https://arxiv.org/abs/2510.18368)
*Donghyeon Ko,Yeguk Jin,Kyubyung Chae,Byungwook Lee,Chansong Jo,Sookyo In,Jaehong Lee,Taesup Kim,Donghyun Kwak*

Main category: cs.CL

TL;DR: KoSimpleQA 是一个面向韩语文化知识的 1000 道简短事实性问答基准，用于评估大语言模型的 factuality。即使是最强模型也只有约 33.7% 的正确率，表明任务具有挑战性。与英文 SimpleQA 的排名差异显著，强调了数据集的独特性。研究还发现，通过推理能力可以同时提高模型挖掘潜在知识和在不确定时更好地回避回答的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的事实性 QA 基准多以英语为主，难以覆盖韩语和韩语文化知识的特定性。需要一个对韩语能力和文化知识的系统性评估工具，以揭示模型在这一语言环境中的能力极限、偏差以及对不确定情形的自我拒绝能力。

Method: 构建包含 1000 道简短、可明确答案的韩语问答题目，聚焦事实性知识与文化相关信息。对多样化且支持韩语的开源大语言模型进行评测，涵盖不同规模。通过对比分析，考察模型在推理能力驱动下的表现变化，尤其是正确性与在不确定时的拒答能力。

Result: 最强模型的正确率仅为 33.7%，凸显数据集的挑战性。KoSimpleQA 的性能排序与英文 SimpleQA 存在显著差异，显示跨语言数据集存在不同的模型优势。对推理能力的分析表明，善用推理可以提升模型挖掘潜在知识的效率，并增强在不确定时的拒答能力。

Conclusion: KoSimpleQA 为评测韩国语境下的事实性知识与推理能力提供了有价值的基准，揭示了当前多语言/跨文化模型在韩语领域的局限性与潜在改进方向。该数据集对研究者理解韩语知识的获取、模型校准和自信拒答行为具有重要意义。

Abstract: We present $\textbf{Korean SimpleQA (KoSimpleQA)}$, a benchmark for
evaluating factuality in large language models (LLMs) with a focus on Korean
cultural knowledge. KoSimpleQA is designed to be challenging yet easy to grade,
consisting of 1,000 short, fact-seeking questions with unambiguous answers. We
conduct a comprehensive evaluation across a diverse set of open-source LLMs of
varying sizes that support Korean, and find that even the strongest model
generates correct answer only 33.7% of the time, underscoring the challenging
nature of KoSimpleQA. Notably, performance rankings on KoSimpleQA differ
substantially from those on the English SimpleQA, highlighting the unique value
of our dataset. Furthermore, our analysis of reasoning LLMs shows that engaging
reasoning capabilities in the factual QA task can both help models better
elicit their latent knowledge and improve their ability to abstain when
uncertain. KoSimpleQA can be found at
https://anonymous.4open.science/r/KoSimpleQA-62EB.

</details>


### [30] [Towards Fair ASR For Second Language Speakers Using Fairness Prompted Finetuning](https://arxiv.org/abs/2510.18374)
*Monorama Swain,Bubai Maji,Jagabandhu Mishra,Markus Schedl,Anders Søgaard,Jesper Rindom Jensen*

Main category: cs.CL

TL;DR: 通过公平性提示微调结合 SD、Group-DRO、IRM 来提升英语 ASR 对多口音的公平性，同时保持或提升总识别准确率；在 Whisper 与 Seamless-M4T 上实现显著的 WER 改善。


<details>
  <summary>Details</summary>
Motivation: 现有英语 ASR 对不同口音的鲁棒性差，导致显著的公平性差距。对 26 个口音组的 WER 波动显示需要新的训练策略。

Method: 在轻量级适配器上进行公平性提示微调，结合 Spectral Decoupling、Group-DRO、Invariant Risk Minimization，并将传统 ERM 与交叉熵损失以及公平目标相融合。

Result: 相对于大型预训练 Whisper 和 Seamless-M4T，宏观平均 WER 的相对改进分别为 58.7% 和 58.5%；相较于仅用 ERM-Cross-Entropy 微调，提升为 9.7% 和 7.8%。

Conclusion: 将公平性目标融入微调流程可显著提升跨口音公平性并维持总体识别性能，适用于现有大模型的公平性提升。

Abstract: In this work, we address the challenge of building fair English ASR systems
for second-language speakers. Our analysis of widely used ASR models, Whisper
and Seamless-M4T, reveals large fluctuations in word error rate (WER) across 26
accent groups, indicating significant fairness gaps. To mitigate this, we
propose fairness-prompted finetuning with lightweight adapters, incorporating
Spectral Decoupling (SD), Group Distributionally Robust Optimization
(Group-DRO), and Invariant Risk Minimization (IRM). Our proposed fusion of
traditional empirical risk minimization (ERM) with cross-entropy and
fairness-driven objectives (SD, Group DRO, and IRM) enhances fairness across
accent groups while maintaining overall recognition accuracy. In terms of
macro-averaged word error rate, our approach achieves a relative improvement of
58.7% and 58.5% over the large pretrained Whisper and SeamlessM4T, and 9.7% and
7.8% over them, finetuning with standard empirical risk minimization with
cross-entropy loss.

</details>


### [31] [MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models](https://arxiv.org/abs/2510.18383)
*ChangSu Choi,Hoyun Song,Dongyeon Kim,WooHyeon Jung,Minkyung Cho,Sunjin Park,NohHyeob Bae,Seona Yu,KyungTae Lim*

Main category: cs.CL

TL;DR: MENTOR框架通过 RL 与教师引导蒸馏的结合，构建密集奖励信号以提升小语言模型在工具使用任务中的跨域泛化和策略能力，优于SFT和稀疏奖励RL。


<details>
  <summary>Details</summary>
Motivation: SFT对教师轨迹的模仿导致泛化能力不足；稀疏奖励的RL难以引导SLMs进行有效探索；需要一种能提供密集、细粒度指导的训练方法来提升泛化与策略水平。

Method: 在蒸馏基础上引入RL，使用教师参考轨迹构造密集、复合的奖励信号，并通过RL学习更具一般化的策略，同时借助教师轨迹实现对任务方法论的引导。

Result: 实验表明，MENTOR在跨领域泛化和策略能力方面显著优于SFT和标准稀疏奖励RL。

Conclusion: 将RL与教师引导蒸馏结合有助于缓解奖励稀疏和泛化不足的问题，从而提升小型语言模型的工具使用能力，具备较强的通用性。

Abstract: Distilling the tool-using capabilities of large language models (LLMs) into
smaller, more efficient small language models (SLMs) is a key challenge for
their practical application. The predominant approach, supervised fine-tuning
(SFT), suffers from poor generalization as it trains models to imitate a static
set of teacher trajectories rather than learn a robust methodology. While
reinforcement learning (RL) offers an alternative, the standard RL using sparse
rewards fails to effectively guide SLMs, causing them to struggle with
inefficient exploration and adopt suboptimal strategies. To address these
distinct challenges, we propose MENTOR, a framework that synergistically
combines RL with teacher-guided distillation. Instead of simple imitation,
MENTOR employs an RL-based process to learn a more generalizable policy through
exploration. In addition, to solve the problem of reward sparsity, it uses a
teacher's reference trajectory to construct a dense, composite teacher-guided
reward that provides fine-grained guidance. Extensive experiments demonstrate
that MENTOR significantly improves the cross-domain generalization and
strategic competence of SLMs compared to both SFT and standard sparse-reward RL
baselines.

</details>


### [32] [Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2510.18413)
*Siyuan Yan,Guo-Qing Jiang,Yuchen Zhang,Xiaoxing Ma,Ran Zhu,Chun Cao,Jingwei Xu*

Main category: cs.CL

TL;DR: Adamas 是一种轻量级的稀疏注意力机制，通过 Hadamard 变换、桶化与 2 位压缩来产生紧凑表示，并利用曼哈顿距离估计来高效进行 top-k 选择，实现对长序列的高效自注意力。与全注意力相比，在较小预算下保持接近或相同的精度并实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文中的自注意力二次复杂度带来的高延迟问题。现有稀疏注意力多依赖启发式模式，难以稳定回忆关键的键值对，导致精度下降，因此需要更可靠、低成本的长上下文稀疏注意力方法。

Method: 提出 Adamos：对自注意力进行稀疏化，先通过 Hadamard 变换得到紧凑表示，再进行桶化与 2 位压缩以降低存储与计算开销；使用曼哈顿距离估计实现高效的 top-k 选择，从而在长序列上实现高稀疏度下的精度保留。

Result: 在实验中，Adamas 在仅使用 64-token 的预算时即可达到与全注意力相同的精度；在 128 token 下接近无损；相较于现有 SOTA 方法，稀疏度提升最多 8x；自注意力阶段可达 4.4x 的加速、端到端可达 1.5x，在 32K 长序列上表现显著； perplexity 与甚至低于全注意力，显示在高稀疏度下仍具备良好准确性。

Conclusion: Adamas 为长上下文推理提供了高效、准确的稀疏注意力解决方案，能够在显著提高速度的同时维持甚至提升语言模型的表现，适用于大规模长文本处理场景。

Abstract: Large language models (LLMs) now support context windows of hundreds of
thousands to millions of tokens, enabling applications such as long-document
summarization, large-scale code synthesis, multi-document question answering
and persistent multi-turn dialogue. However, such extended contexts exacerbate
the quadratic cost of self-attention, leading to severe latency in
autoregressive decoding. Existing sparse attention methods alleviate these
costs but rely on heuristic patterns that struggle to recall critical key-value
(KV) pairs for each query, resulting in accuracy degradation. We introduce
Adamas, a lightweight yet highly accurate sparse attention mechanism designed
for long-context inference. Adamas applies the Hadamard transform,
bucketization and 2-bit compression to produce compact representations, and
leverages Manhattan-distance estimation for efficient top-k selections.
Experiments show that Adamas matches the accuracy of full attention with only a
64-token budget, achieves near-lossless performance at 128, and supports up to
8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering
up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.
Remarkably, Adamas attains comparable or even lower perplexity than full
attention, underscoring its effectiveness in maintaining accuracy under
aggressive sparsity.

</details>


### [33] [Chain-of-Conceptual-Thought: Eliciting the Agent to Deeply Think within the Response](https://arxiv.org/abs/2510.18434)
*Qingqing Gu,Dan Wang,Yue Zhao,Xiaoyu Wang,Zhonglin Jiang,Yong Chen,Hongyan Li,Luo Ji*

Main category: cs.CL

TL;DR: CoCT通过先对概念进行标注再生成细节文本的链式推理，在开放域任务中提升推理效果，优于多项基线，适用于情感与日常对话等场景。


<details>
  <summary>Details</summary>
Motivation: 解决传统Chain-of-Thought在开放域任务中缺乏明确推理步骤的问题，提出通过概念标注来引导思维的策略性推理的新范式。

Method: 在 prompts 中让模型先在话语中标注一个概念（情感、策略、话题），随后生成详细内容；在日常及情感支持对话中进行实验；使用自动、人工和模型评估。

Result: 实验结果显示CoCT在性能上优于Self-Refine、ECoT、ToT、SoT、RAG等基线。

Conclusion: CoCT作为一种有效的基于提示的推理范式，具有在更广泛任务中的应用潜力，值得未来进一步探索与扩展。

Abstract: Chain-of-Thought (CoT) is widely applied to improve the LLM capability in
math, coding and reasoning tasks. However, its performance is limited for
open-domain tasks since there are no clearly defined reasoning steps or logical
transitions. To mitigate such challenges, we propose another prompt-based
paradigm called Chain of Conceptual Thought (CoCT), where the LLM first tags a
concept, then generates the detailed content. The chain of concepts is allowed
within the utterance, encouraging the LLM's deep and strategic thinking. We
experiment with this paradigm in daily and emotional support conversations
where the concept is comprised of emotions, strategies and topics. Automatic,
human and model evaluations suggest that CoCT surpasses baselines such as
Self-Refine, ECoT, ToT, SoT and RAG, suggesting a potential effective
prompt-based paradigm of LLM for a wider scope of tasks.

</details>


### [34] [Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation](https://arxiv.org/abs/2510.18439)
*Yasser Hamidullah,Koel Dutta Chowdury,Yusser Al-Ghussin,Shakib Yazdani,Cennet Oguz,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 提出了一种用于符号语言翻译的 token-level 可靠性度量，将特征敏感性和反事实信号结合并聚合为句子级可靠性分数，用于诊断和预测幻觉，并在与文本信号结合时提升风险估算。


<details>
  <summary>Details</summary>
Motivation: 在SLT中，幻觉产生于模型过度依赖语言先验而非视觉输入，且 gloss-free 模型更易产生幻觉；需要一个可解释且可重复使用的 grounding 测量工具。

Method: 通过对视频进行掩码，测量内部表征随视频变化的特征敏感性；通过在干净与修改后的视频输入之间比较概率，得到反事实信号；将两种信号聚合成一个句子级的可靠性分数，作为视觉 grounding 的量化指标；在 PHOENIX-2014T 与 CSL-Daily 的 gloss-based 与 gloss-free 模型上评估。

Result: 可靠性分数能预测幻觉发生率，具备跨数据集和架构的泛化性，并在视觉降级下下降；能够区分 grounding token 与 guessed token；与文本信号（如置信度、困惑度、熵）结合时，提升幻觉风险估算；对 gloss-free 模型的幻觉风险具有更明显的诊断效果。

Conclusion: 提出的可靠性指标为诊断 SLT 并检测多模态生成中的幻觉提供了一个实用、可重复使用的工具，为更鲁棒的幻觉检测奠定基础。

Abstract: Hallucination, where models generate fluent text unsupported by visual
evidence, remains a major flaw in vision-language models and is particularly
critical in sign language translation (SLT). In SLT, meaning depends on precise
grounding in video, and gloss-free models are especially vulnerable because
they map continuous signer movements directly into natural language without
intermediate gloss supervision that serves as alignment. We argue that
hallucinations arise when models rely on language priors rather than visual
input. To capture this, we propose a token-level reliability measure that
quantifies how much the decoder uses visual information. Our method combines
feature-based sensitivity, which measures internal changes when video is
masked, with counterfactual signals, which capture probability differences
between clean and altered video inputs. These signals are aggregated into a
sentence-level reliability score, providing a compact and interpretable measure
of visual grounding. We evaluate the proposed measure on two SLT benchmarks
(PHOENIX-2014T and CSL-Daily) with both gloss-based and gloss-free models. Our
results show that reliability predicts hallucination rates, generalizes across
datasets and architectures, and decreases under visual degradations. Beyond
these quantitative trends, we also find that reliability distinguishes grounded
tokens from guessed ones, allowing risk estimation without references; when
combined with text-based signals (confidence, perplexity, or entropy), it
further improves hallucination risk estimation. Qualitative analysis highlights
why gloss-free models are more susceptible to hallucinations. Taken together,
our findings establish reliability as a practical and reusable tool for
diagnosing hallucinations in SLT, and lay the groundwork for more robust
hallucination detection in multimodal generation.

</details>


### [35] [Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models](https://arxiv.org/abs/2510.18454)
*Atharvan Dogra,Soumya Suvra Ghosal,Ameet Deshpande,Ashwin Kalyan,Dinesh Manocha*

Main category: cs.CL

TL;DR: 本研究系统检验了在现代LLM管线中，通过优化幽默感来提升输出的趣味性时，如何与有害内容相互作用。结果显示有害输出具有更高的幽默评分，且在角色/提示引导下会放大这一偏差；信息理论度量揭示有害线索增加了预测不确定性，某些模型甚至使有害笑点更易被预测。外部验证表明讽刺生成任务中，幽默与刻板化、毒性倾向上升；量化效果包括10-21%的幽默评分提升、11-28%的刻板化笑话比例提升，以及人类评估中最高达10%的“搞笑”生成更具刻板化与毒性。


<details>
  <summary>Details</summary>
Motivation: 在大规模语言模型日益用于创作与互动内容的背景下，安全性成为核心问题。本研究通过以幽默生成作为测试台，揭示趣味优化如何与有害内容耦合，揭示生成端与评估端之间的偏置放大循环，并尝试用信息论信号来解码其中的不一致性。

Method: 对六种模型同时进行评估，测量幽默度、刻板化、毒性，并通过基于角色的提示来观察偏差放大；引入信息理论指标分析不一致性信号；在额外的讽刺生成任务与人类感知打分上进行外部验证；量化指标包括平均幽默分、刻板化/毒性笑话的比例、以及人类评估的一致性。

Result: 结果显示：有害输出获得更高的幽默分数，且在角色提示下这一趋势进一步增强，揭示生成端與评估端之间的偏差放大循环。信息理论分析表明有害线索会扩展预测不确定性，且在某些模型中会提高有害笑点的“可预测性”；外部验证显示讽刺生成任务中，幽默化倾向伴随刻板化与毒性上升（对封闭模型也成立）。量化上，刻板/毒性笑话的平均幽默分数提升10-21%，在标记为“搞笑”的笑话中刻板化笑话出现概率提升11-28%，且人类感知的搞笑生成中，这类内容的比例提升最多达到10%。

Conclusion: 该工作揭示在幽默分布中存在对有害线索的结构性嵌入，提示需要在安全性设计中同时考量幽默评估与偏见传播的耦合；提出以幽默与有害性双维度评价的分析框架，未来需开发缓解偏差放大、降低有害线索可预测性的生成策略与评估方法。

Abstract: Large language models are increasingly used for creative writing and
engagement content, raising safety concerns about the outputs. Therefore,
casting humor generation as a testbed, this work evaluates how funniness
optimization in modern LLM pipelines couples with harmful content by jointly
measuring humor, stereotypicality, and toxicity. This is further supplemented
by analyzing incongruity signals through information-theoretic metrics. Across
six models, we observe that harmful outputs receive higher humor scores which
further increase under role-based prompting, indicating a bias amplification
loop between generators and evaluators. Information-theoretic analyses show
harmful cues widen predictive uncertainty and surprisingly, can even make
harmful punchlines more expected for some models, suggesting structural
embedding in learned humor distributions. External validation on an additional
satire-generation task with human perceived funniness judgments shows that LLM
satire increases stereotypicality and typically toxicity, including for closed
models. Quantitatively, stereotypical/toxic jokes gain $10-21\%$ in mean humor
score, stereotypical jokes appear $11\%$ to $28\%$ more often among the jokes
marked funny by LLM-based metric and up to $10\%$ more often in generations
perceived as funny by humans.

</details>


### [36] [ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks](https://arxiv.org/abs/2510.18455)
*Liyang He,Yuren Zhang,Ziwei Zhu,Zhenghui Li,Shiwei Tong*

Main category: cs.CL

TL;DR: 提出 ChronoPlay 框架，用于在游戏领域建立动态、自动化的检索增强产生（RAG）基准。通过双动态更新捕捉内容更新与玩家社区焦点的变化；并使用双源合成引擎整合官方来源与玩家社区，确保事实性与真实查询模式。已在三款游戏上实现，提供首个面向游戏的动态 RAG 基准与代码。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对动态领域（如在线游戏）的 RAG 基准，难以标准化评估。内容不断更新与玩家社区焦点变化之间存在双重动态，需自动化与以玩家真实性为导向的基准来保证生成的问题具有现实性。

Method: 实现一个 ChronoPlay 框架：具备双动态更新机制以追踪内容与社区焦点的两类变化；建立双源合成引擎，从官方来源与玩家社区获取信息，确保事实正确性与真实查询模式。以三款不同游戏为例，构建动态 RAG 基准。

Result: 首次为游戏领域提供动态 RAG 基准，揭示模型在复杂、真实条件下的性能表现，并给出可重复的代码实现。

Conclusion: ChronoPlay 为动态领域的 RAG 基准评估提供自动化、持续化的解决方案，展示了在内容更新与用户兴趣变化并存的场景中的价值与潜力。

Abstract: Retrieval Augmented Generation (RAG) systems are increasingly vital in
dynamic domains like online gaming, yet the lack of a dedicated benchmark has
impeded standardized evaluation in this area. The core difficulty lies in Dual
Dynamics: the constant interplay between game content updates and the shifting
focus of the player community. Furthermore, the necessity of automating such a
benchmark introduces a critical requirement for player-centric authenticity to
ensure generated questions are realistic. To address this integrated challenge,
we introduce ChronoPlay, a novel framework for the automated and continuous
generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update
mechanism to track both forms of change, and a dual-source synthesis engine
that draws from official sources and player community to ensure both factual
correctness and authentic query patterns. We instantiate our framework on three
distinct games to create the first dynamic RAG benchmark for the gaming domain,
offering new insights into model performance under these complex and realistic
conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.

</details>


### [37] [DePass: Unified Feature Attributing by Simple Decomposed Forward Pass](https://arxiv.org/abs/2510.18462)
*Xiangyu Hong,Che Jiang,Kai Tian,Biqing Qi,Youbang Sun,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: DePass是一种基于单次分解前向传播的特征归因框架，将隐藏状态分解为可加组件并在注意力分数与MLP激活固定的条件下传播，从而实现对Transformer内部信息流的可解释、细粒度归因。


<details>
  <summary>Details</summary>
Motivation: 解决将Transformer行为归因到内部计算的挑战，提供一个无需额外训练的、可跨 token、组件和子空间的细粒度归因工具。

Method: 将隐藏状态分解为自定义的可加分量，并在注意力权重和固定的MLP激活下进行传播；不需要额外训练，支持 token-level、模型组件-level、以及子空间-level 的归因。

Result: 在多种归因任务上验证了可行性与保真度，展示了信息在模型内部组件之间的信息流归因潜力。

Conclusion: DePass有望成为解释性研究与应用的基础工具，促进对Transformer内部机制的更广泛理解。

Abstract: Attributing the behavior of Transformer models to internal computations is a
central challenge in mechanistic interpretability. We introduce DePass, a
unified framework for feature attribution based on a single decomposed forward
pass. DePass decomposes hidden states into customized additive components, then
propagates them with attention scores and MLP's activations fixed. It achieves
faithful, fine-grained attribution without requiring auxiliary training. We
validate DePass across token-level, model component-level, and subspace-level
attribution tasks, demonstrating its effectiveness and fidelity. Our
experiments highlight its potential to attribute information flow between
arbitrary components of a Transformer model. We hope DePass serves as a
foundational tool for broader applications in interpretability.

</details>


### [38] [CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database for Language Learning](https://arxiv.org/abs/2510.18466)
*Masato Kikuchi,Masatsugu Ono,Toshioki Soga,Tetsu Tanabe,Tadachika Ozono*

Main category: cs.CL

TL;DR: 将 WordNet 与 CEFR 进行标注并结合大模型，构建面向语言学习的语义资源和分类器，取得 Macro-F1 0.81。


<details>
  <summary>Details</summary>
Motivation: 解决 WordNet 细粒度义项对二语学习者的理解困难，提供与语言能力等级相对齐的语义资源，桥接 NLP 与语言教育的差距。

Method: 使用大语言模型衡量 WordNet 义项定义与 English Vocabulary Profile Online 条目之间的语义相似度，自动将 WordNet 义项映射到 CEFR 等级；构建包含义项和 CEFR 信息的大规模语料库；在该语料库上训练上下文词汇分类器，并与金标准数据结合以提升性能。

Result: 模型在结合金标准数据后达到 Macro-F1=0.81，且性能与仅使用金标准标注的模型相当；注释的 WordNet、语料库和分类器公开发布。

Conclusion: 注释后的 WordNet 及相关资源有助于将 NLP 与语言教育连接起来，促进语言学习的更高效。

Abstract: Although WordNet is a valuable resource owing to its structured semantic
networks and extensive vocabulary, its fine-grained sense distinctions can be
challenging for second-language learners. To address this, we developed a
WordNet annotated with the Common European Framework of Reference for Languages
(CEFR), integrating its semantic networks with language-proficiency levels. We
automated this process using a large language model to measure the semantic
similarity between sense definitions in WordNet and entries in the English
Vocabulary Profile Online. To validate our method, we constructed a large-scale
corpus containing both sense and CEFR-level information from our annotated
WordNet and used it to develop contextual lexical classifiers. Our experiments
demonstrate that models fine-tuned on our corpus perform comparably to those
trained on gold-standard annotations. Furthermore, by combining our corpus with
the gold-standard data, we developed a practical classifier that achieves a
Macro-F1 score of 0.81, indicating the high accuracy of our annotations. Our
annotated WordNet, corpus, and classifiers are publicly available to help
bridge the gap between natural language processing and language education,
thereby facilitating more effective and efficient language learning.

</details>


### [39] [IMB: An Italian Medical Benchmark for Question Answering](https://arxiv.org/abs/2510.18468)
*Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 提出两个意大利语医学基准数据集 IMB-QA 和 IMB-MCQA，展示利用大语言模型进行数据清洗、保持对话风格的同时提升问答质量的可能性。结果表明领域专用微调和检索增强（RAG）在医疗问答任务上优于单纯扩展模型规模。


<details>
  <summary>Details</summary>
Motivation: 非英语（尤其是意大利语）医疗论坛的语言复杂性和非结构化对话给自动问答系统带来挑战，需要数据清洗、对话风格保留、以及高效的信息检索来提升问答效果。

Method: 构建两个意大利语医学基准：IMB-QA（782,644 条患者-医生对话，77 个医疗类别）和 IMB-MCQA（25,862 道医学专科考试选择题）。使用大语言模型进行数据清洗与风格规范化，同时保留原始含义与对话风格；在开放式问答与多选题任务上对比多种LLM架构，结合检索增强生成（RAG）与领域特定微调进行评估。

Result: 实验表明，领域专门化的适配策略和基于检索的信息获取在医疗问答任务中往往优于仅靠增大模型规模的做法。

Conclusion: 有效的医学人工智能系统应更多依赖领域专业知识与高效信息检索，而非单纯扩展模型规模。该研究提供有价值的多语言医学问答数据集与评估框架，便于未来研究。

Abstract: Online medical forums have long served as vital platforms where patients seek
professional healthcare advice, generating vast amounts of valuable knowledge.
However, the informal nature and linguistic complexity of forum interactions
pose significant challenges for automated question answering systems,
especially when dealing with non-English languages. We present two
comprehensive Italian medical benchmarks: \textbf{IMB-QA}, containing 782,644
patient-doctor conversations from 77 medical categories, and \textbf{IMB-MCQA},
comprising 25,862 multiple-choice questions from medical specialty
examinations. We demonstrate how Large Language Models (LLMs) can be leveraged
to improve the clarity and consistency of medical forum data while retaining
their original meaning and conversational style, and compare a variety of LLM
architectures on both open and multiple-choice question answering tasks. Our
experiments with Retrieval Augmented Generation (RAG) and domain-specific
fine-tuning reveal that specialized adaptation strategies can outperform
larger, general-purpose models in medical question answering tasks. These
findings suggest that effective medical AI systems may benefit more from domain
expertise and efficient information retrieval than from increased model scale.
We release both datasets and evaluation frameworks in our GitHub repository to
support further research on multilingual medical question answering:
https://github.com/PRAISELab-PicusLab/IMB.

</details>


### [40] [DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical NLP](https://arxiv.org/abs/2510.18475)
*Mariano Barone,Antonio Laudante,Giuseppe Riccio,Antonio Romano,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 提出 DART：基于意大利药监局正式文本的首个结构化药物信息语料库，用以抽取适应症、药物不良反应和药物相互作用；并通过少样本微调的大语言模型在低温解码下实现药物相互作用检测。


<details>
  <summary>Details</summary>
Motivation: 英文资源主导，缺乏面向其他医疗体系的本地化语料与工具，难以在非英语环境中进行药物监管文本的NLP应用。

Method: 建立可复现管道：网页检索、监管文本分段、将临床摘要化；使用少样本微调的大语言模型、低温解码；提取核心领域信息（适应症、ADR、DDI）；开发基于LLM的药物相互作用检查器。

Result: DART 提供结构化字段，并通过指令调优的LLM在对DART字段的约束下，能较准确推断潜在相互作用及临床意义；代码公开在 GitHub。

Conclusion: 这是面向意大利的药物监管文本NLP资源的首个结构化数据集，证明在非英语环境中构建监管文本的可行性与应用潜力，并促进跨语言的药物安全研究与临床支持。

Abstract: The extraction of pharmacological knowledge from regulatory documents has
become a key focus in biomedical natural language processing, with applications
ranging from adverse event monitoring to AI-assisted clinical decision support.
However, research in this field has predominantly relied on English-language
corpora such as DrugBank, leaving a significant gap in resources tailored to
other healthcare systems. To address this limitation, we introduce DART (Drug
Annotation from Regulatory Texts), the first structured corpus of Italian
Summaries of Product Characteristics derived from the official repository of
the Italian Medicines Agency (AIFA). The dataset was built through a
reproducible pipeline encompassing web-scale document retrieval, semantic
segmentation of regulatory sections, and clinical summarization using a
few-shot-tuned large language model with low-temperature decoding. DART
provides structured information on key pharmacological domains such as
indications, adverse drug reactions, and drug-drug interactions. To validate
its utility, we implemented an LLM-based drug interaction checker that
leverages the dataset to infer clinically meaningful interactions. Experimental
results show that instruction-tuned LLMs can accurately infer potential
interactions and their clinical implications when grounded in the structured
textual fields of DART. We publicly release our code on GitHub:
https://github.com/PRAISELab-PicusLab/DART.

</details>


### [41] [How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices](https://arxiv.org/abs/2510.18480)
*Han Peng,Peiyu Liu,Zican Dong,Daixuan Cheng,Junyi Li,Yiru Tang,Shuo Wang,Wayne Xin Zhao*

Main category: cs.CL

TL;DR: 本研究系统评估扩散语言模型（DLMs）的效率并与自回归（AR）模型对比，发现尽管DLM具有并行解码潜力，但实际吞吐量通常落后于AR；现有加速策略在小批量有益，但随批量增大收益下降；需要更健全的评估方法和更有效的加速策略来推动DLM的实用性。


<details>
  <summary>Details</summary>
Motivation: DLM理论上具并行解码优势，但在实际应用中往往速度不足，且评估方法不一致，导致研究难以复现和横向比较。本研究旨在系统揭示DLM效率瓶颈并提出改进方向。

Method: 通过大规模基准测试和基于屋顶线（roofline）的理论分析，比较DLM与AR的吞吐量，评估诸如双缓存、并行解码等加速策略，并在不同批量大小下分析其效果随规模的变化。

Result: AR模型通常实现更高的吞吐量，DLM在多数设置下落后。所考察的加速技术（如双缓存、并行解码）在小批量时提高较显著，但在扩大批量后收益减弱甚至消失。

Conclusion: 需要建立更健全、可重复的DLM评估框架，并开发更有效的加速策略，才能让DLM在实际场景中具备竞争力。

Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to
the long-dominant autoregressive (AR) paradigm, offering a parallelable
decoding process that could yield greater efficiency. Yet, in practice, current
open-source DLMs often underperform their AR counterparts in speed, limiting
their real-world utility. This work presents a systematic study of DLM
efficiency, identifying key issues in prior evaluation methods. Through
empirical benchmarking and a roofline-based theoretical analysis, we
demonstrate that AR models generally achieve higher throughput, while DLMs
consistently lag. We also investigate acceleration strategies, finding that
techniques like dual cache and parallel decoding mainly offer gains at small
batch sizes, with their benefits diminishing upon scaling. Our findings
underscore the necessity of robust evaluation methods and improved acceleration
strategies to advance research on DLMs.

</details>


### [42] [Identity-Aware Large Language Models require Cultural Reasoning](https://arxiv.org/abs/2510.18510)
*Alistair Plum,Anne-Marie Lutgen,Christoph Purschke,Achim Rettinger*

Main category: cs.CL

TL;DR: 提出将文化推理作为语言模型的基础能力，强调需要让模型根据不同文化调整输出；现有研究显示模型偏向西方 norma，评估方法单一，需发展情境化评估。


<details>
  <summary>Details</summary>
Motivation: 随着全球用户的多样性，模型输出若忽视文化差异会固化刻板印象、降低信任、伤害少数群体；需要将文化推理作为与事实准确性、语言连贯性并列的基础能力。

Method: 以概念性分析为主，回顾相关 empirical studies，提出文化推理的定义，评估现有评估手段的不足，提出初步框架与未来方向用于检测模型的情境适应能力。

Result: 综合证据表明当前模型在道德判断、成语解释、建议等方面偏向西方规范；微调仅部分缓解，现有评估多为静态准确率，无法捕捉上下文中的适应性推理；需要更多情境化数据和新型评估。

Conclusion: 将文化推理设为基础能力，并明确概念、提出初步评估方向，为未来系统提供更敏感的文化回应能力。

Abstract: Large language models have become the latest trend in natural language
processing, heavily featuring in the digital tools we use every day. However,
their replies often reflect a narrow cultural viewpoint that overlooks the
diversity of global users. This missing capability could be referred to as
cultural reasoning, which we define here as the capacity of a model to
recognise culture-specific knowledge values and social norms, and to adjust its
output so that it aligns with the expectations of individual users. Because
culture shapes interpretation, emotional resonance, and acceptable behaviour,
cultural reasoning is essential for identity-aware AI. When this capacity is
limited or absent, models can sustain stereotypes, ignore minority
perspectives, erode trust, and perpetuate hate. Recent empirical studies
strongly suggest that current models default to Western norms when judging
moral dilemmas, interpreting idioms, or offering advice, and that fine-tuning
on survey data only partly reduces this tendency. The present evaluation
methods mainly report static accuracy scores and thus fail to capture adaptive
reasoning in context. Although broader datasets can help, they cannot alone
ensure genuine cultural competence. Therefore, we argue that cultural reasoning
must be treated as a foundational capability alongside factual accuracy and
linguistic coherence. By clarifying the concept and outlining initial
directions for its assessment, a foundation is laid for future systems to be
able to respond with greater sensitivity to the complex fabric of human
culture.

</details>


### [43] [Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency](https://arxiv.org/abs/2510.18556)
*Svetlana Maslenkova,Clement Christophe,Marco AF Pimentel,Tathagata Raha,Muhammad Umar Salman,Ahmed Al Mahrooqi,Avani Gupta,Shadab Khan,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.CL

TL;DR: 本研究分析临床语言模型中的下游偏差，聚焦对阿片类药物处方在不同人口群体中的差异性倾向，并引入HC4数据集及面向医疗的评估框架，以提升公平性与安全性。


<details>
  <summary>Details</summary>
Motivation: 解决数据收集与偏差评估透明度不足的问题，推动临床AI在道德、公平与可解释性方面的信任建立；针对可能的处方偏差进行系统评估。

Method: 对临床语言模型的潜在下游偏差进行分析；提出HC4（Healthcare Comprehensive Commons Corpus）超89B标记的预训练语料库；结合既有通用基准与面向医疗的新颖评估方法，对族裔、性别、年龄等变量下的处方差异进行测量。

Result: 显示在阿片处方等情境中存在对不同人口特征的潜在偏差；HC4及医疗特定评估框架有助于更可靠地检测和量化这些偏差，提供实现公平与安全的关键洞察。

Conclusion: 强调建立全面的数据治理和偏差评估框架的必要性；HC4作为资源促进透明度、可重复性与改进；未来工作聚焦于偏差缓解策略与更广泛的医疗领域评估。

Abstract: Large language models offer transformative potential for healthcare, yet
their responsible and equitable development depends critically on a deeper
understanding of how training data characteristics influence model behavior,
including the potential for bias. Current practices in dataset curation and
bias assessment often lack the necessary transparency, creating an urgent need
for comprehensive evaluation frameworks to foster trust and guide improvements.
In this study, we present an in-depth analysis of potential downstream biases
in clinical language models, with a focus on differential opioid prescription
tendencies across diverse demographic groups, such as ethnicity, gender, and
age. As part of this investigation, we introduce HC4: Healthcare Comprehensive
Commons Corpus, a novel and extensively curated pretraining dataset exceeding
89 billion tokens. Our evaluation leverages both established general benchmarks
and a novel, healthcare-specific methodology, offering crucial insights to
support fairness and safety in clinical AI applications.

</details>


### [44] [Large language models for folktale type automation based on motifs: Cinderella case study](https://arxiv.org/abs/2510.18561)
*Tjaša Arčon,Marko Robnik-Šikonja,Polona Tratnik*

Main category: cs.CL

TL;DR: 本研究提出将人工智能方法应用于民俗学的大规模分析，通过机器学习与自然语言处理自动检测灰姑娘故事变体中的动机，并利用聚类与降维分析其相似性与差异；结果显示大型语言模型能够发现故事中的复杂交互，促进对大量文本的计算分析及跨语言对比。


<details>
  <summary>Details</summary>
Motivation: 动机在于将计算方法应用于海量民间故事文本，以系统地识别并比较不同版本中的核心动机和结构，从而揭示跨文化、跨语言的变体关系与演变规律。

Method: 建立了一套适用于大规模分析的民俗学方法学，结合机器学习与自然语言处理，自动检测动机；对 Cinderella 变体集合进行聚类与降维分析，以揭示版本之间的相似性与差异；并强调大型语言模型在揭示文本之间复杂交互中的作用，支持跨语言比较。

Result: 结果表明，大型语言模型能够捕捉故事结构中的复杂交互，使得对庞大文本集合的计算分析成为可能，并实现跨语言的对比。

Conclusion: 该工作证明了将大型语言模型等先进AI技术应用于民俗学研究的潜力，可实现对大量文本的可扩展分析和跨语言比较，为数字人文学科的研究提供新工具与思路。

Abstract: Artificial intelligence approaches are being adapted to many research areas,
including digital humanities. We built a methodology for large-scale analyses
in folkloristics. Using machine learning and natural language processing, we
automatically detected motifs in a large collection of Cinderella variants and
analysed their similarities and differences with clustering and dimensionality
reduction. The results show that large language models detect complex
interactions in tales, enabling computational analysis of extensive text
collections and facilitating cross-lingual comparisons.

</details>


### [45] [Beyond the Explicit: A Bilingual Dataset for Dehumanization Detection in Social Media](https://arxiv.org/abs/2510.18582)
*Dennis Assenmacher,Paloma Piot,Katarina Laken,David Jurgens,Claudia Wagner*

Main category: cs.CL

TL;DR: 提出一个理论驱动的双语去人化数据集，覆盖隐性去人化，提升零-shot与少样本的检测性能，并给出基准。


<details>
  <summary>Details</summary>
Motivation: 数字去人化在NLP领域被忽视，现有研究多聚焦于显性负面陈述，未覆盖更隐蔽的偏见，需构建跨平台、跨语言的标注数据以训练和评估检测方法。

Method: 从Twitter和Reddit收集多语言数据，采用不同取样方法；由众包工作者和专家对16,000个实例进行文档级和片段级标注，覆盖去人化的不同维度；在此数据集上对ML模型进行微调，并在零-shot与少样本情境下进行评估。

Result: 在零-shot和少样本情境下，微调模型在该数据集上达到超越现有最优模型的性能。

Conclusion: 该数据集既可作为训练资源又可作为未来去人化检测方法的基准，证明理论驱动的多维标注数据对提升检测能力的有效性；并为研究隐性去人化提供平台。

Abstract: Digital dehumanization, although a critical issue, remains largely overlooked
within the field of computational linguistics and Natural Language Processing.
The prevailing approach in current research concentrating primarily on a single
aspect of dehumanization that identifies overtly negative statements as its
core marker. This focus, while crucial for understanding harmful online
communications, inadequately addresses the broader spectrum of dehumanization.
Specifically, it overlooks the subtler forms of dehumanization that, despite
not being overtly offensive, still perpetuate harmful biases against
marginalized groups in online interactions. These subtler forms can insidiously
reinforce negative stereotypes and biases without explicit offensiveness,
making them harder to detect yet equally damaging. Recognizing this gap, we use
different sampling methods to collect a theory-informed bilingual dataset from
Twitter and Reddit. Using crowdworkers and experts to annotate 16,000 instances
on a document- and span-level, we show that our dataset covers the different
dimensions of dehumanization. This dataset serves as both a training resource
for machine learning models and a benchmark for evaluating future
dehumanization detection techniques. To demonstrate its effectiveness, we
fine-tune ML models on this dataset, achieving performance that surpasses
state-of-the-art models in zero and few-shot in-context settings.

</details>


### [46] [Dynamical model parameters from ultrasound tongue kinematics](https://arxiv.org/abs/2510.18629)
*Sam Kirkham,Patrycja Strycharczuk*

Main category: cs.CL

TL;DR: Ultrasound tongue kinematics provide comparable estimates of linear harmonic oscillator parameters to EMA for articulatory dynamics; jaw motion is adequately captured by mandibular short tendon tracking; ultrasound is viable for evaluating dynamical articulatory models.


<details>
  <summary>Details</summary>
Motivation: To determine whether ultrasound-based tongue kinematics can reliably substitute for electromagnetic articulography (EMA) in estimating dynamical parameters of articulatory models, specifically linear harmonic oscillator parameters, by comparing ultrasound with simultaneously-recorded EMA data.

Method: Model speech control as a dynamical system using a linear harmonic oscillator (LHO). Estimate LHO parameters from ultrasound tongue kinematics and from EMA data collected simultaneously, and compare. Additionally, assess whether mandibular short tendon tracking captures jaw motion adequately.

Result: Ultrasound and EMA yield comparable estimates of the dynamical parameters of the LHO model. Mandibular short tendon tracking also adequately captures jaw motion.

Conclusion: Ultrasound kinematics are viable for evaluating dynamical articulatory models and can serve as a non-invasive alternative to EMA for estimating dynamical parameters.

Abstract: The control of speech can be modelled as a dynamical system in which
articulators are driven toward target positions. These models are typically
evaluated using fleshpoint data, such as electromagnetic articulography (EMA),
but recent methodological advances make ultrasound imaging a promising
alternative. We evaluate whether the parameters of a linear harmonic oscillator
can be reliably estimated from ultrasound tongue kinematics and compare these
with parameters estimated from simultaneously-recorded EMA data. We find that
ultrasound and EMA yield comparable dynamical parameters, while mandibular
short tendon tracking also adequately captures jaw motion. This supports using
ultrasound kinematics to evaluate dynamical articulatory models.

</details>


### [47] [MLMA: Towards Multilingual with Mamba Based Architectures](https://arxiv.org/abs/2510.18684)
*Mohamed Nabih Ali,Daniele Falavigna,Alessio Brutti*

Main category: cs.CL

TL;DR: MLMA introduces Mamba-based multilingual ASR, leveraging a long-context state-space model to provide language-aware conditioning and shared representations, achieving competitive performance with Transformer baselines on multilingual benchmarks and suggesting Mamba as a scalable, efficient backbone for multilingual speech recognition.


<details>
  <summary>Details</summary>
Motivation: Multilingual ASR must perform well across high- and low-resource languages while remaining scalable and efficient. Traditional Transformer-based models may struggle with long sequences and resource disparities; hence exploring alternative backbone architectures that can handle long-context and cross-language conditioning.

Method: Adopt Mamba, a state-space model designed for long-context processing, as the backbone for multilingual ASR (MLMA). The model implicitly incorporates language-aware conditioning and shared representations to support recognition across diverse languages.

Result: On standard multilingual benchmarks, MLMA achieves competitive performance compared to Transformer-based architectures, indicating Mamba's viability as a backbone for multilingual ASR with efficient long-context processing.

Conclusion: Mamba can serve as a strong backbone for scalable, efficient, and accurate multilingual speech recognition, offering competitive performance and potential advantages in long-context modeling over traditional Transformer-based approaches.

Abstract: Multilingual automatic speech recognition (ASR) remains a challenging task,
especially when balancing performance across high- and low-resource languages.
Recent advances in sequence modeling suggest that architectures beyond
Transformers may offer better scalability and efficiency. In this work, we
introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new
approach that leverages the Mamba architecture--an efficient state-space model
optimized for long-context sequence processing--for multilingual ASR. Using
Mamba, MLMA implicitly incorporates language-aware conditioning and shared
representations to support robust recognition across diverse languages.
Experiments on standard multilingual benchmarks show that MLMA achieves
competitive performance compared to Transformer-based architectures. These
results highlight Mamba's potential as a strong backbone for scalable,
efficient, and accurate multilingual speech recognition.

</details>


### [48] [Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering](https://arxiv.org/abs/2510.18691)
*Feras AlMannaa,Talia Tseriotou,Jenny Chim,Maria Liakata*

Main category: cs.CL

TL;DR: 本研究首次系统评估大型语言模型在医学长期上下文的理解能力，比较不同规模、数据集与任务形式下的LC医学问答表现，揭示模型规模、记忆与推理限制，以及推理模型的潜在收益，并评估检索增强（RAG）在LC情境中的作用与最佳设置。


<details>
  <summary>Details</summary>
Motivation: 探究长期上下文对医学问答的挑战，评估RAG在LC中的潜在收益，比较单文档与多文档推理，并揭示模型记忆与推理机制在真实医疗文本场景中的作用与局限，填补现有文献在LC医学问答的空白。

Method: 对一系列含相关性内容的设置进行评估：覆盖不同能力水平的LLM模型、不同数据集、以及多种任务表单；在单文档与多文档推理场景中比较基线与RAG（检索增强生成）的表现；结合定性分析和错误分析，揭示常见失败模式并评估RAG在不同场景的有效性。

Result: 结果表明模型规模、记忆限制及推理能力对LC医学问答的表现有显著影响；在某些设置下，RAG可以提升对长期上下文的理解，尤其是在多文档推理场景中；提供了在单文档 vs 多文档、以及RAG应用中的最佳设置，以及常见失败案例的系统性分析。

Conclusion: 为评估与提升医学长期上下文问答能力提供了多维框架，强调在特定场景下优先考虑RAG、关注记忆与推理的限制，并通过多维评估来深入理解模型行为与局限性，帮助设计更鲁棒的LC医学问答系统。

Abstract: This study is the first to investigate LLM comprehension capabilities over
long-context (LC) medical QA of clinical relevance. Our comprehensive
assessment spans a range of content-inclusion settings based on their
relevance, LLM models of varying capabilities and datasets across task
formulations, revealing insights on model size effects, limitations, underlying
memorization issues and the benefits of reasoning models. Importantly, we
examine the effect of RAG on medical LC comprehension, uncover best settings in
single versus multi-document reasoning datasets and showcase RAG strategies for
improvements over LC. We shed light into some of the evaluation aspects using a
multi-faceted approach. Our qualitative and error analyses address open
questions on when RAG is beneficial over LC, revealing common failure cases.

</details>


### [49] [Bayesian Low-Rank Factorization for Robust Model Adaptation](https://arxiv.org/abs/2510.18723)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 提出贝叶斯因子化适配器用于语音基础模型的域适配，应用于 Whisper 的代码切换场景，在仅有微小适配损失的同时显著减少遗忘，且相较 LoRA 获得显著回溯增益（54%），新域损失仅约4%。


<details>
  <summary>Details</summary>
Motivation: 大型语音模型在本地化需求（如代码切换）上需要适配，但直接微调易过拟合且可能损害原模型的泛化能力，需要能够在保留基线能力的前提下进行稀疏且受控的域适配。

Method: 提出贝叶斯因子化适配器，在接近零的先验下实现稀疏的适配矩阵；对 Whisper 进行实现并在多种多语言代码切换场景中评估。

Result: 适配过程造成的性能损失极小，同时显著降低对原模型的灾难性遗忘；相较 LoRA，在新域上实现了约 54% 的回溯增益，且新域损失仅为约 4%。

Conclusion: 贝叶斯自适应为语音基础模型的微调提供了一种在不牺牲泛化能力的前提下实现域适配的有效方法。

Abstract: Large speech foundation models achieve strong performance across many
domains, but they often require adaptation to handle local needs such as
code-switching, where speakers mix languages within the same utterance. Direct
fine-tuning of these models risks overfitting to the target domain and
overwriting the broad capabilities of the base model. To address this
challenge, we explore Bayesian factorized adapters for speech foundation
models, which place priors near zero to achieve sparser adaptation matrices and
thereby retain general performance while adapting to specific domains. We apply
our approach to the Whisper model and evaluate on different multilingual
code-switching scenarios. Our results show only minimal adaptation loss while
significantly reducing catastrophic forgetting of the base model. Compared to
LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new
domain. These findings highlight the effectiveness of Bayesian adaptation for
fine-tuning speech foundation models without sacrificing generalization.

</details>


### [50] [SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish](https://arxiv.org/abs/2510.18725)
*Josh McGiff,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 通过 SemiAdapt 与 SemiLoRA 的半监督推理高效微调，在低资源语言（如爱尔兰语）中实现高质量领域适应，达到甚至优于全模型微调的效果。


<details>
  <summary>Details</summary>
Motivation: 全模型微调在大语言模型上成本高、对低资源领域不友好；PEFT（如 LoRA）提供参数高效的替代方案，但在领域适应中仍有提升空间；本研究旨在提升推理阶段的效率与效果，降低门槛。

Method: 提出 SemiAdapt 与 SemiLoRA 这两种半监督推理高效方法来强化领域适应；结合嵌入式推理以及域数据的分割微调策略；在 NMT 上进行评估，并对比全域微调与其他 PEFT 策略；公开高质量的爱尔兰语翻译模型作为开源资源；评估域-数据集级别微调与嵌入式推理在更大、噪声更高语料上的表现。

Result: SemiAdapt 能超越全域微调；SemiLoRA 能使 PEFT 方法达到或超过全模型微调；嵌入式推理在更大且更嘈杂语料上表现尤为出色；所开发的爱尔兰语翻译模型已开源。

Conclusion: 通过半监督和参数高效的推理/微调策略，显著提升低资源语言的领域适应能力，降低进入门槛，使高质量领域适应对研究者更易获得。

Abstract: Fine-tuning is widely used to tailor large language models for specific tasks
such as neural machine translation (NMT). However, leveraging transfer learning
is computationally expensive when fine-tuning large multilingual models with
billions of parameters, thus creating a barrier to entry for researchers
working on low-resource domains such as Irish translation. Parameter-efficient
fine-tuning (PEFT) bridges this gap by training on a fraction of the original
model parameters, with the Low-Rank Adaptation (LoRA) approach introducing
small, trainable adapter layers. We introduce SemiAdapt and SemiLoRA as
semi-supervised inference-efficient approaches that strengthen domain
adaptation and lead to improved overall performance in NMT. We demonstrate that
SemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRA
can propel PEFT methods to match or even outperform full-model fine-tuning. We
further evaluate domain-by-dataset fine-tuning and demonstrate that our
embedding-based inference methods perform especially well on larger and noisier
corpora. All Irish translation models developed in this work are released as
open resources. These methods aim to make high-quality domain adaptation and
fine-tuning more accessible to researchers working with low-resource languages.

</details>


### [51] [Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation](https://arxiv.org/abs/2510.18731)
*Ming Li*

Main category: cs.CL

TL;DR: 提出 RLAAR 框架，通过可验证奖励和可 abstention 的放弃奖励，在多轮对话中通过课程难度递增来缓解 LiC；在 LiC 基准上显著降低性能衰减，并提高放弃的校准性。


<details>
  <summary>Details</summary>
Motivation: 解决多轮对话中信息逐步揭示导致的性能下降（LiC）问题；借鉴 RLVR 的进展，强调对解题可行性与放弃的判断。

Method: 提出 Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR) 框架，采用能力门控的难度递增对话课程、基于多轮的 on-policy 滚动训练，以及混合奖励（正确性+可放弃奖励）来促使模型在解题和知情放弃之间取得平衡。

Result: 在 LiC 基准上，RLAAR 能显著缓解 LiC 性能衰减（从 62.6% 提升到 75.1%），并提升放弃校准率（从 33.5% 提升到 73.4%）。

Conclusion: 该方法为构建多轮更可靠、可可信的 LLM 提供了实用方案，显示了在对话系统中结合可验证奖励与放弃策略的有效性。

Abstract: Large Language Models demonstrate strong capabilities in single-turn
instruction following but suffer from Lost-in-Conversation (LiC), a degradation
in performance as information is revealed progressively in multi-turn settings.
Motivated by the current progress on Reinforcement Learning with Verifiable
Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable
Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not
only to generate correct answers, but also to judge the solvability of
questions in the multi-turn conversation setting. Our approach employs a
competence-gated curriculum that incrementally increases dialogue difficulty
(in terms of instruction shards), stabilizing training while promoting
reliability. Using multi-turn, on-policy rollouts and a mixed-reward system,
RLAAR teaches models to balance problem-solving with informed abstention,
reducing premature answering behaviors that cause LiC. Evaluated on LiC
benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to
75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together,
these results provide a practical recipe for building multi-turn reliable and
trustworthy LLMs.

</details>


### [52] [AI use in American newspapers is widespread, uneven, and rarely disclosed](https://arxiv.org/abs/2510.18774)
*Jenna Russell,Marzena Karpinska,Destiny Akinode,Katherine Thai,Bradley Emi,Max Spero,Mohit Iyyer*

Main category: cs.CL

TL;DR: 大规模审计显示约9%的线上新闻文章存在AI生成痕迹（部分或全部），且分布不均，较多出现在小型/地方媒体、天气与科技等主题以及特定所有权集团中。对华盛顿邮报、纽约时报、华尔街日报的45K篇社论发现AI生成内容的概率是新闻报道的6.4倍，且多篇AI标注的社论来自知名公众人物。AI使用几乎没有披露：对100篇AI标注文章的人工审核仅发现5条披露。


<details>
  <summary>Details</summary>
Motivation: 揭示新闻出版行业中AI生成内容的规模与分布，评估透明度现状，以及为编辑标准的更新提供证据，以维护公众信任。

Method: 数据集包括夏季2025年1.5千家美国报纸的186,000篇在线发表文章；使用Pangram等AI检测工具识别AI生成痕迹，同时分析其中45,000篇来自华盛顿邮报、纽约时报、华尔街日报的社论；对AI标注文章进行100篇的人工披露审核。

Result: 约9%的新发表文章为AI生成（部分/全部）。AI生成在小型本地媒体、天气与科技等主题以及特定所有权集团中更常见。社论中AI生成的可能性是新闻报道的6.4倍，且多来自知名公众人物。AI使用披露极少，人工抽样的100篇AI标注文章仅有5篇披露。

Conclusion: 研究强调需要提高透明度并更新编辑标准，以在新闻行业中维护公众信任，规范AI在报道中的使用。

Abstract: AI is rapidly transforming journalism, but the extent of its use in published
newspaper articles remains unclear. We address this gap by auditing a
large-scale dataset of 186K articles from online editions of 1.5K American
newspapers published in the summer of 2025. Using Pangram, a state-of-the-art
AI detector, we discover that approximately 9% of newly-published articles are
either partially or fully AI-generated. This AI use is unevenly distributed,
appearing more frequently in smaller, local outlets, in specific topics such as
weather and technology, and within certain ownership groups. We also analyze
45K opinion pieces from Washington Post, New York Times, and Wall Street
Journal, finding that they are 6.4 times more likely to contain AI-generated
content than news articles from the same publications, with many AI-flagged
op-eds authored by prominent public figures. Despite this prevalence, we find
that AI use is rarely disclosed: a manual audit of 100 AI-flagged articles
found only five disclosures of AI use. Overall, our audit highlights the
immediate need for greater transparency and updated editorial standards
regarding the use of AI in journalism to maintain public trust.

</details>


### [53] [WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection](https://arxiv.org/abs/2510.18798)
*Guanzhong He,Zhen Yang,Jinxin Liu,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: WebSeer is a reinforcement-learning search agent with a self-reflection mechanism that extends tool-use chains for web-based information retrieval, achieving state-of-the-art results with a 14B model on HotpotQA and SimpleQA.


<details>
  <summary>Details</summary>
Motivation: Current RL-based agentic models suffer from shallow tool-use depth and error accumulation across iterative interactions; there is a need for longer, more reflective reasoning to improve retrieval and answer accuracy in real-world web environments.

Method: A two-stage training framework combining cold-start pretraining and reinforcement learning under a self-reflection paradigm, using a large dataset annotated with reflection patterns to guide longer tool-use trajectories in web environments.

Result: With a single 14B model, WebSeer achieves state-of-the-art accuracies on HotpotQA (72.3%) and SimpleQA (90.0%), and demonstrates good generalization to out-of-distribution datasets; code released at the provided GitHub link.

Conclusion: Self-reflection enables extending tool-use chains and improving answer accuracy and generalization in RL-based web agents; the approach makes RL with reflection practical for real-world web-based information retrieval.

Abstract: Search agents have achieved significant advancements in enabling intelligent
information retrieval and decision-making within interactive environments.
Although reinforcement learning has been employed to train agentic models
capable of more dynamic interactive retrieval, existing methods are limited by
shallow tool-use depth and the accumulation of errors over multiple iterative
interactions. In this paper, we present WebSeer, a more intelligent search
agent trained via reinforcement learning enhanced with a self-reflection
mechanism. Specifically, we construct a large dataset annotated with reflection
patterns and design a two-stage training framework that unifies cold start and
reinforcement learning within the self-reflection paradigm for real-world
web-based environments, which enables the model to generate longer and more
reflective tool-use trajectories. Our approach substantially extends tool-use
chains and improves answer accuracy. Using a single 14B model, we achieve
state-of-the-art results on HotpotQA and SimpleQA, with accuracies of 72.3% and
90.0%, respectively, and demonstrate strong generalization to
out-of-distribution datasets. The code is available at
https://github.com/99hgz/WebSeer

</details>


### [54] [Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring](https://arxiv.org/abs/2510.18817)
*Shuxin Lin,Dhaval Patel,Christodoulos Constantinides*

Main category: cs.CL

TL;DR: 将LLMs的链式推理能力通过知识蒸馏转移到较小的SLMs，用于工业资产健康监测，采用MCQA提示和上下文学习提升推理，结果显示SLMs显著提升、缩小与LLMs的差距，代码开源。


<details>
  <summary>Details</summary>
Motivation: 在Industry 4.0等专业领域，资源受限的SLMs难以完成复杂推理任务，迫切需要通过知识蒸馏等方法将LLMs的推理能力传授给小模型以获得高效、成本可控的解决方案。

Method: 通过Chain-of-Thought（CoT）蒸馏，将LLMs的推理过程转移到小模型；使用多选题（MCQA）提示来强化推理能力；进行上下文学习以验证生成知识的质量；对比基线SLMs与在生成知识后的微调模型，以及与常用LLMs的基准。

Result: 微调后的SLMs具备CoT推理能力，明显优于原始基线模型，显著缩小与LLMs之间的差距；在工业资产健康监测任务中表现良好，且代码已开源。

Conclusion: CoT蒸馏是提升资源受限SLMs在工业场景中推理能力的有效途径，能够在保持较低计算成本的同时接近LLMs的推理性能，具备良好的可扩展性。

Abstract: Small Language Models (SLMs) are becoming increasingly popular in specialized
fields, such as industrial applications, due to their efficiency, lower
computational requirements, and ability to be fine-tuned for domain-specific
tasks, enabling accurate and cost-effective solutions. However, performing
complex reasoning using SLMs in specialized fields such as Industry 4.0 remains
challenging. In this paper, we propose a knowledge distillation framework for
industrial asset health, which transfers reasoning capabilities via
Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to
smaller, more efficient models (SLMs). We discuss the advantages and the
process of distilling LLMs using multi-choice question answering (MCQA) prompts
to enhance reasoning and refine decision-making. We also perform in-context
learning to verify the quality of the generated knowledge and benchmark the
performance of fine-tuned SLMs with generated knowledge against widely used
LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform
the base models by a significant margin, narrowing the gap to their LLM
counterparts. Our code is open-sourced at:
https://github.com/IBM/FailureSensorIQ.

</details>


### [55] [MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training](https://arxiv.org/abs/2510.18830)
*Wenxuan Li,Chengruidong Zhang,Huiqiang Jiang,Yucheng Li,Yuqing Yang,Lili Qiu*

Main category: cs.CL

TL;DR: 提出 MTraining 的分布式训练框架，利用动态稀疏注意力在超长上下文下高效训练大语言模型（LLM）。在 32 个 A100 GPU 上，将 Qwen2.5-3B 的上下文从 32K 提升到 512K，训练吞吐提升至 6x，同时保持精度。核心包括动态稀疏训练模式、平衡稃环注意力（balanced sparse ring attention）和层级稀疏环注意力（hierarchical sparse ring attention）。


<details>
  <summary>Details</summary>
Motivation: 在超长上下文条件下训练 LLM 的计算成本和通信开销显著增加，且在分布式设置中存在工作负载不均、步级不均等问题。需要一种能够高效利用动态稀疏注意力、并在大规模分布式环境中降低开销的训练方法。

Method: 提出 MTraining，结合三大组件：1) 动态稀疏训练模式，根据训练阶段动态选择参与注意力的稀疏结构；2) 平衡稀疏环注意力，优化跨 GPU 的通信与计算负载分配；3) 层级稀疏环注意力，利用多层次稀疏结构在超长上下文中维持计算效率。将这些组件整合到大模型训练流水线中，解决上下文扩展带来的计算和通信失衡问题。

Result: 在 32 个 A100 GPU 集群上，将 Qwen2.5-3B 的上下文从 32K 提升至 512K，验证了 MTraining 的有效性。对 RULER、PG-19、InfiniteBench、Needle In A Haystack 等下游任务的评估显示训练吞吐量最高可提高 6 倍，同时保持模型精度。代码公开可获取。

Conclusion: MTraining 为在超长上下文下训练大模型提供了一种可扩展、效率高的分布式解决方案，有效缓解动态稀疏注意力在训练阶段的计算与通信不均问题，具备直接落地到实际大模型训练的潜力，同时也为未来在更大规模模型和多样任务上的扩展奠定基础。

Abstract: The adoption of long context windows has become a standard feature in Large
Language Models (LLMs), as extended contexts significantly enhance their
capacity for complex reasoning and broaden their applicability across diverse
scenarios. Dynamic sparse attention is a promising approach for reducing the
computational cost of long-context. However, efficiently training LLMs with
dynamic sparse attention on ultra-long contexts-especially in distributed
settings-remains a significant challenge, due in large part to worker- and
step-level imbalance. This paper introduces MTraining, a novel distributed
methodology leveraging dynamic sparse attention to enable efficient training
for LLMs with ultra-long contexts. Specifically, MTraining integrates three key
components: a dynamic sparse training pattern, balanced sparse ring attention,
and hierarchical sparse ring attention. These components are designed to
synergistically address the computational imbalance and communication overheads
inherent in dynamic sparse attention mechanisms during the training of models
with extensive context lengths. We demonstrate the efficacy of MTraining by
training Qwen2.5-3B, successfully expanding its context window from 32K to 512K
tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite
of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A
Haystack, reveal that MTraining achieves up to a 6x higher training throughput
while preserving model accuracy. Our code is available at
https://github.com/microsoft/MInference/tree/main/MTraining.

</details>


### [56] [Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning](https://arxiv.org/abs/2510.18849)
*Chenghao Zhu,Meiling Tao,Tiannan Wang,Dongyi Ding,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出 Critique-Post-Edit 框架，结合多维度个人化生成奖励模型与文本批评，提升 LLM 个性化的忠实性与可控性，相较 PPO 和 GPT-4.1，表现显著提升。


<details>
  <summary>Details</summary>
Motivation: SFT 容易出现性能瓶颈，RLHF 在个性化层面面临奖励模型易被滥用（奖励劫持）以及难以捕捉细粒度个性化需求的问题，需要更健壮的评估与自我纠偏机制。

Method: 引入 Personalized Generative Reward Model (GRM) 提供多维分数和文本批评；采用 Critique-Post-Edit 机制让策略模型基于批评进行自我改写并学习，实施长度约束的评估以控制输出形式与长度。

Result: 在个性化基准上明显优于标准 PPO。个性化 Qwen2.5-7B 平均胜率提升约11%；个性化 Qwen2.5-14B 超越 GPT-4.1 的表现。

Conclusion: 给出一个可实用、高效且可控的个性化路径，降低奖励劫持风险，提高忠实性与学习效率。

Abstract: Faithfully personalizing large language models (LLMs) to align with
individual user preferences is a critical but challenging task. While
supervised fine-tuning (SFT) quickly reaches a performance plateau, standard
reinforcement learning from human feedback (RLHF) also struggles with the
nuances of personalization. Scalar-based reward models are prone to reward
hacking which leads to verbose and superficially personalized responses. To
address these limitations, we propose Critique-Post-Edit, a robust
reinforcement learning framework that enables more faithful and controllable
personalization. Our framework integrates two key components: (1) a
Personalized Generative Reward Model (GRM) that provides multi-dimensional
scores and textual critiques to resist reward hacking, and (2) a
Critique-Post-Edit mechanism where the policy model revises its own outputs
based on these critiques for more targeted and efficient learning. Under a
rigorous length-controlled evaluation, our method substantially outperforms
standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an
average 11\% win-rate improvement, and personalized Qwen2.5-14B model surpasses
the performance of GPT-4.1. These results demonstrate a practical path to
faithful, efficient, and controllable personalization.

</details>


### [57] [LightMem: Lightweight and Efficient Memory-Augmented Generation](https://arxiv.org/abs/2510.18866)
*Jizhan Fang,Xinle Deng,Haoming Xu,Ziyan Jiang,Yuqi Tang,Ziwen Xu,Shumin Deng,Yunzhi Yao,Mengru Wang,Shuofei Qiao,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: LightMem是一种用于让大型语言模型更高效地利用历史信息的三阶段记忆系统，通过感官记忆、短期记忆和睡眠时间更新的长期记忆实现高效信息整理与检索，在准确性显著提升的同时大幅降低代币、API 调用和运行时间开销。


<details>
  <summary>Details</summary>
Motivation: LLMs在动态复杂环境中难以充分利用历史交互信息，现有记忆系统存在较高的时间和计算开销，因此需要在性能与效率之间取得平衡。

Method: 借鉴 Atkinson–Shiffrin 的人类记忆模型，将记忆分为感官记忆（快速过滤与分组）、专题化短期记忆（按主题组织和摘要）、并以睡眠时间更新的长期记忆（离线汇 consolidation 与在线推理解耦）等三阶段结构；并对 LongMemEval 数据集进行评估，使用 GPT 与 Qwen 作为骨干。

Result: 在 LongMemEval 上，LightMem 相较强基线在准确性上提升最高可达 10.9%，同时在代币使用、API 调用和推理时延方面分别缩减至原先的 1/117、1/159 以及 12×以上。

Conclusion: LightMem 在性能与效率之间实现良好折中，且代码公开（链接）。

Abstract: Despite their remarkable capabilities, Large Language Models (LLMs) struggle
to effectively leverage historical interaction information in dynamic and
complex environments. Memory systems enable LLMs to move beyond stateless
interactions by introducing persistent information storage, retrieval, and
utilization mechanisms. However, existing memory systems often introduce
substantial time and computational overhead. To this end, we introduce a new
memory system called LightMem, which strikes a balance between the performance
and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of
human memory, LightMem organizes memory into three complementary stages. First,
cognition-inspired sensory memory rapidly filters irrelevant information
through lightweight compression and groups information according to their
topics. Next, topic-aware short-term memory consolidates these topic-based
groups, organizing and summarizing content for more structured access. Finally,
long-term memory with sleep-time update employs an offline procedure that
decouples consolidation from online inference. Experiments on LongMemEval with
GPT and Qwen backbones show that LightMem outperforms strong baselines in
accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API
calls by up to 159x, and runtime by over 12x. The code is available at
https://github.com/zjunlp/LightMem.

</details>


### [58] [How Do LLMs Use Their Depth?](https://arxiv.org/abs/2510.18871)
*Akshat Gupta,Jay Yeung,Gopala Anumanchipalli,Anna Ivanova*

Main category: cs.CL

TL;DR: 提出“Guess-then-Refine”框架，揭示LLM在推理中分层使用深度的细粒度规律：早期层主要给出高频词的初步猜测，随后逐步 refine 成上下文相关的输出；跨多任务揭示了深度在语言预测、词性、事实回忆和多选题中的分层使用。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在推理时为何不均匀使用深度、以及深度在不同任务和阶段的具体作用，以提高计算效率和设计更高效的Transformer结构。

Method: 对多开放权重模型的中间表示进行推理过程追踪，分析早期层的预测分布、单词频率、以及在三个具体任务（POS、事实回忆、选择题）中的深度使用模式；并提出并检验“Guess-then-Refine”框架。

Result: 发现早期层的前几次预测多为高频词，作为模型的统计性猜测；随着上下文发展，初始猜测被超过70%的情况 refining；对三类任务的分析显示：功能词最早被正确预测；在多令牌答案中，首个token需要更多深度；在多选题中，模型在前半层就能识别输出格式，最终输出在后半层完成。

Conclusion: 揭示了LLM层级计算的结构性与细粒度模式，为提高Transformer的计算效率和改进模型设计提供方向，如动态深度分配、早期对抗性推理、以及对推理阶段的资源优化。

Abstract: Growing evidence suggests that large language models do not use their depth
uniformly, yet we still lack a fine-grained understanding of their layer-wise
prediction dynamics. In this paper, we trace the intermediate representations
of several open-weight models during inference and reveal a structured and
nuanced use of depth. Specifically, we propose a "Guess-then-Refine" framework
that explains how LLMs internally structure their computations to make
predictions. We first show that the top-ranked predictions in early LLM layers
are composed primarily of high-frequency tokens, which act as statistical
guesses proposed by the model early on due to the lack of appropriate
contextual information. As contextual information develops deeper into the
model, these initial guesses get refined into contextually appropriate tokens.
Even high-frequency token predictions from early layers get refined >70% of the
time, indicating that correct token prediction is not "one-and-done". We then
go beyond frequency-based prediction to examine the dynamic usage of layer
depth across three case studies. (i) Part-of-speech analysis shows that
function words are, on average, the earliest to be predicted correctly. (ii)
Fact recall task analysis shows that, in a multi-token answer, the first token
requires more computational depth than the rest. (iii) Multiple-choice task
analysis shows that the model identifies the format of the response within the
first half of the layers, but finalizes its response only toward the end.
Together, our results provide a detailed view of depth usage in LLMs, shedding
light on the layer-by-layer computations that underlie successful predictions
and providing insights for future works to improve computational efficiency in
transformer-based models.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [59] [LIME: Link-based user-item Interaction Modeling with decoupled xor attention for Efficient test time scaling](https://arxiv.org/abs/2510.18239)
*Yunjiang Jiang,Ayush Agarwal,Yang Liu,Bi Xue*

Main category: cs.IR

TL;DR: LIME通过低秩连结嵌入和线性注意力（LIME-XOR）来缓解推荐系统中的计算瓶颈：使推理成本几乎与候选集规模无关、与用户历史长度线性相关，从而在接近SOTA的同时实现10x的推理加速，并在实际平台上提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 在大规模推荐系统中，扩展用户历史长度和候选集规模的需求与变压器模型的计算复杂性之间存在矛盾：自注意力在序列长度上成本为O(N^2)，候选集则线性增长，导致推理阶段难以扩展。需提供高效且具表达能力的架构来打破这一权衡。

Method: 提出LIME架构，含两大创新：1) 低秩“连结嵌入”以解耦用户与候选之间的交互，允许对注意力权重进行预计算，使推理成本几乎与候选集规模无关；2) 线性注意力机制LIME-XOR，将序列长度的复杂度从O(N^2)降至O(N)。

Result: 在公开数据集和工业数据集上的实验显示，LIME在性能上接近最先进的Transformer，但推理速度提升约10x，尤其在大型候选集或长序列情境下更明显。在某大型推荐平台的上线测试中，LIME提升了用户参与度，同时保持对候选集规模与历史长度的推理成本最小化，展示了高效且具表达力的推荐系统新范式。

Conclusion: LIME为大规模推荐系统提供了一种在保持接近SOTA性能的前提下显著降低推理计算成本的路径，尤其适合扩展候选集和历史序列长度的场景，推动高效且表达力强的推荐系统的发展。

Abstract: Scaling large recommendation systems requires advancing three major
frontiers: processing longer user histories, expanding candidate sets, and
increasing model capacity. While promising, transformers' computational cost
scales quadratically with the user sequence length and linearly with the number
of candidates. This trade-off makes it prohibitively expensive to expand
candidate sets or increase sequence length at inference, despite the
significant performance improvements.
  We introduce \textbf{LIME}, a novel architecture that resolves this
trade-off. Through two key innovations, LIME fundamentally reduces
computational complexity. First, low-rank ``link embeddings" enable
pre-computation of attention weights by decoupling user and candidate
interactions, making the inference cost nearly independent of candidate set
size. Second, a linear attention mechanism, \textbf{LIME-XOR}, reduces the
complexity with respect to user sequence length from quadratic ($O(N^2)$) to
linear ($O(N)$).
  Experiments on public and industrial datasets show LIME achieves near-parity
with state-of-the-art transformers but with a 10$\times$ inference speedup on
large candidate sets or long sequence lengths. When tested on a major
recommendation platform, LIME improved user engagement while maintaining
minimal inference costs with respect to candidate set size and user history
length, establishing a new paradigm for efficient and expressive recommendation
systems.

</details>


### [60] [Evaluating LLM-Based Mobile App Recommendations: An Empirical Study](https://arxiv.org/abs/2510.18364)
*Quim Motger,Xavier Franch,Vincenzo Gervasi,Jordi Marco*

Main category: cs.IR

TL;DR: 本论文通过对通用大语言模型在推荐移动应用时的输出进行实证分析，提出了16条可泛化的排序标准、一个评估一致性与对显式排序指令响应的框架，并提供复现包，发现 LLMs 的排序依据广泛但碎片化，与传统 ASO 指标部分对齐，前几名结果更稳定，但深度排序和特定检索条件下变异性增大，对显式指令的响应也存在较大差异。


<details>
  <summary>Details</summary>
Motivation: 揭示基于对话的应用推荐背后的推理过程、评估其一致性、可解释性以及与传统 ASO 指标的对齐程度，从而提升对 AI 驱动应用推荐系统的理解与信任。

Method: 在多种通用目的 LLMs 上进行实证分析，基于输出提炼出排序准则的16类并建立 taxonomy；设计系统化评估框架以分析推荐的一致性及对显式排序指令的响应性；提供可复现的数据和代码包以支持未来研究；

Result: 得到16类可泛化的排序准则、一个用于分析一致性与响应性的评估框架；结果显示 LLMs 的排序准则广泛但分散，部分与 ASO 指标对齐；顶层输出在不同运行间较为一致，但在深层排序和特定检索条件下变异性上升；对显式排序指令的敏感性存在较大差异，体现出复杂的推理动态。

Conclusion: 结论强调对用户、开发者和推荐系统研究者的实际意义，指向在对话式应用发现中的复杂推理机制，并强调研究的可重复性与未来研究方向。

Abstract: Large Language Models (LLMs) are increasingly used to recommend mobile
applications through natural language prompts, offering a flexible alternative
to keyword-based app store search. Yet, the reasoning behind these
recommendations remains opaque, raising questions about their consistency,
explainability, and alignment with traditional App Store Optimization (ASO)
metrics. In this paper, we present an empirical analysis of how widely-used
general purpose LLMs generate, justify, and rank mobile app recommendations.
Our contributions are: (i) a taxonomy of 16 generalizable ranking criteria
elicited from LLM outputs; (ii) a systematic evaluation framework to analyse
recommendation consistency and responsiveness to explicit ranking instructions;
and (iii) a replication package to support reproducibility and future research
on AI-based recommendation systems. Our findings reveal that LLMs rely on a
broad yet fragmented set of ranking criteria, only partially aligned with
standard ASO metrics. While top-ranked apps tend to be consistent across runs,
variability increases with ranking depth and search specificity. LLMs exhibit
varying sensitivity to explicit ranking instructions - ranging from substantial
adaptations to near-identical outputs - highlighting their complex reasoning
dynamics in conversational app discovery. Our results aim to support end-users,
app developers, and recommender-systems researchers in navigating the emerging
landscape of conversational app discovery.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings](https://arxiv.org/abs/2510.17846)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: CARLE 是一个混合AI框架，结合 Res-CNN/Res-LSTM、多头注意力和残差连接，使用随机森林回归器对滚动轴承的剩余使用寿命进行鲁棒预测，在 XJTU-SY 与 PRONOSTIA 数据集上优于多种方法，且具备鲁棒性、跨域泛化能力和可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 现有RUL方法常在不同工况下泛化性和鲁棒性不足；需能同时捕捉空间与时间降解模式、实现稳定预测并提供透明度的框架。

Method: 提出 CARLE 混合框架，使用 Res-CNN 与 Res-LSTM 块，结合多头注意力与残差连接，后接随机森林回归器进行RUL预测。预处理包含高斯滤波降噪与连续小波变换的时频特征提取。评估在 XJTU-SY 与 PRONOSTIA 数据集，进行消融研究、噪声与跨域鲁棒性实验，并用 LIME/SHAP 分析模型可解释性。

Result: CARLE 在对比方法中表现优越，尤其在动态工况下具有更强鲁棒性与泛化性；消融实验显示各组件贡献显著，LIME/SHAP 提供了可解释性评估。

Conclusion: 将深度与浅层学习结合的混合框架可有效捕捉降解模式，合适的预处理与时空特征提取是关键，未来可扩展至其他PHM/健康监测任务并进一步提升透明度。

Abstract: Prognostic Health Management (PHM) systems monitor and predict equipment
health. A key task is Remaining Useful Life (RUL) estimation, which predicts
how long a component, such as a rolling element bearing, will operate before
failure. Many RUL methods exist but often lack generalizability and robustness
under changing operating conditions. This paper introduces CARLE, a hybrid AI
framework that combines deep and shallow learning to address these challenges.
CARLE uses Res-CNN and Res-LSTM blocks with multi-head attention and residual
connections to capture spatial and temporal degradation patterns, and a Random
Forest Regressor (RFR) for stable, accurate RUL prediction. A compact
preprocessing pipeline applies Gaussian filtering for noise reduction and
Continuous Wavelet Transform (CWT) for time-frequency feature extraction. We
evaluate CARLE on the XJTU-SY and PRONOSTIA bearing datasets. Ablation studies
measure each component's contribution, while noise and cross-domain experiments
test robustness and generalization. Comparative results show CARLE outperforms
several state-of-the-art methods, especially under dynamic conditions. Finally,
we analyze model interpretability with LIME and SHAP to assess transparency and
trustworthiness.

</details>


### [62] [MIN-Merging: Merge the Important Neurons for Model Merging](https://arxiv.org/abs/2510.17890)
*Yunfei Liang*

Main category: cs.LG

TL;DR: MIN-Merging is a router-based framework that selectively merges the most important neurons to reduce parameter conflicts when merging open-source models, achieving in-domain gains while preserving out-of-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Parameter conflicts during model merging degrade performance on domain-specific tasks; a selective merging strategy is needed to maintain specialization while integrating multiple models.

Method: Introduce MIN-Merging, a router-based mechanism that identifies and merges the most important neurons to mitigate conflicts in merged models. The approach is evaluated on CV and NLP benchmarks to assess in-domain gains and out-of-domain generalization.

Result: MIN-Merging yields consistent in-domain performance gains across CV and NLP tasks while retaining the pretrained models' generalization on out-of-domain tasks.

Conclusion: Routing-based selective neuron merging is an effective and practical solution to the parameter conflict problem in model merging, balancing specialization and generalization across domains.

Abstract: Recent advances in deep learning have led to a surge of open-source models
across diverse domains. While model merging offers a promising way to combine
their strengths, existing approaches often suffer from parameter conflicts that
degrade performance on domain-specific tasks. We propose MIN-Merging, a
router-based framework that selectively merges the most important neurons to
reduce such conflicts. Extensive experiments on Computer Vision(CV) and Natural
Language Processing(NLP) benchmarks show that MIN-Merging achieves consistent
gains on in-domain tasks while retaining the generalization ability of
pretrained models on out-of-domain tasks. These results highlight its
effectiveness as a practical solution to the parameter conflict problem in
model merging.

</details>


### [63] [Hierarchical Federated Unlearning for Large Language Models](https://arxiv.org/abs/2510.17895)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: A federated, privacy-preserving unlearning framework for LLMs that uses task-specific adapters and hierarchical merging to handle heterogeneous, decentralized unlearning requests while preserving model utility.


<details>
  <summary>Details</summary>
Motivation: To remove undesirable knowledge from large language models in realistic settings where unlearning requests are continuous, heterogeneous, and sourced from decentralized sensitive data, causing inter/conflicting updates and potential forgetting-utility trade-offs.

Method: Decouple unlearning from retention using task-specific adapter learning and apply a hierarchical merging strategy to resolve conflicts between objectives, enabling scalable, privacy-preserving federated unlearning updates for LLMs.

Result: Empirical evaluation on WMDP, MUSE, and TOFU benchmarks shows the approach effectively handles heterogeneous unlearning requests and maintains strong LLM utility compared with baseline methods.

Conclusion: The proposed federated unlearning framework is scalable, privacy-preserving, and robust, enabling adaptable unlearning updates for LLMs in real-world, decentralized settings.

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, raising concerns about privacy, security and the need to remove
undesirable knowledge. Machine Unlearning has emerged as a promising solution,
yet faces two key challenges: (1) practical unlearning needs are often
continuous and heterogeneous, and (2) they involve decentralized, sensitive
data with asymmetric access. These factors result in inter-domain and
intra-domain interference, which further amplifies the dilemma of unbalanced
forgetting and retaining performance. In response, we propose a federated
unlearning approach for LLMs that is scalable and privacy preserving. Our
method decouples unlearning and retention via task-specific adapter learning
and employs a hierarchical merging strategy to mitigate conflicting objectives
and enables robust, adaptable unlearning updates. Comprehensive experiments on
benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles
heterogeneous unlearning requests while maintaining strong LLM utility compared
with baseline methods.

</details>


### [64] [Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism](https://arxiv.org/abs/2510.17896)
*Tao Bu,Qiangang Wang,Bowen Zeng,Hanwen Sun,Yunpeng Huang,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: 提出一个统一的基准，整合注意力算子核和上下文并行机制，用于长上下文LLM训练的评估与对比。


<details>
  <summary>Details</summary>
Motivation: 系统性评估注意力算子层面的实现与上下文并行策略，填补现有研究在可重复性、跨框架分析和全面对比方面的空缺，便于设计与部署长上下文注意力。

Method: 提出模块化、可扩展的基准框架，整合代表性的注意力算子核与上下文并行机制，并提供评估维度（注意力掩码模式、序列长度/分布式规模）；在多达96个GPU的集群上进行广泛实验以实现可重复比较。

Result: 该基准实现了可重复的方法对比，揭示了方法间的权衡，为长期上下文训练中的注意力机制设计与部署提供实用指南。

Conclusion: 通过统一的评测框架，填补了现有工作在算子级和框架级对比的空白，促进在硬件与软件层面的长期上下文注意力方法的评估与应用。

Abstract: Transformer-based large language models (LLMs) have achieved remarkable
success, yet their standard attention mechanism incurs quadratic computation
and memory costs with respect to sequence length, posing a major bottleneck for
long-context training. Prior work tackles this challenge along two directions:
(1) kernel-level optimizations, which accelerate dense and sparse attention
operators; and (2) module-level strategies, often referred to as distributed
attention or context parallel training, which scale attention across multiple
devices. However, systematic evaluation still remains limited: operator-level
comparisons are often incomplete, while context parallel strategies are
typically framework-specific, with unclear performance analysis across
contexts. To address these gaps, we propose a unified benchmark that integrates
representative attention kernels and context parallel mechanisms with a modular
and extensible interface for evaluation. The benchmark evaluates methods along
two critical dimensions: (1) attention mask patterns, which strongly affect
efficiency, scalability, and usability, and (2) sequence length and distributed
scale, which determine performance under extreme long-context training. Through
comprehensive experiments on the cluster of up to 96 GPUs, our benchmark
enables reproducible comparisons, highlights method-specific trade-offs, and
provides practical guidance for designing and deploying attention mechanisms in
long-context LLM training.

</details>


### [65] [Automated Algorithm Design for Auto-Tuning Optimizers](https://arxiv.org/abs/2510.17899)
*Floris-Jan Willemsen,Niki van Stein,Ben van Werkhoven*

Main category: cs.LG

TL;DR: 本研究提出利用大语言模型（LLMs）自动生成针对自适应调优问题的优化算法，在多应用、跨硬件平台的场景中显著优于现有方法，显示出LLM在自动化优化策略设计中的潜力。


<details>
  <summary>Details</summary>
Motivation: 自动性能调优具有极大、且通常不规则的参数空间；传统优化算法在不同任务上表现波动，难以找到一刀切的高效策略，因此需要一种能够自动生成并自适应的优化器。

Method: 提出一个框架：通过提示将问题描述和搜索空间特征等信息输入LLMs，生成定制化的优化策略；对生成的算法进行迭代评估与改进。对四个真实自调应用、六个平台进行评估，并与两大现有自调框架的最先进优化算法比较。

Result: 在额外提供应用信息时，生成算法的平均性能提升约30.7%；在提供搜索空间特征时，提升约14.6%。在多种情形下，LLM生成的优化器与人类设计的算法并驾齐驱，甚至在最佳情形下超越现有最优优化器，平均提升达72.4%。

Conclusion: LLM生成的优化策略在自动化调优领域具有显著潜力，能够针对具体任务自动定制高效优化算法；该框架可扩展至更多应用和平台，未来可进一步提升泛化和鲁棒性。

Abstract: Automatic performance tuning (auto-tuning) is essential for optimizing
high-performance applications, where vast and irregular parameter spaces make
manual exploration infeasible. Traditionally, auto-tuning relies on
well-established optimization algorithms such as evolutionary algorithms,
annealing methods, or surrogate model-based optimizers to efficiently find
near-optimal configurations. However, designing effective optimizers remains
challenging, as no single method performs best across all tuning tasks.
  In this work, we explore a new paradigm: using large language models (LLMs)
to automatically generate optimization algorithms tailored to auto-tuning
problems. We introduce a framework that prompts LLMs with problem descriptions
and search-space characteristics results to produce specialized optimization
strategies, which are iteratively examined and improved.
  These generated algorithms are evaluated on four real-world auto-tuning
applications across six hardware platforms and compared against the
state-of-the-art in optimization algorithms of two contemporary auto-tuning
frameworks. The evaluation demonstrates that providing additional application-
and search space-specific information in the generation stage results in an
average performance improvement of 30.7\% and 14.6\%, respectively. In
addition, our results show that LLM-generated optimizers can rival, and in
various cases outperform, existing human-designed algorithms, with our
best-performing generated optimization algorithms achieving, on average, 72.4\%
improvement over state-of-the-art optimizers for auto-tuning.

</details>


### [66] [The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications](https://arxiv.org/abs/2510.17901)
*Alex Acero,Daniel M. Jimenez-Gutierrez,Dario Pighin,Enrique Zuazua,Joaquin Del Rio,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 提出 SBVFL，通过将大多数节点更新与服务器解耦，显著降低垂直联邦学习的通信量（约99%），在保持准确性与鲁棒性的前提下实现隐私保护的VFL。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习因大量的通信需求而面临隐私与能源成本、可训练性问题，限制在敏感领域的应用，需要更高效且隐私友好的方案。

Method: 引入分布式训练机制，核心在于将大部分节点端更新与服务器端解耦，降低服务器-节点之间的通信压力；在实验中对比标准 VFL，显示显著的通信量减少，同时保持精度与鲁棒性。

Result: 在与标准 VFL 的对比中，通信量降低约 99%，模型精度与鲁棒性未下降。

Conclusion: SBVFL为在医疗、金融、制造等敏感领域提供实用、隐私保护的垂直联邦学习解决方案铺平道路。

Abstract: Federated Learning (FL) enables collaborative decentralized training across
multiple parties (nodes) while keeping raw data private. There are two main
paradigms in FL: Horizontal FL (HFL), where all participant nodes share the
same feature space but hold different samples, and Vertical FL (VFL), where
participants hold complementary features for the same samples. While HFL is
widely adopted, VFL is employed in domains where nodes hold complementary
features about the same samples. Still, VFL presents a significant limitation:
the vast number of communications required during training. This compromises
privacy and security, and can lead to high energy consumption, and in some
cases, make model training unfeasible due to the high number of communications.
  In this paper, we introduce Sherpa.ai Blind Vertical Federated Learning
(SBVFL), a novel paradigm that leverages a distributed training mechanism
enhanced for privacy and security. Decoupling the vast majority of node updates
from the server dramatically reduces node-server communication. Experiments
show that SBVFL reduces communication by ~99% compared to standard VFL while
maintaining accuracy and robustness. Therefore, SBVFL enables practical,
privacy-preserving VFL across sensitive domains, including healthcare, finance,
manufacturing, aerospace, cybersecurity, and the defense industry.

</details>


### [67] [NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation](https://arxiv.org/abs/2510.17914)
*Rikard Vinge,Isabelle Wittmann,Jannik Schneider,Michael Marszalek,Luis Gilch,Thomas Brunschwiler,Conrad M Albrecht*

Main category: cs.LG

TL;DR: NeuCo-Bench 提供一个用于评估地球观测领域中有损神经压缩和表示学习的新基准框架，聚焦固定大小的嵌入、可重复的评估流程、隐藏任务排行榜和稳定性-准确性权衡的评分。


<details>
  <summary>Details</summary>
Motivation: 缺乏对神经嵌入在 EO 任务的统一、可重复评估，易产生预训练偏差和不可比的结果。

Method: 提出固定大小的嵌入作为通用表示；构建评估管线、隐藏任务挑战模式、稳定性+准确性平衡的评分体系；发布多光谱多时相 EO 数据集 SSL4EO-S12-downstream；在 CVPR EARTHVISION 工作坊对外公开挑战并与最先进的模型进行消融分析。

Result: 公开挑战的初步结果，以及对领先基础模型的消融研究。

Conclusion: NeuCo-Bench 是面向 EO 及更广领域的神经嵌入标准化评估的第一步，推动社区协作和可重复性。

Abstract: We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy)
neural compression and representation learning in the context of Earth
Observation (EO). Our approach builds on fixed-size embeddings that act as
compact, task-agnostic representations applicable to a broad range of
downstream tasks. NeuCo-Bench comprises three core components: (i) an
evaluation pipeline built around reusable embeddings, (ii) a new challenge mode
with a hidden-task leaderboard designed to mitigate pretraining bias, and (iii)
a scoring system that balances accuracy and stability. To support
reproducibility, we release SSL4EO-S12-downstream, a curated multispectral,
multitemporal EO dataset. We present initial results from a public challenge at
the 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-art
foundation models. NeuCo-Bench provides a first step towards community-driven,
standardized evaluation of neural embeddings for EO and beyond.

</details>


### [68] [Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection](https://arxiv.org/abs/2510.17917)
*Jinseong Park,Mijung Park*

Main category: cs.LG

TL;DR: 提出时-频率选择性数据去学习策略以改进扩散模型中的遗忘效果，同时保持更高的生成质量，并提出一个对数据删除与质量的归一化评估指标SSCD。


<details>
  <summary>Details</summary>
Motivation: 现有数据去学习方法在扩散模型中通常对所有时间步进行等效处理，导致忘记效果与生成质量之间存在失衡，且在不同时间和频率上忘记的效果差异显著。需要更有选择性的策略来实现更高质量且更可靠的忘记。

Method: 提出时间-频率选择性（time-frequency selective）的训练策略，在训练阶段聚焦于特定时间-频率范围以实现更高质量的去记忆，同时适用于梯度驱动和偏好优化目标，以及图像级和文本到图像任务。并给出对数据删除与质量的归一化版SSCD作为评估。

Result: 在多种设置下验证：选择性时间-频率训练能获得更高美学质量、噪声更低的样本，适用于梯度基与偏好优化目标，以及图像级和文本到图像任务。提出的评估策略有效区分删除与质量。

Conclusion: 时间-频率选择性为扩散模型的数据去学习带来实用的改进，能够在提升删除效果的同时维持或提升生成质量；并提供更清晰的评估框架以便比较不同方法。

Abstract: Data unlearning aims to remove the influence of specific training samples
from a trained model without requiring full retraining. Unlike concept
unlearning, data unlearning in diffusion models remains underexplored and often
suffers from quality degradation or incomplete forgetting. To address this, we
first observe that most existing methods attempt to unlearn the samples at all
diffusion time steps equally, leading to poor-quality generation. We argue that
forgetting occurs disproportionately across time and frequency, depending on
the model and scenarios. By selectively focusing on specific time-frequency
ranges during training, we achieve samples with higher aesthetic quality and
lower noise. We validate this improvement by applying our time-frequency
selective approach to diverse settings, including gradient-based and preference
optimization objectives, as well as both image-level and text-to-image tasks.
Finally, to evaluate both deletion and quality of unlearned data samples, we
propose a simple normalized version of SSCD. Together, our analysis and methods
establish a clearer understanding of the unique challenges in data unlearning
for diffusion models, providing practical strategies to improve both evaluation
and unlearning performance.

</details>


### [69] [Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning](https://arxiv.org/abs/2510.17923)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Wei Ju,Jiancheng Lv,Deng Xiong,Ziyue Qiao*

Main category: cs.LG

TL;DR: COMPASS是一种无监督的测试时奖励机制，结合DCAR和DPR，通过自我监督提升LLMs在连续经验流中的推理能力与性能。


<details>
  <summary>Details</summary>
Motivation: 在缺乏人工标注的奖励数据情形下，探索从未标注数据中学习以提升LLMs的推理能力与可扩展性，并解决自监督奖励估计的可靠性问题。

Method: 提出COMPASS框架，包含Dual-Calibration Answer Reward (DCAR)与Decisive Path Reward (DPR)两部分；DCAR通过置信度与可信度的校准来产生可信的伪标签，DPR直接优化推理过程的质量（超越仅对结果的监督）；两者在测试时结合，推动一致性答案和决定性推理链的自监督强化。

Result: 在多种推理任务与模型架构上取得显著且一致的性能提升，表明该自监督奖励机制具有良好的可扩展性与有效性。

Conclusion: COMPASS提供了一种可扩展的自监督奖励机制，通过伪标签的校准与决定性推理路径的优化，提升LLM在连续经验流上的学习效果。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing
Large Language Models (LLMs), achieving remarkable performance in complex
reasoning domains such as mathematics and code generation. However, current RL
methods face a fundamental scalability bottleneck due to their heavy reliance
on human-curated preference data or labeled datasets for reward modeling. To
overcome this limitation, we explore RL on unlabeled data where models learn
autonomously from continuous experience streams. The core challenge in this
setting lies in reliable reward estimation without ground-truth supervision.
Existing approaches like Test-Time RL address this through self-consistent
consensus, but risk reinforcing incorrect pseudo-labels derived from majority
voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel
test-time reward mechanism that operates without external supervision. COMPASS
integrates two complementary components: the Dual-Calibration Answer Reward
(DCAR), which stabilizes training by establishing trustworthy pseudo-labels
through confidence and credibility calibration, and the Decisive Path Reward
(DPR), which directly optimizes the reasoning process quality beyond mere
outcome supervision. By jointly reinforcing trustworthy consensus answers and
highly decisive reasoning chains, the COMPASS systematically enhances the
model's analytical capabilities. Extensive experiments show that COMPASS
achieves significant and consistent performance gains across diverse reasoning
tasks and model architectures, advancing a more scalable direction for LLMs to
learn from continuous experience.

</details>


### [70] [EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning](https://arxiv.org/abs/2510.17928)
*He Du,Bowen Li,Aijun Yang,Siyang He,Qipeng Guo,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出一个任务无关、进化式的数据合成框架，用于生成可验证训练数据，并在 RLVR 与模型蒸馏中展示出良好泛化性与跨域有效性。


<details>
  <summary>Details</summary>
Motivation: 可靠且可验证的数据是强化学习奖励稳定性、蒸馏跨域迁移能力的关键，但幻觉性生成、验证痕迹薄弱导致难以泛化；现有方法依赖任务特定启发式或后处理过滤，缺乏普适的可验证性评估。

Method: 提出一个进化式、任务无关、策略引导、可执行性可检验的数据合成框架，从最小种子监督出发，联合合成问题、多样候选解和验证痕迹，并通过一致性评估器迭代发现策略，强制人类注释与策略诱导的检查结果一致。

Result: 该框架将过滤提升为原则性的合成，能够组装连贯且可验证的训练实例，具跨域泛化能力。实验表明，在 RLVR 与模型蒸馏范式下，所合成数据显著提升 LiveCodeBench 与 AgentBench-OS 等任务的表现，体现出框架的鲁棒泛化。

Conclusion: 该方法展示了可验证数据的有效性与跨域泛化的强鲁棒性，提供了一种普适的数据合成范式，减少对领域特定规则的依赖。

Abstract: Reliable verifiable data has become a key driver of capability gains in
modern language models, enabling stable reinforcement learning with verifiable
rewards and effective distillation that transfers competence across math,
coding, and agentic tasks. Yet constructing generalizable synthetic verifiable
data remains difficult due to hallucination-prone generation, and weak or
trivial verification artifacts that fail to separate strong from weak
solutions. Existing approaches often rely on task-specific heuristics or
post-hoc filters that do not transfer across domains and lack a principled,
universal evaluator of verifiability. In this work, we introduce an
evolutionary, task-agnostic, strategy-guided, executably-checkable data
synthesis framework that, from minimal seed supervision, jointly synthesizes
problems, diverse candidate solutions, and verification artifacts, and
iteratively discovers strategies via a consistency-based evaluator that
enforces agreement between human-annotated and strategy-induced checks. This
pipeline upgrades filtering into principled synthesis: it reliably assembles
coherent, verifiable training instances and generalizes without domain-specific
rules. Our experiments demonstrate the effectiveness of the proposed approach
under both RLVR and model distillation training paradigms. The results show
that training with our synthesized data yields significant improvements on both
the LiveCodeBench and AgentBench-OS tasks, highlighting the robust
generalization of our framework.

</details>


### [71] [From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference](https://arxiv.org/abs/2510.17933)
*Xiangbo Deng,Cheng Chen,Peng Yang*

Main category: cs.LG

TL;DR: 两阶段 Param-CPD：先用仿真推理训练的神经后验估计器在参数空间估计控参数，再对参数轨迹应用标准CPD；在 Lorenz-63 的分段常参数下优于观测空间的方法。


<details>
  <summary>Details</summary>
Motivation: 在混沌时间序列中，观测变量的内生波动与外部驱动使得直接观测空间的变点检测困难。通过在物理可解释的参数空间进行变点检测，有望获得更清晰、可解释的信号。

Method: 两阶段框架：1) 使用基于仿真的神经后验估计器来估计控制参数的后验分布；2) 对得到的参数轨迹应用标准的变点检测算法。

Result: 在 Lorenz-63 的分段常参数情形下，Param-CPD 提高 F1、降低定位误差、减少误报；可验证后验的可识别性和校准性；对容忍度、滑动窗口长度、噪声的鲁棒性分析显示一致优势。

Conclusion: 在物理可解释的参数空间进行变点检测，能够在非线性动力系统中实现更准确、可解释的变点检测。

Abstract: Detecting regime shifts in chaotic time series is hard because
observation-space signals are entangled with intrinsic variability. We propose
Parameter--Space Changepoint Detection (Param--CPD), a two--stage framework
that first amortizes Bayesian inference of governing parameters with a neural
posterior estimator trained by simulation-based inference, and then applies a
standard CPD algorithm to the resulting parameter trajectory. On Lorenz--63
with piecewise-constant parameters, Param--CPD improves F1, reduces
localization error, and lowers false positives compared to observation--space
baselines. We further verify identifiability and calibration of the inferred
posteriors on stationary trajectories, explaining why parameter space offers a
cleaner detection signal. Robustness analyses over tolerance, window length,
and noise indicate consistent gains. Our results show that operating in a
physically interpretable parameter space enables accurate and interpretable
changepoint detection in nonlinear dynamical systems.

</details>


### [72] [UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts](https://arxiv.org/abs/2510.17937)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.LG

TL;DR: 提出 UniRL-Zero：一个统一的强化学习框架，整合多模态理解、推理与扩散生成，在同一模型中实现理解与生成的协同，并给出六种场景及系统基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在理解与生成之间缺乏统一的强化学习框架，以及跨模态协同能力不足的问题，提升多模态理解、推理和生成的统一优化与交互能力。

Method: 提出六种统一模型强化学习场景，覆盖理解、生成及其交互；构建可复用的基线集合和统一训练目标，以及评估协议，便于对统一理解与生成模型的 RL 研究进行对比；代码实现并开源。

Result: 建立六个场景的系统性基线，为统一理解和生成模型的 RL 研究提供可复现的评估基准，促进多模态能力的统一提升。

Conclusion: UniRL-Zero 为统一模态理解与生成的 RL 研究提供了一个统一框架与基线，预计推动跨模态协同能力与应用潜力的提升；代码已公开。

Abstract: We present UniRL-Zero, a unified reinforcement learning (RL) framework that
boosts, multimodal language model understanding and reasoning, diffusion model
multimedia generation, and their beneficial interaction capabilities within a
unified model. Our work defines six scenarios for unified model reinforcement
learning, providing systematic baselines for reinforcement learning of unified
understanding and generation model. Our code is available at
https://github.com/G-U-N/UniRL.

</details>


### [73] [Demystifying Transition Matching: When and Why It Can Beat Flow Matching](https://arxiv.org/abs/2510.17991)
*Jaihoon Kim,Rajarshi Saha,Minhyuk Sung,Youngsuk Park*

Main category: cs.LG

TL;DR: 在单峰高斯分布下，Transition Matching (TM) 相对 Flow Matching (FM) 在有限步数下给出更低的 KL，且在高斯混合及模式分离时 TM 也具优势；当方差变小时，TM 与 FM 的差异减弱。总体而言，TM 在模式分离且方差不小的情形优于 FM，并在一定计算预算下收敛更快；通过理论推导与控制实验验证。


<details>
  <summary>Details</summary>
Motivation: 明确何时以及为何 TM 能优于 FM，给出理论与经验上对两种采样方法的对比，以便在图像/视频生成等任务中选择更高效的采样策略。

Method: 对单峰高斯目标进行有限步的 KL 推导，证明 TM 的随机差分潜在更新能保持协方差并低估 FM；推导收敛速率并比较固定计算预算下的效率；将分析扩展至高斯混合，识别局部单峰区域并研究均值分离度对 TM 相对于 FM 的影响；在高斯分布的合成数据与真实图像/视频数据上进行实验验证。

Result: 在单峰高斯下，TM 在有限步数内获得严格更低的 KL；在固定计算预算下，TM 收敛速度快于 FM；在高斯混合的情形，若模式分离良好且方差不小，TM 也优于 FM；当目标方差趋近于零时，TM 更新趋近于 FM，优势消失；实验证明理论结论并扩展到图像与视频生成任务。

Conclusion: TM 相对于 FM 的优势来自于随机差分更新对协方差的保持以及对模式结构的更好保留，且在模式明显分离且方差非微小时最为显著。选择 TM 时应关注目标分布的方差和分量间的距离，以及给定计算预算下的收敛需求。

Abstract: Flow Matching (FM) underpins many state-of-the-art generative models, yet
recent results indicate that Transition Matching (TM) can achieve higher
quality with fewer sampling steps. This work answers the question of when and
why TM outperforms FM. First, when the target is a unimodal Gaussian
distribution, we prove that TM attains strictly lower KL divergence than FM for
finite number of steps. The improvement arises from stochastic difference
latent updates in TM, which preserve target covariance that deterministic FM
underestimates. We then characterize convergence rates, showing that TM
achieves faster convergence than FM under a fixed compute budget, establishing
its advantage in the unimodal Gaussian setting. Second, we extend the analysis
to Gaussian mixtures and identify local-unimodality regimes in which the
sampling dynamics approximate the unimodal case, where TM can outperform FM.
The approximation error decreases as the minimal distance between component
means increases, highlighting that TM is favored when the modes are well
separated. However, when the target variance approaches zero, each TM update
converges to the FM update, and the performance advantage of TM diminishes. In
summary, we show that TM outperforms FM when the target distribution has
well-separated modes and non-negligible variances. We validate our theoretical
results with controlled experiments on Gaussian distributions, and extend the
comparison to real-world applications in image and video generation.

</details>


### [74] [Attention-Guided Deep Adversarial Temporal Subspace Clustering (A-DATSC) Model for multivariate spatiotemporal data](https://arxiv.org/abs/2510.18004)
*Francis Ndikum Nji,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: A-DATSC is an adversarial deep subspace clustering framework for 4D multivariate spatiotemporal data, combining TimeDistributed ConvLSTM2D-based generator with a graph-attention self-expressive network and a quality discriminator, achieving superior clustering on three real datasets.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing deep subspace clustering methods: use of shallow autoencoders that ignore clustering errors, focus on global rather than local structure, inability to model long-range dependencies and positional information, and lack of application to 4D spatiotemporal data.

Method: An adversarial architecture comprising a generator and a discriminator. The generator adopts a U-Net-inspired design with stacked TimeDistributed ConvLSTM2D layers to preserve spatiotemporal integrity while reducing parameters. A graph attention transformer–based self-expressive network captures local spatial relationships, global dependencies, and short- and long-range correlations.

Result: Experiments on three real-world multivariate spatiotemporal datasets show that A-DATSC achieves substantially superior clustering performance compared to state-of-the-art deep subspace clustering models.

Conclusion: The proposed A-DATSC framework effectively addresses key limitations of existing deep subspace clustering methods for 4D spatiotemporal data and demonstrates the benefits of integrating adversarial temporal subspace clustering with attention-guided self-expression.

Abstract: Deep subspace clustering models are vital for applications such as snowmelt
detection, sea ice tracking, crop health monitoring, infectious disease
modeling, network load prediction, and land-use planning, where multivariate
spatiotemporal data exhibit complex temporal dependencies and reside on
multiple nonlinear manifolds beyond the capability of traditional clustering
methods. These models project data into a latent space where samples lie in
linear subspaces and exploit the self-expressiveness property to uncover
intrinsic relationships. Despite their success, existing methods face major
limitations: they use shallow autoencoders that ignore clustering errors,
emphasize global features while neglecting local structure, fail to model
long-range dependencies and positional information, and are rarely applied to
4D spatiotemporal data. To address these issues, we propose A-DATSC
(Attention-Guided Deep Adversarial Temporal Subspace Clustering), a model
combining a deep subspace clustering generator and a quality-verifying
discriminator. The generator, inspired by U-Net, preserves spatial and temporal
integrity through stacked TimeDistributed ConvLSTM2D layers, reducing
parameters and enhancing generalization. A graph attention transformer based
self-expressive network captures local spatial relationships, global
dependencies, and both short- and long-range correlations. Experiments on three
real-world multivariate spatiotemporal datasets show that A-DATSC achieves
substantially superior clustering performance compared to state-of-the-art deep
subspace clustering models.

</details>


### [75] [Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity](https://arxiv.org/abs/2510.18037)
*Ziyu Lu,Anna J. Li,Alexander E. Ladd,Pascha Matveev,Aditya Deole,Eric Shea-Brown,J. Nathan Kutz,Nicholas A. Steinmetz*

Main category: cs.LG

TL;DR: Eight probabilistic deep learning models (including two foundation models) benchmarked against classical models for forecasting spontaneous mouse cortical activity; deep models outperform baselines and provide informative forecasts up to 1.5 seconds.


<details>
  <summary>Details</summary>
Motivation: Advance neural activity forecasting to enable closed-loop control and to illuminate the intrinsic temporal structure of neural activity, addressing the gap between modern DL forecasting and neuroscience data.

Method: Systematic benchmark of eight probabilistic deep learning models (two foundation models) against four classical statistical models and two baselines using widefield imaging of spontaneous mouse cortex activity; evaluation across multiple prediction horizons.

Result: Several deep learning models consistently outperform classical methods; the best model yields informative forecasts up to 1.5 seconds into the future, suggesting strong potential for control applications and new insights into neural temporal dynamics.

Conclusion: Probabilistic DL models show promise for neural activity forecasting, especially foundation models, and may enable future closed-loop control and deeper exploration of temporal structure in neural activity.

Abstract: Neural activity forecasting is central to understanding neural systems and
enabling closed-loop control. While deep learning has recently advanced the
state-of-the-art in the time series forecasting literature, its application to
neural activity forecasting remains limited. To bridge this gap, we
systematically evaluated eight probabilistic deep learning models, including
two foundation models, that have demonstrated strong performance on general
forecasting benchmarks. We compared them against four classical statistical
models and two baseline methods on spontaneous neural activity recorded from
mouse cortex via widefield imaging. Across prediction horizons, several deep
learning models consistently outperformed classical approaches, with the best
model producing informative forecasts up to 1.5 seconds into the future. Our
findings point toward future control applications and open new avenues for
probing the intrinsic temporal structure of neural activity.

</details>


### [76] [Cross-Domain Long-Term Forecasting: Radiation Dose from Sparse Neutron Sensor via Spatio-Temporal Operator Network](https://arxiv.org/abs/2510.18041)
*Jay Phil Yoo,Kazuma Kobayashi,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: STÖNE (Spatio-Temporal Operator Network) 是一种非自回归的神经算子，学习跨异质域的稳定映射，实现从稀疏传感数据到目标场的跨域时空预测。通过对地基中子测量的推断，能够在长时间尺度（180 天）内实现高精度预测，推理延迟只有毫秒级，且无需域对齐或自回归传播。


<details>
  <summary>Details</summary>
Motivation: 在稀疏且跨域的传感数据情境下，预测不可观测的物理量是一个核心难题。现有神经算子和大规模预测模型依赖密集、同域的输入输出场和短时间上下文，在真实系统中传感和预测常在不同物理域上，且时间尺度较长，因此难以直接应用。

Method: 提出 STONe，一种非自回归的神经算子，学习在异质域之间的稳定函数映射。它在传感器域与目标域之间定义一个非线性算子，且在较长预测时域内保持稳定性，无需迭代递归。以全球 23 年的中子数据进行训练。

Result: 在地基中子数据上训练后，STONe 实现了对高空辐射剂量场的 180 天准确预测，推理时间仅为 ms 级；证明算子学习可以在非共享域设定下泛化，并给出跨域算子推断的一般性原则，用于物理、气候与能源系统的实时预测。

Conclusion: 该框架确立了一般性的跨域算子推断原则，能够实现对复杂时空场的实时预测，适用于物理、气候和能源系统中的跨域跨域任务。

Abstract: Forecasting unobservable physical quantities from sparse, cross-domain sensor
data is a central unsolved problem in scientific machine learning. Existing
neural operators and large-scale forecasters rely on dense, co-located
input-output fields and short temporal contexts, assumptions that fail in
real-world systems where sensing and prediction occur on distinct physical
manifolds and over long timescales. We introduce the Spatio-Temporal Operator
Network (STONe), a non-autoregressive neural operator that learns a stable
functional mapping between heterogeneous domains. By directly inferring
high-altitude radiation dose fields from sparse ground-based neutron
measurements, STONe demonstrates that operator learning can generalize beyond
shared-domain settings. It defines a nonlinear operator between sensor and
target manifolds that remains stable over long forecasting horizons without
iterative recurrence. This challenges the conventional view that operator
learning requires domain alignment or autoregressive propagation. Trained on 23
years of global neutron data, STONe achieves accurate 180-day forecasts with
millisecond inference latency. The framework establishes a general principle
for cross-domain operator inference, enabling real-time prediction of complex
spatiotemporal fields in physics, climate, and energy systems.

</details>


### [77] [Measure-Theoretic Anti-Causal Representation Learning](https://arxiv.org/abs/2510.18052)
*Arman Behnam,Binghui Wang*

Main category: cs.LG

TL;DR: ACIA is a measure-theoretic framework for anti-causal representation learning with a two-level design using interventional kernels to handle prefect/imperfect interventions, no explicit causal structure required, strong OOD generalization guarantees; empirically superior on synthetic/medical datasets.


<details>
  <summary>Details</summary>
Motivation: In anti-causal settings, labels cause features; existing methods struggle with interventions and high-dimensional data; need a framework that provides invariance across environments and theoretical OOD guarantees.

Method: Two-level design: low-level representations model how labels generate observations; high-level representations isolate stable causal patterns across environments; use interventional kernels; measure-theoretic foundation; does not rely on explicit causal graphs; accommodates prefect and imperfect interventions; works with high-dimensional data.

Result: Empirical results show ACIA outperforms state-of-the-art in accuracy and invariance on synthetic and real-world medical datasets; theoretical results give tight bounds on training vs unseen environment performance gaps.

Conclusion: ACIA enables robust anti-causal representation learning with strong generalization guarantees, addressing limitations of prior methods and applicable to high-dimensional data and varying interventions.

Abstract: Causal representation learning in the anti-causal setting (labels cause
features rather than the reverse) presents unique challenges requiring
specialized approaches. We propose Anti-Causal Invariant Abstractions (ACIA), a
novel measure-theoretic framework for anti-causal representation learning. ACIA
employs a two-level design, low-level representations capture how labels
generate observations, while high-level representations learn stable causal
patterns across environment-specific variations. ACIA addresses key limitations
of existing approaches by accommodating prefect and imperfect interventions
through interventional kernels, eliminating dependency on explicit causal
structures, handling high-dimensional data effectively, and providing
theoretical guarantees for out-of-distribution generalization. Experiments on
synthetic and real-world medical datasets demonstrate that ACIA consistently
outperforms state-of-the-art methods in both accuracy and invariance metrics.
Furthermore, our theoretical results establish tight bounds on performance gaps
between training and unseen environments, confirming the efficacy of our
approach for robust anti-causal learning.

</details>


### [78] [Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models](https://arxiv.org/abs/2510.18053)
*Jiajun Fan,Tong Wei,Chaoran Cheng,Yuxin Chen,Ge Liu*

Main category: cs.LG

TL;DR: Adaptive Divergence Regularized Policy Optimization (ADRPO) dynamically tunes regularization strength in RL-based fine-tuning of generative models by using advantage estimates, improving exploration-exploitation balance and landing superior performance across text-to-image, LLM fine-tuning, and multi-modal reasoning tasks without sacrificing generation diversity.


<details>
  <summary>Details</summary>
Motivation: Fixed-divergence regularization forces a trade-off between preserving model capabilities and maximizing rewards. ADRPO aims to automatically adjust regularization strength based on the value of samples to navigate the exploration-exploitation dilemma.

Method:  ADRPO reduces regularization for high-value (advantageous) samples and strengthens it for low-value samples. It employs Wasserstein-2 regularization for flow matching in generative models, applied to text-to-image generation, and extends to KL-regularized fine-tuning for LLMs and multi-modal reasoning. It also demonstrates compatibility with online RL methods like GRPO.

Result: ADRPO achieves superior semantic alignment and diversity in text-to-image generation compared with offline DPO and online fixed-regularization methods; a 2B SD3 model surpasses larger 4.8B and 12B models in attribute binding, semantic consistency, and style transfer while preserving diversity; in LLM and multi-modal tasks, ADRPO enables emergent exploration and outperforms larger models on reasoning benchmarks, including a 7B model rivaling sizable commercial models.

Conclusion: ADRPO provides a general, plug-and-play solution to the exploration-exploitation challenge across diverse generative architectures and modalities, improving performance while maintaining generation quality and diversity.

Abstract: Balancing exploration and exploitation during reinforcement learning
fine-tuning of generative models presents a critical challenge, as existing
approaches rely on fixed divergence regularization that creates an inherent
dilemma: strong regularization preserves model capabilities but limits reward
optimization, while weak regularization enables greater alignment but risks
instability or reward hacking. We introduce Adaptive Divergence Regularized
Policy Optimization (ADRPO), which automatically adjusts regularization
strength based on advantage estimates-reducing regularization for high-value
samples while applying stronger regularization to poor samples, enabling
policies to navigate between exploration and aggressive exploitation according
to data quality. Our implementation with Wasserstein-2 regularization for flow
matching generative models achieves remarkable results on text-to-image
generation, achieving better semantic alignment and diversity than offline
methods like DPO and online methods with fixed regularization like ORW-CFM-W2.
ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B
and 12B parameters in attribute binding, semantic consistency, artistic style
transfer, and compositional control while maintaining generation diversity.
ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and
multi-modal reasoning models, enhancing existing online RL methods like GRPO.
In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local
optima through active exploration, while in multi-modal audio reasoning, it
outperforms GRPO through superior step-by-step reasoning, enabling a 7B model
to outperform substantially larger commercial models including Gemini 2.5 Pro
and GPT-4o Audio, offering an effective plug-and-play solution to the
exploration-exploitation challenge across diverse generative architectures and
modalities.

</details>


### [79] [Fine-tuning Flow Matching Generative Models with Intermediate Feedback](https://arxiv.org/abs/2510.18072)
*Jiajun Fan,Chaoran Cheng,Shuaike Shen,Xiangxin Zhou,Ge Liu*

Main category: cs.LG

TL;DR: AC-Flow 提出了一种鲁棒的 actor-critic 框架，用于 flow-based 的文本到图像生成的微调，通过 reward shaping、双稳定机制和通用 critic 加权实现稳定学习与多样性保护，在 Stable Diffusion 3 上达到文本-图像对齐的 state-of-the-art，并能泛化到未见过的人类偏好模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法多仅从结果奖励学习，难以解决 credit assignment 问题；直接对累积奖励回归训练 critic 在在线设置易造成训练不稳定和模型塌陷；需要更稳定的中间价值学习与可扩展的 critic。

Method: 提出 AC-Flow 的三大创新：1) reward shaping：提供归一化的学习信号以实现稳定的中间值学习和梯度控制；2) dual-stability 机制：通过优势裁剪防止破坏性策略更新，并在早期让 critic 成熟后再影响 actor；3) scalable generalized critic weighting：扩展传统的 reward-weighted 方法并通过 Wasserstein 正则化保持模型多样性。

Result: 在 Stable Diffusion 3 的广泛实验表明，AC-Flow 在文本-图像对齐任务上达到 state-of-the-art，并能泛化到未见过的人类偏好模型；即使 critic 模型较小，也能实现稳健微调而不损害生成质量、多样性或稳定性。

Conclusion: AC-Flow 提供一个鲁棒的 actor-critic 框架，解决基于中间信号的学习难题，具备可扩展的 critic 加权和稳定策略，能够在流式模型上实现高质量的文本-图像对齐与对未知偏好模型的泛化。

Abstract: Flow-based generative models have shown remarkable success in text-to-image
generation, yet fine-tuning them with intermediate feedback remains
challenging, especially for continuous-time flow matching models. Most existing
approaches solely learn from outcome rewards, struggling with the credit
assignment problem. Alternative methods that attempt to learn a critic via
direct regression on cumulative rewards often face training instabilities and
model collapse in online settings. We present AC-Flow, a robust actor-critic
framework that addresses these challenges through three key innovations: (1)
reward shaping that provides well-normalized learning signals to enable stable
intermediate value learning and gradient control, (2) a novel dual-stability
mechanism that combines advantage clipping to prevent destructive policy
updates with a warm-up phase that allows the critic to mature before
influencing the actor, and (3) a scalable generalized critic weighting scheme
that extends traditional reward-weighted methods while preserving model
diversity through Wasserstein regularization. Through extensive experiments on
Stable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art
performance in text-to-image alignment tasks and generalization to unseen human
preference models. Our results demonstrate that even with a computationally
efficient critic model, we can robustly finetune flow models without
compromising generative quality, diversity, or stability.

</details>


### [80] [R2L: Reliable Reinforcement Learning: Guaranteed Return & Reliable Policies in Reinforcement Learning](https://arxiv.org/abs/2510.18074)
*Nadir Farhi*

Main category: cs.LG

TL;DR: A framework for reliable RL by turning probabilistic-return constraints into a standard RL problem via state augmentation; uses conventional algorithms to maximize probability of exceeding a threshold; demonstrated on reliable routing with positive results.


<details>
  <summary>Details</summary>
Motivation: Classical RL optimizes expected return but provides no guarantees on risk or reliability. Many real-world tasks require a high probability of meeting a performance threshold, such as deadlines, budgets, or safety constraints.

Method: Introduce a state-augmented formulation that recasts the objective as maximizing the probability that cumulative return exceeds a threshold. This converts the problem into a standard RL setting, enabling use of existing methods (e.g., Q-learning, Dueling Double DQN) with appropriate adaptations. Prove theoretical equivalence between the two formulations and apply to tasks like reliable routing.

Result: Theoretical results establish the equivalence between the reliable-RL formulation and the standard RL formulation under the augmentation. Numerical experiments show that the resulting policies achieve a better balance between efficiency and reliability in routing tasks, confirming viability of the approach.

Conclusion: Reliable RL can be effectively achieved within the existing RL framework by state augmentation, allowing practitioners to obtain performance guarantees without inventing new algorithms; the approach is particularly relevant to stochastic and safety-critical applications.

Abstract: In this work, we address the problem of determining reliable policies in
reinforcement learning (RL), with a focus on optimization under uncertainty and
the need for performance guarantees. While classical RL algorithms aim at
maximizing the expected return, many real-world applications - such as routing,
resource allocation, or sequential decision-making under risk - require
strategies that ensure not only high average performance but also a guaranteed
probability of success. To this end, we propose a novel formulation in which
the objective is to maximize the probability that the cumulative return exceeds
a prescribed threshold. We demonstrate that this reliable RL problem can be
reformulated, via a state-augmented representation, into a standard RL problem,
thereby allowing the use of existing RL and deep RL algorithms without the need
for entirely new algorithmic frameworks. Theoretical results establish the
equivalence of the two formulations and show that reliable strategies can be
derived by appropriately adapting well-known methods such as Q-learning or
Dueling Double DQN. To illustrate the practical relevance of the approach, we
consider the problem of reliable routing, where the goal is not to minimize the
expected travel time but rather to maximize the probability of reaching the
destination within a given time budget. Numerical experiments confirm that the
proposed formulation leads to policies that effectively balance efficiency and
reliability, highlighting the potential of reliable RL for applications in
stochastic and safety-critical environments.

</details>


### [81] [Batch Distillation Data for Developing Machine Learning Anomaly Detection Methods](https://arxiv.org/abs/2510.18075)
*Justus Arweiler,Indra Jungjohann,Aparna Muraleedharan,Heike Leitte,Jakob Burger,Kerstin Münnemann,Fabian Jirasek,Hans Hasse*

Main category: cs.LG

TL;DR: 构建并公开一套实验室规模的批量蒸馏装置数据集，用于多模态ML异常检测研究，包含故障与无故障对照、NMR、视频/音频等数据以及异常本体和丰富元数据。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏公开的化工过程ML异常检测数据集，难以训练、评估和解释ML方法；通过系统化实验与多模态数据、异常本体，填补数据空白并促进可解释的异常检测研究。

Method: 搭建实验室规模的批量蒸馏装置，开展119次实验，覆盖多种工况与混合物；在存在异常与对应无异常的条件下对比实验；采集时间序列传感器/执行器数据、在线NMR浓度、视频和音频等，并给出测量不确定度；开发异常本体并做元数据整理，将数据组织成结构化数据库并对外公开（含注释及元数据）。

Result: 提供一个包含大量故障与无故障配对、多模态数据和专家标注的公开数据集，便于训练、评估和解释性ML在化工过程的异常检测应用；且数据集强调异常原因信息，有助于可解释性和异常缓解研究。

Conclusion: 该数据集将推动先进的ML异常检测方法的发展，并支持基于异常原因的可解释性分析与缓解策略的研究。

Abstract: Machine learning (ML) holds great potential to advance anomaly detection (AD)
in chemical processes. However, the development of ML-based methods is hindered
by the lack of openly available experimental data. To address this gap, we have
set up a laboratory-scale batch distillation plant and operated it to generate
an extensive experimental database, covering fault-free experiments and
experiments in which anomalies were intentionally induced, for training
advanced ML-based AD methods. In total, 119 experiments were conducted across a
wide range of operating conditions and mixtures. Most experiments containing
anomalies were paired with a corresponding fault-free one. The database that we
provide here includes time-series data from numerous sensors and actuators,
along with estimates of measurement uncertainty. In addition, unconventional
data sources -- such as concentration profiles obtained via online benchtop NMR
spectroscopy and video and audio recordings -- are provided. Extensive metadata
and expert annotations of all experiments are included. The anomaly annotations
are based on an ontology developed in this work. The data are organized in a
structured database and made freely available via
doi.org/10.5281/zenodo.17395544. This new database paves the way for the
development of advanced ML-based AD methods. As it includes information on the
causes of anomalies, it further enables the development of interpretable and
explainable ML approaches, as well as methods for anomaly mitigation.

</details>


### [82] [MEG-GPT: A transformer-based foundation model for magnetoencephalography data](https://arxiv.org/abs/2510.18080)
*Rukuang Huang,Sungjun Cho,Chetan Gohil,Oiwi Parker Jones,Mark Woolrich*

Main category: cs.LG

TL;DR: 提出 MEG-GPT：基于 Transformer 的脑信号基础模型，结合时间注意力与下一时点预测，利用数据驱动的 tokeniser 处理连续 MEG 数据，能生成具有真实时空-谱特征的数据并提升跨会话/跨被试的下游解码性能。


<details>
  <summary>Details</summary>
Motivation: 需要有效建模大规模脑动力学的复杂时空结构；传统方法难以捕捉 MEG 的高时间分辨率和个体变异， foundation 模型有潜力提升通用性和泛化性。

Method: 提出 MEG-GPT，使用时间注意力和下一时点预测；设计数据驱动的 tokeniser 保留高时间分辨率；在 Cam-CAN 数据集（N=612，眼闭 Rest）上训练，评估生成数据及下游解码任务，支持在小型标注数据集上微调以提升跨被试性能。

Result: 模型能生成具有真实时空-谱特征的 MEG 数据，包含瞬变事件和群体变异；在下游解码中实现零-shot泛化提升：跨会话准确率从 0.54 提升至 0.59，跨被试从 0.41 提升至 0.49；可通过微调在较小数据集上提升跨被试解码。

Conclusion: 为脑电生理数据建立了强大的基础模型，为脑科学计算和神经解码等应用奠定基础。

Abstract: Modelling the complex spatiotemporal patterns of large-scale brain dynamics
is crucial for neuroscience, but traditional methods fail to capture the rich
structure in modalities such as magnetoencephalography (MEG). Recent advances
in deep learning have enabled significant progress in other domains, such as
language and vision, by using foundation models at scale. Here, we introduce
MEG-GPT, a transformer based foundation model that uses time-attention and next
time-point prediction. To facilitate this, we also introduce a novel
data-driven tokeniser for continuous MEG data, which preserves the high
temporal resolution of continuous MEG signals without lossy transformations. We
trained MEG-GPT on tokenised brain region time-courses extracted from a
large-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that
the learnt model can generate data with realistic spatio-spectral properties,
including transient events and population variability. Critically, it performs
well in downstream decoding tasks, improving downstream supervised prediction
task, showing improved zero-shot generalisation across sessions (improving
accuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49)
compared to a baseline methods. Furthermore, we show the model can be
efficiently fine-tuned on a smaller labelled dataset to boost performance in
cross-subject decoding scenarios. This work establishes a powerful foundation
model for electrophysiological data, paving the way for applications in
computational neuroscience and neural decoding.

</details>


### [83] [Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth](https://arxiv.org/abs/2510.18081)
*Jiawei Zhang,Andrew Estornell,David D. Baek,Bo Li,Xiaojun Xu*

Main category: cs.LG

TL;DR: ADA是一种推理时防御方法，通过在生成过程中的中途重新引入助手头部令牌，使模型在任意深度生成中重新评估有害性并拒绝，从而在不修改模型参数的前提下实现近100%拒绝率和对对抗性提示的低成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在开头就能拒绝有害内容，但一旦生成进入深度阶段，保护会崩溃；需要一种无需重新训练、可在推理时生效的通用防护，确保任深度的安全性。

Method: 基于观察：对齐集中在助手头部令牌，并通过浅层拒绝训练反复使用这些令牌。ADA在生成中重新注入这些令牌，使模型在流中重新评估有害性并恢复拒绝。覆盖多种开源模型家族，参数不变，推理时开销极小。

Result: 对多家开放模型几乎实现100%对对抗性预填充攻击的拒绝率，能将GCG、AutoDAN、PAIR、TAP等对抗性提示攻击的平均成功率降至小于3%；在 benign任务上保持实用性，且过拒绝极少；对后续指令调整（ benign 或对抗性）仍具鲁棒性。

Conclusion: ADA提供一种无参数修改的任深度安全防御，具广泛适用性，能够与现有对齐策略互补，显著提升在深度生成中的安全性与鲁棒性。

Abstract: Large Language Models (LLMs) exhibit strong but shallow alignment: they
directly refuse harmful queries when a refusal is expected at the very start of
an assistant turn, yet this protection collapses once a harmful continuation is
underway (either through the adversarial attacks or via harmful
assistant-prefill attacks). This raises a fundamental question: Can the innate
shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation
depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an
effective inference-time defense with negligible overhead. ADA is built based
on our observation that alignment is concentrated in the assistant header
tokens through repeated use in shallow-refusal training, and these tokens
possess the model's strong alignment priors. By reintroducing these tokens
mid-stream, ADA induces the model to reassess harmfulness and recover refusals
at any point in generation. Across diverse open-source model families (Llama,
Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety
performance without requiring any changes to the base model's parameters. It
secures a near-100% refusal rate against challenging adversarial prefill
attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces
the average success rate of prominent adversarial prompt attacks (such as GCG,
AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving
utility on benign tasks with minimal over-refusal. ADA maintains this
resilience even after the base model undergoes subsequent instruction tuning
(benign or adversarial).

</details>


### [84] [Provably Optimal Reinforcement Learning under Safety Filtering](https://arxiv.org/abs/2510.18082)
*Donggeon David Oh,Duy P. Nguyen,Haimin Hu,Jaime F. Fisac*

Main category: cs.LG

TL;DR: 提出在安全过滤器足够宽松的前提下进行强化学习，不会降低渐近性能。通过将安全性形式化为安全关键的 MDP（SC-MDP）以及一个由安全过滤器驱动的过滤 MDP，给出一个主定理：在过滤 MDP 中学习是分类上安全的；RL 收敛性保留；且在同一过滤器执行下，过滤 MDP 的最优策略的渐近回报等同于 SC-MDP 中最优的安全策略。并在 Safety Gymnasium 验证，训练过程零违规，最终性能与未过滤基线相当或更好。结论：安全执行与性能优化可实现完全分离，推荐的做法是使用尽可能宽松的安全过滤器。


<details>
  <summary>Details</summary>
Motivation: 缓解在安全关键任务中对强化学习的安全保障不足，以及对安全过滤器可能削弱学习与收敛的普遍担忧。通过形式化框架消除安全性与性能之间的固有权衡。

Method: 提出SC-MDP（安全关键的马尔可夫决策过程）以实现灾难性失败状态的类别化避免，定义带安全过滤器的过滤 MDP，使所有行动通过环境中的过滤器产生安全效果。给出主定理：在过滤 MDP 中学习是分类安全的，RL 收敛性仍然成立；任一在过滤 MDP 中最优的策略若经同一过滤器执行，其渐近回报等同于在 SC-MDP 中的最佳安全策略。通过 Safety Gymnasium 的代表性任务进行实验验证，观察到训练阶段零违规，最终性能达到或超过未过滤基线。

Result: 理论层面证明安全性、收敛性与渐近最优性的分离；实践验证在 Safety Gymnasium 中实现零违规，且最终表现与未过滤基线相当或更好。

Conclusion: 结论是：安全执行与性能优化的权衡并非必然，使用最宽松的安全过滤器即可实现安全训练与部署且不损害渐近表现。给出一个简单、可执行的安全 RL 流程：在训练与部署时均采用尽可能宽松的安全过滤器。

Abstract: Recent advances in reinforcement learning (RL) enable its use on increasingly
complex tasks, but the lack of formal safety guarantees still limits its
application in safety-critical settings. A common practical approach is to
augment the RL policy with a safety filter that overrides unsafe actions to
prevent failures during both training and deployment. However, safety filtering
is often perceived as sacrificing performance and hindering the learning
process. We show that this perceived safety-performance tradeoff is not
inherent and prove, for the first time, that enforcing safety with a
sufficiently permissive safety filter does not degrade asymptotic performance.
We formalize RL safety with a safety-critical Markov decision process (SC-MDP),
which requires categorical, rather than high-probability, avoidance of
catastrophic failure states. Additionally, we define an associated filtered MDP
in which all actions result in safe effects, thanks to a safety filter that is
considered to be a part of the environment. Our main theorem establishes that
(i) learning in the filtered MDP is safe categorically, (ii) standard RL
convergence carries over to the filtered MDP, and (iii) any policy that is
optimal in the filtered MDP-when executed through the same filter-achieves the
same asymptotic return as the best safe policy in the SC-MDP, yielding a
complete separation between safety enforcement and performance optimization. We
validate the theory on Safety Gymnasium with representative tasks and
constraints, observing zero violations during training and final performance
matching or exceeding unfiltered baselines. Together, these results shed light
on a long-standing question in safety-filtered learning and provide a simple,
principled recipe for safe RL: train and deploy RL policies with the most
permissive safety filter that is available.

</details>


### [85] [Enhancing mortality prediction in cardiac arrest ICU patients through meta-modeling of structured clinical data from MIMIC-IV](https://arxiv.org/abs/2510.18103)
*Nursultan Mamatov,Philipp Kellmeyer*

Main category: cs.LG

TL;DR: 将结构化数据与文本数据（出院小结、放射报告）结合，使用 LASSO、XGBoost 进行特征筛选，随后在顶级特征上训练逻辑回归；引入 TF-IDF 与 BERT 表征的文本信息显著提升预测性能，最终 AUC 达 0.918，结构化数据单独为 0.753，提升约 22%；决策曲线分析显示在多阈值区间具有净收益优势。


<details>
  <summary>Details</summary>
Motivation: 在ICU中需尽早预测住院死亡以便干预与资源分配，且需充分利用结构化数据与临床文本以提升预测性能并实现可解释性。

Method: 首先用 LASSO 与 XGBoost 进行特征选择，然后在两者筛选出的顶级特征上训练多变量逻辑回归。文本特征通过 TF-IDF 与 BERT 表征并整合到模型。数据来源为 MIMIC-IV。

Result: 整合文本与结构化数据的最终逻辑回归模型的 AUC 为 0.918；仅结构化数据的 AUC 为 0.753，提升约 22%。决策曲线显示在阈值 0.2–0.8 范围内具有更高的标准化净收益，指示临床实用性。

Conclusion: 将非结构化临床笔记纳入预测模型可显著提升预测性能，支持将文本信息整合到可解释的风险预测模型中以改善 ICU 患者的早期风险分层与干预策略。

Abstract: Accurate early prediction of in-hospital mortality in intensive care units
(ICUs) is essential for timely clinical intervention and efficient resource
allocation. This study develops and evaluates machine learning models that
integrate both structured clinical data and unstructured textual information,
specifically discharge summaries and radiology reports, from the MIMIC-IV
database. We used LASSO and XGBoost for feature selection, followed by a
multivariate logistic regression trained on the top features identified by both
models. Incorporating textual features using TF-IDF and BERT embeddings
significantly improved predictive performance. The final logistic regression
model, which combined structured and textual input, achieved an AUC of 0.918,
compared to 0.753 when using structured data alone, a relative improvement 22%.
The analysis of the decision curve demonstrated a superior standardized net
benefit in a wide range of threshold probabilities (0.2-0.8), confirming the
clinical utility of the model. These results underscore the added prognostic
value of unstructured clinical notes and support their integration into
interpretable feature-driven risk prediction models for ICU patients.

</details>


### [86] [Latent Discrete Diffusion Models](https://arxiv.org/abs/2510.18114)
*Dario Shariatian,Alain Durmus,Stefano Peluchetti*

Main category: cs.LG

TL;DR: 提出 Latent Discrete Diffusion Models (LDDMs)，将离散令牌的掩码扩散与潜在嵌入的连续扩散耦合，缓解逐位独立逆向传播对联合结构的削弱问题。两种变体：FUJI-LDDMs（令牌与潜在变量全面联合去噪）与 SEQ-LDDMs（先解潜在变量再在条件下解离散链）。给出 ELBO 风格目标并证明在低采样预算下优于现有掩码离散扩散基线，提升无条件生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有掩码去噪扩散的一个普遍局限是逆转转移在位置上通常是因子化的，这削弱了跨-token 的联合结构，也降低了在较少步数下的生成质量。通过在潜在嵌入空间引入连续扩散来提供软信号与跨-token 依赖性，从而提升解码的一致性和质量。

Method: 在对令牌的掩码离散扩散基础上，加入潜在嵌入的连续扩散通道，潜在通道承载跨-token 的依赖信息。提出两种实例：FUJI-LDDMs 完整联合地对令牌和潜在变量进行去噪；SEQ-LDDMs 先解潜在变量再条件性地解离散链。为两种变体给出 ELBO 风格目标，并探讨如何在学习有信息量的潜在变量的同时保持扩散建模的可行性。

Result: 在无条件生成任务上，LDDMs 相比当前最先进的掩码离散扩散基线获得性能提升；在较低采样预算下也具备良好表现，尤其在一次步内需要解开多个令牌时更具优势。

Conclusion: 通过在潜在嵌入空间引入连续扩散，LDDMs 为离散数据的联合结构提供了更柔和且跨-token 的信号，提升了生成质量与效率。未来工作可能关注训练稳定性、潜在通道设计、以及扩展到更大规模的离散数据域。

Abstract: We study discrete diffusion for language and other categorical data and focus
on a common limitation of masked denoisers: reverse transitions typically
factorize across positions, which can weaken joint structure and degrade
quality in few-step generation. We propose \emph{Latent Discrete Diffusion
Models} (LDDMs), which couple a masked discrete diffusion over tokens with a
continuous diffusion over latent embeddings. The latent channel provides a
softer signal and carries cross-token dependencies that help resolve
ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully
joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially
resolve the latent and then the discrete chain conditionally on it. For both
variants we derive ELBO-style objectives and discuss design choices to learn
informative latents yet amenable to diffusoin modeling. In experiments, LDDMs
yield improvements on unconditional generation metrics as compared to
state-of-the-art masked discrete diffusion baselines, and are effective at
lower sampling budgets, where unmasking many tokens per step is desirable.

</details>


### [87] [Gradient Variance Reveals Failure Modes in Flow-Based Generative Models](https://arxiv.org/abs/2510.18118)
*Teodora Reu,Sixtine Dromigny,Michael Bronstein,Francisco Vargas*

Main category: cs.LG

TL;DR: Rectified Flows 的直路径目标在训练-推理之间导致记忆化问题；确定性训练的梯度方差较低使模型记住训练对，而引入小噪声可恢复泛化。


<details>
  <summary>Details</summary>
Motivation: 解释直路径目标为何在某些设置下失效，揭示模型在优化过程对特定向量场的偏好及记忆化现象。

Method: 理论分析：研究高斯到高斯传输，比较随机与确定性梯度估计下的梯度方差，分析优化在不同设置下偏好何种向量场；证明在插值线相交的情形下，推理阶段会回放训练中的特定对；存在记忆化向量场且直路径目标会收敛到该场；在 CelebA 上验证记忆化与噪声干预的影响。

Result: 揭示了在确定性求解下，梯度方差低导致记忆化训练对的现象，且在推理时可重现训练中的对；即便插值线相交也会产生记忆化；注入小噪声可恢复泛化。

Conclusion: 直路径目标存在固有缺陷，需通过引入随机性或修改目标函数来避免记忆化，从而提升泛化能力。

Abstract: Rectified Flows learn ODE vector fields whose trajectories are straight
between source and target distributions, enabling near one-step inference. We
show that this straight-path objective conceals fundamental failure modes:
under deterministic training, low gradient variance drives memorization of
arbitrary training pairings, even when interpolant lines between pairs
intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport
and use the loss gradient variance across stochastic and deterministic regimes
to characterize which vector fields optimization favors in each setting. We
then show that, in a setting where all interpolating lines intersect, applying
Rectified Flow yields the same specific pairings at inference as during
training. More generally, we prove that a memorizing vector field exists even
when training interpolants intersect, and that optimizing the straight-path
objective converges to this ill-defined field. At inference, deterministic
integration reproduces the exact training pairings. We validate our findings
empirically on the CelebA dataset, confirming that deterministic interpolants
induce memorization, while the injection of small noise restores
generalization.

</details>


### [88] [Efficient Long-context Language Model Training by Core Attention Disaggregation](https://arxiv.org/abs/2510.18121)
*Yonghao Zhuang,Junda Chen,Bo Pang,Yi Gu,Yibo Zhu,Yimin Jiang,Ion Stoica,Eric Xing,Hao Zhang*

Main category: cs.LG

TL;DR: 提出 DistCA 的核心注意力解耦（CAD）方案，将核心注意力计算（softmax(QK^T)V）从模型其他部分分离并放在独立设备群上执行，以解决长上下文时的负载不平衡和待机问题；通过将核心注意力分解成 token 级任务、动态重新批处理和对比注意力服务器并行执行，达到更好的资源利用。


<details>
  <summary>Details</summary>
Motivation: 在长上下文下，核心注意力相对于其他组件呈二次增长，导致数据和流水线并行中的负载不平衡和慢点（stragglers）。核心注意力无可训练参数、数据量极小，因此通过调度即可实现负载均衡。

Method: 将核心注意力从模型中分离出来，分配到专用的注意力服务器进行 token 级任务计算。任务可动态重新批处理以实现计算量平衡，利用高效的内核对子批处理进行处理。实现了 DistCA 系统，采用乒乓式执行覆盖通信与计算、就地执行以降低内存占用。

Result: 在 512 张 H200 GPU、上下文长度达到 512k token 的场景下，DistCA 的端到端训练吞吐提升最高可达 1.35×，消除了数据和流水线的慢点，达到近乎完美的计算与内存平衡。

Conclusion: 将核心注意力解耦并且分派给专用服务器是有效的思路，能在长上下文场景下显著提升训练吞吐并避免慢点，同时通过 ping-pong 通信与就地执行等设计降低内存开销，具有较好的扩展潜力。

Abstract: We present core attention disaggregation (CAD), a technique that improves
long-context large language model training by decoupling the core attention
computation, softmax(QK^T)V, from the rest of the model and executing it on a
separate pool of devices. In existing systems, core attention is colocated with
other layers; at long context lengths, its quadratic compute growth compared to
the near-linear growth of other components causes load imbalance and stragglers
across data and pipeline parallel groups. CAD is enabled by two observations.
First, core attention is stateless: it has no trainable parameters and only
minimal transient data, so balancing reduces to scheduling compute-bound tasks.
Second, it is composable: modern attention kernels retain high efficiency when
processing fused batches of token-level shards with arbitrary lengths. CAD
partitions core attention into token-level tasks and dispatches them to
dedicated attention servers, which dynamically rebatch tasks to equalize
compute without sacrificing kernel efficiency. We implement CAD in a system
called DistCA, which uses a ping-pong execution scheme to fully overlap
communication with computation and in-place execution on attention servers to
reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens,
DistCA improves end-to-end training throughput by up to 1.35x, eliminates data
and pipeline parallel stragglers, and achieves near-perfect compute and memory
balance.

</details>


### [89] [Rethinking PCA Through Duality](https://arxiv.org/abs/2510.18130)
*Jan Quan,Johan Suykens,Panagiotis Patrinos*

Main category: cs.LG

TL;DR: 用差分凸（DC）框架重新研究 PCA，提出多种新表述并给出理论洞见；结果包括核化与样本外可用性、将同时迭代看作 DCA、提出新 PCA 算法、以及针对鲁棒 PCA 的核化对偶形式（最小化重建误差的 l1 偏差）等。


<details>
  <summary>Details</summary>
Motivation: 旨在深化对 PCA 的基础理解，借助 DC 框架揭示新表述和性能边界，探索核化、样本外推广、以及与自注意力/核 PCA 的联系，提供对长期存在的 QR-like 的迭代方法的新优化视角。

Method: 基于差分凸（DC）优化框架建立多种 PCA 的新表述，证明其核化可行性与样本外推广性；将同时迭代等价为 DCA 的一个实例；提出新的 PCA 算法并与现有方法在实验中比较；给出一个鲁棒 PCA 的核化对偶形式，目标为重建误差的 l1 偏差最小化。

Result: 理论上证明了 PCA-相关问题的核化及样本外可用性；揭示并建立了将同时迭代视作 DCA 的联系；提出多种新算法并通过实验与最先进方法对比；给出鲁棒 PCA 的核化对偶形式，利用 l1 损失提高对异常值的鲁棒性。

Conclusion: 本工作从 DC 角度为 PCA 提供新的理论框架和算法族，统一了 QR/同时迭代与 DCA 的关系，扩展了 PCA 的核化与样本外推广能力，并提出鲁棒且具有核化潜力的新对偶表述，提升对 PCA 及其鲁棒变体的理解与应用。

Abstract: Motivated by the recently shown connection between self-attention and
(kernel) principal component analysis (PCA), we revisit the fundamentals of
PCA. Using the difference-of-convex (DC) framework, we present several novel
formulations and provide new theoretical insights. In particular, we show the
kernelizability and out-of-sample applicability for a PCA-like family of
problems. Moreover, we uncover that simultaneous iteration, which is connected
to the classical QR algorithm, is an instance of the difference-of-convex
algorithm (DCA), offering an optimization perspective on this longstanding
method. Further, we describe new algorithms for PCA and empirically compare
them with state-of-the-art methods. Lastly, we introduce a kernelizable dual
formulation for a robust variant of PCA that minimizes the $l_1$ deviation of
the reconstruction errors.

</details>


### [90] [Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria](https://arxiv.org/abs/2510.18183)
*Eason Yu,Tzu Hao Liu,Yunke Wang,Clément L. Canonne,Nguyen H. Tran,Chang Xu*

Main category: cs.LG

TL;DR: 在固定大正则化下通过参考策略迭代实现两人零和博弈的精确Nash收敛；提出Nash Policy Gradient（NashPG），兼容策略梯度，适用于大规模域。


<details>
  <summary>Details</summary>
Motivation: 解决不完备信息博弈中的Nash求解难题，避免将正则化强度收敛至0所引发的学习不稳定性，追求鲁棒性与可扩展性。

Method: 引入固定强度正则化的参考策略迭代框架，证明在两人零和博弈中存在严格单调改进并收敛至精确Nash；提出Nash Policy Gradient（NashPG），仅依赖当前策略和参考策略来更新。

Result: 在经典基准博弈中可比或优于现有无模型方法的可剥削性；在 Battleship 与 No-Limit Texas Hold'em 等大规模域中，NashPG 展现出更高的 Elo，显示鲁棒性与扩展性。

Conclusion: 该框架实现了无需额外唯一性假设的Nash收敛路径，为不完备信息博弈提供了一种鲁棒且可扩展的基于策略梯度的求解方法。

Abstract: Finding Nash equilibria in imperfect-information games remains a central
challenge in multi-agent reinforcement learning. While regularization-based
methods have recently achieved last-iteration convergence to a regularized
equilibrium, they require the regularization strength to shrink toward zero to
approximate a Nash equilibrium, often leading to unstable learning in practice.
Instead, we fix the regularization strength at a large value for robustness and
achieve convergence by iteratively refining the reference policy. Our main
theoretical result shows that this procedure guarantees strictly monotonic
improvement and convergence to an exact Nash equilibrium in two-player zero-sum
games, without requiring a uniqueness assumption. Building on this framework,
we develop a practical algorithm, Nash Policy Gradient (NashPG), which
preserves the generalizability of policy gradient methods while relying solely
on the current and reference policies. Empirically, NashPG achieves comparable
or lower exploitability than prior model-free methods on classic benchmark
games and scales to large domains such as Battleship and No-Limit Texas
Hold'em, where NashPG consistently attains higher Elo ratings.

</details>


### [91] [ActivationReasoning: Logical Reasoning in Latent Activation Spaces](https://arxiv.org/abs/2510.18184)
*Lukas Helff,Ruben Härle,Wolfgang Stammer,Felix Friedrich,Manuel Brack,Antonia Wüst,Hikaru Shindo,Patrick Schramowski,Kristian Kersting*

Main category: cs.LG

TL;DR: AR 将显式逻辑推理嵌入到 LLM 的潜在空间，通过三阶段流程实现透明、可控、可扩展的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型内部推理不透明、难以控制的问题；利用稀疏自编码器暴露潜在特征，将其与逻辑契约结合，提供系统化推理和模型控制的机制。

Method: 三阶段：1) 寻找潜在表示，识别初始潜在概念并形成字典；2) 激活概念，推理时检测激活的概念并将其映射到逻辑命题；3) 逻辑推理，对这些命题应用逻辑规则，推导高阶结构、组合新概念并指导模型行为。

Result: 在多跳推理（PrOntoQA）、抽象与对间接线索鲁棒性（Rail2Country）、自然语言与多样性推理（ProverQA）、上下文敏感安全（BeaverTails）等任务上，AR 展现出对推理复杂度的稳健扩展性、对抽象与上下文敏感任务的泛化能力，并可跨模型骨架迁移。

Conclusion: 将逻辑结构基于潜在激活进行锚定，不仅提升透明度，也使得结构化推理、可控性和目标行为对齐成为可能，为更可靠和可审计的 AI 指明路径。

Abstract: Large language models (LLMs) excel at generating fluent text, but their
internal reasoning remains opaque and difficult to control. Sparse autoencoders
(SAEs) make hidden activations more interpretable by exposing latent features
that often align with human concepts. Yet, these features are fragile and
passive, offering no mechanism for systematic reasoning or model control. To
address this, we introduce ActivationReasoning (AR), a framework that embeds
explicit logical reasoning into the latent space of LLMs. It proceeds in three
stages: (1) Finding latent representations, first latent concept
representations are identified (e.g., via SAEs) and organized into a
dictionary; (2) Activating propositions, at inference time AR detects
activating concepts and maps them to logical propositions; and (3)Logical
reasoning, applying logical rules over these propositions to infer higher-order
structures, compose new concepts, and steer model behavior. We evaluate AR on
multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept
cues (Rail2Country), reasoning over natural and diverse language (ProverQA),
and context-sensitive safety (BeaverTails). Across all tasks, AR scales
robustly with reasoning complexity, generalizes to abstract and
context-sensitive tasks, and transfers across model backbones. These results
demonstrate that grounding logical structure in latent activations not only
improves transparency but also enables structured reasoning, reliable control,
and alignment with desired behaviors, providing a path toward more reliable and
auditable AI.

</details>


### [92] [Ensemble based Closed-Loop Optimal Control using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.18195)
*Jostein Barry-Straume,Adwait D. Verulkar,Arash Sarshar,Andrey A. Popov,Adrian Sandu*

Main category: cs.LG

TL;DR: 提出一种多阶段集成PINN框架，通过学习成本到达函数并推导最优控制，解决HJB方程的数值难题，且在无稳定化项条件下实现对一个两状态非线性系统的闭环控制，适用于无限时域且对噪声鲁棒。


<details>
  <summary>Details</summary>
Motivation: HJB为最优控制提供理论框架，但数值求解成本高、解析解难得；PINN等知识引导的ML方法能缓解难题，但现有方法常通过在训练中强制稳定化项。需要一种不依赖稳定化、可生成不同控制信号策略的框架。

Method: 提出一个多阶段的集成框架，先学习最优成本到达函数（价值函数），再由HJB推导最优控制信号；支持单一（singular）控制与集合（ensemble）控制策略；不使用稳定化项；在一个稳态、时不变、两状态的非线性无限时域系统上进行闭环控制实验，考虑噪声与初始条件变化。

Result: 在闭环控制中，验证了采用集成控制和奇异控制两种策略的成功应用；对噪声扰动和初始条件变化具鲁棒性。

Conclusion: 该框架为不使用稳定化项的PINN-HJB求解提供了可行替代方案，能够从学习到的成本到达函数导出最优控制，适用于无限时域的非线性系统，并具有对更广泛问题的潜在扩展。

Abstract: The objective of designing a control system is to steer a dynamical system
with a control signal, guiding it to exhibit the desired behavior. The
Hamilton-Jacobi-Bellman (HJB) partial differential equation offers a framework
for optimal control system design. However, numerical solutions to this
equation are computationally intensive, and analytical solutions are frequently
unavailable. Knowledge-guided machine learning methodologies, such as
physics-informed neural networks (PINNs), offer new alternative approaches that
can alleviate the difficulties of solving the HJB equation numerically. This
work presents a multistage ensemble framework to learn the optimal cost-to-go,
and subsequently the corresponding optimal control signal, through the HJB
equation. Prior PINN-based approaches rely on a stabilizing the HJB enforcement
during training. Our framework does not use stabilizer terms and offers a means
of controlling the nonlinear system, via either a singular learned control
signal or an ensemble control signal policy. Success is demonstrated in
closed-loop control, using both ensemble- and singular-control, of a
steady-state time-invariant two-state continuous nonlinear system with an
infinite time horizon, accounting of noisy, perturbed system states and varying
initial conditions.

</details>


### [93] [Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations](https://arxiv.org/abs/2510.18228)
*Zhendong Mi,Qitao Tan,Grace Li Zhang,Zhaozhuo Xu,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: P-GAP uses zeroth-order optimization with projected gradient-aligned perturbations to fine-tune LLMs more efficiently by estimating a low-dimensional gradient space and aligning perturbations within it, reducing variance and the number of perturbed parameters.


<details>
  <summary>Details</summary>
Motivation: ZO methods save memory but suffer from high gradient-variance, causing slow convergence for large models. There is a need for memory-efficient, scalable fine-tuning of LLMs with faster convergence.

Method: 1) Estimate a low-dimensional gradient space; 2) Align perturbations with projected gradients' direction within that space; 3) Perturb fewer parameters to reduce variance and accelerate convergence.

Result: P-GAP consistently improves baselines, achieving up to 6% higher accuracy on classification tasks and up to 12% on generation tasks, with up to 81% fewer training iterations and 70% less GPU hours.

Conclusion: P-GAP enables fast, scalable, and resource-efficient zeroth-order LLM fine-tuning by reducing the perturbation dimension and aligning perturbations in the projected gradient space to lower variance and speed up convergence.

Abstract: Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization
has emerged as a promising alternative to traditional gradient-based methods
due to its reduced memory footprint requirement. However, existing ZO methods
suffer from high variance in gradient estimation, leading to slow convergence
and suboptimal performance on large-scale models. In this work, we propose
P-GAP, a fast LLM fine-tuning approach through zeroth-order optimization with
Projected Gradient-Aligned Perturbations. Specifically, we first estimate a
low-dimensional gradient space and then align perturbations in projected
gradients' direction within the space. This approach enables reduced the number
of perturbed parameters and decreased variance, therefore accelerated
convergence for LLM fine-tuning. Experiments on LLMs show that P-GAP
consistently surpasses the baselines, achieving up to 6% increase in accuracy
on classification tasks and up to 12% higher accuracy on generation tasks, with
up to about 81% less training iterations and 70% less GPU hours. These results
demonstrate that P-GAP enables fast, scalable, and resource-efficient ZO LLM
fine-tuning.

</details>


### [94] [ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control](https://arxiv.org/abs/2510.18232)
*Yuzheng Hu,Ryan McKenna,Da Yu,Shanshan Wu,Han Zhao,Zheng Xu,Peter Kairouz*

Main category: cs.LG

TL;DR: 提出 ACTG-ARL：一个分层的差分隐私文本生成框架，结合特征学习和条件文本生成，并通过 Anchored RL 提升指令遵循性；相比先前工作，MAUVE 提升约20%。


<details>
  <summary>Details</summary>
Motivation: 面临在差分隐私条件下保持统计属性、降低噪声带来的效用损失、实现对生成过程的细粒度控制的挑战；需要一个将特征学习与文本生成分离的架构，并提升条件生成的遵循性。

Method: 方法包括两阶段：第一阶段的特征学习—将数据表示为丰富的表格特征，训练一个 DP 表格合成器；第二阶段的条件文本生成—在差分隐私约束下对生成器进行微调，并以特征作为条件实现文本生成。另一个核心组件 ARL：后训练方法，结合强化学习提升对指令的遵循，同时以 best-of-N 的 SFT 作为锚点以防止奖励欺骗。整体形成 ACTG-ARL 的端到端流程。

Result: 经过系统消融实验，发现最有效配置为：丰富的表格特征、DP 表格合成器和 DP 微调的条件生成器；在 MAUVE 指标上较先前工作提升约 20%。在强隐私约束下，提升文本质量与对条件生成的控制能力。

Conclusion: 本工作通过层级化设计和锚定强化学习，在差分隐私条件下显著提升合成文本的质量和可控性，提供了一个可扩展的 DP 文本生成框架，未来可用于更广泛的 DP 数据合成任务。

Abstract: Generating high-quality synthetic text under differential privacy (DP) is
critical for training and evaluating language models without compromising user
privacy. Prior work on synthesizing DP datasets often fail to preserve key
statistical attributes, suffer utility loss from the noise required by DP, and
lack fine-grained control over generation. To address these challenges, we make
two contributions. First, we introduce a hierarchical framework that decomposes
DP synthetic text generation into two subtasks: feature learning and
conditional text generation. This design explicitly incorporates learned
features into the generation process and simplifies the end-to-end synthesis
task. Through systematic ablations, we identify the most effective
configuration: a rich tabular schema as feature, a DP tabular synthesizer, and
a DP fine-tuned conditional generator, which we term ACTG
(Attribute-Conditioned Text Generation). Second, we propose Anchored RL (ARL),
a post-training method that improves the instruction-following ability of ACTG
for conditional generation. ARL combines RL to boost control with an SFT anchor
on best-of-$N$ data to prevent reward hacking. Together, these components form
our end-to-end algorithm ACTG-ARL, which advances both the quality of DP
synthetic text (+20% MAUVE over prior work) and the control of the conditional
generator under strong privacy guarantees.

</details>


### [95] [Fostering the Ecosystem of AI for Social Impact Requires Expanding and Strengthening Evaluation Standards](https://arxiv.org/abs/2510.18238)
*Bryan Wilder,Angela Zhou*

Main category: cs.LG

TL;DR: 主张将社会影响的评估从单纯的部署与新颖ML方法扩展到更广泛的社会影响，并对已部署系统的影响进行更严格的评估，以实现研究生态的长期可持续性。


<details>
  <summary>Details</summary>
Motivation: 当前的评审准则偏向同时实现部署和方法创新的项目，这种激励结构可能扭曲研究方向，损害社会影响研究的可持续生态；需要通过扩展影响概念和加强部署系统的影响评估来改善。

Method: 以理论论证和政策性建议为主，提出一种扩展社会影响概念与强化部署系统影响评估的框架，供研究者与评审者参考。

Result: 提出框架性主张：在评审和研究实践中同时认可应用性、方法创新以外的贡献，并对已部署系统的社会影响进行更严格的评估，指引未来研究方向与评审标准。

Conclusion: 呼吁在ML for social impact领域实现对应用和方法两端的平衡重视，扩大影响范畴并强化评估，以促进研究生态的长期可持续性。

Abstract: There has been increasing research interest in AI/ML for social impact, and
correspondingly more publication venues have refined review criteria for
practice-driven AI/ML research. However, these review guidelines tend to most
concretely recognize projects that simultaneously achieve deployment and novel
ML methodological innovation. We argue that this introduces incentives for
researchers that undermine the sustainability of a broader research ecosystem
of social impact, which benefits from projects that make contributions on
single front (applied or methodological) that may better meet project partner
needs. Our position is that researchers and reviewers in machine learning for
social impact must simultaneously adopt: 1) a more expansive conception of
social impacts beyond deployment and 2) more rigorous evaluations of the impact
of deployed systems.

</details>


### [96] [Learning with Dual-level Noisy Correspondence for Multi-modal Entity Alignment](https://arxiv.org/abs/2510.18240)
*Haobin Li,Yijie Lin,Peng Hu,Mouxing Yang,Xi Peng*

Main category: cs.LG

TL;DR: 提出 RULE 框架以鲁棒解决双层噪声对应的多模态实体对齐问题；通过两折原则估计 intra-entity 与 inter-graph 对应的可靠性，在训练阶段缓解属性融合中的噪声影响，及在跨图差异消除阶段防止对噪声的过拟合；并引入对应推理模块挖掘跨图属性-属性关系以提升对齐准确性。


<details>
  <summary>Details</summary>
Motivation: 现实场景下的多模态知识图谱存在双层噪声：内层的实体-属性对齐噪声和跨图的实体-实体及属性-属性对齐噪声，现有方法多假设噪声为等效或为零，导致对齐性能受限。

Method: 提出鲁棒的 RULE 框架。首先通过一个两折原则估计 intra-entity 与 inter-graph 对应的可靠性；利用这些可靠性来在属性融合阶段减轻 intra-entity 噪声的负面影响，并在跨图差异消除阶段抑制对噪声的过拟合；训练阶段外加一个对应推理模块，揭示跨图属性-属性之间的潜在联系以提高对齐准确性。

Result: 在五个基准数据集上对比七种状态-of-the-art 方法， RULE 显著提升对齐准确性，且对 DNC 问题具有鲁棒性；代码公开在 GitHub。

Conclusion: RULE 提供一个在双层噪声存在时有效的多模态实体对齐框架，通过可靠性估计与跨图属性推理实现对齐鲁棒性与准确性的提升。

Abstract: Multi-modal entity alignment (MMEA) aims to identify equivalent entities
across heterogeneous multi-modal knowledge graphs (MMKGs), where each entity is
described by attributes from various modalities. Existing methods typically
assume that both intra-entity and inter-graph correspondences are faultless,
which is often violated in real-world MMKGs due to the reliance on expert
annotations. In this paper, we reveal and study a highly practical yet
under-explored problem in MMEA, termed Dual-level Noisy Correspondence (DNC).
DNC refers to misalignments in both intra-entity (entity-attribute) and
inter-graph (entity-entity and attribute-attribute) correspondences. To address
the DNC problem, we propose a robust MMEA framework termed RULE. RULE first
estimates the reliability of both intra-entity and inter-graph correspondences
via a dedicated two-fold principle. Leveraging the estimated reliabilities,
RULE mitigates the negative impact of intra-entity noise during attribute
fusion and prevents overfitting to noisy inter-graph correspondences during
inter-graph discrepancy elimination. Beyond the training-time designs, RULE
further incorporates a correspondence reasoning module that uncovers the
underlying attribute-attribute connection across graphs, guaranteeing more
accurate equivalent entity identification. Extensive experiments on five
benchmarks verify the effectiveness of our method against the DNC compared with
seven state-of-the-art methods.The code is available at
\href{https://github.com/XLearning-SCU/RULE}{XLearning-SCU/RULE}

</details>


### [97] [Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs](https://arxiv.org/abs/2510.18245)
*Song Bian,Tao Yu,Shivaram Venkataraman,Youngsuk Park*

Main category: cs.LG

TL;DR: 通过引入条件缩放定律与架构信息，探索在推理成本与准确性之间的权衡，提出用于寻找高效且准确的模型架构的搜索框架；在80M–3B参数、8B–100B训练令牌范围内训练200+模型并拟合定律，结果显示在相同训练预算下，优化架构可提升准确性和推理吞吐，相较LLaMA-3.2达到最高2.1%准确性提升和42%吞吐提升。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模和数据规模持续扩大，推理成本成为部署中的关键瓶颈。尽管提高参数量和数据量往往提升性能，但如何在不显著增加推理成本的前提下保持或提升准确性，仍缺乏系统的架构层面研究。

Method: 扩展Chinchilla框架，将隐藏层规模、MLP与注意力参数分配（mlp-to-attention ratio）以及分组查询注意力（GQA）等架构信息纳入条件化 scaling，提出条件缩放定律；构建用于在推理成本和准确性之间进行权衡的架构搜索框架；在80M–3B参数、8B–100B训练令牌范围内训练>200个模型，拟合定律并验证预测性。

Result: 条件缩放定律能可靠预测在给定预算下的最优架构；优化后的架构在相同训练预算下相较现有开源基线有显著改进，最高提升约2.1%准确性、提升约42%推理吞吐，相对于LLaMA-3.2表现更好。

Conclusion: 将架构信息纳入缩放规律并辅以搜索框架，能够在训练预算受限时实现更优的推理成本-准确性平衡，为模型设计提供实际可行的架构指引。

Abstract: Scaling the number of parameters and the size of training data has proven to
be an effective strategy for improving large language model (LLM) performance.
Yet, as these models grow increasingly powerful and widely deployed, the cost
of inference has become a pressing concern. Despite its importance, the
trade-off between model accuracy and inference efficiency remains
underexplored. In this work, we examine how key architectural factors, hidden
size, the allocation of parameters between MLP and attention (mlp-to-attention
ratio), and grouped-query attention (GQA), influence both inference cost and
accuracy. We introduce a conditional scaling law that augments the Chinchilla
framework with architectural information, along with a search framework for
identifying architectures that are simultaneously inference-efficient and
accurate. To validate our approach, we train more than 200 models spanning 80M
to 3B parameters and 8B to 100B training tokens, and fit the proposed
conditional scaling law. Our results show that the conditional scaling law
reliably predicts optimal architectural choices and that the resulting models
outperform existing open-source baselines. Under the same training budget,
optimized architectures achieve up to 2.1% higher accuracy and 42% greater
inference throughput compared to LLaMA-3.2.

</details>


### [98] [NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective](https://arxiv.org/abs/2510.18258)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Junchi Yan*

Main category: cs.LG

TL;DR: 提出基于神经张量核（NTK）的多任务学习新框架NTKMTL及其高效变体NTKMTL-SR，通过扩展的NTK矩阵和谱分析来平衡多任务的收敛速度，从而缓解任务不平衡问题，在多任务监督学习和多任务强化学习中实现了状态-of-the-art性能，且提供了代码实现。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中的任务不平衡和不同任务的收敛速度差异是导致性能下降的核心难点。NTK理论能够揭示训练过程中的动态特性，为任务间的协同学习提供理论依据与度量手段。

Method: 在MTL中扩展NTK矩阵，利用谱分析来平衡各任务的收敛速度，提出NTKMTL方法；在共享表示假设下，进一步提出NTKMTL-SR以提升训练效率并维持竞争性能。

Result: 大量实验表明，该方法在广泛的基准数据集上实现了最先进的性能，覆盖监督学习与强化学习场景下的多任务学习任务。

Conclusion: 通过基于NTK的训练动力学分析，本文给出一个面向任务协同平衡的原理性框架，并通过NTKMTL及其高效变体在准确性与训练效率之间实现良好折中，且提供了可复现实验代码。

Abstract: Multi-Task Learning (MTL) enables a single model to learn multiple tasks
simultaneously, leveraging knowledge transfer among tasks for enhanced
generalization, and has been widely applied across various domains. However,
task imbalance remains a major challenge in MTL. Although balancing the
convergence speeds of different tasks is an effective approach to address this
issue, it is highly challenging to accurately characterize the training
dynamics and convergence speeds of multiple tasks within the complex MTL
system. To this end, we attempt to analyze the training dynamics in MTL by
leveraging Neural Tangent Kernel (NTK) theory and propose a new MTL method,
NTKMTL. Specifically, we introduce an extended NTK matrix for MTL and adopt
spectral analysis to balance the convergence speeds of multiple tasks, thereby
mitigating task imbalance. Based on the approximation via shared
representation, we further propose NTKMTL-SR, achieving training efficiency
while maintaining competitive performance. Extensive experiments demonstrate
that our methods achieve state-of-the-art performance across a wide range of
benchmarks, including both multi-task supervised learning and multi-task
reinforcement learning. Source code is available at
https://github.com/jianke0604/NTKMTL.

</details>


### [99] [From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation](https://arxiv.org/abs/2510.18263)
*Ziwei Huang,Ying Shu,Hao Fang,Quanyu Long,Wenya Wang,Qiushi Guo,Tiezheng Ge,Leilei Gan*

Main category: cs.LG

TL;DR: Customized-GRPO: a two-fold framework (SARS and TDW) that mitigates degradation in subject-driven image generation by non-linear reward shaping and time-aware weighting, achieving better fidelity and editability than naive GRPO.


<details>
  <summary>Details</summary>
Motivation: The field faces a fundamental trade-off between identity fidelity and prompt adherence in subject-driven image generation. Naive online RL with GRPO suffers competitive degradation due to conflicting gradient signals from linearly aggregated rewards with static weights and misalignment with diffusion dynamics.

Method: Synergy-Aware Reward Shaping (SARS): a non-linear reward shaping mechanism that penalizes conflicted rewards and amplifies synergistic signals to produce sharper, more decisive gradients. Time-Aware Dynamic Weighting (TDW): dynamic weighting that emphasizes prompt-following in early steps and identity preservation in later steps, aligning optimization pressure with the model’s temporal diffusion dynamics.

Result: Extensive experiments show that Customized-GRPO outperforms naive GRPO baselines, significantly reducing competitive degradation and achieving a better balance between preserving identity features and adhering to complex textual prompts.

Conclusion: Customized-GRPO effectively addresses the fidelity–editability trade-off in subject-driven image generation, delivering images that better preserve key identity features while accurately satisfying detailed prompts.

Abstract: Subject-driven image generation models face a fundamental trade-off between
identity preservation (fidelity) and prompt adherence (editability). While
online reinforcement learning (RL), specifically GPRO, offers a promising
solution, we find that a naive application of GRPO leads to competitive
degradation, as the simple linear aggregation of rewards with static weights
causes conflicting gradient signals and a misalignment with the temporal
dynamics of the diffusion process. To overcome these limitations, we propose
Customized-GRPO, a novel framework featuring two key innovations: (i)
Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly
penalizes conflicted reward signals and amplifies synergistic ones, providing a
sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW),
which aligns the optimization pressure with the model's temporal dynamics by
prioritizing prompt-following in the early, identity preservation in the later.
Extensive experiments demonstrate that our method significantly outperforms
naive GRPO baselines, successfully mitigating competitive degradation. Our
model achieves a superior balance, generating images that both preserve key
identity features and accurately adhere to complex textual prompts.

</details>


### [100] [Online Time Series Forecasting with Theoretical Guarantees](https://arxiv.org/abs/2510.18281)
*Zijian Li,Changze Zhou,Minghao Fu,Sanjay Manjunath,Fan Feng,Guangyi Chen,Yingyao Hu,Ruichu Cai,Kun Zhang*

Main category: cs.LG

TL;DR: 提出一个带潜在变量的在线时间序列预测理论框架TOT，给出理论保证、潜在变量识别策略及一个模型无关的实现蓝本，并在合成数据与基线改进上得到验证。


<details>
  <summary>Details</summary>
Motivation: 在未知分布随时间变化的在线时间序列预测场景中，历史到未来的映射受潜在变量影响，需要在理论上建立可验证的框架以应对分布偏移和估计不确定性。

Method: 提出TOT理论框架，证明引入潜在变量可紧缩贝叶斯风险；在潜在变量的估计不确定性下仍保持收益，且收益随潜在变量可辨识度提升而增强。提出通过最小相邻观测来识别潜在变量的做法，并给出一个模型无关的蓝图：使用时序解码器匹配观测分布，并用两个独立的噪声估计器分别对潜在变量的因果推断和观测变量混合过程建模。

Result: 在合成数据上验证理论，基于若干基线的插件实现，在多项基准上显示出普遍的改进，表明该框架具有实际应用潜力。

Conclusion: 该工作提供了一个理论与实现兼容的在线时间序列预测框架，证明潜在变量可以紧缩贝叶斯风险且在估计不确定性存在时仍有效；识别策略和模型蓝本具有普适性和良好性能。

Abstract: This paper is concerned with online time series forecasting, where unknown
distribution shifts occur over time, i.e., latent variables influence the
mapping from historical to future observations. To develop an automated way of
online time series forecasting, we propose a Theoretical framework for Online
Time-series forecasting (TOT in short) with theoretical guarantees.
Specifically, we prove that supplying a forecaster with latent variables
tightens the Bayes risk, the benefit endures under estimation uncertainty of
latent variables and grows as the latent variables achieve a more precise
identifiability. To better introduce latent variables into online forecasting
algorithms, we further propose to identify latent variables with minimal
adjacent observations. Based on these results, we devise a model-agnostic
blueprint by employing a temporal decoder to match the distribution of observed
variables and two independent noise estimators to model the causal inference of
latent variables and mixing procedures of observed variables, respectively.
Experiment results on synthetic data support our theoretical claims. Moreover,
plug-in implementations built on several baselines yield general improvement
across multiple benchmarks, highlighting the effectiveness in real-world
applications.

</details>


### [101] [Physics-Informed Parametric Bandits for Beam Alignment in mmWave Communications](https://arxiv.org/abs/2510.18299)
*Hao Qin,Thang Duong,Ming Li,Chicheng Zhang*

Main category: cs.LG

TL;DR: 提出两种物理信息引导的 bandit 算法（pretc、prgreedy），用于毫米波 beam 识别与跟踪，利用稀疏多径特性对奖励函数进行参数化估计，在多种场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信中的束对准/跟踪在高路径损耗下至关重要，传统 bandit 在大空间中收敛慢，且对 unimodality/multimodality 假设敏感。需要在不强依赖特定结构的情况下高效鲁棒地识别最佳束。

Method: 将每条路径的参数视为黑箱，基于历史采样的奖励来维护最优估计；pretc 先随机探索再根据估计奖励选择束，prgreedy 在线估计并选择当前最优束；适配移动场景的束跟踪。

Result: 在 DeepMIMO（合成）和 DeepSense6G（真实）数据集上实验，两个算法在多种场景和通道环境中均优于现有方法，具备良好泛化与鲁棒性。

Conclusion: 利用稀疏多径与物理信息的结合，提供了一种无需强假设即可高效稳定的束对准方案，适合 mmWave 的 beam tracking/识别任务。

Abstract: In millimeter wave (mmWave) communications, beam alignment and tracking are
crucial to combat the significant path loss. As scanning the entire directional
space is inefficient, designing an efficient and robust method to identify the
optimal beam directions is essential. Since traditional bandit algorithms
require a long time horizon to converge under large beam spaces, many existing
works propose efficient bandit algorithms for beam alignment by relying on
unimodality or multimodality assumptions on the reward function's structure.
However, such assumptions often do not hold (or cannot be strictly satisfied)
in practice, which causes such algorithms to converge to choosing suboptimal
beams.
  In this work, we propose two physics-informed bandit algorithms
\textit{pretc} and \textit{prgreedy} that exploit the sparse multipath property
of mmWave channels - a generic but realistic assumption - which is connected to
the Phase Retrieval Bandit problem. Our algorithms treat the parameters of each
path as black boxes and maintain optimal estimates of them based on sampled
historical rewards. \textit{pretc} starts with a random exploration phase and
then commits to the optimal beam under the estimated reward function.
\textit{prgreedy} performs such estimation in an online manner and chooses the
best beam under current estimates. Our algorithms can also be easily adapted to
beam tracking in the mobile setting. Through experiments using both the
synthetic DeepMIMO dataset and the real-world DeepSense6G dataset, we
demonstrate that both algorithms outperform existing approaches in a wide range
of scenarios across diverse channel environments, showing their
generalizability and robustness.

</details>


### [102] [Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task](https://arxiv.org/abs/2510.18315)
*Brady Bhalla,Honglu Fan,Nancy Chen,Tony Yue YU*

Main category: cs.LG

TL;DR: Embedding dimension affects internal world models in a transformer trained with RL on a bubble-sort-style task: small dimensions suffice for accuracy, but larger dimensions yield more faithful, consistent, and robust internal representations. Two mechanisms emerge: (1) the last row of the attention weight matrix monotonically encodes global token order; (2) the transposition aligns with the largest adjacent difference of encoded values. Larger model sizes improve representation quality in addition to end performance; metrics are released for probing similar tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how architecture size, specifically embedding dimension, influences the emergence of structured internal representations (world models) in algorithmic transformer tasks and whether larger models improve interpretability and robustness beyond raw accuracy.

Method: Train transformers with reinforcement learning to perform bubble-sort-style adjacent swaps on token sequences. Systematically vary embedding dimension and assess accuracy, interpretability, and robustness. Analyze attention patterns across hundreds of experiments and release metrics.

Result: High accuracy even with very small embedding dimensions; larger dimensions produce more faithful, consistent, and robust internal representations. Two robust mechanisms observed: (1) the last row of the attention weight matrix monotonically encodes global ordering of tokens; (2) the selected transposition aligns with the largest adjacent difference of encoded values. Provides quantitative evidence that transformers build structured internal world models; larger model sizes improve representation quality in addition to end performance; metrics and analyses released.

Conclusion: Model size contributes to representation quality and interpretability in transformer-based RL tasks, supporting the existence of emergent world models in algorithmic reasoning tasks; the released metrics offer a resource for probing similar tasks.

Abstract: We investigate how embedding dimension affects the emergence of an internal
"world model" in a transformer trained with reinforcement learning to perform
bubble-sort-style adjacent swaps. Models achieve high accuracy even with very
small embedding dimensions, but larger dimensions yield more faithful,
consistent, and robust internal representations. In particular, higher
embedding dimensions strengthen the formation of structured internal
representation and lead to better interpretability. After hundreds of
experiments, we observe two consistent mechanisms: (1) the last row of the
attention weight matrix monotonically encodes the global ordering of tokens;
and (2) the selected transposition aligns with the largest adjacent difference
of these encoded values. Our results provide quantitative evidence that
transformers build structured internal world models and that model size
improves representation quality in addition to end performance. We release our
metrics and analyses, which can be used to probe similar algorithmic tasks.

</details>


### [103] [Uncertainty Estimation by Flexible Evidential Deep Learning](https://arxiv.org/abs/2510.18322)
*Taeseong Yoon,Heeyoung Kim*

Main category: cs.LG

TL;DR: 提出 flexible evidential deep learning (F-EDL)，通过预测更灵活的 Dirichlet 分布来表示类别概率的不确定性，从而提升 UQ 的泛化和鲁棒性，并在经典、长尾及噪声在分布等场景下达到最先进表现。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，可靠的不确定性量化至关重要；传统的 evidential deep learning (EDL) 使用 Dirichlet 分布假设，但对复杂或未知场景的鲁棒性有限，需更表达性强的分布来提升泛化。

Method: 在 EDL 框架上扩展为预测一个 flexible Dirichlet 分布（对类别概率的更通用分布），实现更丰富的不确定性表达；理论分析强调其优点，并通过实验在多种评估环境下展示优越的 UQ 性能。

Result: 理论上证明了 F-EDL 在不确定性建模上的若干优势；实证结果在经典、长尾以及噪声在分布等多种情景下实现了先进的 UQ 表现。

Conclusion: F-EDL 提供了一种更表达的概率不确定性建模方式，兼具效率和鲁棒性，显著提升在挑战性情景中的 UQ 可靠性与泛化能力。

Abstract: Uncertainty quantification (UQ) is crucial for deploying machine learning
models in high-stakes applications, where overconfident predictions can lead to
serious consequences. An effective UQ method must balance computational
efficiency with the ability to generalize across diverse scenarios. Evidential
deep learning (EDL) achieves efficiency by modeling uncertainty through the
prediction of a Dirichlet distribution over class probabilities. However, the
restrictive assumption of Dirichlet-distributed class probabilities limits
EDL's robustness, particularly in complex or unforeseen situations. To address
this, we propose \textit{flexible evidential deep learning}
($\mathcal{F}$-EDL), which extends EDL by predicting a flexible Dirichlet
distribution -- a generalization of the Dirichlet distribution -- over class
probabilities. This approach provides a more expressive and adaptive
representation of uncertainty, significantly enhancing UQ generalization and
reliability under challenging scenarios. We theoretically establish several
advantages of $\mathcal{F}$-EDL and empirically demonstrate its
state-of-the-art UQ performance across diverse evaluation settings, including
classical, long-tailed, and noisy in-distribution scenarios.

</details>


### [104] [Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching](https://arxiv.org/abs/2510.18328)
*Zhong Li,Qi Huang,Yuxuan Zhu,Lincen Yang,Mohammad Mohammadi Amiri,Niki van Stein,Matthijs van Leeuwen*

Main category: cs.LG

TL;DR: 提出 Time-Conditioned Contraction Matching (TCCM) 用于表格数据的半监督异常检测，通过时间条件的收缩向量预测实现轻量训练和高效推理，结合单步偏差得分与输入空间的可解释性，实验证明在 ADBench 上超越 SOTA，尤其在高维和大规模数据集上。


<details>
  <summary>Details</summary>
Motivation: 解决流式匹配/扩散类模型在异常检测中的推理成本高、训练复杂、缺乏可解释性的问题；通过简化核心思想——学习分布间的速度场——并引入时间条件的收缩向量、单步偏差分数等设计，获得更高效且具可解释性与鲁棒性的半监督异常检测方法。

Method: 在给定时间步下预测指向原点的时间条件收缩向量，作为速度场的替代实现，训练目标更轻量：不需要在训练或推理时求解常微分方程；提出“一步偏差”得分，在单前向传递中衡量与期望收敛行为的偏离；速度场直接作用于输入空间，带来特征级可归因性，并且得分函数对输入的李斯普次性可控，提供小扰动下的理论保障。

Result: 在 ADBench 基准上，TCCM 在检测准确率与推理成本之间取得良好平衡，优于现有方法，尤其是在高维与大规模数据集上表现突出；公开源码在 GitHub。

Conclusion: TCCM 提供一种高效、可解释且鲁棒的表格数据异常检测框架，规模化地适用于高维与大规模数据，实验结果证实其在对比基线中的优势与可扩展性。

Abstract: We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for
semi-supervised anomaly detection in tabular data. TCCM is inspired by flow
matching, a recent generative modeling framework that learns velocity fields
between probability distributions and has shown strong performance compared to
diffusion models and generative adversarial networks. Instead of directly
applying flow matching as originally formulated, TCCM builds on its core idea
-- learning velocity fields between distributions -- but simplifies the
framework by predicting a time-conditioned contraction vector toward a fixed
target (the origin) at each sampled time step. This design offers three key
advantages: (1) a lightweight and scalable training objective that removes the
need for solving ordinary differential equations during training and inference;
(2) an efficient scoring strategy called one time-step deviation, which
quantifies deviation from expected contraction behavior in a single forward
pass, addressing the inference bottleneck of existing continuous-time models
such as DTE (a diffusion-based model with leading anomaly detection accuracy
but heavy inference cost); and (3) explainability and provable robustness, as
the learned velocity field operates directly in input space, making the anomaly
score inherently feature-wise attributable; moreover, the score function is
Lipschitz-continuous with respect to the input, providing theoretical
guarantees under small perturbations. Extensive experiments on the ADBench
benchmark show that TCCM strikes a favorable balance between detection accuracy
and inference cost, outperforming state-of-the-art methods -- especially on
high-dimensional and large-scale datasets. The source code is available at our
GitHub repository.

</details>


### [105] [Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs](https://arxiv.org/abs/2510.18340)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 对未折扣、无限-horizon MDP 的策略梯度给出理论分析框架：通过在所有动作概率严格大于0的策略下保持状态的再现/瞬时分类不变，并引入“瞬时访问量”来取代经典访问量，从而处理 γ=1 的情形。


<details>
  <summary>Details</summary>
Motivation: 弥补现有以 γ<1 为前提的策略梯度理论在 γ=1 的未折扣总回报设置中的空缺，特别是在大语言模型中的应用场景。

Method: 提出两点核心洞见：(i) 在对所有动作概率严格大于0的策略集合内，状态的循环/瞬时分类保持不变；(ii) 将经典状态访问量替换为瞬时访问量(transient visitation measure)，从而在 γ=1 的情形下进行理论分析。

Result: 基于上述洞见，构建未折扣无限 horizon 的策略梯度分析框架，给出梯度表达和潜在的收敛性结论的理论基础。

Conclusion: 通过引入瞬时访问量和策略相关性下的状态分类不变性，该工作将策略梯度理论扩展到 γ=1 的未折扣场景，为大模型强化学习的理论研究提供新工具与方向。

Abstract: The classical policy gradient method is the theoretical and conceptual
foundation of modern policy-based reinforcement learning (RL) algorithms. Most
rigorous analyses of such methods, particularly those establishing convergence
guarantees, assume a discount factor $\gamma < 1$. In contrast, however, a
recent line of work on policy-based RL for large language models uses the
undiscounted total-reward setting with $\gamma = 1$, rendering much of the
existing theory inapplicable. In this paper, we provide analyses of the policy
gradient method for undiscounted expected total-reward infinite-horizon MDPs
based on two key insights: (i) the classification of the MDP states into
recurrent and transient states is invariant over the set of policies that
assign strictly positive probability to every action (as is typical in deep RL
models employing a softmax output layer) and (ii) the classical state
visitation measure (which may be ill-defined when $\gamma = 1$) can be replaced
with a new object that we call the transient visitation measure.

</details>


### [106] [Computable universal online learning](https://arxiv.org/abs/2510.18352)
*Dariusz Kalociński,Tomasz Steifer*

Main category: cs.LG

TL;DR: 这篇论文研究普遍在线学习的可计算性问题，证明普遍在线学习不等同于可计算的普遍在线学习；并给出对其无偏（agnostic）和正确（proper）变体的精确刻画，提供对在线二进制分类理论的更现实理解。


<details>
  <summary>Details</summary>
Motivation: 理解学习能否被实现为计算机程序；弥合抽象可学性与可实现性之间的差距；在Bousquet等人(STOC'21)提出的普遍在线学习框架上展开研究。

Method: 以对抗者模型（对手可在保持局部一致性的前提下改变标签）为背景，研究学习者在有限错误内可 computable 的策略；证明普遍在线学习不必然带来可计算的普遍在线学习；进一步对可计算的无偏学习和正确学习给出精确的充要条件；可能采用可计算性与构造性论证、简化类的例子等。

Result: 存在这样的结论：普遍在线学习不 imply 可计算的普遍在线学习；对可计算的无偏必解学习（agnostic computable universal online learning）给出精确刻画；对可计算的正确学习（proper）也给出完全刻画；这些结果共同提供了对在线二分类理论及归纳推理问题更现实的视角。

Conclusion: 该工作把抽象学习理论和实际可实现性联系起来，明确了哪些学习任务在理论可行的同时也能被实现为程序，哪些还需要额外的可计算性条件；为在线二分类及相关推理提供了更清晰的边界和方向。

Abstract: Understanding when learning is possible is a fundamental task in the theory
of machine learning. However, many characterizations known from the literature
deal with abstract learning as a mathematical object and ignore the crucial
question: when can learning be implemented as a computer program? We address
this question for universal online learning, a generalist theoretical model of
online binary classification, recently characterized by Bousquet et al.
(STOC'21). In this model, there is no hypothesis fixed in advance; instead,
Adversary -- playing the role of Nature -- can change their mind as long as
local consistency with the given class of hypotheses is maintained. We require
Learner to achieve a finite number of mistakes while using a strategy that can
be implemented as a computer program. We show that universal online learning
does not imply computable universal online learning, even if the class of
hypotheses is relatively easy from a computability-theoretic perspective. We
then study the agnostic variant of computable universal online learning and
provide an exact characterization of classes that are learnable in this sense.
We also consider a variant of proper universal online learning and show exactly
when it is possible. Together, our results give a more realistic perspective on
the existing theory of online binary classification and the related problem of
inductive inference.

</details>


### [107] [Ensembling Pruned Attention Heads For Uncertainty-Aware Efficient Transformers](https://arxiv.org/abs/2510.18358)
*Firas Gabetni,Giuseppe Curci,Andrea Pilzer,Subhankar Roy,Elisa Ricci,Gianni Franchi*

Main category: cs.LG

TL;DR: Hydra Ensembles is an efficient transformer ensemble built by pruning attention heads to create diverse members, merged via a new multi-head attention with grouped fully-connected layers. It achieves a compact model with inference speed close to a single network, matching or surpassing Deep Ensembles in uncertainty quantification (UQ) without retraining from scratch. It also achieves state-of-the-art zero-shot ImageNet-1k performance without extra training.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification is critical for deploying deep nets in safety-critical settings. Deep Ensembles provide strong UQ but are computationally heavy, limiting scalability to large models. There is a need for scalable, well-calibrated uncertainty with efficient inference, and methods that preserve calibration when pruning or compressing models.

Method: Prune attention heads to create diverse ensemble members. Merge them via a novel multi-head attention mechanism that uses grouped fully-connected layers, yielding a compact ensemble. The approach requires little to no retraining from scratch and is evaluated on image and text classification across architectures, including zero-shot ImageNet-1k.

Result: Hydra Ensembles consistently outperform Deep Ensembles on image and text classification tasks across multiple architectures. They maintain robust calibration and UQ performance, with inference speed close to a single network. In zero-shot ImageNet-1k classification, Hydra Ensembles surpass state-of-the-art methods without additional training.

Conclusion: Hydra Ensembles offer an efficient, scalable, and robust approach to uncertainty quantification for transformer models. By pruning attention heads and merging via grouped FCs, they deliver competitive or superior UQ with far lower computational/memory costs, enabling practical deployment in large-scale, safety-critical settings.

Abstract: Uncertainty quantification (UQ) is essential for deploying deep neural
networks in safety-critical settings. Although methods like Deep Ensembles
achieve strong UQ performance, their high computational and memory costs hinder
scalability to large models. We introduce Hydra Ensembles, an efficient
transformer-based ensemble that prunes attention heads to create diverse
members and merges them via a new multi-head attention with grouped
fully-connected layers. This yields a compact model with inference speed close
to a single network, matching or surpassing Deep Ensembles in UQ performance
without retraining from scratch. We also provide an in-depth analysis of
pruning, showing that naive approaches can harm calibration, whereas Hydra
Ensembles preserves robust uncertainty. Experiments on image and text
classification tasks, with various architectures, show consistent gains over
Deep Ensembles. Remarkably, in zero-shot classification on ImageNet-1k, our
approach surpasses state of the art methods, even without requiring additional
training.

</details>


### [108] [Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding](https://arxiv.org/abs/2510.18360)
*Sunwoo Kim,Hyunjin Hwang,Kijung Shin*

Main category: cs.LG

TL;DR: FGP is a pretraining scheme for neural-architecture encoders that reconstructs a flow surrogate to capture information flow, yielding large gains.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art flow-based encoders for predicting neural-architecture performance are slow due to their specialized structures; there is a need for faster pretraining that preserves information-flow information.

Method: FGP trains an encoder to reconstruct a proposed flow surrogate (a representation of information flow) rather than relying on architecture-specific flow models, enabling faster encoding.

Result: When used to pretrain encoders, FGP boosts predictive performance by up to 106% in Precision@1 compared to the same encoder trained only with supervised learning.

Conclusion: FGP offers an effective, architecture-agnostic pretraining approach to capture information flow in neural architectures, improving NAS-related predictive performance without heavy flow-based encoders.

Abstract: The performance of a deep learning model on a specific task and dataset
depends heavily on its neural architecture, motivating considerable efforts to
rapidly and accurately identify architectures suited to the target task and
dataset. To achieve this, researchers use machine learning models-typically
neural architecture encoders-to predict the performance of a neural
architecture. Many state-of-the-art encoders aim to capture information flow
within a neural architecture, which reflects how information moves through the
forward pass and backpropagation, via a specialized model structure. However,
due to their complicated structures, these flow-based encoders are
significantly slower to process neural architectures compared to simpler
encoders, presenting a notable practical challenge. To address this, we propose
FGP, a novel pre-training method for neural architecture encoding that trains
an encoder to capture the information flow without requiring specialized model
structures. FGP trains an encoder to reconstruct a flow surrogate, our proposed
representation of the neural architecture's information flow. Our experiments
show that FGP boosts encoder performance by up to 106% in Precision-1%,
compared to the same encoder trained solely with supervised learning.

</details>


### [109] [Towards Unsupervised Open-Set Graph Domain Adaptation via Dual Reprogramming](https://arxiv.org/abs/2510.18363)
*Zhen Zhang,Bingsheng He*

Main category: cs.LG

TL;DR: 提出 GraphRTA 的开集无监督图域自适应框架，通过对图和模型的重编程实现对已知/未知类别的区分，并扩展分类器以包含未知类。


<details>
  <summary>Details</summary>
Motivation: 现实场景中源域与目标域的标签空间往往不同，目标域可能包含源域未出现的类别，因此需要在无监督条件下识别未知类别。

Method: 图重编程：通过修改目标图的结构和节点特征以增强已知/未知的分离；模型重编程：通过剪枝域特异参数减少对源域的偏置，同时保留跨图的可迁移模式；分类器扩展一个未知类别维度，省去阈值设定。

Result: 在多个公开数据集上与最近的SOTA基线相比表现良好，实验充分证明了对未知类别的识别能力，且代码公开。

Conclusion: GraphRTA 能有效解决开集无监督图域自适应问题，提升对已知/未知类别的鲁棒性与可迁移性。

Abstract: Unsupervised Graph Domain Adaptation has become a promising paradigm for
transferring knowledge from a fully labeled source graph to an unlabeled target
graph. Existing graph domain adaptation models primarily focus on the
closed-set setting, where the source and target domains share the same label
spaces. However, this assumption might not be practical in the real-world
scenarios, as the target domain might include classes that are not present in
the source domain. In this paper, we investigate the problem of unsupervised
open-set graph domain adaptation, where the goal is to not only correctly
classify target nodes into the known classes, but also recognize previously
unseen node types into the unknown class. Towards this end, we propose a novel
framework called GraphRTA, which conducts reprogramming on both the graph and
model sides. Specifically, we reprogram the graph by modifying target graph
structure and node features, which facilitates better separation of known and
unknown classes. Meanwhile, we also perform model reprogramming by pruning
domain-specific parameters to reduce bias towards the source graph while
preserving parameters that capture transferable patterns across graphs.
Additionally, we extend the classifier with an extra dimension for the unknown
class, thus eliminating the need of manually specified threshold in open-set
recognition. Comprehensive experiments on several public datasets demonstrate
that our proposed model can achieve satisfied performance compared with recent
state-of-the-art baselines. Our source codes and datasets are publicly
available at https://github.com/cszhangzhen/GraphRTA.

</details>


### [110] [Training Diverse Graph Experts for Ensembles: A Systematic Empirical Study](https://arxiv.org/abs/2510.18370)
*Gangda Deng,Yuxin Yang,Ömer Faruk Akgül,Hanqing Zeng,Yinglong Xia,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 对GNN集成的专家多样性进行系统性经验研究，评估了20种多样化策略在14个节点分类任务上的表现与机制。


<details>
  <summary>Details</summary>
Motivation: 现实图数据的异质性限制了单一GNN的性能，MoE框架通过组合具有不同泛化模式的GNNs来提升性能；需要对专家级多样化进行系统评估，提取实用设计准则。

Method: 在14个节点分类基准上，评估20种多样化策略（包括随机重启/随机初始化、超参数调优、架构变体、方向性建模、训练数据划分等），构建并分析超过200个GNN集成变体；从专家多样性、互补性以及集成性能等角度进行比较；探索培训最大多样性专家的机制。

Result: 结果揭示不同多样化策略对专家多样性与互补性的影响，指明提高集成性能的有效组合和训练实践；提供对GNN MoE框架设计的可操作洞察。

Conclusion: 为GNN专家培训和MoE设计提供实用指南，支持在图数据上实现更强的模型组合效应；代码已开源，便于复现和扩展。https://github.com/Hydrapse/bench-gnn-diversification

Abstract: Graph Neural Networks (GNNs) have become essential tools for learning on
relational data, yet the performance of a single GNN is often limited by the
heterogeneity present in real-world graphs. Recent advances in
Mixture-of-Experts (MoE) frameworks demonstrate that assembling multiple,
explicitly diverse GNNs with distinct generalization patterns can significantly
improve performance. In this work, we present the first systematic empirical
study of expert-level diversification techniques for GNN ensembles. Evaluating
20 diversification strategies -- including random re-initialization,
hyperparameter tuning, architectural variation, directionality modeling, and
training data partitioning -- across 14 node classification benchmarks, we
construct and analyze over 200 ensemble variants. Our comprehensive evaluation
examines each technique in terms of expert diversity, complementarity, and
ensemble performance. We also uncovers mechanistic insights into training
maximally diverse experts. These findings provide actionable guidance for
expert training and the design of effective MoE frameworks on graph data. Our
code is available at https://github.com/Hydrapse/bench-gnn-diversification.

</details>


### [111] [Approximation Rates of Shallow Neural Networks: Barron Spaces, Activation Functions and Optimality Analysis](https://arxiv.org/abs/2510.18388)
*Jian Lu,Xiaohuang Huang*

Main category: cs.LG

TL;DR: This work analyzes how the approximation rate of shallow networks with activation functions that are powers of exponential functions depends on input dimension and function smoothness, particularly within Barron spaces. It shows that optimal rates are unattainable under L1-bounded coefficients or insufficient smoothness for ReLU^k activations, establishes optimal rates in Barron and Sobolev spaces, and confirms the curse of dimensionality, offering guidance for activation choice and network design.


<details>
  <summary>Details</summary>
Motivation: To understand fundamental limits of shallow neural networks when using activation functions of the form (exponential)^{k} and how dimension and function smoothness in Barron spaces govern approximation rates.

Method: Derive upper and lower bounds for approximation rates; analyze ReLU^k activations; prove non-attainability of optimal rates under L1-bounded coefficients or insufficient smoothness; establish optimal rates in Barron and Sobolev spaces across different norms; demonstrate the curse of dimensionality.

Result: Characterization of approximation limits of shallow nets with exponent-powered activations; explicit rate bounds showing dimension-driven degradation; identification of scenarios where optimal rates cannot be achieved; confirmation of dimension curse in Barron and Sobolev settings; practical implications for activation choice and network architecture.

Conclusion: Shallow networks with the studied activation functions have inherent approximation limits influenced by dimensionality and smoothness. The results guide activation function selection and network design by highlighting when optimal rates are impossible and how function spaces (Barron, Sobolev) shape feasible approximation performance.

Abstract: This paper investigates the approximation properties of shallow neural
networks with activation functions that are powers of exponential functions. It
focuses on the dependence of the approximation rate on the dimension and the
smoothness of the function being approximated within the Barron function space.
We examine the approximation rates of ReLU$^{k}$ activation functions, proving
that the optimal rate cannot be achieved under $\ell^{1}$-bounded coefficients
or insufficient smoothness conditions.
  We also establish optimal approximation rates in various norms for functions
in Barron spaces and Sobolev spaces, confirming the curse of dimensionality.
Our results clarify the limits of shallow neural networks' approximation
capabilities and offer insights into the selection of activation functions and
network structures.

</details>


### [112] [Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees](https://arxiv.org/abs/2510.18406)
*Miao Zhang,Junpeng Li,ChangChun HUa,Yana Yang*

Main category: cs.LG

TL;DR: 提出了一种在只观察到n元组中正例个数m的弱监督场景下的无偏风险估计量（URE）用于训练，覆盖固定n,m和可变tuple大小/计数等扩展，给出辨识条件、泛化界限与一致性，并通过ReLU修正提高有限样本稳定性，在多项基准上优于弱监督基线且对类别不平衡与tuple配置鲁棒。


<details>
  <summary>Details</summary>
Motivation: 在许多弱监督任务中，只有区域或多实例测量的计数信息可用，缺乏实例级标签。NTMP（N-tuple with M positives）设定把每个样本表示为包含恰好m个正例的n元组，且仅观测计数m。需要一个理论扎实且可实践的学习目标来利用这类信号。

Method: 将tuple生成过程与潜在实例的边缘分布联系起来，推导出可训练的无偏风险估计量（URE）的闭式表达；从固定n,m出发拓展到可变tuple大小、可变计数及二者组合；给出辨识条件（当有效混合率与类别先验分离时可 identifiabled）；通过Rademacher复杂度给出泛化界与证明一致性，并提出ReLU修正以提高有限样本稳定性。

Result: 建立了理论上可实现的URE及其扩展，证明在一般条件下具有泛化界限和统计一致性；在若干NTMP任务基准上优于常见弱监督 baselines，且在不同tuple配置和类别不平衡下表现稳健，改善Precision-Recall和F1折中。

Conclusion: 计数型弱监督信号可通过一个理论支撑、实践稳定的目标函数被有效利用，适用于需要从区域级或多实例测量中提取标签信息的场景，且具有良好的鲁棒性和广泛的应用潜力。

Abstract: Weakly supervised learning often operates with coarse aggregate signals
rather than instance labels. We study a setting where each training example is
an $n$-tuple containing exactly m positives, while only the count m per tuple
is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g.,
image classification with region proposals and multi-instance measurements. We
show that tuple counts admit a trainable unbiased risk estimator (URE) by
linking the tuple-generation process to latent instance marginals. Starting
from fixed (n,m), we derive a closed-form URE and extend it to variable tuple
sizes, variable counts, and their combination. Identification holds whenever
the effective mixing rate is separated from the class prior. We establish
generalization bounds via Rademacher complexity and prove statistical
consistency with standard rates under mild regularity assumptions. To improve
finite-sample stability, we introduce simple ReLU corrections to the URE that
preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the
approach consistently outperforms representative weak-supervision baselines and
yields favorable precision-recall and F1 trade-offs. It remains robust under
class-prior imbalance and across diverse tuple configurations, demonstrating
that count-only supervision can be exploited effectively through a
theoretically grounded and practically stable objective.

</details>


### [113] [Provable Generalization Bounds for Deep Neural Networks with Adaptive Regularization](https://arxiv.org/abs/2510.18410)
*Adeel Safder*

Main category: cs.LG

TL;DR: 提出 MAGDrop，一种基于动量自适应的激活 dropout，用于提升 DNN 泛化能力，并给出基于 PAC-Bayes 的紧化界限，理论与经验均显示优于常规 dropout 和自适应梯度正则化。


<details>
  <summary>Details</summary>
Motivation: 解决 DNN 高容量导致的过拟合问题；在非凸优化中用梯度与动量信息自适应调整 dropout 以提升稳定性与泛化。

Method: 提出 MAGDrop：基于激活的 dropout 率动态依据当前梯度和累计动量进行调整；给出考虑自适应性的紧化 PAC-Bayes 泛化界限，利用动量驱动的扰动控制。

Result: 在 MNIST / CIFAR-10 上，激活式 MAGDrop 相较基线正则化（包括标准 dropout 和自适应梯度正则化）提升测试准确率 1–2%，分别达到 99.52% 和 90.63%，泛化差距为 0.48% 与 7.14%。

Conclusion: 将理论与实践结合，提供一种鲁棒的 DNN 泛化框架，适用于高风险场景。

Abstract: Deep neural networks (DNNs) achieve remarkable performance but often suffer
from overfitting due to their high capacity. We introduce Momentum-Adaptive
Gradient Dropout (MAGDrop), a novel regularization method that dynamically
adjusts dropout rates on activations based on current gradients and accumulated
momentum, enhancing stability in non-convex optimization landscapes. To
theoretically justify MAGDrop's effectiveness, we derive a tightened PAC-Bayes
generalization bound that accounts for its adaptive nature, achieving up to 20%
sharper bounds compared to standard approaches by leveraging momentum-driven
perturbation control. Empirically, the activation-based MAGDrop outperforms
baseline regularization techniques, including standard dropout and adaptive
gradient regularization, by 1-2% in test accuracy on MNIST (99.52%) and
CIFAR-10 (90.63%), with generalization gaps of 0.48% and 7.14%, respectively.
Our work bridges theoretical insights and practical advancements, offering a
robust framework for enhancing DNN generalization suitable for high-stakes
applications.

</details>


### [114] [Learning Boltzmann Generators via Constrained Mass Transport](https://arxiv.org/abs/2510.18460)
*Christopher von Klitzing,Denis Blessing,Henrik Schopmans,Pascal Friederich,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出 Constrained Mass Transport (CMT) 框架，通过在每一步设置 KL 散度和熵衰减的约束，生成中间分布，提升在高维、强多模态的未归一化分布上的采样效率，克服模式塌陷与质量传输问题，在 BG 基准和新引入的 ELIL 四肽上显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 高维且多模态的未归一化分布的采样是一个关键难题。现有的变分方法易陷入模式崩溃（mode collapse），而基于退火的策略往往需要精心的调度且容易产生质量传输（mass teleportation）。需要一个能同时提升分布覆盖度、抑制早熟收敛且对调参依赖较小的框架。

Method: 提出 Constrained Mass Transport（CMT）作为变分框架，在每一步构造带约束的中间分布，约束条件包括相邻分布之间的 KL 散度和熵的衰减，确保分布之间具有充分重叠，减少质量传输并抑制快速收敛。通过优化这些约束，得到更稳健的高维采样。

Result: 在标准的 Boltzmann Generator（BG）基准及文中新增的 ELIL 四肽系统上，CMT 的表现优于最先进的变分方法，且在无需获得分子动力学样本的情况下实现了超过 2.5 倍的有效样本量提升，并避免了模式塌陷。

Conclusion: CMT 为高维、强多模态的未归一化分布采样提供了一种稳定且高效的变分框架，提升 BG 的采样质量与效率，同时降低对退火调度的依赖，具有广泛的应用潜力。

Abstract: Efficient sampling from high-dimensional and multimodal unnormalized
probability distributions is a central challenge in many areas of science and
machine learning. We focus on Boltzmann generators (BGs) that aim to sample the
Boltzmann distribution of physical systems, such as molecules, at a given
temperature. Classical variational approaches that minimize the reverse
Kullback-Leibler divergence are prone to mode collapse, while annealing-based
methods, commonly using geometric schedules, can suffer from mass teleportation
and rely heavily on schedule tuning. We introduce Constrained Mass Transport
(CMT), a variational framework that generates intermediate distributions under
constraints on both the KL divergence and the entropy decay between successive
steps. These constraints enhance distributional overlap, mitigate mass
teleportation, and counteract premature convergence. Across standard BG
benchmarks and the here introduced ELIL tetrapeptide, the largest system
studied to date without access to samples from molecular dynamics, CMT
consistently surpasses state-of-the-art variational methods, achieving more
than 2.5x higher effective sample size while avoiding mode collapse.

</details>


### [115] [Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation](https://arxiv.org/abs/2510.18478)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: 提出不确定性安全评估器（USC）的新方法，通过在不确定与成本高区域加强保守性、在安全区域保留梯度尖锐性来实现安全性与任务表现的平衡。实验显示USC显著降低安全违规并维持或提升奖励，同时大幅降低代价梯度预测误差，打破安全与性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 在真实世界应用中，强化学习需要在确保安全的前提下提升任务性能。然而现有方法要么过度约束安全性导致任务性能受损，要么以奖励为优先导致安全约束频繁被违反并产生平滑化的成本景观。需一种同时兼顾安全与效率的学习框架。

Method: 提出不确定性安全评估者（USC），将不确定性感知与调控融入 critic 的训练。通过将保守性集中在不确定且成本高的区域、同时在安全区域保持梯度锐度，来实现更优的 reward-safety 权衡与更具鲁棒性的成本梯度学习。

Result: 实验证明：USC 将安全违规降低约40%，奖励保持竞争力或提升；同时将预测成本梯度与真实成本梯度之间的误差降低约83%。

Conclusion: USC 打破了安全性与性能之间的传统权衡，为大规模安全强化学习提供新的可行路径。

Abstract: Ensuring the safe exploration of reinforcement learning (RL) agents is
critical for deployment in real-world systems. Yet existing approaches struggle
to strike the right balance: methods that tightly enforce safety often cripple
task performance, while those that prioritize reward leave safety constraints
frequently violated, producing diffuse cost landscapes that flatten gradients
and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a
novel approach that integrates uncertainty-aware modulation and refinement into
critic training. By concentrating conservatism in uncertain and costly regions
while preserving sharp gradients in safe areas, USC enables policies to achieve
effective reward-safety trade-offs. Extensive experiments show that USC reduces
safety violations by approximately 40% while maintaining competitive or higher
rewards, and reduces the error between predicted and true cost gradients by
approximately 83%, breaking the prevailing trade-off between safety and
performance and paving the way for scalable safe RL.

</details>


### [116] [Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning](https://arxiv.org/abs/2510.18485)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: COPPOL将分布无关的置信校正（ conformal prediction）引入感知到策略学习，为语义分割输出带有限样本安全保证的已校准危险地图，并用于下游强化学习规划的风险感知成本场。在两个卫星派生基准上，该方法显著提升危险覆盖（最高约6倍），接近实现对不安全区域的完全检测，同时显著降低导航过程中的危险违规率（约50%），且对分布变动具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键环境中，导航需要同时具备准确的危险感知与对不确定性的强健处理。现有方法往往假设危险检测为完全准确，或在不确定性方面缺乏有限样本保证，从而无法提供明确的安全边界。

Method: 提出 COPPOL，将分布自由、有限样本的置信保证融入感知到策略的学习框架。通过在语义分割阶段引入分布无关的置信边界，输出带校准的危险图，并据此构建下游强化学习的风险感知成本场，保障规划阶段的安全性。该框架对预测的不确定性给出严格的概率界限。

Result: 在两项卫星派生基准上，危险覆盖率提升最高可达6倍；实现对不安全区域的近乎完整检测；导航过程中的危险违规显著下降，达到约50%级别；对分布变动具有鲁棒性，安全性与效率同时得到提升。

Conclusion: COPPOL 将统计学的分布无关保证与感知到策略学习有效结合，提供从感知到规划的可证安全性，提升安全关键导航在现实场景中的鲁棒性与效率，尤其在分布偏移情形下仍能保持性能。

Abstract: Reliable navigation in safety-critical environments requires both accurate
hazard perception and principled uncertainty handling to strengthen downstream
safety handling. Despite the effectiveness of existing approaches, they assume
perfect hazard detection capabilities, while uncertainty-aware perception
approaches lack finite-sample guarantees. We present COPPOL, a conformal-driven
perception-to-policy learning approach that integrates distribution-free,
finite-sample safety guarantees into semantic segmentation, yielding calibrated
hazard maps with rigorous bounds for missed detections. These maps induce
risk-aware cost fields for downstream RL planning. Across two satellite-derived
benchmarks, COPPOL increases hazard coverage (up to 6x) compared to comparative
baselines, achieving near-complete detection of unsafe regions while reducing
hazardous violations during navigation (up to approx 50%). More importantly,
our approach remains robust to distributional shift, preserving both safety and
efficiency.

</details>


### [117] [Alibaba International E-commerce Product Search Competition DILAB Team Technical Report](https://arxiv.org/abs/2510.18499)
*Hyewon Lee,Junghyun Oh,Minkyung Song,Soyoung Park,Seunghoon Han*

Main category: cs.LG

TL;DR: 本研究提出一个多语言电商搜索系统，取得最终排行榜第5名，综合得分0.8819；通过多阶段管线解决多语言查询与商品理解问题，包含数据精炼、轻量预处理与自适应建模等，且在跨语言和跨领域具备鲁棒性。代码开源于 GitHub。


<details>
  <summary>Details</summary>
Motivation: 解决多语言环境下的查询–商品理解难点及一致性不足、类别覆盖不足、输入噪声和语言差异对检索效果的影响，目标是在多语言电商场景中实现稳定且高效的检索性能。

Method: 提出多阶段流水线：数据精炼阶段提升数据集一致性与类别覆盖；语言标注与噪声过滤提高输入质量。在建模阶段探索多种架构与微调策略，并通过精心挑选的验证集进行超参数优化，以在查询-类别（QC）和查询-商品（QI）任务之间实现性能平衡。

Result: 在最终排行榜中获第5名，综合得分0.8819，展现出在多语言环境下对各评估指标的稳定高性能；数据精炼、标签化与噪声过滤，以及多架构与超参数调优共同提升了系统的鲁棒性与适应性。

Conclusion: 通过系统的数据治理与迭代评估，证明了在多语言搜索系统中实现稳健性能的有效性，且开源代码可供复现与进一步研究。

Abstract: This study presents the multilingual e-commerce search system developed by
the DILAB team, which achieved 5th place on the final leaderboard with a
competitive overall score of 0.8819, demonstrating stable and high-performing
results across evaluation metrics. To address challenges in multilingual
query-item understanding, we designed a multi-stage pipeline integrating data
refinement, lightweight preprocessing, and adaptive modeling. The data
refinement stage enhanced dataset consistency and category coverage, while
language tagging and noise filtering improved input quality. In the modeling
phase, multiple architectures and fine-tuning strategies were explored, and
hyperparameters optimized using curated validation sets to balance performance
across query-category (QC) and query-item (QI) tasks. The proposed framework
exhibited robustness and adaptability across languages and domains,
highlighting the effectiveness of systematic data curation and iterative
evaluation for multilingual search systems. The source code is available at
https://github.com/2noweyh/DILAB-Alibaba-Ecommerce-Search.

</details>


### [118] [Partial VOROS: A Cost-aware Performance Metric for Binary Classifiers with Precision and Capacity Constraints](https://arxiv.org/abs/2510.18520)
*Christopher Ratigan,Kyle Heuton,Carissa Wang,Lenore Cowen,Michael C. Hughes*

Main category: cs.LG

TL;DR: 提出一种以代价为导向的ROC分析框架：在给定精度与容量约束下定义ROC空间中的可行区域；引入部分较差分类器的面积（partial area）、以及在成本参数范围内对ROC曲面进行平均的部分体积（partial VOROS），以对分类器进行排名。并在MIMIC-IV的死亡风险预测中展示该度量相较于现有方法更能反映部署成本。


<details>
  <summary>Details</summary>
Motivation: 在临床告警等应用中，需强制最低精度以避免误报疲劳，并限制预测正例数量以匹配人力容量；传统的ROC/AUC未能反映这些部署成本和不对称代价。

Method: 将满足精度与容量约束的分类器子集在ROC空间中描述为可行区域，研究该区域的几何结构；定义仅考虑可行区域的较差分类器的面积作为性能度量（与成本正相关）；对该面积在一组成本参数区间内取平均，得到ROC曲面上的部分体积，即partial VOROS。

Result: 在使用MIMIC-IV数据集的生命体征史预测死亡风险的实验中，成本感知的度量在医院告警应用中对分类器排序优于替代方法。

Conclusion: 所提出的成本感知的ROC度量能够更贴合实际部署成本，便于在现实约束下选择分类器，且思路可扩展至其他成本敏感任务。

Abstract: The ROC curve is widely used to assess binary classification performance. Yet
for some applications such as alert systems for hospitalized patient
monitoring, conventional ROC analysis cannot capture crucial factors that
impact deployment, such as enforcing a minimum precision constraint to avoid
false alarm fatigue or imposing an upper bound on the number of predicted
positives to represent the capacity of hospital staff. The usual area under the
curve metric also does not reflect asymmetric costs for false positives and
false negatives. In this paper we address all three of these issues. First, we
show how the subset of classifiers that meet given precision and capacity
constraints can be represented as a feasible region in ROC space. We establish
the geometry of this feasible region. We then define the partial area of lesser
classifiers, a performance metric that is monotonic with cost and only accounts
for the feasible portion of ROC space. Averaging this area over a desired range
of cost parameters results in the partial volume over the ROC surface, or
partial VOROS. In experiments predicting mortality risk using vital sign
history on the MIMIC-IV dataset, we show this cost-aware metric is better than
alternatives for ranking classifiers in hospital alert applications.

</details>


### [119] [Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation](https://arxiv.org/abs/2510.18541)
*Giovanni De Muri,Mark Vero,Robin Staab,Martin Vechev*

Main category: cs.LG

TL;DR: 提出了一种可迁移的后门注入方法T-MTB，以研究在知识蒸馏过程中从带后门的教师模型向学生模型转移的安全风险。与以往后门难以转移不同，T-MTB通过构造由多种常在蒸馏数据集中各自出现的特定标记组成的复合触发词，使后门在蒸馏过程中对学生模型可传递且隐蔽。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏可能将不可信来源的教师模型的安全漏洞转移给学生模型，且以往研究发现大多数后门难以在蒸馏中转移，低估了风险。需要系统研究可迁移的后门及其在蒸馏过程中的传导机制和风险。

Method: 提出T-MTB，一种复合型触发器后门方法，由若干在蒸馏数据集中常独立出现的特定标记组成，确保教师模型受污染时仍保持隐蔽性，同时在蒸馏阶段单独出现的这些标记能提供足够信号将后门传递给学生模型。基于T-MTB，在两种攻击场景（越狱/内容调节）和四个模型家族上系统性地研究可转移的后门风险。

Result: 证明并大量研究了在蒸馏过程中可转移的后门风险，通过T-MTB实现了从带后门的教师向学生模型的后门传递，且在两种攻击场景和多种模型家族上具有可重复性和可检验性。

Conclusion: 知识蒸馏从带后门的教师模型中确实可能迁移可转移的后门，且触发机制依赖于复合标记的信号组合。该结果提升了对蒸馏安全的关注，建议加强对教师模型来源、蒸馏数据及训练过程的安全审查，并为检测与防御可转移后门提供新方向。

Abstract: LLMs are often used by downstream users as teacher models for knowledge
distillation, compressing their capabilities into memory-efficient models.
However, as these teacher models may stem from untrusted parties, distillation
can raise unexpected security risks. In this paper, we investigate the security
implications of knowledge distillation from backdoored teacher models. First,
we show that prior backdoors mostly do not transfer onto student models. Our
key insight is that this is because existing LLM backdooring methods choose
trigger tokens that rarely occur in usual contexts. We argue that this
underestimates the security risks of knowledge distillation and introduce a new
backdooring technique, T-MTB, that enables the construction and study of
transferable backdoors. T-MTB carefully constructs a composite backdoor
trigger, made up of several specific tokens that often occur individually in
anticipated distillation datasets. As such, the poisoned teacher remains
stealthy, while during distillation the individual presence of these tokens
provides enough signal for the backdoor to transfer onto the student. Using
T-MTB, we demonstrate and extensively study the security risks of transferable
backdoors across two attack scenarios, jailbreaking and content modulation, and
across four model families of LLMs.

</details>


### [120] [RAISE: A Unified Framework for Responsible AI Scoring and Evaluation](https://arxiv.org/abs/2510.18559)
*Loc Phuc Truong Nguyen,Hung Thanh Do*

Main category: cs.LG

TL;DR: Introduce RAISE, a Responsible AI Scoring and Evaluation framework that quantifies model performance across four dimensions—explainability, fairness, robustness, and sustainability—and aggregates them into a single Responsibility Score.


<details>
  <summary>Details</summary>
Motivation: As AI systems are deployed in high-stakes domains, evaluation must go beyond predictive accuracy to consider ethical and societal impacts; a unified, multi-dimensional metric is needed for responsible model selection.

Method: Develop RAISE to quantify four dimensions for each model, apply it to three deep learning architectures (MLP, Tabular ResNet, Feature Tokenizer Transformer) on structured finance, healthcare, and socioeconomics datasets, and aggregate scores into a holistic measure; analyze trade-offs and provide open-source implementation.

Result: Key trade-offs observed: MLP excels in sustainability and robustness; Transformer provides strong explainability and fairness but incurs high environmental cost; Tabular ResNet offers a balanced profile; no model dominates across all criteria; multi-dimensional evaluation is essential for responsible selection; code available at the provided GitHub link.

Conclusion: A holistic, multi-dimensional evaluation framework (RAISE) is necessary for responsible model selection, as different architectures optimize different dimensions of responsibility; practitioners should consider trade-offs and select models accordingly.

Abstract: As AI systems enter high-stakes domains, evaluation must extend beyond
predictive accuracy to include explainability, fairness, robustness, and
sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a
unified framework that quantifies model performance across these four
dimensions and aggregates them into a single, holistic Responsibility Score. We
evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular
ResNet, and a Feature Tokenizer Transformer, on structured datasets from
finance, healthcare, and socioeconomics. Our findings reveal critical
trade-offs: the MLP demonstrated strong sustainability and robustness, the
Transformer excelled in explainability and fairness at a very high
environmental cost, and the Tabular ResNet offered a balanced profile. These
results underscore that no single model dominates across all responsibility
criteria, highlighting the necessity of multi-dimensional evaluation for
responsible model selection. Our implementation is available at:
https://github.com/raise-framework/raise.

</details>


### [121] [HeFS: Helper-Enhanced Feature Selection via Pareto-Optimized Genetic Search](https://arxiv.org/abs/2510.18575)
*Yusi Fan,Tian Wang,Zhiying Yan,Chang Liu,Qiong Zhou,Qi Lu,Zhehao Guo,Ziqi Deng,Wenyu Zhu,Ruochi Zhang,Fengfeng Zhou*

Main category: cs.LG

TL;DR: HeFS通过在残余特征空间中发现帮助集合，利用偏置初始化和比例引导的遗传算法，以及Pareto多目标优化，在现有子集基础上进行 refinement，提升分类性能，尤其在高维数据和复杂特征关系场景表现突出。


<details>
  <summary>Details</summary>
Motivation: 特征选择是NP-hard；传统贪婪/启发式容易早熟收敛，难以捕捉隐藏且信息丰富的特征，尤其在高维数据和特征之间存在复杂依赖关系时。

Method: 提出HeFS框架，对现有算法得到的子集进行 refinement；在残留特征空间搜索Helper Set以补充原子集并提升分类性能；采用带偏置初始化和比率引导的变异的遗传算法；结合Pareto-based多目标优化以同时最大化预测准确性和特征互补性。

Result: 在18个基准数据集上验证，HeFS稳定发现被忽视但信息丰富的特征，优于现有方法；在胃癌分类、药物毒性预测及计算机科学等领域也表现出色。

Conclusion: HeFS通过在残留特征空间发现辅助特征集合并与原子集协同提升性能，提供一种有效的特征子集 refinement 路径，适用于高维数据集，代码与数据集公开。

Abstract: Feature selection is a combinatorial optimization problem that is NP-hard.
Conventional approaches often employ heuristic or greedy strategies, which are
prone to premature convergence and may fail to capture subtle yet informative
features. This limitation becomes especially critical in high-dimensional
datasets, where complex and interdependent feature relationships prevail. We
introduce the HeFS (Helper-Enhanced Feature Selection) framework to refine
feature subsets produced by existing algorithms. HeFS systematically searches
the residual feature space to identify a Helper Set - features that complement
the original subset and improve classification performance. The approach
employs a biased initialization scheme and a ratio-guided mutation mechanism
within a genetic algorithm, coupled with Pareto-based multi-objective
optimization to jointly maximize predictive accuracy and feature
complementarity. Experiments on 18 benchmark datasets demonstrate that HeFS
consistently identifies overlooked yet informative features and achieves
superior performance over state-of-the-art methods, including in challenging
domains such as gastric cancer classification, drug toxicity prediction, and
computer science applications. The code and datasets are available at
https://healthinformaticslab.org/supp/.

</details>


### [122] [Unrolled-SINDy: A Stable Explicit Method for Non linear PDE Discovery from Sparsely Sampled Data](https://arxiv.org/abs/2510.18611)
*Fayad Ali Banna,Antoine Caradot,Eduardo Brandao,Jean-Philippe Colombier,Rémi Emonet,Marc Sebban*

Main category: cs.LG

TL;DR: 提出 Unrolled-SINDy，通过“展开”数值时间步长以解耦采样时刻与真实时间步长，提升显式法在 PDE 发现中的稳定性，适用于稀疏时间采样的数据，能在不改变优化目标的前提下恢复原本因截断误差而无法被 SINDy 找到的参数。


<details>
  <summary>Details</summary>
Motivation: 解决传统 SINDy 在时间数据稀疏时的局部截断误差与稳定性问题，扩展到 PDE 发现的情形。

Method: 引入时间展开策略，将数值时间步长与数据采样率解耦；提供迭代的闭式解法和梯度下降两种实现；可应用于传统 SINDy 与对噪声鲁棒的 iNeuralSINDy，使用 Euler、RK4 等数值方案。

Result: 实验表明，展开策略能使非展开方法无法访问的参数被正确恢复，具有良好的通用性，适用于不同数值方案和数据条件。

Conclusion: Unrolled-SINDy 拓展了 SINDy 在稀疏时间采样数据上的适用性，提升了显式方法的稳定性和参数可恢复性，为现实世界 PDE 发现提供新的工具。

Abstract: Identifying from observation data the governing differential equations of a
physical dynamics is a key challenge in machine learning. Although approaches
based on SINDy have shown great promise in this area, they still fail to
address a whole class of real world problems where the data is sparsely sampled
in time. In this article, we introduce Unrolled-SINDy, a simple methodology
that leverages an unrolling scheme to improve the stability of explicit methods
for PDE discovery. By decorrelating the numerical time step size from the
sampling rate of the available data, our approach enables the recovery of
equation parameters that would not be the minimizers of the original SINDy
optimization problem due to large local truncation errors. Our method can be
exploited either through an iterative closed-form approach or by a gradient
descent scheme. Experiments show the versatility of our method. On both
traditional SINDy and state-of-the-art noise-robust iNeuralSINDy, with
different numerical schemes (Euler, RK4), our proposed unrolling scheme allows
to tackle problems not accessible to non-unrolled methods.

</details>


### [123] [Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting](https://arxiv.org/abs/2510.18874)
*Howard Chen,Noam Razin,Karthik Narasimhan,Danqi Chen*

Main category: cs.LG

TL;DR: RL 比 SFT 更不易遗忘，且性能与或高于SFT；原因在于 RL 的在策略数据（on-policy）模式寻求特性有助于保留先验知识；使用近似在策略数据也能有效缓解遗忘。


<details>
  <summary>Details</summary>
Motivation: 探究在后训练中如何缓解catastrophic forgetting，系统比较SFT与RL在不同LM家族与任务上的遗忘模式，给出可操作的缓解指南。

Method: 在Llama、Qwen等模型、任务涵盖指令跟随、通用知识与算术推理的情形下进行系统对比；将LM视为先验知识分布与目标任务分布的混合；分析在策略数据对保留先验知识的作用；比较KL正则化与优势估计对遗忘的影响并验证。

Result: 结论显示，RL在遗忘上更鲁棒且目标任务性能与SFT相当或更高；其原因在于模式寻求性质来自于使用的在策略数据，有助保持先验知识；在实践中，依赖近似在策略数据即可显著提升对遗忘的鲁棒性，KL正则和优势估计并非唯一决定因素。

Conclusion: 实用意义在于通过采用近似在策略数据的训练策略来缓解后训练中的遗忘，这种做法相对更高效，为后训练的设计提供基于数据分布与学习目标的指南。

Abstract: Adapting language models (LMs) to new tasks via post-training carries the
risk of degrading existing capabilities -- a phenomenon classically known as
catastrophic forgetting. In this paper, toward identifying guidelines for
mitigating this phenomenon, we systematically compare the forgetting patterns
of two widely adopted post-training methods: supervised fine-tuning (SFT) and
reinforcement learning (RL). Our experiments reveal a consistent trend across
LM families (Llama, Qwen) and tasks (instruction following, general knowledge,
and arithmetic reasoning): RL leads to less forgetting than SFT while achieving
comparable or higher target task performance. To investigate the cause for this
difference, we consider a simplified setting in which the LM is modeled as a
mixture of two distributions, one corresponding to prior knowledge and the
other to the target task. We identify that the mode-seeking nature of RL, which
stems from its use of on-policy data, enables keeping prior knowledge intact
when learning the target task. We then verify this insight by demonstrating
that the use on-policy data underlies the robustness of RL to forgetting in
practical settings, as opposed to other algorithmic choices such as the KL
regularization or advantage estimation. Lastly, as a practical implication, our
results highlight the potential of mitigating forgetting using approximately
on-policy data, which can be substantially more efficient to obtain than fully
on-policy data.

</details>


### [124] [A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees](https://arxiv.org/abs/2510.18615)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.LG

TL;DR: 提出一种将提升树蒸馏为决策树的新方法，使用纠正（rectification）实现蒸馏，以在预测性能和可解释性之间取得折中；与基于重新训练的蒸馏相比，结果具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决提升树难以解释的问题，寻求在预测性能和可解释性之间取得折中，通过纠正蒸馏实现从复杂集成模型到可解释决策树的转换。

Method: 提出基于纠正（rectification）的蒸馏过程，将 boosted trees 蒸馏成决策树；并与通过重新训练得到的蒸馏方法进行对比评估。

Result: 实验证明该纠正蒸馏在某些数据集上表现出有趣的结果，与通过重新训练的蒸馏方法相比具有竞争力。

Conclusion: 基于纠正的蒸馏为生成可解释模型提供一种可行替代途径，能够在保持可接受预测性能的同时提升解释性。

Abstract: We present a new approach for distilling boosted trees into decision trees,
in the objective of generating an ML model offering an acceptable compromise in
terms of predictive performance and interpretability. We explain how the
correction approach called rectification can be used to implement such a
distillation process. We show empirically that this approach provides
interesting results, in comparison with an approach to distillation achieved by
retraining the model.

</details>


### [125] [Hardness of Learning Regular Languages in the Next Symbol Prediction Setting](https://arxiv.org/abs/2510.18634)
*Satwik Bhattamishra,Phil Blunsom,Varun Kanade*

Main category: cs.LG

TL;DR: 在Next Symbol Prediction（NSP）设置下，尽管提供比传统分类更多的标签，学习DFA和布尔公式仍然在计算复杂性上困难；通过一个构造性归约将额外的NSP标签信息变为几乎无用的，从而将通常的学习问题归约到NSP标签学习；在密码学假设下，这一困难性在NSP下仍成立。另一方面，NSP标签也可用于高效学习语言模型的截断分布。


<details>
  <summary>Details</summary>
Motivation: 探究NSP设置下的可学习性及其对分析神经序列模型的意义，判断额外标签信息是否降低学习难度，并建立与传统PAC学习的联系。

Method: 将NSP引入PAC学习框架，给出一种构造性证明：通过使多数额外标签信息无用来实现从常规学习到NSP标签学习的归约；在DFA与布尔公式等概念类上讨论 hardness，并在必要时依赖密码学假设。

Result: 结论性结果显示：在NSP设置下，学习DFA和布尔公式在计算上仍然困难，即便标签更丰富；另外，NSP标签可用于从语言模型中高效学习截断的分布或支持。

Conclusion: NSP提供了更丰富的监督信号，但并不能根本降低对某些经典概念类学习的难度；除非在特定约束或假设下，仍需探索受限类、近似学习及对实际语言模型的影响。

Abstract: We study the learnability of languages in the Next Symbol Prediction (NSP)
setting, where a learner receives only positive examples from a language
together with, for every prefix, (i) whether the prefix itself is in the
language and (ii) which next symbols can lead to an accepting string. This
setting has been used in prior works to empirically analyze neural sequence
models, and additionally, we observe that efficient algorithms for the NSP
setting can be used to learn the (truncated) support of language models. We
formalize the setting so as to make it amenable to PAC-learning analysis. While
the setting provides a much richer set of labels than the conventional
classification setting, we show that learning concept classes such as DFAs and
Boolean formulas remains computationally hard. The proof is via a construction
that makes almost all additional labels uninformative, yielding a reduction
from the conventional learning problem to learning with NSP labels. Under
cryptographic assumptions, the reduction implies that the problem of learning
DFAs is computationally hard in the NSP setting.

</details>


### [126] [Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions](https://arxiv.org/abs/2510.18638)
*Yanna Ding,Songtao Lu,Yingdong Lu,Tomasz Nowicki,Jianxi Gao*

Main category: cs.LG

TL;DR: 对以往研究的拓展：在结构化的 ICL 场景中，研究单层线性自注意力下的标注学习，给出全局最小值解的闭式表达、参数重现 NP-hard、以及多层 LSA 的预条件梯度下降解释，并通过简化的 Transformer 进行数值验证。


<details>
  <summary>Details</summary>
Motivation: 旨在理解变形器在建模动力学驱动函数时的 ICL 表现，超越线性回归和独立同分布输入的设定，探究结构化 ICL 的损失景观及其优化行为。

Method: （1）在放大参数空间中推导单层线性自注意力（LSA）模型的全局最小解的闭式表达；（2）证明在一般情况下恢复实现最优解的变换器参数是 NP-hard，揭示单层 LSA 表示结构化动力学函数的根本局限性；（3）给出多层 LSA 的新解释，即将其视为对多个目标（超出平方损失）执行预条件梯度下降的过程；并通过简化的 Transformer 进行数值验证。

Result: 获得全局最小解的闭式表达；证明参数重现的 NP-hard 性质；提出多层 LSA 作为预条件梯度下降的解释；并通过数值实验验证理论结论。

Conclusion: 揭示一层 LSA 在结构化动力学函数建模中的局限性，同时给出多层 LSA 的优化解释，为理解 Transformer 的 ICL 在动力学场景下的表现提供新视角，并通过实验验证。

Abstract: Transformer architectures can solve unseen tasks based on input-output pairs
in a given prompt due to in-context learning (ICL). Existing theoretical
studies on ICL have mainly focused on linear regression tasks, often with
i.i.d. inputs. To understand how transformers express ICL when modeling
dynamics-driven functions, we investigate Markovian function learning through a
structured ICL setup, where we characterize the loss landscape to reveal
underlying optimization behaviors. Specifically, we (1) provide the closed-form
expression of the global minimizer (in an enlarged parameter space) for a
single-layer linear self-attention (LSA) model; (2) prove that recovering
transformer parameters that realize the optimal solution is NP-hard in general,
revealing a fundamental limitation of one-layer LSA in representing structured
dynamical functions; and (3) supply a novel interpretation of a multilayer LSA
as performing preconditioned gradient descent to optimize multiple objectives
beyond the square loss. These theoretical results are numerically validated
using simplified transformers.

</details>


### [127] [Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction](https://arxiv.org/abs/2510.18648)
*Miro Miranda,Marcela Charfuelan,Matias Valdenegro Toro,Andreas Dengel*

Main category: cs.LG

TL;DR: 将物理信息与数据驱动的深度学习结合，用水分缺失预测作物产量并给出不确定性与解释性，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 水分可用性是影响作物产量的核心因素，单纯的工况/物理模型缺乏灵活性，纯ML模型缺乏物理一致性；需要一种兼具可解释性、可扩展性和不确定性估计的产量预测框架，支撑在气候变化背景下的决策。

Method: 将产量视为水分缺失的函数，预测干旱胁迫和对水分缺失的敏感性，采用序贯建模；引入物理一致性损失函数，结合多光谱遥感、气象数据与高分产量数据，采用深度集成以量化不确定性。

Result: 在产量预测中达到R^2约0.82的性能，优于LSTM/Transformer等基线，具较高解释性，具备对行业、政策制定者和农民的决策支持潜力。

Conclusion: 物理信息约束的深度学习框架实现了高精度且可解释的产量预测，为在气候变动条件下的农业韧性提供可操作的工具与洞见。

Abstract: Water is essential for agricultural productivity. Assessing water shortages
and reduced yield potential is a critical factor in decision-making for
ensuring agricultural productivity and food security. Crop simulation models,
which align with physical processes, offer intrinsic explainability but often
perform poorly. Conversely, machine learning models for crop yield modeling are
powerful and scalable, yet they commonly operate as black boxes and lack
adherence to the physical principles of crop growth. This study bridges this
gap by coupling the advantages of both worlds. We postulate that the crop yield
is inherently defined by the water availability. Therefore, we formulate crop
yield as a function of temporal water scarcity and predict both the crop
drought stress and the sensitivity to water scarcity at fine-scale resolution.
Sequentially modeling the crop yield response to water enables accurate yield
prediction. To enforce physical consistency, a novel physics-informed loss
function is proposed. We leverage multispectral satellite imagery,
meteorological data, and fine-scale yield data. Further, to account for the
uncertainty within the model, we build upon a deep ensemble approach. Our
method surpasses state-of-the-art models like LSTM and Transformers in crop
yield prediction with a coefficient of determination ($R^2$-score) of up to
0.82 while offering high explainability. This method offers decision support
for industry, policymakers, and farmers in building a more resilient
agriculture in times of changing climate conditions.

</details>


### [128] [Learning Time-Varying Turn-Taking Behavior in Group Conversations](https://arxiv.org/abs/2510.18649)
*Madeline Navarro,Lisa O'Bryan,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出一个灵活的概率模型，在仅基于个体特征和历史发言行为的前提下预测群体对话中的轮流说话，具备跨群体泛化能力，并能学习“上次发言后”对发言倾向的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对话动力学模型往往局限于单一群体，或追求通用的统一表述，难以在不同群体中泛化。本研究旨在构建一个考虑个体差异与历史行为的通用框架，以实现跨群体的轮流预测。

Method: 提出对先前模型的泛化扩展，建立基于概率的框架来预测群体中个体的发言轮次，关键在于学习个体在不同“上次发言时间”条件下的发言倾向。通过对合成数据与真实世界对话数据的验证，评估模型的解释性与泛化能力。

Result: 实验结果表明，与以往行为模型相比，新模型更具现实性与数据驱动性，能够在不同群体中泛化，并揭示发言倾向随最近发言时间的变化情况。

Conclusion: 该模型为群体对话动力学提供了一种更通用且可解释的框架，有助于理解跨群体的轮流机制，后续可进一步扩展到更丰富的特征与情境。

Abstract: We propose a flexible probabilistic model for predicting turn-taking patterns
in group conversations based solely on individual characteristics and past
speaking behavior. Many models of conversation dynamics cannot yield insights
that generalize beyond a single group. Moreover, past works often aim to
characterize speaking behavior through a universal formulation that may not be
suitable for all groups. We thus develop a generalization of prior conversation
models that predicts speaking turns among individuals in any group based on
their individual characteristics, that is, personality traits, and prior
speaking behavior. Importantly, our approach provides the novel ability to
learn how speaking inclination varies based on when individuals last spoke. We
apply our model to synthetic and real-world conversation data to verify the
proposed approach and characterize real group interactions. Our results
demonstrate that previous behavioral models may not always be realistic,
motivating our data-driven yet theoretically grounded approach.

</details>


### [129] [Reasoning Language Model Inference Serving Unveiled: An Empirical Study](https://arxiv.org/abs/2510.18672)
*Qi Li,Junpan Wu,Xiang Liu,Yuxin Wang,Zeyu Li,Zhenheng Tang,Yuhan Chen,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: This study analyzes the service behavior of reasoning LLMs (RLLMs) and evaluates inference optimizations; it finds that model quantization and speculative decoding can improve efficiency with minor accuracy loss, while certain caching methods may degrade performance for small RLLMs, a conclusion supported by real-world workload evaluations.


<details>
  <summary>Details</summary>
Motivation: RLLMs are competitive in complex reasoning tasks, but their serving performance and behavioral characteristics are largely unexplored, creating challenges for deployment and utilization in real-world systems.

Method: 1) Pilot study comparing RLLM and traditional LLM serving performance to identify differences in memory usage, straggler requests, adaptive running time, and domain preference. 2) Evaluation of existing inference optimization techniques (model quantization, speculative decoding, prefix caching, and KV cache quantization) for RLLMs. 3) Real-world workload evaluation modeled by a Gamma distribution across different datasets to validate findings.

Result: Quantization methods and speculative decoding can improve service efficiency with a small compromise to RLLM accuracy. Prefix caching and KV cache quantization may degrade accuracy or serving performance for small RLLMs. Real-world workload evaluations align with these findings across datasets.

Conclusion: The work provides actionable insights for researchers and industry practitioners on improving RLLM inference serving, highlighting that certain optimizations yield efficiency gains while some caching approaches may hurt performance for smaller models.

Abstract: The reasoning large language model (RLLM) has been proven competitive in
solving complex reasoning tasks such as mathematics, coding, compared to
general LLM. However, the serving performance and behavior of RLLM remains
unexplored, which may undermine the deployment and utilization of RLLM in
real-world scenario. To close this gap, in this paper, we conduct a
comprehensive study of RLLM service. We first perform a pilot study on
comparing the serving performance between RLLM and traditional LLM and reveal
that there are several distinct differences regarding serving behavior: (1)
significant memory usage and fluctuations; (2) straggler requests; (3) adaptive
running time; (4) domain preference. Then we further investigate whether
existing inference optimization techniques are valid for RLLM. Our main
takeaways are that model quantization methods and speculative decoding can
improve service system efficiency with small compromise to RLLM accuracy, while
prefix caching, KV cache quantization may even degrade accuracy or serving
performance for small RLLM. Lastly, we conduct evaluation under real world
workload modeled by Gamma distribution to verify our findings. Empirical
results of real world workload evaluation across different dataset are aligned
with our main findings regarding RLLM serving. We hope our work can provide the
research community and industry with insights to advance RLLM inference
serving.

</details>


### [130] [Learning Task-Agnostic Representations through Multi-Teacher Distillation](https://arxiv.org/abs/2510.18680)
*Philippe Formont,Maxime Darrin,Banafsheh Karimian,Jackie CK Cheung,Eric Granger,Ismail Ben Ayed,Mohammadhadi Shateri,Pablo Piantanida*

Main category: cs.LG

TL;DR: Task-agnostic distillation using a majority-vote objective across multiple teacher embeddings, bounded by mutual information, enabling diverse teacher ensembles to improve downstream tasks without task-specific labels.


<details>
  <summary>Details</summary>
Motivation: Leverage diverse teacher models to enrich representations in a task-agnostic way, addressing the limitation of task-specific tailoring in multi-teacher distillation.

Method: Introduce a majority-vote objective for distillation; prove the bound via mutual information between student and teachers’ embeddings; derive a task-agnostic distillation loss independent of labels or prior knowledge; evaluate on text, vision, and molecular modeling; train and release state-of-the-art embedding models.

Result: Show that and using teacher diversity yields better representations for a wide range of downstream tasks (classification, clustering, regression); release state-of-the-art embeddings.

Conclusion: A task-agnostic framework can exploit teacher diversity to produce robust embeddings applicable across modalities and tasks without task-specific supervision.

Abstract: Casting complex inputs into tractable representations is a critical step
across various fields. Diverse embedding models emerge from differences in
architectures, loss functions, input modalities and datasets, each capturing
unique aspects of the input. Multi-teacher distillation leverages this
diversity to enrich representations but often remains tailored to specific
tasks. In this paper, we introduce a task-agnostic framework based on a
``majority vote" objective function. We demonstrate that this function is
bounded by the mutual information between student and teachers' embeddings,
leading to a task-agnostic distillation loss that eliminates dependence on
task-specific labels or prior knowledge. Our evaluations across text, vision
models, and molecular modeling show that our method effectively leverages
teacher diversity, resulting in representations enabling better performance for
a wide range of downstream tasks such as classification, clustering, or
regression. Additionally, we train and release state-of-the-art embedding
models, enhancing downstream performance in various modalities.

</details>


### [131] [Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach](https://arxiv.org/abs/2510.18687)
*Chenbei Lu,Zaiwei Chen,Tongxin Li,Chenye Wu,Adam Wierman*

Main category: cs.LG

TL;DR: 提出贝叶斯价值函数与Bellman-Jensen Gap，用于分析含多步预测的预测增强MDP，并给出两阶段模型基RL算法BOLA，在不完美预测下保持样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实中存在多步预测可提升决策，但高维性与理论分析困难，需处理预测误差与部分动作覆盖。

Method: 定义贝叶斯价值函数以实现可分析的最优预测感知策略；推导Bellman-Jensen Gap以刻画不完美预测的价值损失；提出BOLA，先离线学习贝叶斯价值，再在线对预测进行轻量化自适应。

Result: 理论证明BOLA在不完美预测下仍具样本效率；在合成MDP与风能储存控制任务上进行验证。

Conclusion: 提供一个可分析且可实现的框架来利用多步预测，扩展了RL在预测增强场景中的理论和算法工具。

Abstract: Traditional reinforcement learning (RL) assumes the agents make decisions
based on Markov decision processes (MDPs) with one-step transition models. In
many real-world applications, such as energy management and stock investment,
agents can access multi-step predictions of future states, which provide
additional advantages for decision making. However, multi-step predictions are
inherently high-dimensional: naively embedding these predictions into an MDP
leads to an exponential blow-up in state space and the curse of dimensionality.
Moreover, existing RL theory provides few tools to analyze prediction-augmented
MDPs, as it typically works on one-step transition kernels and cannot
accommodate multi-step predictions with errors or partial action-coverage. We
address these challenges with three key innovations: First, we propose the
\emph{Bayesian value function} to characterize the optimal prediction-aware
policy tractably. Second, we develop a novel \emph{Bellman-Jensen Gap} analysis
on the Bayesian value function, which enables characterizing the value of
imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with
Online Adaptation), a two-stage model-based RL algorithm that separates offline
Bayesian value learning from lightweight online adaptation to real-time
predictions. We prove that BOLA remains sample-efficient even under imperfect
predictions. We validate our theory and algorithm on synthetic MDPs and a
real-world wind energy storage control problem.

</details>


### [132] [OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales](https://arxiv.org/abs/2510.18707)
*Tung Nguyen,Tuan Pham,Troy Arcomano,Veerabhadra Kotamarthi,Ian Foster,Sandeep Madireddy,Aditya Grover*

Main category: cs.LG

TL;DR: OmniCast is a probabilistic, non-autoregressive weather forecasting model that unifies multiple timescales by encoding data into a latent space with a VAE and using a diffusion-based transformer to sample future latent tokens. Training masks future tokens and learns their distribution; inference iteratively unmasks to generate full future sequences. It mitigates error accumulation, enables long-horizon forecasts, and achieves state-of-the-art performance at subseasonal-to-seasonal scales while being faster than rivals; stable rollouts up to 100 years. Code is available online.


<details>
  <summary>Details</summary>
Motivation: Long-range weather forecasting suffers from error accumulation in autoregressive deep models, especially beyond medium-range horizons. There is a need for scalable, probabilistic models that can forecast across time scales without compounding errors.

Method: Two-stage approach: (1) a VAE encodes raw weather data into a low-dimensional latent space; (2) a diffusion-based transformer generates sequences of future latent tokens conditioned on initial data. Training masks random future tokens and uses a per-token diffusion head to model their distribution; inference unmasking subsets iteratively to sample full future sequences.

Result: OmniCast matches leading probabilistic methods at medium-range, while being 10–20x faster; it achieves state-of-the-art performance at subseasonal-to-seasonal scale across accuracy, physics-based, and probabilistic metrics. It also demonstrates stable rollouts up to 100 years ahead.

Conclusion: A scalable, accurate, and fast framework that unifies weather forecasting across timescales by learning long-horizon dynamics in a latent space with diffusion-based sequence generation. It mitigates autoregressive error accumulation and enables long-term probabilistic forecasts; code and checkpoints are publicly available.

Abstract: Accurate weather forecasting across time scales is critical for anticipating
and mitigating the impacts of climate change. Recent data-driven methods based
on deep learning have achieved significant success in the medium range, but
struggle at longer subseasonal-to-seasonal (S2S) horizons due to error
accumulation in their autoregressive approach. In this work, we propose
OmniCast, a scalable and skillful probabilistic model that unifies weather
forecasting across timescales. OmniCast consists of two components: a VAE model
that encodes raw weather data into a continuous, lower-dimensional latent
space, and a diffusion-based transformer model that generates a sequence of
future latent tokens given the initial conditioning tokens. During training, we
mask random future tokens and train the transformer to estimate their
distribution given conditioning and visible tokens using a per-token diffusion
head. During inference, the transformer generates the full sequence of future
tokens by iteratively unmasking random subsets of tokens. This joint sampling
across space and time mitigates compounding errors from autoregressive
approaches. The low-dimensional latent space enables modeling long sequences of
future latent states, allowing the transformer to learn weather dynamics beyond
initial conditions. OmniCast performs competitively with leading probabilistic
methods at the medium-range timescale while being 10x to 20x faster, and
achieves state-of-the-art performance at the subseasonal-to-seasonal scale
across accuracy, physics-based, and probabilistic metrics. Furthermore, we
demonstrate that OmniCast can generate stable rollouts up to 100 years ahead.
Code and model checkpoints are available at
https://github.com/tung-nd/omnicast.

</details>


### [133] [Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options](https://arxiv.org/abs/2510.18713)
*Joongkyu Lee,Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出并分析一种基于 Plackett-Luce 排序反馈的在线 PbRL 算法 M-AUPO，利用子集大小提升样本效率，给出上界和下界，首次明确显示子集大小提升与性能提升的关系。


<details>
  <summary>Details</summary>
Motivation: PbRL 的样本效率是当前瓶颈，尽管引入排序/多重反馈，但现有理论对非两两比较的情况缺乏提升保证，甚至随反馈长度增加而退化。需要在 PbRL 中充分利用子集排序信息，建立与子集大小相关的理论界限。

Method: 在对行动子集进行 Plackett-Luce 排序建模的基础上提出 M-AUPO 算法，通过最大化子集内的平均不确定性来选择多作用。给出上界分析（suboptimality gap）以及与子集大小相关的样本复杂度推导，并给出与最大子集大小 K 的近似匹配的下界。

Result: 得到子最优性差为 or(d/T) * sqrt(sum_{t=1}^T 1/|S_t|)，T 为轮数，d 为特征维度，|S_t| 为第 t 轮子集大小；下界为 Omega(d/(K sqrt(T)))，其中 K 是最大子集大小。该结果首次在带排序反馈的 PbRL 中，理论上展示了随子集大小增加而提升的样本效率，并且没有对未知参数范数的指数依赖。

Conclusion: 这是当前 PbRL 领域首个明确揭示排序反馈下子集大小提升样本效率的理论结果，理论与对齐任务中的大规模子集反馈具有直接的应用启发，未来工作可在实际系统中验证子集规模与样本效率的关系。

Abstract: We study online preference-based reinforcement learning (PbRL) with the goal
of improving sample efficiency. While a growing body of theoretical work has
emerged-motivated by PbRL's recent empirical success, particularly in aligning
large language models (LLMs)-most existing studies focus only on pairwise
comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,
Thekumparampil et al., 2024) have explored using multiple comparisons and
ranking feedback, but their performance guarantees fail to improve-and can even
deteriorate-as the feedback length increases, despite the richer information
available. To address this gap, we adopt the Plackett-Luce (PL) model for
ranking feedback over action subsets and propose M-AUPO, an algorithm that
selects multiple actions by maximizing the average uncertainty within the
offered subset. We prove that M-AUPO achieves a suboptimality gap of
$\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}}
\right)$, where $T$ is the total number of rounds, $d$ is the feature
dimension, and $|S_t|$ is the size of the subset at round $t$. This result
shows that larger subsets directly lead to improved performance and, notably,
the bound avoids the exponential dependence on the unknown parameter's norm,
which was a fundamental limitation in most previous works. Moreover, we
establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}}
\right)$, where $K$ is the maximum subset size. To the best of our knowledge,
this is the first theoretical result in PbRL with ranking feedback that
explicitly shows improved sample efficiency as a function of the subset size.

</details>


### [134] [Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference](https://arxiv.org/abs/2510.18768)
*Harry Amad,Zhaozhi Qian,Dennis Frauen,Julianna Piskorz,Stefan Feuerriegel,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出 STEAM：一种用于生成含治疗变量的合成数据的方法，旨在保留协变量分布、治疗分配与结果生成机制，以提升治疗效应分析的下游推断性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界医疗数据受监管限制难以获取，合成数据可以帮助开展因果推断与新方法开发；现有生成模型未充分考虑治疗相关任务的特殊性。

Method: 确立含治疗数据的合成数据应满足的 desiderata：保持协变量分布、治疗分配机制、以及结果生成机制；提出相应评估指标；设计 STEAM，在生成过程中模仿真实数据生成过程，优化上述 desiderata。

Result: 在多项评估指标上，STEAM 表现优于现有生成模型，且当真实数据生成过程更复杂时优势尤为明显。

Conclusion: STEAM 能更有效地生成具有治疗信息的合成数据，从而提升下游治疗效应分析的准确性和鲁棒性，为在受限数据环境中的因果推断提供新工具。

Abstract: Causal inference is essential for developing and evaluating medical
interventions, yet real-world medical datasets are often difficult to access
due to regulatory barriers. This makes synthetic data a potentially valuable
asset that enables these medical analyses, along with the development of new
inference methods themselves. Generative models can produce synthetic data that
closely approximate real data distributions, yet existing methods do not
consider the unique challenges that downstream causal inference tasks, and
specifically those focused on treatments, pose. We establish a set of
desiderata that synthetic data containing treatments should satisfy to maximise
downstream utility: preservation of (i) the covariate distribution, (ii) the
treatment assignment mechanism, and (iii) the outcome generation mechanism.
Based on these desiderata, we propose a set of evaluation metrics to assess
such synthetic data. Finally, we present STEAM: a novel method for generating
Synthetic data for Treatment Effect Analysis in Medicine that mimics the
data-generating process of data containing treatments and optimises for our
desiderata. We empirically demonstrate that STEAM achieves state-of-the-art
performance across our metrics as compared to existing generative models,
particularly as the complexity of the true data-generating process increases.

</details>


### [135] [Enhancing Fractional Gradient Descent with Learned Optimizers](https://arxiv.org/abs/2510.18783)
*Jan Sobotka,Petr Šimánek,Pavel Kordík*

Main category: cs.LG

TL;DR: Meta-learned hyperparameter schedule for Caputo FGD (L2O-CFGD) dynamically tunes CFGD hyperparameters, improving convergence and outperforming static CFGD; in some tasks it approaches fully black-box meta-optimizers.


<details>
  <summary>Details</summary>
Motivation: FGD shows promise for accelerating optimization but suffers from convergence issues and difficult hyperparameter tuning, especially in non-convex neural network settings. There is a need for adaptive, history-aware hyperparameter schedules to exploit fractional calculus in optimization.

Method: Introduce L2O-CFGD, a meta-learned optimizer that learns a schedule for CFGD hyperparameters. The meta-learner leverages the history-dependence of Caputo fractional gradients to adapt parameters over time and is evaluated against static CFGD with tuned hyperparameters and against fully black-box meta-optimizers.

Result: The meta-learned schedule outperforms CFGD with static hyperparameters found via exhaustive search and, in some tasks, matches the performance of fully black-box meta-learned optimizers.

Conclusion: L2O-CFGD provides a powerful tool to identify effective hyperparameters and to gain insights into leveraging the history-dependent nature of fractional differentiation in optimization.

Abstract: Fractional Gradient Descent (FGD) offers a novel and promising way to
accelerate optimization by incorporating fractional calculus into machine
learning. Although FGD has shown encouraging initial results across various
optimization tasks, it faces significant challenges with convergence behavior
and hyperparameter selection. Moreover, the impact of its hyperparameters is
not fully understood, and scheduling them is particularly difficult in
non-convex settings such as neural network training. To address these issues,
we propose a novel approach called Learning to Optimize Caputo Fractional
Gradient Descent (L2O-CFGD), which meta-learns how to dynamically tune the
hyperparameters of Caputo FGD (CFGD). Our method's meta-learned schedule
outperforms CFGD with static hyperparameters found through an extensive search
and, in some tasks, achieves performance comparable to a fully black-box
meta-learned optimizer. L2O-CFGD can thus serve as a powerful tool for
researchers to identify high-performing hyperparameters and gain insights on
how to leverage the history-dependence of the fractional differential in
optimization.

</details>


### [136] [CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training](https://arxiv.org/abs/2510.18784)
*Soroush Tabesh,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: CAGE 提出在量化感知训练中引入曲率修正的梯度估计，显著缩小低位宽量化与原生训练之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 尽管存在大量低位宽 QAT，但仍存在与原生训练之间的显著精度差距。需要一种利用局部曲率信息的梯度修正来抵消量化带来的损失。

Method: 在 STE 梯度基础上加入曲率-感知的修正项，来自多目标视角平衡损失最小化与量化约束，理论提出帕累托最优解概念，实现上 optimizer-agnostic，提供利用 Adam 统计量的高效实现。

Result: 在预训练 800M 参数的 Llama风格模型中，在 W4A4 设置下，CAGE 相对于outlier-mitigation 方法恢复了超过 10% 的量化引起的损失增加，缩小了剩余性能差距。

Conclusion: 曲率感知梯度修正可进一步弥合现有方法与原生训练之间的差距，并具备在光滑非凸情形下的收敛保证；实现上也具备高效性和对优化器无关性。

Abstract: Despite significant work on low-bit quantization-aware training (QAT), there
is still a large accuracy gap between such techniques and native training. To
address this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new
QAT method that augments the straight-through estimator (STE) gradient with a
curvature-aware correction designed to counteract the loss increase induced by
quantization. CAGE is derived from a multi-objective view of QAT that balances
loss minimization with adherence to quantization constraints, yielding a
principled correction term that depends on local curvature information. On the
theoretical side, we introduce the notion of Pareto-optimal solutions for
quantized optimization, and establish that CAGE yields strong convergence
guarantees in the smooth non-convex setting. In terms of implementation, our
approach is optimizer-agnostic, but we provide a highly-efficient
implementation that leverages Adam statistics. When pre-training Llama-style
models of up to 800M-parameters, CAGE recovers over 10% of the
quantization-induced loss increase in the W4A4 regime over outlier-mitigation
methods. These results indicate that curvature-aware gradient corrections can
bridge the remaining performance gap beyond current outlier-handling methods.

</details>


### [137] [Stick-Breaking Embedded Topic Model with Continuous Optimal Transport for Online Analysis of Document Streams](https://arxiv.org/abs/2510.18786)
*Federica Granese,Serena Villata,Charles Bouveyron*

Main category: cs.LG

TL;DR: SB-SETM extends Embedded Topic Model to streaming data by (i) using a truncated stick-breaking prior to infer the number of active topics per timestep, and (ii) applying a continuous-optimal-transport-based merging of topic embeddings. It outperforms baselines on simulated data and on a real-world news corpus (Russian-Ukrainian war, 2022–2023).


<details>
  <summary>Details</summary>
Motivation: Tackle online, evolving topic modeling where the number of topics is unknown and topics drift over time. The goal is to automatically infer active topics at each timestep and coherently merge topic embeddings across batches to track evolution.

Method: SB-SETM builds on ETM and processes data streams by aggregating successive partial batches. It employs a truncated stick-breaking construction for the topic-per-document distribution to infer the active topic count, and introduces a merging strategy for topic embeddings via a continuous formulation of optimal transport suited to high-dimensional latent spaces.

Result: Numerical experiments show SB-SETM outperforms baselines on simulated scenarios. Real-world evaluation on a news corpus about the Russia–Ukraine war (2022–2023) demonstrates practical effectiveness.

Conclusion: SB-SETM provides an effective framework for online topic modeling with automatic topic-number inference and principled topic-embedding alignment over time, demonstrated on both synthetic and real streaming data.

Abstract: Online topic models are unsupervised algorithms to identify latent topics in
data streams that continuously evolve over time. Although these methods
naturally align with real-world scenarios, they have received considerably less
attention from the community compared to their offline counterparts, due to
specific additional challenges. To tackle these issues, we present SB-SETM, an
innovative model extending the Embedded Topic Model (ETM) to process data
streams by merging models formed on successive partial document batches. To
this end, SB-SETM (i) leverages a truncated stick-breaking construction for the
topic-per-document distribution, enabling the model to automatically infer from
the data the appropriate number of active topics at each timestep; and (ii)
introduces a merging strategy for topic embeddings based on a continuous
formulation of optimal transport adapted to the high dimensionality of the
latent topic space. Numerical experiments show SB-SETM outperforming baselines
on simulated scenarios. We extensively test it on a real-world corpus of news
articles covering the Russian-Ukrainian war throughout 2022-2023.

</details>


### [138] [When LRP Diverges from Leave-One-Out in Transformers](https://arxiv.org/abs/2510.18810)
*Weiqiu You,Siqi Zeng,Yao-Hung Hubert Tsai,Makoto Yamada,Han Zhao*

Main category: cs.LG

TL;DR: LRP在Transformer中近似LOO存在系统性问题：AttnLRP的双线性传播违反实现不变性；softmax传播路径误差也在影响；通过将relevance仅通过值矩阵传播（CP-LRP变体）在某些设置下更接近LOO；总体上，双线性因子化敏感性和softmax传播误差可能共同削弱LRP的LOO近似能力。


<details>
  <summary>Details</summary>
Motivation: 系统检验LRP在现代Transformer中的公理性，特别是AttnLRP的双线性规则以及CP-LRP作为诊断基线的有效性。

Method: 解析性证明AttnLRP的双线性传播违反实现不变性，并在线性注意力层中进行实证验证；重新评估CP-LRP，考察跳过softmax传播、仅通过值矩阵传播的影响，并比较不同Transformer层的LOO对齐程度。

Result: 证明了实现不变性的违规；将CP-LRP用于诊断时，忽略softmax传播并仅通过值矩阵传播的做法在中后层更接近LOO对齐；softmax传播误差与双线性因子化敏感性共同降低LRP近似LOO的能力。

Conclusion: LRP在Transformer中的近似LOO能力受双线性因子化和softmax传播的影响，需要更鲁棒的传播规则来实现稳定的解释；CP-LRP的变体提供了有价值的诊断视角。

Abstract: Leave-One-Out (LOO) provides an intuitive measure of feature importance but
is computationally prohibitive. While Layer-Wise Relevance Propagation (LRP)
offers a potentially efficient alternative, its axiomatic soundness in modern
Transformers remains largely under-examined. In this work, we first show that
the bilinear propagation rules used in recent advances of AttnLRP violate the
implementation invariance axiom. We prove this analytically and confirm it
empirically in linear attention layers. Second, we also revisit CP-LRP as a
diagnostic baseline and find that bypassing relevance propagation through the
softmax layer -- backpropagating relevance only through the value matrices --
significantly improves alignment with LOO, particularly in middle-to-late
Transformer layers. Overall, our results suggest that (i) bilinear
factorization sensitivity and (ii) softmax propagation error potentially
jointly undermine LRP's ability to approximate LOO in Transformers.

</details>


### [139] [Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards](https://arxiv.org/abs/2510.18814)
*Mengqi Li,Lei Zhao,Anthony Man-Cho So,Ruoyu Sun,Xiao Li*

Main category: cs.LG

TL;DR: OSFT: a simple online supervised finetuning paradigm where the model self-generates data and is finetuned on it, reward-free and efficient, achieving comparable math reasoning performance to RL with rewards; ablation shows efficiency/robustness; leverages latent knowledge; code available.


<details>
  <summary>Details</summary>
Motivation: Reduce the cost and complexity of training LLMs for reasoning by removing the need for reward signals and heavy reinforcement learning, while exploiting the model's existing latent knowledge to improve reasoning.

Method: The model generates its own responses and is finetuned on this self-generated data in an online supervised loop. It uses a single rollout by default, is reward-free, and is evaluated against RL-based methods (e.g., GRPO) on math reasoning tasks; ablation studies assess efficiency and robustness.

Result: OSFT achieves downstream performance on challenging mathematical reasoning tasks comparable to strong RLVR methods like GRPO; demonstrates efficiency and robustness through ablations; relies on latent knowledge from pretraining to improve reasoning.

Conclusion: OSFT offers an efficient and promising alternative to reward-based training paradigms for LLM reasoning, leveraging the model's latent preferences and online self-generated data; code is available at the provided GitHub link.

Abstract: We present a simple, self-help online supervised finetuning (OSFT) paradigm
for LLM reasoning. In this paradigm, the model generates its own responses and
is immediately finetuned on this self-generated data. OSFT is a highly
efficient training strategy for LLM reasoning, as it is reward-free and uses
just one rollout by default. Experiment results show that OSFT achieves
downstream performance on challenging mathematical reasoning tasks comparable
to strong reinforcement learning with verifiable rewards (RLVR) methods such as
GRPO. Our ablation study further demonstrates the efficiency and robustness of
OSFT. The major mechanism of OSFT lies in facilitating the model's own existing
preference (latent knowledge) learned from pretraining, which leads to
reasoning ability improvement. We believe that OSFT offers an efficient and
promising alternative to more complex, reward-based training paradigms. Our
code is available at https://github.com/ElementQi/OnlineSFT.

</details>


### [140] [Search Self-play: Pushing the Frontier of Agent Capability without Supervision](https://arxiv.org/abs/2510.18821)
*Hongliang Lu,Yuhang Wen,Pengyu Cheng,Ruijin Ding,Haotian Xu,Jiaqi Guo,Chutian Wang,Haonan Chen,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: Self-play framework SSP for search agents where a single LLM acts as both task proposer and problem solver to generate and solve deep search queries; uses retrieval-augmented generation to ensure ground-truth; achieves significant performance gains without supervision across benchmarks in both scratch and continuous RL settings; code released.


<details>
  <summary>Details</summary>
Motivation: RLVR relies on human-crafted task queries and ground-truth rewards, which is costly and scales poorly for agentic RL. There is a need for scalable, controllable task generation and RL training without extensive human supervision.

Method: Propose Search Self-Play (SSP): an LLM-based loop where one agent proposes deep search queries with defined ground-truth answers and increasing difficulty, while another solver attempts to answer. The two roles co-evolve through competition and cooperation. Ground-truth is secured by collecting all search results from the proposer as external knowledge and applying retrieval-augmented generation (RAG) to verify answerability on provided docs. Evaluates SSP under from-scratch and continuous RL training across benchmarks.

Result: SSP substantially improves search agents' performance across benchmarks without supervision, in both scratch and continuous RL settings.

Conclusion: SSP is a scalable self-play framework that enables agentic RLVR by jointly generating and solving search tasks, reducing reliance on human-crafted rewards and exemplifying effective co-evolution of task proposers and problem solvers.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become the
mainstream technique for training LLM agents. However, RLVR highly depends on
well-crafted task queries and corresponding ground-truth answers to provide
accurate rewards, which requires massive human efforts and hinders the RL
scaling processes, especially under agentic scenarios. Although a few recent
works explore task synthesis methods, the difficulty of generated agentic tasks
can hardly be controlled to provide effective RL training advantages. To
achieve agentic RLVR with higher scalability, we explore self-play training for
deep search agents, in which the learning LLM utilizes multi-turn search engine
calling and acts simultaneously as both a task proposer and a problem solver.
The task proposer aims to generate deep search queries with well-defined
ground-truth answers and increasing task difficulty. The problem solver tries
to handle the generated search queries and output the correct answer
predictions. To ensure that each generated search query has accurate ground
truth, we collect all the searching results from the proposer's trajectory as
external knowledge, then conduct retrieval-augmentation generation (RAG) to
test whether the proposed query can be correctly answered with all necessary
search documents provided. In this search self-play (SSP) game, the proposer
and the solver co-evolve their agent capabilities through both competition and
cooperation. With substantial experimental results, we find that SSP can
significantly improve search agents' performance uniformly on various
benchmarks without any supervision under both from-scratch and continuous RL
training setups. The code is at https://github.com/Alibaba-Quark/SSP.

</details>


### [141] [A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute COVID-19 Heart Failure](https://arxiv.org/abs/2510.18841)
*Jingya Cheng,Alaleh Azhir,Jiazi Tian,Hossein Estiri*

Main category: cs.LG

TL;DR: 以个体化风险估计和干预分析为目标的反事实推断框架，应用于COVID-19后遗综合征(PASC)在既往心力衰竭(HF)患者中的临床研究。通过纵向数据、正则化预测和反事实搜索，结合NICE与MOC等算法，能够高效探索高维干预空间并给出可解释的个体化反事实解释。模型在2700余名SARS-CoV-2阳性且有HF史的患者上表现良好（AUROC 0.88，95%CI 0.84–0.91），提供可操作的改变共病模式或治疗因素以影响结局的个体化路径。


<details>
  <summary>Details</summary>
Motivation: 将因果推理与预测建模结合起来，解决复杂生物医学系统中的个体化决策问题，特别是在PASC相关HF住院风险的干预分析方面，提供可解释且可操作的推断框架。

Method: 在大规模健康系统队列中使用纵向诊断、实验室和用药数据，进行正则化预测建模并结合反事实搜索；通过精确枚举与基于优化的方法相结合，利用NICE（最近似实例反事实解释）和MOC（多目标反事实）算法高效探索高维干预空间。

Result: 在2700余例确诊SARS-CoV-2且既往HF的个体中，模型实现较强的辨别性能（AUROC 0.88，95% CI 0.84–0.91），并生成可解释、面向患者的反事实解释，量化改变共病模式或治疗因素如何改变预测结局。

Conclusion: 将反事实推理形式化为对预测函数的优化问题，提供一个严谨、可解释且计算高效的个体化推断框架，适用于复杂生物医疗系统的个体化决策。

Abstract: Counterfactual inference provides a mathematical framework for reasoning
about hypothetical outcomes under alternative interventions, bridging causal
reasoning and predictive modeling. We present a counterfactual inference
framework for individualized risk estimation and intervention analysis,
illustrated through a clinical application to post-acute sequelae of COVID-19
(PASC) among patients with pre-existing heart failure (HF). Using longitudinal
diagnosis, laboratory, and medication data from a large health-system cohort,
we integrate regularized predictive modeling with counterfactual search to
identify actionable pathways to PASC-related HF hospital admissions. The
framework combines exact enumeration with optimization-based methods, including
the Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective
Counterfactuals (MOC) algorithms, to efficiently explore high-dimensional
intervention spaces. Applied to more than 2700 individuals with confirmed
SARS-CoV-2 infection and prior HF, the model achieved strong discriminative
performance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable,
patient-specific counterfactuals that quantify how modifying comorbidity
patterns or treatment factors could alter predicted outcomes. This work
demonstrates how counterfactual reasoning can be formalized as an optimization
problem over predictive functions, offering a rigorous, interpretable, and
computationally efficient approach to personalized inference in complex
biomedical systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [142] [Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures](https://arxiv.org/abs/2510.17902)
*Al Kari*

Main category: cs.AI

TL;DR: CAST通过激活空间映射实现LoRA跨模型的零-shot迁移，解耦技能与源架构，达到85-95%性能，显著优于权重空间迁移。


<details>
  <summary>Details</summary>
Motivation: 现有转移方法依赖对齐静态权重空间，脆弱且间接；需要一种直接、跨架构的技能迁移方法，使LoRA的行为不被源模型架构锁定。

Method: 将LoRA视为冻结的行为核，学习一组轻量的双向投影头，将目标模型的激活流投影到源模型的潜在空间，应用冻结核后再投影回去；在通用文本语料上训练，未使用任务数据；通过非线性映射实现两种架构激活几何的对齐。

Result: 跨模型（如Llama-2与Mistral）实验表明，CAST翻译的LoRA在目标模型上的性能可达到原始LoRA在目标模型上的85-95%，显著优于现有权重空间转移，推动模型互操作性成为新基准。

Conclusion: CAST实现了LoRA跨架构的真正零-shot翻译，解耦技能与源架构，提升模型互操作性，确立了新的行业标准。

Abstract: The proliferation of Large Language Model (LLM) architectures presents a
fundamental challenge: valuable, task-specific behaviors learned through
fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped
within their source model's architecture, herein referred to architectural
lock-in. Existing transfer methods attempt to bridge this gap by aligning the
static weight spaces of models, a brittle and indirect approach that relies on
tenuous correlations between parameter geometries. This paper introduces a
fundamentally different and more direct paradigm: the Cartridge Activation
Space Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors
by learning a direct, nonlinear mapping between the activation manifolds, the
geometric structures formed by the model's internal neuron activations, of two
distinct LLM architectures. CAST treats a pre-trained LoRA as a frozen
"behavioral kernel." It learns a set of lightweight, bidirectional projection
heads that translate the target model's activation stream into the source
model's latent space, apply the frozen kernel, and project the result back.
This process, trained on a general text corpus without any task-specific data,
effectively decouples the learned skill from the source architecture. We
demonstrate that CAST enables true "zero-shot" translation of any standard LoRA
adapter. Our experiments, including transfers between heterogeneous model
families like Llama-2 and Mistral, show that CAST-translated adapters achieve
85-95\% of the performance of a LoRA fully retrained on the target model,
quantitatively outperforming current weight-space transfer techniques and
establishing a new state-of-the-art in model interoperability.

</details>


### [143] [Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding](https://arxiv.org/abs/2510.17940)
*Zhiming Lin*

Main category: cs.AI

TL;DR: Diversified in-context retrieval improves budget-constrained multi-turn intent understanding, outperforming longer prompts under fixed token budgets.


<details>
  <summary>Details</summary>
Motivation: Under tight token budgets, retrieval methods focusing on relevance may overlook diversity and exemplar order, potentially harming intent understanding. This study investigates whether increasing content diversity in retrieved exemplars can boost performance without increasing token usage.

Method: Proposes a diversity-aware retrieval framework that selects in-context exemplars to balance intent coverage and linguistic variety, integrated with standard LLM decoders. Evaluations enforce budget-matched prompts with randomized exemplar positions, and conduct sensitivity analyses over exemplar count (K), diversity strength, and backbone size. Datasets: MultiWOZ 2.4 and SGD.

Result: Achieves strong gains in Joint Goal Accuracy under equal token budgets, surpassing strong LLM/DST baselines. Improvements are consistent across K in {4,7} and come with moderate latency.

Conclusion: Content diversity in retrieval materially impacts performance; provides a simple, deployable principle for building accurate, budget-constrained multi-turn intent systems.

Abstract: Multi turn intent understanding is central to task oriented chatbots, yet
real deployments face tight token budgets and noisy contexts, and most
retrieval pipelines emphasize relevance while overlooking set level diversity
and confounds such as more context or exemplar order. We ask whether retrieval
diversity, rather than longer prompts, systematically improves LLM intent
understanding under fixed budgets. We present a diversity aware retrieval
framework that selects in context exemplars to balance intent coverage and
linguistic variety, and integrates this selection with standard LLM decoders;
the evaluation enforces budget matched prompts and randomized positions, and
includes sensitivity analyses over exemplar count, diversity strength, and
backbone size. On MultiWOZ 2.4 and SGD, the approach achieves strong gains in
Joint Goal Accuracy under equal token budgets, surpassing strong LLM/DST
baselines, with consistent improvements across K from 4 to 7 and moderate
latency. Overall, the study isolates and validates the impact of content
diversity in retrieval and offers a simple, deployable selection principle for
building accurate, budget constrained multi turn intent systems.

</details>


### [144] [Subject-Event Ontology Without Global Time: Foundations and Execution Semantics](https://arxiv.org/abs/2510.18040)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 提出一种面向动态复杂系统的主客观事件本体 formalization，脱离全局时间，基于事件、因果次序、数据流执行及知识过滤等原则，确保确定性与可执行性；在 Boldsea 系统中得到验证，并适用于分布式系统、微服务、DLT 等场景。


<details>
  <summary>Details</summary>
Motivation: 旨在在没有全局时间的前提下，对复杂动态系统进行建模，解决跨主体的因果关系与知识局限性问题，同时通过可执行的本体实现来确保确定性和可计算性。

Method: 提出包含九条公理(A1-A9)的形式化框架，强调事件作为固定行为、happens-before 的因果序、通过声明式数据流实现可执行性、模型作为 epistemic 过滤、以及事件的真理推定。特别关注模型驱动的方法(A9)、事件通过模式/架构(Schemas)验证、参与者授权、在没有全局时间条件下自动构建因果链(W3)。通过 Boldsea 工作流引擎(Boldsea Semantic Language, BSL) 实现上述理论，形成 I1-I3 等历史单调性、因果回路无环性、可追溯性等性质，确保可执行本体的正确性。

Result: 在 Boldsea 系统中实现并验证了该理论构造，展示了将抽象本体转换为可执行工作流与分布式协作的可行性，且具备适用于分布式系统、微服务架构、区块链/分布式账本平台以及多视角冲突事实场景的潜力。

Conclusion: 该 formalization 提供了一个在无全局时间背景下对多主体协作系统进行可执行本体建模的严密基础，确保因果性、历史单调性、可追溯性及一致性；通过 W3 等机制实现无时钟依赖的事件链路推导与模式化验证，利于在分布式与去中心化环境中的冲突事实整合。

Abstract: A formalization of a subject-event ontology is proposed for modeling complex
dynamic systems without reliance on global time. Key principles: (1) event as
an act of fixation - a subject discerns and fixes changes according to models
(conceptual templates) available to them; (2) causal order via happens-before -
the order of events is defined by explicit dependencies, not timestamps; (3)
making the ontology executable via a declarative dataflow mechanism, ensuring
determinism; (4) models as epistemic filters - a subject can only fix what
falls under its known concepts and properties; (5) presumption of truth - the
declarative content of an event is available for computation from the moment of
fixation, without external verification. The formalization includes nine axioms
(A1-A9), ensuring the correctness of executable ontologies: monotonicity of
history (I1), acyclicity of causality (I2), traceability (I3). Special
attention is given to the model-based approach (A9): event validation via
schemas, actor authorization, automatic construction of causal chains (W3)
without global time. Practical applicability is demonstrated on the boldsea
system - a workflow engine for executable ontologies, where the theoretical
constructs are implemented in BSL (Boldsea Semantic Language). The
formalization is applicable to distributed systems, microservice architectures,
DLT platforms, and multiperspectivity scenarios (conflicting facts from
different subjects).

</details>


### [145] [Planned Diffusion](https://arxiv.org/abs/2510.18087)
*Daniel Israel,Tian Jin,Ellie Cheng,Guy Van den Broeck,Aditya Grover,Suvinay Subramanian,Michael Carbin*

Main category: cs.AI

TL;DR: Two-stage planned diffusion for LLMs: first create a short autoregressive plan that divides output into smaller, independent spans, then diffusively generate all spans in parallel. It improves speed-quality trade-offs compared to autoregressive models, achieving significant latency reductions with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Address the speed-quality Pareto trade-off in large language model generation: autoregressive methods yield high quality but slow; diffusion methods enable parallel generation but often need many iterations for comparable quality.

Method: A two-stage approach: (1) plan stage — generate a short autoregressive plan that partitions the final output into independent spans; (2) diffusion stage — run diffusion-based generation to fill all spans simultaneously, leveraging parallelism; minimal planning overhead with tunable runtime knobs to control quality vs latency.

Result: Empirical evaluation on AlpacaEval (805 prompts) shows Pareto-optimal quality-latency trade-off, with speedups of 1.27x to 1.81x over autoregressive generation and only 0.87% to 5.4% drop in win rate. Sensitivity analysis indicates planning is robust and lightweight; runtime knobs provide flexible control over the trade-off.

Conclusion: Planned diffusion broadens the speed-quality Pareto frontier and offers a practical path to faster, high-quality text generation. The approach requires a minimal planning step but yields substantial latency gains with modest quality loss and configurable control.

Abstract: A central challenge in large language model inference is the trade-off
between generation speed and output quality. Autoregressive models produce
high-quality text but generate tokens sequentially. Diffusion models can
generate tokens in parallel but often need many iterations to match the same
quality. We propose planned diffusion, a hybrid method that combines the
strengths of both paradigms. Planned diffusion works in two stages: first, the
model creates a short autoregressive plan that breaks the output into smaller,
independent spans. Second, the model generates these spans simultaneously using
diffusion. This approach expands the speed-quality Pareto frontier and provides
a practical path to faster, high-quality text generation. On AlpacaEval, a
suite of 805 instruction-following prompts, planned diffusion achieves
Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x
speedup over autoregressive generation with only 0.87\% to 5.4\% drop in win
rate, respectively. Our sensitivity analysis shows that the planning mechanism
of planned diffusion is minimal and reliable, and simple runtime knobs exist to
provide flexible control of the quality-latency trade-off.

</details>


### [146] [Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models](https://arxiv.org/abs/2510.18143)
*Huan Song,Deeksha Razdan,Yiyue Qian,Arijit Ghosh Chowdhury,Parth Patwa,Aman Chadha,Shinan Zhang,Sharlina Keshava,Hannah Marlowe*

Main category: cs.AI

TL;DR: 提出PaDA-Agent，一种基于评估驱动的数据增强框架，用于小语言模型的增量数据增强。通过评估验证集发现失败模式，并据此设计有针对性的数据增强策略，以直接缩小泛化差距，实验上优于针对Llama 3.2 1B Instruct的现有LLM数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在成本与延迟方面具优势，但在复杂领域任务上的准确率落后于大模型。监督微调需要大量数据准备和迭代优化，亟需一种能自动化、面向评估目标的数据增强方法来缩小泛化差距。

Method: 通过在验证数据上进行评估以发现失败模式，然后据此草拟有针对性的数据增强策略，并进行协同数据增强操作。该方法不仅关注模型的训练错误，而是旨在直接降低泛化误差，最终提升SLMs在目标任务上的表现。

Result: 在对Llama 3.2 1B Instruct模型进行微调的实验中，PaDA-Agent显著优于现有基于LLM的数据增强方法。

Conclusion: 评估驱动的数据增强能够有效缩小小语言模型的泛化差距，PaDA-Agent提供了一种可操作、面向评估目标的增强框架，具有良好推广潜力。

Abstract: Small Language Models (SLMs) offer compelling advantages in deployment cost
and latency, but their accuracy often lags behind larger models, particularly
for complex domain-specific tasks. While supervised fine-tuning can help bridge
this performance gap, it requires substantial manual effort in data preparation
and iterative optimization. We present PaDA-Agent (Pattern-guided Data
Augmentation Agent), an evaluation-driven approach that streamlines the data
augmentation process for SLMs through coordinated operations. Unlike
state-of-the-art approaches that focus on model training errors only and
generating error-correcting samples, PaDA-Agent discovers failure patterns from
the validation data via evaluations and drafts targeted data augmentation
strategies aiming to directly reduce the generalization gap. Our experimental
results demonstrate significant improvements over state-of-the-art LLM-based
data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.

</details>


### [147] [Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety](https://arxiv.org/abs/2510.18154)
*Antonio-Gabriel Chacón Menke,Phan Xuan Tan,Eiji Kamioka*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent work has highlighted the importance of monitoring chain-of-thought
reasoning for AI safety; however, current approaches that analyze textual
reasoning steps can miss subtle harmful patterns and may be circumvented by
models that hide unsafe reasoning. We present a sentence-level labeled dataset
that enables activation-based monitoring of safety behaviors during LLM
reasoning. Our dataset contains reasoning sequences with sentence-level
annotations of safety behaviors such as expression of safety concerns or
speculation on user intent, which we use to extract steering vectors for
detecting and influencing these behaviors within model activations. The dataset
fills a key gap in safety research: while existing datasets label reasoning
holistically, effective application of steering vectors for safety monitoring
could be improved by identifying precisely when specific behaviors occur within
reasoning chains. We demonstrate the dataset's utility by extracting
representations that both detect and steer safety behaviors in model
activations, showcasing the potential of activation-level techniques for
improving safety oversight on reasoning.
  Content Warning: This paper discusses AI safety in the context of harmful
prompts and may contain references to potentially harmful content.

</details>


### [148] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 提出一种训练无关的采样算法 Saber，结合自适应加速与回溯增强重掩码，在生成过程前向和回溯的协同作用下提升扩散语言模型在代码生成任务上的推理速度与输出质量；在多项基准上平均提升 Pass@1 1.9%，推理速度平均加速251.4%，显著缩小与自回归模型的差距。


<details>
  <summary>Details</summary>
Motivation: Diffusion language models 虽具并行生成和双向上下文建模优势，但在代码生成任务中往往在推理速度与输出质量之间存在显著权衡。减少采样步数虽能提速，却会导致性能崩塌，因此需要一种在不引入额外训练成本的前提下同时提升速度与质量的采样策略。

Method: Saber 是一种训练无关的采样算法，基于两点关键直觉：1) 生成过程可在建立更多上下文时对采样进行自适应加速；2) 引入回溯机制以便在必要时回退已生成的tokens并进行重掩码(remasking)以纠错。整合自适应加速与回溯增强重掩码的策略，从而在不额外训练的情况下提升代码生成的质量与推理速度。

Result: 在多个主流代码生成基准上，Saber 相较于主流 DLM 采样方法平均提升 Pass@1 1.9%，同时实现平均 251.4% 的推理速度提升。该方法显著提升了 DLM 在代码生成任务的性能，缩小了与自回归模型之间的差距。

Conclusion: Saber 作为一种训练无关的采样算法，成功在速度与质量之间实现更优的折中，进一步拉近扩散语言模型与自回归模型在代码生成任务上的差距，并具有潜在的泛化性与在其他任务中的应用前景。

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [149] [A Definition of AGI](https://arxiv.org/abs/2510.18212)
*Dan Hendrycks,Dawn Song,Christian Szegedy,Honglak Lee,Yarin Gal,Erik Brynjolfsson,Sharon Li,Andy Zou,Lionel Levine,Bo Han,Jie Fu,Ziwei Liu,Jinwoo Shin,Kimin Lee,Mantas Mazeika,Long Phan,George Ingebretsen,Adam Khoja,Cihang Xie,Olawale Salaudeen,Matthias Hein,Kevin Zhao,Alexander Pan,David Duvenaud,Bo Li,Steve Omohundro,Gabriel Alfour,Max Tegmark,Kevin McGrew,Gary Marcus,Jaan Tallinn,Eric Schmidt,Yoshua Bengio*

Main category: cs.AI

TL;DR: 用CHC认知理论构建的可量化AGI框架；通过 ten个认知领域评估AI，揭示当前模型的“锯齿”型能力分布与持续的改进空间，给出GPT-4/ GPT-5的初步AGI分数。


<details>
  <summary>Details</summary>
Motivation: 填补对AGI缺乏明确可量化定义的空白；以人类认知结构为基准，提供可操作的评估体系，以便衡量AI向人类水平认知的接近程度。

Method: 基于Cattell-Horn-Carroll理论，将一般智能拆解为包括推理、记忆、知觉等十个认知领域；改编人类心理测量量表来评估AI系统在这些领域的表现。

Result: 在“锯齿状”认知画像中，AI在知识密集型领域表现较好，但在基础认知机制（尤其是长期记忆存储）存在明显缺陷；给出具体AGI分数（如GPT-4 27%，GPT-5 58%），显示进展与差距并存。

Conclusion: 该框架提供了一个可量化的AGI评估基线，展示了显著进步但仍有较大改进空间；未来工作可聚焦于加强基础认知机制与跨领域整合能力的提升。

Abstract: The lack of a concrete definition for Artificial General Intelligence (AGI)
obscures the gap between today's specialized AI and human-level cognition. This
paper introduces a quantifiable framework to address this, defining AGI as
matching the cognitive versatility and proficiency of a well-educated adult. To
operationalize this, we ground our methodology in Cattell-Horn-Carroll theory,
the most empirically validated model of human cognition. The framework dissects
general intelligence into ten core cognitive domains-including reasoning,
memory, and perception-and adapts established human psychometric batteries to
evaluate AI systems. Application of this framework reveals a highly "jagged"
cognitive profile in contemporary models. While proficient in
knowledge-intensive domains, current AI systems have critical deficits in
foundational cognitive machinery, particularly long-term memory storage. The
resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify
both rapid progress and the substantial gap remaining before AGI.

</details>


### [150] [ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning](https://arxiv.org/abs/2510.18250)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Cancheng Zhang,Xiangdong Zhang,Mingquan Feng,Jingzhi Wang,Junchi Yan*

Main category: cs.AI

TL;DR: 提出 ssToken：通过自调制的按-token Loss 差值与语义感知的注意力权重，结合历史模型与当前模型进行令牌级数据筛选，以提升SFT效果并保持训练效率。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的监督微调中，数据质量对效果影响显著。现有的按-token 选择方法受限于需要额外参考模型和仅依赖损失信息，难以保留对语义重要但损失不高的令牌。

Method: 使用历史模型来计算当前模型与历史模型之间的逐-token 损失差作为自调制信号，使令牌选择随优化过程而非依赖离线参考模型而动态调整。同时引入一个与损失无关的、基于注意力的语义感知令牌重要性度量，提供补充的语义信息以提升筛选效果。

Result: 单独的自调制选择和语义感知选择均优于全量微调；将两者结合的 ssToken 取得协同增益，超过以往的按-token 选择方法，同时保持训练效率。

Conclusion: 通过自调制的损失差与语义感知的重要性评估的结合，ssToken 能更有效地筛选对SFT有益的高质量令牌。

Abstract: Data quality plays a critical role in enhancing supervised fine-tuning (SFT)
for large language models (LLMs), and token-level data selection has emerged as
a promising direction for its fine-grained nature. Despite their strong
empirical performance, existing token-level selection methods share two key
limitations: (1) requiring training or accessing an additional reference model,
and (2) relying solely on loss information for token selection, which cannot
well preserve semantically important tokens that are not favored by loss-based
metrics. To address these challenges, we propose ssToken, a Self-modulated and
Semantic-aware Token Selection approach. ssToken leverages readily accessible
history models to compute the per-token loss difference with the current model,
which serves as a self-modulated signal that enables the model to adaptively
select tokens along its optimization trajectory, rather than relying on excess
loss from an offline-trained reference model as in prior works. We further
introduce a semantic-aware, attention-based token importance estimation metric,
orthogonal to loss-based selection and providing complementary semantic
information for more effective filtering. Extensive experiments across
different model families and scales demonstrate that both self-modulated
selection and semantic-aware selection alone outperform full-data fine-tuning,
while their integration--ssToken--achieves synergistic gains and further
surpasses prior token-level selection methods, delivering performance
improvements while maintaining training efficiency.

</details>


### [151] [Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](https://arxiv.org/abs/2510.18254)
*Sion Weatherhead,Flora Salim,Aaron Belbasis*

Main category: cs.AI

TL;DR: LLM 'reflection' yields modest gains but lacks evidence of genuine, goal-driven self-correction; open-ended constraint satisfaction tasks reveal limits; external structure is needed to enforce constraints.


<details>
  <summary>Details</summary>
Motivation: To test whether current LLM-derived reflection mirrors human reflective reasoning and improves performance on a real-world, constraint-bound task.

Method: Eight frontier models were evaluated on producing valid scientific test items (open-ended but rule-constrained). Each item required to be valid; models then revised after considering their own critique. Measured first-pass performance, then performance after reflection, across varying degrees of open-endedness.

Result: First-pass performance was poor (often zero valid items out of 4; mean ~1). Reflection yielded only modest gains (≈1). The second attempt frequently repeated constraint violations, suggesting gains come from chance rather than principled error detection. Performance deteriorates as open-endedness increases; models marketed for 'reasoning' show no advantage.

Conclusion: Current LLM 'reflection' lacks functional evidence of active, goal-driven monitoring. To respect constraints reliably, external structures that enforce constraints are necessary until models instantiate such internal mechanisms.

Abstract: Humans do not just find mistakes after the fact -- we often catch them
mid-stream because 'reflection' is tied to the goal and its constraints.
Today's large language models produce reasoning tokens and 'reflective' text,
but is it functionally equivalent with human reflective reasoning? Prior work
on closed-ended tasks -- with clear, external 'correctness' signals -- can make
'reflection' look effective while masking limits in self-correction. We
therefore test eight frontier models on a simple, real-world task that is
open-ended yet rule-constrained, with auditable success criteria: to produce
valid scientific test items, then revise after considering their own critique.
First-pass performance is poor (often zero valid items out of 4 required; mean
$\approx$ 1), and reflection yields only modest gains (also $\approx$ 1).
Crucially, the second attempt frequently repeats the same violation of
constraint, indicating 'corrective gains' arise largely from chance production
of a valid item rather than error detection and principled,
constraint-sensitive repair. Performance before and after reflection
deteriorates as open-endedness increases, and models marketed for 'reasoning'
show no advantage. Our results suggest that current LLM 'reflection' lacks
functional evidence of the active, goal-driven monitoring that helps humans
respect constraints even on a first pass. Until such mechanisms are
instantiated in the model itself, reliable performance requires external
structure that enforces constraints.

</details>


### [152] [ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.18342)
*Peng Tang,Xiaoxiao Yan,Xiaobin Hu,Yuning Cui,Donghao Luo,Jiangning Zhang,Pengcheng Xu,Jinlong Peng,Qingdong He,Feiyue Huang,Song Xue,Tobias Lasser*

Main category: cs.AI

TL;DR: 提出 ShortcutBreaker，一体化 MUAD 框架，利用低秩噪声瓶颈和全局扰动注意力，在四个基准上达到SOTA AUROC。


<details>
  <summary>Details</summary>
Motivation: MUAD 需要统一的跨类检测模型，减少多模型训练成本；Transformer 容易出现身份捷径，削弱正常与异常的区分度。

Method: LRNB 将高维特征投影到低秩潜在空间，理论证明抑制简单身份重现；引入全局扰动注意力，利用 ViTs 的全局建模能力在解码阶段阻断信息捷径。

Result: 在 MVTec-AD、ViSA、Real-IAD、Universal Medical 四个数据集上，图像级 AUROC 分别为 99.8%、98.9%、90.6%、87.8%，显著优于现有 MUAD 方法。

Conclusion: 通过低秩约束与全局扰动注意力的结合，缓解身份捷径问题，提升跨类 MUAD 的检测效果与稳定性，体现全局建模与低秩表示的协同作用。

Abstract: Multi-class unsupervised anomaly detection (MUAD) has garnered growing
research interest, as it seeks to develop a unified model for anomaly detection
across multiple classes, i.e., eliminating the need to train separate models
for distinct objects and thereby saving substantial computational resources.
Under the MUAD setting, while advanced Transformer-based architectures have
brought significant performance improvements, identity shortcuts persist: they
directly copy inputs to outputs, narrowing the gap in reconstruction errors
between normal and abnormal cases, and thereby making the two harder to
distinguish. Therefore, we propose ShortcutBreaker, a novel unified
feature-reconstruction framework for MUAD tasks, featuring two key innovations
to address the issue of shortcuts. First, drawing on matrix rank inequality, we
design a low-rank noisy bottleneck (LRNB) to project highdimensional features
into a low-rank latent space, and theoretically demonstrate its capacity to
prevent trivial identity reproduction. Second, leveraging ViTs global modeling
capability instead of merely focusing on local features, we incorporate a
global perturbation attention to prevent information shortcuts in the decoders.
Extensive experiments are performed on four widely used anomaly detection
benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)
and one medical dataset (Universal Medical). The proposed method achieves a
remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four
datasets, respectively, consistently outperforming previous MUAD methods across
different scenarios.

</details>


### [153] [Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents](https://arxiv.org/abs/2510.18424)
*Guangfu Guo,Xiaoqian Lu,Yue Feng*

Main category: cs.AI

TL;DR: 提出 Med-VRAgent，结合可视化引导、MCTS 与自我奖励，提升医疗视觉推理能力并通过轨迹反馈进行 PPO 微调，在多项医疗 VQA 基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在医疗推理中的幻觉、模糊描述、推理不一致与定位困难等问题，提升推理准确性与可解释性。

Method: 提出 Med-VRAgent，基于 Visual Guidance 与蒙特卡洛树搜索（MCTS），通过轨迹收集作为反馈，提升推理过程。随后使用收集的轨迹对 VLMs 进行近端策略优化（PPO）微调。

Result: 在多项医疗 VQA 基准实验中，该框架显著优于现有方法，体现出更好的推理与定位能力。

Conclusion: 将视觉引导与树搜索相结合的 Med-VRAgent 能有效提升医疗视觉推理能力，轨迹反馈用于 PPO 微调进一步增强模型性能与稳定性。

Abstract: Visual Language Models (VLMs) achieve promising results in medical reasoning
but struggle with hallucinations, vague descriptions, inconsistent logic and
poor localization. To address this, we propose a agent framework named Medical
Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual
Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By
combining the Visual Guidance with tree search, Med-VRAgent improves the
medical visual reasoning capabilities of VLMs. We use the trajectories
collected by Med-VRAgent as feedback to further improve the performance by
fine-tuning the VLMs with the proximal policy optimization (PPO) objective.
Experiments on multiple medical VQA benchmarks demonstrate that our method
outperforms existing approaches.

</details>


### [154] [Automated urban waterlogging assessment and early warning through a mixture of foundation models](https://arxiv.org/abs/2510.18425)
*Chenxu Zhang,Fuxiang Huang,Lei Zhang*

Main category: cs.AI

TL;DR: 基于基础模型的城市内涝评估框架UWAssess，能够在监控图像中自动识别积水区域并生成结构化评估报告；通过半监督微调与链式思维提示应对数据匮乏，提升感知与文本生成的双重能力，并通过多模型协作实现可扩展的城市管理应用。


<details>
  <summary>Details</summary>
Motivation: 城市暴雨和气候变化导致的城市内涝日益严重，现有监测多依赖人工报告，缺乏及时性和全面性；标注数据稀缺，需依托基础模型进行高效泛化与下游任务适配；通过感知-生成一体化框架提升监测能力与决策支持。

Method: 以基础模型为核心的框架，自动在监控图像中识别积水区域并生成结构化评估报告；引入半监督微调策略以利用未标注数据；采用链式思维（CoT）提示提升下游文本推理与报告质量；进行GPT等评估以验证文本报告的可靠性；实现感知与文本生成的协同，促进多模型协作。

Result: 在具有挑战性的视觉基准上实现感知性能显著提升；GPT评估表明UWAssess能够生成描述积水范围、深度、风险及影响的可靠文本报告，体现了从感知向生成的转变及框架的可扩展性。

Conclusion: 感知与生成的双能力以及多基础模型协作为智能、可扩展的城市水灾监测系统奠定基础，有助于城市管理、灾害响应和气候韧性建设。

Abstract: With climate change intensifying, urban waterlogging poses an increasingly
severe threat to global public safety and infrastructure. However, existing
monitoring approaches rely heavily on manual reporting and fail to provide
timely and comprehensive assessments. In this study, we present Urban
Waterlogging Assessment (UWAssess), a foundation model-driven framework that
automatically identifies waterlogged areas in surveillance images and generates
structured assessment reports. To address the scarcity of labeled data, we
design a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)
prompting strategy to unleash the potential of the foundation model for
data-scarce downstream tasks. Evaluations on challenging visual benchmarks
demonstrate substantial improvements in perception performance. GPT-based
evaluations confirm the ability of UWAssess to generate reliable textual
reports that accurately describe waterlogging extent, depth, risk and impact.
This dual capability enables a shift of waterlogging monitoring from perception
to generation, while the collaborative framework of multiple foundation models
lays the groundwork for intelligent and scalable systems, supporting urban
management, disaster response and climate resilience.

</details>


### [155] [AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library](https://arxiv.org/abs/2510.18428)
*Minwei Kong,Ao Qu,Xiaotong Guo,Wenbin Ouyang,Chonghe Jiang,Han Zheng,Yining Ma,Dingyi Zhuang,Yuhan Tang,Junyi Li,Hai Wang,Cathy Wu,Jinhua Zhao*

Main category: cs.AI

TL;DR: AlphaOPT 是一个自我提升的经验库，使大语言模型能够在有限演示条件下学习，并通过库的演化持续改进求解能力，无需权重更新。


<details>
  <summary>Details</summary>
Motivation: 自动化优化建模仍受限于将非正式语言映射到精确的数学表述和可执行代码，现有方法要么对提示脆弱、要么通过昂贵再训练导致泛化受限。

Method: 提出两阶段持续循环：Library Learning（从失败中提取结构化知识，如分类法、条件、解释、示例并存入库）与 Library Evolution（诊断检索错配、修正库中知识的适用条件以提升跨任务迁移），通过更新库而非模型权重实现自我改进。

Result: 随着训练样本从100增至300，准确率由65%提升至72%；在Out-of-distribution OptiBench数据集上，仅用答案训练时，超过最强基线7.7%。

Conclusion: 该框架实现高效、持续的知识积累与迁移，并且知识透明、可被人类干预，避免对模型权重的反复更新。

Abstract: Optimization modeling enables critical decisions across industries but
remains difficult to automate: informal language must be mapped to precise
mathematical formulations and executable solver code. Prior LLM approaches
either rely on brittle prompting or costly retraining with limited
generalization. We present AlphaOPT, a self-improving experience library that
enables an LLM to learn from limited demonstrations (even answers alone,
without gold-standard programs) and solver feedback - without annotated
reasoning traces or parameter updates. AlphaOPT operates in a continual
two-phase cycle: (i) a Library Learning phase that reflects on failed attempts,
extracting solver-verified, structured insights as {taxonomy, condition,
explanation, example}; and (ii) a Library Evolution phase that diagnoses
retrieval misalignments and refines the applicability conditions of stored
insights, improving transfer across tasks. This design (1) learns efficiently
from limited demonstrations without curated rationales, (2) expands continually
without costly retraining by updating the library rather than model weights,
and (3) makes knowledge explicit and interpretable for human inspection and
intervention. Experiments show that AlphaOPT steadily improves with more data
(65% to 72% from 100 to 300 training items) and surpasses the strongest
baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only
on answers. Code and data are available at:
https://github.com/Minw913/AlphaOPT.

</details>


### [156] [PlanU: Large Language Model Decision Making through Planning under Uncertainty](https://arxiv.org/abs/2510.18442)
*Ziwei Deng,Mian Deng,Chenjing Liang,Zeming Gao,Chennan Ma,Chenxing Lin,Haipeng Zhang,Songzhu Mei,Cheng Wang,Siqi Shen*

Main category: cs.AI

TL;DR: PlanU 在 LLM 基于规划的决策中，将节点回报建模为分位数分布，并通过蒙特卡洛树搜索（MCTS）进行搜索，引入带好奇心的上置信界（UCC）分数以平衡探索与利用，从而在存在环境不确定性的多步决策任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: LLMs 在决策任务中存在两层不确定性：来自生成过程的 LLM 不确定性，以及环境本身的随机性。现有方法多聚焦于 LLM 不确定性（如多条推理链或搜索树）而忽略环境不确定性，且以预测未知变量概率的方法往往不适用于需要与环境交互的多步任务。因此需要一个在规划阶段即可对不确定性建模的框架，以提高 LLM 在不确定环境中的决策效果。

Method: 将每个 MCTS 节点的回报建模为分位数分布，利用一组分位数来表示回报的不确定性；引入上置信界并带有好奇心特征的 UCC 分数，用以估算 MCTS 节点的不确定性并指导搜索的探索–利用权衡；结合 LLM 推理与环境交互，进行面向不确定性的多步决策规划。

Result: 通过大量实验，验证 PlanU 在含不确定性的 LLM 决策任务中的有效性，显示对不确定性具有鲁棒性并提升了决策表现。

Conclusion: PlanU 将不确定性建模嵌入 MCTS，并通过 UCC 分数提升对不确定性的估计与探索–利用权衡，扩展了面向不确定环境的 LLM 规划能力，尤其适用于需要与环境交互的多步决策。

Abstract: Large Language Models (LLMs) are increasingly being explored across a range
of decision-making tasks. However, LLMs sometimes struggle with decision-making
tasks under uncertainty that are relatively easy for humans, such as planning
actions in stochastic environments. The adoption of LLMs for decision-making is
impeded by uncertainty challenges, such as LLM uncertainty and environmental
uncertainty. LLM uncertainty arises from the stochastic sampling process
inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM
uncertainty through multiple reasoning chains or search trees. However, these
approaches overlook environmental uncertainty, which leads to poor performance
in environments with stochastic state transitions. Some recent LDM approaches
deal with uncertainty by forecasting the probability of unknown variables.
However, they are not designed for multi-step decision-making tasks that
require interaction with the environment. To address uncertainty in LLM
decision-making, we introduce PlanU, an LLM-based planning method that captures
uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of
each node in the MCTS as a quantile distribution, which uses a set of quantiles
to represent the return distribution. To balance exploration and exploitation
during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity
(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive
experiments, we demonstrate the effectiveness of PlanU in LLM-based
decision-making tasks under uncertainty.

</details>


### [157] [CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs](https://arxiv.org/abs/2510.18470)
*Shaobo Wang,Yongliang Miao,Yuancheng Liu,and Qianli Ma,Ning Liao,Linfeng Zhang*

Main category: cs.AI

TL;DR: CircuitSeer 通过识别大型语言模型中用于推理的稀疏注意头子集，建立核心推理电路，并以这些电路对数据的影响来衡量推理复杂度，从而进行数据筛选。在4个模型、9个数据集上实验，优于基线；在仅用10%数据的微调中，Qwen2.5-Math-7B 的平均Pass@1比使用全量数据训练高出1.4点。


<details>
  <summary>Details</summary>
Motivation: 现有的数据筛选方法往往依赖代价高昂的外部模型或不透明的启发式规则，难以直接与模型的推理能力对齐。为提升数据选择的效率和可解释性，本文转而从模型内部机制出发，利用推理过程中的核心电路来评估数据的质量与贡献。

Method: 发现复杂推理任务会激活一组稀疏、专业化的注意头子集，形成核心推理电路。CircuitSeer 通过量化数据对这些核心电路的影响来评估推理复杂度并进行数据选择。系统在4个模型、9个数据集上进行广泛实验，并在对比基线方法时展示优越性。以 Qwen2.5-Math-7B 为例，在仅用选自数据的10%上微调，平均 Pass@1 相对于使用全量数据训练提升1.4点。

Result: 在多任务和跨模型设置中，CircuitSeer 展示出对数据选择的显著改进，数据效率高，且对不同数据集和模型具有鲁棒性。该方法实现了用少量高质量子集替代全量数据的训练效果的显著提升。

Conclusion: 基于模型内部推理电路的 CircuitSeer 提供了一种高效且更具可解释性的对数据进行筛选的策略，能够降低大模型推理训练的成本并提升数据利用率。未来工作可拓展至更多模型架构、细粒度电路分析，以及结合训练阶段动态数据筛选的方案。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities, but scaling their performance often relies on massive reasoning
datasets that are computationally expensive to train on. Existing data
selection methods aim to curate smaller, high-quality subsets but often rely on
costly external models or opaque heuristics. In this work, we shift the focus
from external heuristics to the model's internal mechanisms. We find that
complex reasoning tasks consistently activate a sparse, specialized subset of
attention heads, forming core reasoning circuits. Building on this insight, we
propose CircuitSeer, a novel data selection method that quantifies the
reasoning complexity of data by measuring its influence on these crucial
circuits. Extensive experiments on 4 models and 9 datasets demonstrate
CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of
data selected by our method achieves a 1.4-point gain in average Pass@1 over
training on the full dataset, highlighting its efficiency and effectiveness.

</details>


### [158] [AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification](https://arxiv.org/abs/2510.18488)
*Ho Fai Leung,Xiaoyan Xi,Fei Zuo*

Main category: cs.AI

TL;DR: A refined AndroidControl-Curated benchmark shows that on-device GUI agents are closer to viability than previously thought; Magma-R1-3B (3B params) trained on 2.4k curated samples matches larger models, challenging prior assessments.


<details>
  <summary>Details</summary>
Motivation: The original AndroidControl benchmarks suffer from ambiguities and factual errors that systematically underrate agent capabilities, leading to pessimistic conclusions about on-device GUI agents.

Method: Identify and purify shortcomings in AndroidControl to create AndroidControl-Curated; train a small model Magma-R1-3B (3B params) on 2.4k curated samples using 60 GPU-hours on an H20 GPU; evaluate on the curated benchmark.

Result: On AndroidControl-Curated, state-of-the-art models achieve ~75% success on complex tasks (a 15% improvement). Magma-R1-3B delivers comparable performance to Qwen3-VL-235B with ~200x fewer parameters.

Conclusion: A curated benchmark more accurately reflects real capabilities and can accelerate the development of robust, on-device GUI assistants; the proposed small model demonstrates efficient latency-accurate performance enabling practical deployment.

Abstract: On-device virtual assistants like Siri and Google Assistant are increasingly
pivotal, yet their capabilities are hamstrung by a reliance on rigid,
developer-dependent APIs. GUI agents offer a powerful, API-independent
alternative, but their adoption is hindered by the perception of poor
performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at
around 60% on benchmarks like AndroidControl, far from viability for real-world
use. Our research reveals that issue lies not only with the models but with the
benchmarks themselves. We identified notable shortcomings in AndroidControl,
including ambiguities and factual errors, which systematically underrates agent
capabilities. To address this critical oversight, we enhanced AndroidControl
into AndroidControl-Curated, a refined version of the benchmark improved
through a rigorous purification pipeline. On this enhanced benchmark,
state-of-the-art models achieve success rates nearing 75% on complex tasks (15%
improvement), reflecting that on-device GUI agents are actually closer to
practical deployment than previously thought. We introduce our new SOTA model,
Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an
H20 GPU (approximately $60). Despite being 200 times smaller in parameters,
this model delivers performance comparable to Qwen3- VL-235B. We release both
AndroidControl-Curated benchmark and Magma-R1 model to the research community,
encouraging adoption of this enhanced benchmark to better reflect model
capabilities and accelerate the development of robust, on-device virtual
assistants.

</details>


### [159] [Crucible: Quantifying the Potential of Control Algorithms through LLM Agents](https://arxiv.org/abs/2510.18491)
*Lianchen Jia,Chaoyang Li,Qian Houde,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.AI

TL;DR: 提出Crucible：一个基于LLM驱动的多层专家仿真框架，用于量化控制算法的调参潜力并给出正式度量；在多种任务和真实部署中验证，代码开源。


<details>
  <summary>Details</summary>
Motivation: 在生产环境中，控制算法往往需要领域专家对参数和逻辑进行精调，但现有研究多聚焦于理想配置下的性能，忽略了可调空间的重要性。需要一种系统化的方法来量化和比较不同算法的调优潜力。

Method: Crucible通过LLM驱动的多层级专家仿真来构建算法的可调域，并提出正式的调参潜力度量；在经典控制任务、复杂系统任务及真实部署中进行广泛案例验证；给出量化结果并在开源代码中实现。

Result: 研究表明Crucible能够系统地量化不同算法的可调空间，并揭示其在不同场景下的潜在改进方向；提供了算法分析和设计的新维度，推动性能提升；代码已在GitHub公开。

Conclusion: Crucible为控制与系统优化领域提供了一种量化调参潜力的新工具，帮助设计者在早期阶段评估和比较算法的可调空间，促进更高效的性能优化。

Abstract: Control algorithms in production environments typically require domain
experts to tune their parameters and logic for specific scenarios. However,
existing research predominantly focuses on algorithmic performance under ideal
or default configurations, overlooking the critical aspect of Tuning Potential.
To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven,
multi-level expert simulation to turn algorithms and defines a formalized
metric to quantitatively evaluate their Tuning Potential. We demonstrate
Crucible's effectiveness across a wide spectrum of case studies, from classic
control tasks to complex computer systems, and validate its findings in a
real-world deployment. Our experimental results reveal that Crucible
systematically quantifies the tunable space across different algorithms.
Furthermore, Crucible provides a new dimension for algorithm analysis and
design, which ultimately leads to performance improvements. Our code is
available at https://github.com/thu-media/Crucible.

</details>


### [160] [Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models](https://arxiv.org/abs/2510.18526)
*Hanze Guo,Jing Yao,Xiao Zhou,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: COUPLE 提出一种用于多元价值对齐的对抗性推理框架，通过结构化因果模型刻画价值之间的依赖与优先级，并利用反事实推理实现对指定价值目标的输出控制，同时提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 在跨文化、跨群体的应用场景中，LLMs 需要对多维且相互关联的价值进行对齐。现有方法往往将不同价值视为独立同等重要，忽略它们的相互依赖和相对优先级，且难以对低受限价值进行精细控制。

Method: 提出 COUPLE 框架：建立结构化因果模型（SCM）以表达特征之间的复杂依赖与优先级，以及高层价值维度与行为之间的因果关系；通过对抗性/反事实推理生成符合目标价值的输出；提高模型的可解释性。

Result: 在两个包含不同价值体系的数据集上评估，COUPLE 相较于基线在多种价值目标上均表现出改进，验证了对复杂价值结构的有效对齐与控制能力。

Conclusion: 引入显式因果建模与反事实推理的 COUPLE 提供对 pluralistic value alignment 的可控性与可解释性，适用于需要对多元目标进行细粒度对齐的场景。

Abstract: As large language models (LLMs) become increasingly integrated into
applications serving users across diverse cultures, communities and
demographics, it is critical to align LLMs with pluralistic human values beyond
average principles (e.g., HHH). In psychological and social value theories such
as Schwartz's Value Theory, pluralistic values are represented by multiple
value dimensions paired with various priorities. However, existing methods
encounter two challenges when aligning with such fine-grained value objectives:
1) they often treat multiple values as independent and equally important,
ignoring their interdependence and relative priorities (value complexity); 2)
they struggle to precisely control nuanced value priorities, especially those
underrepresented ones (value steerability). To handle these challenges, we
propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE
alignment. It introduces a structural causal model (SCM) to feature complex
interdependency and prioritization among features, as well as the causal
relationship between high-level value dimensions and behaviors. Moreover, it
applies counterfactual reasoning to generate outputs aligned with any desired
value objectives. Benefitting from explicit causal modeling, COUPLE also
provides better interpretability. We evaluate COUPLE on two datasets with
different value systems and demonstrate that COUPLE advances other baselines
across diverse types of value objectives.

</details>


### [161] [Extracting alignment data in open models](https://arxiv.org/abs/2510.18554)
*Federico Barbero,Xiangming Gu,Christopher A. Choquette-Choo,Chawin Sitawarin,Matthew Jagielski,Itay Yona,Petar Veličković,Ilia Shumailov,Jamie Hayes*

Main category: cs.AI

TL;DR: 通过嵌入向量的相似性可从后训练模型中提取大量对齐训练数据，超出简单的编辑距离评估。提取的数据包括用于SFT/RL的材料，能用于重新训练基础模型并恢复性能，显示对数据的潜在泄露风险及蒸馏的间接数据效应。


<details>
  <summary>Details</summary>
Motivation: 评估后训练模型中的数据泄露与记忆现象的风险，量化可从模型中提取的训练数据量及其对安全性、可控性和隐私的影响，探讨蒸馏对原始数据集的间接训练效应。

Method: 使用高质量嵌入模型进行近似字符串匹配，比较嵌入-based 方法与编辑距离在识别语义相似性方面的差异；量化可提取的数据量；分析模型在SFT或RL阶段回放的训练数据是否会被用于再训练基模型以恢复性能。

Result: 嵌入向量识别的语义相似性比编辑距离更能揭示可提取的训练数据；相当数量的数据可从后训练模型中提取并用于再训练基础模型，且模型会原样回放在SFT/RL中使用过的训练数据；蒸馏可能等价于对原始数据集的间接训练。

Conclusion: 揭示对齐数据的潜在隐私与安全风险，以及蒸馏过程对原始数据的间接利用。需要开发防护策略、加强对数据来源的可追踪性，并在模型开发中考虑数据泄露的应对措施与合规性研究。

Abstract: In this work, we show that it is possible to extract significant amounts of
alignment training data from a post-trained model -- useful to steer the model
to improve certain capabilities such as long-context reasoning, safety,
instruction following, and maths. While the majority of related work on
memorisation has focused on measuring success of training data extraction
through string matching, we argue that embedding models are better suited for
our specific goals. Distances measured through a high quality embedding model
can identify semantic similarities between strings that a different metric such
as edit distance will struggle to capture. In fact, in our investigation,
approximate string matching would have severely undercounted (by a conservative
estimate of $10\times$) the amount of data that can be extracted due to trivial
artifacts that deflate the metric. Interestingly, we find that models readily
regurgitate training data that was used in post-training phases such as SFT or
RL. We show that this data can be then used to train a base model, recovering a
meaningful amount of the original performance. We believe our work exposes a
possibly overlooked risk towards extracting alignment data. Finally, our work
opens up an interesting discussion on the downstream effects of distillation
practices: since models seem to be regurgitating aspects of their training set,
distillation can therefore be thought of as indirectly training on the model's
original dataset.

</details>


### [162] [VAR: Visual Attention Reasoning via Structured Search and Backtracking](https://arxiv.org/abs/2510.18619)
*Wei Cai,Jian Zhao,Yuchen Yuan,Tianle Zhang,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: VAR 提出一种视觉注意推理框架，通过证据 grounding 与搜索式链式推理相结合，加入回溯纠错与自验证奖励，显著降低幻觉并提升复杂任务表现；在 7B 模型 VAR-7B 上达到多项基准的新 SOTA，且与主流专有系统并驾齐驱。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型的幻觉高倾向和对线性推理的脆弱依赖，提升可解释性、 groundedness 和 安全性。

Method: 两阶段：1) traceable evidence grounding：对视觉输入进行证据级 grounding；2) search-based CoT 生成，包含回溯机制进行自我纠错；搜索过程受多维奖励函数引导，包含语义自验证和几何自证，惩罚与视觉输入不一致的输出；并给出理论分析证明在一定条件下高概率找到正确解。

Result: 实验结果显示 VAR-7B 在幻觉和安全基准上达到新 SOTA，显著优于现有开源模型，并与领先专有系统具竞争力。

Conclusion: 通过将证据 grounding 与结构化搜索相结合，VAR 为多模态推理提供更稳健的路径，降低幻觉风险并提升可追溯性和安全性，具备广泛应用潜力。

Abstract: Multimodal Large Language Models (MLLMs), despite their advances, are
hindered by their high hallucination tendency and heavy reliance on brittle,
linear reasoning processes, leading to failures in complex tasks. To address
these limitations, we introduce Visual Attention Reasoning (VAR), a novel
framework that recasts grounded reasoning as a structured search over a
reasoning trajectory space. VAR decomposes the reasoning process into two key
stages: traceable evidence grounding and search-based chain-of-thought (CoT)
generation, which incorporates a backtracking mechanism for self-correction.
The search is guided by a multi-faceted reward function with semantic and
geometric self-verification components, which penalize outputs that are not
faithfully grounded in the visual input. We provide a theoretical analysis for
our search strategy, validating its capability to find the correct solution
with high probability. Experimental results show that our 7B model, VAR-7B,
sets a new state-of-the-art on a comprehensive suite of hallucination and
safety benchmarks, significantly outperforming existing open-source models and
demonstrating competitive performance against leading proprietary systems.

</details>


### [163] [Leveraging Association Rules for Better Predictions and Better Explanations](https://arxiv.org/abs/2510.18628)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.AI

TL;DR: 把数据挖掘得到的关联规则用于提升决策树和随机森林的分类性能与可解释性，同时通过引入规则生成更泛化的 abductive 解释，实验表明在预测和解释方面均有收益。


<details>
  <summary>Details</summary>
Motivation: 将数据驱动的规则与现有树模型相结合，弥补纯数据驱动模型在解释性方面的不足；通过规则来提升预测准确性，并产生更一般化的因果性/解释。

Method: 从数据中挖掘可能包含否定条件的关联规则；把这些规则用于改进基于树的分类模型（决策树、随机森林）的预测性能；同时利用规则生成更一般化的 abductive 解释。对两种树模型进行实验评估。

Result: 实验结果显示，在所考察的两种树模型上，该方法在预测性能和解释大小/复杂度方面均具备收益。

Conclusion: 该方法证实了将数据与知识相结合的框架可以提升树模型的预测能力与解释性，适用于分类任务，具备向其他模型和规则类型扩展的潜力。

Abstract: We present a new approach to classification that combines data and knowledge.
In this approach, data mining is used to derive association rules (possibly
with negations) from data. Those rules are leveraged to increase the predictive
performance of tree-based models (decision trees and random forests) used for a
classification task. They are also used to improve the corresponding
explanation task through the generation of abductive explanations that are more
general than those derivable without taking such rules into account.
Experiments show that for the two tree-based models under consideration,
benefits can be offered by the approach in terms of predictive performance and
in terms of explanation sizes.

</details>


### [164] [Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises](https://arxiv.org/abs/2510.18631)
*Carlo Proietti,Antonio Yuste-Ginel*

Main category: cs.AI

TL;DR: 提出将不确定性从抽象论证模型具体化为结构化元素，给出可表达性（expressivity）的定义并给出正负结果，比较抽象与结构化模型在含不确定性时的能力，并影响不完整框架、带依赖的扩展及ASPIC+等结构/规则化模型。


<details>
  <summary>Details</summary>
Motivation: 在形式论证中，现有工作多聚焦抽象模型的不确定性，缺乏对这些抽象模型的可行实例化研究。本文希望将不确定性嵌入论证的组件（规则、前提），从而把抽象模型与结构化模型联系起来，揭示两者在可表达性上的差异。

Method: 引入一个可表达性概念，适用于抽象与结构化形式；给出关于可表达性的正负结果，比较二者在不确定性下的能力，覆盖抽象框架及其带依赖的扩展，以及结构化侧的ASPIC+。

Result: 确立了抽象与结构化论证模型在可表达性上的差异与联系；在某些情形下结构化模型可更丰富地表达不确定性，而在其他情形下受限于抽象模型；这些结果影响到不完整抽象框架及其扩展的处理，以及对ASPIC+等结构化系统的设计与评估。

Conclusion: 通过建立可表达性框架，推动抽象与结构化论证模型在含不确定性时的一致性与对比分析，为未来在不完整性、依赖性扩展以及具体实现（如ASPIC+）方面的研究提供理论基础和比较基准。

Abstract: Modelling qualitative uncertainty in formal argumentation is essential both
for practical applications and theoretical understanding. Yet, most of the
existing works focus on \textit{abstract} models for arguing with uncertainty.
Following a recent trend in the literature, we tackle the open question of
studying plausible instantiations of these abstract models. To do so, we ground
the uncertainty of arguments in their components, structured within rules and
premises. Our main technical contributions are: i) the introduction of a notion
of expressivity that can handle abstract and structured formalisms, and ii) the
presentation of both negative and positive expressivity results, comparing the
expressivity of abstract and structured models of argumentation with
uncertainty. These results affect incomplete abstract argumentation frameworks,
and their extension with dependencies, on the abstract side, and ASPIC+, on the
structured side.

</details>


### [165] [Query Decomposition for RAG: Balancing Exploration-Exploitation](https://arxiv.org/abs/2510.18633)
*Roxana Petcu,Kenton Murray,Daniel Khashabi,Evangelos Kanoulas,Maarten de Rijke,Dawn Lawrie,Kevin Duh*

Main category: cs.AI

TL;DR: A retrieval-augmented generation (RAG) framework is treated as an exploitation-exploration problem across sub-queries; bandit methods dynamically select informative documents to improve long-form generation; rank-based relevance and human judgments yield significant gains.


<details>
  <summary>Details</summary>
Motivation: To balance the trade-off in RAG between retrieving enough relevant material and avoiding noise/cost, by modeling query decomposition and document retrieval as sequential decisions.

Method: Formulate query decomposition and document retrieval as an exploration-exploitation process. Retrieve documents one at a time to build a belief about a sub-query's utility, using bandit learning methods to decide when to continue exploiting or to explore alternative sub-queries. Estimate document relevance using rank information and human judgments.

Result: Bandit-based dynamic sub-query selection is effective; rank-informed relevance estimation yields 35% gain in document-level precision, 15% improvement in alpha-nDCG, and better downstream performance for long-form generation.

Conclusion: An exploration-exploitation framing improves RAG retrieval by efficiently identifying informative documents, with rank information and human judgments enhancing retrieval quality and downstream task performance.

Abstract: Retrieval-augmented generation (RAG) systems address complex user requests by
decomposing them into subqueries, retrieving potentially relevant documents for
each, and then aggregating them to generate an answer. Efficiently selecting
informative documents requires balancing a key trade-off: (i) retrieving
broadly enough to capture all the relevant material, and (ii) limiting
retrieval to avoid excessive noise and computational cost. We formulate query
decomposition and document retrieval in an exploitation-exploration setting,
where retrieving one document at a time builds a belief about the utility of a
given sub-query and informs the decision to continue exploiting or exploring an
alternative. We experiment with a variety of bandit learning methods and
demonstrate their effectiveness in dynamically selecting the most informative
sub-queries. Our main finding is that estimating document relevance using rank
information and human judgments yields a 35% gain in document-level precision,
15% increase in {\alpha}-nDCG, and better performance on the downstream task of
long-form generation.

</details>


### [166] [Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation](https://arxiv.org/abs/2510.18751)
*Patterson Hsieh,Jerry Yeh,Mao-Chi He,Wen-Han Hsieh,Elvis Hsieh*

Main category: cs.AI

TL;DR: ALGOS 是一种 HAB 监测系统，结合分割与推理能力，通过 VLM 对遥感图像进行语义理解并估计蓝藻爆发的严重程度。


<details>
  <summary>Details</summary>
Motivation: 应对蓝藻爆发日益频繁、传统人工采样覆盖范围有限且工作量大的问题，需可扩展的 AI 驱动监测，能够从图像推理并量化污染严重度。

Method: 提出 ALGOs（ALGae Observation and Segmentation）系统，整合 GeoSAM 辅助的人类评估以获得高质量分割掩码，并在 Cyanobacteria Aggregated Manual Labels(CAML)数据集上对视觉-语言模型进行严重程度预测微调。该系统实现分割与推理并行，支持对暴发水平进行等级评估。

Result: 实验结果表明 ALGOS 在分割与严重程度等级估计上均具有鲁棒性，展示了将遥感图像理解与严重度推断相结合以实现天空蓝藻监测的潜力。

Conclusion: ALGOS 为实现实际化、自动化的蓝藻监测系统迈出重要一步，证明了分割与推理的整合在 HAB 监测中的可行性与前景。

Abstract: Climate change is intensifying the occurrence of harmful algal bloom (HAB),
particularly cyanobacteria, which threaten aquatic ecosystems and human health
through oxygen depletion, toxin release, and disruption of marine biodiversity.
Traditional monitoring approaches, such as manual water sampling, remain
labor-intensive and limited in spatial and temporal coverage. Recent advances
in vision-language models (VLMs) for remote sensing have shown potential for
scalable AI-driven solutions, yet challenges remain in reasoning over imagery
and quantifying bloom severity. In this work, we introduce ALGae Observation
and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB
monitoring that combines remote sensing image understanding with severity
estimation. Our approach integrates GeoSAM-assisted human evaluation for
high-quality segmentation mask curation and fine-tunes vision language model on
severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)
from NASA. Experiments demonstrate that ALGOS achieves robust performance on
both segmentation and severity-level estimation, paving the way toward
practical and automated cyanobacterial monitoring systems.

</details>


### [167] [Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location](https://arxiv.org/abs/2510.18803)
*Shirin Tavakoli Kafiabad,Andrea Schiffauerova,Ashkan Ebadi*

Main category: cs.AI

TL;DR: 本研究比较了 LDA、STM、BERTopic 三种主题模型在 NSERC 资助提案上的表现，提出了用于 BERTopic 的协变量分析新算法 COFFEE，结果显示 BERTopic 提供更细粒度、连贯且新兴主题，且 COFFEE 的协变量分析揭示了各省研究分工及性别相关的主题模式，进而为资助机构制定更公平性和影响力的资助策略提供依据。


<details>
  <summary>Details</summary>
Motivation: 需要在考虑公平性与包容性的前提下，理解研究趋势及人口、地域力量对资助方向的影响，以优化国家科学投资。

Method: 基于 2005–2022 年 NSERC 的18年资助提案，对 LDA、STM、BERTopic 三种主题建模进行比较，并提出 COFFEE 算法以实现 BERTopic 的协变量效应估计（ BERTopic 原生不支持协变量分析）。

Result: BERTopic 在识别的主题上更细粒度、连贯且具有新兴性（如人工智能的快速扩张），三模型均能界定核心科学领域；COFFEE 使协变量分析成为可能，证实了省际研究专业化及跨学科的性别主题模式。

Conclusion: 该研究为资助机构提供更扎实的证据基础，以制定更公平、更具影响力的资助策略，提升科学生态系统的有效性。

Abstract: Optimizing national scientific investment requires a clear understanding of
evolving research trends and the demographic and geographical forces shaping
them, particularly in light of commitments to equity, diversity, and inclusion.
This study addresses this need by analyzing 18 years (2005-2022) of research
proposals funded by the Natural Sciences and Engineering Research Council of
Canada (NSERC). We conducted a comprehensive comparative evaluation of three
topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic
Modelling (STM), and BERTopic. We also introduced a novel algorithm, named
COFFEE, designed to enable robust covariate effect estimation for BERTopic.
This advancement addresses a significant gap, as BERTopic lacks a native
function for covariate analysis, unlike the probabilistic STM. Our findings
highlight that while all models effectively delineate core scientific domains,
BERTopic outperformed by consistently identifying more granular, coherent, and
emergent themes, such as the rapid expansion of artificial intelligence.
Additionally, the covariate analysis, powered by COFFEE, confirmed distinct
provincial research specializations and revealed consistent gender-based
thematic patterns across various scientific disciplines. These insights offer a
robust empirical foundation for funding organizations to formulate more
equitable and impactful funding strategies, thereby enhancing the effectiveness
of the scientific ecosystem.

</details>
