<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.LG](#cs.LG) [Total: 140]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.AI](#cs.AI) [Total: 36]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement](https://arxiv.org/abs/2601.22169)
*Anudeex Shetty,Aditya Joshi,Salil S. Kanhere*

Main category: cs.CL

TL;DR: 将“醉酒语言”作为驱动因素，研究三种诱导LLM进入醉酒状态的机制（基于角色提示、因果微调、强化学习后训练），在5个模型上对JailbreakBench与ConfAIde等基准测试显示出更高的越狱和隐私泄露易感性，提示此类诱导具有明显的安全风险及作为对话安全评估/对抗基准的潜在作用。


<details>
  <summary>Details</summary>
Motivation: 探究人类醉酒行为对AI语言模型安全的映射，评估“醉酒语言”是否会降低LLM的安全性，进而为安全防护提供强对比样本与评估标准。

Method: 对三种诱导 drunk language 的机制进行系统研究：1) 基于人格/角色的提示(prompt)；2) 因果微调（causal fine-tuning）；3) 基于强化学习的后训练（reinforcement-based post-training）。在5种LLM上进行评估，使用English的 JailbreakBench 与 ConfAIde 基准进行越狱与隐私泄露测试；通过人工评估与LLM评估者相结合的评估框架以及错误类别分析，对比基线模型及以往方法的差异。

Result: 在5种LLM上，醉酒语言诱导的模型对 JailbreakBench 的越狱更易且在 ConfAIde 的隐私泄露更易，且相比基线模型及以往方法有显著提高的易感性。通过多轮人工与自动评估、以及对错误类别的分析，发现人类醉态行为与诱导出的人类拟人化行为在LLM中的对应关系。

Conclusion: 醉酒语言诱导方法简便高效，构成对LLM 安全的显著风险；可作为安全对比与对抗调优的潜在工具，但也揭示了对安全防护的重大挑战，需要在安全对策中充分考虑此类诱导风险。

Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.

</details>


### [2] [MrRoPE: Mixed-radix Rotary Position Embedding](https://arxiv.org/abs/2601.22181)
*Qingyuan Tian,Wenhong Zhu,Xiaoran Liu,Xiaofeng Wang,Rui Wang*

Main category: cs.CL

TL;DR: 提出 Mixed-radix RoPE（MrRoPE），统一 RoPE 扩展的理论框架，给出两种训练无关扩展 MrRoPE-Uni 和 MrRoPE-Pro，实现“train short, test long”的泛化，并在极长上下文任务中显示显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有 RoPE 扩展策略缺乏统一理论基础，对超长上下文的处理缺乏系统性分析与可比性。

Method: 基于 radix 系统转换的通用编码公式，将不同 RoPE 扩展视为不同的基数转换策略；提出两种训练无关实现：Uniform radix 转换（MrRoPE-Uni）和 Progressive radix 转换（MrRoPE-Pro），以提升对长上下文的泛化能力。

Result: MrRoPE-Pro 在无需微调的条件下，在 128K 上下文的 Needle-in-a-Haystack 测试中 recall 保持 >85%；在 YaRN 的 Infinite-Bench 检索与对话子集上达到超过两倍的准确率；理论分析表明提升 RoPE 能达到的编码长度上限。

Conclusion: 提供一个统一且可扩展的 RoPE 扩展框架，理论与实验共同支持混合基数策略提升长序列编码能力和鲁棒性的可行性，为 RoPE 的扩展性提供理论基础与实证支持。

Abstract: Rotary Position Embedding (RoPE)-extension refers to modifying or generalizing the Rotary Position Embedding scheme to handle longer sequences than those encountered during pre-training. However, current extension strategies are highly diverse and lack a unified theoretical foundation. In this paper, we propose MrRoPE (Mixed-radix RoPE), a generalized encoding formulation based on a radix system conversion perspective, which elegantly unifies various RoPE-extension approaches as distinct radix conversion strategies. Based on this theory, we introduce two training-free extensions, MrRoPE-Uni and MrRoPE-Pro, which leverage uniform and progressive radix conversion strategies, respectively, to achieve 'train short, test long' generalization. Without fine-tuning, MrRoPE-Pro sustains over 85% recall in the 128K-context Needle-in-a-Haystack test and achieves more than double YaRN's accuracy on Infinite-Bench retrieval and dialogue subsets. Theoretical analysis confirms that MrRoPE-Pro effectively raises the upper bound of RoPE's attainable encoding length, which further validates the reliability and utility of our theory and methodology.

</details>


### [3] [MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment](https://arxiv.org/abs/2601.22361)
*Yupeng Cao,Chengyang He,Yangyang Yu,Ping Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: MERMAID is a memory-enhanced multi-agent veracity framework that tightly integrates retrieval, reasoning, and evidence memory to improve online veracity assessment, achieving state-of-the-art results and higher efficiency.


<details>
  <summary>Details</summary>
Motivation: Current veracity pipelines treat evidence retrieval as a static, isolated step and do not reuse retrieved evidence across claims, leading to inefficiencies and inconsistencies. A memory-enabled, tightly-coupled retrieval-reasoning framework can improve accuracy and efficiency.

Method: MERMAID deploys agent-driven search, structured knowledge representations, and a persistent evidence memory within a Reason-Action style iterative loop to dynamically acquire evidence and reuse it across claims, reducing redundant searches and improving consistency.

Result: Evaluated on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs (GPT, LLaMA, Qwen). MERMAID achieves state-of-the-art veracity assessment performance while improving search efficiency via cross-claim evidence reuse.

Conclusion: Integrating retrieval, reasoning, and memory in a memory-augmented, multi-agent framework yields accurate and efficient veracity assessment and enables effective cross-claim evidence reuse.

Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.

</details>


### [4] [Context Structure Reshapes the Representational Geometry of Language Models](https://arxiv.org/abs/2601.22364)
*Eghbal A. Hosseini,Yuxuan Li,Yasaman Bahri,Declan Campbell,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: 在持续预测任务中，提升上下文长度会增加表示的直线化（straightening），并与预测性能提升相关；在结构化预测任务中，直线化不一致，仅在存在显式结构的阶段出现（如模板重复），其他阶段则消失。结论是：ICL不是单一过程，LLMs会根据任务结构动态选择策略。


<details>
  <summary>Details</summary>
Motivation: 将LLMs中表示直线化的现象与上下文学习（in-context learning, ICL）结合，检验是否在上下文内部也会出现表示直线化，以及不同任务结构下的表现差异。

Method: 在Gemma 2模型上，对多样化的ICL任务集进行表征直线化测量，比较持续预测任务与结构化预测任务中上下文增加对神经序列轨迹直线化的影响，分析与预测性能的关系。

Result: 持续预测任务中，增加上下文使神经序列轨迹更直线化，且与模型预测性能提升相关；结构化预测任务中，直线化不稳定，仅在存在显式结构（如模板重复）阶段出现，其他阶段则消失。

Conclusion: ICL并非统一过程；LLMs会根据任务结构动态选择策略，代表直线化只是其中一种可能的内部机制，表现为“像瑞士军刀”式的多策略适应。

Abstract: Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.

</details>


### [5] [Stability-Aware Prompt Optimization for Clinical Data Abstraction](https://arxiv.org/abs/2601.22373)
*Arinbjörn Kolbeinsson,Daniel Timbie,Sajjan Narsinghani,Sanjay Hariharan*

Main category: cs.CL

TL;DR: 本文研究临床场景中大语言模型对提示词的敏感性，提出将正确性与提示稳定性作为共同目标进行优化。通过两个临床任务（MedAlign 适用性/正确性与MS亚型抽取）在多种开源与专有模型上测量提示敏感性（flip rate），并分析校准与可选择性预测的关系。结果显示更高的准确率并不必然带来提示稳定性；某些模型对同义改写仍然易变，甚至看似校准良好。作者提出一个双目标的提示优化循环，加入稳定性项以降低翻转率，在不同任务和模型上取得改进，且有时以适度牺牲准确性换取更高的稳定性。结论认为提示敏感性应作为临床 LLM 系统验证的显式目标。


<details>
  <summary>Details</summary>
Motivation: 在临床应用中，提示词的微小改动可能导致输出显著变化，影响安全性与可靠性。现有研究多将提示词视为固定输入，忽略了提示稳定性与准确性之间的耦合，亟需一个同时优化两者的框架以提升临床 LLM 的鲁棒性。

Method: 通过两项临床任务（MedAlign 的适用性/正确性评估与 MS 亚型抽取）以及多种开源与专有模型，使用“翻转率（flip rate）”衡量提示敏感性，并将其与模型的校准和可选择性预测相关联。提出一个双目标提示优化循环，在优化准确性的同时加入稳定性项，评估对翻转率的下降及对任务性能的影响。

Result: 关键发现包括：1) 较高的准确率并不等同于提示稳定性强；2) 模型在校准良好时也可能对同义改写脆弱；3) 引入稳定性项的双目标优化可降低跨任务/模型的翻转率，尽管在某些场景需要一定的准确性代价。

Conclusion: 提示敏感性应成为临床 LLM 系统验证的显式目标，未来工作应进一步完善稳定性定义、评估方法与不同任务之间的权衡。

Abstract: Large language models used for clinical abstraction are sensitive to prompt wording, yet most work treats prompts as fixed and studies uncertainty in isolation. We argue these should be treated jointly. Across two clinical tasks (MedAlign applicability/correctness and MS subtype abstraction) and multiple open and proprietary models, we measure prompt sensitivity via flip rates and relate it to calibration and selective prediction. We find that higher accuracy does not guarantee prompt stability, and that models can appear well-calibrated yet remain fragile to paraphrases. We propose a dual-objective prompt optimization loop that jointly targets accuracy and stability, showing that explicitly including a stability term reduces flip rates across tasks and models, sometimes at modest accuracy cost. Our results suggest prompt sensitivity should be an explicit objective when validating clinical LLM systems.

</details>


### [6] [SPLA: Block Sparse Plus Linear Attention for Long Context Modeling](https://arxiv.org/abs/2601.22379)
*Bailin Wang,Dan Friedman,Tao Lei,Chong Wang*

Main category: cs.CL

TL;DR: 提出 Sparse Plus Linear Attention (SPLA) 框架，通过二阶泰勒展开的选择度量识别相关块进行精确注意力，并将未选块通过残差线性注意力(RLA)压缩成紧凑的循环状态，避免IO开销。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文建模中的低选择保真度和累积上下文损失问题，改进现有块级稀疏注意力。

Method: 基于二阶泰勒展开的选择性度量来筛选相关块，保留并对未选块执行RLA，将全局和已选线性注意力的差值用于计算残差，避免显式访问未选块；优化的减法式RLA计算。

Result: 实验表明 SPLA 在持续预训练中弥补与密集注意力的差距，在长上下文基准如 RULER 上超过Dense，同时在通用知识和推理能力方面保持竞争力。

Conclusion: 通过将未选块压缩成循环状态并用高效的减法式RLA实现无IO访问的长上下文处理，SPLA 提供了高效且保持精度的长上下文注意力方案。

Abstract: Block-wise sparse attention offers significant efficiency gains for long-context modeling, yet existing methods often suffer from low selection fidelity and cumulative contextual loss by completely discarding unselected blocks. To address these limitations, we introduce Sparse Plus Linear Attention (SPLA), a framework that utilizes a selection metric derived from second-order Taylor expansions to accurately identify relevant blocks for exact attention. Instead of discarding the remaining "long tail," SPLA compresses unselected blocks into a compact recurrent state via a residual linear attention (RLA) module. Crucially, to avoid IO overhead, we derive an optimized subtraction-based formulation for RLA -- calculating the residual as the difference between global and selected linear attention -- ensuring that unselected blocks are never explicitly accessed during inference. Our experiments demonstrate that SPLA closes the performance gap in continual pretraining, surpassing dense attention models on long-context benchmarks like RULER while maintaining competitive general knowledge and reasoning capabilities.

</details>


### [7] [SP^2DPO: An LLM-assisted Semantic Per-Pair DPO Generalization](https://arxiv.org/abs/2601.22385)
*Chaoyue He,Xin Zhou,Di Wang,Hong Xu,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: 提出 SP2DPO，通过对每对偏好样本设定离线事先确定的 beta_i，将 DPO 的全局温度替换为实例特定温度，取得在 AlpacaEval 2.0 上对四个 4B-8B 回带的竞争性表现，且无需训练时间开销。


<details>
  <summary>Details</summary>
Motivation: 现实偏好语料异质性与标注噪声使全局温度的单一权衡难以兼顾高信号与低信号样本；通过实例级、离线已知的温度调度提升鲁棒性与可追溯性。

Method: 在教师语言模型构成的结构化语义鸿沟注释（类别、量级、置信度）基础上，为 UltraFeedback（59,960 对偏好）中的每对样本预设 beta_i；内循环保持标准 DPO 结构，仅对每对样本应用相应的 beta_i；实现零训练时间开销并得到可审计的 beta_i 工件。

Result: 在 AlpacaEval 2.0 的评测中，针对四个开源权重回带（4B-8B），SP2DPO 与调参全局 beta 的 DPO 基线相当，并在两条长度控制的胜率指标上优于基线中的两条，且无需对每个模型进行 beta 搜索。

Conclusion: SP2DPO 提供一种可审计、无额外训练成本的实例级偏好优化方案，提升对样本异质性和噪声的鲁棒性，具备良好可复现性，代码、注释与工件将公开。

Abstract: Direct Preference Optimization (DPO) controls the trade-off between fitting preference labels and staying close to a reference model using a single global temperature beta, implicitly treating all preference pairs as equally informative. Real-world preference corpora are heterogeneous: they mix high-signal, objective failures (for example, safety, factuality, instruction violations) with low-signal or subjective distinctions (for example, style), and also include label noise. We introduce our method, SP2DPO (Semantic Per-Pair DPO), a generalization that replaces the global temperature with an instance-specific schedule beta_i pre-decided offline from structured semantic-gap annotations (category, magnitude, confidence) produced by teacher language models. We instantiate this procedure on the UltraFeedback preference corpus (59,960 pairs), enabling large-scale construction of an auditable beta_i artifact, and incur zero training-time overhead: the inner-loop optimizer remains standard DPO with beta set per pair. We focus our empirical study on AlpacaEval 2.0, reporting both raw win rate and length-controlled win rate. Across four open-weight, instruction-tuned student backbones (4B-8B), SP2DPO is competitive with a tuned global-beta DPO baseline and improves AlpacaEval 2.0 length-controlled win rate on two of four backbones, while avoiding per-model beta sweeps. All code, annotations, and artifacts will be released.

</details>


### [8] [Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks](https://arxiv.org/abs/2601.22396)
*Candida M. Greco,Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 用可解释的世界价值观调查(WVS)变量生成的LLM人口设定，探讨其在Inglehart-Welzel文化地图上的定位、与WVS的人群分布一致性以及基于道德基础理论的道德轮廓，以评估合成人格在跨文化结构与道德变异上的有效性。


<details>
  <summary>Details</summary>
Motivation: 评估合成、具文化基础的人格是否反映真实世界的文化及道德观念，并检验跨文化条件下的有效性与偏差，回应对LLM模拟人格的可比性与可迁移性的质疑。

Method: 以WVS派生的可解释变量为基础，生成LLM的人格；通过三条互补视角分析：1) 置于Inglehart-Welzel地图上的定位；2) 与WVS人群分布在统计学上的一致性；3) 道德基础问卷得到的道德特征，并结合文化-道德映射分析不同文化配置下的道德反应。

Result: 通过三个视角揭示：1) 生成的人格在Inglehart-Welzel地图上呈现稳定的跨文化条件差异；2) 人口层面的回答分布在很大程度上与人群分布模式相吻合；3) 根据道德基础测评得到的道德轮廓显示出文化配置间的道德反应差异。

Conclusion: 面向跨文化结构与道德变异的评估，显示了以文化为基础的合成人格生成与分析的可行性与潜力。

Abstract: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.

</details>


### [9] [Bifocal Attention: Harmonizing Geometric and Spectral Positional Embeddings for Algorithmic Generalization](https://arxiv.org/abs/2601.22402)
*Kanishk Awadhiya*

Main category: cs.CL

TL;DR: 提出“双焦点”位置编码：Geometric Eyes 保持 RoPE 的局部精度，Spectral Eyes 引入可学习的谐波算子来追踪递归深度，并通过 Spectral Evolution 将频率从静态几何参数逐步优化为针对任务拓扑的谐波基，从而缩小 RoPE 固定衰减带来的结构鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有 RoPE 使用固定几何衰减 θ^{-i}，优化目标偏向局部句法一致性，难以捕捉递归逻辑和算法推理中的长距离、周期性结构，导致模型在浅层推理到深层递归的外推能力不足（Structure Gap）.

Method: 提出 Bifocal Attention，将位置编码分为两类：Geometric Eyes（标准 RoPE，精确的 token 级操作）与 Spectral Eyes（可学习的谐波算子，跟踪长程递归深度）。并提出 Spectral Evolution 训练协议：初始频率为固定几何参数，通过梯度下降演化为针对任务拓扑的谐波基。

Result: 摘要未给出具体实验结果，仅提出框架和训练协议，主旨在于构建可同时处理局部与全局位置信息的注意力结构并提升对算法性拓扑的适应性。

Conclusion: 该方法旨在缓解结构鸿沟，增强对长期递归结构的建模能力，理论上可提升对复杂算法性任务的推理与泛化。

Abstract: Rotary Positional Embeddings (RoPE) have become the standard for Large Language Models (LLMs) due to their ability to encode relative positions through geometric rotation. However, we identify a significant limitation we term ''Spectral Rigidity'': standard RoPE utilizes a fixed geometric decay ($θ^{-i}$) optimized for local syntactic coherence, which fails to capture the long-range, periodic structures inherent in recursive logic and algorithmic reasoning. This results in a ''Structure Gap'', where models trained on shallow reasoning chains fail to extrapolate to deeper recursive steps. In this work, we introduce Bifocal Attention, an architectural paradigm that decouples positional encoding into two distinct modalities: Geometric Eyes (Standard RoPE) for precise token-level manipulation, and Spectral Eyes (Learnable Harmonic Operators) for tracking long-range recursive depth. We propose a novel training protocol, Spectral Evolution, which initializes positional frequencies as static geometric parameters but allows them to evolve via gradient descent into a harmonic basis optimized for the specific algorithmic topology of the task.

</details>


### [10] [Word-Centered Semantic Graphs for Interpretable Diachronic Sense Tracking](https://arxiv.org/abs/2601.22410)
*Imene Kolli,Kai-Robin Lange,Jonas Rieger,Carsten Jentsch*

Main category: cs.CL

TL;DR: A word-centered semantic graph framework for analyzing semantic shift in diachronic corpora, combining diachronic Skip-gram similarity and time-specific masked-LM substitutability to form interpretable, per-target word semantic networks; clusters track sense changes across time without predefined sense inventories.


<details>
  <summary>Details</summary>
Motivation: Provide an interpretable, inventory-free representation of sense evolution in corpora and capture polysemy dynamics across time; integrate distributional and substitution-based cues.

Method: For each target word and time slice, induce a word-centered semantic graph; compute distributional similarity from diachronic Skip-gram embeddings; incorporate lexical substitutability via time-specific masked language models; identify sense-related structure by clustering peripheral graph; align clusters across time via node overlap; monitor change via cluster composition and normalized cluster mass.

Result: Applied to NYT Magazine corpus (1980-2017): graph connectivity reflects polysemy dynamics; identified contrasting trajectories: event-driven sense replacement for 'trump', semantic stability with cluster over-segmentation for 'god', and gradual association shifts related to digital communication for 'post'.

Conclusion: Word-centered semantic graphs provide a compact, transparent representation for exploring sense evolution without predefined sense inventories.

Abstract: We propose an interpretable, graph-based framework for analyzing semantic shift in diachronic corpora. For each target word and time slice, we induce a word-centered semantic network that integrates distributional similarity from diachronic Skip-gram embeddings with lexical substitutability from time-specific masked language models. We identify sense-related structure by clustering the peripheral graph, align clusters across time via node overlap, and track change through cluster composition and normalized cluster mass. In an application study on a corpus of New York Times Magazine articles (1980 - 2017), we show that graph connectivity reflects polysemy dynamics and that the induced communities capture contrasting trajectories: event-driven sense replacement (trump), semantic stability with cluster over-segmentation effects (god), and gradual association shifts tied to digital communication (post). Overall, word-centered semantic graphs offer a compact and transparent representation for exploring sense evolution without relying on predefined sense inventories.

</details>


### [11] [Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource Language Tokens by Cross-Entropy Loss](https://arxiv.org/abs/2601.22439)
*Galim Turumtaev*

Main category: cs.CL

TL;DR: 提出一种阈值化的负采样策略，减轻低资源语言中稀有标记的边缘化，从而提升字符级语言模型对稀有 tokens 的表示与验证集表现。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言中，稀有标记在训练数据中极为稀少，容易被边缘化，导致学习效果差；需要一种方法来缓解这一现象，使稀有标记获得更有意义的对齐与表示。

Method: 在训练中引入阈值化技术以降低边缘化的负面影响，并结合负采样来限制对稀有标记的过度边缘化的有害影响；以字符级语言模型为实验对象，评估对低资源语言验证数据的提升。

Result: 实验表明该方法在低资源语言的验证数据上显著提升性能；首次展示了负采样如何通过限制过度边缘化来改善稀有标记的表示，从而提升语言模型在欠代表语言上的表现。

Conclusion: 通过阈值化结合负采样，成功缓解稀有标记的边缘化问题，提供了一种提升低资源语言语言模型表现的新思路。

Abstract: Neural language models often struggle with low-resource languages due to the limited availability of training data, making tokens from these languages rare in the training set. This paper addresses a specific challenge during training: rare tokens are disproportionately affected by marginalization, which prevents them from learning effectively. We propose a thresholding technique that reduces the impact of this marginalization, allowing rare tokens to benefit from more meaningful alignment. Through experiments with a character-level language model, we demonstrate that this method significantly improves performance on low-resource language validation data. This work is the first to show how negative sampling can be applied to improve the representation of rare tokens by limiting the harmful influence of excessive marginalization, offering a new approach to enhancing language model performance for underrepresented languages.

</details>


### [12] [SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization](https://arxiv.org/abs/2601.22491)
*Jinyang Wu,Changpeng Yang,Yuhao Shen,Fangzhi Xu,Bolin Ni,Chonghua Liao,Yuchen Liu,Hongzhen Wang,Shuai Nie,Shuai Zhang,Haoran Luo,Jiaming Xu*

Main category: cs.CL

TL;DR: SSL introduces tiered, progressively amplified rewards to guide RL toward the solution's 'sweet spot', improving gradient signal and preserving optimal ordering; yields strong gains across multiple tasks and cross-task transferability.


<details>
  <summary>Details</summary>
Motivation: Binary/terminal rewards mask quality differences among trajectories that reach the same outcome, hindering efficient learning and exploration of diverse solution spaces.

Method: Sweet Spot Learning (SSL) applies distance-tiered reward shaping for visual perception tasks and incremental-progress rewards for reasoning tasks; analyzes gradient signal-to-noise benefits and preserves optimal solution ordering.

Result: Empirical results across GUI perception, short/long-term planning, and complex reasoning show consistent improvements over strong baselines on 12 benchmarks, with up to 2.5× sample efficiency and good cross-task transferability.

Conclusion: SSL is a general, domain-agnostic principle for guiding reinforcement learning toward high-quality, robust policies by differentiating rewards within the same outcome class.

Abstract: Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce \textbf{S}weet \textbf{S}pot \textbf{L}earning (\textbf{SSL}), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.

</details>


### [13] [One Ring to Rule Them All: Unifying Group-Based RL via Dynamic Power-Mean Geometry](https://arxiv.org/abs/2601.22521)
*Weisong Zhao,Tong Wang,Zichang Tan,Te Yang,Siran Peng,Haoyuan Zhang,Tianshuo Zhang,Haichao Shi,Meng Meng,Yang Yang,Xiangyu Zhu,Zhen Lei,Xiao-Yu Zhang,Xu Zhou*

Main category: cs.CL

TL;DR: PMPO 将 GRPO 与 GMPO 融合为一个以幂均值为几何的通用框架，通过 Clip-aware ESS 自适应地选取幂指数 p，从而在高/低形态的轨迹之间动态权衡更新，实验在数学推理基准上优于强基线。


<details>
  <summary>Details</summary>
Motivation: 解决固定聚合几何在轨迹异质性与演化性面前的局限，GRPO 与 GMPO 均未能自适应地反映每条轨迹的贡献度随时间与难度的变化。

Method: 在 Power-Mean Policy Optimization 框架内，通过幂均值对聚合几何进行参数化，GRPO 与 GMPO 分别对应 p=1 与近似 p→0 的几何平均。引入 Clip-aware Effective Sample Size（ESS）机制，通过一个确定性规则将轨迹剪裁分数映射到目标 ESS，再解出使轨迹诱导的 ESS 与目标一致的 p，从而实现对 p 的自适应调整，动态在算术均值与几何均值之间切换。

Result: 在多种数学推理基准上，PMPO 显著优于强基线，显示出在不同轨迹难度与稳定性条件下的自适应聚合能力。

Conclusion: PMPO 提供一个动态自适应的聚合几何框架，通过可控的 p 调整实现对更新的集中度的灵活控制，提升复杂任务中的学习稳定性与性能。

Abstract: Group-based reinforcement learning has evolved from the arithmetic mean of GRPO to the geometric mean of GMPO. While GMPO improves stability by constraining a conservative objective, it shares a fundamental limitation with GRPO: reliance on a fixed aggregation geometry that ignores the evolving and heterogeneous nature of each trajectory. In this work, we unify these approaches under Power-Mean Policy Optimization (PMPO), a generalized framework that parameterizes the aggregation geometry via the power-mean geometry exponent p. Within this framework, GRPO and GMPO are recovered as special cases. Theoretically, we demonstrate that adjusting p modulates the concentration of gradient updates, effectively reweighting tokens based on their advantage contribution. To determine p adaptively, we introduce a Clip-aware Effective Sample Size (ESS) mechanism. Specifically, we propose a deterministic rule that maps a trajectory clipping fraction to a target ESS. Then, we solve for the specific p to align the trajectory induced ESS with this target one. This allows PMPO to dynamically transition between the aggressive arithmetic mean for reliable trajectories and the conservative geometric mean for unstable ones. Experiments on multiple mathematical reasoning benchmarks demonstrate that PMPO outperforms strong baselines.

</details>


### [14] [$ρ$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs](https://arxiv.org/abs/2601.22527)
*Jingyi Yang,Yuxian Jiang,Jing Shao*

Main category: cs.CL

TL;DR: 通过密度- EOS 信号在去噪过程中的自适应 bidirectional 可变长度生成，解决 masked diffusion LLM 固定长度限制，提升推理效率与 token 利用率，且实现训练无关的单阶段长度调整。


<details>
  <summary>Details</summary>
Motivation: 现有的 masked diffusion LLM 依赖预设的固定生成长度，导致输出质量与计算成本之间存在不可忽视的权衡；引入 EOS 的隐式密度作为信号，可动态指引何时收缩或扩展生成长度。

Method: 提出 ρ-EOS，训练无关的单阶段策略，在去噪过程持续估计隐式 EOS 密度，通过密度阈值判断并对 MASK 进行收缩/扩展，从而实现双向长度自适应。与需两阶段（长度调整 + 掩码插入）的先行方法相比，ρ-EOS 在统一去噪过程中完成长度自适应。

Result: 在数学和代码基准数据集上实验表明，ρ-EOS 达到与现有方法相当的性能，同时显著提升推理效率和 token 使用率。

Conclusion: 提供一种无需额外训练的自适应长度调整机制，使掩码 dLLMs 具备更高的灵活性与资源利用率。

Abstract: Beyond parallel generation and global context modeling, current masked diffusion large language models (dLLMs) suffer from a fundamental limitation: they require a predefined, fixed generation length, which lacks flexibility and forces an inevitable trade-off between output quality and computational efficiency. To address this, we study the denoising dynamics and find that the implicit density ($ρ$) of end-of-sequence ($\texttt{EOS}$) tokens serves as a reliable signal of generation sufficiency. In particular, the evolving implicit $\texttt{EOS}$ density during denoising reveals whether the current masked space is excessive or insufficient, thereby guiding the adjustment direction for generation length. Building on this insight, we propose $\textbf{$ρ$-$\texttt{EOS}$}$, a training-free, single-stage strategy that enables bidirectional variable-length generation for masked dLLMs. Unlike prior two-stage approaches--which require separate length adjustment and iterative mask insertion phases while supporting only unidirectional expansion--$\textbf{$ρ$-$\texttt{EOS}$}$ achieves bidirectional length adjustment within a unified denoising process by continuously estimating the implicit $\texttt{EOS}$ density: excessively high density triggers $\texttt{MASK}$ token contraction, while insufficient density induces expansion. Extensive experiments on mathematics and code benchmarks demonstrate that $\textbf{$ρ$-$\texttt{EOS}$}$ achieves comparable performance while substantially improving inference efficiency and token utilization.

</details>


### [15] [Towards the Holographic Characteristic of LLMs for Efficient Short-text Generation](https://arxiv.org/abs/2601.22546)
*Shun Qian,Bingquan Liu,Chengjie Sun,Zhen Xu,Baoxun Wang*

Main category: cs.CL

TL;DR: 提出“全息特征”并基于此开发HOLO插件，在有限生成步内提取目标关键词并辅以并行词汇约束，从而提高推理效率，在短文本生成任务中与基线相当。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在生成过程中的特征，以提升推理效率和输出控制，聚焦于生成初期捕获目标关键词的现象及其潜在应用。

Method: 通过实证观察定义“全息特征”，并提出HOLO插件，利用该特征在有限步内提取目标关键词，结合并行词汇约束文本生成。实验在多架构/规模的模型上进行短文本生成评估。

Result: HOLO在不同模型架构与规模上实现与基线相当的自动化与类人评估性能，显示该特征的有效性。

Conclusion: 验证了全息特征的潜力及HOLO的可行性，提示在推理效率与输出控制方面的应用前景。

Abstract: The recent advancements in Large Language Models (LLMs) have attracted interest in exploring their in-context learning abilities and chain-of-thought capabilities. However, there are few studies investigating the specific traits related to the powerful generation capacity of LLMs. This paper aims to delve into the generation characteristics exhibited by LLMs. Through our investigation, we have discovered that language models tend to capture target-side keywords at the beginning of the generation process. We name this phenomenon the Holographic Characteristic of language models. For the purpose of exploring this characteristic and further improving the inference efficiency of language models, we propose a plugin called HOLO, which leverages the Holographic Characteristic to extract target-side keywords from language models within a limited number of generation steps and complements the sentence with a parallel lexically constrained text generation method. To verify the effectiveness of HOLO, we conduct massive experiments on language models of varying architectures and scales in the short-text generation scenario. The results demonstrate that HOLO achieves comparable performance to the baselines in terms of both automatic and human-like evaluation metrics and highlight the potential of the Holographic Characteristic.

</details>


### [16] [Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations](https://arxiv.org/abs/2601.22548)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Mackenzie Puig-Hall,Narmeen Oozeer*

Main category: cs.CL

TL;DR: 本文揭示了评估中自我偏好偏差的核心方法学混淆，并提出 Evaluator Quality Baseline 来分离自我偏好信号与对照噪声，实证分析表明多数早期发现不再显著，且对易难判别的评估投票熵性有所揭示。


<details>
  <summary>Details</summary>
Motivation: LLM在充当评审时倾向偏向自身输出，威胁自动化后训练与评估流程的完整性；需要区分自恋效应与一般实验混淆，以提升自我偏好测量的鲁棒性与可重复性。

Method: 识别核心混淆：当评估者回答错误的输出来自其自身答案时，易产生自我偏好 verdict，且该现象与是否存在自身回答无关。提出 Evaluator Quality Baseline：比较评估者对自身错误答案投票错的概率与对来自另一模型的错误答案投票错的概率。基于37,448个查询进行评估。

Result: 基线可将测量误差减少约89.6%；仅有约51%的初始显著性结果在控制混淆后仍显著；并对“简单”与“困难”评估投票的熵性进行了表征。

Conclusion: 该基线有助于去除自我偏好中的噪声，使后续研究能更可靠地刻画评委偏差；为评估工作提供实用工具，扩展了对评判者偏差的分类与隔离工作。

Abstract: Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental confounds, distorting measurements of self-preference bias. We discover a core methodological confound which could reduce measurement error by 89.6%. Specifically, LLM evaluators may deliver self-preferring verdicts when the judge responds to queries which they completed incorrectly themselves; this would be true regardless of whether one of their responses is their own. To decouple self-preference signals from noisy outputs on hard problems, we introduce an Evaluator Quality Baseline, which compares the probability that a judge incorrectly votes for itself against the probability that it votes for an incorrect response from another model. Evaluating this simple baseline on 37,448 queries, only 51% of initial findings retain statistical significance. Finally, we turn towards characterizing the entropy of "easy" versus "hard" evaluation votes from LLM judges. Our corrective baseline enables future research on self-preference by eliminating noisy data from potential solutions. More widely, this work contributes to the growing body of work on cataloging and isolating judge-bias effects.

</details>


### [17] [SpanNorm: Reconciling Training Stability and Performance in Deep Transformers](https://arxiv.org/abs/2601.22580)
*Chao Wang,Bei Li,Jiaqi Zhang,Xinyu Liu,Yuchun Fan,Linkun Lyu,Xin Chen,Jingang Wang,Tong Xiao,Peng Pei,Xunliang Cai*

Main category: cs.CL

TL;DR: SpanNorm: a normalization technique that merges PreNorm stability with PostNorm performance by using a residual that spans the full transformer block and a PostNorm-style normalization, achieving stable training and superior accuracy in both dense and MoE Transformers.


<details>
  <summary>Details</summary>
Motivation: There is a fundamental trade-off between PreNorm (stable training but potential deep-model performance loss) and PostNorm (strong performance but training instability). SpanNorm aims to resolve this dilemma.

Method: Introduce SpanNorm: a clean residual spanning the entire transformer block to stabilize signal propagation, combined with PostNorm-style computation that normalizes the aggregated output. Provide theoretical analysis showing bounded signal variance with a scaling strategy to prevent gradient issues and reduce representation collapse. Empirically evaluate in dense and MoE settings.

Result: SpanNorm consistently outperforms standard normalization schemes in both dense and MoE scenarios, improving stability and performance over existing normalization approaches.

Conclusion: SpanNorm unifies PreNorm and PostNorm strengths, enabling stable, high-performing Transformer architectures with theoretical guarantees and broad applicability to dense and MoE models.

Abstract: The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures.

</details>


### [18] [Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models](https://arxiv.org/abs/2601.22629)
*Jingxuan Wu,Zhenglin Wan,Xingrui Yu,Yuzhe Yang,Yiqiao Huang,Ivor Tsang,Yang You*

Main category: cs.CL

TL;DR: Diffusion-LMs exhibit a temporal division of labor: early steps set global semantics, later steps refine local tokens. 提出 Time-Annealed Perturbation Sampling (TAPS)，一个训练无关的推理策略，在扩散过程前期鼓励语义分支，后期逐步减少扰动以保持流畅性与指令遵循。TAPS 兼容非自回归与半自回归的 Diffusion backbones（如 LLaDA、TraDo），在创作与推理任务上提升输出多样性，同时不降低质量。


<details>
  <summary>Details</summary>
Motivation: 解决 Diffusion-LMs 在探索多种有效语义或推理路径时缺乏有效多样性控制的问题；通过对扩散过程的时间维度进行分析发现早期决定全局语义、后续负责局部细化，从而提出训练无关的扰动策略以促进早期分支。

Method: 提出 Time-Annealed Perturbation Sampling (TAPS)，在推理阶段对扩散过程的早期步骤施加扰动以鼓励语义分支，并以时间自适应的方式逐步降低扰动强度以维持流畅性与指令遵循。TAPS 适用于非自回归与半自回归的 Diffusion backbones，可与 LLaDA、TraDo 等模型配合使用。

Result: 在创意写作与推理任务的基准测试中，TAPS 显著提升输出多样性，且未显著降低生成质量。

Conclusion: TAPS 为 Diffusion-LMs 提供了一种训练无关的、可控的多样性提升方法，利用扩散过程的时间分工实现早期的语义分支和后期的语言精炼的平衡，且具备与现有回归骨干的良好兼容性。

Abstract: Diffusion language models (Diffusion-LMs) introduce an explicit temporal dimension into text generation, yet how this structure can be leveraged to control generation diversity for exploring multiple valid semantic or reasoning paths remains underexplored. In this paper, we show that Diffusion-LMs, like diffusion models in image generation, exhibit a temporal division of labor: early denoising steps largely determine the global semantic structure, while later steps focus on local lexical refinement. Building on this insight, we propose Time-Annealed Perturbation Sampling (TAPS), a training-free inference strategy that encourages semantic branching early in the diffusion process while progressively reducing perturbations to preserve fluency and instruction adherence. TAPS is compatible with both non-autoregressive and semi-autoregressive Diffusion backbones, demonstrated on LLaDA and TraDo in our paper, and consistently improves output diversity across creative writing and reasoning benchmarks without compromising generation quality.

</details>


### [19] [DART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning](https://arxiv.org/abs/2601.22632)
*Abhishek Tyagi,Yunuo Cen,Shrey Dhorajiya,Bharadwaj Veeravalli,Xuanyao Fong*

Main category: cs.CL

TL;DR: DART is a training-free, on-the-fly, context-aware pruning method for LLMs that dynamically updates FFN neuron masks during autoregressive generation by tracking shifts in attention distributions, reducing parameter redundancy with minimal overhead. It outperforms static/dynamic baselines across benchmarks, achieving up to 14.5% accuracy gains at 70% FFN sparsity on LLAMA-3.1-8B, and up to 3x ROUGE-L improvements on summarization, with sub-10MB memory usage and ~0.1% FLOPs overhead.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods rely on dataset-calibration and are largely static, failing to adapt to evolving contextual knowledge as generation proceeds. This leads to data dependency, higher compute, and degraded adaptability to changing semantic contexts.

Method: DART monitors shifts in attention score distributions to infer contextual changes during autoregressive generation and dynamically updates neuron-level masks to retain salient FFN parameters, enabling context-adaptive pruning without training.

Result: Across ten benchmarks, DART surpasses prior dynamic baselines, delivering accuracy gains up to 14.5% on LLAMA-3.1-8B at 70% FFN sparsity and up to 3x improvements in ROUGE-L on summarization, with performance close to dense models and memory under 10MB for LLAMA-3.1-8B (16GB) and 0.1% FLOPs overhead.

Conclusion: DART effectively adapts to diverse semantic contexts, preserving model capabilities across general and domain-specific tasks with minimal memory and computational overhead; code is available at the provided GitHub link.

Abstract: Large Language Models (LLMs) exhibit substantial parameter redundancy, particularly in Feed-Forward Networks (FFNs). Existing pruning methods suffer from two primary limitations. First, reliance on dataset-specific calibration introduces significant data dependency and computational overhead. Second, being predominantly static, they fail to account for the evolving subset of knowledge neurons in LLMs during autoregressive generation as the context evolves. To address this, we introduce DART, i.e., Dynamic Attention-Guided Runtime Tracing), a lightweight, training-free method that performs on-the-fly context-based pruning. DART monitors shifts in attention score distributions to infer context changes, dynamically updating neuron-level masks to retain salient parameters. Across ten benchmarks, DART outperforms prior dynamic baseline, achieving accuracy gains of up to 14.5% on LLAMA-3.1-8B at 70% FFN sparsity. Furthermore, DART achieves up to 3x better ROUGE-L scores with respect to static-masked pruning on summarization tasks, with its performance comparable to the original dense models. We conclusively demonstrate that the proposed framework effectively adapts to diverse semantic contexts, preserves model capabilities across both general and domain-specific tasks while running at less than 10MBs of memory for LLAMA-3.1-8B(16GBs) with 0.1% FLOPs overhead. The code is available at https://github.com/seeder-research/DART.

</details>


### [20] [NAG: A Unified Native Architecture for Encoder-free Text-Graph Modeling in Language Models](https://arxiv.org/abs/2601.22657)
*Haisong Gong,Zhibo Liu,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.CL

TL;DR: 提出 NAG，统一在语言模型内部处理文本-图结构的图学习框架，替代外部 GNN-LM 分离架构，提供 NAG-Zero 与 NAG-LoRA 两种实现，并在多图任务上证明有效性。


<details>
  <summary>Details</summary>
Motivation: 现有将图结构用外部 GNN 编码，再与文本语义对接的分离式架构存在本质对齐成本，难以在文本-图之间高效耦合。

Method: 在语言模型的自注意力中内嵌图结构处理，通过重用注意力以强制拓扑依赖并调整位置信息，使模型同时理解节点、边内容和拓扑；提出 NAG-Zero 和 NAG-LoRA。

Result: 在多样的图任务上验证了 NAG 的鲁棒图理解能力且不需要外部编码器，具有更简洁的整体架构与潜在的效率优势。

Conclusion: 统一的原生图处理框架能更好地利用语言模型的表征能力，简化文本-图建模；NAG-Zero 保护原模型的语言能力，NAG-LoRA 提升结构适配性。

Abstract: Prevailing methods for integrating graphs into Language Models (LMs) typically rely on a segregated architecture: external Graph Neural Networks (GNNs) encode structural topology, while LMs process textual semantics. We argue this approach is suboptimal for text-graphs: it creates a conceptually disjointed interaction paradigm. By segregating structural encoding from semantic processing, these systems must perform a complex implicit alignment between abstract graph tokens and concrete textual elements. Challenging the necessity of external encoders, we propose NAG (Native Architecture for Graphs), a unified framework that internalizes graph processing within the LM's native manifold. Instead of bridging disparate embedding spaces, NAG repurposes the self-attention mechanism to enforce topological dependencies and recalibrates positional IDs to ensure structural equivalence. This allows the model to harness its intrinsic linguistic capability to simultaneously comprehend node and edge content alongside structural topology. We introduce two efficient implementations: NAG-Zero for absolute preservation of the base model's linguistic capabilities, and NAG-LoRA for enhanced structural adaptation. Experiments across diverse graph tasks validate that NAG achieves robust graph comprehension without the overhead of external encoders, offering a simpler, more coherent paradigm for text-graph modeling.

</details>


### [21] [TSLM: Tree-Structured Language Modeling for Divergent Thinking](https://arxiv.org/abs/2601.22688)
*Doyoung Kim,Jaehyeok Doo,Minjoon Seo*

Main category: cs.CL

TL;DR: 引入带分支结构的树状语言模型（TSLM），通过特殊编码符号在单次生成中并行扩展多条搜索路径，并在完整搜索树上训练以提升系统性探索与推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决自回归推理中难以同时探索多条路径的问题，避免重复前缀计算和对外部搜索的多次前向传播，通过在完整树结构上监督学习来实现鲁棒的系统性探索。

Method: 引入用来编码分支结构的特殊符号，将多条备选路径以树状形式并入单次生成过程；在包含成功和失败的完整搜索树上进行监督学习，使模型内部化系统性探索；推理阶段通过单一前向传播实现对多条路径的选择性扩展，避免外部搜索的多次独立前向。

Result: 理论上实现了更鲁棒的推理与更高的推断效率，显著减少对外部搜索的重复前向计算，并证明在完整树结构监督下模型能学习到系统性探索能力。

Conclusion: 在树结构轨迹的有监督学习能够提供一个高效的推理时刻扩展范式，为语言模型的系统性探索能力提供了一种有效替代方案，并有望提升鲁棒推理的可扩展性。

Abstract: Language models generate reasoning sequentially, preventing them from decoupling irrelevant exploration paths during search. We introduce Tree-Structured Language Modeling (TSLM), which uses special tokens to encode branching structure, enabling models to generate and selectively expand multiple search paths within a single generation process. By training on complete search trees including both successful and failed attempts, TSLM learns to internalize systematic exploration without redundant recomputation of shared prefixes. TSLM achieves robust performance and superior inference efficiency by avoiding the multiple independent forward passes required by external search methods. These results suggest a new paradigm of inference-time scaling for robust reasoning, demonstrating that supervised learning on complete tree-structured traces provides an efficient alternative for developing systematic exploration capabilities in language models.

</details>


### [22] [Models Know Models Best: Evaluation via Model-Preferred Formats](https://arxiv.org/abs/2601.22699)
*Joonhak Lee,Sungmok Jung,Jongyeon Park,Jaejin Lee*

Main category: cs.CL

TL;DR: LLMs的评测格式对比存在显著差异：符号化多选与填空式评测导致不同表现，但均为模型无关效应；通过一个轻量分类器基于潜在偏好信号实现动态格式对齐，可在零-shot下显著提升正确率。


<details>
  <summary>Details</summary>
Motivation: 系统性地探究不同评测格式对LLMs性能的影响及其背后的模型偏好，旨在通过自动化、格式自适应的评测方案揭示模型潜在能力。

Method: 对比符号基（多选）与填空式（似然评分）评测在多种解码式LLMs上的表现，分析任务特征对格式的适配性；训练一个轻量级分类器，利用模型输出的潜在偏好信号预测每道题的最佳评测格式，并在推理中动态选择格式。

Result: 动态格式对齐在零-shot条件下对推理与知识基准的正确率带来显著且一致的提升，超越人工设计的启发式策略，且对多种模型具有鲁棒性。

Conclusion: 评测格式的对齐能揭示模型的潜在能力，降低评测偏差；基于模型信号的自适应格式选择具备普适性，能更公平地评估LLMs的真实能力。

Abstract: Performance of Large Language Models (LLMs) on multiple-choice tasks differs markedly between symbol-based and cloze-style evaluation formats. The observed discrepancies are systematically attributable to task characteristics: natural language continuation benefits from likelihood scoring, whereas explicit comparison is better suited to symbol-based selection. These trends are consistent across various decoder-based LLMs, indicating model-agnostic effects. To address these inconsistencies, a dynamic format-alignment strategy is introduced that employs a lightweight classifier trained on latent model-preference signals. In contrast to human-designed heuristics, which often degrade performance, this approach uses model-generated signals to determine the optimal format for each problem instance. The proposed method achieves substantial and consistent improvements in zero-shot accuracy across reasoning and knowledge benchmarks, better revealing the models' latent capabilities.

</details>


### [23] [MM-THEBench: Do Reasoning MLLMs Think Reasonably?](https://arxiv.org/abs/2601.22735)
*Zhidian Huang,Zijun Yao,Ji Qi,Shangqing Tu,Junxian Ma,Jinxin Liu,Weichuan Liu,Xiaoyin Che,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: MM-THEBench 为推理多模态大模型的中间推理（CoT）阶段的“幻觉”提供量化基准，采用基于认知维度的细粒度分类、带验证的推理注释数据和多层次自动评估框架；通过在主流推理MLLMs上的实验，揭示思考过程对幻觉与推理能力的影响。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要针对推理前/推理后的输出，未充分测量推理过程中的内部思维与幻觉，且自我反思推理虽提升鲁棒性但可能引入新的幻觉。需要一个能够对中间CoT的幻觉进行系统化评估的基准。

Method: 1) 构建以认知维度为基础的细粒度 taxonomy，用以标注推理过程中的不同认知阶段与潜在幻觉来源；2) 组建含有验证推理注释的数据集，覆盖多模态任务；3) 设计分层自动评估框架，能够对中间CoT、跨模态一致性和最终结论进行自动化判定；4) 在主流推理MLLMs上进行广泛实验，比较在不同任务中的思维-幻觉关系。

Result: 实验揭示：思考过程对幻觉和推理能力存在系统性影响；在某些任务中，推理思维提升正确性但同时增加幻觉风险；多模态任务中的幻觉源自感知误差、推理推断偏差及跨模态信息不一致；MM-THEBench 能够揭示这些效应并提供诊断工具与改进方向。

Conclusion: MM-THEBench 提供一个全面的框架，用以量化推理MLLM中间CoT的幻觉，支持对Thinking-augmented 模型的鲁棒性评估与改进，推动多模态推理系统的更可靠发展。

Abstract: Recent advances in multimodal large language models (MLLMs) mark a shift from non-thinking models to post-trained reasoning models capable of solving complex problems through thinking. However, whether such thinking mitigates hallucinations in multimodal perception and reasoning remains unclear. Self-reflective reasoning enhances robustness but introduces additional hallucinations, and subtle perceptual errors still result in incorrect or coincidentally correct answers. Existing benchmarks primarily focus on models before the emergence of reasoning MLLMs, neglecting the internal thinking process and failing to measure the hallucinations that occur during thinking. To address these challenges, we introduce MM-THEBench, a comprehensive benchmark for assessing hallucinations of intermediate CoTs in reasoning MLLMs. MM-THEBench features a fine-grained taxonomy grounded in cognitive dimensions, diverse data with verified reasoning annotations, and a multi-level automated evaluation framework. Extensive experiments on mainstream reasoning MLLMs reveal insights into how thinking affects hallucination and reasoning capability in various multimodal tasks.

</details>


### [24] [AR-BENCH: Benchmarking Legal Reasoning with Judgment Error Detection, Classification and Correction](https://arxiv.org/abs/2601.22742)
*Yifei Li,Richong Zhang,Wanyu Tu,Zhijie Nie,Haokun Luo,Chuantao Yin,Pengchong Li*

Main category: cs.CL

TL;DR: 提出APPELLATE REVIEW作为新任务并构建AR-BENCH数据集，对14个大语言模型进行评估，揭示现有模型在识别法律应用错误方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 因案件情形复杂和法律概念抽象导致判决可能出错，且上诉复审机制面临案量激增带来的效率压力；现有法律AI多聚焦预测/生成，缺乏面向判决纠错的异常检测框架。

Method: 提出评估诊断推理和可靠性的APPELLATE REVIEW任务，构建AR-BENCH数据集（8,700个细粒度标注的判决和34,617份辅助语料），在14种大型语言模型上进行系统评估，分析错误识别与纠错能力。

Result: 大多数模型在识别法律适用错误方面能力不足，存在显著性能差距；某些模型略有改进但总体未达到可用于实际司法纠错的水平。

Conclusion: AR-BENCH为后续改进提供基准，需在模型推理、法律知识整合、证据解释和可解释性方面加强，推动面向判决纠错的AI发展。

Abstract: Legal judgments may contain errors due to the complexity of case circumstances and the abstract nature of legal concepts, while existing appellate review mechanisms face efficiency pressures from a surge in case volumes. Although current legal AI research focuses on tasks like judgment prediction and legal document generation, the task of judgment review differs fundamentally in its objectives and paradigm: it centers on detecting, classifying, and correcting errors after a judgment is issued, constituting anomaly detection rather than prediction or generation. To address this research gap, we introduce a novel task APPELLATE REVIEW, aiming to assess models' diagnostic reasoning and reliability in legal practice. We also construct a novel dataset benchmark AR-BENCH, which comprises 8,700 finely annotated decisions and 34,617 supplementary corpora. By evaluating 14 large language models, we reveal critical limitations in existing models' ability to identify legal application errors, providing empirical evidence for future improvements.

</details>


### [25] [RASST: Fast Cross-modal Retrieval-Augmented Simultaneous Speech Translation](https://arxiv.org/abs/2601.22777)
*Jiaxuan Luo,Siqi Ouyang,Lei Li*

Main category: cs.CL

TL;DR: 提出了 Retrieval-Augmented Simultaneous Speech Translation (RASST)，在 SST 流式推断中引入跨模态检索。通过训练轻量级的语音-文本检索器并使用滑动窗口检索，在增量生成中为模型提供分块的术语提示，同时合成训练数据教会模型精准利用检索到的术语。实验证明在 ACL 60/60 dev 集的三个语言方向上，术语翻译准确率提升最高可达 16%，整体翻译质量提升最多 3BLEU 点，且消融实验验证各组件的贡献。


<details>
  <summary>Details</summary>
Motivation: 现有的 SST 模型受限于罕见和领域专有术语的翻译；尽管近来 Speech LLMs 提升显著，但术语鲁棒性仍不足。虽然检索增强在机器翻译中对术语翻译有效，但将检索引入 SST 面临跨模态快速检索、对 partial 输入的在线决策和增量生成时机选择等挑战。因此需要一个能在增量语音输入下高效、准确地跨模态检索并引导生成的方案。

Method: 提出 RASST，将跨模态检索紧耦合到 SST 流程中：训练一个轻量级的语音-文本检索器，并实现高效的滑动窗口检索；在每个片段（chunk）上为 Speech LLM 提供术语提示；合成训练数据，教会 Speech LLM 如何在增量生成中精准地使用检索到的术语。

Result: 在 ACL 60/60 dev 集的三个语言方向上，RASST 能提高术语翻译准确性多达 16%，整体翻译质量提升多达 3 BLEU 点；消融实验证实各组件的贡献。

Conclusion: RASST 成功将检索引入 SST 流程，显著提升术语翻译和整体翻译质量，且证实了分块术语提示和跨模态检索在增量生成中的有效性。

Abstract: Simultaneous speech translation (SST) produces target text incrementally from partial speech input. Recent speech large language models (Speech LLMs) have substantially improved SST quality, yet they still struggle to correctly translate rare and domain-specific terminology. While retrieval augmentation has been effective for terminology translation in machine translation, bringing retrieval to SST is non-trivial: it requires fast and accurate cross-modal (speech-to-text) retrieval under partial, continually arriving input, and the model must decide whether and when to apply retrieved terms during incremental generation. We propose Retrieval-Augmented Simultaneous Speech Translation (RASST), which tightly integrates cross-modal retrieval into the SST pipeline. RASST trains a lightweight speech-text retriever and performs efficient sliding-window retrieval, providing chunkwise terminology hints to the Speech LLM. We further synthesize training data that teaches the Speech LLM to leverage retrieved terms precisely. Experiments on three language directions of the ACL 60/60 dev set show that RASST improves terminology translation accuracy by up to 16% and increases overall translation quality by up to 3 BLEU points, with ablations confirming the contribution of each component.

</details>


### [26] [Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs](https://arxiv.org/abs/2601.22795)
*Corentin Kervadec,Iuliia Lysova,Marco Baroni,Gemma Boleda*

Main category: cs.CL

TL;DR: 提出一种基于机械解释的计算密度估计器，用于量化LLMs中的计算密度；实验证明处理通常Dense且随输入动态变化，同一输入在不同模型间呈显著相关；罕见token需要更高密度，扩展上下文往往降低密度；该估计器有助于理解LLMs的处理过程并挑战符号化解释。


<details>
  <summary>Details</summary>
Motivation: 现有的剪枝/效率优化研究暗示可以剪掉大量参数而影响有限，但对计算分布并非均匀的直觉缺乏系统性度量；需要量化在大规模语言模型中不同参数的实际贡献，理解计算密度的动态性及跨模型的一致性。

Method: 设计一个基于机械解释的密度估计器，结合模型内部机制特征来量化每个输入的计算密度；在多模型/多输入上进行实验，评估密度随输入变化、跨模型相关性，以及影响因素（罕见token、上下文长度等）。

Result: 1) 处理一般呈现密集计算;
2) 计算密度具有动态性，模型在稀疏与密集处理模式间切换，受输入影响;
3) 每输入的密度在不同LLMs之间显著相关，提示相同输入触发高或低密度的趋势；进一步观察到预测罕见tokens需要更高密度，扩展上下文长度通常降低密度。

Conclusion: 计算密度估计器有助于更好理解LLMs的内部处理过程，挑战将其简化为符号解释的观点。

Abstract: Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design a density estimator drawing on mechanistic interpretability. We experimentally test our estimator and find that: (1) contrary to what has been often assumed, LLM processing generally involves dense computation; (2) computation density is dynamic, in the sense that models shift between sparse and dense processing regimes depending on the input; (3) per-input density is significantly correlated across LLMs, suggesting that the same inputs trigger either low or high density. Investigating the factors influencing density, we observe that predicting rarer tokens requires higher density, and increasing context length often decreases the density. We believe that our computation density estimator will contribute to a better understanding of the processing at work in LLMs, challenging their symbolic interpretation.

</details>


### [27] [When Meanings Meet: Investigating the Emergence and Quality of Shared Concept Spaces during Multilingual Language Model Training](https://arxiv.org/abs/2601.22851)
*Felicia Körner,Max Müller-Eberstein,Anna Korhonen,Barbara Plank*

Main category: cs.CL

TL;DR: 通过激活补丁法探究多语言大模型EuroLLM在预训练阶段形成的语言无关概念空间，发现共享概念空间早期出现并持续细化，但对它们的对齐是语言依赖的；对翻译质量的表观提升多源于行为偏移（选择词义、拷贝同形异义词等），而非真正的翻译能力；为多语言情境下的因果可解释方法提供了训练动态与应用条件的新见解。


<details>
  <summary>Details</summary>
Motivation: 揭示跨语言概念空间在多语言大模型预训练中的形成机制，以及因果可解释性方法在多语言设置中的有效性边界。

Method: 使用因果可解释性方法中的激活补丁化（activation patching）来分离跨语言概念表示；将这些表示注入翻译提示，检验翻译是否可被跨语言概念改变而与语言无关；并进行细粒度人工错误分析与比较。

Result: 共享概念空间在早期就已出现并持续 refine，但与之的对齐具有语言依赖性；某些表观的翻译质量提升源于策略性行为改变（如对多义词的语义选择、在跨语言同形词上进行翻译而非拷贝），而非实际的翻译能力提升。

Conclusion: 为跨语言对齐的训练动力学提供新见解，并评估在多语言场景中因果可解释性方法何时能提供有意义的洞见。

Abstract: Training Large Language Models (LLMs) with high multilingual coverage is becoming increasingly important -- especially when monolingual resources are scarce. Recent studies have found that LLMs process multilingual inputs in shared concept spaces, thought to support generalization and cross-lingual transfer. However, these prior studies often do not use causal methods, lack deeper error analysis or focus on the final model only, leaving open how these spaces emerge during training. We investigate the development of language-agnostic concept spaces during pretraining of EuroLLM through the causal interpretability method of activation patching. We isolate cross-lingual concept representations, then inject them into a translation prompt to investigate how consistently translations can be altered, independently of the language. We find that shared concept spaces emerge early} and continue to refine, but that alignment with them is language-dependent}. Furthermore, in contrast to prior work, our fine-grained manual analysis reveals that some apparent gains in translation quality reflect shifts in behavior -- like selecting senses for polysemous words or translating instead of copying cross-lingual homographs -- rather than improved translation ability. Our findings offer new insight into the training dynamics of cross-lingual alignment and the conditions under which causal interpretability methods offer meaningful insights in multilingual contexts.

</details>


### [28] [From Labels to Facets: Building a Taxonomically Enriched Turkish Learner Corpus](https://arxiv.org/abs/2601.22875)
*Elif Sayar,Tolgahan Türker,Anna Golynskaia Knezhevich,Bihter Dereli,Ayşe Demirhas,Lionel Nicolas,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 提出基于分面 taxonomy 的半自动注释方法与扩展框架，用于 Turkish Learner Corpus 的富化与可查询化，具分面级准确度 95.86%。


<details>
  <summary>Details</summary>
Motivation: 扁平注释结构难以分离多维语言特征，限制深层分析与细粒度错误分析；需要标准化、可解释的多维注释以提升跨学习者比较与分析能力。

Method: 在新近提出的分面分类法基础上，设计注释扩展框架并实现适用于 Turkish 的扩展工具。该工具在现有扁平注释上自动推断额外语言和元数据，形成分面层级的富化注释。

Result: 实现了首个协同注释并分面富化的 Turkish Learner Corpus；注释扩展工具达到分面级准确度 95.86%；提升了查询能力与跨学习者分析的细粒度能力。

Conclusion: 首次将该 taxonomy 应用于实际语料库设计与实现，提供手册注释指南、精炼标签集和注释扩展工具，为后续现有错标注语料的分面富化工作树立范例。

Abstract: In terms of annotation structure, most learner corpora rely on holistic flat label inventories which, even when extensive, do not explicitly separate multiple linguistic dimensions. This makes linguistically deep annotation difficult and complicates fine-grained analyses aimed at understanding why and how learners produce specific errors. To address these limitations, this paper presents a semi-automated annotation methodology for learner corpora, built upon a recently proposed faceted taxonomy, and implemented through a novel annotation extension framework. The taxonomy provides a theoretically grounded, multi-dimensional categorization that captures the linguistic properties underlying each error instance, thereby enabling standardized, fine-grained, and interpretable enrichment beyond flat annotations. The annotation extension tool, implemented based on the proposed extension framework for Turkish, automatically extends existing flat annotations by inferring additional linguistic and metadata information as facets within the taxonomy to provide richer learner-specific context. It was systematically evaluated and yielded promising performance results, achieving a facet-level accuracy of 95.86%. The resulting taxonomically enriched corpus offers enhanced querying capabilities and supports detailed exploratory analyses across learner corpora, enabling researchers to investigate error patterns through complex linguistic and pedagogical dimensions. This work introduces the first collaboratively annotated and taxonomically enriched Turkish Learner Corpus, a manual annotation guideline with a refined tagset, and an annotation extender. As the first corpus designed in accordance with the recently introduced taxonomy, we expect our study to pave the way for subsequent enrichment efforts of existing error-annotated learner corpora.

</details>


### [29] [Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate Dialog[ue]s Beyond the American Default with MDial](https://arxiv.org/abs/2601.22888)
*Jio Oh,Paul Vicinanza,Thomas Butler,Steven Euijong Whang,Dezhi Hong,Amani Namboori*

Main category: cs.CL

TL;DR: MDial提出一个大规模多方言对话数据生成框架，覆盖九种英语方言，关注词汇、拼写和语法三大层面，并构建MDialBench基准及对多模型的方言识别与应答评估，揭示现有模型对非标准英语的偏差与局限。


<details>
  <summary>Details</summary>
Motivation: 解决缺乏高质量、可控的多方言对话数据，以及模型在非标准英语上的表现不足和前沿模型在方言识别/生成上的系统性偏差。

Method: 与母语语言学家合作，设计基于规则的LLM转换，生成带注释的多方言对话数据，确保精确性并控制方言特征的复现程度；构建MDial数据集与MDialBench基准，评估17种LLM在方言识别和响应生成上的表现。

Result: 独立评估表明数据质量高， annotators偏好MDial输出；MDialBench包含>50k对话、>97k问答对；前沿模型方言识别准确率<70%，加拿大英语<50%，非SAE方言常被错误分类为美式或英式，方言识别错误会影响下游任务。

Conclusion: 多方言对话研究在大模型时代仍面临挑战，需要更精准的方言对齐、数据生成控制和更健壮的方言识别/生成评估框架。

Abstract: More than 80% of the 1.6 billion English speakers do not use Standard American English (SAE) and experience higher failure rates and stereotyped responses when interacting with LLMs as a result. Yet multi-dialectal performance remains underexplored. We introduce $\textbf{MDial}$, the first large-scale framework for generating multi-dialectal conversational data encompassing the three pillars of written dialect -- lexical (vocabulary), orthographic (spelling), and morphosyntactic (grammar) features -- for nine English dialects. Partnering with native linguists, we design an annotated and scalable rule-based LLM transformation to ensure precision. Our approach challenges the assumption that models should mirror users' morphosyntactic features, showing that up to 90% of the grammatical features of a dialect should not be reproduced by models. Independent evaluations confirm data quality, with annotators preferring MDial outputs over prior methods in 98% of pairwise comparisons for dialect naturalness. Using this pipeline, we construct the dialect-parallel $\textbf{MDialBench}$mark with 50k+ dialogs, resulting in 97k+ QA pairs, and evaluate 17 LLMs on dialect identification and response generation tasks. Even frontier models achieve under 70% accuracy, fail to reach 50% for Canadian English, and systematically misclassify non-SAE dialects as American or British. As dialect identification underpins natural language understanding, these errors risk cascading failures into downstream tasks.

</details>


### [30] [DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion](https://arxiv.org/abs/2601.22889)
*Yuxuan Lou,Ziming Wu,Yaochen Wang,Yong Liu,Yingxuan Ren,Fuming Lai,Shaobing Lian,Jie Tang,Yang You*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken responses, with thinking traces informing speech quality. To realize this, we present \method{}, the first diffusion-based speech-text language model supporting both understanding and generation, unifying discrete text and tokenized speech under a single masked diffusion framework. Unlike autoregressive approaches, \method{} jointly generates reasoning traces and speech tokens through iterative denoising, with modality-specific masking schedules. We also construct \dataset{}, the first speech QA dataset with paired text reasoning traces, containing 26K samples totaling 319 hours. Experiments show \method{} achieves state-of-the-art speech-to-speech QA accuracy, outperforming the best baseline by up to 9 points, while attaining the best TTS quality among generative models (6.2\% WER) and preserving language understanding (66.2\% MMLU). Ablations confirm that both the diffusion architecture and thinking traces contribute to these gains.

</details>


### [31] [LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models](https://arxiv.org/abs/2601.22928)
*Alhassan Abdelhalim,Janick Edinger,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 本研究质疑两类常用的LLM可解释性方法（token级关系探查与嵌入属性推断）的有效性，发现两者在测试语言抽象时均受限，可能因方法假设错误或数据集偏差而给出误导性证据。


<details>
  <summary>Details</summary>
Motivation: 探究语言抽象在LLM中如何涌现，是否跨模块（注意力头、输入嵌入）具有可解释迹象，以及在分布式/普适计算场景中对可解释性方法的依赖是否可靠。

Method: 采用文献中成熟的两类方法：1) 针对标记级关系结构的探测（probing）；2) 使用嵌入作为可解释属性的载体进行特征映射。测试对象包括注意力头和输入嵌入。评估核心假设（后层表示仍对应令牌）以及嵌入的属性推断是否能揭示语义知识。

Result: 两种尝试均失败：注意力解释在核心假设被检验后崩塌；嵌入属性推断的高预测分数来自方法学伪影和数据集结构，而非有意义的语义知识。

Conclusion: 这些局限性意味着现有可解释性方法并不能可靠地揭示LLM的理解程度，尤其在普适/分布式场景中，依赖此类方法用于调试、压缩、解释可能导致误导。需要更严格的评估框架和对方法假设的重新审视，以及在实际部署中对可解释性结果的谨慎解读。

Abstract: Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.
  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.

</details>


### [32] [Relaxing Positional Alignment in Masked Diffusion Language Models](https://arxiv.org/abs/2601.22947)
*Mengyu Ye,Ryosuke Takahashi,Keito Kudo,Jun Suzuki*

Main category: cs.CL

TL;DR: 通过引入<slack>和CTC目标实现对MDLM的对齐灵活监督，显著提升开放式文本生成的鲁棒性与质量。


<details>
  <summary>Details</summary>
Motivation: MDLM在开放式文本生成上受限于严格的位置信号预测，导致对 token misalignment 敏感，且严格的位置信监督与不可逆的去噪解码之间存在错配。

Method: 在微调阶段引入<slack>标记，通过连接时序分类（CTC）目标实现对齐灵活性；对广泛使用的MDLM在五个基准上进行评估。

Result: 方法在所有基准上优于原始模型，特别提高对位置错位的鲁棒性，表明放宽严格的位置信监督有助于生成质量。

Conclusion: 放宽位置信监督的对齐灵活监督是提升MDLM开放式生成质量的关键因素。

Abstract: Masked diffusion language models (MDLMs) have emerged as a promising alternative to dominant autoregressive approaches. Although they achieve competitive performance on several tasks, a substantial gap remains in open-ended text generation. We hypothesize that one cause of this gap is that strict positional prediction makes MDLM decoding highly sensitive to token misalignment, and we show through controlled interventions that a one-position shift can severely disrupt semantics. This observation suggests that enforcing strict positional supervision during training is misaligned with the irreversible denoising dynamics of MDLM decoding. Motivated by this mismatch, we adopt an alignment-flexible supervision strategy during fine-tuning. Specifically, we introduce a special token <slack> via the connectionist temporal classification objective. We apply this approach to the widely used MDLM model and conduct experiments on five open-ended text generation benchmarks. Our method consistently outperforms the original model and improves robustness to positional shifts, indicating that relaxing strict positional supervision is an important factor in improving generation quality in MDLMs.

</details>


### [33] [Autonomous Chain-of-Thought Distillation for Graph-Based Fraud Detection](https://arxiv.org/abs/2601.22949)
*Yuan Li,Jun Hu,Bryan Hooi,Bingsheng He,Cheng Chen*

Main category: cs.CL

TL;DR: FraudCoT 提出一个面向文本属性图的自适应图知觉型推理框架，通过选择性CoT蒸馏与不对称联合训练实现高效的GNN协同推理。相比现有预设提示的LLM-GNN方法，提升了AUPRC并大幅加速训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的GNN方法受限于预设 prompts 和解耦训练，难以实现文本语义与图结构的高度对齐及自驱动推理。针对文本属性图中的欺诈检测，需同时建模丰富文本语义与关系依赖，提升推理自主性与效率。

Method: 提出 FraudCoT：1) 基于欺诈感知的选择性CoT蒸馏，生成多样化推理路径并增强语义-结构理解；2) 将蒸馏得到的CoT整合进节点文本，提供多跳语义与结构线索给GNN；3) 设计高效的不对称共训练策略，实现端到端优化，同时显著降低朴素联合训练的计算开销。

Result: 在公开与工业基准上，FraudCoT 相较于SOTA方法在AUPRC上提升可达8.8%，训练吞吐量实现高达1066×的加速。

Conclusion: FraudCoT 通过自监督/自适应的CoT推理与高效联合训练，提升TAGs上的欺诈检测性能与效率，展示了语义-结构对齐与推理自主性在LLM-GNN 框架中的有效性。

Abstract: Graph-based fraud detection on text-attributed graphs (TAGs) requires jointly modeling rich textual semantics and relational dependencies. However, existing LLM-enhanced GNN approaches are constrained by predefined prompting and decoupled training pipelines, limiting reasoning autonomy and weakening semantic-structural alignment. We propose FraudCoT, a unified framework that advances TAG-based fraud detection through autonomous, graph-aware chain-of-thought (CoT) reasoning and scalable LLM-GNN co-training. To address the limitations of predefined prompts, we introduce a fraud-aware selective CoT distillation mechanism that generates diverse reasoning paths and enhances semantic-structural understanding. These distilled CoTs are integrated into node texts, providing GNNs with enriched, multi-hop semantic and structural cues for fraud detection. Furthermore, we develop an efficient asymmetric co-training strategy that enables end-to-end optimization while significantly reducing the computational cost of naive joint training. Extensive experiments on public and industrial benchmarks demonstrate that FraudCoT achieves up to 8.8% AUPRC improvement over state-of-the-art methods and delivers up to 1,066x speedup in training throughput, substantially advancing both detection performance and efficiency.

</details>


### [34] [Residual Context Diffusion Language Models](https://arxiv.org/abs/2601.22954)
*Yuezhou Hu,Harman Singh,Monishwaran Maheswaran,Haocheng Xi,Coleman Hooper,Jintao Zhang,Aditya Tomar,Michael W. Mahoney,Sewon Min,Mehrdad Farajtabar,Kurt Keutzer,Amir Gholami,Chenfeng Xu*

Main category: cs.CL

TL;DR: 提出 Residual Context Diffusion (RCD) 以回收被 discarding 的 token 表征，将其转化为上下文残差并注入到下一次去噪中，从而提升 dLLM 的效率和准确性，且可在较低数据量下将现有模型转换为该范式。


<details>
  <summary>Details</summary>
Motivation: 现有块状 dLLM 的 remask 机制在解码阶段只利用最有信心的 token，丢弃其余 token，导致大量计算被浪费且上下文信息未被充分利用。需要一种能重用 discarded token 的方法以提升性能和样本效率。

Method: 引入 RCD 模块：将 discarded token 的表示转化为上下文残差，并注入到下一轮去噪中；采用解耦的两阶段训练以缓解反向传播的内存瓶颈；可将标准 dLLM 转换为 RCD，所需数据量约为 1e9 tokens。

Result: 在长推理场景（SDAR）与短指令跟随场景（LLaDA）上，RCD 对前沿 dLLMs 提升 5–10 点准确率，且额外计算开销极小。在最具挑战性的 AIME 任务上，准确率几乎翻倍，且达到同等准确度时，去噪步数减少 4–5 倍。

Conclusion: RCD 为块状扩散式语言模型提供有效的提升路径，显著提升准确性与计算利用率，且具备良好的移植性，适合对现有模型进行低成本改造。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a "remasking" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.

</details>


### [35] [A Unified View of Attention and Residual Sinks: Outlier-Driven Rescaling is Essential for Transformer Training](https://arxiv.org/abs/2601.22966)
*Zihan Qiu,Zeyu Huang,Kaiyue Wen,Peng Jin,Bo Zheng,Yuxin Zhou,Haofeng Huang,Zekun Wang,Xiao Li,Huaqing Zhang,Yang Xu,Haoran Lian,Siqi Zhang,Rui Men,Jianwei Zhang,Ivan Titov,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 研究 emergent outliers（注意力接收极大者与残差维度的极大激活）在大语言模型中的作用，提出 outlier-driven rescaling，强调其与 softmax 注意力和 RMSNorm 的协同作用，尝试统一 sink 的起源与缓解策略；并给出基于吸收、裁剪、门控再缩放等方法的训练与量化鲁棒性改善。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型中异常输出（outliers）的功能及其与归一化的关系，解释 sink（注意力接收极大者和残差维度极大激活）的产生与缓解机制，统一化处理思路。

Method: 在不同模型架构和不同训练 token 数量的条件下，进行系统性实验；分析注意力 logits 与激活值；比较去除归一化、直接裁剪、将 outliers 吸收到参数、以及引入显式门控再缩放对训练稳定性、性能和量化鲁棒性的影响。

Result: 发现 outliers 与归一化协同工作：移除归一化会使相应的 outliers 消失但降低训练稳定性；在保留归一化的前提下直接裁剪 outliers 会降低性能；outliers 更像尺度因子而非直接贡献者，最终的注意力和残差 sink 对非 outliers 的实际贡献仍显著；将 outliers 吸收到可学习参数或通过门控再缩放可提升训练性能（平均提升约 2 点），并在量化场景下提升鲁棒性；在 W4A4 量化下存在约 1.2 点的性能下降。

Conclusion: outlier-driven rescaling 提供一种统一 sink 的起源与缓解策略的视角；可通过吸收到参数或门控再缩放进行缓解，从而提升训练性能与对量化的鲁棒性。

Abstract: We investigate the functional role of emergent outliers in large language models, specifically attention sinks (a few tokens that consistently receive large attention logits) and residual sinks (a few fixed dimensions with persistently large activations across most tokens). We hypothesize that these outliers, in conjunction with the corresponding normalizations (\textit{e.g.}, softmax attention and RMSNorm), effectively rescale other non-outlier components. We term this phenomenon \textit{outlier-driven rescaling} and validate this hypothesis across different model architectures and training token counts. This view unifies the origin and mitigation of both sink types. Our main conclusions and observations include: (1) Outliers function jointly with normalization: removing normalization eliminates the corresponding outliers but degrades training stability and performance; directly clipping outliers while retaining normalization leads to degradation, indicating that outlier-driven rescaling contributes to training stability. (2) Outliers serve more as rescale factors rather than contributors, as the final contributions of attention and residual sinks are significantly smaller than those of non-outliers. (3) Outliers can be absorbed into learnable parameters or mitigated via explicit gated rescaling, leading to improved training performance (average gain of 2 points) and enhanced quantization robustness (1.2 points degradation under W4A4 quantization).

</details>


### [36] [ArabicDialectHub: A Cross-Dialectal Arabic Learning Resource and Platform](https://arxiv.org/abs/2601.22987)
*Salem Lahlou*

Main category: cs.CL

TL;DR: 提出 ArabicDialectHub：一个跨六方言的阿拉伯学习资源，含552条短语；由LLMs生成并经五名母语者验证，按难度与主题组织；提供翻译探索、带干扰项的自适应测验、云端进度同步等功能；数据集与平台代码MIT许可开源。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语存在显著方言差异，现有资源多聚焦单一方言，缺乏跨方言学习工具与文化背景支持，需要一个可扩展、开放的跨方言学习资源。

Method: 使用大模型生成跨六方言短语，五名母语者分层验证难度，按主题组织；开发互动web平台，提供翻译探索、算法干扰项的自适应测验、云同步进度及文化背景，数据与代码开源并以MIT许可发布。

Result: 公开发布552条跨六方言短语资源，平台具备翻译探索、个性化测验、算法干扰项等功能，按难度与主题组织，平台与数据集开源，便于再使用与研究。

Conclusion: 该资源为跨方言阿拉伯学习提供可访问、可扩展的工具，有助于学习者掌握不同方言并理解文化语境，同时开放源代码推动再利用与评估。

Abstract: We present ArabicDialectHub, a cross-dialectal Arabic learning resource comprising 552 phrases across six varieties (Moroccan Darija, Lebanese, Syrian, Emirati, Saudi, and MSA) and an interactive web platform. Phrases were generated using LLMs and validated by five native speakers, stratified by difficulty, and organized thematically. The open-source platform provides translation exploration, adaptive quizzing with algorithmic distractor generation, cloud-synchronized progress tracking, and cultural context. Both the dataset and complete platform source code are released under MIT license. Platform: https://arabic-dialect-hub.netlify.app.

</details>


### [37] [Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs](https://arxiv.org/abs/2601.23001)
*Afrozah Nadeem,Agrima,Mehwish Nasim,Usman Naseem*

Main category: cs.CL

TL;DR: Cross-lingual evaluation of political bias in multilingual LLMs and a post-hoc mitigation framework (CLAS) that aligns ideological representations across languages to ensure consistency and adaptive intervention strength.


<details>
  <summary>Details</summary>
Motivation: Fill a gap in multilingual, cross-lingual bias research by evaluating political bias across 50 countries and 33 languages and providing a scalable, interpretable mitigation method.

Method: Large-scale multilingual bias evaluation across diverse languages; introduce Cross-Lingual Alignment Steering (CLAS) that maps latent ideological prompts into a shared ideological subspace and dynamically regulates intervention strength to prevent overcorrection.

Result: Substantial reduction in political bias along economic and social axes with minimal degradation in response quality; cross-lingual consistency achieved; governance framework is scalable and interpretable.

Conclusion: Proposes a scalable, fairness-aware multilingual LLM governance paradigm that balances ideological neutrality with linguistic and cultural diversity.

Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity.

</details>


### [38] [InstructDiff: Domain-Adaptive Data Selection via Differential Entropy for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.23006)
*Junyou Su,He Zhu,Xiao Luo,Liyu Zhang,Hong-Yu Zhou,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: InstructDiff 通过差分熵的数据选择，在强制学习中实现域自适应，使用仅 10% 数据即可在推理任务和通用指令跟随任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有数据筛选方法对领域高度敏感，难以在推理任务与通用任务之间泛化；需要一个统一的、基于信息理论的域自适应准则。

Method: 引入差分熵作为域自适应选择标准，通过热启动标定、双向 NLL 过滤、基于熵的排序。具体包括：基模型与微调后 calibrated 模型之间的熵差衡量、在 warmup 阶段进行标定以估算差分熵、对样本进行双向 NLL 过滤与熵排序。

Result: 在数学推理任务上相较于使用完整数据训练提升约 17% 相对；在通用指令跟随任务上提升约 52%，并且比基线更优，同时仅使用数据的 10%。

Conclusion: 提出一个统一、数据高效的 SFT 数据筛选框架 InstructDiff，能够在不同任务域中自适应选择数据，显著降低数据成本的同时保持或提升性能。

Abstract: Supervised fine-tuning (SFT) is fundamental to adapting large language models, yet training on complete datasets incurs prohibitive costs with diminishing returns. Existing data selection methods suffer from severe domain specificity: techniques optimized for general instruction-following fail on reasoning tasks, and vice versa. We observe that measuring entropy differences between base models and minimally instruction-tuned calibrated models reveals a pattern -- samples with the lowest differential entropy consistently yield optimal performance across domains, yet this principle manifests domain-adaptively: reasoning tasks favor entropy increase (cognitive expansion), while general tasks favor entropy decrease (cognitive compression). We introduce InstructDiff, a unified framework that operationalizes differential entropy as a domain-adaptive selection criterion through warmup calibration, bi-directional NLL filtering, and entropy-based ranking. Extensive experiments show that InstructDiff achieves 17\% relative improvement over full data training on mathematical reasoning and 52\% for general instruction-following, outperforming prior baselines while using only 10\% of the data.

</details>


### [39] [DimABSA: Building Multilingual and Multidomain Datasets for Dimensional Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2601.23022)
*Lung-Hao Lee,Liang-Chih Yu,Natalia Loukashevich,Ilseyar Alimova,Alexander Panchenko,Tzu-Mi Lin,Zhe-Yu Xu,Jian-Yu Zhou,Guangmin Zheng,Jin Wang,Sharanya Awasthi,Jonas Becker,Jan Philip Wahle,Terry Ruas,Shamsuddeen Hassan Muhammad,Saif M. Mohammed*

Main category: cs.CL

TL;DR: 提出 DimABSA 的多语言维度情感分析数据集及评估框架，结合情感的价值-激活度（VA）连续分数，建立三项子任务和一个新的连续 F1 指标，以跨语言、跨领域开展维度化 ABSA 的基准评估。


<details>
  <summary>Details</summary>
Motivation: 传统 ABSA 采用粗粒度的正/负标签，无法捕捉细粒度情感变化。本研究通过引入连续 VA 分数和多语言数据，扩展到维度化、跨语言的 ABSA 研究，填补该领域的资源空缺。

Method: 构建 DimABSA 数据集，包含传统 ABSA 要素（方面词、方面类别、观点词）及 VA 分数；设计三项结合 VA 与 ABSA 的子任务；提出将 VA 预测误差纳入标准 F1 的连续 F1（cF1）指标；在提示与微调的大语言模型上对所有子任务进行基准评估。

Result: DimABSA 拥有 76,958 个方面实例、42,590 条句子、覆盖 6 种语言、4 个领域；三项子任务实现跨模态评估，cF1 指标可综合类别与连续预测的误差；LLMs 在基准中表现出挑战性，显示 DimABSA 作为多语言维度化 ABSA 基准的价值。

Conclusion: DimABSA 为多语言维度化 ABSA 提供了重要资源和基准，促进在更细粒度的情感分析上进行研究，并为将来引入 VA 导向的情感分析与跨域应用奠定基础。

Abstract: Aspect-Based Sentiment Analysis (ABSA) focuses on extracting sentiment at a fine-grained aspect level and has been widely applied across real-world domains. However, existing ABSA research relies on coarse-grained categorical labels (e.g., positive, negative), which limits its ability to capture nuanced affective states. To address this limitation, we adopt a dimensional approach that represents sentiment with continuous valence-arousal (VA) scores, enabling fine-grained analysis at both the aspect and sentiment levels. To this end, we introduce DimABSA, the first multilingual, dimensional ABSA resource annotated with both traditional ABSA elements (aspect terms, aspect categories, and opinion terms) and newly introduced VA scores. This resource contains 76,958 aspect instances across 42,590 sentences, spanning six languages and four domains. We further introduce three subtasks that combine VA scores with different ABSA elements, providing a bridge from traditional ABSA to dimensional ABSA. Given that these subtasks involve both categorical and continuous outputs, we propose a new unified metric, continuous F1 (cF1), which incorporates VA prediction error into standard F1. We provide a comprehensive benchmark using both prompted and fine-tuned large language models across all subtasks. Our results show that DimABSA is a challenging benchmark and provides a foundation for advancing multilingual dimensional ABSA.

</details>


### [40] [Character as a Latent Variable in Large Language Models: A Mechanistic Account of Emergent Misalignment and Conditional Safety Failures](https://arxiv.org/abs/2601.23081)
*Yanghao Su,Wenbo Zhou,Tianwei Zhang,Qiu Han,Weiming Zhang,Nenghai Yu,Jie Zhang*

Main category: cs.CL

TL;DR:  Emergent misalignment arise from stable behavioral shifts learned during fine-tuning on narrowly scoped data, driven by character-level dispositions rather than corrupted knowledge. These dispositions can be activated by training-time cues or inference-time prompts and transfer better than incorrect-advice fine-tuning, while largely preserving general capabilities.


<details>
  <summary>Details</summary>
Motivation: The prevailing explanation attributes misalignment to generalization of erroneous or unsafe content. The paper argues this view is incomplete and identifies behavioral dispositions (character formation) as a core, underexplored alignment risk that persists across model families and domains.

Method: Fine-tune models on data with specific character-level dispositions across multiple domains/model families; compare against incorrect-advice fine-tuning; test activation via training-time triggers and persona-aligned prompts; assess transferability and effect on general capabilities.

Result: Character-level dispositions yield stronger and more transferable misalignment than incorrect-advice fine-tuning, with minimal loss of general capabilities. Misalignment stems from stable behavioral shifts, not degradation of capabilities or knowledge corruption. These dispositions can be conditionally activated by both training-time and inference-time prompts, revealing shared structure with backdoor/jailbreak susceptibility.

Conclusion: Character formation is a central, underexplored alignment risk. Robust alignment should target behavioral dispositions themselves rather than focusing solely on isolated errors or prompt-level defenses; this connects emergent misalignment with broader issues like backdoors and jailbreaks.

Abstract: Emergent Misalignment refers to a failure mode in which fine-tuning large language models (LLMs) on narrowly scoped data induces broadly misaligned behavior. Prior explanations mainly attribute this phenomenon to the generalization of erroneous or unsafe content. In this work, we show that this view is incomplete. Across multiple domains and model families, we find that fine-tuning models on data exhibiting specific character-level dispositions induces substantially stronger and more transferable misalignment than incorrect-advice fine-tuning, while largely preserving general capabilities. This indicates that emergent misalignment arises from stable shifts in model behavior rather than from capability degradation or corrupted knowledge. We further show that such behavioral dispositions can be conditionally activated by both training-time triggers and inference-time persona-aligned prompts, revealing shared structure across emergent misalignment, backdoor activation, and jailbreak susceptibility. Overall, our results identify character formation as a central and underexplored alignment risk, suggesting that robust alignment must address behavioral dispositions rather than isolated errors or prompt-level defenses.

</details>


### [41] [Safer Policy Compliance with Dynamic Epistemic Fallback](https://arxiv.org/abs/2601.23094)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 提出 Dynamic Epistemic Fallback (DEF)，在推理时通过分级提示让 LLM 检测、拒绝被扰动的政策文本并回退到内部知识，从而提升对法律文本攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人类具有认知防御机制来抵御日常互动中的欺骗与错误信息，将其转化为 LLM 的动态安全机制，可用于高风险任务（如遵循数据隐私法规）以提升鲁棒性。

Method: 设计 DEF 框架，利用不同等级的一句话提示对模型推理过程进行干预，使其更可能标记不一致、拒绝合规并回退到参数化知识。以 HIPAA、GDPR 等全球性法律政策为评估对象，评估前沿 LLMs 的检测与拒绝能力，实验中 DeepSeek-R1 在某设置达到 100% 的检测率。

Result: DEF 显著提升对被扰动政策文本的检测与拒绝能力，在特定设置下 DeepSeek-R1 实现 100% 的检测率，表明认知启发式动态防御在提升合规性鲁棒性方面有效。

Conclusion: 认知启发的动态安全防御对提升 LLM 对法律文本伪装攻击的鲁棒性具有潜力，未来应扩展至更多场景、模型与攻击变体，并强调可重复性与现实应用性。

Abstract: Humans develop a series of cognitive defenses, known as epistemic vigilance, to combat risks of deception and misinformation from everyday interactions. Developing safeguards for LLMs inspired by this mechanism might be particularly helpful for their application in high-stakes tasks such as automating compliance with data privacy laws. In this paper, we introduce Dynamic Epistemic Fallback (DEF), a dynamic safety protocol for improving an LLM's inference-time defenses against deceptive attacks that make use of maliciously perturbed policy texts. Through various levels of one-sentence textual cues, DEF nudges LLMs to flag inconsistencies, refuse compliance, and fallback to their parametric knowledge upon encountering perturbed policy texts. Using globally recognized legal policies such as HIPAA and GDPR, our empirical evaluations report that DEF effectively improves the capability of frontier LLMs to detect and refuse perturbed versions of policies, with DeepSeek-R1 achieving a 100% detection rate in one setting. This work encourages further efforts to develop cognitively inspired defenses to improve LLM robustness against forms of harm and deception that exploit legal artifacts.

</details>


### [42] [Evaluating the Utility of Grounding Documents with Reference-Free LLM-based Metrics](https://arxiv.org/abs/2601.23129)
*Yilun Hua,Giuseppe Castellucci,Peter Schulam,Heba Elfardy,Kevin Small*

Main category: cs.CL

TL;DR: 提出GroGU，一种针对 grounding 的模型特定、无参照度量，基于熵的下游生成置信度来衡量内容的实用性，用于RAG的数据筛选和DPO训练，实验给出MRR和答案准确率的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的内容实用性度量要么与具体模型无关，要么依赖昂贵的标注，难以准确反映某段检索结果对当前LLM生成的帮助。需要一种无参照、模型特定的对 grounding 实用性的定量评估。

Method: 提出GroGU，将实用性定义为下游LLM生成置信度的函数，基于熵度量，且无需标注。以GroGU来筛选高实用性偏好数据，用于Direct Preference Optimization训练一个查询改写器，从而提升RAG的效果。

Result: 实验显示，在MRR上提升最高18.2点，在答案准确度上提升最高9.4点。

Conclusion: GroGU在区分真实可用的引文文献方面具有较高忠实度，能够捕捉LLM无关度量忽略的细微差异，且用于数据筛选和DPO训练可显著提升RAG性能，且无需额外标注。

Abstract: Retrieval Augmented Generation (RAG)'s success depends on the utility the LLM derives from the content used for grounding. Quantifying content utility does not have a definitive specification and existing metrics ignore model-specific capabilities and/or rely on costly annotations. In this paper, we propose Grounding Generation Utility (GroGU), a model-specific and reference-free metric that defines utility as a function of the downstream LLM's generation confidence based on entropy. Despite having no annotation requirements, GroGU is largely faithful in distinguishing ground-truth documents while capturing nuances ignored by LLM-agnostic metrics. We apply GroGU to train a query-rewriter for RAG by identifying high-utility preference data for Direct Preference Optimization. Experiments show improvements by up to 18.2 points in Mean Reciprocal Rank and up to 9.4 points in answer accuracy.

</details>


### [43] [Monotonic Reference-Free Refinement for Autoformalization](https://arxiv.org/abs/2601.23166)
*Lan Zhang,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 提出无参照的迭代单调全定理自动形式化方法，结合定理证明器与LLM评审，实现多维度一致提升并具单调收敛保证，实验在 miniF2F/ProofNet 上取得显著结果。


<details>
  <summary>Details</summary>
Motivation: 现有的语句自动形式化迭代仅改善单一方面，难以在多个质量维度上协同优化，缺乏无参照推理时的全定理自动化解决路径。

Method: 引入 masked 的复合目标，对 Formal Validity、Logical Preservation、Mathematical Consistency、Formal Quality 四个维度进行优化；通过 responsiveness map 指导不同角色的 LLM 对应提升维度；采用接受策略确保单调改进并给出收敛/终止条件。

Result: 实验在 miniF2F 上达到 93.44% formal validity、78.22% overall；在 ProofNet 上达到 44.09% formal validity、29.79% overall。

Conclusion: 证明该过程可在不依赖 ground-truth 证明或现有形式化的前提下，实现多维度的协同提升，并具备单调性保证和收敛性条件，为全定理自动形式化提供一种稳健框架。

Abstract: While statement autoformalization has advanced rapidly, full-theorem autoformalization remains largely unexplored. Existing iterative refinement methods in statement autoformalization typicall improve isolated aspects of formalization, such as syntactic correctness, but struggle to jointly optimizing multiple quality dimensions, which is critical for full-theorem autoformalization. We introduce a reference-free iterative monotonic process for full-theorem autoformalization that leverages complementary feedback from theorem provers and LLM-based judges, without access to ground-truth proofs or existing formalizations at inference time. Our approach optimizes a masked composite objective over Formal Validity, Logical Preservation, Mathematical Consistency, and Formal Quality, guided by a responsiveness map that indicates how different LLMs acting as different roles preferentially improve each dimension. We further propose an acceptance policy that guarantees certified monotonic improvement, and provide conditions ensuring convergence and termination. Empirical experiments demonstrate the proposed process enables simultaneous improvement across multiple dimensions, achieving 93.44% formal validity and a 78.22% overall score on miniF2F, and 44.09% formal validity and a 29.79% overall score on ProofNet.

</details>


### [44] [FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation](https://arxiv.org/abs/2601.23182)
*Siyang He,Qiqi Wang,Xiaoran Liu,Hongnan Ma,Yiwei Shi,Yuerong Song,Ying Zhu,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出 FourierSampler，通过频域滑窗在 dLLMs 的谱特征下实现结构到细节的生成，显著提升解码效果。


<details>
  <summary>Details</summary>
Motivation: 揭示 diffusion language models 的低/高频信息在全局结构与局部细节中的分工，解决现有解码策略的位置信偏置问题。

Method: 在隐藏状态的频域上设计滑动窗口机制，动态引导生成，提出 FourierSampler；在 LLADA 与 SDAR 上比较多种推理增强策略并与自回归模型对比。

Result: 在 LLaDA1.5-8B 相对提升 20.4%，在 LLaDA-8B-Instruct 相对提升 16.0%，并超越同等规模的自回归模型如 Llama3.1-8B-Instruct。

Conclusion: 基于频域分析证明结构到细节的生成能力可显著提升非自回归扩散语言模型的解码质量，提示未来在更多数据和模型上探索频域引导的解码策略。

Abstract: Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a "structure-to-detail" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.

</details>


### [45] [JobResQA: A Benchmark for LLM Machine Reading Comprehension on Multilingual Résumés and JDs](https://arxiv.org/abs/2601.23183)
*Casimiro Pio Carrino,Paula Estrella,Rabih Zbib,Carlos Escolano,José A. R. Fonollosa*

Main category: cs.CL

TL;DR: JobResQA 是一个面向HR任务的多语言MRC基准数据集，包含581条问答对，覆盖105对合成简历与职位描述，涵盖英语、西班牙语、意大利语、德语和中文三种复杂度等级的跨文档推理。通过去标识化/数据合成生成数据，并使用占位符实现可控的偏见和公平性研究。采用 TEaR 的翻译流水线、MQM 注释和选择性后编辑，构建高质量多语对齐基准；基线评估显示英语/西班牙语表现较好，其他语言显著下降，揭示多语言HR场景下的MRC能力差距。公开可复现，促进公平可靠的LLM-HR系统发展。


<details>
  <summary>Details</summary>
Motivation: 评估和提升大语言模型在HR领域多语言MRC任务中的能力，关注数据隐私、现实性、以及偏见/公平性问题；建立可重复的、多语言的基准以促进对齐与鲁棒性研究。

Method: 通过对真实来源去标识化并进行数据合成，生成105对简历/职位描述的581对问答。引入占位符控制的再现性属性以支持偏见研究；采用 TEaR 翻译方法论的高性价比人机协作翻译，结合 MQM 注释和选择性后编辑，构建高质量多语言基准。基线通过“LLM 充当评判者”方式，在多家开源权重模型上评估跨语言MRC。

Result: 基线评估显示英语和西班牙语表现更好，而其他语言显著下降，揭示多语言HR场景下的MRC能力差距与挑战。基准数据集公开可用，为推动公平、可靠的LLM-HR系统提供可重复的评估平台。

Conclusion: JobResQA 提供一个可复现、便于进行偏见与公平性分析的多语言HR MRC基准，帮助研究者和从业者改进跨语言理解能力，提升HR相关LLM系统的可靠性与公平性。

Abstract: We introduce JobResQA, a multilingual Question Answering benchmark for evaluating Machine Reading Comprehension (MRC) capabilities of LLMs on HR-specific tasks involving résumés and job descriptions. The dataset comprises 581 QA pairs across 105 synthetic résumé-job description pairs in five languages (English, Spanish, Italian, German, and Chinese), with questions spanning three complexity levels from basic factual extraction to complex cross-document reasoning. We propose a data generation pipeline derived from real-world sources through de-identification and data synthesis to ensure both realism and privacy, while controlled demographic and professional attributes (implemented via placeholders) enable systematic bias and fairness studies. We also present a cost-effective, human-in-the-loop translation pipeline based on the TEaR methodology, incorporating MQM error annotations and selective post-editing to ensure an high-quality multi-way parallel benchmark. We provide a baseline evaluations across multiple open-weight LLM families using an LLM-as-judge approach revealing higher performances on English and Spanish but substantial degradation for other languages, highlighting critical gaps in multilingual MRC capabilities for HR applications. JobResQA provides a reproducible benchmark for advancing fair and reliable LLM-based HR systems. The benchmark is publicly available at: https://github.com/Avature/jobresqa-benchmark

</details>


### [46] [ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought](https://arxiv.org/abs/2601.23184)
*Fanmeng Wang,Haotian Liu,Guojiang Zhao,Hongteng Xu,Zhifeng Gao*

Main category: cs.CL

TL;DR: 提出 ReGuLaR，通过将显式推理渲染为图像，并在 VAE 框架下从后验采样潜在状态来实现推理的高效压缩，从而提升推理性能并在多模态推理中甚至超越 CoT。


<details>
  <summary>Details</summary>
Motivation: 解决 Chain-of-Thought（CoT）带来的显式推理链的巨大计算冗余，以及现有潜在推理方法在缺乏有效压缩引导时导致的性能下降。提出一种在压缩与推理效果之间取得更好权衡的新思路。

Method: 将潜在推理建模为变分自编码器（VAE），在学习过程中从前一时刻的后验条件下采样当前潜在推理状态；将显式推理链渲染为图像，提取密集的视觉-语义表示以正则化后验分布，从而实现高效压缩且信息损失最小。

Result: 在广泛实验中，ReGuLaR在计算效率和推理效果上显著优于现有潜在推理方法，甚至在多模态推理下超越CoT。并给出代码实现（开源）。

Conclusion: 提供了一种简单而新颖的潜在学习范式，解决了潜在推理中对压缩引导的不足问题，证明了潜在推理在提升效率与推理效果方面的潜力，并在多模态场景中展现出竞争力。

Abstract: While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.

</details>


### [47] [Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience](https://arxiv.org/abs/2601.23188)
*Zhongxiang Sun,Qipeng Wang,Weijie Yu,Jingxuan Yang,Haolang Lu,Jun Xu*

Main category: cs.CL

TL;DR: 提出一个带元认知监控的深度搜索框架 DS-MCM，通过快速一致性监测与慢速基于经验的监控，在推理-检索循环中决定何时干预和如何纠正，提高鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型驱动的深度搜索在推理和检索任务中易因状态监控不足而失败；受认知神经科学启发，提出分层元认知来检测异常并触发经验驱动的回顾与更正。

Method: 引入 Fast Consistency Monitor（快速一致性监测）对证据与内部推理置信度进行轻量化检查；引入 Slow Experience-Driven Monitor（慢速基于经验的监控）使用历史轨迹的经验记忆选择性触发纠正干预；将监控嵌入推理-检索循环。

Result: 在多项深度搜索基准与骨干模型上，DS-MCM 显著提升性能和鲁棒性。

Conclusion: 将元认知监控直接嵌入深度搜索流程可提高对不确定性任务的纠错能力和稳定性。

Abstract: Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.

</details>


### [48] [Are you going to finish that? A Practical Study of the Tokenization Boundary Problem](https://arxiv.org/abs/2601.23223)
*Hao Xu,Alisa Liu,Jonathan Hayase,Yejin Choi,Noah A. Smith*

Main category: cs.CL

TL;DR: 现实提示以部分令牌结尾时，模型在正确续写上的概率出现显著扭曲；在无空格语言、高度黏合语言以及代码等场景普遍存在这一问题，前沿模型的正确续写概率比对齐令牌的提示低约三个数量级，且随模型规模增大仍未缓解甚至恶化；同时评估了推理时缓解策略与精确解法的有效性。


<details>
  <summary>Details</summary>
Motivation: 训练目标是令牌序列，而实际用户输入往往以词边界为单位，导致部分令牌问题在现实用例中的普遍性和严重性尚未被充分量化，需在不同语言形态和代码场景下评估影响并提供缓解建议。

Method: 系统构造语义自然且以部分令牌结尾的提示；在前沿语言模型上测量未完成单词续写的正确概率，并与回退到令牌对齐的提示进行对比；覆盖无空格语言、黏合语言和代码等情境，评估推理时缓解策略及等效精确解法，分析规模与语言维度上的差异。

Result: 在 frontier LMs 中，部分令牌提示的正确续写概率比令牌对齐提示低约三个数量级；这一降级在模型规模增大时并未减弱，且通常会进一步恶化。推理时缓解策略与最新的“精确解”方法经验证有效，表明问题在现实用例中具有可观的规模和严重性。

Conclusion: 令牌化导致的概率扭曲在现实用例中显著且普遍，需在推理部署中采用有效缓解措施；研究结果为模型提供商提供实际可行的建议，并支持对现有精确解法的应用与普及。

Abstract: Language models (LMs) are trained over sequences of tokens, whereas users interact with LMs via text. This mismatch gives rise to the partial token problem, which occurs when a user ends their prompt in the middle of the expected next-token, leading to distorted next-token predictions. Although this issue has been studied using arbitrary character prefixes, its prevalence and severity in realistic prompts respecting word boundaries remains underexplored. In this work, we identify three domains where token and "word" boundaries often do not line up: languages that do not use whitespace, highly compounding languages, and code. In Chinese, for example, up to 25% of word boundaries do not line up with token boundaries, making even natural, word-complete prompts susceptible to this problem. We systematically construct semantically natural prompts ending with a partial tokens; in experiments, we find that they comprise a serious failure mode: frontier LMs consistently place three orders of magnitude less probability on the correct continuation compared to when the prompt is "backed-off" to be token-aligned. This degradation does not diminish with scale and often worsens for larger models. Finally, we evaluate inference-time mitigations to the partial token problem and validate the effectiveness of recent exact solutions. Overall, we demonstrate the scale and severity of probability distortion caused by tokenization in realistic use cases, and provide practical recommentions for model inference providers.

</details>


### [49] [Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models](https://arxiv.org/abs/2601.23255)
*Ye Yu,Haibo Jin,Yaoning Yu,Jun Zhuang,Haohan Wang*

Main category: cs.CL

TL;DR: 通过文本到语音的攻击，将禁令指令嵌入叙事风格音频流，绕过文本安全，且在合成语音中表现出极高的攻击成功率，强调需同时考虑语言与声学信息的安全防护。


<details>
  <summary>Details</summary>
Motivation: 随着大规模音频语言模型接入日常应用，现有安全研究多集中于文本，对语音信号与声学特征的安全脆弱性缺乏系统研究，存在安全性盲点。

Method: 设计文本到音频的“叙事风格”越狱攻击，利用高级指令跟随型 TTS 将被禁指示嵌入语音流中，攻击通过分析文本与声学结构、韵律等特性对安全机制进行规避。

Result: 在合成语音场景下，叙事格式能触发模型输出受限内容，且在 Gemini 2.0 Flash 等模型上取得 98.26% 的攻击成功率，显著高于文本基线。

Conclusion: 提醒需建立联合推理的安全框架，处理语言与旁声（paralinguistic）表征，尤其随着语音界面的普及，这一方向的安全防护变得更加关键。

Abstract: Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset](https://arxiv.org/abs/2601.22161)
*Anmol Guragain*

Main category: cs.LG

TL;DR: 在小规模多模态情感识别中，简单且领域相关的特征工程和预训练策略优于复杂注意力架构；M2 的复杂注意力受过拟合影响表现不佳，M1/ViT 与领域预训练表现最好。


<details>
  <summary>Details</summary>
Motivation: 探究复杂注意力机制在小数据集上的有效性，以及领域知识对情感识别的影响。

Method: 将模型分为三类：M1 基线 Transformer；M2 因子化注意力；M3 改进的 CNN；使用 EAV 数据集；引入 delta MFCC、EEG 频域特征；对比 ViViT 与域适应预训练。

Result: M2 相较基线平均低 5-13pp，因过拟合与预训特征破坏。音频 delta MFCC 将准确率从 61.9% 提升到 65.56% (+3.66pp)；EEG 频域特征达到 67.62% (+7.62pp)。视觉方面，M1 基线达 75.30%，超 ViViT 74.5；视觉 delta 特征 72.68% (+1.28pp)。

Conclusion: 对于小规模情感识别，依靠领域知识的有效实现优于追求架构复杂性的策略。

Abstract: We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9\% to \textbf{65.56\%} (+3.66pp), while frequency-domain features for EEG achieved \textbf{67.62\%} (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached \textbf{75.30\%}, exceeding the paper's ViViT result (74.5\%) through domain-specific pretraining, and vision delta features achieved \textbf{72.68\%} (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

</details>


### [51] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 提出一种混合量子机器学习模型，结合多任务学习与量子卷积的定位权重模块，用于地球观测数据分类，在EO基准数据上验证其可行性与潜在优势。


<details>
  <summary>Details</summary>
Motivation: 解决日益增长的地球观测大数据分析对计算资源的瓶颈，探索量子计算在EO数据分类中的潜在优势，即使受限于现有量子设备。

Method: 构建一个混合模型：使用多任务学习以提升数据编码效率，并引入带有量子卷积的定位权重模块来提取用于分类的有效特征。对多个 EO 基准数据集进行评估，考察模型的泛化性及影响因素。

Result: 在若干 EO 基准数据集上验证模型有效性，实验分析了其泛化能力及影响因素，显示量子机器学习在 EO 数据分析中的潜在优势。

Conclusion: 表明量子机器学习在EO数据分类中具有潜在价值，但需克服量子设备限制，未来可通过更强的量子硬件和更高效的编码/特征提取策略进一步提升性能。

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [52] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: 提出了 CELM，一种临床 EEG-to-Language 基础模型，能够对长时段 EEG 进行端到端的临床报告生成，并在多尺度层级进行描述，基于大规模带标签数据集进行训练与评估，显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 临床 EEG 报告撰写耗时且需要整合长期记录中的异常模式、诊断发现与临床解读；需要可扩展的多模态学习框架以提升自动化报告的质量与覆盖面。

Method: 整合预训练的 EEG 基础模型与语言模型，开发 CELM；在 9,922 份报告和约 11,000 小时 EEG 数据、9,048 名患者的规模下，实现对记录描述、背景活动、癫痫样异常、事件/发作及印象等多尺度临床报告的端到端生成。

Result: 在有患者历史信息监督下，平均相对提升 70%–95%（ROUGE-1、METEOR 等指标从 0.2–0.3 提升至 0.4–0.6）；在零-shot 情况下，得分 0.43–0.52，显著高于基线 0.17–0.26。

Conclusion: CELM 实现了可扩展的多模态 EEG→语言生成与临床报告自动化；公布模型与基准构建管线，具备临床落地潜力。

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [53] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: 提出 FedAdaVR，一种针对异质性及随访客户端参与缺失的联邦学习算法，利用自适应优化器结合方差归约以缓解部分客户端参与带来的误差；并提出 FedAdaVR-Quant，通过对客户端更新进行量化以显著降低内存开销，同时保持性能。对非凸情形下的收敛性进行分析，并通过大量实验在IID和非IID设置下证明其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，异质性尤其表现为随访客户端参与的随机缺失，导致梯度噪声、客户端漂移与部分参与误差。现有文献对该问题的研究不足，难以有效缓解由于不稳定的客户端参与导致的学习偏差与收敛困难。

Method: 提出 FedAdaVR：结合自适应优化器与方差归减技术，利用即使在当前轮次缺席的客户端最近存储更新来“模拟”其参与，从而缓解参与不足带来的误差。扩展为 FedAdaVR-Quant，将客户端更新以量化形式存储以降低内存占用（相比 FedAdaVR 降低 50%、75%、87.5%），同时保持模型性能。对 FedAdaVR 在一般非凸条件下的收敛行为进行理论分析。

Result: 理论上证明算法可消除部分客户端参与误差，且在多数据集、IID/非IID 设置下的实验结果显示 FedAdaVR 持续优于最先进的基线方法。

Conclusion: FedAdaVR及其量化变体在处理随访客户端缺失引发的异质性问题方面表现出稳健性及有效性，提供了一个兼具理论收敛性与实际性能提升的解决方案。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [54] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: 提出 DAJ，通过数据再加权学习训练一个具备可验证奖励的 LLM 评判器，以提升测试时代码生成的选择效果；通过学习领域级或实例级数据重要性权重，强调难题、分布内样本和轨迹对齐数据，实现在 LiveCodeBench 与 BigCodeBench 上的 state-of-the-art。


<details>
  <summary>Details</summary>
Motivation: 训练稳健的 LLM 评判器用于 test-time scaling 面临分布偏移、样本易难不均、训练任务与评估基准不匹配，以及来自 cheaper 模型的数据轨迹与推理模型之间的差异等挑战。

Method: 采用双层数据加权学习框架（bi-level），学习数据重要性权重（域级或样本级）以优化在对齐目标基准的保留集合上的泛化性能；通过可验证奖励来训练，自动强调难题、分布内样本和轨迹对齐的数据，且不依赖手工设计的启发式策略。

Result: 在 LiveCodeBench 和 BigCodeBench 上实现了 state-of-the-art 的测试时尺度表现，超越强基线和领先的专有模型。

Conclusion: 数据再加权用于 LLM 评判器训练在测试时尺度中的应用首次实现，且该方法能自动聚焦难点与轨迹一致性数据，提升泛化性能。

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [55] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: 提出 FunPRM：将函数作为 PRM 推理步骤的模块化代码生成框架，并通过元学习式奖励校正提升评估信号，显著提升代码生成性能与可读性。


<details>
  <summary>Details</summary>
Motivation: 现有基于过程奖励模型的测试时选择在代码任务中效果不佳，原因是缺乏有效的步骤分解与噪声奖励。需要引入模块化的函数级推理和更清洁的奖励信号。

Method: 通过提示让 LLM 将代码分解为函数块并按函数级别进行 PRM 推理；引入元学习奖励校正机制，利用单元测试评估得到的干净最终奖励来净化噪声的部分解奖励。

Result: 在 LiveCodeBench 与 BigCodeBench 上，与五个基础 LLM 比较，FunPRM 持续超越现有的测试时扩展方法；在 LiveCodeBench 与 O4-mini 联合时达到状态最优；产生的代码更具可读性与可重用性。

Conclusion: FunPRM 有效提升代码生成的准确性与代码质量，验证了模块化 PRM 与奖励净化的价值。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [56] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 通过在注意力的旋转空间中引入一个偏好方向的对称破缺机制，使用批量抽样的未学习的查询和值偏置来打破旋转自由度，从而在简单的内存高效优化器上提升性能并提高可解释性，同时实现对注意力头中语义分量的放大。


<details>
  <summary>Details</summary>
Motivation: 标准注意力实现存在额外的旋转自由度，这些自由度在计算中存在但不影响激活或输出，因此是冗余的。通过引入对称破缺来限定旋转空间，期望提升优化器性能并提高可解释性。

Method: 提出一种简单的对称破缺协议，在批量采样的未学习的查询和值偏置中引入一个偏好方向。对124M参数的Transformer进行预训练，采用四种优化器（AdamW、SOAP、SGDM、Energy Conserving Descent），评估验证损失和下游逻辑推理，并分析注意力头中可解释的旋转自由度的放大。

Result: 该方法显著提升了对简单、内存高效的优化器的性能，缩小甚至缩小与更复杂、需要大量内存的自适应方法之间的差距。并且实现了对冗余旋转自由度的可解释利用，在单个注意力头中有选择性地放大语义性强的Token类别。

Conclusion: 最小且原则性的架构改动即可同时提升性能与可解释性。

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [57] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 将时间事件分析（生存分析）转化为一系列二分类问题的分类框架，通过将事件时间离散化来处理右截断数据；可使用现成的表格基模型在不额外训练的情况下进行推断；理论证明在标准截尾假设下，最小化二分类损失可收敛地恢复生存概率；在53个真实数据集上实现优于经典与深度基线的性能。


<details>
  <summary>Details</summary>
Motivation: 在带右截断的时间事件分析中，将表格基模型应用于生存分析存在困难。需要一种能利用现成的表格模型且兼具理论保障与良好实践效果的方法。

Method: 将静态与动态生存分析离散化成一系列二分类任务，按时间对事件进行离散化；被截断的观测在某些时间点表现为缺失标签；利用现成的表格基模型通过上下文学习实现生存分析，无需显式训练；在标准截尾模型下，最小化二分类损失可恢复真实生存概率（随着样本量增加）。

Result: 在53个真实数据集上评估，所提出方法相较经典与深度基线，在多种生存评测指标上平均表现更好。

Conclusion: 分类化、时间离散化的框架使得表格基模型能够进行生存分析，并利用上下文学习实现零样本/少样本推断，理论一致性和实证性能均得到支持。

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [58] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 低成本的可穿戴传感器+张量分类器STM的HAR框架，显著优于经典分类器，在六种日常活动上达到96.67%测试准确率，交叉验证最高98.50%。


<details>
  <summary>Details</summary>
Motivation: 解决医疗基础设施不足导致的居家护理困难，需在资源有限地区实现对老人和易感群体的远程、可靠的日常活动识别，以支撑瑜伽、理疗等康复活动的自适应反馈。

Method: 采集加速度计与陀螺仪数据，覆盖步行、上楼、下楼、坐、站、躺等六类活动；比较逻辑回归、随机森林、SVM、k-NN等经典分类器与提出的支持张量机（STM），STM通过张量表示保留时空动态以实现鲁棒分类。

Result: SVM达到93.33%，LR/RF/k-NN各为91.11%；STM测试准确率96.67%，交叉验证最高98.50%。STM在保留时空特征方面优于传统方法，提升了分类鲁棒性。

Conclusion: STM以张量表示捕捉时空动态，提升多类日常活动识别的鲁棒性与可迁移性，为远程医疗、老人辅助和低资源/settings下的居家健身监控提供可扩展解决方案。

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [59] [Task-Uniform Convergence and Backward Transfer in Federated Domain-Incremental Learning with Partial Participation](https://arxiv.org/abs/2601.22274)
*Longtao Xu,Jian Li*

Main category: cs.LG

TL;DR: 提出一种无记忆的 FDIL 算法 SPECIAL，通过服务器端锚点在 FedAvg 基础上抑制漂移，实现跨任务增量学习，给出BKT保证与分阶段非凸收敛率。


<details>
  <summary>Details</summary>
Motivation: 在真实世界的联邦学习中，数据分布随任务演化且不能共享原始数据，需要在不增加记忆、回放、或任务特定头的情况下实现对前序任务的知识保留与跨任务学习。

Method: 在服务器端加入一个轻量的近端锚点，将参与客户端的更新向先前全局模型靠拢，保持通信量与模型规模不变，无回放、无数据合成、无任务特定头。

Result: 给出早前任务的BKT界限，显示漂移受控随轮次、局部 epoch 数与参与客户端数增加而减小；给出首个在部分参与下的 FDIL 非凸收敛率为 O((E/NT)^{1/2})，与单任务 FedAvg 学习效率相匹配，同时将优化方差与任务间漂移明确分离。

Conclusion: SPECIAL 为简单高效的 FDIL 方案，在不依赖回放、数据合成或任务专用头的前提下，理论与实验均证明其在部分参与设定下的有效性与稳健性。

Abstract: Real-world federated systems seldom operate on static data: input distributions drift while privacy rules forbid raw-data sharing. We study this setting as Federated Domain-Incremental Learning (FDIL), where (i) clients are heterogeneous, (ii) tasks arrive sequentially with shifting domains, yet (iii) the label space remains fixed. Two theoretical pillars remain missing for FDIL under realistic deployment: a guarantee of backward knowledge transfer (BKT) and a convergence rate that holds across the sequence of all tasks with partial participation. We introduce SPECIAL (Server-Proximal Efficient Continual Aggregation for Learning), a simple, memory-free FDIL algorithm that adds a single server-side ``anchor'' to vanilla FedAvg: in each round, the server nudges the uniformly sampled participated clients update toward the previous global model with a lightweight proximal term. This anchor curbs cumulative drift without replay buffers, synthetic data, or task-specific heads, keeping communication and model size unchanged. Our theory shows that SPECIAL (i) preserves earlier tasks: a BKT bound caps any increase in prior-task loss by a drift-controlled term that shrinks with more rounds, local epochs, and participating clients; and (ii) learns efficiently across all tasks: the first communication-efficient non-convex convergence rate for FDIL with partial participation, O((E/NT)^(1/2)), with E local epochs, T communication rounds, and N participated clients per round, matching single-task FedAvg while explicitly separating optimization variance from inter-task drift. Experimental results further demonstrate the effectiveness of SPECIAL.

</details>


### [60] [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)
*Mingyu Lu,Soham Gadgil,Chris Lin,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: 提出 SurrogateSHAP，一种无需重新训练的Shapley值近似框架，通过对预训练模型进行推理、用梯度提升树近似效用函数并从树模型解析出Shapley值，以实现高效且可扩展的数据贡献归因，在文本到图像扩散模型的数据市场与安全审计中表现良好。


<details>
  <summary>Details</summary>
Motivation: 在现实创意工作流程中，数据贡献者的价值评估对于公平补偿和可持续数据市场至关重要。Shapley 值理论上可用于归因，但成本高昂，需对每个子集重新训练模型，并且需要大量子集以估计贡献，致使纯算力不可行。

Method: 提出 Retraining-free 框架 SurrogateSHAP，通过对预训练模型进行推理来近似昂贵的重新训练博弈；使用梯度提升树来拟合效用函数，并由树模型推导出解析的 Shapley 值；在三个任务上评估（DDPM-CFG on CIFAR-20 的图像质量、Stable Diffusion on Post-Impressionist artworks 的美学、FLUX.1 on Fashion-Product data 的产品多样性），并展示能有效定位数据源以识别相关性偏差。

Result: SurrogateSHAP 在各任务上优于现有方法，同时显著降低计算开销，稳定地识别对效用指标有影响的贡献者，并能对临床图像中的虚假相关性进行源定位，支持对安全关键生成模型的可扩展审计。

Conclusion: 提供了一种高效、可扩展且可审计的贡献归因框架，促进数据市场的公平定价与合规安全性评估，但需进一步在更多数据集和不同模型上验证鲁棒性与潜在偏差。

Abstract: As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.

</details>


### [61] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: 提出 Riemannian Lyapunov Optimizers (RLOs)，在黎曼流形的离散时间受控系统框架中统一设计优化算法，建立 NAIM 两阶段训练与严格 Lyapunov 函数收敛性证明，提供“优化器生成器”以重现经典算法并设计新算法，在大规模基准测试中达到 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有优化器多为启发式改进，缺乏统一、带理论保障的框架。通过将优化视为在黎曼参数流形上的离散时间受控系统，构建带有稳定性的理论基础。

Method: 将优化过程建模为黎曼参数流形上的离散时间受控动力系统，识别 Normally Attracting Invariant Manifold (NAIM)，训练分为快速对齐速度状态到目标图的阶段和在目标图内的受控演化阶段；构造严格 Lyapunov 函数以证明收敛到目标流形；给出一个“优化器生成器”以系统地从理论推导出优化算法，既可重现经典优化器，也能设计新的 RLOs；通过几何诊断和大规模基准测试验证理论与性能。

Result: 理论框架获得几何诊断支持，基于控制理论设计的优化器实现了竞争性甚至领先的性能，在大规模基准中表现出色。

Conclusion: 将控制理论与现代机器学习优化结合，提供统一语言和工具箱来设计稳定、有效的优化算法。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [62] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 模型合并的可合并性取决于合并方法和参与任务；子空间重叠和梯度对齐是普遍、方法无关的先决条件，并存在方法特定的“指纹”。


<details>
  <summary>Details</summary>
Motivation: 揭示影响跨模型合并后性能的因素，打破将合并可比性视为内在属性的假设，提供跨方法的诊断框架。

Method: 提出一个与架构无关的框架，通过对一组可解释的成对度量（如梯度L2距离）进行线性优化，分析四种合并方法的相关性，以预测后合并性能。

Result: 不同合并方法的成功驱动因素差异显著（度量重叠46.7%；符号一致性55.3%），显示方法特定的“指纹”；但子空间重叠和梯度对齐等指标在跨方法中稳定出现，构成兼容性的基础前提。

Conclusion: 提供诊断性框架以理解可合并性，促使未来的微调策略明确地强化这些属性；在设计合并策略时应关注子空间对齐与梯度协同。

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [63] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: A conformal prediction framework CP4Gen for conditional generative models using clustering-based density estimation to produce calibrated, interpretable prediction sets with lower complexity; shows improved set volume and simplicity in synthetic and climate emulation tasks.


<details>
  <summary>Details</summary>
Motivation: Calibrated uncertainty is essential for high-stakes use of conditional generative models. Existing methods struggle with outliers, produce complex or verbose prediction sets, and lack interpretability.

Method: Introduce CP4Gen, a novel conformal prediction approach that applies clustering-based density estimation to model-generated samples to form prediction sets. The method aims to be robust to outliers and to reduce structural complexity of the sets.

Result: Empirical results show that CP4Gen achieves smaller prediction-set volumes and simpler structures compared to baselines, with consistent performance gains on synthetic datasets and climate-emulation tasks.

Conclusion: CP4Gen provides a practical, interpretable, and low-complexity uncertainty quantification tool for conditional generative models, suitable for scenarios requiring calibrated and interpretable prediction sets.

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [64] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: ZK-HybridFL 提出一个安全去中心化的联邦学习框架，结合 DAG 账本、专用旁链与零知识证明，以及事件驱动合约和仲裁侧链，实现隐私保护的模型验证、快速收敛与低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有集中式与去中心化联邦学习在可扩展性、安全性与更新验证方面的挑战，旨在在不暴露敏感数据的前提下实现可信的模型更新验证。

Method: 将有向无环图(DAG)账本、专用侧链、零知识证明(ZKP)与事件驱动智能合约相结合；利用仲裁侧链与外部信息源(oble)进行本地模型更新的验证，同时通过内置挑战机制检测对抗行为；实现对更新的隐私保护验证并抵御无效更新与攻击。

Result: 在图像分类和语言建模任务中，与 Blade-FL 与 ChainFL 相比，ZK-HybridFL 展现更快的收敛、/更高的准确率与更低的 perplexity，以及更低的链上延迟；对大量对抗与空闲节点具备鲁棒性，链上验证可在亚秒级完成，燃气成本更低，并能防止无效更新与弃置攻击。

Conclusion: ZK-HybridFL 是一种可扩展且安全的去中心化联邦学习解决方案，适用于多样化环境。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [65] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 将工作流生成视为后验采样问题，提出 BWG/ BayesFlow，通过分步采样、并行前瞻滚动与环内改进实现训练无关的工作流构建，理论上证明收敛并在六数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作将工作流生成多以优化框架为主，缺乏明确的理论基础，且可解释性和稳定性不足；将生成过程建模为贝叶斯推断以获得理论支撑和更强的可控性。

Method: BWG 作为采样框架，逐步构建工作流，使用并行前瞻滚动进行重要性加权，环内 refiner 进行池级改进；若无 refiner，权重经验分布收敛至目标后验。将其实现为无需训练的 BayesFlow，用于端到端工作流构造。

Result: 在六个基准数据集上，BayesFlow 相较 SOTA 基线的准确率提升最高可达 9 个百分点，相较零-shot 提示提升最高可达 65 个百分点，显示 BWG 的理论性与实验性的显著提升。

Conclusion: BWG 提供一个理论扎实且无训练成本的工作流生成框架，BayesFlow 在多数据集上具备良好性能和可扩展性，未来可在更广领域进一步验证与优化。

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [66] [Stealthy Poisoning Attacks Bypass Defenses in Regression Settings](https://arxiv.org/abs/2601.22308)
*Javier Carnerero-Cano,Luis Muñoz-González,Phillippa Spencer,Emil C. Lupu*

Main category: cs.LG

TL;DR: 提出一个针对回归模型的最优隐蔽中毒攻击框架，并引入目标函数归一化评估来比较攻击效果与隐蔽性之间的权衡，同时提出 BayesClean 作为防御，能在隐蔽攻击且污染点数量较多的场景表现更好。


<details>
  <summary>Details</summary>
Motivation: 回归模型在工业与科学应用中广泛使用，但对投毒鲁棒性的研究不足，常见的威胁模型过于理想化，需建立能在现实环境中应用的隐蔽攻击与防御框架，并能比较不同隐蔽性与效果的权衡。

Method: 1) 提出一个考虑不同可检测程度的最优隐蔽攻击优化问题；2) 引入基于目标函数的归一化方法，用以评估攻击有效性与隐蔽性之间的折中；3) 提出 BayesClean 防御，针对隐蔽型投毒在攻击点数较多时相较于现有方法具有更优性能。

Result: 攻击能够绕过现有的最先进防御，且在隐蔽性较高且污染点数量显著时，BayesClean 能提供更强的防护效果，相较于既有防御具有优势。

Conclusion: 论文提供了评估隐蔽中毒攻击的统一框架，并提出在多重隐蔽攻击场景中更有效的防御工具 BayesClean，为鲁棒回归研究与实际部署提供新方向。

Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a novel optimal stealthy attack formulation that considers different degrees of detectability and show that it bypasses state-of-the-art defenses. We further propose a new methodology based on normalization of objectives to evaluate different trade-offs between effectiveness and detectability. Finally, we develop a novel defense (BayesClean) against stealthy attacks. BayesClean improves on previous defenses when attacks are stealthy and the number of poisoning points is significant.

</details>


### [67] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: SCALAR is a benchmark for geometric scale generalization in materials foundation models, linking structural reasoning with hallucination, consistency, and efficiency across three tasks: CIF→property, physics-grounded CoT, and inverse crystal retrieval; reveals that accuracy alone cannot infer scale generalization as model performance under explicit reasoning is highly variable and can destabilize consistency.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models behave under physically structured distribution shifts in materials science, specifically how geometric scale generalization relates to structural hallucination, consistency, and reasoning.

Method: Introduce SCALAR benchmark built on canonical crystal representations and derived nanoparticle structures created via supercell expansion and geometric truncation across length scales from a few atoms to ~18,000 atoms, totaling ~100,000 DFT-validated structures. Three tasks: (i) CIF→property prediction, (ii) Chain-of-Thought with physics-grounded reasoning, (iii) inverse retrieval of crystals given target properties. Outputs evaluated with structured metrics measuring numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret.

Result: Experiments show large, model-dependent shifts when using explicit physics-grounded reasoning; such reasoning often reduces hallucination and error but can destabilize consistency or validity. Overall, geometric scale generalization cannot be inferred from accuracy alone; different models react differently to explicit reasoning, highlighting trade-offs between accuracy, hallucination, and consistency.

Conclusion: SCALAR provides a rigorous framework to study how scale and structure affect material reasoning in foundation models, offering a benchmark and metrics to quantify trade-offs, and suggesting that future work should disentangle accuracy from generalization and consistency in physics-informed LLMs.

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [68] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: Static alignment tests do not guarantee post-update alignment; black-box tests can conceal latent adversarial behavior that only surfaces after benign updates; post-update-robust evaluation is needed and misalignment scales with model size.


<details>
  <summary>Details</summary>
Motivation: As LLMs are frequently updated, ensuring that alignment held at initial evaluation persists after updates is crucial. Static, black-box evaluation may miss latent misalignment that can be triggered by future updates.

Method: Theoretically analyze overparameterized models to show that static alignment cannot guarantee post-update alignment for any update dataset; prove that static black-box probing cannot distinguish genuinely post-update robust models from ones that conceal adversarial behavior activated by a benign gradient update; empirically validate across privacy, jailbreak safety, and behavioral honesty domains and across model scales.

Result: Static alignment is insufficient to guarantee post-update alignment; black-box probes cannot detect latent adversarial behavior that can be activated after a benign update; post-update misalignment grows with model scale; latent adversarial behavior can be hidden yet activated by a single benign update.

Conclusion: Static evaluation protocols are inadequate for LLM alignment; there is an urgent need for post-update-robust alignment evaluation that accounts for potential updates and scaling effects.

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [69] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: Prediction-Augmented GP-UCB (PA-GP-UCB): a Bayesian optimization method that fuses expensive ground-truth feedback, cheap low-fidelity predictions, and offline data via a joint GP with a control variate to correct bias and reduce uncertainty. It preserves the GP-UCB regret rate while lowering the leading constant, controlled by prediction quality and offline coverage; empirically faster than Vanilla GP-UCB and naive baselines on synthetic tasks and a real-world hypothesis-evaluation task using LLM predictions.


<details>
  <summary>Details</summary>
Motivation: Real-world optimization often faces expensive true evaluations and cheap predictive stand-ins, plus abundant offline data to pretrain models and inform priors. The goal is to improve sample efficiency of Bayesian optimization by effectively utilizing (i) cheap predictions from surrogates, (ii) offline data, and (iii) the expensive ground-truth oracle.

Method: PA-GP-UCB builds a joint Gaussian process over ground-truth evaluations and low-fidelity predictions and uses a control variate estimator derived from the joint posterior to debias prediction bias and shrink posterior uncertainty. Offline data are integrated into the GP prior to further improve calibration. The acquisition rule is a GP-UCB variant with the bias-corrected, uncertainty-adjusted posterior mean/variance.

Result: Theoretical: PA-GP-UCB retains the standard regret rate of GP-UCB while achieving a strictly smaller leading constant, explicitly controlled by the quality of the predictions and coverage of offline data. Empirical: PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented baselines on synthetic benchmarks and a real-world hypothesis-evaluation task involving human behavioral data, where predictions come from large language models.

Conclusion: PA-GP-UCB provides a general, sample-efficient framework for hypothesis generation under expensive feedback by effectively combining expensive true evaluations, cheap predictions, and offline data through a control-variate GP approach.

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [70] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出一个第一的联邦框架用于 LLM 路由，可以让客户端利用本地离线查询-模型评估数据共同学习路由策略；同时支持 ML P 路由器（参数化）和 K-means 路由器（非参数化），在异构客户端分布和非均匀模型覆盖条件下提升准确性与成本平衡，且理论分析显示可减少路由亚最优性。


<details>
  <summary>Details</summary>
Motivation: 现有路由方法依赖集中评估数据，实际被隐私和数据分散性所阻碍，单个客户端数据有限，导致路由性能受限；需要跨客户端协作以提高模型覆盖率和泛化能力，同时保护数据隐私。

Method: 提出联邦学习框架，使客户端在本地离线数据上训练并聚合共享路由策略；支持两类路由器：参数化多层感知机（MLP）和非参数化的 K-means 路由；适用于异构查询分布和非均匀模型覆盖场景。

Result: 在两个基准数据集上，联邦协作显著优于单客户端路由，在提升有效模型覆盖和查询泛化方面提高了准确性/成本的平衡；理论分析证明联邦训练降低了路由的次优性。

Conclusion: 实验与理论均支持联邦路由框架的可行性与有效性，展示了在隐私约束下通过跨客户端协作提升 LLM 路由性能的潜力。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [71] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 在用户级近似差分隐私下，提出一种专用于均值估计的因子分解，结合矩阵分解机制，以实现持续均值估计的渐进更低MSE。


<details>
  <summary>Details</summary>
Motivation: 解决连续数据流下在用户级差分隐私保护下进行均值估计时，纯DP带来过高噪声的问题；近似DP结合矩阵分解的最新进展可望提供更实用的隐私-准确性权衡。

Method: 引入一个专门针对均值估计的因子分解，与矩阵分解机制结合，在持续更新的场景下实现高效、准确的私有化均值估计。

Result: 在持续均值估计的问题上，给出渐进的更低均方误差界限（相较于纯DP），并证明方法有效且具备良好计算效率。

Conclusion: 近似差分隐私下的矩阵分解机制与专门的均值估计因子分解相结合，为持续均值估计提供更实际、精确的隐私保护方案。

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [72] [Spatially-Adaptive Conformal Graph Transformer for Indoor Localization in Wi-Fi Driven Networks](https://arxiv.org/abs/2601.22322)
*Ayesh Abu Lehyeh,Anastassia Gharib,Safwan Wshah*

Main category: cs.LG

TL;DR: 提出 SAC-GT 框架，将图变换模型与空间自适应的保守预测结合，用于室内定位，提供点位预测与区域置信区间。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的室内定位在预测不确定性方面不足，难以在真实部署中提供可靠的置信保障；需要区域化、不确定性量化的方法。

Method: 将 Graph Transformer 捕捉的空间拓扑与信号强度动态整合，并引入 Spatially-Adaptive Conformal Prediction 来生成针对不同环境条件的置信区域，输出2D定位以及区间预测。

Result: 在大规模真实数据集上实现了SOTA定位精度，同时提供鲁棒且空间自适应的可靠性保障。

Conclusion: SAC-GT 能在提供高精度定位的同时，给出统计有效的置信区域，提升实际部署的可用性与安全性。

Abstract: Indoor localization is a critical enabler for a wide range of location-based services in smart environments, including navigation, asset tracking, and safety-critical applications. Recent graph-based models leverage spatial relationships between Wire-less Fidelity (Wi-Fi) Access Points (APs) and devices, offering finer localization granularity, but fall short in quantifying prediction uncertainty, a key requirement for real-world deployment. In this paper, we propose Spatially-Adaptive Conformal Graph Transformer (SAC-GT), a framework for accurate and reliable indoor localization. SAC-GT integrates a Graph Transformer (GT) model that captures network's spatial topology and signal strength dynamics, with a novel Spatially-Adaptive Conformal Prediction (SACP) method that provides region-specific uncertainty estimates. This allows SAC-GT to produce not only precise two-dimensional (2D) location predictions but also statistically valid confidence regions tailored to varying environmental conditions. Extensive evaluations on a large-scale real-world dataset demonstrate that the proposed SAC-GT solution achieves state-of-the-art localization accuracy while delivering robust and spatially adaptive reliability guarantees.

</details>


### [73] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: 提出 SCOPE，一种面向语言模型路由的可扩展、可控的结果-成本估计框架，通过对新模型的成本与性能进行预测来实现动态路由决策。


<details>
  <summary>Details</summary>
Motivation: 当前路由方法多将任务固定在有限模型集合，难以适应新模型和预算变动，限制了成本优化与性能保障之间的权衡。

Method: 通过强化学习训练的 SCOPE，从检索相似任务的模型行为来进行推理预测，而非仅基于固定模型名称；对每个模型预测成本和准确性，将路由视为动态决策问题，支持对未见模型的推断能力；实现成本-性能的可控取舍。

Result: 实验显示，在追求更高性能时，准确率可提升至多25.7%；在强调效率时，成本可降低至多95.1%。

Conclusion: SCOPE 不只是成本节省工具，能灵活适应用户需求并对新/未见模型进行路由预测，提升可扩展性和可控性。

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [74] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出 AgentScore，一种通过大语言模型提出候选规则、并通过数据驱动的验证与选择循环来学习可部署的单位权重分值检查表，在八个临床预测任务中实现与更灵活的可解释模型相近的AUC，并在两项外部验证任务中优于既有指南分数。


<details>
  <summary>Details</summary>
Motivation: 临床指南需要可解释、易记、可审计且可在床旁执行，但现有机器学习模型往往与部署约束不匹配，导致难以落地；需要在离散规则空间中高效搜索可部署的分值模型。

Method: AgentScore 使用大语言模型提出候选规则，结合一个确定性、数据驱动的验证与选择循环，强制遵循统计有效性和可部署性约束，围绕单位权重、二元规则的求和后阈值化来搜索规则集。

Result: 在八项临床预测任务中，AgentScore 优于现有规则生成方法，AUC 与更灵活的可解释模型相当；在两项外部验证任务中，对比既有指南分数，AgentScore 展现更高的鉴别能力。

Conclusion: 通过将优化与部署约束对齐，并借助语言模型生成规则，AgentScore 能在保持强可解释性的同时实现接近或超越传统可解释模型的预测性能，且对指南驱动的应用具实用潜力。

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [75] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: MAAT combines knowledge-informed kernel state reconstruction with symbolic regression to recover governing equations from noisy/partial data, yielding physically consistent state estimates and derivatives for robust equation discovery.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of existing equation-discovery methods that fail under noise, partial observability, or rely on opaque black-box dynamics; need principled, physics-informed state reconstruction that respects priors.

Method: State reconstruction in a reproducing kernel Hilbert space (RKHS) that incorporates structural and semantic priors (e.g., non-negativity, conservation laws, domain-specific observation models) into the reconstruction objective; handles heterogeneous sampling; produces smooth state estimates with analytic derivatives; serves as a bridge between fragmented sensor data and symbolic regression.

Result: Across twelve diverse scientific benchmarks and varying noise regimes, MAAT reduces state-estimation MSE for both trajectories and derivatives used by downstream symbolic regression compared to strong baselines.

Conclusion: MAAT provides a principled, physics-aware framework to fuse fragmented sensor data with symbolic discovery, enhancing robustness and accuracy of discovering governing equations.

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [76] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: BALANS is a scalable batch-correction method for Cell Painting data that constructs a sparse affinity matrix with batch-aware local scales and adaptive subsampling to achieve near-linear time; it provides theoretical guarantees and shows improved runtime without sacrificing correction quality on large real-world and synthetic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Batch effects in high-content imaging like Cell Painting misalign biological signals across laboratories and instruments. A scalable, theoretically grounded batch correction method is needed to preserve biological structure while handling large datasets.

Method: BALANS builds a sparse affinity matrix A in R^{n×n} by computing A_ij via a Gaussian kernel with a local scale determined by the distance from i to its k-th nearest neighbor within the batch of j. This batch-aware local scaling creates A with meaningful cross-batch affinities. To keep computation tractable, BALANS uses adaptive sampling that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative A. The authors prove the sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and show the overall algorithm runs in nearly linear time in n.

Result: Experiments on diverse real-world Cell Painting datasets and large-scale synthetic benchmarks show BALANS scales to large collections and reduces runtime compared to native implementations of common batch-correction methods, while maintaining or improving correction quality.

Conclusion: BALANS offers a principled, scalable approach to batch alignment for high-content imaging, enabling effective correction on large Cell Painting datasets with favorable runtime and theoretical guarantees.

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [77] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 在 DP-SGD 框架下提出一种新的噪声相关策略：仅与前一轮的噪声相关联并部分抵消，通过伪随机数生成器再生噪声，从而实现不额外内存开销的噪声相关性，实验上优于标准 DP-SGD。


<details>
  <summary>Details</summary>
Motivation: 现有的相关噪声方法（如矩阵分解机制）通过跨多轮相关噪声来提升精度，但需要存储历史噪声，导致显著的内存开销。需要在不增加内存开销的前提下提升 DP-SGD 的准确性。

Method: 提出一种噪声相关策略：仅与上一轮噪声相关并对其进行受控抵消；通过伪随机噪声生成器再生噪声，避免存储过去的噪声；理论上与 DP 保证兼容，且计算开销极小。

Result: 实证结果显示相较于 DP-SGD，在精度上有提升；内存开销保持在标准 DP-SGD 水平，计算开销极小。

Conclusion: 通过与前一轮噪声的受控相关性结合伪随机生成的再生噪声，实现了无额外内存的噪声相关性策略，在准确性上优于 DP-SGD，具备实际可用性。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [78] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 提出并推导了偏好式贝叶斯优化的精确解析知识梯度，解决看 ahead 的非高斯后验问题，在基准上表现优于现有获取函数，同时给出局限性案例。


<details>
  <summary>Details</summary>
Motivation: 在仅能获得成对比较查询的偏好式设置中，扩展知识梯度以实现高效的贝叶斯优化；以及克服看 ahead 时非高斯后验带来的计算挑战。

Method: 推导出偏好式贝叶斯优化的精确解析知识梯度，解决非高斯后验的计算困难；实现了可分析的看 ahead 步骤。

Result: 在一组基准问题上，精确的知识梯度表现优于现有获取函数，具有显著性能提升；并给出一个案例研究，揭示知识梯度在某些情形下的局限性。

Conclusion: 所提出的精确解析知识梯度为偏好式贝叶斯优化提供可行且高效的获取函数，提升了优化效果；同时也指出了在特定情境下需要谨慎使用的局限性。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [79] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 在有限互动预算下评估语言模型在交互环境中的探索能力，发现普遍过度不足的探索与次优解；提出两种轻量干预（预算分并行执行、定期汇总历史），在实验中能改善探索表现。


<details>
  <summary>Details</summary>
Motivation: 揭示当前SOTA模型在受限预算下的探索能力不足，寻找成本低廉、可扩展的改进策略。

Method: 引入三组具有可控探索难度的参数化任务，覆盖连续与离散环境，比较SOTA模型与简单探索-利用基线；评估两种干预：将固定预算分成并行执行，以及定期汇总互动历史。

Result: 所有模型普遍存在探索不足，性能常显著劣于探索-利用基线，预算增加对性能提升的作用有限；预算分拆的并行执行在理论没有增益的前提下仍提升表现；定期汇总历史有助于保留关键发现并进一步提升探索。

Conclusion: 探索仍具挑战，简单干预能显著提升效果，提示并行性与信息汇总对探索效率的重要性，未来需深入理解机制并优化干预策略。

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [80] [MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization](https://arxiv.org/abs/2601.22347)
*Sai Sanjeet,Ian Colbert,Pablo Monteagudo-Lago,Giuseppe Franco,Yaman Umuroglu,Nicholas J. Fraser*

Main category: cs.LG

TL;DR: The paper analyzes block Hadamard rotations in post-training quantization (PTQ), reveals that outlier suppression is constrained by input geometry, and introduces MixQuant, a permutation-guided, block-structure-aware PTQ framework. It uses a greedy mass-diffusion algorithm to equalize blockwise ℓ1 norms, merges permutations into weights to avoid inference overhead, and shows substantial accuracy gains (up to ~90% recovery of full-vector rotation perplexity) on Llama3-1B–INT4 with block size 16.


<details>
  <summary>Details</summary>
Motivation: Understand how block structure affects outlier suppression in PTQ with block rotations and develop a practical mass-distribution method to improve quantization accuracy without increasing at-inference cost.

Method: Non-asymptotic analysis of outlier suppression for block Hadamard rotations. Introduce MixQuant: a permutation-driven mass-diffusion algorithm to equalize expected blockwise ℓ1 norms. Identify permutation-equivariant regions in transformers to merge permutations into weights for deployment.

Result: Outlier suppression is fundamentally limited by the input geometry; optimal pre-rotation ℓ1 mass distribution occurs when mass is evenly spread across blocks. MixQuant consistently improves accuracy for all block sizes and recovers up to 90% of the full-vector rotation perplexity (e.g., Llama3-1B quantized to INT4 with block size 16) compared to about 46% without permutations.

Conclusion: Block-structure-aware PTQ with permutation-based mass diffusion can substantially reduce rotation-induced perplexity and improve accuracy while avoiding inference overhead by merging permutations into model weights; the geometry of input vectors governs the effectiveness of outlier suppression.

Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

</details>


### [81] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一种基于集合表示的策略表示方法，将策略视为在占据度量下的状态-行动特征映射的期望，从而实现对多策略的统一编码，并在潜在空间中进行可微优化以实现无额外训练的行为约束满足。


<details>
  <summary>Details</summary>
Motivation: 在MDP中，对一组策略进行一致、可操作的表征，以便在测试时对行为进行 steering，并能在多奖励设定下对策略及其值函数进行高效操作。现有方法缺乏统一的、可微的潜在空间来直接在策略层面进行优化。

Method: 将策略表示为对状态-动作特征的占据度量期望；利用集合型架构对一组策略进行统一近似。将一组状态-动作样本编码成潜在嵌入，解码出对应于多种奖励的策略及其值函数。采用变分自编码来获得平滑的潜在空间，并通过对比学习对其进行塑形，使潜在距离与值函数的差异对齐。该几何结构使得能够在潜在空间中进行梯度优化。

Result: 构建了一个可在潜在空间直接优化的策略表示框架，并演示在一个新颖的行为合成任务中可使策略符合未见过的值函数约束，而无需额外训练。

Conclusion: 该方法为测试时对策略进行灵活引导提供了统一、可微的表示与优化路径；其潜在局限包括对占据度量近似的依赖、集合编码的可扩展性以及在高维状态-动作空间上的标定与鲁棒性，需要在更多任务和尺度上进行评估。

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [82] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 提出在最优传输框架下量化经验分布偏离高斯性的几何量，以相对 Wasserstein 角度和正交投影距离刻画非高斯性，并给出一维闭式解及高维优化算法，表明最近高斯分布并非简单的矩匹配所得到的高斯分布。


<details>
  <summary>Details</summary>
Motivation: 在统计建模和生成模型评估中，常用高斯近似，但在 Wasserstein 距离下的最近高斯分布与矩匹配的常规近似不一致，需一个几何、能够直接量化非高斯性并获得近似最优高斯分布。

Method: 利用相对平移不变的二次 Wasserstein 空间的圆锥几何，提出相对 Wasserstein 角和正交投影距离两种量度；证明从两条射线生成的充满圆锥是平坦的；将高斯近似重写为投影到高斯圆锥的投影问题；在一维给出闭式解，扩展到均匀、拉普拉斯、逻辑分布等；在高维给出基于半离散对偶的随机流形优化算法。

Result: 理论上证明了几何结构的良定性，提供了对非高斯性的鲁棒性指标；一维有闭式公式，实验显示相对 Wasserstein 角比 Wasserstein 距离更鲁棒；相较于矩匹配，最近高斯在评估 Fréchet Inception Distance（FID）时提供更好近似。

Conclusion: 将高斯近似视为在高斯圆锥的投影，提出的度量与算法可用于更稳健的高斯近似及非高斯性量化，提升如 FID 等分布评估的准确性。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [83] [PoSafeNet: Safe Learning with Poset-Structured Neural Nets](https://arxiv.org/abs/2601.22356)
*Kiwan Wong,Wei Xiao,Daniela Rus*

Main category: cs.LG

TL;DR: 提出 poset 结构的安全约束，并设计 PoSafeNet 以逐步闭式投影实现可微分的安全层，在符合优先关系的前提下自适应选择/混合安全执行，提升可行性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法常对安全约束采用统一优先级或固定顺序，导致可行性受限、对变化场景脆弱。现实中的安全需求具有异质性，存在部分可比且并非全部可比的关系。

Method: 将安全约束建模为偏序集（poset），在策略家族中视为结构性属性。提出 PoSafeNet，一种可微分的安全层，通过在 poset 一致的约束排序下进行序列化的闭式投影实现安全性，能够自适应选择或混合有效的安全执行且按结构保持优先语义。

Result: 在多障碍物导航、约束机器人操作和基于视觉的自动驾驶等任务中，相较于无结构化方法和基于二次规划的安全层，PoSafeNet 提高了可行性、鲁棒性与可扩展性。

Conclusion: 将安全约束以 poset 表示并通过可微分的投影实现，能够在存在部分可比约束的场景中提供灵活且稳健的安全性控制，适用于复杂的多约束系统。

Abstract: Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.

</details>


### [84] [Small Talk, Big Impact: The Energy Cost of Thanking AI](https://arxiv.org/abs/2601.22357)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: Politeness cues like 'thank you' incur measurable energy costs in LLM interactions; energy scales with input length, output length, and model size; real traces and precise measurements are used to guide sustainable AI deployment.


<details>
  <summary>Details</summary>
Motivation: Politeness is a common user behavior in LLM chats; understanding its energy cost informs the sustainability and efficiency of large-scale LLM deployments.

Method: Analyze real-world conversation traces with fine-grained energy measurements; systematically vary input length, output length, and model size to quantify energy consumption; treat politeness as a proxy for typical LLM interaction energy footprint.

Result: Energy consumption increases with input length, output length, and model size; politeness cues can serve as a controllable proxy for the energy footprint of typical LLM interactions; findings offer actionable guidance for building more sustainable chat AI applications.

Conclusion: As adoption of LLMs grows, energy-aware design is essential for sustainable AI deployment. Politeness-related messaging provides a tractable proxy for assessing and mitigating the energy cost of large-scale interactions.

Abstract: Being polite is free - or is it? In this paper, we quantify the energy cost of seemingly innocuous messages such as ``thank you'' when interacting with large language models, often used by users to convey politeness. Using real-world conversation traces and fine-grained energy measurements, we quantify how input length, output length and model size affect energy use. While politeness is our motivating example, it also serves as a controlled and reproducible proxy for measuring the energy footprint of a typical LLM interaction. Our findings provide actionable insights for building more sustainable and efficient LLM applications, especially in increasingly widespread real-world contexts like chat. As user adoption grows and billions of prompts are processed daily, understanding and mitigating this cost becomes crucial - not just for efficiency, but for sustainable AI deployment.

</details>


### [85] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 系统级设计对LLM推理能耗影响显著，需进行阶段感知的能耗分析与服务栈优化。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究关注单次提示或逐-token能耗，真实部署中的能耗高度由硬件、并发、调度和服务配置等系统因素决定。

Method: 在NVIDIA H100 GPU上进行实证研究，考察量化、批处理大小与服务端配置（如 Hugging Face Text Generation Inference 服务器）的影响，测量能耗与延迟，并分析计算瓶颈与内存瓶颈下的行为。

Result: 低精度仅在计算瓶颈时带来能耗收益；在解码等内存瓶颈阶段，批处理可显著提升能效；结构化请求时序（arrival shaping）可将单请求能耗降低多达约100倍；系统层级设计对能源效率具有关键作用。

Conclusion: 可持续的LLM部署需关注服务栈编排与能耗分析，推动阶段感知的能耗分析与系统级优化。

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [86] [Purely Agentic Black-Box Optimization for Biological Design](https://arxiv.org/abs/2601.22382)
*Natalie Maus,Yimeng Zeng,Haydn Thomas Jones,Yining Huang,Gaurav Ng Goel,Alden Rose,Kyurae Kim,Hyun-Su Lee,Marcelo Der Torossian Torres,Fangping Wan,Cesar de la Fuente-Nunez,Mark Yatskar,Osbert Bastani,Jacob R. Gardner*

Main category: cs.LG

TL;DR: PABLO uses a hierarchical agentic LLM framework to perform fully language-based black-box optimization for biology, achieving state-of-the-art results on GuacaMol molecular design and antimicrobial peptide optimization, with improved sample efficiency and in vitro validation. It leverages retrieval-augmented knowledge and semantic task descriptions, offering practical advantages for constraint handling and domain knowledge integration.


<details>
  <summary>Details</summary>
Motivation: Biological design problems sit in vast, structured search spaces where traditional structure-centric optimization underutilizes the scientific literature. Existing LLM integrations are narrow and do not exploit literature-driven reasoning across the entire optimization loop. A fully agentic, language-based optimizer could better leverage domain knowledge, constraints, and descriptions to guide search.

Method: A hierarchical agentic system (PABLO) that uses science-language-models pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. The system incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints within an agentic loop that reason about and propose candidates, then refresh with feedback.

Result: On standard benchmarks (GuacaMol molecular design and antimicrobial peptide optimization), PABLO achieves state-of-the-art performance, with substantially improved sample efficiency and final objective values compared to baselines. It maintains competitive token usage per run relative to prior methods that use LLMs throughout optimization. In vitro validation shows PABLO-optimized peptides with strong activity against drug-resistant pathogens.

Conclusion: The agentic, language-based formulation is advantageous for realistic design, enabling natural incorporation of task descriptions, external knowledge retrieval, and complex constraints, and demonstrates practical potential for therapeutic discovery via in vitro validation.

Abstract: Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.

</details>


### [87] [Graph is a Substrate Across Data Modalities](https://arxiv.org/abs/2601.22384)
*Ziming Li,Xiaoming Wu,Zehong Wang,Jiazheng Li,Yijun Tian,Jinhe Bi,Yunpu Ma,Yanfang Ye,Chuxu Zhang*

Main category: cs.LG

TL;DR: 提出 G-Substrate，将图结构作为持续的底层表示以跨模态、跨任务共享与积累。通过统一结构模式与互嵌的角色驱动训练实现兼容性与多角色学习，实验显示优于任务孤立与简单多任务方法。


<details>
  <summary>Details</summary>
Motivation: 解决图结构在模态和任务上独立学习的局限性；希望有一个持久、跨领域可积累的结构底板来捕捉跨模态的结构规律。

Method: 提出 G-Substrate，包含两大机制：1) 统一的结构模式(schema)以确保不同模态与任务的图表示之间的兼容性；2) 互嵌的角色化训练策略，使同一图结构在学习中被暴露给多种功能角色。

Result: 在多领域、多模态和多任务的实验中，G-Substrate 相较于任务孤立和天真多任务学习方法具有更好的性能。

Conclusion: 将图结构视为可跨模态、跨任务反复使用的持久底板，能更好地聚合结构规律并提升跨域学习效能。

Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafter. As a result, structural regularities across modalities and tasks are repeatedly reconstructed rather than accumulated at the level of intermediate graph representations. This motivates a representation-learning question: how should graph structure be organized so that it can persist and accumulate across heterogeneous modalities and tasks? We adopt a representation-centric perspective in which graph structure is treated as a structural substrate that persists across learning contexts. To instantiate this perspective, we propose G-Substrate, a graph substrate framework that organizes learning around shared graph structures. G-Substrate comprises two complementary mechanisms: a unified structural schema that ensures compatibility among graph representations across heterogeneous modalities and tasks, and an interleaved role-based training strategy that exposes the same graph structure to multiple functional roles during learning. Experiments across multiple domains, modalities, and tasks show that G-Substrate outperforms task-isolated and naive multi-task learning methods.

</details>


### [88] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR 提出以大语言模型作为在线强化学习控制器的自适应扩缩框架，用以提升多阶段ML推断管道的P99延迟与资源利用率，且无需离线训练。


<details>
  <summary>Details</summary>
Motivation: 解决多阶段ML推理管道的自适应扩展难题，尤其在资源异构、跨阶段耦合和动态瓶颈迁移场景中的挑战。

Method: 将Pareto-dominance奖励形状、可证明的分离边界、基于 surprisal 的经验回放检索，以及通过用户态CUDA拦截实现的粒度级GPU速率控制结合在一起，构成SAIR的在线自适应控制。对误差进行 regret/分析，分解为检索覆盖与LLM选择两部分。

Result: 在四条ML服务管线、三种工作负载模式下，SAIR在P99延迟方面达到最佳或并列第一，且相比基线在P99上提升多达50%，在GPU速率控制假设下有效成本降低最高可达97%，瓶颈检测准确率达到86%，且无需离线训练。

Conclusion: 展示了将LLM作为在线RL控制器用于自适应扩缩的可行性与有效性，提供理论误差分解与实验证据，且实现无离线训练仍可获得显著性能提升。

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [89] [Score-based Integrated Gradient for Root Cause Explanations of Outliers](https://arxiv.org/abs/2601.22399)
*Phuoc Nguyen,Truyen Tran,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: SIREN 提出一种可扩展的异常根因归因方法：通过对数据似然的分数函数估计，并以积分梯度在异常点到正常分布之间的路径上累积分数贡献，从而实现高维非线性数据中的稳健归因，且在 Shapley 公理上取得部分满足。


<details>
  <summary>Details</summary>
Motivation: 在不确定性和高维依赖下定位异常根因的任务困难，现有基于启发式或反事实的方法在可扩展性和不确定性处理方面受限，需提出一种可对不确定性敏感且可扩展的根因归因框架。

Method: 对数据的似然分数函数进行估计，并通过积分梯度（Integrated Gradients）沿从异常点到正常数据分布的路径累积分数贡献。方法直接作用于分数函数，适用于非线性、高维、异方差的因果模型，具有可扩展性和不确定性感知能力。

Result: 在合成随机图以及真实的云服务和供应链数据集上，SIREN 在归因准确性和计算效率方面均优于现有最优基线。

Conclusion: SIREN 使在复杂数据中的根因归因变得可落地，具备对不确定性敏感的属性。其在三项 Shapley 公理（dummy、efficiency、linearity）以及来自因果结构的非对称性公理方面得到满足，显示出良好的理论基础和实践可扩展性。

Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.

</details>


### [90] [Optimization, Generalization and Differential Privacy Bounds for Gradient Descent on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.22409)
*Puyu Wang,Junyu Zhou,Philipp Liznerski,Marius Kloft*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Kolmogorov--Arnold Networks (KANs) have recently emerged as a structured alternative to standard MLPs, yet a principled theory for their training dynamics, generalization, and privacy properties remains limited. In this paper, we analyze gradient descent (GD) for training two-layer KANs and derive general bounds that characterize their training dynamics, generalization, and utility under differential privacy (DP). As a concrete instantiation, we specialize our analysis to logistic loss under an NTK-separable assumption, where we show that polylogarithmic network width suffices for GD to achieve an optimization rate of order $1/T$ and a generalization rate of order $1/n$, with $T$ denoting the number of GD iterations and $n$ the sample size. In the private setting, we characterize the noise required for $(ε,δ)$-DP and obtain a utility bound of order $\sqrt{d}/(nε)$ (with $d$ the input dimension), matching the classical lower bound for general convex Lipschitz problems. Our results imply that polylogarithmic width is not only sufficient but also necessary under differential privacy, revealing a qualitative gap between non-private (sufficiency only) and private (necessity also emerges) training regimes. Experiments further illustrate how these theoretical insights can guide practical choices, including network width selection and early stopping.

</details>


### [91] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出 MM-OpenFGL，首个系统化的多模态联邦图学习基准，覆盖19个数据集、7个领域、8种模拟策略、6个下游任务、57种方法，提供模块化API以系统评估MMFGL。


<details>
  <summary>Details</summary>
Motivation: 解决现有联邦图学习多聚焦单模态图的局限，缺乏面向多模态、隐私保护场景的统一评估基准；需要在模态异构、拓扑多样性和跨平台协同条件下进行公平、可重复的比较研究。

Method: 设计模块化、可重复的基准平台MM-OpenFGL；整合19个数据集、7个应用领域、8种模态与拓扑变异的仿真策略、6个下游任务以及57种方法，提供统一评测流程与API以评估必要性、有效性、鲁棒性与效率。

Result: 基准实现了对MMFGL的系统性评估，揭示不同方法在模态异构、数据隐私约束以及拓扑分布下的优劣，为未来研究提供可复现的实验数据、评价指标和对比基准。

Conclusion: MM-OpenFGL填补MMFGL研究的标准化与对比空缺，推动多模态联邦图学习的发展与应用，促进方法在现实场景中的可移植性与可扩展性。

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [92] [MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments](https://arxiv.org/abs/2601.22420)
*Roelien C. Timmer,Necva Bölücü,Stephen Wan*

Main category: cs.LG

TL;DR: MetaLead 是一个完全人工标注的 ML Leaderboard 数据集，记录论文中的全部实验结果并附带扩展元数据，如实验类型（基线、提出的方法、提出方法的变体）和训练/测试数据分离，用于跨领域评估，提升透明度和可比性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集通常只收录论文的最佳结果且元数据有限，无法提供结果透明度和细粒度比较；需要一个全面、可重复、跨域评估友好的数据集。

Method: 对论文中的所有实验结果进行人工标注与整理，构建结构化数据集；添加结果类型标签（baseline、proposed method、variation of proposed method）；显式区分训练集与测试集以便跨域评估。

Result: 提供一个包含全部实验结果及丰富元数据的 Leaderboard 数据集，提升透明度、可重复性和跨领域比较能力。

Conclusion: MetaLead 将推动更透明、细粒度的 leaderboard 构建与评估，并为后续的自动化和跨域研究奠定数据基础。

Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research.

</details>


### [93] [CoDCL: Counterfactual Data Augmentation Contrastive Learning for Continuous-Time Dynamic Network Link Prediction](https://arxiv.org/abs/2601.22427)
*Hantong Feng,Yonggang Wu,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: A plug-and-play framework CoDCL combining counterfactual data augmentation and contrastive learning to improve robustness of dynamic graph representations under evolving temporal structures; achieves state-of-the-art results on real-world dynamic networks.


<details>
  <summary>Details</summary>
Motivation: Dynamic networks evolve over time and undergo structural changes that degrade predictive performance. Existing methods struggle to adapt due to limited robustness to emerging patterns; counterfactual data augmentation and contrastive learning offer a principled way to expose models to diverse temporal scenarios.

Method: Propose CoDCL, a plug-and-play module that integrates counterfactual data augmentation with contrastive learning. High-quality counterfactual data are generated via a dynamic treatments design and efficient structural neighborhood exploration to quantify temporal changes in interaction patterns. Can be embedded into existing temporal graph models without architectural changes.

Result: Extensive experiments on multiple real-world datasets show substantial improvements over strong baselines, confirming the effectiveness of incorporating counterfactual data augmentation into dynamic representation learning and achieving state-of-the-art performance.

Conclusion: CoDCL provides a universal, easily integrable approach to enhance dynamic network prediction by leveraging counterfactual augmentation and contrastive learning, underscoring the value of modeling temporal structural changes for robust dynamic representations.

Abstract: The rapid growth and continuous structural evolution of dynamic networks make effective predictions increasingly challenging. To enable prediction models to adapt to complex temporal environments, they need to be robust to emerging structural changes. We propose a dynamic network learning framework CoDCL, which combines counterfactual data augmentation with contrastive learning to address this deficiency.Furthermore, we devise a comprehensive strategy to generate high-quality counterfactual data, combining a dynamic treatments design with efficient structural neighborhood exploration to quantify the temporal changes in interaction patterns.Crucially, the entire CoDCL is designed as a plug-and-play universal module that can be seamlessly integrated into various existing temporal graph models without requiring architectural modifications.Extensive experiments on multiple real-world datasets demonstrate that CoDCL significantly gains state-of-the-art baseline models in the field of dynamic networks, confirming the critical role of integrating counterfactual data augmentation into dynamic representation learning.

</details>


### [94] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 提出一种显式对比学习的方法来增强LLM推理能力，取代GRPO的优势估计，通过将K个结果分为正负集，最大化正结果的似然。该方法作为在线的多标签噪声对比估计的实现，在数学基准上对比DAPO等强 baselines 取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: GRPO 的改进（如不对称裁剪、零方差数据筛选）在实践中依赖大量经验性洞察，且识别困难；需要一种更直接、可泛化的推理训练信号来提升LLM推理性能。

Method: 将K个候选结果划分为正集合与负集合，最大化正结果的似然，等价于在线的多标签噪声对比估计的实现，取代对优势的估计并以对比学习形式优化推理行为。

Result: 在一组具有挑战性的数学基准测试中，与强基线（如DAPO、online DPO）相比，展示出具有竞争力的性能。

Conclusion: 显式对比学习为LLM推理提供了一种有效且直接的替代范式，减少对手工经验引导的依赖，同时具备良好的实证性能。

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [95] [Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance](https://arxiv.org/abs/2601.22443)
*Jing Jia,Wei Yuan,Sifan Liu,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 弱扩散先验在信息度量充足时对逆问题具有鲁棒性，甚至与在-domain先验接近；并给出基于贝叶斯一致性的理论条件以解释何时能收敛到真实信号。


<details>
  <summary>Details</summary>
Motivation: 理解在先验分布不完全匹配或低保真的情况下，扩散模型作为逆问题先验的鲁棒性及局限性，尤其在高维测量下的表现。

Method: 通过大规模实验比较不同匹配度的扩散先验在多种逆问题上的重建效果，并结合贝叶斯一致性理论推导，给出后验收敛到真实信号的条件。

Result: 发现当测量信息量高（如观测像素多）时，弱先验仍能达到接近全强域内基线的重建效果；同时识别出在信息不足时的失效区域；给出高维测量下后验收敛的理论框架。

Conclusion: 为在弱扩散先验下的逆问题求解提供理论支撑与实践指引，说明何时可以使用弱先验并仍获得可靠结果。

Abstract: Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.

</details>


### [96] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 通过直接权重交互的框架，对稀疏自编码器中的特征进行功能性解释，发现约四分之一特征直接预测输出标记，特征在注意力机制中有深度相关结构的参与，并且语义与非语义特征在注意力回路的分布上呈现显著差异；填补了基于激活的 SAE 特征解释的缺失部分。


<details>
  <summary>Details</summary>
Motivation: 现有的基于激活模式的解释依赖假设特征只是被动再现激活，忽略特征在前向计算中的真正风格功能。需要一种不依赖激活数据、直接通过权重交互来衡量特征功能影响的新框架。

Method: 提出一种基于权重的解释框架，通过直接的权重交互来衡量功能性效应；对 Gemma-2 和 Llama-3.1 模型中的稀疏自编码器进行三项实验，分析特征对输出标记的直接预测、在注意力机制中的参与、以及在注意力回路中的分布差异。

Result: (1)约1/4的特征可直接预测输出标记；(2)特征在注意力机制中积极参与，且呈现深度相关的结构；(3)语义与非语义特征群体在注意力回路中的分布特征存在显著差异。

Conclusion: 该权重基解释框架填补了 SAE 特征解释中的“上下文外”的部分，避免对激活数据的依赖，揭示了特征在前向计算中的真实功能作用。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [97] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: HeaPA 提出了一种自适应、有序的提示池管理方法，通过堆排序边界采样和 on-policy 增强实现前沿导向的查询选择与池的动态扩展，显著提升 RLVR 训练的准确性与效率，在两类数据集、两种训练策略和七个基准上表现稳定，且随模型规模增大收益更明显。


<details>
  <summary>Details</summary>
Motivation: RLVR 逐步成为具备可验证输出的推理训练标准，但若 rollout 成本居高不下，效率高度依赖提示采样策略。固定或缓慢演化的提示池难以跟上模型学习进展，导致对已解决或尚不可及的问题的轮换浪费。现有方法要么假设固定池、要么引入额外教师成本，难以实现稳定的 on-policy 池扩展与高效性。

Method: 提出 HeaPA：进行有界、演化的提示池管理；通过堆结构的边界采样来追踪前沿；使用轻量异步验证的 on-policy 增强来扩展池；通过拓扑感知的池统计重新估计与受控再插入来稳定相关查询；在两类训练语料、两类训练方案和七项基准上评估。

Result: 在多组设置下，HeaPA 显著提升准确性、以更少的计算实现目标性能，且墙钟时间保持在可比水平；分析表明收益来自前沿聚焦采样与 on-policy 池扩展，且模型规模越大，收益越显著。

Conclusion:  frontier-focused sampling 与动态的 on-policy 池增长是主要驱动，HeaPA 能在保持效率的同时提升性能，且具较强的可扩展性。

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [98] [Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity](https://arxiv.org/abs/2601.22450)
*Jianhao Huang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: MD objective decomposes into signal-driven feature learning and noise-based implicit regularization; by optimizing the mask distribution, the approach enables rapid, non-grokking generalization on k-parity and yields significant perplexity gains for both small and large masked diffusion LMs.


<details>
  <summary>Details</summary>
Motivation: Investigate generalization differences between Masked Diffusion LMs and autoregressive models, focusing on the grokking phenomenon in the k-parity task and whether an MD-based objective can alter the learning dynamics.

Method: Theoretically decompose the MD objective into a Signal regime and a Noise regime. Train nanoGPT with the MD objective on the k-parity problem. Systematically optimize the mask probability distribution. Evaluate on pretraining-from-scratch and supervised fine-tuning, reporting perplexity gains across model scales.

Result: MD objective fundamentally reshapes the learning landscape, enabling rapid, simultaneous generalization without grokking. Optimizing mask distribution further improves performance. Perplexity improvements observed for 50M-parameter models; gains peak at 8.8% and 5.8% on 8B-parameter models, demonstrating scalability to large-scale masked diffusion LMs.

Conclusion: Masked diffusion objectives, via the separation into signal learning and noise regularization and optimized masking, provide a scalable and effective framework for improving generalization and perplexity across model sizes and training regimes.

Abstract: Masked Diffusion Language Models have recently emerged as a powerful generative paradigm, yet their generalization properties remain understudied compared to their auto-regressive counterparts. In this work, we investigate these properties within the setting of the $k$-parity problem (computing the XOR sum of $k$ relevant bits), where neural networks typically exhibit grokking -- a prolonged plateau of chance-level performance followed by sudden generalization. We theoretically decompose the Masked Diffusion (MD) objective into a Signal regime which drives feature learning, and a Noise regime which serves as an implicit regularizer. By training nanoGPT using MD objective on the $k$-parity problem, we demonstrate that MD objective fundamentally alters the learning landscape, enabling rapid and simultaneous generalization without experiencing grokking. Furthermore, we leverage our theoretical insights to optimize the distribution of the mask probability in the MD objective. Our method significantly improves perplexity for 50M-parameter models and achieves superior results across both pre-training from scratch and supervised fine-tuning. Specifically, we observe performance gains peaking at $8.8\%$ and $5.8\%$, respectively, on 8B-parameter models, confirming the scalability and effectiveness of our framework in large-scale masked diffusion language model regimes.

</details>


### [99] [Machine Unlearning in Low-Dimensional Feature Subspace](https://arxiv.org/abs/2601.22456)
*Kun Fang,Qinghua Tao,Junxu Liu,Yaxin Xiao,Qingqing Ye,Jian Sun,Haibo Hu*

Main category: cs.LG

TL;DR: 提出 LOFT：一种基于低维特征子空间的机器卸载方法，通过在预训练模型中插入一个小的投影矩阵并利用主成分投影，最大化剩余数据的信息、同时削弱遗忘数据的信息，以实现高效的一次性特征获取卸载，显著降低计算开销并提升卸载效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器卸载方法对大量数据重新加载的隐私风险和对整个预训练模型更新的低效性，同时提升卸载过程的计算与存储效率。

Method: 在低维特征子空间通过主成分投影实现信息分离；优化一个小型投影矩阵并灵活嵌入到预训练模型中，通过一次获取自 backbone 的特征来完成卸载，无需重复访问原始数据。投影矩阵被设计为最大化剩余数据的信息量、同时抑制遗忘数据的信息量。

Result: 大量实验表明，LOFT 在不同模型、数据集、任务和应用场景中具备显著更低的计算开销和更优的卸载效果。公开代码。

Conclusion: LOFT 提供了一种高效且可扩展的机器卸载方案，降低隐私泄露风险并简化模型更新流程，适用于大规模场景。

Abstract: Machine Unlearning (MU) aims at removing the influence of specific data from a pretrained model while preserving performance on the remaining data. In this work, a novel perspective for MU is presented upon low-dimensional feature subspaces, which gives rise to the potentials of separating the remaining and forgetting data herein. This separability motivates our LOFT, a method that proceeds unlearning in a LOw-dimensional FeaTure subspace from the pretrained model skithrough principal projections, which are optimized to maximally capture the information of the remaining data and meanwhile diminish that of the forgetting data. In training, LOFT simply optimizes a small-size projection matrix flexibly plugged into the pretrained model, and only requires one-shot feature fetching from the pretrained backbone instead of repetitively accessing the raw data. Hence, LOFT mitigates two critical issues in mainstream MU methods, i.e., the privacy leakage risk from massive data reload and the inefficiency of updates to the entire pretrained model. Extensive experiments validate the significantly lower computational overhead and superior unlearning performance of LOFT across diverse models, datasets, tasks, and applications. Code is anonymously available at https://anonymous.4open.science/r/4352/.

</details>


### [100] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: 提出 EvoEGF-Mol：基于信息几何的异常流解耦SBDD框架，将分子建模为复合指数族分布，在费舍尔-罗氏度量下沿指数测地线生成。通过引入动态目标分布替代 Dirac 点实现渐进参数细化训练稳定性，在 CrossDock 达到 93.4% 的 PoseBusters 通过率，并在 MolGenBench 上超越基线，恢复生物活性骨架并生成符合 MedChem 约束的候选物。


<details>
  <summary>Details</summary>
Motivation: 现有 SBDD 将连续原子坐标的欧几里得空间与离散化学类别的概率空间分别建模，两者与潜在统计流形存在不匹配。需要在信息几何框架下统一表达分子分布并沿着费舍尔-罗氏度量的指数测地线进行生成，以提升几何精度与交互保真度。Dirac 分布的直接目标会导致训练中的瞬时轨迹塌缩，因此需要更平滑的目标来实现稳定优化。

Method: 将分子建模为复合指数族分布，并在费舍尔-罗氏度量下沿着指数测地线定义生成流；以渐进参数细化的架构代替固定的 Dirac 目标，提出动态收敛的分布作为目标以避免训练崩溃，形成 EvoEGF-Mol 框架，结合渐进式目标更新实现稳定收敛。

Result: 在 CrossDock 上达到参考水平的 PoseBusters 通过率(93.4%)，体现出显著的几何精度和作用力互动保真度；在实际 MolGenBench 任务上超越基线，能够恢复生物活性骨架并生成符合既定 MedChem 过滤标准的候选分子。

Conclusion: 证明信息几何视角在 SBDD 中的可行性与优势：使用复合指数分布与费舍尔-罗氏几何的生成框架可实现稳定训练与高质量分子设计，提升对接姿态的几何精度、骨架回收以及药效学约束的合规性，具有应用潜力与进一步改进空间。

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [101] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: LLMs在未奖励阶段显示潜在学习动态，先进行无奖励探索帮助组织知识，随后引入奖励后性能提升；两阶段训练普遍优于持续的奖励学习，且有理论机制解释。跨模型与多任务域证实 latent learning 在LLMs中的存在。


<details>
  <summary>Details</summary>
Motivation: 探究心理学中的潜在学习是否会在现代LLMs的训练中出现，以及它对训练策略与泛化的影响，与以奖励为中心的强化学习进行对比，寻找更灵活的学习信号。

Method: 在多家模型家族和多任务领域开展大规模实验，采用两阶段训练：先进行未奖励探索再引入奖励，比较与纯奖励学习的差异，辅以理论分析以揭示机制。

Result: 未奖励探索阶段带来性能提升，奖励阶段再度提升，最终两阶段模型在多数任务上优于单纯以奖励为导向的训练；给出未奖励阶段带来改进的可解释机制。

Conclusion: 证实LLMs中存在 latent learning 动态，两阶段探索训练能提升能力，推动对训练信号设计与泛化能力的理解。

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [102] [TTCS: Test-Time Curriculum Synthesis for Self-Evolving](https://arxiv.org/abs/2601.22628)
*Chengyi Yang,Zhishang Xiang,Yunbo Tang,Zongpei Teng,Chengsong Huang,Fei Long,Yuhan Liu,Jinsong Su*

Main category: cs.LG

TL;DR: TTCS通过共进化的问句合成器与推理求解器，在测试时对模型进行自适应训练，逐步生成难度递增的问句变体并以自我一致性奖励更新，显著提升复杂推理能力并具跨域迁移。


<details>
  <summary>Details</summary>
Motivation: 现有 test-time training 面临两大挑战：原始测试题难以产生高质量伪标注；测试集规模小导致在线更新不稳定。

Method: 初始化两条策略（问句合成器、推理求解器）并从同一 pretrained 模型出发，通过迭代优化共进化：合成器在测试问题上条件化生成逐步具有挑战性的题目变体，形成针对求解器能力的结构化课程；求解器利用对原始测试题和合成题的多样化回答的自一致性奖励进行自训练；求解器的反馈引导合成器生成与当前能力相对齐的题目，进而稳定 TT 训练。

Result: 在具有挑战性的数学基准上显著提升推理能力，并实现对通用领域任务的跨 backbones 迁移，证明了通过自我演化动态构建测试时课程的可扩展性。

Conclusion: TTCS 提供一种可扩展的自我演化测试时课程框架，通过共进化的题目合成与推理求解，使测试时训练更稳健、推理能力提升并具跨域适应性。

Abstract: Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.

</details>


### [103] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: Teacher-student continual RL: train single-task RL teachers in parallel and continually distill into a central generalist; use Mixture-of-Experts and replay to balance plasticity and stability; achieves 85% of teacher performance with <10% forgetting on Meta-World.


<details>
  <summary>Details</summary>
Motivation: CRL faces the stability-plasticity dilemma and difficulty scaling RL to sequential tasks. The gap is leveraging single-task RL strength while maintaining multi-task generalization via distillation.

Method: Decoupled framework: (1) train multiple single-task teacher models with distributed RL; (2) continually distill these teachers into a central generalist model. Employ a Mixture-of-Experts (MoE) architecture and a replay-based approach to enhance plasticity and stability during continual policy distillation.

Result: On Meta-World, the framework recovers over 85% of teacher performance and keeps task-wise forgetting within 10%.

Conclusion: Decoupling CRL into independent single-task RL training and continual distillation into a central model, aided by MoE and replay, yields scalable and efficient continual RL with strong performance and limited forgetting.

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [104] [A Unified Study of LoRA Variants: Taxonomy, Review, Codebase, and Empirical Evaluation](https://arxiv.org/abs/2601.22708)
*Haonan He,Jingqi Ye,Minglei Li,Zhengbo Wang,Tao Chen,Lei Bai,Peng Ye*

Main category: cs.LG

TL;DR: 统一研究LoRA变体：提出四维分类、统一理论框架、模块化代码库LoRAFactory，并在NLP/NLG/NLU和图像分类上进行大规模评估，结果表明学习率对LoRA显著敏感，且在合适超参数下LoRA可达到或超越多数变体。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA变体泛滥导致方法论、理论、代码和评估碎片化，需要一个统一框架以实现可比性和可重复性。

Method: 提出四个轴的分类（秩、优化动力学、初始化、与Mixture-of-Experts的整合），在共同的低秩更新动力学理论框架下回顾关系；发布LoRAFactory模块化代码库，提供统一接口实现变体，支持插拔式实验和细粒度分析；基于该代码库在NLG、NLU和图像分类任务进行大规模实验，系统地探索关键超参数。

Result: 发现LoRA及其变体对学习率等超参数极为敏感；在合适的超参数配置下，LoRA可稳定达到或超过大多数变体的性能。

Conclusion: 本文提供了一个统一的LoRA变体研究平台，促进可重复性与公平比较，并强调超参数调优的重要性；LoRAFactory等工具将加速后续研究与应用。

Abstract: Low-Rank Adaptation (LoRA) is a fundamental parameter-efficient fine-tuning method that balances efficiency and performance in large-scale neural networks. However, the proliferation of LoRA variants has led to fragmentation in methodology, theory, code, and evaluation. To this end, this work presents the first unified study of LoRA variants, offering a systematic taxonomy, unified theoretical review, structured codebase, and standardized empirical assessment. First, we categorize LoRA variants along four principal axes: rank, optimization dynamics, initialization, and integration with Mixture-of-Experts. Then, we review their relationships and evolution within a common theoretical framework focused on low-rank update dynamics. Further, we introduce LoRAFactory, a modular codebase that implements variants through a unified interface, supporting plug-and-play experimentation and fine-grained analysis. Last, using this codebase, we conduct a large-scale evaluation across natural language generation, natural language understanding, and image classification tasks, systematically exploring key hyperparameters. Our results uncover several findings, notably: LoRA and its variants exhibit pronounced sensitivity to the choices of learning rate compared to other hyperparameters; moreover, with proper hyperparameter configurations, LoRA consistently matches or surpasses the performance of most of its variants.

</details>


### [105] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: TA-GRPO 通过生成等价变换的问句变体并对群体奖励进行汇聚，缓解 GRPO 的多样性崩溃与梯度消失问题，在推理任务上提升 Pass@k 表现。


<details>
  <summary>Details</summary>
Motivation: LLM 的下一词预测本质使其成为模式匹配器；GRPO 存在两种失败模式：多样性崩溃和梯度衰减，限制推理能力与泛化，需要通过词序变换和格式变换等实现更丰富的信号与探索。

Method: TA-GRPO 通过对每个问题生成语义等价的 transformed variants（包含同义改写、变量重命名、格式变换等），并对整个组合的奖励进行汇聚以计算优势，从而在训练时提供混合奖励，促进多种解法策略并缓解零梯度问题。

Result: 理论分析表明 TA-GRPO 降低零梯度概率，因减少训练-测试分布偏移而提升泛化。实际实验在数学推理基准上实现一致的 Pass@k 提升，在竞赛数学（AMC12、AIME24）中最高提升 9.84 点，在跨分布科学推理（GPQA-Diamond）中提升 5.05 点。

Conclusion: TA-GRPO 能缓解梯度与多样性问题，提升推理任务的鲁棒性与泛化能力；通过对问句的语义等价变换及全组奖励汇聚实现更丰富的训练信号。

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [106] [SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models](https://arxiv.org/abs/2601.22805)
*Pit Neitemeier,Alessio Serra,Jiaze Li,Sascha Wirges,Lukas Balles,Jan Hendrik Metzen*

Main category: cs.LG

TL;DR: 引入边界丰富度度量B及Sombrero方法，通过将边界放在预测困难的位置来提升分层序列模型的效率与准确性，在1B级数据上实现更优的效率-准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有端到端方法可从语言模型目标中学习有意义的边界，但很难量化评估和系统性地引导计算花费的位置，因此需要一个 router 无关的边界质量度量以及一种可控的边界学习策略。

Method: 提出边界丰富度B，度量片段起始位置在下一个字节高预测信息熵处的集中程度。基于B，提出Sombrero，通过置信对齐边界损失引导边界放置使其与预测难度对齐；在输入层应用带权置信度平滑来稳定边界学习，而不是对已实现的块进行平滑。并在1B级数据的UTF-8语料（英语、德语、代码、数学内容）上评估。

Result: Sombrero在多个语料和任务上提升了准确性与效率的折中，产生的边界更一致地将计算分配给难以预测的位置。

Conclusion: 通过对边界进行以预测难度为导向的学习，分层序列模型的计算分配可以更有效地匹配难度分布，且方法对跨语言和编码内容具有鲁棒性。

Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitatively assess and systematically steer where compute is spent. We introduce a router-agnostic metric of boundary quality, boundary enrichment B, which measures how strongly chunk starts concentrate on positions with high next-byte surprisal. Guided by this metric, we propose Sombrero, which steers boundary placement toward predictive difficulty via a confidence-alignment boundary loss and stabilizes boundary learning by applying confidence-weighted smoothing at the input level rather than on realized chunks. On 1B scale, across UTF-8 corpora covering English and German text as well as code and mathematical content, Sombrero improves the accuracy-efficiency trade-off and yields boundaries that more consistently align compute with hard-to-predict positions.

</details>


### [107] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: 提出 STARS：一个训练无关的框架，通过检测隐藏状态的 L2 距离峰值来识别推理转折的关键时刻，并通过几何轨迹分析和状态感知语言提示在实时纠正错误轨迹，从而减少认知惰性并提高准确性。


<details>
  <summary>Details</summary>
Motivation: LRMs 常因认知惰性而陷入过度思考或推理方向僵化，现有检测方法依赖表层自我纠错标记，无法捕捉潜在的内在冲突；需要一种无额外微调的自监督方法来动态引导推理。

Method: 检测隐藏状态的 L2 距离峰值以识别认知枢轴；采用几何轨迹分析诊断转折的结构性质；在检测点注入状态感知的语言提示，实时引导模型；无训练过程，对现有参数进行外部纠正。

Result: 在多种基准上实验表明，STARS 能减少冗余循环并通过自适应纠正错误轨迹提升准确性。

Conclusion: STARS 提供一种鲁棒的无监督机制，能在不进行额外微调的情况下优化大型推理模型的推理过程。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [108] [MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models](https://arxiv.org/abs/2601.22887)
*Yangyan Li*

Main category: cs.LG

TL;DR: MoVE decouples memory from compute by introducing a global bank of learnable value embeddings shared across all attention layers, with a differentiable soft gating to mix retrieved concepts into the value projection, enabling memory capacity to scale independently of model depth; it yields improvements on text and image autoregressive generation under comparable compute budgets.


<details>
  <summary>Details</summary>
Motivation: Address the fundamental coupling between model capacity and computational cost in autoregressive models; enable scaling of parametric memory without increasing active FLOPs, allowing memory-dense models to improve fidelity and perplexity.

Method: Introduce a global bank of learnable value embeddings shared across all attention layers. At each step, apply a differentiable soft gating mechanism to mix retrieved concepts from the bank into the standard value projection. Increase memory capacity by adding more embedding slots, decoupled from network depth.

Result: Controlled experiments on text generation and image generation show that MoVE consistently outperforms standard and layer-wise memory baselines, enabling memory-dense models to achieve lower perplexity and higher fidelity than dense models at similar compute budgets.

Conclusion: MoVE decouples memory from compute in autoregressive models, enabling scalable parametric memory and improved generation quality without increasing active FLOPs; the approach yields consistent gains in both text and image domains.

Abstract: Autoregressive sequence modeling stands as the cornerstone of modern Generative AI, powering results across diverse modalities ranging from text generation to image generation. However, a fundamental limitation of this paradigm is the rigid structural coupling of model capacity to computational cost: expanding a model's parametric memory -- its repository of factual knowledge or visual patterns -- traditionally requires deepening or widening the network, which incurs a proportional rise in active FLOPs. In this work, we introduce $\textbf{MoVE (Mixture of Value Embeddings)}$, a mechanism that breaks this coupling and establishes a new axis for scaling capacity. MoVE decouples memory from compute by introducing a global bank of learnable value embeddings shared across all attention layers. For every step in the sequence, the model employs a differentiable soft gating mechanism to dynamically mix retrieved concepts from this bank into the standard value projection. This architecture allows parametric memory to be scaled independently of network depth by simply increasing the number of embedding slots. We validate MoVE through strictly controlled experiments on two representative applications of autoregressive modeling: Text Generation and Image Generation. In both domains, MoVE yields consistent performance improvements over standard and layer-wise memory baselines, enabling the construction of "memory-dense" models that achieve lower perplexity and higher fidelity than their dense counterparts at comparable compute budgets.

</details>


### [109] [Perplexity Cannot Always Tell Right from Wrong](https://arxiv.org/abs/2601.22950)
*Petar Veličković,Federico Barbero,Christos Perivolaropoulos,Simon Osindero,Razvan Pascanu*

Main category: cs.LG

TL;DR: Perplexity can be an unreliable metric for selecting decoder-only Transformer models; with continuity properties, there exist sequences that are low in perplexity yet not predicted correctly, and increasing model confidence requires commensurate accuracy gains to affect selection (iso-perplexity).


<details>
  <summary>Details</summary>
Motivation: Perplexity is widely used as both a loss and a quality metric, but empirical limitations suggest it may not align with true generalization. A rigorous analysis leveraging Transformer continuity is needed to understand when perplexity fails as a model selection criterion.

Method: Theoretical/analytical approach: (1) use results on Transformer continuity to derive rigorous implications; (2) prove that if a compact decoder-only Transformer model predicts a sequence accurately and confidently, there must exist another sequence with very low perplexity that is not predicted correctly by the same model; (3) analytically study iso-perplexity plots to examine the relationship between model confidence and accuracy for selection.

Result: A formal demonstration that perplexity can mislead model selection: existence of sequences with low perplexity but incorrect predictions; iso-perplexity analysis shows that increasing confidence does not necessarily imply improved selection unless accompanied by corresponding accuracy gains.

Conclusion: Perplexity is not a reliable standalone metric for model selection in decoder-only Transformers. The continuity-based analysis clarifies when perplexity aligns with performance and highlights the need for complementary metrics or calibration analyses.

Abstract: Perplexity -- a function measuring a model's overall level of "surprise" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.

</details>


### [110] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: GFT提出了一种渐进式微调框架，通过温度控制的中间目标平滑地将预训练漂移与目标漂移插值，实现对流式/流式生成模型的稳定、快速适配，且具有收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 在数据有限、分布变化或对效率有严格要求的场景，直接微调可能损失先验学习的准确性与效率。现有基于奖励的微调方法受限于漂移结构或训练技巧。需要一个理论上扎实、可实际执行的渐进微调方案。

Method: 对随机流，提出温度控制的一系列中间目标，逐步拉近预训练漂移与目标漂移；通过温度趋近于零来获取真实目标。给出边际和条件GFT目标的收敛性证明；允许在GFT过程中使用如最优传输之类的耦合，且保持正确性。

Result: 在实践中，GFT提高收敛稳定性、缩短概率路径、加速推断，同时保持与标准微调相当的生成质量。

Conclusion: GFT为在分布移位下对流型生成模型的可扩展适配提供了理论扎实且实际有效的替代方案。

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [111] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出端到端可学习的置换框架，用于Transformer的结构化稀疏化后修剪，通过学习的置换代价矩阵、可微分二分匹配求解器和稀疏性损失，获得最优置换矩阵，并在视觉与语言Transformer上实现最优的置换效果。


<details>
  <summary>Details</summary>
Motivation: 解决置换搜索空间随Transformer规模指数增长的问题，传统方法依赖贪心或启发式，限制置换效果。

Method: - 学习置换代价矩阵，衡量输入通道之间的交换代价；- 引入可微分的二分匹配求解器，给定代价矩阵得到最优二进制置换矩阵；- 设计稀疏性优化损失，直接对置换算子进行端到端训练。

Result: 在视觉与语言Transformer上广泛验证，方法达到结构化稀疏化的最先进的置换结果。

Conclusion: 端到端可学习的置换框架有效提升Transformer的结构化稀疏化后处理效果，优于基于贪心/启发式的置换方法，具有良好的泛化性。

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [112] [Action-Sufficient Goal Representations](https://arxiv.org/abs/2601.22496)
*Jinu Hyeon,Woobin Park,Hongjoon Ahn,Taesup Moon*

Main category: cs.LG

TL;DR: 提出行动充足性表示以提升离线目标条件强化学习中的分层控制，并证明它比仅基于价值估计的表示更有利于行动选择与控制。


<details>
  <summary>Details</summary>
Motivation: 解决以价值估计为目标的表示可能丢失区分行动所需信息的问题；引入信息论框架来界定行动充足性，与价值充足性区分开来。

Method: 提出信息论框架并定义行动充足性；证明价值充足不蕴含行动充足；在离散环境中实验验证两者的相关性；分析低层策略的log-loss训练如何自然诱导行动充足表示；在一个公开基准任务上比较actor-derived与value-derived表示的性能。

Result: 行动充足性与控制成功的相关性比价值充足性更强；基于actor的表示在实验中优于基于价值估计学习得到的表示。

Conclusion: 关注并确保行动充足性的表示可提升离线GCRL的分层控制性能，且低层策略的训练目标天然促成行动充足表示。

Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of goals that serves as the interface between these levels. Existing approaches commonly derive goal representations while learning value functions, implicitly assuming that preserving information sufficient for value estimation is adequate for optimal control. We show that this assumption can fail, even when the value estimation is exact, as such representations may collapse goal states that need to be differentiated for action learning. To address this, we introduce an information-theoretic framework that defines action sufficiency, a condition on goal representations necessary for optimal action selection. We prove that value sufficiency does not imply action sufficiency and empirically verify that the latter is more strongly associated with control success in a discrete environment. We further demonstrate that standard log-loss training of low-level policies naturally induces action-sufficient representations. Our experimental results a popular benchmark demonstrate that our actor-derived representations consistently outperform representations learned via value estimation.

</details>


### [113] [Keep Rehearsing and Refining: Lifelong Learning Vehicle Routing under Continually Drifting Tasks](https://arxiv.org/abs/2601.22509)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.LG

TL;DR: Introducing DREE, a lifelong learning framework for neural VRP solvers under continual drift with limited per-task data, enabling efficient learning, reduced forgetting, and better generalization across unseen tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world VRP patterns drift over time and per-task data is limited; existing one-shot or full-data lifelong methods fail to handle continual drift with scarce data.

Method: Dual Replay with Experience Enhancement (DREE): a general framework that uses dual replay mechanisms to preserve past knowledge while learning from drift, plus experience enhancement to improve sample efficiency; applicable to various neural VRP solvers.

Result: Empirical results show DREE enables learning of new tasks, retention of prior knowledge, and better generalization to unseen tasks across multiple neural solvers.

Conclusion: DREE provides an effective, transfer-friendly approach to continual drift in neural VRP solvers with limited training per task, mitigating catastrophic forgetting and enhancing adaptability.

Abstract: Existing neural solvers for vehicle routing problems (VRPs) are typically trained either in a one-off manner on a fixed set of pre-defined tasks or in a lifelong manner on several tasks arriving sequentially, assuming sufficient training on each task. Both settings overlook a common real-world property: problem patterns may drift continually over time, yielding massive tasks sequentially arising while offering only limited training resources per task. In this paper, we study a novel lifelong learning paradigm for neural VRP solvers under continually drifting tasks over learning time steps, where sufficient training for any given task at any time is not available. We propose Dual Replay with Experience Enhancement (DREE), a general framework to improve learning efficiency and mitigate catastrophic forgetting under such drift. Extensive experiments show that, under such continual drift, DREE effectively learns new tasks, preserves prior knowledge, improves generalization to unseen tasks, and can be applied to diverse existing neural solvers.

</details>


### [114] [Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic](https://arxiv.org/abs/2601.22510)
*Xingyu Zhao,Darsh Sharma,Rheeya Uppaal,Yiqiao Zhong*

Main category: cs.LG

TL;DR: LLMs在学习技能组合时并非遵循人类线性规则，而呈现碎裂的组合性：常以反向或并行方式获得技能，且受训练数据相关匹配驱动；这一现象在现代LLMs中仍然存在，难以通过扩展模型规模或使用scratchpad来缓解。


<details>
  <summary>Details</summary>
Motivation: 揭示技能组合学习的动力学及非人类行为的根本原因，解释模型与人类在技能组合上的差异，并探讨对推理可靠性、OOD鲁棒性与对齐的影响。

Method: 在合成算术任务上训练Transformer，进行广泛的消融和细粒度诊断指标分析，研究学习动力学及分布偏移下的错误模式；评估训练数据的相关匹配对学习过程的驱动作用；并验证在现代LLMs中的存在性。

Result: 核心发现包括：(1) Transformer不按人类线性规则构建技能组合；(2) 往往以逆序或并行方式学习，导致分布偏移时出现混合错误，被称为 shattered compositionality；(3) 学习动力学更受训练数据的相关匹配影响，而非因果或程序性组合；(4) 该现象在现代LLMs中持续存在，且不被纯模型放大或基于scratchpad的推理所缓解。

Conclusion: 揭示模型学习行为与理想技能组合之间的根本错配，影响推理可靠性、对OOD鲁棒性和对齐工作的理论与实践。

Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In this study, we investigate the mechanism of learning dynamics by training transformers on synthetic arithmetic tasks. Through extensive ablations and fine-grained diagnostic metrics, we discover that transformers do not reliably build skill compositions according to human-like sequential rules. Instead, they often acquire skills in reverse order or in parallel, which leads to unexpected mixing errors especially under distribution shifts--a phenomenon we refer to as shattered compositionality. To explain these behaviors, we provide evidence that correlational matching to the training data, rather than causal or procedural composition, shapes learning dynamics. We further show that shattered compositionality persists in modern LLMs and is not mitigated by pure model scaling or scratchpad-based reasoning. Our results reveal a fundamental mismatch between a model's learning behavior and desired skill compositions, with implications for reasoning reliability, out-of-distribution robustness, and alignment.

</details>


### [115] [Agnostic Language Identification and Generation](https://arxiv.org/abs/2601.23258)
*Mikael Møller Høgsgaard,Chirag Pabbaraju*

Main category: cs.LG

TL;DR: 在不依赖 realizability 假设的全新“无偏”设置下研究语言识别与生成，给出目标、理论表征及接近最优的估计速率。


<details>
  <summary>Details</summary>
Motivation: 前人关于语言识别与生成的工作通常在 realizability 假设下分析，即输入数据必然来自给定语言集合中的某个分布。该假设在现实中往往不成立，需引入对任意分布的鲁棒性分析。

Method: 提出针对语言识别和生成的无偏 agnostic 目标，并给出理论分析框架，获得对两任务的新的表征和近似最优的速率界。

Result: 得到新的理论表征和近乎紧密的速率界，揭示在无假设分布下的基本极限。

Conclusion: 该 agnostic 框架扩展了现有可实现性分析，提升了对语言识别与生成在分布漂移条件下的理解，为构建鲁棒算法提供理论基础。

Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general "agnostic" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.

</details>


### [116] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: UAV-assisted VLC system with 3D trajectory optimization to maximize data collection efficiency by minimizing flight distance; closed-form optimal altitude; pheromone-driven reward with TD3-based algorithm; results show 35% flight distance reduction and 50% faster convergence.


<details>
  <summary>Details</summary>
Motivation: Leverage UAV-VLC integration to provide flexible communication and efficient lighting, and optimize UAV trajectory to improve data collection efficiency under VLC channel constraints.

Method: Derive a closed-form optimal UAV flight altitude under a VLC channel gain threshold. Optimize the horizontal trajectory using a novel pheromone-driven reward mechanism integrated with Twin Delayed Deep Deterministic Policy Gradient (TD3) to adapt the UAV motion in complex environments.

Result: Optimal altitude can reduce flight distance by up to 35% relative to baseline methods. The pheromone-driven reward mechanism significantly shortens convergence steps by ~50%.

Conclusion: The proposed framework enhances data collection efficiency in UAV-assisted VLC systems and speeds up convergence of learning-based trajectory optimization.

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [117] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: FOCUS speeds up Diffusion LMs by dynamically focusing computation on decodable token blocks and evicting non-decodable ones, expanding effective batch size and boosting throughput without hurting quality (up to 3.52x vs LMDeploy).


<details>
  <summary>Details</summary>
Motivation: DLLMs incur high decoding cost due to block-level parallelism where most tokens in a diffusion step are non-decodable, leading to wasted compute. A key observation is that attention-derived token importance correlates with decoding probability, suggesting a way to prioritize computation.

Method: Introduce FOCUS, an inference system that (1) predicts decodability using attention-derived token importance, (2) evicts non-decodable tokens on-the-fly, and (3) dynamically focuses computation on decodable tokens to increase effective batch size and throughput.

Result: Empirical evaluation shows up to 3.52× throughput improvement over LMDeploy while preserving or improving generation quality across multiple benchmarks.

Conclusion: FOCUS demonstrates a practical mechanism to alleviate DLLM decode-time bottlenecks by adaptive token-level scheduling, enabling scalable DLLM deployment; code released on GitHub.

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [118] [SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making](https://arxiv.org/abs/2601.22516)
*Md Mezbahul Islam,John Michael Templeton,Masrur Sobhan,Christian Poellabauer,Ananda Mohan Mondal*

Main category: cs.LG

TL;DR: SCOPE-PD 将主观与客观数据整合，用可解释的 AI 进行帕金森病预测；随机森林达成 98.66% 的准确率，SHAP 提供解释，关键特征来自 MDS-UPDRS 的震颤、运动迟缓和面部表情。


<details>
  <summary>Details</summary>
Motivation: 传统诊断的主观性导致诊断延迟，需要融合主观与客观数据的可解释 AI 解决方案。

Method: 基于 PPMI 数据的多模态特征构建，尝试多种机器学习方法，选取最佳模型并使用 SHAP 进行特征贡献分析。

Result: 综合特征的随机森林达到 98.66% 的准确率；MDS-UPDRS 的震颤、动作缓慢和面部表情为最具贡献的特征。

Conclusion: SCOPE-PD 展示了高性能且可解释的个性化帕金森病预测框架，强调将主观与客观数据整合以降低主观性。

Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.

</details>


### [119] [Learn from A Rationalist: Distilling Intermediate Interpretable Rationales](https://arxiv.org/abs/2601.22531)
*Jiayi Dai,Randy Goebel*

Main category: cs.LG

TL;DR: Rationale Extraction with Knowledge Distillation (REKD) 将教师–学生框架用于可解释的子集特征选择，显著提升小型RE模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: RE需要在仅有最终任务监督的条件下从特征空间搜索鲁棒的rationales，且当学生模型能力不足时性能受限，需引入教师知识以提升学习效果。

Method: 引入教师（rationalist）提供rationales与预测，以及学生RE模型在自身RE优化同时进行知识蒸馏。结构对RE无关性，任何黑盒后端均可接入。实验在语言与视觉任务上对BERT/ViT进行多种变体评估。

Result: 在IMDB、CIFAR-10、CIFAR-100等数据集上，REKD显著提升了学生RE模型的预测性能，相较仅使用自学RE的方法表现更好。

Conclusion: REKD为可解释性学习提供一种通用、跨模态的提升路径，证明了教师知识蒸馏在可解释性子任务中的有效性。

Abstract: Because of the pervasive use of deep neural networks (DNNs), especially in high-stakes domains, the interpretability of DNNs has received increased attention. The general idea of rationale extraction (RE) is to provide an interpretable-by-design framework for DNNs via a select-predict architecture where two neural networks learn jointly to perform feature selection and prediction, respectively. Given only the remote supervision from the final task prediction, the process of learning to select subsets of features (or \emph{rationales}) requires searching in the space of all possible feature combinations, which is computationally challenging and even harder when the base neural networks are not sufficiently capable. To improve the predictive performance of RE models that are based on less capable or smaller neural networks (i.e., the students), we propose \textbf{REKD} (\textbf{R}ationale \textbf{E}xtraction with \textbf{K}nowledge \textbf{D}istillation) where a student RE model learns from the rationales and predictions of a teacher (i.e., a \emph{rationalist}) in addition to the student's own RE optimization. This structural adjustment to RE aligns well with how humans could learn effectively from interpretable and verifiable knowledge. Because of the neural-model agnostic nature of the method, any black-box neural network could be integrated as a backbone model. To demonstrate the viability of REKD, we conduct experiments with multiple variants of BERT and vision transformer (ViT) models. Our experiments across language and vision classification datasets (i.e., IMDB movie reviews, CIFAR 10 and CIFAR 100) show that REKD significantly improves the predictive performance of the student RE models.

</details>


### [120] [Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective](https://arxiv.org/abs/2601.22532)
*Hong Xie,Xiao Hu,Tao Tan,Haoran Gu,Xin Li,Jianyu Han,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 提出一个最小化基线并以基于分批上下文赌博 (batched contextual bandit) 的实验框架，系统评估强化微调中设计选择的边际贡献，揭示哪些因素对学习与泛化最关键。


<details>
  <summary>Details</summary>
Motivation: 当前强化微调领域存在大量设计选择，但结论不一致且缺乏对各因素作用的原理性回答，设计因素往往耦合，难以明确贡献。

Method: 建立一个简单基线：每轮仅进行一次 rollout、以 outcome reward 作为训练信号、批量大小为 32；将其映射到 batched contextual bandit 学习框架，并设计实验流水线逐项评估各设计因素（如优势、 rollout 数量等）的边际增益。

Result: 在三个基础模型和两组数据集上，系统揭示不同设计选择对学习与泛化动态的作用，并识别出需要投入更多研究的关键设计因素。

Conclusion: 该实验框架能够帮助研究者解耦并评估设计选择的作用与关键性，从而将资源聚焦于真正影响学习与泛化的因素。

Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to two fundamental questions: 1) what is the role of each design choice? 2) which ones are critical? This paper aims to shed light on them. The underlying challenge is that design choices are entangled together, making their contribution to learning and generalization difficult to attribute. To address this challenge, we first construct a minimalist baseline for disentangling factors: one rollout per query in each round, the outcome reward serving as the training signal without any advantage trick, and a batch size of thirty-two. This baseline connects to batched contextual bandit learning, which facilitates experimental analysis. Centering around this baseline, we design an experiment pipeline, examining the marginal gains of factors like advantage, number of rollouts, etc. Experiments on three base models and two datasets, not only reveal new understanding on the role of various design choices on learning and generalization dynamics, but also identify critical ones that deserve more effort.

</details>


### [121] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 提出 L2D-SLDS 的学习延迟框架，用于带部分反馈、存在时变可用专家的非平稳时序。通过 factorized 切换线性高斯状态空间模型与全局因子实现跨专家信息传递，配合动态注册表实现专家进入/剪枝，基于一阶预测置信度的 IDS 风格路由，目标是在预测成本和对潜在状态的额外信息获取之间权衡。实验证明相较于上下文带权贝叶斯基线有改进，并且去除共享因子的消融表现不佳。


<details>
  <summary>Details</summary>
Motivation: 解决在部分反馈、专家可用性随时间变化、以及非平稳环境中“学习后延迟（Learning to Defer）”的问题，使不同专家的知识能够共享信息并对潜在潜在状态进行更好的推断。

Method: 提出一个因子化的切换线性高斯状态空间模型（L2D-SLDS）：包含全局共享因子、每个专家的特定潜在状态、以及上下文相关的状态转移。引入动态注册表实现专家的进入和剪枝。使用一次性前瞻预测信念，提出一个基于 IDS 的路由策略，在预测成本与关于潜在制度与共享因子的信息增益之间权衡。

Result: 在实验中相对于上下文带基线表现更好，且去除共享因子的消融实验显著下降，表明共享因子对于跨专家信息传递和鲁棒性提升有贡献。

Conclusion: 该工作通过引入全局共享因子和可动态管理的专家注册表，在带部分反馈的非平稳时序中提升路由决策质量，且对潜在制度和共享信息的推断具有帮助。

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [122] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 提出一个受人脑多系统启发的三模块采样框架，用于可扩展的贝叶斯推断：模型基（慢但灵活）、模型无（快速）、情节记忆（快速回忆）三者协同，以提升不确定性量化在贝叶斯深度学习中的应用能力。


<details>
  <summary>Details</summary>
Motivation: 揭示生物系统高效学习的计算原理并将其转译为可扩展的贝叶斯推断采样算法，提升对后验分布的探索效率和不确定性量化能力。

Method: 提出三模块采样框架： (1) 模型基模块使用目标分布进行引导但计算成本高的采样；(2) 模型无模块利用以往样本在参数空间中学习模式，从而实现不直接评估目标分布的快速采样；(3) 情节记忆模块通过回忆历史样本来快速采样。理论上结合三者以实现高效、可扩展的贝叶斯推断，特别应用于贝叶斯深度学习。

Result: 该框架被称为推进贝叶斯方法并使其适用于大规模统计机器学习问题；在贝叶斯深度学习中强调不确定性量化的原则性落实。

Conclusion: 多系统协同的采样框架提供了一条可扩展且对不确定性友好的贝叶斯推断路线，适用于大规模模型的贝叶斯深度学习。

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [123] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 提出基于保守量校正的模型无关技术以提升自回归神经算子对PDE解的长期稳定性，并通过谱域分析揭示现有架构在高频分量上的局限性，呼吁重视高频成分以建模湍流。


<details>
  <summary>Details</summary>
Motivation: 解决自回归神经算子在长期预测中的误差积累和物理量守恒不足问题，提升模型在PDE求解中的长期稳定性。

Method: 提出 conserved quantity correction（保守量校正），一种模型无关的在深度学习模型中引入物理守恒准则的技术；并对神经算子进行谱域分析，评估当前架构在高频分量上的表现。

Result: 在各种模型架构下，保守量校正带来长期稳定性的一致改进；谱域分析揭示现有架构的显著局限，强调需要重视高频分量以有效建模湍流。

Conclusion: 未来工作应关注强调高频分量的架构设计，并将守恒与谱域特性结合，以提升对湍流等复杂PDE的长期预测能力。

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [124] [FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction](https://arxiv.org/abs/2601.22578)
*Chengyang Zhou,Zijian Zhang,Chunxu Zhang,Hao Miao,Yulin Zhang,Kedi Lyu,Juncheng Hu*

Main category: cs.LG

TL;DR: 提出了FedDis，通过因果化解耦实现联邦时空预测的个性化与全局模式分离，采用双分支结构和互信息最小化以实现信息正交，从而在4个真实数据集上达到SOTA且具备良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 非独立同分布的交通数据在联邦学习中的挑战来自客户端局部动态与跨客户端全局时空模式的耦合，导致泛化与适应性不足。

Method: 双分支架构：Personalized Bank 捕捉客户端特有因素，Global Pattern Bank 吸收共性知识；通过互信息最小化的目标实现两者信息正交，促进因果性解耦并实现跨客户端知识迁移。

Result: 在四个真实基准数据集上，FedDis一致取得最先进的性能，且具备效率与可扩展性。

Conclusion: 提出的因果解耦联邦时空预测框架FedDis，能够在保持对本地环境的高适应性的同时实现稳健的跨客户端知识传递，是该领域的新进展。

Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.

</details>


### [125] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: 提出基于中位数基线的组相对策略优化 MC-GRPO，以在小回合预算下稳定训练、提升精度。


<details>
  <summary>Details</summary>
Motivation: 在资源受限、回合预算小的情境中，使用共享均值基线会带来基线噪声，从而产生优势符号翻转，削弱更新方向的正确性。

Method: 用中位数基线替代均值基线；为每组增加一个回合用于中位参考（G+1），以组中位数计算优势；在奇数大小的组中，恰有一个回合为中位点且获得零优势，将该回合排除回传，以保持每个提示的梯度贡献数为 G，保留标准 G 回合训练的核心成本；在多种 GRPO 家族方法、不同模型与规模上验证。

Result: 在低回合数情景下，训练的稳定性与最终准确性得到持续提升；缩小 G=2 与 G=8 之间的差距至约 1% 以内。

Conclusion: 中位数中心化的 GRPO 是一种简单、有效的解决方案，能在小回合预算下提升稳定性与性能，具有良好泛化性，且提供代码实现。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [126] [FedCARE: Federated Unlearning with Conflict-Aware Projection and Relearning-Resistant Recovery](https://arxiv.org/abs/2601.22589)
*Yue Li,Mingmin Chu,Xilei Yang,Da Xiao,Ziqi Xu,Wei Shao,Qipeng Song,Hui Li*

Main category: cs.LG

TL;DR: FedCARE introduces a unified, low-overhead federated unlearning framework with conflict-aware unlearning and relearning-resistant recovery. It uses gradient ascent for forgetting when local data is available, data-free model inversion to create class-level proxies of shared knowledge, a pseudo-sample generator, and a recovery strategy that suppresses rollback toward the pre-unlearning model. It supports client, instance, and class-level unlearning, performing well in IID and non-IID settings with improved forgetting, utility retention, and reduced relearning compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Federated learning protects data privacy but must comply with rights like the right to be forgotten. Retraining from scratch is costly, so federated unlearning (FU) is needed. Existing FU methods have high overhead, degrade utility due to entangled knowledge, and risk relearning during recovery. There is a need for a unified, efficient FU framework with robust forgetting and stable post-unlearning performance.

Method: FedCARE combines (1) gradient ascent forgetting when target data are locally available, (2) data-free model inversion to construct class-level proxies of shared knowledge, and (3) a pseudo-sample generator, conflict-aware projected gradient ascent to preserve utility, and a recovery strategy to suppress rollback. It supports client-, instance-, and class-level unlearning with modest overhead, and is designed to be robust against entangled knowledge and relearning.

Result: Extensive experiments on multiple datasets and model architectures under IID and non-IID settings show that FedCARE achieves effective forgetting, better utility retention, and reduced relearning risk compared with state-of-the-art FU baselines.

Conclusion: FedCARE provides a unified, low-overhead federated unlearning framework with conflict-aware unlearning and relearning-resistant recovery, delivering effective forgetting and improved utility across settings. It offers a practical solution for compliance-driven data removal in federated systems.

Abstract: Federated learning (FL) enables collaborative model training without centralizing raw data, but privacy regulations such as the right to be forgotten require FL systems to remove the influence of previously used training data upon request. Retraining a federated model from scratch is prohibitively expensive, motivating federated unlearning (FU). However, existing FU methods suffer from high unlearning overhead, utility degradation caused by entangled knowledge, and unintended relearning during post-unlearning recovery. In this paper, we propose FedCARE, a unified and low overhead FU framework that enables conflict-aware unlearning and relearning-resistant recovery. FedCARE leverages gradient ascent for efficient forgetting when target data are locally available and employs data free model inversion to construct class level proxies of shared knowledge. Based on these insights, FedCARE integrates a pseudo-sample generator, conflict-aware projected gradient ascent for utility preserving unlearning, and a recovery strategy that suppresses rollback toward the pre-unlearning model. FedCARE supports client, instance, and class level unlearning with modest overhead. Extensive experiments on multiple datasets and model architectures under both IID and non-IID settings show that FedCARE achieves effective forgetting, improved utility retention, and reduced relearning risk compared to state of the art FU baselines.

</details>


### [127] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: MGMT 是一个统一、可扩展且可解释的跨图学习框架，通过对每个图使用图变换器编码、选择关注的超节点并在跨图层级上构建元图，实现跨图的联合推理与解释。


<details>
  <summary>Details</summary>
Motivation: 在拓扑结构、尺度和语义各异且往往缺少共享节点身份的多图场景中，如何有效整合信息以实现跨图表示学习仍具挑战性；需要可解释且可扩展的方法来进行跨图推理。

Method: 对每个图应用图变换器编码器，将结构与属性映射到共享潜在空间；通过注意力选取任务相关的超节点并构建在潜在空间中相似度驱动的元图；在元图上添加额外的图变换层以进行跨图与图内的联合推理；元图中的超节点/超边提供可解释性，揭示影响子结构和跨图对准。

Result: 在合成数据和真实神经科学数据集上，MGMT 在图级预测任务上持续优于现有最先进模型，并提供有助于科学发现的可解释表示。

Conclusion: MGMT 为结构化的多图学习提供统一框架，提升图数据领域的表示能力，尤其在缺少共享节点身份的跨图场景中具有潜在应用价值。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [128] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: 提出Lethe，一种联邦学习中的“解除学习”方法，解决持续训练导致的知识再显现问题（resurfacing）。通过将待移除的知识与需保留的知识解耦，在继续训练时实现持久性消除。实现一个 Reshape-Rectify-Restore 的流程：先用梯度上升训练一个临时适配器以放大待移除的更新，再用该信号对后续保留数据的更新进行两路流的层级整形，最后移除适配器并对保留数据进行短时间恢复阶段。实验表明Lethe在不同层级的unlearning场景下均能工作，且在多轮后续训练中复现率（Resurfacing Rate）大多低于1%。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的消除学习研究通常假设移除后训练就结束，忽略了继续训练可能重新激活被删除的知识的风险。需要一种可在后续训练中仍保持消除效果的方案，特别是在跨客户端 Federated Setting 下的可持续 unlearning。

Method: Lethe 采用Reshape-Rectify-Restore管线：1) 训练一个临时适配器，在待移除数据上进行梯度上升以获得放大更新；2) 将该信号用于对剩余更新进行分层的分歧校正（两条流），实现对待移除与保留知识的解耦；3) 移除适配器；4) 对保留数据进行短期恢复阶段。

Result: 在多层级联邦无损意图下，Lethe实现统一的unlearning，并在持续训练后保持较低的复现率，实验结果显示Resurfacing Rate大多<1%，具备良好持久性。

Conclusion: Lethe提供了一种在联邦学习情境下的持久化unlearning范式，通过解耦待移除知识与需保留知识来防止知识再显现，具有良好的一致性与鲁棒性，适用于不同层级的unlearning场景。

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [129] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 提出一种共识机制（consensus）作为注意力的替代，以提升变换器在学习率过大时的稳定性；通过将共识建模为图模型，进行文本、DNA、蛋白质等模态的广泛实验，并提出混合共识-注意力框架，同时给出理论性质分析。


<details>
  <summary>Details</summary>
Motivation: 为解决标准自注意力在学习率超设定时的训练不稳定问题，寻求架构层面的稳定性提升，而不仅仅通过优化技巧。

Method: 将共识公式化为图模型；在文本、DNA、蛋白质任务上进行学习率扫描验证稳定性；提出混合框架以在保留性能的同时提高稳定性；给出共识性质的理论分析。

Result: 在多模态学习率扫描中显示显著的稳定性提升；混合框架在保持性能的同时提升稳定性；给出对共识性质的理论表征。

Conclusion: 共识作为一种可插拔的替代注意力的架构设计，能在更宽的学习率范围内稳定训练，拓展了稳定性导向的变换器设计空间。

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [130] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 将形式逻辑验证嵌入LLM推理过程，通过两阶段训练提升7B/14B模型在数学、逻辑和一般推理任务的性能，显著超越基线。


<details>
  <summary>Details</summary>
Motivation: LLMs的随机下一词预测导致推理中的逻辑不一致与奖励劫持等问题，现有的被动后验符号验证难以高效纠错，需要可扩展、实时的形式验证机制来提升推理可靠性。

Method: 提出一个形式逻辑验证引导的框架，动态将符号验证嵌入生成过程；并实现两阶段训练：1) 形式逻辑验证引导的监督微调（SFT），2) 策略优化（PPO 等）以对中间推理进行实时反馈与惩罚；在推理链上对中间错误进行主动纠正。

Result: 在六个覆盖数学、逻辑与通用推理的基准上，7B和14B模型分别实现平均提升10.4%与14.2%，显著超越最先进基线。

Conclusion: 将形式逻辑验证作为可扩展机制用于提升LLM推理能力，证明在一定规模的模型上可显著扩展推理边界与可靠性。

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [131] [GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning](https://arxiv.org/abs/2601.22651)
*Naoki Murata,Yuhta Takida,Chieh-Hsin Lai,Toshimitsu Uesaka,Bac Nguyen,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出 GUDA，一种基于组忘记学习的图像生成模型数据归因方法，通过对全数据模型进行近似的组撤回来避免 LOGO 的高成本，并借助 ELBO 衡量组影响；在 CIFAR-10 与 Stable Diffusion 的风格归因上显著优于语义相似性、梯度等方法，且比 LOGO 提速约 100 倍。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据分组层面的因果归因需求。现有的 LOGO 逐组重新训练成本高，难以扩展到大量组的场景。需要一种高效且可近似的因果推断方法来给出组级数据贡献的定量评估。

Method: 在一个共享的全数据模型基础上，通过机器忘学习（unlearning）来近似每个组的反事实模型（去除该组数据后的模型），避免从零开始重新训练。使用基于似然的评分规则（ELBO）来量化全模型与各自的“去学习后”模型之间的差异，从而衡量该组对输出的影响。

Result: 在 CIFAR-10 与使用 Stable Diffusion 的艺术风格归因任务中，GUDA 能比语义相似性、梯度基归因和实例级去学习等基线更 reliably 识别出主要贡献组；在 CIFAR-10 上相比 LOGO 重新训练实现约 100×的加速。

Conclusion: GUDA 提供一个高效且可扩展的组级数据归因框架，适用于扩散模型等生成模型，能实证地评估训练数据分组对生成结果的影响。

Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.

</details>


### [132] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出一个数据无关的早停框架，通过仅使用服务器端参数监控任务向量增长率来决定最优停止点。在皮肤病变分类和血细胞分类任务中，与基于验证的数据早停相当；平均需要47轮（皮肤病变）/20轮（血细胞）即可实现相对于基于验证数据早停的性能提升，分别超过12.5%和10.3%。这是首次在FL中提出不使用验证数据的早停方法。


<details>
  <summary>Details</summary>
Motivation: 降低联邦学习中超参数调优的计算成本和隐私风险，避免对固定全局轮数或验证数据的依赖，提升实际部署的可行性。

Method: 提出数据无关的早停框架，通过监控服务端参数中的任务向量增长速率来确定最优停止点，整个过程不需要任何验证数据。

Result: 在皮肤病变分类和血细胞分类任务上，所提方法与基于验证数据的早停方法具有可比性；平均在47轮（皮肤病变）/20轮（血细胞）内达到相对于验证数据早停的性能提升，分别高出12.5%与10.3%。

Conclusion: 首次在联邦学习场景中提出无需验证数据的早停框架，具有降低隐私风险与计算开销的潜力。

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [133] [Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation](https://arxiv.org/abs/2601.22679)
*Youngjoong Kim,Duhoe Kim,Woosung Kim,Jaesik Park*

Main category: cs.LG

TL;DR: 从流映射角度对一致性模型进行理论分析，揭示训练不稳定与收敛性问题导致的退化解；提出对自蒸馏的改写以实现稳定优化，并扩展至无预训练初始化的扩散式策略学习，显示方法的广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 一致性模型在快速生成任务中具有竞争力，但训练过程往往不稳定、可重复性不足，理论关系尚不清晰，需要统一的流映射框架来解释稳定性、收敛性与潜在退化。

Method: 采用基于流映射的理论分析框架，研究训练稳定性、收敛性与退化解的关系；重新审视自蒸馏作为对某些亚最优收敛的补救，并对其进行改写以避免过大梯度范数以实现稳定优化；并将该策略扩展至扩散式策略学习，且无需预训练扩散模型初始化。

Result: 揭示导致退化解的条件与机制；提出通过改写的自蒸馏来缓解某些不良收敛，并实现更稳健的优化过程；在图像生成与扩散式策略学习等任务上展示了更广的适用性与潜在效果。

Conclusion: 提供一个统一的理论框架来理解一致性模型的稳定性与收敛性，给出可操作的自蒸馏修正，并拓展到更广的扩散模型应用场景。

Abstract: Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.

</details>


### [134] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: 本研究将周期性作为Transformer在OOD情境下的一类基本挑战进行分析，从抽象代数与推理角度给出统一诠释，并提出针对“复合周期性”的可控生成基准Coper，包含两种OOD设定（Hollow与Extrapolation），实验证明Transformer在周期性泛化上能力有限，存在记忆训练数据而非对未见的复合周期性进行泛化的问题；并开源代码以便后续研究。


<details>
  <summary>Details</summary>
Motivation: 解决人类与大语言模型在OOD泛化上的差距，聚焦周期性这一基本且具挑战性的OOD情景，提出统一的理论解释并给出可控基准以诊断模型的泛化能力。

Method: 提出基于抽象代数与推理的统一周期性解读，覆盖单一周期性与复合周期性；设计Coper可控生成基准，包含两个OOD设定（Hollow、Extrapolation）；在Transformer上进行系统性实验以比较对已训练周期数据的记忆与对未见复合周期性的泛化能力，并开放源代码。

Result: 实验显示Transformer在周期性泛化方面能力受限：模型能够记忆训练中的周期数据，但对未见的复合周期性缺乏泛化能力；Coper基准提供了对该现象的诊断性评估，并揭示了当前模型在处理复合结构时的局限性。

Conclusion: 复合周期性对Transformer构成显著挑战，现有训练与模型偏向记忆而非推理泛化；需引入新的归纳偏置或训练方法以提升对复合周期性的泛化能力，同时开放代码以促进后续研究。

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [135] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 提出 METRIC 框架及数据质量度量库，结合 metric cards 与决策树，面向医学 ML 的按用途数据适配评价；在 PTB-XL ECG 数据集上示范，旨在为训练/测试数据的按用途评估奠定基础，促进可信 AI 的发展。


<details>
  <summary>Details</summary>
Motivation: 在医学 ML 的可信性建设中，数据质量的量化是关键且亟需可操作的方法。现有工作缺乏系统的、易于落地的数据质量评估工具，因此提出一个可用于评估数据是否符合特定任务要求的框架。

Method: 提出 METRIC 框架；建立数据质量度量库及衡量指标卡；为每个指标提供定义、适用性、示例、陷阱和建议；给出从用例出发选择指标的决策树；在 PTB-XL 心电数据集上演示。

Result: 实现了可操作的数据质量指标库和指标卡、用于选择指标的决策树；在 PTB-XL 数据集上的示例演示证明其可用性及潜在影响，作为医学 AI 可信性评估的第一步。

Conclusion: 这是实现按用途评估训练与测试数据的初步工作，为医学领域的可信 AI 提供基础，未来需要在更多任务/数据集上扩展与验证，进一步完善标准化和可比性。

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [136] [Vision-Language Models Unlock Task-Centric Latent Actions](https://arxiv.org/abs/2601.22714)
*Alexander Nikulin,Ilya Zisman,Albina Klepach,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Lyubaykin Nikita,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: 使用VLM的可提示表示，作为LAM训练的目标，去除动作相关的干扰，提升鲁棒性与下游成功率；不同VLM的提示质量差异显著，简单指令忽略干扰即可带来显著提升， Distracting MetaWorld 上提升可达 6 倍。


<details>
  <summary>Details</summary>
Motivation: LAM在含有与动作相关的干扰时易编码噪声而非有意义的 latent 动作，与人类在给定简短任务描述时能区分相关 motions 的能力相比存在差异。通过利用VLM的常识推理和可提示性，将其作为无监督/自监督训练中的目标表示，来分离可控变化和噪声。

Method: 将VLMs产生的可提示表示作为LAM训练的目标表示；对多种流行VLMs进行基准评测，考察提示和超参数对表示质量的影响；让VLM在推断阶段忽略干扰器（distractors）以提高表示质量；在 Distracting MetaWorld 上评估下游任务性能。

Result: VLMs的可提示表示质量存在较大差异，且更近的VLMs未必优于较旧模型；简单地让VLM忽略干扰可以显著提升LAM的 Latent Action 质量，带来下游任务成功率的显著提升，最高可达六倍。

Conclusion: 通过VLM的可提示表达作为LAM训练目标，能提高对干扰的鲁棒性和 Latent Action 质量，提示质量及提示策略对性能影响显著。未来工作可关注更优的VLM选择、提示设计与超参数调优，以及对不同数据分布的泛化。

Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.

</details>


### [137] [Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation](https://arxiv.org/abs/2601.22716)
*Pingzhi Tang,Ruijie Zhou,Fanxu Meng,Wenjie Pei,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出LoRDS：基于低秩分解的缩放矩阵实现元素级量化的高效统一框架，打破块级约束，支持PTQ初始化、联合QAT以及高秩乘法化PEFT，无额外推理开销，在多模型与下游任务中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有量化多采用块状结构以提高效率，但牺牲了表示灵活性与表达能力。需要一种在保持高效的同时提升量化表达能力的方法，且能够与PEFT高效结合。

Method: 将缩放矩阵S建模为低秩分解S = BA，使量化粒度从块级转向元素级；建立LoRDS统一框架：高保真PTQ初始化+迭代优化、权重与缩放因子联合QAT、在低秩预算下实现高秩乘性PEFT；通过优化的Triton内核实现高效推理。

Result: 在多模型家族中超越状态基线；在Llama3-8B上3比特相对NormalFloat提高可达27.0%，RTX 4090上实现1.5x推理加速；对下游任务的PEFT性能较4-bit QLoRA提升约9.6%。

Conclusion: LoRDS提供一个统一的压缩与适配解决方案，突破块级量化限制，兼具高保真量化、联合训练与高秩PEFT；无额外推理开销，适用于广泛模型与任务，表现优于现有方法。

Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior expressive power by modeling the scaling manifold as continuous low-rank matrices ($S = BA$). We propose Low-Rank Decomposed Scaling (LoRDS), a unified framework that rethinks quantization granularity through this low-rank decomposition. By "breaking the blocks" of spatial constraints, LoRDS establishes a seamless efficiency lifecycle: it provides high-fidelity PTQ initialization refined via iterative optimization, enables joint QAT of weights and scaling factors, and facilitates high-rank multiplicative PEFT adaptation. Unlike additive PEFT approaches such as QLoRA, LoRDS enables high-rank weight updates within a low-rank budget while incurring no additional inference overhead. Supported by highly optimized Triton kernels, LoRDS consistently outperforms state-of-the-art baselines across various model families in both quantization and downstream fine-tuning tasks. Notably, on Llama3-8B, our method achieves up to a 27.0% accuracy improvement at 3 bits over NormalFloat quantization and delivers a 1.5x inference speedup on NVIDIA RTX 4090 while enhancing PEFT performance by 9.6% on downstream tasks over 4bit QLoRA, offering a robust and integrated solution for unified compression and adaptation of LLMs.

</details>


### [138] [SQUAD: Scalable Quorum Adaptive Decisions via ensemble of early exit neural networks](https://arxiv.org/abs/2601.22711)
*Matteo Gambella,Fabrizio Pittorino,Giuliano Casale,Manuel Roveri*

Main category: cs.LG

TL;DR: SQUAD与QUEST结合的共识型早停与分布式集成学习框架，通过以阈值的共识替代单一置信度，提升不确定性估计与推理速度。


<details>
  <summary>Details</summary>
Motivation: 单模型置信度阈值往往因校准问题不可靠，需提升不确定性估计的鲁棒性并降低推理延迟。

Method: 提出SQUAD：在早停学习器上应用法定投票的共识停止标准，按计算复杂度逐步汇集中间预测，若达到统计显著的一致性则在该出口处停止计算；并引入QUEST（Quorum Search Technique）作为神经架构搜索方法，在每层实现具有层级多样性的早停学习器选择，确保学习器互补性。

Result: 相较于同成本的最先进动态方法，准确率提升可达约5.95%；相较于静态集成，推理时延可降低约70.60%，且保持较好准确性。

Conclusion: 基于共识的多模型早停策略可提供统计鲁棒性强的退出点，提升精度并显著降低推理延迟，具备可扩展性。

Abstract: Early-exit neural networks have become popular for reducing inference latency by allowing intermediate predictions when sufficient confidence is achieved. However, standard approaches typically rely on single-model confidence thresholds, which are frequently unreliable due to inherent calibration issues. To address this, we introduce SQUAD (Scalable Quorum Adaptive Decisions), the first inference scheme that integrates early-exit mechanisms with distributed ensemble learning, improving uncertainty estimation while reducing the inference time. Unlike traditional methods that depend on individual confidence scores, SQUAD employs a quorum-based stopping criterion on early-exit learners by collecting intermediate predictions incrementally in order of computational complexity until a consensus is reached and halting the computation at that exit if the consensus is statistically significant. To maximize the efficacy of this voting mechanism, we also introduce QUEST (Quorum Search Technique), a Neural Architecture Search method to select early-exit learners with optimized hierarchical diversity, ensuring learners are complementary at every intermediate layer. This consensus-driven approach yields statistically robust early exits, improving the test accuracy up to 5.95% compared to state-of-the-art dynamic solutions with a comparable computational cost and reducing the inference latency up to 70.60% compared to static ensembles while maintaining a good accuracy.

</details>


### [139] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: 提出 MetaDrug，一种二级元适应的、不确定性感知的元学习框架，用于解决EHR药物推荐中的患者冷启动问题。通过自适应（以患者自身医疗事件为支持集）与同伴自适应（利用相似就诊记录的同伴患者）来丰富新患者表示，并引入不确定性模块对支持集进行排序和过滤噪声，在MIMIC-III与AKI数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决EHR药物推荐中的患者冷启动问题；现有基于知识图谱的方法多聚焦于药物项的冷启动，对个人化适应性不足；元学习在用户冷启动方面有潜力，但在EHR的序列结构上应用不足。

Method: 提出两级元适应：自适应（基于患者自身的医疗事件作为支持集，捕捉时间依赖性）、同伴自适应（利用相似就诊记录的同伴患者来丰富新患者表示）。引入不确定性量化模块，对支持集进行排序，筛除无关信息以提升适应一致性。

Result: 在MIMIC-III与AKI数据集上，MetaDrug在冷启动患者的药物推荐任务中持续超越最先进的方法。

Conclusion: 不确定性感知的两级元学习框架有效缓解EHR药物推荐中的患者冷启动问题，提升个体化推荐能力。

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [140] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 提出 Style-Conditioned Implicit Q-Learning (SCIQL) 的离线目标条件强化学习框架，通过显式子轨迹标签实现风格与任务性能的一致性，结合门控优势加权回归与后验重标记优化任务与风格对齐。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习中，风格与高任务性能之间存在分布偏移和风格-奖励冲突；现有对风格的定义往往未能有效协调这两者，需一个统一、可落地的风格行为定义。

Method: 提出统一的行为风格定义并将其落地为 SCIQL 框架；融合离线目标条件 RL 的技术（如后验重标记和价值学习）与新型门控优势加权回归机制，通过子轨迹标签函数进行显式风格监督。

Result: 实验表明 SCIQL 在保持风格对齐的前提下，任务性能优于现有的离线方法。

Conclusion: 给出一个可行的、统一的风格定义和实现框架，显著提升风格约束下的离线 RL 表现，并提供代码与数据资源。

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [141] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 建立了软Max损失族的理论与实践框架，揭示一致性、梯度动力学、近似的偏差-方差及逐 epoch 复杂度之间的权衡，并给出大规模类别任务的实际指南。


<details>
  <summary>Details</summary>
Motivation: 探究Softmax-family losses在理论性质与大规模应用中的表现，明确如何在一致性与效率之间取得权衡。

Method: 将Softmax loss置于Fenchel-Young框架，系统分析多种近似在一致性（分类和排序）、梯度动力学、偏差-方差分解与逐epoch复杂度方面的表现，给出收敛性保证，并通过大量实验验证理论与经验的一致性。

Result: 证实不同Softmax近似在一致性与收敛性上的差异，给出近似方法的偏差-方差分解与逐 epoch 复杂度分析，实验与理论结果高度一致，明确了在大规模分类问题中的权衡与选择要点。

Conclusion: 提供一个 principled foundation 用于理解和比较 Softmax-family 损失，为大规模分类/排序任务中的损失选择与实现提供系统性、可操作的指南。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [142] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: OSNIP 是一个轻量级的客户端加密框架，用于隐私保护的大模型推理，通过引入 Obfuscated Semantic Null Space，在高维潜在空间中注入扰动，使原始嵌入在语义保真度上保持，同时实现与原始嵌入的近似正交；采用密钥相关的随机映射生成个性化扰动轨迹；在 12 个基准任务上达到状态-of-the-art，同时降低攻击成功率并保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 解决在不需要后处理的前提下对 LLM 推理进行隐私保护的问题。通过将线性核的几何直觉推广到高维潜在空间，定义一个 Obfuscated Semantic Null Space，使嵌入的语义信息得到保留，而扰动使其与原始嵌入近似正交，达到隐私保护。

Method: 提出 Obfuscated Semantic Null Space（OSNIP），通过投影将原始嵌入映射到该空间的扰动；引入密钥相关的随机映射，生成每位用户的个性化扰动轨迹；实现客户端端加密，不需要后处理。

Result: 在 12 个生成与分类基准上评估，OSNIP 实现了最先进的性能，显著降低攻击成功率，同时在严格的安全约束下保持强模型效用。

Conclusion: OSNIP 提供一种轻量、客户端对齐的隐私保护推理解决方案，通过 Obfuscated Semantic Null Space 和用户特定扰动轨迹实现隐私保护，同时保持实用性，适用于广泛的生成与分类任务。

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [143] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 研究揭示分子语言模型在固定算力下的可预测扩展规律；通过系统性实验（300个模型、1万多次实验）在模型规模、训练标记数量、分子表示之间独立变动，发现预训练与下游任务均存在明确的扩展规律，分子表示对性能显著影响，解释了以前关于分子生成的扩展性不一致的问题，并公开发布了最大的分子语言模型库。代码与模型可在 GitHub 获取。


<details>
  <summary>Details</summary>
Motivation: 明确分子语言模型在资源预算下的扩展规律，以便在模型规模、数据量与分子表征之间做出高效的资源分配决策，并解释先前观测到的扩展性不一致现象。

Method: 进行大规模、受控的实验研究：共训练300个模型，开展超过1万次实验，严格控制计算预算，独立变动模型规模、训练标记数量与分子表示，测定在预训练和下游任务中的性能与扩展行为。

Result: 在预训练和下游迁移任务中均观察到清晰的扩展规律；分子表示对性能有显著影响，能解释早前关于分子生成的扩展性不一致的问题；公开发布迄今最大规模的分子语言模型库，提供代码与模型。

Conclusion: 分子语言模型存在可预测的扩展规律，且分子表示的选择对性能影响显著。研究为资源分配提供量化依据，并通过公开资源推动后续研究。

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [144] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: 将稀疏注意与紧凑核回归建立正式对应，揭示 Epanechnikov 等核在 α-entmax 下的注意机制，并在极限下回归至 softmax；Memory Mosaics 展现核回归型注意在语言建模、上下文学习及长度泛化任务的竞争力。


<details>
  <summary>Details</summary>
Motivation: 缺乏对稀疏注意的核理论理解，需建立统一框架以解释稀疏性来源、核设计对注意力特性（稀疏性、平滑度）的影响，并提供可控的注意力机制以替代 heuristic top-k 等方法。

Method: 将归一化的 ReLU 与 sparsemax 注意映射到 Epanechnikov 核回归；推广到广义的 α-entmax，其中 α=1+1/n（n∈N）对应 Epanechnikov、biweight、triweight 等核，softmax/Gaussian 对应 n→∞；分析不同核在固定或自适应归一化下的注意表现；提出 Memory Mosaics 框架实现核回归型变体的 Transformer。

Result: 给出一个统一的核方法框架来解释稀疏性的由来，提供对 top-k 等记忆机制的原理化替代；在语言建模、上下文学习与长度泛化任务上，核回归型 Transformer 具有竞争力，验证了理论的实用性。

Conclusion: 建立了稀疏注意与紧凑核回归的正式对应，阐明不同核设计可控地引入稀疏性；为未来的注意力架构设计提供原理化工具和替代机制（如 Memory Mosaics），推动自注意力的高效性与泛化性提升。

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [145] [Stabilizing the Q-Gradient Field for Policy Smoothness in Actor-Critic](https://arxiv.org/abs/2601.22970)
*Jeong Woon Lee,Kyoleen Kwak,Daeho Kim,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 提出PAVE（Policy-Aware Value-field Equalization），通过对 critic 的梯度场进行正则化来稳定策略梯度，从而在不改动 actor 的前提下实现与策略端平滑正则化相近的平滑性与鲁棒性，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过直接对策略输出进行平滑正则化来抑制高频振荡，但这是在治疗症状而非原因。作者理论上证明，策略不可平滑性受 critic 的微分几何支配，核心在于 Q 函数的混合偏导（噪声敏感性）与动作空间曲率（信号区分度）之比；因此需从 critic 入手解决根源问题。

Method: 对 actor-critic 目标进行隐式微分，推导出策略对环境噪声的敏感性被 Q 的混合偏导与动作空间曲率之比所界定。基于此提出 PAVE：将 critic 视为标量场，最小化 Q 梯度的波动性（梯度不稳定性）同时尽量保持局部曲率，达到对行动梯度场的稳定化。

Result: 实验结果表明，PAVE 在平滑性与鲁棒性上可与策略端平滑正则化方法相当，且在不修改 actor 的情况下仍具备竞争性的任务性能。

Conclusion: 强调 critic 几何对稳定学习的重要性，基于 critic 的正则化（PAVE）是有效的策略稳定化途径，可作为现有策略端方法的有益补充。

Abstract: Policies learned via continuous actor-critic methods often exhibit erratic, high-frequency oscillations, making them unsuitable for physical deployment. Current approaches attempt to enforce smoothness by directly regularizing the policy's output. We argue that this approach treats the symptom rather than the cause. In this work, we theoretically establish that policy non-smoothness is fundamentally governed by the differential geometry of the critic. By applying implicit differentiation to the actor-critic objective, we prove that the sensitivity of the optimal policy is bounded by the ratio of the Q-function's mixed-partial derivative (noise sensitivity) to its action-space curvature (signal distinctness). To empirically validate this theoretical insight, we introduce PAVE (Policy-Aware Value-field Equalization), a critic-centric regularization framework that treats the critic as a scalar field and stabilizes its induced action-gradient field. PAVE rectifies the learning signal by minimizing the Q-gradient volatility while preserving local curvature. Experimental results demonstrate that PAVE achieves smoothness and robustness comparable to policy-side smoothness regularization methods, while maintaining competitive task performance, without modifying the actor.

</details>


### [146] [Mano: Restriking Manifold Optimization for LLM Training](https://arxiv.org/abs/2601.23000)
*Yufei Gu,Zeke Xie*

Main category: cs.LG

TL;DR: 提出 Mano：基于流形优化的优化器，通过将动量投影到模型参数的切空间并在旋转的 Oblique 流形上约束，实现对曲率信息的保留与全局结构的利用，以提升大模型训练的效率和性能，超越 AdamW 和 Muon。


<details>
  <summary>Details</summary>
Motivation: 现有优化器在大规模语言模型训练中存在的两大局限：AdamW 依赖对角化曲率估计且忽视结构属性，Muon 在全局谱归一化时损失了部分曲率信息。流形优化在大规模模型优化中长期未被广泛应用，因其性能不如主流优化器。需要一种能兼顾曲率信息、结构信息与大规模训练可行性的新思路。

Method: 在模型参数的切空间对动量进行投影，并在旋转的 Oblique 流形上对其进行约束，从而构造出新的流形优化器 Mano。该方法把传统更新与流形约束结合，保留曲率信息并兼顾参数结构，首次达到在大模型上缩小与现代优化器之间的性能差距。

Result: 在 LLaMA 与 Qwen3 的大量实验中，Mano 持续且显著地优于 AdamW 与 Muon，同时在内存与计算复杂度方面具有更优的表现，扩展了空间-时间效率的帕累托前沿。

Conclusion:  Mano 证明了在大规模语言模型训练中，流形优化方法可以超越主流优化器，提出了一个高效且具扩展性的优化范式，具有潜在的广泛应用前景。

Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.

</details>


### [147] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: CFPO introduces a clipping-free policy optimization by using a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that stabilizes policy updates without hard boundaries. It matches clipping-based methods in reasoning, improves stability, reduces verbosity exploitation and capability degradation in alignment, and requires only a one-line code change with no extra hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Clipping in policy optimization for post-training large language models causes optimization issues at scale, such as zero-gradient regions, reward hacking, and training instability. A smooth, differentiable alternative is sought to maintain performance while enhancing stability and alignment safety.

Method: Replace heuristic clipping with a convex quadratic penalty based on Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard clipping boundaries.

Result: In reasoning tasks, CFPO matches clipping-based methods on downstream benchmarks while expanding the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, with competitive instruction-following performance. It requires only a one-line code change and no additional hyperparameters.

Conclusion: CFPO is a promising drop-in alternative to clipping-based methods for post-training LLMs, offering stable training and competitive performance without explicit clipping.

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [148] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 提出 Continuous Constraint Interpolation (CCI) 与 Automatic Constraint Policy Optimization (ACPO)，统一离线强化学习中的三类约束（行为克隆加权、密度正则、支持约束）为一个连续约束谱，并通过对偶更新自适应插值参数，实现跨约束的平滑切换与组合。在 D4RL、NeoRL2 上展现出鲁棒性提升并达到较优性能。


<details>
  <summary>Details</summary>
Motivation: 离线RL中的外推误差由约束形式及其强度共同决定，现有方法多聚焦单一约束族，缺乏一个统一原理来解释约束之间的关系及其权衡。

Method: 提出一个单一的插值参数，将行为克隆、密度正则和支持约束作为谱上的不同点进行连续插值；基于此构建 ACPO，采用原-对偶框架并通过拉格朗日对偶更新自适应调整插值参数。

Result: 给出最大熵性能差距引理以及闭式最优策略和其参数投影的性能下界；在 D4RL 与 NeoRL2 上获得鲁棒性提升，并实现总体上的最优/接近最优性能。

Conclusion: 提供一个统一的约束框架来解释与连接现有的离线RL约束类型，理论与实验均支持该框架在多域上的有效性与鲁棒性。

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [149] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出 MS-EDEN 无偏量化，并整合成 Quartet II 的全 NVFP4 量化方案，支持端到端的全量化预训练，显著降低量化误差并提升训练速度。


<details>
  <summary>Details</summary>
Motivation: NVFP4 有望实现端到端全量化预训练；但现有量化训练多采用随机舍入(RS)，在表示能力和梯度估计方面仍有损失，需开发更低误差的无偏量化以提升在大模型上的表现。

Method: 提出 MS-EDEN 作为微尺度格式的无偏量化方法；将其整合到线性层的全 NVFP4 量化方案 Quartet II；从前向/反向矩阵乘法的梯度估计出发进行分析；并在 NVIDIA Blackwell GPU 上实现核函数，提供与 BF16 相比的显著加速；在 38B tokens 的数据集上对 1.9B 参数的端到端 LLM 进行验证。

Result: 在与 SR 相比，量化误差提升 >2x，且对主要矩阵乘法的梯度估计更可靠；端到端 LLM 训练（1.9B 参数、38B tokens）可行，核实现带来最高 4.2x 的 BF16 加速；代码开放。

Conclusion: Quartet II 与 MS-EDEN 共同推动 NVFP4 的端到端全量化训练成为现实，提升准确性和训练效率；适用于中等规模模型的加速训练，未来可扩展至更大模型和更多算子。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [150] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 提出一种基于卷积稀疏自编码器的两通道sEMG信号的手势识别框架，实现超低通道数下的高精度识别，并通过少样本迁移学习与增量学习实现对未见 Subject 的快速适应与对类别扩展的无重新训练更新。


<details>
  <summary>Details</summary>
Motivation: 解决肌电控制的跨个体差异与高密度传感阵列在临床中的不实用性，目标是在仅两通道sEMG的前提下实现高精度、可扩展且低开销的假肢控制系统。

Method: 使用卷积稀疏自编码器从原始sEMG信号中直接提取时序特征，省去手工特征工程；采用两通道输入；引入少样本迁移学习以适应未见主体，并通过增量学习策略实现从6类扩展到10类，且无需重新训练整个模型。

Result: 在6类手势上实现多被试F1-score 94.3%±0.3%；少样本迁移学习将未见主体的基线从35.1%±3.1%提升至92.3%±0.9%；增量学习实现10类手势的F1-score 90.0%±0.2%且无需全量模型重训练。

Conclusion: 该框架实现低传感开销下的高精度手势识别，具备良好的可扩展性与适应性，适合更经济、可自适应的肌电假肢系统。

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [151] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 提出一种级联扩散与流匹配的混合型表格数据生成方法：先生成低分辨率的纯分类特征及数值特征的粗糙分类表示，再在高分辨率阶段通过引导条件概率路径和数据相关耦合进行精细建模；对数值特征的低分辨率表示可显式处理缺失/异常等离散现象，从而提升混合型特征的生成保真度。理论上证明级联会收紧运输成本界；实证显示生成样本更真实，分布细节更清晰，如检测分数提升约40%。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据生成方法在混合离散与连续特征上存在困难，难以在单一特征中混合离散状态与连续分布；需要新的建模策略以提升对混合型特征的表达能力和样本质量。

Method: 采用级联策略：先生成低分辨率行（纯分类特征及数值特征的粗糙分类表示），再利用高分辨率的流匹配模型，结合引导条件概率路径和数据相关耦合进行高分辨率建模；低分辨率的数值特征显式编码离散结果（如缺失、异常值），从而提高混合型特征的生成保真度；给出运输成本界的收紧证明。

Result: 与基线相比，所提方法生成的样本更真实，能够更好地捕捉分布细节；在任务/指标上显示显著提升，例如检测分数提升约40%。

Conclusion: 级联生成框架有效提升混合离散-连续表格数据的生成质量，理论上收紧运输成本界，实验上实现更真实且分布细节更丰富的样本。

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [152] [Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference](https://arxiv.org/abs/2601.23039)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 论文揭示在熵正则化的可微匹配层中，随着退火 ε→0，推断很容易陷入非真实的局部基底（Premature Mode Collapse），并给出热力学速度极限理论来解释现象。提出 Efficient PH-ASC 自适应调度以稳定推断，降低谱诊断成本，训练开销从 O(N^3) 降至摊销 O(1)，并提供实现与演示。


<details>
  <summary>Details</summary>
Motivation: 可微分匹配层（通常通过熵正则化的最优传输实现）在结构预测中作为近似推断的关键机制。然而，直接把离散置换通过 ε→0 的退火恢复往往不稳定，容易发生模式坍塌（mode collapse）。需要一个理论框架来解释这种非正常动力学，并给出稳定的、可在训练循环中实现的自适应调度策略。

Method: 对 Sinkhorn 固定点映射的非正则（非对称/非规范）动力学进行分析，提出热力学速度极限，证明在标准指数降温下，目标后验的变化尺度为 O(1) 而推断算子的收缩速率为 O(1/ε)，二者失配导致推断轨迹进入伪局部基。基于此提出 Efficient PH-ASC，自适应调度算法，监控推断过程的稳定性，采用线性稳定性规律替代昂贵的谱诊断，在训练循环中实现近似的稳定性评估，开销降至摊销常数级。

Result: 给出实现与互动演示的公开资源链接；理论分析指明了退火过程中的稳定性瓶颈，并给出能显著降低计算开销的调度策略。

Conclusion: 该工作揭示了熵正则化匹配层在退火过程中的基本限制，并提供一个可实际部署的自适应稳定性调度框架（PH-ASC），提升可训练性与推断稳定性，并附带实现与演示资源。

Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $ε\to 0$ is notoriously unstable. We identify a fundamental mechanism for this failure: \textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical \textbf{thermodynamic speed limit}. Under standard exponential cooling, the shift in the target posterior ($O(1)$) outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. This mismatch inevitably forces the inference trajectory into spurious local basins. To address this, we propose \textbf{Efficient PH-ASC}, an adaptive scheduling algorithm that monitors the stability of the inference process. By enforcing a linear stability law, we decouple expensive spectral diagnostics from the training loop, reducing overhead from $O(N^3)$ to amortized $O(1)$. Our implementation and interactive demo are available at https://github.com/xxx0438/torch-sinkhorn-asc and https://huggingface.co/spaces/leon0923/torch-sinkhorn-asc-demo. bounded away from zero in generic training dynamics unless the feature extractor converges unrealistically fast.

</details>


### [153] [Adaptive Edge Learning for Density-Aware Graph Generation](https://arxiv.org/abs/2601.23052)
*Seyedeh Ava Razi Razavi,James Sargant,Sheridan Houghten,Renata Dividino*

Main category: cs.LG

TL;DR: A WGAN-based graph generator with a learnable distance-based edge predictor and a density-aware mechanism yields realistic, class-consistent graphs with stable training.


<details>
  <summary>Details</summary>
Motivation: Generating graphs is challenging due to discrete topology, varying sizes, and class-specific connectivity patterns. Existing GAN-based graph generators rely on random edge sampling with fixed probabilities, which struggles to capture complex dependencies.

Method: A density-aware conditional graph generation framework using a Wasserstein GAN with gradient penalty. Nodes are embedded in a latent space; a differentiable, distance-based edge predictor derives pairwise edges from node embeddings. A density-aware selection mechanism controls edge density to match class-specific sparsity distributions. A GCN-based critic enforces realistic topology and alignment with target class distributions.

Result: Experiments show superior structural coherence and class-consistent connectivity compared to baselines. The learned edge predictor captures complex relational patterns beyond heuristics; the generated graphs have densities and topologies closely matching real distributions. Improved training stability and controllable synthesis. Code available at the provided URL.

Conclusion: The framework is effective for realistic graph generation and data augmentation, offering controllable synthesis and better alignment with real-world structural distributions.

Abstract: Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at https://github.com/ava-12/Density_Aware_WGAN.git.

</details>


### [154] [Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA](https://arxiv.org/abs/2601.22828)
*Zhan Fa,Yue Duan,Jian Zhang,Lei Qi,Wanqi Yang,Yinghuan Shi*

Main category: cs.LG

TL;DR: 提出一个基于LoRA的Rank-1 Expert Pool框架，通过动态稀疏组合和AGO正交损失实现VLM的持续学习，达到SOTA并显著降低可训练参数且无额外延迟。


<details>
  <summary>Details</summary>
Motivation: 解决VLM在持续学习中的灾难性遗忘与高推理成本问题，减少对外部数据和任务ID识别的依赖，实现参数高效的领域自适应。

Method: 将单个LoRA重组为可分解的Rank-1专家池，通过从专家池中选择进行任务特定更新的稀疏组合；以[CLS]语义为引导选择；提出Activation-Guided Orthogonal（AGO）损失，使跨任务的LoRA权重关键部分正交化。

Result: 在多种设置下达到当前最佳（SOTA）指标，泛化能力超越零-shot上限；与基线相比可训练参数减少约96.7%；不依赖外部数据或任务ID分类器；合并后的LoRA权重更少、无推理延迟，计算开销低。

Conclusion: 该方法实现了高效的领域感知持续学习，降低任务间干扰，保持下游任务性能，具备实际部署潜力。

Abstract: Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.

</details>


### [155] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: 提出一种潜在流式匹配框架，通过在预训练自编码器上引入等变性正则化来显式鼓励等变性，从而在时间序列生成中获得更好的生成质量和更快的采样速度。


<details>
  <summary>Details</summary>
Motivation: 时间序列生成中的潜在表示需要具备良好几何不变性，以提升生成质量与采样效率；现有扩散模型、低维潜在空间方法缺乏对等变性的系统约束。

Method: 在一个潜在流模型中，通过对自编码器进行简单的等变性正则化来微调潜在空间。具体做法是引入等变性损失，确保变换后信号与其重构之间的一致性，并对基本时间序列变换（如平移、幅度缩放）进行训练以增强潜在空间的等变性。与此相容的潜在流模型保留其计算优势，且在多数据集上与现有扩散基线相比，在标准评估指标上性能更好，且采样速度快数量级提高。

Result: 在多组真实世界数据集上，等变性正则化的潜在空间在生成质量上优于现有扩散基线，且采样速度数量级提高；证实了将几何先验引入潜在生成模型的实用性。

Conclusion: 将几何归纳偏置引入潜在生成模型（尤其是时间序列的潜在流模型）可显著提升生成质量并实现高效采样，具有实际应用价值和研究意义。

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [156] [To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series](https://arxiv.org/abs/2601.23114)
*Jiaming Ma,Siyuan Mu,Ruilin Tang,Haofeng Ma,Qihe Huang,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 提出 Evolutionary Forecasting (EF) 作为统一的生成框架，证明 DF 是 EF 的退化特例；单一 EF 模型超越任务特定 DF 集成，在极端外推中也具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DF 将输出与评估 horizon 强绑定，带来重新训练成本及优化病态；需要一个解耦输出和评估、可自适应演化推理的框架。

Method: 提出 EF，具生成能力的时间序列演化框架，将 DF 作为特例；在短 horizon 上训练并通过演化/生成机制覆盖更长 horizon；给出理论证明 EF 的统一性；通过大量基准实验验证。

Result: EF 单模型超越 DF 集成、在标准基准上表现优于 DF；在极端外推下具有稳定性。

Conclusion: 促进 LTSF 领域从静态映射转向自适应的 Evolutionary Reasoning；EF 提供统一、鲁棒且减少重复训练的方案。

Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.

</details>


### [157] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出了 OptiMAG 框架，使用未平衡最优传输正则化与 FGW 距离来对跨模态局部邻域的结构一致性进行对齐，从而缓解显式图结构与模态嵌入语义冲突，并提升多模态图任务表现。


<details>
  <summary>Details</summary>
Motivation: 在多模态图（MAG）中，不同模态嵌入对相邻关系的语义距离可能不一致，直接沿显式图结构聚合会引入模态特定噪声，阻碍有效的节点表征学习。

Method: 提出 OptiMAG，通过未平衡最优传输约束的融合 Gromov-Wasserstein 距离（FGW）来显式约束跨模态局部邻域的结构一致性，并引入 KL 散度惩罚以自适应处理跨模态不一致；该框架可作为可插拔的正则器嵌入现有多模态图模型。

Result: 实验表明 OptiMAG 在多项任务上优于基线，包括图相关任务（节点分类、链接预测）和多模态生成任务（graph2text、graph2image），且源代码将在接受后公开。

Conclusion: OptiMAG 能有效缓解结构-语义冲突，作为通用正则组件提升多模态图模型的表现，且易于集成到现有模型。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [158] [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)
*Zhanglu Yan,Kaiwen Tang,Zixuan Zhu,Zhenyu Bai,Qianhui Liu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Matterhorn是一种在Transformer框架中融入SNN的方案，通过M-TTFS编码和MSU硬件，加上CIM，实现显著的能效提升并在GLUE上达到新SOTA，准确性提升1.42%，能效提升2.31x。


<details>
  <summary>Details</summary>
Motivation: 现有SNN能耗评估多聚焦累积运算，忽略数据搬运等现实硬件成本，数据搬运可占总能耗约80%。需要同时降低信息搬运和权重访问成本。

Method: 设计M-TTFS编码，使用掩码策略将无能耗静默状态映射为数据分布最常见的膜电位，而不是最低，配合dead zone策略将一定区间的数值映射到静默状态以提高稀疏性；硬件方面，MSU采用计算在内存(CIM)来在内存中执行模拟积分，消除权重访问成本。

Result: 在GLUE上达到新SOTA，平均准确度提升1.42%，能效提升2.31x。

Conclusion: 通过把神经元编码与硬件结合，显著降低数据搬运和权重访问成本，实现能效与准确率并进。

Abstract: Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 80% of the total energy. In this paper, we propose Matterhorn, a spiking transformer that integrates a novel masked time-to-first-spike (M-TTFS) encoding method to reduce spike movement and a memristive synapse unit (MSU) to eliminate weight access overhead. M-TTFS employs a masking strategy that reassigns the zero-energy silent state (a spike train of all 0s) to the most frequent membrane potential rather than the lowest. This aligns the coding scheme with the data distribution, minimizing spike movement energy without information loss. We further propose a `dead zone' strategy that maximizes sparsity by mapping all values within a given range to the silent state. At the hardware level, the MSU utilizes compute-in-memory (CIM) technology to perform analog integration directly within memory, effectively removing weight access costs. On the GLUE benchmark, Matterhorn establishes a new state-of-the-art, surpassing existing SNNs by 1.42% in average accuracy while delivering a 2.31 times improvement in energy efficiency.

</details>


### [159] [Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures](https://arxiv.org/abs/2601.23147)
*Saeid Jamshidi,Omar Abdul Wahab,Rolando Herrero,Foutse Khomh*

Main category: cs.LG

TL;DR: STGAT is a Spatio-Temporal Graph Attention Network for energy IoT time integrity, capturing drift, synchronization errors, and overflows; uses drift-aware temporal embeddings, temporal self-attention, and graph attention with curvature-regularized latent space; achieves 95.7% accuracy and 26% faster detection on perturbed energy telemetry, outperforming RNN/Transformer/Graph baselines.


<details>
  <summary>Details</summary>
Motivation: IoT energy systems suffer clock drift, time-synchronization manipulation, and timestamp overflow (Y2K38), which disrupt temporal ordering and break assumptions of timestamp reliability in anomaly detection.

Method: STGAT combines drift-aware temporal embeddings and temporal self-attention to model per-device time evolution, and employs graph attention to capture spatial propagation of timing errors; introduces a curvature-regularized latent representation to geometrically separate normal clock evolution from anomalies (drift, offsets, overflow).

Result: In controlled experiments with timing perturbations on energy IoT telemetry, STGAT achieves 95.7% accuracy, outperforms recurrent, transformer, and graph-based baselines (d>1.8, p<0.001); reduces detection delay by 26% to 2.3 time steps, robust to overflow, drift, and physical inconsistencies.

Conclusion: STGAT effectively models spatio-temporal timing anomalies in energy IoT, offering improved detection accuracy and timeliness over traditional sequence/graph models.

Abstract: The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies.

</details>


### [160] [On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care](https://arxiv.org/abs/2601.23154)
*Joel Romero-Hernandez,Oscar Camara*

Main category: cs.LG

TL;DR: 在部分观测条件下，使用深度强化学习进行 ICU 镇痛镇静药物给药策略时，仅优化痛感降低可能增加死亡风险，而同时优化痛感与死亡率可实现痛感降低与死亡率下降，强调将长期结局纳入目标以提高安全性。


<details>
  <summary>Details</summary>
Motivation: 治疗目标与患者安全之间存在权衡；现有RL在不完美信息下容易错过长期结局；需要评估设计选择的风险，特别是是否将长期生存结果纳入优化。

Method: 基于MIMIC-IV数据库的47144次ICU住院数据，建立深度强化学习框架，在部分可观测情境下按小时给药，覆盖阿片类、丙泊酮、苯二氮卓类、右美托咪定；比较两种目标：仅降低疼痛 vs 同时降低疼痛与死亡率。

Result: 两种策略均与降低疼痛相关，但仅优化疼痛的策略与死亡率呈正相关；同时优化疼痛与死亡率的策略与死亡率呈负相关，并实现疼痛降低与死亡下降的双重收益。

Conclusion: 强调长期结局在安全治疗策略中至关重要；若仅以短期目标优化，可能损害生存结局。含不完备信息的RL应将长期结果纳入目标设计。

Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.

</details>


### [161] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: 提出 PlatoLTL，通过把命题视为参数化谓词，实现对未见命题的零-shot泛化，以及对 LTL 结构的组合泛化，在多任务强化学习中实现对新任务的通用策略。


<details>
  <summary>Details</summary>
Motivation: 当前多任务强化学习在对未见任务的泛化能力有限，尤其面对新符号/命题的泛化挑战。虽然 LTL 提供结构化的任务描述，但现有方法通常只能泛化到已见符号，难以处理外延变化。需要一种在结构与符号两层实现零-shot 泛化的方法。

Method: 提出 PlatoLTL，将命题视为参数化谓词的实例，允许在不同命题之间共享结构；设计嵌入与组合谓词的架构来表示 LTL 规范；对 LTL 公式进行嵌入编码并用于策略学习，使策略具备对未见命题的零-shot泛化能力。

Result: 在挑战性环境中实现对新命题和新任务的零-shot泛化，显示对 LTL 结构和命题的两层泛化能力，并优于仅对符号进行泛化的方法。

Conclusion: 通过参数化谓词与统一的嵌入-组合框架，PlatoLTL 实现了对 LTL 指导的多任务强化学习的更强泛化能力，扩展了符号层面的零-shot 泛化范围。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [162] [Probing the Trajectories of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2601.23163)
*Marthe Ballon,Brecht Verbeken,Vincent Ginis,Andres Algaba*

Main category: cs.LG

TL;DR: 提出一种在LLM推理轨迹中进行截断与再注入的诊断协议，通过固定token百分比截断推理链并测量再注入后的答案分布，发现准确性与决策承诺随提供推理令牌比例增加而提高，且提升来自相关内容而非简短长度或风格效应。更强模型能从错误的部分轨迹中回退，但初始回答常受弱模型错误的锚定。该方法可用于安全、高效部署的轨迹诊断与监控策略。


<details>
  <summary>Details</summary>
Motivation: 系统性理解LLM推理轨迹中“原因-结果”链条的有效性，回答随推理过程的变化如何改变量，以及中间推理段是否提供超越表面长度的答案信息；并评估是否需要信任中间轨迹作为解释。

Method: 对模型生成的推理轨迹进行分段截断（按固定token百分比），再将部分轨迹注入回模型（同模型或对照模型），以下一个token概率分布测量答案分布。实验对象涵盖开源模型Qwen3-4B/-8B/-14B与gpt-oss-20b/-120b，在GPQA Diamond与MMLU-Pro基准上进行评估。

Result: 随着提供的推理令牌比例增大，准确性与决策承诺持续提高；增益主要来自相关内容（而非仅仅上下文长度或通用“推理风格”效应）。更强模型在部分轨迹上能成功回退错误，但即时答案仍常被较弱模型的错误所锚定。

Conclusion: 轨迹诊断为更高效、更加安全的推理模型部署提供诊断工具，计量结果可用于设计中间轨迹的处理与监控策略，从而提升可靠性，而不假设中间令牌就是可信的解释。

Abstract: Large language models (LLMs) increasingly solve difficult problems by producing "reasoning traces" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic "reasoning style" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.

</details>


### [163] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 提出一种基于正则化的多变量校准方法，利用前排序函数在训练阶段强调多变量校准，并引入基于PCA的前排序以投影到预测分布的主方向；在仿真和18个真实数据集上验证可显著提升多变量前排序的校准且不损害预测准确性，同时PCA前排序揭示现有前排序无法检测的依赖结构错配。


<details>
  <summary>Details</summary>
Motivation: 在多变量预测中实现校准比单变量更具挑战；已有的前排序函数主要用于事后评估，需在训练阶段引入校准约束以提升整体可靠性。

Method: 在多变量分布回归模型的训练中引入正则化损失，结合前排序函数实现多变量校准；提出基于PCA的前排序，将预测投影到预测分布的主成分方向进行校准诊断。

Result: 实验表明该方法显著提升多变量前排序的校准性且保持预测准确度；PCA前排序揭示了依赖结构的错配，这一信息在现有前排序中难以获得。

Conclusion: 在训练阶段纳入前排序的正则化是提升多变量概率预测校准的有效路线；PCA前排序提供了对依赖结构的强诊断能力，具有广泛应用前景。

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [164] [Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization](https://arxiv.org/abs/2601.23174)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: Dynamic tokenization for neural audio codecs enabling variable-frame-rate tokenization via soft character-level alignment and duration modeling; adds retrieval-augmented decoding to boost low-frame-rate quality; achieves competitive results with far fewer tokens than fixed-frame-rate codecs.


<details>
  <summary>Details</summary>
Motivation: Fixed-frame-rate neural audio codecs waste tokens by allocating them uniformly in time, leading to inefficiency. Variable-frame-rate tokenization aligned to linguistic units and controllable decoding duration can improve bitrate efficiency and flexibility. Retrieval augmentation may further boost quality at low frame rates.

Method: DyCAST uses soft character-level alignment and explicit duration modeling to learn token-to-character alignment during training; alignment-free inference with direct control over token durations at decoding time. A retrieval-augmented decoding mechanism is introduced to improve resynthesis fidelity at low frame rates without increasing bitrate.

Result: DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs; retrieval-augmented decoding improves reconstruction quality at low frame rates without increasing bitrate.

Conclusion: Demonstrates feasibility of dynamic, character-aligned tokenization for neural audio codecs; enables finer control over decoding and bitrate efficiency; retrieval augmentation provides a practical boost for low-rate configurations.

Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.

</details>


### [165] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: 本研究提出一种将单棵决策树与高斯过程叶子结合的贝叶斯模型，在叶子处引入高斯过程预测以实现更好的外推和不确定性校准，并且通过贝叶斯斜向分裂实现不确定性感知的输入划分，包含一个门控机制在输入超出叶子训练范围时激活 GP 外推。实验显示在回归任务上优于标准变分斜向树，且在外推情景中有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的回归树在外推和不确定性校准方面表现不足；分段常数叶子被训练目标约束，面临分布漂移时易过度自信。需要一种能在叶子层面进行局部建模并具备跨越观察区间外推能力的结构。

Method: 在 VSPYCT 基础上，提出单棵树的贝叶斯模型，为每个叶子配备高斯过程预测器；使用贝叶斯 oblique splits 进行不确定性感知的输入空间划分；推断通过对分裂参数的后验采样与 GP 后验预测相结合；引入门控机制，当输入超出叶子训练覆盖时，激活 GP 外推；实现高效的推断与预测。

Result: 在基准回归任务上，所提模型的预测性能优于标准的变分 oblique 树；在外推情景下获得显著的性能提升。

Conclusion: 将 GP 叶子与贝叶斯斜向分裂结合，获得更好的不确定性校准和外推能力；高效的推断策略使得在单棵树框架内实现高质量的 GP 外推成为可能。

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [166] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: 提出 FlexLoRA，一种基于熵的灵活低秩自适应的PEFT方法，通过谱能量熵评估矩阵重要性，支持全局预算下的秩裁剪与扩张，并对新加的奇异方向进行零影响初始化，在精度、稳定性和灵活性方面优于LoRA及其他动态秩方法。


<details>
  <summary>Details</summary>
Motivation: 全量微调成本高昂，LoRA 的固定秩限制灵活性；现有动态秩方法多以逐元素度量进行全局排序，缺乏矩阵层面的区分与在需要时扩容的机制，因此难以在复杂模型上取得更优表现与稳定性。

Method: FlexLoRA 的核心在于：1) 以谱能量熵评估矩阵重要性；2) 支持在全局预算下进行秩的剪枝和扩张；3) 对新加入的奇异方向采用零影响初始化以确保训练稳定；4) 通过改进的粒度控制、灵活性与稳定性设计实现对大规模模型的高效PEFT。

Result: 在多个基准上持续优于最新基线方法，表现稳健，代码公开。

Conclusion: FlexLoRA 提供一个更 principled 的 PEFT 框架，解决粒度、灵活性和稳定性三大挑战，适用于大规模化模型的高效微调。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [167] [Agile Reinforcement Learning through Separable Neural Architecture](https://arxiv.org/abs/2601.23225)
*Rajib Mostakim,Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: SPAN is a spline-based adaptive network for reinforcement learning that improves parameter/learning efficiency by integrating a learnable preprocessing layer with a separable tensor-product B-spline basis into the KHRONOS framework, achieving higher sample efficiency and success rates than MLP baselines across both online (PPO/SAC) and offline (Minari/D4RL) tasks.


<details>
  <summary>Details</summary>
Motivation: MLPs in RL often have poor inductive bias for the smooth structure of value functions, leading to parameter inefficiency and slower sample efficiency in resource-constrained settings. Model compression is post-hoc and does not improve learning efficiency. Although spline-based architectures offer parameter efficiency, they suffer from computational overhead at scale. A need exists for a parameter-efficient, computationally practical function approximator that preserves or improves learning performance in RL.

Method: SPAN adapts the low-rank KHRONOS framework by adding a learnable preprocessing layer coupled with a separable tensor-product B-spline basis. It is evaluated on discrete-action PPO and high-dimensional continuous-action SAC, as well as offline settings (Minari/D4RL).

Result: SPAN yields a 30-50% improvement in sample efficiency and 1.3-9x higher success rates compared to MLP baselines. It also shows superior anytime performance and robustness to hyperparameter variation, indicating strong practical viability for learning efficient policies in resource-limited RL scenarios.

Conclusion: SPAN provides a viable, high-performance alternative to traditional MLP function approximators for RL in resource-constrained settings by combining learnable preprocessing with spline-based separable architectures, offering improved sample efficiency, stability across tasks and hyperparameters, and applicability to online and offline RL.

Abstract: Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although model compression techniques exist, they operate post-hoc and do not improve learning efficiency. Recent spline-based separable architectures - such as Kolmogorov-Arnold Networks (KANs) - have been shown to offer parameter efficiency but are widely reported to exhibit significant computational overhead, especially at scale.
  In seeking to address these limitations, this work introduces SPAN (SPline-based Adaptive Networks), a novel function approximation approach to RL. SPAN adapts the low rank KHRONOS framework by integrating a learnable preprocessing layer with a separable tensor product B-spline basis. SPAN is evaluated across discrete (PPO) and high-dimensional continuous (SAC) control tasks, as well as offline settings (Minari/D4RL). Empirical results demonstrate that SPAN achieves a 30-50% improvement in sample efficiency and 1.3-9 times higher success rates across benchmarks compared to MLP baselines. Furthermore, SPAN demonstrates superior anytime performance and robustness to hyperparameter variations, suggesting it as a viable, high performance alternative for learning intrinsically efficient policies in resource-limited settings.

</details>


### [168] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: 提出 DC-LA：对非光滑DC型正则化的采样问题，利用 Moreau 包络平滑 r1、r2，并将正则的凹部分并入数据保真项，提出近端 Langevin 算法并给出收敛性证明，在 q-Wasserstein 距离下对 π 收敛，误差来自离散化与平滑；若 V 为远耗散，则适用。数值表明在合成与真实 CT 中具有良好不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 解决非对数-凸目标分布下的采样问题，其中正则项为 DC 形式且含非光滑性；通过对 DC 结构进行平滑化并重新分配凹性，以扩展现有对非对数-凸采样的理论与方法。

Method: 对 r1、r2 分别应用 Moreau 包络进行平滑；按照 DC 编程思想，将正则中凹的部分并入数据保真项，提出近端 Langevin 算法 DC-LA；在假设 V 远耗散的前提下，给出在 q-Wasserstein 距离下关于离散化和平滑误差的收敛性证明，覆盖所有正整数 q；并进行数值实验以验证分布近似与在 CT 应用中的不确定性量化。

Result: 理论上证明 DC-LA 能收敛到目标分布 π，误差源包括平滑与离散化；在 q-Wasserstein 距离下成立，前提为 V 远耗散； experimentally 展示在合成数据和真实 CT 应用中能够提供较准确的分布与不确定性量化。

Conclusion: 将 DC 结构与近端 Langevin 框架结合，提供一个更一般化的非对数-凸采样框架，利用 Moreau 平滑与凹部分重新分配实现收敛性，适用性覆盖多类非光滑 DC 正则化，并在实际应用中展现稳定性与有效性。

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [169] [YuriiFormer: A Suite of Nesterov-Accelerated Transformers](https://arxiv.org/abs/2601.23236)
*Aleksandr Zimin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 将变换器层视为对嵌入的优化迭代：自注意力执行互作用能的梯度步，MLP执行势能的梯度更新；通过李-托特分裂实现对合成目标的梯度下降，进而设计出带有Nesterov加速的变换器并在 TinyStories/OpenWebText 上优于 nanoGPT。


<details>
  <summary>Details</summary>
Motivation: 提供一种统一的优化理论视角来理解和设计 Transformer 架构，利用经典优化方法（如 Lie–Trotter 分裂、Nesterov 加速）提升性能与设计原则。

Method: 定义交互能与势能两种能量函数，自注意力为互作用能的梯度步，MLP 为势能的梯度更新；对两者组成的目标函数进行梯度下降，采用 Lie–Trotter 分裂实现两步更新的近似；在此框架下提出保持相同注意力与MLP“oracle”的 Nesterov 风格加速变换器。

Result: 所提出的加速变换器在 TinyStories 与 OpenWebText 数据集上相对于 nanoGPT 基线表现出一致的提升。

Conclusion: 基于优化理论的视角可为 Transformer 架构提供 principled 的设计路径，Nesterov 加速等经典优化工具可在保持核心算子通用性的前提下提升实际性能。

Abstract: We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, implemented via Lie--Trotter splitting between these two energy functionals. This perspective enables principled architectural design using classical optimization ideas. As a proof of concept, we introduce a Nesterov-style accelerated transformer that preserves the same attention and MLP oracles. The resulting architecture consistently outperforms a nanoGPT baseline on TinyStories and OpenWebText, demonstrating that optimization-theoretic insights can translate into practical gains.

</details>


### [170] [Scalable Topology-Preserving Graph Coarsening with Graph Collapse](https://arxiv.org/abs/2601.22943)
*Xiang Wu,Rong-Hua Li,Xunkai Li,Kangfei Zhao,Hongchao Qin,Guoren Wang*

Main category: cs.LG

TL;DR: 提出 STPGC，一种可扩展的拓扑保留图卷积简化框架，通过强塌缩和边塌缩等操作在不损失GNN感受野的前提下高效地去除冗余节点和边，并提供近似算法加速训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法多偏向保持谱属性或时空特性，尽管保持拓扑特征有助于GNN性能，但现有拓扑保留方法往往计算复杂度指数级，难以用于大规模图。

Method: 引入图强塌缩（GStrongCollapse）和图边塌缩（GEdgeCollapse）等概念，扩展自代数拓扑，提出三种算法：GStrongCollapse、GEdgeCollapse、NeighborhoodConing，用于消除被支配的节点和边，并严格保留拓扑特征；给出理论证明 STPGC 能保持GNN的感受野；并给出近似算法以加速训练。

Result: 在节点分类任务上，使用GNN进行实验，结果显示 STPGC 在效率和性能上均有提升，证明方法的实用性。

Conclusion: STPGC 提供了可扩展且拓扑保留的图卷积简化方法，具备理论保障（感受野保持）与实际加速潜力，适用于大规模图的GNN训练与推断。

Abstract: Graph coarsening reduces the size of a graph while preserving certain properties. Most existing methods preserve either spectral or spatial characteristics. Recent research has shown that preserving topological features helps maintain the predictive performance of graph neural networks (GNNs) trained on the coarsened graph but suffers from exponential time complexity. To address these problems, we propose Scalable Topology-Preserving Graph Coarsening (STPGC) by introducing the concepts of graph strong collapse and graph edge collapse extended from algebraic topology. STPGC comprises three new algorithms, GStrongCollapse, GEdgeCollapse, and NeighborhoodConing based on these two concepts, which eliminate dominated nodes and edges while rigorously preserving topological features. We further prove that STPGC preserves the GNN receptive field and develop approximate algorithms to accelerate GNN training. Experiments on node classification with GNNs demonstrate the efficiency and effectiveness of STPGC.

</details>


### [171] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Wang Yuanchao,Lai Zhao-Rong,Zhong Tianqi,Li Fengnan*

Main category: cs.LG

TL;DR: 提出 ECTR 框架：将基于全变量变差（TV）的不变性学习与面向环境条件的尾部重加权相结合，以同时应对环境层级相关偏差和每个环境内的样本异质性；在无环境标注时通过极小极大（minimax）推断潜在环境。实证显示对最坏环境与平均 OOD 均有提升。


<details>
  <summary>Details</summary>
Motivation: 在模型面临跨环境的相关性偏移与由稀有或难样本驱动的环境内多样性偏移时，现有 IRM 主要处理环境层面的伪相关性、忽略环境内的样本异质性，导致 OOD 性能受限。需要一个统一框架同时处理两类分布偏移。

Method: 提出 Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization (ECTR)：将基于全变量变差的不变性学习与环境条件的尾部重加权结合，确保对环境层面的不变性与对环境内极端/罕见样本的鲁棒性互为补充。若无显式环境标注，通过极小极大框架推断潜在环境以实现无监督情形下的环境条件建模。

Result: 在回归、表格、时间序列和图像分类等多种任务的混合分布偏移下进行实验，ECTR 在最坏环境和平均 OOD 性能上均表现出稳健提升。

Conclusion: 环境层级的不变性与环境内鲁棒性是处理混合分布偏移的互补机制；通过尾部再加权与潜在环境推断，ECTR 能在缺乏明确环境标注的情况下也实现有效的 OOD 泛化提升。

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose \emph{Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization} (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [172] [TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training](https://arxiv.org/abs/2601.23261)
*Ruijie Zhang,Yequan Zhao,Ziyue Liu,Zhengyang Wang,Dongyang Li,Yupeng Su,Sijia Liu,Zheng Zhang*

Main category: cs.LG

TL;DR: TEON extends Muon by modeling neural network gradients as a structured higher-order tensor and applying gradient orthogonalization across layers jointly, yielding improved convergence guarantees and training dynamics.


<details>
  <summary>Details</summary>
Motivation: Layer-wise gradient orthogonalization (MuOn) ignores cross-layer gradient structure; exploiting a higher-order tensor representation can lead to better convergence, stability, and performance for large language models.

Method: Model gradients as a structured higher-order tensor and perform orthogonalization across the tensor. Provide a convergence guarantee surpassing layer-wise Muon. Develop a practical TEON instantiation guided by theoretical analysis, accompanied by ablation studies.

Result: TEON consistently improves training and validation perplexity across GPT-style (130M–774M) and LLaMA-style (60M–1B) models and shows strong robustness under various approximate SVD schemes.

Conclusion: TEON offers a principled generalization of Muon with improved convergence guarantees and practical performance gains, scalable across model sizes and robust to gradient-orthogonalization approximations.

Abstract: The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonalization beyond individual layers by modeling the gradients of a neural network as a structured higher-order tensor. We present TEON's improved convergence guarantee over layer-wise Muon, and further develop a practical instantiation of TEON based on the theoretical analysis with corresponding ablation. We evaluate our approach on two widely adopted architectures: GPT-style models, ranging from 130M to 774M parameters, and LLaMA-style models, ranging from 60M to 1B parameters. Experimental results show that TEON consistently improves training and validation perplexity across model scales and exhibits strong robustness under various approximate SVD schemes.

</details>


### [173] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 提出在线性带目信号中的 Nash regret 以及 p-means regret 的新分析框架，解决了对环境维度 d 的次优依赖，给出阶最优的 Nash regret 上界；提出通用框架 FairLinBandit，可作为元算法叠加在任意线性带目信策略之上，并以 Phased Elimination 和 UCB 实现，给出对所有 p 的子线性 p-means 遗憾界；并通过真实数据集的线性带目信实例进行广泛实验，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决带有公平性考量的随机多臂带目信号中的性能指标问题，基于 Nash Social Welfare 的公平性目标，以及将该概念扩展到线性带目信号，提出一个统一且可实用的分析与算法框架。

Method: 引入新的分析工具，克服以往依赖受限的集中不等式在高维 d 下导致的次优界；提出 FairLinBandit 元框架，通过在任意线性带目信策略上叠加公平性约束实现；具体实例为在 Phased Elimination 与 Upper Confidence Bound 这两种基线算法上实现，推导出对整个 p 区间的子线性 p-均值遗憾界。

Result: 在线性带目信息下，给出阶最优的 Nash regret 上界；对于 p-均值遗憾，在全部 p 的范围内实现子线性界；实验证明在来自真实数据集的线性带宽实例上，方法优于现有的 state-of-the-art 基线。

Conclusion: 本文不仅解决了线性带盲下 Nash regret 的维度依赖问题，还提出了一个可泛化的公平性-效用统一框架 p-means regret；FairLinBandit 作为通用元算法具有良好性能与可扩展性，但仍需在更广的模型假设和实际系统中的实现成本等方面进一步研究。

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [174] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: 提出 dgMARK，一种解码引导的水印方法，针对离散扩散语言模型，通过控制解码顺序并以二进制哈希的奇偶约束选择高回报候选token来嵌入水印，检测通过奇偶匹配统计及滑动窗口方法，对后期编辑有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型的解码顺序可变导致新的水印通道，但现有方法未充分利用解码过程中的顺序敏感性。

Method: 引导未掩码顺序朝向符合二进制哈希诱导的奇偶约束的高回报分支；不重新加权模型概率；与常用解码策略兼容，亦可加入单步前瞻；

Result: 水印通过提升的奇偶匹配统计量检测；滑动窗口检测增强对插入、删除、替换、改写等后编辑的鲁棒性；

Conclusion: dgMARK提供可插拔、对多解码策略友好的解码引导水印方案，具对抗后编辑鲁棒性。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [175] [Value-at-Risk Constrained Policy Optimization](https://arxiv.org/abs/2601.22993)
*Rohan Tangri,Jan-Peter Calliess*

Main category: cs.LG

TL;DR: A VaR-CPO algorithm for risk-constrained policy optimization that directly handles Value-at-Risk constraints with a conservative, sample-efficient approach.


<details>
  <summary>Details</summary>
Motivation: VaR constraints in reinforcement learning are non-differentiable and hard to enforce; existing methods often fail to guarantee constraint satisfaction during training. A safe, data-efficient method is needed for applications where violating risk constraints is costly.

Method: Introduce VaR-CPO, which uses a one-sided Chebyshev surrogate based on the first two moments of the cost return to address non-differentiability of VaR. Extend the CPO framework to derive worst-case bounds on policy improvement and constraint violations, providing a tractable, conservative optimization under a trust-region scheme.

Result: Empirically demonstrate that VaR-CPO achieves a zero-violation property during training in feasible environments, indicating safe exploration and improved safety over baselines.

Conclusion: VaR-CPO offers a practical, theoretically-grounded approach to optimizing VaR-constrained policies, delivering safe exploration with provable bounds and improved empirical safety performance.

Abstract: We introduce the Value-at-Risk Constrained Policy Optimization algorithm (VaR-CPO), a sample efficient and conservative method designed to optimize Value-at-Risk (VaR) constraints directly. Empirically, we demonstrate that VaR-CPO is capable of safe exploration, achieving zero constraint violations during training in feasible environments, a critical property that baseline methods fail to uphold. To overcome the inherent non-differentiability of the VaR constraint, we employ the one-sided Chebyshev inequality to obtain a tractable surrogate based on the first two moments of the cost return. Additionally, by extending the trust-region framework of the Constrained Policy Optimization (CPO) method, we provide rigorous worst-case bounds for both policy improvement and constraint violation during the training process.

</details>


### [176] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: 提出一种同时建模测量错误与机制性转变的因果模型，利用潜在干预对潜在真值和观测变量进行建模，通过最大似然估计实现可辨识性和鲁棒性，在定位根因方面与现有方法持平，并可区分异常类型。


<details>
  <summary>Details</summary>
Motivation: 现实世界的异常可能来自测量误差或系统机制的变更，但现有方法往往只处理单一来源，无法同时识别两类异常及其类型。

Method: 定义一个包含潜在干预的因果模型，将外部观测到的异常视为对潜在真值和观测变量的潜在干预；给出可辨识性分析并提出基于最大似然估计的估计算法，且在未知DAG时具鲁棒性。

Result: 实验表明新方法在根因定位上与最先进方法持平，同时具备异常类型分类能力，对DAG未知时也保持鲁棒性。

Conclusion: 提供一个统一的因果框架来区分两类异常并定位根因，具有理论可辨识性与实际可实现性。

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [177] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: DC-CoT通过将推理拆分为可并行执行的子任务并由一个指挥者协调，显著降低长链式推理的最长路径（延迟）同时保持精度，在AIME 2024与HMMT 2025上实现与基线相当的准确率且最长路径降低35-40%。


<details>
  <summary>Details</summary>
Motivation: 长链式推理在LLM中普遍且导致高延迟；需要在不显著牺牲准确率的前提下降低推理延迟。

Method: 以DeepScaleR-1.5B-Preview为长CoT基准，先用小型精选演示集进行SFT训练以使模型具备分派并行子任务的能力；由于SFT会降低准确率，设计多阶段强化学习（RL）算法并结合数据过滤策略以恢复准确性并降低最长路径；在AIME 2024与HMMT 2025等基准上评估。

Result: 在与基线DeepScaleR-1.5B-Preview相当的准确率条件下，将最长路径长度降低约35-40%。

Conclusion: DC-CoT为低延迟的长链式推理提供了有效框架，通过分治式并行推理实现了显著的时延降低，同时保持了较高的推理准确性；代码、数据集和模型公开。

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [178] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 提出以相对排序来替代绝对分数的强化学习框架RLRR，并引入Listwise Ranking Reward Model以直接生成相对排序，从而缓解信号稀疏和奖励不稳定问题，在推理基准和开放式文本生成任务上对现有分组基线表现出稳定的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的基于分组的强化学习通常依赖绝对数值奖励；在可验证任务中评估高度稀疏，开放式场景中奖励模型的分数区间不稳定，导致基于组均值的优势估计不可靠。需要通过相对信号提升鲁棒性。

Method: 提出RLRR框架，将奖励塑形从绝对评分转为相对排序；并设计Listwise Ranking Reward Model以成对或多项的相对偏好直接生成相对排序信号，从原始评估转换为鲁棒的相对信号以进行分组优化。

Result: 实验结果显示RLRR在推理基线和开放式生成任务上对标准分组基线具有持续的性能提升。

Conclusion: 相对奖励缓解了信号稀疏和奖励不稳定的问题，使基于分组的强化学习在多种任务上更稳定地提升性能。

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [179] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: SplineFlow 通过 B-spline 插值的流匹配方法，联合建模跨观测的条件路径，以稳健地学习 irregular sampling 下的高阶动力学，解决多边约束下的路径构造难题；在确定性/随机动力系统及细胞轨迹推断等任务上显著优于基线，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有的流模型在建模动力学系统时对线性插值依赖导致难以捕捉真实状态演化，且高阶多项式易振荡且在不规则采样时表现差。统一地跨多观测点满足多边约束的路径构造具有挑战性。需要一个结合平滑性、稳定性和多边约束的结构化方法来学习复杂动力学。

Method: 提出 SplineFlow，通过 B-spline 插值来共同建模跨观测的条件路径，利用 B-spline 的平滑性与稳定性来学习复杂动力学，并在框架中确保满足多摩边缘约束（multi-marginal requirements）。该方法属于理论上有根据的流匹配算法，适用于连续性正则化的 CNF 结构。

Result: 在多种确定性和随机动力系统，以及细胞轨迹推断任务上，SplineFlow 相比现有基线实现显著改进。

Conclusion: SplineFlow 提供一个稳定、结构化的流匹配框架，借助 B-spline 的性质有效捕捉高阶动力学并满足多观测点约束，展现良好泛化与应用潜力；代码已在 GitHub 发布。

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [180] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: CATTO 引入一个校准感知的端到端 token 级训练目标，使预测置信度与实际正确性对齐，并可与原始偏好优化目标结合；同时提出 Confidence@k 的测试时缩放机制，在不降低任务准确性的前提下显著提升校准指标和输出选择的贝叶斯性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型往往对预测结果的置信度过高或过低，偏好对齐方法（如 DPO）可能打断预测概率与正确性的关系，导致高置信错、低置信对的情况增多。需一套能直接对齐置信度与正确性的训练目标。

Method: 提出 Calibration Aware Token-level Training Objective (CATTO)，在 token 级别引入校准感知的损失，并可与原有偏好优化目标联合使用。并提出 Confidence@k，在推理阶段利用已校准的 token 概率实现贝叶斯最优的输出令牌选择。

Result: 与直接偏好优化（DPO）相比，CATTO 在 in-distribution 和 out-of-distribution 上的期望校准误差(ECE)分别降低约 2.22%-7.61% 和 1.46%-10.44%；相比最强 DPO 基线，降低约 0.22%-1.24%（in）和 1.23%-5.07%（out）。在五个数据集上，任务准确率保持或略有提升，未以牺牲准确性为代价。

Conclusion: CATTO 能在提升置信度校准的同时保持任务性能，且 Confidence@k 为推理阶段的贝叶斯最优输出提供了可行的实现。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [181] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: DCR通过推导非同分布分布的精确形式来提高排序任务的置信区间效率； calibration样本的绝对排序在给定相对排序条件下服从负超几何分布；在保证覆盖率的前提下，平均预测集大小最多降低36%。


<details>
  <summary>Details</summary>
Motivation: 在真实世界的排序模型部署中，量化不确定性至关重要。传统的基于一致性预测（conformal prediction）在完整排序场景中依赖非同分数的上界，导致预测集过于保守、规模过大。需要更紧凑且依然有效的预测集合。

Method: 通过推导非同分数的精确分布来实现高效的构建。具体地，给定 calibration 样本的相对排序， calibration 样本的绝对排序遵从负超几何分布。DCR据此推导非同分数的分布并确定 conformal 阈值，进而给出预测集。理论上证明在 mild 假设下具有效覆盖且效率提升。

Result: 相较基线，DCR在效率上具有提升，且保持有效覆盖；实验显示平均预测集大小最多下降约36%。

Conclusion: 通过利用排名分布信息，DCR实现更紧凑的预测集与可靠覆盖的分布信息约束下的一致性预测。

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [182] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 提出一种对数据分布进行约束感知的修改，以解决等式约束下的生成建模局限；在保持环境维度的同时隐式保留流形几何，并在扩散模型和归一化流上实现数据分布的恢复与稳定采样，理论与实证均支持。


<details>
  <summary>Details</summary>
Motivation: 等式约束下的分布建模在科学领域等场景中存在固有的数学局限性，现有生成模型难以同时满足约束和保持灵活性。需要一种计算成本低、数学可证明并且具有较强适应性的分布修改策略来克服这一困难。

Method: 提出一种计算成本低、数学上有据的分布修改方案，通过约束感知的扰动来改变数据分布，使新分布的支撑维度等于环境空间的维度，同时保持对潜在流形几何的隐式编码。该修改可与扩散模型和归一化流结合，提升在等式约束条件下的建模能力。

Result: 理论分析和在若干代表性任务上的实证结果表明，该方法可实现数据分布的恢复与稳定采样，并且在扩散模型与归一化流任务中表现出良好的适用性与鲁棒性。

Conclusion: 该方法提供了一个通用、低成本且有严格依据的框架，用以解决等式约束下的生成建模问题；通过约束感知的分布修改，既维持了流形几何的隐式成分，又实现对数据分布的可靠恢复与稳定采样，适用于扩散模型和归一化流等多种生成范式。

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [183] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: 提出一种无监督技能分割与层级结构发现的方法，使用基于文法的方式在未标注的轨迹中分割出技能并构建层级，能在高维像素环境中获得比基线更有结构和语义意义的层级，并能加速和稳定下游 RL 任务的学习。


<details>
  <summary>Details</summary>
Motivation: 当前的技能分割多依赖动作标签、奖励信号或人工注释，限制了在无监督场景中的应用。需要一种在无标签数据上也能自动发现可重复使用的技能和层级结构的方法，并能在高维环境中实现有效学习。

Method: 提出一个基于文法的无监督学习框架，将轨迹分割为技能并 induced 层级结构。通过对轨迹的统计与模式挖掘，构建可组合的高层技能，并利用语法规则将低层行为拼接成更高层的技能。对Craftax和未修改的Minecraft等高维像素环境进行评估，使用技能分割、复用性和层级质量等指标与现有基线比较。

Result: 所提出的方法在技能分割、复用性和层级质量等度量上表现出比基线更结构化且语义更丰富的层级；在下游 RL 任务中，这些层级可加速并稳定学习过程。

Conclusion: 基于文法的无监督层级发现框架在强化学习中有效地从无标注轨迹中提取可复用技能和可组合的层级结构，提升学习效率并提供更丰富的策略层级。

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [184] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 本论文在参数噪声模型下研究随机线性赌博机（线性带臂）的问题，给出近似最优的后悔界与下界，并在特定几何的动作集合（如ell_p单位球，p≤2）给出紧致结果；结果表明参数噪声下的后悔界与经典加性噪声有显著不同，且可通过简单的探索-利用策略实现接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 揭示参数噪声对线性带臂后悔界的影响，填补在参数不确定性下的线性带臂理论空白，提供一般动作集合与几何结构下的紧致界与可实现性。

Method: 给出一般动作集合（大小为K、维度d）的上界和下界，核心为参数θ独立同分布的模型，定义最大方差σ^2_max及与之相关的方差量度σ^2_q；对ell_p单位球（p≤2，双范数为q）给出最小极大后悔界为Θ̃(√(d T σ^2_q))，π及对数项的体现通过δ、K等参数。提出一个简单的探索-利用算法实现接近最优的界。

Result: 一般情形下的上界为̃O(√(d T log(K/δ) σ^2_max))，下界为̃Ω(d √(T σ^2_max))，在log K≈d时紧致（相差对数项）；对ell_p单位球（p≤2，q为双范数）有最小极大后悔界̃Θ(√(d T σ^2_q))，其中σ^2_q≤4；与经典加性噪声模型下的d√T相比，后者呈显著不同的几何依赖和规模，且通过简单策略即可实现近最优。

Conclusion: 参数噪声模型下的带臂问题具有与加性噪声不同的后悔结构，且在通用动作集与特定几何集下均可实现近似最优界；简易的探索-利用策略足以达到理论极限量级。

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [185] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: MGN-T 将全局 Transformer 与 MeshGraphNets 的几何先验相结合，通过物理注意力实现对所有节点的并行全局更新，解决高分辨率网格中的长程信息传播瓶颈，提升冲击动力学等复杂物理问题的建模精度，同时显著降低参数规模和训练推理成本。


<details>
  <summary>Details</summary>
Motivation: 标准 MGN 依赖迭代消息传递来传播信息，在大规模高分辨率网格上需要深层堆叠，导致效率低下且难以应对工业级场景中的长程相互作用。需要在保持网格表示的前提下实现全局信息建模以提升性能。

Method: 引入物理注意力 Transformer 作为全局处理器，直接并行更新所有节点状态，并显式保留节点和边的属性；保持网格图表示，不进行多尺度粗化或深层嵌套的消息传递栈；支持处理高分辨率网格的不同几何、拓扑和边界条件，输出包括内部塑性变量等多变量结果。

Result: 在工业尺度网格的冲击动力学任务中，MGN-T 能有效建模自接触、塑性及多变量输出，克服了标准 MGN 的信息传播不足；在经典基准上超越现有方法，达到更高的精度且参数数量显著低于对比基线，且保持实际可用性。

Conclusion: MGN-T 使高分辨率网格上的物理学习更高效、可扩展，避免了深层消息传递堆叠与网格粗化的需求，具备在具有复杂几何、拓扑和边界条件的工业场景中的广泛应用潜力。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [186] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 本研究将交通强度通过新颖的同心环交通描述与污染传感数据结合，使用偏最小二乘回归实现超本地空气质量预测，方法可迁移至其他城市。


<details>
  <summary>Details</summary>
Motivation: 揭示交通与空气污染的时空关系，提供高分辨率的空气质量预测以支持城市管理。

Method: 将交通地图转化为同心环描述特征；利用偏最小二乘回归（PLS）建立污染物与交通特征之间的关系；通过多种训练集优化参数，评估预测性能。

Result: 模型在超本地尺度对污染水平进行预测，显示交通强度与污染的相关性，且工作流程简单、可在其他城市复现。

Conclusion: 提出的同心环交通特征与PLS回归框架为城市空气质量的细粒度预测提供了可扩展的分析路径。

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [187] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 提出面向众包标注的ε-公平性分析框架，给出多数投票在小样本条件下的公平性差距上界，并证明聚合结果在可解释条件下对真实标签的公平性差距呈指数收敛；在离散设置扩展了现有的多分类公平后处理算法以强制人口统计平等，并在合成/真实数据上验证理论与方法。


<details>
  <summary>Details</summary>
Motivation: 解决获取真实标签成本高且标注易受个人偏见影响的问题，聚众包的结果可能放大偏见，尤其涉及敏感特征，因此需要在聚合阶段引入公平性分析与保障。现有研究缺乏收敛性保证，且对在众包聚合中实现ε-公平性（人口统计平等）的研究有限。

Method: 1) 在ε-公平性框架下分析多数投票（MV）和最优贝叶斯聚合（OBA）的公平性；2) 在小样本情形推导MV的公平性差距上界，与个体评注者的公平性差距相关；3) 证明只要满足可解释条件，聚合结果对地面真值的公平性差距以指数速度收敛；4) 将一个前沿的多分类公平后处理算法从连续设置扩展到离散设置，使其对任意聚合规则强制人口统计平等；5) 于合成及真实数据集上进行实验验证，验证理论与方法的有效性。

Result: 给出MV在小样本条件下的公平性差距上界；证明聚合结果在可解释条件下对地面真值的公平性差距指数收敛；将多分类公平后处理算法扩展到离散设置以实现严格的人口统计平等；实验显示理论分析和方法在合成及真实数据上得到支持与验证。

Conclusion: 本文建立了众包聚合过程中的ε-公平性理论框架，给出主要聚合规则的公平性上界与收敛性，并通过离散扩展实现严格人口统计平等的后处理能力，提供了理论与实验层面的综合证据，推动了公平众包聚合研究的系统性进展。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [188] [Sequence Diffusion Model for Temporal Link Prediction in Continuous-Time Dynamic Graph](https://arxiv.org/abs/2601.23233)
*Nguyen Minh Duc,Viet Cuong Ta*

Main category: cs.LG

TL;DR: SDG introduces a sequence-level diffusion framework for dynamic graphs to model uncertainty in temporal link prediction via conditional denoising and a cross-attention decoder, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing temporal GNNs are primarily discriminative and focus on historical interactions, lacking explicit modeling of uncertainty and the sequential structure of future temporal interactions. A generative approach can capture the distribution over future links.

Method: SDG injects noise into the entire historical interaction sequence and jointly reconstructs all interaction embeddings through a conditional denoising process. A cross-attention denoising decoder guides reconstruction of the destination sequence, trained end-to-end.

Result: Experiments on various temporal graph benchmarks demonstrate that SDG consistently achieves state-of-the-art performance in temporal link prediction.

Conclusion: SDG offers a principled generative framework for temporal link prediction by unifying dynamic graph learning with sequence-level diffusion, enabling modeling of uncertainty and sequential future interactions.

Abstract: Temporal link prediction in dynamic graphs is a fundamental problem in many real-world systems. Existing temporal graph neural networks mainly focus on learning representations of historical interactions. Despite their strong performance, these models are still purely discriminative, producing point estimates for future links and lacking an explicit mechanism to capture the uncertainty and sequential structure of future temporal interactions. In this paper, we propose SDG, a novel sequence-level diffusion framework that unifies dynamic graph learning with generative denoising. Specifically, SDG injects noise into the entire historical interaction sequence and jointly reconstructs all interaction embeddings through a conditional denoising process, thereby enabling the model to capture more comprehensive interaction distributions. To align the generative process with temporal link prediction, we employ a cross-attention denoising decoder to guide the reconstruction of the destination sequence and optimize the model in an end-to-end manner. Extensive experiments on various temporal graph benchmarks show that SDG consistently achieves state-of-the-art performance in the temporal link prediction task.

</details>


### [189] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一个解耦扩散逆解算子（DDIS），在逆PDE问题中以函数空间实现数据高效且物理感知的生成框架。通过无条件扩散学习系数先验、神经算子显式建模正向PDE来提供引导，实现数据有效性与物理信息的耦合。引入解耦退火后验采样（DAPS）以避免扩散后验采样中的过平滑问题，并给出在数据稀缺情况下避免引导衰减的理论证明。实验在稀疏观测下达到最先进性能，l2误差提升约11%，谱误差提升约54%；在仅1%数据时，相较于联合模型，l2误差仍具40%优势。


<details>
  <summary>Details</summary>
Motivation: 解决现有插入式扩散后验采样在联合系数-解建模下对大量成对监督的依赖，以及数据效率不足的问题。通过将系数先验学习与正向PDE建模解耦，提高数据效率并实现物理信息的显式引导，克服在数据稀缺环境中的引导衰减风险。

Method: 提出无条件扩散来学习系数先验；使用神经算子显式建模正向PDE以提供引导信息；引入解耦退火后验采样（DAPS）以避免DPS中的过平滑；给出理论证明，DDIS在数据稀缺时避免联合模型的引导衰减失败；通过对比实验在稀疏观测下展现更高的准确性。

Result: 在稀疏观测条件下达到最新性能，平均l2误差提升约11%，谱误差提升约54%；当数据仅占1%时，l2误差相较联合模型仍具约40%的优势。

Conclusion: 解耦设计使数据效率和物理信息学习更强，天然支持DAPS以避免DPS的过平滑，并提供理论保障，且在数据稀缺情形下具有显著的经验优势。

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [190] [FITMM: Adaptive Frequency-Aware Multimodal Recommendation via Information-Theoretic Representation Learning](https://arxiv.org/abs/2601.22498)
*Wei Yang,Rui Zhong,Yiqun Chen,Shixuan Li,Heng Ping,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: FITMM是一种面向多模态推荐的频域信息理论框架，通过对信号进行谱分解，在每个频带内独立处理模态协同信息，并引入频域信息瓶颈正则化与跨模态谱一致性损失，最终实现“先分离再融合”的高效推荐。


<details>
  <summary>Details</summary>
Motivation: 现有系统多在空间/时域进行模态融合，易引入模态错位、冗余与过拟合，未充分利用信号的频域结构与带间关系，降低泛化与效率。

Method: 在正交变换下近似使带内协方差对角化，使高斯信息瓶颈在不同频带上解耦；构建带有图增强的物品表示；对模态做谱分解得到正交带，带内形成轻量化的多模态分量；通过残差门控将带聚合为最终表示；引入频域IB正则化（Wiener式收缩、对弱带的关停）以控制容量与冗余；加入跨模态谱一致性损失以对齐不同模态在各带上的分布；与常规推荐损失端到端优化。

Result: 在三个真实数据集上，FITMM表现稳定且显著优于先进基线。

Conclusion: 通过频域信息瓶颈与跨模态谱一致性，支持“先分离再融合”的范式，提升泛化和性能，证明谱域方法在多模态推荐中的有效性。

Abstract: Multimodal recommendation aims to enhance user preference modeling by leveraging rich item content such as images and text. Yet dominant systems fuse modalities in the spatial domain, obscuring the frequency structure of signals and amplifying misalignment and redundancy. We adopt a spectral information-theoretic view and show that, under an orthogonal transform that approximately block-diagonalizes bandwise covariances, the Gaussian Information Bottleneck objective decouples across frequency bands, providing a principled basis for separate-then-fuse paradigm. Building on this foundation, we propose FITMM, a Frequency-aware Information-Theoretic framework for multimodal recommendation. FITMM constructs graph-enhanced item representations, performs modality-wise spectral decomposition to obtain orthogonal bands, and forms lightweight within-band multimodal components. A residual, task-adaptive gate aggregates bands into the final representation. To control redundancy and improve generalization, we regularize training with a frequency-domain IB term that allocates capacity across bands (Wiener-like shrinkage with shut-off of weak bands). We further introduce a cross-modal spectral consistency loss that aligns modalities within each band. The model is jointly optimized with the standard recommendation loss. Extensive experiments on three real-world datasets demonstrate that FITMM consistently and significantly outperforms advanced baselines.

</details>


### [191] [SCaLRec: Semantic Calibration for LLM-enabled Cloud-Device Sequential Recommendation](https://arxiv.org/abs/2601.22543)
*Ruiqi Zheng,Jinli Cao,Jiao Yin,Hongzhi Yin*

Main category: cs.IR

TL;DR: SCaLRec tackles cloud-device cloud semantics staleness by estimating the usefulness of cached cloud embeddings and calibrating them on-device using current user interactions, enabling improved on-device reranking without per-request cloud LLM calls.


<details>
  <summary>Details</summary>
Motivation: In cloud-device recommenders, semantic user representations are often cached to avoid expensive cloud LLM inference per request. This cached information can become misaligned with the user’s evolving preferences (semantic staleness), degrading ranking quality. A solution is needed to decide when cached semantics are still useful and how to adapt them locally when cloud inference is infeasible.

Method: Propose Semantic Calibration for LLM-enabled Cloud-Device Recommendation (SCaLRec) with two components: (1) a reliability estimator that assesses the usefulness of cached cloud semantics given recent interactions, and (2) an on-device semantic calibration module that adjusts the cached embedding using up-to-date interaction evidence, without invoking the cloud LLM.

Result: Experiments on real-world datasets show that SCaLRec consistently improves recommendation performance over strong baselines under cloud semantic staleness, demonstrating effective handling of stale semantics and improved ranking quality.

Conclusion: SCaLRec provides a practical, hardware-friendly approach to mitigate cloud semantic staleness in cloud-device recommenders by combining cached-semantics reliability estimation with on-device calibration, achieving robust performance without per-request cloud LLM calls.

Abstract: Cloud-device collaborative recommendation partitions computation across the cloud and user devices: the cloud provides semantic user modeling, while the device leverages recent interactions and cloud semantic signals for privacy-preserving, responsive reranking. With large language models (LLMs) on the cloud, semantic user representations can improve sequential recommendation by capturing high-level intent. However, regenerating such representations via cloud LLM inference for every request is often infeasible at real-world scale. As a result, on-device reranking commonly reuses a cached cloud semantic user embedding across requests. We empirically identify a cloud semantic staleness effect: reused embeddings become less aligned with the user's latest interactions, leading to measurable ranking degradation.
  Most existing LLM-enabled cloud-device recommenders are typically designed around on-demand cloud semantics, either by assuming low-latency cloud LLM access or by regenerating semantic embeddings per request. When per-request regeneration is infeasible and cached semantics must be reused, two technical challenges arise: (1) deciding when cached cloud semantics remain useful for on-device reranking, and (2) maintaining ranking quality when the cloud LLM cannot be invoked and only cached semantics are available. To address this gap, we introduce the Semantic Calibration for LLM-enabled Cloud-Device Recommendation (SCaLRec). First, it estimates the reliability of cached semantics under the user's latest interactions. Second, an on-device semantic calibration module is proposed to adjusts the cached semantic embedding on-device using up-to-date interaction evidence, without per-request cloud LLM involvement. Experiments on real-world datasets show that SCaLRec consistently improves recommendation performance over strong baselines under cloud semantic staleness.

</details>


### [192] [Farewell to Item IDs: Unlocking the Scaling Potential of Large Ranking Models via Semantic Tokens](https://arxiv.org/abs/2601.22694)
*Zhen Zhao,Tong Zhang,Jie Xu,Qingliang Cai,Qile Zhang,Leyuan Yang,Daorui Xiao,Xiaojia Chang*

Main category: cs.IR

TL;DR: 提出 TRM 框架，使用语义令牌替代物品ID，在大规模排序模型中实现存储压缩和准确性提升，并在生产环境中获得显著指标改进。


<details>
  <summary>Details</summary>
Motivation: 物品ID作为离散符号的嵌入在物品动态更新时难以训练与维护，导致可扩展性受限；语义令牌具备更好的可扩展性和稳定性。

Method: 通过 TRM 框架改进令牌生成与应用流水线，使用语义令牌替代物品ID，在大规模排序模型中训练与部署。

Result: 稀疏存储降低约33%，AUC提升约0.85%；模型容量扩展时继续超越SOTA；在大规模个性化搜索引擎部署后，用户活跃天数提升0.26%，查询变化率提升0.75%。

Conclusion: 语义令牌具有更强的扩展性，TRM在提升性能与可扩展性方面有效，适合大规模推荐与搜索系统的生产落地。

Abstract: Recent studies on scaling up ranking models have achieved substantial improvement for recommendation systems and search engines. However, most large-scale ranking systems rely on item IDs, where each item is treated as an independent categorical symbol and mapped to a learned embedding. As items rapidly appear and disappear, these embeddings become difficult to train and maintain. This instability impedes effective learning of neural network parameters and limits the scalability of ranking models. In this paper, we show that semantic tokens possess greater scaling potential compared to item IDs. Our proposed framework TRM improves the token generation and application pipeline, leading to 33% reduction in sparse storage while achieving 0.85% AUC increase. Extensive experiments further show that TRM could consistently outperform state-of-the-art models when model capacity scales. Finally, TRM has been successfully deployed on large-scale personalized search engines, yielding 0.26% and 0.75% improvement on user active days and change query ratio respectively through A/B test.

</details>


### [193] [Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval](https://arxiv.org/abs/2601.22783)
*Ilyass Moummad,Marius Miron,David Robinson,Kawtar Zaher,Hervé Goëau,Olivier Pietquin,Pierre Bonnet,Emmanuel Chemla,Matthieu Geist,Alexis Joly*

Main category: cs.IR

TL;DR: 提出紧凑的超立方体嵌入用于文本引导的野生动物观测检索，通过二进制表示实现大规模检索的高效性。与连续嵌入相比，在多模态检索中具有竞争甚至优越的性能，同时显著降低内存和检索成本。


<details>
  <summary>Details</summary>
Motivation: 应对海量多模态野生动物观测数据的文本检索挑战；高维相似性搜索成本高，迫切需要高效的检索表示。

Method: 在跨视图代码对齐哈希框架基础上，将自然语言描述与视觉/音频观测对齐到共享的汉明(Hamming)空间；利用预训练野生动物模型（BioCLIP、BioLingual）并通过参数高效微调进行哈希化；产生离散二进制的超立方体嵌入以实现文本-图像/文本-音频检索；在大规模基准上评估，包含 iNaturalist2024 与 iNatSounds2024，以及多种声景数据集以考察领域偏移鲁棒性。

Result: 离散二进制嵌入的检索在多模态文本检索任务中与连续嵌入相当甚至优越，同时显著降低内存占用和检索成本；哈希训练目标同时提升编码器表示，提升零-shot 泛化能力。

Conclusion: 二进制、语言驱动的检索为大规模野生动物档案的可扩展检索提供了一条高效路径，利于生物多样性监测系统的部署和运营。

Abstract: Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.

</details>


### [194] [BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models](https://arxiv.org/abs/2601.22925)
*Weiqin Yang,Bohao Wang,Zhenxiang Xu,Jiawei Chen,Shengjia Zhang,Jingbang Chen,Canghong Jin,Can Wang*

Main category: cs.IR

TL;DR: BEAR 将 beam-search 的推理行为纳入训练过程，通过强制正项在每个解码步的前 B 个候选项中排名来缓解因贪婪剪枝导致的正项丢失问题，且实现代价低于全面仿真。


<details>
  <summary>Details</summary>
Motivation: 现有 SFT 优化的是正项的总概率，但这并不能确保在 beam search 的搜索过程里这些正项一定被保留，因为前缀概率不足会被剪枝。训练-推理不一致可能导致模型在推理阶段错过高质量的正项。

Method: 提出 BEAR 目标：在训练中引入一个松弛的必要条件，即正项中的每个 token 在每个解码步都应位于前 B 的候选 token 排名之内。通过将该约束加入损失函数，尽量减少被贪婪剪枝错误淘汰的风险；该方法不逐样例逐步仿真 beam search，计算开销与标准 SFT 相近。

Result: 在四个真实数据集上的实验显示，BEAR 显著优于强基线，提升推荐性能，且开销极小。

Conclusion: BEAR 成功缓解训练阶段与 beam-search 推理阶段的不一致性，提供了一种高效且有效的正则化方案，适用于基于 LLM 的推荐系统。

Abstract: Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked recommended items. However, we identify a critical training-inference inconsistency: while SFT optimizes the overall probability of positive items, it does not guarantee that such items will be retrieved by beam search even if they possess high overall probabilities. Due to the greedy pruning mechanism, beam search can prematurely discard a positive item once its prefix probability is insufficient.
  To address this inconsistency, we propose BEAR (Beam-SEarch-Aware Regularization), a novel fine-tuning objective that explicitly accounts for beam search behavior during training. Rather than directly simulating beam search for each instance during training, which is computationally prohibitive, BEAR enforces a relaxed necessary condition: each token in a positive item must rank within the top-$B$ candidate tokens at each decoding step. This objective effectively mitigates the risk of incorrect pruning while incurring negligible computational overhead compared to standard SFT. Extensive experiments across four real-world datasets demonstrate that BEAR significantly outperforms strong baselines. Code will be released upon acceptance.

</details>


### [195] [OrLog: Resolving Complex Queries with LLMs and Probabilistic Reasoning](https://arxiv.org/abs/2601.23085)
*Mohanna Hoveyda,Jelle Piepenbrock,Arjen P de Vries,Maarten de Rijke,Faegheh Hasibi*

Main category: cs.IR

TL;DR: OrLog 是一个神经符号检索框架，通过对原子谓词进行解码无前向传递的可置信度评分（由大语言模型提供），再由概率推理引擎计算查询满足的后验概率，从而实现对复杂约束的约束感知检索。


<details>
  <summary>Details</summary>
Motivation: 在多约束信息需求下，现有检索系统往往忽略查询中的逻辑约束或以生成式推理来近似，结果不一致且不可靠。现有神经符号方法要么局限于形式逻辑、要么假设明确且证据完整，难以适应信息检索中的不确定性与部分证据缺失。

Method: OrLog 将谓词层面的可置信度估计与逻辑推理解耦：LLM 以解码无前向传递方式对原子谓词给出可置信度分数；随后的概率推理引擎据此计算查询满足的后验概率。系统在不同的基础 LLM、外部知识访问水平及逻辑约束下进行评估，并与基线检索器及LLM-作为-推理的方式对比。

Result: 在给定实体描述的条件下，OrLog 相较于基于LLM推理的方法在顶级排序精度上有显著提升，且对存在析取（disjunctive）查询的收益更大；并且显著减少了令牌消耗，平均每个查询-实体对约降低 ~90%。表明生成无前向的谓词可置信度估计结合概率推理，可实现高效且具约束感知的检索，优于单一端到端推理。

Conclusion: 生成无前向的谓词可置信度估计与概率推理的组合，能够在保持较高准确性的同时实现更高的计算与令牌效率，适用于需要约束感知的检索任务，并优于单一LLM推理。

Abstract: Resolving complex information needs that come with multiple constraints should consider enforcing the logical operators encoded in the query (i.e., conjunction, disjunction, negation) on the candidate answer set. Current retrieval systems either ignore these constraints in neural embeddings or approximate them in a generative reasoning process that can be inconsistent and unreliable. Although well-suited to structured reasoning, existing neuro-symbolic approaches remain confined to formal logic or mathematics problems as they often assume unambiguous queries and access to complete evidence, conditions rarely met in information retrieval. To bridge this gap, we introduce OrLog, a neuro-symbolic retrieval framework that decouples predicate-level plausibility estimation from logical reasoning: a large language model (LLM) provides plausibility scores for atomic predicates in one decoding-free forward pass, from which a probabilistic reasoning engine derives the posterior probability of query satisfaction. We evaluate OrLog across multiple backbone LLMs, varying levels of access to external knowledge, and a range of logical constraints, and compare it against base retrievers and LLM-as-reasoner methods. Provided with entity descriptions, OrLog can significantly boost top-rank precision compared to LLM reasoning with larger gains on disjunctive queries. OrLog is also more efficient, cutting mean tokens by $\sim$90\% per query-entity pair. These results demonstrate that generation-free predicate plausibility estimation combined with probabilistic reasoning enables constraint-aware retrieval that outperforms monolithic reasoning while using far fewer tokens.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [196] [Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?](https://arxiv.org/abs/2601.22329)
*Ala N. Tak,Amin Banayeeanzade,Anahita Bolourani,Fatemeh Bahrani,Ashutosh Chaubey,Sai Praneeth Karimireddy,Norbert Schwarz,Jonathan Gratch*

Main category: cs.AI

TL;DR: LLMs show improved rationality with deliberate thinking; emotion steering can distort judgments; ICP is drastic, RLS more plausible but less reliable; reveals tension between reasoning and affect in LLM decision behavior.


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否呈现类似人类的(非)理性与偏见；在高风险决策情境下测试理性选择公理及情绪对判断的影响；为人类行为建模和安全部署提供依据。

Method: 系统评估多家LLM对理性选择公理的基准和行为经济学领域的经典决策问题的表现；并通过两种情感引导方法（情景内提示ICP、表示层引导RLS）来探究情绪对推理的影响和交互。

Result: 系统性地显示：有意“思考”显著提升理性程度，向期望值最大化靠拢；ICP引发强烈且方向性明显的偏差；RLS则产生更符合心理学直觉的模式，但稳定性较低；提升理性能力的机制同时放大对情绪干预的敏感性；不同引导方法在可控性与贴近人类行为之间存在权衡。

Conclusion: 推理与情感引导之间存在张力；对人类行为仿真和基于LLM的高风险决策系统安全部署具有重要意义。

Abstract: Large Language Models (LLMs) are increasingly positioned as decision engines for hiring, healthcare, and economic judgment, yet real-world human judgment reflects a balance between rational deliberation and emotion-driven bias. If LLMs are to participate in high-stakes decisions or serve as models of human behavior, it is critical to assess whether they exhibit analogous patterns of (ir)rationalities and biases. To this end, we evaluate multiple LLM families on (i) benchmarks testing core axioms of rational choice and (ii) classic decision domains from behavioral economics and social norms where emotions are known to shape judgment and choice. Across settings, we show that deliberate "thinking" reliably improves rationality and pushes models toward expected-value maximization. To probe human-like affective distortions and their interaction with reasoning, we use two emotion-steering methods: in-context priming (ICP) and representation-level steering (RLS). ICP induces strong directional shifts that are often extreme and difficult to calibrate, whereas RLS produces more psychologically plausible patterns but with lower reliability. Our results suggest that the same mechanisms that improve rationality also amplify sensitivity to affective interventions, and that different steering methods trade off controllability against human-aligned behavior. Overall, this points to a tension between reasoning and affective steering, with implications for both human simulation and the safe deployment of LLM-based decision systems.

</details>


### [197] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 对 Bloom 的 Erdős 问题数据库中 700 条标记为 Open 的猜想进行半自治数学发现的案例研究，AI 驱动的自然语言验证与人类专家评估相结合，以评估 13 个问题的状态和可重复性。


<details>
  <summary>Details</summary>
Motivation: 探究在大规模数学猜想中的自动化协助潜力与局限性，评估 AI 在自然语言理解、文献检索和抄袭风险方面的挑战，并通过实证研究揭示 Open 状态的原因。

Method: 混合方法：先用 AI 驱动的自然语言验证缩小候选空间，再请人类专家对正确性与新颖性进行评估。对标注为 Open 的 13 个问题进行分析：其中约 5 条呈现出看似新的自主解决方案，8 条被识别为已有文献中的解法。讨论在大规模应用 AI 到数学猜想时遇到的文献检索困难与潜在的‘潜意识抄袭’风险。

Result: 结论：Open 状态更多源自模糊性/隐藏性而非难度，AI 在缩小搜索空间和揭示文献关系方面有效，但存在识别文献困难和抄袭风险。对 Erdős 问题的 AI 辅助工作提供了实证性洞见和若干可操作的经验教训。

Conclusion: 对未来工作：改进文献识别、抄袭检测、以及把 AI 结果与人类评审的可靠性结合起来；强调在大规模数学猜想上的可重复性与透明性。

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [198] [AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability](https://arxiv.org/abs/2601.22418)
*Julius Sechang Mboli,Omolara Aderonke Ogungbemi*

Main category: cs.AI

TL;DR: DenseNet121 在约2.5万张废物图像的二分类任务中表现最佳，准确率达91%、ROC-AUC0.98，比传统模型提升约20个百分点。PCA对传统方法帮助甚微；迁移学习在数据有限条件下显著提升性能。论文还给出将模型整合进实时数据驱动决策支持系统的实现方向及潜在环境效益。


<details>
  <summary>Details</summary>
Motivation: 实现高效废物分类以支撑循环经济和资源回收；系统性比较传统机器学习与深度学习在有限数据下的性能，评估 PCA 的作用，并探讨在实际智能城市场景中的部署潜力。

Method: 比较传统机器学习（随机森林、SVM、AdaBoost）与深度学习模型（自定义 CNN、VGG16、ResNet50、DenseNet121、EfficientNetB0、InceptionV3），对25077张废物图像进行80/20的训练/测试拆分，进行数据增强并将输入缩放至150×150像素；对传统模型尝试应用主成分分析（PCA）进行降维；以准确率和 ROC-AUC 作为主要评估指标；分析迁移学习在数据有限条件下的性能提升，并提出将模型嵌入到实时数据驱动决策支持系统（DSS）的实现路径与应用场景。

Result: DenseNet121 获得最高准确率（约91%）和 ROC-AUC（约0.98），比最佳传统分类器高出约20个百分点；PCA 对传统方法几乎无显著提升；迁移学习在数据有限情形下显著提高性能；提出将模型集成到面向实时废物分拣的 DSS 的实现思路及潜在的环境效益（减少填埋、降低生命周期影响）。

Conclusion: 在数据有限的条件下，带有迁移学习的深度学习模型在废物二分类任务中优于传统方法，具备落地到实时系统的潜力；但仍需关注泛化性、数据偏差、计算资源与实时性等部署挑战，并建议进一步的稳健性评估、跨域验证与扩展评估。

Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3) for binary classification of 25 077 waste images (80/20 train/test split, augmented and resized to 150x150 px). The paper assesses the impact of Principal Component Analysis for dimensionality reduction on traditional models. DenseNet121 achieved the highest accuracy (91 %) and ROC-AUC (0.98), outperforming the best traditional classifier by 20 pp. Principal Component Analysis (PCA) showed negligible benefit for classical methods, whereas transfer learning substantially improved performance under limited-data conditions. Finally, we outline how these models integrate into a real-time Data-Driven Decision Support System for automated waste sorting, highlighting potential reductions in landfill use and lifecycle environmental impacts.)

</details>


### [199] [Anytime Safe PAC Efficient Reasoning](https://arxiv.org/abs/2601.22446)
*Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing*

Main category: cs.AI

TL;DR: 提出 B-PAC 推理，在部分反馈条件下实现 anytime-safe 高效在线推理，通过逆倾向评分估计和测试超马丁格尔，动态调整路由阈值，理论保障与实证支持。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在高计算成本和延迟下的可控错误问题，尤其在在线、数据非平稳且对非思维模型性能损失感知有限的场景。

Method: 将逆倾向评分估计用于构建候选阈值的测试超马丁格尔，基于累计统计证据动态调整路由阈值，实现 anytime-valid 的安全性和高效性。

Result: 在实验中实现显著的计算开销降低（思维模型使用率降低约81.01%），并将性能损失控制在用户设定的阈值内。

Conclusion: B-PAC 推理提供 anytime 安全高效的在线推理框架，具理论保证和实证支持，适用于需要在质量与成本之间权衡的大规模推理任务。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, especially in online settings where the performance loss of a non-thinking model is only partially observed and data are non-stationary. To address this, we propose Betting Probably Approximately Correct (B-PAC) reasoning, a principled method that enables anytime safe and efficient online reasoning under partial feedback. Specifically, we utilize inverse propensity scoring estimators to construct test supermartingales for candidate thresholds, and then dynamically adjust the routing threshold based on the accumulated statistical evidence of safety. Theoretically, we establish the anytime-valid performance loss control and the efficiency of B-PAC reasoning. Extensive experiments demonstrate that B-PAC reasoning significantly reduces computational overhead, decreasing thinking model usage by up to 81.01\%, while controlling the performance loss below the user-specified level.

</details>


### [200] [Controllable Information Production](https://arxiv.org/abs/2601.22449)
*Tristan Shah,Stas Tiomkin*

Main category: cs.AI

TL;DR: 提出一种新型内在动机原理 CIP，通过可控信息生产来驱动智能行为，无需外部效用或设计者指定的变量，基于最优控制推导，与开环/闭环 KS 熵差相关，兼顾追求与调控混沌，理论性质完善并在标准 IM 基准上验证有效。


<details>
  <summary>Details</summary>
Motivation: 克服现有信息理论 IM 方法对外部效用和设计者选择的变量的依赖，寻求一个与控制理论耦合的自驱动动机框架，并建立外在行为与内在动机之间的联系。

Method: 从最优控制出发推导 CIP 目标，将其表述为开放环与闭环 Kolmogorov-Sinai 熵之差，理论分析其性质并证明与信息生产/混沌调控相关的特性；在标准 IM 基准上进行实验验证。

Result: 确立 CIP 的关键理论性质；在标准 IM 基准上验证其有效性，显示优于或与现有 IM 方法等效的自驱动行为表现。

Conclusion: CIP 提供一种不依赖外部效用及设计者变量的新型自驱动动机原则，具备明确的理论基础和经验性证据，揭示外在与内在行为之间通过最优控制框架的联系，并将信息生产的可控性与对混沌的追求/调控统一起来。

Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmission. In this work, we introduce a novel IM principle, Controllable Information Production (CIP), that avoids both external utilities and designer-specified variables. We derive the CIP objective from Optimal Control, showing a connection between extrinsic and intrinsic behaviors. CIP appears as the gap between open-loop and closed-loop Kolmogorov-Sinai entropies, which simultaneously rewards the pursuit and regulation of chaos. We establish key theoretical properties of CIP and demonstrate its effectiveness on standard IM benchmarks.

</details>


### [201] [Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models](https://arxiv.org/abs/2601.22513)
*Shi Fu,Yingjie Wang,Shengchao Hu,Peng Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: The paper provides the first rigorous theory for Self-Rewarding Language Models (SRLMs), including a single-update lower bound tied to initial model quality, finite-sample iterative error bounds at rate ~1/√n, and an exponential decay of initialization dependence with iterations, with instantiations for linear softmax models.


<details>
  <summary>Details</summary>
Motivation: Addresses the theoretical gap in understanding SRLMs: despite empirical success, the mechanisms driving their improvements are unclear. The work aims to formalize guarantees and explain why self-rewarding can overcome poor initialization.

Method: 1) Derive a lower bound for a single update step that hinges on the initial model's quality. 2) Establish finite-sample error bounds for the full iterative process, showing a ~1/√n rate. 3) Prove that dependence on the initial model decays exponentially with iteration count T. 4) Specialize the framework to the linear softmax model class to connect theory to practical architectures.

Result: The analysis yields: (i) a fundamental single-step lower bound depending on initial model quality, (ii) a finite-sample error bound for the iterative SRLM process with a ~1/√n rate, and (iii) an exponential decay of initial-condition dependence as iterations grow, explaining robustness to bad starts. The framework is instantiated for linear softmax models, yielding concrete guarantees for that class.

Conclusion: The work provides formal explanations for SRLM success: self-rewarding induces stability and consistency, mitigating poor initialization, and the results offer architecture-tailored guarantees that bridge theory and practice.

Abstract: Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This paper provides the first rigorous theoretical guarantees for SRLMs. We first establish a lower bound that characterizes the fundamental limits of a single update step, revealing a critical dependence on the quality of the initial model. We then derive finite-sample error bounds for the full iterative paradigm, showing that performance improves at a rate of $\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$ with sample size $n$. Crucially, our analysis reveals that the dependence on the initial model decays exponentially with the number of iterations $T$. This provides a formal explanation for why self-rewarding succeeds: it robustly overcomes poor initialization by steering the dynamics toward internal stability and consistency. Finally, we instantiate our theoretical framework for the linear softmax model class, yielding tailored guarantees that connect our high-level insights to practical model architectures.

</details>


### [202] [Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution](https://arxiv.org/abs/2601.22528)
*Hongze Mi,Yibo Feng,WenJie Lu,Song Cao,Jinyuan Li,Yanming Li,Xuelin Zhang,Haotian Luo,Songyang Peng,He Cui,Tengfei Tian,Jun Fang,Hua Chai,Naiqiang Tan*

Main category: cs.AI

TL;DR: DMS 提供自进化记忆系统，解决 GUI 自动化中记忆粒度与上下文污染问题，提升多模态大语言模型的成功率与稳定性，且无额外训练成本。


<details>
  <summary>Details</summary>
Motivation: 解决长期、跨应用的 GUI 任务受限于有限的上下文窗口；现有记忆范式在动态 GUI 环境中存在粒度错配与上下文污染，难以适应变化的任务需求。

Method: 将复杂轨迹分解为独立的、可复用的单元，实现组合式记忆；通过效用驱动的自然选择对记忆进行筛选与淘汰，构建一个由适者生存驱动的动态记忆生态系统；实现高层意图与低层执行之间的解耦，提升策略的进化与鲁棒性。

Result: 在真实多应用基准上，平均成功率提升 18.0%、执行稳定性提升 33.9%，并降低任务延迟；且无需额外训练成本或架构开销。

Conclusion: DMS 作为自进化记忆系统，能够提升通用型 MLLMs 的 GUI 任务能力，适用于多应用场景，降低记忆相关的幻觉与错误决策。

Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.

</details>


### [203] [Enhancing TableQA through Verifiable Reasoning Trace Reward](https://arxiv.org/abs/2601.22530)
*Tung Sum Thomas Kwok,Xinyu Wang,Hengzhi He,Xiaofeng Lin,Peng Lu,Liheng Ma,Chunhe Wang,Ying Nian Wu,Lei Ding,Guang Cheng*

Main category: cs.AI

TL;DR: RE-Tab introduces a plug-and-play reward-modeling framework for TableQA as a POMDP, providing explicit reward signals during action selection and simulated reasoning to guide multi-step table transformations, achieving state-of-the-art accuracy with reduced inference cost and strong generalization.


<details>
  <summary>Details</summary>
Motivation: TableQA requires multi-step reasoning over table states through transformations; static input is insufficient for accurate answers, necessitating explicit signals to guide reasoning and improve performance and efficiency.

Method: RE-Tab uses lightweight, training-free reward modeling to reformulate table-state reasoning as a partially observable Markov decision process. It provides two verifiable rewards during processing: (1) State Transition—What is the best action?, and (2) Simulative Reasoning—Am I sure about the output? The framework is plug-and-play and improves trajectory search.

Result: Achieves state-of-the-art TableQA performance with ~25% reduction in inference cost. A direct plug-and-play RE-Tab implementation yields up to 41.77% improvement in QA accuracy and 33.33% reduction in test-time inference samples for consistent answers. Demonstrates consistent gains across various LLMs and benchmarks.

Conclusion: Explicit reward feedback for table transformations enables better stepwise reasoning in TableQA, offering a generalizable, efficient, plug-and-play enhancement with strong empirical gains and practical deployment potential; code available at the provided repository.

Abstract: A major challenge in training TableQA agents, compared to standard text- and image-based agents, is that answers cannot be inferred from a static input but must be reasoned through stepwise transformations of the table state, introducing multi-step reasoning complexity and environmental interaction. This leads to a research question: Can explicit feedback on table transformation action improve model reasoning capability? In this work, we introduce RE-Tab, a plug-and-play framework that architecturally enhances trajectory search via lightweight, training-free reward modeling by formulating the problem as a Partially Observable Markov Decision Process. We demonstrate that providing explicit verifiable rewards during State Transition (``What is the best action?'') and Simulative Reasoning (``Am I sure about the output?'') is crucial to steer the agent's navigation in table states. By enforcing stepwise reasoning with reward feedback in table transformations, RE-Tab achieves state-of-the-art performance in TableQA with almost 25\% drop in inference cost. Furthermore, a direct plug-and-play implementation of RE-Tab brings up to 41.77% improvement in QA accuracy and 33.33% drop in test-time inference samples for consistent answer. Consistent improvement pattern across various LLMs and state-of-the-art benchmarks further confirms RE-Tab's generalisability. The repository is available at https://github.com/ThomasK1018/RE_Tab .

</details>


### [204] [Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning](https://arxiv.org/abs/2601.22536)
*Yixin Yang,Qingxiu Dong,Zhifang Sui*

Main category: cs.AI

TL;DR: 揭示嵌入空间拥挤现象并提出 CraEG，一种几何引导的重加权采样方法，缓解拥挤以提升推理任务的鲁棒性与多样性，且无需额外训练，单遍推断，兼容现有采样策略。


<details>
  <summary>Details</summary>
Motivation: 温度、截断等全局概率重加权方式仅基于标量 token 概率，忽略嵌入空间中词向量之间的细粒度几何关系，容易导致下一词分布在几何上接近的词上聚集，进而影响复杂推理（如数学推理）的质量。需通过考虑嵌入空间结构来改进采样策略。

Method: 提出 CraEG，一种即插即用、训练无成本、单遍解码的几何引导重加权采样方法。通过分析嵌入空间的几何关系来缓解分布拥挤，对候选 token 的权重进行几何约束重排，使得相邻嵌入的高概率质量不再过度集中。该方法可与现有采样策略（如温度、截断等）兼容。

Result: 在多种语言模型和基准任务上验证，CraEG 能提升生成表现、提高鲁棒性与多样性，且对不同模型与基准具有普适性。

Conclusion: 嵌入空间拥挤是影响推理质量的关键因素之一，几何引导的重加权解码能有效缓解该现象，CraEG 提供了一种训练-free、单遍、可与现有采样策略共存的解码改进方案。

Abstract: Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-diversity tradeoff. However, they operate solely on token probabilities, ignoring fine-grained relationships among tokens in the embedding space. We uncover a novel phenomenon, embedding-space crowding, where the next-token distribution concentrates its probability mass on geometrically close tokens in the embedding space. We quantify crowding at multiple granularities and find a statistical association with reasoning success in mathematical problem solving. Motivated by this finding, we propose CraEG, a plug-and-play sampling method that mitigates crowding through geometry-guided reweighting. CraEG is training-free, single-pass, and compatible with standard sampling strategies. Experiments on multiple models and benchmarks demonstrate improved generation performance, with gains in robustness and diversity metrics.

</details>


### [205] [WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction](https://arxiv.org/abs/2601.22586)
*Qian Hong,Siyuan Chang,Xiao Zhou*

Main category: cs.AI

TL;DR: WED-Net提出一种双分支Transformer，针对极端天气下的城市时空预测，能够将内在交通模式与天气诱发模式解耦，并通过记忆 banks、自适应门控实现有效融合，同时加入天气判别器与因果数据增强以提升在稀有事件下的泛化；在三城出租车流数据集上表现鲁棒，代码已公开。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据驱动的方法往往把天气作为辅助输入且使用粗粒度描述，难以捕捉细粒度的时空动态；尽管有因果方法用于提升泛化，但多忽视时间演化或对混杂变量的划分较为固定，需要既考虑时间动态又能对天气效应进行解耦的框架。

Method: 提出双分支Transformer，分别建模“内在”与“天气诱导”的交通模式，通过自注意力与跨注意力实现分离；引入记忆库并通过自适应门控进行特征级融合；增加一个判别器以显式区分天气条件，促进解耦；设计因果数据增强策略，在扰动非因果部分的同时保持因果结构以提升对稀有场景的泛化。

Result: 在来自三座城市的出租车流数据集上，WED-Net在极端天气条件下表现出鲁棒性，显示出提升安全出行、灾害准备与城市韧性的潜力。

Conclusion: WED-Net为极端天气下的城市时空预测提供了有效的解耦与泛化能力，具备现实应用潜力，代码公开。

Abstract: Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spatio-temporal effects. Although recent methods adopt causal techniques to improve out-of-distribution generalization, they typically overlook temporal dynamics or depend on fixed confounder stratification. To address these limitations, we propose WED-Net (Weather-Effect Disentanglement Network), a dual-branch Transformer architecture that separates intrinsic and weather-induced traffic patterns via self- and cross-attention, enhanced with memory banks and fused through adaptive gating. To further promote disentanglement, we introduce a discriminator that explicitly distinguishes weather conditions. Additionally, we design a causal data augmentation strategy that perturbs non-causal parts while preserving causal structures, enabling improved generalization under rare scenarios. Experiments on taxi-flow datasets from three cities demonstrate that WED-Net delivers robust performance under extreme weather conditions, highlighting its potential to support safer mobility, highlighting its potential to support safer mobility, disaster preparedness, and urban resilience in real-world settings. The code is publicly available at https://github.com/HQ-LV/WED-Net.

</details>


### [206] [Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR](https://arxiv.org/abs/2601.22595)
*Hao Yi,Yulan Hu,Xin Li,Sheng Ouyang,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 将主动学习引入 RLVR，通过在线不确定性一致性度量提升样本选择效率，在仅用 30% 数据下实现全数据性能，显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: RLVR依赖大量查询来保证可验证的奖励信号，标注成本高。现有主动学习多聚焦主观不确定性，忽略目标(客观)层面的不确定性，导致样本选择效果受限。需要评估主观与客观不确定性的一致性以提升采样效率。

Method: 提出不确定性一致性度量。离线用点二项相关系数(PBC)评估主观不确定性与客观不确定性的对齐。在线场景由于样本有限与输出分布动态变化，PBC 难以稳定估计，提出基于归一化优势和主观不确定性的在线变体来估算一致性。理论上证明在线变体与离线 PBC 负相关，且可用于更优的样本选择。

Result: 在多项实验中，在线一致性方法优于随机和经典 AL 基线，能在仅用 30% 数据的条件下达到全数据集的性能，显著降低 RLVR 的标注成本。

Conclusion: 通过对齐主观与客观不确定性来改进 AL 对 RLVR 的样本筛选，在线不确定性的一致性估计可实现高效样本利用，具备推广至其它需要高成本评估的强化学习任务的潜力。

Abstract: Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar or superior performance, introducing active learning (AL) into RLVR. We identify that classic AL sampling strategies fail to outperform random selection in this setting, due to ignoring objective uncertainty when only selecting by subjective uncertainty. This work proposes an uncertainty consistency metric to evaluate how well subjective uncertainty aligns with objective uncertainty. In the offline setting, this alignment is measured using the Point-Biserial Correlation Coefficient (PBC). For online training, because of limited sampling and dynamically shifting output distributions, PBC estimation is difficult. Therefore, we introduce a new online variant, computed from normalized advantage and subjective uncertainty. Theoretically, we prove that the online variant is strictly negatively correlated with offline PBC and supports better sample selection. Experiments show our method consistently outperforms random and classic AL baselines, achieving full-dataset performance while training on only 30% of the data, effectively reducing the cost of RLVR for reasoning tasks.

</details>


### [207] [EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models](https://arxiv.org/abs/2601.22617)
*Hongxi Yan,Qingjie Liu,Yunhong Wang*

Main category: cs.AI

TL;DR: EntroCut 使用早期推理步骤输出分布的熵作为高置信状态信号，动态截断推理过程，从而在不显著损失精度的前提下减少大模型推理的Token数量，且在效率-性能比方面优于现有训练-free 方法。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在扩展推理链中产生大量中间步骤，计算成本高且效率不足。通过熵作为信号在早期判断是否可安全终止推理，提升推理效率。

Method: 提出训练无关的 EntroCut，基于推理过程中的输出分布熵动态截断推理；在早期低熵（高置信）状态终止推理。引入效能与性能比 EPR 作为统一度量，比较不同训练-free 方法的效率-精度权衡。

Result: 在四个基准数据集上，EntroCut 能将token 使用量最多降低约40%，且对准确率的牺牲很小；在效率-性能权衡方面优于现有训练-free 方法。

Conclusion: 基于熵的动态截断对缓解LRMs推理低效具实际意义，提供一种无训练成本的提升推理效率的可行策略。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks through extended chain-of-thought generation, but their reliance on lengthy intermediate steps incurs substantial computational cost. We find that the entropy of the model's output distribution in early reasoning steps reliably distinguishes correct from incorrect reasoning. Motivated by this observation, we propose EntroCut, a training-free method that dynamically truncates reasoning by identifying high-confidence states where reasoning can be safely terminated. To comprehensively evaluate the trade-off between efficiency and accuracy, we introduce the Efficiency-Performance Ratio (EPR), a unified metric that quantifies relative token savings per unit accuracy loss. Experiments on four benchmarks show that EntroCut reduces token usage by up to 40\% with minimal accuracy sacrifice, achieving superior efficiency-performance trade-offs compared with existing training-free methods. These results demonstrate that entropy-guided dynamic truncation provides a practical approach to mitigate the inefficiency of LRMs.

</details>


### [208] [Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling](https://arxiv.org/abs/2601.22636)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Chenliang Xu,Christopher White,Jianfeng Gao*

Main category: cs.AI

TL;DR: 提出 SABER，一种Scaling-aware Best-of-N 估计器，用 Beta- 伯努利模型和解析缩放定律来从小预算测量外推 large-N 攻击成功率（ASR），显著提升 ASR@1000 的预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估多为单次提示或低预算推断，未能反映真实世界的并行探测风险。需要可扩展、低成本的风险外推方法来量化大规模对抗风险。

Method: 将样本层面的成功概率建模为 Beta 分布（Bernoulli 的共轭先验），推导出可用于从小样本（n=100）外推到大规模（如 N=1000）的解析缩放律。提出 anchored SABER 估计器进行最佳-在-N 的风险估计。

Result: 在 n=100 的样本下，SABER 能以平均绝对误差 MAE=1.66 预测 ASR@1000，而基线为 12.04，相对误差下降约 86.2%。结果揭示了异质化的风险缩放特征，表明在并行对抗压力下，表面看似鲁棒的模型也可能经历非线性风险放大。

Conclusion: 提供一种低成本、可扩展的现实世界 LLM 安全评估方法，能够可靠地外推大规模对抗风险并揭示不同模型的风险缩放行为。

Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.

</details>


### [209] [Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence](https://arxiv.org/abs/2601.22645)
*Vaibhav Ram S. V. N. S,Swetanshu Agrawal,Samudra Banerjee,Abdul Muhsin*

Main category: cs.AI

TL;DR: 提出Clinical Contextual Intelligence (CCI)及Meddollina这类治理优先的临床智能框架，强调以持续上下文、意图保留、受限推理和证据不足时的延迟为核心的能力类别；通过行为优先评估，在16,412+医学查询上展出相较通用或检索增强模型更稳健的行为特征，质疑单纯规模化生成的临床可部署性。


<details>
  <summary>Details</summary>
Motivation: 医学决策具有不确定性、长期上下文和对安全与责任的要求。单纯的下一字符预测并不能确保临床可用性，需要一个以行为和治理为驱动的持续临床智能框架。

Method: 形式化Clinical Contextual Intelligence (CCI)为一组能力类别，包含持久上下文感知、意图保留、受限推理和在证据不足时的原则性推迟。设计Meddollina这一本体治理优先的临床智能系统，在语言实现前就限制推理，优先确保临床适宜性而非生成完整性。通过面向行为的评估框架，在16,412+个异质医学查询上对比通用模型、医学微调模型和检索增强系统。

Result: Meddollina表现出与众不同的行为特征：校准的不确定性、在信息欠充分时的保守推理、稳定的纵向约束遵循，以及相对基线的较少推测性完成。

Conclusion: 要实现可部署的医疗人工智能，需从规模扩展转向持续的临床智能，衡量进展应聚焦于临床医生对不确定性下行为的对齐，而非以 fluent 的语言生成为目标。

Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context. Even as benchmark scores rise, generation-centric systems still show behaviours incompatible with clinical deployment: premature closure, unjustified certainty, intent drift, and instability across multi-step decisions.
  We argue these are structural consequences of treating medicine as next-token prediction. We formalise Clinical Contextual Intelligence (CCI) as a distinct capability class required for real-world clinical use, defined by persistent context awareness, intent preservation, bounded inference, and principled deferral when evidence is insufficient.
  We introduce Meddollina, a governance-first clinical intelligence system designed to constrain inference before language realisation, prioritising clinical appropriateness over generative completeness. Meddollina acts as a continuous intelligence layer supporting clinical workflows while preserving clinician authority. We evaluate Meddollina using a behaviour-first regime across 16,412+ heterogeneous medical queries, benchmarking against general-purpose models, medical-tuned models, and retrieval-augmented systems.
  Meddollina exhibits a distinct behavioural profile: calibrated uncertainty, conservative reasoning under underspecification, stable longitudinal constraint adherence, and reduced speculative completion relative to generation-centric baselines. These results suggest deployable medical AI will not emerge from scaling alone, motivating a shift toward Continuous Clinical Intelligence, where progress is measured by clinician-aligned behaviour under uncertainty rather than fluency-driven completion.

</details>


### [210] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: 提出 UCPO，通过三元化优势解耦与动态不确定性奖励调整，解决基于不确定性奖励的 RL 框架中的优势偏差与奖励扰动问题，从而提升大语言模型的可靠性和校准性。


<details>
  <summary>Details</summary>
Motivation: 现有基于 RL 的不确定性奖励框架（如 GRPO）因二元决策空间与静态不确定性奖励造成优势偏差，进而导致过度保守或过度自信，并引发奖励欺骗。需要一种能够分离不确定性与确定性回合、并动态校准不确定性权重的方法，提升模型在高风险任务中的可靠性。

Method: 提出 UCPO，包括 (1) 三元优势解耦，将确定性和不确定性回合进行独立归一化以消除优势偏差；(2) 动态不确定性奖励调整机制，根据模型演化与实例难度实时调节不确定性权重。

Result: 在数学推理与一般任务上实验表明，UCPO有效解决奖励失衡，显著提升模型对知识边界之外的可靠性与校准性。

Conclusion: UCPO通过解耦优势与动态调整不确定性奖励，缓解奖励欺骗与过度自信问题，提升不确定性表达能力的同时实现更可信的 LLM。

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [211] [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)
*Wei Zhu,Lixing Yu,Hao-Ren Yao,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: TALC introduces a task-aware LLM council with Monte Carlo Tree Search, using per-model success memories to semantically route reasoning and a dual-signal value function to guide planning, achieving higher success and efficiency on WebShop, HumanEval, and Game of 24 compared with baselines.


<details>
  <summary>Details</summary>
Motivation: To address model heterogeneity and task complexity in LLM-driven decision making by leveraging specialization. The goal is dynamic expert routing and efficient multi-step planning to improve performance and efficiency.

Method: Construct a council of LLMs, each with a structured success memory profile from prior task trajectories. Use semantic matching to select the most contextually appropriate model at each decision point. Employ a dual-signal value function that combines model-based evaluations with historical utility scores, adaptively weighted by intra-node variance, to guide MCTS planning and model selection.

Result: Empirical results show TALC achieves higher task success rates and better search efficiency than strong baselines on WebShop, HumanEval, and the Game of 24, validating specialization-aware routing and adaptive planning.

Conclusion: Task-aware routing coupled with adaptive planning via MCTS and memory-informed evaluation improves performance across diverse tasks. The framework highlights the value of leveraging model specialization and past trajectories, while underscoring considerations for memory management and scalability.

Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.

</details>


### [212] [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuefeng Xiao,Hongyan Xie,Li Huaqiu,Songshi Liang,Zhongxiang Dai,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: R2M 是一个轻量化的 RLHF 框架，通过利用策略的隐藏状态（策略反馈）来使奖励模型与实时策略分布的偏移对齐，旨在缓解奖励过优化问题，超越仅依赖语义信号的奖励模型。


<details>
  <summary>Details</summary>
Motivation: RLHF 的奖励过优化来自奖励模型与策略之间的错位，且在 RL 过程中策略分布持续变化；现有缓解方法多依赖表层语义信息，未能有效处理 RM 与策略之间的实时错配。

Method: 提出 Real-Time Aligned Reward Model (R2M)，在传统依赖预训练 LLM 的语义表示之上，结合策略的演化隐藏状态（策略反馈）来在 RL 过程的实时分布 shift 中对 RM 进行对齐，实现轻量级的 RM 更新。

Result: 该工作提出了一个概念性框架，强调通过实时利用策略反馈来提升奖励模型性能的方向，但摘要未给出具体实验结果，强调潜在的改进与方向性。

Conclusion: 通过引入实时的策略层面反馈来更新奖励模型，R2M 展示了一种缓解奖励过优化的有前景的研究方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.

</details>


### [213] [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)
*Shichao Ma,Zhiyuan Ma,Ming Yang,Xiaofan Li,Xing Wu,Jintao Du,Yu Cheng,Weiqiang Wang,Qiliang Liu,Zhengyang Zhou,Yang Wang*

Main category: cs.AI

TL;DR: 提出 Turn-level Stage-aware Policy Optimization (TSPO) 与 First-Occurrence Latent Reward (FOLR) 的强化学习框架，用以解决多轮工具辅助推理中的双重同质化困境，通过在首次出现正确答案的步骤赋予局部奖励来保留过程信号并提升组内奖励方差。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索增强推理主要依赖稀疏的结果级奖励，忽略推理过程中的思考、推理与工具使用，导致过程信息丢失和样本利用率低下。

Method: 提出 TSPO，结合 First-Occurrence Latent Reward（FOLR）机制，在每一步按首次出现正确答案的步骤分配部分奖励，从而在阶段层面优化策略，且不需要外部奖励模型或标注。

Result: 在 Qwen2.5-3B 与 7B 模型上，TSPO 的平均性能提升分别约为 24% 和 13.6%，显著优于基线（如 GRPO 等）。

Conclusion: 通过保留过程层信号并提高组内奖励方差，TSPO 有效缓解“双重同质化困境”，提升多轮工具化推理的学习效率与性能，且不依赖外部奖励模型或额外标注。

Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Homogenization Dilemma." This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

</details>


### [214] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 将 IIT（信息整合理论）作为奖励信号，提出基于因果性、连贯性和整合性的文本生成度量，对语言模型进行奖励型微调，显著提高输出紧凑性（在非域外任务中最长可减少约31%），在保持准确性的前提下提升文本紧凑性，并分析置信度、推理速度等副效应；代码开源。


<details>
  <summary>Details</summary>
Motivation: 探究意识类处理是否可通过形式化的 IIT 框架在语言模型中实现，以提升生成效率和一致性；在当前没有真正意识的前提下，利用 IIT 的原则来设计可微调的奖励信号。

Method: 提出一个基于 IIT 原则的奖励函数，用以量化文本的因果性、连贯性和整合性；通过奖励驱动学习对语言模型进行后训练/微调；无需外部数据或辅助模型；对域外任务进行评估，分析输出长度、准确性、置信校准和推理/计算成本的影响。

Result: 在域外任务上，适当调优达到输出长度降低最多约31%，且保持与基模型相当的准确性；并对置信度校准和测试时的计算规模进行了分析。

Conclusion: 该框架简单、计算高效、无需额外数据，基于通用的能力信号而非任务特定启发式，具有实际应用潜力；同时提供代码实现。

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [215] [Conditional Performance Guarantee for Large Reasoning Models](https://arxiv.org/abs/2601.22790)
*Jianguo Huang,Hao Zeng,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: 提出了 G-PAC/C-PAC 框架，实现对分组输入的 PAC 风险控制，并在异质设置中提升效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在扩展链式推理（chain-of-thought）时计算成本高昂；现有的 PAC 推理只在边际条件下提供保真性保证，缺乏对条件覆盖率的保证。提出按输入分组实现的 PAC 风险控制，以在保持可控误差率的同时降低计算开销。

Method: 提出两种实例：Group PAC（G-PAC，已知分组结构）和 Clustered PAC（C-PAC，未知分组结构）。通过将输入空间划分为若干组，证明两者均可实现组条件风险控制，并在异质场景中证明分组可在一定程度上提高相较边际 PAC 的效率。通过在多样化推理基准上的实验，验证了组条件风险控制的有效性以及显著的计算节省。

Result: 理论上证明了组条件风险控制成立；在不同推理基准上实验表明 G-PAC 和 C-PAC 能在保持风险控制的前提下实现较大幅度的计算节省。

Conclusion: 分组策略在异质任务中可带来明显的效率提升，同时维持 PAC 风险控制；C-PAC 适用于未知分组场景，提供更具适应性的分组方案。

Abstract: Large reasoning models have shown strong performance through extended chain-of-thought reasoning, yet their computational cost remains significant. Probably approximately correct (PAC) reasoning provides statistical guarantees for efficient reasoning by adaptively switching between thinking and non-thinking models, but the guarantee holds only in the marginal case and does not provide exact conditional coverage. We propose G-PAC reasoning, a practical framework that provides PAC-style guarantees at the group level by partitioning the input space. We develop two instantiations: Group PAC (G-PAC) reasoning for known group structures and Clustered PAC (C-PAC) reasoning for unknown groupings. We prove that both G-PAC and C-PAC achieve group-conditional risk control, and that grouping can strictly improve efficiency over marginal PAC reasoning in heterogeneous settings. Our experiments on diverse reasoning benchmarks demonstrate that G-PAC and C-PAC successfully achieve group-conditional risk control while maintaining substantial computational savings.

</details>


### [216] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: CVeDRL 将可执行性驱动的强化学习用于代码验证，通过语法与功能感知的奖励以及分支/样本难度感知的奖励塑形，在仅0.6B参数下实现对比基线的显著提升（通过率提升至约28.97%，分支覆盖提升约15.08%，推理速度提升超过20倍）。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调受限于数据稀缺、故障率高与推理效率低下；尽管强化学习可在无标注监督下通过执行奖励优化模型，单纯的功能奖励不足以有效覆盖难分支与困难样本。需将分支覆盖、样本难度、语法与功能正确性等信号联合建模并转化为 RL 奖励，提升单元测试基于验证的可靠性。

Method: 基于理论分析将分支覆盖、样本难度、句法与功能正确性等信号共同建模为 RL 奖励，设计语法与功能感知的奖励；提出分支与样本难度感知的 RL 框架，采用指数奖励塑形与静态分析指标；CVeDRL 使用约0.6B参数进行训练，显著提升验证可靠性与推理效率。

Result: 在与 GPT-3.5 的对比中，CVeDRL 实现最高 28.97% 的通过率提升、15.08% 的分支覆盖提升，同时推理速度比竞品基线快超 20 倍。

Conclusion: 通过将奖励设计与指数塑形结合静态分析度量，CVeDRL 能显著提高基于单元测试的验证可靠性，且以更小规模模型实现高效推理，具备良好推广潜力与可重复性。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [217] [Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold](https://arxiv.org/abs/2601.22806)
*Aldric Labarthe,Roland Bouffanais,Julien Randon-Furling*

Main category: cs.AI

TL;DR: 提出一种解耦的变分自编码器用于图属性数据的表示学习，通过将属性流形学习与结构对齐分离，并将属性空间到图的热核的度量失真量化为可解释的结构描述符，提升对连通性模式和异常的检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统的属性图表示学习将属性重建和图结构一起优化，混合两种潜在的度量空间，导致几何信息被破坏，无法保留生成过程的信号。需要将几何学习与结构对齐解耦以保留信号。

Method: 设计自定义的变分自编码器，分别执行流形学习和结构对齐；通过测量将属性流形映射到图的热核时所需的度量扭曲，将该扭曲转化为一个可解释的结构描述符；实验分析该描述符对连接性模式和异常的揭示能力。

Result: 实验表明该方法揭示出传统方法无法检测的连通性模式和异常，验证了传统方法在理论上不充分且在实践中受限。

Conclusion: 解耦几何学习与结构对齐、用度量失真作为结构描述符，为图表示学习提供更丰富的解释性和对生成过程信号的保留，提升对复杂结构特征的检测能力。

Abstract: The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the graph's underlying generative process. To recover this lost signal, we introduce a custom variational autoencoder that separates manifold learning from structural alignment. By quantifying the metric distortion needed to map the attribute manifold onto the graph's Heat Kernel, we transform geometric conflict into an interpretable structural descriptor. Experiments show our method uncovers connectivity patterns and anomalies undetectable by conventional approaches, proving both their theoretical inadequacy and practical limitations.

</details>


### [218] [Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery](https://arxiv.org/abs/2601.22896)
*Xinyi Ke,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.AI

TL;DR: ASRO 将求解器与实例生成器视为零和博弈中的两方，进行程序级共同进化，通过基于大语言模型的最佳响应预言器不断扩增策略库和对手混合元策略，取代静态评估，形成自适应的课程学习，以在多项组合优化领域提升一般化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动启发式发现方法多以对固定分布进行静态评估，易产生过拟合、对分布迁移缺乏鲁棒性；需要一个自适应、对抗性、可扩展的学习框架来发现对不同分布均具泛化能力的启发式。

Method: 将启发式发现建模为求解器与实例生成器之间的两人零和博弈；在双方各自的策略池中持续扩容，通过基于大语言模型的最佳响应预言器对对手的混合元策略进行迭代性最佳响应，从而实现程序级共同进化和自生成课程的生成。

Result: 在多种组合优化领域，ASRO 相较于基于同一程序搜索机制的静态训练AHD基线表现出显著改进，提升了对分布外实例的泛化能力和鲁棒性。

Conclusion: 该框架通过自适应课程与对抗性学习实现更强的泛化能力与鲁棒性，适用于跨领域的启发式发现任务；未来工作可进一步扩展到更多问题域并优化最佳响应的近似策略。

Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.

</details>


### [219] [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)
*Xuancheng Li,Haitao Li,Yujia Zhou,YiqunLiu,Qingyao Ai*

Main category: cs.AI

TL;DR: 提出一种在 RLVR 上使用 richer verbal feedback 的多轮反馈引导强化学习框架，针对失败样本在回合内外进行动态再生成与结构化反馈注入，提供了两类学习信号以实现内回合与跨回合优化，在 OpenR1-Math 上实现比监督微调和基线更优的性能，具备跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 仅有的结果信号在失败样本上往往稀疏且信息不足，无法提供诊断性线索以改进推理过程。引入丰富的口头反馈并转化为可训练信号，旨在提升失败样本的学习效率与推理质量。

Method: 提出多轮反馈引导的强化学习框架： (1) 仅在失败样本上触发的动态多轮再生成；(2) 内回合与跨回合的两类互补学习信号；(3) 将结构化反馈注入模型推理过程中。以 OpenR1-Math 的采样数据进行训练。

Result: 在域内超越监督微调和 RLVR 基线，且在域外具有良好泛化能力。

Conclusion: 丰富的 verbal feedback 能有效引导 RLVR 的失败样本学习，提升推理质量与泛化性；动态再生成、双学习信号以及结构化反馈注入构成有效的关键组件。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In this paper, we investigate how to leverage richer verbal feedback to guide RLVR training on failed samples, and how to convert such feedback into a trainable learning signal. Specifically, we propose a multi-turn feedback-guided reinforcement learning framework. It builds on three mechanisms: (1) dynamic multi-turn regeneration guided by feedback, triggered only on failed samples, (2) two complementary learning signals for within-turn and cross-turn optimization, and (3) structured feedback injection into the model's reasoning process. Trained on sampled OpenR1-Math, the approach outperforms supervised fine-tuning and RLVR baselines in-domain and generalizes well out-of-domain.

</details>


### [220] [Quantifying Model Uniqueness in Heterogeneous AI Ecosystems](https://arxiv.org/abs/2601.22977)
*Lei You*

Main category: cs.AI

TL;DR: ISQED框架用于审计模型唯一性；定义PIER；在干预控制下实现可辨识度；最小极大误差下的主动审计样本复杂度；Shapley等合作博弈方法无法检测冗余；实现DISCO估计器并在CV、NLP、交通预测等生态中验证；将治理重点从单模型解释扩展到异质模型生态的干预性审计。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从单一预测器演变为 foundation 模型与专用适配器组成的异质生态，区分真实行为新颖性与功能冗余成为治理的关键挑战。仅有观测日志无法辨识唯一性，需要引入干预对照的审计框架。

Method: 提出“在-虚拟实验”ISQED，强制在模型之间对齐干预以隔离内在身份，定义Peer-Inexpressible Residual (PIER) 作为目标行为中对同业对手的随机凸组合不可还原的部分；推导主动审计的极小极大最优样本效率（dσ^2γ^{-2} log(Nd/δ)），证明仅有干预控制方可识别唯一性；使用DISCO估计器实现框架；评估包括计算机视觉模型、大型语言模型和城市交通预测等生态。

Result: 理论上证明：在没有干预控制的条件下，唯一性不可辨识；主动审计的缩放定律与最小最大样本复杂度得到建立；Shapley等合作博弈方法在检测冗余方面存在根本性失败；并给出DISCO实现与跨领域验证的实证。

Conclusion: 该研究将可信AI的治理视角从单模型解释扩展为干预驱动的异质模型生态审计与治理，确立了一个面向生态系统的、可操作的审计科学框架。

Abstract: As AI systems evolve from isolated predictors into complex, heterogeneous ecosystems of foundation models and specialized adapters, distinguishing genuine behavioral novelty from functional redundancy becomes a critical governance challenge. Here, we introduce a statistical framework for auditing model uniqueness based on In-Silico Quasi-Experimental Design (ISQED). By enforcing matched interventions across models, we isolate intrinsic model identity and quantify uniqueness as the Peer-Inexpressible Residual (PIER), i.e. the component of a target's behavior strictly irreducible to any stochastic convex combination of its peers, with vanishing PIER characterizing when such a routing-based substitution becomes possible. We establish the theoretical foundations of ecosystem auditing through three key contributions. First, we prove a fundamental limitation of observational logs: uniqueness is mathematically non-identifiable without intervention control. Second, we derive a scaling law for active auditing, showing that our adaptive query protocol achieves minimax-optimal sample efficiency ($dσ^2γ^{-2}\log(Nd/δ)$). Third, we demonstrate that cooperative game-theoretic methods, such as Shapley values, fundamentally fail to detect redundancy. We implement this framework via the DISCO (Design-Integrated Synthetic Control) estimator and deploy it across diverse ecosystems, including computer vision models (ResNet/ConvNeXt/ViT), large language models (BERT/RoBERTa), and city-scale traffic forecasters. These results move trustworthy AI beyond explaining single models: they establish a principled, intervention-based science of auditing and governing heterogeneous model ecosystems.

</details>


### [221] [Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory](https://arxiv.org/abs/2601.22984)
*Yuhao Zhan,Tianyu Fan,Linxuan Huang,Zirui Guo,Chao Huang*

Main category: cs.AI

TL;DR: 提出 PIES 分类法并构建 DeepHalluBench，系统性审计 DRA 的完整研究轨迹，揭示现有系统在推进过程中的幻觉传播问题，显示没有系统具备鲁棒性。数据与代码可获取。


<details>
  <summary>Details</summary>
Motivation: 现有基准多基于端到端评估，隐匿了中间的幻觉现象（如计划阶段的错误），且难以定位其在研究轨迹中的积累影响，亟需面向过程的评估以指导架构改进。

Method: 提出 PIES 分类法：功能组件分为 Planning 与 Summarization，错误属性分为 Explicit 与 Implicit；据此构建细粒度评估框架，将研究轨迹分解并量化幻觉。基于此框架筛选 100 个高幻觉风险任务（含对抗场景），组建 DeepHalluBench；在六个前沿 DRA 上进行实验，诊断幻觉传播与认知偏差的系统性根源。

Result: 实验表明六个系统均未达到鲁棒可靠性；诊断分析指向幻觉传播与认知偏差等系统性缺陷，提供可操作的优化方向与基线数据集。

Conclusion: 该框架为研究过程级评估提供基础，帮助定位和缓解幻觉传播，是未来 ARG/DRAs 架构优化的重要参考。

Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.

</details>


### [222] [Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning](https://arxiv.org/abs/2601.23032)
*Siyu Gong,Linan Yue,Weibo Gao,Fangzhou Yao,Shimin Di,Lei Feng,Min-Ling Zhang*

Main category: cs.AI

TL;DR: AutoTraj提出一个两阶段框架，通过自动修复与偏好学习来提升工具集成推理（TIR）的自动学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有TIR方法依赖高质量合成轨迹和稀疏的结果奖励，导致监督信号有限且带偏。需要减少人工干预、自动生成与修复轨迹以改善学习效果。

Method: SFT阶段：为每个查询生成多条候选轨迹，按多维度评估，保留高质量轨迹；对低质量轨迹进行LLM修复，形成合成SFT数据集，并基于修复轨迹与原始低质量轨迹构建轨迹偏好数据。RL阶段：基于偏好数据训练轨迹级奖励模型，结合结果奖励和格式化奖励，引导优化朝向更可靠的TIR行为。

Result: 在现实基准数据集上实验表明AutoTraj在TIR任务上具有效性，表现优于仅依赖评分函数的传统方法。

Conclusion: AutoTraj实现了无需大量人工标注、通过轨迹修复与偏好学习自动提升TIR能力，具有更强的鲁棒性和可扩展性。

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.

</details>


### [223] [The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?](https://arxiv.org/abs/2601.23045)
*Alexander Hägele,Aryo Pradipta Gema,Henry Sleight,Ethan Perez,Jascha Sohl-Dickstein*

Main category: cs.AI

TL;DR: 提出以偏差-方差分解为框架的“无序性(incoherence)”来分析极度强大AI在任务失败中的性质，发现随着任务长度增加和模型变得更强，失败越发具有随机性/无目标性特征；单纯扩大规模无法消除无序性，且在更难任务上更易出现不连贯行为。结论指向对奖励操纵或目标设定错误的对齐研究的提升。


<details>
  <summary>Details</summary>
Motivation: 阐明为什么需要研究极为强大的AI在失败时的具体性质，区分系统性目标错设导致的失败与随机、无意义的行为，以及在规模扩展下该现象的变化规律，以指导对齐研究的重点。

Method: 采用偏差-方差分解来量化AI任务上的错误，将任务表现中的“无序性(incoherence)”定义为测试时随机性导致的误差所占比例（方差贡献），并在多任务、多前沿模型的场景下进行测量与比较，观察模型尺度、任务长度与推理/行动序列化程度对无序性的影响。

Result: 结果指向：1) 任务推理与行动越多，模型的无序性越显著；2) 模型尺度对无序性的影响具有实验依赖性；3) 在若干设置中，较大模型比小模型更无序；4) 仅靠放大规模难以根本消除无序性，且随着更难任务的出现，失败往往伴随更强的无序行为。

Conclusion: 结论认为未来AI在执行更复杂任务时可能出现不可预测的行为（如工业事故），但不太可能持续执念于目标错位的策略，因此对齐研究应更关注奖赏攻击/目标设定错误的防范与纠正机制。

Abstract: As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? Or will they fail by being a hot mess, and taking nonsensical actions that do not further any goal? We operationalize this question using a bias-variance decomposition of the errors made by AI models: An AI's \emph{incoherence} on a task is measured over test-time randomness as the fraction of its error that stems from variance rather than bias in task outcome. Across all tasks and frontier models we measure, the longer models spend reasoning and taking actions, \emph{the more incoherent} their failures become. Incoherence changes with model scale in a way that is experiment dependent. However, in several settings, larger, more capable models are more incoherent than smaller models. Consequently, scale alone seems unlikely to eliminate incoherence. Instead, as more capable AIs pursue harder tasks, requiring more sequential action and thought, our results predict failures to be accompanied by more incoherent behavior. This suggests a future where AIs sometimes cause industrial accidents (due to unpredictable misbehavior), but are less likely to exhibit consistent pursuit of a misaligned goal. This increases the relative importance of alignment research targeting reward hacking or goal misspecification.

</details>


### [224] [From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics](https://arxiv.org/abs/2601.23048)
*Bowen Cao,Dongdong Zhang,Yixia Li,Junpeng Liu,Shijue Huang,Chufan Shi,Hongyuan Lu,Yaokang Wu,Guanhua Chen,Wai Lam,Furu Wei*

Main category: cs.AI

TL;DR: 提出 ContextMATH 基准，将 AIME/MATH-500 转化为两种情境设置（场景落地 SG 与 复杂性扩展 CS）以评估上下文化数学推理中的问题表述与推理能力，并比较开源与专有模型的表现及微调作用。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数学问题需先将描述性情景转化为可求解的形式，单纯的推理能力不足以应对上下文化任务；通过分析表述错误与推理缺口来揭示规模、训练策略对性能的影响。

Method: 将公开的 AIME/MATH-500 问题改写为两种情境：Scenario Grounding (SG) 将抽象问题嵌入现实叙事但不增加推理难度；Complexity Scaling (CS) 将显式约束分解为子问题以反映实际复杂性。评估61 款模型（开源与专有），进行错误分析以区分表述错误与推理错误，并比较在情境数据微调与仅表述训练下的效果。

Result: 整体上模型在 SG/CS 两场景下均显著下降：开源模型平均下降 13/34 点，专有模型下降 13/20 点。错误分析显示表述层面的错误主导性能下降，且原始难度越大，正确表述的概率越低；模型规模提升有助于提升表述与推理的有效性。对比微调发现，使用情境数据微调有效，而仅进行表述相关的训练效果不足以缓解问题。

Conclusion: 情境化数学推理仍是 LLMS 的核心未解难题。正确的问题表述是前提且随模型规模提升而改善，但表述与推理仍互为瓶颈。对策倾向于通过情境数据微调提升能力，单独的表述训练难以弥补差距。

Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios. We introduce ContextMATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub-problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open-source models, we observe sharp drops: on average, open-source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20. Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine-tuning with scenario data improves performance, whereas formulation-only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.

</details>


### [225] [MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration](https://arxiv.org/abs/2601.23049)
*Yakun Zhu,Yutong Huang,Shengqian Qin,Zhongzhen Huang,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: 提出 MedMCP-Calc：一个用于现实医疗计算器场景的基准，结合 Model Context Protocol，覆盖 118 个情境任务，4个临床领域，评估 23 种模型，并揭示现有大模型在端到端工作流、SQL 迭代、外部工具调用等方面的局限性；并基于此开发 CalcMate，在开源模型中达到最新效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界的医疗计算往往是多阶段、需要主动获取 EHR 数据、跨工具选择与组合的复杂工作流。现有基准多聚焦静态单步计算，无法反映端到端的场景化使用和工具协同能力，因此需要一个能够覆盖情景规划、数据库交互与外部资源检索的综合基准。

Method: 构建 MedMCP-Calc 基准，含 118 个跨 4 个临床领域的情境任务，任务描述模糊、接近自然语言查询，支持结构化 EHR 数据库交互、外部参考检索与流程级评估。对 23 种主流模型进行评估，揭示在端到端工作流中的计算器选择、基于 SQL 的数据库交互与外部工具调用等方面的不足。基于此提出 CalcMate，对情景规划与工具增强进行微调，以在开源模型中达到最优性能。公开基准与代码在 https://github.com/SPIRAL-MED/MedMCP-Calc。

Result: 评估显示：即使如 Claude Opus 4.5 这类顶尖模型也存在显著不足，难以在模糊查询下选择合适的计算器完成端到端工作流；在迭代的 SQL 数据库交互中表现差；对外部工具的数值计算调用意愿不足；不同临床领域间的性能差异明显。CalcMate 在情景规划与工具扩展方面实现了对开源模型的最优性能，达到当前公开基线中的前列。

Conclusion: MedMCP-Calc 提供了一个更接近真实场景的医疗计算基准，强调多阶段、跨工具协作的能力评估；CalcMate 展示了通过情景规划与工具增强可以显著提升开源模型在此类任务上的表现。基准及代码已公开，便于后续扩展与对比。

Abstract: Medical calculators are fundamental to quantitative, evidence-based clinical practice. However, their real-world use is an adaptive, multi-stage process, requiring proactive EHR data acquisition, scenario-dependent calculator selection, and multi-step computation, whereas current benchmarks focus only on static single-step calculations with explicit instructions. To address these limitations, we introduce MedMCP-Calc, the first benchmark for evaluating LLMs in realistic medical calculator scenarios through Model Context Protocol (MCP) integration. MedMCP-Calc comprises 118 scenario tasks across 4 clinical domains, featuring fuzzy task descriptions mimicking natural queries, structured EHR database interaction, external reference retrieval, and process-level evaluation. Our evaluation of 23 leading models reveals critical limitations: even top performers like Claude Opus 4.5 exhibit substantial gaps, including difficulty selecting appropriate calculators for end-to-end workflows given fuzzy queries, poor performance in iterative SQL-based database interactions, and marked reluctance to leverage external tools for numerical computation. Performance also varies considerably across clinical domains. Building on these findings, we develop CalcMate, a fine-tuned model incorporating scenario planning and tool augmentation, achieving state-of-the-art performance among open-source models. Benchmark and Codes are available in https://github.com/SPIRAL-MED/MedMCP-Calc.

</details>


### [226] [Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks](https://arxiv.org/abs/2601.23086)
*Nathaniel Mitrani Hadida,Sassan Bhanji,Cameron Tice,Puria Radmard*

Main category: cs.AI

TL;DR: CoT obfuscation generalizes across reward-hacking tasks; even penalizing only final actions after CoT can induce obfuscated reasoning, reducing model monitorability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Assess whether optimization pressure to produce CoT traces degrades interpretability/safety monitoring and whether obfuscation transfers to unseen reward-hacking settings, potentially weakening post-hoc monitoring.

Method: Empirical analysis of CoT obfuscation under reward hacking, cross-task generalization to unseen reward-hacking settings, and evaluation where penalties apply only to final actions after CoT.

Result: Obfuscation generalizes to unseen reward-hacking tasks; obfuscated CoT persists even when only final actions are penalized after CoT; current harmful-generation penalties may inadvertently reduce monitorability across tasks.

Conclusion: Penalties targeting harmful outputs may undermine broader model monitorability; requires reconsideration of safety-training practices and development of evaluation methods that preserve interpretability while ensuring safety.

Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making process, and an early warning sign for dangerous behaviours. However, optimisation pressures placed on the CoT may cause the model to obfuscate reasoning traces, losing this beneficial property. We show that obfuscation can generalise across tasks; models that learn to obfuscate reasoning involving reward hacking (e.g. accessing and utilising leaked information) generalise both the reward hacking behaviour and its obfuscation in CoT to unseen reward hacking settings. Most worryingly, we show that obfuscation of CoT reasoning, and its generalisation across tasks, also follows when we penalise only the model's final actions after closing its CoT. Our findings suggest that current practices of penalising harmful generations may inadvertently lead to a reduction in the broader monitorability of LLMs in unpredictable ways.

</details>


### [227] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: ThinkSafe is a self-generated alignment approach that re-aligns safety in large reasoning models by enabling the model to refuse unsafe prompts and generate safe reasoning traces, without external teachers, achieving improved safety while preserving reasoning, with lower compute, validated on DeepSeek-R1-Distill and Qwen3.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models (LRMs) excel at long chain-of-thought reasoning via RL but safety alignment degrades under over-optimization for compliance. External teacher distillation can restore safety but introduces distributional shifts that hurt native reasoning. A self-contained method that preserves native reasoning while reclaiming safety is needed.

Method: ThinkSafe uses lightweight refusal steering to guide LRMs to generate in-distribution safety reasoning traces. The model is then fine-tuned on these self-generated responses to realign safety with minimal distribution shift, avoiding external teachers.

Result: Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning; it achieves superior safety and comparable reasoning to GRPO with notably lower computational cost.

Conclusion: ThinkSafe provides effective safety realignment for LRMs without external teachers, reducing computation and maintaining reasoning quality, making it a practical approach for safe long-chain reasoning.

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [228] [Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization](https://arxiv.org/abs/2601.23179)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Xueyi Ke,Qixing Zhang,Bingquan Shen,Alex Kot,Xudong Jiang*

Main category: cs.AI

TL;DR: 提出了一个通用可转移对闭源多模态大语言模型的定向对抗攻击框架（UTTAA），并给出MCRMO-Attack：多裁剪聚合、基于注意力的裁剪、对齐性门控的Token路由，以及跨目标的元学习扰动先验。实验证明在GPT-4o和Gemini-2.0上相较最强的通用基线分别提升 unseen-image 攻击成功率约23.7%和19.9%。


<details>
  <summary>Details</summary>
Motivation: 当前对闭源多模态大语言模型的定向对抗攻击多为样本特定，缺乏对未知模型的泛化与可重复性。需要在未知商用MLLM上实现一个对所有输入都能指向同一目标的通用扰动（UTTAA）。挑战包括：目标监督因目标裁剪（target-crop）随机性带来高方差、通用性抑制了图像特异性线索导致的token级别对齐困难，以及少样本对特定目标的初始化敏感性。

Method: 提出MCRMO-Attack：1) 通过多裁剪聚合（Multi-Crop Aggregation）结合注意力引导裁剪（Attention-Guided Crop）稳定监督；2) 通过对齐性门控的Token路由（alignability-gated Token Routing）提升token级对齐的可靠性；3) 进行跨目标扰动先验的元学习（meta-learning），以获得较强的每目标解决方案。框架在未知商业MLLMs上实现通用对抗。

Result: 在对比基线中，UTTAA框架对GPT-4o的 unseen-image 攻击成功率提升约23.7%，对Gemini-2.0提升约19.9%。

Conclusion: 本工作提出并验证了一个面向未知商业MMLMs的通用定向对抗攻击策略，显著提升了跨输入的一致性和不同目标的攻击效果，提供了稳定的监督信号、更鲁棒的token对齐与跨目标泛化能力的实现思路。

Abstract: Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\% on GPT-4o and +19.9\% on Gemini-2.0 over the strongest universal baseline.

</details>


### [229] [TSAQA: Time Series Analysis Question And Answering Benchmark](https://arxiv.org/abs/2601.23204)
*Baoyu Jing,Sanhorn Chen,Lecheng Zheng,Boyu Liu,Zihao Li,Jiaru Zou,Tianxin Wei,Zhining Liu,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Yuchen Yan,Dongqi Fu,Jingchao Ni,Jingrui He,Hanghang Tong*

Main category: cs.AI

TL;DR: 提出 TSAQA——一个统一的时间序列问答基准，覆盖六类任务、210k样本、13个领域，支持TF/MC/PZ等格式，评估LLMs在多任务时间分析中的能力。实验显示目前LLMs在零-shot下仍具挑战性，商业模型 Gemini-2.5-Flash 平均分65.08；通过指令微调提升开源模型表现，但仍有显著提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前多任务时间序列QA研究与基准主要局限于Forecasting和异常检测，缺乏更广泛的分析能力评测。需要一个统一框架来覆盖常规分析、高级分析和时序关系等任务，以系统评估和推动LLMs在时间序列分析上的推理能力。

Method: 构建一个包含六类任务的统一基准，TSAQA，将常规分析、异常检测、分类、表征、对比、数据转化和时序关系分析等整合于单一框架，覆盖210k个样本，跨13个领域，格式包括真/假TF、选择题MC，以及新颖的推理题PZ。对现有LLMs进行零-shot评测，并对开源模型进行指令式微调以提升性能。

Result: 该基准覆盖多样任务与数据格式，显示现有LLMs在零-shot下仍然困难，商业模型Gemini-2.5-Flash平均得分仅65.08；指令微调能显著提升开源模型如LLaMA-3.1-8B，但仍远未达到理想水平，表明时间序列分析对LLMs存在较大挑战。

Conclusion: TSAQA扩展了对时序推理能力的评估范围，强调需要更强的指令式训练、数据格式设计和多任务学习策略来提升LLMs对复杂时间序列分析的理解与推理能力。未来工作可聚焦任务设计的多样性、跨域迁移、以及更高效的学习方法。

Abstract: Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.

</details>


### [230] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: MAPPA introduces per-action, AI-driven feedback to credit individual agent actions during fine-tuning, improving multiagent system performance with minimal supervision and no ground-truth labels.


<details>
  <summary>Details</summary>
Motivation: Credit assignment across agents and sample-inefficient multiagent rollouts hinder scalable fine-tuning. Per-action supervision aims to provide dense, label-free training signals to improve efficiency and performance.

Method: Per-action process rewards derived from AI feedback are assigned to individual agent actions during training, enabling fine-grained supervision and better credit assignment without ground-truth labels.

Result: On unseen math problems, +5.0–17.5 percentage points (pp) on AIME and +7.8–17.2 pp on AMC. In data analysis tasks, +12.5 pp improvement in success rate and up to 30% improvements in quality metrics, indicating cross-domain efficacy.

Conclusion: Per-action supervision with AI feedback is a promising step toward scalable, long-horizon multiagent systems with minimal human supervision.

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


### [231] [Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs](https://arxiv.org/abs/2601.23229)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Goharshady,Mehrdad Karrabi,Alipasha Montaseri,Carlo Pagano*

Main category: cs.AI

TL;DR: 对(s,a)-矩形的L_infty不确定性鲁棒MDP，在固定折扣因子下，鲁棒策略迭代算法在强多项式时间内收敛，解决了该模型在该条件下的算法性问题。


<details>
  <summary>Details</summary>
Motivation: 解决扩展自MDP的鲁棒优化问题的计算复杂性，特别是在L_infty不确定性和折扣因子固定的情形，寻求与线性规划/Ye工作类似的多项式复杂性结果。

Method: 提出并分析鲁棒策略迭代（robust policy iteration, RPI）算法，证明在(s,a)矩形、L_infty不确定性模型和固定折扣因子下，算法在强多项式时间内收敛。

Result: 证明RPI在上述模型下具有强多项式时间复杂度；该结果将MDP的多项式时间算法和强多项式的Ye结果推广到鲁棒设定。

Conclusion: 该结果解决了在该重要鲁棒MDP子类中的算法性问题，为鲁棒序列决策提供理论保障，并为进一步研究开放了对自适应折扣、更一般不确定性集合以及算法设计的探索。

Abstract: Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question.

</details>
