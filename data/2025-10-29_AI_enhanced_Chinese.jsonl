{"id": "2510.23990", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.23990", "abs": "https://arxiv.org/abs/2510.23990", "authors": ["Maruf Ahmed Mridul", "Oshani Seneviratne"], "title": "Resource-Efficient LLM Application for Structured Transformation of Unstructured Financial Contracts", "comment": "5 pages, 1 figure, 2 tables", "summary": "The transformation of unstructured legal contracts into standardized,\nmachine-readable formats is essential for automating financial workflows. The\nCommon Domain Model (CDM) provides a standardized framework for this purpose,\nbut converting complex legal documents like Credit Support Annexes (CSAs) into\nCDM representations remains a significant challenge. In this paper, we present\nan extension of the CDMizer framework, a template-driven solution that ensures\nsyntactic correctness and adherence to the CDM schema during contract-to-CDM\nconversion. We apply this extended framework to a real-world task, comparing\nits performance with a benchmark developed by the International Swaps and\nDerivatives Association (ISDA) for CSA clause extraction. Our results show that\nCDMizer, when integrated with a significantly smaller, open-source Large\nLanguage Model (LLM), achieves competitive performance in terms of accuracy and\nefficiency against larger, proprietary models. This work underscores the\npotential of resource-efficient solutions to automate legal contract\ntransformation, offering a cost-effective and scalable approach that can meet\nthe needs of financial institutions with constrained resources or strict data\nprivacy requirements.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86CDMizer\u6846\u67b6\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7684\u4fe1\u7528\u62c5\u4fdd\u9644\u4ef6\u7b49\u590d\u6742\u6cd5\u52a1\u5408\u540c\u8f6c\u5316\u4e3aCDM\u8868\u793a\u3002\u901a\u8fc7\u6a21\u677f\u9a71\u52a8\u7684\u8f6c\u6362\u65b9\u6cd5\u786e\u4fdd\u8bed\u6cd5\u6b63\u786e\u6027\u4e0eCDM\u89c4\u8303\u5bf9\u9f50\uff0c\u5e76\u4e0eISDA\u7684\u57fa\u51c6\u8fdb\u884c\u5bf9\u6bd4\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u96c6\u6210\u4e00\u4e2a\u663e\u8457\u8f83\u5c0f\u7684\u5f00\u6e90\u5927\u6a21\u578b\uff08LLM\uff09\u7684\u60c5\u51b5\u4e0b\uff0cCDMizer\u4ecd\u80fd\u5728\u51c6\u786e\u6027\u4e0e\u6548\u7387\u4e0a\u4e0e\u5927\u578b\u5546\u4e1a\u6a21\u578b\u76f8\u5f53\uff0c\u4f53\u73b0\u4e86\u8d44\u6e90\u9ad8\u6548\u89e3\u6cd5\u5728\u91d1\u878d\u5408\u540c\u81ea\u52a8\u5316\u4e2d\u7684\u6f5c\u529b\u4e0e\u53ef\u884c\u6027\u3002", "motivation": "\u5c06\u975e\u7ed3\u6784\u5316\u6cd5\u52a1\u5408\u540c\uff08\u5982CSA\uff09\u8f6c\u5316\u4e3a\u53ef\u673a\u5668\u8bfb\u53d6\u7684CDM\u683c\u5f0f\uff0c\u662f\u5b9e\u73b0\u81ea\u52a8\u5316\u91d1\u878d\u5de5\u4f5c\u6d41\u7684\u5173\u952e\u6b65\u9aa4\u3002\u6311\u6218\u5728\u4e8e\u590d\u6742\u6027\u9ad8\u3001\u9700\u8981\u4e25\u683c\u7684CDM\u8bed\u6cd5/\u7ed3\u6784\u9075\u5faa\uff0c\u540c\u65f6\u53d7\u9650\u4e8e\u6570\u636e\u9690\u79c1\u548c\u8d44\u6e90\u6210\u672c\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5bf9CDMizer\u7684\u6269\u5c55\uff0c\u91c7\u7528\u6a21\u677f\u9a71\u52a8\u7684\u8f6c\u6362\u6846\u67b6\uff0c\u786e\u4fdd\u8f93\u51fa\u4e25\u683c\u7b26\u5408CDM\u6a21\u5f0f\u4e0e\u8bed\u6cd5\u7ea6\u675f\u3002\u5c06\u6269\u5c55\u6846\u67b6\u5e94\u7528\u4e8e\u771f\u5b9e\u4efb\u52a1\uff0c\u5e76\u4e0eISDA\u7684CSA\u6761\u6b3e\u62bd\u53d6\u57fa\u51c6\u8fdb\u884c\u5bf9\u7167\u8bc4\u4f30\uff0c\u540c\u65f6\u5c06\u4e00\u4e2a\u663e\u8457\u8f83\u5c0f\u7684\u5f00\u6e90LLM\u4e0e\u7cfb\u7edf\u96c6\u6210\uff0c\u8bc4\u4f30\u5728\u51c6\u786e\u6027\u4e0e\u6548\u7387\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5728\u4e0e\u5c0f\u578b\u5f00\u6e90LLM\u7ed3\u5408\u65f6\uff0cCDMizer\u80fd\u8fbe\u5230\u4e0e\u66f4\u5927\u3001\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\uff0c\u8bc1\u660e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u53ef\u884c\u6027\u548c\u7ade\u4e89\u529b\u3002", "conclusion": "\u6269\u5c55\u540e\u7684CDMizer\u4e3a\u91d1\u878d\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u66f4\u4f4e\u3001\u53ef\u6269\u5c55\u4e14\u7b26\u5408\u9690\u79c1\u8981\u6c42\u7684\u6cd5\u5f8b\u5951\u7ea6\u8f6c\u6362\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u5c06\u6765\u63a8\u5e7f\u81f3\u66f4\u591a\u7c7b\u578b\u7684\u6cd5\u5f8b\u5408\u540c\u8f6c\u6362\u4efb\u52a1\u3002"}}
{"id": "2510.23766", "categories": ["cs.CL", "68T05", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.23766", "abs": "https://arxiv.org/abs/2510.23766", "authors": ["Ramshankar Bhuvaneswaran", "Handan Liu"], "title": "BitSkip: An Empirical Analysis of Quantization and Early Exit Composition", "comment": "Submitted to JMLR", "summary": "The pursuit of efficient Large Language Models (LLMs) has led to increasingly\ncomplex techniques like extreme quantization and dynamic routing. While\nindividual benefits of these methods are well-documented, their compositional\neffects remain poorly understood. This paper introduces BitSkip, a hybrid\narchitectural framework for systematically explor- ing these interactions.\nCounter-intuitively, our findings reveal that a simple 8-bit quantized model\nwithout Hadamard transform (BitSkip-V1) not only outperforms its more complex\n4-bit and Hadamard-enhanced counterparts but also competes the full-precision\nbaseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard\ntransforms, even at 8- bit precision, catastrophically degraded performance by\nover 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe\ndemonstrates superior early-exit characteristics, with layer 18 providing\noptimal 32.5% speed gain for minimal 4% quality loss.", "AI": {"tldr": "BitSkip \u4e3a\u4e00\u79cd\u6df7\u5408\u67b6\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u91cf\u5316\u3001\u52a8\u6001\u8def\u7531\u7b49\u6280\u672f\u7684\u7ec4\u5408\u6548\u5e94\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6ca1\u6709Hadamard\u53d8\u6362\u65f6\uff0c8\u4f4d\u91cf\u5316\u7684 BitSkip-V1 \u4e0d\u4f46\u8d85\u8d8a\u4e864\u4f4d\u53caHadamard\u589e\u5f3a\u7684\u7248\u672c\uff0c\u800c\u4e14\u5728\u8d28\u91cf\u4e0a\u53ef\u4e0e\u5168\u7cbe\u5ea6\u57fa\u7ebf\u63a5\u8fd1\uff08\u56f0\u60d1\u5ea61.13\u5bf91.19\uff09\u3002Hadamard\u53d8\u6362\u57288\u4f4d\u4e0b\u4f1a\u5bfc\u81f4\u6027\u80fd\u707e\u96be\u6027\u4e0b\u964d\uff0c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002BitSkip-V1 \u7684\u65e9\u505c\u7279\u6027\u826f\u597d\uff0c\u5c4218\u5b9e\u73b0\u7ea632.5% \u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4ec5\u4ee54%\u7684\u8d28\u91cf\u635f\u5931\u4e3a\u4ee3\u4ef7\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6781\u7aef\u91cf\u5316\u3001\u52a8\u6001\u8def\u7531\u7b49\u6280\u672f\u7684\u7ec4\u5408\u6548\u5e94\u8ba4\u8bc6\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6846\u67b6\u6765\u5206\u6790\u4e0d\u540c\u6280\u672f\u53e0\u52a0\u7684\u4e92\u76f8\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u63ed\u793a\u7ec4\u5408\u6548\u5e94\u7684\u7814\u7a76\u8def\u7ebf\u3002", "method": "\u63d0\u51fa BitSkip\uff0c\u8fd9\u662f\u4e00\u4e2a\u6df7\u5408\u67b6\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u63a2\u7a76\u91cf\u5316\u7cbe\u5ea6\u3001 Hadamard \u53d8\u6362\u3001\u4ee5\u53ca\u5176\u4ed6\u8def\u7531\u673a\u5236\u7b49\u5728\u4e0d\u540c\u7ec4\u5408\u4e0b\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u57288\u4f4d\u4e0e4\u4f4d\u4e4b\u95f4\u3001\u662f\u5426\u4f7f\u7528 Hadamard\u3001\u4ee5\u53ca\u5f15\u5165\u65e9\u505c\u7b49\u7b56\u7565\u7684\u5b9e\u9a8c\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u56f0\u60d1\u5ea6\u3001\u901f\u5ea6\u589e\u76ca\u7b49\u6307\u6807\u3002", "result": "8\u4f4d BitSkip-V1 \u4f18\u4e8e4\u4f4d\u53caHadamard\u589e\u5f3a\u7248\u672c\uff0c\u4e14\u5728\u67d0\u4e9b\u6307\u6807\u4e0a\u63a5\u8fd1\u5168\u7cbe\u5ea6\u57fa\u7ebf\uff08\u56f0\u60d1\u5ea61.13 vs 1.19\uff09\u3002\u57288\u4f4d\u4e0b\u5f15\u5165Hadamard\u53d8\u6362\u4f1a\u9020\u6210\u6027\u80fd\u707e\u96be\u6027\u4e0b\u964d\uff0c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u589e\u5e45\u53ef\u80fd\u8d85\u8fc737,000%\u3002BitSkip-V1 \u7684\u65e9\u505c\u7279\u6027\u826f\u597d\uff0c\u5c4218\u63d0\u4f9b\u7ea632.5%\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u4ec5\u5e26\u6765\u7ea64%\u7684\u8d28\u91cf\u635f\u5931\u3002", "conclusion": "\u7b80\u5355\u76848\u4f4d\u91cf\u5316\uff08BitSkip-V1\uff09\u5728\u7ec4\u5408\u6548\u5e94\u65b9\u9762\u6bd4\u590d\u6742\u76844\u4f4d\u4e0eHadamard\u7ec4\u5408\u66f4\u4f18\uff0cHadamard\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u7cfb\u7edf\u5316\u6846\u67b6 BitSkip \u80fd\u5e2e\u52a9\u63ed\u793a\u4e0d\u540c\u4f18\u5316\u6280\u672f\u7684\u4e92\u76f8\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u65e9\u505c\u7b56\u7565\u5b9e\u73b0\u53ef\u89c2\u7684\u901f\u5ea6\u63d0\u5347\u800c\u6210\u672c\u6709\u9650\u7684\u8d28\u91cf\u635f\u5931\u3002"}}
{"id": "2510.24369", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24369", "abs": "https://arxiv.org/abs/2510.24369", "authors": ["Yutian Xiao", "Meng Yuan", "Fuzhen Zhuang", "Wei Chen", "Shukuan Wang", "Shanqi Liu", "Chao Feng", "Wenhui Yu", "Xiang Li", "Lantao Hu", "Han Li", "Zhao Zhang"], "title": "DUET: Dual Model Co-Training for Entire Space CTR Prediction", "comment": null, "summary": "The pre-ranking stage plays a pivotal role in large-scale recommender systems\nbut faces an intrinsic trade-off between model expressiveness and computational\nefficiency. Owing to the massive candidate pool and strict latency constraints,\nindustry systems often rely on lightweight two-tower architectures, which are\ncomputationally efficient yet limited in estimation capability. As a result,\nthey struggle to capture the complex synergistic and suppressive relationships\namong candidate items, which are essential for producing contextually coherent\nand diverse recommendation lists. Moreover, this simplicity further amplifies\nthe Sample Selection Bias (SSB) problem, as coarse-grained models trained on\nbiased exposure data must generalize to a much larger candidate space with\ndistinct distributions.\n  To address these issues, we propose \\textbf{DUET} (\\textbf{DU}al Model\nCo-Training for \\textbf{E}ntire Space C\\textbf{T}R Prediction), a set-wise\npre-ranking framework that achieves expressive modeling under tight\ncomputational budgets. Instead of scoring items independently, DUET performs\nset-level prediction over the entire candidate subset in a single forward pass,\nenabling information-aware interactions among candidates while amortizing the\ncomputational cost across the set. Moreover, a dual model co-training mechanism\nextends supervision to unexposed items via mutual pseudo-label refinement,\neffectively mitigating SSB. Validated through extensive offline experiments and\nonline A/B testing, DUET consistently outperforms state-of-the-art baselines\nand achieves improvements across multiple core business metrics. At present,\nDUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the\nmain traffic for hundreds of millions of users.", "AI": {"tldr": "\u63d0\u51fa DUET\uff0c\u4e00\u79cd\u96c6\u5408\u7ea7\u524d\u6392\u5e8f\u6846\u67b6\uff0c\u5728\u6709\u9650\u9884\u7b97\u4e0b\u5b9e\u73b0\u5bf9\u6574\u4e2a\u5019\u9009\u5b50\u96c6\u7684\u9ad8\u8868\u8fbe\u529b\u5efa\u6a21\uff0c\u901a\u8fc7\u53cc\u6a21\u6001\u5171\u8bad\u7ec3\u7f13\u89e3\u6837\u672c\u9009\u62e9\u504f\u5dee\uff0c\u5e76\u5728\u5b9e\u9645\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u4e24\u5854\u5f0f\u524d\u6392\u5e8f\u5728\u8868\u8fbe\u80fd\u529b\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u6743\u8861\uff0c\u96be\u4ee5\u5efa\u6a21\u5019\u9009\u9879\u4e4b\u95f4\u7684\u534f\u540c\u4e0e\u7ea6\u675f\u5173\u7cfb\uff0c\u4e14\u6613\u653e\u5927\u6837\u672c\u9009\u62e9\u504f\u5dee(SSB)\u3002\u9700\u8981\u5728\u4fdd\u6301\u4f4e\u5ef6\u65f6\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u5347\u5efa\u6a21\u8868\u8fbe\u529b\u548c\u5bf9\u5019\u9009\u96c6\u7684\u591a\u6837\u6027\u63a7\u5236\u3002", "method": "\u5f15\u5165\u96c6\u5408\u7ea7\u9884\u6d4b\uff0c\u5728\u4e00\u4e2a\u524d\u5411\u8ba1\u7b97\u4e2d\u5bf9\u6574\u4e2a\u5019\u9009\u5b50\u96c6\u8fdb\u884c\u9884\u6d4b\uff0c\u5229\u7528\u5019\u9009\u9879\u4e4b\u95f4\u7684\u4fe1\u606f\u4ea4\u4e92\uff1b\u5e76\u901a\u8fc7\u53cc\u6a21\u5171\u8bad\u7ec3\u673a\u5236\uff0c\u5229\u7528\u672a\u66b4\u9732\u9879\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u4e92\u76f8 refine\uff0c\u7f13\u89e3SSB\u3002", "result": "\u5728\u79bb\u7ebf\u5927\u89c4\u6a21\u5b9e\u9a8c\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cDUET\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u5e76\u5728\u591a\u9879\u6838\u5fc3\u4e1a\u52a1\u6307\u6807\u4e0a\u83b7\u5f97\u63d0\u5347\uff1b\u5df2\u5728\u5feb\u624b\u53ca\u5feb\u624bLite\u7b49\u5e94\u7528\u5168\u91cf\u6295\u653e\uff0c\u670d\u52a1\u6570\u4ee5\u4ebf\u8ba1\u7528\u6237\u3002", "conclusion": "DUET\u5728\u4e25\u683c\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u66f4\u5177\u8868\u73b0\u529b\u7684\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u53cc\u6a21\u5171\u8bad\u7ec3\u7f13\u89e3SSB\uff0c\u5177\u5907\u5927\u89c4\u6a21\u5b9e\u9645\u843d\u5730\u80fd\u529b\u3002"}}
{"id": "2510.23828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23828", "abs": "https://arxiv.org/abs/2510.23828", "authors": ["Mena Attia", "Aashiq Muhamed", "Mai Alkhamissi", "Thamar Solorio", "Mona Diab"], "title": "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language", "comment": null, "summary": "We present a comprehensive evaluation of the ability of large language models\n(LLMs) to process culturally grounded language, specifically to understand and\npragmatically use figurative expressions that encode local knowledge and\ncultural nuance. Using figurative language as a proxy for cultural nuance and\nlocal knowledge, we design evaluation tasks for contextual understanding,\npragmatic use, and connotation interpretation in Arabic and English. We\nevaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,\nmultidialectal Arabic proverbs, and English proverbs. Our results show a\nconsistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower\nthan for English proverbs, and performance for Egyptian idioms is 10.28% lower\nthan for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%\nrelative to understanding, though providing contextual idiomatic sentences\nimproves accuracy by 10.66%. Models also struggle with connotative meaning,\nreaching at most 85.58% agreement with human annotators on idioms with 100%\ninter-annotator agreement. These findings demonstrate that figurative language\nserves as an effective diagnostic for cultural reasoning: while LLMs can often\ninterpret figurative meaning, they face challenges in using it appropriately.\nTo support future research, we release Kinayat, the first dataset of Egyptian\nArabic idioms designed for both figurative understanding and pragmatic use\nevaluation.", "AI": {"tldr": "LLMs\u80fd\u7406\u89e3\u5e76\u8fd0\u7528\u5177\u6587\u5316\u542b\u4e49\u7684\u8bed\u8a00\uff0c\u4f46\u5728\u672c\u5730\u5316\u6bd4\u55bb\u7684 pragmatics \u4e0a\u5b58\u5728\u6311\u6218\uff1b\u63d0\u4f9b Egyptian Arabic idioms\u3001Arabic proverbs\u3001English proverbs \u7684\u8bc4\u4f30\uff0c\u4ee5\u53ca Kinayat \u6570\u636e\u96c6\u3002", "motivation": "\u901a\u8fc7\u7528\u5177\u6587\u5316\u542b\u4e49\u7684\u6bd4\u55bb\u6765\u8bca\u65adLLMs\u7684\u6587\u5316\u63a8\u7406\u80fd\u529b\uff0c\u6d4b\u8bd5\u7406\u89e3\u3001\u8bed\u7528\u4f7f\u7528\u548c\u9690\u542b\u610f\u4e49\u89e3\u8bfb\u3002", "method": "\u5bf922\u79cd\u5f00\u653e/\u95ed\u6e90LLMs \u5728\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed Idioms\u3001\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\uff08\u591a\u65b9\u8a00\uff09\u3001\u82f1\u8bed\u8c1a\u8bed\u4e0a\u8fdb\u884c\u4efb\u52a1\u8bc4\u4f30\uff0c\u8bbe\u7acb\u60c5\u5883\u7406\u89e3\u3001\u8bed\u7528\u4f7f\u7528\u3001\u9690\u55bb\u5185\u6db5\u89e3\u8bfb\u4e09\u9879\u4efb\u52a1\uff1b\u6bd4\u8f83\u4e09\u7ec4\u8bed\u8a00\u7684\u8868\u73b0\u3002", "result": "\u963f\u62c9\u4f2f\u8c1a\u8bed\u5e73\u5747\u51c6\u786e\u5ea6\u6bd4\u82f1\u8bed\u8c1a\u8bed\u4f4e4.29%\uff1b\u57c3\u53ca\u65b9\u8a00\u6210\u8bed\u6bd4\u963f\u62c9\u4f2f\u8c1a\u8bed\u4f4e10.28%\uff1b\u8bed\u7528\u4f7f\u7528\u4efb\u52a1\u51c6\u786e\u5ea6\u8f83\u7406\u89e3\u4f4e14.07%\uff0c\u4f46\u63d0\u4f9b\u60c5\u5883\u53e5\u5b50\u540e\u63d0\u534710.66%\uff1b\u9690\u8ba2\u4e49\u4e49\u610f\u4e49\u4e00\u81f4\u6027\u6700\u9ad8\u5728\u5bf9100%\u8de8\u8bc4\u6ce8\u4e00\u81f4\u6027\u7684\u6210\u8bed\u4e0a\u8fbe85.58%\uff1b\u63d0\u51faKinayat\u6570\u636e\u96c6\uff0c\u9996\u4e2a\u7528\u4e8e\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u6210\u8bed\u7684\u7406\u89e3\u4e0e\u8bed\u7528\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "conclusion": "\u5177\u8c61\u5f81\u6027\u8bed\u8a00\u53ef\u4f5c\u4e3a\u6587\u5316\u63a8\u7406\u7684\u6709\u529b\u8bca\u65ad\uff0cLLMs\u5728\u7406\u89e3\u65b9\u9762\u901a\u5e38\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6070\u5f53\u5730\u4f7f\u7528 figurative language \u4ee5\u4f53\u73b0\u6587\u5316\u8bed\u5883\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002"}}
{"id": "2510.24402", "categories": ["cs.IR", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.24402", "abs": "https://arxiv.org/abs/2510.24402", "authors": ["Michail Dadopoulos", "Anestis Ladas", "Stratos Moschidis", "Ioannis Negkakis"], "title": "Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering", "comment": "Preprint version submitted to the International Journal of Accounting\n  Information Systems; currently under major revision. 20 pages, 1 figure, 1\n  table", "summary": "Retrieval-Augmented Generation (RAG) struggles on long, structured financial\nfilings where relevant evidence is sparse and cross-referenced. This paper\npresents a systematic investigation of advanced metadata-driven\nRetrieval-Augmented Generation (RAG) techniques, proposing and evaluating a\nnovel, multi-stage RAG architecture that leverages LLM-generated metadata. We\nintroduce a sophisticated indexing pipeline to create contextually rich\ndocument chunks and benchmark a spectrum of enhancements, including\npre-retrieval filtering, post-retrieval reranking, and enriched embeddings,\nbenchmarked on the FinanceBench dataset. Our results reveal that while a\npowerful reranker is essential for precision, the most significant performance\ngains come from embedding chunk metadata directly with text (\"contextual\nchunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval\noptimizations with these contextual embeddings to achieve superior performance.\nAdditionally, we present a custom metadata reranker that offers a compelling,\ncost-effective alternative to commercial solutions, highlighting a practical\ntrade-off between peak performance and operational efficiency. This study\nprovides a blueprint for building robust, metadata-aware RAG systems for\nfinancial document analysis.", "AI": {"tldr": "\u9488\u5bf9\u957f\u7bc7\u7ed3\u6784\u5316\u91d1\u878d\u62ab\u9732\u7684RAG\uff0c\u63d0\u51fa\u5143\u6570\u636e\u9a71\u52a8\u7684\u591a\u9636\u6bb5RAG\u67b6\u6784\u4e0e\u4e0a\u4e0b\u6587\u5316\u5206\u5757\u5d4c\u5165\uff0c\u7ed3\u5408LLM\u751f\u6210\u7684\u5143\u6570\u636e\uff0c\u5e76\u5f15\u5165\u81ea\u5efa\u7684\u5143\u6570\u636e\u91cd\u6392\u5668\uff0c\u5728FinanceBench\u4e0a\u5b9e\u73b0\u5bf9\u68c0\u7d22-\u751f\u6210\u8d28\u91cf\u7684\u663e\u8457\u63d0\u5347\uff0c\u7ed9\u51fa\u53ef\u843d\u5730\u7684\u91d1\u878d\u6587\u6863\u5206\u6790RAG\u84dd\u672c\u3002", "motivation": "\u5f53\u524d\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u957f\u7bc7\u3001\u7ed3\u6784\u5316\u4e14\u8bc1\u636e\u7a00\u758f\u3001\u9700\u8981\u8de8\u5f15\u7528\u7684\u91d1\u878d\u62ab\u9732\u573a\u666f\u4e2d\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u63d0\u9ad8\u68c0\u7d22-\u751f\u6210\u7684\u4e00\u81f4\u6027\u4e0e\u53ef\u63a7\u6027\uff0c\u9700\u5229\u7528\u5143\u6570\u636e\u9a71\u52a8\u7684\u7d22\u5f15\u3001\u5206\u5757\u7b56\u7565\u548c\u5d4c\u5165\u8868\u793a\u6765\u589e\u5f3a\u8bc1\u636e\u547d\u4e2d\u7387\u4e0e\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002\u540c\u65f6\uff0c\u73b0\u5b9e\u5e94\u7528\u9700\u8981\u517c\u5177\u6027\u80fd\u4e0e\u6210\u672c\u6548\u76ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u63d0\u51fa\u591a\u9636\u6bb5RAG\u67b6\u6784\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u6587\u6863\u5143\u6570\u636e\u4ee5\u9a71\u52a8\u68c0\u7d22\u4e0e\u91cd\u6392\uff1b2) \u8bbe\u8ba1\u590d\u6742\u7684\u7d22\u5f15\u7ba1\u7ebf\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u6587\u6863\u5757\uff08contextual chunks\uff09\uff1b3) \u5b9e\u73b0\u524d\u7f6e\u68c0\u7d22\u8fc7\u6ee4\u3001\u68c0\u7d22\u540e\u91cd\u6392\u5e8f\u3001\u4ee5\u53ca\u5d4c\u5165\u4e2d\u5305\u542b\u5143\u6570\u636e\u7684\u589e\u5f3a\u5d4c\u5165\uff1b4) \u5728 FinanceBench \u6570\u636e\u96c6\u4e0a\u5bf9\u4e0a\u8ff0\u7b56\u7565\u8fdb\u884c\u7cfb\u7edf\u6027\u57fa\u51c6\uff1b5) \u63d0\u51fa\u81ea\u5b9a\u4e49\u5143\u6570\u636e\u91cd\u6392\u5668\uff0c\u4f5c\u4e3a\u6210\u672c\u53cb\u597d\u66ff\u4ee3\u5546\u4e1a\u5316\u65b9\u6848\u3002", "result": "\u7ed3\u8bba\u6027\u53d1\u73b0\u5305\u62ec\uff1aa) \u5f3a\u5316\u7684\u91cd\u6392\u5e8f\u5668\u5bf9\u63d0\u5347\u68c0\u7d22\u7684\u7cbe\u5ea6\u81f3\u5173\u91cd\u8981\uff1bb) \u5c06\u5206\u5757\u6587\u672c\u4e0e\u5143\u6570\u636e\u5d4c\u5165\u7ed3\u5408\uff08\u4e0a\u4e0b\u6587\u5757\uff09\u80fd\u5e26\u6765\u6700\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff1bc) \u6700\u4f18\u67b6\u6784\u6765\u81ea\u4e8e\u5c06LLM\u9a71\u52a8\u7684\u524d\u7f6e\u68c0\u7d22\u4f18\u5316\u4e0e contextual embeddings \u76f8\u7ed3\u5408\uff1bd) \u81ea\u5b9a\u4e49\u5143\u6570\u636e\u91cd\u6392\u5668\u5728\u5cf0\u503c\u6027\u80fd\u4e0e\u8fd0\u8425\u6210\u672c\u4e4b\u95f4\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u7684\u6743\u8861\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u91d1\u878d\u6587\u6863\u5206\u6790\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u3001\u5143\u6570\u636e\u611f\u77e5\u7684RAG\u7cfb\u7edf\u8bbe\u8ba1\u84dd\u672c\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u5143\u6570\u636e\u5d4c\u5165\u4e0e\u591a\u9636\u6bb5\u68c0\u7d22\u4f18\u5316\u5b9e\u73b0\u53ef\u843d\u5730\u7684\u9ad8\u7cbe\u5ea6RAG\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5bf9\u6210\u672c\u4e0e\u6027\u80fd\u7684\u6743\u8861\u3002"}}
{"id": "2510.23842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23842", "abs": "https://arxiv.org/abs/2510.23842", "authors": ["Saki Imai", "Lee Kezar", "Laurel Aichler", "Mert Inan", "Erin Walker", "Alicia Wooten", "Lorna Quandt", "Malihe Alikhani"], "title": "How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse", "comment": null, "summary": "Most state-of-the-art sign language models are trained on interpreter or\nisolated vocabulary data, which overlooks the variability that characterizes\nnatural dialogue. However, human communication dynamically adapts to contexts\nand interlocutors through spatiotemporal changes and articulation style. This\nspecifically manifests itself in educational settings, where novel vocabularies\nare used by teachers, and students. To address this gap, we collect a motion\ncapture dataset of American Sign Language (ASL) STEM (Science, Technology,\nEngineering, and Mathematics) dialogue that enables quantitative comparison\nbetween dyadic interactive signing, solo signed lecture, and interpreted\narticles. Using continuous kinematic features, we disentangle dialogue-specific\nentrainment from individual effort reduction and show spatiotemporal changes\nacross repeated mentions of STEM terms. On average, dialogue signs are\n24.6%-44.6% shorter in duration than the isolated signs, and show significant\nreductions absent in monologue contexts. Finally, we evaluate sign embedding\nmodels on their ability to recognize STEM signs and approximate how entrained\nthe participants become over time. Our study bridges linguistic analysis and\ncomputational modeling to understand how pragmatics shape sign articulation and\nits representation in sign language technologies.", "AI": {"tldr": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7528\u4e8eASL STEM\u5bf9\u8bdd\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u5bf9\u8bdd\u3001\u72ec\u767d\u8bb2\u5ea7\u548c\u7ffb\u8bd1\u6587\u7ae0\u7b49\u60c5\u5883\u4e2d\u7684\u624b\u8bed\uff1b\u53d1\u73b0\u5bf9\u8bdd\u4e2d\u7684\u624b\u8bed\u6bd4\u72ec\u7acb\u624b\u8bed\u66f4\u77ed\u4e14\u5448\u73b0entrainment\uff1b\u8bc4\u4f30\u624b\u8bed\u5d4c\u5165\u6a21\u578b\u4ee5\u8bc6\u522bSTEM\u624b\u8bed\u5e76\u4f30\u8ba1entrainment\u7a0b\u5ea6\u3002", "motivation": "\u5f53\u524d\u5927\u90e8\u5206\u524d\u6cbf\u624b\u8bed\u6a21\u578b\u4ee5\u53e3\u8bd1\u5458\u6216\u72ec\u7acb\u8bcd\u6c47\u6570\u636e\u8bad\u7ec3\uff0c\u5ffd\u89c6\u81ea\u7136\u5bf9\u8bdd\u4e2d\u7684\u53d8\u5f02\u6027\u3002\u4eba\u7c7b\u6c9f\u901a\u5728\u65f6\u7a7a\u3001\u53d1\u5f0f\u7b49\u65b9\u9762\u968f\u60c5\u5883\u548c\u5bf9\u8bdd\u8005\u800c\u53d8\u5316\uff0c\u6559\u80b2\u60c5\u5883\u4e2d\u6559\u5e08\u548c\u5b66\u751f\u4f1a\u4f7f\u7528\u65b0\u8bcd\u6c47\uff0c\u9700\u7814\u7a76\u5bf9\u8bdd\u4e2d\u7684\u6307\u624b\u52bf\u53d8\u5f02\u53ca\u5176\u8868\u5f81\u3002", "method": "\u6784\u5efaASL STEM\u5bf9\u8bdd\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\uff0c\u5b9a\u91cf\u6bd4\u8f83\u4e8c\u4eba\u4e92\u52a8\u7b7e\u540d\u3001\u72ec\u767d\u8bb2\u6388\u548c\u89e3\u8bfb\u6587\u7ae0\u7684\u7b7e\u540d\u3002\u5229\u7528\u8fde\u7eed\u8fd0\u52a8\u5b66\u7279\u5f81\uff0c\u533a\u5206\u5bf9\u8bdd entrainment \u4e0e\u4e2a\u4eba\u52aa\u529b\u51cf\u5c11\uff0c\u5206\u6790\u91cd\u590d\u63d0\u53caSTEM\u672f\u8bed\u65f6\u7684\u65f6\u7a7a\u53d8\u5316\u3002", "result": "\u5bf9\u8bdd\u4e2d\u7684\u7b7e\u540d\u5728\u6301\u7eed\u65f6\u95f4\u4e0a\u5e73\u5747\u6bd4\u72ec\u7acb\u7b7e\u540d\u77ed24.6%\u201344.6%\uff0c\u5728\u72ec\u767d\u60c5\u5883\u4e2d\u672a\u51fa\u73b0\u7684\u663e\u8457\u51cf\u5c11\uff1b\u5e76\u5448\u73b0\u663e\u8457\u7684\u5bf9\u8bdd\u7279\u5f02\u6027\u65f6\u7a7a\u53d8\u5316\u4e0eentrainment\u3002\u5bf9\u624b\u8bed\u5d4c\u5165\u6a21\u578b\u5728\u8bc6\u522bSTEM\u672f\u8bed\u65b9\u9762\u6709\u826f\u597d\u8868\u73b0\uff0c\u5e76\u80fd\u968f\u7740\u65f6\u95f4\u4f30\u8ba1\u53c2\u4e0e\u8005\u7684 entrainment \u7a0b\u5ea6\u3002", "conclusion": "\u672c\u5de5\u4f5c\u5c06\u8bed\u8a00\u5206\u6790\u4e0e\u8ba1\u7b97\u5efa\u6a21\u7ed3\u5408\uff0c\u5e2e\u52a9\u7406\u89e3\u8bed\u7528\u5b66\u5982\u4f55\u5f71\u54cd\u624b\u8bed\u8868\u8fbe\u53ca\u5176\u5728\u624b\u8bed\u6280\u672f\u4e2d\u7684\u8868\u793a\u3002"}}
{"id": "2510.24430", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24430", "abs": "https://arxiv.org/abs/2510.24430", "authors": ["Yejin Kim", "Shaghayegh Agah", "Mayur Nankani", "Neeraj Sharma", "Feifei Peng", "Maria Peifer", "Sardar Hamidian", "H Howie Huang"], "title": "From Time and Place to Preference: LLM-Driven Geo-Temporal Context in Recommendations", "comment": null, "summary": "Most recommender systems treat timestamps as numeric or cyclical values,\noverlooking real-world context such as holidays, events, and seasonal patterns.\nWe propose a scalable framework that uses large language models (LLMs) to\ngenerate geo-temporal embeddings from only a timestamp and coarse location,\ncapturing holidays, seasonal trends, and local/global events. We then introduce\na geo-temporal embedding informativeness test as a lightweight diagnostic,\ndemonstrating on MovieLens, LastFM, and a production dataset that these\nembeddings provide predictive signal consistent with the outcomes of full model\nintegrations. Geo-temporal embeddings are incorporated into sequential models\nthrough (1) direct feature fusion with metadata embeddings or (2) an auxiliary\nloss that enforces semantic and geo-temporal alignment. Our findings highlight\nthe need for adaptive or hybrid recommendation strategies, and we release a\ncontext-enriched MovieLens dataset to support future research.", "AI": {"tldr": "LLMs are used to generate geo-temporal embeddings from a timestamp and coarse location to capture holidays, seasonal trends, and events; these embeddings can be integrated into sequential recommenders via feature fusion or an auxiliary loss, and a lightweight informativeness test validates their predictive signal on MovieLens, LastFM, and a production dataset; a context-enriched MovieLens dataset is released.", "motivation": "Fill the gap where time is treated as simple numeric or cyclical and ignores real-world context (holidays, events, seasonal patterns) that affect user behavior.", "method": "1) Generate geo-temporal embeddings from timestamp + coarse location using large language models. 2) Propose a geo-temporal embedding informativeness test as a lightweight diagnostic. 3) Evaluate on MovieLens, LastFM, and a production dataset to show embedding signal aligns with full model integrations. 4) Incorporate embeddings into sequential models via (a) direct feature fusion with metadata embeddings or (b) an auxiliary loss enforcing semantic/geo-temporal alignment. 5) Release a context-enriched MovieLens dataset.", "result": "The geo-temporal embeddings provide predictive signal consistent with full-model integrations across the evaluated datasets. The approach demonstrates that incorporating contextualized temporal information can augment sequential recommender models without full end-to-end retraining.", "conclusion": "Geo-temporal context is valuable for recommender systems; adaptive or hybrid strategies are needed to leverage contextual signals effectively. The authors also contribute a context-enriched MovieLens dataset to support future research."}}
{"id": "2510.23617", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23617", "abs": "https://arxiv.org/abs/2510.23617", "authors": ["Phuong Q. Dao", "Mark Roantree", "Vuong M. Ngo"], "title": "An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis", "comment": "The paper has been accepted for presentation at the MEDES 2025\n  conference", "summary": "Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by\njointly analyzing data from multiple modalities typically text and images\noffering a richer and more accurate interpretation than unimodal approaches. In\nthis paper, we first propose BERT-ViT-EF, a novel model that combines powerful\nTransformer-based encoders BERT for textual input and ViT for visual input\nthrough an early fusion strategy. This approach facilitates deeper cross-modal\ninteractions and more effective joint representation learning. To further\nenhance the model's capability, we propose an extension called the Dual\nTransformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN\nincorporates an additional Transformer encoder layer after BERT to refine\ntextual context (before fusion) and employs contrastive learning to align text\nand image representations, fostering robust multimodal feature learning.\nEmpirical results on two widely used MSA benchmarks MVSA-Single and TumEmo\ndemonstrate the effectiveness of our approach. DTCN achieves best accuracy\n(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on\nMVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements\nhighlight the benefits of early fusion and deeper contextual modeling in\nTransformer-based multimodal sentiment analysis.", "AI": {"tldr": "\u63d0\u51fa BERT-ViT-EF \u4e0e DTCN \u7684\u7aef\u5230\u7aef\u8de8\u6a21\u6001\u60c5\u611f\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c-\u56fe\u50cf\u7684\u65e9\u671f\u878d\u5408\u548c\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u60c5\u611f\u8bc6\u522b\uff0cTumEmo \u548c MVSA-Single \u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\uff0c\u901a\u8fc7\u66f4\u5f3a\u7684\u8de8\u6a21\u6001\u4e92\u52a8\u4e0e\u4e0a\u4e0b\u6587\u5efa\u6a21\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u60c5\u611f\u7406\u89e3\uff1b\u5355\u6a21\u6001\u65b9\u6cd5\u53d7\u9650\uff0c\u9700\u66f4\u6df1\u5c42\u6b21\u7684\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u3002", "method": "\u9996\u5148\u63d0\u51fa BERT-ViT-EF\uff0c\u5c06 BERT\uff08\u6587\u672c\uff09\u548c ViT\uff08\u89c6\u89c9\uff09\u8fdb\u884c\u65e9\u671f\u878d\u5408\uff0c\u4ee5\u4fc3\u8fdb\u8de8\u6a21\u6001\u4e92\u52a8\u3002\u968f\u540e\u5728\u6b64\u57fa\u7840\u4e0a\u6269\u5c55 DTCN\uff1a\u5728 BERT \u4e4b\u540e\u589e\u52a0\u989d\u5916 Transformer \u7f16\u7801\u5c42\u4ee5\u7ec6\u5316\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u518d\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u6587\u672c\u4e0e\u56fe\u50cf\u8868\u5f81\uff0c\u5f3a\u5316\u591a\u6a21\u6001\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728 MVSA-Single \u4e0e TumEmo \u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u7ed3\u679c\uff1aTumEmo 78.4% \u7cbe\u5ea6\u4e0e 78.3% F1\uff1bMVSA-Single 76.6% \u7cbe\u5ea6\u4e0e 75.9% F1\u3002", "conclusion": "\u65e9\u671f\u878d\u5408\u4e0e\u66f4\u6df1\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\uff08\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\uff09\u80fd\u63d0\u5347 Transformer \u57fa\u4e8e\u591a\u6a21\u6001\u7684\u60c5\u611f\u5206\u6790\u6548\u679c\u3002"}}
{"id": "2510.23845", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23845", "abs": "https://arxiv.org/abs/2510.23845", "authors": ["Grace Byun", "Rebecca Lipschutz", "Sean T. Minton", "Abigail Lott", "Jinho D. Choi"], "title": "CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection", "comment": null, "summary": "Detecting mental health crisis situations such as suicide ideation, rape,\ndomestic violence, child abuse, and sexual harassment is a critical yet\nunderexplored challenge for language models. When such situations arise during\nuser--model interactions, models must reliably flag them, as failure to do so\ncan have serious consequences. In this work, we introduce CRADLE BENCH, a\nbenchmark for multi-faceted crisis detection. Unlike previous efforts that\nfocus on a limited set of crisis types, our benchmark covers seven types\ndefined in line with clinical standards and is the first to incorporate\ntemporal labels. Our benchmark provides 600 clinician-annotated evaluation\nexamples and 420 development examples, together with a training corpus of\naround 4K examples automatically labeled using a majority-vote ensemble of\nmultiple language models, which significantly outperforms single-model\nannotation. We further fine-tune six crisis detection models on subsets defined\nby consensus and unanimous ensemble agreement, providing complementary models\ntrained under different agreement criteria.", "AI": {"tldr": "\u63d0\u51fa CRADLE BENCH \u57fa\u51c6\u7528\u4e8e\u591a\u7c7b\u578b\u5371\u673a\u68c0\u6d4b\uff0c\u8986\u76d6\u4e03\u79cd\u5371\u673a\u7c7b\u578b\u5e76\u5f15\u5165\u65f6\u5e8f\u6807\u7b7e\uff1b\u63d0\u4f9b600\u6761\u4e34\u5e8a\u6807\u6ce8\u8bc4\u4f30\u6837\u672c\u548c420\u6761\u5f00\u53d1\u6837\u672c\uff0c\u4ee5\u53ca\u7ea64K\u6761\u901a\u8fc7\u591a\u6570\u6295\u7968\u96c6\u6210\u6807\u6ce8\u7684\u8bad\u7ec3\u8bed\u6599\uff1b\u5e76\u5728\u5171\u8bc6\u4e0e\u5168\u6743\u4e00\u81f4\u6027\u5206\u652f\u4e0a\u5fae\u8c03\u516d\u4e2a\u6a21\u578b\uff0c\u63d0\u4f9b\u4e92\u8865\u7684\u6a21\u578b\u96c6\u5408\u3002", "motivation": "\u5728\u7528\u6237-\u6a21\u578b\u4ea4\u4e92\u4e2d\u5bf9\u5371\u673a\u60c5\u666f\uff08\u5982\u81ea\u6740\u610f\u5ff5\u3001\u5f3a\u5978\u3001\u5bb6\u66b4\u3001\u8650\u5f85\u3001\u6027\u9a9a\u6270\u7b49\uff09\u8fdb\u884c\u53ef\u9760\u68c0\u6d4b\u5341\u5206\u5173\u952e\u4e14\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\uff1b\u73b0\u6709\u5de5\u4f5c\u591a\u805a\u7126\u5c11\u6570\u7c7b\u578b\uff0c\u4e14\u7f3a\u4e4f\u65f6\u5e8f\u6807\u7b7e\u4e0e\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b89\u5168\u6027\u4e0e\u53ef\u91cd\u590d\u6027\u3002", "method": "\u6784\u5efa CRADLE BENCH\uff0c\u8986\u76d6\u4e03\u79cd\u5371\u673a\u7c7b\u578b\u5e76\u7eb3\u5165\u65f6\u95f4\u6807\u7b7e\uff1b\u63d0\u4f9b600\u6761\u4e34\u5e8a\u6807\u6ce8\u8bc4\u4f30\u6837\u672c\u4e0e420\u6761\u5f00\u53d1\u6837\u672c\uff0c\u4ee5\u53ca\u7ea64K\u6761\u901a\u8fc7\u591a\u6570\u6295\u7968\u96c6\u6210\u81ea\u52a8\u6807\u6ce8\u7684\u8bad\u7ec3\u8bed\u6599\uff1b\u5bf9\u516d\u4e2a\u5371\u673a\u68c0\u6d4b\u6a21\u578b\u5728\u5171\u8bc6\u4e0e\u5168\u6743\u4e00\u81f4\u7684\u5b50\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5f62\u6210\u5728\u4e0d\u540c\u4e00\u81f4\u6027\u51c6\u5219\u4e0b\u7684\u8865\u5145\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8bc4\u4f30\u4e0e\u5f00\u53d1\u96c6\u89c4\u6a21\u660e\u786e\uff0c\u8bad\u7ec3\u8bed\u6599\u901a\u8fc7\u591a\u6a21\u578b\u6295\u7968\u6807\u6ce8\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u578b\uff1b\u516d\u4e2a\u5fae\u8c03\u6a21\u578b\u5728\u4e0d\u540c\u4e00\u81f4\u6027\u6761\u4ef6\u4e0b\u5c55\u73b0\u4e92\u8865\u6027\uff0c\u63d0\u5347\u6574\u4f53\u5371\u673a\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CRADLE BENCH \u63d0\u4f9b\u4e86\u4e00\u4e2a\u8986\u76d6\u5e7f\u3001\u5177\u65f6\u5e8f\u6807\u7b7e\u7684\u5371\u673a\u68c0\u6d4b\u57fa\u51c6\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4e0d\u540c\u4e00\u81f4\u6027\u51c6\u5219\u7684\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\uff0c\u6709\u52a9\u4e8e\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u66f4\u53ef\u9760\u5730\u8bc6\u522b\u548c\u5e94\u5bf9\u5371\u673a\u60c5\u666f\u3002"}}
{"id": "2510.24431", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24431", "abs": "https://arxiv.org/abs/2510.24431", "authors": ["Xiaoyu Kong", "Leheng Sheng", "Junfei Tan", "Yuxin Chen", "Jiancan Wu", "An Zhang", "Xiang Wang", "Xiangnan He"], "title": "MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation", "comment": "Technical Report", "summary": "The recent success of large language models (LLMs) has renewed interest in\nwhether recommender systems can achieve similar scaling benefits. Conventional\nrecommenders, dominated by massive embedding tables, tend to plateau as\nembedding dimensions grow. In contrast, the emerging generative paradigm\nreplaces embeddings with compact Semantic ID (SID) sequences produced by\nautoregressive Transformers. Yet most industrial deployments remain\nproprietary, leaving two fundamental questions open: (1) Do the expected\nscaling laws hold on public benchmarks? (2) What is the minimal post-training\nrecipe that enables competitive performance?\n  We present MiniOneRec, to the best of our knowledge, the first fully\nopen-source generative recommendation framework, which provides an end-to-end\nworkflow spanning SID construction, supervised fine-tuning, and\nrecommendation-oriented reinforcement learning. We generate SIDs via a Residual\nQuantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters\non the Amazon Review dataset. Our experiments reveal a consistent downward\ntrend in both training and evaluation losses with increasing model size,\nvalidating the parameter efficiency of the generative approach. To further\nenhance performance, we propose a lightweight yet effective post-training\npipeline that (1) enforces full-process SID alignment and (2) applies\nreinforcement learning with constrained decoding and hybrid rewards. Together,\nthese techniques yield significant improvements in both ranking accuracy and\ncandidate diversity.", "AI": {"tldr": "MiniOneRec \u662f\u9996\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u8bed\u4e49ID\uff08SID\uff09\u7684\u6a21\u578b\u5728\u53c2\u6570\u89c4\u6a21\u4ece0.5B\u52307B\u65f6\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u5b9e\u7528\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\u5728\u4e9a\u9a6c\u900a\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6392\u540d\u51c6\u786e\u6027\u548c\u5019\u9009\u591a\u6837\u6027\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u56de\u7b54\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\uff0c\u751f\u6210\u5f0f\u63a8\u8350\u662f\u5426\u9075\u5faa\u53ef\u6269\u5c55\u6027\u89c4\u5f8b\uff1f\u4ee5\u53ca\u5b9e\u73b0\u7ade\u4e89\u6027\u80fd\u6240\u9700\u7684\u6700\u5c0f\u5316\u540e\u8bad\u7ec3\u65b9\u6848\u662f\u4ec0\u4e48\uff1f\u5de5\u4e1a\u5316\u90e8\u7f72\u7684\u95ed\u6e90\u7279\u6027\u6210\u4e3a\u7814\u7a76\u7684\u963b\u788d\u3002", "method": "\u63d0\u51fa MiniOneRec\uff1a\u4e00\u4e2a\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec SID \u6784\u9020\u3001\u76d1\u7763\u5fae\u8c03\u548c\u9762\u5411\u63a8\u8350\u7684\u5f3a\u5316\u5b66\u4e60\u3002SID \u901a\u8fc7 Residual Quantized VAE \u751f\u6210\uff0c\u5e76\u5bf9 0.5B\u20137B \u53c2\u6570\u7684 Qwen \u7cfb\u5217\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u5728\u4e9a\u9a6c\u900a\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002\u63d0\u51fa\u4e00\u4e2a\u8f7b\u91cf\u4f46\u6709\u6548\u7684\u540e\u8bad\u7ec3\u6d41\u6c34\u7ebf\uff1a\u786e\u4fdd\u5168\u6d41\u7a0b SID \u5bf9\u9f50\uff0c\u4ee5\u53ca\u5728\u53d7\u9650\u89e3\u7801\u548c\u6df7\u5408\u5956\u52b1\u4e0b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u63d0\u5347\u6392\u5e8f\u51c6\u786e\u6027\u548c\u5019\u9009\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u89c4\u6a21\u6269\u5927\u65f6\u8bad\u7ec3\u548c\u8bc4\u4f30\u635f\u5931\u5448\u4e0b\u964d\u8d8b\u52bf\uff0c\u9a8c\u8bc1\u4e86\u751f\u6210\u5f0f\u65b9\u6cd5\u7684\u53c2\u6570\u9ad8\u6548\u6027\u3002\u540e\u8bad\u7ec3\u6d41\u7a0b\u5728\u5b9e\u73b0 SID \u5168\u6d41\u7a0b\u5bf9\u9f50\u5e76\u7ed3\u5408\u53d7\u9650\u89e3\u7801\u3001\u6df7\u5408\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u540e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6392\u5e8f\u51c6\u786e\u6027\u4e0e\u5019\u9009\u591a\u6837\u6027\u3002", "conclusion": "\u9996\u6b21\u516c\u5f00\u7684\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\u9a8c\u8bc1\u4e86\u53ef\u6269\u5c55\u6027\u5047\u8bbe\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u6210\u7acb\u6027\uff0c\u5e76\u7ed9\u51fa\u4e00\u4e2a\u53ef\u64cd\u4f5c\u7684\u6700\u5c0f\u540e\u8bad\u7ec3\u65b9\u6848\uff1b\u6b64\u5de5\u4f5c\u4e3a\u751f\u6210\u5f0f\u63a8\u8350\u7684\u7814\u7a76\u4e0e\u793e\u533a\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u7ebf\u4e0e\u65b9\u5411\u3002"}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "AI \u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u4f5c\u7528\u5177\u6709\u53cc\u91cd\u6027\uff1a\u80fd\u591f\u62d3\u5c55\u5b66\u79d1\u6027\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9b AI \u65b9\u6cd5\u53ef\u80fd\u9020\u6210\u5bf9\u521b\u9020\u6027\u8def\u5f84\u7684\u66ff\u4ee3\uff0c\u4ece\u800c\u53ef\u80fd\u524a\u5f31\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002\u901a\u8fc7\u4e24\u5219\u6570\u5b66\u6848\u4f8b\u8bf4\u660e\u3002", "motivation": "\u7406\u89e3 AI \u5bf9\u5b66\u79d1\u5185\u521b\u9020\u529b\u7684\u5f71\u54cd\uff0c\u533a\u5206\u521b\u9020\u6027\u65b9\u6cd5\u4e0e\u521b\u9020\u6027\u4ea7\u7269\uff0c\u63d0\u51fa\u201c\u5b66\u79d1\u6027\u521b\u9020\u529b\u201d\u7684\u6982\u5ff5\uff0c\u4ee5\u53ca\u5176\u5728\u6570\u5b66\u4e0e\u79d1\u5b66\u4e2d\u7684\u8106\u5f31\u6027\u3002", "method": "\u57fa\u4e8e\u54f2\u5b66\u5bf9\u521b\u9020\u529b\u7684\u6700\u65b0\u7814\u7a76\uff0c\u8fdb\u884c\u7406\u8bba\u5206\u6790\u5e76\u9009\u53d6\u4e24\u4f8b\u6570\u5b66\u6848\u4f8b\u6765\u5bf9\u6bd4\u8ba1\u7b97/AI \u5bf9\u5b66\u79d1\u6027\u521b\u9020\u529b\u7684\u4fc3\u8fdb\u4e0e\u66ff\u4ee3\u6548\u5e94\u3002", "result": "\u8ba1\u7b97\u53ef\u4ee5\u6269\u5c55\u5b66\u79d1\u6027\u521b\u9020\u529b\uff1b\u4f46\u90e8\u5206 AI \u65b9\u6cd5\u53ef\u80fd\u524a\u5f31\u751a\u81f3\u66ff\u4ee3\u8fd9\u79cd\u521b\u9020\u529b\uff0c\u8fdb\u800c\u53ef\u80fd\u6539\u53d8\u6216\u964d\u4f4e\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "conclusion": "\u9700\u8981\u5bf9 AI \u5728\u5b66\u79d1\u6027\u521b\u9020\u529b\u4e2d\u7684\u89d2\u8272\u8fdb\u884c\u5ba1\u614e\u6574\u5408\uff0c\u907f\u514d\u5bf9\u79d1\u5b66\u4ef7\u503c\u7684\u524a\u5f31\uff0c\u63a2\u7d22\u4fdd\u6301\u521b\u9020\u529b\u4e0e\u5b66\u79d1\u4e13\u95e8\u77e5\u8bc6\u4e4b\u95f4\u7684\u5e73\u8861\u3002"}}
{"id": "2510.23621", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23621", "abs": "https://arxiv.org/abs/2510.23621", "authors": ["Alexandre Benoit"], "title": "Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields", "comment": "78 pages, 21 figures", "summary": "Machine-learning force fields can deliver accurate molecular dynamics (MD) at\nhigh computational cost. For SO(3)-equivariant models such as MACE, there is\nlittle systematic evidence on whether reduced-precision arithmetic and\nGPU-optimized kernels can cut this cost without harming physical fidelity. This\nthesis aims to make MACE cheaper and faster while preserving accuracy by\nidentifying computational bottlenecks and evaluating low-precision execution\npolicies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA\ncuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32\naccumulation) for inference, short NVT and long NPT water simulations, and toy\ntraining runs under reproducible, steady-state timing. cuEquivariance reduces\ninference latency by about $3\\times$. Casting only linear layers to BF16/FP16\nwithin an FP32 model yields roughly 4x additional speedups, while energies and\nthermodynamic observables in NVT/NPT MD remain within run-to-run variability.\nHalf-precision weights during training degrade force RMSE. Mixing e3nn and cuEq\nmodules without explicit adapters causes representation mismatches. Fused\nequivariant kernels and mixed-precision inference can substantially accelerate\nstate-of-the-art force fields with negligible impact on downstream MD. A\npractical policy is to use cuEquivariance with FP32 by default and enable\nBF16/FP16 for linear layers (keeping FP32 accumulations) for maximum\nthroughput, while training remains in FP32. Further gains are expected on\nAmpere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and\npipeline fusion.", "AI": {"tldr": "\u5bf9 MACE \u7b49 SO(3)-\u7b49\u53d8\u6a21\u578b\u5728\u4f4e\u7cbe\u5ea6\u6267\u884c\u4e0b\u7684\u6210\u672c\u4e0e\u51c6\u786e\u6027\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u63d0\u51fa\u4ee5 cuEquivariance + FP32 \u4e3a\u57fa\u7ebf\uff0c\u5e76\u5728\u7ebf\u6027\u5c42\u6df7\u5408 BF16/FP16 \u6765\u83b7\u5f97\u663e\u8457\u52a0\u901f\uff0c\u540c\u65f6\u6ce8\u610f\u8bad\u7ec3\u9636\u6bb5\u7684\u7cbe\u5ea6\u4e0e\u6a21\u5757\u517c\u5bb9\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u4fdd\u8bc1\u7269\u7406\u4fdd\u771f\u5ea6\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u673a\u5668\u5b66\u4e60\u529b\u573a\uff08\u5982 MACE\uff09\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u63a2\u7a76\u4f4e\u7cbe\u5ea6\u7b97\u529b\u4e0e GPU \u5185\u6838\u4f18\u5316\u5bf9\u63a8\u7406\u4e0e\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u7684\u5f71\u54cd\u3002", "method": "\u7aef\u5230\u7aef\u53ca\u9010\u5757\u5256\u6790 MACE \u7684\u74f6\u9888\uff0c\u6bd4\u8f83 e3nn \u4e0e NVIDIA cuEquivariance \u540e\u7aef\uff1b\u5728\u4e0d\u540c\u6570\u503c\u7cbe\u5ea6\u8bbe\u7f6e\uff08FP64/FP32/BF16/FP16\uff0cFP32 \u7d2f\u52a0\uff09\u4e0b\u8bc4\u4f30\u63a8\u7406\u3001\u77ed\u65f6 NVT \u4e0e\u957f\u65f6 NPT \u7684\u6c34\u5f97\u5230\u7684\u80fd\u91cf\u4e0e\u70ed\u529b\u5b66\u89c2\u6d4b\u91cf\uff0c\u4ee5\u53ca toy \u8bad\u7ec3\u573a\u666f\uff0c\u5e76\u5728\u53ef\u91cd\u590d\u6027\u57fa\u51c6\u4e0b\u8fdb\u884c\u65f6\u5e8f\u6027\u80fd\u6bd4\u8f83\u3002", "result": "cuEquivariance \u5c06\u63a8\u7406\u5ef6\u8fdf\u7ea6\u964d\u4f4e3\u500d\uff1b\u4ec5\u5c06\u7ebf\u6027\u5c42\u8f6c\u6362\u4e3a BF16/FP16\uff08\u5728 FP32 \u6a21\u578b\u5185\uff09\u53ef\u518d\u5e26\u6765\u7ea64\u500d\u52a0\u901f\uff0c\u540c\u65f6\u5728 NVT/NPT MD \u4e2d\u80fd\u91cf\u4e0e\u70ed\u529b\u89c2\u6d4b\u4fdd\u6301\u5728\u8fd0\u884c\u6ce2\u52a8\u8303\u56f4\u5185\uff1b\u534a\u7cbe\u5ea6\u6743\u91cd\u5728\u8bad\u7ec3\u4e2d\u964d\u4f4e\u529b\u7684 RMSE\uff1b\u6df7\u5408 e3nn \u4e0e cuEq \u6a21\u5757\u4f1a\u5bfc\u81f4\u8868\u793a\u4e0d\u5339\u914d\uff1b\u878d\u5408\u7684\u6838\u4e0e\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u5728\u5bf9\u4e0b\u6e38 MD \u7684\u5f71\u54cd\u5f88\u5c0f\u3002\u63d0\u51fa\u4ee5 cuEquivariance + FP32 \u4e3a\u9ed8\u8ba4\uff0c\u7ebf\u6027\u5c42\u5f00\u542f BF16/FP16\uff08FP32 \u7d2f\u52a0\uff09\uff0c\u8bad\u7ec3\u4fdd\u6301 FP32\uff0c\u672a\u6765\u5728 Ampere/Hopper \u7b49\u67b6\u6784\u4e0a\u53ef\u83b7\u5f97\u66f4\u591a\u6536\u76ca\u3002", "conclusion": "\u7ed9\u51fa\u4e00\u4e2a\u5b9e\u7528\u7684\u6df7\u5408\u7cbe\u5ea6\u7b56\u7565\uff1a\u9ed8\u8ba4\u4f7f\u7528 cuEquivariance \u914d\u5408 FP32\uff0c\u7ebf\u6027\u5c42\u4f7f\u7528 BF16/FP16\uff08\u4fdd\u6301 FP32 \u7d2f\u52a0\uff09\uff0c\u4ee5\u83b7\u5f97\u6700\u5927\u541e\u5410\u5e76\u5c3d\u91cf\u4fdd\u6301\u7269\u7406\u91cf\u7684\u51c6\u786e\u6027\uff1b\u5728\u8bad\u7ec3\u9636\u6bb5\u7ee7\u7eed\u4f7f\u7528 FP32\uff1b\u5e76\u9884\u671f\u5728\u66f4\u5148\u8fdb\u7684 GPU \u67b6\u6784\u548c\u66f4\u4f4e\u5c42\u6b21\u7684\u5185\u6838\u4f18\u5316\u4e0a\u5b9e\u73b0\u989d\u5916\u63d0\u5347\u3002"}}
{"id": "2510.24469", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24469", "abs": "https://arxiv.org/abs/2510.24469", "authors": ["Durga Prasad Maram", "Dhruvin Gandhi", "Zonghai Yao", "Gayathri Akkinapalli", "Franck Dernoncourt", "Yu Wang", "Ryan A. Rossi", "Nesreen K. Ahmed"], "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization", "comment": null, "summary": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.", "AI": {"tldr": "A training-free, critique-refine framework (PerFine) for personalized text generation using iterative, profile-grounded feedback from a critic LLM to improve alignment with user style, tone, and topics.", "motivation": "Personalization requires aligning generated text with a target user's style, tone, and topical focus, but existing retrieval-augmented methods often drift and require training.", "method": "In inference, a generator drafts conditioned on a retrieved profile; a critic LLM, conditioned on the same profile, provides structured feedback on tone, vocabulary, sentence structure, and topicality. The generator revises the draft; a knockout strategy retains the best draft across iterations. Optional inference-time strategies include Best-of-N and Topic Extraction.", "result": "On Yelp, Goodreads, and Amazon data, PerFine outperforms PGraphRAG with GEval gains of +7-13%; improvements stabilize over 3-5 refinement iterations and scale with larger critics.", "conclusion": "Post-hoc, profile-aware feedback is a powerful, training-free, model-agnostic paradigm for personalized LLM generation across domains."}}
{"id": "2510.23744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23744", "abs": "https://arxiv.org/abs/2510.23744", "authors": ["Eline M. Bovy", "Caleb Probine", "Marnix Suilen", "Ufuk Topcu", "Nils Jansen"], "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability", "comment": "Accepted at NeurIPS 2025", "summary": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete\nmodel uncertainty. ME-POMDPs represent a finite set of POMDPs that share the\nsame state, action, and observation spaces, but may arbitrarily vary in their\ntransition, observation, and reward models. Such models arise, for instance,\nwhen multiple domain experts disagree on how to model a problem. The goal is to\nfind a single policy that is robust against any choice of POMDP within the set,\ni.e., a policy that maximizes the worst-case reward across all POMDPs. We\ngeneralize and expand on existing work in the following way. First, we show\nthat ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which\nwe call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any\narbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its\ntransition and reward functions or only in its observation and reward\nfunctions, while preserving (optimal) policies. We then devise exact and\napproximate (point-based) algorithms to compute robust policies for AB-POMDPs,\nand thus ME-POMDPs. We demonstrate that we can compute policies for standard\nPOMDP benchmarks extended to the multi-environment setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u591a\u73af\u5883POMDP\uff08ME-POMDP\uff09\u53ca\u5bf9\u6297\u4fe1\u5ff5\u6269\u5c55\uff08AB-POMDP\uff09\uff0c\u7ed9\u51fa\u5c06ME-POMDP\u7b80\u5316\u4e3a\u4ec5\u5728\u8f6c\u79fb/\u5956\u52b1\u6216\u89c2\u6d4b/\u5956\u52b1\u65b9\u9762\u53d8\u5316\u7684\u7b49\u4ef7\u5f62\u5f0f\uff0c\u4ee5\u53ca\u7cbe\u786e\u4e0e\u70b9\u57fa\u8fd1\u4f3c\u7b97\u6cd5\u4ee5\u6c42\u89e3\u9c81\u68d2\u7b56\u7565\uff0c\u5e76\u5728\u6807\u51c6POMDP\u57fa\u51c6\u4e0a\u6269\u5c55\u9a8c\u8bc1\u3002", "motivation": "\u5728\u4e0d\u540c\u9886\u57df\u4e13\u5bb6\u5bf9\u95ee\u9898\u5efa\u6a21\u5b58\u5728\u5206\u6b67\u65f6\uff0c\u9700\u8981\u4e00\u4e2a\u5bf9\u6240\u6709\u53ef\u80fd\u73af\u5883\u90fd\u9c81\u68d2\u7684\u7b56\u7565\uff0c\u80fd\u5728\u591a\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u5b9e\u73b0\u7a33\u5065\u51b3\u7b56\u3002", "method": "\u5c06ME-POMDP\u63a8\u5e7f\u4e3a\u5177\u6709\u5bf9\u6297\u6027\u521d\u59cb\u4fe1\u5ff5\u96c6\u5408\u7684AB-POMDP\uff1b\u8bc1\u660e\u4efb\u610fME-POMDP\u53ef\u7ea6\u5316\u4e3a\u4ec5\u53d8\u66f4\u8f6c\u79fb\u4e0e\u5956\u52b1\u6216\u4ec5\u53d8\u66f4\u89c2\u6d4b\u4e0e\u5956\u52b1\u7684\u7b49\u4ef7\u5f62\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f18\u7b56\u7565\uff1b\u63d0\u51fa\u7cbe\u786e\u4e0e\u70b9\u57fa\u7684\u8fd1\u4f3c\u7b97\u6cd5\u4ee5\u6c42\u89e3\u9c81\u68d2\u7b56\u7565\u3002", "result": "\u7ed9\u51fa\u53ef\u5c06ME-POMDP/AB-POMDP\u6c42\u89e3\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5728\u6807\u51c6POMDP\u57fa\u51c6\u4e0a\u6269\u5c55\u5230\u591a\u73af\u5883\u8bbe\u7f6e\uff0c\u6f14\u793a\u53ef\u83b7\u5f97\u9c81\u68d2\u7b56\u7565\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4eceME-POMDP\u5230AB-POMDP\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u7ed9\u51fa\u53ef\u884c\u7684\u6c42\u89e3\u7b97\u6cd5\uff08\u7cbe\u786e\u4e0e\u8fd1\u4f3c\uff09\uff0c\u9a8c\u8bc1\u4e86\u5728\u591a\u73af\u5883\u60c5\u5f62\u4e0b\u6c42\u89e3\u9c81\u68d2\u7b56\u7565\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.23622", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23622", "abs": "https://arxiv.org/abs/2510.23622", "authors": ["Alyssa Gerhart", "Balaji Iyangar"], "title": "Adversarially-Aware Architecture Design for Robust Medical AI Systems", "comment": null, "summary": "Adversarial attacks pose a severe risk to AI systems used in healthcare,\ncapable of misleading models into dangerous misclassifications that can delay\ntreatments or cause misdiagnoses. These attacks, often imperceptible to human\nperception, threaten patient safety, particularly in underserved populations.\nOur study explores these vulnerabilities through empirical experimentation on a\ndermatological dataset, where adversarial methods significantly reduce\nclassification accuracy. Through detailed threat modeling, experimental\nbenchmarking, and model evaluation, we demonstrate both the severity of the\nthreat and the partial success of defenses like adversarial training and\ndistillation. Our results show that while defenses reduce attack success rates,\nthey must be balanced against model performance on clean data. We conclude with\na call for integrated technical, ethical, and policy-based approaches to build\nmore resilient, equitable AI in healthcare.", "AI": {"tldr": "Adversarial attacks threaten healthcare AI; defenses help but reduce clean accuracy; calls for integrated technical, ethical, and policy solutions.", "motivation": "To protect patient safety and equity by understanding adversarial vulnerabilities in healthcare AI, especially for dermatology, and to motivate robust defenses.", "method": "Empirical experimentation on a dermatological dataset, including threat modeling, benchmarking, and model evaluation; evaluation of defenses such as adversarial training and distillation.", "result": "Attacks substantially reduce classification accuracy; defenses lower attack success rates but incur a cost to clean-data performance; defenses are only partially effective.", "conclusion": "Need integrated approaches combining technical defenses with ethical guidelines and policy frameworks to build resilient and equitable AI in healthcare."}}
{"id": "2510.23854", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23854", "abs": "https://arxiv.org/abs/2510.23854", "authors": ["Jyotika Singh", "Weiyi Sun", "Amit Agarwal", "Viji Krishnamurthy", "Yassine Benajiba", "Sujith Ravi", "Dan Roth"], "title": "Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs", "comment": "Accepted at EMNLP 2025", "summary": "In modern industry systems like multi-turn chat agents, Text-to-SQL\ntechnology bridges natural language (NL) questions and database (DB) querying.\nThe conversion of tabular DB results into NL representations (NLRs) enables the\nchat-based interaction. Currently, NLR generation is typically handled by large\nlanguage models (LLMs), but information loss or errors in presenting tabular\nresults in NL remains largely unexplored. This paper introduces a novel\nevaluation method - Combo-Eval - for judgment of LLM-generated NLRs that\ncombines the benefits of multiple existing methods, optimizing evaluation\nfidelity and achieving a significant reduction in LLM calls by 25-61%.\nAccompanying our method is NLR-BIRD, the first dedicated dataset for NLR\nbenchmarking. Through human evaluations, we demonstrate the superior alignment\nof Combo-Eval with human judgments, applicable across scenarios with and\nwithout ground truth references.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5c06\u591a\u79cd\u8bc4\u4f30\u65b9\u6cd5\u7ed3\u5408\u4ee5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u8868\u683c\u7ed3\u679c\u81ea\u7136\u8bed\u8a00\u8868\u793a\uff08NLR\uff09\u7684\u65b0\u6846\u67b6Combo-Eval\uff0c\u5e76\u8f85\u4ee5\u9996\u4e2aNLR\u57fa\u51c6\u6570\u636e\u96c6NLR-BIRD\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8bc4\u4f30\u4fdd\u771f\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4eLLM\u8c03\u7528\u6b21\u6570\uff0825-61%\uff09\uff0c\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u4e0e\u4eba\u5de5\u5224\u65ad\u9ad8\u5ea6\u5bf9\u9f50\uff0c\u9002\u7528\u4e8e\u6709\u65e0\u53c2\u8003\u7b54\u6848\u7684\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u5c06\u8868\u683c\u6570\u636e\u5e93\u67e5\u8be2\u7ed3\u679c\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u8868\u793a\u7684\u4efb\u52a1\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u73b0\u6709NLR\u8bc4\u4f30\u65b9\u6cd5\u5728\u4fdd\u771f\u6027\u4e0e\u4fe1\u606f\u635f\u5931\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002\u5927\u91cf\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884cNLR\u751f\u6210\u548c\u8bc4\u4f30\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u4e14\u6613\u4ea7\u751f\u9519\u8bef\u3002\u9700\u8981\u4e00\u4e2a\u66f4\u9ad8\u4fdd\u771f\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u5728\u65e0\u53c2\u8003\u7b54\u6848\u573a\u666f\u4e0b\u4f7f\u7528\u7684\u8bc4\u4f30\u65b9\u6848\uff0c\u540c\u65f6\u63d0\u4f9b\u4e00\u4e2a\u6807\u51c6\u5316\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "Combo-Eval\u901a\u8fc7\u878d\u5408\u591a\u79cd\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u51fa\u4e00\u4e2a\u7efc\u5408\u5224\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u751f\u6210\u7684NLR\u7684\u8d28\u91cf\u4e0e\u4e00\u81f4\u6027\u3002\u5e76\u53d1\u5e03NLR-BIRD\uff0c\u4f5c\u4e3a\u9996\u4e2a\u9762\u5411NLR\u8bc4\u6d4b\u7684\u4e13\u7528\u6570\u636e\u96c6\uff0c\u5305\u542b\u4eba\u7c7b\u6807\u6ce8\u4ee5\u7528\u4e8e\u5bf9\u6bd4\u548c\u57fa\u7ebf\u5efa\u7acb\u3002", "result": "\u4e0e\u5355\u4e00\u8bc4\u4f30\u65b9\u6cd5\u76f8\u6bd4\uff0cCombo-Eval\u5728\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u5728\u8bc4\u4f30\u65f6\u663e\u8457\u964d\u4f4e\u4e86LLM\u8c03\u7528\u6b21\u6570\uff0825-61%\uff09\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5b58\u5728\u53c2\u8003\u7b54\u6848\u4e0e\u65e0\u53c2\u8003\u7b54\u6848\u7684\u573a\u666f\u3002", "conclusion": "Combo-Eval\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u4fdd\u771f\u4e14\u66f4\u9ad8\u6548\u7684NLR\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u914d\u5957NLR-BIRD\u6570\u636e\u96c6\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u7528\u6027\u4e0e\u63a8\u5e7f\u6f5c\u529b\uff0c\u80fd\u591f\u63a8\u52a8\u6587\u672c\u5230SQL\u7b49\u9700\u8981\u5c06\u8868\u683c\u7ed3\u679c\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u8868\u793a\u4efb\u52a1\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2510.24652", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24652", "abs": "https://arxiv.org/abs/2510.24652", "authors": ["Jiawei Zhou", "Lei Chen"], "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning", "comment": null, "summary": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.", "AI": {"tldr": "R3\u662f\u4e00\u4e2a\u9762\u5411RAG\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u8bd5\u9a8c\u4e0e\u53cd\u9988\u7684\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u68c0\u7d22\u5668\uff0c\u5728\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5728RAG\u73af\u5883\u4e2d\u52a8\u6001\u63a2\u7d22\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e14\u9ad8\u6548\u8bad\u7ec3\u3002", "motivation": "\u968f\u7740RAG\u5e94\u7528\u7684\u666e\u53ca\uff0c\u4fe1\u606f\u68c0\u7d22\u4ece\u4e3a\u4eba\u7c7b\u68c0\u7d22\u8f6c\u5411\u4e3aAI\u7cfb\u7edf\u68c0\u7d22\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u96be\u4ee5\u4e8b\u524d\u5b9a\u4e49/\u6807\u6ce8\u76f8\u5173\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728RAG\u73af\u5883\u4e2d\u81ea\u6211\u6539\u8fdb\u7684\u68c0\u7d22\u5668\u3002", "method": "\u5728\u8bad\u7ec3\u9636\u6bb5\uff0c\u68c0\u7d22\u7ed3\u679c\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u751f\u6210\u5bf9\u6bd4\u4fe1\u53f7\u4ee5\u5f15\u5bfc\u68c0\u7d22\u5668\u7684\u81ea\u6211\u63d0\u5347\uff1b\u91c7\u7528\u8bd5\u9a8c-\u53cd\u9988\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\uff0c\u907f\u514d\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff1b\u5728RAG\u6846\u67b6\u4e2d\u52a8\u6001\u63a2\u7d22\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u5728\u591a\u4efb\u52a1\u4e0a\u663e\u793aR3\u5c06\u68c0\u7d22\u5668\u6027\u80fd\u63d0\u53475.2%\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u56684.9%\uff0c\u5e76\u8fbe\u5230\u4e0eLLM\u589e\u5f3a\u68c0\u7d22\u53ca\u57fa\u4e8e\u540e\u8bad\u7ec3/\u6307\u4ee4\u5fae\u8c03\u7684LLM\u7cfb\u7edf\u76f8\u5f53\u7684\u6548\u679c\uff1b\u8bad\u7ec3\u6240\u9700\u8d44\u6e90\u4ec54\u4e2aGPU\uff0c\u5355\u65e5\u5185\u5b8c\u6210\u3002", "conclusion": "R3\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684RAG\u68c0\u7d22\u4f18\u5316\u65b9\u6848\uff0c\u53ef\u5728\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u68c0\u7d22\u8d28\u91cf\uff0c\u5e76\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u7684\u68c0\u7d22\u65b9\u6cd5\u76f8\u7ade\u4e89\u3002"}}
{"id": "2510.23746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23746", "abs": "https://arxiv.org/abs/2510.23746", "authors": ["Laura Mismetti", "Marvin Alberts", "Andreas Krause", "Mara Graziani"], "title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "comment": null, "summary": "Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.", "AI": {"tldr": "\u57fa\u4e8e\u6d4b\u8bd5\u65f6\u5fae\u8c03\u7684\u53d8\u6362\u6a21\u578b\u5b9e\u73b0\u4ece\u4e32\u8054\u8d28\u8c31\u76f4\u63a5\u7aef\u5230\u7aef\u7684\u53bb\u65b0\u5206\u5b50\u7ed3\u6784\u751f\u6210\uff0c\u5728NPLIB1\u548cMassSpecGym\u4e0a\u663e\u8457\u4f18\u4e8eDiffMS", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5e93\u5339\u914d\u6216\u591a\u6b65\u9aa4\u6d41\u7a0b\uff0c\u5bf9\u672a\u5728\u53c2\u8003\u6570\u636e\u5e93\u4e2d\u51fa\u73b0\u7684\u5316\u5408\u7269\u8bc6\u522b\u56f0\u96be\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u7aef\u5230\u7aef\u3001\u53ef\u751f\u6210\u65b0\u5206\u5b50\u7ed3\u6784\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5728\u9884\u8bad\u7ec3\u53d8\u6362\u6a21\u578b\u4e0a\u5f15\u5165\u6d4b\u8bd5\u65f6\u5fae\u8c03\uff0c\u4f7f\u5176\u80fd\u591f\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u6620\u5c04\u5230\u5206\u5b50\u7ed3\u6784\uff0c\u8df3\u8fc7\u4e2d\u95f4\u7684\u7247\u6bb5\u9884\u6d4b\u6216\u6307\u7eb9\u9884\u6d4b\u7b49\u6b65\u9aa4\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u53bb\u65b0\u5206\u5b50\u751f\u6210", "result": "\u5728NPLIB1\u548cMassSpecGym\u57fa\u51c6\u4e0a\u5206\u522b\u8d85\u8d8aDiffMS 100%\u548c20%\uff1bMassSpecGym\u4e0a\u4e0e\u4f20\u7edf\u5fae\u8c03\u76f8\u6bd4\u63d0\u5347\u7ea662%\uff1b\u5728\u9884\u6d4b\u504f\u79bb\u771f\u5b9e\u503c\u65f6\uff0c\u751f\u6210\u7684\u5019\u9009\u5206\u5b50\u4ecd\u5177\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u4fbf\u4e8e\u4eba\u5de5\u89e3\u8bfb", "conclusion": "\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u53ef\u4ee5\u8ba9\u6a21\u578b\u5feb\u901f\u9002\u5e94\u65b0\u8c31\u56fe\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u7a33\u5b9a\u7684\u53bb\u65b0\u5206\u5b50\u8bc6\u522b\u7ed3\u679c\uff0c\u964d\u4f4e\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u5347\u8bc6\u522b\u53ef\u9760\u6027"}}
{"id": "2510.23624", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23624", "abs": "https://arxiv.org/abs/2510.23624", "authors": ["Tiago Mendon\u00e7a dos Santos", "Rafael Izbicki", "Lu\u00eds Gustavo Esteves"], "title": "DiNo and RanBu: Lightweight Predictions from Shallow Random Forests", "comment": null, "summary": "Random Forest ensembles are a strong baseline for tabular prediction tasks,\nbut their reliance on hundreds of deep trees often results in high inference\nlatency and memory demands, limiting deployment in latency-sensitive or\nresource-constrained environments. We introduce DiNo (Distance with Nodes) and\nRanBu (Random Bushes), two shallow-forest methods that convert a small set of\ndepth-limited trees into efficient, distance-weighted predictors. DiNo measures\ncophenetic distances via the most recent common ancestor of observation pairs,\nwhile RanBu applies kernel smoothing to Breiman's classical proximity measure.\nBoth approaches operate entirely after forest training: no additional trees are\ngrown, and tuning of the single bandwidth parameter $h$ requires only\nlightweight matrix-vector operations. Across three synthetic benchmarks and 25\npublic datasets, RanBu matches or exceeds the accuracy of full-depth random\nforests-particularly in high-noise settings-while reducing training plus\ninference time by up to 95\\%. DiNo achieves the best bias-variance trade-off in\nlow-noise regimes at a modest computational cost. Both methods extend directly\nto quantile regression, maintaining accuracy with substantial speed gains. The\nimplementation is available as an open-source R/C++ package at\nhttps://github.com/tiagomendonca/dirf. We focus on structured tabular random\nsamples (i.i.d.), leaving extensions to other modalities for future work.", "AI": {"tldr": "Two shallow-forest methods, DiNo and RanBu, convert a few depth-limited trees into efficient distance-weighted predictors, achieving competitive accuracy with large speed-ups; RanBu uses kernel-smoothed proximity, DiNo uses cophenetic distances; both post-training, no new trees; speedups up to 95%, applicable to quantile regression, open-source package.", "motivation": "Reduce inference latency and memory footprint of random forests on tabular data while preserving accuracy, by post-training distance-weighted prediction using existing shallow trees.", "method": "Compute pairwise distances in the forest after training: DiNo uses cophenetic distance via most recent common ancestor; RanBu uses kernel smoothing on proximity; single bandwidth h; both require only light matrix-vector operations; no extra trees.", "result": "RanBu matches/exceeds full-depth RF accuracy, especially in high-noise settings; reduces training+inference time up to 95%; DiNo provides best bias-variance trade-off in low-noise regime; both extend to quantile regression; tested on 3 synthetic benchmarks and 25 public datasets; open-source package.", "conclusion": "Distance-weighted shallow-forest predictors can deliver competitive performance with substantial speed gains on tabular data; DiNo and RanBu offer practical, low-cost alternatives; extendable to quantile regression; future work to other modalities."}}
{"id": "2510.23870", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23870", "abs": "https://arxiv.org/abs/2510.23870", "authors": ["Marianne Menglin Liu", "Sai Ashish Somayajula", "Syed Fahad Allam Shah", "Sujith Ravi", "Dan Roth"], "title": "OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning", "comment": null, "summary": "We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge\n2025, a bilingual benchmark requiring complex reasoning such as arithmetic,\ncommonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding\nthe second-best system by more than 6% in execution accuracy (EX), with 55.0%\nin English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).\nOur system follows an agentic framework with two components: Planner agent that\ngenerates stepwise natural language plans, and SQL agent that converts these\nplans into executable SQL. Since SQL agent reliably adheres to the plan, our\nrefinements focus on the planner. Unlike prior methods that rely on multiple\nsub-agents for planning and suffer from orchestration overhead, we introduce a\nfeedback-guided meta-prompting strategy to refine a single planner. Failure\ncases from a held-out set are clustered with human input, and an LLM distills\nthem into corrective guidelines that are integrated into the planner's system\nprompt, improving generalization without added complexity. For the multilingual\nscenario, to address transliteration and entity mismatch issues, we incorporate\nentity-linking guidelines that generate alternative surface forms for entities\nand explicitly include them in the plan. Finally, we enhance reliability\nthrough plan diversification: multiple candidate plans are generated for each\nquery, with the SQL agent producing a query for each plan, and final output\nselected via majority voting over their executions.", "AI": {"tldr": "OraPlan-SQL is a bilingual NL2SQL system for the Archer NL2SQL Evaluation Challenge 2025. It ranks first, improves planner via feedback-guided meta-prompting, uses plan diversification with majority voting, and adds multilingual entity-linking guidelines.", "motivation": "The task demands complex reasoning (arithmetic, commonsense, hypothetical inference) in a bilingual NL2SQL setting, with high demand for reliability and generalization across English and Chinese.", "method": "An agentic framework with a Planner agent generating stepwise natural language plans and a SQL agent converting plans to executable SQL. A single planner is refined via feedback-guided meta-prompting, clustering failure cases and distilling corrective guidelines. Multilingual improvements include entity-linking guidelines for surface form variants. Plan diversification produces multiple candidate plans, with the SQL agent executing each and voting by majority for the final output.", "result": "OraPlan-SQL ranked first in the competition, surpassing the second-best system by more than 6% in execution accuracy (EX). Achieved 55.0% EX in English and 56.7% EX in Chinese, with SQL validity >99%.", "conclusion": "A single, feedback-guided planner with plan diversification and multilingual entity-linking can achieve competitive bilingual NL2SQL performance without the overhead of multi-agent orchestration, improving generalization and reliability."}}
{"id": "2510.24701", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24701", "abs": "https://arxiv.org/abs/2510.24701", "authors": ["Tongyi DeepResearch Team", "Baixuan Li", "Bo Zhang", "Dingchu Zhang", "Fei Huang", "Guangyu Li", "Guoxin Chen", "Huifeng Yin", "Jialong Wu", "Jingren Zhou", "Kuan Li", "Liangcai Su", "Litu Ou", "Liwen Zhang", "Pengjun Xie", "Rui Ye", "Wenbiao Yin", "Xinmiao Yu", "Xinyu Wang", "Xixi Wu", "Xuanzhong Chen", "Yida Zhao", "Zhen Zhang", "Zhengwei Tao", "Zhongwang Zhang", "Zile Qiao", "Chenxi Wang", "Donglei Yu", "Gang Fu", "Haiyang Shen", "Jiayin Yang", "Jun Lin", "Junkai Zhang", "Kui Zeng", "Li Yang", "Hailong Yin", "Maojia Song", "Ming Yan", "Peng Xia", "Qian Xiao", "Rui Min", "Ruixue Ding", "Runnan Fang", "Shaowei Chen", "Shen Huang", "Shihang Wang", "Shihao Cai", "Weizhou Shen", "Xiaobin Wang", "Xin Guan", "Xinyu Geng", "Yingcheng Shi", "Yuning Wu", "Zhuo Chen", "Zijian Li", "Yong Jiang"], "title": "Tongyi DeepResearch Technical Report", "comment": "https://tongyi-agent.github.io/blog", "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.", "AI": {"tldr": "Tongyi DeepResearch is a 30.5B-parameter agentic LLM designed for long-horizon, deep information-seeking research tasks, trained with an end-to-end framework that combines agentic mid-training and agentic post-training. It uses a fully automatic data synthesis pipeline and customized environments for each training stage, achieving state-of-the-art on several agentic deep research benchmarks and open-sourcing the model and framework.", "motivation": "To enable autonomous, long-horizon deep research capabilities with scalable reasoning and information seeking, addressing the need for cost-efficient data generation and stable, end-to-end agentic behavior.", "method": "End-to-end training framework combining agentic mid-training and agentic post-training; a fully automatic data synthesis pipeline; customized environments for each training stage to ensure stable interactions; 30.5B total parameters with 3.3B activated per token; open-source release.", "result": "State-of-the-art performance across a range of agentic deep research benchmarks including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510.", "conclusion": "Open-source the model, framework, and complete solutions to empower the community."}}
{"id": "2510.23772", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23772", "abs": "https://arxiv.org/abs/2510.23772", "authors": ["Vivek Veeriah", "Federico Barbero", "Marcus Chiam", "Xidong Feng", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Johan Obando-Ceron", "Jiaxin Shi", "Shaobo Hou", "Satinder Singh", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions", "comment": "Accepted at the Creative AI Track, NeurIPS 2025", "summary": "The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.", "AI": {"tldr": "AI\u7cfb\u7edf\u751f\u6210\u68cb\u9898\uff0c\u5f3a\u8c03\u7f8e\u611f\u3001\u65b0\u9896\u6027\u3001\u53cd\u76f4\u89c9\u4e0e\u72ec\u7279\u89e3\u6cd5\uff1b\u901a\u8fc7\u4e09\u4f4d\u56fd\u9645\u5927\u5e08\u8bc4\u5ba1\u6765\u8bc4\u4f30\u521b\u9020\u6027\u3002", "motivation": "\u63a2\u7a76\u751f\u6210\u5f0fAI\u5728\u68cb\u9898\u521b\u9020\u4e2d\u7684\u6f5c\u529b\uff0c\u662f\u5426\u80fd\u4ea7\u51fa\u5177\u6709\u7f8e\u5b66\u4e0e\u521b\u65b0\u6027\u7684\u9898\u9762\u4e0e\u89e3\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aAI\u7cfb\u7edf\u7528\u4e8e\u751f\u6210\u68cb\u9898\uff0c\u5173\u6ce8\u7f8e\u611f\u3001\u521b\u65b0\u6027\u4e0e\u53cd\u76f4\u89c9\u89e3\u6cd5\uff1b\u7b80\u8981\u63cf\u8ff0\u65b9\u6cd5\u5e76\u6307\u5411\u6280\u672f\u8bba\u6587\u4ee5\u83b7\u53d6\u8be6\u7ec6\u5b9e\u73b0\u3002\u8bc4\u4f30\u901a\u8fc7\u5411\u4e09\u4f4d\u68cb\u7c7b\u5927\u5e08\uff08Amatzia Avni\u3001Jonathan Levitt\u3001Matthew Sadler\uff09\u5c55\u793aAI\u751f\u6210\u7684\u68cb\u9898\u96c6\uff0c\u8acb\u4ed6\u4eec\u6311\u9009\u559c\u7231\u4e4b\u4f5c\u5e76\u89e3\u91ca\u5176\u5438\u5f15\u529b\u3002", "result": "\u4e13\u5bb6\u4ece\u4e2d\u6311\u9009\u51fa\u559c\u7231\u7684\u68cb\u9898\u5e76\u7ed9\u51fa\u7406\u7531\uff0c\u8bc4\u4ef7\u4ee5\u5b9a\u6027\u5206\u6790\u4e3a\u4e3b\uff0c\u5f3a\u8c03\u521b\u9020\u6027\u3001\u6311\u6218\u6027\u548c\u7f8e\u5b66\u8bbe\u8ba1\uff0c\u4f46\u672a\u7ed9\u51fa\u91cf\u5316\u6307\u6807\u3002", "conclusion": "\u5de5\u4f5c\u5c55\u793a\u4e86AI\u5728\u68cb\u9898\u521b\u4f5c\u9886\u57df\u7684\u6f5c\u529b\u4e0e\u53ef\u8bc4\u4f30\u6027\uff0c\u5e76\u6307\u51fa\u9700\u8981\u66f4\u591a\u7ec6\u8282\u4e0e\u5927\u89c4\u6a21\u8bc4\u4f30\u4ee5\u63d0\u5347\u53ef\u4fe1\u5ea6\u4e0e\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2510.23626", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23626", "abs": "https://arxiv.org/abs/2510.23626", "authors": ["Shuang Geng", "Wenli Zhang", "Jiaheng Xie", "Rui Wang", "Sudha Ram"], "title": "From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media", "comment": "Presented at SWAIB2025 and HICSS2026", "summary": "Social media user-generated content (UGC) provides real-time, self-reported\nindicators of mental health conditions such as depression, offering a valuable\nsource for predictive analytics. While prior studies integrate medical\nknowledge to improve prediction accuracy, they overlook the opportunity to\nsimultaneously expand such knowledge through predictive processes. We develop a\nClosed-Loop Large Language Model (LLM)-Knowledge Graph framework that\nintegrates prediction and knowledge expansion in an iterative learning cycle.\nIn the knowledge-aware depression detection phase, the LLM jointly performs\ndepression detection and entity extraction, while the knowledge graph\nrepresents and weights these entities to refine prediction performance. In the\nknowledge refinement and expansion phase, new entities, relationships, and\nentity types extracted by the LLM are incorporated into the knowledge graph\nunder expert supervision, enabling continual knowledge evolution. Using\nlarge-scale UGC, the framework enhances both predictive accuracy and medical\nunderstanding. Expert evaluations confirmed the discovery of clinically\nmeaningful symptoms, comorbidities, and social triggers complementary to\nexisting literature. We conceptualize and operationalize\nprediction-through-learning and learning-through-prediction as mutually\nreinforcing processes, advancing both methodological and theoretical\nunderstanding in predictive analytics. The framework demonstrates the\nco-evolution of computational models and domain knowledge, offering a\nfoundation for adaptive, data-driven knowledge systems applicable to other\ndynamic risk monitoring contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u95ed\u73af\u7684\u5927\u8bed\u8a00\u6a21\u578b-\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u6291\u90c1\u68c0\u6d4b\u4e0e\u5b9e\u4f53\u63d0\u53d6\u5b9e\u73b0\u77e5\u8bc6\u56fe\u8c31\u7684\u81ea\u6211\u6269\u5c55\u4e0e\u6f14\u5316\uff0c\u4ece\u800c\u5728\u5927\u89c4\u6a21UGC\u4e0a\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u533b\u5b66\u7406\u89e3\u3002", "motivation": "\u5f25\u8865\u73b0\u6709\u5de5\u4f5c\u4ec5\u5c06\u533b\u5b66\u77e5\u8bc6\u7528\u4e8e\u9884\u6d4b\u800c\u672a\u5229\u7528\u9884\u6d4b\u8fc7\u7a0b\u6269\u5c55\u77e5\u8bc6\u7684\u4e0d\u8db3\uff0c\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u9884\u6d4b\u548c\u77e5\u8bc6\u6269\u5c55\u4e92\u4e3a\u4fc3\u8fdb\u7684\u81ea\u9002\u5e94\u77e5\u8bc6\u7cfb\u7edf\u3002", "method": "\u5728\u6291\u90c1\u68c0\u6d4b\u9636\u6bb5\uff0cLLM\u540c\u65f6\u8fdb\u884c\u6291\u90c1\u68c0\u6d4b\u4e0e\u5b9e\u4f53\u62bd\u53d6\uff0c\u77e5\u8bc6\u56fe\u8c31\u5bf9\u62bd\u53d6\u7684\u5b9e\u4f53\u8fdb\u884c\u8868\u793a\u4e0e\u52a0\u6743\u4ee5\u63d0\u5347\u9884\u6d4b\u3002\u77e5\u8bc6 refinement \u9636\u6bb5\u5728\u4e13\u5bb6\u76d1\u7763\u4e0b\u5c06\u65b0\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u5b9e\u4f53\u7c7b\u578b\u52a0\u5165\u77e5\u8bc6\u56fe\u8c31\uff0c\u5b9e\u73b0\u6301\u7eed\u77e5\u8bc6\u8fdb\u5316\u3002", "result": "\u5728\u5927\u89c4\u6a21\u7528\u6237\u751f\u6210\u5185\u5bb9\u4e0a\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u533b\u5b66\u7406\u89e3\uff0c\u4e13\u5bb6\u8bc4\u4f30\u53d1\u73b0\u4e86\u4e34\u5e8a\u4e0a\u6709\u610f\u4e49\u7684\u75c7\u72b6\u3001\u5171\u75c5\u4e0e\u793e\u4f1a\u89e6\u53d1\u56e0\u7d20\uff0c\u9a8c\u8bc1\u4e86\u9884\u6d4b-\u5b66\u4e60\u5faa\u73af\u7684\u4e92\u8865\u6027\u3002", "conclusion": "\u63d0\u51fa\u9884\u6d4b-through-learning \u4e0e learning-through-prediction \u7684\u4e92\u8865\u95ed\u73af\uff0c\u63a8\u52a8\u81ea\u9002\u5e94\u3001\u6570\u636e\u9a71\u52a8\u7684\u77e5\u8bc6\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u5e76\u5177\u5907\u63a8\u5e7f\u5230\u5176\u4ed6\u52a8\u6001\u98ce\u9669\u76d1\u6d4b\u573a\u666f\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.23884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23884", "abs": "https://arxiv.org/abs/2510.23884", "authors": ["Tananun Songdechakraiwut", "Michael Lutz"], "title": "Language Models for Longitudinal Clinical Prediction", "comment": null, "summary": "We explore a lightweight framework that adapts frozen large language models\nto analyze longitudinal clinical data. The approach integrates patient history\nand context within the language model space to generate accurate forecasts\nwithout model fine-tuning. Applied to neuropsychological assessments, it\nachieves accurate and reliable performance even with minimal training data,\nshowing promise for early-stage Alzheimer's monitoring.", "AI": {"tldr": "\u4e00\u4e2a\u8f7b\u91cf\u5316\u6846\u67b6\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u957f\u671f\u4e34\u5e8a\u6570\u636e\u8fdb\u884c\u5206\u6790\uff0c\u4e0d\u9700\u5fae\u8c03\u5373\u53ef\u751f\u6210\u9884\u6d4b\u3002\u5bf9\u795e\u7ecf\u5fc3\u7406\u8bc4\u4f30\u6548\u679c\u826f\u597d\uff0c\u6570\u636e\u6781\u5c11\u65f6\u4e5f\u5177\u5907\u7a33\u5b9a\u6027\uff0c\u9002\u5408\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u76d1\u6d4b\u3002", "motivation": "\u5728\u4e34\u5e8a\u6570\u636e\u4e2d\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u7684\u9884\u6d4b\u4e0e\u76d1\u6d4b\uff0c\u907f\u514d\u5927\u89c4\u6a21\u5fae\u8c03\u5e26\u6765\u7684\u8ba1\u7b97\u548c\u9690\u79c1\u6210\u672c\uff0c\u540c\u65f6\u517c\u987e\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5c06\u60a3\u8005\u5386\u53f2\u4e0e\u4e34\u5e8a\u4e0a\u4e0b\u6587\u5d4c\u5165\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u5165\u7a7a\u95f4\uff0c\u4f5c\u4e3a\u63d0\u793a\u4fe1\u606f\uff0c\u5229\u7528\u51bb\u7ed3\u7684LLM\u8fdb\u884c\u63a8\u65ad\uff0c\u907f\u514d\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff1b\u5728\u795e\u7ecf\u5fc3\u7406\u8bc4\u4f30\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u51c6\u786e\u3001\u53ef\u9760\u7684\u9884\u6d4b\uff0c\u663e\u793a\u51fa\u5728\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u76d1\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u6570\u636e\u6709\u9650\u573a\u666f\u63d0\u4f9b\u4e00\u6761\u6570\u636e\u9ad8\u6548\u7684\u9884\u6d4b\u8def\u5f84\uff0c\u5177\u6709\u5e94\u7528\u524d\u666f\u548c\u8fdb\u4e00\u6b65\u4f18\u5316\u7a7a\u95f4\u3002"}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6279\u8bc4\u6027\u5206\u6790\u4e86\u5728\u8ba1\u7b97\u75c5\u7406\u9886\u57df\u5c06\u901a\u7528\u57fa\u7840\u6a21\u578b\u5e94\u7528\u7684\u5c40\u9650\u6027\uff0c\u6307\u51fa\u8bca\u65ad\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u3001\u51e0\u4f55\u7a33\u5b9a\u6027\u3001\u8ba1\u7b97\u6210\u672c\u548c\u5b89\u5168\u7b49\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u8db3\uff0c\u6e90\u4e8e\u751f\u7269\u5b66\u590d\u6742\u6027\u3001\u65e0\u6548\u81ea\u76d1\u7763\u3001\u8fc7\u5ea6\u6cdb\u5316\u3001\u67b6\u6784\u8fc7\u4e8e\u590d\u6742\u3001\u7f3a\u4e4f\u9886\u57df\u521b\u65b0\u3001\u6570\u636e\u4e0d\u8db3\u4ee5\u53ca\u7ec4\u7ec7\u5207\u7247\u5927\u5c0f\u7684\u6838\u5fc3\u8bbe\u8ba1\u7f3a\u9677\u7b49\u4e03\u5927\u539f\u56e0\uff0c\u547c\u5401\u4ece\u6839\u672c\u91cd\u65b0\u601d\u8003\u8be5\u8303\u5f0f\u3002", "motivation": "\u89e3\u91ca\u4e3a\u4f55\u76f4\u63a5\u8fc1\u79fb\u901a\u7528\u57fa\u7840\u6a21\u578b\u81f3\u75c5\u7406\u9886\u57df\u4f1a\u9047\u5230\u6311\u6218\uff0c\u63ed\u793a\u6f5c\u5728\u7684\u6982\u5ff5\u9519\u4f4d\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002", "method": "\u57fa\u4e8e\u7cfb\u7edf\u8bc4\u4ef7\u4e0e\u5206\u6790\uff0c\u68b3\u7406\u5f53\u524d\u75c5\u7406\u57fa\u7840\u6a21\u578b\u7684\u8868\u73b0\u3001\u7f3a\u9677\u53ca\u6f5c\u5728\u539f\u56e0\uff0c\u63d0\u51fa\u4e03\u5927\u6210\u56e0\uff0c\u5e76\u8ba8\u8bba\u4e0e\u7ec4\u7ec7\u75c5\u7406\u5f62\u6001\u76f8\u5173\u7684\u8bbe\u8ba1\u8003\u91cf\u4e0e\u7814\u7a76\u8def\u5f84\u3002", "result": "\u603b\u7ed3\u51fa\u4e03\u5927\u539f\u56e0\u53ca\u5bf9\u7b56\uff1a\u751f\u7269\u5b66\u590d\u6742\u6027\u3001\u65e0\u6548\u81ea\u76d1\u7763\u3001\u8fc7\u5ea6\u6cdb\u5316\u3001\u67b6\u6784\u590d\u6742\u3001\u7f3a\u4e4f\u9886\u57df\u521b\u65b0\u3001\u6570\u636e\u4e0d\u8db3\u3001\u4ee5\u53ca\u5207\u7247\u5927\u5c0f\u8bbe\u8ba1\u7f3a\u9677\uff0c\u6307\u51fa\u9700\u91cd\u65b0\u8bbe\u8ba1\u8303\u5f0f\u4ee5\u5951\u5408\u7ec4\u7ec7\u75c5\u7406\u7684\u672c\u8d28\u3002", "conclusion": "\u5f53\u524d\u75c5\u7406\u57fa\u7840\u6a21\u578b\u5728\u6982\u5ff5\u4e0a\u4e0e\u75c5\u53d8\u7ec4\u7ec7\u7684\u672c\u8d28\u4e0d\u5339\u914d\uff0c\u9700\u5bf9\u8303\u5f0f\u8fdb\u884c\u6839\u672c\u6027 rethink\uff0c\u63a8\u52a8\u66f4\u5177\u9886\u57df\u9002\u914d\u6027\u7684\u6a21\u578b\u4e0e\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2510.23629", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23629", "abs": "https://arxiv.org/abs/2510.23629", "authors": ["Nuo Chen", "Zehua Li", "Keqin Bao", "Junyang Lin", "Dayiheng Liu"], "title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models", "comment": null, "summary": "Building robust and general reasoning ability is a central goal in the\ndevelopment of large language models (LLMs). Recent efforts increasingly turn\nto code as a rich training source, given its inherent logical structure and\ndiverse reasoning paradigms such as divide-and-conquer, topological ordering,\nand enumeration. However, reasoning in code is often expressed implicitly and\nentangled with syntactic or implementation noise, making direct training on raw\ncode suboptimal.To address this, we introduce TracePile, a large-scale corpus\nof 2.6 million samples that transforms code execution into explicit,\nstep-by-step chain-of-thought-style rationales, which we call Chain of\nExecution (CoE). The corpus spans domains including mathematics, classical\nalgorithms and algorithmic competition, and is enriched with variable-tracing\nquestions and code rewritings to enhance logical granularity and code\ndiversity. We evaluate TracePile using three training setups:\ncontinue-pretraining, instruction tuning after pretraining, and two-stage\nfinetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,\nand Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and\nalgorithms demonstrate consistent improvements. Notably, TracePile boosts\nLLaMA3.1-8B by 7.1\\% on average across nine math datasets and delivers clear\ngains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.", "AI": {"tldr": "\u63d0\u51fa TracePile\uff1a\u4e00\u4e2a2.6M\u6837\u672c\u7684\u5927\u89c4\u6a21\u4ee3\u7801\u6267\u884c\u94fe\u5f0f\u63a8\u7406\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u663e\u5f0f\u7684 Chain of Execution \u5c06\u63a8\u7406\u8fc7\u7a0b\u5316\uff0c\u63d0\u5347\u591a\u6a21\u578b\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u903b\u8f91\u3001\u7b97\u6cd5\u7b49\u4efb\u52a1\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5728\u4e24\u9636\u6bb5\u5fae\u8c03\u548c\u4e0d\u540c\u57fa\u7ebf\u6a21\u578b\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u4ee3\u7801\u5177\u6709\u6e05\u6670\u7684\u903b\u8f91\u7ed3\u6784\uff0c\u76f4\u63a5\u5728\u539f\u59cb\u4ee3\u7801\u4e0a\u8bad\u7ec3\u96be\u4ee5\u83b7\u5f97\u6e05\u6670\u7684\u63a8\u7406\u8def\u5f84\uff1b\u4ee3\u7801\u4e2d\u7684\u9690\u5f0f\u63a8\u7406\u548c\u5b9e\u73b0\u566a\u58f0\u4f1a\u524a\u5f31\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u4e00\u4e2a\u53ef\u89e3\u91ca\u3001\u53ef\u63a7\u7684\u63a8\u7406\u8f68\u8ff9\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u904d\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa2.6M\u6837\u672c\u7684 TracePile\uff0c\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3a\u663e\u5f0f\u7684\u3001\u9010\u6b65\u7684 Chain of Execution\uff08CoE\uff09\u63a8\u7406\uff1b\u8986\u76d6\u6570\u5b66\u3001\u7ecf\u5178\u7b97\u6cd5\u3001\u7b97\u6cd5\u7ade\u8d5b\u7b49\u9886\u57df\uff0c\u5e76\u52a0\u5165\u53d8\u91cf\u8ffd\u8e2a\u9898\u548c\u4ee3\u7801\u6539\u5199\u4ee5\u63d0\u5347\u903b\u8f91\u7c92\u5ea6\u548c\u4ee3\u7801\u591a\u6837\u6027\uff1b\u901a\u8fc7\u4e09\u79cd\u8bad\u7ec3\u8bbe\u7f6e\uff08continue-pretraining\u3001pretraining\u540e\u6307\u4ee4\u5fae\u8c03\u3001\u4e24\u9636\u6bb5\u5fae\u8c03\uff09\u5bf9\u56db\u4e2a\u57fa\u7ebf\u6a21\u578b\uff08LLaMA 3\u3001LLaMA 3.1\u3001Qwen-2.5\u3001Qwen-2.5 Coder\uff09\u572820\u4e2a\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5404\u57fa\u7ebf\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\uff1a\u5728\u4e5d\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\uff0cLLaMA3.1-8B\u5e73\u5747\u63d0\u53477.1%\uff1b\u5728 LiveCodeBench\u3001CRUX\u3001MMLU \u7b49\u4efb\u52a1\u4e0a\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u53d6\u5f97\u663e\u8457\u6536\u76ca\u3002", "conclusion": "\u663e\u5f0f\u7684\u4ee3\u7801\u6267\u884c\u8f68\u8ff9\uff08CoE\uff09\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0cTracePile\u4e3a\u4ee3\u7801\u9a71\u52a8\u7684\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u8d44\u6e90\uff0c\u5177\u5907\u8de8\u9886\u57df\u548c\u8de8\u6a21\u578b\u7684\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2510.23896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23896", "abs": "https://arxiv.org/abs/2510.23896", "authors": ["Kosei Uemura", "Miaoran Zhang", "David Ifeoluwa Adelani"], "title": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages", "comment": null, "summary": "Text embeddings are an essential building component of several NLP tasks such\nas retrieval-augmented generation which is crucial for preventing\nhallucinations in LLMs. Despite the recent release of massively multilingual\nMTEB (MMTEB), African languages remain underrepresented, with existing tasks\noften repurposed from translation benchmarks such as FLORES clustering or\nSIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB\ncovering 59 languages, 14 tasks, and 38 datasets, including six newly added\ndatasets. Unlike many MMTEB datasets that include fewer than five languages,\nthe new additions span 14 to 56 African languages and introduce entirely new\ntasks, such as hate speech detection, intent detection, and emotion\nclassification, which were not previously covered. Complementing this, we\npresent AfriE5, an adaptation of the instruction-tuned mE5 model to African\nlanguages through cross-lingual contrastive distillation. Our evaluation shows\nthat AfriE5 achieves state-of-the-art performance, outperforming strong\nbaselines such as Gemini-Embeddings and mE5.", "AI": {"tldr": " AfriMTEB extends MMTEB with 59\u8bed\u8a00\u300114\u4efb\u52a1\u300138\u6570\u636e\u96c6\uff08\u542b6\u4e2a\u65b0\u6570\u636e\u96c6\uff09\uff0c\u8986\u76d6\u5e7f\u6cdb\u975e\u6d32\u8bed\u8a00\u5e76\u5f15\u5165\u65b0\u4efb\u52a1\uff1bAfriE5 \u901a\u8fc7\u8de8\u8bed\u5bf9\u6bd4\u84b8\u998f\u5c06 mE5 \u672c\u5730\u5316\u81f3\u975e\u6d32\u8bed\u8a00\uff0c\u8fbe\u5230state-of-the-art\u7684\u5d4c\u5165\u6a21\u578b\u6027\u80fd\u3002", "motivation": " \u975e\u6d32\u8bed\u8a00\u5728\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u8bc4\u6d4b\u57fa\u51c6\u4e2d\u957f\u671f\u7f3a\u4e4f\u4ee3\u8868\u6027\uff0c\u73b0\u6709\u4efb\u52a1\u591a\u6765\u81ea\u7ffb\u8bd1\u57fa\u51c6\u7684\u518d\u5229\u7528\uff0c\u5bfc\u81f4\u5d4c\u5165\u6a21\u578b\u5bf9\u975e\u6d32\u8bed\u8a00\u7684\u8986\u76d6\u4e0d\u8db3\uff0c\u5f71\u54cd\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7b49\u5e94\u7528\u7684\u9c81\u68d2\u6027\u548c\u51cf\u5c11\u5e7b\u89c9\u3002\u9700\u6269\u5927\u8bed\u8a00\u8986\u76d6\u3001\u5f15\u5165\u65b0\u4efb\u52a1\u5e76\u63d0\u5347\u8de8\u8bed\u8a00\u8868\u793a\u80fd\u529b\u3002", "method": " \u6784\u5efa AfriMTEB\uff1a\u8986\u76d659\u79cd\u8bed\u8a00\u300114\u9879\u4efb\u52a1\u300138\u4e2a\u6570\u636e\u96c6\uff08\u5176\u4e2d6\u4e2a\u4e3a\u65b0\u6570\u636e\u96c6\uff09\uff0c\u6db5\u76d6 hate speech\u3001intent detection\u3001emotion classification \u7b49\u65b0\u4efb\u52a1\uff1b\u53d1\u5e03 AfriE5\uff0c\u5c06 mE5 \u901a\u8fc7\u8de8\u8bed\u5bf9\u6bd4\u84b8\u998f\u9002\u914d\u81f3\u975e\u6d32\u8bed\u8a00\u4ee5\u63d0\u5347\u8de8\u8bed\u8a00\u7ed3\u6784\u548c\u6307\u4ee4\u9002\u5e94\u6027\uff1b\u5bf9\u6bd4 Gemini-Embeddings\u3001mE5 \u7b49\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": " AfriMTEB \u7684\u6269\u5c55\u4f7f\u5728\u975e\u6d32\u8bed\u8a00\u4e0a\u7684\u5d4c\u5165\u8bc4\u4f30\u66f4\u5168\u9762\uff1bAfriE5 \u5728\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u73b0\u6709\u5d4c\u5165\u6a21\u578b\u7684\u6700\u4f18\u6027\u80fd\uff0c\u4f18\u4e8e Gemini-Embeddings \u4e0e mE5\u3002", "conclusion": " AfriMTEB \u63d0\u4f9b\u4e86\u9762\u5411\u975e\u6d32\u8bed\u8a00\u7684\u66f4\u5168\u9762\u8bc4\u6d4b\u57fa\u51c6\uff0cAfriE5 \u5c55\u793a\u4e86\u901a\u8fc7\u8de8\u8bed\u5bf9\u6bd4\u84b8\u998f\u5b9e\u73b0\u5bf9\u975e\u6d32\u8bed\u8a00\u7684\u6709\u6548\u9002\u914d\u548c\u5f3a\u52bf\u8868\u73b0\u3002\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5145\u6570\u636e\u8d28\u91cf\u3001\u8bc4\u6d4b\u4efb\u52a1\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u8bc4\u4f30\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u4e0e\u8de8\u57df\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2510.23822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23822", "abs": "https://arxiv.org/abs/2510.23822", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "comment": null, "summary": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning\nremain challenging for large language models (LLMs). Sequential prompting\nmethods are prone to context drift, loss of goal information, and recurrent\nfailure cycles, while hierarchical prompting methods often weaken cross-level\ncontinuity or incur substantial runtime overhead. We introduce ReCAP (Recursive\nContext-Aware Reasoning and Planning), a hierarchical framework with shared\ncontext for reasoning and planning in LLMs. ReCAP combines three key\nmechanisms: (i) plan-ahead decomposition, in which the model generates a full\nsubtask list, executes the first item, and refines the remainder; (ii)\nstructured re-injection of parent plans, maintaining consistent multi-level\ncontext during recursive return; and (iii) memory-efficient execution, bounding\nthe active prompt so costs scale linearly with task depth. Together these\nmechanisms align high-level goals with low-level actions, reduce redundant\nprompting, and preserve coherent context updates across recursion. Experiments\ndemonstrate that ReCAP substantially improves subgoal alignment and success\nrates on various long-horizon reasoning benchmarks, achieving a 32% gain on\nsynchronous Robotouille and a 29% improvement on asynchronous Robotouille under\nthe strict pass@1 protocol.", "AI": {"tldr": "ReCAP\uff1a\u4e00\u79cd\u9012\u5f52\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63a8\u7406\u4e0e\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u5212\u524d\u77bb\u3001\u7236\u8ba1\u5212\u7684\u7ed3\u6784\u5316\u518d\u6ce8\u5165\u548c\u5185\u5b58\u9ad8\u6548\u6267\u884c\uff0c\u5728\u957f\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u5b50\u76ee\u6807\u5bf9\u9f50\u548c\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u8ddd\u79bb\u591a\u6b65\u63a8\u7406\u4e2d\u5bb9\u6613\u51fa\u73b0\u7684\u4e0a\u4e0b\u6587\u6f02\u79fb\u3001\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u4ee5\u53ca\u8de8\u5c42\u6b21\u8fde\u7eed\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u8fd0\u884c\u5f00\u9500\u3002", "method": "\u5f15\u5165 ReCAP \u6846\u67b6\uff0c\u7ed3\u5408\uff1a(i) plan-ahead decomposition\uff1a\u751f\u6210\u5b8c\u6574\u5b50\u4efb\u52a1\u6e05\u5355\uff0c\u6267\u884c\u9996\u9879\u5e76\u5bf9\u5269\u4f59\u8fdb\u884c\u8fed\u4ee3 refinements\uff1b(ii) structured re-injection of parent plans\uff1a\u9012\u5f52\u8fd4\u56de\u65f6\u4ee5\u7ed3\u6784\u5316\u65b9\u5f0f\u91cd\u65b0\u6ce8\u5165\u7236\u8ba1\u5212\uff0c\u7ef4\u6301\u591a\u5c42\u4e0a\u4e0b\u6587\u7684\u4e00\u81f4\u6027\uff1b(iii) memory-efficient execution\uff1a\u5c06\u6d3b\u52a8\u63d0\u793a\u9650\u5236\u5728\u5bf9\u6df1\u5ea6\u7ebf\u6027\u589e\u957f\u7684\u6210\u672c\u63a7\u5236\u4e0b\uff0c\u786e\u4fdd\u6210\u672c\u968f\u4efb\u52a1\u6df1\u5ea6\u7ebf\u6027\u589e\u957f\u3002", "result": "\u5728\u957f\u65f6\u5e8f\u57fa\u51c6\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cReCAP \u663e\u8457\u63d0\u5347\u5b50\u76ee\u6807\u5bf9\u9f50\u4e0e\u6210\u529f\u7387\uff1a\u540c\u6b65\u4efb\u52a1 Robotouille \u7684\u4e25\u683c pass@1 \u63d0\u5347\u7ea632%\uff0c\u5f02\u6b65\u4efb\u52a1 Robotouille \u63d0\u5347\u7ea629%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u66f4\u597d\u5730\u8fde\u63a5\u9ad8\u5c42\u76ee\u6807\u4e0e\u4f4e\u5c42\u884c\u52a8\u3001\u51cf\u5c11\u5197\u4f59\u63d0\u793a\uff0c\u4ee5\u53ca\u5728\u9012\u5f52\u6267\u884c\u4e2d\u7ef4\u6301\u4e00\u81f4\u7684\u4e0a\u4e0b\u6587\u66f4\u65b0\uff0c\u663e\u8457\u6539\u5584\u957f\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23630", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23630", "abs": "https://arxiv.org/abs/2510.23630", "authors": ["Ninghui Feng", "Yiyan Qi"], "title": "NUM2EVENT: Interpretable Event Reasoning from Numerical time-series", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated impressive multimodal\nreasoning capabilities, yet their understanding of purely numerical time-series\nsignals remains limited. Existing approaches mainly focus on forecasting or\ntrend description, without uncovering the latent events that drive numerical\nchanges or explaining the reasoning process behind them. In this work, we\nintroduce the task of number-to-event reasoning and decoding, which aims to\ninfer interpretable structured events from numerical inputs, even when current\ntext is unavailable. To address the data scarcity and semantic alignment\nchallenges, we propose a reasoning-aware framework that integrates an\nagent-guided event extractor (AGE), a marked multivariate Hawkes-based\nsynthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a\ntime-series encoder with a structured decoder. Our model explicitly reasons\nover numerical changes, generates intermediate explanations, and outputs\nstructured event hypotheses. Experiments on multi-domain datasets show that our\nmethod substantially outperforms strong LLM baselines in event-level precision\nand recall. These results suggest a new direction for bridging quantitative\nreasoning and semantic understanding, enabling LLMs to explain and predict\nevents directly from numerical dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4ece\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u4e2d\u8fdb\u884c number-to-event \u63a8\u7406\u7684\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7 AGE\u3001EveDTS \u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u4e8b\u4ef6\u8f93\u51fa\uff0c\u663e\u8457\u4f18\u4e8e LLM \u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u591a\u805a\u7126\u9884\u6d4b/\u63cf\u8ff0\u8d8b\u52bf\uff0c\u7f3a\u4e4f\u5bf9\u4ea7\u751f\u6570\u503c\u53d8\u5316\u7684\u9690\u6027\u4e8b\u4ef6\u7684\u63a8\u65ad\u548c\u89e3\u91ca\uff1b\u5728\u7f3a\u4e4f\u6587\u672c\u4fe1\u606f\u65f6\u4e5f\u9700\u8981\u8bed\u4e49\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165 AGE\uff08agent-guided event extractor\uff09\u3001EveDTS\uff08\u5e26\u6807\u8bb0\u7684\u591a\u53d8\u91cf Hawkes \u751f\u6210\u5668\uff09\u53ca\u4e24\u9636\u6bb5\u5fae\u8c03\uff08\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668 + \u7ed3\u6784\u5316\u89e3\u7801\u5668\uff09\uff0c\u901a\u8fc7\u628a\u6570\u503c\u53d8\u5316\u8f6c\u5316\u4e3a\u4e2d\u95f4\u89e3\u91ca\u5e76\u8f93\u51fa\u7ed3\u6784\u5316\u4e8b\u4ef6\u5047\u8bbe\u3002", "result": "\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u5f3a\u5927\u7684 LLM \u57fa\u7ebf\u76f8\u6bd4\uff0c\u4e8b\u4ef6\u7ea7\u7684\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u4e3a\u91cf\u5316\u63a8\u7406\u548c\u8bed\u4e49\u7406\u89e3\u4e4b\u95f4\u642d\u6865\uff0c\u8d4b\u80fd LLM \u4ece\u6570\u503c\u52a8\u6001\u4e2d\u76f4\u63a5\u89e3\u91ca\u548c\u9884\u6d4b\u4e8b\u4ef6\u3002"}}
{"id": "2510.23921", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23921", "abs": "https://arxiv.org/abs/2510.23921", "authors": ["Kaveh Eskandari Miandoab", "Mahammed Kamruzzaman", "Arshia Gharooni", "Gene Louis Kim", "Vasanth Sarathy", "Ninareh Mehrabi"], "title": "Breaking the Benchmark: Revealing LLM Bias via Minimal Contextual Augmentation", "comment": "9 pages, 3 figures, 3 tables", "summary": "Large Language Models have been shown to demonstrate stereotypical biases in\ntheir representations and behavior due to the discriminative nature of the data\nthat they have been trained on. Despite significant progress in the development\nof methods and models that refrain from using stereotypical information in\ntheir decision-making, recent work has shown that approaches used for bias\nalignment are brittle. In this work, we introduce a novel and general\naugmentation framework that involves three plug-and-play steps and is\napplicable to a number of fairness evaluation benchmarks. Through application\nof augmentation to a fairness evaluation dataset (Bias Benchmark for Question\nAnswering (BBQ)), we find that Large Language Models (LLMs), including\nstate-of-the-art open and closed weight models, are susceptible to\nperturbations to their inputs, showcasing a higher likelihood to behave\nstereotypically. Furthermore, we find that such models are more likely to have\nbiased behavior in cases where the target demographic belongs to a community\nless studied by the literature, underlining the need to expand the fairness and\nsafety research to include more diverse communities.", "AI": {"tldr": "\u901a\u8fc7\u4e09\u6b65\u63d2\u4ef6\u5f0f\u589e\u5f3a\u6846\u67b6\u5bf9\u504f\u89c1\u8bc4\u4f30\u8fdb\u884c\u901a\u7528\u5316\u5904\u7406\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u8f93\u5165\u6270\u52a8\u4e0b\uff0cLLMs\u66f4\u6613\u5c55\u793a\u523b\u677f\u504f\u89c1\uff0c\u4e14\u5bf9\u8f83\u5c11\u7814\u7a76\u7684\u7fa4\u4f53\u66f4\u6613\u4ea7\u751f\u504f\u89c1\uff0c\u63d0\u793a\u9700\u6269\u5927\u516c\u5e73\u4e0e\u5b89\u5168\u7814\u7a76\u8986\u76d6\u9762\u3002", "motivation": "LLMs \u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\uff0c\u4f1a\u4f53\u73b0\u548c\u653e\u5927\u523b\u677f\u504f\u89c1\uff1b\u73b0\u6709\u7684\u504f\u89c1\u5bf9\u9f50\u65b9\u6cd5\u8106\u5f31\u4e14\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u591a\u6837\u5316\u8f93\u5165\uff1b\u9700\u8981\u4e00\u4e2a\u901a\u7528\u7684\u3001\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u516c\u5e73\u8bc4\u4f30\u57fa\u51c6\u7684\u589e\u5f3a\u6846\u67b6\u6765\u66f4\u7a33\u5065\u5730\u8bc4\u4f30\u4e0e\u7f13\u89e3\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u589e\u5f3a\u6846\u67b6\uff0c\u5305\u542b\u4e09\u6b65\u53ef\u63d2\u62d4\u7684\u6d41\u7a0b\uff0c\u5c06\u5176\u5e94\u7528\u5230\u516c\u5e73\u8bc4\u4f30\u6570\u636e\u96c6 BBQ\uff08Bias Benchmark for Question Answering\uff09\u3002\u901a\u8fc7\u5bf9\u8f93\u5165\u8fdb\u884c\u589e\u5f3a/\u6270\u52a8\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u504f\u89c1\u884c\u4e3a\u4e0a\u7684\u9c81\u68d2\u6027\u4e0e\u654f\u611f\u6027\u3002", "result": "\u5728\u5e94\u7528\u589e\u5f3a\u540e\uff0cLLMs\uff08\u5305\u62ec\u516c\u5f00\u4e0e\u95ed\u6e90\u6a21\u578b\uff09\u5bf9\u8f93\u5165\u6270\u52a8\u66f4\u6613\u8868\u73b0\u51fa\u523b\u677f\u504f\u89c1\uff1b\u5728\u76ee\u6807\u4eba\u7fa4\u5c5e\u4e8e\u8f83\u5c11\u88ab\u6587\u732e\u7814\u7a76\u7684\u7fa4\u4f53\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u504f\u89c1\u6982\u7387\uff0c\u51f8\u663e\u9700\u8981\u5c06\u516c\u5e73\u4e0e\u5b89\u5168\u7814\u7a76\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u591a\u6837\u7fa4\u4f53\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u6269\u5927\u5bf9\u591a\u6837\u5316\u7fa4\u4f53\u7684\u516c\u5e73\u4e0e\u5b89\u5168\u7814\u7a76\uff0c\u5c06\u9c81\u68d2\u6027\u4f5c\u4e3a\u504f\u89c1\u8bc4\u4f30\u7684\u6838\u5fc3\uff0c\u5e76\u63a8\u52a8\u66f4\u5e7f\u6cdb\u7684\u8bc4\u4f30\u57fa\u51c6\u4e0e\u7f13\u89e3\u7b56\u7565\u7684\u5f00\u53d1\u3002"}}
{"id": "2510.23631", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23631", "abs": "https://arxiv.org/abs/2510.23631", "authors": ["Yuxuan Tang", "Yifan Feng"], "title": "Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling", "comment": null, "summary": "Alignment of large language models (LLMs) has predominantly relied on\npairwise preference optimization, where annotators select the better of two\nresponses to a prompt. While simple, this approach overlooks the opportunity to\nlearn from richer forms of human feedback, such as multiwise comparisons and\ntop-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a\nunified framework that bridges preference optimization with (ranked) choice\nmodeling via maximum likelihood estimation. The framework is flexible,\nsupporting both utility-based and rank-based choice models. It subsumes several\nexisting pairwise methods (e.g., DPO, SimPO), while providing principled\ntraining objectives for richer feedback formats. We instantiate this framework\nwith two representative ranked choice models (Multinomial Logit and\nMallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across\nAlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms\ncompetitive baselines. RCPO shows how directly leveraging ranked preference\ndata, combined with the right choice models, yields more effective alignment.\nIt offers a versatile and extensible foundation for incorporating (ranked)\nchoice modeling into LLM training.", "AI": {"tldr": "\u63d0\u51faRCPO\u6846\u67b6\u6765\u6269\u5c55\u5bf9LLM\u7684\u504f\u597d\u4f18\u5316\uff0c\u4ece\u5355\u7eaf\u7684\u4e24\u4e24\u6bd4\u8f83\u6269\u5c55\u5230\u591a\u9879\u9009\u62e9\u4e0e\u6392\u5e8f\uff0c\u7edf\u4e00\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u6574\u5408\u6392\u5e8f\u504f\u597d\u4e0e\u9009\u62e9\u6a21\u578b\uff0c\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u591a\u57fa\u4e8e\u4e24\u4e24\u504f\u597d\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u66f4\u4e30\u5bcc\u7684\u53cd\u9988\u5f62\u5f0f\uff08\u591a\u9879\u6bd4\u8f83\u3001Top-k\u6392\u5e8f\uff09\uff0c\u4ee5\u81f4\u6f5c\u5728\u4fe1\u606f\u672a\u88ab\u6316\u6398\uff0c\u63d0\u5347\u7a7a\u95f4\u6709\u9650\u3002", "method": "\u63d0\u51fa ranked choice preference optimization (RCPO) \u6846\u67b6\uff0c\u5c06\u504f\u597d\u4f18\u5316\u4e0e\uff08\u6392\u5e8f\uff09\u9009\u62e9\u6a21\u578b\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u8054\u5408\u8d77\u6765\uff0c\u6846\u67b6\u7075\u6d3b\uff0c\u652f\u6301\u57fa\u4e8e\u6548\u7528\u7684\u4e0e\u57fa\u4e8e\u6392\u5e8f\u7684\u9009\u62e9\u6a21\u578b\uff0c\u80fd\u8986\u76d6\u5e76\u7b80\u5316\u73b0\u6709\u7684\u5bf9\u5076\u65b9\u6cd5\uff08\u5982DPO\u3001SimPO\uff09\uff0c\u5e76\u63d0\u4f9b\u5bf9\u66f4\u4e30\u5bcc\u53cd\u9988\u5f62\u5f0f\u7684\u8bad\u7ec3\u76ee\u6807\u3002\u4ee5\u4e24\u79cd\u6392\u5e8f\u9009\u62e9\u6a21\u578b\uff08\u591a\u9879\u5f0f\u903b\u8f91\u56de\u5f52 MNL \u4e0e Mallows-RMJ\uff09\u4e3a\u4f8b\u5b9e\u73b0\u3002", "result": "\u5728 Llama-3-8B-Instruct \u4e0e Gemma-2-9B-it \u8eab\u4e0a\u8fdb\u884c AlpacaEval 2 \u4e0e Arena-Hard \u57fa\u51c6\u6d4b\u8bd5\uff0cRCPO \u5747\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u663e\u73b0\u51fa\u76f4\u63a5\u5229\u7528\u6392\u5e8f\u504f\u597d\u6570\u636e\u53ca\u5408\u9002\u7684\u9009\u62e9\u6a21\u578b\u80fd\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5bf9\u9f50\u3002", "conclusion": "RCPO \u4e3a\u5c06\u6392\u5e8f\u504f\u597d\u6570\u636e\u4ee5\u53ca\u6392\u5e8f/\u6548\u7528\u9009\u62e9\u6a21\u578b\u6574\u5408\u5165 LLM \u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u5e76\u6709\u671b\u63a8\u52a8\u66f4\u4e30\u5bcc\u7684\u4eba\u7c7b\u53cd\u9988\u5728\u5bf9\u9f50\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.23856", "categories": ["cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.23856", "abs": "https://arxiv.org/abs/2510.23856", "authors": ["Segev Shlomov", "Alon Oved", "Sami Marreed", "Ido Levy", "Offer Akrabi", "Avi Yaeli", "\u0141ukasz Str\u0105k", "Elizabeth Koumpan", "Yinon Goldshtein", "Eilam Shapira", "Nir Mashkif", "Asaf Adi"], "title": "From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production", "comment": "AAAI Conference on Artificial Intelligence", "summary": "Agents are rapidly advancing in automating digital work, but enterprises face\na harder challenge: moving beyond prototypes to deployed systems that deliver\nmeasurable business value. This path is complicated by fragmented frameworks,\nslow development, and the absence of standardized evaluation practices.\nGeneralist agents have emerged as a promising direction, excelling on academic\nbenchmarks and offering flexibility across task types, applications, and\nmodalities. Yet, evidence of their use in production enterprise settings\nremains limited. This paper reports IBM's experience developing and piloting\nthe Computer Using Generalist Agent (CUGA), which has been open-sourced for the\ncommunity (https://github.com/cuga-project/cuga-agent). CUGA adopts a\nhierarchical planner--executor architecture with strong analytical foundations,\nachieving state-of-the-art performance on AppWorld and WebArena. Beyond\nbenchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing\ntalent acquisition domain, addressing enterprise requirements for scalability,\nauditability, safety, and governance. To support assessment, we introduce\nBPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary\nevaluations, CUGA approached the accuracy of specialized agents while\nindicating potential for reducing development time and cost. Our contribution\nis twofold: presenting early evidence of generalist agents operating at\nenterprise scale, and distilling technical and organizational lessons from this\ninitial pilot. We outline requirements and next steps for advancing\nresearch-grade architectures like CUGA into robust, enterprise-ready systems.", "AI": {"tldr": "IBM's CUGA: a generalist agent with hierarchical planner\u2013executor architecture, open-sourced; state-of-the-art on benchmarks; piloted in enterprise BPO talent acquisition; introduces BPO-TA benchmark; shows near-specialized accuracy and potential enterprise benefits; outlines lessons and path to enterprise readiness.", "motivation": "Bridge the gap between prototype generalist agents and deployed enterprise systems by addressing fragmentation, slow development, and lack of standardized evaluation practices.", "method": "Developed CUGA with a hierarchical planner\u2013executor architecture; evaluated on AppWorld and WebArena benchmarks; piloted in a Business-Process-Outsourcing talent acquisition domain; introduced a 26-task BPO-TA benchmark; preliminary assessments.", "result": "Achieved state-of-the-art performance on AppWorld and WebArena; in the enterprise pilot, approached the accuracy of specialized agents and showed potential to reduce development time and cost.", "conclusion": "Provides initial evidence that generalist agents can operate at enterprise scale; distils technical and organizational lessons from the pilot; outlines requirements and next steps for maturing research-grade architectures like CUGA into robust, enterprise-ready systems."}}
{"id": "2510.23632", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23632", "abs": "https://arxiv.org/abs/2510.23632", "authors": ["Guozhong Li", "Muhannad Alhumaidi", "Spiros Skiadopoulos", "Panos Kalnis"], "title": "LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression", "comment": null, "summary": "The rapid growth of high-resolution scientific simulations and observation\nsystems is generating massive spatiotemporal datasets, making efficient,\nerror-bounded compression increasingly important. Meanwhile, decoder-only large\nlanguage models (LLMs) have demonstrated remarkable capabilities in modeling\ncomplex sequential data. In this paper, we propose LLMCOMP, a novel lossy\ncompression paradigm that leverages decoder-only large LLMs to model scientific\ndata. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via\nZ-order curves to preserve locality, and applies coverage-guided sampling to\nenhance training efficiency. An autoregressive transformer is then trained with\nspatial-temporal embeddings to model token transitions. During compression, the\nmodel performs top-k prediction, storing only rank indices and fallback\ncorrections to ensure strict error bounds. Experiments on multiple reanalysis\ndatasets show that LLMCOMP consistently outperforms state-of-the-art\ncompressors, achieving up to 30% higher compression ratios under strict error\nbounds. These results highlight the potential of LLMs as general-purpose\ncompressors for high-fidelity scientific data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u5de8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6709\u635f\u538b\u7f29\u6846\u67b6 LLMCOMP\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u79d1\u5b66\u6570\u636e\u3002\u901a\u8fc7\u5c06 3D \u573a\u91cf\u91cf\u5316\u4e3a\u79bb\u6563\u6807\u8bb0\u3001Z-order \u6392\u5217\u4ee5\u4fdd\u6301\u5c40\u90e8\u6027\u3001\u8986\u76d6\u5f15\u5bfc\u91c7\u6837\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u4f7f\u7528\u5e26\u65f6\u7a7a\u5d4c\u5165\u7684\u81ea\u56de\u5f52 Transformer \u5efa\u6a21\u6807\u8bb0\u8f6c\u79fb\u3002\u5728\u538b\u7f29\u9636\u6bb5\u6267\u884c top-k \u9884\u6d4b\uff0c\u4ec5\u5b58\u50a8\u79e9\u7d22\u5f15\u53ca\u56de\u9000\u7ea0\u9519\u4ee5\u4e25\u683c\u4fdd\u8bc1\u8bef\u5dee\u754c\u9650\u3002\u5b9e\u9a8c\u8868\u660e\u5728\u591a\u7ec4\u518d\u5206\u6790\u6570\u636e\u96c6\u4e0a\uff0cLLMCOMP \u76f8\u8f83\u4e8e\u524d\u6cbf\u538b\u7f29\u5668\u5728\u4e25\u683c\u8bef\u5dee\u7ea6\u675f\u4e0b\u5b9e\u73b0\u6700\u9ad8\u53ef\u8fbe 30% \u7684\u66f4\u9ad8\u538b\u7f29\u6bd4\u3002", "motivation": "\u5e94\u5bf9\u79d1\u5b66\u6570\u636e\u5feb\u901f\u589e\u957f\u5e26\u6765\u7684\u5927\u89c4\u6a21\u65f6\u7a7a\u6570\u636e\u5b58\u50a8\u4e0e\u4f20\u8f93\u6311\u6218\uff0c\u73b0\u6709\u7684\u6709\u635f\u538b\u7f29\u5728\u8bef\u5dee\u754c\u9650\u4e0e\u538b\u7f29\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u6700\u8fd1\u7684\u89e3\u7801\u5668\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5efa\u6a21\u590d\u6742\u5e8f\u5217\u6570\u636e\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u672c\u6587\u5c1d\u8bd5\u5c06\u5176\u7528\u4e8e\u79d1\u5b66\u6570\u636e\u7684\u538b\u7f29\u3002", "method": "\u5c06 3D \u573a\u91cf\u79bb\u6563\u5316\u4e3a token\uff1b\u91c7\u7528 Z-order \u66f2\u7ebf\u5bf9 token \u8fdb\u884c\u5e03\u5c40\u4ee5\u4fdd\u7559\u5c40\u90e8\u6027\uff1b\u5229\u7528\u8986\u76d6\u5f15\u5bfc\u91c7\u6837\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff1b\u4f7f\u7528\u5e26\u7a7a\u95f4\u2013\u65f6\u95f4\u5d4c\u5165\u7684\u81ea\u56de\u5f52 Transformer \u6765\u5efa\u6a21 token \u8f6c\u79fb\u3002\u538b\u7f29\u65f6\u6267\u884c top-k \u9884\u6d4b\uff0c\u4ec5\u5b58\u50a8 rank \u7d22\u5f15\u548c\u56de\u9000\u7ea0\u9519\uff0c\u786e\u4fdd\u4e25\u683c\u7684\u8bef\u5dee\u754c\u9650\u3002", "result": "\u5728\u591a\u4e2a\u518d\u5206\u6790\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLMCOMP \u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u538b\u7f29\u5668\uff0c\u5728\u4e25\u683c\u8bef\u5dee\u8fb9\u754c\u4e0b\u5b9e\u73b0\u6700\u9ad8\u53ef\u8fbe 30% \u7684\u538b\u7f29\u6bd4\u63d0\u5347\u3002", "conclusion": "\u8bc1\u660e\u4e86 LLMs \u4f5c\u4e3a\u901a\u7528\u9ad8\u4fdd\u771f\u79d1\u5b66\u6570\u636e\u538b\u7f29\u5668\u7684\u6f5c\u529b\uff0c\u63ed\u793a\u4e86\u89e3\u7801\u5668\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u538b\u7f29\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.23941", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23941", "abs": "https://arxiv.org/abs/2510.23941", "authors": ["Soham Satyadharma", "Fatemeh Sheikholeslami", "Swati Kaul", "Aziz Umit Batur", "Suleiman A. Khan"], "title": "Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs", "comment": null, "summary": "We introduce a novel, training free cascade for auto-prompting Large Language\nModels (LLMs) to assess product quality in e-commerce. Our system requires no\ntraining labels or model fine-tuning, instead automatically generating and\nrefining prompts for evaluating attribute quality across tens of thousands of\nproduct category-attribute pairs. Starting from a seed of human-crafted\nprompts, the cascade progressively optimizes instructions to meet\ncatalog-specific requirements. This approach bridges the gap between general\nlanguage understanding and domain-specific knowledge at scale in complex\nindustrial catalogs. Our extensive empirical evaluations shows the auto-prompt\ncascade improves precision and recall by $8-10\\%$ over traditional\nchain-of-thought prompting. Notably, it achieves these gains while reducing\ndomain expert effort from 5.1 hours to 3 minutes per attribute - a $99\\%$\nreduction. Additionally, the cascade generalizes effectively across five\nlanguages and multiple quality assessment tasks, consistently maintaining\nperformance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bad\u7ec3\u65e0\u5173\u7684\u81ea\u6211\u63d0\u793a\u7ea7\u8054\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u548c refine \u63d0\u793a\u6765\u8bc4\u4f30\u7535\u5546\u4e2d\u7684\u4ea7\u54c1\u8d28\u91cf\uff0c\u8986\u76d6\u6570\u4e07\u79cd\u7c7b-\u5c5e\u6027\u5bf9\uff1b\u5728\u4e0d\u9700\u8981\u6807\u7b7e\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u73878-10%\uff0c\u663e\u8457\u964d\u4f4e\u9886\u57df\u4e13\u5bb6\u52b3\u52a8\u91cf\uff08\u4ece5.1\u5c0f\u65f6\u964d\u81f33\u5206\u949f/\u5c5e\u6027\uff0c\u7ea699%\u964d\u4f4e\uff09\uff0c\u5e76\u5728\u4e94\u79cd\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u826f\u597d\u6cdb\u5316\u3002", "motivation": "\u5f25\u8865\u901a\u7528\u8bed\u8a00\u7406\u89e3\u4e0e\u9762\u5411\u5927\u89c4\u6a21\u590d\u6742\u76ee\u5f55\u7684\u9886\u57df\u77e5\u8bc6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff1b\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6807\u6ce8\u6570\u636e\u6216\u5fae\u8c03\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff1b\u9700\u8981\u4e00\u79cd\u65e0\u8bad\u7ec3\u6807\u7b7e\u3001\u53ef\u6269\u5c55\u4e14\u8de8\u8bed\u8a00\u7684\u8bc4\u4f30\u65b9\u6848\u4ee5\u63d0\u5347\u4ea7\u54c1\u8d28\u91cf\u8bc4\u4ef7\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8bad\u7ec3-free\u7684\u7ea7\u8054\u81ea\u63d0\u793a\u6846\u67b6\uff0c\u4ece\u4eba\u7c7b\u8bbe\u8ba1\u7684\u79cd\u5b50\u63d0\u793a\u51fa\u53d1\uff0c\u9010\u6b65\u4f18\u5316\u6307\u4ee4\u4ee5\u6ee1\u8db3\u76ee\u5f55\u7279\u5b9a\u9700\u6c42\uff1b\u9488\u5bf9\u6570\u4e07\u79cd\u7c7b-\u5c5e\u6027\u5bf9\u81ea\u52a8\u751f\u6210\u4e0e refinement \u63d0\u793a\uff0c\u7528\u4e8e\u8bc4\u4f30\u5c5e\u6027\u8d28\u91cf\uff1b\u65e0\u9700\u6807\u6ce8\u6570\u636e\u6216\u6a21\u578b\u5fae\u8c03\uff0c\u8de8\u8bed\u8a00\u548c\u8de8\u4efb\u52a1\u5b9e\u73b0\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5927\u91cf\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u81ea\u63d0\u793a\u7ea7\u8054\u5728\u7cbe\u786e\u5ea6\u4e0e\u53ec\u56de\u7387\u65b9\u9762\u6bd4\u4f20\u7edf\u7684\u9010\u6b65\u601d\u7ef4\u63d0\u793a\uff08chain-of-thought\uff09\u63d0\u53478-10%\uff1b\u5728\u5c5e\u6027\u8bc4\u4f30\u4e0a\u5c06\u57df\u4e13\u5bb6\u5de5\u4f5c\u91cf\u4ece5.1\u5c0f\u65f6\u964d\u81f33\u5206\u949f/\u5c5e\u6027\uff0c\u964d\u4f4e\u5e45\u5ea6\u7ea699%\uff1b\u5e76\u4e14\u5bf9\u4e94\u79cd\u8bed\u8a00\u53ca\u591a\u9879\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u5177\u5907\u826f\u597d\u6cdb\u5316\uff0c\u4e14\u6027\u80fd\u63d0\u5347\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u81ea\u63d0\u793a\u7ea7\u8054\u8fde\u63a5\u4e86\u901a\u7528\u8bed\u8a00\u7406\u89e3\u4e0e\u9886\u57df\u77e5\u8bc6\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u590d\u6742\u76ee\u5f55\u4e2d\u63d0\u4f9b\u65e0\u8bad\u7ec3\u9700\u6c42\u7684\u9ad8\u6548\u4ea7\u54c1\u8d28\u91cf\u8bc4\u4f30\uff0c\u4e14\u5177\u5907\u8de8\u8bed\u8a00\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.23881", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23881", "abs": "https://arxiv.org/abs/2510.23881", "authors": ["Xidong Feng", "Vivek Veeriah", "Marcus Chiam", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Federico Barbero", "Johan Obando-Ceron", "Jiaxin Shi", "Satinder Singh", "Shaobo Hou", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Generating Creative Chess Puzzles", "comment": null, "summary": "While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.", "AI": {"tldr": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u57fa\u4e8e\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u68cb\u9898\u7684\u53cd\u76f4\u89c9\u6027\u3001\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\uff0c\u5e76\u83b7\u5f97\u4eba\u7c7b\u4e13\u5bb6\u7684\u9ad8\u5ea6\u8bc4\u4ef7\uff0c\u6700\u7ec8\u4ea7\u51faAI\u751f\u6210\u68cb\u9898\u96c6\u518c\u3002", "motivation": "\u5728\u68cb\u9898\u751f\u6210\u9886\u57df\uff0cGenAI \u5f80\u5f80\u96be\u4ee5\u4ea7\u51fa\u771f\u6b63\u6709\u521b\u9020\u6027\u3001\u5ba1\u7f8e\u6027\u4e14\u5177\u6311\u6218\u6027\u7684\u9898\u76ee\u3002\u672c\u7814\u7a76\u8bd5\u56fe\u901a\u8fc7\u8bbe\u8ba1\u66f4\u8d34\u8fd1\u4e13\u4e1a\u8bc4\u4f30\u7684\u5956\u52b1\u673a\u5236\u6765\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u5bf9\u751f\u6210\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u65b0\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f18\u5316\u68cb\u9898\u7684\u65b0\u9896\u6027\u3001\u53cd\u76f4\u89c9\u6027\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u611f\u3002", "result": "\u53cd\u76f4\u89c9\u9898\u76ee\u6bd4\u4f8b\u7531 0.22%\uff08\u6709\u76d1\u7763\uff09\u63d0\u5347\u81f3 2.5%\uff0c\u8d85\u8fc7\u6570\u636e\u96c6\u6c34\u5e73\uff082.1%\uff09\u548c\u6700\u4f73Lichess\u6a21\u578b\uff080.4%\uff09\uff1b\u68cb\u9898\u5728\u65b0\u9896\u6027\u4e0e\u591a\u6837\u6027\u65b9\u9762\u7b26\u5408\u57fa\u51c6\uff0c\u4e14\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u66f4\u5177\u521b\u9020\u6027\u3001\u8da3\u5473\u6027\u548c\u53cd\u76f4\u89c9\u6027\uff0c\u8fbe\u5230\u63a5\u8fd1\u7ecf\u5178\u4f5c\u54c1\u7684\u6c34\u5e73\uff1b\u6700\u7ec8\u4ea7\u51fa\u4e00\u672cAI\u751f\u6210\u68cb\u9898\u96c6\u518c\uff0c\u83b7\u5f97\u4e09\u4f4d\u4e16\u754c\u7ea7\u4e13\u5bb6\u7684\u521b\u610f\u8ba4\u53ef\u3002", "conclusion": "\u5c06\u68cb\u5f15\u64ce\u7edf\u8ba1\u4fe1\u606f\u878d\u5165\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u68cb\u9898\u7684\u521b\u9020\u6027\u4e0e\u591a\u6837\u6027\uff0c\u8bc1\u660e\u6b64\u7c7b\u8bc4\u4f30\u5bfc\u5411\u7684\u5956\u52b1\u8bbe\u8ba1\u5bf9\u751f\u6210\u6027\u4efb\u52a1\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff1b\u672a\u6765\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u68cb\u7c7b\u96be\u9898\u4e0e\u66f4\u5e7f\u6cdb\u7684\u521b\u610f\u4efb\u52a1\u3002"}}
{"id": "2510.23633", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23633", "abs": "https://arxiv.org/abs/2510.23633", "authors": ["Xun Su", "Hiroyuki Kasai"], "title": "Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models", "comment": "9 pages", "summary": "Pretrained diffusion models have demonstrated strong capabilities in\nzero-shot inverse problem solving by incorporating observation information into\nthe generation process of the diffusion models. However, this presents an\ninherent dilemma: excessive integration can disrupt the generative process,\nwhile insufficient integration fails to emphasize the constraints imposed by\nthe inverse problem. To address this, we propose \\emph{Noise Combination\nSampling}, a novel method that synthesizes an optimal noise vector from a noise\nsubspace to approximate the measurement score, replacing the noise term in the\nstandard Denoising Diffusion Probabilistic Models process. This enables\nconditional information to be naturally embedded into the generation process\nwithout reliance on step-wise hyperparameter tuning. Our method can be applied\nto a wide range of inverse problem solvers, including image compression, and,\nparticularly when the number of generation steps $T$ is small, achieves\nsuperior performance with negligible computational overhead, significantly\nimproving robustness and stability.", "AI": {"tldr": "\u63d0\u51fa Noise Combination Sampling (NCS) \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u566a\u58f0\u5b50\u7a7a\u95f4\u5408\u6210\u6700\u4f18\u566a\u58f0\u5411\u91cf\u6765\u903c\u8fd1\u89c2\u6d4b\u5206\u6570\uff0c\u66ff\u4ee3\u6807\u51c6 DDPM \u7684\u566a\u58f0\u9879\uff0c\u4ece\u800c\u5728\u4e0d\u4f9d\u8d56\u5927\u91cf\u9010\u6b65\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u60c5\u51b5\u4e0b\u81ea\u7136\u5730\u5c06\u6761\u4ef6\u4fe1\u606f\u5d4c\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u5bf9\u5c11\u6b65\u751f\u6210\u5c24\u5176\u6709\u6548\u3002", "motivation": "\u89e3\u51b3\u5c06\u89c2\u6d4b\u4fe1\u606f\u6ce8\u5165\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u65f6\u7684\u4e24\u96be\uff1a\u82e5\u96c6\u6210\u8fc7\u591a\u4f1a\u6270\u4e71\u751f\u6210\uff0c\u82e5\u96c6\u6210\u4e0d\u8db3\u5219\u96be\u4ee5\u6ee1\u8db3\u9006\u95ee\u9898\u7684\u7ea6\u675f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u4e0d\u6270\u4e71\u751f\u6210\u8fc7\u7a0b\u7684\u524d\u63d0\u4e0b\u6709\u6548\u5229\u7528\u89c2\u6d4b\u4fe1\u606f\u7684\u65b9\u5f0f\uff0c\u63d0\u5347\u96f6-shot \u9006\u95ee\u9898\u6c42\u89e3\u7684\u9c81\u68d2\u6027\u4e0e\u7a33\u5b9a\u6027\u3002", "method": "Noise Combination Sampling \u901a\u8fc7\u5728\u566a\u58f0\u5b50\u7a7a\u95f4\u4e2d\u641c\u7d22\u5e76\u7ec4\u5408\u51fa\u4e00\u4e2a\u6700\u4f18\u7684\u566a\u58f0\u5411\u91cf\uff0c\u4ee5\u8fd1\u4f3c\u6d4b\u91cf\u5206\u6570\uff0c\u53d6\u4ee3\u6807\u51c6 DDPM \u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u9879\u3002\u8be5\u5411\u91cf\u7528\u4e8e\u6761\u4ef6\u4fe1\u606f\u7684\u5d4c\u5165\uff0c\u4f7f\u5f97\u4e0d\u9700\u5bf9\u6bcf\u4e00\u6b65\u8fdb\u884c\u8d85\u53c2\u6570\u5fae\u8c03\uff0c\u5373\u53ef\u5c06\u89c2\u6d4b\u4fe1\u606f\u6574\u5408\u5230\u751f\u6210\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5305\u62ec\u56fe\u50cf\u538b\u7f29\u5728\u5185\u7684\u591a\u79cd\u9006\u95ee\u9898\u6c42\u89e3\u5668\uff0c\u4e14\u5728\u751f\u6210\u6b65\u6570 T \u8f83\u5c0f\u65f6\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "result": "\u5728\u5c11\u6b65\u751f\u6210\u60c5\u5f62\u4e0b\uff0c\u663e\u793a\u51fa\u4f18\u4e8e\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8ba1\u7b97\u5f00\u9500\u8fd1\u4e4e\u65e0\u589e\u52a0\uff0c\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002\u5e94\u7528\u4e8e\u56fe\u50cf\u538b\u7f29\u7b49\u9006\u95ee\u9898\u65f6\u5177\u6709\u826f\u597d\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u8c03\u53c2\u3001\u4e14\u9c81\u68d2\u6027\u66f4\u5f3a\u7684\u5c06\u6761\u4ef6\u4fe1\u606f\u5d4c\u5165\u751f\u6210\u8fc7\u7a0b\u7684\u9014\u5f84\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u5c24\u5176\u5bf9\u4e8e\u5c11\u6b65\u6269\u6563\u7684\u9006\u95ee\u9898\u6c42\u89e3\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u672a\u6765\u53ef\u80fd\u62d3\u5c55\u5230\u66f4\u591a\u6269\u6563\u6a21\u578b\u4e0e\u4efb\u52a1\u573a\u666f\u3002"}}
{"id": "2510.23946", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23946", "abs": "https://arxiv.org/abs/2510.23946", "authors": ["Tananun Songdechakraiwut"], "title": "Leveraging LLMs for Early Alzheimer's Prediction", "comment": null, "summary": "We present a connectome-informed LLM framework that encodes dynamic fMRI\nconnectivity as temporal sequences, applies robust normalization, and maps\nthese data into a representation suitable for a frozen pre-trained LLM for\nclinical prediction. Applied to early Alzheimer's detection, our method\nachieves sensitive prediction with error rates well below clinically recognized\nmargins, with implications for timely Alzheimer's intervention.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8fde\u63a5\u7ec4\u4fe1\u606f\u9a71\u52a8\u7684LLM\u6846\u67b6\uff0c\u5c06\u52a8\u6001fMRI\u8fde\u63a5\u6027\u7f16\u7801\u4e3a\u65f6\u95f4\u5e8f\u5217\u5e76\u6620\u5c04\u5230\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3LLM\uff0c\u7528\u4e8e\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\uff0c\u8fbe\u5230\u4f4e\u8bef\u5dee\u7684\u9ad8\u7075\u654f\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u7684\u4fe1\u53f7\u590d\u6742\u6027\u4e0e\u6a21\u578b\u8fc1\u79fb\u74f6\u9888\uff0c\u5229\u7528\u5927\u8111\u8fde\u63a5\u7ec4\u7684\u65f6\u5e8f\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3LLM\u7684\u8868\u8fbe\u80fd\u529b\u5b9e\u73b0\u4e34\u5e8a\u9884\u6d4b\u3002", "method": "\u5c06\u52a8\u6001fMRI\u8fde\u63a5\u6027\u7f16\u7801\u4e3a\u65f6\u95f4\u5e8f\u5217\uff1b\u8fdb\u884c\u9c81\u68d2\u5f52\u4e00\u5316\u4ee5\u964d\u4f4e\u4e2a\u4f53\u5dee\u5f02\u4e0e\u566a\u58f0\uff1b\u5c06\u5e8f\u5217\u6620\u5c04\u5230\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3LLM\u7684\u8868\u793a\u7a7a\u95f4\u4ee5\u8fdb\u884c\u4e34\u5e8a\u9884\u6d4b\u3002", "result": "\u5728\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u68c0\u6d4b\u4efb\u52a1\u4e0a\u663e\u793a\u9ad8\u7075\u654f\u6027\u9884\u6d4b\uff0c\u8bef\u5dee\u7387\u663e\u8457\u4f4e\u4e8e\u4e34\u5e8a\u516c\u8ba4\u7684\u754c\u9650\uff0c\u663e\u793a\u51fa\u5bf9\u5e72\u9884\u65f6\u673a\u7684\u6f5c\u5728\u5e2e\u52a9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u793a\u5c06\u8fde\u63a5\u7ec4\u4fe1\u606f\u4e0eLLM\u7ed3\u5408\u7684\u6846\u67b6\u5177\u5907\u4e34\u5e8a\u53ef\u884c\u6027\u4e0e\u5e7f\u6cdb\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u795e\u7ecf\u75be\u75c5\u7684\u65e9\u671f\u9884\u6d4b\u3002"}}
{"id": "2510.23882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "AI": {"tldr": "\u7528\u6570\u5b57\u5b6a\u751f\u5c06\u7269\u7406\u3001\u6570\u636e\u9a71\u52a8\u4e0e\u6df7\u5408\u5efa\u6a21\u5e94\u7528\u4e8e\u6e29\u5ba4\u7cfb\u7edf\u7684\u52a8\u6001\u5efa\u6a21\u4e0e\u63a7\u5236\uff0c\u6bd4\u8f83\u56db\u79cd\u9884\u6d4b\u6a21\u578b\u5e76\u8bc4\u4f30\u4e09\u79cd\u63a7\u5236\u7b56\u7565\u5728\u63d2\u503c\u548c\u5916\u63a8\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5728 dynamical system \u7684\u5efa\u6a21\u4e0e\u63a7\u5236\u4e2d\u63a2\u7d22\u7269\u7406\u3001\u6570\u636e\u9a71\u52a8\u4e0e\u6df7\u5408\u65b9\u6cd5\u7684\u7ed3\u5408\uff0c\u4ee5\u53ca\u4e0d\u540c\u63a7\u5236\u7b56\u7565\u5728\u53ef\u9884\u6d4b\u6027\u3001\u9c81\u68d2\u6027\u548c\u5b9e\u73b0\u6210\u672c\u4e0a\u7684\u6743\u8861\uff0c\u4ee5\u63d0\u5347\u7cfb\u7edf\u7684\u9884\u6d4b\u4e0e\u63a7\u5236\u80fd\u529b\u3002", "method": "\u5b9e\u73b0\u5e76\u6bd4\u8f83 Linear\u3001Physics-Based Modeling (PBM)\u3001LSTM\u3001HAM \u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff1b\u5b9e\u73b0 Model Predictive Control (MPC)\u3001Reinforcement Learning (RL)\u3001\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLM) \u7684\u63a7\u5236\uff1b\u5728\u4e00\u4e2a\u5c0f\u578b\u6e29\u5ba4\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\u8fdb\u884c\u63d2\u503c\u4e0e\u5916\u63a8\u573a\u666f\u7684\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "HAM \u5728\u51c6\u786e\u6027\u3001\u6cdb\u5316\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\uff1bLSTM \u5728\u9ad8\u7cbe\u5ea6\u4f46\u8d44\u6e90\u6210\u672c\u8f83\u9ad8\uff1bMPC \u7a33\u5065\u3001\u53ef\u9884\u6d4b\uff0cRL \u5177\u6709\u8f83\u5f3a\u9002\u5e94\u6027\uff0cLLM \u63a7\u5236\u5728\u4e0e\u9884\u6d4b\u5de5\u5177\u8026\u5408\u65f6\u63d0\u4f9b\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u9a71\u52a8\u7684\u6df7\u5408\u5efa\u6a21\u4e0e\u591a\u6837\u5316\u63a7\u5236\u7b56\u7565\u80fd\u591f\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5b9e\u73b0\u6027\u80fd\u6298\u4e2d\u4e0e\u534f\u540c\u4f18\u5316\uff0cHAM+MPC/RL/LLM \u7684\u7ec4\u5408\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.23634", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23634", "abs": "https://arxiv.org/abs/2510.23634", "authors": ["Soutrik Sarangi", "Yonatan Sverdlov", "Nadav Dym", "Abir De"], "title": "Monotone and Separable Set Functions: Characterizations and Neural Models", "comment": null, "summary": "Motivated by applications for set containment problems, we consider the\nfollowing fundamental problem: can we design set-to-vector functions so that\nthe natural partial order on sets is preserved, namely $S\\subseteq T \\text{ if\nand only if } F(S)\\leq F(T) $. We call functions satisfying this property\nMonotone and Separating (MAS) set functions. % We establish lower and upper\nbounds for the vector dimension necessary to obtain MAS functions, as a\nfunction of the cardinality of the multisets and the underlying ground set. In\nthe important case of an infinite ground set, we show that MAS functions do not\nexist, but provide a model called our which provably enjoys a relaxed MAS\nproperty we name \"weakly MAS\" and is stable in the sense of Holder continuity.\nWe also show that MAS functions can be used to construct universal models that\nare monotone by construction and can approximate all monotone set functions.\nExperimentally, we consider a variety of set containment tasks. The experiments\nshow the benefit of using our our model, in comparison with standard set models\nwhich do not incorporate set containment as an inductive bias. Our code is\navailable in https://github.com/yonatansverdlov/Monotone-Embedding.", "AI": {"tldr": "\u7814\u7a76\u8bbe\u8ba1\u96c6\u5408\u5230\u5411\u91cf\u7684 Monotone and Separating (MAS) \u51fd\u6570\u4ee5\u4fdd\u6301\u96c6\u5408\u5305\u542b\u5173\u7cfb S \u2286 T \u5f53\u4e14\u4ec5\u5f53 F(S) \u2264 F(T)\u3002\u7ed9\u51fa\u5e95\u5c42\u7ef4\u5ea6\u7684\u4e0a\u4e0b\u754c\uff1b\u5728\u65e0\u9650\u5e95\u96c6\u65f6\u4e0d\u5b58\u5728\u4e25\u683c MAS\uff0c\u63d0\u51fa\u5f31 MAS \u6a21\u578b\u53ca\u5176 Holder \u8fde\u7eed\u6027\uff1b\u8bc1\u660e MAS \u53ef\u6784\u9020\u901a\u7528\u5355\u8c03\u8fd1\u4f3c\u6a21\u578b\u5e76\u5728\u591a\u79cd\u96c6\u5408\u5305\u542b\u4efb\u52a1\u4e2d\u83b7\u5f97\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4ee3\u7801\u516c\u5f00\u3002", "motivation": "\u4e3a\u96c6\u5408\u5305\u542b\u95ee\u9898\u63d0\u4f9b\u4e00\u4e2a\u80fd\u591f\u4fdd\u6301\u96c6\u5408\u504f\u5e8f\u7684\u5411\u91cf\u5316\u8868\u793a\u3002\u82e5 S \u2286 T\uff0c\u5219 F(S) \u2264 F(T)\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u8fd9\u6837\u7684 MAS \u51fd\u6570\u6709\u52a9\u4e8e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4ee5\u5411\u91cf\u5f62\u5f0f\u5904\u7406\u96c6\u5408\u6570\u636e\u5e76\u4fdd\u7559\u7ed3\u6784\u5173\u7cfb\u3002", "method": "\u63d0\u51fa MAS \u7684\u5b9a\u4e49\u5e76\u7814\u7a76\u5176\u7ef4\u5ea6\u9700\u6c42\uff0c\u7ed9\u51fa\u968f\u591a\u91cd\u96c6\u57fa\u6570\u548c\u5e95\u96c6\u5927\u5c0f\u7684\u4e0b\u754c\u4e0e\u4e0a\u754c\u3002\u8bc1\u660e\u5728\u65e0\u9650\u5e95\u96c6\u4e0a\u4e0d\u5b58\u5728\u4e25\u683c\u7684 MAS \u51fd\u6570\uff1b\u63d0\u51fa\u540d\u4e3a\u201cour\u201d\u7684\u6a21\u578b\u4ee5\u53ca\u4e00\u4e2a\u540d\u4e3a\u201cweakly MAS\u201d\u7684\u677e\u5f1b\u5c5e\u6027\uff0c\u5e76\u8bc1\u660e\u5176\u5728 Holder \u8fde\u7eed\u6027\u4e0b\u7684\u7a33\u5b9a\u6027\u3002\u5c55\u793a\u5982\u4f55\u5229\u7528 MAS \u6784\u9020\u80fd\u591f\u4ece\u5934\u5b9e\u73b0\u7684\u5355\u8c03\u901a\u7528\u6a21\u578b\uff0c\u5e76\u5177\u5907\u5bf9\u6240\u6709\u5355\u8c03\u96c6\u5408\u51fd\u6570\u7684\u8fd1\u4f3c\u80fd\u529b\uff1b\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u5305\u542b\u5173\u7cfb\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u4e0e\u4e0d\u5229\u7528\u96c6\u5408\u5305\u542b\u504f\u7f6e\u7684\u6807\u51c6\u96c6\u5408\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5f97\u5230\u5173\u4e8e\u9700\u8981\u7684\u5411\u91cf\u7ef4\u5ea6\u7684\u4e0a\u4e0b\u754c\uff1b\u65e0\u9650\u5e95\u96c6\u65f6\u4e0d\u5b58\u5728\u4e25\u683c\u7684 MAS\uff1b\u63d0\u51fa\u5f31 MAS \u6a21\u578b\u53ca Holder \u8fde\u7eed\u6027\u6027\u8d28\uff1b\u8868\u660e MAS \u6784\u9020\u7684\u901a\u7528\u5355\u8c03\u8fd1\u4f3c\u6a21\u578b\u5177\u5907\u826f\u597d\u7406\u8bba\u6027\u8d28\u4e0e\u5b9e\u8df5\u6548\u679c\uff1b\u5b9e\u9a8c\u663e\u793a\u5728\u96c6\u5408\u5305\u542b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff1b\u63d0\u4f9b\u516c\u5f00\u4ee3\u7801\u94fe\u63a5\u3002", "conclusion": "MAS \u51fd\u6570\u4e3a\u5904\u7406\u5355\u8c03\u96c6\u5408\u51fd\u6570\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u5c3d\u7ba1\u65e0\u9650\u5e95\u96c6\u60c5\u5f62\u9700\u8981\u653e\u5bbd\u4e3a\u5f31 MAS\uff0c\u4f46\u4ecd\u80fd\u901a\u8fc7\u5f31\u5316\u6027\u8d28\u83b7\u5f97\u7a33\u5b9a\u7684\u8fd1\u4f3c\u4e0e\u6cdb\u5316\u80fd\u529b\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8bc1\u5b9e\u4e86\u5176\u53ef\u884c\u6027\u4e0e\u5b9e\u7528\u6027\uff0c\u5e76\u9644\u5e26\u4ee3\u7801\u5b9e\u73b0\u3002"}}
{"id": "2510.23949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23949", "abs": "https://arxiv.org/abs/2510.23949", "authors": ["Kyomin Hwang", "Hyeonjin Kim", "Seungyeon Kim", "Sunghyun Wee", "Nojun Kwak"], "title": "Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs", "comment": null, "summary": "There have been a couple of studies showing that attempting to erase\nmultilingual knowledge using only English data is insufficient for multilingual\nLLMs. However, their analyses remain highly performance-oriented. In this\npaper, we switch the point of view to evaluation, and address an additional\nblind spot which reveals itself when the multilingual LLM is fully finetuned\nwith parallel multilingual dataset before unlearning. Here, language confusion\noccurs whereby a model responds in language different from that of the input\nprompt. Language confusion is a problematic phenomenon in unlearning, causing\nthe standard reference-based metrics to fail. We tackle this phenomenon in\nthree steps: (1) introduce N-gram-based Language-Mix (N-Mix) score to\nquantitatively show the language confusion is pervasive and consistent in\nmultilingual LLMs, (2) demonstrate that reference-based metrics result in false\nnegatives when N-Mix score is high, and(3) suggest the need of new type of\nunlearning evaluation that can directly assess the content of the generated\nsentences. We call this type of metrics as semantic-based metric.", "AI": {"tldr": "\u8bed\u8a00\u6df7\u6dc6\u5728\u591a\u8bed\u8a00LLMs\u7684\u53bb\u5b66\u4e60\u4e2d\u666e\u904d\u5b58\u5728\uff1b\u63d0\u51faN-\u6df7\u5408\u8bed\u8a00\u5206\u6570N-Mix\u63ed\u793a\u8be5\u95ee\u9898\uff0c\u4f20\u7edf\u57fa\u4e8e\u53c2\u8003\u7684\u8bc4\u4f30\u6613\u4ea7\u751f\u5047\u9634\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u4ee5\u8bed\u4e49\u5185\u5bb9\u4e3a\u5bfc\u5411\u7684\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u4ee5\u82f1\u8bed\u6570\u636e\u5bf9\u591a\u8bed\u8a00LLMs\u8fdb\u884c\u6027\u80fd\u5bfc\u5411\u7684\u8bc4\u4f30\uff0c\u5ffd\u89c6\u4e86\u53bb\u5b66\u4e60\u540e\u7684\u8bc4\u4f30\u76f2\u70b9\u3002\u5f53\u6a21\u578b\u5728\u5b8c\u5168\u7528\u5e76\u884c\u591a\u8bed\u8a00\u6570\u636e\u5fae\u8c03\u540e\u518d\u8fdb\u884c\u53bb\u5b66\u4e60\uff0c\u53ef\u80fd\u51fa\u73b0\u8f93\u5165\u8bed\u8a00\u4e0e\u8f93\u51fa\u8bed\u8a00\u4e0d\u4e00\u81f4\u7684\u8bed\u8a00\u6df7\u6dc6\uff0c\u5bfc\u81f4\u6807\u51c6\u53c2\u8003\u6307\u6807\u5931\u6548\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eN-gram\u7684Language-Mix\uff08N-Mix\uff09\u5206\u6570\uff0c\u7528\u4ee5\u5b9a\u91cf\u8861\u91cf\u8bed\u8a00\u6df7\u6dc6\u7684\u5e7f\u6cdb\u6027\u4e0e\u7a33\u5b9a\u6027\uff1b\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u8be5\u73b0\u8c61\u5728\u591a\u8bed\u8a00LLMs\u4e2d\u7684\u666e\u904d\u6027\uff1b\u5206\u6790\u5728N-Mix\u5206\u6570\u8f83\u9ad8\u65f6\u53c2\u8003\u57fa\u51c6\u7684\u5047\u9634\u6027\uff1b\u63d0\u51fa\u9700\u8981\u4e00\u79cd\u65b0\u7684\u53bb\u5b66\u4e60\u8bc4\u4f30\u7c7b\u578b\uff0c\u5373\u8bed\u4e49\u578b\u8bc4\u4f30\uff0c\u7528\u4ee5\u76f4\u63a5\u8bc4\u4f30\u751f\u6210\u53e5\u5b50\u7684\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8bed\u8a00\u6df7\u6dc6\u73b0\u8c61\u666e\u904d\u4e14\u7a33\u5b9a\u5b58\u5728\uff1b\u5f53N-Mix\u5206\u6570\u8f83\u9ad8\u65f6\uff0c\u5e38\u7528\u7684\u53c2\u8003\u57fa\u51c6\u4f1a\u4ea7\u751f\u5047\u9634\u6027\uff1b\u63d0\u51fa\u5e76\u8bba\u8bc1\u9700\u8981\u91c7\u7528\u8bed\u4e49\u6027\u8bc4\u4f30\u6765\u66f4\u51c6\u786e\u5730\u8861\u91cf\u53bb\u5b66\u4e60\u7684\u6548\u679c\u3002", "conclusion": "\u5e94\u5f15\u5165\u4ee5\u8bed\u4e49\u5185\u5bb9\u4e3a\u6838\u5fc3\u7684\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cf\u53bb\u5b66\u4e60\u7684\u6210\u6548\uff0c\u5173\u6ce8\u751f\u6210\u6587\u672c\u7684\u5b9e\u9645\u8bed\u4e49\u542b\u4e49\uff1bN-Mix\u4e3a\u91cf\u5316\u8bed\u8a00\u6df7\u6dc6\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u6548\u8def\u5f84\uff0c\u672a\u6765\u7814\u7a76\u5e94\u7ed3\u5408\u8bed\u4e49\u8bc4\u4f30\u63a8\u52a8\u591a\u8bed\u8a00LLMs\u7684\u53bb\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdb\u3002"}}
{"id": "2510.23635", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23635", "abs": "https://arxiv.org/abs/2510.23635", "authors": ["Andrea Bontempelli", "Matteo Busso", "Leonardo Javier Malcotti", "Fausto Giunchiglia"], "title": "Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning", "comment": null, "summary": "Any digital personal assistant, whether used to support task performance,\nanswer questions, or manage work and daily life, including fitness schedules,\nrequires high-quality annotations to function properly. However, user\nannotations, whether actively produced or inferred from context (e.g., data\nfrom smartphone sensors), are often subject to errors and noise. Previous\nresearch on Skeptical Learning (SKEL) addressed the issue of noisy labels by\ncomparing offline active annotations with passive data, allowing for an\nevaluation of annotation accuracy. However, this evaluation did not include\nconfirmation from end-users, the best judges of their own context. In this\nstudy, we evaluate SKEL's performance in real-world conditions with actual\nusers who can refine the input labels based on their current perspectives and\nneeds. The study involves university students using the iLog mobile application\non their devices over a period of four weeks. The results highlight the\nchallenges of finding the right balance between user effort and data quality,\nas well as the potential benefits of using SKEL, which include reduced\nannotation effort and improved quality of collected data.", "AI": {"tldr": "\u5728\u771f\u5b9e\u7528\u6237\u573a\u666f\u4e2d\u8bc4\u4f30 Skeptical Learning (SKEL) \u5bf9\u566a\u58f0\u6807\u6ce8\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5e73\u8861\u7528\u6237\u6295\u5165\u4e0e\u6570\u636e\u8d28\u91cf\u5177\u6709\u6311\u6218\uff0c\u4f46\u6f5c\u5728\u5730\u964d\u4f4e\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u63d0\u5347\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u6570\u5b57\u4e2a\u4eba\u52a9\u7406\u7684\u9ad8\u8d28\u91cf\u6ce8\u91ca\u5bf9\u529f\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6ce8\u91ca\u53ef\u80fd\u6765\u81ea\u4f20\u611f\u5668\u7b49\u4e0a\u4e0b\u6587\u800c\u4ea7\u751f\u566a\u58f0\uff0c\u9700\u7528\u6237\u786e\u8ba4\u3002\u5148\u524d\u7684 SKEL \u7814\u7a76\u4ec5\u6bd4\u8f83\u79bb\u7ebf\u6807\u6ce8\u4e0e\u88ab\u52a8\u6570\u636e\uff0c\u672a\u7eb3\u5165\u6700\u7ec8\u7528\u6237\u7684\u786e\u8ba4\uff0c\u56e0\u6b64\u9700\u8981\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "method": "\u5728\u56db\u5468\u65f6\u95f4\u5185\u8ba9\u5927\u5b66\u751f\u4f7f\u7528 iLog \u79fb\u52a8\u5e94\u7528\uff0c\u771f\u5b9e\u7528\u6237\u5bf9\u8f93\u5165\u6807\u7b7e\u8fdb\u884c\u4fee\u8ba2\u548c\u786e\u8ba4\uff0c\u4ee5\u8bc4\u4f30 SKEL \u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u63ed\u793a\u5728\u63d0\u5347\u6570\u636e\u8d28\u91cf\u4e0e\u964d\u4f4e\u6807\u6ce8\u5de5\u4f5c\u91cf\u4e4b\u95f4\u5b58\u5728\u6311\u6218\uff1b\u4f46 SKEL \u5c55\u73b0\u4e86\u964d\u4f4e\u6ce8\u91ca\u6210\u672c\u5e76\u63d0\u5347\u6570\u636e\u8d28\u91cf\u7684\u6f5c\u529b\u3002", "conclusion": "\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u8868\u660e SKEL \u5177\u6709\u6f5c\u5728\u4ef7\u503c\uff0c\u4f46\u9700\u5728\u7528\u6237\u8d1f\u62c5\u4e0e\u6570\u636e\u8d28\u91cf\u4e4b\u95f4\u5b9e\u73b0\u6743\u8861\uff0c\u5e76\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u8bbe\u8ba1\u6307\u5f15\u3002"}}
{"id": "2510.23995", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23995", "abs": "https://arxiv.org/abs/2510.23995", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Haochun Wang", "Bin Qin"], "title": "M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems", "comment": null, "summary": "Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing\nmedical question-answering systems through the integration of large language\nmodels (LLMs) with external medical literature. LLMs can retrieve relevant\nmedical articles to generate more professional responses efficiently. However,\ncurrent RAG applications still face problems. They generate incorrect\ninformation, such as hallucinations, and they fail to use external knowledge\ncorrectly. To solve these issues, we propose a new method named M-Eval. This\nmethod is inspired by the heterogeneity analysis approach used in\nEvidence-Based Medicine (EBM). Our approach can check for factual errors in RAG\nresponses using evidence from multiple sources. First, we extract additional\nmedical literature from external knowledge bases. Then, we retrieve the\nevidence documents generated by the RAG system. We use heterogeneity analysis\nto check whether the evidence supports different viewpoints in the response. In\naddition to verifying the accuracy of the response, we also assess the\nreliability of the evidence provided by the RAG system. Our method shows an\nimprovement of up to 23.31% accuracy across various LLMs. This work can help\ndetect errors in current RAG-based medical systems. It also makes the\napplications of LLMs more reliable and reduces diagnostic errors.", "AI": {"tldr": "\u63d0\u51fa M-Eval\uff0c\u7528\u8bc1\u636e\u5f02\u8d28\u6027\u5206\u6790\u6765\u9a8c\u8bc1\u57fa\u4e8eRAG\u7684\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u8bc1\u636e\u4e0e\u89c2\u70b9\u7684\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u68c0\u6d4b\u9519\u8bef\u4fe1\u606f\u4e0e\u63d0\u9ad8\u51c6\u786e\u6027\uff1b\u5728\u591a\u79cdLLM\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u63d0\u5347\u7ea623.31%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3RAG\u5728\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u5e7b\u89c9\u3001\u9519\u8bef\u4f7f\u7528\u5916\u90e8\u77e5\u8bc6\u7b49\u95ee\u9898\uff0c\u901a\u8fc7\u8de8\u6765\u6e90\u8bc1\u636e\u7684\u5f02\u8d28\u6027\u5206\u6790\u6765\u63d0\u9ad8\u56de\u7b54\u7684\u53ef\u9760\u6027\u3002", "method": "\u4ece\u5916\u90e8\u77e5\u8bc6\u5e93\u63d0\u53d6\u989d\u5916\u533b\u5b66\u6587\u732e\uff1b\u68c0\u7d22RAG\u7cfb\u7edf\u751f\u6210\u7684\u8bc1\u636e\u6587\u732e\uff1b\u5229\u7528\u5f02\u8d28\u6027\u5206\u6790\u68c0\u67e5\u8bc1\u636e\u662f\u5426\u652f\u6301\u56de\u7b54\u4e2d\u7684\u4e0d\u540c\u89c2\u70b9\uff0c\u5e76\u8bc4\u4f30\u8bc1\u636e\u7684\u53ef\u4fe1\u5ea6\u3002", "result": "\u5728\u591a\u79cdLLM\u4e0a\uff0c\u51c6\u786e\u7387\u63d0\u5347\u6700\u5927\u8fbe\u523023.31%\uff0c\u6709\u52a9\u4e8e\u53d1\u73b0\u548c\u7ea0\u6b63RAG\u533b\u5b66\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\uff0c\u63d0\u9ad8\u53ef\u9760\u6027\u5e76\u964d\u4f4e\u8bca\u65ad\u9519\u8bef\u3002", "conclusion": "M-Eval \u4e3a\u57fa\u4e8eRAG\u7684\u533b\u5b66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bc1\u636e\u6838\u9a8c\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6e90\u8bc1\u636e\u7684\u6bd4\u8f83\u4e0e\u53ef\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u63d0\u5347\u56de\u7b54\u7684\u6b63\u786e\u6027\u4e0e\u53ef\u4f9d\u8d56\u6027\u3002"}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u63a8\u65ad\u7684 LVLM \u94fe\u8def\u63a8\u7406\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u5956\u52b1\u548c\u591a\u6837\u6027\u63a2\u7d22\u4fc3\u8fdb\u591a\u6837\u4e14\u9ad8\u6982\u7387\u7684\u4e2d\u95f4\u63a8\u7406\uff0c\u540c\u65f6\u4ee5\u8fb9\u9645\u4f3c\u7136\u66ff\u4ee3\u4ee3\u4ef7\u6602\u8d35\u7684\u641c\u7d22\u4ee5\u6392\u540d\u63a8\u7406\u548c\u7b54\u6848\uff0c\u4ece\u800c\u63d0\u5347\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u6548\u679c\u3001\u6cdb\u5316\u548c\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3001\u7b56\u7565\u4f18\u5316\uff08PPO/GRPO\uff09\u5728\u672a\u89c1\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5e76\u4e14\u9ad8\u5ea6\u4f9d\u8d56\u504f\u5411\u7684\u5956\u52b1\u6a21\u578b\uff0c\u96be\u4ee5\u7a33\u5b9a\u5730\u63d0\u5347\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8d28\u91cf\u3002\u9700\u8981\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u4ee5\u540e\u9a8c\u63a8\u7406\u4e3a\u6838\u5fc3\u3001\u5e76\u80fd\u63d0\u9ad8\u63a8\u7406\u591a\u6837\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u5c06 LVLM \u7684\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u7406\uff0c\u91c7\u7528\u53cb\u597d\u7684\u8fd1\u4f3c\u7684\u53d8\u5206\u63a8\u65ad\uff08amortized variational inference\uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u5f15\u5165\u9762\u5411\u591a\u6837\u6027\u63a2\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u8bbe\u8ba1\u7a00\u758f\u7684\u3001\u9010\u8bcd\uff08token-level\uff09\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u9f13\u52b1\u591a\u6837\u4e14\u9ad8\u4f3c\u7136\u7684\u6f5c\u5728\u4e2d\u95f4\u63a8\u7406\uff08CoT\uff09\uff0c\u4ee5\u514b\u670d\u786e\u5b9a\u6027\u91c7\u6837\u548c\u5956\u52b1\u52ab\u6301\u95ee\u9898\u3002\u8fd8\u63d0\u51fa\u4e00\u79cd\u8d1d\u53f6\u65af\u63a8\u65ad\u5c3a\u5ea6\u5316\u7b56\u7565\uff0c\u7528\u8fb9\u9645\u4f3c\u7136\u6765\u66ff\u4ee3\u6210\u672c\u9ad8\u6602\u7684 Best-of-N \u548c Beam Search\uff0c\u4ee5\u9ad8\u6548\u5730\u6392\u5e8f\u6700\u4f18\u7684\u63a8\u7406\u8def\u5f84\u548c\u7b54\u6848\u3002", "result": "\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6709\u6548\u6027\u3001\u6cdb\u5316\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8d85\u8d8a\u73b0\u6709 LVLM \u57fa\u7ebf\uff0c\u8fbe\u5230\u66f4\u4f18\u79c0\u7684\u7efc\u5408\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u540e\u9a8c\u63a8\u7406\u5e76\u7ed3\u5408\u53d8\u5206\u8fd1\u4f3c\u3001\u7a00\u758f\u591a\u6837\u6027\u5956\u52b1\u4e0e\u8fb9\u9645\u4f3c\u7136\u6392\u5e8f\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u66f4\u9c81\u68d2\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347 LVLMs \u7684\u63a8\u7406\u80fd\u529b\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.23998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23998", "abs": "https://arxiv.org/abs/2510.23998", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Bin Qin"], "title": "PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine", "comment": null, "summary": "Evidence-based medicine (EBM) research has always been of paramount\nimportance. It is important to find appropriate medical theoretical support for\nthe needs from physicians or patients to reduce the occurrence of medical\naccidents. This process is often carried out by human querying relevant\nliterature databases, which lacks objectivity and efficiency. Therefore,\nresearchers utilize retrieval-augmented generation (RAG) to search for evidence\nand generate responses automatically. However, current RAG methods struggle to\nhandle complex queries in real-world clinical scenarios. For example, when\nqueries lack certain information or use imprecise language, the model may\nretrieve irrelevant evidence and generate unhelpful answers. To address this\nissue, we present the PICOs-RAG to expand the user queries into a better\nformat. Our method can expand and normalize the queries into professional ones\nand use the PICO format, a search strategy tool present in EBM, to extract the\nmost important information used for retrieval. This approach significantly\nenhances retrieval efficiency and relevance, resulting in up to an 8.8\\%\nimprovement compared to the baseline evaluated by our method. Thereby the\nPICOs-RAG improves the performance of the large language models into a helpful\nand reliable medical assistant in EBM.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u67e5\u8be2\u6269\u5c55\u5e76\u89c4\u8303\u5316\u4e3aPICO\u683c\u5f0f\uff0cPICOs-RAG\u663e\u8457\u63d0\u5347EBM\u68c0\u7d22\u7684\u6548\u7387\u4e0e\u76f8\u5173\u6027\uff0c\u7ea68.8%\u63d0\u5347\uff0c\u4f7fLLM\u6210\u4e3a\u66f4\u53ef\u9760\u7684\u533b\u7597\u52a9\u624b\u3002", "motivation": "EBM\u7814\u7a76\u9700\u8981\u533b\u751f\u548c\u60a3\u8005\u7684\u7406\u8bba\u652f\u6301\u4ee5\u51cf\u5c11\u533b\u7597\u4e8b\u6545\uff0c\u4f46\u4eba\u5de5\u67e5\u8be2\u8bc1\u636e\u7f3a\u4e4f\u5ba2\u89c2\u6027\u4e0e\u6548\u7387\u3002\u73b0\u6709RAG\u5728\u73b0\u5b9e\u4e34\u5e8a\u4e2d\u5bf9\u590d\u6742\u67e5\u8be2\u6548\u679c\u53d7\u9650\uff0c\u6613\u8fd4\u56de\u76f8\u5173\u6027\u4f4e\u7684\u8bc1\u636e\u6216\u7ed9\u51fa\u65e0\u7528\u56de\u7b54\u3002", "method": "\u63d0\u51faPICOs-RAG\uff0c\u5bf9\u7528\u6237\u67e5\u8be2\u8fdb\u884c\u6269\u5c55\u548c\u89c4\u8303\u5316\uff0c\u8f6c\u5316\u4e3a\u4e13\u4e1a\u5316\u7684PICO\u683c\u5f0f\u4ee5\u63d0\u53d6\u68c0\u7d22\u6240\u9700\u7684\u5173\u952e\u4fe1\u606f\uff0c\u63d0\u5347\u68c0\u7d22\u6548\u7387\u4e0e\u76f8\u5173\u6027\u3002", "result": "\u76f8\u8f83\u57fa\u7ebf\uff0c\u68c0\u7d22\u6548\u7387\u4e0e\u76f8\u5173\u6027\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff0c\u6700\u9ad8\u53ef\u8fbe8.8%\u7684\u6539\u8fdb\u3002", "conclusion": "PI COs-RAG\u63d0\u9ad8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728EBM\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u5176\u6210\u4e3a\u66f4\u6709\u5e2e\u52a9\u3001\u53ef\u9760\u7684\u533b\u5b66\u52a9\u624b\u3002"}}
{"id": "2510.23942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23942", "abs": "https://arxiv.org/abs/2510.23942", "authors": ["Sridhar Mahadevan"], "title": "Decentralized Causal Discovery using Judo Calculus", "comment": "54 pages", "summary": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u76f4\u89c9\u4e3b\u4e49\u7684\u53bb\u4e2d\u5fc3\u5316\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff08judo calculus\uff09\uff0c\u5728\u4e00\u4e2a sheaf \u7684\u62d3\u6251\u4e2d\u5b9e\u73b0\u5c40\u90e8\u771f\u7406\u7684\u56e0\u679c\u63a8\u65ad\uff0c\u901a\u8fc7 Lawvere-Tierney \u64cd\u4f5c\u7b26\u9009\u62e9\u76f8\u5173 regime\uff0c\u7ed3\u5408\u73b0\u6709\u7684 score/constraint/gradient \u65b9\u6cd5\uff0c\u5c55\u73b0\u5bf9\u751f\u7269\u3001\u7ecf\u6d4e\u7b49\u9886\u57df\u7684\u5b9e\u9a8c\u4f18\u52bf\uff0c\u63d0\u5347\u6548\u7387\u4e0e\u6027\u80fd\u3002", "motivation": "\u56e0\u679c\u6548\u5e94\u5f80\u5f80\u53d7\u60c5\u5883/\u5236\u5ea6\u56e0\u7d20\u5f71\u54cd\uff08\u5982\u5e74\u9f84\u3001\u56fd\u5bb6\u3001\u5242\u91cf\u3001\u57fa\u56e0\u578b\u3001\u5b9e\u9a8c\u5ba4\u534f\u8bae\u7b49\uff09\uff0c\u9700\u8981\u5728\u4e0d\u540c regime \u4e0b\u8fdb\u884c\u5c40\u90e8\u4e14\u4e00\u81f4\u7684\u63a8\u65ad\u3002\u4f20\u7edf\u5168\u5c40\u5047\u8bbe\u65b9\u6cd5\u53ef\u80fd\u96be\u4ee5\u5904\u7406\u60c5\u5883\u4f9d\u8d56\u4e0e\u5927\u89c4\u6a21\u6570\u636e\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\uff0c\u56e0\u800c\u9700\u8981\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u5c40\u90e8\u5316\u7684\u6846\u67b6\u6765\u5b9e\u73b0\u56e0\u679c\u53d1\u73b0\u3002", "method": "\u5728 topos of sheaves \u6846\u67b6\u4e2d\u5f62\u5f0f\u5316 judo calculus\uff0c\u4f7f\u7528 Lawvere\u2013Tierney \u6a21\u6001\u7b97\u5b50 j \u6765\u9009\u62e9\u76f8\u5173 regime\uff0c\u4f7f\u56e0\u679c\u65ad\u8a00\u5728\u4e00\u4e2a regime \u8986\u76d6\u65cf\u4e0a\u6210\u7acb\uff08j-stability \u4e3a\u6784\u9020\u6027\u4e14\u4e00\u81f4\u7684\u5c40\u90e8\u771f\u7406\uff09\u3002\u6784\u5efa\u4e00\u4e2a\u7b97\u6cd5\u5b9e\u73b0\u6846\u67b6\uff0c\u5c06 j-do-calculus \u4e0e\u73b0\u6709\u7684 score-based\u3001constraint-based\u3001gradient-based \u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u6574\u5408\uff0c\u5e76\u4ee5\u53bb\u4e2d\u5fc3\u5316\u65b9\u5f0f\u8fdb\u884c\u8ba1\u7b97\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff08\u751f\u7269\u5b66\u3001\u7ecf\u6d4e\u5b66\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u53bb\u4e2d\u5fc3\u5316\u7684 sheaf-\u7406\u8bba\u56e0\u679c\u53d1\u73b0\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5bf9\u6bd4\u7ecf\u5178\u65b9\u6cd5\u5448\u73b0\u66f4\u597d\u7684\u56e0\u679c\u53d1\u73b0\u6027\u80fd\u3002", "conclusion": "\u63d0\u4f9b\u4e00\u4e2a\u7406\u8bba\u4e0e\u5b9e\u73b0\u5e76\u5b58\u7684\u5c40\u90e8 regime \u4e0b\u56e0\u679c\u63a8\u65ad\u6846\u67b6\uff0c\u901a\u8fc7 j-stability \u4e0e j-do-calculus \u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u7684\u76f4\u89c9\u4e3b\u4e49\u56e0\u679c\u53d1\u73b0\uff0c\u517c\u5bb9\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u5728\u60c5\u5883\u4f9d\u8d56\u6027\u573a\u666f\u4e2d\u7684\u6027\u80fd\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.23637", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.23637", "abs": "https://arxiv.org/abs/2510.23637", "authors": ["Job Petrov\u010di\u010d", "David Eliecer Narvaez Denis", "Ljup\u010do Todorovski"], "title": "Combining Textual and Structural Information for Premise Selection in Lean", "comment": null, "summary": "Premise selection is a key bottleneck for scaling theorem proving in large\nformal libraries. Yet existing language-based methods often treat premises in\nisolation, ignoring the web of dependencies that connects them. We present a\ngraph-augmented approach that combines dense text embeddings of Lean\nformalizations with graph neural networks over a heterogeneous dependency graph\ncapturing both state--premise and premise--premise relations. On the LeanDojo\nBenchmark, our method outperforms the ReProver language-based baseline by over\n25% across standard retrieval metrics. These results demonstrate the power of\nrelational information for more effective premise selection.", "AI": {"tldr": "Graph-augmented premise selection that fuses dense text embeddings with a heterogeneous dependency graph improves LeanDojo retrieval; surpasses ReProver baseline by >25%.", "motivation": "Premise selection is a bottleneck in scaling theorem proving; existing language-based methods treat premises in isolation and neglect dependencies among premises.", "method": "Construct a heterogeneous dependency graph encoding state-premise and premise-premise relations; compute dense embeddings of Lean formalizations; apply graph neural networks over the graph to guide premise selection.", "result": "On the LeanDojo Benchmark, the proposed method outperforms the ReProver language-based baseline by over 25% across standard retrieval metrics.", "conclusion": "Relational and structural information captured by a graph, when fused with dense text representations, significantly improves premise selection and can enhance scalability of automated theorem proving."}}
{"id": "2510.24003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24003", "abs": "https://arxiv.org/abs/2510.24003", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Haochun Wang", "Bin Qin"], "title": "META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine", "comment": null, "summary": "Evidence-based medicine (EBM) holds a crucial role in clinical application.\nGiven suitable medical articles, doctors effectively reduce the incidence of\nmisdiagnoses. Researchers find it efficient to use large language models (LLMs)\ntechniques like RAG for EBM tasks. However, the EBM maintains stringent\nrequirements for evidence, and RAG applications in EBM struggle to efficiently\ndistinguish high-quality evidence. Therefore, inspired by the meta-analysis\nused in EBM, we provide a new method to re-rank and filter the medical\nevidence. This method presents multiple principles to filter the best evidence\nfor LLMs to diagnose. We employ a combination of several EBM methods to emulate\nthe meta-analysis, which includes reliability analysis, heterogeneity analysis,\nand extrapolation analysis. These processes allow the users to retrieve the\nbest medical evidence for the LLMs. Ultimately, we evaluate these high-quality\narticles and show an accuracy improvement of up to 11.4% in our experiments and\nresults. Our method successfully enables RAG to extract higher-quality and more\nreliable evidence from the PubMed dataset. This work can reduce the infusion of\nincorrect knowledge into responses and help users receive more effective\nreplies.", "AI": {"tldr": "\u57fa\u4e8e\u5143\u5206\u6790\u7406\u5ff5\u7684\u8bc1\u636e\u518d\u6392\u5e8f\u4e0e\u7b5b\u9009\u65b9\u6cd5\uff0c\u63d0\u5347LLM\u5728RAG\u4e2d\u5bf9\u9ad8\u8d28\u91cf\u533b\u5b66\u8bc1\u636e\u7684\u5229\u7528\uff1b\u5728PubMed\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe11.4%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "EBM\u5bf9\u8bc1\u636e\u8d28\u91cf\u8981\u6c42\u4e25\u683c\uff0cRAG\u5728EBM\u573a\u666f\u4e2d\u96be\u4ee5\u533a\u5206\u9ad8\u8d28\u91cf\u8bc1\u636e\uff0c\u6613\u5c06\u9519\u8bef\u4fe1\u606f\u6ce8\u5165\u6a21\u578b\u56de\u7b54\u3002\u9700\u8981\u4e00\u4e2a\u7cfb\u7edf\u5316\u3001\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7b5b\u9009\u4e0e\u518d\u6392\u5e8f\u673a\u5236\u3002", "method": "\u501f\u9274\u5143\u5206\u6790\u7684\u539f\u5219\uff0c\u7ed3\u5408\u53ef\u9760\u6027\u5206\u6790\u3001\u5f02\u8d28\u6027\u5206\u6790\u548c\u5916\u63a8\u5206\u6790\uff0c\u5bf9PubMed\u7b49\u6765\u6e90\u7684\u8bc1\u636e\u8fdb\u884c\u8bc4\u4f30\u3001\u6392\u5e8f\u4e0e\u7b5b\u9009\uff0c\u5c06\u9ad8\u8d28\u91cf\u8bc1\u636e\u63d0\u4f9b\u7ed9LLMs\u7528\u4e8e\u8bca\u65ad\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u901a\u8fc7\u7b5b\u9009\u540e\u7684\u8bc1\u636e\uff0c\u6a21\u578b\u51c6\u786e\u6027\u63d0\u5347\u6700\u591a11.4%\uff1b\u8bc1\u660e\u6240\u63d0\u65b9\u6cd5\u80fd\u5e2e\u52a9RAG\u4ecePubMed\u63d0\u53d6\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u52a0\u53ef\u9760\u7684\u8bc1\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u964d\u4f4e\u9519\u8bef\u77e5\u8bc6\u6ce8\u5165\uff0c\u63d0\u5347\u57fa\u4e8e\u8bc1\u636e\u7684\u56de\u7b54\u8d28\u91cf\uff0c\u4e3aEBM\u4efb\u52a1\u4e2d\u7684LLM\u5e94\u7528\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u8bc1\u636e\u57fa\u7840\u3002"}}
{"id": "2510.23965", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23965", "abs": "https://arxiv.org/abs/2510.23965", "authors": ["Aymane El Gadarri", "Ali Aouad", "Vivek F. Farias"], "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity", "comment": null, "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a sign estimator \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u805a\u5408\u6b65\u9aa4\u4e2d\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u53d6\u4ee3\u4ea4\u53c9\u71b5\uff0c\u83b7\u5f97\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u5e8f\u5217\u6392\u5e8f\u5bf9\u9f50\u4f30\u8ba1\uff1b\u5728\u73b0\u5b9e\u6a21\u62df\u4e2d\u663e\u8457\u964d\u4f4e\u504f\u597d distortion\uff0c\u4e0e\u6807\u51c6 RLHF \u76f8\u6bd4\u63d0\u5347\u7a33\u5065\u6027\u4e0e\u7b80\u4fbf\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u5728\u9762\u5bf9\u4eba\u7c7b\u504f\u597d\u5f02\u8d28\u6027\u65f6\u4f1a\u4ea7\u751f\u504f\u5dee\uff0c\u96be\u4ee5\u83b7\u5f97\u4eba\u7fa4\u5c42\u9762\u7684\u6548\u7528\uff08\u793e\u4f1a\u798f\u5229\uff09\u7684\u4e00\u81f4\u4f30\u8ba1\uff0c\u5bfc\u81f4\u5bf9\u9f50\u7ed3\u679c\u53d7\u4e2a\u4f53\u5dee\u5f02\u5f71\u54cd\u663e\u8457\u3002", "method": "\u628a\u5bf9\u6210\u5bf9\u6bd4\u8f83\u6570\u636e\u7684\u805a\u5408\u4ece\u4ea4\u53c9\u71b5\u6539\u4e3a\u4e8c\u5143\u5206\u7c7b\u635f\u5931\uff0c\u63d0\u51fa sign estimator\uff1b\u5728\u8f83\u5f31\u5047\u8bbe\u4e0b\u5b9e\u73b0\u4e00\u81f4\u6027\uff0c\u7ed9\u51fa\u591a\u9879\u5f0f\u9636\u7684\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\uff1b\u5728\u6570\u5b57\u5b6a\u751f\u7b49\u73b0\u5b9e\u4eff\u771f\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u5305\u542b\u591a\u4eba\u7269\u8bbe\u7684\u6570\u5b57\u5b6a\u751f\u4eff\u771f\u7684\u60c5\u666f\u4e0b\uff0csign estimator \u5c06\u504f\u597d\u626d\u66f2\u663e\u8457\u964d\u4f4e\uff0c\u89d2\u5ea6\u8bef\u5dee\u7ea6\u4e0b\u964d35%\uff0c\u5bf9\u771f\u5b9e\u4eba\u7fa4\u504f\u597d\u7684\u4e00\u81f4\u6027\u4ece12%\u964d\u81f38%\uff1b\u76f8\u8f83\u4e8e\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u5f02\u8d28\u6027\u7684\u9762\u677f\u6570\u636e\u542f\u53d1\u5f0f\uff0c\u4fdd\u6301\u5b9e\u73b0\u7b80\u5355\u6027\u5e76\u5177\u6709\u66f4\u5f3a\u7a33\u5065\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u7b80\u4fbf\u7684\u524d\u63d0\u4e0b\u63d0\u4f9b\u4e00\u81f4\u7684\u5e8f\u5217\u5bf9\u9f50\u4f30\u8ba1\uff0c\u4e0e\u73b0\u6709 RLHF \u7b49\u65b9\u6cd5\u76f8\u6bd4\u5bf9\u5f02\u8d28\u6027\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u5e76\u7ed9\u51fa\u7b2c\u4e00\u6279\u591a\u9879\u5f0f\u7ea7\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u3002"}}
{"id": "2510.23639", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.23639", "abs": "https://arxiv.org/abs/2510.23639", "authors": ["Jonathan Amar", "Edward Liu", "Alessandra Breschi", "Liangliang Zhang", "Pouya Kheradpour", "Sylvia Li", "Lisa Soleymani Lehmann", "Alessandro Giulianelli", "Matt Edwards", "Yugang Jia", "David Nola", "Raghav Mani", "Pankaj Vats", "Jesse Tetreault", "T. J. Chen", "Cory Y. McLean"], "title": "Integrating Genomics into Multimodal EHR Foundation Models", "comment": null, "summary": "This paper introduces an innovative Electronic Health Record (EHR) foundation\nmodel that integrates Polygenic Risk Scores (PRS) as a foundational data\nmodality, moving beyond traditional EHR-only approaches to build more holistic\nhealth profiles. Leveraging the extensive and diverse data from the All of Us\n(AoU) Research Program, this multimodal framework aims to learn complex\nrelationships between clinical data and genetic predispositions. The\nmethodology extends advancements in generative AI to the EHR foundation model\nspace, enhancing predictive capabilities and interpretability. Evaluation on\nAoU data demonstrates the model's predictive value for the onset of various\nconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay\nbetween PRS and EHR data. The work also explores transfer learning for custom\nclassification tasks, showcasing the architecture's versatility and efficiency.\nThis approach is pivotal for unlocking new insights into disease prediction,\nproactive health management, risk stratification, and personalized treatment\nstrategies, laying the groundwork for more personalized, equitable, and\nactionable real-world evidence generation in healthcare.", "AI": {"tldr": "\u5c06\u57fa\u4e8e\u591a\u6a21\u6001\u7684EHR foundation model\u4e0e\u591a\u57fa\u56e0\u98ce\u9669\u8bc4\u5206\uff08PRS\uff09\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u75be\u75c5\u53d1\u75c5\u7684\u9884\u6d4b\uff0c\u7279\u522b\u662f2\u578b\u7cd6\u5c3f\u75c5\uff0c\u5e76\u63a2\u7d22\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b66\u4e60\u7684\u6f5c\u529b\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u7684EHR\u65b9\u6cd5\u591a\u4ee5\u8868\u578b\u6570\u636e\u4e3a\u4e3b\uff0c\u5ffd\u89c6\u9057\u4f20 predisposition \u7684\u4fe1\u606f\u3002\u901a\u8fc7\u5c06PRS\u4f5c\u4e3a\u57fa\u7840\u6a21\u6001\u5e76\u4e0eEHR\u6570\u636e\u5728All of Us\u7814\u7a76\u4eba\u7fa4\u4e2d\u8054\u5408\u5efa\u6a21\uff0c\u53ef\u6784\u5efa\u66f4\u5168\u9762\u7684\u5065\u5eb7\u753b\u50cf\uff0c\u63d0\u5347\u9884\u6d4b\u80fd\u529b\u3001\u98ce\u9669\u5206\u5c42\u4e0e\u4e2a\u6027\u5316\u5e72\u9884\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u63a8\u52a8\u516c\u5e73\u6027\u4e0e\u5b9e\u8bc1\u8bc1\u636e\u7684\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u6001EHR foundation model\uff0c\u5c06PRS\u6574\u5408\u4e3a\u57fa\u7840\u6570\u636e\u6a21\u6001\uff0c\u5229\u7528All of Us\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002\u65b9\u6cd5\u5728\u751f\u6210\u5f0fAI\u7684\u6846\u67b6\u4e0b\u62d3\u5c55EHR\u57fa\u7840\u6a21\u578b\uff0c\u5173\u6ce8\u9884\u6d4b\u80fd\u529b\u4e0e\u53ef\u89e3\u91ca\u6027\u3002\u5305\u62ec\u5bf9\u4e0d\u540c\u75be\u75c5\u53d1\u75c5\u7684\u9884\u6d4b\u3001\u5c24\u4ee5T2D\u4e3a\u793a\u4f8b\uff0c\u4ee5\u53ca\u63a2\u7d22\u5bf9\u4e0b\u6e38\u81ea\u5b9a\u4e49\u5206\u7c7b\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5728AoU\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u8be5\u6a21\u578b\u5bf9\u591a\u79cd\u75be\u75c5\u7684\u53d1\u75c5\u9884\u6d4b\u5177\u6709\u663e\u8457\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5bf9Type 2 Diabetes\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u63ed\u793aPRS\u4e0eEHR\u6570\u636e\u4e4b\u95f4\u7684\u4ea4\u4e92\u5173\u7cfb\u3002\u8fd8\u5c55\u793a\u4e86\u5bf9\u81ea\u5b9a\u4e49\u5206\u7c7b\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\uff0c\u4f53\u73b0\u67b6\u6784\u7684\u591a\u6837\u6027\u4e0e\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u63ed\u793a\u75be\u75c5\u9884\u6d4b\u7684\u65b0\u6d1e\u89c1\u3001\u63a8\u52a8\u4e3b\u52a8\u5065\u5eb7\u7ba1\u7406\u4e0e\u98ce\u9669\u5206\u5c42\uff0c\u4fc3\u8fdb\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\u7684\u5b9e\u73b0\uff0c\u4e3a\u5728\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\uff08RWE\uff09\u751f\u6210\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7a0b\u5ea6\u7684\u4e2a\u6027\u5316\u3001\u516c\u5e73\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.24014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24014", "abs": "https://arxiv.org/abs/2510.24014", "authors": ["Yizhu Jiao", "Sha Li", "Sizhe Zhou", "Heng Ji", "Jiawei Han"], "title": "TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents", "comment": "ACL 2025. Source code: https://github.com/yzjiao/Text2DB", "summary": "The task of information extraction (IE) is to extract structured knowledge\nfrom text. However, it is often not straightforward to utilize IE output due to\nthe mismatch between the IE ontology and the downstream application needs. We\npropose a new formulation of IE TEXT2DB that emphasizes the integration of IE\noutput and the target database (or knowledge base). Given a user instruction, a\ndocument set, and a database, our task requires the model to update the\ndatabase with values from the document set to satisfy the user instruction.\nThis task requires understanding user instructions for what to extract and\nadapting to the given DB/KB schema for how to extract on the fly. To evaluate\nthis new task, we introduce a new benchmark featuring common demands such as\ndata infilling, row population, and column addition. In addition, we propose an\nLLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer\ncomponent that interacts with the database, the Planner component that\ngenerates a code-based plan with calls to IE models, and the Analyzer component\nthat provides feedback regarding code quality before execution. Experiments\nshow that OPAL can successfully adapt to diverse database schemas by generating\ndifferent code plans and calling the required IE models. We also highlight\ndifficult cases such as dealing with large databases with complex dependencies\nand extraction hallucination, which we believe deserve further investigation.\nSource code: https://github.com/yzjiao/Text2DB", "AI": {"tldr": "\u63d0\u51fa\u6587\u672c\u4fe1\u606f\u62bd\u53d6\uff08IE\uff09\u4e0e\u76ee\u6807\u6570\u636e\u5e93/\u77e5\u8bc6\u5e93\u5bf9\u9f50\u7684\u65b0\u4efb\u52a1 TEXT2DB\uff0c\u8981\u6c42\u6a21\u578b\u5728\u7ed9\u5b9a\u7528\u6237\u6307\u4ee4\u3001\u6587\u6863\u96c6\u5408\u548c\u6570\u636e\u5e93\u7684\u60c5\u51b5\u4e0b\u66f4\u65b0\u6570\u636e\u5e93\u4ee5\u6ee1\u8db3\u6307\u4ee4\u3002\u901a\u8fc7 OPAL \u6846\u67b6\uff08Observe-Plan-Analyze LLM\uff09\u5b9e\u73b0\u5bf9\u6570\u636e\u5e93\u7684\u4ea4\u4e92\u3001\u57fa\u4e8e\u4ee3\u7801\u7684\u8ba1\u5212\u53ca\u8d28\u91cf\u5206\u6790\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u6570\u636e\u5e93\u67b6\u6784\uff0c\u4f46\u5728\u5927\u6570\u636e\u5e93\u548c\u63d0\u53d6\u5e7b\u89c9\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u5e76\u63d0\u4f9b\u5f00\u6e90\u5b9e\u73b0\u3002", "motivation": "IE \u7684\u8f93\u51fa\u5f80\u5f80\u4e0e\u4e0b\u6e38\u5e94\u7528\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u7f3a\u4e4f\u5bf9\u6570\u636e\u5e93\u67b6\u6784\u7684\u81ea\u9002\u5e94\u5bf9\u9f50\u3002\u672c\u6587\u63d0\u51fa\u5c06 IE \u7ed3\u679c\u4e0e\u76ee\u6807\u6570\u636e\u5e93\u7ed3\u5408\uff0c\u6309\u7528\u6237\u6307\u4ee4\u548c\u6570\u636e\u5e93\u6a21\u5f0f\u52a8\u6001\u62bd\u53d6\u4e0e\u586b\u5145\uff0c\u4ece\u800c\u63d0\u5347\u8de8\u4efb\u52a1\u7684\u53ef\u7528\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5b9a\u4e49 TEXT2DB \u95ee\u9898\u5e76\u5f15\u5165\u65b0\u57fa\u51c6\uff0c\u6db5\u76d6\u6570\u636e\u586b\u5145\u3001\u884c\u586b\u5145\u3001\u5217\u6dfb\u52a0\u7b49\u573a\u666f\u3002\u63d0\u51fa OPAL \u6846\u67b6\uff1aObserver \u4e0e\u6570\u636e\u5e93\u4ea4\u4e92\u3001Planner \u751f\u6210\u57fa\u4e8e\u4ee3\u7801\u7684\u6267\u884c\u8ba1\u5212\uff08\u8c03\u7528 IE \u6a21\u578b\uff09\u3001Analyzer \u63d0\u4f9b\u4ee3\u7801\u8d28\u91cf\u53cd\u9988\u540e\u518d\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e OPAL \u80fd\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u5e93\u6a21\u5f0f\u751f\u6210\u4e0d\u540c\u7684\u4ee3\u7801\u8ba1\u5212\u5e76\u8c03\u7528\u6240\u9700\u7684 IE \u6a21\u578b\uff0c\u5177\u5907\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u5e93\u548c\u590d\u6742\u4f9d\u8d56\u7684\u9002\u5e94\u6027\uff0c\u4f46\u4ecd\u5b58\u5728\u63d0\u53d6\u5e7b\u89c9\u7b49\u56f0\u96be\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "TEXT2DB \u4e0e OPAL \u5171\u540c\u6784\u5efa\u4e86 IE \u4e0e\u6570\u636e\u5e93\u5bf9\u9f50\u7684\u65b0\u8303\u5f0f\uff0c\u63d0\u5347\u4e86\u5728\u591a\u6837\u6570\u636e\u5e93\u4e0a\u7684\u53ef\u8fc1\u79fb\u6027\uff1b\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u63d0\u5347\u5bf9\u5927\u6570\u636e\u5e93\u7684\u6269\u5c55\u6027\u3001\u964d\u4f4e\u5e7b\u89c9\u3001\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u7b49\u3002"}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "A conditioned deep learning model incorporating individuals' social infrastructure resilience (SIR) and local spatial context improves prediction of post-disruption movement patterns from large-scale, sparse data.", "motivation": "Predicting movement shifts after disruptive events is challenging due to (1) lack of measures for heterogeneous SIR, (2) limited features like sociodemographics, (3) complex interactions between movement and spatial context, and (4) sparse, irregular data.", "method": "Introduce a conditioned deep learning model that uses individuals' SIR as conditioning input to capture non-linear relationships between movement patterns and local spatial context, applied to large-scale, sparse individual-level data.", "result": "Experiments show that incorporating SIR and spatial context enhances the model's ability to predict post-event movement patterns; the conditioned model captures divergent shifts among individuals with similar pre-event patterns but differing SIR.", "conclusion": "Integrating SIR with spatial context in a conditioned model improves post-disruption movement predictions and reveals heterogeneity in responses linked to resilience differences."}}
{"id": "2510.23640", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23640", "abs": "https://arxiv.org/abs/2510.23640", "authors": ["Zihao Jing", "Yan Sun", "Yan Yi Li", "Sugitha Janarthanan", "Alana Deng", "Pingzhao Hu"], "title": "Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning", "comment": "Accepted by NeurIPS 2025", "summary": "Multimodal molecular models often suffer from 3D conformer unreliability and\nmodality collapse, limiting their robustness and generalization. We propose\nMuMo, a structured multimodal fusion framework that addresses these challenges\nin molecular representation through two key strategies. To reduce the\ninstability of conformer-dependent fusion, we design a Structured Fusion\nPipeline (SFP) that combines 2D topology and 3D geometry into a unified and\nstable structural prior. To mitigate modality collapse caused by naive fusion,\nwe introduce a Progressive Injection (PI) mechanism that asymmetrically\nintegrates this prior into the sequence stream, preserving modality-specific\nmodeling while enabling cross-modal enrichment. Built on a state space\nbackbone, MuMo supports long-range dependency modeling and robust information\npropagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and\nMoleculeNet, MuMo achieves an average improvement of 2.7% over the\nbest-performing baseline on each task, ranking first on 22 of them, including a\n27% improvement on the LD50 task. These results validate its robustness to 3D\nconformer noise and the effectiveness of multimodal fusion in molecular\nrepresentation. The code is available at: github.com/selmiss/MuMo.", "AI": {"tldr": "MuMo\u63d0\u51fa\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u5b50\u8868\u793a\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u878d\u5408\u7ba1\u7ebf\uff08SFP\uff09\u5c062D\u62d3\u6251\u4e0e3D\u51e0\u4f55\u7edf\u4e00\u4e3a\u7ed3\u6784\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u6e10\u8fdb\u6ce8\u5165\uff08PI\uff09\u5c06\u8be5\u5148\u9a8c\u5f15\u5165\u5e8f\u5217\u6d41\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u4e0e\u8de8\u6a21\u6001\u4e92\u8865\u3002", "motivation": "\u73b0\u6709\u7684\u4e09\u7ef4\u6784\u8c61\u4e0d\u7a33\u5b9a\u548c\u6a21\u6001\u878d\u5408\u7684\u7b80\u5355\u5316\u4f1a\u5bfc\u81f4\u591a\u6a21\u6001\u5206\u5b50\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4e0b\u964d\u4e0e\u6cdb\u5316\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u4e2a\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u878d\u5408\u65b9\u6848\u3002", "method": "\u63d0\u51faStructured Fusion Pipeline\uff08SFP\uff09\u5c062D\u62d3\u6251\u4e0e3D\u51e0\u4f55\u6574\u5408\u4e3a\u7edf\u4e00\u7684\u7ed3\u6784\u5148\u9a8c\uff1b\u5f15\u5165Progressive Injection\uff08PI\uff09\u673a\u5236\u5bf9\u8be5\u5148\u9a8c\u8fdb\u884c\u4e0d\u5bf9\u79f0\u6ce8\u5165\uff0c\u4fdd\u6301\u5404\u6a21\u6001\u7684\u7279\u5b9a\u5efa\u6a21\u540c\u65f6\u5b9e\u73b0\u8de8\u6a21\u6001\u4e30\u5bcc\u3002\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4(backbone)\u5b9e\u73b0\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u4e0e\u4fe1\u606f\u4f20\u9012\u3002", "result": "\u5728\u6765\u81eaTherapeutics Data Commons (TDC) \u4e0e MoleculeNet \u768429\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cMuMo\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u7684\u5e73\u5747\u63d0\u5347\u4e3a2.7%\uff0c\u572822\u4e2a\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u4e00\uff1bLD50\u4efb\u52a1\u63d0\u5347\u8fbe27%\u3002\u8868\u660e\u5bf93D\u6784\u8c61\u566a\u58f0\u7684\u9c81\u68d2\u6027\u53ca\u591a\u6a21\u6001\u878d\u5408\u5728\u5206\u5b50\u8868\u793a\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MuMo\u901a\u8fc7\u7ed3\u6784\u5316\u878d\u5408\u4e0e\u6e10\u8fdb\u6ce8\u5165\u5b9e\u73b0\u7a33\u5b9a\u7684\u591a\u6a21\u6001\u5206\u5b50\u8868\u793a\uff0c\u5728\u591a\u4efb\u52a1\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5bf9\u672a\u6765\u7684\u9c81\u68d2\u5206\u5b50\u8868\u5f81\u5177\u6709\u53c2\u8003\u4ef7\u503c\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.24020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24020", "abs": "https://arxiv.org/abs/2510.24020", "authors": ["Hao An", "Yang Xu"], "title": "Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward", "comment": "23pages, 4figures", "summary": "Mitigating hallucinations in Large Language Models (LLMs) is critical for\ntheir reliable deployment. Existing methods typically fine-tune LLMs to abstain\nfrom answering questions beyond their knowledge scope. However, these methods\noften rely on coarse-grained signals to guide LLMs to abstain, such as overall\nconfidence or uncertainty scores on multiple sampled answers, which may result\nin an imprecise awareness of the model's own knowledge boundaries. To this end,\nwe propose a novel reinforcement learning framework built on\n$\\textbf{\\underline{Fi}ne-grained \\underline{S}emantic \\underline{Co}nfidence\n\\underline{Re}ward (\\Ours)}$, which guides LLMs to abstain via sample-specific\nconfidence. Specifically, our method operates by sampling multiple candidate\nanswers and conducting semantic clustering, then training the LLM to retain\nanswers within high-confidence clusters and discard those within low-confidence\nones, thereby promoting accurate post-hoc abstention. Additionally, we propose\na new metric for evaluating the reliability of abstention fine-tuning tasks\nmore comprehensively. Our method significantly enhances reliability in both\nin-domain and out-of-distribution benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a Fine-grained Semantic Confidence Reward \u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u591a\u5019\u9009\u7b54\u6848\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\u5e76\u5728\u9ad8\u7f6e\u4fe1\u7c07\u4e2d\u4fdd\u7559\u7b54\u6848\u3001\u4f4e\u7f6e\u4fe1\u7c07\u4e2d\u5254\u9664\u6765\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684 abstention\uff0c\u8f85\u4ee5\u65b0\u8bc4\u6d4b\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u5728IN/OD\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6a21\u578b\u62d2\u7b54\u7684\u7b56\u7565\u591a\u4f9d\u8d56\u6574\u4f53\u6216\u4e0d\u786e\u5b9a\u6027\u5206\u6570\u6765\u7ea6\u675f\uff0c\u65e0\u6cd5\u7cbe\u786e\u8bc6\u522b\u6a21\u578b\u77e5\u8bc6\u8fb9\u754c\uff0c\u5bfc\u81f4\u5bf9\u77e5\u8bc6\u76f2\u533a\u7684\u628a\u63e1\u4e0d\u5145\u5206\u3002", "method": "\u5728\u91c7\u6837\u591a\u4efd\u5019\u9009\u7b54\u6848\u540e\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\uff0c\u8bad\u7ec3\u6a21\u578b\u5728\u9ad8\u7f6e\u4fe1\u7c07\u4e2d\u4fdd\u7559\u7b54\u6848\u3001\u5728\u4f4e\u7f6e\u4fe1\u7c07\u4e2d\u5254\u9664\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8fd9\u6837\u4e00\u4e2a\u6837\u672c\u7279\u5b9a\u7684\u7f6e\u4fe1\u56de\u62a5\uff08\u6837\u672c\u7ea7\u522b\u7684\u7f6e\u4fe1\u5ea6\uff09\u3002\u540c\u65f6\u63d0\u51fa\u65b0\u9896\u7684 abstention \u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728IN-domain\u548cout-of-distribution\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347 abstention \u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u6b64\u65b9\u6cd5\u63d0\u5347\u4e86\u5bf9\u6a21\u578b\u77e5\u8bc6\u8fb9\u754c\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9c81\u68d2\u4e14\u53ef\u63a7\u7684\u62d2\u7b54\u884c\u4e3a\uff1b\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3001\u6269\u5c55\u5230\u66f4\u591a\u4efb\u52a1\u573a\u666f\u3002"}}
{"id": "2510.24021", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24021", "abs": "https://arxiv.org/abs/2510.24021", "authors": ["Haiduo Huang", "Jiangcheng Song", "Yadong Zhang", "Pengju Ren"], "title": "SpecKD: Speculative Decoding for Effective Knowledge Distillation of LLMs", "comment": null, "summary": "Knowledge Distillation (KD) has become a cornerstone technique for\ncompressing Large Language Models (LLMs) into smaller, more efficient student\nmodels. However, conventional KD approaches typically apply the distillation\nloss uniformly across all tokens, regardless of the teacher's confidence. This\nindiscriminate mimicry can introduce noise, as the student is forced to learn\nfrom the teacher's uncertain or high-entropy predictions, which may ultimately\nharm student performance-especially when the teacher is much larger and more\npowerful. To address this, we propose Speculative Knowledge Distillation\n(SpecKD), a novel, plug-and-play framework that introduces a dynamic,\ntoken-level gating mechanism inspired by the \"propose-and-verify\" paradigm of\nspeculative decoding. At each step, the student's token proposal is verified\nagainst the teacher's distribution; the distillation loss is selectively\napplied only to \"accepted\" tokens, while \"rejected\" tokens are masked out.\nExtensive experiments on diverse text generation tasks show that SpecKD\nconsistently and significantly outperforms strong KD baselines, leading to more\nstable training and more capable student models, and achieving state-of-the-art\nresults.", "AI": {"tldr": "\u63d0\u51fa SpecKD\uff0c\u4e00\u79cd\u57fa\u4e8e\u9010 token \u7684\u95e8\u63a7\u84b8\u998f\u6846\u67b6\uff0c\u53ea\u6709\u88ab\u6559\u5e08\u9a8c\u8bc1\u4e3a\u53ef\u4fe1\u7684 token\u624d\u53c2\u4e0e\u84b8\u998f\uff0c\u4ece\u800c\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u5bf9\u6240\u6709 token \u4e00\u89c6\u540c\u4ec1\u5730\u5e94\u7528\u84b8\u998f\u635f\u5931\uff0c\u6613\u5c06\u6559\u5e08\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u9ad8\u71b5\u9884\u6d4b\u8f6c\u5316\u4e3a\u84b8\u998f\u566a\u58f0\uff0c\u5c24\u5176\u5f53\u6559\u5e08\u6a21\u578b\u8fdc\u8d85\u5b66\u751f\u65f6\u6548\u679c\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u53ef\u63d2\u62d4\u7684 Speculative Knowledge Distillation\uff08SpecKD\uff09\uff0c\u91c7\u7528 propose-and-verify \u7684\u95e8\u63a7\u673a\u5236\u3002\u5728\u6bcf\u4e00\u6b65\uff0c\u5b66\u751f\u63d0\u51fa token \u9884\u6d4b\uff0c\u6559\u5e08\u5206\u5e03\u5bf9\u5176\u8fdb\u884c\u9a8c\u8bc1\uff1b\u4ec5\u5bf9\u201c\u63a5\u53d7\u201dtoken\u5e94\u7528\u84b8\u998f\u635f\u5931\uff0c\u201c\u62d2\u7edd\u201dtoken\u5219\u88ab\u5c4f\u853d\uff0c\u84b8\u998f\u635f\u5931\u4ec5\u5bf9\u7ecf\u7b5b\u9009\u7684 token \u751f\u6548\u3002", "result": "\u5728\u591a\u79cd\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\uff0cSpecKD \u76f8\u5bf9\u5f3a\u57fa\u7ebf\u5177\u6709\u663e\u8457\u63d0\u5347\uff0c\u8bad\u7ec3\u66f4\u7a33\u5b9a\uff0c\u5b66\u751f\u6a21\u578b\u66f4\u5f3a\uff0c\u8fbe\u5230\u6216\u63a5\u8fd1\u5f53\u524d\u7684\u6700\u4f18\u6c34\u5e73\uff08state-of-the-art\uff09\u3002", "conclusion": "SpecKD \u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u63d2\u62d4\u3001\u52a8\u6001\u7684\u95e8\u63a7\u84b8\u998f\u6846\u67b6\uff0c\u5229\u7528\u6559\u5e08\u4fe1\u5fc3\u8fdb\u884c\u6309\u9700\u84b8\u998f\uff0c\u4ee5\u51cf\u5c11\u566a\u58f0\u3001\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.24028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24028", "abs": "https://arxiv.org/abs/2510.24028", "authors": ["Tingyue Pan", "Mingyue Cheng", "Shilong Zhang", "Zhiding Liu", "Xiaoyu Tao", "Yucong Luo", "Jintao Zhang", "Qi Liu"], "title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "comment": null, "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.", "AI": {"tldr": "OneCast decouples seasonal and trend components for cross-domain time-series forecasting, using a light seasonal projection module and a semantic-aware tokenizer with masked discrete diffusion for the trend, achieving superior generalization across eight domains.", "motivation": "To address poor cross-domain generalization in heterogeneous time series when treating them as undifferentiated sequences; existing methods extend single-domain models but fail to handle domain-specific trend shifts and inconsistent periodic patterns.", "method": "Decompose time series into seasonal and trend components. Seasonal branch uses a lightweight projection with interpretable basis functions to reconstruct periodic patterns. Trend branch encodes segments as discrete tokens via a semantic-aware tokenizer and performs inference with a masked discrete diffusion process. Finally, combine both branches to produce the forecast.", "result": "Empirical evaluation on eight domains shows OneCast largely outperforms state-of-the-art baselines.", "conclusion": "Decoupling seasonal and trend components with specialized generative pathways yields better cross-domain forecasting and generalization; the modular design facilitates domain-specific adaptations."}}
{"id": "2510.24023", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24023", "abs": "https://arxiv.org/abs/2510.24023", "authors": ["Saujas Vaduguru", "Yilun Hua", "Yoav Artzi", "Daniel Fried"], "title": "Success and Cost Elicit Convention Formation for Efficient Communication", "comment": null, "summary": "Humans leverage shared conversational context to become increasingly\nsuccessful and efficient at communicating over time. One manifestation of this\nis the formation of ad hoc linguistic conventions, which allow people to\ncoordinate on short, less costly utterances that are understood using shared\nconversational context. We present a method to train large multimodal models to\nform conventions, enabling efficient communication. Our approach uses simulated\nreference games between models, and requires no additional human-produced data.\nIn repeated reference games involving photographs and tangram images, our\nmethod enables models to communicate efficiently with people: reducing the\nmessage length by up to 41% while increasing success by 15% over the course of\nthe interaction. Human listeners respond faster when interacting with our model\nthat forms conventions. We also show that training based on success or cost\nalone is insufficient - both are necessary to elicit convention formation.", "AI": {"tldr": "\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u5bf9\u5c40\u7684\u8de8\u6a21\u6001\u53c2\u8003\u6e38\u620f\uff0c\u80fd\u591f\u5728\u4e0d\u7ed9\u4eba\u7c7b\u989d\u5916\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5f62\u6210\u8bed\u8a00/\u7b26\u53f7\u4e0a\u7684\u4fbf\u5229\u6027\u516c\u7ea6\uff0c\u4f7f\u5f97\u4e0e\u4eba\u7c7b\u4e92\u52a8\u65f6\u901a\u4fe1\u66f4\u9ad8\u6548\u4e14\u66f4\u5feb\u3002", "motivation": "\u89e3\u51b3\u8de8\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u9ad8\u6548\u6c9f\u901a\u95ee\u9898\uff0c\u7814\u7a76\u6a21\u578b\u5728\u65e0\u76d1\u7763\u81ea\u6211\u5bf9\u8bdd\u4e2d\u662f\u5426\u80fd\u81ea\u53d1\u5f62\u6210\u7528\u4e8e\u7f29\u77ed\u8868\u8fbe\u3001\u63d0\u5347\u7406\u89e3\u901f\u5ea6\u7684\u5171\u8bc6\u6027\u8bed\u8a00\u516c\u7ea6\u3002", "method": "\u5728\u591a\u6a21\u6001\u8f93\u5165\uff08\u7167\u7247\u548cTangram\u56fe\u50cf\uff09\u4e0a\u8fdb\u884c\u91cd\u590d\u7684\u53c2\u8003\u6e38\u620f\uff0c\u7531\u6a21\u578b\u4e4b\u95f4\u5bf9\u8bdd\u4ee5\u5f62\u6210\u901a\u7528\u6c9f\u901a\u7b56\u7565\uff1b\u8bad\u7ec3\u8fc7\u7a0b\u4e0d\u4f9d\u8d56\u989d\u5916\u7684\u4eba\u7c7b\u6570\u636e\u3002", "result": "\u5728\u91cd\u590d\u7684\u53c2\u8003\u6e38\u620f\u4e2d\uff0c\u6a21\u578b\u4e0e\u4eba\u7c7b\u7684\u6c9f\u901a\u6548\u7387\u663e\u8457\u63d0\u5347\uff1a\u4fe1\u606f\u91cf\u51cf\u5c11\u6700\u591a41%\uff0c\u4e92\u52a8\u6210\u529f\u7387\u63d0\u5347\u7ea615%\uff1b\u4eba\u7c7b\u542c\u4f17\u5728\u4e0e\u5f62\u6210\u516c\u7ea6\u7684\u6a21\u578b\u4ea4\u4e92\u65f6\u53cd\u5e94\u66f4\u5feb\uff1b\u4ec5\u57fa\u4e8e\u6210\u529f\u6216\u6210\u672c\u7684\u8bad\u7ec3\u90fd\u4e0d\u8db3\u4ee5\u521b\u9020\u516c\u7ea6\uff0c\u4e8c\u8005\u5fc5\u987b\u540c\u65f6\u5b58\u5728\u3002", "conclusion": "\u901a\u8fc7\u81ea\u6211\u5bf9\u5c40\u548c\u8054\u5408\u4fe1\u53f7\uff08\u6210\u529f\u4e0e\u6210\u672c\uff09\u7684\u8bad\u7ec3\uff0c\u6a21\u578b\u80fd\u81ea\u53d1\u5f62\u6210\u6c9f\u901a\u516c\u7ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u7684\u66f4\u9ad8\u6548\u4e92\u52a8\uff0c\u5177\u6709\u5c06\u6765\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u8de8\u6a21\u6001\u4efb\u52a1\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.23650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23650", "abs": "https://arxiv.org/abs/2510.23650", "authors": ["Wei Xia"], "title": "Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs", "comment": null, "summary": "We proposed Static and Dynamic -- two zero-shot logits-layer debiasing\nmethods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits\nintervention outperforms hidden-layer approaches. We show semantic-aware logits\nintervention is stable and effective for debiasing aligned LLMs.", "AI": {"tldr": "\u63d0\u51fa\u9759\u6001\u4e0e\u52a8\u6001\u4e24\u79cd\u96f6-shot logits \u5c42\u53bb\u504f\u65b9\u6cd5\uff1b\u52a8\u6001\u5728\u964d\u4f4e\u504f\u5dee\u7684\u540c\u65f6\u4fdd\u6301\u6d41\u7545\u6027\uff0c\u504f\u5dee\u4e0b\u964d\u53ef\u8fbe\u7ea670%\uff1blogits \u5e72\u9884\u4f18\u4e8e\u9690\u85cf\u5c42\u65b9\u6cd5\uff1b\u8bed\u4e49\u611f\u77e5\u7684 logits \u5e72\u9884\u5728\u5bf9\u9f50\u7684 LLMs \u4e0a\u7a33\u5065\u6709\u6548\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5bf9\u9f50\u540e\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1/\u504f\u5411\u95ee\u9898\uff0c\u5229\u7528\u96f6-shot \u7684 logits \u5c42\u5e72\u9884\u5b9e\u73b0\u53bb\u504f\uff0c\u540c\u65f6\u5c3d\u91cf\u51cf\u5c11\u5bf9\u8bed\u8a00\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa Static\uff08\u9759\u6001\uff09\u4e0e Dynamic\uff08\u52a8\u6001\uff09\u4e24\u79cd\u57fa\u4e8e logits \u7684\u53bb\u504f\u7b56\u7565\uff0c\u901a\u8fc7\u5bf9 logits \u7684\u5e72\u9884\u5b9e\u73b0\u504f\u5dee\u6291\u5236\uff1b\u52a8\u6001\u65b9\u6cd5\u5728\u63a8\u65ad\u9636\u6bb5\u5bf9\u504f\u5411\u8fdb\u884c\u81ea\u9002\u5e94\u8c03\u6574\uff0c\u62a5\u544a\u5728\u4e0d\u663e\u8457\u727a\u7272 fluency \u7684\u60c5\u51b5\u4e0b\u5c06\u504f\u5dee\u964d\u4f4e\u7ea670%\uff1b\u5f3a\u8c03\u8bed\u4e49\u611f\u77e5\u7684 logits \u5e72\u9884\u7684\u7a33\u5b9a\u6027\u4e0e\u6709\u6548\u6027\u3002", "result": "\u52a8\u6001\u53bb\u504f\u5728\u504f\u5dee\u964d\u4f4e\u65b9\u9762\u8fbe\u5230\u7ea670%\uff0c\u5e76\u4e14\u5bf9\u8bed\u8a00\u6d41\u7545\u6027\u5f71\u54cd\u6700\u5c0f\uff1blogits \u5e72\u9884\u4f18\u4e8e\u9690\u85cf\u5c42\u65b9\u6cd5\uff1b\u8bed\u4e49\u611f\u77e5\u7684 logits \u5e72\u9884\u5728\u5bf9\u9f50\u7684 LLMs \u4e0a\u8868\u73b0\u7a33\u5b9a\u4e14\u6709\u6548\u3002", "conclusion": "\u57fa\u4e8e logits \u5c42\u7684\u53bb\u504f\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684\u53bb\u504f\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u5bf9\u9f50\u540e\u7684\u6a21\u578b\u4e2d\uff0c\u8bed\u4e49\u611f\u77e5\u7684\u5e72\u9884\u7b56\u7565\u5177\u6709\u8f83\u597d\u7684\u4e00\u81f4\u6027\u548c\u53ef\u63a8\u5e7f\u6027\u3002"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "Random Forest \u56de\u5f52\u5728 EV \u8ddf\u8f66\u884c\u4e3a\u5efa\u6a21\u4e2d\u4f18\u4e8e\u7ecf\u5178\u7269\u7406\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e RMSE\uff1bRF \u4f7f\u7528 spacing\u3001speed\u3001gap type \u4f5c\u4e3a\u8f93\u5165\u5bf9\u52a0\u901f\u5ea6\u8fdb\u884c\u9884\u6d4b\uff0c\u9002\u7528\u4e8e EV \u6df7\u5408\u81ea\u6cbb\u4ea4\u901a\u7684\u4eff\u771f\u4e0e\u5206\u6790\u3002", "motivation": "\u7406\u89e3\u7535\u52a8\u6c7d\u8f66\u7684\u9a7e\u9a76\u884c\u4e3a\u4ee5\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u548c\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u7684\u5f00\u53d1\u80fd\u529b\uff1b\u6bd4\u8f83\u7ecf\u5178\u6a21\u578b\u4e0e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728 EV \u8ddf\u8f66\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u5728\u6df7\u5408\u81ea\u6cbb\u4ea4\u901a\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c EV \u8ddf\u8f66\u6570\u636e\uff0c\u5728\u4e0d\u540c\u95f4\u8ddd\uff08 medium\u3001long\u3001extra long\uff09\u6761\u4ef6\u4e0b\uff0c\u5bf9 IDM\u3001OVM\u3001OVRV\u3001\u7b80\u5316 CACC \u7b49\u7ecf\u5178\u6a21\u578b\u53c2\u6570\u8fdb\u884c RMSE \u6700\u5c0f\u5316\u6807\u5b9a\uff1b\u91c7\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u4ee5 spacing\u3001speed\u3001gap type \u4e3a\u8f93\u5165\u9884\u6d4b\u52a0\u901f\u5ea6\uff1b\u6bd4\u8f83\u5404\u6a21\u578b\u7684 RMSE\uff0c\u8bc4\u4f30\u5728\u4e0d\u540c gap \u7684\u8868\u73b0\u3002", "result": "RF \u56de\u5f52\u5728\u5404\u95f4\u8ddd\u6761\u4ef6\u4e0b\u7684 RMSE \u6700\u4f4e\uff0c\u8868\u73b0\u4f18\u4e8e\u7269\u7406\u6a21\u578b\u3002\u5177\u4f53\u6570\u503c\uff1amedium gap 0.0046\u3001long gap 0.0016\u3001extra long gap 0.0025\uff1b\u5728\u7269\u7406\u6a21\u578b\u4e2d\uff0cCACC \u5728 long gap \u4e0b\u7684 RMSE \u4e3a 2.67\uff0c\u4e3a\u57fa\u7ebf\u4e2d\u8868\u73b0\u6700\u597d\u8005\uff1b\u603b\u4f53\u8868\u660e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u591a\u573a\u666f\u4e0b\u7684\u9884\u6d4b\u7cbe\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728 EV \u884c\u4e3a\u4eff\u771f\u4e0e\u6df7\u5408\u81ea\u6cbb\u4ea4\u901a\u5206\u6790\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e EV \u96c6\u6210\u73af\u5883\u7684\u884c\u4e3a\u9884\u6d4b\u548c\u4eff\u771f\u7814\u7a76\u3002"}}
{"id": "2510.23652", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23652", "abs": "https://arxiv.org/abs/2510.23652", "authors": ["Yao Lu", "Yuqi Li", "Wenbin Xie", "Shanqing Yu", "Qi Xuan", "Zhaowei Zhu", "Shiping Wen"], "title": "The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models", "comment": null, "summary": "Although large language models (LLMs) have achieved revolutionary\nbreakthroughs in many fields, their large model size and high computational\ncost pose significant challenges for practical deployment on\nresource-constrained edge devices. To this end, layer pruning has been proposed\nto reduce the computational overhead by directly removing redundant layers.\nHowever, existing layer pruning methods typically rely on hand-crafted metrics\nto evaluate and remove individual layers, while ignoring the dependencies\nbetween layers. This can disrupt the model's information flow and severely\ndegrade performance. To address these issues, we propose CLP, a novel\ncontinuous layer pruning framework that introduces two key innovations: a\ndifferentiable concave gate algorithm that automatically identifies the best\ncontinuous layer segments for pruning via gradient-based optimization; and a\ncutoff endpoint tuning strategy that effectively restores model performance by\nfine-tuning only the layers adjacent to the pruned segments. Extensive\nexperiments across multiple model architectures (including LLaMA2, LLaMA3 and\nQwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly\noutperforms existing state-of-the-art baselines. For example, at a pruning rate\nof $20\\%$, CLP achieves an average performance retention of $95.34\\%$ on\nLLaMA3-70B, outperforming baselines by $4.29\\%$-$30.52\\%$. Furthermore, CLP can\nbe seamlessly combined with quantization to further compress the model with\nonly a slight performance loss.", "AI": {"tldr": "\u63d0\u51fa CLP \u7684\u8fde\u7eed\u5c42 pruning \u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u51f9\u95e8\u63a7\u81ea\u52a8\u8bc6\u522b\u9700\u8981\u88c1\u526a\u7684\u6700\u4f73\u8fde\u7eed\u5c42\u7247\u6bb5\uff1b\u4f7f\u7528\u7aef\u70b9\u622a\u65ad\u8c03\u4f18\u7b56\u7565\u4ec5\u5bf9\u88c1\u526a\u7247\u6bb5\u76f8\u90bb\u7684\u5c42\u8fdb\u884c\u5fae\u8c03\u4ee5\u6062\u590d\u6027\u80fd\uff1b\u5728 LLaMA2/3\u3001Qwen \u7b49\u6a21\u578b\u4e0a 7B-70B \u7684\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u53ef\u4e0e\u91cf\u5316\u7ed3\u5408\u4ee5\u8fdb\u4e00\u6b65\u538b\u7f29\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u73b0\u6709\u5c42\u88c1\u526a\u591a\u4f9d\u8d56\u624b\u5de5\u6307\u6807\u4e14\u5ffd\u89c6\u5c42\u4e4b\u95f4\u7684\u4f9d\u8d56\uff0c\u6613\u7834\u574f\u4fe1\u606f\u6d41\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u63d0\u51fa\u53ef\u5fae\u5206\u7684\u8fde\u7eed\u5c42\u88c1\u526a\u4ee5\u540c\u65f6\u8003\u8651\u5c42\u95f4\u5173\u7cfb\u5e76\u4f18\u5316\u88c1\u526a\u65b9\u6848\u3002", "method": "\u5f15\u5165\u53ef\u5fae\u5206\u7684\u51f9\u95e8\u63a7\u7b97\u6cd5\u7528\u4e8e\u8bc6\u522b\u6700\u4f73\u8fde\u7eed\u5c42\u7247\u6bb5\u4ee5\u8fdb\u884c\u88c1\u526a\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u5b9e\u73b0\uff1b\u540c\u65f6\u63d0\u51fa\u7aef\u70b9\u622a\u65ad\u8c03\u4f18\u7b56\u7565\uff0c\u5728\u88c1\u526a\u540e\u4ec5\u5bf9\u88c1\u526a\u7247\u6bb5\u76f8\u90bb\u7684\u82e5\u5e72\u5c42\u8fdb\u884c\u5fae\u8c03\u4ee5\u6062\u590d\u6027\u80fd\uff1b\u53ef\u4e0e\u91cf\u5316\u7ed3\u5408\u3002\u5bf9\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u89c4\u6a21\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u5728\u88c1\u526a\u7387 20% \u4e0b\uff0cLLaMA3-70B \u7684\u5e73\u5747\u6027\u80fd\u4fdd\u7559\u7387\u8fbe\u5230 95.34%\uff0c\u8f83\u57fa\u7ebf\u63d0\u5347 4.29%-30.52%\uff1b\u5e76\u4e14\u4e0e\u91cf\u5316\u7ed3\u5408\u65f6\u4ecd\u4ec5\u6709\u8f7b\u5fae\u6027\u80fd\u635f\u5931\u3002", "conclusion": "CLP \u63d0\u4f9b\u4e86\u5bf9\u8fde\u7eed\u5c42\u88c1\u526a\u7684\u6709\u6548\u9014\u5f84\uff0c\u8003\u8651\u5c42\u4f9d\u8d56\u5173\u7cfb\u4e14\u901a\u8fc7\u53ef\u5fae\u5206\u4f18\u5316\u5b9e\u73b0\u66f4\u4f18\u7684\u88c1\u526a\u9009\u62e9\uff0c\u4e14\u4e0e\u91cf\u5316\u517c\u5bb9\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21 LLM \u7684\u9ad8\u6548\u90e8\u7f72\u3002"}}
{"id": "2510.24073", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24073", "abs": "https://arxiv.org/abs/2510.24073", "authors": ["Xinwei Wu", "Heng Liu", "Jiang Zhou", "Xiaohu Zhao", "Linlong Xu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling Hallucination in Translation", "comment": null, "summary": "Large Language Models (LLMs) have advanced machine translation but remain\nvulnerable to hallucinations. Unfortunately, existing MT benchmarks are not\ncapable of exposing failures in multilingual LLMs. To disclose hallucination in\nmultilingual LLMs, we introduce a diagnostic framework with a taxonomy that\nseparates Instruction Detachment from Source Detachment. Guided by this\ntaxonomy, we create HalloMTBench, a multilingual, human-verified benchmark\nacross 11 English-to-X directions. We employed 4 frontier LLMs to generate\ncandidates and scrutinize these candidates with an ensemble of LLM judges, and\nexpert validation. In this way, we curate 5,435 high-quality instances. We have\nevaluated 17 LLMs on HalloMTBench. Results reveal distinct ``hallucination\ntriggers'' -- unique failure patterns reflecting model scale, source length\nsensitivity, linguistic biases, and Reinforcement-Learning (RL) amplified\nlanguage mixing. HalloMTBench offers a forward-looking testbed for diagnosing\nLLM translation failures. HalloMTBench is available in\nhttps://huggingface.co/collections/AIDC-AI/marco-mt.", "AI": {"tldr": "\u6784\u5efa HalloMTBench\uff0c\u8bca\u65ad\u591a\u8bed\u8a00LLM\u7ffb\u8bd1\u4e2d\u7684\u5e7b\u89c9\uff1b\u57fa\u4e8e Instruction Detachment \u4e0e Source Detachment \u7684\u5206\u7c7b\u6846\u67b6\uff1b\u8986\u76d611\u5bf9\u82f1-X\u65b9\u5411\uff1b4\u4e2a\u524d\u6cbfLLM\u751f\u6210\u5019\u9009\uff0c\u7ecf\u8fc7LLM\u8bc4\u5ba1\u4e0e\u4e13\u5bb6\u6821\u9a8c\uff0c\u91c7\u6837\u91cf5,435\u6761\uff1b\u5bf917\u4e2aLLM\u8fdb\u884c\u8bc4\u6d4b\uff0c\u63ed\u793a\u5e7b\u89c9\u89e6\u53d1\u56e0\u7d20\u4e0e\u6a21\u5f0f\uff1b\u516c\u5f00\u53ef\u7528\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u7ffb\u8bd1\u57fa\u51c6\u4e0d\u8db3\u4ee5\u63ed\u9732\u591a\u8bed\u8a00LLM\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u8bca\u65ad\u6846\u67b6\u4e0e\u8de8\u8bed\u8a00\u57fa\u51c6\u6765\u7cfb\u7edf\u5730\u68c0\u6d4b\u3001\u5206\u6790\u548c\u5bf9\u6bd4\u5e7b\u89c9\u8868\u73b0\u3002", "method": "\u63d0\u51fa Instruction Detachment \u4e0e Source Detachment \u7684\u5e7b\u89c9\u5206\u7c7b\u6cd5\uff1b\u5229\u75284\u4e2a\u524d\u6cbfLLM\u751f\u6210\u5019\u9009\uff0c\u5e76\u901a\u8fc7LLM\u8bc4\u5ba1\u548c\u4e13\u5bb6\u9a8c\u8bc1\u8fdb\u884c\u591a\u8f6e\u7b5b\u9009\uff1b\u6784\u5efa11\u82f1-\u591a\u8bed\u79cd\u65b9\u5411\u7684 HalloMTBench\uff0c\u51715,435\u6761\u9ad8\u8d28\u91cf\u6837\u672c\uff1b\u5bf917\u4e2aLLM\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "\u53d1\u73b0\u5b58\u5728\u4e0d\u540c\u7684\u201c\u5e7b\u89c9\u89e6\u53d1\u56e0\u5b50\u201d\uff0c\u5305\u62ec\u6a21\u578b\u89c4\u6a21\u3001\u6e90\u6587\u672c\u957f\u5ea6\u654f\u611f\u6027\u3001\u8bed\u8a00\u504f\u89c1\uff0c\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u5f71\u54cd\u4e0b\u7684\u8bed\u8a00\u6df7\u7528\u7b49\u7279\u5f81\uff0c\u5448\u73b0\u51fa\u591a\u6837\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "HalloMTBench\u4e3a\u8bca\u65adLLM\u7ffb\u8bd1\u5931\u8d25\u63d0\u4f9b\u4e00\u4e2a\u524d\u77bb\u6027\u3001\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7814\u7a76\u8005\u53ef\u5728 HuggingFace \u4e0a\u83b7\u53d6\u548c\u4f7f\u7528\u3002"}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "\u4e00\u4e2a\u900f\u660e\u7684\u75c5\u7406\u5b66\u4eba\u5de5\u667a\u80fd\u52a9\u624b\uff08HistoLens\uff09\uff0c\u80fd\u591f\u5bf9\u7ec4\u7ec7\u5207\u7247\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\uff0c\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u5e76\u63d0\u4f9b\u201c\u89c6\u89c9\u8bc1\u636e\u201d\u70ed\u56fe\u6765\u89e3\u91ca\u53d1\u73b0\uff0c\u540c\u65f6\u6392\u9664\u80cc\u666f\u566a\u58f0\uff0c\u4f7f\u533b\u751f\u4ecd\u7136\u638c\u63a7\u8bca\u65ad\u8fc7\u7a0b\u3002", "motivation": "\u7834\u89e3\u9ed1\u76d2AI\u5728\u533b\u5b66\u5b9e\u8df5\u4e2d\u7684\u4e0d\u4fe1\u4efb\u611f\uff0c\u5b9e\u73b0\u75c5\u7406\u5b66\u5bb6\u4e0eAI\u7684\u534f\u4f5c\u4f19\u4f34\u5173\u7cfb\uff0c\u4ee5\u66f4\u5feb\u4e14\u66f4\u6709\u4fe1\u5fc3\u7684\u8bca\u65ad\u3002", "method": "\u6784\u5efa\u4e86HistoLens\uff0c\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u754c\u9762\uff0c\u5c06\u7528\u6237\u63d0\u95ee\u8f6c\u5316\u4e3a\u5bf9AI\u5f15\u64ce\u7684\u7cbe\u786e\u67e5\u8be2\uff0c\u8f93\u51fa\u7ed3\u6784\u5316\u62a5\u544a\uff1b\u5728\u9700\u8981\u65f6\u63d0\u4f9b\u70ed\u56fe\u7b49\u89c6\u89c9\u8bc1\u636e\uff0c\u6307\u793aAI\u7528\u4e8e\u5206\u6790\u7684\u5177\u4f53\u7ec6\u80de\u4e0e\u533a\u57df\uff1b\u901a\u8fc7\u8bad\u7ec3\u4f7f\u7cfb\u7edf\u4ec5\u805a\u7126\u4e8e\u60a3\u8005\u7ec4\u7ec7\uff0c\u5ffd\u7565\u80cc\u666f\u566a\u58f0\u3002", "result": "\u539f\u578b\u663e\u793a\u51fa\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u4e0e\u89c6\u89c9\u8bc1\u636e\uff0c\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u9a8c\u8bc1AI\u53d1\u73b0\uff0c\u63d0\u5347\u5de5\u4f5c\u6d41\u7684\u900f\u660e\u5ea6\u3001\u8bca\u65ad\u901f\u5ea6\u548c\u4fe1\u5fc3\u3002", "conclusion": "\u900f\u660e\u3001\u534f\u4f5c\u578b\u7684AI\uff08\u5982HistoLens\uff09\u80fd\u591f\u5728\u75c5\u7406\u5b66\u5bb6\u76d1\u7763\u4e0b\u63d0\u5347\u8bca\u65ad\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u4e0e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u66f4\u597d\u5730\u6574\u5408\u3002"}}
{"id": "2510.23656", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23656", "abs": "https://arxiv.org/abs/2510.23656", "authors": ["Fuqiang Liu", "Weiping Ding", "Luis Miranda-Moreno", "Lijun Sun"], "title": "Error Adjustment Based on Spatiotemporal Correlation Fusion for Traffic Forecasting", "comment": "12 pages, 7 figures, 3 tables", "summary": "Deep neural networks (DNNs) play a significant role in an increasing body of\nresearch on traffic forecasting due to their effectively capturing\nspatiotemporal patterns embedded in traffic data. A general assumption of\ntraining the said forecasting models via mean squared error estimation is that\nthe errors across time steps and spatial positions are uncorrelated. However,\nthis assumption does not really hold because of the autocorrelation caused by\nboth the temporality and spatiality of traffic data. This gap limits the\nperformance of DNN-based forecasting models and is overlooked by current\nstudies. To fill up this gap, this paper proposes Spatiotemporally\nAutocorrelated Error Adjustment (SAEA), a novel and general framework designed\nto systematically adjust autocorrelated prediction errors in traffic\nforecasting. Unlike existing approaches that assume prediction errors follow a\nrandom Gaussian noise distribution, SAEA models these errors as a\nspatiotemporal vector autoregressive (VAR) process to capture their intrinsic\ndependencies. First, it explicitly captures both spatial and temporal error\ncorrelations by a coefficient matrix, which is then embedded into a newly\nformulated cost function. Second, a structurally sparse regularization is\nintroduced to incorporate prior spatial information, ensuring that the learned\ncoefficient matrix aligns with the inherent road network structure. Finally, an\ninference process with test-time error adjustment is designed to dynamically\nrefine predictions, mitigating the impact of autocorrelated errors in real-time\nforecasting. The effectiveness of the proposed approach is verified on\ndifferent traffic datasets. Results across a wide range of traffic forecasting\nmodels show that our method enhances performance in almost all cases.", "AI": {"tldr": "SAEA\u662f\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u6d4b\u8bef\u5dee\u5efa\u6a21\u4e3a\u65f6\u7a7a\u5411\u91cf\u81ea\u56de\u5f52\u8fc7\u7a0b\u6765\u7ea0\u6b63\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u81ea\u76f8\u5173\u8bef\u5dee\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u7ed3\u6784\u7a00\u758f\u6b63\u5219\u548c\u6d4b\u8bd5\u65f6\u8bef\u5dee\u8c03\u6574\uff0c\u4ee5\u63d0\u5347\u591a\u79cd\u6a21\u578b\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709MSE\u8bad\u7ec3\u5047\u8bbe\u8bef\u5dee\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u72ec\u7acb\uff0c\u4f46\u4ea4\u901a\u6570\u636e\u7684\u65f6\u7a7a\u81ea\u76f8\u5173\u6027\u663e\u8457\uff0c\u5bfc\u81f4\u9884\u6d4b\u8bef\u5dee\u672a\u88ab\u5145\u5206\u5efa\u6a21\uff0c\u4ece\u800c\u9650\u5236\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5c06\u9884\u6d4b\u8bef\u5dee\u5efa\u6a21\u4e3a\u65f6\u7a7aVAR\u8fc7\u7a0b\uff0c\u5229\u7528\u7cfb\u6570\u77e9\u9635\u6355\u6349\u65f6\u7a7a\u76f8\u5173\u6027\u5e76\u5d4c\u5165\u65b0\u6210\u672c\u51fd\u6570\uff1b\u5f15\u5165\u7ed3\u6784\u7a00\u758f\u6b63\u5219\u4ee5\u5bf9\u9f50\u8def\u7f51\u7ed3\u6784\u4e2d\u7684\u5148\u9a8c\u4fe1\u606f\uff1b\u5728\u63a8\u7406\u9636\u6bb5\u8fdb\u884c\u6d4b\u8bd5\u65f6\u7684\u8bef\u5dee\u8c03\u6574\u4ee5\u5b9e\u65f6\u6539\u8fdb\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u4ea4\u901a\u6570\u636e\u96c6\u548c\u4e0d\u540c\u9884\u6d4b\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cSAEA\u5728\u51e0\u4e4e\u6240\u6709\u573a\u666f\u90fd\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "SAEA\u63d0\u4f9b\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u6765\u7cfb\u7edf\u6027\u5730\u8c03\u6574\u81ea\u76f8\u5173\u8bef\u5dee\uff0c\u63d0\u5347\u4ea4\u901a\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u4e0e\u6f5c\u5728\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24081", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24081", "abs": "https://arxiv.org/abs/2510.24081", "authors": ["Tyler A. Chang", "Catherine Arnett", "Abdelrahman Eldesokey", "Abdelrahman Sadallah", "Abeer Kashar", "Abolade Daud", "Abosede Grace Olanihun", "Adamu Labaran Mohammed", "Adeyemi Praise", "Adhikarinayum Meerajita Sharma", "Aditi Gupta", "Afitab Iyigun", "Afonso Simpl\u00edcio", "Ahmed Essouaied", "Aicha Chorana", "Akhil Eppa", "Akintunde Oladipo", "Akshay Ramesh", "Aleksei Dorkin", "Alfred Malengo Kondoro", "Alham Fikri Aji", "Ali Eren \u00c7etinta\u015f", "Allan Hanbury", "Alou Dembele", "Alp Niksarli", "\u00c1lvaro Arroyo", "Amin Bajand", "Amol Khanna", "Ana Chkhaidze", "Ana Condez", "Andiswa Mkhonto", "Andrew Hoblitzell", "Andrew Tran", "Angelos Poulis", "Anirban Majumder", "Anna Vacalopoulou", "Annette Kuuipolani Kanahele Wong", "Annika Simonsen", "Anton Kovalev", "Ashvanth. S", "Ayodeji Joseph Lana", "Barkin Kinay", "Bashar Alhafni", "Benedict Cibalinda Busole", "Bernard Ghanem", "Bharti Nathani", "Biljana Stojanovska \u0110uri\u0107", "Bola Agbonile", "Bragi Bergsson", "Bruce Torres Fischer", "Burak Tutar", "Burcu Alaku\u015f \u00c7\u0131nar", "Cade J. Kanoniakapueo Kane", "Can Udomcharoenchaikit", "Catherine Arnett", "Chadi Helwe", "Chaithra Reddy Nerella", "Chen Cecilia Liu", "Chiamaka Glory Nwokolo", "Cristina Espa\u00f1a-Bonet", "Cynthia Amol", "DaeYeop Lee", "Dana Arad", "Daniil Dzenhaliou", "Daria Pugacheva", "Dasol Choi", "Daud Abolade", "David Liu", "David Semedo", "Deborah Popoola", "Deividas Mataciunas", "Delphine Nyaboke", "Dhyuthy Krishna Kumar", "Diogo Gl\u00f3ria-Silva", "Diogo Tavares", "Divyanshu Goyal", "DongGeon Lee", "Ebele Nwamaka Anajemba", "Egonu Ngozi Grace", "Elena Mickel", "Elena Tutubalina", "Elias Herranen", "Emile Anand", "Emmanuel Habumuremyi", "Emuobonuvie Maria Ajiboye", "Eryawan Presma Yulianrifat", "Esther Adenuga", "Ewa Rudnicka", "Faith Olabisi Itiola", "Faran Taimoor Butt", "Fathima Thekkekara", "Fatima Haouari", "Filbert Aurelian Tjiaranata", "Firas Laakom", "Francesca Grasso", "Francesco Orabona", "Francesco Periti", "Gbenga Kayode Solomon", "Gia Nghia Ngo", "Gloria Udhehdhe-oze", "Gon\u00e7alo Martins", "Gopi Naga Sai Ram Challagolla", "Guijin Son", "Gulnaz Abdykadyrova", "Hafsteinn Einarsson", "Hai Hu", "Hamidreza Saffari", "Hamza Zaidi", "Haopeng Zhang", "Harethah Abu Shairah", "Harry Vuong", "Hele-Andra Kuulmets", "Houda Bouamor", "Hwanjo Yu", "Iben Nyholm Debess", "\u0130brahim Ethem Deveci", "Ikhlasul Akmal Hanif", "Ikhyun Cho", "In\u00eas Calvo", "In\u00eas Vieira", "Isaac Manzi", "Ismail Daud", "Itay Itzhak", "Iuliia", "Alekseenko", "Ivan Belashkin", "Ivan Spada", "Ivan Zhelyazkov", "Jacob Brinton", "Jafar Isbarov", "Jaka \u010cibej", "Jan \u010cuhel", "Jan Koco\u0144", "Jauza Akbar Krito", "Jebish Purbey", "Jennifer Mickel", "Jennifer Za", "Jenny Kunz", "Jihae Jeong", "Jimena Tena D\u00e1valos", "Jinu Lee", "Jo\u00e3o Magalh\u00e3es", "John Yi", "Jongin Kim", "Joseph Chataignon", "Joseph Marvin Imperial", "Jubeerathan Thevakumar", "Judith Land", "Junchen Jiang", "Jungwhan Kim", "Kairit Sirts", "Kamesh R", "Kamesh V", "Kanda Patrick Tshinu", "K\u00e4triin Kukk", "Kaustubh Ponkshe", "Kavsar Huseynova", "Ke He", "Kelly Buchanan", "Kengatharaiyer Sarveswaran", "Kerem Zaman", "Khalil Mrini", "Kian Kyars", "Krister Kruusmaa", "Kusum Chouhan", "Lainitha Krishnakumar", "Laura Castro S\u00e1nchez", "Laura Porrino Moscoso", "Leshem Choshen", "Levent Sencan", "Lilja \u00d8vrelid", "Lisa Alazraki", "Lovina Ehimen-Ugbede", "Luheerathan Thevakumar", "Luxshan Thavarasa", "Mahnoor Malik", "Mamadou K. Keita", "Mansi Jangid", "Marco De Santis", "Marcos Garc\u00eda", "Marek Suppa", "Mariam D'Ciofalo", "Marii Ojastu", "Maryam Sikander", "Mausami Narayan", "Maximos Skandalis", "Mehak Mehak", "Mehmet \u0130lteri\u015f Bozkurt", "Melaku Bayu Workie", "Menan Velayuthan", "Michael Leventhal", "Micha\u0142 Marci\u0144czuk", "Mirna Poto\u010dnjak", "Mohammadamin Shafiei", "Mridul Sharma", "Mrityunjaya Indoria", "Muhammad Ravi Shulthan Habibi", "Murat Koli\u0107", "Nada Galant", "Naphat Permpredanun", "Narada Maugin", "Nicholas Kluge Corr\u00eaa", "Nikola Ljube\u0161i\u0107", "Nirmal Thomas", "Nisansa de Silva", "Nisheeth Joshi", "Nitish Ponkshe", "Nizar Habash", "Nneoma C. Udeze", "Noel Thomas", "No\u00e9mi Ligeti-Nagy", "Nouhoum Coulibaly", "Nsengiyumva Faustin", "Odunayo Kareemat Buliaminu", "Odunayo Ogundepo", "Oghojafor Godswill Fejiro", "Ogundipe Blessing Funmilola", "Okechukwu God'spraise", "Olanrewaju Samuel", "Olaoye Deborah Oluwaseun", "Olasoji Akindejoye", "Olga Popova", "Olga Snissarenko", "Onyinye Anulika Chiemezie", "Orkun Kinay", "Osman Tursun", "Owoeye Tobiloba Moses", "Oyelade Oluwafemi Joshua", "Oyesanmi Fiyinfoluwa", "Pablo Gamallo", "Pablo Rodr\u00edguez Fern\u00e1ndez", "Palak Arora", "Pedro Valente", "Peter Rupnik", "Philip Oghenesuowho Ekiugbo", "Pramit Sahoo", "Prokopis Prokopidis", "Pua Niau-Puhipau", "Quadri Yahya", "Rachele Mignone", "Raghav Singhal", "Ram Mohan Rao Kadiyala", "Raphael Merx", "Rapheal Afolayan", "Ratnavel Rajalakshmi", "Rishav Ghosh", "Romina Oji", "Ron Kekeha Solis", "Rui Guerra", "Rushikesh Zawar", "Sa'ad Nasir Bashir", "Saeed Alzaabi", "Sahil Sandeep", "Sai Pavan Batchu", "SaiSandeep Kantareddy", "Salsabila Zahirah Pranida", "Sam Buchanan", "Samuel Rutunda", "Sander Land", "Sarah Sulollari", "Sardar Ali", "Saroj Sapkota", "Saulius Tautvaisas", "Sayambhu Sen", "Sayantani Banerjee", "Sebastien Diarra", "SenthilNathan. M", "Sewoong Lee", "Shaan Shah", "Shankar Venkitachalam", "Sharifa Djurabaeva", "Sharon Ibejih", "Shivanya Shomir Dutta", "Siddhant Gupta", "Silvia Paniagua Su\u00e1rez", "Sina Ahmadi", "Sivasuthan Sukumar", "Siyuan Song", "Snegha A.", "Sokratis Sofianopoulos", "Sona Elza Simon", "Sonja Ben\u010dina", "Sophie Gvasalia", "Sphurti Kirit More", "Spyros Dragazis", "Stephan P. Kaufhold", "Suba. S", "Sultan AlRashed", "Surangika Ranathunga", "Taiga Someya", "Taja Kuzman Punger\u0161ek", "Tal Haklay", "Tasi'u Jibril", "Tatsuya Aoyama", "Tea Abashidze", "Terenz Jomar Dela Cruz", "Terra Blevins", "Themistoklis Nikas", "Theresa Dora Idoko", "Thu Mai Do", "Tilek Chubakov", "Tommaso Gargiani", "Uma Rathore", "Uni Johannesen", "Uwuma Doris Ugwu", "Vallerie Alexandra Putra", "Vanya Bannihatti Kumar", "Varsha Jeyarajalingam", "Varvara Arzt", "Vasudevan Nedumpozhimana", "Viktoria Ondrejova", "Viktoryia Horbik", "Vishnu Vardhan Reddy Kummitha", "Vuk Dini\u0107", "Walelign Tewabe Sewunetie", "Winston Wu", "Xiaojing Zhao", "Yacouba Diarra", "Yaniv Nikankin", "Yash Mathur", "Yixi Chen", "Yiyuan Li", "Yolanda Xavier", "Yonatan Belinkov", "Yusuf Ismail Abayomi", "Zaid Alyafeai", "Zhengyang Shan", "Zhi Rui Tam", "Zilu Tang", "Zuzana Nadova", "Baber Abbasi", "Stella Biderman", "David Stap", "Duygu Ataman", "Fabian Schmidt", "Hila Gonen", "Jiayi Wang", "David Ifeoluwa Adelani"], "title": "Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures", "comment": "Preprint", "summary": "To date, there exist almost no culturally-specific evaluation benchmarks for\nlarge language models (LLMs) that cover a large number of languages and\ncultures. In this paper, we present Global PIQA, a participatory commonsense\nreasoning benchmark for over 100 languages, constructed by hand by 335\nresearchers from 65 countries around the world. The 116 language varieties in\nGlobal PIQA cover five continents, 14 language families, and 23 writing\nsystems. In the non-parallel split of Global PIQA, over 50% of examples\nreference local foods, customs, traditions, or other culturally-specific\nelements. We find that state-of-the-art LLMs perform well on Global PIQA in\naggregate, but they exhibit weaker performance in lower-resource languages (up\nto a 37% accuracy gap, despite random chance at 50%). Open models generally\nperform worse than proprietary models. Global PIQA highlights that in many\nlanguages and cultures, everyday knowledge remains an area for improvement,\nalongside more widely-discussed capabilities such as complex reasoning and\nexpert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA\nprovides a glimpse into the wide diversity of cultures in which human language\nis embedded.", "AI": {"tldr": "\u5168\u7403\u9996\u4e2a\u8de8\u8bed\u8a00\u3001\u8de8\u6587\u5316\u7684\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6 Global PIQA\uff0c\u8986\u76d6100+\u8bed\u8a00\uff0c\u7531335\u540d\u7814\u7a76\u8005\u5171\u540c\u6784\u5efa\uff0c\u5f3a\u8c03\u65e5\u5e38\u6587\u5316\u77e5\u8bc6\u5bf9LLMs\u8bc4\u4ef7\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8986\u76d6\u5e7f\u6cdb\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u96be\u4ee5\u8861\u91cfLLMs\u5728\u65e5\u5e38\u3001\u6587\u5316\u76f8\u5173\u77e5\u8bc6\u65b9\u9762\u7684\u8868\u73b0\uff1b\u5e0c\u671b\u63a8\u52a8\u5bf9\u591a\u8bed\u8a00\u6587\u5316\u77e5\u8bc6\u7684\u5173\u6ce8\u4e0e\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7335\u540d\u7814\u7a76\u8005\u6765\u81ea65\u4e2a\u56fd\u5bb6\u624b\u5de5\u6784\u5efa\uff0c\u8986\u76d6116\u79cd\u8bed\u8a00\u53d8\u4f53\uff0c\u975e\u5e73\u884c\u5206\u5e03\uff1b\u6570\u636e\u4e2d\u6709\u8d85\u8fc750%\u7684\u6837\u672c\u6d89\u53ca\u5f53\u5730\u98df\u54c1\u3001\u98ce\u4fd7\u7b49\u6587\u5316\u5143\u7d20\uff1b\u5bf9\u6bd4\u5206\u6790\u516c\u5f00\u6a21\u578b\u4e0e\u4e13\u6709\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728\u6574\u4f53\u4e0a\uff0c\u6700\u5148\u8fdb\u7684LLMs\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u8d44\u6e90\u8f83\u5c11\u7684\u8bed\u8a00\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08\u6700\u9ad8\u53ef\u8fbe37%\u7684\u51c6\u786e\u7387\u5dee\u8ddd\uff0c\u968f\u673a\u731c\u6d4b\u4e3a50%\uff09\uff1b\u5f00\u653e\u6a21\u578b\u901a\u5e38\u4e0d\u5982\u4e13\u6709\u6a21\u578b\uff1b\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u5316\u77e5\u8bc6\u662f\u9700\u6539\u8fdb\u7684\u9886\u57df\u3002", "conclusion": "Global PIQA \u5c55\u793a\u8bed\u8a00\u4e2d\u6f5c\u5728\u7684\u6587\u5316\u591a\u6837\u6027\uff0c\u63d0\u4f9b\u4e00\u79cd\u9762\u5411\u591a\u8bed\u79cd\u3001\u6587\u5316\u80cc\u666f\u7684LLM\u8bc4\u4f30\u8def\u5f84\uff0c\u671f\u671b\u4fc3\u8fdb\u5bf9\u65e5\u5e38\u77e5\u8bc6\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u7684\u63d0\u5347\u4e0e\u7814\u7a76\uff0c\u5e76\u4fc3\u8fdb\u5bf9\u6587\u5316\u5d4c\u5165\u8bed\u8a00\u7684\u7406\u89e3\u3002"}}
{"id": "2510.24145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24145", "abs": "https://arxiv.org/abs/2510.24145", "authors": ["Yu Luo", "Jiamin Jiang", "Jingfei Feng", "Lei Tao", "Qingliang Zhang", "Xidao Wen", "Yongqian Sun", "Shenglin Zhang", "Jielong Huang", "Nan Qi", "Dan Pei"], "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems", "comment": null, "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.", "AI": {"tldr": "OpsAgent \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u81ea\u8fdb\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e91\u7aef\u4e8b\u4ef6\u7ba1\u7406\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5904\u7406\u89c2\u6d4b\u6570\u636e\uff0c\u5c06\u5f02\u6784\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5177\u5907\u900f\u660e\u534f\u4f5c\u8bca\u65ad\u3001\u53cc\u81ea\u6211\u8fdb\u5316\u673a\u5236\uff0c\u5728 OPENRCA \u57fa\u51c6\u4e0a\u8fbe\u5230 state-of-the-art\uff0c\u5177\u53ef\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u53ef\u90e8\u7f72\u6027\u3002", "motivation": "\u624b\u52a8\u4e8b\u4ef6\u7ba1\u7406\u5de5\u4f5c\u91cf\u5927\u3001\u5bb9\u6613\u51fa\u9519\uff1b\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u96be\u4ee5\u8de8\u7cfb\u7edf\u6cdb\u5316\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u90e8\u7f72\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u900f\u660e\u4e14\u53ef\u6301\u7eed\u6f14\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165 OpsAgent\uff1a\u4e00\u4e2a\u8bad\u7ec3\u65e0\u5173\u7684\u6570\u636e\u5904\u7406\u5668\uff0c\u5c06\u5f02\u6784\u89c2\u6d4b\u6570\u636e\uff08\u6307\u6807\u3001\u65e5\u5fd7\u3001\u8ffd\u8e2a\uff09\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff1b\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u786e\u4fdd\u8bca\u65ad\u63a8\u7406\u53ef\u89e3\u91ca\u3001\u53ef\u5ba1\u8ba1\uff1b\u4e00\u4e2a\u53cc\u91cd\u81ea\u6211\u8fdb\u5316\u673a\u5236\uff0c\u5c06\u5185\u90e8\u6a21\u578b\u66f4\u65b0\u4e0e\u5916\u90e8\u7ecf\u9a8c\u79ef\u7d2f\u7ed3\u5408\u8d77\u6765\uff0c\u6301\u7eed\u63d0\u5347\u80fd\u529b\u5e76\u95ed\u5408\u90e8\u7f72\u5faa\u73af\u3002", "result": "\u5728 OPENRCA \u57fa\u51c6\u4e0a\u7684\u5168\u9762\u5b9e\u9a8c\u8868\u660e\u8fbe\u5230\u884c\u4e1a\u9886\u5148\u6c34\u5e73\uff1bOpsAgent \u5177\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6210\u672c\u6548\u76ca\u5e76\u5177\u81ea\u8fdb\u5316\u7279\u6027\u3002", "conclusion": "OpsAgent \u63d0\u4f9b\u4e00\u4e2a\u5728\u73b0\u5b9e\u4e91\u73af\u5883\u4e2d\u53ef\u90e8\u7f72\u3001\u53ef\u6301\u7eed\u8fd0\u884c\u7684\u4e8b\u4ef6\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u6301\u7eed\u80fd\u529b\u589e\u957f\uff0c\u663e\u8457\u51cf\u8f7b\u4eba\u5de5\u5e72\u9884\u3002"}}
{"id": "2510.23657", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23657", "abs": "https://arxiv.org/abs/2510.23657", "authors": ["Saklain Niam", "Tashfiqur Rahman", "Md. Amjad Patwary", "Mukarram Hossain"], "title": "A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops", "comment": null, "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.", "AI": {"tldr": "\u7528\u673a\u5668\u5b66\u4e60\u6846\u67b6\u9884\u6d4b\u8c46\u79d1\u548c\u5341\u5b57\u82b1\u79d1\u4f5c\u7269\u5728\u4ecb\u7535\u963b\u653e\u7535(DBD)\u7b49\u79bb\u5b50\u4f53\u5904\u7406\u540e\u79cd\u5b50\u53d1\u82bd\u63d0\u5347\u7684\u6548\u679c\uff0c\u91c7\u7528 Extra Trees \u7b49\u6a21\u578b\uff0c\u53d6\u5f97\u9ad8\u7cbe\u5ea6\u5e76\u63ed\u793a hormetic \u54cd\u5e94\u4e0e\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u6846\u67b6\u96c6\u6210\u5230 MLflow \u4ee5\u7528\u4e8e\u7cbe\u51c6\u519c\u4e1a\u7684\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u7531\u4e8e\u79cd\u5b50-\u7b49\u79bb\u5b50\u4f53-\u73af\u5883\u4e4b\u95f4\u7684\u590d\u6742\u8026\u5408\uff0c\u9884\u6d4b CP \u5904\u7406\u5bf9\u53d1\u82bd\u7684\u63d0\u5347\u5b58\u5728\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\uff0c\u4e9f\u9700\u4e00\u4e2a\u8de8\u54c1\u79cd\u7684\u9884\u6d4b\u5de5\u5177\u6765\u4f18\u5316\u5904\u7406\u53c2\u6570\u3002", "method": "\u5728\u8c46\u7c7b\u3001\u9ea6\u82bd\u3001\u5411\u65e5\u8475\u3001\u841d\u535c\u548c\u756a\u8304\u7b49\u4f5c\u7269\u4e0a\u8bc4\u4f30\u591a\u6a21\u578b\uff08GB\u3001XGB\u3001ET\u53ca\u6df7\u5408\u6a21\u578b\uff09\u3002ET \u8868\u73b0\u6700\u4f73\uff0c\u7ecf\u7279\u5f81\u7b5b\u9009\u540e\u6027\u80fd\u63d0\u5347\uff1b\u63ed\u793a hormetic \u54cd\u5e94\uff1a<7 kV \u6216 <200 s\u51e0\u4e4e\u65e0\u6548\uff0c7\u201315 kV \u4e14 200\u2013500 s \u8fbe\u5230\u6700\u5927\u53d1\u82bd\u7387\uff0c>20 kV \u6216\u8fc7\u957f\u66b4\u9732\u964d\u4f4e\u53d1\u82bd\uff1b\u653e\u7535\u529f\u7387\u662f\u4e3b\u5bfc\u56e0\u7d20\uff0c\u53d1\u82bd\u7387\u5728 \u2265100 W \u7684\u4f4e\u66b4\u9732\u65f6\u95f4\u4e0b\u8fbe\u5230\u6700\u9ad8\u3002\u5bf9\u4e0d\u540c\u7269\u79cd/\u54c1\u7cfb\u7684\u9884\u6d4b\uff0c\u841d\u535c MAE 1.46\u3001\u9ec4\u8c46 2.05 \u5177\u9ad8\u4e00\u81f4\u6027\uff1b\u5411\u65e5\u8475\u53d8\u5f02\u6027\u7565\u9ad8\uff083.80\uff09\uff1b\u54c1\u7cfb\u65b9\u9762\uff0cWilliams/1.23\u3001Sari/1.33 \u51fa\u8272\uff0c\u800c Arian/2.86\u3001Ny\u00edrs\u00e9gi fekete/3.74 \u76f8\u5bf9\u8f83\u5dee\u3002", "result": "ET \u7684\u51b3\u5b9a\u7cfb\u6570 R^2 \u4e3a 0.919\uff0cRMSE=3.21\uff0cMAE=2.62\uff1b\u7279\u5f81\u7b5b\u9009\u540e R^2 \u63d0\u5347\u81f3 0.925\u3002\u5bf9\u54c1\u79cd\u5c42\u7ea7\uff0c\u841d\u535c MAE=1.46\u3001\u9ec4\u8c46 MAE=2.05 \u8868\u73b0\u7a33\u5b9a\uff0c\u5411\u65e5\u8475\u7565\u9ad8\u53d8\u5f02\uff08MAE=3.80\uff09\u3002\u6700\u4f18\u54c1\u79cd/\u54c1\u7cfb\u9884\u6d4b\uff1aWilliams MAE=1.23\u3001Sari 1.33\uff1bArian 2.86\u3001Ny\u00edrs\u00e9gi fekete 3.74\u3002\u7814\u7a76\u8fd8\u5c06\u8be5\u6846\u67b6\u5d4c\u5165 MLflow\uff0c\u5f62\u6210\u7528\u4e8e\u7cbe\u51c6\u519c\u4e1a\u7684 CP \u5904\u7406\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u53ef\u5bf9\u591a\u7269\u79cd/\u54c1\u7cfb\u7684 CP \u53d1\u82bd\u63d0\u5347\u8fdb\u884c\u9ad8\u6548\u9884\u6d4b\uff0c\u5e76\u63ed\u793a\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u548c hormetic \u54cd\u5e94\u6a21\u5f0f\uff0c\u4e14\u53ef\u901a\u8fc7 MLflow \u63d0\u4f9b\u843d\u5730\u7684\u51b3\u7b56\u652f\u6301\uff0c\u652f\u6301\u5728\u7cbe\u5bc6\u519c\u4e1a\u4e2d\u5bf9 CP \u5904\u7406\u53c2\u6570\u8fdb\u884c\u4f18\u5316\u3002"}}
{"id": "2510.24096", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24096", "abs": "https://arxiv.org/abs/2510.24096", "authors": ["Md. Rezuwan Hassan", "Azmol Hossain", "Kanij Fatema", "Rubayet Sabbir Faruque", "Tanmoy Shome", "Ruwad Naswan", "Trina Chakraborty", "Md. Foriduzzaman Zihad", "Tawsif Tashwar Dipto", "Nazia Tasnim", "Nazmuddoha Ansary", "Md. Mehedi Hasan Shawon", "Ahmed Imtiaz Humayun", "Md. Golam Rabiul Alam", "Farig Sadeque", "Asif Sushmit"], "title": "RegSpeech12: A Regional Corpus of Bengali Spontaneous Speech Across Dialects", "comment": "26 pages", "summary": "The Bengali language, spoken extensively across South Asia and among\ndiasporic communities, exhibits considerable dialectal diversity shaped by\ngeography, culture, and history. Phonological and pronunciation-based\nclassifications broadly identify five principal dialect groups: Eastern\nBengali, Manbhumi, Rangpuri, Varendri, and Rarhi. Within Bangladesh, further\ndistinctions emerge through variation in vocabulary, syntax, and morphology, as\nobserved in regions such as Chittagong, Sylhet, Rangpur, Rajshahi, Noakhali,\nand Barishal. Despite this linguistic richness, systematic research on the\ncomputational processing of Bengali dialects remains limited. This study seeks\nto document and analyze the phonetic and morphological properties of these\ndialects while exploring the feasibility of building computational models\nparticularly Automatic Speech Recognition (ASR) systems tailored to regional\nvarieties. Such efforts hold potential for applications in virtual assistants\nand broader language technologies, contributing to both the preservation of\ndialectal diversity and the advancement of inclusive digital tools for\nBengali-speaking communities. The dataset created for this study is released\nfor public use.", "AI": {"tldr": "\u603b\u7ed3\uff1a\u8bb0\u5f55\u5b5f\u52a0\u62c9\u8bed\u4e3b\u8981\u65b9\u8a00\u7684\u8bed\u97f3\u4e0e\u5f62\u6001\u7279\u5f81\uff0c\u8bc4\u4f30\u9762\u5411\u533a\u57df\u53d8\u4f53\u7684ASR\u53ef\u884c\u6027\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u5177\u6709\u663e\u8457\u7684\u65b9\u8a00\u591a\u6837\u6027\uff0c\u73b0\u6709\u8ba1\u7b97\u5904\u7406\u7814\u7a76\u4e0d\u8db3\uff0c\u533a\u57df\u5316ASR\u6709\u52a9\u4e8e\u63d0\u5347\u5305\u5bb9\u6027\u4e0e\u5de5\u5177\u666e\u53ca\uff0c\u540c\u65f6\u6709\u52a9\u4e8e\u65b9\u8a00\u4fdd\u62a4\u4e0e\u6570\u5b57\u5316\u53d1\u5c55\u3002", "method": "\u5bf9Eastern Bengali\u3001Manbhumi\u3001Rangpuri\u3001Varendri\u3001Rarhi\u7b49\u4e94\u5927\u65b9\u8a00\u7ec4\u8fdb\u884c\u8bed\u97f3\u3001\u97f3\u7cfb\u3001\u8bcd\u6c47\u3001\u53e5\u6cd5\u4e0e\u5f62\u6001\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u8bc4\u4f30\u6784\u5efa\u533a\u57df\u5316ASR\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5f00\u53d1\u5e76\u516c\u5f00\u7528\u4e8e\u7814\u7a76\u7684\u6570\u636e\u96c6\u3002", "result": "\u63d0\u51fa\u5404\u65b9\u8a00\u7684\u8bed\u97f3\u4e0e\u5f62\u6001\u7279\u5f81\u6846\u67b6\uff0c\u521d\u6b65\u9a8c\u8bc1\u533a\u57df\u5316ASR\u6f5c\u529b\uff0c\u6570\u636e\u96c6\u5df2\u5bf9\u516c\u4f17\u5f00\u653e\u4f9b\u540e\u7eed\u7814\u7a76\u4f7f\u7528\u3002", "conclusion": "\u5f3a\u8c03\u65b9\u8a00\u591a\u6837\u6027\u5728\u6570\u5b57\u5de5\u5177\u4e2d\u7684\u7eb3\u5165\u4e0e\u4fdd\u62a4\u610f\u4e49\uff0c\u672a\u6765\u5de5\u4f5c\u805a\u7126\u63d0\u5347\u8de8\u65b9\u8a00\u9c81\u68d2\u6027\u4e0e\u6269\u5c55\u6570\u636e\u96c6\u7684\u8986\u76d6\u8303\u56f4\u3002"}}
{"id": "2510.24151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24151", "abs": "https://arxiv.org/abs/2510.24151", "authors": ["Bingsen Qiu", "Zijian Liu", "Xiao Liu", "Haoshen Yang", "Zeren Gao", "Bingjie Wang", "Feier Zhang", "Yixuan Qin", "Chunyan Li"], "title": "BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data", "comment": null, "summary": "Building training-ready multi-hop question answering (QA) datasets that truly\nstress a model's retrieval and reasoning abilities remains highly challenging\nrecently. While there have been a few recent evaluation datasets that capture\nthe characteristics of hard-to-search but easy-to-verify problems -- requiring\nthe integration of ambiguous, indirect, and cross-domain cues -- these data\nresources remain scarce and are mostly designed for evaluation, making them\nunsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).\nMeanwhile, manually curating non-trivially retrievable questions -- where\nanswers cannot be found through a single direct query but instead require\nmulti-hop reasoning over oblique and loosely connected evidence -- incurs\nprohibitive human costs and fails to scale, creating a critical data bottleneck\nfor training high-capability retrieval-and-reasoning agents.\n  To address this, we present an automated framework for generating\nhigh-difficulty, training-ready multi-hop questions from semi-structured\nknowledge sources. The system (i) grows diverse, logically labeled evidence\nclusters through Natural Language Inference (NLI)-based relation typing and\ndiversity-aware expansion; (ii) applies reverse question construction to\ncompose oblique cues so that isolated signals are underinformative but their\ncombination uniquely identifies the target entity; and (iii) enforces quality\nwith a two-step evaluation pipeline that combines multi-model consensus\nfiltering with structured constraint decomposition and evidence-based matching.\nThe result is a scalable process that yields complex, retrieval-resistant yet\nverifiable questions suitable for SFT/RL training as well as challenging\nevaluation, substantially reducing human curation effort while preserving the\ndifficulty profile of strong evaluation benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u6e90\u751f\u6210\u9ad8\u96be\u5ea6\u3001\u8bad\u7ec3\u5c31\u7eea\u7684\u591a\u8df3\u95ee\u7b54\u6570\u636e\uff0c\u7ed3\u5408 NLI \u5173\u7cfb\u7c7b\u578b\u3001\u9006\u5411\u9898\u751f\u6210\u3001\u53cc\u6b65\u8d28\u91cf\u7b5b\u9009\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u7528\u4e8e SFT/RL \u7684\u8bad\u7ec3\u548c\u8bc4\u6d4b\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u591a\u8df3QA\u6570\u636e\u96be\u4ee5\u7528\u4e8e\u76d1\u7763\u5fae\u8c03/\u5f3a\u5316\u5b66\u4e60\uff0c\u73b0\u6709\u8bc4\u6d4b\u6570\u636e\u7a00\u7f3a\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u5f62\u6210\u8bad\u7ec3\u9ad8\u80fd\u529b\u68c0\u7d22-\u63a8\u7406\u6a21\u578b\u7684\u6570\u636e\u74f6\u9888\u3002", "method": "1) \u901a\u8fc7 NLI \u5173\u7cfb\u7c7b\u578b\u548c\u591a\u6837\u6027\u6269\u5c55\u6784\u9020\u591a\u6837\u7684\u8bc1\u636e\u7c07 2) \u91c7\u7528\u9006\u5411\u95ee\u9898\u6784\u9020\u751f\u6210\u9690\u6027\u7ebf\u7d22\uff0c\u7ec4\u5408\u4fe1\u53f7\u624d\u80fd\u552f\u4e00\u6307\u5411\u76ee\u6807\u5b9e\u4f53 3) \u4e24\u6b65\u8bc4\u4f30\uff1a\u591a\u6a21\u578b\u5171\u8bc6\u7b5b\u9009 + \u7ed3\u6784\u5316\u7ea6\u675f\u5206\u89e3\u548c\u8bc1\u636e\u5bf9\u9f50\uff0c\u786e\u4fdd\u8d28\u91cf", "result": "\u5f97\u5230\u53ef\u89c4\u6a21\u5316\u7684\u590d\u6742\u3001\u68c0\u7d22\u6297\u6027\u4f46\u53ef\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u9002\u5408 SFT/RL \u8bad\u7ec3\u548c\u6311\u6218\u6027\u8bc4\u6d4b\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u7559\u5f3a\u8bc4\u6d4b\u57fa\u51c6\u7684\u96be\u5ea6\u7279\u5f81", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u751f\u6210\u9ad8\u96be\u5ea6\u591a\u8df3\u95ee\u7b54\u6570\u636e\uff0c\u4fc3\u8fdb\u66f4\u5f3a\u7684\u68c0\u7d22\u4e0e\u63a8\u7406\u80fd\u529b\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u5e76\u53ef\u7528\u4e8e\u8bc4\u6d4b\u6570\u636e\u7684\u6784\u5efa\u3002"}}
{"id": "2510.24102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24102", "abs": "https://arxiv.org/abs/2510.24102", "authors": ["Yihan Wang", "Peiyu Liu", "Runyu Chen", "Jiaxing Pu", "Wei Xu"], "title": "Squrve: A Unified and Modular Framework for Complex Real-World Text-to-SQL Tasks", "comment": null, "summary": "Text-to-SQL technology has evolved rapidly, with diverse academic methods\nachieving impressive results. However, deploying these techniques in real-world\nsystems remains challenging due to limited integration tools. Despite these\nadvances, we introduce Squrve, a unified, modular, and extensive Text-to-SQL\nframework designed to bring together research advances and real-world\napplications. Squrve first establishes a universal execution paradigm that\nstandardizes invocation interfaces, then proposes a multi-actor collaboration\nmechanism based on seven abstracted effective atomic actor components.\nExperiments on widely adopted benchmarks demonstrate that the collaborative\nworkflows consistently outperform the original individual methods, thereby\nopening up a new effective avenue for tackling complex real-world queries. The\ncodes are available at https://github.com/Satissss/Squrve.", "AI": {"tldr": "Squrve: \u4e00\u4e2a\u7edf\u4e00\u3001\u6a21\u5757\u5316\u7684 Text-to-SQL \u6846\u67b6\uff0c\u901a\u8fc7 universal execution paradigm\uff08\u901a\u7528\u6267\u884c\u8303\u5f0f\uff09\u548c\u4e03\u79cd\u539f\u5b50\u89d2\u8272\u7ec4\u4ef6\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u5355\u4e00\u65b9\u6cd5\uff0c\u63a8\u52a8\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u843d\u5730\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d Text-to-SQL \u7684\u843d\u5730\u53d7\u5230\u6709\u9650\u7684\u96c6\u6210\u5de5\u5177\u548c\u4e92\u64cd\u4f5c\u6027\u7684\u9650\u5236\u3002\u672c\u6587\u63d0\u51fa\u7edf\u4e00\u7684\u6267\u884c\u63a5\u53e3\u548c\u57fa\u4e8e\u4e03\u4e2a\u539f\u5b50\u7ec4\u4ef6\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u7814\u7a76\u8fdb\u5c55\u5411\u5b9e\u9645\u573a\u666f\u7684\u8fc1\u79fb\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u63d0\u51fa\u901a\u7528\u6267\u884c\u8303\u5f0f\uff0c\u6807\u51c6\u5316\u8c03\u7528\u63a5\u53e3\uff1b\u57fa\u4e8e\u4e03\u79cd\u62bd\u8c61\u7684\u539f\u5b50\u6267\u884c\u7ec4\u4ef6\u6784\u5efa\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\uff1b\u6784\u5efa\u6a21\u5757\u5316\u6846\u67b6 Squrve \u5e76\u5728\u5e7f\u6cdb\u91c7\u7528\u7684\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u534f\u4f5c\u5de5\u4f5c\u6d41\u5728\u57fa\u51c6\u4e0a\u7a33\u5b9a\u4f18\u4e8e\u539f\u59cb\u7684\u5355\u4e00\u65b9\u6cd5\uff0c\u663e\u793a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u590d\u6742\u771f\u5b9e\u67e5\u8be2\u4e2d\u7684\u6709\u6548\u6027\uff1b\u4ee3\u7801\u5f00\u6e90\u53ef\u83b7\u53d6\u3002", "conclusion": "Squrve \u4e3a Text-to-SQL \u7684\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u4fc3\u8fdb\u7814\u7a76\u8fdb\u5c55\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u7ed3\u5408\uff0c\u5e76\u4e3a\u5e94\u5bf9\u590d\u6742\u73b0\u5b9e\u67e5\u8be2\u5f00\u8f9f\u65b0\u8def\u5f84\u3002"}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "\u63d0\u51fa Boundless Large Model (BLM1)\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u8de8\u6570\u5b57\u4e0e\u7269\u7406\u7a7a\u95f4\u7684\u8de8\u7a7a\u95f4\u3001\u8de8\u4efb\u52a1\u3001\u8de8 embodiment \u6cdb\u5316\u3002\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9636\u6bb5I \u6ce8\u5165\u5177\u4f53\u73b0\u4ee3\u77e5\u8bc6\uff0c\u9636\u6bb5II \u6784\u5efa\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u7684\u7b56\u7565\u6a21\u5757\u5f15\u5bfc\u63a7\u5236\uff0c\u4e14\u80cc\u6808\u6a21\u578b\u4e0d\u5fae\u8c03\u3002\u901a\u8fc7\u56db\u79cd\u673a\u5668\u4eba\u8bbe\u5b9a\u548c\u516d\u9879\u4efb\u52a1\u7684\u81ea\u91c7\u96c6\u6f14\u793a\u96c6\uff0c\u5728\u6570\u5b57\u4efb\u52a1\u4e0a\u6bd4\u5176\u4ed6\u6a21\u578b\u63d0\u5347\u7ea66%\uff0c\u5728\u7269\u7406\u4efb\u52a1\u4e0a\u63d0\u5347\u7ea63%\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u4e0e embodiment \u95f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff1b\u89c6\u89c9\u8bed\u8a00\u884c\u52a8\u6a21\u578b\u53ea\u80fd\u4ea7\u751f\u4f4e\u7ea7\u52a8\u4f5c\uff0c\u96be\u4ee5\u5177\u6709\u9c81\u68d2\u7684\u9ad8\u5c42 embodied \u63a8\u7406\uff1b\u73b0\u6709\u7684ELLMs \u591a\u5904\u4e8e\u6570\u5b57\u7a7a\u95f4\uff0c\u96be\u4ee5\u8fc1\u79fb\u5230\u7269\u7406\u4e16\u754c\u3002\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\u5728\u6570\u5b57/\u7269\u7406\u7a7a\u95f4\u3001\u4e0d\u540c embodiment \u4e0e\u4efb\u52a1\u95f4\u65e0\u7f1d\u5de5\u4f5c\u3002", "method": "\u9636\u6bb5I\uff1a\u901a\u8fc7\u7ecf\u8fc7\u7b5b\u9009\u7684\u6570\u5b57\u8bed\u6599\u6ce8\u5165 embodied \u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\uff1b\u9636\u6bb5II\uff1a\u901a\u8fc7\u4e00\u4e2a\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u8bad\u7ec3\u7b56\u7565\u6a21\u5757\uff0c\u4ece MLLM \u63d0\u53d6\u9ad8\u5c42\u8bed\u4e49\u4ee5\u6307\u5bfc\u63a7\u5236\uff0c\u4e14\u4e0d\u5bf9 MLLM \u4e3b\u5e72\u8fdb\u884c\u5fae\u8c03\u3002\u652f\u6491\u65b9\u6cd5\u4e3a\u81ea\u91c7\u96c6\u7684\u8de8 embodiment \u6f14\u793a\u96c6\uff0c\u6db5\u76d6\u56db\u79cd\u673a\u5668\u4eba embodiment \u4e0e\u516d\u9879\u6e10\u8fdb\u4efb\u52a1\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5355\u4e2a BLM1 \u5b9e\u4f8b\u5728\u6570\u5b57\u4e0e\u7269\u7406\u57fa\u51c6\u4e0a\u5747\u4f18\u4e8e\u56db\u5927\u6a21\u578b\u5bb6\u65cf\uff08MLLMs\u3001ELLMs\u3001VLAs\u3001GMLMs\uff09\uff0c\u6570\u5b57\u4efb\u52a1\u63d0\u5347\u7ea66%\uff0c\u7269\u7406\u4efb\u52a1\u7ea63%\u3002", "conclusion": "BLM1 \u6210\u4e3a\u4e00\u4e2a\u5177\u5907\u8de8\u7a7a\u95f4\u3001\u8de8\u4efb\u52a1\u3001\u8de8 embodiment \u6cdb\u5316\u80fd\u529b\u7684\u5355\u4e00\u6a21\u578b\uff0c\u901a\u8fc7\u5c06 embodied \u77e5\u8bc6\u878d\u5165 MLLM \u4e0e\u9ad8\u5c42\u8bed\u4e49\u5f15\u5bfc\u63a7\u5236\u7684\u7b56\u7565\u6a21\u5757\u7ed3\u5408\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u8de8 embodiment \u63a7\u5236\u4e0e\u4efb\u52a1\u5b8c\u6210\u3002"}}
{"id": "2510.23659", "categories": ["cs.LG", "cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.23659", "abs": "https://arxiv.org/abs/2510.23659", "authors": ["Md. Farhan Shahriyar", "Gazi Tanbhir", "Abdullah Md Raihan Chy"], "title": "Quantum Machine Learning for Image Classification: A Hybrid Model of Residual Network with Quantum Support Vector Machine", "comment": null, "summary": "Recently, there has been growing attention on combining quantum machine\nlearning (QML) with classical deep learning approaches, as computational\ntechniques are key to improving the performance of image classification tasks.\nThis study presents a hybrid approach that uses ResNet-50 (Residual Network)\nfor feature extraction and Quantum Support Vector Machines (QSVM) for\nclassification in the context of potato disease detection. Classical machine\nlearning as well as deep learning models often struggle with high-dimensional\nand complex datasets, necessitating advanced techniques like quantum computing\nto improve classification efficiency. In our research, we use ResNet-50 to\nextract deep feature representations from RGB images of potato diseases. These\nfeatures are then subjected to dimensionality reduction using Principal\nComponent Analysis (PCA). The resulting features are processed through QSVM\nmodels which apply various quantum feature maps such as ZZ, Z, and Pauli-X to\ntransform classical data into quantum states. To assess the model performance,\nwe compared it with classical machine learning algorithms such as Support\nVector Machine (SVM) and Random Forest (RF) using five-fold stratified\ncross-validation for comprehensive evaluation. The experimental results\ndemonstrate that the Z-feature map-based QSVM outperforms classical models,\nachieving an accuracy of 99.23 percent, surpassing both SVM and RF models. This\nresearch highlights the advantages of integrating quantum computing into image\nclassification and provides a potential disease detection solution through\nhybrid quantum-classical modeling.", "AI": {"tldr": "\u5c06 ResNet-50 \u7684\u6df1\u5ea6\u7279\u5f81\u4e0e\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff08QSVM\uff09\u76f8\u7ed3\u5408\uff0c\u7528 PCA \u8fdb\u884c\u964d\u7ef4\u5e76\u5e94\u7528\u4e0d\u540c\u91cf\u5b50\u7279\u5f81\u6620\u5c04\uff08\u5982 ZZ\u3001Z\u3001Pauli-X\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u5728 potato \u75be\u75c5\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86 99.23% \u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u7ecf\u5178\u7684 SVM \u548c RF\u3002", "motivation": "\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u548c\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5728\u9ad8\u7ef4\u6570\u636e\u4e0e\u590d\u6742\u6a21\u5f0f\u4e0a\u53ef\u80fd\u53d7\u9650\u3002\u5f15\u5165\u91cf\u5b50\u8ba1\u7b97\u7684\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6846\u67b6\uff0c\u671f\u671b\u901a\u8fc7\u91cf\u5b50\u7279\u5f81\u6620\u5c04\u63d0\u9ad8\u5206\u7c7b\u6548\u7387\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u98df\u54c1\u4f5c\u7269\u75be\u75c5\u68c0\u6d4b\u7b49\u5e94\u7528\u573a\u666f\u3002", "method": "\u4f7f\u7528 ResNet-50 \u63d0\u53d6 RGB \u56fe\u50cf\u7684\u6df1\u5ea6\u7279\u5f81\uff1b\u5bf9\u7279\u5f81\u8fdb\u884c PCA \u964d\u7ef4\uff1b\u5c06\u964d\u7ef4\u540e\u7684\u7279\u5f81\u8f93\u5165\u5230 QSVM\uff0c\u5e76\u63a2\u7d22\u591a\u79cd\u91cf\u5b50\u7279\u5f81\u6620\u5c04\uff08ZZ\u3001Z\u3001Pauli-X\uff09\u4ee5\u5c06\u6570\u636e\u7f16\u7801\u4e3a\u91cf\u5b50\u6001\uff1b\u901a\u8fc7\u4e94\u6298\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u4e0e\u4f20\u7edf SVM\u3001RF \u8fdb\u884c\u6bd4\u8f83\u3002", "result": "Z \u7279\u5f81\u6620\u5c04\u7684 QSVM \u8d85\u8d8a\u7ecf\u5178\u6a21\u578b\uff0c\u8fbe\u5230 99.23% \u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e SVM \u4e0e RF\u3002", "conclusion": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u5efa\u6a21\u5728\u56fe\u50cf\u5206\u7c7b\uff0c\u5c24\u5176\u662f\u75be\u75c5\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u5728\u4f18\u52bf\uff0c\u663e\u793a\u91cf\u5b50\u7279\u5f81\u6620\u5c04\u5bf9\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.24166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24166", "abs": "https://arxiv.org/abs/2510.24166", "authors": ["Xin Yang", "Yuhang Zhang", "Wei Li", "Xin Lin", "Wenbin Zou", "Chen Xu"], "title": "UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration", "comment": null, "summary": "Motion planning is a critical component of autonomous vehicle decision-making\nsystems, directly determining trajectory safety and driving efficiency. While\ndeep learning approaches have advanced planning capabilities, existing methods\nremain confined to single-dataset training, limiting their robustness in\nplanning.\n  Through systematic analysis, we discover that vehicular trajectory\ndistributions and history-future correlations demonstrate remarkable\nconsistency across different datasets. Based on these findings, we propose\nUniPlanner, the first planning framework designed for multi-dataset integration\nin autonomous vehicle decision-making. UniPlanner achieves unified\ncross-dataset learning through three synergistic innovations.\n  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates\nhistory-future trajectory pairs from multiple datasets, using historical\ntrajectory similarity to retrieve relevant futures and generate cross-dataset\nplanning guidance.\n  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust\nhistory-future correlations from multiple datasets, transforming historical\ntrajectories into universal planning priors. Its gradient-free design ensures\nthe introduction of valuable priors while preventing shortcut learning, making\nthe planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)\nparadigm implements adaptive dropout to selectively suppress planning priors\nduring training for robust learning, while enabling full prior utilization\nduring inference to maximize planning performance.", "AI": {"tldr": "UniPlanner\uff1a\u9996\u4e2a\u9762\u5411\u591a\u6570\u636e\u96c6\u7684\u81ea\u4e3b\u8f66\u9053\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7HFTDN\u3001GFTM\u3001S2D\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u4e0e\u53ef\u8fc1\u79fb\u89c4\u5212\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u89c4\u5212\u65b9\u6cd5\u5f80\u5f80\u5728\u5355\u4e00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u9c81\u68d2\u6027\u53d7\u9650\u3002\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u53d1\u73b0\u4e0d\u540c\u6570\u636e\u96c6\u7684\u8f66\u8f86\u8f68\u8ff9\u5206\u5e03\u4e0e\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\u5177\u6709\u663e\u8457\u7684\u4e00\u81f4\u6027\uff0c\u63ed\u793a\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u5927\u6a21\u5757\uff1a1) History-Future Trajectory Dictionary Network (HFTDN)\uff1a\u4ece\u591a\u4e2a\u6570\u636e\u96c6\u4e2d\u805a\u5408\u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5bf9\uff0c\u901a\u8fc7\u5386\u53f2\u8f68\u8ff9\u76f8\u4f3c\u6027\u68c0\u7d22\u76f8\u5173\u7684\u672a\u6765\u8f68\u8ff9\uff0c\u63d0\u4f9b\u8de8\u6570\u636e\u96c6\u7684\u89c4\u5212\u6307\u5f15\uff1b2) Gradient-Free Trajectory Mapper (GFTM)\uff1a\u5b66\u4e60\u5386\u53f2-\u672a\u6765\u7684\u76f8\u5173\u6027\uff0c\u8f6c\u5316\u5386\u53f2\u8f68\u8ff9\u4e3a\u901a\u7528\u7684\u89c4\u5212\u5148\u9a8c\uff0c\u91c7\u7528\u65e0\u68af\u5ea6\u8bbe\u8ba1\u4ee5\u9632\u6b62\u5feb\u6377\u5b66\u4e60\u5e76\u786e\u4fdd\u77e5\u8bc6\u5b89\u5168\u8fc1\u79fb\uff1b3) Sparse-to-Dense (S2D) paradigm\uff1a\u5728\u8bad\u7ec3\u4e2d\u901a\u8fc7\u81ea\u9002\u5e94 dropout \u6709\u9009\u62e9\u5730\u6291\u5236\u89c4\u5212\u5148\u9a8c\uff0c\u63a8\u7406\u9636\u6bb5\u5219\u5145\u5206\u5229\u7528\u5148\u9a8c\u4ee5\u63d0\u5347\u89c4\u5212\u6027\u80fd\u3002", "result": "\u539f\u6587\u672a\u7ed9\u51fa\u5177\u4f53\u5b9a\u91cf\u7ed3\u679c\uff0c\u4e3b\u8981\u5c55\u793a\u6846\u67b6\u8bbe\u8ba1\u3001\u539f\u7406\u4e0e\u6f5c\u5728\u6548\u679c\uff0c\u9700\u540e\u7eed\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "UniPlanner\u5b9e\u73b0\u4e86\u8de8\u6570\u636e\u96c6\u7684\u7edf\u4e00\u89c4\u5212\u5b66\u4e60\uff0c\u8d4b\u4e88\u89c4\u5212\u77e5\u8bc6\u66f4\u5f3a\u7684\u53ef\u8fc1\u79fb\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u63d0\u5347\u591a\u6570\u636e\u96c6\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u7a33\u5b9a\u6027\u4e0e\u6548\u7387\u3002"}}
{"id": "2510.23660", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23660", "abs": "https://arxiv.org/abs/2510.23660", "authors": ["Gazi Tanbhir", "Md. Farhan Shahriyar", "Abdullah Md Raihan Chy"], "title": "Quanvolutional Neural Networks for Pneumonia Detection: An Efficient Quantum-Assisted Feature Extraction Paradigm", "comment": null, "summary": "Pneumonia poses a significant global health challenge, demanding accurate and\ntimely diagnosis. While deep learning, particularly Convolutional Neural\nNetworks (CNNs), has shown promise in medical image analysis for pneumonia\ndetection, CNNs often suffer from high computational costs, limitations in\nfeature representation, and challenges in generalizing from smaller datasets.\nTo address these limitations, we explore the application of Quanvolutional\nNeural Networks (QNNs), leveraging quantum computing for enhanced feature\nextraction. This paper introduces a novel hybrid quantum-classical model for\npneumonia detection using the PneumoniaMNIST dataset. Our approach utilizes a\nquanvolutional layer with a parameterized quantum circuit (PQC) to process 2x2\nimage patches, employing rotational Y-gates for data encoding and entangling\nlayers to generate non-classical feature representations. These\nquantum-extracted features are then fed into a classical neural network for\nclassification. Experimental results demonstrate that the proposed QNN achieves\na higher validation accuracy of 83.33 percent compared to a comparable\nclassical CNN which achieves 73.33 percent. This enhanced convergence and\nsample efficiency highlight the potential of QNNs for medical image analysis,\nparticularly in scenarios with limited labeled data. This research lays the\nfoundation for integrating quantum computing into deep-learning-driven medical\ndiagnostic systems, offering a computationally efficient alternative to\ntraditional approaches.", "AI": {"tldr": "A hybrid quantum-classical model (Quanvolutional Neural Network) for pneumonia detection on PneumoniaMNIST uses a PQC-based quanvolutional layer on 2x2 patches to extract non-classical features, feeding into a classical NN; achieves higher validation accuracy (83.33%) than a comparable CNN (73.33%).", "motivation": "Address CNN limitations: high computational costs, limited feature representation, and poor generalization on small datasets; explore quantum feature extraction to improve performance and data efficiency in medical image analysis.", "method": "A quanvolutional layer with a parameterized quantum circuit (PQC) processes 2x2 image patches. Data encoding uses rotational Y-gates; entangling layers generate non-classical representations. Quantum features are fed to a classical neural network for classification.", "result": "The proposed QNN achieves validation accuracy of 83.33% on PneumoniaMNIST, outperforming a comparable classical CNN at 73.33%; improved convergence and sample efficiency on limited labeled data.", "conclusion": "Quantum computing can be integrated into deep-learning-based medical diagnostics to provide potentially more efficient feature extraction, offering a computationally efficient alternative to traditional approaches, especially with small datasets."}}
{"id": "2510.24139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24139", "abs": "https://arxiv.org/abs/2510.24139", "authors": ["Chanwoo Park", "Suyoung Park", "Yelim Ahn", "Jongmin Kim", "Jongyeon Park", "Jaejin Lee"], "title": "Beyond Line-Level Filtering for the Pretraining Corpora of LLMs", "comment": "submitted to ACL ARR Rolling Review", "summary": "While traditional line-level filtering techniques, such as line-level\ndeduplication and trailing-punctuation filters, are commonly used, these basic\nmethods can sometimes discard valuable content, negatively affecting downstream\nperformance. In this paper, we introduce two methods-pattern-aware line-level\ndeduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by\nenhancing the conventional filtering techniques. Our approach not only\nconsiders line-level signals but also takes into account their sequential\ndistribution across documents, enabling us to retain structurally important\ncontent that might otherwise be removed. We evaluate these proposed methods by\ntraining small language models (1 B parameters) in both English and Korean. The\nresults demonstrate that our methods consistently improve performance on\nmultiple-choice benchmarks and significantly enhance generative\nquestion-answering accuracy on both SQuAD v1 and KorQuAD v1.", "AI": {"tldr": "Pattern-aware line-level filtering improves data cleaning for LM training, preserving content and boosting QA/MC performance.", "motivation": "Conventional line-level filters risk discarding valuable content and hurting downstream tasks; propose pattern-aware methods to preserve structural content.", "method": "Introduce PLD (pattern-aware line-level deduplication) and PTF (pattern-aware trailing punctuation filtering); extend line-level signals with sequential distribution across documents to retain important content; evaluate by training 1B-parameter English and Korean LMs.", "result": "Consistent improvements on multiple-choice benchmarks; significant gains in generative QA accuracy on SQuAD v1 and KorQuAD v1.", "conclusion": "Pattern-aware filtering enhances data quality for LM pretraining, leading to better cross-lingual QA and MC performance; technique preserves content that conventional filters might remove."}}
{"id": "2510.23663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23663", "abs": "https://arxiv.org/abs/2510.23663", "authors": ["Padmanabhan Jagannathan Prajesh", "Kaliaperumal Ragunath", "Miriam Gordon", "Bruce Rathgeber", "Suresh Neethirajan"], "title": "AI-Driven Carbon Monitoring: Transformer-Based Reconstruction of Atmospheric CO2 in Canadian Poultry Regions", "comment": null, "summary": "Accurate mapping of column-averaged CO2 (XCO2) over agricultural landscapes\nis essential for guiding emission mitigation strategies. We present a\nSpatiotemporal Vision Transformer with Wavelets (ST-ViWT) framework that\nreconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 across\nsouthern Canada, emphasizing poultry-intensive regions. The model fuses wavelet\ntime-frequency representations with transformer attention over meteorology,\nvegetation indices, topography, and land cover. On 2024 OCO-2 data, ST-ViWT\nattains R2 = 0.984 and RMSE = 0.468 ppm; 92.3 percent of gap-filled predictions\nlie within +/-1 ppm. Independent validation with TCCON shows robust\ngeneralization (bias = -0.14 ppm; r = 0.928), including faithful reproduction\nof the late-summer drawdown. Spatial analysis across 14 poultry regions reveals\na moderate positive association between facility density and XCO2 (r = 0.43);\nhigh-density areas exhibit larger seasonal amplitudes (9.57 ppm) and enhanced\nsummer variability. Compared with conventional interpolation and standard\nmachine-learning baselines, ST-ViWT yields seamless 0.25 degree CO2 surfaces\nwith explicit uncertainties, enabling year-round coverage despite sparse\nobservations. The approach supports integration of satellite constraints with\nnational inventories and precision livestock platforms to benchmark emissions,\nrefine region-specific factors, and verify interventions. Importantly,\ntransformer-based Earth observation enables scalable, transparent, spatially\nexplicit carbon accounting, hotspot prioritization, and policy-relevant\nmitigation assessment.", "AI": {"tldr": "\u63d0\u51fa ST-ViWT\uff0c\u5c06\u5c0f\u6ce2\u65f6\u9891\u8868\u793a\u4e0e\u53d8\u6362\u5668\u6ce8\u610f\u529b\u7ed3\u5408\uff0c\u5728 OCO-2 \u6570\u636e\u4e0b\u5bf9\u5357\u90e8\u52a0\u62ff\u5927\u7684 XCO2 \u8fdb\u884c\u8fde\u7eed\u3001\u5e26\u4e0d\u786e\u5b9a\u6027\u7684\u573a\u91cd\u5efa\uff0c\u91cd\u70b9\u5173\u6ce8\u517b\u79bd\u5bc6\u96c6\u533a\u3002", "motivation": "\u5728\u519c\u4e1a\u666f\u89c2\u4e2d\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u3001\u5e26\u4e0d\u786e\u5b9a\u6027\u7684 XCO2 \u65f6\u7a7a\u573a\u91cd\u5efa\uff0c\u4ee5\u652f\u6491\u6392\u653e\u7f13\u89e3\u3001\u6838\u67e5\u4e0e\u533a\u57df\u6cbb\u7406\uff1b\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u63d2\u503c\uff0c\u96be\u4ee5\u6355\u6349\u65f6\u7a7a\u5f02\u8d28\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u96be\u4ee5\u878d\u5408\u536b\u661f\u89c2\u6d4b\u4e0e\u5730\u9762\u89c2\u6d4b\u3002", "method": "\u63d0\u51fa ST-ViWT\uff08\u65f6\u7a7a\u89c6\u89c9\u53d8\u6362\u5668 + \u5c0f\u6ce2\uff09\u6846\u67b6\uff0c\u5c06\u5c0f\u6ce2\u65f6\u9891\u8868\u5f81\u4e0e\u53d8\u6362\u5668\u6ce8\u610f\u529b\u878d\u5165\u5bf9\u6c14\u8c61\u3001\u690d\u88ab\u6307\u6570\u3001\u5730\u5f62\u3001\u571f\u5730\u8986\u76d6\u7b49\u7279\u5f81\u7684\u878d\u5408\uff0c\u4ece OCO-2 \u6570\u636e\u51fa\u53d1\u91cd\u5efa\u8fde\u7eed XCO2 \u573a\u5e76\u7ed9\u51fa\u4e0d\u786e\u5b9a\u6027\u3002\u901a\u8fc7\u72ec\u7acb TCCON \u9a8c\u8bc1\u4ee5\u8bc4\u4f30\u6cdb\u5316\u6027\u4e0e\u533a\u57df\u7279\u5f02\u6027\u3002", "result": "\u5728 2024 \u5e74 OCO-2 \u6570\u636e\u4e0a\uff0cR^2 = 0.984\u3001RMSE = 0.468 ppm\uff1b92.3% \u7684\u7f3a\u53e3\u586b\u8865\u9884\u6d4b\u843d\u5728 +/-1 ppm \u5185\uff1b\u72ec\u7acb TCCON \u9a8c\u8bc1\u504f\u5dee -0.14 ppm\uff0c\u76f8\u5173\u7cfb\u6570 r = 0.928\uff0c\u80fd\u5fe0\u5b9e\u518d\u73b0\u665a\u590f\u7684\u4e0b\u964d\u8d8b\u52bf\uff1b\u5bf9 14 \u4e2a\u517b\u79bd\u533a\u57df\u7684\u7a7a\u95f4\u5206\u6790\u663e\u793a\u8bbe\u65bd\u5bc6\u5ea6\u4e0e XCO2 \u7684\u76f8\u5173\u6027 r = 0.43\uff0c\u9ad8\u5bc6\u5ea6\u533a\u57df\u5448\u73b0\u66f4\u5927\u632f\u5e45\uff089.57 ppm\uff09\u548c\u590f\u5b63\u53d8\u52a8\uff1b\u76f8\u8f83\u4f20\u7edf\u63d2\u503c\u548c\u6807\u51c6\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\uff0cST-ViWT \u63d0\u4f9b 0.25 \u5ea6\u5206\u8fa8\u7387\u7684\u65e0\u7f1d CO2 \u8868\u9762\u5e76\u5305\u542b\u663e\u5f0f\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u89c2\u6d4b\u7a00\u758f\u60c5\u5f62\u4e0b\u7684\u5168\u5e74\u8986\u76d6\u3002", "conclusion": "\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u5730\u7403\u89c2\u6d4b\u65b9\u6cd5\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u900f\u660e\u3001\u7a7a\u95f4\u663e\u5f0f\u7684\u78b3\u6838\u7b97\u3001\u70ed\u70b9\u4f18\u5148\u7ea7\u6392\u5e8f\u4e0e\u9762\u5411\u653f\u7b56\u7684\u51cf\u6392\u8bc4\u4f30\uff1b\u6709\u52a9\u4e8e\u5c06\u536b\u661f\u7ea6\u675f\u4e0e\u56fd\u5bb6\u6e05\u5355\u548c\u7cbe\u51c6\u755c\u7267\u5e73\u53f0\u6574\u5408\uff0c\u4ee5\u57fa\u51c6\u6392\u653e\u3001\u7ec6\u5316\u533a\u57df\u56e0\u7d20\u5e76\u9a8c\u8bc1\u5e72\u9884\u6548\u679c\u3002"}}
{"id": "2510.24150", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24150", "abs": "https://arxiv.org/abs/2510.24150", "authors": ["Chanwoo Park", "Suyoung Park", "JiA Kang", "Jongyeon Park", "Sangho Kim", "Hyunji M. Park", "Sumin Bae", "Mingyu Kang", "Jaejin Lee"], "title": "Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean", "comment": "submitted to ACL ARR Rolling Review", "summary": "We present Ko-MuSR, the first benchmark to comprehensively evaluate\nmultistep, soft reasoning in long Korean narratives while minimizing data\ncontamination. Built following MuSR, Ko-MuSR features fully Korean narratives,\nreasoning chains, and multiple-choice questions verified by human annotators\nfor logical consistency and answerability. Evaluations of four large language\nmodels -- two multilingual and two Korean-specialized -- show that multilingual\nmodels outperform Korean-focused ones even in Korean reasoning tasks,\nindicating cross-lingual generalization of reasoning ability. Carefully\ndesigned prompting strategies, which combine few-shot examples, reasoning\ntraces, and task-specific hints, further boost accuracy, approaching\nhuman-level performance. Ko-MuSR offers a solid foundation for advancing Korean\nNLP by enabling systematic evaluation of long-context reasoning and prompting\nstrategies.", "AI": {"tldr": "Ko-MuSR\u9996\u6b21\u5efa\u7acb\u9762\u5411\u957f\u97e9\u8bed\u53d9\u4e8b\u7684\u591a\u6b65\u8f6f\u63a8\u7406\u57fa\u51c6\uff0c\u9a8c\u8bc1\u591a\u8bed\u6a21\u578b\u4e0e\u97e9\u8bed\u4e13\u578b\u6a21\u578b\u5728\u97e9\u8bed\u63a8\u7406\u4e0a\u7684\u8de8\u8bed\u8a00\u6cdb\u5316\uff0c\u4e14\u63d0\u793a\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u957f\u6587\u60c5\u5883\u4e0b\u8de8\u6b65\u9aa4\u63a8\u7406\u7684\u89c4\u8303\u5316\u8bc4\u6d4b\uff0c\u9700\u5728\u97e9\u8a9e\u8bed\u5883\u4e0b\u51cf\u5c11\u6570\u636e\u6c61\u67d3\uff0c\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u4e0e\u63d0\u793a\u7b56\u7565\u5bf9\u590d\u6742\u63a8\u7406\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1Ko-MuSR\uff1a\u7eaf\u97e9\u8bed\u53d9\u4e8b\u3001\u63a8\u7406\u94fe\u3001\u5e26\u4eba\u5de5\u6838\u9a8c\u7684\u591a\u9879\u9009\u62e9\u9898\uff1b\u56db\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff08\u4e24\u79cd\u591a\u8bed\u6a21\u578b\u3001\u4e24\u79cd\u97e9\u8bed\u4e13\u578b\u6a21\u578b\uff09\u8bc4\u6d4b\uff1b\u4f7f\u7528\u7ed3\u5408\u5c11\u91cf\u793a\u4f8b\u3001\u63a8\u7406\u8f68\u8ff9\u4e0e\u4efb\u52a1\u63d0\u793a\u7684\u63d0\u793a\u7b56\u7565\u3002", "result": "\u591a\u8bed\u6a21\u578b\u5728\u97e9\u8bed\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u97e9\u8bed\u4e13\u578b\u6a21\u578b\uff0c\u663e\u793a\u8de8\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u6cdb\u5316\uff1b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u7b56\u7565\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u3002", "conclusion": "Ko-MuSR\u4e3a\u97e9\u8bedNLP\u63d0\u4f9b\u7a33\u5b9a\u7684\u957f-context\u63a8\u7406\u4e0e\u63d0\u793a\u7b56\u7565\u8bc4\u4f30\u57fa\u7ebf\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5bf9\u957f\u6587\u672c\u63a8\u7406\u548c\u63d0\u793a\u4f18\u5316\u7684\u7cfb\u7edf\u7814\u7a76\u3002"}}
{"id": "2510.23665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23665", "abs": "https://arxiv.org/abs/2510.23665", "authors": ["Juan C. Leon Alcazar", "Mattia Soldan", "Mohammad Saatialsoruji", "Alejandro Pardo", "Hani Itani", "Juan Camilo Perez", "Bernard Ghanem"], "title": "Transformers from Compressed Representations", "comment": null, "summary": "Compressed file formats are the corner stone of efficient data storage and\ntransmission, yet their potential for representation learning remains largely\nunderexplored. We introduce TEMPEST (TransformErs froM comPressed\nrEpreSenTations), a method that exploits the inherent byte-stream structure of\ncompressed files to design an effective tokenization and encoding strategy. By\nleveraging this compact encoding, a standard transformer can directly learn\nsemantic representations from compressed data streams, bypassing the need for\nraw byte-level processing or full media decoding. Our proposal substantially\nreduces the number of tokens required for semantic classification, thereby\nlowering both computational complexity and memory usage. Through extensive\nexperiments across diverse datasets, coding schemes, and modalities, we show\nthat TEMPEST achieves accuracy competitive wit the state-of-the-art while\ndelivering efficiency gains in memory and compute.", "AI": {"tldr": "TEMPEST is a transformer-based approach that tokenizes and encodes compressed file byte-streams to learn semantic representations directly from compressed data, reducing token counts and computational resources while achieving competitive accuracy.", "motivation": "Compressed file formats are central to storage/transmission efficiency, but their potential for representation learning remains largely untapped. Learning from compressed streams could bypass full decoding while preserving semantics.", "method": "Design a tokenization/encoding strategy that leverages the inherent byte-stream structure of compressed files, enabling a standard transformer to learn semantic representations directly from compressed data without raw byte processing or decoding.", "result": "TEMPEST achieves accuracy competitive with state-of-the-art while delivering efficiency gains in memory and compute, due to requiring fewer tokens. Extensive experiments show robustness across datasets, coding schemes, and modalities.", "conclusion": "Demonstrates that semantic representations can be learned directly from compressed data streams using transformers, offering practical efficiency benefits for diverse modalities."}}
{"id": "2510.24178", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24178", "abs": "https://arxiv.org/abs/2510.24178", "authors": ["Aaron Scott", "Maike Z\u00fcfle", "Jan Niehues"], "title": "MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations", "comment": null, "summary": "Sarcasm is a complex form of figurative language in which the intended\nmeaning contradicts the literal one. Its prevalence in social media and popular\nculture poses persistent challenges for natural language understanding,\nsentiment analysis, and content moderation. With the emergence of multimodal\nlarge language models, sarcasm detection extends beyond text and requires\nintegrating cues from audio and vision. We present MuSaG, the first German\nmultimodal sarcasm detection dataset, consisting of 33 minutes of manually\nselected and human-annotated statements from German television shows. Each\ninstance provides aligned text, audio, and video modalities, annotated\nseparately by humans, enabling evaluation in unimodal and multimodal settings.\nWe benchmark nine open-source and commercial models, spanning text, audio,\nvision, and multimodal architectures, and compare their performance to human\nannotations. Our results show that while humans rely heavily on audio in\nconversational settings, models perform best on text. This highlights a gap in\ncurrent multimodal models and motivates the use of MuSaG for developing models\nbetter suited to realistic scenarios. We release MuSaG publicly to support\nfuture research on multimodal sarcasm detection and human-model alignment.", "AI": {"tldr": "\u63d0\u51faMuSaG\uff1a\u9996\u4e2a\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5305\u542b33\u5206\u949f\u5fb7\u8bed\u7535\u89c6\u8282\u76ee\u7684\u4eba\u7c7b\u6807\u6ce8\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u9891\uff0c\u652f\u6301\u5355\u6a21\u6001\u4e0e\u591a\u6a21\u6001\u8bc4\u4f30\uff1b\u5bf9\u4e5d\u79cd\u6a21\u578b\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u53d1\u73b0\u4eba\u7c7b\u504f\u5411\u4f7f\u7528\u97f3\u9891\u7ebf\u7d22\uff0c\u6a21\u578b\u5728\u6587\u672c\u4e0a\u8868\u73b0\u6700\u597d\uff0c\u63ed\u793a\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u7684\u5dee\u8ddd\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002", "motivation": "\u8bbd\u523a\u662f\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u96be\u70b9\uff0c\u5c24\u5176\u5728\u793e\u4ea4\u5a92\u4f53\u73af\u5883\u4e2d\u3002\u591a\u6a21\u6001\u5927\u6a21\u578b\u9700\u8981\u7ed3\u5408\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u9891\u7ebf\u7d22\u6765\u8fdb\u884c\u8bbd\u523a\u68c0\u6d4b\uff1b\u5fb7\u8bed\u8d44\u6e90\u532e\u4e4f\uff0c\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u5355\u6a21\u6001\u6216\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u4eba\u7c7b\u6807\u6ce8\u3002\u672c\u6587\u52a8\u673a\u5728\u4e8e\u63d0\u4f9b\u4e00\u4e2a\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u6570\u636e\u96c6\uff0c\u4fc3\u4f7f\u6a21\u578b\u63d0\u5347\u5bf9\u97f3\u9891\u548c\u89c6\u89c9\u7ebf\u7d22\u7684\u5229\u7528\uff0c\u4ee5\u63d0\u5347\u4eba\u673a\u5bf9\u9f50\u4e0e\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u3002", "method": "\u6570\u636e\u96c6\u6784\u5efa\uff1a\u9009\u53d633\u5206\u949f\u7684\u5fb7\u8bed\u7535\u89c6\u8282\u76ee\u7247\u6bb5\uff0c\u8fdb\u884c\u9010\u53e5\u5bf9\u9f50\u7684\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u9891\u6a21\u6001\u6807\u6ce8\uff0c\u5e76\u7531\u591a\u540d annotator \u72ec\u7acb\u6807\u6ce8\u3002\u8bc4\u4f30\u7b56\u7565\uff1a\u57fa\u4e8e\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u89c9\u53ca\u5176\u7ec4\u5408\u7684\u4e5d\u79cd\u5f00\u6e90/\u5546\u7528\u6a21\u578b\u8fdb\u884c\u57fa\u7ebf\u8bc4\u4f30\uff0c\u5e76\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8fdb\u884c\u5bf9\u6bd4\u3002\u6570\u636e\u516c\u5f00\u53d1\u884c\u4ee5\u4fbf\u540e\u7eed\u7814\u7a76\u3002", "result": "\u4eba\u7c7b\u5728\u5bf9\u8bdd\u573a\u666f\u4e2d\u5bf9\u97f3\u9891\u7ebf\u7d22\u7684\u4f9d\u8d56\u8f83\u5f3a\uff1b\u6a21\u578b\u5728\u6587\u672c\u6a21\u6001\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u6574\u4f53\u591a\u6a21\u6001\u7cfb\u7edf\u5c1a\u672a\u5145\u5206\u5229\u7528\u97f3\u9891\u548c\u89c6\u9891\u7ebf\u7d22\uff0c\u5bfc\u81f4\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u5dee\u8ddd\u4ecd\u7136\u5b58\u5728\u3002\u4e5d\u79cd\u6a21\u578b\u7684\u57fa\u7ebf\u7ed3\u679c\u63ed\u793a\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u60c5\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u597d\u5730\u878d\u5408\u97f3\u9891/\u89c6\u89c9\u4fe1\u606f\u4ee5\u5b9e\u73b0\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u8bbd\u523a\u68c0\u6d4b\u3002", "conclusion": "MuSaG\u88ab\u516c\u5f00\u53d1\u5e03\uff0c\u4f5c\u4e3a\u4fc3\u8fdb\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u4e0e\u4eba\u673a\u5bf9\u9f50\u7814\u7a76\u7684\u57fa\u51c6\u8d44\u6e90\uff0c\u672a\u6765\u5de5\u4f5c\u5e94\u52a0\u5f3a\u5bf9\u97f3\u9891\u4e0e\u89c6\u9891\u4fe1\u53f7\u7684\u6709\u6548\u6574\u5408\uff0c\u4ee5\u53ca\u5728\u66f4\u5e7f\u6cdb\u8bed\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u4e0e\u516c\u5e73\u6027\u8bc4\u4f30\u3002"}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u5728 MCTS \u7684\u62bd\u8c61\u65b9\u6cd5\u4e2d\uff0c\u5f53\u540c\u4e00\u7236\u8282\u70b9\u4e0b\u7684\u591a\u6761\u52a8\u4f5c\u6620\u5c04\u5230\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u65f6\uff0c\u4f1a\u4ea7\u751f\u76f8\u540c\u7684 UCB \u503c\uff0c\u5bfc\u81f4\u9700\u8981\u6253\u7834\u5e73\u5c40\u7684\u7b56\u7565\uff0c\u4f46\u73b0\u6709\u5982 pruned OGA \u672a\u6ce8\u610f\u5230\u8fd9\u4e00\u70b9\uff0c\u9ed8\u8ba4\u91c7\u7528\u968f\u673a\u6253\u7834\u3002\u8bba\u6587\u63d0\u51fa\u5e76\u5b9e\u9a8c\u8bc4\u4f30\u591a\u79cd\u201c\u540c\u62bd\u8c61\u5185\u7684\u6253\u7834\u7b56\u7565\u201d\uff0c\u5176\u4e2d\u82e5\u5e72\u7b56\u7565\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u5b9a\u4e0b\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "motivation": "\u63d0\u9ad8 MCTS \u7684\u6837\u672c\u6548\u7387\uff0c\u5229\u7528\u72b6\u6001/\u52a8\u4f5c\u62bd\u8c61\u6765\u5171\u4eab\u4fe1\u606f\u3002\u4f46\u73b0\u6709\u62bd\u8c61\u76f4\u63a5\u7528\u4e8e UCB \u65f6\u5ffd\u7565\u4e86\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u4e0b\u591a\u4e2a\u52a8\u4f5c\u4f1a\u5171\u4eab\u76f8\u540c UCB \u7684\u60c5\u5f62\uff0c\u9700\u5f15\u5165\u66f4\u6709\u6548\u7684\u540c\u62bd\u8c61\u5185\u6253\u7834\u7b56\u7565\uff1b\u5728\u73b0\u6709\u7684 pruned OGA \u7b49\u65b9\u6cd5\u4e2d\u672a\u5145\u5206\u5173\u6ce8\u8fd9\u4e00\u70b9\uff0c\u5f80\u5f80\u4ee5\u968f\u673a\u6253\u7834\u4e3a\u9ed8\u8ba4\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u591a\u79cd intra-abstraction policy\uff08\u540c\u62bd\u8c61\u5185\u7684\u6253\u7834\u7b56\u7565\uff09\u7528\u4e8e\u5728\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u5185\u5bf9\u52a8\u4f5c\u8fdb\u884c\u533a\u5206\u4e0e\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u8fd9\u4e9b\u7b56\u7565\u76f8\u5bf9\u4e8e\u968f\u673a\u6253\u7834\u5728\u4e0d\u540c\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u7684\u82e5\u5e72 intra-abstraction policy \u5728\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u5b9a\u4e0b\u4f18\u4e8e\u968f\u673a\u6253\u7834\u7b56\u7565\uff0c\u63d0\u5347\u4e86 MCTS \u7684\u6027\u80fd\u3002", "conclusion": "\u540c\u62bd\u8c61\u5185\u7684\u6253\u7834\u7b56\u7565\u5bf9\u63d0\u5347 MCTS \u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u5177\u6709\u663e\u8457\u4f5c\u7528\uff0c\u63ed\u793a\u4e86 pruned OGA \u7b49\u65b9\u6cd5\u4e2d\u672a\u5145\u5206\u8003\u8651\u7684\u6253\u7834\u60c5\u5f62\uff0c\u5efa\u8bae\u5728\u62bd\u8c61\u7ed3\u5408\u4e2d\u5f15\u5165\u66f4\u5408\u7406\u7684 intra-abstraction \u7b56\u7565\u4ee5\u6539\u8fdb\u641c\u7d22\u6548\u7387\u3002"}}
{"id": "2510.23667", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.23667", "abs": "https://arxiv.org/abs/2510.23667", "authors": ["Amin Heyrani Nobari", "Lyle Regenwetter", "Cyril Picard", "Ligong Han", "Faez Ahmed"], "title": "Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization", "comment": null, "summary": "Structural topology optimization (TO) is central to engineering design but\nremains computationally intensive due to complex physics and hard constraints.\nExisting deep-learning methods are limited to fixed square grids, a few\nhand-coded boundary conditions, and post-hoc optimization, preventing general\ndeployment. We introduce Optimize Any Topology (OAT), a foundation-model\nframework that directly predicts minimum-compliance layouts for arbitrary\naspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines\na resolution- and shape-agnostic autoencoder with an implicit neural-field\ndecoder and a conditional latent-diffusion model trained on OpenTO, a new\ncorpus of 2.2 million optimized structures covering 2 million unique\nboundary-condition configurations. On four public benchmarks and two\nchallenging unseen tests, OAT lowers mean compliance up to 90% relative to the\nbest prior models and delivers sub-1 second inference on a single GPU across\nresolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These\nresults establish OAT as a general, fast, and resolution-free framework for\nphysics-aware topology optimization and provide a large-scale dataset to spur\nfurther research in generative modeling for inverse design. Code & data can be\nfound at https://github.com/ahnobari/OptimizeAnyTopology.", "AI": {"tldr": "\u63d0\u51fa Optimize Any Topology (OAT)\u2014\u2014\u4e00\u4e2a\u9762\u5411\u62d3\u6251\u4f18\u5316\u7684\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4efb\u610f\u7eb5\u6a2a\u6bd4\u3001\u5206\u8fa8\u7387\u3001\u4f53\u79ef\u5206\u6570\u3001\u8f7d\u8377\u4e0e\u5939\u5177\u6761\u4ef6\u4e0b\u76f4\u63a5\u9884\u6d4b\u6700\u5c0f\u5408\u89c4\u6027\u62d3\u6251\u5e03\u5c40\uff0c\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u5927\u89c4\u6a21\u6570\u636e\u96c6 OpenTO\u3002", "motivation": "\u62d3\u6251\u4f18\u5316\u5728\u5de5\u7a0b\u8bbe\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u7f51\u683c\u3001\u53d7\u9650\u8fb9\u754c\u6761\u4ef6\u53ca\u540e\u5904\u7406\u4f18\u5316\uff0c\u96be\u4ee5\u5b9e\u73b0\u5e7f\u6cdb\u90e8\u7f72\u3002", "method": "\u63d0\u51fa OAT\uff0c\u7ed3\u5408\u4e00\u4e2a\u5206\u8fa8\u7387\u4e0e\u5f62\u72b6\u65e0\u5173\u7684\u81ea\u7f16\u7801\u5668\u3001\u4e00\u4e2a\u9690\u5f0f\u795e\u573a\u89e3\u7801\u5668\u548c\u4e00\u4e2a\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff1b\u5728\u5305\u542b 2.2 \u767e\u4e07\u4f18\u5316\u7ed3\u6784\u7684 OpenTO \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u8be5\u6570\u636e\u8986\u76d6\u7ea6 200 \u4e07\u79cd\u8fb9\u754c\u6761\u4ef6\u914d\u7f6e\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\u548c\u4e24\u7ec4\u672a\u89c1\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u5408\u89c4\u6027\u76f8\u8f83\u4e8e\u5148\u524d\u6700\u4f73\u6a21\u578b\u4e0b\u964d\u6700\u591a\u53ef\u8fbe\u7ea690%\uff1b\u5728\u5355 GPU \u4e0a\u7684\u63a8\u7406\u65f6\u95f4\u5c0f\u4e8e 1 \u79d2\uff0c\u8986\u76d6 64\u00d764 \u81f3 256\u00d7256 \u7684\u5206\u8fa8\u7387\u3001\u7eb5\u6a2a\u6bd4\u9ad8\u8fbe 10:1 \u7684\u573a\u666f\u5747\u53ef\u5904\u7406\u3002", "conclusion": "OAT \u662f\u4e00\u4e2a\u901a\u7528\u3001\u5feb\u901f\u3001\u5206\u8fa8\u7387\u65e0\u5173\u7684\u7269\u7406\u611f\u77e5\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u9006\u8bbe\u8ba1\u9886\u57df\u7684\u751f\u6210\u6a21\u578b\u7814\u7a76\uff1b\u4ee3\u7801\u4e0e\u6570\u636e\u516c\u5f00\u4e8e GitHub\u3002"}}
{"id": "2510.24179", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24179", "abs": "https://arxiv.org/abs/2510.24179", "authors": ["Iv\u00e1n Mart\u00ednez-Murillo", "Paloma Moreda", "Elena Lloret"], "title": "Exploring the Influence of Relevant Knowledge for Natural Language Generation Interpretability", "comment": null, "summary": "This paper explores the influence of external knowledge integration in\nNatural Language Generation (NLG), focusing on a commonsense generation task.\nWe extend the CommonGen dataset by creating KITGI, a benchmark that pairs input\nconcept sets with retrieved semantic relations from ConceptNet and includes\nmanually annotated outputs. Using the T5-Large model, we compare sentence\ngeneration under two conditions: with full external knowledge and with filtered\nknowledge where highly relevant relations were deliberately removed. Our\ninterpretability benchmark follows a three-stage method: (1) identifying and\nremoving key knowledge, (2) regenerating sentences, and (3) manually assessing\noutputs for commonsense plausibility and concept coverage. Results show that\nsentences generated with full knowledge achieved 91\\% correctness across both\ncriteria, while filtering reduced performance drastically to 6\\%. These\nfindings demonstrate that relevant external knowledge is critical for\nmaintaining both coherence and concept coverage in NLG. This work highlights\nthe importance of designing interpretable, knowledge-enhanced NLG systems and\ncalls for evaluation frameworks that capture the underlying reasoning beyond\nsurface-level metrics.", "AI": {"tldr": "\u5916\u90e8\u77e5\u8bc6\u6574\u5408\u5bf9\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff08NLG\uff09\u4e2d\u7684\u5e38\u8bc6\u6027\u4ea7\u51fa\u6709\u663e\u8457\u5f71\u54cd\uff1b\u901a\u8fc7 KITGI \u6570\u636e\u96c6\u6269\u5c55 CommonGen\uff0c\u7ed3\u5408 ConceptNet \u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u4f7f\u7528 T5-Large\uff0c\u5728\u5145\u8db3\u77e5\u8bc6\u4e0e\u53bb\u9664\u5173\u952e\u76f8\u5173\u77e5\u8bc6\u7684\u4e24\u79cd\u6761\u4ef6\u4e0b\u8fdb\u884c\u5bf9\u6bd4\uff1b\u7ed3\u679c\u663e\u793a\u5b8c\u6574\u77e5\u8bc6\u4e0b\u7684\u6b63\u786e\u6027\u8fbe\u5230 91%\uff0c\u8fc7\u6ee4\u77e5\u8bc6\u540e\u964d\u81f3 6%\uff0c\u5f3a\u8c03\u76f8\u5173\u5916\u90e8\u77e5\u8bc6\u5bf9\u4fdd\u6301\u8fde\u8d2f\u6027\u4e0e\u6982\u5ff5\u8986\u76d6\u7684\u91cd\u8981\u6027\uff1b\u5e76\u63d0\u51fa\u9700\u8981\u53ef\u89e3\u91ca\u7684\u77e5\u8bc6\u589e\u5f3a\u578b NLG \u7cfb\u7edf\u4e0e\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u5916\u90e8\u77e5\u8bc6\u5728\u5e38\u8bc6\u6027NLG\u4e2d\u7684\u4f5c\u7528\uff0c\u6269\u5c55\u6570\u636e\u96c6\u4ee5\u652f\u6301\u77e5\u8bc6\u68c0\u7d22\u4e0e\u6574\u5408\u7684\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u65b9\u6cd5\u6765\u63ed\u793a\u77e5\u8bc6\u5982\u4f55\u5f71\u54cd\u751f\u6210\u7ed3\u679c\u3002", "method": "\u5efa\u7acb KITGI \u57fa\u51c6\uff1a\u7ed9\u5b9a\u8f93\u5165\u6982\u5ff5\u96c6\u5e76\u68c0\u7d22 ConceptNet \u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u9644\u5e26\u4eba\u5de5\u6807\u6ce8\u7684\u8f93\u51fa\uff1b\u4f7f\u7528 T5-Large \u8fdb\u884c\u4e24\u79cd\u8bbe\u5b9a\uff1a\u5b8c\u6574\u77e5\u8bc6\u4e0e\u7b5b\u9009\u540e\u5220\u9664\u9ad8\u5ea6\u76f8\u5173\u5173\u7cfb\u7684\u77e5\u8bc6\uff1b\u91c7\u7528\u4e09\u9636\u6bb5\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\uff1a1\uff09\u8bc6\u522b\u5e76\u5220\u9664\u5173\u952e\u77e5\u8bc6\uff0c2\uff09\u91cd\u65b0\u751f\u6210\u53e5\u5b50\uff0c3\uff09\u4eba\u5de5\u8bc4\u4f30\u8f93\u51fa\u7684\u5e38\u8bc6\u6027\u53ef\u4fe1\u5ea6\u4e0e\u6982\u5ff5\u8986\u76d6\uff1b", "result": "\u5b8c\u6574\u77e5\u8bc6\u6761\u4ef6\u4e0b\uff0c\u751f\u6210\u53e5\u5b50\u5728\u4e24\u9879\u8bc4\u4f30\u6807\u51c6\u4e0a\u6b63\u786e\u7387\u8fbe 91%\uff1b\u8fc7\u6ee4\u540e\u4ec5\u5269\u7ea6 6% \u7684\u6b63\u786e\u7387\uff1b\u8bc1\u5b9e\u5916\u90e8\u77e5\u8bc6\u7684\u76f8\u5173\u6027\u5bf9\u4fdd\u6301\u8fde\u8d2f\u6027\u4e0e\u6982\u5ff5\u8986\u76d6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5916\u90e8\u77e5\u8bc6\u5bf9 NLG \u7684\u8d28\u91cf\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u9700\u53d1\u5c55\u53ef\u89e3\u91ca\u7684\u77e5\u8bc6\u589e\u5f3a\u578b NLG \u7cfb\u7edf\u53ca\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u8d85\u8d8a\u8868\u9762\u6307\u6807\u8bc4\u4f30\u5e76\u63ed\u793a\u6f5c\u5728\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "Self-Indicator: \u7528\u8f93\u5165\u4e0e\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u77e9\u9635\u79e9\u6765\u8bc4\u4f30\u63a8\u7406\u6b63\u786e\u6027\uff0c\u4ece\u800c\u5bf9\u5019\u9009\u63a8\u7406\u8def\u5f84\u8fdb\u884c\u81ea\u9002\u5e94\u52a0\u6743\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u5f00\u9500\u7684\u81ea\u9a8c\u8bc1\u548c\u63d0\u5347\u591a\u9879\u57fa\u51c6\u7684\u51c6\u786e\u6027\u3002", "motivation": "LLMs \u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u548c\u5e7b\u89c9\uff0c\u4e9f\u9700\u9ad8\u6548\u4e14\u4e0d\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\u7684\u8f93\u51fa\u6821\u9a8c\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\u597d\u7684\u9a8c\u8bc1\u5668\u6216\u590d\u6742\u63d0\u793a\uff0c\u4ee3\u4ef7\u9ad8\u4e14\u57df\u4f9d\u8d56\u6027\u5f3a\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u8f93\u5165\u95ee\u9898\u4e0e\u6bcf\u6761\u63a8\u7406\u8def\u5f84\u7684\u76f8\u5173\u77e9\u9635\u7684\u79e9\uff0c\u5c06\u5176\u4f5c\u4e3a\u63a8\u7406\u6b63\u786e\u6027\u7684\u9c81\u68d2\u6307\u793a\u5668\uff1b\u8be5\u6307\u6807\u5b8c\u5168\u6765\u81ea\u6a21\u578b\u5185\u90e8\uff0c\u65e0\u9700\u8bad\u7ec3\u989d\u5916\u6a21\u578b\u3002\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u53ef\u63d2\u62d4\u7684 Self-Indicator\uff0c\u5bf9\u5019\u9009\u63a8\u7406\u8def\u5f84\u8fdb\u884c\u518d\u52a0\u6743\uff0c\u5728\u591a\u79cd\u6a21\u578b\u5c3a\u5ea6\u548c\u5bb6\u65cf\u4e0a\u5b9e\u73b0\u663e\u8457\u6539\u8fdb\uff0c\u5177\u6709\u975e\u5e38\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u8def\u5f84\u65b9\u9762\u8fbe\u5230>75% \u7684\u51c6\u786e\u7387\uff1b\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u5ea6\u63d0\u5347\u8d85\u8fc78%\u3002", "conclusion": "Self-Indicator \u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u6709\u6548\u4e14\u4f4e\u5f00\u9500\u7684\u81ea\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5177\u5907\u826f\u597d\u6cdb\u5316\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u89c4\u6a21\u7684 LLMs \u4e0a\u65e0\u7f1d\u5e94\u7528\uff0c\u907f\u514d\u989d\u5916\u8d44\u6e90\u6295\u5165\u3002"}}
{"id": "2510.23668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23668", "abs": "https://arxiv.org/abs/2510.23668", "authors": ["Fujiang Yuan", "Yangrui Fan", "Xiaohuan Bing", "Zhen Tian", "Chunhong Yuan", "Yankang Li"], "title": "Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems", "comment": null, "summary": "Accurate traffic flow forecasting is essential for intelligent transportation\nsystems and urban traffic management. However, single model approaches often\nfail to capture the complex, nonlinear, and multi scale temporal patterns in\ntraffic flow data. This study proposes a decomposition driven hybrid framework\nthat integrates Seasonal Trend decomposition using Loess (STL) with three\ncomplementary predictive models. STL first decomposes the original time series\ninto trend, seasonal, and residual components. Then, a Long Short Term Memory\n(LSTM) network models long term trends, an Autoregressive Integrated Moving\nAverage (ARIMA) model captures seasonal periodicity, and an Extreme Gradient\nBoosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The\nfinal forecast is obtained through multiplicative integration of the sub model\npredictions. Using 998 traffic flow records from a New York City intersection\nbetween November and December 2015, results show that the LSTM ARIMA XGBoost\nhybrid model significantly outperforms standalone models including LSTM, ARIMA,\nand XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy\neffectively isolates temporal characteristics, allowing each model to\nspecialize, thereby improving prediction accuracy, interpretability, and\nrobustness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e STL \u5206\u89e3\u7684\u6df7\u5408\u6a21\u578b\u6846\u67b6\uff0c\u5c06 STL \u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u4e0e\u6b8b\u5dee\uff0c\u5206\u522b\u7528 LSTM\u3001ARIMA\u3001XGBoost \u5efa\u6a21\uff0c\u518d\u901a\u8fc7\u4e58\u6cd5\u65b9\u5f0f\u6574\u5408\u9884\u6d4b\uff0c\u63d0\u5347\u4ea4\u901a\u6d41\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4ea4\u901a\u6d41\u6570\u636e\u5177\u6709\u590d\u6742\u3001\u975e\u7ebf\u6027\u548c\u591a\u5c3a\u5ea6\u7684\u65f6\u5e8f\u6a21\u5f0f\uff0c\u5355\u4e00\u6a21\u578b\u5f80\u5f80\u96be\u4ee5\u540c\u65f6\u6355\u6349\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u4e0e\u975e\u7ebf\u6027\u6b8b\u5dee\uff0c\u9700\u901a\u8fc7\u5206\u89e3\u4e0e\u4e13\u95e8\u5316\u6a21\u578b\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u4e0e\u89e3\u91ca\u6027\u3002", "method": "\u5bf9\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c STL \u5206\u89e3\uff0c\u5f97\u5230\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u4e0e\u6b8b\u5dee\uff1b\u7528 LSTM \u5efa\u6a21\u8d8b\u52bf\u3001\u7528 ARIMA \u6355\u6349\u5b63\u8282\u6027\u3001\u7528 XGBoost \u9884\u6d4b\u975e\u7ebf\u6027\u6b8b\u5dee\uff0c\u7136\u540e\u5bf9\u5b50\u6a21\u578b\u9884\u6d4b\u8fdb\u884c\u4e58\u6cd5\u6574\u5408\uff0c\u5f97\u5230\u6700\u7ec8\u9884\u6d4b\u3002\u6570\u636e\u6765\u81ea 2015 \u5e74 11\u201312 \u6708\u7ebd\u7ea6\u5e02\u4e00\u4e2a\u4ea4\u53c9\u8def\u53e3\u7684 998 \u6761\u4ea4\u901a\u6d41\u8bb0\u5f55\u3002", "result": "\u4e0e\u5355\u72ec\u7684 LSTM\u3001ARIMA\u3001XGBoost \u76f8\u6bd4\uff0cLSTM-ARIMA-XGBoost \u6df7\u5408\u6a21\u578b\u5728 MAE\u3001RMSE \u4e0e R^2 \u7b49\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8bf4\u660e\u5206\u89e3\u7b56\u7565\u6709\u52a9\u4e8e\u628a\u65f6\u5e8f\u7279\u5f81\u9010\u6b65\u63d0\u53d6\u5e76\u7531\u4e13\u95e8\u6a21\u578b\u5904\u7406\u3002", "conclusion": "\u57fa\u4e8e\u5206\u89e3\u7684\u6df7\u5408\u5efa\u6a21\u5bf9\u4ea4\u901a\u6d41\u9884\u6d4b\u5177\u6709\u663e\u8457\u6709\u6548\u6027\uff0c\u5229\u7528 STL \u5c06\u65f6\u95f4\u5e8f\u5217\u62c6\u5206\u540e\u8ba9\u4e0d\u540c\u6a21\u578b\u4e13\u6ce8\u5404\u81ea\u6210\u5206\uff0c\u4ece\u800c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24208", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24208", "abs": "https://arxiv.org/abs/2510.24208", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "title": "Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment", "comment": "an early-stage version", "summary": "Large Language Models (LLMs) encode vast amounts of knowledge in their\nmassive parameters, which is accessible to locate, trace, and analyze. Despite\nadvances in neural interpretability, it is still not clear how to transfer\nknowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).\nA key problem is enabling effective and efficient knowledge transfer across\nLLMs of different scales, which is essential for achieving greater flexibility\nand broader applicability in transferring knowledge between LLMs. Due to neural\nincompatibility, referring to the architectural and parametric differences\nbetween LLMs of varying scales, existing methods that directly reuse layer\nparameters are severely limited. In this paper, we identify the semantic\nalignment in latent space as the fundamental prerequisite for LLM cross-scale\nknowledge transfer. Instead of directly using the layer parameters, our\napproach takes activations as the medium of layer-wise knowledge transfer.\nLeveraging the semantics in latent space, our approach is simple and\noutperforms prior work, better aligning model behaviors across varying scales.\nEvaluations on four benchmarks demonstrate the efficacy of our method. Further\nanalysis reveals the key factors easing cross-scale knowledge transfer and\nprovides insights into the nature of latent semantic alignment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6fc0\u6d3b\u7684\u8de8\u5c3a\u5ea6\u53c2\u6570\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\uff0c\u5229\u7528\u6f5c\u5728\u8bed\u4e49\u5bf9\u9f50\u4f5c\u4e3a\u524d\u63d0\u6761\u4ef6\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u7684LLM\u4e4b\u95f4\u5b9e\u73b0\u77e5\u8bc6\u7684\u5c42\u7ea7\u4f20\u9012\uff0c\u6548\u679c\u4f18\u4e8e\u76f4\u63a5\u590d\u7528\u5c42\u53c2\u6570\u7684\u65b9\u6cd5\u3002", "motivation": "LLMs\u5305\u542b\u5927\u91cf\u77e5\u8bc6\uff0c\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u9762\u4e34\u67b6\u6784\u4e0e\u53c2\u6570\u5dee\u5f02\u5bfc\u81f4\u7684\u76f4\u63a5\u53c2\u6570\u590d\u7528\u4e0d\u53ef\u884c\u6027\u3002\u9700\u8981\u5728\u6f5c\u5728\u8bed\u4e49\u5c42\u9762\u5b9e\u73b0\u5bf9\u9f50\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u3002", "method": "\u4e0d\u76f4\u63a5\u590d\u7528\u5c42\u53c2\u6570\uff0c\u800c\u4ee5\u6fc0\u6d3b\u4f5c\u4e3a\u77e5\u8bc6\u4f20\u9012\u7684\u4ecb\u8d28\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\u6765\u5b8c\u6210\u8de8\u5c3a\u5ea6\u7684\u5c42\u7ea7\u77e5\u8bc6\u8f6c\u79fb\u3002\u8be5\u505a\u6cd5\u5229\u7528\u8bed\u4e49\u5c42\u9762\u7684\u5bf9\u9f50\u6765\u4f7f\u4e0d\u540c\u5c3a\u5ea6\u7684\u6a21\u578b\u884c\u4e3a\u4e00\u81f4\u6027\u66f4\u9ad8\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6240\u63d0\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u524d\u4eba\u65b9\u6cd5\uff0c\u4e14\u5206\u6790\u63ed\u793a\u4e86\u5f71\u54cd\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u5bf9\u6f5c\u5728\u8bed\u4e49\u5bf9\u9f50\u7684\u672c\u8d28\u63d0\u4f9b\u89c1\u89e3\u3002", "conclusion": "\u8574\u542b\u5728\u6f5c\u5728\u8bed\u4e49\u5bf9\u9f50\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u662f\u5b9e\u73b0\u8de8\u5c3a\u5ea6PKT\u7684\u6839\u672c\u524d\u63d0\uff1b\u57fa\u4e8e\u6fc0\u6d3b\u7684\u4f20\u9012\u65b9\u6cd5\u7b80\u6d01\u800c\u6709\u6548\uff0c\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.23671", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23671", "abs": "https://arxiv.org/abs/2510.23671", "authors": ["Marmik Chaudhari", "Jeremi Nuer", "Rome Thorstenson"], "title": "Sparsity and Superposition in Mixture of Experts", "comment": null, "summary": "Mixture of Experts (MoE) models have become central to scaling large language\nmodels, yet their mechanistic differences from dense networks remain poorly\nunderstood. Previous work has explored how dense models use\n\\textit{superposition} to represent more features than dimensions, and how\nsuperposition is a function of feature sparsity and feature importance. MoE\nmodels cannot be explained mechanistically through the same lens. We find that\nneither feature sparsity nor feature importance cause discontinuous phase\nchanges, and that network sparsity (the ratio of active to total experts)\nbetter characterizes MoEs. We develop new metrics for measuring superposition\nacross experts. Our findings demonstrate that models with greater network\nsparsity exhibit greater \\emph{monosemanticity}. We propose a new definition of\nexpert specialization based on monosemantic feature representation rather than\nload balancing, showing that experts naturally organize around coherent feature\ncombinations when initialized appropriately. These results suggest that network\nsparsity in MoEs may enable more interpretable models without sacrificing\nperformance, challenging the common assumption that interpretability and\ncapability are fundamentally at odds.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24222", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24222", "abs": "https://arxiv.org/abs/2510.24222", "authors": ["Adi Simhi", "Jonathan Herzig", "Itay Itzhak", "Dana Arad", "Zorik Gekhman", "Roi Reichart", "Fazl Barez", "Gabriel Stanovsky", "Idan Szpektor", "Yonatan Belinkov"], "title": "HACK: Hallucinations Along Certainty and Knowledge Axes", "comment": "The code is available at\n  https://github.com/technion-cs-nlp/HACK_Hallucinations_Along_Certainty_and_Knowledge_axes", "summary": "Hallucinations in LLMs present a critical barrier to their reliable usage.\nExisting research usually categorizes hallucination by their external\nproperties rather than by the LLMs' underlying internal properties. This\nexternal focus overlooks that hallucinations may require tailored mitigation\nstrategies based on their underlying mechanism. We propose a framework for\ncategorizing hallucinations along two axes: knowledge and certainty. Since\nparametric knowledge and certainty may vary across models, our categorization\nmethod involves a model-specific dataset construction process that\ndifferentiates between those types of hallucinations. Along the knowledge axis,\nwe distinguish between hallucinations caused by a lack of knowledge and those\noccurring despite the model having the knowledge of the correct response. To\nvalidate our framework along the knowledge axis, we apply steering mitigation,\nwhich relies on the existence of parametric knowledge to manipulate model\nactivations. This addresses the lack of existing methods to validate knowledge\ncategorization by showing a significant difference between the two\nhallucination types. We further analyze the distinct knowledge and\nhallucination patterns between models, showing that different hallucinations do\noccur despite shared parametric knowledge. Turning to the certainty axis, we\nidentify a particularly concerning subset of hallucinations where models\nhallucinate with certainty despite having the correct knowledge internally. We\nintroduce a new evaluation metric to measure the effectiveness of mitigation\nmethods on this subset, revealing that while some methods perform well on\naverage, they fail disproportionately on these critical cases. Our findings\nhighlight the importance of considering both knowledge and certainty in\nhallucination analysis and call for targeted mitigation approaches that\nconsider the hallucination underlying factors.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u7ef4\u6846\u67b6\uff0c\u5c06LLM\u7684\u5e7b\u89c9\u6309\u77e5\u8bc6\u7ef4\u5ea6\uff08\u7f3a\u4e4f\u77e5\u8bc6 vs \u5df2\u5177\u5907\u6b63\u786e\u77e5\u8bc6\u5374\u9519\u62a5\uff09\u4e0e\u786e\u5b9a\u6027\u7ef4\u5ea6\uff08\u9ad8\u7f6e\u4fe1\u5ea6\u4e0b\u7684\u9519\u8bef\uff09\u5206\u7c7b\uff1b\u901a\u8fc7\u6a21\u578b\u7279\u5b9a\u6570\u636e\u96c6\u6784\u5efa\u548c\u57fa\u4e8e\u5b9a\u4f4d\u7684\u9a71\u52a8\uff08steering\uff09\u7f13\u89e3\u9a8c\u8bc1\u77e5\u8bc6\u5206\u7c7b\uff1b\u63ed\u793a\u6a21\u578b\u4e4b\u95f4\u7684\u4e0d\u540c\u5e7b\u89c9\u6a21\u5f0f\uff0c\u5e76\u8bc6\u522b\u5728\u5185\u90e8\u62e5\u6709\u6b63\u786e\u77e5\u8bc6\u65f6\u4ecd\u9ad8\u7f6e\u4fe1\u5ea6\u5730\u5e7b\u89c9\u7684\u5173\u952e\u5b50\u96c6\uff0c\u63d0\u51fa\u65b0\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cf\u7f13\u89e3\u5728\u8be5\u5b50\u96c6\u4e0a\u7684\u6548\u679c\uff0c\u5f3a\u8c03\u9700\u9488\u5bf9\u5185\u90e8\u673a\u5236\u7684\u5e7b\u89c9\u8fdb\u884c\u5b9a\u5236\u5316\u7f13\u89e3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u6309\u5916\u90e8\u7279\u5f81\u5bf9\u5e7b\u89c9\u8fdb\u884c\u5206\u7c7b\uff0c\u5ffd\u89c6\u4e86\u5e7b\u89c9\u7684\u6f5c\u5728\u5185\u90e8\u673a\u5236\uff0c\u5bfc\u81f4\u65e0\u6cd5\u9488\u5bf9\u6027\u5730\u8bbe\u8ba1\u7f13\u89e3\u7b56\u7565\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u77e5\u8bc6\u4e0e\u786e\u5b9a\u6027\u4e24\u7ef4\u5bf9\u5e7b\u89c9\u8fdb\u884c\u673a\u5236\u5316\u5206\u7c7b\uff0c\u4ee5\u63d0\u5347\u7f13\u89e3\u7684\u6709\u6548\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u8f74\u6846\u67b6\uff1a\u77e5\u8bc6\u8f74\u5c06\u5e7b\u89c9\u5206\u4e3a\u56e0\u7f3a\u4e4f\u77e5\u8bc6\u800c\u4ea7\u751f\u7684\u5e7b\u89c9\u548c\u5728\u6a21\u578b\u5177\u5907\u6b63\u786e\u77e5\u8bc6\u65f6\u4ecd\u4ea7\u751f\u9519\u8bef\u56de\u7b54\u7684\u5e7b\u89c9\uff1b\u786e\u5b9a\u6027\u8f74\u8bc6\u522b\u5728\u5185\u90e8\u62e5\u6709\u6b63\u786e\u77e5\u8bc6\u65f6\u4ecd\u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u5e7b\u89c9\u7684\u5b50\u96c6\u3002\u4e3a\u77e5\u8bc6\u8f74\u7684\u9a8c\u8bc1\uff0c\u8fdb\u884c\u6a21\u578b\u7279\u5b9a\u7684\u6570\u636e\u96c6\u6784\u9020\u4ee5\u533a\u5206\u4e24\u7c7b\u5e7b\u89c9\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e\u9a71\u52a8/\u5f15\u5bfc\u7684\u7f13\u89e3\uff08steering mitigation\uff09\u6765\u64cd\u4f5c\u6a21\u578b\u6fc0\u6d3b\u4ee5\u5229\u7528\u53c2\u6570\u5316\u77e5\u8bc6\u3002\u8fdb\u4e00\u6b65\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u77e5\u8bc6\u4e0e\u5e7b\u89c9\u6a21\u5f0f\uff0c\u53d1\u73b0\u5171\u4eab\u53c2\u6570\u77e5\u8bc6\u5e76\u4e0d\u7b49\u4e8e\u76f8\u540c\u5e7b\u89c9\u6a21\u5f0f\u3002\u4e3a\u8bc4\u4f30\u5bf9\u8be5\u5b50\u96c6\u7684\u7f13\u89e3\u6548\u679c\uff0c\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u53d1\u73b0\u67d0\u4e9b\u7f13\u89e3\u65b9\u6cd5\u5728\u5e73\u5747\u6c34\u5e73\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u5728\u5173\u952e\u5b50\u96c6\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "result": "\u5728\u77e5\u8bc6\u8f74\u4e0a\uff0c\u7f3a\u4e4f\u77e5\u8bc6\u4e0e\u5177\u5907\u77e5\u8bc6\u4f46\u9519\u8bef\u7684\u4e24\u7c7b\u5e7b\u89c9\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u4e0d\u540c\u6a21\u578b\u5448\u73b0\u51fa\u4e0d\u540c\u7684\u5e7b\u89c9\u6a21\u5f0f\uff0c\u5373\u4f7f\u5b83\u4eec\u5177\u6709\u76f8\u540c\u7684\u53c2\u6570\u5316\u77e5\u8bc6\uff1b\u5bf9\u5185\u90e8\u77e5\u8bc6\u4ecd\u9ad8\u7f6e\u4fe1\u5ea6\u5e7b\u89c9\u7684\u5b50\u96c6\u66b4\u9732\u4e86\u7f13\u89e3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e73\u5747\u6307\u6807\u63a9\u76d6\u4e86\u5bf9\u5173\u952e\u5b50\u96c6\u7684\u52a3\u52bf\uff1b\u65b0\u6307\u6807\u63ed\u793a\u4e86\u7f13\u89e3\u7b56\u7565\u5728\u7279\u5b9a\u60c5\u5f62\u4e0b\u7684\u4e0d\u8db3\u3002", "conclusion": "\u540c\u65f6\u8003\u91cf\u77e5\u8bc6\u4e0e\u786e\u5b9a\u6027\u4e24\u4e2a\u7ef4\u5ea6\u5bf9\u4e8e\u7406\u89e3\u548c\u7f13\u89e3LLM\u5e7b\u89c9\u81f3\u5173\u91cd\u8981\uff0c\u5e94\u53d1\u5c55\u9488\u5bf9\u6f5c\u5728\u673a\u5236\u7684\u5b9a\u5236\u5316\u7f13\u89e3\u7b56\u7565\u3002\u672a\u6765\u5de5\u4f5c\u5e94\u805a\u7126\u5728\u8bc6\u522b\u4e0d\u540c\u5e7b\u89c9\u7c7b\u578b\u7684\u6839\u672c\u539f\u56e0\u5e76\u8bbe\u8ba1\u76ee\u6807\u5316\u7684\u5e72\u9884\u624b\u6bb5\uff0c\u4ee5\u63d0\u5347\u5bf9\u5173\u952e\u5b50\u96c6\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24337", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24337", "abs": "https://arxiv.org/abs/2510.24337", "authors": ["Daria Kravets-Meinke", "Hannah Schmid-Petri", "Sonja Niemann", "Ute Schmid"], "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research", "comment": null, "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly\nbeing used in communication research for content analysis. Studies show that\ngLLMs can outperform both crowd workers and trained coders, such as research\nassistants, on various coding tasks relevant to communication science, often at\na fraction of the time and cost. Additionally, gLLMs can decode implicit\nmeanings and contextual information, be instructed using natural language,\ndeployed with only basic programming skills, and require little to no annotated\ndata beyond a validation dataset - constituting a paradigm shift in automated\ncontent analysis. Despite their potential, the integration of gLLMs into the\nmethodological toolkit of communication research remains underdeveloped. In\ngLLM-assisted quantitative content analysis, researchers must address at least\nseven critical challenges that impact result quality: (1) codebook development,\n(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)\niterative refinement, (6) validation of the model's reliability, and\noptionally, (7) performance enhancement. This paper synthesizes emerging\nresearch on gLLM-assisted quantitative content analysis and proposes a\ncomprehensive best-practice guide to navigate these challenges. Our goal is to\nmake gLLM-based content analysis more accessible to a broader range of\ncommunication researchers and ensure adherence to established disciplinary\nquality standards of validity, reliability, reproducibility, and research\nethics.", "AI": {"tldr": "\u7efc\u8ff0\u5e76\u63d0\u51fa\u9762\u5411\u4f20\u64ad\u7814\u7a76\u7684gLLM\u8f85\u52a9\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u7684\u4e03\u5927\u6311\u6218\u53ca\u4e00\u5957\u7efc\u5408\u6700\u4f73\u5b9e\u8df5\u6307\u5357\u3002", "motivation": "\u5c3d\u7ba1gLLMs\u5728\u5185\u5bb9\u5206\u6790\u4e2d\u663e\u793a\u51fa\u6548\u7387\u4e0e\u8d28\u91cf\u6f5c\u529b\uff0c\u4f46\u5728\u4f20\u64ad\u7814\u7a76\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u5c1a\u672a\u6210\u719f\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u65b9\u6cd5\u6765\u786e\u4fdd\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u4f26\u7406\u5408\u89c4\u3002", "method": "\u7efc\u5408\u8fd1\u6765\u76f8\u5173\u7814\u7a76\uff0c\u63d0\u70bc\u4e03\u5927\u5173\u952e\u6311\u6218\uff0c\u63d0\u51fa\u5206\u6b65\u7684\u6700\u4f73\u5b9e\u8df5\u6846\u67b6\uff0c\u6db5\u76d6\u7f16\u7801\u672c\uff08codebook\uff09\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u4e0e\u53c2\u6570\u9009\u62e9\u3001\u8fed\u4ee3\u4fee\u6b63\u3001\u4ee5\u53ca\u53ef\u9760\u6027\u9a8c\u8bc1\u7b49\u3002", "result": "\u5f62\u6210\u4e00\u4e2a\u53ef\u64cd\u4f5c\u7684\u6307\u5357\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u5c06gLLM\u5e94\u7528\u4e8e\u91cf\u5316\u5185\u5bb9\u5206\u6790\uff0c\u540c\u65f6\u4fdd\u6301\u5b66\u79d1\u89c4\u8303\uff0c\u964d\u4f4e\u95e8\u69db\uff0c\u63d0\u5347\u53ef\u91cd\u590d\u6027\u4e0e\u900f\u660e\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u8be5\u6307\u5357\uff0c\u4fc3\u8fdbgLLM\u5728\u4f20\u64ad\u7814\u7a76\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u63a8\u52a8\u65b9\u6cd5\u8bba\u7684\u73b0\u4ee3\u5316\u4e0e\u4f26\u7406\u6027\u3002"}}
{"id": "2510.23672", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23672", "abs": "https://arxiv.org/abs/2510.23672", "authors": ["Xiangfei Qiu", "Xingjian Wu", "Hanyin Cheng", "Xvyuan Liu", "Chenjuan Guo", "Jilin Hu", "Bin Yang"], "title": "DBLoss: Decomposition-based Loss Function for Time Series Forecasting", "comment": "Accepted by NeurIPS 2025", "summary": "Time series forecasting holds significant value in various domains such as\neconomics, traffic, energy, and AIOps, as accurate predictions facilitate\ninformed decision-making. However, the existing Mean Squared Error (MSE) loss\nfunction sometimes fails to accurately capture the seasonality or trend within\nthe forecasting horizon, even when decomposition modules are used in the\nforward propagation to model the trend and seasonality separately. To address\nthese challenges, we propose a simple yet effective Decomposition-Based Loss\nfunction called DBLoss. This method uses exponential moving averages to\ndecompose the time series into seasonal and trend components within the\nforecasting horizon, and then calculates the loss for each of these components\nseparately, followed by weighting them. As a general loss function, DBLoss can\nbe combined with any deep learning forecasting model. Extensive experiments\ndemonstrate that DBLoss significantly improves the performance of\nstate-of-the-art models across diverse real-world datasets and provides a new\nperspective on the design of time series loss functions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u89e3\u7684\u635f\u5931\u51fd\u6570 DBLoss\uff0c\u901a\u8fc7\u5728\u9884\u6d4b\u533a\u95f4\u5185\u5229\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\u5c06\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u548c\u8d8b\u52bf\u6210\u5206\uff0c\u7136\u540e\u5bf9\u6bcf\u4e2a\u5206\u91cf\u5355\u72ec\u8ba1\u7b97\u635f\u5931\u5e76\u52a0\u6743\uff0c\u4f5c\u4e3a\u901a\u7528\u635f\u5931\u51fd\u6570\u53ef\u4e0e\u4efb\u610f\u6df1\u5ea6\u5b66\u4e60 forecasting \u6a21\u578b\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6a21\u578b\u5728\u591a\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u5728\u9884\u6d4b\u533a\u95f4\u5185\u5f80\u5f80\u96be\u4ee5\u51c6\u786e\u6355\u6349\u5b63\u8282\u6027\u548c\u8d8b\u52bf\uff0c\u5373\u4f7f\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u4f7f\u7528\u5206\u89e3\u6a21\u5757\u4e5f\u4e0d\u80fd\u5145\u5206\u5bf9\u9f50\u8bad\u7ec3\u76ee\u6807\uff0c\u5bfc\u81f4\u9884\u6d4b\u8bef\u5dee\u672a\u80fd\u53cd\u6620\u65f6\u95f4\u5e8f\u5217\u7684\u7ed3\u6784\u6027\u7279\u5f81\u3002\u9700\u8981\u4e00\u4e2a\u4e0e\u5206\u89e3\u4e00\u81f4\u7684\u635f\u5931\u51fd\u6570\u6765\u66f4\u597d\u5730\u6307\u5bfc\u6a21\u578b\u5b66\u4e60\u3002", "method": "\u5728\u9884\u6d4b\u533a\u95f4\u5185\u4f7f\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u5bf9\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u5206\u89e3\uff0c\u5f97\u5230\u5b63\u8282\u6210\u5206\u548c\u8d8b\u52bf\u6210\u5206\uff1b\u5bf9\u8fd9\u4e24\u4e2a\u5206\u91cf\u5206\u522b\u8ba1\u7b97\u635f\u5931\uff0c\u7136\u540e\u8fdb\u884c\u52a0\u6743\u7ec4\u5408\uff0c\u5f62\u6210 DBLoss\uff1b\u8be5\u635f\u5931\u51fd\u6570\u4e3a\u901a\u7528\u635f\u5931\u51fd\u6570\uff0c\u53ef\u4e0e\u4efb\u610f\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u7ed3\u5408\u3002", "result": "\u5728\u591a\u7ec4\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cDBLoss \u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8868\u660e\u5c06\u5206\u89e3\u601d\u60f3\u76f4\u63a5\u878d\u5165\u635f\u5931\u51fd\u6570\u53ef\u4ee5\u5e26\u6765\u5b9e\u8d28\u6027\u6536\u76ca\uff0c\u5e76\u4e3a\u65f6\u95f4\u5e8f\u5217\u635f\u5931\u51fd\u6570\u7684\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "conclusion": "DBLoss \u4f5c\u4e3a\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u901a\u7528\u635f\u5931\u51fd\u6570\uff0c\u80fd\u591f\u4e0e\u73b0\u6709\u6a21\u578b\u65e0\u7f1d\u7ed3\u5408\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u793a\u672a\u6765\u5728\u57fa\u4e8e\u5206\u89e3\u7684\u635f\u5931\u8bbe\u8ba1\u65b9\u9762\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2510.24236", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24236", "abs": "https://arxiv.org/abs/2510.24236", "authors": ["Teague McMillan", "Gabriele Dominici", "Martin Gjoreski", "Marc Langheinrich"], "title": "Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: NeurIPS 2025 Workshop on Evaluating the Evolving LLM\n  Lifecycle: Benchmarks, Emergent Abilities, and Scaling", "summary": "Large Language Models (LLMs) often produce explanations that do not\nfaithfully reflect the factors driving their predictions. In healthcare\nsettings, such unfaithfulness is especially problematic: explanations that omit\nsalient clinical cues or mask spurious shortcuts can undermine clinician trust\nand lead to unsafe decision support. We study how inference and training-time\nchoices shape explanation faithfulness, focusing on factors practitioners can\ncontrol at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA\n8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),\nand manipulate the number and type of few-shot examples, prompting strategies,\nand training procedure. Our results show: (i) both the quantity and quality of\nfew-shot examples significantly impact model faithfulness; (ii) faithfulness is\nsensitive to prompting design; (iii) the instruction-tuning phase improves\nmeasured faithfulness on MedQA. These findings offer insights into strategies\nfor enhancing the interpretability and trustworthiness of LLMs in sensitive\ndomains.", "AI": {"tldr": "Few-shot\u6570\u91cf\u4e0e\u8d28\u91cf\u3001\u63d0\u793a\u8bbe\u8ba1\u4ee5\u53ca\u6307\u4ee4\u5fae\u8c03\u7b49\u56e0\u7d20\u663e\u8457\u5f71\u54cdLLMs\u5728\u533b\u7597\u548c\u504f\u89c1\u573a\u666f\u4e2d\u7684\u89e3\u91ca\u4fe1\u5b9e\u6027\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\uff0c\u89e3\u91ca\u82e5\u672a\u771f\u5b9e\u53cd\u6620\u5f71\u54cd\u9884\u6d4b\u7684\u56e0\u7d20\uff0c\u53ef\u80fd\u524a\u5f31\u4e34\u5e8a\u4fe1\u4efb\u5e76\u5e26\u6765\u5b89\u5168\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u7406\u89e3\u90e8\u7f72\u9636\u6bb5\u53ef\u63a7\u56e0\u7d20\u5bf9\u89e3\u91ca\u4fe1\u5b9e\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5728GPT-4.1-mini\u3001LLaMA 70B\u3001LLaMA 8B\u8fd9\u4e09\u79cd\u6a21\u578b\u4e0a\uff0c\u4f7f\u7528 BBQ\uff08\u793e\u4f1a\u504f\u89c1\uff09\u4e0e MedQA\uff08\u533b\u7597\u8bb8\u53ef\u9898\u76ee\uff09\u4e24\u7ec4\u6570\u636e\uff0c\u901a\u8fc7\u8c03\u6574few-shot\u793a\u4f8b\u6570\u91cf\u4e0e\u7c7b\u578b\u3001 prompting \u7b56\u7565\uff0c\u4ee5\u53ca\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u6765\u8bc4\u4f30\u89e3\u91ca\u7684\u4fe1\u5b9e\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a(i) few-shot\u793a\u4f8b\u7684\u6570\u91cf\u4e0e\u8d28\u91cf\u663e\u8457\u5f71\u54cd\u4fe1\u5b9e\u6027\uff1b(ii) \u4fe1\u5b9e\u6027\u5bf9 prompting \u8bbe\u8ba1\u654f\u611f\uff1b(iii) \u6307\u4ee4\u5fae\u8c03\u9636\u6bb5\u5728 MedQA \u4e0a\u63d0\u5347\u4e86\u6d4b\u91cf\u7684\u4fe1\u5b9e\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5728\u654f\u611f\u9886\u57df\u63d0\u5347LLMs\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u7b56\u7565\u3002"}}
{"id": "2510.23681", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23681", "abs": "https://arxiv.org/abs/2510.23681", "authors": ["Carl Hvarfner", "David Eriksson", "Eytan Bakshy", "Max Balandat"], "title": "Informed Initialization for Bayesian Optimization and Active Learning", "comment": "28 pages", "summary": "Bayesian Optimization is a widely used method for optimizing expensive\nblack-box functions, relying on probabilistic surrogate models such as Gaussian\nProcesses. The quality of the surrogate model is crucial for good optimization\nperformance, especially in the few-shot setting where only a small number of\nbatches of points can be evaluated. In this setting, the initialization plays a\ncritical role in shaping the surrogate's predictive quality and guiding\nsubsequent optimization. Despite this, practitioners typically rely on\n(quasi-)random designs to cover the input space. However, such approaches\nneglect two key factors: (a) space-filling designs may not be desirable to\nreduce predictive uncertainty, and (b) efficient hyperparameter learning during\ninitialization is essential for high-quality prediction, which may conflict\nwith space-filling designs. To address these limitations, we propose\nHyperparameter-Informed Predictive Exploration (HIPE), a novel acquisition\nstrategy that balances predictive uncertainty reduction with hyperparameter\nlearning using information-theoretic principles. We derive a closed-form\nexpression for HIPE in the Gaussian Process setting and demonstrate its\neffectiveness through extensive experiments in active learning and few-shot BO.\nOur results show that HIPE outperforms standard initialization strategies in\nterms of predictive accuracy, hyperparameter identification, and subsequent\noptimization performance, particularly in large-batch, few-shot settings\nrelevant to many real-world Bayesian Optimization applications.", "AI": {"tldr": "HIPE is an acquisition function for Bayesian optimization that balances predictive uncertainty reduction with hyperparameter learning, addressing initialization in few-shot, large-batch BO.", "motivation": "In few-shot Bayesian optimization, initialization critically shapes the GP surrogate. Traditional space-filling designs may hurt hyperparameter learning and predictive uncertainty; there is a need to jointly optimize for uncertainty reduction and hyperparameter identifiability.", "method": "Introduce Hyperparameter-Informed Predictive Exploration (HIPE), an information-theoretic acquisition function. Derive a closed-form expression for HIPE in the Gaussian Process setting and validate it on active learning and few-shot BO with large batches.", "result": "HIPE improves predictive accuracy, hyperparameter identification, and subsequent optimization performance compared with standard initialization strategies, with notable gains in large-batch, few-shot scenarios.", "conclusion": "HIPE provides an effective initialization strategy for BO and active learning by balancing uncertainty reduction and hyperparameter learning, leading to better early-stage performance in GP-based models."}}
{"id": "2510.24247", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24247", "abs": "https://arxiv.org/abs/2510.24247", "authors": ["Ahmad Ghannam", "Naif Alharthi", "Faris Alasmary", "Kholood Al Tabash", "Shouq Sadah", "Lahouari Ghouti"], "title": "Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations", "comment": null, "summary": "In this work, we tackle the Diacritic Restoration (DR) task for Arabic\ndialectal sentences using a multimodal approach that combines both textual and\nspeech information. We propose a model that represents the text modality using\nan encoder extracted from our own pre-trained model named CATT. The speech\ncomponent is handled by the encoder module of the OpenAI Whisper base model.\nOur solution is designed following two integration strategies. The former\nconsists of fusing the speech tokens with the input at an early stage, where\nthe 1500 frames of the audio segment are averaged over 10 consecutive frames,\nresulting in 150 speech tokens. To ensure embedding compatibility, these\naveraged tokens are processed through a linear projection layer prior to\nmerging them with the text tokens. Contextual encoding is guaranteed by the\nCATT encoder module. The latter strategy relies on cross-attention, where text\nand speech embeddings are fused. The cross-attention output is then fed to the\nCATT classification head for token-level diacritic prediction. To further\nimprove model robustness, we randomly deactivate the speech input during\ntraining, allowing the model to perform well with or without speech. Our\nexperiments show that the proposed approach achieves a word error rate (WER) of\n0.25 and a character error rate (CER) of 0.9 on the development set. On the\ntest set, our model achieved WER and CER scores of 0.55 and 0.13, respectively.", "AI": {"tldr": "Multimodal diacritic restoration for Arabic dialectal sentences using text (CATT) and speech (Whisper) encoders, with two fusion strategies (early fusion of averaged speech tokens and cross-attention fusion), plus speech dropout during training. Evaluated on dev/test with WER and CER metrics.", "motivation": "Improve Arabic dialectal diacritic restoration by incorporating speech cues to complement textual information, addressing limitations of text-only models.", "method": "Two integration strategies: (1) early fusion\u2014average 1500 audio frames to 150 speech tokens, project via linear layer, concatenate with text tokens, processed by CATT encoder; (2) cross-attention fusion\u2014text and speech embeddings undergo cross-attention, then passed to CATT classification head for token-level diacritic prediction. Training augments with random speech input dropout for robustness. The OF OpenAI Whisper base model is used for speech encoder; CATT handles text encoding.", "result": "Dev set: WER 0.25, CER 0.9. Test set: WER 0.55, CER 0.13.", "conclusion": "A robust multimodal framework for Arabic DR showing competitive WER/CER gains and resilience to speech absence during inference; suggests effectiveness of combining text and speech representations and of dual fusion strategies."}}
{"id": "2510.24250", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24250", "abs": "https://arxiv.org/abs/2510.24250", "authors": ["Syed Zohaib Hassan", "P\u00e5l Halvorsen", "Miriam S. Johnson", "Pierre Lison"], "title": "Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations", "comment": "11 pages excluding references and appendix. 3 figures and 6 tables", "summary": "Large Language Models (LLMs), predominantly trained on adult conversational\ndata, face significant challenges when generating authentic, child-like\ndialogue for specialized applications. We present a comparative study\nevaluating five different LLMs (GPT-4, RUTER-LLAMA-2-13b, GPTSW, NorMistral-7b,\nand NorBloom-7b) to generate age-appropriate Norwegian conversations for\nchildren aged 5 and 9 years. Through a blind evaluation by eleven education\nprofessionals using both real child interview data and LLM-generated text\nsamples, we assessed authenticity and developmental appropriateness. Our\nresults show that evaluators achieved strong inter-rater reliability (ICC=0.75)\nand demonstrated higher accuracy in age prediction for younger children\n(5-year-olds) compared to older children (9-year-olds). While GPT-4 and\nNorBloom-7b performed relatively well, most models generated language perceived\nas more linguistically advanced than the target age groups. These findings\nhighlight critical data-related challenges in developing LLM systems for\nspecialized applications involving children, particularly in low-resource\nlanguages where comprehensive age-appropriate lexical resources are scarce.", "AI": {"tldr": "\u5728\u4e94\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8bc4\u4f30\u751f\u6210\u632a\u5a01\u8bed5\u5c81\u4e0e9\u5c81\u513f\u7ae5\u5bf9\u8bdd\u7684\u771f\u5b9e\u6027\u4e0e\u53d1\u5c55\u9002\u5b9c\u6027\uff1b\u7ed3\u679c\u8868\u660e\u8bc4\u4f30\u8005\u4e00\u81f4\u6027\u826f\u597d\uff0c5\u5c81\u6837\u672c\u7684\u5e74\u9f84\u8bc6\u522b\u66f4\u51c6\u786e\uff1bGPT-4\u4e0eNorBloom-7b\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u591a\u6570\u6a21\u578b\u751f\u6210\u7684\u8bed\u8a00\u504f\u5411\u9ad8\u4e8e\u76ee\u6807\u5e74\u9f84\uff1b\u6570\u636e\u8d44\u6e90\u532e\u4e4f\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u6b64\u7c7b\u5e94\u7528\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u95e8\u573a\u666f\uff08\u513f\u7ae5\u5bf9\u8bdd\uff09\u4e2d\u7684\u53ef\u7528\u6027\u4e0e\u5b89\u5168\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7f3a\u4e4f\u5e74\u9f84\u9002\u5b9c\u7684\u8bcd\u6c47\u8d44\u6e90\uff1b\u73b0\u6709\u6a21\u578b\u591a\u4ee5\u6210\u4eba\u5bf9\u8bdd\u6570\u636e\u4e3a\u4e3b\uff0c\u96be\u4ee5\u751f\u6210\u771f\u5b9e\u7684\u513f\u7ae5\u8bed\u8a00\u98ce\u683c\u3002", "method": "\u5c06\u4e94\u79cd\u6a21\u578b\uff08GPT-4, RUTER-LLAMA-2-13b, GPTSW, NorMistral-7b, NorBloom-7b\uff09\u7528\u4e8e\u751f\u6210\u632a\u5a01\u8bed\u76845\u5c81\u548c9\u5c81\u513f\u7ae5\u5bf9\u8bdd\uff1b\u901a\u8fc711\u540d\u6559\u80b2\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u76f2\u8bc4\uff0c\u7ed3\u5408\u771f\u5b9e\u513f\u7ae5\u8bbf\u8c08\u6570\u636e\u4e0eLLM\u751f\u6210\u6587\u672c\uff0c\u8bc4\u4f30\u771f\u5b9e\u6027\u4e0e\u53d1\u5c55\u9002\u5b9c\u6027\u3002", "result": "\u8bc4\u4f30\u8005\u95f4ICC=0.75\uff0c5\u5c81\u6837\u672c\u7684\u5e74\u9f84\u9884\u6d4b\u66f4\u51c6\u786e\uff1bGPT-4\u548cNorBloom-7b\u8868\u73b0\u76f8\u5bf9\u8f83\u597d\uff1b\u591a\u6570\u6a21\u578b\u7684\u8f93\u51fa\u8bed\u8a00\u88ab\u8bc4\u4f30\u4e3a\u6bd4\u76ee\u6807\u5e74\u9f84\u66f4\u9ad8\u7ea7\u3002", "conclusion": "\u51f8\u663e\u5728\u513f\u7ae5\u76f8\u5173\u5e94\u7528\u4e2d\uff0c\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\uff0c\u6570\u636e\u76f8\u5173\u6311\u6218\uff08\u7f3a\u4e4f\u5e74\u9f84\u9002\u5b9c\u7684\u8bcd\u6c47\u8d44\u6e90\u3001\u6807\u6ce8\u6570\u636e\u7b49\uff09\u5bf9LLM\u7cfb\u7edf\u5f00\u53d1\u7684\u5173\u952e\u5f71\u54cd\uff1b\u9700\u6539\u8fdb\u6570\u636e\u83b7\u53d6\u3001\u8bed\u8a00\u53d1\u5c55\u9002\u914d\u4e0e\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2510.23685", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23685", "abs": "https://arxiv.org/abs/2510.23685", "authors": ["Junwen Ma", "Mingyu Ge", "Yisen Wang", "Yong Zhang", "Weicheng Fu"], "title": "Parallel BiLSTM-Transformer networks for forecasting chaotic dynamics", "comment": "9 pages,7 figures", "summary": "The nonlinear nature of chaotic systems results in extreme sensitivity to\ninitial conditions and highly intricate dynamical behaviors, posing fundamental\nchallenges for accurately predicting their evolution. To overcome the\nlimitation that conventional approaches fail to capture both local features and\nglobal dependencies in chaotic time series simultaneously, this study proposes\na parallel predictive framework integrating Transformer and Bidirectional Long\nShort-Term Memory (BiLSTM) networks. The hybrid model employs a dual-branch\narchitecture, where the Transformer branch mainly captures long-range\ndependencies while the BiLSTM branch focuses on extracting local temporal\nfeatures. The complementary representations from the two branches are fused in\na dedicated feature-fusion layer to enhance predictive accuracy. As\nillustrating examples, the model's performance is systematically evaluated on\ntwo representative tasks in the Lorenz system. The first is autonomous\nevolution prediction, in which the model recursively extrapolates system\ntrajectories from the time-delay embeddings of the state vector to evaluate\nlong-term tracking accuracy and stability. The second is inference of\nunmeasured variable, where the model reconstructs the unobserved states from\nthe time-delay embeddings of partial observations to assess its\nstate-completion capability. The results consistently indicate that the\nproposed hybrid framework outperforms both single-branch architectures across\ntasks, demonstrating its robustness and effectiveness in chaotic system\nprediction.", "AI": {"tldr": "\u53cc\u5206\u652f Transformer-BiLSTM \u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u878d\u5408\u63d0\u5347\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\u7684\u9884\u6d4b\u548c\u672a\u89c2\u6d4b\u53d8\u91cf\u7684\u91cd\u6784\uff0c\u5728 Lorenz \u7cfb\u7edf\u4e0a\u4f18\u4e8e\u5355\u5206\u652f\u6a21\u578b\u3002", "motivation": "\u6df7\u6c8c\u7cfb\u7edf\u5bf9\u521d\u503c\u6781\u5ea6\u654f\u611f\uff0c\u5355\u4e00\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u540c\u65f6\u6355\u83b7\u5c40\u90e8\u65f6\u5e8f\u7279\u5f81\u4e0e\u5168\u5c40\u4f9d\u8d56\uff1b\u9700\u8981\u5c06\u5168\u5c40\u957f\u7a0b\u4f9d\u8d56\u4e0e\u5c40\u90e8\u65f6\u95f4\u7279\u5f81\u6709\u6548\u878d\u5408\u7684\u6a21\u578b\u3002", "method": "\u5e76\u884c\u53cc\u5206\u652f\u7ed3\u6784\uff1aTransformer \u5206\u652f\u63d0\u53d6\u5168\u5c40\u957f\u7a0b\u4f9d\u8d56\uff0cBiLSTM \u5206\u652f\u63d0\u53d6\u5c40\u90e8\u65f6\u95f4\u7279\u5f81\uff1b\u901a\u8fc7\u4e13\u7528\u7279\u5f81\u878d\u5408\u5c42\u6574\u5408\u4e24\u8005\u8868\u793a\uff1b\u4efb\u52a1\u5305\u62ec\u81ea\u4e3b\u6f14\u5316\u9884\u6d4b\uff08\u4ee5\u65f6\u5ef6\u5d4c\u5165\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\uff09\u548c\u672a\u89c2\u6d4b\u53d8\u91cf\u63a8\u65ad\uff08\u4ece\u90e8\u5206\u89c2\u6d4b\u7684\u65f6\u5ef6\u5d4c\u5165\u91cd\u6784\u72b6\u6001\uff09\uff1b\u5728\u6d1b\u4f26\u5179\u7cfb\u7edf\u4e0a\u8fdb\u884c\u9012\u5f52\u5916\u63a8\u4e0e\u72b6\u6001\u91cd\u5efa\u7684\u8bc4\u4f30\u3002", "result": "\u6df7\u5408\u6846\u67b6\u5728\u5404\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u5355\u5206\u652f\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u5c06\u5168\u5c40\u4e0e\u5c40\u90e8\u8868\u5f81\u6709\u6548\u878d\u5408\u7684\u53cc\u5206\u652f Transformer-BiLSTM \u67b6\u6784\u5bf9\u6df7\u6c8c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e0e\u72b6\u6001\u91cd\u6784\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u5177\u6709\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2510.24256", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24256", "abs": "https://arxiv.org/abs/2510.24256", "authors": ["Jack Merullo", "Srihita Vatsavaya", "Lucius Bushnaq", "Owen Lewis"], "title": "From Memorization to Reasoning in the Spectrum of Loss Curvature", "comment": null, "summary": "We characterize how memorization is represented in transformer models and\nshow that it can be disentangled in the weights of both language models (LMs)\nand vision transformers (ViTs) using a decomposition based on the loss\nlandscape curvature. This insight is based on prior theoretical and empirical\nwork showing that the curvature for memorized training points is much sharper\nthan non memorized, meaning ordering weight components from high to low\ncurvature can reveal a distinction without explicit labels. This motivates a\nweight editing procedure that suppresses far more recitation of untargeted\nmemorized data more effectively than a recent unlearning method\n(BalancedSubnet), while maintaining lower perplexity. Since the basis of\ncurvature has a natural interpretation for shared structure in model weights,\nwe analyze the editing procedure extensively on its effect on downstream tasks\nin LMs, and find that fact retrieval and arithmetic are specifically and\nconsistently negatively affected, even though open book fact retrieval and\ngeneral logical reasoning is conserved. We posit these tasks rely heavily on\nspecialized directions in weight space rather than general purpose mechanisms,\nregardless of whether those individual datapoints are memorized. We support\nthis by showing a correspondence between task data's activation strength with\nlow curvature components that we edit out, and the drop in task performance\nafter the edit. Our work enhances the understanding of memorization in neural\nnetworks with practical applications towards removing it, and provides evidence\nfor idiosyncratic, narrowly-used structures involved in solving tasks like math\nand fact retrieval.", "AI": {"tldr": "Memorization in transformer models is encoded in weight-space curvature; a curvature-based decomposition enables targeted editing to suppress memorized content more effectively than prior methods while preserving perplexity; editing reveals that fact retrieval and arithmetic rely on narrow, specialized weight directions, whereas broader open-book reasoning is preserved.", "motivation": "Understand how memorization manifests in neural networks (LMs and ViTs) by analyzing curvature-sensitive weight components, and develop practical methods to remove memorized content with minimal impact on general performance.", "method": "Decompose model weights via loss landscape curvature to rank components by their association with memorized data; apply an editing procedure to suppress far more memorized data than a recent method (BalancedSubnet); compare perplexity and downstream tasks; analyze the relationship between task activations and low-curvature components across LMs and ViTs.", "result": "Memorization is reflected in high-curvature components; weight-editing can suppress memorized content more effectively than BalancedSubnet while maintaining lower perplexity; after editing, fact retrieval and arithmetic degrade consistently, while open-book retrieval and general logical reasoning are preserved; low-curvature components edited out correlate with drops in task performance, suggesting these tasks rely on specialized weight directions.", "conclusion": "Curvature-based weight decomposition provides a principled lens to study and remove memorization, revealing idiosyncratic, narrowly-used weight structures underlying specific tasks like math and fact retrieval; the approach balances data privacy concerns with preserving core model capabilities."}}
{"id": "2510.24383", "categories": ["cs.AI", "cs.CY", "cs.MA", "I.2.11; I.2.1; I.2.4; K.4.1; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.24383", "abs": "https://arxiv.org/abs/2510.24383", "authors": ["Juraj Mavra\u010di\u0107"], "title": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents", "comment": "First published on 19/10/2025. Canonical archived record and DOI:\n  10.5281/zenodo.17391796", "summary": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale.", "AI": {"tldr": "Policy Cards provide a machine-readable, deployment-layer standard to specify and enforce AI constraints at runtime, with automatic validation and crosswalks to assurance frameworks.", "motivation": "Operationalize governance by encoding regulatory, ethical, and operational constraints directly with AI agents to enable runtime compliance, auditing, and accountability.", "method": "Introduce Policy Cards as an integral agent artifact that encodes allow/deny rules, obligations, and evidentiary requirements, linking to runtime enforcement and continuous-audit pipelines; extend existing transparency artifacts (Model/Data/System Cards) with a normative layer; provide auto-validation, versioning, and mappings to assurance frameworks like NIST AI RMF, ISO/IEC 42001, and EU AI Act.", "result": "A framework for verifiable compliance in autonomous/multi-agent ecosystems; supports distributed assurance and practical integration of governance with engineering practice.", "conclusion": "Policy Cards enable accountable autonomy at scale by bridging high-level governance with implementable, verifiable runtime constraints."}}
{"id": "2510.23693", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.23693", "abs": "https://arxiv.org/abs/2510.23693", "authors": ["Joachim Baumann"], "title": "On the Societal Impact of Machine Learning", "comment": "PhD thesis", "summary": "This PhD thesis investigates the societal impact of machine learning (ML). ML\nincreasingly informs consequential decisions and recommendations, significantly\naffecting many aspects of our lives. As these data-driven systems are often\ndeveloped without explicit fairness considerations, they carry the risk of\ndiscriminatory effects. The contributions in this thesis enable more\nappropriate measurement of fairness in ML systems, systematic decomposition of\nML systems to anticipate bias dynamics, and effective interventions that reduce\nalgorithmic discrimination while maintaining system utility. I conclude by\ndiscussing ongoing challenges and future research directions as ML systems,\nincluding generative artificial intelligence, become increasingly integrated\ninto society. This work offers a foundation for ensuring that ML's societal\nimpact aligns with broader social values.", "AI": {"tldr": "\u672c\u8bba\u6587/\u535a\u58eb\u8bba\u6587\u805a\u7126\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7684\u793e\u4f1a\u5f71\u54cd\uff0c\u5f3a\u8c03\u5728 ML \u7cfb\u7edf\u4e2d\u8fdb\u884c\u66f4\u5408\u9002\u7684\u516c\u5e73\u6027\u6d4b\u91cf\u3001\u5bf9\u7cfb\u7edf\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u89e3\u4ee5\u9884\u6d4b\u504f\u5dee\u52a8\u6001\uff0c\u4ee5\u53ca\u5728\u4fdd\u6301\u7cfb\u7edf\u6548\u7528\u7684\u524d\u63d0\u4e0b\u5b9e\u65bd\u6709\u6548\u5e72\u9884\u4ee5\u964d\u4f4e\u7b97\u6cd5\u6b67\u89c6\uff1b\u5e76\u8ba8\u8bba\u672a\u6765\u6311\u6218\u4e0e\u7814\u7a76\u65b9\u5411\uff0c\u5c24\u5176\u662f\u751f\u6210\u5f0f AI \u7b49\u65b0\u5174\u6280\u672f\u7684\u6574\u5408\u3002", "motivation": "ML \u73b0\u5728\u5728\u91cd\u8981\u4e14\u6709\u91cd\u5927\u540e\u679c\u7684\u51b3\u7b56\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u82e5\u7f3a\u4e4f\u660e\u786e\u7684\u516c\u5e73\u6027\u8003\u91cf\uff0c\u5bb9\u6613\u4ea7\u751f\u6b67\u89c6\u6027\u5f71\u54cd\u3002\u9700\u8981\u5efa\u7acb\u8861\u91cf\u3001\u516c\u5e73\u6027\u5206\u6790\u4e0e\u5e72\u9884\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd ML \u7684\u793e\u4f1a\u5f71\u54cd\u7b26\u5408\u793e\u4f1a\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u66f4\u5408\u9002\u7684\u516c\u5e73\u6027\u6d4b\u91cf\u65b9\u6cd5\uff1b\u5efa\u7acb\u7528\u4e8e\u7cfb\u7edf\u6027\u5206\u89e3 ML \u7cfb\u7edf\u4ee5\u9884\u6d4b\u504f\u5dee\u52a8\u6001\u7684\u6846\u67b6\uff1b\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u5728\u4e0d\u727a\u7272\u7cfb\u7edf\u6548\u7528\u7684\u60c5\u51b5\u4e0b\u964d\u4f4e\u7b97\u6cd5\u6b67\u89c6\u7684\u5e72\u9884\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u53ef\u64cd\u4f5c\u7684\u516c\u5e73\u6027\u6d4b\u91cf\u6846\u67b6\u3001\u504f\u5dee\u4f20\u64ad\u7684\u7cfb\u7edf\u6027\u5206\u89e3\u65b9\u6cd5\uff0c\u4ee5\u53ca\u80fd\u5728\u4fdd\u6301\u7cfb\u7edf\u6548\u7528\u7684\u524d\u63d0\u4e0b\u51cf\u5c11\u6b67\u89c6\u7684\u5e72\u9884\u624b\u6bb5\u3002", "conclusion": "\u603b\u7ed3\u5f53\u524d\u6311\u6218\u5e76\u6307\u660e\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7b49\u65b0\u5174 ML \u6280\u672f\u5728\u793e\u4f1a\u4e2d\u7684\u6e17\u900f\uff0c\u9700\u8981\u7ee7\u7eed\u5b8c\u5584\u4ee5\u786e\u4fdd ML \u7684\u793e\u4f1a\u5f71\u54cd\u4e0e\u5e7f\u6cdb\u7684\u793e\u4f1a\u4ef7\u503c\u4e00\u81f4\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2510.24390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24390", "abs": "https://arxiv.org/abs/2510.24390", "authors": ["Xianjun Gao", "Jianchun Liu", "Hongli Xu", "Liusheng Huang"], "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion", "comment": null, "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.", "AI": {"tldr": "Orion \u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u4f9d\u8d56\u611f\u77e5\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u70b9\u751f\u6210\u4e0e\u57fa\u4e8e\u4f9d\u8d56\u56fe\u7684\u5e76\u884c\u5c55\u5f00\uff0c\u7ed3\u5408\u6d41\u6c34\u7ebf\u8c03\u5ea6\u5b9e\u73b0\u8de8\u67e5\u8be2\u5e76\u884c\uff0c\u4ece\u800c\u5728\u5b9e\u65f6 Web \u573a\u666f\u4e2d\u63d0\u5347\u63a8\u7406\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "motivation": "\u5728\u5b9e\u65f6\u3001\u9ad8\u541e\u5410\u7684 Web \u5e94\u7528\u4e2d\uff0cLLM \u63a8\u7406\u5e38\u53d7\u9650\u4e8e\u4e32\u884c\u751f\u6210\u4e0e\u786c\u6027\u63a8\u7406\u7b56\u7565\uff0c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u8d28\u91cf\u8981\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u5e38\u5728\u6548\u7387\u4e0e\u8d28\u91cf\u4e4b\u95f4\u505a\u6743\u8861\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a\u2460 \u5173\u952e\u70b9\u751f\u6210\uff1a\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u5c11\u6837\u672c\u63d0\u793a\u63d0\u70bc\u7ed3\u6784\u5316\u7684\u5173\u952e\u8981\u70b9\uff1b\u2461 \u5185\u5bb9\u5e76\u884c\u5c55\u5f00\uff1a\u57fa\u4e8e\u4f9d\u8d56\u56fe\u5e76\u884c\u6269\u5c55\u8fd9\u4e9b\u8981\u70b9\uff0c\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u3002\u8fd8\u5f15\u5165\u6d41\u6c34\u7ebf\u8c03\u5ea6\uff0c\u6309 GPU \u8ba1\u7b97\u7279\u6027\u5728\u591a\u8f6e\u67e5\u8be2\u4e4b\u95f4\u5b9e\u73b0\u8de8\u67e5\u8be2\u5e76\u884c\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u4e0a\uff0c\u63a8\u7406\u4ee4\u724c\u751f\u6210\u901f\u5ea6\u63d0\u5347\u81f3 4.33x\u3001\u56de\u7b54\u5ef6\u8fdf\u964d\u4f4e\u81f3 3.42x\uff0c\u4e14\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u70b9\u95f4\u4f9d\u8d56\uff0c\u63a8\u7406\u8d28\u91cf\u63d0\u5347\u7ea6 18.75%\u3002", "conclusion": "\u901a\u8fc7\u5206\u79bb\u7684\u5173\u952e\u70b9\u751f\u6210\u4e0e\u5e76\u884c\u6269\u5c55\u5e76\u7ed3\u5408\u8de8\u67e5\u8be2\u8c03\u5ea6\uff0cOrion \u540c\u65f6\u63d0\u5347\u4e86\u5b9e\u65f6 Web \u573a\u666f\u4e0b\u7684\u63a8\u7406\u6548\u7387\u4e0e\u8d28\u91cf\uff0c\u4e3a\u5927\u89c4\u6a21\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u66f4\u53ef\u6269\u5c55\u7684\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23727", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23727", "abs": "https://arxiv.org/abs/2510.23727", "authors": ["Anisha Saha", "Varsha Suresh", "Timothy Hospedales", "Vera Demberg"], "title": "MUStReason: A Benchmark for Diagnosing Pragmatic Reasoning in Video-LMs for Multimodal Sarcasm Detection", "comment": null, "summary": "Sarcasm is a specific type of irony which involves discerning what is said\nfrom what is meant. Detecting sarcasm depends not only on the literal content\nof an utterance but also on non-verbal cues such as speaker's tonality, facial\nexpressions and conversational context. However, current multimodal models\nstruggle with complex tasks like sarcasm detection, which require identifying\nrelevant cues across modalities and pragmatically reasoning over them to infer\nthe speaker's intention. To explore these limitations in VideoLMs, we introduce\nMUStReason, a diagnostic benchmark enriched with annotations of\nmodality-specific relevant cues and underlying reasoning steps to identify\nsarcastic intent. In addition to benchmarking sarcasm classification\nperformance in VideoLMs, using MUStReason we quantitatively and qualitatively\nevaluate the generated reasoning by disentangling the problem into perception\nand reasoning, we propose PragCoT, a framework that steers VideoLMs to focus on\nimplied intentions over literal meaning, a property core to detecting sarcasm.", "AI": {"tldr": "\u63d0\u51faMUStReason\u8bca\u65ad\u57fa\u51c6\u7528\u4e8e\u8de8\u6a21\u6001\u89c6\u9891\u7684\u8bbd\u523a\u68c0\u6d4b\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u63a8\u7406\uff0c\u53e6\u7ed9\u51faPragCoT\u6846\u67b6\u4ee5\u805a\u7126\u9690\u542b\u610f\u56fe\u3002", "motivation": "\u8bbd\u523a\u6d89\u53ca\u8bf4\u8bdd\u5185\u5bb9\u4e0e\u9690\u542b\u610f\u56fe\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4f9d\u8d56\u975e\u8bed\u8a00\u7ebf\u7d22\u4e0e\u4e0a\u4e0b\u6587\u3002\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u5728\u8bc6\u522b\u76f8\u5173\u7ebf\u7d22\u5e76\u8fdb\u884c\u8de8\u6a21\u6001\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bca\u65ad\u57fa\u51c6\u548c\u63a8\u7406\u8bc4\u4f30\u6765\u63a8\u8fdb\u7814\u7a76\u3002", "method": "\u6784\u5efaMUStReason\uff0c\u9644\u5e26\u6a21\u6001\u7279\u5b9a\u76f8\u5173\u7ebf\u7d22\u4e0e\u6f5c\u5728\u63a8\u7406\u6b65\u9aa4\u6ce8\u91ca\uff0c\u4fbf\u4e8e\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u611f\u77e5\u4e0e\u63a8\u7406\u4e24\u90e8\u5206\u3002\u5bf9\u751f\u6210\u7684\u63a8\u7406\u8fdb\u884c\u5b9a\u91cf\u4e0e\u5b9a\u6027\u8bc4\u4f30\u3002\u63d0\u51faPragCoT\u6846\u67b6\uff0c\u5f15\u5bfcVideoLMs\u5173\u6ce8\u9690\u542b\u610f\u56fe\u800c\u975e\u5b57\u9762\u610f\u4e49\u3002", "result": "\u57fa\u4e8eMUStReason\u5bf9\u8bbd\u523a\u5206\u7c7b\u6027\u80fd\u8fdb\u884c\u57fa\u51c6\u8bc4\u4f30\uff0c\u5e76\u5bf9\u751f\u6210\u7684\u63a8\u7406\u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\uff0c\u5c55\u793a\u8bca\u65ad\u6027\u8bc4\u4f30\u7684\u6709\u6548\u6027\u3002", "conclusion": "MUStReason\u4e0ePragCoT\u4e3a\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u63d0\u4f9b\u8bca\u65ad\u6027\u57fa\u51c6\u4e0e\u53ef\u8fc1\u79fb\u7684\u63a8\u7406\u5f15\u5bfc\u6846\u67b6\uff0c\u4fc3\u8fdb\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u9690\u542b\u610f\u56fe\u3002"}}
{"id": "2510.24295", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24295", "abs": "https://arxiv.org/abs/2510.24295", "authors": ["M\u0103d\u0103lina Zgreab\u0103n", "Tejaswini Deoskar", "Lasha Abzianidze"], "title": "MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language Inference", "comment": "Pre-print", "summary": "In recent years, many generalization benchmarks have shown language models'\nlack of robustness in natural language inference (NLI). However, manually\ncreating new benchmarks is costly, while automatically generating high-quality\nones, even by modifying existing benchmarks, is extremely difficult. In this\npaper, we propose a methodology for automatically generating high-quality\nvariants of original NLI problems by replacing open-class words, while\ncrucially preserving their underlying reasoning. We dub our generalization test\nas MERGE (Minimal Expression-Replacements GEneralization), which evaluates the\ncorrectness of models' predictions across reasoning-preserving variants of the\noriginal problem. Our results show that NLI models' perform 4-20% worse on\nvariants, suggesting low generalizability even on such minimally altered\nproblems. We also analyse how word class of the replacements, word probability,\nand plausibility influence NLI models' performance.", "AI": {"tldr": "\u63d0\u51fa MERGE\uff08Minimal Expression-Replacements GEneralization\uff09\u4f5c\u4e3a\u81ea\u52a8\u751f\u6210\u539f\u59cb NLI \u95ee\u9898\u7684\u9ad8\u8d28\u91cf\u53d8\u4f53\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u66ff\u6362\u5f00\u653e\u7c7b\u8bcd\u4e14\u5c3d\u91cf\u4fdd\u6301\u63a8\u7406\u7ed3\u6784\uff0c\u7528\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff1b\u5b9e\u9a8c\u53d1\u73b0\u6a21\u578b\u5728\u53d8\u4f53\u4e0a\u7684\u6b63\u786e\u7387\u4e0b\u964d 4-20%\uff0c\u8868\u660e\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4e14\u66ff\u6362\u8bcd\u7684\u8bcd\u7c7b\u3001\u6982\u7387\u53ca\u53ef plausibility \u5bf9\u7ed3\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u901a\u7528\u6027\u57fa\u51c6\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff1b\u624b\u52a8\u6784\u9020\u57fa\u51c6\u6210\u672c\u9ad8\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u53d8\u4f53\u56f0\u96be\uff1b\u9700\u8981\u4e00\u79cd\u5728\u4e0d\u6539\u53d8\u63a8\u7406\u672c\u8d28\u7684\u524d\u63d0\u4e0b\u81ea\u52a8\u6269\u5c55\u6d4b\u8bd5\u96c6\u4ee5\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa MERGE\uff1a\u901a\u8fc7\u5bf9\u539f\u59cb NLI \u95ee\u9898\u7684\u5f00\u653e\u7c7b\u8bcd\u8fdb\u884c\u6700\u5c0f\u5316\u8868\u8fbe\u66ff\u6362\u6765\u751f\u6210\u53d8\u4f53\uff0c\u5c3d\u91cf\u4fdd\u6301\u539f\u6709\u63a8\u7406\u7ed3\u6784\u548c\u6b63\u786e\u6027\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u53d8\u4f53\u4e0a\u7684\u9884\u6d4b\u6b63\u786e\u6027\u3002", "result": "\u5728\u53d8\u4f53\u4e0a\uff0cNLI \u6a21\u578b\u7684\u8868\u73b0\u4e0b\u964d 4-20%\uff0c\u8868\u660e\u5728\u6781\u5c0f\u6539\u52a8\u7684\u60c5\u5f62\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff1b\u5e76\u5206\u6790\u66ff\u6362\u8bcd\u7684\u8bcd\u7c7b\u3001\u51fa\u73b0\u6982\u7387\u548c\u53ef\u884c\u6027\u7b49\u56e0\u7d20\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": " MERGE \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u8d28\u91cf\u7684\u3001\u53ef\u81ea\u52a8\u5316\u7684\u6cdb\u5316\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709 NLI \u6a21\u578b\u5728\u5bf9\u6700\u5c0f\u8bed\u4e49\u8868\u8fbe\u53d8\u4f53\u4e0a\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u9700\u5173\u6ce8\u66ff\u6362\u7b56\u7565\u5bf9\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24302", "abs": "https://arxiv.org/abs/2510.24302", "authors": ["Shangyu Xing", "Siyuan Wang", "Chenyuan Yang", "Xinyu Dai", "Xiang Ren"], "title": "Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration in Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly with\nalgorithms like Group Relative Policy Optimization (GRPO), has proven highly\neffective in enhancing the reasoning capabilities of large language models.\nHowever, a critical bottleneck in current pipelines lies in the limited\ndiversity of sampled trajectories during group rollouts. Homogeneous\ntrajectories and their associated rewards would diminish the return signals for\npolicy updates, thereby hindering effective policy learning. This lack of\ndiversity stems primarily from token-level stochastic sampling, where local\nvariations are likely to collapse into near-identical reasoning paths. To\naddress this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a\nnovel rollout strategy designed to explicitly promotes trajectory-level\ndiversity by enforcing branching into different candidate tokens likely to\nyield distinct continuations. Specifically, LATR iteratively operates in three\nstages: (1) branching at high-uncertainty generation steps, (2) performing\nlookahead simulation for each new branch, and (3) pruning branches that\nexhibits prolonged similarity during simulation. Compared with stochastic\nSampling, LATR accelerates policy learning by 131% on average and improves\nfinal pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy\nOptimization (DAPO) algorithms across different reasoning tasks. Our code and\ndata are publicly available at https://github.com/starreeze/latr.", "AI": {"tldr": "\u63d0\u51fa Lookahead Tree-Based Rollouts (LATR) \u4ee5\u63d0\u5347\u8f68\u8ff9\u5c42\u9762\u7684\u591a\u6837\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u6548\u7387\u4e0e\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5728\u7ec4\u56de\u653e\u4e2d\u5b58\u5728\u9ad8\u540c\u8d28\u6027\uff0ctoken \u7ea7\u968f\u673a\u91c7\u6837\u6613\u5bfc\u81f4\u76f8\u4f3c\u7684\u63a8\u7406\u8def\u5f84\uff0c\u524a\u5f31\u7b56\u7565\u66f4\u65b0\u7684\u56de\u62a5\u4fe1\u53f7\u3002", "method": "LATR \u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u6b65\u9aa4\u8fdb\u884c\u5206\u652f\uff1b\u5bf9\u6bcf\u4e2a\u65b0\u5206\u652f\u6267\u884c\u524d\u77bb\u4eff\u771f\uff1b\u5bf9\u5728\u4eff\u771f\u4e2d\u8868\u73b0\u51fa\u957f\u671f\u76f8\u4f3c\u6027\u7684\u5206\u652f\u8fdb\u884c\u526a\u679d\u3002\u4e0e\u968f\u673a\u91c7\u6837\u76f8\u6bd4\uff0c\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u901f\u5ea6\u5e76\u6539\u5584\u6700\u7ec8\u6027\u80fd\u3002", "result": "\u76f8\u8f83\u4e8e\u968f\u673a\u91c7\u6837\uff0cLATR \u5e73\u5747\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u901f\u5ea6\u7ea6131%\uff0c\u5728 GRPO \u548c DAPO \u4e0a\u7684\u6700\u7ec8 pass@1 \u63d0\u5347\u7ea64.2%\uff0c\u5e76\u5728\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u4fc3\u8fdb\u8f68\u8ff9\u5c42\u9762\u7684\u591a\u6837\u6027\uff0cLATR \u80fd\u6709\u6548\u63d0\u5347\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u4e14\u4ee3\u7801\u4e0e\u6570\u636e\u516c\u5f00\u3002"}}
{"id": "2510.24320", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24320", "abs": "https://arxiv.org/abs/2510.24320", "authors": ["Zhiheng Xi", "Jixuan Huang", "Xin Guo", "Boyang Hong", "Dingwen Yang", "Xiaoran Fan", "Shuo Li", "Zehui Chen", "Junjie Ye", "Siyu Yuan", "Zhengyin Du", "Xuesong Yao", "Yufei Xu", "Jiecao Chen", "Rui Zheng", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning", "comment": "Preprint, 25 pages, 9 figures. Code:\n  https://github.com/WooooDyy/Critique-RL", "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.", "AI": {"tldr": "\u63d0\u51fa Critique-RL\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u5728\u65e0\u5f3a\u76d1\u7763\u4e0b\u8bad\u7ec3\u8bc4\u5ba1\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u8bc4\u4f30\u5e76\u6539\u8fdb\u6a21\u578b\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u66f4\u5f3a\u76d1\u7763\u7684\u8bc4\u5ba1\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u6613\u6269\u5c55\uff1b\u9700\u8981\u4e00\u79cd\u65e0\u5f3a\u76d1\u7763\u7684\u5728\u7ebfRL\u65b9\u6cd5\u6765\u63d0\u5347\u8bc4\u5ba1\u8d28\u91cf\u3002", "method": "\u4e24\u9636\u6bb5\u4f18\u5316\uff1a1) \u8bc4\u5ba1\u5668(discriminator)\u901a\u8fc7\u76f4\u63a5\u7684\u89c4\u5219\u5956\u52b1\u63d0\u5347\u8fa8\u522b\u6027\uff1b2) \u5f15\u5165\u57fa\u4e8e actor refinment \u7684\u95f4\u63a5\u5956\u52b1\u63d0\u9ad8\u8bc4\u5ba1\u5668\u7684\u6709\u7528\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u6b63\u5219\u5316\u4fdd\u6301\u8fa8\u522b\u6027\uff1b\u53cc\u65b9\u5728 actor-critic \u4e92\u52a8\u4e2d\u8fed\u4ee3\u3002", "result": "\u5728\u591a\u4efb\u52a1/\u591a\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cQwen2.5-7B \u5728\u57df\u5185\u4efb\u52a1\u63d0\u5347\u7ea69.02%\uff0c\u5728\u57df\u5916\u4efb\u52a1\u63d0\u5347\u7ea65.70%\uff0c\u663e\u793a\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": " Critique-RL \u80fd\u5728\u4e0d\u4f9d\u8d56\u5f3a\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u8bc4\u5ba1\u8bed\u8a00\u6a21\u578b\u7684\u8fa8\u522b\u529b\u4e0e\u6709\u7528\u6027\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u6a21\u578b\u8868\u73b0\u3002"}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "\u5bf9\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u516b\u4e2a\u81ea\u8bbe\u8ba1\u63a8\u7406\u9898\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5bf9\u6bd4\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\u548c\u4e0d\u8db3\u3002", "motivation": "\u63d0\u5347\u5bf9LLMs\u63a8\u7406\u80fd\u529b\u7684\u7406\u89e3\uff0c\u8d85\u8d8a\u8868\u9762\u7684\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4fe1\u606f\u7406\u89e3\u3001\u63a8\u7406\u548c\u903b\u8f91\u63a8\u65ad\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528GPT\u3001Claude\u3001DeepSeek\u3001Gemini\u3001Grok\u3001Llama\u3001Mistral\u3001Perplexity\u3001Sabi\\'a \u7b49\u6a21\u578b\uff0c\u5bf9\u516b\u4e2a\u81ea\u8bbe\u7684\u63a8\u7406\u95ee\u9898\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u540c\u9898\u7684\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u4e0e\u4eba\u7c7b\u5bf9\u7167\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u5728\u67d0\u4e9b\u9898\u578b\u4e0a\uff0c\u6a21\u578b\u96be\u4ee5\u8fdb\u884c\u7a33\u5b9a\u4e14\u53ef\u9760\u7684\u63a8\u7406\uff0c\u663e\u793a\u51fa\u5bf9\u63a8\u7406\u548c\u6f14\u7ece\u63a8\u65ad\u7684\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u5f53\u524dLLMs\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u793a\u9700\u8981\u66f4\u5177\u9488\u5bf9\u6027\u7684\u63a8\u7406\u8bc4\u4f30\u548c\u6a21\u578b\u6539\u8fdb\uff0c\u4ee5\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u7684\u5dee\u8ddd\uff0c\u5e76\u63a8\u52a8\u66f4\u53ef\u9760\u7684\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24328", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24328", "abs": "https://arxiv.org/abs/2510.24328", "authors": ["Hunzalah Hassan Bhatti", "Firoj Alam"], "title": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants", "comment": "Cultural Knowledge, Everyday Knowledge, Open-Ended Question,\n  Chain-of-Thought, Large Language Models, Native, Multilingual, Language\n  Diversity", "summary": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8de8\u8bed\u8a00\u8bc4\u4f30\u4e0e\u8bad\u7ec3\u6846\u67b6\uff0c\u7ffb\u8bd1\u5e76\u8f6c\u5316MCQs\u4e3aOEQs\uff0c\u8bc4\u4f30\u591a\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u7528Chain-of-Thought\u589e\u5f3a\u63a8\u7406\uff0c\u6700\u7ec8\u516c\u5f00\u8de8\u8bed\u8a00\u5bf9\u9f50\u6570\u636e\u96c6\u3002", "motivation": "\u5f25\u5408\u6587\u5316\u4e0e\u65b9\u8a00\u5bf9LLMs\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u5bf9\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u53ca\u5176\u6587\u5316\u80cc\u666f\u7684\u7406\u89e3\u4e0e\u516c\u5e73\u6027\u3002", "method": "- \u5c06MSA\u7684\u591a\u9879\u9009\u62e9\u9898\u7ffb\u8bd1\u6210\u82f1\u8bed\u548c\u591a\u79cd\u963f\u62c9\u4f2f\u65b9\u8a00\uff1b- \u5c06\u9898\u76ee\u8f6c\u6362\u4e3a\u5f00\u653e\u5f0f\u95ee\u7b54\uff08OEQ\uff09\uff1b- \u5bf9\u6bd4\u8bc4\u4f30\u96f6-shot\u4e0e\u5fae\u8c03\u6a21\u578b\u5728MCQ\u4e0eOEQ\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1b- \u751f\u6210Chain-of-Thought\u63a8\u7406\u7ebf\u7d22\u7528\u4e8e\u5bf9\u6a21\u578b\u7684\u9010\u6b65\u63a8\u7406\u5fae\u8c03\uff1b- \u6269\u5c55\u5e76\u5bf9\u9f50\u8de8\u8bed\u8a00\u53d8\u91cf\u7684\u73b0\u6709\u6570\u636e\u96c6\uff0c\u786e\u4fddQAs\u5e76\u884c\u5bf9\u9f50\uff1b- \u8ba1\u5212\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u65b9\u8a00\u4e0a\u7684\u8868\u73b0\u8f83\u5f31\uff0c\u5b58\u5728\u6587\u5316\u76f8\u5173\u4e0e\u65b9\u8a00\u77e5\u8bc6\u7684\u4e0d\u8db3\uff1b\u4ee5\u963f\u62c9\u4f2f\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u5728MCQ\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728OEQ\u4e0a\u8868\u73b0\u6b20\u4f73\uff1b\u5f15\u5165CoT\u540e\uff0c\u5224\u65ad\u7684\u6b63\u786e\u6027\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5728\u57fa\u4e8en-gram\u7684\u8bc4\u4f30\u6307\u6807\u4e0a\u5448\u73b0\u6df7\u5408\u7ed3\u679c\u3002", "conclusion": "\u5c06\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u652f\u6491\u66f4\u591a\u5173\u4e8e\u6587\u5316\u4e0e\u8bed\u8a00\u5305\u5bb9\u6027\u8bc4\u4f30\u7684\u7814\u7a76\uff1b\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u5728\u65b9\u8a00\u53caOEQ\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2510.23794", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23794", "abs": "https://arxiv.org/abs/2510.23794", "authors": ["Jun Liu", "Tao Zhou", "Jiarui Li", "Xiaohui Zhong", "Peng Zhang", "Jie Feng", "Lei Chen", "Hao Li"], "title": "Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction", "comment": "30 pages, 21 figures, 1 table", "summary": "Tropical cyclones (TCs) are highly destructive and inherently uncertain\nweather systems. Ensemble forecasting helps quantify these uncertainties, yet\ntraditional systems are constrained by high computational costs and limited\ncapability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a\nlearnable perturbation scheme for ensemble generation, representing a novel\nAI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with\nECMWF-ENS using all 90 global TCs in 2018, examining their performance in\nTC-related physical variables, track and intensity forecasts, and the\nassociated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear\nadvantages in predicting TC-related physical variables, and achieves more\naccurate track forecasts with reduced ensemble spread, though it still\nunderestimates intensity relative to observations. Further dynamical and\nthermodynamical analyses reveal that FuXi-ENS better captures large-scale\ncirculation, with moisture turbulent energy more tightly concentrated around\nthe TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution.\nThese findings highlight the potential of learnable perturbations to improve TC\nforecasting skill and provide valuable insights for advancing AI-based ensemble\nprediction of extreme weather events that have significant societal impacts.", "AI": {"tldr": "FuXi-ENS \u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u6270\u52a8\u751f\u6210\u96c6\u5408\u9884\u62a5\uff0c\u6bd4\u8f83\u5176\u4e0e ECMWF-ENS \u57282018\u5e74\u5168\u740390\u4e2a\u70ed\u5e26\u6c14\u65cb\u4e0a\u7684\u9884\u6d4b\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a FuXi-ENS \u5728\u70ed\u5e26\u6c14\u65cb\u76f8\u5173\u7269\u7406\u53d8\u91cf\u548c\u8def\u5f84\u9884\u62a5\u4e0a\u6709\u4f18\u52bf\uff0c\u4e14\u96c6\u5408\u6563\u5e03\u66f4\u5c0f\uff0c\u4f46\u5bf9\u5f3a\u5ea6\u4ecd\u4f4e\u4f30\uff1b\u5728\u52a8\u529b\u70ed\u529b\u573a\u4e0a\u5bf9\u5927\u5c3a\u5ea6\u73af\u6d41\u4e0e\u6696\u5fc3\u533a\u6da1\u52a8\u80fd\u7684\u5206\u5e03\u66f4\u805a\u62e2\u3002\u7ed3\u8bba\uff1a\u53ef\u5b66\u4e60\u6270\u52a8\u5177\u6709\u63d0\u5347AI\u9a71\u52a8\u96c6\u5408\u9884\u62a5\u7684\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u70ed\u5e26\u6c14\u65cb\u9884\u62a5\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u96be\u9898\u3001\u964d\u4f4e\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u4ee5\u53ca\u6539\u8fdb\u5bf9\u5927\u6c14\u975e\u7ebf\u6027\u6548\u5e94\u7684\u518d\u73b0\u80fd\u529b\uff0c\u63a2\u7d22\u57fa\u4e8eAI\u7684\u53ef\u5b66\u4e60\u6270\u52a8\u5728\u96c6\u5408\u9884\u62a5\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5c06 FuXi-ENS \u4e0e ECMWF-ENS \u57282018\u5e74\u5168\u740390\u4e2a\u70ed\u5e26\u6c14\u65cb\u4e0a\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u70ed\u5e26\u6c14\u65cb\u76f8\u5173\u7269\u7406\u53d8\u91cf\u3001\u8def\u5f84\u4e0e\u5f3a\u5ea6\u9884\u62a5\u53ca\u76f8\u5173\u7684\u52a8\u529b\u4e0e\u70ed\u529b\u573a\u3002\u901a\u8fc7\u5206\u6790\u5927\u5c3a\u5ea6\u73af\u6d41\u3001\u6696\u6838\u5468\u56f4\u7684\u6c34\u6c7d\u6e4d\u80fd\u5206\u5e03\u7b49\u6765\u8bc4\u4f30\u4e24\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u4e0e\u70ed\u529b\u5b66\u8868\u73b0\u3002", "result": "FuXi-ENS \u5728\u70ed\u5e26\u6c14\u65cb\u76f8\u5173\u7269\u7406\u53d8\u91cf\u7684\u9884\u6d4b\u66f4\u4e3a\u6e05\u6670\uff0c\u8f68\u8ff9\u9884\u62a5\u66f4\u51c6\u786e\u4e14\u96c6\u5408\u6563\u5e03\u8f83\u5c0f\uff0c\u4f46\u5bf9\u5f3a\u5ea6\u4ecd\u5b58\u5728\u4f4e\u4f30\uff0c\u4e0e\u89c2\u6d4b\u76f8\u6bd4\u6709\u6240\u4e0d\u8db3\u3002\u52a8\u529b\u70ed\u529b\u5206\u6790\u663e\u793a FuXi-ENS \u80fd\u66f4\u597d\u5730\u523b\u753b\u5927\u5c3a\u5ea6\u73af\u6d41\uff0c\u6696\u6838\u9644\u8fd1\u6c34\u6c7d\u6e4d\u80fd\u5206\u5e03\u66f4\u96c6\u4e2d\uff1b\u800c ECMWF-ENS \u7684\u5206\u5e03\u66f4\u5206\u6563\u3002", "conclusion": "\u53ef\u5b66\u4e60\u6270\u52a8\u4e3a\u6539\u8fdb\u70ed\u5e26\u6c14\u65cb\u53ca\u6781\u7aef\u5929\u6c14\u7684AI\u9a71\u52a8\u96c6\u5408\u9884\u62a5\u63d0\u4f9b\u6f5c\u529b\uff0c\u4e14\u6709\u52a9\u4e8e\u63a8\u8fdb\u57fa\u4e8e AI \u7684\u96c6\u5408\u9884\u62a5\u5728\u793e\u4f1a\u5f71\u54cd\u91cd\u5927\u4e8b\u4ef6\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.24345", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24345", "abs": "https://arxiv.org/abs/2510.24345", "authors": ["Zikai Xiao", "Fei Huang", "Jianhong Tu", "Jianhui Wei", "Wen Ma", "Yuxuan Zhou", "Jian Wu", "Bowen Yu", "Zuozhu Liu", "Junyang Lin"], "title": "LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability", "comment": "EMNLP Findings 2025", "summary": "Generating long, informative, and factual outputs remains a major challenge\nfor Large Language Models (LLMs). Existing benchmarks for long-form generation\ntypically assess real-world queries with hard-to-verify metrics or use\nsynthetic setups that ease evaluation but overlook real-world intricacies. In\nthis paper, we introduce \\textbf{LongWeave}, which balances real-world and\nverifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval\nconstructs tasks by first defining verifiable targets within real-world\nscenarios, then systematically generating corresponding queries, textual\nmaterials, and constraints based on these targets. This ensures that tasks are\nboth realistic and objectively assessable, enabling rigorous assessment of\nmodel capabilities in meeting complex real-world constraints. LongWeave\nsupports customizable input/output lengths (up to 64K/8K tokens) across seven\ndistinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models\nencounter significant challenges in long-form generation as real-world\ncomplexity and output length increase.", "AI": {"tldr": "LongWeave \u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u6838\u9a8c\u7684\u8bc4\u4f30\u6846\u67b6 CoV-Eval\uff0c\u7528\u4e8e\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u8868\u8ff0\u8f93\u51fa\u8fdb\u884c\u73b0\u5b9e\u573a\u666f\u4e0e\u53ef\u9a8c\u8bc1\u6027\u5e76\u91cd\u7684\u8bc4\u4f30\u3002\u7cfb\u7edf\u652f\u6301\u4e03\u7c7b\u4efb\u52a1\u3001\u8f93\u5165/\u8f93\u51fa\u957f\u5ea6\u53ef\u6269\u5c55\u81f3 64K/8K tokens\uff0c\u5728 23 \u79cd\u6a21\u578b\u4e0a\u7684\u8bc4\u6d4b\u663e\u793a\uff0c\u968f\u7740\u590d\u6742\u6027\u548c\u8f93\u51fa\u957f\u5ea6\u589e\u52a0\uff0c\u6700\u5148\u8fdb\u6a21\u578b\u4ecd\u663e\u8457\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u957f\u7bc7\u751f\u6210\u57fa\u51c6\u8981\u4e48\u4f9d\u8d56\u96be\u4ee5\u9a8c\u8bc1\u7684\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\uff0c\u8981\u4e48\u4f7f\u7528\u7b80\u5316\u7684\u3001\u8131\u79bb\u771f\u5b9e\u4e16\u754c\u7ec6\u8282\u7684\u5408\u6210\u573a\u666f\uff0c\u5bfc\u81f4\u5bf9\u6a21\u578b\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u957f\u8868\u8ff0\u80fd\u529b\u8bc4\u4f30\u4e0d\u5145\u5206\u3002\u9700\u8981\u4e00\u4e2a\u517c\u987e\u73b0\u5b9e\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e14\u80fd\u8986\u76d6\u8f83\u5927\u8f93\u51fa\u957f\u5ea6\uff0c\u4ee5\u771f\u5b9e\u4e16\u754c\u7ea6\u675f\u6765\u8861\u91cf\u6a21\u578b\u80fd\u529b\u3002", "method": "CoV-Eval \u901a\u8fc7\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5b9a\u4e49\u53ef\u9a8c\u8bc1\u7684\u76ee\u6807\uff0c\u5e76\u636e\u6b64\u7cfb\u7edf\u5316\u751f\u6210\u76f8\u5e94\u7684\u95ee\u9898\u3001\u6587\u672c\u6750\u6599\u548c\u7ea6\u675f\u6761\u4ef6\uff0c\u4f7f\u4efb\u52a1\u65e2\u771f\u5b9e\u53c8\u53ef\u5ba2\u89c2\u8bc4\u4f30\u3002LongWeave \u63d0\u4f9b\u53ef\u5b9a\u5236\u7684\u8f93\u5165/\u8f93\u51fa\u957f\u5ea6\uff08\u9ad8\u8fbe 64K/8K token\uff09\uff0c\u6db5\u76d6\u4e03\u9879\u4efb\u52a1\uff0c\u5bf9 23 \u79cd LLMs \u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "\u8bc4\u6d4b\u53d1\u73b0\uff0c\u5373\u4fbf\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5728\u5904\u7406\u9ad8\u590d\u6742\u5ea6\u73b0\u5b9e\u573a\u666f\u4ee5\u53ca\u5927\u89c4\u6a21\u8f93\u51fa\u65f6\u4e5f\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u957f\u8868\u8ff0\u80fd\u529b\u4ecd\u6709\u660e\u663e\u4e0d\u8db3\uff0c\u9a8c\u8bc1\u6027\u8bc4\u4f30\u6709\u52a9\u4e8e\u63ed\u793a\u6a21\u578b\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "LongWeave \u4e0e CoV-Eval \u4e3a\u957f\u7bc7\u751f\u6210\u7684\u4e25\u683c\u3001\u73b0\u5b9e\u4e0e\u53ef\u9a8c\u8bc1\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u6539\u8fdb\u6a21\u578b\u548c\u6280\u672f\u4ee5\u63d0\u5347\u73b0\u5b9e\u4e16\u754c\u957f\u8868\u8ff0\u80fd\u529b\u7684\u9700\u6c42\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5e73\u53f0\u4ee5\u8986\u76d6\u591a\u6837\u4efb\u52a1\u4e0e\u6781\u7aef\u957f\u5ea6\u3002"}}
{"id": "2510.23802", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23802", "abs": "https://arxiv.org/abs/2510.23802", "authors": ["Nathan Paek", "Yongyi Zang", "Qihui Yang", "Randal Leistikow"], "title": "Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders", "comment": "Accepted to NeurIPS 2025 Mechanistic Interpretability Workshop", "summary": "While sparse autoencoders (SAEs) successfully extract interpretable features\nfrom language models, applying them to audio generation faces unique\nchallenges: audio's dense nature requires compression that obscures semantic\nmeaning, and automatic feature characterization remains limited. We propose a\nframework for interpreting audio generative models by mapping their latent\nrepresentations to human-interpretable acoustic concepts. We train SAEs on\naudio autoencoder latents, then learn linear mappings from SAE features to\ndiscretized acoustic properties (pitch, amplitude, and timbre). This enables\nboth controllable manipulation and analysis of the AI music generation process,\nrevealing how acoustic properties emerge during synthesis. We validate our\napproach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer)\naudio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music\nmodel, to demonstrate how pitch, timbre, and loudness evolve throughout\ngeneration. While our work is only done on audio modality, our framework can be\nextended to interpretable analysis of visual latent space generation models.", "AI": {"tldr": "\u63d0\u51fa\u628a\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u7528\u4e8e\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7\u7ebf\u6027\u6620\u5c04\u5c06SAE\u7279\u5f81\u6620\u5c04\u5230\u79bb\u6563\u5316\u7684\u58f0\u5b66\u5c5e\u6027\uff08\u97f3\u9ad8\u3001\u632f\u5e45\u3001\u97f3\u8272\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9AI\u97f3\u4e50\u751f\u6210\u7684\u53ef\u63a7\u548c\u5206\u6790\u3002\u5bf9\u8fde\u7eed\u4e0e\u79bb\u6563\u97f3\u9891\u6f5c\u5728\u7a7a\u95f4\uff08DiffRhythm-VAE\u3001EnCodec\u3001WavTokenizer\uff09\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u5206\u6790DiffRhythm\u7684\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u58f0\u5b66\u5c5e\u6027\u6f14\u5316\u3002\u8be5\u6846\u67b6\u4ec5\u5728\u97f3\u9891\u6a21\u6001\u4e0b\u5b9e\u73b0\uff0c\u7406\u8bba\u4e0a\u53ef\u6269\u5c55\u5230\u89c6\u89c9\u6a21\u6001\u3002", "motivation": "\u89e3\u51b3\u97f3\u9891\u751f\u6210\u4e2d\u8bed\u4e49\u4fe1\u606f\u7684\u9ad8\u5bc6\u5ea6\u6027\u5bfc\u81f4\u7684\u538b\u7f29\u56f0\u96be\uff0c\u4ee5\u53ca\u81ea\u52a8\u7279\u5f81\u8868\u5f81\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e00\u79cd\u5c06\u6f5c\u5728\u8868\u793a\u6620\u5c04\u5230\u53ef\u89e3\u91ca\u58f0\u5b66\u6982\u5ff5\u7684\u6846\u67b6\u3002", "method": "\u5728\u97f3\u9891\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\u8bad\u7ec3SAEs\uff0c\u5b66\u4e60\u4eceSAE\u7279\u5f81\u5230\u79bb\u6563\u5316\u58f0\u5b66\u5c5e\u6027\u7684\u7ebf\u6027\u6620\u5c04\uff08pitch\u3001amplitude\u3001timbre\uff09\uff0c\u5b9e\u73b0\u53ef\u63a7\u64cd\u4f5c\u548c\u5206\u6790\uff1b\u5bf9DiffRhythm-VAE\u3001EnCodec\u3001WavTokenizer\u7b49\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5206\u6790Pitch\u3001Timbre\u3001Loudness\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6f14\u53d8\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u7ebf\u6027\u6620\u5c04\u53ef\u4ee5\u4ece\u6f5c\u5728\u7a7a\u95f4\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u58f0\u5b66\u5c5e\u6027\uff0c\u5e76\u5728\u8fde\u7eed\u548c\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u5c5e\u6027\u7684\u64cd\u63a7\u548c\u5206\u6790\uff0c\u63ed\u793a\u58f0\u5b66\u5c5e\u6027\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5f62\u6210\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4e3a\u97f3\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u63a7\u6027\uff0c\u5e76\u5177\u6709\u6f5c\u5728\u6269\u5c55\u6027\u5230\u5176\u4ed6\u6a21\u6001\u7684\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2510.23804", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23804", "abs": "https://arxiv.org/abs/2510.23804", "authors": ["Adela DePavia", "Vasileios Charisopoulos", "Rebecca Willett"], "title": "How do simple rotations affect the implicit bias of Adam?", "comment": null, "summary": "Adaptive gradient methods such as Adam and Adagrad are widely used in machine\nlearning, yet their effect on the generalization of learned models -- relative\nto methods like gradient descent -- remains poorly understood. Prior work on\nbinary classification suggests that Adam exhibits a ``richness bias,'' which\ncan help it learn nonlinear decision boundaries closer to the Bayes-optimal\ndecision boundary relative to gradient descent. However, the coordinate-wise\npreconditioning scheme employed by Adam renders the overall method sensitive to\northogonal transformations of feature space. We show that this sensitivity can\nmanifest as a reversal of Adam's competitive advantage: even small rotations of\nthe underlying data distribution can make Adam forfeit its richness bias and\nconverge to a linear decision boundary that is farther from the Bayes-optimal\ndecision boundary than the one learned by gradient descent. To alleviate this\nissue, we show that a recently proposed reparameterization method -- which\napplies an orthogonal transformation to the optimization objective -- endows\nany first-order method with equivariance to data rotations, and we empirically\ndemonstrate its ability to restore Adam's bias towards rich decision\nboundaries.", "AI": {"tldr": "Adam \u7684\u4f18\u52bf\u5728\u4e8e\u6570\u636e\u65cb\u8f6c\u4e0b\u53ef\u80fd\u6d88\u5931\uff1b\u5e94\u7528\u6b63\u4ea4\u91cd\u53c2\u6570\u5316\u540e\uff0c\u7b2c\u4e00\u9636\u4f18\u5316\u5668\u5bf9\u6570\u636e\u65cb\u8f6c\u5177\u5907\u7b49\u53d8\u6027\uff0c\u80fd\u6062\u590d\u5176\u201c\u4e30\u5bcc\u6027\u504f\u7f6e\u201d\u3002", "motivation": "\u5206\u6790\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u6cd5\uff08\u5982 Adam\u3001Adagrad\uff09\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u8003\u8651\u6570\u636e\u7684\u6b63\u4ea4\u65cb\u8f6c\uff08\u65cb\u8f6c\u53d8\u6362\uff09\u5bf9\u5b83\u4eec\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u6062\u590d\u5bf9\u4e30\u5bcc\u8fb9\u754c\u7684\u504f\u597d\u3002", "method": "\u5bf9\u5750\u6807\u9884\u8bbe(\u9884\u6761\u4ef6)\u5728\u6b63\u4ea4\u6570\u636e\u65cb\u8f6c\u4e0b\u7684\u884c\u4e3a\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff1b\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83 Adam\u3001Adagrad\u3001\u68af\u5ea6\u4e0b\u964d\u5728\u65cb\u8f6c\u6570\u636e\u4e0a\u7684\u8fb9\u754c\u7279\u5f81\uff1b\u5f15\u5165\u4e00\u79cd\u6700\u8fd1\u63d0\u51fa\u7684\u91cd\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5bf9\u4f18\u5316\u76ee\u6807\u5e94\u7528\u6b63\u4ea4\u53d8\u6362\uff0c\u4f7f\u4f18\u5316\u8fc7\u7a0b\u5bf9\u6570\u636e\u65cb\u8f6c\u5177\u6709\u7b49\u53d8\u6027\uff0c\u5e76\u6d4b\u8bd5\u5176\u5bf9\u5404\u79cd\u4e00\u9636\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u5c0f\u5e45\u5ea6\u7684\u6570\u636e\u65cb\u8f6c\u5373\u53ef\u4f7f Adam \u7684\u4e30\u5bcc\u6027\u504f\u7f6e\u6d88\u5931\uff0c\u6536\u655b\u5230\u6bd4\u68af\u5ea6\u4e0b\u964d\u66f4\u8fdc\u79bb Bayes \u6700\u4f18\u8fb9\u754c\u7684\u7ebf\u6027\u8fb9\u754c\uff1b\u800c\u901a\u8fc7\u91cd\u53c2\u6570\u5316\uff08\u5bf9\u4f18\u5316\u76ee\u6807\u8fdb\u884c\u6b63\u4ea4\u53d8\u6362\uff09\u540e\uff0c\u80fd\u591f\u4e3a\u4efb\u4e00\u4e00\u9636\u65b9\u6cd5\u5f15\u5165\u5bf9\u6570\u636e\u65cb\u8f6c\u7684\u7b49\u53d8\u6027\uff0c\u6062\u590d Adam \u5bf9\u4e30\u5bcc\u8fb9\u754c\u7684\u504f\u597d\u3002", "conclusion": "\u6b63\u4ea4\u91cd\u53c2\u6570\u5316\u4e3a\u4e00\u9636\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u6709\u6548\u7684\u65cb\u8f6c\u7b49\u53d8\u6027\u673a\u5236\uff0c\u80fd\u591f\u5728\u65cb\u8f6c\u6570\u636e\u5206\u5e03\u4e0b\u4fdd\u6301\u6216\u6062\u590d\u5bf9\u4e30\u5bcc\u8fb9\u754c\u7684\u504f\u597d\uff0c\u4ece\u800c\u6539\u5584\u6cdb\u5316\u8868\u73b0\u3002"}}
{"id": "2510.24425", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24425", "abs": "https://arxiv.org/abs/2510.24425", "authors": ["Guangyu Xie", "Yice Zhang", "Jianzhu Bao", "Qianlong Wang", "Yang Sun", "Bingbing Wang", "Ruifeng Xu"], "title": "Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis Models", "comment": "Accepted by EMNLP 2025. 22 pages, 9 figures. The first two authors\n  contribute equally", "summary": "Recent efforts leverage knowledge distillation techniques to develop\nlightweight and practical sentiment analysis models. These methods are grounded\nin human-written instructions and large-scale user texts. Despite the promising\nresults, two key challenges remain: (1) manually written instructions are\nlimited in diversity and quantity, making them insufficient to ensure\ncomprehensive coverage of distilled knowledge; (2) large-scale user texts incur\nhigh computational cost, hindering the practicality of these methods. To this\nend, we introduce COMPEFFDIST, a comprehensive and efficient distillation\nframework for sentiment analysis. Our framework consists of two key modules:\nattribute-based automatic instruction construction and difficulty-based data\nfiltering, which correspondingly tackle the aforementioned challenges. Applying\nour method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we\nenable 3B student models to match the performance of 20x larger teacher models\non most tasks. In addition, our approach greatly outperforms baseline methods\nin data efficiency, attaining the same performance level with only 10% of the\ndata.", "AI": {"tldr": "\u63d0\u51fa COMPEFFDIST\uff1a\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u9ad8\u6548\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u60c5\u611f\u5206\u6790\uff0c\u89e3\u51b3\u624b\u5de5\u6307\u4ee4\u8986\u76d6\u4e0d\u8db3\u548c\u5927\u89c4\u6a21\u7528\u6237\u6587\u672c\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u901a\u8fc7\u57fa\u4e8e\u5c5e\u6027\u7684\u81ea\u52a8\u6307\u4ee4\u751f\u6210\u4e0e\u57fa\u4e8e\u96be\u5ea6\u7684\u6570\u636e\u7b5b\u9009\uff0c\u5b9e\u73b0\u5c0f\u6a21\u578b\u5728\u591a\u6570\u4efb\u52a1\u4e0a\u8fbe\u5230\u66f4\u5927\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u5e76\u663e\u8457\u63d0\u5347\u6570\u636e\u6548\u7387\uff08\u4ec5\u7528 10% \u6570\u636e\uff09\u3002", "motivation": "\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u53d7\u9650\u4e8e\u624b\u5de5\u7f16\u5199\u7684\u6307\u4ee4\u5728\u591a\u6837\u6027\u548c\u6570\u91cf\u4e0a\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u4f7f\u7528\u5927\u89c4\u6a21\u7528\u6237\u6587\u672c\u5e26\u6765\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u96be\u4ee5\u5b9e\u73b0\u5168\u9762\u4e14\u9ad8\u6548\u7684\u84b8\u998f\u3002", "method": "\u63d0\u51fa COMPEFFDIST\uff0c\u5305\u542b\u4e24\u5927\u6a21\u5757\uff1a1) \u5c5e\u6027\u9a71\u52a8\u7684\u81ea\u52a8\u6307\u4ee4\u751f\u6210\uff0c\u7528\u4ee5\u6269\u5c55\u6307\u4ee4\u8986\u76d6\u9762\uff1b2) \u96be\u5ea6\u5206\u7ea7\u7684\u6570\u636e\u7b5b\u9009\uff0c\u4ee5\u964d\u4f4e\u6570\u636e\u91cf\u9700\u6c42\u5e76\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002\u65b9\u6cd5\u5728\u591a\u7c7b\u6a21\u578b\uff08Llama-3\u3001Qwen-3\u3001Gemma-3\uff09\u4e0a\u9a8c\u8bc1\uff0c\u5f62\u6210 3B \u5b66\u751f\u6a21\u578b\u4e0e\u6559\u5e08\u6a21\u578b\u7684\u6c34\u5e73\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u6a21\u578b\u7cfb\u5217\u4e0a\uff0c3B \u5b66\u751f\u6a21\u578b\u53ef\u8fbe\u5230 20 \u500d\u89c4\u6a21\u6559\u5e08\u6a21\u578b\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff1b\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6570\u636e\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u540c\u7b49\u6027\u80fd\u4ec5\u9700\u7ea6 10% \u7684\u6570\u636e\u3002", "conclusion": "COMPEFFDIST \u4e3a\u60c5\u611f\u5206\u6790\u7684\u84b8\u998f\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u6307\u4ee4\u751f\u6210\u4e0e\u96be\u5ea6\u9a71\u52a8\u7684\u6570\u636e\u7b5b\u9009\uff0c\u6709\u6548\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u8868\u73b0\u5e76\u964d\u4f4e\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.24528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24528", "abs": "https://arxiv.org/abs/2510.24528", "authors": ["Zihan Chen", "Song Wang", "Xingbo Fu", "Chengshuai Shi", "Zhenyu Lei", "Cong Shen", "Jundong Li"], "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning", "comment": null, "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.", "AI": {"tldr": "A cost-efficient two-stage pipeline for in-context learning (ICL): use cross-task prompts to pseudo-label a small subset of a target task with an LLM, then apply a graph-based label propagation to assign labels to the remaining target examples without further LLM queries, and finally use the fully pseudo-labeled data to construct in-task demonstrations for ICL. This achieves competitive performance with lower labeling costs across five tasks.", "motivation": "Labeling data for new or challenging tasks is expensive. The paper seeks to reduce dependence on LLM queries for data labeling by reusing cross-task exemplars and propagating labels to many instances, enabling scalable construction of in-task demonstrations for ICL.", "method": "1) Use cross-task exemplars to prompt an LLM and pseudo-label a small set of target task instances. 2) Apply a graph-based label propagation method to spread the label information to the remaining target examples without additional LLM queries. 3) Use the fully pseudo-labeled dataset to create in-task demonstrations for ICL.", "result": "Empirical evaluation on five tasks shows the proposed pipeline achieves strong performance while significantly lowering labeling costs compared to methods requiring full in-task labeling or exhaustive LLM querying for labeling.", "conclusion": "A hybrid approach that combines cross-task supervision with LLM-free label propagation can produce effective in-task demonstrations for ICL with reduced labeling effort, offering a scalable solution for data labeling in new tasks."}}
{"id": "2510.23810", "categories": ["cs.LG", "math.AP", "physics.comp-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23810", "abs": "https://arxiv.org/abs/2510.23810", "authors": ["Sumanta Roy", "Bahador Bahmani", "Ioannis G. Kevrekidis", "Michael D. Shields"], "title": "A Physics-informed Multi-resolution Neural Operator", "comment": "26 pages, 14 figures, 4 tables", "summary": "The predictive accuracy of operator learning frameworks depends on the\nquality and quantity of available training data (input-output function pairs),\noften requiring substantial amounts of high-fidelity data, which can be\nchallenging to obtain in some real-world engineering applications. These\ndatasets may be unevenly discretized from one realization to another, with the\ngrid resolution varying across samples. In this study, we introduce a\nphysics-informed operator learning approach by extending the Resolution\nIndependent Neural Operator (RINO) framework to a fully data-free setup,\naddressing both challenges simultaneously. Here, the arbitrarily (but\nsufficiently finely) discretized input functions are projected onto a latent\nembedding space (i.e., a vector space of finite dimensions), using pre-trained\nbasis functions. The operator associated with the underlying partial\ndifferential equations (PDEs) is then approximated by a simple multi-layer\nperceptron (MLP), which takes as input a latent code along with spatiotemporal\ncoordinates to produce the solution in the physical space. The PDEs are\nenforced via a finite difference solver in the physical space. The validation\nand performance of the proposed method are benchmarked on several numerical\nexamples with multi-resolution data, where input functions are sampled at\nvarying resolutions, including both coarse and fine discretizations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u65e0\u5173\u3001\u5206\u8fa8\u7387\u65e0\u5173\u7684\u7b97\u5b50\u5b66\u4e60\u6846\u67b6\uff0c\u6269\u5c55 RINO\uff0c\u4f7f\u5176\u5728\u6570\u636e\u7f3a\u4e4f\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5904\u7406\u591a\u5206\u8fa8\u7387\u8f93\u5165\u5e76\u89e3 PDE\u3002", "motivation": "\u5728\u5de5\u7a0b\u5e94\u7528\u4e2d\uff0c\u7b97\u5b50\u5b66\u4e60\u9700\u8981\u5927\u91cf\u9ad8\u4fdd\u771f\u6570\u636e\uff0c\u4e14\u4e0d\u540c\u6837\u672c\u5e38\u4ee5\u4e0d\u540c\u7f51\u683c\u79bb\u6563\u5316\uff0c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u4e0d\u4e00\u81f4\uff0c\u4e9f\u9700\u4e00\u79cd\u4e0d\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u5bf9\u7f51\u683c\u5206\u8fa8\u7387\u9c81\u68d2\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "method": "\u5c06\u4efb\u610f\u5206\u8fa8\u7387\u7684\u8f93\u5165\u51fd\u6570\u6295\u5f71\u5230\u6f5c\u5728\u5d4c\u5165\u7a7a\u95f4\uff08\u4f7f\u7528\u9884\u8bad\u7ec3\u57fa\u51fd\u6570\uff09\uff0c\u7528\u7b80\u5355\u7684\u591a\u5c42\u611f\u77e5\u673a\u8fd1\u4f3c PDE \u7684\u7b97\u5b50\uff0c\u8f93\u5165\u5305\u62ec\u6f5c\u5728\u7801\u548c\u65f6\u7a7a\u5750\u6807\uff0c\u76f4\u63a5\u5728\u7269\u7406\u7a7a\u95f4\u8f93\u51fa\u89e3\uff1b\u901a\u8fc7\u5728\u7269\u7406\u7a7a\u95f4\u5f15\u5165\u6709\u9650\u5dee\u5206\u6c42\u89e3\u5668\u5bf9 PDE \u8fdb\u884c\u7269\u7406\u7ea6\u675f\u3002\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u65e0\u4f9d\u8d56\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u5206\u8fa8\u7387\u6570\u636e\u3002", "result": "\u5728\u82e5\u5e72\u6570\u503c\u4f8b\u5b50\u4e0a\u5bf9\u591a\u5206\u8fa8\u7387\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8f93\u5165\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u91c7\u6837\uff0c\u65b9\u6cd5\u5c55\u73b0\u5bf9\u7f51\u683c\u79bb\u6563\u5316\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u5927\u91cf\u9ad8\u4fdd\u771f\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u6570\u636e\u65e0\u5173\u7b97\u5b50\u5b66\u4e60\u4e0e\u7269\u7406\u7ea6\u675f\u76f8\u7ed3\u5408\uff0c\u6210\u529f\u5b9e\u73b0\u8de8\u7f51\u683c\u5206\u8fa8\u7387\u7684\u6570\u636e\u65e0\u4f9d\u8d56 PDE \u6c42\u89e3\uff0c\u63d0\u4f9b\u4e00\u4e2a\u53ef\u63a8\u5e7f\u7684\u591a\u5206\u8fa8\u7387\u7b97\u5b50\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2510.24427", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24427", "abs": "https://arxiv.org/abs/2510.24427", "authors": ["Ken Gu", "Advait Bhat", "Mike A Merrill", "Robert West", "Xin Liu", "Daniel McDuff", "Tim Althoff"], "title": "SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models", "comment": null, "summary": "Evaluating the reasoning ability of language models (LMs) is complicated by\ntheir extensive parametric world knowledge, where benchmark performance often\nreflects factual recall rather than genuine reasoning. Existing datasets and\napproaches (e.g., temporal filtering, paraphrasing, adversarial substitution)\ncannot cleanly separate the two. We present SynthWorlds, a framework that\ndisentangles task reasoning complexity from factual knowledge. In SynthWorlds,\nwe construct parallel corpora representing two worlds with identical\ninterconnected structure: a real-mapped world, where models may exploit\nparametric knowledge, and a synthetic-mapped world, where such knowledge is\nmeaningless. On top of these corpora, we design two mirrored tasks as case\nstudies: multi-hop question answering and page navigation, which maintain equal\nreasoning difficulty across worlds. Experiments in parametric-only (e.g.,\nclosed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings\nreveal a persistent knowledge advantage gap, defined as the performance boost\nmodels gain from memorized parametric world knowledge. Knowledge acquisition\nand integration mechanisms reduce but do not eliminate this gap, highlighting\nopportunities for system improvements. Fully automatic and scalable,\nSynthWorlds provides a controlled environment for evaluating LMs in ways that\nwere previously challenging, enabling precise and testable comparisons of\nreasoning and memorization.", "AI": {"tldr": "SynthWorlds\u6784\u5efa\u5bf9\u7167\u4e16\u754c\u6846\u67b6\u4ee5\u5206\u79bb\u63a8\u7406\u4e0e\u77e5\u8bc6\u8bb0\u5fc6\uff0c\u63d0\u4f9b\u4e00\u4e2a\u53ef\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u73af\u5883\u6765\u533a\u5206\u63a8\u7406\u80fd\u529b\u4e0e\u53c2\u6570\u5316\u4e16\u754c\u77e5\u8bc6\u7684\u5229\u7528\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u63a8\u7406\u80fd\u529b\u88ab\u5927\u91cf\u53c2\u6570\u5316\u4e16\u754c\u77e5\u8bc6\u6240\u63a9\u76d6\u7684\u95ee\u9898\uff1b\u73b0\u6709\u6570\u636e\u96c6\u65e0\u6cd5\u6e05\u6670\u533a\u5206\u771f\u6b63\u63a8\u7406\u4e0e\u4e8b\u5b9e\u8bb0\u5fc6\u3002", "method": "\u6784\u5efa\u4e24\u5957\u7b49\u7ed3\u6784\u3001\u4e92\u4e3a\u955c\u50cf\u7684\u4e16\u754c\u6570\u636e\u96c6\uff1a\u771f\u5b9e\u6620\u5c04\u4e16\u754c\u4e0e\u5408\u6210\u6620\u5c04\u4e16\u754c\uff0c\u5728\u4e24\u8005\u4e0a\u8bbe\u8ba1\u4e24\u7ec4\u7b49\u96be\u5ea6\u7684\u4efb\u52a1\uff08\u591a\u8df3\u95ee\u7b54\u3001\u9875\u9762\u5bfc\u822a\uff09\uff0c\u5e76\u5728\u53c2\u6570\u5316\u77e5\u8bc6\u4e0e\u77e5\u8bc6\u589e\u5f3a\u573a\u666f\u4e0b\u6bd4\u8f83\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5b58\u5728\u6301\u7eed\u7684\u201c\u77e5\u8bc6\u4f18\u52bf\u5dee\u8ddd\u201d\uff1a\u6a21\u578b\u4ece\u8bb0\u5fc6\u4e2d\u83b7\u5f97\u7684\u6027\u80fd\u63d0\u5347\u5728\u4e24\u79cd\u4e16\u754c\u95f4\u4ecd\u7136\u660e\u663e\uff1b\u77e5\u8bc6\u83b7\u53d6\u4e0e\u6574\u5408\u867d\u7f13\u89e3\u4f46\u672a\u6d88\u9664\u8be5\u5dee\u8ddd\uff1b\u6846\u67b6\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\uff0c\u4fbf\u4e8e\u5bf9\u63a8\u7406\u4e0e\u8bb0\u5fc6\u8fdb\u884c\u7cbe\u786e\u6bd4\u8f83\u3002", "conclusion": "SynthWorlds\u63d0\u4f9b\u4e00\u4e2a\u53d7\u63a7\u73af\u5883\u6765\u8bc4\u4f30LM\u7684\u63a8\u7406\u4e0e\u8bb0\u5fc6\uff0c\u5728\u4ee5\u5f80\u96be\u4ee5\u5b9e\u73b0\u7684\u5c42\u9762\u4e0a\u5b9e\u73b0\u53ef\u6d4b\u8bd5\u7684\u5bf9\u6bd4\uff0c\u5e76\u63ed\u793a\u7cfb\u7edf\u6539\u8fdb\u7684\u5951\u673a\u3002"}}
{"id": "2510.23817", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.23817", "abs": "https://arxiv.org/abs/2510.23817", "authors": ["Pedro Cortes dos Santos", "Matheus Becali Rocha", "Renato A Krohling"], "title": "Combining SHAP and Causal Analysis for Interpretable Fault Detection in Industrial Processes", "comment": null, "summary": "Industrial processes generate complex data that challenge fault detection\nsystems, often yielding opaque or underwhelming results despite advanced\nmachine learning techniques. This study tackles such difficulties using the\nTennessee Eastman Process, a well-established benchmark known for its intricate\ndynamics, to develop an innovative fault detection framework. Initial attempts\nwith standard models revealed limitations in both performance and\ninterpretability, prompting a shift toward a more tractable approach. By\nemploying SHAP (SHapley Additive exPlanations), we transform the problem into a\nmore manageable and transparent form, pinpointing the most critical process\nfeatures driving fault predictions. This reduction in complexity unlocks the\nability to apply causal analysis through Directed Acyclic Graphs, generated by\nmultiple algorithms, to uncover the underlying mechanisms of fault propagation.\nThe resulting causal structures align strikingly with SHAP findings,\nconsistently highlighting key process elements-like cooling and separation\nsystems-as pivotal to fault development. Together, these methods not only\nenhance detection accuracy but also provide operators with clear, actionable\ninsights into fault origins, a synergy that, to our knowledge, has not been\npreviously explored in this context. This dual approach bridges predictive\npower with causal understanding, offering a robust tool for monitoring complex\nmanufacturing environments and paving the way for smarter, more interpretable\nfault detection in industrial systems.", "AI": {"tldr": "\u901a\u8fc7\u5c06 SHAP \u4e0e\u56e0\u679c DAGs \u7ed3\u5408\u7684\u53ef\u89e3\u91ca\u578b\u6545\u969c\u68c0\u6d4b\u6846\u67b6\uff0c\u5728 Tennessee Eastman \u8fc7\u7a0b\u4e2d\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6027\u7684\u56e0\u679c\u6d1e\u5bdf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6545\u969c\u68c0\u6d4b\u5728\u590d\u6742\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u9700\u63d0\u5347\u5bf9\u6545\u969c\u6765\u6e90\u7684\u7406\u89e3\u4e0e\u8bca\u65ad\u80fd\u529b\u3002", "method": "\u521d\u6b65\u91c7\u7528\u6807\u51c6\u6a21\u578b\u4ee5\u8bc4\u4f30\u57fa\u7ebf\u6027\u80fd\u540e\u8f6c\u5411\u53ef\u89e3\u91ca\u6027\u9a71\u52a8\u7684\u6846\u67b6\uff1a\u4f7f\u7528 SHAP \u8bc6\u522b\u6700\u5173\u952e\u7684\u8fc7\u7a0b\u7279\u5f81\u4ee5\u7b80\u5316\u95ee\u9898\uff0c\u518d\u57fa\u4e8e\u591a\u79cd\u7b97\u6cd5\u751f\u6210\u7684\u6709\u5411\u65e0\u73af\u56fe\u8fdb\u884c\u56e0\u679c\u5206\u6790\uff0c\u63ed\u793a\u6545\u969c\u4f20\u64ad\u673a\u5236\u3002", "result": "SHAP \u786e\u5b9a\u7684\u5173\u952e\u7279\u5f81\uff08\u5982\u51b7\u5374\u4e0e\u5206\u79bb\u7cfb\u7edf\uff09\u4e0e\u6545\u969c\u53d1\u5c55\u9ad8\u5ea6\u76f8\u5173\uff1b\u56e0\u679c\u7ed3\u6784\u4e0e SHAP \u7ed3\u679c\u4e00\u81f4\uff0c\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u5e76\u4e3a\u64cd\u4f5c\u5458\u63d0\u4f9b\u6e05\u6670\u7684\u6545\u969c\u6765\u6e90\u6d1e\u5bdf\u3002", "conclusion": "\u4e24\u8005\u7ed3\u5408\u7684\u6846\u67b6\u540c\u65f6\u63d0\u5347\u9884\u6d4b\u80fd\u529b\u4e0e\u56e0\u679c\u7406\u89e3\uff0c\u4e3a\u5728\u590d\u6742\u5236\u9020\u73af\u5883\u4e2d\u7684\u76d1\u63a7\u63d0\u4f9b\u66f4\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u7684\u6545\u969c\u68c0\u6d4b\u5de5\u5177\u3002"}}
{"id": "2510.24434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24434", "abs": "https://arxiv.org/abs/2510.24434", "authors": ["Julian Valline", "Cedric Lothritz", "Jordi Cabot"], "title": "LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed Data", "comment": null, "summary": "The effectiveness of instruction-tuned Large Language Models (LLMs) is often\nlimited in low-resource linguistic settings due to a lack of high-quality\ntraining data. We introduce LuxIT, a novel, monolingual instruction tuning\ndataset for Luxembourgish developed to mitigate this challenge. We synthesize\nthe dataset from a corpus of native Luxembourgish texts, utilizing\nDeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following\ngeneration, we apply a quality assurance process, employing an LLM-as-a-judge\napproach. To investigate the practical utility of the dataset, we fine-tune\nseveral smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base\nmodels on Luxembourgish language proficiency examinations, however, yields\nmixed results, with performance varying significantly across different models.\nLuxIT represents a critical contribution to Luxembourgish natural language\nprocessing and offers a replicable monolingual methodology, though our findings\nhighlight the need for further research to optimize its application.", "AI": {"tldr": "LuxIT\uff1a\u9762\u5411\u5362\u68ee\u5821\u8bed\u7684\u5355\u8bed\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4ece\u672c\u5730\u6587\u672c\u5408\u6210\u5e76\u7528LLM\u4f5c\u4e3a\u8bc4\u5ba1\u8fdb\u884c\u8d28\u91cf\u63a7\u5236\uff0c\u5fae\u8c03\u82e5\u5e72\u5c0f\u578bLLM\uff0c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u5728\u5362\u68ee\u5821\u8bed\u8bed\u8a00\u80fd\u529b\u8003\u8bd5\u4e0a\u5448\u73b0\u6df7\u5408\uff0c\u8868\u660e\u6570\u636e\u96c6\u5177\u6709\u91cd\u8981\u8d21\u732e\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e2d\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u5fae\u8c03\u6570\u636e\uff0c\u5362\u68ee\u5821\u8bed\u9700\u8981\u53ef\u91cd\u590d\u3001\u5355\u8bed\u7684\u6784\u5efa\u65b9\u6cd5\u6765\u63d0\u5347LLMs\u8868\u73b0\u3002", "method": "\u4ece\u539f\u751f\u5362\u68ee\u5821\u8bed\u6587\u672c\u8bed\u6599\u4e2d\u5408\u6210LuxIT\uff0c\u9009\u53d6DeepSeek-R1-0528\u4f5c\u4e3a\u751f\u6210\u6e90\uff1b\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5ba1\u5bf9\u751f\u6210\u5185\u5bb9\u8fdb\u884c\u8d28\u91cf\u7b5b\u9009\uff1b\u5728\u82e5\u5e72\u5c0f\u578bLLMs\u4e0a\u5bf9LuxIT\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u5362\u68ee\u5821\u8bed\u8bed\u8a00\u80fd\u529b\u8003\u8bd5\u65b9\u9762\u8fdb\u884c\u57fa\u7ebf\u548c\u5fae\u8c03\u540e\u7684\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u5362\u68ee\u5821\u8bed\u8003\u8bd5\u4e2d\u7684\u8868\u73b0\u5448\u73b0\u6df7\u5408\u6027\uff0c\u6a21\u578b\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u672a\u5448\u73b0\u7edf\u4e00\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LuxIT\u4e3a\u5362\u68ee\u5821\u8bedNLP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u952e\u3001\u53ef\u590d\u73b0\u7684\u5355\u8bed\u6570\u636e\u4e0e\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u4f46\u7814\u7a76\u7ed3\u679c\u63d0\u793a\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u6570\u636e\u751f\u6210\u3001\u8bc4\u5ba1\u4ee5\u53ca\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u7684\u9002\u914d\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.23818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23818", "abs": "https://arxiv.org/abs/2510.23818", "authors": ["Yilang Zhang", "Xiaodong Yang", "Yiwei Cai", "Georgios B. Giannakis"], "title": "ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning", "comment": null, "summary": "As large language models (LLMs) continue to scale in size, the computational\noverhead has become a major bottleneck for task-specific fine-tuning. While\nlow-rank adaptation (LoRA) effectively curtails this cost by confining the\nweight updates to a low-dimensional subspace, such a restriction can hinder\neffectiveness and slow convergence. This contribution deals with these\nlimitations by accumulating progressively a high-rank weight update from\nconsecutive low-rank increments. Specifically, the per update optimal low-rank\nmatrix is identified to minimize the loss function and closely approximate full\nfine-tuning. To endow efficient and seamless optimization without restarting,\nthis optimal choice is formed by appropriately scaling the columns of the\noriginal low-rank matrix. Rigorous performance guarantees reveal that the\noptimal scaling can be found analytically. Extensive numerical tests with\npopular LLMs scaling up to 12 billion parameters demonstrate a consistent\nperformance gain and fast convergence relative to state-of-the-art LoRA\nvariants on diverse tasks including natural language understanding, commonsense\nreasoning, and mathematical problem solving.", "AI": {"tldr": "\u4ee5\u9ad8\u79e9\u589e\u91cf\u9010\u6b65\u6269\u5c55 LoRA \u7684\u8868\u793a\u80fd\u529b\uff0c\u901a\u8fc7\u5bf9\u6bcf\u6b21\u66f4\u65b0\u7684\u4f4e\u79e9\u77e9\u9635\u8fdb\u884c\u6700\u4f18\u7f29\u653e\u5e76\u53e0\u52a0\uff0c\u5b9e\u73b0\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5fae\u8c03\u6548\u679c\u548c\u6536\u655b\u901f\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u6a21\u578b\u5fae\u8c03\u4e2d\u57fa\u4e8e\u4f4e\u79e9\u8fd1\u4f3c\u7684\u6210\u672c\u4e0e\u6548\u679c\u4e4b\u95f4\u7684\u6743\u8861\uff1b\u73b0\u6709 LoRA \u53d7\u9650\u4e8e\u4f4e\u79e9\u4ec5\u80fd\u63d0\u4f9b\u6709\u9650\u8868\u793a\u80fd\u529b\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u6536\u655b\u53d8\u6162\u3002", "method": "\u5728\u6bcf\u6b21\u66f4\u65b0\u65f6\uff0c\u8bc6\u522b\u51fa\u4f7f\u635f\u5931\u6700\u5c0f\u5316\u4e14\u5c3d\u53ef\u80fd\u63a5\u8fd1\u5168\u5fae\u8c03\u7684\u4f4e\u79e9\u77e9\u9635\uff1b\u901a\u8fc7\u5bf9\u539f\u59cb\u4f4e\u79e9\u77e9\u9635\u7684\u5217\u8fdb\u884c\u9002\u5f53\u7f29\u653e\u6765\u5f62\u6210\u6574\u4f53\u9ad8\u79e9\u66f4\u65b0\uff0c\u5e76\u7ed9\u51fa\u89e3\u6790\u7684\u6700\u4f18\u7f29\u653e\u7cfb\u6570\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\uff08NLU\u3001\u5e38\u8bc6\u63a8\u7406\u3001\u6570\u5b66\u9898\u89e3\u7b49\uff09\u548c\u53c2\u6570\u89c4\u6a21\u9ad8\u8fbe 120 \u4ebf\u53c2\u6570\u7684 LLM \u4e0a\uff0c\u663e\u793a\u76f8\u5bf9\u4e8e\u4e3b\u6d41 LoRA \u53d8\u4f53\u5177\u6709\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u548c\u66f4\u5feb\u7684\u6536\u655b\u3002", "conclusion": "\u5c06\u4f4e\u79e9\u589e\u91cf\u7cfb\u7edf\u5730\u653e\u5927\u5e76\u53e0\u52a0\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4f4e\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u9ad8\u7684\u5fae\u8c03\u5bb9\u91cf\u548c\u66f4\u4f18\u7684\u4f18\u5316\u6548\u7387\uff0c\u63d0\u4f9b\u5bf9 LoRA \u7684\u6709\u6548\u6539\u8fdb\u8def\u5f84\u3002"}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": " foundation models \u4e0e\u591a\u6a21\u6001\u5927\u6a21\u578b\u6b63\u5728\u63a8\u52a8\u4f5c\u7269\u75c5\u5bb3\u73b0\u573a\u7ba1\u7406\uff08SSDM\uff09\uff0cVLMs\u9886\u5148\u4e8eLLMs\uff1bRL/AL\u5728\u667a\u80fd\u55b7\u6d12\u4e2d\u4ecd\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\uff1b\u6570\u5b57\u5b6a\u751f\u53ef\u7528\u4e8e\u865a\u62df\u4eff\u771f\uff0c\u9700\u89e3\u51b3 sim-to-real\uff1b\u4eba\u673a\u534f\u4f5c\u6709\u9650\uff1b\u672a\u6765\u5c06\u4ee5\u591a\u6a21\u6001 FMs\u3001\u5b9e\u65f6\u53cd\u9988\u4e3a\u4e3b\u5bfc\u3002", "motivation": "\u63d0\u9ad8SSDMs\u7684\u51c6\u786e\u6027\u3001\u9002\u5e94\u6027\u548c\u5b9e\u5730\u53ef\u64cd\u4f5c\u6027\uff0c\u501f\u52a9FMs\u5c06\u89c6\u89c9\u3001\u6587\u672c\u548c\u4ea4\u4e92\u7ed3\u5408\uff0c\u4fc3\u8fdb\u65e9\u671f\u75c7\u72b6\u68c0\u6d4b\u3001\u75c7\u72b6\u89e3\u91ca\u548c\u6cbb\u7597\u51b3\u7b56\u3002", "method": "\u5bf9\u7ea640\u7bc7\u5173\u4e8eFM\u5728SSDM\u4e2d\u7684\u5e94\u7528\u7684\u6587\u732e\u8fdb\u884c\u7efc\u8ff0\uff0c\u91cd\u70b9\u5206\u6790LLMs\u4e0eVLMs\u3001AL\u3001RL\u548c\u6570\u5b57\u5b6a\u751f\u5728\u5b9a\u70b9\u55b7\u6d12\u4e2d\u7684\u5e94\u7528\u4e0e\u6311\u6218\uff0c\u5e76\u7ed9\u51fa\u672a\u6765\u8d8b\u52bf\u4e0e\u8d44\u6e90\u3002", "result": "\u5173\u952e\u53d1\u73b0\u5305\u62ec\uff1aa) 2023\u201324\u6587\u732e\u8fc5\u731b\u589e\u957f\uff1bb) VLMs \u591a\u4e8e LLMs\uff0c\u51fa\u7248\u91cf\u63d0\u53475\u201310\u500d\uff1bc) RL\u4e0eAL\u5728\u667a\u80fd\u55b7\u6d12\u9886\u57df\u5c1a\u5904\u4e8e\u521d\u59cb\u9636\u6bb5\uff1bd) \u6570\u5b57\u5b6a\u751f\u7ed3\u5408RL\u53ef\u5b9e\u73b0\u865a\u62df\u5b9a\u70b9\u55b7\u6d12\u4eff\u771f\uff1be) \u4eff\u771f\u2013\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u662f\u73b0\u5b9e\u90e8\u7f72\u7684\u5173\u952e\u74f6\u9888\uff1bf) \u4eba\u673a\u534f\u4f5c\uff0c\u5c24\u5176\u662f\u4eba\u53c2\u4e0e\u7684\u73af\u8282\uff0c\u4ecd\u7136\u6709\u9650\uff1bg) \u591a\u6a21\u6001FM\u4e0e\u5b9e\u65f6\u53cd\u9988\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3SSDM\u3002", "conclusion": "\u5efa\u8bae\u7814\u7a76\u4e0e\u4ea7\u4e1a\u5171\u540c\u805a\u7126\uff1a\u6539\u8fdb\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3001\u52a0\u5f3a\u4eba\u673a\u534f\u4f5c\u3001\u53d1\u5c55\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e0e\u73b0\u573a\u53cd\u9988\u95ed\u73af\u3001\u5efa\u8bbe\u5f00\u653e\u8d44\u6e90\u5e93\u4ee5\u52a0\u901f\u6570\u636e\u3001\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u5171\u4eab\u3002"}}
{"id": "2510.23866", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23866", "abs": "https://arxiv.org/abs/2510.23866", "authors": ["Paul Rosu", "Muchang Bahng", "Erick Jiang", "Rico Zhu", "Vahid Tarokh"], "title": "A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling", "comment": null, "summary": "This work presents a physics-conditioned latent diffusion model tailored for\ndynamical downscaling of atmospheric data, with a focus on reconstructing\nhigh-resolution 2-m temperature fields. Building upon a pre-existing diffusion\narchitecture and employing a residual formulation against a reference UNet, we\nintegrate a partial differential equation (PDE) loss term into the model's\ntraining objective. The PDE loss is computed in the full resolution (pixel)\nspace by decoding the latent representation and is designed to enforce physical\nconsistency through a finite-difference approximation of an effective\nadvection-diffusion balance. Empirical observations indicate that conventional\ndiffusion training already yields low PDE residuals, and we investigate how\nfine-tuning with this additional loss further regularizes the model and\nenhances the physical plausibility of the generated fields. The entirety of our\ncodebase is available on Github, for future reference and development.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7269\u7406\u6761\u4ef6\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5927\u6c14\u6570\u636e\u7684\u52a8\u6001\u4e0b\u5c3a\u5ea6\u91cd\u5efa2\u7c73\u6e29\u5ea6\u573a\uff1b\u5728\u8bad\u7ec3\u4e2d\u52a0\u5165\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u635f\u5931\uff0c\u901a\u8fc7\u89e3\u7801\u6f5c\u5728\u8868\u793a\u5728\u50cf\u7d20\u7ea7\u5168\u5206\u8fa8\u7387\u8ba1\u7b97\uff0c\u5229\u7528\u6709\u9650\u5dee\u5206\u8fd1\u4f3c\u7684\u6709\u6548\u5bf9\u6d41\u6269\u6563\u5e73\u8861\u6765\u7ea6\u675f\u7269\u7406\u4e00\u81f4\u6027\uff1b\u7ed3\u679c\u663e\u793a\u5e38\u89c4\u6269\u6563\u8bad\u7ec3\u5df2\u4ea7\u751f\u4f4ePDE\u6b8b\u5dee\uff0c\u8fdb\u4e00\u6b65\u5fae\u8c03\u53ef\u63d0\u5347\u7269\u7406\u5408\u7406\u6027\uff1b\u4ee3\u7801\u6258\u7ba1\u5728GitHub\u3002", "motivation": "\u5728\u4e0d\u727a\u7272\u6570\u636e\u9a71\u52a8\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u83b7\u5f97\u7269\u7406\u4e0a\u53ef\u89e3\u91ca\u4e14\u7b26\u5408\u5927\u6c14\u65b9\u7a0b\u7684\u9ad8\u5206\u8fa8\u7387\u6e29\u5ea6\u573a\u3002\u4ec5\u4f7f\u7528\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u6269\u6563\u6a21\u578b\u5f80\u5f80\u7f3a\u4e4f\u7269\u7406\u7ea6\u675f\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6216\u4e0d\u7b26\u5408\u57fa\u672c\u5b88\u6052\u3002\u901a\u8fc7\u5f15\u5165PDE\u5c42\u9762\u7684\u635f\u5931\uff0c\u660e\u786e\u5c06\u7269\u7406\u65b9\u7a0b\u5d4c\u5165\u6a21\u578b\u8bad\u7ec3\u3002", "method": "\u5728\u73b0\u6709\u6269\u6563\u67b6\u6784\u57fa\u7840\u4e0a\u91c7\u7528\u6b8b\u5dee\u5f62\u5f0f\u5bf9\u9f50\u53c2\u8003UNet\uff1b\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u52a0\u5165PDE\u635f\u5931\uff1bPDE\u5728\u5168\u5206\u8fa8\u7387\u50cf\u7d20\u7a7a\u95f4\u901a\u8fc7\u89e3\u7801\u6f5c\u5728\u8868\u793a\u83b7\u5f97\uff0c\u5e76\u91c7\u7528\u6709\u9650\u5dee\u5206\u8fd1\u4f3c\u7684\u6709\u6548\u5bf9\u6d41\u6269\u6563\u5e73\u8861\u6765\u8ba1\u7b97\u6b8b\u5dee\uff1b\u5bf9\u6bd4\u7eaf\u6269\u6563\u8bad\u7ec3\u4e0e\u5fae\u8c03\u540e\u7684\u6548\u679c\uff1b\u4ee3\u7801\u5f00\u653e\u5728GitHub\u3002", "result": "\u7ecf\u9a8c\u89c2\u5bdf\u8868\u660e\uff0c\u5e38\u89c4\u6269\u6563\u8bad\u7ec3\u5df2\u7ecf\u80fd\u5b9e\u73b0\u8f83\u4f4e\u7684PDE\u6b8b\u5dee\uff1b\u901a\u8fc7\u52a0\u5165PDE\u635f\u5931\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6b63\u5219\u5316\u6a21\u578b\uff0c\u63d0\u5347\u751f\u6210\u573a\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "conclusion": "\u5c06\u7269\u7406\u4fe1\u606f\u4ee5PDE\u635f\u5931\u5f62\u5f0f\u6ce8\u5165\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u5927\u6c14\u4e0b\u5c3a\u5ea6\u4e2d\u7684\u5e94\u7528\u662f\u53ef\u884c\u7684\uff0c\u4e14\u6709\u52a9\u4e8e\u63d0\u5347\u751f\u6210\u573a\u7684\u7269\u7406\u4e00\u81f4\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u4ee3\u7801\u5f00\u6e90\u4fbf\u4e8e\u590d\u73b0\u53ca\u540e\u7eed\u6539\u8fdb\u3002"}}
{"id": "2510.24446", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24446", "abs": "https://arxiv.org/abs/2510.24446", "authors": ["Viktoriia Zinkovich", "Anton Antonov", "Andrei Spiridonov", "Denis Shepelev", "Andrey Moskalenko", "Daria Pugacheva", "Elena Tutubalina", "Andrey Kuznetsov", "Vlad Shakhuro"], "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.", "AI": {"tldr": "\u63d0\u51fa SPARTA\uff0c\u4e00\u79cd\u9ed1\u7bb1\u3001\u53e5\u5b50\u7ea7\u7684\u5bf9\u6297\u6027\u6587\u672c\u6539\u5199\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u4e0d\u53d8\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u591a\u6a21\u6001\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u591a\u6a21\u6001\u63a8\u7406\u5206\u5272\u6a21\u578b\u5bf9\u8bed\u4e49\u7b49\u4ef7\u6539\u5199\u4ecd\u7136\u8106\u5f31\u3002", "motivation": "\u5f53\u524d\u5de5\u4f5c\u4e3b\u8981\u7814\u7a76\u5bf9\u56fe\u50cf\u6270\u52a8\u7684\u5bf9\u6297\u6027\uff0c\u800c\u5bf9\u7b49\u4e49\u7684\u6587\u672c\u6539\u5199\u5bf9\u6a21\u578b\u5f71\u54cd\u672a\u88ab\u5145\u5206\u7814\u7a76\uff1b\u73b0\u5b9e\u4e2d\u7528\u6237\u8868\u8fbe\u610f\u56fe\u7684\u65b9\u5f0f\u591a\u6837\uff0c\u9700\u8bc4\u4f30\u6a21\u578b\u5bf9\u8bed\u4e49\u7b49\u4ef7\u6539\u5199\u7684\u9c81\u68d2\u6027\u3002", "method": "SPARTA \u65b9\u6cd5\uff1a\u5728\u6587\u672c\u81ea\u7f16\u7801\u5668\u7684\u4f4e\u7ef4\u8bed\u4e49\u6f5c\u7a7a\u95f4\u8fdb\u884c\u53e5\u5b50\u7ea7\u4f18\u5316\uff0c\u5c5e\u4e8e\u9ed1\u7bb1\u653b\u51fb\uff1b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u641c\u7d22\uff0c\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u4e14\u8bed\u4e49\u7b49\u4ef7\u7684\u6539\u5199\uff1b\u5efa\u7acb\u81ea\u52a8\u8bc4\u4f30\u534f\u8bae\u5e76\u7528\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u3002", "result": "SPARTA \u5728 ReasonSeg \u548c LLMSeg-40k \u6570\u636e\u96c6\u4e0a\uff0c\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa\u591a\u8fbe 2 \u500d\uff1b\u8868\u660e\u5148\u8fdb\u7684\u63a8\u7406\u5206\u5272\u6a21\u578b\u5728\u4e25\u683c\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u7ea6\u675f\u4e0b\u4ecd\u7136\u6613\u53d7\u5bf9\u6297\u6027\u6539\u5199\u653b\u51fb\u3002", "conclusion": "\u5bf9\u6297\u6027\u6587\u672c\u6539\u5199\u63ed\u793a\u4e86\u591a\u6a21\u6001\u63a8\u7406\u5206\u5272\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9700\u52a0\u5f3a\u5bf9\u8bed\u4e49\u7b49\u4ef7\u6539\u5199\u7684\u9c81\u68d2\u6027\u7814\u7a76\uff1b\u6587\u4e2d\u63d0\u4f9b\u4ee3\u7801\u4e0e\u6570\u636e\u4ee5\u4fc3\u8fdb\u540e\u7eed\u5de5\u4f5c\u3002"}}
{"id": "2510.23868", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23868", "abs": "https://arxiv.org/abs/2510.23868", "authors": ["Zhichao Wang"], "title": "GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA", "comment": null, "summary": "I propose \\textbf{G}roup-relative \\textbf{I}mplicit \\textbf{F}ine\n\\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning\nLLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT\nminimizes the discrepancy between implicit and explicit reward models. It\ncombines three key ideas: (1) the online multi-response generation and\nnormalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the\nimplicit-explicit reward alignment principle of UNA. By jointly normalizing the\nimplicit and explicit rewards, GIFT eliminates an otherwise intractable term\nthat prevents effective use of implicit rewards. This normalization transforms\nthe complex reward maximization objective into a simple mean squared error\n(MSE) loss between the normalized reward functions, converting a non-convex\noptimization problem into a convex, stable, and analytically differentiable\nformulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy\nand thus retains exploration capability. Compared to GRPO, it requires fewer\nhyperparameters, converges faster, and generalizes better with significantly\nreduced training overfitting. Empirically, GIFT achieves superior reasoning and\nalignment performance on mathematical benchmarks while remaining\ncomputationally efficient.", "AI": {"tldr": "GIFT \u662f\u4e00\u79cd\u65b0\u7684\u6309\u9700\u7b56\u7565\u68af\u5ea6\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u663e\u5f0f\u4e0e\u9690\u5f0f\u5956\u52b1\uff0c\u5c06\u4f18\u5316\u76ee\u6807\u4ece\u975e\u51f8\u8f6c\u4e3a\u53ef\u5fae\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u4ece\u800c\u9ad8\u6548\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u9690\u5f0f\u5956\u52b1\u5bf9\u9f50\u4e2d\u7684\u96be\u9898\u4e0e\u79bb\u7ebf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u5728\u4fdd\u6301\u63a2\u7d22\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6536\u655b\u901f\u5ea6\u3001\u8d85\u53c2\u6570\u53cb\u597d\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06 GRPO \u7684\u5728\u7ebf\u591a\u54cd\u5e94\u751f\u6210\u4e0e\u5f52\u4e00\u5316\u3001DPO \u7684\u9690\u5f0f\u5956\u52b1\u4ee5\u53ca UNA \u7684\u9690\u5f0f-\u663e\u5f0f\u5bf9\u9f50\u539f\u5219\u7ed3\u5408\uff1b\u5bf9\u9690\u5f0f\u4e0e\u663e\u5f0f\u5956\u52b1\u8fdb\u884c\u8054\u5408\u5f52\u4e00\u5316\uff0c\u6d88\u9664\u96be\u4ee5\u5904\u7406\u7684\u9879\uff1b\u5c06\u4f18\u5316\u76ee\u6807\u8f6c\u5316\u4e3a\u4e24\u8005\u5f52\u4e00\u5316\u5956\u52b1\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\uff1b\u4fdd\u6301 on-policy \u4ee5\u7ef4\u6301\u63a2\u7d22\u6027\uff0c\u76f8\u8f83 GRPO \u5177\u6709\u66f4\u5c11\u8d85\u53c2\u6570\u3001\u66f4\u5feb\u6536\u655b\u3001\u66f4\u597d\u6cdb\u5316\uff0c\u8bad\u7ec3\u8fc7\u62df\u5408\u663e\u8457\u964d\u4f4e\u3002", "result": "\u5728\u6570\u5b66\u57fa\u51c6\u4e0a\uff0cGIFT \u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u63a8\u7406\u548c\u5bf9\u9f50\u6027\u80fd\uff0c\u540c\u65f6\u5177\u5907\u8f83\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "GIFT \u63d0\u4f9b\u4e00\u4e2a\u7a33\u5b9a\u3001\u9ad8\u6548\u4e14\u53ef\u5fae\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u5f52\u4e00\u5316\u89e3\u51b3\u9690\u5f0f\u5956\u52b1\u7684\u96be\u9898\uff0c\u517c\u5177\u63a2\u7d22\u6027\u4e0e\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24450", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24450", "abs": "https://arxiv.org/abs/2510.24450", "authors": ["\u0160pela Vintar", "Taja Kuzman Punger\u0161ek", "Mojca Brglez", "Nikola Ljube\u0161i\u0107"], "title": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a Set of Best Practices", "comment": "12 pages, 1 figure. Submitted to the LREC 2026 conference", "summary": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.", "AI": {"tldr": "\u5bf9\u591a\u8bed\u8a00/\u975e\u82f1\u8bedLLM\u57fa\u51c6\u7684\u6846\u67b6\u6027\u5206\u6790\uff1a\u63d0\u51fa\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u573a\u666f\u7684\u57fa\u51c6\u5206\u7c7b\u6cd5\uff0c\u4ee5\u53ca\u9762\u5411\u6b27\u6d32\u8bed\u8a00\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u8d28\u91cf\u6807\u51c6\uff0c\u4ee5\u63d0\u5347\u8bc4\u4f30\u7684\u8bed\u8a00\u4e0e\u6587\u5316\u654f\u611f\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u975e\u82f1\u8bed\u8bed\u8a00\u7684LLM\u8bc4\u4f30\u548c\u57fa\u51c6\u5efa\u7acb\u4ecd\u76f8\u5bf9\u6b20\u7f3a\uff0c\u4e9f\u9700\u5728\u6b27\u6d32\u8bed\u8a00\u5c42\u9762\u5b9e\u73b0\u66f4\u534f\u8c03\u7684\u57fa\u51c6\u4f53\u7cfb\uff0c\u5e76\u63d0\u5347\u8bc4\u4f30\u65b9\u6cd5\u7684\u8bed\u8a00\u4e0e\u6587\u5316\u654f\u611f\u6027\u3002", "method": "\u5bf9\u8fd1\u671fLLM\u57fa\u51c6\u53d1\u5c55\u7684\u73b0\u72b6\u8fdb\u884c\u7b80\u8981\u56de\u987e\uff0c\u63d0\u51fa\u9762\u5411\u591a\u8bed\u8a00\u6216\u975e\u82f1\u8bed\u7528\u4f8b\u7684\u4e13\u95e8\u5206\u7c7b\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e00\u7ec4\u6700\u4f73\u5b9e\u8df5\u4e0e\u8d28\u91cf\u6807\u51c6\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7684\u6b27\u6d32\u8bed\u8a00\u57fa\u51c6\u5de5\u4f5c\u3002", "result": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u573a\u666f\u7684\u57fa\u51c6\u5206\u7c7b\u6cd5\u548c\u4e00\u5957\u57fa\u4e8e\u8d28\u91cf\u7684\u6700\u4f73\u5b9e\u8df5\u6846\u67b6\uff1b\u5728\u6458\u8981\u4e2d\u672a\u7ed9\u51fa\u5b9e\u8bc1\u7ed3\u679c\uff0c\u76ee\u6807\u5728\u4e8e\u4e3a\u540e\u7eed\u57fa\u51c6\u5efa\u8bbe\u63d0\u4f9b\u65b9\u5411\u548c\u89c4\u8303\u3002", "conclusion": "\u5f3a\u8c03\u5728\u8bc4\u4f30\u65b9\u6cd5\u4e2d\u63d0\u9ad8\u8bed\u8a00\u548c\u6587\u5316\u654f\u611f\u6027\uff0c\u5e76\u63a8\u52a8\u6b27\u6d32\u8bed\u8a00\u7684\u57fa\u51c6\u5f00\u53d1\u66f4\u52a0\u534f\u8c03\u3002"}}
{"id": "2510.24690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24690", "abs": "https://arxiv.org/abs/2510.24690", "authors": ["Shengjie Liu", "Li Dong", "Zhenyu Zhang"], "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning", "comment": "4 pages, 2 figures, short paper, NeurIPS 2025 workshop on Bridging\n  Language, Agent, and World Models for Reasoning and Planning", "summary": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.", "AI": {"tldr": "\u6784\u5efa\u5e76\u878d\u5408\u96c6\u6210\u4e24\u7c7b\u77e5\u8bc6\u56fe\u8c31\u4ee5\u63d0\u5347\u57fa\u4e8e\u5de5\u5177\u7684\u8ba1\u5212\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u5de5\u5177\u4f9d\u8d56\u4e0e\u7a0b\u5e8f\u6027\u77e5\u8bc6\u4e4b\u95f4\u7f3a\u4e4f\u7edf\u4e00\u5efa\u6a21\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u793a\u4f8b\u4ea7\u7269\uff08\u8ba1\u5212\uff09\u751f\u6210\u8d28\u91cf\u3002", "method": "1) \u4ece\u5de5\u5177\u6a21\u5f0f\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\uff0c\u5305\u542b\u63cf\u8ff0\u3001\u53c2\u6570\u4e0e\u8f93\u51fa\u8d1f\u8f7d\uff0c\u5e76\u8fdb\u884c\u6df1\u5ea6\u7814\u7a76\u98ce\u683c\u5206\u6790\uff1b2) \u4ece\u5185\u90e8\u6587\u6863\u548cSOP\u6d3e\u751f\u4e92\u8865\u77e5\u8bc6\u56fe\u8c31\uff1b3) \u5c06\u4e24\u8005\u878d\u5408\uff1b4) \u91c7\u7528\u6df1\u5ea6-\u7a00\u758f\u96c6\u6210\u7b56\u7565\uff0c\u5c06\u7ed3\u6784\u5316\u7684\u5de5\u5177\u4f9d\u8d56\u4e0e\u7a0b\u5e8f\u6027\u77e5\u8bc6\u5bf9\u9f50\u4ee5\u751f\u6210\u793a\u4f8b\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u7edf\u4e00\u6846\u67b6\u80fd\u6709\u6548\u5efa\u6a21\u5de5\u5177\u4ea4\u4e92\uff0c\u63d0\u5347\u8ba1\u5212\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u5c06\u5de5\u5177\u56fe\u8c31\u4e0e\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u76f8\u7ed3\u5408\uff0c\u53ef\u589e\u5f3a\u5de5\u5177\u9a71\u52a8\u7684\u63a8\u7406\u4e0e\u8ba1\u5212\u80fd\u529b\u3002"}}
{"id": "2411.09539", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2411.09539", "abs": "https://arxiv.org/abs/2411.09539", "authors": ["Marton Szep", "Daniel Rueckert", "R\u00fcdiger von Eisenhart-Rothe", "Florian Hinterwimmer"], "title": "Fine-tuning Large Language Models with Limited Data: A Survey and Practical Guide", "comment": "Accepted to TACL. Pre-MIT Press version. Major restructuring; added\n  preference alignment section and additional tables. 36 pages", "summary": "Fine-tuning large language models (LLMs) with limited data poses a practical\nchallenge in low-resource languages, specialized domains, and constrained\ndeployment settings. While pre-trained LLMs provide strong foundations,\neffective adaptation under data scarcity requires focused and efficient\nfine-tuning techniques. This paper presents a structured and practical survey\nof recent methods for fine-tuning LLMs in data-scarce scenarios. We\nsystematically review parameter-efficient fine-tuning techniques that lower\ntraining and deployment costs, domain and cross-lingual adaptation methods for\nboth encoder and decoder models, and model specialization strategies. We\nfurther examine preference alignment approaches that guide model behavior using\nlimited human or synthetic feedback, emphasizing sample and compute efficiency.\nThroughout, we highlight empirical trade-offs, selection criteria, and best\npractices for choosing suitable techniques based on task constraints, including\nmodel scaling, data scaling, and the mitigation of catastrophic forgetting. The\naim is to equip researchers and practitioners with actionable insights for\neffectively fine-tuning LLMs when data and resources are limited.", "AI": {"tldr": "\u5bf9\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u7efc\u5408\u6027\u7efc\u8ff0\uff0c\u805a\u7126\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001\u9886\u57df\u4e0e\u8de8\u8bed\u8a00\u9002\u914d\u3001\u6a21\u578b\u5b9a\u5236\uff0c\u4ee5\u53ca\u6709\u9650\u53cd\u9988\u4e0b\u7684\u504f\u597d\u5bf9\u9f50\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6743\u8861\u4e0e\u5b9e\u8df5\u6307\u5357\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u3001\u4e13\u4e1a\u9886\u57df\u548c\u53d7\u9650\u90e8\u7f72\u6761\u4ef6\u4e0b\uff0c\u5982\u4f55\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5730\u9002\u914d\u548c\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u964d\u4f4e\u6210\u672c\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u6027\u56de\u987e\u5e76\u5206\u7c7b\u8fd1\u5e74\u65b9\u6cd5\uff0c\u8986\u76d6\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001\u9886\u57df/\u8de8\u8bed\u8a00\u9002\u914d\uff08\u7f16\u7801\u5668\u4e0e\u89e3\u7801\u5668\u6a21\u578b\uff09\u3001\u6a21\u578b\u4e13\u4e1a\u5316\u7b56\u7565\u3001\u4ee5\u53ca\u5728\u6709\u9650\u4eba\u7c7b\u6216\u5408\u6210\u53cd\u9988\u4e0b\u7684\u504f\u597d\u5bf9\u9f50\uff1b\u8ba8\u8bba\u7ecf\u9a8c\u6027\u6743\u8861\u3001\u9009\u62e9\u6807\u51c6\u4e0e\u6700\u4f73\u5b9e\u8df5\u3002", "result": "\u63d0\u51fa\u9762\u5411\u7814\u7a76\u8005\u548c\u5b9e\u52a1\u8005\u7684\u53ef\u64cd\u4f5c\u6d1e\u89c1\u4e0e\u6846\u67b6\uff0c\u5e2e\u52a9\u5728\u6570\u636e\u4e0e\u8d44\u6e90\u53d7\u9650\u65f6\u9009\u62e9\u5408\u9002\u7684\u5fae\u8c03\u6280\u672f\uff0c\u5e76\u7ed9\u51fa\u5728\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u89c4\u6a21\u548c\u707e\u96be\u6027\u9057\u5fd8\u7b49\u65b9\u9762\u7684\u98ce\u9669\u4e0e\u5bf9\u7b56\u3002", "conclusion": "\u5728\u6570\u636e\u548c\u8d44\u6e90\u6709\u9650\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u5408\u9002\u7684\u5fae\u8c03\u6280\u672f\u7ec4\u5408\u4e0e\u5b9e\u8df5\u6307\u5357\uff0c\u53ef\u4ee5\u9ad8\u6548\u5730\u5fae\u8c03LLMs\uff0c\u4f7f\u5176\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u5177\u5907\u826f\u597d\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.23901", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23901", "abs": "https://arxiv.org/abs/2510.23901", "authors": ["Cristobal Heredia", "Pedro Chumpitaz-Flores", "Kaixun Hua"], "title": "RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal Regression Trees", "comment": "20 pages, 1 figure, uses ICLR 2026 LaTeX style. Submitted to arXiv as\n  a preprint version", "summary": "Mixed-integer programming (MIP) has emerged as a powerful framework for\nlearning optimal decision trees. Yet, existing MIP approaches for regression\ntasks are either limited to purely binary features or become computationally\nintractable when continuous, large-scale data are involved. Naively binarizing\ncontinuous features sacrifices global optimality and often yields needlessly\ndeep trees. We recast the optimal regression-tree training as a two-stage\noptimization problem and propose Reduced-Space Optimal Regression Trees\n(RS-ORT) - a specialized branch-and-bound (BB) algorithm that branches\nexclusively on tree-structural variables. This design guarantees the\nalgorithm's convergence and its independence from the number of training\nsamples. Leveraging the model's structure, we introduce several bound\ntightening techniques - closed-form leaf prediction, empirical threshold\ndiscretization, and exact depth-1 subtree parsing - that combine with\ndecomposable upper and lower bounding strategies to accelerate the training.\nThe BB node-wise decomposition enables trivial parallel execution, further\nalleviating the computational intractability even for million-size datasets.\nBased on the empirical studies on several regression benchmarks containing both\nbinary and continuous features, RS-ORT also delivers superior training and\ntesting performance than state-of-the-art methods. Notably, on datasets with up\nto 2,000,000 samples with continuous features, RS-ORT can obtain guaranteed\ntraining performance with a simpler tree structure and a better generalization\nability in four hours.", "AI": {"tldr": "\u63d0\u51fa Reduced-Space Optimal Regression Trees (RS-ORT)\uff0c\u901a\u8fc7\u57fa\u4e8e\u5206\u652f\u9650\u5b9a\u7684\u5206\u652f\u8fb9\u754c\u6cd5(BB)\uff0c\u5c06\u56de\u5f52\u6811\u7684\u6700\u4f18\u8bad\u7ec3\u8f6c\u5316\u4e3a\u4e24\u9636\u6bb5\u4f18\u5316\uff0c\u4e13\u95e8\u5728\u6811\u7ed3\u6784\u53d8\u91cf\u4e0a\u8fdb\u884c\u5206\u652f\uff0c\u5b9e\u73b0\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u7684\u53ef\u63a7\u6c42\u89e3\u548c\u5e76\u884c\u5316.", "motivation": "\u73b0\u6709\u7528\u4e8e\u56de\u5f52\u7684\u6df7\u5408\u6574\u6570\u89c4\u5212(MIP)\u65b9\u6cd5\u8981\u4e48\u53ea\u80fd\u5904\u7406\u4e8c\u5143\u7279\u5f81\uff0c\u8981\u4e48\u5728\u8fde\u7eed\u5927\u89c4\u6a21\u6570\u636e\u4e0b\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002\u5bf9\u8fde\u7eed\u7279\u5f81\u8fdb\u884c\u6734\u7d20\u4e8c\u503c\u5316\u4f1a\u727a\u7272\u5168\u5c40\u6700\u4f18\u6027\u5e76\u5bfc\u81f4\u8fc7\u6df1\u7684\u6811\u3002\u9700\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5177\u5168\u5c40\u6700\u4f18\u6027\u7684\u56de\u5f52\u6811\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5c06\u6700\u4f18\u56de\u5f52\u6811\u8bad\u7ec3\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e24\u9636\u6bb5\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa Reduced-Space Optimal Regression Trees (RS-ORT)\uff0c\u662f\u4e00\u79cd\u4e13\u95e8\u7684\u5206\u652f\u9650\u754c\u7b97\u6cd5\uff0c\u4ec5\u5bf9\u6811\u7ed3\u6784\u53d8\u91cf\u8fdb\u884c\u5206\u652f\uff1b\u7ed3\u5408\u95ed\u5f0f\u53f6\u5b50\u9884\u6d4b\u3001\u7ecf\u9a8c\u9608\u503c\u79bb\u6563\u5316\u3001\u7cbe\u786e\u7684\u6df1\u5ea6-1 \u5b50\u6811\u89e3\u6790\u7b49 bound-tightening \u6280\u672f\u548c\u53ef\u5206\u89e3\u7684\u4e0a/\u4e0b\u754c\u6765\u52a0\u901f\u6c42\u89e3\uff1b\u8282\u70b9\u7ea7\u522b\u7684 BB \u5206\u89e3\u652f\u6301\u5929\u751f\u5e76\u884c\uff0c\u4e14\u4e0e\u6837\u672c\u6570\u91cf\u65e0\u5173\u7684\u6536\u655b\u6027\u3002", "result": "\u5728\u5305\u542b\u4e8c\u5143\u4e0e\u8fde\u7eed\u7279\u5f81\u7684\u591a\u9879\u56de\u5f52\u57fa\u51c6\u4e0a\uff0cRS-ORT \u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u6811\u65b9\u6cd5\uff0c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6027\u80fd\u66f4\u4f18\uff1b\u5728\u542b\u5927\u89c4\u6a21\u8fde\u7eed\u7279\u5f81\u7684\u6570\u636e\u96c6\uff08\u591a\u8fbe 2,000,000 \u4e2a\u6837\u672c\uff09\u4e0a\uff0c\u56db\u5c0f\u65f6\u5185\u53ef\u83b7\u5f97\u6709\u4fdd\u8bc1\u7684\u8bad\u7ec3\u6027\u80fd\u3001\u4e14\u6811\u7ed3\u6784\u66f4\u7b80\u5355\u3001\u6cdb\u5316\u80fd\u529b\u66f4\u597d\u3002", "conclusion": "RS-ORT \u63d0\u4f9b\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5177\u5907\u5168\u5c40\u6700\u4f18\u6027\u4fdd\u969c\u7684\u56de\u5f52\u6811\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u51cf\u5c11\u7a7a\u95f4\u7684 BB \u7ed3\u6784\u548c\u591a\u79cd bound-tightening \u6280\u5de7\u5b9e\u73b0\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u7684\u9ad8\u6548\u6c42\u89e3\u4e0e\u5e76\u884c\u5316\uff0c\u5177\u5907\u826f\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u6cdb\u5316\u6027\u3002"}}
{"id": "2510.24476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24476", "abs": "https://arxiv.org/abs/2510.24476", "authors": ["Yihan Li", "Xiyuan Fu", "Ghanshyam Verma", "Paul Buitelaar", "Mingming Liu"], "title": "Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems", "comment": "25 pages, 7 figures, 3 tables", "summary": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u63d0\u51fa\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u63a8\u7406\u589e\u5f3a\u7ed3\u5408\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5206\u6790\u5b83\u4eec\u5bf9\u5e7b\u89c9\u7684\u7f13\u89e3\u4f5c\u7528\uff0c\u63d0\u51fa\u77e5\u8bc6\u578b\u4e0e\u903b\u8f91\u578b\u5e7b\u89c9\u7684\u5206\u95e8\u522b\u7c7b\uff0c\u5e76\u7ed9\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u548c\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u8bc4\u4f30\u4e0e\u57fa\u51c6\u3002", "motivation": "LLMs\u7684\u5e7b\u89c9\u662f\u53ef\u9760\u90e8\u7f72\u7684\u5173\u952e\u969c\u788d\uff1b\u73b0\u6709\u7684\u7f13\u89e3\u7b56\u7565\uff08RAG\u3001\u63a8\u7406\u589e\u5f3a\uff09\u591a\u4ee5\u5355\u72ec\u7814\u7a76\uff0c\u7f3a\u4e4f\u5bf9\u4e8c\u8005\u534f\u540c\u6548\u5e94\u53ca\u673a\u5236\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u5e94\u7528\u5bfc\u5411\u7684\u80fd\u529b\u63d0\u5347\u89c6\u89d2\uff0c\u6784\u5efa\u5206\u7c7b\u578b\u7684\u5e7b\u89c9\u5206\u7c7b\u3001\u7cfb\u7edf\u6027\u5206\u6790RAG\u4e0e\u63a8\u7406\u5728\u5404\u7c7b\u578b\u5e7b\u89c9\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u4e24\u8005\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8f85\u4ee5\u771f\u5b9e\u5e94\u7528\u3001\u8bc4\u4f30\u4e0e\u57fa\u51c6\u7684\u652f\u6491\u3002", "result": "\u63d0\u51fa\u533a\u5206\u77e5\u8bc6\u578b\u5e7b\u89c9\u4e0e\u903b\u8f91\u578b\u5e7b\u89c9\u7684 taxonomy\uff1b\u7cfb\u7edf\u5206\u6790RAG\u4e0e\u63a8\u7406\u5982\u4f55\u5206\u522b\u6216\u534f\u540c\u7f13\u89e3\u5404\u7c7b\u5e7b\u89c9\uff1b\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u3001\u8bc4\u4f30\u548c\u57fa\u51c6\u3002", "conclusion": "RAG\u4e0e\u63a8\u7406\u589e\u5f3a\u5728Agentic Systems\u4e2d\u5177\u5907\u4e92\u8865\u4f5c\u7528\uff0c\u80fd\u591f\u5728\u521b\u9020\u529b\u4e0e\u53ef\u9760\u6027\u4e4b\u95f4\u5b9e\u73b0\u5e73\u8861\uff1b\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u7406\u8bba\u4e0e\u5b9e\u8df5\u6307\u5f15\u3002"}}
{"id": "2505.22820", "categories": ["cs.LG", "cs.AI", "econ.TH", "stat.ML"], "pdf": "https://arxiv.org/pdf/2505.22820", "abs": "https://arxiv.org/abs/2505.22820", "authors": ["Ayush Sawarni", "Sahasrajit Sarmasarkar", "Vasilis Syrgkanis"], "title": "Preference Learning with Response Time: Robust Losses and Guarantees", "comment": "Accepted at NeurIPS 2025", "summary": "This paper investigates the integration of response time data into human\npreference learning frameworks for more effective reward model elicitation.\nWhile binary preference data has become fundamental in fine-tuning foundation\nmodels, generative AI systems, and other large-scale models, the valuable\ntemporal information inherent in user decision-making remains largely\nunexploited. We propose novel methodologies to incorporate response time\ninformation alongside binary choice data, leveraging the Evidence Accumulation\nDrift Diffusion (EZ) model, under which response time is informative of the\npreference strength. We develop Neyman-orthogonal loss functions that achieve\noracle convergence rates for reward model learning, matching the theoretical\noptimal rates that would be attained if the expected response times for each\nquery were known a priori. Our theoretical analysis demonstrates that for\nlinear reward functions, conventional preference learning suffers from error\nrates that scale exponentially with reward magnitude. In contrast, our response\ntime-augmented approach reduces this to polynomial scaling, representing a\nsignificant improvement in sample efficiency. We extend these guarantees to\nnon-parametric reward function spaces, establishing convergence properties for\nmore complex, realistic reward models. Our extensive experiments validate our\ntheoretical findings in the context of preference learning over images.", "AI": {"tldr": "\u5c06\u54cd\u5e94\u65f6\u95f4\u4fe1\u606f\u4e0e\u4e8c\u5143\u504f\u597d\u7ed3\u5408\uff0c\u5229\u7528EZ\u6a21\u578b\u548cNeyman-orthogonal\u635f\u5931\u5b9e\u73b0\u5956\u8d4f\u6a21\u578b\u7684\u9ad8\u6548\u5b66\u4e60\uff0c\u7406\u8bba\u4e0e\u5b9e\u8bc1\u5747\u8868\u660e\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\u3002", "motivation": "\u4e8c\u5143\u504f\u597d\u6570\u636e\u5ffd\u7565\u4e86\u7528\u6237\u51b3\u7b56\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u54cd\u5e94\u65f6\u95f4\u5305\u542b\u5bf9\u504f\u597d\u5f3a\u5ea6\u7684\u5173\u952e\u4fe1\u53f7\uff1b\u5c06\u5176\u6574\u5408\u6709\u671b\u63d0\u5347\u5927\u89c4\u6a21\u6a21\u578b\u7684\u5956\u8d4f\u6a21\u578b\u5b66\u4e60\u6548\u7387\u3002", "method": "\u4ee5EZ\uff08Evidence Accumulation Drift Diffusion\uff09\u6a21\u578b\u5c06\u54cd\u5e94\u65f6\u95f4\u4e0e\u504f\u597d\u5f3a\u5ea6\u5efa\u6a21\uff1b\u63d0\u51faNeyman-orthogonal\u635f\u5931\u4ee5\u5b9e\u73b0\u76f2\u70b9\u65e0\u504f\u4f30\u8ba1\uff0c\u83b7\u5f97\u63a5\u8fd1\u7406\u60f3\u7684\u6536\u655b\u901f\u7387\uff1b\u5bf9\u7ebf\u6027\u548c\u975e\u53c2\u6570\u5956\u8d4f\u51fd\u6570\u7a7a\u95f4\u7ed9\u51fa\u6536\u655b\u6027\u5206\u6790\uff1b\u5728\u56fe\u50cf\u504f\u597d\u573a\u666f\u4e2d\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u4ee5\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u7406\u8bba\u4e0a\uff0c\u7ebf\u6027\u5956\u8d4f\u51fd\u6570\u4e0b\u4f20\u7edf\u504f\u597d\u5b66\u4e60\u7684\u8bef\u5dee\u968f\u5956\u8d4f\u5e45\u5ea6\u5448\u6307\u6570\u7ea7\u589e\u957f\uff1b\u6240\u63d0\u65b9\u6cd5\u5c06\u5176\u964d\u81f3\u591a\u9879\u5f0f\u589e\u957f\uff0c\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\uff1b\u5bf9\u975e\u53c2\u6570\u7a7a\u95f4\u4e5f\u7ed9\u51fa\u6536\u655b\u6027\u8d28\uff1b\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u7406\u8bba\u7ed3\u8bba\uff0c\u8bc1\u5b9e\u54cd\u5e94\u65f6\u95f4\u4fe1\u606f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u54cd\u5e94\u65f6\u95f4\u7eb3\u5165\u504f\u597d\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347\u5956\u8d4f\u6a21\u578b\u7684\u6837\u672c\u6548\u7387\uff0c\u5bf9\u5927\u89c4\u6a21\u6a21\u578b\u7684\u504f\u597d\u5b66\u4e60\u5177\u6709\u6f5c\u5728\u5f71\u54cd\uff0c\u4e14\u9002\u7528\u4e8e\u66f4\u590d\u6742\u7684\u5956\u8d4f\u6a21\u578b\u3002"}}
{"id": "2510.23912", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23912", "abs": "https://arxiv.org/abs/2510.23912", "authors": ["Marko Karbevski", "Antonij Mijoski"], "title": "Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers", "comment": null, "summary": "The Query, Key, Value weight triplet is a building block of current attention\nmechanisms in state-of-the-art LLMs. We theoretically investigate whether this\ntriplet can be reduced, proving under simplifying assumptions that the Query\nweights are redundant, thereby reducing the number of non-embedding/lm-head\nparameters by over 8%. We validate the theory on full-complexity GPT-3 small\narchitectures (with layer normalization, skip connections, and weight decay)\ntrained from scratch, demonstrating that the reduced model achieves comparable\nvalidation loss to standard baselines. These findings motivate the\ninvestigation of the Query weight redundancy at scale.", "AI": {"tldr": "\u7406\u8bba\u4e0e\u5b9e\u8bc1\u8868\u660eQuery\u6743\u91cd\u53ef\u5197\u4f59\uff0c\u51cf\u5c11>8%\u7684\u975e\u5d4c\u5165/LM\u5934\u53c2\u6570\uff1b\u5728GPT-3\u5c0f\u578b\u67b6\u6784\u4e0a\u4ece\u96f6\u8bad\u7ec3\u540e\uff0c\u6027\u80fd\u4e0e\u57fa\u7ebf\u76f8\u8fd1\u3002", "motivation": "\u7406\u89e3\u6ce8\u610f\u529b\u4e2d\u7684Query\u3001Key\u3001Value\u4e09\u5143\u7ec4\u662f\u5426\u53ef\u7b80\u5316\uff0c\u7279\u522b\u662fQuery\u6743\u91cd\uff0c\u4ee5\u964d\u4f4e\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002", "method": "\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u5728\u7b80\u5316\u5047\u8bbe\u4e0b\u8bc1\u660eQuery\u6743\u91cd\u5197\u4f59\uff1b\u5728\u5305\u542b\u5c42\u5f52\u4e00\u5316\u3001\u8df3\u8dc3\u8fde\u63a5\u548c\u6743\u91cd\u8870\u51cf\u7684\u5b8c\u6574\u590d\u6742\u5ea6GPT-3\u5c0f\u578b\u67b6\u6784\u4e0a\u4ece\u5934\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u7ed3\u8bba\u3002", "result": "\u7406\u8bba\u8bc1\u660eQuery\u6743\u91cd\u5197\u4f59\u53ef\u5c06\u975e\u5d4c\u5165/LM-head\u53c2\u6570\u51cf\u5c11\u8d858%\uff1b\u5b9e\u9a8c\u8bc1\u660e\u51cf\u5c11Query\u6743\u91cd\u7684\u6a21\u578b\u5728\u9a8c\u8bc1\u635f\u5931\u4e0a\u4e0e\u6807\u51c6\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "\u7ed3\u679c\u652f\u6301\u5728\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u4e0a\u7ee7\u7eed\u7814\u7a76Query\u6743\u91cd\u7684\u5197\u4f59\u6027\uff0c\u663e\u793a\u6ce8\u610f\u529b\u6a21\u5757\u53ef\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u8bbe\u8ba1\uff0c\u4fc3\u4f7f\u8fdb\u4e00\u6b65\u7684\u89c4\u6a21\u5316\u5b9e\u9a8c\u3002"}}
{"id": "2510.24488", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24488", "abs": "https://arxiv.org/abs/2510.24488", "authors": ["Katherine Abramski", "Giulio Rossetti", "Massimo Stella"], "title": "A word association network methodology for evaluating implicit biases in LLMs compared to humans", "comment": "24 pages, 13 figures, 3 tables", "summary": "As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies.", "AI": {"tldr": "A novel word association network method to evaluate implicit biases in LLMs by simulating semantic priming in LLM-generated word association networks; enables cross-model and human comparisons across gender, religion, ethnicity, sexual orientation, and politics; provides a scalable framework with quantitative and qualitative bias assessments.", "motivation": "Implicit social biases in LLMs are often not explicit and hard to detect; there is a need for evaluation methods that probe implicit knowledge representations and align LLMs with human cognition.", "method": "Prompt-based approach that induces semantic priming in LLM-generated word association networks, constructs a network of word associations, and derives metrics to assess biases; enables direct comparisons between different LLMs and humans.", "result": "Applied to humans and several widely used LLMs, revealing convergences and divergences between LLM and human biases; highlights potential risks of using LLMs; demonstrates a scalable, generalizable evaluation framework.", "conclusion": "Provides a systematic, scalable framework for evaluating and comparing biases across multiple LLMs and humans, contributing to transparent and socially responsible language technologies."}}
{"id": "2510.23914", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23914", "abs": "https://arxiv.org/abs/2510.23914", "authors": ["Arsenii Mustafin", "Xinyi Sheng", "Dominik Baumann"], "title": "Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs", "comment": "12 pages, 1 figure", "summary": "The theoretical analysis of Markov Decision Processes (MDPs) is commonly\nsplit into two cases - the average-reward case and the discounted-reward case -\nwhich, while sharing similarities, are typically analyzed separately. In this\nwork, we extend a recently introduced geometric interpretation of MDPs for the\ndiscounted-reward case to the average-reward case, thereby unifying both. This\nallows us to extend a major result known for the discounted-reward case to the\naverage-reward case: under a unique and ergodic optimal policy, the Value\nIteration algorithm achieves a geometric convergence rate.", "AI": {"tldr": "\u672c\u6587\u5c06\u5e73\u5747\u5956\u52b1\u4e0e\u6298\u6263\u5956\u52b1\u4e24\u7c7b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u51e0\u4f55\u89e3\u91ca\u7edf\u4e00\u8d77\u6765\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5c06\u5bf9\u6298\u6263\u5956\u52b1\u7684\u5173\u952e\u7ed3\u679c\u63a8\u5e7f\u5230\u5e73\u5747\u5956\u52b1\u60c5\u5f62\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u552f\u4e00\u4e14\u904d\u5386\u6027\u6700\u4f18\u7b56\u7565\u5b58\u5728\u7684\u524d\u63d0\u4e0b\uff0c\u503c\u8fed\u4ee3\uff08Value Iteration\uff09\u7b97\u6cd5\u5177\u6709\u51e0\u4f55\u6536\u655b\u7387\u3002", "motivation": "\u7406\u8bba\u4e0a\u5bf9MDP\u7684\u5206\u6790\u901a\u5e38\u5c06\u5e73\u5747\u5956\u52b1\u4e0e\u6298\u6263\u5956\u52b1\u5206\u5f00\u5904\u7406\uff0c\u5bfc\u81f4\u7ed3\u8bba\u5206\u6563\u4e14\u5e94\u7528\u53d7\u9650\u3002\u82e5\u80fd\u5728\u7edf\u4e00\u7684\u51e0\u4f55\u6846\u67b6\u4e0b\u540c\u65f6\u8986\u76d6\u4e24\u7c7b\u95ee\u9898\uff0c\u5c06\u63d0\u5347\u5bf9\u6536\u655b\u6027\u3001\u7a33\u5b9a\u6027\u7b49\u6027\u8d28\u7684\u7406\u89e3\uff0c\u5e76\u53ef\u80fd\u7b80\u5316\u5206\u6790\u4e0e\u7b97\u6cd5\u8bbe\u8ba1\u3002", "method": "\u4f5c\u8005\u5c06\u6700\u8fd1\u5f15\u5165\u7684\u5bf9\u6298\u6263\u5956\u52b1\u60c5\u5f62\u7684\u51e0\u4f55\u89e3\u91ca\u6269\u5c55\u5230\u5e73\u5747\u5956\u52b1\u60c5\u5f62\uff0c\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u51e0\u4f55\u6846\u67b6\uff1b\u5728\u8be5\u6846\u67b6\u4e0b\u8bc1\u660e\u5728\u5b58\u5728\u552f\u4e00\u4e14\u904d\u5386\u6027\uff08ergodic\uff09\u7684\u6700\u4f18\u7b56\u7565\u65f6\uff0c\u503c\u8fed\u4ee3\u5177\u5907\u51e0\u4f55\u6536\u655b\u7387\u3002", "result": "\u5c06\u6298\u6263\u60c5\u5f62\u7684\u51e0\u4f55\u6536\u655b\u7ed3\u679c\u63a8\u5e7f\u5230\u5e73\u5747\u5956\u52b1\u60c5\u5f62\uff0c\u8bc1\u660e\u5728\u552f\u4e00\u4e14\u904d\u5386\u6027\u6700\u4f18\u7b56\u7565\u4e0b\uff0c\u503c\u8fed\u4ee3\u53ef\u5b9e\u73b0\u51e0\u4f55\u7ea7\u522b\u7684\u6536\u655b\u901f\u7387\u3002", "conclusion": "\u7ed9\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\uff0c\u8fde\u63a5\u4e86\u4e24\u7c7bMDP\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u63d0\u5347\u5bf9MDP\u6536\u655b\u6027\u5206\u6790\u7684\u5e7f\u5ea6\u4e0e\u6df1\u5ea6\uff0c\u5e76\u53ef\u80fd\u4e3a\u672a\u6765\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u7406\u8bba\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u5de5\u5177\u4e0e\u89c6\u89d2\u3002"}}
{"id": "2510.24505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24505", "abs": "https://arxiv.org/abs/2510.24505", "authors": ["Qing Zong", "Jiayu Liu", "Tianshi Zheng", "Chunyang Li", "Baixuan Xu", "Haochen Shi", "Weiqi Wang", "Zhaowei Wang", "Chunkit Chan", "Yangqiu Song"], "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?", "comment": null, "summary": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.", "AI": {"tldr": "Use natural language critiques to calibrate LLM confidence; introduce Self-Critique and CritiCal methods; CritiCal outperforms baselines and even GPT-4o on complex reasoning, with strong OOD generalization.", "motivation": "Gold-standard confidence labels are scarce and hard to obtain; traditional verbalization of confidence often fails to capture necessary reasoning for accurate calibration; natural language critiques offer a data-efficient way to improve verbalized confidence.", "method": "LMMs generate natural language critiques about their own confidence (Self-Critique) and a dedicated Critique Calibration training (CritiCal) that leverages these critiques to align confidence with reasoning, focusing on uncertainty for open-ended tasks and confidence for multiple-choice tasks; evaluate across tasks and compare to baselines including GPT-4o.", "result": "CritiCal significantly outperforms Self-Critique and other baselines, sometimes surpassing GPT-4o on complex tasks; shows robust generalization in out-of-distribution settings.", "conclusion": "Natural language critiques are effective for confidence calibration in LLMs; CritiCal advances reliability and generalization, offering a scalable approach to safer LLM deployment."}}
{"id": "2510.24530", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24530", "abs": "https://arxiv.org/abs/2510.24530", "authors": ["Eric G. C. Laporte"], "title": "Lev\u00e9e d'ambigu\u00eft\u00e9s par grammaires locales", "comment": "in French language", "summary": "Many words are ambiguous in terms of their part of speech (POS). However,\nwhen a word appears in a text, this ambiguity is generally much reduced.\nDisambiguating POS involves using context to reduce the number of POS\nassociated with words, and is one of the main challenges of lexical tagging.\nThe problem of labeling words by POS frequently arises in natural language\nprocessing, for example for spelling correction, grammar or style checking,\nexpression recognition, text-to-speech conversion, text corpus analysis, etc.\nLexical tagging systems are thus useful as an initial component of many natural\nlanguage processing systems. A number of recent lexical tagging systems produce\nmultiple solutions when the text is lexically ambiguous or the uniquely correct\nsolution cannot be found. These contributions aim to guarantee a zero silence\nrate: the correct tag(s) for a word must never be discarded. This objective is\nunrealistic for systems that tag each word uniquely. This article concerns a\nlexical disambiguation method adapted to the objective of a zero silence rate\nand implemented in Silberztein's INTEX system (1993). We present here a formal\ndescription of this method. We show that to verify a local disambiguation\ngrammar in this framework, it is not sufficient to consider the transducer\npaths separately: one needs to verify their interactions. Similarly, if a\ncombination of multiple transducers is used, the result cannot be predicted by\nconsidering them in isolation. Furthermore, when examining the initial labeling\nof a text as produced by INTEX, ideas for disambiguation rules come\nspontaneously, but grammatical intuitions may turn out to be inaccurate, often\ndue to an unforeseen construction or ambiguity. If a zero silence rate is\ntargeted, local grammars must be carefully tested. This is where a detailed\nspecification of what a grammar will do once applied to texts would be\nnecessary.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u96f6\u9759\u9ed8\u7387\u7684\u8bcd\u6027\u6d88\u6b67\u4e49\u65b9\u6cd5\uff0c\u5f3a\u8c03\u9700\u8003\u8651\u8de8\u8f6c\u5bfc\u7684\u76f8\u4e92\u4f5c\u7528\u4e0e\u7ec4\u5408\u7684\u6574\u4f53\u6548\u679c\uff0c\u800c\u975e\u9010\u6761\u8def\u5f84\u7684\u72ec\u7acb\u5206\u6790\u3002", "motivation": "\u8bcd\u6027\u6b67\u4e49\u5728\u6587\u672c\u4e2d\u901a\u5e38\u8f83\u5c11\uff0c\u4f46\u5728\u6807\u6ce8\u65f6\u9700\u8981\u901a\u8fc7\u4e0a\u4e0b\u6587\u964d\u4f4e\u6b67\u4e49\uff1b\u96f6\u9759\u9ed8\u7387\u76ee\u6807\u8981\u6c42\u4e0d\u80fd\u4e22\u5f03\u6b63\u786e\u6807\u7b7e\uff0c\u56e0\u6b64\u9700\u8981\u5f3a\u5065\u7684\u6d88\u6b67\u4e49\u7b56\u7565\u3002", "method": "\u5728 Silberztein \u7684 INTEX \u7cfb\u7edf\u4e2d\u5bf9\u8be5\u6d88\u6b67\u4e49\u65b9\u6cd5\u8fdb\u884c\u5f62\u5f0f\u5316\u63cf\u8ff0\uff0c\u6307\u51fa\u9a8c\u8bc1\u5c40\u90e8\u6d88\u6b67\u4e49\u8bed\u6cd5\u65f6\u5fc5\u987b\u8003\u8651\u8f6c\u5bfc\u8def\u5f84\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ee5\u53ca\u591a\u4e2a\u8f6c\u5bfc\u7ec4\u5408\u7684\u6574\u4f53\u9884\u6d4b\u4e0d\u80fd\u7b80\u5355\u5206\u89e3\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\u5355\u72ec\u8003\u5bdf\u8f6c\u5bfc\u8def\u5f84\u6216\u5206\u79bb\u7684\u8f6c\u5bfc\u7ec4\u5408\u4e0d\u80fd\u9884\u6d4b\u6574\u4f53\u884c\u4e3a\uff0c\u4e14\u5bf9\u521d\u59cb\u6807\u6ce8\u7684\u6d88\u6b67\u4e49\u89c4\u5219\u7684\u76f4\u89c9\u53ef\u80fd\u5728\u67d0\u4e9b\u6784\u9020\u6216\u6b67\u4e49\u4e0b\u9519\u8bef\uff0c\u9700\u8981\u901a\u8fc7\u7cfb\u7edf\u7684\u6d4b\u8bd5\u6765\u4fdd\u8bc1\u96f6\u9759\u9ed8\u7387\u7684\u5b9e\u73b0\u3002", "conclusion": "\u8981\u5b9e\u73b0\u96f6\u9759\u9ed8\u7387\uff0c\u9700\u8981\u5bf9\u8bed\u6cd5\u89c4\u5219\u8fdb\u884c\u8be6\u5c3d\u7684\u89c4\u8303\u548c\u6d4b\u8bd5\uff1b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8bc4\u4f30\u8f6c\u5bfc\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ee5\u907f\u514d\u4e0d\u6b63\u786e\u7684\u6d88\u6b67\u4e49\u7ed3\u679c\u3002"}}
{"id": "2510.23931", "categories": ["cs.LG", "cs.CR", "cs.DC", "68T07 (Primary) 68M14, 68P27, 68Q32, 94A16, 62H35 (Secondary)", "I.2.11; I.2.6; C.2.4; D.4.6; K.4.1"], "pdf": "https://arxiv.org/pdf/2510.23931", "abs": "https://arxiv.org/abs/2510.23931", "authors": ["Miguel Fernandez-de-Retana", "Unai Zulaika", "Rub\u00e9n S\u00e1nchez-Corcuera", "Aitor Almeida"], "title": "Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments", "comment": "17 pages, 12 figures", "summary": "Federated Learning (FL) allows for the training of Machine Learning models in\na collaborative manner without the need to share sensitive data. However, it\nremains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private\ninformation from the shared model updates. In this work, we investigate the\neffectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD\nand a variant based on explicit regularization (PDP-SGD) - as defenses against\nGLAs. To this end, we evaluate the performance of several computer vision\nmodels trained under varying privacy levels on a simple classification task,\nand then analyze the quality of private data reconstructions obtained from the\nintercepted gradients in a simulated FL environment. Our results demonstrate\nthat DP-SGD significantly mitigates the risk of gradient leakage attacks,\nalbeit with a moderate trade-off in model utility. In contrast, PDP-SGD\nmaintains strong classification performance but proves ineffective as a\npractical defense against reconstruction attacks. These findings highlight the\nimportance of empirically evaluating privacy mechanisms beyond their\ntheoretical guarantees, particularly in distributed learning scenarios where\ninformation leakage may represent an unassumable critical threat to data\nsecurity and privacy.", "AI": {"tldr": "DP-SGD \u80fd\u663e\u8457\u51cf\u8f7b\u68af\u5ea6\u6cc4\u9732\u98ce\u9669\uff0c\u4f46\u5bf9\u6a21\u578b\u6548\u7528\u6709\u4e2d\u7b49\u6298\u4e2d\uff1bPDP-SGD \u80fd\u4fdd\u6301\u8f83\u9ad8\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u5bf9\u5b9e\u9645\u9632\u5fa1\u91cd\u5efa\u653b\u51fb\u6548\u679c\u6709\u9650\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u68af\u5ea6\u6cc4\u9732\u53ef\u80fd\u4ece\u5171\u4eab\u7684\u68af\u5ea6\u4fe1\u606f\u4e2d\u6062\u590d\u79c1\u6709\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u5b9e\u9645\u53ef\u7528\u7684\u9690\u79c1\u673a\u5236\u53ca\u5176\u5bf9\u6297\u6cc4\u9732\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u662f\u7406\u8bba\u9690\u79c1\u4fdd\u8bc1\u3002", "method": "\u5728\u4e00\u4e2a\u7b80\u5355\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u5bf9\u591a\u79cd\u6a21\u578b\u5728\u4e0d\u540c\u9690\u79c1\u7b49\u7ea7\u4e0b\u8fdb\u884c\u8bad\u7ec3\uff1b\u5728\u6a21\u62df\u7684FL\u73af\u5883\u4e2d\uff0c\u8bc4\u4f30\u88ab\u622a\u83b7\u7684\u68af\u5ea6\u80fd\u5426\u91cd\u5efa\u79c1\u6709\u6570\u636e\uff0c\u4ee5\u6b64\u6bd4\u8f83 DP-SGD \u4e0e PDP-SGD \u7684\u9632\u62a4\u6548\u679c\u3002", "result": "DP-SGD \u663e\u8457\u7f13\u89e3\u68af\u5ea6\u6cc4\u9732\u98ce\u9669\uff0c\u4ee3\u4ef7\u662f\u6a21\u578b\u6027\u80fd\u6709\u4e2d\u7b49\u7a0b\u5ea6\u7684\u4e0b\u964d\uff1bPDP-SGD \u867d\u4fdd\u6301\u8f83\u9ad8\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u5bf9\u91cd\u5efa\u653b\u51fb\u7684\u5b9e\u9645\u9632\u62a4\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u9700\u8981\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u4ee5\u8861\u91cf\u9690\u79c1\u673a\u5236\u5728\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u9632\u62a4\u6548\u679c\uff0c\u5355\u7eaf\u7684\u7406\u8bba\u9690\u79c1\u4fdd\u8bc1\u4e0d\u8db3\u4ee5\u786e\u4fdd\u6570\u636e\u5b89\u5168\u3002"}}
{"id": "2510.24538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24538", "abs": "https://arxiv.org/abs/2510.24538", "authors": ["Venkata S Govindarajan", "Laura Biester"], "title": "Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written", "comment": null, "summary": "Textual humor is enormously diverse and computational studies need to account\nfor this range, including intentionally bad humor. In this paper, we curate and\nanalyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to\nbetter understand \"bad\" humor in English. Standard humor detection models\nperform poorly on our corpus, and an analysis of literary devices finds that\nthese sentences combine features common in existing humor datasets (e.g., puns,\nirony) with metaphor, metafiction and simile. LLMs prompted to synthesize\ncontest-style sentences imitate the form but exaggerate the effect by\nover-using certain literary devices, and including far more novel\nadjective-noun bigrams than human writers. Data, code and analysis are\navailable at https://github.com/venkatasg/bulwer-lytton", "AI": {"tldr": "\u901a\u8fc7Bulwer-Lytton\u7ade\u8d5b\u6587\u672c\u63ed\u793a\u201c\u7cdf\u7cd5\u5e7d\u9ed8\u201d\u7684\u7279\u5f81\uff0c\u73b0\u6709\u5e7d\u9ed8\u68c0\u6d4b\u5728\u6b64\u8bed\u6599\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0cLLMs\u5728\u4eff\u5199\u65f6\u653e\u5927\u67d0\u4e9b\u6587\u5b66\u624b\u6cd5\u5e76\u4ea7\u751f\u5927\u91cf\u65b0\u9896\u5f62\u5bb9\u8bcd-\u540d\u8bcd\u642d\u914d\u3002", "motivation": "\u9700\u8981\u8986\u76d6\u66f4\u5e7f\u6cdb\u7684\u5e7d\u9ed8\u5f62\u5f0f\uff08\u5305\u62ec\u6545\u610f\u7cdf\u7cd5\u3001\u8bbd\u523a\u6027\u7b49\uff09\uff0c\u4ee5\u4e30\u5bcc\u6570\u636e\u9a71\u52a8\u7684\u5e7d\u9ed8\u7814\u7a76\u3001\u8bc4\u4f30\u53ca\u751f\u6210\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6784\u5efaBulwer-Lytton\u8bed\u6599\u5e93\uff0c\u8bc4\u4f30\u73b0\u6709\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u5bf9\u8be5\u8bed\u6599\u7684\u8868\u73b0\uff1b\u5bf9\u6587\u672c\u4e2d\u7684\u4fee\u8f9e\u624b\u6cd5\uff08\u53cc\u5173\u3001\u8bbd\u523a\u3001\u9690\u55bb\u3001\u5143\u865a\u6784\u3001\u660e\u55bb\u7b49\uff09\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff1b\u901a\u8fc7\u63d0\u793a\u5bf9\u6bd4\u8ba9LLMs\u751f\u6210contest\u98ce\u683c\u53e5\u5b50\u5e76\u4e0e\u4eba\u7c7b\u6587\u672c\u8fdb\u884c\u5bf9\u7167\uff1b\u516c\u5f00\u6570\u636e\u3001\u4ee3\u7801\u4e0e\u5206\u6790\u3002", "result": "\u73b0\u6709\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u5728\u6b64\u8bed\u6599\u4e0a\u663e\u8457\u8868\u73b0\u4e0d\u8db3\uff1b\u6587\u672c\u540c\u65f6\u5177\u5907\u5e38\u89c1\u5e7d\u9ed8\u6570\u636e\u96c6\u7279\u5f81\u53ca\u989d\u5916\u7684\u9690\u55bb\u3001\u5143\u865a\u6784\u3001\u660e\u55bb\u7b49\u7ed3\u6784\uff1bLLMs\u5728\u6a21\u4eff\u683c\u5f0f\u65f6\u8fc7\u5ea6\u4f7f\u7528\u67d0\u4e9b\u6587\u5b66\u624b\u6cd5\uff0c\u5e76\u4ea7\u751f\u6bd4\u4eba\u7c7b\u66f4\u4e30\u5bcc\u7684\u5f62\u5bb9\u8bcd-\u540d\u8bcd\u4e8c\u5143\u642d\u914d\u3002", "conclusion": "\u8981\u66f4\u597d\u5730\u7406\u89e3\u548c\u751f\u6210\u201c\u7cdf\u7cd5\u5e7d\u9ed8\u201d\uff0c\u9700\u7eb3\u5165\u66f4\u5e7f\u6cdb\u7684\u6587\u5b66\u624b\u6cd5\u548c\u6570\u636e\u8986\u76d6\uff0c\u5e76\u901a\u8fc7\u5f00\u653e\u6570\u636e\u4e0e\u4ee3\u7801\u63d0\u5347\u7814\u7a76\u7684\u590d\u73b0\u6027\u548c\u5e94\u7528\u6027\u3002"}}
{"id": "2510.24541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24541", "abs": "https://arxiv.org/abs/2510.24541", "authors": ["Seyoung Song", "Nawon Kim", "Songeun Chae", "Kiwoong Park", "Jiho Jin", "Haneul Yoo", "Kyunghyun Cho", "Alice Oh"], "title": "Open Korean Historical Corpus: A Millennia-Scale Diachronic Collection of Public Domain Texts", "comment": "Dataset and code available at https://github.com/seyoungsong/OKHC", "summary": "The history of the Korean language is characterized by a discrepancy between\nits spoken and written forms and a pivotal shift from Chinese characters to the\nHangul alphabet. However, this linguistic evolution has remained largely\nunexplored in NLP due to a lack of accessible historical corpora. To address\nthis gap, we introduce the Open Korean Historical Corpus, a large-scale, openly\nlicensed dataset spanning 1,300 years and 6 languages, as well as\nunder-represented writing systems like Korean-style Sinitic (Idu) and\nHanja-Hangul mixed script. This corpus contains 18 million documents and 5\nbillion tokens from 19 sources, ranging from the 7th century to 2025. We\nleverage this resource to quantitatively analyze major linguistic shifts: (1)\nIdu usage peaked in the 1860s before declining sharply; (2) the transition from\nHanja to Hangul was a rapid transformation starting around 1890; and (3) North\nKorea's lexical divergence causes modern tokenizers to produce up to 51 times\nhigher out-of-vocabulary rates. This work provides a foundational resource for\nquantitative diachronic analysis by capturing the history of the Korean\nlanguage. Moreover, it can serve as a pre-training corpus for large language\nmodels, potentially improving their understanding of Sino-Korean vocabulary in\nmodern Hangul as well as archaic writing systems.", "AI": {"tldr": "Open Korean Historical Corpus: \u4e00\u4e2a\u8de8 1300 \u5e74\u3001\u8986\u76d6 6 \u79cd\u8bed\u8a00\u7684\u5927\u89c4\u6a21\u5386\u53f2\u8bed\u6599\u5e93\uff0c18 \u767e\u4e07\u6587\u6863\u300150 \u4ebf\u8bcd\u3001\u6765\u81ea 19 \u4e2a\u6765\u6e90\uff0c\u6db5\u76d6 Idu\u3001\u6c49\u5b57- Hangul \u6df7\u5408\u4e66\u5199\u7b49\u7f55\u89c1\u4f53\u7cfb\uff0c\u63ed\u793a\u8bed\u8a00\u6f14\u53d8\u5e76\u53ef\u7528\u4e8e\u5bf9\u73b0\u4ee3\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u3002", "motivation": "\u5f25\u8865 NLP \u9886\u57df\u5728\u97e9\u8bed\u5386\u53f2\u7814\u7a76\u4e2d\u7684\u6570\u636e\u7a7a\u7f3a\uff0c\u7f3a\u4e4f\u53ef\u83b7\u53d6\u7684\u5386\u53f2\u8bed\u6599\u9650\u5236\u4e86\u5b9a\u91cf diachronic \u5206\u6790\u4e0e\u8de8\u5199\u4f5c\u7cfb\u7edf\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efa Open Korean Historical Corpus\uff0c\u65f6\u95f4\u8de8\u5ea6\u81ea\u516c\u5143 7 \u4e16\u7eaa\u81f3 2025 \u5e74\uff0c\u5305\u542b 18M \u6587\u6863\u30015B \u8bcd\u300119 \u4e2a\u6765\u6e90\u30016 \u79cd\u8bed\u8a00\uff0c\u8986\u76d6\u6c49\u5b57-\u97e9\u5b57\u6df7\u5408\u5199\u4f5c\u4e0e Idu \u7b49\u7f55\u89c1\u4f53\u7cfb\uff0c\u63d0\u4f9b\u5f00\u653e\u8bb8\u53ef\uff1b\u53ef\u7528\u4e8e\u5b9a\u91cf\u5206\u6790\u5386\u53f2\u8bed\u8a00\u53d8\u5316\uff0c\u5e76\u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u3002", "result": "\u5b9a\u91cf\u5206\u6790\u63ed\u793a\uff1a1) Idu \u4f7f\u7528\u5728 1860 \u5e74\u4ee3\u8fbe\u5230\u5cf0\u503c\u540e\u8fc5\u901f\u4e0b\u964d\uff1b2) \u6c49\u5b57\u8f6c Hangul \u7684\u8f6c\u53d8\u81ea 1890 \u5e74\u524d\u540e\u5f00\u59cb\u5e76\u5feb\u901f\u5b8c\u6210\uff1b3) \u5317\u671d\u9c9c\u8bcd\u6c47\u5dee\u5f02\u5bfc\u81f4\u73b0\u4ee3\u5206\u8bcd\u5668\u7684 OOV \u7387\u9ad8\u51fa\u591a\u8fbe 51 \u500d\uff1b\u8be5\u6570\u636e\u96c6\u4e3a\u5386\u53f2\u8bed\u8a00\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u8d44\u6e90\uff0c\u5e76\u53ef\u7528\u4e8e\u63d0\u5347\u5bf9 Sino-Korean \u8bcd\u6c47\u7684\u7406\u89e3\u7684\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u8bed\u6599\u5e93\u4e3a\u5b9a\u91cf\u65e5\u53f2\u5206\u6790\u5960\u5b9a\u57fa\u7840\uff0c\u4fc3\u8fdb\u5bf9\u73b0\u4ee3 Hangul \u4e0e\u53e4\u4ee3\u4e66\u5199\u7cfb\u7edf\u53ca Sino-Korean \u8bcd\u6c47\u7684\u7406\u89e3\uff0c\u5e76\u5177\u6709\u663e\u8457\u7684\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2510.23940", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23940", "abs": "https://arxiv.org/abs/2510.23940", "authors": ["Anastasia-Maria Leventi-Peetz", "J\u00f6rg-Volker Peetz", "Kai Weber", "Nikolaos Zacharis"], "title": "Modeling Biological Multifunctionality with Echo State Networks", "comment": "26 pages, 17 figures, 6 tables, 23 references", "summary": "In this work, a three-dimensional multicomponent reaction-diffusion model has\nbeen developed, combining excitable-system dynamics with diffusion processes\nand sharing conceptual features with the FitzHugh-Nagumo model. Designed to\ncapture the spatiotemporal behavior of biological systems, particularly\nelectrophysiological processes, the model was solved numerically to generate\ntime-series data. These data were subsequently used to train and evaluate an\nEcho State Network (ESN), which successfully reproduced the system's dynamic\nbehavior. The results demonstrate that simulating biological dynamics using\ndata-driven, multifunctional ESN models is both feasible and effective.", "AI": {"tldr": "3D\u591a\u7ec4\u5206\u53cd\u5e94\u6269\u6563\u6a21\u578b\u7528\u4e8e\u751f\u7269\u7535\u751f\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u5177\u5907FitzHugh\u2013Nagumo\u7279\u5f81\u7684\u6269\u6563\u8fc7\u7a0b\uff1b\u901a\u8fc7\u6570\u503c\u6c42\u89e3\u83b7\u5f97\u65f6\u95f4\u5e8f\u5217\u540e\uff0c\u7528Echo State Network (ESN) \u5b66\u4e60\u5e76\u518d\u73b0\u7cfb\u7edf\u52a8\u529b\u5b66\u3002", "motivation": "\u5c06\u7269\u7406\u9a71\u52a8\u7684\u53cd\u5e94\u6269\u6563\u6a21\u578b\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u7ed3\u5408\uff0c\u4ee5\u6355\u6349\u751f\u7269\u5b66\u7cfb\u7edf\u7684\u65f6\u7a7a\u52a8\u6001\u5e76\u63d0\u5347\u9884\u6d4b/\u518d\u73b0\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e09\u7ef4\u591a\u7ec4\u5206\u53cd\u5e94-\u6269\u6563\u6a21\u578b\uff0c\u7406\u8bba\u4e0a\u5177\u5907FitzHugh\u2013Nagumo\u7684\u5174\u594b-\u6291\u5236\u52a8\u529b\u5b66\uff1b\u5bf9\u7cfb\u7edf\u8fdb\u884c\u6570\u503c\u6c42\u89e3\u4ee5\u4ea7\u751f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff1b\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e00\u4e2aEcho State Network\uff0c\u4ee5\u518d\u73b0\u7cfb\u7edf\u52a8\u529b\u5b66\u3002", "result": "ESN\u6210\u529f\u518d\u73b0\u4e86\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u8868\u660e\u5728\u6570\u636e\u9a71\u52a8\u3001\u5177\u591a\u529f\u80fd\u6027\u7684ESN\u6a21\u578b\u4e0b\uff0c\u6a21\u62df\u751f\u7269\u52a8\u529b\u5b66\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684ESN\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u7269\u7406\u6a21\u578b\u7684\u6709\u529b\u8865\u5145\uff0c\u63d0\u5347\u5bf9\u590d\u6742\u751f\u7269\u7cfb\u7edf\u7684\u9884\u6d4b\u4e0e\u518d\u73b0\u80fd\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u4e0e\u9a8c\u8bc1\u3002"}}
{"id": "2510.24570", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24570", "abs": "https://arxiv.org/abs/2510.24570", "authors": ["Rapha\u00ebl Bagat", "Irina Illina", "Emmanuel Vincent"], "title": "BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation", "comment": "Submitted to ICASSP 2026", "summary": "Automatic Speech Recognition (ASR) systems, despite large multilingual\ntraining, struggle in out-of-domain and low-resource scenarios where labeled\ndata is scarce. We propose BEARD (BEST-RQ Encoder Adaptation with Re-training\nand Distillation), a novel framework designed to adapt Whisper's encoder using\nunlabeled data. Unlike traditional self-supervised learning methods, BEARD\nuniquely combines a BEST-RQ objective with knowledge distillation from a frozen\nteacher encoder, ensuring the encoder's complementarity with the pre-trained\ndecoder. Our experiments focus on the ATCO2 corpus from the challenging Air\nTraffic Control (ATC) communications domain, characterized by non-native\nspeech, noise, and specialized phraseology. Using about 5,000 hours of\nuntranscribed speech for BEARD and 2 hours of transcribed speech for\nfine-tuning, the proposed approach significantly outperforms previous baseline\nand fine-tuned model, achieving a relative improvement of 12% compared to the\nfine-tuned model. To the best of our knowledge, this is the first work to use a\nself-supervised learning objective for domain adaptation of Whisper.", "AI": {"tldr": "\u63d0\u51fa BEARD \u6846\u67b6\uff0c\u901a\u8fc7 BEST-RQ \u81ea\u76d1\u7763\u76ee\u6807\u4e0e\u51bb\u7ed3\u6559\u5e08\u7f16\u7801\u5668\u7684\u77e5\u8bc6\u84b8\u998f\u6765\u9002\u914d Whisper \u7684\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u63d0\u5347\u5728 ATCO2/ATC \u573a\u666f\u4e0b\u7684\u8bc6\u522b\u6548\u679c\uff1b\u9996\u6b21\u5728 Whisper \u4e0a\u4f7f\u7528\u81ea\u76d1\u7763\u76ee\u6807\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u3002", "motivation": "\u5728\u51fa\u57df\u4e0e\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\uff0c\u5c3d\u7ba1\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u8bad\u7ec3\uff0cASR \u4ecd\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e0e\u57df\u5dee\u95ee\u9898\u3002\u9700\u8981\u5728\u4fdd\u8bc1\u89e3\u7801\u5668\u534f\u540c\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u5347\u7f16\u7801\u5668\u5bf9\u9886\u57df\u7279\u5f81\u7684\u8868\u5f81\u80fd\u529b\uff1b\u672a\u6807\u6ce8\u6570\u636e\u8d44\u6e90\u4e30\u5bcc\uff0c\u5229\u7528\u81ea\u76d1\u7763\u4fe1\u53f7\u8fdb\u884c\u57df\u9002\u914d\u662f\u53ef\u884c\u8def\u5f84\u3002", "method": "BEARD \u5c06 BEST-RQ \u81ea\u76d1\u7763\u76ee\u6807\u4e0e\u5bf9\u51bb\u7ed3\u6559\u5e08\u7f16\u7801\u5668\u7684\u77e5\u8bc6\u84b8\u998f\u7ed3\u5408\uff0c\u5bf9 Whisper \u7684\u7f16\u7801\u5668\u8fdb\u884c\u518d\u8bad\u7ec3\u4ee5\u5b9e\u73b0\u5bf9\u9886\u57df\u7684\u9002\u914d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u7684\u4e92\u8865\u6027\u3002\u8bad\u7ec3\u4e2d\u4f7f\u7528\u7ea6 5,000 \u5c0f\u65f6\u672a\u6807\u6ce8\u8bed\u97f3\u8fdb\u884c\u7f16\u7801\u5668\u81ea\u9002\u5e94\uff0c2 \u5c0f\u65f6\u5e26\u6807\u6ce8\u6570\u636e\u7528\u4e8e\u5fae\u8c03\uff0c\u6559\u5e08\u7f16\u7801\u5668\u4fdd\u6301\u51bb\u7ed3\u3002", "result": "\u5728 ATCO2/ATC \u573a\u666f\u7684\u5b9e\u9a8c\u4e2d\uff0cBEARD \u663e\u8457\u4f18\u4e8e\u5148\u524d\u57fa\u7ebf\u548c\u76f4\u63a5\u5fae\u8c03\u6a21\u578b\uff0c\u76f8\u5bf9\u4e8e\u5fae\u8c03\u6a21\u578b\u5b9e\u73b0\u7ea6 12% \u7684\u76f8\u5bf9\u63d0\u5347\uff1b\u8fd9\u662f\u9996\u6b21\u5728 Whisper \u4e0a\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u8fdb\u884c\u9886\u57df\u81ea\u9002\u914d\u7684\u5de5\u4f5c\u3002", "conclusion": "BEARD \u4e3a Whisper \u7f16\u7801\u5668\u7684\u9886\u57df\u81ea\u9002\u914d\u63d0\u4f9b\u4e00\u79cd\u6709\u6548\u601d\u8def\uff1a\u5728\u4e0d\u5927\u5e45\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u76ee\u6807\u4e0e\u84b8\u998f\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u5bf9\u9886\u57df\u7279\u5f81\u7684\u63d0\u5347\uff0c\u5e76\u5177\u5907\u4e0e\u89e3\u7801\u5668\u7684\u534f\u540c\u517c\u5bb9\u6027\u3002"}}
{"id": "2510.23948", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23948", "abs": "https://arxiv.org/abs/2510.23948", "authors": ["Qianfeng Wen", "Zhenwei Tang", "Ashton Anderson"], "title": "ChessQA: Evaluating Large Language Models for Chess Understanding", "comment": "33 pages,8 figures", "summary": "Chess provides an ideal testbed for evaluating the reasoning, modeling, and\nabstraction capabilities of large language models (LLMs), as it has\nwell-defined structure and objective ground truth while admitting a wide\nspectrum of skill levels. However, existing evaluations of LLM ability in chess\nare ad hoc and narrow in scope, making it difficult to accurately measure LLM\nchess understanding and how it varies with scale, post-training methodologies,\nor architecture choices. We present ChessQA, a comprehensive benchmark that\nassesses LLM chess understanding across five task categories (Structural,\nMotifs, Short Tactics, Position Judgment, and Semantic), which approximately\ncorrespond to the ascending abstractions that players master as they accumulate\nchess knowledge, from understanding basic rules and learning tactical motifs to\ncorrectly calculating tactics, evaluating positions, and semantically\ndescribing high-level concepts. In this way, ChessQA captures a more\ncomprehensive picture of chess ability and understanding, going significantly\nbeyond the simple move quality evaluations done previously, and offers a\ncontrolled, consistent setting for diagnosis and comparison. Furthermore,\nChessQA is inherently dynamic, with prompts, answer keys, and construction\nscripts that can evolve as models improve. Evaluating a range of contemporary\nLLMs, we find persistent weaknesses across all five categories and provide\nresults and error analyses by category. We will release the code, periodically\nrefreshed datasets, and a public leaderboard to support further research.", "AI": {"tldr": "ChessQA \u662f\u4e00\u4e2a\u5305\u542b\u4e94\u5927\u4efb\u52a1\u7c7b\u522b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4ee5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68cb\u5c40\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u8d70\u5b50\u8d28\u91cf\u8bc4\u4ef7\uff0c\u5e76\u5177\u5907\u52a8\u6001\u66f4\u65b0\u4e0e\u516c\u5f00\u6392\u884c\u699c\u7279\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u68cb\u7c7b\u7684LLM\u8bc4\u4f30\u591a\u4e3a\u96f6\u6563\u4e14\u8303\u56f4\u6709\u9650\uff0c\u96be\u4ee5\u8861\u91cf\u6a21\u578b\u5728\u5c3a\u5ea6\u3001\u540e\u8bad\u7ec3\u65b9\u6cd5\u3001\u67b6\u6784\u5dee\u5f02\u4e0b\u7684\u68cb\u7406\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u4e00\u4e2a\u5168\u9762\u3001\u53ef\u8bca\u65ad\u3001\u53ef\u6bd4\u8f83\u7684\u57fa\u51c6\u6765\u7cfb\u7edf\u5730\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u68cb\u7c7b\u7406\u89e3\u3002", "method": "\u63d0\u51fa ChessQA\uff0c\u8986\u76d6\u4e94\u5927\u4efb\u52a1\u7c7b\u522b\uff1a\u7ed3\u6784\u6027\uff08Structural\uff09\u3001\u4e3b\u9898/\u68cb\u7406\u6a21\u5f0f\uff08Motifs\uff09\u3001\u77ed\u6218\u672f\uff08Short Tactics\uff09\u3001\u68cb\u5c40\u5224\u65ad\uff08Position Judgment\uff09\u3001\u8bed\u4e49\u7406\u89e3\uff08Semantic\uff09\u3002\u4efb\u52a1\u8bbe\u8ba1\u6309\u68cb\u624b\u77e5\u8bc6\u5c42\u7ea7\u9012\u589e\u62bd\u8c61\uff0c\u4ece\u89c4\u5219\u4e0e\u6218\u672f\u4e3b\u9898\uff0c\u5230\u6b63\u786e\u8ba1\u7b97\u6218\u672f\u3001\u8bc4\u4f30\u68cb\u5c40\u5e76\u8bed\u4e49\u63cf\u8ff0\u9ad8\u5c42\u6982\u5ff5\u3002\u57fa\u51c6\u5177\u5907\u52a8\u6001\u7279\u6027\uff1a\u63d0\u793a\u3001\u7b54\u6848\u8981\u70b9\u3001\u6784\u5efa\u811a\u672c\u53ef\u968f\u6a21\u578b\u8fdb\u6b65\u800c\u6f14\u5316\u3002\u5bf9\u591a\u79cd\u5f53\u4ee3LLM\u8fdb\u884c\u8bc4\u6d4b\uff0c\u7ed9\u51fa\u9010\u7c7b\u7684\u7ed3\u679c\u4e0e\u9519\u8bef\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u8bca\u65ad\u4e0e\u6bd4\u8f83\u7684\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u8bc4\u6d4b\u663e\u793a\u6240\u6709\u7c7b\u522b\u91cc\uff0c\u5f53\u524dLLMs\u4ecd\u666e\u904d\u5b58\u5728\u5f31\u70b9\uff0c\u4e94\u7c7b\u4efb\u52a1\u5747\u5448\u73b0\u51fa\u53ef\u91cd\u590d\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u7814\u7a76\u7ed9\u51fa\u6309\u7c7b\u522b\u7684\u7ed3\u679c\u4e0e\u9519\u8bef\u5206\u6790\u3002", "conclusion": "ChessQA \u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u3001\u52a8\u6001\u7684\u68cb\u7406\u7406\u89e3\u8bc4\u4f30\u57fa\u51c6\uff0c\u4fbf\u4e8e\u8bca\u65ad\u548c\u6a2a\u5411\u6bd4\u8f83\uff0c\u5e76\u5c06\u6301\u7eed\u53d1\u5e03\u4ee3\u7801\u3001\u66f4\u65b0\u7684\u6570\u636e\u96c6\u4e0e\u516c\u5f00\u6392\u884c\u699c\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2510.23966", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23966", "abs": "https://arxiv.org/abs/2510.23966", "authors": ["Scott Emmons", "Roland S. Zimmermann", "David K. Elson", "Rohin Shah"], "title": "A Pragmatic Way to Measure Chain-of-Thought Monitorability", "comment": "The first two authors contributed equally", "summary": "While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI\nsafety, this opportunity could be lost through shifts in training practices or\nmodel architecture. To help preserve monitorability, we propose a pragmatic way\nto measure two components of it: legibility (whether the reasoning can be\nfollowed by a human) and coverage (whether the CoT contains all the reasoning\nneeded for a human to also produce the final output). We implement these\nmetrics with an autorater prompt that enables any capable LLM to compute the\nlegibility and coverage of existing CoTs. After sanity-checking our prompted\nautorater with synthetic CoT degradations, we apply it to several frontier\nmodels on challenging benchmarks, finding that they exhibit high\nmonitorability. We present these metrics, including our complete autorater\nprompt, as a tool for developers to track how design decisions impact\nmonitorability. While the exact prompt we share is still a preliminary version\nunder ongoing development, we are sharing it now in the hopes that others in\nthe community will find it useful. Our method helps measure the default\nmonitorability of CoT - it should be seen as a complement, not a replacement,\nfor the adversarial stress-testing needed to test robustness against\ndeliberately evasive models.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u8bfb\u6027\u4e0e\u8986\u76d6\u5ea6\u4e24\u7ef4\u5ea6\u7684 CoT \u76d1\u63a7\u5ea6\u91cf\uff0c\u901a\u8fc7 autorater \u63d0\u793a\u8bc4\u4f30\u73b0\u6709 CoT \u7684\u53ef\u8ffd\u8e2a\u6027\uff0c\u5e76\u5bf9\u524d\u6cbf\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4f5c\u4e3a\u8bbe\u8ba1\u51b3\u7b56\u5f71\u54cd monitorability \u7684\u5de5\u5177\u3002", "motivation": "\u4e3a\u9632\u6b62\u8bad\u7ec3\u5b9e\u8df5\u6216\u6a21\u578b\u67b6\u6784\u7684\u53d8\u5316\u524a\u5f31 Chain-of-Thought \u7684\u53ef\u76d1\u63a7\u6027\uff0c\u63d0\u51fa\u53ef\u91cf\u5316\u7684\u76d1\u63a7\u6027\u6307\u6807\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a autorater \u63d0\u793a\uff0c\u8ba9\u4efb\u610f\u5f3a\u529b\u7684 LLM \u8ba1\u7b97 legibility\uff08\u53ef\u88ab\u4eba\u7c7b\u8ddf\u968f\u7684\u63a8\u7406\uff09\u4e0e coverage\uff08\u63a8\u7406\u4e2d\u5305\u542b\u5168\u90e8\u5fc5\u8981\u6b65\u9aa4\uff09\u4e24\u9879\uff1b\u5148\u5bf9\u5408\u6210\u7684 CoT \u964d\u7ea7\u8fdb\u884c\u5065\u5168\u6027\u6d4b\u8bd5\uff0c\u7136\u540e\u5728\u591a\u79cd\u524d\u6cbf\u6a21\u578b\u548c\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff0c\u516c\u5e03\u5b8c\u6574\u7684 autorater \u63d0\u793a\u4ee5\u4f9b\u5f00\u53d1\u8005\u4f7f\u7528\u3002", "result": "\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5c55\u73b0\u51fa\u8f83\u9ad8\u7684\u76d1\u63a7\u6027\uff1b\u63d0\u4f9b\u5b8c\u6574\u7684 autorater \u63d0\u793a\u4f5c\u4e3a\u5de5\u5177\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u8ffd\u8e2a\u8bbe\u8ba1\u51b3\u7b56\u5bf9 monitorability \u7684\u5f71\u54cd\uff0c\u4f46\u63d0\u793a\u4ecd\u5728\u521d\u6b65\u7248\u672c\uff0c\u9700\u8981\u7ee7\u7eed\u53d1\u5c55\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u8861\u91cf CoT \u9ed8\u8ba4 monitorability \u7684\u5de5\u5177\uff0c\u4f5c\u4e3a\u5bf9\u6297\u6027\u538b\u529b\u6d4b\u8bd5\u7684\u8865\u5145\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bbe\u8ba1\u51b3\u7b56\u5bf9\u76d1\u63a7\u6027\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.24592", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24592", "abs": "https://arxiv.org/abs/2510.24592", "authors": ["Guoxin Chen", "Jing Wu", "Xinjie Chen", "Wayne Xin Zhao", "Ruihua Song", "Chengxi Li", "Kai Fan", "Dayiheng Liu", "Minpeng Liao"], "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization", "comment": "Ongoing Work", "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.", "AI": {"tldr": "ReForm: a reflective autoformalization framework with Prospective Bounded Sequence Optimization (PBSO) and ConsistencyCheck. It iteratively refines formal statements via semantic feedback, achieving large gains over baselines and revealing inherent semantic challenges in autoformalization even for humans.", "motivation": "Large Language Models often translate natural language math into formal statements but fail to preserve semantic intent. Autoformalization needs self-reflection and iterative refinement mechanisms like humans to ensure semantic fidelity.", "method": "Propose ReForm that integrates semantic consistency evaluation into the autoformalization loop for iterative refinement. Train a reflective model with PBSO, using position-based rewards to balance accurate formalization and semantic validation. Evaluate on four benchmarks and introduce ConsistencyCheck (859 expert-annotated items) to assess evaluator reliability and semantic difficulty.", "result": "Across four benchmarks, ReForm achieves an average improvement of 17.2 percentage points over the strongest baselines. ConsistencyCheck reveals that even expert annotators make semantic errors in up to 38.5% of cases, highlighting the intrinsic difficulty of autoformalization.", "conclusion": "Reflective autoformalization with semantic consistency checks and position-aware training substantially boosts performance and reliability. The ConsistencyCheck benchmark provides a more robust evaluation of LLM-based autoformalization and underscores ongoing challenges in preserving semantic intent."}}
{"id": "2510.23972", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23972", "abs": "https://arxiv.org/abs/2510.23972", "authors": ["Andra\u017e Jelin\u010di\u010d", "Owen Lockwood", "Akhil Garlapati", "Guillaume Verdon", "Trevor McCourt"], "title": "An efficient probabilistic hardware architecture for diffusion-like models", "comment": "9 pages, 6 figures", "summary": "The proliferation of probabilistic AI has promoted proposals for specialized\nstochastic computers. Despite promising efficiency gains, these proposals have\nfailed to gain traction because they rely on fundamentally limited modeling\ntechniques and exotic, unscalable hardware. In this work, we address these\nshortcomings by proposing an all-transistor probabilistic computer that\nimplements powerful denoising models at the hardware level. A system-level\nanalysis indicates that devices based on our architecture could achieve\nperformance parity with GPUs on a simple image benchmark using approximately\n10,000 times less energy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5168\u6676\u4f53\u7ba1\u6982\u7387\u6027\u8ba1\u7b97\u673a\uff0c\u901a\u8fc7\u786c\u4ef6\u5b9e\u73b0\u5f3a\u5927\u53bb\u566a\u6a21\u578b\uff0c\u5728\u7b80\u5355\u56fe\u50cf\u57fa\u51c6\u4e0a\u4e0eGPU\u6027\u80fd\u76f8\u5f53\uff0c\u80fd\u8017\u7ea6\u964d\u4f4e10000\u500d\u3002", "motivation": "\u968f\u7740\u6982\u7387AI\u7684\u53d1\u5c55\uff0c\u4e3b\u5f20\u4e13\u7528\u968f\u673a\u8ba1\u7b97\u673a\u7684\u65b9\u6848\u5374\u56e0\u5efa\u6a21\u548c\u786c\u4ef6\u6269\u5c55\u6027\u53d7\u9650\u800c\u96be\u4ee5\u843d\u5730\u3002\u672c\u5de5\u4f5c\u8bd5\u56fe\u901a\u8fc7\u5168\u6676\u4f53\u7ba1\u5b9e\u73b0\u6765\u514b\u670d\u8fd9\u4e9b\u77ed\u677f\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u786c\u4ef6\u7ea7\u6982\u7387\u63a8\u65ad\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5168\u6676\u4f53\u7ba1\u7684\u6982\u7387\u6027\u8ba1\u7b97\u67b6\u6784\uff0c\u5728\u786c\u4ef6\u5c42\u9762\u5b9e\u73b0\u5f3a\u5927\u7684\u53bb\u566a\u6a21\u578b\uff1b\u5bf9\u7cfb\u7edf\u7ea7\u8fdb\u884c\u5206\u6790\uff0c\u8bc4\u4f30\u5728\u7b80\u5355\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0e\u80fd\u8017\u3002", "result": "\u7406\u8bba/\u7cfb\u7edf\u5206\u6790\u8868\u660e\uff0c\u8be5\u67b6\u6784\u53ef\u5728\u7b80\u5355\u56fe\u50cf\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e0eGPU\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u80fd\u8017\u964d\u4f4e\u7ea610,000\u00d7\u3002", "conclusion": "\u5168\u6676\u4f53\u7ba1\u6982\u7387\u6027\u8ba1\u7b97\u673a\u5c55\u793a\u51fa\u5728\u9ad8\u6548\u5b9e\u73b0\u6982\u7387AI\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u786c\u4ef6\u5c42\u9762\u7684\u53bb\u566a\u5f0f\u63a8\u65ad\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.24605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24605", "abs": "https://arxiv.org/abs/2510.24605", "authors": ["Yicun Yang", "Cong Wang", "Shaobo Wang", "Zichen Wen", "Biqing Qi", "Hanlin Xu", "Linfeng Zhang"], "title": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "comment": null, "summary": "Diffusion-based large language models (dLLMs) have exhibited substantial\npotential for parallel text generation, which may enable more efficient\ngeneration compared to autoregressive models. However, current dLLMs suffer\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\nhave to be determined before decoding as a hyper-parameter, leading to issues\nin efficiency and flexibility. To solve these problems, in this work, we\npropose to train a diffusion LLM with native variable generation lengths,\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\npredict the [EOS] token in the generated text, which makes a dLLM be able to\nnatively infer in a block diffusion manner, while still maintaining the ability\nof global bi-directional (full) attention and high parallelism. Experiments on\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\nautoregressive models such as Qwen and Llama. Our method achieves higher\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\nsupporting their practical use in real-world applications. Codes and models\nhave been released.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5177\u5907\u539f\u751f\u53ef\u53d8\u751f\u6210\u957f\u5ea6\u7684\u6269\u6563\u5f0f\u8bed\u8a00\u6a21\u578b dLLM-Var\uff0c\u901a\u8fc7\u9884\u6d4b [EOS] \u5b9e\u73b0\u5728\u533a\u5757\u6269\u6563\u4e2d\u81ea\u9002\u5e94\u7ec8\u6b62\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u548c\u9ad8\u5e76\u884c\u6027\uff1b\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u5b9e\u73b0\u663e\u8457\u63d0\u5347\uff08\u76f8\u5bf9\u4f20\u7edf\u6269\u6563\u5f0f\u63a8\u65ad 30.1x\u3001\u76f8\u5bf9\u81ea\u56de\u5f52\u6a21\u578b\u5982 Qwen/Llama 2.4x\uff09\uff0c\u5e76\u516c\u5f00\u4ee3\u7801\u4e0e\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u5f0f\u8bed\u8a00\u6a21\u578b\u666e\u904d\u5177\u6709\u56fa\u5b9a\u7684\u751f\u6210\u957f\u5ea6\uff0c\u8fd9\u9650\u5236\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u7075\u6d3b\u6027\u4e0e\u6548\u7387\uff0c\u4e14\u96be\u4ee5\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e2d\u81ea\u9002\u5e94\u63a8\u65ad\u957f\u5ea6\u3002\u9700\u5f00\u53d1\u5177\u5907\u539f\u751f\u53ef\u53d8\u957f\u5ea6\u751f\u6210\u80fd\u529b\u7684\u6269\u6563\u5f0f\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u5728\u4fdd\u6301\u5e76\u884c\u6027\u548c\u5168\u5c40\u6ce8\u610f\u529b\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u6548\u7387\u4e0e\u7075\u6d3b\u6027\u3002", "method": "\u5728\u6269\u6563\u5f0f\u8bed\u8a00\u6a21\u578b\u4e2d\u5f15\u5165\u539f\u751f\u7684\u533a\u5757\u6269\u6563\u751f\u6210\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u4f7f\u6a21\u578b\u80fd\u591f\u7cbe\u51c6\u5730\u9884\u6d4b [EOS] token\uff0c\u4ece\u800c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u7ec8\u6b62\uff1b\u4fdd\u6301\u5168\u5c40\u53cc\u5411\u6ce8\u610f\u529b\u4ee5\u7ef4\u6301\u4fe1\u606f\u6574\u5408\uff0c\u5e76\u5b9e\u73b0\u9ad8\u5e76\u884c\u7684\u533a\u5757\u7ea7\u63a8\u65ad\u3002", "result": "\u4e0e\u4f20\u7edf\u6269\u6563\u5f0f\u63a8\u65ad\u76f8\u6bd4\uff0c\u8fbe\u5230 30.1x \u7684\u52a0\u901f\uff0c\u8d85\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u5982 Qwen \u4e0e Llama \u7684\u7ea6 2.4x \u52a0\u901f\uff1b\u5728\u51c6\u786e\u6027\u65b9\u9762\u4e5f\u63d0\u5347\uff0c\u5f3a\u8c03\u8be5\u65b9\u6cd5\u6709\u671b\u4f7f dLLMs \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u5907\u53ef\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u539f\u751f\u53ef\u53d8\u751f\u6210\u957f\u5ea6\u7684\u533a\u5757\u6269\u6563\u673a\u5236\uff0c\u6269\u6563\u5f0fLLMs \u53ef\u5b9e\u73b0\u9ad8\u5e76\u884c\u3001\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u63a8\u7406\uff0c\u5177\u5907\u843d\u5730\u5e94\u7528\u6f5c\u529b\uff1b\u5e76\u5df2\u516c\u5f00\u4ee3\u7801\u4e0e\u6a21\u578b\uff0c\u4fbf\u4e8e\u590d\u73b0\u4e0e\u5e94\u7528\u3002"}}
{"id": "2510.23974", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23974", "abs": "https://arxiv.org/abs/2510.23974", "authors": ["Byeonghu Na", "Minsang Park", "Gyuwon Sim", "Donghyeok Shin", "HeeSun Bae", "Mina Kang", "Se Jung Kwon", "Wanmo Kang", "Il-Chul Moon"], "title": "Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models", "comment": "Accepted at NeurIPS 2025", "summary": "Text-to-image diffusion models rely on text embeddings from a pre-trained\ntext encoder, but these embeddings remain fixed across all diffusion timesteps,\nlimiting their adaptability to the generative process. We propose Diffusion\nAdaptive Text Embedding (DATE), which dynamically updates text embeddings at\neach diffusion timestep based on intermediate perturbed data. We formulate an\noptimization problem and derive an update rule that refines the text embeddings\nat each sampling step to improve alignment and preference between the mean\npredicted image and the text. This allows DATE to dynamically adapts the text\nconditions to the reverse-diffused images throughout diffusion sampling without\nrequiring additional model training. Through theoretical analysis and empirical\nresults, we show that DATE maintains the generative capability of the model\nwhile providing superior text-image alignment over fixed text embeddings across\nvarious tasks, including multi-concept generation and text-guided image\nediting. Our code is available at https://github.com/aailab-kaist/DATE.", "AI": {"tldr": "DATE\u52a8\u6001\u66f4\u65b0\u6587\u672c\u5d4c\u5165\u5728\u6bcf\u4e2a\u6269\u6563\u91c7\u6837\u6b65\u9aa4\u4e2d\uff0c\u6839\u636e\u4e2d\u95f4\u6270\u52a8\u6570\u636e\u8fdb\u884c\u4f18\u5316\u66f4\u65b0\uff0c\u4ee5\u63d0\u5347\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u6982\u5ff5\u751f\u6210\u548c\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u4f7f\u7528\u7684\u6587\u672c\u5d4c\u5165\u5728\u6574\u4e2a\u6269\u6563\u8fc7\u7a0b\u4fdd\u6301\u56fa\u5b9a\uff0c\u9650\u5236\u4e86\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u9002\u5e94\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5728\u91c7\u6837\u9636\u6bb5\u52a8\u6001\u8c03\u6574\u6587\u672c\u6761\u4ef6\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u4f18\u5316\u95ee\u9898\u5e94\u7528\u4e8e\u6bcf\u4e2a\u91c7\u6837\u6b65\u9aa4\uff0c\u57fa\u4e8e\u4e2d\u95f4\u6270\u52a8\u6570\u636e\u63a8\u5bfc\u51fa\u6587\u672c\u5d4c\u5165\u7684\u66f4\u65b0\u89c4\u5219\uff0c\u4f7f\u5d4c\u5165\u968f\u6269\u6563\u8fc7\u7a0b\u52a8\u6001\u8c03\u6574\u4ee5\u66f4\u597d\u5730\u5bf9\u9f50\u9884\u6d4b\u7684\u5747\u503c\u56fe\u50cf\u548c\u6587\u672c\uff0c\u4e14\u4e0d\u9700\u8981\u989d\u5916\u7684\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u4e2d\uff0cDATE\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u80fd\u529b\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u4f9b\u4e86\u6bd4\u56fa\u5b9a\u6587\u672c\u5d4c\u5165\u66f4\u4f18\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\uff0c\u5728\u591a\u6982\u5ff5\u751f\u6210\u548c\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff1b\u4ee3\u7801\u516c\u5f00\u3002", "conclusion": "DATE\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u52a8\u6001\u6761\u4ef6\u80fd\u529b\uff0c\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u4e0e\u504f\u597d\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.24606", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24606", "abs": "https://arxiv.org/abs/2510.24606", "authors": ["Siheng Xiong", "Joe Zou", "Faramarz Fekri", "Yae Jee Cho"], "title": "Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs", "comment": "Accepted to NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "The quadratic cost of attention hinders the scalability of long-context LLMs,\nespecially in resource-constrained settings. Existing static sparse methods\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\nreduce the cost of attention, but poorly adapts to the content-dependent\nvariations in attention due to their staticity. While previous work has\nproposed several dynamic approaches to improve flexibility, they still depend\non predefined templates or heuristic mechanisms. Such strategies reduce\ngenerality and prune tokens that remain contextually important, limiting their\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\nonline without retraining. Our proposed DHSA adaptively segments sequences into\nvariable-length chunks, then computes chunk representations by aggregating the\ntoken embeddings within each chunk. To avoid the bias introduced by varying\nchunk lengths, we apply length-normalized aggregation that scales the averaged\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\nchunk-level similarity scores to token level similarities to calculate\nimportance scores that determine which token-level interactions should be\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\nLongBench show that DHSA matches dense attention in accuracy, while reducing\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\nrepresentative baselines such as block sparse attention, DHSA achieves\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\ncost, offering an efficient and adaptable solution for long-context on-device\nLLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u52a8\u6001\u5206\u5c42\u7a00\u758f\u6ce8\u610f\u529b\uff08DHSA\uff09\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\u5728\u7ebf\u9884\u6d4b\u6ce8\u610f\u529b\u7a00\u758f\u5ea6\uff0c\u901a\u8fc7\u53ef\u53d8\u957f\u5ea6\u5206\u5757\u548c\u957f\u5ea6\u5f52\u4e00\u5316\u805a\u5408\uff0c\u7ed3\u5408\u4ece\u5757\u7ea7\u76f8\u4f3c\u5ea6\u56de\u63a8\u5230\u7c92\u5ea6\u7684\u4ea4\u4e92\u6743\u91cd\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0a\u5b9e\u73b0\u63a5\u8fd1\u5bc6\u96c6\u6ce8\u610f\u529b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u524d\u671f\u5ef6\u8fdf\u4e0e\u663e\u5b58\u5360\u7528\u3002", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587Transformer\u4e2d\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u65b9\u6210\u672c\u74f6\u9888\uff0c\u5e76\u514b\u670d\u73b0\u6709\u9759\u6001/\u6a21\u677f\u4f9d\u8d56\u7684\u5c40\u9650\uff0c\u63d0\u5347\u5bf9\u5185\u5bb9\u76f8\u5173\u6027\u53d8\u5316\u7684\u9002\u5e94\u6027\u4e0e\u6a21\u578b\u901a\u7528\u6027\u3002", "method": "\u5c06\u5e8f\u5217\u5212\u5206\u4e3a\u53ef\u53d8\u957f\u5ea6\u7684\u5757\uff0c\u5757\u5185\u805a\u5408\u5f97\u5230\u5757\u8868\u793a\uff1b\u5bf9\u957f\u5ea6\u8fdb\u884c\u5e73\u65b9\u6839\u5f52\u4e00\u5316\u4ee5\u51cf\u5c0f\u5757\u957f\u77ed\u5e26\u6765\u7684\u504f\u5dee\uff1b\u518d\u5c06\u5757\u7ea7\u76f8\u4f3c\u5ea6\u4e0a\u91c7\u6837\u56de\u5230\u7c92\u5ea6\u7684 token-level \u76f8\u4f3c\u5ea6\u4ee5\u4ea7\u751f\u91cd\u8981\u6027\u5206\u6570\uff0c\u4ece\u800c\u51b3\u5b9a\u4fdd\u7559\u7684 token \u4ea4\u4e92\uff1b\u5728\u7ebf\u9884\u6d4b\u7a00\u758f\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728 Gemma2 \u7684 Needle-in-a-Haystack \u4e0e LongBench \u57fa\u51c6\u4e0a\uff0cDHSA \u7684\u51c6\u786e\u6027\u4e0e\u5bc6\u96c6\u6ce8\u610f\u529b\u76f8\u5f53\uff0c\u524d\u671f\u5ef6\u8fdf\u964d\u4f4e 20-60%\uff0c\u5cf0\u503c\u663e\u5b58\u964d\u4f4e\u7ea6 35%\uff1b\u76f8\u8f83\u4e8e\u533a\u5757\u7a00\u758f\u7b49\u57fa\u7ebf\uff0c DHSA \u5c55\u73b0 6-18% \u7684\u76f8\u5bf9\u51c6\u786e\u6027\u63d0\u5347\uff0c\u6210\u672c\u76f8\u5f53\u6216\u66f4\u4f4e\u3002", "conclusion": "\u63d0\u51fa\u7684 DHSA \u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u957f\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u65b9\u6848\uff0c\u517c\u5177\u51c6\u786e\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u53cb\u597d\u6027\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u5728\u8bbe\u5907\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u573a\u666f\u3002"}}
{"id": "2510.24619", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24619", "abs": "https://arxiv.org/abs/2510.24619", "authors": ["Snegha A", "Sayambhu Sen", "Piyush Singh Pasi", "Abhishek Singhania", "Preethi Jyothi"], "title": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation", "comment": "12 Pages", "summary": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.", "AI": {"tldr": "Prefix-based prefix tuning methods can surpass LoRA for zero-shot cross-lingual transfer in decoder-only LLMs across 35+ languages, delivering gains up to 6% with only ~1.23M learnable parameters, and scaling effectively from 1B to 24B models.", "motivation": "To address the challenge of adapting decoder-only LLMs to new multilingual tasks with parameter-efficient fine-tuning, exploring prefix-based methods which are under-explored for zero-shot transfer.", "method": "Systematic study of three prefix-based methods (soft prompt tuning, prefix tuning, Llama Adapter) for zero-shot cross-lingual transfer from English to 35+ languages, across linguistic families and scripts, and model scales from 1B to 24B; empirical evaluation on Llama 3.1 8B and Mistral v0.3 7B, comparing against LoRA baselines.", "result": "Prefix methods outpace LoRA baselines by up to 6% on Belebele; similar gains observed with Mistral 7B; only 1.23M learnable parameters used for prefix tuning, with consistent improvements across benchmarks.", "conclusion": "Prefix-based techniques are an effective and scalable alternative to LoRA for zero-shot cross-lingual transfer in decoder-only LLMs, particularly beneficial in low-resource multilingual settings."}}
{"id": "2510.23980", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.23980", "abs": "https://arxiv.org/abs/2510.23980", "authors": ["Guojing Cong", "Tom Potok", "Hamed Poursiami", "Maryam Parsa"], "title": "HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing", "comment": null, "summary": "We present a novel algorithm, \\hdgc, that marries graph convolution with\nbinding and bundling operations in hyperdimensional computing for transductive\ngraph learning. For prediction accuracy \\hdgc outperforms major and popular\ngraph neural network implementations as well as state-of-the-art\nhyperdimensional computing implementations for a collection of homophilic\ngraphs and heterophilic graphs. Compared with the most accurate learning\nmethodologies we have tested, on the same target GPU platform, \\hdgc is on\naverage 9561.0 and 144.5 times faster than \\gcnii, a graph neural network\nimplementation and HDGL, a hyperdimensional computing implementation,\nrespectively. As the majority of the learning operates on binary vectors, we\nexpect outstanding energy performance of \\hdgc on neuromorphic and emerging\nprocess-in-memory devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bHDGC\u7b97\u6cd5\uff0c\u5c06\u56fe\u5377\u79ef\u4e0e\u8d85\u7ef4\u8ba1\u7b97\u4e2d\u7684\u7ed1\u5b9a\uff08binding\uff09\u548c\u6253\u5305\uff08bundling\uff09\u64cd\u4f5c\u7ed3\u5408\uff0c\u7528\u4e8e\u4f20\u5bfc\u5f0f\u56fe\u5b66\u4e60\u3002\u5728\u591a\u79cd\u540c\u8d28\u6027/\u5f02\u8d28\u6027\u56fe\u4e0a\uff0cHDGC\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4e3b\u6d41GNN\u548c\u57fa\u4e8eHD\u7684\u6700\u65b0\u5b9e\u73b0\uff1b\u4e14\u5728\u76f8\u540cGPU\u5e73\u53f0\u4e0b\uff0cHDGC\u5e73\u5747\u6bd4GCNII\u5feb\u7ea69561\u500d\u3001\u6bd4HDGL\u5feb\u7ea6144.5\u500d\u3002\u7531\u4e8e\u5927\u90e8\u5206\u5b66\u4e60\u5728\u4e8c\u8fdb\u5236\u5411\u91cf\u4e0a\u8fdb\u884c\uff0c\u672a\u6765\u5728\u795e\u7ecf\u5f62\u6001\u548c\u5185\u5b58\u8ba1\u7b97\u4e2d\u5177\u5907\u51fa\u8272\u7684\u80fd\u6548\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709GNN\u548cHD\u65b9\u6cd5\u5728\u4f20\u5bfc\u5f0f\u56fe\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u548c\u80fd\u6548\u74f6\u9888\uff1b\u901a\u8fc7\u5c06\u56fe\u5377\u79ef\u4e0e\u8d85\u7ef4\u8ba1\u7b97\u7684\u7ed1\u5b9a/\u6253\u5305\u64cd\u4f5c\u7ed3\u5408\uff0c\u63d0\u5347\u51c6\u786e\u6027\u5e76\u5e26\u6765\u5927\u5e45\u52a0\u901f\uff0c\u5145\u5206\u5229\u7528\u4e8c\u8fdb\u5236\u5411\u91cf\u7684\u9ad8\u6548\u8868\u793a\u3002", "method": "\u63d0\u51faHDGC\u7b97\u6cd5\uff0c\u5c06\u56fe\u5377\u79ef\u4e0e\u7ed1\u5b9a/\u6253\u5305\u7b49\u8d85\u7ef4\u8ba1\u7b97\u64cd\u4f5c\u8026\u5408\uff0c\u9002\u7528\u4e8e\u4f20\u5bfc\u5f0f\u56fe\u5b66\u4e60\u3002\u4ee5\u540c\u8d28\u6027\u4e0e\u5f02\u8d28\u6027\u56fe\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\u5408\uff0c\u6bd4\u8f83\u4e0e\u4e3b\u6d41GNN\u548cHD\u5b9e\u73b0\u7684\u51c6\u786e\u6027\u4e0e\u901f\u5ea6\uff0c\u5f3a\u8c03\u5728\u540c\u4e00GPU\u5e73\u53f0\u4e0a\u7684\u5bf9\u6bd4\u3002", "result": "\u5728\u591a\u7c7b\u56fe\u4e0a\uff0cHDGC\u5728\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u4e3b\u6d41GNN\u4e0e\u6700\u5148\u8fdbHD\u5b9e\u73b0\uff1b\u5728\u540c\u4e00\u76ee\u6807GPU\u5e73\u53f0\uff0cHDGC\u5bf9GCNII\u5e73\u5747\u5feb\u4e86\u7ea69561x\uff0c\u5bf9HDGL\u5feb\u7ea6144.5x\uff1b\u5927\u90e8\u5206\u5b66\u4e60\u5728\u4e8c\u8fdb\u5236\u5411\u91cf\u4e0a\u8fdb\u884c\uff0c\u9884\u8ba1\u5728\u795e\u7ecf\u5f62\u6001\u4e0e\u5185\u5b58\u8ba1\u7b97\u8bbe\u5907\u4e0a\u5177\u6709\u51fa\u8272\u7684\u80fd\u6548\u3002", "conclusion": "HDGC\u7684\u7ed3\u5408\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u4e0e\u6781\u9ad8\u901f\u5ea6\u7684\u6f5c\u529b\uff0c\u4e14\u5bf9\u4e8c\u8fdb\u5236\u5411\u91cf\u53cb\u597d\uff0c\u5177\u6709\u5728\u4e13\u4e1a\u786c\u4ef6\u4e0a\u7684\u9ad8\u6548\u5b9e\u73b0\u524d\u666f\uff1b\u672a\u6765\u5de5\u4f5c\u53ef\u805a\u7126\u786c\u4ef6\u52a0\u901f\u548c\u5728\u66f4\u5927\u89c4\u6a21\u56fe\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.24626", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24626", "abs": "https://arxiv.org/abs/2510.24626", "authors": ["William Held", "David Hall", "Percy Liang", "Diyi Yang"], "title": "Relative Scaling Laws for LLMs", "comment": null, "summary": "Scaling laws describe how language models improve with additional data,\nparameters, and compute. While widely used, they are typically measured on\naggregate test sets. Aggregate evaluations yield clean trends but average over\nheterogeneous subpopulations, obscuring performance disparities. We introduce\nrelative scaling laws, which track how performance gaps between test\ndistributions evolve with scale rather than focusing solely on absolute error.\nUsing 255 decoder-only Transformers trained under matched-compute (IsoFLOP)\nbudgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we\nfind diverse trajectories: academic domains on MMLU converge toward parity;\nregional English dialects shift depending on population size; and clusters of\nAI risk behaviours split, with capability- and influence-related risks\nincreasing during pretraining while adversarial risks do not. These results\nshow that although scaling improves overall performance, it is not a universal\nequalizer. To support further study, we release all model checkpoints from this\nwork to enable practitioners to measure relative alongside traditional scaling\nlaws, in order to better prioritize robustness challenges in light of the\nbitter lesson.", "AI": {"tldr": "\u76f8\u5bf9\u7f29\u653e\u5b9a\u5f8b\u63ed\u793a\u6a21\u578b\u5c3a\u5ea6\u589e\u5927\u65f6\uff0c\u4e0d\u540c\u5b50\u5206\u5e03\u4e4b\u95f4\u7684\u8868\u73b0\u5dee\u8ddd\u968f\u89c4\u6a21\u7684\u53d8\u5316\u8d8b\u52bf\uff1b\u7f29\u653e\u63d0\u5347\u603b\u4f53\u6027\u80fd\uff0c\u4f46\u5e76\u975e\u666e\u904d\u7684\u2018\u5747\u8861\u5668\u2019\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7f29\u653e\u89c4\u5f8b\uff08\u805a\u5408\u8bc4\u4f30\uff09\u63a9\u76d6\u7684\u5b50\u7fa4\u4f53\u5dee\u5f02\u95ee\u9898\uff0c\u63ed\u793a\u5728\u4e0d\u540c\u5206\u5e03\u4e0b\u89c4\u6a21\u5316\u5bf9\u9c81\u68d2\u6027\u4e0e\u98ce\u9669\u7684\u5f71\u54cd\uff0c\u4ee5\u66f4\u597d\u5730\u6307\u5357\u9488\u9488\u5bf9\u6027\u63d0\u5347\u3002", "method": "\u5728\u7b49\u8ba1\u7b97\u9884\u7b97\uff08IsoFLOP\uff09\u4e0b\uff0c\u8bad\u7ec3255\u4e2a\u89e3\u7801\u5668\u5f0f Transformer\uff0c\u89c4\u6a21\u8986\u76d61e18\u81f31e20 FLOPs\uff0c\u4f7f\u7528\u6807\u51c6\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff1b\u901a\u8fc7\u76f8\u5bf9\u7f29\u653e\u5f8b\u8ddf\u8e2a\u6d4b\u8bd5\u5206\u5e03\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u968f\u89c4\u6a21\u7684\u6f14\u53d8\uff0c\u8986\u76d6\u5b66\u672f\u9886\u57df\uff08MMLU\uff09\u3001\u533a\u57df\u82f1\u8bed\u65b9\u8a00\u3001\u4ee5\u53caAI\u98ce\u9669\u884c\u4e3a\u7684\u805a\u7c7b\u7b49\u5b50\u5206\u5e03\u3002", "result": "\u5f97\u5230\u591a\u6837\u5316\u8f68\u8ff9\uff1a\u5b66\u672f\u9886\u57dfMMLU\u8d8b\u5411\u5e73\u8861\uff1b\u533a\u57df\u82f1\u8bed\u65b9\u8a00\u7684\u5dee\u8ddd\u968f\u6bcd\u4f53\u89c4\u6a21\u53d8\u5316\u800c\u6539\u53d8\uff1bAI\u98ce\u9669\u884c\u4e3a\u5f62\u6210\u7c07\u72b6\u5206\u5316\uff0c\u5176\u4e2d\u80fd\u529b\u4e0e\u5f71\u54cd\u76f8\u5173\u7684\u98ce\u9669\u968f\u9884\u8bad\u7ec3\u9636\u6bb5\u589e\u52a0\u800c\u63d0\u5347\uff0c\u800c\u5bf9\u6297\u6027\u98ce\u9669\u672a\u663e\u8457\u4e0a\u5347\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u603b\u4f53\u6027\u80fd\u63d0\u5347\uff0c\u76f8\u5bf9\u7f29\u653e\u5f8b\u663e\u793a\u7f29\u653e\u5e76\u975e\u666e\u904d\u7684\u201c\u5747\u8861\u5668\u201d\uff1b\u7814\u7a76\u8005\u516c\u5f00\u4e86\u6240\u6709\u68c0\u67e5\u70b9\u4ee5\u4fbf\u5e76\u884c\u6d4b\u91cf\u76f8\u5bf9\u4e0e\u4f20\u7edf\u7f29\u653e\u89c4\u5f8b\u3002", "conclusion": "\u7f29\u653e\u53ef\u4ee5\u63d0\u5347\u603b\u4f53\u6027\u80fd\u4f46\u4e0d\u81ea\u52a8\u6d88\u9664\u5206\u5e03\u95f4\u5dee\u5f02\uff1b\u5f15\u5165\u76f8\u5bf9\u7f29\u653e\u5f8b\u4f5c\u4e3a\u9c81\u68d2\u6027\u5206\u6790\u7684\u91cd\u8981\u5de5\u5177\uff0c\u53ef\u5728\u6269\u5c55\u89c4\u6a21\u7684\u540c\u65f6\u66f4\u597d\u5730\u8bc6\u522b\u5e76\u7f13\u89e3\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2510.24628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24628", "abs": "https://arxiv.org/abs/2510.24628", "authors": ["Anh Ngo", "Nicolas Rollet", "Catherine Pelachaud", "Chloe Clavel"], "title": "\"Mm, Wat?\" Detecting Other-initiated Repair Requests in Dialogue", "comment": "9 pages", "summary": "Maintaining mutual understanding is a key component in human-human\nconversation to avoid conversation breakdowns, in which repair, particularly\nOther-Initiated Repair (OIR, when one speaker signals trouble and prompts the\nother to resolve), plays a vital role. However, Conversational Agents (CAs)\nstill fail to recognize user repair initiation, leading to breakdowns or\ndisengagement. This work proposes a multimodal model to automatically detect\nrepair initiation in Dutch dialogues by integrating linguistic and prosodic\nfeatures grounded in Conversation Analysis. The results show that prosodic cues\ncomplement linguistic features and significantly improve the results of\npretrained text and audio embeddings, offering insights into how different\nfeatures interact. Future directions include incorporating visual cues,\nexploring multilingual and cross-context corpora to assess the robustness and\ngeneralizability.", "AI": {"tldr": "\u5bf9\u8377\u5170\u5bf9\u8bdd\u4e2d\u7684\u4fee\u590d\u542f\u52a8\u8fdb\u884c\u591a\u6a21\u6001\u68c0\u6d4b\uff0c\u901a\u8fc7\u6574\u5408\u8bed\u8a00\u4e0e\u97f5\u5f8b\u7279\u5f81\uff0c\u5728\u4f1a\u8bdd\u5206\u6790\u6846\u67b6\u4e0b\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u5728\u4eba\u673a\u5bf9\u8bdd\u4e2d\uff0c\u4fdd\u6301\u4e92\u90fd\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4fee\u590d\u542f\u52a8\u662f\u9884\u9632\u5bf9\u8bdd\u4e2d\u65ad\u7684\u5173\u952e\u673a\u5236\uff1b\u73b0\u6709\u5bf9\u8bdd\u7cfb\u7edf\u96be\u4ee5\u8bc6\u522b\u7528\u6237\u53d1\u8d77\u7684\u4fee\u590d\uff0c\u5bfc\u81f4\u5bf9\u8bdd\u5931\u8d25\u6216\u8131\u79bb\uff1b\u9700\u8981\u5229\u7528\u8bed\u8a00\u53ca\u58f0\u97f3\u7ebf\u7d22\u6765\u68c0\u6d4b\u4fee\u590d\u610f\u56fe\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5c06\u8bed\u8a00\u7279\u5f81\uff08\u6587\u672c\u3001\u9884\u8bad\u7ec3\u5d4c\u5165\uff09\u4e0e\u97f5\u5f8b\u7279\u5f81\uff08\u97f3\u9891\uff09\u7ed3\u5408\uff0c\u57fa\u4e8e\u4f1a\u8bdd\u5206\u6790\u6846\u67b6\u8fdb\u884c\u7279\u5f81\u8bbe\u8ba1\u4e0e\u878d\u5408\uff0c\u4ee5\u8377\u5170\u8bed\u5bf9\u8bdd\u6570\u636e\u4e3a\u8bc4\u4f30\u5bf9\u8c61\u3002", "result": "\u97f5\u5f8b\u7ebf\u7d22\u80fd\u8865\u5145\u8bed\u8a00\u7279\u5f81\uff0c\u4e0e\u6587\u672c/\u97f3\u9891\u7684\u9884\u8bad\u7ec3\u5d4c\u5165\u7ed3\u5408\u540e\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u63ed\u793a\u4e0d\u540c\u7279\u5f81\u7684\u4ea4\u4e92\u4f5c\u7528\uff1b\u5bf9\u6bd4\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u6539\u5584\u4e86\u4fee\u590d\u542f\u52a8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8bc1\u5b9e\u4e86\u591a\u6a21\u6001\u7ebf\u7d22\u5728\u4fee\u590d\u542f\u52a8\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u65b9\u5411\uff0c\u5982\u5f15\u5165\u89c6\u542c\u4fe1\u606f\u3001\u8fdb\u884c\u591a\u8bed\u79cd\u548c\u8de8\u573a\u666f\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u4ee5\u63d0\u5347\u6cdb\u5316\u6027\u3002"}}
{"id": "2510.23992", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23992", "abs": "https://arxiv.org/abs/2510.23992", "authors": ["Yuxiao Wen", "Yanjun Han", "Zhengyuan Zhou"], "title": "Optimal Arm Elimination Algorithms for Combinatorial Bandits", "comment": null, "summary": "Combinatorial bandits extend the classical bandit framework to settings where\nthe learner selects multiple arms in each round, motivated by applications such\nas online recommendation and assortment optimization. While extensions of upper\nconfidence bound (UCB) algorithms arise naturally in this context, adapting arm\nelimination methods has proved more challenging. We introduce a novel\nelimination scheme that partitions arms into three categories (confirmed,\nactive, and eliminated), and incorporates explicit exploration to update these\nsets. We demonstrate the efficacy of our algorithm in two settings: the\ncombinatorial multi-armed bandit with general graph feedback, and the\ncombinatorial linear contextual bandit. In both cases, our approach achieves\nnear-optimal regret, whereas UCB-based methods can provably fail due to\ninsufficient explicit exploration. Matching lower bounds are also provided.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e09\u96c6\u5408\u6d88\u9664\u6846\u67b6\uff08\u786e\u8ba4/\u6d3b\u8dc3/\u6dd8\u6c70\uff09\u7528\u4e8e\u7ec4\u5408\u81c2\u5e26\u6765\u591a\u81c2\u573a\u666f\uff0c\u5f15\u5165\u663e\u5f0f\u63a2\u7d22\u4ee5\u66f4\u65b0\u96c6\u5408\uff0c\u83b7\u5f97\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\u754c\uff1b\u5728\u5e26\u56fe\u53cd\u9988\u7684\u7ec4\u5408\u81c2\u548c\u7ec4\u5408\u7ebf\u6027\u4e0a\u4e0b\u6587\u60c5\u5883\u4e0b\u5747\u6210\u7acb\uff0c\u5e76\u7ed9\u51fa\u5339\u914d\u4e0b\u754c\u3002", "motivation": "\u5728\u7ec4\u5408\u81c2\u95ee\u9898\u4e2d\uff0c learner \u9700\u5728\u6bcf\u8f6e\u540c\u65f6\u9009\u53d6\u591a\u81c2\uff0c\u7b80\u5355\u7684 UCB \u6269\u5c55\u5e38\u5e38\u9762\u4e34\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u63d0\u51fa\u65b0\u7684\u6d88\u9664\u7b56\u7565\u4ee5\u66f4\u6709\u6548\u5730\u9a71\u52a8\u63a2\u7d22\u4e0e\u6d88\u9664\uff0c\u4ee5\u8fbe\u5230\u7406\u8bba\u4e0a\u7684\u5f3a\u4fdd\u754c\u3002", "method": "\u5c06\u81c2\u5212\u5206\u4e3a\u4e09\u7c7b\u96c6\u5408\uff08confirmed/active/eliminated\uff09\uff0c\u901a\u8fc7\u663e\u5f0f\u63a2\u7d22\u66f4\u65b0\u8fd9\u4e09\u7c7b\u96c6\u5408\uff1b\u5728\u4e24\u7c7b\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\uff1a1) \u7ec4\u5408\u591a\u81c2\u5e26 generally graph feedback\uff1b2) \u7ec4\u5408\u7ebf\u6027\u4e0a\u4e0b\u6587\u5e26\u3002\u901a\u8fc7\u5206\u6790\u7ed9\u51fa\u9057\u61be\u754c\uff0c\u5e76\u5f3a\u8c03\u82e5\u4ec5\u7528 UCB \u800c\u7f3a\u4e4f\u663e\u5f0f\u63a2\u7d22\u5c06\u4f1a\u5931\u8d25\u3002", "result": "\u7ed9\u51fa\u8fd1\u6700\u4f18\u9057\u61be\u754c\uff0c\u5e76\u7ed9\u51fa\u4e0e\u4e0b\u754c\u5339\u914d\u7684\u7ed3\u679c\uff1b\u4e24\u79cd\u8bbe\u7f6e\u5747\u8bc1\u5b9e\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u4f9b\u4e00\u79cd\u9c81\u68d2\u7684\u66ff\u4ee3 UCB \u7684\u7ec4\u5408\u6d88\u9664\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5e26\u56fe\u53cd\u9988\u7684\u7ec4\u5408\u81c2\u4e0e\u7ec4\u5408\u7ebf\u6027\u4e0a\u4e0b\u6587\u60c5\u5883\uff0c\u5f25\u8865\u4f20\u7edf\u6d88\u9664\u7b56\u7565\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.24636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24636", "abs": "https://arxiv.org/abs/2510.24636", "authors": ["Ziyou Hu", "Zhengliang Shi", "Minghang Zhu", "Haitao Li", "Teng Sun", "Pengjie Ren", "Suzan Verberne", "Zhaochun Ren"], "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning", "comment": null, "summary": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.", "AI": {"tldr": "OpenRM\uff1a\u4e00\u79cd\u9762\u5411\u5f00\u653e\u6027\u56de\u7b54\u7684\u5de5\u5177\u589e\u5f3a\u578b\u957f\u6587\u672c\u5956\u8d4f\u6a21\u578b\uff0c\u901a\u8fc7\u8c03\u7528\u5916\u90e8\u5de5\u5177\u83b7\u53d6\u8bc1\u636e\u8fdb\u884c\u5224\u65ad\uff0c\u5728\u77e5\u8bc6\u5bc6\u96c6\u548c\u957f\u6587\u4efb\u52a1\u4e2d\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u9700\u8981\u5916\u90e8\u8bc1\u636e\u6765\u9a8c\u8bc1\u6b63\u786e\u6027\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u96be\u4ee5\u5206\u8fa8\u957f\u6587\u672c\u4e2d\u7684\u5fae\u5c0f\u8d28\u91cf\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5bf9\u77e5\u8bc6\u5bc6\u96c6\u4efb\u52a1\u7684\u53ef\u9760\u8bc4\u4f30\u4e0e\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5bf927K\u5bf9\u6bd4\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u51fa\u53ef\u63a7\u6570\u636e\u5408\u6210\u6846\u67b6\u751f\u6210\u5bf9\u6bd4\u6570\u636e\uff1b\u5956\u8d4f\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u540c\u65f6\u76d1\u7763\u4e2d\u95f4\u5de5\u5177\u4f7f\u7528\u548c\u6700\u7ec8\u7ed3\u679c\u6b63\u786e\u6027\uff0c\u5b66\u4e60\u8bc1\u636e\u9a71\u52a8\u7684\u5224\u5b9a\u7b56\u7565\u3002", "result": "\u5728\u4e09\u7ec4\u65b0\u6570\u636e\u96c6\u548c\u4e24\u7ec4\u5e38\u7528\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5956\u52b1\u6a21\u578b\uff1b\u5728\u63a8\u7406\u9636\u6bb5\u7684\u54cd\u5e94\u9009\u62e9\u548c\u8bad\u7ec3\u9636\u6bb5\u7684\u6570\u636e\u9009\u62e9\u4efb\u52a1\u4e2d\u5747\u63d0\u5347\u4e86\u4e0b\u6e38\u5bf9\u9f50\u6027\u80fd\u3002", "conclusion": "\u5de5\u5177\u9a71\u52a8\u7684\u8bc1\u636e\u5316\u8bc4\u4f30\u6709\u671b\u63d0\u5347\u957f\u6587\u4efb\u52a1\u7684\u8bc4\u4f30\u53ef\u9760\u6027\u4e0e\u6269\u5c55\u6027\uff0c\u4ece\u800c\u63a8\u52a8\u5927\u6a21\u578b\u5bf9\u9f50\u7814\u7a76\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2510.23994", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23994", "abs": "https://arxiv.org/abs/2510.23994", "authors": ["Geoffery Agorku", "Sarah Hernandez", "Hayley Hames", "Cade Wagner"], "title": "Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory Derived Features: Proof of Concept", "comment": null, "summary": "Accurate, real-time estimation of barge quantity on inland waterways remains\na critical challenge due to the non-self-propelled nature of barges and the\nlimitations of existing monitoring systems. This study introduces a novel\nmethod to use Automatic Identification System (AIS) vessel tracking data to\npredict the number of barges in tow using Machine Learning (ML). To train and\ntest the model, barge instances were manually annotated from satellite scenes\nacross the Lower Mississippi River. Labeled images were matched to AIS vessel\ntracks using a spatiotemporal matching procedure. A comprehensive set of 30\nAIS-derived features capturing vessel geometry, dynamic movement, and\ntrajectory patterns were created and evaluated using Recursive Feature\nElimination (RFE) to identify the most predictive variables. Six regression\nmodels, including ensemble, kernel-based, and generalized linear approaches,\nwere trained and evaluated. The Poisson Regressor model yielded the best\nperformance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of\nthe 30 features. The feature importance analysis revealed that metrics\ncapturing vessel maneuverability such as course entropy, speed variability and\ntrip length were most predictive of barge count. The proposed approach provides\na scalable, readily implementable method for enhancing Maritime Domain\nAwareness (MDA), with strong potential applications in lock scheduling, port\nmanagement, and freight planning. Future work will expand the proof of concept\npresented here to explore model transferability to other inland rivers with\ndiffering operational and environmental conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e AIS \u6570\u636e\u7684\u5b9e\u65f6\u8239\u961f\u6570\u91cf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u5728\u822a\u9053\u4e2d\u7684\u9a73\u8239\u6570\u91cf\u3002Poisson \u56de\u5f52\u5668\u8868\u73b0\u6700\u4f73\uff0cMAE \u4e3a 1.92 \u8258\uff0c\u4f7f\u7528 12 \u4e2a\u7279\u5f81\uff1b\u7279\u5f81\u91cd\u8981\u6027\u663e\u793a\u822a\u5411\u6df7\u4e71\u5ea6\u3001\u901f\u5ea6\u6ce2\u52a8\u548c\u822a\u7a0b\u5bf9\u9a73\u8239\u6570\u91cf\u9884\u6d4b\u6700\u5177\u4fe1\u606f\u91cf\u3002", "motivation": "\u5b9e\u65f6\u3001\u51c6\u786e\u5730\u4f30\u7b97\u5185\u9646\u6c34\u9053\u7684\u9a73\u8239\u6570\u91cf\u4ecd\u5177\u6311\u6218\u6027\uff0c\u96be\u4ee5\u4f9d\u8d56\u73b0\u6709\u76d1\u6d4b\u7cfb\u7edf\uff1b\u9a73\u8239\u975e\u81ea propulsion \u7279\u6027\u53ca\u73b0\u6709\u76d1\u63a7\u624b\u6bb5\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u5f00\u53d1\u57fa\u4e8e AIS \u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u6d77\u4e8b\u9886\u57df\u6001\u52bf\u611f\u77e5\uff08MDA\uff09\u3002", "method": "\u901a\u8fc7\u536b\u661f\u573a\u666f\u5bf9 Lower Mississippi River \u7684\u9a73\u8239\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u5c06\u6807\u6ce8\u56fe\u50cf\u4e0e AIS \u822a\u8ff9\u8fdb\u884c\u65f6\u7a7a\u5339\u914d\uff1b\u751f\u6210 30 \u4e2a AIS \u6d3e\u751f\u7279\u5f81\uff0c\u6db5\u76d6\u8239\u8236\u51e0\u4f55\u3001\u52a8\u529b\u5b66\u4e0e\u8f68\u8ff9\u7279\u5f81\uff1b\u4f7f\u7528\u9012\u5f52\u7279\u5f81\u6d88\u9664\uff08RFE\uff09\u7b5b\u9009\u6700\u5177\u9884\u6d4b\u529b\u7684\u53d8\u91cf\uff1b\u8bad\u7ec3\u516d\u79cd\u56de\u5f52\u6a21\u578b\uff08\u96c6\u6210\u3001\u6838\u65b9\u6cd5\u3001\u5e7f\u4e49\u7ebf\u6027\u7b49\uff09\uff0c\u5e76\u8bc4\u4f30\u6027\u80fd\uff1b\u6700\u7ec8\u9009\u62e9 Poisson \u56de\u5f52\u5668\u4f5c\u4e3a\u6700\u4f73\u6a21\u578b\u3002", "result": "\u5728 12 \u4e2a\u7279\u5f81\u6761\u4ef6\u4e0b\uff0cPoisson \u56de\u5f52\u5668\u5b9e\u73b0 MAE 1.92 \u7684\u9884\u6d4b\u8bef\u5dee\uff1b\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u663e\u793a\u822a\u5411 entropy\u3001\u901f\u5ea6\u6ce2\u52a8\u53ca\u884c\u7a0b\u957f\u5ea6\u7b49\u53ef\u63cf\u8ff0\u8239\u8236\u673a\u52a8\u6027\u7684\u6307\u6807\u6700\u5177\u9884\u6d4b\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u4e14\u6613\u4e8e\u5728\u6d77\u4e8b\u57df\u6001\u52bf\u611f\u77e5\u4e2d\u843d\u5730\uff0c\u53ef\u5e94\u7528\u4e8e\u8239\u95f8\u8c03\u5ea6\u3001\u6e2f\u53e3\u7ba1\u7406\u548c\u8d27\u8fd0\u89c4\u5212\u7b49\u573a\u666f\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u63a2\u7d22\u5c06\u6a21\u578b\u8fc1\u79fb\u5230\u5176\u4ed6\u5185\u9646\u6cb3\u9053\u4ee5\u9002\u5e94\u4e0d\u540c\u8fd0\u8425\u4e0e\u73af\u5883\u6761\u4ef6\u3002"}}
{"id": "2510.24647", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24647", "abs": "https://arxiv.org/abs/2510.24647", "authors": ["Hugo Rydel-Johnston", "Alex Kafkas"], "title": "Quantifying the Effects of Word Length, Frequency, and Predictability on Dyslexia", "comment": null, "summary": "We ask where, and under what conditions, dyslexic reading costs arise in a\nlarge-scale naturalistic reading dataset. Using eye-tracking aligned to\nword-level features (word length, frequency, and predictability), we model how\neach feature influences dyslexic time costs. We find that all three features\nrobustly change reading times in both typical and dyslexic readers, and that\ndyslexic readers show stronger sensitivities to each, especially\npredictability. Counterfactual manipulations of these features substantially\nnarrow the dyslexic-control gap by about one third, with predictability showing\nthe strongest effect, followed by length and frequency. These patterns align\nwith dyslexia theories that posit heightened demands on linguistic working\nmemory and phonological encoding, and they motivate further work on lexical\ncomplexity and parafoveal preview benefits to explain the remaining gap. In\nshort, we quantify when extra dyslexic costs arise, how large they are, and\noffer actionable guidance for interventions and computational models for\ndyslexics.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u5927\u89c4\u6a21\u81ea\u7136\u9605\u8bfb\u6570\u636e\u96c6\u4e2d\u91cf\u5316\u4e86\u5728\u4f55\u65f6\u3001\u4ee5\u4f55\u79cd\u6761\u4ef6\u4e0b\uff0c\u9605\u8bfb\u6210\u672c\u5728\u9605\u8bfb\u969c\u788d\u4eba\u7fa4\u4e2d\u7684\u51fa\u73b0\uff0c\u4ee5\u53ca\u4e09\u5927\u8bcd\u6c47\u7279\u5f81\uff08\u8bcd\u957f\u3001\u8bcd\u9891\u3001\u53ef\u9884\u6d4b\u6027\uff09\u5bf9\u9605\u8bfb\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e09\u8005\u5747\u663e\u8457\u5f71\u54cd\u9605\u8bfb\u65f6\u95f4\uff0c\u4e14\u9605\u8bfb\u969c\u788d\u8005\u5bf9\u5404\u7279\u5f81\u66f4\u654f\u611f\uff0c\u5c24\u5176\u662f\u53ef\u9884\u6d4b\u6027\u3002\u901a\u8fc7\u5bf9\u8fd9\u4e9b\u7279\u5f81\u7684\u53cd\u4e8b\u5b9e\u64cd\u63a7\uff0c\u53ef\u5c06\u969c\u788d\u7fa4\u4f53\u4e0e\u5bf9\u7167\u7ec4\u7684\u5dee\u8ddd\u7f29\u5c0f\u7ea6\u4e09\u5206\u4e4b\u4e00\uff0c\u4e14\u53ef\u9884\u6d4b\u6027\u8d21\u732e\u6700\u5927\uff0c\u5176\u6b21\u662f\u957f\u5ea6\u548c\u9891\u7387\u3002\u7814\u7a76\u4e0e\u65e8\u5728\u652f\u6301\u5de5\u4f5c\u8bb0\u5fc6\u4e0e\u97f3\u4f4d\u7f16\u7801\u7406\u8bba\u7684 Dyslexia \u7406\u8bba\u76f8\u4e00\u81f4\uff0c\u5e76\u63d0\u793a\u5728\u8bcd\u6c47\u590d\u6742\u5ea6\u4e0e\u65c1\u89c6\u9884\u89c8\uff08parafoveal preview\uff09\u7b49\u65b9\u9762\u7684\u540e\u7eed\u7814\u7a76\u3002", "motivation": "\u63ed\u793a\u5927\u89c4\u6a21\u81ea\u7136\u9605\u8bfb\u4efb\u52a1\u4e2d\uff0c\u54ea\u4e9b\u6761\u4ef6\u548c\u7279\u5f81\u4f1a\u653e\u5927\u6216\u51cf\u8f7b\u9605\u8bfb\u969c\u788d\u4eba\u58eb\u7684\u989d\u5916\u6210\u672c\uff1b\u5b9a\u91cf\u5206\u89e3\u8bcd\u6c47\u7279\u5f81\u5bf9 Dyslexia \u9605\u8bfb\u6210\u672c\u7684\u8d21\u732e\uff0c\u4ece\u800c\u4e3a\u5e72\u9884\u548c\u7406\u8bba\u6a21\u578b\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u4fe1\u606f\u3002", "method": "\u5728\u5927\u89c4\u6a21\u81ea\u7136\u9605\u8bfb\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u773c\u52a8\u8ffd\u8e2a\u5e76\u5bf9\u9f50\u5230\u9010\u8bcd\u7279\u5f81\uff08\u8bcd\u957f\u3001\u8bcd\u9891\u3001\u53ef\u9884\u6d4b\u6027\uff09\uff0c\u5efa\u7acb\u6a21\u578b\u4ee5\u8bc4\u4f30\u5404\u7279\u5f81\u5bf9 Dyslexic \u65f6\u95f4\u6210\u672c\u7684\u4f5c\u7528\uff1b\u901a\u8fc7\u5bf9\u8fd9\u4e9b\u7279\u5f81\u8fdb\u884c\u53cd\u4e8b\u5b9e\u64cd\u63a7\uff0c\u8bc4\u4f30\u5bf9\u7167\u7ec4\u4e0e Dyslexic \u7684\u5dee\u8ddd\u53d8\u5316\u3002", "result": "\u4e09\u5927\u7279\u5f81\uff08\u8bcd\u957f\u3001\u8bcd\u9891\u3001\u53ef\u9884\u6d4b\u6027\uff09\u5747\u5bf9\u5178\u578b\u8bfb\u8005\u548c Dyslexic \u8bfb\u8005\u7684\u9605\u8bfb\u65f6\u95f4\u5177\u6709\u7a33\u5065\u5f71\u54cd\uff0c\u4e14 Dyslexic \u5bf9\u8fd9\u4e9b\u7279\u5f81\u7684\u654f\u611f\u6027\u66f4\u5f3a\uff0c\u7279\u522b\u662f\u53ef\u9884\u6d4b\u6027\u3002\u5bf9\u7279\u5f81\u7684\u53cd\u4e8b\u5b9e\u64cd\u63a7\u5c06 Dyslexic-\u63a7\u5236\u5dee\u8ddd\u663e\u8457\u7f29\u5c0f\u7ea6\u4e09\u5206\u4e4b\u4e00\uff0c\u5176\u4e2d\u53ef\u9884\u6d4b\u6027\u8d21\u732e\u6700\u5927\uff0c\u5176\u6b21\u662f\u8bcd\u957f\uff0c\u968f\u540e\u662f\u8bcd\u9891\u3002\u7ed3\u679c\u4e0e\u5021\u5bfc\u7684\u7406\u8bba\uff08\u8bed\u8a00\u5de5\u4f5c\u8bb0\u5fc6\u548c\u8bed\u97f3\u7f16\u7801\u7684\u66f4\u9ad8\u9700\u6c42\uff09\u76f8\u4e00\u81f4\uff0c\u5e76\u6307\u5411\u5728\u8bcd\u6c47\u590d\u6742\u6027\u548c\u65c1\u6ce8\u9884\u89c8\u65b9\u9762\u7684\u540e\u7eed\u7814\u7a76\u4ee5\u89e3\u91ca\u5269\u4f59\u5dee\u8ddd\u3002", "conclusion": "\u7ed3\u679c\u652f\u6301\u5bf9 Dyslexia \u7684\u8bed\u8a00\u5de5\u4f5c\u8bb0\u5fc6\u4e0e\u97f3\u4f4d\u7f16\u7801\u8d1f\u62c5\u7684\u7406\u8bba\uff0c\u5e76\u4e3a\u5e72\u9884\u7b56\u7565\u548c\u8ba1\u7b97\u6a21\u578b\u63d0\u4f9b\u57fa\u4e8e\u53ef\u63a7\u8bcd\u6c47\u7279\u5f81\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u63d0\u793a\u901a\u8fc7\u8c03\u6574\u8bcd\u6c47\u96be\u5ea6\u548c\u63d0\u4f9b\u66f4\u597d\u7684\u9884\u6d4b\u6027\u4fe1\u606f\u53ef\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u9605\u8bfb\u6210\u672c\u3002"}}
{"id": "2510.24012", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24012", "abs": "https://arxiv.org/abs/2510.24012", "authors": ["Byeonghu Na", "Mina Kang", "Jiseok Kwak", "Minsang Park", "Jiwoo Shin", "SeJoon Jun", "Gayoung Lee", "Jin-Hwa Kim", "Il-Chul Moon"], "title": "Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models", "comment": "Accepted at NeurIPS 2025", "summary": "Text-to-image models have recently made significant advances in generating\nrealistic and semantically coherent images, driven by advanced diffusion models\nand large-scale web-crawled datasets. However, these datasets often contain\ninappropriate or biased content, raising concerns about the generation of\nharmful outputs when provided with malicious text prompts. We propose Safe Text\nembedding Guidance (STG), a training-free approach to improve the safety of\ndiffusion models by guiding the text embeddings during sampling. STG adjusts\nthe text embeddings based on a safety function evaluated on the expected final\ndenoised image, allowing the model to generate safer outputs without additional\ntraining. Theoretically, we show that STG aligns the underlying model\ndistribution with safety constraints, thereby achieving safer outputs while\nminimally affecting generation quality. Experiments on various safety\nscenarios, including nudity, violence, and artist-style removal, show that STG\nconsistently outperforms both training-based and training-free baselines in\nremoving unsafe content while preserving the core semantic intent of input\nprompts. Our code is available at https://github.com/aailab-kaist/STG.", "AI": {"tldr": "STG\u901a\u8fc7\u5728\u91c7\u6837\u9636\u6bb5\u5bf9\u6587\u672c\u5d4c\u5165\u8fdb\u884c\u5b89\u5168\u5f15\u5bfc\uff0c\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\uff0c\u8c03\u6574\u6a21\u578b\u5bf9\u8f93\u5165\u6587\u672c\u7684\u7406\u89e3\uff0c\u4ee5\u751f\u6210\u66f4\u5b89\u5168\u7684\u8f93\u51fa\uff0c\u540c\u65f6\u5c3d\u91cf\u4fdd\u7559\u539f\u59cb\u8bed\u4e49\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u5f80\u5f80\u5305\u542b\u4e0d\u5f53\u6216\u5e26\u504f\u89c1\u7684\u5185\u5bb9\uff0c\u5bb9\u6613\u5728\u6076\u610f\u63d0\u793a\u4e0b\u751f\u6210\u6709\u5bb3\u8f93\u51fa\u3002\u9700\u8981\u4e00\u79cd\u65e0\u8bad\u7ec3\u6210\u672c\u7684\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5bf9\u6587\u672c\u5d4c\u5165\u5e94\u7528\u4e00\u4e2a\u5b89\u5168\u51fd\u6570\uff0c\u5bf9\u9884\u671f\u7684\u6700\u7ec8\u53bb\u566a\u56fe\u50cf\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ee5\u6b64\u8c03\u6574\u6587\u672c\u5d4c\u5165\u7684\u65b9\u5411\uff0c\u4ece\u800c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u9075\u5faa\u5b89\u5168\u7ea6\u675f\uff0c\u800c\u65e0\u9700\u5bf9\u6a21\u578b\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u88f8\u4f53\u3001\u66b4\u529b\u548c\u827a\u672f\u98ce\u683c\u79fb\u9664\u7b49\u5b89\u5168\u573a\u666f\u4e0b\uff0cSTG\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff08\u5305\u62ec\u8bad\u7ec3\u578b\u548c\u8bad\u7ec3\u65e0\u5173\u7684\u65b9\u6cd5\uff09\uff0c\u65e2\u80fd\u53bb\u9664\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u53c8\u5c3d\u91cf\u4fdd\u7559\u8f93\u5165\u63d0\u793a\u7684\u6838\u5fc3\u8bed\u4e49\u3002", "conclusion": "\u7406\u8bba\u4e0a\uff0cSTG\u5c06\u57fa\u7840\u6a21\u578b\u7684\u5206\u5e03\u4e0e\u5b89\u5168\u7ea6\u675f\u5bf9\u9f50\uff0c\u5728\u4e0d\u663e\u8457\u964d\u4f4e\u751f\u6210\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u5b89\u5168\u6027\uff1b\u4f5c\u4e3a\u65e0\u8bad\u7ec3\u6210\u672c\u7684\u5b89\u5168\u63a7\u5236\u624b\u6bb5\uff0cSTG\u5bf9\u591a\u79cd\u573a\u666f\u5177\u6709\u666e\u904d\u9002\u7528\u6027\u3002"}}
{"id": "2510.24664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24664", "abs": "https://arxiv.org/abs/2510.24664", "authors": ["Parker Riley", "Daniel Deutsch", "Mara Finkelstein", "Colten DiIanni", "Juraj Juraska", "Markus Freitag"], "title": "MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine Translation", "comment": null, "summary": "Human evaluation of machine translation is in an arms race with translation\nmodel quality: as our models get better, our evaluation methods need to be\nimproved to ensure that quality gains are not lost in evaluation noise. To this\nend, we experiment with a two-stage version of the current state-of-the-art\ntranslation evaluation paradigm (MQM), which we call MQM re-annotation. In this\nsetup, an MQM annotator reviews and edits a set of pre-existing MQM\nannotations, that may have come from themselves, another human annotator, or an\nautomatic MQM annotation system. We demonstrate that rater behavior in\nre-annotation aligns with our goals, and that re-annotation results in\nhigher-quality annotations, mostly due to finding errors that were missed\nduring the first pass.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e24\u9636\u6bb5 MQM \u91cd\u65b0\u6807\u6ce8\uff08MQM re-annotation\uff09\uff0c\u901a\u8fc7\u8ba9\u6807\u6ce8\u8005\u5bf9\u5df2\u6709 MQM \u6807\u6ce8\u8fdb\u884c\u5ba1\u9605\u548c\u4fee\u6539\uff0c\u6539\u5584\u6807\u6ce8\u8d28\u91cf\uff0c\u5e76\u53d1\u73b0\u7b2c\u4e00\u8f6e\u672a\u6355\u6349\u7684\u9519\u8bef\uff0c\u4ece\u800c\u51cf\u5c11\u8bc4\u4f30\u566a\u58f0\u3002", "motivation": "\u968f\u7740\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u8d28\u91cf\u63d0\u5347\uff0c\u73b0\u6709 MQM \u8bc4\u4f30\u65b9\u6cd5\u7684\u566a\u58f0\u53ef\u80fd\u63a9\u76d6\u771f\u6b63\u7684\u8d28\u91cf\u63d0\u5347\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u8bc4\u4f30\u6d41\u7a0b\u4ee5\u786e\u4fdd\u8bc4\u4f30\u4e0e\u6a21\u578b\u8d28\u91cf\u7684\u63d0\u5347\u540c\u6b65\uff1b\u901a\u8fc7\u4e24\u9636\u6bb5\u91cd\u6807\u6ce8\u6765\u63d0\u5347\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u548c\u53d1\u73b0\u529b\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e24\u9636\u6bb5 MQM \u91cd\u65b0\u6807\u6ce8\uff1a\u4e00\u4e2a MQM \u6807\u6ce8\u5458\u5bf9\u4e00\u7ec4\u9884\u5148\u5b58\u5728\u7684 MQM \u6807\u6ce8\u8fdb\u884c\u5ba1\u9605\u548c\u7f16\u8f91\uff08\u6807\u6ce8\u6765\u6e90\u5305\u62ec\u81ea\u8eab\u3001\u53e6\u4e00\u540d\u4eba\u5de5\u6807\u6ce8\u8005\u6216\u81ea\u52a8 MQM \u7cfb\u7edf\uff09\uff0c\u5e76\u5206\u6790\u91cd\u6807\u6ce8\u8fc7\u7a0b\u4e2d\u7684\u6807\u6ce8\u8005\u884c\u4e3a\u662f\u5426\u7b26\u5408\u8bc4\u4f30\u76ee\u6807\u3002\u8bc4\u4f30\u91cd\u6807\u6ce8\u662f\u5426\u63d0\u9ad8\u6807\u6ce8\u8d28\u91cf\u3001\u662f\u5426\u53d1\u73b0\u7b2c\u4e00\u8f6e\u672a\u68c0\u6d4b\u7684\u9519\u8bef\u3002", "result": "\u91cd\u6807\u6ce8\u7ed3\u679c\u663e\u793a\u6807\u6ce8\u8d28\u91cf\u66f4\u9ad8\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u53d1\u73b0\u4e86\u5728\u7b2c\u4e00\u8f6e\u4e2d\u9057\u6f0f\u7684\u9519\u8bef\uff1b\u6807\u6ce8\u8005\u884c\u4e3a\u4e0e\u8bc4\u4f30\u76ee\u6807\u8d8b\u4e8e\u4e00\u81f4\u3002", "conclusion": "\u4e24\u9636\u6bb5 MQM \u91cd\u65b0\u6807\u6ce8\u4e3a\u63d0\u9ad8\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u7684\u7a33\u5b9a\u6027\u4e0e\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u6a21\u578b\u3001\u7cfb\u7edf\u8d28\u91cf\u63d0\u5347\u65f6\u51cf\u5c11\u8bc4\u4f30\u566a\u58f0\u3002"}}
{"id": "2510.24027", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24027", "abs": "https://arxiv.org/abs/2510.24027", "authors": ["Zibo Liu", "Zhe Jiang", "Zelin Xu", "Tingsong Xiao", "Yupu Zhang", "Zhengkun Xiao", "Haibo Wang", "Shigang Chen"], "title": "Spatio-temporal Multivariate Time Series Forecast with Chosen Variables", "comment": "In submission", "summary": "Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series\nof $n$ spatially distributed variables in a period of recent past to forecast\ntheir values in a period of near future. It has important applications in\nspatio-temporal sensing forecast such as road traffic prediction and air\npollution prediction. Recent papers have addressed a practical problem of\nmissing variables in the model input, which arises in the sensing applications\nwhere the number $m$ of sensors is far less than the number $n$ of locations to\nbe monitored, due to budget constraints. We observe that the state of the art\nassumes that the $m$ variables (i.e., locations with sensors) in the model\ninput are pre-determined and the important problem of how to choose the $m$\nvariables in the input has never been studied. This paper fills the gap by\nstudying a new problem of STMF with chosen variables, which optimally selects\n$m$-out-of-$n$ variables for the model input in order to maximize the forecast\naccuracy. We propose a unified framework that jointly performs variable\nselection and model optimization for both forecast accuracy and model\nefficiency. It consists of three novel technical components: (1) masked\nvariable-parameter pruning, which progressively prunes less informative\nvariables and attention parameters through quantile-based masking; (2)\nprioritized variable-parameter replay, which replays low-loss past samples to\npreserve learned knowledge for model stability; (3) dynamic extrapolation\nmechanism, which propagates information from variables selected for the input\nto all other variables via learnable spatial embeddings and adjacency\ninformation. Experiments on five real-world datasets show that our work\nsignificantly outperforms the state-of-the-art baselines in both accuracy and\nefficiency, demonstrating the effectiveness of joint variable selection and\nmodel optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u65f6\u7a7a\u591a\u53d8\u91cf\u9884\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u7ed9\u5b9a\u89c2\u6d4b\u53d8\u91cf\u6570m\u7684\u524d\u63d0\u4e0b\u9009\u62e9m\u4e2a\u6700\u6709\u4fe1\u606f\u7684\u53d8\u91cf\u4ee5\u6700\u5927\u5316\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u4e09\u5927\u6280\u672f\u7ec4\u4ef6\u5b9e\u73b0\u53d8\u91cf\u9009\u62e9\u548c\u6a21\u578b\u4f18\u5316\uff0c\u4e14\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5728\u4f20\u611f\u5e94\u7528\u4e2d\u4f20\u611f\u5668\u6570\u91cf\u8fdc\u5c0f\u4e8e\u76d1\u6d4b\u53d8\u91cf\u6570\u91cf\u65f6\uff0c\u5982\u4f55\u5728\u8f93\u5165\u4e2d\u9009\u62e9\u54ea\u4e9b\u53d8\u91cf\u6210\u4e3a\u6838\u5fc3\u95ee\u9898\uff1b\u73b0\u6709\u5de5\u4f5c\u5047\u8bbe\u8f93\u5165\u53d8\u91cf\u5df2\u786e\u5b9a\uff0c\u672a\u7814\u7a76\u5982\u4f55\u9009\u62e9\uff1b\u9700\u8981\u8054\u5408\u53d8\u91cf\u9009\u62e9\u4e0e\u6a21\u578b\u4f18\u5316\u6765\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u4e0e\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e09\u5927\u6280\u672f\u7ec4\u4ef6\uff1a1) \u57fa\u4e8e\u5206\u4f4d\u6570\u63a9\u7801\u7684\u53d8\u91cf-\u53c2\u6570\u88c1\u526a\uff0c\u7528\u63a9\u7801\u9010\u6b65\u5254\u9664\u4e0d\u4fe1\u606f\u53d8\u91cf\u548c\u6ce8\u610f\u529b\u53c2\u6570\uff1b2) \u4f18\u5148\u7ea7\u53d8\u91cf-\u53c2\u6570\u56de\u653e\uff0c\u901a\u8fc7\u5bf9\u4f4e\u635f\u5931\u7684\u5386\u53f2\u6837\u672c\u91cd\u653e\u4ee5\u4fdd\u6301\u5b66\u4e60\u5e76\u63d0\u5347\u7a33\u5b9a\u6027\uff1b3) \u52a8\u6001\u5916\u63a8\u673a\u5236\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u5d4c\u5165\u548c\u90bb\u63a5\u4fe1\u606f\uff0c\u5c06\u9009\u5b9a\u53d8\u91cf\u7684\u4fe1\u606f\u4f20\u64ad\u5230\u6240\u6709\u53d8\u91cf\uff0c\u4ee5\u5b9e\u73b0\u8de8\u53d8\u91cf\u7684\u4fe1\u606f\u6574\u5408\u3002\u6846\u67b6\u540c\u65f6\u5b8c\u6210\u53d8\u91cf\u9009\u62e9\u4e0e\u6a21\u578b\u4f18\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u53d8\u91cf\u9009\u62e9\u4e0e\u6a21\u578b\u4f18\u5316\u7684\u8054\u5408\u6709\u6548\u6027\u3002", "conclusion": "\u8bc1\u5b9e\u8054\u5408\u53d8\u91cf\u9009\u62e9\u548c\u6a21\u578b\u4f18\u5316\u5bf9\u4e8eSTMF\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u3001\u51c6\u786e\u9884\u6d4b\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.24035", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24035", "abs": "https://arxiv.org/abs/2510.24035", "authors": ["Xinqi Li", "Yiqun Liu", "Shan Jiang", "Enrong Zheng", "Huaijin Zheng", "Wenhao Dai", "Haodong Deng", "Dianhai Yu", "Yanjun Ma"], "title": "GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler Research", "comment": null, "summary": "We introduce GraphNet, a dataset of 2.7K real-world deep learning\ncomputational graphs with rich metadata, spanning six major task categories\nacross multiple deep learning frameworks. To evaluate tensor compiler\nperformance on these samples, we propose the benchmark metric Speedup Score\nS(t), which jointly considers runtime speedup and execution correctness under\ntunable tolerance levels, offering a reliable measure of general optimization\ncapability. Furthermore, we extend S(t) to the Error-aware Speedup Score ES(t),\nwhich incorporates error information and helps compiler developers identify key\nperformance bottlenecks. In this report, we benchmark the default tensor\ncompilers, CINN for PaddlePaddle and TorchInductor for PyTorch, on computer\nvision (CV) and natural language processing (NLP) samples to demonstrate the\npracticality of GraphNet. The full construction pipeline with graph extraction\nand compiler evaluation tools is available at\nhttps://github.com/PaddlePaddle/GraphNet .", "AI": {"tldr": "\u63d0\u51fa GraphNet \u6570\u636e\u96c6\u53ca Speedup Score \u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f20\u91cf\u7f16\u8bd1\u5668\u5728\u771f\u5b9e\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u56fe\u4e0a\u7684\u6027\u80fd\u4e0e\u6b63\u786e\u6027\uff0c\u914d\u5408 ES(t) \u91cf\u5316\u9519\u8bef\u4fe1\u606f\uff0c\u5e76\u5728 CV/NLP \u4e0a\u5bf9 CINN \u4e0e TorchInductor \u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5f53\u524d\u5f20\u91cf\u7f16\u8bd1\u5668\u8bc4\u4f30\u7f3a\u4e4f\u73b0\u5b9e\u4e16\u754c\u8ba1\u7b97\u56fe\u7684\u8986\u76d6\u3001\u5bf9\u6b63\u786e\u6027\u4e0e\u5bb9\u9519\u6027\u91cf\u5316\u7684\u7edf\u4e00\u6307\u6807\uff0c\u4ee5\u53ca\u8de8\u6846\u67b6\u3001\u591a\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u5e2e\u52a9\u8bc6\u522b\u74f6\u9888\u3001\u63a8\u52a8\u4f18\u5316\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa GraphNet\uff1a2.7K \u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u56fe\uff0c\u5305\u542b\u4e30\u5bcc\u5143\u6570\u636e\uff0c\u8986\u76d6\u516d\u5927\u4efb\u52a1\u7c7b\u522b\u5e76\u8de8\u591a\u6846\u67b6\uff1b\u63d0\u51fa Speedup Score S(t)\uff0c\u8054\u5408\u8003\u8651\u8fd0\u884c\u65f6\u52a0\u901f\u4e0e\u6267\u884c\u6b63\u786e\u6027\uff08\u5141\u8bb8\u5bb9\u5fcd\u5ea6\uff09\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u901a\u7528\u4f18\u5316\u80fd\u529b\u8bc4\u4f30\uff1b\u6269\u5c55\u4e3a Error-aware Speedup Score ES(t)\uff0c\u7eb3\u5165\u9519\u8bef\u4fe1\u606f\u4ee5\u5b9a\u4f4d\u6027\u80fd\u74f6\u9888\uff1b\u5bf9 PaddlePaddle \u7684 CINN \u4e0e PyTorch \u7684 TorchInductor \u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6837\u672c\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1b\u63d0\u4f9b\u5b8c\u6574\u7684\u56fe\u63d0\u53d6\u4e0e\u7f16\u8bd1\u5668\u8bc4\u4f30\u5de5\u5177\u53ca\u5f00\u6e90\u7ba1\u7ebf\u3002", "result": "\u5728CV\u4e0eNLP\u6837\u672c\u4e0a\u9a8c\u8bc1\u4e86 GraphNet \u7684\u53ef\u884c\u6027\u4e0e\u5b9e\u7528\u6027\uff0cS(t) \u4e0e ES(t) \u80fd\u6709\u6548\u53cd\u6620\u4f18\u5316\u80fd\u529b\u4e0e\u9519\u8bef\u4fe1\u606f\uff0c\u4e3a\u7f16\u8bd1\u5668\u8bc4\u4f30\u63d0\u4f9b\u53ef\u91cd\u590d\u3001\u8de8\u6846\u67b6\u7684\u57fa\u51c6\u3002", "conclusion": "GraphNet \u63d0\u4f9b\u8986\u76d6\u73b0\u5b9e\u4e16\u754c\u3001\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0e\u91cf\u5316\u6307\u6807\uff0c\u4fbf\u4e8e\u5f00\u53d1\u8005\u8bc6\u522b\u74f6\u9888\u3001\u63a8\u52a8\u5f20\u91cf\u7f16\u8bd1\u5668\u7684\u4f18\u5316\uff0c\u76f8\u5173\u5b9e\u73b0\u4e0e\u6570\u636e\u7ba1\u7ebf\u5f00\u6e90\u3002"}}
{"id": "2510.24677", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24677", "abs": "https://arxiv.org/abs/2510.24677", "authors": ["Xun Liang", "Huayi Lai", "Hanyu Wang", "Wentao Zhang", "Linfeng Zhang", "Yanfang Chen", "Feiyu Xiong", "Zhiyu Li"], "title": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation", "comment": "15 pages, 9 figures", "summary": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor", "AI": {"tldr": "RPNA\u6846\u67b6\u8bc4\u4f30\u63d0\u793a\u6027\u89d2\u8272\u5bf9LLMs\u5728\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u89d2\u8272\u63d0\u793a\u5e76\u672a\u589e\u5f3a\u533b\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4e3b\u8981\u6539\u53d8\u8868\u5c42\u8bed\u8a00\u7279\u5f81\uff0c\u672a\u5448\u73b0\u4e0d\u540c\u884c\u4e3a\u80cc\u540e\u7684\u8ba4\u77e5\u5dee\u5f02\uff0c\u6838\u5fc3\u51b3\u7b56\u673a\u5236\u4e00\u81f4\u3002", "motivation": "\u63a2\u7a76\u5728\u63d0\u793a\u6027\u89d2\u8272\u626e\u6f14\uff08PBRP\uff09\u4e2d\uff0c\u89d2\u8272\u63d0\u793a\u662f\u5426\u4f1a\u5f15\u53d1LLMs\u7684\u4e0d\u540c\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u8fd8\u662f\u4ec5\u6539\u53d8\u8bed\u8a00\u98ce\u683c\uff1b\u901a\u8fc7\u795e\u7ecf\u5143\u5207\u9664\u4e0e\u8868\u5f81\u5206\u6790\u8bc4\u4f30\u662f\u5426\u5b58\u5728\u89d2\u8272\u7279\u5f02\u7684\u63a8\u7406\u8def\u5f84\u3002", "method": "\u63d0\u51faRP-Neuron-Activated Evaluation Framework(RPNA)\uff0c\u5728\u4e09\u7ec4\u533b\u5b66QA\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u89d2\u8272\u63d0\u793a\u6548\u5e94\uff1b\u7ed3\u5408\u795e\u7ecf\u5143\u6d88\u878d\u548c representation analysis \u6765\u68c0\u6d4b\u63a8\u7406\u8def\u5f84\u7684\u6539\u53d8\uff1b\u6bd4\u8f83\u4e0d\u540c\u4e34\u5e8a\u89d2\u8272\uff08\u533b\u5b66\u751f\u3001\u4f4f\u9662\u533b\u5e08\u3001\u4e3b\u6cbb\u533b\u5e08\uff09\u7684\u63a8\u7406\u5dee\u5f02\u3002", "result": "\u7ed3\u679c\u663e\u793a\u89d2\u8272\u63d0\u793a\u5e76\u672a\u663e\u8457\u63d0\u5347\u533b\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u66f4\u591a\u5730\u5f71\u54cd\u8868\u5c42\u8bed\u8a00\u7279\u5f81\uff0c\u672a\u53d1\u73b0\u660e\u663e\u7684\u63a8\u7406\u8def\u5f84\u6216\u8ba4\u77e5\u5dee\u5f02\uff1b\u5404\u89d2\u8272\u7684\u6838\u5fc3\u51b3\u7b56\u673a\u5236\u4fdd\u6301\u4e00\u81f4\uff0c\u73b0\u6709\u7684PBRP\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u518d\u73b0\u771f\u5b9e\u4e34\u5e8a\u7684\u8ba4\u77e5\u590d\u6742\u6027\u3002", "conclusion": "\u63d0\u793a\u6027\u89d2\u8272\u626e\u6f14\u5728\u533b\u7597AI\u4e2d\u5177\u6709\u5c40\u9650\u6027\uff0c\u5e94\u5f53\u53d1\u5c55\u80fd\u591f\u6a21\u62df\u771f\u5b9e\u8ba4\u77e5\u8fc7\u7a0b\u7684\u6a21\u578b\uff0c\u800c\u975e\u4ec5\u6a21\u4eff\u8bed\u8a00\u98ce\u683c\uff1b\u76f8\u5173\u4ee3\u7801\u5df2\u5728\u4ed3\u5e93\u516c\u5f00\u3002"}}
{"id": "2510.24684", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24684", "abs": "https://arxiv.org/abs/2510.24684", "authors": ["Bo Liu", "Chuanyang Jin", "Seungone Kim", "Weizhe Yuan", "Wenting Zhao", "Ilia Kulikov", "Xian Li", "Sainbayar Sukhbaatar", "Jack Lanchantin", "Jason Weston"], "title": "SPICE: Self-Play In Corpus Environments Improves Reasoning", "comment": null, "summary": "Self-improving systems require environmental interaction for continuous\nadaptation. We introduce SPICE (Self-Play In Corpus Environments), a\nreinforcement learning framework where a single model acts in two roles: a\nChallenger that mines documents from a large corpus to generate diverse\nreasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,\nthe Challenger creates an automatic curriculum at the frontier of the\nReasoner's capability, while corpus grounding provides the rich,\nnear-inexhaustible external signal necessary for sustained improvement. Unlike\nexisting ungrounded self-play methods that offer more limited benefits, SPICE\nachieves consistent gains across mathematical (+8.9%) and general reasoning\n(+9.8%) benchmarks on multiple model families. Our analysis reveals how\ndocument grounding is a key ingredient in SPICE to continuously generate its\nown increasingly challenging goals and achieve them, enabling sustained\nself-improvement.", "AI": {"tldr": "\u63d0\u51fa SPICE\uff1a\u4e00\u4e2a\u5728\u5927\u8bed\u6599\u5e93\u4e0a\u901a\u8fc7\u81ea\u6211\u5bf9\u6297\u7684\u6846\u67b6\uff0c Challenger \u751f\u6210\u4efb\u52a1\u3001Reasoner \u89e3\u51b3\uff0c\u7ed3\u5408\u6587\u6863 grounding \u5b9e\u73b0\u6301\u7eed\u81ea\u6211\u63d0\u5347\uff0c\u63d0\u5347\u6570\u5b66\u4e0e\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u6211\u5bf9\u5f08\u7f3a\u4e4f\u73af\u5883\u4fe1\u53f7\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u5916\u90e8\u4fe1\u53f7\u6765\u9a71\u52a8\u957f\u671f\u6539\u8fdb\uff1b\u5728\u65e0\u5916\u90e8\u73af\u5883\u7684\u81ea\u6211\u5bf9\u5f08\u4e2d\u901a\u5e38\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u53cc\u89d2\u8272\u6a21\u578b\uff1aChallenger \u6316\u6398\u8bed\u6599\u5e93\u4ee5\u4ea7\u751f\u591a\u6837\u7684\u63a8\u7406\u4efb\u52a1\uff1bReasoner \u89e3\u51b3\uff1b\u901a\u8fc7\u5bf9\u6297\u6027\u52a8\u6001\u6784\u5efa\u524d\u6cbf\u96be\u5ea6\u7684\u81ea\u52a8\u8bfe\u7a0b\uff1b\u6587\u6863 grounding \u63d0\u4f9b\u6301\u7eed\u3001\u51e0\u4e4e\u65e0\u7a77\u7684\u5916\u90e8\u4fe1\u53f7\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u548c\u4e00\u822c\u63a8\u7406\u57fa\u51c6\u4e0a\uff0c\u5bf9\u591a\u79cd\u6a21\u578b\u5bb6\u65cf\u5b9e\u73b0\u7a33\u5b9a\u589e\u76ca\uff0c\u5206\u522b\u7ea6 +8.9% \u548c +9.8%\u3002", "conclusion": "\u6587\u6863 grounding \u662f SPICE \u6301\u7eed\u751f\u6210\u66f4\u5177\u6311\u6218\u76ee\u6807\u5e76\u8fbe\u6210\u5b83\u4eec\u7684\u5173\u952e\uff0c\u4fc3\u6210\u6301\u7eed\u81ea\u6211\u63d0\u5347\u3002"}}
{"id": "2510.24043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24043", "abs": "https://arxiv.org/abs/2510.24043", "authors": ["Akira Tamamori"], "title": "Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection", "comment": "10 pages, 4 figures; submitted to The IEICE Transactions on\n  Information and Systems", "summary": "This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection\nframework that overcomes the coexisting limitations of conventional\nprojection-based methods: their reliance on a fixed statistical metric and\ntheir assumption of a single data structure. Our framework uniquely synthesizes\nthree key concepts: (1) a generalized loss-based outlyingness measure (PLO)\nthat replaces the fixed metric with flexible, adaptive loss functions like our\nproposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear\ndata structures; and (3) a subsequent local clustering stage to handle\nmulti-modal distributions. Comprehensive 5-fold cross-validation experiments on\n10 benchmark datasets, with automated hyperparameter optimization, demonstrate\nthat Two-Stage LKPLO achieves state-of-the-art performance. It significantly\noutperforms strong baselines on datasets with challenging structures where\nexisting methods fail, most notably on multi-cluster data (Optdigits) and\ncomplex, high-dimensional data (Arrhythmia). Furthermore, an ablation study\nempirically confirms that the synergistic combination of both the kernelization\nand localization stages is indispensable for its superior performance. This\nwork contributes a powerful new tool for a significant class of outlier\ndetection problems and underscores the importance of hybrid, multi-stage\narchitectures.", "AI": {"tldr": "\u63d0\u51fa Two-Stage LKPLO\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5\u79bb\u7fa4\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u81ea\u9002\u5e94\u635f\u5931\u7684\u79bb\u7fa4\u5ea6PLO\u3001\u5168\u5c40\u6838PCA\u548c\u5c40\u90e8\u805a\u7c7b\u5b9e\u73b0\u5bf9\u975e\u7ebf\u6027\u3001\u591a\u6a21\u6001\u6570\u636e\u7684\u9c81\u68d2\u68c0\u6d4b\uff0c\u572810\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\uff0c\u4e14\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u6838\u5316\u4e0e\u5c40\u90e8\u5316\u7684\u7ec4\u5408\u4e0d\u53ef\u7f3a\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6295\u5f71\u5f0f\u79bb\u7fa4\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u56fa\u5b9a\u635f\u5931\u6307\u6807\u4e0e\u5355\u4e00\u6570\u636e\u7ed3\u6784\u7684\u4f9d\u8d56\uff0c\u96be\u4ee5\u5904\u7406\u975e\u7ebf\u6027\u3001\u590d\u6742\u5206\u5e03\u4e0b\u7684\u79bb\u7fa4\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u4e09\u9636\u6bb5\u8bbe\u8ba1\uff1a1) PLO\uff1a\u57fa\u4e8e\u635f\u5931\u7684\u5e7f\u4e49\u79bb\u7fa4\u5ea6\uff0c\u91c7\u7528SVM\u6837\u5f0f\u7684\u81ea\u9002\u5e94\u635f\u5931\uff1b2) \u5168\u5c40\u6838PCA\uff0c\u5c06\u6570\u636e\u7ed3\u6784\u7ebf\u6027\u5316\uff1b3) \u5c40\u90e8\u805a\u7c7b\u9636\u6bb5\u5904\u7406\u591a\u6a21\u6001\u5206\u5e03\uff0c\u5e76\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c5\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u81ea\u52a8\u8d85\u53c2\u4f18\u5316\u3002", "result": "\u572810\u4e2a benchmark \u6570\u636e\u96c6\u4e0a\uff0c5\u6298CV\u53ca\u81ea\u52a8\u8d85\u53c2\u4f18\u5316\u4e0b\uff0cTwo-Stage LKPLO \u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u591a\u7c07\u6570\u636e\uff08Optdigits\uff09\u548c\u9ad8\u7ef4\u6570\u636e\uff08Arrhythmia\uff09\u4e0a\u8868\u73b0\u51fa\u8272\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u6838\u5316\u4e0e\u5c40\u90e8\u5316\u7684\u7ec4\u5408\u5bf9\u6027\u80fd\u63d0\u5347\u662f\u5173\u952e\u3002", "conclusion": "\u6df7\u5408\u7684\u591a\u9636\u6bb5\u67b6\u6784\u4e3a\u79bb\u7fa4\u68c0\u6d4b\u63d0\u4f9b\u5f3a\u5927\u5de5\u5177\uff0c\u51f8\u663e\u4e86\u878d\u5408\u5185\u6838\u5316\u4e0e\u5c40\u90e8\u5316\u7684\u7b56\u7565\u5728\u5904\u7406\u590d\u6742\u5206\u5e03\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.24044", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24044", "abs": "https://arxiv.org/abs/2510.24044", "authors": ["Hui Sun", "Zheng Xie", "Hao-Yuan He", "Ming Li"], "title": "Mitigating Negative Transfer via Reducing Environmental Disagreement", "comment": "13 pages, 5 figures", "summary": "Unsupervised Domain Adaptation~(UDA) focuses on transferring knowledge from a\nlabeled source domain to an unlabeled target domain, addressing the challenge\nof \\emph{domain shift}. Significant domain shifts hinder effective knowledge\ntransfer, leading to \\emph{negative transfer} and deteriorating model\nperformance. Therefore, mitigating negative transfer is essential. This study\nrevisits negative transfer through the lens of causally disentangled learning,\nemphasizing cross-domain discriminative disagreement on non-causal\nenvironmental features as a critical factor. Our theoretical analysis reveals\nthat overreliance on non-causal environmental features as the environment\nevolves can cause discriminative disagreements~(termed \\emph{environmental\ndisagreement}), thereby resulting in negative transfer. To address this, we\npropose Reducing Environmental Disagreement~(RED), which disentangles each\nsample into domain-invariant causal features and domain-specific non-causal\nenvironmental features via adversarially training domain-specific environmental\nfeature extractors in the opposite domains. Subsequently, RED estimates and\nreduces environmental disagreement based on domain-specific non-causal\nenvironmental features. Experimental results confirm that RED effectively\nmitigates negative transfer and achieves state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa RED \u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bcf\u4e2a\u6837\u672c\u5c06\u57df\u4e0d\u53d8\u7684\u56e0\u679c\u7279\u5f81\u4e0e\u57df\u7279\u5f02\u7684\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\u8fdb\u884c\u5bf9\u6297\u6027\u63d0\u53d6\uff0c\u5e76\u964d\u4f4e\u73af\u5883\u5206\u6b67\uff0c\u4ece\u800c\u7f13\u89e3\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u8d1f\u8fc1\u79fb\uff0c\u8fbe\u5230\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "UDA \u9762\u4e34\u57df\u95f4\u8fc1\u79fb\u5bfc\u81f4\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u4f5c\u8005\u4ece\u56e0\u679c\u89e3\u8026\u89d2\u5ea6\u51fa\u53d1\uff0c\u8bc6\u522b\u8de8\u57df discriminative disagreement\uff08\u5bf9\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\u7684\u5224\u522b\u6027\u5206\u6b67\uff09\u662f\u8d1f\u8fc1\u79fb\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u63d0\u51fa RED\uff1a\u901a\u8fc7\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e2d\u5206\u522b\u5b66\u4e60\u57df\u7279\u5f02\u7684\u73af\u5883\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5c06\u6bcf\u4e2a\u6837\u672c\u5206\u89e3\u4e3a\u57df\u4e0d\u53d8\u7684\u56e0\u679c\u7279\u5f81\u548c\u57df\u7279\u5f02\u7684\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\uff1b\u518d\u57fa\u4e8e\u57df\u7279\u5f02\u7684\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\u4f30\u8ba1\u5e76\u51cf\u5c11\u73af\u5883\u5206\u6b67\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e RED \u80fd\u6709\u6548\u7f13\u89e3\u8d1f\u8fc1\u79fb\uff0c\u5e76\u8fbe\u5230\u6216\u63a5\u8fd1\u73b0\u6709\u65b9\u6cd5\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5730 disentangle \u56e0\u679c\u4e0e\u73af\u5883\u7279\u5f81\u5e76\u51cf\u5c11\u8de8\u57df\u73af\u5883\u5206\u6b67\uff0cUDA \u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\u53ef\u4ee5\u5f97\u5230\u7f13\u89e3\uff0c\u4e14\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.24695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24695", "abs": "https://arxiv.org/abs/2510.24695", "authors": ["Xuanzhong Chen", "Zile Qiao", "Guoxin Chen", "Liangcai Su", "Zhen Zhang", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Jingren Zhou", "Yong Jiang"], "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.", "AI": {"tldr": "ZPD-inspired data synthesis for LLM frontier tasks via AgentFrontier Engine; introduces ZPD Exam and shows AgentFrontier-30B-A3B achieving state-of-the-art on hard benchmarks.", "motivation": "Bridge data scarcity for frontier reasoning tasks by guiding data generation within the LLM's Zone of Proximal Development, enabling more capable agents.", "method": "AgentFrontier Engine to synthesize multidisciplinary data situated in the LLM's ZPD; supports continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning; ZPD Exam benchmark; training AgentFrontier-30B-A3B.", "result": "State-of-the-art results on challenging benchmarks such as Humanity's Last Exam, surpassing some leading proprietary agents.", "conclusion": "ZPD-guided data synthesis offers a scalable, effective path to building more capable LLM agents; the approach provides a dynamic benchmark for frontier evaluation."}}
{"id": "2510.24046", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24046", "abs": "https://arxiv.org/abs/2510.24046", "authors": ["Tu Anh Hoang Nguyen", "Dang Nguyen", "Tri-Nhan Vo", "Thuc Duy Le", "Sunil Gupta"], "title": "Causal-Aware Generative Adversarial Networks with Reinforcement Learning", "comment": null, "summary": "The utility of tabular data for tasks ranging from model training to\nlarge-scale data analysis is often constrained by privacy concerns or\nregulatory hurdles. While existing data generation methods, particularly those\nbased on Generative Adversarial Networks (GANs), have shown promise, they\nfrequently struggle with capturing complex causal relationship, maintaining\ndata utility, and providing provable privacy guarantees suitable for enterprise\ndeployment. We introduce CA-GAN, a novel generative framework specifically\nengineered to address these challenges for real-world tabular datasets. CA-GAN\nutilizes a two-step approach: causal graph extraction to learn a robust,\ncomprehensive causal relationship in the data's manifold, followed by a custom\nConditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates\nexclusively as per the structure of nodes in the causal graph. More\nimportantly, the generator is trained with a new Reinforcement Learning-based\nobjective that aligns the causal graphs constructed from real and fake data,\nensuring the causal awareness in both training and sampling phases. We\ndemonstrate CA-GAN superiority over six SOTA methods across 14 tabular\ndatasets. Our evaluations, focused on core data engineering metrics: causal\npreservation, utility preservation, and privacy preservation. Our method offers\na practical, high-performance solution for data engineers seeking to create\nhigh-quality, privacy-compliant synthetic datasets to benchmark database\nsystems, accelerate software development, and facilitate secure data-driven\nresearch.", "AI": {"tldr": "CA-GAN\u63d0\u51fa\u4e00\u4e2a\u4e24\u6b65\u5f0f\u8868\u683c\u6570\u636e\u751f\u6210\u6846\u67b6\uff1a\u5148\u8fdb\u884c\u56e0\u679c\u56fe\u63d0\u53d6\u4ee5\u6355\u6349\u6570\u636e\u7684\u56e0\u679c\u7ed3\u6784\uff0c\u518d\u7528\u4e13\u95e8\u7684\u6761\u4ef6WGAN-GP\u5728\u8be5\u7ed3\u6784\u4e0b\u751f\u6210\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u4f7f\u771f\u5b9e\u6570\u636e\u4e0e\u4f2a\u6570\u636e\u7684\u56e0\u679c\u56fe\u5bf9\u9f50\u3002\u572814\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e6\u79cdSOTA\u65b9\u6cd5\uff0c\u6838\u5fc3\u6307\u6807\u8986\u76d6\u56e0\u679c\u4fdd\u771f\u3001\u5b9e\u7528\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\uff0c\u9002\u5408\u4f01\u4e1a\u7ea7\u65e0\u9690\u79c1\u6cc4\u9732\u7684\u6570\u636e\u751f\u6210\u9700\u6c42\u3002", "motivation": "\u5728\u9690\u79c1\u5408\u89c4\u548c\u6570\u636e\u5b89\u5168\u7684\u7ea6\u675f\u4e0b\uff0c\u5982\u4f55\u5728\u4fdd\u6301\u9ad8\u6570\u636e\u5b9e\u7528\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u662f\u4f01\u4e1a\u7ea7\u8868\u683c\u6570\u636e\u751f\u6210\u7684\u5173\u952e\u6311\u6218\u3002\u73b0\u6709GAN\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u56e0\u679c\u5173\u7cfb\u3001\u7ef4\u6301\u6570\u636e\u6548\u7528\u4e14\u7f3a\u4e4f\u53ef\u8bc1\u660e\u7684\u9690\u79c1\u4fdd\u969c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u5728\u56e0\u679c\u5c42\u6b21\u4e0a\u628a\u63a7\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faCA-GAN\u7684\u4e24\u6b65\u6d41\u7a0b\uff1a1) \u4ece\u771f\u5b9e\u6570\u636e\u4e2d\u63d0\u53d6\u9c81\u68d2\u4e14\u5b8c\u6574\u7684\u56e0\u679c\u5173\u7cfb\u56fe\uff0c\u6784\u5efa\u6570\u636e\u5728\u7ed3\u6784\u4e0a\u7684\u56e0\u679c\u7ea6\u675f\uff1b2) \u5728\u8be5\u56e0\u679c\u7ed3\u6784\u4e0b\u8bad\u7ec3\u4e13\u95e8\u7684\u6761\u4ef6WGAN-GP\u751f\u6210\u5668\uff0c\u786e\u4fdd\u751f\u6210\u8fc7\u7a0b\u4e25\u683c\u9075\u5faa\u56fe\u7ed3\u6784\uff0c\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4f7f\u8bad\u7ec3\u65f6\u7684\u771f\u5b9e\u548c\u4f2a\u6570\u636e\u5728\u56e0\u679c\u56fe\u5c42\u9762\u4fdd\u6301\u4e00\u81f4\u6027\uff08\u56e0\u679c\u611f\u77e5\uff09\u3002", "result": "\u572814\u4e2a\u8868\u683c\u6570\u636e\u96c6\u4e0a\u4e0e\u516d\u79cdSOTA\u65b9\u6cd5\u6bd4\u8f83\uff0cCA-GAN\u5728\u6838\u5fc3\u6570\u636e\u5de5\u7a0b\u6307\u6807\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u56e0\u679c\u4fdd\u6301\u3001\u6570\u636e\u5b9e\u7528\u6027\u4fdd\u6301\u548c\u9690\u79c1\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u663e\u793a\u51fa\u5728\u4f01\u4e1a\u573a\u666f\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9690\u79c1\u5408\u89c4\u7684\u5408\u6210\u6570\u636e\u7684\u6f5c\u529b\u3002", "conclusion": "CA-GAN\u4e3a\u771f\u5b9e\u4e16\u754c\u7684\u8868\u683c\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u9ad8\u6027\u80fd\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u4fbf\u4e8e\u57fa\u51c6\u6570\u636e\u5e93\u7cfb\u7edf\u3001\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u4ee5\u53ca\u5b89\u5168\u6570\u636e\u9a71\u52a8\u7814\u7a76\u7684\u843d\u5730\u90e8\u7f72\uff1b\u5176\u56e0\u679c\u611f\u77e5\u4e0e\u9690\u79c1\u4fdd\u62a4\u7279\u6027\u6709\u671b\u63a8\u52a8\u4f01\u4e1a\u7ea7\u6570\u636e\u5171\u4eab\u4e0e\u5206\u6790\u6d3b\u52a8\u3002"}}
{"id": "2510.24698", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24698", "abs": "https://arxiv.org/abs/2510.24698", "authors": ["Baixuan Li", "Dingchu Zhang", "Jialong Wu", "Wenbiao Yin", "Zhengwei Tao", "Yida Zhao", "Liwen Zhang", "Haiyang Shen", "Runnan Fang", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking", "comment": null, "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.", "AI": {"tldr": "Two-stage ParallelMuse framework for deep information-seeking (IS) agents that improves exploration efficiency and long-horizon reasoning: Stage 1 partitions generated sequences into functional regions to enable uncertainty-guided path reuse and branching; Stage 2 compresses reasoning to losslessly synthesize a final answer. Empirical results show up to 62% performance gains and 10-30% reduction in exploratory tokens across open-source agents and benchmarks.", "motivation": "Address inefficiency in parallel thinking for IS agents caused by re-rolling out from scratch, and the difficulty of integrating long-horizon reasoning due to limited context capacity during answer generation.", "method": "Stage 1 Functionality-Specified Partial Rollout: partition sequences into functional regions and apply uncertainty-guided path reuse and branching to enhance exploration efficiency. Stage 2 Compressed Reasoning Aggregation: identify and compress redundancy in reasoning steps to produce a coherent final answer with lossless aggregation.", "result": "Empirical evaluation across multiple open-source agents and benchmarks shows up to 62% performance improvement with 10-30% reduction in exploratory token consumption.", "conclusion": "ParallelMuse effectively mitigates inefficiency of parallel thinking and enables deeper, more coherent reasoning in IS agents, delivering substantial performance gains and efficiency improvements."}}
{"id": "2510.24053", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.24053", "abs": "https://arxiv.org/abs/2510.24053", "authors": ["Jacob B. Roberts", "Catherine R. Ji", "Isaac Donnell", "Thomas D. Young", "Allison N. Pearson", "Graham A. Hudson", "Leah S. Keiser", "Mia Wesselkamper", "Peter H. Winegar", "Janik Ludwig", "Sarah H. Klass", "Isha V. Sheth", "Ezechinyere C. Ukabiala", "Maria C. T. Astolfi", "Benjamin Eysenbach", "Jay D. Keasling"], "title": "Low-N Protein Activity Optimization with FolDE", "comment": "18 pages, 4 figures. Preprint. Open-source software available at\n  https://github.com/JBEI/foldy", "summary": "Proteins are traditionally optimized through the costly construction and\nmeasurement of many mutants. Active Learning-assisted Directed Evolution (ALDE)\nalleviates that cost by predicting the best improvements and iteratively\ntesting mutants to inform predictions. However, existing ALDE methods face a\ncritical limitation: selecting the highest-predicted mutants in each round\nyields homogeneous training data insufficient for accurate prediction models in\nsubsequent rounds. Here we present FolDE, an ALDE method designed to maximize\nend-of-campaign success. In simulations across 20 protein targets, FolDE\ndiscovers 23% more top 10% mutants than the best baseline ALDE method (p=0.005)\nand is 55% more likely to find top 1% mutants. FolDE achieves this primarily\nthrough naturalness-based warm-starting, which augments limited activity\nmeasurements with protein language model outputs to improve activity\nprediction. We also introduce a constant-liar batch selector, which improves\nbatch diversity; this is important in multi-mutation campaigns but had limited\neffect in our benchmarks. The complete workflow is freely available as\nopen-source software, making efficient protein optimization accessible to any\nlaboratory.", "AI": {"tldr": "FolDE \u662f\u4e00\u79cd\u9762\u5411\u6700\u7ec8\u6210\u529f\u7684 ALDE \u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u6027\u6696\u542f\u52a8\u4e0e\u6052\u5b9a\u9a97\u5b50(batch)\u7b5b\u9009\u63d0\u5347\u8de8\u8f6e\u9884\u6d4b\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u4e0e\u9884\u6d4b\u6027\u80fd\uff0c\u572820\u4e2a\u86cb\u767d\u9776\u6807\u7684\u6a21\u62df\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u73b0\u6709 ALDE \u5728\u6bcf\u8f6e\u4ec5\u9009\u53d6\u6700\u9ad8\u9884\u6d4b\u503c\u5bfc\u81f4\u8bad\u7ec3\u6570\u636e\u540c\u8d28\u5316\u3001\u96be\u4ee5\u5728\u540e\u7eed\u8f6e\u6b21\u51c6\u786e\u9884\u6d4b\u7684\u95ee\u9898\uff1b\u76ee\u6807\u662f\u63d0\u9ad8\u7aef\u5230\u7aef\u7684\u4f18\u5316\u6210\u529f\u7387", "method": "\u5f15\u5165 FolDE\uff0c\u5305\u62ec\u81ea\u7136\u6027\u6696\u542f\u52a8\uff08\u7ed3\u5408\u6d3b\u52a8\u6d4b\u91cf\u548c\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u6765\u6539\u5584\u6d3b\u6027\u9884\u6d4b\uff09\u548c\u6052\u5b9a liar batch selector\uff0c\u7528\u4e8e\u589e\u5f3a\u6279\u91cf\u591a\u53d8\u4f53\u7b5b\u9009\u8fc7\u7a0b\u7684\u591a\u6837\u6027", "result": "\u572820\u4e2a\u86cb\u767d\u9776\u6807\u7684\u6a21\u62df\u4e2d\uff0cFolDE\u6bd4\u6700\u4f73\u57fa\u7ebf ALDE \u63d0\u4f9b23%\u7684\u524d10%\u53d8\u4f53\u63d0\u5347\uff08p=0.005\uff09\uff0c\u627e\u5230\u524d1%\u53d8\u4f53\u7684\u6982\u7387\u63d0\u9ad855%", "conclusion": "FolDE \u901a\u8fc7\u81ea\u7136\u6027\u6696\u542f\u52a8\u548c\u6052\u5b9a liar \u7b5b\u9009\u663e\u8457\u63d0\u5347\u7aef\u5230\u7aef\u86cb\u767d\u8d28\u4f18\u5316\u6548\u7387\uff0c\u5f00\u6e90\u5b9e\u73b0\u53ef\u4f9b\u5b9e\u9a8c\u5ba4\u4f7f\u7528\uff1b\u6052\u5b9a liar \u5bf9\u591a\u53d8\u4f53\u5b9e\u9a8c\u6709\u5229\u4e8e\u6279\u91cf\u591a\u6837\u6027\uff0c\u4f46\u5728\u672c\u57fa\u51c6\u4e2d\u5f71\u54cd\u6709\u9650"}}
{"id": "2510.24088", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24088", "abs": "https://arxiv.org/abs/2510.24088", "authors": ["Moongyu Jeon", "Sangwoo Shin", "Dongjae Jeon", "Albert No"], "title": "Information-Theoretic Discrete Diffusion", "comment": "Accepted at NeurIPS 2025", "summary": "We present an information-theoretic framework for discrete diffusion models\nthat yields principled estimators of log-likelihood using score-matching\nlosses. Inspired by the I-MMSE identity for the Gaussian setup, we derive\nanalogous results for the discrete setting. Specifically, we introduce the\nInformation-Minimum Denoising Score Entropy (I-MDSE) relation, which links\nmutual information between data and its diffused version to the minimum\ndenoising score entropy (DSE) loss. We extend this theory to masked diffusion\nand establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)\nrelation, connecting cross-entropy losses to mutual information in discrete\nmasked processes. These results provide a time-integral decomposition of the\nlog-likelihood of the data in terms of optimal score-based losses, showing that\ncommonly used losses such as DSE and DCE are not merely variational bounds but\ntight and principled estimators of log-likelihood. The I-MDCE decomposition\nfurther enables practical extensions, including time-free formula, conditional\nlikelihood estimation in prompt-response tasks, and coupled Monte Carlo\nestimation of likelihood ratios. Experiments on synthetic and real-world data\nconfirm the accuracy, variance stability, and utility of our estimators. The\ncode is publicly available at https://github.com/Dongjae0324/infodis.", "AI": {"tldr": "\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u4fe1\u606f\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa I-MDSE \u4e0e I-MDCE \u5173\u7cfb\uff0c\u5c06\u4e92\u4fe1\u606f\u4e0e\u53bb\u566a\u5206\u6570\u71b5/\u4ea4\u53c9\u71b5\u8054\u7cfb\u8d77\u6765\uff0c\u7ed9\u51fa\u5bf9\u6570\u4f3c\u7136\u7684\u65f6\u95f4\u79ef\u5206\u5206\u89e3\uff1b\u6269\u5c55\u5230\u63a9\u7801\u6269\u6563\uff0c\u652f\u6301\u65f6\u95f4\u65e0\u5173\u516c\u5f0f\u3001\u6761\u4ef6\u4f3c\u7136\u4f30\u8ba1\u548c\u8026\u5408\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\uff1b\u5b9e\u9a8c\u8bc1\u660e\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u4e0e\u65b9\u5dee\u7a33\u5b9a\u6027\uff0c\u4ee3\u7801\u516c\u5f00\u3002", "motivation": "\u9700\u8981\u5bf9\u79bb\u6563\u6269\u6563\u6a21\u578b\u8fdb\u884c principled \u7684\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\uff1b\u53d7 I-MMSE \u7684\u542f\u53d1\uff0c\u5c06\u9ad8\u65af\u60c5\u5f62\u7684\u4fe1\u606f\u7406\u8bba\u7ed3\u679c\u63a8\u5e7f\u5230\u79bb\u6563\u60c5\u5f62\uff1b\u901a\u8fc7\u63a9\u7801\u6269\u6563\u62d3\u5c55\u9002\u7528\u573a\u666f\uff0c\u63d0\u5347\u7406\u8bba\u4e0e\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa I-MDSE\uff0c\u5c06\u6570\u636e\u53ca\u5176\u6269\u6563\u7248\u672c\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u4e0e\u6700\u5c0f\u53bb\u566a\u5206\u6570\u71b5\u635f\u5931\u8054\u7cfb\uff1b\u6269\u5c55\u81f3\u63a9\u7801\u6269\u6563\uff0c\u63d0\u51fa I-MDCE\uff0c\u5c06\u4ea4\u53c9\u71b5\u635f\u5931\u4e0e\u4e92\u4fe1\u606f\u8054\u7cfb\u8d77\u6765\uff1b\u7ed9\u51fa\u5bf9\u6570\u4f3c\u7136\u7684\u65f6\u95f4\u79ef\u5206\u5206\u89e3\uff0c\u5206\u6790 DSE/DCE \u7684\u6027\u8d28\uff0c\u5e76\u63d0\u4f9b\u65f6\u95f4\u65e0\u5173\u516c\u5f0f\u3001\u6761\u4ef6\u4f3c\u7136\u4f30\u8ba1\u548c\u8026\u5408\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u7b49\u5b9e\u73b0\u8def\u5f84\u3002", "result": "\u7ed9\u51fa\u7406\u8bba\u7ed3\u679c\uff0c\u8868\u660e DSE/DCE \u662f\u5bf9\u6570\u4f3c\u7136\u7684\u7d27\u81f4\u4f30\u8ba1\u5668\u4e14\u53ef\u7528\u4e8e\u65f6\u95f4\u65e0\u5173\u548c\u6761\u4ef6\u573a\u666f\uff1b\u63d0\u51fa\u7684\u6269\u5c55\u5728\u5b9e\u8df5\u4e2d\u53ef\u63d0\u9ad8\u63a8\u65ad\u80fd\u529b\u548c\u7a33\u5b9a\u6027\uff1b\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\u51c6\u786e\u6027\u4e0e\u65b9\u5dee\u7a33\u5b9a\u6027\uff1b\u4ee3\u7801\u516c\u5f00\u3002", "conclusion": "\u8be5\u4fe1\u606f\u7406\u8bba\u6846\u67b6\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86 principled \u7684\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u5de5\u5177\uff0c\u901a\u8fc7\u5bf9\u63a9\u7801\u6269\u6563\u7b49\u6269\u5c55\u589e\u5f3a\u4e86\u9002\u7528\u6027\u4e0e\u5b9e\u7528\u6027\uff0c\u63a8\u52a8\u76f8\u5173\u4efb\u52a1\u7684\u63a8\u65ad\u4e0e\u8bc4\u4f30\u3002"}}
{"id": "2510.24095", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24095", "abs": "https://arxiv.org/abs/2510.24095", "authors": ["Vedant Gupta", "Haotian Fu", "Calvin Luo", "Yiding Jiang", "George Konidaris"], "title": "Learning Parameterized Skills from Demonstrations", "comment": "Neurips 2025", "summary": "We present DEPS, an end-to-end algorithm for discovering parameterized skills\nfrom expert demonstrations. Our method learns parameterized skill policies\njointly with a meta-policy that selects the appropriate discrete skill and\ncontinuous parameters at each timestep. Using a combination of temporal\nvariational inference and information-theoretic regularization methods, we\naddress the challenge of degeneracy common in latent variable models, ensuring\nthat the learned skills are temporally extended, semantically meaningful, and\nadaptable. We empirically show that learning parameterized skills from\nmultitask expert demonstrations significantly improves generalization to unseen\ntasks. Our method outperforms multitask as well as skill learning baselines on\nboth LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers\ninterpretable parameterized skills, such as an object grasping skill whose\ncontinuous arguments define the grasp location.", "AI": {"tldr": "DEPS is an end-to-end framework that learns parameterized skills from multitask demonstrations by jointly training parameterized skill policies and a discrete meta-policy, with temporal variational inference and information-theoretic regularization to produce temporally extended, interpretable skills and improved generalization on LIBERO and MetaWorld.", "motivation": "The work targets learning reusable, parameterized skills from expert demonstrations and addresses latent-variable degeneracy that can cause collapse or non-semantic representations. The goal is to obtain temporally extended, semantically meaningful skills that can adapt across unseen tasks.", "method": "DEPS integrates parameterized skill policies with a discrete meta-policy selecting skill type and continuous parameters at each timestep. It employs temporal variational inference and information-theoretic regularization to mitigate degeneracy in latent variables, trained on multitask expert demonstrations to discover interpretable skills.", "result": "Empirically, DEPS improves generalization to unseen tasks, outperforming multitask and skill-learning baselines on LIBERO and MetaWorld benchmarks, and discovers interpretable skills (e.g., an object grasping skill with continuous grasp location parameters).", "conclusion": "DEPS provides an effective end-to-end approach for discovering parameterized, temporally extended, interpretable skills from demonstrations, addressing latent degeneracy and enabling better generalization across tasks."}}
{"id": "2510.24706", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24706", "abs": "https://arxiv.org/abs/2510.24706", "authors": ["Shuqing Li", "Jiayi Yan", "Chenyu Niu", "Jen-tse Huang", "Yun Peng", "Wenxuan Wang", "Yepang Liu", "Michael R. Lyu"], "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?", "comment": null, "summary": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.", "AI": {"tldr": "ComboBench \u662f\u4e00\u4e2a\u8bc4\u6d4b LLM \u5c06\u8bed\u4e49\u884c\u52a8\u8f6c\u5316\u4e3a VR \u8bbe\u5907\u64cd\u4f5c\u5e8f\u5217\u7684\u57fa\u51c6\uff0c\u8986\u76d6 262 \u4e2a\u573a\u666f\uff1b\u7ed3\u679c\u663e\u793a LLM \u5728\u4efb\u52a1\u5206\u89e3\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u8fc7\u7a0b\u6027\u63a8\u7406\u548c\u7a7a\u95f4\u7406\u89e3\u4e0a\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\uff0c\u4e14\u4e0d\u540c\u6e38\u620f\u4e4b\u95f4\u5dee\u5f02\u663e\u8457\uff1b\u5c11\u6837\u672c\u5b66\u4e60\u6709\u663e\u8457\u63d0\u5347\u6f5c\u529b\u3002", "motivation": "VR \u6e38\u620f\u9700\u8981\u5c06\u62bd\u8c61\u8bed\u4e49\u8f6c\u5316\u4e3a\u5177\u4f53\u7684\u8bbe\u5907\u64cd\u4f5c\uff0cLLMs \u662f\u5426\u5177\u5907\u8fd9\u79cd\u5d4c\u5165\u5f0f\u3001\u8de8\u611f\u77e5-\u8de8\u52a8\u4f5c\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5176\u80fd\u529b\u4e0e\u5c40\u9650\u3002", "method": "\u63d0\u51fa ComboBench \u57fa\u51c6\uff0c\u9009\u53d6\u56db\u6b3e\u6d41\u884c VR \u6e38\u620f\uff08Half-Life: Alyx\u3001Into the Radius\u3001Moss: Book II\u3001Vivecraft\uff09\u5171 262 \u4e2a\u60c5\u666f\uff1b\u8bc4\u4f30\u4e03\u79cd LLM\uff08GPT-3.5\u3001GPT-4\u3001GPT-4o\u3001Gemini-1.5-Pro\u3001LLaMA-3-8B\u3001Mixtral-8x7B\u3001GLM-4-Flash\uff09\u4e0e\u4eba\u5de5 ground truth \u548c\u4eba\u7c7b\u8868\u73b0\u7684\u5bf9\u6bd4\uff1b\u5206\u6790\u4efb\u52a1\u5206\u89e3\u3001\u8fc7\u7a0b\u6027\u63a8\u7406\u3001\u7a7a\u95f4\u7406\u89e3\uff0c\u5e76\u8003\u5bdf\u5c11-shot \u6837\u4f8b\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff1b\u516c\u5f00\u6570\u636e\u4e0e\u6750\u6599\u3002", "result": "\u9876\u5c16\u6a21\u578b Gemini-1.5-Pro \u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u4efb\u52a1\u5206\u89e3\u80fd\u529b\uff0c\u4f46\u5728\u8fc7\u7a0b\u6027\u63a8\u7406\u4e0e\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u4ecd\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\uff1b\u4e0d\u540c\u6e38\u620f\u7684\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u63d0\u793a\u4e92\u52a8\u590d\u6742\u6027\u5bf9\u80fd\u529b\u7684\u5f71\u54cd\uff1b\u5c11-shot \u793a\u4f8b\u80fd\u663e\u8457\u63d0\u5347\u8868\u73b0\uff0c\u663e\u793a\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u9002\u5e94\u6027\u63d0\u5347\u6f5c\u529b\uff1b\u603b\u4f53\u4e0a\u5c1a\u672a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u65b9\u5411\u3002", "conclusion": "LLMs \u5177\u5907\u5c06\u8bed\u4e49\u884c\u52a8\u8f6c\u5316\u4e3a VR \u64cd\u4f5c\u7684\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u6c34\u5e73\u4e0d\u8db3\u4ee5\u66ff\u4ee3\u4eba\u7c7b\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u548c\u8fc7\u7a0b\u63a8\u7406\u65b9\u9762\uff1b\u901a\u8fc7\u9488\u5bf9\u6027\u63d0\u793a\u548c\u5c11\u6837\u672c\u5b66\u4e60\u53ef\u4ee5\u63d0\u5347\u80fd\u529b\uff0c\u672a\u6765\u5de5\u4f5c\u53ef\u805a\u7126\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u3001\u8de8\u6e38\u620f\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u6269\u5c55\u5230\u66f4\u591a VR \u573a\u666f\u4e0e\u786c\u4ef6\u3002"}}
{"id": "2510.24120", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24120", "abs": "https://arxiv.org/abs/2510.24120", "authors": ["Ziyu Liu", "Yijing Liu", "Jianfei Yuan", "Minzhi Yan", "Le Yue", "Honghui Xiong", "Yi Yang"], "title": "Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation", "comment": null, "summary": "Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance\nretrieval in Large Language Model (LLM)-based question answering. It is\nespecially beneficial in domains such as biomedicine, law, and political\nscience, where effective retrieval often involves multi-hop reasoning over\nproprietary documents. However, these methods demand numerous LLM calls to\nextract entities and relations from text chunks, incurring prohibitive costs at\nscale. Through a carefully designed ablation study, we observe that certain\nwords (termed concepts) and their associated documents are more important.\nBased on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its\ncore comprises a chunk selection method and an LLM-independent concept graph.\nThe former selects salient document chunks to reduce KG construction costs; the\nlatter closes knowledge gaps introduced by chunk selection at zero cost.\nEvaluations on multiple real-world datasets show that G2ConS outperforms all\nbaselines in construction cost, retrieval effectiveness, and answering quality.", "AI": {"tldr": "G2ConS \u901a\u8fc7\u9009\u62e9\u5173\u952e\u6587\u6863\u5757\u4e0e\u6784\u5efa\u4e00\u4e2a\u4e0e LLM \u65e0\u5173\u7684\u6982\u5ff5\u56fe\uff0c\u663e\u8457\u964d\u4f4e\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6210\u672c\uff0c\u540c\u65f6\u63d0\u5347\u68c0\u7d22\u6548\u679c\u548c\u95ee\u7b54\u8d28\u91cf\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5728\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u95ee\u7b54\uff08RAG\uff09\u4e2d\uff0cKG \u6784\u5efa\u9700\u5927\u91cf\u5bf9\u6587\u672c\u5757\u8fdb\u884c\u5b9e\u4f53\u548c\u5173\u7cfb\u62bd\u53d6\uff0c\u5bfc\u81f4\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u6210\u672c\u9ad8\u4f01\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u53d1\u73b0\uff0c\u53ea\u6709\u90e8\u5206\u6982\u5ff5\u53ca\u5176\u76f8\u5173\u6587\u6863\u6700\u4e3a\u5173\u952e\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u8bbe\u8ba1\u9ad8\u6548\u7684\u5757\u9009\u62e9\u4e0e\u8865\u5145\u56fe\u6765\u964d\u4f4e\u6210\u672c\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u63d0\u51fa Graph-Guided Concept Selection\uff08G2ConS\uff09\uff0c\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff1a\u4e00\u662f chunk selection\uff0c\u6311\u9009 salient \u6587\u6863\u5757\u4ee5\u964d\u4f4e KG \u6784\u5efa\u6210\u672c\uff1b\u4e8c\u662f LLM \u65e0\u5173\u7684 concept graph\uff0c\u7528\u4ee5\u5728\u96f6\u6210\u672c\u4e0b\u5f25\u8865\u56e0\u5757\u9009\u62e9\u5f15\u5165\u7684\u77e5\u8bc6\u7a7a\u7f3a\u3002\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\u4e0e\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5728\u6784\u5efa\u6210\u672c\u3001\u68c0\u7d22\u6709\u6548\u6027\u548c\u95ee\u7b54\u8d28\u91cf\u65b9\u9762\uff0cG2ConS \u5747\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6709\u9009\u62e9\u6027\u7684\u6587\u6863\u5757\u63d0\u53d6\u4e0e\u6982\u5ff5\u56fe\uff0cG2ConS \u5b9e\u73b0\u4e86\u66f4\u4f4e\u6210\u672c\u7684 KG \u6784\u5efa\u5e76\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u68c0\u7d22\u4e0e\u95ee\u7b54\u6027\u80fd\uff0c\u4e3a\u57fa\u4e8e KG \u7684 LLM \u95ee\u7b54\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24707", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24707", "abs": "https://arxiv.org/abs/2510.24707", "authors": ["Juraj Juraska", "Tobias Domhan", "Mara Finkelstein", "Tetsuji Nakagawa", "Geza Kovacs", "Daniel Deutsch", "Pidong Wang", "Markus Freitag"], "title": "MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25 Evaluation Shared Task", "comment": "Accepted to WMT25", "summary": "In this paper, we present our submissions to the unified WMT25 Translation\nEvaluation Shared Task. For the Quality Score Prediction subtask, we create a\nnew generation of MetricX with improvements in the input format and the\ntraining protocol, while for the Error Span Detection subtask we develop a new\nmodel, GemSpanEval, trained to predict error spans along with their severities\nand categories. Both systems are based on the state-of-the-art multilingual\nopen-weights model Gemma 3, fine-tuned on publicly available WMT data. We\ndemonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture\nwith a regression head on top, can be trained to effectively predict both MQM\nand ESA quality scores, and significantly outperforms its predecessor. Our\ndecoder-only GemSpanEval model, on the other hand, we show to be competitive in\nerror span detection with xCOMET, a strong encoder-only sequence-tagging\nbaseline. With error span detection formulated as a generative task, we\ninstruct the model to also output the context for each predicted error span,\nthus ensuring that error spans are identified unambiguously.", "AI": {"tldr": "\u63d0\u51faMetricX-25\u4e0eGemSpanEval\u7528\u4e8eWMT25\u7ffb\u8bd1\u8bc4\u4f30\u4efb\u52a1\uff0c\u57fa\u4e8eGemma 3\u5fae\u8c03\uff0cMetricX-25\u5728MQM\u4e0eESA\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u524d\u4ee3\uff0cGemSpanEval\u4e0exCOMET\u7ade\u4e89\u4e14\u901a\u8fc7\u751f\u6210\u5f0f\u8f93\u51fa\u4e0a\u4e0b\u6587\u4f7f\u9519\u8bef\u533a\u95f4\u66f4\u6e05\u6670\u3002", "motivation": "\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7684\u4e24\u4e2a\u5b50\u4efb\u52a1\u7684\u6027\u80fd\uff1a\u8d28\u91cf\u5206\u6570\u9884\u6d4b\u4e0e\u9519\u8bef\u533a\u95f4\u68c0\u6d4b\uff1b\u5229\u7528\u5f3a\u5927\u591a\u8bed\u8a00\u5f00\u6e90\u6a21\u578bGemma 3\uff0c\u5e76\u5728\u516c\u5f00WMT\u6570\u636e\u4e0a\u5fae\u8c03\u3002", "method": "MetricX-25\uff1a\u5c06Gemma 3\u6539\u4e3a\u7f16\u7801\u5668\u67b6\u6784\u5e76\u5728\u4e0a\u65b9\u52a0\u56de\u5f52\u5934\uff1bGemSpanEval\uff1a\u91c7\u7528\u89e3\u7801\u5668\u6a21\u578b\uff0c\u8bad\u7ec3\u4ee5\u9884\u6d4b\u9519\u8bef\u533a\u95f4\u3001\u4e25\u91cd\u6027\u548c\u7c7b\u522b\uff0c\u5e76\u4ee5\u751f\u6210\u65b9\u5f0f\u8f93\u51fa\u6bcf\u4e2a\u533a\u95f4\u7684\u4e0a\u4e0b\u6587\u3002", "result": "MetricX-25\u663e\u8457\u8d85\u8d8a\u524d\u4ee3\uff1bGemSpanEval\u5728\u9519\u8bef\u533a\u6bb5\u68c0\u6d4b\u65b9\u9762\u4e0exCOMET\u7ade\u4e89\uff1b\u5c06\u9519\u533a\u68c0\u6d4b\u8bbe\u4e3a\u751f\u6210\u4efb\u52a1\u5e76\u8f93\u51fa\u4e0a\u4e0b\u6587\uff0c\u786e\u4fdd\u533a\u6bb5\u7684\u65e0\u6b67\u4e49\u6027\u3002", "conclusion": "\u8bc1\u660e\u57fa\u4e8eGemma 3\u7684\u53cc\u4efb\u52a1\u67b6\u6784\u5728WMT25\u4e0a\u5177\u6709\u8f83\u597d\u8868\u73b0\uff0c\u751f\u6210\u5f0f\u9519\u8bef\u533a\u95f4\u8f93\u51fa\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u4e0e\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24160", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24160", "abs": "https://arxiv.org/abs/2510.24160", "authors": ["Aiqing Zhu", "Beatrice W. Soh", "Grigorios A. Pavliotis", "Qianxiao Li"], "title": "Identifiable learning of dissipative dynamics", "comment": null, "summary": "Complex dissipative systems appear across science and engineering, from\npolymers and active matter to learning algorithms. These systems operate far\nfrom equilibrium, where energy dissipation and time irreversibility are key to\ntheir behavior, but are difficult to quantify from data. Learning accurate and\ninterpretable models of such dynamics remains a major challenge: the models\nmust be expressive enough to describe diverse processes, yet constrained enough\nto remain physically meaningful and mathematically identifiable. Here, we\nintroduce I-OnsagerNet, a neural framework that learns dissipative stochastic\ndynamics directly from trajectories while ensuring both interpretability and\nuniqueness. I-OnsagerNet extends the Onsager principle to guarantee that the\nlearned potential is obtained from the stationary density and that the drift\ndecomposes cleanly into time-reversible and time-irreversible components, as\ndictated by the Helmholtz decomposition. Our approach enables us to calculate\nthe entropy production and to quantify irreversibility, offering a principled\nway to detect and quantify deviations from equilibrium. Applications to polymer\nstretching in elongational flow and to stochastic gradient Langevin dynamics\nreveal new insights, including super-linear scaling of barrier heights and\nsub-linear scaling of entropy production rates with the strain rate, and the\nsuppression of irreversibility with increasing batch size. I-OnsagerNet thus\nestablishes a general, data-driven framework for discovering and interpreting\nnon-equilibrium dynamics.", "AI": {"tldr": "I-OnsagerNet learns dissipative stochastic dynamics from trajectory data, enforcing Onsager symmetry and Helmholtz decomposition for interpretable, unique models; enables entropy production and irreversibility quantification; reveals scaling insights in polymer flows and Langevin dynamics; provides a general framework for non-equilibrium dynamics.", "motivation": "Non-equilibrium dissipative systems are ubiquitous but hard to quantify; existing models struggle with expressivity vs. physical constraints and identifiability.", "method": "Extend Onsager principle to learn potential from stationary density; decompose drift into reversible (gradient of potential) and irreversible (divergence-free) parts via Helmholtz decomposition; enforce uniqueness; compute entropy production; apply to polymer stretching and SGD-Langevin dynamics.", "result": "Obtains entropy production and irreversibility measures; reveals super-linear barrier height scaling with strain rate, sub-linear entropy production rate scaling with strain rate, and irreversibility suppression with larger batch sizes.", "conclusion": "Provides a general, data-driven framework for discovering and interpreting non-equilibrium dynamics with interpretability and uniqueness guarantees."}}
{"id": "2510.24173", "categories": ["cs.LG", "cs.NA", "math.DS", "math.NA", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.24173", "abs": "https://arxiv.org/abs/2510.24173", "authors": ["Yiheng Du", "Aditi S. Krishnapriyan"], "title": "EddyFormer: Accelerated Neural Simulations of Three-Dimensional Turbulence at Scale", "comment": "NeurIPS 2025", "summary": "Computationally resolving turbulence remains a central challenge in fluid\ndynamics due to its multi-scale interactions. Fully resolving large-scale\nturbulence through direct numerical simulation (DNS) is computationally\nprohibitive, motivating data-driven machine learning alternatives. In this\nwork, we propose EddyFormer, a Transformer-based spectral-element (SEM)\narchitecture for large-scale turbulence simulation that combines the accuracy\nof spectral methods with the scalability of the attention mechanism. We\nintroduce an SEM tokenization that decomposes the flow into grid-scale and\nsubgrid-scale components, enabling capture of both local and global features.\nWe create a new three-dimensional isotropic turbulence dataset and train\nEddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x\nspeedup over DNS. When applied to unseen domains up to 4x larger than in\ntraining, EddyFormer preserves accuracy on physics-invariant metrics-energy\nspectra, correlation functions, and structure functions-showing domain\ngeneralization. On The Well benchmark suite of diverse turbulent flows,\nEddyFormer resolves cases where prior ML models fail to converge, accurately\nreproducing complex dynamics across a wide range of physical conditions.", "AI": {"tldr": "EddyFormer \u5c06 Transformer \u4e0e\u8c31\u5143\u65b9\u6cd5\u7ed3\u5408\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6e4d\u6d41\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\uff0c\u5b9e\u73b0 DNS \u7ea7\u7cbe\u5ea6\uff08256^3\uff09\u5e76\u6bd4 DNS \u5feb\u7ea6 30 \u500d\uff0c\u4e14\u5177\u5907\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u5728\u591a\u79cd\u6d41\u573a\u57fa\u51c6\u4e0a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u76f4\u63a5\u6570\u503c\u6a21\u62df DNS \u5728\u5927\u5c3a\u5ea6\u6e4d\u6d41\u4e2d\u6210\u672c\u6781\u9ad8\uff1b\u9700\u8981\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u4e0e\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa SEM Tokenization\uff0c\u5c06\u6d41\u573a\u5206\u89e3\u4e3a\u7f51\u683c\u5c3a\u5ea6\u4e0e\u5b50\u7f51\u683c\u5c3a\u5ea6\uff0c\u7ed3\u5408\u8c31\u65b9\u6cd5\u7684\u5c40\u90e8\u51c6\u786e\u6027\u4e0e Transformer \u7684\u5168\u5c40\u5efa\u6a21\u80fd\u529b\uff1b\u6784\u5efa\u4e09\u7ef4\u5404\u5411\u540c\u6027\u6e4d\u6d41\u6570\u636e\u96c6\uff0c\u8bad\u7ec3 EddyFormer\uff0c\u5728 256^3 \u5206\u8fa8\u7387\u4e0b\u8fbe\u5230 DNS \u7ea7\u7cbe\u5ea6\uff0c\u4e14\u5bf9\u6700\u5927 4x \u7684\u57df\u5927\u5c0f\u4fdd\u6301\u7cbe\u5ea6\uff1b\u5728 Well \u57fa\u51c6\u7b49\u591a\u7ec4\u573a\u666f\u4e0a\u663e\u793a\u51fa\u826f\u597d\u57df\u6cdb\u5316\u4e0e\u5728\u6b64\u524d\u6a21\u578b\u96be\u4ee5\u6536\u655b\u7684\u6848\u4f8b\u4e2d\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u76f8\u8f83 DNS \u5b9e\u73b0\u7ea6 30x \u7684\u52a0\u901f\uff0c\u5e76\u5728\u80fd\u8c31\u3001\u76f8\u5173\u51fd\u6570\u548c\u7ed3\u6784\u51fd\u6570\u7b49\u7269\u7406\u4e0d\u53d8\u91cf\u6307\u6807\u4e0a\u4fdd\u6301\u826f\u597d\u7cbe\u5ea6\uff1b\u5bf9\u672a\u89c1\u57df\u5177\u5907\u8f83\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u7269\u7406\u6761\u4ef6\u4e0b\u4e5f\u80fd\u51c6\u786e\u518d\u73b0\u590d\u6742\u52a8\u529b\u5b66\uff1b\u5728 Well \u57fa\u51c6\u4e0a\u4f18\u4e8e\u6b64\u524d\u7684 ML \u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6536\u655b\u6027\u95ee\u9898\u3002", "conclusion": "\u8bc1\u660e\u57fa\u4e8e Transformer \u7684 SEM \u67b6\u6784\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6e4d\u6d41\u4eff\u771f\u4e0e\u826f\u597d\u53ef\u6269\u5c55\u6027\uff0c\u663e\u793a\u4e86\u5c06\u673a\u5668\u5b66\u4e60\u4e0e\u7269\u7406\u4e00\u81f4\u6027\u7ed3\u5408\u4ee5\u89e3\u51b3\u5927\u5c3a\u5ea6\u6da1\u6d41\u4eff\u771f\u7684\u6f5c\u529b\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.24180", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24180", "abs": "https://arxiv.org/abs/2510.24180", "authors": ["Arpita Kundu", "Joyita Chakraborty", "Anindita Desarkar", "Aritra Sen", "Srushti Anil Patil", "Vishwanathan Raman"], "title": "V-SAT: Video Subtitle Annotation Tool", "comment": null, "summary": "The surge of audiovisual content on streaming platforms and social media has\nheightened the demand for accurate and accessible subtitles. However, existing\nsubtitle generation methods primarily speech-based transcription or OCR-based\nextraction suffer from several shortcomings, including poor synchronization,\nincorrect or harmful text, inconsistent formatting, inappropriate reading\nspeeds, and the inability to adapt to dynamic audio-visual contexts. Current\napproaches often address isolated issues, leaving post-editing as a\nlabor-intensive and time-consuming process. In this paper, we introduce V-SAT\n(Video Subtitle Annotation Tool), a unified framework that automatically\ndetects and corrects a wide range of subtitle quality issues. By combining\nLarge Language Models(LLMs), Vision-Language Models (VLMs), Image Processing,\nand Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from\nboth audio and video. Subtitle quality improved, with the SUBER score reduced\nfrom 9.6 to 3.54 after resolving all language mode issues and F1-scores of\n~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality\nresults, providing the first comprehensive solution for robust subtitle\nannotation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u5b57\u5e55\u6ce8\u91ca\u5de5\u5177V-SAT\uff0c\u901a\u8fc7\u878d\u5408LLM\u3001VLM\u3001\u56fe\u50cf\u5904\u7406\u548cASR\uff0c\u81ea\u52a8\u68c0\u6d4b\u5e76\u7ea0\u6b63\u5b57\u5e55\u8d28\u91cf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5b57\u5e55\u8d28\u91cf\u5e76\u964d\u4f4e\u4eba\u5de5\u540e\u671f\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\u591a\u805a\u7126\u4e8e\u8bed\u97f3\u8f6c\u5199\u6216OCR\u63d0\u53d6\uff0c\u5b58\u5728\u540c\u6b65\u6027\u5dee\u3001\u6587\u672c\u4e0d\u51c6\u6216\u6709\u5bb3\u3001\u683c\u5f0f\u4e0d\u4e00\u81f4\u3001\u9605\u8bfb\u901f\u5ea6\u4e0d\u5f53\u3001\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u97f3\u89c6\u9891\u573a\u666f\u7b49\u95ee\u9898\uff0c\u540e\u671f\u7f16\u8f91\u8017\u65f6\u8017\u529b\uff0c\u7f3a\u4e4f\u5bf9\u591a\u79cd\u5b57\u5e55\u8d28\u91cf\u95ee\u9898\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efaV-SAT\u6846\u67b6\uff0c\u7ed3\u5408LLMs\u3001VLMs\u3001\u56fe\u50cf\u5904\u7406\u548cASR\uff0c\u5145\u5206\u5229\u7528\u97f3\u9891\u4e0e\u89c6\u9891\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u81ea\u52a8\u68c0\u6d4b\u5e76\u4fee\u6b63\u591a\u7c7b\u5b57\u5e55\u8d28\u91cf\u95ee\u9898\uff0c\u6db5\u76d6\u8bed\u8a00\u548c\u56fe\u50cf\u6a21\u6001\u7684\u95ee\u9898\uff0c\u91c7\u7528\u4eba\u673a\u534f\u540c\u7684\u6821\u9a8c\u6765\u786e\u4fdd\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002", "result": "\u5b57\u5e55\u8d28\u91cf\u63d0\u5347\u663e\u8457\uff1aSUBER\u8bc4\u5206\u4ece9.6\u4e0b\u964d\u81f33.54\uff08\u89e3\u51b3\u6240\u6709\u8bed\u8a00\u6a21\u6001\u95ee\u9898\u540e\uff09\uff0c\u56fe\u50cf\u6a21\u6001\u95ee\u9898\u7684F1-score\u7ea60.80\uff1b\u5e76\u901a\u8fc7\u4eba\u673a\u4ea4\u4e92\u9a8c\u8bc1\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u5168\u9762\u7684\u9c81\u68d2\u5b57\u5e55\u6807\u6ce8\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "V-SAT\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u79cd\u5b57\u5e55\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u5347\u5b57\u5e55\u8d28\u91cf\u5e76\u964d\u4f4e\u540e\u671f\u5de5\u4f5c\u91cf\uff0c\u5177\u5907\u6210\u4e3a\u9c81\u68d2\u5b57\u5e55\u6ce8\u91ca\u7684\u5168\u9762\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24200", "categories": ["cs.LG", "cs.CR", "cs.DC", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.24200", "abs": "https://arxiv.org/abs/2510.24200", "authors": ["Alexander Bakarsky", "Dimitar I. Dimitrov", "Maximilian Baader", "Martin Vechev"], "title": "SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning", "comment": "Published at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)", "summary": "Federated Learning has seen an increased deployment in real-world scenarios\nrecently, as it enables the distributed training of machine learning models\nwithout explicit data sharing between individual clients. Yet, the introduction\nof the so-called gradient inversion attacks has fundamentally challenged its\nprivacy-preserving properties. Unfortunately, as these attacks mostly rely on\ndirect data optimization without any formal guarantees, the vulnerability of\nreal-world systems remains in dispute and requires tedious testing for each new\nfederated deployment. To overcome these issues, recently the SPEAR attack was\nintroduced, which is based on a theoretical analysis of the gradients of linear\nlayers with ReLU activations. While SPEAR is an important theoretical\nbreakthrough, the attack's practicality was severely limited by its exponential\nruntime in the batch size b. In this work, we fill this gap by applying\nState-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the\nproblem of gradient inversion on linear layers with ReLU activations tractable.\nOur experiments demonstrate that our new attack, SPEAR++, retains all desirable\nproperties of SPEAR, such as robustness to DP noise and FedAvg aggregation,\nwhile being applicable to 10x bigger batch sizes.", "AI": {"tldr": "SPEAR++ is a scalable gradient-inversion attack for linear layers with ReLU, enabling reconstruction for batch sizes 10\u00d7 larger while preserving robustness to DP noise and FedAvg.", "motivation": "Federated Learning privacy is threatened by gradient inversion. The original SPEAR attack was theoretically strong but impractically slow (exponential in batch size). There is a need for a scalable, practically applicable attack to evaluate real-world deployments.", "method": "Introduce sparsely-used dictionary learning techniques to the gradient-inversion problem for networks with linear layers and ReLU activations, building on SPEAR's theoretical framework to achieve tractable reconstruction at large batch sizes, and showing resilience to DP noise and FedAvg aggregation.", "result": "Empirical results show SPEAR++ retains SPEAR\u2019s desirable properties (robustness to differential privacy noise and FedAvg aggregation) while enabling the attack at batch sizes up to 10\u00d7 larger than before.", "conclusion": "SPEAR++ makes gradient-inversion attacks on common FL architectures practical at realistic batch sizes, highlighting persistent privacy risks in federated settings and motivating stronger defenses."}}
{"id": "2510.24216", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24216", "abs": "https://arxiv.org/abs/2510.24216", "authors": ["Fan Xu", "Hao Wu", "Kun Wang", "Nan Wang", "Qingsong Wen", "Xian Wu", "Wei Gong", "Xibin Zhao"], "title": "Unlocking Out-of-Distribution Generalization in Dynamics through Physics-Guided Augmentation", "comment": null, "summary": "In dynamical system modeling, traditional numerical methods are limited by\nhigh computational costs, while modern data-driven approaches struggle with\ndata scarcity and distribution shifts. To address these fundamental\nlimitations, we first propose SPARK, a physics-guided quantitative augmentation\nplugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate\nphysical parameters into a physics-rich discrete state dictionary. This state\ndictionary then acts as a structured dictionary of physical states, enabling\nthe creation of new, physically-plausible training samples via principled\ninterpolation in the latent space. Further, for downstream prediction, these\naugmented representations are seamlessly integrated with a Fourier-enhanced\nGraph ODE, a combination designed to robustly model the enriched data\ndistribution while capturing long-term temporal dependencies. Extensive\nexperiments on diverse benchmarks demonstrate that SPARK significantly\noutperforms state-of-the-art baselines, particularly in challenging\nout-of-distribution scenarios and data-scarce regimes, proving the efficacy of\nour physics-guided augmentation paradigm.", "AI": {"tldr": "SPARK\u662f\u4e00\u79cd\u7269\u7406\u5f15\u5bfc\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u5c06\u7269\u7406\u53c2\u6570\u7f16\u7801\u8fdb\u79bb\u6563\u7269\u7406\u6001\u5b57\u5178\uff0c\u5229\u7528\u8be5\u5b57\u5178\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u6709\u610f\u4e49\u63d2\u503c\u751f\u6210\u65b0\u6837\u672c\uff0c\u5e76\u5c06\u589e\u5f3a\u8868\u793a\u4e0eFourier\u589e\u5f3a\u7684Graph ODE\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u52a8\u529b\u7cfb\u7edf\u7684\u9c81\u68d2\u5efa\u6a21\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u548c\u5206\u5e03\u504f\u79fb\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u4e0e\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u8106\u5f31\u6027\uff1b\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u7684\u589e\u5e7f\u8303\u5f0f\u63d0\u5347\u6570\u636e\u5206\u5e03\u4e0e\u957f\u671f\u65f6\u5e8f\u5efa\u6a21\u80fd\u529b\u3002", "method": "SPARK\u4f7f\u7528\u91cd\u5efa\u81ea\u7f16\u7801\u5668\u5c06\u7269\u7406\u53c2\u6570\u7eb3\u5165\u4e00\u4e2a\u7269\u7406\u4e30\u5bcc\u7684\u79bb\u6563\u72b6\u6001\u5b57\u5178\uff0c\u5f62\u6210\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u7269\u7406\u6001\u5b57\u5178\uff1b\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u6709\u539f\u5219\u7684\u63d2\u503c\u751f\u6210\u65b0\u7684\u3001\u7269\u7406\u4e0a\u53ef\u884c\u7684\u8bad\u7ec3\u6837\u672c\u3002\u4e0b\u6e38\u9884\u6d4b\u9636\u6bb5\u5c06\u589e\u5f3a\u540e\u7684\u8868\u793a\u4e0eFourier\u589e\u5f3a\u7684Graph ODE\u65e0\u7f1d\u96c6\u6210\uff0c\u4ee5\u66f4\u9c81\u68d2\u5730\u5efa\u6a21\u4e30\u5bcc\u7684\u6570\u636e\u5206\u5e03\u5e76\u6355\u6349\u957f\u671f\u4f9d\u8d56\u3002", "result": "\u5728\u591a\u6837\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSPARK\u663e\u8457\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u60c5\u666f\u548c\u6570\u636e\u7a00\u7f3a\u9636\u6bb5\uff0c\u8bc1\u660e\u4e86\u7269\u7406\u5f15\u5bfc\u589e\u5e7f\u8303\u5f0f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u9a8c\u8bc1\u4e86\u7269\u7406\u5f15\u5bfc\u7684\u589e\u5e7f\u7b56\u7565\u5728\u52a8\u529b\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u6570\u636e\u53d7\u9650\u4e0e\u5206\u5e03\u53d8\u5316\u6761\u4ef6\u4e0b\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u7684\u65b0\u8def\u5f84\u3002"}}
{"id": "2510.24217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24217", "abs": "https://arxiv.org/abs/2510.24217", "authors": ["Alisher Turubayev", "Anna Shopova", "Fabian Lange", "Mahmut Kamalak", "Paul Mattes", "Victoria Ayvasky", "Bert Arnrich", "Bjarne Pfitzner", "Robin P. van de Water"], "title": "Closing Gaps: An Imputation Analysis of ICU Vital Signs", "comment": "Preprint", "summary": "As more Intensive Care Unit (ICU) data becomes available, the interest in\ndeveloping clinical prediction models to improve healthcare protocols\nincreases. However, the lack of data quality still hinders clinical prediction\nusing Machine Learning (ML). Many vital sign measurements, such as heart rate,\ncontain sizeable missing segments, leaving gaps in the data that could\nnegatively impact prediction performance. Previous works have introduced\nnumerous time-series imputation techniques. Nevertheless, more comprehensive\nwork is needed to compare a representative set of methods for imputing ICU\nvital signs and determine the best practice. In reality, ad-hoc imputation\ntechniques that could decrease prediction accuracy, like zero imputation, are\nstill used. In this work, we compare established imputation techniques to guide\nresearchers in improving the performance of clinical prediction models by\nselecting the most accurate imputation technique. We introduce an extensible\nand reusable benchmark with currently 15 imputation and 4 amputation methods,\ncreated for benchmarking on major ICU datasets. We hope to provide a\ncomparative basis and facilitate further ML development to bring more models\ninto clinical practice.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6bd4\u8f8315\u79cd\u63d2\u8865\uff08\u7f3a\u5931\u503c\u586b\u8865\uff09\u65b9\u6cd5\u548c4\u79cd\u201c\u622a\u65ad/\u5254\u9664\u201d\u65b9\u6cd5\u5728ICU\u5173\u952e\u751f\u547d\u4f53\u5f81\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u65e8\u5728\u63d0\u5347\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u7684\u8d28\u91cf\u3002", "motivation": "ICU\u6570\u636e\u7f3a\u5931\u5e7f\u6cdb\u4e14\u6570\u636e\u8d28\u91cf\u5bf9ML\u9884\u6d4b\u5f71\u54cd\u663e\u8457\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u6bd4\u4e0d\u8db3\uff0c\u5b58\u5728\u96f6\u586b\u5145\u7b49\u4e0d\u826f\u505a\u6cd5\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u57fa\u51c6\u6765\u6307\u5bfc\u9009\u62e9\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u91cd\u7528\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u5305\u542b15\u79cd\u63d2\u8865\u65b9\u6cd5\u548c4\u79cd\u5220\u51cf/\u622a\u65ad\u65b9\u6cd5\uff0c\u5728\u4e3b\u8981ICU\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u6458\u8981\u672a\u7ed9\u51fa\u5177\u4f53\u7ed3\u679c\uff1b\u4e3b\u8d21\u732e\u662f\u63d0\u4f9b\u57fa\u51c6\u6846\u67b6\u548c\u5bf9\u6bd4\u7684\u65b9\u5411\u6307\u5f15\uff0c\u5f3a\u8c03\u9009\u62e9\u5408\u9002\u7684\u63d2\u8865\u65b9\u6cd5\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u4ee5\u7edf\u4e00\u8bc4\u4f30ICU\u63d2\u8865\u65b9\u6cd5\uff0c\u4fc3\u8fdbML\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.24233", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24233", "abs": "https://arxiv.org/abs/2510.24233", "authors": ["Antoine Szatkownik", "Aur\u00e9lien Decelle", "Beatriz Seoane", "Nicolas Bereux", "L\u00e9o Planche", "Guillaume Charpiat", "Burak Yelmen", "Flora Jay", "Cyril Furtlehner"], "title": "PRIVET: Privacy Metric Based on Extreme Value Theory", "comment": null, "summary": "Deep generative models are often trained on sensitive data, such as genetic\nsequences, health data, or more broadly, any copyrighted, licensed or protected\ncontent. This raises critical concerns around privacy-preserving synthetic\ndata, and more specifically around privacy leakage, an issue closely tied to\noverfitting. Existing methods almost exclusively rely on global criteria to\nestimate the risk of privacy failure associated to a model, offering only\nquantitative non interpretable insights. The absence of rigorous evaluation\nmethods for data privacy at the sample-level may hinder the practical\ndeployment of synthetic data in real-world applications. Using extreme value\nstatistics on nearest-neighbor distances, we propose PRIVET, a generic\nsample-based, modality-agnostic algorithm that assigns an individual privacy\nleak score to each synthetic sample. We empirically demonstrate that PRIVET\nreliably detects instances of memorization and privacy leakage across diverse\ndata modalities, including settings with very high dimensionality, limited\nsample sizes such as genetic data and even under underfitting regimes. We\ncompare our method to existing approaches under controlled settings and show\nits advantage in providing both dataset level and sample level assessments\nthrough qualitative and quantitative outputs. Additionally, our analysis\nreveals limitations in existing computer vision embeddings to yield\nperceptually meaningful distances when identifying near-duplicate samples.", "AI": {"tldr": "A per-sample privacy leakage scoring method for synthetic data, PRIVET, using extreme value statistics on nearest-neighbor distances to detect memorization across modalities, offering dataset- and sample-level privacy assessment.", "motivation": "Current privacy evaluation methods for synthetic data rely mainly on global criteria and provide non-interpretable, dataset-level insights. There is a need for rigorous, interpretable, sample-level privacy evaluation to enable practical deployment of privacy-preserving synthetic data.", "method": "PRIVET is a generic, modality-agnostic algorithm that assigns an individual privacy leak score to each synthetic sample by applying extreme value statistics to nearest-neighbor distances.", "result": "Empirically, PRIVET reliably detects memorization and privacy leakage across diverse data modalities, including very high-dimensional data, small sample sizes (e.g., genetic data), and underfitting regimes. It provides both dataset-level and sample-level assessments with qualitative and quantitative outputs; also highlights limitations of existing computer vision embeddings in yielding perceptually meaningful distances for near-duplicate identification.", "conclusion": "PRIVET offers a practical, interpretable, sample-level privacy evaluation tool for synthetic data, complementing global criteria and aiding safer deployment, while also indicating limitations in CV embeddings for near-duplicate detection."}}
{"id": "2510.24234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24234", "abs": "https://arxiv.org/abs/2510.24234", "authors": ["Ludovic Schwartz", "Hamish Flynn", "Gergely Neu"], "title": "Sparse Optimistic Information Directed Sampling", "comment": null, "summary": "Many high-dimensional online decision-making problems can be modeled as\nstochastic sparse linear bandits. Most existing algorithms are designed to\nachieve optimal worst-case regret in either the data-rich regime, where\npolynomial depen- dence on the ambient dimension is unavoidable, or the\ndata-poor regime, where dimension-independence is possible at the cost of worse\ndependence on the num- ber of rounds. In contrast, the sparse Information\nDirected Sampling (IDS) algo- rithm satisfies a Bayesian regret bound that has\nthe optimal rate in both regimes simultaneously. In this work, we explore the\nuse of Sparse Optimistic Informa- tion Directed Sampling (SOIDS) to achieve the\nsame adaptivity in the worst-case setting, without Bayesian assumptions.\nThrough a novel analysis that enables the use of a time-dependent learning\nrate, we show that SOIDS can optimally balance information and regret. Our\nresults extend the theoretical guarantees of IDS, pro- viding the first\nalgorithm that simultaneously achieves optimal worst-case regret in both the\ndata-rich and data-poor regimes. We empirically demonstrate the good\nperformance of SOIDS.", "AI": {"tldr": "SOIDS provides optimal worst-case regret in both data-rich and data-poor regimes for sparse linear bandits, without Bayesian assumptions, by using a time-dependent learning rate to balance information and regret.", "motivation": "To obtain adaptivity in the worst-case setting between regimes and remove reliance on Bayesian priors while maintaining optimal regret in sparse online decision problems.", "method": "Develop sparse optimistic IDS (SOIDS) with a time-varying learning rate; extend information-directed sampling to the sparse setting and analyze its regret under worst-case adversarial sequences.", "result": "Theoretical guarantees showing that SOIDS achieves optimal worst-case regret in both regimes; first algorithm to do so; empirical results demonstrating good performance.", "conclusion": "SOIDS closes the gap between data-rich and data-poor regimes in worst-case analysis for sparse linear bandits and provides practical adaptive performance without Bayesian assumptions."}}
{"id": "2510.24235", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24235", "abs": "https://arxiv.org/abs/2510.24235", "authors": ["Ai Jian", "Jingqing Ruan", "Xing Ma", "Dailin Li", "QianLin Zhou", "Ke Zeng", "Xunliang Cai"], "title": "PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling", "comment": null, "summary": "Reward models (RMs) are central to reinforcement learning from human feedback\n(RLHF), providing the critical supervision signals that align large language\nmodels (LLMs) with human preferences. While generative reward models (GRMs)\noffer greater interpretability than traditional scalar RMs, current training\nparadigms remain limited. Pair-wise methods rely on binary good-versus-bad\nlabels, which cause mismatches for point-wise inference and necessitate complex\npairing strategies for effective application in RLHF. On the other hand,\npoint-wise methods require more elaborate absolute labeling with rubric-driven\ncriteria, resulting in poor adaptability and high annotation costs. In this\nwork, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a\nunified framework that integrates a preference-aware reward (PAR) mechanism\nwith dynamic rubric adaptation. PaTaRM leverages relative preference\ninformation from pairwise data to construct robust point-wise training signals,\neliminating the need for explicit point-wise labels. Simultaneously, it employs\na task-adaptive rubric system that flexibly generates evaluation criteria for\nboth global task consistency and instance-specific fine-grained reasoning. This\ndesign enables efficient, generalizable, and interpretable reward modeling for\nRLHF. Extensive experiments show that PaTaRM achieves an average relative\nimprovement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B\nmodels. Furthermore, PaTaRM boosts downstream RLHF performance, with an average\nimprovement of 13.6% across IFEval and InFoBench benchmarks, confirming its\neffectiveness and robustness. Our code is available at\nhttps://github.com/JaneEyre0530/PaTaRM.", "AI": {"tldr": "PaTaRM\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u5956\u52b1\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5bf9\u5076\u504f\u597d\u4fe1\u606f\u8f6c\u5316\u4e3a\u70b9\u5f0f\u8bad\u7ec3\u4fe1\u53f7\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u8bc4\u4f30\u6807\u51c6\u751f\u6210\u5168\u5c40\u4e00\u81f4\u6027\u4e0e\u5b9e\u4f8b\u7ea7\u63a8\u7406\u7684\u8bc4\u4ef7\u51c6\u5219\uff0c\u4ece\u800c\u5728RLHF\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u7684\u914d\u5bf9\uff08pairwise\uff09\u65b9\u6cd5\u5c06\u6807\u7b7e\u7b80\u5316\u4e3a\u4e8c\u5143\u7684\u597d/\u574f\uff0c\u5bfc\u81f4\u70b9\u5bf9\u70b9\u63a8\u65ad\u4e0d\u5339\u914d\u4e14\u9700\u8981\u590d\u6742\u7684\u914d\u5bf9\u7b56\u7565\uff1b\u70b9\u5f0f\u65b9\u6cd5\u9700\u8981\u7edd\u5bf9\u6807\u7b7e\u3001\u51c6\u5219\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u4e14\u9002\u5e94\u6027\u5dee\u3002\u4e24\u8005\u5404\u81ea\u5b58\u5728\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u9002\u5e94\u6027\u5dee\u7b49\u5c40\u9650\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u540c\u65f6\u5229\u7528\u76f8\u5bf9\u504f\u597d\u4fe1\u53f7\u5e76\u5e26\u6765\u7075\u6d3b\u8bc4\u4f30\u51c6\u5219\u7684\u65b9\u6cd5\u3002", "method": "PaTaRM\u7ed3\u5408\u4e00\u4e2a\u504f\u597d\u611f\u77e5\u5956\u52b1\uff08PAR\uff09\u673a\u5236\uff0c\u5c06\u914d\u5bf9\u6570\u636e\u4e2d\u7684\u76f8\u5bf9\u504f\u597d\u8f6c\u5316\u4e3a\u7a33\u5065\u7684\u70b9\u5f0f\u8bad\u7ec3\u4fe1\u53f7\uff1b\u540c\u65f6\u5f15\u5165\u4efb\u52a1\u81ea\u9002\u5e94\u7684rubric\u7cfb\u7edf\uff0c\u52a8\u6001\u751f\u6210\u5168\u5c40\u4efb\u52a1\u4e00\u81f4\u6027\u4e0e\u5b9e\u4f8b\u7ea7\u7ec6\u7c92\u5ea6\u63a8\u7406\u7684\u8bc4\u4f30\u51c6\u5219\u3002\u8fd9\u4e00\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u4e14\u53ef\u89e3\u91ca\u7684RLHF\u5956\u52b1\u5efa\u6a21\uff0c\u4e14\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u826f\u597d\u9002\u914d\u6027\u3002", "result": "\u5728RewardBench\u548cRMBench\u4e0a\u5bf9Qwen3-8B\u4e0eQwen3-14B\u6a21\u578b\u7684\u5e73\u5747\u76f8\u5bf9\u63d0\u5347\u4e3a4.7%\uff1b\u5728\u4e0b\u6e38RLHF\u4efb\u52a1IFEval\u4e0eInFoBench\u4e0a\uff0c\u5e73\u5747\u63d0\u5347\u4e3a13.6%\uff0c\u8868\u660ePaTaRM\u5177\u6709\u7a33\u5065\u6027\u4e0e\u6709\u6548\u6027\u3002\u4ee3\u7801\u516c\u5f00\u5728GitHub\u3002", "conclusion": "PaTaRM\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6848\uff0c\u80fd\u5c06\u504f\u597d\u4fe1\u606f\u6709\u6548\u8f6c\u5316\u4e3a\u70b9\u5f0f\u8bad\u7ec3\u4fe1\u53f7\u5e76\u7ed3\u5408\u7075\u6d3b\u7684\u8bc4\u4f30\u51c6\u5219\uff0c\u663e\u8457\u63d0\u5347RLHF\u7684\u5956\u52b1\u6a21\u578b\u8d28\u91cf\u4e0e\u4e0b\u6e38\u6027\u80fd\u3002"}}
{"id": "2510.24240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24240", "abs": "https://arxiv.org/abs/2510.24240", "authors": ["Edward Markai", "Sina Molavipour"], "title": "Temporal Knowledge Graph Hyperedge Forecasting: Exploring Entity-to-Category Link Prediction", "comment": null, "summary": "Temporal Knowledge Graphs have emerged as a powerful way of not only modeling\nstatic relationships between entities but also the dynamics of how relations\nevolve over time. As these informational structures can be used to store\ninformation from a real-world setting, such as a news flow, predicting future\ngraph components to a certain extent equates predicting real-world events. Most\nof the research in this field focuses on embedding-based methods, often\nleveraging convolutional neural net architectures. These solutions act as black\nboxes, limiting insight. In this paper, we explore an extension to an\nestablished rule-based framework, TLogic, that yields a high accuracy in\ncombination with explainable predictions. This offers transparency and allows\nthe end-user to critically evaluate the rules applied at the end of the\nprediction stage. The new rule format incorporates entity category as a key\ncomponent with the purpose of limiting rule application only to relevant\nentities. When categories are unknown for building the graph, we propose a\ndata-driven method to generate them with an LLM-based approach. Additionally,\nwe investigate the choice of aggregation method for scores of retrieved\nentities when performing category prediction.", "AI": {"tldr": "\u5bf9 TLogic \u7684\u6269\u5c55\uff0c\u901a\u8fc7\u5f15\u5165\u5b9e\u4f53\u7c7b\u522b\u6765\u9650\u5b9a\u89c4\u5219\u7684\u9002\u7528\u8303\u56f4\uff0c\u5e76\u5728\u7c7b\u522b\u672a\u77e5\u65f6\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u751f\u6210\u7c7b\u522b\uff0c\u540c\u65f6\u7814\u7a76\u68c0\u7d22\u5b9e\u4f53\u5206\u6570\u7684\u805a\u5408\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u4e14\u5177\u9ad8\u51c6\u786e\u6027\u7684\u65f6\u6001\u77e5\u8bc6\u56fe\u9884\u6d4b\u3002", "motivation": "\u65f6\u6001\u77e5\u8bc6\u56fe\u8c31\u5728\u5efa\u6a21\u52a8\u6001\u5173\u7cfb\u65b9\u9762\u5f88\u5f3a\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u662f\u9ed1\u7bb1\u7684\u5d4c\u5165\u5f0f\u6a21\u578b\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7\u5728\u89c4\u5219\u6846\u67b6\u4e2d\u5f15\u5165\u7c7b\u522b\u4fe1\u606f\uff0c\u53ef\u4ee5\u63d0\u5347\u900f\u660e\u5ea6\u5e76\u9650\u5b9a\u89c4\u5219\u7684\u9002\u7528\u5bf9\u8c61\uff1b\u5f53\u7c7b\u522b\u672a\u77e5\u65f6\uff0c\u5229\u7528LLM\u751f\u6210\u7c7b\u522b\u6709\u52a9\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u7c7b\u522b\u5212\u5206\uff1b\u805a\u5408\u7b56\u7565\u5bf9\u7c7b\u522b\u9884\u6d4b\u4e5f\u6709\u5f71\u54cd\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5728 TLogic \u6846\u67b6\u4e2d\u589e\u52a0\u4e00\u79cd\u65b0\u7684\u89c4\u5219\u8868\u793a\u5f62\u5f0f\uff0c\u5c06\u5b9e\u4f53\u7c7b\u522b\u4f5c\u4e3a\u5173\u952e\u7ec4\u4ef6\uff0c\u7528\u4ee5\u9650\u5236\u89c4\u5219\u53ea\u5728\u76f8\u5173\u5b9e\u4f53\u4e0a\u5e94\u7528\u3002\u9488\u5bf9\u7c7b\u522b\u672a\u77e5\u7684\u60c5\u51b5\uff0c\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6765\u751f\u6210\u7c7b\u522b\u3002\u5e76\u5bf9\u68c0\u7d22\u5b9e\u4f53\u5206\u6570\u7684\u805a\u5408\u65b9\u6cd5\u8fdb\u884c\u7814\u7a76\uff0c\u4ee5\u63d0\u5347\u7c7b\u522b\u9884\u6d4b\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u6269\u5c55\u5728\u4fdd\u6301\u8f83\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u89e3\u91ca\u6027\uff0c\u6700\u7ec8\u7528\u6237\u53ef\u4ee5\u5728\u9884\u6d4b\u9636\u6bb5\u672b\u5bf9\u89c4\u5219\u8fdb\u884c\u8bc4\u4f30\uff1b\u4e0d\u540c\u7684\u805a\u5408\u7b56\u7565\u5bf9\u7c7b\u522b\u9884\u6d4b\u6709\u5f71\u54cd\uff0cLLM\u9a71\u52a8\u7684\u7c7b\u522b\u751f\u6210\u5728\u6570\u636e\u4e0d\u8db3\u573a\u666f\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u5c06\u5b9e\u4f53\u7c7b\u522b\u7eb3\u5165\u89c4\u5219\u6846\u67b6\u53ef\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u5e94\u7528\u8303\u56f4\uff1b\u5728\u7c7b\u522b\u7f3a\u5931\u65f6\uff0cLLM\u9a71\u52a8\u7684\u7c7b\u522b\u751f\u6210\u63d0\u4f9b\u53ef\u884c\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff1b\u5bf9\u805a\u5408\u7b56\u7565\u7684\u63a2\u7d22\u4e3a\u7c7b\u522b\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9009\u9879\u3002"}}
{"id": "2510.24273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24273", "abs": "https://arxiv.org/abs/2510.24273", "authors": ["Junlin Mu", "Hantao Huang", "Jihang Zhang", "Minghui Yu", "Tao Wang", "Yidong Li"], "title": "SALS: Sparse Attention in Latent Space for KV cache Compression", "comment": null, "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.", "AI": {"tldr": "SALS \u63d0\u51fa\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u7a00\u758f\u6ce8\u610f\u529b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u79e9\u6295\u5f71\u5c06 KV \u7f13\u5b58\u538b\u7f29\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u6267\u884c RoPE-free \u7684 QK \u4ea4\u4e92\uff0c\u63d0\u5347\u7aef\u5230\u7aef\u541e\u5410\u4e0e\u6ce8\u610f\u529b\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u4e0a\u4e0b\u6587\u9700\u8981\u5de8\u5927\u7684 KV \u7f13\u5b58\uff0c\u4f20\u7edf\u4f4e\u79e9\u538b\u7f29\u5728 RoPE \u673a\u5236\u4e0b\u96be\u4ee5\u4fdd\u6301\u7cbe\u5ea6\u4e14\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6548\u91cd\u5efa\u3002\u9700\u8981\u4e00\u79cd\u5728\u4e0d\u91cd\u65b0\u6784\u5efa\u5168\u90e8 KV \u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u538b\u7f29\u4e0e\u52a0\u901f\u7684\u65b9\u6cd5\u3002", "method": "\u5c06 KV \u7f13\u5b58\u6295\u5f71\u5230\u4e00\u4e2a\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u4f4e\u79e9\u6295\u5f71\u5b9e\u73b0\u538b\u7f29\uff1b\u5728\u8be5\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7a00\u758f\u7684 token \u9009\u62e9\uff0c\u4f7f\u7528 RoPE-free \u7684 QK \u4ea4\u4e92\u4ee5\u907f\u514d RoPE \u5e26\u6765\u7684\u65b9\u5dee\u653e\u5927\uff1b\u4ec5\u91cd\u6784\u91cd\u8981 token\uff0c\u907f\u514d\u5168\u91cf KV \u7f13\u5b58\u91cd\u5efa\u3002", "result": "\u5728 LLaMA2-7b-chat\u3001Mistral-7b \u7b49\u6a21\u578b\u4e0a\u4ee5\u53ca RULER-128k \u7684\u6d4b\u8bd5\u4e2d\uff0c\u8fbe\u5230 SOTA \u7684\u6027\u80fd\u3002\u76f8\u8f83\u4e8e FlashAttention2\uff0c\u5728 4K \u5e8f\u5217\u4e0b\u5b9e\u73b0\u7ea6 6.4x \u7684 KV \u7f13\u5b58\u538b\u7f29\u548c 5.7x \u7684\u6ce8\u610f\u529b\u8fd0\u7b97\u52a0\u901f\uff1b\u7aef\u5230\u7aef\u541e\u5410\u91cf\u5206\u522b\u6bd4 GPT-fast \u63d0\u5347\u7ea6 1.4x\uff084K\uff09\u548c 4.5x\uff0832K\uff09", "conclusion": "SALS \u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u4f4e\u79e9\u6295\u5f71\u548c RoPE-free \u7684\u4ea4\u4e92\uff0c\u5b9e\u73b0 KV \u7f13\u5b58\u7684\u9ad8\u6548\u538b\u7f29\u4e0e\u6ce8\u610f\u529b\u52a0\u901f\uff0c\u5728\u591a\u6a21\u578b\u548c\u57fa\u51c6\u4e0a\u8bc1\u660e\u4e86\u826f\u597d\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.24310", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24310", "abs": "https://arxiv.org/abs/2510.24310", "authors": ["Guus Toussaint", "Arno Knobbe"], "title": "EDC: Equation Discovery for Classification", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Lecture Notes in Computer Science, and is available online at\n  https://doi.org/10.1007/978-3-032-05461-6_9", "summary": "Equation Discovery techniques have shown considerable success in regression\ntasks, where they are used to discover concise and interpretable models\n(\\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary\nclassification framework. Our proposed method EDC finds analytical functions of\nmanageable size that specify the location and shape of the decision boundary.\nIn extensive experiments on artificial and real-life data, we demonstrate how\nEDC is able to discover both the structure of the target equation as well as\nthe value of its parameters, outperforming the current state-of-the-art\nED-based classification methods in binary classification and achieving\nperformance comparable to the state of the art in binary classification. We\nsuggest a grammar of modest complexity that appears to work well on the tested\ndatasets but argue that the exact grammar -- and thus the complexity of the\nmodels -- is configurable, and especially domain-specific expressions can be\nincluded in the pattern language, where that is required. The presented grammar\nconsists of a series of summands (additive terms) that include linear,\nquadratic and exponential terms, as well as products of two features (producing\nhyperbolic curves ideal for capturing XOR-like dependencies). The experiments\ndemonstrate that this grammar allows fairly flexible decision boundaries while\nnot so rich to cause overfitting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eED\u7684\u4e8c\u5206\u7c7b\u6846\u67b6EDC\uff0c\u901a\u8fc7\u53ef\u63a7\u590d\u6742\u5ea6\u7684\u89e3\u6790\u51fd\u6570\u6765\u754c\u5b9a\u51b3\u7b56\u8fb9\u754c\uff0c\u80fd\u591f\u540c\u65f6\u53d1\u73b0\u8fb9\u754c\u7684\u7ed3\u6784\u548c\u53c2\u6570\uff0c\u4e14\u76f8\u8f83\u4e8e\u73b0\u6709ED\u65b9\u6cd5\u8868\u73b0\u4f18\u8d8a\u4e14\u63a5\u8fd1\u6700\u4f18\u4e8c\u5206\u7c7b\u6c34\u5e73\u3002", "motivation": "\u5728\u53ef\u89e3\u91ca\u6027\u7684\u9700\u6c42\u9a71\u52a8\u4e0b\uff0c\u6269\u5c55\u65b9\u7a0b\u53d1\u73b0\uff08ED\uff09\u5230\u4e8c\u5206\u7c7b\u4efb\u52a1\uff0c\u5bfb\u627e\u65e2\u7b80\u6d01\u53c8\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u540c\u65f6\u5e94\u5bf9XOR\u7b49\u975e\u7ebf\u6027\u4f9d\u8d56\u4ee5\u53ca\u8fb9\u754c\u5f62\u72b6\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faEDC\u6846\u67b6\uff1a\u901a\u8fc7\u4e00\u4e2a\u76f8\u5bf9\u7b80\u6d01\u7684\u6a21\u5f0f\u8bed\u8a00\uff0c\u751f\u6210\u53ef\u89e3\u6790\u7684\u8fb9\u754c\u51fd\u6570\u3002\u6a21\u5f0f\u8bed\u8a00\u5305\u542b\u7ebf\u6027\u3001\u4e8c\u6b21\u3001\u6307\u6570\u9879\uff0c\u4ee5\u53ca\u4e24\u4e2a\u7279\u5f81\u7684\u4e58\u79ef\u7b49\u5206\u9879\uff0c\u5f62\u6210\u4e00\u7cfb\u5217\u52a0\u6cd5\u9879\u4ee5\u63cf\u8ff0\u8fb9\u754c\uff1b\u53ef\u914d\u7f6e\u7684\u8bed\u6cd5\u5141\u8bb8\u52a0\u5165\u57df\u7279\u5b9a\u8868\u8fbe\u3002\u901a\u8fc7\u5bf9\u7ed3\u6784\u548c\u53c2\u6570\u7684\u8054\u5408\u641c\u7d22\uff0c\u53d1\u73b0\u8fb9\u754c\u65b9\u7a0b\u53ca\u5176\u53c2\u6570\uff0c\u5e76\u5728\u4eba\u5de5\u4e0e\u771f\u5b9e\u6570\u636e\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0e\u73b0\u6709ED\u5206\u7c7b\u65b9\u6cd5\u548c\u4e8c\u5206\u7c7b\u6700\u4f18\u6c34\u5e73\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEDC\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u7684ED\u57fa\u5206\u7c7b\u65b9\u6cd5\uff0c\u4e14\u5728\u603b\u4f53\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\uff1b\u6240\u63d0\u51fa\u7684\u8bed\u6cd5\u65e2\u8db3\u4ee5\u63d0\u4f9b\u7075\u6d3b\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u53c8\u4e0d\u8fc7\u5ea6\u62df\u5408\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8bc1\u660e\u4e86\u4e00\u4e2a modest \u590d\u6742\u5ea6\u7684\u6a21\u5f0f\u8bed\u8a00\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u4e8c\u5206\u7c7b\u8fb9\u754c\uff0c\u6a21\u578b\u53ef\u901a\u8fc7\u914d\u7f6e\u8bed\u6cd5\u548c\u52a0\u5165\u9886\u57df\u8868\u8fbe\u6765\u9002\u5e94\u5177\u4f53\u4efb\u52a1\uff0c\u540c\u65f6\u80fd\u591f\u6355\u6349\u5982XOR\u578b\u4f9d\u8d56\u548c\u53cc\u7279\u5f81\u4e58\u79ef\u5bfc\u81f4\u7684\u8d85\u66f2\u7ebf\u8fb9\u754c\u3002"}}
{"id": "2510.24318", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24318", "abs": "https://arxiv.org/abs/2510.24318", "authors": ["Prajit Bhaskaran", "Tom Viering"], "title": "Transformers can do Bayesian Clustering", "comment": null, "summary": "Bayesian clustering accounts for uncertainty but is computationally demanding\nat scale. Furthermore, real-world datasets often contain missing values, and\nsimple imputation ignores the associated uncertainty, resulting in suboptimal\nresults. We present Cluster-PFN, a Transformer-based model that extends\nPrior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained\nentirely on synthetic datasets generated from a finite Gaussian Mixture Model\n(GMM) prior, Cluster-PFN learns to estimate the posterior distribution over\nboth the number of clusters and the cluster assignments. Our method estimates\nthe number of clusters more accurately than handcrafted model selection\nprocedures such as AIC, BIC and Variational Inference (VI), and achieves\nclustering quality competitive with VI while being orders of magnitude faster.\nCluster-PFN can be trained on complex priors that include missing data,\noutperforming imputation-based baselines on real-world genomic datasets, at\nhigh missingness. These results show that the Cluster-PFN can provide scalable\nand flexible Bayesian clustering.", "AI": {"tldr": "Cluster-PFN \u5c06 PFN \u6269\u5c55\u5230\u65e0\u76d1\u7763\u8d1d\u53f6\u65af\u805a\u7c7b\uff0c\u5229\u7528 Transformer \u5728\u5408\u6210 GMM \u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u80fd\u591f\u5728\u4f30\u8ba1\u7c07\u6570\u4e0e\u7c07\u5206\u914d\u65b9\u9762\u540c\u65f6\u8003\u8651\u4e0d\u786e\u5b9a\u6027\uff0c\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u5728\u7f3a\u5931\u6570\u636e\u573a\u666f\u4e0b\u4f18\u4e8e\u57fa\u4e8e\u586b\u5145\u7684\u57fa\u7ebf\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e2d\uff0c\u8d1d\u53f6\u65af\u805a\u7c7b\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u73b0\u5b9e\u6570\u636e\u5e38\u5305\u542b\u7f3a\u5931\u503c\uff0c\u7b80\u5355\u586b\u5145\u4f1a\u5ffd\u7565\u4e0d\u786e\u5b9a\u6027\u3001\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u80fd\u5904\u7406\u7f3a\u5931\u6570\u636e\u7684\u8d1d\u53f6\u65af\u805a\u7c7b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa Cluster-PFN\uff0c\u57fa\u4e8e Transformer \u7684\u6a21\u578b\uff0c\u5c06 Prior-Data Fitted Networks (PFN) \u6269\u5c55\u5230\u65e0\u76d1\u7763\u8d1d\u53f6\u65af\u805a\u7c7b\u3002\u901a\u8fc7\u5728\u6765\u81ea\u6709\u9650\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5148\u9a8c\u7684\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u5b66\u4e60\u5bf9\u7c07\u6570\u4e0e\u5206\u914d\u7684\u540e\u9a8c\u5206\u5e03\u7684\u4f30\u8ba1\u3002\u53ef\u5904\u7406\u5305\u542b\u7f3a\u5931\u6570\u636e\u7684\u590d\u6742\u5148\u9a8c\uff0c\u4e14\u5b9e\u73b0\u7aef\u5230\u7aef\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728\u7c07\u6570\u4f30\u8ba1\u4e0a\u4f18\u4e8e AIC\u3001BIC\u3001\u53d8\u5206\u63a8\u65ad\u7b49\u624b\u5de5\u6a21\u578b\u9009\u62e9\uff1b\u805a\u7c7b\u8d28\u91cf\u4e0e VI \u76f8\u5f53\uff0c\u540c\u65f6\u901f\u5ea6\u6570\u91cf\u7ea7\u63d0\u5347\uff1b\u5728\u9ad8\u7f3a\u5931\u60c5\u51b5\u4e0b\uff0c\u80fd\u5728\u771f\u5b9e\u57fa\u56e0\u7ec4\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u4e8e\u586b\u5145\u7684\u57fa\u7ebf\uff0c\u663e\u793a\u4e86\u5bf9\u7f3a\u5931\u6570\u636e\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Cluster-PFN \u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u8d1d\u53f6\u65af\u805a\u7c7b\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u901f\u5ea6\uff0c\u5e76\u5728\u5b58\u5728\u7f3a\u5931\u6570\u636e\u7684\u73b0\u5b9e\u4efb\u52a1\u4e2d\u5177\u5907\u826f\u597d\u8868\u73b0\u3002"}}
{"id": "2510.24331", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24331", "abs": "https://arxiv.org/abs/2510.24331", "authors": ["Gabriel O. dos Santos", "Esther Colombini", "Sandra Avila"], "title": "What do vision-language models see in the context? Investigating multimodal in-context learning", "comment": null, "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.", "AI": {"tldr": "VLM \u7684\u5728-context \u5b66\u4e60\u80fd\u529b\u53d7\u9650\uff0c\u63d0\u793a\u8bbe\u8ba1\u3001\u67b6\u6784\u4e0e\u8bad\u7ec3\u7b56\u7565\u5bf9\u8868\u73b0\u6709\u663e\u8457\u5f71\u54cd\uff1b\u591a\u6a21\u6001\u6574\u5408\u80fd\u529b\u4e0d\u8db3\uff0c\u5f53\u524d\u591a\u503e\u5411\u6587\u672c\u7ebf\u7d22\u800c\u975e\u89c6\u89c9\u4fe1\u606f\uff0c\u6307\u4ee4\u5fae\u8c03\u53ef\u80fd\u964d\u4f4e\u5bf9 IC L \u6f14\u793a\u7684\u4f9d\u8d56\u3002", "motivation": "\u5c3d\u7ba1\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u5e7f\u6cdb\u7814\u7a76\u4e86\u5728-context \u5b66\u4e60\uff08ICL\uff09\uff0c\u4f46\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u6548\u679c\u5c1a\u672a\u7cfb\u7edf\u5316\u7814\u7a76\u3002\u672c\u6587\u5bf9\u4e03\u4e2a\u6a21\u578b\u3001\u56db\u79cd\u67b6\u6784\u3001\u4e09\u9879\u56fe\u50cf\u63cf\u8ff0\u57fa\u51c6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5206\u6790\u63d0\u793a\u8bbe\u8ba1\u3001\u67b6\u6784\u9009\u62e9\u4e0e\u8bad\u7ec3\u7b56\u7565\u5bf9\u591a\u6a21\u6001 ICL \u7684\u5f71\u54cd\uff0c\u5e76\u9996\u6b21\u5206\u6790\u968f\u7740\u6f14\u793a\u6570\u91cf\u589e\u52a0\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "method": "\u8bc4\u4f30\u4e03\u4e2a\u6a21\u578b\u3001\u56db\u79cd\u67b6\u6784\u3001\u4e09\u9879\u56fe\u50cf\u63cf\u8ff0\u57fa\u51c6\uff0c\u5206\u6790\u63d0\u793a\u8bbe\u8ba1\u3001\u67b6\u6784\u9009\u62e9\u3001\u8bad\u7ec3\u7b56\u7565\u5bf9\u591a\u6a21\u6001 ICL \u7684\u5f71\u54cd\uff1b\u6bd4\u8f83\u5728 imagetext interleaved \u6570\u636e\u8bad\u7ec3\u4e0e\u6307\u4ee4\u5fae\u8c03\u4e0b\u7684\u8868\u73b0\uff1b\u8fdb\u884c\u6ce8\u610f\u529b\u5206\u6790\u4ee5\u89c2\u5bdf\u968f\u6f14\u793a\u6570\u91cf\u589e\u591a\u7684\u6ce8\u610f\u529b\u5206\u5e03\u53d8\u5316\u3002", "result": "\u5728 imagetext interleaved \u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u80fd\u63d0\u5347 ICL \u8868\u73b0\uff0c\u4f46\u672a\u5fc5\u5b9e\u73b0\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\u7684\u6709\u6548\u6574\u5408\uff1b\u6307\u4ee4\u5fae\u8c03\u63d0\u5347\u6307\u4ee4\u9075\u5faa\u4f46\u53ef\u80fd\u964d\u4f4e\u5bf9 ICL \u6f14\u793a\u7684\u4f9d\u8d56\uff0c\u8868\u660e\u6307\u4ee4\u5bf9\u9f50\u4e0e\u5728-context \u81ea\u9002\u5e94\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1b\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\u5f53\u524d VLMs \u4e3b\u8981\u5173\u6ce8\u6587\u672c\u7ebf\u7d22\uff0c\u672a\u5145\u5206\u5229\u7528\u89c6\u89c9\u4fe1\u606f\uff0c\u663e\u793a\u591a\u6a21\u6001\u6574\u5408\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d VLM \u5728 ICL \u80fd\u529b\u65b9\u9762\u7684\u5173\u952e\u5c40\u9650\uff0c\u5e76\u4e3a\u63d0\u5347\u4ece\u591a\u6a21\u6001\u5728-context \u793a\u4f8b\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.24356", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24356", "abs": "https://arxiv.org/abs/2510.24356", "authors": ["Suman Sanyal"], "title": "Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning", "comment": null, "summary": "We introduce Perception Learning (PeL), a paradigm that optimizes an agent's\nsensory interface $f_\\phi:\\mathcal{X}\\to\\mathcal{Z}$ using task-agnostic\nsignals, decoupled from downstream decision learning\n$g_\\theta:\\mathcal{Z}\\to\\mathcal{Y}$. PeL directly targets label-free\nperceptual properties, such as stability to nuisances, informativeness without\ncollapse, and controlled geometry, assessed via objective\nrepresentation-invariant metrics. We formalize the separation of perception and\ndecision, define perceptual properties independent of objectives or\nreparameterizations, and prove that PeL updates preserving sufficient\ninvariants are orthogonal to Bayes task-risk gradients. Additionally, we\nprovide a suite of task-agnostic evaluation metrics to certify perceptual\nquality.", "AI": {"tldr": "\u63d0\u51fa Perception Learning (PeL) \u6846\u67b6\uff0c\u5c06\u611f\u77e5\u63a5\u53e3 f_\u03c6 \u4e0e\u51b3\u7b56\u6620\u5c04 g_\u03b8 \u89e3\u8026\uff0c\u901a\u8fc7\u4efb\u52a1\u65e0\u5173\u7684\u4fe1\u53f7\u4f18\u5316\u611f\u77e5\u8868\u793a\uff0c\u5f3a\u8c03\u5bf9\u5e72\u6270\u7684\u4e0d\u53d8\u6027\u3001\u4fe1\u606f\u6027\u4e0e\u51e0\u4f55\u5c5e\u6027\u7684\u53ef\u63a7\u6027\uff0c\u5e76\u7ed9\u51fa\u4e00\u7ec4\u4e0d\u4f9d\u8d56\u5177\u4f53\u4efb\u52a1\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u611f\u77e5\u4e0e\u4e0b\u6e38\u51b3\u7b56\u8026\u5408\u5e26\u6765\u7684\u8868\u5f81\u584c\u7f29\u3001\u5bf9\u5e72\u6270\u654f\u611f\u3001\u51e0\u4f55\u7ed3\u6784\u4e0d\u53ef\u63a7\u7b49\u95ee\u9898\uff1b\u5e0c\u671b\u901a\u8fc7\u72ec\u7acb\u4f18\u5316\u611f\u77e5\u5c42\u5b9e\u73b0\u8de8\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u8fc1\u79fb\u6027\u3002", "method": "\u5f62\u5f0f\u5316\u611f\u77e5\u4e0e\u51b3\u7b56\u5206\u79bb\uff0c\u63d0\u51fa\u611f\u77e5\u5c5e\u6027\u72ec\u7acb\u4e8e\u76ee\u6807\u51fd\u6570\u4e0e\u53c2\u6570\u5316\u7684\u5b9a\u4e49\uff1b\u8bc1\u660e\u5728\u4fdd\u6301\u5145\u5206\u4e0d\u53d8\u6027\u524d\u63d0\u4e0b\uff0cPeL \u7684\u66f4\u65b0\u4e0e\u8d1d\u53f6\u65af\u98ce\u9669\u68af\u5ea6\u6b63\u4ea4\uff1b\u63d0\u4f9b\u4efb\u52a1\u65e0\u5173\u7684\u611f\u77e5\u8bc4\u4f30\u6307\u6807\u4ee5 certify \u611f\u77e5\u8d28\u91cf\u3002", "result": "\u7ed9\u51fa\u7406\u8bba\u5c42\u9762\u7684\u5206\u79bb\u4e0e\u6b63\u4ea4\u6027\u7ed3\u8bba\uff1b\u5b9a\u4e49\u611f\u77e5\u5c5e\u6027\u53ca\u5176\u72ec\u7acb\u6027\uff1b\u63d0\u51fa\u7528\u4e8e\u611f\u77e5\u8d28\u91cf\u8bc4\u4f30\u7684\u4efb\u52a1\u65e0\u5173\u6307\u6807\u96c6\u3002", "conclusion": "PeL \u63d0\u4f9b\u4e00\u4e2a\u53ef\u7406\u8bba\u5316\u7684\u611f\u77e5\u5b66\u4e60\u6846\u67b6\uff0c\u4fc3\u6210\u5bf9\u611f\u77e5\u80fd\u529b\u7684\u72ec\u7acb\u4f18\u5316\u4e0e\u53ef\u89e3\u91ca\u8bc4\u4f30\uff0c\u63d0\u5347\u8de8\u4efb\u52a1\u548c\u8fc1\u79fb\u60c5\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u6027\u3002"}}
{"id": "2510.24368", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24368", "abs": "https://arxiv.org/abs/2510.24368", "authors": ["Maria Gabriela Valeriano", "David Kohan Marzag\u00e3o", "Alfredo Montelongo", "Carlos Roberto Veiga Kiffer", "Natan Katz", "Ana Carolina Lorena"], "title": "Filtering instances and rejecting predictions to obtain reliable models in healthcare", "comment": "This paper is under review at Machine Learning (Springer)", "summary": "Machine Learning (ML) models are widely used in high-stakes domains such as\nhealthcare, where the reliability of predictions is critical. However, these\nmodels often fail to account for uncertainty, providing predictions even with\nlow confidence. This work proposes a novel two-step data-centric approach to\nenhance the performance of ML models by improving data quality and filtering\nlow-confidence predictions. The first step involves leveraging Instance\nHardness (IH) to filter problematic instances during training, thereby refining\nthe dataset. The second step introduces a confidence-based rejection mechanism\nduring inference, ensuring that only reliable predictions are retained. We\nevaluate our approach using three real-world healthcare datasets, demonstrating\nits effectiveness at improving model reliability while balancing predictive\nperformance and rejection rate. Additionally, we use alternative criteria -\ninfluence values for filtering and uncertainty for rejection - as baselines to\nevaluate the efficiency of the proposed method. The results demonstrate that\nintegrating IH filtering with confidence-based rejection effectively enhances\nmodel performance while preserving a large proportion of instances. This\napproach provides a practical method for deploying ML systems in\nsafety-critical applications.", "AI": {"tldr": "\u4e24\u6b65\u6570\u636e\u4e2d\u5fc3\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5b9e\u4f8b\u56f0\u96be\u7684\u8bad\u7ec3\u6570\u636e\u8fc7\u6ee4 + \u63a8\u7406\u9636\u6bb5\u7684\u7f6e\u4fe1\u5ea6\u62d2\u7edd\u673a\u5236\uff0c\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u63d0\u5347\u53ef\u9760\u6027\uff0c\u540c\u65f6\u5c3d\u91cf\u4fdd\u7559\u5927\u91cf\u6837\u672c\u3002", "motivation": "\u9ad8\u98ce\u9669\u9886\u57df\u7684ML\u9884\u6d4b\u82e5\u5ffd\u7565\u4e0d\u786e\u5b9a\u6027\uff0c\u6613\u5728\u5173\u952e\u573a\u666f\u4ea7\u751f\u9519\u8bef\u3002\u9700\u8981\u5728\u4e0d\u663e\u8457\u964d\u4f4e\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u90e8\u7f72\u6027\u3002", "method": "\u6b65\u9aa41\uff1a\u5728\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u5b9e\u4f8b\u56f0\u96be\uff08IH\uff09\u7b5b\u9009\u5e76\u6e05\u6d17\u95ee\u9898\u5b9e\u4f8b\u4ee5\u4f18\u5316\u6570\u636e\u8d28\u91cf\uff1b\u6b65\u9aa42\uff1a\u5728\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u62d2\u7edd\u673a\u5236\uff0c\u4ec5\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u9884\u6d4b\u3002\u4e3a\u5bf9\u6bd4\uff0c\u91c7\u7528\u5f71\u54cd\u503c\u7b5b\u9009\u548c\u4e0d\u786e\u5b9a\u5ea6\u62d2\u7edd\u7b49\u57fa\u7ebf\u3002\u8bc4\u4f30\u5728\u4e09\u4e2a\u771f\u5b9e\u533b\u7597\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u53ef\u9760\u6027\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6837\u672c\u4fdd\u7559\u7387\uff1b\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6027\u80fd\u4e0e\u62d2\u7edd\u7387\u4e4b\u95f4\u5b9e\u73b0\u66f4\u4f18\u7684\u6743\u8861\uff0c\u663e\u73b0\u51fa\u5bf9\u9ad8\u98ce\u9669\u573a\u666f\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "IH\u8fc7\u6ee4\u4e0e\u7f6e\u4fe1\u5ea6\u62d2\u7edd\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u843d\u5730\u7684ML\u90e8\u7f72\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u533b\u7597\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2510.24375", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24375", "abs": "https://arxiv.org/abs/2510.24375", "authors": ["Yuanyuan Wu", "Zhenlin Qin", "Zhenliang Ma"], "title": "A Comprehensive Evaluation Framework for Synthetic Trip Data Generation in Public Transport", "comment": null, "summary": "Synthetic data offers a promising solution to the privacy and accessibility\nchallenges of using smart card data in public transport research. Despite rapid\nprogress in generative modeling, there is limited attention to comprehensive\nevaluation, leaving unclear how reliable, safe, and useful synthetic data truly\nare. Existing evaluations remain fragmented, typically limited to\npopulation-level representativeness or record-level privacy, without\nconsidering group-level variations or task-specific utility. To address this\ngap, we propose a Representativeness-Privacy-Utility (RPU) framework that\nsystematically evaluates synthetic trip data across three complementary\ndimensions and three hierarchical levels (record, group, population). The\nframework integrates a consistent set of metrics to quantify similarity,\ndisclosure risk, and practical usefulness, enabling transparent and balanced\nassessment of synthetic data quality. We apply the framework to benchmark\ntwelve representative generation methods, spanning conventional statistical\nmodels, deep generative networks, and privacy-enhanced variants. Results show\nthat synthetic data do not inherently guarantee privacy and there is no\n\"one-size-fits-all\" model, the trade-off between privacy and\nrepresentativeness/utility is obvious. Conditional Tabular generative\nadversarial network (CTGAN) provide the most balanced trade-off and is\nsuggested for practical applications. The RPU framework provides a systematic\nand reproducible basis for researchers and practitioners to compare synthetic\ndata generation techniques and select appropriate methods in public transport\napplications.", "AI": {"tldr": "\u63d0\u51fa RPU \u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u516c\u5171\u4ea4\u901a\u5408\u6210\u51fa\u884c\u6570\u636e\u5728\u4ee3\u8868\u6027\u3001\u9690\u79c1\u548c\u6548\u7528\u4e09\u4e2a\u7ef4\u5ea6\u3001\u4e09\u4e2a\u5c42\u7ea7\u4e0a\u7684\u8d28\u91cf\uff1b\u5bf9 12 \u79cd\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6bd4\u8f83\uff0cCTGAN \u5728\u6743\u8861\u65b9\u9762\u8868\u73b0\u6700\u597d\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u8bc4\u4f30\u788e\u7247\u5316\u3001\u7f3a\u4e4f\u7fa4\u4f53\u5c42\u7ea7\u4e0e\u4efb\u52a1\u7279\u5b9a\u6548\u7528\u8003\u91cf\u7684\u95ee\u9898\uff1b\u63d0\u5347\u5408\u6210\u6570\u636e\u5728\u9690\u79c1\u4fdd\u62a4\u4e0e\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u53ef\u63a7\u6027\u3002", "method": "\u6784\u5efa\u4e09\u7ef4\u4e09\u5c42\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6574\u5408\u4e00\u81f4\u7684\u76f8\u4f3c\u6027\u3001\u62ab\u9732\u98ce\u9669\u548c\u5b9e\u7528\u6027\u6307\u6807\uff0c\u8986\u76d6\u8bb0\u5f55\u3001\u7fa4\u4f53\u3001\u603b\u4f53\u4e09\u4e2a\u5c42\u7ea7\uff0c\u5bf9 12 \u79cd\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u68c0\u9a8c\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5408\u6210\u6570\u636e\u5e76\u4e0d\u5929\u7136\u4fdd\u62a4\u9690\u79c1\uff0c\u4e14\u4e0d\u5b58\u5728\u5355\u4e00\u6700\u4f73\u6a21\u578b\uff1b\u9690\u79c1-\u4ee3\u8868\u6027/\u6548\u7528\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1bCTGAN \u5b9e\u73b0\u6700\u5e73\u8861\u7684\u6298\u4e2d\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "RPU \u6846\u67b6\u63d0\u4f9b\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u6bd4\u8f83\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5e76\u5728\u516c\u5171\u4ea4\u901a\u5e94\u7528\u4e2d\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.24380", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24380", "abs": "https://arxiv.org/abs/2510.24380", "authors": ["Aryan Pedawi", "Jordi Silvestre-Ryan", "Bradley Worley", "Darren J Hsu", "Kushal S Shah", "Elias Stehle", "Jingrong Zhang", "Izhar Wallach"], "title": "APEX: Approximate-but-exhaustive search for ultra-large combinatorial synthesis libraries", "comment": null, "summary": "Make-on-demand combinatorial synthesis libraries (CSLs) like Enamine REAL\nhave significantly enabled drug discovery efforts. However, their large size\npresents a challenge for virtual screening, where the goal is to identify the\ntop compounds in a library according to a computational objective (e.g.,\noptimizing docking score) subject to computational constraints under a limited\ncomputational budget. For current library sizes -- numbering in the tens of\nbillions of compounds -- and scoring functions of interest, a routine virtual\nscreening campaign may be limited to scoring fewer than 0.1% of the available\ncompounds, leaving potentially many high scoring compounds undiscovered.\nFurthermore, as constraints (and sometimes objectives) change during the course\nof a virtual screening campaign, existing virtual screening algorithms\ntypically offer little room for amortization. We propose the\napproximate-but-exhaustive search protocol for CSLs, or APEX. APEX utilizes a\nneural network surrogate that exploits the structure of CSLs in the prediction\nof objectives and constraints to make full enumeration on a consumer GPU\npossible in under a minute, allowing for exact retrieval of approximate top-$k$\nsets. To demonstrate APEX's capabilities, we develop a benchmark CSL comprised\nof more than 10 million compounds, all of which have been annotated with their\ndocking scores on five medically relevant targets along with physicohemical\nproperties measured with RDKit such that, for any objective and set of\nconstraints, the ground truth top-$k$ compounds can be identified and compared\nagainst the retrievals from any virtual screening algorithm. We show APEX's\nconsistently strong performance both in retrieval accuracy and runtime compared\nto alternative methods.", "AI": {"tldr": "APEX enables approximate-but-exhaustive search over make-on-demand combinatorial libraries (CSLs) by using a neural-network surrogate to predict objectives/constraints, enabling near-exhaustive top-k retrieval on consumer GPUs in under a minute. It outperforms alternatives in both retrieval accuracy and runtime on a benchmark with 10M compounds annotated with docking scores across five targets and RDKit properties.", "motivation": "Virtual screening on massive CSLs (tens of billions of compounds) is bottlenecked by the computational budget and by changing constraints/objectives, leaving many high-scoring compounds undiscovered. There is a need for fast, scalable methods that can amortize across campaigns and adapt to new conditions.", "method": "Train a neural-network surrogate that exploits the structure of CSLs to predict docking scores and physico-chemical properties. Use this surrogate to perform full enumeration on a consumer GPU, achieving an approximate-but-exhaustive top-k retrieval. Build a benchmark CSL with >10 million compounds annotated with docking scores on five targets and RDKit-derived properties to gauge ground-truth top-k performance.", "result": "APEX achieves under-a-minute full enumeration on a consumer GPU and provides accurate approximate top-k retrieval, consistently outperforming alternative methods in both accuracy and runtime on the benchmark.", "conclusion": "APEX offers a scalable, fast, approximate-exhaustive search framework for CSLs, enabling near-exhaustive top-k retrieval within practical budgets and adapting to changing objectives during virtual screening campaigns."}}
{"id": "2510.24473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24473", "abs": "https://arxiv.org/abs/2510.24473", "authors": ["Lucas Buk Cardoso", "Simone Aldrey Angelo", "Yasmin Pacheco Gil Bonilha", "Fernando Maia", "Adeylson Guimar\u00e3es Ribeiro", "Maria Paula Curado", "Gisele Aparecida Fernandes", "Vanderlei Cunha Parro", "Fl\u00e1vio Almeida de Magalh\u00e3es Cipparrone", "Alexandre Dias Porto Chiavegatto Filho", "Tatiana Natasha Toporcov"], "title": "Methodology for Comparing Machine Learning Algorithms for Survival Analysis", "comment": null, "summary": "This study presents a comparative methodological analysis of six machine\nlearning models for survival analysis (MLSA). Using data from nearly 45,000\ncolorectal cancer patients in the Hospital-Based Cancer Registries of S\\~ao\nPaulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for\nSurvival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),\nXGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival\nconsidering censored data. Hyperparameter optimization was performed with\ndifferent samplers, and model performance was assessed using the Concordance\nIndex (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score\n(IBS). Survival curves produced by the models were compared with predictions\nfrom classification algorithms, and predictor interpretation was conducted\nusing SHAP and permutation importance. XGB-AFT achieved the best performance\n(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results\nhighlight the potential and applicability of MLSA to improve survival\nprediction and support decision making.", "AI": {"tldr": "\u5bf9\u516d\u79cd\u673a\u5668\u5b66\u4e60\u751f\u5b58\u5206\u6790\u6a21\u578b\u5728\u7ea64.5\u4e07\u4f8b\u7ed3\u76f4\u80a0\u764c\u60a3\u8005\u4e2d\u7684\u6bd4\u8f83\u5206\u6790\uff0cXGB-AFT\u8868\u73b0\u6700\u4f73\uff0cC-Index 0.7618\uff0c\u663e\u793aMLSAs\u5728\u751f\u5b58\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u65e8\u5728\u6bd4\u8f83\u4e0d\u540cMLSAs\u5728\u5e26\u5220\u5931\u6570\u636e\u7684\u751f\u5b58\u5206\u6790\u4e2d\u7684\u9884\u6d4b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u6bd4\u8f83 RSF\u3001GBSA\u3001SSVM\u3001XGB-Cox\u3001XGB-AFT\u3001LGBM \u516d\u79cd\u6a21\u578b\uff0c\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\uff08\u4f7f\u7528\u4e0d\u540c\u91c7\u6837\u5668\uff09\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ec C-Index\u3001C-Index IPCW\u3001\u65f6\u95f4\u4f9d\u8d56AUC\u3001\u7efc\u5408Brier\u5206\u6570IBS\uff1b\u4ee5\u751f\u5b58\u66f2\u7ebf\u6bd4\u8f83\u5206\u7c7b\u7b97\u6cd5\u9884\u6d4b\uff0c\u5e76\u7528SHAP\u4e0e\u7f6e\u6362\u91cd\u8981\u6027\u8bc4\u4f30\u53d8\u91cf\u89e3\u91ca\u6027\u3002", "result": "XGB-AFT\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff1aC-Index 0.7618\uff1bIPCW 0.7532\uff1bGBSA\u4e0eRSF\u7d27\u968f\u5176\u540e\u3002\u6a21\u578b\u53ef\u4e0e\u5206\u7c7b\u7b97\u6cd5\u7684\u751f\u5b58\u66f2\u7ebf\u76f8\u6bd4\u5bf9\uff0c\u663e\u793aMLSAs\u53ef\u63d0\u5347\u751f\u5b58\u9884\u6d4b\u5e76\u652f\u6301\u51b3\u7b56\u3002", "conclusion": "MLSAs\u5728\u7ed3\u76f4\u80a0\u764c\u751f\u5b58\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2510.24500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24500", "abs": "https://arxiv.org/abs/2510.24500", "authors": ["Yong Huang", "Zhongqi Yang", "Amir Rahmani"], "title": "MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis Trajectories in the ICU", "comment": null, "summary": "Sepsis is a leading cause of mortality in intensive care units (ICUs), yet\nexisting research often relies on outdated datasets, non-reproducible\npreprocessing pipelines, and limited coverage of clinical interventions. We\nintroduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from\nthe MIMIC-IV database, designed to support reproducible modeling of sepsis\ntrajectories. Our cohort includes 35,239 ICU patients with time-aligned\nclinical variables and standardized treatment data, including vasopressors,\nfluids, mechanical ventilation and antibiotics. We describe a transparent\npreprocessing pipeline-based on Sepsis-3 criteria, structured imputation\nstrategies, and treatment inclusion-and release it alongside benchmark tasks\nfocused on early mortality prediction, length-of-stay estimation, and shock\nonset classification. Empirical results demonstrate that incorporating\ntreatment variables substantially improves model performance, particularly for\nTransformer-based architectures. MIMIC-Sepsis serves as a robust platform for\nevaluating predictive and sequential models in critical care research.", "AI": {"tldr": "\u63d0\u51fa MIMIC-Sepsis\uff1a\u6765\u81ea MIMIC-IV \u7684 35,239 ICU \u75c5\u4eba\u65f6\u95f4\u5bf9\u9f50\u53d8\u91cf\u4e0e\u6cbb\u7597\u6570\u636e\u7684\u6807\u51c6\u5316\u961f\u5217\u548c\u57fa\u51c6\u6846\u67b6\uff0c\u63d0\u4f9b\u57fa\u4e8e Sepsis-3 \u7684\u9884\u5904\u7406\u3001\u586b\u5145\u7b56\u7565\u548c\u7528\u836f\u6570\u636e\u7684\u5f00\u653e\u57fa\u51c6\u4efb\u52a1\uff0c\u663e\u793a\u7eb3\u5165\u6cbb\u7597\u53d8\u91cf\u663e\u8457\u63d0\u5347\u6a21\u578b\uff08\u5c24\u5176\u662f Transformer\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u4f7f\u7528\u8fc7\u65f6\u6570\u636e\u96c6\u3001\u4e0d\u53ef\u91cd\u590d\u7684\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u4ee5\u53ca\u5bf9\u6cbb\u7597\u5e72\u9884\u8986\u76d6\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u4e2a\u53ef\u91cd\u590d\u3001\u53ef\u6bd4\u8f83\u7684\u961f\u5217\u4e0e\u57fa\u51c6\u6765\u8bc4\u4f30\u5371\u91cd\u79d1\u5ba4\u7684\u9884\u6d4b\u4e0e\u5e8f\u5217\u6a21\u578b\u3002", "method": "\u4ece MIMIC-IV \u6784\u5efa\u961f\u5217\uff0c\u5305\u542b 35,239 \u4f8b ICU \u75c5\u4eba\uff0c\u65f6\u95f4\u5bf9\u9f50\u7684\u4e34\u5e8a\u53d8\u91cf\u4e0e\u6807\u51c6\u5316\u6cbb\u7597\u6570\u636e\uff08\u8840\u7ba1\u6d3b\u6027\u836f\u7269\u3001\u6db2\u4f53\u3001\u673a\u68b0\u901a\u6c14\u3001\u6297\u751f\u7d20\uff09\uff1b\u57fa\u4e8e Sepsis-3 \u7684\u9884\u5904\u7406\u7ba1\u7ebf\u3001\u7ed3\u6784\u5316\u63d2\u8865\u7b56\u7565\u3001\u6cbb\u7597\u7eb3\u5165\u4e0e\u53d1\u5e03\uff1b\u63d0\u4f9b\u65e9\u671f\u6b7b\u4ea1\u9884\u6d4b\u3001\u4f4f\u9662\u65f6\u957f\u4f30\u8ba1\u3001\u4f11\u514b\u53d1\u75c5\u5206\u7c7b\u7b49\u57fa\u51c6\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5c06\u6cbb\u7597\u53d8\u91cf\u7eb3\u5165\u6a21\u578b\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u5bf9 Transformer \u67b6\u6784\u5c24\u5176\u6709\u5229\u3002", "conclusion": "MIMIC-Sepsis \u6210\u4e3a\u8bc4\u4f30\u5371\u91cd\u62a4\u7406\u9884\u6d4b\u4e0e\u5e8f\u5217\u6a21\u578b\u7684\u9c81\u68d2\u5e73\u53f0\uff0c\u4fc3\u8fdb\u53ef\u91cd\u590d\u3001\u53ef\u6bd4\u8f83\u7684\u7814\u7a76\u3002"}}
{"id": "2510.24503", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24503", "abs": "https://arxiv.org/abs/2510.24503", "authors": ["Mortesa Hussaini", "Jan Thei\u00df", "Anthony Stein"], "title": "Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments", "comment": null, "summary": "In the context of Federated Learning with heterogeneous data environments,\nlocal models tend to converge to their own local model optima during local\ntraining steps, deviating from the overall data distributions. Aggregation of\nthese local updates, e.g., with FedAvg, often does not align with the global\nmodel optimum (client drift), resulting in an update that is suboptimal for\nmost clients. Personalized Federated Learning approaches address this challenge\nby exclusively focusing on the average local performances of clients' models on\ntheir own data distribution. Generalization to out-of-distribution samples,\nwhich is a substantial benefit of FedAvg and represents a significant component\nof robustness, appears to be inadequately incorporated into the assessment and\nevaluation processes. This study involves a thorough evaluation of Federated\nLearning approaches, encompassing both their local performance and their\ngeneralization capabilities. Therefore, we examine different stages within a\nsingle communication round to enable a more nuanced understanding of the\nconsidered metrics. Furthermore, we propose and incorporate a modified approach\nof FedAvg, designated as Federated Learning with Individualized Updates (FLIU),\nextending the algorithm by a straightforward individualization step with an\nadaptive personalization factor. We evaluate and compare the approaches\nempirically using MNIST and CIFAR-10 under various distributional conditions,\nincluding benchmark IID and pathological non-IID, as well as additional novel\ntest environments with Dirichlet distribution specifically developed to stress\nthe algorithms on complex data heterogeneity.", "AI": {"tldr": "\u63d0\u51faFLIU\u5e76\u5728\u591a\u79cdIID/\u975eIID\u5206\u5e03\u4e0b\u5bf9FedAvg\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u5f3a\u8c03\u5c40\u90e8\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u5ba2\u6237\u7aef\u6f02\u79fb\u5bfc\u81f4\u5168\u5c40\u6700\u4f18\u4e0e\u672c\u5730\u6a21\u578b\u5c40\u90e8\u6700\u4f18\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5e76\u6307\u51fa\u5bf9\u6cdb\u5316\u80fd\u529b\uff08\u5c24\u5176\u662f\u5bf9out-of-distribution\u6837\u672c\u7684\u9c81\u68d2\u6027\uff09\u7684\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "\u5728FedAvg\u57fa\u7840\u4e0a\u589e\u52a0\u4e00\u4e2a\u7b80\u6613\u7684\u4e2a\u6027\u5316\u66f4\u65b0\u6b65\u9aa4\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u56e0\u5b50\uff1b\u5728MNIST\u3001CIFAR-10\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8986\u76d6IID\u3001\u975eIID\u3001Dirichlet\u5206\u5e03\u7b49\u590d\u6742\u6570\u636e\u5f02\u8d28\u6027\u573a\u666f\uff0c\u5e76\u5728\u5355\u8f6e\u7684\u4e0d\u540c\u9636\u6bb5\u5206\u6790\u591a\u79cd\u6307\u6807\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u6846\u67b6\u7528\u4e8e\u8bc4\u4f30\u5c40\u90e8\u8868\u73b0\u4e0e\u6cdb\u5316\u80fd\u529b\u5728\u4e0d\u540c\u5206\u5e03\u4e0e\u9636\u6bb5\u7684\u8868\u73b0\uff1b\u63d0\u51fa\u7684FLIU\u5728\u8003\u8651\u4e2a\u6027\u5316\u66f4\u65b0\u540e\uff0c\u5bf9\u5c40\u90e8\u4e0e\u5168\u5c40\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u5b9a\u91cf\u8bc4\u4f30\u9014\u5f84\u3002", "conclusion": "\u5f3a\u8c03\u540c\u65f6\u8bc4\u4f30\u5c40\u90e8\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684FLIU\u63d0\u4f9b\u4e00\u79cd\u7b80\u5355\u4e14\u6709\u6548\u7684FedAvg\u6269\u5c55\uff0c\u4f7f\u5f97\u4e2a\u6027\u5316\u66f4\u65b0\u80fd\u66f4\u597d\u5339\u914d\u5404\u5ba2\u6237\u7aef\u7684\u6570\u636e\u5206\u5e03\u3002"}}
{"id": "2510.24561", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24561", "abs": "https://arxiv.org/abs/2510.24561", "authors": ["Qingyue Zhang", "Chang Chu", "Tianren Peng", "Qi Li", "Xiangyang Luo", "Zhihao Jiang", "Shao-Lun Huang"], "title": "LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis", "comment": null, "summary": "With the widespread adoption of LLMs, LoRA has become a dominant method for\nPEFT, and its initialization methods have attracted increasing attention.\nHowever, existing methods have notable limitations: many methods do not\nincorporate target-domain data, while gradient-based methods exploit data only\nat a shallow level by relying on one-step gradient decomposition, which remains\nunsatisfactory due to the weak empirical performance of the one-step\nfine-tuning model that serves as their basis, as well as the fact that these\nmethods either lack a rigorous theoretical foundation or depend heavily on\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\nframework for data-aware LoRA initialization based on asymptotic analysis.\nStarting from a general optimization objective that minimizes the expectation\nof the parameter discrepancy between the fine-tuned and target models, we\nderive an optimization problem with two components: a bias term, which is\nrelated to the parameter distance between the fine-tuned and target models, and\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\na variance term, which accounts for the uncertainty introduced by sampling\nstochasticity through the Fisher information. By solving this problem, we\nobtain an optimal initialization strategy for LoRA. Building on this\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\nestimates the terms in the optimization problem from a small set of target\ndomain samples and obtains the optimal LoRA initialization. Empirical results\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\naccuracy over existing initialization methods. Additional studies show faster,\nmore stable convergence, robustness across ranks, and only a small\ninitialization overhead for LoRA-DA. The source code will be released upon\npublication.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6570\u636e\u611f\u77e5\u7684LoRA\u521d\u59cb\u5316\u65b9\u6cd5LoRA-DA\uff0c\u5728\u7406\u8bba\u4e0a\u5c06LoRA\u521d\u59cb\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u504f\u7f6e\u9879\u548c\u65b9\u5dee\u9879\u7684\u6700\u4f18\u5316\uff0c\u5229\u7528Fisher\u4fe1\u606f\u4fdd\u6301\u5404\u5411\u5f02\u6027\u5e76\u901a\u8fc7\u5c11\u91cf\u76ee\u6807\u57df\u6837\u672c\u4f30\u8ba1\u53c2\u6570\u3002\u5b9e\u8bc1\u663e\u793a\u5728\u591a\u9879\u57fa\u51c6\u4e0a\u9ad8\u4e8e\u73b0\u6709\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u6536\u655b\u66f4\u5feb\u3001\u66f4\u7a33\u5b9a\u3001\u5bf9\u79e9\u9c81\u68d2\uff0c\u5e76\u4e14\u521d\u59cb\u5316\u5f00\u9500\u8f83\u5c0f\u3002", "motivation": "\u73b0\u6709\u7684LoRA\u521d\u59cb\u5316\u65b9\u6cd5\u8981\u4e48\u4e0d\u5229\u7528\u76ee\u6807\u57df\u6570\u636e\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u4e00\u6b21\u68af\u5ea6\u5206\u89e3\uff0c\u7406\u8bba\u57fa\u7840\u8584\u5f31\u4e14\u5e38\u5047\u8bbe\u5404\u5411\u540c\u6027\u3002\u9700\u8981\u4e00\u4e2a\u6570\u636e\u611f\u77e5\u3001\u7406\u8bba\u4e0a\u6709\u652f\u6491\u7684\u521d\u59cb\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u5404\u5411\u5f02\u6027\u7684\u524d\u63d0\u4e0b\u5229\u7528\u76ee\u6807\u57df\u6570\u636e\u4ee5\u63d0\u5347\u6700\u7ec8\u6027\u80fd\u3002", "method": "\u4ece\u6700\u5c0f\u5316\u5fae\u8c03\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u4e4b\u95f4\u53c2\u6570\u5dee\u5f02\u7684\u671f\u671b\u51fa\u53d1\uff0c\u5efa\u7acb\u5305\u542b\u4e24\u90e8\u5206\u7684\u4f18\u5316\u95ee\u9898\uff1a\u4e00\u4e2a\u504f\u7f6e\u9879\uff0c\u8868\u793aFine-tuned\u4e0e\u76ee\u6807\u6a21\u578b\u4e4b\u95f4\u7684\u53c2\u6570\u8ddd\u79bb\uff0c\u4f7f\u7528Fisher-\u68af\u5ea6\u8fd1\u4f3c\u4ee5\u4fdd\u7559\u5404\u5411\u5f02\u6027\uff1b\u4e00\u4e2a\u65b9\u5dee\u9879\uff0c\u901a\u8fc7Fisher\u4fe1\u606f\u523b\u753b\u91c7\u6837\u968f\u673a\u6027\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u901a\u8fc7\u6c42\u89e3\u8be5\u95ee\u9898\u5f97\u5230LoRA\u7684\u6700\u4f18\u521d\u59cb\u5316\u3002\u57fa\u4e8e\u8be5\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51faLoRA-DA\u7b97\u6cd5\uff1a\u4ec5\u7528\u5c11\u91cf\u76ee\u6807\u57df\u6837\u672c\u4f30\u8ba1\u95ee\u9898\u4e2d\u7684\u5404\u9879\u5e76\u5f97\u5230\u6700\u4f18LoRA\u521d\u59cb\u5316\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u4e0a\uff0cLoRA-DA\u76f8\u8f83\u73b0\u6709\u521d\u59cb\u5316\u65b9\u6cd5\u5728\u6700\u7ec8\u51c6\u786e\u7387\u4e0a\u5177\u6709\u4e00\u81f4\u7684\u63d0\u5347\uff1b\u8fd8\u8868\u73b0\u51fa\u66f4\u5feb\u3001\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u3001\u5bf9\u79e9\u7684\u9c81\u68d2\u6027\u589e\u5f3a\uff0c\u4ee5\u53ca\u4ec5\u6709\u8f83\u5c0f\u7684\u521d\u59cb\u5316\u5f00\u9500\u3002\u4f5c\u8005\u8fd8\u8ba1\u5212\u53d1\u5e03\u6e90\u4ee3\u7801\u3002", "conclusion": "\u4ee5\u6e10\u8fdb\u6781\u9650\u5206\u6790\u4e3a\u652f\u6491\u7684\u6570\u636e\u611f\u77e5LoRA\u521d\u59cb\u5316\u6846\u67b6\u80fd\u591f\u5728\u4fdd\u6301\u5404\u5411\u5f02\u6027\u7684\u540c\u65f6\u6709\u6548\u5229\u7528\u76ee\u6807\u57df\u6570\u636e\uff0c\u63d0\u5347LoRA\u7684\u521d\u59cb\u5316\u6548\u679c\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002LoRA-DA\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u521d\u59cb\u5316\u65b9\u6848\uff0c\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u6f5c\u529b\u3002"}}
{"id": "2510.24574", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24574", "abs": "https://arxiv.org/abs/2510.24574", "authors": ["Hao Wang", "Licheng Pan", "Yuan Lu", "Zhixuan Chu", "Xiaoxi Li", "Shuting He", "Zhichao Chen", "Haoxuan Li", "Qingsong Wen", "Zhouchen Lin"], "title": "DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment", "comment": null, "summary": "Training time-series forecast models requires aligning the conditional\ndistribution of model forecasts with that of the label sequence. The standard\ndirect forecast (DF) approach resorts to minimize the conditional negative\nlog-likelihood of the label sequence, typically estimated using the mean\nsquared error. However, this estimation proves to be biased in the presence of\nlabel autocorrelation. In this paper, we propose DistDF, which achieves\nalignment by alternatively minimizing a discrepancy between the conditional\nforecast and label distributions. Because conditional discrepancies are\ndifficult to estimate from finite time-series observations, we introduce a\nnewly proposed joint-distribution Wasserstein discrepancy for time-series\nforecasting, which provably upper bounds the conditional discrepancy of\ninterest. This discrepancy admits tractable, differentiable estimation from\nempirical samples and integrates seamlessly with gradient-based training.\nExtensive experiments show that DistDF improves the performance diverse\nforecast models and achieves the state-of-the-art forecasting performance. Code\nis available at https://anonymous.4open.science/r/DistDF-F66B.", "AI": {"tldr": "DistDF \u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65f6\u5e8f\u9884\u6d4b\u7684\u8054\u5408\u5206\u5e03 Wasserstein \u8ddd\u79bb\uff0c\u7528\u4ee5\u5bf9\u9f50\u6761\u4ef6\u9884\u6d4b\u5206\u5e03\u4e0e\u6807\u7b7e\u5e8f\u5217\u7684\u5206\u5e03\uff0c\u63d0\u4f9b\u53ef\u5fae\u4e14\u53ef\u4f30\u8ba1\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u591a\u79cd\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u76f4\u63a5\u9884\u6d4b\uff08DF\uff09\u5728\u5904\u7406\u5e26\u81ea\u76f8\u5173\u7684\u6807\u7b7e\u5e8f\u5217\u65f6\uff0c\u4f7f\u7528\u5e73\u5747\u5e73\u65b9\u8bef\u5dee\u7b49\u5c40\u90e8\u6761\u4ef6\u6307\u6807\u53ef\u80fd\u5e26\u6765\u504f\u5dee\u7684\u95ee\u9898\uff1b\u9700\u8981\u4e00\u79cd\u80fd\u4ece\u5168\u5c40\u5c42\u9762\u5bf9\u9f50\u9884\u6d4b\u5206\u5e03\u4e0e\u6807\u7b7e\u5206\u5e03\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa DistDF\uff0c\u901a\u8fc7\u4ea4\u66ff\u6700\u5c0f\u5316\u9884\u6d4b\u5206\u5e03\u4e0e\u6807\u7b7e\u5206\u5e03\u4e4b\u95f4\u7684\u6761\u4ef6\u5206\u5e03\u5dee\u5f02\uff1b\u5f15\u5165\u65f6\u5e8f\u7684\u8054\u5408\u5206\u5e03 Wasserstein \u5dee\u5f02\uff0c\u8be5\u5dee\u5f02\u53ef\u8bc1\u81ea\u4e0a\u754c\u6761\u4ef6\u5dee\u5f02\uff0c\u4e14\u53ef\u4ece\u7ecf\u9a8c\u6837\u672c\u4e2d\u53ef\u5fae\u5730\u4f30\u8ba1\uff0c\u4fbf\u4e8e\u7ed3\u5408\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u663e\u793a DistDF \u80fd\u63d0\u5347\u591a\u79cd\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u6548\u679c\u3002", "conclusion": "DistDF \u4e3a\u65f6\u5e8f\u9884\u6d4b\u4e2d\u7684\u6761\u4ef6\u5206\u5e03\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u5b9e\u73b0\u3001\u53ef\u4f18\u5316\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.24577", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24577", "abs": "https://arxiv.org/abs/2510.24577", "authors": ["He Yang", "Fei Ren", "Hai-Sui Yu", "Xiaohui Chen", "Pei-Zhi Zhuang"], "title": "Physics-Informed Extreme Learning Machine (PIELM): Opportunities and Challenges", "comment": null, "summary": "We are very delighted to see the fast development of physics-informed extreme\nlearning machine (PIELM) in recent years for higher computation efficiency and\naccuracy in physics-informed machine learning. As a summary or review on PIELM\nis currently not available, we would like to take this opportunity to show our\nperspective and experience for this promising research direction. We can see\nmany efforts are made to solve PDEs with sharp gradients, nonlinearities,\nhigh-frequency behavior, hard constraints, uncertainty, multiphysics coupling.\nDespite the success, many urgent challenges remain to be tackled, which also\nprovides us opportunities to develop more robust, interpretable, and\ngeneralizable PIELM frameworks with applications in science and engineering.", "AI": {"tldr": "\u5bf9\u7269\u7406\u4fe1\u606f\u6781\u9650\u5b66\u4e60\u673a\uff08PIELM\uff09\u7684\u7efc\u8ff0\u6027\u5206\u6790\uff0c\u805a\u7126\u5176\u5728\u6c42\u89e3\u5177\u6709\u5c16\u9510\u68af\u5ea6\u3001\u975e\u7ebf\u6027\u3001\u9ad8\u9891\u632f\u8361\u7b49PDE\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\u3001\u6311\u6218\u4e0e\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "PIELM\u5728\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\u65b9\u9762\u53d6\u5f97\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u7efc\u8ff0\uff0c\u672c\u6587\u5c1d\u8bd5\u68b3\u7406\u7814\u7a76\u73b0\u72b6\u3001\u7ecf\u9a8c\u4e0e\u672a\u6765\u6311\u6218\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4ee5\u89c2\u70b9\u6027\u7efc\u8ff0\u7684\u5f62\u5f0f\uff0c\u57fa\u4e8e\u4f5c\u8005\u7ecf\u9a8c\u603b\u7ed3PIELM\u5728PDE\u6c42\u89e3\u4e2d\u7684\u65b9\u6cd5\u3001\u9002\u7528\u573a\u666f\u3001\u9762\u4e34\u7684\u95ee\u9898\uff0c\u5e76\u8ba8\u8bba\u53ef\u6539\u8fdb\u7684\u65b9\u5411\uff08\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u80fd\u529b\u3001\u8de8\u9886\u57df\u5e94\u7528\u7b49\uff09\u3002", "result": "\u63d0\u4f9b\u5bf9PIELM\u76f8\u5173\u7814\u7a76\u7684\u7ed3\u6784\u5316\u89c1\u89e3\u3001\u5bf9\u4e0d\u540c\u95ee\u9898\uff08\u5982\u5c16\u9510\u68af\u5ea6\u3001\u975e\u7ebf\u6027\u3001\u9ad8\u9891\u3001\u8026\u5408\u591a\u7269\u7406\u573a\u7b49\uff09\u7684\u9002\u7528\u6027\u5206\u6790\uff0c\u4ee5\u53ca\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u4e0e\u673a\u4f1a\u3002", "conclusion": "\u547c\u5401\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u3001\u53ef\u63a8\u5e7f\u7684PIELM\u6846\u67b6\uff0c\u5e76\u5728\u79d1\u5b66\u4e0e\u5de5\u7a0b\u5e94\u7528\u4e2d\u6269\u5927\u5176\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2510.24633", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.24633", "abs": "https://arxiv.org/abs/2510.24633", "authors": ["Mingyue Liu", "Andrew Cropper"], "title": "Symbolic Snapshot Ensembles", "comment": null, "summary": "Inductive logic programming (ILP) is a form of logical machine learning. Most\nILP algorithms learn a single hypothesis from a single training run. Ensemble\nmethods train an ILP algorithm multiple times to learn multiple hypotheses. In\nthis paper, we train an ILP algorithm only once and save intermediate\nhypotheses. We then combine the hypotheses using a minimum description length\nweighting scheme. Our experiments on multiple benchmarks, including game\nplaying and visual reasoning, show that our approach improves predictive\naccuracy by 4% with less than 1% computational overhead.", "AI": {"tldr": "\u5728\u5355\u6b21\u8bad\u7ec3\u4e2d\u4fdd\u5b58\u4e2d\u95f4\u7684 ILP \u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u6743\u91cd\u8fdb\u884c\u7ec4\u5408\uff0c\u6784\u5efa\u4e00\u4e2a\u9ad8\u6548\u7684 ILP \u96c6\u6210\u65b9\u6cd5\uff0c\u5728\u591a\u9879\u57fa\u51c6\u4e0a\u63d0\u5347\u7ea64%\u7684\u51c6\u786e\u6027\uff0c\u5f00\u9500\u5c0f\u4e8e1%\u3002", "motivation": "\u89e3\u51b3 ILP \u96c6\u6210\u6210\u672c\u9ad8\u3001\u9700\u8981\u591a\u6b21\u72ec\u7acb\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5728\u4e00\u6b21\u8bad\u7ec3\u4e2d\u4fdd\u5b58\u4e2d\u95f4\u7ed3\u679c\u5e76\u8fdb\u884c\u52a0\u6743\u7ec4\u5408\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u96c6\u6210\u5b66\u4e60\u3002", "method": "\u5728\u4e00\u6b21\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u5b58\u4e2d\u95f4\u4ea7\u751f\u7684\u5047\u8bbe\uff0c\u5e76\u4f7f\u7528\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u4f5c\u4e3a\u6743\u91cd\u5bf9\u8fd9\u4e9b\u5047\u8bbe\u8fdb\u884c\u52a0\u6743\u7ec4\u5408\uff0c\u5f62\u6210\u6700\u7ec8\u7684\u96c6\u6210\u6a21\u578b\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\uff08\u5305\u62ec\u535a\u5f08\u4e0e\u89c6\u89c9\u63a8\u7406\uff09\u4e0a\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u5347\u7ea64%\uff0c\u8ba1\u7b97\u5f00\u9500\u4e0d\u8db31%\u3002", "conclusion": "\u5355\u6b21\u8bad\u7ec3\u3001\u4fdd\u7559\u4e2d\u95f4\u5047\u8bbe\u5e76\u4ee5 MDL \u6743\u91cd\u6574\u5408\u7684\u7b56\u7565\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6bd4\u5355\u4e00 ILP \u66f4\u4f18\u7684\u6027\u80fd\uff0c\u4e14\u6210\u672c\u6781\u4f4e\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.24639", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24639", "abs": "https://arxiv.org/abs/2510.24639", "authors": ["Pedro P. Sanchez", "Damian Machlanski", "Steven McDonagh", "Sotirios A. Tsaftaris"], "title": "Causal Ordering for Structure Learning From Time Series", "comment": "32 pages", "summary": "Predicting causal structure from time series data is crucial for\nunderstanding complex phenomena in physiology, brain connectivity, climate\ndynamics, and socio-economic behaviour. Causal discovery in time series is\nhindered by the combinatorial complexity of identifying true causal\nrelationships, especially as the number of variables and time points grow. A\ncommon approach to simplify the task is the so-called ordering-based methods.\nTraditional ordering methods inherently limit the representational capacity of\nthe resulting model. In this work, we fix this issue by leveraging multiple\nvalid causal orderings, instead of a single one as standard practice. We\npropose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based\ncausal discovery for temporal data. By integrating multiple orderings, DOTS\neffectively recovers the transitive closure of the underlying directed acyclic\ngraph, mitigating spurious artifacts inherent in single-ordering approaches. We\nformalise the problem under standard assumptions such as stationarity and the\nadditive noise model, and leverage score matching with diffusion processes to\nenable efficient Hessian estimation. Extensive experiments validate the\napproach. Empirical evaluations on synthetic and real-world datasets\ndemonstrate that DOTS outperforms state-of-the-art baselines, offering a\nscalable and robust approach to temporal causal discovery. On synthetic\nbenchmarks ($d{=}\\!3-\\!6$ variables, $T{=}200\\!-\\!5{,}000$ samples), DOTS\nimproves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the\nCausalTime real-world benchmark ($d{=}20\\!-\\!36$), while baselines remain the\nbest on individual datasets, DOTS attains the highest average summary-graph\n$F1$ while halving runtime relative to graph-optimisation methods. These\nresults establish DOTS as a scalable and accurate solution for temporal causal\ndiscovery.", "AI": {"tldr": "DOTS \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u8fc7\u7a0b\u7684\u65f6\u5e8f\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u91cd\u6709\u6548\u6392\u5e8f\u6765\u63a8\u65ad\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u7ed3\u6784\uff0c\u80fd\u591f\u6062\u590d\u4f20\u9012\u95ed\u5305\u3001\u63d0\u9ad8 F1\uff0c\u5e76\u5177\u5907\u826f\u597d\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u65f6\u5e8f\u6570\u636e\u7684\u56e0\u679c\u53d1\u73b0\u53d7\u9650\u4e8e\u53d8\u91cf\u6570\u91cf\u4e0e\u65f6\u95f4\u70b9\u7684\u6307\u6570\u589e\u957f\u4ee5\u53ca\u5355\u4e00\u6392\u5e8f\u7684\u8868\u793a\u80fd\u529b\uff0c\u9700\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u5229\u7528\u591a\u91cd\u6392\u5e8f\u4fe1\u606f\u3002", "method": "\u63d0\u51fa DOTS\uff0c\u4f7f\u7528\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u5206\u6570\u5339\u914d\u5b9e\u73b0\u9ad8\u6548 Hessian \u4f30\u8ba1\uff0c\u5e76\u5728\u591a\u79cd\u53ef\u884c\u56e0\u679c\u6392\u5e8f\u4e0b\u63a8\u65ad\u65f6\u5e8f\u56fe\u7684\u4f20\u9012\u95ed\u5305\uff0c\u63d0\u5347\u5bf9\u771f\u5b9e\u56e0\u679c\u7ed3\u6784\u7684\u6062\u590d\u80fd\u529b\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\uff08d=3-6, T=200-5000\uff09\u4e2d\uff0c\u5e73\u5747\u7a97\u53e3\u56fe F1 \u4ece 0.63 \u63d0\u5347\u5230 0.81\uff1b\u5728 CausalTime \u57fa\u51c6\uff08d=20-36\uff09\u4e0a\uff0cDOTS \u53d6\u5f97\u6700\u9ad8\u7684\u6458\u8981\u56fe F1\uff0c\u4e14\u8fd0\u884c\u65f6\u95f4\u76f8\u6bd4\u56fe\u4f18\u5316\u65b9\u6cd5\u964d\u4f4e\u7ea6\u4e00\u534a\uff0c\u6574\u4f53\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "DOTS \u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u65f6\u5e8f\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u6392\u5e8f\uff0c\u6709\u6548\u51cf\u5c11\u5355\u6392\u5e8f\u5e26\u6765\u7684\u4f2a\u76f8\u5173\u4e0e\u7ed3\u6784\u504f\u5dee\uff0c\u9002\u7528\u4e8e\u590d\u6742\u591a\u53d8\u91cf\u65f6\u5e8f\u6570\u636e\u7684\u56e0\u679c\u63a8\u65ad\u3002"}}
{"id": "2510.24670", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.24670", "abs": "https://arxiv.org/abs/2510.24670", "authors": ["Genesis Research Team", "Alejandro Dobles", "Nina Jovic", "Kenneth Leidal", "Pranav Murugan", "David C. Williams", "Drausin Wulsin", "Nate Gruver", "Christina X. Ji", "Korrawat Pruegsanusak", "Gianluca Scarpellini", "Ansh Sharma", "Wojciech Swiderski", "Andrea Bootsma", "Richard Strong Bowen", "Charlotte Chen", "Jamin Chen", "Marc Andr\u00e9 D\u00e4mgen", "Roy Tal Dew", "Benjamin DiFrancesco", "J. D. Fishman", "Alla Ivanova", "Zach Kagin", "David Li-Bland", "Zuli Liu", "Igor Morozov", "Jeffrey Ouyang-Zhang", "Frank C. Pickard IV", "Kushal S. Shah", "Ben Shor", "Gabriel Monteiro da Silva", "Maxx Tessmer", "Carl Tilbury", "Cyr Vetcher", "Daniel Zeng", "Maruan Al-Shedivat", "Aleksandra Faust", "Evan N. Feinberg", "Michael V. LeVine", "Matteus Pan"], "title": "Pearl: A Foundation Model for Placing Every Atom in the Right Location", "comment": null, "summary": "Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining.", "AI": {"tldr": "Pearl introduces a scalable foundation model for protein-ligand cofolding featuring large-scale synthetic data, an SO(3)-equivariant diffusion module, and controllable inference via multi-chain templating. It achieves state-of-the-art performance on key metrics (RMSD<2 \u00c5, physically valid poses) and surpasses AlphaFold3 and open baselines on Runs N' Poses and PoseBusters, with further gains in pocket-conditional cofolding. Performance scales with training data size.", "motivation": "Predicting accurate 3D structures of protein\u2013ligand complexes is hindered by limited experimental data, inefficient architectures, physically invalid poses, and underutilization of auxiliary inference-time information. A scalable, physically aware foundation model could improve generalization and practical utility in drug design.", "method": "Three core innovations: (1) large-scale synthetic data training to overcome data scarcity; (2) an SO(3)-equivariant diffusion module that respects 3D rotational symmetry for better generalization and sample efficiency; (3) controllable inference with a generalized multi-chain templating system supporting both protein and non-polymeric components and dual unconditional/conditional modes.", "result": "Pearl achieves state-of-the-art performance in generating accurate (RMSD < 2 \u00c5) and physically valid poses, surpassing AlphaFold3 and other open baselines on Runs N' Poses and PoseBusters benchmarks by ~14.5% and ~14.2% respectively. In pocket-conditional cofolding on a challenging real-world target set, it shows ~3.6\u00d7 improvement at RMSD < 1 \u00c5. Model performance correlates with synthetic dataset size used in training.", "conclusion": "Pearl establishes a new state-of-the-art in protein\u2013ligand cofolding, demonstrating that large-scale synthetic data, SO(3)-equivariant diffusion, and controllable inference yield better 3D predictions and physical validity, with clear data-efficiency benefits and practical implications for drug discovery."}}
{"id": "2510.24672", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24672", "abs": "https://arxiv.org/abs/2510.24672", "authors": ["Burak Var\u0131c\u0131", "Che-Ping Tsai", "Ritabrata Ray", "Nicholas M. Boffi", "Pradeep Ravikumar"], "title": "Eigenfunction Extraction for Ordered Representation Learning", "comment": null, "summary": "Recent advances in representation learning reveal that widely used\nobjectives, such as contrastive and non-contrastive, implicitly perform\nspectral decomposition of a contextual kernel, induced by the relationship\nbetween inputs and their contexts. Yet, these methods recover only the linear\nspan of top eigenfunctions of the kernel, whereas exact spectral decomposition\nis essential for understanding feature ordering and importance. In this work,\nwe propose a general framework to extract ordered and identifiable\neigenfunctions, based on modular building blocks designed to satisfy key\ndesiderata, including compatibility with the contextual kernel and scalability\nto modern settings. We then show how two main methodological paradigms,\nlow-rank approximation and Rayleigh quotient optimization, align with this\nframework for eigenfunction extraction. Finally, we validate our approach on\nsynthetic kernels and demonstrate on real-world image datasets that the\nrecovered eigenvalues act as effective importance scores for feature selection,\nenabling principled efficiency-accuracy tradeoffs via adaptive-dimensional\nrepresentations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u7528\u4e8e\u4ece\u4e0a\u4e0b\u6587\u6838\uff08contextual kernel\uff09\u4e2d\u63d0\u53d6\u6709\u5e8f\u4e14\u53ef\u8fa8\u8bc6\u7684\u7279\u5f81\u672c\u5f81\uff08eigenfunctions\uff09\uff0c\u7a81\u7834\u4ec5\u80fd\u6062\u590d\u524d\u51e0\u4e2a\u7ebf\u6027\u7279\u5f81\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u548cRayleigh\u5546\u4f18\u5316\u4e24\u5927\u8303\u5f0f\u5b9e\u73b0\u672c\u5f81\u51fd\u6570\u63d0\u53d6\uff0c\u5e76\u5728\u5408\u6210\u6838\u548c\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u7279\u5f81\u503c\u4f5c\u4e3a\u91cd\u8981\u6027\u5206\u6570\u7528\u4e8e\u7279\u5f81\u9009\u62e9\u548c\u81ea\u9002\u5e94\u7ef4\u5ea6\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u9690\u5f0f\u8fdb\u884c\u6838\u7684\u8c31\u5206\u89e3\uff0c\u4f46\u901a\u5e38\u53ea\u6062\u590d\u6838\u7684\u524d\u51e0\u4e2a\u7279\u5f81\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u672a\u80fd\u83b7\u5f97\u5b8c\u6574\u7684\u6709\u5e8f\u4e14\u53ef\u8fa8\u8bc6\u7684\u7279\u5f81\u672c\u5f81\u3002\u8fd9\u9650\u5236\u4e86\u5bf9\u7279\u5f81\u6392\u5217\u548c\u91cd\u8981\u6027\u7684\u7406\u89e3\uff0c\u4ee5\u53ca\u53ef\u63a7\u7684\u7ef4\u5ea6\u7f29\u51cf\u80fd\u529b\uff0c\u4e14\u9700\u5177\u5907\u5bf9\u73b0\u4ee3\u6570\u636e\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u5757\u5316\u6784\u5efa\u5757\u7ec4\u6210\u7684\u901a\u7528\u6846\u67b6\uff0c\u786e\u4fdd\u4e0e\u4e0a\u4e0b\u6587\u6838\u7684\u517c\u5bb9\u6027\u5e76\u53ef\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6570\u636e\u3002\u5c06\u4e24\u6761\u4e3b\u7ebf\u201c\u4f4e\u79e9\u8fd1\u4f3c\u201d\u548c\u201cRayleigh\u5546\u4f18\u5316\u201d\u5bf9\u9f50\u5230\u8be5\u6846\u67b6\u4ee5\u5b9e\u73b0\u672c\u5f81\u51fd\u6570\u7684\u63d0\u53d6\uff0c\u5f3a\u8c03\u53ef\u8fa8\u8bc6\u6027\u3001\u53ef\u6269\u5c55\u6027\u4ee5\u53ca\u5bf9\u6838\u7684\u9002\u914d\u6027\u3002\u901a\u8fc7\u7406\u8bba\u6620\u5c04\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u5982\u4f55\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u83b7\u5f97\u6709\u5e8f\u7684\u7279\u5f81\u672c\u5f81\u53ca\u5176\u5bf9\u5e94\u7684\u7279\u5f81\u503c\u3002", "result": "\u5728\u5408\u6210\u6838\u4e0a\u9a8c\u8bc1 recovered \u672c\u5f81\u51fd\u6570\u4e0e ground truth \u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u663e\u793a\uff1a\u672c\u5f81\u503c\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u7279\u5f81\u91cd\u8981\u6027\u5206\u6570\uff0c\u652f\u6301\u901a\u8fc7\u81ea\u9002\u5e94\u7ef4\u5ea6\u8868\u793a\u5b9e\u73b0\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "\u7ed9\u51fa\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u6709\u5e8f\u672c\u5f81\u51fd\u6570\u63d0\u53d6\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4ec5\u590d\u73b0\u524d\u82e5\u5e72\u7ebf\u6027\u7279\u5f81\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u672c\u5f81\u503c\u5b9e\u73b0\u57fa\u4e8e\u8c31\u5206\u89e3\u7684\u7279\u5f81\u9009\u62e9\u3002"}}
{"id": "2510.24674", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24674", "abs": "https://arxiv.org/abs/2510.24674", "authors": ["Bram De Cooman", "Johan Suykens"], "title": "Learning to Drive Safely with Hybrid Options", "comment": null, "summary": "Out of the many deep reinforcement learning approaches for autonomous\ndriving, only few make use of the options (or skills) framework. That is\nsurprising, as this framework is naturally suited for hierarchical control\napplications in general, and autonomous driving tasks in specific. Therefore,\nin this work the options framework is applied and tailored to autonomous\ndriving tasks on highways. More specifically, we define dedicated options for\nlongitudinal and lateral manoeuvres with embedded safety and comfort\nconstraints. This way, prior domain knowledge can be incorporated into the\nlearning process and the learned driving behaviour can be constrained more\neasily. We propose several setups for hierarchical control with options and\nderive practical algorithms following state-of-the-art reinforcement learning\ntechniques. By separately selecting actions for longitudinal and lateral\ncontrol, the introduced policies over combined and hybrid options obtain the\nsame expressiveness and flexibility that human drivers have, while being easier\nto interpret than classical policies over continuous actions. Of all the\ninvestigated approaches, these flexible policies over hybrid options perform\nthe best under varying traffic conditions, outperforming the baseline policies\nover actions.", "AI": {"tldr": "\u5c06\u9009\u9879\u6846\u67b6\u5e94\u7528\u4e8e\u9ad8\u901f\u516c\u8def\u81ea\u52a8\u9a7e\u9a76\uff0c\u5b9a\u4e49\u7eb5\u5411\u4e0e\u6a2a\u5411\u64cd\u7eb5\u7684\u4e13\u7528\u9009\u9879\u5e76\u5d4c\u5165\u5b89\u5168\u548c\u8212\u9002\u7ea6\u675f\uff0c\u63d0\u51fa\u591a\u79cd\u5206\u5c42\u63a7\u5236\u8bbe\u7f6e\uff0c\u6df7\u5408\u9009\u9879\u7b56\u7565\u5728\u591a\u53d8\u4ea4\u901a\u6761\u4ef6\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u7684\u52a8\u4f5c\u7b56\u7565\uff0c\u540c\u65f6\u5177\u5907\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5c11\u6709\u5de5\u4f5c\u7cfb\u7edf\u6027\u5730\u5c06\u5c42\u6b21\u5316\u7684\u9009\u9879\u6846\u67b6\u7528\u4e8e\u9a7e\u9a76\u63a7\u5236\u3002\u9009\u9879\u6846\u67b6\u5929\u7136\u9002\u5408\u628a\u9886\u57df\u77e5\u8bc6\u4ee5\u5b89\u5168\u4e0e\u8212\u9002\u7ea6\u675f\u5d4c\u5165\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3001\u9c81\u68d2\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5b9a\u4e49\u7eb5\u5411\u4e0e\u6a2a\u5411 manoeuvres \u7684\u4e13\u7528\u9009\u9879\uff0c\u5e76\u5728\u6bcf\u4e2a\u9009\u9879\u4e2d\u5d4c\u5165\u5b89\u5168\u4e0e\u8212\u9002\u7ea6\u675f\u3002\u63d0\u51fa\u591a\u79cd\u5e26\u6709\u9009\u9879\u7684\u5206\u5c42\u63a7\u5236\u8bbe\u7f6e\uff0c\u5e76\u57fa\u4e8e\u524d\u6cbf\u7684\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u63a8\u5bfc\u53ef\u5b9e\u73b0\u7684\u7b97\u6cd5\u3002\u901a\u8fc7\u5c06\u7eb5\u5411\u4e0e\u6a2a\u5411\u63a7\u5236\u5206\u79bb\u9009\u62e9\u52a8\u4f5c\uff0c\u5f97\u5230\u5bf9\u7ec4\u5408\u548c\u6df7\u5408\u9009\u9879\u7684\u7b56\u7565\uff0c\u8fbe\u5230\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u76f8\u5f53\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u7075\u6d3b\u6027\uff0c\u540c\u65f6\u6bd4\u4f20\u7edf\u8fde\u7eed\u52a8\u4f5c\u7b56\u7565\u66f4\u6613\u89e3\u91ca\u3002", "result": "\u5728\u6240\u8bc4\u4f30\u7684\u573a\u666f\u4e2d\uff0c\u9762\u5411\u6df7\u5408\u9009\u9879\u7684\u7b56\u7565\u8868\u73b0\u6700\u597d\uff0c\u80fd\u5728\u53d8\u5316\u7684\u4ea4\u901a\u6761\u4ef6\u4e0b\u8d85\u8d8a\u57fa\u7ebf\u7684\u52a8\u4f5c\u7b56\u7565\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u7075\u6d3b\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "\u9009\u9879\u6846\u67b6\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5c24\u5176\u662f\u6df7\u5408\u9009\u9879\u7684\u5206\u5c42\u63a7\u5236\u65e2\u4fdd\u7559\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u53c8\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u4e0e\u5bf9\u5b89\u5168/\u8212\u9002\u7ea6\u675f\u7684\u878d\u5408\u80fd\u529b\u3002"}}
