<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.IR](#cs.IR) [Total: 17]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.LG](#cs.LG) [Total: 44]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Toward General Semantic Chunking: A Discriminative Framework for Ultra-Long Documents](https://arxiv.org/abs/2602.23370)
*Kaifeng Wu,Junyan Wu,Qiang Liu,Jiarui Zhang,Wen Xu*

Main category: cs.CL

TL;DR: 该论文针对超长文档主题分割问题，提出了一种基于Qwen3-0.6B的判别式分割模型，通过跨窗口上下文融合层和重叠滑动窗口策略，支持13k tokens的单次输入，并结合标量修正的向量融合方法提升下游检索效率，在WIKI-727K数据集上实现了更优的宏平均F1值且推理速度提升两个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有长文档主题分割方法存在明显缺陷：传统判别式模型受限于固定窗口，无法建模文档级语义；生成式大语言模型虽能输出段落边界，但推理成本高且难以支持超长输入。亟需一种高效、可扩展的超长文本分割方案。

Method: 1) 基于Qwen3-0.6B构建判别式分割模型，增加跨窗口上下文融合层和边界分类头；2) 采用重叠滑动窗口策略，支持单次输入高达13k tokens；3) 提出标量修正的向量融合方法，将超长段落表示压缩为单向量且不损失语义。

Result: 在WIKI-727K数据集上，相比Jina发布的三种基于Qwen2-0.5B的生成式模型，该方法取得了更优的宏平均F1值，且推理速度提升两个数量级。

Conclusion: 该模型显著提升了长文档处理的实用性与可扩展性，为超长文本的段落边界检测与下游检索任务提供了高效解决方案。

Abstract: Long-document topic segmentation plays an important role in information retrieval and document understanding, yet existing methods still show clear shortcomings in ultra-long text settings. Traditional discriminative models are constrained by fixed windows and cannot model document-level semantics; generative large language models can output paragraph boundaries, but inference is expensive and long inputs are difficult to support. To address these issues, we propose a discriminative segmentation model based on Qwen3-0.6B. On top of the backbone network, we add a cross-window context fusion layer and a boundary classification head, and combine them with an overlapping sliding-window strategy. Our model supports single-pass inputs of up to 13k tokens and can be extended to ultra-long documents for paragraph boundary detection. To further enhance downstream retrieval efficiency, we derive a vector fusion method with scalar correction, which compresses the representation of ultra-long segments into a single vector without semantic loss. Experiments on the Wikipedia long-document topic segmentation dataset WIKI-727K show that, compared with three generative models based on Qwen2-0.5B released by Jina, our method achieves a better macro-averaged F1 and delivers two orders of magnitude faster inference, substantially improving the practicality and scalability of long-document processing.

</details>


### [2] [Task-Lens: Cross-Task Utility Based Speech Dataset Profiling for Low-Resource Indian Languages](https://arxiv.org/abs/2602.23388)
*Swati Sharma,Divya V. Sharma,Anubha Gupta*

Main category: cs.CL

TL;DR: 本文提出Task-Lens框架，对50个印度语音数据集（涵盖26种语言）在9个下游任务上的适用性进行跨任务分析，发现许多数据集蕴含未充分利用的元数据可支持多任务应用，为资源匮乏语言的研究提供新方向。


<details>
  <summary>Details</summary>
Motivation: 随着包容性语音技术需求的增长，低资源语言的多语种数据集需求日益凸显，但现有研究对这些语言中特定任务的资源认知有限，尤其在印度这样语言多样性极高的国家。传统调查仅关注单一任务，缺乏跨任务分析，无法有效缓解数据稀缺问题。因此，本文旨在通过跨任务分析现有印度语音数据集，挖掘其在多个下游任务中的潜力，识别资源严重不足的任务和语言。

Method: 本研究提出Task-Lens框架，对50个印度语音数据集（覆盖26种语言）在9个下游语音任务上的准备度进行评估。首先分析哪些数据集包含适合特定任务的元数据和属性；其次提出任务对齐的增强方案以释放数据集的完整下游潜力；最后识别当前资源严重不足的任务和印度语言。

Result: 研究发现，许多印度语音数据集包含未被充分利用的元数据，可支持多个下游任务。通过揭示跨任务关联和缺口，Task-Lens使研究者能够探索现有数据集的更广泛应用，并优先为资源不足的任务和语言创建新数据集。

Conclusion: Task-Lens通过跨任务分析框架，不仅揭示了印度语音数据集在多任务应用中的潜力，还识别了关键的语言和任务缺口。该框架为研究者提供了系统化的方法，以最大化现有数据集的效用，并为低资源语言的数据集创建优先级提供指导，从而推动包容性语音技术的发展。

Abstract: The rising demand for inclusive speech technologies amplifies the need for multilingual datasets for Natural Language Processing (NLP) research. However, limited awareness of existing task-specific resources in low-resource languages hinders research. This challenge is especially acute in linguistically diverse countries, such as India. Cross-task profiling of existing Indian speech datasets can alleviate the data scarcity challenge. This involves investigating the utility of datasets across multiple downstream tasks rather than focusing on a single task. Prior surveys typically catalogue datasets for a single task, leaving comprehensive cross-task profiling as an open opportunity. Therefore, we propose Task-Lens, a cross-task survey that assesses the readiness of 50 Indian speech datasets spanning 26 languages for nine downstream speech tasks. First, we analyze which datasets contain metadata and properties suitable for specific tasks. Next, we propose task-aligned enhancements to unlock datasets to their full downstream potential. Finally, we identify tasks and Indian languages that are critically underserved by current resources. Our findings reveal that many Indian speech datasets contain untapped metadata that can support multiple downstream tasks. By uncovering cross-task linkages and gaps, Task-Lens enables researchers to explore the broader applicability of existing datasets and to prioritize dataset creation for underserved tasks and languages.

</details>


### [3] [Truncated Step-Level Sampling with Process Rewards for Retrieval-Augmented Reasoning](https://arxiv.org/abs/2602.23440)
*Chris Samarinas,Haw-Shiuan Chang,Hamed Zamani*

Main category: cs.CL

TL;DR: SLATE框架通过截断步骤采样和LLM-as-judge密集奖励，解决了LLM与搜索引擎强化学习中的信用分配问题，降低了梯度方差并在多跳QA任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练LLM与搜索引擎进行多步推理时面临信用分配难题——稀疏奖励无法归因到具体步骤，而过程奖励方法依赖启发式评分且采样完整轨迹导致高方差。

Method: 提出SLATE框架，结合截断步骤级采样（生成共享前缀、仅下一步不同的k个轨迹）和密集LLM-as-judge奖励（用LLM评估每一步推理、查询和答案质量），理论上证明其可将优势估计方差降低T倍。

Result: 在七个问答基准测试上，SLATE持续优于稀疏奖励和过程奖励基线，在多跳任务和较小模型上提升最显著。

Conclusion: SLATE通过截断采样和密集LLM评估有效解决了信用分配和方差问题，为LLM与工具结合的强化学习提供了更高效的训练范式。

Abstract: Training large language models to reason with search engines via reinforcement learning is hindered by a fundamental credit assignment problem: existing methods such as Search-R1 provide only a sparse outcome reward after an entire multi-step trajectory, making it infeasible to attribute success or failure to individual reasoning and retrieval decisions. Process-reward methods like StepSearch alleviate this by introducing step-level supervision, but rely on heuristic rewards such as TF-IDF overlap with gold documents, and still sample k complete trajectories per example, retaining high gradient variance. We propose SLATE, a framework built on two complementary ideas: (1) truncated step-level sampling, which generates k trajectories that share a common prefix and differ only at the next step, and (2) dense LLM-as-judge rewards, which replace heuristic scoring with a capable LLM evaluator that assesses the quality of each reasoning step, search query, and answer, providing richer and more reliable supervision. We theoretically prove that under the same dense reward structure, truncated sampling reduces the variance of advantage estimates by up to a factor of T compared to full-trajectory sampling for T-step trajectories, yielding lower-variance, better-targeted policy gradients. Experiments on seven QA benchmarks confirm that SLATE consistently outperforms both sparse-reward and process-reward baselines, with the largest gains on harder multi-hop tasks and smaller models.

</details>


### [4] [CiteAudit: You Cited It, But Did You Read It? A Benchmark for Verifying Scientific References in the LLM Era](https://arxiv.org/abs/2602.23452)
*Zhengqing Yuan,Kaiwen Shi,Zheyuan Zhang,Lichao Sun,Nitesh V. Chawla,Yanfang Ye*

Main category: cs.CL

TL;DR: 本研究针对大语言模型生成虚假但看似合理的学术引用问题，提出了首个综合性的基准测试与检测框架。该框架采用多智能体验证流程，通过声明提取、证据检索、段落匹配、推理和校准判断来评估引用真实性，构建了大规模人工验证数据集，实验表明其显著优于现有方法，为LLM时代的学术引用审计提供了可扩展的基础设施。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的虚假引用已出现在顶级机器学习会议的投稿和录用论文中，暴露了同行评审的漏洞；同时参考文献列表快速增长使人工核查不可行，而现有自动化工具对噪声和异构引用格式脆弱且缺乏标准化评估。

Method: 提出多智能体验证流水线，将引用检查分解为声明提取、证据检索、段落匹配、推理和校准判断五个步骤；构建了大规模跨领域人工验证数据集，并定义了统一的引用忠实度和证据对齐度量标准。

Result: 实验揭示了先进大语言模型存在大量引用错误，且本框架在准确性和可解释性上均显著优于先前方法。

Conclusion: 本工作为LLM时代的学术引用审计提供了首个可扩展的基础设施及实用工具，有助于提升科学参考文献的可靠性。

Abstract: Scientific research relies on accurate citation for attribution and integrity, yet large language models (LLMs) introduce a new risk: fabricated references that appear plausible but correspond to no real publications. Such hallucinated citations have already been observed in submissions and accepted papers at major machine learning venues, exposing vulnerabilities in peer review. Meanwhile, rapidly growing reference lists make manual verification impractical, and existing automated tools remain fragile to noisy and heterogeneous citation formats and lack standardized evaluation. We present the first comprehensive benchmark and detection framework for hallucinated citations in scientific writing. Our multi-agent verification pipeline decomposes citation checking into claim extraction, evidence retrieval, passage matching, reasoning, and calibrated judgment to assess whether a cited source truly supports its claim. We construct a large-scale human-validated dataset across domains and define unified metrics for citation faithfulness and evidence alignment. Experiments with state-of-the-art LLMs reveal substantial citation errors and show that our framework significantly outperforms prior methods in both accuracy and interpretability. This work provides the first scalable infrastructure for auditing citations in the LLM era and practical tools to improve the trustworthiness of scientific references.

</details>


### [5] [FHIRPath-QA: Executable Question Answering over FHIR Electronic Health Records](https://arxiv.org/abs/2602.23479)
*Michael Frew,Nishit Bheda,Bryan Tripp*

Main category: cs.CL

TL;DR: 本研究创建FHIRPath-QA数据集，提出将患者自然语言问题转为FHIRPath查询的范式，替代传统自由文本生成，减少LLM依赖与幻觉风险，揭示LLM在该任务上的性能短板及微调的有效性，为EHR精准问答提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有EHR患者门户虽提供记录访问，但缺乏精准回答个性化问题的能力。基于LLM的临床问答面临效率低、幻觉风险高及部署难等挑战。此外，缺乏开放的患者特异性QA数据集与基准，制约了相关算法研究与应用发展。

Method: 基于MIMIC-IV FHIR演示数据构建FHIRPath-QA数据集，包含1.4万余对患者/临床医生表述的自然语言问题与已验证的FHIRPath查询及标准答案。提出text-to-FHIRPath范式，将推理从文本生成转为查询合成。通过监督微调评估SOTA LLM性能。

Result: 实验显示，当前SOTA LLM在处理患者语言歧义及FHIRPath查询生成方面表现欠佳，错误率较高。但经监督微调后，性能显著提升，证明了该范式在降低LLM使用成本的同时具备可行性。

Conclusion: text-to-FHIRPath合成范式为开发安全、高效、互操作的EHR患者问答应用提供了实用基础。FHIRPath-QA数据集与基准可作为未来研究的重要起点，推动临床自然语言处理技术的临床落地。

Abstract: Though patients are increasingly granted digital access to their electronic health records (EHRs), existing interfaces may not support precise, trustworthy answers to patient-specific questions. Large language models (LLM) show promise in clinical question answering (QA), but retrieval-based approaches are computationally inefficient, prone to hallucination, and difficult to deploy over real-life EHRs. In this work, we introduce FHIRPath-QA, the first open dataset and benchmark for patient-specific QA that includes open-standard FHIRPath queries over real-world clinical data. We propose a text-to-FHIRPath QA paradigm that shifts reasoning from free-text generation to FHIRPath query synthesis, significantly reducing LLM usage. Built on MIMIC-IV on FHIR Demo, the dataset pairs over 14k natural language questions in patient and clinician phrasing with validated FHIRPath queries and answers. Further, we demonstrate that state-of-the-art LLMs struggle to deal with ambiguity in patient language and perform poorly in FHIRPath query synthesis. However, they benefit strongly from supervised fine-tuning. Our results highlight that text-to-FHIRPath synthesis has the potential to serve as a practical foundation for safe, efficient, and interoperable consumer health applications, and our dataset and benchmark serve as a starting point for future research on the topic. The full dataset and generation code is available at: https://github.com/mooshifrew/fhirpath-qa.

</details>


### [6] [IDP Accelerator: Agentic Document Intelligence from Extraction to Compliance Validation](https://arxiv.org/abs/2602.23481)
*Md Mofijul Islam,Md Sirajus Salekin,Joe King,Priyashree Roy,Vamsi Thilak Gudi,Spencer Romo,Akhil Nooney,Boyi Xie,Bob Strahan,Diego A. Socolinsky*

Main category: cs.CL

TL;DR: 本文提出IDP Accelerator智能文档处理框架，该框架包含四大核心组件（DocSplit数据集/分类器、多模态LLM提取、MCP合规分析、LLM验证），在医疗行业生产部署中实现98%分类准确率、处理延迟降低80%、运营成本减少77%。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决工业NLP中从无结构文档提取结构化信息的基础性挑战，指出尽管大语言模型支持零样本提取，但传统流水线无法处理多文档包、复杂推理和严格合规要求。

Method: 作者提出IDP Accelerator框架，包含四个关键组件：1）DocSplit基准数据集及采用BIO标记的多模态分类器，用于复杂文档包分割；2）基于多模态LLM的可配置提取模块；3）符合模型上下文协议（MCP）的Agentic分析模块，通过安全沙箱化代码执行提供数据访问；4）规则验证模块，使用LLM驱动逻辑替代确定性引擎进行复杂合规检查。

Result: 在领先医疗提供者的生产部署中，该系统实现98%分类准确率、处理延迟降低80%、运营成本较传统基线减少77%，并提供了交互式Web界面供用户上传文档包、可视化分类结果和探索提取数据。

Conclusion: IDP Accelerator成功展示了端到端Agentic AI文档智能解决方案的工业可行性，跨行业有效且医疗部署成果显著，框架已开源并提供在线演示。

Abstract: Understanding and extracting structured insights from unstructured documents remains a foundational challenge in industrial NLP. While Large Language Models (LLMs) enable zero-shot extraction, traditional pipelines often fail to handle multi-document packets, complex reasoning, and strict compliance requirements. We present IDP (Intelligent Document Processing) Accelerator, a framework enabling agentic AI for end-to-end document intelligence with four key components: (1) DocSplit, a novel benchmark dataset and multimodal classifier using BIO tagging to segment complex document packets; (2) configurable Extraction Module leveraging multimodal LLMs to transform unstructured content into structured data; (3) Agentic Analytics Module, compliant with the Model Context Protocol (MCP) providing data access through secure, sandboxed code execution; and (4) Rule Validation Module replacing deterministic engines with LLM-driven logic for complex compliance checks. The interactive demonstration enables users to upload document packets, visualize classification results, and explore extracted data through an intuitive web interface. We demonstrate effectiveness across industries, highlighting a production deployment at a leading healthcare provider achieving 98% classification accuracy, 80% reduced processing latency, and 77% lower operational costs over legacy baselines. IDP Accelerator is open-sourced with a live demonstration available to the community.

</details>


### [7] [Humans and LLMs Diverge on Probabilistic Inferences](https://arxiv.org/abs/2602.23546)
*Gaurav Kamath,Sreenath Madathil,Sebastian Schuster,Marie-Catherine de Marneffe,Siva Reddy*

Main category: cs.CL

TL;DR: 本研究探讨人类概率推理与LLM表现的差距。作者创建ProbCOPA数据集（210个概率推理问题，每题25-30人标注），发现人类判断呈现分级变化，而8个先进LLM无法生成类似人类的概率分布，揭示了人机推理的本质差异。


<details>
  <summary>Details</summary>
Motivation: 人类推理常需基于有限信息作出概率性推断，这与确定性逻辑推理不同。尽管LLM在逻辑数学任务上表现出色，但其在开放性、非确定性推理中的行为尚未被充分研究。系统评估LLM的概率推理能力，对理解其认知局限性至关重要。

Method: 1) 构建ProbCOPA：210个人工设计的英语概率推理问题，每题由25-30名参与者标注可能性；2) 测试8个先进推理LLM；3) 对比人类与模型的响应分布；4) 分析LLM推理链，识别通用推理模式。

Result: 1) 人类响应呈现分级且多样的概率判断；2) 所有测试LLM均未能产生人类类似的概率分布；3) 推理链分析发现LLM使用特定模式评估概率推理；4) 人机在概率推理上存在系统性差异。

Conclusion: 当前LLM在概率推理方面存在显著局限，无法模拟人类的概率性判断分布。该研究强调了在非确定性环境中评估推理能力的必要性，为未来改进指明了方向。

Abstract: Human reasoning often involves working over limited information to arrive at probabilistic conclusions. In its simplest form, this involves making an inference that is not strictly entailed by a premise, but rather only likely given the premise. While reasoning LLMs have demonstrated strong performance on logical and mathematical tasks, their behavior on such open-ended, non-deterministic inferences remains largely unexplored. We introduce ProbCOPA, a dataset of 210 handcrafted probabilistic inferences in English, each annotated for inference likelihood by 25--30 human participants. We find that human responses are graded and varied, revealing probabilistic judgments of the inferences in our dataset. Comparing these judgments with responses from eight state-of-the-art reasoning LLMs, we show that models consistently fail to produce human-like distributions. Finally, analyzing LLM reasoning chains, we find evidence of a common reasoning pattern used to evaluate such inferences. Our findings reveal persistent differences between humans and LLMs, and underscore the need to evaluate reasoning beyond deterministic settings.

</details>


### [8] [Multi-Agent Causal Reasoning for Suicide Ideation Detection Through Online Conversations](https://arxiv.org/abs/2602.23577)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Shijie Zhang,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: 针对社交媒体自杀风险检测中现有方法互动捕捉局限与忽视隐藏偏见的问题，本文提出多智能体因果推理（MACR）框架。该框架通过推理智能体生成反事实用户反应以扩展互动上下文，通过偏见感知决策智能体采用前门调整策略缓解从众与模仿自杀等隐藏影响，实验证明其在真实对话数据集上有效且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 自杀是全球重大公共卫生挑战，社交媒体虽为早期风险检测提供机遇，但现有方法存在双重缺陷：依赖预定义规则导致互动捕捉范围狭窄，且忽视用户从众与自杀模仿行为等隐藏影响，这些因素显著影响在线社区的自杀表达与传播。

Method: 提出MACR框架，包含：1）推理智能体：基于认知评估理论生成反事实用户反应，通过认知、情感、行为三维度结构化分析扩展互动，各维度设专门子智能体；2）偏见感知决策智能体：利用反事实反应实施前门调整策略以缓解隐藏偏见。双智能体协作实现偏见抑制与上下文增强。

Result: 在真实世界对话数据集上的广泛实验验证了MACR框架在识别自杀风险方面的有效性和鲁棒性。

Conclusion: MACR框架通过因果推理与偏见感知的协同机制，有效克服传统方法在互动规模与隐藏偏见上的局限，为社交媒体自杀风险早期检测提供了更可靠的解决方案，具有重要公共卫生价值。

Abstract: Suicide remains a pressing global public health concern. While social media platforms offer opportunities for early risk detection through online conversation trees, existing approaches face two major limitations: (1) They rely on predefined rules (e.g., quotes or relies) to log conversations that capture only a narrow spectrum of user interactions, and (2) They overlook hidden influences such as user conformity and suicide copycat behavior, which can significantly affect suicidal expression and propagation in online communities. To address these limitations, we propose a Multi-Agent Causal Reasoning (MACR) framework that collaboratively employs a Reasoning Agent to scale user interactions and a Bias-aware Decision-Making Agent to mitigate harmful biases arising from hidden influences. The Reasoning Agent integrates cognitive appraisal theory to generate counterfactual user reactions to posts, thereby scaling user interactions. It analyses these reactions through structured dimensions, i.e., cognitive, emotional, and behavioral patterns, with a dedicated sub-agent responsible for each dimension. The Bias-aware Decision-Making Agent mitigates hidden biases through a front-door adjustment strategy, leveraging the counterfactual user reactions produced by the Reasoning Agent. Through the collaboration of reasoning and bias-aware decision making, the proposed MACR framework not only alleviates hidden biases, but also enriches contextual information of user interactions with counterfactual knowledge. Extensive experiments on real-world conversational datasets demonstrate the effectiveness and robustness of MACR in identifying suicide risk.

</details>


### [9] [BRIDGE the Gap: Mitigating Bias Amplification in Automated Scoring of English Language Learners via Inter-group Data Augmentation](https://arxiv.org/abs/2602.23580)
*Yun Wang,Xuansheng Wu,Jingyuan Huang,Lei Liu,Xiaoming Zhai,Ninghao Liu*

Main category: cs.CL

TL;DR: 该论文提出BRIDGE框架，通过合成高质量英语学习者（ELL）样本，有效减少自动化评分系统中的预测偏差，在保持整体性能的同时实现与真实数据相当的公平性提升，为大规模评估提供了经济高效的公平性解决方案。


<details>
  <summary>Details</summary>
Motivation: 在教育评估领域，基于深度学习和大型语言模型的自动化评分系统存在严重的偏见放大风险，特别是对于英语学习者（ELL）等代表性不足的群体。由于少数群体（高分ELL）样本稀缺，采用经验风险最小化训练的模型会偏向多数群体（非ELL）的语言模式，导致即使掌握相当领域知识的ELL学生也被低估分数，损害评分公平性。

Method: 提出BRIDGE（Bias-Reducing Inter-group Data GEneration）框架，通过将丰富的高分非ELL样本中的构念相关（即符合评分标准）内容"粘贴"到真实的ELL语言模式中，合成高质量ELL样本。同时引入判别器模型确保合成样本质量，专门针对低资源评估场景设计。

Result: 在加州科学测试（CAST）数据集上的实验表明，BRIDGE在保持整体评分性能的同时，有效减少了高分ELL学生的预测偏差。值得注意的是，该方法实现的公平性提升与使用额外真实人类数据相当。

Conclusion: 该方法为确保大规模评估中的公平评分提供了一种经济高效的解决方案，能够在不显著增加成本的情况下实现教育评估的公平性目标。

Abstract: In the field of educational assessment, automated scoring systems increasingly rely on deep learning and large language models (LLMs). However, these systems face significant risks of bias amplification, where model prediction gaps between student groups become larger than those observed in training data. This issue is especially severe for underrepresented groups such as English Language Learners (ELLs), as models may inherit and further magnify existing disparities in the data. We identify that this issue is closely tied to representation bias: the scarcity of minority (high-scoring ELL) samples makes models trained with empirical risk minimization favor majority (non-ELL) linguistic patterns. Consequently, models tend to under-predict ELL students who even demonstrate comparable domain knowledge but use different linguistic patterns, thereby undermining the fairness of automated scoring outcomes. To mitigate this, we propose BRIDGE, a Bias-Reducing Inter-group Data GEneration framework designed for low-resource assessment settings. Instead of relying on the limited minority samples, BRIDGE synthesizes high-scoring ELL samples by "pasting" construct-relevant (i.e., rubric-aligned knowledge and evidence) content from abundant high-scoring non-ELL samples into authentic ELL linguistic patterns. We further introduce a discriminator model to ensure the quality of synthetic samples. Experiments on California Science Test (CAST) datasets demonstrate that BRIDGE effectively reduces prediction bias for high-scoring ELL students while maintaining overall scoring performance. Notably, our method achieves fairness gains comparable to using additional real human data, offering a cost-effective solution for ensuring equitable scoring in large-scale assessments.

</details>


### [10] [LFQA-HP-1M: A Large-Scale Human Preference Dataset for Long-Form Question Answering](https://arxiv.org/abs/2602.23603)
*Rafid Ishrak Jahan,Fahmid Shahriar Iqbal,Sagnik Ray Choudhury*

Main category: cs.CL

TL;DR: 本文针对长文本问答(LFQA)评估指标与人工判断脱节的问题，推出包含130万条人工成对偏好标注的数据集LFQA-HP-1M及九个质量评估标准。研究表明，基于这些标准的简单线性模型性能媲美先进大语言模型评估器，同时揭示了LLM评估器存在传递性不一致、位置偏见、冗长偏见以及对对抗性扰动的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 长文本问答需要精细评估多句子解释性回答，但现有评估指标往往无法准确反映人类判断，影响了评估的可靠性和有效性。

Method: 构建大规模人工成对偏好数据集LFQA-HP-1M；设计九个答案质量评估细则；训练基于这些特征的线性模型并与SOTA LLM评估器对比；系统检验LLM评估器的传递性一致性、位置偏见、冗长偏见及对对抗性扰动的脆弱性。

Result: 基于九项细则的线性模型在评估性能上与先进LLM评估器相当；发现LLM评估器存在传递性逻辑不一致、显著的位置偏见和冗长偏见；验证了LLM评估器易受对抗性攻击影响。

Conclusion: 本研究提供了最大的公开LFQA偏好数据集之一，建立了透明可靠的细则驱动评估框架，揭示了当前LLM评估器的多重脆弱性，为未来开发更可靠的评估方法奠定了重要基础。

Abstract: Long-form question answering (LFQA) demands nuanced evaluation of multi-sentence explanatory responses, yet existing metrics often fail to reflect human judgment. We present LFQA-HP-1M, a large-scale dataset comprising 1.3M human pairwise preference annotations for LFQA. We propose nine rubrics for answer quality evaluation, and show that simple linear models based on these features perform comparably to state-of-the-art LLM evaluators. We further examine transitivity consistency, positional bias, and verbosity biases in LLM evaluators and demonstrate their vulnerability to adversarial perturbations. Overall, this work provides one of the largest public LFQA preference datasets and a rubric-driven framework for transparent and reliable evaluation.

</details>


### [11] [LLM-Driven Multi-Turn Task-Oriented Dialogue Synthesis for Realistic Reasoning](https://arxiv.org/abs/2602.23610)
*Yu Zhu,Kai Yang*

Main category: cs.CL

TL;DR: 针对现有LLM推理评测基准过于简化、脱离实际的问题，本文提出了一种LLM驱动的框架，通过三级优化生成基于真实场景的多轮任务导向对话，构建高质量数据集以评估和增强LLM在实际应用中的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 任务导向对话系统的构建依赖于LLM的推理能力，但现有评测基准存在三大缺陷：数据集过于简单抽象，脱离真实任务流程和领域约束；预训练数据污染影响评估可靠性；传统众包构建方式劳动密集且难以规模化。这些限制阻碍了LLM推理能力的有效评估和实际应用。

Method: 提出基于三级优化的LLM驱动框架，自动生成扎根于真实推理场景的多轮任务导向对话。生成的对话富含现实世界信息，具有强上下文连贯性。围绕这些对话设计推理任务，并通过迭代优化持续提升任务质量和挑战性，形成高质量的评测数据集。

Result: 实验显示，该框架生成的合成数据提供了非平凡的推理挑战，能有效评估和提升LLM的推理能力，为LLM实际逻辑推理能力的评估和增强提供了有价值的基准。

Conclusion: 该研究成功解决了现有评测基准与真实场景脱节的问题，通过自动合成高质量对话推理数据，为评估和增强LLM在实际应用中的逻辑推理能力建立了新基准，推动了更可靠的任务导向对话系统的发展。

Abstract: The reasoning capability of large language models (LLMs), defined as their ability to analyze, infer, and make decisions based on input information, is essential for building intelligent task-oriented dialogue systems. However, existing benchmarks do not sufficiently reflect the complexity of real-world scenarios, which limits their effectiveness in evaluating and enhancing LLM reasoning in practical contexts. Many current reasoning datasets are overly simplistic and abstract, often disconnected from realistic task flows, domain constraints, and operational rules, making it difficult to effectively evaluate LLMs' logical reasoning ability. In addition, data contamination from pretraining corpora undermines the reliability of evaluation results, and traditional crowdsourcing methods for dataset construction are labor-intensive and difficult to scale. To address these challenges, we propose a LLM-driven framework for synthesizing multi-turn, task-oriented dialogues grounded in realistic reasoning scenarios, leveraging trilevel optimization to enhance dialogue quality. Our method generates dialogues grounded in authentic task scenarios, enriched with real-world information, and exhibiting strong contextual coherence. Corresponding reasoning tasks are carefully designed around these dialogues and iteratively refined to continuously improve the tasks' quality and challenge. The resulting dataset serves as a valuable benchmark for assessing and advancing the realistic logical reasoning capabilities of LLMs. Experimental results show that our synthetic data-based reasoning tasks introduce non-trivial reasoning challenges and provide meaningful support for improving the reasoning capabilities of LLMs.

</details>


### [12] [TRIZ-RAGNER: A Retrieval-Augmented Large Language Model for TRIZ-Aware Named Entity Recognition in Patent-Based Contradiction Mining](https://arxiv.org/abs/2602.23656)
*Zitong Xu,Yuqing Wu,Yue Zhao*

Main category: cs.CL

TL;DR: 本文提出TRIZ-RAGNER，一种检索增强的大语言模型框架，用于专利文本中的TRIZ矛盾参数挖掘。该框架通过TRIZ知识库稠密检索、交叉编码器重排序和结构化LLM提示，将矛盾识别转化为命名实体识别任务，有效缓解语义歧义与LLM幻觉问题，在PaTRIZ数据集上达到84.2%的F1值，较最强基线提升7.3个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有TRIZ矛盾挖掘方法依赖规则系统或传统机器学习，面临语义歧义、领域依赖和泛化能力有限等瓶颈。大语言模型虽具强大语义理解能力，但直接应用存在幻觉和TRIZ知识结构化不足的缺陷，亟需融合领域知识与LLM推理的解决方案。

Method: 提出TRIZ-RAGNER框架，将矛盾挖掘重构为语义级命名实体识别任务。核心设计包括：1)基于TRIZ知识库的稠密检索获取相关参数；2)交叉编码器重排序精炼上下文；3)结构化提示引导LLM抽取改善与恶化参数。通过注入领域特定的TRIZ知识，抑制语义噪声并提升提取一致性。

Result: 在PaTRIZ数据集上的实验表明，该框架在TRIZ矛盾对识别中实现85.6%精确率、82.9%召回率和84.2% F1值。相比基于提示增强的GPT最强基线，F1值绝对提升7.3个百分点，显著优于传统序列标注模型与LLM基线。

Conclusion: 检索增强的TRIZ知识注入机制有效提升了专利矛盾挖掘的鲁棒性与准确性，验证了外部知识库与LLM推理结合在领域特定信息提取中的有效性，为系统化创新提供了更可靠的技术路径。

Abstract: TRIZ-based contradiction mining is a fundamental task in patent analysis and systematic innovation, as it enables the identification of improving and worsening technical parameters that drive inventive problem solving. However, existing approaches largely rely on rule-based systems or traditional machine learning models, which struggle with semantic ambiguity, domain dependency, and limited generalization when processing complex patent language. Recently, large language models (LLMs) have shown strong semantic understanding capabilities, yet their direct application to TRIZ parameter extraction remains challenging due to hallucination and insufficient grounding in structured TRIZ knowledge. To address these limitations, this paper proposes TRIZ-RAGNER, a retrieval-augmented large language model framework for TRIZ-aware named entity recognition in patent-based contradiction mining. TRIZ-RAGNER reformulates contradiction mining as a semantic-level NER task and integrates dense retrieval over a TRIZ knowledge base, cross-encoder reranking for context refinement, and structured LLM prompting to extract improving and worsening parameters from patent sentences. By injecting domain-specific TRIZ knowledge into the LLM reasoning process, the proposed framework effectively reduces semantic noise and improves extraction consistency. Experiments on the PaTRIZ dataset demonstrate that TRIZ-RAGNER consistently outperforms traditional sequence labeling models and LLM-based baselines. The proposed framework achieves a precision of 85.6%, a recall of 82.9%, and an F1-score of 84.2% in TRIZ contradiction pair identification. Compared with the strongest baseline using prompt-enhanced GPT, TRIZ-RAGNER yields an absolute F1-score improvement of 7.3 percentage points, confirming the effectiveness of retrieval-augmented TRIZ knowledge grounding for robust and accurate patent-based contradiction mining.

</details>


### [13] [From Static Benchmarks to Dynamic Protocol: Agent-Centric Text Anomaly Detection for Evaluating LLM Reasoning](https://arxiv.org/abs/2602.23729)
*Seungdong Yoa,Sanghyu Yoon,Suhee Yoon,Dongmin Kim,Ye Seul Sim,Junhyun Lee,Woohyung Lim*

Main category: cs.CL

TL;DR: 本文提出一种基于智能体的动态评测范式，通过教师、编排者和学生三个智能体协同生成、验证和解决问题，实现自动扩展难度的基准测试。该方法以文本异常检测为主要评估形式，能够系统性地暴露传统基准测试无法发现的推理错误，为评估持续演进的大语言模型提供了可持续方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的评估主要依赖静态数据集，这些数据集扩展性有限且无法捕捉模型不断演进的推理能力。静态数据集难以适应快速发展的大模型技术，容易过时并产生评估偏差。

Method: 提出三智能体协同的动态评测协议：教师智能体生成候选问题，编排者智能体严格验证问题有效性并防御对抗攻击，学生智能体尝试解决验证后的问题。无效问题由教师智能体修订直至通过验证；当学生正确解题后，编排者会促使教师生成更具挑战性的变体。该协议使评测难度随智能体能力提升自动扩展。

Result: 采用需要跨句逻辑推理且能抵抗模式匹配捷径的文本异常检测作为主要评估形式，证明该协议能系统性地暴露传统基准测试无法揭示的边界情况推理错误。同时提倡沿多个互补维度评估系统，包括跨模型成对性能和初始问题与编排者最终确定问题之间的进步程度。

Conclusion: 通过将评估焦点从固定数据集转向动态协议，该研究为大语言模型的持续演进提供了可持续的评估方向，并提出了以智能体为中心的基准测试协同演化的研究议程，为未来模型评估奠定基础。

Abstract: The evaluation of large language models (LLMs) has predominantly relied on static datasets, which offer limited scalability and fail to capture the evolving reasoning capabilities of recent models. To overcome these limitations, we propose an agent-centric benchmarking paradigm that moves beyond static datasets by introducing a dynamic protocol in which autonomous agents iteratively generate, validate, and solve problems. Within this protocol, a teacher agent generates candidate problems, an orchestrator agent rigorously verifies their validity and guards against adversarial attacks, and a student agent attempts to solve the validated problems. An invalid problem is revised by the teacher agent until it passes validation. If the student correctly solves the problem, the orchestrator prompts the teacher to generate more challenging variants. Consequently, the benchmark scales in difficulty automatically as more capable agents are substituted into any role, enabling progressive evaluation of large language models without manually curated datasets. Adopting text anomaly detection as our primary evaluation format, which demands cross-sentence logical inference and resists pattern-matching shortcuts, we demonstrate that this protocol systematically exposes corner-case reasoning errors that conventional benchmarks fail to reveal. We further advocate evaluating systems along several complementary axes including cross-model pairwise performance and progress between the initial and orchestrator-finalized problems. By shifting the focus from fixed datasets to dynamic protocols, our approach offers a sustainable direction for evaluating ever-evolving language models and introduces a research agenda centered on the co-evolution of agent-centric benchmarks.

</details>


### [14] [Structured Prompt Optimization for Few-Shot Text Classification via Semantic Alignment in Latent Space](https://arxiv.org/abs/2602.23753)
*Jiasen Zheng,Zijun Zhou,Huajun Zhang,Junjiang Lin,Jingyun Jia,Qi Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种基于结构化提示的少样本文本分类优化框架，通过引入多维度语义因子提示和跨空间对齐机制，有效解决了语义纠缠、标签结构不清晰和特征表示不足等问题，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 针对少样本文本分类中存在的语义纠缠、标签结构不清晰和特征表示不足等关键问题，需要在低资源条件下提升模型的语义理解能力和任务适应能力。现有方法未能有效区分不同语义因素的关系，且文本表示与标签语义之间的一致性不足，导致分类边界模糊。

Method: 该框架首先使用预训练语言模型对输入文本进行编码获取基础语义表示；随后引入由多维度语义因子构成的结构化提示，通过可学习的组合机制将其与文本特征融合，形成具有清晰边界的任务相关表示；为增强文本表示与标签语义的一致性，构建了结构化标签嵌入矩阵并采用跨空间对齐机制确保文本特征与标签属性的稳定匹配；此外，通过施加提示正交约束和联合优化目标，保持不同语义因子间的独立性，使结构化提示为分类决策提供透明可控的指导。

Result: 实验结果表明，该结构化提示优化框架有效缓解了少样本文本分类中的语义冲突和标签歧义问题，在准确率、精确率、召回率和AUC等指标上均有显著提升，并展现出强大的跨任务适用性。三种敏感性实验（学习率、提示长度、数据规模）验证了该框架在不同条件下的稳定性和鲁棒性。

Conclusion: 所提出的结构化提示优化框架通过显式建模语义结构和标签关系，为少样本文本分类提供了有效的解决方案，其透明可控的提示设计思路为低资源场景下的文本理解任务提供了新的研究方向。

Abstract: This study addresses the issues of semantic entanglement, unclear label structure, and insufficient feature representation in few-shot text classification, and proposes an optimization framework based on structured prompts to enhance semantic understanding and task adaptation under low-resource conditions. The framework first uses a pretrained language model to encode the input text and obtain basic semantic representations. It then introduces structured prompts composed of multi-dimensional semantic factors and integrates them with text features through a learnable combination mechanism, which forms task-related representations with clear boundaries in the latent space. To further strengthen the consistency between text representations and label semantics, the method constructs a structured label embedding matrix and employs a cross-space alignment mechanism to ensure stable matching between textual features and label attributes. In addition, the model applies prompt orthogonality constraints and a joint optimization objective to maintain independence across different semantic factors in the prompts, allowing the structured prompts to provide transparent and controllable guidance for classification decisions. Three types of sensitivity experiments, including learning rate sensitivity, prompt length sensitivity, and data scale sensitivity, are designed to evaluate the stability and robustness of the framework under different conditions. Experimental results show that the proposed structured prompt optimization framework effectively alleviates semantic conflicts and label ambiguity in few-shot text classification. It significantly improves performance on accuracy, precision, recall, and AUC, and demonstrates strong cross-task applicability.

</details>


### [15] [Divide and Conquer: Accelerating Diffusion-Based Large Language Models via Adaptive Parallel Decoding](https://arxiv.org/abs/2602.23792)
*Xiangzhong Luo,Yilin An,Zhicheng Yu,Weichen Liu,Xu Yang*

Main category: cs.CL

TL;DR: 本文提出DiCo自适应并行解码方法，通过三阶段分治范式解决扩散大语言模型(dLLMs)理论并行性与实际性能之间的差距问题，实现推理加速的同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)虽理论上支持每步并行生成多token，但实践仍采用逐token解码，因直接并行解码会导致生成质量与稳定性下降，存在理论并行性与实际性能间的显著鸿沟。

Method: DiCo采用三阶段分治策略：划分阶段识别种子token并构建局部簇；征服阶段在各簇上并行解码；完成阶段用细粒度复合解码处理剩余token。划分与征服阶段重复交替直至收敛。

Result: 实验证明DiCo可实现显著推理加速，同时保持具有竞争力的生成质量。

Conclusion: DiCo有效释放了dLLMs的并行潜力，弥合了理论与实践的差距，为高效推理提供了新方案。

Abstract: Diffusion-based large language models (dLLMs) have shown promising performance across various reasoning tasks, establishing themselves as an alternative to autoregressive large language models (LLMs). Unlike autoregressive LLMs that generate one token per step based on all previous tokens, dLLMs theoretically enable parallel generation of multiple tokens at each decoding step. However, recent dLLMs still favor one-token-per-step generation in practice, as directly decoding multiple masked tokens often leads to degraded generation quality and stability. This reveals a substantial gap between the theoretical parallelism and practical performance of dLLMs. To bridge this gap, we introduce an adaptive parallel decoding approach, namely DiCo, which features a three-phase divide-and-conquer paradigm to unleash the inherent parallelism of dLLMs. During the Divide phase, DiCo first explores the input masked sequence and identifies masked tokens as seed tokens, which are then expanded to construct a set of local clusters. During the Conquer phase, DiCo performs parallel decoding across different local clusters constructed in the Divide phase. The divide-and-conquer process repeatedly alternates between the Divide and Conquer phases until convergence. During the Finalize phase, DiCo decodes the remaining few masked tokens using an effective fine-grained compound decoding scheme to finalize the generation. Extensive experiments demonstrate that DiCo can achieve significant inference speedups while maintaining competitive generation quality.

</details>


### [16] [GLUScope: A Tool for Analyzing GLU Neurons in Transformer Language Models](https://arxiv.org/abs/2602.23826)
*Sebastian Gerstner,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文提出了GLUScope，一个面向可解释性研究的开源工具，用于分析基于Transformer的语言模型中的神经元。该工具专注于支持SwiGLU等门控激活函数的新型模型，通过展示神经元四种符号组合（门控与激活的正负组合）对应的文本示例和出现频率，帮助研究人员理解神经元行为，并提供了新颖的洞察案例。


<details>
  <summary>Details</summary>
Motivation: 现有神经元分析工具主要针对较早期的模型，而当前语言模型广泛采用SwiGLU等门控激活结构。这种结构下，仅分析正激活值不足以全面理解神经元功能，因为门控单元和激活单元均可独立取正负值，形成四种功能迥异的符号组合。缺乏专门工具限制了研究人员对这些新模型的理解。

Method: 开发开源工具GLUScope，针对每个神经元，系统性地展示其四种符号组合（门控正×激活正、门控正×激活负、门控负×激活正、门控负×激活负）所对应的代表性文本示例，并量化各组合的出现频率，以支持功能模式识别。

Result: 工具能够揭示不同符号组合下神经元的差异化行为模式，帮助研究人员发现传统正激活分析无法捕捉的新颖语义特征和功能机制，并已通过在线演示验证其有效性。

Conclusion: GLUScope为门控激活模型的可解释性研究提供了实用工具，其四象限符号组合分析框架深化了对神经元功能的理解，有助于推动模型可解释性领域的发展。

Abstract: We present GLUScope, an open-source tool for analyzing neurons in Transformer-based language models, intended for interpretability researchers. We focus on more recent models than previous tools do; specifically we consider gated activation functions such as SwiGLU. This introduces a new challenge: understanding positive activations is not enough. Instead, both the gate and the in activation of a neuron can be positive or negative, leading to four different possible sign combinations that in some cases have quite different functionalities. Accordingly, for any neuron, our tool shows text examples for each of the four sign combinations, and indicates how often each combination occurs. We describe examples of how our tool can lead to novel insights. A demo is available at https: //sjgerstner.github.io/gluscope.

</details>


### [17] [CLFEC: A New Task for Unified Linguistic and Factual Error Correction in paragraph-level Chinese Professional Writing](https://arxiv.org/abs/2602.23845)
*Jian Kai,Zidong Zhang,Jiwen Chen,Zhengxiang Wu,Songtao Sun,Fuyang Li,Yang Cao,Qiang Liu*

Main category: cs.CL

TL;DR: 本文提出CLFEC（中文语言与事实纠错）新任务，构建多领域专业写作数据集，系统研究LLM纠错范式，发现统一处理语言与事实错误优于分离处理，为工业级自动校对系统提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有中文纠错研究多聚焦拼写语法，事实纠错常被独立处理。但在段落级专业写作中，语言错误与事实错误高频共现且相互影响，亟需统一纠错框架。

Method: 1) 构建覆盖时政、金融、法律、医疗的混合领域专业写作数据集；2) 系统研究从提示学习、检索增强生成(RAG)到智能体工作流的LLM纠错范式；3) 分析实际挑战与解决方案。

Result: 1) 同一上下文中处理混合错误的效果优于分离处理；2) 专业纠错模型泛化能力有限，事实修复需证据支撑；3) 智能体工作流在合适基模型下效果显著；4) 存在过度纠错与混合错误段落处理困难等问题。

Conclusion: 研究验证了联合纠错的必要性，所构建数据集与实证结果为工业场景下构建可靠全自动校对系统提供了重要指导方向。

Abstract: Chinese text correction has traditionally focused on spelling and grammar, while factual error correction is usually treated separately. However, in paragraph-level Chinese professional writing, linguistic (word/grammar/punctuation) and factual errors frequently co-occur and interact, making unified correction both necessary and challenging. This paper introduces CLFEC (Chinese Linguistic & Factual Error Correction), a new task for joint linguistic and factual correction. We construct a mixed, multi-domain Chinese professional writing dataset spanning current affairs, finance, law, and medicine. We then conduct a systematic study of LLM-based correction paradigms, from prompting to retrieval-augmented generation (RAG) and agentic workflows. The analysis reveals practical challenges, including limited generalization of specialized correction models, the need for evidence grounding for factual repair, the difficulty of mixed-error paragraphs, and over-correction on clean inputs. Results further show that handling linguistic and factual Error within the same context outperform decoupled processes, and that agentic workflows can be effective with suitable backbone models. Overall, our dataset and empirical findings provide guidance for building reliable, fully automatic proofreading systems in industrial settings.

</details>


### [18] [The Astonishing Ability of Large Language Models to Parse Jabberwockified Language](https://arxiv.org/abs/2602.23928)
*Gary Lupyan,Senyi Yang*

Main category: cs.CL

TL;DR: 本研究揭示大语言模型能从内容词被随机替换为无意义字符串的严重退化英文文本中惊人地恢复语义，表明结构线索（形态句法、封闭词类）对词义的约束远超预期，这对理解语言结构和高效语言处理机制具有重要启示。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否从内容词被随机替换为无意义字符串的"胡言乱语化"英文文本中恢复原始语义，以检验结构线索对词义的约束作用，并探讨生物或人工系统中高效语言处理可能的认知与计算基础。

Method: 研究者将英文文本中的内容词随机替换为无意义字符串，生成"胡言乱语化"的退化文本（例如："At the ghybe of the swuint..."），然后测试大语言模型将这些文本翻译回常规英文的能力，并与原始文本进行对比。

Result: 实验发现大语言模型展现出惊人的语义恢复能力，其翻译结果在许多情况下接近原始文本。这表明句法结构、形态标记和封闭词类等结构线索对词义的约束程度远超人们此前想象，为理解语言理解机制提供了新证据。

Conclusion: 大语言模型处理退化文本的能力虽超越人类，但对语言学理论具有重要价值。研究认为，高效的语言处理（无论生物或人工系统）可能依赖于句法、词汇语义和世界知识三者之间极为紧密的整合机制，这一发现为认知语言学和人工智能提供了新的研究方向。

Abstract: We show that large language models (LLMs) have an astonishing ability to recover meaning from severely degraded English texts. Texts in which content words have been randomly substituted by nonsense strings, e.g., "At the ghybe of the swuint, we are haiveed to Wourge Phrear-gwurr, who sproles into an ghitch flount with his crurp", can be translated to conventional English that is, in many cases, close to the original text, e.g., "At the start of the story, we meet a man, Chow, who moves into an apartment building with his wife." These results show that structural cues (e.g., morphosyntax, closed-class words) constrain lexical meaning to a much larger degree than imagined. Although the abilities of LLMs to make sense of "Jabberwockified" English are clearly superhuman, they are highly relevant to understanding linguistic structure and suggest that efficient language processing either in biological or artificial systems likely benefits from very tight integration between syntax, lexical semantics, and general world knowledge.

</details>


### [19] [Benchmarking BERT-based Models for Sentence-level Topic Classification in Nepali Language](https://arxiv.org/abs/2602.23940)
*Nischal Karki,Bipesh Subedi,Prakash Poudyal,Rupak Raj Ghimire,Bal Krishna Bal*

Main category: cs.CL

TL;DR: 本研究评估10种BERT变体在25,006句五类尼泊尔语主题分类任务上的表现。Indic模型MuRIL-large以90.60%的F1分数最优，NepBERTa次之（88.26%），为尼泊尔语NLP建立了可靠基线。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔语作为天城文书写系统的低资源语言，在NLP领域尚未得到充分研究。尽管基于Transformer的多语言模型在多语言任务上取得显著进展，但缺乏针对尼泊尔语的系统性评估。本研究旨在填补这一研究空白。

Method: 研究在包含25,006个句子的平衡尼泊尔语数据集（涵盖5个主题域）上，对mBERT、XLM-R、MuRIL、DevBERT、HindiBERT、IndicBERT和NepBERTa等10个预训练模型进行微调。采用准确率、加权精确率、召回率、F1分数和AUROC作为评估指标。

Result: Indic模型显著优于多语言和单语模型，其中MuRIL-large取得最高F1分数90.60%。专为尼泊尔语设计的NepBERTa表现同样出色，F1分数达到88.26%。实验提供了全面的性能基准。

Conclusion: 本研究为尼泊尔语主题分类建立了强有力的基线，证明基于印度语料预训练的模型在低资源语言任务上的有效性。成果将促进未来文档级分类及更广泛的尼泊尔语NLP应用，并为其他低资源语言研究提供重要参考。

Abstract: Transformer-based models such as BERT have significantly advanced Natural Language Processing (NLP) across many languages. However, Nepali, a low-resource language written in Devanagari script, remains relatively underexplored. This study benchmarks multilingual, Indic, Hindi, and Nepali BERT variants to evaluate their effectiveness in Nepali topic classification. Ten pre-trained models, including mBERT, XLM-R, MuRIL, DevBERT, HindiBERT, IndicBERT, and NepBERTa, were fine-tuned and tested on the balanced Nepali dataset containing 25,006 sentences across five conceptual domains and the performance was evaluated using accuracy, weighted precision, recall, F1-score, and AUROC metrics. The results reveal that Indic models, particularly MuRIL-large, achieved the highest F1-score of 90.60%, outperforming multilingual and monolingual models. NepBERTa also performed competitively with an F1-score of 88.26%. Overall, these findings establish a robust baseline for future document-level classification and broader Nepali NLP applications.

</details>


### [20] [EDDA-Coordinata: An Annotated Dataset of Historical Geographic Coordinates](https://arxiv.org/abs/2602.23941)
*Ludovic Moncla,Pierre Nugues,Thierry Joliveau,Katherine McDonough*

Main category: cs.CL

TL;DR: 本文构建了18世纪《百科全书》地理坐标的金标准数据集，提出基于Transformer的两步流水线模型，实现坐标自动识别与标准化，在跨语言跨领域测试中展现良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 历史文献中的地理坐标表达方式多样且精度不一，自动提取困难。为提升早期现代数字化文本的坐标检索效果，需建立高质量训练数据并开发鲁棒的自动化方法。

Method: 从74,000篇数字化文章中标注15,278条地理条目（4,798条含坐标），构建金标准数据集；采用两阶段流水线：先训练分类器识别含坐标条目，再用检索模型提取并归一化坐标；测试了编码-解码和解码架构的Transformer模型。

Result: 交叉验证达到86%的精确匹配分数；在18世纪法语Trevoux词典上得分为61%；在19世纪英语《大英百科全书》第七版上得分为77%，证明模型具有跨语言、跨领域泛化性。

Conclusion: 金标准数据集为训练提供了有效支撑，两步法在跨语言和跨领域场景下表现出良好的通用性和适应性。

Abstract: This paper introduces a dataset of enriched geographic coordinates retrieved from Diderot and d'Alembert's eighteenth-century Encyclopedie. Automatically recovering geographic coordinates from historical texts is a complex task, as they are expressed in a variety of ways and with varying levels of precision. To improve retrieval of coordinates from similar digitized early modern texts, we have created a gold standard dataset, trained models, published the resulting inferred and normalized coordinate data, and experimented applying these models to new texts. From 74,000 total articles in each of the digitized versions of the Encyclopedie from ARTFL and ENCCRE, we examined 15,278 geographical entries, manually identifying 4,798 containing coordinates, and 10,480 with descriptive but non-numerical references. Leveraging our gold standard annotations, we trained transformer-based models to retrieve and normalize coordinates. The pipeline presented here combines a classifier to identify coordinate-bearing entries and a second model for retrieval, tested across encoder-decoder and decoder architectures. Cross-validation yielded an 86% EM score. On an out-of-domain eighteenth-century Trevoux dictionary (also in French), our fine-tuned model had a 61% EM score, while for the nineteenth-century, 7th edition of the Encyclopaedia Britannica in English, the EM was 77%. These findings highlight the gold standard dataset's usefulness as training data, and our two-step method's cross-lingual, cross-domain generalizability.

</details>


### [21] [MemEmo: Evaluating Emotion in Memory Systems of Agents](https://arxiv.org/abs/2602.23944)
*Peng Liu,Zhen Tao,Jihao Zhao,Ding Chen,Yansong Zhang,Cuiping Li,Zhiyu Li,Hong Chen*

Main category: cs.CL

TL;DR: 该论文针对大型语言模型在长对话中情感记忆处理的不足，提出了一种情感增强的记忆评估基准HLME，从情感信息提取、记忆更新和问答三个维度评估现有记忆系统，发现无一系统能在所有任务上表现稳健，为未来研究提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的记忆系统虽能缓解长对话中的上下文丢失问题，但与人类认知相比，其在情感相关信息处理方面的效能尚不明确，存在研究空白。

Method: 提出情感增强的记忆评估基准，开发人类相似记忆情感（HLME）数据集，从情感信息提取、情感记忆更新和情感记忆问答三个维度对主流及先进记忆系统进行评估。

Result: 实验结果表明，所评估的记忆系统在全部三项任务上均未能实现稳健性能，揭示了当前系统在情感记忆处理方面的根本缺陷。

Conclusion: 研究客观指出了现有记忆系统在情感记忆处理上的不足，为未来研究和系统优化指明了新方向。

Abstract: Memory systems address the challenge of context loss in Large Language Model during prolonged interactions. However, compared to human cognition, the efficacy of these systems in processing emotion-related information remains inconclusive. To address this gap, we propose an emotion-enhanced memory evaluation benchmark to assess the performance of mainstream and state-of-the-art memory systems in handling affective information. We developed the \textbf{H}uman-\textbf{L}ike \textbf{M}emory \textbf{E}motion (\textbf{HLME}) dataset, which evaluates memory systems across three dimensions: emotional information extraction, emotional memory updating, and emotional memory question answering. Experimental results indicate that none of the evaluated systems achieve robust performance across all three tasks. Our findings provide an objective perspective on the current deficiencies of memory systems in processing emotional memories and suggest a new trajectory for future research and system optimization.

</details>


### [22] [The GRADIEND Python Package: An End-to-End System for Gradient-Based Feature Learning](https://arxiv.org/abs/2602.23993)
*Jonathan Drechsel,Steffen Herbold*

Main category: cs.CL

TL;DR: 本文介绍 gradiend，一个开源 Python 工具包，用于实现 GRADIEND 方法。该方法通过提取语言模型中的事实-反事实梯度（MLM 和 CLM）来学习可解释的特征方向。该工具包提供数据创建、训练、评估、可视化、可控权重更新的持久化模型改写以及多特征比较的统一工作流，并在英语代词范式和大型特征对比实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究提出了 GRADIEND 方法用于从语言模型梯度中提取特征方向，但缺乏可复现、可扩展的开箱即用工具。本文旨在通过开发开源 Python 包，将这一方法操作化，为研究人员提供统一的工作流，降低梯度特征分析与模型改写的门槛，促进可解释性 AI 的实用化。

Method: 1) 实现 GRADIEND 算法：从 MLM（掩码语言模型）和 CLM（因果语言模型）的事实-反事实梯度中学习特征方向向量；2) 构建 gradiend 工具包：提供特征数据创建、模型训练、评估指标、可视化分析、基于梯度更新的持久化模型参数编辑以及多特征对比功能；3) 设计模块化架构支持端到端实验流程。

Result: 成功开发并开源了 gradiend Python 包；在英语代词范式上完成了特征方向学习与模型改写验证；通过大规模特征对比实验复现了先前研究的典型用例，证明了工具包的实用性和可复现性。

Conclusion: gradiend 工具包有效实现了 GRADIEND 方法的操作化，为语言模型的梯度特征分析、可解释性研究与可控编辑提供了标准化工具，有助于推动模型行为分析与安全改进的科学研究。

Abstract: We present gradiend, an open-source Python package that operationalizes the GRADIEND method for learning feature directions from factual-counterfactual MLM and CLM gradients in language models. The package provides a unified workflow for feature-related data creation, training, evaluation, visualization, persistent model rewriting via controlled weight updates, and multi-feature comparison. We demonstrate GRADIEND on an English pronoun paradigm and on a large-scale feature comparison that reproduces prior use cases.

</details>


### [23] [Dialect and Gender Bias in YouTube's Spanish Captioning System](https://arxiv.org/abs/2602.24002)
*Iris Dania Jimenez,Christoph Kern*

Main category: cs.CL

TL;DR: 研究调查YouTube西班牙语自动字幕系统对方言的偏见，通过比较不同地区男女说话者的字幕质量，发现系统性差异，表明算法需针对多样化用户群体校准。


<details>
  <summary>Details</summary>
Motivation: 西班牙语在21个国家有超过4.41亿使用者，存在多种方言变体，但YouTube仅提供单一西班牙语自动字幕选项，可能对某些方言造成偏见，影响内容可访问性和用户体验。

Method: 研究通过比较分析来自不同西班牙语地区男女说话者的自动字幕质量，识别可归因于特定方言的系统性差异，以评估YouTube自动语音识别系统的表现。

Result: 研究发现YouTube自动字幕系统对不同西班牙语方言存在系统性偏见，识别质量存在显著差异，某些方言表现明显较差。

Conclusion: 该研究为数字平台算法技术需要校准以适应多样化用户群体提供了进一步证据，强调确保技术公平性和包容性的重要性。

Abstract: Spanish is the official language of twenty-one countries and is spoken by over 441 million people. Naturally, there are many variations in how Spanish is spoken across these countries. Media platforms such as YouTube rely on automatic speech recognition systems to make their content accessible to different groups of users. However, YouTube offers only one option for automatically generating captions in Spanish. This raises the question: could this captioning system be biased against certain Spanish dialects? This study examines the potential biases in YouTube's automatic captioning system by analyzing its performance across various Spanish dialects. By comparing the quality of captions for female and male speakers from different regions, we identify systematic disparities which can be attributed to specific dialects. Our study provides further evidence that algorithmic technologies deployed on digital platforms need to be calibrated to the diverse needs and experiences of their user populations.

</details>


### [24] [Task Complexity Matters: An Empirical Study of Reasoning in LLMs for Sentiment Analysis](https://arxiv.org/abs/2602.24060)
*Donghao Huang,Zhaoxia Wang*

Main category: cs.CL

TL;DR: 本研究通过系统评估504种配置挑战了"推理普遍提升大模型性能"的主流观点，发现推理效果具有强烈任务依赖性：二元分类任务F1下降达-19.9pp，27类情感识别提升+16.0pp。基础模型在效率-性能权衡上占优，推理仅在复杂任务中值得2.1-54倍的计算开销。


<details>
  <summary>Details</summary>
Motivation: 挑战当前关于推理能力能普遍提升大语言模型在所有语言任务上表现的主流叙事，通过全面评估验证其真实有效性。

Method: 对7个模型家族的504种配置进行系统性评估，涵盖自适应、条件和强化学习推理架构，在二元、五类和27类情感分析数据集上测试，并结合定性错误分析揭示机制。

Result: (1)推理效果依赖任务复杂度，二元分类F1下降达-19.9pp，27类情感识别提升+16.0pp；(2)蒸馏推理在简单任务上比基础模型低3-18pp；(3)少样本学习普遍优于零样本；(4)基础模型主导帕累托前沿，推理仅复杂情感识别任务中值得2.1-54倍开销；(5)定性分析揭示推理通过系统性过度思考损害简单任务性能。

Conclusion: 推理并不能普遍提升大模型性能，其有效性具有强烈任务依赖性。基础模型对简单任务更高效，推理机制仅在细粒度情感识别等复杂场景中才具价值，挑战了当前主流假设。

Abstract: Large language models (LLMs) with reasoning capabilities have fueled a compelling narrative that reasoning universally improves performance across language tasks. We test this claim through a comprehensive evaluation of 504 configurations across seven model families--including adaptive, conditional, and reinforcement learning-based reasoning architectures--on sentiment analysis datasets of varying granularity (binary, five-class, and 27-class emotion). Our findings reveal that reasoning effectiveness is strongly task-dependent, challenging prevailing assumptions: (1) Reasoning shows task-complexity dependence--binary classification degrades up to -19.9 F1 percentage points (pp), while 27-class emotion recognition gains up to +16.0pp; (2) Distilled reasoning variants underperform base models by 3-18 pp on simpler tasks, though few-shot prompting enables partial recovery; (3) Few-shot learning improves over zero-shot in most cases regardless of model type, with gains varying by architecture and task complexity; (4) Pareto frontier analysis shows base models dominate efficiency-performance trade-offs, with reasoning justified only for complex emotion recognition despite 2.1x-54x computational overhead. We complement these quantitative findings with qualitative error analysis revealing that reasoning degrades simpler tasks through systematic over-deliberation, offering mechanistic insight beyond the high-level overthinking hypothesis.

</details>


### [25] [Preference Packing: Efficient Preference Optimization for Large Language Models](https://arxiv.org/abs/2602.24082)
*Jaekyung Cho*

Main category: cs.CL

TL;DR: 提出 preference packing 方法，针对奖励模型和 DPO 等同一提示对应多个响应的训练场景，通过减少重复注意力计算和 KV 缓存内存，实现至少 37% 训练时间降低，并可与 batch sorting 结合达到 3.22 倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模持续扩大，资源高效训练变得至关重要。在奖励模型和直接偏好优化（DPO）等场景中，同一输入提示对应多个不同响应，传统方法存在重复计算和内存浪费问题，亟需优化。

Method: 提出 preference packing 技术，通过将相同输入提示的样本智能打包，减少重复的注意力运算和 KV 缓存内存占用，提升资源利用率。

Result: 在纯文本和图文数据集上的实验验证了该方法的有效性，训练时间至少减少 37%。与 batch sorting 等现有优化技术结合后，整体加速比达 3.22 倍。

Conclusion: preference packing 是一种适用于重复提示训练场景的高效资源优化方法，能显著提升训练效率，且与现有优化技术兼容，具有实用价值。

Abstract: Resource-efficient training optimization techniques are becoming increasingly important as the size of large language models (LLMs) continues to grow. In particular, batch packing is commonly used in pre-training and supervised fine-tuning to achieve resource-efficient training. We propose preference packing, a method to enhance resource efficiency in training techniques that use data with different responses for the same input prompt, such as reward models or Direct Preference Optimization (DPO). Preference packing improves resource efficiency by reducing the attention operations for duplicate input prompts and decreasing KV cache memory usage. We conducted experiments on text-only datasets and image-included datasets and achieved at least 37% reduction in training time. Notably, this method can be applied alongside existing optimization techniques such as batch sorting, resulting in a 3.22x speedup.

</details>


### [26] [ARGUS: Seeing the Influence of Narrative Features on Persuasion in Argumentative Texts](https://arxiv.org/abs/2602.24109)
*Sara Nabhani,Federico Pianzola,Khalid Al-Khatib,Malvina Nissim*

Main category: cs.CL

TL;DR: 本文提出ARGUS框架，用于系统研究叙事对在线论证说服力的影响。通过标注ChangeMyView语料库并识别故事及六大叙事特征，结合编码器分类器和零样本大语言模型，大规模分析不同叙事维度如何影响说服效果。


<details>
  <summary>Details</summary>
Motivation: 尽管故事常被视为强大的说服工具，但其在在线非结构化论证中的具体作用机制尚未得到充分探索，缺乏系统的研究框架和分析方法。

Method: 1) 构建新ChangeMyView语料库，标注故事存在性及六大关键叙事特征；2) 整合两个理论框架分别捕捉文本叙事特征及其对接收者的影响；3) 结合基于编码器的分类器和零样本大语言模型进行特征识别；4) 大规模应用以分析各叙事维度对说服成功的差异化影响。

Result: 摘要主要聚焦框架构建，未明确呈现具体数值结果。但该框架成功实现了叙事特征的大规模自动化识别与分析，建立了可扩展的研究管道，为揭示叙事维度与说服效果之间的关系提供了方法论基础。

Conclusion: ARGUS框架填补了在线论证中叙事说服力研究的空白，通过系统化标注和分析方法，为理解叙事特征如何影响说服效果提供了理论和工具支撑，未来可应用于更广泛的论证场景研究。

Abstract: Can narratives make arguments more persuasive? And to this end, which narrative features matter most? Although stories are often seen as powerful tools for persuasion, their specific role in online, unstructured argumentation remains underexplored. To address this gap, we present ARGUS, a framework for studying the impact of narration on persuasion in argumentative discourse. ARGUS introduces a new ChangeMyView corpus annotated for story presence and six key narrative features, integrating insights from two established theoretical frameworks that capture both textual narrative features and their effects on recipients. Leveraging both encoder-based classifiers and zero-shot large language models (LLMs), ARGUS identifies stories and narrative features and applies them at scale to examine how different narrative dimensions influence persuasion success in online argumentation.

</details>


### [27] [Terminology Rarity Predicts Catastrophic Failure in LLM Translation of Low-Resource Ancient Languages: Evidence from Ancient Greek](https://arxiv.org/abs/2602.24119)
*James L. Zainaldin,Cameron Pattison,Manuela Marai,Jacob Wu,Mark J. Schiefsky*

Main category: cs.CL

TL;DR: 本研究首次系统评估了大型语言模型对古希腊医学术语文本的翻译质量。通过专家人工评估和自动化指标分析发现，LLM在已有英译本的文本上表现接近专家水平，但在术语密集的未翻译药理学文本上质量显著下降，且术语罕见度是预测翻译失败的关键因素。自动化指标整体相关性有限，无法区分高质量译文。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型处理低资源古代语言（如古希腊语）技术文本的翻译能力，为古典学研究提供新的工具方法，并评估自动化评估指标在此类特殊领域的适用性。

Method: 选取伽伦医学著作中已翻译和未翻译的两类文本，使用Claude、Gemini、ChatGPT三个商业LLM生成译文。采用标准自动化指标（BLEU、COMET等）和经修改的多维质量评估框架（MQM），由领域专家团队对60篇译文进行人工评估，并结合语料库频率分析术语罕见度与翻译质量的关系。

Result: 在已翻译文本上，LLM译文平均MQM得分高达95.2/100，接近专家水平；在未翻译文本上平均得分为79.9/100，但排除两个术语极密集段落后，差距缩小至4分以内。术语罕见度与翻译质量呈强负相关（r=-0.97）。自动化指标仅在质量差异大的文本上与人工评价中度相关，无法区分高质量译文间的差异。

Conclusion: LLM在古典学术翻译中具有应用潜力，尤其对已有参考译本的文本；但面对术语密度高的未翻译文本时仍面临挑战。术语罕见度是核心难点。自动化评估指标需针对低资源古代语言进行专门优化，未来应构建结合领域知识的专用评估流程。

Abstract: This study presents the first systematic, reference-free human evaluation of large language model (LLM) machine translation (MT) for Ancient Greek (AG) technical prose. We evaluate translations by three commercial LLMs (Claude, Gemini, ChatGPT) of twenty paragraph-length passages from two works by the Greek physician Galen of Pergamum (ca. 129-216 CE): On Mixtures, which has two published English translations, and On the Composition of Drugs according to Kinds, which has never been fully translated into English. We assess translation quality using both standard automated evaluation metrics (BLEU, chrF++, METEOR, ROUGE-L, BERTScore, COMET, BLEURT) and expert human evaluation via a modified Multidimensional Quality Metrics (MQM) framework applied to all 60 translations by a team of domain specialists. On the previously translated expository text, LLMs achieved high translation quality (mean MQM score 95.2/100), with performance approaching expert level. On the untranslated pharmacological text, aggregate quality was lower (79.9/100) but with high variance driven by two passages presenting extreme terminological density; excluding these, scores converged to within 4 points of the translated text. Terminology rarity, operationalized via corpus frequency in the literary Diorisis Ancient Greek Corpus, emerged as a strong predictor of translation failure (r = -.97 for passage-level quality on the untranslated text). Automated metrics showed moderate correlation with human judgment overall on the text with a wide quality spread (Composition), but no metric discriminated among high-quality translations. We discuss implications for the use of LLMs in Classical scholarship and for the design of automated evaluation pipelines for low-resource ancient languages.

</details>


### [28] [CoME: Empowering Channel-of-Mobile-Experts with Informative Hybrid-Capabilities Reasoning](https://arxiv.org/abs/2602.24142)
*Yuxuan Liu,Weikai Xu,Kun Huang,Changyu Chen,Jiankun Zhao,Pengzhi Gao,Wei Liu,Jian Luan,Shuo Shang,Bo Du,Ji-Rong Wen,Rui Yan*

Main category: cs.CL

TL;DR: 针对移动智能体混合推理能力难以解耦增强与平衡集成的问题，本文提出CoME框架，通过四专家分工、渐进式训练及InfoGain-Driven DPO优化，在AITZ和AMEX数据集上实现性能突破。


<details>
  <summary>Details</summary>
Motivation: 移动智能体需同步实现屏幕摘要、子任务规划、行动决策与执行等多阶段推理能力，但现有方法存在能力耦合严重、难以平衡优化的瓶颈，限制了实际应用效果。

Method: 提出Channel-of-Mobile-Experts (CoME)四专家架构，每专家专精于特定推理阶段。采用输出导向激活机制动态调用专家。设计渐进式三阶段训练：Expert-FT实现专家能力解耦增强，Router-FT确保专家激活与推理阶段对齐，CoT-FT促进多能力协同优化。引入InfoGain-Driven DPO，利用信息增益量化中间步骤贡献，引导模型进行更有效的推理以抑制错误传播。

Result: 在AITZ和AMEX两个标准数据集上的综合实验表明，CoME架构显著优于现有密集移动智能体模型及传统MoE方法。

Conclusion: CoME通过专家化分工与渐进训练策略，有效解决了混合推理能力的解耦增强与平衡集成难题，InfoGain-Driven DPO进一步缓解了错误传播问题，为移动智能体推理提供了新范式。

Abstract: Mobile Agents can autonomously execute user instructions, which requires hybrid-capabilities reasoning, including screen summary, subtask planning, action decision and action function. However, existing agents struggle to achieve both decoupled enhancement and balanced integration of these capabilities. To address these challenges, we propose Channel-of-Mobile-Experts (CoME), a novel agent architecture consisting of four distinct experts, each aligned with a specific reasoning stage, CoME activates the corresponding expert to generate output tokens in each reasoning stage via output-oriented activation. To empower CoME with hybrid-capabilities reasoning, we introduce a progressive training strategy: Expert-FT enables decoupling and enhancement of different experts' capability; Router-FT aligns expert activation with the different reasoning stage; CoT-FT facilitates seamless collaboration and balanced optimization across multiple capabilities. To mitigate error propagation in hybrid-capabilities reasoning, we propose InfoGain-Driven DPO (Info-DPO), which uses information gain to evaluate the contribution of each intermediate step, thereby guiding CoME toward more informative reasoning. Comprehensive experiments show that CoME outperforms dense mobile agents and MoE methods on both AITZ and AMEX datasets.

</details>


### [29] [ArgLLM-App: An Interactive System for Argumentative Reasoning with Large Language Models](https://arxiv.org/abs/2602.24172)
*Adam Dejl,Deniz Gorur,Francesca Toni*

Main category: cs.CL

TL;DR: 本文提出ArgLLM-App，一个基于论证型大语言模型（ArgLLMs）的Web系统，用于二元决策任务。该系统提供可视化解释、人机交互界面，允许用户识别并质疑系统推理错误，具备高模块化架构，可集成可信外部信息来源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型决策过程缺乏透明度和可解释性，形成“黑箱”问题。通过将大语言模型与计算论证结合，旨在构建可忠实解释且可供人类质询的决策系统，增强AI决策的可信度和可问责性。

Method: 开发了一个名为ArgLLM-App的Web应用程序，部署论证增强的大语言模型智能体处理二元分类任务。系统核心功能包括：推理过程可视化、用户交互界面用于识别和反驳系统错误、模块化设计以及外部可信数据源集成能力。

Result: 系统已公开发布于https://argllm.app，并配套演示视频（https://youtu.be/vzwlGOr0sPM）。成功实现了论证型大语言模型的用户端应用，支持实时人机协同纠错。

Conclusion: ArgLLM-App为可信赖AI提供了实用化路径，通过可解释性与可辩驳性设计，使大语言模型决策过程对用户透明且可修正。模块化架构与外部信息源集成增强了其在真实场景中的适用性，为负责任AI部署建立了重要范例。

Abstract: Argumentative LLMs (ArgLLMs) are an existing approach leveraging Large Language Models (LLMs) and computational argumentation for decision-making, with the aim of making the resulting decisions faithfully explainable to and contestable by humans. Here we propose a web-based system implementing ArgLLM-empowered agents for binary tasks. ArgLLM-App supports visualisation of the produced explanations and interaction with human users, allowing them to identify and contest any mistakes in the system's reasoning. It is highly modular and enables drawing information from trusted external sources. ArgLLM-App is publicly available at https://argllm.app, with a video demonstration at https://youtu.be/vzwlGOr0sPM.

</details>


### [30] [Task-Centric Acceleration of Small-Language Models](https://arxiv.org/abs/2602.24174)
*Dor Tsur,Sharon Adar,Ran Levy*

Main category: cs.CL

TL;DR: 本文提出TASC框架，通过任务自适应序列压缩加速小语言模型。包含两种方法：TASC-ft在微调时扩展词表加入高频输出n-gram；TASC-spec在推理时构建n-gram草稿模型进行推测解码，无需训练且能绕过词表对齐限制。在低输出变异性任务上显著提升推理效率且保持性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在特定任务应用中虽比大模型高效，但在高并发、低延迟场景下仍需进一步优化效率。现有方法存在局限，如微调成本高或推测解码受词表对齐约束。因此需要一种既能提升推理效率又不牺牲任务性能的压缩加速框架。

Method: 提出两阶段框架：1）TASC-ft：迭代式扩充分词器词表，加入任务输出中的高频n-gram，然后微调模型适配扩展词表；2）TASC-spec：推理时基于任务输出语料构建轻量级n-gram草稿模型，融合任务先验和上下文信息进行推测解码，无需额外训练，避免草稿与目标模型间的词表对齐问题。

Result: 在多个低输出变异性生成任务上验证，两种方法均显示一致的推理效率提升，同时保持任务性能不下降。

Conclusion: TASC框架通过任务自适应的序列压缩策略，为小语言模型提供了高效的微调与推理加速方案，特别适用于对延迟敏感的高吞吐量场景，具有实用价值。

Abstract: Small language models (SLMs) have emerged as efficient alternatives to large language models for task-specific applications. However, they are often employed in high-volume, low-latency settings, where efficiency is crucial. We propose TASC, Task-Adaptive Sequence Compression, a framework for SLM acceleration comprising two use-cases: When performing SLM fine-tuning, we propose TASC-ft, which iteratively enriches the tokenizer vocabulary with high-frequency output n-grams and then fine-tunes the model to utilize the expanded vocabulary. Next, we propose an inference-time method, termed TASC-spec. TASC-spec is a lightweight, training-free speculative decoding method that constructs an n-gram draft model from the task's output corpus, mixing task and context n-gram information.TASC-spec avoids any additional training, while bypassing draft-target vocabulary alignment constraints. We demonstrate the effectiveness of both methods across multiple low output-variability generation tasks. Our methods show consistent improvements in inference efficiency while maintaining task performance.

</details>


### [31] [MT-PingEval: Evaluating Multi-Turn Collaboration with Private Information Games](https://arxiv.org/abs/2602.24188)
*Jacob Eisenstein,Fantine Huot,Adam Fisch,Jonathan Berant,Mirella Lapata*

Main category: cs.CL

TL;DR: 本文提出MT-PingEval框架，通过协作游戏评估多轮交互中语言模型的私有信息沟通能力。研究发现现有模型无法利用多轮协作超越非交互基线，存在显著协作规划缺陷。人类通过更高连贯性的对话实现更优的token效率。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在多轮协作对话中存在根本性弱点，无法有效管理私有信息交流。本研究旨在通过可扩展的评估方法，深入分析模型在交互式协作中的表现，揭示其规划执行缺陷，为提升真实世界沟通能力提供研究基础。

Method: 提出可扩展的MT-PingEval评估框架，采用需要交流私有信息的协作游戏套件，在固定token预算下进行可变轮次的交互式扩展分析，对比多轮协作场景与非交互基线场景的性能差异。

Result: 实验表明多数语言模型无法通过多轮协作改善性能，仍显著落后于非交互基线。模型存在协作规划与执行的重大缺陷，且无单一语言特征（如奉承、信息密度、话语连贯性）能完全解释此弱点。人类通过产生更连贯的对话实现更高任务成功率与token效率。

Conclusion: 研究揭示出现有语言模型在多轮协作对话中存在根本性缺陷，尤其在主动管理私有信息方面。MT-PingEval框架有望推动该领域研究，促进模型向具备真实世界协作能力的方向发展。

Abstract: We present a scalable methodology for evaluating language models in multi-turn interactions, using a suite of collaborative games that require effective communication about private information. This enables an interactive scaling analysis, in which a fixed token budget is divided over a variable number of turns. We find that in many cases, language models are unable to use interactive collaboration to improve over the non-interactive baseline scenario in which one agent attempts to summarize its information and the other agent immediately acts -- despite substantial headroom. This suggests that state-of-the-art models still suffer from significant weaknesses in planning and executing multi-turn collaborative conversations. We analyze the linguistic features of these dialogues, assessing the roles of sycophancy, information density, and discourse coherence. While there is no single linguistic explanation for the collaborative weaknesses of contemporary language models, we note that humans achieve comparable task success at superior token efficiency by producing dialogues that are more coherent than those produced by most language models. The proactive management of private information is a defining feature of real-world communication, and we hope that MT-PingEval will drive further work towards improving this capability.

</details>


### [32] [Controllable Reasoning Models Are Private Thinkers](https://arxiv.org/abs/2602.24210)
*Haritz Puerto,Haonan Li,Xudong Han,Timothy Baldwin,Iryna Gurevych*

Main category: cs.CL

TL;DR: 针对推理模型访问敏感数据时存在的隐私泄露风险，本研究提出在推理痕迹层面实施指令约束以增强隐私保护。通过在新构建的含推理痕迹限制的数据集上微调模型，并采用分离推理与答案生成的双LoRA适配器策略，在6个1.7B-14B参数的模型上验证，指令遵循性能提升达20.9点，隐私保护提升达51.9个百分点，但需权衡任务效用。


<details>
  <summary>Details</summary>
Motivation: 现有AI推理模型在处理敏感用户数据时，其难以控制的推理痕迹可能导致隐私信息意外泄露。当前研究多聚焦于最终输出的安全性，忽视了推理过程本身的隐私风险。因此，研究者认为必须在推理痕迹层面引入指令遵循机制，以提升模型的隐私保护能力。

Method: 研究构建了一个新的指令遵循数据集，明确对推理痕迹施加限制。方法采用独立的LoRA适配器解耦推理过程与答案生成，在两个模型家族的六个模型（参数量1.7B至14B）上进行微调，并在两个指令遵循基准和两个隐私基准上进行系统评估。

Result: 实验结果显示，该方法在指令遵循性能上获得最高20.9点的提升，在隐私基准上获得最高51.9个百分点的提升。然而，性能改进伴随着任务效用的代价，反映了推理性能与指令遵循能力之间的内在权衡。研究在多种模型规模和架构上验证了方法的有效性。

Conclusion: 研究证实，强化推理模型在推理痕迹中的指令遵循行为可显著增强隐私保护，为开发隐私感知的智能体提供了新方向。尽管存在与任务效用的权衡，但该方向具有前瞻性价值。

Abstract: AI agents powered by reasoning models require access to sensitive user data. However, their reasoning traces are difficult to control, which can result in the unintended leakage of private information to external parties. We propose training models to follow instructions not only in the final answer, but also in reasoning traces, potentially under different constraints. We hypothesize that improving their instruction following abilities in the reasoning traces can improve their privacy-preservation skills. To demonstrate this, we fine-tune models on a new instruction-following dataset with explicit restrictions on reasoning traces. We further introduce a generation strategy that decouples reasoning and answer generation using separate LoRA adapters. We evaluate our approach on six models from two model families, ranging from 1.7B to 14B parameters, across two instruction-following benchmarks and two privacy benchmarks. Our method yields substantial improvements, achieving gains of up to 20.9 points in instruction-following performance and up to 51.9 percentage points on privacy benchmarks. These improvements, however, can come at the cost of task utility, due to the trade-off between reasoning performance and instruction-following abilities. Overall, our results show that improving instruction-following behavior in reasoning models can significantly enhance privacy, suggesting a promising direction for the development of future privacy-aware agents. Our code and data are available at https://github.com/UKPLab/arxiv2026-controllable-reasoning-models

</details>


### [33] [Do LLMs Benefit From Their Own Words?](https://arxiv.org/abs/2602.24287)
*Jenny Y. Huang,Leshem Choshen,Ramon Astudillo,Tamara Broderick,Jacob Andreas*

Main category: cs.CL

TL;DR: 该研究发现大语言模型多轮对话中省略助手历史回复在大部分情况下不影响质量，反而能减少10倍上下文长度，并识别出36.4%自包含提示及"上下文污染"现象，提出选择性过滤方法可提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统多轮对话默认保留助手历史回复，但这一设计是否必要缺乏验证。论文重新审视该设计选择，探究模型是否真需要依赖自身先前回复来生成高质量响应。

Method: 使用真实世界多轮对话数据，对比标准全上下文提示与仅用户轮次提示（完全省略先前助手回复）的效果，在三个开放推理模型和一个先进模型上进行测试。

Result: 发现移除助手历史在大部分轮次不影响质量；可减少高达10倍上下文长度；36.4%提示为自包含；许多后续提示仅凭当前和先前用户轮次即可回答；识别出"上下文污染"现象（模型过度依赖先前回复导致错误、幻觉或风格污染并跨轮传播）；设计了上下文过滤方法。

Conclusion: 有选择地省略助手历史既能提升响应质量（避免污染），又能显著降低内存消耗，挑战了传统保留全部对话历史的设计假设。

Abstract: Multi-turn interactions with large language models typically retain the assistant's own past responses in the conversation history. In this work, we revisit this design choice by asking whether large language models benefit from conditioning on their own prior responses. Using in-the-wild, multi-turn conversations, we compare standard (full-context) prompting with a user-turn-only prompting approach that omits all previous assistant responses, across three open reasoning models and one state-of-the-art model. To our surprise, we find that removing prior assistant responses does not affect response quality on a large fraction of turns. Omitting assistant-side history can reduce cumulative context lengths by up to 10x. To explain this result, we find that multi-turn conversations consist of a substantial proportion (36.4%) of self-contained prompts, and that many follow-up prompts provide sufficient instruction to be answered using only the current user turn and prior user turns. When analyzing cases where user-turn-only prompting substantially outperforms full context, we identify instances of context pollution, in which models over-condition on their previous responses, introducing errors, hallucinations, or stylistic artifacts that propagate across turns. Motivated by these findings, we design a context-filtering approach that selectively omits assistant-side context. Our findings suggest that selectively omitting assistant history can improve response quality while reducing memory consumption.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [34] [Reason to Contrast: A Cascaded Multimodal Retrieval Framework](https://arxiv.org/abs/2602.23369)
*Xuanming Cui,Hong-You Chen,Hao Yu,Hao Yuan,Zihao Wang,Shlok Kumar Mishra,Hanchao Yu,Yonghuan Yang,Jun Xiao,Ser-Nam Lim,Jianpeng Cheng,Qi Guo,Xiangjun Fan*

Main category: cs.IR

TL;DR: TTE-v2提出一种混合多模态检索框架，通过基于推理token预算（而非模型/嵌入大小）的性能扩展机制，在测试时利用额外推理步骤进行重排序，增强查询-候选项交互。在MMEB-V2基准上，7B模型达到75.7% SOTA准确率，2B模型媲美使用更大外部数据训练的7B基线，验证了token级扩展的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统双编码器架构的性能瓶颈在于嵌入维度，尽管TTE通过预嵌入推理有所改进，但现有方法未能充分利用测试时token预算和推理能力进行细粒度交互与反馈优化。

Method: TTE-v2采用级联架构：1) 初始多模态检索；2) 基于额外推理token的重排序阶段，实现更丰富的测试时查询-候选项交互；3) 重排序提供细粒度监督信号，用于困难负样本挖掘和假阴性过滤，形成强化上游检索器的反馈循环。该方法通过增加推理token实现性能扩展。

Result: MMEB-V2基准测试显示，TTE-v2-7B达到75.7%的SOTA准确率；TTE-v2-2B性能匹配或超越使用显著更大外部数据训练的领先7B模型，证明token级扩展的效率优势。

Conclusion: 研究证明了基于token预算的推理驱动扩展可作为传统模型/嵌入大小扩展的替代方案，为多模态检索提供了一种高效、可扩展的新范式，具有广阔的应用前景。

Abstract: Traditional multimodal retrieval systems rely primarily on bi-encoder architectures, where performance is closely tied to embedding dimensionality. Recent work, Think-Then-Embed (TTE), shows that incorporating multimodal reasoning to elicit additional informative tokens before embedding can further improve retrieval. In this paper, we extend this paradigm with TTE-v2, a hybrid multimodal retrieval framework that introduces reasoning-driven performance scaling based on additional input token budget rather than model or embedding size. Our approach augments the initial multimodal retrieval with additional reasoning steps for reranking, enabling more expressive query-candidate interactions at test time. The reranking stage further provides fine-grained supervision for hard negative mining and false negative filtering, creating a feedback loop that effectively strengthens the upstream retriever. This cascaded design delivers substantial test-time improvements based on intermediate reasoning token scaling. Experiments on the MMEB-V2 benchmark demonstrate that TTE-v2-7B achieves a new state-of-the-art accuracy of 75.7%, and that TTE-v2-2B matches or surpasses leading 7B models trained with significantly larger external data. Our results highlight the promise of token-wise scaling as an alternative scaling paradigm for multimodal retrieval.

</details>


### [35] [Democratizing GraphRAG: Linear, CPU-Only Graph Retrieval for Multi-Hop QA](https://arxiv.org/abs/2602.23372)
*Qizhi Wang*

Main category: cs.IR

TL;DR: SPRIG是一种CPU-only、线性时间、无token成本的GraphRAG方案，以轻量级NER共现图取代LLM建图，结合个性化PageRank实现28%的成本优化且Recall@10几乎不变，为GraphRAG普及提供了经济可行的技术路径。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG系统依赖昂贵的LLM建图和GPU推理，导致部署成本高、难以普及。本研究旨在开发一种低成本、易部署的GraphRAG方案，消除token费用和GPU依赖，实现技术民主化。

Method: 提出SPRIG框架：采用CPU-only架构，实现线性时间复杂度；使用命名实体识别(NER)构建轻量级共现图，替代LLM图构建；采用个性化PageRank(PPR)进行多跳检索；全程无需token。

Result: 系统在保持Recall@10几乎不变的前提下，实现28%的成本/效率优化。研究还明确了CPU友好图检索在何时能提升多跳召回率，以及纯词法混合(RRF)何时已足够，为实际应用提供决策依据。

Conclusion: SPRIG通过消除token成本和GPU需求，在维持检索性能的同时大幅降低部署门槛，为GraphRAG技术的民主化和大规模应用铺平了道路，提供了现实可行的技术方案。

Abstract: GraphRAG systems improve multi-hop retrieval by modeling structure, but many approaches rely on expensive LLM-based graph construction and GPU-heavy inference. We present SPRIG (Seeded Propagation for Retrieval In Graphs), a CPU-only, linear-time, token-free GraphRAG pipeline that replaces LLM graph building with lightweight NER-driven co-occurrence graphs and uses Personalized PageRank (PPR) for 28% with negligible Recall@10 changes. The results characterize when CPU-friendly graph retrieval helps multi-hop recall and when strong lexical hybrids (RRF) are sufficient, outlining a realistic path to democratizing GraphRAG without token costs or GPU requirements.

</details>


### [36] [Higress-RAG: A Holistic Optimization Framework for Enterprise Retrieval-Augmented Generation via Dual Hybrid Retrieval, Adaptive Routing, and CRAG](https://arxiv.org/abs/2602.23374)
*Weixi Lin*

Main category: cs.IR

TL;DR: 本文针对企业RAG系统从概念验证到生产部署面临的检索精度低、幻觉率高和延迟不可接受三大挑战，提出基于模型上下文协议（MCP）的Higress RAG MCP Server架构。该架构采用"全链路优化"策略，通过自适应路由、语义缓存、混合检索和纠错RAG（CRAG）等创新技术，在Higress技术文档上的实验验证了其可扩展性和抗幻觉能力，为企业AI部署提供了生产级解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成（RAG）范式通过结合参数化记忆与非参数化外部数据促进了LLM在企业知识管理系统中的集成，但从概念验证向生产级RAG系统过渡仍面临三大瓶颈：复杂查询检索精度不足、生成阶段幻觉率高，以及实时应用延迟不可接受。这些挑战严重制约了企业级AI应用的可行性和可靠性，亟需系统化的架构创新来解决。

Method: 本研究提出基于模型上下文协议（MCP）的Higress RAG MCP Server企业架构，采用"全链路优化"策略。核心方法包括：1）分层架构编排自适应路由、语义缓存、混合检索和纠错RAG（CRAG）流水线；2）开发Higress-Native Splitter实现结构感知数据 ingestion；3）应用互反排序融合（RRF）合并稠密与稀疏检索信号；4）设计动态阈值50ms级语义缓存机制。通过优化从查询重写到检索后纠错的完整生命周期来系统性解决生产瓶颈。

Result: 在Higress技术文档和博客的领域特定数据集上进行的实验评估验证了系统架构的鲁棒性。结果表明，该架构通过全链路优化策略显著提升了检索精度，降低了幻觉率，并实现了50ms级的低延迟响应。系统展现了良好的可扩展性，为企业AI部署提供了抗幻觉、高性能的生产级解决方案。

Conclusion: Higress RAG MCP Server通过全链路优化策略成功解决了生产级RAG系统的核心挑战，其创新的分层架构和关键技术组件（自适应路由、语义缓存、混合检索、CRAG）有效平衡了精度、幻觉和延迟三者间的权衡。该架构为企业部署可扩展、可靠的AI应用提供了可行的技术路径，标志着RAG技术从实验室概念向企业生产环境的重要迈进。

Abstract: The integration of Large Language Models (LLMs) into enterprise knowledge management systems has been catalyzed by the Retrieval-Augmented Generation (RAG) paradigm, which augments parametric memory with non-parametric external data. However, the transition from proof-of-concept to production-grade RAG systems is hindered by three persistent challenges: low retrieval precision for complex queries, high rates of hallucination in the generation phase, and unacceptable latency for real-time applications. This paper presents a comprehensive analysis of the Higress RAG MCP Server, a novel, enterprise-centric architecture designed to resolve these bottlenecks through a "Full-Link Optimization" strategy. Built upon the Model Context Protocol (MCP), the system introduces a layered architecture that orchestrates a sophisticated pipeline of Adaptive Routing, Semantic Caching, Hybrid Retrieval, and Corrective RAG (CRAG). We detail the technical implementation of key innovations, including the Higress-Native Splitter for structure-aware data ingestion, the application of Reciprocal Rank Fusion (RRF) for merging dense and sparse retrieval signals, and a 50ms-latency Semantic Caching mechanism with dynamic thresholding. Experimental evaluations on domain-specific Higress technical documentation and blogs verify the system's architectural robustness. The results demonstrate that by optimizing the entire retrieval lifecycle - from pre-retrieval query rewriting to post-retrieval corrective evaluation - the Higress RAG system offers a scalable, hallucination-resistant solution for enterprise AI deployment.

</details>


### [37] [Unified Learning-to-Rank for Multi-Channel Retrieval in Large-Scale E-Commerce Search](https://arxiv.org/abs/2602.23530)
*Aditya Gaydhani,Guangyue Xu,Dhanush Kamath,Ankit Singh,Alex Li*

Main category: cs.IR

TL;DR: 本文提出一种用于电商多通道检索的统一排序模型，将融合问题重构为查询感知的学习排序任务。该模型动态学习通道权重与交互，联合优化点击、加购、购买等目标，并利用用户行为信号捕捉短期意图。实验显示转化率提升2.85%，延迟低于50毫秒，已在Target.com部署。


<details>
  <summary>Details</summary>
Motivation: 大型电商搜索需从海量商品库中呈现多样化商品，现代系统依赖多个专用检索通道。然而，传统排名融合方法（如RRF和加权交错）采用固定全局权重且独立处理通道，无法适应查询特定的通道效用和跨通道交互，在严格延迟约束下难以有效优化用户转化等业务指标。

Method: 本文将多通道融合问题重新定义为查询依赖的异质源学习排序问题，提出通道感知的统一排序框架。该模型联合优化点击、加购和购买等多目标，并融入通道特定目标，同时引入近期用户行为信号以捕获影响转化率的关键短期意图变化。

Result: 在线A/B测试表明，所提方法显著优于传统排名融合方法，使用户转化率提升2.85%。模型满足生产环境延迟要求，p95延迟低于50毫秒，并成功部署于Target.com。

Conclusion: 该研究成功将多通道检索融合问题转化为可学习的排序任务，通过查询感知的动态权重学习和跨通道交互建模，在提升商业指标的同时保证系统性能，为大型电商平台的排序架构提供了有效解决方案。

Abstract: Large-scale e-commerce search must surface a broad set of items from a vast catalog, ranging from bestselling products to new, trending, or seasonal items. Modern systems therefore rely on multiple specialized retrieval channels to surface products, each designed to satisfy a specific objective. A key challenge is how to effectively merge documents from these heterogeneous channels into a single ranked list under strict latency constraints while optimizing for business KPIs such as user conversion. Rank-based fusion methods such as Reciprocal Rank Fusion (RRF) and Weighted Interleaving rely on fixed global channel weights and treat channels independently, failing to account for query-specific channel utility and cross-channel interactions. We observe that multi-channel fusion can be reformulated as a query-dependent learning-to-rank problem over heterogeneous candidate sources. In this paper, we propose a unified ranking model that learns to merge and rank documents from multiple retrieval channels. We formulate the problem as a channel-aware learning-to-rank task that jointly optimizes clicks, add-to-carts, and purchases while incorporating channel-specific objectives. We further incorporate recent user behavioral signals to capture short-term intent shifts that are critical for improving conversion in multi-channel ranking. Our online A/B experiments show that the proposed approach outperforms rank-based fusion methods, leading to a +2.85\% improvement in user conversion. The model satisfies production latency requirements, achieving a p95 latency of under 50\,ms, and is deployed on Target.com.

</details>


### [38] [Synthetic Data Powers Product Retrieval for Long-tail Knowledge-Intensive Queries in E-commerce Search](https://arxiv.org/abs/2602.23620)
*Gui Ling,Weiyuan Li,Yue Jiang,Wenjun Peng,Xingxian Liu,Dongshuai Li,Fuyu Lv,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: 针对电商搜索中知识密集型长尾查询的挑战，本文提出一种数据合成框架，通过将离线查询重写模型的能力蒸馏至在线检索系统，利用LLM训练多候选重写模型并生成高质量查询-商品对，显著提升检索效果与用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有电商产品检索系统在处理长尾、知识密集型查询时表现不佳，这些查询语言模式多样、缺乏明确购买意图、需领域知识推理，且行为日志稀少，造成检索优化的持续挑战。

Method: 提出一种高效数据合成框架，核心是隐式蒸馏离线查询重写模型能力至在线检索系统。利用大语言模型的强语义理解能力，训练融合多重奖励信号的多候选查询重写模型，通过离线检索管道生成精选的查询-商品对，以缓解重写查询的分布偏移问题。

Result: 实验证明，无需额外技巧，仅将该合成数据加入检索模型训练即可带来显著提升。在线Side-By-Side人工评估表明用户搜索体验显著增强。

Conclusion: 该框架有效解决了知识密集型长尾查询的检索难题，通过模型蒸馏与数据合成策略显著改善召回质量，为电商搜索系统优化提供了可行的新路径。

Abstract: Product retrieval is the backbone of e-commerce search: for each user query, it identifies a high-recall candidate set from billions of items, laying the foundation for high-quality ranking and user experience. Despite extensive optimization for mainstream queries, existing systems still struggle with long-tail queries, especially knowledge-intensive ones. These queries exhibit diverse linguistic patterns, often lack explicit purchase intent, and require domain-specific knowledge reasoning for accurate interpretation. They also suffer from a shortage of reliable behavioral logs, which makes such queries a persistent challenge for retrieval optimization. To address these issues, we propose an efficient data synthesis framework tailored to retrieval involving long-tail, knowledge-intensive queries. The key idea is to implicitly distill the capabilities of a powerful offline query-rewriting model into an efficient online retrieval system. Leveraging the strong language understanding of LLMs, we train a multi-candidate query rewriting model with multiple reward signals and capture its rewriting capability in well-curated query-product pairs through a powerful offline retrieval pipeline. This design mitigates distributional shift in rewritten queries, which might otherwise limit incremental recall or introduce irrelevant products. Experiments demonstrate that without any additional tricks, simply incorporating this synthetic data into retrieval model training leads to significant improvements. Online Side-By-Side (SBS) human evaluation results indicate a notable enhancement in user search experience.

</details>


### [39] [Learning to Reflect and Correct: Towards Better Decoding Trajectories for Large-Scale Generative Recommendation](https://arxiv.org/abs/2602.23639)
*Haibo Xing,Hao Deng,Lingyu Mu,Jinxin Hu,Yu Zhang,Xiaoyi Zeng,Jing Zhang*

Main category: cs.IR

TL;DR: 本文提出首个生成式推荐的结构化反思-修正框架GRC，通过生成-反思-修正三阶段流程和GRPO强化学习优化，显著提升推荐质量和商业收益。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐模型采用单步解码且缺乏显式优化机制，导致早期偏差累积并降低推荐质量，亟需结构化反思和修正方法来解决这一问题。

Method: 提出GRC框架，将标准解码扩展为生成-反思-修正三阶段流程；引入监督式反思-修正模板，实现语义token空间的结构化反思和修正；采用GRPO强化学习优化整个GRC轨迹，设计token级和轨迹级奖励函数；提出熵引导的反思调度策略EGRS，在beam搜索中动态分配修正资源给高不确定性轨迹。

Result: 在真实世界数据集上，GRC相比六种SOTA基线模型性能提升最高达15.74%；在线A/B测试显示，仅带来适度延迟开销的同时，广告收入提升1.79%。

Conclusion: GRC框架通过结构化反思修正机制和强化学习优化，有效解决了生成式推荐中的偏差累积问题，在提升推荐质量的同时创造了显著商业价值，证明了其在工业级大规模推荐系统中的实用性和有效性。

Abstract: Generative Recommendation (GR) has become a promising paradigm for large-scale recommendation systems. However, existing GR models typically perform single-pass decoding without explicit refinement, causing early deviations to accumulate and ultimately degrade recommendation quality. To tackle this problem, we propose GRC, which is, to our knowledge, the first structured reflection-correction framework for GR that extends standard decoding into a Generation-Reflection-Correction (GRC) process. Concretely, GRC introduces a supervised reflection-correction template that decomposes the decoding process into initial draft generation, multi-granular reflection, and reflection-guided correction, thereby enabling structured reflection and correction in the semantic token space. To further explore the enlarged refinement space introduced by the GRC process, we optimize the entire GRC trajectory with GRPO-based reinforcement learning, under a carefully designed reward function with token-level and trajectory-level signals. For efficient online serving, we propose an Entropy-Guided Reflection Scheduling (EGRS) strategy that dynamically allocates more correction budget to high-uncertainty decoding trajectories during beam search. Extensive experiments on real-world datasets show that GRC consistently outperforms six state-of-the-art baselines by up to 15.74%, and online A/B tests demonstrate its substantial practical value in large-scale industrial recommendation, delivering a 1.79% lift in advertising revenue with only modest latency overhead.

</details>


### [40] [FuXi-Linear: Unleashing the Power of Linear Attention in Long-term Time-aware Sequential Recommendation](https://arxiv.org/abs/2602.23671)
*Yufei Ye,Wei Guo,Hao Wang,Luankang Zhang,Heng Chang,Hong Zhu,Yuyang Ye,Yong Liu,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: 本文提出FuXi-Linear，一种线性复杂度的长序列推荐模型，通过时间保留通道独立处理周期性信号并防止串扰，以及线性位置通道增强位置编码，在千级序列上实现SOTA性能，预填充和解码阶段分别获得最高10倍和21倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统的二次复杂度注意力机制限制了对长用户序列的处理能力和推理速度。现有线性注意力研究面临三大挑战：时间信号被忽视或简单耦合导致相互干扰、周期性行为未被利用；现有线性框架位置信息不足；研究集中于短序列和浅层架构。

Method: 提出FuXi-Linear模型，包含两个关键组件：(1)时间保留通道，利用时序数据独立计算周期性注意力权重，避免时间与语义信号串扰；(2)线性位置通道，通过可学习核函数在常数复杂度内集成位置信息。此外，模型展现了千级序列尺度下的稳健幂律缩放特性。

Result: 在长达数千token的序列上，FuXi-Linear在推荐质量上超越现有最优模型，相比竞争基线在预填充阶段实现最高10倍加速，在解码阶段实现最高21倍加速。

Conclusion: FuXi-Linear通过分离时序与语义信号处理的创新架构，有效解决了长序列推荐中的效率与效果权衡问题，为大规模推荐系统提供了具有线性复杂度的实用解决方案。

Abstract: Modern recommendation systems primarily rely on attention mechanisms with quadratic complexity, which limits their ability to handle long user sequences and slows down inference. While linear attention is a promising alternative, existing research faces three critical challenges: (1) temporal signals are often overlooked or integrated via naive coupling that causes mutual interference between temporal and semantic signals while neglecting behavioral periodicity; (2) insufficient positional information provided by existing linear frameworks; and (3) a primary focus on short sequences and shallow architectures. To address these issues, we propose FuXi-Linear, a linear-complexity model designed for efficient long-sequence recommendation. Our approach introduces two key components: (1) a Temporal Retention Channel that independently computes periodic attention weights using temporal data, preventing crosstalk between temporal and semantic signals; (2) a Linear Positional Channel that integrates positional information through learnable kernels within linear complexity. Moreover, we demonstrate that FuXi-Linear exhibits a robust power-law scaling property at a thousand-length scale, a characteristic largely unexplored in prior linear recommendation studies. Extensive experiments on sequences of several thousand tokens demonstrate that FuXi-Linear outperforms state-of-the-art models in recommendation quality, while achieving up to 10$\times$ speedup in the prefill stage and up to 21$\times$ speedup in the decode stage compared to competitive baselines. Our code has been released in a public repository https://github.com/USTC-StarTeam/fuxi-linear.

</details>


### [41] [Recommending Search Filters To Improve Conversions At Airbnb](https://arxiv.org/abs/2602.23717)
*Hao Li,Kedar Bellare,Siyu Yang,Sherry Chen,Liwei He,Stephanie Moyerman,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 本文提出了一种创新的机器学习应用，开发了搜索过滤器推荐系统以直接提升Airbnb的预订转化率。该系统从零构建，解决了冷启动和高并发服务挑战，经A/B测试验证在多界面部署后带来显著转化率提升。


<details>
  <summary>Details</summary>
Motivation: 搜索过滤器虽在在线平台中普遍存在，但其对驱动转化的直接效果在学术研究中尚未得到充分探索。Airbnb作为连接房客与房东的双边市场，拥有大量多样化住宿库存，过滤器是用户决策的关键工具，但如何主动优化过滤器推荐以最大化预订转化率仍是一个重要且未被系统解决的问题。

Method: 研究引入了一个专门针对漏斗下游转化（预订）的建模框架，通过推荐中间工具（搜索过滤器）来优化最终转化率。系统从0到1构建，采用机器学习技术解决冷启动问题并满足严格的线上服务延迟要求，设计了完整的过滤器推荐引擎架构。

Result: 所开发的系统已在Airbnb生产环境成功部署，支持多个用户界面，并通过严格的在线A/B测试验证带来了统计显著的增量预订转化率提升。消融研究进一步量化了各设计选择对效果的贡献，证实了方法的有效性。

Conclusion: 本研究通过聚焦转化导向的过滤器推荐，确保搜索过滤器发挥其终极价值。成果不仅填补了学术界在过滤器推荐与转化率关系方面的研究空白，也为在线市场平台提供了可复制的优化框架，具有重要的理论和实践意义。

Abstract: Airbnb, a two-sided online marketplace connecting guests and hosts, offers a diverse and unique inventory of accommodations, experiences, and services. Search filters play an important role in helping guests navigate this variety by refining search results to align with their needs. Yet, while search filters are designed to facilitate conversions in online marketplaces, their direct impact on driving conversions remains underexplored in the existing literature.
  This paper bridges this gap by presenting a novel application of machine learning techniques to recommend search filters aimed at improving booking conversions. We introduce a modeling framework that directly targets lower-funnel conversions (bookings) by recommending intermediate tools, i.e. search filters. Leveraging the framework, we designed and built the filter recommendation system at Airbnb from the ground up, addressing challenges like cold start and stringent serving requirements.
  The filter recommendation system we developed has been successfully deployed at Airbnb, powering multiple user interfaces and driving incremental booking conversion lifts, as validated through online A/B testing. An ablation study further validates the effectiveness of our approach and key design choices. By focusing on conversion-oriented filter recommendations, our work ensures that search filters serve their ultimate purpose at Airbnb - helping guests find and book their ideal accommodations.

</details>


### [42] [UniFAR: A Unified Facet-Aware Retrieval Framework for Scientific Documents](https://arxiv.org/abs/2602.23766)
*Zheng Dou,Zhao Zhang,Deqing Wang,Yikun Ban,Fuzhen Zhuang*

Main category: cs.IR

TL;DR: 本文针对科学文档检索从文档-文档检索向问题-文档检索的转变，提出UniFAR统一框架来解决粒度、语义和训练信号三方面的不匹配问题，通过自适应粒度聚合、可学习方面锚点和联合训练实现双任务统一，实验证明其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有科学文档检索方法主要基于文档中心表示学习，适用于文档-文档检索。但大语言模型和RAG的兴起使检索场景转向问题驱动的问答式检索，导致现有方法在输入粒度（长文档vs短问题）、语义焦点（科学论述结构vs特定问题意图）和训练信号（引用相似度vs问题相关性）三方面出现系统性不匹配。

Method: 提出UniFAR（统一方面感知检索框架），采用自适应多粒度聚合解决粒度差异，通过可学习方面锚点对齐文档结构与问题意图，并利用联合训练统一文档-文档和问答式检索的监督信号，在单一架构中同时支持两种检索任务。

Result: 实验结果表明，UniFAR在多个检索任务和基础模型上均持续优于先前方法，验证了该框架在提升检索性能方面的有效性和跨模型泛化能力。

Conclusion: UniFAR成功弥合了文档中心模型与问答式检索之间的鸿沟，为科学文档检索提供了一种统一的解决方案，展现了在实际应用中的实用价值和通用性。

Abstract: Existing scientific document retrieval (SDR) methods primarily rely on document-centric representations learned from inter-document relationships for document-document (doc-doc) retrieval. However, the rise of LLMs and RAG has shifted SDR toward question-driven retrieval, where documents are retrieved in response to natural-language questions (q-doc). This change has led to systematic mismatches between document-centric models and question-driven retrieval, including (1) input granularity (long documents vs. short questions), (2) semantic focus (scientific discourse structure vs. specific question intent), and (3) training signals (citation-based similarity vs. question-oriented relevance). To this end, we propose UniFAR, a Unified Facet-Aware Retrieval framework to jointly support doc-doc and q-doc SDR within a single architecture. UniFAR reconciles granularity differences through adaptive multi-granularity aggregation, aligns document structure with question intent via learnable facet anchors, and unifies doc-doc and q-doc supervision through joint training. Experimental results show that UniFAR consistently outperforms prior methods across multiple retrieval tasks and base models, confirming its effectiveness and generality.

</details>


### [43] [HotelQuEST: Balancing Quality and Efficiency in Agentic Search](https://arxiv.org/abs/2602.23949)
*Guy Hadad,Shadi Iskander,Oren Kalinsky,Sofia Tolmach,Ran Levy,Haggai Roitman*

Main category: cs.IR

TL;DR: 本文针对智能体搜索基准忽视效率与未明确偏好的问题，提出HotelQuEST基准（214个酒店查询），揭示大模型智能体虽准确率更高，但因冗余工具调用和次优路由导致成本激增，指出成本感知优化潜力巨大。


<details>
  <summary>Details</summary>
Motivation: 现有智能体搜索基准过度关注质量而忽略实际部署关键的效率因素，且对真实用户查询中普遍存在的未明确偏好问题缺乏探索，致使许多高性能系统在现实中难以应用。

Method: 构建HotelQuEST基准，包含214个从简单事实到复杂查询的酒店搜索任务，覆盖全难度谱系；通过收集澄清信息将标注者隐式偏好显式化，以评估智能体处理未明确偏好的能力。

Result: 实验表明，LLM智能体准确率优于传统检索器，但成本显著更高，主要源于冗余工具调用和未能按查询复杂度匹配模型能力的次优路由策略。

Conclusion: 分析揭示了当前智能体搜索系统的效率瓶颈，证明了通过成本感知优化提升实用性的巨大改进空间。

Abstract: Agentic search has emerged as a promising paradigm for adaptive retrieval systems powered by large language models (LLMs). However, existing benchmarks primarily focus on quality, overlooking efficiency factors that are critical for real-world deployment. Moreover, real-world user queries often contain underspecified preferences, a challenge that remains largely underexplored in current agentic search evaluation. As a result, many agentic search systems remain impractical despite their impressive performance. In this work, we introduce HotelQuEST, a benchmark comprising 214 hotel search queries that range from simple factual requests to complex queries, enabling evaluation across the full spectrum of query difficulty. We further address the challenge of evaluating underspecified user preferences by collecting clarifications that make annotators' implicit preferences explicit for evaluation. We find that LLM-based agents achieve higher accuracy than traditional retrievers, but at substantially higher costs due to redundant tool calls and suboptimal routing that fails to match query complexity to model capability. Our analysis exposes inefficiencies in current agentic search systems and demonstrates substantial potential for cost-aware optimization.

</details>


### [44] [RAD-DPO: Robust Adaptive Denoising Direct Preference Optimization for Generative Retrieval in E-commerce](https://arxiv.org/abs/2602.23964)
*Zhiguo Chen,Guohao Sun,Yiming Qiu,Xingzhi Yao,Mingming Li,Huimu Wang,Yangqi Zhang,Songlin Wang,Sulong Xu*

Main category: cs.IR

TL;DR: 针对生成式检索在电商搜索中的偏好对齐难题，本文提出RAD-DPO方法，通过token级梯度截断保护语义ID前缀结构、动态奖励加权抑制噪声伪负样本、以及多标签全局对比学习缓解概率挤压效应，显著提升了排序质量与训练效率。


<details>
  <summary>Details</summary>
Motivation: 生成式检索通过自回归解码语义ID已成为电商搜索的重要范式，但难以有效对齐复杂用户偏好。直接应用直接偏好优化(DPO)存在三方面局限：一是会惩罚共享的层次化前缀，引发梯度冲突；二是对隐式反馈中的噪声伪负样本敏感；三是在多标签查询场景下，会加剧有效候选项目间的概率"挤压效应"。

Method: 为解决上述问题，提出RAD-DPO框架，包含三项关键技术：1) token级梯度分离机制，在反向传播时保护正确的前缀结构；2) 基于相似度的动态奖励加权策略，降低噪声样本的影响；3) 融合全局监督微调损失的多标签全局对比学习目标，显式扩大正样本覆盖范围。

Result: 在大型电商平台的广泛离线实验和在线A/B测试表明，RAD-DPO在排序质量和训练效率方面均取得显著提升。

Conclusion: RAD-DPO为生成式检索的偏好对齐问题提供了有效解决方案，具有重要的实际应用价值。

Abstract: Generative Retrieval (GR) has emerged as a powerful paradigm in e-commerce search, retrieving items via autoregressive decoding of Semantic IDs (SIDs). However, aligning GR with complex user preferences remains challenging. While Direct Preference Optimization (DPO) offers an efficient alignment solution, its direct application to structured SIDs suffers from three limitations: (i) it penalizes shared hierarchical prefixes, causing gradient conflicts; (ii) it is vulnerable to noisy pseudo-negatives from implicit feedback; and (iii) in multi-label queries with multiple relevant items, it exacerbates a probability "squeezing effect" among valid candidates. To address these issues, we propose RAD-DPO, which introduces token-level gradient detachment to protect prefix structures, similarity-based dynamic reward weighting to mitigate label noise, and a multi-label global contrastive objective integrated with global SFT loss to explicitly expand positive coverage. Extensive offline experiments and online A/B testing on a large-scale e-commerce platform demonstrate significant improvements in ranking quality and training efficiency.

</details>


### [45] [Towards Efficient and Generalizable Retrieval: Adaptive Semantic Quantization and Residual Knowledge Transfer](https://arxiv.org/abs/2602.23978)
*Huimu Wang,Xingzhi Yao,Yiming Qiu,Qinghong Zhang,Haotian Wang,Yufan Cui,Songlin Wang,Sulong Xu,Mingming Li*

Main category: cs.IR

TL;DR: 本文提出 SA^2CRQ 框架解决语义 ID 生成式检索中头部项目 ID 碰撞与尾部项目泛化能力不足的问题。通过路径熵动态分配码长，并利用头部项目学习的语义流形正则化尾部项目表示，在工业级搜索系统和公开数据集上均取得显著提升，尤其在冷启动检索场景。


<details>
  <summary>Details</summary>
Motivation: 语义 ID 生成式检索在工业应用中存在固有矛盾：头部项目因数据量大易出现 ID 碰撞，损害下游任务性能；而数据稀疏的尾部项目（含冷启动项目）泛化能力有限，导致表示学习不充分，影响整体检索效果。

Method: 提出 SA^2CRQ 双组件框架：1) 顺序自适应残差量化 (SARQ) 根据项目路径熵动态分配码长，头部项目获得更长判别性 ID，尾部项目获得更短泛化性 ID；2) 锚定课程残差量化 (ACRQ) 利用头部项目预训练的冻结语义流形，正则化并加速尾部项目的表示学习，缓解数据稀疏性。

Result: 在大型工业搜索系统和多个公开数据集上的实验表明，SA^2CRQ 相比现有基线方法获得一致性能提升，尤其在冷启动检索场景下改善显著，验证了框架的有效性。

Conclusion: SA^2CRQ 通过结合自适应量化与课程学习策略，有效平衡了头部项目判别性与尾部项目泛化性的矛盾，为工业级检索系统提供了可扩展的解决方案，对冷启动问题具有重要实践价值。

Abstract: While semantic ID-based generative retrieval enables efficient end-to-end modeling in industrial applications, these methods face a persistent trade-off: head items are susceptible to ID collisions that negatively impact downstream tasks, whereas data-sparse tail items, including cold-start items, exhibit limited generalization. To address this issue, we propose the Anchored Curriculum with Sequential Adaptive Quantization (SA^2CRQ) framework. The framework introduces Sequential Adaptive Residual Quantization (SARQ) to dynamically allocate code lengths based on item path entropy, assigning longer, discriminative IDs to head items and shorter, generalizable IDs to tail items. To mitigate data sparsity, the Anchored Curriculum Residual Quantization (ACRQ) component utilizes a frozen semantic manifold learned from head items to regularize and accelerate the representation learning of tail items. Experimental results from a large-scale industrial search system and multiple public datasets indicate that SA^2CRQ yields consistent improvements over existing baselines, particularly in cold-start retrieval scenarios.

</details>


### [46] [Robust Aggregation for Federated Sequential Recommendation with Sparse and Poisoned Data](https://arxiv.org/abs/2602.23982)
*Minh Hieu Nguyen*

Main category: cs.IR

TL;DR: 针对联邦学习中的序列推荐系统，本文提出了一种鲁棒聚合框架，通过防御感知聚合机制、表示层约束和序列感知正则化，同时解决数据稀疏性和对抗性攻击问题。


<details>
  <summary>Details</summary>
Motivation: 联邦序列推荐虽能通过分布式训练保护用户隐私，但面临两大挑战：一是客户端交互序列短且高度稀疏，导致用户表示学习不可靠；二是联邦优化过程易受恶意或损坏的客户端更新影响，中毒梯度会严重扭曲全局模型，且时序动态性使信号聚合更加复杂。

Method: 提出一种鲁棒聚合框架，包含三个核心机制：1）防御感知聚合机制，识别并降低不可靠客户端更新的权重，同时保留稀疏但良性参与者的信息；2）表示层约束，稳定用户和物品嵌入，防止中毒贡献主导全局参数空间；3）序列感知正则化，在有限本地观测下保持用户建模的时序一致性。

Result: 摘要未提供具体实验结果，但该框架旨在提升联邦序列推荐在稀疏和对抗条件下的鲁棒性。

Conclusion: 该研究为联邦序列推荐提供了一种全面的鲁棒性解决方案，有望构建更可靠、更隐私保护的序列推荐系统。

Abstract: Federated sequential recommendation distributes model training across user devices so that behavioural data remains local, reducing privacy risks. Yet, this setting introduces two intertwined difficulties. On the one hand, individual clients typically contribute only short and highly sparse interaction sequences, limiting the reliability of learned user representations. On the other hand, the federated optimisation process is vulnerable to malicious or corrupted client updates, where poisoned gradients can significantly distort the global model. These challenges are particularly severe in sequential recommendation, where temporal dynamics further complicate signal aggregation. To address this problem, we propose a robust aggregation framework tailored for federated sequential recommendation under sparse and adversarial conditions. Instead of relying on standard averaging, our method introduces a defence-aware aggregation mechanism that identifies and down-weights unreliable client updates while preserving informative signals from sparse but benign participants. The framework incorporates representation-level constraints to stabilise user and item embeddings, preventing poisoned or anomalous contributions from dominating the global parameter space. In addition, we integrate sequence-aware regularisation to maintain temporal coherence in user modelling despite limited local observations.

</details>


### [47] [Colour Contrast on the Web: A WCAG 2.1 Level AA Compliance Audit of Common Crawl's Top 500 Domains](https://arxiv.org/abs/2602.24067)
*Thom Vaughan,Pedro Ortiz Suarez*

Main category: cs.IR

TL;DR: 本研究对2026年2月Common Crawl抓取的500个最常见域名进行了WCAG 2.1/2.2 AA级颜色对比度自动化审计。通过对240个首页的静态CSS分析，发现40.9%的颜色组合未达到正常文本4.5:1的对比度要求，中位通过率为62.7%，仅20.4%的网站完全合规，表明颜色对比度仍是网站可访问性的主要障碍。


<details>
  <summary>Details</summary>
Motivation: 颜色对比度不足是网站可访问性的重要障碍，影响视障用户的访问体验。尽管WCAG标准已存在多年，但实际合规情况缺乏大规模数据支撑。本研究旨在通过可重复的自动化方法，系统评估顶尖网站的颜色对比度合规现状，揭示问题的普遍性和严重程度。

Method: 研究采用静态分析方法，从Common Crawl的CC-MAIN-2026-08公开WARC档案中提取500个最常见域名的页面内容，避免了对目标服务器的负载并确保结果可重复。对其中240个首页的CSS进行解析，提取4,327个独特的前景色/背景色配对，并自动化评估其是否符合WCAG 2.1/2.2 Level AA的4.5:1对比度阈值。

Result: 分析显示，40.9%（1,771个）的颜色配对未达到正常文本所需的4.5:1对比度阈值。网站间表现差异显著，中位通过率为62.7%，但仅20.4%的网站在所有检测到的颜色配对上实现完全合规。结果表明颜色对比度问题在主流网站中仍然普遍存在。

Conclusion: 颜色对比度合规性仍然是绝大多数网站的可访问性障碍，即使在流量最高的网站中也远未得到充分解决。研究结果凸显了持续改进网站设计和加强可访问性标准执行的必要性，不同域名类别间存在的显著差异也表明需要针对性的优化策略。

Abstract: We present a large-scale automated audit of WCAG 2.1/2.2 Level AA colour contrast compliance across the 500 most frequently crawled registered domains in Common Crawl's CC-MAIN-2026-08 February 2026 crawl archive. Rather than conducting a live crawl, all page content was sourced from Common Crawl's open WARC archives, ensuring reproducibility and eliminating any load on target web servers. Our static CSS analysis of 240 homepages identified 4,327 unique foreground/background colour pairings, of which 1,771 (40.9%) failed to meet the 4.5:1 contrast ratio threshold for normal text. The median per-site pass rate was 62.7%, with 20.4% of sites achieving full compliance across all detected colour pairings. These findings suggest that colour contrast remains a widespread accessibility barrier on the most prominent websites, with significant variation across domain categories.

</details>


### [48] [Recommendation Algorithms: A Comparative Study in Movie Domain](https://arxiv.org/abs/2602.24125)
*Rohit Chivukula,T. Jaya Lakshmi,Hemlata Sharma,C. H. S. N. P. Sairam Rallabandi*

Main category: cs.IR

TL;DR: 本文将电影推荐问题视为回归任务，基于Netflix数据集提取多种特征，对比XGBoost、KNN和MF等算法，发现基于矩阵分解的方法在RMSE指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 推荐系统显著提升电商收入，Netflix作为热门流媒体平台，其电影推荐系统具有重要研究价值。本文旨在通过提取新颖特征并对比多种回归算法，提升推荐准确性。

Method: 本文将推荐问题建模为回归任务。首先对Netflix数据集进行探索性数据分析，随后提取聚合特征、基于矩阵分解的特征以及用户和电影相似度特征。采用XGBoost、K最近邻和矩阵分解算法（来自Python Surprise库）进行推荐预测，并使用均方根误差（RMSE）评估模型性能。

Result: 实验结果表明，基于矩阵分解（MF）的算法在均方根误差（RMSE）指标上表现最优，能够提供最佳推荐效果。

Conclusion: 本研究证实，在电影推荐回归任务中，基于矩阵分解的方法相比XGBoost和K最近邻算法具有更优越的性能，为提升推荐系统准确性提供了有效途径。

Abstract: Intelligent recommendation systems have clearly increased the revenue of well-known e-commerce firms. Users receive product recommendations from recommendation systems. Cinematic recommendations are made to users by a movie recommendation system. There have been numerous approaches to the problem of recommendation in the literature. It is viewed as a regression task in this research. A regression model was built using novel properties extracted from the dataset and used as features in the model. For experimentation, the Netflix challenge dataset has been used. Video streaming service Netflix is a popular choice for many. Customers' prior viewing habits are taken into account when Netflix makes movie recommendations to them. An exploratory data analysis on the Netflix dataset was conducted to gain insights into user rating behaviour and movie characteristics. Various kinds of features, including aggregating, Matrix Factorization (MF) based, and user and movie similarity based, have been extracted in the subsequent stages. In addition to a feature in the XGBoost regression algorithm, the K-Nearest Neighbors and MF algorithms from Python's Surprise library are used for recommendations. Based on Root Mean Square Error (RMSE), MF-based algorithms have provided the best recommendations.

</details>


### [49] [Beyond the Click: A Framework for Inferring Cognitive Traces in Search](https://arxiv.org/abs/2602.24265)
*Saber Zerhoudi,Michael Granitzer*

Main category: cs.IR

TL;DR: 该论文提出了一个从用户行为日志中推断认知轨迹的框架，利用基于信息觅食理论的多智能体系统和专家判断，显著提升用户模拟器在预测会话结果和用户困难恢复任务上的性能，并开源了AOL和Stack Overflow等数据集的标注集及工具。


<details>
  <summary>Details</summary>
Motivation: 现有用户模拟器仅能复制用户行为，缺乏对潜在认知过程（如困惑或满意）的理解，而大规模交互日志虽记录行为却无法反映用户思维感受，导致模拟器与人机交互真实场景存在差距。

Method: 提出基于信息觅食理论（IFT）和人类专家判断的多智能体系统框架，从行为日志中推断认知轨迹。通过智能体模拟用户认知过程，结合专家知识标注，构建能够反映用户思维状态的行为表征。

Result: 认知轨迹显著提升了模型在预测会话结果和用户困难恢复任务上的性能。同时发布了AOL和Stack Overflow等公开数据集的标注集，以及可供研究者应用于自有数据的开源工具。

Conclusion: 该工作提供了构建更类人用户模拟器所需的工具和评估数据，使检索系统能够在以用户为导向的性能维度上进行评估，弥补了行为数据与认知状态之间的鸿沟。

Abstract: User simulators are essential for evaluating search systems, but they primarily copy user actions without understanding the underlying thought process. This gap exists since large-scale interaction logs record what users do, but not what they might be thinking or feeling, such as confusion or satisfaction. To solve this problem, we present a framework to infer cognitive traces from behavior logs. Our method uses a multi-agent system grounded in Information Foraging Theory (IFT) and human expert judgment. These traces improve model performance on tasks like forecasting session outcomes and user struggle recovery. We release a collection of annotations for several public datasets, including AOL and Stack Overflow, and an open-source tool that allows researchers to apply our method to their own data. This work provides the tools and data needed to build more human-like user simulators and to assess retrieval systems on user-oriented dimensions of performance.

</details>


### [50] [Resources for Automated Evaluation of Assistive RAG Systems that Help Readers with News Trustworthiness Assessment](https://arxiv.org/abs/2602.24277)
*Dake Zhang,Mark D. Smucker,Charles L. A. Clarke*

Main category: cs.IR

TL;DR: TREC 2025 DRAGUN任务开发了可重用的资源（两个任务、评估量表和自动评判系统），用于评估帮助读者判断新闻可信度的RAG系统，结果显示自动评判与人类评估具有较强相关性（τ>0.67）。


<details>
  <summary>Details</summary>
Motivation: 面对在线新闻中可靠报道与虚假信息混杂的挑战，本文通过组织TREC 2025 DRAGUN任务，旨在开发和评估能够生成面向读者且充分引证的辅助性RAG系统，以帮助读者准确评估新闻可信度。

Method: 本文作为DRAGUN任务的组织者，设计了两个核心任务：任务1（问题生成）要求生成10个排序的调查性问题；任务2（报告生成）基于MS MARCO V2.1语料库生成250字可信度评估报告。通过TREC评估员为30篇新闻文章构建了重要性加权评估量表，并开发了AutoJudge自动化评判流程进行结果评估。

Result: 自动评判系统AutoJudge的排序结果与人类评估高度一致，其中任务1的Kendall's τ系数为0.678，任务2达到0.872。成功创建了可复用的任务框架、评估基准和自动化评估工具，为RAG系统评估研究提供了高质量资源。

Conclusion: 该研究不仅为辅助新闻可信度评估的RAG系统提供了标准化评估方案，更以人工评估为金标准，推动了自动化RAG评估方法的研究，对提升新闻可信度评估技术的可扩展性和效率具有重要意义。

Abstract: Many readers today struggle to assess the trustworthiness of online news because reliable reporting coexists with misinformation. The TREC 2025 DRAGUN (Detection, Retrieval, and Augmented Generation for Understanding News) Track provided a venue for researchers to develop and evaluate assistive RAG systems that support readers' news trustworthiness assessment by producing reader-oriented, well-attributed reports. As the organizers of the DRAGUN track, we describe the resources that we have newly developed to allow for the reuse of the track's tasks. The track had two tasks: (Task 1) Question Generation, producing 10 ranked investigative questions; and (Task 2, the main task) Report Generation, producing a 250-word report grounded in the MS MARCO V2.1 Segmented Corpus. As part of the track's evaluation, we had TREC assessors create importance-weighted rubrics of questions with expected short answers for 30 different news articles. These rubrics represent the information that assessors believe is important for readers to assess an article's trustworthiness. The assessors then used their rubrics to manually judge the participating teams' submitted runs. To make these tasks and their rubrics reusable, we have created an automated process to judge runs not part of the original assessing. We show that our AutoJudge ranks existing runs well compared to the TREC human-assessed evaluation (Kendall's $τ= 0.678$ for Task 1 and $τ= 0.872$ for Task 2). These resources enable both the evaluation of RAG systems for assistive news trustworthiness assessment and, with the human evaluation as a benchmark, research on improving automated RAG evaluation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [HumanMCP: A Human-Like Query Dataset for Evaluating MCP Tool Retrieval Performance](https://arxiv.org/abs/2602.23367)
*Shubh Laddha,Lucas Changbencharoen,Win Kuptivej,Surya Shringla,Archana Vaidheeswaran,Yash Bhaskar*

Main category: cs.AI

TL;DR: 针对MCP服务器工具评估缺乏真实用户查询的问题，本文基于MCP Zero数据集，首次构建了覆盖308个服务器、2800个工具的大规模数据集，为每个工具生成多个不同用户画像的查询，涵盖从精确任务到模糊探索的多样化意图。


<details>
  <summary>Details</summary>
Motivation: 现有MCP服务器数据集和基准测试缺乏真实、类人的用户查询，无法反映不同用户表达请求的方式，导致评估结果泛化性差且可靠性虚高，成为评价工具使用和生态系统发展的关键瓶颈。

Method: 在MCP Zero数据集基础上，为2800个工具开发多个独特的用户画像，生成从精确任务请求到模糊探索命令的多样化用户查询，模拟真实世界交互模式的复杂性。

Result: 成功构建了首个大规模MCP数据集，涵盖308个MCP服务器的2800个工具，每个工具配对多个高质量、多样化的用户查询，真实反映不同用户的意图层次。

Conclusion: 该数据集填补了MCP服务器工具评估领域缺乏真实用户查询的关键空白，为生态系统发展提供了更可靠的评估基准，有助于提升工具使用能力评价的准确性和泛化性。

Abstract: Model Context Protocol (MCP) servers contain a collection of thousands of open-source standardized tools, linking LLMs to external systems; however, existing datasets and benchmarks lack realistic, human-like user queries, remaining a critical gap in evaluating the tool usage and ecosystems of MCP servers. Existing datasets often do contain tool descriptions but fail to represent how different users portray their requests, leading to poor generalization and inflated reliability of certain benchmarks. This paper introduces the first large-scale MCP dataset featuring diverse, high-quality diverse user queries generated specifically to match 2800 tools across 308 MCP servers, developing on the MCP Zero dataset. Each tool is paired with multiple unique user personas that we have generated, to capture varying levels of user intent ranging from precise task requests, and ambiguous, exploratory commands, reflecting the complexity of real-world interaction patterns.

</details>


### [52] [Causal Identification from Counterfactual Data: Completeness and Bounding Results](https://arxiv.org/abs/2602.23541)
*Arvind Raghavan,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本研究突破因果层次第三层数据不可获得的传统假设，开发CTFIDU+算法并证明其完备性，首次系统刻画了混合观测、干预与部分反事实数据下反事实识别的理论边界，确立了非参数因果推断的基本极限，并为不可识别量提供了基于可实现反事实数据的新边界。


<details>
  <summary>Details</summary>
Motivation: 传统反事实识别完备性研究局限于因果层次的前两层（观测与干预分布），因假设第三层反事实分布无法获得。尽管Raghavan & Bareinboim (2025)提出了"反事实可实现性"概念，形式化了一类可直接估计的第三层分布，但关键问题仍未解决：在可获取部分第三层数据的前提下，哪些额外反事实量变得可识别？此问题答案将界定因果推断能力的根本边界。

Method: 提出CTFIDU+算法，用于从任意第三层反事实分布集合中识别反事实查询，并严格证明其完备性。该算法统一处理观测、干预与可实现反事实数据的混合场景。针对不可精确识别的反事实量，进一步推导出基于可实现数据的新型解析边界，并通过模拟验证其有效性。

Result: (1) CTFIDU+算法对任意可实现第三层分布下的反事实识别具有完备性；(2) 确立了从物理可实现分布中识别反事实量的理论极限，揭示了非参数设置下精确因果推断的根本限制；(3) 为不可识别的关键反事实量推导出新解析边界，模拟证实反事实数据能显著收紧这些边界。

Conclusion: 该工作通过形式化第三层数据的可获得性，完整刻画了混合数据环境下的反事实识别能力边界，将因果推断理论前沿推进至新层次。CTFIDU+算法为实践者提供完备工具，同时揭示精确推断的基本限制。对于不可识别量，利用可实现反事实数据可获得更紧边界，兼具理论深度与实践指导价值。

Abstract: Previous work establishing completeness results for $\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\textit{counterfactual realizabilty}$. This leaves open the question of what $\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice.

</details>


### [53] [Planning under Distribution Shifts with Causal POMDPs](https://arxiv.org/abs/2602.23545)
*Matteo Ceriscioli,Karthika Mohan*

Main category: cs.AI

TL;DR: 提出一种基于因果知识的部分可观测马尔可夫决策过程（POMDP）理论框架，用于在分布漂移下进行规划。将环境变化表示为对因果POMDP的干预，从而评估假设变化下的计划并主动识别环境已改变的组件。证明在增强的信念空间中价值函数保持分段线性且凸（PWLC），维持了α‑向量POMDP方法的可处理性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的规划常受到分布漂移的挑战：在一种条件下获得的模型在状态分布或环境动态发生变化时可能失效，导致先前学习的策略失败。因此需要一种能够在部分可观测且分布漂移的情况下进行鲁棒规划的框架。

Method: 1. 将因果知识引入POMDP，构建因果POMDP模型。  
2. 将环境漂移建模为该模型上的干预（intervention），从而能够在假设的变化下评估计划并主动识别哪些环境组件发生了改变。  
3. 维护并更新对隐状态和底层领域的信念（belief），并证明在增强的信念空间中价值函数仍保持分段线性且凸（PWLC）。

Result: 在增强的信念空间中，价值函数保持PWLC性质。这使得可以使用α‑向量方法进行有效的POMDP规划，从而在分布漂移下仍然保持计算的可处理性。

Conclusion: 该因果POMDP框架能够在部分可观测且存在分布漂移的环境中实现鲁棒规划。通过保持PWLC结构，保留了α‑向量方法的效率，为实现可处理的适应性决策提供了理论基础。

Abstract: In the real world, planning is often challenged by distribution shifts. As such, a model of the environment obtained under one set of conditions may no longer remain valid as the distribution of states or the environment dynamics change, which in turn causes previously learned strategies to fail. In this work, we propose a theoretical framework for planning under partial observability using Partially Observable Markov Decision Processes (POMDPs) formulated using causal knowledge. By representing shifts in the environment as interventions on this causal POMDP, the framework enables evaluating plans under hypothesized changes and actively identifying which components of the environment have been altered. We show how to maintain and update a belief over both the latent state and the underlying domain, and we prove that the value function remains piecewise linear and convex (PWLC) in this augmented belief space. Preservation of PWLC under distribution shifts has the advantage of maintaining the tractability of planning via $α$-vector-based POMDP methods.

</details>


### [54] [Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem](https://arxiv.org/abs/2602.23579)
*Guillem Rodríguez-Corominas,Maria J. Blesa,Christian Blum*

Main category: cs.AI

TL;DR: 提出一种融合强化学习的混合算法 RL-CMSA，用于对称单仓库最小化最大路径的多旅行商问题 (min-max mTSP)。算法通过 Q 值引导的概率聚类构建多样解，合并路径形成紧凑池，求解受限集合覆盖 MILP，并利用跨路径移除、移动、交换操作进行局部优化；Q 值根据高质量解中的城市共现进行增强，池通过老化和剪枝自适应更新。计算实验表明，RL-CMSA 在随机和 TSPLIB 实例上均能持续获得（近）最优解，并在可比时间限制下优于最先进的混合遗传算法，尤其在实例规模和旅行商数量增大时优势更明显。


<details>
  <summary>Details</summary>
Motivation: 多旅行商问题在实际物流配送、车辆路径规划中广泛应用，而最小化最长路径（min‑max）能有效平衡各 salesman 的工作负荷，提升系统整体效率。随着问题规模增大，传统精确算法和启发式方法在求解速度和解质量上均面临挑战，亟需一种能够在较短时间内获得高质量解的新方法。

Method: RL‑CMSA 方法包括以下步骤：1）构造阶段：利用已学习的配对 Q 值指导概率聚类，生成多样化的初始路径；2）合并阶段：将构造出的路径合并为一个紧凑的候选池；3）求解阶段：基于受限集合覆盖的混合整数线性规划（MILP）从池中选择最优路径组合；4）自适应阶段：对选出的解进行跨路径的移除、移动、交换等局部搜索，提升解质量；5）强化学习更新：根据高质量解中城市对的共现频率更新 Q 值，强化优秀配对的权重；6）池管理：采用老化和剪枝策略动态维护池的多样性与质量，防止早熟收敛。该方法将精确优化与强化学习引导的构造相结合，平衡探索与利用。

Result: 在随机生成和 TSPLIB 标准测试集上的实验结果表明，RL‑CMSA 能够在规定时间内持续找到（近）最优解，且在所有对比实例中均优于或与最先进的混合遗传算法相当。尤其在实例规模更大、旅行商数量更多时，RL‑CMSA 的性能优势更加显著，体现出良好的可扩展性和鲁棒性。

Conclusion: RL‑CMSA 通过强化学习引导的构造与精确优化相结合，为最小化最大路径的多旅行商问题提供了一种高效且可扩展的求解框架。实验验证其在解质量、收敛速度及大规模实例处理能力方面均优于现有先进算法，具备在实际物流与路径规划系统中应用的潜力。

Abstract: The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase.

</details>


### [55] [SleepLM: Natural-Language Intelligence for Human Sleep](https://arxiv.org/abs/2602.23605)
*Zongzhe Xu,Zitao Shuai,Eideen Mozaffari,Ravi S. Aysola,Rajesh Kumar,Yuzhe Yang*

Main category: cs.AI

TL;DR: 提出SleepLM睡眠语言基础模型家族，通过自然语言实现睡眠对齐、解释与交互。解决现有系统局限于封闭标签空间、无法泛化至新睡眠现象的问题。构建首个大规模睡眠文本数据集（10万+小时，1万+个体），采用对比对齐、字幕生成与信号重建的统一预训练目标。实验证明其在零/少样本学习、跨模态检索和睡眠字幕生成上超越现有最先进技术，并具备语言引导事件定位、洞察生成和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的睡眠分析系统只能在封闭标签空间（如预定义睡眠阶段或事件）中运行，无法用自然语言描述、查询或泛化到新的睡眠现象，严重限制了睡眠生理学的深入理解和临床应用。因此，迫切需要一种能够桥接自然语言与多导睡眠图数据的基础模型，以实现更灵活、可解释和个性化的睡眠分析。

Method: 1. 提出SleepLM模型家族，桥接自然语言与多模态多导睡眠图数据，构建语言接地的睡眠生理表征。2. 设计多级睡眠字幕生成流水线，创建首个大规模睡眠-文本配对数据集，涵盖超10,000名个体的100,000多小时睡眠记录。3. 采用统一预训练目标，联合优化对比对齐、字幕生成和信号重建三个任务，以增强生理保真度与跨模态交互能力。

Result: 在真实世界睡眠理解任务的广泛实验中，SleepLM在零样本和少样本学习、跨模态检索、睡眠字幕生成等关键指标上均显著超越现有最优方法。此外，模型展现出语言引导的事件定位、针对性洞察生成以及对未见任务的零样本泛化等高级能力，验证了统一预训练目标的有效性。所有代码和数据将开源以促进后续研究。

Conclusion: SleepLM首次将基础模型引入睡眠领域，成功实现自然语言与睡眠生理学的对齐，为睡眠研究提供了全新的交互范式。通过开源，该工作将促进睡眠医学的民主化和个性化发展，未来有望应用于临床诊断、健康监测和科研教育等多个场景，推动睡眠健康领域的智能化转型。

Abstract: We present SleepLM, a family of sleep-language foundation models that enable human sleep alignment, interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to describe, query, or generalize to novel sleep phenomena. SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines contrastive alignment, caption generation, and signal reconstruction to better capture physiological fidelity and cross-modal interactions. Extensive experiments on real-world sleep understanding tasks verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including language-guided event localization, targeted insight generation, and zero-shot generalization to unseen tasks. All code and data will be open-sourced.

</details>


### [56] [MMKG-RDS: Reasoning Data Synthesis via Deep Mining of Multimodal Knowledge Graphs](https://arxiv.org/abs/2602.23632)
*Lun Zhan,Feng Xiong,Huanyong Liu,Feng Zhang,Yuhui Yin*

Main category: cs.AI

TL;DR: 本文提出基于多模态知识图谱的推理数据合成框架MMKG-RDS，通过细粒度知识提取、可定制路径采样和多维度质量评分解决现有方法在长尾知识覆盖、可解释性等方面的局限。在涵盖五个领域14,950个样本的基准测试上，仅用少量合成数据微调Qwen3模型即实现9.2%的推理准确率提升，生成的数据对表格和公式任务具有挑战性，为复杂基准测试构建提供新方案。


<details>
  <summary>Details</summary>
Motivation: 现有高质量训练数据合成方法在长尾知识覆盖、效果验证和可解释性方面存在明显不足。传统知识图谱方法在功能性、粒度精细度、可定制性和评估体系方面仍有欠缺，难以满足领域模型推理能力增强的实际需求。

Method: 提出MMKG-RDS框架，采用多模态知识图谱作为核心，实现细粒度知识提取、用户可定制的路径采样策略，以及多维度的数据质量评分机制，提供灵活可扩展的推理数据合成方案。

Result: 构建MMKG-RDS-Bench基准数据集，覆盖五个领域、17种任务类型，共14,950个样本。实验表明，使用少量该框架合成的数据微调Qwen3系列模型（0.6B/8B/32B参数）可使推理准确率显著提升9.2%。此外，生成的数据在涉及表格和公式的任务上表现出独特性，对现有模型构成挑战，适用于构建更复杂的推理基准测试。

Conclusion: MMKG-RDS框架通过多模态知识图谱有效解决了高质量推理数据合成面临的多重挑战，实现了细粒度、可定制化的数据生成与评估。该方法为提升领域模型推理能力提供了有效途径，其生成的挑战性数据对推动复杂推理基准测试的发展具有重要价值。

Abstract: Synthesizing high-quality training data is crucial for enhancing domain models' reasoning abilities. Existing methods face limitations in long-tail knowledge coverage, effectiveness verification, and interpretability. Knowledge-graph-based approaches still fall short in functionality, granularity, customizability, and evaluation. To address these issues, we propose MMKG-RDS, a flexible framework for reasoning data synthesis that leverages multimodal knowledge graphs. It supports fine-grained knowledge extraction, customizable path sampling, and multidimensional data quality scoring. We validate MMKG-RDS with the MMKG-RDS-Bench dataset, covering five domains, 17 task types, and 14,950 samples. Experimental results show fine-tuning Qwen3 models (0.6B/8B/32B) on a small number of synthesized samples improves reasoning accuracy by 9.2%. The framework also generates distinct data, challenging existing models on tasks involving tables and formulas, useful for complex benchmark construction. The dataset and code are available at https://github.com/360AILAB-NLP/MMKG-RDS

</details>


### [57] [AI Must Embrace Specialization via Superhuman Adaptable Intelligence](https://arxiv.org/abs/2602.23643)
*Judah Goldfeder,Philippe Wyder,Yann LeCun,Ravid Shwartz Ziv*

Main category: cs.AI

TL;DR: 本文批判性审视了通用人工智能（AGI）概念，认为尽管各界对其定义缺乏共识，但即便在最清晰的表述下，AGI仍是一个描述AI未来的有缺陷框架。论文论证了人类并非真正"通用"，主张AI应拥抱专业化而非追求通用性，并提出"超人类适应智能（SAI）"作为替代概念——指能在人类能做的重要事情上超越人类，并填补人类能力空白的智能。SAI旨在为被AGI过载定义模糊化的AI讨论提供更清晰的指导框架。


<details>
  <summary>Details</summary>
Motivation: 当前从AI从业者到政策制定者、活动家都在讨论AGI，但对其定义缺乏共识。核心问题在于：以"能做人类所能做的一切"来定义AGI是否合理？人类是否真的具有"通用性"？这种模糊且可能错误的概念框架会误导对AI未来的讨论和发展方向。

Method: 通过批判性分析现有主流AGI定义的合理性、实用性和真正通用性，指出其概念缺陷。进而提出应放弃对通用性的追求，转向专业化路径，并引入"超人类适应智能（SAI）"作为新的概念框架。SAI定义为能够学习并在人类能做的任何重要事情上超越人类，同时填补人类能力空白的智能形式。

Result: 论证了AGI概念的根本缺陷，成功提出了SAI作为替代性概念框架。SAI强调在专业化基础上追求超人类表现，而非人类水平的通用性。这一新概念能够澄清被AGI过载定义所模糊的AI讨论，为未来AI发展提供更清晰的指导方向。

Conclusion: AGI是一个有缺陷的概念，不应作为AI未来的描述框架。AI发展应拥抱专业化并在专业化中追求超人类表现。SAI提供了更精确的概念工具，有助于推动AI领域的理性讨论和未来发展方向规划。这一概念转换对AI研究和政策制定具有重要指导意义。

Abstract: Everyone from AI executives and researchers to doomsayers, politicians, and activists is talking about Artificial General Intelligence (AGI). Yet, they often don't seem to agree on its exact definition. One common definition of AGI is an AI that can do everything a human can do, but are humans truly general? In this paper, we address what's wrong with our conception of AGI, and why, even in its most coherent formulation, it is a flawed concept to describe the future of AI. We explore whether the most widely accepted definitions are plausible, useful, and truly general. We argue that AI must embrace specialization, rather than strive for generality, and in its specialization strive for superhuman performance, and introduce Superhuman Adaptable Intelligence (SAI). SAI is defined as intelligence that can learn to exceed humans at anything important that we can do, and that can fill in the skill gaps where humans are incapable. We then lay out how SAI can help hone a discussion around AI that was blurred by an overloaded definition of AGI, and extrapolate the implications of using it as a guide for the future.

</details>


### [58] [ODAR: Principled Adaptive Routing for LLM Reasoning via Active Inference](https://arxiv.org/abs/2602.23681)
*Siyuan Ma,Bo Gao,Xiaojun Jia,Simeng Qin,Tianlin Li,Ke Ma,Xiaoshuang Jia,Wenqi Ren,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出ODAR-Expert自适应路由框架，通过可原则性资源分配优化准确率-效率权衡。该方法利用摊销主动推理难度估计器动态调度查询至启发式快智能体和深思式慢智能体，并引入基于自由能原理的风险敏感融合机制，通过最小化变分自由能目标（平衡对数似然与认知不确定性）选择答案，而非启发式投票。在23个基准测试中表现优异（MATH达98.2%，HLE达54.8%），在匹配计算量下提升计算-准确率前沿，并在开源栈上降低82%计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理虽从参数扩展转向测试时计算扩展，但多数方法仍依赖统一的暴力采样策略（如固定best-of-N或自洽性），存在成本高、可归因性差、易引发收益递减的过度思考等问题。因此需要一种原则性的资源分配机制来优化准确率与效率的权衡，而非单纯增加测试时计算量。

Method: 提出ODAR-Expert框架：1）基于摊销主动推理的难度估计器，动态路由查询至启发式Fast Agent和深思式Slow Agent；2）引入自由能原理的风险敏感融合机制，通过最小化变分自由能目标函数，将对数似然与认知不确定性（varentropy）相结合，作为异构候选答案的决策原则，替代临时性的投票策略。

Result: 在23个基准测试中实现强大且一致的增益：MATH准确率98.2%，Humanity's Last Exam（HLE）达54.8%；在计算量匹配设置下改善了计算-准确率前沿；在完全开源栈（Llama 4 + DeepSeek）上复现并超越同质采样策略，同时降低82%计算成本。

Conclusion: 研究结果表明，最优思考扩展需要基于自由能的适应性资源分配与决策机制，而非简单地增加测试时计算量。该框架为大规模语言模型推理提供了一种高效且可解释的范式转变。

Abstract: The paradigm of large language model (LLM) reasoning is shifting from parameter scaling to test-time compute scaling, yet many existing approaches still rely on uniform brute-force sampling (for example, fixed best-of-N or self-consistency) that is costly, hard to attribute, and can trigger overthinking with diminishing returns. We propose ODAR-Expert, an adaptive routing framework that optimizes the accuracy-efficiency trade-off via principled resource allocation. ODAR uses a difficulty estimator grounded in amortized active inference to dynamically route queries between a heuristic Fast Agent and a deliberative Slow Agent. We further introduce a free-energy-principled, risk-sensitive fusion mechanism that selects answers by minimizing a variational free energy objective, balancing log-likelihood with epistemic uncertainty (varentropy) as a principled alternative to ad hoc voting over heterogeneous candidates. Extensive evaluation across 23 benchmarks shows strong and consistent gains, including 98.2% accuracy on MATH and 54.8% on Humanity's Last Exam (HLE), while improving the compute-accuracy frontier under compute-matched settings. We also validate reproducibility on a fully open-source stack (Llama 4 + DeepSeek), where ODAR surpasses homogeneous sampling strategies while reducing computational costs by 82%. Overall, our results suggest that thinking-optimal scaling requires adaptive resource allocation with free-energy-based decision-making rather than simply increasing test-time compute.

</details>


### [59] [From Flat Logs to Causal Graphs: Hierarchical Failure Attribution for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.23701)
*Yawen Wang,Wenjie Wu,Junjie Wang,Qing Wang*

Main category: cs.AI

TL;DR: 针对LLM驱动的多智能体系统故障归因问题，本文提出CHIEF框架，通过构建层次化因果图、分层回溯和因果筛选，有效识别故障根本原因，在Who&When基准测试中超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统在复杂任务中表现出色但存在脆弱性和不透明的故障机制。现有归因方法将执行日志视为线性序列，无法解析系统内部的复杂因果关系，导致可观测性弱、责任边界模糊。

Method: CHIEF框架包含三层：1) 将混乱轨迹转化为结构化层次因果图；2) 采用分层神谕引导回溯，通过虚拟神谕高效剪枝搜索空间；3) 通过渐进式因果筛选实现反事实归因，区分根本原因与衍生症状。

Result: 在Who&When基准测试上，CHIEF在智能体和步骤级别准确率上均优于八种先进基线方法。消融实验验证了各模块的关键作用。

Conclusion: CHIEF通过层次因果图建模有效解决了MAS故障归因难题，提供了更强的可观测性和明确的责任边界，为提升系统鲁棒性提供了新思路。

Abstract: LLM-powered Multi-Agent Systems (MAS) have demonstrated remarkable capabilities in complex domains but suffer from inherent fragility and opaque failure mechanisms. Existing failure attribution methods, whether relying on direct prompting, costly replays, or supervised fine-tuning, typically treat execution logs as flat sequences. This linear perspective fails to disentangle the intricate causal links inherent to MAS, leading to weak observability and ambiguous responsibility boundaries. To address these challenges, we propose CHIEF, a novel framework that transforms chaotic trajectories into a structured hierarchical causal graph. It then employs hierarchical oracle-guided backtracking to efficiently prune the search space via sybthesized virtual oracles. Finally, it implements counterfactual attribution via a progressive causal screening strategy to rigorously distinguish true root causes from propagated symptoms. Experiments on Who&When benchmark show that CHIEF outperforms eight strong and state-of-the-art baselines on both agent- and step-level accuracy. Ablation studies further confirm the critical role of each proposed module.

</details>


### [60] [ProductResearch: Training E-Commerce Deep Research Agents via Multi-Agent Synthetic Trajectory Distillation](https://arxiv.org/abs/2602.23716)
*Jiangyuan Wang,Kejun Xiao,Huaipeng Zhao,Tao Luo,Xiaoyi Zeng*

Main category: cs.AI

TL;DR: 本文提出ProductResearch多智能体框架，通过合成高质量长时程工具使用轨迹并经过反思内化过程，训练电商购物智能体，使紧凑MoE模型在响应全面性、研究深度和用户感知效用方面显著提升，性能接近前沿专业深度研究系统。


<details>
  <summary>Details</summary>
Motivation: 现有LLM电商购物智能体缺乏复杂产品研究所需的交互深度与上下文广度，而Deep Research范式直接迁移至电商领域存在领域鸿沟，亟需构建更适配电商场景的智能体训练方案。

Method: 提出ProductResearch多智能体框架：用户智能体从行为历史推断细粒度购物意图；主管智能体协调研究智能体进行迭代协作，生成合成轨迹并产出产品研究报告；通过反思内化流程将多智能体交互提炼为连贯的单角色训练样本。

Result: 基于合成轨迹微调的紧凑MoE模型在多项指标上显著超越基线模型，其综合性能接近前沿商业深度研究系统，验证了合成轨迹训练的有效性。

Conclusion: 多智能体合成轨迹训练范式被确立为增强LLM购物辅助能力的有效且可扩展方案，为电商智能体训练提供了新思路。

Abstract: Large Language Model (LLM)-based agents show promise for e-commerce conversational shopping, yet existing implementations lack the interaction depth and contextual breadth required for complex product research. Meanwhile, the Deep Research paradigm, despite advancing information synthesis in web search, suffers from domain gaps when transferred to e-commerce. We propose ProductResearch, a multi-agent framework that synthesizes high-fidelity, long-horizon tool-use trajectories for training robust e-commerce shopping agents. The framework employs a User Agent to infer nuanced shopping intents from behavioral histories, and a Supervisor Agent that orchestrates iterative collaboration with a Research Agent to generate synthetic trajectories culminating in comprehensive, insightful product research reports. These trajectories are rigorously filtered and distilled through a reflective internalization process that consolidates multi-agent supervisory interactions into coherent single-role training examples, enabling effective fine-tuning of LLM agents for complex shopping inquiries. Extensive experiments show that a compact MoE model fine-tuned on our synthetic data achieves substantial improvements over its base model in response comprehensiveness, research depth, and user-perceived utility, approaching the performance of frontier proprietary deep research systems and establishing multi-agent synthetic trajectory training as an effective and scalable paradigm for enhancing LLM-based shopping assistance.

</details>


### [61] [The Auton Agentic AI Framework](https://arxiv.org/abs/2602.23720)
*Sheng Cao,Zhao Chang,Chang Li,Hannan Li,Liyao Fu,Ji Tang*

Main category: cs.AI

TL;DR: 针对生成式AI向智能体AI转型中LLM随机输出与后端确定性需求间的架构失配问题，本文提出Auton智能体框架，通过认知蓝图与运行时引擎的严格分离，结合增强POMDP模型、分层记忆、约束流形安全机制、三级自我演化及并行图执行等优化技术，实现可移植、可审计、安全的自主智能体系统。


<details>
  <summary>Details</summary>
Motivation: 人工智能正经历从生成式AI（概率性生成文本和图像）到智能体AI（代表用户在外部环境中执行动作的自主系统）的转型。这一转型暴露了一个根本性的架构失配问题：大语言模型产生随机、非结构化输出，而它们必须控制的底层基础设施（数据库、API、云服务）则需要确定性、符合模式规范的输入。现有方法无法解决这种结构性矛盾，导致智能体系统开发缺乏标准化、难以审计且存在安全隐患。

Method: 本文提出Auton智能体AI框架，采用认知蓝图（声明式、语言无关的智能体身份和能力规范）与运行时引擎（平台特定的执行基质）的严格分离架构。该方法包括：1）通过模型上下文协议（MCP）实现跨语言可移植性和模块化工具集成；2）将智能体执行形式化为带潜在推理空间的增强部分可观测马尔可夫决策过程（POMDP）；3）设计受生物情景记忆启发的分层记忆整合架构；4）提出约束流形形式主义，通过策略投影而非事后过滤实现安全强化；5）建立涵盖上下文适应到强化学习的三级自我演化框架；6）实施并行图执行、推测推理和动态上下文剪枝等运行时优化。

Result: 该框架提供了一个标准化的自主智能体系统创建、执行和治理架构。形式化模型为智能体决策提供了理论基础；分层记忆系统支持长期学习和经验复用；约束流形机制实现了内置安全性而非外部过滤；三级演化框架支持智能体持续改进；运行时优化显著降低了多步智能体工作流的端到端延迟。实验验证表明该架构能有效弥合LLM与后端系统间的语义鸿沟。

Conclusion: Auton框架通过认知蓝图与运行时引擎的解耦，系统性解决了生成式AI向智能体AI转型中的架构失配挑战。该框架不仅实现了跨语言可移植性和形式化可审计性，还通过约束流形和自我演化机制确保了安全性与适应性，为构建标准化、可信赖的自主智能体系统提供了完整解决方案，对推动智能体AI的工业化应用具有重要意义。

Abstract: The field of Artificial Intelligence is undergoing a transition from Generative AI -- probabilistic generation of text and images -- to Agentic AI, in which autonomous systems execute actions within external environments on behalf of users. This transition exposes a fundamental architectural mismatch: Large Language Models (LLMs) produce stochastic, unstructured outputs, whereas the backend infrastructure they must control -- databases, APIs, cloud services -- requires deterministic, schema-conformant inputs. The present paper describes the Auton Agentic AI Framework, a principled architecture for standardizing the creation, execution, and governance of autonomous agent systems. The framework is organized around a strict separation between the Cognitive Blueprint, a declarative, language-agnostic specification of agent identity and capabilities, and the Runtime Engine, the platform-specific execution substrate that instantiates and runs the agent. This separation enables cross-language portability, formal auditability, and modular tool integration via the Model Context Protocol (MCP). The paper formalizes the agent execution model as an augmented Partially Observable Markov Decision Process (POMDP) with a latent reasoning space, introduces a hierarchical memory consolidation architecture inspired by biological episodic memory systems, defines a constraint manifold formalism for safety enforcement via policy projection rather than post-hoc filtering, presents a three-level self-evolution framework spanning in-context adaptation through reinforcement learning, and describes runtime optimizations -- including parallel graph execution, speculative inference, and dynamic context pruning -- that reduce end-to-end latency for multi-step agent workflows.

</details>


### [62] [Unlocking Cognitive Capabilities and Analyzing the Perception-Logic Trade-off](https://arxiv.org/abs/2602.23730)
*Longyin Zhang,Shuo Sun,Yingxu He,Won Cheng Yi Lewis,Muhammad Huzaifah Bin Md Shahrin,Hardik Bhupendra Sailor,Heng Meng Jeremy Wong,Tarun Kumar Vangani,Yi Ma,Qiongqiong Wang,Minh Duc Pham,Ridong Jiang,Jingtao Li,Jingyi Liao,Zhuohan Liu,Yanfeng Lu,Manas Gupta,Ai Ti Aw*

Main category: cs.AI

TL;DR: 本文介绍MERaLiON2-Omni (Alpha)，一个100亿参数的多语言全感知多模态大模型，专为东南亚地区设计。研究提出解耦再融合感知与推理的渐进训练范式，通过正交模态适配构建区域感知主干，并利用生成-评判-精炼管道合成高质量多模态推理数据。在SEA-Omni基准测试中揭示了效率-稳定性悖论：推理能力虽非线性提升抽象任务表现，却引发时序漂移和视觉过度解读等感知不稳定现象。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽追求全感知能力，但将稳健的感官grounding与复杂推理有效融合仍是重大挑战，尤其对东南亚等数据代表性不足的区域。现有模型难以处理区域特定文化语境（如新加坡式英语混码、本地文化地标），亟需开发具备区域适应性的感知-推理协同架构。

Method: 采用两阶段渐进训练：第一阶段解耦系统1（感知）与系统2（推理），通过正交模态适配技术将东南亚特色视听线索与多语言LLM对齐，构建稳健感知主干；第二阶段融合阶段采用生成-评判-精炼框架，利用超级大模型过滤幻觉并通过共识机制合成银标数据，实现文本链式推理向多模态场景迁移。

Result: 在SEA-Omni基准测试中发现效率-稳定性悖论：推理作为非线性放大器显著提升数学与指令遵循等抽象任务性能，但同步引入低层次感知不稳定性，具体表现为长时音频的时序漂移（推理导致声学时间戳失同步）和视觉过度解读（逻辑覆盖像素级现实）两大现象。

Conclusion: 本研究报告了MERaLiON2-Omni的架构设计与数据高效训练方案，通过诊断分析揭示了感知稳健性与结构化推理间的内在权衡。发现表明，感知与推理能力的融合存在根本性矛盾，为未来多模态模型在追求高性能的同时保持感知稳定性提供了关键洞见。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) pursue omni-perception capabilities, yet integrating robust sensory grounding with complex reasoning remains a challenge, particularly for underrepresented regions. In this report, we introduce the research preview of MERaLiON2-Omni (Alpha), a 10B-parameter multilingual omni-perception tailored for Southeast Asia (SEA). We present a progressive training pipeline that explicitly decouples and then integrates "System 1" (Perception) and "System 2" (Reasoning) capabilities. First, we establish a robust Perception Backbone by aligning region-specific audio-visual cues (e.g., Singlish code-switching, local cultural landmarks) with a multilingual LLM through orthogonal modality adaptation. Second, to inject cognitive capabilities without large-scale supervision, we propose a cost-effective Generate-Judge-Refine pipeline. By utilizing a Super-LLM to filter hallucinations and resolve conflicts via a consensus mechanism, we synthesize high-quality silver data that transfers textual Chain-of-Thought reasoning to multimodal scenarios.
  Comprehensive evaluation on our newly introduced SEA-Omni Benchmark Suite reveals an Efficiency-Stability Paradox: while reasoning acts as a non-linear amplifier for abstract tasks (boosting mathematical and instruction-following performance significantly), it introduces instability in low-level sensory processing. Specifically, we identify Temporal Drift in long-context audio, where extended reasoning desynchronizes the model from acoustic timestamps, and Visual Over-interpretation, where logic overrides pixel-level reality. This report details the architecture, the data-efficient training recipe, and a diagnostic analysis of the trade-offs between robust perception and structured reasoning.

</details>


### [63] [Reasoning-Driven Multimodal LLM for Domain Generalization](https://arxiv.org/abs/2602.23777)
*Zhipeng Xu,Zilong Wang,Xinyang Jiang,Dongsheng Li,De Cheng,Nannan Wang*

Main category: cs.AI

TL;DR: 本文针对深度学习中的领域泛化问题，提出了一种基于多模态大语言模型推理能力的新方法。通过构建推理链来推导图像类别，研究揭示了基于推理的监督相比直接标签监督更具挑战性，并存在语义丰富度与优化效率的权衡问题。提出的RD-MLDG框架结合多任务交叉训练和自对齐推理正则化，在标准DomainBed数据集上实现了最先进的性能，证明了推理信号对领域泛化的价值。


<details>
  <summary>Details</summary>
Motivation: 当前领域泛化方法主要关注视觉特征不变性，未能充分利用多模态大语言模型的推理能力。本文旨在探索构建推理链来推导图像类别的潜力，以实现更鲁棒的领域外泛化预测，并系统研究推理在领域泛化中的作用。

Method: 提出RD-MLDG框架，包含两个核心组件：1) MTCT（多任务交叉训练），引入额外的直接分类路径来引导推理监督；2) SARR（自对齐推理正则化），通过迭代自标注保持推理链的语义丰富度，同时缓解推理模式不匹配问题。研究基于新构建的DomainBed-Reasoning数据集进行。

Result: 在PACS、VLCS、OfficeHome、TerraInc等标准DomainBed数据集上，RD-MLDG均达到最先进的性能表现，验证了所提方法的有效性。

Conclusion: 推理链可作为领域泛化的重要互补信号，利用多模态大语言模型的推理能力能够显著提升模型在领域偏移下的鲁棒性，为领域泛化研究提供了新方向。

Abstract: This paper addresses the domain generalization (DG) problem in deep learning. While most DG methods focus on enforcing visual feature invariance, we leverage the reasoning capability of multimodal large language models (MLLMs) and explore the potential of constructing reasoning chains that derives image categories to achieve more robust predictions under domain shift. To this end, we systematically study the role of reasoning in DG using DomainBed-Reasoning, a newly constructed extension of DomainBed dataset, in which each sample is paired with class-relevant reasoning chains. Our analysis reveals two key challenges: (i) fine-tuning MLLMs with reasoning chains for classification is more challenging than direct label supervision, since the model must optimize complex reasoning sequences before label prediction; and (ii) mismatches in reasoning patterns between supervision signals and fine-tuned MLLMs lead to a trade-off between semantic richness (informative but harder to optimize) and optimization efficiency (easier to optimize but less informative). To address these issues, we propose RD-MLDG (Reasoning-Driven Multimodal LLM for Domain Generalization), a framework with two components: (i) MTCT (Multi-Task Cross-Training), which introduces an additional direct classification pathway to guide reasoning supervision; and (ii) SARR (Self-Aligned Reasoning Regularization), which preserves the semantic richness of reasoning chains while mitigating reasoning-pattern mismatches via iterative self-labeling. Experiments on standard DomainBed datasets (PACS, VLCS, OfficeHome, TerraInc) demonstrate that RD-MLDG achieves state-of-the-art performances, highlighting reasoning as a promising complementary signal for robust out-of-domain generalization.

</details>


### [64] [EMO-R3: Reflective Reinforcement Learning for Emotional Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2602.23802)
*Yiyang Fang,Wenke Huang,Pei Fu,Yihao Yang,Kehua Su,Zhenbo Luo,Jian Luan,Mang Ye*

Main category: cs.AI

TL;DR: 本文提出EMO-R3框架，通过结构化情感思考和反思性情感奖励机制，利用反思强化学习方法提升多模态大语言模型的情感推理能力与可解释性，在多个视觉情感理解基准测试中实现性能突破。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM难以捕捉人类情感的复杂性与主观性，监督微调方法存在泛化能力差、可解释性不足的问题，而Group Relative Policy Optimization等强化学习方法未能与情感认知的内在特性有效对齐。

Method: 提出反思强化学习框架EMO-R3，核心包括：1）结构化情感思考，引导模型进行可解释的逐步情感推理；2）反思性情感奖励函数，基于视觉-文本一致性与情感连贯性实现推理过程的自评估与优化。

Result: 大量实验验证表明，EMO-R3显著提升了模型的可解释性与情感智能水平，在多项视觉情感理解基准测试中达到业界最优性能。

Conclusion: EMO-R3通过结合结构化思考与反思奖励机制，有效解决了MLLM情感推理的可解释性与性能瓶颈，为提升模型情感认知能力提供了新范式。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual reasoning and understanding tasks but still struggle to capture the complexity and subjectivity of human emotions. Existing approaches based on supervised fine-tuning often suffer from limited generalization and poor interpretability, while reinforcement learning methods such as Group Relative Policy Optimization fail to align with the intrinsic characteristics of emotional cognition. To address these challenges, we propose Reflective Reinforcement Learning for Emotional Reasoning (EMO-R3), a framework designed to enhance the emotional reasoning ability of MLLMs. Specifically, we introduce Structured Emotional Thinking to guide the model to perform step-by-step emotional reasoning in a structured and interpretable manner, and design a Reflective Emotional Reward that enables the model to re-evaluate its reasoning based on visual-text consistency and emotional coherence. Extensive experiments demonstrate that EMO-R3 significantly improves both the interpretability and emotional intelligence of MLLMs, achieving superior performance across multiple visual emotional understanding benchmarks.

</details>


### [65] [RF-Agent: Automated Reward Function Design via Language Agent Tree Search](https://arxiv.org/abs/2602.23876)
*Ning Gao,Xiuhui Zhang,Xingyu Jiang,Mukang You,Mohan Zhang,Yue Deng*

Main category: cs.AI

TL;DR: 针对低级别控制任务中奖励函数设计难题，现有LLM方法存在历史反馈利用不足和搜索效率低下的问题。本文提出RF-Agent框架，将LLM视为语言智能体并结合蒙特卡洛树搜索，通过序列决策和多阶段上下文推理提升奖励函数优化效率，在17个多样控制任务上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的奖励函数生成方法采用贪婪或进化算法进行迭代优化，但存在历史反馈信息利用率低、搜索效率差的问题，导致在复杂控制任务中性能提升有限。

Method: 提出RF-Agent框架，将大语言模型作为语言智能体，将奖励函数设计建模为序列决策过程。集成蒙特卡洛树搜索(MCTS)管理优化流程，利用LLM的多阶段上下文推理能力，在搜索过程中有效整合历史信息，识别高性能奖励函数。

Result: 在17个多样低级别控制任务上取得优异实验结果，验证了方法有效性。源代码已开源。

Conclusion: RF-Agent通过结合LLM上下文推理与MCTS高效搜索，显著提升了奖励函数设计效率，为复杂控制任务的自动化奖励生成提供了新范式。

Abstract: Designing efficient reward functions for low-level control tasks is a challenging problem. Recent research aims to reduce reliance on expert experience by using Large Language Models (LLMs) with task information to generate dense reward functions. These methods typically rely on training results as feedback, iteratively generating new reward functions with greedy or evolutionary algorithms. However, they suffer from poor utilization of historical feedback and inefficient search, resulting in limited improvements in complex control tasks. To address this challenge, we propose RF-Agent, a framework that treats LLMs as language agents and frames reward function design as a sequential decision-making process, enhancing optimization through better contextual reasoning. RF-Agent integrates Monte Carlo Tree Search (MCTS) to manage the reward design and optimization process, leveraging the multi-stage contextual reasoning ability of LLMs. This approach better utilizes historical information and improves search efficiency to identify promising reward functions. Outstanding experimental results in 17 diverse low-level control tasks demonstrate the effectiveness of our method. The source code is available at https://github.com/deng-ai-lab/RF-Agent.

</details>


### [66] [Pessimistic Auxiliary Policy for Offline Reinforcement Learning](https://arxiv.org/abs/2602.23974)
*Fan Zhang,Baoru Huang,Xin Zhang*

Main category: cs.AI

TL;DR: 针对离线强化学习中分布外动作导致的近似误差累积和高估问题，本文提出一种悲观辅助策略，通过最大化Q函数的下置信界来采样可靠动作，有效降低了误差累积并提升了其他离线RL方法的性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习从预收集数据集中学习策略，避免了不安全的在线交互，但学习过程中不可避免会访问分布外动作，引入近似误差，导致误差累积和严重的高估问题。

Method: 构造一种新的悲观辅助策略，通过最大化Q函数的下置信界来选择动作。该策略在学习策略附近表现出相对较高的价值和较低的不确定性，避免采样具有潜在高误差的高价值动作。

Result: 在离线强化学习基准测试上的广泛实验表明，利用该悲观辅助策略能有效提升其他离线RL方法的效果。

Conclusion: 提出的悲观辅助策略通过减少近似误差累积，为改善离线强化学习算法性能提供了一种有效途径。

Abstract: Offline reinforcement learning aims to learn an agent from pre-collected datasets, avoiding unsafe and inefficient real-time interaction. However, inevitable access to out-ofdistribution actions during the learning process introduces approximation errors, causing the error accumulation and considerable overestimation. In this paper, we construct a new pessimistic auxiliary policy for sampling reliable actions. Specifically, we develop a pessimistic auxiliary strategy by maximizing the lower confidence bound of the Q-function. The pessimistic auxiliary strategy exhibits a relatively high value and low uncertainty in the vicinity of the learned policy, avoiding the learned policy sampling high-value actions with potentially high errors during the learning process. Less approximation error introduced by sampled action from pessimistic auxiliary strategy leads to the alleviation of error accumulation. Extensive experiments on offline reinforcement learning benchmarks reveal that utilizing the pessimistic auxiliary strategy can effectively improve the efficacy of other offline RL approaches.

</details>


### [67] [Portfolio Reinforcement Learning with Scenario-Context Rollout](https://arxiv.org/abs/2602.24037)
*Vanya Priscillia Bendatu,Yao Lu*

Main category: cs.AI

TL;DR: 本文针对市场制度转换导致的分布偏移问题，提出宏观条件化情景上下文展开（SCR）方法生成压力情景下的次日多元收益情景。通过分析并解决时序差分学习中的奖励-转移不匹配问题，构建反事实下一状态稳定RL评论家训练，在31个美股和ETF投资组合的样本外测试中实现夏普比率最高提升76%、最大回撤最高降低53%的显著改进。


<details>
  <summary>Details</summary>
Motivation: 市场制度转换引发分布偏移，导致投资组合再平衡策略性能退化。历史数据无法提供反事实情景，使得基于情景化奖励的时序差分学习出现奖励-转移不匹配问题，进而 destabilizing RL critic training。

Method: 提出宏观条件化情景上下文展开（SCR）生成压力事件下的合理收益情景；分析奖励-转移不匹配导致混合评估目标的问题；构建基于展开结果的反事实下一状态，增强评论家智能体的bootstrap目标，实现稳定学习和偏差-方差权衡。

Result: 在31个不同的美国股票和ETF投资组合样本外评估中，相比经典和RL-based基线，该方法夏普比率提升最高达76%，最大回撤降低最高达53%。

Conclusion: 通过解决情景化奖励引入的内在不一致性，所提方法有效应对了市场制度转换挑战，为投资组合再平衡提供了稳定的强化学习框架，并在实际市场中验证了显著的风险调整后收益改进。

Abstract: Market regime shifts induce distribution shifts that can degrade the performance of portfolio rebalancing policies. We propose macro-conditioned scenario-context rollout (SCR) that generates plausible next-day multivariate return scenarios under stress events. However, doing so faces new challenges, as history will never tell what would have happened differently. As a result, incorporating scenario-based rewards from rollouts introduces a reward--transition mismatch in temporal-difference learning, destabilizing RL critic training.
  We analyze this inconsistency and show it leads to a mixed evaluation target. Guided by this analysis, we construct a counterfactual next state using the rollout-implied continuations and augment the critic agent's bootstrap target. Doing so stabilizes the learning and provides a viable bias-variance tradeoff.
  In out-of-sample evaluations across 31 distinct universes of U.S. equity and ETF portfolios, our method improves Sharpe ratio by up to 76% and reduces maximum drawdown by up to 53% compared with classic and RL-based portfolio rebalancing baselines.

</details>


### [68] [CIRCLE: A Framework for Evaluating AI from a Real-World Lens](https://arxiv.org/abs/2602.24055)
*Reva Schwartz,Carina Westling,Morgan Briggs,Marzieh Fadaee,Isar Nejadgholi,Matthew Holmes,Fariza Rashid,Maya Carlyle,Afaf Taïk,Kyra Wilson,Peter Douglas,Theodora Skeadas,Gabriella Waters,Rumman Chowdhury,Thiago Lacerda*

Main category: cs.AI

TL;DR: 本文提出CIRCLE框架，一个六阶段的基于生命周期的框架，旨在弥合模型中心性能指标与AI部署实际效果之间的差距，通过将利益相关者关注转化为可测量信号，实现基于实际下游影响的治理。


<details>
  <summary>Details</summary>
Motivation: 现有框架如MLOps关注系统稳定性，基准测试衡量抽象能力，但AI技术栈外的决策者缺乏关于AI在真实世界用户变化和约束下行为的系统证据。参与式设计和算法审计分别存在局部性和回顾性局限。

Method: CIRCLE框架通过六阶段生命周期方法，将TEVV中的验证阶段操作化，形式化地将技术栈外的利益相关者关注转化为可测量信号。整合实地测试、红队演练和纵向研究等方法，形成协调流水线。

Result: 产生系统性知识：既能在不同场景间进行比较，又对本地情境敏感。为治理提供基于实际下游影响而非理论能力的证据支持。

Conclusion: 该框架使AI治理能够基于实际部署效果而非理论性能，提供更前瞻性、结构化的协议来连接情境敏感的定性洞察与可扩展的定量指标。

Abstract: This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities.

</details>


### [69] [Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction](https://arxiv.org/abs/2602.24080)
*Xiang Li,Jiabao Gao,Sipei Lin,Xuan Zhou,Chi Zhang,Bo Cheng,Jiale Han,Benyou Wang*

Main category: cs.AI

TL;DR: 本研究首次对语音到语音(S2S)系统进行了图灵测试，收集了2,968条人工评判，发现现有系统均未通过测试，揭示了人机对话的自然度差距。研究进一步开发了18维细粒度评估体系，诊断出瓶颈在于副语言学特征、情感表达和对话人格，并提出可解释模型实现自动化评估。


<details>
  <summary>Details</summary>
Motivation: 受图灵测试启发，研究现代语音到语音系统是否能像人类一样自然对话，以评估当前对话式AI的真实水平并识别改进方向。

Method: 首次对9个先进S2S系统和28名人类参与者进行图灵测试，收集2,968条人工评判；构建包含18个人类相似性维度的细粒度分类体系并对对话进行众包标注；开发可解释模型利用细粒度评分进行人机判别。

Result: 所有被评估的S2S系统均未通过图灵测试；瓶颈不在于语义理解，而在于副语言学特征、情感表达能力和对话人格；现成AI模型作为图灵测试评判者表现不可靠；提出的可解释模型能实现准确透明的人机判别。

Conclusion: 本研究建立了首个S2S系统人类相似性评估基准，超越二元结果提供详细诊断洞察，为提升对话AI系统的类人性铺平道路。

Abstract: The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems.

</details>


### [70] [Artificial Agency Program: Curiosity, compression, and communication in agents](https://arxiv.org/abs/2602.24100)
*Richard Csaky*

Main category: cs.AI

TL;DR: 本文提出"人工主体计划"(AAP)，一个将AI系统构建为现实嵌入、资源受限主体的立场与研究议程。核心论点是AI作为扩展人类-工具系统的一部分最有价值，能增强感知、理解和执行能力并减少人机环境与工具间的界面摩擦。该计划将预测压缩、内在动机、授权控制、界面质量及语言/自我通信统一为选择性信息瓶颈，以可证伪方式构建，包含显式成本、分阶段实验和多模态标记化测试平台，旨在连接内在动机、信息论、热力学、有限理性与现代推理系统的概念与实验框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究缺乏对现实嵌入性和资源约束的系统性考量，导致人机协作效率低下。作者认为AI应作为增强人类能力的工具系统组成部分而非独立智能体，需通过好奇心驱动的学习进展来优化物理与计算约束下的感知、理解和执行能力，同时最小化人-工具-环境接口间的摩擦成本。现有方法未能有效统一信息论、热力学和有限理性等基础理论，亟需一个整合性研究框架来推动具身AI发展。

Method: 采用可证伪的研究纲领设计，包含三个核心方法：1) 显式成本建模，将物理与计算约束量化为可优化的预算分配问题；2) 分阶段实验验证，通过可控实验逐步验证理论假设；3) 构建多模态标记化测试平台，使主体在有限预算下动态分配于观察、行动与 deliberation 三个维度。理论层面将预测压缩、内在动机、授权控制、界面质量及自我通信统一建模为选择性信息瓶颈，并引入信息论、热力学、有限理性作为基础分析工具。

Result: 提出了完整的AAP理论框架，实现了五大概念的数学统一：预测压缩对应信息瓶颈，内在动机体现为学习进展，授权控制表征为可控状态空间，界面质量定义为互信息优化，语言/自我通信视为内部信息瓶颈。设计了具体的多模态测试平台，其中主体必须权衡观察、行动与 deliberation 的预算分配。建立了连接信息论、热力学与有限理性的理论桥梁，为现代推理系统提供了可证伪的实验研究范式。

Conclusion: 该计划为构建现实世界中的资源受限AI主体提供了系统性的概念与实验框架，通过将内在动机、信息论、热力学和有限理性统一，有望推动AI从抽象推理向具身智能转变。最终目标是创建能够与人类无缝协作、在物理约束下通过好奇心驱动持续学习、并优化整体系统效能的智能工具，为下一代人机协同系统奠定理论基础。

Abstract: This paper presents the Artificial Agency Program (AAP), a position and research agenda for building AI systems as reality embedded, resource-bounded agents whose development is driven by curiosity-as-learning-progress under physical and computational constraints. The central thesis is that AI is most useful when treated as part of an extended human--tool system that increases sensing, understanding, and actuation capability while reducing friction at the interface between people, tools, and environments. The agenda unifies predictive compression, intrinsic motivation, empowerment and control, interface quality (unification), and language/self-communication as selective information bottlenecks. We formulate these ideas as a falsifiable program with explicit costs, staged experiments, and a concrete multimodal tokenized testbed in which an agent allocates limited budget among observation, action, and deliberation. The aim is to provide a conceptual and experimental framework that connects intrinsic motivation, information theory, thermodynamics, bounded rationality, and modern reasoning systems

</details>


### [71] [Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance](https://arxiv.org/abs/2602.24110)
*Yanwei Ren,Haotian Zhang,Likang Xiao,Xikai Zhang,Jiaxing Huang,Jiayan Qiu,Baosheng Yu,Quan Chen,Liu Liu*

Main category: cs.AI

TL;DR: 本文提出SCOPE框架，通过过程奖励模型对部分正确的推理轨迹进行逐步修正，解决了RLVR中因粗粒度奖励导致的探索空间过早缩小问题，在数学推理任务上达到46.6%的SOTA准确率。


<details>
  <summary>Details</summary>
Motivation: 标准结果监督的RLVR会将有大部分正确步骤但因少量错误失败的轨迹与完全错误轨迹同等惩罚，这种粗粒度反馈导致模型丢弃有价值的轨迹，降低探索多样性并过早缩小搜索空间。现有方法未能有效利用模型自身生成的部分正确轨迹。

Method: SCOPE框架利用过程奖励模型识别次优轨迹中的首个错误步骤，并应用细粒度的离策略逐步修正。通过精确修复部分正确的轨迹，该方法能够有效挽救这些轨迹并扩大探索空间。

Result: 在数学推理任务上平均准确率达46.6%，分布外推理任务准确率达53.4%，轨迹多样性得分提升13.5%，建立了新的SOTA结果。

Conclusion: SCOPE通过精细化的轨迹修正有效缓解了RLVR的探索空间缩小问题，显著提升了模型性能，展示了在复杂推理任务上的强大泛化能力。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.

</details>


### [72] [Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume](https://arxiv.org/abs/2602.24195)
*Gregory Kang Ruey Lau,Hieu Dao,Nicole Kan Hui Lin,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: 本文提出UMPIRE，一种无需训练的多模态大语言模型不确定性量化框架。该方法通过计算模型采样响应的语义体积并调整不一致性，仅利用内部特征即可跨模态工作，无需外部工具。在图像、音频、视频文本基准测试中显著优于基线方法，并能泛化到图像/音频生成等非文本输出任务。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型功能强大，但其可能产生看似合理却错误的输出，阻碍了可靠部署。准确的不确定性度量可帮助识别不可靠查询并转交人工或更大模型处理。然而现有方法存在局限：仅限特定模态、依赖外部工具或计算成本高昂。因此需要一种通用、高效且无需外部依赖的不确定性量化方案。

Method: UMPIRE框架采用训练-free方式，仅利用模型内部模态特征。其核心是计算给定任务实例的采样MLLM响应的"不一致性调整的语义体积"，同时捕捉样本的全局语义多样性与基于内部模型信心的局部响应不一致性。该方法通过语义聚类和置信度加权实现不确定性量化。

Result: 大量实验表明，UMPIRE在图像、音频和视频文本基准测试中持续优于基线方法，在错误检测和不确定性校准方面表现优异，尤其在对抗性和分布外设置下效果显著。此外，该方法成功泛化至图像和音频生成等非文本输出任务。

Conclusion: UMPIRE为多模态大语言模型提供了一种高效、通用且无需训练的不确定性量化方案，解决了现有方法的局限性。其理论分析和实验验证表明，该框架能可靠识别模型错误，为提升MLLM部署安全性提供了实用工具，并展现出良好的跨任务泛化能力。

Abstract: Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.

</details>


### [73] [A Minimal Agent for Automated Theorem Proving](https://arxiv.org/abs/2602.24273)
*Borja Requena Pozo,Austin Letson,Krystian Nowakowski,Izan Beltran Ferreiro,Leopoldo Sarra*

Main category: cs.AI

TL;DR: 提出了一个极简的智能体基线系统，用于系统化比较不同AI定理证明器架构。该系统实现了SOTA系统的核心功能（迭代证明优化、库搜索和上下文管理），在多个基准测试中表现优异，同时架构更简单。研究表明迭代方法在样本效率和成本效益方面优于单次生成方法。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 当前AI定理证明器架构多样，缺乏统一的比较基准，使得不同方法之间的系统性对比变得困难。需要一个最小化的智能体框架来实现公平比较，同时保持足够的表达力来评估各种设计选择。

Method: 设计了一个包含三个核心模块的极简智能体基线：1）迭代证明优化机制；2）库搜索功能；3）上下文管理系统。在不同质的基准测试集上评估该系统，并比较了多种流行模型和设计选择。

Result: 该基线系统在性能上达到与SOTA方法竞争的水平，同时架构显著简化。关键发现是迭代方法相比单次生成具有一致性优势，特别是在样本效率和成本效益方面表现更好。

Conclusion: 迭代证明生成是AI定理证明的关键范式，具有显著的效率优势。该开源基线为未来研究提供了标准化参考，也为社区提供了可访问的证明工具，有助于推动领域发展。

Abstract: We propose a minimal agentic baseline that enables systematic comparison across different AI-based theorem prover architectures. This design implements the core features shared among state-of-the-art systems: iterative proof refinement, library search and context management. We evaluate our baseline using qualitatively different benchmarks and compare various popular models and design choices, and demonstrate competitive performance compared to state-of-the-art approaches, while using a significantly simpler architecture. Our results demonstrate consistent advantages of an iterative approach over multiple single-shot generations, especially in terms of sample efficiency and cost effectiveness. The implementation is released open-source as a candidate reference for future research and as an accessible prover for the community.

</details>


### [74] [DARE-bench: Evaluating Modeling and Instruction Fidelity of LLMs in Data Science](https://arxiv.org/abs/2602.24288)
*Fan Shu,Yite Wang,Ruofan Wu,Boyi Liu,Zhewei Yao,Yuxiong He,Feng Yan*

Main category: cs.AI

TL;DR: 本文提出DARE-bench，一个含6,300个可验证Kaggle任务的基准测试，用于评估LLM数据科学能力。实验显示强模型如gpt-o4-mini表现不佳，而基于该基准的微调可使Qwen3模型准确率提升最高8倍。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在两大缺陷：缺乏标准化的过程感知评估（无法衡量指令遵循和过程保真度），以及准确标注训练数据的稀缺性。随着LLM在复杂多步数据科学任务中应用需求激增，亟需一个更准确、可复现的评估基准。

Method: 作者构建了DARE-bench，包含6,300个源自Kaggle的数据科学任务，所有任务均设有可验证的客观答案，避免依赖主观的人或模型评判。该基准同时提供大规模训练数据集和评估集，覆盖广泛的任务类型并支持智能体工具的使用。

Result: 广泛评估显示，即使是gpt-o4-mini等高性能模型在机器学习建模任务中表现仍然不佳。使用DARE-bench训练数据进行微调效果显著：监督微调使Qwen3-32B准确率提升1.83倍，强化学习使Qwen3-4B准确率提升超过8倍。

Conclusion: DARE-bench通过提供可验证的客观评估和大规模高质量训练数据，不仅作为准确的评估基准，更成为关键的训练资源，验证了其在推动LLM数据科学能力提升方面的重要价值。

Abstract: The fast-growing demands in using Large Language Models (LLMs) to tackle complex multi-step data science tasks create an emergent need for accurate benchmarking. There are two major gaps in existing benchmarks: (i) the lack of standardized, process-aware evaluation that captures instruction adherence and process fidelity, and (ii) the scarcity of accurately labeled training data. To bridge these gaps, we introduce DARE-bench, a benchmark designed for machine learning modeling and data science instruction following. Unlike many existing benchmarks that rely on human- or model-based judges, all tasks in DARE-bench have verifiable ground truth, ensuring objective and reproducible evaluation. To cover a broad range of tasks and support agentic tools, DARE-bench consists of 6,300 Kaggle-derived tasks and provides both large-scale training data and evaluation sets. Extensive evaluations show that even highly capable models such as gpt-o4-mini struggle to achieve good performance, especially in machine learning modeling tasks. Using DARE-bench training tasks for fine-tuning can substantially improve model performance. For example, supervised fine-tuning boosts Qwen3-32B's accuracy by 1.83x and reinforcement learning boosts Qwen3-4B's accuracy by more than 8x. These significant improvements verify the importance of DARE-bench both as an accurate evaluation benchmark and critical training data.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [75] [Detoxifying LLMs via Representation Erasure-Based Preference Optimization](https://arxiv.org/abs/2602.23391)
*Nazanin Mohammadi Sepahvand,Eleni Triantafillou,Hugo Larochelle,Doina Precup,Daniel M. Roy,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）易产生毒性输出，现有DPO/NPO等防御方法鲁棒性不足：易受对抗提示攻击，且可通过微调重学习攻击轻易撤销。机制研究表明这些编辑仅停留在表面，有害表示"方向"依然存在。为此，本文提出REPO（表示擦除偏好优化），将去毒化重构为token级偏好问题，强制有毒表示向良性表示收敛，实现深层、鲁棒的毒性神经元编辑，在重学习攻击和增强GCG越狱等高级威胁下表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管基于DPO、NPO等算法的防御能降低毒性输出概率，但研究表明这些编辑是表面的：线性探测揭示有害"方向"仍存在于表示中。此类方法易受对抗提示攻击，且可通过微调重学习攻击轻易撤销。因此需要一种能深层编辑毒性编码、保持模型效用的鲁棒去毒化方法。

Method: 提出Representation Erasure-based Preference Optimization (REPO)，将去毒化重新定义为token级偏好问题。通过新颖的偏好优化目标，利用偏好数据强制有毒续体的表示向其良性对应体收敛，从而在表示层面对毒性编码神经元进行深层、局部化编辑。

Result: 详尽评估显示，REPO实现最先进鲁棒性：成功阻止包括重学习攻击和增强GCG越狱在内的复杂威胁，而这些攻击可使现有表示级和输出级方法失效。机制分析证实，REPO对毒性编码神经元进行深层编辑，同时保持模型的通用效用。

Conclusion: REPO通过表示擦除和token级偏好优化，提供了一种更鲁棒、更深层的LLM去毒化方案，能永久编辑毒性编码神经元，抵御高级攻击，为安全部署LLMs提供了有效途径。

Abstract: Large language models (LLMs) trained on webscale data can produce toxic outputs, raising concerns for safe deployment. Prior defenses, based on applications of DPO, NPO, and similar algorithms, reduce the likelihood of harmful continuations, but not robustly so: they are vulnerable to adversarial prompting and easily undone by fine-tuning-based relearning attacks. Indeed, research has shown that these edits to the model are superficial: linear probing reveals that harmful "directions" remain present in representations. To address this, we propose Representation Erasure-based Preference Optimization (REPO), reformulating detoxification as a token-level preference problem. Using a novel objective with preference data, we force the representations of toxic continuations to converge toward their benign counterparts. Our mechanistic analysis reveals that this granular approach is critical: unlike baselines, REPO induces deep, localized edits to toxicity-encoding neurons while preserving general model utility. Exhaustive evaluations show that REPO achieves state-of-the-art robustness, stopping sophisticated threats-including relearning attacks and enhanced GCG jailbreaks-where existing representation- and output-based methods fail.

</details>


### [76] [Long Range Frequency Tuning for QML](https://arxiv.org/abs/2602.23409)
*Michael Poppel,Jonas Stein,Sebastian Wölckert,Markus Baumann,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 针对量子机器学习中可训练频率编码的梯度优化存在频率可达性限制（仅能移动约±1单位）的问题，本文提出基于三元编码的网格初始化方法，以O(log₃(ω_max))的电路深度确保目标频率位于局部可到达范围内，在合成数据和真实航班数据集上分别实现0.9969和0.9671的中位R²得分，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管可训练频率编码理论上能将量子电路深度优化至与目标频谱大小匹配，但实际训练中梯度下降无法有效调节频率预因子，导致频率可达性受限，当目标频率超出可移动范围时优化失败，制约了该方法的实用性。

Method: 采用网格初始化策略结合三元编码方案，生成密集的整数频率谱，使目标频率自动落入梯度优化可达范围内，在保持指数级优于固定频率方法的同时，解决了频率训练难题。

Result: 在合成高频移位数据集上，三元网格初始化中位R²达0.9969，较基线0.1841提升显著；在Flight Passengers真实数据集上，中位R²为0.9671，较基线0.7876提升22.8%，验证了方法的有效性。

Conclusion: 该方法成功突破了可训练频率编码的频率可达性瓶颈，在保持电路效率的前提下显著提升了训练稳定性和模型性能，为量子机器学习在实际应用中的可扩展性提供了重要解决方案。

Abstract: Quantum machine learning models using angle encoding naturally represent truncated Fourier series, providing universal function approximation capabilities with sufficient circuit depth. For unary fixed-frequency encodings, circuit depth scales as O(omega_max * (omega_max + epsilon^{-2})) with target frequency magnitude omega_max and precision epsilon. Trainable-frequency approaches theoretically reduce this to match the target spectrum size, requiring only as many encoding gates as frequencies in the target spectrum. Despite this compelling efficiency, their practical effectiveness hinges on a key assumption: that gradient-based optimization can drive prefactors to arbitrary target values. We demonstrate through systematic experiments that frequency prefactors exhibit limited trainability: movement is constrained to approximately +/-1 units with typical learning rates. When target frequencies lie outside this reachable range, optimization frequently fails. To overcome this frequency reachability limitation, we propose grid-based initialization using ternary encodings, which generate dense integer frequency spectra. While this approach requires O(log_3(omega_max)) encoding gates -- more than the theoretical optimum but exponentially fewer than fixed-frequency methods -- it ensures target frequencies lie within the locally reachable range. On synthetic targets with three shifted high frequencies, ternary grid initialization achieves a median R^2 score of 0.9969, compared to 0.1841 for the trainable-frequency baseline. For the real-world Flight Passengers dataset, ternary grid initialization achieves a median R^2 score of 0.9671, representing a 22.8% improvement over trainable-frequency initialization (median R^2 = 0.7876).

</details>


### [77] [EvoX: Meta-Evolution for Automated Discovery](https://arxiv.org/abs/2602.23413)
*Shu Liu,Shubham Agarwal,Monishwaran Maheswaran,Mert Cemri,Zhifei Li,Qiuyang Mang,Ashwin Naren,Ethan Boneh,Audrey Cheng,Melissa Z. Pan,Alexander Du,Kurt Keutzer,Alexandros G. Dimakis,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.LG

TL;DR: 本文提出EvoX，一种自适应进化方法，通过联合演化候选解与搜索策略，动态调整优化过程。在200个真实任务上，EvoX在多数任务上超越现有AI进化方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动进化方法采用固定搜索策略和静态参数（如探索-利用比率），无法适应不同任务或在任务内随搜索空间变化而调整，导致优化效果受限。

Method: EvoX将搜索策略本身作为进化对象，采用双层演化机制：在演化候选解的同时，根据历史评估结果动态调整先验解的选择与变异策略，实现搜索策略的自适应切换。

Result: 在近200个涵盖程序、提示和算法优化的真实世界任务上，EvoX在大多数任务中性能优于AlphaEvolve、OpenEvolve、GEPA和ShinkaEvolve。

Conclusion: EvoX通过自我优化的进化机制，有效解决了静态搜索策略的适应性问题，为LLM驱动的进化计算提供了更强大的方法论。

Abstract: Recent work such as AlphaEvolve has shown that combining LLM-driven optimization with evolutionary search can effectively improve programs, prompts, and algorithms across domains. In this paradigm, previously evaluated solutions are reused to guide the model toward new candidate solutions. Crucially, the effectiveness of this evolution process depends on the search strategy: how prior solutions are selected and varied to generate new candidates. However, most existing methods rely on fixed search strategies with predefined knobs (e.g., explore-exploit ratios) that remain static throughout execution. While effective in some settings, these approaches often fail to adapt across tasks, or even within the same task as the search space changes over time. We introduce EvoX, an adaptive evolution method that optimizes its own evolution process. EvoX jointly evolves candidate solutions and the search strategies used to generate them, continuously updating how prior solutions are selected and varied based on progress. This enables the system to dynamically shift between different search strategies during the optimization process. Across nearly 200 real-world optimization tasks, EvoX outperforms existing AI-driven evolutionary methods including AlphaEvolve, OpenEvolve, GEPA, and ShinkaEvolve on the majority of tasks.

</details>


### [78] [Human Supervision as an Information Bottleneck: A Unified Theory of Error Floors in Human-Guided Learning](https://arxiv.org/abs/2602.23446)
*Alejandro Rodriguez Dominguez*

Main category: cs.LG

TL;DR: 该论文提出"人类限定智能极限"理论，论证人类监督通道的结构性不充分（标注噪声、偏好扭曲、语义压缩）导致任何学习者都存在严格正的超额风险下界，仅靠模型扩展无法消除持久错误，但辅助非人类信号可提升监督容量并突破该极限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在人类生成数据和反馈上训练，但仍存在源于标注噪声、主观偏好和语言表达带宽限制的持久错误。作者认为这些限制反映了监督通道的结构性属性，而非模型规模或优化问题，旨在解释为何单纯扩展无法消除这些对齐错误。

Method: 建立统一理论，证明当人类监督通道对潜在评估目标不充分时，会作为信息压缩通道，为任何受其主导的学习者诱导严格正的超额风险下限。通过算子理论、PAC-Bayes、信息论、因果推断、范畴论和RLHF博弈论分析六种框架形式化该极限，并从标注噪声、偏好扭曲、语义压缩进行结构分解。在真实偏好数据、合成任务和外部可验证基准上验证理论预测的结构特征。

Result: 实验证实：纯人类监督存在持久错误平台，而充分的信息辅助通道（检索、程序执行、工具）能严格降低或消除超额错误。该理论解释了扩展本身无法消除人类对齐错误的根本原因。

Conclusion: 人类监督的结构性限制构成性能的根本边界，扩展不足以突破，需借助非人类辅助信号提升监督容量才能突破人类限定智能极限。

Abstract: Large language models are trained primarily on human-generated data and feedback, yet they exhibit persistent errors arising from annotation noise, subjective preferences, and the limited expressive bandwidth of natural language. We argue that these limitations reflect structural properties of the supervision channel rather than model scale or optimization. We develop a unified theory showing that whenever the human supervision channel is not sufficient for a latent evaluation target, it acts as an information-reducing channel that induces a strictly positive excess-risk floor for any learner dominated by it. We formalize this Human-Bounded Intelligence limit and show that across six complementary frameworks (operator theory, PAC-Bayes, information theory, causal inference, category theory, and game-theoretic analyses of reinforcement learning from human feedback), non-sufficiency yields strictly positive lower bounds arising from the same structural decomposition into annotation noise, preference distortion, and semantic compression. The theory explains why scaling alone cannot eliminate persistent human-aligned errors and characterizes conditions under which auxiliary non-human signals (e.g., retrieval, program execution, tools) increase effective supervision capacity and collapse the floor by restoring information about the latent target. Experiments on real preference data, synthetic known-target tasks, and externally verifiable benchmarks confirm the predicted structural signatures: human-only supervision exhibits a persistent floor, while sufficiently informative auxiliary channels strictly reduce or eliminate excess error.

</details>


### [79] [Global Interpretability via Automated Preprocessing: A Framework Inspired by Psychiatric Questionnaires](https://arxiv.org/abs/2602.23459)
*Eric V. Strobl*

Main category: cs.LG

TL;DR: 针对精神疾病问卷预测中准确性与可解释性的权衡问题，本研究提出REFINE两阶段方法。该方法将非线性能力限制在基线预处理模块以估计稳定项目值，随后用线性模型预测未来症状严重程度，实现了预测性能与全局可解释性的统一。


<details>
  <summary>Details</summary>
Motivation: 精神疾病问卷高度依赖上下文且预测效度有限，传统非线性模型虽能提高准确性但可解释性不足，影响临床信任。需要借鉴影像学和组学领域的稳定信号提取策略，开发既准确又透明的精神疾病预测方法。

Method: 采用解耦预处理与预测的两阶段框架REFINE（冗余利用随访信息非线性增强）：第一阶段利用非线性模块从纵向问卷数据中提取稳定的基线项目值，消除访视和工具特异性伪影；第二阶段通过线性映射模型将稳定化项目值与未来严重程度关联，通过系数矩阵实现全局可解释性。

Result: 在多项精神及非精神疾病纵向预测实验中，REFINE在保持清晰全局归因的同时，性能显著优于其他可解释方法。

Conclusion: 通过在预处理阶段集中非线性建模、预测阶段保持线性的策略，REFINE有效解决了精神疾病预测的"准确性-可解释性"困境，为临床决策提供了既可靠又透明的工具。

Abstract: Psychiatric questionnaires are highly context sensitive and often only weakly predict subsequent symptom severity, which makes the prognostic relationship difficult to learn. Although flexible nonlinear models can improve predictive accuracy, their limited interpretability can erode clinical trust. In fields such as imaging and omics, investigators commonly address visit- and instrument-specific artifacts by extracting stable signal through preprocessing and then fitting an interpretable linear model. We adopt the same strategy for questionnaire data by decoupling preprocessing from prediction: we restrict nonlinear capacity to a baseline preprocessing module that estimates stable item values, and then learn a linear mapping from these stabilized baseline items to future severity. We refer to this two-stage method as REFINE (Redundancy-Exploiting Follow-up-Informed Nonlinear Enhancement), which concentrates nonlinearity in preprocessing while keeping the prognostic relationship transparently linear and therefore globally interpretable through a coefficient matrix, rather than through post hoc local attributions. In experiments, REFINE outperforms other interpretable approaches while preserving clear global attribution of prognostic factors across psychiatric and non-psychiatric longitudinal prediction tasks.

</details>


### [80] [Uncertainty-aware Language Guidance for Concept Bottleneck Models](https://arxiv.org/abs/2602.23495)
*Yangyi Li,Mengdi Huai*

Main category: cs.LG

TL;DR: 本文提出不确定性感知概念瓶颈模型，通过量化大语言模型标注概念的不确定性并融入训练过程，解决了现有方法忽视LLM幻觉风险且缺乏分布无关保证的问题，提供理论分析并在真实数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型依赖人工标注，成本高昂；现有基于大语言模型的自动标注方法忽视概念不确定性且未将其纳入训练，导致模型易受LLM幻觉影响，可靠性不足。

Method: 提出不确定性感知CBM框架：1）采用分布无关的统计方法，严格量化LLM标注概念标签的不确定性；2）将量化后的不确定性融入模型训练，通过区分不同概念标注的可靠性来优化学习过程。

Result: 提供理论分析证明方法有效性与可靠性保证；在多个真实世界数据集上的实验验证了该方法能够处理概念不确定性，提升模型鲁棒性。

Conclusion: 成功将不确定性量化机制引入基于LLM的CBM框架，解决了现有方法的关键缺陷，为开发更可靠、可解释的AI模型提供了新思路，促进了CBM的实际应用。

Abstract: Concept Bottleneck Models (CBMs) provide inherent interpretability by first mapping input samples to high-level semantic concepts, followed by a combination of these concepts for the final classification. However, the annotation of human-understandable concepts requires extensive expert knowledge and labor, constraining the broad adoption of CBMs. On the other hand, there are a few works that leverage the knowledge of large language models (LLMs) to construct concept bottlenecks. Nevertheless, they face two essential limitations: First, they overlook the uncertainty associated with the concepts annotated by LLMs and lack a valid mechanism to quantify uncertainty about the annotated concepts, increasing the risk of errors due to hallucinations from LLMs. Additionally, they fail to incorporate the uncertainty associated with these annotations into the learning process for concept bottleneck models. To address these limitations, we propose a novel uncertainty-aware CBM method, which not only rigorously quantifies the uncertainty of LLM-annotated concept labels with valid and distribution-free guarantees, but also incorporates quantified concept uncertainty into the CBM training procedure to account for varying levels of reliability across LLM-annotated concepts. We also provide the theoretical analysis for our proposed method. Extensive experiments on the real-world datasets validate the desired properties of our proposed methods.

</details>


### [81] [FedDAG: Clustered Federated Learning via Global Data and Gradient Integration for Heterogeneous Environments](https://arxiv.org/abs/2602.23504)
*Anik Pramanik,Murat Kantarcioglu,Vincent Oria,Shantanu Sharma*

Main category: cs.LG

TL;DR: FedDAG提出一种新的聚类联邦学习框架，通过融合数据和梯度信息的加权类级相似性度量进行更全面的客户端聚类，并采用双编码器架构实现跨集群特征迁移，在多样化异构数据基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在客户端数据异构时性能下降。现有聚类联邦学习方法仅依赖数据相似性或梯度相似性之一，评估不全面；同时限制知识共享仅在集群内部，无法利用跨集群的多样性信息。

Method: FedDAG框架提出：(1)一种加权类级相似性度量，融合数据和梯度信息进行更全面的聚类；(2)采用双编码器架构，主编码器在自身客户端数据上训练，辅助编码器利用互补集群的梯度进行优化，实现跨集群特征迁移同时保持集群特异性。

Result: 在多样化基准测试和数据异构性设置上的实验表明，FedDAG在准确率上持续优于现有最先进的聚类联邦学习方法。

Conclusion: FedDAG通过全面的相似性度量和跨集群特征迁移机制，有效解决了联邦学习中的数据异构性问题，为异构环境下的联邦学习提供了新的解决方案。

Abstract: Federated Learning (FL) enables a group of clients to collaboratively train a model without sharing individual data, but its performance drops when client data are heterogeneous. Clustered FL tackles this by grouping similar clients. However, existing clustered FL approaches rely solely on either data similarity or gradient similarity; however, this results in an incomplete assessment of client similarities. Prior clustered FL approaches also restrict knowledge and representation sharing to clients within the same cluster. This prevents cluster models from benefiting from the diverse client population across clusters. To address these limitations, FedDAG introduces a clustered FL framework, FedDAG, that employs a weighted, class-wise similarity metric that integrates both data and gradient information, providing a more holistic measure of similarity during clustering. In addition, FedDAG adopts a dual-encoder architecture for cluster models, comprising a primary encoder trained on its own clients' data and a secondary encoder refined using gradients from complementary clusters. This enables cross-cluster feature transfer while preserving cluster-specific specialization. Experiments on diverse benchmarks and data heterogeneity settings show that FedDAG consistently outperforms state-of-the-art clustered FL baselines in accuracy.

</details>


### [82] [Sample Size Calculations for Developing Clinical Prediction Models: Overview and pmsims R package](https://arxiv.org/abs/2602.23507)
*Diana Shamsutdinova,Felix Zimmer,Oyebayo Ridwan Olaniran,Sarah Markham,Daniel Stahl,Gordon Forbes,Ewan Carr*

Main category: cs.LG

TL;DR: 该论文针对临床预测模型开发中的样本量确定难题，提出了一种结合学习曲线、高斯过程优化和保证原则的模拟方法(pmsims R包)，相比现有方法提供更灵活、高效且可解释的解决方案，并通过案例研究验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 临床预测模型在医疗决策中应用日益广泛，但最小样本量确定仍是关键且未解决的挑战。样本量不足会导致过拟合、泛化能力差和预测偏差。现有方法(启发式规则、闭式公式、模拟方法)在复杂数据结构和机器学习模型中的灵活性和准确性差异较大，亟需改进。

Method: 系统回顾现有样本量估算方法，提出区分均值型与保证型标准的理论框架；在此基础上开发新型模拟方法，整合学习曲线、高斯过程优化和保证原则，以识别以高概率达到目标性能的样本量；将此方法实现为开源、模型无关的R包pmsims。

Result: 案例研究表明，样本量估计在不同方法、性能指标和建模策略间差异显著。相比现有工具，pmsims提供更灵活、高效且可解释的解决方案，能容纳多样模型和用户自定义指标，并显式考虑模型性能的变异性。

Conclusion: 该框架和软件通过结合灵活性与计算效率，推进了临床预测模型的样本量方法论。未来工作应扩展至层次化和多模态数据，纳入公平性和稳定性指标，并解决缺失数据和复杂依赖结构等挑战。

Abstract: Background: Clinical prediction models are increasingly used to inform healthcare decisions, but determining the minimum sample size for their development remains a critical and unresolved challenge. Inadequate sample sizes can lead to overfitting, poor generalisability, and biased predictions. Existing approaches, such as heuristic rules, closed-form formulas, and simulation-based methods, vary in flexibility and accuracy, particularly for complex data structures and machine learning models. Methods: We review current methodologies for sample size estimation in prediction modelling and introduce a conceptual framework that distinguishes between mean-based and assurance-based criteria. Building on this, we propose a novel simulation-based approach that integrates learning curves, Gaussian Process optimisation, and assurance principles to identify sample sizes that achieve target performance with high probability. This approach is implemented in pmsims, an open-source, model-agnostic R package. Results: Through case studies, we demonstrate that sample size estimates vary substantially across methods, performance metrics, and modelling strategies. Compared to existing tools, pmsims provides flexible, efficient, and interpretable solutions that accommodate diverse models and user-defined metrics while explicitly accounting for variability in model performance. Conclusions: Our framework and software advance sample size methodology for clinical prediction modelling by combining flexibility with computational efficiency. Future work should extend these methods to hierarchical and multimodal data, incorporate fairness and stability metrics, and address challenges such as missing data and complex dependency structures.

</details>


### [83] [Active Value Querying to Minimize Additive Error in Subadditive Set Function Learning](https://arxiv.org/abs/2602.23529)
*Martin Černý,David Sychrovský,Filip Úradník,Jakub Černý*

Main category: cs.LG

TL;DR: 本文针对值缺失的次可加集函数，研究如何通过最小化最小与最大补全间的距离来高效近似未知函数，提出离线和在线查询策略，并实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 次可加集函数在组合拍卖、组合优化和可解释机器学习中具有关键作用，但完整指定需指数级子集值。当值来自外部计算（如机器学习模型重训练）时，成本高昂。值缺失导致严重歧义，尤其当需优化不完整函数时。尽管确定性值查询对乘法误差近似次可加函数存在不可近似性结果，但加法误差近似问题尚未充分探索。

Method: 通过主动查询额外子集的值，针对具有已知先验的函数类别，分别设计离线和在线两种策略以最小化最小与最大补全间的距离。

Result: 1) 深入分析了不同类别集函数在值缺失下的最小/最大补全及二者距离；2) 基于先验知识开发最小化距离的方法，支持离线和在线查询；3) 在实际场景中实验验证算法性能。

Conclusion: 该研究为次可加集函数的值缺失问题提供了系统理论和实用算法，在降低资源消耗的同时保证了近似质量，在计算经济学和AI应用中具有重要价值。

Abstract: Subadditive set functions play a pivotal role in computational economics (especially in combinatorial auctions), combinatorial optimization or artificial intelligence applications such as interpretable machine learning. However, specifying a set function requires assigning values to an exponentially large number of subsets in general, a task that is often resource-intensive in practice, particularly when the values derive from external sources such as retraining of machine learning models. A~simple omission of certain values introduces ambiguity that becomes even more significant when the incomplete set function has to be further optimized over. Motivated by the well-known result about inapproximability of subadditive functions using deterministic value queries with respect to a multiplicative error, we study a problem of approximating an unknown subadditive (or a subclass of thereof) set function with respect to an additive error -- i. e., we aim to efficiently close the distance between minimal and maximal completions. Our contributions are threefold: (i) a thorough exploration of minimal and maximal completions of different classes of set functions with missing values and an analysis of their resulting distance; (ii) the development of methods to minimize this distance over classes of set functions with a known prior, achieved by disclosing values of additional subsets in both offline and online manner; and (iii) empirical demonstrations of the algorithms' performance in practical scenarios.

</details>


### [84] [Dynamics of Learning under User Choice: Overspecialization and Peer-Model Probing](https://arxiv.org/abs/2602.23565)
*Adhyyan Narang,Sarah Dean,Lillian J Ratliff,Maryam Fazel*

Main category: cs.LG

TL;DR: 本文揭示了多平台机器学习中的"过专门化陷阱"现象，即学习者通过反馈机制几乎必然收敛到任意差的全局性能模型。受知识蒸馏启发，作者提出探测算法，使学习者能够通过探测同行模型预测来学习非自身用户。理论证明在探测源信息充分（如市场领导者或多数良好同行）时，算法几乎必然收敛到有界全局风险的平稳点，并通过三个真实数据集的半合成实验验证。


<details>
  <summary>Details</summary>
Motivation: 在经济相关的机器学习部署场景中，多个平台共享同一用户池，用户自主选择偏好平台。现有研究仅关注学习者观察数据分布上的局部损失。然而，存在反馈诱导的"过专门化陷阱"：学习者优化现有偏好用户的过程使其对外部用户吸引力下降，导致观察数据进一步受限，从而几乎必然收敛到任意差的全局性能模型，即使低全局损失的模型存在。

Method: 借鉴现代机器学习中的知识蒸馏思想，提出探测算法，允许学习者探测同行模型的预测以学习未选择自身的用户。通过理论分析表征探测成功条件：当探测源信息充分（如已知市场领导者或具有良好全局性能的大多数同行）时，该过程几乎必然收敛到具有有界全局风险的平稳点。

Result: 理论分析表明，在探测源信息充分的条件下（市场领导者或良好全局性能同行占多数），探测算法能够几乎必然收敛到具有有界全局风险的平稳点。研究者在MovieLens、Census和Amazon Sentiment数据集上进行的半合成实验验证了这些理论发现。

Conclusion: 本研究揭示了多平台竞争环境中的过专门化陷阱机制，提出了基于知识蒸馏的探测解决方案。研究表明，通过利用同行模型信息，学习者能够突破自选择偏差，实现可控的全局性能。这为平台在用户自选择场景下避免过拟合、提升模型泛化能力提供了理论保证和实践指导。

Abstract: In many economically relevant contexts where machine learning is deployed, multiple platforms obtain data from the same pool of users, each of whom selects the platform that best serves them. Prior work in this setting focuses exclusively on the "local" losses of learners on the distribution of data that they observe. We find that there exist instances where learners who use existing algorithms almost surely converge to models with arbitrarily poor global performance, even when models with low full-population loss exist. This happens through a feedback-induced mechanism, which we call the overspecialization trap: as learners optimize for users who already prefer them, they become less attractive to users outside this base, which further restricts the data they observe. Inspired by the recent use of knowledge distillation in modern ML, we propose an algorithm that allows learners to "probe" the predictions of peer models, enabling them to learn about users who do not select them. Our analysis characterizes when probing succeeds: this procedure converges almost surely to a stationary point with bounded full-population risk when probing sources are sufficiently informative, e.g., a known market leader or a majority of peers with good global performance. We verify our findings with semi-synthetic experiments on the MovieLens, Census, and Amazon Sentiment datasets.

</details>


### [85] [SDMixer: Sparse Dual-Mixer for Time Series Forecasting](https://arxiv.org/abs/2602.23581)
*Xiang Ao*

Main category: cs.LG

TL;DR: 这篇论文提出了一种双流稀疏Mixer预测框架来解决多变量时间序列预测中的多尺度特征、弱相关性和噪声干扰问题。该框架分别在频域和时间域提取全局趋势和局部动态特征，并通过稀疏机制过滤无效信息，在多个真实数据集上实现了领先的预测性能。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列预测在交通、能源和金融等领域应用广泛，但现有模型受限于数据的多尺度特性、变量间弱相关性和噪声干扰，导致预测性能不佳。这些问题限制了传统方法在实际场景中的应用效果。

Method: 提出双流稀疏Mixer框架，通过频域分支提取全局趋势特征，时间域分支捕捉局部动态特征。引入稀疏机制筛选有效信息，强化跨变量依赖建模。采用双路并行结构同时处理不同尺度的模式信息。

Result: 在多个真实世界场景数据集上的实验结果表明，该方法实现了领先的预测性能。验证了框架在解决多尺度特征、弱相关性和噪声问题方面的有效性，展现了良好的泛化能力。

Conclusion: 双流稀疏Mixer框架有效提升了多变量时间序列预测的准确性，通过频域-时间域协同建模和稀疏过滤机制，显著增强了模型处理复杂数据模式的能力，具有实际应用价值。

Abstract: Multivariate time series forecasting is widely applied in fields such as transportation, energy, and finance. However, the data commonly suffers from issues of multi-scale characteristics, weak correlations, and noise interference, which limit the predictive performance of existing models. This paper proposes a dual-stream sparse Mixer prediction framework that extracts global trends and local dynamic features from sequences in both the frequency and time domains, respectively. It employs a sparsity mechanism to filter out invalid information, thereby enhancing the accuracy of cross-variable dependency modeling. Experimental results demonstrate that this method achieves leading performance on multiple real-world scenario datasets, validating its effectiveness and generality. The code is available at https://github.com/SDMixer/SDMixer

</details>


### [86] [When Does Multimodal Learning Help in Healthcare? A Benchmark on EHR and Chest X-Ray Fusion](https://arxiv.org/abs/2602.23614)
*Kejing Yin,Haizhou Xu,Wenfang Yao,Chen Liu,Zijie Chen,Yui Haang Cheung,William K. Cheung,Jing Qin*

Main category: cs.LG

TL;DR: 本文通过系统性基准测试分析了电子健康记录(EHR)与胸部X光(CXR)多模态融合在临床预测中的实际效果，发现在模态完整时可提升性能，但在模态缺失和公平性约束下效果显著下降，并发布了灵活的开源基准测试工具包CareBench。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在临床决策支持中前景广阔，但在实际应用中，特别是在模态缺失和公平性约束下，多模态学习何时真正有效仍不明确。为此，本研究旨在系统性地回答四个基本问题：多模态融合何时能提升临床预测性能、不同融合策略的优劣比较、现有方法对缺失模态的鲁棒性，以及多模态模型是否能够实现算法公平性。

Method: 研究基于MIMIC-IV和MIMIC-CXR的标准化队列，对EHR与CXR的多模态融合进行系统性基准测试，通过统一的评估框架回答上述四个核心问题。

Result: 实验发现：1) 多模态融合在模态完整时能提升性能，增益集中在需要EHR和CXR互补信息的疾病上；2) 跨模态学习机制可捕捉超越简单拼接的临床依赖关系，但EHR的丰富时序结构导致显著的模态不平衡，仅靠架构复杂性无法克服；3) 在真实缺失场景下，除非模型显式设计以处理不完整输入，否则多模态收益迅速衰减；4) 多模态融合并不能改善算法公平性，亚组差异主要源于不同人口统计组间的不均衡敏感性；5) 研究同时发布了灵活可插拔的基准测试工具包CareBench，支持新模型和数据集的集成。

Conclusion: 本研究为理解多模态学习在临床应用中的实际价值提供了可操作的指导，明确了其在不同场景下的有效性边界和失效原因，为开发既有效又可靠的临床可部署多模态系统奠定了重要基础。

Abstract: Machine learning holds promise for advancing clinical decision support, yet it remains unclear when multimodal learning truly helps in practice, particularly under modality missingness and fairness constraints. In this work, we conduct a systematic benchmark of multimodal fusion between Electronic Health Records (EHR) and chest X-rays (CXR) on standardized cohorts from MIMIC-IV and MIMIC-CXR, aiming to answer four fundamental questions: when multimodal fusion improves clinical prediction, how different fusion strategies compare, how robust existing methods are to missing modalities, and whether multimodal models achieve algorithmic fairness. Our study reveals several key insights. Multimodal fusion improves performance when modalities are complete, with gains concentrating in diseases that require complementary information from both EHR and CXR. While cross-modal learning mechanisms capture clinically meaningful dependencies beyond simple concatenation, the rich temporal structure of EHR introduces strong modality imbalance that architectural complexity alone cannot overcome. Under realistic missingness, multimodal benefits rapidly degrade unless models are explicitly designed to handle incomplete inputs. Moreover, multimodal fusion does not inherently improve fairness, with subgroup disparities mainly arising from unequal sensitivity across demographic groups. To support reproducible and extensible evaluation, we further release a flexible benchmarking toolkit that enables plug-and-play integration of new models and datasets. Together, this work provides actionable guidance on when multimodal learning helps, when it fails, and why, laying the foundation for developing clinically deployable multimodal systems that are both effective and reliable. The open-source toolkit can be found at https://github.com/jakeykj/CareBench.

</details>


### [87] [On the Convergence of Single-Loop Stochastic Bilevel Optimization with Approximate Implicit Differentiation](https://arxiv.org/abs/2602.23633)
*Yubo Zhou,Luo Luo,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: 该论文对单循环随机近似隐式微分（SSAID）算法进行精细化收敛分析，证明其在O(κ^7 ε^{-2})的Oracle复杂度下能够达到ε-驻点，既匹配多循环方法的最优收敛速率，又首次显式揭示了条件数κ的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 随机双层优化是元学习和超参数优化的核心框架，单循环算法虽在实际中广泛应用，但其理论理解尤其在随机情况下相对滞后。现有分析往往收敛速率次优，或将关键的下层条件数κ隐藏在Lipschitz常数中，未能显式刻画其影响。

Method: 通过精细的概率分析、随机近似技术以及隐函数定理，针对SSAID算法推导出收敛上界，并显式分离出κ的幂次。

Result: 理论结果表明，SSAID在O(κ^7 ε^{-2})的Oracle复杂度下达到ε-驻点。这一结果首次在随机AID-based单循环方法中给出κ的显式、细粒度刻画，并且其ε^{-2}的速率与最先进的多循环方法（如stocBiO）相匹配。

Conclusion: 该研究证明SSAID不仅是一种启发式算法，更具备严格的理论保证，其收敛性能可与主流多循环框架相媲美，为单循环算法在随机双层优化中的理论理解和应用奠定了坚实基础。

Abstract: Stochastic Bilevel Optimization has emerged as a fundamental framework for meta-learning and hyperparameter optimization. Despite the practical prevalence of single-loop algorithms--which update lower and upper variables concurrently--their theoretical understanding, particularly in the stochastic regime, remains significantly underdeveloped compared to their multi-loop counterparts. Existing analyses often yield suboptimal convergence rates or obscure the critical dependence on the lower-level condition number $κ$, frequently burying it within generic Lipschitz constants. In this paper, we bridge this gap by providing a refined convergence analysis of the Single-loop Stochastic Approximate Implicit Differentiation (SSAID) algorithm. We prove that SSAID achieves an $ε$-stationary point with an oracle complexity of $\mathcal{O}(κ^7 ε^{-2})$. Our result is noteworthy in two aspects: (i) it matches the optimal $\mathcal{O}(ε^{-2})$ rate of state-of-the-art multi-loop methods (e.g., stocBiO) while maintaining the computational efficiency of a single-loop update; and (ii) it provides the first explicit, fine-grained characterization of the $κ$-dependence for stochastic AID-based single-loop methods. This work demonstrates that SSAID is not merely a heuristic approach, but admits a rigorous theoretical foundation with convergence guarantees competitive with mainstream multi-loop frameworks.

</details>


### [88] [FlexGuard: Continuous Risk Scoring for Strictness-Adaptive LLM Content Moderation](https://arxiv.org/abs/2602.23636)
*Zhihao Ding,Jinming Li,Ze Lu,Jieming Shi*

Main category: cs.LG

TL;DR: 针对LLM内容安全审核中严格度定义固定导致的模型脆弱性问题，本文提出FlexGuard——一种输出连续风险分数的LLM审核器，通过风险对齐优化和阈值选择策略实现严格度自适应，在FlexBench基准测试中展现出显著优于传统二元分类器的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM护栏模型将内容审核简化为固定二元分类任务，隐含假设危害性定义是静态不变的。然而实际应用中，审核严格度因平台而异且随时间演变，导致二元审核器在面对变化的审核要求时表现脆弱，亟需能够动态适应不同严格度标准的审核方案。

Method: 1) 构建FlexBench基准，支持多严格度体系下的可控评估；2) 提出FlexGuard审核器，输出校准的连续风险分数反映风险严重性，通过阈值化实现特定严格度决策；3) 采用风险对齐优化训练策略提升分数-严重性一致性；4) 提供部署时可用的阈值选择策略以适应目标严格度。

Result: 实验揭示现有审核器存在严重跨严格度不一致性：同一模型在不同严格度体系下性能差异巨大。FlexGuard在FlexBench及公开基准上均实现更高审核准确率，并在严格度变化时表现出显著更强的鲁棒性，有效解决了传统二元审核器的适应性缺陷。

Conclusion: 连续风险分数表示结合阈值化策略是构建实用化LLM内容审核系统的有效路径。FlexGuard通过风险对齐优化实现了严格度自适应，为动态演化的审核需求提供了可扩展解决方案，对推动LLM安全部署具有重要实践价值。

Abstract: Ensuring the safety of LLM-generated content is essential for real-world deployment. Most existing guardrail models formulate moderation as a fixed binary classification task, implicitly assuming a fixed definition of harmfulness. In practice, enforcement strictness - how conservatively harmfulness is defined and enforced - varies across platforms and evolves over time, making binary moderators brittle under shifting requirements. We first introduce FlexBench, a strictness-adaptive LLM moderation benchmark that enables controlled evaluation under multiple strictness regimes. Experiments on FlexBench reveal substantial cross-strictness inconsistency in existing moderators: models that perform well under one regime can degrade substantially under others, limiting their practical usability. To address this, we propose FlexGuard, an LLM-based moderator that outputs a calibrated continuous risk score reflecting risk severity and supports strictness-specific decisions via thresholding. We train FlexGuard via risk-alignment optimization to improve score-severity consistency and provide practical threshold selection strategies to adapt to target strictness at deployment. Experiments on FlexBench and public benchmarks demonstrate that FlexGuard achieves higher moderation accuracy and substantially improved robustness under varying strictness. We release the source code and data to support reproducibility.

</details>


### [89] [FedRot-LoRA: Mitigating Rotational Misalignment in Federated LoRA](https://arxiv.org/abs/2602.23638)
*Haoran Zhang,Dongjun Kim,Seohyeon Cha,Haris Vikalo*

Main category: cs.LG

TL;DR: 该论文提出FedRot-LoRA，通过正交变换对齐客户端低秩更新因子，解决联邦LoRA中因旋转不变性导致的子空间失配和聚合误差问题，在不增加通信开销的前提下提升异构数据下的训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 联邦LoRA采用因子-wise平均进行聚合，但低秩分解的旋转不变性使各客户端更新可位于不同语义等价的子空间，直接平均导致破坏性干涉。现有研究未解决此根本问题，严重影响联邦学习效果。

Method: 提出FedRot-LoRA框架，在聚合前对客户端低秩因子施加正交变换实现对齐，保持更新语义不变的同时最小化子空间差异，并提供收敛性分析证明对齐可减小聚合误差上界。

Result: 在自然语言理解和生成任务上，FedRot-LoRA在不同数据异构程度和LoRA秩设置下均显著优于基线方法，训练过程更稳定，性能提升明显。

Conclusion: 旋转对齐是联邦LoRA中消除聚合误差的关键技术，该工作为去中心化数据高效微调大语言模型提供了理论和方法支撑。

Abstract: Federated LoRA provides a communication-efficient mechanism for fine-tuning large language models on decentralized data. In practice, however, a discrepancy between the factor-wise averaging used to preserve low rank and the mathematically correct aggregation of local updates can cause significant aggregation error and unstable training. We argue that a major source of this problem is rotational misalignment, arising from the rotational invariance of low-rank factorizations -- semantically equivalent updates can be represented in different latent subspaces across clients since $(B_i R_i)(R_i^\top A_i) = B_i A_i$. When such misaligned factors are averaged directly, they interfere destructively and degrade the global update. To address this issue, we propose FedRot-LoRA, a federated LoRA framework that aligns client updates via orthogonal transformations prior to aggregation. This alignment preserves the semantic update while reducing cross-client subspace mismatch, without increasing communication cost or restricting model expressivity. We provide a convergence analysis that examines the aggregation error induced by factor-wise averaging and shows how rotational alignment yields a tighter upper bound on this error. Extensive experiments on natural language understanding and generative tasks demonstrate that FedRot-LoRA consistently outperforms existing federated LoRA baselines across a range of heterogeneity levels and LoRA ranks.

</details>


### [90] [Selective Denoising Diffusion Model for Time Series Anomaly Detection](https://arxiv.org/abs/2602.23662)
*Kohei Obata,Zheng Chen,Yasuko Matsubara,Lingwei Zhu,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 本文提出AnomalyFilter，一种用于时间序列异常检测的新型扩散模型方法，通过掩码训练噪声和实例去噪时不添加噪声，实现仅对异常部分去噪并保留正常部分的选择性过滤，克服了现有条件重构方法的性能限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的时间序列异常检测方法采用条件策略从白噪声重构输入实例，但难以准确重构正常部分，导致检测性能不理想。需要设计一种能更好保留正常模式、专注于异常检测的扩散模型噪声策略。

Method: 方法包含两个核心组件：1）训练阶段对高斯噪声进行掩码处理；2）去噪过程中不再向实例添加噪声。这种设计使模型成为选择性过滤器，仅对实例中的异常部分进行去噪，同时完整保留正常部分。两个简单组件的协同作用显著提升了朴素扩散模型的性能。

Result: 在五个数据集上的大量实验表明，AnomalyFilter在正常部分上实现了显著更低的重建误差，为异常检测的有效性提供了实证支持。该方法通过专注于噪声设计，展示了扩散模型在时间序列异常检测中的新应用范式。

Conclusion: AnomalyFilter是针对时间序列异常检测专门设计的扩散模型噪声策略的开创性方法。它代表了一种新颖的研究方向，通过选择性过滤而非完全重构的思路，为扩散模型在该领域的应用提供了新的理论视角和实践价值。

Abstract: Time series anomaly detection (TSAD) has been an important area of research for decades, with reconstruction-based methods, mostly based on generative models, gaining popularity and demonstrating success. Diffusion models have recently attracted attention due to their advanced generative capabilities. Existing diffusion-based methods for TSAD rely on a conditional strategy, which reconstructs input instances from white noise with the aid of the conditioner. However, this poses challenges in accurately reconstructing the normal parts, resulting in suboptimal detection performance. In response, we propose a novel diffusion-based method, named AnomalyFilter, which acts as a selective filter that only denoises anomaly parts in the instance while retaining normal parts. To build such a filter, we mask Gaussian noise during the training phase and conduct the denoising process without adding noise to the instances. The synergy of the two simple components greatly enhances the performance of naive diffusion models. Extensive experiments on five datasets demonstrate that AnomalyFilter achieves notably low reconstruction error on normal parts, providing empirical support for its effectiveness in anomaly detection. AnomalyFilter represents a pioneering approach that focuses on the noise design of diffusion models specifically tailored for TSAD.

</details>


### [91] [Disentangled Mode-Specific Representations for Tensor Time Series via Contrastive Learning](https://arxiv.org/abs/2602.23663)
*Kohei Obata,Taichi Murayama,Zheng Chen,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 本文提出MoST模型，一种针对多模态张量时间序列的新型表示学习方法。该方法通过张量切片降低结构复杂度，利用对比学习框架将表示分解为模式特定特征和模式不变特征，在真实数据集上分类和预测精度均优于现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 多模态张量时间序列广泛存在于搜索引擎、环境监测等领域，但其内在复杂度阻碍了丰富表示的学习。现有方法未能有效处理张量结构并解耦不同模式间的特征关系。

Method: MoST采用张量切片策略简化TTS结构复杂度，学习可分解为个体非时序模式的表示。每个表示捕获模式特定特征（同一模式内变量间关系）和模式不变特征（不同模式间共性）。通过对比学习框架优化参数，损失函数包含模式特定和模式不变两部分，将解耦表示作为数据增强有效利用。

Result: 在真实世界数据集上的大量实验表明，MoST在分类和预测准确率方面持续优于现有最优方法。

Conclusion: MoST通过张量切片和对比学习成功解耦多模态张量时间序列的复杂特征，实现了高效的表示学习，为相关应用提供了新的解决方案。代码已开源。

Abstract: Multi-mode tensor time series (TTS) can be found in many domains, such as search engines and environmental monitoring systems. Learning representations of a TTS benefits various applications, but it is also challenging since the complexities inherent in the tensor hinder the realization of rich representations. In this paper, we propose a novel representation learning method designed specifically for TTS, namely MoST. Specifically, MoST uses a tensor slicing approach to reduce the complexity of the TTS structure and learns representations that can be disentangled into individual non-temporal modes. Each representation captures mode-specific features, which are the relationship between variables within the same mode, and mode-invariant features, which are in common in representations of different modes. We employ a contrastive learning framework to learn parameters; the loss function comprises two parts intended to learn representation in a mode-specific way and mode-invariant way, effectively exploiting disentangled representations as augmentations. Extensive experiments on real-world datasets show that MoST consistently outperforms the state-of-the-art methods in terms of classification and forecasting accuracy. Code is available at https://github.com/KoheiObata/MoST.

</details>


### [92] [Optimizer-Induced Low-Dimensional Drift and Transverse Dynamics in Transformer Training](https://arxiv.org/abs/2602.23696)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本研究通过非中心化行归一化轨迹PCA揭示小型Transformer训练轨迹呈现主导漂移方向与横向残余动力学。AdamW与SGD在相同损失下展现显著几何差异：AdamW为多维漂移结构，SGD近乎共线演化。重加热选择性地扰动横向分量，表明优化器塑造超越损失值的学习轨迹几何结构。


<details>
  <summary>Details</summary>
Motivation: 探究不同优化器如何塑造模型训练的动态几何结构，超越传统损失分析，理解优化算法对参数演化路径和有效维度的影响机制。

Method: 采用非中心化行归一化轨迹主成分分析(tPCA)量化小型Transformer参数更新轨迹，对比匹配损失水平下AdamW与SGD族优化器的几何特征差异。

Result: 早期训练中单一主成分捕获大部分累积参数移动，剩余成分编码辅助探针性能的振荡行为；瞬时梯度与主导方向对齐度低；AdamW呈现多维漂移结构，SGD族呈现近乎共线参数演化与较弱探针动力学；重加热选择性地扰动横向分量。

Conclusion: 优化器选择深刻影响学习轨迹的有效维度与几何结构，这种影响无法仅从损失值中体现，揭示了不同优化算法在训练动态上的本质差异。

Abstract: We study the geometry of training trajectories in small transformer models and find that parameter updates organize into a dominant drift direction with transverse residual dynamics. Using uncentered, row-normalized trajectory PCA, we show that a single direction captures a large fraction of cumulative parameter movement early in training, while remaining components encode oscillatory behavior in auxiliary probe performance. Instantaneous gradients exhibit little alignment with this dominant direction, indicating that it arises from accumulated optimizer updates rather than per-batch gradient structure. Comparing AdamW with SGD variants at matched loss levels reveals substantial differences in trajectory geometry: AdamW develops multi-dimensional drift structure, whereas SGD-family optimizers produce nearly colinear parameter evolution and weaker probe dynamics. Reheating selectively perturbs transverse components with minimal effect on the dominant drift coordinate. These findings suggest that optimizer choice shapes the effective dimensionality and structure of learning trajectories beyond what is apparent from loss values alone.

</details>


### [93] [Bridging Dynamics Gaps via Diffusion Schrödinger Bridge for Cross-Domain Reinforcement Learning](https://arxiv.org/abs/2602.23737)
*Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TL;DR: 提出BDGxRL框架，通过扩散薛定谔桥对齐源域与目标域动态差异，结合奖励调制机制，实现无需目标域环境交互的跨域策略学习，在MuJoCo基准测试中优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 跨域强化学习面临目标域无环境交互和奖励监督的关键挑战，导致直接策略学习不可行，现有方法难以有效桥接动态间隙。

Method: 1）使用扩散薛定谔桥将源域转移与目标域离线演示编码的动态对齐；2）设计基于状态转移的奖励调制机制；3）完全在源域内进行目标导向策略学习，无需访问目标环境。

Result: 在MuJoCo跨域基准测试中性能超越现有最优基线，并展现出对转移动态变化的强适应能力。

Conclusion: BDGxRL框架有效解决了无目标域交互下的跨域迁移难题，为强化学习在实际跨域应用中的部署提供了可行方案。

Abstract: Cross-domain reinforcement learning (RL) aims to learn transferable policies under dynamics shifts between source and target domains. A key challenge lies in the lack of target-domain environment interaction and reward supervision, which prevents direct policy learning. To address this challenge, we propose Bridging Dynamics Gaps for Cross-Domain Reinforcement Learning (BDGxRL), a novel framework that leverages Diffusion Schrödinger Bridge (DSB) to align source transitions with target-domain dynamics encoded in offline demonstrations. Moreover, we introduce a reward modulation mechanism that estimates rewards based on state transitions, applying to DSB-aligned samples to ensure consistency between rewards and target-domain dynamics. BDGxRL performs target-oriented policy learning entirely within the source domain, without access to the target environment or its rewards. Experiments on MuJoCo cross-domain benchmarks demonstrate that BDGxRL outperforms state-of-the-art baselines and shows strong adaptability under transition dynamics shifts.

</details>


### [94] [OPTIAGENT: A Physics-Driven Agentic Framework for Automated Optical Design](https://arxiv.org/abs/2602.23761)
*Yuyu Geng,Lei Sun,Yao Gao,Xinxin Hu,Zhonghua Yi,Xiaolong Qian,Weijian Hu,Jian Bai,Kaiwei Wang*

Main category: cs.LG

TL;DR: 本研究首次将大语言模型应用于光学设计领域，通过构建OptiDesignQA数据集、采用混合目标函数注入专业知识、使用光学词典奖励引导的DrGRPO算法进行物理驱动的强化学习，并与专业光学优化程序集成，成功实现了非光学专家也能设计功能性镜头系统，在性能上优于传统优化算法和其他LLM方法。


<details>
  <summary>Details</summary>
Motivation: 光学设计是一个高度非凸的优化问题，严重依赖人类启发式专业知识和特定领域知识，而大语言模型虽具备丰富的光学知识，但在实际镜头系统设计中的能力仍然受限，存在专业知识应用鸿沟。

Method: 1) 构建OptiDesignQA数据集，包含经典教材镜头系统和自动化算法生成的新颖配置；2) 通过全系统合成与镜头补全的混合目标向LLM注入光学专业知识；3) 采用基于光学词典奖励的Group Relative Policy Optimization Done Right (DrGRPO)进行物理驱动的策略对齐，奖励机制包含结构格式奖励、物理可行性奖励、光线操控精度和LLM启发式奖励；4) 集成专业光学优化程序进行端到端微调和精度优化。

Result: 在基准测试中，本研究方法相较于传统基于优化的自动化设计算法和其他LLM基线方法均表现出显著优越性，验证了所提方法的有效性。

Conclusion: 本研究成功弥合了光学设计领域的专业知识鸿沟，使未经正式光学训练的用户能够开发功能性镜头系统，为LLM在光学工程领域的应用开辟了重要方向，并展示了物理奖励机制与领域知识注入相结合的强大潜力。

Abstract: Optical design is the process of configuring optical elements to precisely manipulate light for high-fidelity imaging. It is inherently a highly non-convex optimization problem that relies heavily on human heuristic expertise and domain-specific knowledge. While Large Language Models (LLMs) possess extensive optical knowledge, their capabilities in leveraging the knowledge in designing lens system remain significantly constrained. This work represents the first attempt to employ LLMs in the field of optical design. We bridge the expertise gap by enabling users without formal optical training to successfully develop functional lens systems. Concretely, we curate a comprehensive dataset, named OptiDesignQA, which encompasses both classical lens systems sourced from standard optical textbooks and novel configurations generated by automated design algorithms for training and evaluation. Furthermore, we inject domain-specific optical expertise into the LLM through a hybrid objective of full-system synthesis and lens completion. To align the model with optical principles, we employ Group Relative Policy Optimization Done Right (DrGRPO) guided by Optical Lexicographic Reward for physics-driven policy alignment. This reward system incorporates structural format rewards, physical feasibility rewards, light-manipulation accuracy, and LLM-based heuristics. Finally, our model integrates with specialized optical optimization routines for end-to-end fine-tuning and precision refinement. We benchmark our proposed method against both traditional optimization-based automated design algorithms and LLM counterparts, and experimental results show the superiority of our method.

</details>


### [95] [MAGE: Multi-scale Autoregressive Generation for Offline Reinforcement Learning](https://arxiv.org/abs/2602.23770)
*Chenxing Lin,Xinhui Gao,Haipeng Zhang,Xinran Li,Haitao Wang,Songzhu Mei,Chenglu Wen,Weiquan Liu,Siqi Shen,Cheng Wang*

Main category: cs.LG

TL;DR: 本文提出MAGE，一种基于多尺度自回归生成的离线强化学习方法，通过条件引导的多尺度自动编码器和从粗到细生成轨迹的多尺度Transformer，有效捕捉长时序稀疏奖励任务中的多层次时间依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成的离线强化学习方法在长时序稀疏奖励任务中表现不佳，且层次化生成方法未能充分利用轨迹中固有的多尺度时间结构，导致性能次优。

Method: MAGE采用条件引导的多尺度自动编码器学习层次化轨迹表示，并设计多尺度Transformer以从粗到细的时间尺度自回归生成轨迹表示，同时通过条件引导解码器精确控制短期行为。

Result: 在五个离线强化学习基准测试中与十五种基线算法的广泛实验表明，MAGE成功融合了多尺度轨迹建模与条件引导，在长时序稀疏奖励环境中生成了连贯且可控的轨迹。

Conclusion: MAGE能够有效捕捉轨迹的多分辨率时间依赖性，为长时序稀疏奖励离线强化学习问题提供了有效的多尺度生成建模方案。

Abstract: Generative models have gained significant traction in offline reinforcement learning (RL) due to their ability to model complex trajectory distributions. However, existing generation-based approaches still struggle with long-horizon tasks characterized by sparse rewards. Some hierarchical generation methods have been developed to mitigate this issue by decomposing the original problem into shorter-horizon subproblems using one policy and generating detailed actions with another. While effective, these methods often overlook the multi-scale temporal structure inherent in trajectories, resulting in suboptimal performance. To overcome these limitations, we propose MAGE, a Multi-scale Autoregressive GEneration-based offline RL method. MAGE incorporates a condition-guided multi-scale autoencoder to learn hierarchical trajectory representations, along with a multi-scale transformer that autoregressively generates trajectory representations from coarse to fine temporal scales. MAGE effectively captures temporal dependencies of trajectories at multiple resolutions. Additionally, a condition-guided decoder is employed to exert precise control over short-term behaviors. Extensive experiments on five offline RL benchmarks against fifteen baseline algorithms show that MAGE successfully integrates multi-scale trajectory modeling with conditional guidance, generating coherent and controllable trajectories in long-horizon sparse-reward settings.

</details>


### [96] [Provable Subspace Identification of Nonlinear Multi-view CCA](https://arxiv.org/abs/2602.23785)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: 本文研究多视角非线性典型相关分析（CCA）的可识别性，将问题重新框架为基不变子空间识别。证明在适当潜在先验和谱分离条件下，多视角CCA可在视角正交模糊下恢复信号子空间；当视角数N≥3时，可分离所有视角共享的联合相关子空间并消除私有噪声，通过谱扰动理论建立有限样本一致性界，实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 非线性典型相关分析（CCA）是多视角学习的核心方法，但其在非线性场景下的可识别性理论尚不完善。传统精确解混方法被证明是病态问题，导致理论保证缺失。本研究旨在建立非线性多视角CCA的可识别性理论，明确在存在视角私有噪声的情况下，能够从多个非线性观测中可靠恢复何种结构，为多视角表示学习提供理论基础。

Method: 本文提出将多视角CCA重新框架化为基不变子空间识别问题，避免直接求解病态的精确解混。在合适的潜在变量先验和谱分离条件下，利用经验互协方差矩阵的浓度性质，通过谱扰动理论推导子空间误差的显式上界，从而建立有限样本一致性保证。

Result: 理论结果表明：1）在N个视角下，算法可恢复成对相关的信号子空间，至多存在视角正交模糊；2）当N≥3时，目标函数能唯一地分离所有视角共享的联合相关子空间，同时消除视角私有变化；3）基于谱扰动理论获得了显式的子空间估计误差界，证明了有限样本一致性；4）合成数据和渲染图像实验验证了理论发现并确认了假设条件的必要性。

Conclusion: 本研究成功建立了非线性多视角CCA的可识别性理论，通过基不变子空间识别框架解决了精确解混的病态性问题。理论揭示了多视角信息融合的本质优势（N≥3时的唯一可识别性），并通过严格的有限样本分析提供了算法的理论保证。实验结果验证了理论的有效性，为非线性多视角学习提供了坚实的理论基础。

Abstract: We investigate the identifiability of nonlinear Canonical Correlation Analysis (CCA) in a multi-view setup, where each view is generated by an unknown nonlinear map applied to a linear mixture of shared latents and view-private noise. Rather than attempting exact unmixing, a problem proven to be ill-posed, we instead reframe multi-view CCA as a basis-invariant subspace identification problem. We prove that, under suitable latent priors and spectral separation conditions, multi-view CCA recovers the pairwise correlated signal subspaces up to view-wise orthogonal ambiguity. For $N \geq 3$ views, the objective provably isolates the jointly correlated subspaces shared across all views while eliminating view-private variations. We further establish finite-sample consistency guarantees by translating the concentration of empirical cross-covariances into explicit subspace error bounds via spectral perturbation theory. Experiments on synthetic and rendered image datasets validate our theoretical findings and confirm the necessity of the assumed conditions.

</details>


### [97] [GRAIL: Post-hoc Compensation by Linear Reconstruction for Compressed Networks](https://arxiv.org/abs/2602.23795)
*Wenwu Tang,Dong Wang,Lothar Thiele,Olga Saukh*

Main category: cs.LG

TL;DR: 提出GRAIL，一种零微调的后置块级补偿方法，通过Gram矩阵和岭回归重建隐藏表示，无需反向传播即可在各种模型上提升压缩后的精度。


<details>
  <summary>Details</summary>
Motivation: 结构化深度模型压缩虽能降低内存和推理开销，但激进压缩导致的精度损失常需后压缩微调，而标注数据缺失或训练成本过高使微调难以实施。

Method: GRAIL利用小校准集，通过Gram矩阵捕获隐藏激活统计特性，采用岭回归学习从压缩表示到原始表示的线性重建映射，并将其融合到下游投影权重中，实现块级输入输出行为恢复。

Result: 在ResNets、ViTs和解码器-only大语言模型上，GRAIL在实用压缩率下持续优于无数据/数据感知剪枝和折叠基线，仅带来可管理开销且无需梯度回传。

Conclusion: GRAIL提供了一种简单高效、无需微调的后压缩补偿方案，能显著恢复压缩模型精度，且适用于多种选择器和模型架构。

Abstract: Structured deep model compression methods are hardware-friendly and substantially reduce memory and inference costs. However, under aggressive compression, the resulting accuracy degradation often necessitates post-compression finetuning, which can be impractical due to missing labeled data or high training cost. We propose post-hoc blockwise compensation, called GRAIL, a simple zero-finetuning step applied after model compression that restores each block's input-output behavior using a small calibration set. The method summarizes hidden activations via a Gram matrix and applies ridge regression to linearly reconstruct the original hidden representation from the reduced one. The resulting reconstruction map is absorbed into the downstream projection weights, while the upstream layer is compressed. The approach is selector-agnostic (Magnitude, Wanda, Gram-based selection, or folding), data-aware (requiring only a few forward passes without gradients or labels), and recovers classic pruning or folding when the Gram matrix is near identity, indicating weak inter-channel correlations. Across ResNets, ViTs, and decoder-only LLMs, GRAIL consistently improves accuracy or perplexity over data-free and data-aware pruning or folding baselines in practical compression regimes, with manageable overhead and no backpropagation. The code is available at https://github.com/TWWinde/GRAIL.

</details>


### [98] [MPU: Towards Secure and Privacy-Preserving Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2602.23798)
*Tiantong Wang,Xinyu Yan,Tiantong Wu,Yurong Hao,Yong Jiang,Fei Huang,Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: 针对大型语言模型机器遗忘中的隐私困境（服务器参数与客户端遗忘集均不能共享），本文提出MPU框架，通过预处理的随机化副本生成和后处理的更新聚合，在保护双方隐私的前提下实现高效遗忘。在10%噪声下，7种算法的平均性能退化低于1%，部分算法在1%噪声下甚至优于无噪声基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的机器遗忘面临双重非披露约束：服务器无法共享模型参数，客户端无法共享遗忘数据集。这导致传统遗忘方法难以同时保护双方隐私。需要一种隐私保护框架，使服务器能在不泄露原始参数的情况下协助客户端完成遗忘，同时客户端也能在不暴露遗忘数据的情况下进行本地更新。

Method: 提出MPU（多扰动副本遗忘）框架，包含两个服务器端模块：1）预处理模块：生成多个扰动并重参数化的模型实例，客户端基于私有遗忘集进行本地遗忘；2）后处理模块：通过逆重参数化和谐波去噪聚合更新，减轻扰动影响。该框架与算法无关，适用于多种遗忘方法。

Result: 在7种遗忘算法上的实验表明：MPU在10%噪声水平下达到与无噪声基线相当的性能，平均性能退化低于1%；在1%噪声下，部分算法甚至优于无噪声基线。证明了该框架在保护隐私的同时能维持高效的遗忘性能。

Conclusion: MPU框架成功解决了机器遗忘中的隐私困境，实现了服务器参数和客户端遗忘集的双重保护。该框架具有算法无关性、实现简单且性能优越的特点，为大型语言模型的隐私保护遗忘提供了可行解决方案。

Abstract: Machine unlearning for large language models often faces a privacy dilemma in which strict constraints prohibit sharing either the server's parameters or the client's forget set. To address this dual non-disclosure constraint, we propose MPU, an algorithm-agnostic privacy-preserving Multiple Perturbed Copies Unlearning framework that primarily introduces two server-side modules: Pre-Process for randomized copy generation and Post-Process for update aggregation. In Pre-Process, the server distributes multiple perturbed and reparameterized model instances, allowing the client to execute unlearning locally on its private forget set without accessing the server's exact original parameters. After local unlearning, the server performs Post-Process by inverting the reparameterization and aggregating updates with a harmonic denoising procedure to alleviate the impact of perturbation. Experiments with seven unlearning algorithms show that MPU achieves comparable unlearning performance to noise-free baselines, with most algorithms' average degradation well below 1% under 10% noise, and can even outperform the noise-free baseline for some algorithms under 1% noise. Code is available at https://github.com/Tristan-SHU/MPU.

</details>


### [99] [Beyond State-Wise Mirror Descent: Offline Policy Optimization with Parameteric Policies](https://arxiv.org/abs/2602.23811)
*Xiang Li,Nan Jiang,Yuheng Zhang*

Main category: cs.LG

TL;DR: 本文研究一般函数近似下的离线强化学习理论，针对现有算法（如PSPI）仅适用于有限小动作空间且需隐式诱导策略的局限，将理论保证扩展至大规模/连续动作空间的参数化策略类。通过识别上下文耦合的核心难点并连接镜像下降与自然策略梯度，获得了新颖的理论分析、保证与算法洞见，并揭示了离线强化学习与模仿学习之间的意外统一性。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习理论虽通过悲观主义可学习良好策略，但仅限于有限动作空间且需依赖状态镜像下降和从价值函数隐式诱导策略，无法适应实践中广泛使用的独立参数化策略表示，尤其在大规模/连续动作空间中。为此，本文致力于将理论保证扩展至更实际的参数化策略设定。

Method: 将镜像下降扩展至参数化策略类，识别"上下文耦合"为核心难点，并通过建立镜像下降与自然策略梯度的联系，发展新的理论分析与算法框架。

Result: 获得了参数化策略类在离线强化学习下的新理论分析与保证，解决了上下文耦合难题，得到了算法层面的新洞见，并发现了离线强化学习与模仿学习之间的意外统一性。

Conclusion: 本研究成功将离线强化学习的理论保证推广至更符合实际的大规模/连续动作空间参数化策略场景，通过镜像下降与自然策略梯度的桥梁，不仅攻克了上下文耦合难题，还揭示了离线强化学习与模仿学习之间的深刻内在联系。

Abstract: We investigate the theoretical aspects of offline reinforcement learning (RL) under general function approximation. While prior works (e.g., Xie et al., 2021) have established the theoretical foundations of learning a good policy from offline data via pessimism, existing algorithms that are computationally tractable (often in an oracle-efficient sense), such as PSPI, only apply to finite and small action spaces. Moreover, these algorithms rely on state-wise mirror descent and require actors to be implicitly induced from the critic functions, failing to accommodate standalone policy parameterization which is ubiquitous in practice. In this work, we address these limitations and extend the theoretical guarantees to parameterized policy classes over large or continuous action spaces. When extending mirror descent to parameterized policies, we identify contextual coupling as the core difficulty, and show how connecting mirror descent to natural policy gradient leads to novel analyses, guarantees, and algorithmic insights, including a surprising unification between offline RL and imitation learning.

</details>


### [100] [Learning to maintain safety through expert demonstrations in settings with unknown constraints: A Q-learning perspective](https://arxiv.org/abs/2602.23816)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: 本文提出SafeQIL算法，解决在未知约束条件下从安全演示轨迹中学习策略的问题。通过Q值融合任务奖励与安全性评估，平衡保守性与高回报探索，最大化有前景轨迹的概率。


<details>
  <summary>Details</summary>
Motivation: 在约束马尔可夫决策过程中，人类演示提供了安全执行任务的轨迹，但约束条件与成本函数未知。需要学习一个策略，既能拟合高奖励轨迹，又能确保安全性，在保守遵循演示与探索潜在高回报但可能不安全的动作之间取得平衡。

Method: 提出安全Q逆约束强化学习(SafeQIL)算法。将状态-动作对的"前景"量化为Q值，该值综合任务特定奖励和状态安全性评估。从逆强化学习视角，在约束条件下实现安全Q学习，旨在学习一个能最大化最有前景轨迹概率的策略。

Result: 在具有挑战性的基准任务上，SafeQIL与现有先进逆约束强化学习算法进行对比实验，展示了其性能优势。

Conclusion: 该方法有效解决了未知约束下从演示中学习安全策略的核心问题，通过Q值融合奖励与安全评估，能够在保持安全性的同时识别并执行有前景的轨迹，为约束强化学习提供了新思路。

Abstract: Given a set of trajectories demonstrating the execution of a task safely in a constrained MDP with observable rewards but with unknown constraints and non-observable costs, we aim to find a policy that maximizes the likelihood of demonstrated trajectories trading the balance between being conservative and increasing significantly the likelihood of high-rewarding trajectories but with potentially unsafe steps. Having these objectives, we aim towards learning a policy that maximizes the probability of the most $promising$ trajectories with respect to the demonstrations. In so doing, we formulate the ``promise" of individual state-action pairs in terms of $Q$ values, which depend on task-specific rewards as well as on the assessment of states' safety, mixing expectations in terms of rewards and safety. This entails a safe Q-learning perspective of the inverse learning problem under constraints: The devised Safe $Q$ Inverse Constrained Reinforcement Learning (SafeQIL) algorithm is compared to state-of-the art inverse constraint reinforcement learning algorithms to a set of challenging benchmark tasks, showing its merits.

</details>


### [101] [Inferring Chronic Treatment Onset from ePrescription Data: A Renewal Process Approach](https://arxiv.org/abs/2602.23824)
*Pavlin G. Poličar,Dalibor Stanimirović,Blaž Zupan*

Main category: cs.LG

TL;DR: 针对电子健康记录左删失导致疾病发病时间难以确定的问题，本研究提出一种基于处方续签动态的概率框架，通过检测从零星用药到持续治疗的转变来推断慢性病发病时间。


<details>
  <summary>Details</summary>
Motivation: 纵向电子健康记录(EHR)数据常存在左删失，使得诊断记录不完整且不可靠，难以准确判断疾病发病时间。门诊处方数据则形成续签轨迹，提供了疾病管理的连续信号，可作为推断发病时间的替代数据源。

Method: 提出概率框架，将处方动态建模为续签过程，采用变点检测方法在两个体制间识别转变：基线泊松模型（代表零星处方）和威布尔续签模型（代表持续治疗）。

Result: 基于240万人全国性电子处方数据集验证显示：相比朴素规则方法，该框架产生更合理的时间发病估计，在强左删失条件下显著减少不合理早期检测；检测性能因疾病类型而异，且与处方密度高度相关。

Conclusion: 该方法有效解决了EHR左删失问题，为基于治疗数据推断疾病发病时间提供了可靠的概率建模框架，但性能受限于处方密度，揭示了治疗基础发病推断的优势与局限。

Abstract: Longitudinal electronic health record (EHR) data are often left-censored, making diagnosis records incomplete and unreliable for determining disease onset. In contrast, outpatient prescriptions form renewal-based trajectories that provide a continuous signal of disease management. We propose a probabilistic framework to infer chronic treatment onset by modeling prescription dynamics as a renewal process and detecting transitions from sporadic to sustained therapy via change-point detection between a baseline Poisson (sporadic prescribing) regime and a regime-specific Weibull (sustained therapy) renewal model. Using a nationwide ePrescription dataset of 2.4 million individuals, we show that the approach yields more temporally plausible onset estimates than naive rule-based triggering, substantially reducing implausible early detections under strong left censoring. Detection performance varies across diseases and is strongly associated with prescription density, highlighting both the strengths and limits of treatment-based onset inference.

</details>


### [102] [FedNSAM:Consistency of Local and Global Flatness for Federated Learning](https://arxiv.org/abs/2602.23827)
*Junkang Liu,Fanhua Shang,Yuxuan Tian,Hongying Liu,Yuanyuan Liu*

Main category: cs.LG

TL;DR: 该论文针对联邦学习中因多步本地更新与数据异质性导致的全局模型性能退化问题，提出FedNSAM算法。通过引入全局Nesterov动量协调全局与局部平坦度一致性，理论证明其较FedSAM具有更紧的收敛界，并在CNN与Transformer模型上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的多步本地更新与数据异质性通常导致更尖锐的全局最小值，从而损害模型泛化能力。现有方法将锐度感知最小化(SAM)引入本地训练，但在高度异质数据下，局部平坦性无法保证全局平坦性，使得SAM在联邦学习中的有效性受限。

Method: 定义平坦度距离解释上述现象，提出FedNSAM算法。该算法将全局Nesterov动量作为客户端全局扰动估计与外推方向，嵌入到本地更新过程中，以实现全局与局部平坦度的协调一致。

Result: 理论上通过Nesterov外推获得了比FedSAM更紧的收敛边界。在CNN和Transformer架构上的广泛实验表明，FedNSAM在性能和效率方面均优于基线方法。

Conclusion: FedNSAM通过全局Nesterov动量机制有效解决了联邦学习中局部与全局平坦度不一致的核心问题，为数据异质性场景下的联邦学习提供了更优的理论保证和实践效果。

Abstract: In federated learning (FL), multi-step local updates and data heterogeneity usually lead to sharper global minima, which degrades the performance of the global model. Popular FL algorithms integrate sharpness-aware minimization (SAM) into local training to address this issue. However, in the high data heterogeneity setting, the flatness in local training does not imply the flatness of the global model. Therefore, minimizing the sharpness of the local loss surfaces on the client data does not enable the effectiveness of SAM in FL to improve the generalization ability of the global model. We define the \textbf{flatness distance} to explain this phenomenon. By rethinking the SAM in FL and theoretically analyzing the \textbf{flatness distance}, we propose a novel \textbf{FedNSAM} algorithm that accelerates the SAM algorithm by introducing global Nesterov momentum into the local update to harmonize the consistency of global and local flatness. \textbf{FedNSAM} uses the global Nesterov momentum as the direction of local estimation of client global perturbations and extrapolation. Theoretically, we prove a tighter convergence bound than FedSAM by Nesterov extrapolation. Empirically, we conduct comprehensive experiments on CNN and Transformer models to verify the superior performance and efficiency of \textbf{FedNSAM}. The code is available at https://github.com/junkangLiu0/FedNSAM.

</details>


### [103] [ULW-SleepNet: An Ultra-Lightweight Network for Multimodal Sleep Stage Scoring](https://arxiv.org/abs/2602.23852)
*Zhaowen Wang,Dongdong Zhou,Qi Xu,Fengyu Cong,Mohammad Al-Sa'd,Jenni Raitoharju*

Main category: cs.LG

TL;DR: 本文提出ULW-SleepNet，一种超轻量级多模态睡眠分期评分框架，通过创新的双流可分离卷积块和参数共享机制，显著降低了模型复杂度（仅13.3K参数），同时在标准数据集上保持高准确率（86.9%/81.4%），为可穿戴设备上的实时睡眠监测提供了可行方案。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在自动睡眠分期方面取得了进展，但现有模型计算开销大且多为单通道脑电设计，难以有效处理多模态多导睡眠图数据，限制了其在可穿戴设备和物联网场景中的实际应用。因此需要开发既高效又准确的多模态轻量级模型。

Method: 提出ULW-SleepNet框架，核心包括：1）双流可分离卷积块高效融合多生理信号特征；2）深度可分离卷积大幅减少计算量；3）通道级参数共享进一步压缩模型规模；4）全局平均池化替代全连接层。该架构专为多模态PSG数据优化。

Result: 在Sleep-EDF-20和Sleep-EDF-78数据集上分别达到86.9%和81.4%的准确率，模型仅含13.3K参数和7.89M浮点运算量。与现有最优方法相比，参数量减少高达98.6%而性能损失极小，计算效率显著提升。

Conclusion: ULW-SleepNet证明了在保持竞争力准确率的前提下，通过创新架构设计可实现睡眠分期模型的极致轻量化，为资源受限的可穿戴设备和物联网平台上的实时睡眠监测应用铺平了道路，具有明确的临床转化价值。

Abstract: Automatic sleep stage scoring is crucial for the diagnosis and treatment of sleep disorders. Although deep learning models have advanced the field, many existing models are computationally demanding and designed for single-channel electroencephalography (EEG), limiting their practicality for multimodal polysomnography (PSG) data. To overcome this, we propose ULW-SleepNet, an ultra-lightweight multimodal sleep stage scoring framework that efficiently integrates information from multiple physiological signals. ULW-SleepNet incorporates a novel Dual-Stream Separable Convolution (DSSC) Block, depthwise separable convolutions, channel-wise parameter sharing, and global average pooling to reduce computational overhead while maintaining competitive accuracy. Evaluated on the Sleep-EDF-20 and Sleep-EDF-78 datasets, ULW-SleepNet achieves accuracies of 86.9% and 81.4%, respectively, with only 13.3K parameters and 7.89M FLOPs. Compared to state-of-the-art methods, our model reduces parameters by up to 98.6% with only marginal performance loss, demonstrating its strong potential for real-time sleep monitoring on wearable and IoT devices. The source code for this study is publicly available at https://github.com/wzw999/ULW-SLEEPNET.

</details>


### [104] [A Theory of Random Graph Shift in Truncated-Spectrum vRKHS](https://arxiv.org/abs/2602.23880)
*Zhang Wan,Tingting Mu,Samuel Kaski*

Main category: cs.LG

TL;DR: 本文通过随机图生成模型(RGM)理论框架研究域偏移下的图分类问题，基于向量值再生核希尔伯特空间(vRKHS)推导出泛化误差界，其偏移惩罚可分解为域差异、谱几何和振幅三项，并通过实验验证了各项的理论洞察。


<details>
  <summary>Details</summary>
Motivation: 经典域适应理论虽能处理图分布偏移，但较少利用图的结构化信息本身。图的非欧几里得特性和专用学习架构使得对图分布偏移进行细粒度分析变得复杂，需要新的理论框架来精细刻画偏移机制。

Method: 假设随机图模型为数据生成过程，利用其与函数空间假设复杂度的联系，构建基于向量值再生核希尔伯特空间(vRKHS)的理论形式化体系，从而推导泛化性能界。

Result: 推导出一个泛化误差界，其域偏移惩罚项可分解为三个部分：(i)域差异项，(ii)由可及截断谱总结的谱几何项，(iii)聚合收敛性与构造稳定性效应的振幅项。

Conclusion: 该理论为图域偏移提供了细粒度分析工具，三项分解揭示了偏移的不同来源，实验验证了理论有效性，为设计更鲁棒的图域适应算法提供了理论指导。

Abstract: This paper develops a theory of graph classification under domain shift through a random-graph generative lens, where we consider intra-class graphs sharing the same random graph model (RGM) and the domain shift induced by changes in RGM components. While classic domain adaptation (DA) theories have well-underpinned existing techniques to handle graph distribution shift, the information of graph samples, which are itself structured objects, is less explored. The non-Euclidean nature of graphs and specialized architectures for graph learning further complicate a fine-grained analysis of graph distribution shifts. In this paper, we propose a theory that assumes RGM as the data generative process, exploiting its connection to hypothesis complexity in function space perspective for such fine-grained analysis. Building on a vector-valued reproducing kernel Hilbert space (vRKHS) formulation, we derive a generalization bound whose shift penalty admits a factorization into (i) a domain discrepancy term, (ii) a spectral-geometry term summarized by the accessible truncated spectrum, and (iii) an amplitude term that aggregates convergence and construction-stability effects. We empirically verify the insights on these terms in both real data and simulations.

</details>


### [105] [RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models](https://arxiv.org/abs/2602.24040)
*Daniel Yang,Samuel Stante,Florian Redhardt,Lena Libon,Parnian Kassraie,Ido Hakimi,Barna Pásztor,Andreas Krause*

Main category: cs.LG

TL;DR: 本文提出RewardUQ统一框架，系统评估奖励模型的不确定性量化方法，实验发现模型规模和初始化对性能影响最为关键，并开源了Python工具包。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖点估计，忽略了有限人类反馈带来的认知不确定性。量化不确定性可降低标注成本并缓解过度优化，但缺乏系统比较，导致理解不足。

Method: 构建RewardUQ框架，在准确性和校准指标下对比常见方法，并提出融合双维度的新排序策略以简化评估。

Result: 实验显示模型大小和初始化对性能影响最显著，多数先前研究若采用替代设计选择可获得更好效果。

Conclusion: 为促进新方法开发与下游应用部署，开源RewardUQ Python包。

Abstract: Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq.

</details>


### [106] [MINT: Multimodal Imaging-to-Speech Knowledge Transfer for Early Alzheimer's Screening](https://arxiv.org/abs/2602.23994)
*Vrushank Ahire,Yogesh Kumar,Anouck Girard,M. A. Ganaie*

Main category: cs.LG

TL;DR: 提出MINT多模态框架，通过将MRI生物标志物结构迁移到语音编码器，实现阿尔茨海默病早期筛查。该方法在不依赖MRI扫描的前提下，使语音分析获得与纯语音基线相当的性能，同时保持生物学依据，为大规模认知筛查提供新路径。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病中轻度认知障碍（MCI）是衰老向痴呆转变的关键阶段。结构MRI虽能提供可靠生物标志物，但成本高且需专业设备，难以大规模应用。语音分析虽无创便捷，但现有语音分类器独立于神经影像开发，缺乏生物学基础，在区分认知正常（CN）与MCI时可靠性有限。亟需一种方法，既能利用MRI的生物学有效性，又能发挥语音分析的可及性优势。

Method: MINT框架采用三阶段跨模态知识迁移：1）训练基于1228名受试者的MRI教师模型，构建CN与MCI分类的神经影像嵌入空间；2）通过残差投影头将语音表征对齐到冻结的影像流形，结合几何损失保持影像编码器保真度；3）推理时使用冻结的MRI分类器对对齐后的语音嵌入进行分类，无需扫描仪。关键设计包括：冻结MRI流形、残差投影结构、几何损失函数。

Result: 在ADNI-4数据集上，对齐后的语音AUC达0.720，与纯语音基线（0.711）相当，且无需影像数据；多模态融合AUC达0.973，显著优于单独MRI（0.958）。消融实验表明，dropout正则化和自监督预训练是框架成功的关键设计。该工作首次证实了MRI向语音的知识迁移在阿尔茨海默病早期筛查中的可行性。

Conclusion: 本研究首次实现了MRI到语音的知识迁移用于阿尔茨海默病早期筛查，建立了无需推理期神经影像的生物学grounded路径，为大规模认知功能分诊提供了可扩展的解决方案，兼具生物学有效性与临床可及性。

Abstract: Alzheimer's disease is a progressive neurodegenerative disorder in which mild cognitive impairment (MCI) marks a critical transition between aging and dementia. Neuroimaging modalities, such as structural MRI, provide biomarkers of this transition; however, their high costs and infrastructure needs limit their deployment at a population scale. Speech analysis offers a non-invasive alternative, but speech-only classifiers are developed independently of neuroimaging, leaving decision boundaries biologically ungrounded and limiting reliability on the subtle CN-versus-MCI distinction. We propose MINT (Multimodal Imaging-to-Speech Knowledge Transfer), a three-stage cross-modal framework that transfers biomarker structure from MRI into a speech encoder at training time. An MRI teacher, trained on 1,228 subjects, defines a compact neuroimaging embedding space for CN-versus-MCI classification. A residual projection head aligns speech representations to this frozen imaging manifold via a combined geometric loss, adapting speech to the learned biomarker space while preserving imaging encoder fidelity. The frozen MRI classifier, which is never exposed to speech, is applied to aligned embeddings at inference and requires no scanner. Evaluation on ADNI-4 shows aligned speech achieves performance comparable to speech-only baselines (AUC 0.720 vs 0.711) while requiring no imaging at inference, demonstrating that MRI-derived decision boundaries can ground speech representations. Multimodal fusion improves over MRI alone (0.973 vs 0.958). Ablation studies identify dropout regularization and self-supervised pretraining as critical design decisions. To our knowledge, this is the first demonstration of MRI-to-speech knowledge transfer for early Alzheimer's screening, establishing a biologically grounded pathway for population-level cognitive triage without neuroimaging at inference.

</details>


### [107] [Foundation World Models for Agents that Learn, Verify, and Adapt Reliably Beyond Static Environments](https://arxiv.org/abs/2602.23997)
*Florent Delgrange*

Main category: cs.LG

TL;DR: 本文提出"基础世界模型"框架，通过可学习奖励模型、自适应形式验证、在线抽象校准和测试时合成四个核心组件，解决当前世界模型在固定环境假设下无法支持智能体在开放世界中持续演进的问题，旨在实现智能体的可靠适应、可验证程序合成与少样本策略学习。


<details>
  <summary>Details</summary>
Motivation: 标准方法假设任务和环境固定且缺乏新颖性，严重限制了世界模型支持智能体在动态变化条件下自主演进策略的能力。下一代自主智能体必须在开放世界中同时实现高效学习、可靠行动和行为自适应，现有范式无法兼顾这三者。

Method: 提出包含四个组件的研究议程：1) 从规范中学习可微分奖励模型，支持目标导向优化；2) 将自适应形式验证机制深度整合至学习全流程；3) 通过在线抽象校准量化模型预测的不确定性；4) 在测试时由验证器引导进行程序合成与动态世界模型生成。四者协同实现表示、学习与推理的统一。

Result: 该框架使智能体能够：合成可通过形式验证的可靠程序、从极少量交互样本中快速推导新策略、在适应环境新颖性的同时严格保持行为正确性，并建立学习-推理-适应的闭环机制。

Conclusion: 基础世界模型作为支撑学习、推理与适应的基质，不仅为智能体提供高性能行为能力，更重要的是赋予其解释与论证行为合理性的可解释性，为下一代开放世界自主智能体的实现奠定了理论基础。

Abstract: The next generation of autonomous agents must not only learn efficiently but also act reliably and adapt their behavior in open worlds. Standard approaches typically assume fixed tasks and environments with little or no novelty, which limits world models' ability to support agents that must evolve their policies as conditions change. This paper outlines a vision for foundation world models: persistent, compositional representations that unify reinforcement learning, reactive/program synthesis, and abstraction mechanisms. We propose an agenda built around four components: (i) learnable reward models from specifications to support optimization with clear objectives; (ii) adaptive formal verification integrated throughout learning; (iii) online abstraction calibration to quantify the reliability of the model's predictions; and (iv) test-time synthesis and world-model generation guided by verifiers. Together, these components enable agents to synthesize verifiable programs, derive new policies from a small number of interactions, and maintain correctness while adapting to novelty. The resulting framework positions foundation world models as a substrate for learning, reasoning, and adaptation, laying the groundwork for agents that not only act well but can explain and justify the behavior they adopt.

</details>


### [108] [InfoNCE Induces Gaussian Distribution](https://arxiv.org/abs/2602.24012)
*Roy Betser,Eyal Gofer,Meir Yossef Levi,Guy Gilboa*

Main category: cs.LG

TL;DR: 该论文证明InfoNCE对比学习目标会诱导表示空间产生高斯结构，在两种互补的理论框架下分析表示投影的渐近高斯性，并通过合成数据和CIFAR-10实验验证，为对比学习表示提供了理论解释基础。


<details>
  <summary>Details</summary>
Motivation: 对比学习中普遍观察到表示呈现高斯分布特性，但缺乏理论解释。本研究旨在揭示InfoNCE目标函数与高斯结构之间的本质联系，为理解对比学习表示提供理论基础，并支持后续应用开发。

Method: 研究采用双重分析框架：(1)在一定的对齐性和集中性假设下，证明高维表示投影渐近服从多元高斯分布；(2)在更宽松的假设下，通过添加促进低特征范数和高特征熵的微小渐近消失正则项，获得相似渐近结果。实验在合成数据集和CIFAR-10上，使用多种编码器架构进行验证。

Result: 理论证明InfoNCE目标确实会在表示中诱导高斯结构；实验验证了在多种架构和数据集上，对比学习表示始终表现出高斯行为；所提出的正则化方法在较弱假设下同样能实现高斯结构。

Conclusion: 该发现为对比学习中表示的高斯现象提供了原理性解释，所建立的高斯模型使得对学习表示的解析处理成为可能，预期将支持对比学习在各类应用中的进一步发展。

Abstract: Contrastive learning has become a cornerstone of modern representation learning, allowing training with massive unlabeled data for both task-specific and general (foundation) models. A prototypical loss in contrastive training is InfoNCE and its variants. In this work, we show that the InfoNCE objective induces Gaussian structure in representations that emerge from contrastive training. We establish this result in two complementary regimes. First, we show that under certain alignment and concentration assumptions, projections of the high-dimensional representation asymptotically approach a multivariate Gaussian distribution. Next, under less strict assumptions, we show that adding a small asymptotically vanishing regularization term that promotes low feature norm and high feature entropy leads to similar asymptotic results. We support our analysis with experiments on synthetic and CIFAR-10 datasets across multiple encoder architectures and sizes, demonstrating consistent Gaussian behavior. This perspective provides a principled explanation for commonly observed Gaussianity in contrastive representations. The resulting Gaussian model enables principled analytical treatment of learned representations and is expected to support a wide range of applications in contrastive learning.

</details>


### [109] [pathsig: A GPU-Accelerated Library for Truncated and Projected Path Signatures](https://arxiv.org/abs/2602.24066)
*Tobias Nygaard*

Main category: cs.LG

TL;DR: 本文提出pathsig，一个PyTorch原生库，用于在字基中高效计算路径签名。通过CUDA内核在前缀闭字集上并行更新签名系数，该库实现了高GPU吞吐量和近最小峰值内存，相比现有库在截断签名计算上提速10-30倍，在需要反向传播的训练中提速4-10倍，并支持用户自定义字集投影和非均匀截断，以生成更紧凑的表示。


<details>
  <summary>Details</summary>
Motivation: 尽管路径签名在序列数据表示中具有理论保证和良好性能，并从固定特征提取器发展为机器学习模型的可训练组件，但现有库在大规模基于梯度的学习中缺乏所需的可扩展性。

Method: 开发PyTorch原生库pathsig，直接在字基中计算路径签名；利用CUDA内核在前缀闭字集上并行更新签名系数；支持将无限维签名投影到用户指定字集以及基于非均匀路径规则性的各向异性截断。

Result: 实现了高GPU吞吐量和接近最小的峰值内存占用。与其他库相比，截断签名计算速度提升10-30倍，需要反向传播的训练过程加速4-10倍。

Conclusion: pathsig通过GPU优化和灵活截断策略，为大规模机器学习应用提供了可扩展的路径签名计算解决方案，能够有效降低维度和计算成本。

Abstract: Path signatures provide a rich representation of sequential data, with strong theoretical guarantees and good performance in a variety of machine-learning tasks. While signatures have progressed from fixed feature extractors to trainable components of machine-learning models, existing libraries often lack the required scalability for large-scale, gradient-based learning. To address this gap, this paper introduces pathsig, a PyTorch-native library that computes path signatures directly in the word basis. By using CUDA kernels to update signature coefficients in parallel over prefix-closed word sets, pathsig achieves high GPU throughput and near-minimal peak memory. Compared with other libraries, pathsig achieves 10-30x speedups for computation of truncated signatures and up to 4-10x speedups in training that require backpropagation through the signature. Beyond regular truncation, pathsig supports projections of the (infinite-dimensional) signature onto user-specified sets of words and anisotropic truncation motivated by inhomogeneous path regularity, enabling more compact representations that can reduce dimensionality, redundancy, and computational cost.

</details>


### [110] [Leveraging Non-linear Dimension Reduction and Random Walk Co-occurrence for Node Embedding](https://arxiv.org/abs/2602.24069)
*Ryan DeWolfe*

Main category: cs.LG

TL;DR: 本文提出COVE——一种可解释的高维节点嵌入方法，通过非线性降维技术突破传统低维约束，在UMAP降维后略微提升了聚类与链路预测性能，其UMAP-HDBSCAN流程在社区检测基准上与Louvain算法表现相当。


<details>
  <summary>Details</summary>
Motivation: 传统节点嵌入方法受限于低维表示瓶颈且缺乏可解释性。本研究旨在利用高维嵌入空间保留更多信息，并通过非线性降维技术实现可解释性，从而提升节点分析任务的性能表现。

Method: 受神经嵌入方法启发，基于随机游走共现作为相似性指标构建高维嵌入，与扩散过程密切相关。采用UMAP进行非线性降维，并扩展社区检测基准进行评估。

Result: COVE经UMAP降维后，在聚类和链路预测任务上性能较基线方法略有提升。在扩展的社区检测基准测试中，COVE-UMAP-HDBSCAN全流程与广泛使用的Louvain算法性能相近。

Conclusion: COVE验证了高维可解释嵌入结合非线性降维的有效性，在保持可解释性的同时，各项任务性能达到或接近主流算法水平，为节点表示学习提供了新思路。

Abstract: Leveraging non-linear dimension reduction techniques, we remove the low dimension constraint from node embedding and propose COVE, an explainable high dimensional embedding that, when reduced to low dimension with UMAP, slightly increases performance on clustering and link prediction tasks. The embedding is inspired by neural embedding methods that use co-occurrence on a random walk as an indication of similarity, and is closely related to a diffusion process. Extending on recent community detection benchmarks, we find that a COVE UMAP HDBSCAN pipeline performs similarly to the popular Louvain algorithm.

</details>


### [111] [Neural Diffusion Intensity Models for Point Process Data](https://arxiv.org/abs/2602.24083)
*Xinlong Du,Harsha Honnappa,Vinayak Rao*

Main category: cs.LG

TL;DR: 针对Cox过程的非参数强度估计和事后推断难题，提出神经扩散强度模型(NDIM)，基于神经随机微分方程和扩大滤波理论，证明条件化观测后强度保持扩散结构并通过显式漂移校正使变分族包含真实后验，单向前向传播替代重复MCMC实现数量级加速。


<details>
  <summary>Details</summary>
Motivation: Cox过程虽能建模过度离散点过程数据，但其非参数强度估计和事后推断通常计算难解，依赖昂贵的MCMC方法，难以扩展至大规模应用，亟需高效推断框架。

Method: 提出基于神经随机微分方程的变分推断框架，利用扩大滤波理论证明观测条件下潜强度仍保持扩散结构；设计可摊销编码器通过模拟漂移校正SDE将可变长度事件序列映射至后验强度路径，实现单次前向传播。

Result: 理论保证变分族包含真实后验且ELBO最大化等价于最大似然估计；实验表明能准确恢复强度动态，相较MCMC方法获得数个数量级的速度提升。

Conclusion: 该工作为Cox过程提供了理论严谨且计算高效的变分推断新范式，通过神经SDE与漂移校正机制，在保持推断准确性的同时实现大规模点过程分析的可扩展性。

Abstract: Cox processes model overdispersed point process data via a latent stochastic intensity, but both nonparametric estimation of the intensity model and posterior inference over intensity paths are typically intractable, relying on expensive MCMC methods. We introduce Neural Diffusion Intensity Models, a variational framework for Cox processes driven by neural SDEs. Our key theoretical result, based on enlargement of filtrations, shows that conditioning on point process observations preserves the diffusion structure of the latent intensity with an explicit drift correction. This guarantees the variational family contains the true posterior, so that ELBO maximization coincides with maximum likelihood estimation under sufficient model capacity. We design an amortized encoder architecture that maps variable-length event sequences to posterior intensity paths by simulating the drift-corrected SDE, replacing repeated MCMC runs with a single forward pass. Experiments on synthetic and real-world data demonstrate accurate recovery of latent intensity dynamics and posterior paths, with orders-of-magnitude speedups over MCMC-based methods.

</details>


### [112] [Learning with a Budget: Identifying the Best Arm with Resource Constraints](https://arxiv.org/abs/2602.24146)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 针对资源约束下的最佳臂识别问题，本文提出资源配给逐次折半算法(SH-RR)，将资源感知分配机制融入经典逐次折半框架，统一了随机和确定性消耗设置的理论分析，并定义了新的有效消耗度量。


<details>
  <summary>Details</summary>
Motivation: 在许多应用场景中，评估不同替代方案的成本或资源消耗存在显著异质性。传统最佳臂识别方法未考虑资源限制，而实际资源往往有限。因此，研究如何在资源约束下高效识别最佳方案具有重要的理论和实践意义。

Method: 提出基于资源配给的逐次折半算法(SH-RR)，通过在每一轮中根据资源约束动态配给资源给候选臂，并结合经典逐次折半的淘汰机制，逐步缩小搜索空间，最终识别最优臂。该算法同时适用于随机和确定性资源消耗设置。

Result: 主要贡献包括：1) 提出了统一的SH-RR算法框架，解决了BAIwRC问题；2) 在随机和确定性两种资源消耗模型下建立了系统的理论分析；3) 引入了新的有效消耗度量指标，为算法性能评估提供了理论工具。

Conclusion: 本研究成功扩展了经典最佳臂识别方法，通过资源配给机制使其能够处理资源约束场景，为实际应用中有限资源下的决策优化提供了有效的算法解决方案和理论支撑。

Abstract: In many applications, evaluating the effectiveness of different alternatives comes with varying costs or resource usage. Motivated by such heterogeneity, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem, where an agent seeks to identify the best alternative (aka arm) in the presence of resource constraints. Each arm pull consumes one or more types of limited resources. We make two key contributions. First, we propose the Successive Halving with Resource Rationing (SH-RR) algorithm, which integrates resource-aware allocation into the classical successive halving framework on best arm identification. The SH-RR algorithm unifies the theoretical analysis for both the stochastic and deterministic consumption settings, with a new \textit{effective consumption measure

</details>


### [113] [Sandwiching Polynomials for Geometric Concepts with Low Intrinsic Dimension](https://arxiv.org/abs/2602.24178)
*Adam R. Klivans,Konstantinos Stavropoulos,Arsen Vasilyan*

Main category: cs.LG

TL;DR: 本文提出了一种构造低次夹逼多项式的新方法，在Gaussian分布下对k个半空间的函数实现了poly(k)次逼近，相比之前2^O(k)次有指数级改进，且方法更简单，直接利用边界光滑性和高维逼近理论。


<details>
  <summary>Details</summary>
Motivation: 低次夹逼多项式在分布偏移、可测试学习及污染学习等挑战性学习场景中展现出意外强大能力，但此前对基本函数类（如k半空间函数）的构造存在指数级次数上界，亟需改进。

Method: 利用目标函数边界的平滑性构造夹逼Lipschitz函数，结合高维逼近理论，避免使用复杂的FT-磨光法，证明过程简洁直接。

Result: 1) 对Gaussian分布下k半空间函数获得poly(k)次夹逼多项式，相比之前2^O(k)次实现指数级提升；2) 对低维多项式阈值函数（PTFs）获得双重指数级改进，且无需Kane的FT-磨光技术。

Conclusion: 本方法为夹逼多项式构造提供了普适性改进，特别对低维光滑边界函数类，在次数上取得突破性优化，为高维学习理论提供了更强大工具。

Abstract: Recent work has shown the surprising power of low-degree sandwiching polynomial approximators in the context of challenging learning settings such as learning with distribution shift, testable learning, and learning with contamination. A pair of sandwiching polynomials approximate a target function in expectation while also providing pointwise upper and lower bounds on the function's values. In this paper, we give a new method for constructing low-degree sandwiching polynomials that yield greatly improved degree bounds for several fundamental function classes and marginal distributions. In particular, we obtain degree $\mathrm{poly}(k)$ sandwiching polynomials for functions of $k$ halfspaces under the Gaussian distribution, improving exponentially over the prior $2^{O(k)}$ bound. More broadly, our approach applies to function classes that are low-dimensional and have smooth boundary.
  In contrast to prior work, our proof is relatively simple and directly uses the smoothness of the target function's boundary to construct sandwiching Lipschitz functions, which are amenable to results from high-dimensional approximation theory. For low-dimensional polynomial threshold functions (PTFs) with respect to Gaussians, we obtain doubly exponential improvements without applying the FT-mollification method of Kane used in the best previous result.

</details>


### [114] [Multi-Objective Reinforcement Learning for Large-Scale Tote Allocation in Human-Robot Collaborative Fulfillment Centers](https://arxiv.org/abs/2602.24182)
*Sikata Sengupta,Guangyi Liu,Omer Gottesman,Joseph W Durham,Michael Kearns,Aaron Roth,Michael Caldara*

Main category: cs.LG

TL;DR: 本文将履约中心的容器整合过程建模为大规模多目标强化学习问题，基于零和博弈理论提出最小最大优化方法。该方法能有效平衡处理速度、资源使用和空间利用率等竞争目标，满足实际约束，并通过理论框架解决时间平均解的振荡行为，在大规模工业系统中展现出应用前景。


<details>
  <summary>Details</summary>
Motivation: 容器式履约中心需优化物品在人与机器人工作站间的流转以释放空间并提升容器利用率，此过程涉及处理速度、资源消耗和空间利用率等多个竞争目标的权衡，同时受限于实际运营约束。传统方法难以应对高维状态空间和动态系统行为带来的挑战，亟需新的优化框架。

Method: 研究者将问题形式化为大规模多目标强化学习（MORL）任务，利用约束强化学习的最新理论进展，基于零和博弈中的最佳响应和无遗憾动力学实现原则性最小最大策略学习。进一步提出理论框架处理误差抵消问题，解决时间平均解的振荡行为，确保返回单次迭代的拉格朗日值接近博弈最小最大值。

Result: 在现实仓库仿真中的策略评估表明，该方法能有效权衡各竞争目标，实证发现学习到的单一策略能同时满足所有约束（尽管无理论保证）。所提框架可返回拉格朗日值接近最小最大值的单次迭代解，有效处理振荡问题。

Conclusion: 该研究验证了多目标强化学习结合最小最大优化在解决大规模工业系统复杂决策问题中的潜力，为处理高维状态空间和多目标权衡提供了新思路，展示了重要的实际应用价值。

Abstract: Optimizing the consolidation process in container-based fulfillment centers requires trading off competing objectives such as processing speed, resource usage, and space utilization while adhering to a range of real-world operational constraints. This process involves moving items between containers via a combination of human and robotic workstations to free up space for inbound inventory and increase container utilization. We formulate this problem as a large-scale Multi-Objective Reinforcement Learning (MORL) task with high-dimensional state spaces and dynamic system behavior. Our method builds on recent theoretical advances in solving constrained RL problems via best-response and no-regret dynamics in zero-sum games, enabling principled minimax policy learning. Policy evaluation on realistic warehouse simulations shows that our approach effectively trades off objectives, and we empirically observe that it learns a single policy that simultaneously satisfies all constraints, even if this is not theoretically guaranteed. We further introduce a theoretical framework to handle the problem of error cancellation, where time-averaged solutions display oscillatory behavior. This method returns a single iterate whose Lagrangian value is close to the minimax value of the game. These results demonstrate the promise of MORL in solving complex, high-impact decision-making problems in large-scale industrial systems.

</details>


### [115] [Flow-Based Density Ratio Estimation for Intractable Distributions with Applications in Genomics](https://arxiv.org/abs/2602.24201)
*Egor Antipov,Alessandro Palma,Lorenzo Consoli,Stephan Günnemann,Andrea Dittadi,Fabian J. Theis*

Main category: cs.LG

TL;DR: 提出基于条件感知流匹配的单一动力学框架，高效估计密度比，在模拟基准测试中表现优异，并成功应用于单细胞基因组学的处理效应估计与批次校正评估。


<details>
  <summary>Details</summary>
Motivation: 密度比估计是概率建模的核心问题，可实现不同数据生成过程下样本似然的比较。归一化流等精确似然模型虽具前景，但计算昂贵，需为每个分布单独模拟似然积分。

Method: 利用条件感知流匹配技术，推导出沿生成轨迹追踪密度比的单一动力学公式，避免对每个分布进行独立计算。

Result: 在闭式比率估计的模拟基准测试中达到竞争性能；支持单细胞基因组学中的多种任务，包括跨实验条件的处理效应估计和批次校正评估。

Conclusion: 该方法通过单一动力学公式显著提升了密度比估计的计算效率，在模拟数据与单细胞基因组学实际应用中均表现出色，为概率建模提供了高效的计算框架。

Abstract: Estimating density ratios between pairs of intractable data distributions is a core problem in probabilistic modeling, enabling principled comparisons of sample likelihoods under different data-generating processes across conditions and covariates. While exact-likelihood models such as normalizing flows offer a promising approach to density ratio estimation, naive flow-based evaluations are computationally expensive, as they require simulating costly likelihood integrals for each distribution separately. In this work, we leverage condition-aware flow matching to derive a single dynamical formulation for tracking density ratios along generative trajectories. We demonstrate competitive performance on simulated benchmarks for closed-form ratio estimation, and show that our method supports versatile tasks in single-cell genomics data analysis, where likelihood-based comparisons of cellular states across experimental conditions enable treatment effect estimation and batch correction evaluation.

</details>


### [116] [The Stability of Online Algorithms in Performative Prediction](https://arxiv.org/abs/2602.24207)
*Gabriele Farina,Juan Carlos Perdomo*

Main category: cs.LG

TL;DR: 该论文证明了在performative prediction环境中，任何无遗憾算法都会收敛至一个performatively stable equilibrium，无需对模型影响数据分布的机制施加任何限制。


<details>
  <summary>Details</summary>
Motivation: 算法预测在决策中会产生反馈循环：部署的模型会改变数据分布，而该分布又用于模型重训练。这一动态关系已被Perdomo等人形式化为performative prediction。此前该领域的积极结果均依赖于对模型影响分布的强假设限制。

Method: 通过鞅论分析与随机化技术，避免了对模型影响分布方式的任何前提假设，从而绕过了寻找稳定模型的相关困难性结果。

Result: 主要结果是一个无条件归约：任何在performative设置中部署的无遗憾算法都会收敛到（混合）performatively stable equilibrium——即模型主动塑造数据分布，使其自身预测在事后看来达到最优的解。

Conclusion: 该理论揭示了梯度下降等常见算法为何天然具有稳定效应并能防止失控反馈循环。研究旨在促进在线优化与performativity之间的未来技术转移。

Abstract: The use of algorithmic predictions in decision-making leads to a feedback loop where the models we deploy actively influence the data distributions we see, and later use to retrain on. This dynamic was formalized by Perdomo et al. 2020 in their work on performative prediction. Our main result is an unconditional reduction showing that any no-regret algorithm deployed in performative settings converges to a (mixed) performatively stable equilibrium: a solution in which models actively shape data distributions in ways that their own predictions look optimal in hindsight. Prior to our work, all positive results in this area made strong restrictions on how models influenced distributions. By using a martingale argument and allowing randomization, we avoid any such assumption and sidestep recent hardness results for finding stable models. Lastly, on a more conceptual note, our connection sheds light on why common algorithms, like gradient descent, are naturally stabilizing and prevent runaway feedback loops. We hope our work enables future technical transfer of ideas between online optimization and performativity.

</details>


### [117] [An Efficient Unsupervised Federated Learning Approach for Anomaly Detection in Heterogeneous IoT Networks](https://arxiv.org/abs/2602.24209)
*Mohsen Tajgardan,Atena Shiranzaei,Mahdi Rabbani,Reza Khoshkangini,Mahtab Jamali*

Main category: cs.LG

TL;DR: 针对物联网数据异质性挑战，本研究提出一种高效无监督联邦学习框架，通过融合异常检测与设备识别两个互补数据集的共享特征并保留特定特征，结合SHAP可解释AI技术，在真实物联网数据集上显著提升异常检测准确率，为去中心化环境提供了优化方案。


<details>
  <summary>Details</summary>
Motivation: 物联网设备在能力、数据格式和通信约束方面的异质性导致特征异构性，严重影响联邦学习的全局模型性能与隐私保护效果。在无需集中数据的无监督异常检测场景中，特征差异使模型训练复杂化，阻碍了联邦学习在实际物联网环境中的有效部署。

Method: 提出基于共享特征融合的无监督联邦学习框架，利用两个独立物联网数据集（异常检测数据集与设备识别数据集）的共享特征进行协同训练，同时保护各数据集特有特征。引入SHAP等可解释AI技术，识别影响本地模型决策的关键特征，增强框架透明度与可解释性。

Result: 在真实物联网数据集上的实验结果表明，该框架在异常检测准确率上显著优于传统联邦学习方法，验证了利用互补数据集共享特征优化无监督联邦学习的有效性和优越性。

Conclusion: 本研究证实通过共享互补数据集特征可有效克服物联网数据异质性，优化无监督联邦学习性能，在去中心化环境中实现更精准的异常检测，为联邦学习在异构物联网场景的实际应用提供了新思路与技术支撑。

Abstract: Federated learning (FL) is an effective paradigm for distributed environments such as the Internet of Things (IoT), where data from diverse devices with varying functionalities remains localized while contributing to a shared global model. By eliminating the need to transmit raw data, FL inherently preserves privacy. However, the heterogeneous nature of IoT data, stemming from differences in device capabilities, data formats, and communication constraints, poses significant challenges to maintaining both global model performance and privacy. In the context of IoT-based anomaly detection, unsupervised FL offers a promising means to identify abnormal behavior without centralized data aggregation. Nevertheless, feature heterogeneity across devices complicates model training and optimization, hindering effective implementation. In this study we propose an efficient unsupervised FL framework that enhances anomaly detection by leveraging shared features from two distinct IoT datasets: one focused on anomaly detection and the other on device identification, while preserving dataset-specific features. To improve transparency and interpretability, we employ explainable AI techniques, such as SHAP, to identify key features influencing local model decisions. Experiments conducted on real-world IoT datasets demonstrate that the proposed method significantly outperforms conventional FL approaches in anomaly detection accuracy. This work underscores the potential of using shared features from complementary datasets to optimize unsupervised federated learning and achieve superior anomaly detection results in decentralized IoT environments.

</details>


### [118] [Adaptive Combinatorial Experimental Design: Pareto Optimality for Decision-Making and Inference](https://arxiv.org/abs/2602.24231)
*Hongrui Xie,Junyu Cao,Kan Xu*

Main category: cs.LG

TL;DR: 这是一篇关于自适应组合实验设计的开创性研究，探讨了组合多臂老虎机（CMAB）中遗憾最小化与统计功效之间的权衡。论文提出了两种算法MixCombKL和MixCombUCB，分别在完全反馈和半反馈机制下实现了帕累托最优学习，并建立了多目标决策的组合实验理论框架。


<details>
  <summary>Details</summary>
Motivation: 传统组合多臂老虎机研究主要关注遗憾最小化，但在实际实验设计中（如A/B测试、推荐系统），还需要对奖励差距进行准确统计推断。这就产生了根本性冲突：减少遗憾需要反复利用高奖励臂，而准确的统计推断需要充分探索次优动作。本文旨在首次系统性地研究这种权衡，为自适应组合实验设计建立理论基础。

Method: 1. 通过帕累托最优概念形式化遗憾与统计功效的权衡关系；2. 建立CMAB中帕累托高效学习的等价条件；3. 针对不同信息结构（完全反馈vs半反馈）设计两种算法：MixCombKL（适用于完全反馈）和MixCombUCB（适用于半反馈）；4. 提供理论保证，证明算法在有限时间内同时达到遗憾和估计误差的帕累托最优。

Result: 1. 理论证明两种算法均实现帕累托最优；2. 获得遗憾和臂间差距估计误差的有限时间保证；3. 发现更丰富的反馈（半反馈）显著收紧可达成的帕累托前沿；4. 核心收益来源于所提方法下估计精度的提升。

Conclusion: 本研究建立了自适应组合实验设计的多目标决策原则性框架，首次揭示了遗憾最小化与统计功效之间的内在权衡机制，为需要在探索-利用与统计推断之间平衡的实际应用提供了理论基础和实用算法。

Abstract: In this paper, we provide the first investigation into adaptive combinatorial experimental design, focusing on the trade-off between regret minimization and statistical power in combinatorial multi-armed bandits (CMAB). While minimizing regret requires repeated exploitation of high-reward arms, accurate inference on reward gaps requires sufficient exploration of suboptimal actions. We formalize this trade-off through the concept of Pareto optimality and establish equivalent conditions for Pareto-efficient learning in CMAB. We consider two relevant cases under different information structures, i.e., full-bandit feedback and semi-bandit feedback, and propose two algorithms MixCombKL and MixCombUCB respectively for these two cases. We provide theoretical guarantees showing that both algorithms are Pareto optimal, achieving finite-time guarantees on both regret and estimation error of arm gaps. Our results further reveal that richer feedback significantly tightens the attainable Pareto frontier, with the primary gains arising from improved estimation accuracy under our proposed methods. Taken together, these findings establish a principled framework for adaptive combinatorial experimentation in multi-objective decision-making.

</details>
