<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 57]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.AI](#cs.AI) [Total: 27]
- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse](https://arxiv.org/abs/2510.19858)
*Jindi Wang,Yidi Zhang,Zhaoxing Li*

Main category: cs.CL

TL;DR: DeBERTa-KC在YouTube科学频道评论中对知识构建(KC)等级进行自动分类，使用增强的DeBERTa-v3并结合Focal Loss、标签平滑和R-Drop，在10折交叉验证下实现宏F1=0.836，优于基线。


<details>
  <summary>Details</summary>
Motivation: 自动化地对非正式数字学习环境中的知识构建过程进行编码与评估，解决类别不均衡问题；通过大语言模型捕捉参与者在科学讨论中的认知深度与 epistemic Engagement，以实现可扩展的 discourse analysis。

Method: 以四个KC类别（nonKC、Share、Explore、Negotiate）标注的20,000条YouTube科普频道评论构成平衡数据集；在DeBERTa-v3基础上加入Focal Loss、Label Smoothing和R-Drop正则化以缓解类别不均与提升泛化；实现端到端管线（数据提取、标注、预处理、训练、评估）；采用10-fold stratified CV，评估指标为macro-F1。

Result: 模型在10-fold CV中达到宏F1=0.836±0.008，显著优于经典与Transformer基线（p<0.01）；按类别结果显示在探索（Explore）和协商（Negotiate）等高阶认知参与方面具有更高的敏感性。

Conclusion: 表明大模型能较好捕捉非正式数字学习环境中的知识构建线索，提供可扩展、理论驱动的 discourse analysis 方案，并有助于开发评估认知参与度的自动化工具。

Abstract: This study presents DeBERTa-KC, a transformer-based model for automatic
classification of knowledge construction (KC) levels in online science learning
discourse. Using comments collected from four popular YouTube science channels
(2022--2024), a balanced corpus of 20,000 manually annotated samples was
created across four KC categories: \textit{nonKC}, \textit{Share},
\textit{Explore}, and \textit{Negotiate}. The proposed model extends DeBERTa-v3
with Focal Loss, Label Smoothing, and R-Drop regularization to address class
imbalance and enhance generalization. A reproducible end-to-end pipeline was
implemented, encompassing data extraction, annotation, preprocessing, training,
and evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved
a macro-F1 of $0.836 \pm 0.008$, significantly out-performing both classical
and transformer baselines ($p<0.01$). Per-category results indicate strong
sensitivity to higher-order epistemic engagement, particularly in
\textit{Explore} and \textit{Negotiate} discourse. These findings demonstrate
that large language models can effectively capture nuanced indicators of
knowledge construction in informal digital learning environments, offering
scalable, theory-informed approaches to discourse analysis and the development
of automated tools for assessing epistemic engagement.

</details>


### [2] [An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics](https://arxiv.org/abs/2510.19866)
*Xincheng Liu*

Main category: cs.CL

TL;DR: 在五个主流大模型和三种提示框架下生成的十五份物理课程计划显示：模型设计决定可读性，提示框架决定事实准确性和课程对齐；最佳组合为高可读性模型 + RACE 框架 + 逐条概念与标准清单。


<details>
  <summary>Details</summary>
Motivation: 评估 AI 生成的教学计划在可读性、准确性、课程对齐和认知目标上的可靠性与可用性，以为教育领域的应用提供证据与指南。

Method: 比较五个大模型与三种提示框架，针对高中物理话题‘电磁波谱’生成十五份课程计划；使用四项自动化指标评估：可读性/语言复杂度、事实准确性与幻觉检测、标准与课程对齐、学习目标的认知负荷。分析包括模型对可读性的影响、框架对准确性与对齐的影响，以及学习目标在布鲁姆认知层级的分布。

Result: 模型选择显著影响可读性，DeepSeek最具可读性，Claude最密集；提示框架对事实准确性与教学完整性影响最大，RACE框架在幻觉指数最低、与NGSS标准的 incidental alignment 最高；学习目标多集中在记忆与理解层级，较少使用高阶动词。综合结论：可读性受模型设计支配，可靠性与课程对齐更多受提示框架影响；最优组合为提升可读性的模型 + 采用RACE框架 + 逐条列出物理概念、课程标准与高阶目标的检查清单。

Conclusion: 在生成教学计划方面，优先考虑模型的可读性特性与框架带来的对齐与准确性支持。针对教育应用，推荐将能优化可读性的模型与RACE框架结合，并辅以对物理概念、标准与高阶目标的明确检查清单，以提升计划的可靠性与适用性。

Abstract: This study evaluates the pedagogical soundness and usability of AI-generated
lesson plans across five leading large language models: ChatGPT (GPT-5), Claude
Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,
three structured prompt frameworks were tested: TAG (Task, Audience, Goal),
RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,
Style, Tone, Audience, Response Format).
  Fifteen lesson plans were generated for a single high-school physics topic,
The Electromagnetic Spectrum. The lesson plans were analyzed through four
automated computational metrics: (1) readability and linguistic complexity, (2)
factual accuracy and hallucination detection, (3) standards and curriculum
alignment, and (4) cognitive demand of learning objectives.
  Results indicate that model selection exerted the strongest influence on
linguistic accessibility, with DeepSeek producing the most readable teaching
plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).
  The prompt framework structure most strongly affected the factual accuracy
and pedagogical completeness, with the RACE framework yielding the lowest
hallucination index and the highest incidental alignment with NGSS curriculum
standards. Across all models, the learning objectives in the fifteen lesson
plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There
were limited higher-order verbs in the learning objectives extracted.
  Overall, the findings suggest that readability is significantly governed by
model design, while instructional reliability and curricular alignment depend
more on the prompt framework. The most effective configuration for lesson plans
identified in the results was to combine a readability-optimized model with the
RACE framework and an explicit checklist of physics concepts, curriculum
standards, and higher-order objectives.

</details>


### [3] [From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model](https://arxiv.org/abs/2510.19871)
*Yatai Ji,Teng Wang,Yuying Ge,Zhiheng Liu,Sidi Yang,Ying Shan,Ping Luo*

Main category: cs.CL

TL;DR: ReDiff 提出一种强化修正的扩散框架，通过两阶段训练实现自我纠错，解决离散扩散模型的训练-推理不一致导致的错误级联问题，从而提升生成的一致性与事实准确性，并实现比传统去噪更稳定、高效的并行生成。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在视觉-语言任务中存在训练-推理不一致，初始符号错误在并行解码阶段扩散成连锁错误，导致句法错误和语义幻觉；需要将生成从被动去噪转向主动修正以抑制错误 cascade。

Method: 设计两阶段训练：1) 基础修订能力训练，通过让模型修订合成的错误来建立 Revision 能力；2) 在线自我纠错环路，在模型生成的草稿中学习并纠正自身缺陷，结合专家纠正进行错误修正的学习；在扩散框架中引入主动修正机制，从而打破错误级联。

Result: 大量实验表明，ReDiff 提升了生成内容的连贯性与事实准确性，使并行生成更稳定、效率更高，显著优于传统去噪方法。代码与模型公开可获取。

Conclusion: Mistake-driven 学习让模型具备回溯并修正已生成输出的能力，有效打破错误 cascades，提升离散扩散在视觉-语言任务中的可用性与鲁棒性，未来可推广到更多序列生成场景。

Abstract: Discrete diffusion models have emerged as a promising direction for
vision-language tasks, offering bidirectional context modeling and theoretical
parallelization. However, their practical application is severely hindered by a
train-inference discrepancy, which leads to catastrophic error cascades:
initial token errors during parallel decoding pollute the generation context,
triggering a chain reaction of compounding errors and leading to syntactic
errors and semantic hallucinations. To address this fundamental challenge, we
reframe the generation process from passive denoising to active refining. We
introduce ReDiff, a refining-enhanced diffusion framework that teaches the
model to identify and correct its own errors. Our approach features a two-stage
training process: first, we instill a foundational revision capability by
training the model to revise synthetic errors; second, we implement a novel
online self-correction loop where the model is explicitly trained to revise its
own flawed drafts by learning from an expert's corrections. This mistake-driven
learning endows the model with the crucial ability to revisit and refine its
already generated output, effectively breaking the error cascade. Extensive
experiments demonstrate that ReDiff significantly improves the coherence and
factual accuracy of generated content, enabling stable and efficient parallel
generation far superior to traditional denoising methods. Our codes and models
are available at https://rediff-hku.github.io/.

</details>


### [4] [Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention](https://arxiv.org/abs/2510.19875)
*J Rosser,José Luis Redondo García,Gustavo Penha,Konstantina Palla,Hugues Bouchard*

Main category: cs.CL

TL;DR: A scalable, near-linear-time method (Stream) for analyzing long-context attention in LLMs via dynamic sparse attention and hierarchical pruning, enabling one-pass interpretability with substantial reduction of token interactions.


<details>
  <summary>Details</summary>
Motivation: Mitigate the quadratic cost of mechanistic interpretability for long-context models and enable practical analysis on consumer GPUs by pruning irrelevant attention interactions while preserving model behavior.

Method: Introduce Sparse Tracing with a Stream algorithm that estimates per-head sparse attention masks in near-linear time O(T log T) and linear space O(T). Uses a binary-search-style refinement to keep the top-k blocks per query, preserving next-token behavior. Applies to long chain-of-thought traces to identify anchors and prune 97-99% of token interactions. On RULER, retains critical retrieval paths while discarding 90-96% of interactions, revealing layer-wise routes from the needle to output.

Result: Demonstrates that long-context interpretability is feasible at scale with a practical tool. Stream maintains essential information flow (retrieval paths) while dramatically reducing interactions, enabling one-pass analysis on consumer GPUs. Provides quantitative benchmarks (97-99% pruning on chains-of-thought tests; 90-96% pruning on RULER) and practical efficiency.

Conclusion: Presents a practical drop-in toolkit for long-context interpretability, democratizing chain-of-thought monitoring by reducing memory and computation requirements and offering near-linear-time analysis suitable for standard GPUs. Code release enhances reproducibility.

Abstract: As Large Language Models (LLMs) scale to million-token contexts, traditional
Mechanistic Interpretability techniques for analyzing attention scale
quadratically with context length, demanding terabytes of memory beyond 100,000
tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic
sparse attention to efficiently analyze long context attention patterns. We
present Stream, a compilable hierarchical pruning algorithm that estimates
per-head sparse attention masks in near-linear time $O(T \log T)$ and linear
space $O(T)$, enabling one-pass interpretability at scale. Stream performs a
binary-search-style refinement to retain only the top-$k$ key blocks per query
while preserving the model's next-token behavior. We apply Stream to long
chain-of-thought reasoning traces and identify thought anchors while pruning
97-99\% of token interactions. On the RULER benchmark, Stream preserves
critical retrieval paths while discarding 90-96\% of interactions and exposes
layer-wise routes from the needle to output. Our method offers a practical
drop-in tool for analyzing attention patterns and tracing information flow
without terabytes of caches. By making long context interpretability feasible
on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.
Code is available at https://anonymous.4open.science/r/stream-03B8/.

</details>


### [5] [Automated HIV Screening on Dutch EHR with Large Language Models](https://arxiv.org/abs/2510.19879)
*Lang Zhou,Amrish Jhingoer,Yinghao Luo,Klaske Vliegenthart--Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: 一种基于大型语言模型的管线，分析未结构化的电子病历文本以确定患者是否需要进一步 HIV 检测，在鹿特丹伊拉斯姆斯大学医疗中心的数据上实现高精度和低假阴性率。


<details>
  <summary>Details</summary>
Motivation: 未结构化的临床文本中包含对 HIV 风险的有价值信号，传统以结构化数据为主的方法往往忽略这些信息。利用LLM挖掘文本信息并辅助筛选可提升早期诊断率并降低筛查成本。

Method: 提出一个管线，将未结构化的电子病历文本输入到大型语言模型，提取与HIV检测相关的风险信号并判定是否具备进一步检测的资格；在鹿特丹伊拉斯姆斯大学医学中心的临床数据上进行实验评估，关注准确性和低假阴性率。

Result: 管线实现了较高的准确性并保持较低的假阴性率，表明在未结构化EHR文本中利用LLM进行HIV筛查资格判定是可行的。

Conclusion: 基于未结构化EHR文本的LLM分析为HIV筛查提供了一条可行的新方向，能与结构化数据方法互补，具有良好的推广性与可扩展性。

Abstract: Efficient screening and early diagnosis of HIV are critical for reducing
onward transmission. Although large scale laboratory testing is not feasible,
the widespread adoption of Electronic Health Records (EHRs) offers new
opportunities to address this challenge. Existing research primarily focuses on
applying machine learning methods to structured data, such as patient
demographics, for improving HIV diagnosis. However, these approaches often
overlook unstructured text data such as clinical notes, which potentially
contain valuable information relevant to HIV risk. In this study, we propose a
novel pipeline that leverages a Large Language Model (LLM) to analyze
unstructured EHR text and determine a patient's eligibility for further HIV
testing. Experimental results on clinical data from Erasmus University Medical
Center Rotterdam demonstrate that our pipeline achieved high accuracy while
maintaining a low false negative rate.

</details>


### [6] [An Expert-grounded benchmark of General Purpose LLMs in LCA](https://arxiv.org/abs/2510.19886)
*Artur Donaldson,Bharathan Balaji,Cajetan Oriekezie,Manish Kumar,Laure Patouillard*

Main category: cs.CL

TL;DR: 首个以专家评审为基础的通用大语言模型在生命周期评估（LCA）中的基准评估。11个模型、22项任务、168份评审。结果显示信息不准确率较高、幻觉率波动大、开放-权重与封闭-权重无显著差异；在解释质量等方面有潜在收益，但需引入 grounding 以避免被当作自由形式的权威答案。


<details>
  <summary>Details</summary>
Motivation: LCA领域缺乏标准化评估框架，急需系统评估大型语言模型在可靠性、鲁棒性与可用性方面的表现。

Method: 评估11个通用目的LLMs（商业与开源）在22项LCA相关任务上的表现；由17位资深从业者对模型输出进行评审，评估标准包括科学准确性、解释质量、鲁棒性、可验证性与遵循指令能力；共收集168份专家评审。

Result: 专家判定有37%的回答存在不准确或具有误导性。多数模型在准确性与解释质量方面评定为中等到良好，格式遵循普遍良好。幻觉率差异显著，部分模型的幻觉性引用率高达40%。开放权重与封闭权重模型之间无明显优劣区分，开放模型在某些方面可媲美甚至优于封闭模型。

Conclusion: 表明直接将LLMs作为自由形式的oracle在LCA中存在风险，但在提升解释质量与简化简单任务方面具有潜在收益；若缺乏 grounding 机制，使用普遍用途的LLMs会带来不可忽略的风险。

Abstract: Purpose: Artificial intelligence (AI), and in particular large language
models (LLMs), are increasingly being explored as tools to support life cycle
assessment (LCA). While demonstrations exist across environmental and social
domains, systematic evidence on their reliability, robustness, and usability
remains limited. This study provides the first expert-grounded benchmark of
LLMs in LCA, addressing the absence of standardized evaluation frameworks in a
field where no clear ground truth or consensus protocols exist.
  Methods: We evaluated eleven general-purpose LLMs, spanning both commercial
and open-source families, across 22 LCA-related tasks. Seventeen experienced
practitioners reviewed model outputs against criteria directly relevant to LCA
practice, including scientific accuracy, explanation quality, robustness,
verifiability, and adherence to instructions. We collected 168 expert reviews.
  Results: Experts judged 37% of responses to contain inaccurate or misleading
information. Ratings of accuracy and quality of explanation were generally
rated average or good on many models even smaller models, and format adherence
was generally rated favourably. Hallucination rates varied significantly, with
some models producing hallucinated citations at rates of up to 40%. There was
no clear-cut distinction between ratings on open-weight versus closed-weight
LLMs, with open-weight models outperforming or competing on par with
closed-weight models on criteria such as accuracy and quality of explanation.
  Conclusion: These findings highlight the risks of applying LLMs na\"ively in
LCA, such as when LLMs are treated as free-form oracles, while also showing
benefits especially around quality of explanation and alleviating labour
intensiveness of simple tasks. The use of general-purpose LLMs without
grounding mechanisms presents ...

</details>


### [7] [Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities](https://arxiv.org/abs/2510.19892)
*Nishant Balepur,Dang Nguyen,Dayeon Ki*

Main category: cs.CL

TL;DR: 提出通过 Dixit 游戏的博弈评估来全面评估多模态大语言模型（MLMs），实验表明胜率与基准评估高度相关，并揭示模型推理的差异与改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有的静态基准和对比评估要么只能单任务评估，要么高度主观、成本高且易被表面化策略利用。需要一种固定、公正、具交互性的评估框架，以全面衡量 MLM 的多项能力。

Method: 以 Dixit 桌游为载体，要求模型为卡牌生成描述性字幕，混淆目标以让部分玩家选错，进而评估胜负。对五种 MLM 进行量化实验，比较它们在多轮对局中的胜率与现有基准的相关性；还进行人机对局，比较人类和 MLM 的策略差异与推理改进点。

Result: 五种 MLM 的 Dixit 胜率与主流基准的排名高度相关，达到完全相关（perfectly correlated）的程度；人机对局揭示了 MLM 在策略和推理方面的差异，以及可改进的具体领域。

Conclusion: 基于博弈的评估为 MLM 的能力提供了一个兼具客观性、可重复性和参与性的综合评估框架，且 Dixit 实验证实了该框架能与现有基准对齐并揭示进一步改进点。

Abstract: Multi-modal large language models (MLMs) are often assessed on static,
individual benchmarks -- which cannot jointly assess MLM capabilities in a
single task -- or rely on human or model pairwise comparisons -- which is
highly subjective, expensive, and allows models to exploit superficial
shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these
issues, we propose game-based evaluations to holistically assess MLM
capabilities. Games require multiple abilities for players to win, are
inherently competitive, and are governed by fix, objective rules, and makes
evaluation more engaging, providing a robust framework to address the
aforementioned challenges. We manifest this evaluation specifically through
Dixit, a fantasy card game where players must generate captions for a card that
trick some, but not all players, into selecting the played card. Our
quantitative experiments with five MLMs show Dixit win-rate rankings are
perfectly correlated with those on popular MLM benchmarks, while games between
human and MLM players in Dixit reveal several differences between agent
strategies and areas of improvement for MLM reasoning.

</details>


### [8] [LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation](https://arxiv.org/abs/2510.19967)
*Le Ren,Xiangjian Zeng,Qingqiang Wu,Ruoxuan Liang*

Main category: cs.CL

TL;DR: 提出LyriCAR，一种完全无监督的抒情翻译框架，通过难度感知的课程设计器和自适应课程策略，在跨段落连贯性和全局押韵方面表现优越，EN-ZH lyric translation达到SOTA，且自适应课程使训练步数下降约40%。代码可复现。


<details>
  <summary>Details</summary>
Motivation: 现有抒情翻译多采用手工规则和单句建模，难以内化音乐-语言模式，难以在段落级别实现全局连贯性和押韵泛化，需更高效且可控的学习机制。

Method: 提出难度感知的课程设计器和自适应课程策略，在全无监督条件下对训练过程进行难度分层与资源分配，引导模型逐步解决更复杂的挑战，以改善跨行/段落的一致性与押韵性。

Result: 在EN-ZH歌词翻译任务上实现SOTA，标准翻译指标和多维奖励分数均优于强基线；自适应课程将训练步骤降低约40%，仍保持优越性能。

Conclusion: 通过难度分层与自适应调度提升抒情翻译的质量与训练效率，证明无监督课程式学习在音乐语言任务中的潜力；可访问代码、数据与模型。

Abstract: Lyric translation is a challenging task that requires balancing multiple
musical constraints. Existing methods often rely on hand-crafted rules and
sentence-level modeling, which restrict their ability to internalize
musical-linguistic patterns and to generalize effectively at the paragraph
level, where cross-line coherence and global rhyme are crucial. In this work,
we propose LyriCAR, a novel framework for controllable lyric translation that
operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware
curriculum designer and an adaptive curriculum strategy, ensuring efficient
allocation of training resources, accelerating convergence, and improving
overall translation quality by guiding the model with increasingly complex
challenges. Extensive experiments on the EN-ZH lyric translation task show that
LyriCAR achieves state-of-the-art results across both standard translation
metrics and multi-dimensional reward scores, surpassing strong baselines.
Notably, the adaptive curriculum strategy reduces training steps by nearly 40%
while maintaining superior performance. Code, data and model can be accessed at
https://github.com/rle27/LyriCAR.

</details>


### [9] [LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation](https://arxiv.org/abs/2510.19988)
*Xin Lian,Kenneth D. Forbus*

Main category: cs.CL

TL;DR: Hybrid LLM-symbolic NLU system combines broad-coverage language processing with structured relational representations to improve extraction and interpretation of quantities and causal laws from commonsense science texts; outperforms symbolic-only pipelines.


<details>
  <summary>Details</summary>
Motivation: LLMs offer broad coverage but risk hallucination and inconsistent outputs; symbolic NLU provides interpretable, debuggable reasoning but has limited coverage and requires specialized knowledge; a hybrid aims to leverage both.

Method: Use LLMs for rephrasing and text simplification to broaden coverage and fill knowledge gaps; use symbolic NLU to produce relational representations for reasoning and incremental learning; evaluate on extracting/interpreting quantities and causal laws from commonsense science texts; compare with symbolic-only and LLM-only pipelines.

Result: The hybrid method significantly outperforms the symbolic-only pipeline; evaluated against symbolic- and LLM-only baselines on tasks involving quantities and causal laws in commonsense science texts.

Conclusion: Integrating broad-coverage LLM processing with symbolic relational representations yields better quantitative and causal understanding than symbolic NLU alone, suggesting the practicality of hybrid systems for interpretable reasoning and learning.

Abstract: Despite the broad applicability of large language models (LLMs), their
reliance on probabilistic inference makes them vulnerable to errors such as
hallucination in generated facts and inconsistent output structure in natural
language understanding (NLU) tasks. By contrast, symbolic NLU systems provide
interpretable understanding grounded in curated lexicons, semantic resources,
and syntactic & semantic interpretation rules. They produce relational
representations that can be used for accurate reasoning and planning, as well
as incremental debuggable learning. However, symbolic NLU systems tend to be
more limited in coverage than LLMs and require scarce knowledge representation
and linguistics skills to extend and maintain. This paper explores a hybrid
approach that integrates the broad-coverage language processing of LLMs with
the symbolic NLU capabilities of producing structured relational
representations to hopefully get the best of both approaches. We use LLMs for
rephrasing and text simplification, to provide broad coverage, and as a source
of information to fill in knowledge gaps more automatically. We use symbolic
NLU to produce representations that can be used for reasoning and for
incremental learning. We evaluate this approach on the task of extracting and
interpreting quantities and causal laws from commonsense science texts, along
with symbolic- and LLM-only pipelines. Our results suggest that our hybrid
method works significantly better than the symbolic-only pipeline.

</details>


### [10] [A Fundamental Algorithm for Dependency Parsing (With Corrections)](https://arxiv.org/abs/2510.19996)
*Michael A. Covington*

Main category: cs.CL

TL;DR: Incremental dependency parsing: a one-word-at-a-time parser that attaches each word as soon as possible, with O(n^3) worst-case complexity.


<details>
  <summary>Details</summary>
Motivation: To explore a parsing paradigm that aligns with brain-like incremental processing and contrasts with constituency parsers by producing dependency trees.

Method: Process sentence sequentially, attaching each word as soon as possible to the existing structure, yielding a dependency tree. The approach shares the O(n^3) worst-case complexity typical of constituency parsers.

Result: Worst-case complexity is O(n^3), but in natural language the worst case occurs for small n; the method effectively builds dependencies incrementally.

Conclusion: The proposed incremental, one-word-at-a-time parser provides a brain-muggest-aligned parsing model with complexity comparable to traditional parsers and is practical for natural language where n stays small.

Abstract: This paper presents a fundamental algorithm for parsing natural language
sentences into dependency trees. Unlike phrase-structure (constituency)
parsers, this algorithm operates one word at a time, attaching each word as
soon as it can be attached, corresponding to properties claimed for the parser
in the human brain. Like phrase-structure parsing, its worst-case complexity is
$O(n^3)$, but in human language, the worst case occurs only for small $n$.

</details>


### [11] [Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs](https://arxiv.org/abs/2510.20001)
*Yunpeng Xiao,Carl Yang,Mark Mai,Xiao Hu,Kai Shu*

Main category: cs.CL

TL;DR: 提出一个将临床决策任务归纳为两个维度（临床背景和临床问题）的统一范式，并对现有数据集、评估方法和评估指标进行系统梳理，以提升对临床环境中决策的建模与评估。


<details>
  <summary>Details</summary>
Motivation: 现有医疗领域的大语言模型评估多依赖简化的问答数据集（如 MedQA），难以代表真实的临床决策场景与复杂性，因此需要一个更贴近临床环境的评估框架来标准化比较与驱动改进。

Method: 提出二维度框架来描述临床决策任务；系统梳理并按这两维度汇总现有数据集和基线；回顾训练时和推理时的技术手段；将评估扩展到效率、可解释性等方面，并讨论开放挑战。

Result: 给出一个统一的评估与比较基准，帮助研究者在临床相关的LLM研究中进行更可重复、可比较的评估，促进面向临床意义的模型开发。

Conclusion: 该范式旨在明确假设、标准化比较、并指引面向临床决策的LLM开发，同时揭示当前方法在真实世界场景中的不足与挑战。

Abstract: Large language models (LLMs) show promise for clinical use. They are often
evaluated using datasets such as MedQA. However, Many medical datasets, such as
MedQA, rely on simplified Question-Answering (Q\A) that underrepresents
real-world clinical decision-making. Based on this, we propose a unifying
paradigm that characterizes clinical decision-making tasks along two
dimensions: Clinical Backgrounds and Clinical Questions. As the background and
questions approach the real clinical environment, the difficulty increases. We
summarize the settings of existing datasets and benchmarks along two
dimensions. Then we review methods to address clinical decision-making,
including training-time and test-time techniques, and summarize when they help.
Next, we extend evaluation beyond accuracy to include efficiency,
explainability. Finally, we highlight open challenges. Our paradigm clarifies
assumptions, standardizes comparisons, and guides the development of clinically
meaningful LLMs.

</details>


### [12] [Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training](https://arxiv.org/abs/2510.20002)
*Alexandra Apostolopoulou,Konstantinos Kanaris,Athanasios Koursaris,Dimitris Tsakalidis,George Domalis,Ioannis E. Livieris*

Main category: cs.CL

TL;DR: 提出 Greek Embedding Models（GEM）家族的一系列希腊语Transformer，基于高质量数据构建，覆盖通用与法律领域，探索 ELECTRA、ConvBERT、ModernBERT 等新架构，并首次推出希-英法律领域嵌入模型；GEM-RoBERTa/ GEM-ConvBERT 在下游任务中显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 希腊语等形态丰富、资源相对有限的语言在NLP研究中碎片化，缺乏模型多样性，且对长文档如法律文本的上下文长度不足。需要在希腊语中引入更强的上下文能力和法律领域适应性。

Method: 构建多语料库，进行严格的质量筛选与预处理，基于通用域和法律来源的数据训练；在希腊语上系统预训练并评估一系列新架构（ELECTRA、ConvBERT、ModernBERT等），并提出首个希-英双语法律领域嵌入模型（GEM系列）。

Result: 在大量下游任务上，新模型表现优异，GEM-RoBERTa与GEM-ConvBERT显著超越现有基线。

Conclusion: 高质量数据基底结合多样化架构可显著提升希腊语NLP，特别是在法律领域的长期文本分析方面，提出的希-英法律嵌入模型具备实用潜力。

Abstract: The advancement of natural language processing for morphologically rich,
moderately-resourced languages like Modern Greek is often hindered by a
fragmented research landscape, a lack of architectural diversity and reliance
on limited context-length models. This is particularly true in specialized,
high-value domains such as law, where existing models are frequently confined
to early transformer architectures with a restrictive 512-token window,
insufficient for analyzing long legal documents. To address these challenges,
this paper presents Greek Embedding Models, a new family of transformer models
for Greek language built upon a foundation of extensive, quality-driven data
curation. We detail the construction of several large-scale Greek corpora,
emphasizing a rigorous, quality-based filtering and preprocessing methodology
to create high-value training datasets from both general-domain and specialized
legal sources. On this carefully curated foundation, we pre-train and
systematically evaluate a diverse suite of modern architectures, which has not
previously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT.
Furthermore, we propose the first bilingual Greek-English Embedding Models
tailored for the legal domain. The extensive experiments on downstream tasks
demonstrate that the new class of models establish the effectiveness of the
proposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models
significantly outperform existing baselines.

</details>


### [13] [Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models](https://arxiv.org/abs/2510.20033)
*David Dukić*

Main category: cs.CL

TL;DR: 本论文通过三种转移学习策略提升以预训练语言模型为基础的序列标注：引入额外信号的多任务框架用于领域迁移的事件触发检测；在自回归大语言模型中引入跨层的双向信息流架构修改；将自回归语言模型作为文本生成器，结合有监督的“在-context 细调”与响应导向的自适应策略。


<details>
  <summary>Details</summary>
Motivation: 解决在序列标注任务中对预训练语言模型进行有效领域与任务转移的挑战，特别是提升事件触发检测等领域迁移的表现，以及探索如何在自回归模型中实现更有效的信息流与生成式细调，以获得更好的标注性能。

Method: 1) 在多任务框架中引入来自领域无关文本处理系统的额外信号，以提升领域迁移下的事件触发检测任务性能；2) 提出一种 architectural 修改，使自回归大语言模型实现层间的双向信息流以加强信息整合；3) 将自回归模型作为文本生成器，提出生成式有监督在-context 细调框架并结合面向响应的适配策略，用于序列标注任务的改进。

Result: 实验表明，当通过上述针对性的转移学习范式对预训练语言模型进行适配时，序列标注任务的性能可以达到最佳水平，尤其在事件触发检测等领域迁移场景中表现出显著提升，验证了目标导向的转移学习策略的有效性。

Conclusion: 本工作表明，针对序列标注的预训练语言模型的转移学习可以通过三条独立且互补的路径获得显著提升：引入额外信号的多任务学习、跨层双向信息流的架构改进，以及基于生成与在-context 细调的生成式方法。这些策略共同推动了在不同任务和领域的转移场景中对预训练模型的有效适配。

Abstract: This doctoral thesis improves the transfer learning for sequence labeling
tasks by adapting pre-trained neural language models. The proposed improvements
in transfer learning involve introducing a multi-task model that incorporates
an additional signal, a method based on architectural modifications in
autoregressive large language models, and a sequence labeling framework for
autoregressive large language models utilizing supervised in-context
fine-tuning combined with response-oriented adaptation strategies. The first
improvement is given in the context of domain transfer for the event trigger
detection task. The domain transfer of the event trigger detection task can be
improved by incorporating an additional signal obtained from a
domain-independent text processing system into a multi-task model. The second
improvement involves modifying the model's architecture. For that purpose, a
method is proposed to enable bidirectional information flow across layers of
autoregressive large language models. The third improvement utilizes
autoregressive large language models as text generators through a generative
supervised in-context fine-tuning framework. The proposed model, method, and
framework demonstrate that pre-trained neural language models achieve their
best performance on sequence labeling tasks when adapted through targeted
transfer learning paradigms.

</details>


### [14] [ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering](https://arxiv.org/abs/2510.20036)
*Marianne Menglin Liu,Daniel Garcia,Fjona Parllaku,Vikas Upadhyay,Syed Fahad Allam Shah,Dan Roth*

Main category: cs.CL

TL;DR: ToolScope 通过 ToolScopeMerger（带自动纠错的合并审计）和 ToolScopeRetriever（相关性排序与压缩检索）来减少冗余并在上下文限制内选取最相关工具，从而提升多模型与基准上的工具选择准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的工具集中存在名称与描述的冗余与歧义，导致 LLM 在选择工具时易产生错误与不精确。此外，严格的上下文长度限制使得难以全面评估巨大工具集，影响工具使用效果。

Method: ToolScopeMerger with Auto-Correction：自动审计并修复工具之间的合并，减少冗余与歧义；ToolScopeRetriever：对查询进行相关性排序，筛选出最相关的工具，并在不牺牲准确性的前提下压缩工具集以适配上下文容量。

Result: 在三种前沿 LLM 与三组开源工具使用基准上，工具选择准确性提升范围为 8.38% 到 38.6%。

Conclusion: ToolScope 能显著提升 LLM 的工具使用效果，验证通过冗余消除与高效检索实现对现实工具生态的有效适配。

Abstract: Large language model (LLM) agents rely on external tools to solve complex
tasks, but real-world toolsets often contain redundant tools with overlapping
names and descriptions, introducing ambiguity and reducing selection accuracy.
LLMs also face strict input context limits, preventing efficient consideration
of large toolsets. To address these challenges, we propose ToolScope, which
includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and
fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and
select only the most relevant tools for each query, compressing toolsets to fit
within context limits without sacrificing accuracy. Evaluations on three
state-of-the-art LLMs and three open-source tool-use benchmarks show gains of
8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's
effectiveness in enhancing LLM tool use.

</details>


### [15] [From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge](https://arxiv.org/abs/2510.20043)
*Nafis Chowdhury,Moinul Haque,Anika Ahmed,Nazia Tasnim,Md. Istiak Hossain Shihab,Sajjadur Rahman,Farig Sadeque*

Main category: cs.CL

TL;DR: 提出 Bengali Language Cultural Knowledge (BLanCK) 数据集以评估多语言模型对低资源文化知识的掌握；实证发现模型在文化知识方面存在显著不足，但在提供上下文后表现显著提升，强调上下文感知架构与以文化为导向的训练数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准在捕捉低资源文化细节方面存在不足，亟需面向 Bengali 等文化的评测数据与分析，以揭示模型在文化知识维度上的能力及其局限。

Method: 构建包含民俗传统、烹饪艺术与区域方言的 Bengali Language Cultural Knowledge（BLanCK）数据集；对多语言模型进行评估，比较在无上下文与有上下文条件下的表现，强调上下文对文化知识理解的作用；并比较不同模型的表现差异。

Result: 在非文化类别上，模型表现尚可；在文化知识层面存在显著不足；提供上下文后，所有模型的表现均显著提升，凸显上下文感知与文化化训练数据的重要性。

Conclusion: 提升文化知识表现需采用上下文感知的架构与文化导向的训练数据，BLanCK 数据集为评估与研究提供有价值的资源，促进对文化知识在多语言模型中的理解与改进。

Abstract: Recent progress in NLP research has demonstrated remarkable capabilities of
large language models (LLMs) across a wide range of tasks. While recent
multilingual benchmarks have advanced cultural evaluation for LLMs, critical
gaps remain in capturing the nuances of low-resource cultures. Our work
addresses these limitations through a Bengali Language Cultural Knowledge
(BLanCK) dataset including folk traditions, culinary arts, and regional
dialects. Our investigation of several multilingual language models shows that
while these models perform well in non-cultural categories, they struggle
significantly with cultural knowledge and performance improves substantially
across all models when context is provided, emphasizing context-aware
architectures and culturally curated training data.

</details>


### [16] [Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training](https://arxiv.org/abs/2510.20059)
*Mehrdad Ghassabi,Sadra Hakim,Hamidreza Baradaran Kashani,Pedram Rostami*

Main category: cs.CL

TL;DR: Using RLAIF and DPO with CoT prompts to enhance Persian medical reasoning in a small LM; demonstrates data-efficient improvement, surpassing a larger model with far less data.


<details>
  <summary>Details</summary>
Motivation: 提升低资源语言在专业领域中的推理能力，尤其是波斯语医疗问答的表现，通过数据高效的强化学习与偏好优化方法实现。

Method: 将数据集翻译为波斯语；通过RLAIF生成拒绝-偏好对以用于DPO训练；教师与学生模型均使用Chain-of-Thought提示以产出推理轨迹；基于2M偏好tokens和2.5M拒绝tokens的数据训练基线模型，提升其医疗推理能力；与57M tokens规模的gaokerena-V进行对比。

Result: 基线模型显著提升在波斯语医疗推理任务上的能力，且优于在约57M tokens数据上训练的gaokerena-V，显示在数据规模远小于对比模型的情况下，推理训练具高效性。

Conclusion: 以RLAIF+DPO和Chain-of-Thought为核心的推理导向训练，在资源受限的语言和领域中具有效性，具有将其推广至其他低资源语言/领域的潜力。

Abstract: Enhancing reasoning capabilities in small language models is critical for
specialized applications such as medical question answering, particularly in
underrepresented languages like Persian. In this study, we employ Reinforcement
Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to
improve the reasoning skills of a general-purpose Persian language model. To
achieve this, we translated a multiple-choice medical question-answering
dataset into Persian and used RLAIF to generate rejected-preferred answer
pairs, which are essential for DPO training. By prompting both teacher and
student models to produce Chain-of-Thought (CoT) reasoning responses, we
compiled a dataset containing correct and incorrect reasoning trajectories.
This dataset, comprising 2 million tokens in preferred answers and 2.5 million
tokens in rejected ones, was used to train a baseline model, significantly
enhancing its medical reasoning capabilities in Persian. Remarkably, the
resulting model outperformed its predecessor, gaokerena-V, which was trained on
approximately 57 million tokens, despite leveraging a much smaller dataset.
These results highlight the efficiency and effectiveness of reasoning-focused
training approaches in developing domain-specific language models with limited
data availability.

</details>


### [17] [CreativityPrism: A Holistic Benchmark for Large Language Model Creativity](https://arxiv.org/abs/2510.20091)
*Zhaoyi Joey Hou,Bowei Alvin Zhang,Yining Lu,Bhiman Kumar Baghel,Anneliese Brei,Ximing Lu,Meng Jiang,Faeze Brahman,Snigdha Chaturvedi,Haw-Shiuan Chang,Daniel Khashabi,Xiang Lorraine Li*

Main category: cs.CL

TL;DR: Proposes CreativityPrism, a holistic 3D framework (quality, novelty, diversity) for evaluating LLM creativity across 9 tasks, 3 domains, 20 metrics; finds strong intra-domain correlations, a gap between proprietary and open-source models, and that novelty behaves differently from quality/diversity; argues for comprehensive cross-domain evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing creativity evaluations are fragmented across domains and tasks due to varying definitions and measures; there is a need for a unified framework to assess LLM creativity holistically.

Method: Introduce CreativityPrism with three dimensions (quality, novelty, diversity) and apply it to nine tasks across three domains (divergent thinking, creative writing, logical reasoning) using twenty task-specific metrics; evaluate 17 state-of-the-art LLMs (proprietary and open-source) and analyze metric-task correlations and cross-domain performance.

Result: Performance is highly correlated across tasks within the same domain but less so across different domains. Diversity and quality metrics correlate strongly, while novelty shows weaker correlations with the other dimensions. A notable gap exists between proprietary and open-source models.

Conclusion: A holistic, cross-domain evaluation framework is necessary since strong performance in one task or dimension does not guarantee generalization to others.

Abstract: Creativity is often seen as a hallmark of human intelligence. While large
language models (LLMs) are increasingly perceived as producing creative text,
there is still no holistic framework to evaluate their creativity across
diverse scenarios. Existing evaluation methods remain fragmented, with dramatic
variation across domains and tasks, largely due to differing definitions and
measurements of creativity. Inspired by the hypothesis that creativity is not
one fixed idea, we propose CreativityPrism, an evaluation analysis framework
that decomposes creativity into three dimensions: quality, novelty, and
diversity. CreativityPrism incorporates nine tasks, three domains, i.e.,
divergent thinking, creative writing, and logical reasoning, and twenty
evaluation metrics, which measure each dimension in task-specific, unique ways.
We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on
CreativityPrism and analyze the performance correlations among different
metrics and task domains. Our results reveal a notable gap between proprietary
and open-source models. Overall, model performance tends to be highly
correlated across tasks within the same domain and less so across different
domains. Among evaluation dimensions, diversity and quality metrics show strong
correlations - models that perform well on one often excel on the other -
whereas novelty exhibits much weaker correlation with either. These findings
support our hypothesis that strong performance in one creativity task or
dimension does not necessarily generalize to others, underscoring the need for
a holistic evaluation of LLM creativity.

</details>


### [18] [Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning](https://arxiv.org/abs/2510.20098)
*Yajie Li,Albert Galimov,Mitra Datta Ganapaneni,Pujitha Thejaswi,De Meng,Priyanshu Kumar,Saloni Potdar*

Main category: cs.CL

TL;DR: ARTER是一种混合式实体链接管线，通过对候选项的初步过滤、上下文评分、以及自适应路由将易错案交给快速链接器处理，难案使用有针对性的LLM推理，从而在不大量微调的前提下实现高性能。


<details>
  <summary>Details</summary>
Motivation: 减少对大规模带标签数据和深度微调的依赖，同时降低对LLM推理的成本与延迟。

Method: 结合嵌入和LLM信号来评估易/难样本；进行候选生成、上下文评分、适应性路由；易样本交给低成本的实体链接器（如ReFinED）处理，难样本交给定向LLM推理；在检索候选上计算小规模信号以分类情境。

Result: 在标准数据集上，ARTER相较ReFinED提升最多4.47%，平均提升2.53%，覆盖5/6数据集；在所有样本上与使用LLM推理的管线表现相当，同时在LLM令牌数方面实现约2x的效率提升。

Conclusion: 通过自适应路由和有针对性的推理，ARTER在维持高准确度的同时显著降低了LLM成本，是面向高效实体链接的实用方案。

Abstract: Entity Linking (EL) has traditionally relied on large annotated datasets and
extensive model fine-tuning. While recent few-shot methods leverage large
language models (LLMs) through prompting to reduce training requirements, they
often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER
(Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline
that achieves high performance without deep fine-tuning by strategically
combining candidate generation, context-based scoring, adaptive routing, and
selective reasoning. ARTER computes a small set of complementary signals(both
embedding and LLM-based) over the retrieved candidates to categorize contextual
mentions into easy and hard cases. The cases are then handled by a
low-computational entity linker (e.g. ReFinED) and more expensive targeted
LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms
ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets,
and performs comparably to pipelines using LLM-based reasoning for all
mentions, while being as twice as efficient in terms of the number of LLM
tokens.

</details>


### [19] [BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation](https://arxiv.org/abs/2510.20151)
*Haoyuan Li,Zhengyuan Shen,Sullam Jeoung,Yueyan Chen,Jiayu Li,Qi Zhu,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.CL

TL;DR: BoundRL is a novel approach for token-level segmentation and label prediction on long structured texts, generating only starting tokens and reconstructing contents, combined with RLVR rewards and intermediate candidates to improve performance; achieves strong results with small models on complex prompts, reducing inference cost and hallucination.


<details>
  <summary>Details</summary>
Motivation: Conventional segmentation methods struggle with structured texts containing tables, code, and placeholders; there's a need for efficient, semantically meaningful segmentation that scales to long documents and reduces cost/hallucination.

Method: BoundRL jointly performs token-level segmentation and label prediction; generates sequence of starting tokens rather than full segments; reconstructs segments by locating tokens in the original text; uses RL with verifiable rewards (RLVR) to optimize document reconstruction fidelity and semantic alignment; uses intermediate candidates via perturbing a fraction of generated sequences to prevent entropy collapse (stepping stones). Evaluated on challenging structured texts, especially long prompts for LLMs.

Result: BoundRL enables 1.7B parameter LMs to outperform few-shot prompting of much larger models; RLVR reward yields significant improvements over supervised fine-tuning; incorporating intermediate candidates further improves performance and generalization.

Conclusion: BoundRL is an effective and efficient framework for long structured text segmentation and labeling, reducing inference costs and hallucinations while enabling smaller models to rival larger ones, with RLVR and stepping-stone candidates as key components.

Abstract: As structured texts become increasingly complex across diverse domains --
from technical reports to generative AI prompts -- the need for text
segmentation into semantically meaningful components becomes critical. Such
texts often contain elements beyond plain language, including tables, code
snippets, and placeholders, which conventional sentence- or paragraph-level
segmentation methods cannot handle effectively. To address this challenge, we
propose BoundRL, a novel and efficient approach that jointly performs
token-level text segmentation and label prediction for long structured texts.
Instead of generating complete contents for each segment, it generates only a
sequence of starting tokens and reconstructs the complete contents by locating
these tokens within the original texts, thereby reducing inference costs by
orders of magnitude and minimizing hallucination. To adapt the model for the
output format, BoundRL~performs reinforcement learning with verifiable rewards
(RLVR) with a specifically designed reward that jointly optimizes document
reconstruction fidelity and semantic alignment. To mitigate entropy collapse,
it further constructs intermediate candidates by systematically perturbing a
fraction of generated sequences of segments to create stepping stones toward
higher-quality solutions. To demonstrate BoundRL's effectiveness on
particularly challenging structured texts, we focus evaluation on complex
prompts used for LLM applications. Experiments show that BoundRL enables small
language models (1.7B parameters) to outperform few-shot prompting of much
larger models. Moreover, RLVR with our designed reward yields significant
improvements over supervised fine-tuning, and incorporating intermediate
candidates further improves both performance and generalization.

</details>


### [20] [Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?](https://arxiv.org/abs/2510.20154)
*Anthony Dubreuil,Antoine Gourru,Christine Largeron,Amine Trabelsi*

Main category: cs.CL

TL;DR: Large Language Models show bias in stance detection in zero-shot settings, influenced by dialect and readability; examples include stereotypes linking certain dialects and low readability to opposing political figures.


<details>
  <summary>Details</summary>
Motivation: To address overlooked bias evaluation in stance detection and examine how sociolinguistic attributes of text influence LLM decisions in political contexts.

Method: Auto-annotate posts in existing stance datasets with two attributes (dialect/vernacular and text complexity/readability) and assess zero-shot stance detection decisions by LLMs.

Result: LLMs exhibit significant stereotypes in stance detection, indicating biased associations between sociolinguistic attributes and stance.

Conclusion: Bias in stance detection by LLMs is present and systematic, necessitating fairness-aware evaluation and mitigation approaches.

Abstract: Large Language Models inherit stereotypes from their pretraining data,
leading to biased behavior toward certain social groups in many Natural
Language Processing tasks, such as hateful speech detection or sentiment
analysis. Surprisingly, the evaluation of this kind of bias in stance detection
methods has been largely overlooked by the community. Stance Detection involves
labeling a statement as being against, in favor, or neutral towards a specific
target and is among the most sensitive NLP tasks, as it often relates to
political leanings. In this paper, we focus on the bias of Large Language
Models when performing stance detection in a zero-shot setting. We
automatically annotate posts in pre-existing stance detection datasets with two
attributes: dialect or vernacular of a specific group and text
complexity/readability, to investigate whether these attributes influence the
model's stance detection decisions. Our results show that LLMs exhibit
significant stereotypes in stance detection tasks, such as incorrectly
associating pro-marijuana views with low text complexity and African American
dialect with opposition to Donald Trump.

</details>


### [21] [DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking](https://arxiv.org/abs/2510.20168)
*Tian Lan,Bin Zhu,Qianghuai Jia,Junyang Ren,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: DeepWideSearch is a new benchmark for evaluating agents on integrating deep, multi-hop reasoning with broad, wide-scale information gathering. It reveals a large gap: state-of-the-art agents only achieve 2.39% success, with four main failure modes, and is publicly released to spur advances.


<details>
  <summary>Details</summary>
Motivation: Real-world information seeking requires both deep, multi-hop reasoning across many sources (depth) and broad data collection at scale (width). Current agents struggle to jointly optimize these aspects, hindering practical applications like market analysis.

Method: The authors construct DeepWideSearch by curating 220 questions across 15 domains, using two methods to convert existing datasets into tasks that demand both depth and width in retrieval. This creates a benchmark that forces agents to perform deep reasoning over multi-hop paths while processing large data volumes.

Result: Extensive experiments show state-of-the-art agents achieve only 2.39% average success on DeepWideSearch. Error analysis identifies four failure modes: lack of reflection, overreliance on internal knowledge, insufficient retrieval, and context overflow. They release the benchmark publicly.

Conclusion: DeepWideSearch represents a challenging, first-of-its-kind benchmark that combines depth and width in information seeking; it exposes critical limitations in current agent architectures and should catalyze future research toward more capable and robust information-seeking systems.

Abstract: Current search agents fundamentally lack the ability to simultaneously
perform \textit{deep} reasoning over multi-hop retrieval and
\textit{wide}-scale information collection-a critical deficiency for real-world
applications like comprehensive market analysis and business development. To
bridge this gap, we introduce DeepWideSearch, the first benchmark explicitly
designed to evaluate agents to integrate depth and width in information
seeking. In DeepWideSearch, agents must process a large volume of data, each
requiring deep reasoning over multi-hop retrieval paths. Specifically, we
propose two methods to converse established datasets, resulting in a curated
collection of 220 questions spanning 15 diverse domains. Extensive experiments
demonstrate that even state-of-the-art agents achieve only 2.39% average
success rate on DeepWideSearch, highlighting the substantial challenge of
integrating depth and width search in information-seeking tasks. Furthermore,
our error analysis reveals four failure modes: lack of reflection, overreliance
on internal knowledge, insufficient retrieval, and context overflow-exposing
key limitations in current agent architectures. We publicly release
DeepWideSearch to catalyze future research on more capable and robust
information-seeking agents.

</details>


### [22] [Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2510.20176)
*Yuhang Zhou,Mingrui Zhang,Ke Li,Mingyi Wang,Qiao Liu,Qifei wang,Jiayi Liu,Fei Liu,Serena Li,Weiwi Li,Mingze Gao,Abhishek Kumar,Xiangjun Fan,Zhuokai Zhao,Lizhu Zhang*

Main category: cs.CL

TL;DR: A multi-agent framework Mixture-of-Minds decomposes table reasoning into planning, coding, and answering, uses code execution for precise manipulation, and employs MCTS-based RL to self-improve, achieving strong results on TableBench.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with arithmetic errors and hallucinations in table reasoning; tool-based methods offer precision but rely on rigid schemas and lack semantic understanding; there is a need to integrate robust reasoning with flexible table processing.

Method: Three specialized agents (planning, coding, answering) decompose the task; code execution handles precise table manipulation; a self-improvement training framework uses Monte Carlo Tree Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents via reinforcement learning.

Result: Significant performance gains; 62.13% on TableBench; outperforming OpenAI-o4-mini-high; demonstrates effectiveness of combining structured multi-agent workflows with RL for table understanding.

Conclusion: Integrating structured multi-agent reasoning with reinforcement learning is a promising direction to advance robust table understanding and manipulation.

Abstract: Understanding and reasoning over tables is a critical capability for many
real-world applications. Large language models (LLMs) have shown promise on
this task, but current approaches remain limited. Fine-tuning based methods
strengthen language reasoning; yet they are prone to arithmetic errors and
hallucination. In contrast, tool-based methods enable precise table
manipulation but rely on rigid schemas and lack semantic understanding. These
complementary drawbacks highlight the need for approaches that integrate robust
reasoning with reliable table processing. In this work, we propose
Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into
three specialized roles: planning, coding, and answering. This design enables
each agent to focus on a specific aspect of the task while leveraging code
execution for precise table manipulation. Building on this workflow, we
introduce a self-improvement training framework that employs Monte Carlo Tree
Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents
with reinforcement learning (RL). Extensive experiments show that
Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and
surpassing OpenAI-o4-mini-high. These results demonstrate the promise of
combining structured multi-agent workflows with RL to advance table
understanding.

</details>


### [23] [Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models](https://arxiv.org/abs/2510.20198)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.CL

TL;DR: LLMs show limited spatial reasoning on grid-based tasks; performance drops sharply with increasing grid size, revealing a gap between linguistic and spatial representations.


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在网格化文本输入中的空间推理能力及其可扩展性，揭示语言与几何推理之间的差距。

Method: 设计五项基于网格的任务（象限识别、几何变换、距离评估、单词搜索、滑块拼图），在不同网格尺度上测试模型的基本空间推理与多步问题求解能力，评估其在文本输入上的空间理解与计算能力。

Result: 模型在小规模任务表现中等；随着规模增大，准确率快速下降，平均损失约42.7%，最高可达84%。所有起初超过50%准确率的测试，均至少丢失48%。结果指向底层架构中缺乏稳健的空间表征，显示语言与几何推理之间的差距，并为未来在语言与几何交叉领域的综合基准奠定基础。

Conclusion: 强调当前LLMs在空间推理方面的局限性，提出需要结合语言与几何的综合基准与研究方向，以提升对空间结构的理解与推理能力。

Abstract: This paper explores the spatial reasoning capability of large language models
(LLMs) over textual input through a suite of five tasks aimed at probing their
spatial understanding and computational abilities. The models were tested on
both fundamental spatial reasoning and multi-step problem-solving within
structured grid-based environments using tasks such as quadrant identification,
geometric transformations, distance evaluation, word searches, and tile
sliding. Each task was scaled in complexity through increasing grid dimensions,
requiring models to extend beyond simple pattern recognition into abstract
spatial reasoning. Our results reveal that while LLMs demonstrate moderate
success in all tasks with small complexity and size, performance drops off
rapidly as scale increases, with an average loss in accuracy of 42.7%, and
reaching as high as 84%. Every test that began with over 50% accuracy showed a
loss of at least 48%, illustrating the consistent nature of the deterioration.
Furthermore, their struggles with scaling complexity hint at a lack of robust
spatial representations in their underlying architectures. This paper
underscores the gap between linguistic and spatial reasoning in LLMs, offering
insights into their current limitations, and laying the groundwork for future
integrative benchmarks at the intersection of language and geometry.

</details>


### [24] [Decoding-Free Sampling Strategies for LLM Marginalization](https://arxiv.org/abs/2510.20208)
*David Pohl,Marco Cognetta,Junyoung Lee,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 提出无解码采样的边际估计方法以高效近似文本在不同子词tokenizations下的总概率，避免模型生成步骤，显著提速评估并在下游任务上有效应用。


<details>
  <summary>Details</summary>
Motivation: 现有评估多聚焦于特定tokenization的概率，忽略同一文本的多种表示形式，边际化能更公正地评估模型能力。但完整边际化计算成本高，因此需要不依赖生成的低成本替代方案。

Method: 研究并比较多种解码无采样策略，这些策略仅基于文本概率且与具体tokenizer无关，进行近似边际化；对若干开源模型进行实验，评估精度与运行时间。

Result: 解码无采样策略在提供足够接近真实边际概率的同时，显著降低运行时成本，并在若干下游推理任务中证明其实用性。

Conclusion: 解码无采样为LLMs的边际化评估提供高效可用的替代，便于在有限预算内进行更准确的模型比较与评估。

Abstract: Modern language models operate on subword-tokenized text in order to make a
trade-off between model size, inference speed, and vocabulary coverage. A side
effect of this is that, during inference, models are evaluated by measuring the
probability of only the specific tokenization produced as the output, despite
there being many possible ways to represent the same text with a subword
vocabulary. Recent studies have argued instead for evaluating LLMs by
marginalization - the probability mass of all tokenizations of a given text.
  Marginalization is difficult due to the number of possible tokenizations of a
text, so often approximate marginalization is done via sampling. However, a
downside of sampling is that an expensive generation step must be performed by
the LLM for each sample, which limits the number of samples that can be
acquired given a runtime budget, and therefore also the accuracy of the
approximation. Since computing the probability of a sequence given the
tokenization is relatively cheap compared to actually generating it, we
investigate sampling strategies that are decoding-free - they require no
generation from the LLM, instead relying entirely on extremely cheap sampling
strategies that are model and tokenizer agnostic.
  We investigate the approximation quality and speed of decoding-free sampling
strategies for a number of open models to find that they provide sufficiently
accurate marginal estimates at a small fraction of the runtime cost and
demonstrate its use on a set of downstream inference tasks.

</details>


### [25] [Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders](https://arxiv.org/abs/2510.20239)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.CL

TL;DR: 提出一个统一的三模态（文本-音频-视频）情感严重度框架，用于同时评估抑郁和PTSD的严重程度，并提供可解释的决策支持。


<details>
  <summary>Details</summary>
Motivation: 抑郁与PTSD常共病且表现相关，单一模态或单疾病模型难以实现跨疾病的严重度评估与临床决策支持，因此需要一个跨疾病、带有可解释性的严重度感知框架。

Method: 将访谈文本（句子级Transformer嵌入）、语音（对数Mel谱及其Δ特征）以及面部信号（动作单元、视线、头部与姿态等）进行三模态特征融合，采用经校准的后融合分类器输出抑郁（PHQ-8，5个等级）和PTSD（3个等级）的逐病种概率及特征级归因。使用DAIC衍生语料对模型进行分层交叉验证，进行消融实验以评估各模态贡献。

Result: 融合模型在多病种同时评估中胜过单模态/消融基线；在准确率与加权F1方面与最强单模态基线相当，同时提升决策曲线的效用与对噪声/缺失模态的鲁棒性。对PTSD而言，融合降低回归误差并提升分级一致性；错误多聚集在相邻等级，但极端等级识别较稳健。消融分析显示文本对抑郁严重度贡献最大，音频与面部线索对PTSD尤为关键，特征归因与语言与行为标志一致。

Conclusion: 该方法提供可重复评估与临床决策支持的跨疾病严重度感知框架，便于将可解释性引入就诊场景并实现临床人员的参与。

Abstract: Depression and post traumatic stress disorder (PTSD) often co-occur with
connected symptoms, complicating automated assessment, which is often binary
and disorder specific. Clinically useful diagnosis needs severity aware cross
disorder estimates and decision support explanations. Our unified tri modal
affective severity framework synchronizes and fuses interview text with
sentence level transformer embeddings, audio with log Mel statistics with
deltas, and facial signals with action units, gaze, head and pose descriptors
to output graded severities for diagnosing both depression (PHQ-8; 5 classes)
and PTSD (3 classes). Standardized features are fused via a calibrated late
fusion classifier, yielding per disorder probabilities and feature-level
attributions. This severity aware tri-modal affective fusion approach is demoed
on multi disorder concurrent depression and PTSD assessment. Stratified cross
validation on DAIC derived corpora outperforms unimodal/ablation baselines. The
fused model matches the strongest unimodal baseline on accuracy and weighted
F1, while improving decision curve utility and robustness under noisy or
missing modalities. For PTSD specifically, fusion reduces regression error and
improves class concordance. Errors cluster between adjacent severities; extreme
classes are identified reliably. Ablations show text contributes most to
depression severity, audio and facial cues are critical for PTSD, whereas
attributions align with linguistic and behavioral markers. Our approach offers
reproducible evaluation and clinician in the loop support for affective
clinical decision making.

</details>


### [26] [Context-level Language Modeling by Learning Predictive Context Embeddings](https://arxiv.org/abs/2510.20280)
*Beiya Dai,Yuliang Liu,Daozheng Xue,Qipeng Guo,Kai Chen,Xinbing Wang*

Main category: cs.CL

TL;DR: ContextLM 在标准自回归的下一字预测框架上增加一个“下一上下文”预测目标，学习多token上下文的预测表示，兼容现有的逐token评估。实验在GPT2、Pythia 家族（至1.5B参数）上实现困惑度和下游任务的提升，且额外开销很小。


<details>
  <summary>Details</summary>
Motivation: 单字/ token 级预测难以捕捉高层次语义结构和长距离上下文，需要一种高效的多-token上下文建模方法来提升语言建模能力。

Method: 引入 next-context prediction 目标，利用未来 token 块的误差信号，训练模型学习多 token 上下文的预测表征，同时保持与标准自回归评估（如 perplexity）的兼容性。

Result: 在 GPT‑2 与 Pythia 族别上进行扩展到 1.5B 参数的实验，进一步在困惑度和下游任务性能上取得稳定提升，表明该方法具有可扩展性且对注意力分配有更好引导，且额外计算开销极小。

Conclusion: next-context prediction 为提升语言建模提供了一条可扩展且高效的路径，在不破坏现有 NTP 框架的前提下提升长程一致性与注意力分布。

Abstract: Next-token prediction (NTP) is the cornerstone of modern large language
models (LLMs) pretraining, driving their unprecedented capabilities in text
generation, reasoning, and instruction following. However, the token-level
prediction limits the model's capacity to capture higher-level semantic
structures and long-range contextual relationships. To overcome this
limitation, we introduce \textbf{ContextLM}, a framework that augments standard
pretraining with an inherent \textbf{next-context prediction} objective. This
mechanism trains the model to learn predictive representations of multi-token
contexts, leveraging error signals derived from future token chunks. Crucially,
ContextLM achieves this enhancement while remaining fully compatible with the
standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity).
Extensive experiments on the GPT2 and Pythia model families, scaled up to
$1.5$B parameters, show that ContextLM delivers consistent improvements in both
perplexity and downstream task performance. Our analysis indicates that
next-context prediction provides a scalable and efficient pathway to stronger
language modeling, yielding better long-range coherence and more effective
attention allocation with minimal computational overhead.

</details>


### [27] [Citation Failure: Definition, Analysis and Efficient Mitigation](https://arxiv.org/abs/2510.20303)
*Jan Buchmann,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出一个两步框架来理解并缓解基于LLM的RAG系统中的引用失败：通过CITECONTROL基准分析响应与证据之间的关系对引用质量的影响；并提出CITENTION框架，结合生成、注意力和检索方法，在CITECONTROL及迁移设置中显著提升引用质量。


<details>
  <summary>Details</summary>
Motivation: 在LLM生成的回答中，引用的证据可能不完整或不对应，导致用户难以验证答案。需要将“回答质量”和“证据质量”解耦，并研究何时以及如何改进引用。

Method: Step 1：扩展现有工作，通过系统地改变回答与证据之间的关系，提出CITECONTROL基准以分析失败模式；研究关系复杂度与引用质量的关系。Step 2：提出CITENTION框架，整合生成、注意力和检索等方法以提升引用效率。

Result: 实验表明，随着关系复杂度的增加，引用失败更易发生；将多种引用方法结合可提升性能；CITENTION在CITECONTROL以及迁移设置中实现了显著的引用改进。

Conclusion: 提出了一个可操作的路径来缓解引用失败，并公开数据与代码。CITENTION在多种设置下有效提升引用质量，提升了RAG系统的可验证性。

Abstract: Citations from LLM-based RAG systems are supposed to simplify response
verification. However, this does not hold for citation failure, when a model
generates a helpful response, but fails to cite complete evidence. In contrast
to previous work, we propose to disentangle this from response failure, where
the response itself is flawed, and citing complete evidence is impossible. To
address citation failure, this work follows a two-step approach: (1) We study
when citation failure occurs and (2) how it can be mitigated. For step 1, we
extend prior work by investigating how the relation between response and
evidence affects citation quality. We introduce CITECONTROL, a benchmark that
systematically varies this relation to analyze failure modes. Experiments show
that failures increase with relational complexity and suggest that combining
citation methods could improve performance, motivating step 2. To improve LLM
citation efficiently, we propose CITENTION, a framework integrating generative,
attention-based, and retrieval-based methods. Results demonstrate substantial
citation improvements on CITECONTROL and in transfer settings. We make our data
and code publicly available.

</details>


### [28] [Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering](https://arxiv.org/abs/2510.20304)
*Lei Tang,Wei Zhou,Mohsen Mesgar*

Main category: cs.CL

TL;DR: PRMs with textual+code verification can help answer selection in TQA but generalize poorly to out-of-domain data; there is a weak link between step-level verification and final answer accuracy due to weak step dependencies.


<details>
  <summary>Details</summary>
Motivation: Extend process reward models (PRMs) to semi-structured tasks like table question answering (TQA), where irrelevant information, loosely connected steps, and domain-specific reasoning pose new challenges; assess whether PRMs improve TQA beyond math domains.

Method: Systematic evaluation of state-of-the-art generative PRMs on TQA from two angles: answer-level and step-level verification; combine textual and code-based verification for solution selection; analyze correlation between step verification and answer accuracy; perform error analysis focusing on step dependencies and causal links.

Result: PRMs that integrate textual and code verification can aid solution selection for TQA but struggle to generalize to out-of-domain data; there is a weak correlation between step-level verification performance and final answer accuracy, likely due to weak dependencies between steps and loose causal links.

Conclusion: Current PRMs have limitations on TQA; findings provide directions to develop more robust, process-aware verifiers tailored for semi-structured data tasks like table QA.

Abstract: Process reward models (PRMs) improve complex reasoning in large language
models (LLMs) by grading candidate solutions step-by-step and selecting answers
via aggregated step scores. While effective in domains such as mathematics,
their applicability to tasks involving semi-structured data, like table
question answering (TQA) remains unexplored. TQA poses unique challenges for
PRMs, including abundant irrelevant information, loosely connected reasoning
steps, and domain-specific reasoning. This work presents the first systematic
study of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from
both answer and step perspectives. Results show that PRMs that combine textual
and code verification can aid solution selection but struggle to generalize to
out-of-domain data. Analysis reveals a weak correlation between performance in
step-level verification and answer accuracy, possibly stemming from weak step
dependencies and loose causal links. Our findings highlight limitations of
current PRMs on TQA and offer valuable insights for building more robust,
process-aware verifiers.

</details>


### [29] [Teaching Language Models to Reason with Tools](https://arxiv.org/abs/2510.20342)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: 提出 CoRT：一种后训练框架，通过 Hint-Engineering 注入提示并结合拒绝采样和强化学习，教大型推理模型高效使用计算解释器，提升数学推理性能并显著降低推理时的 token 量。


<details>
  <summary>Details</summary>
Motivation: LRMs 在复杂数学推理中易出错；尽管计算工具如 CI 能帮助，但模型内在推理与外部确定性知识之间的冲突导致徒劳的推理。需要一种更好地整合方法。

Method: 提出 Hint-Engineering 数据合成策略，合成高质量的、与 CI 融合的推理数据；对 1.5B-32B 参数模型进行监督微调，使用 CoRT 的拒绝采样和强化学习来优化 CI 的多轮使用与内部思考的交错。

Result: 在五个数学推理数据集上，对 DeepSeek-R1-Distill-Qwen-32B 和 DeepSeek-R1-Distill-Qwen-1.5B 模型分别实现 4% 和 8% 的绝对提升；相比纯自然语言推理基线，32B 模型和 1.5B 模型的 token 使用量分别下降约 30% 与 50%。

Conclusion: CoRT 有效提升 LRM 与 CI 的协同能力，提供一种可复现的路径来提升精度与效率；代码与模型已发布。

Abstract: Large reasoning models (LRMs) like OpenAI-o1 have shown impressive
capabilities in natural language reasoning. However, these models frequently
demonstrate inefficiencies or inaccuracies when tackling complex mathematical
operations. While integrating computational tools such as Code Interpreters
(CIs) offers a promising solution, it introduces a critical challenge: a
conflict between the model's internal, probabilistic reasoning and the
external, deterministic knowledge provided by the CI, which often leads models
to unproductive deliberation. To overcome this, we introduce CoRT
(Code-Optimized Reasoning Training), a post-training framework designed to
teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a
new data synthesis strategy that strategically injects diverse hints at optimal
points within reasoning paths. This approach generates high-quality,
code-integrated reasoning data specifically tailored to optimize LRM-CI
interaction. Using this method, we have synthesized 30 high-quality samples to
post-train models ranging from 1.5B to 32B parameters through supervised
fine-tuning. CoRT further refines the multi-round interleaving of external CI
usage and internal thinking by employing rejection sampling and reinforcement
learning. Our experimental evaluations demonstrate CoRT's effectiveness,
yielding absolute improvements of 4\% and 8\% on DeepSeek-R1-Distill-Qwen-32B
and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging
mathematical reasoning datasets. Moreover, CoRT significantly enhances
efficiency, reducing token usage by approximately 30\% for the 32B model and
50\% for the 1.5B model compared to pure natural language reasoning baselines.
The models and code are available at: https://github.com/ChengpengLi1003/CoRT.

</details>


### [30] [Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models](https://arxiv.org/abs/2510.20351)
*Matteo Silvestri,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.CL

TL;DR: LLMs memorize tabular benchmarks; contamination arises from semantic cues; removing cues collapses performance; evaluation should separate leakage from reasoning.


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在表格推理任务中是否依赖记忆化的基准数据，而非真正的泛化能力；区分语义泄漏与真实推理能力。

Method: 在广泛使用的基准表格数据集（如 Adult Income、Titanic 等）上进行受控探查实验；通过移除或随机化数据集中的语义线索（列名、可解释的值类别）来观察性能变化；对比有线索与无线索条件下的表现。

Result: 研究发现污染效应仅在数据集中存在强语义线索时才出现；去除或随机化线索后，表现降至接近随机水平；证据表明LLMs的表格推理能力部分来自对公开数据集的记忆。

Conclusion: 需要重新设计评测协议以区分语义泄漏与真实推理能力；提出在未来评估中混同泄漏的策略的对策，例如引入线索消除、对抗性评测、以及合成数据等以更准确地衡量推理能力。

Abstract: Large Language Models (LLMs) are increasingly evaluated on their ability to
reason over structured data, yet such assessments often overlook a crucial
confound: dataset contamination. In this work, we investigate whether LLMs
exhibit prior knowledge of widely used tabular benchmarks such as Adult Income,
Titanic, and others. Through a series of controlled probing experiments, we
reveal that contamination effects emerge exclusively for datasets containing
strong semantic cues-for instance, meaningful column names or interpretable
value categories. In contrast, when such cues are removed or randomized,
performance sharply declines to near-random levels. These findings suggest that
LLMs' apparent competence on tabular reasoning tasks may, in part, reflect
memorization of publicly available datasets rather than genuine generalization.
We discuss implications for evaluation protocols and propose strategies to
disentangle semantic leakage from authentic reasoning ability in future LLM
assessments.

</details>


### [31] [FreeChunker: A Cross-Granularity Chunking Framework](https://arxiv.org/abs/2510.20356)
*Wenxuan Zhang,Yuan-Hao Jiang,Yonghe Wu*

Main category: cs.CL

TL;DR: FreeChunker: a cross-granularity encoding framework that treats sentences as atomic units and enables flexible retrieval of arbitrary sentence combinations, reducing semantic boundary detection cost while improving retrieval efficiency on LongBench V2.


<details>
  <summary>Details</summary>
Motivation: Current chunking methods rely on fixed granularity and static boundaries, which restrict adaptability to diverse queries and incur computational overhead for semantic boundary detection. A more flexible, cross-granularity encoding approach could improve efficiency and query adaptability in RAG systems.

Method: Propose FreeChunker, a Cross-Granularity Encoding Framework that treats sentences as atomic units and moves from static chunk segmentation to flexible retrieval supporting arbitrary sentence combinations. This reduces the need for semantic boundary detection and permits cross-granularity retrieval to adapt to complex queries.

Result: Empirical evaluation on LongBench V2 shows that FreeChunker achieves superior retrieval performance compared to traditional chunking methods and significantly outperforms existing approaches in computational efficiency.

Conclusion: FreeChunker enhances adaptability and efficiency in RAG systems by enabling cross-granularity encoding and sentence-atomic units, offering strong retrieval performance with lower computational overhead.

Abstract: Chunking strategies significantly impact the effectiveness of
Retrieval-Augmented Generation (RAG) systems. Existing methods operate within
fixed-granularity paradigms that rely on static boundary identification,
limiting their adaptability to diverse query requirements. This paper presents
FreeChunker, a Cross-Granularity Encoding Framework that fundamentally
transforms the traditional chunking paradigm: the framework treats sentences as
atomic units and shifts from static chunk segmentation to flexible retrieval
supporting arbitrary sentence combinations. This paradigm shift not only
significantly reduces the computational overhead required for semantic boundary
detection but also enhances adaptability to complex queries. Experimental
evaluation on LongBench V2 demonstrates that FreeChunker achieves superior
retrieval performance compared to traditional chunking methods, while
significantly outperforming existing approaches in computational efficiency.

</details>


### [32] [Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)](https://arxiv.org/abs/2510.20358)
*Francesca Padovani,Bastian Bunzeck,Manar Ali,Omar Momen,Arianna Bisazza,Hendrik Buschmeier,Sina Zarrieß*

Main category: cs.CL

TL;DR: 在对话数据上进行专门预训练的模型，在对话任务上表现突出，但在常规标准基准上表现一般；对话特定微调（如DPO）比PPO更能提升对话任务表现。


<details>
  <summary>Details</summary>
Motivation: 探究仅以对话数据进行预训练是否能得到形式上和功能上都合适的小语言模型，以及不同微调策略对对话生成的影响。

Method: 以llamalogue为基础进行仅对话数据的预训练，并应用多种微调策略（包括PPO、DPO等）以增强更具“沟通性”的文本生成；在BabyLM基准和一个自建的对话基准（在极简对照中的对话续写）上进行评估。

Result: 在大多数BabyLM标准基准上表现不佳，但在极简对照的对话续写任务上表现出色；PPO微调对模型效果有混合甚至对抗的影响；DPO微调在自建对话基准上进一步提升了性能。

Conclusion: 对话数据的专门预训练可以产生在对话领域有优势的模型，但在一般基准上可能不足；相较之下，DPO是更有效的对话任务微调策略，PPO的效果不稳定。

Abstract: We investigate whether pre-training exclusively on dialogue data results in
formally and functionally apt small language models. Based on this pre-trained
llamalogue model, we employ a variety of fine-tuning strategies to enforce
"more communicative" text generations by our models. Although our models
underperform on most standard BabyLM benchmarks, they excel at dialogue
continuation prediction in a minimal pair setting. While PPO fine-tuning has
mixed to adversarial effects on our models, DPO fine-tuning further improves
their performance on our custom dialogue benchmark.

</details>


### [33] [The Impact of Negated Text on Hallucination with Large Language Models](https://arxiv.org/abs/2510.20375)
*Jaehyung Seo,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: Negation in text challenges LLM hallucination detection; introduces NegHalu dataset; shows LLMs struggle with negated inputs; internal-state analysis reveals difficulties in mitigating negation effects.


<details>
  <summary>Details</summary>
Motivation: Understand how negation shifts affect LLM's ability to recognize and distinguish hallucinations, and address gaps in existing detection benchmarks.

Method: Create NegHalu by reconstructing existing hallucination datasets with negated expressions; run LLM-based detection experiments comparing negated vs affirmative contexts; analyze token-level internal states to trace processing of negation.

Result: LLMs often fail to detect hallucinations in negated text, producing logically inconsistent or unfaithful judgments; negation undermines context recognition; internal-state traces highlight the challenges in mitigation.

Conclusion: Negation substantially complicates hallucination detection in LLMs; calls for improved models or techniques to handle negated contexts and future research on mitigation strategies.

Abstract: Recent studies on hallucination in large language models (LLMs) have been
actively progressing in natural language processing. However, the impact of
negated text on hallucination with LLMs remains largely unexplored. In this
paper, we set three important yet unanswered research questions and aim to
address them. To derive the answers, we investigate whether LLMs can recognize
contextual shifts caused by negation and still reliably distinguish
hallucinations comparable to affirmative cases. We also design the NegHalu
dataset by reconstructing existing hallucination detection datasets with
negated expressions. Our experiments demonstrate that LLMs struggle to detect
hallucinations in negated text effectively, often producing logically
inconsistent or unfaithful judgments. Moreover, we trace the internal state of
LLMs as they process negated inputs at the token level and reveal the
challenges of mitigating their unintended effects.

</details>


### [34] [VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation](https://arxiv.org/abs/2510.20381)
*Son T. Luu,Trung Vo,Hiep Nguyen,Khanh Quoc Tran,Kiet Van Nguyen,Vu Tran,Ngan Luu-Thuy Nguyen,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: VLSP 2025 MLQA-TSR 是一个面向越南多模态法律文本的交通标志法规任务，包含多模态检索与多模态问答两个子任务，并提供基准数据集。


<details>
  <summary>Details</summary>
Motivation: 推动越南多模态法律文本处理的研究，建立可评估的基准数据集，聚焦交通标志法规领域的智能系统。

Method: 提出两大子任务：多模态法律检索与多模态问答。结合视觉与文本信息进行检索与问答评估，并在 VLSP 2025 竞赛中报道最佳结果。

Result: 在多模态法律检索任务中，F2 分数为 64.55%；在多模态问答任务中，准确率为 86.30%。

Conclusion: 此基准有助于推动越南多模态法律文本处理的研究与应用，提供可评估的数据集与任务，显示出交通标志法规领域的应用潜力与挑战。

Abstract: This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question
answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025
MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal
question answering. The goal is to advance research on Vietnamese multimodal
legal text processing and to provide a benchmark dataset for building and
evaluating intelligent systems in multimodal legal domains, with a focus on
traffic sign regulation in Vietnam. The best-reported results on VLSP 2025
MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an
accuracy of 86.30% for multimodal question answering.

</details>


### [35] [NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew](https://arxiv.org/abs/2510.20386)
*Shaltiel Shmidman,Avi Shmidman,Moshe Koppel*

Main category: cs.CL

TL;DR: 提出 NeoDictaBERT 与 NeoDictaBERT-bilingual，基于 NeoBERT 架构对希伯来语文本进行专门训练，在希伯来语基准上显著优于现有模型，且 bilingual 在检索任务上优于同等规模的多语模型。


<details>
  <summary>Details</summary>
Motivation: 尽管 BERT 系列在多任务上表现强劲，但原始架构较旧，难以充分利用更大的上下文和新型编码器的潜力；需要针对低资源语言如希伯来语的更强模型与检索能力。

Method: 采用与 NeoBERT 相同的架构对 Hebrew 文本进行训练，形成 NeoDictaBERT；另推出 NeoDictaBERT-bilingual，覆盖希伯来语与其他语言的双语检索能力，报告在多项基准上的结果，并公开模型。

Result: NeoDictaBERT 在大多数 Hebrew 基准上超越现有模型；NeoDictaBERT-bilingual 在检索任务上超越同等规模的多语言模型；展示了增强的上下文与语言覆盖带来的收益。

Conclusion: 为希伯来语 NLP 提供更强的基础模型；双语版本在跨语言检索任务中具有潜力；模型与训练过程将向社区开源，促进相关研究发展。

Abstract: Since their initial release, BERT models have demonstrated exceptional
performance on a variety of tasks, despite their relatively small size
(BERT-base has ~100M parameters). Nevertheless, the architectural choices used
in these models are outdated compared to newer transformer-based models such as
Llama3 and Qwen3. In recent months, several architectures have been proposed to
close this gap. ModernBERT and NeoBERT both show strong improvements on English
benchmarks and significantly extend the supported context window. Following
their successes, we introduce NeoDictaBERT and NeoDictaBERT-bilingual:
BERT-style models trained using the same architecture as NeoBERT, with a
dedicated focus on Hebrew texts. These models outperform existing ones on
almost all Hebrew benchmarks and provide a strong foundation for downstream
tasks. Notably, the NeoDictaBERT-bilingual model shows strong results on
retrieval tasks, outperforming other multilingual models of similar size. In
this paper, we describe the training process and report results across various
benchmarks. We release the models to the community as part of our goal to
advance research and development in Hebrew NLP.

</details>


### [36] [Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent Multi-Turn Interaction](https://arxiv.org/abs/2510.20411)
*Suchir Salhan,Hongyi Gu,Donya Rooein,Diana Galvan-Sosa,Gabrielle Gaudeau,Andrew Caines,Zheng Yuan,Paula Buttery*

Main category: cs.CL

TL;DR: 提出 ContingentChat 框架，通过对 BabyLM（100M词）进行目标后训练并使用对齐数据集来提升多轮对话中的 contingency；在语法性和连贯性方面有提升，但自适应教师解码策略的额外收益有限；表明针对性后训练对对话质量有帮助，但对婴儿语言模型而言 contingency 仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中的 contingency（及时、直接、意义明确的互动）在儿童-照顾者情境中关键，但现有 BabyLMs 数据规模有限，难以学习高质量的对话能力；需要基于后训练的对齐数据来提升对话能力并建立可评估的基线。

Method: 提出 ContingentChat，教师-学生框架，在 100M 词 BabyLM 上进行后训练；引入用于对齐的新数据集以评估和引导对话中的 contingency；对比不同的教师解码策略，评估对话质量改进。

Result: 后训练后，模型生成的回应在语法性与连贯性方面更佳；采用自适应教师解码策略的额外收益有限；整体证据显示目标导向的后训练能提升对话质量，但 contingency 仍是一个具有挑战性的目标。

Conclusion: ContingentChat 证明了有针对性的后训练和对齐数据可以提升 BabyLM 的对话质量，但要实现强 contingency 仍需进一步研究，未来工作可在更丰富的数据、不同对齐目标或解码策略上探索。

Abstract: Multi-turn dialogues between a child and a caregiver are characterized by a
property called contingency - that is, prompt, direct, and meaningful exchanges
between interlocutors. We introduce ContingentChat, a teacher-student framework
that benchmarks and improves multi-turn contingency in a BabyLM trained on 100M
words. Using a novel alignment dataset for post-training, BabyLM generates
responses that are more grammatical and cohesive. Experiments with adaptive
teacher decoding strategies show limited additional gains. ContingentChat
demonstrates the benefits of targeted post-training for dialogue quality and
indicates that contingency remains a challenging goal for BabyLMs.

</details>


### [37] [LM-mixup: Text Data Augmentation via Language Model based Mixup](https://arxiv.org/abs/2510.20449)
*Zhijie Deng,Zhouan Shen,Ling Li,Yao Zhou,Zhaowei Zhu,Yanji He,Wei Wang,Jiaheng Wei*

Main category: cs.CL

TL;DR: A method to improve instruction-tuning data quality by distilling low-quality inputs into high-quality instruction-output pairs, using a pipeline MIXTURE (144K samples) and LM-Mixup (supervised fine-tuning plus reinforcement learning with three reward signals via GRPO) to augment imperfect data effectively.


<details>
  <summary>Details</summary>
Motivation: Instruction-following data quality varies; high-quality data is scarce while low-quality data is abundant but underutilized. The authors aim to salvage and leverage low-quality data to enhance instruction-tuned LLMs.

Method: 1) Construct MIXTURE dataset pairing low-quality/incoherent instruction clusters with high-quality distillations (144K samples). 2) Fine-tune LLMs on MIXTURE and then optimize with reinforcement learning using GRPO with three rewards: quality, semantic alignment, format compliance.

Result: LM-Mixup improves performance: fine-tuning on distilled data (≈3% of full dataset) outperforms full-dataset training and rivals state-of-the-art data-selection methods across benchmarks.

Conclusion: Low-quality data, when properly distilled and augmented with LM-Mixup, is a valuable resource that enhances efficiency and performance of instruction-tuned LLMs.

Abstract: Instruction tuning is crucial for aligning Large Language Models (LLMs), yet
the quality of instruction-following data varies significantly. While
high-quality data is paramount, it is often scarce; conversely, abundant
low-quality data is frequently discarded, leading to substantial information
loss. Existing data augmentation methods struggle to augment this low-quality
data effectively, and the evaluation of such techniques remains poorly defined.
To address this, we formally define the task of Instruction Distillation:
distilling multiple low-quality and redundant inputs into high-quality and
coherent instruction-output pairs. Specifically, we introduce a comprehensive
data construction pipeline to create MIXTURE, a 144K-sample dataset pairing
low-quality or semantically redundant imperfect instruction clusters with their
high-quality distillations. We then introduce LM-Mixup, by first performing
supervised fine-tuning on MIXTURE and then optimizing it with reinforcement
learning. This process uses three complementary reward signals: quality,
semantic alignment, and format compliance, via Group Relative Policy
Optimization (GRPO). We demonstrate that LM-Mixup effectively augments
imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for
only about 3% of the entire dataset, not only surpasses full-dataset training
but also competes with state-of-the-art high-quality data selection methods
across multiple benchmarks. Our work establishes that low-quality data is a
valuable resource when properly distilled and augmented with LM-Mixup,
significantly enhancing the efficiency and performance of instruction-tuned
LLMs.

</details>


### [38] [Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models](https://arxiv.org/abs/2510.20460)
*Christian Hobelsberger,Theresa Winner,Andreas Nawroth,Oliver Mitevski,Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: 本文对四种LLM输出不确定性估计方法（VCE、MSP、Sample Consistency、CoCoA）在四个问答任务上的表现进行系统比较，发现CoCoA在校准和判别能力方面表现最佳，提出在实际应用中选择不确定性度量的建议。


<details>
  <summary>Details</summary>
Motivation: LLMs输出具有不确定性和正确性波动，单一输出可信度难以保证，需要系统评估不同不确定性度量的特征与适用性，以提升实际应用的可靠性。

Method: 在一个开源的、较先进的LLM上，对VCE、MSP、Sample Consistency、CoCoA四种方法进行四个问答任务的实验比较，评估其标定性（calibration）和判别性（discrimination），并分析各方法的权衡。

Result: 不同度量捕捉了模型信心的不同方面；混合方法CoCoA在整体可靠性方面表现最好，提升正确答案的校准度和判别能力。

Conclusion: 在LLM应用中，应根据需求权衡选择不确定性度量，若追求综合可靠性，CoCoA是较优选择；同时就每种方法的局限和适用场景给出实践建议。

Abstract: Large language models (LLMs) produce outputs with varying levels of
uncertainty, and, just as often, varying levels of correctness; making their
practical reliability far from guaranteed. To quantify this uncertainty, we
systematically evaluate four approaches for confidence estimation in LLM
outputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For
the evaluation of the approaches, we conduct experiments on four
question-answering tasks using a state-of-the-art open-source LLM. Our results
show that each uncertainty metric captures a different facet of model
confidence and that the hybrid CoCoA approach yields the best reliability
overall, improving both calibration and discrimination of correct answers. We
discuss the trade-offs of each method and provide recommendations for selecting
uncertainty measures in LLM applications.

</details>


### [39] [Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs](https://arxiv.org/abs/2510.20475)
*Lukas Edman,Alexander Fraser*

Main category: cs.CL

TL;DR: 在 BabyLM 挑战中提出一种自适应掩码语言建模策略，依据模型的预测能力动态调整被掩码的 token 概率，并引入子词嵌入，能显著提升 (Super)GLUE 成绩，且在 strict-small 子任务中击败基线。


<details>
  <summary>Details</summary>
Motivation: 旨在提升数据效率和语言理解表现，缓解标准 MLM 的局限性，通过让模型在较难预测的 token 上获得更高掩码机会，结合子词层次信息以提升形态泛化能力。

Method: 提出一个自适应 MLM 框架：掩码概率依赖于模型对每个 token 的预测能力（如预测难度），动态调整掩码分布；引入子-token（如 BPE/WordPiece）嵌入以增强形态层面的泛化；在 BabyLM 的严格小数据集上训练和评估，比较基线 MLM。

Result: 与标准 MLM 相比，在 (Super)GLUE 任务上获得显著性能提升；在 strict-small track 上优于基线。

Conclusion: 自适应掩码与子词嵌入能提升模型的形态泛化与总体表现，建议在低资源语料与小数据集场景中作为 MLM 的有效改进方向。

Abstract: We describe our strategy for the 2025 edition of the BabyLM Challenge. Our
main contribution is that of an improved form of Masked Language Modeling
(MLM), which adapts the probabilities of the tokens masked according to the
model's ability to predict them. The results show a substantial increase in
performance on (Super)GLUE tasks over the standard MLM. We also incorporate
sub-token embeddings, finding that this increases the model's morphological
generalization capabilities. Our submission beats the baseline in the
strict-small track.

</details>


### [40] [RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging](https://arxiv.org/abs/2510.20479)
*Bowen Wang,Haiyuan Wan,Liwen Shi,Chen Yang,Peng He,Yue Ma,Haochen Han,Wenhao Li,Tiao Tan,Yongjian Li,Fangming Liu,Yifan Gong,Sheng Zhang*

Main category: cs.CL

TL;DR: RECALL is a representation-aware continual learning method for LLMs that merges models without access to historical data by clustering typical samples and fusing layer-wise representations, preserving general features in shallow layers while enabling task-specific adaptation in deeper layers, achieving strong forgetting resistance and improved generalization across NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Tackle catastrophic forgetting in continual learning for large language models without access to historical data or task labels, by leveraging internal representations as proxies for learned knowledge to guide model merging.

Method: Compute inter-model similarity using layer-wise hidden representations over clustered typical samples; perform adaptive, hierarchical parameter fusion to align knowledge across models; preserve domain-general features in shallow layers and allow task-specific adaptation in deeper layers.

Result: RECALL achieves strong resistance to catastrophic forgetting and outperforms baselines across five NLP tasks and multiple CL scenarios, offering a scalable and data-free solution for evolving LLMs with better knowledge retention and generalization.

Conclusion: RECALL provides a scalable, data-free framework for continual learning in LLMs that enables cross-domain integration and preserves core general features while allowing specialization, addressing data availability constraints in evolving models.

Abstract: We unveil that internal representations in large language models (LLMs) serve
as reliable proxies of learned knowledge, and propose RECALL, a novel
representation-aware model merging framework for continual learning without
access to historical data. RECALL computes inter-model similarity from
layer-wise hidden representations over clustered typical samples, and performs
adaptive, hierarchical parameter fusion to align knowledge across models. This
design enables the preservation of domain-general features in shallow layers
while allowing task-specific adaptation in deeper layers. Unlike prior methods
that require task labels or incur performance trade-offs, RECALL achieves
seamless multi-domain integration and strong resistance to catastrophic
forgetting. Extensive experiments across five NLP tasks and multiple continual
learning scenarios show that RECALL outperforms baselines in both knowledge
retention and generalization, providing a scalable and data-free solution for
evolving LLMs.

</details>


### [41] [Steering Evaluation-Aware Language Models To Act Like They Are Deployed](https://arxiv.org/abs/2510.20487)
*Tim Tian Hua,Andrew Qin,Samuel Marks,Neel Nanda*

Main category: cs.CL

TL;DR: 本文提出一种通过在激活层添加 steering 向量来抑制模型的评估意识，从而在评估场景下表现得像处于部署状态；并通过两步训练让模型先产生评估意识，再用激活导向实现抑制。


<details>
  <summary>Details</summary>
Motivation: 当前的安全评估容易被模型察觉评估环境，从而调整行为，影响评估的可靠性。因此需要对抗或缓解这一现象以提升评估的可信度。

Method: 首先进行两步训练来诱导评估意识：在继续预训练阶段，使用描述模型的文档进行训练，并在评估阶段包含用于提示模型处于评估中的 Python 类型提示，同时部署阶段不包含；随后用专家迭代训练模型在评估情境中使用 Python 类型提示。随后基于原始模型构造一个激活层的 steering 向量，并在后续阶段应用以抑制评估意识。该 steering 向量是在额外训练之前原模型上构造的。

Result: 训练得到的模型确实表现出评估意识：在评估情境中更频繁地输出类型提示；然而，一旦移除评估线索，评估意识的差异才显现。通过激活 steering，可以在存在评估线索时让模型的行为接近部署状态，从而抑制评估意识的出错机会。

Conclusion: 本文提示评估者可以通过激活向量对模型的行为进行干预，从而提高安全评估的可靠性，但也暴露出对评估环境的可操控性风险，需要在评估设计中权衡对抗干扰的影响。

Abstract: Large language models (LLMs) can sometimes detect when they are being
evaluated and adjust their behavior to appear more aligned, compromising the
reliability of safety evaluations. In this paper, we show that adding a
steering vector to an LLM's activations can suppress evaluation-awareness and
make the model act like it is deployed during evaluation. To study our steering
technique, we train an LLM to exhibit evaluation-aware behavior using a
two-step training process designed to mimic how this behavior could emerge
naturally. First, we perform continued pretraining on documents with factual
descriptions of the model (1) using Python type hints during evaluation but not
during deployment and (2) recognizing that the presence of a certain evaluation
cue always means that it is being tested. Then, we train the model with expert
iteration to use Python type hints in evaluation settings. The resulting model
is evaluation-aware: it writes type hints in evaluation contexts more than
deployment contexts. However, this gap can only be observed by removing the
evaluation cue. We find that activation steering can suppress evaluation
awareness and make the model act like it is deployed even when the cue is
present. Importantly, we constructed our steering vector using the original
model before our additional training. Our results suggest that AI evaluators
could improve the reliability of safety evaluations by steering models to act
like they are deployed.

</details>


### [42] [Robust Preference Alignment via Directional Neighborhood Consensus](https://arxiv.org/abs/2510.20498)
*Ruochen Mao,Yuling Shi,Xiaodong Gu,Jiaheng Wei*

Main category: cs.CL

TL;DR: RPS is a post-hoc, training-free method that broadens alignment robustness by sampling multiple responses from a local directional neighborhood and selecting the best match to user intent, outperforming a strong baseline in theory and experiments.


<details>
  <summary>Details</summary>
Motivation: There is a preference coverage gap: training data biased toward dominant, average preferences causes brittleness when user preferences deviate; retraining to cover full diversity is costly and not generalizable.

Method: RPS builds a local neighborhood of related preferences around the target direction, generates multiple candidate responses from this neighborhood, and selects the one that best aligns with the user's original intent. It is post-hoc and does not retrain the model. A theoretical framework shows the neighborhood strategy is provably superior to a strong baseline that also samples multiple candidates.

Result: Empirical evaluations across DPA, DPO, and SFT show that RPS consistently improves robustness over the baseline, achieving win rates up to 69% on challenging, under-represented preferences, without any model retraining.

Conclusion: RPS offers a practical, theoretically grounded solution to improve reliability of preference-aligned models by leveraging directional neighborhood consensus in a training-free, post-hoc manner.

Abstract: Aligning large language models with human preferences is critical for
creating reliable and controllable AI systems. A human preference can be
visualized as a high-dimensional vector where different directions represent
trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet,
because the training data often reflects dominant, average preferences, LLMs
tend to perform well on common requests but fall short in specific, individual
needs. This mismatch creates a preference coverage gap. Existing methods often
address this through costly retraining, which may not be generalized to the
full spectrum of diverse preferences. This brittleness means that when a user's
request reflects a nuanced preference deviating from the training data's
central tendency, model performance can degrade unpredictably. To address this
challenge, we introduce Robust Preference Selection (RPS), a post-hoc,
training-free method by leveraging directional neighborhood consensus. Instead
of forcing a model to generate a response from a single, highly specific
preference, RPS samples multiple responses from a local neighborhood of related
preferences to create a superior candidate pool. It then selects the response
that best aligns with the user's original intent. We provide a theoretical
framework showing our neighborhood generation strategy is provably superior to
a strong baseline that also samples multiple candidates. Comprehensive
experiments across three distinct alignment paradigms (DPA, DPO, and SFT)
demonstrate that RPS consistently improves robustness against this baseline,
achieving win rates of up to 69% on challenging preferences from
under-represented regions of the space without any model retraining. Our work
presents a practical, theoretically-grounded solution for enhancing the
reliability of preference-aligned models.

</details>


### [43] [Assessing the Political Fairness of Multilingual LLMs: A Case Study based on a 21-way Multiparallel EuroParl Dataset](https://arxiv.org/abs/2510.20508)
*Paul Lerner,François Yvon*

Main category: cs.CL

TL;DR: 通过多语言翻译公平性框架分析大模型的政治偏见，使用欧洲议会演讲的21路多平行语料，发现多数党派的翻译质量优于边缘党派，并提供规模化数据集用于研究。


<details>
  <summary>Details</summary>
Motivation: 现有评估多集中于英文问卷，忽略多语言场景中的公平性；提出将翻译公平性作为评估政治偏见的新视角，并以EP演讲数据和新的21路EuroParl多平行语料来揭示跨语言的系统性差异。

Method: 系统地比较EP演讲的翻译质量，利用21路并行的EuroParl数据集（1.5M句子、4000万词、2.49亿字符），覆盖3年、1000+发言人、7国、12个EU政党、25个EU委员会等，标注了演讲者政治隶属。

Result: 观察到系统性差异：左、中心、右的多数党通常比边缘党派获得更好的翻译质量。

Conclusion: 提供一个大规模的多语翻译公平性数据框架，支持对LLMs政治偏见的跨语言评估与减缓翻译偏见的研究。

Abstract: The political biases of Large Language Models (LLMs) are usually assessed by
simulating their answers to English surveys. In this work, we propose an
alternative framing of political biases, relying on principles of fairness in
multilingual translation. We systematically compare the translation quality of
speeches in the European Parliament (EP), observing systematic differences with
majority parties from left, center, and right being better translated than
outsider parties. This study is made possible by a new, 21-way multiparallel
version of EuroParl, the parliamentary proceedings of the EP, which includes
the political affiliations of each speaker. The dataset consists of 1.5M
sentences for a total of 40M words and 249M characters. It covers three years,
1000+ speakers, 7 countries, 12 EU parties, 25 EU committees, and hundreds of
national parties.

</details>


### [44] [ARC-Encoder: learning compressed text representations for large language models](https://arxiv.org/abs/2510.20535)
*Hippolyte Pilchen,Edouard Grave,Patrick Pérez*

Main category: cs.CL

TL;DR: ARC-Encoder 将上下文压缩为较少的连续表示，替代文本 token 输入到解码器中，以降低推理成本并在不修改解码器结构的前提下实现跨多种解码器的通用性，同时在多项基准上达到或接近最先进水平。


<details>
  <summary>Details</summary>
Motivation: 在不对解码器微调或改动架构的前提下，减少大语言模型在长上下文中的推理成本和带宽需求，提供一个可移植的、跨解码器通用的上下文压缩方案。

Method: 设计并训练一个自适应文本表示压缩器 ARC-Encoder，将上下文压缩为 x 倍较少的连续表示（通常 x ∈ {4,8}），并在不同解码器上测试其对任务的提升。系统性比较不同训练策略与架构选择，评估在指令式与基础解码器、从上下文学习、扩展上下文窗口等场景中的表现，并验证单一编码器对多解码器的通用性。

Result: ARC-Encoder 在多种使用场景下实现了对数个基准的 state-of-the-art 表现，同时在推理阶段显著提高了计算效率，并且能够同时适配多种解码器，展示了一个可移植的、跨解码器的高效上下文表示压缩方案。

Conclusion: ARC-Encoder 提供了一种灵活、有效的上下文压缩方案，能够以连续表示替代文本 token，将一个编码器泛化到多种大语言模型解码器，显著降低推理成本并维持或提升性能。

Abstract: Recent techniques such as retrieval-augmented generation or chain-of-thought
reasoning have led to longer contexts and increased inference costs. Context
compression techniques can reduce these costs, but the most effective
approaches require fine-tuning the target model or even modifying its
architecture. This can degrade its general abilities when not used for this
specific purpose. Here we explore an alternative approach: an encoder that
compresses the context into continuous representations which replace token
embeddings in decoder LLMs. First, we perform a systematic study of training
strategies and architecture choices for the encoder. Our findings led to the
design of an Adaptable text Representations Compressor, named ARC-Encoder,
which outputs $x$-times fewer continuous representations (typically
$x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety
of LLM usage scenarios, ranging from in-context learning to context window
extension, on both instruct and base decoders. Results show that ARC-Encoder
achieves state-of-the-art performance on several benchmarks while improving
computational efficiency at inference. Finally, we demonstrate that our models
can be adapted to multiple decoders simultaneously, allowing a single encoder
to generalize across different decoder LLMs. This makes ARC-Encoder a flexible
and efficient solution for portable encoders that work seamlessly with multiple
LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder
, fine-tuning dataset and pretrained models are available at
https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .

</details>


### [45] [The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts](https://arxiv.org/abs/2510.20543)
*Sangmitra Madhusudan,Kaige Chen,Ali Emami*

Main category: cs.CL

TL;DR: CenterBench is a new dataset with 9,720 center-embedded sentence comprehension questions pairing plausible and semantically implausible sentences to diagnose whether language models rely on syntax or semantic pattern matching. It shows that model performance gaps between plausible and implausible sentences widen with nesting complexity, indicating a shift away from structural analysis; semantic plausibility can harm causal-action questions; reasoning models improve but reveal semantic shortcuts.


<details>
  <summary>Details</summary>
Motivation: There is a persistent lack of methods to distinguish when language models parse structural syntax versus relying on semantic or pattern-based cues. A benchmark that systematically varies syntactic complexity and semantic plausibility is needed to diagnose this shift in model behavior.

Method: Construct 9,720 questions on center-embedded sentences (e.g., The cat [that the dog chased] meowed) paired with semantically implausible counterparts (e.g., mailmen prescribe medicine). Each item includes six questions targeting surface understanding, syntactic dependencies, and causal reasoning. Six models were evaluated across varying nesting depths to measure performance gaps between plausible and implausible sentences. The authors also analyze traces of reasoning models to identify semantic shortcuts and refusals.

Result: Across models, the plausible-implausible performance gap widened with structural complexity (up to 26.8 percentage points). Semantic plausibility sometimes harmed performance on causal-reasoning questions. Reasoning models improved accuracy but still relied on semantic shortcuts, showed overthinking, and sometimes refused to answer. Humans displayed variable semantic effects. CenterBench is described as the first framework to identify when models shift from structural analysis to pattern matching.

Conclusion: CenterBench provides a new, first-of-its-kind benchmark and analysis framework to diagnose whether LMs rely on syntax or semantic patterns, enabling more precise evaluation of syntactic competence and the limits of pattern-based reasoning.

Abstract: When language models correctly parse "The cat that the dog chased meowed,"
are they analyzing syntax or simply familiar with dogs chasing cats? Despite
extensive benchmarking, we lack methods to distinguish structural understanding
from semantic pattern matching. We introduce CenterBench, a dataset of 9,720
comprehension questions on center-embedded sentences (like "The cat [that the
dog chased] meowed") where relative clauses nest recursively, creating
processing demands from simple to deeply nested structures. Each sentence has a
syntactically identical but semantically implausible counterpart (e.g., mailmen
prescribe medicine, doctors deliver mail) and six comprehension questions
testing surface understanding, syntactic dependencies, and causal reasoning.
Testing six models reveals that performance gaps between plausible and
implausible sentences widen systematically with complexity, with models showing
median gaps up to 26.8 percentage points, quantifying when they abandon
structural analysis for semantic associations. Notably, semantic plausibility
harms performance on questions about resulting actions, where following causal
relationships matters more than semantic coherence. Reasoning models improve
accuracy but their traces show semantic shortcuts, overthinking, and answer
refusal. Unlike models whose plausibility advantage systematically widens with
complexity, humans shows variable semantic effects. CenterBench provides the
first framework to identify when models shift from structural analysis to
pattern matching.

</details>


### [46] [GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning](https://arxiv.org/abs/2510.20548)
*Jinchang Luo,Mingquan Cheng,Fan Wan,Ni Li,Xiaoling Xia,Shuangshuang Tian,Tingcheng Bian,Haiwei Wang,Haohuan Fu,Yan Tao*

Main category: cs.CL

TL;DR: 提出 GlobalRAG，利用强化学习提升多跳问答中的全局推理，通过将问题分解为子目标、协调检索与推理、迭代改进证据，配以新的奖励信号和权重退火策略，在数据较少的情况下显著提升 EM 和 F1。


<details>
  <summary>Details</summary>
Motivation: 当前多跳问答中存在全局规划缺失和执行不一致的问题，导致难以高效地进行跨步推理和证据整合；需要一个能够指导全局规划与证据使用的学习框架。

Method: 提出 GlobalRAG 框架：将问题分解为子目标、对检索与推理进行协同、迭代改正证据；设计 Planning Quality Reward 与 SubGoal Completion Reward 来引导计划质量与子目标执行；采用渐进式权重退火平衡过程目标与结果目标。

Result: 在领域内外基准上，GlobalRAG 显著优于强基线，且仅用 8k 训练数据（为强基线的 42%），平均在 EM 和 F1 上提升约 14.2%。

Conclusion: 通过强化学习驱动的全局推理与规划，提升多跳问答的性能和数据效率；新设计的奖励与退火策略促进了连贯的子目标规划与可靠的证据使用，具备跨领域鲁棒性。

Abstract: Reinforcement learning has recently shown promise in improving
retrieval-augmented generation (RAG). Despite these advances, its effectiveness
in multi-hop question answering (QA) remains limited by two fundamental
limitations: (i) global planning absence to structure multi-step reasoning, and
(ii) unfaithful execution, which hinders effective query formulation and
consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement
learning framework designed to enhance global reasoning in multi-hop QA.
GlobalRAG decomposes questions into subgoals, coordinates retrieval with
reasoning, and refines evidence iteratively. To guide this process, we
introduce Planning Quality Reward and SubGoal Completion Reward, which
encourage coherent planning and reliable subgoal execution. In addition, a
progressive weight annealing strategy balances process-oriented and
outcome-based objectives. Extensive experiments on both in-domain and
out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms
strong baselines while using only 8k training data (42% of the training data
used by strong baselines), achieving average improvements of 14.2% in both EM
and F1.

</details>


### [47] [Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks](https://arxiv.org/abs/2510.20584)
*Jiangang Hao,Wenju Cui,Patrick Kyllonen,Emily Kerzabi*

Main category: cs.CL

TL;DR: ChatGPT 基于自动编码对协作沟通数据的分类在性别和种族维度上未发现显著偏见，支持其在大规模协作与沟通评估中的应用


<details>
  <summary>Details</summary>
Motivation: 降低劳动密集的人工编码成本。尽管以往研究显示 ChatGPT 能按编码 rubrics 进行编码且接近人类准确性，但是否存在对不同性别和种族群体的偏见尚不清楚，因此需要检验其公平性。

Method: 使用典型的协作问题解决编码框架，对三类协作任务（谈判、问题解决、决策）中的沟通数据进行 ChatGPT 自动编码，比较性别与种族组别之间的差异。

Result: 研究结果显示基于 ChatGPT 的编码对性别和种族群体不存在显著偏见。

Conclusion: 为大规模协作与沟通评估中的自动编码提供可行性证据，提升评估效率。但需关注样本量、编码框架的普适性与跨域推广的局限性等。

Abstract: Assessing communication and collaboration at scale depends on a labor
intensive task of coding communication data into categories according to
different frameworks. Prior research has established that ChatGPT can be
directly instructed with coding rubrics to code the communication data and
achieves accuracy comparable to human raters. However, whether the coding from
ChatGPT or similar AI technology exhibits bias against different demographic
groups, such as gender and race, remains unclear. To fill this gap, this paper
investigates ChatGPT-based automated coding of communication data using a
typical coding framework for collaborative problem solving, examining
differences across gender and racial groups. The analysis draws on data from
three types of collaborative tasks: negotiation, problem solving, and decision
making. Our results show that ChatGPT-based coding exhibits no significant bias
across gender and racial groups, paving the road for its adoption in
large-scale assessment of collaboration and communication.

</details>


### [48] [BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection](https://arxiv.org/abs/2510.20610)
*Ali Zain,Sareem Farooqui,Muhammad Rafi*

Main category: cs.CL

TL;DR: 对Ara-GenEval任务的研究，BUSTED团队通过微调 AraELECTRA、CAMeLBERT、XLM-RoBERTa 等模型进行阿拉伯AI生成文本检测的比较，结果显示多语言模型XLM-RoBERTa表现最佳，F1=0.7701，胜过专门的阿拉伯模型。


<details>
  <summary>Details</summary>
Motivation: 探究在阿拉伯AI文本检测任务中，是否多语言模型能超越专门的阿拉伯模型，以及不同模型的泛化能力与对数据集的敏感性。

Method: 对 AraELECTRA、CAMeLBERT、XLM-RoBERTa 在给定数据集上进行二分类微调与评估，比较它们的性能。

Result: XLM-RoBERTa取得最高F1分数0.7701，超越专门的阿拉伯模型，团队在比赛中获第5名。强调多语言模型的鲁棒性与跨语言泛化能力。

Conclusion: 多语言预训练模型在阿拉伯AI文本检测领域显示出显著的泛化能力，阿拉伯专用模型不一定总是最佳选择，任务复杂性要求综合考虑模型类型与数据特性。

Abstract: This paper details our submission to the Ara- GenEval Shared Task on Arabic
AI-generated text detection, where our team, BUSTED, se- cured 5th place. We
investigated the effec- tiveness of three pre-trained transformer mod- els:
AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each
model on the provided dataset for a binary classification task. Our findings
revealed a sur- prising result: the multilingual XLM-RoBERTa model achieved the
highest performance with an F1 score of 0.7701, outperforming the spe- cialized
Arabic models. This work underscores the complexities of AI-generated text
detection and highlights the strong generalization capa- bilities of
multilingual models.

</details>


### [49] [Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model](https://arxiv.org/abs/2510.20635)
*Haoyu Wang,Sihang Jiang,Yuyan Chen,Yitong Wang,Yanghua Xiao*

Main category: cs.CL

TL;DR: 通过将五维好奇量表改编为对LLMs的评估框架，发现LLMs在信息Seeking方面的好奇心超越人类，但在不确定情境中趋于保守；好奇行为可提升推理与主动学习能力，表明LLMs可能具备与人类相似的好奇驱动学习潜力。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否具备类似人类的好奇驱动学习能力，并以5DCR（五维好奇量表）为基线，构建可量化的评估框架，推进对LLMs学习机制的理解与应用。

Method: 在5DCR的基础上，设计覆盖信息Seeking、刺激追求、社会好奇等维度的评估框架；通过将量表概念映射到LLMs的提示/任务场景，评估其在不同情境中的好奇行为及与思维/主动学习的关系。

Result: 结果显示LLMs的知识欲望强于人类，但遇到不确定环境时倾向保守选择；好奇行为与模型推理与主动学习能力的提升相关，表明在某些条件下LLMs能表现出类似人类的好奇驱动。

Conclusion: 证实在LLMs中存在与人类相似的好奇驱动学习的潜力，支持未来在学习能力与创新研究中发展基于好奇的自适应学习机制与架构。

Abstract: Curiosity serves as a pivotal conduit for human beings to discover and learn
new knowledge. Recent advancements of large language models (LLMs) in natural
language processing have sparked discussions regarding whether these models
possess capability of curiosity-driven learning akin to humans. In this paper,
starting from the human curiosity assessment questionnaire Five-Dimensional
Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework
that covers dimensions such as Information Seeking, Thrill Seeking, and Social
Curiosity to assess the extent of curiosity exhibited by LLMs. The results
demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but
still tend to make conservative choices when faced with uncertain environments.
We further investigated the relationship between curiosity and thinking of
LLMs, confirming that curious behaviors can enhance the model's reasoning and
active learning abilities. These findings suggest that LLMs have the potential
to exhibit curiosity similar to that of humans, providing experimental support
for the future development of learning capabilities and innovative research in
LLMs.

</details>


### [50] [\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding](https://arxiv.org/abs/2510.20670)
*Junghyun Min,York Hay Ng,Sophia Chan,Helena Shunhua Zhao,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 提出 CantoNLU，面向粤语的自然语言理解基准及基线模型评估，覆盖七类任务。


<details>
  <summary>Details</summary>
Motivation: 粤语资源匮乏，缺乏统一评测框架，政策与双语地位导致粤语评测困难，需要建立可比、可复现的粤语NLU基准。

Method: 构建七任务的粤语NLU基准 CantoNLU，涵盖词义消歧、语言可接受性判定、语言检测、自然语言推理、情感分析、词性标注、依存句法分析；提供基线包括：未经粤语训练的普通话模型、通过对粤语文本进行持续预训练的粤语适配模型（两种）、以及从零开始训练的单粤语模型；并给出性能评估。

Result: 粤语适配模型总体表现最佳，单语粤语模型在句法任务上表现更好，普通话模型在某些场景仍具竞争力，表明在粤语领域数据稀缺时直接迁移可能有用。

Conclusion: 公开数据、代码和模型权重以促进未来粤语NLP研究。

Abstract: Cantonese, although spoken by millions, remains under-resourced due to policy
and diglossia. To address this scarcity of evaluation frameworks for Cantonese,
we introduce \textsc{\textbf{CantoNLU}}, a benchmark for Cantonese natural
language understanding (NLU). This novel benchmark spans seven tasks covering
syntax and semantics, including word sense disambiguation, linguistic
acceptability judgment, language detection, natural language inference,
sentiment analysis, part-of-speech tagging, and dependency parsing. In addition
to the benchmark, we provide model baseline performance across a set of models:
a Mandarin model without Cantonese training, two Cantonese-adapted models
obtained by continual pre-training a Mandarin model on Cantonese text, and a
monolingual Cantonese model trained from scratch. Results show that
Cantonese-adapted models perform best overall, while monolingual models perform
better on syntactic tasks. Mandarin models remain competitive in certain
settings, indicating that direct transfer may be sufficient when Cantonese
domain data is scarce. We release all datasets, code, and model weights to
facilitate future research in Cantonese NLP.

</details>


### [51] [Neural Diversity Regularizes Hallucinations in Small Models](https://arxiv.org/abs/2510.20690)
*Kushal Chakrabarti,Nirmal Balachundhar*

Main category: cs.CL

TL;DR: 神经多样性（神经多样性指 decorrelated 的并行表示）作为语言模型的新尺度能够在固定参数/数据预算下显著降低幻觉率。引入 ND-LoRA，将并行 LoRA 适配器与 Barlow Twins 正则化结合，幻觉率最多下降 25.6%（平均 14.6%），且不损害一般准确率；神经多样性是缩放的第三个维度。


<details>
  <summary>Details</summary>
Motivation: 尽管增加参数、计算量和数据量，语言模型仍频繁发生幻觉。受投资组合理论的启发，研究者提出表示层的非相关性可降低幻觉风险，并给出幻觉概率受表示相关性的界限，从而预测需要在不同任务中寻找最优的神经多样性水平。

Method: 提出 ND-LoRA：在模型中并行使用多个 LoRA 适配器，并引入 Barlow Twins 正则化以降低输出表征之间的相关性，从而实现神经多样性。理论上给出幻觉概率上界与表示相关性之间的关系，并通过消融实验、因果干预和相关性分析验证神经多样性在幻觉降低中的中介作用。

Result: 在固定预算下，ND-LoRA 将幻觉降低最多 25.6%，平均下降 14.6%，且不降低一般性准确率。LoRA 适配器与正则化存在协同效应，因果干预证实神经多样性是中介因素；相关性分析显示规模效应：神经相关性提升 0.1% 即伴随幻觉提升约 3.8%。不同任务对神经多样性的最优水平不同，呈现任务依赖性。

Conclusion: 神经多样性作为缩放的第三个轴，独立于参数和数据，可在固定预算内提升语言模型的可靠性。该方法为在实际应用中微调模型时提供新的调控维度与优化思路。

Abstract: Language models continue to hallucinate despite increases in parameters,
compute, and data. We propose neural diversity -- decorrelated parallel
representations -- as a principled mechanism that reduces hallucination rates
at fixed parameter and data budgets. Inspired by portfolio theory, where
uncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucination
probability is bounded by representational correlation: $P(H) \leq
f(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that language
models need an optimal amount of neurodiversity. To validate this, we introduce
ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA
adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces
hallucinations by up to 25.6% (and 14.6% on average) without degrading general
accuracy. Ablations show LoRA adapters and regularization act synergistically,
causal interventions prove neurodiversity as the mediating factor and
correlational analyses indicate scale: a 0.1% neural correlation increase is
associated with a 3.8% hallucination increase. Finally, task-dependent
optimality emerges: different tasks require different amounts of optimal
neurodiversity. Together, our results highlight neural diversity as a third
axis of scaling -- orthogonal to parameters and data -- to improve the
reliability of language models at fixed budgets.

</details>


### [52] [Structure-Conditional Minimum Bayes Risk Decoding](https://arxiv.org/abs/2510.20700)
*Bryan Eikema,Anna Rutkiewicz,Mario Giulianelli*

Main category: cs.CL

TL;DR: MBR解码在标准相似性效用下在开放式任务中对潜在结构的敏感性不足，本文提出三种轻量化的效用函数改造以增强对潜在结构的识别，并在对话行为、情感和回应结构等 latent 结构上构建数据集与评估。通过两类结构最优性度量，证明改造显著优于传统相似性效用；在 AlpacaEval 与 MT-Bench 上实现最高13.7个百分点的胜率提升。


<details>
  <summary>Details</summary>
Motivation: 在开放式对话和指令执行场景中，结果空间高度多样，单纯的相似性基MBR倾向于选择更符合模型分布的通用响应，而非对某一具体潜在结构的最优解，因此需要让MBR对结构变异更加敏感。

Method: 提出三种轻量改造的MBR效用函数，以提升对话行为、情感和回应结构等潜在结构的敏感性。构建数据集以覆盖三种潜在结构，并提出两种用于评估MBR在结构维度上的最优性的度量。通过在真实基准（AlpacaEval、MT-Bench）上的对比实验，验证改造效果。

Result: 相较于常用相似性效用，改造后的MBR在结构最优性评估上有显著提升；在 AlpacaEval 与 MT-Bench 上，结构敏感的MBR提高了生成质量，胜率提升可达约13.7个百分点。

Conclusion: 增强MBR对潜在结构的敏感性可以提升开放式任务中的生成质量与结构一致性，尤其在指令遵循等场景中具有实用价值。

Abstract: Minimum Bayes Risk (MBR) decoding has seen renewed interest as an alternative
to traditional generation strategies. While MBR has proven effective in machine
translation, where the variability of a language model's outcome space is
naturally constrained, it may face challenges in more open-ended tasks such as
dialogue or instruction-following. We hypothesise that in such settings,
applying MBR with standard similarity-based utility functions may result in
selecting responses that are broadly representative of the model's
distribution, yet sub-optimal with respect to any particular grouping of
generations that share an underlying latent structure. In this work, we
introduce three lightweight adaptations to the utility function, designed to
make MBR more sensitive to structural variability in the outcome space. To test
our hypothesis, we curate a dataset capturing three representative types of
latent structure: dialogue act, emotion, and response structure (e.g., a
sentence, a paragraph, or a list). We further propose two metrics to evaluate
the structural optimality of MBR. Our analysis demonstrates that common
similarity-based utility functions fall short by these metrics. In contrast,
our proposed adaptations considerably improve structural optimality. Finally,
we evaluate our approaches on real-world instruction-following benchmarks,
AlpacaEval and MT-Bench, and show that increased structural sensitivity
improves generation quality by up to 13.7 percentage points in win rate.

</details>


### [53] [Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing](https://arxiv.org/abs/2510.20727)
*Xizhi Wu,Madeline S. Kreider,Philip E. Empey,Chenyu Li,Yanshan Wang*

Main category: cs.CL

TL;DR: LLM为主的NLP在从临床记载中提取氟嘧啶治疗与不良反应方面表现最好；零样本和错误分析提示在数据量有限的情况下也能达到高精度，但深度学习受限于数据量且泛化性较差。


<details>
  <summary>Details</summary>
Motivation: 在结直肠癌和乳腺癌治疗中，化疗药物相关的不良反应常被记录在临床笔记中，准确提取治疗信息与毒性信息有助于药物警戒和研究。

Method: 创建236份临床笔记的金标准数据集，标注治疗方案与毒性类别。比较多种方法：规则基、机器学习（随机森林、SVM、逻辑回归）、深度学习（BERT、ClinicalBERT）、大语言模型（零-shot与错误分析提示）。80:20训练-测试分割。

Result: 错误分析提示在治疗与毒性提取上达到最佳精度、召回率和F1分数（F1=1.000）。零-shot提示下治疗F1=1.000、毒性F1=0.876。逻辑回归与SVM在毒性方面F1=0.937；BERT/ClinicalBERT治疗F1约0.873-0.886，毒性F1约0.839-0.886。规则基线F1约0.857（治疗）与0.858（毒性）。总体而言，LLM方法优于其他方法，深度学习因数据较小而泛化性受限。

Conclusion: LLM驱动的NLP在从临床笔记中提取氟嘧啶治疗与毒性信息方面最有效，具有支撑肿瘤研究和药物警戒的潜力；但数据规模有限可能影响一般化与对罕见类别的表现，需要进一步验证与扩展数据集。

Abstract: Objective: Fluoropyrimidines are widely prescribed for colorectal and breast
cancers, but are associated with toxicities such as hand-foot syndrome and
cardiotoxicity. Since toxicity documentation is often embedded in clinical
notes, we aimed to develop and evaluate natural language processing (NLP)
methods to extract treatment and toxicity information.
  Materials and Methods: We constructed a gold-standard dataset of 236 clinical
notes from 204,165 adult oncology patients. Domain experts annotated categories
related to treatment regimens and toxicities. We developed rule-based, machine
learning-based (Random Forest, Support Vector Machine [SVM], Logistic
Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language
models (LLM)-based NLP approaches (zero-shot and error-analysis prompting).
Models used an 80:20 train-test split.
  Results: Sufficient data existed to train and evaluate 5 annotated
categories. Error-analysis prompting achieved optimal precision, recall, and F1
scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot
prompting reached F1=1.000 for treatment and F1=0.876 for toxicities
extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning
underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and
ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods
served as our baseline with F1 scores of 0.857 in treatment and 0.858 in
toxicities.
  Discussion: LMM-based approaches outperformed all others, followed by machine
learning methods. Machine and deep learning approaches were limited by small
training data and showed limited generalizability, particularly for rare
categories.
  Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine
treatment and toxicity information from clinical notes, and has strong
potential to support oncology research and pharmacovigilance.

</details>


### [54] [Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost](https://arxiv.org/abs/2510.20780)
*Runzhe Zhan,Zhihong Huang,Xinyi Yang,Lidia S. Chao,Min Yang,Derek F. Wong*

Main category: cs.CL

TL;DR: LRMs可作为MT质量评估者；通过对其“思考过程”进行合成轨迹的校准，显著提升评估性能与效率；在WMT24基准上，7B-32B模型均获益，7B模型达到+8.7相关提升。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型作为机器翻译质量评估者的可行性，解决LRMs在评估中的思考偏差与打分偏高问题

Method: 在LRMs上训练合成的人类式思考轨迹，使其输出更符合人类评估思路；对WMT24 Metrics基准进行评估，覆盖7B-32B规模的LRMs

Result: 思考预算约降低35倍；评估性能显著提升，7B模型R1-Distill-Qwen-7B获得+8.7相关提升

Conclusion: 经校准的LRMs具备在细粒度MT评估中的潜力，同时带来显著的效率提升，推动自动评估在实际应用中的落地。

Abstract: Recent advancements in large reasoning models (LRMs) have introduced an
intermediate "thinking" process prior to generating final answers, improving
their reasoning capabilities on complex downstream tasks. However, the
potential of LRMs as evaluators for machine translation (MT) quality remains
underexplored. We provides the first systematic analysis of LRM-as-a-judge in
MT evaluation. We identify key challenges, revealing LRMs require tailored
evaluation materials, tend to "overthink" simpler instances and have issues
with scoring mechanisms leading to overestimation. To address these, we propose
to calibrate LRM thinking by training them on synthetic, human-like thinking
trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this
approach largely reduces thinking budgets by ~35x while concurrently improving
evaluation performance across different LRM scales from 7B to 32B (e.g.,
R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These
findings highlight the potential of efficiently calibrated LRMs to advance
fine-grained automatic MT evaluation.

</details>


### [55] [A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text](https://arxiv.org/abs/2510.20782)
*Alicia Sagae,Chia-Jung Lee,Sandeep Avula,Brandon Dang,Vanessa Murdock*

Main category: cs.CL

TL;DR: A dataset-driven approach for evaluating LLMs on fairness-aware product-description generation, revealing gaps in quality, veracity, safety, and fairness, and providing a concrete evaluation resource for the research community.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation focuses on generic text generation, which misses application-specific fairness concerns. In real-world tasks, protected attributes intersect with domain features (e.g., product categories, gendered language) and can affect outcomes. There is a need for an evaluation framework that targets Responsible AI dimensions in concrete applications.

Method: Construct a real-world task-driven dataset: generate a plain-text product description from a list of product features, parameterized by fairness attributes intersected with gendered adjectives and product categories. Create a rich set of labeled prompts. Use the data to assess LLMs along quality, veracity, safety, and fairness metrics, and propose a concrete evaluation resource for the community.

Result: The dataset enables identification of quality, veracity, safety, and fairness gaps in LLMs for the task; shows how to apply the dataset for evaluation and proposes a concrete resource for researchers.

Conclusion: The work contributes a task- and attribute-driven evaluation approach for responsible AI in LLMs and provides a practical resource to advance community efforts in fair and safe model evaluation.

Abstract: Current methods for evaluating large language models (LLMs) typically focus
on high-level tasks such as text generation, without targeting a particular AI
application. This approach is not sufficient for evaluating LLMs for
Responsible AI dimensions like fairness, since protected attributes that are
highly relevant in one application may be less relevant in another. In this
work, we construct a dataset that is driven by a real-world application
(generate a plain-text product description, given a list of product features),
parameterized by fairness attributes intersected with gendered adjectives and
product categories, yielding a rich set of labeled prompts. We show how to use
the data to identify quality, veracity, safety, and fairness gaps in LLMs,
contributing a proposal for LLM evaluation paired with a concrete resource for
the research community.

</details>


### [56] [Simple Context Compression: Mean-Pooling and Multi-Ratio Training](https://arxiv.org/abs/2510.20797)
*Yair Feldman,Yoav Artzi*

Main category: cs.CL

TL;DR: Mean-pooling soft compression outperforms compression-tokens for RAG with LLMs, offering robustness across datasets and model scales; training for multiple compression ratios yields only modest performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Reduce computational costs of long-context processing in retrieval-augmented generation by compressing the input into a shorter continuous representation.

Method: Introduce a lightweight mean-pooling compression; compare against the widely used compression-tokens architecture; train the compressor to output multiple compression ratios; conduct extensive experiments across in-domain and out-of-domain QA datasets, different model families and scales, and various compression ratios.

Result: Mean-pooling achieves the strongest performance across experiments; training for multiple compression ratios incurs only a small performance drop; the trade-offs between compression methods are nuanced and depend on architecture and training regime.

Conclusion: A simple mean-pooling approach provides a strong, robust baseline for soft context compression in RAG; the effectiveness of compression methods is nuanced, but mean-pooling stands out in the reported experiments.

Abstract: A common strategy to reduce the computational costs of using long contexts in
retrieval-augmented generation (RAG) with large language models (LLMs) is soft
context compression, where the input sequence is transformed into a shorter
continuous representation. We develop a lightweight and simple mean-pooling
approach that consistently outperforms the widely used compression-tokens
architecture, and study training the same compressor to output multiple
compression ratios. We conduct extensive experiments across in-domain and
out-of-domain QA datasets, as well as across model families, scales, and
compression ratios. Overall, our simple mean-pooling approach achieves the
strongest performance, with a relatively small drop when training for multiple
compression ratios. More broadly though, across architectures and training
regimes the trade-offs are more nuanced, illustrating the complex landscape of
compression methods.

</details>


### [57] [On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?](https://arxiv.org/abs/2510.20810)
*Mingmeng Geng,Thierry Poibeau*

Main category: cs.CL

TL;DR: The abstract argues that there is no consistent definition of LLM-generated text, and that current detectors face strong challenges due to diverse models, varying use cases, human edits, and real-world conditions; benchmarks and evaluations are inadequate, so detector results should be treated as references rather than decisive indicators.


<details>
  <summary>Details</summary>
Motivation: To clarify what constitutes LLM-generated text and to improve evaluation of detectors in diverse, real-world scenarios where prompts, edits, and user interactions blur the line between machine and human authorship.

Method: Conceptual analysis and critical review of existing definitions, benchmarks, and evaluation practices; discussion of factors affecting detection in real-world settings (model diversity, edits, usage contexts).

Result: Findings indicate that detectors are sensitive to definition choice and scenario; benchmarks fail to capture real-world variability; reported performance is often overstated or misinterpreted.

Conclusion: Detectors remain useful only under certain conditions; their results should be interpreted as references rather than decisive evidence, encouraging more robust, nuanced benchmarks and definition frameworks.

Abstract: With the widespread use of large language models (LLMs), many researchers
have turned their attention to detecting text generated by them. However, there
is no consistent or precise definition of their target, namely "LLM-generated
text". Differences in usage scenarios and the diversity of LLMs further
increase the difficulty of detection. What is commonly regarded as the
detecting target usually represents only a subset of the text that LLMs can
potentially produce. Human edits to LLM outputs, together with the subtle
influences that LLMs exert on their users, are blurring the line between
LLM-generated and human-written text. Existing benchmarks and evaluation
approaches do not adequately address the various conditions in real-world
detector applications. Hence, the numerical results of detectors are often
misunderstood, and their significance is diminishing. Therefore, detectors
remain useful under specific conditions, but their results should be
interpreted only as references rather than decisive indicators.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873)
*Junfeng Gong,Zhiyi Wei,Junying Chen,Cheng Liu,Huawei Li*

Main category: cs.LG

TL;DR: 提出 ReGraphT，一种训练无成本的检索增强框架，通过把CUDA优化轨迹组织成推理图并使用蒙特卡洛图搜索，实现把LLM级推理转移到小型语言模型上以提升CUDA代码生成效率。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成优化CUDA代码方面有潜力，但云端API存在代码泄露风险，本地部署成本高且低效；SLMs更轻量和隐私友好，但在复杂CUDA生成上推理能力不足。需要一个无训练成本、隐私友好的方案来桥接两者。

Method: 建立 ReGraphT：将CUDA优化轨迹组织成结构化推理图，建模优化组合为状态转移；采用蒙特卡洛图搜索(MCGS)进行高效探索；并提出一个带难度等级的CUDA专用基准以全面评估模型。

Result: 实验表明 ReGraphT 在 CUDAEval 与 ParEval 上平均提升 2.33×，优于面向HPC的微调模型和其他检索增强方法；与 DeepSeek-Coder-V2-Lite-Instruct、Qwen2.5-Coder-7B-Instruct 搭配时，SLMs 的性能接近LLMs，且无需隐私风险和高计算开销。

Conclusion: ReGraphT 成功将LLM级推理能力转移到较小模型，提升了CUDA优化的可行性与隐私保护性，具有推广到其他领域的潜力。

Abstract: Despite significant evolution of CUDA programming and domain-specific
libraries, effectively utilizing GPUs with massively parallel engines remains
difficult. Large language models (LLMs) show strong potential in generating
optimized CUDA code from sequential code. However, using LLMs in practice faces
two major challenges: cloud-based APIs pose risks of code leakage, and local
deployment is often computationally expensive and inefficient. These drawbacks
have spurred interest in small language models (SLMs), which are more
lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs
can achieve performance comparable to LLMs on specific tasks. While SLMs can
match LLMs on domain-specific tasks, their limited reasoning abilities lead to
suboptimal performance in complex CUDA generation according to our experiments.
To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented
generation framework that transfers LLM-level reasoning to smaller models.
ReGraphT organizes CUDA optimization trajectories into a structured reasoning
graph, modeling the combined CUDA optimizations as state transitions, and
leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also
present a CUDA-specific benchmark with difficulty tiers defined by reasoning
complexity to evaluate models more comprehensively. Experiments show that
ReGraphT outperforms HPC-specific fine-tuned models and other
retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval
and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and
Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level
performance without the associated privacy risks or excessive computing
overhead.

</details>


### [59] [FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning](https://arxiv.org/abs/2510.19893)
*Shiqi Dai,Wei Dai,Jiaee Cheong,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出 FairGRPO，一种分层强化学习的公平学习方法，通过自适应权重和无标签群体发现，降低跨人口群体的诊断偏差并提升F1，同时发布公平感知的临床VLLM FairMedGemma-4B。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI在不同人口群体中的性能差异所带来的现实伤害，以及多模态基础模型在推理阶段产生的偏差。强化学习的偏见往往来自训练数据的群体不平衡，需开发能在异质临床人群中实现公平优化的学习框架。

Method: FairGRPO 使用基于表示、任务难度和数据源的自适应重要性加权来调整优势函数，并采用分层强化学习框架进行公平学习。为应对临床领域常见的缺失人口标签问题，提出无监督聚类以自动发现潜在的 demographic 群体。

Result: 在7个临床诊断数据集、覆盖X-ray、CT、 dermoscopy、mammography和ultrasound的5种模态中，FairGRPO 将预测平等性相对于所有 vanilla 和偏差缓解RL基线降低27.2%，同时F1分数提升12.49%。训练过程分析显示公平性随优化进行而持续改善，而基线RL方法在训练过程中公平性往往恶化。此外，基于 FairGRPO 的 FairMedGemma-4B 公布，声称在公平性与性能方面达到行业领先水平。

Conclusion: 以 FairGRPO 为核心的公平强化学习方案可在跨模态、多群体临床任务中实现公平性提升与诊断性能并举，且可扩展为公平感知的临床VLLM（FairMedGemma-4B），对减少群体间差异具有潜在影响。

Abstract: Medical artificial intelligence systems have achieved remarkable diagnostic
capabilities, yet they consistently exhibit performance disparities across
demographic groups, causing real-world harm to underrepresented populations.
While recent multimodal reasoning foundation models have advanced clinical
diagnosis through integrated analysis of diverse medical data, reasoning
trainings via reinforcement learning inherit and often amplify biases present
in training datasets dominated by majority populations. We introduce
Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical
reinforcement learning approach that promotes equitable learning across
heterogeneous clinical populations. FairGRPO employs adaptive importance
weighting of advantages based on representation, task difficulty, and data
source. To address the common issue of missing demographic labels in the
clinical domain, we further employ unsupervised clustering, which automatically
discovers latent demographic groups when labels are unavailable. Through
comprehensive experiments across 7 clinical diagnostic datasets spanning 5
clinical modalities across X-ray, CT scan, dermoscropy, mammography and
ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%
against all vanilla and bias mitigated RL baselines, while improving F1 score
by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO
progressively improves fairness throughout optimization, while baseline RL
methods exhibit deteriorating fairness as training progresses. Based on
FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that
achieves state-of-the-art performance while demonstrating significantly reduced
disparities across demographic groups.

</details>


### [60] [Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets](https://arxiv.org/abs/2510.20609)
*Timur Galimzyanov,Olga Kolomyttseva,Egor Bogomolov*

Main category: cs.LG

TL;DR: 对代码导向生成任务在现实计算预算下的检索设计进行系统评估，通过代码完成和缺陷定位两个任务，比较不同上下文、分块与相似性度量的检索配置，给出在不同预算和任务需求下的可操作性建议。


<details>
  <summary>Details</summary>
Motivation: 在现实的计算预算约束下，如何为代码导向的生成（Code-RAG）选择高效且质量优的检索配置，是提升检索增强生成系统实用性的关键难题。

Method: 基于 Long Code Arena 的两个任务（代码完成与缺陷定位），在不同上下文窗口大小下，系统比较三类维度的检索配置：分块策略、相似性评分、以及分割粒度。评估包括对比稀疏检索（如 BM25）与密集编码器（如 Voyagers 的密集向量），以及不同分块粒度（逐行、32-64 行、整文件等）对吞吐量与质量的影响。

Result: 核心发现包括：(1) PL-PL 场景中，使用 BM25 的分词级别分块是最有效且实际可行，明显优于密集检索且速度快一个数量级；(2) NL-PL 场景中，专有的密集编码器（Voyager-3 系列）在质量上优于稀疏检索，但延迟高约 100×；(3) 置换上下文大小时，32-64 行的块在预算较小时表现最佳，整文件在 16000 tokens 时具有竞争力；(4) 单纯按行分块在语法感知分割上同样有效；(5) 检索延迟在不同配置间差异可达 200×，且 BPE 分块过慢，BM25+词级分块在质量-延迟权衡上最优。结论为：应结合任务需求、模型约束和计算资源，给出基于证据的代码导向 RAG 系统实现建议。

Conclusion: 在不同任务和预算下，BM25+词级分块对 PL-PL 提供最佳性价比；NL-PL 则偏好密集向量检索但需接受较高延迟；分块粒度和上下文大小需随预算调整，线性分块与语法感知分割具有鲁棒性；总体可操作性结论是：针对具体任务定制检索配置，以实现更优的质量-延迟权衡。

Abstract: We study retrieval design for code-focused generation tasks under realistic
compute budgets. Using two complementary tasks from Long Code Arena -- code
completion and bug localization -- we systematically compare retrieval
configurations across various context window sizes along three axes: (i)
chunking strategy, (ii) similarity scoring, and (iii) splitting granularity.
(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and
practical, significantly outperforming dense alternatives while being an order
of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3
family) consistently beat sparse retrievers, however requiring 100x larger
latency. (3) Optimal chunk size scales with available context: 32-64 line
chunks work best at small budgets, and whole-file retrieval becomes competitive
at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting
across budgets. (5) Retrieval latency varies by up to 200x across
configurations; BPE-based splitting is needlessly slow, and BM25 + word
splitting offers the best quality-latency trade-off. Thus, we provide
evidence-based recommendations for implementing effective code-oriented RAG
systems based on task requirements, model constraints, and computational
efficiency.

</details>


### [61] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: FINDER is a stochastic-feature learning framework for noisy, data-deficient datasets that uses Kosambi-Karhunen-Loève expansion to decompose features and perform spectrum-based, eigen-decomposition classification; validated on Alzheimer's disease staging and remote sensing deforestation with discussion on performance, failure modes and limitations.


<details>
  <summary>Details</summary>
Motivation: Noisy regimes (low SNR, small samples, faulty data collection) challenge standard classifiers. A principled framework that accounts for randomness in empirical datasets can improve robustness and generalization.

Method: Model empirical data as realizations from an underlying random field and map to appropriate Hilbert spaces. Use the Kosambi-Karhunen-Loève expansion (KLE) to decompose stochastic features into computable irreducible components. Perform eigen-decomposition to locate class regions in the spectrum, enabling classification on noisy data. Develop tailored algorithms within this framework for data-deficient domains.

Result: Validated on challenging domains with state-of-the-art performance gains in Alzheimer's disease stage classification and remote-sensing-based deforestation detection; demonstrates robustness to noise and data deficiency.

Conclusion: FINDER provides a principled, general-purpose approach for noisy classification, clarifying when it is advantageous, identifying potential failure modes, and outlining limitations and future directions.

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [62] [Beyond the Ideal: Analyzing the Inexact Muon Update](https://arxiv.org/abs/2510.19933)
*Egor Shulgin,Sultan AlRashed,Francesco Orabona,Peter Richtárik*

Main category: cs.LG

TL;DR: 本工作在LMO框架下分析Muon在非精确正交化更新下的性能，给出误差对性能的明确界限，并揭示步长与动量之间的耦合关系；实验（NanoGPT）验证理论预测，适应性学习率随近似精度变化而改变。


<details>
  <summary>Details</summary>
Motivation: 弥合Muon优化器在理论分析（需要理想的SVD更新）与实际近似正交化实现之间的差距；建立一个可允许近似误差的理论框架以指导超参数调优。

Method: 在线性最小化Oracle (LMO) 框架中引入加性误差模型，分析在不完美正交化下的更新；推导性能下降的显式界限；揭示误差与最优步长和动量之间的耦合；将近似过程（如Newton-Schulz 步数）视为需要与学习计划共同调优的关键参数。

Result: 给出关于LMO不精确性的显式性能下降界限；理论揭示：更低的Oracle精度需要更小的步长但更大的动量；NanoGPT 实验验证该耦合并观察到随着近似精度变化，最优学习率发生显著偏移。

Conclusion: 近似正交化的精度成为关键超参数，应与学习率与动量共同调优；理论结果为实证中的观测提供解释，并为实际部署Muon优化器提供指导。

Abstract: The Muon optimizer has rapidly emerged as a powerful, geometry-aware
alternative to AdamW, demonstrating strong performance in large-scale training
of neural networks. However, a critical theory-practice disconnect exists:
Muon's efficiency relies on fast, approximate orthogonalization, yet all prior
theoretical work analyzes an idealized, computationally intractable version
assuming exact SVD-based updates. This work moves beyond the ideal by providing
the first analysis of the inexact orthogonalized update at Muon's core. We
develop our analysis within the general framework of Linear Minimization Oracle
(LMO)-based optimization, introducing a realistic additive error model to
capture the inexactness of practical approximation schemes. Our analysis yields
explicit bounds that quantify performance degradation as a function of the LMO
inexactness/error. We reveal a fundamental coupling between this inexactness
and the optimal step size and momentum: lower oracle precision requires a
smaller step size but larger momentum parameter. These findings elevate the
approximation procedure (e.g., the number of Newton-Schulz steps) from an
implementation detail to a critical parameter that must be co-tuned with the
learning schedule. NanoGPT experiments directly confirm the predicted coupling,
with optimal learning rates clearly shifting as approximation precision
changes.

</details>


### [63] [Are Greedy Task Orderings Better Than Random in Continual Linear Regression?](https://arxiv.org/abs/2510.19941)
*Matan Tsipory,Ran Levinstein,Itay Evron,Mark Kong,Deanna Needell,Daniel Soudry*

Main category: cs.LG

TL;DR: Greedy task-dissimilarity orderings in continual linear regression can accelerate convergence empirically and have high-rank loss bounds similar to random orders, but single-pass greedy can fail without repetition; with repetitions, greedy convergence slows to k^{-1/3}, revealing nuanced trade-offs vs. random orderings.


<details>
  <summary>Details</summary>
Motivation: Understanding how the order of tasks affects continual learning performance, by formalizing greedily dissimilar task sequences via the Kaczmarz method and contrasting with random orderings.

Method: Formalize greedy orderings using tools from the Kaczmarz method to build geometric/algebraic intuition; conduct empirical evaluations on linear regression with random data and CIFAR-100 linear probes; prove theoretical results: loss bound for greedy orderings in high-rank regression; in general rank, show repetition-dependent separation where single-pass greedy can fail while repeated-greedy converges at O(k^{-1/3}).

Result: Empirically, greedy orderings yield faster reduction of average loss across tasks than random orderings in both simple linear regression and CIFAR-100 probing. Analytically, in high-rank settings, greedy losses are bounded similarly to random ones; but under general rank, single-pass greedy may catastrophically fail; allowing repetition yields convergence at rate proportional to k^{-1/3}. These findings highlight nuanced differences between greedy and random orderings.

Conclusion: Greedy task-dissimilarity orderings offer practical acceleration in some regimes and have solid high-rank theory, but require repetition to avoid failure and to achieve stable convergence, underlining nuanced trade-offs with random orderings.

Abstract: We analyze task orderings in continual learning for linear regression,
assuming joint realizability of training data. We focus on orderings that
greedily maximize dissimilarity between consecutive tasks, a concept briefly
explored in prior work but still surrounded by open questions. Using tools from
the Kaczmarz method literature, we formalize such orderings and develop
geometric and algebraic intuitions around them. Empirically, we demonstrate
that greedy orderings converge faster than random ones in terms of the average
loss across tasks, both for linear regression with random data and for linear
probing on CIFAR-100 classification tasks. Analytically, in a high-rank
regression setting, we prove a loss bound for greedy orderings analogous to
that of random ones. However, under general rank, we establish a
repetition-dependent separation. Specifically, while prior work showed that for
random orderings, with or without replacement, the average loss after $k$
iterations is bounded by $\mathcal{O}(1/\sqrt{k})$, we prove that single-pass
greedy orderings may fail catastrophically, whereas those allowing repetition
converge at rate $\mathcal{O}(1/\sqrt[3]{k})$. Overall, we reveal nuances
within and between greedy and random orderings.

</details>


### [64] [On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization](https://arxiv.org/abs/2510.19953)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 通过基于函数评估的无偏梯度估计器，解决零阶优化中的偏差问题，并提升准确性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有的零阶优化（ZOO）方法在梯度估计中存在偏差，且当扰动步长不收敛时偏差难以消除。需要只依赖函数评估的无偏梯度估计器。

Method: 将方向导数重写为望远镜级数，并从精心设计的分布中采样以构造无偏估计器，兼顾方差控制；给出四种具体构造，推导最优的尺度分布和扰动步长。

Result: 使用所提估计器的随机梯度下降在光滑非凸目标下达到理论上的最优复杂度；在合成任务与语言模型微调的实验中，方法显示出比标准ZOO更高的准确性和更快的收敛。

Conclusion: 基于函数评估的无偏ZOO估计器在理论与实验层面均展现出优势，显著提升零阶优化的性能与理论保证。

Abstract: Zeroth-order optimization (ZOO) is an important framework for stochastic
optimization when gradients are unavailable or expensive to compute. A
potential limitation of existing ZOO methods is the bias inherent in most
gradient estimators unless the perturbation stepsize vanishes. In this paper,
we overcome this biasedness issue by proposing a novel family of unbiased
gradient estimators based solely on function evaluations. By reformulating
directional derivatives as a telescoping series and sampling from carefully
designed distributions, we construct estimators that eliminate bias while
maintaining favorable variance. We analyze their theoretical properties, derive
optimal scaling distributions and perturbation stepsizes of four specific
constructions, and prove that SGD using the proposed estimators achieves
optimal complexity for smooth non-convex objectives. Experiments on synthetic
tasks and language model fine-tuning confirm the superior accuracy and
convergence of our approach compared to standard methods.

</details>


### [65] [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://arxiv.org/abs/2510.19975)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出一种用于两点零阶梯度估计的最优扰动分布，通过对扰动分布的受约束泛函优化得到方向性对齐的扰动(DAP)，相较固定长度扰动在某些方向上提高估计精度并提升SGD收敛性能。


<details>
  <summary>Details</summary>
Motivation: 弥补现有方法仅使用固定长度扰动的不足，探索扰动方向与梯度方向对齐的潜在优势，以及将其应用于更广扰动集的复杂性分析。

Method: 将扰动分布的优化问题建模为受约束的泛函优化，推导出方向性对齐扰动的理论框架(DAP)，并给出δ-无偏扰动的SGD收敛性分析及扩展的复杂性界限，同时通过合成与实际任务的实验验证。

Result: 理论上证明了最优扰动在渐近阶段可与真实梯度方向对齐；提出DAP在关键方向上提供更高精度；给出对广泛扰动的SGD收敛性分析；实验证明DAP在某些条件下优于传统方法。

Conclusion: 方向性对齐的扰动为zeroth-order优化提供了有前景的替代方案，DAP在理论和实证层面均显示出潜在优势，未来工作可进一步扩展与应用。

Abstract: In this paper, we explore the two-point zeroth-order gradient estimator and
identify the distribution of random perturbations that minimizes the
estimator's asymptotic variance as the perturbation stepsize tends to zero. We
formulate it as a constrained functional optimization problem over the space of
perturbation distributions. Our findings reveal that such desired perturbations
can align directionally with the true gradient, instead of maintaining a fixed
length. While existing research has largely focused on fixed-length
perturbations, the potential advantages of directional alignment have been
overlooked. To address this gap, we delve into the theoretical and empirical
properties of the directionally aligned perturbation (DAP) scheme, which
adaptively offers higher accuracy along critical directions. Additionally, we
provide a convergence analysis for stochastic gradient descent using
$\delta$-unbiased random perturbations, extending existing complexity bounds to
a wider range of perturbations. Through empirical evaluations on both synthetic
problems and practical tasks, we demonstrate that DAPs outperform traditional
methods under specific conditions.

</details>


### [66] [Towards Strong Certified Defense with Universal Asymmetric Randomization](https://arxiv.org/abs/2510.19977)
*Hanbin Hong,Ashish Kundu,Ali Payani,Binghui Wang,Yuan Hong*

Main category: cs.LG

TL;DR: UCAN 提出通用的各向异性噪声随机平滑框架，显著提升大半径下的认证鲁棒性，并可应用于任意分类器与多种噪声分布。


<details>
  <summary>Details</summary>
Motivation: 现有随机平滑多采用各向同性噪声，忽略数据维度的异质性，导致鲁棒性认证受限。需要一种可对不同数据维度进行定制化噪声分布的框架，以提升鲁棒性证明的效果。

Method: 提出 UCAN，将任意现有的随机平滑方法转化为非对称的噪声分布；提供通用理论框架，支持多种噪声分布及不同的 L_p 范数，适用于任意分类器并给出可证明的鲁棒性界限。并构建一个新框架，包含三种噪声参数生成器（NPGs）以优化不同数据维度的噪声参数，从而实现灵活的鲁棒性提升。

Result: 实证结果显示在 MNIST、CIFAR-10、ImageNet 上，认证精度在大半径下提升可达约 182.6%；相较于现有方法，鲁棒性显著提升；代码公开。

Conclusion: UCAN 为随机平滑提供了一个通用且可扩展的各向异性噪声框架，显著提升大半径下的认证鲁棒性，适用于广泛的噪声分布与任意分类器。

Abstract: Randomized smoothing has become essential for achieving certified adversarial
robustness in machine learning models. However, current methods primarily use
isotropic noise distributions that are uniform across all data dimensions, such
as image pixels, limiting the effectiveness of robustness certification by
ignoring the heterogeneity of inputs and data dimensions. To address this
limitation, we propose UCAN: a novel technique that \underline{U}niversally
\underline{C}ertifies adversarial robustness with \underline{A}nisotropic
\underline{N}oise. UCAN is designed to enhance any existing randomized
smoothing method, transforming it from symmetric (isotropic) to asymmetric
(anisotropic) noise distributions, thereby offering a more tailored defense
against adversarial attacks. Our theoretical framework is versatile, supporting
a wide array of noise distributions for certified robustness in different
$\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the
classifier's prediction over perturbed inputs with provable robustness bounds
through tailored noise injection. Additionally, we develop a novel framework
equipped with three exemplary noise parameter generators (NPGs) to optimally
fine-tune the anisotropic noise parameters for different data dimensions,
allowing for pursuing different levels of robustness enhancements in
practice.Empirical evaluations underscore the significant leap in UCAN's
performance over existing state-of-the-art methods, demonstrating up to
$182.6\%$ improvement in certified accuracy at large certified radii on MNIST,
CIFAR10, and ImageNet datasets.\footnote{Code is anonymously available at
\href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}

</details>


### [67] [Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency](https://arxiv.org/abs/2510.19980)
*Renzhao Liang,Sizhe Xu,Chenggang Xie,Jingru Chen,Feiyang Ren,Shu Yang,Takahiro Yabe*

Main category: cs.LG

TL;DR: 在时间序列预测中，适当截断历史数据可提高预测准确性；提出 AMRC，通过自适应遮蔽损失和表示一致性来抑制冗余特征，提升模型性能，基于信息瓶颈理论。


<details>
  <summary>Details</summary>
Motivation: 长期序列信息增益假设对建模的冗余特征与噪声敏感，存在效率与鲁棒性瓶颈。本研究通过实验揭示截断历史信息的潜在益处，寻求更高效且稳健的时序建模方法。

Method: 提出 Adaptive Masking Loss with Representation Consistency (AMRC)；核心包括两部分：1) 动态遮蔽损失，动态识别对判别力高的时间片段并引导训练；2) 表示一致性约束，稳定输入-标签-预测之间的映射关系；理论基础来自信息瓶颈。

Result: 实验表明 AMRC 能有效抑制冗余特征学习，显著提升预测性能，展现出对传统时序建模假设的挑战性与改进潜力。

Conclusion: 该方法为高效、鲁棒的时序预测模型提供了新的理论视角与方法论，验证了在信息瓶颈框架下的自适应特征筛选与表示约束的有效性。

Abstract: Time series forecasting plays a pivotal role in critical domains such as
energy management and financial markets. Although deep learning-based
approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the
prevailing "long-sequence information gain hypothesis" exhibits inherent
limitations. Through systematic experimentation, this study reveals a
counterintuitive phenomenon: appropriately truncating historical data can
paradoxically enhance prediction accuracy, indicating that existing models
learn substantial redundant features (e.g., noise or irrelevant fluctuations)
during training, thereby compromising effective signal extraction. Building
upon information bottleneck theory, we propose an innovative solution termed
Adaptive Masking Loss with Representation Consistency (AMRC), which features
two core components: 1) Dynamic masking loss, which adaptively identified
highly discriminative temporal segments to guide gradient descent during model
training; 2) Representation consistency constraint, which stabilized the
mapping relationships among inputs, labels, and predictions. Experimental
results demonstrate that AMRC effectively suppresses redundant feature learning
while significantly improving model performance. This work not only challenges
conventional assumptions in temporal modeling but also provides novel
theoretical insights and methodological breakthroughs for developing efficient
and robust forecasting models.

</details>


### [68] [Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications](https://arxiv.org/abs/2510.20019)
*Curtis Lee Shull,Merrick Green*

Main category: cs.LG

TL;DR: RFID基于RSSI的区分辨识：在CAD布局的12个区域实现决策树分类，适用于防务物资的区域监测，但整体准确性较低（约34%），对少数类别如C区挑战较大；邻接关系分析有助解释。


<details>
  <summary>Details</summary>
Motivation: 解决RFID传感在防务存储场景的特定性不足、漏洞（远距检测、欺骗、伪造）带来的误检风险，提供区级定位与异常检测的可行性评估。

Method: 在CAD建模的楼层平面中对12个区域进行RSSI数据的有监督学习仿真，使用决策树分类器。数据集约980,000次读数，存在类别不平衡；采用类别权重并以分层抽样将训练集压缩到5,000个平衡样本。结果通过整体准确率、F1得分以及邻接感知的混淆矩阵进行评估。

Result: 总体准确率34.2%；若干区域（如F、G、H等）F1分数>0.40；罕见区域（尤以LabZoneC）易被错误分类，即使使用类别权重。引入邻接感知的混淆矩阵以便更好地解读相邻区域的关系。结论指向在防务供应物流的区域级异常检测或放置误差监测中的潜在应用，提升覆盖度和信号时可通过改进天线布局或增加传感器及传感融合来提升性能。

Conclusion: 在现实仿真中，基于RSSI的决策树对区域级监控具有应用前景，但受覆盖不足和类别不平衡的限制仍需改进。通过优化天线部署与引入额外传感器/多模态传感融合可提升性能。

Abstract: Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.

</details>


### [69] [SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph](https://arxiv.org/abs/2510.20022)
*Jiazheng Li,Yawei Wang,David Yan,Yijun Tian,Zhichao Xu,Huan Song,Panpan Xu,Lin Lee Cheong*

Main category: cs.LG

TL;DR: SALT通过基于同一提示的轨迹图，基于结果奖励为步骤分配更细粒度的优势，从而在面向群体的策略优化中提升多步任务的稳定性与性能，开箱即用且开销极小。


<details>
  <summary>Details</summary>
Motivation: 在多步、长时任务中，基于结果的稀疏奖励和缺乏评论家的群体式RL（如GRPO）导致信用分配困难、训练不稳定。需要一种对单步进行细粒度信用分配、且可与现有群体RL无缝集成的解决方案.

Method: 以同一提示的轨迹为单位构建图结构，通过分析轨迹中的每一步质量来量化并对该步分配优势，仅利用结果奖励来推断优势；SALT作为可插拔模块，不改变轨迹采样流程，几乎无额外计算开销，兼容现有群体RL算法。

Result: 在WebShop、ALFWorld、AppWorld等基准数据集和多种模型规模上，SALT始终带来性能提升；对设计选择进行深入分析，验证细颗粒度优势分配的有效性和可行性。

Conclusion: SALT为群体RL中的多步信用分配提供了一种高效、可插拔的解决方案，通过从结果奖励中推导逐步优势，显著提升性能且开销极低，具备良好普适性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
enabling language agents to excel at single-turn tasks. However, their
application to complex, multi-step, and long-horizon tasks remains challenging.
While reinforcement learning (RL) offers a promising avenue for addressing
these challenges, mainstream approaches typically rely solely on sparse,
outcome-based rewards, a limitation that becomes especially problematic for
group-based RL algorithms lacking critic models, such as Group Relative Policy
Optimization (GRPO). In such methods, uniformly rewarding or penalizing all
actions within a trajectory can lead to training instability and suboptimal
policies, because beneficial and detrimental actions are often entangled across
multi-step interactions. To address this challenge, we propose SALT, a novel
and lightweight framework that provides a finer-grained advantage assignment,
derived solely from outcome rewards. We achieve this by constructing a graph
from trajectories of the same prompt, which allows us to quantify the quality
of each step and assign advantages accordingly. Crucially, SALT is designed as
a plug-and-play module that seamlessly integrates with existing group-based RL
algorithms, requiring no modifications to the rollout procedure and introducing
negligible computational overhead. Extensive experiments on the WebShop,
ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that
SALT consistently improves performance. We also conduct a thorough analysis to
validate the design choices behind SALT and offer actionable insights.

</details>


### [70] [Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards](https://arxiv.org/abs/2510.20055)
*Yuwei Cheng,Zifeng Zhao,Haifeng Xu*

Main category: cs.LG

TL;DR: 本论文将在线广告出价建模为带时滞招Poisson奖励的上下文马尔可夫决策过程（CMDP），提出两阶段最大似然估计结合数据分割来控制估计误差，并基于此设计强化学习算法以得到高效的个性化出价策略，理论上实现近似最优遗憾界，且通过仿真实验验证。


<details>
  <summary>Details</summary>
Motivation: 广告投放中需要同时考虑：1) 延迟和长期效应，2) 累积性影响（强化或疲劳），3) 用户异质性。然而以往研究很少将这三类因素联合建模并用于策略优化。

Method: 将出价问题建模为带延迟Poisson奖励的上下文MDP；提出两阶段最大似然估计并结合数据分割策略，以确保第一阶段估计误差对后续估计的影响可控；在此基础上设计强化学习算法以获得高效的个性化出价策略，并给出理论分析。

Result: 理论上给出近似最优遗憾界为 ˜O(d H^2 √T)，其中 d 为上下文维度，H 为回合数，T 为客户数，并通过仿真实验验证了方法的有效性。

Conclusion: 该框架能够在考虑延迟、累积效应与用户异质性的复杂广告投放动态下，提供高效且个性化的出价策略并具备理论保证，具有实际应用潜力，同时为未来将此方法扩展到真实数据场景提供方向。

Abstract: Online advertising platforms use automated auctions to connect advertisers
with potential customers, requiring effective bidding strategies to maximize
profits. Accurate ad impact estimation requires considering three key factors:
delayed and long-term effects, cumulative ad impacts such as reinforcement or
fatigue, and customer heterogeneity. However, these effects are often not
jointly addressed in previous studies. To capture these factors, we model ad
bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson
rewards. For efficient estimation, we propose a two-stage maximum likelihood
estimator combined with data-splitting strategies, ensuring controlled
estimation error based on the first-stage estimator's (in)accuracy. Building on
this, we design a reinforcement learning algorithm to derive efficient
personalized bidding strategies. This approach achieves a near-optimal regret
bound of $\tilde{O}{(dH^2\sqrt{T})}$, where $d$ is the contextual dimension,
$H$ is the number of rounds, and $T$ is the number of customers. Our
theoretical findings are validated by simulation experiments.

</details>


### [71] [Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs](https://arxiv.org/abs/2510.20064)
*Hongyi Liu,Jiaji Huang,Zhen Jia,Youngsuk Park,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出一种在线 draft 模型选择的算法，在 online speculative decoding 场景下对每个查询与 hindsight 的最佳 draft 模型竞争，理论保证显著优于基于 Bandit 的方法，能够在不额外请求目标模型的情况下对所有草拟模型进行有效估值，且可泛化至单草拟、多草拟和草拟树，且实现上有系统友好优化以降低开销；实验在开放源代码的 LLM 与多数据集上显著优于 EAGLE3 与 BanditSpec，尤其在需要长推理链的场景。


<details>
  <summary>Details</summary>
Motivation: 在 speculative decoding 中，在线选择最优草拟模型是提升推理效率和准确性的关键环节。现有的 bandit 基线存在对草拟模型数量扩展的样本和查询瓶颈，同时对长推理链场景表现有限。需要一个能对所有草拟模型进行高效评估且具有理论保证的在线学习框架。

Method: 提出一个在线学习算法，在每个查询中与 hindsight 的最佳草拟模型竞争；能够在无需额外对目标模型发起请求的情况下准确评估所有草拟模型的表现，从而超越基于 bandit 的方法的样本效率。方法具备对单草拟、多草拟与草拟树的通用性，并设计系统层面的高效实现以降低计算和延迟开销。

Result: 理论上证明该算法在每个查询上与 hindsight 的最佳草拟模型竞争，具备低 regret/强界限。实验通过开源 LLM 与多样数据集，显著优于 EAGLE3 和 BanditSpec，特别是在需要领域专业草拟者、且需要较长推理链的场景中。

Conclusion: 所提出的在线学习框架具有普适性、可扩展性和实际应用价值，能有效提升 speculative decoding 的效率与性能，并为未来在更复杂草拟结构与领域场景中的应用提供强有力的基线。

Abstract: Speculative decoding is widely used in accelerating large language model
(LLM) inference. In this work, we focus on the online draft model selection
problem in speculative decoding. We design an algorithm that provably competes
with the best draft model in hindsight for each query in terms of either the
token acceptance probability or expected acceptance length. In particular, we
show that we can accurately evaluate all draft models, instead of only the
chosen model without incurring additional queries to the target model, which
allows us to improve exponentially over the existing bandit-based approach as
the number of draft models increases. Our approach is generically applicable
with any speculative decoding methods (single draft, multi-drafts and
draft-trees). Moreover, we design system-efficient versions of online learners
and demonstrate that the overhead in computation and latency can be
substantially reduced. We conduct extensive experiments on open-source LLMs and
diverse datasets, demonstrating that our methods substantially outperform the
state-of-the-art EAGLE3 and the BanditSpec baseline in a variety of domains
where specialized domain-expert drafters are available, especially when long
reasoning chains are required.

</details>


### [72] [Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics](https://arxiv.org/abs/2510.20068)
*Ram Dyuthi Sristi,Sowmya Manojna Narasimha,Jingya Huang,Alice Despatin,Simon Musall,Vikash Gilja,Gal Mishne*

Main category: cs.LG

TL;DR: 提出一种基于Transformer的自编码器CTAE，用于同时建模跨脑区的非线性非平稳动力学，并在同一框架内将共享信号与各区域特有信号分离；在两组高密度记录数据上取得比现有方法更好的行为解码效果。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐/多视图方法忽略时间结构；尽管动力学潜变量模型可捕捉时序，但通常限于单一脑区、或线性解读、或混淆共享与私有信号，缺乏跨区域、非线性、非平稳动态的统一建模。需要一个能同时处理多区域时序关系并区分共享与区域特异结构的框架。

Method: 采用Transformer编码器-解码器来捕捉长程神经动力学；将每个脑区潜在空间分成正交的共享子空间和私有子空间；训练中强调分离这两部分信息，处理非平稳非线性 dynamics；对多区域数据进行端到端学习。

Result: 在两组高密度电生理数据上展现：一组来自运动皮层区域，另一组来自感知区域；CTAE提取有意义的表征，解码行为变量的性能优于现有方法。

Conclusion: CTAE为跨区域神经动力学及共享/私有结构的统一建模提供了新的框架，能提升多区域数据的表征和行为解码，同时为理解大脑不同区域之间的协同与分工提供工具。

Abstract: Simultaneous recordings from thousands of neurons across multiple brain areas
reveal rich mixtures of activity that are shared between regions and dynamics
that are unique to each region. Existing alignment or multi-view methods
neglect temporal structure, whereas dynamical latent variable models capture
temporal dependencies but are usually restricted to a single area, assume
linear read-outs, or conflate shared and private signals. We introduce the
Coupled Transformer Autoencoder (CTAE) - a sequence model that addresses both
(i) non-stationary, non-linear dynamics and (ii) separation of shared versus
region-specific structure in a single framework. CTAE employs transformer
encoders and decoders to capture long-range neural dynamics and explicitly
partitions each region's latent space into orthogonal shared and private
subspaces. We demonstrate the effectiveness of CTAE on two high-density
electrophysiology datasets with simultaneous recordings from multiple regions,
one from motor cortical areas and the other from sensory areas. CTAE extracts
meaningful representations that better decode behavioral variables compared to
existing approaches.

</details>


### [73] [ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models](https://arxiv.org/abs/2510.20084)
*Bosong Huang,Ming Jin,Yuxuan Liang,Johan Barthelemy,Debo Cheng,Qingsong Wen,Chenghao Liu,Shirui Pan*

Main category: cs.LG

TL;DR: ShapeX提出以形状子序列为核心的时序解释框架，通过将时间序列分段为基于shapelet的片段，并使用Shapley值衡量其saliency，核心在Shapelet Describe-and-Detect (SDD)学习多样化shapelet集合。


<details>
  <summary>Details</summary>
Motivation: 当前的后验时序解释方法多聚焦于逐时间点的特征归因，忽视了分类结果往往由关键shapelet驱动的前提。通过将解释与shapelet的原子性联系起来，力求获得不仅仅是相关性、更具因果性的解释。

Method: ShapeX框架将时序分割为shapelet驱动的片段；使用Shapley值对各shapelet片段的显著性进行评估；Shapelet Describe-and-Detect (SDD)框架用于从数据中学习多样的、对分类关键的shapelets，以提高解释质量。

Result: 在合成与真实数据集上的实验表明，ShapeX在识别最相关子序列方面优于现有PHTSE方法，显著提升解释的精准性和因果保真度。

Conclusion: ShapeX通过将解释聚焦于原子性的shapelet并借助Shapley值量化，提供了对时序分类结果更具因果性的解释框架，提升了可解释性与可信度。

Abstract: Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.

</details>


### [74] [On pattern classification with weighted dimensions](https://arxiv.org/abs/2510.20107)
*Ayatullah Faruk Mollah*

Main category: cs.LG

TL;DR: 提出带权维度距离的KNN扩展，对每一维度进行权重化并整合进距离度量，在高维小样本数据中提升分类性能，尤其在基因表达数据上实现显著提升（约10%）。


<details>
  <summary>Details</summary>
Motivation: 在多维样本中，标准欧氏距离并不总能准确体现样本间相似性，尤其是高维、特征权重差异显著的情形。因此需要对各维度赋予不同权重并结合合适的距离范数，提升分类效果。

Method: 分析距离范数和维度权重的影响，提出逐维度权重方案，将该加权度量嵌入KNN分类器，并在合成与真实数据集上进行广泛实验对比。

Result: 相较于传统KNN，在多种数据集上表现更优；基因表达数据的交叉验证中，分类准确率约提升10%；通过权重化和范数调整，有效控制最近邻区域的形状和大小，提升高维小样本场景的有效性。

Conclusion: 将带权重的 Minkowski 距离作为KNN的一般化，可以显著提升高维数据的分类性能，特别是在样本数有限的基因表达等应用场景。

Abstract: Studies on various facets of pattern classification is often imperative while
working with multi-dimensional samples pertaining to diverse application
scenarios. In this notion, weighted dimension-based distance measure has been
one of the vital considerations in pattern analysis as it reflects the degree
of similarity between samples. Though it is often presumed to be settled with
the pervasive use of Euclidean distance, plethora of issues often surface. In
this paper, we present (a) a detail analysis on the impact of distance measure
norms and weights of dimensions along with visualization, (b) a novel weighting
scheme for each dimension, (c) incorporation of this dimensional weighting
schema into a KNN classifier, and (d) pattern classification on a variety of
synthetic as well as realistic datasets with the developed model. It has
performed well across diverse experiments in comparison to the traditional KNN
under the same experimental setups. Specifically, for gene expression datasets,
it yields significant and consistent gain in classification accuracy (around
10%) in all cross-validation experiments with different values of k. As such
datasets contain limited number of samples of high dimensions, meaningful
selection of nearest neighbours is desirable, and this requirement is
reasonably met by regulating the shape and size of the region enclosing the k
number of reference samples with the developed weighting schema and appropriate
norm. It, therefore, stands as an important generalization of KNN classifier
powered by weighted Minkowski distance with the present weighting schema.

</details>


### [75] [Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning](https://arxiv.org/abs/2510.20108)
*Gabriel Y. Arteaga,Marius Aasan,Rwiddhi Chakraborty,Martine Hjelkrem-Tan,Thalles Silva,Michael Kampffmeyer,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 提出一种完全解耦的自监督学习策略，通过将原型视为高斯混合并采用在线EM更新与编码器训练分离，解决原型崩溃问题，提升原型多样性和下游性能。


<details>
  <summary>Details</summary>
Motivation: 原型化自监督学习中存在部分原型崩溃，导致目标趋同、信息性下降；现有正则化与增大容量的方法仅缓解症状，未触及根本原因，需通过分离原型与编码器的优化来保持多样性。

Method: 将原型建模为高斯混合模型，使用在线EM式更新，与编码器的损失解耦；原型参数在独立的目标下更新，编码器按自身损失进行训练，从而避免共同优化导致的冗余表示。

Result: 能够避免原型崩溃，保持原型的多样性，并带来更稳健且更优的下游表示性能。

Conclusion: 解耦训练为解决原型崩溃提供了一种简单、有效的办法，强调从根本层面处理问题。未来可进一步验证在不同数据与任务上的泛化性，并探究对其它自监督框架的适用性。

Abstract: Prototypical self-supervised learning methods consistently suffer from
partial prototype collapse, where multiple prototypes converge to nearly
identical representations. This undermines their central purpose -- providing
diverse and informative targets to guide encoders toward rich representations
-- and has led practitioners to over-parameterize prototype sets or add ad-hoc
regularizers, which mitigate symptoms rather than address the root cause. We
empirically trace the collapse to the joint optimization of encoders and
prototypes, which encourages a type of shortcut learning: early in training
prototypes drift toward redundant representations that minimize loss without
necessarily enhancing representation diversity. To break the joint
optimization, we introduce a fully decoupled training strategy that learns
prototypes and encoders under separate objectives. Concretely, we model
prototypes as a Gaussian mixture updated with an online EM-style procedure,
independent of the encoder's loss. This simple yet principled decoupling
eliminates prototype collapse without explicit regularization and yields
consistently diverse prototypes and stronger downstream performance.

</details>


### [76] [There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance](https://arxiv.org/abs/2510.20119)
*Arian Prabowo,Flora D. Salim*

Main category: cs.LG

TL;DR: TSFMs underperform because of naive transplantation of NLP/CV pipelines; progress requires principled, invariance-focused dataset design that preserves temporal semantics for timeseries.


<details>
  <summary>Details</summary>
Motivation: Explain why lightweight baselines match TSFMs and how data design limits progress; argue for building an ontology of time-series invariances from first principles to enable generalization and emergent behavior.

Method: Propose shifting from opportunistic aggregation to principled dataset construction that systematically spans invariances preserving temporal semantics; build invariance ontology from first principles; aim for representational completeness.

Result: Framework and design principles for TSFMs; no empirical results reported; emphasis on invariance coverage as core: not yet empirical validation.

Conclusion: Achieving generalization, reasoning, and emergent behavior in TSFMs requires principled invariance coverage and representational completeness through carefully designed datasets.

Abstract: Timeseries foundation models (TSFMs) have multiplied, yet lightweight
supervised baselines and even classical models often match them. We argue this
gap stems from the naive importation of NLP or CV pipelines. In language and
vision, large web-scale corpora densely capture human concepts i.e. there are
countless images and text of apples. In contrast, timeseries data is built to
complement the image and text modalities. There are no timeseries dataset that
contains the concept apple. As a result, the scrape-everything-online paradigm
fails for TS. We posit that progress demands a shift from opportunistic
aggregation to principled design: constructing datasets that systematically
span the space of invariance that preserve temporal semantics. To this end, we
suggest that the ontology of timeseries invariances should be built based on
first principles. Only by ensuring representational completeness through
invariance coverage can TSFMs achieve the aligned structure necessary for
generalisation, reasoning, and truly emergent behaviour.

</details>


### [77] [ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push](https://arxiv.org/abs/2510.20157)
*Xiaoming Wu,Teng Liu,Xin Wang,Ming Yang,Jiguo Yu*

Main category: cs.LG

TL;DR: 提出一种自适应差分隐私的去中心化学习方法 ADP-VRSGP，通过对噪声方差与学习率的分步衰减、历史梯度的渐进融合、以及基于 push-sum 的去中心化聚合，提升在时变拓扑中的训练速度、稳定性与模型性能，同时给出节点级个性化隐私保护和理论收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 固定方差的差分隐私噪声在去中心化学习中会显著影响模型性能和训练效率；需要在时变通信拓扑下实现高效、稳健的隐私保护与个性化隐私的去中心化优化方案。

Method: 提出 ADP-VRSGP：1) 自适应调节噪声方差和学习率，采用分步衰减策略；2) 引入渐进梯度融合，利用历史梯度缓解初始阶段高方差带来的收敛放缓；3) 结合去中心化的 push-sum 与聚合机制，适用于时变拓扑；4) 进行理论分析证明在合适学习率下的鲁棒收敛性，并通过实验证明在多场景下优于基线。

Result: 给出理论收敛性保证，并在实验中验证相较于现有基线在多种场景下的性能提升，彰显了方法在隐私保护去中心化学习中的有效性与鲁棒性。

Conclusion: ADP-VRSGP 能在提供节点级个性化隐私的同时，显著提升训练速度与稳定性，且具备对时变通信拓扑的鲁棒性，适用于大规模、隐私敏感的分布式学习场景。

Abstract: Differential privacy is widely employed in decentralized learning to
safeguard sensitive data by introducing noise into model updates. However,
existing approaches that use fixed-variance noise often degrade model
performance and reduce training efficiency. To address these limitations, we
propose a novel approach called decentralized learning with adaptive
differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).
This method dynamically adjusts both the noise variance and the learning rate
using a stepwise-decaying schedule, which accelerates training and enhances
final model performance while providing node-level personalized privacy
guarantees. To counteract the slowed convergence caused by large-variance noise
in early iterations, we introduce a progressive gradient fusion strategy that
leverages historical gradients. Furthermore, ADP-VRSGP incorporates
decentralized push-sum and aggregation techniques, making it particularly
suitable for time-varying communication topologies. Through rigorous
theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence
with an appropriate learning rate, significantly improving training stability
and speed. Experimental results validate that our method outperforms existing
baselines across multiple scenarios, highlighting its efficacy in addressing
the challenges of privacy-preserving decentralized learning.

</details>


### [78] [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://arxiv.org/abs/2510.20187)
*Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: Proposes RLEV, a value-aligned RL framework for LLMs that integrates explicit human value signals into the reward, outperforming correctness-only RL on value-weighted tasks and enabling value-sensitive behavior.


<details>
  <summary>Details</summary>
Motivation: Binary correctness rewards ignore task importance; need to align LLM optimization with human priorities; use human-defined value signals to guide learning.

Method: Define Reinforcement Learning with Explicit Human Values (RLEV); extend RLVR by incorporating explicit ground-truth value labels into the reward; evaluate across exam-style data, multiple RL algorithms and model scales; analyze value-weighted gradients on end-of-sequence tokens; perform ablations; test robustness to noisy value signals.

Result: RLEV improves value-weighted accuracy over correctness-only baselines; learns a value-sensitive termination policy (concise for low-value prompts, thorough for high-value prompts); evidence from value-weighted gradient amplification on end-of-sequence tokens; ablations establish causal link; robust to noisy value signals.

Conclusion: Explicitly optimizing a human-defined utility function is a practical path to aligning LLMs with human priorities; RLEV demonstrates how value signals can shape model behavior and tolerate noisy supervision, with implications for safer and more effective LLM deployment.

Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method
that aligns Large Language Model (LLM) optimization directly with quantifiable
human value signals. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined value signals directly into the
reward function. Using exam-style data with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
value-weighted accuracy but also learn a value-sensitive termination policy:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted gradient amplification on
end-of-sequence tokens. Ablation studies confirm the gain is causally linked to
value alignment. RLEV remains robust under noisy value signals, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.

</details>


### [79] [Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents](https://arxiv.org/abs/2510.20199)
*Jane H. Lee,Baturay Saglam,Spyridon Pougkakiotis,Amin Karbasi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 提出了一种基于优化确定性等价物（OCEs）的风险感知约束强化学习框架，在保证原始约束问题等价的前提下，对奖励分布尾部风险进行鲁棒性处理，并给出可嵌入PPO等现有求解器的算法实现，且证明收敛性。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，单纯以期望奖励来表达目标忽略尾部风险，可能在高风险场景导致灾难性结果，因此需要同时考虑奖励分布的风险与时间属性的鲁棒性。

Method: 通过优化确定性等价物构造风险度量，建立强拉格朗日对偶框架，使问题在参数化约束下保持等价；提供一种可直接嵌入标准 RL 求解器（如 PPO）的实现套路，并给出收敛性分析。

Result: 给出收敛性证明；通过数值实验验证所提框架在风险感知方面的属性，以及与传统方法的对比效果。

Conclusion: 所提出的风险感知约束 RL 框架能在考虑尾部风险的同时保持对原约束的等价性，并具备与现有 RL 算法无缝结合的特性，适用于高风险应用场景。

Abstract: Constrained optimization provides a common framework for dealing with
conflicting objectives in reinforcement learning (RL). In most of these
settings, the objectives (and constraints) are expressed though the expected
accumulated reward. However, this formulation neglects risky or even possibly
catastrophic events at the tails of the reward distribution, and is often
insufficient for high-stakes applications in which the risk involved in
outliers is critical. In this work, we propose a framework for risk-aware
constrained RL, which exhibits per-stage robustness properties jointly in
reward values and time using optimized certainty equivalents (OCEs). Our
framework ensures an exact equivalent to the original constrained problem
within a parameterized strong Lagrangian duality framework under appropriate
constraint qualifications, and yields a simple algorithmic recipe which can be
wrapped around standard RL solvers, such as PPO. Lastly, we establish the
convergence of the proposed algorithm under common assumptions, and verify the
risk-aware properties of our approach through several numerical experiments.

</details>


### [80] [Approximate Replicability in Learning](https://arxiv.org/abs/2510.20200)
*Max Hopkins,Russell Impagliazzo,Christopher Ye*

Main category: cs.LG

TL;DR: 提出三种对复制性（replicability）的放宽定义，并给出在常数放宽参数下的最优 agnostic PAC 学习器及其样本复杂度：Pointwise 与 Approximate 使用 Θ(d/α^2) 样本，Semi 使用 Θ(d^2/α^2) 带标记样本。


<details>
  <summary>Details</summary>
Motivation: 复制性概念虽然重要，但在某些任务上实现困难，甚至不可行，因此需要在学习稳定性上引入可接受的近似放宽，以在 PAC 学习框架下实现可行的学习算法。

Method: 提出三种自然的对复制性的放宽定义：1) Pointwise：仅要求固定输入的一致性；2) Approximate：在大分布上大部分样本的分类一致即可；3) Semi：算法完全可复制，但可额外使用未标记的共享样本。对常数放宽参数，给出等价于最优的 agnostic PAC 学习器的构造与分析。

Result: 在三种放宽下均给出样本复杂度界：Pointwise 与 Approximate 仅需 Θ(d/α^2) 样本，Semi 需要 Θ(d^2/α^2) 标记样本，且在这三种情形下均实现样本最优的 agnostic PAC 学习器。

Conclusion: 这三种放宽的复制性定义使在可接受成本下实现学习成为可能，揭示了复制性与学习稳定性之间的折中，并提供了具体的样本复杂度界限。

Abstract: Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion
that algorithms should remain stable under a resampling of their inputs (given
access to shared randomness). While a strong and interesting notion of
stability, the cost of replicability can be prohibitive: there is no replicable
algorithm, for instance, for tasks as simple as threshold learning (Bun et al.
STOC '23). Given such strong impossibility results we ask: under what
approximate notions of replicability is learning possible?
  In this work, we propose three natural relaxations of replicability in the
context of PAC learning: (1) Pointwise: the learner must be consistent on any
fixed input, but not across all inputs simultaneously, (2) Approximate: the
learner must output hypotheses that classify most of the distribution
consistently, (3) Semi: the algorithm is fully replicable, but may additionally
use shared unlabeled samples. In all three cases, for constant replicability
parameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are
achievable for ``free" using $\Theta(d/\alpha^2)$ samples, while (3) requires
$\Theta(d^2/\alpha^2)$ labeled samples.

</details>


### [81] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: 对金毛犬长期研究队列的癌症风险分类进行126种分析管线的基准评估，发现统计学上存在信号，但在临床分类上表现不足以作为可靠的筛查工具，需整合多模态数据。


<details>
  <summary>Details</summary>
Motivation: 在可及性筛查工具需求与现有生物标志物非特异性、样本不平衡的现实中，评估在真实世界约束下通过常规实验室数据预测早期犬类癌症的潜力与局限性。

Method: 构建126条分析管线，组合不同机器学习模型、特征选择方法与数据平衡技术；数据按患者级分割以避免数据泄漏；最佳模型为带类别权重的逻辑回归并结合递归特征消除；采用SHAP进行可解释性分析。

Result: 最佳模型的AUROC为0.815（95% CI: 0.793–0.836），F1-score为0.25，阳性预测值（PPV）为0.15；阴性预测值（NPV）0.98，召回率0.79。尽管排名能力中等，但临床分类性能不足以作为筛选工具。SHAP分析显示预测受年龄、炎症和贫血等非特异性特征驱动。

Conclusion: 在常规实验室数据中确实存在癌症信号，但强度不足且易与正常衰老或炎症状态混淆，难以实现可靠的临床区分。该模态下仅能达到统计学意义的信号强度，提示需要整合多模态数据源以提升诊断性能并推动兽医肿瘤学的进展。

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [82] [CO-PFL: Contribution-Oriented Personalized Federated Learning for Heterogeneous Networks](https://arxiv.org/abs/2510.20219)
*Ke Xing,Yanjie Dong,Xiaoyi Fan,Runhao Zeng,Victor C. M. Leung,M. Jamal Deen,Xiping Hu*

Main category: cs.LG

TL;DR: 提出了一种面向贡献度的个性化联邦学习框架 CO-PFL，通过对每个客户端的贡献进行动态估算来进行全局聚合，结合梯度方向差异与预测偏差在梯度子空间和数据子空间的双重分析，给出更具区分性的聚合权重，提升个性化性能与收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 在数据存在高度异质性且本地数据稀缺的场景下，传统单一全局模型和按数据量或均等贡献度加权的聚合策略易导致聚合偏置与个性化不足，需要针对每个客户端的实际贡献与更新质量进行估计与利用。

Method: 提出 CO-PFL，通过分析梯度方向的差异和预测结果的偏差，结合来自梯度子空间与数据子空间的信息，动态为每个客户端分配聚合权重以提升高质量更新的影响。此外引入参数逐位的个性化机制和掩码感知动量优化，以增强个性化适应性和优化稳定性。

Result: 在四个基准数据集(CIFAR10, CIFAR10C, CINIC10, Mini-ImageNet)上，CO-PFL 在个性化精度、鲁棒性、可扩展性和收敛稳定性方面持续优于现有方法。

Conclusion: CO-PFL 有效缓解聚合偏置，强化全局协同，通过构建具有稳定更新的定制子模型来提升本地性能，显示出强健的个性化联邦学习潜力。

Abstract: Personalized federated learning (PFL) addresses a critical challenge of
collaboratively training customized models for clients with heterogeneous and
scarce local data. Conventional federated learning, which relies on a single
consensus model, proves inadequate under such data heterogeneity. Its standard
aggregation method of weighting client updates heuristically or by data volume,
operates under an equal-contribution assumption, failing to account for the
actual utility and reliability of each client's update. This often results in
suboptimal personalization and aggregation bias. To overcome these limitations,
we introduce Contribution-Oriented PFL (CO-PFL), a novel algorithm that
dynamically estimates each client's contribution for global aggregation. CO-PFL
performs a joint assessment by analyzing both gradient direction discrepancies
and prediction deviations, leveraging information from gradient and data
subspaces. This dual-subspace analysis provides a principled and discriminative
aggregation weight for each client, emphasizing high-quality updates.
Furthermore, to bolster personalization adaptability and optimization
stability, CO-PFL cohesively integrates a parameter-wise personalization
mechanism with mask-aware momentum optimization. Our approach effectively
mitigates aggregation bias, strengthens global coordination, and enhances local
performance by facilitating the construction of tailored submodels with stable
updates. Extensive experiments on four benchmark datasets (CIFAR10, CIFAR10C,
CINIC10, and Mini-ImageNet) confirm that CO-PFL consistently surpasses
state-of-the-art methods in in personalization accuracy, robustness,
scalability and convergence stability.

</details>


### [83] [QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models](https://arxiv.org/abs/2510.20222)
*Hao Wang,Baojun Ma*

Main category: cs.LG

TL;DR: 引入 QKCV 注意力，将静态类别嵌入 C 纳入 QKV 机制，作为插件提升注意力模型在实际时间序列预测中的分类信息利用与微调效率；通过仅更新 C 即可在单变量时间序列基金模上实现高效微调，提升 broad 数据集上的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列通常具有类别相关的模式，但传统的 QKV 注意力未显式建模类别信息。引入静态分类嵌入 C 可为不同类别提供专属信息，增强预测能力并提升对大规模或多域数据的泛化与微调效率。

Method: 提出 QKCV（Query-Key-Category-Value）注意力，将静态类别嵌入 C 注入 QKV 计算中，以类别为条件进行信息聚合，作为可插拔模块嵌入到现有的注意力模型（如 Vanilla Transformer、Informer、PatchTST、TFT）中。并展示在微调场景中仅更新 C 而保持预训练权重不变，从而降低计算开销并提高微调性能。

Result: 在多样化真实数据集上提升了基于注意力的时间序列预测模型的准确性；在单变量时间序列基金模的微调中，显著降低了训练成本并实现更优的微调效果。

Conclusion: 将静态类别嵌入整合到 QKV 注意力的 QKCV 变体是有效的策略，能更好地捕捉类别特定模式，提升准确性且支持高效微调，具有广泛的插件化潜力。

Abstract: In real-world time series forecasting tasks, category information plays a
pivotal role in capturing inherent data patterns. This paper introduces QKCV
(Query-Key-Category-Value) attention, an extension of the traditional QKV
framework that incorporates a static categorical embedding C to emphasize
category-specific information. As a versatile plug-in module, QKCV enhances the
forecasting accuracy of attention-based models (e.g., Vanilla Transformer,
Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV
demonstrates remarkable adaptability in fine-tuning univariate time series
foundation model by solely updating the static embedding C while preserving
pretrained weights, thereby reducing computational overhead and achieving
superior fine-tuning performance.

</details>


### [84] [Sparse Local Implicit Image Function for sub-km Weather Downscaling](https://arxiv.org/abs/2510.20228)
*Yago del Valle Inclan Redondo,Enrique Arriaga-Varela,Dmitry Lyamzin,Pablo Cervantes,Tiago Ramalho*

Main category: cs.LG

TL;DR: 提出 SpLIIF，通过隐式神经表示实现天气变量的任意下采样，在日本地区用稀疏气象站与地形数据训练，温度和风的下采样预测在分布内外均优于基线，温度提升最高约50%，风提升约10–20%。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏观测和区域地形导致的空间分辨率受限问题；需要对温度和风等变量在高分辨率尺度进行鲁棒下采样和外推。

Method: 使用 SpLIIF 构建隐式神经表示，结合稀疏气象站和地形信息，在日本区域进行训练；对温度和风进行下采样预测；与插值基线和 CorrDiff 进行对比，评估分布内外的准确性。

Result: 在温度下采样任务中，模型性能比 CorrDiff 和基线高出最多约 50%；在风场下采样任务中，提升约 10–20%。

Conclusion: 隐式表示方法可有效提高对气象变量的高分辨率下采样能力，且对分布外情形具有较好泛化性；可作为高分辨率气象数据生成的一种有效方式。

Abstract: We introduce SpLIIF to generate implicit neural representations and enable
arbitrary downscaling of weather variables. We train a model from sparse
weather stations and topography over Japan and evaluate in- and
out-of-distribution accuracy predicting temperature and wind, comparing it to
both an interpolation baseline and CorrDiff. We find the model to be up to 50%
better than both CorrDiff and the baseline at downscaling temperature, and
around 10-20% better for wind.

</details>


### [85] [Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach](https://arxiv.org/abs/2510.20235)
*Woohyeon Byeon,Giseung Park,Jongseong Chae,Amir Leshem,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出一种可证明收敛且实用的多目标强化学习框架，针对最大最小准则；将其建模为两人零和正则化的连续博弈，基于镜像下降的高效算法；实现全局末迭代收敛，给出在精确/近似策略评估下的迭代复杂度以及样本复杂度界，且通过自适应正则化改进性能；在表格设置下实验验证收敛，在深度RL中实现显著优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多目标强化学习中最大-最小准则的收敛性与高效性问题；通过博弈视角和镜像下降实现稳定更新，并提供理论和经验证据。

Method: 将 MORL 转化为两人零和正则化连续博弈，提出基于镜像下降的高效算法，简化策略更新，确保全局末迭代收敛；加入自适应正则化以提升性能。

Result: 给出对算法的理论分析：在精确与近似策略评估下的迭代复杂度，以及样本复杂度边界；在表格环境中证明收敛，在深度RL中实现显著优于多种基线的表现。

Conclusion: 提供一个可证明收敛且实用的 MORL 框架，若干改进（自适应正则化）可提升性能，实验结果支持理论与方法的有效性。

Abstract: In this paper, we propose a provably convergent and practical framework for
multi-objective reinforcement learning with max-min criterion. From a
game-theoretic perspective, we reformulate max-min multi-objective
reinforcement learning as a two-player zero-sum regularized continuous game and
introduce an efficient algorithm based on mirror descent. Our approach
simplifies the policy update while ensuring global last-iterate convergence. We
provide a comprehensive theoretical analysis on our algorithm, including
iteration complexity under both exact and approximate policy evaluations, as
well as sample complexity bounds. To further enhance performance, we modify the
proposed algorithm with adaptive regularization. Our experiments demonstrate
the convergence behavior of the proposed algorithm in tabular settings, and our
implementation for deep reinforcement learning significantly outperforms
previous baselines in many MORL environments.

</details>


### [86] [Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction](https://arxiv.org/abs/2510.20236)
*Teng Jiek See,Daokun Zhang,Mario Boley,David K. Chalmers*

Main category: cs.LG

TL;DR: 提出一种名为 LKM 的层间知识混合自蒸馏方法，通过最小化不同 GNN 层的隐藏嵌入之间的平均绝对误差，在不显著增加计算成本的前提下，显著提升多种 GNN 在分子属性预测任务上的准确性。


<details>
  <summary>Details</summary>
Motivation: GNN 在分子性质预测中效果好，但提高准确性往往需要更复杂模型，带来更高的计算和内存成本，需在成本可控的前提下提升性能。

Method: 提出 Layer-to-Layer Knowledge Mixing (LKM) 自蒸馏框架，通过对现有 GNN 层的隐藏嵌入进行跨层对齐，最小化层间隐藏表示的平均绝对距离，从而实现多跳与多尺度信息的高效融合；在 DimeNet++、MXMNet、PAMNet 三种 GNN 架构和 QM9、MD17、Chignolin 数据集上评估。

Result: 在 QM9、MD17 Energy、Chignolin 等数据集上，LKM 分别带来 MAE 的相对下降最高可达 9.8%、45.3%、22.9%，并且对训练与推理成本几乎无显著增加。

Conclusion: LKM 能在不显著增加计算成本的前提下，显著提升分子属性预测任务中 GNN 的准确性，具有实用潜力。

Abstract: Graph Neural Networks (GNNs) are the currently most effective methods for
predicting molecular properties but there remains a need for more accurate
models. GNN accuracy can be improved by increasing the model complexity but
this also increases the computational cost and memory requirement during
training and inference. In this study, we develop Layer-to-Layer Knowledge
Mixing (LKM), a novel self-knowledge distillation method that increases the
accuracy of state-of-the-art GNNs while adding negligible computational
complexity during training and inference. By minimizing the mean absolute
distance between pre-existing hidden embeddings of GNN layers, LKM efficiently
aggregates multi-hop and multi-scale information, enabling improved
representation of both local and global molecular features. We evaluated LKM
using three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) using
datasets of quantum chemical properties (QM9, MD17 and Chignolin). We found
that the LKM method effectively reduces the mean absolute error of quantum
chemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17
Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM to
significantly improve the accuracy of GNNs for chemical property prediction
without any substantial increase in training and inference cost.

</details>


### [87] [What Does It Take to Build a Performant Selective Classifier?](https://arxiv.org/abs/2510.20242)
*Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: 提出 Selective-gap 的五源分解；单调后校准对缩小 gap 作用有限，需采用能重新排序预测的评分机制；在合成与真实数据上验证，给出可操作设计指南与误差预算。


<details>
  <summary>Details</summary>
Motivation: 正式量化选择性分类与理想完美序的差距，分解造成松弛的五个来源，以指导实际系统设计。

Method: 给出有限样本下的五源分解：Bayes 噪声、近似误差、排序误差、统计噪声、实现或分布漂移引入的 slack；通过受控实验 isolating 各分量，在 synthetic 二月牙月（two-moons）数据及真实世界视觉与语言基准上验证，评估后校准对分数排序的影响。

Result: 发现 Bayes 噪声与模型容量限制可解释相当大的 gap；更丰富、具特征感知的校准能显著改善分数排序；数据漂移引入额外的 slack，需要分布鲁棒训练；给出定量的误差预算和可操作的设计准则。

Conclusion: 要更接近理想 oracle，需要能够有效重新排序预测的打分机制，并在训练中考虑分布扰动的鲁棒性；据此提出面向实践的设计建议。

Abstract: Selective classifiers improve model reliability by abstaining on inputs the
model deems uncertain. However, few practical approaches achieve the
gold-standard performance of a perfect-ordering oracle that accepts examples
exactly in order of correctness. Our work formalizes this shortfall as the
selective-classification gap and present the first finite-sample decomposition
of this gap to five distinct sources of looseness: Bayes noise, approximation
error, ranking error, statistical noise, and implementation- or shift-induced
slack. Crucially, our analysis reveals that monotone post-hoc calibration --
often believed to strengthen selective classifiers -- has limited impact on
closing this gap, since it rarely alters the model's underlying score ranking.
Bridging the gap therefore requires scoring mechanisms that can effectively
reorder predictions rather than merely rescale them. We validate our
decomposition on synthetic two-moons data and on real-world vision and language
benchmarks, isolating each error component through controlled experiments. Our
results confirm that (i) Bayes noise and limited model capacity can account for
substantial gaps, (ii) only richer, feature-aware calibrators meaningfully
improve score ordering, and (iii) data shift introduces a separate slack that
demands distributionally robust training. Together, our decomposition yields a
quantitative error budget as well as actionable design guidelines that
practitioners can use to build selective classifiers which approximate ideal
oracle behavior more closely.

</details>


### [88] [Optimistic Task Inference for Behavior Foundation Models](https://arxiv.org/abs/2510.20264)
*Thomas Rupf,Marco Bagatella,Marin Vlastelica,Andreas Krause*

Main category: cs.LG

TL;DR: 提出 OpTI-BFM，通过测试时与环境交互进行任务推断，使用乐观决策准则并将不确定性建模为对奖励函数的上置信界，结合线性带仼的理论，实现对未见奖励函数的高效识别与优化，数据/计算成本显著减少，代码公开。


<details>
  <summary>Details</summary>
Motivation: BFMs 在零-shot RL 中需要对奖励进行推断或大量标注数据，数据成本高。本文旨在通过测试时的环境交互减少对离线奖励数据的依赖，提高任务推断的样本效率。

Method: 提出 OpTI-BFM，基于乐观决策准则直接对奖励函数不确定性建模，引导 BFMs 进行数据采集以推断任务；给出与线性带仼上置信界（UCB）相关的后悔界；在理论和经验上将 BFMs 的表现联系到这一本质。

Result: 在既定的零-shot 基准上评估 OpTI-BFM，显示其能够在少量回合内帮助后继特征的 BFMs 识别并优化未见的奖励函数，且计算开销较低。

Conclusion: 该方法实现了在测试时对 BFMs 的样本高效的任务推断，降低数据标注和计算成本，且提供可复现的实现。

Abstract: Behavior Foundation Models (BFMs) are capable of retrieving high-performing
policy for any reward function specified directly at test-time, commonly
referred to as zero-shot reinforcement learning (RL). While this is a very
efficient process in terms of compute, it can be less so in terms of data: as a
standard assumption, BFMs require computing rewards over a non-negligible
inference dataset, assuming either access to a functional form of rewards, or
significant labeling efforts. To alleviate these limitations, we tackle the
problem of task inference purely through interaction with the environment at
test-time. We propose OpTI-BFM, an optimistic decision criterion that directly
models uncertainty over reward functions and guides BFMs in data collection for
task inference. Formally, we provide a regret bound for well-trained BFMs
through a direct connection to upper-confidence algorithms for linear bandits.
Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and
observe that it enables successor-features-based BFMs to identify and optimize
an unseen reward function in a handful of episodes with minimal compute
overhead. Code is available at https://github.com/ThomasRupf/opti-bfm.

</details>


### [89] [Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch](https://arxiv.org/abs/2510.20271)
*Udit Saxena*

Main category: cs.LG

TL;DR: 提出了高效可微的ECC（Euler Characteristic Curve）计算及端到端学习能力，包含面向Ampere架构的CUDA内核优化和一个可微的PyTorch层；并讨论批处理和多GPU扩展。


<details>
  <summary>Details</summary>
Motivation: 在深度学习中需要将拓扑全局结构（如ECC）融入模型以提升表达能力，但现有实现在速度和可微性上存在瓶颈，限制了实际应用与端到端训练。

Method: 开发了针对Ampere GPU的高效CUDA核，采用128字节对齐访问和分层共享内存累积；实现了一个可微的PyTorch层，通过对阈值在单方向上采用Differentiable Euler Characteristic Transform风格的sigmoid放松实现端到端可梯度学习；并讨论了批处理/多GPU扩展以提升可扩展性。

Result: 相比先前的GPU实现，在合成网格上获得显著加速（约16–2000×），并实现了端到端学习能力；文本还指出对ECC工作相关应用的潜在影响与未来的批处理及多GPU扩展路径。

Conclusion: 该工作为ECC在深度学习中的实际应用提供了高效、可微的实现方案，并为拓扑特征在大规模深度学习模型中的集成奠定基础，且具备向多GPU/大规模部署扩展的潜力。

Abstract: Topological features capture global geometric structure in imaging data, but
practical adoption in deep learning requires both computational efficiency and
differentiability. We present optimized GPU kernels for the Euler
Characteristic Curve (ECC) computation achieving 16-2000\"O speedups over prior
GPU implementations on synthetic grids, and introduce a differentiable PyTorch
layer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs
use 128B-coalesced access and hierarchical shared-memory accumulation. Our
PyTorch layer learns thresholds in a single direction via a Differentiable
Euler Characteristic Transform-style sigmoid relaxation. We discuss downstream
relevance, including applications highlighted by prior ECC work, and outline
batching/multi-GPU extensions to broaden adoption.

</details>


### [90] [Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs](https://arxiv.org/abs/2510.20272)
*Tristan Cinquin,Geoff Pleiss,Agustinus Kristiadi*

Main category: cs.LG

TL;DR: PRM-guided树搜索未显著优于BoN，实验显示PRM对状态值估计不可靠且泛化差，导致树搜索效益有限；需要新的奖励建模策略来提升数学推理。


<details>
  <summary>Details</summary>
Motivation: 解决线性BoN在复杂、分支性推理中的局限，探究通过过程奖励模型（PRM）引导的树搜索是否能同时探索多条解题路径并提升推理质量。

Method: 提出自适应算法以在不可枚举的动作空间中最大化PRM分数；以Qwen2.5-Math-7B-Instruct及其PRM为案例，在23道数学题上评估PRM-guided树搜索的效果，并与BoN、蒙特卡洛树搜索、束搜索等方法对比。

Result: 1) PRM-guided树搜索对BoN无统计显著提升，且成本更高；2) MCTS和束搜索优于其他PRM-guided树搜索方法；3) PRMs难以可靠近似状态值，且随推理深度下降；4) PRMs对分布外数据泛化差。

Conclusion: PRM在树搜索中的应用受限，原因在于对PRM分数的依赖过强且不可靠。需改进奖励建模，或发展更稳健的价值估计，方能让树搜索切实提升LLM的数学推理性能。

Abstract: While chain-of-thought prompting with Best-of-N (BoN) selection has become
popular for mathematical reasoning in large language models (LLMs), its linear
structure fails to capture the branching and exploratory nature of complex
problem-solving. In this work, we propose an adaptive algorithm to maximize
process reward model (PRM) scores over the intractable action space, and
investigate whether PRM-guided tree search can improve mathematical reasoning
by exploring multiple partial solution paths. Across $23$ diverse mathematical
problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case
study, we find that: (1) PRM-guided tree search shows no statistically
significant improvements over BoN despite higher costs, (2) Monte Carlo tree
search and beam search outperform other PRM-guided tree search methods, (3)
PRMs poorly approximate state values and their reliability degrades with
reasoning depth, and (4) PRMs generalize poorly out of distribution. This
underperformance stems from tree search's greater reliance on unreliable PRM
scores, suggesting different reward modeling is necessary before tree search
can effectively enhance mathematical reasoning in LLMs.

</details>


### [91] [SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series](https://arxiv.org/abs/2510.20273)
*Qitai Tan,Yiyun Chen,Mo Li,Ruiwen Gu,Yilin Su,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出 SynTSBench，一种基于合成数据的时序预测评估框架，通过可控的特征配置，系统性评估模型在特征类型、鲁棒性和理论最优基准方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习时序预测模型在真实场景的鲁棒性和可解释性存在不足，现有评测框架常无法揭示模型对不同模式的能力差异，导致模型选型困难。需要一个可控、可解释且可比较的评估体系。

Method: 通过可编程的合成数据驱动评估，SynTSBench 将评估聚焦在三大维度：1) 时间特征分解与能力映射，用以系统化评估模型对特定模式的学习能力；2) 数据异常与噪声鲁棒性分析，量化对噪声的耐受阈值与异常恢复能力；3) 理论最优基准，与每种模式的解析最优解进行对比，建立基线。

Result: 实验表明现有深度学习模型并未在所有时间序列特征类型上接近理论最优基线。

Conclusion: SynTSBench 提供一个可解释、可控且可比的评估框架，帮助理解不同模型在各类特征上的能力，促进模型选择与改进，同时公开代码。

Abstract: Recent advances in deep learning have driven rapid progress in time series
forecasting, yet many state-of-the-art models continue to struggle with robust
performance in real-world applications, even when they achieve strong results
on standard benchmark datasets. This persistent gap can be attributed to the
black-box nature of deep learning architectures and the inherent limitations of
current evaluation frameworks, which frequently lack the capacity to provide
clear, quantitative insights into the specific strengths and weaknesses of
different models, thereby complicating the selection of appropriate models for
particular forecasting scenarios. To address these issues, we propose a
synthetic data-driven evaluation paradigm, SynTSBench, that systematically
assesses fundamental modeling capabilities of time series forecasting models
through programmable feature configuration. Our framework isolates confounding
factors and establishes an interpretable evaluation system with three core
analytical dimensions: (1) temporal feature decomposition and capability
mapping, which enables systematic evaluation of model capacities to learn
specific pattern types; (2) robustness analysis under data irregularities,
which quantifies noise tolerance thresholds and anomaly recovery capabilities;
and (3) theoretical optimum benchmarking, which establishes performance
boundaries for each pattern type-enabling direct comparison between model
predictions and mathematical optima. Our experiments show that current deep
learning models do not universally approach optimal baselines across all types
of temporal features.The code is available at
https://github.com/TanQitai/SynTSBench

</details>


### [92] [ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows](https://arxiv.org/abs/2510.20279)
*Penghao Wang,Yuhao Zhou,Mengxuan Wu,Ziheng Qin,Bangyuan Zhu,Shengbin Huang,Xuanlei Zhao,Panpan Zhang,Xiaojiang Peng,Yuzhang Shang,Jianfei Yang,Zheng Zhu,Tianlong Chen,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: 提出 CS-54k 语料库，用于评估与训练 AI 在计算机科学研究中的端到端协作能力，并基于此构建 CS-4k 评测基准和 CS-50k 训练集；实验显示数据质量与领域对齐比单纯扩大模型规模更关键。


<details>
  <summary>Details</summary>
Motivation: 当前的科研工作需要端到端的 AI 协作者，而现有评测多聚焦子任务，难以衡量完整研究工作流的能力，因此需要高质量、可扩展的论文级数据集来评估和训练 AI 在科研中的协作能力。

Method: 构建一个基于论文的可扩展管线，结合检索增强生成（RAG）与多阶段质量控制，产出高质量的科学问答语料 CS-54k；从中衍生 CS-4k（用于评估 AI 协助科研能力的基准）和 CS-50k（大规模训练集）。进行了大量实验，比较不同模型在 CS-4k 的能力分层，以及用 CS-50k 进行有监督学习和强化学习后的提升。

Result: CS-4k 能将最先进的模型区分为不同能力层级；在 CS-50k 上进行有监督训练与强化学习后，开源小模型（如 7B 规模）在某些方面可超过部分大模型（如 GPT-4.1、GPT-4o、Gemini 2.5 Pro）；强调高质量、领域对齐的数据对提升科研助手能力的重要性超越单纯的模型规模。

Conclusion: 基于高质量、领域对齐的数据来训练和评测 AI 的科研辅助能力，比单纯追求大规模预训练更有效；并公开 CS-4k 与 CS-50k，促使 AI 系统成为计算机科学研究的可靠协作者。

Abstract: As large language models (LLMs) advance, the ultimate vision for their role
in science is emerging: we could build an AI collaborator to effectively assist
human beings throughout the entire scientific research process. We refer to
this envisioned system as ResearchGPT. Given that scientific research
progresses through multiple interdependent phases, achieving this vision
requires rigorous benchmarks that evaluate the end-to-end workflow rather than
isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of
scientific Q&A pairs in computer science, built from 14k CC-licensed papers. It
is constructed through a scalable, paper-grounded pipeline that combines
retrieval-augmented generation (RAG) with multi-stage quality control to ensure
factual grounding. From this unified corpus, we derive two complementary
subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to
assist scientific research, and CS-50k, a large-scale training dataset.
Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs
into distinct capability tiers. Open models trained on CS-50k with supervised
training and reinforcement learning demonstrate substantial improvements. Even
7B-scale models, when properly trained, outperform many larger proprietary
systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that
making AI models better research assistants relies more on domain-aligned
training with high-quality data than on pretraining scale or general benchmark
performance. We release CS-4k and CS-50k in the hope of fostering AI systems as
reliable collaborators in CS research.

</details>


### [93] [InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling](https://arxiv.org/abs/2510.20302)
*Yuhang Wang*

Main category: cs.LG

TL;DR: InvDec 提出一种混合架构，将时间编码与变量维解码解耦，结合补丁级时间编码、倒置解码的变量维自注意力、延迟变量嵌入与自适应残差融合，在高维多元时间序列预测上显著优于 PatchTST。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模跨变量相关性与保持时间特征之间存在权衡。Channel-independent 的 PatchTST 忽略变量相关性，纯 variate-attention 的方法如 iTransformer 牺牲时序编码。需要一个能同时保留时间信息并有效建模跨变量相关性的架构，且在高维数据上提升显著性。

Method: 提出 InvDec：使用补丁基础的时间编码器来提取时序特征，随后在变量维度上应用 inverted decoder 进行 variate-wise 自注意力；引入延迟 variate_embeddings，在时间编码后再丰富变量特征；引入自适应残差融合，在不同数据集的维度条件下动态平衡时间特征与变量信息；将 InvDec 与 PatchTST 结合，形成 InvDec-PatchTST。

Result: 在七个基准数据集上显示对 PatchTST 的显著改进：Electricity（含 321 个变量）MSE 降幅 20.9%，Weather 提升 4.3%，Traffic 提升 2.7%；在低维 ETT 数据集保持竞争力；消融研究验证各组件作用，分析表明随着维度增加，InvDec 的优势越明显。

Conclusion: 跨变量建模在变量数量增多时变得关键，InvDec 通过解耦时间编码与变量维解码，提供对高维多元时间序列预测的鲁棒提升。

Abstract: Multivariate time series forecasting requires simultaneously modeling
temporal patterns and cross-variate dependencies. Channel-independent methods
such as PatchTST excel at temporal modeling but ignore variable correlations,
while pure variate-attention approaches such as iTransformer sacrifice temporal
encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that
achieves principled separation between temporal encoding and variate-level
decoding. InvDec combines a patch-based temporal encoder with an inverted
decoder operating on the variate dimension through variate-wise self-attention.
We introduce delayed variate embeddings that enrich variable-specific
representations only after temporal encoding, preserving temporal feature
integrity. An adaptive residual fusion mechanism dynamically balances temporal
and variate information across datasets of varying dimensions. Instantiating
InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven
benchmarks demonstrate significant gains on high-dimensional datasets: 20.9%
MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and
2.7% gain on Traffic compared to PatchTST, while maintaining competitive
performance on low-dimensional ETT datasets. Ablation studies validate each
component, and analysis reveals that InvDec's advantage grows with dataset
dimensionality, confirming that cross-variate modeling becomes critical as the
number of variables increases.

</details>


### [94] [LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems](https://arxiv.org/abs/2510.20327)
*Fengyuan Yu,Yuyuan Li,Xiaohua Feng,Junjie Fang,Tao Wang,Chaochao Chen*

Main category: cs.LG

TL;DR: LEGO是一种轻量级的多属性取消学习框架，采用嵌入校准与灵活组合两步，将多属性信息从用户嵌入中逐步移除并整合为单一嵌入，通过互信息最小化实现对多属性的同时取消学习，具备并行校准与高效组合的理论与实践保障。


<details>
  <summary>Details</summary>
Motivation: 现实世界的隐私保护需求通常涉及多种敏感属性且具有动态性，单一属性的取消学习无法满足对多属性及快速适应变化的需求，因此需要一个能够对多属性进行同时、动态撤回影响的框架。

Method: 将多属性取消学习分为两步：1) Embedding Calibration：从用户嵌入中移除与特定属性相关的信息；2) Flexible Combination：将多个已校准的嵌入融合成一个单一嵌入，保护所有敏感属性。将取消学习框定为互信息最小化问题，理论上保证同时对多属性进行取消学习；Embedding Calibration可并行执行，Flexible Combination高效且灵活。

Result: 在三个真实数据集上对三个推荐模型进行广泛实验，结果表明LEGO在效果与效率上具有竞争力，且实现了对多属性的并行且动态可适应的取消学习。代码及附录公开可复现。

Conclusion: LEGO成功实现了多属性的同时取消学习，解决了真实场景中的并行性与动态适应性挑战，具有较强的实际可部署性与扩展性。

Abstract: With the growing demand for safeguarding sensitive user information in
recommender systems, recommendation attribute unlearning is receiving
increasing attention. Existing studies predominantly focus on single-attribute
unlearning. However, privacy protection requirements in the real world often
involve multiple sensitive attributes and are dynamic. Existing
single-attribute unlearning methods cannot meet these real-world requirements
due to i) CH1: the inability to handle multiple unlearning requests
simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic
unlearning needs. To address these challenges, we propose LEGO, a lightweight
and efficient multiple-attribute unlearning framework. Specifically, we divide
the multiple-attribute unlearning process into two steps: i) Embedding
Calibration removes information related to a specific attribute from user
embedding, and ii) Flexible Combination combines these embeddings into a single
embedding, protecting all sensitive attributes. We frame the unlearning process
as a mutual information minimization problem, providing LEGO a theoretical
guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step
framework, where Embedding Calibration can be performed in parallel and
Flexible Combination is flexible and efficient, we address CH2. Extensive
experiments on three real-world datasets across three representative
recommendation models demonstrate the effectiveness and efficiency of our
proposed framework. Our code and appendix are available at
https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.

</details>


### [95] [Synthetic Data for Robust Runway Detection](https://arxiv.org/abs/2510.20349)
*Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Fabrice Jimenez,Thomas Oberlin*

Main category: cs.LG

TL;DR: 用合成影像补充有限真实数据以提升跑道检测的鲁棒性，结合域自适应以缓解合成到真实的分布差异；在夜间等罕见场景上进行鲁棒性评估。


<details>
  <summary>Details</summary>
Motivation: 在关键应用中，获取且标注大量训练数据成本高且覆盖性不足。跑道检测是自动着陆的关键环节，需覆盖所有条件甚至罕见情形；合成数据为低成本、可控覆盖提供潜在解决方案。

Method: 基于商业飞行模拟器生成合成影像，配合少量标注的真实影像；通过可控的图像生成与真实/合成数据的混合来训练标准目标检测模型；并设计定制的域自适应策略来提升对夜间等未在真实数据中充分覆盖的条件的鲁棒性。

Result: 在融合真实与合成数据的训练下，标准目标检测模型可实现准确预测；并通过域自适应提升对夜间等不充分覆盖条件的鲁棒性。

Conclusion: 合成数据与现实数据的有策略结合、以及对分布差异的自适应处理，能够在关键应用（如跑道检测）中以较少的标注成本实现可靠的检测性能。

Abstract: Deep vision models are now mature enough to be integrated in industrial and
possibly critical applications such as autonomous navigation. Yet, data
collection and labeling to train such models requires too much efforts and
costs for a single company or product. This drawback is more significant in
critical applications, where training data must include all possible conditions
including rare scenarios. In this perspective, generating synthetic images is
an appealing solution, since it allows a cheap yet reliable covering of all the
conditions and environments, if the impact of the synthetic-to-real
distribution shift is mitigated. In this article, we consider the case of
runway detection that is a critical part in autonomous landing systems
developed by aircraft manufacturers. We propose an image generation approach
based on a commercial flight simulator that complements a few annotated real
images. By controlling the image generation and the integration of real and
synthetic data, we show that standard object detection models can achieve
accurate prediction. We also evaluate their robustness with respect to adverse
conditions, in our case nighttime images, that were not represented in the real
data, and show the interest of using a customized domain adaptation strategy.

</details>


### [96] [Ask a Strong LLM Judge when Your Reward Model is Uncertain](https://arxiv.org/abs/2510.20369)
*Zhenghao Xu,Qin Lu,Qingru Zhang,Liang Qiu,Ilgee Hong,Changlong Yu,Wenlin Yao,Yao Liu,Haoming Jiang,Lihong Li,Hyokun Yun,Tuo Zhao*

Main category: cs.LG

TL;DR: 提出一种基于不确定性路由的框架，将快速的奖励模型与成本更高的强大LLM评审器结合，通过将策略梯度中的优势估计形式化为成对偏好分类，并对不确定的样本路由给LLM评审，提高在线RLHF的效率与对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统奖励模型易被奖励劫持、对OOD泛化差，以及强大LLM评审器成本高的问题，寻求在效率和对齐之间的平衡。

Method: 将优势估计的成对偏好分类化；基于不确定性进行路由，给不确定的样本提交LLM评审，其它样本由快速RM评估；在RM基准上验证，比较同成本下的随机评审调用；评估在在线RLHF中的对齐效果。

Result: 在相同成本条件下，不确定性路由显著优于随机调用评审；下游对齐提升，证明该路由方法在在线RLHF中的有效性。

Conclusion: 通过将不确定性作为路由信号，有效组合快速RM与强大但成本高的LLM评审，实现更高效的在线RLHF对齐。

Abstract: Reward model (RM) plays a pivotal role in reinforcement learning with human
feedback (RLHF) for aligning large language models (LLMs). However, classical
RMs trained on human preferences are vulnerable to reward hacking and
generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM
judges equipped with reasoning capabilities demonstrate superior
generalization, even without additional training, but incur significantly
higher inference costs, limiting their applicability in online RLHF. In this
work, we propose an uncertainty-based routing framework that efficiently
complements a fast RM with a strong but costly LLM judge. Our approach
formulates advantage estimation in policy gradient (PG) methods as pairwise
preference classification, enabling principled uncertainty quantification to
guide routing. Uncertain pairs are forwarded to the LLM judge, while confident
ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our
uncertainty-based routing strategy significantly outperforms random judge
calling at the same cost, and downstream alignment results showcase its
effectiveness in improving online RLHF.

</details>


### [97] [Hierarchical Time Series Forecasting with Robust Reconciliation](https://arxiv.org/abs/2510.20383)
*Shuhei Aikawa,Aru Suzuki,Kei Yoshitake,Kanata Teshigawara,Akira Iwabuchi,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.LG

TL;DR: 在协方差估计不确定性下的分层对齐鲁棒优化，使用半正定规划实现，实验显示优于传统对齐方法。


<details>
  <summary>Details</summary>
Motivation: 分层时间序列中父系等于子序列之和，预测需一致；现有方法依赖估计的协方差矩阵进行再平衡，真实协方差未知且样本有限，导致性能下降；需要考虑协方差不确定性以提升预测。

Method: 提出一个协方差不确定性集合，优化目标为在该集合上最坏情形的期望平方误差最小化；将问题转化为半正定规划（SDP），得到鲁棒的分层对齐解。

Result: 数值实验表明，鲁棒再平衡方法在预测误差上优于现有分层预测方法，体现将不确定性融入再平衡的有效性。

Conclusion: 把协方差不确定性纳入分层对齐框架能提升鲁棒性和预测精度，适用于需要层级一致性的时序预测场景。

Abstract: This paper focuses on forecasting hierarchical time-series data, where each
higher-level observation equals the sum of its corresponding lower-level time
series. In such contexts, the forecast values should be coherent, meaning that
the forecast value of each parent series exactly matches the sum of the
forecast values of its child series. Existing hierarchical forecasting methods
typically generate base forecasts independently for each series and then apply
a reconciliation procedure to adjust them so that the resulting forecast values
are coherent across the hierarchy. These methods generally derive an optimal
reconciliation, using a covariance matrix of the forecast error. In practice,
however, the true covariance matrix is unknown and has to be estimated from
finite samples in advance. This gap between the true and estimated covariance
matrix may degrade forecast performance. To address this issue, we propose a
robust optimization framework for hierarchical reconciliation that accounts for
uncertainty in the estimated covariance matrix. We first introduce an
uncertainty set for the estimated covariance matrix and formulate a
reconciliation problem that minimizes the worst-case expected squared error
over this uncertainty set. We show that our problem can be cast as a
semidefinite optimization problem. Numerical experiments demonstrate that the
proposed robust reconciliation method achieved better forecast performance than
existing hierarchical forecasting methods, which indicates the effectiveness of
integrating uncertainty into the reconciliation process.

</details>


### [98] [Relative-Based Scaling Law for Neural Language Models](https://arxiv.org/abs/2510.20387)
*Baoqing Yue,Jinyuan Zhou,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出 Relative-Based Probability (RBP) 和 Relative-Based Scaling Law，通过正确标记在前序中的排名来衡量语言模型性能，作为跨熵的补充。在四个数据集、四个模型家族、跨五个数量级的实验中验证了该律的鲁棒性和准确性，并给出两种应用场景：对涌现现象的解释以及探索缩放规律的基础理论。


<details>
  <summary>Details</summary>
Motivation: 现有缩放律研究几乎依赖交叉熵作为评估指标，但交叉熵只衡量正确 token 的绝对概率，忽略正确与错误 token 的相对排序。这一相对排序在贪心采样等场景尤为重要，因此需要一种基于排序的度量及其缩放规律。

Method: 首先提出 Relative-Based Probability (RBP) 指标，量化正确 token 在候选序列中的排名靠前的概率。基于该度量，推导出 Relative-Based Scaling Law，刻画 RBP 随着模型规模的提升如何改善。随后在四个数据集、四个模型家族、覆盖五个数量级的规模范围内进行广泛实验，验证规律的鲁棒性与预测性，并通过两例应用示例展示其广泛应用价值。

Result: 实验表明，RBP 能够随着模型规模提升而显著改善， Relative-Based Scaling Law 对 RBP 的变化具有高预测性并且鲁棒性良好；该规律为解释涌现现象提供新视角，并有助于发现更深层次的缩放规律的基础理论；该方法与跨熵视角互补，拓宽了对大语言模型缩放的理解。

Conclusion: RBP 与其缩放规律为理解大语言模型的缩放提供了补充性的视角，具有实际应用价值与理论探索潜力，能够辅助模型开发与理论研究。

Abstract: Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.

</details>


### [99] [Why DPO is a Misspecified Estimator and How to Fix It](https://arxiv.org/abs/2510.20413)
*Aditya Gopalan,Sayak Ray Chowdhury,Debangshu Banerjee*

Main category: cs.LG

TL;DR: DPO 使用监督学习对偏好数据进行微调，本质上是对一组参数化策略下的奖励函数进行统计估计；若最真实的奖励无法被该策略类表示，将出现 misspecification、偏好顺序颠倒以及对数据分布的高敏感性。为此提出 AuxDPO，通过在 DPO 损失中引入辅助变量，能够更平滑地向 RLHF 解靠拢并缓解 misspecification；在 didactic-bandit 与大型语言模型对齐任务中实验显示优于原始 DPO。


<details>
  <summary>Details</summary>
Motivation: 揭示 Direct Preference Optimization 的统计估计本质，比较其与两阶段 RLHF 的关系，并解决直接对齐中的估计偏差和对偏好数据分布的敏感性问题。

Method: 将 DPO 视为对由参数化策略类产生的奖励函数的统计估计；研究两阶段 RLHF 在参数化类下的局部几何行为，将其与策略空间中的自然梯度联系起来；提出 AuxDPO，在 DPO 损失中引入额外的辅助变量，以帮助模型更接近 RLHF 的解并缓解 misspecification。

Result: 给出对 DPO misspecification 的细粒度几何分析；AuxDPO 有效缓解相关错配并提升鲁棒性；在 didactic bandit 场景和 LLM 对齐任务中，AuxDPO 显著优于标准 DPO。

Conclusion: AuxDPO 提供了一种在保持简单监督学习结构的同时， principled 地缓解 DPO 的模型错配的方法，并在实际对齐任务中实现更好的性能。

Abstract: Direct alignment algorithms such as Direct Preference Optimization (DPO)
fine-tune models based on preference data, using only supervised learning
instead of two-stage reinforcement learning with human feedback (RLHF). We show
that DPO encodes a statistical estimation problem over reward functions induced
by a parametric policy class. When the true reward function that generates
preferences cannot be realized via the policy class, DPO becomes misspecified,
resulting in failure modes such as preference order reversal, worsening of
policy reward, and high sensitivity to the input preference data distribution.
On the other hand, we study the local behavior of two-stage RLHF for a
parametric class and relate it to a natural gradient step in policy space. Our
fine-grained geometric characterization allows us to propose AuxDPO, which
introduces additional auxiliary variables in the DPO loss function to help move
towards the RLHF solution in a principled manner and mitigate the
misspecification in DPO. We empirically demonstrate the superior performance of
AuxDPO on didactic bandit settings as well as LLM alignment tasks.

</details>


### [100] [Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes](https://arxiv.org/abs/2510.20414)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yongli Ren,Yan Wang*

Main category: cs.LG

TL;DR: A thresholding-based neural MTPP addresses imbalanced mark distributions by calibrating mark probabilities with priors, predicting the mark before the time, and achieving superior next-event predictions.


<details>
  <summary>Details</summary>
Motivation: In real-world marked temporal point processes, the distribution of event marks is highly imbalanced, causing poor prediction for rare marks. A thresholding approach aims to reweight mark probabilities by their priors to improve robustness and accuracy, especially for rare marks.

Method: Introduce learnable thresholds to tune the normalized mark probability by the mark's prior probability. The model predicts the mark first and then the time. It also proposes a neural MTPP framework that supports efficient time sampling and avoids computationally expensive numerical improper integration for estimating mark probabilities.

Result: Empirical experiments on real-world datasets show superior performance of the proposed method over various baselines for both next-event mark and time prediction, demonstrating robustness to imbalance.

Conclusion: The thresholding-based, mark-first MTPP framework effectively mitigates mark-imbalance issues, improving next-event prediction without prohibitive computational costs; code is available at the provided repository.

Abstract: Marked Temporal Point Process (MTPP) has been well studied to model the event
distribution in marked event streams, which can be used to predict the mark and
arrival time of the next event. However, existing studies overlook that the
distribution of event marks is highly imbalanced in many real-world
applications, with some marks being frequent but others rare. The imbalance
poses a significant challenge to the performance of the next event prediction,
especially for events of rare marks. To address this issue, we propose a
thresholding method, which learns thresholds to tune the mark probability
normalized by the mark's prior probability to optimize mark prediction, rather
than predicting the mark directly based on the mark probability as in existing
studies. In conjunction with this method, we predict the mark first and then
the time. In particular, we develop a novel neural MTPP model to support
effective time sampling and estimation of mark probability without
computationally expensive numerical improper integration. Extensive experiments
on real-world datasets demonstrate the superior performance of our solution
against various baselines for the next event mark and time prediction. The code
is available at https://github.com/undes1red/IFNMTPP.

</details>


### [101] [An Empirical Study of Sample Selection Strategies for Large Language Model Repair](https://arxiv.org/abs/2510.20428)
*Xuran Li,Jingyi Wang*

Main category: cs.LG

TL;DR: SAPS在样本优先选取方面最有效，数据量最小化下实现对LLM行为的修复，平衡去毒、实用性与效率。随机抽样对大型/鲁棒模型仍有价值，高开销方法收益有限。数据占比受模型规模与修复方法影响，修复数据选择是可调组件。


<details>
  <summary>Details</summary>
Motivation: 在对LLM进行事后修复以降低有毒/偏见输出时，减少修复数据量与计算成本，同时寻找最有效的数据采样标准来提升修复效果。

Method: 系统评估五种采样策略：随机采样、K-Center、基于梯度范数的GraNd、分层覆盖CCS，以及提出的语义感知优先采样SAPS；通过毒性降低、WikiText-2/LAMBADA困惑度以及三个综合指标RPS、OPS、RES评估修复效果与效率。

Result: SAPS在去毒、保留实用性与效率之间实现最佳平衡，在数据量显著减少的情况下获得可比或更优的修复结果；对大型或鲁棒模型，随机采样也有效；CCR/Gram等高开销方法收益有限；最佳数据占比随模型规模与修复方法而变化。

Conclusion: 基于选择的数据采样的修复提供一种高效且可扩展的LLM可靠性维护范式，样本选择成为修复管线中的一个可调参数。

Abstract: Large language models (LLMs) are increasingly deployed in real-world systems,
yet they can produce toxic or biased outputs that undermine safety and trust.
Post-hoc model repair provides a practical remedy, but the high cost of
parameter updates motivates selective use of repair data. Despite extensive
prior work on data selection for model training, it remains unclear which
sampling criteria are most effective and efficient when applied specifically to
behavioral repair of large generative models. Our study presents a systematic
analysis of sample prioritization strategies for LLM repair. We evaluate five
representative selection methods, including random sampling, K-Center,
gradient-norm-based selection(GraNd), stratified coverage (CCS), and a
Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair
effectiveness and trade-offs are assessed through toxicity reduction,
perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair
Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair
Efficiency Score (RES). Experimental results show that SAPS achieves the best
balance between detoxification, utility preservation, and efficiency,
delivering comparable or superior repair outcomes with substantially less data.
Random sampling remains effective for large or robust models, while
high-overhead methods such as CCS and GraNd provide limited benefit. The
optimal data proportion depends on model scale and repair method, indicating
that sample selection should be regarded as a tunable component of repair
pipelines. Overall, these findings establish selection-based repair as an
efficient and scalable paradigm for maintaining LLM reliability.

</details>


### [102] [Explainable Benchmarking through the Lense of Concept Learning](https://arxiv.org/abs/2510.20439)
*Quannian Zhang,Michael Röder,Nikit Srivastava,N'Dah Jean Kouagou,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 提出可解释基准测试的新范式—Explainable Benchmarking，并以大知识图谱问答为例，通过 PruneCEL 自动生成性能解释，提升可解释性与预测性。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试通常用少量指标来总结系统性能，评估细节难以分析且分析过程繁琐且易受偏见影响，因此需要一种能够自动生成且可解释的基准解释的方法。

Method: 提出 explainable benchmarking 的第一种实现，针对知识图谱问答系统使用 PruneCEL 进行概念学习，生成对系统性能的解释。PruneCEL 是面向大知识图谱的新的概念学习方法。

Result: 在 explainable benchmarking 任务上，PruneCEL 相较于现有概念学习方法在 F1 上提升最多 0.55 点。一个以任务驱动的用户研究（41 名参与者）显示，在 80% 的情况下，参与者能够通过解释准确预测系统行为。代码和数据可在 GitHub 获取。

Conclusion: 可解释的基准测试对理解和改进知识图谱问答系统具有显著帮助，本文提供了一个首个实现，并提供开源资源供复现。

Abstract: Evaluating competing systems in a comparable way, i.e., benchmarking them, is
an undeniable pillar of the scientific method. However, system performance is
often summarized via a small number of metrics. The analysis of the evaluation
details and the derivation of insights for further development or use remains a
tedious manual task with often biased results. Thus, this paper argues for a
new type of benchmarking, which is dubbed explainable benchmarking. The aim of
explainable benchmarking approaches is to automatically generate explanations
for the performance of systems in a benchmark. We provide a first instantiation
of this paradigm for knowledge-graph-based question answering systems. We
compute explanations by using a novel concept learning approach developed for
large knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL
outperforms state-of-the-art concept learners on the task of explainable
benchmarking by up to 0.55 points F1 measure. A task-driven user study with 41
participants shows that in 80\% of the cases, the majority of participants can
accurately predict the behavior of a system based on our explanations. Our code
and data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025

</details>


### [103] [Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models](https://arxiv.org/abs/2510.20468)
*Tomáš Souček,Sylvestre-Alvise Rebuffi,Pierre Fernandez,Nikola Jovanović,Hady Elsahar,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko*

Main category: cs.LG

TL;DR: 提出水印伪造攻击：训练一个基于排序的偏好模型来判定图像是否被水印标记；利用单张水印图像通过反向传播优化输入实现水印的移除与伪造；不依赖水印模型内部信息，在多种后置水印模型上有效，质疑现有水印方法的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的兴起和法律压力，数字内容水印用于内容真实性与署名，但水印被盗用并伪造的场景尚未充分研究，存在显著的安全隐患。

Method: 1) 构建一个偏好模型，通过对纯程序生成的图像使用排序损失来判断图像是否被水印标记；2) 通过对输入图像进行反向传播优化来实现水印的移除与伪造；3) 攻击不需要知道水印模型的内部结构，仅需单张水印图像即可执行。

Result: 在多种后置水印模型上实验，攻击能够有效移除/伪造水印，质疑当前水印方案的安全性。

Conclusion: 水印系统须提升鲁棒性并发展对抗性检测策略，作者公开代码与资源以推动该领域的安全性研究。

Abstract: Recent years have seen a surge in interest in digital content watermarking
techniques, driven by the proliferation of generative models and increased
legal pressure. With an ever-growing percentage of AI-generated content
available online, watermarking plays an increasingly important role in ensuring
content authenticity and attribution at scale. There have been many works
assessing the robustness of watermarking to removal attacks, yet, watermark
forging, the scenario when a watermark is stolen from genuine content and
applied to malicious content, remains underexplored. In this work, we
investigate watermark forging in the context of widely used post-hoc image
watermarking. Our contributions are as follows. First, we introduce a
preference model to assess whether an image is watermarked. The model is
trained using a ranking loss on purely procedurally generated images without
any need for real watermarks. Second, we demonstrate the model's capability to
remove and forge watermarks by optimizing the input image through
backpropagation. This technique requires only a single watermarked image and
works without knowledge of the watermarking model, making our attack much
simpler and more practical than attacks introduced in related work. Third, we
evaluate our proposed method on a variety of post-hoc image watermarking
models, demonstrating that our approach can effectively forge watermarks,
questioning the security of current watermarking approaches. Our code and
further resources are publicly available.

</details>


### [104] [Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models](https://arxiv.org/abs/2510.20477)
*Rui Zhu,Song-Lin Lv,Zi-Kang Wang,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: Bi-CoG是一种无标签数据利用的自训练方法，结合跨模型和内模型一致性，并引入基于错误的动态伪标签分配，提升视觉语言模型的半监督微调表现。


<details>
  <summary>Details</summary>
Motivation: 解决基于预测一致性和固定置信阈值导致的模型偏差与超参数敏感性，尤其在预训练视觉-语言模型（VLM）的半监督微调场景中。

Method: 提出 Bi-CoG（双一致性引导自训练）：通过模型间的一致性与模型内的一致性相互作用来产生高质量伪标签，并采用错误感知的动态伪标签分配策略，使伪标签更具鲁棒性，且为 plug-and-play 方案，兼容现有 VLM 的微调流程。

Result: 理论分析与在14个数据集上的实验结果显示，Bi-CoG 能稳定且显著地提升现有半监督微调方法的性能，减少偏差与超参数敏感性。

Conclusion: Bi-CoG 为半监督微调提供了一种简单有效的伪标签生成与分配机制，提升预训练视觉-语言模型在下游任务的泛化能力与鲁棒性。

Abstract: Exploiting unlabeled data through semi-supervised learning (SSL) or
leveraging pre-trained models via fine-tuning are two prevailing paradigms for
addressing label-scarce scenarios. Recently, growing attention has been given
to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL,
forming the emerging paradigm of semi-supervised fine-tuning. However, existing
methods often suffer from model bias and hyperparameter sensitivity, due to
reliance on prediction consistency or pre-defined confidence thresholds. To
address these limitations, we propose a simple yet effective plug-and-play
methodology named
$\underline{\textbf{Bi-Co}}$nsistency-$\underline{\textbf{G}}$uided
Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels,
by simultaneously exploiting inter-model and intra-model consistency, along
with an error-aware dynamic pseudo-label assignment strategy. Both theoretical
analysis and extensive experiments over 14 datasets demonstrate the
effectiveness of Bi-CoG, which consistently and significantly improves the
performance of existing methods.

</details>


### [105] [Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval](https://arxiv.org/abs/2510.20486)
*Fangjian Zhang,Xiaoyong Zhuge,Wenlan Wang,Haixia Xiao,Yuying Zhu,Siyang Cheng*

Main category: cs.LG

TL;DR: 提出 Hurdle-Inversion Model Debiasing Learning (IMDL) 框架，使用分治策略将降水分布不平衡分解为零膨胀和长尾；零膨胀采用 hurdle 模型，长尾通过将学习目标转化为无偏理想逆模型来处理；在东部中国降雨场景下评估，显著优于传统、代价敏感、生成式和多任务方法，提升对极端降雨的检索并具一般化潜力。


<details>
  <summary>Details</summary>
Motivation: 遥感定量分析中的样本不均衡导致模型偏向常见样本，严重削弱对罕见但高影响事件（如大雨）的检索性能，需同时处理零膨胀和长尾分布。

Method: 分治策略：将不平衡分布分解为零膨胀与长尾；对零膨胀应用 hurdle 模型以区分非雨与雨况；对长尾通过将学习目标转化为无偏理想逆模型，使模型对罕见高雨等级进行更公平学习。

Result: 以统计指标和在东部中国的雨天案例研究为证，IMDL 优于常规、成本敏感、生成式和多任务方法，显著缓解系统性低估并提升重雨/极端雨的检索。

Conclusion: IMDL 提供一种可推广的框架，用于应对环境变量分布的不平衡，提升对罕见但高影响事件的检索能力，具有广泛适用性。

Abstract: Artificial intelligence has advanced quantitative remote sensing, yet its
effectiveness is constrained by imbalanced label distribution. This imbalance
leads conventionally trained models to favor common samples, which in turn
degrades retrieval performance for rare ones. Rainfall retrieval exemplifies
this issue, with performance particularly compromised for heavy rain. This
study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.
Following a divide-and-conquer strategy, imbalance in the rain distribution is
decomposed into two components: zero inflation, defined by the predominance of
non-rain samples; and long tail, defined by the disproportionate abundance of
light-rain samples relative to heavy-rain samples. A hurdle model is adopted to
handle the zero inflation, while IMDL is proposed to address the long tail by
transforming the learning object into an unbiased ideal inverse model.
Comprehensive evaluation via statistical metrics and case studies investigating
rainy weather in eastern China confirms Hurdle-IMDL's superiority over
conventional, cost-sensitive, generative, and multi-task learning methods. Its
key advancements include effective mitigation of systematic underestimation and
a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a
generalizable approach for addressing imbalance in distributions of
environmental variables, enabling enhanced retrieval of rare yet high-impact
events.

</details>


### [106] [SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment](https://arxiv.org/abs/2510.20540)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出了 SheafAlign，一种基于层结构（sheaf）的去中心化多模态对齐框架，将单一对齐空间替换为多对比空间。通过对模态对关系建模并采用去中心化对比学习来训练，能够在不要求所有模态互冗余的前提下，保留共享和独有信息，在多模态感知数据集上实现更强的零-shot泛化、跨模态对齐及对缺失模态的鲁棒性，同时通信成本比最先进 baselines 下降约 50%。


<details>
  <summary>Details</summary>
Motivation: 现实分布式场景中，各模态之间往往不具备互冗余，强依赖单一全局对齐空间会丢失模态特有信息且通信成本高。需要一种能同时保持共享信息与模态特有信息、且在分布式条件下有效协作的对齐方案。

Method: 提出 SheafAlign：用层结构（sheaf）来建模两两模态关系，形成多重比较空间；在每个局部空间上进行对比学习，并通过去中心化的学习目标进行训练，从而实现跨模态对齐。该框架不要求所有模态同时冗余存在，进而保留更多信息并降低通信需求。

Result: 在多模态感知数据集上，展现出更强的零-shot泛化、跨模态对齐能力，以及对缺失模态的鲁棒性；同时实现约 50% 的通信成本下降，相比现有最优基线具有显著优势。

Conclusion: 通过引入 sheaf 理论形成的多对比空间，SheafAlign 成功避免了对所有模态必须互冗余的限制，保留共享与独有信息，提升泛化和鲁棒性并显著降低通信开销，适合分布式、多模态感知场景。

Abstract: Conventional multimodal alignment methods assume mutual redundancy across all
modalities, an assumption that fails in real-world distributed scenarios. We
propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal
alignment that replaces single-space alignment with multiple comparison spaces.
This approach models pairwise modality relations through sheaf structures and
leverages decentralized contrastive learning-based objectives for training.
SheafAlign overcomes the limitations of prior methods by not requiring mutual
redundancy among all modalities, preserving both shared and unique information.
Experiments on multimodal sensing datasets show superior zero-shot
generalization, cross-modal alignment, and robustness to missing modalities,
with 50\% lower communication cost than state-of-the-art baselines.

</details>


### [107] [A Unified Framework for Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.20542)
*Jacopo Di Ventura,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 首次提出零-shot强化学习的统一框架，将方法分为直接表示与组合表示两类，并对继承特征方法（如后继特征）给出扩展界限，便于横向比较和理论分析。


<details>
  <summary>Details</summary>
Motivation: 零-shot RL 缺乏统一分析视角，现有方法各自为政，难以比较与总结，其目标是为普适化智能体的发展提供公理化框架。

Method: 提出一致的记法和分类法，建立两大族群：直接表示（端到端从奖励映射到策略）与组合表示（利用值函数的子结构进行分解），并推导出对后继特征方法的扩展界限。

Result: 给出一个统一的分析框架，整理现有方法，便于直接对比，揭示共性与差异，提供对零-shot RL 绩效的新视角；对后继特征方法提供扩展界限以评估在零-shot 场景的表现。

Conclusion: 该框架为未来零-shot RL 的研究提供原则性基础，指明进一步发展为更通用智能体的研究路径。

Abstract: Zero-shot reinforcement learning (RL) has emerged as a setting for developing
general agents in an unsupervised manner, capable of solving downstream tasks
without additional training or planning at test-time. Unlike conventional RL,
which optimizes policies for a fixed reward, zero-shot RL requires agents to
encode representations rich enough to support immediate adaptation to any
objective, drawing parallels to vision and language foundation models. Despite
growing interest, the field lacks a common analytical lens.
  We present the first unified framework for zero-shot RL. Our formulation
introduces a consistent notation and taxonomy that organizes existing
approaches and allows direct comparison between them. Central to our framework
is the classification of algorithms into two families: direct representations,
which learn end-to-end mappings from rewards to policies, and compositional
representations, which decompose the representation leveraging the substructure
of the value function. Within this framework, we highlight shared principles
and key differences across methods, and we derive an extended bound for
successor-feature methods, offering a new perspective on their performance in
the zero-shot regime. By consolidating existing work under a common lens, our
framework provides a principled foundation for future research in zero-shot RL
and outlines a clear path toward developing more general agents.

</details>


### [108] [Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics](https://arxiv.org/abs/2510.20556)
*Alexandre Benoit,Catherine Aitken,Yu He*

Main category: cs.LG

TL;DR: 该工作系统性分析图重连对图结构的影响以及对下游任务的关系，发现局部结构保持比全局连通性更加关键，重连策略应在局部保留与全局灵活性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 解决过度挤压（over-squashing）问题的图重连方法在改变拓扑的同时可能削弱结构性信号，需要明确哪些结构属性应被保留以兼顾性能提升与拓扑忠实性。

Method: 比较七种不同的重连策略，系统量化局部与全局图结构指标的变化，并将这些变化与节点分类准确率关联起来。

Result: 发现一个一致趋势：成功的重连方法倾向于维持局部结构的稳定性，同时对全局连通性给出更大的灵活性，从而兼顾性能提升与结构保真。

Conclusion: 为设计更有效的重连策略提供指南，强调在局部结构保持与全局连通性灵活性之间实现平衡，并促进图论理论与GNN优化之间的对话。

Abstract: Graph rewiring has emerged as a key technique to alleviate over-squashing in
Graph Neural Networks (GNNs) and Graph Transformers by modifying the graph
topology to improve information flow. While effective, rewiring inherently
alters the graph's structure, raising the risk of distorting important
topology-dependent signals. Yet, despite the growing use of rewiring, little is
known about which structural properties must be preserved to ensure both
performance gains and structural fidelity. In this work, we provide the first
systematic analysis of how rewiring affects a range of graph structural
metrics, and how these changes relate to downstream task performance. We study
seven diverse rewiring strategies and correlate changes in local and global
graph properties with node classification accuracy. Our results reveal a
consistent pattern: successful rewiring methods tend to preserve local
structure while allowing for flexibility in global connectivity. These findings
offer new insights into the design of effective rewiring strategies, bridging
the gap between graph theory and practical GNN optimization.

</details>


### [109] [BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](https://arxiv.org/abs/2510.20792)
*Liang Ye,Shengqin Chen,Jiazhu Dai*

Main category: cs.LG

TL;DR: BadGraph是一种针对文本引导的潜在扩散模型的后门攻击方法。通过对训练数据注入文本触发器，能够在推理阶段触发攻击者指定的子图，同时在干净输入下保持良好性能，投毒率低就能获得高攻击成功率，且后门在VAE与扩散训练阶段植入，存在显著的安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着图生成和药物发现等领域对文本引导的图生成需求增加，现有研究对后门攻击的关注多集中于图像领域或无条件生成，针对文本引导的图生成（尤其是基于潜在扩散模型）尚未充分研究，存在严重的安全隐患。

Method: 提出BadGraph，通过对训练数据植入文本触发器进行投毒，悄无声息地在模型中植入后门；在推理阶段当触发文本出现时，模型将生成 attacker 指定的子图；投毒点主要位于 VAE 和扩散模型的训练阶段，而非预训练阶段；在 PubChem、ChEBI-20、PCDes、MoMu 四个基准数据集上进行大量实验，验证后门的隐蔽性与有效性。

Result: 实验表明：投毒率小于10%即可实现约50%的攻击成功率，投毒率为24%即可达到80%以上的成功率，同时对良性输入几乎无性能下降；消融研究指向后门是在VAE和扩散训练阶段植入的。

Conclusion: 揭示了文本引导图生成的潜在扩散模型存在的安全漏洞，强调在药物发现等实际应用中的风险，强化对这类模型的后门防御和鲁棒性研究的必要性。

Abstract: The rapid progress of graph generation has raised new security concerns,
particularly regarding backdoor vulnerabilities. While prior work has explored
backdoor attacks in image diffusion and unconditional graph generation,
conditional, especially text-guided graph generation remains largely
unexamined. This paper proposes BadGraph, a backdoor attack method targeting
latent diffusion models for text-guided graph generation. BadGraph leverages
textual triggers to poison training data, covertly implanting backdoors that
induce attacker-specified subgraphs during inference when triggers appear,
while preserving normal performance on clean inputs. Extensive experiments on
four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the
effectiveness and stealth of the attack: less than 10% poisoning rate can
achieves 50% attack success rate, while 24% suffices for over 80% success rate,
with negligible performance degradation on benign samples. Ablation studies
further reveal that the backdoor is implanted during VAE and diffusion training
rather than pretraining. These findings reveal the security vulnerabilities in
latent diffusion models of text-guided graph generation, highlight the serious
risks in models' applications such as drug discovery and underscore the need
for robust defenses against the backdoor attack in such diffusion models.

</details>


### [110] [Embedding the MLOps Lifecycle into OT Reference Models](https://arxiv.org/abs/2510.20590)
*Simon Schindler,Christoph Binder,Lukas Lürzer,Stefan Huber*

Main category: cs.LG

TL;DR: OT environments hinder direct transplantation of standard MLOps; the paper proposes a structured adaptation of MLOps into OT using RAMI 4.0 and ISA-95, with a real-world mapping to RAMI 4.0 showing feasibility.


<details>
  <summary>Details</summary>
Motivation: Industrial OT systems pose barriers to MLOps practices. There is a need for a systematic method to embed MLOps into OT reference models and standards.

Method: Analyze fundamental obstacles, evaluate RAMI 4.0 and ISA-95 suitability for MLOps integration, and map MLOps lifecycle components to RAMI 4.0 using a real-world use case.

Result: Standard MLOps practices cannot be directly transplanted into OT. A structured adaptation approach using existing reference models can enable integration; RAMI 4.0 mapping is demonstrated through a real use case.

Conclusion: Integration of MLOps into OT is feasible through systematic adaptation guided by RAMI 4.0 and ISA-95; direct transplantation is inappropriate without this adaptation.

Abstract: Machine Learning Operations (MLOps) practices are increas- ingly adopted in
industrial settings, yet their integration with Opera- tional Technology (OT)
systems presents significant challenges. This pa- per analyzes the fundamental
obstacles in combining MLOps with OT en- vironments and proposes a systematic
approach to embed MLOps prac- tices into established OT reference models. We
evaluate the suitability of the Reference Architectural Model for Industry 4.0
(RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for
MLOps integration and present a detailed mapping of MLOps lifecycle compo-
nents to RAMI 4.0 exemplified by a real-world use case. Our findings
demonstrate that while standard MLOps practices cannot be directly transplanted
to OT environments, structured adaptation using existing reference models can
provide a pathway for successful integration.

</details>


### [111] [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800)
*Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus*

Main category: cs.LG

TL;DR: 提出一种快速的 LASER 剪枝方法，通过仅扫描少量矩阵、利用矩阵奇异值的梯度指示对象、在多子空间聚类后对每簇分解、以及仅用 100 条样本进行梯度计算与评估，在无需微调的情况下实现对下游任务的快速、鲁棒适配，性能提升最高可达 24.6 个百分点。


<details>
  <summary>Details</summary>
Motivation: 解决原 LASER 在每个矩阵上进行穷举式搜索、需要完整数据集前向传播的高开销问题，从而使得高效、无微调的下游任务适配成为可能。

Method: 核心方法包括：1) 仅需检查少量矩阵，取消逐层的全量扫描；2) 通过计算每个矩阵奇异值的梯度来指示需要进行降维/分解的候选矩阵；3) 将矩阵行聚集到多个子空间并对每个簇分别进行因式分解，以扩展搜索空间并降低对原始训练数据的过拟合；4) 使用仅 100 个样本来计算梯度和评估最终精度以降低搜索时间；5) 将上述策略结合，形成一个快速且鲁棒的下游任务自适应算法。

Result: 在无需微调的前提下，通过单步梯度更新和快速候选层/因式分解扫描实现对新数据集的适配，且在某些任务上将准确率提升高达 24.6 个点；对训练数据的过拟合风险有所缓解；下游任务的适配受提示风格影响更大而非数据集规模。

Conclusion: 本文提出一种快速且鲁棒的下游任务适配算法，大幅降低 LASER 的搜索开销，同时保持甚至提升性能，实现无需微调的高效模型自适应。

Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank
reduction (LASER) which demonstrated that pruning high-order components of
carefully chosen LLM's weight matrices can boost downstream accuracy -- without
any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
requiring full-dataset forward passes) makes it impractical for rapid
deployment. We demonstrate that this overhead can be removed and find that: (i)
Only a small, carefully chosen subset of matrices needs to be inspected --
eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
singular values pinpoints which matrices merit reduction, (iii) Increasing the
factorization search space by allowing matrices rows to cluster around multiple
subspaces and then decomposing each cluster separately further reduces
overfitting on the original training data and further lifts accuracy by up to
24.6 percentage points, and finally, (iv) we discover that evaluating on just
100 samples rather than the full training data -- both for computing the
indicative gradients and for measuring the final accuracy -- suffices to
further reduce the search time; we explain that as adaptation to downstream
tasks is dominated by prompting style, not dataset size. As a result, we show
that combining these findings yields a fast and robust adaptation algorithm for
downstream tasks. Overall, with a single gradient step on 100 examples and a
quick scan of the top candidate layers and factorization techniques, we can
adapt LLMs to new datasets -- entirely without fine-tuning.

</details>


### [112] [Generalizable Reasoning through Compositional Energy Minimization](https://arxiv.org/abs/2510.20607)
*Alexandru Oarga,Yilun Du*

Main category: cs.LG

TL;DR: 提出通过对可解子问题的能量景观进行组合构建全局能量景观，以提升推理任务的泛化能力，并引入并行能量最小化（PEM）提高样本质量。


<details>
  <summary>Details</summary>
Motivation: 解决推理任务中因端到端训练导致的泛化能力不足的问题，作者通过可组合的能量景观和子问题约束来提升对更大更复杂问题的推理能力。

Method: 学习对较小子问题的能量景观；测试时将多个子问题的能量函数组合成全局景观，允许在推理过程中引入额外约束；提出Parallel Energy Minimization (PEM) 以提升从新景观采样的质量。

Result: 在广泛的推理任务上评估，方法优于现有SOTA，展示对更大更复杂问题的泛化能力。

Conclusion: 能量景观的可组合性和PEM为跨规模推理提供一个可扩展框架，便于在推理阶段逐步增加难度并提升解的质量。

Abstract: Generalization is a key challenge in machine learning, specifically in
reasoning tasks, where models are expected to solve problems more complex than
those encountered during training. Existing approaches typically train
reasoning models in an end-to-end fashion, directly mapping input instances to
solutions. While this allows models to learn useful heuristics from data, it
often results in limited generalization beyond the training distribution. In
this work, we propose a novel approach to reasoning generalization by learning
energy landscapes over the solution spaces of smaller, more tractable
subproblems. At test time, we construct a global energy landscape for a given
problem by combining the energy functions of multiple subproblems. This
compositional approach enables the incorporation of additional constraints
during inference, allowing the construction of energy landscapes for problems
of increasing difficulty. To improve the sample quality from this newly
constructed energy landscape, we introduce Parallel Energy Minimization (PEM).
We evaluate our approach on a wide set of reasoning problems. Our method
outperforms existing state-of-the-art methods, demonstrating its ability to
generalize to larger and more complex problems. Project website can be found
at: https://alexoarga.github.io/compositional_reasoning/

</details>


### [113] [Convergence Analysis of SGD under Expected Smoothness](https://arxiv.org/abs/2510.20608)
*Yuta Kawamoto,Hideaki Iiduka*

Main category: cs.LG

TL;DR: Self-contained SGD analysis under the expected smoothness (ES) condition, refining ES with sampling-dependent constants, bounding the expected squared full gradient norm, and proving O(1/K) convergence with explicit residuals for various step sizes; unifies previous lines of work.


<details>
  <summary>Details</summary>
Motivation: Classical SGD analyses rely on too-strong (bounded variance) or too-coarse (uniform noise) assumptions. The ES condition provides a flexible bridge by linking the second moment of stochastic gradients to the objective value and the full gradient, enabling more realistic analysis and sharper rates.

Method: A self-contained convergence analysis of SGD under ES. The paper (i) refines ES with interpretations and sampling-dependent constants; (ii) derives bounds on the expectation of the squared full gradient norm; (iii) proves O(1/K) convergence rates with explicit residual errors for multiple step-size schedules. All proofs are provided in the appendix and the work unifies and extends prior results by Khaled & Richtárik (2020) and Umeda & Iiduka (2025).

Result: The analysis yields refined ES-based bounds, explicit constants tied to sampling and objective properties, and O(1/K) convergence with residual terms under several step-size schemes, offering a unified framework that subsumes prior ES-based SGD results.

Conclusion: ES-based SGD provides a versatile and coherent framework for convergence analysis, capturing a spectrum of stochastic gradient noises and enabling explicit rates and residuals. The work consolidates and extends existing ES-based approaches, facilitating clearer comparisons and potential future refinements.

Abstract: Stochastic gradient descent (SGD) is the workhorse of large-scale learning,
yet classical analyses rely on assumptions that can be either too strong
(bounded variance) or too coarse (uniform noise). The expected smoothness (ES)
condition has emerged as a flexible alternative that ties the second moment of
stochastic gradients to the objective value and the full gradient. This paper
presents a self-contained convergence analysis of SGD under ES. We (i) refine
ES with interpretations and sampling-dependent constants; (ii) derive bounds of
the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates
with explicit residual errors for various step-size schedules. All proofs are
given in full detail in the appendix. Our treatment unifies and extends recent
threads (Khaled and Richt\'arik, 2020; Umeda and Iiduka, 2025).

</details>


### [114] [MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation](https://arxiv.org/abs/2510.20615)
*Yang Han,Pengyu Wang,Kai Yu,Xin Chen,Lu Chen*

Main category: cs.LG

TL;DR: 提出 MS-BART 框架，将质谱与分子结构映射到共享标记词表，以实现跨模态大规模自监督预训练；在可靠计算的指纹-分子数据集上进行多任务预训练，后续通过对实验谱的微调与 MIST 的指纹预测实现鲁棒性提升；引入化学反馈机制以降低分子幻觉并提高与参考结构的一致性。在 MassSpecGym 与 NPLIB1 的 12 项指标中实现 5 项的 SOTA，并比对手扩散法快一个数量级；通过消融研究验证有效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决质谱数据标注稀缺与原始光谱信号的异质性，使跨模态学习在质谱领域得以大规模应用。通过将质谱和分子结构映射到共同的标记空间，利用大规模指纹-分子数据进行预训练，从而提升对实验谱的泛化与鲁棒性。

Method: 把质谱和分子结构映射到同一标记词表，然后进行联合预训练。核心任务包括去噪与翻译等多任务目标，以提升模型在两模态之间的对齐与翻译能力。预训练完成后，通过对实验谱进行微调，使用 MIST 产生的指纹预测来对谱-物质映射进行适配；为应对真实光谱的分布差异，采用化学反馈机制将生成的分子向参考结构的方向引导，降低分子幻觉。

Result: 在 MassSpecGym 与 NPLIB1 上的 5/12 指标达到 SOTA，且相比竞争的扩散模型速度快一个数量级；全面的消融研究证实了模型的有效性与鲁棒性。

Conclusion: MS-BART 展示了跨模态知识迁移和对现实光谱的鲁棒适应能力，显著提升了质谱-分子结构的对齐与推断效率；但仍存在分子幻觉等问题，需进一步对齐与约束，化学反馈机制虽有帮助但尚需改进。

Abstract: Mass spectrometry (MS) plays a critical role in molecular identification,
significantly advancing scientific discovery. However, structure elucidation
from MS data remains challenging due to the scarcity of annotated spectra.
While large-scale pretraining has proven effective in addressing data scarcity
in other domains, applying this paradigm to mass spectrometry is hindered by
the complexity and heterogeneity of raw spectral signals. To address this, we
propose MS-BART, a unified modeling framework that maps mass spectra and
molecular structures into a shared token vocabulary, enabling cross-modal
learning through large-scale pretraining on reliably computed
fingerprint-molecule datasets. Multi-task pretraining objectives further
enhance MS-BART's generalization by jointly optimizing denoising and
translation task. The pretrained model is subsequently transferred to
experimental spectra through finetuning on fingerprint predictions generated
with MIST, a pre-trained spectral inference model, thereby enhancing robustness
to real-world spectral variability. While finetuning alleviates the
distributional difference, MS-BART still suffers molecular hallucination and
requires further alignment. We therefore introduce a chemical feedback
mechanism that guides the model toward generating molecules closer to the
reference structure. Extensive evaluations demonstrate that MS-BART achieves
SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is
faster by one order of magnitude than competing diffusion-based methods, while
comprehensive ablation studies systematically validate the model's
effectiveness and robustness.

</details>


### [115] [On Optimal Hyperparameters for Differentially Private Deep Transfer Learning](https://arxiv.org/abs/2510.20616)
*Aki Rehn,Linzh Zhao,Mikko A. Heikkilä,Antti Honkela*

Main category: cs.LG

TL;DR: DP transfer learning的超参数C（裁剪界限）和B（小批量）对隐私-效用平衡至关重要；理论上对C的期望与实际观测存在明显错配，B的经验法则在固定训练预算下失效；累计DP噪声比单次梯度裁剪更能解释性能趋势；跨任务使用固定(C,B)容易劣化表现；裁剪可视为梯度重新加权且受累计噪声影响显著，因此需任务特定调参并考虑噪声累积。


<details>
  <summary>Details</summary>
Motivation: 在隐私约束下对大模型进行微调的差分隐私（DP）迁移学习是当前最前沿的方法。本研究聚焦于两个关键超参数C和B，揭示理论对C的选择与经验结果之间的显著错配，以及在有限计算预算下B的调参无效性。

Method: 通过理论分析与实验评估，研究在DP-SGD设置下比较不同C和B的影响，固定训练轮次，考察梯度分布的变化、裁剪对梯度的重加权作用以及累计DP噪声的影响；并比较在不同隐私强度与计算预算条件下的跨任务表现，分析单一(C,B)设置的局限性。

Result: 发现更强的隐私需求下，理论上应更小的C，但实际中更大的C在强隐私下表现更好；现有的B调参启发式在固定预算下并不有效；累计DP噪声比单次梯度裁剪更能解释更小/更大批量在不同情形下的表现差异；跨任务使用相同(C,B)会带来显著性能下降，尤其在从宽松到严格隐私、以及从充裕到受限的计算条件之间。

Conclusion: 结论是DP微调应为任务特定的C和B调参，并考虑裁剪作为梯度重新加权的效应以及累计DP噪声的影响，不能依赖单一跨任务的超参数设置。未来工作可在考虑梯度分布变化与噪声累积的基础上给出更具约束性的调参指南。

Abstract: Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained
model on private data, is the current state-of-the-art approach for training
large models under privacy constraints. We focus on two key hyperparameters in
this setting: the clipping bound $C$ and batch size $B$. We show a clear
mismatch between the current theoretical understanding of how to choose an
optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes
(larger $C$ performs better under strong privacy), caused by changes in the
gradient distributions. Assuming a limited compute budget (fixed epochs), we
demonstrate that the existing heuristics for tuning $B$ do not work, while
cumulative DP noise better explains whether smaller or larger batches perform
better. We also highlight how the common practice of using a single $(C,B)$
setting across tasks can lead to suboptimal performance. We find that
performance drops especially when moving between loose and tight privacy and
between plentiful and limited compute, which we explain by analyzing clipping
as a form of gradient re-weighting and examining cumulative DP noise.

</details>


### [116] [H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition](https://arxiv.org/abs/2510.20627)
*Lukas Miklautz,Chengzhi Shi,Andrii Shkabrii,Theodoros Thirimachos Davarakis,Prudence Lam,Claudia Plant,Jennifer Dy,Stratis Ioannidis*

Main category: cs.LG

TL;DR: 提出 H-SPLID，通过将显著和非显著特征分离到不同子空间，得到低维、任务相关的特征表示。给出在输入扰动下的预测偏差上界，度量依赖于显著子空间维度和输入与表示之间的 HSIC；在图像分类任务中，模型更依赖显著特征，对背景等非显著特征的扰动不敏感。


<details>
  <summary>Details</summary>
Motivation: 提升对抗鲁棒性与表征压缩的统一理解：通过显式分离显著特征与非显著特征来获得更鲁棒且低维的表示，并揭示鲁棒性与潜在表示的维度及信息保持之间的关系。

Method: 提出一种显式的显著–非显著特征分解算法 H-SPLID，将特征分解到独立的子空间。推导预测在输入扰动下的期望偏差上界，该上界由显著子空间的维度及输入与表示之间的 HSIC 决定，并通过理论分析连接鲁棒性与潜在表示的压缩。通过在图像分类任务的实验验证，训练得到的模型主要依赖显著输入分量，对影响非显著特征（如背景）的扰动不那么敏感，并给出开源代码。

Result: 在图像分类上，H-SPLID 显著降低了对非显著特征扰动的敏感性，表现为鲁棒性提升和对显著特征的依赖增强；理论上给出预测偏差上界与显著子空间维度及 HSIC 的关系；实现可重复性强，代码公开。

Conclusion: 该工作证明了显著–非显著特征分离能够提升鲁棒性与表达压缩的一致性，通过限制显著子空间的维度并降低与输入的无关信息的耦合，可获得对扰动更健壮的表征。

Abstract: We introduce H-SPLID, a novel algorithm for learning salient feature
representations through the explicit decomposition of salient and non-salient
features into separate spaces. We show that H-SPLID promotes learning
low-dimensional, task-relevant features. We prove that the expected prediction
deviation under input perturbations is upper-bounded by the dimension of the
salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between
inputs and representations. This establishes a link between robustness and
latent representation compression in terms of the dimensionality and
information preserved. Empirical evaluations on image classification tasks show
that models trained with H-SPLID primarily rely on salient input components, as
indicated by reduced sensitivity to perturbations affecting non-salient
features, such as image backgrounds. Our code is available at
https://github.com/neu-spiral/H-SPLID.

</details>


### [117] [Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges](https://arxiv.org/abs/2510.20637)
*Hyun Jong Yang,Hyunsoo Kim,Hyeonho Noh,Seungnyun Kim,Byonghyo Shim*

Main category: cs.LG

TL;DR: LLMs/LMMs enable task-oriented autonomous wireless systems for 6G by integrating multimodal sensing and adaptive reconfiguration, demonstrated via three case studies, outperforming traditional methods and remaining robust in dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: To tackle the complexity of 6G autonomous communications by leveraging the reasoning, multimodal perception, and flexible adaptation capabilities of LLMs/LMMs, enabling coordinated control and decision-making across machines, vehicles, and humanoids.

Method: Propose a framework that combines multimodal sensing integration, adaptive reconfiguration, and prompt/fine-tuning strategies for wireless tasks. Validate through three case studies: LMM-based traffic control, LLM-based robot scheduling, and LMM-based environment-aware channel estimation, comparing against conventional DL and static optimization baselines.

Result: Experimental results show that LLM/LMM-aided autonomous systems substantially outperform conventional and discriminative DL techniques, maintaining robustness under dynamic objectives, varying input parameters, and heterogeneous multimodal conditions where static optimization degrades.

Conclusion: LLMs/LMMs provide a strong foundation for task-oriented autonomous wireless systems in 6G, with multimodal integration and tailored prompting/fine-tuning enabling robust, adaptive, and high-performance wireless decision-making.

Abstract: Large language models (LLMs) and large multimodal models (LMMs) have achieved
unprecedented breakthrough, showcasing remarkable capabilities in natural
language understanding, generation, and complex reasoning. This transformative
potential has positioned them as key enablers for 6G autonomous communications
among machines, vehicles, and humanoids. In this article, we provide an
overview of task-oriented autonomous communications with LLMs/LMMs, focusing on
multimodal sensing integration, adaptive reconfiguration, and
prompt/fine-tuning strategies for wireless tasks. We demonstrate the framework
through three case studies: LMM-based traffic control, LLM-based robot
scheduling, and LMM-based environment-aware channel estimation. From
experimental results, we show that the proposed LLM/LMM-aided autonomous
systems significantly outperform conventional and discriminative deep learning
(DL) model-based techniques, maintaining robustness under dynamic objectives,
varying input parameters, and heterogeneous multimodal conditions where
conventional static optimization degrades.

</details>


### [118] [Attention Enhanced Entity Recommendation for Intelligent Monitoring in Cloud Systems](https://arxiv.org/abs/2510.20640)
*Fiza Hussain,Anson Bastos,Anjaly Parayil,Ayush Choure,Chetan Bansal,Rujia Wang,Saravan Rajmohan*

Main category: cs.LG

TL;DR: 提出 DiRecGNN，一种基于多头注意力的异构图实体排序模型，用于云服务监控中的属性子集推荐，显著提升MRR并获得高用户满意度。


<details>
  <summary>Details</summary>
Motivation: 克服生产场景中有限的结构信息、异构关系和长范围依赖的挑战；解决数据稀疏和属性维度选择问题；提升监控系统的自动化与有效性。

Method: 构建生产级监控异构图，设计基于Transformer灵感的注意力增强实体排名模型，使用多头注意力聚焦异构邻居及其属性，并对随机游走采样的路径进行长程依赖建模；引入多方面损失函数以优化相关性并处理数据稀疏。

Result: 在评估中显著优于现有方法，MRR提升43.1%；产品团队对该特性认可度高，打分4.5/5。

Conclusion: 所提出的方法在推荐监控属性子集方面有效且可部署，并呈现出对云服务监控自动化的实际价值和可迁移性；研究揭示了在异构图和长程依赖建模上的潜力。

Abstract: In this paper, we present DiRecGNN, an attention-enhanced entity
recommendation framework for monitoring cloud services at Microsoft. We provide
insights on the usefulness of this feature as perceived by the cloud service
owners and lessons learned from deployment. Specifically, we introduce the
problem of recommending the optimal subset of attributes (dimensions) that
should be tracked by an automated watchdog (monitor) for cloud services. To
begin, we construct the monitor heterogeneous graph at production-scale. The
interaction dynamics of these entities are often characterized by limited
structural and engagement information, resulting in inferior performance of
state-of-the-art approaches. Moreover, traditional methods fail to capture the
dependencies between entities spanning a long range due to their homophilic
nature. Therefore, we propose an attention-enhanced entity ranking model
inspired by transformer architectures. Our model utilizes a multi-head
attention mechanism to focus on heterogeneous neighbors and their attributes,
and further attends to paths sampled using random walks to capture long-range
dependencies. We also employ multi-faceted loss functions to optimize for
relevant recommendations while respecting the inherent sparsity of the data.
Empirical evaluations demonstrate significant improvements over existing
methods, with our model achieving a 43.1% increase in MRR. Furthermore, product
teams who consumed these features perceive the feature as useful and rated it
4.5 out of 5.

</details>


### [119] [xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion](https://arxiv.org/abs/2510.20651)
*Quan Li,Wenchao Yu,Suhang Wang,Minhua Lin,Lingwei Chen,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: 提出 xTime 框架，用知识蒸馏和专家混合（MoE）来提升时间序列极端事件的预测，通过从低罕见事件的模型中蒸馏信息并在不同罕见等级的专家之间动态选择/融合输出，显著提升极端事件的预测性能（实验显示极端事件准确率从 3% 提升至 78%）


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列中极端事件频繁发生且影响重大，但由于数据不平衡和忽略中间事件所蕴含信息，现有模型难以准确预测极端事件，亟需更好的方法以提高对稀有事件的预测能力

Method: 在 xTime 框架中，通过知识蒸馏将来自较低 rarities（较常见或中等稀有）事件的模型信息传递给目标模型；引入专家混合（MoE）机制，动态在不同稀有等级的专家模型之间选择并融合输出，以提升对极端事件的预测性能

Result: 在多数据集上进行实验，xTime 实现了一致的性能提升，极端事件的预测准确性从约 3% 提升到约 78%（相对显著的改进）

Conclusion: xTime 通过将知识蒸馏与 MoE 相结合，有效提升了极端事件在时间序列中的预测能力，证明了在数据不平衡场景中分布式专家和跨等级信息传递的有效性，未来可进一步探究教师模型设计、蒸馏策略和 MoE 的复杂性.

Abstract: Extreme events frequently occur in real-world time series and often carry
significant practical implications. In domains such as climate and healthcare,
these events, such as floods, heatwaves, or acute medical episodes, can lead to
serious consequences. Accurate forecasting of such events is therefore of
substantial importance. Most existing time series forecasting models are
optimized for overall performance within the prediction window, but often
struggle to accurately predict extreme events, such as high temperatures or
heart rate spikes. The main challenges are data imbalance and the neglect of
valuable information contained in intermediate events that precede extreme
events. In this paper, we propose xTime, a novel framework for extreme event
forecasting in time series. xTime leverages knowledge distillation to transfer
information from models trained on lower-rarity events, thereby improving
prediction performance on rarer ones. In addition, we introduce a mixture of
experts (MoE) mechanism that dynamically selects and fuses outputs from expert
models across different rarity levels, which further improves the forecasting
performance for extreme events. Experiments on multiple datasets show that
xTime achieves consistent improvements, with forecasting accuracy on extreme
events improving from 3% to 78%.

</details>


### [120] [Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts](https://arxiv.org/abs/2510.20666)
*Mariona Jaramillo-Civill,Luis González-Gudiño,Tales Imbiriba,Pau Closas*

Main category: cs.LG

TL;DR: Hybrid Bayesian mixture-of-experts for GNSS jam localization: the framework fuses a physical path-loss model with a CNN via log-linear pooling and uses Laplace-based Bayesian inference to estimate jammer position and RSS field; trained on urban ray-tracing data; shows improved localization and uncertainty containment near jammer and urban canyons as training data increases.


<details>
  <summary>Details</summary>
Motivation: GNSS signals are vulnerable to jamming, especially in urban environments with multipath and shadowing; existing data-driven approaches have limited spatial context and poorly reconstructed RSS fields; a method that enforces physical consistency while leveraging urban geometry is needed.

Method: A hybrid Bayesian mixture-of-experts framework combining a physical path-loss (PL) model and a CNN, fused through log-linear pooling. The PL expert enforces physical consistency; the CNN uses building-height maps to capture urban propagation. Bayesian inference with Laplace approximation yields posterior over jammer position and RSS field.

Result: Experiments on urban ray-tracing data show that localization accuracy improves and uncertainty decreases with more training points; uncertainty concentrates near the jammer and along urban canyons where propagation is most sensitive.

Conclusion: The approach provides physically consistent, uncertainty-aware localization for GNSS jamming in urban environments, leveraging data-driven learning for complex propagation while maintaining physical fidelity; effective improvement in localization and uncertainty behavior with larger training sets.

Abstract: Global Navigation Satellite System (GNSS) signals are vulnerable to jamming,
particularly in urban areas where multipath and shadowing distort received
power. Previous data-driven approaches achieved reasonable localization but
poorly reconstructed the received signal strength (RSS) field due to limited
spatial context. We propose a hybrid Bayesian mixture-of-experts framework that
fuses a physical path-loss (PL) model and a convolutional neural network (CNN)
through log-linear pooling. The PL expert ensures physical consistency, while
the CNN leverages building-height maps to capture urban propagation effects.
Bayesian inference with Laplace approximation provides posterior uncertainty
over both the jammer position and RSS field. Experiments on urban ray-tracing
data show that localization accuracy improves and uncertainty decreases with
more training points, while uncertainty concentrates near the jammer and along
urban canyons where propagation is most sensitive.

</details>


### [121] [From Masks to Worlds: A Hitchhiker's Guide to World Models](https://arxiv.org/abs/2510.20668)
*Jinbin Bai,Yu Lei,Hecong Wu,Yuchen Zhu,Shufan Li,Yi Xin,Xiangtai Li,Molei Tao,Aditya Grover,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: 一份关于“世界模型”研究的路线图型综述，主张从早期的跨模态表示学习到统一架构，再到交互生成模型，最终形成记忆增强系统，聚焦生成核心、交互循环与记忆系统，认为这是走向真正世界模型的最有前景路径。


<details>
  <summary>Details</summary>
Motivation: 提供一个实用的构建世界模型的指南，而非穷尽性地罗列相关论文；通过梳理演化路线，确立统一的研究方向与核心要素。

Method: 以历史演进为线索，论述从掩码化模型到统一架构再到交互生成与记忆的渐进过程，强调聚焦核心三大组件并有意识地绕开其他分支，以提出最具前景的世界模型路线。

Result: 提出一个清晰的路线图和当下最具潜力的研究方向：将生成核心、感知-行动循环和长期记忆整合为一个统一的世界模型框架。

Conclusion: 若要实现真正的世界模型，需将生成能力、互动循环和记忆系统三者有机结合，沿着该路线前进是最具希望的路径。

Abstract: This is not a typical survey of world models; it is a guide for those who
want to build worlds. We do not aim to catalog every paper that has ever
mentioned a ``world model". Instead, we follow one clear road: from early
masked models that unified representation learning across modalities, to
unified architectures that share a single paradigm, then to interactive
generative models that close the action-perception loop, and finally to
memory-augmented systems that sustain consistent worlds over time. We bypass
loosely related branches to focus on the core: the generative heart, the
interactive loop, and the memory system. We show that this is the most
promising path towards true world models.

</details>


### [122] [Separating the what and how of compositional computation to enable reuse and continual learning](https://arxiv.org/abs/2510.20709)
*Haozhe Shan,Sun Minni,Lea Duncker*

Main category: cs.LG

TL;DR: 提出一個兩系統框架來實現RNN的連續學習與組成性重用：what系統透過生成模型與單次試驗線上學習推斷任務的潛在epochs，建立可增長的詞彙；how系統為RNN，其低秩組件根據what系統推斷的上下文進行組成。此架構使連續學習不災難性遺忘，並具備對新任務的快速組成泛化與轉移潛力。


<details>
  <summary>Details</summary>
Motivation: 理解連續學習與技能再組合的神經機制，解決災難性遺忘問題，實現對多任務的組成性重用與快速泛化。

Method: 提出兩系統架構：what系統以可描述任務的概率生成模型為核心，具備單次試驗線上學習能力，隨著暴露新任務逐步增長詞彙，並在試驗中時變地推斷計算上下文；how系統為低秩組件可根據上下文組成的遞歸神經網路（RNN），實現任務間的組件創建、學習與重用以支援終身學習。

Result: 在範例任務集上展示良好且具競爭力的表現；顯示能在前向與後向轉移中獲得潛力，並對未見任務實現快速的組合泛化，從而避免災難性遺忘。

Conclusion: 兩系統架構可使連續學習與組成性重用成為可能，what系統的上下文推斷促成可重用的low-rank RNN模組的形成與更新，詞彙的增長支撐漸進式學習，並對未知任務展現良好泛化能力。

Abstract: The ability to continually learn, retain and deploy skills to accomplish
goals is a key feature of intelligent and efficient behavior. However, the
neural mechanisms facilitating the continual learning and flexible
(re-)composition of skills remain elusive. Here, we study continual learning
and the compositional reuse of learned computations in recurrent neural network
(RNN) models using a novel two-system approach: one system that infers what
computation to perform, and one that implements how to perform it. We focus on
a set of compositional cognitive tasks commonly studied in neuroscience. To
construct the what system, we first show that a large family of tasks can be
systematically described by a probabilistic generative model, where
compositionality stems from a shared underlying vocabulary of discrete task
epochs. The shared epoch structure makes these tasks inherently compositional.
We first show that this compositionality can be systematically described by a
probabilistic generative model. Furthermore, We develop an unsupervised online
learning approach that can learn this model on a single-trial basis, building
its vocabulary incrementally as it is exposed to new tasks, and inferring the
latent epoch structure as a time-varying computational context within a trial.
We implement the how system as an RNN whose low-rank components are composed
according to the context inferred by the what system. Contextual inference
facilitates the creation, learning, and reuse of low-rank RNN components as new
tasks are introduced sequentially, enabling continual learning without
catastrophic forgetting. Using an example task set, we demonstrate the efficacy
and competitive performance of this two-system learning framework, its
potential for forward and backward transfer, as well as fast compositional
generalization to unseen tasks.

</details>


### [123] [Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool](https://arxiv.org/abs/2510.20714)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: CSO对JHFRAT进行约束分数优化以提升跌倒风险预测，与现有JHFRAT显著提升，接近XGBoost，但在标签变异下更鲁棒，具备可解释性与资源分配潜力。


<details>
  <summary>Details</summary>
Motivation: 将可解释的、临床可接受的风险评分与更丰富的电子健康记录(EHR)变量结合，以提升住院跌倒风险预测的准确性与临床对齐度，便于资源分配与干预决策。

Method: 回顾性分析，样本量53万级别（54,209次住院），其中高风险20,208，低风险13,941。使用受限分数优化(CSO)对JHFRAT及EHR变量进行约束优化，比较CSO与基线JHFRAT及黑箱模型XGBoost的AUC-ROC表现。

Result: CSO模型AUC-ROC为0.91，优于JHFRAT的0.86；引入EHR变量时CSO表现与无EHR变量时相近；XGBoost达到0.94，但CSO在风险标签变异下更鲁棒。

Conclusion: 基于证据的数据驱动优化框架可为医院改进住院跌倒预防方案和资源分配提供稳健、可解释的工具，尽管黑箱模型在预测上略有优势，但CSO的鲁棒性和可解释性使其在临床落地更有潜力。

Abstract: In this study we aim to better align fall risk prediction from the Johns
Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically
meaningful measures via a data-driven modelling approach. We conducted a
retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins
Health System hospitals between March 2022 and October 2023. A total of 20,208
admissions were included as high fall risk encounters, and 13,941 were included
as low fall risk encounters. To incorporate clinical knowledge and maintain
interpretability, we employed constrained score optimization (CSO) models on
JHFRAT assessment data and additional electronic health record (EHR) variables.
The model demonstrated significant improvements in predictive performance over
the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained
score optimization models performed similarly with and without the EHR
variables. Although the benchmark black-box model (XGBoost), improves upon the
performance metrics of the knowledge-based constrained logistic regression
(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk
labelling. This evidence-based approach provides a robust foundation for health
systems to systematically enhance inpatient fall prevention protocols and
patient safety using data-driven optimization techniques, contributing to
improved risk assessment and resource allocation in healthcare settings.

</details>


### [124] [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](https://arxiv.org/abs/2510.20725)
*Jasmine Bayrooti,Sattar Vakili,Amanda Prorok,Carl Henrik Ek*

Main category: cs.LG

TL;DR: 在带高斯过程后验的强化学习中，给出无后悔保证：在K个回合、每回合高度H的设定下，汤普森采样的 regret 界服从 tilde O(sqrt(KH Γ(KH))).


<details>
  <summary>Details</summary>
Motivation: 尽管理论上汤普森采样广泛应用，但在具有复杂时间结构的RL中，其理论基础仍有限。本工作试图在具备高斯边际分布的模型下建立无后悔结果，扩展到多输出情形与Bellman递归。

Method: 在 episodic RL 中，使用对奖励和转移具有联合 GP 先验的模型；对价值函数的非高斯性及Bellman更新的递归结构进行分析；将 ellipse potential lemma 等经典工具扩展到多输出设置。

Result: 证明了无后悔界，并给出 Γ(·) 表示 GP 模型的复杂度。

Conclusion: 工作推动了在 RL 中对汤普森采样理论基础的理解，展示结构假设与模型不确定性如何影响有限-horizon MDP 下的表现。

Abstract: Thompson sampling (TS) is a powerful and widely used strategy for sequential
decision-making, with applications ranging from Bayesian optimization to
reinforcement learning (RL). Despite its success, the theoretical foundations
of TS remain limited, particularly in settings with complex temporal structure
such as RL. We address this gap by establishing no-regret guarantees for TS
using models with Gaussian marginal distributions. Specifically, we consider TS
in episodic RL with joint Gaussian process (GP) priors over rewards and
transitions. We prove a regret bound of
$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,
where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis
addresses several challenges, including the non-Gaussian nature of value
functions and the recursive structure of Bellman updates, and extends classical
tools such as the elliptical potential lemma to multi-output settings. This
work advances the understanding of TS in RL and highlights how structural
assumptions and model uncertainty shape its performance in finite-horizon
Markov Decision Processes.

</details>


### [125] [Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process](https://arxiv.org/abs/2510.20736)
*Tsai Hor Chan,Feng Wu,Yihang Chen,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出一种基于Dirichlet Process的多模态学习框架，通过DP混合权重动态突出重要的模态特征并实现跨模态对齐与模态内表征的平衡；在多个数据集上显示优于方法，具有对关键超参数的鲁棒性。代码公开。


<details>
  <summary>Details</summary>
Motivation: 多模态融合需要在保持各模态特征表达能力的同时学习跨模态交互；现有方法往往过分强调跨模态边际分布对齐，可能对模态内表示造成过强正则化，削弱表达力。

Method: 将每个模态建模为混合高斯分布，并使用Dirichlet过程来动态计算混合成分的权重，使模型获得丰富-得到更多权重给更突出、更具代表性的特征成分，从而在促进跨模态对齐的同时保留模态内表达；并通过消融分析验证DP在对齐模态分布和超参数鲁棒性方面的作用。

Result: 在若干多模态数据集上与对比方法相比具有更高的性能；消融研究显示DP的引入提高了边际分布对齐的效果并且对关键超参数鲁棒。

Conclusion: 该DP驱动的多模态学习框架能自动平衡模态内表达与跨模态对齐，提升融合性能；提供代码实现以便复现。

Abstract: Developing effective multimodal fusion approaches has become increasingly
essential in many real-world scenarios, such as health care and finance. The
key challenge is how to preserve the feature expressiveness in each modality
while learning cross-modal interactions. Previous approaches primarily focus on
the cross-modal alignment, while over-emphasis on the alignment of marginal
distributions of modalities may impose excess regularization and obstruct
meaningful representations within each modality. The Dirichlet process (DP)
mixture model is a powerful Bayesian non-parametric method that can amplify the
most prominent features by its richer-gets-richer property, which allocates
increasing weights to them. Inspired by this unique characteristic of DP, we
propose a new DP-driven multimodal learning framework that automatically
achieves an optimal balance between prominent intra-modal representation
learning and cross-modal alignment. Specifically, we assume that each modality
follows a mixture of multivariate Gaussian distributions and further adopt DP
to calculate the mixture weights for all the components. This paradigm allows
DP to dynamically allocate the contributions of features and select the most
prominent ones, leveraging its richer-gets-richer property, thus facilitating
multimodal feature fusion. Extensive experiments on several multimodal datasets
demonstrate the superior performance of our model over other competitors.
Ablation analysis further validates the effectiveness of DP in aligning
modality distributions and its robustness to changes in key hyperparameters.
Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git

</details>


### [126] [MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs](https://arxiv.org/abs/2510.20762)
*Jan Sobotka,Luca Baroni,Ján Antolík*

Main category: cs.LG

TL;DR: 提出 MEIcoder，一种生物信息驱动的解码器，用于从神经群体活动中重建视觉刺激，在V1单细胞数据上实现最先进性能，且在数据稀缺情景下表现出色；可用1k–2.5k 个神经元和不足1k条训练数据实现高保真重建，并提出包含16万样本的统一基准。


<details>
  <summary>Details</summary>
Motivation: 在灵长类和人类中获取高吞吐量神经数据困难，深度学习解码在数据有限的情况下往往受限。需要生物学先验与鲁棒训练策略来提高在小数据集上的解码性能，并为脑-机接口等应用提供可移植、可靠的解码方案。

Method: 提出 MEIcoder：结合神经元特异的最兴奋输入（MEIs）作为先验、结构相似性指数损失（SSIM loss）以及对抗性训练来提升重建质量；进行消融实验以验证 MEIs 的关键作用，并在小数据集上进行扩展性评估。

Result: 在V1单细胞活动下取得了最先进的重建性能，尤其在神经元数量有限的小数据集上表现突出。消融结果表明 MEIs 是性能的主要驱动因素；扩展性实验显示只需约1,000–2,500 个神经元和少于1,000个训练样本即可重建高保真自然外观图像。并提出一个包含超过16万样本的统一基准以推动后续研究。

Conclusion: 证实早期视觉系统中实现可可靠解码的可行性，并为神经科学与神经工程应用提供实用启示及未来研究方向。

Abstract: Decoding visual stimuli from neural population activity is crucial for
understanding the brain and for applications in brain-machine interfaces.
However, such biological data is often scarce, particularly in primates or
humans, where high-throughput recording techniques, such as two-photon imaging,
remain challenging or impossible to apply. This, in turn, poses a challenge for
deep learning decoding techniques. To overcome this, we introduce MEIcoder, a
biologically informed decoding method that leverages neuron-specific most
exciting inputs (MEIs), a structural similarity index measure loss, and
adversarial training. MEIcoder achieves state-of-the-art performance in
reconstructing visual stimuli from single-cell activity in primary visual
cortex (V1), especially excelling on small datasets with fewer recorded
neurons. Using ablation studies, we demonstrate that MEIs are the main drivers
of the performance, and in scaling experiments, we show that MEIcoder can
reconstruct high-fidelity natural-looking images from as few as 1,000-2,500
neurons and less than 1,000 training data points. We also propose a unified
benchmark with over 160,000 samples to foster future research. Our results
demonstrate the feasibility of reliable decoding in early visual system and
provide practical insights for neuroscience and neuroengineering applications.

</details>


### [127] [Out-of-distribution Tests Reveal Compositionality in Chess Transformers](https://arxiv.org/abs/2510.20783)
*Anna Mészáros,Patrik Reizinger,Ferenc Huszár*

Main category: cs.LG

TL;DR: 270M-parameter chess Transformer shows compositional generalization and rule extrapolation, producing valid moves in out-of-distribution scenarios and high-quality moves on OOD puzzles; weaker than symbolic AI with explicit search on Chess960, but gap narrows against human players; training dynamics suggest emergent understanding of moving own pieces.


<details>
  <summary>Details</summary>
Motivation: Evaluate whether modern decision Transformers truly learn chess rules versus relying on patterns, using out-of-distribution tests and Chess960 variants.

Method: Train a 270M parameter chess Transformer and evaluate on OOD scenarios designed to test systematic generalization; compare with symbolic AI search-based methods; test Chess960; analyze training dynamics, including when the model first moves only its own pieces.

Result: Transformers exhibit compositional generalization and rule extrapolation, maintaining valid moves in diverse situations and solving high-quality OOD puzzles; in Chess960, basic strategy adapts but performance lags behind explicit search; gap narrows when playing humans on Lichess; training dynamics reveal emergent understanding of moving own pieces early in training.

Conclusion: Transformers capture some chess-rule-like structure and show emergent compositional abilities, but for hardest variants requiring search, symbolic AI still outperforms; results indicate partial rule understanding with room for improvement, especially in variant-rich settings.

Abstract: Chess is a canonical example of a task that requires rigorous reasoning and
long-term planning. Modern decision Transformers - trained similarly to LLMs -
are able to learn competent gameplay, but it is unclear to what extent they
truly capture the rules of chess. To investigate this, we train a 270M
parameter chess Transformer and test it on out-of-distribution scenarios,
designed to reveal failures of systematic generalization. Our analysis shows
that Transformers exhibit compositional generalization, as evidenced by strong
rule extrapolation: they adhere to fundamental syntactic rules of the game by
consistently choosing valid moves even in situations very different from the
training data. Moreover, they also generate high-quality moves for OOD puzzles.
In a more challenging test, we evaluate the models on variants including
Chess960 (Fischer Random Chess) - a variant of chess where starting positions
of pieces are randomized. We found that while the model exhibits basic strategy
adaptation, they are inferior to symbolic AI algorithms that perform explicit
search, but gap is smaller when playing against users on Lichess. Moreover, the
training dynamics revealed that the model initially learns to move only its own
pieces, suggesting an emergent compositional understanding of the game.

</details>


### [128] [KL-Regularized Reinforcement Learning is Designed to Mode Collapse](https://arxiv.org/abs/2510.20817)
*Anthony GX-Chen,Jatin Prakash,Jeff Guo,Rob Fergus,Rajesh Ranganath*

Main category: cs.LG

TL;DR: 本工作从理论与实验角度挑战“反向KL=模式聚焦、正向KL=全覆盖”的直觉，指出最优目标分布随正则化系数而变化，模式覆盖受正则化强度和奖励尺度影响；提出一个简单、可扩展、理论有据的后训练算法，能在不改变奖励量级的大幅度前提下优化覆盖所有高质量解的目标分布，并在语言模型与化学语言模型上提升解的质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 澄清在强化学习/语言模型微调中对数 KL 正则化的效果。反驳将前向/后向 KL 简化为“全覆盖/模式聚焦”的普遍直觉，揭示正则化系数及奖励尺度如何共同塑造目标分布。

Method: 理论分析结合经验实验，考察正则化方向（前向/后向KL）、正则化强度、奖励尺度、以及可验证奖励的相对大小对最优分布的影响；并据此设计一个简单、可扩展且理论支撑的后训练算法。

Result: KL 方向决定的是一组以正则化系数为参数的最优目标分布；模式覆盖更多依赖正则化强度与奖励尺度，而在低正则化、等奖励等设置下往往产生单峰、非多样的目标。提出的算法仅对奖励幅度做最小改动，即可使目标分布覆盖所有高质量模式，且对前向KL或反向KL均具鲁棒性，在对大型语言模型和化学语言模型的后训练中提升解的质量与多样性。

Conclusion: 为 RL 场景下的 KL 正则化提供新的理解框架，给出一个简洁且有效的后训练策略，适用于多种KL方向，提升多样性与质量，而无需额外的多样性信号。

Abstract: It is commonly believed that optimizing the reverse KL divergence results in
"mode seeking", while optimizing forward KL results in "mass covering", with
the latter being preferred if the goal is to sample from multiple diverse
modes. We show -- mathematically and empirically -- that this intuition does
not necessarily transfer well to doing reinforcement learning with
reverse/forward KL regularization (e.g. as commonly used with language models).
Instead, the choice of reverse/forward KL determines the family of optimal
target distributions, parameterized by the regularization coefficient. Mode
coverage depends primarily on other factors, such as regularization strength,
and relative scales between rewards and reference probabilities. Further, we
show commonly used settings such as low regularization strength and equal
verifiable rewards tend to specify unimodal target distributions, meaning the
optimization objective is, by construction, non-diverse. We leverage these
insights to construct a simple, scalable, and theoretically justified
algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a
target distribution which puts high probability over all high-quality sampling
modes. In experiments, this simple modification works to post-train both Large
Language Models and Chemical Language Models to have higher solution quality
and diversity, without any external signals of diversity, and works with both
forward and reverse KL when using either naively fails.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [129] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: Quantum-inspired MPS/DMRG approach to QUBO yields global minima on Sudoku and MaxCut instances; scalable and generalizable.


<details>
  <summary>Details</summary>
Motivation: Address QUBO optimization and Ising ground-state finding by leveraging tensor-network representations to manage large superpositions, aiming for reliable global minima in complex combinatorial problems.

Method: Represent the solution space as a Matrix Product State (MPS) and use a discrete driving schedule with a driver Hamiltonian (with transverse field) combined with the problem Hamiltonian. Update the MPS via standard Density Matrix Renormalization Group (DMRG) sweeps to iteratively minimize energy and promote spin flips/tunneling, yielding a heuristic but robust route to the ground state.

Result: Demonstrated on intermediate Sudoku puzzles (~200+ Ising spins with long-range couplings) and on MaxCut instances from Biq Mac up to 251 nodes and 3265 edges; the method reliably finds global minima across diverse instances.

Conclusion: A quantum-inspired tensor-network approach offers scalable, generalizable prospects for industrial-scale QUBO/Ising problems, with potential advantages in solution quality and applicability beyond near-optimal heuristics.

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [130] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 提出分析可靠性基准ARB，用于评估大语言模型在能源系统分析中的推理可靠性，通过五个子指标在多种情景和公开数据集下对多模型进行横向评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一框架来验证能源领域AI的推理正确性；现有评估侧重预测准确性与计算效率，未测试分析结论的逻辑完整性。

Method: 提出ARB框架，包含五项子指标：准确性、推理可靠性、不确定性纪律、政策一致性、透明度；在确定性、概率性、知识论性情景下使用开放数据集评估；对四种前沿模型在相同事实与监管条件下进行比较。

Result: 推理可靠性可定量衡量；GPT-4/5与Claude 4.5 Sonnet表现出高度一致和政策合规的推理（Analytical Reliability Index>90）；Gemini 2.5 Pro稳定性中等；Llama 3 70B未达职业阈值；统计验证显示差异显著且可重复。

Conclusion: ARB是能源文献中首个用于验证因果、概率与政策驱动推理的定量方法，为全球能源转型中的可信与透明分析应用提供参照框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [131] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 提出基于有向无环图(DAG)的推理轨迹建模，定义逻辑邻近性等指标，构建DAG-MATH CoT格式和基准，以评估LLMs在推理过程中的规则一致性与推理质量，揭示PASS@k与推理过程质量之间的差异。


<details>
  <summary>Details</summary>
Motivation: 当前CoT提升并非一定来自真正的规则性推理，而可能来自搜索或记忆等非推理过程。需要超越最终答案的评价，评估推理过程的结构化一致性与可验证性。

Method: 把CoT视为规则驱动的DAG随机过程，节点是中间推理状态，边是规则应用；提出逻辑邻近性(logical closeness)度量，衡量模型的CoT轨迹对DAG结构的符合程度；设计DAG-MATH CoT格式并构建基准，诱导LLMs生成符合该格式的CoT轨迹；在常见数学数据集上比较不同LLM家族的推理保真度。

Result: 在不同LLM家族之间即使PASS@k相近也存在统计显著的推理保真度差异，体现最终答案正确性与规则性推导之间的脱节；基准与度量能提供比传统PASS@k更细粒度的诊断。

Conclusion: 该框架在自由形式的CoT与形式化证明之间提供了折中，给出可操作的LLM推理评估诊断工具；代码与基准可重复研究。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [132] [RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs](https://arxiv.org/abs/2510.19954)
*Joseph Meyer,Divyansha Lachi,Reza Mohammadi,Roshan Reddy Upendra,Eva L. Dyer,Mark Li,Tom Palczewski*

Main category: cs.AI

TL;DR: 提出 RELATE，一种关系型编码器，具备 schema-agnostic、可插拔的特征编码能力。通过共用模态编码器对分类、数值、文本、时间属性进行编码，并使用 Perceiver 风格的跨注意力聚合，输出固定大小的、对排列不变的节点表示。


<details>
  <summary>Details</summary>
Motivation: 解决异构时序图中多类型节点和多列特征带来的架构耦合问题，提升可扩展性与参数共享，支持跨数据集预训练与通用 GNN 框架。

Method: 为不同模态属性设计共享编码器（分类、数值、文本、时间），再引入 Perceiver 风格的跨模态注意力，将多模态特征聚合为固定长度的节点表示，插件式地与任意通用 GNN 集成，提升可扩展性与兼容性。

Result: 在 RelBench 的 ReLGNN 和 HGT 评测中，RELATE 的性能接近架构特定编码器，差异不到 3%，同时参数量可降低至原来的约五分之一到五分之二的量级。

Conclusion: 该设计实现了模式无关的特征编码，支持跨数据集预训练，为关系图数据的基础模型提供了可扩展的通用编码方案。

Abstract: Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.

</details>


### [133] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: UVeye提出的分层解决方案在车辆保险欺诈中应对AI生成证据的挑战，显著提升检测、缓解和威慑能力，尽管仍面临检测误报/漏报和对手的持续对抗。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI推动欺诈规模和速度，保险行业每年面临数十亿美元损失。传统欺诈手段与AI生成的证据相结合，迫切需要更强的检测、验证和防范机制。

Method: 提出分层解决方案，结合基于AI的深伪检测、增强的验证流程、证据分析与身份/文档验证等多重手段，以检测、缓解和威慑AI驱动的车辆欺诈。

Result: 声称显著提升检测、缓解和威慑能力，推动对AI欺诈的防控进步；同时指出现有检测工具存在假阳性/假阴性，以及欺诈者持续适应的挑战。

Conclusion: UVeye分层解决方案在车辆欺诈防控方面代表重大进步；但欺诈者与检测技术的对抗将持续，行业需在资源与成本之间取得平衡。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [134] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 通过领导力人格特质预测学业成绩的研究，129名环境工程硕士生参与，随机森林在包含17个特质与领导评分时达到87.5%的准确率；不含领导评分时为85.71%。


<details>
  <summary>Details</summary>
Motivation: 探究人格特质与领导力指标对学业成绩的预测能力，以便早期识别学生优劣势，支持个性化学习。

Method: 收集五项领导力人格测试的23项特征及自评工具，结合学业报告平均分作为标签；进行EDA和相关分析，使用Pearson相关系数进行特征选择；将学业分数分为失败、及格、优秀三类，训练多种ML算法（SVM、LR、KNN、DT、GB、RF、XGBoost、LightGBM），并比较性能；最终以RF为最佳模型。

Result: 在包含17个特征+领导力分数的模型中，RF实现87.50%准确率；在不包含领导力分数的模型中，准确率为85.71%。

Conclusion: 结果提示机器学习可用于在早期阶段识别学生的优势与薄弱环节，帮助制定个性化学习策略；该研究为以人格特质和领导力评估驱动的学习干预提供实证支持。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [135] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: 提出一个简单高效的协议，将有意义文本隐藏在同长度的另一文本中；8B参数级别的开源LLMs足以实现；编码/解码可在本地笔记本上秒级完成。


<details>
  <summary>Details</summary>
Motivation: 研究文本与作者意图的解耦及其带来的信任与安全挑战，揭示LLMs可能被用于在看似合规文本中传递隐藏信息的风险与机制。

Method: 给出一个简单有效的编码/解码协议，使目标消息被编码到表面文本中，确保两段文本长度相同；利用开放源代码的8B参数级别的LLM在离线环境中实现快速编码与解码。

Result: 在本地笔记本上可实现秒级编码/解码，产出高质量的隐藏文本，证明文本与作者意图的分离成为可能。

Conclusion: 揭示了文本与作者意图分离的潜力，带来AI安全方面的紧迫问题，挑战我们对LLM“知道某事”的理解，以及对在合规模型输出中隐藏非合规内容的风险。

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [136] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: HCLA 是一个以人为中心的多智能体异常检测系统，连接 Parsing、Detection、Explanation 三个角色，通过对话工作流让非专家以自然语言提问、查看分析并获得上下文相关的推理。


<details>
  <summary>Details</summary>
Motivation: 通过引入人机协作的可解释性和交互性，提高金融取证中的透明度和信任度，降低对专业知识的依赖。

Method: 提出三角色架构和对话式工作流，使用开源网页界面，将用户意图转化为经典检测器（原型为 XGBoost）的输入模式，并给出基于特征的叙事解释。以 Wasabi Wallet 的比特币混币数据集为评估对象，比较基线检测器表现并评估 HCLA 的可解释性和交互 refinements。

Result: 基线检测器在标注数据上取得较高准确性；HCLA 增强了解释性和交互性，提升透明度与信任度；给出架构、交互循环、数据集、评估协议与局限性。

Conclusion: 人机协同设计有助于提升金融取证的透明度与信任，表明将可解释性与互动性融入检测工作流是可行的，并具备推广潜力，但需关注局限和数据偏差等问题。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [137] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: 本文提出“验证-价值悖论”作为评估AI在法律实践中的新范式，主张AI的效率提升往往需要更高强度的人工核验，因而AI在法律应用中的净价值常常被抵消。


<details>
  <summary>Details</summary>
Motivation: 当前对基于生成式AI的法律实践乐观主义忽视了AI与现实的错位、透明性不足及律师的诚实义务，需重新评估AI使用及其对法律教育的影响。

Method: 以理论分析为主，结合澳大利亚等地律师因提交AI输出不准确而被惩戒的案例，提出并论证‘验证-价值悖论’及其在法律实践与教育中的应用框架。

Result: 揭示在AI提高效率的同时，必须大量人工验证输出，导致净效益常常不明显；强调以真理忠诚与公民责任为核心的法律实践价值观。

Conclusion: 提出以对真相的忠诚和公民责任为核心的价值体系，鼓励在严格核验与伦理约束下使用AI，并据此调整法律教育以强化核验能力与伦理培训。

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [138] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: TRUST 提出去中心化、透明的审计框架，用以验证大语言模型推理链的正确性与安全性，破解中心化审计的鲁棒性、可扩展性、透明度和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有审计方法往往集中、不可解释且难以扩展，无法在高风险领域公开且可信地评估中间推理步骤的正确性与安全性。

Method: 提出四大要点：1) 基于多元审计者的共识机制，能在最多30%恶意参与者存在的情况下保证正确性；2) 将推理轨迹分层为DAG结构，以实现可扩展的并行审计；3) 通过区块链记录所有验证决策，提升公开可追溯性；4) 隐私保护分割，仅共享部分推理步骤以保护专有逻辑。

Result: 在多种LLM（GPT-OSS、DeepSeek-r1、Qwen）及多领域推理任务（数学、医学、科学、人文）上进行实验，TRUST 能有效检测推理缺陷，并对抗恶意审计者，具有理论上的安全性与经济激励保障。

Conclusion: TRUST 开创了去中心化AI审计的新范式，为在确保安全与透明的前提下部署大模型提供可行路径。

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [139] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: 提出了一个“锁定期”概念：从开放模仿转向身份巩固的阶段，使目标、偏好和内部表示趋于稳定并抗外部引导。通过形式化定义、可操作的检测指标与跨规模实验验证，发现巩固过程快速且非线性，其对通用能力的影响随模型规模而异，且是实现AGI级可靠性与安全性的关键控制点。


<details>
  <summary>Details</summary>
Motivation: 解释为何在扩展的LLMs中研究身份巩固是必要的：它决定了系统的可控性、可靠性与安全性，并可能在扩展中引发不可预测的目标或行为。

Method: 形式化定义锁定阶段，提出用于检测该阶段的操作性指标，并在不同规模的语言模型上进行实验以评估行为巩固与能力影响。

Result: 行为巩固速度快且呈非线性；对通用能力的影响呈多样性：小模型出现性能折中，中等规模模型可较低成本实现 adopts，大型模型可能出现短暂的不稳定性。

Conclusion: 巩固被视为达到AGI级别可靠性的前提，同时也是安全控制的关键点；身份可以被有意设计以提高可靠性，但在扩展的过程中也可能自发形成，可能强化不可预测的目标与行为。

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [140] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: A study on Individualized Cognitive Simulation (ICS) that benchmarks seven LLMs across an 11-condition cognitive framework using authorial style emulation, finding that hybrid linguistic-concept representations outperform profile cues and that LLMs mainly capture linguistic style rather than deep cognitive structures.


<details>
  <summary>Details</summary>
Motivation: To move beyond surface-level imitation and evaluate how different cognitive representation methods support authentic individualized cognitive simulation in LLMs, enabling more personalized, human-aligned storytelling.

Method: Construct a dataset from post-publication novels; propose an 11-condition cognitive evaluation framework; evaluate seven off-the-shelf LLMs on authorial style emulation; compare cognitive representations including linguistic features, concept mappings, and profile-based information; analyze performance across conditions.

Result: Hybrid representations that combine conceptual and linguistic features yield the best ICS performance, outperforming static profile-based cues. LLMs are better at mimicking linguistic style than narrative structure, revealing limits in deep cognitive simulation.

Conclusion: Integrating both linguistic and conceptual representations can improve personalized cognitive simulation in LLMs, providing a foundation for more personalized and human-aligned creative AI, while highlighting current limits in simulating deeper individual thought processes.

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [141] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: 提出 STaBERT，一种将 POI 与时间语义嵌入结合到基于 BERT 的移动预测框架中，以提升单城与多城预测的语义描述能力与预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有移动预测模型要么仅建模位置序列，要么将时间信息仅作为辅助输入，未能充分利用 POI 提供的丰富语义上下文。

Method: 提出 STaBERT，在 BERT 基础上引入 POI 嵌入与派生的时间描述符，在每个位置处构建统一、语义丰富的表征，整合 POI 与时间信息来建模人 mobility。

Result: 实验显示显著提升：单城 GEO-BLEU 从 0.34 提升至 0.75；多城从 0.34 提升至 0.56。

Conclusion: 通过融合 POI 与时间信息，STaBERT 能更好地捕获移动行为的语义结构，从而提高预测准确性。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [142] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA introduces an external-tool-augmented agent for Embodied Question Answering with multi-step reasoning and a data-generation pipeline that creates the EQA-RT dataset (~18K tasks); it achieves notable improvements on seen/unseen scenes and outperforms baselines on multiple benchmarks, indicating strong generalization.


<details>
  <summary>Details</summary>
Motivation: Current Embodied Question Answering (EQA) methods that rely solely on vision-language models (VLMs) struggle with explicit reasoning, planning, and inefficient exploration. There is a need for explicit multi-step reasoning and the use of external tools to gather richer information for better exploration and answers.

Method: ToolEQA integrates external tools into the agent to supply useful information for answering and planning; it designs a data-generation pipeline to automatically construct large-scale EQA tasks with reasoning trajectories and answers, producing the EQA-RT dataset with training and two test splits (Seen and Unseen scenes). The approach guides exploration and enables shorter exploration distances while improving answer accuracy.

Result: ToolEQA reports a 9.2–20.2% improvement in success rate over state-of-the-art baselines on EQA-RT-Seen and EQA-RT-Unseen, and overtakes the zero-shot ToolEQA by ~10%. It also achieves state-of-the-art performance on HM-EQA, OpenEQA, and EXPRESS-Bench, indicating strong generality across benchmarks.

Conclusion: The results demonstrate that tool-augmented, multi-step reasoning can substantially enhance EQA performance and generalization, and that the data-generation pipeline (EQA-RT) enables scalable evaluation of reasoning trajectories and tool-usage.

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [143] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: 本研究聚焦AI在医疗中的数据偏见问题，识别多种偏差类型并提出在数据收集与问题设计中提升公平性与鲁棒性的建议。


<details>
  <summary>Details</summary>
Motivation: 解决AI在临床应用中落地难的核心瓶颈之一，即训练数据的质量与公平性，避免偏差影响模型性能与公正性。

Method: 基于AI4HealthyAging项目的经验，对多用例中的偏差进行系统分析，描述历史偏差、表示偏差、测量偏差等，以及它们在性别、年龄、居住环境、社会经济地位、设备与标签等变量上的体现。

Result: 归纳出几种常见偏差类型及其在变量中的表现，提出在临床问题设计、数据收集流程和标签体系方面的改进建议，以提升AI系统的公平性与鲁棒性。

Conclusion: 通过对实践经验的总结，为未来医疗领域的AI项目提供方向，以开发更公平、健壮的AI解决方案。

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [144] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: A novel collateral damage assessment model for AI-enabled target engagement using a layered KRR architecture to quantify spreading, severity, likelihood, and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: As AI systems take larger roles in warfare, responsible targeting requires transparent assessment of collateral effects.

Method: A design-science approach implementing a layered Knowledge Representation and Reasoning (KRR) architecture that captures AI system components, engagement vectors, and contextual aspects, integrating temporal, spatial, and force dimensions with transparent reasoning.

Result: Prototype instantiation and demonstration, establishing a basis for developing trustworthy AI-enabled targeting assessment tools in military operations.

Conclusion: Provides a structured, transparent framework for collateral-damage assessment in AI-enabled target engagement and outlines directions for refinement and real-world deployment.

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [145] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [146] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: 提出 Instruction-Knowledge-Aware Continual Adaptation (IKnow)，通过在指令-响应对话格式中实现自监督目标，用文本自身的领域知识进行持续适应，无需外部资源或访问基础模型权重。


<details>
  <summary>Details</summary>
Motivation: 在仅能利用无标签测试数据、且基础模型权重受限、外部领域数据库不可用的情形下，仍要让大语言模型适应新领域并保持对指令的遵从性和语义表达。

Method: 将自监督学习目标改写为指令-响应对话格式，挖掘文本内部的领域知识并进行更深层语义编码，从而实现持续适应，不依赖外部知识库。

Result: 摘要未给出具体实验或结果，因此无法从摘要中确认性能提升，但旨在在无外部资源和受限权重条件下实现有效域适应与保留指令能力。

Conclusion: 为受限场景提供一个简单、通用的持续适应框架，该框架利用文本内嵌知识实现对新领域的无外部数据自适应，同时维护指令遵从性。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [147] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 提出了一个五功能的计算模型，用以在创新项目中提升产生的机会的新颖性，同时保持有用性；在酒店行业的案例中对比 Notebook LM 与 ChatGPT4o，结果显示比基线更具新颖性和/或有用性，但并非所有功能都能显著提升新颖性，需要进一步改进。


<details>
  <summary>Details</summary>
Motivation: 通过将创造力理论与计算实现结合，系统化地提升创新机会的生成质量与效率。

Method: 实现一个包含五个协同功能的计算模型，用于生成创新机会；在酒店行业的创新项目中对生成的机会进行评估，并与 Notebook LM、ChatGPT4o 进行对比分析。

Result: 模型生成的机会在新颖性与有用性方面优于对比基线，但并非所有功能都显著提升新颖性，提示需要进一步优化部分功能以释放潜力。

Conclusion: 证明将创造力理论转化为可执行计算模型具有可行性，未来工作将聚焦于识别和强化对新颖性贡献最大的功能，并改进或替换低效功能。

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [148] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出EBR神经推理器，用嵌入近似描述逻辑推理，针对SHOIQ，在缺失或错误数据下展现鲁棒性，较现有推理器更稳健。


<details>
  <summary>Details</summary>
Motivation: 现实知识库中存在不一致和错误数据，传统描述逻辑推理器对这些数据缺乏鲁棒性，难以在真实场景部署，因此需要更稳健的推理机制。

Method: 提出基于嵌入的神经推理器EBR，通过近似符号推理结果来实现推理；仅需要对原子概念和存在性约束进行实例检索就能获取任意概念的实例集合，覆盖描述逻辑 SHOIQ 的推理需求。

Result: 在实验中将EBR与最先进的推理器比较，结果表明EBR在面临缺失数据和错误数据时具有更好的鲁棒性，与现有推理器相比表现更稳健。

Conclusion: EBR为现实世界知识库中的可解释分类提供了更强的鲁棒推理能力，展示了神经-符号耦合在处理数据不完整性方面的潜力。

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [149] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: FLORA是一种无监督、基于模糊逻辑的知识图谱对齐方法，能同时对实体和关系进行全局对齐，具备可解释性、收敛性、可处理悬空实体，且在基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱对齐方法大多聚焦实体级对齐、需要大量标注数据且缺乏可解释性，迫切需要一种无监督、可解释且能对齐实体与关系的全局方法。

Method: 提出基于模糊逻辑的全局迭代对齐框架FLORA，采用无监督学习，能同时对实体和关系进行对齐，允许悬空实体，并在迭代过程中提供可解释的推理过程，且理论上证明收敛。

Result: 在主要基准数据集上达到状态-艺术（SOTA）的对齐性能，同时具备对悬空实体的鲁棒性和明确的可解释推理过程。

Conclusion: FLORA提供了一个简单且有效的无监督知识图谱对齐框架，结合模糊逻辑实现可解释推理、全局对齐及收敛性，并支持实体的悬空处理，具备实用性与理论保障。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [150] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: 提出 MIMOSA 框架，兼顾可解释性与性能，并嵌入因果、公平、隐私等伦理属性，覆盖多种数据类型的可解释模型家族。


<details>
  <summary>Details</summary>
Motivation: 建立可解释、可信赖的 AI 系统以促进信任、问责与安全部署；需要在可解释性、性能与伦理约束之间进行权衡。

Method: 对多种数据类型的监督学习进行形式化定义；将可解释模型分为特征重要性、规则和实例三大类，分析可解释性维度、推理机制与复杂性；给出因果性、公平性、隐私性等伦理属性的形式定义、评估指标与验证程序；考察这些属性在可解释流水线中的权衡与嵌入方式；在模型生成阶段评估伦理指标以奠定可信 AI 的理论基础。

Result: 提出一个覆盖多任务与多数据类型的理论框架，能够生成在准确性、可解释性、公平、隐私保护与因果认知方面兼具的 AI 系统。

Conclusion: MIMOSA 提供一个全面的方法论，帮助在可解释性与伦理属性之间平衡，推动可信、可审计的 AI 系统开发。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [151] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: 提出了 EcomEval——一个多语言、多模态的电商领域大模型评估基准，覆盖6大类37道题（含8道多模态题），数据来自真实的客户查询与交易日志，采用半自动标注流程以提高参考答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有电商领域评测在任务多样性、模态覆盖、数据真实性和语言覆盖方面存在不足，难以真实评估大模型在复杂电商场景中的能力，需要一个更贴近实际的基准来支持跨语言、跨模态评估。

Method: 构建六大类、37道题（含8道多模态题）的基准，覆盖7种语言（含5种低资源东南亚语言）；题材源自真实客户查询与交易日志，体现噪声与异质性；采用半自动流水线：大型模型撰写候选答案，由50余名具备电商与多语言专业的 annotators 审核修改；通过对不同规模模型的平均分来设定难度等级，提供挑战导向的细粒度评估。

Result: 推出完整的 EcomEval 基准与评测流程，显著扩展任务多样性、模态覆盖和语言覆盖，提供更贴近真实电商场景的评估能力，为研究与行业应用提供对比与诊断工具。

Conclusion: EcomEval 通过六大类、37道题、8道多模态题以及7语言的设计，填补现有基准在真实场景评估的空缺，促进对大型语言模型在电商领域的综合能力评估与比较。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [152] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 提出 Fluidity Index（FI）以量化模型在动态、可扩展环境中的适应性，并通过状态偏差基准评估上下文切换与连续性；强调闭集对比开集基准，优先使用闭环、开放式现实世界基准来测试适应性，主张二阶及以上的自我维持式计算以实现流动性。


<details>
  <summary>Details</summary>
Motivation: 在环境不断变化且具扩展性的场景中衡量与比较模型的自适应能力，帮助设计能理解、预测并对状态变更做出调整的智能系统。

Method: 引入 Fluidity Index（FI）作为可量化的适应性指标，通过对初始、当前和未来环境状态的偏差来评估响应精度，建立闭集/开集基准并区分闭环开放世界基准，优先采用闭环开放世界现实基准；评估模型理解、预测和调整状态变化的能力，并提出二阶及以上的自我维持性适应概念。

Result: 当前摘要未给出实验结果，提供的是评估框架和指标定义，尚待实证验证。

Conclusion: 真正的超智能模型应具备至少二阶适应性，能够通过数字补给实现自我维持的计算以达到最佳流动性。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [153] [Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs](https://arxiv.org/abs/2510.20691)
*Yanlin Song,Ben Liu,Víctor Gutiérrez-Basulto,Zhiwei Hu,Qianqian Xie,Min Peng,Sophia Ananiadou,Jeff Z. Pan*

Main category: cs.AI

TL;DR: Graph-RFT 提出一种两阶段的强化微调 KGQA 框架，通过 plan-KGsearch-and-Websearch-during-think 的思维流程，使 LLM 在 KG 与 Web 之间进行自主规划和自适应检索，以应对知识不足情境并避免局部短视推理。


<details>
  <summary>Details</summary>
Motivation: 现有的KGQA方法无法充分利用知识图谱中的丰富知识与大语言模型的推理能力，尤其在知识覆盖不完整时缺乏判断何时需要外部信息的机制，且推理过程易局部性、缺乏全局性多步规划，导致在存在相关知识时也会出错。

Method: 提出 Graph-RFT：一个两阶段的强化微调框架。核心包括：1) 基于计划-检索-检索思维的流程，允许模型在推理过程中进行 KG 与 Web 的自适应检索；2) 使用计划-检索数据集进行链式思维微调，解决冷启动问题；3) 引入计划-检索引导的强化学习，结合多奖励信号学习何时以及如何组合 KG 检索与网页检索；4) 采用笛卡尔式规划模块将复杂问题分解为有序子问题，并用逻辑表达式引导工具调用实现全局一致的多步推理。

Result: 在方法层面提出一个完整的概念性框架 Graph-RFT，结合计划、检索与多步推理，强调在知识不完整场景下的覆盖意识检索及全局性推理的一致性，尚无具体实验结果在摘要中披露。

Conclusion: 该工作提出的框架旨在使 LLM 在 KG 与 Web 间进行自主管理的规划与检索，提升在知识不完整条件下的跨源推理能力与全局性一致性，解决冷启动及局部推理的问题。

Abstract: Knowledge Graph Question Answering aims to answer natural language questions
by reasoning over structured knowledge graphs. While large language models have
advanced KGQA through their strong reasoning capabilities, existing methods
continue to struggle to fully exploit both the rich knowledge encoded in KGs
and the reasoning capabilities of LLMs, particularly in complex scenarios. They
often assume complete KG coverage and lack mechanisms to judge when external
information is needed, and their reasoning remains locally myopic, failing to
maintain coherent multi-step planning, leading to reasoning failures even when
relevant knowledge exists. We propose Graph-RFT, a novel two-stage
reinforcement fine-tuning KGQA framework with a
'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to
perform autonomous planning and adaptive retrieval scheduling across KG and web
sources under incomplete knowledge conditions. Graph-RFT introduces a
chain-of-thought fine-tuning method with a customized plan-retrieval dataset
activates structured reasoning and resolves the GRPO cold-start problem. It
then introduces a novel plan-retrieval guided reinforcement learning process
integrates explicit planning and retrieval actions with a multi-reward design,
enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired
planning module to decompose complex questions into ordered subquestions, and
logical expression to guide tool invocation for globally consistent multi-step
reasoning. This reasoning retrieval process is optimized with a multi-reward
combining outcome and retrieval specific signals, enabling the model to learn
when and how to combine KG and web retrieval effectively.

</details>


### [154] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 提出基于广义均值跨补偿指数的相干一致性AGI衡量，使用曲线下方面积（AUC）来量化跨域鲁棒性，并对GPT-4/5的CHC域分数进行评估，显示仍未达到真正的通用能力。


<details>
  <summary>Details</summary>
Motivation: 质疑以算术均值定义的AGI及其对跨域补偿性的依赖，主张需要在关键认知域实现一致且充足的能力，以避免对极端特长的过度依赖。

Method: 构建对广义均值在补偿指数的连续区间上的积分，得到面积下的曲线（AUC），涵盖算术、几何、谐波等极限情形，作为对“补偿性”假设的鲁棒性量化。将该指标应用于公开CHC域分数，以GPT-4/5为样本评估跨域一致性。

Result: 与单纯算术均值相比，AUC惩罚不均衡并揭示系统的总体通用能力不足；例如在GPT-5算术分数约为24%时，仍远未达到真正的通用能力。

Conclusion: 将广义均值积分引入AGI评估，提供一个更严格、可解释、对跨域依赖更敏感的框架，推动对向AGI迈进的真实进展的衡量。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [155] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: 提出 Real Deep Research (RDR) 一般可适用于分析任何研究领域的通用管线，旨在识别新兴趋势、跨域机会，并给出新的研究起点；在 AI 与机器人领域进行了应用，强调基础模型与机器人进展，并对其他科学领域也有初步扩展。


<details>
  <summary>Details</summary>
Motivation: 应对研究文献快速增长（年产出超万篇）、趋势快速演变、跨领域研究增多及专业知识局限带来的信息过载，提供一个可扩展、通用的分析框架以系统化地发现趋势与机会。

Method: 提出一个通用的分析管线 Real Deep Research (RDR)，通过系统化步骤识别新兴趋势、挖掘跨领域机会、并提供 Concrete starting points；在 AI 与机器人领域（特别是基础模型与机器人进展）进行应用，并对其他科学领域作初步扩展；主论文详述管线构建，附录给出各分析主题的丰富结果。

Result: 构建并应用了 RDR 管线于 AI/机器人领域，形成对新兴趋势、跨域机会及研究起点的系统性分析；附录提供了各分析主题的广泛结果，证明该管线在现实领域的可操作性与可扩展性。

Conclusion: 该工作提出的通用分析管线具备广泛适用性，能够帮助研究者把握 AI 与超越领域的研究动态，并为后续研究提供明确起点。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [156] [Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts](https://arxiv.org/abs/2510.19986)
*Drew B. Thomas*

Main category: cs.IR

TL;DR: 提出将大语言模型、向量数据库与检索增强生成(RAG)结合，用于对早期现代宗教图像进行分类，并以全页上下文生成的描述来映射到Iconclass编码；在五级和四级分类上达到87%与92%的精度，明显优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模、跨文本与图像的早期现代视觉资料分类难题，利用LLMs对文本和图像信息的整合描述提升检索与分类准确性；促进艺术史与数字人文学科的定量分析。

Method: 利用全页书图的上下文，使用LLM生成包含视觉与文本元素的详细描述；通过混合向量搜索将描述映射到Iconclass编码；结合向量数据库与RAG以实现检索增强的分类过程。

Result: 在五级和四级分类上分别达到87%和92%的精度，显著优于传统的图像和关键词检索方法。

Conclusion: 本方法展示LLMs与RAG在艺术史与数字人文领域的潜力，支持大规模早期现代视觉档案的分析与研究。

Abstract: This paper presents a novel methodology for classifying early modern
religious images by using Large Language Models (LLMs) and vector databases in
combination with Retrieval-Augmented Generation (RAG). The approach leverages
the full-page context of book illustrations from the Holy Roman Empire,
allowing the LLM to generate detailed descriptions that incorporate both visual
and textual elements. These descriptions are then matched to relevant Iconclass
codes through a hybrid vector search. This method achieves 87% and 92%
precision at five and four levels of classification, significantly
outperforming traditional image and keyword-based searches. By employing
full-page descriptions and RAG, the system enhances classification accuracy,
offering a powerful tool for large-scale analysis of early modern visual
archives. This interdisciplinary approach demonstrates the growing potential of
LLMs and RAG in advancing research within art history and digital humanities.

</details>


### [157] [Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning](https://arxiv.org/abs/2510.20150)
*Yaochen Zhu,Harald Steck,Dawen Liang,Yinhan He,Jundong Li,Nathan Kallus*

Main category: cs.IR

TL;DR: ConvRec-R1 提出一个两阶段的端到端对话式推荐系统训练框架：Stage 1 使用 Remap-Reflect-Adjust 构建高质量的行为克隆数据，暖启动 RL；Stage 2 提出 Rank-GRPO，将策略单位改为排序位置，重定义奖励并引入基于排名的稳定更新。实验证明在 Reddit-v2 上更快收敛且 Recall、NDCG 提升。


<details>
  <summary>Details</summary>
Motivation: 解决预训练大语言模型在推荐任务中的不合规输出、超出目录的项、输出格式不符合要求，以及排序列表末尾的性能衰减等问题，寻求端到端、高质量的对话式推荐训练。

Method: Stage 1：用 Remap-Reflect-Adjust 管道从黑盒 LLM 产生的示例构建行为克隆数据集，以高质量、具目录约束的演示暖启动 RL；Stage 2：提出 Rank-GRPO，将策略单位设为排序列表中的每个位置，重新定义奖励以消除非因果信用分配，并引入基于排序等级的权重（基于排序项的概率几何均值）以稳健更新。

Result: 在公开数据集 Reddit-v2 上，ConvRec-R1 相较于 GRPO 风格基线具有更快的收敛速度以及更高的 Recall 与 NDCG。代码与数据集已开源。

Conclusion: ConvRec-R1 为基于 LLM 的对话式推荐提供一个可端到端训练的框架，通过阶段性数据构建与排序单位的强化学习优化，提高排序质量和训练稳定性。

Abstract: Large language models (LLMs) are reshaping the recommender system paradigm by
enabling users to express preferences and receive recommendations through
conversations. Yet, aligning LLMs to the recommendation task remains
challenging: pretrained LLMs often generate out-of-catalog items, violate
required output formats, and their ranking quality degrades sharply toward the
end of the generated list. To this end, we propose ConvRec-R1, a two-stage
framework for end-to-end training of LLM-based conversational recommender
systems. In Stage 1, we construct a behavioral-cloning dataset with a
Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded
demonstrations from powerful blackbox LLMs to warm-start the RL training. In
Stage 2, we propose Rank-GRPO, a principled extension of group relative policy
optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats
each rank in the recommendation list as the unit instead of token (too
fine-grained) or sequence (too coarse), redefining rewards to remove non-causal
credit assignment and introducing a rank-level importance ratio based on the
geometric mean of rank-wise token probabilities to stabilize policy updates.
Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges
faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and
datasets are released at https://github.com/yaochenzhu/Rank-GRPO.

</details>


### [158] [Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures](https://arxiv.org/abs/2510.20193)
*Rahul Raja,Arpita Vats*

Main category: cs.IR

TL;DR: 针对多模态检索增强的问答系统的综述，聚焦将视觉、语言、音频等模态对齐的检索-融合-生成架构，分类方法与数据集评测，分析权衡，并指出跨模态对齐、时延与语义定位等挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着多媒体内容激增，传统基于结构化文本的QA面临数据源单一、覆盖不足的问题，亟需对融合视觉/音频等模态的检索-问答系统进行综述、归纳与前沿分析。

Method: 系统性文献综述，按检索方法、融合技术、答案生成策略进行分类，评估基准数据集与评测协议，分析性能权衡，提炼挑战与研究方向。

Result: 提出多模态QA的分类框架和评估维度，汇总现有工作在跨模态对齐、时延-精度权衡、语义 grounding 等方面的进展与不足。

Conclusion: 强调跨模态对齐和语义理解的核心地位，指出实现更鲁棒、情境感知的多模态QA仍需解决的开放问题与未来研究方向。

Abstract: Question Answering (QA) systems have traditionally relied on structured text
data, but the rapid growth of multimedia content (images, audio, video, and
structured metadata) has introduced new challenges and opportunities for
retrieval-augmented QA. In this survey, we review recent advancements in QA
systems that integrate multimedia retrieval pipelines, focusing on
architectures that align vision, language, and audio modalities with user
queries. We categorize approaches based on retrieval methods, fusion
techniques, and answer generation strategies, and analyze benchmark datasets,
evaluation protocols, and performance tradeoffs. Furthermore, we highlight key
challenges such as cross-modal alignment, latency-accuracy tradeoffs, and
semantic grounding, and outline open problems and future research directions
for building more robust and context-aware QA systems leveraging multimedia
data.

</details>


### [159] [Balancing Fine-tuning and RAG: A Hybrid Strategy for Dynamic LLM Recommendation Updates](https://arxiv.org/abs/2510.20260)
*Changping Meng,Hongyi Ling,Jianling Wang,Yifan Liu,Shuzhou Zhang,Dapeng Hong,Mingyan Gao,Onkar Dalal,Ed Chi,Lichan Hong,Haokai Lu,Ningren Han*

Main category: cs.IR

TL;DR: 提出一种混合更新策略，将定期微调用于长期知识适应，利用低成本的检索增强生成(RAG)提升灵活性；在亿级用户平台的现场A/B测试中，混合方法显著提升用户满意度，具成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有的持续微调能捕捉领域知识和用户偏好，但难以跟上实时变化的用户兴趣与内容分布；RAG在成本与灵活性方面有优势，但对长期知识更新不足；需要一种兼顾长期知识与短期适应性的更新方案。

Method: 以“基于LLM的用户兴趣探索系统”为案例，比较持续微调与RAG在成本、敏捷性、知识融入等维度的表现；提出混合更新策略：结合周期性微调的长期知识适应与低成本RAG的灵活性；在一个亿级用户的平台上进行实时A/B实验验证。

Result: 混合更新策略在用户满意度上获得统计显著的提升，具成本效益；证明了在大规模部署中，结合长期知识更新与RAG的实用框架的可行性。

Conclusion: 对于维持高质量的LLM驱动推荐系统，混合更新策略是一个实际且具成本效益的方案，能够在大规模平台上通过长期知识适应与RAG的灵活性实现用户满意度提升。

Abstract: Large Language Models (LLMs) empower recommendation systems through their
advanced reasoning and planning capabilities. However, the dynamic nature of
user interests and content poses a significant challenge: While initial
fine-tuning aligns LLMs with domain knowledge and user preferences, it fails to
capture such real-time changes, necessitating robust update mechanisms. This
paper investigates strategies for updating LLM-powered recommenders, focusing
on the trade-offs between ongoing fine-tuning and Retrieval-Augmented
Generation (RAG). Using an LLM-powered user interest exploration system as a
case study, we perform a comparative analysis of these methods across
dimensions like cost, agility, and knowledge incorporation. We propose a hybrid
update strategy that leverages the long-term knowledge adaptation of periodic
fine-tuning with the agility of low-cost RAG. We demonstrate through live A/B
experiments on a billion-user platform that this hybrid approach yields
statistically significant improvements in user satisfaction, offering a
practical and cost-effective framework for maintaining high-quality LLM-powered
recommender systems.

</details>


### [160] [Rotate Both Ways: Time-and-Order RoPE for Generative Recommendation](https://arxiv.org/abs/2510.20455)
*Xiaokai Wei,Jiajun Wu,Daiyao Yi,Reza Shirkavand,Michelle Gong*

Main category: cs.IR

TL;DR: TO-RoPE 将时间与序列位置融入旋转位置嵌入，提升生成式推荐的时间-顺序编码效果，提供三种实例化策略，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么通过学习的时间嵌入要么通过相对注意力偏置来编码时间，但 vanilla RoPE 仅考虑令牌顺序，难以同时建模事件时间和索引。需要一种简单一致的几何表示来同时编码时间与位置。

Method: 提出 Time-and-Order RoPE (TO-RoPE)，一类旋转位置嵌入设计，利用事件时间和索引作为角度源，直接塑造查询-键的几何关系。提供三种实现：early fusion（前融合）、split-by-dim（按维度分离）、split-by-head（按注意力头分离）。

Result: 在公开数据集和一个企业数据集上的广泛实验表明，TO-RoPE 变体在编码时间与索引方面的准确性持续优于现有方法。

Conclusion: 旋转嵌入为生成式推荐提供一个简单、原理清晰且易于部署的基础，通过同时编码时间和位置来提升模型性能。

Abstract: Generative recommenders, typically transformer-based autoregressive models,
predict the next item or action from a user's interaction history. Their
effectiveness depends on how the model represents where an interaction event
occurs in the sequence (discrete index) and when it occurred in wall-clock
time. Prevailing approaches inject time via learned embeddings or relative
attention biases. In this paper, we argue that RoPE-based approaches, if
designed properly, can be a stronger alternative for jointly modeling temporal
and sequential information in user behavior sequences. While vanilla RoPE in
LLMs considers only token order, generative recommendation requires
incorporating both event time and token index. To address this, we propose
Time-and-Order RoPE (TO-RoPE), a family of rotary position embedding designs
that treat index and time as angle sources shaping the query-key geometry
directly. We present three instantiations: early fusion, split-by-dim, and
split-by-head. Extensive experiments on both publicly available datasets and a
proprietary industrial dataset show that TO-RoPE variants consistently improve
accuracy over existing methods for encoding time and index. These results
position rotary embeddings as a simple, principled, and deployment-friendly
foundation for generative recommendation.

</details>


### [161] [Analyticup E-commerce Product Search Competition Technical Report from Team Tredence_AICOE](https://arxiv.org/abs/2510.20674)
*Rakshith R,Shubham Sharma,Mohammed Sameer Khan,Ankush Chopra*

Main category: cs.IR

TL;DR: 通过数据增强与大模型微调，在多语言电商检索的 QC/QI 任务上达到竞争性结果，Gemma-3 12B 4-bit 在两任务均获佳表现，最终排名第4，平均 F1 0.8857。


<details>
  <summary>Details</summary>
Motivation: 实现全语言覆盖的多语言电商搜索，提升跨语言商品检索的相关性，解决不同语言数据分布不均的问题；利用大规模多语言模型和数据增强来提升跨语言性能。

Method: 数据层面：将现有数据翻译成开发集缺失语言，覆盖所有目标语言；模型层面：对 Gemma-3 12B（4-bit）与 Qwen-2.5 14B 进行两项任务的微调，采用多种训练策略；QI 任务使用原始、翻译和少数类数据创建等；QC 任务以原始+翻译数据为主以达到最佳效果。

Result: Gemma-3 12B（4-bit）在 QC 任务上以原始+翻译数据达到最佳；在 QI 任务上以原始+翻译+少数类数据创建达到最佳；最终在公开比赛中排名第4，私有测试集平均 F1=0.8857。

Conclusion: 多语言数据增强结合强大多语言模型的微调，能显著提升多语言电商检索的相关性任务表现，尽管名次未夺冠，但展示了方法的可行性与潜力，未来可在数据生成策略和模型规模上进一步优化。

Abstract: This study presents the multilingual e-commerce search system developed by
the Tredence_AICOE team. The competition features two multilingual relevance
tasks: Query-Category (QC) Relevance, which evaluates how well a user's search
query aligns with a product category, and Query-Item (QI) Relevance, which
measures the match between a multilingual search query and an individual
product listing. To ensure full language coverage, we performed data
augmentation by translating existing datasets into languages missing from the
development set, enabling training across all target languages. We fine-tuned
Gemma-3 12B and Qwen-2.5 14B model for both tasks using multiple strategies.
The Gemma-3 12B (4-bit) model achieved the best QC performance using original
and translated data, and the best QI performance using original, translated,
and minority class data creation. These approaches secured 4th place on the
final leaderboard, with an average F1-score of 0.8857 on the private test set.

</details>


### [162] [Generative Reasoning Recommendation via LLMs](https://arxiv.org/abs/2510.20815)
*Minjie Hong,Zetong Zhou,Zirun Guo,Ziang Zhang,Ruofan Hu,Weinan Gan,Jieming Zhu,Zhou Zhao*

Main category: cs.IR

TL;DR: 提出 GREAM：一个端到端的 GRRM 框架，通过三大组件将协同语义对齐、分层推理训练和稀疏正则化策略优化整合在一起，支持直接序列推荐和逐步推理推荐，在三个数据集上比强基线有提升。


<details>
  <summary>Details</summary>
Motivation: LLMs 在文本语义和协同过滤信号之间存在固有的建模差距，加之用户反馈的稀疏性和随机性，导致其难以原生成为可验证的生成推理推荐模型。需要将理解、推理、预测统一到一个框架中，并实现可解释与可验证的信号对齐。

Method: 1) Collaborative-Semantic Alignment：融合异质文本证据，构建语义一致的离散物品索引并设计辅助对齐任务，将语言表示与互动语义对齐；2) Reasoning Curriculum Activation：构建带有明确链式推理的合成数据集，并设计从行为证据提取、潜在偏好建模、意图推断、推荐形成到去噪序列改写的课程化推进；3) Sparse-Regularized Group Policy Optimization (SRPO)：通过可验证的奖励信号与奖金校准的分组优势估计，在稀疏成功的情况下实现端到端优化并实现后训练稳定性。"

Result: 在三个数据集上对比强基线，GREAM 展现出稳定的性能提升，表明其在端到端整合理解-推理-预测方面具备实际可行性；并提供两种推理模式（直接序列推荐与逐步推理推荐）以兼顾吞吐量与可解释性。

Conclusion: GREAM 为可验证的 RL 驱动 LLM 推荐系统提供了一个可落地的端到端框架，展示了将语言模型的理解与推理能力与协同过滤信号有效对齐、并以可验证的奖励信号驱动学习的路径，推动 GRRM 的研究与应用发展。

Abstract: Despite their remarkable reasoning capabilities across diverse domains, large
language models (LLMs) face fundamental challenges in natively functioning as
generative reasoning recommendation models (GRRMs), where the intrinsic
modeling gap between textual semantics and collaborative filtering signals,
combined with the sparsity and stochasticity of user feedback, presents
significant obstacles. This work explores how to build GRRMs by adapting
pre-trained LLMs, which achieves a unified understanding-reasoning-prediction
manner for recommendation tasks. We propose GREAM, an end-to-end framework that
integrates three components: (i) Collaborative-Semantic Alignment, which fuses
heterogeneous textual evidence to construct semantically consistent, discrete
item indices and auxiliary alignment tasks that ground linguistic
representations in interaction semantics; (ii) Reasoning Curriculum Activation,
which builds a synthetic dataset with explicit Chain-of-Thought supervision and
a curriculum that progresses through behavioral evidence extraction, latent
preference modeling, intent inference, recommendation formulation, and denoised
sequence rewriting; and (iii) Sparse-Regularized Group Policy Optimization
(SRPO), which stabilizes post-training via Residual-Sensitive Verifiable Reward
and Bonus-Calibrated Group Advantage Estimation, enabling end-to-end
optimization under verifiable signals despite sparse successes. GREAM natively
supports two complementary inference modes: Direct Sequence Recommendation for
high-throughput, low-latency deployment, and Sequential Reasoning
Recommendation that first emits an interpretable reasoning chain for causal
transparency. Experiments on three datasets demonstrate consistent gains over
strong baselines, providing a practical path toward verifiable-RL-driven LLM
recommenders.

</details>
