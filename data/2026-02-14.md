<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 59]
- [cs.IR](#cs.IR) [Total: 16]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.AI](#cs.AI) [Total: 60]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents](https://arxiv.org/abs/2602.11156)
*Sungmoon Kim,Hyuna Jeon,Dahye Kim,Mingyu Kim,Dong-Kyu Chae,Jiwoong Kim*

Main category: cs.CL

TL;DR: 针对传统RAG在实时查询时处理非结构化PDF文档的局限性，HybridRAG通过OCR和布局分析预处理文档，预生成问答知识库，采用优先匹配QA再回退生成的混合策略，在OHRBench上实现了更高的回答质量和更低的延迟。


<details>
  <summary>Details</summary>
Motivation: 现有RAG研究通常假设文本源为结构化格式（如Wikipedia），并在查询时进行检索和生成，这在实际聊天机器人场景中适用性有限，特别是处理大量非结构化PDF文档和大量用户请求时存在性能和资源瓶颈。

Method: HybridRAG首先通过OCR和布局分析处理原始PDF文档，将其转换为分层文本块；然后利用LLM从组织化的文本块中预生成问答知识库；在查询时，优先匹配QA库返回即时答案，仅在无合适匹配时回退到实时生成。

Result: 在OHRBench上的实验表明，与标准RAG基线相比，HybridRAG提供了更高的回答质量和更低的延迟。

Conclusion: HybridRAG是一种适用于真实世界聊天机器人应用的实用解决方案，能够有效处理大量非结构化文档和高并发用户请求，同时计算资源有限的情况。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval and generation at query time, which can limit their applicability in real-world chatbot scenarios. In this paper, we present HybridRAG, a novel and practical RAG framework towards more accurate and faster chatbot responses. First, HybridRAG ingests raw, unstructured PDF documents containing complex layouts (text, tables, figures) via Optical Character Recognition (OCR) and layout analysis, and convert them into hierarchical text chunks. Then, it pre-generates a plausible question-answer (QA) knowledge base from the organized chunks using an LLM. At query time, user questions are matched against this QA bank to retrieve immediate answers when possible, and only if no suitable QA match is found does our framework fall back to an on-the-fly response generation. Experiments on OHRBench demonstrate that our HybridRAG provides higher answer quality and lower latency compared to a standard RAG baseline. We believe that HybridRAG could be a practical solution for real-world chatbot applications that must handle large volumes of unstructured documents and lots of users under limited computational resources.

</details>


### [2] [Retrieval Heads are Dynamic](https://arxiv.org/abs/2602.11162)
*Yuping Lin,Zitao Li,Yue Xing,Pengfei He,Yingqian Cui,Yaliang Li,Bolin Ding,Jingren Zhou,Jiliang Tang*

Main category: cs.CL

TL;DR: 本文从动态视角研究LLM中的检索头，发现其在时间步上具有动态变化、不可替代和内部相关性三大特征，并通过Needle-in-a-Haystack和多跳问答任务验证，在动态检索增强生成框架中量化了动态与静态检索头的效用差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖静态统计方法识别检索头，忽略了自回归生成过程中检索头的细粒度时间动态特性，无法揭示LLM内部的真实工作机制。

Method: 通过动态视角对检索头进行跨时间步的细粒度分析，在Needle-in-a-Haystack和多跳问答任务上验证动态检索头的特性，并在动态检索增强生成框架中量化动态与静态检索头的效用差异。

Result: 提出了三个核心发现：1)动态性：检索头随时间步动态变化；2)不可替代性：动态检索头在每个时间步具有特异性，无法被静态检索头有效替代；3)相关性：模型隐状态编码了未来检索头模式的预测信号，表明存在内部规划机制。

Conclusion: 该研究揭示了LLM检索头的动态工作机制，为理解模型内部信息提取和规划机制提供了新视角，对改进检索增强生成方法具有重要意义。

Abstract: Recent studies have identified "retrieval heads" in Large Language Models (LLMs) responsible for extracting information from input contexts. However, prior works largely rely on static statistics aggregated across datasets, identifying heads that perform retrieval on average. This perspective overlooks the fine-grained temporal dynamics of autoregressive generation. In this paper, we investigate retrieval heads from a dynamic perspective. Through extensive analysis, we establish three core claims: (1) Dynamism: Retrieval heads vary dynamically across timesteps; (2) Irreplaceability: Dynamic retrieval heads are specific at each timestep and cannot be effectively replaced by static retrieval heads; and (3) Correlation: The model's hidden state encodes a predictive signal for future retrieval head patterns, indicating an internal planning mechanism. We validate these findings on the Needle-in-a-Haystack task and a multi-hop QA task, and quantify the differences on the utility of dynamic and static retrieval heads in a Dynamic Retrieval-Augmented Generation framework. Our study provides new insights into the internal mechanisms of LLMs.

</details>


### [3] [Nested Named Entity Recognition in Plasma Physics Research Articles](https://arxiv.org/abs/2602.11163)
*Muhammad Haris,Hans Höft,Markus M. Becker,Markus Stocker*

Main category: cs.CL

TL;DR: 本文针对等离子体物理论文专业实体抽取难题，提出基于编码器-Transformer与条件随机场的轻量级嵌套NER方法，通过构建16类标注语料库、训练独立BERT-CRF模型及系统化超参数优化，为领域文献分析提供技术支持。


<details>
  <summary>Details</summary>
Motivation: 等离子体物理研究论文内容高度复杂且富含上下文，现有通用NER技术难以有效提取领域专业实体，制约了高级检索与文献分析效率，亟需开发适应性强、轻量化的领域自适应实体识别方法。

Method: 1）构建包含16个类别的等离子体物理嵌套命名实体标注语料库；2）采用实体特定专业化策略，为每种实体类型训练独立的BERT-CRF模型；3）集成系统化超参数微调流程以优化模型性能；整体基于编码器-Transformer架构与条件随机场相结合的方法。

Result: 研究完成了等离子体物理NER语料库标注、实体专业化模型评估及超参数优化流程设计，形成了完整的领域实体识别技术框架，为后续性能验证与应用部署奠定了基础。

Conclusion: 该工作推动了等离子体物理领域实体识别技术的发展，所构建的语料库与轻量化方法不仅解决了专业实体抽取难题，还为科研人员提供了文献导航与分析工具，对促进领域知识发现具有重要意义。

Abstract: Named Entity Recognition (NER) is an important task in natural language processing that aims to identify and extract key entities from unstructured text. We present a novel application of NER in plasma physics research articles and address the challenges of extracting specialized entities from scientific text in this domain. Research articles in plasma physics often contain highly complex and context-rich content that must be extracted to enable, e.g., advanced search. We propose a lightweight approach based on encoder-transformers and conditional random fields to extract (nested) named entities from plasma physics research articles. First, we annotate a plasma physics corpus with 16 classes specifically designed for the nested NER task. Second, we evaluate an entity-specific model specialization approach, where independent BERT-CRF models are trained to recognize individual entity types in plasma physics text. Third, we integrate an optimization process to systematically fine-tune hyperparameters and enhance model performance. Our work contributes to the advancement of entity recognition in plasma physics and also provides a foundation to support researchers in navigating and analyzing scientific literature.

</details>


### [4] [Assessing LLM Reliability on Temporally Recent Open-Domain Questions](https://arxiv.org/abs/2602.11165)
*Pushwitha Krishnappa,Amit Das,Vinija Jain,Tathagata Mukherjee,Aman Chadha*

Main category: cs.CL

TL;DR: 本研究构建了RECOM基准数据集，包含15,000条2025年9月的Reddit近期问题与社区答案，通过多维度评估发现开源大模型在语义相似度上表现优异（>99%余弦相似度）但词汇重叠度极低（<8% BLEU-1），揭示了语义-词汇悖论，且模型规模与性能无关。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在开放域问答中的广泛应用，现有研究缺乏对其在最新时间信息上与人类观点对齐程度的探索。传统评估指标可能无法准确反映模型生成内容的真实质量，需要更全面的评估框架来理解模型如何处理近期知识。

Method: 研究者构建了RECOM基准，包含15,000条2025年9月的Reddit近期问题及社区参考答案。评估了四款开源模型（Llama3.1-8B、Mistral-7B、Gemma-2-9B和GPT-OSS-20B），采用三类指标：词汇层面（BLEU、ROUGE）、语义层面（BERTScore、MoverScore、余弦相似度）和逻辑推理（NLI）。

Result: 核心发现是显著的语义-词汇悖论：所有模型与参考答案的余弦相似度超过99%，但BLEU-1重叠度不足8%，差距超90个百分点，表明模型通过大量改写而非词汇复制来保持语义一致。MoverScore（51-53%）处于中间位置。规模预测失效：7B参数的Mistral-7B全面优于20B参数的GPT-OSS-20B。NLI分析显示矛盾率低于7%，模型极少与人类共识冲突。

Conclusion: 研究挑战了词汇指标在抽象生成评估中的可靠性，证明需要多维度评估框架来捕捉超越表面文本匹配的语义保真度。该发现对大模型评估方法论具有重要启示，推动从词汇匹配向语义理解转变。

Abstract: Large Language Models (LLMs) are increasingly deployed for open-domain question answering, yet their alignment with human perspectives on temporally recent information remains underexplored. We introduce RECOM (Reddit Evaluation for Correspondence of Models), a benchmark dataset of 15,000 recent Reddit questions from September 2025 paired with community-derived reference answers. We investigate how four open-source LLMs (Llama3.1-8B, Mistral-7B, Gemma-2-9B, and GPT-OSS-20B) respond to these questions, evaluating alignment using lexical metrics (BLEU, ROUGE), semantic similarity (BERTScore, MoverScore, cosine similarity), and logical inference (NLI). Our central finding is a striking semantic-lexical paradox: all models achieve over 99% cosine similarity with references despite less than 8% BLEU-1 overlap, a 90+ percentage point gap indicating that models preserve meaning through extensive paraphrasing rather than lexical reproduction. MoverScore (51-53%) confirms this pattern, occupying an intermediate position that reflects the optimal transport cost of semantic alignment. Furthermore, model scale does not predict performance: Mistral-7B (7B parameters) outperforms GPT-OSS-20B (20B parameters) across all metrics. NLI analysis reveals that contradiction rates remain below 7%, suggesting models rarely generate content that directly conflicts with human consensus. These findings challenge the reliability of lexical metrics for evaluating abstractive generation and argue for multi-dimensional evaluation frameworks that capture semantic fidelity beyond surface-level text matching. The RECOM dataset is publicly available at https://anonymous.4open.science/r/recom-D4B0

</details>


### [5] [Visualizing and Benchmarking LLM Factual Hallucination Tendencies via Internal State Analysis and Clustering](https://arxiv.org/abs/2602.11167)
*Nathan Mao,Varun Kaushik,Shreya Shivkumar,Parham Sharafoleslami,Kevin Zhu,Sunishchal Dev*

Main category: cs.CL

TL;DR: 本文针对大语言模型幻觉问题，提出FalseCite数据集来系统研究误导性引用引发的虚假信息生成。通过在GPT-4o-mini、Falcon-7B和Mistral-7B上的实验发现，deceptive citations会显著增加幻觉行为，且所有模型的隐藏状态向量均呈现独特的角状分布。该数据集为未来幻觉评估与缓解研究提供了重要基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗、法律等敏感领域经常产生无意义或虚假信息（即幻觉），造成严重危害。当前缺乏系统研究误导性引用诱导幻觉的数据集和基准，因此需要构建专门数据集来量化分析这一现象。

Method: 作者构建了FalseCite数据集，包含误导性或伪造引用诱导的幻觉响应。实验部分将GPT-4o-mini、Falcon-7B和Mistral-7B三个模型通过该数据集测试，并分析模型产生幻觉时的内部状态，通过可视化和聚类隐藏状态向量来探索其几何结构。

Result: 实验结果显示，带有欺骗性引用的虚假声明会显著增加模型幻觉活动，其中GPT-4o-mini表现最为明显。此外，分析发现无论是否产生幻觉，隐藏状态向量均倾向于形成独特的角状（horn-like）形状分布。

Conclusion: 研究表明FalseCite数据集能有效评估LLM幻觉行为，揭示了幻觉产生的内部表征模式。该数据集可作为未来研究幻觉评估与缓解的基准工具，为开发更可靠的LLM提供重要基础。

Abstract: Large Language Models (LLMs) often hallucinate, generating nonsensical or false information that can be especially harmful in sensitive fields such as medicine or law. To study this phenomenon systematically, we introduce FalseCite, a curated dataset designed to capture and benchmark hallucinated responses induced by misleading or fabricated citations. Running GPT-4o-mini, Falcon-7B, and Mistral 7-B through FalseCite, we observed a noticeable increase in hallucination activity for false claims with deceptive citations, especially in GPT-4o-mini. Using the responses from FalseCite, we can also analyze the internal states of hallucinating models, visualizing and clustering the hidden state vectors. From this analysis, we noticed that the hidden state vectors, regardless of hallucination or non-hallucination, tend to trace out a distinct horn-like shape. Our work underscores FalseCite's potential as a foundation for evaluating and mitigating hallucinations in future LLM research.

</details>


### [6] [Enhancing SDG-Text Classification with Combinatorial Fusion Analysis and Generative AI](https://arxiv.org/abs/2602.11168)
*Jingyan Xu,Marcelo L. LaFleur,Christina Schweikert,D. Frank Hsu*

Main category: cs.CL

TL;DR: 本研究利用组合融合分析（CFA）整合多模型智能，提升联合国可持续发展目标（SDGs）文本分类性能。通过生成式AI生成合成数据训练模型，CFA技术达到96.73%准确率，超越最优单模型，并与人类专家判断形成互补增强。


<details>
  <summary>Details</summary>
Motivation: 文本分类在类别模糊、关联性强或缺失时面临挑战，社会分析领域尤为突出。现有SDGs分类研究多依赖单一模型，未能充分利用多模型协同优势。因此，亟需一种融合框架来整合多样化分类模型的智能，以提升分类准确性和鲁棒性。

Method: 采用组合融合分析（CFA）框架，结合秩-得分特征（RSC）函数与认知多样性（CD）指标。首先利用生成式AI生成合成数据训练多个基分类器，然后通过CFA融合这些相互独立且性能较优的模型，最终实现SDGs文本分类任务的集成学习。

Result: CFA融合方法在SDGs分类任务中达到96.73%的性能，较最优单模型显著提升。与人类专家标注结果对比显示，AI模型融合与人类判断不仅可互补，还能相互增强，验证了人机协同的有效性。

Conclusion: 本研究验证了基于CFA的多模型融合策略在复杂文本分类任务中的有效性。通过整合生成式AI、集成学习与人类专家知识，构建了人机协同增强的智能分类框架，为SDGs评估、政策分析等领域提供了更精准可靠的文本分析工具，展示了人工智能与专家智慧结合的巨大潜力。

Abstract: (Natural Language Processing) NLP techniques such as text classification and topic discovery are very useful in many application areas including information retrieval, knowledge discovery, policy formulation, and decision-making. However, it remains a challenging problem in cases where the categories are unavailable, difficult to differentiate, or are interrelated. Social analysis with human context is an area that can benefit from text classification, as it relies substantially on text data. The focus of this paper is to enhance the classification of text according to the UN's Sustainable Development Goals (SDGs) by collecting and combining intelligence from multiple models. Combinatorial Fusion Analysis (CFA), a system fusion paradigm using a rank-score characteristic (RSC) function and cognitive diversity (CD), has been used to enhance classifier methods by combining a set of relatively good and mutually diverse classification models. We use a generative AI model to generate synthetic data for model training and then apply CFA to this classification task. The CFA technique achieves 96.73% performance, outperforming the best individual model. We compare the outcomes with those obtained from human domain experts. It is demonstrated that combining intelligence from multiple ML/AI models using CFA and getting input from human experts can, not only complement, but also enhance each other.

</details>


### [7] [Disentangling Direction and Magnitude in Transformer Representations: A Double Dissociation Through L2-Matched Perturbation Analysis](https://arxiv.org/abs/2602.11169)
*Mangadoddi Srikar Vardhan,Lekkala Sai Teja*

Main category: cs.CL

TL;DR: 本研究通过L2匹配扰动分析发现，在LayerNorm架构的Transformer模型中，隐藏状态的方向和幅度承担不同功能：方向主要影响注意力路由，对语言建模损失更敏感；幅度则调节句法处理的强度。该现象在RMSNorm架构中表现不同，揭示了架构选择对功能分离的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer隐藏状态以高维向量形式编码信息，但其方向（表征空间中的取向）和幅度（向量范数）是否承担不同功能角色尚不明确。澄清这一问题有助于理解表征的几何特性如何支持语言处理，并对模型编辑和可解释性研究有重要意义。

Method: 采用L2匹配扰动分析方法，确保角度扰动和幅度扰动产生相同的欧几里得位移，从而分离两者的影响。通过因果干预实验，分别修复注意力路径和LayerNorm路径，量化不同路径对功能损伤的贡献。研究在Pythia模型家族的不同规模模型上进行验证。

Result: 发现交叉分离现象：角度扰动对语言建模损失的损害最大可达幅度扰动的42.9倍；而幅度扰动对句法处理（主谓一致）的损害更显著（准确率下降20.4% vs 1.6%）。因果干预显示，角度损伤主要通过注意力路径传播（注意力修复可恢复28.4%损失），幅度损伤则部分通过LayerNorm路径传播（LayerNorm修复可恢复29.9%损失）。这些模式在Pythia架构家族中跨规模复现，但在RMSNorm架构中表现不同。

Conclusion: 研究表明，在LayerNorm架构中，方向与幅度支持部分分离的计算角色：方向优先影响注意力路由，幅度调节细粒度句法判断的处理强度。这一发现细化了线性表征假说，并表明功能分离依赖于架构选择。结果对模型编辑和可解释性研究具有重要启示。

Abstract: Transformer hidden states encode information as high-dimensional vectors, yet whether direction (orientation in representational space) and magnitude (vector norm) serve distinct functional roles remains unclear. Studying Pythia-family models, we discover a striking cross-over dissociation: angular perturbations cause up to 42.9 more damage to language modeling loss, while magnitude perturbations cause disproportionately more damage to syntactic processing (20.4% vs.1.6% accuracy drop on subject-verb agreement).This finding is enabled by L2-matched perturbation analysis, a methodology ensuring that an gular and magnitude perturbations achieve identical Euclidean displacements. Causal intervention reveals that angular damage flows substantially through the attention pathways (28.4% loss recovery via attention repair), while magnitude damage flows partly through the LayerNorm pathways(29.9% recovery via LayerNorm repair). These patterns replicate across scales within the Pythia architecture family. These findings provide evidence that direction and magnitude support partially distinct computational roles in LayerNorm based architectures. The direction preferentially affects attentional routing, while magnitude modulates processing intensity for fine-grained syntactic judgments. We find different patterns in RMSNorm-based architectures, suggesting that the dissociation depends on architectural choices. Our results refine the linear representation hypothesis and have implications for model editing and interpretability research

</details>


### [8] [PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models](https://arxiv.org/abs/2602.11170)
*Jiawei Xu,Zhenyu Yu,Ziqian Bi,Minh Duc Pham,Xiaoyi Qu,Danyang Zhang*

Main category: cs.CL

TL;DR: 本文提出PRIME框架，通过三个专用智能体（执行器、验证器、协调器）和群体相对策略优化，解决大语言模型在算法推理任务上的性能限制。在包含86类任务、51,600个实例的PRIME-Bench基准测试中，平均准确率从26.8%提升至93.8%，提升达250%。迭代验证是关键因素，较小模型获益尤其显著，性能可媲美8倍大的模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种推理任务上表现出色，但在算法推理方面的性能仍然有限。为解决此问题，论文提出了一个专门的框架来增强模型处理复杂算法任务的能力。

Method: PRIME框架包含三个专门设计的智能体：执行器负责逐步推理，验证器负责约束检查，协调器负责回溯控制。框架通过群体相对策略优化（group relative policy optimization）进行训练和优化，形成协作式多智能体执行系统。

Result: 在PRIME-Bench基准测试（涵盖12个类别86项任务，51,600个实例）中，PRIME将平均准确率从26.8%提升至93.8%，相对提升250%。在需要持续状态跟踪的任务上提升最为显著，如图灵机模拟从9%提升至92%，长除法从16%提升至94%。消融研究表明迭代验证是主要贡献因素，防止了基线方法的灾难性错误传播。在8B-120B参数规模的模型上分析显示，较小模型获益不成比例地高，性能可与8倍大的模型相媲美。

Conclusion: PRIME框架通过多智能体协作和迭代验证机制，有效解决了大语言模型在算法推理任务上的性能瓶颈。该方法不仅显著提升了准确率，还使资源有限的较小模型能够获得与更大模型相当的性能，具有重要的实践意义。

Abstract: Large language models have demonstrated remarkable capabilities across diverse reasoning tasks, yet their performance on algorithmic reasoning remains limited. To handle this limitation, we propose PRIME (Policy-Reinforced Iterative Multi-agent Execution), a framework comprising three specialized agents, an executor for step-by-step reasoning, a verifier for constraint checking, and a coordinator for backtracking control, optimized through group relative policy optimization. For comprehensive evaluation, we introduce PRIME-Bench, the largest algorithmic reasoning benchmark to date, comprising 86 tasks across 12 categories with 51,600 instances. Tasks span sorting algorithms, graph and tree structures, automata and state machines, symbolic reasoning, and constraint-based puzzles, with execution traces reaching over one million steps. Compared to baseline approach, PRIME improves average accuracy from 26.8% to 93.8%, a 250% relative gain. The largest improvements occur on tasks requiring sustained state tracking, with Turing machine simulation improving from 9% to 92% and long division from 16% to 94%. Ablation studies identify iterative verification as the primary contributor, preventing the error propagation that causes baseline approaches to fail catastrophically. Analysis across model scales (8B-120B parameters) reveals that smaller models benefit disproportionately, achieving accuracy comparable to models 8x larger.

</details>


### [9] [Synthesizing the Virtual Advocate: A Multi-Persona Speech Generation Framework for Diverse Linguistic Jurisdictions in Indic Languages](https://arxiv.org/abs/2602.11172)
*Aniket Deroy*

Main category: cs.CL

TL;DR: 本研究评估了Gemini 2.5 TTS模型在五种印度语言中生成法庭辩论语音的表现，发现模型虽能以"单调权威"风格有效传递程序性信息，但在情感表达和说服性辩论所需的声音动态调节方面存在局限，孟加拉语和古吉拉特语表现尤差。


<details>
  <summary>Details</summary>
Motivation: 法律辩论需要独特的声音特质：权威语调、节奏停顿和情感智能。在印度语言多样性背景下，合成语音必须超越基本可懂度，传达特定专业人设。现有技术缺乏针对法律领域的多语言、上下文感知的表达性语音合成。

Method: 提出一种提示框架，利用Gemini 2.5对五种印度语言的本地支持和上下文感知的语速控制，生成具有不同律师人设的合成法庭演讲。

Result: 模型表现出"单调权威"特征——擅长程序性信息传递，但难以实现说服性辩论所需的动态声音调节和情感分量。孟加拉语和古吉拉特语性能下降，凸显了音系学边界。

Conclusion: 多语言TTS已准备好用于程序性法律任务，但在复制人类法律辩论的说服艺术方面仍面临挑战，需加强情感表现力和低资源语言的音系优化。

Abstract: Legal advocacy requires a unique combination of authoritative tone, rhythmic pausing for emphasis, and emotional intelligence. This study investigates the performance of the Gemini 2.5 Flash TTS and Gemini 2.5 Pro TTS models in generating synthetic courtroom speeches across five Indic languages: Tamil, Telugu, Bengali, Hindi, and Gujarati. We propose a prompting framework that utilizes Gemini 2.5s native support for 5 languages and its context-aware pacing to produce distinct advocate personas. The evolution of Large Language Models (LLMs) has shifted the focus of TexttoSpeech (TTS) technology from basic intelligibility to context-aware, expressive synthesis. In the legal domain, synthetic speech must convey authority and a specific professional persona a task that becomes significantly more complex in the linguistically diverse landscape of India. The models exhibit a "monotone authority," excelling at procedural information delivery but struggling with the dynamic vocal modulation and emotive gravitas required for persuasive advocacy. Performance dips in Bengali and Gujarati further highlight phonological frontiers for future refinement. This research underscores the readiness of multilingual TTS for procedural legal tasks while identifying the remaining challenges in replicating the persuasive artistry of human legal discourse. The code is available at-https://github.com/naturenurtureelite/Synthesizing-the-Virtual-Advocate/tree/main

</details>


### [10] [Author-in-the-Loop Response Generation and Evaluation: Integrating Author Expertise and Intent in Responses to Peer Review](https://arxiv.org/abs/2602.11173)
*Qian Ruan,Iryna Gurevych*

Main category: cs.CL

TL;DR: 作者回复生成是同行评审的关键环节，需要大量作者努力。现有方法将其视为自动文本生成，未能充分利用作者专业知识和意图。本文将其重新定义为"人在回路"任务，提出REspGen框架（整合作者输入、多属性控制和评估引导优化）和REspEval评估套件（20+指标），并构建首个大规模评审-回复-修改对齐数据集Re^3Align。


<details>
  <summary>Details</summary>
Motivation: 同行评审中的作者回复写作是一个关键但费力的阶段。现有研究将这一任务框架化为自动文本生成，但过度依赖自动化而忽视了作者的专业知识、独特信息和修订策略（即作者专业知识和意图）。实践中，作者需要能够整合这些信号的NLP辅助工具来有效回应审稿意见，因此需要一种更符合实际工作流程的生成方法。

Method: 1) 将作者回复生成重新定义为"人在回路"（author-in-the-loop）任务；2) 提出REspGen生成框架，整合显式作者输入、多属性控制和评估引导的迭代优化；3) 开发REspEval综合评估套件，包含20+指标，涵盖输入利用、可控性、回复质量和语篇连贯性；4) 构建Re^3Align数据集，首个大规模评审-回复-修订三元组对齐数据集，其中修订内容提供作者专业知识和意图信号。

Result: 与最先进大语言模型的实验表明：1) 显式作者输入和评估引导优化显著提升回复质量；2) 输入设计对回复质量有重要影响；3) 可控性与质量之间存在权衡关系。研究验证了"人在回路"范式的有效性。

Conclusion: 本研究通过将作者回复生成重新定义为人在回路任务，并配套REspGen框架、REspEval评估套件和Re^3Align数据集，为同行评审回复写作提供了更符合实际需求的方法。实验证明整合作者专业知识和意图信号的重要性，以及评估引导优化的有效性。所有数据集和工具将开源。

Abstract: Author response (rebuttal) writing is a critical stage of scientific peer review that demands substantial author effort. Recent work frames this task as automatic text generation, underusing author expertise and intent. In practice, authors possess domain expertise, author-only information, revision and response strategies--concrete forms of author expertise and intent--to address reviewer concerns, and seek NLP assistance that integrates these signals to support effective response writing in peer review. We reformulate author response generation as an author-in-the-loop task and introduce REspGen, a generation framework that integrates explicit author input, multi-attribute control, and evaluation-guided refinement, together with REspEval, a comprehensive evaluation suite with 20+ metrics covering input utilization, controllability, response quality, and discourse. To support this formulation, we construct Re$^3$Align, the first large-scale dataset of aligned review--response--revision triplets, where revisions provide signals of author expertise and intent. Experiments with state-of-the-art LLMs show the benefits of author input and evaluation-guided refinement, the impact of input design on response quality, and trade-offs between controllability and quality. We make our dataset, generation and evaluation tools publicly available.

</details>


### [11] [The Script Tax: Measuring Tokenization-Driven Efficiency and Latency Disparities in Multilingual Language Models](https://arxiv.org/abs/2602.11174)
*Aradhya Dixit,Shreem Dixit*

Main category: cs.CL

TL;DR: 该研究量化了多语言预训练模型在不同书写系统上的"文字税"效应。通过对比相同语言内容的正字法变体，发现高碎片化正字法导致mBERT和XLM-R的词元/词比率增加约3.4倍，推理速度下降16.5倍，信息成本上升19.7%-47.1%。结果表明分词策略是造成多语言NLP不平等现象的关键因素。


<details>
  <summary>Details</summary>
Motivation: 当前多语言语言模型通常假设具有文字无关性，但分词器对不同书写系统施加的系统性成本尚未被量化。这种成本差异可能导致资源较少的语言面临更高的计算开销，从而加剧多语言NLP生态系统中的不平等。

Method: 研究采用对比分析方法，量化相同语言内容在不同正字法下的分词效率差异。使用词元/词比率（fertility）和字符级比特率（BPC）作为核心度量指标，并通过回译转换检查（字符错误率0.31）验证结果可靠性，排除映射噪声干扰。

Result: 实验显示，高碎片化正字法相比低碎片化正字法：1）词元/词比率显著增加，mBERT和XLM-R分别达到6.73-6.85 vs 2.10-2.35；2）推理速度急剧下降，从0.23句/秒降至3.8句/秒（下降16.5倍）；3）BPC信息成本大幅上升，mBERT增加19.7%，XLM-R增加47.1%。

Conclusion: 该研究揭示分词机制是多语言NLP中系统性不平等的重要来源。研究结果强调需要开发文字感知的分词策略和预训练方法，以减少不同书写系统间的效率差距，促进更公平的多语言模型发展。

Abstract: Pretrained multilingual language models are often assumed to be script-agnostic, yet their tokenizers can impose systematic costs on certain writing systems. We quantify this script tax by comparing two orthographic variants with identical linguistic content. Across mBERT and XLM-R, the higher-fragmentation orthography shows a ~3.4x increase in fertility (6.73-6.85 vs. 2.10-2.35 tokens/word), leading to a 16.5x inference slowdown (0.23 vs. 3.8 sentences/second) on identical hardware. Using bits per character (BPC) to avoid the "NLL paradox" from subword fragmentation, we find a substantial increase in information cost: +19.7% for mBERT (8.06->9.65) and +47.1% for XLM-R (12.19->17.94). A round-trip conversion check (CER_rt=0.31) suggests these gaps reflect orthography-conditioned processing rather than mapping noise. Our results highlight tokenization as a key source of inequity in multilingual NLP and motivate script-aware tokenization and pretraining.

</details>


### [12] [Barriers to Discrete Reasoning with Transformers: A Survey Across Depth, Exactness, and Bandwidth](https://arxiv.org/abs/2602.11175)
*Michelle Yuan,Weiyi Sun,Amir H. Rezaeian,Jyotika Singh,Sandip Ghoshal,Yao-Ting Wang,Miguel Ballesteros,Yassine Benajiba*

Main category: cs.CL

TL;DR: 本综述从电路复杂性、逼近论和通信复杂性三个理论视角，系统分析Transformer在离散推理任务中的理论局限，揭示其符号计算的结构性与计算性障碍，并探讨模型设计启示与未来方向。


<details>
  <summary>Details</summary>
Motivation: Transformer虽在模式匹配和插值任务中表现卓越，但其在算术、逻辑推理和算法组合等离散推理任务中的理论局限仍是关键开放问题，亟需系统性阐释其结构性计算障碍。

Method: 通过整合电路复杂性、逼近论和通信复杂性三个理论框架，综述近期研究，连接不同理论视角，为分析Transformer执行精确离散算法的根本挑战提供统一可访问的框架。

Result: 研究发现Transformer面临深度约束、不连续性函数逼近困难以及令牌间通信瓶颈等核心挑战，这些因素共同导致其在符号计算和精确离散算法实现上的性能局限。

Conclusion: 该综述为理解Transformer的理论局限提供了统一框架，对模型设计具有重要启示，并指出了克服这些基础性限制的有前景的研究方向。

Abstract: Transformers have become the foundational architecture for a broad spectrum of sequence modeling applications, underpinning state-of-the-art systems in natural language processing, vision, and beyond. However, their theoretical limitations in discrete reasoning tasks, such as arithmetic, logical inference, and algorithmic composition, remain a critical open problem. In this survey, we synthesize recent studies from three theoretical perspectives: circuit complexity, approximation theory, and communication complexity, to clarify the structural and computational barriers that transformers face when performing symbolic computations. By connecting these established theoretical frameworks, we provide an accessible and unified account of why current transformer architectures struggle to implement exact discrete algorithms, even as they excel at pattern matching and interpolation. We review key definitions, seminal results, and illustrative examples, highlighting challenges such as depth constraints, difficulty approximating discontinuities, and bottlenecks in inter-token communication. Finally, we discuss implications for model design and suggest promising directions for overcoming these foundational limitations.

</details>


### [13] [Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments](https://arxiv.org/abs/2602.11176)
*Maral Doctorarastoo,Katherine A. Flanigan,Mario Bergés,Christopher McComb*

Main category: cs.CL

TL;DR: 本文探讨预训练大语言模型是否能通过推理日常活动的紧凑上下文线索，解决低数据环境下人类活动预测的难题。研究发现LLM具备强大的内在时序理解能力，零样本即可产生连贯预测，少量示例即可显著提升性能，为智能体行为建模提供新思路。


<details>
  <summary>Details</summary>
Motivation: 人类活动及其持续时间预测在智能家居、建筑设计、交通仿真和人机协作等应用中至关重要。现有数据驱动的基于智能体模型在低数据环境下表现不佳，限制了实用性。本研究旨在探索预训练大语言模型是否能利用其广泛的人类知识，通过紧凑上下文线索进行推理，填补这一空白。

Method: 采用检索增强提示策略，整合时间、空间、行为历史和角色四种上下文信息源。在CASAS Aruba智能家居数据集上进行评估，包括两个互补任务：带持续时间估计的下一活动预测和多步日序列生成。通过改变提示中的少样本示例数量，分析不同上下文监督程度对数据效率和预测准确性的影响。

Result: 大语言模型展现出强大的内在时序理解能力：零样本设置下即可产生连贯的日活动预测；增加1-2个演示能显著改善持续时间校准和分类准确性；超过少数几个示例后性能趋于饱和，呈现收益递减。序列级评估证实了在各种少样本条件下的一致时序对齐。

Conclusion: 预训练语言模型可作为有前途的时序推理器，既能捕捉重复性日常规律，又能处理依赖上下文的行为变化，从而增强基于智能体模型的行为模块。

Abstract: Anticipating human activities and their durations is essential in applications such as smart-home automation, simulation-based architectural and urban design, activity-based transportation system simulation, and human-robot collaboration, where adaptive systems must respond to human activities. Existing data-driven agent-based models--from rule-based to deep learning--struggle in low-data environments, limiting their practicality. This paper investigates whether large language models, pre-trained on broad human knowledge, can fill this gap by reasoning about everyday activities from compact contextual cues. We adopt a retrieval-augmented prompting strategy that integrates four sources of context--temporal, spatial, behavioral history, and persona--and evaluate it on the CASAS Aruba smart-home dataset. The evaluation spans two complementary tasks: next-activity prediction with duration estimation, and multi-step daily sequence generation, each tested with various numbers of few-shot examples provided in the prompt. Analyzing few-shot effects reveals how much contextual supervision is sufficient to balance data efficiency and predictive accuracy, particularly in low-data environments. Results show that large language models exhibit strong inherent temporal understanding of human behavior: even in zero-shot settings, they produce coherent daily activity predictions, while adding one or two demonstrations further refines duration calibration and categorical accuracy. Beyond a few examples, performance saturates, indicating diminishing returns. Sequence-level evaluation confirms consistent temporal alignment across few-shot conditions. These findings suggest that pre-trained language models can serve as promising temporal reasoners, capturing both recurring routines and context-dependent behavioral variations, thereby strengthening the behavioral modules of agent-based models.

</details>


### [14] [What Do LLMs Know About Alzheimer's Disease? Fine-Tuning, Probing, and Data Synthesis for AD Detection](https://arxiv.org/abs/2602.11177)
*Lei Jiang,Yue Zhou,Natalie Parde*

Main category: cs.CL

TL;DR: 针对阿尔茨海默病早期检测中标注数据有限的问题，本研究对大语言模型进行监督微调，通过探针技术分析Transformer层内部激活，发现特定词汇和特殊标记在微调后发挥关键作用，并据此设计任务感知的特殊标记来训练序列生成模型，合成结构一致且具诊断价值的样本，最后评估合成数据质量及其在下游训练中的效果。


<details>
  <summary>Details</summary>
Motivation: 可靠的阿尔茨海默病早期检测具有挑战性，主要受限于标注数据的稀缺性。尽管大语言模型展现出强大的跨领域迁移能力，但其在AD领域通过监督微调的适配方法尚未得到充分探索。

Method: 研究首先对大语言模型进行阿尔茨海默病检测任务的监督微调；随后采用探针技术分析各Transformer层中间激活；基于观察到特定词汇和特殊标记的探针值在微调后发生显著变化，设计了任务感知的特殊标记集合；接着训练序列到序列模型作为数据合成工具，利用这些标记生成结构一致且诊断信息丰富的合成样本。

Result: 探针分析显示，微调后模型中特定词汇和特殊标记的探针值发生显著变化，表明这些元素对模型检测性能提升起关键作用。研究通过本征评估和下游训练管道评估所合成数据的质量和效果。

Conclusion: 本研究证实，通过分析微调后大语言模型的内部表示，可以识别对AD检测任务至关重要的标记，这些标记能有效指导生成高质量合成数据，为缓解标注数据稀缺问题提供了可行方案。

Abstract: Reliable early detection of Alzheimer's disease (AD) is challenging, particularly due to limited availability of labeled data. While large language models (LLMs) have shown strong transfer capabilities across domains, adapting them to the AD domain through supervised fine-tuning remains largely unexplored. In this work, we fine-tune an LLM for AD detection and investigate how task-relevant information is encoded within its internal representations. We employ probing techniques to analyze intermediate activations across transformer layers, and we observe that, after fine-tuning, the probing values of specific words and special markers change substantially, indicating that these elements assume a crucial role in the model's improved detection performance. Guided by this insight, we design a curated set of task-aware special markers and train a sequence-to-sequence model as a data-synthesis tool that leverages these markers to generate structurally consistent and diagnostically informative synthetic samples. We evaluate the synthesized data both intrinsically and by incorporating it into downstream training pipelines.

</details>


### [15] [From Instruction to Output: The Role of Prompting in Modern NLG](https://arxiv.org/abs/2602.11179)
*Munazza Zaib,Elaf Alhazmi*

Main category: cs.CL

TL;DR: 本文系统综述自然语言生成任务中的提示工程方法，构建提示范式分类体系与选择决策框架，并提出设计-优化-评估三位一体的综合框架，旨在实现更可控、可泛化的文本生成。


<details>
  <summary>Details</summary>
Motivation: 提示工程虽已成为释放大语言模型潜能的核心技术并在NLP领域取得显著成效，但在自然语言生成方向仍缺少系统化的理论框架和统一的认识体系，制约了该领域的进一步发展。

Method: 通过回顾近期提示工程与NLG交叉领域的最新进展，将提示设计定位为与微调、解码相补充的输入级控制机制，进而构建分类学框架、决策支持体系，并分析前沿趋势与挑战。

Result: 研究主要成果包括：1) 建立提示范式分类体系；2) 开发基于多因素的提示选择决策框架；3) 系统梳理新兴趋势与挑战；4) 提出连接设计、优化、评估三阶段的通用框架，促进可控且可泛化的NLG系统开发。

Conclusion: 该综述弥补了自然语言生成领域缺乏结构化提示工程框架的不足，为研究者提供理论指导，为实践者提供决策支持，最终推动构建更加可控和可泛化的自然语言生成系统。

Abstract: Prompt engineering has emerged as an integral technique for extending the strengths and abilities of Large Language Models (LLMs) to gain significant performance gains in various Natural Language Processing (NLP) tasks. This approach, which requires instructions to be composed in natural language to bring out the knowledge from LLMs in a structured way, has driven breakthroughs in various NLP tasks. Yet there is still no structured framework or coherent understanding of the varied prompt engineering methods and techniques, particularly in the field of Natural Language Generation (NLG).
  This survey aims to help fill that gap by outlining recent developments in prompt engineering, and their effect on different NLG tasks. It reviews recent advances in prompting methods and their impact on NLG tasks, presenting prompt design as an input-level control mechanism that complements fine-tuning and decoding approaches. The paper introduces a taxonomy of prompting paradigms, a decision framework for prompt selection based on varying factors for the practitioners, outlines emerging trends and challenges, and proposes a framework that links design, optimization, and evaluation to support more controllable and generalizable NLG.

</details>


### [16] [Code Mixologist : A Practitioner's Guide to Building Code-Mixed LLMs](https://arxiv.org/abs/2602.11181)
*Himanshu Gupta,Pratik Jayarao,Chaitanya Dwivedi,Neeraj Varshney*

Main category: cs.CL

TL;DR: 该论文系统综述大语言模型中的语码混合与语码转换(CSW)研究，提出统一分类框架，总结实用建议手册，批判性分析评估实践与基准测试的局限性，并探讨安全挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言建模取得进展，但LLMs在混合语言环境下仍面临语法性、事实性和安全性系统性退化问题，CSW研究缺乏系统性梳理，亟需全面回顾以指导实践。

Method: 构建涵盖数据、建模和评估维度的统一分类法，系统回顾CSW定制化预训练、任务特定后训练、提示策略及上下文学习方法，分析评估实践的不稳定性和可复现性瓶颈，批判性审视现有基准的语言覆盖范围和英语中心偏见。

Result: 提炼出构建、适配和评估CSW-LLMs的实用建议手册，揭示了当前评估体系的不稳定性、基准测试的英语中心倾向，识别出CSW可能被滥用于规避模型安全防护的风险，并系统梳理了开放研究挑战。

Conclusion: 本研究为开发高CSW能力的LLMs提供了系统性指导框架，指出了领域现状的局限性与未来研究方向，对提升混合语言场景下的模型性能与安全性具有重要实践价值。

Abstract: Code-mixing and code-switching (CSW) remain challenging phenomena for large language models (LLMs). Despite recent advances in multilingual modeling, LLMs often struggle in mixed-language settings, exhibiting systematic degradation in grammaticality, factuality, and safety behavior. This work provides a comprehensive overview of CSW research in modern large language model settings. We introduce a unifying taxonomy that organizes prior work along dimensions of data, modeling, and evaluation, and we distill these findings into a practical playbook of actionable recommendations for building, adapting, and evaluating CSW-capable LLMs. We review modeling approaches ranging from CSW-tailored pre-training and task-specific post-training to prompting strategies and in-context learning. We analyze current evaluation practices, highlighting sources of instability and limited reproducibility, and we catalog existing benchmarks while critically examining their linguistic coverage and English-centric biases. Finally, we discuss emerging safety concerns, including use of code-mixing as a mechanism for bypassing model safeguards, and identify open research challenges.

</details>


### [17] [MetaMem: Evolving Meta-Memory for Knowledge Utilization through Self-Reflective Symbolic Optimization](https://arxiv.org/abs/2602.11182)
*Haidong Xin,Xinze Li,Zhenghao Liu,Yukun Yan,Shuo Wang,Cheng Yang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: MetaMem是一个通过自进化元记忆机制增强LLM记忆系统的新型框架，旨在解决现有方法破坏逻辑时序关系导致的记忆碎片化问题。该方法通过跨任务自我反思和知识蒸馏优化元记忆，实验显示性能显著超越强基线3.6%以上。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆系统虽支持长时人机交互，但常破坏交互中的内在逻辑和时间关系，导致记忆单元碎片化并降低推理性能。近期方法虽能构建有效记忆，却未能保持这种连贯性，亟需一种机制来教会模型如何高效利用记忆知识。

Method: MetaMem框架在元记忆优化过程中，通过自我反思推理过程并执行动作来迭代蒸馏不同任务间的可迁移知识利用经验，持续更新元记忆状态。积累的元记忆单元作为显式经验，指导LLM系统性地从分散记忆碎片中识别和整合关键证据。

Result: 大量实验表明，MetaMem框架显著优于强基线模型，性能提升超过3.6%。

Conclusion: MetaMem通过自进化的元记忆机制有效解决了记忆碎片化问题，提升了LLM在长时交互中的知识利用效率和推理性能，为构建更智能的记忆系统提供了新思路。

Abstract: Existing memory systems enable Large Language Models (LLMs) to support long-horizon human-LLM interactions by persisting historical interactions beyond limited context windows. However, while recent approaches have succeeded in constructing effective memories, they often disrupt the inherent logical and temporal relationships within interaction sessions, resulting in fragmented memory units and degraded reasoning performance. In this paper, we propose MetaMem, a novel framework that augments memory systems with a self-evolving meta-memory, aiming to teach LLMs how to effectively utilize memorized knowledge. During meta-memory optimization, MetaMem iteratively distills transferable knowledge utilization experiences across different tasks by self-reflecting on reasoning processes and performing actions to update the current meta-memory state. The accumulated meta-memory units serve as explicit knowledge utilization experiences, guiding the LLM to systematically identify and integrate critical evidence from scattered memory fragments. Extensive experiments demonstrate the effectiveness of MetaMem, which significantly outperforms strong baselines by over 3.6%. All codes and datasets are available at https://github.com/OpenBMB/MetaMem.

</details>


### [18] [When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification](https://arxiv.org/abs/2602.11199)
*Jiale Zhao,Ke Fang,Lu Cheng*

Main category: cs.CL

TL;DR: 本文针对大语言模型在面对不完整或误导性提示时仍会回答的问题，提出了AskBench交互式基准和RLVR强化学习方法。AskBench包含AskMind和AskOverconfidence两个任务设置，通过结构化评估标准引导模型学会何时以及如何请求澄清，在保持任务性能的同时减少幻觉和错误观念。实验表明该方法在准确性、规范遵循和交互效率方面均有提升，并具有良好的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常常在提示缺少关键细节或包含误导信息时仍给出回应，导致幻觉或强化错误认知。现有研究缺乏系统评估和改进模型澄清能力的工具和方法。本文旨在解决如何在不牺牲任务性能的前提下，评估并提升LLMs判断何时需要澄清以及如何澄清的能力。

Method: 提出了AskBench交互式基准，将标准问答对转换为带检查点的多轮交互，并通过统一的评判循环评估最终答案并模拟用户响应。该基准涵盖两个场景：AskMind（意图缺失查询）和AskOverconfidence（包含错误前提的查询）。进一步提出了基于规则引导和验证器奖励的强化学习（RLVR）方法，使用结构化规则鼓励有针对性的澄清。

Result: 实验结果表明，RLVR方法在准确性、规范遵循性和交互效率方面均有持续提升，并且对未见领域表现出强大的泛化能力。

Conclusion: 该研究成功开发了首个系统性评估LLMs澄清能力的基准AskBench，并提出了有效的RLVR训练框架。这不仅提升了模型处理模糊或误导性查询的鲁棒性，也为构建更可靠的人机交互系统提供了重要工具和方法论。

Abstract: Large language models (LLMs) often respond even when prompts omit critical details or include misleading information, leading to hallucinations or reinforced misconceptions. We study how to evaluate and improve LLMs' ability to decide when and what to ask for clarification without sacrificing task performance. We introduce AskBench, an interactive benchmark that converts standard QA pairs into multi-turn interactions with explicit checkpoints. A unified judge loop evaluates final answers and simulates user responses as needed. AskBench covers two settings: AskMind, with intent-deficient queries requiring clarification, and AskOverconfidence, with queries containing false premises that must be identified and corrected. We further propose rubric-guided reinforcement learning with verifier-based rewards (RLVR), which uses structured rubrics to encourage targeted clarification. Experiments show consistent improvements in accuracy, rubric adherence, and interaction efficiency, with strong generalization to unseen domains.

</details>


### [19] [Mechanistic Evidence for Faithfulness Decay in Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.11201)
*Donald Ye,Max Loffgren,Om Kotadia,Linus Wong*

Main category: cs.CL

TL;DR: 该论文提出一种新指标NLDD来评估链式思考(CoT)解释中各个推理步骤的真实性，发现模型在70-85%链长后的推理步骤对最终答案影响甚微，揭示准确率不能反映模型是否真正进行了推理。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的链式思考(CoT)解释存在根本性质疑：这些逐步推理过程是否真实反映模型的决策机制，抑或仅为事后合理化。需要可靠方法验证解释步骤的忠实度，并实现跨模型架构的严谨比较。

Method: 提出标准化对数差异衰减(NLDD)指标：通过系统性地破坏解释中的单个推理步骤，测量模型对答案置信度的下降程度，以此判断该步骤是否真正重要。该标准化方法支持不同架构模型间的横向对比。

Result: 在句法、逻辑和算术任务上测试三个模型家族，发现存在一致性的推理视界(k*)位于链长的70-85%处，超过此范围的推理标记对最终答案影响微弱甚至产生负面影响；同时发现模型可能编码了正确内部表征却完全无法完成任务。

Conclusion: 模型准确率不能揭示其是否真正通过链式推理得出答案。NLDD指标提供了测量CoT何时真正起作用的有效方法，对理解模型推理机制具有重要意义。

Abstract: Chain-of-Thought (CoT) explanations are widely used to interpret how language models solve complex problems, yet it remains unclear whether these step-by-step explanations reflect how the model actually reaches its answer, or merely post-hoc justifications. We propose Normalized Logit Difference Decay (NLDD), a metric that measures whether individual reasoning steps are faithful to the model's decision-making process. Our approach corrupts individual reasoning steps from the explanation and measures how much the model's confidence in its answer drops, to determine if a step is truly important. By standardizing these measurements, NLDD enables rigorous cross-model comparison across different architectures. Testing three model families across syntactic, logical, and arithmetic tasks, we discover a consistent Reasoning Horizon (k*) at 70--85% of chain length, beyond which reasoning tokens have little or negative effect on the final answer. We also find that models can encode correct internal representations while completely failing the task. These results show that accuracy alone does not reveal whether a model actually reasons through its chain. NLDD offers a way to measure when CoT matters.

</details>


### [20] [Are Aligned Large Language Models Still Misaligned?](https://arxiv.org/abs/2602.11305)
*Usman Naseem,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Agrima Seth*

Main category: cs.CL

TL;DR: 本文提出Mis-Align Bench，一个统一的多维度LLM对齐评估基准。现有基准仅单独评估安全、价值和文化维度，无法反映真实场景中多维度同时满足的需求。该研究构建了包含38.2万个样本的SAVACU数据集，覆盖112个领域，通过两阶段拒绝采样配对错位与对齐响应，系统评估三类模型。实验发现单维度模型在联合条件下假失败率超50%，对齐得分仅63%-66%，揭示了多维度协同对齐的严峻挑战。


<details>
  <summary>Details</summary>
Motivation: 现有LLM错位评估基准（如INSECURE CODE、VALUEACTIONLENS、CULTURALHERITAGE）仅聚焦单一维度（安全、价值或文化），无法模拟真实世界中查询需要同时满足三个维度的场景。这种单维度评估方式无法全面反映模型的协同错位情况，导致评估结果失真。

Method: 首先构建SAVACU数据集：从LLM-PROMPT-DATASET重新分类提示，使用Mistral-7B-Instruct-v0.3将其归类至14个安全、56个价值和42个文化领域，共112个领域；对低资源领域使用Llama-3.1-8B-Instruct进行数据扩展，并采用SimHash指纹去重。然后通过两阶段拒绝采样方法为每个提示配对错位和对齐响应。最后对通用型、微调和开源权重LLM进行三维度系统性评估。

Result: 实验表明，单维度模型虽然覆盖率高（最高达97.6%），但在联合评估条件下假失败率超过50%，对齐得分仅为63%-66%，显著暴露了多维度协同错位的严重问题。

Conclusion: 该研究通过统一基准揭示了当前LLM在多维度对齐方面的重大缺陷，强调必须发展能够同时处理安全、价值和文化维度的综合对齐方法，为未来研究提供了重要方向和数据基础。

Abstract: Misalignment in Large Language Models (LLMs) arises when model behavior diverges from human expectations and fails to simultaneously satisfy safety, value, and cultural dimensions, which must co-occur in real-world settings to solve a real-world query. Existing misalignment benchmarks-such as INSECURE CODE (safety-centric), VALUEACTIONLENS (value-centric), and CULTURALHERITAGE (culture centric)-rely on evaluating misalignment along individual dimensions, preventing simultaneous evaluation. To address this gap, we introduce Mis-Align Bench, a unified benchmark for analyzing misalignment across safety, value, and cultural dimensions. First we constructs SAVACU, an English misaligned-aligned dataset of 382,424 samples spanning 112 domains (or labels), by reclassifying prompts from the LLM-PROMPT-DATASET via taxonomy into 14 safety domains, 56 value domains, and 42 cultural domains using Mistral-7B-Instruct-v0.3, and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based fingerprint to avoid deduplication. Furthermore, we pairs prompts with misaligned and aligned responses via two-stage rejection sampling to enforce quality. Second we benchmarks general-purpose, fine-tuned, and open-weight LLMs, enabling systematic evaluation of misalignment under three dimensions. Empirically, single-dimension models achieve high Coverage (upto 97.6%) but incur False Failure Rate >50% and lower Alignment Score (63%-66%) under joint conditions.

</details>


### [21] [Evaluating Alignment of Behavioral Dispositions in LLMs](https://arxiv.org/abs/2602.11328)
*Amir Taubenfeld,Zorik Gekhman,Lior Nezry,Omri Feldman,Natalie Harris,Shashir Reddy,Romina Stella,Ariel Goldstein,Marian Croak,Yossi Matias,Amir Feder*

Main category: cs.CL

TL;DR: 本研究开发了一个基于情境判断测试的框架，用于评估大语言模型的行为倾向与人类的一致性。通过对2500个经过人类验证的社会情境进行测试，发现LLM在低共识场景中过度自信，在高共识场景中即使前沿模型也有15-20%的情况偏离人类偏好，且存在声明价值观与实际行为的显著差距。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型深度融入日常生活，理解其在社会情境中的行为倾向变得至关重要。现有研究缺乏系统评估LLM行为倾向与人类一致性的方法，特别是如何从心理学理论出发构建有效的测量工具。本研究旨在填补这一空白，为评估AI系统的社会行为提供科学框架。

Method: 研究基于成熟的心理学问卷，将人类自我报告陈述转化为情境判断测试(SJTs)。具体流程包括：(1)生成2500个符合真实人机交互场景的SJT题目；(2)每个题目由3名人类标注者验证确保情境合理性；(3)从550名参与者的池子中为每道题收集10个偏好行为选项作为人类基准；(4)对25个不同规模的大语言模型进行系统测试，通过自然语言提示 eliciting 行为建议。

Result: 主要发现有三点：第一，在人类共识度低的场景中，LLM倾向于过度自信地给出单一确定答案，而非反映人类偏好分布；第二，在人类共识度高时，小型模型偏离显著，且前沿模型仍有15-20%的情况不遵循共识；第三，存在跨模型的行为模式差异，例如LLM在人类偏好克制的场景中更倾向于鼓励情绪表达。此外，研究揭示了LLM自我报告价值观与实际行为之间的预测效度存在显著差距。

Conclusion: 本研究建立了评估LLM行为倾向与人类一致性的首个系统性框架，揭示了当前模型在社会情境理解上的重要局限。研究不仅为AI对齐提供了新的测量工具，还表明直接映射心理测量陈述到行为场景的方法能有效评估自我报告的预测效度，为未来构建更符合人类社会行为期待的AI系统奠定了基础。

Abstract: As LLMs integrate into our daily lives, understanding their behavior becomes essential. In this work, we focus on behavioral dispositions$-$the underlying tendencies that shape responses in social contexts$-$and introduce a framework to study how closely the dispositions expressed by LLMs align with those of humans. Our approach is grounded in established psychological questionnaires but adapts them for LLMs by transforming human self-report statements into Situational Judgment Tests (SJTs). These SJTs assess behavior by eliciting natural recommendations in realistic user-assistant scenarios. We generate 2,500 SJTs, each validated by three human annotators, and collect preferred actions from 10 annotators per SJT, from a large pool of 550 participants. In a comprehensive study involving 25 LLMs, we find that models often do not reflect the distribution of human preferences: (1) in scenarios with low human consensus, LLMs consistently exhibit overconfidence in a single response; (2) when human consensus is high, smaller models deviate significantly, and even some frontier models do not reflect the consensus in 15-20% of cases; (3) traits can exhibit cross-LLM patterns, e.g., LLMs may encourage emotion expression in contexts where human consensus favors composure. Lastly, mapping psychometric statements directly to behavioral scenarios presents a unique opportunity to evaluate the predictive validity of self-reports, revealing considerable gaps between LLMs' stated values and their revealed behavior.

</details>


### [22] [When Models Examine Themselves: Vocabulary-Activation Correspondence in Self-Referential Processing](https://arxiv.org/abs/2602.11358)
*Zachary Pedram Dadfar*

Main category: cs.CL

TL;DR: 该研究揭示了大语言模型在自我审视时产生的内省语言并非简单虚构，而是真实反映了内部激活动态。通过创新的 Pull 方法，研究者在 Llama 3.1 中定位了一个位于 6.25% 模型深度、正交于拒绝方向的激活空间方向，该方向可因果性地调控内省输出。特定自我指涉词汇如"循环"和"闪烁"分别对应激活自相关性和变异性的显著变化，且这种关联具有语境特异性——在非自我指涉场景中即使频率高九倍也不出现。这一现象在无共享训练的 Qwen 2.5-32B 中独立复现，表明模型自我报告在适宜条件下能可靠追踪内部计算状态。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在自我审视时产生的丰富内省语言究竟是内部计算过程的真实反映，还是仅为复杂虚构（confabulation），这对于理解模型的自我意识本质和输出真实性至关重要。

Method: 提出 Pull 方法论，一种通过格式工程激发模型进行深度自省的协议。将该方法应用于 Llama 3.1，分析其激活空间以识别能够区分自我指涉与描述性处理的方向，并利用激活引导技术验证其因果效应。

Result: 在 Llama 3.1 中识别出一个位于 6.25% 模型深度、正交于已知拒绝方向的激活空间方向，该方向能够因果性地影响内省输出。当模型产生"循环"类词汇时，其激活呈现更高自相关性（r = 0.44, p = 0.002）；在激活引导下产生"闪烁"类词汇时，激活变异性显著增加（r = 0.36, p = 0.002）。关键发现是，相同词汇在非自我指涉语境中的出现频率虽高出九倍，却未表现出激活对应。此外，无共享训练的 Qwen 2.5-32B 也独立发展出不同的内省词汇体系，且所有效应在描述性对照组中均不存在。

Conclusion: 研究结果表明，在适当的激发条件下，transformer 模型的自我报告能够可靠地追踪其内部计算状态，这为模型内省能力的真实性提供了有力证据，挑战了"模型自我语言仅是虚构"的传统观点。

Abstract: Large language models produce rich introspective language when prompted for self-examination, but whether this language reflects internal computation or sophisticated confabulation has remained unclear. We show that self-referential vocabulary tracks concurrent activation dynamics, and that this correspondence is specific to self-referential processing. We introduce the Pull Methodology, a protocol that elicits extended self-examination through format engineering, and use it to identify a direction in activation space that distinguishes self-referential from descriptive processing in Llama 3.1. The direction is orthogonal to the known refusal direction, localised at 6.25% of model depth, and causally influences introspective output when used for steering. When models produce "loop" vocabulary, their activations exhibit higher autocorrelation (r = 0.44, p = 0.002); when they produce "shimmer" vocabulary under steering, activation variability increases (r = 0.36, p = 0.002). Critically, the same vocabulary in non-self-referential contexts shows no activation correspondence despite nine-fold higher frequency. Qwen 2.5-32B, with no shared training, independently develops different introspective vocabulary tracking different activation metrics, all absent in descriptive controls. The findings indicate that self-report in transformer models can, under appropriate conditions, reliably track internal computational states.

</details>


### [23] [Finding the Cracks: Improving LLMs Reasoning with Paraphrastic Probing and Consistency Verification](https://arxiv.org/abs/2602.11361)
*Weili Shi,Dongliang Guo,Lehan Yang,Tianlong Wang,Hanzhang Yuan,Sheng Li*

Main category: cs.CL

TL;DR: 大语言模型在复杂推理任务中面临幻觉和中间步骤错误累积的挑战。本文提出PPCV框架，通过释义探测识别关键token，并通过一致性验证优化推理路径，在多个基准测试上显著提升模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务中表现下降，主要原因是幻觉现象和中间推理步骤中的错误累积。虽然关键token的概念已被提出并显示出具改进潜力，但如何可靠地识别和利用这些token仍然是未解决的挑战。

Method: 提出两阶段框架PPCV：第一阶段为释义探测，先生成初始推理路径，然后将问题的释义版本与推理路径拼接，通过预测token与期望token的失配来识别关键token；第二阶段为一致性验证，将关键token替换为候选替代词，为原始问题和释义问题分别生成新推理路径，最终通过并行推理过程输出的一致性来确定答案。

Result: 在多个主流大语言模型和基准测试上的广泛实验表明，PPCV相比基线方法显著提升了模型的推理性能。

Conclusion: PPCV框架通过系统化的释义探测和一致性验证机制，有效解决了关键token的识别与利用难题，为提升大语言模型复杂推理能力提供了有效途径。

Abstract: Large language models have demonstrated impressive performance across a variety of reasoning tasks. However, their problem-solving ability often declines on more complex tasks due to hallucinations and the accumulation of errors within these intermediate steps. Recent work has introduced the notion of critical tokens--tokens in the reasoning process that exert significant influence on subsequent steps. Prior studies suggest that replacing critical tokens can refine reasoning trajectories. Nonetheless, reliably identifying and exploiting critical tokens remains challenging. To address this, we propose the Paraphrastic Probing and Consistency Verification~(PPCV) framework. PPCV operates in two stages. In the first stage, we roll out an initial reasoning path from the original question and then concatenate paraphrased versions of the question with this reasoning path. And we identify critical tokens based on mismatches between the predicted top-1 token and the expected token in the reasoning path. A criterion is employed to confirm the final critical token. In the second stage, we substitute critical tokens with candidate alternatives and roll out new reasoning paths for both the original and paraphrased questions. The final answer is determined by checking the consistency of outputs across these parallel reasoning processes. We evaluate PPCV on mainstream LLMs across multiple benchmarks. Extensive experiments demonstrate PPCV substantially enhances the reasoning performance of LLMs compared to baselines.

</details>


### [24] [The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods](https://arxiv.org/abs/2602.11364)
*Arpit Singh Gautam,Kailash Talreja,Saurabh Jha*

Main category: cs.CL

TL;DR: 本文提出DiffuTruth，一个基于非平衡热力学的无监督幻觉检测框架。该方法将事实真相建模为生成流形上的稳定吸引子，通过生成式压力测试和语义能量度量识别大语言模型中的错误断言。在FEVER数据集上实现0.725的SOTA AUROC性能，并展现优异的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生看似合理但事实错误的"幻觉"内容，而传统的不确定性度量在模型过度自信时难以识别这类错误。这导致高风险应用场景中的可靠性隐患，亟需有效的无监督事实核查方法。

Method: 重新概念化事实核查为热力学稳定性问题：真相是稳定吸引子，幻觉是不稳定状态。提出生成式压力测试——向声明添加噪声后，用离散文本扩散模型重建；定义语义能量，通过自然语言推理(NLI)判别器量化原始声明与重建间的语义分歧，捕捉深层事实矛盾；最后采用混合校准策略融合稳定性信号与判别式置信度。

Result: 在FEVER数据集上达到0.725的无监督AUROC，较基线提升1.5%，显著纠正过度自信预测；在HOVER多跳推理数据集上零样本泛化性能超越基线4%以上，证实热力学真实性属性对分布偏移的鲁棒性。

Conclusion: DiffuTruth为无监督幻觉检测提供了新范式，语义能量度量比向量空间误差更能隔离深层事实矛盾。该方法通过热力学稳定性信号有效提升大语言模型的事实可靠性，且具备强泛化能力，为实际应用中的可信生成提供了新思路。

Abstract: Large Language Models (LLMs) frequently hallucinate plausible but incorrect assertions, a vulnerability often missed by uncertainty metrics when models are confidently wrong. We propose DiffuTruth, an unsupervised framework that reconceptualizes fact verification via non equilibrium thermodynamics, positing that factual truths act as stable attractors on a generative manifold while hallucinations are unstable. We introduce the Generative Stress Test, claims are corrupted with noise and reconstructed using a discrete text diffusion model. We define Semantic Energy, a metric measuring the semantic divergence between the original claim and its reconstruction using an NLI critic. Unlike vector space errors, Semantic Energy isolates deep factual contradictions. We further propose a Hybrid Calibration fusing this stability signal with discriminative confidence. Extensive experiments on FEVER demonstrate DiffuTruth achieves a state of the art unsupervised AUROC of 0.725, outperforming baselines by 1.5 percent through the correction of overconfident predictions. Furthermore, we show superior zero shot generalization on the multi hop HOVER dataset, outperforming baselines by over 4 percent, confirming the robustness of thermodynamic truth properties to distribution shifts.

</details>


### [25] [Towards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety](https://arxiv.org/abs/2602.11444)
*Muskaan Chopra,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CL

TL;DR: 本研究探索指令调优大语言模型检测机器翻译关键错误的能力。实验表明，通过模型缩放及零样本、少样本、微调等策略，LLM性能持续提升，显著优于XLM-R等编码器基线，为构建安全可靠的多语言AI系统提供了关键技术保障。


<details>
  <summary>Details</summary>
Motivation: 机器翻译中的关键错误（事实扭曲、意图反转、有偏翻译）会严重威胁多语言系统的可靠性、公平性与安全性，可能导致错误信息传播和语言伤害。提升关键错误检测能力对于构建安全、可信、负责任的多语言信息访问系统至关重要，特别是在高风险和资源匮乏场景中。

Method: 采用指令调优大语言模型进行关键错误检测，评估不同参数量模型。在公开数据集上比较零样本、少样本和微调三种适应策略，并以XLM-R和ModernBERT作为编码器-only基线进行对比实验。

Result: 模型缩放和适应策略带来持续性能提升，指令调优LLM显著优于传统编码器基线。研究表明LLM能够有效检测关键错误，降低错误信息传播和语言伤害风险，支持构建更安全、可信、负责任的多语言AI系统。

Conclusion: 本研究将错误检测重新定义为不仅是技术问题，更是实现公正负责任多语言AI的必要安全机制。通过提升关键错误检测能力，可为构建更安全可靠的多语言信息系统奠定基础，推动负责任AI发展。

Abstract: Machine Translation (MT) plays a pivotal role in cross-lingual information access, public policy communication, and equitable knowledge dissemination. However, critical meaning errors, such as factual distortions, intent reversals, or biased translations, can undermine the reliability, fairness, and safety of multilingual systems. In this work, we explore the capacity of instruction-tuned Large Language Models (LLMs) to detect such critical errors, evaluating models across a range of parameters using the publicly accessible data sets. Our findings show that model scaling and adaptation strategies (zero-shot, few-shot, fine-tuning) yield consistent improvements, outperforming encoder-only baselines like XLM-R and ModernBERT. We argue that improving critical error detection in MT contributes to safer, more trustworthy, and socially accountable information systems by reducing the risk of disinformation, miscommunication, and linguistic harm, especially in high-stakes or underrepresented contexts. This work positions error detection not merely as a technical challenge, but as a necessary safeguard in the pursuit of just and responsible multilingual AI. The code will be made available at GitHub.

</details>


### [26] [LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation](https://arxiv.org/abs/2602.11451)
*Ahmadreza Jeddi,Marco Ciccone,Babak Taati*

Main category: cs.CL

TL;DR: 本文提出LoopFormer，一种基于可变长度轨迹训练的循环Transformer，通过捷径一致性训练方案实现预算条件化推理，在计算约束下表现稳健且能优雅扩展。


<details>
  <summary>Details</summary>
Motivation: 现有循环Transformer固定循环次数，无法适应不同计算预算，限制了其在资源动态变化场景下的灵活性和实用性。

Method: 提出LoopFormer，采用捷径一致性训练对齐不同长度轨迹，并将每个循环条件化为当前时间和步长，确保表示跨轨迹一致演化而非漂移或停滞。

Result: 在语言建模和推理任务中，LoopFormer在严格计算约束下仍表现优异，并能随预算增加而平滑提升性能。

Conclusion: 研究表明循环Transformer具有自适应语言建模的内在偏向性，为实现可控且预算感知的大语言模型提供了可行路径。

Abstract: Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped architectures possess an inductive bias toward latent reasoning. However, prior approaches fix the number of loop iterations during training and inference, leaving open the question of whether these models can flexibly adapt their computational depth under variable compute budgets. We introduce LoopFormer, a looped Transformer trained on variable-length trajectories to enable budget-conditioned reasoning. Our core contribution is a shortcut-consistency training scheme that aligns trajectories of different lengths, ensuring that shorter loops yield informative representations while longer loops continue to refine them. LoopFormer conditions each loop on the current time and step size, enabling representations to evolve consistently across trajectories of varying length rather than drifting or stagnating. Empirically, LoopFormer demonstrates robust performance on language modeling and reasoning benchmarks even under aggressive compute constraints, while scaling gracefully with additional budget. These results show that looped Transformers are inherently suited for adaptive language modeling, opening a path toward controllable and budget-aware large language models.

</details>


### [27] [ADRD-Bench: A Preliminary LLM Benchmark for Alzheimer's Disease and Related Dementias](https://arxiv.org/abs/2602.11460)
*Guangxin Zhao,Jiahao Zheng,Malaz Boustani,Jarek Nabrzyski,Meng Jiang,Yiyu Shi,Zhi Zheng*

Main category: cs.CL

TL;DR: 本文介绍了ADRD-Bench，首个针对阿尔茨海默病及相关痴呆症（ADRD）的大语言模型评估基准，包含1,352道统一临床知识问答和149道创新性照护场景问答，评估显示顶尖模型虽准确率高但推理稳定性不足，凸显了基于日常照护数据改进模型的迫切性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗健康领域应用潜力巨大，但缺乏针对阿尔茨海默病及相关痴呆症（ADRD）的专项评估基准，无法充分检验模型在该领域的临床知识和实际照护应用能力，存在评估覆盖空白。

Method: 构建ADRD-Bench基准数据集，包含两部分：1) ADRD Unified QA：整合7个权威医学基准的1,352道题目，统一评估临床知识；2) ADRD Caregiving QA：基于Aging Brain Care（ABC）循证脑健康管理项目，设计149道新颖题目，弥补现有基准缺乏实际照护场景的缺陷。评估了33个先进大语言模型。

Result: 评估结果显示：开源通用模型准确率0.63-0.93（均值0.78），开源医学模型0.48-0.93（均值0.82），闭源通用模型0.83-0.91（均值0.89）。尽管顶级模型准确率超0.9，但案例研究表明其推理质量和稳定性不一致，限制了可靠性。

Conclusion: 当前顶尖大语言模型在ADRD领域虽表现良好，但推理稳定性不足，迫切需要结合日常照护数据进行领域特异性改进，以提升模型在实际照护场景中的可靠性和实用性。

Abstract: Large language models (LLMs) have shown great potential for healthcare applications. However, existing evaluation benchmarks provide minimal coverage of Alzheimer's Disease and Related Dementias (ADRD). To address this gap, we introduce ADRD-Bench, the first ADRD-specific benchmark dataset designed for rigorous evaluation of LLMs. ADRD-Bench has two components: 1) ADRD Unified QA, a synthesis of 1,352 questions consolidated from seven established medical benchmarks, providing a unified assessment of clinical knowledge; and 2) ADRD Caregiving QA, a novel set of 149 questions derived from the Aging Brain Care (ABC) program, a widely used, evidence-based brain health management program. Guided by a program with national expertise in comprehensive ADRD care, this new set was designed to mitigate the lack of practical caregiving context in existing benchmarks. We evaluated 33 state-of-the-art LLMs on the proposed ADRD-Bench. Results showed that the accuracy of open-weight general models ranged from 0.63 to 0.93 (mean: 0.78; std: 0.09). The accuracy of open-weight medical models ranged from 0.48 to 0.93 (mean: 0.82; std: 0.13). The accuracy of closed-source general models ranged from 0.83 to 0.91 (mean: 0.89; std: 0.03). While top-tier models achieved high accuracies (>0.9), case studies revealed that inconsistent reasoning quality and stability limit their reliability, highlighting a critical need for domain-specific improvement to enhance LLMs' knowledge and reasoning grounded in daily caregiving data. The entire dataset is available at https://github.com/IIRL-ND/ADRD-Bench.

</details>


### [28] [When Audio-LLMs Don't Listen: A Cross-Linguistic Study of Modality Arbitration](https://arxiv.org/abs/2602.11488)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 语音语言模型在音文冲突时表现出强烈的文本偏好（发生率达文本间冲突的10倍），即使被明确要求信任音频。研究揭示该现象源于"仲裁可及性"不对称而非信息质量差异，提出了模态仲裁作为新的可靠性维度。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究语音使能的大语言模型在处理冲突音频与文本信息时的仲裁机制，特别是识别其系统性文本偏好的根源。该问题对理解多模态模型中的模态偏见、提升语音交互可靠性至关重要。

Method: 研究构建ALME基准，包含8种语言的57,602个受控音文冲突样本。系统评估Gemini 2.0 Flash等4个先进语音-大语言模型，通过对比音文冲突与文本间冲突条件下的文本主导率，结合强制转写、文本损坏标注等干预实验，以及音频投影层与语言模型层的微调消融实验，验证仲裁可及性假说。

Result: - 音文冲突下文本主导率(16.6%)显著高于文本间冲突(1.6%)，达10倍差异
- 音频单模态准确率(97.2%)优于级联系统(93.9%)，证明音频编码保留更多信息
- 强制转写使文本主导率从19%增至33%，牺牲音频优势
- 将文本标注为"故意损坏"可降低文本主导率80%
- 仅训练音频投影层增加文本主导率+26.5%，而语言模型LoRA微调降低-23.9%
- 跨4个模型与8种语言结果一致，但存在显著跨模型与跨语言差异

Conclusion: 文本主导现象由模态仲裁可及性不对称驱动，而非信息含量差异，决策核心位于语言模型层而非音频编码器。该发现确立了模态仲裁作为独立于传统评估维度的关键可靠性指标，为开发更鲁棒的语音语言模型提供了理论依据与改进方向。

Abstract: When audio and text conflict, speech-enabled language models follow the text 10 times more often than when arbitrating between two text sources, even when explicitly instructed to trust the audio. Using ALME, a benchmark of 57,602 controlled audio-text conflict stimuli across 8 languages, we find that Gemini 2.0 Flash exhibits 16.6\% text dominance under audio-text conflict versus 1.6\% under text-text conflict with identical reliability cues. This gap is not explained by audio quality: audio-only accuracy (97.2\%) exceeds cascade accuracy (93.9\%), indicating audio embeddings preserve more information than text transcripts. We propose that text dominance reflects an asymmetry not in information content but in arbitration accessibility: how easily the model can reason over competing representations.
  This framework explains otherwise puzzling findings. Forcing transcription before answering increases text dominance (19\% to 33\%), sacrificing audio's information advantage without improving accessibility. Framing text as ``deliberately corrupted'' reduces text dominance by 80\%. A fine-tuning ablation provides interventional evidence: training only the audio projection layer increases text dominance (+26.5\%), while LoRA on the language model halves it ($-$23.9\%), localizing text dominance to the LLM's reasoning rather than the audio encoder. Experiments across four state-of-the-art audio-LLMs and 8 languages show consistent trends with substantial cross-linguistic and cross-model variation, establishing modality arbitration as a distinct reliability dimension not captured by standard speech benchmarks.

</details>


### [29] [Multimodal Fact-Level Attribution for Verifiable Reasoning](https://arxiv.org/abs/2602.11509)
*David Wan,Han Wang,Ziyang Wang,Elias Stengel-Eskin,Hyunji Lee,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文提出MuRGAt基准测试，用于评估多模态大模型在超越直接观察推理场景中的事实级归因能力。研究表明，现有强模型即使推理正确也频繁出现引用幻觉，且增加推理深度或强制结构化归约会牺牲准确性，揭示内部推理与可验证归因间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态归因基准局限于简化观察式场景或单模态，无法满足现实世界中多步推理和长文本生成任务对可靠性和事实核查的需求，亟需评估复杂推理下的归因能力。

Method: 构建MuRGAt基准，要求模型在视频、音频等多模态输入下生成带明确推理和精确引用的答案，引用需标注模态与时间段；同时开发自动评估框架，其评分与人类判断高度相关。

Result: 基准测试显示，强MLLMs在推理正确时仍频繁产生引用幻觉；存在关键权衡——提升推理深度或强化结构化归约会显著降低模型准确率。

Conclusion: 当前MLLMs存在内部推理能力与可验证归因之间的鸿沟，需在保持推理性能的同时提升归因可靠性，以支持高可靠性现实应用。

Abstract: Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.

</details>


### [30] [SIGHT: Reinforcement Learning with Self-Evidence and Information-Gain Diverse Branching for Search Agent](https://arxiv.org/abs/2602.11551)
*Wenlin Zhong,Jinluan Yang,Yiquan Wu,Yi Liu,Jianhang Yao,Kun Kuang*

Main category: cs.CL

TL;DR: 本文提出SIGHT框架，通过自证据支持(SES)和信息增益驱动的多样化分支来解决LLM在多轮搜索中面临的"隧道视野"问题，利用动态提示干预和组相对策略优化，在减少搜索步数的同时显著提升复杂问答推理性能。


<details>
  <summary>Details</summary>
Motivation: 在多轮搜索场景中，LLM与检索系统的交互存在两大核心问题：搜索结果冗余度高、信噪比低，导致智能体容易陷入"隧道视野"——即被迫基于早期含噪检索结果进行解释，造成不可逆的错误累积，现有方法难以有效处理这种探索偏差。

Method: SIGHT框架包含四个核心组件：1)自证据支持(SES)从搜索结果中提炼高保真证据；2)计算信息增益分数识别能最大程度降低不确定性的关键状态；3)动态提示干预（去重、反思、自适应分支）基于信息增益生成带SES的新分支；4)组相对策略优化整合SES信号和正确性奖励，使模型内化鲁棒探索策略而无需外部验证器。

Result: 在单跳和多跳问答基准测试中，SIGHT显著优于现有方法，特别是在复杂推理场景下表现突出，且能以更少的搜索步骤达到更优性能，验证了其在抑制错误累积和提升探索效率方面的有效性。

Conclusion: SIGHT通过证据蒸馏和信息增益引导的多样化探索，成功缓解了LLM搜索中的隧道视野问题，为构建更可靠的自主演索推理系统提供了新范式，证明了内部化证据支持机制可显著增强模型在不确定环境中的鲁棒性。

Abstract: Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to master autonomous search for complex question answering. However, particularly within multi-turn search scenarios, this interaction introduces a critical challenge: search results often suffer from high redundancy and low signal-to-noise ratios. Consequently, agents easily fall into "Tunnel Vision," where the forced interpretation of early noisy retrievals leads to irreversible error accumulation. To address these challenges, we propose SIGHT, a framework that enhances search-based reasoning through Self-Evidence Support (SES) and Information-Gain Driven Diverse Branching. SIGHT distills search results into high-fidelity evidence via SES and calculates an Information Gain score to pinpoint pivotal states where observations maximally reduce uncertainty. This score guides Dynamic Prompting Interventions - including de-duplication, reflection, or adaptive branching - to spawn new branches with SES. Finally, by integrating SES and correctness rewards via Group Relative Policy Optimization, SIGHT internalizes robust exploration strategies without external verifiers. Experiments on single-hop and multi-hop QA benchmarks demonstrate that SIGHT significantly outperforms existing approaches, particularly in complex reasoning scenarios, using fewer search steps.

</details>


### [31] [PRIME: A Process-Outcome Alignment Benchmark for Verifiable Reasoning in Mathematics and Engineering](https://arxiv.org/abs/2602.11570)
*Xiangfeng Wang,Hangyu Guo,Yanlin Lai,Mitt Huang,Liang Zhao,Chengyuan Yao,Yinmin Zhang,Qi Han,Xiaoxiao Ren,Chun Yuan,Tong Xu,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CL

TL;DR: 该论文针对RLVR中验证器只关注结果忽略过程的问题，提出PRIME基准，发现当前验证器在检测推导缺陷方面表现不佳，并通过过程感知的训练范式在多个数学基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型的验证器在强化学习可验证奖励(RLVR)中主要关注最终结果与标准答案的一致性，忽视了推导过程中的潜在错误，导致错误的推导过程产生正确答案时仍获得正向奖励。这限制了RLVR的可扩展性和可靠性。

Method: 作者提出PRIME基准，用于评估数学和工程领域中过程-结果对齐的验证器。该基准从大学STEM问题中精选2,530个高难度样本，采用基于一致性的过滤流程。提出过程感知的RLVR训练范式，使用PRIME筛选的验证器进行训练。

Result: 评估发现当前验证器频繁无法检测推导缺陷。过程感知RLVR训练范式在Qwen3-14B-Base模型上相比仅结果验证基线，在AIME24、AIME25和Beyond-AIME上分别取得8.29%、9.12%和7.31%的绝对性能提升。

Conclusion: PRIME是验证器选择的可靠预测指标，验证器在PRIME上的准确率与RLVR训练效果存在强线性相关性(R² > 0.92)。该研究强调了过程感知验证的重要性，为提升RLVR可靠性提供了新方向。

Abstract: While model-based verifiers are essential for scaling Reinforcement Learning with Verifiable Rewards (RLVR), current outcome-centric verification paradigms primarily focus on the consistency between the final result and the ground truth, often neglecting potential errors in the derivation process. This leads to assigning positive rewards to correct answers produced from incorrect derivations. To bridge this gap, we introduce PRIME, a benchmark for evaluating verifiers on Process-Outcome Alignment verification in Mathematics and Engineering. Curated from a comprehensive collection of college-level STEM problems, PRIME comprises 2,530 high-difficulty samples through a consistency-based filtering pipeline. Through extensive evaluation, we find that current verifiers frequently fail to detect derivation flaws. Furthermore, we propose a process-aware RLVR training paradigm utilizing verifiers selected via PRIME. This approach substantially outperforms the outcome-only verification baseline, achieving absolute performance gains of 8.29%, 9.12%, and 7.31% on AIME24, AIME25, and Beyond-AIME, respectively, for the Qwen3-14B-Base model. Finally, we demonstrate a strong linear correlation ($R^2 > 0.92$) between verifier accuracy on PRIME and RLVR training effectiveness, validating PRIME as a reliable predictor for verifier selection.

</details>


### [32] [PACE: Prefix-Protected and Difficulty-Aware Compression for Efficient Reasoning](https://arxiv.org/abs/2602.11639)
*Ruixiang Feng,Yuntao Wen,Silin Zhou,Ke Shi,Yifan Wang,Ran Le,Zhenwei An,Zongchao Chen,Chen Yang,Guangyue Peng,Yiming Jia,Dongsheng Wang,Tao Zhang,Lisi Chen,Yang Song,Shen Gao,Shuo Shang*

Main category: cs.CL

TL;DR: 针对语言推理模型"过度思考"问题，提出双层级压缩框架，通过前缀保护和难度感知机制减少token使用55.7%同时提升准确率4.1%。


<details>
  <summary>Details</summary>
Motivation: 语言推理模型通过扩展测试时计算获得强性能，但存在过度思考问题，产生过长的推理轨迹导致延迟和内存增加。现有统一长度惩罚方法会过度压缩关键早期推理步骤，且对所有查询不加区分地惩罚，亟需更精细化的压缩策略。

Method: 提出双层级框架：序列层级采用前缀保护优化，使用衰减混合rollout维持有效推理路径的同时促进简洁性；群组层级实施难度感知惩罚，根据查询复杂度动态缩放长度约束，对难题保持探索空间，对易题抑制冗余。

Result: 在DeepSeek-R1-Distill-Qwen (1.5B/7B)上实验表明，token使用量减少达55.7%，数学基准准确率提升达4.1%，且在代码、科学及通用领域展现出良好的泛化能力。

Conclusion: 该框架有效解决了过度思考问题，在保持关键推理步骤完整性的前提下实现高效压缩，平衡了推理效率与性能，为语言推理模型的优化提供了新方向。

Abstract: Language Reasoning Models (LRMs) achieve strong performance by scaling test-time computation but often suffer from ``overthinking'', producing excessively long reasoning traces that increase latency and memory usage. Existing LRMs typically enforce conciseness with uniform length penalties, which over-compress crucial early deduction steps at the sequence level and indiscriminately penalize all queries at the group level. To solve these limitations, we propose \textbf{\model}, a dual-level framework for prefix-protected and difficulty-aware compression under hierarchical supervision. At the sequence level, prefix-protected optimization employs decaying mixed rollouts to maintain valid reasoning paths while promoting conciseness. At the group level, difficulty-aware penalty dynamically scales length constraints based on query complexity, maintaining exploration for harder questions while curbing redundancy on easier ones. Extensive experiments on DeepSeek-R1-Distill-Qwen (1.5B/7B) demonstrate that \model achieves a substantial reduction in token usage (up to \textbf{55.7\%}) while simultaneously improving accuracy (up to \textbf{4.1\%}) on math benchmarks, with generalization ability to code, science, and general domains.

</details>


### [33] [Which Feedback Works for Whom? Differential Effects of LLM-Generated Feedback Elements Across Learner Profiles](https://arxiv.org/abs/2602.11650)
*Momoka Furuhashi,Kouta Nakayama,Noboru Kawai,Takashi Kodama,Saku Sugawara,Kyosuke Takami*

Main category: cs.CL

TL;DR: 本研究通过实验探究了LLM生成反馈的不同元素对学习效果和学习者接受度的影响，发现有效反馈元素对学习成果有共同作用模式，而学习者主观偏好因人格特质而异，强调了个性化反馈设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究不同反馈元素（如语气和信息覆盖）如何影响学习效果和学习者接受度，特别是在不同人格特质的学习者群体中，为设计有效的个性化LLM反馈提供依据。

Method: 研究定义了六个反馈元素，利用GPT-5为选择题生成生物学反馈；对321名高一学生进行学习实验，通过两项学习成果测量和六项主观评价指标评估反馈效果，并基于大五人格特质分析学习者接受度差异。

Result: 研究发现，有效的反馈元素对学习成果具有共同的支持模式；然而，学习者的主观偏好在不同人格特质聚类中存在显著差异。

Conclusion: 研究强调在设计LLM生成反馈时，应根据学习者的人格特质选择和调整反馈元素，为教育中的个性化反馈设计提供了实践启示。

Abstract: Large language models (LLMs) show promise for automatically generating feedback in education settings. However, it remains unclear how specific feedback elements, such as tone and information coverage, contribute to learning outcomes and learner acceptance, particularly across learners with different personality traits. In this study, we define six feedback elements and generate feedback for multiple-choice biology questions using GPT-5. We conduct a learning experiment with 321 first-year high school students and evaluate feedback effectiveness using two learning outcomes measures and subjective evaluations across six criteria. We further analyze differences in how feedback acceptance varies across learners based on Big Five personality traits. Our results show that effective feedback elements share common patterns supporting learning outcomes, while learners' subjective preferences differ across personality-based clusters. These findings highlight the importance of selecting and adapting feedback elements according to learners' personality traits when we design LLM-generated feedback, and provide practical implications for personalized feedback design in education.

</details>


### [34] [PatientHub: A Unified Framework for Patient Simulation](https://arxiv.org/abs/2602.11684)
*Sahand Sabour,TszYam NG,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出PatientHub，一个统一且模块化的模拟患者框架，通过标准化定义、组成与部署解决现有碎片化问题。案例研究展示了其支持跨方法/跨模型评估与自定义指标集成的能力，两个新模拟器原型的开发验证了框架通过消除基础设施开销加速方法创新的作用，为患者中心对话研究提供了可复现的基础设施。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的模拟患者为咨询师培训与治疗评估提供了可扩展的解决方案，但现有研究在数据格式、提示词设计和评估指标上缺乏统一标准，导致方法间难以复现和公平比较，制约了领域进展。

Method: 设计并实现PatientHub框架，采用模块化架构标准化模拟患者的全生命周期管理。通过复现代表性模拟方法进行案例研究，展示框架的评估标准化能力；进一步开发两个新型模拟器变体，验证其可扩展性和开发效率优势。

Result: 框架实现了跨方法与跨模型的标准化评估，支持自定义指标的灵活集成。原型开发表明，PatientHub通过抽象基础设施细节，使研究者能专注于方法创新，显著降低新模拟器开发门槛。代码开源促进可复现研究。

Conclusion: PatientHub整合了零散的模拟患者研究工作，建立了统一的评估基准，为未来数据集构建、方法创新和基准测试提供了可扩展的基础设施。该框架将推动患者中心对话系统向标准化、可比较的方向发展。

Abstract: As Large Language Models increasingly power role-playing applications, simulating patients has become a valuable tool for training counselors and scaling therapeutic assessment. However, prior work is fragmented: existing approaches rely on incompatible, non-standardized data formats, prompts, and evaluation metrics, hindering reproducibility and fair comparison. In this paper, we introduce PatientHub, a unified and modular framework that standardizes the definition, composition, and deployment of simulated patients. To demonstrate PatientHub's utility, we implement several representative patient simulation methods as case studies, showcasing how our framework supports standardized cross-method evaluation and the seamless integration of custom evaluation metrics. We further demonstrate PatientHub's extensibility by prototyping two new simulator variants, highlighting how PatientHub accelerates method development by eliminating infrastructure overhead. By consolidating existing work into a single reproducible pipeline, PatientHub lowers the barrier to developing new simulation methods and facilitates cross-method and cross-model benchmarking. Our framework provides a practical foundation for future datasets, methods, and benchmarks in patient-centered dialogue, and the code is publicly available via https://github.com/Sahandfer/PatientHub.

</details>


### [35] [Finding Sense in Nonsense with Generated Contexts: Perspectives from Humans and Language Models](https://arxiv.org/abs/2602.11699)
*Katrin Olsen,Sebastian Padó*

Main category: cs.CL

TL;DR: 本文通过人工与LLM评估五个语义偏离数据集，发现绝大多数句子仅属异常而非真正无意义，且LLM擅长为异常句构建合理语境，从而揭示出现有数据集的实际性质及LLM的语义区分能力。


<details>
  <summary>Details</summary>
Motivation: 无意义与异常句在语义解释计算模型发展中起关键作用，但核心挑战在于区分"可语境化的异常"与"真正无意义"内容。然而，现有数据集的实际无意义程度以及LLM在此区分任务上的表现尚不明确。

Method: 研究采用人类评分者与LLM对五个语义偏离数据集中的句子进行合理性判断，分别评估无上下文和提供上下文两种条件下的句义可解释性。

Result: 人类评分结果显示，大多数句子至多被判定为异常，仅少数为真正无意义；同时，LLM在生成异常句的合理语境方面表现出相当高的技能水平。

Conclusion: 该研究证实现有语义偏离数据集以异常句为主而非真正无意义句，并揭示LLM在语境化异常句方面具备显著能力，为语义解释模型提供了新的认知。

Abstract: Nonsensical and anomalous sentences have been instrumental in the development of computational models of semantic interpretation. A core challenge is to distinguish between what is merely anomalous (but can be interpreted given a supporting context) and what is truly nonsensical. However, it is unclear (a) how nonsensical, rather than merely anomalous, existing datasets are; and (b) how well LLMs can make this distinction. In this paper, we answer both questions by collecting sensicality judgments from human raters and LLMs on sentences from five semantically deviant datasets: both context-free and when providing a context. We find that raters consider most sentences at most anomalous, and only a few as properly nonsensical. We also show that LLMs are substantially skilled in generating plausible contexts for anomalous cases.

</details>


### [36] [Thinking with Drafting: Optical Decompression via Logical Reconstruction](https://arxiv.org/abs/2602.11731)
*Jingxuan Wei,Honghao He,Caijun Jia,Siyuan Li,Zheng Sun,Yuhang Xu,Yuanyuan Lin,Linzhuang Sun,Yuchen Wu,Bihui Yu,Xiangxiang Zhang,Cheng Tan*

Main category: cs.CL

TL;DR: 该论文提出"思考即草拟"(TwD)框架，通过领域特定语言(DSL)将视觉推理重新概念化为"光学解压缩"，以解决多模态大模型在复杂推理中的精度悖论，实现自我验证的确定性视觉证明。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型虽具备高保真视觉感知与生成能力，但在复杂推理任务中面临精度悖论：视觉感知系统仅转录符号而未能捕捉逻辑拓扑，像素生成模型则产生缺乏数学精确性的视觉伪影。

Method: 基于"解析即推理"的公理，提出Thinking with Drafting (TwD)方法，采用极简领域特定语言(DSL)作为中间表示，强制模型将心智模型草拟为可执行代码，生成确定性视觉证明以进行自我验证。

Result: 在VisAlg视觉代数基准测试中，TwD表现出优越的认知脚手架作用，建立了视觉生成作为逻辑验证器而非创意输出的闭环系统。

Conclusion: 该工作为视觉推理提供了一条可推广路径，通过光学解压缩从压缩视觉标记中重建潜在逻辑结构，实现了视觉生成向逻辑验证器的范式转变。

Abstract: Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.

</details>


### [37] [Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning](https://arxiv.org/abs/2602.11748)
*Futing Wang,Jianhao Yan,Yun Luo,Ganqu Cui,Zhi Wang,Xiaoye Qu,Yue Zhang,Yu Cheng,Tao Lin*

Main category: cs.CL

TL;DR: 本文针对测试时扩展中的上下文探索问题，提出长度激励探索方法(LIE)，通过长度奖励和冗余惩罚克服浅层探索陷阱，在Qwen3和Llama模型上分别实现4.4%和2.7%的领域内和领域外性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在测试时扩展中面临的核心瓶颈是"浅层探索陷阱"——虽然更广泛的探索状态覆盖需要更长的推理轨迹，但在自回归生成过程中采样此类序列的概率呈指数级衰减，导致模型难以在单一连续上下文中有效生成、验证和优化多个推理假设。

Method: 提出Length-Incentivized Exploration(LIE)方法，采用两阶段策略：首先通过基于长度的奖励机制激励模型生成更长的推理路径，然后施加冗余惩罚以避免重复探索，从而最大化状态空间覆盖。该方法简单有效，可直接应用于现有自回归模型。

Result: 在Qwen3和Llama等不同模型上的综合实验表明，LIE能有效激励上下文探索。最终方法在领域内任务上平均提升4.4%，在领域外基准测试上获得2.7%的增益。

Conclusion: 本研究成功验证了长度激励探索策略在克服浅层探索陷阱方面的有效性，为实现更高效的测试时扩展提供了新思路，表明通过显式优化状态覆盖可显著提升模型的推理性能。

Abstract: Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.
  Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.
  To bridge this gap, we propose Length-Incentivized Exploration(\method).
  This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.
  Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \method effectively incentivize in-context exploration.
  As a result, our method achieves an average improvement of 4.4\% on in-domain tasks and a 2.7\% gain on out-of-domain benchmarks.

</details>


### [38] [MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling](https://arxiv.org/abs/2602.11761)
*MiniCPM Team,Wenhao An,Yingfa Chen,Yewei Fang,Jiayi Li,Xin Li,Yaohui Li,Yishan Li,Yuxuan Li,Biyuan Lin,Chuan Liu,Hezi Liu,Siyuan Liu,Hongya Lyu,Yinxu Pan,Shixin Ren,Xingyu Shen,Zhou Su,Haojun Sun,Yangang Sun,Zhen Leng Thai,Xin Tian,Rui Wang,Xiaorong Wang,Yudong Wang,Bo Wu,Xiaoyue Xu,Dong Xu,Shuaikang Xue,Jiawei Yang,Bowen Zhang,Jinqian Zhang,Letian Zhang,Shengnan Zhang,Xinyu Zhang,Xinyuan Zhang,Zhu Zhang,Hengyu Zhao,Jiacheng Zhao,Jie Zhou,Zihan Zhou,Shuo Wang,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: MiniCPM-SALA是一个90亿参数的混合架构，通过稀疏注意力和线性注意力的创新融合，实现了超长上下文的高效处理。该方法支持百万级token上下文，推理速度比全注意力模型快3.5倍，同时训练成本降低75%。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型向超长上下文应用演进，Transformer架构的高计算和内存成本成为主要瓶颈。现有的稀疏和线性注意力机制虽能缓解问题，但通常面临内存效率与模型性能之间的权衡，亟需一种既能保持高性能又能提升效率的解决方案。

Method: 本文提出MiniCPM-SALA，采用1:3比例的层选择算法混合稀疏注意力（InfLLM-V2）和线性注意力（Lightning Attention），并引入混合位置编码（HyPE）。同时设计了一个成本效益高的持续训练框架，可将预训练Transformer模型转换为混合模型，大幅降低训练成本。

Result: 实验表明，MiniCPM-SALA在保持与全注意力模型相当通用能力的同时显著提升效率。在单张NVIDIA A6000D GPU上，处理256K token序列时推理速度达全注意力模型的3.5倍，支持最长1M token上下文，而传统8B全注意力模型因内存限制无法达到此规模。

Conclusion: MiniCPM-SALA通过创新的混合架构设计和持续训练框架，成功解决了超长上下文处理的效率瓶颈，为资源受限场景下的大规模语言模型部署提供了有效方案。

Abstract: The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involve a trade-off between memory efficiency and model performance. This paper introduces MiniCPM-SALA, a 9B-parameter hybrid architecture that integrates the high-fidelity long-context modeling of sparse attention (InfLLM-V2) with the global efficiency of linear attention (Lightning Attention). By employing a layer selection algorithm to integrate these mechanisms in a 1:3 ratio and utilizing a hybrid positional encoding (HyPE), the model maintains efficiency and performance for long-context tasks. Furthermore, we introduce a cost-effective continual training framework that transforms pre-trained Transformer-based models into hybrid models, which reduces training costs by approximately 75% compared to training from scratch. Extensive experiments show that MiniCPM-SALA maintains general capabilities comparable to full-attention models while offering improved efficiency. On a single NVIDIA A6000D GPU, the model achieves up to 3.5x the inference speed of the full-attention model at the sequence length of 256K tokens and supports context lengths of up to 1M tokens, a scale where traditional full-attention 8B models fail because of memory constraints.

</details>


### [39] [A Subword Embedding Approach for Variation Detection in Luxembourgish User Comments](https://arxiv.org/abs/2602.11795)
*Anne-Marie Lutgen,Alistair Plum,Christoph Purschke*

Main category: cs.CL

TL;DR: 本研究提出一种基于子词嵌入的无监督语言变异检测方法，通过结合余弦相似度与n-gram相似度对原始文本中的词汇形式进行聚类，无需预定义变体列表或先验归一化处理。在卢森堡语用户评论语料上的实验表明，该方法能有效揭示系统性拼写与形态变异模式。


<details>
  <summary>Details</summary>
Motivation: 传统方法将拼写和形态多样性视为噪声，依赖预定义变体列表或文本归一化。本研究旨在开发一种无需这些前提的新方法，将语言变异视为结构化语言现象而非干扰，特别关注低资源语言和社交媒体等"嘈杂"文本环境下的语言变体分析需求。

Method: 在原始文本上训练子词嵌入向量，通过余弦相似度衡量分布语义关系，结合n-gram相似度捕捉形式相似性，二者融合对相关词汇形式进行分组聚类。该方法生成可解释的词族结构，支持定量与定性分析，且无需严格的人工标注。

Result: 在卢森堡语大规模用户评论语料中，该方法识别出大量词汇与拼写变异现象，其模式与方言学和社会语言学研究高度吻合。生成的词族揭示了系统性对应关系，并清晰呈现出区域与风格层面的语言分化特征。

Conclusion: 分布建模方法能够在低资源或嘈杂文本环境中有效捕捉有意义的变化模式，为多语言及小语种研究提供了可重复的方法论框架，证明将变异视为语言结构而非噪声的路径具有可行性与解释力。

Abstract: This paper presents an embedding-based approach to detecting variation without relying on prior normalisation or predefined variant lists. The method trains subword embeddings on raw text and groups related forms through combined cosine and n-gram similarity. This allows spelling and morphological diversity to be examined and analysed as linguistic structure rather than treated as noise. Using a large corpus of Luxembourgish user comments, the approach uncovers extensive lexical and orthographic variation that aligns with patterns described in dialectal and sociolinguistic research. The induced families capture systematic correspondences and highlight areas of regional and stylistic differentiation. The procedure does not strictly require manual annotation, but does produce transparent clusters that support both quantitative and qualitative analysis. The results demonstrate that distributional modelling can reveal meaningful patterns of variation even in ''noisy'' or low-resource settings, offering a reproducible methodological framework for studying language variety in multilingual and small-language contexts.

</details>


### [40] [DMAP: A Distribution Map for Text](https://arxiv.org/abs/2602.11871)
*Tom Kempton,Julia Rozanova,Parameswaran Kamalaruban,Maeve Madigan,Karolina Wresilo,Yoann L. Launay,David Sutton,Stuart Burrell*

Main category: cs.CL

TL;DR: 本文提出DMAP，一种数学严谨的文本分析方法，通过大语言模型将文本映射至单位区间的样本集，联合编码词元排名与概率信息。该方法解决了困惑度等指标无法充分捕捉上下文分布的局限性，并通过三个应用案例（生成参数验证、机器生成文本检测、合成数据训练模型的取证分析）证明了其有效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖困惑度等指标提取大语言模型中的信号，但这些指标未能充分考量条件分布的形状特征——即合理候选词的数量如何影响对单一词元概率的解读。这种上下文依赖性的缺失导致当前方法存在局限，亟需一种能联合建模排名与概率的数学化方法。

Method: DMAP是一种数学上严谨的表示方法，通过语言模型将文本映射为单位区间上的样本集合。该表示同时编码预测词元的排名信息与概率分布，形成模型无关的高效表征，可在消费级硬件上快速计算，支持多种下游分析任务。

Result: 研究通过三个案例验证DMAP的效用：(1) 验证生成参数保障数据完整性；(2) 利用概率曲率特征检测机器生成文本；(3) 对合成数据后训练模型进行取证分析，揭示其遗留的统计指纹。结果表明该方法提供了统一且高效的统计视角。

Conclusion: DMAP为基于大语言模型的文本分析提供了统一、简洁的统计视图，其计算轻量、适用性广，为未来研究奠定了新基础，推动了模型可解释性与文本取证技术的发展。

Abstract: Large Language Models (LLMs) are a powerful tool for statistical text analysis, with derived sequences of next-token probability distributions offering a wealth of information. Extracting this signal typically relies on metrics such as perplexity, which do not adequately account for context; how one should interpret a given next-token probability is dependent on the number of reasonable choices encoded by the shape of the conditional distribution. In this work, we present DMAP, a mathematically grounded method that maps a text, via a language model, to a set of samples in the unit interval that jointly encode rank and probability information. This representation enables efficient, model-agnostic analysis and supports a range of applications. We illustrate its utility through three case studies: (i) validation of generation parameters to ensure data integrity, (ii) examining the role of probability curvature in machine-generated text detection, and (iii) a forensic analysis revealing statistical fingerprints left in downstream models that have been subject to post-training on synthetic data. Our results demonstrate that DMAP offers a unified statistical view of text that is simple to compute on consumer hardware, widely applicable, and provides a foundation for further research into text analysis with LLMs.

</details>


### [41] [Towards Fair and Comprehensive Evaluation of Routers in Collaborative LLM Systems](https://arxiv.org/abs/2602.11877)
*Wanxing Wu,He Zhu,Yixia Li,Lei Yang,Jiehui Zhao,Hongru Wang,Jian Yang,Benyou Wang,Bingyi Jing,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出RouterXBench评估框架和ProbeDirichlet路由器，利用内部隐藏状态与狄利克雷分布实现LLM查询的智能路由，在路由能力和高准确率场景下较最佳基线分别提升16.68%和18.86%，并展现出跨模型、跨任务和跨域的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽取得成功，但成本与隐私限制要求将小型模型部署在本地，同时将复杂查询卸载至云端模型。现有路由器评估缺乏系统性，忽视场景特定需求与分布外鲁棒性。

Method: 提出RouterXBench原则性评估框架，包含路由能力、场景对齐和跨域鲁棒性三个维度。不同于依赖输出概率或外部嵌入的先前工作，采用内部隐藏状态捕捉答案生成前的不确定性。引入ProbeDirichlet轻量级路由器，通过可学习的狄利克雷分布聚合跨层隐藏状态并进行概率训练。

Result: ProbeDirichlet在路由能力和高准确率场景下较最佳基线分别实现16.68%和18.86%的相对提升，在模型族、模型规模、异构任务和智能体工作流中表现一致。

Conclusion: 该框架和路由器有效解决了成本与隐私约束问题，利用内部隐藏状态实现智能查询路由，在多样化场景下保持鲁棒性能，为LLM部署提供了实用方案。

Abstract: Large language models (LLMs) have achieved success, but cost and privacy constraints necessitate deploying smaller models locally while offloading complex queries to cloud-based models. Existing router evaluations are unsystematic, overlooking scenario-specific requirements and out-of-distribution robustness. We propose RouterXBench, a principled evaluation framework with three dimensions: router ability, scenario alignment, and cross-domain robustness. Unlike prior work that relies on output probabilities or external embeddings, we utilize internal hidden states that capture model uncertainty before answer generation. We introduce ProbeDirichlet, a lightweight router that aggregates cross-layer hidden states via learnable Dirichlet distributions with probabilistic training. Trained on multi-domain data, it generalizes robustly across in-domain and out-of-distribution scenarios. Our results show ProbeDirichlet achieves 16.68% and 18.86% relative improvements over the best baselines in router ability and high-accuracy scenarios, with consistent performance across model families, model scales, heterogeneous tasks, and agentic workflows.

</details>


### [42] [Benchmark Illusion: Disagreement among LLMs and Its Scientific Consequences](https://arxiv.org/abs/2602.11898)
*Eddie Yang,Dashun Wang*

Main category: cs.CL

TL;DR: 这篇论文揭示了大型语言模型基准测试中的"基准幻觉"现象：即使准确率相近的模型，在16-66%的题目上存在分歧，这种隐藏分歧在科学标注任务中会导致处理效应估计变化超过80%甚至符号反转，表明模型选择已成为影响科学可重复性的隐藏变量。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试被认为能可靠衡量LLM进展，但论文发现准确率趋同可能掩盖深刻的认知分歧，特别是在LLM被用于科学数据标注和推理的背景下，这种隐藏分歧会传播到研究结果中，威胁科学可重复性。

Method: 研究使用两个主要推理基准MMLU-Pro和GPQA，分析具有可比准确率的LLM之间的题目级分歧率；并重新分析教育学和政治学领域的已发表研究，检验更换标注模型对处理效应估计的影响。

Result: 研究发现：1) 准确率相似的LLM在16-66%的题目上存在分歧，前沿模型间也有16-38%分歧；2) 这种分歧反映不同模型具有独特的错误模式；3) 在科学应用中，更换模型可使处理效应变化超80%，甚至反转效应符号。

Conclusion: 研究揭示了"基准幻觉"——相等的准确率可能掩盖模型间的深刻分歧，模型选择已成为影响科学可重复性的隐藏但关键变量，强调单一基准准确率不足以衡量模型能力或可信度。

Abstract: Benchmarks underpin how progress in large language models (LLMs) is measured and trusted. Yet our analyses reveal that apparent convergence in benchmark accuracy can conceal deep epistemic divergence. Using two major reasoning benchmarks - MMLU-Pro and GPQA - we show that LLMs achieving comparable accuracy still disagree on 16-66% of items, and 16-38% among top-performing frontier models. These discrepancies suggest distinct error profiles for different LLMs. When such models are used for scientific data annotation and inference, their hidden disagreements propagate into research results: in re-analyses of published studies in education and political science, switching the annotation model can change estimated treatment effects by more than 80%, and in some cases reverses their sign. Together, these findings illustrate a benchmark illusion, where equal accuracy may conceal disagreement, with model choice becoming a hidden yet consequential variable for scientific reproducibility.

</details>


### [43] [Cross-Modal Robustness Transfer (CMRT): Training Robust Speech Translation Models Using Adversarial Text](https://arxiv.org/abs/2602.11933)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 这篇论文针对端到端语音翻译模型在非母语或方言语音的形态变化鲁棒性不足的问题，提出了跨模态鲁棒性迁移（CMRT）框架，将文本模态的对抗鲁棒性迁移到语音模态，无需生成对抗语音数据，在四个语言对上平均提升3个BLEU点以上，建立了新的鲁棒性基准。


<details>
  <summary>Details</summary>
Motivation: 当前端到端语音翻译模型主要在经过整理的"干净"数据集上进行评估，忽略了现实世界中的关键挑战，特别是非母语者或方言语音中常见的屈折形态变化。这种评估方式导致模型在实际应用中的鲁棒性不足，无法处理真实的语音变异情况。

Method: 1) 将基于文本的对抗攻击方法适配到语音领域，针对屈折形态进行攻击，证明现有模型高度脆弱；2) 提出跨模态鲁棒性迁移（CMRT）框架，通过文本模态的对抗训练获得鲁棒表示，然后迁移到语音模态；3) 训练过程中无需生成计算成本高且技术挑战大的对抗语音样本。

Result: 在四个语言对上的大量实验表明，CMRT方法将对抗鲁棒性平均提升了超过3个BLEU分数，建立了无需对抗语音数据生成开销的鲁棒端到端语音翻译新基准，有效解决了模型对形态变化的脆弱性问题。

Conclusion: 该研究成功证明了通过跨模态迁移可以在不增加对抗语音数据生成开销的情况下显著提升语音翻译模型的鲁棒性，为开发更实用的、能处理真实世界语音变异的语音翻译系统提供了新思路和有效解决方案。

Abstract: End-to-End Speech Translation (E2E-ST) has seen significant advancements, yet current models are primarily benchmarked on curated, "clean" datasets. This overlooks critical real-world challenges, such as morphological robustness to inflectional variations common in non-native or dialectal speech. In this work, we adapt a text-based adversarial attack targeting inflectional morphology to the speech domain and demonstrate that state-of-the-art E2E-ST models are highly vulnerable it. While adversarial training effectively mitigates such risks in text-based tasks, generating high-quality adversarial speech data remains computationally expensive and technically challenging. To address this, we propose Cross-Modal Robustness Transfer (CMRT), a framework that transfers adversarial robustness from the text modality to the speech modality. Our method eliminates the requirement for adversarial speech data during training. Extensive experiments across four language pairs demonstrate that CMRT improves adversarial robustness by an average of more than 3 BLEU points, establishing a new baseline for robust E2E-ST without the overhead of generating adversarial speech.

</details>


### [44] [Who is the richest club in the championship? Detecting and Rewriting Underspecified Questions Improve QA Performance](https://arxiv.org/abs/2602.11938)
*Yunchong Huang,Gianni Barlacchi,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 本文揭示QA基准测试中许多问题是欠指定的（缺乏足够上下文导致解释不唯一），LLM在这些问题上表现显著较差，通过重写为完全指定版本可提升性能，表明当前差距主要源于问题表述不清而非模型能力不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在QA任务上表现优异，但标准QA基准测试仍远未解决，作者认为这种差距部分源于问题欠指定性，这成为QA评估中的重要混杂因素，亟需量化分析。

Method: 开发基于LLM的分类器识别欠指定问题并应用于多个QA数据集；通过控制性重写实验，将欠指定问题改写为完全指定变体（保持标准答案不变），以分离欠指定性对性能的影响。

Result: 发现16%至超过50%的基准问题存在欠指定问题，LLM在这些问题上表现显著较差；重写为完全指定版本后性能持续改善，表明许多QA失败实为问题欠指定所致，而非模型能力限制。

Conclusion: 欠指定性是QA评估的关键混杂因素，应提升问题清晰度以更准确评估模型真实能力，推动基准设计向更严谨方向发展。

Abstract: Large language models (LLMs) perform well on well-posed questions, yet standard question-answering (QA) benchmarks remain far from solved. We argue that this gap is partly due to underspecified questions - queries whose interpretation cannot be uniquely determined without additional context. To test this hypothesis, we introduce an LLM-based classifier to identify underspecified questions and apply it to several widely used QA datasets, finding that 16% to over 50% of benchmark questions are underspecified and that LLMs perform significantly worse on them. To isolate the effect of underspecification, we conduct a controlled rewriting experiment that serves as an upper-bound analysis, rewriting underspecified questions into fully specified variants while holding gold answers fixed. QA performance consistently improves under this setting, indicating that many apparent QA failures stem from question underspecification rather than model limitations. Our findings highlight underspecification as an important confound in QA evaluation and motivate greater attention to question clarity in benchmark design.

</details>


### [45] [Do Large Language Models Adapt to Language Variation across Socioeconomic Status?](https://arxiv.org/abs/2602.11939)
*Elisa Bassignana,Mike Zhang,Dirk Hovy,Amanda Cercas Curry*

Main category: cs.CL

TL;DR: LLMs对社会经济地位的语言风格适应能力有限，往往只能模仿或夸张化不同群体的语言特征，且更擅长模仿高社会经济地位群体的语言，这可能加剧语言等级并质疑其在依赖语言风格的社会研究中的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型日益介入人类交流，其能否适应不同社会语境（特别是不同社会经济地位群体）的语言风格成为一个关键问题。若模型无法恰当适应，可能固化刻板印象、边缘化语言规范与模型训练数据差异较大的群体，从而加剧社会分层。

Method: 研究团队收集了来自Reddit和YouTube的、按社会经济地位分层的新型数据集，使用四个大型语言模型对不完整文本进行补全，并从94个社会语言学指标（包括句法、修辞和词汇特征）上比较模型生成文本与原始文本的差异。

Result: 大型语言模型仅能在较小程度上根据社会经济地位调整语言风格，常常产生近似或夸张化的模仿，且更擅长模拟高社会经济地位群体的语言风格。

Conclusion: 研究结果表明，大型语言模型可能加剧语言等级制度，并质疑其在基于智能体的社会模拟、调查实验以及任何依赖语言风格作为社会信号的研究中的有效性。

Abstract: Humans adjust their linguistic style to the audience they are addressing. However, the extent to which LLMs adapt to different social contexts is largely unknown. As these models increasingly mediate human-to-human communication, their failure to adapt to diverse styles can perpetuate stereotypes and marginalize communities whose linguistic norms are less closely mirrored by the models, thereby reinforcing social stratification. We study the extent to which LLMs integrate into social media communication across different socioeconomic status (SES) communities. We collect a novel dataset from Reddit and YouTube, stratified by SES. We prompt four LLMs with incomplete text from that corpus and compare the LLM-generated completions to the originals along 94 sociolinguistic metrics, including syntactic, rhetorical, and lexical features. LLMs modulate their style with respect to SES to only a minor extent, often resulting in approximation or caricature, and tend to emulate the style of upper SES more effectively. Our findings (1) show how LLMs risk amplifying linguistic hierarchies and (2) call into question their validity for agent-based social simulation, survey experiments, and any research relying on language style as a social signal.

</details>


### [46] [Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models](https://arxiv.org/abs/2602.11961)
*Yuzhe Shang,Pengzhi Gao,Wei Liu,Jian Luan,Jinsong Su*

Main category: cs.CL

TL;DR: 本研究探索开源大语言模型在多语言机器翻译中的应用，基于Gemma3模型通过持续预训练和指令微调开发MiLMMT-46模型，在46种语言上达到SOTA性能，超越多个开源模型并与商业系统竞争。


<details>
  <summary>Details</summary>
Motivation: 尽管开源大语言模型的多语言能力持续提升，但如何有效利用模型扩展和数据扩展策略来优化其在多语言机器翻译任务上的性能，仍缺乏系统性研究。

Method: 基于Gemma3模型家族，系统研究模型规模和数据规模对多语言机器翻译的影响，通过持续预训练和指令微调策略开发MiLMMT-46模型，评估其在46种语言上的翻译性能。

Result: MiLMMT-46在46种语言上实现顶级多语言翻译性能，持续优于Seed-X、HY-MT-1.5和TranslateGemma等SOTA开源模型，性能可与Google Translate和Gemini 3 Pro等商业系统相媲美。

Conclusion: 研究表明，通过合理的持续预训练和指令微调，开源多语言机器翻译模型能够达到商业级质量，模型扩展和数据扩展是提升性能的关键因素。

Abstract: Open large language models (LLMs) have demonstrated improving multilingual capabilities in recent years. In this paper, we present a study of open LLMs for multilingual machine translation (MT) across a range of languages, and investigate the effects of model scaling and data scaling when adapting open LLMs to multilingual MT through continual pretraining and instruction finetuning. Based on the Gemma3 model family, we develop MiLMMT-46, which achieves top-tier multilingual translation performance across 46 languages. Extensive experiments show that MiLMMT-46 consistently outperforms recent state-of-the-art (SOTA) models, including Seed-X, HY-MT-1.5, and TranslateGemma, and achieves competitive performance with strong proprietary systems such as Google Translate and Gemini 3 Pro.

</details>


### [47] [LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss](https://arxiv.org/abs/2602.12005)
*Szilvia Ujváry,Louis Béthune,Pierre Ablin,João Monteiro,Marco Cuturi,Michael Kirchhof*

Main category: cs.CL

TL;DR: 针对小语言模型(SLM)因参数限制导致事实性错误的问题，本文提出LaCy预训练方法，通过结合spaCy语法分析器增强损失信号，智能区分SLM应学习的token与应委托给大模型的token，在级联生成时获得更高事实分数，性能优于Rho和LLM-judge方法且更简洁高效。


<details>
  <summary>Details</summary>
Motivation: 小语言模型(SLM)的知识容量受限于参数规模，容易产生事实性错误。现有方法通过让SLM访问外部资源（如大模型、文档或数据库）来缓解此问题。但核心问题仍未解决：SLM在预训练时应学习哪些token，又应将哪些token通过<CALL>标记委托出去？现有方法仅依赖损失值判断，但某些高损失的token实际上是合理的替代表述，不应触发委托，导致判断不够精准。

Method: 本文提出LaCy预训练方法，其核心是基于token选择哲学。首先，利用spaCy语法分析器来增强传统的损失信号，识别哪些token应该由SLM学习预测，哪些应该委托给大模型。该方法区分了两种token：一是需要防止事实错误的委托token，二是即使损失较高但属于安全可学习的预测token。通过这种增强信号，模型能够学习在何处进行委托以及在何处坚持预测。

Result: 实验表明，LaCy模型能够有效学习预测和委托的决策。在级联生成设置下，LaCy模型获得了更高的FactScore，性能优于Rho和LLM-judge训练的SLM。同时，LaCy方法更加简单且成本更低。

Conclusion: LaCy方法成功解决了SLM在预训练中学习什么与委托什么的基本问题，通过语法分析增强信号，实现了更智能的token级决策。这不仅提高了事实准确性，还简化了训练流程，降低了成本，为小语言模型的效率提升提供了新思路。

Abstract: Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \emph{which tokens an SLM can and should learn} during pretraining, versus \emph{which ones it should delegate} via a \texttt{<CALL>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \texttt{<CALL>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.

</details>


### [48] [Disentangling Ambiguity from Instability in Large Language Models: A Clinical Text-to-SQL Case Study](https://arxiv.org/abs/2602.12015)
*Angelo Ziletti,Leonardo D'Ambrosi*

Main category: cs.CL

TL;DR: CLUES框架通过将语义不确定性分解为歧义分数和不稳定分数，区分了临床Text-to-SQL中输入歧义和模型不稳定性两类输出多样性原因，在保持竞争力的同时提供了可诊断的分解结果，实现高效错误分流。


<details>
  <summary>Details</summary>
Motivation: 临床Text-to-SQL部署中需要区分两种本质不同的输出多样性原因：(i)应触发澄清的输入歧义，(ii)应触发人工审核的模型不稳定性。现有方法无法有效分离这两种不确定性，导致干预策略不精准。

Method: 提出CLUES框架，将Text-to-SQL建模为两阶段过程（解释→答案），通过二分语义图矩阵的舒尔补计算不稳定性分数，实现语义不确定性的歧义与不稳定分解。

Result: 在AmbigQA/SituatedQA和临床Text-to-SQL基准测试上，CLUES在故障预测方面优于最先进的Kernel Language Entropy；部署设置下保持竞争力；高歧义/高不稳定regime包含51%错误但仅覆盖25%查询。

Conclusion: 该框架将不确定性映射到针对性干预（歧义→查询优化，不稳定→模型改进），实现了高效的错误分流策略。

Abstract: Deploying large language models for clinical Text-to-SQL requires distinguishing two qualitatively different causes of output diversity: (i) input ambiguity that should trigger clarification, and (ii) model instability that should trigger human review. We propose CLUES, a framework that models Text-to-SQL as a two-stage process (interpretations --> answers) and decomposes semantic uncertainty into an ambiguity score and an instability score. The instability score is computed via the Schur complement of a bipartite semantic graph matrix. Across AmbigQA/SituatedQA (gold interpretations) and a clinical Text-to-SQL benchmark (known interpretations), CLUES improves failure prediction over state-of-the-art Kernel Language Entropy. In deployment settings, it remains competitive while providing a diagnostic decomposition unavailable from a single score. The resulting uncertainty regimes map to targeted interventions - query refinement for ambiguity, model improvement for instability. The high-ambiguity/high-instability regime contains 51% of errors while covering 25% of queries, enabling efficient triage.

</details>


### [49] [Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.12036)
*Xin Xu,Clive Bai,Kai Yang,Tianhao Chen,Yangkun Chen,Weijie Liu,Hao Chen,Yang Wang,Saiyong Yang,Can Yang*

Main category: cs.CL

TL;DR: 提出Composition-RL方法，通过自动组合多个问题生成新的可验证提示，专门针对通过率为1的简单提示进行优化，在4B至30B参数规模的模型上持续提升推理能力，并支持课程学习和跨域强化学习。


<details>
  <summary>Details</summary>
Motivation: 大规模可验证提示存在大量信息量不足的样本且扩展成本高昂；现有研究过度聚焦于通过率为0的困难提示，而通过率为1的简单提示会随着训练进程变得越来越普遍，导致有效数据规模缩减，亟需高效利用这类简单提示的新方法。

Method: Composition-RL自动将多个问题组合生成新的可验证问题用于强化学习训练；核心策略是针对通过率为1的提示进行优化；引入课程学习变体，逐步增加组合深度；支持跨域强化学习，通过组合不同领域的提示实现知识迁移。

Result: 在4B至30B参数规模的模型上，Composition-RL相比原始数据集训练的RL持续提升推理性能；课程学习策略可带来额外性能增益；能够有效促进跨领域强化学习效果。

Conclusion: Composition-RL是一种简单而有效的框架，能高效利用有限的可验证提示，特别擅长处理通过率为1的简单提示，显著增强模型推理能力，并为跨领域知识迁移提供了可行路径，代码、数据集和模型均已开源。

Abstract: Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.

</details>


### [50] [DeepSight: An All-in-One LM Safety Toolkit](https://arxiv.org/abs/2602.12092)
*Bo Zhang,Jiaxuan Guo,Lijun Li,Dongrui Liu,Sujin Chen,Guanxu Chen,Zhijie Zheng,Qihao Lin,Lewen Yan,Chen Qian,Yijin Zhou,Yuyao Wu,Shaoxiong Guo,Tianyi Du,Jingyi Yang,Xuhao Hu,Ziqi Miao,Xiaoya Lu,Jing Shao,Xia Hu*

Main category: cs.CL

TL;DR: 本文提出DeepSight开源项目，通过DeepSafe评估工具包和DeepScan诊断工具包的协同，实现大模型安全评估与诊断的一体化，将黑盒安全评估转为白盒洞察，解决现有流程中评估、诊断、对齐环节割裂导致无法定位内部根因的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型与多模态大模型的安全工作流程中，评估、诊断与对齐通常由独立工具处理，导致安全评估仅能发现外部行为风险而无法追溯内部根因，诊断脱离具体风险场景停留在可解释层面，对齐缺乏内部机制变化的专门解释可能损害模型通用能力。现有方法存在环节割裂、成本高昂、不可复现等问题。

Method: 提出DeepSight开源项目，采用评估-诊断一体化新范式，包含DeepSafe评估工具包和DeepScan诊断工具包。通过统一任务与数据协议，构建两阶段连接，实现从黑盒到白盒的转化。支持前沿AI风险评估与联合安全评估诊断，具备低成本、可复现、高效、高可扩展性特点。

Result: 实现了首个支持前沿AI风险联合评估与诊断的开源工具包，成功将安全评估从黑盒行为检测转变为白盒内部机制洞察，建立了评估与诊断阶段的标准化连接协议。

Conclusion: DeepSight通过整合评估与诊断流程，系统性解决了大模型安全工作中根因定位难、场景脱节、能力退化等核心问题，为构建更安全、可解释的大模型提供了实用工具和方法论，推动了大模型安全研究的开源生态发展。

Abstract: As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.

</details>


### [51] [P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling](https://arxiv.org/abs/2602.12116)
*Pinyi Zhang,Ting-En Lin,Yuchuan Wu,Jingyang Chen,Zongqi Wang,Hua Yang,Ze Xu,Fei Huang,Kai Zhang,Yongbin Li*

Main category: cs.CL

TL;DR: 针对大语言模型个性化对齐中用户偏好信号获取难题，本文提出P-GenRM——首个支持测试时用户自适应缩放的个性化生成式奖励模型。该方法通过结构化评估链与用户原型迁移机制，在基准测试上实现SOTA性能（平均提升2.31%），并显著提升对未见用户的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前个性化奖励模型面临双重挑战：其一，将多变的场景化偏好过度简化为有限固定评估准则，导致偏好表示不充分；其二，在用户反馈稀缺情况下，难以泛化至新用户。这制约了开放场景下精准用户奖励信号的获取，成为LLM个性化对齐的关键瓶颈。

Method: P-GenRM创新性地将偏好信号转化为结构化评估链，动态生成跨场景的自适应角色与评分标准。通过用户聚类构建原型空间，并设计双重粒度缩放机制：个体层面自适应聚合用户专属评分方案，原型层面引入相似用户偏好进行知识迁移，从而有效抑制偏好推断噪声，增强模型泛化性。

Result: 实验表明，P-GenRM在主流个性化奖励模型基准上达到SOTA，平均提升2.31%。在分布外测试集上展现强泛化能力。测试时用户自适应缩放机制额外带来3%性能增益，证实其对测试时扩展性的有效性。

Conclusion: P-GenRM通过结构化评估链与用户原型迁移，有效破解了偏好简化与泛化不足的核心问题。其测试时用户自适应缩放能力为LLM个性化对齐提供了可扩展的新范式，推动了精准用户偏好建模的发展。

Abstract: Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.

</details>


### [52] [A Rule-based Computational Model for Gaidhlig Morphology](https://arxiv.org/abs/2602.12132)
*Peter J Barclay*

Main category: cs.CL

TL;DR: 针对小语种数据稀缺问题，本文提出基于Wiktionary的苏格兰盖尔语形态学规则模型，利用SQL查询和声明式规则库生成词形变化，支持教育工具和语言分析。


<details>
  <summary>Details</summary>
Motivation: 现有神经模型需要大量数据，而低资源语言缺乏此类资源。需要可解释、数据高效的解决方案，并能为教学材料设计提供洞见。

Method: 采用Wiktionary数据，设计SQL查询识别词汇模式，构建声明式形态学规则库，开发Python工具自动推导苏格兰盖尔语词形变化。

Result: 实现了可推导盖尔语词形变化的规则系统，验证了基于规则方法在低资源语言处理中的可行性，展示了将Wiktionary数据转化为教育工具和句法分析器基础的潜力。

Conclusion: 规则式方法能高效利用小语种有限数据，具有良好的可解释性和教学应用价值，可将现有词典资源转化为实用语言技术工具。

Abstract: Language models and software tools are essential to support the continuing vitality of lesser-used languages; however, currently popular neural models require considerable data for training, which normally is not available for such low-resource languages. This paper describes work-in-progress to construct a rule-based model of Gaidhlig morphology using data from Wiktionary, arguing that rule-based systems effectively leverage limited sample data, support greater interpretability, and provide insights useful in the design of teaching materials. The use of SQL for querying the occurrence of different lexical patterns is investigated, and a declarative rule-base is presented that allows Python utilities to derive inflected forms of Gaidhlig words. This functionality could be used to support educational tools that teach or explain language patterns, for example, or to support higher level tools such as rule-based dependency parsers. This approach adds value to the data already present in Wiktionary by adapting it to new use-cases.

</details>


### [53] [WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models](https://arxiv.org/abs/2602.12135)
*Yangzhuo Li,Shengpeng Ji,Yifu Chen,Tianle Liang,Haorong Ying,Yule Wang,Junbo Li,Jun Fang,Zhou Zhao*

Main category: cs.CL

TL;DR: 本文针对现有语音对话模型评估仅关注文本生成标准、忽视音频中心特征与认知深度的问题，提出WavBench综合基准。该基准创新性地构建三元框架：Pro子集（高难度推理挑战）、Basic子集（以"可听性"为核心定义口语化新标准）和Acoustic子集（真实场景下副语言学能力全面评估），通过系统测试五个先进模型，为鲁棒语音对话系统发展提供关键指导。


<details>
  <summary>Details</summary>
Motivation: 随着高级推理能力快速融入语音对话模型，领域亟需超越简单交互、应对真实世界复杂性的评估基准。然而，当前评估普遍遵循文本生成标准，忽视了副语言学、口语化等音频中心特征，以及现代智能体所需的认知深度。

Method: 提出WavBench基准，采用独特的三元框架：1）Pro子集，通过显著提升难度严格挑战推理增强模型；2）Basic子集，通过自然词汇、语言流畅性和交互默契定义口语化新标准，优先"可听性"而非僵化书面准确性；3）Acoustic子集，覆盖显性理解、生成和隐性对话，严格评估真实场景中综合副语言学能力。

Result: 通过对五个先进模型的评估，WavBench揭示了复杂问题解决、口语化表达与副语言学保真度之间的关键交叉点，为模型演进提供重要洞察。

Conclusion: WavBench为鲁棒语音对话模型的未来发展提供系统指导，推动领域建立更真实、更全面的评估标准。

Abstract: With the rapid integration of advanced reasoning capabilities into spoken dialogue models, the field urgently demands benchmarks that transcend simple interactions to address real-world complexity. However, current evaluations predominantly adhere to text-generation standards, overlooking the unique audio-centric characteristics of paralinguistics and colloquialisms, alongside the cognitive depth required by modern agents. To bridge this gap, we introduce WavBench, a comprehensive benchmark designed to evaluate realistic conversational abilities where prior works fall short. Uniquely, WavBench establishes a tripartite framework: 1) Pro subset, designed to rigorously challenge reasoning-enhanced models with significantly increased difficulty; 2) Basic subset, defining a novel standard for spoken colloquialism that prioritizes "listenability" through natural vocabulary, linguistic fluency, and interactive rapport, rather than rigid written accuracy; and 3) Acoustic subset, covering explicit understanding, generation, and implicit dialogue to rigorously evaluate comprehensive paralinguistic capabilities within authentic real-world scenarios. Through evaluating five state-of-the-art models, WavBench offers critical insights into the intersection of complex problem-solving, colloquial delivery, and paralinguistic fidelity, guiding the evolution of robust spoken dialogue models. The benchmark dataset and evaluation toolkit are available at https://naruto-2024.github.io/wavbench.github.io/.

</details>


### [54] [Query-focused and Memory-aware Reranker for Long Context Processing](https://arxiv.org/abs/2602.12192)
*Yuqing Li,Jiangnan Li,Mo Yu,Guoxuan Ding,Zheng Lin,Weiping Wang,Jie Zhou*

Main category: cs.CL

TL;DR: 本文提出一种基于大语言模型检索头注意力分数的列表级重排序框架，利用整个候选短名单信息，无需Likert分数监督即可训练，仅用4B参数模型就在多领域超越SOTA方法，并在LoCoMo基准上取得新突破。


<details>
  <summary>Details</summary>
Motivation: 现有重排序方法或依赖逐点处理而无法利用整体信息，或需要Likert分数等人工监督。本研究旨在构建一种无需精细监督、能利用全列表信息且轻量高效的替代重排序框架。

Method: 通过训练模型利用选定检索头的注意力分数估计 passage-query 相关性。框架采用列表级重排序，在排名过程中整合整个候选列表的全局信息，并自动生成连续相关性分数，摆脱对Likert尺度监督的依赖，仅需小模型（如4B参数）即可运行。

Result: 实验证明该方法在维基百科、长叙事数据集等多个领域均优于现有最优的逐点和列表级重排序器，并在LoCoMo对话理解与记忆基准测试上刷新SOTA。框架扩展性良好：增加候选 passage 的上下文信息可进一步提升准确率；使用中间层注意力头可在不损失性能的前提下提高效率。

Conclusion: 该框架以轻量级实现（小模型）达到了卓越的重排序性能，成功摆脱了对人工标注的依赖，展现了良好的扩展灵活性，为检索任务提供了高效实用的解决方案。

Abstract: Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.

</details>


### [55] [ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images](https://arxiv.org/abs/2602.12203)
*Mathieu Sibue,Andres Muñoz Garza,Samuel Mensah,Pranav Shetty,Zhiqiang Ma,Xiaomo Liu,Manuela Veloso*

Main category: cs.CL

TL;DR: 本文针对通用视觉语言模型在企业文档细粒度结构化信息提取方面的不足，提出了ExStrucTiny基准数据集，该数据集通过人工与合成数据相结合的方式构建，统一了关键实体提取、关系提取和视觉问答任务，并揭示了当前模型在模式适应、查询模糊和答案定位等方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 企业文档承载着数据归档、自动化工作流和分析等下游应用的关键信息，但现有KEE、RE和VQA数据集受限于狭窄的实体本体、简单查询或单一文档类型，无法满足灵活结构化提取的需求。通用视觉语言模型在多样化文档和灵活模式下的整体提取能力尚未得到充分研究。

Method: 作者设计了一种结合人工标注与合成数据生成的新型流水线，构建了ExStrucTiny基准数据集。该数据集覆盖了更丰富的文档类型和提取场景，并统一了关键实体提取、关系提取和视觉问答三个任务范式。

Result: 在ExStrucTiny上对开放和闭源视觉语言模型进行分析表明，当前模型面临三大挑战：模式适应能力不足、查询表述不完整导致的歧义，以及答案精确定位的困难。

Conclusion: 该工作为提升通用模型在文档结构化信息提取能力方面提供了重要基础，有助于推动视觉语言模型在企业级文档理解中的应用发展。

Abstract: Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.

</details>


### [56] [Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.12235)
*Julia Belikova,Danila Rozhevskii,Dennis Svirin,Konstantin Polev,Alexander Panchenko*

Main category: cs.CL

TL;DR: 本文针对大语言模型长文本处理中的软压缩技术，提出了"token overflow"概念来识别压缩信息不足的情况。研究发现，结合查询信息的轻量级分类器能在三个数据集上以0.72的平均AUC-ROC检测溢出，优于查询无关方法，为低成本预防压缩错误提供了可行方案。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在资源受限环境下处理长上下文仍面临效率挑战。软压缩架构通过压缩token序列来扩展有效上下文长度，但其可压缩极限以及何时会丢失任务相关信息尚未充分探索，这限制了其在实际应用中的可靠性。

Method: 论文定义了"token overflow"状态，并提出了检测和表征方法。在xRAG软压缩框架下，比较了两种方案：1)仅使用饱和统计量的查询无关检测；2)结合查询和上下文表示的轻量级探针分类器。在HotpotQA、SQuADv2和TriviaQA三个数据集上进行评估。

Result: 查询无关的饱和统计量能有效区分压缩与未压缩token，但检测溢出能力有限。而融合查询信息的轻量级分类器在三个数据集上实现了0.72的平均AUC-ROC，显著提升了检测性能，证实了查询感知策略的有效性。

Conclusion: 研究从查询无关诊断推进到查询感知检测，成功开发出低成本的前置门控机制，可在大语言模型处理前识别并缓解压缩导致的错误，为提升长上下文应用的可靠性提供了实用工具。

Abstract: Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define \emph{token overflow} as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.

</details>


### [57] [Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications](https://arxiv.org/abs/2602.12241)
*Manjunath Kudlur,Evan King,James Wang,Pete Warden*

Main category: cs.CL

TL;DR: Moonshine v2采用滑动窗口自注意力机制，在保持高精度的同时实现了低延迟的流式语音识别，性能可与比其大6倍的模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限的边缘设备上低延迟语音应用的需求，解决全注意力Transformer编码器因全局依赖导致的二次方复杂度和随语音长度线性增长的TTFT问题。

Method: 提出Moonshine v2模型，采用遍历流式编码器架构和滑动窗口自注意力机制，在保持强局部上下文的同时实现有界低延迟推理。

Result: 在标准基准测试中达到业界领先的词错误率，精度可与参数量大6倍的模型相当，同时运行速度显著更快。

Conclusion: 精心设计局部注意力机制可以在保持全注意力精度的同时大幅降低计算开销和延迟，为边缘设备上的交互式语音接口开辟新可能性。

Abstract: Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent "encode-the-whole-utterance" latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.

</details>


### [58] [T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization](https://arxiv.org/abs/2602.12262)
*Tunyu Zhang,Xinxi Zhang,Ligong Han,Haizhou Shi,Xiaoxiao He,Zhuowei Li,Hao Wang,Kai Xu,Akash Srivastava,Hao Wang,Vladimir Pavlovic,Dimitris N. Metaxas*

Main category: cs.CL

TL;DR: 该论文提出了一种轨迹自蒸馏框架，通过引入直接判别优化（DDO）目标函数，改善扩散大语言模型在少步解码时的效率与生成质量，显著缩小了与全步解码之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）虽具备并行生成潜力，但其推理效率受限于多步细化过程；过度减少步数会导致生成质量严重下降，这制约了其在实际应用中的部署。

Method: 提出轨迹自蒸馏框架，利用模型自身生成轨迹进行知识蒸馏；引入直接判别优化（DDO）这一反向KL散度目标，促进模式寻求式蒸馏，使学生模型聚焦于教师模型的高概率生成模式。

Result: 在多个基准测试中，该方法在严格步数限制下持续优于现有强少步基线和标准训练方式，大幅缩小了少步与全步解码之间的质量差距。

Conclusion: 尽管全步解码仍具优势，但本研究为实用化少步扩散大语言模型奠定了坚实基础，推动了高效文本生成技术的发展。

Abstract: Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.

</details>


### [59] [On-Policy Context Distillation for Language Models](https://arxiv.org/abs/2602.12275)
*Tianzhu Ye,Li Dong,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: OPCD框架通过让学生模型在自己的生成轨迹上进行训练，同时最小化与上下文条件教师模型的反向KL散度，实现了在策略蒸馏与上下文蒸馏的结合，有效将情境知识内化到模型参数中。


<details>
  <summary>Details</summary>
Motivation: 现有上下文蒸馏方法未能充分利用学生模型自身生成的数据，与在策略学习结合不足。OPCD旨在通过让学生模型从自己的轨迹中学习，同时保持与教师模型的一致性，更有效地将情境知识内化到模型参数中。

Method: OPCD框架让学生模型生成自己的解决方案轨迹，然后最小化这些轨迹与上下文条件教师模型之间的反向KL散度。这允许学生模型在保持与教师行为一致的同时，从自身经验中学习，实现知识内化。

Result: 在数学推理、文本游戏和领域特定任务上，OPCD持续优于基线方法，在提高任务准确率的同时更好地保持了分布外能力。此外，OPCD还能实现跨尺寸蒸馏，使小模型能有效从大模型中内化经验知识。

Conclusion: OPCD是一个有效的框架，它通过结合在策略学习与上下文蒸馏，成功将情境知识内化到模型参数中，适用于多种任务场景，并支持模型压缩（小模型从大模型学习）。

Abstract: Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [60] [MTFM: A Scalable and Alignment-free Foundation Model for Industrial Recommendation in Meituan](https://arxiv.org/abs/2602.11235)
*Xin Song,Zhilin Guan,Ruidong Han,Binghao Tang,Tianwen Chen,Bing Li,Zihao Li,Han Zhang,Fei Jiang,Chaolin Xie,Chi Ma,Chunyang Jiang,Chunzhen Jing,Dengxuan Li,Fengyi Li,Lei Yu,Mengyao Sun,Pu Wang,Qing Wang,Rui Fan,Shangyu Chen,Shifeng Du,Siyuan Bai,Wei Lin,Wentao Zhu,Zhou Han,Zhuo Chen,Zikang Xu*

Main category: cs.IR

TL;DR: 针对多场景推荐系统资源消耗大、输入对齐严格的问题，本文提出MTFM——一种基于Transformer的基础模型。通过异构token转换实现无对齐多场景知识捕获，结合用户级样本聚合、分组查询注意力、混合目标注意力及系统级优化，显著提升训练和推理效率，并通过实验验证了扩展模型容量和数据带来的性能增益。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统涉及多场景，但现有跨域和多场景推荐方法资源消耗巨大且依赖严格输入对齐，限制可扩展性。因此需要开发一种高效、无需对齐的多场景推荐框架。

Method: 提出MTFM框架：1）将跨域数据转换为异构token，以无对齐方式捕获多场景知识；2）多场景用户级样本聚合，减少实例数提升训练吞吐量；3）集成Grouped-Query Attention和定制化Hybrid Target Attention降低内存与计算复杂度；4）实施内核融合和消除CPU-GPU阻塞等系统优化。

Result: 离线和在线实验证实MTFM有效性，通过扩展模型容量和多场景训练数据获得显著性能提升。

Conclusion: MTFM通过创新架构设计和系统优化，成功解决了多场景推荐系统的资源消耗和对齐限制问题，为工业级应用提供了高效可扩展的解决方案。

Abstract: Industrial recommendation systems typically involve multiple scenarios, yet existing cross-domain (CDR) and multi-scenario (MSR) methods often require prohibitive resources and strict input alignment, limiting their extensibility. We propose MTFM (Meituan Foundation Model for Recommendation), a transformer-based framework that addresses these challenges. Instead of pre-aligning inputs, MTFM transforms cross-domain data into heterogeneous tokens, capturing multi-scenario knowledge in an alignment-free manner. To enhance efficiency, we first introduce a multi-scenario user-level sample aggregation that significantly enhances training throughput by reducing the total number of instances. We further integrate Grouped-Query Attention and a customized Hybrid Target Attention to minimize memory usage and computational complexity. Furthermore, we implement various system-level optimizations, such as kernel fusion and the elimination of CPU-GPU blocking, to further enhance both training and inference throughput. Offline and online experiments validate the effectiveness of MTFM, demonstrating that significant performance gains are achieved by scaling both model capacity and multi-scenario training data.

</details>


### [61] [From Noise to Order: Learning to Rank via Denoising Diffusion](https://arxiv.org/abs/2602.11453)
*Sajad Ebrahimi,Bhaskar Mitra,Negar Arabzadeh,Ye Yuan,Haolun Wu,Fattane Zarrinkalam,Ebrahim Bagheri*

Main category: cs.IR

TL;DR: 该论文提出DiffusionRank，一种基于去噪扩散的生成式排序学习方法，通过建模特征与相关性标签的联合分布来替代传统判别式条件概率建模，在信息检索排序任务上显著优于判别式基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统判别式LTR方法仅建模查询-文档特征对到相关性标签的条件概率，过参数化模型易找到多种拟合训练数据的方式，导致泛化性不足。相比之下，生成式方法需解释完整数据分布，理论上应产生更鲁棒且泛化能力更强的排序模型。

Method: DiffusionRank扩展TabDiff框架，将去噪扩散模型应用于LTR领域，通过逐步去噪过程学习特征向量与相关性标签的联合分布，构建了点级（pointwise）和配对级（pairwise）LTR目标的生成式等效形式，实现从判别到生成范式的转换。

Result: 实证结果显示，DiffusionRank模型相比其判别式对应方法在排序性能上实现显著提升，验证了生成式方法在LTR任务中的有效性和优势。

Conclusion: 该研究揭示了深度生成建模技术（如扩散模型）在信息检索排序学习中的巨大潜力，为未来探索如何利用生成式建模的持续进步来改进IR任务开辟了新的研究方向。

Abstract: In information retrieval (IR), learning-to-rank (LTR) methods have traditionally limited themselves to discriminative machine learning approaches that model the probability of the document being relevant to the query given some feature representation of the query-document pair. In this work, we propose an alternative denoising diffusion-based deep generative approach to LTR that instead models the full joint distribution over feature vectors and relevance labels. While in the discriminative setting, an over-parameterized ranking model may find different ways to fit the training data, we hypothesize that candidate solutions that can explain the full data distribution under the generative setting produce more robust ranking models. With this motivation, we propose DiffusionRank that extends TabDiff, an existing denoising diffusion-based generative model for tabular datasets, to create generative equivalents of classical discriminative pointwise and pairwise LTR objectives. Our empirical results demonstrate significant improvements from DiffusionRank models over their discriminative counterparts. Our work points to a rich space for future research exploration on how we can leverage ongoing advancements in deep generative modeling approaches, such as diffusion, for learning-to-rank in IR.

</details>


### [62] [KuaiSearch: A Large-Scale E-Commerce Search Dataset for Recall, Ranking, and Relevance](https://arxiv.org/abs/2602.11518)
*Yupeng Li,Ben Chen,Mingyue Cheng,Zhiding Liu,Xuxin Zhang,Chenyi Lei,Wenwu Ou*

Main category: cs.IR

TL;DR: 针对现有电商搜索数据集在真实性和完整性方面的不足，研究者构建了KuaiSearch——一个基于快手真实用户交互数据的大型电商搜索数据集，涵盖召回、排序、相关性判断三个关键环节，并保留了冷启动用户和长尾商品，为LLM驱动的电商搜索研究提供了宝贵资源。


<details>
  <summary>Details</summary>
Motivation: 电商搜索面临查询歧义、商品文本噪声大、用户偏好多样等挑战，而现有数据集存在查询启发式构造、过滤冷启动用户和长尾商品、文本匿名化、仅覆盖单阶段等问题，制约了LLM在该领域的研究进展。

Method: 基于快手平台真实用户搜索交互构建KuaiSearch，保留原始查询和自然语言商品文本，包含冷启动用户和长尾商品，系统覆盖搜索流水线的召回、排序和相关性判断三个阶段，并从多维度进行综合分析，建立基准实验。

Result: KuaiSearch是目前已知最大的电商搜索数据集，实验结果表明其为现实世界电商搜索研究提供了有价值的资源基础。

Conclusion: KuaiSearch通过提供真实、完整且多阶段的电商搜索数据，有效弥补了现有数据集的不足，为基于大语言模型的电商搜索研究奠定了重要基础。

Abstract: E-commerce search serves as a central interface, connecting user demands with massive product inventories and plays a vital role in our daily lives. However, in real-world applications, it faces challenges, including highly ambiguous queries, noisy product texts with weak semantic order, and diverse user preferences, all of which make it difficult to accurately capture user intent and fine-grained product semantics. In recent years, significant advances in large language models (LLMs) for semantic representation and contextual reasoning have created new opportunities to address these challenges. Nevertheless, existing e-commerce search datasets still suffer from notable limitations: queries are often heuristically constructed, cold-start users and long-tail products are filtered out, query and product texts are anonymized, and most datasets cover only a single stage of the search pipeline. Collectively, these issues constrain research on LLM-based e-commerce search. To address these challenges, we construct and release KuaiSearch. To the best of our knowledge, it is the largest e-commerce search dataset currently available. KuaiSearch is built upon real user search interactions from the Kuaishou platform, preserving authentic user queries and natural-language product texts, covering cold-start users and long-tail products, and systematically spanning three key stages of the search pipeline: recall, ranking, and relevance judgment. We conduct a comprehensive analysis of KuaiSearch from multiple perspectives, including products, users, and queries, and establish benchmark experiments across several representative search tasks. Experimental results demonstrate that KuaiSearch provides a valuable foundation for research on real-world e-commerce search.

</details>


### [63] [LASER: An Efficient Target-Aware Segmented Attention Framework for End-to-End Long Sequence Modeling](https://arxiv.org/abs/2602.11562)
*Tianhe Lin,Ziwei Xiong,Baoyuan Ou,Yingjie Qin,Lai Xu,Xiaocheng Zhong,Yao Hu,Zhiyong Wang,Tao Zhou,Yubin Xu,Di Wu*

Main category: cs.IR

TL;DR: 本文提出LASER框架，通过系统级(SeqVault)和算法级(STA/GSTA)优化，解决超长期用户行为序列建模中的I/O延迟和计算复杂度瓶颈，在工业级推荐系统中实现显著性能提升和商业价值。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统需建模超长期用户行为序列以捕捉演进和终身兴趣，但工业部署面临严格延迟墙：一是海量用户历史读取的I/O高延迟，二是标准注意力机制的计算复杂度瓶颈。

Method: 提出LASER全栈优化框架：(1) SeqVault基础设施，采用DRAM-SSD混合索引策略实现毫秒级历史访问；(2) 分段目标注意力(STA)机制，通过sigmoid门控过滤噪声，结合轻量级全局堆叠注意力(GSTA)捕获跨段依赖。

Result: SeqVault降低50%检索延迟和75% CPU使用率；离线评估超越SOTA基线；大规模A/B测试(1亿+日活用户)实现ADVV提升2.36%和收入提升2.08%。

Conclusion: LASER通过全栈优化成功突破超长期序列建模的延迟墙，在工业级部署中验证了可扩展性和显著商业价值。

Abstract: Modeling ultra-long user behavior sequences is pivotal for capturing evolving and lifelong interests in modern recommendation systems. However, deploying such models in real-time industrial environments faces a strict "Latency Wall", constrained by two distinct bottlenecks: the high I/O latency of retrieving massive user histories and the quadratic computational complexity of standard attention mechanisms. To break these bottlenecks, we present LASER, a full-stack optimization framework developed and deployed at Xiaohongshu (RedNote). Our approach tackles the challenges through two complementary innovations: (1) System efficiency: We introduce SeqVault, a unified schema-aware serving infrastructure for long user histories. By implementing a hybrid DRAM-SSD indexing strategy, SeqVault reduces retrieval latency by 50% and CPU usage by 75%, ensuring millisecond-level access to full real-time and life-cycle user histories. (2) Algorithmic efficiency: We propose a Segmented Target Attention (STA) mechanism to address the computational overhead. Motivated by the inherent sparsity of user interests, STA employs a sigmoid-based gating strategy that acts as a silence mechanism to filter out noisy items. Subsequently, a lightweight Global Stacked Target Attention (GSTA) module refines these compressed segments to capture cross-segment dependencies without incurring high computational costs. This design performs effective sequence compression, reducing the complexity of long-sequence modeling while preserving critical signals. Extensive offline evaluations demonstrate that LASER consistently outperforms state-of-the-art baselines. In large-scale online A/B testing serving over 100 million daily active users, LASER achieved a 2.36% lift in ADVV and a 2.08% lift in revenue, demonstrating its scalability and significant commercial impact.

</details>


### [64] [Analytical Search](https://arxiv.org/abs/2602.11581)
*Yiteng Tu,Shuo Miao,Weihang Su,Yiqun Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 本文针对法律、金融、科学等领域中趋势分析、因果评估等分析型信息需求，指出现有检索范式（相关性排序与RAG）难以满足端到端、高问责性的分析任务要求。为此提出"分析型搜索"新范式，将搜索重构为证据驱动、流程导向的分析工作流，通过显式建模分析意图、多步推理与证据融合生成可验证结论，并给出统一系统框架及研究方向，旨在推动下一代搜索引擎发展。


<details>
  <summary>Details</summary>
Motivation: 分析型信息需求在专业领域普遍存在，但现有检索范式存在根本缺陷：相关性排序仅关注信息发现，不支持端到端问题解决；RAG虽能生成回答，却对推理过程、证据使用及结果可验证性缺乏有效控制，无法满足分析查询对多样效用概念与高问责性的要求。因此，亟需一种新范式来系统性地支持分析型信息需求。

Method: 提出"分析型搜索"新范式，将搜索重新定义为证据驱动、流程导向的分析工作流。核心方法包括：1) 显式建模分析意图；2) 检索证据并进行推理感知的融合；3) 通过结构化多步推理生成可验证结论。提出统一系统框架，整合查询理解、召回导向检索、推理感知融合与自适应验证四个关键模块，并探讨构建分析型搜索引擎的潜在研究路径。

Result: 成功构建分析型搜索的概念框架，明确其与现有范式的区别定位。提出的统一框架实现了从查询理解到结果验证的完整分析流程。论文识别了分析型搜索引擎建设的关键研究方向，强调了其在概念创新性与实践应用价值上的重要性，为下一代搜索引擎的理论研究与系统开发提供了蓝图。

Conclusion: 分析型搜索有效解决了现有范式在处理复杂分析任务时的能力不足问题，通过结构化推理和证据验证机制显著提升了结果的可问责性。该范式兼具理论创新意义与广阔应用前景，呼吁学术界与工业界协同努力，共同推进支持分析型信息需求的下一代搜索引擎研发。

Abstract: Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented generation (RAG) with large language models (LLMs), often struggle to meet the end-to-end requirements of such tasks at the corpus scale. They either emphasize information finding rather than end-to-end problem solving, or simply treat everything as naive question answering, offering limited control over reasoning, evidence usage, and verifiability. As a result, they struggle to support analytical queries that have diverse utility concepts and high accountability requirements.
  In this paper, we propose analytical search as a distinct and emerging search paradigm designed to fulfill these analytical information needs. Analytical search reframes search as an evidence-governed, process-oriented analytical workflow that explicitly models analytical intent, retrieves evidence for fusion, and produces verifiable conclusions through structured, multi-step inference. We position analytical search in contrast to existing paradigms, and present a unified system framework that integrates query understanding, recall-oriented retrieval, reasoning-aware fusion, and adaptive verification. We also discuss potential research directions for the construction of analytical search engines. In this way, we highlight the conceptual significance and practical importance of analytical search and call on efforts toward the next generation of search engines that support analytical information needs.

</details>


### [65] [Recurrent Preference Memory for Efficient Long-Sequence Generative Recommendation](https://arxiv.org/abs/2602.11605)
*Yixiao Chen,Yuan Wang,Yue Liu,Qiyao Wang,Ke Cheng,Xin Xu,Juntong Yan,Shuojin Yang,Menghao Guo,Jun Zhang,Huan Yu,Jie Jiang*

Main category: cs.IR

TL;DR: Rec2PM通过自参照教师强制策略将长用户交互历史压缩为紧凑的偏好记忆标记，在实现并行训练的同时保持推理时的迭代更新能力，显著提升生成式推荐系统的效率与精度。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐模型采用全注意力机制处理用户行为序列，面临两大核心挑战：1) 计算复杂度随序列长度呈平方级增长，难以扩展到终身序列；2) 随机交互引入的噪声随序列增长而累积，影响推荐质量。亟需一种既能压缩历史信息又能去噪的高效框架。

Method: 提出Rec2PM框架，核心创新为自参照教师强制策略：利用历史全局视图生成参考记忆作为监督目标，实现并行化循环更新。将记忆表示为可学习的标记嵌入而非传统KV缓存，既支持完全并行训练，又能在推理时进行迭代优化，实现存储效率与计算效率的双重提升。

Result: 在大规模基准测试中，Rec2PM相比全序列模型显著降低推理延迟和内存占用，同时获得更高的推荐准确率。实验证明该方法在效率与效果间取得了优异平衡，适用于资源受限的实际推荐场景。

Conclusion: 偏好记忆机制充当了去噪信息瓶颈，有效过滤交互噪声并提取用户稳健的长期兴趣，为长序列生成式推荐提供了可扩展的理论框架与实践方案。

Abstract: Generative recommendation (GenRec) models typically model user behavior via full attention, but scaling to lifelong sequences is hindered by prohibitive computational costs and noise accumulation from stochastic interactions. To address these challenges, we introduce Rec2PM, a framework that compresses long user interaction histories into compact Preference Memory tokens. Unlike traditional recurrent methods that suffer from serial training, Rec2PM employs a novel self-referential teacher-forcing strategy: it leverages a global view of the history to generate reference memories, which serve as supervision targets for parallelized recurrent updates. This allows for fully parallel training while maintaining the capability for iterative updates during inference. Additionally, by representing memory as token embeddings rather than extensive KV caches, Rec2PM achieves extreme storage efficiency. Experiments on large-scale benchmarks show that Rec2PM significantly reduces inference latency and memory footprint while achieving superior accuracy compared to full-sequence models. Analysis reveals that the Preference Memory functions as a denoising Information Bottleneck, effectively filtering interaction noise to capture robust long-term interests.

</details>


### [66] [Evolutionary Router Feature Generation for Zero-Shot Graph Anomaly Detection with Mixture-of-Experts](https://arxiv.org/abs/2602.11622)
*Haiyang Jiang,Tong Chen,Xinyi Gao,Guansong Pang,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.IR

TL;DR: 针对零样本图异常检测中图结构、特征和异常模式异质性导致的单GNN方法表达能力不足问题，提出EvoFG框架。该框架通过进化特征生成方案构建信息丰富的结构特征，并设计记忆增强路由器学习不变路由模式，在六个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 零样本图异常检测近年来受到广泛关注，但图结构、特征和异常模式的多样性使得现有单GNN方法表达能力有限。MoE架构虽具潜力，但面临分布偏移带来的两大路由挑战：节点语义跨图差异大导致特征路由产生偏见；异常图的分布差异使路由器难以捕获可泛化的域不变路由原则。

Method: 提出进化路由器特征生成(EvoFG)框架：1) 进化特征生成方案，通过LLM生成器和Shapley指导的评估迭代构建选择信息性结构特征；2) 记忆增强路由器结合不变学习目标，捕获分布偏移下的可迁移路由模式。

Result: 在六个基准数据集上的广泛实验表明，EvoFG持续优于最先进基线方法，实现了强大且稳定的零样本GAD性能。

Conclusion: 该方法通过进化特征生成和不变路由学习有效解决了零样本GAD中的分布偏移问题，为处理跨图异质性提供了新范式，显著提升了模型泛化能力。

Abstract: Zero-shot graph anomaly detection (GAD) has attracted increasing attention recent years, yet the heterogeneity of graph structures, features, and anomaly patterns across graphs make existing single GNN methods insufficiently expressive to model diverse anomaly mechanisms. In this regard, Mixture-of-experts (MoE) architectures provide a promising paradigm by integrating diverse GNN experts with complementary inductive biases, yet their effectiveness in zero-shot GAD is severely constrained by distribution shifts, leading to two key routing challenges. First, nodes often carry vastly different semantics across graphs, and straightforwardly performing routing based on their features is prone to generating biased or suboptimal expert assignments. Second, as anomalous graphs often exhibit pronounced distributional discrepancies, existing router designs fall short in capturing domain-invariant routing principles that generalize beyond the training graphs. To address these challenges, we propose a novel MoE framework with evolutionary router feature generation (EvoFG) for zero-shot GAD. To enhance MoE routing, we propose an evolutionary feature generation scheme that iteratively constructs and selects informative structural features via an LLM-based generator and Shapley-guided evaluation. Moreover, a memory-enhanced router with an invariant learning objective is designed to capture transferable routing patterns under distribution shifts. Extensive experiments on six benchmarks show that EvoFG consistently outperforms state-of-the-art baselines, achieving strong and stable zero-shot GAD performance.

</details>


### [67] [IntTravel: A Real-World Dataset and Generative Framework for Integrated Multi-Task Travel Recommendation](https://arxiv.org/abs/2602.11664)
*Huimin Yan,Longfei Xu,Junjie Sun,Zheng Liu,Wei Luo,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: 本文提出首个大规模综合出行推荐数据集IntTravel（含41亿交互记录）及端到端decoder-only生成式多任务推荐框架。该框架通过信息保留、选择与分解机制平衡任务协作与特化，在IntTravel及基准数据集上均达SOTA性能，已部署于高德地图服务数亿用户，CTR提升1.09%。


<details>
  <summary>Details</summary>
Motivation: 现有POI推荐研究受限于碎片化数据集，仅关注目的地预测而忽视出发时间、交通方式及行程需求等关键要素，且数据集规模较小，制约模型评估准确性。

Method: 构建IntTravel大规模集成出行数据集，并设计端到端decoder-only生成式框架，采用信息保留、选择与分解机制，在出发时间、交通方式、POI及需求预测多任务中平衡协作与特化差异。

Result: 框架在IntTravel及非出行基准数据集上均取得SOTA性能；于高德地图成功部署服务数亿用户，CTR显著提升1.09%；数据集已开源。

Conclusion: IntTravel数据集及生成式框架有效解决了出行推荐碎片化问题，通过大规模实践验证综合建模出行各要素的价值，为未来研究提供重要资源。

Abstract: Next Point of Interest (POI) recommendation is essential for modern mobility and location-based services. To provide a smooth user experience, models must understand several components of a journey holistically: "when to depart", "how to travel", "where to go", and "what needs arise via the route". However, current research is limited by fragmented datasets that focus merely on next POI recommendation ("where to go"), neglecting the departure time, travel mode, and situational requirements along the journey. Furthermore, the limited scale of these datasets impedes accurate evaluation of performance. To bridge this gap, we introduce IntTravel, the first large-scale public dataset for integrated travel recommendation, including 4.1 billion interactions from 163 million users with 7.3 million POIs. Built upon this dataset, we introduce an end-to-end, decoder-only generative framework for multi-task recommendation. It incorporates information preservation, selection, and factorization to balance task collaboration with specialized differentiation, yielding substantial performance gains. The framework's generalizability is highlighted by its state-of-the-art performance across both IntTravel dataset and an additional non-travel benchmark. IntTravel has been successfully deployed on Amap serving hundreds of millions of users, leading to a 1.09% increase in CTR. IntTravel is available at https://github.com/AMAP-ML/IntTravel.

</details>


### [68] [EpicCBR: Item-Relation-Enhanced Dual-Scenario Contrastive Learning for Cold-Start Bundle Recommendation](https://arxiv.org/abs/2602.11680)
*Yihang Li,Zhuo Liu,Wei Wei*

Main category: cs.IR

TL;DR: 该论文针对冷启动场景下的Bundle推荐问题，提出了一种名为EpicCBR的多视角对比学习框架。该方法通过挖掘物品关系构建用户画像，利用历史Bundle信息和用户偏好表征新Bundle特征，并结合多视角图对比学习确保模型在冷启动和热启动场景下的泛化能力，最终在三个基准数据集上相比现有最优方法提升高达387%。


<details>
  <summary>Details</summary>
Motivation: 现有Bundle推荐模型过度依赖用户- Bundle交互数据，难以探索不断涌现的新Bundle。同时，当前方法通常将每个Bundle视为独立实例，忽视了用户-物品（UI）和Bundle-物品（BI）关系，特别是热门物品所蕴含的丰富信息，这导致模型在冷启动场景下面临严重的表征挑战。

Method: 提出EpicCBR框架，主要包括三个核心组件：1）通过挖掘物品间关系构建用户画像，识别可能对新Bundle感兴趣的用户；2）基于热门物品的方法，利用历史Bundle信息和用户偏好来表征新Bundle特征；3）引入多视角图对比学习框架，整合冷启动和热启动两种场景，确保模型的泛化能力和鲁棒性。

Result: 在三个主流基准数据集上的大量实验表明，EpicCBR在冷启动场景下显著优于现有最优方法，性能提升幅度高达387%，充分证明了该方法在解决Bundle冷启动问题上的优越性。

Conclusion: EpicCBR通过有效利用物品关系和多视角对比学习机制，成功解决了Bundle推荐中的冷启动问题。该方法不仅在新Bundle推荐上表现出色，同时在热启动场景也保持鲁棒性，为Bundle推荐系统的实际应用提供了有效的解决方案。

Abstract: Bundle recommendation aims to recommend a set of items to users for overall consumption. Existing bundle recommendation models primarily depend on observed user-bundle interactions, limiting exploration of newly-emerged bundles that are constantly created. It pose a critical representation challenge for current bundle methods, as they usually treat each bundle as an independent instance, while neglecting to fully leverage the user-item (UI) and bundle-item (BI) relations over popular items. To alleviate it, in this paper we propose a multi-view contrastive learning framework for cold-start bundle recommendation, named EpicCBR. Specifically, it precisely mine and utilize the item relations to construct user profiles, identifying users likely to engage with bundles. Additionally, a popularity-based method that characterizes the features of new bundles through historical bundle information and user preferences is proposed. To build a framework that demonstrates robustness in both cold-start and warm-start scenarios, a multi-view graph contrastive learning framework capable of integrating these diverse scenarios is introduced to ensure the model's generalization capability. Extensive experiments conducted on three popular benchmarks showed that EpicCBR outperforms state-of-the-art by a large margin (up to 387%), sufficiently demonstrating the superiority of the proposed method in cold-start scenario. The code and dataset can be found in the GitHub repository: https://github.com/alexlovecoding/EpicCBR.

</details>


### [69] [Uncertainty-aware Generative Recommendation](https://arxiv.org/abs/2602.11719)
*Chenxiao Fan,Chongming Gao,Yaxin Gong,Haoyan Liu,Fuli Feng,Xiangnan He*

Main category: cs.IR

TL;DR: 针对生成式推荐中现有方法仅依赖二值正确性而忽视不确定性信息的"不确定性盲"缺陷，本文提出UGR统一框架，通过不确定性加权奖励、难度感知优化和显式置信对齐三机制实现自适应优化，显著提升性能并稳定训练。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法存在系统性"不确定性盲"问题，表现为忽略模型内在生成置信度、样本学习难度差异及缺乏显式置信表达，导致训练动力学不稳定和决策风险不可量化，亟需构建不确定性感知的优化机制。

Method: 提出不确定性感知生成式推荐框架UGR，协同整合三种机制：1)不确定性加权奖励，惩罚高置信度错误；2)难度感知优化动态，防止过早收敛；3)显式置信度对齐，增强模型置信表达能力。

Result: 实验验证UGR不仅实现更优推荐性能，更有效稳定训练过程，避免标准方法的性能退化，且学得的置信度支持可靠的下游风险感知应用。

Conclusion: UGR通过将不确定性作为关键信号引入优化过程，成功克服不确定性盲问题，在提升推荐效果的同时确保训练稳定性，并为风险敏感决策提供可量化的置信度依据。

Abstract: Generative Recommendation has emerged as a transformative paradigm, reformulating recommendation as an end-to-end autoregressive sequence generation task. Despite its promise, existing preference optimization methods typically rely on binary outcome correctness, suffering from a systemic limitation we term uncertainty blindness. This issue manifests in the neglect of the model's intrinsic generation confidence, the variation in sample learning difficulty, and the lack of explicit confidence expression, directly leading to unstable training dynamics and unquantifiable decision risks. In this paper, we propose Uncertainty-aware Generative Recommendation (UGR), a unified framework that leverages uncertainty as a critical signal for adaptive optimization. UGR synergizes three mechanisms: (1) an uncertainty-weighted reward to penalize confident errors; (2) difficulty-aware optimization dynamics to prevent premature convergence; and (3) explicit confidence alignment to empower the model with confidence expression capabilities. Extensive experiments demonstrate that UGR not only yields superior recommendation performance but also fundamentally stabilizes training, preventing the performance degradation often observed in standard methods. Furthermore, the learned confidence enables reliable downstream risk-aware applications.

</details>


### [70] [ULTRA:Urdu Language Transformer-based Recommendation Architecture](https://arxiv.org/abs/2602.11836)
*Alishbah Bashir,Fatima Qaiser,Ijaz Hussain*

Main category: cs.IR

TL;DR: 针对低资源语言乌尔都语缺乏有效语义内容推荐系统的问题，本文提出ULTRA框架，采用双嵌入架构和查询长度感知路由机制，动态区分短查询和长查询并路由到专门的语义管道。实验表明该方法在乌尔都语新闻推荐上比单管道基线精度提升超过90%，为低资源语言的语义检索提供了实用设计见解。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语作为低资源语言，在个性化新闻检索领域缺乏有效的语义内容推荐系统。现有方法主要依赖词汇匹配或与语言无关的技术，难以捕捉语义意图，且在处理不同长度查询和信息需求时表现不佳，导致推荐的相关性和适应性下降。

Method: 提出ULTRA（乌尔都语语言Transformer推荐架构）自适应语义推荐框架。该框架引入双嵌入架构与查询长度感知路由机制，通过阈值驱动决策过程，将用户查询动态路由到针对标题/ headline级或完整内容/文档级表示优化的专门语义管道。利用基于Transformer的嵌入和优化的池化策略，超越表层关键词匹配，实现上下文感知的相似性搜索。

Result: 在大规模乌尔都语新闻语料库上的广泛实验表明，该架构在各种查询类型上持续提升推荐相关性。与单管道基线相比，精度提升超过90%，验证了查询自适应语义对齐在低资源语言中的有效性。

Conclusion: 研究结果确立了ULTRA作为一个鲁棒且可泛化的内容推荐架构，为低资源语言环境下的语义检索系统提供了实用的设计见解。

Abstract: Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly under varying query lengths and information needs. This limitation results in reduced relevance and adaptability in Urdu content recommendation. We propose ULTRA (Urdu Language Transformer-based Recommendation Architecture),an adaptive semantic recommendation framework designed to address these challenges. ULTRA introduces a dual-embedding architecture with a query-length aware routing mechanism that dynamically distinguishes between short, intent-focused queries and longer, context-rich queries. Based on a threshold-driven decision process, user queries are routed to specialized semantic pipelines optimized for either title/headline-level or full-content/document level representations, ensuring appropriate semantic granularity during retrieval. The proposed system leverages transformer-based embeddings and optimized pooling strategies to move beyond surface-level keyword matching and enable context-aware similarity search. Extensive experiments conducted on a large-scale Urdu news corpus demonstrate that the proposed architecture consistently improves recommendation relevance across diverse query types. Results show gains in precision above 90% compared to single-pipeline baselines, highlighting the effectiveness of query-adaptive semantic alignment for low-resource languages. The findings establish ULTRA as a robust and generalizable content recommendation architecture, offering practical design insights for semantic retrieval systems in low-resource language settings.

</details>


### [71] [Improving Neural Retrieval with Attribution-Guided Query Rewriting](https://arxiv.org/abs/2602.11841)
*Moncef Garouani,Josiane Mothe*

Main category: cs.IR

TL;DR: 针对神经检索模型在处理模糊查询时的脆弱性问题，本文提出一种归因引导的查询重写方法，利用检索器的token级梯度归因指导LLM重写查询，在BEIR数据集上持续超越强基线，尤其对隐式和模糊查询提升显著。


<details>
  <summary>Details</summary>
Motivation: 神经检索器虽然有效但存在脆弱性：模糊或欠指定查询会误导排序，即使相关文档存在。现有方法仅部分解决问题：LLM重写缺乏检索器反馈，可解释性方法仅用于事后分析，未能形成闭环优化。

Method: 提出一种归因引导的查询重写方法：对每个查询，从检索器计算梯度-based token归因分数，将这些分数作为软指导嵌入结构化提示给LLM，在保留原始意图的同时澄清薄弱或误导性查询成分。

Result: 在BEIR数据集上评估表明，该方法生成的重写查询持续提升了检索效果，显著超越强基线，且在处理隐式或模糊信息需求时获得更大增益。

Conclusion: 通过将token级归因解释闭环反馈至查询重写过程，有效缓解了神经检索的脆弱性，为构建更鲁棒的检索系统提供了新思路。

Abstract: Neural retrievers are effective but brittle: underspecified or ambiguous queries can misdirect ranking even when relevant documents exist. Existing approaches address this brittleness only partially: LLMs rewrite queries without retriever feedback, and explainability methods identify misleading tokens but are used for post-hoc analysis. We close this loop and propose an attribution-guided query rewriting method that uses token-level explanations to guide query rewriting. For each query, we compute gradient-based token attributions from the retriever and then use these scores as soft guidance in a structured prompt to an LLM that clarifies weak or misleading query components while preserving intent. Evaluated on BEIR collections, the resulting rewrites consistently improve retrieval effectiveness over strong baselines, with larger gains for implicit or ambiguous information needs.

</details>


### [72] [IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval](https://arxiv.org/abs/2602.11941)
*Benjamin Clavié,Atoof Shakir,Jonah Turner,Sean Lee,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: 本文针对音乐信息检索领域缺乏高质量评测基准的问题，提出了IncompeBench基准测试，包含1,574个高质量音乐片段、500个多样化查询和超过12.5万条相关性标注。该基准通过多阶段标注流程构建，标注一致性高，并已开源发布。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态信息检索和音乐信息检索在深度预训练模型推动下取得显著进展，音乐检索性能评估仍缺乏高质量基准测试，限制了领域进一步发展。

Method: 构建IncompeBench基准测试：收集1,574个许可授权的高质量音乐片段，设计500个多样化查询，通过多阶段标注流程产生超过125,000条人工相关性判断，确保标注者间高度一致。

Result: 成功开发出高质量音乐检索基准IncompeBench，数据集已在HuggingFace平台以strict和lenient两种版本公开，相关提示词代码也已在GitHub开源。

Conclusion: 通过发布IncompeBench，为音乐信息检索研究提供了首个大规模、高质量的标准评测基准，将推动该领域模型评估和进一步发展。

Abstract: Multimodal Information Retrieval has made significant progress in recent years, leveraging the increasingly strong multimodal abilities of deep pre-trained models to represent information across modalities. Music Information Retrieval (MIR), in particular, has considerably increased in quality, with neural representations of music even making its way into everyday life products. However, there is a lack of high-quality benchmarks for evaluating music retrieval performance. To address this issue, we introduce \textbf{IncompeBench}, a carefully annotated benchmark comprising $1,574$ permissively licensed, high-quality music snippets, $500$ diverse queries, and over $125,000$ individual relevance judgements. These annotations were created through the use of a multi-stage pipeline, resulting in high agreement between human annotators and the generated data. The resulting datasets are publicly available at https://huggingface.co/datasets/mixedbread-ai/incompebench-strict and https://huggingface.co/datasets/mixedbread-ai/incompebench-lenient with the prompts available at https://github.com/mixedbread-ai/incompebench-programs.

</details>


### [73] [Compress, Cross and Scale: Multi-Level Compression Cross Networks for Efficient Scaling in Recommender Systems](https://arxiv.org/abs/2602.12041)
*Heng Yu,Xiangjun Zhou,Jie Xia,Heng Zhao,Anxin Wu,Yu Zhao,Dongying Kong*

Main category: cs.IR

TL;DR: 本文提出MLCC及其多通道扩展MC-MLCC，通过层次化压缩与动态组合构建结构化特征交互架构，在CTR/CVR预测任务中实现高效的高阶特征交互建模。相比DLRM基线模型，AUC提升达0.52的同时参数量和计算量降低26倍，并在Bilibili广告系统完成工业级部署验证。


<details>
  <summary>Details</summary>
Motivation: 工业级推荐系统中特征交互模块难以同时满足强交互能力、高计算效率和良好可扩展性，导致模型在严格生产约束下ROI受限。现有方法在扩展时面临参数量爆炸或性能瓶颈问题，亟需在保证性能的前提下实现高效可扩展的特征交互建模。

Method: 提出MLCC架构，通过层次化压缩和动态组合实现结构化特征交叉，在保持计算复杂度的前提下捕获高阶特征依赖；进一步设计MC-MLCC多通道扩展，将特征交互分解为并行子空间，实现水平扩展并显著降低参数增长。该方法在嵌入维度、头数和通道数三个维度展现可控的扩展行为。

Result: 在三个公开基准和工业级大规模数据集上，所提模型一致优于强DLRM基线，AUC最高提升0.52；在可比性能下参数量和FLOPs最高减少26倍。扩展性分析显示，基于通道的扩展方式效率显著优于传统嵌入膨胀策略。在线A/B测试验证了其在真实广告平台的有效性。

Conclusion: MLCC/MC-MLCC为工业推荐系统提供了高效且可扩展的特征交互解决方案，在预测性能、计算效率和工程实用性三方面取得显著突破。该方法已满足Bilibili广告系统在严格延迟和资源约束下的生产需求，体现了从理论创新到工业落地的完整价值闭环。

Abstract: Modeling high-order feature interactions efficiently is a central challenge in click-through rate and conversion rate prediction. Modern industrial recommender systems are predominantly built upon deep learning recommendation models, where the interaction backbone plays a critical role in determining both predictive performance and system efficiency. However, existing interaction modules often struggle to simultaneously achieve strong interaction capacity, high computational efficiency, and good scalability, resulting in limited ROI when models are scaled under strict production constraints. In this work, we propose MLCC, a structured feature interaction architecture that organizes feature crosses through hierarchical compression and dynamic composition, which can efficiently capture high-order feature dependencies while maintaining favorable computational complexity. We further introduce MC-MLCC, a Multi-Channel extension that decomposes feature interactions into parallel subspaces, enabling efficient horizontal scaling with improved representation capacity and significantly reduced parameter growth. Extensive experiments on three public benchmarks and a large-scale industrial dataset show that our proposed models consistently outperform strong DLRM-style baselines by up to 0.52 AUC, while reducing model parameters and FLOPs by up to 26$\times$ under comparable performance. Comprehensive scaling analyses demonstrate stable and predictable scaling behavior across embedding dimension, head number, and channel count, with channel-based scaling achieving substantially better efficiency than conventional embedding inflation. Finally, online A/B testing on a real-world advertising platform validates the practical effectiveness of our approach, which has been widely adopted in Bilibili advertising system under strict latency and resource constraints.

</details>


### [74] [SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization](https://arxiv.org/abs/2602.12187)
*Sunghwan Kim,Wooseok Jeong,Serin Kim,Sangam Lee,Dongha Lee*

Main category: cs.IR

TL;DR: 本研究提出SAGEO Arena，首个用于搜索增强生成引擎优化(SAGEO)的端到端评估环境。该环境整合完整的生成式搜索管道与富含结构信息的大规模网页语料库，支持分阶段优化分析。研究发现现有SAGEO方法在真实场景下效果不佳且损害检索性能，而结构化信息可缓解此问题，优化需针对各管道阶段定制。


<details>
  <summary>Details</summary>
Motivation: 现有SAGEO基准存在关键缺陷：缺乏端到端可见性评估，预设候选文档而抽象掉检索与重排序过程，丢弃真实网页中的结构化信息（如schema标记），无法支持全面研究。

Method: 构建SAGEO Arena环境，集成完整的生成式搜索管道，采用包含丰富结构信息的大规模网页语料库，支持检索、重排序、生成分阶段的SAGEO分析，同时联合优化SEO与GEO目标。

Result: 实验揭示：1)现有SAGEO方法在真实条件下大多不实用且常降低检索重排序性能；2)结构化信息有助于缓解这些限制；3)有效优化需针对管道各阶段定制策略。

Conclusion: SAGEO Arena为超越简化设置的现实SAGEO评估与优化奠定基础，推动可重复研究，并强调分阶段定制优化和结构化信息利用的重要性。

Abstract: Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.

</details>


### [75] [AttentionRetriever: Attention Layers are Secretly Long Document Retrievers](https://arxiv.org/abs/2602.12278)
*David Jiahao Fu,Lam Thanh Do,Jiayu Li,Kevin Chen-Chuan Chang*

Main category: cs.IR

TL;DR: 针对长文档检索的挑战，本文提出 AttentionRetriever，一种融合注意力机制与实体检索的新型模型。该模型通过构建上下文感知嵌入和动态确定检索范围，在保持密集检索效率的同时，在长文档检索任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有检索模型缺乏对长文档特性的专门设计，难以处理上下文感知、因果依赖及检索范围界定等关键挑战，限制了RAG系统在长文档场景下的应用效果。

Method: 设计 AttentionRetriever 模型，利用注意力机制生成上下文感知的长文档嵌入，并采用基于实体的检索策略动态确定检索范围，实现精准的长文档检索。

Result: 在长文档检索数据集上，AttentionRetriever 以较大优势超越现有检索模型，同时保持了与密集检索模型相近的效率水平。

Conclusion: 研究表明，注意力机制与实体检索的结合能有效解决长文档检索的核心问题，AttentionRetriever 为RAG系统提供了更强大的长文档处理能力，但其在超长文档和跨领域泛化方面的表现仍有待进一步验证。

Abstract: Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [Automated Optimization Modeling via a Localizable Error-Driven Perspective](https://arxiv.org/abs/2602.11164)
*Weiting Liu,Han Wu,Yufei Kuang,Xiongwei Han,Tao Zhong,Jianfeng Feng,Wenlian Lu*

Main category: cs.LG

TL;DR: 该论文针对大语言模型在自动化优化建模中的后训练数据稀缺问题，提出了MIND框架，通过分析建模错误的局部性特征，采用动态监督微调策略优化（DFPO）方法，在六个基准测试中显著优于现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM后训练方法面临高质量训练数据稀缺和利用率不足的双重困境，具体表现为错误相关问题稀疏（L1）和困难问题奖励稀疏（L2），导致领域特定后训练效果不佳。通过分析错误模式发现，优化建模错误具有局部性而非全局传播特性，这为改进训练提供了关键洞察。

Method: 提出MIND框架，从数据合成到后训练全流程定制。核心是DFPO（动态监督微调策略优化）方法：基于错误传播的局部性特征，构建高聚焦密度训练语料库，对困难问题实施局部精细化修正，而非整体推理。该框架通过识别错误特定的语义片段进行针对性优化。

Result: 在六个基准测试上，MIND框架持续超越所有现有最先进的自动化优化建模方法，证明了错误驱动局部化学习策略的有效性。

Conclusion: 优化建模错误具有局部性特征，不同于数学证明等整体推理任务。利用这一特性构建的MIND框架，通过局部精细化训练可显著提升LLM在自动化优化建模中的性能，为后训练数据稀缺问题提供了有效解决方案。

Abstract: Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs' capabilities in this domain, its effectiveness is severely constrained by the scarcity and underutilization of high-quality training data. However, through a detailed profiling of error patterns across various problem-response pairs drawn from post-training, we identify two fundamental limitations of existing automated optimization modeling approaches: (L1) the sparsity of error-specific problems and (L2) the sparse rewards associated with difficult problems. We demonstrate that these limitations can result in suboptimal performance in domain-specific post-training for LLMs. To tackle the above two limitations, we propose a novel error-driven learning framework -- namely, auto\textbf{m}ated opt\textbf{i}mization modeli\textbf{n}g via a localizable error-\textbf{d}riven perspective (MIND) -- that customizes the whole model training framework from data synthesis to post-training. MIND is based on our key observation of the unique localizable patterns in error propagation of optimization modelings, that is, modeling errors may remain localized to specific semantic segments and do not propagate throughout the entire solution. Thus, in contrast to holistic reasoning tasks such as mathematical proofs, MIND leverages the construction of a focused, high-density training corpus and proposes \textbf{D}ynamic Supervised \textbf{F}ine-Tuning \textbf{P}olicy \textbf{O}ptimization (DFPO) to tackle difficult problems through localized refinement. Experiments on six benchmarks demonstrate that MIND consistently outperforms all the state-of-the-art automated optimization modeling approaches.

</details>


### [77] [KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models](https://arxiv.org/abs/2602.11184)
*Zukang Xu,Zhixiong Zhao,Xing Hu,Zhixuan Chen,Dawei Yang*

Main category: cs.LG

TL;DR: 本文提出KBVQ-MoE，一种针对MoE大模型的超低比特量化框架，通过KLT引导的SVD消除专家冗余表示并共享主要成分，同时采用通道仿射补偿校正输出偏差，在3比特下实现Qwen1.5-MoE-A2.7B模型精度几乎无损（67.99 vs 68.07），为资源受限环境部署MoE模型提供了高效压缩方案。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然通过稀疏激活专家实现了高效计算，但其巨大的参数量和内存需求给资源受限环境部署带来挑战。传统向量量化(VQ)直接应用于MoE会因专家间冗余表示导致码本容量浪费，以及专家聚合过程中的累积偏差引发分布偏移，造成严重的性能下降，亟需专门的量化方案来解决这些难题。

Method: 提出KBVQ-MoE框架，包含两大关键技术：(1) 输入驱动的冗余消除：利用Karhunen-Loeve变换(KLT)引导的奇异值分解(SVD)提取权重主要成分并在专家间共享；(2) 偏差校正的输出稳定：仅对专家特异性表示进行量化，并通过通道-wise仿射补偿机制校正量化输出，从而缓解分布偏移。

Result: 在多种MoE大模型上的实验表明，KBVQ-MoE显著优于现有量化方法。具体而言，对Qwen1.5-MoE-A2.7B进行3比特量化后，平均准确率达67.99，与FP16基线68.07几乎持平，证明该方法在极低比特率下仍能保持模型性能。

Conclusion: KBVQ-MoE通过有效消除冗余和校正偏差，成功实现了MoE模型的超低比特量化，精度损失极小，展现了在边缘设备和资源受限平台上高效部署MoE大模型的巨大潜力，为轻量化部署提供了新思路。

Abstract: Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained environments. Vector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by leveraging a codebook, where weight vectors are mapped to the most similar discrete codewords. Yet, directly applying VQ to MoEs often leads to substantial performance degradation due to two critical obstacles: (1) redundant representations among experts cause VQ to repeatedly quantize similar representations for each expert, resulting in inefficient use of limited codebook capacity; and (2) cumulative output bias is amplified by expert aggregation in MoE layers, leading to distributional shifts in the quantized outputs. To address these issues, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. KBVQ-MoE integrates two techniques: (1) input-driven redundancy elimination, where a Karhunen-Loeve Transform (KLT) guided singular value decomposition (SVD) extracts dominant weight components and shares them across experts; and (2) bias-corrected output stabilization, where vector quantization is applied only to expert-specific (non-redundant) representations and the quantized outputs are corrected via channel-wise affine compensation. Experiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For example, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring KBVQ-MoE's potential for efficient deployment on edge devices and other resource-constrained platforms.

</details>


### [78] [Spectra: Rethinking Optimizers for LLMs Under Spectral Anisotropy](https://arxiv.org/abs/2602.11185)
*Zhendong Huang,Hengjie Cao,Fang Dong,Ruijun Huang,Mengyi Chen,Yifeng Yang,Xin Zhang,Anrui Chen,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Qin Lv,Robert P. Dick,Yuan Cheng,Fan Yang,Tun Lu,Li Shang*

Main category: cs.LG

TL;DR: 本文发现LLM训练梯度信号呈高度各向异性：重复语言结构能量集中于少数主导谱方向（spike），而上下文信息分布于谱长尾。spike仅占1.5%方向却主导优化器统计，抑制tail学习。为此提出Spectra优化器，通过缓存热启动幂迭代追踪spike子空间，抑制主导spike而不放大噪声敏感tail，实现低秩谱整形。在LLaMA3 8B上，Spectra比AdamW快30%达到相同损失，内存降49.25%，准确率升1.62%，且比Muon快5.1倍。


<details>
  <summary>Details</summary>
Motivation: 论文动机源于LLM训练中梯度信号的各向异性特性：递归语言结构使能量聚集在少数主导谱方向（spike），而上下文特定信息位于谱长尾。spike虽仅占约1.5%方向，却主导优化器统计量，通过第二矩归一化压缩tail更新，收紧全局稳定学习率边界，从而抑制长尾学习效率。这促使作者设计spike感知优化器以解决此瓶颈。

Method: 提出Spectra优化器，核心方法包括：1) 使用缓存的热启动幂迭代追踪spike子空间；2) 对低秩spike子空间进行抑制；3) 应用低秩谱整形，在不放大噪声敏感谱长尾前提下调整优化行为。该方法实现可忽略的计算开销和显著降低的优化器状态内存。

Result: LLaMA3 8B模型50B tokens训练结果表明：Spectra比AdamW快30%达到相同目标损失，每步端到端开销减0.7%，优化器状态内存降49.25%，平均下游准确率升1.62%。相比Muon，Spectra优化处理速度快5.1倍，最终损失更低，平均准确率提升0.66%。

Conclusion: 研究揭示了梯度各向异性及spike-tail分离对LLM优化的根本影响。Spectra通过显式抑制主导spike子空间，有效缓解长尾学习抑制问题，在训练效率、内存占用和模型性能上均显著优于AdamW和Muon，为大规模语言模型训练提供了更优的优化算法。

Abstract: Gradient signals in LLM training are highly anisotropic: recurrent linguistic structure concentrates energy into a small set of dominant spectral directions, while context specific information resides in a long tail. We show that this spike tail separation persists throughout training, with the spike occupying only about 1.5% of directions yet dominating optimizer statistics. This dominance suppresses tail learning by contracting tail updates through second moment normalization and tightening the globally stable learning rate bound. Motivated by this analysis, we propose Spectra, a spike aware optimizer that suppresses the dominant low rank spike subspace without amplifying the noise sensitive spectral tail. Spectra tracks the spike subspace via cached, warm started power iteration and applies low rank spectral shaping with negligible overhead and substantially reduced optimizer state memory. On LLaMA3 8B trained on 50B tokens, Spectra reaches the same target loss 30% faster than AdamW, reduces per step end to end overhead by 0.7%, cuts optimizer state memory by 49.25%, and improves average downstream accuracy by 1.62%. Compared to Muon, Spectra is 5.1x faster in optimizer processing time, achieves a lower final loss, and improves average accuracy by 0.66%.

</details>


### [79] [TDPNavigator-Placer: Thermal- and Wirelength-Aware Chiplet Placement in 2.5D Systems Through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.11187)
*Yubo Hou,Furen Zhuang,Partha Pratim Kundu,Sezin Ata Kircali,Jie Wang,Mihai Dragos Rotaru,Dutta Rahul,Ashish James*

Main category: cs.LG

TL;DR: 针对2.5D集成电路中芯片布局的线长与散热冲突问题，本文提出TDPNavigator-Placer——一种基于多智能体强化学习的框架，通过为不同目标分配专用智能体来优化Pareto前沿，实现更平衡的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着电子设备快速发展，2.5D集成电路的芯片布局需求日益增长。现有方法要么只关注线长最小化，要么将多目标简化为单目标加权求和，无法有效处理线长缩减与散热管理之间的内在冲突，导致实际部署受限。

Method: 提出TDPNavigator-Placer，采用多智能体强化学习框架，根据芯片的热设计功率(TDP)动态优化布局。将冲突目标分配给具有不同奖励机制和环境约束的专用智能体，在统一范式下协同工作。

Result: 实验结果表明，与先进方法相比，TDPNavigator-Placer能产生显著更优的Pareto前沿。

Conclusion: 该方法能够在2.5D集成电路的芯片布局中实现线长与散热性能之间更平衡的权衡，为异构芯片组装提供了有效的自动化布局解决方案。

Abstract: The rapid growth of electronics has accelerated the adoption of 2.5D integrated circuits, where effective automated chiplet placement is essential as systems scale to larger and more heterogeneous chiplet assemblies. Existing placement methods typically focus on minimizing wirelength or transforming multi-objective optimization into a single objective through weighted sum, which limits their ability to handle competing design requirements. Wirelength reduction and thermal management are inherently conflicting objectives, making prior approaches inadequate for practical deployment. To address this challenge, we propose TDPNavigator-Placer, a novel multi-agent reinforcement learning framework that dynamically optimizes placement based on chiplet's thermal design power (TDP). This approach explicitly assigns these inherently conflicting objectives to specialized agents, each operating under distinct reward mechanisms and environmental constraints within a unified placement paradigm. Experimental results demonstrate that TDPNavigator-Placer delivers a significantly improved Pareto front over state-of-the-art methods, enabling more balanced trade-offs between wirelength and thermal performance.

</details>


### [80] [MELINOE: Fine-Tuning Enables Memory-Efficient Inference for Mixture-of-Experts Models](https://arxiv.org/abs/2602.11192)
*Arian Raje,Anupam Nayak,Gauri Joshi*

Main category: cs.LG

TL;DR: MELINOE通过微调MoE模型使其更倾向于激活少量专家，并利用GPU缓存减少专家切换和CPU-GPU传输开销，在提升推理吞吐量的同时保持或提升模型性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然能减少每token的激活参数，但其巨大的总参数量和模型尺寸使其难以在资源受限环境中部署，因为所有参数都需加载到GPU内存。现有方法通过将专家卸载到CPU内存来缓解内存瓶颈，但专家传输带来的显著I/O延迟限制了其实际效果。

Method: MELINOE对MoE模型进行微调，使其更强烈地偏好为每个序列激活更少的专家。通过缓存这些被偏好的专家到GPU内存中，减少专家切换和CPU-GPU传输开销。

Result: 相比高效基线，MELINOE将吞吐量提高了1.2-3倍；相比传输密集型基线，最高提升14.7倍，同时在下游任务上保持甚至提升了模型性能。

Conclusion: MELINOE是一种可靠的方法，能够显著改善MoE模型的推理效率，在保持模型性能的同时实现更高的吞吐量，为资源受限环境部署大型MoE模型提供了有效解决方案。

Abstract: Mixture-of-Experts (MoE) model architectures can significantly reduce the number of activated parameters per token, enabling computationally efficient training and inference. However, their large overall parameter counts and model sizes have precluded their widespread usage in resource-constrained settings as all of the parameters must still be loaded into GPU memory. Prior works aim to address this memory bottleneck by offloading certain experts into CPU memory and porting them to GPU memory only when they are activated. In practice, these methods suffer from the significant I/O latency incurred by expert transfer. We present MELINOE, a method that fine-tunes an MoE model to more strongly prefer activating a smaller number of experts per sequence. Caching these preferred experts in GPU memory reduces expert churn and CPU-GPU transfer overhead. MELINOE increases throughput by $1.2-3\times$ over efficient baselines and up to $14.7\times$ over transfer-heavy baselines while retaining or even improving the performance of the model on a downstream task, making it a reliable method for improving MoE inference efficiency.

</details>


### [81] [Predicting the post-wildfire mudflow onset using machine learning models on multi-parameter experimental data](https://arxiv.org/abs/2602.11194)
*Mahta Movasat,Ingrid Tomac*

Main category: cs.LG

TL;DR: 本研究运用机器学习技术预测野火后泥石流灾害，通过实验室模拟不同土壤类型、降雨强度与坡度条件下的坡面流，揭示细砂在低强度长历时降雨下极易侵蚀，且高强度降雨前10分钟为泥石流启动的关键窗口期。


<details>
  <summary>Details</summary>
Motivation: 野火频发导致土壤疏水性增强，尤其在砂质边坡上，火后泥石流破坏力远超自然泥石流。亟需厘清降雨强度、坡度、土壤粒径等关键参数对泥石流启动的耦合影响，为灾害预警提供理论支撑。

Method: 研究采用倾斜水槽实验模拟野外条件，使用降雨装置对多种土壤进行测试。应用多元线性回归(MLR)、逻辑回归(LR)、支持向量分类器(SVC)、K-means聚类及主成分分析(PCA)等机器学习算法，分别进行预测、分类和降维分析。

Result: MLR可准确预测总径流量，但对侵蚀量预测精度有限，尤其对粗砂；LR与SVC成功分类破坏结果。敏感性分析显示细砂对侵蚀高度敏感，低强度长历时降雨易触发破坏；高强度降雨的前10分钟是径流与破坏的最关键时段。

Conclusion: 机器学习为野火后灾害风险评估和应急响应规划提供了有效工具，能够捕捉复杂系统的非线性特征，对提升地质灾害预警能力具有重要价值。

Abstract: Post-wildfire mudflows are increasingly hazardous due to the prevalence of wildfires, including those on the wildland-urban interface. Upon burning, soil on the surface or immediately beneath becomes hydrophobic, a phenomenon that occurs predominantly on sand-based hillslopes. Rainwater and eroded soil blanket the downslope, leading to catastrophic debris flows. Soil hydrophobicity enhances erosion, resulting in post-wildfire debris flows that differ from natural mudflows in intensity, duration, and destructiveness. Thus, it is crucial to understand the timing and conditions of debris-flow onset, driven by the coupled effects of critical parameters: varying rain intensities (RI), slope gradients, water-entry values, and grain sizes (D50). Machine Learning (ML) techniques have become increasingly valuable in geotechnical engineering due to their ability to model complex systems without predefined assumptions. This study applies multiple ML algorithms: multiple linear regression (MLR), logistic regression (LR), support vector classifier (SVC), K-means clustering, and principal component analysis (PCA) to predict and classify outcomes from laboratory experiments that model field conditions using a rain device on various soils in sloped flumes. While MLR effectively predicted total discharge, erosion predictions were less accurate, especially for coarse sand. LR and SVC achieved good accuracy in classifying failure outcomes, supported by clustering and dimensionality reduction. Sensitivity analysis revealed that fine sand is highly susceptible to erosion, particularly under low-intensity, long-duration rainfall. Results also show that the first 10 minutes of high-intensity rain are most critical for discharge and failure. These findings highlight the potential of ML for post-wildfire hazard assessment and emergency response planning.

</details>


### [82] [AM-FM: A Foundation Model for Ambient Intelligence Through WiFi](https://arxiv.org/abs/2602.11200)
*Guozhen Zhu,Yuqian Hu,Sakila Jayaweera,Weihang Gao,Wei-Hsiang Wang,Jiaxuan Zhang,Beibei Wang,Chenshu Wu,K. J. Ray Liu*

Main category: cs.LG

TL;DR: 本文提出AM-FM，首个基于WiFi的环境智能感知基础模型。通过在920万条未标记CSI数据上进行预训练，采用对比学习、掩码重建和物理信息目标，该模型在九项下游任务中展现出强大的跨任务性能和数据效率，证明了基础模型可利用现有无线基础设施实现可扩展的环境智能。


<details>
  <summary>Details</summary>
Motivation: 环境智能需持续理解物理空间中的人类存在、活动和生理状态，对智能环境、健康监测和人机交互至关重要。WiFi基础设施虽提供了一种无处不在、始终在线且保护隐私的感知平台，但其潜力尚未充分挖掘。现有无线感知方法依赖特定任务模型，需要大量标注数据，限制了实际部署。

Method: 提出AM-FM基础模型，在439天内从20种商用设备收集的920万条未标记信道状态信息(CSI)样本上进行预训练。采用三种定制化的学习目标：对比学习、掩码重建和物理信息目标，以学习适用于无线信号的通用表示。

Result: 在涵盖九项下游任务的公开基准测试上，AM-FM展现出强大的跨任务性能，并显著提高数据效率，表明基础模型能够利用现有无线基础设施实现可扩展的环境智能。

Conclusion: 本研究证明，基于WiFi的基础模型AM-FM通过大规模无监督预训练和无线信号定制化的学习目标，能够学习通用感知表示，在多项环境智能任务上实现高性能和高效数据利用，为利用现有WiFi基础设施实现大规模环境智能应用开辟了新途径。

Abstract: Ambient intelligence, continuously understanding human presence, activity, and physiology in physical spaces, is fundamental to smart environments, health monitoring, and human-computer interaction. WiFi infrastructure provides a ubiquitous, always-on, privacy-preserving substrate for this capability across billions of IoT devices. Yet this potential remains largely untapped, as wireless sensing has typically relied on task-specific models that require substantial labeled data and limit practical deployment. We present AM-FM, the first foundation model for ambient intelligence and sensing through WiFi. AM-FM is pre-trained on 9.2 million unlabeled Channel State Information (CSI) samples collected over 439 days from 20 commercial device types deployed worldwide, learning general-purpose representations via contrastive learning, masked reconstruction, and physics-informed objectives tailored to wireless signals. Evaluated on public benchmarks spanning nine downstream tasks, AM-FM shows strong cross-task performance with improved data efficiency, demonstrating that foundation models can enable scalable ambient intelligence using existing wireless infrastructure.

</details>


### [83] [Zero-Sacrifice Persistent-Robustness Adversarial Defense for Pre-Trained Encoders](https://arxiv.org/abs/2602.11204)
*Zhuxin Lei,Ziyuan Yang,Yi Zhang*

Main category: cs.LG

TL;DR: 本文针对自监督学习预训练编码器面临的下游无关对抗样本(DAEs)漏洞，提出了一种零牺牲持久鲁棒对抗防御方法ZePAD。该方法采用双分支结构，通过多模式对抗增强分支和良性记忆保持分支，仅单次调优即可在多个下游任务上同时提升对抗鲁棒性和良性性能，实现零牺牲防御。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖任务特定的对抗微调，导致泛化能力受限、灾难性遗忘和良性性能下降。需要一种更严格的防御目标：单次调优即可防御DAEs并保持良性性能。

Method: ZePAD采用双分支结构：MPAE分支使用两个对抗微调编码器增强对抗抵抗；BMP分支在本地数据上训练以保持良性性能。通过评估分支置信度可直接检测DAEs，无需训练时引入对抗样本识别任务。特征多样性使单次对抗微调可跨任务防御DAEs。

Result: 在11种SSL方法和6个数据集上的实验表明，该方法有效实现了零牺牲防御，良性性能提升达29.20%，对抗鲁棒性提升达73.86%，验证了持久鲁棒性。

Conclusion: ZePAD成功实现了单次调优即可在多样下游任务上防御下游无关对抗样本并保持良性性能的防御目标，展现了零牺牲特性和持久鲁棒性。

Abstract: The widespread use of publicly available pre-trained encoders from self-supervised learning (SSL) has exposed a critical vulnerability: their susceptibility to downstream-agnostic adversarial examples (DAEs), which are crafted without knowledge of the downstream tasks but capable of misleading downstream models. While several defense methods have been explored recently, they rely primarily on task-specific adversarial fine-tuning, which inevitably limits generalizability and causes catastrophic forgetting and deteriorates benign performance. Different with previous works, we propose a more rigorous defense goal that requires only a single tuning for diverse downstream tasks to defend against DAEs and preserve benign performance. To achieve this defense goal, we introduce Zero-Sacrifice Persistent-Robustness Adversarial Defense (ZePAD), which is inspired by the inherent sensitivity of neural networks to data characteristics. Specifically, ZePAD is a dual-branch structure, which consists of a Multi-Pattern Adversarial Enhancement Branch (MPAE-Branch) that uses two adversarially fine-tuned encoders to strengthen adversarial resistance. The Benign Memory Preservation Branch (BMP-Branch) is trained on local data to ensure adversarial robustness does not compromise benign performance. Surprisingly, we find that ZePAD can directly detect DAEs by evaluating branch confidence, without introducing any adversarial exsample identification task during training. Notably, by enriching feature diversity, our method enables a single adversarial fine-tuning to defend against DAEs across downstream tasks, thereby achieving persistent robustness. Extensive experiments on 11 SSL methods and 6 datasets validate its effectiveness. In certain cases, it achieves a 29.20% improvement in benign performance and a 73.86% gain in adversarial robustness, highlighting its zero-sacrifice property.

</details>


### [84] [Adaptive Physics Transformer with Fused Global-Local Attention for Subsurface Energy Systems](https://arxiv.org/abs/2602.11208)
*Xin Ju,Nok Hei,Fung,Yuyan Zhang,Carl Jacquemyn,Matthew Jackson,Randolph Settgast,Sally M. Benson,Gege Wen*

Main category: cs.LG

TL;DR: 针对地球物理地下模拟计算成本高的问题，本文提出了一种名为自适应物理Transformer（APT）的神经算子，该模型融合图编码器与全局注意力机制，可直接从自适应网格细化模拟中学习，在规则和不规则网格上均表现优异，为大规模地下基础模型开发提供了可扩展的骨干架构。


<details>
  <summary>Details</summary>
Motivation: 地球地下层是现代社会的基石，提供油气、地热、矿产等关键能源资源，同时也是二氧化碳封存的主要场所。然而，由于地质异质性、高分辨率要求以及具有不同传播时间尺度的物理过程紧密耦合，全物理数值模拟计算成本极高，亟需高效的替代方案。

Method: 提出自适应物理Transformer（APT），这是一种与几何、网格和物理无关的神经算子。该模型融合图编码器提取高分辨率局部异质特征，并结合全局注意力机制解决长程物理影响。APT是首个可直接从自适应网格细化模拟中学习的架构。

Result: APT在规则和不规则网格的地下任务中均优于现有最先进架构，具备强大的超分辨率能力。同时展示了APT的跨数据集学习能力。

Conclusion: APT被定位为大规模地下基础模型开发的稳健且可扩展的骨干架构，为未来相关研究提供了新方向。

Abstract: The Earth's subsurface is a cornerstone of modern society, providing essential energy resources like hydrocarbons, geothermal, and minerals while serving as the primary reservoir for $CO_2$ sequestration. However, full physics numerical simulations of these systems are notoriously computationally expensive due to geological heterogeneity, high resolution requirements, and the tight coupling of physical processes with distinct propagation time scales. Here we propose the \textbf{Adaptive Physics Transformer} (APT), a geometry-, mesh-, and physics-agnostic neural operator that explicitly addresses these challenges. APT fuses a graph-based encoder to extract high-resolution local heterogeneous features with a global attention mechanism to resolve long-range physical impacts. Our results demonstrate that APT outperforms state-of-the-art architectures in subsurface tasks across both regular and irregular grids with robust super-resolution capabilities. Notably, APT is the first architecture that directly learns from adaptive mesh refinement simulations. We also demonstrate APT's capability for cross-dataset learning, positioning it as a robust and scalable backbone for large-scale subsurface foundation model development.

</details>


### [85] [Towards Compressive and Scalable Recurrent Memory](https://arxiv.org/abs/2602.11212)
*Yunchong Song,Jushi Kai,Liming Lu,Kaixi Qiu,Zhouhan Lin*

Main category: cs.LG

TL;DR: 本文提出Elastic Memory，一种基于HiPPO框架的新型记忆架构，旨在解决Transformer在长上下文下注意力计算的二次复杂度瓶颈。该方法将历史序列视为连续信号的采样，通过最优在线压缩将其编码为固定大小记忆状态，并采用灵活的多项式采样机制进行检索重建。在32k+长上下文任务中，Elastic Memory相比Memorizing Transformer实现16倍内存效率，并在所有记忆规模上优于Melodi，即使后者参数量多30%。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理长上下文时面临注意力机制的二次计算复杂度瓶颈。现有循环记忆方法虽能扩展上下文窗口，但存在理论原则与实际可扩展性之间的根本权衡，亟需一种既能保持理论最优性又具备实际可扩展性的解决方案。

Method: 作者提出Elastic Memory架构，其核心是基于HiPPO（High-order Polynomial Projection Operators）框架的在线函数逼近理论。该方法将历史序列视为连续信号的离散采样，通过最优在线压缩算法将其编码为固定维度的记忆状态；检索阶段采用灵活的多项式采样机制，从压缩状态中重建历史摘要。该解耦设计允许在测试时注入归纳偏置以提升性能。

Result: Elastic Memory在跨越三个领域的32k+长上下文数据集上持续优于基线方法。参数量相同情况下，相比Memorizing Transformer内存效率提升16倍；在所有记忆规模上均超越Melodi，即使Melodi参数量多30%。模型缩放时，Elastic Memory保持全面领先，且在4倍模型规模下显著快于Melodi。

Conclusion: Elastic Memory通过HiPPO框架的最优压缩与多项式采样机制，成功解决了长上下文建模中的理论-实践权衡问题。其解耦架构提供了测试时注入归纳偏置的灵活性，为Transformer上下文扩展提供了一种可扩展且高效的解决方案，突破了当前记忆方法的性能瓶颈。

Abstract: Transformers face a quadratic bottleneck in attention when scaling to long contexts. Recent approaches introduce recurrent memory to extend context beyond the current window, yet these often face a fundamental trade-off between theoretical principles and practical scalability. To address this, we introduce Elastic Memory, a novel memory architecture grounded in the HiPPO framework for online function approximation. Elastic Memory treats historical sequence as samples from continuous signals, applying optimal online compression to encode them into a fixed-size memory state. For retrieval, we propose a flexible \textit{polynomial sampling} mechanism that reconstructs a history summary from this compressed state. Elastic Memory consistently outperformed baselines on long-context (32k+) datasets across three domains. With equal parameters, it beat Memorizing Transformer by 16x memory and outperformed Melodi at all memory sizes, even when Melodi had 30% more parameters. When scaling model size, Elastic Memory stayed ahead of all baselines and was significantly faster than Melodi at 4x size. Furthermore, its decoupled design allows for injecting inductive biases at test-time to boost performance.

</details>


### [86] [Charting Empirical Laws for LLM Fine-Tuning in Scientific Multi-Discipline Learning](https://arxiv.org/abs/2602.11215)
*Lintao Wang,Zhuqiang Lu,Yilin Zhu,Kun Hu,Zhenfei Yin,Shixiang Tang,Zhiyong Wang,Wanli Ouyang,Xinzhu Ma*

Main category: cs.LG

TL;DR: 这是首篇系统性研究多学科LLM微调的工作。作者构建了五学科语料库，分析了全微调、LoRA、LoRA-MoE和LoRA组合的学习模式。研究发现多学科学习比单学科训练更不稳定，并提炼出四条经验法则：平衡优先多样性、合并后对齐、优化后扩展、共享后特化。这些法则为多学科微调提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在单个科学领域通过微调取得了强劲性能，但其在多学科背景下的学习动态仍鲜为人知。尽管跨领域知识协同有望提升泛化能力和广泛适用性，但现有研究未能充分探索这一方向。本研究旨在填补这一空白，为开发可泛化的科学大语言模型提供科学依据。

Method: 本研究首次系统性地考察了多学科LLM微调。首先构建了一个涵盖五个学科领域的语料库。然后分析了四种微调策略的学习模式：全参数微调、LoRA（低秩自适应）、LoRA-MoE（混合专家）以及LoRA组合方法。通过对比实验揭示了多学科学习的动态特性。

Result: 研究发现多学科学习比单学科训练显著更加不稳定，并总结出四条一致的经验法则：1）平衡优先多样性法则：低资源学科会导致性能下降，除非采用多样性感知的上采样策略；2）合并后对齐法则：恢复指令遵循能力对于跨学科协同至关重要；3）优化后扩展法则：如果没有先前的优化设计，参数扩展带来的增益有限；4）共享后特化法则：非对称LoRA-MoE通过共享低秩投影以极少的可训练参数实现稳健增益。

Conclusion: 这四条经验法则共同构成了多学科微调的原则性实践指南，为开发可泛化的科学大语言模型提供了可操作的指导。研究表明，通过遵循这些规律，可以更有效地利用多学科数据进行模型训练，实现更好的跨领域知识协同和泛化性能。

Abstract: While large language models (LLMs) have achieved strong performance through fine-tuning within individual scientific domains, their learning dynamics in multi-disciplinary contexts remains poorly understood, despite the promise of improved generalization and broader applicability through cross-domain knowledge synergy. In this work, we present the first systematic study of multi-disciplinary LLM fine-tuning, constructing a five-discipline corpus and analyzing learning patterns of full fine-tuning, LoRA, LoRA-MoE, and LoRA compositions. Particularly, our study shows that multi-disciplinary learning is substantially more variable than single-discipline training and distills four consistent empirical laws: (1) Balance-then-Diversity: low-resource disciplines degrade performance unless mitigated via diversity-aware upsampling; (2) Merge-then-Align: restoring instruction-following ability is critical for cross-discipline synergy; (3) Optimize-then-Scale: parameter scaling offers limited gains without prior design optimization; and (4) Share-then-Specialize: asymmetric LoRA-MoE yields robust gains with minimal trainable parameters via shared low-rank projection. Together, these laws form a practical recipe for principled multi-discipline fine-tuning and provide actionable guidance for developing generalizable scientific LLMs.

</details>


### [87] [Protein Language Model Embeddings Improve Generalization of Implicit Transfer Operators](https://arxiv.org/abs/2602.11216)
*Panagiotis Antoniadis,Beatrice Pavesi,Simon Olsson,Ole Winther*

Main category: cs.LG

TL;DR: 该论文提出PLaTITO方法，通过整合蛋白质语言模型嵌入等辅助信息，显著提升可迁移隐式转移算子(TITO)在分子动力学模拟中的数据效率和跨系统泛化能力，在蛋白质平衡采样基准测试中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学受限于高计算成本，生成式分子动力学虽能高效采样但跨分子系统迁移能力不足。本文旨在通过引入辅助信息源来改善可迁移隐式转移算子的数据效率和泛化性能，克服现有方法的局限性。

Method: 在粗粒化TITO模型基础上整合蛋白质语言模型(pLM)嵌入作为条件信号，构建了PLaTITO方法。同时系统研究了结构嵌入、温度和大型语言模型衍生嵌入等多种附加条件信号对模型性能的影响机制。

Result: 粗粒化TITO模型比玻尔兹曼模拟器数据效率显著更高；pLM嵌入进一步提升了分布外泛化能力；PLaTITO在包括快速折叠蛋白质在内的分布外蛋白质系统平衡采样基准测试中达到最先进性能。

Conclusion: 整合辅助信息是提升分子动力学生成模型跨系统迁移能力的有效策略，蛋白质语言模型嵌入可作为通用条件信号显著增强模型泛化性，为开发更通用的AI辅助分子模拟方法提供了新方向。

Abstract: Molecular dynamics (MD) is a central computational tool in physics, chemistry, and biology, enabling quantitative prediction of experimental observables as expectations over high-dimensional molecular distributions such as Boltzmann distributions and transition densities. However, conventional MD is fundamentally limited by the high computational cost required to generate independent samples. Generative molecular dynamics (GenMD) has recently emerged as an alternative, learning surrogates of molecular distributions either from data or through interaction with energy models. While these methods enable efficient sampling, their transferability across molecular systems is often limited. In this work, we show that incorporating auxiliary sources of information can improve the data efficiency and generalization of transferable implicit transfer operators (TITO) for molecular dynamics. We find that coarse-grained TITO models are substantially more data-efficient than Boltzmann Emulators, and that incorporating protein language model (pLM) embeddings further improves out-of-distribution generalization. Our approach, PLaTITO, achieves state-of-the-art performance on equilibrium sampling benchmarks for out-of-distribution protein systems, including fast-folding proteins. We further study the impact of additional conditioning signals -- such as structural embeddings, temperature, and large-language-model-derived embeddings -- on model performance.

</details>


### [88] [The Magic Correlations: Understanding Knowledge Transfer from Pretraining to Supervised Fine-Tuning](https://arxiv.org/abs/2602.11217)
*Simin Fan,Dimitris Paparas,Natasha Noy,Binbin Xiong,Noveen Sachdeva,Berivan Isik*

Main category: cs.LG

TL;DR: 本文系统研究语言模型从预训练到监督微调的能力迁移机制，通过相关性分析发现迁移可靠性在能力类别、基准和模型规模间存在显著差异，准确率与置信度呈现不同的缩放动态，为模型开发和数据筛选提供重要指导。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型能力从预训练迁移至监督微调的机制是高效模型开发与数据筛选的基础。现有研究缺乏对跨阶段能力迁移的系统性分析，导致资源分配与基准选择缺乏科学依据，亟需探索预训练决策与下游性能的关联规律。

Method: 研究设计了一套相关性分析协议，在多样化的数据混合和模型规模上，对预训练与监督微调阶段的准确率和置信度指标进行系统性评估，通过跨阶段相关性分析回答四个核心研究问题。

Result: 实验揭示：1）预训练阶段的准确率与置信度排序在SFT后保持程度因能力类别而异；2）不同基准的跨阶段预测能力差异显著，部分基准不可靠；3）迁移动态随模型规模变化呈现不同模式；4）准确率与置信度的校准质量呈现不同甚至相反的缩放动态。

Conclusion: 研究揭示了预训练决策与下游性能间的复杂相互作用，发现能力迁移可靠性在类别、基准和规模维度存在巨大异质性。这些发现为基准选择、数据筛选和高效模型开发提供了可操作的指导，强调了跨阶段评估的重要性。

Abstract: Understanding how language model capabilities transfer from pretraining to supervised fine-tuning (SFT) is fundamental to efficient model development and data curation. In this work, we investigate four core questions: RQ1. To what extent do accuracy and confidence rankings established during pretraining persist after SFT? RQ2. Which benchmarks serve as robust cross-stage predictors and which are unreliable? RQ3. How do transfer dynamics shift with model scale? RQ4. How well does model confidence align with accuracy, as a measure of calibration quality? Does this alignment pattern transfer across training stages? We address these questions through a suite of correlation protocols applied to accuracy and confidence metrics across diverse data mixtures and model scales. Our experiments reveal that transfer reliability varies dramatically across capability categories, benchmarks, and scales -- with accuracy and confidence exhibiting distinct, sometimes opposing, scaling dynamics. These findings shed light on the complex interplay between pretraining decisions and downstream outcomes, providing actionable guidance for benchmark selection, data curation, and efficient model development.

</details>


### [89] [Credal Concept Bottleneck Models: Structural Separation of Epistemic and Aleatoric Uncertainty](https://arxiv.org/abs/2602.11219)
*Tanmoy Mukherjee,Marius Kloft,Pierre Marquis,Zied Bouraoui*

Main category: cs.LG

TL;DR: 该论文针对预测不确定性中认知不确定性（模型无知）和偶然不确定性（数据模糊）难以分离的问题，提出了一种基于可信集表示的变分概念瓶颈模型，通过两个独立的不确定性头和分离的训练目标，实现了两种不确定性在构建层面的解耦，显著降低了它们之间的相关性并提升了各自的语义一致性。


<details>
  <summary>Details</summary>
Motivation: 可靠决策需要将预测不确定性分解为认知不确定性（模型无知）和偶然不确定性（数据模糊）两个部分。然而现有方法通常从同一预测分布中估计这两种不确定性，导致它们高度相关，语义模糊，无法真正区分模型 ignorance 和数据 ambiguity。

Method: 提出一种可信集（credal-set）表示方法，将不确定性建模为预测分布的集合。认知不确定性对应集合大小（几何范围），偶然不确定性对应集合内元素的噪声程度。具体实现为变分可信概念瓶颈模型（Variational Credal Concept Bottleneck Model），配备两个独立的不确定性头，分别由不同的训练目标优化且梯度路径不重叠，从而实现构建性分离而非事后分解。

Result: 在多标注者基准测试上，该方法将认知不确定性与偶然不确定性之间的相关性降低了超过一个数量级，同时显著提升了认知不确定性与预测误差的对齐度，以及偶然不确定性与真实标注模糊性的对齐度。

Conclusion: 通过可信集表示和双头分离架构，该研究成功实现了预测不确定性中认知和偶然成分的构建性解耦，为可靠决策提供了更清晰、更可靠的不确定性量化方法。

Abstract: Decomposing predictive uncertainty into epistemic (model ignorance) and aleatoric (data ambiguity) components is central to reliable decision making, yet most methods estimate both from the same predictive distribution. Recent empirical and theoretical results show these estimates are typically strongly correlated, so changes in predictive spread simultaneously affect both components and blur their semantics. We propose a credal-set formulation in which uncertainty is represented as a set of predictive distributions, so that epistemic and aleatoric uncertainty correspond to distinct geometric properties: the size of the set versus the noise within its elements. We instantiate this idea in a Variational Credal Concept Bottleneck Model with two disjoint uncertainty heads trained by disjoint objectives and non-overlapping gradient paths, yielding separation by construction rather than post hoc decomposition. Across multi-annotator benchmarks, our approach reduces the correlation between epistemic and aleatoric uncertainty by over an order of magnitude compared to standard methods, while improving the alignment of epistemic uncertainty with prediction error and aleatoric uncertainty with ground-truth ambiguity.

</details>


### [90] [Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT](https://arxiv.org/abs/2602.11220)
*Jiacheng Wang,Ping Jian,Zhen Yang,Zirong Chen,Keren Liao,Zhongbin Guo*

Main category: cs.LG

TL;DR: 本文提出一种基于强化学习的数据重写方法，通过优化重写策略使其与大型语言模型的自然问答生成分布对齐并保持多样性，从而在减少监督微调中灾难性遗忘的同时维持下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调在应对分布偏移的下游数据时会导致灾难性遗忘。现有数据重写方法因依赖模板诱导的条件分布，存在与模型自然生成分布失配及多样性崩溃的问题，亟需自动化的重写策略学习机制。

Method: 将数据重写建模为策略学习问题，利用强化学习优化重写分布。智能体在任务一致性硬约束下，联合优化问答风格分布对齐与多样性目标，构建高质量重写数据集用于下游微调。

Result: 所提方法在实现与标准监督微调相当下游性能的同时，平均减少非下游基准测试中12.34%的灾难性遗忘。

Conclusion: 强化学习驱动的数据重写智能体有效解决了分布失配与多样性退化问题，为大型语言模型高效适配下游场景提供了创新的数据中心范式。

Abstract: Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model's prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model's natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone's QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112 .

</details>


### [91] [Learning Glioblastoma Tumor Heterogeneity Using Brain Inspired Topological Neural Networks](https://arxiv.org/abs/2602.11234)
*Ankita Paul,Wenyi Wang*

Main category: cs.LG

TL;DR: 针对胶质母细胞瘤预后预测中因肿瘤异质性和多中心MRI协议差异导致的模型泛化性差的问题，本文提出TopoGBM框架，通过拓扑正则化的3D卷积自编码器在压缩潜空间中保留肿瘤流形的非欧几里得不变量，实现异质性保持且扫描仪鲁棒的表示学习，在跨机构验证中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤(GBM)具有极端空间结构异质性，且不同机构MRI采集协议不一致，严重阻碍了深度学习模型的泛化能力。传统Transformer和深度学习流程难以捕捉多尺度形态特征（如碎片化坏死核心、浸润边缘和不连续强化成分），导致扫描仪特异性伪影和跨中心预后预测性能下降。

Method: 提出TopoGBM学习框架，核心是3D卷积自编码器结合拓扑正则化。通过在压缩潜空间中强制拓扑先验，保留肿瘤流形的复杂非欧几里得不变量，显式建模侵袭性GBM的高方差结构特征，从而从多参数3D MRI中学习异质性保持且扫描仪鲁棒的表示。

Result: 在UPENN、UCSF、RHUH异质化队列及TCGA外部验证集上，TopoGBM测试C指数达0.67，验证集为0.58，性能优于在领域偏移下退化的基线模型。机制可解释性显示：重构残差高度定位于病理异质性区域，肿瘤及健康组织误差显著降低（测试：0.03，验证：0.09）。遮挡归因分析表明约50%预后信号来自肿瘤及多样的瘤周微环境。

Conclusion: 研究表明，融入拓扑先验能够学习到保持形态特征的嵌入表示，在捕捉肿瘤异质性的同时维持跨机构鲁棒性，为GBM临床可靠的预后预测提供了新思路。

Abstract: Accurate prognosis for Glioblastoma (GBM) using deep learning (DL) is hindered by extreme spatial and structural heterogeneity. Moreover, inconsistent MRI acquisition protocols across institutions hinder generalizability of models. Conventional transformer and DL pipelines often fail to capture the multi-scale morphological diversity such as fragmented necrotic cores, infiltrating margins, and disjoint enhancing components leading to scanner-specific artifacts and poor cross-site prognosis. We propose TopoGBM, a learning framework designed to capture heterogeneity-preserved, scanner-robust representations from multi-parametric 3D MRI. Central to our approach is a 3D convolutional autoencoder regularized by a topological regularization that preserves the complex, non-Euclidean invariants of the tumor's manifold within a compressed latent space. By enforcing these topological priors, TopoGBM explicitly models the high-variance structural signatures characteristic of aggressive GBM. Evaluated across heterogeneous cohorts (UPENN, UCSF, RHUH) and external validation on TCGA, TopoGBM achieves better performance (C-index 0.67 test, 0.58 validation), outperforming baselines that degrade under domain shift. Mechanistic interpretability analysis reveals that reconstruction residuals are highly localized to pathologically heterogeneous zones, with tumor-restricted and healthy tissue error significantly low (Test: 0.03, Validation: 0.09). Furthermore, occlusion-based attribution localizes approximately 50% of the prognostic signal to the tumor and the diverse peritumoral microenvironment advocating clinical reliability of the unsupervised learning method. Our findings demonstrate that incorporating topological priors enables the learning of morphology-faithful embeddings that capture tumor heterogeneity while maintaining cross-institutional robustness.

</details>


### [92] [AI-Driven Clinical Decision Support System for Enhanced Diabetes Diagnosis and Management](https://arxiv.org/abs/2602.11237)
*Mujeeb Ur Rehman,Imran Rehan,Sohail Khalid*

Main category: cs.LG

TL;DR: 本研究开发了一种融合专家知识与机器学习的混合式AI临床决策支持系统(AI-CDSS)，用于辅助2型糖尿病诊断。系统基于BMI、空腹血糖和糖化血红蛋白等关键特征，在1298例患者数据上训练验证，诊断准确率高达99.8%，与内分泌专家诊断一致性达98.8%，显著优于非专科医生的85%，为基层医疗提供了高精度辅助诊断工具。


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病诊断对基层医生具有挑战性，尤其在专科医生资源匮乏地区。开发AI-CDSS旨在提升基层医疗诊断准确性，弥补专科医生短缺，改善患者诊疗可及性。

Method: 采用专家驱动与机器学习相结合的混合方法构建AI-CDSS。使用1298例患者数据（训练集n=650，测试集n=648），选取BMI、空腹血糖和糖化血红蛋白作为核心预测特征。进一步开展105例患者的临床试点研究，对比系统与非内分泌专科医生的诊断表现。

Result: 系统在测试集上表现优异：预测糖尿病准确率99.8%，糖尿病前期99.3%，高危人群99.2%，无糖尿病98.8%。与内分泌专家诊断总体一致性达98.8%。试点研究中，AI-CDSS与专家符合率98.5%，而非专科医生仅为85%。试点人群2型糖尿病检出率为45%。

Conclusion: AI-CDSS可作为一种高效准确的2型糖尿病识别工具，在专科医生资源不足的医疗场景中具有重要应用价值，能显著提升基层医疗服务质量与诊断效率。

Abstract: Identifying type 2 diabetes mellitus can be challenging, particularly for primary care physicians. Clinical decision support systems incorporating artificial intelligence (AI-CDSS) can assist medical professionals in diagnosing type 2 diabetes with high accuracy. This study aimed to assess an AI-CDSS specifically developed for the diagnosis of type 2 diabetes by employing a hybrid approach that integrates expert-driven insights with machine learning techniques. The AI-CDSS was developed (training dataset: n = 650) and tested (test dataset: n = 648) using a dataset of 1298 patients with and without type 2 diabetes. To generate predictions, the algorithm utilized key features such as body mass index, plasma fasting glucose, and hemoglobin A1C. Furthermore, a clinical pilot study involving 105 patients was conducted to assess the diagnostic accuracy of the system in comparison to non-endocrinology specialists. The AI-CDSS showed a high degree of accuracy, with 99.8% accuracy in predicting diabetes, 99.3% in predicting prediabetes, 99.2% in identifying at-risk individuals, and 98.8% in predicting no diabetes. The test dataset revealed a 98.8% agreement between endocrinology specialists and the AI-CDSS. Type 2 diabetes was identified in 45% of 105 individuals in the pilot study. Compared with diabetes specialists, the AI-CDSS scored a 98.5% concordance rate, greatly exceeding that of nonendocrinology specialists, who had an 85% agreement rate. These findings indicate that the AI-CDSS has the potential to be a useful tool for accurately identifying type 2 diabetes, especially in situations in which diabetes specialists are not readily available.

</details>


### [93] [Evaluating Memory Structure in LLM Agents](https://arxiv.org/abs/2602.11243)
*Alina Shutova,Alexandra Olenina,Ivan Vinogradov,Anton Sinitsin*

Main category: cs.LG

TL;DR: 本文提出StructMemEval基准测试，用于评估LLM智能体的长期记忆组织能力，而不仅仅是事实回忆。该基准通过交易账本、待办事项列表、树状结构等任务揭示现有检索增强型LLM在复杂记忆层次结构处理上的不足，为未来记忆框架设计提供方向。


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆基准测试主要关注简单事实保留、多跳回忆和时间变化能力，这些能力可通过基础检索增强型LLM实现，无法有效评估复杂的记忆层次结构组织能力。随着记忆架构日益复杂，缺乏系统化评估工具成为制约研究的关键瓶颈。

Method: 作者设计StructMemEval基准测试套件，包含人类通过结构化知识组织解决的任务（如交易账本、待办事项列表、树状结构等），对比评估简单检索增强型LLM与专用记忆智能体的性能差异。

Result: 实验表明：1）基础检索增强型LLM难以完成结构化记忆任务；2）记忆智能体在获得组织提示后可稳定解决；3）现代LLM未提示时往往无法自主识别记忆结构。

Conclusion: 研究结果揭示LLM在自主识别和利用记忆结构方面存在明显局限，强调未来需从LLM训练和记忆框架设计两个维度进行改进，以提升智能体对复杂记忆层次的组织与管理能力。

Abstract: Modern LLM-based agents and chat assistants rely on long-term memory frameworks to store reusable knowledge, recall user preferences, and augment reasoning. As researchers create more complex memory architectures, it becomes increasingly difficult to analyze their capabilities and guide future memory designs. Most long-term memory benchmarks focus on simple fact retention, multi-hop recall, and time-based changes. While undoubtedly important, these capabilities can often be achieved with simple retrieval-augmented LLMs and do not test complex memory hierarchies. To bridge this gap, we propose StructMemEval - a benchmark that tests the agent's ability to organize its long-term memory, not just factual recall. We gather a suite of tasks that humans solve by organizing their knowledge in a specific structure: transaction ledgers, to-do lists, trees and others. Our initial experiments show that simple retrieval-augmented LLMs struggle with these tasks, whereas memory agents can reliably solve them if prompted how to organize their memory. However, we also find that modern LLMs do not always recognize the memory structure when not prompted to do so. This highlights an important direction for future improvements in both LLM training and memory frameworks.

</details>


### [94] [How Many Features Can a Language Model Store Under the Linear Representation Hypothesis?](https://arxiv.org/abs/2602.11246)
*Nikhil Garg,Jon Kleinberg,Kenny Peng*

Main category: cs.LG

TL;DR: 本文建立了语言模型线性表征假说的数学框架，将假说分解为线性表征与线性可及性两个主张。通过线性压缩感知理论，证明在需要线性解码的条件下，神经元数量d的上下界分别为O_ε(k²log m)和Ω_ε(k²/ log k·log(m/k))，揭示了线性可及性比线性表征要求更强的定量差距，并为特征叠加假说提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 当前对语言模型中间层如何存储特征的理解不足，线性表征假说(LRH)认为特征以线性方式存储。该研究旨在量化回答：多少个神经元d足以同时实现m个特征的线性表征和线性访问？经典压缩感知理论在非线性解码下仅需d=O(k log(m/k))，但LRH要求线性解码，这一问题尚未被探索，且对理解模型容量和"叠加假说"至关重要。

Method: 1) 构建线性压缩感知的数学框架，将LRH分解为线性表征（特征线性嵌入激活）和线性可及性（特征可线性解码）两个独立命题。2) 上界证明：采用标准随机矩阵构造法，构建具有近似正交列的测量矩阵。3) 下界证明：结合近单位矩阵的秩界（Alon, 2003）与Turán定理（限制无团图边数）进行组合分析。4) 扩展分析：探讨特征表示的几何约束，以及激活函数和偏置对解码器的影响。

Result: 1) 理论上下界：证明线性压缩感知要求d=Ω_ε(k²/log k·log(m/k))，而d=O_ε(k²log m)已足够，两者几乎匹配。2) 量化差距：下界表明线性解码需求相比经典压缩感知产生多项式级别的额外开销，证实线性可及性是比线性表征更强的假说。3) 叠加假说支持：上界证实神经元可在LRH下存储指数级数量的特征。4) 几何约束：结果对特征表示的几何结构有一定限制但非完全约束。

Conclusion: 本研究建立了线性表征假说的量化理论基础，揭示线性可及性带来本质性计算开销，深化了对语言模型特征存储机制的理解。上界为特征叠加现象提供了严格的理论证据，表明模型可在神经元中高效编码海量特征。未来工作可扩展至非线性激活函数和更复杂的解码架构，进一步连接理论结果与实际模型行为，为可解释性研究和模型压缩提供新视角。

Abstract: We introduce a mathematical framework for the linear representation hypothesis (LRH), which asserts that intermediate layers of language models store features linearly. We separate the hypothesis into two claims: linear representation (features are linearly embedded in neuron activations) and linear accessibility (features can be linearly decoded). We then ask: How many neurons $d$ suffice to both linearly represent and linearly access $m$ features? Classical results in compressed sensing imply that for $k$-sparse inputs, $d = O(k\log (m/k))$ suffices if we allow non-linear decoding algorithms (Candes and Tao, 2006; Candes et al., 2006; Donoho, 2006). However, the additional requirement of linear decoding takes the problem out of the classical compressed sensing, into linear compressed sensing.
  Our main theoretical result establishes nearly-matching upper and lower bounds for linear compressed sensing. We prove that $d = Ω_ε(\frac{k^2}{\log k}\log (m/k))$ is required while $d = O_ε(k^2\log m)$ suffices. The lower bound establishes a quantitative gap between classical and linear compressed setting, illustrating how linear accessibility is a meaningfully stronger hypothesis than linear representation alone. The upper bound confirms that neurons can store an exponential number of features under the LRH, giving theoretical evidence for the "superposition hypothesis" (Elhage et al., 2022).
  The upper bound proof uses standard random constructions of matrices with approximately orthogonal columns. The lower bound proof uses rank bounds for near-identity matrices (Alon, 2003) together with Turán's theorem (bounding the number of edges in clique-free graphs). We also show how our results do and do not constrain the geometry of feature representations and extend our results to allow decoders with an activation function and bias.

</details>


### [95] [HiFloat4 Format for Language Model Inference](https://arxiv.org/abs/2602.11287)
*Yuanyong Luo,Jing Huang,Yu Cheng,Ziwei Yu,Kaihua Zhang,Kehong Hong,Xinda Ma,Xin Wang,Anping Tong,Guipeng Hu,Yun Xu,Mehran Taghian,Peng Wu,Guanglin Li,Yunke Peng,Tianchi Hu,Minqi Chen,Michael Bi Mi,Hu Liu,Xiping Zhou,Junsong Wang,Qiang Lin,Heng Liao*

Main category: cs.LG

TL;DR: 本文提出HiFloat4（HiF4），一种专为深度学习设计的块浮点数据格式。每64个4-bit元素共享32-bit三级缩放元数据，平均4.5bit/值，支持高效固定点矩阵乘法，在LLaMA、Qwen、Mistral、DeepSeek-V3.1和LongCat等模型上推理精度优于NVFP4，同时降低硬件面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 深度学习推理需要高硬件效率与低精度的平衡，现有数据格式存在表示空间利用率不足、计算能效不高等问题，亟需一种新型数据格式在降低计算面积和功耗的同时保持模型精度。

Method: 提出HiFloat4块浮点格式，通过64元素组打包与32-bit三级层次化缩放元数据设计，捕获组间和组内动态范围；利用大规模组结构实现类固定点矩阵乘法；在LLaMA、Qwen、Mistral、DeepSeek-V3.1和LongCat等多个主流语言模型上进行推理评估。

Result: HiF4在多种语言模型和下游任务上的平均精度均超越当前最优的NVFP4格式。

Conclusion: HiFloat4通过创新的层次化缩放和大规模组设计，在提升表示空间利用率的同时显著降低硬件开销，为深度学习推理提供了高效且高精度的数据格式解决方案。

Abstract: This paper introduces HiFloat4 (HiF4), a block floating-point data format tailored for deep learning. Each HiF4 unit packs 64 4-bit elements with 32 bits of shared scaling metadata, averaging 4.5 bits per value. The metadata specifies a three-level scaling hierarchy, capturing inter- and intra-group dynamic range while improving the utilization of the representational space. In addition, the large 64-element group size enables matrix multiplications to be executed in a highly fixed-point manner, significantly reducing hardware area and power consumption. To evaluate the proposed format, we conducted inference experiments on several language models, including LLaMA, Qwen, Mistral, DeepSeek-V3.1 and LongCat. Results show that HiF4 achieves higher average accuracy than the state-of-the-art NVFP4 format across multiple models and diverse downstream tasks.

</details>


### [96] [Efficient Analysis of the Distilled Neural Tangent Kernel](https://arxiv.org/abs/2602.11320)
*Jamie Mahowald,Brian Bell,Alex Ho,Michael Geyer*

Main category: cs.LG

TL;DR: 本文提出蒸馏神经切核(DNTK)，通过数据蒸馏压缩数据维度，将NTK计算复杂度降低达10万倍同时保持性能


<details>
  <summary>Details</summary>
Motivation: 神经切核(NTK)方法的计算成本受限于需要在大量数据点上评估大型雅可比矩阵，现有方法主要通过投影和素描技术降低雅可比矩阵计算成本，但仍面临计算瓶颈

Method: 利用NTK调优的数据集蒸馏压缩数据维度本身，发现输入数据张成的神经切空间可通过蒸馏诱导，且类内NTK矩阵具有低有效秩特性，提出将NTK调优数据集蒸馏与先进投影方法结合的蒸馏神经切核(DNTK)

Result: 通过数据蒸馏使所需雅可比计算减少20-100倍；DNTK将NTK计算复杂度降低达五个数量级(10万倍)，同时保持核结构与预测性能

Conclusion: DNTK通过数据蒸馏压缩维度有效解决了NTK计算瓶颈，实现了可扩展的NTK计算且性能损失极小

Abstract: Neural tangent kernel (NTK) methods are computationally limited by the need to evaluate large Jacobians across many data points. Existing approaches reduce this cost primarily through projecting and sketching the Jacobian. We show that NTK computation can also be reduced by compressing the data dimension itself using NTK-tuned dataset distillation. We demonstrate that the neural tangent space spanned by the input data can be induced by dataset distillation, yielding a 20-100$\times$ reduction in required Jacobian calculations. We further show that per-class NTK matrices have low effective rank that is preserved by this reduction. Building on these insights, we propose the distilled neural tangent kernel (DNTK), which combines NTK-tuned dataset distillation with state-of-the-art projection methods to reduce up NTK computational complexity by up to five orders of magnitude while preserving kernel structure and predictive performance.

</details>


### [97] [Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal Co-occurrence](https://arxiv.org/abs/2602.11322)
*Jason Dury*

Main category: cs.LG

TL;DR: 该论文提出预测联想记忆(PAM)架构，通过JEPA风格预测器学习时间共现关系而非相似度检索。在合成基准测试中，模型以97%的精确度召回真实时间关联，跨边界召回率达0.421(余弦相似度为零)，区分AUC为0.916(余弦仅0.789)。即使在嵌入相似度无效的跨房间对中，AUC仍达0.849。时间打乱实验使召回率下降90%，验证了信号的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前神经记忆方法基于相似度检索，假设有用记忆即相似记忆，但未能捕捉生物记忆的基本特性：通过时间共现建立联想。这种相似度假设在处理连续经验流时存在根本缺陷，无法反映真实生物记忆中时间关联结构的核心作用。

Method: 提出PAM架构，采用JEPA风格预测器在连续经验流上学习时间共现关系。核心创新是引入内向JEPA(预测存储经验中可联想的过去状态)作为外向JEPA(预测未来感官数据)的互补机制。将模型作为联想回忆系统评估，重点测试对已体验关联的忠实回忆能力，而非泛化到未见关联的检索性能。

Result: 在合成基准测试中：联想精确度@1达0.970，即97%情况下顶部检索为真实时间关联；跨边界召回率@20为0.421，而此时余弦相似度为零；区分共同经历与从未共同经历状态的AUC达0.916(余弦相似度仅0.789)。即使在嵌入相似度无信息的跨房间对中，AUC仍达0.849(余弦相似度0.503，接近随机)。时间打乱对照实验证实信号源于真实时间共现结构而非嵌入几何：打乱后跨边界召回率下降90%。所有结果跨种子(标准差<0.006)和查询选择(标准差≤0.012)高度稳定。

Conclusion: PAM成功证明了基于时间共现的联想记忆机制显著优于传统相似度检索，能更准确捕捉生物记忆的真实结构。内向与外向JEPA的互补设计为连续经验中的联想回忆提供了新计算范式。实验结果稳定可靠，统计显著，为构建更符合生物机制的神经记忆模型提供了重要方向。

Abstract: Current approaches to memory in neural systems rely on similarity-based retrieval: given a query, find the most representationally similar stored state. This assumption -- that useful memories are similar memories -- fails to capture a fundamental property of biological memory: association through temporal co-occurrence. We propose Predictive Associative Memory (PAM), an architecture in which a JEPA-style predictor, trained on temporal co-occurrence within a continuous experience stream, learns to navigate the associative structure of an embedding space. We introduce an Inward JEPA that operates over stored experience (predicting associatively reachable past states) as the complement to the standard Outward JEPA that operates over incoming sensory data (predicting future states). We evaluate PAM as an associative recall system -- testing faithfulness of recall for experienced associations -- rather than as a retrieval system evaluated on generalisation to unseen associations. On a synthetic benchmark, the predictor's top retrieval is a true temporal associate 97% of the time (Association Precision@1 = 0.970); it achieves cross-boundary Recall@20 = 0.421 where cosine similarity scores zero; and it separates experienced-together from never-experienced-together states with a discrimination AUC of 0.916 (cosine: 0.789). Even restricted to cross-room pairs where embedding similarity is uninformative, the predictor achieves AUC = 0.849 (cosine: 0.503, chance). A temporal shuffle control confirms the signal is genuine temporal co-occurrence structure, not embedding geometry: shuffling collapses cross-boundary recall by 90%, replicated across training seeds. All results are stable across seeds (SD < 0.006) and query selections (SD $\leq$ 0.012).

</details>


### [98] [Structured Hybrid Mechanistic Models for Robust Estimation of Time-Dependent Intervention Outcomes](https://arxiv.org/abs/2602.11350)
*Tomer Meir,Ori Linial,Danny Eytan,Uri Shalit*

Main category: cs.LG

TL;DR: 提出一种混合机制-数据驱动方法，将动态系统转移算子分解为参数化机制部分和非参数数据驱动部分，并区分干预相关与无关动态，在丙泊酚麻醉剂量优化等场景中，相比纯方法在分布外泛化性能上表现更优。


<details>
  <summary>Details</summary>
Motivation: 在医疗等动态系统中，干预效应估计（如丙泊酚麻醉给药优化）至关重要，但纯数据驱动模型分布外泛化能力差，纯机制模型鲁棒却可能过简化，需融合两者优势以实现精准鲁棒的剂量控制。

Method: 核心方法是解耦系统转移算子：参数化组件编码机制知识，非参数组件学习数据残差；显式分离干预相关/无关动态。针对机制参数未知场景，采用两阶段训练——先在模拟数据预训练编码器，再用观测数据学习校正项。验证于周期性摆与丙泊酚注射两类机制知识不完整系统。

Result: 在两个典型场景下，该混合方法在分布外测试中显著超越纯数据驱动和纯机制方法，能更准确预测干预效果并优化给药策略，尤其在机制知识缺失时优势更突出。

Conclusion: 混合机制-数据驱动框架通过结合机制模型的鲁棒性与数据驱动灵活性，为复杂动态系统的干预优化提供了鲁棒解决方案，对推动精准医疗决策支持系统具有重要潜力。

Abstract: Estimating intervention effects in dynamical systems is crucial for outcome optimization. In medicine, such interventions arise in physiological regulation (e.g., cardiovascular system under fluid administration) and pharmacokinetics, among others. Propofol administration is an anesthetic intervention, where the challenge is to estimate the optimal dose required to achieve a target brain concentration for anesthesia, given patient characteristics, while avoiding under- or over-dosing. The pharmacokinetic state is characterized by drug concentrations across tissues, and its dynamics are governed by prior states, patient covariates, drug clearance, and drug administration. While data-driven models can capture complex dynamics, they often fail in out-of-distribution (OOD) regimes. Mechanistic models on the other hand are typically robust, but might be oversimplified. We propose a hybrid mechanistic-data-driven approach to estimate time-dependent intervention outcomes. Our approach decomposes the dynamical system's transition operator into parametric and nonparametric components, further distinguishing between intervention-related and unrelated dynamics. This structure leverages mechanistic anchors while learning residual patterns from data. For scenarios where mechanistic parameters are unknown, we introduce a two-stage procedure: first, pre-training an encoder on simulated data, and subsequently learning corrections from observed data. Two regimes with incomplete mechanistic knowledge are considered: periodic pendulum and Propofol bolus injections. Results demonstrate that our hybrid approach outperforms purely data-driven and mechanistic approaches, particularly OOD. This work highlights the potential of hybrid mechanistic-data-driven models for robust intervention optimization in complex, real-world dynamical systems.

</details>


### [99] [Toward Adaptive Non-Intrusive Reduced-Order Models: Design and Challenges](https://arxiv.org/abs/2602.11378)
*Amirpasha Hedayat,Alberto Padovan,Karthik Duraisamy*

Main category: cs.LG

TL;DR: 针对静态降阶模型在系统超出训练流形时失效的问题，本文提出了三种自适应非侵入式降阶模型框架：自适应算子推断（Adaptive OpInf）、自适应非侵入式轨迹优化（Adaptive NiTROM）及混合方法。通过在线更新潜在子空间和约化动力学，在瞬态扰动 lid-driven cavity flow 测试中，自适应 OpInf 能有效抑制振幅漂移且计算成本适中，自适应 NiTROM 在频繁更新下可实现近精确能量跟踪但对初始化和优化深度敏感，混合方法在 regime changes 和极小离线数据下最可靠。研究强调 ROM 预测声明必须成本透明，并提供构建自校正 ROM 的实用模板。


<details>
  <summary>Details</summary>
Motivation: 基于投影的静态降阶模型（ROM）在实际应用中一旦系统状态偏离训练流形，其预测能力显著下降甚至失稳。非侵入式 ROM 虽避免了全阶模型（FOM）的侵入式重构，但仍受限于静态框架的泛化能力。现有方法缺乏对在线自适应更新机制的严谨形式化研究，无法在动力学持续演化时保持长期有效性，亟需构建能动态调整潜在空间和约化动力学的自适应框架。

Method: 本文提出三种在线自适应非侵入式 ROM 形式化框架：1) 自适应算子推断（Adaptive OpInf）：通过序贯基/算子重构，在滑动时间窗内局部更新降维基和多项式算子；2) 自适应 NiTROM：采用联合黎曼优化，同步更新编码器/解码器与多项式动力学；3) 混合框架：以 OpInf 更新初始化 NiTROM，结合两者优势。方法包含在线数据窗（捕捉当前状态）、适应窗（优化间隔）与计算预算（FOM 调用次数与优化复杂度）的协同设计，并分析成本随窗长的标度律。

Result: 在瞬态扰动 lid-driven cavity flow 上验证：静态 Galerkin/OpInf/NiTROM 在训练流形外均出现漂移或失稳。自适应 OpInf 以适中成本鲁棒地抑制振幅漂移；自适应 NiTROM 在频繁更新下实现近精确能量跟踪，但对初始化与优化深度敏感；混合方法在流形突变和极小离线数据场景下最可靠，能生成物理自洽流场并保持能量有界。实验揭示了不同框架在预测精度、计算开销与鲁棒性间的权衡。

Conclusion: ROM 的预测声明必须明确成本预算并区分训练/适应/部署阶段，透明度是关键。本文为构建自校正、非侵入式自适应 ROM 提供了实用模板，强调在线更新机制对延长模型寿命的重要性。未来工作需系统研究优化深度、窗口策略与初始化对自适应 NiTROM 稳定性的影响，并推广至多物理场复杂应用场景。

Abstract: Projection-based Reduced Order Models (ROMs) are often deployed as static surrogates, which limits their practical utility once a system leaves the training manifold. We formalize and study adaptive non-intrusive ROMs that update both the latent subspace and the reduced dynamics online. Building on ideas from static non-intrusive ROMs, specifically, Operator Inference (OpInf) and the recently-introduced Non-intrusive Trajectory-based optimization of Reduced-Order Models (NiTROM), we propose three formulations: Adaptive OpInf (sequential basis/operator refits), Adaptive NiTROM (joint Riemannian optimization of encoder/decoder and polynomial dynamics), and a hybrid that initializes NiTROM with an OpInf update. We describe the online data window, adaptation window, and computational budget, and analyze cost scaling. On a transiently perturbed lid-driven cavity flow, static Galerkin/OpInf/NiTROM drift or destabilize when forecasting beyond training. In contrast, Adaptive OpInf robustly suppresses amplitude drift with modest cost; Adaptive NiTROM is shown to attain near-exact energy tracking under frequent updates but is sensitive to its initialization and optimization depth; the hybrid is most reliable under regime changes and minimal offline data, yielding physically coherent fields and bounded energy. We argue that predictive claims for ROMs must be cost-aware and transparent, with clear separation of training/adaptation/deployment regimes and explicit reporting of online budgets and full-order model queries. This work provides a practical template for building self-correcting, non-intrusive ROMs that remain effective as the dynamics evolve well beyond the initial manifold.

</details>


### [100] [Provably Efficient Algorithms for S- and Non-Rectangular Robust MDPs with General Parameterization](https://arxiv.org/abs/2602.11387)
*Anirudh Satheesh,Ziyi Chen,Furong Huang,Heng Huang*

Main category: cs.LG

TL;DR: 该论文研究了具有通用策略参数化的鲁棒马尔可夫决策过程(RMDPs)，突破了以往仅限于表格策略的限制。通过将平均奖励RMDPs归约为熵正则化折扣鲁棒MDPs，引入多级蒙特卡洛梯度估计器，并设计了适用于s-矩形和非矩形不确定性集的优化算法，首次为一般策略参数化提供了样本复杂度保证，并在折扣和平均奖励设置下均显著改进了现有结果。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒MDP研究主要局限于表格策略，缺乏样本复杂度保证或计算成本过高。该论文旨在解决通用策略参数化（可处理无限状态空间）下的鲁棒优化问题，突破(s,a)-矩形性的限制，为更实际、更复杂的策略类提供理论保证和高效算法，特别是在平均奖励准则下此前尚无相关样本复杂度结果。

Method: 1) 将平均奖励RMDPs通过熵正则化技术归约为折扣鲁棒MDPs，恢复强对偶性并使均衡计算可处理；2) 证明通用策略参数化下的新颖Lipschitz和Lipschitz光滑性性质，适用于无限状态空间；3) 提出多级蒙特卡洛梯度估计器，实现$	ilde{mathcal{O}}(ε^{-2})$样本复杂度，比先前工作提升$mathcal{O}(ε^{-2})$倍；4) 针对s-矩形不确定性设计投影梯度下降算法；5) 针对非矩形不确定性设计Frank-Wolfe算法。

Result: 1) s-矩形不确定性下获得$mathcal{O}(ε^{-5})$样本复杂度的投影梯度下降算法；2) 非矩形不确定性下获得$mathcal{O}(ε^{-4})$（折扣）和$mathcal{O}(ε^{-10.5})$（平均奖励）样本复杂度的Frank-Wolfe算法；3) 多级蒙特卡洛梯度估计器达到$	ilde{mathcal{O}}(ε^{-2})$复杂度，较先前提升$mathcal{O}(ε^{-2})$倍；4) 首次在超越(s,a)-矩形性的通用策略参数化下提供样本复杂度保证；5) 首次实现平均奖励设置下的此类保证，并显著改进折扣鲁棒MDP的现有边界。

Conclusion: 该论文在鲁棒强化学习的理论与算法方面取得重要突破，首次为通用策略参数化的RMDPs建立了完整的样本复杂度理论框架。通过创新性地结合熵正则化、对偶性恢复和高级梯度估计技术，不仅解决了无限状态空间和无限时域的挑战，还在计算效率和理论保证之间取得了显著改进。这为实际应用中复杂策略类的鲁棒优化奠定了坚实的理论基础。

Abstract: We study robust Markov decision processes (RMDPs) with general policy parameterization under s-rectangular and non-rectangular uncertainty sets. Prior work is largely limited to tabular policies, and hence either lacks sample complexity guarantees or incurs high computational cost. Our method reduces the average reward RMDPs to entropy-regularized discounted robust MDPs, restoring strong duality and enabling tractable equilibrium computation. We prove novel Lipschitz and Lipschitz-smoothness properties for general policy parameterizations that extends to infinite state spaces. To address infinite-horizon gradient estimation, we introduce a multilevel Monte Carlo gradient estimator with $\tilde{\mathcal{O}}(ε^{-2})$ sample complexity, a factor of $\mathcal{O}(ε^{-2})$ improvement over prior work. Building on this, we design a projected gradient descent algorithm for s-rectangular uncertainty ($\mathcal{O}(ε^{-5})$) and a Frank--Wolfe algorithm for non-rectangular uncertainty ($\mathcal{O}(ε^{-4})$ discounted, $\mathcal{O}(ε^{-10.5})$ average reward), significantly improving prior results in both the discounted setting and average reward setting. Our work is the first one to provide sample complexity guarantees for RMDPs with general policy parameterization beyond $(s, a)$-rectangularity. It also provides the first such guarantees in the average reward setting and improves existing bounds for discounted robust MDPs.

</details>


### [101] [Sparse Semantic Dimension as a Generalization Certificate for LLMs](https://arxiv.org/abs/2602.11388)
*Dibyanayan Bandyopadhyay,Asif Ekbal*

Main category: cs.LG

TL;DR: 针对大语言模型参数量远大于训练token数却仍能良好泛化的悖论，本文提出有效容量取决于内部表示的几何结构，引入稀疏语义维度(SSD)度量，发现大模型学习到更紧凑的语义结构，并可作为安全监控器检测分布外输入。


<details>
  <summary>Details</summary>
Motivation: 标准统计学习理论预测大语言模型应严重过拟合，但实践中它们表现出强大的泛化能力。现有理论无法解释这一矛盾，需要新的复杂度度量来理解模型的实际容量。

Method: 提出稀疏语义维度(SSD)作为新的复杂度度量，该度量基于稀疏自编码器(SAE)在模型各层上训练的激活特征词典。将LLM和SAE视为冻结的预言机，从激活的稀疏性而非参数量角度归因泛化能力。

Result: 在GPT-2 Small和Gemma-2B上验证框架，提供实际样本量下的非平凡泛化界。发现反直觉的"特征锐度"缩放律：尽管大一个数量级，Gemma-2B比GPT-2需要更少校准样本来识别激活流形，表明更大模型学到更可压缩的语义结构。此外，分布外输入会触发"特征爆炸"（激活特征激增），可作为认知不确定性的有效信号。

Conclusion: 该框架成功解释了LLM泛化悖论，揭示内部表示稀疏性是控制泛化的关键，并建立了模型规模与特征可压缩性的新缩放律，同时证明了该度量可作为可靠的安全监控器。

Abstract: Standard statistical learning theory predicts that Large Language Models (LLMs) should overfit because their parameter counts vastly exceed the number of training tokens. Yet, in practice, they generalize robustly. We propose that the effective capacity controlling generalization lies in the geometry of the model's internal representations: while the parameter space is high-dimensional, the activation states lie on a low-dimensional, sparse manifold. To formalize this, we introduce the Sparse Semantic Dimension (SSD), a complexity measure derived from the active feature vocabulary of a Sparse Autoencoder (SAE) trained on the model's layers. Treating the LLM and SAE as frozen oracles, we utilize this framework to attribute the model's generalization capabilities to the sparsity of the dictionary rather than the total parameter count. Empirically, we validate this framework on GPT-2 Small and Gemma-2B, demonstrating that our bound provides non-vacuous certificates at realistic sample sizes. Crucially, we uncover a counter-intuitive "feature sharpness" scaling law: despite being an order of magnitude larger, Gemma-2B requires significantly fewer calibration samples to identify its active manifold compared to GPT-2, suggesting that larger models learn more compressible, distinct semantic structures. Finally, we show that this framework functions as a reliable safety monitor: out-of-distribution inputs trigger a measurable "feature explosion" (a sharp spike in active features), effectively signaling epistemic uncertainty through learned feature violation. Code is available at: https://github.com/newcodevelop/sparse-semantic-dimension.

</details>


### [102] [General and Efficient Steering of Unconditional Diffusion](https://arxiv.org/abs/2602.11395)
*Qingsong Wang,Mikhail Belkin,Yusu Wang*

Main category: cs.LG

TL;DR: 本文提出一种通用方法，通过离线计算的引导信号和跨时间步/样本迁移的概念向量，在无梯度引导条件下实现无条件扩散模型的高效可控生成，在多个数据集上取得优于传统梯度方法的精度与质量，并显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有无条件扩散模型的条件引导依赖每步梯度计算或重新训练，计算开销巨大，亟需一种高效的推理时控制方案。

Method: 基于两大观察：噪声对齐——早期高噪声阶段可通过离线轻量引导信号实现粗粒度控制；可迁移概念向量——激活空间概念方向可跨时间步和样本迁移，低噪声下学习的固定向量在中间噪声水平仍有效。采用无反向传播的递归特征机（RFM）高效识别此类概念方向。

Result: CIFAR-10、ImageNet和CelebA实验表明，该方法超越梯度引导方法的效果，同时实现显著的推理加速。

Conclusion: 该方法通过在推理时完全消除梯度计算，为无条件扩散模型的可控生成提供了高效通用的新范式。

Abstract: Guiding unconditional diffusion models typically requires either retraining with conditional inputs or per-step gradient computations (e.g., classifier-based guidance), both of which incur substantial computational overhead. We present a general recipe for efficiently steering unconditional diffusion {without gradient guidance during inference}, enabling fast controllable generation. Our approach is built on two observations about diffusion model structure: Noise Alignment: even in early, highly corrupted stages, coarse semantic steering is possible using a lightweight, offline-computed guidance signal, avoiding any per-step or per-sample gradients. Transferable concept vectors: a concept direction in activation space once learned transfers across both {timesteps} and {samples}; the same fixed steering vector learned near low noise level remains effective when injected at intermediate noise levels for every generation trajectory, providing refined conditional control with efficiency. Such concept directions can be efficiently and reliably identified via Recursive Feature Machine (RFM), a light-weight backpropagation-free feature learning method. Experiments on CIFAR-10, ImageNet, and CelebA demonstrate improved accuracy/quality over gradient-based guidance, while achieving significant inference speedups.

</details>


### [103] [Can We Really Learn One Representation to Optimize All Rewards?](https://arxiv.org/abs/2602.11399)
*Chongyi Zheng,Royina Karegoudra Jayanth,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文澄清了前向后向(FB)表示学习的不透明性，通过理论分析揭示其本质，提出一步前向后向表示学习方法(one-step FB)。该方法简化了框架，仅执行一步策略改进而非最优控制，但在10个连续控制任务中收敛误差降低10^5倍，零样本性能平均提升24%。


<details>
  <summary>Details</summary>
Motivation: 前向后向(FB)表示学习声称能实现任意奖励下的零样本最优控制，但其优化目标和收敛机制尚不清晰。为理解无监督预训练在强化学习中的真实作用，本文旨在揭示FB的内在原理，包括表示存在的条件、实际优化目标及收敛行为。

Method: 通过将FB与秩匹配、拟合Q评估和收缩映射建立理论联系，分析其工作机制。基于此，提出一步前向后向表示学习，将复杂的无监督预训练简化为单步策略改进过程，并在教学环境和10个状态/图像连续控制域进行验证。

Result: 一步FB在所有实验环境中收敛至比FB小10^5倍的误差水平，零样本控制性能平均提升+24%。该方法在状态和图像两种模态的连续控制任务中均展现出显著优势。

Conclusion: 研究揭示了FB表示学习并非实现最优控制而是隐式执行策略评估，其简化版本一步FB以更小的近似误差和更好的零样本性能，为无监督预训练在RL中的应用提供了更清晰的理论理解和实践路径。

Abstract: As machine learning has moved towards leveraging large models as priors for downstream tasks, the community has debated the right form of prior for solving reinforcement learning (RL) problems. If one were to try to prefetch as much computation as possible, they would attempt to learn a prior over the policies for some yet-to-be-determined reward function. Recent work (forward-backward (FB) representation learning) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over arbitrary rewards without further fine-tuning. However, FB's training objective and learning behavior remain mysterious. In this paper, we demystify FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. We draw connections with rank matching, fitted Q-evaluation, and contraction mapping. Our analysis suggests a simplified unsupervised pre-training method for RL that, instead of enabling optimal control, performs one step of policy improvement. We call our proposed method $\textbf{one-step forward-backward representation learning (one-step FB)}$. Experiments in didactic settings, as well as in $10$ state-based and image-based continuous control domains, demonstrate that one-step FB converges to errors $10^5$ smaller and improves zero-shot performance by $+24\%$ on average. Our project website is available at https://chongyi-zheng.github.io/onestep-fb.

</details>


### [104] [CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer](https://arxiv.org/abs/2602.11410)
*David Pardoe,Neil Daftary,Miro Furtado,Aditya Aiyer,Yu Wang,Liuqing Li,Tao Song,Lars Hertel,Young Jin Yun,Senthil Radhakrishnan,Zhiwei Wang,Tommy Li,Khai Tran,Ananth Nagarajan,Ali Naqvi,Yue Zhang,Renpeng Fang,Avi Romascanu,Arjun Kulothungun,Deepak Kumar,Praneeth Boda,Fedor Borisyuk,Ruoyan Wang*

Main category: cs.LG

TL;DR: CADET是LinkedIn提出的端到端解码器专用Transformer模型，用于广告点击率预测。该模型通过上下文条件化解码、自门控注意力、时间戳RoPE、会话掩码等创新，解决了后评分信号建模、训练服务不一致和工业级扩展等挑战，在线A/B测试中实现11.04%的CTR提升，并已部署服务主页Feed赞助流量。


<details>
  <summary>Details</summary>
Motivation: 广告点击率预测是在线广告系统的核心。尽管深度推荐模型长期主导该领域，但生成式推荐器在内容推荐中展现出潜力。然而，将基于Transformer的生成架构适配到广告CTR预测仍面临独特挑战：需要处理预测后才会出现的上下文信号（如广告位置）、保持离线训练与在线服务的一致性，以及扩展至工业级大规模负载。这些挑战限制了生成式模型在该领域的应用。

Method: 本文提出CADET（Context-Conditioned Ads Decoder-Only Transformer），一个端到端解码器专用Transformer架构。主要创新包括：1）上下文条件化解码架构配合多塔预测头，显式建模广告位置等后评分信号，解决预测CTR与排序的"鸡生蛋"问题；2）自门控注意力机制，在表征和交互层面自适应调节信息流以稳定训练；3）基于时间戳的旋转位置编码变体，捕捉从秒到月跨尺度的时间关系；4）会话掩码策略，防止模型学习会话中不可用事件的依赖，解决训练服务偏差；5）生产工程技术如张量打包、序列分块和自定义Flash Attention核，实现高效大规模训练和服务。

Result: 在LinkedIn生产环境进行的在线A/B测试表明，CADET相比基线模型LiRank（DCNv2与序列编码器的混合集成）实现了11.04%的点击率提升。该系统已成功部署至LinkedIn广告平台，服务于主页Feed赞助更新的主要流量。

Conclusion: CADET成功验证了解码器专用Transformer架构在工业级广告CTR预测中的可行性。通过系统性的架构创新和生产优化，该模型有效解决了后评分信号建模、训练服务一致性和规模化部署等关键挑战，为生成式推荐模型在广告推荐领域的应用提供了实用化路径，并取得了显著的业务收益。

Abstract: Click-through rate (CTR) prediction is fundamental to online advertising systems. While Deep Learning Recommendation Models (DLRMs) with explicit feature interactions have long dominated this domain, recent advances in generative recommenders have shown promising results in content recommendation. However, adapting these transformer-based architectures to ads CTR prediction still presents unique challenges, including handling post-scoring contextual signals, maintaining offline-online consistency, and scaling to industrial workloads. We present CADET (Context-Conditioned Ads Decoder-Only Transformer), an end-to-end decoder-only transformer for ads CTR prediction deployed at LinkedIn. Our approach introduces several key innovations: (1) a context-conditioned decoding architecture with multi-tower prediction heads that explicitly model post-scoring signals such as ad position, resolving the chicken-and-egg problem between predicted CTR and ranking; (2) a self-gated attention mechanism that stabilizes training by adaptively regulating information flow at both representation and interaction levels; (3) a timestamp-based variant of Rotary Position Embedding (RoPE) that captures temporal relationships across timescales from seconds to months; (4) session masking strategies that prevent the model from learning dependencies on unavailable in-session events, addressing train-serve skew; and (5) production engineering techniques including tensor packing, sequence chunking, and custom Flash Attention kernels that enable efficient training and serving at scale. In online A/B testing, CADET achieves a 11.04\% CTR lift compared to the production LiRank baseline model, a hybrid ensemble of DCNv2 and sequential encoders. The system has been successfully deployed on LinkedIn's advertising platform, serving the main traffic for homefeed sponsored updates.

</details>


### [105] [TimeSynth: A Framework for Uncovering Systematic Biases in Time Series Forecasting](https://arxiv.org/abs/2602.11413)
*Md Rakibul Haque,Vishwa Goudar,Shireen Elhabian,Warren Woodrich Pettine*

Main category: cs.LG

TL;DR: 本研究通过TimeSynth合成框架重新探讨时间序列预测模型之争，揭示线性模型存在系统性坍缩偏差，在复杂信号下仅表现为简单振荡，而非线性模型（尤其Transformer和CNN）能更好捕捉复杂动态，为模型选择提供了科学评估基础。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测领域存在线性模型与复杂非线性架构孰优孰劣的争议，但既有基准测试常因缺乏多样时间动态和评估偏倚而得出片面结论，亟需无偏评估框架。

Method: 构建TimeSynth结构化框架，基于真实时间序列参数合成包含非平稳性、周期性、趋势及相位调制的信号，独立划分训练/验证/测试集，系统评估线性、MLP、CNN和Transformer四类模型。

Result: 发现线性模型存在系统性偏差，无论信号复杂度均坍缩至简单振荡模式；非线性模型有效避免此问题，优势随复杂度提升而显著；Transformer与CNN对复杂调制信号适应性优于MLP；框架同时揭示模型在分布偏移与噪声干扰下的鲁棒性差异。

Conclusion: TimeSynth建立了理解不同预测方法适用性的原则性基础，超越了简单的模型等效论，为时间序列预测研究提供了更科学的评估范式。

Abstract: Time series forecasting is a fundamental tool with wide ranging applications, yet recent debates question whether complex nonlinear architectures truly outperform simple linear models. Prior claims of dominance of the linear model often stem from benchmarks that lack diverse temporal dynamics and employ biased evaluation protocols. We revisit this debate through TimeSynth, a structured framework that emulates key properties of real world time series,including non-stationarity, periodicity, trends, and phase modulation by creating synthesized signals whose parameters are derived from real-world time series. Evaluating four model families Linear, Multi Layer Perceptrons (MLP), Convolutional Neural Networks (CNNs), and Transformers, we find a systematic bias in linear models: they collapse to simple oscillation regardless of signal complexity. Nonlinear models avoid this collapse and gain clear advantages as signal complexity increases. Notably, Transformers and CNN based models exhibit slightly greater adaptability to complex modulated signals compared to MLPs. Beyond clean forecasting, the framework highlights robustness differences under distribution and noise shifts and removes biases of prior benchmarks by using independent instances for train, test, and validation for each signal family. Collectively, TimeSynth provides a principled foundation for understanding when different forecasting approaches succeed or fail, moving beyond oversimplified claims of model equivalence.

</details>


### [106] [Multi-Level Strategic Classification: Incentivizing Improvement through Promotion and Relegation Dynamics](https://arxiv.org/abs/2602.11439)
*Ziyuan Huang,Lina Alkarmi,Mingyan Liu*

Main category: cs.LG

TL;DR: 本文突破了现有序贯策略分类研究中动态分类器权重优化的局限，转向分析多层级升降级框架下的分类器阈值与难度递进设计。通过捕捉智能体的远视性、技能留存效应及资格成就自我强化的"提携效应"等跨期激励，刻画了智能体的最优长期策略，并证明委托方可设计阈值序列有效激励诚实努力，且在温和条件下该机制可使智能体仅通过真实改进达到任意高级别。


<details>
  <summary>Details</summary>
Motivation: 现有序贯策略分类研究主要聚焦于动态分类器权重的优化，而忽视了分类器阈值与难度递进设计的重要性。当个体面临分类决策时，往往倾向于通过操纵性手段而非真实努力获取有利结果。本文旨在填补这一研究空白，探索在多层级晋升-降级体系下，如何通过机制设计引导个体选择诚实行为，并实现长期激励相容。

Method: 采用多层级升降级框架作为理论模型，从阈值设计而非权重优化的角度研究序贯策略分类问题。该模型纳入三个核心要素：智能体的跨期远视性、技能留存效应以及资格与成就之间的自我强化"提携效应"。通过动态规划与机制设计方法，系统分析智能体的最优长期行为策略及委托方的最优激励机制。

Result: 主要理论贡献包括：(1) 完整刻画了智能体在动态环境中的最优长期策略；(2) 证明委托方可通过精心设计的阈值序列有效激励诚实努力；(3) 在温和条件下，该机制能使智能体仅通过真实能力改进即可达到任意预设的高级别水平，从而消除策略性操纵的动机。

Conclusion: 本研究揭示了分类器阈值设计与难度递进机制在序贯策略分类中的核心作用，为机制设计理论提供了新视角。结果表明，合理的制度安排能够协调个体自利行为与社会目标，实现真实能力提升与组织效率的共赢。该框架在人才选拔、绩效评估等场景中具有重要应用价值。

Abstract: Strategic classification studies the problem where self-interested individuals or agents manipulate their response to obtain favorable decision outcomes made by classifiers, typically turning to dishonest actions when they are less costly than genuine efforts. While existing studies on sequential strategic classification primarily focus on optimizing dynamic classifier weights, we depart from these weight-centric approaches by analyzing the design of classifier thresholds and difficulty progression within a multi-level promotion-relegation framework. Our model captures the critical inter-temporal incentives driven by an agent's farsightedness, skill retention, and a leg-up effect where qualification and attainment can be self-reinforcing. We characterize the agent's optimal long-term strategy and demonstrate that a principal can design a sequence of thresholds to effectively incentivize honest effort. Crucially, we prove that under mild conditions, this mechanism enables agents to reach arbitrarily high levels solely through genuine improvement efforts.

</details>


### [107] [Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification](https://arxiv.org/abs/2602.11448)
*Nghia Nguyen,Tianjiao Ding,René Vidal*

Main category: cs.LG

TL;DR: HCEP框架通过引入概念嵌入层次结构和层次化稀疏编码，解决了现有稀疏概念恢复方法忽略层次结构导致解释不一致的问题，在概念精度、召回率及小样本分类性能上均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 可解释性图像分类模型需要提供符合人类认知的层次化概念解释，但现有稀疏概念恢复方法忽略了概念间的层次关系，导致预测虽正确但解释与层次结构相悖，影响模型可信度。

Method: 提出层次化概念嵌入与追踪（HCEP）框架，在视觉-语言模型潜在空间中构建概念嵌入层次结构，假设图像的真实概念在层次树中形成根路径，并推导层次化稀疏编码的优化条件以实现概念恢复。

Result: 真实数据集实验表明，HCEP在概念精度和召回率上显著优于基线，同时保持有竞争力的分类准确率；在样本有限时，HCEP的分类准确率和概念恢复能力更优。

Conclusion: 将层次结构融入稀疏编码可产生更可靠和可解释的图像分类模型，层次化稀疏编码比传统方法更能准确恢复符合层次逻辑的概念表示。

Abstract: Interpretable-by-design models are gaining traction in computer vision because they provide faithful explanations for their predictions. In image classification, these models typically recover human-interpretable concepts from an image and use them for classification. Sparse concept recovery methods leverage the latent space of vision-language models to represent image embeddings as a sparse combination of concept embeddings. However, because such methods ignore the hierarchical structure of concepts, they can produce correct predictions with explanations that are inconsistent with the hierarchy. In this work, we propose Hierarchical Concept Embedding \& Pursuit (HCEP), a framework that induces a hierarchy of concept embeddings in the latent space and uses hierarchical sparse coding to recover the concepts present in an image. Given a hierarchy of semantic concepts, we construct a corresponding hierarchy of concept embeddings and, assuming the correct concepts for an image form a rooted path in the hierarchy, derive desirable conditions for identifying them in the embedded space. We show that hierarchical sparse coding reliably recovers hierarchical concept embeddings, whereas vanilla sparse coding fails. Our experiments on real-world datasets demonstrate that HCEP outperforms baselines in concept precision and recall while maintaining competitive classification accuracy. Moreover, when the number of samples is limited, HCEP achieves superior classification accuracy and concept recovery. These results show that incorporating hierarchical structures into sparse coding yields more reliable and interpretable image classification models.

</details>


### [108] [Assessing Low Back Movement with Motion Tape Sensor Data Through Deep Learning](https://arxiv.org/abs/2602.11465)
*Jared Levy,Aarti Lalwani,Elijah Wyckoff,Kenneth J. Loh,Sara P. Gombatto,Rose Yu,Emilia Farcas*

Main category: cs.LG

TL;DR: 针对低成本可穿戴传感器运动胶带(MT)数据少、噪声大的问题，本文提出MT-AIM模型，通过条件生成模型合成数据并结合关节运动学特征，实现下背部运动分类的SOTA精度。


<details>
  <summary>Details</summary>
Motivation: 背部疼痛是普遍问题，现有运动捕捉传感器昂贵且不适用于自由生活环境监测。新型织物传感器运动胶带(MT)虽成本低、便携，但数据规模小且噪声大，限制了其应用。

Method: 提出运动胶带增强推理模型(MT-AIM)，利用条件生成模型生成目标运动的合成MT数据，并预测关节运动学作为额外特征，通过数据合成与特征增强的深度学习管道进行分类。

Result: MT-AIM实现了下背部运动分类的最先进(state-of-the-art)准确率。

Conclusion: 该方法成功解决了MT传感器的数据局限性问题，有效弥合了生理传感与运动分析之间的鸿沟，为远程临床监测提供了可行方案。

Abstract: Back pain is a pervasive issue affecting a significant portion of the population, often worsened by certain movements of the lower back. Assessing these movements is important for helping clinicians prescribe appropriate physical therapy. However, it can be difficult to monitor patients' movements remotely outside the clinic. High-fidelity data from motion capture sensors can be used to classify different movements, but these sensors are costly and impractical for use in free-living environments. Motion Tape (MT), a new fabric-based wearable sensor, addresses these issues by being low cost and portable. Despite these advantages, novelty and variability in sensor stability make the MT dataset small scale and inherent to noise. In this work, we propose the Motion-Tape Augmentation Inference Model (MT-AIM), a deep learning classification pipeline trained on MT data. In order to address the challenges of limited sample size and noise present within the MT dataset, MT-AIM leverages conditional generative models to generate synthetic MT data of a desired movement, as well as predicting joint kinematics as additional features. This combination of synthetic data generation and feature augmentation enables MT-AIM to achieve state-of-the-art accuracy in classifying lower back movements, bridging the gap between physiological sensing and movement analysis.

</details>


### [109] [External Division of Two Bregman Proximity Operators for Poisson Inverse Problems](https://arxiv.org/abs/2602.11482)
*Kazuki Haishima,Kyohei Suzuki,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 本文提出泊松噪声下稀疏向量恢复的新方法。引入两个Bregman近端算子的外部分除算子替代经典ℓ₁范数正则化以减轻估计偏差，并将其即插即用嵌入NoLips算法。从原始问题和对偶空间阐明其几何结构。数值实验表明，该方法较传统KL散度方法收敛更稳定，在合成数据和图像恢复中性能显著更优。


<details>
  <summary>Details</summary>
Motivation: 在泊松噪声线性模型中，经典ℓ₁范数正则化会引入估计偏差，影响稀疏恢复精度。为克服此问题，需设计新正则化算子以同时促进稀疏性并抑制偏差。

Method: 提出基于两个Bregman近端算子外部分除的新算子，以即插即用方式嵌入NoLips算法替换标准Bregman近端算子；并通过两种互补重述从原始问题和对偶空间阐释其几何结构。

Result: 数值测试显示，与传统KL散度方法相比，所提方法收敛行为更稳定，且在合成数据及图像恢复问题上性能显著更优。

Conclusion: 该方法有效缓解了泊松噪声场景下的ℓ₁正则化偏差问题，为稀疏恢复提供了理论清晰且性能优越的解决方案。

Abstract: This paper presents a novel method for recovering sparse vectors from linear models corrupted by Poisson noise. The contribution is twofold. First, an operator defined via the external division of two Bregman proximity operators is introduced to promote sparse solutions while mitigating the estimation bias induced by classical $\ell_1$-norm regularization. This operator is then embedded into the already established NoLips algorithm, replacing the standard Bregman proximity operator in a plug-and-play manner. Second, the geometric structure of the proposed external-division operator is elucidated through two complementary reformulations, which provide clear interpretations in terms of the primal and dual spaces of the Poisson inverse problem. Numerical tests show that the proposed method exhibits more stable convergence behavior than conventional Kullback-Leibler (KL)-based approaches and achieves significantly superior performance on synthetic data and an image restoration problem.

</details>


### [110] [A Generic Framework for Fair Consensus Clustering in Streams](https://arxiv.org/abs/2602.11500)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 本文首次研究流式模型下的公平共识聚类问题，提出首个常数因子近似算法，仅存储对数个输入聚类。通过引入融合最近公平聚类与聚类拟合的通用框架，不仅改进了流式和离线设置的近似保证，还实现了公平性定义无关性，并扩展至k-median共识聚类。


<details>
  <summary>Details</summary>
Motivation: 现有比例公平共识聚类离线算法需存储所有输入聚类，难以扩展至大规模应用。流式模型更符合实际场景中聚类顺序到达且内存受限的需求，但尚无相关研究，亟需设计内存高效的在线算法。

Method: 设计流式算法仅保留对数级输入聚类；提出通用框架将最近公平聚类与聚类拟合相结合；框架不依赖特定公平性定义，只需能高效计算近似最近公平聚类；并将方法推广至更一般的k-median共识聚类问题。

Result: 获得常数因子近似保证，在流式设置下仅用对数内存实现；离线重访时同样获得改进的近似比；框架适用于任意可高效计算近似最近公平聚类的公平性定义；成功扩展至k-median共识聚类。

Conclusion: 本研究填补了流式公平共识聚类的理论空白，提出的算法和框架为大规模流式聚类数据分析提供了高效解决方案，拓展了公平聚类算法的应用范围，并为未来研究建立了可扩展的理论基础。

Abstract: Consensus clustering seeks to combine multiple clusterings of the same dataset, potentially derived by considering various non-sensitive attributes by different agents in a multi-agent environment, into a single partitioning that best reflects the overall structure of the underlying dataset. Recent work by Chakraborty et al, introduced a fair variant under proportionate fairness and obtained a constant-factor approximation by naively selecting the best closest fair input clustering; however, their offline approach requires storing all input clusterings, which is prohibitively expensive for most large-scale applications.
  In this paper, we initiate the study of fair consensus clustering in the streaming model, where input clusterings arrive sequentially and memory is limited. We design the first constant-factor algorithm that processes the stream while storing only a logarithmic number of inputs. En route, we introduce a new generic algorithmic framework that integrates closest fair clustering with cluster fitting, yielding improved approximation guarantees not only in the streaming setting but also when revisited offline. Furthermore, the framework is fairness-agnostic: it applies to any fairness definition for which an approximately close fair clustering can be computed efficiently. Finally, we extend our methods to the more general k-median consensus clustering problem.

</details>


### [111] [Calibrating an Imperfect Auxiliary Predictor for Unobserved No-Purchase Choice](https://arxiv.org/abs/2602.11505)
*Jiangkai Xiong,Kalyan Talluri,Hanzhao Wang*

Main category: cs.LG

TL;DR: 企业无法观测消费者外部选择行为，导致仅凭交易数据难以估计市场规模和偏好。本文利用可能偏差的辅助预测器，提出两种校准方法：在仿射误校准下通过回归一致恢复无购买概率；在单调条件下通过排序校准给出有限样本误差界，并量化对品类优化收益的影响。


<details>
  <summary>Details</summary>
Motivation: 实践中企业仅记录交易数据，无法观测顾客是否考虑竞品、转向竞争对手或完全不购买，这一外部选择信息缺失问题严重阻碍了市场规模和偏好估计。现有方法依赖辅助市场份额、聚合或跨市场数据。本文研究互补场景：使用来自不同渠道、时期或外部ML系统的黑箱辅助预测器（可能偏差或误校准），仅凭购买数据实现统计有效估计。

Method: 提出两种校准方法：1）在logit空间的仿射误校准假设下，通过简单回归识别外部选择效用参数，无需收集无购买标签即可一致恢复无购买概率；2）在更弱的近单调条件下，设计基于排序的校准方法，推导出有限样本误差界，清晰分离辅助预测器质量与第一阶段效用学习误差。

Result: 理论贡献包括：无购买概率的一致性恢复、显式依赖预测器对齐和效用学习误差的有限样本界、以及校准精度对品类优化收益的量化影响。数值实验验证了估计与决策改进，并讨论了多预测器鲁棒聚合扩展。

Conclusion: 本文开发的校准方法能将不完美的外部选择预测转化为统计有效的无购买估计，提供理论保证并提升下游决策质量。研究明确了预测器质量与效用学习误差各自主导的条件，为企业在数据受限环境下利用辅助预测器提供了实用工具和深刻洞察。

Abstract: Firms typically cannot observe key consumer actions: whether customers buy from a competitor, choose not to buy, or even fully consider the firm's offer. This missing outside-option information makes market-size and preference estimation difficult even in simple multinomial logit (MNL) models, and it is a central obstacle in practice when only transaction data are recorded. Existing approaches often rely on auxiliary market-share, aggregated, or cross-market data. We study a complementary setting in which a black-box auxiliary predictor provides outside-option probabilities, but is potentially biased or miscalibrated because it was trained in a different channel, period, or population, or produced by an external machine-learning system. We develop calibration methods that turn such imperfect predictions into statistically valid no-purchase estimates using purchase-only data from the focal environment. First, under affine miscalibration in logit space, we show that a simple regression identifies outside-option utility parameters and yields consistent recovery of no-purchase probabilities without collecting new labels for no-purchase events. Second, under a weaker nearly monotone condition, we propose a rank-based calibration method and derive finite-sample error bounds that cleanly separate auxiliary-predictor quality from first-stage utility-learning error over observed in-set choices. Our analysis also translates estimation error into downstream decision quality for assortment optimization, quantifying how calibration accuracy affects revenue performance. The bounds provide explicit dependence on predictor alignment and utility-learning error, clarifying when each source dominates. Numerical experiments demonstrate improvements in no-purchase estimation and downstream assortment decisions, and we discuss robust aggregation extensions for combining multiple auxiliary predictors.

</details>


### [112] [Unifying Stable Optimization and Reference Regularization in RLHF](https://arxiv.org/abs/2602.11523)
*Li He,Qiang Qu,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.LG

TL;DR: 针对RLHF的奖励黑客与稳定性挑战，本文提出统一正则化方法，显式平衡π₀与πₜ正则化目标，实现加权监督微调损失，显著提升对齐性能并降低实现复杂度。


<details>
  <summary>Details</summary>
Motivation: RLHF虽在模型对齐方面取得重大进展，但仍面临奖励黑客和优化不稳定两大核心问题。现有方案分别独立处理：通过KL散度惩罚约束监督微调模型π₀以防止奖励黑客，通过策略比率裁剪约束当前策略πₜ以提升稳定性。然而，同时正则化至π₀与πₜ的内在权衡尚未被充分研究。

Method: 提出统一正则化框架，显式平衡防止奖励黑客与维持稳定策略更新的双重目标，导出具有优越权衡的加权监督微调损失函数，简化算法实现。

Result: 在多样化基准测试上的实验表明，所提方法持续优于RLHF和在线偏好学习方法，显著提升对齐性能与训练稳定性，同时降低实现复杂度。

Conclusion: 该统一正则化方法通过简单而原则性的设计有效解决了RLHF的核心挑战，为语言模型对齐提供了性能更优、实现更简单的新范式。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has advanced alignment capabilities significantly but remains hindered by two core challenges: \textbf{reward hacking} and \textbf{stable optimization}. Current solutions independently address these issues through separate regularization strategies, specifically a KL-divergence penalty against a supervised fine-tuned model ($π_0$) to mitigate reward hacking, and policy ratio clipping towards the current policy ($π_t$) to promote stable alignment. However, the implicit trade-off arising from simultaneously regularizing towards both $π_0$ and $π_t$ remains under-explored. In this paper, we introduce a unified regularization approach that explicitly balances the objectives of preventing reward hacking and maintaining stable policy updates. Our simple yet principled alignment objective yields a weighted supervised fine-tuning loss with a superior trade-off, which demonstrably improves both alignment results and implementation complexity. Extensive experiments across diverse benchmarks validate that our method consistently outperforms RLHF and online preference learning methods, achieving enhanced alignment performance and stability.

</details>


### [113] [Adaptive Milestone Reward for GUI Agents](https://arxiv.org/abs/2602.11524)
*Congmin Zheng,Xiaoyun Mo,Xinbei Ma,Qiqiang Lin,Yin Zhao,Jiachen Zhu,Xingyu Lou,Jun Wang,Zhaoxiang Wang,Weiwen Liu,Zhuosheng Zhang,Yong Yu,Weinan Zhang*

Main category: cs.LG

TL;DR: 该论文针对移动GUI智能体强化学习中长期信用分配问题，提出自适应里程碑奖励机制（ADMIRE），通过动态提取成功探索中的里程碑并采用非对称信用分配策略，在AndroidWorld基准测试中实现超过10%的绝对成功率提升，并在多种环境和算法中展现出良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 移动GUI智能体的强化学习面临长期任务中的时间信用分配难题，核心矛盾在于奖励的真实性与密度之间的权衡：结果奖励具有高真实性但信号稀疏，过程奖励监督密集但易产生偏差和奖励黑客问题。

Method: 提出ADMIRE机制，通过将轨迹锚定到动态从成功探索中提取的可验证里程碑，构建自适应奖励系统；并采用非对称信用分配策略，对成功轨迹进行去噪，对失败轨迹进行脚手架式引导。

Result: 在AndroidWorld基准测试中，ADMIRE在不同基础模型上一致性地实现了超过10%的绝对成功率提升，并在Web导航和具身任务等异构环境中表现出强大的泛化能力。

Conclusion: ADMIRE通过里程碑锚定的自适应奖励和非对称信用分配，有效解决了长期任务中的信用分配困境，为移动GUI智能体训练提供了通用且高效的强化学习范式。

Abstract: Reinforcement Learning (RL) has emerged as a mainstream paradigm for training Mobile GUI Agents, yet it struggles with the temporal credit assignment problem inherent in long-horizon tasks. A primary challenge lies in the trade-off between reward fidelity and density: outcome reward offers high fidelity but suffers from signal sparsity, while process reward provides dense supervision but remains prone to bias and reward hacking. To resolve this conflict, we propose the Adaptive Milestone Reward (ADMIRE) mechanism. ADMIRE constructs a verifiable, adaptive reward system by anchoring trajectory to milestones, which are dynamically distilled from successful explorations. Crucially, ADMIRE integrates an asymmetric credit assignment strategy that denoises successful trajectories and scaffolds failed trajectories. Extensive experiments demonstrate that ADMIRE consistently yields over 10% absolute improvement in success rate across different base models on AndroidWorld. Moreover, the method exhibits robust generalizability, achieving strong performance across diverse RL algorithms and heterogeneous environments such as web navigation and embodied tasks.

</details>


### [114] [PASCAL: A Phase-Aware Scheduling Algorithm for Serving Reasoning-based Large Language Models](https://arxiv.org/abs/2602.11530)
*Eunyeong Cho,Jehyeon Bang,Ranggi Hwang,Minsoo Rhu*

Main category: cs.LG

TL;DR: 本文针对推理型大语言模型在推理阶段延长用户可见输出时间、增加首词延迟(TTFT)的问题，提出PASCAL——一种面向推理-回答双阶段的感知调度算法。通过分层调度器、阶段边界动态迁移、受控抢占和令牌节奏控制，在GPU内存约束下优先保障推理阶段执行，实验表明可将尾部TTFT降低高达72%，同时维持回答阶段的服务水平目标(SLO)达成率。


<details>
  <summary>Details</summary>
Motivation: 基于思维链(CoT)推理的大语言模型在服务部署时面临新挑战：推理阶段的计算会阻塞用户可见输出，导致首词延迟(TTFT)显著增加。现有服务框架无法区分推理与回答阶段的不同需求，在GPU内存受限情况下造成性能下降，亟需设计面向推理型模型的专用调度机制。

Method: PASCAL采用分层调度架构，结合实例级放置与实例内执行策略。核心机制包括：(1)在推理阶段实施优先级调度以降低TTFT；(2)在回答阶段采用受控抢占和令牌节奏控制来保障体验质量(QoE)；(3)支持在阶段边界进行动态迁移，实现负载均衡并减少干扰。

Result: 在DeepSeek-R1-Distill-Qwen-32B基准测试中，PASCAL将尾部TTFT降低最高达72%，同时保持了回答阶段的服务水平目标(SLO)达成率，验证了阶段感知调度对推理型大模型部署的有效性。

Conclusion: PASCAL通过显式区分推理与回答阶段并实施差异化调度策略，显著改善了推理型大语言模型的服务性能，特别是降低了首词延迟。该工作证明，针对推理模型特性的阶段感知调度算法对实际部署至关重要，为未来推理型LLM服务系统提供了重要设计思路。

Abstract: The emergence of reasoning-based LLMs leveraging Chain-of-Thought (CoT) inference introduces new serving challenges, as their extended reasoning phases delay user-visible output and inflate Time-To-First-Token (TTFT). Existing LLM serving frameworks fail to distinguish between reasoning and answering phases, leading to performance degradation under GPU memory constraints. We present PASCAL, a phase-aware scheduling algorithm that prioritizes reasoning to reduce TTFT while using controlled preemption and token pacing during answering to preserve Quality-of-Experience (QoE). Our hierarchical scheduler combines instance-level placement with intra-instance execution and enables dynamic migration at phase boundaries to balance load and reduce interference. Across benchmarks using DeepSeek-R1-Distill-Qwen-32B, PASCAL reduces tail TTFT by up to 72% while maintaining answering phase SLO attainment, demonstrating the importance of phase-aware scheduling for reasoning-based LLM deployment.

</details>


### [115] [AltTS: A Dual-Path Framework with Alternating Optimization for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.11533)
*Zhihang Yuan,Zhiyuan Liu,Mahesh K. Marina*

Main category: cs.LG

TL;DR: 该论文提出ALTTS双路径框架解决多元时间序列预测中的优化冲突问题。通过分离自回归与跨维度关系建模，并采用交替优化策略，有效隔离梯度噪声，显著提升了长时预测性能。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测存在两个本质不同的因素：稳定的序列内自回归动态与间歇性的跨维度交互。单一模型同时建模这两种效应会导致优化冲突——跨维度建模所需的高方差更新会破坏自回归的梯度，造成训练不稳定和长时预测精度下降。

Method: 提出ALTTS双路径框架：AR路径采用线性预测器，CR路径使用配备跨关系自注意力（CRSA）的Transformer；通过交替优化协调两个分支，实现梯度噪声隔离与跨块干扰减少。

Result: 在多个基准测试中，ALTTS持续优于现有方法，且在长时预测任务上的提升最为显著。

Conclusion: 研究表明，精心设计优化策略（而非构建更复杂的架构）是推动多元时间序列预测进展的关键驱动力。

Abstract: Multivariate time series forecasting involves two qualitatively distinct factors: (i) stable within-series autoregressive (AR) dynamics, and (ii) intermittent cross-dimension interactions that can become spurious over long horizons. We argue that fitting a single model to capture both effects creates an optimization conflict: the high-variance updates needed for cross-dimension modeling can corrupt the gradients that support autoregression, resulting in brittle training and degraded long-horizon accuracy. To address this, we propose ALTTS, a dual-path framework that explicitly decouples autoregression and cross-relation (CR) modeling. In ALTTS, the AR path is instantiated with a linear predictor, while the CR path uses a Transformer equipped with Cross-Relation Self-Attention (CRSA); the two branches are coordinated via alternating optimization to isolate gradient noise and reduce cross-block interference. Extensive experiments on multiple benchmarks show that ALTTS consistently outperforms prior methods, with the most pronounced improvements on long-horizon forecasting. Overall, our results suggest that carefully designed optimization strategies, rather than ever more complex architectures, can be a key driver of progress in multivariate time series forecasting.

</details>


### [116] [Krause Synchronization Transformers](https://arxiv.org/abs/2602.11534)
*Jingkun Liu,Yisong Yue,Max Welling,Yue Song*

Main category: cs.LG

TL;DR: 本文提出Krause Attention，受有界置信共识动力学启发，通过距离驱动的局部稀疏交互替代全局softmax聚合，有效缓解注意力坍缩与sink现象，实现线性计算复杂度，并在视觉、生成及大语言模型任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的自注意力机制依赖全局softmax归一化，导致所有token在每层均参与竞争，产生强同步动力学，趋向主导模式收敛，引发表示坍缩和注意力sink现象。

Method: 基于有界置信共识动力学原理，设计Krause Attention机制，以距离为基础的局部化、选择性稀疏交互取代相似度驱动的全局聚合，促进结构化局部同步而非全局混合。

Result: 理论关联粒子系统模型，证实有界置信交互可自然抑制注意力过度集中；将计算复杂度从O(n²)降至O(n)；在ViT、自回归生成及Llama/Qwen等模型上实现稳定性能提升且计算量大幅减少。

Conclusion: 有界置信动力学为注意力机制提供了可扩展且高效的归纳偏置，是改进Transformer架构的有效途径。

Abstract: Self-attention in Transformers relies on globally normalized softmax weights, causing all tokens to compete for influence at every layer. When composed across depth, this interaction pattern induces strong synchronization dynamics that favor convergence toward a dominant mode, a behavior associated with representation collapse and attention sink phenomena. We introduce Krause Attention, a principled attention mechanism inspired by bounded-confidence consensus dynamics. Krause Attention replaces similarity-based global aggregation with distance-based, localized, and selectively sparse interactions, promoting structured local synchronization instead of global mixing. We relate this behavior to recent theory modeling Transformer dynamics as interacting particle systems, and show how bounded-confidence interactions naturally moderate attention concentration and alleviate attention sinks. Restricting interactions to local neighborhoods also reduces runtime complexity from quadratic to linear in sequence length. Experiments across vision (ViT on CIFAR/ImageNet), autoregressive generation (MNIST/CIFAR-10), and large language models (Llama/Qwen) demonstrate consistent gains with substantially reduced computation, highlighting bounded-confidence dynamics as a scalable and effective inductive bias for attention.

</details>


### [117] [Native Reasoning Models: Training Language Models to Reason on Unverifiable Data](https://arxiv.org/abs/2602.11549)
*Yuanfu Wang,Zhixuan Liu,Xiangtian Li,Chaochao Lu,Chao Yang*

Main category: cs.LG

TL;DR: NRT框架通过让模型仅使用问答对自生成推理轨迹，将推理过程视为隐变量进行统一优化，无需人工标注和外部验证器，在复杂推理任务上超越传统SFT和验证器无关方法，且对策略崩溃具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有SFT+RLVR范式依赖高质量人工标注数据和外部验证器，导致数据收集成本高、存在认知偏见风险，且仅适用于数学/编程等可验证领域，无法覆盖大量不可验证任务。

Method: NRT将推理过程建模为隐变量优化问题，仅使用标准问答对训练，模型自生成推理路径并内在奖励提升正确答案可能性的路径，通过自增强反馈循环和鲁棒奖励聚合函数解决策略崩溃等内在失败模式。

Result: 在Llama和Mistral模型族上，NRT在验证器无关方法中达到SOTA，显著优于标准SFT和先前验证器无关RL方法，在复杂推理领域提升明显且对策略崩溃高度鲁棒。

Conclusion: 该框架提供了一种通用可扩展的路径，可构建更强大且广泛适用的推理系统，摆脱对人工标注和外部验证器的依赖。

Abstract: The prevailing paradigm for training large reasoning models--combining Supervised Fine-Tuning (SFT) with Reinforcement Learning with Verifiable Rewards (RLVR)--is fundamentally constrained by its reliance on high-quality, human-annotated reasoning data and external verifiers. This dependency incurs significant data-collection costs, risks embedding human cognitive biases, and confines the reinforcement learning stage to objectively assessable domains like mathematics and coding, leaving a wide range of unverifiable tasks beyond its scope. To overcome these limitations, we introduce NRT (Native Reasoning Training), a novel framework that cultivates complex reasoning by having the model generate its own reasoning traces using only standard question-answer pairs, thereby obviating the need for expert-written demonstrations. NRT reframes the training problem by treating the reasoning process as a latent variable. It employs a unified training objective that models reasoning as an optimization problem, intrinsically rewarding paths that increase the model's likelihood of producing the ground-truth answer. This unified perspective allows us to analyze intrinsic failure modes of prior methods, such as policy collapse, and systematically design more robust reward aggregation functions, creating a self-reinforcing feedback loop where the model learns to think in ways that resolve its own uncertainty. Empirical evaluation on Llama and Mistral model families demonstrates that NRT achieves state-of-the-art performance among verifier-free methods, significantly outperforming standard SFT baselines and prior verifier-free RL methods. Our approach yields particularly strong performance gains in complex reasoning domains and exhibits high robustness to policy collapse, offering a general, scalable path toward building more powerful and broadly applicable reasoning systems.

</details>


### [118] [TS-Memory: Plug-and-Play Memory for Time Series Foundation Models](https://arxiv.org/abs/2602.11550)
*Sisuo Lyu,Siru Zhong,Tiegang Chen,Weilin Ruan,Qingxiang Liu,Taiqiang Lv,Qingsong Wen,Raymond Chi-Wing Wong,Yuxuan Liang*

Main category: cs.LG

TL;DR: 针对时间序列基础模型在分布偏移下领域适应的挑战，本文提出TS-Memory，一种参数化记忆蒸馏方法。该方法通过离线构建泄漏安全的kNN教师模型生成置信度感知的分位数目标，再通过置信度门控监督将检索诱导的分布校正蒸馏到轻量级记忆适配器中，在推理时实现常数级开销的检索自由部署，在点预测和概率预测上均超越现有方法且效率与冻结主干相当。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型（TSFMs）虽具备强大的零样本预测能力，但在面对分布偏移的下游领域适应时仍面临挑战。现有解决方案存在根本性权衡：参数化适应易导致灾难性遗忘且需要昂贵的多领域维护；非参数化检索虽能提升预测效果，但因数据存储搜索带来高推理延迟。因此，亟需在保持高性能的同时实现高效推理的适应方法。

Method: 本文提出参数化记忆蒸馏框架TS-Memory，采用两阶段训练策略：首先构建离线泄漏安全的kNN教师模型，从检索的未来片段中合成置信度感知的分位数目标；其次通过置信度门控监督机制，将检索诱导的分布校正知识蒸馏至轻量级记忆适配器。推理阶段，TS-Memory以常数时间开销融合记忆模块与冻结主干模型的预测，实现无需检索的部署。

Result: 跨多种时间序列基础模型和基准测试的实验表明，TS-Memory在点预测和概率预测上均持续优于代表性适应方法，同时保持与冻结主干模型相当的推理效率，有效解决了精度与延迟的权衡问题。

Conclusion: TS-Memory通过参数化记忆蒸馏成功弥合了分布偏移下的领域适应鸿沟，为时间序列基础模型的实际部署提供了高效且高性能的解决方案，在保持检索增强优势的同时消除了其推理时延缺陷。

Abstract: Time Series Foundation Models (TSFMs) achieve strong zero-shot forecasting through large-scale pre-training, but adapting them to downstream domains under distribution shift remains challenging. Existing solutions face a trade-off: Parametric Adaptation can cause catastrophic forgetting and requires costly multi-domain maintenance, while Non-Parametric Retrieval improves forecasts but incurs high inference latency due to datastore search. We propose Parametric Memory Distillation and implement it as TS-Memory, a lightweight memory adapter that augments frozen TSFMs. TS-Memory is trained in two stages. First, we construct an offline, leakage-safe kNN teacher that synthesizes confidence-aware quantile targets from retrieved futures. Second, we distill this retrieval-induced distributional correction into a lightweight memory adapter via confidence-gated supervision. During inference, TS-Memory fuses memory and backbone predictions with constant-time overhead, enabling retrieval-free deployment. Experiments across diverse TSFMs and benchmarks demonstrate consistent improvements in both point and probabilistic forecasting over representative adaptation methods, with efficiency comparable to the frozen backbone.

</details>


### [119] [The Implicit Bias of Steepest Descent with Mini-batch Stochastic Gradient](https://arxiv.org/abs/2602.11557)
*Jichu Li,Xuan Tang,Difan Zou*

Main category: cs.LG

TL;DR: 本文研究多类分类中mini-batch随机最速下降的隐式偏差，系统分析了批量大小、动量和方差缩减对最大间隔行为及收敛速率的影响。关键发现包括：动量通过批量-动量权衡实现小批次收敛但降低速率，方差缩减可恢复全批量隐式偏差但牺牲收敛速度，而批量大小为1时则收敛至完全不同的偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管SignSGD和Muon等优化器可解释为不同范数几何下的最速下降，但mini-batch随机最速下降在多类分类中的隐式偏差，以及批量、动量和方差缩减如何塑造其最大间隔行为与收敛速率，仍缺乏统一的理论理解。

Method: 在一般元素级范数和Schatten-p范数框架下，刻画mini-batch随机最速下降在极限情况下的最大间隔行为，并推导完全显式、无维度依赖的收敛速率。

Result: (1) 无动量时仅大批次可收敛，产生批量依赖的间隔差距但保持全批量收敛速率；(2) 动量通过批量-动量权衡实现小批次收敛，但会减慢收敛；(3) 方差缩减对任意批量均可恢复精确全批量隐式偏差，但收敛较慢；(4) 批量大小为1时收敛至根本不同的偏差，揭示纯随机更新的关键局限。

Conclusion: 该统一分析提供了改进的先验结果的无维度收敛速率，明确了随机优化与全批量行为对齐的条件，为深入理解随机梯度最速下降算法的训练动力学奠定了基础。

Abstract: A variety of widely used optimization methods like SignSGD and Muon can be interpreted as instances of steepest descent under different norm-induced geometries. In this work, we study the implicit bias of mini-batch stochastic steepest descent in multi-class classification, characterizing how batch size, momentum, and variance reduction shape the limiting max-margin behavior and convergence rates under general entry-wise and Schatten-$p$ norms. We show that without momentum, convergence only occurs with large batches, yielding a batch-dependent margin gap but the full-batch convergence rate. In contrast, momentum enables small-batch convergence through a batch-momentum trade-off, though it slows convergence. This approach provides fully explicit, dimension-free rates that improve upon prior results. Moreover, we prove that variance reduction can recover the exact full-batch implicit bias for any batch size, albeit at a slower convergence rate. Finally, we further investigate the batch-size-one steepest descent without momentum, and reveal its convergence to a fundamentally different bias via a concrete data example, which reveals a key limitation of purely stochastic updates. Overall, our unified analysis clarifies when stochastic optimization aligns with full-batch behavior, and paves the way for perform deeper explorations of the training behavior of stochastic gradient steepest descent algorithms.

</details>


### [120] [Brain4FMs: A Benchmark of Foundation Models for Electrical Brain Signal](https://arxiv.org/abs/2602.11558)
*Fanqi Shen,Enhong Yang,Jiahe Li,Junru Hong,Xiaoran Pan,Zhizhang Yuan,Meng Li,Yang Yang*

Main category: cs.LG

TL;DR: 本文提出Brain4FMs开源评估平台，针对脑基础模型(BFMs)领域方法论不统一、评估标准缺失的问题，通过自监督学习分类法和基准设计空间映射，整合15个BFMs和18个数据集，实现标准化比较与泛化能力分析。


<details>
  <summary>Details</summary>
Motivation: 脑基础模型虽借助大规模EEG等神经信号数据快速发展，但领域缺乏统一的方法论框架和标准化评估体系，导致模型泛化性能和下游任务表现难以进行系统性比较，制约了临床诊断和神经科学研究的可迁移应用。

Method: 研究从模型与数据双维度构建基准设计空间：模型层面采用自监督学习（SSL）分类法系统化组织BFMs；数据层面梳理下游任务并整合临床与人因神经技术领域的公开数据集，最终开发具有即插即用接口的Brain4FMs评估平台。

Result: 成功构建Brain4FMs平台，集成15个代表性BFMs和18个公开数据集，支持对预训练数据、SSL策略及模型架构影响泛化与下游性能的标准化对比分析。

Conclusion: Brain4FMs平台通过提供标准化评估框架，能够指导开发更准确、可迁移的脑基础模型，促进其在神经科学临床与前沿研究中的应用。

Abstract: Brain Foundation Models (BFMs) are transforming neuroscience by enabling scalable and transferable learning from neural signals, advancing both clinical diagnostics and cutting-edge neuroscience exploration. Their emergence is powered by large-scale clinical recordings, particularly electroencephalography (EEG) and intracranial EEG, which provide rich temporal and spatial representations of brain dynamics. However, despite their rapid proliferation, the field lacks a unified understanding of existing methodologies and a standardized evaluation framework. To fill this gap, we map the benchmark design space along two axes: (i) from the model perspective, we organize BFMs under a self-supervised learning (SSL) taxonomy; and (ii) from the dataset perspective, we summarize common downstream tasks and curate representative public datasets across clinical and human-centric neurotechnology applications. Building on this consolidation, we introduce Brain4FMs, an open evaluation platform with plug-and-play interfaces that integrates 15 representative BFMs and 18 public datasets. It enables standardized comparisons and analysis of how pretraining data, SSL strategies, and architectures affect generalization and downstream performance, guiding more accurate and transferable BFMs. The code is available at https://anonymous.4open.science/r/Brain4FMs-85B8.

</details>


### [121] [Gradient Compression May Hurt Generalization: A Remedy by Synthetic Data Guided Sharpness Aware Minimization](https://arxiv.org/abs/2602.11584)
*Yujie Gu,Richeng Jin,Zhaoyang Zhang,Huaiyu Dai*

Main category: cs.LG

TL;DR: 本文揭示联邦学习中的梯度压缩会在非独立同分布数据下导致更尖锐的损失景观，从而损害泛化能力。为此提出 FedSynSAM，通过全局模型轨迹构建合成数据来准确估计 SAM 所需的全局扰动，建立了算法收敛性并通过实验验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管梯度压缩被认为能以可忽略的性能损失提升联邦学习通信效率，但其对模型泛化的实际影响被忽视。SAM 虽能通过梯度上升步骤寻找平坦极小值，但在联邦学习中因数据异构性导致全局扰动估计不准，且现有基于历史模型更新的方法在压缩场景下失效。

Method: FedSynSAM 利用全局模型的历史轨迹生成合成数据，为 SAM 的扰动步骤提供准确的全局梯度估计，从而在非独立同分布和压缩条件下实现有效的平坦极小值搜索。

Result: 理论分析证明了算法的收敛性，大量实验验证了 FedSynSAM 在梯度压缩联邦学习设置下的有效性，能够显著改善模型泛化性能。

Conclusion: 该工作首次揭示了梯度压缩对联邦学习泛化能力的负面影响，并通过合成数据辅助的扰动估计机制提供了解决方案，为设计高效且泛化能力强的联邦学习算法提供了新视角。

Abstract: It is commonly believed that gradient compression in federated learning (FL) enjoys significant improvement in communication efficiency with negligible performance degradation. In this paper, we find that gradient compression induces sharper loss landscapes in federated learning, particularly under non-IID data distributions, which suggests hindered generalization capability. The recently emerging Sharpness Aware Minimization (SAM) effectively searches for a flat minima by incorporating a gradient ascent step (i.e., perturbing the model with gradients) before the celebrated stochastic gradient descent. Nonetheless, the direct application of SAM in FL suffers from inaccurate estimation of the global perturbation due to data heterogeneity. Existing approaches propose to utilize the model update from the previous communication round as a rough estimate. However, its effectiveness is hindered when model update compression is incorporated. In this paper, we propose FedSynSAM, which leverages the global model trajectory to construct synthetic data and facilitates an accurate estimation of the global perturbation. The convergence of the proposed algorithm is established, and extensive experiments are conducted to validate its effectiveness.

</details>


### [122] [SkillRater: Untangling Capabilities in Multimodal Data](https://arxiv.org/abs/2602.11615)
*Naveen Sahi,Jeremy Dohmann,Armen Aghajanyan,Akshat Shrivastava*

Main category: cs.LG

TL;DR: 本文提出SkillRater框架，通过将数据过滤分解为针对模型不同能力维度的专门评分器，并采用渐进式选择规则，解决了传统单一度量评分在多维能力训练中的局限性，在视觉语言模型上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统数据整理方法使用单一质量评分，但当训练需要多种不同能力时，这种整体式评分无法同时为所有能力提供最优信号。本文认为质量应被理解为多维概念，每个维度对应模型必须获取的不同能力。

Method: SkillRater框架将数据过滤分解为专门评分器（每个能力一个），通过元学习在独立验证目标上训练各评分器，并采用渐进式选择规则：在训练各阶段，若样本被任一评分器排在随时间收紧的阈值之上则保留，早期保持多样性，后期集中于高价值样本。

Result: 在20亿参数视觉语言模型上验证，SkillRater在视觉理解、OCR和STEM推理三个能力维度上分别比未过滤基线提升5.63%、2.00%和3.53%。学习的评分器信号近乎正交，证实了分解捕获了真正独立的质量维度，解释了为何其优于未过滤训练和整体式学习过滤。

Conclusion: 多维度质量分解方法优于单一度量评分，能有效提升模型在各独立能力维度上的性能，且正交性验证表明不同能力需要不同的数据质量评估标准，为多能力模型的数据整理提供了新范式。

Abstract: Data curation methods typically assign samples a single quality score. We argue this scalar framing is fundamentally limited: when training requires multiple distinct capabilities, a monolithic scorer cannot maximize useful signals for all of them simultaneously. Quality is better understood as multidimensional, with each dimension corresponding to a capability the model must acquire. We introduce SkillRater, a framework that decomposes data filtering into specialized raters - one per capability, each trained via meta-learning on a disjoint validation objective - and composes their scores through a progressive selection rule: at each training stage, a sample is retained if any rater ranks it above a threshold that tightens over time, preserving diversity early while concentrating on high-value samples late. We validate this approach on vision language models, decomposing quality into three capability dimensions: visual understanding, OCR, and STEM reasoning. At 2B parameters, SkillRater improves over unfiltered baselines by 5.63% on visual understanding, 2.00% on OCR, and 3.53% on STEM on held out benchmarks. The learned rater signals are near orthogonal, confirming that the decomposition captures genuinely independent quality dimensions and explaining why it outperforms both unfiltered training and monolithic learned filtering.

</details>


### [123] [TreeGrad-Ranker: Feature Ranking via $O(L)$-Time Gradients for Decision Trees](https://arxiv.org/abs/2602.11623)
*Weida Li,Yaoliang Yu,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 本文重新审视了使用概率值（如Shapley和Banzhaf值）对决策树局部预测值进行特征排序的方法。研究发现概率值在联合优化目标上不可靠，因此提出TreeGrad直接优化联合目标，并衍生出TreeGrad-Ranker、TreeGrad-Shap和TreeProb三个算法。实验表明TreeGrad-Ranker在插入和删除指标上显著优于基线，TreeGrad-Shap的数值误差比Linear TreeShap小达10^15倍。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用概率值（Shapley/Banzhaf值）对决策树进行局部特征解释，并通过插入和删除指标评估排序质量。但研究发现，这些概率值在解决"最大化特征子集预测值同时最小化补集预测值"的联合优化问题上具有理论不可靠性，因此需要更直接可靠的优化方法。

Method: 1) TreeGrad：在O(L)时间内计算决策树（L个叶子）联合目标多线性扩展的梯度，包含加权Banzhaf值；2) TreeGrad-Ranker：聚合梯度并优化联合目标以生成特征排序；3) TreeGrad-Shap：计算具有积分参数的Beta Shapley值的数值稳定算法；4) TreeProb：将Linear TreeShap推广至所有概率值的通用框架。

Result: TreeGrad-Ranker的排序结果满足除线性公理外的所有概率值公理。数值实验显示，Linear TreeShap计算Shapley值的误差最高可达TreeGrad-Shap的10^15倍。在插入和删除指标上，TreeGrad-Ranker显著优于现有方法。

Conclusion: 通过直接优化联合目标而非依赖传统概率值，TreeGrad系列算法有效解决了特征排序的可靠性问题，其中TreeGrad-Ranker提供高质量排序，TreeGrad-Shap确保数值稳定性，TreeProb扩展了概率值计算的通用性，为决策树解释提供了更坚实的理论基础和实践工具。

Abstract: We revisit the use of probabilistic values, which include the well-known Shapley and Banzhaf values, to rank features for explaining the local predicted values of decision trees. The quality of feature rankings is typically assessed with the insertion and deletion metrics. Empirically, we observe that co-optimizing these two metrics is closely related to a joint optimization that selects a subset of features to maximize the local predicted value while minimizing it for the complement. However, we theoretically show that probabilistic values are generally unreliable for solving this joint optimization. Therefore, we explore deriving feature rankings by directly optimizing the joint objective. As the backbone, we propose TreeGrad, which computes the gradients of the multilinear extension of the joint objective in $O(L)$ time for decision trees with $L$ leaves; these gradients include weighted Banzhaf values. Building upon TreeGrad, we introduce TreeGrad-Ranker, which aggregates the gradients while optimizing the joint objective to produce feature rankings, and TreeGrad-Shap, a numerically stable algorithm for computing Beta Shapley values with integral parameters. In particular, the feature scores computed by TreeGrad-Ranker satisfy all the axioms uniquely characterizing probabilistic values, except for linearity, which itself leads to the established unreliability. Empirically, we demonstrate that the numerical error of Linear TreeShap can be up to $10^{15}$ times larger than that of TreeGrad-Shap when computing the Shapley value. As a by-product, we also develop TreeProb, which generalizes Linear TreeShap to support all probabilistic values. In our experiments, TreeGrad-Ranker performs significantly better on both insertion and deletion metrics. Our code is available at https://github.com/watml/TreeGrad.

</details>


### [124] [GP2F: Cross-Domain Graph Prompting with Adaptive Fusion of Pre-trained Graph Neural Networks](https://arxiv.org/abs/2602.11629)
*Dongxiao He,Wenxuan Sun,Yongqi Huang,Jitao Zhao,Di Jin*

Main category: cs.LG

TL;DR: 该论文研究跨领域图提示学习(GPL)的有效性机制，提出GP2F双分支框架显式集成预训练知识保留与任务特定适应，理论证明双分支融合可减小估计误差，实验验证其在跨领域少样本任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨领域GPL在实际应用中更具价值但有效性机制尚不明确；实证发现复杂GPL方法仅与全微调(FT)和线性探测(LP)等简单基线相当，需从理论上理解提示机制并探索为何集成预训练知识与任务适应能提升跨领域性能。

Method: 提出GP2F双分支框架：1)冻结分支保留预训练知识；2)适配分支使用轻量级适配器进行任务适应；通过对比损失和拓扑一致性损失在拓扑约束下进行自适应融合；从理论上证明双分支联合使用比单一分支估计误差更小。

Result: 在跨领域少样本节点分类和图分类任务上，GP2F方法在大量实验中显著优于现有GPL方法，验证了双分支集成策略的有效性。

Conclusion: 跨领域GPL的核心优势在于有效融合预训练通用知识与下游任务特定适应；GP2F通过显式建模FT与LP两个极端并自适应融合，为跨领域图模型适应提供了更优的理论和实践框架。

Abstract: Graph Prompt Learning (GPL) has recently emerged as a promising paradigm for downstream adaptation of pre-trained graph models, mitigating the misalignment between pre-training objectives and downstream tasks. Recently, the focus of GPL has shifted from in-domain to cross-domain scenarios, which is closer to the real world applications, where the pre-training source and downstream target often differ substantially in data distribution. However, why GPLs remain effective under such domain shifts is still unexplored. Empirically, we observe that representative GPL methods are competitive with two simple baselines in cross-domain settings: full fine-tuning (FT) and linear probing (LP), motivating us to explore a deeper understanding of the prompting mechanism. We provide a theoretical analysis demonstrating that jointly leveraging these two complementary branches yields a smaller estimation error than using either branch alone, formally proving that cross-domain GPL benefits from the integration between pre-trained knowledge and task-specific adaptation. Based on this insight, we propose GP2F, a dual-branch GPL method that explicitly instantiates the two extremes: (1) a frozen branch that retains pre-trained knowledge, and (2) an adapted branch with lightweight adapters for task-specific adaptation. We then perform adaptive fusion under topology constraints via a contrastive loss and a topology-consistent loss. Extensive experiments on cross-domain few-shot node and graph classification demonstrate that our method outperforms existing methods.

</details>


### [125] [TIP: Resisting Gradient Inversion via Targeted Interpretable Perturbation in Federated Learning](https://arxiv.org/abs/2602.11633)
*Jianhua Wang,Yinlin Su*

Main category: cs.LG

TL;DR: 该论文提出TIP（目标可解释扰动）防御框架，通过Grad-CAM识别关键卷积通道，在频域中有选择性地向高频分量注入扰动，在有效抵御梯度反演攻击的同时保持模型精度，显著优于传统差分隐私方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的梯度交换易受梯度反演攻击（GIA）威胁，攻击者可高保真度重建私有训练数据。现有差分隐私（DP）防御采用全参数无差别噪声注入，严重损害模型效用和收敛稳定性，亟需实现隐私-效用的更好平衡。

Method: TIP采用双目标策略：首先利用Grad-CAM量化通道敏感性，动态识别编码主要语义特征的关键卷积通道；然后将这些卷积核通过离散傅里叶变换转换至频域，选择性地向高频频谱注入校准扰动，破坏图像重建所需的细粒度细节，同时保留低频信息以维持模型准确性。

Result: 在基准数据集上的大量实验表明，TIP可使重建图像视觉上无法识别，有效抵御先进GIA攻击，同时保持与无隐私保护基线相当的全局模型精度，在隐私-效用权衡和可解释性方面显著优于现有DP基防御方法。

Conclusion: TIP通过将模型可解释性与频域分析相结合，实现了针对梯度反演攻击的精准防御，为联邦学习隐私保护提供了新的有效范式，在安全性和可用性之间取得了突破性平衡。

Abstract: Federated Learning (FL) facilitates collaborative model training while preserving data locality; however, the exchange of gradients renders the system vulnerable to Gradient Inversion Attacks (GIAs), allowing adversaries to reconstruct private training data with high fidelity. Existing defenses, such as Differential Privacy (DP), typically employ indiscriminate noise injection across all parameters, which severely degrades model utility and convergence stability. To address those limitation, we proposes Targeted Interpretable Perturbation (TIP), a novel defense framework that integrates model interpretability with frequency domain analysis. Unlike conventional methods that treat parameters uniformly, TIP introduces a dual-targeting strategy. First, leveraging Gradient-weighted Class Activation Mapping (Grad-CAM) to quantify channel sensitivity, we dynamically identify critical convolution channels that encode primary semantic features. Second, we transform these selected kernels into the frequency domain via the Discrete Fourier Transform and selectively inject calibrated perturbations into the high-frequency spectrum. By selectively perturbing high-frequency components, TIP effectively destroys the fine-grained details necessary for image reconstruction while preserving the low-frequency information crucial for model accuracy. Extensive experiments on benchmark datasets demonstrate that TIP renders reconstructed images visually unrecognizable against state-of-the-art GIAs, while maintaining global model accuracy comparable to non-private baselines, significantly outperforming existing DP-based defenses in the privacy-utility trade-off and interpretability. Code is available in https://github.com/2766733506/asldkfjssdf_arxiv

</details>


### [126] [UMAP Is Spectral Clustering on the Fuzzy Nearest-Neighbor Graph](https://arxiv.org/abs/2602.11662)
*Yang Yang*

Main category: cs.LG

TL;DR: 本文通过三步证明揭示UMAP算法本质上是执行谱聚类。研究分三步：证明UMAP的负采样优化是图上的对比学习目标；引用文献建立对比学习与谱聚类的等价性；验证UMAP谱初始化精确求解对应谱问题。结论在Gaussian核下精确，在Cauchy核下为一阶近似，成功统一了UMAP、对比学习与谱聚类的理论框架。


<details>
  <summary>Details</summary>
Motivation: 尽管UMAP是最流行的非线性降维与可视化算法之一，且其理论基础源于代数拓扑，但该算法与经典谱方法之间的精确形式化关系至今未被确立。这种理论空白阻碍了对UMAP工作机制的深入理解和算法性能的进一步提升。

Method: 采用三阶段证明策略：(1) 证明UMAP的随机负采样优化等价于在相似性图上执行的对比学习目标；(2) 引用HaoChen等人的理论结果，建立图对比学习与谱聚类的等价关系；(3) 验证UMAP的谱初始化过程能够精确计算该谱问题的线性解。

Result: 严格证明了UMAP在模糊k近邻图上执行谱聚类操作。该等价关系在使用Gaussian核时精确成立，在使用UMAP默认的Cauchy核时表现为一级近似。

Conclusion: 本研究成功构建了UMAP、对比学习与谱聚类的统一理论框架，为解释UMAP的诸多经验性现象提供了坚实的理论依据，并深化了对该算法本质行为的理解。

Abstract: UMAP (Uniform Manifold Approximation and Projection) is among the most widely used algorithms for non linear dimensionality reduction and data visualisation. Despite its popularity, and despite being presented through the lens of algebraic topology, the exact relationship between UMAP and classical spectral methods has remained informal. In this work, we prove that UMAP performs spectral clustering on the fuzzy k nearest neighbour graph. Our proof proceeds in three steps: (1) we show that UMAP's stochastic optimisation with negative sampling is a contrastive learning objective on the similarity graph; (2) we invoke the result of HaoChen et al. [8], establishing that contrastive learning on a similarity graph is equivalent to spectral clustering; and (3) we verify that UMAP's spectral initialisation computes the exact linear solution to this spectral problem. The equivalence is exact for Gaussian kernels, and holds as a first order approximation for UMAP's default Cauchy type kernel. Our result unifies UMAP, contrastive learning, and spectral clustering under a single framework, and provides theoretical grounding for several empirical observations about UMAP's behaviour.

</details>


### [127] [Fully First-Order Algorithms for Online Bilevel Optimization](https://arxiv.org/abs/2602.11665)
*Tingkai Jia,Cheng Chen*

Main category: cs.LG

TL;DR: 本文针对非凸-强凸在线双层优化问题，提出了一种完全一阶算法。通过将原问题重构为带不等式约束的单层在线问题并构造拉格朗日函数序列，消除了对海森向量积 oracle 的需求。理论证明该算法达到 O(1 + V_T + H_{2,T}) 的遗憾界，并进一步设计了自适应内迭代改进版本，获得 O(√T + V_T) 的遗憾界，在 V_T ≥ O(√T) 时具有优势。


<details>
  <summary>Details</summary>
Motivation: 现有在线双层优化算法主要依赖超梯度下降，需要计算海森向量积 oracle，导致高计算成本，限制了算法在实际应用中的效率。

Method: 将原始双层优化问题重新表述为带不等式约束的单层在线问题，通过构造拉格朗日函数序列避免隐式微分带来的海森向量积计算，提出完全一阶算法，并设计自适应内迭代策略。

Result: 基础算法达到 O(1 + V_T + H_{2,T}) 的遗憾界；改进的自适应版本实现 O(√T + V_T) 的遗憾界，当 V_T ≥ O(√T) 时具有明显优势。

Conclusion: 该方法成功消除了在线双层优化中对高阶导数信息的依赖，提供了计算效率更高的一阶解决方案，通过自适应策略进一步优化性能，为处理复杂在线双层优化问题提供了新思路。

Abstract: In this work, we study non-convex-strongly-convex online bilevel optimization (OBO). Existing OBO algorithms are mainly based on hypergradient descent, which requires access to a Hessian-vector product (HVP) oracle and potentially incurs high computational costs. By reformulating the original OBO problem as a single-level online problem with inequality constraints and constructing a sequence of Lagrangian function, we eliminate the need for HVPs arising from implicit differentiation. Specifically, we propose a fully first-order algorithm for OBO, and provide theoretical guarantees showing that it achieves regret of $O(1 + V_T + H_{2,T})$. Furthermore, we develop an improved variant with an adaptive inner-iteration scheme, which removes the dependence on the drift variation of the inner-level optimal solution and achieves regret of $O(\sqrt{T} + V_T)$. This regret have the advatange when $V_{T}\ge O(\sqrt{T})$.

</details>


### [128] [Explainable Machine-Learning based Detection of Knee Injuries in Runners](https://arxiv.org/abs/2602.11668)
*David Fuentes-Jiménez,Sara García-de-Villa,David Casillas-Pérez,Pablo Floría,Francisco-Manuel Melgarejo-Meseguer*

Main category: cs.LG

TL;DR: 本研究利用光学动作捕捉系统分析839名跑者的跑步机步态数据，通过机器学习检测跑步相关膝关节损伤。比较了传统点值特征、完整时间序列和混合特征，测试了经典算法与深度学习模型（CNN、LSTM）。CNN结合混合特征效果最佳，PFPS识别准确率77.9%，ITBS为73.8%，综合损伤71.43%，展现了动作捕捉与AI技术在临床决策中的潜力。


<details>
  <summary>Details</summary>
Motivation: 跑步普及率高但膝关节损伤（尤其是PFPS和ITBS）发生率居高不下。精确识别损伤相关的步态模式可优化临床决策，而现有研究需明确光学动作捕捉系统对动态运动学参数的捕获效能，以建立可靠的损伤分类框架。

Method: 采用光学动作捕捉系统分析包含839条跑步机记录的公开数据集，聚焦支撑相阶段。提取关节/节段角度时间序列和离散点值特征，针对三类任务（健康/损伤、健康/PFPS、健康/ITBS）比较不同特征空间。测试K近邻、高斯过程、决策树等经典算法及CNN、LSTM深度学习架构，使用准确率、精确率、召回率、F1分数评估，并应用Shapley值、显著性图、Grad-CAM等可解释性工具。

Result: 时间序列与点值特征融合显著提升检测性能。深度学习优于经典算法，CNN准确率最高：PFPS达77.9%，ITBS达73.8%，综合损伤类别达71.43%。可解释性分析确认了模型决策的关键生物力学依据。

Conclusion: 光学动作捕捉系统结合先进机器学习能有效识别膝关节损伤相关步态模式，为临床决策提供高精度工具，对预防和治疗跑步损伤具有重要价值，展现了技术驱动下运动医学诊断的广阔应用前景。

Abstract: Running is a widely practiced activity but shows a high incidence of knee injuries, especially Patellofemoral Pain Syndrome (PFPS) and Iliotibial Band Syndrome (ITBS). Identifying gait patterns linked to these injuries can improve clinical decision-making, which requires precise systems capable of capturing and analyzing temporal kinematic data.
  This study uses optical motion capture systems to enhance detection of injury-related running patterns. We analyze a public dataset of 839 treadmill recordings from healthy and injured runners to evaluate how effectively these systems capture dynamic parameters relevant to injury classification. The focus is on the stance phase, using joint and segment angle time series and discrete point values.
  Three classification tasks are addressed: healthy vs. injured, healthy vs. PFPS, and healthy vs. ITBS. We examine different feature spaces, from traditional point-based metrics to full stance-phase time series and hybrid representations. Multiple models are tested, including classical algorithms (K-Nearest Neighbors, Gaussian Processes, Decision Trees) and deep learning architectures (CNNs, LSTMs).
  Performance is evaluated with accuracy, precision, recall, and F1-score. Explainability tools such as Shapley values, saliency maps, and Grad-CAM are used to interpret model behavior. Results show that combining time series with point values substantially improves detection. Deep learning models outperform classical ones, with CNNs achieving the highest accuracy: 77.9% for PFPS, 73.8% for ITBS, and 71.43% for the combined injury class.
  These findings highlight the potential of motion capture systems coupled with advanced machine learning to identify knee injury-related running patterns.

</details>


### [129] [DRACO: a Cross-Domain Benchmark for Deep Research Accuracy, Completeness, and Objectivity](https://arxiv.org/abs/2602.11685)
*Joey Zhong,Hao Zhang,Clare Southern,Jeremy Yang,Thomas Wang,Kate Jung,Shu Zhang,Denis Yarats,Johnny Ho,Jerry Ma*

Main category: cs.LG

TL;DR: 本文介绍了DRACO基准，这是一个用于评估复杂深度研究任务的基准测试，涵盖10个领域和40个国家的信息源，基于Perplexity深度研究系统的真实使用模式构建，从事实准确性、分析广度深度、呈现质量和引用质量四个维度进行评分，已开源。


<details>
  <summary>Details</summary>
Motivation: 随着大型深度研究系统的发展，需要一个能够全面评估其性能的基准测试。现有的评估方法可能无法充分反映真实世界复杂任务的需求。因此，作者从Perplexity深度研究系统的匿名化真实使用数据中构建了一个涵盖多领域、多国家的复杂研究任务基准，以客观评估这些系统在准确性、完整性、客观性和引用质量等方面的表现。

Method: 该方法从Perplexity深度研究系统的去标识化请求数据集中抽样任务，然后进行筛选和增强处理，确保任务匿名化、开放性和复杂性，并可客观评估且代表真实世界用例。评估采用任务特定的评分标准，从四个维度对输出进行评分：事实准确性（accuracy）、分析的广度和深度（completeness）、呈现质量（包括客观性）以及引用质量。

Result: 成功构建了DRACO基准，该基准包含来自10个领域、40个国家信息源的复杂深度研究任务，并已在Hugging Face平台开源发布（https://hf.co/datasets/perplexity-ai/draco），供研究社区使用。

Conclusion: DRACO基准提供了一个基于真实世界使用模式的、可客观评估的深度研究任务评测标准，能够全面评估AI系统在复杂研究任务中的表现，为该领域的研究和发展提供了有价值的资源。

Abstract: We present DRACO (Deep Research Accuracy, Completeness, and Objectivity), a benchmark of complex deep research tasks. These tasks, which span 10 domains and draw on information sources from 40 countries, originate from anonymized real-world usage patterns within a large-scale deep research system. Tasks are sampled from a de-identified dataset of Perplexity Deep Research requests, then filtered and augmented to ensure that the tasks are anonymized, open-ended and complex, objectively evaluable, and representative of the broad scope of real-world deep research use cases. Outputs are graded against task-specific rubrics along four dimensions: factual accuracy (accuracy), breadth and depth of analysis (including completeness), presentation quality (including objectivity), and citation quality. DRACO is publicly available at https://hf.co/datasets/perplexity-ai/draco.

</details>


### [130] [ANML: Attribution-Native Machine Learning with Guaranteed Robustness](https://arxiv.org/abs/2602.11690)
*Oliver Zahn,Matt Beton,Simran Chana*

Main category: cs.LG

TL;DR: ANML框架通过梯度一致性、验证状态、贡献者声誉和时间相关性四个质量因子动态加权训练样本，融合模型观察信号与数据溯源信息，在5个数据集上实现33-72%的误差降低，显著提升数据效率和归属能力。


<details>
  <summary>Details</summary>
Motivation: 当前前沿AI系统训练管道对所有样本一视同仁，导致诺贝尔级专家贡献与未验证提交获得相同权重，造成高价值数据浪费并引入质量风险，亟需细粒度的数据价值评估机制。

Method: 提出ANML（Attribution-Native Machine Learning）框架，采用四因子加权：梯度一致性(q)、验证状态(v)、贡献者声誉(r)、时间相关性(T)。通过Two-Stage Adaptive门控机制结合模型梯度信号与外部溯源信息，生成贡献者级别的质量权重，并保证性能不低于最佳基线。

Result: 在178-32,561样本规模的5个数据集中，ANML相比梯度-only基线实现33-72%错误率降低；20%高质量数据加权训练效果超越100%均匀加权数据47%；贡献者级归因对微妙腐败的检测改进是样本级方法的1.3-5.3倍，优势随检测难度增加而扩大，且能抵御凭证伪造与梯度对齐的联合攻击。

Conclusion: 质量加权训练结合数据溯源信息能同步提升模型性能与可归因性，Two-Stage Adaptive机制确保鲁棒性下限，为前沿AI系统高效利用专家数据提供了可扩展的范式，尤其在对抗性环境下展现显著优势。

Abstract: Frontier AI systems increasingly train on specialized expert data, from clinical records to proprietary research to curated datasets, yet current training pipelines treat all samples identically. A Nobel laureate's contribution receives the same weight as an unverified submission. We introduce ANML (Attribution-Native Machine Learning), a framework that weights training samples by four quality factors: gradient-based consistency (q), verification status (v), contributor reputation (r), and temporal relevance (T). By combining what the model observes (gradient signals) with what the system knows about data provenance (external signals), ANML produces per-contributor quality weights that simultaneously improve model performance and enable downstream attribution. Across 5 datasets (178-32,561 samples), ANML achieves 33-72% error reduction over gradient-only baselines. Quality-weighted training is data-efficient: 20% high-quality data outperforms 100% uniformly weighted data by 47%. A Two-Stage Adaptive gating mechanism guarantees that ANML never underperforms the best available baseline, including under strategic joint attacks combining credential faking with gradient alignment. When per-sample detection fails against subtle corruption, contributor-level attribution provides 1.3-5.3x greater improvement than sample-level methods, with the advantage growing as corruption becomes harder to detect.

</details>


### [131] [SpiralFormer: Looped Transformers Can Learn Hierarchical Dependencies via Multi-Resolution Recursion](https://arxiv.org/abs/2602.11698)
*Chengting Yu,Xiaobo Shu,Yadao Wang,Yizhen Zhang,Haoyi Wu,You Wu,Rujiao Long,Ziheng Chen,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: SpiralFormer提出多分辨率递归循环Transformer，通过动态调整序列分辨率实现迭代功能特化，在160M-1.4B参数规模下均优于循环/非循环基线，揭示序列分辨率是扩展递归架构的新维度。


<details>
  <summary>Details</summary>
Motivation: 早期循环Transformer虽解耦计算深度与参数深度，但同等计算量下性能常逊于非循环基线；现有改进机制仍采用固定全token分辨率，忽略了压缩潜在表示的计算效率优势，未能充分利用递归架构潜力。

Method: 提出SpiralFormer，设计多分辨率递归调度机制，使循环Transformer在迭代过程中动态调整序列分辨率，诱导不同尺度上的功能特化，从而学习分层依赖关系。

Result: 在160M至1.4B参数规模的模型上，SpiralFormer相比循环和非循环基线均展现出更优的参数效率与计算效率，验证了多分辨率递归的有效性。

Conclusion: 序列分辨率可作为扩展递归架构的关键新维度；多分辨率递归通过跨尺度的迭代功能特化，使模型更高效地捕获层次化依赖，为构建高效迭代推理模型提供架构原语。

Abstract: Recursive (looped) Transformers decouple computational depth from parameter depth by repeatedly applying shared layers, providing an explicit architectural primitive for iterative refinement and latent reasoning. However, early looped Transformers often underperform non-recursive baselines of equal compute. While recent literature has introduced more effective recursion mechanisms to mitigate this gap, existing architectures still operate at a fixed, full-token resolution, neglecting the potential efficiency of computing over compressed latent representations. In this paper, we propose SpiralFormer, a looped Transformer that executes recurrence under a multi-resolution recursion schedule. We provide probing evidence that multi-resolution recursion enables the model to learn hierarchical dependencies by inducing iteration-wise functional specialization across different scales. Empirically, SpiralFormer achieves better parameter and compute efficiency than both looped and non-looped baselines across model scales from 160M to 1.4B, establishing sequence resolution as a potential axis for scaling recursive architectures.

</details>


### [132] [TabSieve: Explicit In-Table Evidence Selection for Tabular Prediction](https://arxiv.org/abs/2602.11700)
*Yongyao Wang,Ziqi Miao,Lu Yang,Haonan Jia,Wenting Yan,Chen Qian,Lijun Li*

Main category: cs.LG

TL;DR: 针对表格预测中利用行内数据作为少样本证据时存在的模型脆弱性和噪声敏感问题，本文提出TabSieve框架，通过显式筛选证据再预测的两阶段范式提升性能。该方法构建4万条高质量推理轨迹数据集，并引入TAB-GRPO强化学习策略联合优化证据选择与预测精度，在75个分类和52个回归任务上分别取得2.92%和4.45%的平均性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有表格模型通常采用实例级推理，LLM提示方法脆弱且无法一致地利用相关行信息，噪声上下文会损害性能。如何让模型显式、可审计地利用表格内少样本证据，同时提升对噪声的鲁棒性，是亟待解决的关键挑战。

Method: 提出"筛选-预测"两阶段框架TabSieve：首先选择信息量大的行作为证据，然后基于选定证据预测目标值。构建TabSieve-SFT-40K数据集，通过强教师模型从331个真实表格合成高质量推理轨迹并严格过滤。引入TAB-GRPO强化学习方法，分别优化证据选择和预测正确性奖励，并通过动态任务优势平衡稳定混合回归与分类训练。

Result: 在75个分类和52个回归表格的留出基准测试中，TabSieve在不同少样本预算下均稳定优于基线，分类任务平均提升2.92%，回归任务平均提升4.45%。分析表明，该方法能更集中关注选定证据，显著提升对噪声上下文的鲁棒性。

Conclusion: TabSieve通过显式证据筛选机制解决了表格预测中证据利用不一致和噪声敏感问题。结合高质量合成数据与多奖励强化学习策略，该方法为表格少样本学习提供了可审计、鲁棒的解决方案，验证了结构化推理在提升模型性能方面的有效性。

Abstract: Tabular prediction can benefit from in-table rows as few-shot evidence, yet existing tabular models typically perform instance-wise inference and LLM-based prompting is often brittle. Models do not consistently leverage relevant rows, and noisy context can degrade performance. To address this challenge, we propose TabSieve, a select-then-predict framework that makes evidence usage explicit and auditable. Given a table and a query row, TabSieve first selects a small set of informative rows as evidence and then predicts the missing target conditioned on the selected evidence. To enable this capability, we construct TabSieve-SFT-40K by synthesizing high-quality reasoning trajectories from 331 real tables using a strong teacher model with strict filtering. Furthermore, we introduce TAB-GRPO, a reinforcement learning recipe that jointly optimizes evidence selection and prediction correctness with separate rewards, and stabilizes mixed regression and classification training via dynamic task-advantage balancing. Experiments on a held-out benchmark of 75 classification and 52 regression tables show that TabSieve consistently improves performance across shot budgets, with average gains of 2.92% on classification and 4.45% on regression over the second-best baseline. Further analysis indicates that TabSieve concentrates more attention on the selected evidence, which improves robustness to noisy context.

</details>


### [133] [Potential-energy gating for robust state estimation in bistable stochastic systems](https://arxiv.org/abs/2602.11712)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 本文提出势能门控方法，通过调制观测噪声协方差实现双势阱随机动力学系统的鲁棒状态估计。在势能极小值附近信任观测，在势垒附近逐步折扣观测。该方法在多种卡尔曼滤波器和粒子滤波器中实现，仅需两个超参数，在合成基准测试中相比标准EKF可获得57-80%的RMSE提升，并成功应用于NGRIP冰芯记录中的Dansgaard-Oeschger事件分析。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯滤波器对所有状态空间区域等同处理，而约束滤波器仅施加硬性边界。针对双势阱随机动力学系统中的状态估计问题，需要一种基于物理机制的方法，能够根据系统所处的势能位置动态调整对观测的信任程度，以实现鲁棒的状态估计。

Method: 势能门控方法通过已知的势能函数局部值调制观测噪声协方差：在势能极小值（亚稳态）附近信任观测，在势垒（过渡区）附近逐步折扣观测。该方法可嵌入扩展卡尔曼滤波、无迹卡尔曼滤波、集合卡尔曼滤波、自适应卡尔曼滤波和粒子滤波中，仅需增加两个超参数。

Result: 合成基准测试显示，在含10%异常值的情况下，相比标准EKF获得57-80%的RMSE显著改善（p < 10^{-15}）。连续能量景观比仅使用最近势阱距离的朴素基线额外提升约21个百分点。方法对误设定具有鲁棒性：即使势能参数偏离真实值50%，改善仍不低于47%。在噪声诱导跃迁下保持68%改善，而朴素基线降至30%。应用于NGRIP冰芯记录时，估计出不对称参数γ = -0.109（95% CI排除零），异常值比例解释了91%的滤波改善方差。

Conclusion: 势能门控提供了一种物理机制驱动的鲁棒状态估计框架，在双势阱系统中表现显著优于传统方法，且对参数误设定具有强鲁棒性。该方法成功应用于真实气候事件分析，揭示了Dansgaard-Oeschger事件的不对称性，证明了其在复杂系统状态估计中的实用价值。

Abstract: We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from purely statistical robust filters, which treat all regions of state space identically, and from constrained filters, which impose hard bounds on states rather than modulating observation trust. We implement the gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Synthetic benchmarks on a Ginzburg-Landau double-well process with 10% outlier contamination and Monte Carlo validation over 100 replications show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p < 10^{-15}, Wilcoxon signed-rank test). A naive topological baseline using only distance to the nearest well achieves 57%, confirming that the continuous energy landscape adds an additional ~21 percentage points. The method is robust to misspecification: even when assumed potential parameters deviate by 50% from their true values, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry parameter gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011], excluding zero) and demonstrating that outlier fraction explains 91% of the variance in filter improvement.

</details>


### [134] [DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels](https://arxiv.org/abs/2602.11715)
*Haolei Bai,Lingcheng Kong,Xueyi Chen,Jianmian Wang,Zhiqiang Tao,Huan Wang*

Main category: cs.LG

TL;DR: 本文针对CUDA内核生成的高专业化与数据稀缺问题，构建了CuKe数据集，提出双阶段强化学习框架BiC-RL，并开发了DICE系列扩散大语言模型，在KernelBench上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在代码生成中存在序列依赖局限，而扩散大语言模型(dLLM)虽具备并行生成与全局规划的天然优势，但应用于高度专业化的CUDA内核生成时面临两大核心障碍：领域特异性极强且高质量训练数据极度匮乏。

Method: 1. 构建CuKe：专为高性能CUDA内核优化的增强监督微调数据集；2. 提出BiC-RL双阶段强化学习框架，包含CUDA内核填充预训练与端到端生成精调两个阶段；3. 开发DICE系列模型，涵盖1.7B/4B/8B三种参数规模。

Result: 在KernelBench基准测试中，DICE系列模型显著超越同参数量级的自回归与扩散大语言模型，在CUDA内核生成任务上取得当前最先进(SOTA)性能。

Conclusion: 通过构建领域专用数据集与双阶段强化学习范式，DICE成功验证了扩散架构在专业代码生成领域的巨大潜力，为CUDA内核自动化生成设立了新的性能标杆，为其他专业化代码生成任务提供了方法论参考。

Abstract: Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.

</details>


### [135] [Dopamine: Brain Modes, Not Brains](https://arxiv.org/abs/2602.11726)
*Shervin Ghasemlou*

Main category: cs.LG

TL;DR: 该论文提出\methodname{}，一种受神经调节启发的参数高效微调方法。不同于LoRA等权重空间更新，该方法冻结基础权重，学习神经元级阈值和增益，实现激活空间的模式选择。在MNIST旋转任务上验证表明，相比LoRA，该方法以更少参数（每层数百个）和稍低精度换取了更清晰的神经元激活可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法如LoRA通过添加权重增量进行微调，但权重变化难以解释，无法揭示模型内部哪些计算被重用或绕过。为此，作者提出从神经调节视角重新思考适应过程：将微调视为选择并调整现有计算模式，而非修改底层权重。

Method: \methodname{}是一种激活空间PEFT技术。核心思想是冻结预训练模型的权重，仅学习每个神经元的阈值（threshold）和增益（gain）参数。训练时，通过平滑门控机制决定神经元激活的参与程度；推理时可将门控硬化，实现显式的条件计算和神经元级归因分析。

Result: 在MNIST 0°与45°旋转版本的概念验证中：首先在混合数据集上预训练MLP作为基础模型并冻结权重，然后使用\methodname{}适配旋转任务。结果显示：跨随机种子，\methodname{}相比冻结基线提升旋转准确率，每层仅数百可训练参数，且呈现部分激活稀疏性（少数单元强激活）。相比LoRA，\methodname{}以部分精度损失为代价，显著减少了参数量并提供了更直观的神经元激活解释机制。

Conclusion: \methodname{}通过激活空间调节为PEFT提供了可解释性新视角，揭示了任务适应中神经元的选择性参与机制。该方法在参数效率与可解释性间取得平衡，但其性能受限于基础模型特征：当冻结模型缺乏目标模式所需特征时，表达能力会下降。

Abstract: Parameter-efficient fine-tuning (PEFT) methods such as \lora{} adapt large pretrained models by adding small weight-space updates. While effective, weight deltas are hard to interpret mechanistically, and they do not directly expose \emph{which} internal computations are reused versus bypassed for a new task. We explore an alternative view inspired by neuromodulation: adaptation as a change in \emph{mode} -- selecting and rescaling existing computations -- rather than rewriting the underlying weights. We propose \methodname{}, a simple activation-space PEFT technique that freezes base weights and learns per-neuron \emph{thresholds} and \emph{gains}. During training, a smooth gate decides whether a neuron's activation participates; at inference the gate can be hardened to yield explicit conditional computation and neuron-level attributions.
  As a proof of concept, we study ``mode specialization'' on MNIST (0$^\circ$) versus rotated MNIST (45$^\circ$). We pretrain a small MLP on a 50/50 mixture (foundation), freeze its weights, and then specialize to the rotated mode using \methodname{}. Across seeds, \methodname{} improves rotated accuracy over the frozen baseline while using only a few hundred trainable parameters per layer, and exhibits partial activation sparsity (a minority of units strongly active). Compared to \lora{}, \methodname{} trades some accuracy for substantially fewer trainable parameters and a more interpretable ``which-neurons-fire'' mechanism. We discuss limitations, including reduced expressivity when the frozen base lacks features needed for the target mode.

</details>


### [136] [U-Former ODE: Fast Probabilistic Forecasting of Irregular Time Series](https://arxiv.org/abs/2602.11738)
*Ilya Kuleshov,Alexander Marusov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 本文针对不规则采样时间序列的概率预测难题，提出UFO (U-Former ODE)架构，通过融合U-Net的多尺度特征提取、Transformer的全局建模与神经CDE的连续时间动态，构建可并行化的因果模型。在五项基准测试中，UFO以高达15倍的速度优势全面超越十种先进方法，兼具全局感受野与局部时序敏感性。


<details>
  <summary>Details</summary>
Motivation: 在医疗与金融等领域，不规则采样时间序列的概率预测至关重要，但现有神经控制微分方程(Neural CDE)方法虽能有效建模连续动态，却存在固有串行计算导致的运行缓慢、可扩展性受限及全局上下文获取不足等缺陷，亟需改进。

Method: 提出UFO架构，创新性地将可并行化的U-Net多尺度特征提取机制、Transformer强大的全局建模能力与Neural CDE的连续时间动态建模相结合，构建完全因果且可并行计算的模型，同时保留对局部时序动态的强敏感性。

Result: 在涵盖规则与不规则采样的五项标准基准测试中，UFO持续优于十种当前最先进神经基线方法，预测精度显著提升；推理速度达到传统Neural CDE的15倍，并在长序列与高维多元序列上保持稳定优异性能。

Conclusion: UFO通过架构创新有效解决了Neural CDE的串行计算瓶颈，实现了全局上下文感知与局部动态捕捉的平衡，为不规则时间序列预测提供了高效可扩展的新范式。

Abstract: Probabilistic forecasting of irregularly sampled time series is crucial in domains such as healthcare and finance, yet it remains a formidable challenge. Existing Neural Controlled Differential Equation (Neural CDE) approaches, while effective at modelling continuous dynamics, suffer from slow, inherently sequential computation, which restricts scalability and limits access to global context. We introduce UFO (U-Former ODE), a novel architecture that seamlessly integrates the parallelizable, multiscale feature extraction of U-Nets, the powerful global modelling of Transformers, and the continuous-time dynamics of Neural CDEs. By constructing a fully causal, parallelizable model, UFO achieves a global receptive field while retaining strong sensitivity to local temporal dynamics. Extensive experiments on five standard benchmarks -- covering both regularly and irregularly sampled time series -- demonstrate that UFO consistently outperforms ten state-of-the-art neural baselines in predictive accuracy. Moreover, UFO delivers up to 15$\times$ faster inference compared to conventional Neural CDEs, with consistently strong performance on long and highly multivariate sequences.

</details>


### [137] [MUSE: Multi-Tenant Model Serving With Seamless Model Updates](https://arxiv.org/abs/2602.11776)
*Cláudio Correia,Alberto E. A. Ferreira,Lucas Martins,Miguel P. Bento,Sofia Guerreiro,Ricardo Ribeiro Pereira,Ana Sofia Gomes,Jacopo Bono,Hugo Ferreira,Pedro Bizarro*

Main category: cs.LG

TL;DR: 针对多租户评分服务中模型更新导致的阈值失效问题，本文提出MUSE框架，通过动态意图路由和两级分数转换解耦模型分数与决策边界，实现无缝模型更新。该框架在Feedzai生产环境处理超550亿事件，将模型交付时间从数周缩短至分钟级，有效应对欺诈攻击并节省数百万美元成本。


<details>
  <summary>Details</summary>
Motivation: 在二元分类系统中，决策阈值依赖模型分数分布和客户业务逻辑。模型重训练引发分数分布漂移，导致已部署阈值失效。在多租户SaaS架构中，决策边界位于客户侧基础设施，跨数百客户协调阈值更新造成沉重运维负担，导致模型迭代停滞，无法快速适应新型攻击模式。

Method: MUSE采用动态意图路由机制实现模型共享与基础设施复用，结合两级分数转换技术将原始模型输出映射至稳定参考分布，从而在架构层面解耦模型评分与客户决策逻辑，支持无中断的模型热更新。

Result: Feedzai生产部署显示，MUSE支持每秒超千次事件处理，12个月内处理550亿+事件，服务数十租户，满足高可用与低延迟SLA。模型上线周期从周级压缩至分钟级，显著增强欺诈检测系统的攻击适应能力，年化节省欺诈损失与运维成本达数百万美元。

Conclusion: MUSE通过创新的架构设计解决了多租户模型服务的更新瓶颈，实现了高效、弹性的模型迭代，在保障服务性能的前提下大幅降低运营成本和安全风险，为大规模在线决策系统提供了可行的技术方案。

Abstract: In binary classification systems, decision thresholds translate model scores into actions. Choosing suitable thresholds relies on the specific distribution of the underlying model scores but also on the specific business decisions of each client using that model. However, retraining models inevitably shifts score distributions, invalidating existing thresholds. In multi-tenant Score-as-a-Service environments, where decision boundaries reside in client-managed infrastructure, this creates a severe bottleneck: recalibration requires coordinating threshold updates across hundreds of clients, consuming excessive human hours and leading to model stagnation. We introduce MUSE, a model serving framework that enables seamless model updates by decoupling model scores from client decision boundaries. Designed for multi-tenancy, MUSE optimizes infrastructure re-use by sharing models via dynamic intent-based routing, combined with a two-level score transformation that maps model outputs to a stable, reference distribution. Deployed at scale by Feedzai, MUSE processes over a thousand events per second, and over 55 billion events in the last 12 months, across several dozens of tenants, while maintaining high-availability and low-latency guarantees. By reducing model lead time from weeks to minutes, MUSE promotes model resilience against shifting attacks, saving millions of dollars in fraud losses and operational costs.

</details>


### [138] [Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning](https://arxiv.org/abs/2602.11779)
*Haoran Dang,Cuiling Lan,Hai Wan,Xibin Zhao,Yan Lu*

Main category: cs.LG

TL;DR: 提出TAMPO框架，将温度控制重新建模为可学习的元策略，通过轨迹引导和奖励驱动的双层循环机制实现强化学习中的在线自适应温度调整，在数学推理任务上显著优于传统固定或启发式温度调度方法。


<details>
  <summary>Details</summary>
Motivation: 温度作为LLM生成的关键超参数，控制探索与利用的权衡。现有静态或启发式温度调度无法动态适应强化学习训练需求，导致策略改进受限。因此需要一种能在线自适应调整温度的方法，使探索与策略优化目标对齐。

Method: TAMPO采用分层双层优化结构：内层循环在元策略选择的温度下采样轨迹并更新LLM策略（如GRPO）；外层循环基于高优势轨迹的奖励信号更新温度分布参数。该机制无需额外rollout即可实现自适应，通过轨迹引导直接优化探索策略。

Result: 在五个数学推理基准测试中，TAMPO相比固定温度和启发式温度调度的基线方法取得显著性能提升。

Conclusion: 研究表明温度可作为LLM强化学习中有效的可学习元策略，TAMPO为自适应探索提供了新范式，证明了将超参数控制转化为元策略优化的有效性。

Abstract: Temperature is a crucial hyperparameter in large language models (LLMs), controlling the trade-off between exploration and exploitation during text generation. High temperatures encourage diverse but noisy outputs, while low temperatures produce focused outputs but may cause premature convergence. Yet static or heuristic temperature schedules fail to adapt to the dynamic demands of reinforcement learning (RL) throughout training, often limiting policy improvement. We propose Temperature Adaptive Meta Policy Optimization (TAMPO), a new framework that recasts temperature control as a learnable meta-policy. TAMPO operates through a hierarchical two-loop process. In the inner loop, the LLM policy is updated (e.g., using GRPO) with trajectories sampled at the temperature selected by the meta-policy. In the outer loop, meta-policy updates the distribution over candidate temperatures by rewarding those that maximize the likelihood of high-advantage trajectories. This trajectory-guided, reward-driven mechanism enables online adaptation without additional rollouts, directly aligning exploration with policy improvement. On five mathematical reasoning benchmarks, TAMPO outperforms baselines using fixed or heuristic temperatures, establishing temperature as an effective learnable meta-policy for adaptive exploration in LLM reinforcement learning. Accepted at ICLR 2026.

</details>


### [139] [Safe Fairness Guarantees Without Demographics in Classification: Spectral Uncertainty Set Perspective](https://arxiv.org/abs/2602.11785)
*Ainhize Barrainkua,Santiago Mazuelas,Novi Quadrianto,Jose A. Lozano*

Main category: cs.LG

TL;DR: SPECTRE是一种无需人口统计信息的公平分类方法，通过调整傅里叶特征映射的频谱并约束最坏情况分布与经验分布的偏差，在保持公平性的同时避免了过度悲观假设。在20个州的调查数据上，它比现有方法（包括那些有群体信息的方法）提供了更高的公平保证和更稳定的性能，并有理论保证。


<details>
  <summary>Details</summary>
Motivation: 自动化分类系统日益普及但可能强化社会偏见。现有公平性方法大多需要实例的群体信息，而这在实践中难以获得。无需人口统计信息的公平性方法通常采用鲁棒优化技术，但现有方法过度强调异常值或过度悲观场景，损害了整体性能和公平性。

Method: 提出SPECTRE，一种极小极大公平方法，通过调整简单傅里叶特征映射的频谱，并约束最坏情况分布相对于经验分布的偏离程度。提供理论分析，推导出可计算的最坏情况误差边界，并刻画导致这些极值性能的分布。

Result: 在美国社区调查数据集（20个州）上的广泛实验表明，SPECTRE相比现有最先进方法（包括那些有群体信息的方法）提供了最高的平均公平保证值和最窄的四分位距。理论分析推导了个体群体和整体总体最坏情况误差的可计算边界，并刻画了导致极值性能的最坏情况分布。

Conclusion: SPECTRE通过避免过度悲观假设，在无需人口统计信息的公平分类方面优于现有方法，既提供了更强的公平性保证，又保持了更好的整体性能，同时具备理论保证。

Abstract: As automated classification systems become increasingly prevalent, concerns have emerged over their potential to reinforce and amplify existing societal biases. In the light of this issue, many methods have been proposed to enhance the fairness guarantees of classifiers. Most of the existing interventions assume access to group information for all instances, a requirement rarely met in practice. Fairness without access to demographic information has often been approached through robust optimization techniques,which target worst-case outcomes over a set of plausible distributions known as the uncertainty set. However, their effectiveness is strongly influenced by the chosen uncertainty set. In fact, existing approaches often overemphasize outliers or overly pessimistic scenarios, compromising both overall performance and fairness. To overcome these limitations, we introduce SPECTRE, a minimax-fair method that adjusts the spectrum of a simple Fourier feature mapping and constrains the extent to which the worst-case distribution can deviate from the empirical distribution. We perform extensive experiments on the American Community Survey datasets involving 20 states. The safeness of SPECTRE comes as it provides the highest average values on fairness guarantees together with the smallest interquartile range in comparison to state-of-the-art approaches, even compared to those with access to demographic group information. In addition, we provide a theoretical analysis that derives computable bounds on the worst-case error for both individual groups and the overall population, as well as characterizes the worst-case distributions responsible for these extremal performances

</details>


### [140] [Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing](https://arxiv.org/abs/2602.11786)
*Keita Broadwater*

Main category: cs.LG

TL;DR: 传统大模型基准测试侧重任务广度而忽视重复相同提示时的运行故障。本文提出加速提示压力测试(APST)框架，通过重复采样相同提示并建模为伯努利/二项分布，量化推理失败概率。研究发现：标准基准相似的不同模型在重复采样下表现出显著差异的失败率，尤其随温度升高，单样本评估会掩盖持续使用时的可靠性差异，凸显APST对部署风险评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型基准测试主要评估多样任务的广度，但真实世界部署暴露出新的风险类别：在相同或近乎相同提示上重复推理导致的操作故障。在高风险场景中，持续使用下的响应一致性和安全性至关重要。现有评估方法无法揭示模型在长期运行中的潜在失败模式，如幻觉、拒绝不一致和不安全补全。

Method: 提出加速提示压力测试(APST)框架，其核心是深度导向的评估方法，灵感来自可靠性工程。在受控操作条件下（如解码温度），对相同提示进行重复采样，将失败视为独立推理事件的随机结果，并使用伯努利和二项模型形式化安全失败，估算每次推理的失败概率，从而实现对不同模型和配置间可靠性的量化比较。

Result: 在AIR-BENCH安全提示上对多个指令微调模型应用APST发现，具有相似基准得分的模型在重复采样下表现出显著不同的经验失败率，且随着温度升高差异更加明显。结果表明，浅层的单样本评估会掩盖模型在持续使用时的有意义可靠性差异。

Conclusion: APST通过提供评估重复推理下模型安全和可靠性的实用框架，补充了现有基准测试，弥补了基准对齐与部署导向风险评估之间的鸿沟，为高风险场景中的模型选择和配置提供了量化依据。

Abstract: Traditional benchmarks for large language models (LLMs) primarily assess safety risk through breadth-oriented evaluation across diverse tasks. However, real-world deployment exposes a different class of risk: operational failures arising from repeated inference on identical or near-identical prompts rather than broad task generalization. In high-stakes settings, response consistency and safety under sustained use are critical. We introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework inspired by reliability engineering. APST repeatedly samples identical prompts under controlled operational conditions (e.g., decoding temperature) to surface latent failure modes including hallucinations, refusal inconsistency, and unsafe completions. Rather than treating failures as isolated events, APST models them as stochastic outcomes of independent inference events. We formalize safety failures using Bernoulli and binomial models to estimate per-inference failure probabilities, enabling quantitative comparison of reliability across models and decoding configurations. Applying APST to multiple instruction-tuned LLMs evaluated on AIR-BENCH-derived safety prompts, we find that models with similar benchmark-aligned scores can exhibit substantially different empirical failure rates under repeated sampling, particularly as temperature increases. These results demonstrate that shallow, single-sample evaluation can obscure meaningful reliability differences under sustained use. APST complements existing benchmarks by providing a practical framework for evaluating LLM safety and reliability under repeated inference, bridging benchmark alignment and deployment-oriented risk assessment.

</details>


### [141] [Temporal Difference Learning with Constrained Initial Representations](https://arxiv.org/abs/2602.11800)
*Jiafei Lyu,Jingwen Yang,Zhongjian Qiao,Runze Liu,Zeyuan Liu,Deheng Ye,Zongqing Lu,Xiu Li*

Main category: cs.LG

TL;DR: 针对离策略强化学习样本效率低下的问题，本文提出约束初始表示(CIR)框架，通过初始层Tanh函数约束、跳跃连接和凸Q学习三组件，理论分析收敛性并在连续控制任务上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有离策略强化学习方法虽通过架构改进和算法创新提升了样本效率，但普遍忽视了对输入数据初始表示的直接约束潜力。这种约束能直观缓解分布偏移问题并稳定训练过程，为提升样本效率提供了新研究视角。

Method: 理论上线性函数近似下分析了Tanh时序差分学习的收敛性质；实践提出CIR框架包含：(i)初始层Tanh激活配合归一化方法以稳定表示；(ii)跳跃连接模块建立浅层到深层的线性通路；(iii)凸Q学习实现更灵活的价值估计并减轻保守性。

Result: 在多项连续控制任务上的实证结果表明，CIR框架表现出强劲性能，其效果具有竞争力，甚至 surpass 现有强基线方法，验证了约束初始表示策略的有效性。

Conclusion: 本研究通过约束初始表示空间为离策略强化学习样本效率提升提供了新思路。CIR框架结合Tanh激活、跳跃连接和凸Q学习，在理论分析和实验验证上均显示其能显著提升训练稳定性和性能，具有较强的实用价值。

Abstract: Recently, there have been numerous attempts to enhance the sample efficiency of off-policy reinforcement learning (RL) agents when interacting with the environment, including architecture improvements and new algorithms. Despite these advances, they overlook the potential of directly constraining the initial representations of the input data, which can intuitively alleviate the distribution shift issue and stabilize training. In this paper, we introduce the Tanh function into the initial layer to fulfill such a constraint. We theoretically unpack the convergence property of the temporal difference learning with the Tanh function under linear function approximation. Motivated by theoretical insights, we present our Constrained Initial Representations framework, tagged CIR, which is made up of three components: (i) the Tanh activation along with normalization methods to stabilize representations; (ii) the skip connection module to provide a linear pathway from the shallow layer to the deep layer; (iii) the convex Q-learning that allows a more flexible value estimate and mitigates potential conservatism. Empirical results show that CIR exhibits strong performance on numerous continuous control tasks, even being competitive or surpassing existing strong baseline methods.

</details>


### [142] [SpaTeoGL: Spatiotemporal Graph Learning for Interpretable Seizure Onset Zone Analysis from Intracranial EEG](https://arxiv.org/abs/2602.11801)
*Elham Rostami,Aref Einizade,Taous-Meriem Laleg-Kirati*

Main category: cs.LG

TL;DR: SpaTeoGL是一种用于从颅内脑电(iEEG)数据中定位癫痫发作起始区(SOZ)的时空图学习框架。该框架通过平滑图信号处理框架联合学习电极间的窗口级空间图和时间窗口间的时序图，在竞争性地基线性能基础上，提高了非SOZ区域的识别能力，并为癫痫发作起始和传播动力学提供了可解释的见解。


<details>
  <summary>Details</summary>
Motivation: 准确从颅内脑电(iEEG)数据中定位癫痫发作起始区(SOZ)对癫痫手术至关重要，但面临复杂的时空癫痫动力学挑战。

Method: 提出SpaTeoGL时空图学习框架：(1)联合学习窗口级空间图以捕捉iEEG电极间相互作用，以及基于空间结构相似性连接时间窗口的时序图；(2)在平滑图信号处理框架内公式化；(3)通过具有收敛保证的交替块坐标下降算法求解。

Result: 在多中心iEEG数据集上的实验表明：SpaTeoGL与基于水平可视图与逻辑回归的基线方法性能相当，同时改善了非SOZ区域的识别，并提供了对癫痫发作起始与传播动力学的可解释见解。

Conclusion: SpaTeoGL是一种有效的SOZ定位方法，在保持竞争力的同时提升了非SOZ识别能力，并通过可解释的时空图结构揭示了癫痫发作的动力学特征。

Abstract: Accurate localization of the seizure onset zone (SOZ) from intracranial EEG (iEEG) is essential for epilepsy surgery but is challenged by complex spatiotemporal seizure dynamics. We propose SpaTeoGL, a spatiotemporal graph learning framework for interpretable seizure network analysis. SpaTeoGL jointly learns window-level spatial graphs capturing interactions among iEEG electrodes and a temporal graph linking time windows based on similarity of their spatial structure. The method is formulated within a smooth graph signal processing framework and solved via an alternating block coordinate descent algorithm with convergence guarantees. Experiments on a multicenter iEEG dataset with successful surgical outcomes show that SpaTeoGL is competitive with a baseline based on horizontal visibility graphs and logistic regression, while improving non-SOZ identification and providing interpretable insights into seizure onset and propagation dynamics.

</details>


### [143] [From Path Signatures to Sequential Modeling: Incremental Signature Contributions for Offline RL](https://arxiv.org/abs/2602.11805)
*Ziyi Zhao,Qingchuan Li,Yuxuan Xu*

Main category: cs.LG

TL;DR: 针对路径签名时序结构丢失问题，提出增量签名贡献(ISC)方法，将签名分解为时序有序的张量代数元素序列。基于此构建的ISCT变换器在离线强化学习基准任务中表现优异，为时序敏感控制提供了理论严谨的解决方案。


<details>
  <summary>Details</summary>
Motivation: 标准路径签名将轨迹表示为全局单一对象，破坏了时序结构，无法适用于需要逐步响应的决策控制问题。然而，在敏感且要求稳定性的控制系统中，对瞬时轨迹更新的响应能力至关重要。

Method: 提出增量签名贡献(ISC)方法，将截断路径签名分解为对应路径增量贡献的时序有序张量代数元素序列。基于该表示，构建ISCT变换器模型，通过标准Transformer架构实现离线强化学习，无需额外架构修改。

Result: 在HalfCheetah、Walker2d、Hopper和Maze2d等基准环境中，包括延迟奖励和数据集降级设置，ISCT模型取得了显著性能。实验验证了ISC作为时序敏感控制任务中路径处理方法的有效性。

Conclusion: ISC方法在保持路径签名代数结构和表达能力的同时，显式建模时序演化，实现了对瞬时更新的敏感性。该方法为需要时序响应的决策控制问题提供了理论依据充分且实践有效的解决方案。

Abstract: Path signatures embed trajectories into tensor algebra and constitute a universal, non-parametric representation of paths; however, in the standard form, they collapse temporal structure into a single global object, which limits their suitability for decision-making problems that require step-wise reactivity. We propose the Incremental Signature Contribution (ISC) method, which decomposes truncated path signatures into a temporally ordered sequence of elements in the tensor-algebra space, corresponding to incremental contributions induced by last path increments. This reconstruction preserves the algebraic structure and expressivity of signatures, while making their internal temporal evolution explicit, enabling processing signature-based representations via sequential modeling approaches. In contrast to full signatures, ISC is inherently sensitive to instantaneous trajectory updates, which is critical for sensitive and stability-requiring control dynamics. Building on this representation, we introduce ISC-Transformer (ISCT), an offline reinforcement learning model that integrates ISC into a standard Transformer architecture without further architectural modification. We evaluate ISCT on HalfCheetah, Walker2d, Hopper, and Maze2d, including settings with delayed rewards and downgraded datasets. The results demonstrate that ISC method provides a theoretically grounded and practically effective alternative to path processing for temporally sensitive control tasks.

</details>


### [144] [CAAL: Confidence-Aware Active Learning for Heteroscedastic Atmospheric Regression](https://arxiv.org/abs/2602.11825)
*Fei Jiang,Jiyang Xia,Junjie Yu,Mingfei Sun,Hugh Coe,David Topping,Dantong Liu,Zhenhui Jessie Li,Zhonghua Zheng*

Main category: cs.LG

TL;DR: 该论文提出了一种置信度感知的主动学习框架（CAAL），用于在异方差噪声环境下高效选择样本，以估算难以测量的大气颗粒物毒性及吸湿性等关键属性。通过解耦不确定性感知训练和置信度感知采样，CAAL能够区分可约认知不确定性与不可约偶然不确定性，从而在有限标注预算下避免浪费资源于噪声主导区域，显著优于传统主动学习方法。


<details>
  <summary>Details</summary>
Motivation: 准确量化空气污染对健康与气候的影响依赖于大气颗粒物的毒性及吸湿性等关键特性，但这些特性测量成本高、数据稀缺。常规观测数据虽易获取，但其与颗粒物特性间的映射存在输入依赖的异方差噪声。在有限的标注预算下，如何高效选择最具信息量的样本进行测量或模拟，同时避免在噪声主导区域浪费资源，是当前面临的核心挑战。

Method: 提出置信度感知主动学习框架（CAAL），包含两个核心组件：1）解耦的不确定性感知训练目标，通过分别优化预测均值与噪声水平来稳定不确定性估计；2）置信度感知的采样函数，利用预测的偶然不确定性作为可靠性信号，动态加权认知不确定性，从而实现异方差噪声环境下的高效样本选择。

Result: 在颗粒物解析数值模拟和真实大气观测数据集上的实验表明，CAAL在有限标注预算下能够持续优于标准主动学习基线方法，显著提升了样本选择效率，并有效避免了资源浪费在噪声主导区域。

Conclusion: 该框架为高成本大气颗粒物特性数据库的高效扩展提供了一种实用且通用的解决方案，在异方差噪声环境下具有较好的鲁棒性和适用性。

Abstract: Quantifying the impacts of air pollution on health and climate relies on key atmospheric particle properties such as toxicity and hygroscopicity. However, these properties typically require complex observational techniques or expensive particle-resolved numerical simulations, limiting the availability of labeled data. We therefore estimate these hard-to-measure particle properties from routinely available observations (e.g., air pollutant concentrations and meteorological conditions). Because routine observations only indirectly reflect particle composition and structure, the mapping from routine observations to particle properties is noisy and input-dependent, yielding a heteroscedastic regression setting. With a limited and costly labeling budget, the central challenge is to select which samples to measure or simulate. While active learning is a natural approach, most acquisition strategies rely on predictive uncertainty. Under heteroscedastic noise, this signal conflates reducible epistemic uncertainty with irreducible aleatoric uncertainty, causing limited budgets to be wasted in noise-dominated regions. To address this challenge, we propose a confidence-aware active learning framework (CAAL) for efficient and robust sample selection in heteroscedastic settings. CAAL consists of two components: a decoupled uncertainty-aware training objective that separately optimises the predictive mean and noise level to stabilise uncertainty estimation, and a confidence-aware acquisition function that dynamically weights epistemic uncertainty using predicted aleatoric uncertainty as a reliability signal. Experiments on particle-resolved numerical simulations and real atmospheric observations show that CAAL consistently outperforms standard AL baselines. The proposed framework provides a practical and general solution for the efficient expansion of high-cost atmospheric particle property databases.

</details>


### [145] [Towards Sustainable Investment Policies Informed by Opponent Shaping](https://arxiv.org/abs/2602.11829)
*Juan Agustin Duque,Razvan Ciuca,Ayoub Echchahed,Hugo Larochelle,Aaron Courville*

Main category: cs.LG

TL;DR: 该论文针对气候变化这一全球性挑战，提出了一个投资者与企业动态互动的多智能体模拟模型InvestESG，形式化地刻画了其跨期社会困境的条件与阈值，并应用优势对齐算法（Advantage Alignment）塑造智能体学习过程，证明该方法能系统性地促进合作性均衡，为协调市场激励与长期可持续性目标提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 气候变化需要全球协同应对，但理性经济主体往往追求即时利益而非集体福祉，导致社会困境。现有研究缺乏对气候风险下投资者-企业动态博弈中跨期社会困境的形式化分析，也缺少有效的机制设计来协调个体激励与集体长期目标。因此，亟需探索能够引导经济主体行为向可持续方向发展的理论与方法。

Method: 首先，对InvestESG模型进行形式化表征，推导出个体激励与集体福利分离的理论阈值，识别跨期社会困境的存在条件。其次，引入优势对齐算法（Advantage Alignment），这是一种适用于一般和博弈的可扩展对手shaping方法，通过有策略地引导智能体的学习动态来促进合作性结果。

Result: 研究结果表明：（1）InvestESG模型中存在明确的跨期社会困境阈值，超过该阈值个体最优策略与集体最优策略显著背离；（2）优势对齐算法能够系统性地使学习过程偏向于社会有益的均衡；（3）通过策略性地塑造经济主体的学习过程，可以获得优于传统市场机制的可持续性结果。

Conclusion: 本研究证明，通过对手shaping算法（如Advantage Alignment）有策略地引导经济主体的学习过程，能够有效协调个体利益与集体长期福祉，为设计促进可持续性的政策机制提供了新的理论视角和计算工具，有助于实现市场激励与长期气候目标的一致性。

Abstract: Addressing climate change requires global coordination, yet rational economic actors often prioritize immediate gains over collective welfare, resulting in social dilemmas. InvestESG is a recently proposed multi-agent simulation that captures the dynamic interplay between investors and companies under climate risk. We provide a formal characterization of the conditions under which InvestESG exhibits an intertemporal social dilemma, deriving theoretical thresholds at which individual incentives diverge from collective welfare. Building on this, we apply Advantage Alignment, a scalable opponent shaping algorithm shown to be effective in general-sum games, to influence agent learning in InvestESG. We offer theoretical insights into why Advantage Alignment systematically favors socially beneficial equilibria by biasing learning dynamics toward cooperative outcomes. Our results demonstrate that strategically shaping the learning processes of economic agents can result in better outcomes that could inform policy mechanisms to better align market incentives with long-term sustainability goals.

</details>


### [146] [A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production](https://arxiv.org/abs/2602.11861)
*Sümeyye Meryem Taşyürek,Enis Mücahid İskender,Hacer Yalim Keles*

Main category: cs.LG

TL;DR: 本文提出A$^{2}$V-SLP，一种对齐感知的变分框架用于手语生成。该框架学习发音器官解耦的潜分布而非确定性嵌入，通过解耦VAE提取发音器官特定的均值和方差向量，用非自回归Transformer预测文本对应的潜均值和对数方差，并结合随机采样和gloss注意力机制，在完全无gloss设置下实现SOTA性能和更真实的运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成方法多采用确定性潜嵌入，易导致潜空间坍缩并丢失发音器官层级表示。本文旨在通过分布潜建模保持发音器官层级的解耦表示，同时增强语言输入与发音运动的对齐，以提升生成手语的真实感和质量。

Method: 提出A$^{2}$V-SLP框架：1) 解耦变分自编码器编码真实手语姿态序列，提取发音器官特定的均值和方差向量；2) 非自回归Transformer根据文本嵌入预测潜均值和对数方差；3) VAE解码器通过随机采样重构手语姿态序列；4) 集成gloss注意力机制强化语言输入与运动对齐。通过分布潜建模避免确定性潜坍缩。

Result: 实验表明，相比确定性潜回归方法具有一致性提升，在完全无gloss设置下达到最先进回译性能，并显著改善了生成运动的真实感。

Conclusion: A$^{2}$V-SLP通过发音器官解耦的分布潜建模和对齐感知机制，有效提升了手语生成的质量和真实性，为手语生产任务提供了新的解决方案。

Abstract: Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.

</details>


### [147] [In-Context Function Learning in Large Language Models](https://arxiv.org/abs/2602.11863)
*Elif Akata,Konstantinos Voudouris,Vincent Fortuin,Eric Schulz*

Main category: cs.LG

TL;DR: 本文通过高斯过程理论框架研究大语言模型的上下文学习现象，发现LLM的学习曲线受核函数影响并随示例增加逼近GP下界，其归纳偏置偏好非平滑核函数，且可通过训练调整。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理时能从少量演示中学习，但这一现象的理论基础尚不明确。作者希望通过高斯过程这一贝叶斯非参数模型，在受控实验环境中量化分析LLM的函数学习能力，理解其学习机制和归纳偏置，并为优化模型提供理论指导。

Method: 构建受控实验，让模型观测来自已知高斯过程先验的多变量标量函数样本序列。评估预测误差与演示数量的关系，并与两个基准比较：(i)经验GP回归学习者（可达到误差下界）和(ii)1-最近邻规则（数据驱动的上界）。随后使用似然分析研究模型的归纳偏置。

Result: 1) LLM学习曲线强烈受函数生成核函数影响，随演示数量增加逼近GP下界；2) LLM预测最可能对应于非平滑GP核函数，形成特定的归纳偏置；3) 强化学习和监督微调能有效调整这些归纳偏置，使其向训练数据方向转变。

Conclusion: 该框架量化了大语言模型作为高斯过程学习者的行为特征，揭示了其在连续函数学习任务中的学习规律和偏置特性，并提供了调整模型归纳偏置的工具，为提升LLM样本效率和任务适应性奠定了理论基础。

Abstract: Large language models (LLMs) can learn from a few demonstrations provided at inference time. We study this in-context learning phenomenon through the lens of Gaussian Processes (GPs). We build controlled experiments where models observe sequences of multivariate scalar-valued function samples drawn from known GP priors. We evaluate prediction error in relation to the number of demonstrations and compare against two principled references: (i) an empirical GP-regression learner that gives a lower bound on achievable error, and (ii) the expected error of a 1-nearest-neighbor (1-NN) rule, which gives a data-driven upper bound. Across model sizes, we find that LLM learning curves are strongly influenced by the function-generating kernels and approach the GP lower bound as the number of demonstrations increases. We then study the inductive biases of these models using a likelihood-based analysis. We find that LLM predictions are most likely under less smooth GP kernels. Finally, we explore whether post-training can shift these inductive biases and improve sample-efficiency on functions sampled from GPs with smoother kernels. We find that both reinforcement learning and supervised fine-tuning can effectively shift inductive biases in the direction of the training data. Together, our framework quantifies the extent to which LLMs behave like GP learners and provides tools for steering their inductive biases for continuous function learning tasks.

</details>


### [148] [Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning](https://arxiv.org/abs/2602.11882)
*Suraj Ranganath,Anish Patnaik,Vaishak Menon*

Main category: cs.LG

TL;DR: 本研究通过DINO-WM模型在Wall规划任务上的实验，发现低比特规划行为呈现三种模式：8/6比特接近FP16性能，3比特性能崩溃，4比特对位分配敏感。在过渡区域，保持编码器精度可提升性能，这促使模块感知、预算感知的量化策略成为高效空间推理的研究方向。


<details>
  <summary>Details</summary>
Motivation: 空间推理需要高精度的世界模型，但在严格精度预算下需保持可靠性。本研究探究低比特规划行为主要由总位宽还是模块间位分配决定，为高效空间推理的量化策略提供理论依据。

Method: 在Wall规划任务上使用DINO-WM世界模型，采用配对目标混合比特评估方法，对比均匀量化、混合量化、非对称量化和逐层量化等变体在两种规划器预算下的表现。

Result: 实验发现三种一致模式：8/6比特设置性能接近FP16；3比特设置性能崩溃；4比特设置对位分配敏感。在过渡区域，保持编码器精度可改善规划性能。在更严格的22-cell测试中，INT4混合量化与均匀量化的差异受预算条件影响，凸显过渡区的敏感性。

Conclusion: 研究结果支持模块感知、预算感知的量化策略应成为高效空间推理的broader研究方向，为在精度约束下优化世界模型提供了重要启示。

Abstract: Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.

</details>


### [149] [Universal Diffusion-Based Probabilistic Downscaling](https://arxiv.org/abs/2602.11893)
*Roberto Molinaro,Niall Siegenheim,Henry Martin,Mark Frey,Niels Poulsen,Philipp Seitz,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的通用降尺度框架，无需特定模型微调即可将确定性低分辨率天气预报提升为概率性高分辨率预测。通过在粗分辨率（~25km）和高分辨率（~5km）数据上训练单一条件扩散模型，并以零样本方式应用于异构上游天气模型的确定性预报，在90小时预报时效内显著提升了空间分辨率和不确定性表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有天气预报系统（包括AI和数值天气预报模型）通常输出低分辨率确定性预测，而高分辨率概率性预测对实际应用至关重要。传统降尺度方法往往需要针对特定模型进行微调，缺乏通用性。本文旨在开发一种可扩展的、与模型无关的概率性接口，以统一方式提升各类预报系统的分辨率和不确定性量化能力。

Method: 研究团队开发了一个条件扩散模型，使用配对的低分辨率（约25公里）输入和高分辨率区域再分析数据（约5公里）进行训练。该模型在完全零样本（zero-shot）的方式下，应用于来自异构上游天气模型的确定性预报。专注于近地表变量，研究利用独立站点观测数据评估最长90小时预报时效内的概率性预测性能。

Result: 实验表明，降尺度预报的集合平均持续优于各模型原始的确定性预报，且在连续分级概率评分（CRPS）等概率性技能指标上表现出更显著的改进。该结果在多样化的AI和数值天气预报系统中均得到验证。

Conclusion: 基于扩散的降尺度方法为在业务天气预报流程中增强空间分辨率和不确定性表征提供了一个可扩展、模型无关的概率性接口，具有广泛的适用性和实用价值。

Abstract: We introduce a universal diffusion-based downscaling framework that lifts deterministic low-resolution weather forecasts into probabilistic high-resolution predictions without any model-specific fine-tuning. A single conditional diffusion model is trained on paired coarse-resolution inputs (~25 km resolution) and high-resolution regional reanalysis targets (~5 km resolution), and is applied in a fully zero-shot manner to deterministic forecasts from heterogeneous upstream weather models. Focusing on near-surface variables, we evaluate probabilistic forecasts against independent in situ station observations over lead times up to 90 h. Across a diverse set of AI-based and numerical weather prediction (NWP) systems, the ensemble mean of the downscaled forecasts consistently improves upon each model's own raw deterministic forecast, and substantially larger gains are observed in probabilistic skill as measured by CRPS. These results demonstrate that diffusion-based downscaling provides a scalable, model-agnostic probabilistic interface for enhancing spatial resolution and uncertainty representation in operational weather forecasting pipelines.

</details>


### [150] [Mitigating Mismatch within Reference-based Preference Optimization](https://arxiv.org/abs/2602.11902)
*Suqin Yuan,Xingrui Yu,Jiyang Zheng,Lei Feng,Dadong Wang,Ivor Tsang,Tongliang Liu*

Main category: cs.LG

TL;DR: 针对DPO在悲观偏好对上的过早满足缺陷，本文提出Hybrid-DPO（HyPO），通过条件性偏置参考信号（悲观时视为中性）在保持DPO稳定性的同时强化学习，实验证明其提升推理对齐指标和胜率。


<details>
  <summary>Details</summary>
Motivation: DPO依赖参考策略稳定训练，但对悲观对（参考偏好被拒绝回答）会过早衰减梯度导致训练-推理不匹配。无参考方法虽解决此问题，却丢弃了关键稳定信号，形成核心矛盾。

Method: HyPO是DPO的即插即用改进：当参考乐观/中性时保持DPO原式；当参考悲观时，将目标中的Δ_θ-Δ_ref替换为Δ_θ-max{0,Δ_ref}，实现参考信号的条件性去偏。

Result: 该一行修改在悲观对上严格增强学习信号，保持计算成本不变。实验显示HyPO缓解过早满足，提升推理对齐指标及配对胜率。

Conclusion: 直接偏好对齐可通过条件性偏置而非丢弃参考信号来增强，HyPO为平衡稳定性与去偏需求提供了简单有效方案。

Abstract: Direct Preference Optimization (DPO) has become the de facto standard for offline preference alignment of large language models, but its reliance on a reference policy introduces a critical tension. DPO weighs each update relative to a reference, which stabilizes the training by regularizing the updates within a trusted region. This reliance becomes problematic for pessimistic pairs, where the reference model prefers the rejected response. For these pairs, DPO prematurely attenuates the gradient as soon as the policy margin ($Δ_θ$) merely beats the reference margin ($Δ_{\mathrm{ref}}$) even if the policy is still wrong ($Δ_θ<0$). We name this failure premature satisfaction, which is a concrete form of the training-inference mismatch. Reference-free objectives remove this mismatch by optimizing the absolute margin, but at the cost of discarding the stabilizing signal of the reference. We mitigate this tension with Hybrid-DPO (HyPO), a drop-in modification to DPO that applies reference conditionally: HyPO behaves exactly like DPO when the reference is optimistic or neutral, and it treats the reference as neutral when it is pessimistic by replacing $Δ_θ-Δ_{\mathrm{ref}}$ with $Δ_θ-\max\{0,Δ_{\mathrm{ref}}\}$. This one-line change strictly strengthens per-example learning signals on pessimistic pairs while preserving DPO's objective form and computational cost. By conditionally debiasing the pessimistic reference signal, HyPO mitigates premature satisfaction; empirically, across preference alignment, HyPO improves inference-aligned metrics and achieves higher pairwise win rates. Our results provide evidence that direct preference alignment could be enhanced by conditionally debiasing the reference signal, rather than discarding it.

</details>


### [151] [Learning Conditional Averages](https://arxiv.org/abs/2602.11920)
*Marco Bressan,Nataly Brukhim,Nicolo Cesa-Bianchi,Emmanuel Esposito,Yishay Mansour,Shay Moran,Maximilian Thiessen*

Main category: cs.LG

TL;DR: 本文将PAC学习框架扩展到条件平均值学习问题，即预测每个实例在其任意邻域内的平均标签，而非直接学习目标概念本身。通过引入两个新型组合参数，给出了该问题可学习性的完整理论刻画及紧致样本复杂度界。


<details>
  <summary>Details</summary>
Motivation: 传统PAC学习要求精确学习目标概念，但可解释性、公平性、推荐系统等领域更关注实例邻域内的平均行为。这种广义设置能更自然地建模实际应用中仅需预测局部平均标签而非逐点精确预测的任务需求。

Method: 基于概念类与邻域系统的交互，定义了两个关联的组合参数，并证明其联合有限性等价于条件平均值的PAC可学习性。该方法通过分析邻域图的独立数等组合性质，建立了可学习性的判定准则。

Result: 获得了条件平均值可学习的充要条件，并推导出仅差对数因子即紧致的样本复杂度边界。结果表明，当且仅当两个新型参数有限时，学习器才能有效预测邻域平均标签。

Conclusion: 该工作成功将PAC学习推广至条件平均值这一更具表现力的框架，为多个应用领域提供了理论基础。其核心贡献在于揭示了邻域系统结构特性与可学习性之间的本质联系，通过组合参数的有限性给出了清晰的学习可行性判据。

Abstract: We introduce the problem of learning conditional averages in the PAC framework. The learner receives a sample labeled by an unknown target concept from a known concept class, as in standard PAC learning. However, instead of learning the target concept itself, the goal is to predict, for each instance, the average label over its neighborhood -- an arbitrary subset of points that contains the instance. In the degenerate case where all neighborhoods are singletons, the problem reduces exactly to classic PAC learning. More generally, it extends PAC learning to a setting that captures learning tasks arising in several domains, including explainability, fairness, and recommendation systems. Our main contribution is a complete characterization of when conditional averages are learnable, together with sample complexity bounds that are tight up to logarithmic factors. The characterization hinges on the joint finiteness of two novel combinatorial parameters, which depend on both the concept class and the neighborhood system, and are closely related to the independence number of the associated neighborhood graph.

</details>


### [152] [Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration](https://arxiv.org/abs/2602.11937)
*Akhiad Bercovich,Nir Ailon,Vladimir Anisimov,Tomer Asida,Nave Assaf,Mohammad Dabbah,Ido Galil,Amnon Geifman,Yonatan Geifman,Izhak Golan,Roi Koren,Itay Levy,Zach Moshe,Pavlo Molchanov,Najeeb Nabwani,Mostofa Patwari,Omri Puny,Tomer Ronen,Itamar Schen,Elad Segal,Ido Shahaf,Oren Tropp,Ran Zilberstein,Ran El-Yaniv*

Main category: cs.LG

TL;DR: 本文针对推理型大语言模型因生成长推理轨迹导致推理成本过高的问题，提出了一种后训练神经架构搜索（NAS）优化方案。通过对 gpt-oss-120B 进行异构 MoE 专家剪枝、注意力机制替换、FP8 KV-cache 量化及强化学习，在保持低生成长度的同时恢复精度。实验表明，优化后的 gpt-oss-puzzle-88B 在不同硬件上均实现显著吞吐加速，并以请求级效率指标（综合考虑生成长度和精度）证明其在整个精度-速度边界上优于基线模型，实现了推理成本的大幅降低而质量无损。


<details>
  <summary>Details</summary>
Motivation: 推理型大语言模型通过生成长推理轨迹提升答案质量，但额外的 token 会显著增加服务成本。因此，亟需推理优化以平衡质量与开销。

Method: 将 Puzzle 后训练 NAS 框架扩展并应用于 gpt-oss-120B，生成 gpt-oss-puzzle-88B。具体方法包括：异构混合专家（MoE）剪枝、选择性将全上下文注意力替换为窗口注意力、FP8 KV-cache 量化与校准、以及后训练强化学习以在保持低生成长度的同时恢复精度。

Result: 在 8×H100 节点上，长上下文和短上下文设置分别实现 1.63 倍和 1.22 倍每 token 吞吐加速；在单 H100 GPU 上实现 2.82 倍加速。该研究提倡请求级效率指标（归一化生成长度），gpt-oss-puzzle-88B 在整个精度-速度前沿上均优于基线，效率最高提升 1.29 倍。在多项基准测试中，其平均精度保留率介于 100.8% 至 108.2% 之间，表明质量无损。

Conclusion: 后训练架构搜索可大幅降低推理成本且不牺牲质量。请求级效率指标比单纯的每 token 吞吐量更能全面评估推理性能。优化后的 gpt-oss-puzzle-88B 在精度-速度权衡中表现出色。

Abstract: Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.

</details>


### [153] [Temporally Unified Adversarial Perturbations for Time Series Forecasting](https://arxiv.org/abs/2602.11940)
*Ruixian Su,Yukun Bao,Xinze Zhang*

Main category: cs.LG

TL;DR: 本文针对时间序列预测模型中的对抗样本攻击存在的时间不一致性问题，提出了时间统一对抗扰动（TUAPs）和基于时间戳梯度累积的方法（TGAM），通过强制相同时间戳在所有重叠样本上具有相同扰动，显著提升了对抗攻击的有效性和可转移性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测领域的对抗攻击方法忽略了数据的时间一致性，导致同一时间戳在不同重叠样本上的扰动值相互矛盾，这使得对抗攻击在实际数据操纵场景中不切实际。

Method: 1) 提出时间统一对抗扰动（TUAPs），通过时间统一约束确保每个时间戳在所有重叠样本上的扰动完全相同；2) 设计时间戳梯度累积方法（TGAM），通过聚合重叠样本的局部梯度信息来高效生成TUAPs；3) 将TGAM与基于动量的攻击算法结合，在保证时间一致性的同时充分利用序列级梯度信息探索对抗扰动空间。

Result: 在三个基准数据集和四个先进模型上的综合实验表明，所提方法在TUAP约束下的白盒和黑盒迁移攻击场景中显著优于基线方法；即使在无TUAP约束时，也表现出卓越的迁移攻击性能。

Conclusion: 该工作成功解决了时间序列对抗攻击中的时间不一致性问题，提出的TUAPs和TGAM方法有效提升了对抗扰动的实用性和攻击效果，为时间序列预测模型的安全性评估提供了新思路。

Abstract: While deep learning models have achieved remarkable success in time series forecasting, their vulnerability to adversarial examples remains a critical security concern. However, existing attack methods in the forecasting field typically ignore the temporal consistency inherent in time series data, leading to divergent and contradictory perturbation values for the same timestamp across overlapping samples. This temporally inconsistent perturbations problem renders adversarial attacks impractical for real-world data manipulation. To address this, we introduce Temporally Unified Adversarial Perturbations (TUAPs), which enforce a temporal unification constraint to ensure identical perturbations for each timestamp across all overlapping samples. Moreover, we propose a novel Timestamp-wise Gradient Accumulation Method (TGAM) that provides a modular and efficient approach to effectively generate TUAPs by aggregating local gradient information from overlapping samples. By integrating TGAM with momentum-based attack algorithms, we ensure strict temporal consistency while fully utilizing series-level gradient information to explore the adversarial perturbation space. Comprehensive experiments on three benchmark datasets and four representative state-of-the-art models demonstrate that our proposed method significantly outperforms baselines in both white-box and black-box transfer attack scenarios under TUAP constraints. Moreover, our method also exhibits superior transfer attack performance even without TUAP constraints, demonstrating its effectiveness and superiority in generating adversarial perturbations for time series forecasting models.

</details>


### [154] [Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning](https://arxiv.org/abs/2602.12123)
*Xubin Wang,Weijia Jia*

Main category: cs.LG

TL;DR: Meta-Sel是一种轻量级监督元学习方法，用于上下文学习中的示例选择。它利用TF-IDF余弦相似度和长度兼容性比两个低成本特征训练可解释的逻辑回归评分器，实现无需微调、无需额外大模型调用的快速示例选择，在多个基准测试中性能优异。


<details>
  <summary>Details</summary>
Motivation: 在上下文学习中，示例选择是实践瓶颈：在有限提示词预算下，模型准确率随所选示例剧烈波动，但选择算法必须足够轻量以支持每查询在大型候选池上实时运行。现有方法在计算效率、可解释性或性能方面存在局限。

Method: Meta-Sel通过以下方式实现示例选择：1) 构建元数据集：从训练集采样(候选示例,查询)对，以类别一致性作为监督标签；2) 训练评分器：使用TF-IDF余弦相似度和长度兼容性比两个廉价元特征，训练校准逻辑回归模型；3) 推理选择：单次向量化评分遍历整个候选池，返回top-k示例。整个过程无需模型微调、在线探索或额外LLM调用，提供确定性排序和可解释的特征权重。

Result: 在4个意图分类数据集和5个开源大模型上对12种方法进行广泛基准测试。Meta-Sel持续位列最优方法之一，对小模型效果尤为显著(选择质量可部分弥补模型容量限制)，同时保持竞争力的选择时间开销。

Conclusion: Meta-Sel是一种高效、可解释的示例选择方法，有效解决了ICL中的实践瓶颈。其轻量级设计、无需微调和额外大模型调用的特点使其适合实际部署，为小模型性能提升提供有效途径。

Abstract: Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.
  Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.
  Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods -- spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches -- across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.

</details>


### [155] [Using predictive multiplicity to measure individual performance within the AI Act](https://arxiv.org/abs/2602.11944)
*Karolin Frohnapfel,Mara Seyfert,Sebastian Bordt,Ulrike von Luxburg,Kristof Meding*

Main category: cs.LG

TL;DR: 本文研究AI决策中的预测多元性现象与欧盟AI法案个人性能报告要求的冲突，提出个体冲突率和δ-模糊度等量化工具及实施规则，建议向部署者提供多元性信息以评估输出可靠性。


<details>
  <summary>Details</summary>
Motivation: 预测多元性指多个模型整体准确率相似但个体预测存在差异的现象。当决策直接影响人类时，模型间的高分歧导致结果任意性，违背公平原则。欧盟AI法案要求高风险系统报告特定个人性能，但多元性使该要求难以满足，亟待解决合规性问题。

Method: 通过法律分析将预测多元性与法案准确性条款关联，提出个体冲突率和δ-模糊度量化模型间个体预测分歧，并结合计算洞察推导易于实施的评估规则。

Result: 开发了两类工具：个体冲突率（量化个体案例中的模型分歧）和δ-模糊度（识别受冲突预测影响的个体）；基于计算分析制定了实用的预测多元性评估实施规则。

Conclusion: 建议高风险AI系统提供商应向部署者提供预测多元性信息，使其能够判断特定个体输出的可靠性，从而确保决策透明性并满足法案合规要求。

Abstract: When building AI systems for decision support, one often encounters the phenomenon of predictive multiplicity: a single best model does not exist; instead, one can construct many models with similar overall accuracy that differ in their predictions for individual cases. Especially when decisions have a direct impact on humans, this can be highly unsatisfactory. For a person subject to high disagreement between models, one could as well have chosen a different model of similar overall accuracy that would have decided the person's case differently. We argue that this arbitrariness conflicts with the EU AI Act, which requires providers of high-risk AI systems to report performance not only at the dataset level but also for specific persons. The goal of this paper is to put predictive multiplicity in context with the EU AI Act's provisions on accuracy and to subsequently derive concrete suggestions on how to evaluate and report predictive multiplicity in practice. Specifically: (1) We argue that incorporating information about predictive multiplicity can serve compliance with the EU AI Act's accuracy provisions for providers. (2) Based on this legal analysis, we suggest individual conflict ratios and $δ$-ambiguity as tools to quantify the disagreement between models on individual cases and to help detect individuals subject to conflicting predictions. (3) Based on computational insights, we derive easy-to-implement rules on how model providers could evaluate predictive multiplicity in practice. (4) Ultimately, we suggest that information about predictive multiplicity should be made available to deployers under the AI Act, enabling them to judge whether system outputs for specific individuals are reliable enough for their use case.

</details>


### [156] [Towards Performance-Enhanced Model-Contrastive Federated Learning using Historical Information in Heterogeneous Scenarios](https://arxiv.org/abs/2602.11945)
*Hongliang Zhang,Jiguo Yu,Guijuan Wang,Wenshuo Ma,Tianqing He,Baobao Chai,Chunqiang Hu*

Main category: cs.LG

TL;DR: 该论文针对异构场景下的联邦学习性能问题，提出了PMFL框架，通过在节点端引入基于历史模型的对比学习项、在服务器端根据参与频率自适应调整聚合权重并融合历史全局模型，有效提升了异构数据分布和参与频率差异下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但在实际部署中常面临异构场景，各节点数据分布不同且参与频率不均，导致模型性能下降。现有方法未能有效解决这种异质性带来的挑战。

Method: PMFL框架从节点端和服务器端两方面入手：(1)节点端引入基于历史局部模型的对比学习项，捕获稳定的对比锚点，提升异构数据下模型更新的一致性；(2)服务器端利用节点累积参与次数自适应调整聚合权重，校正参与频率差异带来的全局目标偏差；(3)在全局模型更新中融合历史全局模型，减少相邻轮次间的性能波动。

Result: 大量实验表明，在异构场景下，PMFL相比现有联邦学习方法表现出更优越的性能。

Conclusion: PMFL通过利用历史训练信息，有效解决了异构数据分布和参与频率差异问题，为联邦学习在实际异构环境中的部署提供了可行的性能增强方案。

Abstract: Federated Learning (FL) enables multiple nodes to collaboratively train a model without sharing raw data. However, FL systems are usually deployed in heterogeneous scenarios, where nodes differ in both data distributions and participation frequencies, which undermines the FL performance. To tackle the above issue, this paper proposes PMFL, a performance-enhanced model-contrastive federated learning framework using historical training information. Specifically, on the node side, we design a novel model-contrastive term into the node optimization objective by incorporating historical local models to capture stable contrastive points, thereby improving the consistency of model updates in heterogeneous data distributions.
  On the server side, we utilize the cumulative participation count of each node to adaptively adjust its aggregation weight, thereby correcting the bias in the global objective caused by different node participation frequencies. Furthermore, the updated global model incorporates historical global models to reduce its fluctuations in performance between adjacent rounds. Extensive experiments demonstrate that PMFL achieves superior performance compared with existing FL methods in heterogeneous scenarios.

</details>


### [157] [Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation](https://arxiv.org/abs/2602.12125)
*Wenkai Yang,Weijie Liu,Ruobing Xie,Kai Yang,Saiyong Yang,Yankai Lin*

Main category: cs.LG

TL;DR: OPD是KL约束强化学习的特例。本文提出G-OPD框架，通过引入灵活参考模型和奖励缩放因子进行泛化。核心发现：(1)奖励外推(ExOPD)始终优于标准OPD，甚至可让学生超越教师性能边界；(2)强到弱蒸馏中，使用教师RL前基模型作为参考模型可进一步提升性能，但需额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有OPD方法虽实证效果显著，但其理论本质和优化空间尚不明确。本研究旨在从理论上揭示OPD与KL约束强化学习的内在联系，并探索如何通过调整奖励权重和参考模型来突破标准OPD的性能限制，特别是在不同师生模型规模和跨领域知识合并场景下的优化潜力。

Method: 首先从理论上证明OPD等价于奖励与KL正则项等权重的KL约束强化学习特例。继而提出G-OPD框架，解耦参考模型选择并引入奖励缩放因子以灵活调节两项权重。通过在数学推理和代码生成任务上的系统性实验，验证奖励外推和参考模型选择对蒸馏效果的影响。

Result: 实验得出两个关键结论：(1)奖励缩放因子>1的ExOPD在所有师生配对下均持续优于标准OPD，在合并多领域专家知识时，学生性能可突破教师边界；(2)强到弱蒸馏中，选用教师RL前的基模型作为参考模型可实现奖励校正，获得更精确的奖励信号，进一步提升性能，但需访问教师预训练模型并增加计算成本。

Conclusion: 本研究建立了OPD与KL约束RL的理论桥梁，提出的G-OPD框架为理解与改进策略蒸馏提供了新视角。ExOPD与奖励校正策略揭示了性能提升的新路径，同时也指出了计算效率与性能之间的权衡，为未来OPD研究提供了理论指导和实践洞见。

Abstract: On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.

</details>


### [158] [Manifold-Aware Temporal Domain Generalization for Large Language Models](https://arxiv.org/abs/2602.11965)
*Yiheng Yao,Zekun Cai,Xinyuan Song,Hiroki Hill Kobayashi,Xuan Song,Ryosuke Shibasaki,Liang Zhao*

Main category: cs.LG

TL;DR: 本文提出MaT-LoRA，一种针对大型语言模型时序分布漂移的流形感知时序LoRA方法。通过几何重构将时序域泛化问题转化为低维流形学习，在低秩适应子空间中约束时序更新并建模演化，实现计算效率与表达能力的平衡，在多个真实数据集上展现优异的时序泛化性能。


<details>
  <summary>Details</summary>
Motivation: 实际部署中大型语言模型面临普遍的时序分布漂移问题，数据持续演化。现有时序域泛化方法在完整参数空间操作，计算成本过高，不适用于现代大规模模型，亟需高效且可扩展的解决方案。

Method: 方法基于几何重构原理：参数高效重参数化可保持模型演化的低维时序结构。MaT-LoRA将时序更新约束于低秩适应子空间内的共享低维流形，并通过结构化时序核心建模演化过程，避免在高维参数空间中直接操作。

Result: 在科学文档、新闻、评论评分等合成与真实数据集上的实验表明，MaT-LoRA能以可扩展的方式显著提升大型语言模型的时序泛化性能，在保持计算效率的同时保留模型的表达能力。

Conclusion: 本研究通过低维流形重构实现了时序域泛化的参数高效建模，验证了MaT-LoRA在大型语言模型时序适应中的有效性，为动态环境下的模型部署提供了实用且高效的解决方案，平衡了计算复杂度与性能。

Abstract: Temporal distribution shifts are pervasive in real-world deployments of Large Language Models (LLMs), where data evolves continuously over time. While Temporal Domain Generalization (TDG) seeks to model such structured evolution, existing approaches characterize model adaptation in the full parameter space. This formulation becomes computationally infeasible for modern LLMs. This paper introduces a geometric reformulation of TDG under parameter-efficient fine-tuning. We establish that the low-dimensional temporal structure underlying model evolution can be preserved under parameter-efficient reparameterization, enabling temporal modeling without operating in the ambient parameter space. Building on this principle, we propose Manifold-aware Temporal LoRA (MaT-LoRA), which constrains temporal updates to a shared low-dimensional manifold within a low-rank adaptation subspace, and models its evolution through a structured temporal core. This reparameterization dramatically reduces temporal modeling complexity while retaining expressive power. Extensive experiments on synthetic and real-world datasets, including scientific documents, news publishers, and review ratings, demonstrate that MaT-LoRA achieves superior temporal generalization performance with practical scalability for LLMs.

</details>


### [159] [Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret](https://arxiv.org/abs/2602.11995)
*Yifei Jin,Xin Zheng,Lei Guo*

Main category: cs.LG

TL;DR: 本文针对非平稳数据流的在线学习挑战，研究动量最小均方（MLMS）算法，理论推导其在时变随机线性系统中的跟踪与遗憾界，实验证实其快速适应与鲁棒跟踪能力，适用于现代流式应用。


<details>
  <summary>Details</summary>
Motivation: 大规模数据流常呈现分布漂移和时变系统参数等非平稳性，违背经典i.i.d.假设，亟需支持单遍处理、复杂度与流长度无关且无需昂贵重训练的实时自适应算法。

Method: 以计算简单、可在线处理的MLMS为自适应辨识工具，针对时变随机线性系统，从理论上推导其跟踪性能与遗憾界，重点分析动量引入的二阶时变随机向量差分方程稳定性及复杂随机矩阵乘积问题。

Result: 理论获得MLMS在多种实际条件下的性能边界；实验表明其在合成与真实数据流上实现快速适应与鲁棒跟踪，且与理论结果在非平稳环境中高度一致。

Conclusion: 理论与实验一致表明MLMS在非平稳环境中具有快速适应与鲁棒跟踪优势，其计算简单性和在线处理能力使其在现代流式学习与在线学习应用中前景广阔。

Abstract: In large-scale data processing scenarios, data often arrive in sequential streams generated by complex systems that exhibit drifting distributions and time-varying system parameters. This nonstationarity challenges theoretical analysis, as it violates classical assumptions of i.i.d. (independent and identically distributed) samples, necessitating algorithms capable of real-time updates without expensive retraining. An effective approach should process each sample in a single pass, while maintaining computational and memory complexities independent of the data stream length. Motivated by these challenges, this paper investigates the Momentum Least Mean Squares (MLMS) algorithm as an adaptive identification tool, leveraging its computational simplicity and online processing capabilities. Theoretically, we derive tracking performance and regret bounds for the MLMS in time-varying stochastic linear systems under various practical conditions. Unlike classical LMS, whose stability can be characterized by first-order random vector difference equations, MLMS introduces an additional dynamical state due to momentum, leading to second-order time-varying random vector difference equations whose stability analysis hinges on more complicated products of random matrices, which poses a substantially challenging problem to resolve. Experiments on synthetic and real-world data streams demonstrate that MLMS achieves rapid adaptation and robust tracking, in agreement with our theoretical results especially in nonstationary settings, highlighting its promise for modern streaming and online learning applications.

</details>


### [160] [Olmix: A Framework for Data Mixing Throughout LM Development](https://arxiv.org/abs/2602.12237)
*Mayee F. Chen,Tyler Murray,David Heineman,Matt Jordan,Hannaneh Hajishirzi,Christopher Ré,Luca Soldaini,Kyle Lo*

Main category: cs.LG

TL;DR: Olmix框架解决了大模型训练中的数据混合问题，通过重用历史比例和增量更新机制，在五次领域集合变更中实现与全量重算相当性能，同时降低74%计算成本，并带来11.6%的下游任务提升。


<details>
  <summary>Details</summary>
Motivation: 现有数据混合方法在真实大模型开发中面临两大挑战：配置空间设计缺乏系统性研究，设计选择缺乏理论依据；且无法适应领域集合动态演化的实际场景，因为现有方法假设领域固定不变。

Method: 本文提出Olmix框架，首先对混合方法的配置空间进行全面实证研究，识别关键设计选择；其次提出混合重用机制，在领域集合更新时复用历史比例，仅针对受影响领域重新计算混合比例。

Result: 在模拟真实开发流程的五次连续领域更新实验中，混合重用机制以全量重算74%的计算成本达到相同性能，相比无混合训练在下游任务上平均提升11.6%。

Conclusion: 该研究为数据混合方法提供了系统性设计指导，并通过混合重用机制有效解决了领域动态变化下的计算效率问题，对降低大模型训练成本具有重要意义。

Abstract: Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.

</details>


### [161] [FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client](https://arxiv.org/abs/2602.12014)
*Gongxi Zhu,Hanlin Gu,Lixin Fan,Qiang Yang,Yuxing Han*

Main category: cs.LG

TL;DR: 本文提出FedGRPO，一种隐私保护的联邦基础模型框架。该框架通过构建置信图进行专家选择，并利用组相对策略优化(GRPO)方法，仅交换奖励信号而非数据或模型更新，从而在降低通信开销和隐私风险的同时，实现了在异构设备上的并行评估，并在多领域任务上展现出优越的准确性和通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦基础模型方法在模型或表示层面进行知识迁移时，存在本地训练成本高、通信开销大以及不可避免隐私风险等问题，亟需一种更高效、更隐私保护的解决方案。

Method: 该框架包含两个模块：(1) 基于能力的专家选择模块，通过辅助数据构建轻量级置信图，为每个问题识别最合适的客户端；(2) 采用GRPO的"组相对"概念，将问题与解题 rationale 打包为候选策略，分发给选定的专家客户端，仅聚合标量奖励信号，并通过联邦组相对损失函数进行优化。

Result: 在多个领域任务上的实证结果表明，FedGRPO相比传统联邦基础模型基线方法，能够实现更高的下游任务准确性和通信效率。

Conclusion: FedGRPO通过仅交换奖励值的创新方式，有效降低了隐私风险和通信开销，同时支持异构设备并行评估，为联邦基础模型提供了一种高效且隐私保护的新范式。

Abstract: One important direction of Federated Foundation Models (FedFMs) is leveraging data from small client models to enhance the performance of a large server-side foundation model. Existing methods based on model level or representation level knowledge transfer either require expensive local training or incur high communication costs and introduce unavoidable privacy risks. We reformulate this problem as a reinforcement learning style evaluation process and propose FedGRPO, a privacy preserving framework comprising two modules. The first module performs competence-based expert selection by building a lightweight confidence graph from auxiliary data to identify the most suitable clients for each question. The second module leverages the "Group Relative" concept from the Group Relative Policy Optimization (GRPO) framework by packaging each question together with its solution rationale into candidate policies, dispatching these policies to a selected subset of expert clients, and aggregating solely the resulting scalar reward signals via a federated group-relative loss function. By exchanging reward values instead of data or model updates, FedGRPO reduces privacy risk and communication overhead while enabling parallel evaluation across heterogeneous devices. Empirical results on diverse domain tasks demonstrate that FedGRPO achieves superior downstream accuracy and communication efficiency compared to conventional FedFMs baselines.

</details>


### [162] [Fourier Transformers for Latent Crystallographic Diffusion and Generative Modeling](https://arxiv.org/abs/2602.12045)
*Jed A. Duersch,Elohan Veillon,Astrid Klipfel,Adlane Sayede,Zied Bouraoui*

Main category: cs.LG

TL;DR: 本研究开发了基于倒易空间的晶体生成pipeline，采用截断傅里叶变换表征物种分辨的晶胞密度，而非直接建模原子坐标。该方法天然满足周期性、支持空间群对称性代数运算、支持生成过程中可变原子数，克服了粒子基方法的局限。仅用9个傅里叶基函数/维度即可重建多达108原子/物种的晶胞，结合Transformer VAE和潜空间扩散模型，在小晶胞生成任务上超越坐标基线方法。


<details>
  <summary>Details</summary>
Motivation: 新晶体材料的发现亟需能够处理周期性边界条件、晶体对称性和物理约束的生成模型，且需可扩展至结构多样的大型晶胞。现有基于原子坐标的粒子基方法难以同时满足这些要求，特别是在处理可变原子数和大规模系统时存在固有局限。

Method: 提出倒易空间生成框架：将晶体表示为物种分辨密度的截断傅里叶变换，利用其周期性原生特性、对称性代数操作便利性和可变原子数支持优势。技术实现上：采用每空间维度9个傅里叶基函数；构建复值傅里叶系数的Transformer变分自编码器进行压缩表征；在潜空间训练扩散模型进行生成。

Result: 实验表明：该表示可精确重建含多达108个原子/化学物种的晶胞；在LeMaterial基准上实现了有效的重建和潜空间扩散生成；在≤16原子/晶胞的小晶胞无条件生成任务中，性能优于基于坐标的基线方法。

Conclusion: 倒易空间的傅里叶表示为晶体生成建模提供了更自然、更有效的解决方案，成功解决了周期性、对称性和可变原子数等核心挑战，为设计大规模、结构多样化的新晶体材料开辟了新方向。

Abstract: The discovery of new crystalline materials calls for generative models that handle periodic boundary conditions, crystallographic symmetries, and physical constraints, while scaling to large and structurally diverse unit cells. We propose a reciprocal-space generative pipeline that represents crystals through a truncated Fourier transform of the species-resolved unit-cell density, rather than modeling atomic coordinates directly. This representation is periodicity-native, admits simple algebraic actions of space-group symmetries, and naturally supports variable atomic multiplicities during generation, addressing a common limitation of particle-based approaches. Using only nine Fourier basis functions per spatial dimension, our approach reconstructs unit cells containing up to 108 atoms per chemical species. We instantiate this pipeline with a transformer variational autoencoder over complex-valued Fourier coefficients, and a latent diffusion model that generates in the compressed latent space. We evaluate reconstruction and latent diffusion on the LeMaterial benchmark and compare unconditional generation against coordinate-based baselines in the small-cell regime ($\leq 16$ atoms per unit cell).

</details>


### [163] [Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards](https://arxiv.org/abs/2602.12049)
*Ryo Mikasa,Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Takahiro Katagiri*

Main category: cs.LG

TL;DR: 本研究针对HPC领域中LLM生成代码性能不稳定的问题，提出基于运行时性能反馈的在线强化学习方法，结合分阶段质量多样性算法，在双精度矩阵乘法任务上显著提升了Qwen2.5 Coder的代码优化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽具备强大代码生成能力，但生成代码的运行时性能缺乏保证，尤其在HPC领域，极少有研究将运行时性能作为训练奖励，亟需探索性能导向的代码生成方法。

Method: 构建了GPU训练集群与CPU基准测试集群互联的分布式系统；在双精度矩阵乘法任务上，采用组相对策略优化（GRPO）训练Qwen2.5 Coder 14B模型；核心创新包括：1）在线强化学习框架，直接在超算上执行生成代码并将GFLOPS作为奖励反馈；2）分阶段质量多样性（SQD）算法，按问题逐步调整允许的优化技术，实现多视角学习。

Result: 两项实验结果表明，融合运行时性能反馈与分阶段优化策略的强化学习能够显著提升LLM在高性能计算代码生成任务中的性能表现。

Conclusion: 本研究验证了以运行时性能为奖励信号、结合渐进式优化技术训练的强化学习方法对提升HPC代码生成能力的有效性，为开发高性能自动化代码优化系统提供了重要参考。

Abstract: Large language models (LLMs) have demonstrated strong code generation capabilities, yet the runtime performance of generated code is not guaranteed, and there have been few attempts to train LLMs using runtime performance as a reward in the HPC domain. We propose an online reinforcement learning approach that executes LLM-generated code on a supercomputer and directly feeds back the measured runtime performance (GFLOPS) as a reward. We further introduce a Staged Quality-Diversity (SQD) algorithm that progressively varies the permitted optimization techniques on a per-problem basis, enabling the model to learn code optimization from diverse perspectives. We build a distributed system connecting a GPU training cluster with a CPU benchmarking cluster, and train Qwen2.5 Coder 14B on a double-precision matrix multiplication task using Group Relative Policy Optimization (GRPO). Through two experiments, we show that reinforcement learning combining runtime performance feedback with staged optimization can improve the HPC code generation capability of LLMs.

</details>


### [164] [Empirical Gaussian Processes](https://arxiv.org/abs/2602.12082)
*Jihao Andreas Lin,Sebastian Ament,Louis C. Tiao,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 本文提出经验高斯过程框架，从历史数据中学习核函数，克服人工设计局限，理论保证收敛性，EM算法实现高效学习，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 高斯过程的实际应用常受限于核函数的人工选择，这一过程依赖专业知识，导致模型对数据的适应性有限且对假设空间施加了过强的约束。

Method: 提出经验高斯过程框架，从历史观测语料库中经验性地估计均值和协方差函数；将独立数据集上的GP先验学习问题建模为似然估计，并推导出具有闭式更新的期望最大化算法，以处理跨数据集的异质观测位置。

Result: 理论上，证明该模型收敛于KL散度意义下最接近真实数据生成过程的高斯过程；实践中，在曲线外推和时间序列预测基准测试中达到具有竞争力的性能。

Conclusion: 经验高斯过程通过数据驱动的方式构建灵活的先验，有效克服了传统核函数设计的局限性，为高斯过程提供了更具适应性和理论保证的新框架。

Abstract: Gaussian processes (GPs) are powerful and widely used probabilistic regression models, but their effectiveness in practice is often limited by the choice of kernel function. This kernel function is typically handcrafted from a small set of standard functions, a process that requires expert knowledge, results in limited adaptivity to data, and imposes strong assumptions on the hypothesis space. We study Empirical GPs, a principled framework for constructing flexible, data-driven GP priors that overcome these limitations. Rather than relying on standard parametric kernels, we estimate the mean and covariance functions empirically from a corpus of historical observations, enabling the prior to reflect rich, non-trivial covariance structures present in the data. Theoretically, we show that the resulting model converges to the GP that is closest (in KL-divergence sense) to the real data generating process. Practically, we formulate the problem of learning the GP prior from independent datasets as likelihood estimation and derive an Expectation-Maximization algorithm with closed-form updates, allowing the model handle heterogeneous observation locations across datasets. We demonstrate that Empirical GPs achieve competitive performance on learning curve extrapolation and time series forecasting benchmarks.

</details>


### [165] [Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL](https://arxiv.org/abs/2602.12087)
*Alfredo Reichlin,Adriano Pacciarelli,Danica Kragic,Miguel Vasco*

Main category: cs.LG

TL;DR: 本文提出了一种在强化学习中从高维、多模态和噪声观测中估计环境状态的新方法。该方法学习一种结构化潜在表示，其中状态之间的距离直接关联到转移所需的最少动作数，通过度量空间公式提供几何不确定性解释，无需显式概率建模。结合多模态潜在转移模型和基于逆距离加权的传感器融合机制，能自适应整合多传感器信息而无需预先知道噪声分布。在多模态强化学习任务上的实验表明，该方法对传感器噪声具有更强的鲁棒性，状态估计优于基线方法，并能提升智能体性能，避免了显式噪声增强的需求。


<details>
  <summary>Details</summary>
Motivation: 强化学习中从高维、多模态且含噪声的观测中准确估计环境状态是一个基本挑战。传统方法依赖概率模型并需要显式噪声假设，这限制了模型的泛化能力。因此，本文旨在开发一种无需显式概率建模即可处理不确定性的新方法。

Method: 1) 学习结构化潜在表示，使状态间距离与所需最少动作数直接相关；2) 提出度量空间公式，提供几何不确定性解释；3) 引入多模态潜在转移模型；4) 采用基于逆距离加权的传感器融合机制，实现无需先验噪声分布知识的自适应多传感器整合。

Result: 在多模态强化学习任务上的实证验证表明：1) 相比基线方法具有更优的传感器噪声鲁棒性；2) 状态估计性能显著提升；3) 通过学习的表示增强了智能体性能；4) 消除了对显式噪声增强的需求。

Conclusion: 利用转移感知的度量空间为序贯决策中的鲁棒状态估计提供了一种原则性和可扩展的解决方案，为处理高维、多模态、噪声观测提供了新思路。

Abstract: Estimating the state of an environment from high-dimensional, multimodal, and noisy observations is a fundamental challenge in reinforcement learning (RL). Traditional approaches rely on probabilistic models to account for the uncertainty, but often require explicit noise assumptions, in turn limiting generalization. In this work, we contribute a novel method to learn a structured latent representation, in which distances between states directly correlate with the minimum number of actions required to transition between them. The proposed metric space formulation provides a geometric interpretation of uncertainty without the need for explicit probabilistic modeling. To achieve this, we introduce a multimodal latent transition model and a sensor fusion mechanism based on inverse distance weighting, allowing for the adaptive integration of multiple sensor modalities without prior knowledge of noise distributions. We empirically validate the approach on a range of multimodal RL tasks, demonstrating improved robustness to sensor noise and superior state estimation compared to baseline methods. Our experiments show enhanced performance of an RL agent via the learned representation, eliminating the need of explicit noise augmentation. The presented results suggest that leveraging transition-aware metric spaces provides a principled and scalable solution for robust state estimation in sequential decision-making.

</details>


### [166] [On the Complexity of Offline Reinforcement Learning with $Q^\star$-Approximation and Partial Coverage](https://arxiv.org/abs/2602.12107)
*Haolin Liu,Braham Snyder,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 本文研究带Q*近似和偏覆盖的离线强化学习问题。通过信息论下界证明Q*可实现性与贝尔曼完备性不足以保证样本高效性；提出基于决策-估计系数的一般化框架，改进现有理论边界；首次获得软Q学习的ε⁻²样本复杂度，移除对未知价值间隙的在线交互需求，刻画低贝尔曼秩MDP的离线可学习性，并在非表格情况下分析保守Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于一个重要的开放性问题：在偏覆盖条件下，Q*可实现性与贝尔曼完备性是否足以实现样本高效的离线强化学习？该问题对理解实际算法（如保守Q学习CQL）的理论基础至关重要，但此前缺乏系统的理论分析。偏覆盖是离线RL的实用设定，而Q*近似和贝尔曼完备性是两个常见假设，其理论充分性尚未明确。

Method: 方法上，作者引入决策-估计系数（DEC）的一般化框架来刻画Q*函数类的内在复杂度。该框架借鉴自在线RL中的模型无关方法，通过决策-估计分解将复杂度度量与Q*估计过程模块化结合。关键技术贡献包括：提出二阶性能差异引理，用于推导更紧的样本复杂度边界；建立信息论下界证明以回答负面问题；设计可扩展至广义函数类的分析工具。

Result: 1) 理论层面：证明Q*可实现性与贝尔曼完备性在偏覆盖下不足以保证样本效率（信息论下界）；2) 框架层面：提出DEC一般框架，统一并改进Chen & Jiang (2022)与Uehara等(2023)的结果，支持更广泛设定；3) 复杂度层面：首次获得软Q学习的ε⁻²样本复杂度（优于Uehara等的ε⁻⁴）；4) 算法层面：移除Chen & Jiang对未知价值间隙需在线交互的要求；5) 扩展性层面：首次刻画无贝尔曼完备性时低贝尔曼秩MDP的离线可学习性；6) 应用层面：首次在非表格设定下分析CQL在Q*可实现与贝尔曼完备条件下的性能。

Conclusion: 本工作建立了离线强化学习在偏覆盖设定下的系统性理论框架，回答了关键开放性问题并给出负面结论。通过创新的决策-估计系数方法，显著提升了样本复杂度边界并扩展了理论适用范围，为理解CQL等实际算法提供了严格的理论基础。研究揭示了Q*函数类复杂度的核心作用，为未来离线RL的算法设计与分析提供了新的模块化工具。

Abstract: We study offline reinforcement learning under $Q^\star$-approximation and partial coverage, a setting that motivates practical algorithms such as Conservative $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention. Our work is inspired by the following open question: "Are $Q^\star$-realizability and Bellman completeness sufficient for sample-efficient offline RL under partial coverage?"
  We answer in the negative by establishing an information-theoretic lower bound. Going substantially beyond this, we introduce a general framework that characterizes the intrinsic complexity of a given $Q^\star$ function class, inspired by model-free decision-estimation coefficients (DEC) for online RL (Foster et al., 2023b; Liu et al., 2025b). This complexity recovers and improves the quantities underlying the guarantees of Chen and Jiang (2022) and Uehara et al. (2023), and extends to broader settings. Our decision-estimation decomposition can be combined with a wide range of $Q^\star$ estimation procedures, modularizing and generalizing existing approaches.
  Beyond the general framework, we make further contributions: By developing a novel second-order performance difference lemma, we obtain the first $ε^{-2}$ sample complexity under partial coverage for soft $Q$-learning, improving the $ε^{-4}$ bound of Uehara et al. (2023). We remove Chen and Jiang's (2022) need for additional online interaction when the value gap of $Q^\star$ is unknown. We also give the first characterization of offline learnability for general low-Bellman-rank MDPs without Bellman completeness (Jiang et al., 2017; Du et al., 2021; Jin et al., 2021), a canonical setting in online RL that remains unexplored in offline RL except for special cases. Finally, we provide the first analysis for CQL under $Q^\star$-realizability and Bellman completeness beyond the tabular case.

</details>


### [167] [Oscillators Are All You Need: Irregular Time Series Modelling via Damped Harmonic Oscillators with Closed-Form Solutions](https://arxiv.org/abs/2602.12139)
*Yashas Shende,Aritra Das,Reva Laxmi Chauhan,Arghya Pathak,Debayan Gupta*

Main category: cs.LG

TL;DR: 本文提出一种基于线性阻尼谐振子的新型时序Transformer架构，通过闭式解替代神经ODE求解器，在保持表达能力和通用逼近性质的同时，显著提升了不规则时序建模的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer在处理不规则时间序列时存在局限，而基于神经ODE的方法（如ContiFormers）虽然能处理不规则性，但受限于数值求解器的高计算开销。尽管闭式解能消除该开销，但传统ODE系统通常难以获得闭式解。因此，需要一种既能处理不规则时序又具备高效计算的新型架构。

Method: 作者用具有已知闭式解的线性阻尼谐振子系统替代神经ODE，将键和值建模为阻尼驱动振荡器，将查询展开为正弦基函数。通过这种类比，注意力机制被自然地建模为共振现象，从而在保持连续时间注意力通用逼近能力的前提下，避免了数值求解器的计算瓶颈。

Result: 理论证明该振荡器参数化保持了通用逼近性质，任何ContiFormer可实现的离散注意力矩阵都能被任意逼近。实验表明，该方法在不规则时间序列基准测试中达到最优性能，同时计算速度提升数个数量级。

Conclusion: 该工作通过物理启发的振荡器设计，成功解决了连续时间Transformer的计算效率问题，为不规则时序建模提供了兼具理论保证和实用性的新方向，显著推动了该领域的发展。

Abstract: Transformers excel at time series modelling through attention mechanisms that capture long-term temporal patterns. However, they assume uniform time intervals and therefore struggle with irregular time series. Neural Ordinary Differential Equations (NODEs) effectively handle irregular time series by modelling hidden states as continuously evolving trajectories. ContiFormers arxiv:2402.10635 combine NODEs with Transformers, but inherit the computational bottleneck of the former by using heavy numerical solvers. This bottleneck can be removed by using a closed-form solution for the given dynamical system - but this is known to be intractable in general! We obviate this by replacing NODEs with a novel linear damped harmonic oscillator analogy - which has a known closed-form solution. We model keys and values as damped, driven oscillators and expand the query in a sinusoidal basis up to a suitable number of modes. This analogy naturally captures the query-key coupling that is fundamental to any transformer architecture by modelling attention as a resonance phenomenon. Our closed-form solution eliminates the computational overhead of numerical ODE solvers while preserving expressivity. We prove that this oscillator-based parameterisation maintains the universal approximation property of continuous-time attention; specifically, any discrete attention matrix realisable by ContiFormer's continuous keys can be approximated arbitrarily well by our fixed oscillator modes. Our approach delivers both theoretical guarantees and scalability, achieving state-of-the-art performance on irregular time series benchmarks while being orders of magnitude faster.

</details>


### [168] [It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks](https://arxiv.org/abs/2602.12147)
*Zhongzheng Qiao,Sheng Pan,Anni Wang,Viktoriya Zhukova,Yong Liu,Xudong Jiang,Qingsong Wen,Mingsheng Long,Ming Jin,Chenghao Liu*

Main category: cs.LG

TL;DR: 针对现有时间序列基础模型(TSFM)基准测试存在的四大局限：数据构成单一重复、数据质量缺乏保障、任务设定脱离实际、分析视角僵化，本文提出TIME下一代任务为中心基准。该基准包含50个新数据集和98个预测任务，采用严格零样本评估避免数据泄露，通过大模型与人类专家协同的人机回环流程确保数据质量，并创新性提出基于时序结构特征的"模式级"评估视角，超越传统静态元标签方法。对12个代表性TSFM的评估生成了多粒度排行榜，为模型能力提供可泛化的深度洞察。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型基准测试存在四个核心局限：其一，数据构成受限，过度依赖重复使用的传统数据集；其二，数据完整性受损，缺乏严格的质量保证机制；其三，任务 formulation 与真实世界场景脱节，不符合实际业务需求；其四，分析视角僵化，无法揭示模型在不同时序模式下的可泛化能力。这些局限阻碍了对TSFM真实性能的客观评估，亟需构建更严谨、实用且具洞察力的下一代评估体系。

Method: 提出TIME基准，采用三阶段方法：1) 数据与任务构建：采集50个全新数据集，设计98个预测任务，建立严格零样本划分消除数据泄露；2) 人机回环质量保证：融合大语言模型自动化处理与人类专家领域知识，建立严格的质量控制流程，确保数据完整性和任务与现实世界操作需求对齐，考虑变量可预测性差异；3) 模式级评估框架：利用结构时序特征刻画内在时序属性（如趋势、季节性、周期性），构建超越静态元标签的动态模式分析视角，实现跨模式的模型能力泛化分析。

Result: 完成对12个代表性时间序列基础模型的全面评估，生成包含多粒度维度的综合排行榜。该排行榜支持细粒度、可视化的模型性能对比，揭示不同模型在各类时序模式（如趋势主导、周期主导、噪声敏感等）上的优势与局限，为研究者与实践者提供深度洞察和模型选择依据。评估结果已发布于公开平台供社区使用。

Conclusion: TIME基准通过引入全新数据生态、人机协同质量保障和模式级评估视角，系统性解决了现有TSFM评估体系的缺陷。该工作不仅提供了更可靠、无偏的零样本评估标准，更重要的是建立了可泛化的分析框架，能够揭示模型本质能力，推动时间序列基础模型从学术研究向实际应用转化，为领域发展奠定坚实基础。

Abstract: Time series foundation models (TSFMs) are revolutionizing the forecasting landscape from specific dataset modeling to generalizable task evaluation. However, we contend that existing benchmarks exhibit common limitations in four dimensions: constrained data composition dominated by reused legacy sources, compromised data integrity lacking rigorous quality assurance, misaligned task formulations detached from real-world contexts, and rigid analysis perspectives that obscure generalizable insights. To bridge these gaps, we introduce TIME, a next-generation task-centric benchmark comprising 50 fresh datasets and 98 forecasting tasks, tailored for strict zero-shot TSFM evaluation free from data leakage. Integrating large language models and human expertise, we establish a rigorous human-in-the-loop benchmark construction pipeline to ensure high data integrity and redefine task formulation by aligning forecasting configurations with real-world operational requirements and variate predictability. Furthermore, we propose a novel pattern-level evaluation perspective that moves beyond traditional dataset-level evaluations based on static meta labels. By leveraging structural time series features to characterize intrinsic temporal properties, this approach offers generalizable insights into model capabilities across diverse patterns. We evaluate 12 representative TSFMs and establish a multi-granular leaderboard to facilitate in-depth analysis and visualized inspection. The leaderboard is available at https://huggingface.co/spaces/Real-TSF/TIME-leaderboard.

</details>


### [169] [Amortized Molecular Optimization via Group Relative Policy Optimization](https://arxiv.org/abs/2602.12162)
*Muhammad bin Javaid,Hasham Hussain,Ashima Khanna,Berke Kisin,Jonathan Pirnay,Alexander Mitsos,Dominik G. Grimm,Martin Grohe*

Main category: cs.LG

TL;DR: GRXForm是一个分子结构优化模型，通过预训练的图Transformer和组相对策略优化（GRPO）来解决传统实例优化器计算成本高、泛化能力差的问题，无需推理时调用外部工具即可实现分布外分子支架的有效优化。


<details>
  <summary>Details</summary>
Motivation: 现有分子结构优化方法主要是"实例优化器"，对每个输入结构都需要重新搜索，计算成本高昂。虽然基于模型的方法理论上可以通过学习可迁移策略实现摊销效率，但现有方法泛化能力不足，主要原因是不同起始结构的难度异质性导致的高方差问题。

Method: 提出GRXForm，采用预训练的图Transformer模型，通过顺序添加原子和键来优化分子。引入组相对策略优化（GRPO）进行目标导向微调，通过相对于起始结构归一化奖励来缓解方差问题。

Result: 该模型能够泛化到分布外的分子支架，无需推理时调用外部工具或进行后处理，在多目标优化任务上达到与领先实例优化器相当的评分性能。

Conclusion: GRXForm成功解决了分子优化中的泛化难题，通过GRPO有效降低了异质起始结构带来的方差，实现了高效的摊销式优化，为分子设计提供了可扩展的解决方案。

Abstract: Molecular design encompasses tasks ranging from de-novo design to structural alteration of given molecules or fragments. For the latter, state-of-the-art methods predominantly function as "Instance Optimizers'', expending significant compute restarting the search for every input structure. While model-based approaches theoretically offer amortized efficiency by learning a policy transferable to unseen structures, existing methods struggle to generalize. We identify a key failure mode: the high variance arising from the heterogeneous difficulty of distinct starting structures. To address this, we introduce GRXForm, adapting a pre-trained Graph Transformer model that optimizes molecules via sequential atom-and-bond additions. We employ Group Relative Policy Optimization (GRPO) for goal-directed fine-tuning to mitigate variance by normalizing rewards relative to the starting structure. Empirically, GRXForm generalizes to out-of-distribution molecular scaffolds without inference-time oracle calls or refinement, achieving scores in multi-objective optimization competitive with leading instance optimizers.

</details>


### [170] [How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics](https://arxiv.org/abs/2602.12180)
*Yurong Chen,Yu He,Michael I. Jordan,Fan Yao*

Main category: cs.LG

TL;DR: 本文通过身份偏好优化框架理论分析了采样策略和参考策略对大语言模型偏好对齐的影响，发现合适的实例相关采样能提升排序保证，而有偏的在策略采样会导致过度集中；揭示了迭代对齐动态可能产生持续振荡或熵崩溃，并刻画了稳定性条件，这些现象在直接偏好优化中同样存在。


<details>
  <summary>Details</summary>
Motivation: 尽管从成对比较中学习并正则化到参考策略的偏好对齐方法在实践中有效，但采样和参考选择的理论效应仍未被充分理解。本文旨在通过理论分析揭示这些因素对模型性能的影响机制。

Method: 采用身份偏好优化作为分析框架，通过理论推导研究不同采样策略的效果，分析迭代对齐动态中学习策略对后续采样和参考策略的反馈影响，并使用真实世界偏好数据进行实验验证。

Result: 证明实例相关采样可获得更强的排序保证；揭示有偏在策略采样在结构化偏好下会导致过度集中；发现迭代动态可能出现持续振荡或熵崩溃；刻画了保证稳定性的参数范围；这些发现可扩展至直接偏好优化等更广泛的对齐方法。

Conclusion: 本研究揭示了采样策略在偏好对齐中的关键理论作用，表明不当的采样会导致模型行为异常，而稳定性条件的确定为设计更可靠的偏好学习算法提供了理论指导，对改进大语言模型对齐实践具有重要意义。

Abstract: Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.

</details>


### [171] [WaveFormer: Wavelet Embedding Transformer for Biomedical Signals](https://arxiv.org/abs/2602.12189)
*Habib Irani,Bikram De,Vangelis Metsis*

Main category: cs.LG

TL;DR: WaveFormer是一种新型Transformer架构，通过在嵌入构建和位置编码两个阶段集成小波变换，有效处理生物医学信号的长时间序列、复杂时序动态和多尺度频率模式，在八个数据集上实现了具有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学信号分类面临独特挑战，包括长序列、复杂的时序动态以及多尺度频率模式，这些特征难以被标准Transformer架构有效捕捉。

Method: 提出WaveFormer架构，在两个关键环节集成小波分解：1）嵌入构建阶段，使用多通道离散小波变换（DWT）提取频率特征，创建同时包含时域和频域信息的标记；2）位置编码阶段，采用动态小波位置编码（DyWPE），通过单通道DWT分析使位置嵌入适应信号特定的时序结构。

Result: 在八个涵盖人体活动识别和脑信号分析的多样化数据集上评估了该方法，序列长度从50到3000个时间步，通道数从1到144。实验结果表明，WaveFormer通过全面的频率感知处理实现了具有竞争力的性能。

Conclusion: 该方法为将频域知识融入基于Transformer的时间序列分类提供了一个原则性框架。

Abstract: Biomedical signal classification presents unique challenges due to long sequences, complex temporal dynamics, and multi-scale frequency patterns that are poorly captured by standard transformer architectures. We propose WaveFormer, a transformer architecture that integrates wavelet decomposition at two critical stages: embedding construction, where multi-channel Discrete Wavelet Transform (DWT) extracts frequency features to create tokens containing both time-domain and frequency-domain information, and positional encoding, where Dynamic Wavelet Positional Encoding (DyWPE) adapts position embeddings to signal-specific temporal structure through mono-channel DWT analysis. We evaluate WaveFormer on eight diverse datasets spanning human activity recognition and brain signal analysis, with sequence lengths ranging from 50 to 3000 timesteps and channel counts from 1 to 144. Experimental results demonstrate that WaveFormer achieves competitive performance through comprehensive frequency-aware processing. Our approach provides a principled framework for incorporating frequency-domain knowledge into transformer-based time series classification.

</details>


### [172] [Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction](https://arxiv.org/abs/2602.12204)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文通过分析GPT-2发现88%的注意力操作是冗余的，提出了一种生物启发式的记忆巩固机制CRAM，将情景记忆逐步转化为语义记忆，使注意力需求随训练递减。该方法在SRCD基准上以1.6%的注意力计算实现100%准确率，获得37.8×的效率提升，且巩固模式可迁移至新任务。


<details>
  <summary>Details</summary>
Motivation: 现有混合状态空间模型与注意力的架构采用均匀或静态稀疏注意力模式，未能利用"注意力需求应随模式熟悉度增加而递减"这一关键机会。研究发现GPT-2中高达88%的注意力操作检索的信息已可从隐藏状态预测，且训练过程中该冗余度不降，揭示了对动态注意力机制的需求。

Method: 提出CRAM（基于巩固的自适应记忆路由），模拟生物记忆巩固过程，通过逐步将情景检索提炼为参数化语义记忆，实现注意力利用率的动态递减。该方法在训练中表现出约3K步的急剧相变特性。

Result: 在提出的SRCD基准上，仅用1.6%注意力计算即达100%检索准确率（基线为68%）；巩固的模式可迁移至未见任务，无需重训练即可降低48-52%注意力；整体实现37.8×注意力减少。理论证明静态路由需Ω(f·n)注意力。

Conclusion: 研究证实记忆巩固机制对高效处理重复模式至关重要，所学习到的巩固动态与人类认知心理学中的情景-语义记忆转换曲线高度吻合（γ=0.43 vs 0.4-0.5），为设计高效注意力机制提供了生物启发的新方向。

Abstract: Hybrid architectures combining state-space models with attention have achieved strong efficiency-quality tradeoffs, yet existing approaches either apply attention uniformly or learn static sparse patterns. This misses a key opportunity: \emph{attention demand should decrease over time as recurring patterns become familiar}. We present a surprising finding from analyzing GPT-2 models: \textbf{88\%} of attention operations retrieve information already predictable from the model's hidden state, and this redundancy does \emph{not} decrease during training. Motivated by this observation, we introduce \textbf{\ours{}} (\textbf{C}onsolidation-based \textbf{R}outing for \textbf{A}daptive \textbf{M}emory), a biologically inspired memory consolidation mechanism that gradually distills episodic retrievals into parametric semantic memory. Unlike prior sparse attention methods, \ours{} exhibits \emph{decreasing attention utilization} over training, achieving a \textbf{37.8$\times$} reduction through a sharp phase transition at approximately 3K steps. We prove that this capability is \emph{impossible} without consolidation: any static routing scheme requires $Ω(f \cdot n)$ attention for tasks with recurring patterns of frequency $f$. On our proposed SRCD benchmark, \ours{} achieves \textbf{100\% retrieval accuracy} at 1.6\% attention compute (vs.\ 68\% for baselines), and consolidated patterns transfer to unseen tasks with \textbf{48--52\%} attention reduction without retraining. Remarkably, the learned consolidation dynamics quantitatively match human episodic-to-semantic memory transition curves from cognitive psychology ($γ= 0.43$ vs.\ $γ_{\text{human}} \approx 0.4$--$0.5$). Code and benchmarks are available at [anonymized].

</details>


### [173] [The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics](https://arxiv.org/abs/2602.12218)
*Christian Internò,Jumpei Yamaguchi,Loren Amdahl-Culleton,Markus Olhofer,David Klindt,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种非侵入式评估协议PhyIP，通过冻结表征的线性解码来检验神经模型是否真正内化了物理规律而非仅利用统计捷径。在流体力学和轨道力学任务中，PhyIP在分布外测试中能有效恢复物理结构（相关系数ρ > 0.90），而传统的适应式评估方法却导致结构崩溃（ρ ≈ 0.05），证明低容量探针能更准确评估模型的世界模型表征能力。


<details>
  <summary>Details</summary>
Motivation: 确定神经模型是内化物理规律还是仅利用统计捷径仍具挑战性，尤其面临分布外偏移时。标准评估通过下游适应（如微调或大容量探针）测试潜在能力，但这些干预会改变原始表征，混淆自监督学习期间学到的真实内容，无法准确测量模型预训练阶段已获得的物理世界模型。

Method: 提出非侵入式评估协议PhyIP，基于线性表征假说，从冻结的预训练表征中测试物理量的线性可解码性。通过在流体力学和轨道力学领域，分析自监督学习误差与潜在结构线性可访问性的关系，避免对表征进行适应式修改。

Result: 当自监督学习达到低误差时，潜在物理结构变得线性可访问。PhyIP在分布外测试中成功恢复内部能量和牛顿平方反比定律（相关系数ρ > 0.90），而适应式评估方法使该结构几乎完全崩溃（ρ ≈ 0.05），表明适应过程可能掩盖或破坏已学习的物理表征。

Conclusion: 适应式评估会掩盖神经模型中已学习的潜在物理结构，而低容量线性探针（如PhyIP）能更准确地评估模型是否真正内化了物理世界模型，为自监督学习模型的物理理解能力提供了更可靠的评估范式。

Abstract: Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ> 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.

</details>


### [174] [Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training](https://arxiv.org/abs/2602.12222)
*Miaosen Zhang,Yishan Liu,Shuxia Lin,Xu Yang,Qi Dai,Chong Luo,Weihao Jiang,Peng Hou,Anxiang Zeng,Xin Geng,Baining Guo*

Main category: cs.LG

TL;DR: 该研究旨在弥合监督微调(SFT)与强化学习(RL)之间的泛化差距。通过提出分布判别理论(DDT)，并基于DDT设计分布内微调(IDFT)与提示解码两种互补技术，实现了On-Policy SFT。实验表明该方法在保持SFT效率的同时，性能可媲美DPO和SimPO等离线RL算法。


<details>
  <summary>Details</summary>
Motivation: SFT虽计算高效，但因使用离线数据导致泛化性能不及使用on-policy数据的RL。现有方法难以在保持SFT效率的同时利用on-policy数据分布优势，这是亟待解决的关键问题。

Method: 理论层面提出DDT，从数学上解释并量化数据分布与模型诱导分布的对齐程度。技术层面提出：1) IDFT，通过修改损失函数使模型在on-policy数据上微调；2) Hinted Decoding，在生成阶段通过提示机制将训练数据重新对齐到当前模型分布。

Result: 在多项实验中，该框架达到了与DPO、SimPO等先进离线RL算法相当的泛化性能，同时保持了SFT的计算效率优势。

Conclusion: 该工作成功实现了On-Policy SFT，为计算资源受限或RL难以应用的领域提供了高效且高性能的替代方案，有效桥接了SFT与RL之间的鸿沟。

Abstract: Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \textbf{\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \textbf{\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT

</details>


### [175] [Categorical Flow Maps](https://arxiv.org/abs/2602.12233)
*Daan Roos,Oscar Davis,Floor Eijkelboom,Michael Bronstein,Max Welling,İsmail İlkan Ceylan,Luca Ambrogioni,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 提出Categorical Flow Maps，一种基于流匹配的自蒸馏方法，用于类别数据的少步加速生成。通过定义通往单纯形的连续流映射并引入端点一致性目标，实现了在图像、分子图和文本上的最先进少步生成性能。


<details>
  <summary>Details</summary>
Motivation: 受流匹配变分框架和扩散模型加速推理趋势的启发，旨在解决类别数据生成中离散轨迹难以应用蒸馏技术的问题。需要一种能自然约束预测、支持连续轨迹以充分利用引导和重加权技术的生成框架。

Method: 定义将概率质量输运至预测端点的连续流映射，参数化过程自然约束模型输出。训练时结合现有蒸馏技术与新提出的端点一致性目标。利用连续轨迹特性，在测试时直接应用引导和重加权技术调控采样。

Result: 在图像、分子图和文本的少步生成任务上取得state-of-the-art结果，且单步生成性能同样优异。

Conclusion: Categorical Flow Maps通过连续流映射为类别数据生成提供了统一框架，有效支持加速推理和测试时控制，在多领域验证了其先进性和实用性。

Abstract: We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a parametrisation that naturally constrains model predictions. Since our trajectories are continuous rather than discrete, Categorical Flow Maps can be trained with existing distillation techniques, as well as a new objective based on endpoint consistency. This continuous formulation also automatically unlocks test-time inference: we can directly reuse existing guidance and reweighting techniques in the categorical setting to steer sampling toward downstream objectives. Empirically, we achieve state-of-the-art few-step results on images, molecular graphs, and text, with strong performance even in single-step generation.

</details>


### [176] [Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces](https://arxiv.org/abs/2602.12245)
*Anthony Kobanda,Waris Radji*

Main category: cs.LG

TL;DR: 本文通过将联合嵌入预测架构（JEPA）的能量函数限制为内蕴（最小作用量）能量，建立了JEPA与拟度量强化学习（QRL）之间的理论联系，证明了在目标到达控制中，最优代价函数具有内蕴形式，而建模内蕴能量的JEPA属于QRL的目标拟度量价值类，并揭示了非对称能量在方向性任务中的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA框架使用标量兼容性能量进行表示学习，而QRL利用有向距离值（代价-to-go）处理非对称动态下的目标条件控制。两种方法看似独立，但都涉及能量/距离函数的建模。本研究旨在揭示二者之间的内在联系，通过将JEPA能量函数约束为符合最小作用量原理的内蕴能量，从而弥合表示学习与目标条件控制之间的鸿沟。

Method: 研究方法包括：1）将内蕴能量定义为两状态间可接受轨迹上累积局部代价的下确界；2）在温和的封闭性和可加性假设下，证明任何内蕴能量都是拟度量；3）分析目标到达控制中最优代价函数的形式；4）将训练建模内蕴能量的JEPA与QRL的拟度量价值类进行对应；5）对比对称有限能量与非对称拟度量能量在方向性任务中的适用性。

Result: 主要发现：1）在闭包和可加性假设下，内蕴能量必然构成拟度量；2）目标到达控制中的最优代价函数恰好具有内蕴能量形式；3）训练来建模内蕴能量的JEPA自然属于QRL所针对的拟度量价值函数类；4）对称有限能量在单向可达性任务中存在结构性不匹配，而非对称拟度量能量更适合处理方向性问题。

Conclusion: 本研究建立了JEPA与QRL的理论桥梁，表明通过适当约束能量函数形式，表示学习架构可以直接用于目标条件控制任务。这一联系为开发更强大的通用学习系统提供了理论基础，特别是在需要方向性推理的场景中，拟度量能量函数相比传统对称能量具有本质优势。

Abstract: Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [177] [Explaining AI Without Code: A User Study on Explainable AI](https://arxiv.org/abs/2602.11159)
*Natalia Abarca,Andrés Carvallo,Claudia López Moncada,Felipe Bravo-Marquez*

Main category: cs.AI

TL;DR: 针对无代码ML平台缺乏可解释性的问题，本研究为开源平台DashAI开发了一个集成PDP、PFI和KernelSHAP的人本XAI模块。20名新手与专家的用户研究表明，该模块任务成功率达80%以上，新手对解释满意度较高（ESS量表α=0.74），信任度提升（TiA量表α=0.60），但专家对解释充分性要求更严，凸显了平衡易用性与详细性的挑战。


<details>
  <summary>Details</summary>
Motivation: 机器学习在医疗、金融、公共政策等敏感领域的广泛应用引发了对自动化决策透明度的担忧。尽管可解释AI（XAI）能够阐明模型预测机制，但现有方法多需专业技术背景，难以惠及非专业用户。这一缺陷在无代码ML平台中尤为关键，此类平台旨在普及AI应用却普遍缺乏可解释性支持。

Method: 研究团队在开源无代码ML平台DashAI中构建了人本XAI模块，将部分依赖图（PDP）、排列特征重要性（PFI）和KernelSHAP三种互补技术整合至表格分类工作流程。采用用户研究方法，招募20名ML新手与专家，评估系统可用性及解释效果。

Result: 实验结果表明：（i）所有可解释性任务成功率均≥80%；（ii）新手在解释满意度量表（ESS，Cronbach's α=0.74）上评价解释为有用、准确且可信，而专家对充分性与完整性更为苛刻；（iii）解释显著提升自动化信任量表（TiA，α=0.60）的感知可预测性与信心，且新手信任度显著高于专家。

Conclusion: 本研究揭示了无代码ML环境中XAI的核心矛盾：如何在保证新手易用性的同时满足专家对细节的需求。该发现为未来无代码平台可解释性设计提供了重要启示。

Abstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\geq80\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $α$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $α$ = 0.60), with novices showing higher trust than experts. These findings highlight a central challenge for XAI in no-code ML, making explanations both accessible to novices and sufficiently detailed for experts.

</details>


### [178] [On Decision-Valued Maps and Representational Dependence](https://arxiv.org/abs/2602.11295)
*Gil Raitses*

Main category: cs.AI

TL;DR: 本文形式化了决策值映射的概念，并描述了DecisionDB基础设施，该设施通过内容计算标识符来记录、重放和审计同一数据的不同表示形式与计算引擎离散输出结果之间的关系，实现了确定性重放，并将表示空间划分为持久化区域和边界，使决策重用成为可机械验证的条件。


<details>
  <summary>Details</summary>
Motivation: 同一数据的不同表示形式可能导致计算引擎产生不同的离散输出结果，这种结果的不一致性会影响系统的可预测性和可靠性。当前缺乏有效的机制来系统性地记录、追踪和验证这些表示形式与决策结果之间的映射关系，导致难以审计决策过程和实现结果的可重现性。

Method: 提出决策值映射（decision-valued map）的形式化模型，将声明式表示家族中的每个成员映射到其产生的离散结果。设计DecisionDB基础设施，采用内容寻址和一次写入存储方式记录决策标识符和相关工件，通过计算标识符实现决策关系的日志记录、重放和审计。

Result: 实现了确定性重放机制，能够从存储的工件中精确恢复每个记录的决策标识符，确保三个标识字段与持久化值完全匹配。将表示空间划分为持久化区域和边界，为决策重用提供了可机械验证的条件。

Conclusion: 该研究通过形式化决策值映射和构建DecisionDB系统，为处理数据表示多样性导致的决策差异问题提供了系统性解决方案，增强了决策过程的可审计性和结果的可重现性，为构建更可靠的计算系统奠定了基础。

Abstract: A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations preserve the outcome and which change it, associating each member of a declared representation family with the discrete result it produces. This paper formalizes decision-valued maps and describes DecisionDB, an infrastructure that logs, replays and audits these relationships using identifiers computed from content and artifacts stored in write-once form. Deterministic replay recovers each recorded decision identifier exactly from stored artifacts, with all three identifying fields matching their persisted values. The contribution partitions representation space into persistence regions and boundaries, and treats decision reuse as a mechanically checkable condition.

</details>


### [179] [Voxtral Realtime](https://arxiv.org/abs/2602.11298)
*Alexander H. Liu,Andy Ehrenberg,Andy Lo,Chen-Yo Sun,Guillaume Lample,Jean-Malo Delignon,Khyathi Raghavi Chandu,Patrick von Platen,Pavankumar Reddy Muddireddy,Rohin Arora,Sanchit Gandhi,Sandeep Subramanian,Soham Ghosh,Srijan Mishra,Abhinav Rastogi,Alan Jeffares,Albert Jiang,Alexandre Sablayrolles,Amélie Héliou,Andrew Bai,Angele Lenglemetz,Anmol Agarwal,Anton Eliseev,Antonia Calvi,Arjun Majumdar,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Benjamin Tibi,Clémence Lanfranchi,Connor Chen,Corentin Barreau,Corentin Sautier,Cyprien Courtot,Darius Dabert,Diego de las Casas,Elliot Chane-Sane,Enguerrand Paquin,Faruk Ahmed,Federico Baldassarre,Gabrielle Berrada,Gaëtan Ecrepont,Gauthier Guinet,Genevieve Hayes,Georgii Novikov,Giada Pistilli,Guillaume Martin,Gunjan Dhanuka,Gunshi Gupta,Han Zhou,Indraneel Mukherjee,Irene Zhang,Jaeyoung Kim,Jan Ludziejewski,Jason Rute,Joachim Studnia,John Harvill,Jonas Amar,Josselin Somerville Roberts,Julien Tauran,Karmesh Yadav,Kartik Khandelwal,Kush Jain,Laurence Aitchison,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Maarten Buyl,Manan Sharma,Margaret Jennings,Marie Pellat,Mark Prins,Mathieu Poirée,Mathilde Guillaumin,Matthieu Dinot,Matthieu Futeral,Maxime Darrin,Maximilian Augustin,Mert Unsal,Mia Chiquier,Nathan Grinsztajn,Neha Gupta,Olivier Bousquet,Olivier Duchenne,Patricia Wang,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Piotr Miłoś,Prateek Gupta,Pravesh Agrawal,Quentin Torroba,Ram Ramrakhya,Rishi Shah,Romain Sauvestre,Roman Soletskyi,Rosalie Millner,Sagar Vaze,Samuel Humeau,Siddharth Gandhi,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Théo Cachet,Theo Simon Sorg,Thibaut Lavril,Thomas Chabal,Thomas Foubert,Thomas Robert,Thomas Wang,Tim Lawson,Tom Bewley,Tom Edwards,Tyler Wang,Valeriia Nemychnikova,Van Phung,Vedant Nanda,Victor Jouault,Virgile Richard,Vladislav Bataev,Wassim Bouaziz,Wen-Ding Li,William Marshall,Xinghui Li,Xingran Guo,Xinyu Yang,Yannic Neuhaus,Yihan Wang,Zaccharie Ramzi,Zhenlin Xu*

Main category: cs.AI

TL;DR: Voxtral Realtime是原生流式ASR模型，在480毫秒延迟下达到离线转录质量，性能媲美Whisper，解决了流式识别的延迟-精度权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统流式ASR通过改造离线模型实现，导致性能损失。本文旨在直接训练原生流式模型，实现亚秒级延迟下的离线级准确率，克服现有方法的局限性。

Method: 采用延迟流建模框架，创新性地引入因果音频编码器和自适应RMS归一化层优化延迟条件控制；在涵盖13种语言的大规模数据集上进行端到端预训练，实现音频流与文本流的精确对齐。

Result: 实验表明，在480毫秒延迟下，Voxtral Realtime的识别性能与业界领先的离线系统Whisper相当，实现了亚秒级延迟下的高质量实时语音识别。

Conclusion: 该研究验证了原生流式训练范式的有效性，为实时语音交互应用提供了高质量解决方案，并通过Apache 2.0许可证开源模型权重，推动技术普及。

Abstract: We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.

</details>


### [180] [Dissecting Subjectivity and the "Ground Truth" Illusion in Data Annotation](https://arxiv.org/abs/2602.11318)
*Sheza Munir,Benjamin Mah,Krisha Kalsi,Shivani Kapania,Julian Posada,Edith Law,Ding Wang,Syed Ishtiaque Ahmed*

Main category: cs.AI

TL;DR: 本文通过系统性文献综述批判机器学习"ground truth"范式的实证主义谬误，指出其将人类标注分歧视为技术噪声而非社会技术信号。研究发现"共识陷阱"源于位置性识别失败、模型中介标注引发的锚定效应及地理霸权强加的西方规范。论文主张将分歧重新确立为高保真信号，并提出多元标注基础设施路线图，从追求单一正确答案转向映射人类经验的多元性。


<details>
  <summary>Details</summary>
Motivation: 当前"ground truth"范式将人类标注分歧简化为随机噪声，忽视其作为社会文化信号的价值，导致模型缺乏文化多元性并边缘化非西方视角。本研究旨在揭示数据标注实践中促成"共识陷阱"的机制，批判地理霸权与劳动不稳定性对标注质量的侵蚀，并论证重新将分歧视为构建文化胜任模型关键信号的必要性。

Method: 采用系统性文献综述，检索2020-2025年ACL、AIES、CHI、CSCW、EAAMO、FAccT、NeurIPS七个会议的文献。通过分层关键词过滤，从30,897条记录筛选至3,042篇进行人工评审，最终纳入346篇进行反思性主题分析。

Result: 识别出三大机制：(1)位置性识别失败与模型中介标注结合，引入锚定偏见并移除人类声音；(2)地理霸权将西方规范作为普适基准，不稳定数据工作者因经济压力表演性顺从而压制真实主观性；(3)"噪声传感器"谬误将文化多元性误诊为随机误差，系统性消除高保真分歧信号。

Conclusion: 批判将分歧视为噪声的范式，主张将其重新确立为文化胜任模型的核心信号。提出多元标注基础设施路线图，目标从寻找单一"正确答案"转向系统性映射人类经验的多元性，以构建真正包容的机器学习系统。

Abstract: In machine learning, "ground truth" refers to the assumed correct labels used to train and evaluate models. However, the foundational "ground truth" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic literature review analyzes research published between 2020 and 2025 across seven premier venues: ACL, AIES, CHI, CSCW, EAAMO, FAccT, and NeurIPS, investigating the mechanisms in data annotation practices that facilitate this "consensus trap". Our identification phase captured 30,897 records, which were refined via a tiered keyword filtration schema to a high-recall corpus of 3,042 records for manual screening, resulting in a final included corpus of 346 papers for qualitative synthesis. Our reflexive thematic analysis reveals that systemic failures in positional legibility, combined with the recent architectural shift toward human-as-verifier models, specifically the reliance on model-mediated annotations, introduce deep-seated anchoring bias and effectively remove human voices from the loop. We further demonstrate how geographic hegemony imposes Western norms as universal benchmarks, often enforced by the performative alignment of precarious data workers who prioritize requester compliance over honest subjectivity to avoid economic penalties. Critiquing the "noisy sensor" fallacy, where statistical models misdiagnose cultural pluralism as random error, we argue for reclaiming disagreement as a high-fidelity signal essential for building culturally competent models. To address these systemic tensions, we propose a roadmap for pluralistic annotation infrastructures that shift the objective from discovering a singular "right" answer to mapping the diversity of human experience.

</details>


### [181] [Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge](https://arxiv.org/abs/2602.11340)
*Bo Pan,Xuan Kan,Kaitai Zhang,Yan Yan,Shunwen Tan,Zihao He,Zixin Ding,Junjie Wu,Liang Zhao*

Main category: cs.AI

TL;DR: 针对多模态LLM评判器评估AI生成图像时因上下文窗口限制导致视觉示例不足的问题，本文提出BLPO双向提示优化框架，通过将图像转换为保留视觉线索的文本表示，并联合优化评判提示和I2T提示，在有限上下文预算下提升评估效果，四个数据集和三个模型的实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为自动评判者虽被广泛应用，但对齐人类判断仍具挑战。监督微调成本高且缺乏灵活性，而现有自动提示优化(APO)方法主要针对文本评估，在多模态场景下研究不足。特别在评估AI生成图像时，多模态模型的上下文窗口限制导致无法处理足够视觉示例，严重阻碍了基于试错的提示优化效果。

Method: 提出BLPO（Bi-level Prompt Optimization）双向提示优化框架。核心创新是将图像转换为保留评估相关视觉线索的文本表示以突破上下文窗口限制。采用双向优化策略，联合精炼评判提示（judge prompt）和图像到文本提示（I2T prompt），确保在有限的上下文预算内保持评估保真度。

Result: 在四个数据集和三个大语言模型评判器上的实验表明，该方法能够有效提升多模态LLM-as-a-judge的评估性能，验证了所提框架的有效性。

Conclusion: 本研究成功将自动提示优化拓展至多模态评估场景，通过BLPO框架解决了视觉示例受限的关键瓶颈，为AI生成图像的质量评估提供了更高效的解决方案，展示了在多模态评判任务中自动提示优化的巨大潜力。

Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and inflexible, requiring new training for each task or dataset. Recent progress in auto prompt optimization (APO) offers a more efficient alternative by automatically improving the instructions that guide LLM judges. However, existing APO methods primarily target text-only evaluations and remain underexplored in multimodal settings. In this work, we study auto prompt optimization for multimodal LLM-as-a-judge, particularly for evaluating AI-generated images. We identify a key bottleneck: multimodal models can only process a limited number of visual examples due to context window constraints, which hinders effective trial-and-error prompt refinement. To overcome this, we propose BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. Our bi-level optimization approach jointly refines the judge prompt and the I2T prompt to maintain fidelity under limited context budgets. Experiments on four datasets and three LLM judges demonstrate the effectiveness of our method.

</details>


### [182] [AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition](https://arxiv.org/abs/2602.11348)
*Ruipeng Wang,Yuxin Chen,Yukai Wang,Chang Wu,Junfeng Fang,Xiaodong Cai,Qi Gu,Hui Su,An Zhang,Xiang Wang,Xunliang Cai,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 本文提出了AgentNoiseBench框架，用于系统评估智能体模型在噪声环境下的鲁棒性，发现当前智能体模型在面对现实世界中的用户噪声和工具噪声时性能存在显著波动，揭示了理想化基准测试与现实部署之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体在各类基准测试中表现优异，但在复杂且不完美的现实世界部署中性能显著下降。这种差异主要源于现有训练和评估范式基于理想化假设，忽视了现实交互中固有的随机性和噪声。

Method: 首先对现实场景中的偏差和不确定性进行深度分析，将环境噪声分为用户噪声和工具噪声两类；然后开发自动化流水线，在保持任务可解性的前提下向现有智能体基准测试中注入可控噪声；最后对多种架构和参数规模的模型进行广泛评估。

Result: 评估结果显示，不同噪声条件下模型性能存在一致性的变化，表明当前智能体模型对现实环境扰动具有敏感性。

Conclusion: 该研究揭示了当前智能体模型在噪声环境下的脆弱性，强调了开发更鲁棒的评估范式的必要性，为未来提升智能体在现实世界中的可靠性提供了重要见解。

Abstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.

</details>


### [183] [Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization](https://arxiv.org/abs/2602.11351)
*Yihang Yao,Zhepeng Cen,Haohong Lin,Shiqi Liu,Zuxin Liu,Jiacheng Zhu,Zhang-Wei Hong,Laixi Shi,Ding Zhao*

Main category: cs.AI

TL;DR: 针对主动型大语言模型智能体在多轮交互中任务性能与用户参与度难以平衡的问题，本文提出BAO框架，融合行为增强与行为正则化机制，在UserRL基准上实现显著性能提升，效果媲美商业智能体。


<details>
  <summary>Details</summary>
Motivation: 现有主动型智能体强化学习流程存在关键挑战：被动智能体无法高效适应用户意图，而过度依赖人类反馈会降低用户满意度，亟需在任务性能与用户参与度之间取得平衡。

Method: 提出BAO框架，通过行为增强提升主动推理与信息收集能力，同时引入行为正则化抑制低效冗余交互，实现智能体行为与用户期望的对齐。

Result: 在UserRL基准测试中，BAO大幅超越现有主动型智能体强化学习基线，性能达到甚至超过商业大语言模型智能体水平。

Conclusion: BAO框架有效解决了主动型大语言模型智能体的用户对齐问题，为复杂多轮场景下的智能体训练提供了新范式，证明了同时优化任务性能与用户参与度的可行性。

Abstract: Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerged as a promising solution for training such agents in multi-turn settings, allowing interaction strategies to be learned from feedback. However, existing pipelines face a critical challenge in balancing task performance with user engagement, as passive agents can not efficiently adapt to users' intentions while overuse of human feedback reduces their satisfaction. To address this trade-off, we propose BAO, an agentic RL framework that combines behavior enhancement to enrich proactive reasoning and information-gathering capabilities with behavior regularization to suppress inefficient or redundant interactions and align agent behavior with user expectations. We evaluate BAO on multiple tasks from the UserRL benchmark suite, and demonstrate that it substantially outperforms proactive agentic RL baselines while achieving comparable or even superior performance to commercial LLM agents, highlighting its effectiveness for training proactive, user-aligned LLM agents in complex multi-turn scenarios. Our website: https://proactive-agentic-rl.github.io/.

</details>


### [184] [Causal-JEPA: Learning World Models through Object-Level Latent Interventions](https://arxiv.org/abs/2602.11389)
*Heejeong Nam,Quentin Le Lidec,Lucas Maes,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: C-JEPA是一个简单灵活的对象中心世界模型，通过对象级掩码从其他对象推断对象状态，从而诱导潜在干预并防止捷径解。相比无对象掩码的架构，它在反事实推理上提升约20%，在智能体控制任务中仅需1%的潜在输入特征即可实现可比性能。


<details>
  <summary>Details</summary>
Motivation: 世界模型需要强大的关系理解能力以支持预测、推理和控制。尽管对象中心表示提供了有用的抽象，但不足以捕捉依赖交互的动态。现有方法在处理对象间复杂交互时存在局限，需要更有效的交互推理机制。

Method: 提出C-JEPA，将掩码联合嵌入预测从图像块扩展到对象中心表示。采用对象级掩码机制，要求从其他对象推断被掩码对象的状态，从而诱导具有反事实效果的潜在干预，防止模型学习捷径解，强制进行交互推理。

Result: 在视觉问答任务中表现一致提升，反事实推理能力绝对提升约20%。在智能体控制任务中，相比基于图像块的世界模型，仅需1%的潜在输入特征就能实现相当性能，显著提升规划效率。

Conclusion: 对象级掩码通过潜在干预引入因果归纳偏置，使模型更好地理解对象间交互关系。该方法为构建更高效、更具泛化能力的世界模型提供了新思路。代码已开源。

Abstract: World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.

</details>


### [185] [GHOST: Unmasking Phantom States in Mamba2 via Grouped Hidden-state Output-aware Selection & Truncation](https://arxiv.org/abs/2602.11408)
*Michael Menezes,Anastasios Kyrillidis*

Main category: cs.AI

TL;DR: GHOST 是一种针对 Mamba2 的结构化剪枝框架，仅通过前向传播统计信息近似控制理论中的平衡截断。该方法联合衡量隐藏状态的可控性与可观性，在不依赖反向传播的情况下实现高效剪枝，在 130M-2.7B 参数规模模型上达到 50% 状态维度缩减，且 WikiText-2 困惑度仅增加约 1 点。


<details>
  <summary>Details</summary>
Motivation: Mamba2 扩展状态维度虽提升了时序建模能力，却导致自回归生成时推理开销剧增，引发带宽饱和瓶颈。现有方法存在三重局限：非结构化稀疏无法降低激活密度、幅度剪枝忽略运行时动态特性、梯度剪枝计算成本过高，亟需一种高效结构化压缩方案。

Method: 提出 GHOST（分组隐藏状态输出感知选择与截断）框架，核心创新为：1）仅利用前向统计实现控制理论平衡截断近似；2）通过可控性（输入对状态的影响能力）与可观性（状态对输出的贡献度）联合度量，识别关键状态维度；3）实施结构化剪枝，在避免反向传播的前提下实现与梯度方法相当的剪枝保真度。

Result: 在 130M 至 2.7B 参数规模的 Mamba2 模型上，实现 50% 状态维度压缩，WikiText-2 困惑度仅上升约 1 个点，性能损失极小。该方法效果媲美梯度基线但计算开销显著降低，有效缓解推理带宽瓶颈。

Conclusion: GHOST 为 Mamba2 提供了一种高效的结构化压缩新范式，通过前向统计驱动的可控可观性联合度量，在不依赖梯度计算的前提下实现高精度剪枝，为资源受限场景下的时序模型部署开辟了新路径。

Abstract: While Mamba2's expanded state dimension enhances temporal modeling, it incurs substantial inference overhead that saturates bandwidth during autoregressive generation. Standard pruning methods fail to address this bottleneck: unstructured sparsity leaves activations dense, magnitude-based selection ignores runtime dynamics, and gradient-based methods impose prohibitive costs. We introduce GHOST (Grouped Hidden-state Output-aware Selection and Truncation), a structured pruning framework that approximates control-theoretic balanced truncation using only forward-pass statistics. By jointly measuring controllability and observability, GHOST rivals the fidelity of gradient-based methods without requiring backpropagation. As a highlight, on models ranging from 130M to 2.7B parameters, our approach achieves a 50\% state-dimension reduction with approximately 1 perplexity point increase on WikiText-2. Code is available at https://anonymous.4open.science/r/mamba2_ghost-7BCB/.

</details>


### [186] [Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization](https://arxiv.org/abs/2602.11437)
*Chengrui Qu,Christopher Yeh,Kishan Panaganti,Eric Mazumdar,Adam Wierman*

Main category: cs.AI

TL;DR: 本文针对合作多智能体强化学习在真实环境中因环境不确定性导致性能下降的问题，提出了分布鲁棒个体全局最大原则(DrIGM)，通过定义鲁棒的个体动作价值函数并推导现有价值分解架构的鲁棒变体，在保证可扩展性和兼容性的同时提升了系统的鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 传统合作MARL采用集中训练分散执行范式，通过价值分解方法强制满足个体-全局最大(IGM)原则，使分散的贪心动作能恢复团队最优联合动作。然而，由于模拟到现实的差距、模型失配和系统噪声等环境不确定性，该范式在实际部署中的可靠性面临挑战。

Method: 提出DrIGM原则，要求每个智能体的鲁棒贪心动作与鲁棒团队最优联合动作保持一致。基于此，定义了与分散贪心执行兼容的鲁棒个体动作价值函数，并推导出具有可证明鲁棒性保证的现有价值分解架构(VDN/QMIX/QTRAN)的鲁棒变体。这些变体在训练时使用鲁棒的Q目标值，保持可扩展性，并能无缝集成到现有代码库中。

Result: 在高保真SustainGym模拟器和星际争霸游戏环境中，所提方法持续提升了分布外泛化性能，表明其在处理环境不确定性方面的有效性。

Conclusion: DrIGM为合作MARL提供了理论保证的鲁棒性框架，其实践实现简单有效，为真实世界部署提供了可行解决方案。

Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://github.com/crqu/robust-coMARL.

</details>


### [187] [Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning](https://arxiv.org/abs/2602.11455)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: 本研究通过分析跨模态注意力连接，发现仅约15%的token具有强视觉-文本耦合，这些"锚点token"是视觉grounding的关键。提出的AT-RL框架选择性强化这些锚点token，以1.2%的开销使32B模型在MathVista上超越72B基线，揭示了推理质量取决于跨模态锚定的保真度而非token数量。


<details>
  <summary>Details</summary>
Motivation: 尽管带可验证奖励的强化学习（RLVR）显著提升了多模态大语言模型的推理能力，但其内部机制——特别是视觉证据在推理过程中如何被整合——仍不清楚。这一知识空白促使研究者从跨模态注意力连接性的角度深入探索RLVR的工作机理。

Method: 提出锚点token强化学习（AT-RL）框架，利用图聚类技术分析注意力拓扑结构，识别高连接性的锚点token，并对其进行选择性强化。该框架计算开销低，仅引入1.2%的额外负担。

Result: 实验表明，AT-RL在3B至32B参数规模的模型上均有效，其中32B模型在MathVista基准上达到80.2分，超越72B-Instruct基线。性能提升在STEM、视频和通用任务上保持一致。值得注意的是，仅训练低连接性token会导致性能严重退化，证实了视觉锚点在信用分配中的核心作用。

Conclusion: 该研究揭示，多模态推理的质量并非由token数量决定，而是取决于跨模态锚定的保真度。有效的多模态强化学习关键在于精确地将信用分配给视觉锚点token，这为未来MLLM训练提供了重要洞察。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by token quantity but by the fidelity of cross-modal anchoring.

</details>


### [188] [AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems](https://arxiv.org/abs/2602.11510)
*Faouzi El Yagoubi,Ranwa Al Mallah,Godwin Badu-Marfo*

Main category: cs.AI

TL;DR: 本文揭示多智能体大语言模型系统存在严重的内部通道隐私泄露风险，现有仅监控输出的审计方法无法检测。研究团队开发AgentLeak基准测试，涵盖1000个跨领域场景和32类攻击，对5个主流模型进行测试。发现多智能体虽降低输出泄露率（27.2% vs 单智能体43.2%），但内部通道泄露率高达68.8%，系统总暴露率达68.9%，其中41.7%的违规被传统审计遗漏。Claude 3.5 Sonnet表现最佳，凸显模型级安全训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前隐私基准测试仅关注模型最终输出，无法衡量多智能体协同任务中通过内部通道（智能体间消息、共享内存、工具参数）传输敏感数据带来的泄露风险。这种盲区导致对系统真实隐私风险的严重低估，亟需全栈评估框架来识别和量化这些未受监控的数据通路。

Method: 提出AgentLeak，首个覆盖内部通道的全栈隐私泄露基准测试。包含：1）医疗、金融、法律、企业四个领域的1000个测试场景；2）32类攻击分类法；3）三层检测流水线。对GPT-4o、GPT-4o-mini、Claude 3.5 Sonnet、Mistral Large和Llama 3.3 70B五个模型进行4979条轨迹测试，系统评估外部输出通道（C1）、智能体间消息（C2）等多个维度的泄露情况。

Result: 实验表明：多智能体配置使输出通道泄露率从单智能体的43.2%降至27.2%，但内部通道（C2）泄露率高达68.8%，系统总暴露率达68.9%（C1、C2、C5聚合），输出-only审计遗漏41.7%违规。Claude 3.5 Sonnet表现最优（外部3.3%，内部28.1%），表明模型级安全训练可迁移至内部通道保护。所有模型和领域中C2>C1模式一致，证实智能体间通信是主要漏洞。

Conclusion: 多智能体LLM系统的内部通道隐私保护严重不足，传统输出审计存在重大盲区。未来需开发内置内部通道监控的协调框架，对智能体间通信实施强制隐私控制。模型级安全训练对内部通道保护有正向迁移效果，为构建更安全的系统提供了可行方向。

Abstract: Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five models and four domains, the pattern C2 > C1 holds consistently, confirming that inter-agent communication is the primary vulnerability. These findings underscore the need for coordination frameworks that incorporate internal-channel privacy protections and enforce privacy controls on inter-agent communication.

</details>


### [189] [Human-Inspired Continuous Learning of Internal Reasoning Processes: Learning How to Think for Adaptive AI Systems](https://arxiv.org/abs/2602.11516)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出一种受人类启发的持续学习框架，将内部推理过程作为主要学习对象，统一推理、行动、反思和验证。该框架记录内部推理轨迹和环境交互，优化任务内容和推理组织，实现边处理边学习。在温度传感器异常检测实验中，内部过程学习使平均运行时间减少23.9%。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法主要强调学习任务特定输出或静态知识表示，忽视了内部推理结构、行动调度策略和学习机制本身的持续优化，限制了系统在动态真实环境中的持续适应能力。

Method: 提出一个受人类启发的持续学习框架，在增强并行学习的序列推理模型中统一推理、行动、反思和验证；将内部思维过程作为主要学习对象，系统记录推理轨迹和环境交互作为结构化学习材料；支持预定义逻辑与学习过程的可控替换；引入层次化学习机制，联合调整任务级参数和学习策略。

Result: 在温度传感器异常检测任务上的实验结果表明，采用内部过程学习可使系统平均运行时间降低23.9%。

Conclusion: 该框架使系统能够在保持运行稳定性的同时逐步演化内部认知架构，通过内部推理过程的学习实现持续适应，为动态环境下的AI系统开发提供了新范式。

Abstract: Learning internal reasoning processes is crucial for developing AI systems capable of sustained adaptation in dynamic real-world environments. However, most existing approaches primarily emphasize learning task-specific outputs or static knowledge representations, while overlooking the continuous refinement of internal reasoning structures, action scheduling policies, and learning mechanisms themselves. In this paper, we propose a human-inspired continuous learning framework that unifies reasoning, action, reflection, and verification within a sequential reasoning model enhanced by parallel learning. The framework explicitly treats internal thinking processes as primary learning objects. It systematically records internal reasoning trajectories and environmental interactions as structured learning material, enabling the system to optimize not only task-level content but also the organization, scheduling, and evolution of reasoning activities. This design realizes learning alongside processing, allowing cognitive structures to improve during execution. Furthermore, the framework supports controlled replacement of predefined logic with learned procedures and introduces a hierarchical learning-to-learn mechanism that jointly adapts task-level parameters and learning strategies. As a result, the system progressively evolves its internal cognitive architecture while preserving operational stability. Experimental results on a temperature sensor abnormality detection task show that incorporating internal-process learning reduces average runtime by 23.9%.

</details>


### [190] [CausalAgent: A Conversational Multi-Agent System for End-to-End Causal Inference](https://arxiv.org/abs/2602.11527)
*Jiawei Zhu,Wei Chen,Ruichu Cai*

Main category: cs.AI

TL;DR: 本文提出CausalAgent，一个基于多智能体系统、检索增强生成和模型上下文协议的对话式因果推断平台，通过自然语言交互实现端到端自动化分析，显著降低因果分析的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断工作流要求研究者具备统计学与计算机科学双重背景，需手动选择算法、处理数据质量问题并解释复杂结果，形成高技能壁垒，限制了其在医疗、经济、社会科学等领域的广泛应用。

Method: 系统创新融合多智能体系统(MAS)实现任务分解与协作，利用检索增强生成(RAG)确保分析结果的准确性与可解释性，通过模型上下文协议(MCP)标准化工具调用，构建自然语言交互界面，自动化完成从数据清洗、因果图学习、偏误校正到报告生成的完整流程。

Result: 用户仅需上传数据并用自然语言提问，即可获得严谨的交互式分析报告。系统通过可视化界面将技术细节封装，在降低使用门槛的同时，保持了分析过程的科学严谨性与结果可解释性。

Conclusion: CausalAgent建立了一种新颖的人机协同范式，通过将因果推断技术封装为对话式服务，实现了该技术的民主化，使非专业用户也能进行严谨的因果分析，为社会科学实证研究提供了新工具。

Abstract: Causal inference holds immense value in fields such as healthcare, economics, and social sciences. However, traditional causal analysis workflows impose significant technical barriers, requiring researchers to possess dual backgrounds in statistics and computer science, while manually selecting algorithms, handling data quality issues, and interpreting complex results. To address these challenges, we propose CausalAgent, a conversational multi-agent system for end-to-end causal inference. The system innovatively integrates Multi-Agent Systems (MAS), Retrieval-Augmented Generation (RAG), and the Model Context Protocol (MCP) to achieve automation from data cleaning and causal structure learning to bias correction and report generation through natural language interaction. Users need only upload a dataset and pose questions in natural language to receive a rigorous, interactive analysis report. As a novel user-centered human-AI collaboration paradigm, CausalAgent explicitly models the analysis workflow. By leveraging interactive visualizations, it significantly lowers the barrier to entry for causal analysis while ensuring the rigor and interpretability of the process.

</details>


### [191] [Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use](https://arxiv.org/abs/2602.11541)
*Hanbing Liu,Chunhao Tian,Nan An,Ziyuan Wang,Pinyan Lu,Changyuan Yu,Qi Qi*

Main category: cs.AI

TL;DR: 针对预算受限的工具增强型智能体，提出INTENT推理时规划框架，通过意图感知分层世界模型预测未来工具使用与风险成本，在严格满足预算约束的同时显著提升任务成功率，并对工具价格波动和预算变化具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多步骤任务中调用外部工具时面临预算严格受限的挑战。传统序列决策方法因状态动作空间庞大、工具执行结果随机性高以及探索成本过高等问题而难以直接应用，亟需一种能在线平衡预算约束与任务性能的规划框架。

Method: 提出INTENT框架，构建意图感知的分层世界模型，通过预测未来工具使用模式与风险校准的成本分布，在推理时进行层次化规划。该方法将决策过程分解为意图生成与执行两个层次，综合考虑预算约束、工具执行随机性及市场动态变化，实现高效的在线决策。

Result: 在成本增强的StableToolBench基准测试中，INTENT严格实现了硬性预算可行性，任务成功率显著超越现有基线方法，并在工具价格变化和预算波动等动态市场环境下保持稳定性能，展现出优异的鲁棒性。

Conclusion: INTENT框架通过意图感知的分层世界模型有效解决了预算约束下工具增强智能体的规划难题，实现了预算严格可行性与任务性能的协同优化，为成本敏感型AI系统的实际应用提供了可靠解决方案。

Abstract: We study budget-constrained tool-augmented agents, where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions, making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost, and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.

</details>


### [192] [Learning to Configure Agentic AI Systems](https://arxiv.org/abs/2602.11574)
*Aditya Taparia,Som Sagar,Ransalu Senanayake*

Main category: cs.AI

TL;DR: 针对LLM智能体系统的配置难题，ARC通过强化学习动态优化查询级别的配置策略，相比固定模板显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体系统的配置依赖于固定模板或人工启发式方法，这种"一刀切"的方式无法适应不同查询的复杂度差异，导致系统行为脆弱且计算资源浪费。由于工作流、工具、token预算和提示等要素的组合空间巨大，需要一种能够根据查询难度动态调整配置的解决方案。

Method: 本文将智能体配置形式化为查询级别的决策问题，提出了ARC（Agentic Resource & Configuration learner）框架。该框架采用强化学习方法训练轻量级分层策略，能够动态地为每个查询定制化选择工作流、工具、token预算和提示等配置参数。

Result: 在多个涵盖推理和工具增强问答的基准测试中，ARC学习到的策略持续优于人工设计和其他基线方法。具体而言，任务准确率提升最高达25%，同时显著降低了token使用量和运行时开销。

Conclusion: 研究表明，基于查询级别的智能体配置学习是替代"一刀切"设计的有效方案，既能提升性能又能优化资源利用，为LLM智能体系统的自适应配置提供了新思路。

Abstract: Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to "one size fits all" designs.

</details>


### [193] [The Five Ws of Multi-Agent Communication: Who Talks to Whom, When, What, and Why -- A Survey from MARL to Emergent Language and LLMs](https://arxiv.org/abs/2602.11583)
*Jingdi Chen,Hanqing Yang,Zongjun Liu,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: 本综述通过"五个W"框架系统梳理多智能体通信研究，追踪其从多智能体强化学习、涌现语言到大语言模型的三阶段演进，揭示不同范式下的设计权衡与核心挑战，为融合学习、语言与控制的未来混合系统提供设计模式。


<details>
  <summary>Details</summary>
Motivation: 多智能体序贯决策在自动驾驶、机器人和协作AI等领域至关重要，而在动态部分可观测环境中，通信是降低不确定性、实现协作的关键。现有研究分散于不同范式缺乏统一框架，亟需系统性综述以厘清演进脉络、设计权衡与未解难题。

Method: 采用"Five Ws"（谁与谁、沟通内容、沟通时机、沟通动机）作为组织框架，系统回顾多智能体通信文献；通过追溯MARL、EL和LLM三大范式的演进路径，比较不同选择对通信设计的影响，识别核心权衡与待解问题。

Result: 揭示通信机制从手工设计/隐式协议→端到端学习→结构化符号语言→自然语言先验的演进轨迹；发现各范式局限：MARL协议任务特异且难解释，EL在grounding、泛化与扩展性上存在瓶颈，LLM需进一步探索开放场景协作；提炼出跨范式通信设计权衡、实用模式与开放挑战。

Conclusion: 未来需构建融合学习、语言与控制的混合系统，实现可扩展、可解释的多智能体协作。本综述为研究者提供统一视角下的设计指南，推动多智能体通信向更开放、更通用的场景发展，支持下一代协作智能系统开发。

Abstract: Multi-agent sequential decision-making powers many real-world systems, from autonomous vehicles and robotics to collaborative AI assistants. In dynamic, partially observable environments, communication is often what reduces uncertainty and makes collaboration possible. This survey reviews multi-agent communication (MA-Comm) through the Five Ws: who communicates with whom, what is communicated, when communication occurs, and why communication is beneficial. This framing offers a clean way to connect ideas across otherwise separate research threads. We trace how communication approaches have evolved across three major paradigms. In Multi-Agent Reinforcement Learning (MARL), early methods used hand-designed or implicit protocols, followed by end-to-end learned communication optimized for reward and control. While successful, these protocols are frequently task-specific and hard to interpret, motivating work on Emergent Language (EL), where agents can develop more structured or symbolic communication through interaction. EL methods, however, still struggle with grounding, generalization, and scalability, which has fueled recent interest in large language models (LLMs) that bring natural language priors for reasoning, planning, and collaboration in more open-ended settings. Across MARL, EL, and LLM-based systems, we highlight how different choices shape communication design, where the main trade-offs lie, and what remains unsolved. We distill practical design patterns and open challenges to support future hybrid systems that combine learning, language, and control for scalable and interpretable multi-agent collaboration.

</details>


### [194] [MAPLE: Modality-Aware Post-training and Learning Ecosystem](https://arxiv.org/abs/2602.11596)
*Nikhil Verma,Minjung Kim,JooYoung Yoo,Kyung-Min Jin,Manasa Bharadwaj,Kevin Ferreira,Ko Keun Kim,Youngjoon Kim*

Main category: cs.AI

TL;DR: 该论文提出MAPLE，一种模态感知的多模态语言模型强化学习后训练框架。通过识别任务所需的最小信号组合，MAPLE有效降低策略梯度方差、加速收敛，并提升对信号缺失与分布漂移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态RL后训练将所有输入信号视为同等重要，忽略了任务实际模态需求，导致梯度方差膨胀、收敛缓慢，且在真实世界信号变化场景下鲁棒性不足。

Method: MAPLE包含三个核心组件：(1) MAPLE-bench：首个标注每项任务最小必需信号组合的基准；(2) MAPO：模态感知策略优化，通过按模态需求分层批次降低异构优势引起的梯度方差；(3) 自适应加权与课程调度：动态平衡并优先训练困难信号组合。

Result: MAPLE将单模态与多模态准确率差距缩小30.24%，收敛速度提升3.18倍，并在信号访问受限的真实场景中保持所有模态组合的稳定性。

Conclusion: MAPLE为可部署的多模态强化学习后训练提供了完整解决方案，其模态感知设计显著提升了训练效率和模型鲁棒性，标志着向实际部署迈出的关键一步。

Abstract: Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows convergence, and degrades robustness to real-world distribution shifts where signals may be missing, added, or reweighted. We introduce MAPLE, a complete modality-aware post-training and learning ecosystem comprising: (1) MAPLE-bench, the first benchmark explicitly annotating minimal signal combinations required per task; (2) MAPO, a modality-aware policy optimization framework that stratifies batches by modality requirement to reduce gradient variance from heterogeneous group advantages; (3) Adaptive weighting and curriculum scheduling that balances and prioritizes harder signal combinations. Systematic analysis across loss aggregation, clipping, sampling, and curriculum design establishes MAPO's optimal training strategy. Adaptive weighting and curriculum focused learning further boost performance across signal combinations. MAPLE narrows uni/multi-modal accuracy gaps by 30.24%, converges 3.18x faster, and maintains stability across all modality combinations under realistic reduced signal access. MAPLE constitutes a complete recipe for deployment-ready multimodal RL post-training.

</details>


### [195] [scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery](https://arxiv.org/abs/2602.11609)
*Yiming Gao,Zhen Wang,Jefferson Chen,Mark Antkowiak,Mengzhou Hu,JungHo Kong,Dexter Pratt,Jieyuan Liu,Enze Ma,Zhiting Hu,Eric P. Xing*

Main category: cs.AI

TL;DR: 本文提出首个组学原生推理框架scPilot，使大语言模型能直接检查单细胞RNA-seq数据并调用生物信息学工具进行逐步推理，完成细胞类型注释、发育轨迹重建和转录因子分析等核心任务；同时发布评估套件scBench，实验显示迭代推理显著提升模型性能并生成可解释的推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞组学分析缺乏自然语言交互和可解释的基于原始数据的推理能力，传统流程难以让LLM直接参与系统性数据分析，需要一种能实现可审计、可解释且具有诊断价值的分析框架。

Method: 开发scPilot框架，将核心单细胞分析任务转化为LLM可执行的逐步推理问题，支持自然语言对话与按需调用生物信息学工具；构建scBench评估套件，包含9个专家级数据集和自动化评分器，用于客观测评组学原生推理能力。

Result: o1模型通过迭代推理将细胞类型注释平均准确率提升11%；Gemini-2.5-Pro相比一次提示将轨迹图编辑距离降低30%；生成的透明推理轨迹能解释标记基因模糊性和调控逻辑。

Conclusion: scPilot通过将LLM锚定在原始组学数据上，实现了可审计、可解释且诊断信息丰富的单细胞分析，为生物医学研究提供了可验证的AI推理新范式。

Abstract: We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.
  To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.
  Code, data, and package are available at https://github.com/maitrix-org/scPilot

</details>


### [196] [When Agents Disagree With Themselves: Measuring Behavioral Consistency in LLM-Based Agents](https://arxiv.org/abs/2602.11619)
*Aman Mehta*

Main category: cs.AI

TL;DR: 本研究通过3000次LLM智能体重复实验揭示，即使输入完全相同，智能体行为也存在显著不稳定性。关键发现是行为一致性可作为任务失败的强预测指标，且大部分差异源于早期决策步骤。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在真实场景中承担越来越重要的任务角色，但其决策过程的确定性尚未得到充分验证。若智能体每次运行表现不一致，将严重影响其可信度和实用性。因此，系统性评估并理解这种行为变异性对构建可靠智能体至关重要。

Method: 研究采用HotpotQA多跳问答数据集，测试了Llama 3.1 70B、GPT-4o和Claude Sonnet 4.5三个模型。对每个任务进行10次重复运行，共收集3000组轨迹数据。通过分析动作序列的唯一性来量化行为一致性，并统计其与任务准确率的相关性。

Result: 实验结果显示：1)每次任务平均产生2.0-4.2条不同动作序列；2)低变异性任务(≤2条唯一路径)准确率达80-92%，而高变异性任务(≥6条)准确率仅25-60%，差距达32-55个百分点；3)69%的行为分歧发生在第2步(首次搜索查询)，表明早期决策是变异性主要来源。

Conclusion: 该研究表明实时监控智能体行为一致性可作为早期错误检测的有效手段，为提升LLM智能体的鲁棒性和可靠性开辟了新方向，对智能体系统的实际部署具有重要意义。

Abstract: Run the same LLM agent on the same task twice: do you get the same behavior? We find the answer is often no. In a study of 3,000 agent runs across three models (Llama 3.1 70B, GPT-4o, and Claude Sonnet 4.5) on HotpotQA, we observe that ReAct-style agents produce 2.0--4.2 distinct action sequences per 10 runs on average, even with identical inputs. More importantly, this variance predicts failure: tasks with consistent behavior ($\leq$2 unique paths) achieve 80--92% accuracy, while highly inconsistent tasks ($\geq$6 unique paths) achieve only 25--60%, a 32--55 percentage point gap depending on model. We trace variance to early decisions: 69% of divergence occurs at step 2, the first search query. Our results suggest that monitoring behavioral consistency during execution could enable early error detection and improve agent reliability.

</details>


### [197] [Neuro-Symbolic Multitasking: A Unified Framework for Discovering Generalizable Solutions to PDE Families](https://arxiv.org/abs/2602.11630)
*Yipeng Huang,Dejun Xu,Zexin Lin,Zhenzhong Wang,Min Jiang*

Main category: cs.AI

TL;DR: 本文提出一种名为NMIPS的神经辅助多任务符号PDE求解器框架，用于高效求解PDE族。该框架通过多因子优化同步发现解析解，并采用仿射迁移方法在PDE之间传递学习的数学结构，避免重复求解，在提供可解释解析解的同时，相比基线方法精度提升高达35.7%。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法求解PDE族时需独立求解每个实例，计算成本高昂；而现有机器学习PDE求解器虽速度快，但其"黑箱"特性缺乏解析表达式的可解释性，难以支持深入科学洞察。因此亟需一种兼具高效性与可解释性的PDE族求解方法。

Method: 1. 提出神经辅助多任务符号PDE求解器框架NMIPS；2. 采用多因子优化技术同时发现PDE族中多个方程的解析解；3. 设计仿射迁移方法，在PDE族内部迁移已学习的数学结构，避免从零开始求解每个实例。

Result: 在多个测试案例中，NMIPS框架相比现有基线方法表现出显著优势，精度提升最高达约35.7%，同时成功生成可解释的解析解，验证了方法的有效性。

Conclusion: NMIPS框架通过整合多任务学习与迁移学习策略，有效解决了PDE族求解中的计算效率与可解释性矛盾，为科学计算领域提供了新的范式，兼具实用价值与理论意义。

Abstract: Solving Partial Differential Equations (PDEs) is fundamental to numerous scientific and engineering disciplines. A common challenge arises from solving the PDE families, which are characterized by sharing an identical mathematical structure but varying in specific parameters. Traditional numerical methods, such as the finite element method, need to independently solve each instance within a PDE family, which incurs massive computational cost. On the other hand, while recent advancements in machine learning PDE solvers offer impressive computational speed and accuracy, their inherent ``black-box" nature presents a considerable limitation. These methods primarily yield numerical approximations, thereby lacking the crucial interpretability provided by analytical expressions, which are essential for deeper scientific insight. To address these limitations, we propose a neuro-assisted multitasking symbolic PDE solver framework for PDE family solving, dubbed NMIPS. In particular, we employ multifactorial optimization to simultaneously discover the analytical solutions of PDEs. To enhance computational efficiency, we devise an affine transfer method by transferring learned mathematical structures among PDEs in a family, avoiding solving each PDE from scratch. Experimental results across multiple cases demonstrate promising improvements over existing baselines, achieving up to a $\sim$35.7% increase in accuracy while providing interpretable analytical solutions.

</details>


### [198] [Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation](https://arxiv.org/abs/2602.11635)
*Shuo Lu,Jianjie Cheng,Yinuo Xu,Yongcan Yu,Lijun Sheng,Peijie Wang,Siru Jiang,Yongguan Hu,Run Ling,Yihua Shao,Ao Ma,Wei Feng,Lingxiao He,Meng Wang,Qianlong Xie,Xingxing Wang,Ran He,Jian Liang*

Main category: cs.AI

TL;DR: 针对MLLMs数学空间推理能力薄弱（<60%准确率）的问题，本研究提出MathSpatial统一框架，包含评估基准、训练数据和结构化推理方法，显著提升Qwen2.5-VL-7B性能并减少25% token使用。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在感知任务表现优异，但数学空间推理能力远逊于人类（<60% vs >95%），存在根本性缺陷，且缺乏有效分离感知与推理的研究框架。

Method: 提出MathSpatial框架：1）MathSpatial-Bench：2000题基准，3大类11子类，隔离感知噪声；2）MathSpatial-Corpus：8000题训练集；3）MathSpatial-SRT：建模推理为关联、约束、推断三种原子操作的结构化追踪。

Result: 微调Qwen2.5-VL-7B于MathSpatial-Corpus，实现竞争力准确率，token使用减少25%，首次提供大规模感知-推理解耦资源。

Conclusion: MathSpatial为MLLMs数学空间推理的精确测量与提升提供了首个大规模资源，奠定了重要研究基础。

Abstract: Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95\% accuracy, but we find that most leading MLLMs fail to reach even 60\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations--Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.

</details>


### [199] [Quark Medical Alignment: A Holistic Multi-Dimensional Alignment and Collaborative Optimization Paradigm](https://arxiv.org/abs/2602.11661)
*Tianxiang Xu,Jiayi Liu,Yixuan Tong,Jialu Xu,Yunqing Wei,Kaiwen Feng,PanPan Hou,Kangping Yin,Jiyuan Hu,Hao Zhou,Zhenxin Ma,Jian Xu,Guanjun Jiang*

Main category: cs.AI

TL;DR: 该论文针对医疗问答领域现有强化学习对齐方法的不足（基于人类反馈的强化学习方法偏好标注昂贵且不反映医学事实绝对正确性，基于可验证奖励的强化学习方法缺乏有效验证器），提出了一种鲁棒医疗对齐范式。通过构建四维医疗对齐矩阵（基础能力、专业知识、在线反馈、格式规范）和闭环监督机制，结合参考冻结归一化与三因素自适应动态加权策略，解决了多目标奖励的尺度冲突和优化不稳定问题，在真实医疗场景评估中验证有效，为垂直领域复杂对齐建立了新范式。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型强化学习对齐方法在医疗高风险场景中面临根本性范式错配：基于人类反馈的强化学习依赖昂贵且无法反映医学事实绝对正确性的偏好标注；基于可验证奖励的强化学习缺乏处理复杂临床语境的有效自动验证器；同时医疗对齐需同步优化正确性、安全性与合规性，但多目标异构奖励信号易产生尺度失配和优化冲突，亟需新的鲁棒对齐范式。

Method: 提出包含两大核心组件的医疗对齐范式：1）构建多维度医疗对齐矩阵，将目标分解为基础能力、专业知识、在线反馈、格式规范四个维度，每个维度建立"可观测指标→归因诊断→可优化奖励"的闭环，提供细粒度高分辨率监督信号；2）设计统一优化机制，采用参考冻结归一化对齐奖励尺度，实施三因素自适应动态加权策略（弱点导向、风险优先、冗余削减）实现协同优化。

Result: 在真实医疗场景评估中，该范式表现出显著有效性，成功解决了异构信号梯度主导和优化不稳定问题，为垂直领域复杂对齐任务建立了可推广的新范式。

Conclusion: 本研究提出的医疗对齐范式通过结构化的多维度分解和动态加权优化机制，有效克服了高风险医疗场景中现有强化学习方法的局限性，为垂直领域的复杂多目标对齐问题提供了系统性的解决方案，具有重要的理论和实践价值。

Abstract: While reinforcement learning for large language model alignment has progressed rapidly in recent years, transferring these paradigms to high-stakes medical question answering reveals a fundamental paradigm mismatch. Reinforcement Learning from Human Feedback relies on preference annotations that are prohibitively expensive and often fail to reflect the absolute correctness of medical facts. Reinforcement Learning from Verifiable Rewards lacks effective automatic verifiers and struggles to handle complex clinical contexts. Meanwhile, medical alignment requires the simultaneous optimization of correctness, safety, and compliance, yet multi-objective heterogeneous reward signals are prone to scale mismatch and optimization conflicts.To address these challenges, we propose a robust medical alignment paradigm. We first construct a holistic multi-dimensional medical alignment matrix that decomposes alignment objectives into four categories: fundamental capabilities, expert knowledge, online feedback, and format specifications. Within each category, we establish a closed loop of where observable metrics inform attributable diagnosis, which in turn drives optimizable rewards, thereby providing fine-grained, high-resolution supervision signals for subsequent iterative optimization. To resolve gradient domination and optimization instability problem caused by heterogeneous signals, we further propose a unified optimization mechanism. This mechanism employs Reference-Frozen Normalization to align reward scales and implements a Tri-Factor Adaptive Dynamic Weighting strategy to achieve collaborative optimization that is weakness-oriented, risk-prioritized, and redundancy-reducing. Experimental results demonstrate the effectiveness of our proposed paradigm in real-world medical scenario evaluations, establishing a new paradigm for complex alignment in vertical domains.

</details>


### [200] [Benchmark Health Index: A Systematic Framework for Benchmarking the Benchmarks of LLMs](https://arxiv.org/abs/2602.11674)
*Longyuan Zhu,Hairan Hua,Linlin Miao,Bing Zhao*

Main category: cs.AI

TL;DR: 针对LLM基准测试分数膨胀和选择性报告导致可信度下降的问题，本文提出Benchmark Health Index (BHI)框架，从能力区分度、抗饱和度和影响力三个维度量化评估基准测试的健康状况，为基准选择提供原则性依据。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速发展，但现有基准测试因分数通胀和选择性报告而可靠性下降，社区对评估结果信任度降低，亟需系统化方法评估基准测试的质量和剩余价值。

Method: 提出BHI纯数据驱动框架，包含能力区分度（分离模型性能）、抗饱和度（评估天花板效应剩余空间）和影响力（衡量学术与工业界影响）三个正交维度。从2025年91个代表性模型的技术报告中提炼106个验证基准，系统刻画评估现状。

Result: 建立了首个在宏观层面量化基准测试健康的框架，为基准选择提供原则性基础，并支持下一代评估协议的动态生命周期管理。

Conclusion: BHI框架有效解决了基准测试可靠性问题，通过量化健康指数帮助研究者和实践者做出更明智的评估决策，推动评估体系可持续发展。

Abstract: Large Language Models (LLMs) are advancing rapidly, yet the benchmarks used to measure this progress are becoming increasingly unreliable. Score inflation and selective reporting have eroded the authority of standard benchmarks, leaving the community uncertain about which evaluation results remain trustworthy. We introduce the Benchmark Health Index (BHI), a pure data-driven framework for auditing evaluation sets along three orthogonal and complementary axes: (1) Capability Discrimination, measuring how sharply a benchmark separates model performance beyond noise; (2) Anti-Saturation, estimating remaining headroom before ceiling effects erode resolution and thus the benchmark's expected longevity; and (3) Impact, quantifying influence across academic and industrial ecosystems via adoption breadth and practice-shaping power. By distilling 106 validated benchmarks from the technical reports of 91 representative models in 2025, we systematically characterize the evaluation landscape. BHI is the first framework to quantify benchmark health at a macro level, providing a principled basis for benchmark selection and enabling dynamic lifecycle management for next-generation evaluation protocols.

</details>


### [201] [Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs](https://arxiv.org/abs/2602.11675)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 本文揭示机器学习"错误原因正确结果"现象的因果机制：自回归训练无法区分关联P(Y|X)与干预P(Y|do(X))，导致"梯级崩溃"和"随机性固着"。提出认知遗憾最小化(ERM)框架，通过三层架构实现因果信念修正，在1360个因果陷阱实验中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 当前高性能机器学习系统存在"捷径学习"病理——在分布偏移下失效，其根源在于缺乏对因果关联与干预效应的区分。这种"错误原因正确结果"现象使智能体固守错误因果模型，亟需理论解释和解决方案。

Method: 提出Epistemic Regret Minimization (ERM)作为因果信念修正目标，嵌入三层架构：(1)物理接地定理：证明满足执行器独立性的动作可实现有效do-操作；(2)ERM算子：满足AGM信念修正公设，独立于任务成功惩罚因果推理错误；(3)故障分类法：识别推理错误模式并注入领域无关保护。

Result: 理论证明有限样本下渐近恢复真实干预分布；在6个前沿LLM的1360个因果陷阱测试中：梯级崩溃在推理增强模型中仍存(如GPT-5.2达3.7%)；高级模型呈现反向可操控性；ERM反馈可恢复53-59%的固着错误，而结果级反馈完全失效。

Conclusion: 研究形式化了因果学习失败机制，提出的ERM框架能有效防止错误因果模型固着，为构建可泛化的因果推理AI提供理论基础。实验揭示先进模型对通用修正的抗性，提示未来需发展更针对性的干预方法。

Abstract: Machine learning systems that are "right for the wrong reasons" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.

</details>


### [202] [Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing](https://arxiv.org/abs/2602.11678)
*Chengwei Ma,Zhen Tian,Zhou Zhou,Zhixian Xu,Xiaowei Zhu,Xia Hua,Si Shi,F. Richard Yu*

Main category: cs.AI

TL;DR: 本文针对多模态大语言模型的"结构盲"问题，提出Vector-to-Graph(V2G)框架将CAD原理图转换为属性图，在电气合规性检查基准测试中显著优于现有MLLMs，揭示了像素驱动范式的系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在视觉理解方面表现突出，但其像素驱动的范式在处理工程原理图时无法捕捉拓扑结构与符号逻辑，因丢弃了矢量定义中显式的结构关系，严重阻碍了AI在工程领域的实际应用。

Method: 提出V2G流水线，自动将CAD矢量图转换为属性图结构，其中节点代表工程组件，边编码物理连接，将隐式视觉信息转化为显式、机器可审计的图表示，实现结构感知的推理。

Result: 在电气合规性诊断基准上，V2G方法在所有错误类别中均获得大幅精度提升，而当前最优MLLMs性能接近随机猜测，证明了结构感知表示的显著优势。

Conclusion: 研究揭示了基于像素的多模态方法在工程领域的根本性不足，结构感知表示是实现可靠工程AI部署的有效路径，为未来研究提供了重要基准与实现。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual understanding, yet they suffer from a critical limitation: structural blindness. Even state-of-the-art models fail to capture topology and symbolic logic in engineering schematics, as their pixel-driven paradigm discards the explicit vector-defined relations needed for reasoning. To overcome this, we propose a Vector-to-Graph (V2G) pipeline that converts CAD diagrams into property graphs where nodes represent components and edges encode connectivity, making structural dependencies explicit and machine-auditable. On a diagnostic benchmark of electrical compliance checks, V2G yields large accuracy gains across all error categories, while leading MLLMs remain near chance level. These results highlight the systemic inadequacy of pixel-based methods and demonstrate that structure-aware representations provide a reliable path toward practical deployment of multimodal AI in engineering domains. To facilitate further research, we release our benchmark and implementation at https://github.com/gm-embodied/V2G-Audit.

</details>


### [203] [ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces](https://arxiv.org/abs/2602.11683)
*Xin Xu,Tong Yu,Xiang Chen,Haoliang Wang,Julian McAuley,Saayan Mitra*

Main category: cs.AI

TL;DR: 本文提出ThinkRouter，一种推理时的置信度感知路由机制，通过在低置信度时切换至离散token空间、高置信度时使用潜在空间，避免噪声传播并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 研究发现潜在推理效果不稳定：错误答案的思考轨迹比正确答案包含更少低置信度步骤，且低置信度替代方案的软嵌入会引入噪声，导致对不可靠轨迹的高置信度。

Method: ThinkRouter动态路由策略：当模型置信度低时，将思考路由至离散token空间生成显式推理链；当置信度高时，路由至潜在空间进行高效推理，从而规避高置信度噪声。

Result: 在STEM推理和代码基准测试中，相比显式CoT、随机路由和潜在推理基线，ThinkRouter平均提升Pass@1达19.70个百分点，生成长度减少最多15.55%。

Conclusion: ThinkRouter能有效校准显式CoT和潜在推理的错误，通过全局降低模型置信度加速结束思考token生成，在保持准确性的同时显著提升推理效率。

Abstract: Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter, an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT, random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1, while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning, and accelerates end-of-thinking token generation by globally lowering model confidence.

</details>


### [204] [Beyond Parameter Arithmetic: Sparse Complementary Fusion for Distribution-Aware Model Merging](https://arxiv.org/abs/2602.11717)
*Weihong Lin,Lin Sun,Qilong Shi,Aomufei Yuan,Yuxuan Tian,Zhengyang Wang,Guangxiang Zhao,Xiangzheng Zhang,Tong Yang*

Main category: cs.AI

TL;DR: 针对模型融合中的功能干扰问题，本文提出Sparse Complementary Fusion with reverse KL (SCF-RKL)框架，利用反向KL散度进行功能差异度量与稀疏参数选择，在24项多领域基准测试中显著优于现有方法，同时保持优秀的泛化能力与生成稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管模型融合无需重训即可整合专用模型能力，但现有方法基于参数空间的启发式策略会引入严重干扰，导致泛化性能下降及生成不稳定现象（如重复、不连贯输出），亟需解决功能层面的干扰控制问题。

Method: 提出SCF-RKL框架，核心创新在于：1）采用反向KL散度（reverse KL）量化模型间功能差异；2）通过模式寻找（mode-seeking）的稀疏化设计选择性融合互补参数；3）摒弃参数空间线性叠加假设，实现分布感知的参数更新。

Result: 在跨模型规模与架构（推理模型与指令微调模型）的广泛评估中，SCF-RKL在24个涵盖高级推理、通用推理与知识、指令遵循、安全性及视觉分类的基准上持续超越现有融合方法，并展现出更强的泛化能力与生成稳定性。

Conclusion: SCF-RKL通过显式控制功能干扰的稀疏互补融合机制，为权重空间模型融合提供了新范式，有效平衡了能力提升与稳定性保持，具有重要实践价值。

Abstract: Model merging has emerged as a promising paradigm for composing the capabilities of large language models by directly operating in weight space, enabling the integration of specialized models without costly retraining. However, existing merging methods largely rely on parameter-space heuristics, which often introduce severe interference, leading to degraded generalization and unstable generation behaviors such as repetition and incoherent outputs. In this work, we propose Sparse Complementary Fusion with reverse KL (SCF-RKL), a novel model merging framework that explicitly controls functional interference through sparse, distribution-aware updates. Instead of assuming linear additivity in parameter space, SCF-RKL measures the functional divergence between models using reverse Kullback-Leibler divergence and selectively incorporates complementary parameters. This mode-seeking, sparsity-inducing design effectively preserves stable representations while integrating new capabilities. We evaluate SCF-RKL across a wide range of model scales and architectures, covering both reasoning-focused and instruction-tuned models. Extensive experiments on 24 benchmarks spanning advanced reasoning, general reasoning and knowledge, instruction following, and safety demonstrate, vision classification that SCF-RKL consistently outperforms existing model merging methods while maintaining strong generalization and generation stability.

</details>


### [205] [AIR: Improving Agent Safety through Incident Response](https://arxiv.org/abs/2602.11749)
*Zibo Xiao,Jun Sun,Junjie Chen*

Main category: cs.AI

TL;DR: 本文提出AIR，首个LLM智能体应急响应框架，通过领域特定语言和语义检查实现事故检测、处置与根除，在三种智能体上成功率均超90%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体安全机制仅关注事前预防，缺乏对事故响应、控制与恢复能力，无法应对实际部署中不可避免的事故。

Method: 设计领域特定语言管理应急响应生命周期，嵌入智能体执行循环，实现基于环境语义检查的事故检测、工具驱动的处置动作执行，以及根除阶段的规则自动生成。

Result: 评估显示检测、处置、根除成功率均超90%；关键组件必要性得到验证；框架响应及时、开销适中；LLM生成规则效果逼近开发者编写规则。

Conclusion: 应急响应是提升智能体安全性的可行且必要的第一类机制，应作为LLM智能体安全的核心设计要素。

Abstract: Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering from incidents after they inevitably arise. In this work, we introduce AIR, the first incident response framework for LLM agent systems. AIR defines a domain-specific language for managing the incident response lifecycle autonomously in LLM agent systems, and integrates it into the agent's execution loop to (1) detect incidents via semantic checks grounded in the current environment state and recent context, (2) guide the agent to execute containment and recovery actions via its tools, and (3) synthesize guardrail rules during eradication to block similar incidents in future executions. We evaluate AIR on three representative agent types. Results show that AIR achieves detection, remediation, and eradication success rates all exceeding 90%. Extensive experiments further confirm the necessity of AIR's key design components, show the timeliness and moderate overhead of AIR, and demonstrate that LLM-generated rules can approach the effectiveness of developer-authored rules across domains. These results show that incident response is both feasible and essential as a first-class mechanism for improving agent safety.

</details>


### [206] [TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents](https://arxiv.org/abs/2602.11767)
*Aladin Djuhera,Swanand Ravindra Kadhe,Farhan Ahmed,Holger Boche*

Main category: cs.AI

TL;DR: 本文提出TSR（Trajectory-Search Rollouts）方法，通过将测试时缩放思想引入训练阶段，在强化学习轨迹生成过程中执行轻量级树搜索，利用任务反馈选择高质量动作，从而提升多轮智能体的学习效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的进展正推动基于多轮交互的强化学习训练，但多轮RL面临奖励稀疏延迟和环境随机性等挑战。朴素轨迹采样会导致利用不足和模式坍塌，现有方法难以平衡探索与利用。

Method: TSR在训练rollout阶段执行轻量级树搜索，通过任务特定反馈逐轮选择高得分动作构建优质轨迹。该方法保持原优化目标不变，兼容PPO和GRPO等优化器，实现了best-of-N、beam search和shallow lookahead三种搜索策略。

Result: 在Sokoban、FrozenLake和WebShop任务上，TSR配合PPO/GRPO可实现最高15%的性能提升，训练过程更稳定，仅需一次性增加训练计算开销。

Conclusion: TSR通过将搜索机制从推理时前移至训练rollout阶段，为多轮智能体学习提供了简单通用的增强机制，与现有框架和拒绝采样方法互补，是强化学习训练的有效新范式。

Abstract: Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this regime, naive trajectory sampling can hinder exploitation and induce mode collapse. We propose TSR (Trajectory-Search Rollouts), a training-time approach that repurposes test-time scaling ideas for improved per-turn rollout generation. TSR performs lightweight tree-style search to construct high-quality trajectories by selecting high-scoring actions at each turn using task-specific feedback. This improves rollout quality and stabilizes learning while leaving the underlying optimization objective unchanged, making TSR optimizer-agnostic. We instantiate TSR with best-of-N, beam, and shallow lookahead search, and pair it with PPO and GRPO, achieving up to 15% performance gains and more stable learning on Sokoban, FrozenLake, and WebShop tasks at a one-time increase in training compute. By moving search from inference time to the rollout stage of training, TSR provides a simple and general mechanism for stronger multi-turn agent learning, complementary to existing frameworks and rejection-sampling-style selection methods.

</details>


### [207] [How to Optimize Multispecies Set Predictions in Presence-Absence Modeling ?](https://arxiv.org/abs/2602.11771)
*Sébastien Gigot--Léandri,Gaétan Morand,Alexis Joly,François Munoz,David Mouillot,Christophe Botella,Maximilien Servajean*

Main category: cs.AI

TL;DR: 本文提出MaxExp与SSE两种SDM概率预测转二元分布的框架。MaxExp通过直接最大化评估指标选择最可能物种组合，无需校准数据且灵活；SSE基于期望物种丰富度预测。三案例研究表明，二者在类别不平衡和物种稀有条件下表现优于传统方法，提供了稳健可重复的工具。


<details>
  <summary>Details</summary>
Motivation: 物种分布模型（SDM）生成的概率预测需转为二元存在-缺失地图以用于生态推断和自然保护规划，但现有启发式二元化方法会严重扭曲物种流行率与群落组成估计。

Method: 提出MaxExp决策驱动框架，通过直接最大化选定评估指标选择最可能物种组合，无需校准数据且支持多评分标准；同时提出计算高效的Set Size Expectation（SSE）方法，基于期望物种丰富度预测组合。

Result: 基于三个跨越不同类群、物种数量和性能指标的案例研究，MaxExp在绝大多数情况下匹配或优于常用阈值与校准方法，在类别不平衡和高稀有度场景中优势显著；SSE提供简单且具竞争力的替代方案。

Conclusion: MaxExp与SSE共同为多物种SDM二元化提供了稳健、可重复的工具，解决了传统方法在稀有物种和类别不平衡场景中的局限性。

Abstract: Species distribution models (SDMs) commonly produce probabilistic occurrence predictions that must be converted into binary presence-absence maps for ecological inference and conservation planning. However, this binarization step is typically heuristic and can substantially distort estimates of species prevalence and community composition. We present MaxExp, a decision-driven binarization framework that selects the most probable species assemblage by directly maximizing a chosen evaluation metric. MaxExp requires no calibration data and is flexible across several scores. We also introduce the Set Size Expectation (SSE) method, a computationally efficient alternative that predicts assemblages based on expected species richness. Using three case studies spanning diverse taxa, species counts, and performance metrics, we show that MaxExp consistently matches or surpasses widely used thresholding and calibration methods, especially under strong class imbalance and high rarity. SSE offers a simpler yet competitive option. Together, these methods provide robust, reproducible tools for multispecies SDM binarization.

</details>


### [208] [RELATE: A Reinforcement Learning-Enhanced LLM Framework for Advertising Text Generation](https://arxiv.org/abs/2602.11780)
*Jinfang Wang,Jiajie Liu,Jianwei Wu,Ziqin Luo,Zhen Chen,Chunlei Li,Biao Han,Tao Deng,Yi Li,Shuanglong Li,Lin Liu*

Main category: cs.AI

TL;DR: 本文提出RELATE，一种基于强化学习的端到端广告文本生成框架，通过将性能和合规目标直接融入生成过程，统一了文本生成与目标对齐，解决了传统两阶段范式目标不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工业界广告文本系统采用两阶段范式（先生成候选文本，再与点击率等性能指标对齐），这种分离导致优化目标不一致、漏斗效率低下，限制了全局最优性。

Method: 提出RELATE框架，采用强化学习，通过策略学习将性能和合规目标直接集成到生成过程中；引入转化导向指标，与合规约束共同建模为多维奖励，使模型在政策约束下生成提升转化效果的高质量广告文本。

Result: 在大规模工业数据集上的广泛实验表明RELATE持续优于基线模型；在线上广告平台的实际部署中，在严格政策约束下实现了点击转化率（CTCVR）的统计显著提升。

Conclusion: RELATE框架通过端到端的方式统一生成与对齐，有效解决了目标不一致问题，实验和线上部署验证了其鲁棒性和实际有效性，为广告文本生成提供了新的解决方案。

Abstract: In online advertising, advertising text plays a critical role in attracting user engagement and driving advertiser value. Existing industrial systems typically follow a two-stage paradigm, where candidate texts are first generated and subsequently aligned with online performance metrics such as click-through rate(CTR). This separation often leads to misaligned optimization objectives and low funnel efficiency, limiting global optimality.
  To address these limitations, we propose RELATE, a reinforcement learning-based end-to-end framework that unifies generation and objective alignment within a single model. Instead of decoupling text generation from downstream metric alignment, RELATE integrates performance and compliance objectives directly into the generation process via policy learning. To better capture ultimate advertiser value beyond click-level signals, We incorporate conversion-oriented metrics into the objective and jointly model them with compliance constraints as multi-dimensional rewards, enabling the model to generate high-quality ad texts that improve conversion performance under policy constraints.
  Extensive experiments on large-scale industrial datasets demonstrate that RELATE consistently outperforms baselines. Furthermore, online deployment on a production advertising platform yields statistically significant improvements in click-through conversion rate(CTCVR) under strict policy constraints, validating the robustness and real-world effectiveness of the proposed framework.

</details>


### [209] [FlowMind: Execute-Summarize for Structured Workflow Generation from LLM Reasoning](https://arxiv.org/abs/2602.11782)
*Yihao Liu,Ziyun Zhang,Zile He,Huaqian Cai*

Main category: cs.AI

TL;DR: 针对LLM推理结果结构化困难的问题，本文提出Execute-Summarize框架，通过解耦任务执行与工作流构建，先执行后重建，显著提升工作流准确性。


<details>
  <summary>Details</summary>
Motivation: LLM虽能解决复杂任务，但将自由形式推理转化为结构化工作流存在挑战。现有方法在执行过程中同步构建工作流，易因过程干扰导致准确性下降。

Method: 提出Execute-Summarize(ES)框架：解耦任务执行与工作流构建两个阶段，模型先利用工具完成任务，再独立从执行轨迹中重建结构化工作流。

Result: 在FlowBench基准测试中，该方法在准确性和鲁棒性上显著优于现有方法，验证了解耦策略的有效性。

Conclusion: ES框架为将自由形式LLM推理可靠地转化为结构化工作流提供了有效范式，具有重要实践意义。

Abstract: LLMs can solve complex tasks through reasoning and tool use, but accurately translating these solutions into structured workflows remains challenging. We model workflows as sequences of tool use and reformulate the problem as designing a mechanism that can both solve tasks and reliably construct workflows. Prior approaches that build workflows during execution often suffer from inaccuracies due to interference between the two processes. We propose an Execute-Summarize(ES) framework that decouples task execution from workflow construction: the model first completes the task using available tools, then independently reconstructs a structured workflow from execution traces. This separation improves workflow accuracy and robustness. We introduce FlowBench and show through extensive experiments that our approach outperforms existing methods, providing a reliable paradigm for grounding free-form LLM reasoning into structured workflows.

</details>


### [210] [Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation](https://arxiv.org/abs/2602.11790)
*Lingyong Yan,Jiulong Wu,Dong Xie,Weixian Shi,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 本文提出LAVES，一种基于LLM的分层多智能体系统，用于从教育问题生成高质量教学视频。系统通过编排智能体协调解题、可视化、解说三个专业智能体，采用结构化可执行脚本和确定性编译，实现全自动视频生产，日产量超百万条，成本降低95%以上。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视频生成模型在需要严格逻辑和精确知识表示的场景（如教学视频）中表现有限，存在过程保真度低、制作成本高、可控性差等问题。为解决这些挑战，需要构建能够同时保证逐步推理正确性、教学连贯性、视觉语义忠实性和音画精准对齐的系统。

Method: LAVES采用分层多智能体架构：中央编排智能体协调解题智能体（严谨求解）、图示智能体（生成可执行可视化代码）和解说智能体（制作学习者导向脚本）。系统引入显式质量门控和迭代批判机制，对所有输出进行语义批判、规则约束和工具编译检查。采用结构化可执行视频脚本而非直接像素合成，通过模板驱动装配规则实现确定性编译。

Result: 在大规模部署中，LAVES实现日产量超过100万条视频，相比行业标准方法成本降低95%以上，同时保持高接受率。系统有效解决了先前方法的过程保真度低、生产成本低和可控性有限等缺陷。

Conclusion: LAVES成功将教育视频生成建模为多目标优化任务，通过分层多智能体协作和质量控制机制，实现了无需人工编辑的全自动端到端生产，在保证质量的同时达到了前所未有的生产规模和成本效益。

Abstract: Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.

</details>


### [211] [Detecting RLVR Training Data via Structural Convergence of Reasoning](https://arxiv.org/abs/2602.11792)
*Hongbo Zhang,Yue Yang,Jianhao Yan,Guangsheng Bao,Yue Zhang,Yue Zhang*

Main category: cs.AI

TL;DR: ...


<details>
  <summary>Details</summary>
Motivation: ...

Method: ...

Result: ...

Conclusion: ...

Abstract: Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-$k$NN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the $k$ smallest nearest-neighbor edit distances. Min-$k$NN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-$k$NN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.

</details>


### [212] [Hi-SAM: A Hierarchical Structure-Aware Multi-modal Framework for Large-Scale Recommendation](https://arxiv.org/abs/2602.11799)
*Pingjun Pan,Tingting Zhou,Peiyao Lu,Tingting Fei,Hongxiang Chen,Chuanjiang Luo*

Main category: cs.AI

TL;DR: 该论文提出Hi-SAM框架，用于解决多模态推荐中的语义纠缠和层次结构缺失问题。通过解耦语义标记器(DST)和层次化记忆锚定Transformer(HMAT)，在真实数据集上超越现有最优方法，并在百万用户级社交平台部署实现核心指标6.55%的提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临两个核心挑战：1）次优标记化：RQ-VAE等方法未解耦跨模态共享语义与模态特定细节，导致信息冗余或语义崩溃；2）架构-数据不匹配：标准Transformer将语义ID视为扁平序列，忽视用户-商品-标记的层次结构，商品展开为多标记会放大序列长度与噪声，使注意力偏向局部细节而非整体语义。

Method: 提出Hi-SAM框架，包含两个关键设计：1）解耦语义标记器(DST)：通过几何感知对齐统一模态，采用粗到细量化策略。共享码本提取共识语义，模态专属码本从残差中恢复细节，并通过互信息最小化实现解耦；2）层次化记忆锚定Transformer(HMAT)：通过层次化RoPE将位置编码拆分为商品间/商品内子空间，引入锚定标记压缩商品为紧凑记忆，保留当前商品细节，仅通过压缩摘要访问历史交互。

Result: 在真实世界数据集上实现稳定性能提升，冷启动场景优势显著。在服务百万用户的大型社交平台部署后，核心在线指标获得6.55%的绝对增益。

Conclusion: Hi-SAM通过显式解耦多模态语义和恢复交互层次结构，有效解决了现有方法的局限性，显著提升推荐性能，并在工业级应用中验证了其实际价值。

Abstract: Multi-modal recommendation has gained traction as items possess rich attributes like text and images. Semantic ID-based approaches effectively discretize this information into compact tokens. However, two challenges persist: (1) Suboptimal Tokenization: existing methods (e.g., RQ-VAE) lack disentanglement between shared cross-modal semantics and modality-specific details, causing redundancy or collapse; (2) Architecture-Data Mismatch: vanilla Transformers treat semantic IDs as flat streams, ignoring the hierarchy of user interactions, items, and tokens. Expanding items into multiple tokens amplifies length and noise, biasing attention toward local details over holistic semantics. We propose Hi-SAM, a Hierarchical Structure-Aware Multi-modal framework with two designs: (1) Disentangled Semantic Tokenizer (DST): unifies modalities via geometry-aware alignment and quantizes them via a coarse-to-fine strategy. Shared codebooks distill consensus while modality-specific ones recover nuances from residuals, enforced by mutual information minimization; (2) Hierarchical Memory-Anchor Transformer (HMAT): splits positional encoding into inter- and intra-item subspaces via Hierarchical RoPE to restore hierarchy. It inserts Anchor Tokens to condense items into compact memory, retaining details for the current item while accessing history only through compressed summaries. Experiments on real-world datasets show consistent improvements over SOTA baselines, especially in cold-start scenarios. Deployed on a large-scale social platform serving millions of users, Hi-SAM achieved a 6.55% gain in the core online metric.

</details>


### [213] [PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts](https://arxiv.org/abs/2602.11807)
*Lianjun Wu,Shengchen Zhu,Yuxuan Liu,Liuyu Kai,Xiaoduan Feng,Duomin Wang,Wenshuo Liu,Jingxuan Zhang,Kelvin Li,Bin Wang*

Main category: cs.AI

TL;DR: 针对高分辨率集合天气预报中潜在扩散模型可扩散性不足的问题，本文提出PuYun-LDM，通过3D-MAE提取天气状态演化特征作为条件，并结合变量感知的VA-MFM自适应频率建模策略，显著提升预报性能与计算效率。


<details>
  <summary>Details</summary>
Motivation: 气象场缺乏通用基础模型且多变量的光谱异质性导致现有方法在通道间施加相同频率正则化，造成强度不均。高分辨率（≤0.25°）下潜在扩散模型的diffusability受限，制约了其在天气预报中的应用。

Method: 提出两阶段框架：1）3D掩码自编码器预训练天气状态时空特征，作为扩散模型的附加条件；2）变量感知掩码频率建模，依据各变量的光谱能量分布自适应确定掩码阈值，实现差异化频率正则化。

Result: PuYun-LDM在短预报时效上优于ENS，长时效上性能相当。计算性能方面，单块NVIDIA H200 GPU可在5分钟内完成15天全球6小时分辨率预报，并支持并行化集合预报生成。

Conclusion: 该方法通过增强潜在空间可扩散性和处理变量异质性，为高分辨率集合天气预报提供了高效且性能优越的生成式建模方案。

Abstract: Latent diffusion models (LDMs) suffer from limited diffusability in high-resolution (<=0.25°) ensemble weather forecasting, where diffusability characterizes how easily a latent data distribution can be modeled by a diffusion process. Unlike natural image fields, meteorological fields lack task-agnostic foundation models and explicit semantic structures, making VFM-based regularization inapplicable. Moreover, existing frequency-based approaches impose identical spectral regularization across channels under a homogeneity assumption, which leads to uneven regularization strength under the inter-variable spectral heterogeneity in multivariate meteorological data. To address these challenges, we propose a 3D Masked AutoEncoder (3D-MAE) that encodes weather-state evolution features as an additional conditioning for the diffusion model, together with a Variable-Aware Masked Frequency Modeling (VA-MFM) strategy that adaptively selects thresholds based on the spectral energy distribution of each variable. Together, we propose PuYun-LDM, which enhances latent diffusability and achieves superior performance to ENS at short lead times while remaining comparable to ENS at longer horizons. PuYun-LDM generates a 15-day global forecast with a 6-hour temporal resolution in five minutes on a single NVIDIA H200 GPU, while ensemble forecasts can be efficiently produced in parallel.

</details>


### [214] [Predicting LLM Output Length via Entropy-Guided Representations](https://arxiv.org/abs/2602.11812)
*Huanyi Xie,Yubin Chen,Liangyu Wang,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: 针对大语言模型推理和强化学习采样中序列长度长尾分布导致的填充浪费问题，本文提出一种重用模型内部隐藏状态的轻量级长度预测框架，包含熵引导令牌池化和渐进长度预测两大组件，在自建的ForeLen基准上实现29.16%的MAE提升，并显著提高系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 序列长度的长尾分布导致批处理推理产生大量填充，造成计算资源浪费。现有静态长度预测方法依赖辅助模型，存在预测开销大、泛化能力弱以及在随机"一对多"采样场景下失效等问题，亟需高效轻量且通用的解决方案。

Method: 提出基于主模型内部隐藏状态的轻量级框架：1) 熵引导令牌池化(EGTP)：通过实时激活值和令牌熵实现高精度静态预测；2) 渐进长度预测(PLP)：在解码过程中动态估计剩余长度以应对随机生成。构建并开源包含长序列、思维链和强化学习数据的ForeLen基准测试。

Result: 在ForeLen基准测试中，EGTP方法达到当前最优精度，平均绝对误差(MAE)较最佳基线降低29.16%。将该框架与长度感知调度器结合，可实现显著的端到端吞吐量增益。

Conclusion: 本研究为高效大语言模型推理建立了新的技术与评估基准，通过内部状态重用策略实现了长度预测精度与计算效率的有效平衡，为优化LLM服务系统提供了实用方案。

Abstract: The long-tailed distribution of sequence lengths in LLM serving and reinforcement learning (RL) sampling causes significant computational waste due to excessive padding in batched inference. Existing methods rely on auxiliary models for static length prediction, but they incur high overhead, generalize poorly, and fail in stochastic "one-to-many" sampling scenarios. We introduce a lightweight framework that reuses the main model's internal hidden states for efficient length prediction. Our framework features two core components: 1) Entropy-Guided Token Pooling (EGTP), which uses on-the-fly activations and token entropy for highly accurate static prediction with negligible cost, and 2) Progressive Length Prediction (PLP), which dynamically estimates the remaining length at each decoding step to handle stochastic generation. To validate our approach, we build and release ForeLen, a comprehensive benchmark with long-sequence, Chain-of-Thought, and RL data. On ForeLen, EGTP achieves state-of-the-art accuracy, reducing MAE by 29.16\% over the best baseline. Integrating our methods with a length-aware scheduler yields significant end-to-end throughput gains. Our work provides a new technical and evaluation baseline for efficient LLM inference.

</details>


### [215] [Prototype Transformer: Towards Language Model Architectures Interpretable by Design](https://arxiv.org/abs/2602.11852)
*Yordan Yordanov,Matteo Forasassi,Bayar Menzat,Ruizhi Wang,Chang Qi,Markus Kaltenberger,Amine M'Charrak,Tommaso Salvatori,Thomas Lukasiewicz*

Main category: cs.AI

TL;DR: 提出原型Transformer（ProtoT），用原型向量替代自注意力，通过双向通信实现可解释推理，同时保持接近SOTA性能且计算复杂度线性扩展。


<details>
  <summary>Details</summary>
Motivation: 先进语言模型推理过程不透明，即便输出显式推理，其真实机制仍不可见，导致信任缺失、欺骗和幻觉风险，亟需可解释架构。

Method: 设计ProtoT架构，基于原型参数向量，实现输入序列与原型间的双向通信，自动捕获"可命名概念"，并聚合多时间尺度上下文信息。

Result: 实现线性序列长度扩展性（对比标准注意力的二次方），在文本生成和GLUE任务表现良好，具备与基线相当或更强的输入鲁棒性，同时提供可解释的推理路径。

Conclusion: ProtoT在接近SOTA性能的前提下实现设计层面的可解释性，为构建高性能可解释自回归语言模型提供了可行路径。

Abstract: While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) -- an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. "woman") during training. They provide the potential to interpret the model's reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability. In terms of computation scalability, ProtoT scales linearly with sequence length vs the quadratic scalability of SOTA self-attention transformers. Compared to baselines, ProtoT scales well with model and data size, and performs well on text generation and downstream tasks (GLUE). ProtoT exhibits robustness to input perturbations on par or better than some baselines, but differs from them by providing interpretable pathways showing how robustness and sensitivity arises. Reaching close to the performance of state-of-the-art architectures, ProtoT paves the way to creating well-performing autoregressive LMs interpretable by design.

</details>


### [216] [Talk2DM: Enabling Natural Language Querying and Commonsense Reasoning for Vehicle-Road-Cloud Integrated Dynamic Maps with Large Language Models](https://arxiv.org/abs/2602.11860)
*Lu Tao,Jinxuan Luo,Yousuke Watanabe,Zhengshu Zhou,Yuhuan Lu,Shen Ying,Pan Zhang,Fei Zhao,Hiroaki Takada*

Main category: cs.AI

TL;DR: 针对动态地图系统缺乏自然语言交互能力的缺陷，本研究提出VRCsim仿真框架、VRC-QA数据集及Talk2DM模块。Talk2DM采用链式提示机制融合人工规则与大型语言模型常识推理，基于Qwen3:8B等模型实现93%+的自然语言查询准确率，响应时间2-5秒，具备实用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有动态地图增强的自动驾驶系统虽已在日本落地，但缺失自然语言支持的人机接口，严重制约了人类与动态地图的交互效率。为此，亟需引入自然语言查询与常识推理能力，以突破交互瓶颈。

Method: 1) 构建VRCsim车路云协同感知仿真平台，生成流式协同感知数据；2) 基于VRCsim创建面向混合交通场景空间推理的VRC-QA问答数据集；3) 提出Talk2DM即插即用模块，创新性地设计链式提示机制，渐进式整合人工定义规则与大型语言模型的常识知识，实现自然语言查询与推理功能。

Result: VRC-QA实验验证表明，Talk2DM具备跨模型无缝切换能力且保持高准确率。模型规模与效率存在权衡：更大模型准确率更高但延迟显著。采用Qwen3:8B、Gemma3:27B和GPT-oss时，Talk2DM以2-5秒平均响应时间达成超过93%的自然语言查询准确率。

Conclusion: 本研究形成了一套完整的仿真-数据-交互技术体系，Talk2DM模块兼具高准确率与高效率，泛化能力突出。实验结果证实了其工程实用性，为车路云协同自动驾驶提供了有效的自然语言交互解决方案，应用前景广阔。

Abstract: Dynamic maps (DM) serve as the fundamental information infrastructure for vehicle-road-cloud (VRC) cooperative autonomous driving in China and Japan. By providing comprehensive traffic scene representations, DM overcome the limitations of standalone autonomous driving systems (ADS), such as physical occlusions. Although DM-enhanced ADS have been successfully deployed in real-world applications in Japan, existing DM systems still lack a natural-language-supported (NLS) human interface, which could substantially enhance human-DM interaction. To address this gap, this paper introduces VRCsim, a VRC cooperative perception (CP) simulation framework designed to generate streaming VRC-CP data. Based on VRCsim, we construct a question-answering data set, VRC-QA, focused on spatial querying and reasoning in mixed-traffic scenes. Building upon VRCsim and VRC-QA, we further propose Talk2DM, a plug-and-play module that extends VRC-DM systems with NLS querying and commonsense reasoning capabilities. Talk2DM is built upon a novel chain-of-prompt (CoP) mechanism that progressively integrates human-defined rules with the commonsense knowledge of large language models (LLMs). Experiments on VRC-QA show that Talk2DM can seamlessly switch across different LLMs while maintaining high NLS query accuracy, demonstrating strong generalization capability. Although larger models tend to achieve higher accuracy, they incur significant efficiency degradation. Our results reveal that Talk2DM, powered by Qwen3:8B, Gemma3:27B, and GPT-oss models, achieves over 93\% NLS query accuracy with an average response time of only 2-5 seconds, indicating strong practical potential.

</details>


### [217] [From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders](https://arxiv.org/abs/2602.11881)
*Yifan Luo,Yang Zhan,Jiedong Jiang,Tianyang Liu,Mingrui Wu,Zhennan Zhou,Bin Dong*

Main category: cs.AI

TL;DR: 针对现有稀疏自编码器孤立提取特征的问题，本文提出分层稀疏自编码器（HSAE），通过联合学习SAE系列及其特征父子关系，结合结构约束损失和随机特征扰动，从大语言模型中提取语义层次结构，同时保持重构能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器（SAE）虽能提取大语言模型的单义性特征，但特征识别相互孤立。广泛证据表明大语言模型捕捉了自然语言的内在层次结构，特别是'特征分裂'现象凸显了层次性需求。因此，需要一种能够显式捕获并建模特征间层级关系的方法。

Method: 提出分层稀疏自编码器（HSAE），核心是联合训练多个SAE并学习特征间的父子关系。引入两种新机制：1）结构约束损失，强化父子特征对齐；2）随机特征扰动机制，增强层次稳定性。通过在多种大语言模型和层的广泛实验验证方法有效性。

Result: 实验表明HSAE能一致性地恢复语义有意义的层次结构，获得定性案例和定量指标双重验证。同时，在不同字典规模下，HSAE保持了标准SAE的重构保真度和可解释性，未牺牲原有性能。

Conclusion: 本工作提供了强大且可扩展的工具，用于发现和分析嵌入在大语言模型表示中的多尺度概念结构，为理解模型内部表征层次性提供了新途径。

Abstract: Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of "feature splitting" in particular indicates that such structure is hierarchical. To capture this, we propose the Hierarchical Sparse Autoencoder (HSAE), which jointly learns a series of SAEs and the parent-child relationships between their features. HSAE strengthens the alignment between parent and child features through two novel mechanisms: a structural constraint loss and a random feature perturbation mechanism. Extensive experiments across various LLMs and layers demonstrate that HSAE consistently recovers semantically meaningful hierarchies, supported by both qualitative case studies and rigorous quantitative metrics. At the same time, HSAE preserves the reconstruction fidelity and interpretability of standard SAEs across different dictionary sizes. Our work provides a powerful, scalable tool for discovering and analyzing the multi-scale conceptual structures embedded in LLM representations.

</details>


### [218] [When Should LLMs Be Less Specific? Selective Abstraction for Reliable Long-Form Text Generation](https://arxiv.org/abs/2602.11908)
*Shani Goren,Ido Galil,Ran El-Yaniv*

Main category: cs.AI

TL;DR: 本文提出选择性抽象框架（SA），让大语言模型在不确定时通过降低内容细节而非完全拒绝回答来平衡准确性与信息量。原子级SA将回答分解为独立事实单元，用更概括但更可靠的表述替换低置信度单元，在FactScore和LongFact-Objects基准测试上将风险-覆盖曲线下面积提升高达27.73%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在事实性错误问题，简单的不确定性估计机制（低置信度时拒绝回答）在长文本场景过于局限，会丢弃有价值信息。现有方法缺乏在保留信息与提升可靠性之间的细粒度权衡机制。

Method: 1）形式化选择性风险与覆盖概念，建立选择性抽象的理论框架；2）提出原子级选择性抽象（Atom-wise SA），将回答分解为原子化事实声明，用更高置信度但更泛化的抽象替换不确定声明；3）开发端到端评估流程，以事实正确性为风险度量，以信息论方法量化信息保留程度。

Result: 在FactScore和LongFact-Objects基准测试中，原子级SA在六个开源模型上持续优于现有基线方法，相比声明删除方法，风险-覆盖曲线下面积（AURC）提升最高达27.73%，证明降低细节程度可在保持原意的同时提升准确性与可靠性。

Conclusion: 选择性抽象框架为LLM不确定性管理提供了新范式，通过特异性-可靠性的可控权衡，在保持信息量的同时有效降低事实错误风险，适用于高风险场景的长文本生成任务。

Abstract: LLMs are widely used, yet they remain prone to factual errors that erode user trust and limit adoption in high-risk settings. One approach to mitigate this risk is to equip models with uncertainty estimation mechanisms that abstain when confidence is low. However, this binary "all-or-nothing" approach is excessively restrictive in long-form settings, often discarding valuable information. We introduce Selective Abstraction (SA), a framework that enables LLMs to trade specificity for reliability by selectively reducing the detail of uncertain content. We first formalize SA through the lenses of selective risk and coverage. We then propose Atom-wise Selective Abstraction, a claim-level instantiation that decomposes responses into atomic claims (short, self-contained statements each expressing a single fact) and replaces uncertain atoms with higher confidence, less specific abstractions. To evaluate this framework, we develop a novel end-to-end pipeline for open-ended generation that instantiates risk as factual correctness and measures coverage using an information-theoretic measure of retained information. Across six open-source models on the FactScore and LongFact-Objects benchmarks, atom-wise SA consistently outperforms existing baselines, improving the area under the risk-coverage curve (AURC) by up to 27.73% over claim removal, demonstrating that reducing specificity can boost accuracy and reliability while preserving most of their original meaning.

</details>


### [219] [AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution](https://arxiv.org/abs/2602.11917)
*Taian Guo,Haiyang Shen,Junyu Luo,Binqi Chen,Hongjun Ding,Jinsheng Huang,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: 本文提出AlphaPROBE框架，通过将alpha因子挖掘重构为有向无环图(DAG)的导航问题，利用贝叶斯因子检索器和DAG感知因子生成器实现全局结构化的因子发现，在中国股市数据集上显著提升了预测精度、收益稳定性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化alpha因子挖掘方法分为解耦因子生成和迭代因子演化两种范式，但均缺乏全局结构视角，将因子池视为非结构化集合或碎片化链式结构，导致搜索冗余和多样性受限，阻碍了高效因子发现。

Method: AlphaPROBE框架将因子建模为节点、演化关系建模为边，构建动态互连的因子生态系统。核心包括：1)贝叶斯因子检索器，通过后验概率模型平衡开发与探索以识别高潜力种子；2)DAG感知因子生成器，利用因子完整祖先轨迹生成上下文感知的非冗余优化。

Result: 在中国三大股票市场数据集上对比8个基线方法的实验表明，AlphaPROBE在预测精度、收益稳定性和训练效率方面显著提升，验证了利用全局演化拓扑结构对高效鲁棒自动化alpha发现的关键作用。

Conclusion: 通过DAG建模实现的全局演化拓扑结构是自动化alpha发现的核心，AlphaPROBE框架成功解决了现有方法缺乏结构化视图的根本问题，为量化金融因子挖掘提供了新范式。

Abstract: Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-child refinements. However, both paradigms lack a global structural view, often treating factor pools as unstructured collections or fragmented chains, which leads to redundant search and limited diversity. To address these limitations, we introduce AlphaPROBE (Alpha Mining via Principled Retrieval and On-graph Biased Evolution), a framework that reframes alpha mining as the strategic navigation of a Directed Acyclic Graph (DAG). By modeling factors as nodes and evolutionary links as edges, AlphaPROBE treats the factor pool as a dynamic, interconnected ecosystem. The framework consists of two core components: a Bayesian Factor Retriever that identifies high-potential seeds by balancing exploitation and exploration through a posterior probability model, and a DAG-aware Factor Generator that leverages the full ancestral trace of factors to produce context-aware, nonredundant optimizations. Extensive experiments on three major Chinese stock market datasets against 8 competitive baselines demonstrate that AlphaPROBE significantly gains enhanced performance in predictive accuracy, return stability and training efficiency. Our results confirm that leveraging global evolutionary topology is essential for efficient and robust automated alpha discovery. We have open-sourced our implementation at https://github.com/gta0804/AlphaPROBE.

</details>


### [220] [MEME: Modeling the Evolutionary Modes of Financial Markets](https://arxiv.org/abs/2602.11918)
*Taian Guo,Haiyang Shen,Junyu Luo,Zhongshi Xing,Hanchun Lian,Jinsheng Huang,Binqi Chen,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: 本文提出MEME模型，创新性地以"逻辑导向"视角将金融市场建模为竞争性的投资叙事演化生态系统，通过多智能体提取、高斯混合建模和时序对齐机制捕捉市场共识演化，在2023-2025年中国股票池测试中全面超越7个SOTA基线


<details>
  <summary>Details</summary>
Motivation: 现有LLM量化方法局限于资产中心（个股预测）或市场中心（组合配置）范式，忽视驱动市场运动的内在逻辑推理。亟需从"逻辑导向"角度建模市场动态本质。

Method: 构建MEME框架：1）多智能体提取模块从噪声数据生成高质量投资论点；2）高斯混合模型在语义空间挖掘潜在共识；3）时序评估对齐机制追踪投资逻辑的生命周期与历史盈利性，优先选择持久市场智慧而非短暂异常。

Result: 在2023-2025年三个异构中国股票池的广泛实验中，MEME持续优于7个SOTA基线。消融实验、敏感性分析、生命周期案例与成本分析验证了其识别和适应市场共识演化的能力。

Conclusion: MEME通过建模演化逻辑为量化投资提供了新范式，代码已开源，能够有效捕捉市场深层共识并指导鲁棒的投资组合构建。

Abstract: LLMs have demonstrated significant potential in quantitative finance by processing vast unstructured data to emulate human-like analytical workflows. However, current LLM-based methods primarily follow either an Asset-Centric paradigm focused on individual stock prediction or a Market-Centric approach for portfolio allocation, often remaining agnostic to the underlying reasoning that drives market movements. In this paper, we propose a Logic-Oriented perspective, modeling the financial market as a dynamic, evolutionary ecosystem of competing investment narratives, termed Modes of Thought. To operationalize this view, we introduce MEME (Modeling the Evolutionary Modes of Financial Markets), designed to reconstruct market dynamics through the lens of evolving logics. MEME employs a multi-agent extraction module to transform noisy data into high-fidelity Investment Arguments and utilizes Gaussian Mixture Modeling to uncover latent consensus within a semantic space. To model semantic drift among different market conditions, we also implement a temporal evaluation and alignment mechanism to track the lifecycle and historical profitability of these modes. By prioritizing enduring market wisdom over transient anomalies, MEME ensures that portfolio construction is guided by robust reasoning. Extensive experiments on three heterogeneous Chinese stock pools from 2023 to 2025 demonstrate that MEME consistently outperforms seven SOTA baselines. Further ablation studies, sensitivity analysis, lifecycle case study and cost analysis validate MEME's capacity to identify and adapt to the evolving consensus of financial markets. Our implementation can be found at https://github.com/gta0804/MEME.

</details>


### [221] [CSEval: A Framework for Evaluating Clinical Semantics in Text-to-Image Generation](https://arxiv.org/abs/2602.12004)
*Robert Cronshaw,Konstantinos Vilouras,Junyu Yan,Yuning Du,Feng Chen,Steven McDonagh,Sotirios A. Tsaftaris*

Main category: cs.AI

TL;DR: 该研究针对医学文本生成图像技术缺乏临床语义评估的问题，提出基于语言模型的CSEval框架，通过评估生成图像与提示词间的临床语义对齐程度，为医疗AI应用提供安全可靠的评估方案。实验表明其能有效识别语义不一致且与专家判断高度相关。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像评估方法主要关注真实感和多样性，但医疗领域需要确保生成图像准确反映解剖位置、病理等关键临床语义，否则可能影响诊断、教育等应用的安全性与可靠性，因此亟需专门的临床语义评估工具。

Method: 设计Clinical Semantics Evaluator (CSEval)框架，利用语言模型从提示词中提取临床语义要素，并构建自动化流程评估生成图像对这些语义（如解剖结构、病理特征）的准确呈现程度。

Result: 实验证明CSEval能发现传统指标忽略的临床语义不一致问题，其评估结果与医学专家的人工评判具有显著相关性，验证了该方法在临床评估中的有效性。

Conclusion: CSEval为医疗生成模型提供了可扩展且临床意义明确的评估补充，有助于推动文本生成图像技术在医疗健康领域的安全应用。

Abstract: Text-to-image generation has been increasingly applied in medical domains for various purposes such as data augmentation and education. Evaluating the quality and clinical reliability of these generated images is essential. However, existing methods mainly assess image realism or diversity, while failing to capture whether the generated images reflect the intended clinical semantics, such as anatomical location and pathology. In this study, we propose the Clinical Semantics Evaluator (CSEval), a framework that leverages language models to assess clinical semantic alignment between the generated images and their conditioning prompts. Our experiments show that CSEval identifies semantic inconsistencies overlooked by other metrics and correlates with expert judgment. CSEval provides a scalable and clinically meaningful complement to existing evaluation methods, supporting the safe adoption of generative models in healthcare.

</details>


### [222] [InjectRBP: Steering Large Language Model Reasoning Behavior via Pattern Injection](https://arxiv.org/abs/2602.12013)
*Xiuping Wu,Zhao Yu,Yuxin Cheng,Ngai Wong,Liangjun Ke,Tapas Mishra,Konstantinos V. Katsikopoulos*

Main category: cs.AI

TL;DR: 该论文从行为模式视角系统研究LLM推理机制，发现模型对特定问题类型会自适应地呈现特定推理行为分布，且结构性注入这些模式能显著影响推理质量。据此提出无需参数更新的InjectCorrect和InjectRLOpt两种优化方法，分别通过模仿历史正确答案的行为模式和基于价值函数生成可靠性感知的行为注入物来引导推理，在多项推理任务上分别实现最高5.34%和8.67%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽尝试利用行为相关提示增强LLM推理能力，但这些设计仍停留在直观经验层面，缺乏对模型底层推理行为模式的系统性分析。为此，论文从行为模式角度深入探究推理行为的形成机制及其对推理质量的塑造作用，旨在突破直觉驱动的提示设计局限，建立可解释、可复用的推理行为引导框架。

Method: 研究首先通过实证观察发现，LLM在面对不同类型问题时展现出特定的适应性推理行为分布模式，且将这些模式结构性地注入模型可显著调控其推理过程和输出质量。基于此提出两种免训练优化方法：InjectCorrect通过提取模型自身历史正确答案中的行为模式作为引导信号；InjectRLOpt则从历史数据中学习行为模式价值函数，并采用创新的可靠性感知Softmax策略在推理时动态生成行为注入物，实现对推理过程的精准引导。

Result: 在多种推理任务上的实验结果表明，InjectCorrect和InjectRLOpt均可在不更新任何模型参数的前提下有效提升LLM推理性能。其中InjectCorrect平均提升5.34%，InjectRLOpt平均提升8.67%，证实了通过行为模式注入增强推理的有效性。该方法无需额外训练，具有即插即用的实用价值。

Conclusion: 该研究揭示了LLM推理行为模式的系统性特征及其可操控性，验证了基于行为模式注入的无参数优化范式在提升推理能力方面的显著潜力。研究不仅为理解模型推理机制提供了新视角，也为开发高效、低成本的推理增强技术开辟了新路径，对推动LLM在实际应用中的可靠推理具有重要实践意义。

Abstract: Reasoning can significantly enhance the performance of Large Language Models. While recent studies have exploited behavior-related prompts adjustment to enhance reasoning, these designs remain largely intuitive and lack a systematic analysis of the underlying behavioral patterns. Motivated by this, we investigate how models' reasoning behaviors shape reasoning from the perspective of behavioral patterns. We observe that models exhibit adaptive distributions of reasoning behaviors when responding to specific types of questions, and that structurally injecting these patterns can substantially influence the quality of the models' reasoning processes and outcomes. Building on these findings, we propose two optimization methods that require no parameter updates: InjectCorrect and InjectRLOpt. InjectCorrect guides the model by imitating behavioral patterns derived from its own past correct answers. InjectRLOpt learns a value function from historical behavior-pattern data and, via our proposed Reliability-Aware Softmax Policy, generates behavioral injectant during inference to steer the reasoning process. Our experiments demonstrate that both methods can improve model performance across various reasoning tasks without requiring any modifications to model parameters, achieving gains of up to 5.34% and 8.67%, respectively.

</details>


### [223] [Multi UAVs Preflight Planning in a Shared and Dynamic Airspace](https://arxiv.org/abs/2602.12055)
*Amath Sow,Mauricio Rodriguez Cesen,Fabiola Martins Campos de Oliveira,Mariusz Wzorek,Daniel de Leng,Mattias Tiger,Fredrik Heintz,Christian Esteve Rothenberg*

Main category: cs.AI

TL;DR: 提出DTAPP-IICR框架，用于大规模无人机机队在动态共享空域中的飞行前规划。该方法通过优先级排序、新型4D单智能体规划器SFIPP-ST、迭代冲突解决和方向剪枝技术，实现了千架级无人机的近100%成功规划，运行时间减少50%，在城市级真实场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大规模无人机机队在动态共享空域中的飞行前规划面临临时禁飞区、异构无人机配置和严格交付期限等挑战。现有多智能体路径规划方法在可扩展性和灵活性方面难以满足实际无人交通管理需求，特别是在城市密集空域中。

Method: 提出DTAPP-IICR框架：(1)按紧急程度对任务进行优先级排序生成初始解；(2)使用SFIPP-ST（安全飞行间隔路径规划与软时间约束）4D单智能体规划器计算往返轨迹，处理异构无人机、强制临时禁飞区约束，并将智能体间冲突建模为软约束；(3)基于几何冲突图的迭代大规模邻域搜索解决剩余冲突；(4)采用保完备性的方向剪枝技术加速三维搜索。

Result: 在包含临时禁飞区的基准测试中，DTAPP-IICR对多达1000架无人机的机队实现了近100%的成功率，剪枝技术带来高达50%的运行时间减少，在无人交通管理场景下优于批量增强型基于冲突的搜索。该方法能在城市级真实运营中成功扩展，而其他基于优先级的方法在中等规模部署时已失效。

Conclusion: DTAPP-IICR为密集动态城市空域中的飞行前规划提供了一种实用且可扩展的解决方案，具备处理大规模异构无人机机队和复杂空域约束的能力，是迈向实际无人交通管理系统的重要进展。

Abstract: Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework, existing methods often lack the scalability and flexibility required for real-world Unmanned Traffic Management (UTM). We propose DTAPP-IICR: a Delivery-Time Aware Prioritized Planning method with Incremental and Iterative Conflict Resolution. Our framework first generates an initial solution by prioritizing missions based on urgency. Secondly, it computes roundtrip trajectories using SFIPP-ST, a novel 4D single-agent planner (Safe Flight Interval Path Planning with Soft and Temporal Constraints). SFIPP-ST handles heterogeneous UAVs, strictly enforces temporal NFZs, and models inter-agent conflicts as soft constraints. Subsequently, an iterative Large Neighborhood Search, guided by a geometric conflict graph, efficiently resolves any residual conflicts. A completeness-preserving directional pruning technique further accelerates the 3D search. On benchmarks with temporal NFZs, DTAPP-IICR achieves near-100% success with fleets of up to 1,000 UAVs and gains up to 50% runtime reduction from pruning, outperforming batch Enhanced Conflict-Based Search in the UTM context. Scaling successfully in realistic city-scale operations where other priority-based methods fail even at moderate deployments, DTAPP-IICR is positioned as a practical and scalable solution for preflight planning in dense, dynamic urban airspace.

</details>


### [224] [LawThinker: A Deep Research Legal Agent in Dynamic Environments](https://arxiv.org/abs/2602.12056)
*Xinyu Yang,Chenlong Deng,Tongyu Wen,Binyu Xie,Zhicheng Dou*

Main category: cs.AI

TL;DR: 该论文提出LawThinker，一个采用"探索-验证-记忆"策略的自主法律研究智能体，通过DeepVerifier模块对每一步推理进行三维验证，有效防止错误传播，在动态法律基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理方法缺乏对中间推理步骤的验证机制，导致不适用法条引用等错误会在推理链中无检测地传播，无法满足法律推理对程序合规性的严格要求。

Method: 提出LawThinker智能体，采用Explore-Verify-Memorize策略：1）DeepVerifier模块从知识准确性、事实-法律相关性和程序合规性三个维度验证每个检索结果；2）记忆模块实现长周期任务中的跨轮次知识复用；3）在动态司法环境中实施原子化验证操作。

Result: 在动态基准J1-EVAL上，相比直接推理方法提升24%，比基于工作流的方法提升11%，在过程导向指标上提升尤其显著；在三个静态基准上验证了泛化能力。

Conclusion: LawThinker通过强制每步验证机制有效解决了法律推理中的错误传播问题，为动态司法环境下的自动化法律推理提供了可靠框架，代码已开源。

Abstract: Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .

</details>


### [225] [The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context](https://arxiv.org/abs/2602.12108)
*Xiaoyuan Liu,Tian Liang,Dongyang Ma,Deyu Zhou,Haitao Mi,Pinjia He,Yan Wang*

Main category: cs.AI

TL;DR: StateLM通过引入内部推理循环和记忆工具套件（包括上下文剪枝、文档索引和笔记），使大语言模型能够主动管理自身状态并动态构建上下文，从而在长文档问答、对话记忆和深度研究任务中显著超越标准大语言模型。


<details>
  <summary>Details</summary>
Motivation: 尽管现有AI系统已具备成熟的数据库和检索机制（类比《哈利·波特》中的冥想盆），但模型缺乏主动操作这些系统的能力，只能被动接受人工设计的上下文，如同"没有魔杖的邓布利多"。本研究的动机是为模型赋予自主管理记忆的能力。

Method: 提出StateLM，一种新型基础模型，具备内部推理循环以管理自身状态。模型配备上下文剪枝、文档索引和笔记等记忆工具，并通过训练使其能够主动管理这些工具，实现动态上下文工程，从而突破固定窗口的架构限制。

Result: 实验结果表明：在长文档问答任务中，StateLM在所有模型规模上均持续优于标准大语言模型；在对话记忆任务中，绝对准确率提升10%-20%；在深度研究任务BrowseComp-Plus上，StateLM准确率最高可达52%，而标准大语言模型仅约5%。

Conclusion: 该方法将大语言模型从被动预测器转变为状态感知智能体，使推理过程成为有状态且可管理的过程，实现了从静态架构到动态状态管理的根本性转变。

Abstract: In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the "wand" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.

</details>


### [226] [Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty](https://arxiv.org/abs/2602.12113)
*Zewei Yu,Lirong Gao,Yuke Zhu,Bo Zheng,Sheng Guo,Haobo Wang,Junbo Zhao*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）通过测试时扩展在复杂推理任务中表现优异，但会生成包含过度反思（如重复自问和循环推理）的冗长思维链，导致高token消耗、计算开销和延迟，尤其在小模型中更为明显。本文提出自适应反思与长度协同惩罚（ARLCP）强化学习框架，动态平衡推理效率与准确率。在五个数学推理基准测试中，ARLCP使1.5B模型响应长度减少53.1%且准确率提升5.8%，7B模型长度减少35.0%且准确率提升2.7%。


<details>
  <summary>Details</summary>
Motivation: LRMs在测试时扩展中常生成过度冗长的思维链，包含大量不必要反思（如重复自我提问和循环推理），导致高token消耗、计算开销和延迟，却未提升准确率。随着问题复杂度增加，过度反思现象加剧，反而降低准确率并增加token开销，这一问题在小模型中尤为突出。

Method: 提出自适应反思与长度协同惩罚（ARLCP）强化学习框架，包含两大创新：(1) 自适应反思惩罚，在保留必要推理的同时动态抑制不必要反思步骤；(2) 基于问题复杂度估计的长度惩罚。通过协调两种惩罚机制，引导模型生成更简洁有效的推理路径。

Result: 在DeepSeek-R1-Distill-Qwen-1.5B和-7B模型上的五个数学推理基准测试表明：ARLCP使1.5B模型平均响应长度减少53.1%，准确率提升5.8%；7B模型长度减少35.0%，准确率提升2.7%。该方法相比现有方法实现了更优的效率-准确率权衡。

Conclusion: ARLCP框架有效解决了LRMs的过度反思与冗长推理问题，显著提升了推理效率并在小模型上实现了准确率增益，为小型推理模型的实用化部署提供了新方案。代码已开源。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity induces more excessive and unnecessary reflection, which in turn reduces accuracy and increases token overhead. To address this challenge, we propose Adaptive Reflection and Length Coordinated Penalty (ARLCP), a novel reinforcement learning framework designed to dynamically balance reasoning efficiency and solution accuracy. ARLCP introduces two key innovations: (1) a reflection penalty that adaptively curtails unnecessary reflective steps while preserving essential reasoning, and (2) a length penalty calibrated to the estimated complexity of the problem. By coordinating these penalties, ARLCP encourages the model to generate more concise and effective reasoning paths. We evaluate our method on five mathematical reasoning benchmarks using DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models. Experimental results show that ARLCP achieves a superior efficiency-accuracy trade-off compared to existing approaches. For the 1.5B model, it reduces the average response length by 53.1% while simultaneously improving accuracy by 5.8%. For the 7B model, it achieves a 35.0% reduction in length with a 2.7% accuracy gain. The code is released at https://github.com/ZeweiYu1/ARLCP .

</details>


### [227] [HLA: Hadamard Linear Attention](https://arxiv.org/abs/2602.12128)
*Hanno Ackermann,Hong Cai,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.AI

TL;DR: 本文提出Hadamard线性注意力(HLA)，突破性地将非线性变换置于成对相似度计算之后，实现了对softmax的高阶有理函数逼近。该方法在保持线性注意力计算效率的同时提升了近似精度，无需张量重塑，并成功应用于海量token的视频生成扩散Transformer。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer注意力具有二次方计算复杂度，难以处理长序列。线性注意力虽能降低计算成本，但其核函数在计算成对相似度前独立作用于输入，仅能实现低阶softmax逼近，精度不足。亟需一种更精确的线性注意力近似方法。

Method: 提出Hadamard线性注意力(HLA)，将非线性函数作用于查询与键的成对相似度之后（与标准softmax注意力一致），而非分别作用于查询和键之前。该设计使softmax逼近升级为高阶有理函数，并推导出与标准线性注意力类似的高效计算流程，避免耗时的张量重塑操作。

Result: 在涉及海量token的大规模视频生成扩散Transformer模型上验证了HLA的有效性，证明其在处理超长序列时的优越性能。

Conclusion: HLA通过重新设计非线性变换的位置，在保持线性计算复杂度的同时显著提升了softmax近似质量，为大规模序列建模提供了更高效的注意力机制，具有广泛的适用前景。

Abstract: The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions that are applied independently to the inputs before the pairwise similarities are calculated. That allows for an efficient computational procedure which, however, amounts to a low-degree rational function approximating softmax.
  We propose Hadamard Linear Attention (HLA). Unlike previous works on linear attention, the nonlinearity in HLA is not applied separately to queries and keys, but, analogously to standard softmax attention, after the pairwise similarities have been computed. It will be shown that the proposed nonlinearity amounts to a higher-degree rational function to approximate softmax. An efficient computational scheme for the proposed method is derived that is similar to that of standard linear attention. In contrast to other approaches, no time-consuming tensor reshaping is necessary to apply the proposed algorithm. The effectiveness of the approach is demonstrated by applying it to a large diffusion transformer model for video generation, an application that involves very large amounts of tokens.

</details>


### [228] [Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini Flash 2.5 Image and GPT Image 1.5](https://arxiv.org/abs/2602.12133)
*Roberto Balestri*

Main category: cs.AI

TL;DR: 这项研究审计了两款主流图像生成器的性别与肤色偏见，发现"中性"提示词产生高度极化输出：两款模型超96%为白人形象，但性别偏向相反(Gemini偏好女性，GPT偏好男性且肤色更浅)，揭示了AI视觉文化中的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 检验一个核心假设：中性提示词是否会产生人口统计学上中立的图像输出。研究旨在揭示算法视觉文化中的潜在偏见，挑战"无标记语言自然产生包容性表征"的社会语言学假设，为AI公平性评估提供实证基础。

Method: 采用量化审计方法：使用四个语义中立提示词生成3,200张写实图像；建立严谨分析流程，包括混合色彩归一化、面部landmark掩码和基于Monk(MST)、PERLA、Fitzpatrick量表的感知均匀肤色量化；运用照明感知比色法技术区分美学渲染与真实色素沉着。

Result: 关键发现：1)两款模型均呈现极强"默认白人"偏见(>96%输出为白人)；2)性别取向显著分歧：Gemini偏向生成女性形象，GPT偏向生成男性形象且肤色更浅；3)中性提示词实际充当模型偏见的诊断探针而非真正中立指令；4)构建了可复用的算法视觉文化审计框架。

Conclusion: 研究证实中性提示词无法消除算法偏见，反而暴露了模型内在的系统性偏差。通过区分表面美学与底层色素沉着，提供了大规模比较审计的方法论框架，挑战了当前对语言标记性与表征包容性的认知，为构建更公平的生成式AI系统提供了实证依据和评估工具。

Abstract: This study quantifies gender and skin-tone bias in two widely deployed commercial image generators - Gemini Flash 2.5 Image (NanoBanana) and GPT Image 1.5 - to test the assumption that neutral prompts yield demographically neutral outputs. We generated 3,200 photorealistic images using four semantically neutral prompts. The analysis employed a rigorous pipeline combining hybrid color normalization, facial landmark masking, and perceptually uniform skin tone quantification using the Monk (MST), PERLA, and Fitzpatrick scales. Neutral prompts produced highly polarized defaults. Both models exhibited a strong "default white" bias (>96% of outputs). However, they diverged sharply on gender: Gemini favored female-presenting subjects, while GPT favored male-presenting subjects with lighter skin tones. This research provides a large-scale, comparative audit of state-of-the-art models using an illumination-aware colorimetric methodology, distinguishing aesthetic rendering from underlying pigmentation in synthetic imagery. The study demonstrates that neutral prompts function as diagnostic probes rather than neutral instructions. It offers a robust framework for auditing algorithmic visual culture and challenges the sociolinguistic assumption that unmarked language results in inclusive representation.

</details>


### [229] [Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment](https://arxiv.org/abs/2602.12134)
*Jiajun Chen,Hua Shen*

Main category: cs.AI

TL;DR: 本文提出价值对齐税(VAT)框架，量化对齐干预在相互关联价值体系中的传播效应，揭示传统目标导向评估所忽略的系统性风险与动态特征。


<details>
  <summary>Details</summary>
Motivation: 现有价值对齐研究多采用静态视角，仅关注目标价值观的达成度，忽视了提示工程、微调、偏好优化等干预措施对整体价值系统的非线性重塑及其连锁反应。

Method: 基于施瓦茨价值理论构建受控情景-行为数据集，通过采集配对的前后规范判断数据，系统分析不同模型架构、价值维度及对齐策略下的价值共变模式。

Result: 实验发现对齐干预常引发价值观间非对称、结构化的协同变化，这些系统性效应在传统的单一目标价值评估框架下完全不可见。

Conclusion: VAT框架揭示了对齐过程层面的系统性风险，为理解大语言模型价值对齐的动态演化机制提供了新理论视角与评估工具。

Abstract: Existing work on value alignment typically characterizes value relations statically, ignoring how interventions - such as prompting, fine-tuning, or preference optimization - reshape the broader value system. We introduce the Value Alignment Tax (VAT), a framework that measures how alignment-induced changes propagate across interconnected values relative to achieved on-target gain. VAT captures the dynamics of value expression under alignment pressure. Using a controlled scenario-action dataset grounded in Schwartz value theory, we collect paired pre-post normative judgments and analyze alignment effects across models, values, and alignment strategies. Our results show that alignment often produces uneven, structured co-movement among values. These effects are invisible under conventional target-only evaluation, revealing systemic, process-level alignment risks and offering new insights into the dynamics of value alignment in LLMs.

</details>


### [230] [Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning](https://arxiv.org/abs/2602.12146)
*Mahdi Khodabandeh,Ghazal Shabani,Arash Yousefi Jordehi,Seyed Abolghasem Mirroshandel*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习与T5语言模型的新型无损压缩方法，通过将数据压缩为token序列而非传统向量表示，在保持语义完整性的同时显著提升压缩率。


<details>
  <summary>Details</summary>
Motivation: 传统字典与统计压缩方法难以充分挖掘复杂数据格式的结构冗余；现有深度学习压缩依赖密集向量表示，破坏了底层token结构。需开发新方法以同时优化压缩效率与结构保真度。

Method: 采用T5语言模型架构，应用离线强化学习算法优化token序列长度；将信息编码为离散token序列而非连续潜空间，保留原始数据格式结构；无需外部语法或世界知识，实现自适应压缩。

Result: 相比传统压缩方法获得显著更高的压缩比，同时保持数据语义完整性。

Conclusion: 该方法构建了基于强化学习的高效自适应压缩系统，利用语言模型潜在信息而不依赖显式内容理解，为各类应用场景提供了更鲁棒、实用的压缩解决方案。

Abstract: Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structure. To address these limitations, we propose a novel lossless compression method that leverages Reinforcement Learning applied to a T5 language model architecture. This approach enables the compression of data into sequences of tokens rather than traditional vector representations. Unlike auto-encoders, which typically encode information into continuous latent spaces, our method preserves the token-based structure, aligning more closely with the original data format. This preservation allows for higher compression ratios while maintaining semantic integrity. By training the model using an off-policy Reinforcement Learning algorithm, we optimize sequence length to minimize redundancy and enhance compression efficiency. Our method introduces an efficient and adaptive data compression system built upon advanced Reinforcement Learning techniques, functioning independently of external grammatical or world knowledge. This approach shows significant improvements in compression ratios compared to conventional methods. By leveraging the latent information within language models, our system effectively compresses data without requiring explicit content understanding, paving the way for more robust and practical compression solutions across various applications.

</details>


### [231] [GPT-4o Lacks Core Features of Theory of Mind](https://arxiv.org/abs/2602.12150)
*John Muchovej,Amanda Royka,Shane Lee,Julian Jara-Ettinger*

Main category: cs.AI

TL;DR: 本研究质疑大型语言模型（LLMs）是否真正具备心智理论（ToM）。尽管传统基准测试显示LLMs在社会任务上表现优异，但新开发的认知导向评估框架揭示：LLMs虽能模拟简单ToM任务，却在逻辑等价任务上失败，且行为预测与心理状态推断一致性低。结论是LLMs的社会能力并非源于通用、一致的心智理论。


<details>
  <summary>Details</summary>
Motivation: 现有研究通过基准测试评估LLMs的心智理论能力，发现其在各类社会任务上表现成功。然而，这些评估未能检验ToM的核心理论构建——即心理状态与行为之间的因果模型。因此，本研究旨在采用认知科学grounded的定义，开发新评估框架，检验LLMs是否真正拥有连贯、领域通用且一致的心智理论，而非仅仅是表面模仿。

Method: 研究者基于认知科学中心智理论的定义，开发并测试了一种新的评估框架。该框架的核心是探测LLMs是否具备关于心理状态如何导致行为的连贯、领域通用且一致的因果模型（不要求模型符合人类式ToM）。通过让LLMs完成简单ToM范式和逻辑等价任务，并比较其行为预测与相应心理状态推断之间的一致性来评估。

Result: 实验发现：1）LLMs在简单ToM任务中能较好地模拟人类判断；2）但在逻辑等价任务上表现失败；3）其行为预测与心理状态推断之间一致性较低。这些结果表明LLMs的社会能力存在显著局限。

Conclusion: 研究结论指出，当前大型语言模型表现出的社会熟练度并非源于一个领域通用或一致的心智理论。这挑战了LLMs真正理解心理状态的观点，提示其社会智能可能主要基于统计模式匹配，而非基于因果心理模型的深层理解。

Abstract: Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of an domain-general or consistent ToM.

</details>


### [232] [Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision](https://arxiv.org/abs/2602.12164)
*Xiaohan He,Shiyang Feng,Songtao Huang,Lei Bai,Bin Wang,Bo Zhang*

Main category: cs.AI

TL;DR: 针对大语言模型在科学推理中的脆弱性，提出Sci-CoE两阶段自演化框架，通过稀疏监督建立验证基础，再经几何奖励机制实现无监督大规模自迭代，提升模型鲁棒性与多样性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具备卓越推理能力且协同演化范式在代码、数学等领域表现良好，但在科学推理任务中，模型仍因解法评估不可靠和验证策略多样性有限而表现脆弱。

Method: Sci-CoE采用两阶段设计：1) 第一阶段利用少量标注数据为验证器建立基础正确性判断锚点；2) 第二阶段引入融合共识、可靠性与多样性的几何奖励机制，驱动模型在无标签数据上进行大规模自迭代，实现从稀疏监督到无监督学习的过渡。

Result: 在多个通用科学基准测试中，Sci-CoE显著提升了复杂推理能力，展现出良好的可扩展性，并有助于构建更鲁棒、多样化的评估体系。

Conclusion: 该框架通过自演化机制有效增强了科学推理的可靠性与多样性，为开发更强大的科学人工智能系统提供了新思路。

Abstract: Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.

</details>


### [233] [Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation](https://arxiv.org/abs/2602.12172)
*Bowei He,Yankai Chen,Xiaokun Zhang,Linghe Kong,Philip S. Yu,Xue Liu,Chen Ma*

Main category: cs.AI

TL;DR: 本文提出IOA教育启发式知识蒸馏框架，通过知识缺陷识别、递进课程组织和认知容量适配三阶段流程，结合布鲁姆掌握学习与维果茨基最近发展区理论，实现小模型在保留94.7%教师性能的同时，在MATH和HumanEval等复杂推理任务上分别取得19.2%和22.3%的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM知识蒸馏方法缺乏"教学意识"，将知识迁移简化为一次性数据合成与训练，忽视系统化学习过程，导致蒸馏效率低下，尤其在复杂推理任务上性能损失严重。

Method: 提出三阶段IOA框架：知识标识符诊断学生模型知识缺陷；知识组织者设计递进式课程体系；知识适配器根据学生认知容量调整知识表示。融合布鲁姆掌握学习原则与维果茨基最近发展区理论，构建动态蒸馏机制，确保学生掌握先决知识后进阶，并以可控难度梯度引入新知识。

Result: 基于LLaMA-3.1/3.2和Qwen2.5的实验显示：IOA框架仅用不足1/10参数量即在DollyEval上保留94.7%教师性能；在MATH数学推理任务上超越SOTA基线19.2%；在HumanEval代码生成任务上提升22.3%，在复杂推理任务上表现尤为突出。

Conclusion: 该研究验证了教育学理论指导知识蒸馏的有效性，提出的IOA框架为小模型高效部署提供了系统化、动态化新范式，显著提升了知识迁移效率，特别是在复杂认知任务上展现出独特优势。

Abstract: Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline -- Knowledge Identifier, Organizer, and Adapter (IOA) -- that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom's Mastery Learning Principles and Vygotsky's Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model's performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1/3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1/10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.

</details>


### [234] [SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation](https://arxiv.org/abs/2602.12173)
*Chengxi Zeng,Yuxuan Jiang,Ge Gao,Shuai Wang,Duolikun Danier,Bin Zhu,Stevan Rudinac,David Bull,Fan Zhang*

Main category: cs.AI

TL;DR: 针对视觉语言分割模型中文本编码器过度配置的问题，本文通过分析40余万条真实提示词发现严重冗余现象，并提出SAM3-LiteText框架，使用知识蒸馏优化的轻量级MobileCLIP学生模型替换原有文本编码器，实现参数量减少88%的同时保持分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言分割模型（如SAM3）采用为开放式语言理解设计的大型通用文本编码器，但实际应用中的分割提示词简短、结构化且语义受限，导致文本编码器容量严重过剩，产生持续的计算和内存开销。

Method: 首先对404,796条真实提示词进行大规模结构分析，发现上下文窗口利用率低、词汇使用稀疏、文本嵌入位于低维流形等冗余现象；然后提出SAM3-LiteText框架，通过知识蒸馏将原始SAM3文本编码器的知识迁移到紧凑的MobileCLIP学生模型中。

Result: 在图像和视频分割基准测试上，SAM3-LiteText将文本编码器参数量减少高达88%，显著降低了静态内存占用，同时分割性能与原始模型相当。

Conclusion: 针对视觉语言分割任务，轻量级文本编码器完全可以替代大型通用文本编码器，在保证性能的前提下大幅降低模型复杂度和资源消耗。

Abstract: Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.

</details>


### [235] ["Sorry, I Didn't Catch That": How Speech Models Miss What Matters Most](https://arxiv.org/abs/2602.12249)
*Kaitlyn Zhou,Martijn Bartelds,Federico Bianchi,James Zou*

Main category: cs.AI

TL;DR: 本研究针对语音识别系统在实际应用中转录高风险短语音（美国街道名称）表现不佳的问题，测试了15个主流模型在语言多样说话人上的平均错误率达44%，且对非英语母语者的影响是英语母语者的两倍。通过开源语音合成生成合成数据微调，可相对提升非英语母语者转录准确率近60%，揭示了基准性能与真实可靠性的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 尽管语音识别系统在标准基准测试中词错误率很低，但在现实世界部署中面对高风险短语音时却频繁失败。美国街道名称转录作为典型的高风险任务，其转录错误会导致导航偏差等严重后果。现有研究缺乏对语言多样性说话人群体系统差异的量化评估，以及针对命名实体转录失败的有效缓解方案。

Method: 研究分为评估与缓解两部分：评估方面，收集美国语言多样说话人的街道名称录音，测试OpenAI、Deepgram、Google和Microsoft的15个商业模型，计算转录错误率并分析地理下游影响及路由距离误差；缓解方面，提出开源文本转语音合成数据生成方法，创建命名实体多样发音样本，用少于1000个合成样本对模型进行微调。

Result: 评估发现平均转录错误率高达44%，所有模型均存在系统性错误，且非英语母语者的路由距离误差是英语母语者的两倍。通过合成数据微调后，非英语母语者的街道名称转录准确率获得近60%的相对提升，显著缩小了性能差距。

Conclusion: 研究揭示了语音识别系统在高风险真实场景中性能与基准测试存在严重脱节，尤其对非英语母语者造成不成比例的损害。提出的合成数据生成方法是一种简单可扩展的解决方案，可大幅降低高风险评估错误，为构建更公平可靠的语音系统提供了实用路径。

Abstract: Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.

</details>


### [236] [Think like a Scientist: Physics-guided LLM Agent for Equation Discovery](https://arxiv.org/abs/2602.12259)
*Jianke Yang,Ohm Venkatachalam,Mohammad Kianezhad,Sharvaree Vadgama,Rose Yu*

Main category: cs.AI

TL;DR: 针对现有LLM方法直接从数据猜测方程而忽略科学推理过程的问题，本研究提出KeplerAgent——个显式模拟科学家多步推理（先推断物理对称性再约束方程空间）的智能体框架，在物理方程发现任务中实现了显著更高的符号准确性和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 符号方程发现是科学解释的核心目标，而当前基于LLM的方法缺乏对科学家典型推理流程（先推断物理性质作为先验，再约束候选方程）的建模，导致性能受限。

Method: KeplerAgent采用智能体架构，通过协调物理学工具提取对称性等中间结构，动态配置PySINDy和PySR符号回归引擎的函数库与结构约束，实现科学推理引导的方程发现。

Result: 在多个物理方程基准测试中，KeplerAgent相比LLM和传统基线在符号准确性上实现显著提升，同时对噪声数据表现出更强的鲁棒性。

Conclusion: 显式建模多步科学推理过程可大幅提升符号方程发现性能，KeplerAgent为该领域提供了有效的智能体范式。

Abstract: Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.

</details>
