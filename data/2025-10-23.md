<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 67]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 71]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Contextual Augmentation for Entity Linking using Large Language Models](https://arxiv.org/abs/2510.18888)
*Daniel Vollmers,Hamada M. Zahera,Diego Moussallem,Axel-Cyrille Ngonga Ngomo*

Main category: cs.CL

TL;DR: 一个微调后的实体链接模型，联合执行命名实体识别与消歧，并利用大语言模型丰富上下文，在域外数据上达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 传统的两步流程（NER 与消歧）成本高且效果受限，迫切需要一个能够端到端联合建模并通过丰富上下文提升消歧能力的方案，尤其在域外数据上的泛化性。

Method: 对一个模型进行微调，使其在一个统一框架中同时执行实体识别与消歧；并利用大语言模型来丰富实体提及的上下文信息，以提高消歧效果；在基准数据集上与多种基线方法进行对比评估。

Result: 在域外数据集上实现了状态-of-the-art的性能，优于多种基线方法，显示出在跨领域场景中的鲁棒性提升。

Conclusion: 端到端的联合建模结合LLM上下文增强能显著提升实体链接的效果，尤其在域外数据上的泛化能力有所增强。

Abstract: Entity Linking involves detecting and linking entity mentions in natural
language texts to a knowledge graph. Traditional methods use a two-step process
with separate models for entity recognition and disambiguation, which can be
computationally intensive and less effective. We propose a fine-tuned model
that jointly integrates entity recognition and disambiguation in a unified
framework. Furthermore, our approach leverages large language models to enrich
the context of entity mentions, yielding better performance in entity
disambiguation. We evaluated our approach on benchmark datasets and compared
with several baselines. The evaluation results show that our approach achieves
state-of-the-art performance on out-of-domain datasets.

</details>


### [2] [Small Language Models Offer Significant Potential for Science Community](https://arxiv.org/abs/2510.18890)
*Jian Zhang*

Main category: cs.CL

TL;DR: 使用 MiniLM 构建高效、低成本的地球科学文献信息检索与分析框架，替代大模型实现精准的语义检索、情感与主题分析，并追踪结论演变与研究趋势。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在文献检索中表现突出，但存在信息偏差和高计算成本；作者提出以自由获取的 MiniLM 实现可扩展、低成本的信息检索解决方案。

Method: 构建约7700万句、来自95份地球科学期刊（2000-2024）的高质量语料库；采用语义检索和句级索引进行信息提取；通过情感分析与无监督聚类分析句子中的情感与主题，以追踪结论与研究优先级的演变。

Result: 与如 ChatGPT-4 等大模型相比，MiniLM 能高效提取领域专有信息与量化发现，且源于已确立的跨学科来源；具备识别大量专家验证信息的能力，并可用于趋势与矛盾分析等应用。

Conclusion: MiniLM 在地质科学领域展示出显著潜力，适用于事实/图像检索、趋势分析、矛盾分析及教育等多种场景。

Abstract: Recent advancements in natural language processing, particularly with large
language models (LLMs), are transforming how scientists engage with the
literature. While the adoption of LLMs is increasing, concerns remain regarding
potential information biases and computational costs. Rather than LLMs, I
developed a framework to evaluate the feasibility of precise, rapid, and
cost-effective information retrieval from extensive geoscience literature using
freely available small language models (MiniLMs). A curated corpus of
approximately 77 million high-quality sentences, extracted from 95 leading
peer-reviewed geoscience journals such as Geophysical Research Letters and
Earth and Planetary Science Letters published during years 2000 to 2024, was
constructed. MiniLMs enable a computationally efficient approach for extracting
relevant domain-specific information from these corpora through semantic search
techniques and sentence-level indexing. This approach, unlike LLMs such as
ChatGPT-4 that often produces generalized responses, excels at identifying
substantial amounts of expert-verified information with established,
multi-disciplinary sources, especially for information with quantitative
findings. Furthermore, by analyzing emotional tone via sentiment analysis and
topical clusters through unsupervised clustering within sentences, MiniLM
provides a powerful tool for tracking the evolution of conclusions, research
priorities, advancements, and emerging questions within geoscience communities.
Overall, MiniLM holds significant potential within the geoscience community for
applications such as fact and image retrievals, trend analyses, contradiction
analyses, and educational purposes.

</details>


### [3] [DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code](https://arxiv.org/abs/2510.18904)
*Shriyansh Agrawal,Aidan Lau,Sanyam Shah,Ahan M R,Kevin Zhu,Sunishchal Dev,Vasu Sharma*

Main category: cs.CL

TL;DR: 通过对 RoBERTa 与 CodeBERTa 之类的编码器进行微调，使用专门的数据集对代码和自然语言进行二分类检测，显著提升检测准确性与跨域鲁棒性，同时大幅降低推理成本和显存需求，相比零-shot 检测器（如 Fast DetectGPT、GPTZero）展现出优越性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多语言文本与源码生成中的广泛应用，机器生成内容检测的需求日益迫切。然而现有检测器多采用零-shot 策略，存在计算成本高、准确性不足或两者难以兼得的问题，且在跨域和对抗性变换下的鲁棒性需要提升。

Method: 对 RoBERTa、CodeBERTa 这类预训练的编码器进行微调，使用专门的源码与自然语言数据集来进行二分类训练；输入长度设定为 512-token；在跨生成器偏移和对抗性变换如改述、回译、代码格式化/重命名等场景下评估鲁棒性；与零-shot 检测器（如 Fast DetectGPT、GPTZero）进行对比；并发布训练、评测脚本、种子与配置以确保可重复性。

Result: 编码器在二分类任务上实现 AUROC 0.97–0.99、macro-F1 0.89–0.94；相比大模型，计算成本显著降低，推理延迟降低 8–12 倍，峰值显存降低 3–5 倍（512 tokens 输入）；在跨生成器迁移与对抗变换下仍保持 ≥92% 的干净 AUROC；并提供可重复性列表与实现脚本。

Conclusion: 微调的小型编码器模型能够在高准确性与跨域鲁棒性之间取得良好权衡，并显著提升检测效率，适用于跨域的机器生成内容检测场景。未来工作可关注更大范围的数据源、对抗性攻击防御及实际落地集成。

Abstract: The prevalence of Large Language Models (LLMs) for generating multilingual
text and source code has only increased the imperative for machine-generated
content detectors to be accurate and efficient across domains. Current
detectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or
GPTZero, either incur high computational cost or lack sufficient accuracy,
often with a trade-off between the two, leaving room for further improvement.
To address these gaps, we propose the fine-tuning of encoder-only Small
Language Models (SLMs), in particular, the pre-trained models of RoBERTA and
CodeBERTa using specialized datasets on source code and other natural language
to prove that for the task of binary classification, SLMs outperform LLMs by a
huge margin whilst using a fraction of compute. Our encoders achieve AUROC $=
0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by
$8$-$12\times$ and peak VRAM by $3$-$5\times$ at $512$-token inputs. Under
cross-generator shifts and adversarial transformations (paraphrase,
back-translation; code formatting/renaming), performance retains $\geq 92%$ of
clean AUROC. We release training and evaluation scripts with seeds and configs;
a reproducibility checklist is also included.

</details>


### [4] [When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs](https://arxiv.org/abs/2510.18892)
*Richard J. Young,Brandon Gillins,Alice M. Matthews*

Main category: cs.CL

TL;DR: 提出一套20条提示的精简评测框架，快速诊断LLM的指令遵循能力，并在256个模型上进行大规模实证研究。


<details>
  <summary>Details</summary>
Motivation: 在现有基准资源丰富但评估成本高、且模型可能仅记忆化应答的背景下，需更快速、可重复且针对性强的评测工具以诊断指令遵循模式。

Method: 设计20个覆盖格式、内容约束、逻辑顺序和多步执行等方面的提示，验证基本功能后纳入分析；对OpenAI、Anthropic、Google、Meta、Mistral等厂商及Qwen等新兴实现进行对比评估；通过公开开放路由平台OpenRouter获取的模型集；进行严格的方法学以避免选择偏差。

Result: 该框架可在不需要大资源的情况下提供诊断性评估，揭示一致的失败模式和特定指令类型的挑战性，构成对当前LLM指令遵循能力的较全面的实证分析。

Conclusion: 提供一个实用的评估工具和对当代LLM指令遵循能力的广泛比较，但需在论文里提供更详尽的提示清单、评分标准、可重复性细节，以及不同任务域的外部有效性等。

Abstract: Despite widespread deployment of Large Language Models, systematic evaluation
of instruction-following capabilities remains challenging. While comprehensive
benchmarks exist, focused assessments that quickly diagnose specific
instruction adherence patterns are valuable. As newer models may be trained on
existing benchmarks, novel evaluation approaches are needed to assess genuine
capabilities rather than memorized performance. This paper presents a
streamlined evaluation framework using twenty carefully designed prompts to
assess LLM instruction-following across diverse task categories. We demonstrate
this framework through a large-scale empirical study conducted on October 14,
2025, testing 256 verified working models from 331 available via OpenRouter. To
ensure methodological rigor and prevent selection bias, we first verified each
model's basic functionality before inclusion. Unlike large-scale benchmarks
requiring extensive computational resources, our approach offers a practical
diagnostic tool researchers and practitioners can readily apply. Our
methodology builds upon verifiable instructions while introducing a compact
test suite balancing comprehensiveness with efficiency. Each prompt targets
distinct aspects of instruction following, including format compliance, content
constraints, logical sequencing, and multi-step task execution. We evaluate
models from major providers (OpenAI, Anthropic, Google, Meta, Mistral) and
emerging implementations (Qwen, DeepSeek, community models), providing
comparative performance analysis. Our findings reveal consistent failure modes
and identify specific instruction types posing particular challenges. This work
contributes both a practical evaluation tool and one of the most comprehensive
empirical analyses of instruction-following capabilities across the
contemporary LLM landscape.

</details>


### [5] [The Massive Legal Embedding Benchmark (MLEB)](https://arxiv.org/abs/2510.19365)
*Umar Butler,Abdur-Rahman Butler,Adrian Lucas Malec*

Main category: cs.CL

TL;DR: MLEB 是一个开源的法律信息检索基准，包含十个专家注释数据集，覆盖多司法辖区与多种文档类型与任务，部分数据集为新构建，目标在于推动跨 Jurisdiction 的可复现实验。


<details>
  <summary>Details</summary>
Motivation: 当前的法律信息检索评测在规模、跨域和跨法域的一致性方面不足，MLBE 旨在填补这些空缺，提供大型、 diverse、可复现的评测资源，促进跨辖区比较与方法改进。

Method: 构建十个数据集，覆盖 US、UK、EU、澳大利亚、爱尔兰和新加坡的司法辖区；文档类型包括案例、立法、监管指南、合同与文献；任务类型涵盖检索、零-shot 分类和问答；其中七个数据集为新构建，用以填补领域和辖区空缺；对数据进行专家注释，并公开代码、结果与数据以实现可重复评估。

Result: 提出一个规模化、多域的开源评测基准（MLEB），提供数据集、标注方法与评测工具，并开放相关代码与数据以支持复现性研究。

Conclusion: MLEB 为法律信息检索领域提供了一个全面且可扩展的评测平台，促进跨辖区比较和方法改进，同时提升研究的可复现性。

Abstract: We present the Massive Legal Embedding Benchmark (MLEB), the largest, most
diverse, and most comprehensive open-source benchmark for legal information
retrieval to date. MLEB consists of ten expert-annotated datasets spanning
multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore),
document types (cases, legislation, regulatory guidance, contracts, and
literature), and task types (search, zero-shot classification, and question
answering). Seven of the datasets in MLEB were newly constructed in order to
fill domain and jurisdictional gaps in the open-source legal information
retrieval landscape. We document our methodology in building MLEB and creating
the new constituent datasets, and release our code, results, and data openly to
assist with reproducible evaluations.

</details>


### [6] [Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali to Sylheti](https://arxiv.org/abs/2510.18898)
*Mangsura Kabir Oni,Tabia Tanzin Prama*

Main category: cs.CL

TL;DR: 本研究比较了孟加拉语—西尔泰语的翻译：通过对多语言Transformer模型进行微调与零-shot大语言模型的比较，微调模型在低资源语言上显著优于LLMs，其中mBART-50在翻译的充足性方面表现最好，MarianMT在字符级保真度方面得分最高。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如西尔泰语）在机器翻译中的不足，探索针对特定任务的适应性微调与大语言模型在零-shot情景下的对比效果。

Method: 使用多语言Transformer模型对孟加拉语到西尔泰语进行微调，并与零-shot大语言模型进行对比评估；具体模型包括mBART-50与MarianMT等。

Result: 微调模型显著优于LLMs；mBART-50在翻译充足性方面取得最高表现；MarianMT在字符级保真度方面表现最强。

Conclusion: 针对低资源语言的任务特异性适应对提升翻译质量至关重要，研究有助于推动包容性语言技术的发展。

Abstract: Machine Translation (MT) has advanced from rule-based and statistical methods
to neural approaches based on the Transformer architecture. While these methods
have achieved impressive results for high-resource languages, low-resource
varieties such as Sylheti remain underexplored. In this work, we investigate
Bengali-to-Sylheti translation by fine-tuning multilingual Transformer models
and comparing them with zero-shot large language models (LLMs). Experimental
results demonstrate that fine-tuned models significantly outperform LLMs, with
mBART-50 achieving the highest translation adequacy and MarianMT showing the
strongest character-level fidelity. These findings highlight the importance of
task-specific adaptation for underrepresented languages and contribute to
ongoing efforts toward inclusive language technologies.

</details>


### [7] [ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers](https://arxiv.org/abs/2510.19791)
*Saptarshi Sengupta,Zhengyu Zhou,Jun Araki,Xingbo Wang,Bingqing Wang,Suhang Wang,Zhe Feng*

Main category: cs.CL

TL;DR: ToolDreamer通过生成合成的工具描述（TD）来引导检索器，从而在大规模工具集合下提升工具检索的准确性与可扩展性，缓解LLM上下文窗口的负担。


<details>
  <summary>Details</summary>
Motivation: 现有方法把工具按用户查询与工具描述之间的相似度排序，但用户请求的自然语言往往与TD语言不对齐，导致检索子opt。对于大工具集，需外部检索来提供相关工具，且传统方法难以覆盖查询多样性，限制了工具的有效使用。

Method: 提出ToolDreamer框架：利用LLM生成假设性（合成）的工具描述（synthetic Tool Descriptions）来条件化检索器，使其基于潜在有用的TD对工具进行检索，而非仅基于原始TD与查询的直接相似度。通过在ToolRet数据集上实验，验证该框架对稀疏与密集检索器在有/无训练情况下的性能提升，展示其对不同检索器架构的适用性。

Result: 实验结果表明，借助合成TD对检索器进行条件化后，稀疏和密集检索器均获得性能提升；框架对训练与非训练场景均有效，证明其对大规模工具集合的定位与检索能力的提升。

Conclusion: ToolDreamer通过在TD空间中引入合成描述来对检索器进行条件化，使LLM更高效地从海量工具中检索相关工具，缓解上下文窗口限制并提升整体对话能力与推理效率，具备很强的灵活性与扩展性。

Abstract: Tool calling has become increasingly popular for Large Language Models
(LLMs). However, for large tool sets, the resulting tokens would exceed the
LLM's context window limit, making it impossible to include every tool. Hence,
an external retriever is used to provide LLMs with the most relevant tools for
a query. Existing retrieval models rank tools based on the similarity between a
user query and a tool description (TD). This leads to suboptimal retrieval as
user requests are often poorly aligned with the language of TD. To remedy the
issue, we propose ToolDreamer, a framework to condition retriever models to
fetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,
description of tools that the LLM feels will be potentially useful for the
query. The framework enables a more natural alignment between queries and tools
within the language space of TD's. We apply ToolDreamer on the ToolRet dataset
and show that our method improves the performance of sparse and dense
retrievers with and without training, thus showcasing its flexibility. Through
our proposed framework, our aim is to offload a portion of the reasoning burden
to the retriever so that the LLM may effectively handle a large collection of
tools without inundating its context window.

</details>


### [8] [Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets](https://arxiv.org/abs/2510.18908)
*Wangjiaxuan Xin,Shuhua Yin,Shi Chen,Yaorong Ge*

Main category: cs.CL

TL;DR: TM-Rephrase 将推文重新表述为更正式的语言，以提升在主题模型中的效果，尤其对口语化文本的改进显著。


<details>
  <summary>Details</summary>
Motivation: 短文本、非正式语言和噪声阻碍了传统主题建模的效果，需要一个模型无关的前处理框架来提升可解释性和多样性。

Method: 利用大语言模型对COVID-19相关推文进行两种重述策略（一般化、口语化到正式化），然后将处理后的文本用于多种主题模型进行比较，评估其对主题一致性、唯一性和多样性的影响。

Result: 在25,027条推文数据集上，TM-Rephrase 提高了主题一致性、唯一性和多样性，并降低主题冗余，口语化→正式化策略对大多数方法尤其是 LDA 的改进最显著。

Conclusion: 该框架是一个模型无关的预处理方法，能够提升公共卫生相关社交媒体分析中的主题建模效果，对理解健康危机中的公共话语具有广泛意义。

Abstract: Social media platforms such as Twitter (now X) provide rich data for
analyzing public discourse, especially during crises such as the COVID-19
pandemic. However, the brevity, informality, and noise of social media short
texts often hinder the effectiveness of traditional topic modeling, producing
incoherent or redundant topics that are often difficult to interpret. To
address these challenges, we have developed \emph{TM-Rephrase}, a
model-agnostic framework that leverages large language models (LLMs) to
rephrase raw tweets into more standardized and formal language prior to topic
modeling. Using a dataset of 25,027 COVID-19-related Twitter posts, we
investigate the effects of two rephrasing strategies, general- and
colloquial-to-formal-rephrasing, on multiple topic modeling methods. Results
demonstrate that \emph{TM-Rephrase} improves three metrics measuring topic
modeling performance (i.e., topic coherence, topic uniqueness, and topic
diversity) while reducing topic redundancy of most topic modeling algorithms,
with the colloquial-to-formal strategy yielding the greatest performance gains
and especially for the Latent Dirichlet Allocation (LDA) algorithm. This study
contributes to a model-agnostic approach to enhancing topic modeling in public
health related social media analysis, with broad implications for improved
understanding of public discourse in health crisis as well as other important
domains.

</details>


### [9] [Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection](https://arxiv.org/abs/2510.18909)
*Hongyi He,Xiao Liu,Zhenghao Lin,Mingni Tang,Yi Cheng,Jintao Wang,Wenjie Li,Peng Cheng,Yeyun Gong*

Main category: cs.CL

TL;DR: ODiS: 将多维评分正交化并在每个正交维度内选取高质量数据以实现多样性与质量兼顾的预训练数据采样方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于单/多维分数的选择易产生非单调性，顶级分数数据可能缺乏多样性，导致对相关维度的偏倚。需要将相关指标分解为正交特征维度以保持多样性。

Method: 对语言质量、知识质量、理解难度等多维分数进行PCA去相关化，得到正交维度；在每个维度上训练基于Roberta的回归评分模型以对数据进行大规模推断；在每个正交维度内选择顶级数据，组合成训练集。

Result: ODiS的数据在不同维度之间的重叠小于2%，体现正交性；基于ODiS数据训练的模型在下游基准上明显优于基线。

Conclusion: 正交化、面向多样性的选择是提升LLM预训练数据质量与覆盖面的关键，能改善下游效果。

Abstract: High-quality pre-training data is crutial for large language models, where
quality captures factual reliability and semantic value, and diversity ensures
broad coverage and distributional heterogeneity. Existing approaches typically
rely on single or multiple-dimensional score-based selection. However, directly
selecting top-scored data often degrades performance, and sampling from a
broader range is required to recover results. The above non-monotonicity
between dataset scores and downstream benchmark results reveals a fundamental
bias: score-based methods collapse correlated dimensions, causing top-scored
data to appear high-quality while systematically overlooking diversity. We
argue that ensuring diversity requires decomposing correlated metrics into
orthogonal feature dimensions, from which the top-scored data can be directly
selected. Therefore, we proposed the Orthogonal Diversity-Aware Selection
(ODiS) algorithm, which preserves both quality and diversity during data
selection. First, ODiS evaluates data from multiple dimensions, covering
language quality, knowledge quality, and comprehension difficulty. The
multi-dimensional scores are then decorrelated via Principal Component Analysis
(PCA), yielding orthogonal evaluation dimensions. For each dimension, a
Roberta-based scorer is trained to regress the data onto PCA-projected scores,
enabling scalable inference on large corpora. Finally, ODiS constructs the
training dataset by selecting top-scored data within each orthogonal dimension,
thereby ensuring both quality and diversity. Empirical results show that
ODiS-selected data exhibit less than 2\% inter-dimension overlap, confirming
orthogonality between dimensions. More importantly, models trained with
ODiS-selected data significantly outperform other baselines on downstream
benchmarks, highlighting the necessity of orthogonal, diversity-aware data
selection for LLMs.

</details>


### [10] [Context-aware Fairness Evaluation and Mitigation in LLMs](https://arxiv.org/abs/2510.18914)
*Afrozah Nadeem,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 提出一种动态图/可逆的剪枝框架，在推理阶段对上下文触发的神经元进行自适应遮罩，以实现对对话中偏见与不一致性的记忆感知缓解，支持多语言单轮与多轮对话。


<details>
  <summary>Details</summary>
Motivation: 训练阶段或数据层面的缓解方法成本高、不可逆且难以快速适应新上下文；静态剪枝在上下文变化时缺乏灵活性，需开发可在推理时动态调整的缓解策略。

Method: 在推理阶段检测上下文相关的神经元激活，应用自适应遮罩来调控其对生成的影响，实现细粒度、记忆感知的缓解效果，保持知识与跨语言的一致性，覆盖单轮和多轮对话场景。

Result: 给出一种可逆、动态的框架，能够在推理时实现对偏见和不当行为的控制，并在多语言对话中保持更连贯、可控的行为。

Conclusion: 动态、上下文感知的剪枝-遮罩方法为对话式大模型提供灵活、实时的缓解能力，无需重新训练即可提升公平性与鲁棒性，适用范围可扩展至更多场景。

Abstract: Large language models often display undesirable behaviors embedded in their
internal representations, undermining fairness, inconsistency drift,
amplification of harmful content, and the propagation of unwanted patterns
during extended dialogue and conversations. Although training-time or
data-centric methods attempt to reduce these effects, they are computationally
expensive, irreversible once deployed, and slow to adapt to new conversational
contexts. Pruning-based methods provide a flexible and transparent way to
reduce bias by adjusting the neurons responsible for certain behaviors.
However, most existing approaches are static; once a neuron is removed, the
model loses the ability to adapt when the conversation or context changes. To
address this, we propose a dynamic, reversible, pruning-based framework that
detects context-aware neuron activations and applies adaptive masking to
modulate their influence during generation. Our inference-time solution
provides fine-grained, memory-aware mitigation with knowledge-preserved, more
coherent behavior across multilingual single- and multi-turn dialogues,
enabling dynamic fairness control in real-world conversational AI.

</details>


### [11] [MMAO-Bench: MultiModal All in One Benchmark Reveals Compositional Law between Uni-modal and Omni-modal in OmniModels](https://arxiv.org/abs/2510.18915)
*Chen Chen,ZeYang Hu,Fengjiao Chen,Liya Ma,Jiaxing Liu,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: MMAO-Bench 是一个用于评估单模态和全模态理解能力的基准；实验表明跨模态与单模态性能之间存在可组合关系，且全模态能力在弱模型上表现为瓶颈，在强模型上则带来协同提升。


<details>
  <summary>Details</summary>
Motivation: 当前研究中，单模态与全模态理解之间的相关性尚不清楚，需要一个高质量、多样化的基准来系统评估，以推动全模态模型的智能进化。

Method: 提出高质量、多样性的全模态基准 MMAO-Bench：包含1880个人工筛选样本、44类任务，以及一种创新的多步开放式推理题型，用来更好评估复杂推理；对单模态和全模态的理解能力进行评测。

Result: 实验结果显示跨模态与单模态性能之间存在可组合规律；全模态能力在弱模型上表现出瓶颈效应，在强模型上则表现出协同提升。

Conclusion: MMAO-Bench 为评估单模态与全模态理解提供了高质量、具有广泛覆盖的框架，有助于推动全模态智能的研究与发展；多步开放式推理题型有助于更全面地评估复杂推理能力。

Abstract: Multimodal Large Languages models have been progressing from uni-modal
understanding toward unifying visual, audio and language modalities,
collectively termed omni models. However, the correlation between uni-modal and
omni-modal remains unclear, which requires comprehensive evaluation to drive
omni model's intelligence evolution. In this work, we propose a novel, high
quality and diversity omni model benchmark, MultiModal All in One Benchmark
(MMAO-Bench), which effectively assesses both uni-modal and omni-modal
understanding capabilities. The benchmark consists of 1880 human curated
samples, across 44 task types, and a innovative multi-step open-ended question
type that better assess complex reasoning tasks. Experimental result shows the
compositional law between cross-modal and uni-modal performance and the
omni-modal capability manifests as a bottleneck effect on weak models, while
exhibiting synergistic promotion on strong models.

</details>


### [12] [ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge](https://arxiv.org/abs/2510.18941)
*Zhilin Wang,Jaehun Jung,Ximing Lu,Shizhe Diao,Ellie Evans,Jiaqi Zeng,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.CL

TL;DR: ProfBench 提供一个7000+ 的人类专家标注的专业领域评估集与可扩展的 LLM-Judges 评测框架，显著降低评测成本并揭示当前模型在专业任务上的局限性。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，LLMs 需要处理专业文档、信息综合与完整报告生成等任务，而不仅仅是数学、编程或短问答，因此需要一个覆盖专业领域的评估基准来衡量进展。

Method: 构建 ProfBench，覆盖 Physics PhD、Chemistry PhD、Finance MBA、Consulting MBA 四个专业领域，含超7000条评估项，由具备相应领域知识的专家进行标注；开发鲁棒的 LLM-Judges 来评估这些评估项，努力缓解自我强化偏差并将评测成本降低2-3个数量级，以提升评测的公平性与可访问性。

Result: 即便是前沿模型，如 GPT-5-high，也仅在 ProfBench 上达到65.9% 的总体性能，显示现有最强大模型在专业领域任务上仍存在显著挑战；同时观察到专有模型与开源模型之间的性能差异，以及“扩展思维”在解决复杂专业任务中的作用。

Conclusion: ProfBench 提供一个具有挑战性且可扩展的专业领域评测基准，公开的数據与代码促进复现实验，并强调需要更高层次的推理能力和成本效益的评测方法。

Abstract: Evaluating progress in large language models (LLMs) is often constrained by
the challenge of verifying responses, limiting assessments to tasks like
mathematics, programming, and short-form question-answering. However, many
real-world applications require evaluating LLMs in processing professional
documents, synthesizing information, and generating comprehensive reports in
response to user queries. We introduce ProfBench: a set of over 7000
response-criterion pairs as evaluated by human-experts with professional
knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We
build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by
mitigating self-enhancement bias and reducing the cost of evaluation by 2-3
orders of magnitude, to make it fair and accessible to the broader community.
Our findings reveal that ProfBench poses significant challenges even for
state-of-the-art LLMs, with top-performing models like GPT-5-high achieving
only 65.9\% overall performance. Furthermore, we identify notable performance
disparities between proprietary and open-weight models and provide insights
into the role that extended thinking plays in addressing complex,
professional-domain tasks. Data:
https://huggingface.co/datasets/nvidia/ProfBench and Code:
https://github.com/NVlabs/ProfBench

</details>


### [13] [Dynamic Evaluation for Oversensitivity in LLMs](https://arxiv.org/abs/2510.19005)
*Sophia Xiao Pu,Sitao Cheng,Xin Eric Wang,William Yang Wang*

Main category: cs.CL

TL;DR: A dynamic benchmark OVERBENCH addresses oversensitivity in LLMs by generating model-specific, evolving datasets to capture defensive prompts and mitigate degradation of static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Static evaluation datasets become outdated as models evolve, leading to data contamination and unreliable assessment of what constitutes harmless vs. harmful content. Oversensitivity disrupts interactions and blurs harm boundaries. A dynamic, model-aware evaluation is needed.

Method: A framework that dynamically generates challenging datasets tailored to each model's behavior, then aggregates these into OVERBENCH, a benchmark spanning diverse LLM families with 450,000 samples from 25 models.

Result: OVERBENCH enables a dynamic, evolving view of oversensitivity, allowing continuous monitoring of defensive triggers as models progress and revealing vulnerabilities that static datasets miss.

Conclusion: Dynamic, model-specific benchmarking provides more accurate, up-to-date evaluation of oversensitivity and helps track how defensive patterns evolve, guiding safer and more robust model development.

Abstract: Oversensitivity occurs when language models defensively reject prompts that
are actually benign. This behavior not only disrupts user interactions but also
obscures the boundary between harmful and harmless content. Existing benchmarks
rely on static datasets that degrade overtime as models evolve, leading to data
contamination and diminished evaluative power. To address this, we develop a
framework that dynamically generates model-specific challenging datasets,
capturing emerging defensive patterns and aligning with each model's unique
behavior. Building on this approach, we construct OVERBENCH, a benchmark that
aggregates these datasets across diverse LLM families, encompassing 450,000
samples from 25 models. OVERBENCH provides a dynamic and evolving perspective
on oversensitivity, allowing for continuous monitoring of defensive triggers as
models advance, highlighting vulnerabilities that static datasets overlook.

</details>


### [14] [Are they lovers or friends? Evaluating LLMs' Social Reasoning in English and Korean Dialogues](https://arxiv.org/abs/2510.19028)
*Eunsu Kim,Junyeong Park,Juhyun Oh,Kiwoong Park,Seyoung Song,A. Seza Dogruoz,Najoung Kim,Alice Oh*

Main category: cs.CL

TL;DR: SCRIPTS: a 1k-dialogue English/Korean dataset to evaluate LLMs' social reasoning about interpersonal relationships; results show English accuracy ~75–80%, Korean ~58–69%, with 10–25% of responses claiming Unlikely relationships; CoT prompts yield little benefit and can amplify biases.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can reason about social relationships in dialogues, a crucial but underexplored capability for safe and realistic human–AI interaction.

Method: Create SCRIPTS from movie scripts; annotate each dialogue with probabilistic relational labels by bilingual annotators; evaluate nine models on the task; compare English vs Korean performance; test chain-of-thought prompting.

Result: English performance ~75–80%; Korean 58–69%; 10–25% of model outputs assign Unlikely relationships; thinking models/CoT provide minimal benefits and may amplify biases.

Conclusion: Current LLMs have significant limitations in social reasoning; more work is needed to develop socially-aware language models and reduce bias.

Abstract: As large language models (LLMs) are increasingly used in human-AI
interactions, their social reasoning capabilities in interpersonal contexts are
critical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean,
sourced from movie scripts. The task involves evaluating models' social
reasoning capability to infer the interpersonal relationships (e.g., friends,
sisters, lovers) between speakers in each dialogue. Each dialogue is annotated
with probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by
native (or equivalent) Korean and English speakers from Korea and the U.S.
Evaluating nine models on our task, current proprietary LLMs achieve around
75-80% on the English dataset, whereas their performance on Korean drops to
58-69%. More strikingly, models select Unlikely relationships in 10-25% of
their responses. Furthermore, we find that thinking models and chain-of-thought
prompting, effective for general reasoning, provide minimal benefits for social
reasoning and occasionally amplify social biases. Our findings reveal
significant limitations in current LLMs' social reasoning capabilities,
highlighting the need for efforts to develop socially-aware language models.

</details>


### [15] [Re:Member: Emotional Question Generation from Personal Memories](https://arxiv.org/abs/2510.19030)
*Zackary Rackauckas,Nobuaki Minematsu,Julia Hirschberg*

Main category: cs.CL

TL;DR: Re:Member 是一个将情感表达、记忆绑定与个性化多媒体结合的语言学习原型系统，通过用户的个人视频与目标语言的风格化问句来提升情感回忆和对话参与度。


<details>
  <summary>Details</summary>
Motivation: 旨在通过情感表达与个人媒体提升二语学习的参与感和沉浸感，补充传统语言学习在情感连接和记忆唤起方面的不足；探索情感语音与视觉上下文的同步生成在教育技术中的应用边界。

Method: 提出一个模块化生成流水线：基于 WhisperX 的转写对齐、3 帧视觉采样、以及 Style-BERT-VITS2 的情感合成，将情感语调与视觉上下文对齐，并以个人视频为输入源来生成目标语言的风格化问句，作为情感驱动的学习探究。

Result: 作为一种 stylized interaction probe 设计，展示了情感与个人媒介在学习者中心教育技术中的作用，强调其可行性与潜在影响，但原文未给出定量评估结果，侧重示范性与概念验证。

Conclusion: Re:Member 证明了情感驱动、记忆绑定与个性化多媒体在二语学习中的应用潜能，未来可在系统评估、可扩展性、以及与学习分析结合方面开展更深入研究。

Abstract: We present Re:Member, a system that explores how emotionally expressive,
memory-grounded interaction can support more engaging second language (L2)
learning. By drawing on users' personal videos and generating stylized spoken
questions in the target language, Re:Member is designed to encourage affective
recall and conversational engagement. The system aligns emotional tone with
visual context, using expressive speech styles such as whispers or late-night
tones to evoke specific moods. It combines WhisperX-based transcript alignment,
3-frame visual sampling, and Style-BERT-VITS2 for emotional synthesis within a
modular generation pipeline. Designed as a stylized interaction probe,
Re:Member highlights the role of affect and personal media in learner-centered
educational technologies.

</details>


### [16] [When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable LLM Evaluation](https://arxiv.org/abs/2510.19032)
*Abeer Badawi,Elahe Rahimi,Md Tahmid Rahman Laskar,Sheri Grach,Lindsay Bertrand,Lames Danok,Jimmy Huang,Frank Rudzicz,Elham Dolatabadi*

Main category: cs.CL

TL;DR: 提出 MentalBench-100k 和 MentalAlign-70k 两个大规模评估基准，用于评估大语言模型在心理健康对话中的表现及判评者的一致性，并通过 Affective Cognitive Agreement Framework 来量化 LLM 判评者与人类专家之间的对齐情况。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs在心理健康支持中的评估规模性、可靠性不足，常使用合成数据或社交媒体数据，缺乏可信的自动评审者框架；需要大规模数据集与 judge reliability 的评估方法来提升评估可信度。

Method: 构建 MentalBench-100k：来自三组真实场景数据的10,000个单轮对话，每个对话配9个LLM生成的回复，总计100,000对话样本。MentalAlign-70k：在七个属性的七项指标上，将四个高表现的LLM评审者与人类专家进行对比，产生7个属性中的CSS和ARS，共70,000次评分。使用 Affective Cognitive Agreement Framework（ICC及置信区间）来衡量LLM评审者与人类专家之间的一致性、稳定性和偏差。

Result: 发现LLM评审在情感上存在系统性放大（inflation），对认知属性如引导和信息性具有较高的可靠性，但对同理心的精度较低，安全性和相关性方面存在部分不稳定。总体表明在大规模评估中，仍需关注判评者偏差和属性维度可靠性。

Conclusion: 提供了新的方法学与实证基线，推动心理健康领域中大规模、可靠的LLM评估。并公开基准与代码。

Abstract: Evaluating Large Language Models (LLMs) for mental health support is
challenging due to the emotionally and cognitively complex nature of
therapeutic dialogue. Existing benchmarks are limited in scale, reliability,
often relying on synthetic or social media data, and lack frameworks to assess
when automated judges can be trusted. To address the need for large-scale
dialogue datasets and judge reliability assessment, we introduce two benchmarks
that provide a framework for generation and evaluation. MentalBench-100k
consolidates 10,000 one-turn conversations from three real scenarios datasets,
each paired with nine LLM-generated responses, yielding 100,000 response pairs.
MentalAlign-70k}reframes evaluation by comparing four high-performing LLM
judges with human experts across 70,000 ratings on seven attributes, grouped
into Cognitive Support Score (CSS) and Affective Resonance Score (ARS). We then
employ the Affective Cognitive Agreement Framework, a statistical methodology
using intraclass correlation coefficients (ICC) with confidence intervals to
quantify agreement, consistency, and bias between LLM judges and human experts.
Our analysis reveals systematic inflation by LLM judges, strong reliability for
cognitive attributes such as guidance and informativeness, reduced precision
for empathy, and some unreliability in safety and relevance. Our contributions
establish new methodological and empirical foundations for reliable,
large-scale evaluation of LLMs in mental health. We release the benchmarks and
codes at: https://github.com/abeerbadawi/MentalBench/

</details>


### [17] [From Memorization to Generalization: Fine-Tuning Large Language Models for Biomedical Term-to-Identifier Normalization](https://arxiv.org/abs/2510.19036)
*Suswitha Pericharla,Daniel B. Hier,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: Fine-tuning Llama 3.1 8B yields uneven gains across biomedical ontologies for term-to-identifier mapping: memorization is strong when identifiers are popular (GO up to 77%), generalization mainly occurs for gene mappings (GENE 13.9%), with limited transfer for HPO/GO. GPT-4o remains superior; embedding results support lexicalization effects.


<details>
  <summary>Details</summary>
Motivation: Automated term normalization is essential for semantic interoperability in biomedical data. LLMs show promise but performance varies across terminologies; understanding when fine-tuning helps can guide data curation and model use.

Method: The study evaluated memorization (training-term performance) and generalization (validation-term performance) across multiple biomedical ontologies. It fine-tuned Llama 3.1 8B and compared against baselines (including GPT-4o). Term-to-identifier accuracy was measured. Embedding analyses examined semantic alignment between terms and identifiers, and factors such as identifier popularity and lexicalization were analyzed.

Result: Memorization gains were strong for GO (up to 77%) but minimal for HPO. Generalization was observed only for GENE mappings (13.9% gain); fine-tuning for HPO and GO yielded negligible transfer. Baseline accuracy varied by model scale, with GPT-4o outperforming Llama variants. Embeddings showed tight alignment between gene symbols and protein names but weak alignment between terms and GO/HPO identifiers, indicating limited lexicalization. Two interacting factors predicted fine-tuning success: identifier popularity (pretraining exposure) and lexicalization (e.g., gene symbols enabling generalization).

Conclusion: The findings offer a predictive framework: fine-tuning enhances factual recall when identifiers are popular; it enables generalization when identifiers are lexicalized; it fares poorly when identifiers are sparse or non-lexicalized. This can guide when and how to fine-tune models for term normalization tasks.

Abstract: Effective biomedical data integration depends on automated term
normalization, the mapping of natural language biomedical terms to standardized
identifiers. This linking of terms to identifiers is essential for semantic
interoperability. Large language models (LLMs) show promise for this task but
perform unevenly across terminologies. We evaluated both memorization
(training-term performance) and generalization (validation-term performance)
across multiple biomedical ontologies. Fine-tuning Llama 3.1 8B revealed marked
differences by terminology. GO mappings showed strong memorization gains (up to
77% improvement in term-to-identifier accuracy), whereas HPO showed minimal
improvement. Generalization occurred only for protein-gene (GENE) mappings
(13.9% gain), while fine-tuning for HPO and GO yielded negligible transfer.
Baseline accuracy varied by model scale, with GPT-4o outperforming both Llama
variants for all terminologies. Embedding analyses showed tight semantic
alignment between gene symbols and protein names but weak alignment between
terms and identifiers for GO or HPO, consistent with limited lexicalization.
Fine-tuning success depended on two interacting factors: identifier popularity
and lexicalization. Popular identifiers were more likely encountered during
pretraining, enhancing memorization. Lexicalized identifiers, such as gene
symbols, enabled semantic generalization. By contrast, arbitrary identifiers in
GO and HPO constrained models to rote learning. These findings provide a
predictive framework for when fine-tuning enhances factual recall versus when
it fails due to sparse or non-lexicalized identifiers.

</details>


### [18] [That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation](https://arxiv.org/abs/2510.19116)
*Jaesung Bae,Cameron Churchwell,Mitchell Hermon,Tsun-An Hsieh,Jocelyn Xu,Yekaterina Yegorova,Mark Hasegawa-Johnson,Heng Ji*

Main category: cs.CL

TL;DR: 本文研究大语言模型在提示信息与参数知识不一致时的行为，聚焦代码生成，提出一个领域无关的冲突框架与针对代码场景的评估数据集，并展示了可检测知识冲突及激活层干预的潜力。


<details>
  <summary>Details</summary>
Motivation: 扩展知识冲突研究从问答领域到代码生成，建立通用的冲突构建与解读框架，评估在实际编程任务中的鲁棒性与可控性，并提供可复现的数据与方法。

Method: 提出领域无关的知识冲突构建与解读框架；开发针对代码冲突场景的评估方法与数据集；通过实验评估大模型对知识冲突的检测能力和激活级别干预的效果。

Result: 结论显示： (1) 足够大的LLMs在参数中编码了知识冲突的概念，能够以高达80.65%的准确率检测知识冲突；(2) 基于激活层面的干预可在干预成功率上实现最高12.6%的提升，优于随机基线；(3) 效果受模型规模、任务领域与干预方向的平衡影响。

Conclusion: 表明模型参数中确实存在可检测的知识冲突信号，激活层干预具有提升冲突处理效果的潜力；未来需在规模、领域适配和干预策略之间进行权衡，并将代码与数据公开以促进复现。

Abstract: This paper investigates how large language models (LLMs) behave when faced
with discrepancies between their parametric knowledge and conflicting
information contained in a prompt. Building on prior question-answering (QA)
research, we extend the investigation of knowledge conflicts to the realm of
code generation. We propose a domain-agnostic framework for constructing and
interpreting such conflicts, along with a novel evaluation method and dataset
tailored to code conflict scenarios. Our experiments indicate that sufficiently
large LLMs encode the notion of a knowledge conflict in their parameters,
enabling us to detect knowledge conflicts with up to \textbf{80.65\%} accuracy.
Building on these insights, we show that activation-level steering can achieve
up to a \textbf{12.6\%} improvement in steering success over a random baseline.
However, effectiveness depends critically on balancing model size, task domain,
and steering direction. The experiment code and data will be made publicly
available after acceptance.

</details>


### [19] [A Graph Signal Processing Framework for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.19117)
*Valentin Noël*

Main category: cs.CL

TL;DR: 通过将 Transformer 层建模为由注意力驱动的动态图上的图信号，提出基于图信号处理的光谱分析框架用于区分事实推理与幻觉，并给出一个基于光谱特征的检测器，达到 88.75% 的准确度。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型的事实推理与幻觉之间的区分问题，超越基于困惑度等简单指标；希望通过光谱几何来捕捉推理模式与错误行为的普遍性。

Method: 将 Transformer 层视作由注意力诱导的动态图，令 token 嵌入作为信号在图上承载；通过图信号处理计算 Dirichlet 能量、谱熵、高频能量比等诊断量，与计算稳定性建立理论联系；跨 GPT 架构的实验揭示事实性陈述呈现能量山型且低频收敛，幻觉类型呈现不同谱特征；构建一个使用光谱签名的简单检测器。

Result: 发现普遍的光谱模式：事实性陈述具有能量山形且对低频收敛；不同幻觉类型呈现不同的谱特征；逻辑矛盾导致谱显著不稳定（效应量 g>1.0）；语义错误稳定但呈现连通性漂移；替代性幻觉呈现中等扰动。基于光谱特征的检测器达到 88.75% 的准确度，优于基于困惑度的基线（约 75%）。

Conclusion: 光谱几何可能捕捉推理模式和错误行为，为大型语言模型的幻觉检测提供一个潜在框架，并具有实际应用价值。

Abstract: Large language models achieve impressive results but distinguishing factual
reasoning from hallucinations remains challenging. We propose a spectral
analysis framework that models transformer layers as dynamic graphs induced by
attention, with token embeddings as signals on these graphs. Through graph
signal processing, we define diagnostics including Dirichlet energy, spectral
entropy, and high-frequency energy ratios, with theoretical connections to
computational stability. Experiments across GPT architectures suggest universal
spectral patterns: factual statements exhibit consistent "energy mountain"
behavior with low-frequency convergence, while different hallucination types
show distinct signatures. Logical contradictions destabilize spectra with large
effect sizes ($g>1.0$), semantic errors remain stable but show connectivity
drift, and substitution hallucinations display intermediate perturbations. A
simple detector using spectral signatures achieves 88.75% accuracy versus 75%
for perplexity-based baselines, demonstrating practical utility. These findings
indicate that spectral geometry may capture reasoning patterns and error
behaviors, potentially offering a framework for hallucination detection in
large language models.

</details>


### [20] [Training-Free Spectral Fingerprints of Voice Processing in Transformers](https://arxiv.org/abs/2510.19131)
*Valentin Noël*

Main category: cs.CL

TL;DR: 通过对注意力诱导的 token 图进行图信号处理和谱分析，比较 20 种语言在三个模型家族中的语态变换下的代数连通性变化，揭示语言与架构特有的谱指纹，并与行为表现相关，提供训练自由的诊断框架。


<details>
  <summary>Details</summary>
Motivation: 探究不同 Transformer 架构在实现相同语言计算时是否通过不同连接模式留下可检测的计算指纹，以及训练偏好是否会产生可观测的谱学印记，进而以简单诊断工具评估模型偏差和可靠性。

Method: 对注意力诱导的 token 图进行图信号处理，关注代数连通性（Fiedler 值 Δλ2），在层 2–5 的早期窗口比较 3 种模型（Phi-3-Mini、Qwen2.5-7B、LLaMA-3.2-1B）在 20 种语言的语态变换场景的变化，此外通过针对性注意头 ablation 来验证其功能相关性。

Result: 发现语言与模型特征化的谱指纹：Phi-3-Mini 在英语语态变换的早期层显示显著干扰（平均 Δλ2 约 -0.446），其他语言影响较小；Qwen2.5-7B 的变化较小且分布较广，且对形态丰富语言影响最大；LLaMA-3.2-1B 呈现系统性但程度较温和的响应。Δλ2 的变化与行为差异高度相关（r 约 -0.976），并可被头部 ablation 调制，指向早期注意力结构的作用。结论是，训练偏好会留下可检测的计算印记，框架可用于区分推理模式，作为一个简单、无训练成本的诊断工具，用于揭示架构偏差与提高模型可靠性。

Conclusion: 框架证明训练重点能留下谱指纹，呈现为在语法变换中的专门处理策略；该方法可作为训练无关的诊断工具，用于区分推理模式、识别架构偏差、评估模型可靠性。

Abstract: Different transformer architectures implement identical linguistic
computations via distinct connectivity patterns, yielding model imprinted
``computational fingerprints'' detectable through spectral analysis. Using
graph signal processing on attention induced token graphs, we track changes in
algebraic connectivity (Fiedler value, $\Delta\lambda_2$) under voice
alternation across 20 languages and three model families, with a prespecified
early window (layers 2--5). Our analysis uncovers clear architectural
signatures: Phi-3-Mini shows a dramatic English specific early layer disruption
($\overline{\Delta\lambda_2}_{[2,5]}\!\approx\!-0.446$) while effects in 19
other languages are minimal, consistent with public documentation that
positions the model primarily for English use. Qwen2.5-7B displays small,
distributed shifts that are largest for morphologically rich languages, and
LLaMA-3.2-1B exhibits systematic but muted responses. These spectral signatures
correlate strongly with behavioral differences (Phi-3: $r=-0.976$) and are
modulated by targeted attention head ablations, linking the effect to early
attention structure and confirming functional relevance. Taken together, the
findings are consistent with the view that training emphasis can leave
detectable computational imprints: specialized processing strategies that
manifest as measurable connectivity patterns during syntactic transformations.
Beyond voice alternation, the framework differentiates reasoning modes,
indicating utility as a simple, training free diagnostic for revealing
architectural biases and supporting model reliability analysis.

</details>


### [21] [Tibetan Language and AI: A Comprehensive Survey of Resources, Methods and Challenges](https://arxiv.org/abs/2510.19144)
*Cheng Huang,Nyima Tashi,Fan Gao,Yutong Liu,Jiahao Li,Hao Tian,Siyang Jiang,Thupten Tsering,Ban Ma-bao,Renzeg Duojie,Gadeng Luosang,Rinchen Dongrub,Dorje Tashi,Jin Zhang,Xiao Feng,Hao Wang,Jie Tang,Guojie Tang,Xiangxiang Wang,Jia Zhang,Tsengdar Lee,Yongbin Yu*

Main category: cs.CL

TL;DR: 对藏语AI研究的全面综述，梳理数据资源、任务、工具及挑战，提出跨语言、跨模态与社区驱动的协作路径。


<details>
  <summary>Details</summary>
Motivation: 藏语属于资源匮乏的语言，缺乏可获取的数据、统一基准和工具，制约AI系统在文本与语音领域的发展及跨语言迁移。

Method: 系统性地整理现有数据集与工具，评估各任务中的方法并在可比情况下对性能进行比较，分析瓶颈，探讨跨语言迁移、多模态学习和社区驱动的资源建设。

Result: 提供数据集与工具的分类与评估框架；对文本与语音相关任务、机器翻译、LLMs等的现状进行汇总与对比；识别数据稀缺、正字差异、评估指标不统一等瓶颈，并提出跨语言、跨模态与社区参与的解决方向。

Conclusion: 本综述将作为未来藏语AI研究的基础性参考，推动低资源语言的包容性与可持续AI生态的建设，并鼓励学术与社区的协作。

Abstract: Tibetan, one of the major low-resource languages in Asia, presents unique
linguistic and sociocultural characteristics that pose both challenges and
opportunities for AI research. Despite increasing interest in developing AI
systems for underrepresented languages, Tibetan has received limited attention
due to a lack of accessible data resources, standardized benchmarks, and
dedicated tools. This paper provides a comprehensive survey of the current
state of Tibetan AI in the AI domain, covering textual and speech data
resources, NLP tasks, machine translation, speech recognition, and recent
developments in LLMs. We systematically categorize existing datasets and tools,
evaluate methods used across different tasks, and compare performance where
possible. We also identify persistent bottlenecks such as data sparsity,
orthographic variation, and the lack of unified evaluation metrics.
Additionally, we discuss the potential of cross-lingual transfer, multi-modal
learning, and community-driven resource creation. This survey aims to serve as
a foundational reference for future work on Tibetan AI research and encourages
collaborative efforts to build an inclusive and sustainable AI ecosystem for
low-resource languages.

</details>


### [22] [Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop RAG](https://arxiv.org/abs/2510.19171)
*Jihwan Bang,Juntae Lee,Seunghan Yang,Sungha Choi*

Main category: cs.CL

TL;DR: A structured multi-hop RAG method TSSS improves efficiency by template-based reasoning with cached prefixes and a retriever-based terminator, achieving state-of-the-art accuracy on HotpotQA, 2WikiMultiHop, and MuSiQue with competitive efficiency, suitable for on-device inference.


<details>
  <summary>Details</summary>
Motivation: Existing iterative prompting for multi-hop RAG regenerates predictable token sequences and relies on stochastic stopping, causing token waste and unstable termination. There is a need for efficient, reliable termination and reduced token generation.

Method: (i) Template-based reasoning that caches recurring prefixes and anchors sub-queries to the main question to reduce token generation and stabilize reasoning. (ii) A retriever-based terminator that deterministically halts reasoning when additional sub-queries collapse into repetition.

Result: On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS achieves state-of-the-art accuracy and competitive efficiency among RAG-CoT approaches, particularly suitable for efficiency-constrained scenarios like on-device inference.

Conclusion: Separating structured reasoning from termination control leads to faster inference and more reliable answers, making TSSS effective for efficiency-constrained multi-hop RAG tasks.

Abstract: Multi-hop retrieval-augmented generation (RAG) is a promising strategy for
complex reasoning, yet existing iterative prompting approaches remain
inefficient. They often regenerate predictable token sequences at every step
and rely on stochastic stopping, leading to excessive token usage and unstable
termination. We propose TSSS (Think Straight, Stop Smart), a structured
multi-hop RAG framework designed for efficiency. TSSS introduces (i) a
template-based reasoning that caches recurring prefixes and anchors sub-queries
to the main question, reducing token generation cost while promoting stable
reasoning, and (ii) a retriever-based terminator, which deterministically halts
reasoning once additional sub-queries collapse into repetition. This separation
of structured reasoning and termination control enables both faster inference
and more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS
achieves state-of-the-art accuracy and competitive efficiency among RAG-CoT
approaches, highlighting its effectiveness in efficiency-constrained scenarios
such as on-device inference.

</details>


### [23] [When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA](https://arxiv.org/abs/2510.19172)
*Nishanth Sridhar Nakshatri,Shamik Roy,Manoj Ghuhan Arivazhagan,Hanhan Zhou,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.CL

TL;DR: 提出 evolveQA 基准来评估大语言模型在时间演变知识上的表现，基于三个真实且带时间戳的语料源（AWS 更新、Azure 变更、WHO 疾病疫情报告）来发现知识如何随时间变化，并对不同知识截断日期的答案给出金标准。结果显示相较于静态知识问题，模型在演变知识上的表现下降明显，最高可达约 31%。


<details>
  <summary>Details</summary>
Motivation: LLMs 在知识随时间演化时容易产生冲突与错误。现有基准多基于结构化知识库，关注广泛且易记忆的实体，且缺乏能公平评估不同知识截断日期的动态结构。需要一个能够捕捉知识随时间变化且能区分不同知识截断点的基准，以更真实地评估模型在时序信息上的鲁棒性。

Method: 从三个真实的带时间戳的语料源（AWS 更新、Azure 变更、WHO 疾病疫情报告）中识别自然发生的知识演化，构建可对不同 knowledge cut-off 日期给出 gold 答案的问题。设计三种知识探测格式，对 12 个开源与闭源 LLM 进行评测，比较它们在具有时序信息的问题上的表现与静态知识基准的差异。

Result: 在 12 种模型中，演变知识任务的性能显著低于静态知识问题，下降幅度高达 31%，涵盖开源与闭源模型及多种 probing 格式，显示当前 LLMs 对随时间变化的知识缺乏稳健性。

Conclusion: 演变知识基准如 evolveQA 能揭示 LLMs 在时间维度上的薄弱环节，强调需要将时间信息纳入评估与模型设计中，可能的方向包括时间感知检索、动态记忆更新或对时态冲突的显式处理。

Abstract: LLMs often fail to handle temporal knowledge conflicts--contradictions
arising when facts evolve over time within their training data. Existing
studies evaluate this phenomenon through benchmarks built on structured
knowledge bases like Wikidata, but they focus on widely-covered,
easily-memorized popular entities and lack the dynamic structure needed to
fairly evaluate LLMs with different knowledge cut-off dates. We introduce
evolveQA, a benchmark specifically designed to evaluate LLMs on temporally
evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS
updates, Azure changes, and WHO disease outbreak reports. Our framework
identifies naturally occurring knowledge evolution and generates questions with
gold answers tailored to different LLM knowledge cut-off dates. Through
extensive evaluation of 12 open and closed-source LLMs across 3 knowledge
probing formats, we demonstrate significant performance drops of up to 31% on
evolveQA compared to static knowledge questions.

</details>


### [24] [Interpretable Question Answering with Knowledge Graphs](https://arxiv.org/abs/2510.19181)
*Kartikeya Aneja,Manasvi Srivastava,Subhayan Das,Nagender Aneja*

Main category: cs.CL

TL;DR: 提出一个完全基于知识图谱的问答流水线，避免RAG，利用对从知识图谱检索得到的实体关系边的小型改写模型进行改写，并通过图检索进行回答生成，结果在CRAG基准上通过LLM评估显示较高准确度（71.9%/54.4%）。


<details>
  <summary>Details</summary>
Motivation: 旨在解决基于RAG的检索对大语言模型依赖带来的局限性与潜在错误，加强可解释性与效率；探索将问答转化为知识图谱并通过图检索获取答案的可行性。

Method: 工作分为两阶段：第一阶段对文档进行预处理以生成问答对集合；第二阶段将这些问答对转换为知识图谱，并利用嵌入与模糊匹配进行图检索；检索后的图被查询、重新排序并对边关系进行改写后生成最终答案。核心在于用一个小型改写模型对检索得到的实体关系边进行 paraphrase，以提升匹配与回答质。

Result: 在CRAG基准上，采用LLM作为评审者进行评估，使用LLama-3.2得到71.9%的准确率，使用GPT-3.5-Turbo得到54.4%的准确率。

Conclusion: 证明在不进行RAG的前提下，通过知识图谱检索、边关系改写与图级推理也能实现可观的问答性能；该方法的优势在于减少对大模型的依赖并提升可解释性与效率，但对改写模型质量、评估偏见以及数据集局限性需进一步研究。

Abstract: This paper presents a question answering system that operates exclusively on
a knowledge graph retrieval without relying on retrieval augmented generation
(RAG) with large language models (LLMs). Instead, a small paraphraser model is
used to paraphrase the entity relationship edges retrieved from querying the
knowledge graph. The proposed pipeline is divided into two main stages. The
first stage involves pre-processing a document to generate sets of
question-answer (QA) pairs. The second stage converts these QAs into a
knowledge graph from which graph-based retrieval is performed using embeddings
and fuzzy techniques. The graph is queried, re-ranked, and paraphrased to
generate a final answer. This work includes an evaluation using LLM-as-a-judge
on the CRAG benchmark, which resulted in accuracies of 71.9% and 54.4% using
LLAMA-3.2 and GPT-3.5-Turbo, respectively.

</details>


### [25] [Multi-Faceted Evaluation of Tool-Augmented Dialogue Systems](https://arxiv.org/abs/2510.19186)
*Zhaoyi Joey Hou,Tanya Shourya,Yingfan Wang,Shamik Roy,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.CL

TL;DR: 提出 TRACE 和 SCOPE 来评估工具增强对话系统的可用性与鲁棒性。TRACE 通过合成多样化的错误场景来覆盖工具使用中的关键错误，SCOPE 则自动发现对话中的错误模式与评估标准。实验显示 SCOPE 在用户满意度信号误导时也显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有评估要么聚焦用户满意度，要么评估工具调用能力，难以捕捉多轮对话中工具结果被误解等复杂错误，导致对系统鲁棒性的评估不足。

Method: 设计 TRACE：一个系统合成的工具增强对话基准，覆盖多种错误类型；设计 SCOPE：一个自动发现对话中错误模式和评估标准的评估框架。通过对比实验验证 SCOPE 相较基线在难例上的性能提升。

Result: 实验结果显示，SCOPE 在识别并区分多种错误模式方面优于基线，尤其是在用户满意度信号易误导的难例中，提升显著。

Conclusion: 提出的 TRACE 与 SCOPE 提供更全面的工具增强对话评估框架，有助于揭示和量化多轮对话中因工具误用或误解导致的错误，从而提升评估鲁棒性。

Abstract: Evaluating conversational AI systems that use external tools is challenging,
as errors can arise from complex interactions among user, agent, and tools.
While existing evaluation methods assess either user satisfaction or agents'
tool-calling capabilities, they fail to capture critical errors in multi-turn
tool-augmented dialogues-such as when agents misinterpret tool results yet
appear satisfactory to users. We introduce TRACE, a benchmark of systematically
synthesized tool-augmented conversations covering diverse error cases, and
SCOPE, an evaluation framework that automatically discovers diverse error
patterns and evaluation rubrics in tool-augmented dialogues. Experiments show
SCOPE significantly outperforms the baseline, particularly on challenging cases
where user satisfaction signals are misleading.

</details>


### [26] [Modality Matching Matters: Calibrating Language Distances for Cross-Lingual Transfer in URIEL+](https://arxiv.org/abs/2510.19217)
*York Hay Ng,Aditya Khan,Xiang Lu,Matteo Salloum,Michael Zhou,Phuong H. Hoang,A. Seza Doğruöz,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 提出面向类型匹配的语言距离框架，结合结构化表示与鲁棒综合距离，提升跨语言迁移的语言选择与多任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有知识库如 URIEL+ 提供地理、遗传和类型距离，但存在向量一刀切、缺乏统一的聚合机制两大局限，难以适应多样化的语言数据结构。

Method: 为每种距离类型引入结构化表示：地理使用说话者加权分布、系谱采用双曲嵌入、类型学采用潜变量模型，并将它们统一成一个任务无关的综合距离。

Result: 在迁移语言选择任务中，这些结构化表示与综合距离在多种NLP任务上稳健提升跨语言迁移性能，提供更为 principled 与有效的多语言研究工具。

Conclusion: 该框架为跨语言距离提供更系统、可解释且鲁棒的工具，改进迁移语言选择并提升多任务表现。

Abstract: Existing linguistic knowledge bases such as URIEL+ provide valuable
geographic, genetic and typological distances for cross-lingual transfer but
suffer from two key limitations. One, their one-size-fits-all vector
representations are ill-suited to the diverse structures of linguistic data,
and two, they lack a principled method for aggregating these signals into a
single, comprehensive score. In this paper, we address these gaps by
introducing a framework for type-matched language distances. We propose novel,
structure-aware representations for each distance type: speaker-weighted
distributions for geography, hyperbolic embeddings for genealogy, and a latent
variables model for typology. We unify these signals into a robust,
task-agnostic composite distance. In selecting transfer languages, our
representations and composite distances consistently improve performance across
a wide range of NLP tasks, providing a more principled and effective toolkit
for multilingual research.

</details>


### [27] [Difficulty-Controllable Multiple-Choice Question Generation Using Large Language Models and Direct Preference Optimization](https://arxiv.org/abs/2510.19265)
*Yuto Tomikawa,Masaki Uto*

Main category: cs.CL

TL;DR: 提出一种面向阅读理解的难度可控多项选择题生成方法，使用直接偏好优化训练的大语言模型来提升难度控制的准确性。


<details>
  <summary>Details</summary>
Motivation: 教育场景中广泛使用的MCQ是必要的自适应学习工具，但现有神经QG方法难以直接生成MCQ，且对难度控制的准确性训练不足。

Method: 提出一种难度可控的MCQ生成方法，结合一个通过直接偏好优化(DPO)训练的大语言模型，以提高输出难度与标签的一致性；目标是直接生成MCQ而非自由文本。

Result: 摘要未给出具体结果，但声称该方法可提升难度控制的准确性。

Conclusion: 将大语言模型与DPO结合用于可控MCQ生成，解决现有方法的两大不足，适用于教育自适应学习。

Abstract: Difficulty-controllable question generation for reading comprehension has
gained significant attention in the field of education as a fundamental tool
for adaptive learning support. Although several neural question generation
methods have recently succeeded in controlling difficulty, conventional
approaches still face two major limitations. First, they cannot directly
generate multiple-choice questions, which are the most widely used question
type in educational contexts. Second, they are not explicitly trained to
optimize the accuracy of difficulty control, leaving room for further
improvement in difficulty controllability. To address these limitations, this
study proposes a novel difficulty-controllable multiple-choice question
generation method for reading comprehension which leverages a large language
model trained using a direct preference optimization technique to improve the
accuracy of difficulty control.

</details>


### [28] [TheMCPCompany: Creating General-purpose Agents with Task-specific Tools](https://arxiv.org/abs/2510.19286)
*Reza Esfandiarpoor,Vishwas Suryanarayanan,Stephen H. Bach,Vishal Chowdhary,Anthony Aue*

Main category: cs.CL

TL;DR: TheMCPCompany shows tool-calling can outperform browser-based agents under perfect tool retrieval, but scaling to thousands of tools in enterprise tasks remains hard for current models, highlighting the need for better reasoning and retrieval.


<details>
  <summary>Details</summary>
Motivation: Evaluate tool-calling agents operating on real-world services with large tool sets, to assess their practicality and scalability compared to browser-based agents.

Method: Build TheMCPCompany benchmark by exposing REST API-based MCP servers with over 18,000 tools, provide manually annotated ground-truth tools, and evaluate using ground-truth and retrieved tools across tasks; compare against browser-based agents; analyze model performance (including GPT-5) in simple vs. complex environments.

Result: Perfect tool retrieval improves performance and reduces costs; smaller models fail to exploit tools via retrieval; GPT-5 with retrieval approaches performance with ground-truth tools; current models struggle to navigate tens of thousands of tools and combine them non-trivially in complex enterprise settings.

Conclusion: Better reasoning and retrieval models are needed to effectively utilize large tool libraries; tool-based agents show promise but face scalability challenges in enterprise environments.

Abstract: Since the introduction of the Model Context Protocol (MCP), the number of
available tools for Large Language Models (LLMs) has increased significantly.
These task-specific tool sets offer an alternative to general-purpose tools
such as web browsers, while being easier to develop and maintain than GUIs.
However, current general-purpose agents predominantly rely on web browsers for
interacting with the environment. Here, we introduce TheMCPCompany, a benchmark
for evaluating tool-calling agents on tasks that involve interacting with
various real-world services. We use the REST APIs of these services to create
MCP servers, which include over 18,000 tools. We also provide manually
annotated ground-truth tools for each task. In our experiments, we use the
ground truth tools to show the potential of tool-calling agents for both
improving performance and reducing costs assuming perfect tool retrieval. Next,
we explore agent performance using tool retrieval to study the real-world
practicality of tool-based agents. While all models with tool retrieval perform
similarly or better than browser-based agents, smaller models cannot take full
advantage of the available tools through retrieval. On the other hand, GPT-5's
performance with tool retrieval is very close to its performance with
ground-truth tools. Overall, our work shows that the most advanced reasoning
models are effective at discovering tools in simpler environments, but
seriously struggle with navigating complex enterprise environments.
TheMCPCompany reveals that navigating tens of thousands of tools and combining
them in non-trivial ways to solve complex problems is still a challenging task
for current models and requires both better reasoning and better retrieval
models.

</details>


### [29] [JointCQ: Improving Factual Hallucination Detection with Joint Claim and Query Generation](https://arxiv.org/abs/2510.19310)
*Fan Xu,Huixuan Zhang,Zhenliang Zhang,Jiahao Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: JointCQ 提出一个联合生成框架，用于在幻觉检测中同时进行 claim extraction 与 query generation，并通过对合成数据进行筛选与微调提升性能，在多项开放域问答基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在 claim extraction 的上下文信息保留不足以及 query generation 的特异性不足，导致幻觉检测链路性能下降。需要一个能同时提取主张并生成高质量查询的端到端解决方案。

Method: 设计并实现 JointCQ 框架，进行联合的主张提取和查询生成；通过精心设计的评估准则筛选合成训练数据；对语言模型进行微调，使其能够同时完成主张提取与查询生成，从而为后续检索和证据核验提供可靠、信息量丰富的输入。

Result: 在多个开放域问答幻觉检测基准上，人们的方法性能优于现有方法，显示出更高的准确性与鲁棒性。

Conclusion: 联合的 claim 与 query 生成能显著提升幻觉检测管线的性能，推动更可信、透明的语言模型系统。

Abstract: Current large language models (LLMs) often suffer from hallucination issues,
i,e, generating content that appears factual but is actually unreliable. A
typical hallucination detection pipeline involves response decomposition (i.e.,
claim extraction), query generation, evidence collection (i.e., search or
retrieval), and claim verification. However, existing methods exhibit
limitations in the first two stages, such as context loss during claim
extraction and low specificity in query generation, resulting in degraded
performance across the hallucination detection pipeline. In this work, we
introduce JointCQ https://github.com/pku0xff/JointCQ, a joint claim-and-query
generation framework designed to construct an effective and efficient
claim-query generator. Our framework leverages elaborately designed evaluation
criteria to filter synthesized training data, and finetunes a language model
for joint claim extraction and query generation, providing reliable and
informative inputs for downstream search and verification. Experimental results
demonstrate that our method outperforms previous methods on multiple
open-domain QA hallucination detection benchmarks, advancing the goal of more
trustworthy and transparent language model systems.

</details>


### [30] [KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints](https://arxiv.org/abs/2510.19316)
*Kailin Jiang,Hongbo Jiang,Ning Jiang,Zhi Gao,Jinhe Bi,Yuchen Ren,Bin Li,Yuntao Du,Lei Liu,Qing Li*

Main category: cs.CL

TL;DR: 提出 KORE：一种面向知识的增广与约束相结合的方法，用于在大模态模型中注入新知识并保持旧知识，提升新知识注入和防止灾难性遗忘的能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型的知识随训练而定且静态，难以跟上现实世界的发展，需持续知识获取；现有方法易遗忘旧知识。

Method: KORE 由两部分组成：知识导向的增广和约束，以及将先前知识存储在LMM线性层激活的协方差矩阵中；通过将新知识条目转换为结构化、完整的知识来确保准确学习；使用将原始权重投射到协方差矩阵的零空间来初始化适配器，从而定义微调方向以尽量减少对旧知识的干扰。

Result: 在 LLaVA-v1.5-7B、LLaVA-v1.5-13B、Qwen2.5-VL-7B等多种模型上进行广泛实验，KORE 在新知识注入方面表现优越，并有效缓解灾难性遗忘。

Conclusion: KORE 能在注入新知识的同时保持旧知识，促进大模态模型的持续知识获取与适应。

Abstract: Large Multimodal Models encode extensive factual knowledge in their
pre-trained weights. However, its knowledge remains static and limited, unable
to keep pace with real-world developments, which hinders continuous knowledge
acquisition. Effective knowledge injection thus becomes critical, involving two
goals: knowledge adaptation (injecting new knowledge) and knowledge retention
(preserving old knowledge). Existing methods often struggle to learn new
knowledge and suffer from catastrophic forgetting. To address this, we propose
KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints
for injecting new knowledge into large multimodal models while preserving old
knowledge. Unlike general text or image data augmentation, KORE automatically
converts individual knowledge items into structured and comprehensive knowledge
to ensure that the model accurately learns new knowledge, enabling accurate
adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix
of LMM's linear layer activations and initializes the adapter by projecting the
original weights into the matrix's null space, defining a fine-tuning direction
that minimizes interference with previous knowledge, enabling powerful
retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,
LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new
knowledge injection performance and effectively mitigates catastrophic
forgetting.

</details>


### [31] [HAD: HAllucination Detection Language Models Based on a Comprehensive Hallucination Taxonomy](https://arxiv.org/abs/2510.19318)
*Fan Xu,Xinyu Hu,Zhenghan Yu,Li Lin,Xu Zhang,Yang Zhang,Wei Zhou,Jinjie Gu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出了一种11类幻觉塔形的分类体系，并开发了HAD模型，将幻觉检测、跨度级定位与纠错整合在一个推理过程；基于约9万条合成数据训练，构建了HADTest测试集（2248条样本），在内外域数据上普遍优于基线，在HaluEval、FactCHD、FaithBench上达到SOTA，展示了鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: NLG模型（尤其大语言模型）容易产生“幻觉”信息，亟需一个统一、端到端的检测、定位和纠错方案，以提高输出的可靠性和可用性；同时需要一个覆盖多任务场景的测试集来评估鲁棒性。

Method: 提出11类幻觉类型的综合分类体系；设计HAD模型，将幻觉检测、跨度级定位（span-level identification）与纠错整合到单次推理中；使用约90K条合成数据进行训练；人工标注测试集HADTest（2248条）用于评估。

Result: 在内外域测试集上，HAD模型通常优于现有基线，实现HaluEval、FactCHD和FaithBench等数据集上的最新性能。

Conclusion: HAD为幻觉检测提供一个端到端的解决方案，具备跨任务的适用性与鲁棒性；丰富的合成数据与测试集有助于推动该领域的研究与评测标准。

Abstract: The increasing reliance on natural language generation (NLG) models,
particularly large language models, has raised concerns about the reliability
and accuracy of their outputs. A key challenge is hallucination, where models
produce plausible but incorrect information. As a result, hallucination
detection has become a critical task. In this work, we introduce a
comprehensive hallucination taxonomy with 11 categories across various NLG
tasks and propose the HAllucination Detection (HAD) models
https://github.com/pku0xff/HAD, which integrate hallucination detection,
span-level identification, and correction into a single inference process.
Trained on an elaborate synthetic dataset of about 90K samples, our HAD models
are versatile and can be applied to various NLG tasks. We also carefully
annotate a test set for hallucination detection, called HADTest, which contains
2,248 samples. Evaluations on in-domain and out-of-domain test sets show that
our HAD models generally outperform the existing baselines, achieving
state-of-the-art results on HaluEval, FactCHD, and FaithBench, confirming their
robustness and versatility.

</details>


### [32] [Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization](https://arxiv.org/abs/2510.19325)
*Junjie Song,Yiwen Liu,Dapeng Li,Yin Sun,Shukun Fu,Siqi Chen,Yuji Cao*

Main category: cs.CL

TL;DR: 提出基于超体积优化(HVO)的强化学习策略，用于多目标文本摘要的平衡优化，通过动态调整各子目标分数来趋近帕累托前沿，实验显示优于GRPO，且7B模型经HVO后可媲美GPT-4且生成更短。


<details>
  <summary>Details</summary>
Motivation: 文本摘要需要在一致性、连贯性、相关性与流畅性等多目标间取得权衡，传统RL-基于LLMs的方法在多目标优化上存在局部最优或不平衡的问题；因此需要一种能动态权衡多目标的优化方法。

Method: 引入超体积优化(HVO)作为 RL 训练中的 reward 策略，通过超体积度量动态在目标组之间调整分数，以引导模型逐步逼近帕累托前沿，从而得到在多个维度上更均衡的摘要。与GRPO等方法进行对比实验。

Result: 在多个常用摘要数据集上，HVO办法在总体分数以及各维度平衡性上均优于GRPO；采用7B基础模型并经HVO处理的模型，其摘要质量接近GPT-4且生成长度更短。

Conclusion: HVO为基于LLMs的多目标文本摘要提供了一个有效的优化框架，能在多维度目标之间实现更好的权衡与均衡输出，并可在较小模型上达到接近大规模模型的性能，具有良好的应用前景和可公开的代码。

Abstract: Text summarization is a crucial task that requires the simultaneous
optimization of multiple objectives, including consistency, coherence,
relevance, and fluency, which presents considerable challenges. Although large
language models (LLMs) have demonstrated remarkable performance, enhanced by
reinforcement learning (RL), few studies have focused on optimizing the
multi-objective problem of summarization through RL based on LLMs. In this
paper, we introduce hypervolume optimization (HVO), a novel optimization
strategy that dynamically adjusts the scores between groups during the reward
process in RL by using the hypervolume method. This method guides the model's
optimization to progressively approximate the pareto front, thereby generating
balanced summaries across multiple objectives. Experimental results on several
representative summarization datasets demonstrate that our method outperforms
group relative policy optimization (GRPO) in overall scores and shows more
balanced performance across different dimensions. Moreover, a 7B foundation
model enhanced by HVO performs comparably to GPT-4 in the summarization task,
while maintaining a shorter generation length. Our code is publicly available
at https://github.com/ai4business-LiAuto/HVO.git

</details>


### [33] [Slot Filling as a Reasoning Task for SpeechLLMs](https://arxiv.org/abs/2510.19326)
*Kadri Hacioglu,Manjunath K E,Andreas Stolcke*

Main category: cs.CL

TL;DR: 将推理能力引入语音大模型以提升端到端槽位填充任务，通过链式思维分解任务、构建推理数据集并进行监督微调；基础模型的领域契合度影响效果，混合文本基础模型的多模态操作表现最佳。


<details>
  <summary>Details</summary>
Motivation: 通过在语音LLMs中引入显性推理步骤，提升端到端槽位填充的可解释性与性能，受益于最近推理LLMs的发展。

Method: 采用链式推理框架将槽位填充任务分解为多步推理；构建推理数据集并对语音LLM进行有监督微调；比较普通与推理型语音LLMs，并用不同类型/尺寸的文本基础模型进行实验；提出混合语音LLMs以同时保留直接与推理两种模式。

Result: 引入推理(中间步骤)可提升性能；但主要用于数学/逻辑/编程领域的文本LLM作为基础模型时可能表现不佳；基于混合文本基础模型并同时保留两种模式的混合语音LLMs性能优于仅使用单一模式的模型。

Conclusion: 将推理能力整合入语音LLMs有望提升端到端槽位填充的效果，基础模型的领域适配性影响结果；同时保留多种操作模式的混合模型在该任务上更具潜力。

Abstract: We propose integration of reasoning into speech large language models
(speechLLMs) for the end-to-end slot-filling task. Inspired by the recent
development of reasoning LLMs, we use a chain-of-thought framework to decompose
the slot-filling task into multiple reasoning steps, create a reasoning dataset
and apply the supervised fine-tuning strategy to a speechLLM. We distinguish
between regular and reasoning speechLLMs and experiment with different types
and sizes of LLMs as their text foundation models. We demonstrate performance
improvements by introducing reasoning (intermediate) steps. However, we show
that a reasoning textual LLM developed mainly for math, logic and coding
domains might be inferior as a foundation model for a reasoning speechLLM. We
further show that hybrid speechLLMs, built on a hybrid text foundation LLM and
fine-tuned to preserve both direct and reasoning modes of operation, have
better performance than those fine-tuned employing only one mode of operation.

</details>


### [34] [Algorithmic Fairness in NLP: Persona-Infused LLMs for Human-Centric Hate Speech Detection](https://arxiv.org/abs/2510.19331)
*Ewelina Gajewska,Arda Derbent,Jaroslaw A Chudziak,Katarzyna Budzynska*

Main category: cs.CL

TL;DR: 通过在大型语言模型中引入注解者人设（Persona-LLMs），结合浅层与深度情境化的人格提示（含基于RAG的人格开发），比较组内/组外身份对 hate speech 检测的性能与公平性的影响，探索社会身份信息在减少偏见方面的潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 弥合自动化 hate speech 检测中的偏见与不公平问题。探索在模型提示中加入 socio-demographic 属性、并通过不同层级的人设来改良识别与公平性表现的可行性与效果。

Method: 使用 Google Gemini 与 OpenAI GPT-4.1-mini，在两种 persona 提示策略下评估：1) 浅层 persona 提示；2) 基于 Retrieval-Augmented Generation（RAG）的深度情境化 persona 开发。对比 in-group 与 out-group 注释者人设对检测性能和跨群体公平性的影响。

Result: 结果显示 persona‑based 方法具有在某些群体上降低偏见、提升公平性的潜力，但也存在局限性与实现挑战，例如对隐私、身份特征的敏感性、以及不同模型对同一人设的解读差异。

Conclusion: 将社会人口统计属性融入 LLM 的人格化设计可作为改进 hate speech 检测公平性的途径，但需谨慎处理隐私、偏见放大风险及实现复杂性，未来研究应聚焦于稳健性、跨文化适用性与伦理边界。

Abstract: In this paper, we investigate how personalising Large Language Models
(Persona-LLMs) with annotator personas affects their sensitivity to hate
speech, particularly regarding biases linked to shared or differing identities
between annotators and targets. To this end, we employ Google's Gemini and
OpenAI's GPT-4.1-mini models and two persona-prompting methods: shallow persona
prompting and a deeply contextualised persona development based on
Retrieval-Augmented Generation (RAG) to incorporate richer persona profiles. We
analyse the impact of using in-group and out-group annotator personas on the
models' detection performance and fairness across diverse social groups. This
work bridges psychological insights on group identity with advanced NLP
techniques, demonstrating that incorporating socio-demographic attributes into
LLMs can address bias in automated hate speech detection. Our results highlight
both the potential and limitations of persona-based approaches in reducing
bias, offering valuable insights for developing more equitable hate speech
detection systems.

</details>


### [35] [Local Obfuscation by GLINER for Impartial Context Aware Lineage: Development and evaluation of PII Removal system](https://arxiv.org/abs/2510.19346)
*Prakrithi Shivaprakash,Lekhansh Shukla,Animesh Mukherjee,Prabhat Chand,Pratima Murthy*

Main category: cs.CL

TL;DR: 提出 LOGICAL 本地可部署的 PII 去识别系统，基于微调的 GLiNER，在电子健康记录中实现高精度的 PII 去识别（微平均 F1=0.980），显著优于对比模型；95% 文档完全去标识，CPU 条件下也能运行，适合资源受限环境；存在 2% 实体级漏识别，需人工干预。


<details>
  <summary>Details</summary>
Motivation: 在研究与 AI 开发中，需要在保护隐私的前提下去识别并去除临床笔记中的个人可识别信息（PII），同时避免高成本的云端大模型和 API 服务，尤其在资源有限的环境。

Method: 提出并微调 GLiNER（通用型且轻量化的命名实体识别模型）构成 LOGICAL。使用 1515 份来着自精神科医院的 EHR 文档，定义 9 类 PII 进行去识别；对 2849 条文本进行微调，使用 376 条测试文本，按字符级别评估，比较对象包括 Microsoft Azure NER、Microsoft Presidio、Gemini-Pro-2.5、Llama-3.3-70B-Instruct。模型在 CPU 笔记本上无专用 GPU 即可运行。

Result: 微平均 F1=0.980，显著优于 Gemini-Pro-2.5（F1=0.845）；LOGICAL 能 95% 的文档完全完成去标识，而下一最佳方案只有 64%；实体级漏检率约 2%，需跨模型的人工参与以确保安全性。31

Conclusion: 细化的专用变换模型如 GLiNER 提供了高准确性、较低计算资源需求与数据隐私保障的 PII 去识别方案，是对资源密集型大模型的有效替代；在“源头去标识”的策略下，可为研究与 AI 开发构建去识别数据集，同时在资源受限环境中提升可用性，建议结合人工复核以确保安全性。

Abstract: Removing Personally Identifiable Information (PII) from clinical notes in
Electronic Health Records (EHRs) is essential for research and AI development.
While Large Language Models (LLMs) are powerful, their high computational costs
and the data privacy risks of API-based services limit their use, especially in
low-resource settings. To address this, we developed LOGICAL (Local Obfuscation
by GLINER for Impartial Context-Aware Lineage), an efficient, locally
deployable PII removal system built on a fine-tuned Generalist and Lightweight
Named Entity Recognition (GLiNER) model. We used 1515 clinical documents from a
psychiatric hospital's EHR system. We defined nine PII categories for removal.
A modern-gliner-bi-large-v1.0 model was fine-tuned on 2849 text instances and
evaluated on a test set of 376 instances using character-level precision,
recall, and F1-score. We compared its performance against Microsoft Azure NER,
Microsoft Presidio, and zero-shot prompting with Gemini-Pro-2.5 and
Llama-3.3-70B-Instruct. The fine-tuned GLiNER model achieved superior
performance, with an overall micro-average F1-score of 0.980, significantly
outperforming Gemini-Pro-2.5 (F1-score: 0.845). LOGICAL correctly sanitised 95%
of documents completely, compared to 64% for the next-best solution. The model
operated efficiently on a standard laptop without a dedicated GPU. However, a
2% entity-level false negative rate underscores the need for human-in-the-loop
validation across all tested systems. Fine-tuned, specialised transformer
models like GLiNER offer an accurate, computationally efficient, and secure
solution for PII removal from clinical notes. This "sanitisation at the source"
approach is a practical alternative to resource-intensive LLMs, enabling the
creation of de-identified datasets for research and AI development while
preserving data privacy, particularly in resource-constrained environments.

</details>


### [36] [Modeling Turn-Taking with Semantically Informed Gestures](https://arxiv.org/abs/2510.19350)
*Varsha Suresh,M. Hamza Mughal,Christian Theobalt,Vera Demberg*

Main category: cs.CL

TL;DR: 扩展的 DnD Gesture++ 数据集用于研究在对话中的转话（turn-taking）预测，通过将文本、语音和手势信息结合在一个混合专家框架中，验证了语义手势对提升转话预测的贡献信号。


<details>
  <summary>Details</summary>
Motivation: 尽管语言和声学特征有信息量，手势提供互补的线索，尤其在多方对话的转话管理中。通过在 DnD Gesture 基础上增加 2,663 个语义手势标注，旨在探索语义层面的手势如何与文本/音频共同提升转话预测性能。

Method: 采用混合专家（Mixture-of-Experts,MoE）框架，将文本、音频与手势整合，用于转话预测。数据来自扩展后的 DnD Gesture++，并对手势进行了分为显像（iconic）、隐喻（metaphoric）、指示性（deictic）和话语型（discourse）等语义类型的标注，以引导学习。

Result: 实验结果显示，融入语义引导的手势比基线模型具有一致的性能提升，体现了多模态信息中手势的互补作用在转话预测中的价值。

Conclusion: 语义层面的手势可显著提升多模态对话中的转话预测效果，扩展数据集与混合专家融合策略共同促进对话能产出更自然的转话管理。

Abstract: In conversation, humans use multimodal cues, such as speech, gestures, and
gaze, to manage turn-taking. While linguistic and acoustic features are
informative, gestures provide complementary cues for modeling these
transitions. To study this, we introduce DnD Gesture++, an extension of the
multi-party DnD Gesture corpus enriched with 2,663 semantic gesture annotations
spanning iconic, metaphoric, deictic, and discourse types. Using this dataset,
we model turn-taking prediction through a Mixture-of-Experts framework
integrating text, audio, and gestures. Experiments show that incorporating
semantically guided gestures yields consistent performance gains over
baselines, demonstrating their complementary role in multimodal turn-taking.

</details>


### [37] [M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2510.19358)
*Yejin Kwon,Taewoo Kang,Hyunsoo Yoon,Changouk Kim*

Main category: cs.CL

TL;DR: M3-SLU 是一个面向多说话人、多轮对话的多模态大语言模型基准，聚焦说话者属性推理，汇集自四大开源语料并提供两项对话中的说话者识别任务。


<details>
  <summary>Details</summary>
Motivation: 现有的语音和文本理解模型在是谁在说、何时说等说话者属性推理方面仍显不足，缺乏专门的多模态基准来评估和提升这方面能力。

Method: 从 CHiME-6、MELD、MultiDialog、AMI 四个开源语料中构建数据集，汇集超过12,000 条经过验证的实例，包含音频、转写和元数据；设计两项任务：1) 说话者属性问答（Speaker-Attributed Question Answering）与 2) 通过话语匹配进行说话者归属（Speaker Attribution via Utterance Matching）；给出级联流水线与端到端 MLLMs 的基线，评估采用 LLM-as-Judge 与准确率。

Result: 实验显示模型能够捕捉到“说了什么”，但在识别“谁说的”方面存在显著不足，暴露说话者感知在对话理解中的核心挑战。

Conclusion: M3-SLU 提供一个具有挑战性的基准，旨在推动说话者感知的多模态理解研究及相关方法的发展。

Abstract: We present M3-SLU, a new multimodal large language model (MLLM) benchmark for
evaluating multi-speaker, multi-turn spoken language understanding. While
recent models show strong performance in speech and text comprehension, they
still struggle with speaker-attributed reasoning, the ability to understand who
said what and when in natural conversations. M3-SLU is built from four open
corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000
validated instances with paired audio, transcripts, and metadata. It includes
two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker
Attribution via Utterance Matching. We provide baseline results for both
cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and
accuracy metrics. Results show that while models can capture what was said,
they often fail to identify who said it, revealing a key gap in speaker-aware
dialogue understanding. M3-SLU offers as a challenging benchmark to advance
research in speaker-aware multimodal understanding.

</details>


### [38] [LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts](https://arxiv.org/abs/2510.19363)
*Siyuan Wang,Gaokai Zhang,Li Lyna Zhang,Ning Shang,Fan Yang,Dongyao Chen,Mao Yang*

Main category: cs.CL

TL;DR: LoongRL 提出数据驱动的强化学习方法，通过 KeyChain 将短多跳问答转化为高难度长上下文任务，在 16K 训练样本上实现对 128K 任务的高效解决，显著提升长上下文多跳问答性能，且保持短上下文能力。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文推理的挑战与缺乏高难度 RL 数据的瓶颈，探索从短上下文 RL 能力迁移到长上下文推理的可扩展方法。

Method: KeyChain 将短问答转化成长上下文任务，通过插入 UUID 链隐藏真实问题，需逐步追踪链路、检索相关事实并进行推理。通过 RL 训练获得计划-检索-推理-重新检查的演绎路径，具备对训练长度的泛化能力。对 16K 数据训练即可有效解决 128K 任务，降低全长度 RL 的成本。

Result: 在 Qwen2.5-7B 和 14B 模型上，长上下文多跳问答准确率分别提升约 23.5% 和 21.1% 的绝对值；LoongRL-14B 得分 74.2，接近 o3-mini(74.5) 与 DeepSeek-R1(74.9)；提升长上下文检索能力，能通过 128K 针对性测试（needle-in-a-haystack），并保留短上下文推理能力。

Conclusion: 本工作表明数据驱动的强化学习结合合成数据（KeyChain）能够催生出适用于长上下文推理的演绎性策略，且小尺度模型也能通过高效训练接近更大模型的性能，提升鲁棒性与检索能力。

Abstract: Reasoning over long contexts is essential for large language models. While
reinforcement learning (RL) enhances short-context reasoning by inducing "Aha"
moments in chain-of-thought, the advanced thinking patterns required for
long-context reasoning remain largely unexplored, and high-difficulty RL data
are scarce. In this paper, we introduce LoongRL, a data-driven RL method for
advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis
approach that transforms short multi-hop QA into high-difficulty long-context
tasks by inserting UUID chains that hide the true question among large
collections of distracting documents. Solving these tasks requires the model to
trace the correct chain step-by-step, identify the true question, retrieve
relevant facts and reason over them to answer correctly. RL training on
KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning
pattern that generalizes far beyond training length. Models trained at 16K
effectively solve 128K tasks without prohibitive full-length RL rollout costs.
On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA
accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches
a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5)
and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all
128K needle-in-a-haystack stress tests, and preserves short-context reasoning
capabilities.

</details>


### [39] [MoE-Prism: Disentangling Monolithic Experts for Elastic MoE Services via Model-System Co-Designs](https://arxiv.org/abs/2510.19366)
*Xinfeng Xia,Jiacheng Liu,Xiaofeng Hou,Peng Tang,Mingxuan Zhang,Wenfeng Wang,Chao Li*

Main category: cs.CL

TL;DR: MoE-Prism通过离线重构和在线调度，将MoE从刚性单体专家转化为弹性服务，提供更多可控运行点并改善吞吐/延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的Mixture-of-Experts（MoE）模型依赖于通过top-k将参数路由到少量单体专家，导致“质量跳跃”的格局，难以在成本与质量之间取得平衡；缺乏对不同服务水平目标（SLOs）的自适应性，易造成资源过度预分配。

Method: 提出两阶段模型-系统协同设计：1) 离线重构引擎，将单体专家拆分为细粒度的子专家，使用以元启发式为基础的分区优化求解器在不重新训练的条件下保持局部功能性。2) 在线调度引擎，基于QoS感知的调度策略，提升云端吞吐同时实现内存受限设备的延迟优化卸载。

Result: 在三个MoE模型上评估，MoE-Prism实现了比基线多4倍以上的、更稳定的不同工作点；在严格的延迟预算下，吞吐量提升可达约19.9%；在资源受限情境下，平均延迟可降低约10.36%。

Conclusion: MoE-Prism提供了连接模型与系统的关键“控制杆”，使AI服务具备自适应、高效且具有QoS感知能力的弹性能力，推动下一代自适应AI服务的发展。

Abstract: Mixture-of-Experts (MoE) models, the state-of-the-art in large-scale AI,
achieve high quality by sparsely activating parameters. However, their reliance
on routing between a few monolithic experts via a top-k mechanism creates a
"quality cliff", offering only a few coarse-grained operating points. This
inflexibility forces a difficult trade-off between cost and quality, preventing
adaptation to diverse Service Level Objectives (SLOs) and leading to
significant resource over-provisioning.
  This paper introduces MoE-Prism, a model-system co-design that transforms
rigid MoE models into elastic services. Our methodology is divided into two
phases. First, an \emph{Offline Refactoring Engine} systematically deconstructs
monolithic experts into fine-grained "sub-experts." This engine employs a
partitioning optimization solver that uses a metaheuristic-based approach to
group neurons, preserving functional locality without requiring retraining.
Second, an \emph{Online Scheduling Engine} leverages this new elasticity
through QoS-aware scheduling. It implements specialized policies to solve
complex system problems, including maximizing throughput in cloud deployments
and managing latency-optimized offloading for memory-constrained devices. Our
evaluation across three different MoE models shows that MoE-Prismprovides over
4 times more distinct, stable operating points than the baseline. This allows
an AI service to dynamically improve throughput by up to 19.9\% under a strict
latency budget or reduce latency by up to 10.36\% under limited resources.
MoE-Prism provides the critical "control knob" to bridge the model-system gap,
enabling the next generation of adaptive, efficient, and QoS-aware AI services.

</details>


### [40] [Sign Language Translation with Sentence Embedding Supervision](https://arxiv.org/abs/2510.19367)
*Yasser Hamidullah,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 提出一种无注释的SLT训练方式：用目标句子的句子嵌入作为伪 gloss 来监督训练，显著提升无 gloss 数据集上的翻译性能并缩小与有 gloss 系统的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的 gloss 标注在大规模数据中稀缺且跨数据集差异大，妨碍端到端或中间步骤的SLT系统的普及与泛化。

Method: 在训练时使用目标语言句子的嵌入作为 gloss 替代，学习一种从原始文本得到的监督信号。方法支持单语与多语嵌入，评估对象包括德语PHOENIX-2014T与美语How2Sign数据集，以及单语/多语翻译系统。

Result: 该伪 gloss 监督显著优于其他无 gloss 的方法，在无 gloss 数据集上达到新的状态-艺水平；且在未使用额外 SLT 数据进行 pretraining 的情况下，缩小了无 gloss 与有 gloss 系统之间的差距。

Conclusion: 以目标句子嵌入作为伪 gloss 的训练策略在多语言场景中有效，能够在缺少人工 gloss 的条件下实现有竞争力的SLT表现，推动无注释SLT研究的发展。

Abstract: State-of-the-art sign language translation (SLT) systems facilitate the
learning process through gloss annotations, either in an end2end manner or by
involving an intermediate step. Unfortunately, gloss labelled sign language
data is usually not available at scale and, when available, gloss annotations
widely differ from dataset to dataset. We present a novel approach using
sentence embeddings of the target sentences at training time that take the role
of glosses. The new kind of supervision does not need any manual annotation but
it is learned on raw textual data. As our approach easily facilitates
multilinguality, we evaluate it on datasets covering German (PHOENIX-2014T) and
American (How2Sign) sign languages and experiment with mono- and multilingual
sentence embeddings and translation systems. Our approach significantly
outperforms other gloss-free approaches, setting the new state-of-the-art for
data sets where glosses are not available and when no additional SLT datasets
are used for pretraining, diminishing the gap between gloss-free and
gloss-dependent systems.

</details>


### [41] [SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision](https://arxiv.org/abs/2510.19398)
*Yasser Hamidullah,Shakib Yazdani,Cennet Oguz,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 提出语言无关的多模态嵌入监督SLT，实现直接多语言翻译；通过耦合增强提升鲁棒性和BLEURT分数，尤其在低资源场景。


<details>
  <summary>Details</summary>
Motivation: 现有SLT多依赖单语言文本、或将gloss监督替代为文本嵌入，仍受语言与模态绑定限制，缺乏跨语言通用性与扩展性。

Method: 采用语言无关的多模态嵌入（文本和语音来自多语言）来监督SLT，并提出耦合增强：多语言目标扩增与视频级扰动的组合，以提高模型鲁棒性。

Result: 在BLEURT评价上，相比文本嵌入监督，获得一致提升，且在低资源设置中提升幅度更大。

Conclusion: 语言无关嵌入监督结合耦合增强，提供可扩展且语义鲁棒的SLT训练替代方案，便于跨语言扩展。

Abstract: Sign language translation (SLT) is typically trained with text in a single
spoken language, which limits scalability and cross-language generalization.
Earlier approaches have replaced gloss supervision with text-based sentence
embeddings, but up to now, these remain tied to a specific language and
modality. In contrast, here we employ language-agnostic, multimodal embeddings
trained on text and speech from multiple languages to supervise SLT, enabling
direct multilingual translation. To address data scarcity, we propose a coupled
augmentation method that combines multilingual target augmentations (i.e.
translations into many languages) with video-level perturbations, improving
model robustness. Experiments show consistent BLEURT gains over text-only
sentence embedding supervision, with larger improvements in low-resource
settings. Our results demonstrate that language-agnostic embedding supervision,
combined with coupled augmentation, provides a scalable and semantically robust
alternative to traditional SLT training.

</details>


### [42] [ToMMeR -- Efficient Entity Mention Detection from Large Language Models](https://arxiv.org/abs/2510.19410)
*Victor Morand,Nadi Tomeh,Josiane Mothe,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: ToMMeR demonstrates that mention detection can be largely recovered from early transformer layers using a tiny model, achieving high recall and precision, and that span-classification heads yield near-SOTA NER performance.


<details>
  <summary>Details</summary>
Motivation: Grounded in info extraction, mention detection is a foundational bottleneck. The paper investigates whether structured entity representations exist in early LM layers and if they can be efficiently recovered with small models, across diverse architectures.

Method: Probe early LLM layers with a lightweight model (<300K params) across 13 NER benchmarks to assess mention detection; compare across architectures from 14M–15B parameters; use an LLM as a judge for precision; extend ToMMeR with span classification heads to evaluate NER performance.

Result: Achieves 93% recall in zero-shot mention detection and >90% precision when judged by an LLM; cross-model DICE scores >75% across 14M–15B parameter architectures; with span heads, reaches 80–87% F1 on standard NER benchmarks.

Conclusion: Structured entity representations exist in early transformer layers and can be efficiently recovered with minimal parameters; mention detection emerges naturally from language modeling, and adding span-classification heads enables near-SOTA NER performance.

Abstract: Identifying which text spans refer to entities -- mention detection -- is
both foundational for information extraction and a known performance
bottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing
mention detection capabilities from early LLM layers. Across 13 NER benchmarks,
ToMMeR achieves 93\% recall zero-shot, with over 90\% precision using an LLM as
a judge showing that ToMMeR rarely produces spurious predictions despite high
recall. Cross-model analysis reveals that diverse architectures (14M-15B
parameters) converge on similar mention boundaries (DICE >75\%), confirming
that mention detection emerges naturally from language modeling. When extended
with span classification heads, ToMMeR achieves near SOTA NER performance
(80-87\% F1 on standard benchmarks). Our work provides evidence that structured
entity representations exist in early transformer layers and can be efficiently
recovered with minimal parameters.

</details>


### [43] [Spatio-temporal Sign Language Representation and Translation](https://arxiv.org/abs/2510.19413)
*Yasser Hamidullah,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 提出一个端到端的时空特征学习的手语翻译模型，在单一模型中同时学习时空特征与翻译；在开发集有约5 BLEU点的表现，但测试集显著下降至约0.11 BLEU点，表明泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 解决现有手语翻译通常将特征提取与翻译分离，未充分利用视频的时间动态，期望通过统一端到端模型提高对新数据集的泛化。

Method: 提出一个端到端的系统，使得时空特征表示与翻译在同一模型中学习；使用视频帧特征作为输入，直接从视频到文本的序列到序列学习，抛弃分离的特征提取阶段。

Result: 在开发集上获得 5±1 BLEU；但在测试集上显著下降，约 0.11±0.06 BLEU。

Conclusion: 端到端的时空SLT方法具有潜力，但在泛化方面存在明显挑战，需进一步改进正则化、数据增强或域适应等以提升测试集表现。

Abstract: This paper describes the DFKI-MLT submission to the WMT-SLT 2022 sign
language translation (SLT) task from Swiss German Sign Language (video) into
German (text). State-of-the-art techniques for SLT use a generic seq2seq
architecture with customized input embeddings. Instead of word embeddings as
used in textual machine translation, SLT systems use features extracted from
video frames. Standard approaches often do not benefit from temporal features.
In our participation, we present a system that learns spatio-temporal feature
representations and translation in a single model, resulting in a real
end-to-end architecture expected to better generalize to new data sets. Our
best system achieved $5\pm1$ BLEU points on the development set, but the
performance on the test dropped to $0.11\pm0.06$ BLEU points.

</details>


### [44] [BLiSS 1.0: Evaluating Bilingual Learner Competence in Second Language Small Language Models](https://arxiv.org/abs/2510.19419)
*Yuan Gao,Suchir Salhan,Andrew Caines,Paula Buttery,Weiwei Sun*

Main category: cs.CL

TL;DR: BLiSS 1.0 introduces a selective tolerance benchmark to compare naturalistic learner errors vs artificial errors, using 2.8M naturalistic learner sentences and 136,867 triplets; findings show selective tolerance is distinct from grammaticality and depends on training paradigm.


<details>
  <summary>Details</summary>
Motivation: To align evaluation with cognitively plausible language acquisition and distinguish models that mimic human-like learning patterns from standard performance benchmarks.

Method: Create a controlled triplet dataset (corrected, learner, artificial) from naturalistic learner sentences; test models on selecting the more plausible error; analyze across training paradigms; evaluate compatibility with human-like learning patterns.

Result: Selective tolerance is a distinct capability from grammaticality; performance clusters strongly by training paradigm, validating BLiSS as a robust tool to measure how training objectives shape alignment with human language acquisition patterns.

Conclusion: BLiSS provides a robust benchmark to evaluate cognitive alignment and the impact of training objectives on models' ability to mirror human interlingual syntactic structure and error patterns.

Abstract: To bridge the gap between performance-oriented benchmarks and the evaluation
of cognitively inspired models, we introduce BLiSS 1.0, a Benchmark of Learner
Interlingual Syntactic Structure. Our benchmark operationalizes a new paradigm
of selective tolerance, testing whether a model finds a naturalistic learner
error more plausible than a matched, artificial error within the same sentence.
Constructed from over 2.8 million naturalistic learner sentences, BLiSS
provides 136,867 controlled triplets (corrected, learner, artificial) for this
purpose. Experiments on a diverse suite of models demonstrate that selective
tolerance is a distinct capability from standard grammaticality, with
performance clustering strongly by training paradigm. This validates BLiSS as a
robust tool for measuring how different training objectives impact a model's
alignment with the systematic patterns of human language acquisition.

</details>


### [45] [MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models](https://arxiv.org/abs/2510.19457)
*Kailin Jiang,Ning Jiang,Yuchen Ren,Yuchen Li,Yifan Gao,Jinhe Bi,Yunpu Ma,Qingqing Liu,Xianhao Wang,Yifan Jia,Hongbo Jiang,Yaocong Hu,Bin Li,Lei Liu,Yuntao Du*

Main category: cs.CL

TL;DR: MINED 是一个面向时间敏感知识的综合基准，覆盖 6 个维度和 11 项任务，基于维基百科样本构建，评测 15 种大模型的时间理解能力，结果显示 Gemini-2.5-Pro 表现最好但总体仍有差距；并探讨通过知识编辑更新知识的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决大模型在时间敏感事实上的静态表征不足的问题。现有基准多为静态设计，难以充分评估模型对随时间变化的知识的掌握，需构建覆盖多维度的时间意识评测。

Method: 从维基百科抽取样本，由两名专业 annotator 标注，得到 2104 条时间敏感知识样本，覆盖六种知识类型；围绕六个维度和十一项任务设计评测；对 15 个常用大模型进行评估，采用 CEM 指标衡量；并尝试使用知识编辑方法更新时间敏感知识，评估编辑在单次编辑情境中的有效性。

Result: 在 15 种模型中，Gemini-2.5-Pro 的平均 CEM 得分最高，为 63.07；多为开源模型在时间理解能力上仍显不足；模型在组织类知识上表现最好，但在体育领域最差；知识编辑在单次编辑情境下可以较为有效地更新知识。

Conclusion: MINED 为评估大模型时间敏感知识提供了一个更全面的基准，揭示了当前模型在时间理解方面的不足及差异化表现，同时初步验证了知识编辑的更新潜力，但仍需进一步扩大覆盖、改进评价指标并验证多编辑场景的鲁棒性。

Abstract: Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal
pre-training, yet their static representations struggle to maintain an accurate
understanding of time-sensitive factual knowledge. Existing benchmarks remain
constrained by static designs, inadequately evaluating LMMs' ability to
understand time-sensitive knowledge. To address this gap, we propose MINED, a
comprehensive benchmark that evaluates temporal awareness along 6 key
dimensions and 11 challenging tasks: cognition, awareness, trustworthiness,
understanding, reasoning, and robustness. MINED is constructed from Wikipedia
by two professional annotators, containing 2,104 time-sensitive knowledge
samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED
shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,
while most open-source LMMs still lack time understanding ability. Meanwhile,
LMMs perform best on organization knowledge, whereas their performance is
weakest on sport. To address these challenges, we investigate the feasibility
of updating time-sensitive knowledge in LMMs through knowledge editing methods
and observe that LMMs can effectively update knowledge via knowledge editing
methods in single editing scenarios.

</details>


### [46] [Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition](https://arxiv.org/abs/2510.19471)
*Yuu Jinnai*

Main category: cs.CL

TL;DR: MBR 解码在 Whisper 系列模型上的英语/日语任务中，普遍优于束搜索，显示其在离线 ASR 与 ST 任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 将文本到文本任务中取得的样本基 MBR 解码的成功扩展到语音到文本任务，探索其在 ASR/ST 的高精度解码效果，并填补现有解码策略的空白。

Method: 在英语和日语的 ASR 与 ST 任务上，对 Whisper 及其派生模型进行评估，比较样本基 MBR 解码与束搜索；提供开源实现。

Result: MBR 解码在大多数实验设置中优于束搜索，显示其在离线高精度任务中的有效性与潜力。

Conclusion: MBR 是离线 ASR 与 ST 的有前景解码方法，值得进一步研究和广泛应用；论文附带开源代码。

Abstract: Recent work has shown that sample-based Minimum Bayes Risk (MBR) decoding
outperforms beam search in text-to-text generation tasks, such as machine
translation, text summarization, and image captioning. On the other hand, beam
search is the current practice for speech-to-text tasks such as automatic
speech recognition (ASR) and Speech Translation (ST). Given that MBR decoding
is effective in text-to-text generation tasks, it is reasonable to expect it to
also be effective for speech-to-text tasks. In this paper, we evaluate MBR
decoding for ASR and ST tasks on English and Japanese using Whisper and its
derivative models. We observe that the accuracy of MBR decoding outperforms
that of beam search in most of the experimental settings we have evaluated. The
results show that MBR decoding is a promising method for offline ASR and ST
tasks that require high accuracy. The code is available at
https://github.com/CyberAgentAILab/mbr-for-asr

</details>


### [47] [Machine Text Detectors are Membership Inference Attacks](https://arxiv.org/abs/2510.19492)
*Ryuto Koike,Liam Dugan,Masahiro Kaneko,Chris Callison-Burch,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 研究 MIAs 与机器文本检测的跨任务迁移性。理论上，两任务在最优度量上相同；经验证这类方法的近似程度与跨任务表现有强相关性；引入统一评测套件 MINT，且 Binoculars 在两任务均表现出强迁移力。


<details>
  <summary>Details</summary>
Motivation: 尽管目标不同，MIAs 与机器文本检测都利用语言模型分布信号，方法论存在共性，可能存在更强的跨任务方法与洞见。本研究旨在揭示迁移性、统一度量并促进跨任务协作。

Method: 理论上证明在两类任务中实现最佳性能的度量是同一度量；广泛的经验分析，评估 7 种现有 MIA 方法与 5 种文本检测方法在 13 个领域与 10 种文本生成器上的跨任务迁移；提出并实现 MINT 作为统一评测套件，集成来自两任务的 15 种方法。

Result: 在跨任务测试中观察到很强的等级相关性（rho > 0.6），即方法在一个任务中的强势通常在另一个任务也有强表现。Binoculars 原为文本检测工具，在 MIA 任务上也达到最前沿水平。

Conclusion: 强调需要更强的跨任务认知与协作，MINT 将促进公平评估和跨任务方法的发展，推动两研究领域的联合进步。

Abstract: Although membership inference attacks (MIAs) and machine-generated text
detection target different goals, identifying training samples and synthetic
texts, their methods often exploit similar signals based on a language model's
probability distribution. Despite this shared methodological foundation, the
two tasks have been independently studied, which may lead to conclusions that
overlook stronger methods and valuable insights developed in the other task. In
this work, we theoretically and empirically investigate the transferability,
i.e., how well a method originally developed for one task performs on the
other, between MIAs and machine text detection. For our theoretical
contribution, we prove that the metric that achieves the asymptotically highest
performance on both tasks is the same. We unify a large proportion of the
existing literature in the context of this optimal metric and hypothesize that
the accuracy with which a given method approximates this metric is directly
correlated with its transferability. Our large-scale empirical experiments,
including 7 state-of-the-art MIA methods and 5 state-of-the-art machine text
detectors across 13 domains and 10 generators, demonstrate very strong rank
correlation (rho > 0.6) in cross-task performance. We notably find that
Binoculars, originally designed for machine text detection, achieves
state-of-the-art performance on MIA benchmarks as well, demonstrating the
practical impact of the transferability. Our findings highlight the need for
greater cross-task awareness and collaboration between the two research
communities. To facilitate cross-task developments and fair evaluations, we
introduce MINT, a unified evaluation suite for MIAs and machine-generated text
detection, with implementation of 15 recent methods from both tasks.

</details>


### [48] [What is the Best Sequence Length for BABYLM?](https://arxiv.org/abs/2510.19493)
*Suchir Salhan,Richard Diehl Martinez,Zébulon Goriely,Paula Buttery*

Main category: cs.CL

TL;DR: 在 BabyLM 任务中，序列长度对预训练效果显著且依赖任务与架构，长期结论是需针对性选择长度而非统一设定。


<details>
  <summary>Details</summary>
Motivation: 探究在给定训练数据和计算预算下，固定较短序列长度的普遍做法是否仍适用于 BabyLM，以及长度对不同能力（如语法泛化与形态学类比推理）的影响。

Method: 在约1亿词的训练数据和固定计算预算下，对比125M参数的Mamba与OPT模型在不同序列长度设置下的预训练表现，评估对多任务的影响。

Result: 总体而言，较长的上下文通常更有利，但最佳长度受任务类型和模型架构影响而不同。短序列对语法泛化任务足够，较长上下文对形态学类比推理任务更具优势。

Conclusion: 应根据具体任务和模型架构在 BabyLM 预训练中选择合适的序列长度，而非盲目追求更长或固定长度。

Abstract: Transformer language models typically operate with a fixed-length context
window, which has grown in step with large-scale pretraining datasets. In the
BabyLM Challenge, however, many past submissions have defaulted to using much
shorter sequence lengths. We examine the impact of sequence length on BabyLM
pretraining, to answer the simple question: what sequence length should we be
using when training Baby LMs? Using 100M-word training data and fixed compute
budgets, we compare 125M-parameter Mamba and OPT models, finding that although
longer is often better, the optimal length depends on both task and
architecture. Shorter sequences are sufficient for grammatical generalization
tasks whereas longer contexts benefit morphological analogical reasoning tasks.

</details>


### [49] [Lookahead Routing for Large Language Models](https://arxiv.org/abs/2510.19506)
*Canbin Huang,Tianyuan Shi,Yuhua Zhu,Ruijun Chen,Xiaojun Quan*

Main category: cs.CL

TL;DR: 提出 Lookahead 框架，在路由决策中通过预测潜在模型输出的潜在表示来预测合适的模型，从而在不进行完整推理的情况下实现更高效的多模型路由；在七个公开基准上平均提升 7.7%。


<details>
  <summary>Details</summary>
Motivation: 现有基于输入的路由将任务仅作为分类问题处理，忽略输出过程中的隐性信息、意图与上下文线索，导致复杂/模糊查询的路由子最优。

Method: 建立 Lookahead 框架，预测潜在模型输出的潜在表示（latent representations），以在不执行完整推理前获取信息来指导路由决策；实现基于因果语言模型与掩码语言模型的两种方法；在七个公开基准上评估。

Result: 在指令跟随、数学推理、代码生成等领域的七个公开基准上，Lookahead 持续优于现有路由基线，平均提升 7.7%，并提供了代码实现（GitHub 链接）。

Conclusion: 通过预测输出潜在表示来实现更 informed 的路由，降低推理开销的同时提高多模型系统的整体表现，适用于复杂查询的路由优化。

Abstract: Large language model (LLM) routers improve the efficiency of multi-model
systems by directing each query to the most appropriate model while leveraging
the diverse strengths of heterogeneous LLMs. Most existing approaches frame
routing as a classification problem based solely on the input query. While this
reduces overhead by avoiding inference across all models, it overlooks valuable
information that could be gleaned from potential outputs and fails to capture
implicit intent or contextual nuances that often emerge only during response
generation. These limitations can result in suboptimal routing decisions,
particularly for complex or ambiguous queries that require deeper semantic
understanding. To address this challenge, we propose Lookahead, a routing
framework that "foresees" potential model outputs by predicting their latent
representations and uses these predictions to guide model selection, thus
enabling more informed routing without full inference. Within this framework,
we implement two approaches based on causal and masked language models.
Empirical evaluations across seven public benchmarks - spanning instruction
following, mathematical reasoning, and code generation - show that Lookahead
consistently outperforms existing routing baselines, achieving an average
performance gain of 7.7% over the state-of-the-art. Our code is available at
https://github.com/huangcb01/lookahead-routing.

</details>


### [50] [Which Evaluation for Which Model? A Taxonomy for Speech Model Assessment](https://arxiv.org/abs/2510.19509)
*Maureen de Seyssel,Eeshan Gunesh Dhekane*

Main category: cs.CL

TL;DR: 提出一个用于语音基础模型的统一三轴评估分类法：评估维度、所需模型能力、任务/协议要求三条正交轴，方便将评估与模型能力对齐，并揭示对韵律、交互和推理等方面评估的系统性不足，作为设计、解读和扩展评估基准的实践指南。


<details>
  <summary>Details</summary>
Motivation: 当前对语音基础模型的评估在任务和模型类型间缺乏统一性，导致不同模型在不同方面表现突出时难以进行公平对比。需要一个 principled 的框架来匹配评估方法与模型能力，以便选择合适的评估、解读结果、并指导未来基准设计。

Method: 提出包含三个正交轴的 taxonomy：1) 评估的方面（如表示学习、语音生成、交互式对话等），2) 试图完成任务所需的模型能力（如实时处理、生成能力等），3) 任务或协议要求（如微调数据量、人类判断等）。对现有的评估与基准进行分类映射，分析其对能力和方法学需求的对应关系，并揭示覆盖不足和方向性趋势。

Result: 提供了一个概念性基础和实际指南，帮助研究者在选择、解读和扩展语音模型评估时更具一致性和前瞻性，并明确了诸如韵律、交互与推理等方面的研究空白与优先级。

Conclusion: 该三轴 taxonomy 为对齐模型与评估方法提供了系统性框架，有助于未来基准的设计方针、跨任务的比较以及评估方法的扩展与改进。

Abstract: Speech foundation models have recently achieved remarkable capabilities
across a wide range of tasks. However, their evaluation remains disjointed
across tasks and model types. Different models excel at distinct aspects of
speech processing and thus require different evaluation protocols. This paper
proposes a unified taxonomy that addresses the question: Which evaluation is
appropriate for which model? The taxonomy defines three orthogonal axes: the
\textbf{evaluation aspect} being measured, the model capabilities required to
attempt the task, and the task or protocol requirements needed to perform it.
We classify a broad set of existing evaluations and benchmarks along these
axes, spanning areas such as representation learning, speech generation, and
interactive dialogue. By mapping each evaluation to the capabilities a model
exposes (e.g., speech generation, real-time processing) and to its
methodological demands (e.g., fine-tuning data, human judgment), the taxonomy
provides a principled framework for aligning models with suitable evaluation
methods. It also reveals systematic gaps, such as limited coverage of prosody,
interaction, or reasoning, that highlight priorities for future benchmark
design. Overall, this work offers a conceptual foundation and practical guide
for selecting, interpreting, and extending evaluations of speech models.

</details>


### [51] [Conditions for Catastrophic Forgetting in Multilingual Translation](https://arxiv.org/abs/2510.19546)
*Danni Liu,Jan Niehues*

Main category: cs.CL

TL;DR: Forgetting in multilingual fine-tuning is mainly driven by the model/data size ratio; instruction-following ability matters more than architecture; parameter-efficient fine-tuning does not reliably prevent forgetting; cross-lingual alignment reduces forgetting and enables transfer to unseen languages.


<details>
  <summary>Details</summary>
Motivation: The literature on multilingual fine-tuning reports fragmented results about when catastrophic forgetting occurs. A systematic, controlled study is needed to identify the conditions that trigger forgetting across languages.

Method: Systematic empirical study using machine translation as a testbed. Controlled experiments across model architectures, data scales, and fine-tuning approaches, comparing full vs. parameter-efficient fine-tuning and evaluating cross-lingual alignment."

Result: Key determinants of forgetting: the relative scale between model and data size is the primary factor. Instruction-following ability is more important for retaining multilingual knowledge than architectural choices. Parameter-efficient fine-tuning offers no clear advantage over full fine-tuning in mitigating forgetting. Cross-lingual alignment helps mitigate forgetting and promotes positive transfer to unseen target languages.

Conclusion: To mitigate forgetting, emphasize instruction-following capabilities and cross-lingual alignment. Architecture is less critical than tuning signals; ensure data scale is appropriate for the model, as mismatches drive forgetting. Parameter-efficient methods alone may not be sufficient; alignment strategies can both reduce forgetting and enable transfer to new languages.

Abstract: Fine-tuning multilingual foundation models on specific languages often
induces catastrophic forgetting, degrading performance on languages unseen in
fine-tuning. While this phenomenon is widely-documented, the literature
presents fragmented results about when forgetting occurs. To address this
ambiguity, we conduct a systematic empirical study using machine translation as
a testbed to identify the conditions that trigger catastrophic forgetting in
multilingual fine-tuning. Through controlled experiments across different model
architectures, data scales, and fine-tuning approaches, we reveal that the
relative scale between model and data size is a primary determinant of
forgetting. Moreover, we demonstrate that a model's instruction-following
ability is more critical for retaining multilingual knowledge than its
architecture. Contrary to assumptions, parameter-efficient fine-tuning offers
no clear advantage over full fine-tuning in mitigating forgetting. Lastly, we
show that cross-lingual alignment can mitigate forgetting while also
facilitating positive transfer to unseen target languages.

</details>


### [52] [Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark](https://arxiv.org/abs/2510.19585)
*Yu Wu,Ke Shu,Jonas Fischer,Lidia Pivovarova,David Rosson,Eetu Mäkelä,Mikko Tolonen*

Main category: cs.CL

TL;DR: 提出并评估在混合语言历史文献中提取拉丁文本片段的任务，基于724页多模态数据集对大模型进行基准测试，证明可实现的拉丁文本检测并首次分析模型能力与局限。


<details>
  <summary>Details</summary>
Motivation: 动机在于在历史文本中自动定位和提取拉丁文本片段，以支持学术研究、档案管理和历史信息检索，克服混合语言和多样排版带来的挑战。

Method: 对724页注释数据的多模态数据集进行基准测试，评估大规模基础模型在拉丁文本检测上的性能，并对不同模型、布局变体进行分析。

Result: 结果显示，使用当代模型可以实现可靠的拉丁文本检测，并对模型能力与局限进行了首次全面分析。

Conclusion: 结论表明该任务在现有大模型条件下具备可行性，首次系统分析了它们的能力与限制，并给出未来改进的方向和更大规模评估的必要性。

Abstract: This paper presents a novel task of extracting Latin fragments from
mixed-language historical documents with varied layouts. We benchmark and
evaluate the performance of large foundation models against a multimodal
dataset of 724 annotated pages. The results demonstrate that reliable Latin
detection with contemporary models is achievable. Our study provides the first
comprehensive analysis of these models' capabilities and limits for this task.

</details>


### [53] [PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models](https://arxiv.org/abs/2510.19616)
*Farhan Farsi,Shayan Bali,Fatemeh Valeh,Parsa Ghofrani,Alireza Pakniat,Kian Kashfipour,Amir H. Payberah*

Main category: cs.CL

TL;DR: PBBQ是一个面向波斯语语言模型的社会偏见基准数据集，包含16类文化主题、约37,000道问题，基于250名多样化受访者和社会科学专家的问卷开发。通过在多种开源/闭源及波斯语微调模型上评估，发现当前模型存在显著的社会偏见，并在输出中往往复制人类偏见模式。该数据集计划公开发布，用于评估与缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 当前对波斯语文化背景下的社会偏见资源匮乏，且大型语言模型的对齐问题需要在特定语言/文化场景下进行验证。该研究旨在填补波斯语情境中的偏见评估空白，促进模型的公平性与合规性。

Method: 通过250名多元背景参与者完成问卷，涵盖16个文化类别，开发出37,000多个经过精心筛选的问题；与社会科学专家合作以确保有效性。对多种开源和闭源模型，以及波斯语专用微调模型，在PBBQ上进行基准测试，比较模型输出与人类回答的偏差模式。

Result: 现有LLMs在波斯文化中表现出显著的社会偏见；与人类回答相比，模型输出往往重现人类偏见模式，体现了学习表征与文化刻板印象之间的复杂关系。

Conclusion: 一旦论文被接收，PBBQ数据集将向公众开放，以用于未来的评估与偏见缓解工作。内容警告：论文包含不安全内容。

Abstract: With the increasing adoption of large language models (LLMs), ensuring their
alignment with social norms has become a critical concern. While prior research
has examined bias detection in various languages, there remains a significant
gap in resources addressing social biases within Persian cultural contexts. In
this work, we introduce PBBQ, a comprehensive benchmark dataset designed to
evaluate social biases in Persian LLMs. Our benchmark, which encompasses 16
cultural categories, was developed through questionnaires completed by 250
diverse individuals across multiple demographics, in close collaboration with
social science experts to ensure its validity. The resulting PBBQ dataset
contains over 37,000 carefully curated questions, providing a foundation for
the evaluation and mitigation of bias in Persian language models. We benchmark
several open-source LLMs, a closed-source model, and Persian-specific
fine-tuned models on PBBQ. Our findings reveal that current LLMs exhibit
significant social biases across Persian culture. Additionally, by comparing
model outputs to human responses, we observe that LLMs often replicate human
bias patterns, highlighting the complex interplay between learned
representations and cultural stereotypes.Upon acceptance of the paper, our PBBQ
dataset will be publicly available for use in future work. Content warning:
This paper contains unsafe content.

</details>


### [54] [CrossNews-UA: A Cross-lingual News Semantic Similarity Benchmark for Ukrainian, Polish, Russian, and English](https://arxiv.org/abs/2510.19628)
*Daryna Dementieva,Evgeniya Sukhodolskaya,Alexander Fraser*

Main category: cs.CL

TL;DR: 提出一个可扩展且可解释的众包流程，构建跨语言新闻相似度数据集 CrossNews-UA，聚焦乌克兰语为中心并覆盖波兰语、俄语和英语，通过 4W（谁、什么、在哪里、何时）进行语义相似度标注，并对多种模型（从词袋到 Transformer 再到大语言模型）进行评估，揭示跨语言新闻分析的挑战与模型表现差异。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言新闻分析数据集多由记者/专家手工整理，难以扩展至新语言。需要可扩展、可解释的标注方式来提升跨语言对照的可用性和可解释性，从而更好地进行新闻真伪核验。

Method: 建立可扩展的众包流程，使用乌克兰语为中心语言，覆盖波兰语、俄语、英语，收集新闻对并用 4W 标注语义相似度并给出详细理由。对比多种模型：传统 Bag-of-Words、Transformer 基础架构、以及大型语言模型（LLMs）。

Result: 数据集 CrossNews-UA 已建立并展示了跨语言新闻分析中的挑战；不同模型在多语言场景下的表现存在差异，提供对模型能力与局限性的洞察。

Conclusion: 众包驱动的跨语言新闻相似度数据集具有可扩展性与可解释性潜力，可促进多语言新闻分析的发展，并为未来在新的语言上扩展提供方法论参考。

Abstract: In the era of social networks and rapid misinformation spread, news analysis
remains a critical task. Detecting fake news across multiple languages,
particularly beyond English, poses significant challenges. Cross-lingual news
comparison offers a promising approach to verify information by leveraging
external sources in different languages (Chen and Shu, 2024). However, existing
datasets for cross-lingual news analysis (Chen et al., 2022a) were manually
curated by journalists and experts, limiting their scalability and adaptability
to new languages. In this work, we address this gap by introducing a scalable,
explainable crowdsourcing pipeline for cross-lingual news similarity
assessment. Using this pipeline, we collected a novel dataset CrossNews-UA of
news pairs in Ukrainian as a central language with linguistically and
contextually relevant languages-Polish, Russian, and English. Each news pair is
annotated for semantic similarity with detailed justifications based on the 4W
criteria (Who, What, Where, When). We further tested a range of models, from
traditional bag-of-words, Transformer-based architectures to large language
models (LLMs). Our results highlight the challenges in multilingual news
analysis and offer insights into models performance.

</details>


### [55] [Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent](https://arxiv.org/abs/2510.19641)
*Yangshijie Zhang,Xinda Wang,Jialin Liu,Wenqiang Wang,Zhicong Ma,Xingxing Jia*

Main category: cs.CL

TL;DR: 提出了一种基于风格的攻击SAD，利用风格化文本（字体/表情符号）在模型中的分词差异，制造人机感知差，分为轻量版与强力版，在情感分析、机器翻译及一些商用服务中显示出强攻击性能，并可能威胁文本到图像/文本到语音等多模态任务。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的风格化文本流行，用户使用字体与表情来表达个性，但人类易读而模型把这些字符视为不同的标记，导致模型性能受影响，存在人机感知差导致的漏洞，需要评估与防御策略。

Method: 提出Style Attack Disguise SAD，设计轻量版与强力版两种规模，进行基于风格的对抗攻击评估，覆盖情感分类、机器翻译等任务，跨传统模型、大模型（LLM）及商业服务， 并初步展示对多模态生成任务的潜在威胁.

Result: 在多种模型与任务上，SAD表现出显著攻击能力，超出基线，且对多模态任务有潜在影响，证明风格化字符的对抗风险。

Conclusion: 风格化文本引发的分词与表征差异可成为NLP系统的安全隐患，需要进一步研究防御与鲁棒性提升，以及对多模态系统的综合评估。

Abstract: With social media growth, users employ stylistic fonts and font-like emoji to
express individuality, creating visually appealing text that remains
human-readable. However, these fonts introduce hidden vulnerabilities in NLP
models: while humans easily read stylistic text, models process these
characters as distinct tokens, causing interference. We identify this
human-model perception gap and propose a style-based attack, Style Attack
Disguise (SAD). We design two sizes: light for query efficiency and strong for
superior attack performance. Experiments on sentiment classification and
machine translation across traditional models, LLMs, and commercial services
demonstrate SAD's strong attack performance. We also show SAD's potential
threats to multimodal tasks including text-to-image and text-to-speech
generation.

</details>


### [56] [LLavaCode: Compressed Code Representations for Retrieval-Augmented Code Generation](https://arxiv.org/abs/2510.19644)
*Daria Cherniuk,Nikita Sukhorukov,Nikita Sushko,Daniil Gusak,Danil Sivtsov,Elena Tutubalina,Evgeny Frolov*

Main category: cs.CL

TL;DR: 通过将代码上下文压缩为少量的、语义丰富的单令牌向量，LlavaCode 在较小投影模块下实现更高质量的代码生成，同时显著减少检索上下文的需求，从而提升交互式代码完成的速度。


<details>
  <summary>Details</summary>
Motivation: 在需要来自代码仓库上下文的检索增强生成（RAG）场景中，完整的上下文会显著扩展序列长度，导致推理变慢，影响 IDE 等交互体验。

Method: 提出 LlavaCode 框架：将代码压缩成紧凑的、可由代码大模型解释的语义向量；使用一个小型投影模块将上下文映射为少量单令牌向量；对比完整的 RAG 管道，评估生成质量与延迟。

Result: 通过压缩上下文，EM 与 ES 指标显著提升，同时检索开销下降；TTFT 相比全 RAG 流程实现了 20-38% 的降低，几乎无显著延迟增加（延迟增量可忽略）。

Conclusion: 压缩后的上下文在保持或提升生成质量的同时显著降低了检索负担与推理时延，适合在 IDE 等互动场景中用于代码补全。

Abstract: Retrieval-augmented generation has emerged as one of the most effective
approaches for code completion, particularly when context from a surrounding
repository is essential. However, incorporating context significantly extends
sequence length, leading to slower inference - a critical limitation for
interactive settings such as IDEs. In this work, we introduce LlavaCode, a
framework that compresses code into compact, semantically rich representations
interpretable by code LLM, enhancing generation quality while reducing the
retrieved context to only a few compressed single-token vectors. Using a small
projector module we can significantly increase the EM and ES metrics of coding
model with negligible latency increase. Our experiments demonstrate that
compressed context enables 20-38% reduction in Time-to-First-Token (TTFT) on
line completion tasks compared to full-RAG pipelines.

</details>


### [57] [Unraveling Emotions with Pre-Trained Models](https://arxiv.org/abs/2510.19668)
*Alejandro Pajón-Sanmartín,Francisco De Arriba-Pérez,Silvia García-Méndez,Fátima Leal,Benedita Malheiro,Juan Carlos Burguillo-Rial*

Main category: cs.CL

TL;DR: 本研究比较微调与提示工程在情感识别中的效果，发现微调模型在情感识别上可达到70%以上的准确率；而LLMs若经过结构化的提示设计与情感分组亦能提升表现。


<details>
  <summary>Details</summary>
Motivation: 在开放文本中进行情感分析时，存在上下文歧义、语言变异及复杂情感表达等挑战，通用大模型难以直接应用，需要评估微调与提示设计对情感检测的有效性。

Method: 在三个场景下评估：(i) 微调的预训练模型与通用LLMs使用简单提示的性能；(ii) 不同情感提示设计在LLMs中的有效性；(iii) 情感分组技术对这些模型的影响。

Result: 实验结果显示：对比微调的预训练模型，性能达到70%+；LLMs若实施结构化提示与情感分组，亦能提升情感识别性能。

Conclusion: 结论表明针对情境的提示工程与情感分组可提升开放文本情感分析的效果，微调模型在情感识别任务上具有较强表现，相关方法有助于提升情感分析、人机交互与用户行为理解的应用。

Abstract: Transformer models have significantly advanced the field of emotion
recognition. However, there are still open challenges when exploring open-ended
queries for Large Language Models (LLMs). Although current models offer good
results, automatic emotion analysis in open texts presents significant
challenges, such as contextual ambiguity, linguistic variability, and
difficulty interpreting complex emotional expressions. These limitations make
the direct application of generalist models difficult. Accordingly, this work
compares the effectiveness of fine-tuning and prompt engineering in emotion
detection in three distinct scenarios: (i) performance of fine-tuned
pre-trained models and general-purpose LLMs using simple prompts; (ii)
effectiveness of different emotion prompt designs with LLMs; and (iii) impact
of emotion grouping techniques on these models. Experimental tests attain
metrics above 70% with a fine-tuned pre-trained model for emotion recognition.
Moreover, the findings highlight that LLMs require structured prompt
engineering and emotion grouping to enhance their performance. These
advancements improve sentiment analysis, human-computer interaction, and
understanding of user behavior across various domains.

</details>


### [58] [DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference](https://arxiv.org/abs/2510.19669)
*Xiang Liu,Xuming Hu,Xiaowen Chu,Eunsol Choi*

Main category: cs.CL

TL;DR: DiffAdapt 通过对问题难度和推理轨迹熵的估计，按 Easy/Normal/Hard 选择不同的推理策略，减少 token 消耗并保持或提升准确性，且无需对基础 LLM 微调。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在推理过程中的“过度思考”与高计算成本问题，寻求更高效的推理路径。

Method: 构建一个轻量探针，对 LLM 的最终隐藏状态进行分类，基于难度与熵值动态选择 Easy/Normal/Hard 三类推理策略；每类策略使用固定提示、温度和最大 token 长度；不微调基础 LLM，仅微调探针；在五个模型和八项基准上进行评估。

Result: 在可比甚至更高的准确度基础上，将 token 使用量降低最多约 22.4%。

Conclusion: 提出了一种实际可行的高效推理路径，展示了基于难度自适应推理在保持性能前提下实现计算效率提升的潜力。

Abstract: Recent reasoning Large Language Models (LLMs) demonstrate remarkable
problem-solving abilities but often generate long thinking traces whose utility
is unclear. Our work aims to improve their efficiency, enabling them to reach
high performance without overthinking. First, we analyze the entropy of token
probabilities in reasoning traces. Across three models, we observe a consistent
U-shaped entropy pattern: high entropy on easy problems despite high accuracy,
low entropy on problems with medium difficulty, and high entropy on hard
problems reflecting uncertainty. Specifically, we notice 22--25\% entropy
reduction from easy to medium difficulty regions, suggesting an {overthinking}
phenomenon on easy instances. Building on these insights, we introduce
\textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard
inference strategies per question based on their difficulty and reasoning trace
entropy. Each inference strategy consists of a fixed prompt, temperature and
maximum token length. In contrast to existing efficiency optimization methods,
our approach does not fine-tune base LLM but a small probe that classifies
LLM's final hidden state, allowing inexpensive adaptation. We comprehensively
evaluate our method on five models and eight benchmarks. Our method achieves
comparable or improved accuracy while reducing token usage by up to 22.4\%,
establishing a practical path toward compute-efficient reasoning.

</details>


### [59] [CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation](https://arxiv.org/abs/2510.19670)
*Hasan Akgul,Mari Eplik,Javier Rojas,Aina Binti Abdullah,Pieter van der Merwe*

Main category: cs.CL

TL;DR: CoSense-LLM 提出一个边缘优先的多模态传感数据到语义表达的系统，通过 SenseFusion、Edge-RAG、PromptRouter 和 Secure Execution，将连续传感流转化为离散语义码，配合边缘检索与云端协作，实现低延迟、低能耗、低带宽与隐私保护的可验证推理。


<details>
  <summary>Details</summary>
Motivation: 在严格的时延、带宽、能耗与隐私约束下，让大语言模型能够利用连续传感数据（如 Wi-Fi CSI、IMU、音频、RFID、视觉等）进行推理，同时提升事实一致性并降低成本。

Method: 提出四大组件：SenseFusion 将传感嵌入对齐并压缩为短的离散码序列；Edge-RAG 在本地进行混合检索，将生成绑定到站点策略与 notes；PromptRouter 基于成本和不确定性选择边缘生成、边缘+检索或云端升级；Secure Execution 提供可审计的脱敏路径，确保数据最小化且原始波形不离设备。结合分页/流式 KV 缓存、FlashAttention、 speculative decoding、量化 LoRA 等 serving 优化，支持设备端个性化和联邦更新，适用于家庭、办公、诊所等场景。

Result: 在边缘主导路径下实现子秒级 p95 总延迟，偏向本地检索以降低跨层令牌和带宽成本；仅传输离散码和脱敏元数据以保护隐私；Edge-RAG 提升事实一致性，经过标定的不确定性支持选择性陷入与受控升级；KV 与解码加速器降低每次决策能耗。结果支持边缘优先设计，将语义、隐私、可预测延迟视为同等目标，适应干扰多的环境中的大模型部署。

Conclusion: 该工作证明了在多模态传感输入下的边缘优先架构可实现更强的隐私保护、可控的延迟以及更低的能耗，同时维持对话与推理的语义准确性和可解释性。

Abstract: We present CoSense-LLM, an edge-first framework that turns continuous
multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and
lightweight vision) into compact, verifiable semantic tokens and coordinates
with large language models under explicit latency, energy, bandwidth, and
privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight
encoder that aligns sensor embeddings with language and compresses them into
short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer
that grounds generation in site specific policies and notes; (iii)
PromptRouter, a cost and uncertainty aware policy that selects edge only
generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure
Execution, an auditable redaction path that enforces data minimization so raw
waveforms never leave the device. The system works with modern serving
optimizations, including paged or streaming KV caches, FlashAttention style
kernels, speculative decoding, and quantized LoRA adapters, and supports on
device personalization and federated updates under non IID drift. Across home,
office, and clinic deployments, CoSense-LLM delivers grounded explanations
while meeting tight service level objectives: it sustains sub second (p95) end
to end latency on edge dominant paths, reduces inter tier token and bandwidth
costs by preferring local retrieval grounded responses, and preserves privacy
by transmitting only discrete codes and redacted metadata. Ablations show that
Edge-RAG improves factual consistency and reduces contradictions, calibrated
uncertainty enables selective abstention and controlled escalations, and KV
plus decoding accelerators lower energy per decision. The results support an
edge first design that treats semantics, privacy, and predictable latency as co
equal goals for large model deployments in interference prone environments.

</details>


### [60] [Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings](https://arxiv.org/abs/2510.19694)
*Cesar Gonzalez-Gutierrez,Dirk Hovy*

Main category: cs.CL

TL;DR: Prompt embeddings的质量受提示影响，但与任务相关性的提升并不总是带来更高的内部表示质量，提示相关性与表示之间不存在简单的一致关系，需要探究其他影响因素。


<details>
  <summary>Details</summary>
Motivation: 理解提示在零样本任务中如何影响LM的内部表示，以及预训练嵌入如何支持就地（in-context）任务求解；挑战“更相关提示等于更好表示”的假设。

Method: 通过一系列探测实验研究提示嵌入，分析用于零-shot分类的多种提示模板组合，评估提示对内部表示的影响。

Result: 提示确实会影响表示质量，但这些变化与任务相关提示的相关性并非稳定或一致地相关，无法简单地用相关性强的提示来预测更好的表示。

Conclusion: 需要重新审视提示设计对表示质量的作用，可能存在其他因素影响表示学习；未来工作应揭示影响提示诱导表示的其他因素，并给出更可靠的预测指标。

Abstract: Prompting is a common approach for leveraging LMs in zero-shot settings.
However, the underlying mechanisms that enable LMs to perform diverse tasks
without task-specific supervision remain poorly understood. Studying the
relationship between prompting and the quality of internal representations can
shed light on how pre-trained embeddings may support in-context task solving.
In this empirical study, we conduct a series of probing experiments on prompt
embeddings, analyzing various combinations of prompt templates for zero-shot
classification. Our findings show that while prompting affects the quality of
representations, these changes do not consistently correlate with the relevance
of the prompts to the target task. This result challenges the assumption that
more relevant prompts necessarily lead to better representations. We further
analyze potential factors that may contribute to this unexpected behavior.

</details>


### [61] [From Answers to Guidance: A Proactive Dialogue System for Legal Documents](https://arxiv.org/abs/2510.19723)
*Ashish Chouhan,Michael Gertz*

Main category: cs.CL

TL;DR: 提出 EUDial 数据集与 LexGuide 框架，通过主动、结构化的多轮对话来提升公民对欧盟法律信息的理解与获取。


<details>
  <summary>Details</summary>
Motivation: 弥合欧盟法律信息的可访问性与公民理解之间的差距；现有开源资源虽可用但对非专业人士不友好；需要一个能主动引导的对话系统。

Method: 基于欧盟议会研究服务 AskEP 的 204 篇博客，构建 880 轮对话（平均每轮 4.3 次），包含初始问题、结构化答案、后续问题；提出 LexGuide，结合检索增强生成、分层主题组织来规划对话进程。

Result: 实验显示，主动、结构化导航实现更全面的法律要点覆盖和对话连贯性；EUDial 与 LexGuide 可作为推进主动法律对话系统的实用资源。

Conclusion: EUDial 与 LexGuide 为面向公民的法律对话系统提供了可落地的数据与方法，促进法律信息的可理解性与可用性。

Abstract: The accessibility of legal information remains a constant challenge,
particularly for laypersons seeking to understand and apply complex
institutional texts. While the European Union provides open access to
legislation, parliamentary responses, and regulatory documents, these resources
can be challenging for laypeople to explore. In this paper, we introduce
EUDial, a proactive multi-turn dialogue dataset constructed from 204 blogs
curated by the Citizens' Enquiries Unit (AskEP) of the European Parliamentary
Research Service. EUDial contains 880 dialogue turns (averaging 4.3 turns per
dialogue), where each dialogue includes initial questions, structured answers,
and follow-up questions. Beyond dataset construction, we propose the LexGuide
framework that leverages retrieval-augmented generation with hierarchical topic
organization to structure dialogue progression, ensuring both comprehensive
coverage of legal aspects and coherence across conversational turns. The
results demonstrate that proactive, structured navigation closes the gap
between the availability of legal information and citizen comprehension,
establishing EUDial and LexGuide as practical resources for advancing proactive
legal dialogue systems.

</details>


### [62] [SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration](https://arxiv.org/abs/2510.19767)
*Xichen Zhang,Sitong Wu,Haoru Tan,Shaozuo Yu,Yinghao Zhu,Ziyi He,Jiaya Jia*

Main category: cs.CL

TL;DR: SmartSwitch 是一种插拔式推理框架，旨在解决长链推理中的 underthinking，通过感知模块实时监控推理过程、评估潜在想法的价值，并在必要时干预以深化探索，从而提升不同规模的大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决 LongCoT 过程中的 underthinking 问题，即模型在推理中浅层思考、频繁切换而缺乏充分探索，导致性能与 token 效率下降。

Method: 感知模块识别推理中的切换点并用一个现成的过程奖励模型（PRM）评估前一想法的潜力；若发现前一高潜力的想法被过早放弃，干预模块中断当前推理，回退到切换前的位置，并插入深化提示以促进沿着更有潜力的路径深入探索。

Result: 在具有挑战性的数学推理基准上对不同规模的大语言模型进行了广泛实验，结果表明该方法显著提升模型推理性能。

Conclusion: 该框架为长链推理提供一个简单且可插拔的解决方案，有效缓解 underthinking 问题，并提升推理深度与效率，且可无缝集成到现有的大语言模型中。

Abstract: The long chain-of-thought (LongCoT) capability is central to the recent
breakthroughs achieved by large language models in complex reasoning tasks.
However, the accompanying issue of ''underthinking'', where models exhibit
shallow reasoning by frequently switching thoughts without sufficient
exploration, limits both performance and token efficiency. To address this
problem, we propose a simple yet effective reasoning strategy: the SmartSwitch
inference framework. This framework can be easily integrated into any large
language model as a plug-and-play solution, continuously monitoring the model's
reasoning process to detect underthinking and guide it toward deeper
exploration of promising but overlooked thoughts. Specifically, the perception
module identifies points where thoughts switch and evaluates the potential of
the preceding thought using an off-the-shelf process reward model (PRM). If a
high-potential thought is found to be prematurely abandoned, the intervention
module interrupts the ongoing inference, backtracks to the point before the
switch, and inserts a "deepening prompt" to encourage further exploration along
that promising path. Extensive experiments on challenging mathematical
reasoning benchmarks demonstrate that our method significantly enhances the
performance of various large language models of different sizes.

</details>


### [63] [AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders](https://arxiv.org/abs/2510.19779)
*Yuezhou Hu,Jiaxin Guo,Xinyu Feng,Tuo Zhao*

Main category: cs.CL

TL;DR: AdaSPEC 在知识蒸馏中引入选择性令牌筛选，通过参考模型筛掉难以拟合的令牌，使草拟模型在简单令牌上更好对齐目标模型，从而提高令牌接受率并提升整体生成质量，优于 DistillSpec。


<details>
  <summary>Details</summary>
Motivation: Speculative Decoding 依赖小草拟模型产生预测，需与大目标模型对齐；传统 KD 目标是最小化所有令牌的 KL 散度，与 SD 的目标（提高接受率）不完全一致，导致草拟模型受限，难以充分吸收目标知识。

Method: 引入参考模型对令牌难度进行评估，筛选出难以拟合的令牌并从蒸馏中剔除；对剩余的简单令牌进行 KD，使草拟模型在这些令牌上更接近目标模型。通过这种选择性 KD，提升整体令牌接受率，同时保持或提升生成质量。对算术推理、指令执行、编程和摘要等任务进行评估，模型配置包含 31M/1.4B 和 350M/2.7B，比较 DistillSpec，结果显示 AdaSPEC 在所有任务中接受率提升最多 15%。代码开源。

Result: 在所有评估任务中持续超越 DistillSpec；令牌接受率提升可达 15% 左右；在多种模型规模下均显示鲁棒性。

Conclusion: 通过在 KD 过程中引入选择性令牌筛选，AdaSPEC 有效提升 SD 的对齐性与令牌接受率，同时不显著损害生成质量；方法对多任务与不同模型规模具备良好泛化性，且代码公开便于复现实验。

Abstract: Speculative Decoding (SD) accelerates large language model inference by
employing a small draft model to generate predictions, which are then verified
by a larger target model. The effectiveness of SD hinges on the alignment
between these models, which is typically enhanced by Knowledge Distillation
(KD). However, conventional KD methods aim to minimize the KL divergence
between the draft and target models across all tokens, a goal that is
misaligned with the true objective of SD, which is to maximize token acceptance
rate. Therefore, draft models often struggle to fully assimilate the target
model's knowledge due to capacity constraints, leading to suboptimal
performance. To address this challenge, we propose AdaSPEC, a novel method that
incorporates selective token filtering into the KD process. AdaSPEC utilizes a
reference model to identify and filter out difficult-to-fit tokens, enabling
the distillation of a draft model that better aligns with the target model on
simpler tokens. This approach improves the overall token acceptance rate
without compromising generation quality. We evaluate AdaSPEC across diverse
tasks, including arithmetic reasoning, instruction-following, coding, and
summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.
Our results demonstrate that AdaSPEC consistently outperforms the
state-of-the-art DistillSpec method, achieving higher acceptance rates across
all tasks (up to 15\%). The code is publicly available at
https://github.com/yuezhouhu/adaspec.

</details>


### [64] [Adapting Multilingual Models to Code-Mixed Tasks via Model Merging](https://arxiv.org/abs/2510.19782)
*Prashant Kodali,Vaishnavi Shivkumar,Swarang Joshi,Monojit Choudhary,Ponnurangam Kumaraguru,Manish Shrivastava*

Main category: cs.CL

TL;DR: 通过在代码混合文本上进行继续预训练并将得到的适配检查点与基模型合并，再进行下游任务微调，显著优于直接微调和仅 CPT 的方案，且在跨语言组合转移方面表现更强。


<details>
  <summary>Details</summary>
Motivation: 探究模型合并（model merging）作为对抗传统适配策略的实际备选路径，特别针对代码混合场景，评估其对下游分类任务的效果、跨对/跨语言的迁移能力，以及在不同数据场景下的适配策略。

Method: 从多语言基模型出发：先在未标注的代码混合文本上进行继续预训练(CPT)以获得适配检查点；将该检查点与基模型合并；再在下游任务数据上进行微调。评估对象为英文-印地语(En-Hi)和英文-西班牙语(En-Es)的句子分类（情感、仇恨言论），使用XLM-R和Llama-3.2-1B等模型。比较基线：全微调、CPT→FT，以及跨对语言的转移评估（En-Hi 训练后在 En-Ta、En-Ml 上测试）。

Result: 合并后的模型在大多数场景下优于全微调和 CPT→FT；F1 提升约2–5点相对于全微调，约1–2点优于 CPT→FT；零-shot/少样本提示在大型LLM上仍落后于微调与合并后的模型。跨对语言转移中，合并模型的迁移力比单语英语基线更强（如 TV/TIES 在 0.65–0.68 F1，相比全微调的 0.61–0.63），表明代码混合知识对低资源对具有更高的鲁棒性。

Conclusion: 给出适合常见数据情境（仅标注数据、标注+未标注、仅转移）的适配策略；并讨论对更大规模模型及更广任务场景的局限性与扩展性。

Abstract: We study model merging as a practical alternative to conventional adaptation
strategies for code-mixed NLP. Starting from a multilingual base model, we: (i)
perform continued pre-training (CPT) on unlabeled code-mixed text to obtain an
adapted checkpoint, (ii) merge checkpoint with the base model, and (iii)
fine-tune (FT) on the downstream task data. We evaluate our approach for
sentence classification (sentiment and hate speech) task in English-Hindi
(En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our
results show that merged models consistently outperform full fine-tuning and
CPT->FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2
points over CPT->FT, indicating that unlabeled data is leveraged more
effectively via merging than via CPT alone. Zero-/few-shot prompting with
larger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged
checkpoints, underscoring limits of in-context learning for code-mixed inputs.
We further test cross-pair transfer by training on En-Hi and evaluating on
En-Ta and En-Ml: merged checkpoints transfer more strongly than
monolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs
0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more
reliable substrate for low-resource pairs. We conclude with adaptation recipes
matched to common data regimes (labeled only; labeled+unlabeled; transfer-only)
and discuss limitations and scaling considerations for broader tasks and larger
models.

</details>


### [65] [The Art of Asking: Multilingual Prompt Optimization for Synthetic Data](https://arxiv.org/abs/2510.19806)
*David Mora,Viraat Aryabumi,Wei-Yin Ko,Sara Hooker,Julia Kreutzer,Marzieh Fadaee*

Main category: cs.CL

TL;DR: 本论文提出了通过优化提示空间而非简单翻译来提升多语言大模型的性能。引入一个轻量级的提示空间优化框架，对翻译后的提示进行自然度、文化适配和难度增强的变换，在12种语言、7大语言家族上应用，显著优于仅翻译基线，体现出更高的鲁棒性与文化贴近性。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大模型多使用翻译驱动的提示，这种做法往往以英语为中心，忽略文化维度，限制模型的泛化能力。需要挖掘并利用提示输入空间本身以改善跨语言表现。

Method: 提出一个轻量级的提示空间优化框架，对翻译后的提示进行系统性变换，涵盖自然度（Naturalness）、文化适配（Cultural Adaptation）、难度增强（Difficulty Enhancement）。基于现成的多语言大模型，在12种语言、7个语言家庭上对提示进行变换并评估。

Result: 在相同数据条件下，相比翻译为基线，方法在下游任务上获得显著且稳定的提升：Global-MMLU准确率提升4.7%，Flores XCometXL提升2.4%，mArenaHard偏好度提升35.3%。

Conclusion: 将提示空间优化作为一种简单但强大的范式，有助于构建更鲁棒、文化更贴近、全球化能力更强的多语言大模型。

Abstract: Synthetic data has become a cornerstone for scaling large language models,
yet its multilingual use remains bottlenecked by translation-based prompts.
This strategy inherits English-centric framing and style and neglects cultural
dimensions, ultimately constraining model generalization. We argue that the
overlooked prompt space-the very inputs that define training
distributions-offers a more powerful lever for improving multilingual
performance. We introduce a lightweight framework for prompt-space
optimization, where translated prompts are systematically transformed for
Naturalness, Cultural Adaptation, and Difficulty Enhancement. Using an
off-the-shelf multilingual LLM, we apply these transformations to prompts for
12 languages spanning 7 families. Under identical data conditions, our
approaches achieve substantial and consistent downstream improvements over the
translation-only baseline: +4.7% on Global-MMLU accuracy, +2.4% on Flores
XCometXL and +35.3% wins in preferences on mArenaHard. We establish
prompt-space optimization as a simple yet powerful paradigm for building
multilingual LLMs that are more robust, culturally grounded, and globally
capable.

</details>


### [66] [Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning](https://arxiv.org/abs/2510.19807)
*Xichen Zhang,Sitong Wu,Yinghao Zhu,Haoru Tan,Shaozuo Yu,Ziyi He,Jiaya Jia*

Main category: cs.CL

TL;DR: 提出Scaf-GRPO，一种分阶段训练框架，在模型学习停滞时注入层级提示，从而突破学习悬崖并提升在AIME24等数学基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 解决学习悬崖导致的零奖励信号问题，使策略优化在高难度任务中仍有梯度可用，提升LLMs的自我推理能力。

Method: 诊断学习停滞，逐步注入提示：从抽象概念到具体步骤的分层提示，在模型独立学习停滞时提供最小化的帮助，帮助模型自行构建解答。

Result: 在Qwen2.5-Math-7B对AIME24基准测试上，相较于vanilla GRPO提升相对44.3%的pass@1。

Conclusion: 该框架有效解锁模型的高难度推理能力，推进自主推理边界。

Abstract: Reinforcement learning from verifiable rewards has emerged as a powerful
technique for enhancing the complex reasoning abilities of Large Language
Models (LLMs). However, these methods are fundamentally constrained by the
''learning cliff'' phenomenon: when faced with problems far beyond their
current capabilities, models consistently fail, yielding a persistent
zero-reward signal. In policy optimization algorithms like GRPO, this collapses
the advantage calculation to zero, rendering these difficult problems invisible
to the learning gradient and stalling progress. To overcome this, we introduce
Scaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive
training framework that strategically provides minimal guidance only when a
model's independent learning has plateaued. The framework first diagnoses
learning stagnation and then intervenes by injecting tiered in-prompt hints,
ranging from abstract concepts to concrete steps, enabling the model to
construct a valid solution by itself. Extensive experiments on challenging
mathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the
pass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative
44.3% over a vanilla GRPO baseline. This result demonstrates our framework
provides a robust and effective methodology for unlocking a model's ability to
solve problems previously beyond its reach, a critical step towards extending
the frontier of autonomous reasoning in LLM.

</details>


### [67] [Hubble: a Model Suite to Advance the Study of LLM Memorization](https://arxiv.org/abs/2510.19811)
*Johnny Tian-Zheng Wei,Ameya Godbole,Mohammad Aflah Khan,Ryan Wang,Xiaoyuan Zhu,James Flemings,Nitya Kashyap,Krishna P. Gummadi,Willie Neiswanger,Robin Jia*

Main category: cs.CL

TL;DR: Hubble 是一组开源的语言模型，用于系统研究模型记忆（memorization），通过标准和扰动版本来分析敏感数据在不同训练条件下的记忆行为，提供可重复的基准和实验洞见，使得人口化/机器学习撤回等研究成为可能。


<details>
  <summary>Details</summary>
Motivation: 深入理解大型语言模型在训练数据中的敏感信息记忆机制，以及在不同数据频次和训练阶段安排下的可控性与可忘性，促进安全和隐私友好的模型开发。

Method: 发布8个模型（1B/8B参数，100B/500B tokens）以及6个扰动模型，标准模型在大英文语料上预训练，扰动模型在训练中插入受控文本（如书籍段落、传记、测试集）以模拟记忆风险。比较不同数据频率与语料规模对记忆的影响，展示在不同预训练阶段插入文本对记忆的持续性影响，评估记忆的“忘记”条件。

Result: 记忆风险与敏感数据在训练语料相对频率及语料规模相关；同一密码在较小语料中更易被记住，且若持续接触不足，敏感数据亦可被遗忘。提出两条实践建议：通过扩大语料规模稀释敏感数据、以及让敏感数据更早出现在训练中以减少长期记忆。Hubble 还提供了用于成员推断和机器取消学习的理想测试平台，并邀请社区进一步基准和扩展研究。

Conclusion: Hubble 为记忆研究提供了可控且可重复的开源基准，明确了影响记忆的因素并展示了可忘记的情境，具备成为成员推断与机器删记等任务测试床的潜力，值得研究者和工程实践者共同参与。

Abstract: We present Hubble, a suite of fully open-source large language models (LLMs)
for the scientific study of LLM memorization. Hubble models come in standard
and perturbed variants: standard models are pretrained on a large English
corpus, and perturbed models are trained in the same way but with controlled
insertion of text (e.g., book passages, biographies, and test sets) designed to
emulate key memorization risks. Our core release includes 8 models -- standard
and perturbed models with 1B or 8B parameters, pretrained on 100B or 500B
tokens -- establishing that memorization risks are determined by the frequency
of sensitive data relative to size of the training corpus (i.e., a password
appearing once in a smaller corpus is memorized better than the same password
in a larger corpus). Our release also includes 6 perturbed models with text
inserted at different pretraining phases, showing that sensitive data without
continued exposure can be forgotten. These findings suggest two best practices
for addressing memorization risks: to dilute sensitive data by increasing the
size of the training corpus, and to order sensitive data to appear earlier in
training. Beyond these general empirical findings, Hubble enables a broad range
of memorization research; for example, analyzing the biographies reveals how
readily different types of private information are memorized. We also
demonstrate that the randomized insertions in Hubble make it an ideal testbed
for membership inference and machine unlearning, and invite the community to
further explore, benchmark, and build upon our work.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality](https://arxiv.org/abs/2510.18982)
*Arpan Mukherjee,Marcello Bullo,Debabrota Basu,Deniz Gündüz*

Main category: cs.AI

TL;DR: 提出一个将可验证测试时扩展视为运输问题的框架，揭示覆盖率、验证器收敛域和子最优性之间的三段式关系，并分析序列/分批两类采样算法的计算复杂度，以及在Qwen、Llama、Gemma上的实证验证。


<details>
  <summary>Details</summary>
Motivation: 当前测试时缩放结合验证器提高了LLM性能，但验证器的角色及其不完美性尚未被充分研究；需要一个统一框架来量化覆盖率、验证器的收敛区域与采样子最优性之间的几何关系。

Method: 将可验证测试时扩展建模为运输问题，定义覆盖率、验证器ROC和子最优性，推导出三种子最优性随覆盖率变化的三种工作状态（运输、策略改进、饱和）; 设计并分析两类采样算法（顺序和分批），讨论其计算复杂度对权衡的影响。

Result: 理论上揭示三种 regimes，并通过对Qwen、Llama、Gemma模型的实验验证了理论结论。

Conclusion: 提出一个统一框架来理解测试时扩展中的验证器影响，为设计采样算法和验证器提供指引，并在多模型上获得一致性结果。

Abstract: While test-time scaling with verification has shown promise in improving the
performance of large language models (LLMs), the role of the verifier and its
imperfections remain underexplored. The effect of verification manifests
through interactions of three quantities: (i) the generator's coverage, (ii)
the verifier's region of convergence (ROC), and (iii) the sampling algorithm's
sub-optimality. Though recent studies capture subsets of these factors, a
unified framework quantifying the geometry of their interplay is missing. We
frame verifiable test-time scaling as a transport problem. This characterizes
the interaction of coverage, ROC, and sub-optimality, and uncovers that the
sub-optimality--coverage curve exhibits three regimes. A transport regime --
where sub-optimality increases with coverage, a policy improvement regime --
where sub-optimality may decrease with coverage, depending on the verifier's
ROC, and a saturation regime -- where sub-optimality plateaus, unaffected by
coverage. We further propose and analyze two classes of sampling algorithms --
sequential and batched, and examine how their computational complexities shape
these trade-offs. Empirical results with Qwen, Llama, and Gemma models
corroborate our theoretical findings.

</details>


### [69] [Timely Clinical Diagnosis through Active Test Selection](https://arxiv.org/abs/2510.18988)
*Silas Ruhrberg Estévez,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 提出 ACTMED 的诊断框架，将贝叶斯实验设计与大语言模型结合，用以自适应选择最能降低诊断不确定性的测试，在临床诊断中实现更高的准确性、可解释性与资源效率，同时保留临床人员参与。


<details>
  <summary>Details</summary>
Motivation: 在临床诊断中，诊断过程是序贯且受资源制约的，高压环境下容易出错；现有 ML 往往依赖静态、完全观测的数据集，难以反映真实世界的推理与决策成本，因此需要一个能模拟真实诊断推理、并且对数据要求更低的框架。

Method: 在每一步，ACTMED 选择能够最大化对当前患者诊断不确定性减少的测试；采用贝叶斯实验设计衡量信息增益，使用大语言模型作为灵活的模拟器，生成可能的患者状态分布并支持 belief 更新；临床医生参与过程，审阅测试建议、解读中间输出并应用临床判断。

Result: 在真实世界数据集上进行评估，显示该框架通过优化测试选择提高诊断准确性、可解释性和资源利用效率，同时在不同环境中具有更低对领域特定数据的依赖并具有一定的泛化能力。

Conclusion: 这是朝向透明、适应性强、与临床医生高度契合的诊断系统的一步，能够在减少对领域特定数据的依赖的前提下实现跨场景的推广。

Abstract: There is growing interest in using machine learning (ML) to support clinical
diag- nosis, but most approaches rely on static, fully observed datasets and
fail to reflect the sequential, resource-aware reasoning clinicians use in
practice. Diagnosis remains complex and error prone, especially in
high-pressure or resource-limited settings, underscoring the need for
frameworks that help clinicians make timely and cost-effective decisions. We
propose ACTMED (Adaptive Clinical Test selection via Model-based Experimental
Design), a diagnostic framework that integrates Bayesian Experimental Design
(BED) with large language models (LLMs) to better emulate real-world diagnostic
reasoning. At each step, ACTMED selects the test expected to yield the greatest
reduction in diagnostic uncertainty for a given patient. LLMs act as flexible
simulators, generating plausible patient state distributions and supporting
belief updates without requiring structured, task-specific training data.
Clinicians can remain in the loop; reviewing test suggestions, interpreting
intermediate outputs, and applying clinical judgment throughout. We evaluate
ACTMED on real-world datasets and show it can optimize test selection to
improve diagnostic accuracy, interpretability, and resource use. This
represents a step to- ward transparent, adaptive, and clinician-aligned
diagnostic systems that generalize across settings with reduced reliance on
domain-specific data.

</details>


### [70] [The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS](https://arxiv.org/abs/2510.19055)
*Brandon James Carone,Iran R. Roman,Pablo Ripollés*

Main category: cs.AI

TL;DR: MUSE 基准用于评估音乐理解中的基本感知与结构推理，包含10项任务；在4个SOTA模型与200名人类基线下，存在显著差距，且链式推理提示效果不稳定甚至有害。


<details>
  <summary>Details</summary>
Motivation: 揭示多模态大语言模型在音乐感知与关系推理方面的基本能力与局限，避免只看高层表现，推动更稳健的音乐理解模型。

Method: 设计开放源代码的10任务MUSE基准，覆盖音乐感知的基本技能与结构性推理；在Gemini Pro（两种版本）、Qwen2.5-Omni、Audio-Flamingo 3等模型上评估，并与N=200的人类基线比较；额外验证 Chain-of-Thought 提示的影响。

Result: Gemini Pro 在基本感知上表现较好；Qwen2.5-Omni和Audio-Flamingo 3 表现接近随机猜测，显示严重的感知缺陷；不同模型之间能力差异显著；链式推理提示往往不稳定甚至有害。

Conclusion: 该基准提供一个关键工具来评估音乐表示的鲁棒性，促使开发出对音乐领域更稳健的多模态AI系统；公开资源有助于推动领域透明度和改进。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated capabilities in
audio understanding, but current evaluations may obscure fundamental weaknesses
in relational reasoning. We introduce the Music Understanding and Structural
Evaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to
probe fundamental music perception skills. We evaluate four SOTA models (Gemini
Pro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human
baseline (N=200). Our results reveal a wide variance in SOTA capabilities and a
persistent gap with human experts. While Gemini Pro succeeds on basic
perception, Qwen and Audio Flamingo 3 perform at or near chance, exposing
severe perceptual deficits. Furthermore, we find Chain-of-Thought (CoT)
prompting provides inconsistent, often detrimental results. Our work provides a
critical tool for evaluating invariant musical representations and driving
development of more robust AI systems.

</details>


### [71] [A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist](https://arxiv.org/abs/2510.19139)
*Sohyeon Jeon,Hyung-Chul Lee*

Main category: cs.AI

TL;DR: LLMs在评估 CONSORT 遵从性方面表现出显著的认知/推理差异，提示当前医疗合规自动化的局限性及需理解其认知适应性以提升可解释性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在医疗领域对 CONSORT 报告的符合性及其背后的认知与元认知策略，以促进更可解释和可靠的医学AI。

Method: 以专家验证数据，采用行为学与元认知分析框架，系统比较两种代表性LLM，在三种提示条件下对CONSORT项的回答进行评估与比较。

Result: 在对不同 CONSORT 项的处理、提示类型及推理风格、显式不确定性和替代解释等方面，模型之间存在明显差异，且响应模式受提示条件影响。

Conclusion: 目前在临床合规自动化方面存在显著局限；理解其认知适应性和策略性行为对于开发更可解释、可靠的医疗AI至关重要。

Abstract: Despite the rapid expansion of Large Language Models (LLMs) in healthcare,
the ability of these systems to assess clinical trial reporting according to
CONSORT standards remains unclear, particularly with respect to their cognitive
and reasoning strategies. This study applies a behavioral and metacognitive
analytic approach with expert-validated data, systematically comparing two
representative LLMs under three prompt conditions. Clear differences emerged in
how the models approached various CONSORT items, and prompt types, including
shifts in reasoning style, explicit uncertainty, and alternative
interpretations shaped response patterns. Our results highlight the current
limitations of these systems in clinical compliance automation and underscore
the importance of understanding their cognitive adaptations and strategic
behavior in developing more explainable and reliable medical AI.

</details>


### [72] [The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models](https://arxiv.org/abs/2510.19176)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Main category: cs.AI

TL;DR: 本论文将 Mode Selection 视为早期退出的更具挑战性的变体：在推理开始前就需决定使用长链思维（Long-CoT）还是短链思维（Short-CoT），通过零步思考实现低成本推理，并将 Early Exit 作为在推理过程中的最优停止点。实证对九个基线进行比较，结果显示提示驱动的方法在信息极少时分类能力不足，而利用模型内部信息的方法更具优势但稳定性不足；现有仅基于模型输出信息的方法难以有效解决 Mode Selection。代码公开。


<details>
  <summary>Details</summary>
Motivation: 在推理任务中，链式思考显著提升性能，但伴随高计算开销。本文试图通过 Mode Selection 与 Early Exit 的结合，在推理之初就做出是否进行长思维的决策，从而降低总体计算成本。

Method: 对九个基线进行实证研究，比较 Long-CoT/Short-CoT 的 Thinking 与 NoThinking 模式；提出零步思维的概念，即在推理开始前给予假设性“思考”以避免实际推理开销；评估提示驱动方法与依赖模型内部信息的方法在信息受限条件下的表现，并分析稳定性问题。

Result: 提示驱动方法在信息有限时分类能力不足；基于模型内部信息的方法表现更好但存在稳定性问题；仅依赖模型输出信息的方法难以有效解决 Mode Selection 的挑战。

Conclusion: Mode Selection 是比 Early Exit 更具挑战的任务，需要超越仅依赖模型输出信息的策略，并关注在低信息场景下的稳定性与鲁棒性；未来工作应加强对信息来源与决策时机的综合利用。

Abstract: Reasoning models have demonstrated exceptional performance in tasks such as
mathematics and logical reasoning, primarily due to their ability to engage in
step-by-step thinking during the reasoning process. However, this often leads
to overthinking, resulting in unnecessary computational overhead. To address
this issue, Mode Selection aims to automatically decide between Long-CoT
(Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking
mode. Simultaneously, Early Exit determines the optimal stopping point during
the iterative reasoning process. Both methods seek to reduce the computational
burden. In this paper, we first identify Mode Selection as a more challenging
variant of the Early Exit problem, as they share similar objectives but differ
in decision timing. While Early Exit focuses on determining the best stopping
point for concise reasoning at inference time, Mode Selection must make this
decision at the beginning of the reasoning process, relying on pre-defined fake
thoughts without engaging in an explicit reasoning process, referred to as
zero-step thinking. Through empirical studies on nine baselines, we observe
that prompt-based approaches often fail due to their limited classification
capabilities when provided with minimal hand-crafted information. In contrast,
approaches that leverage internal information generally perform better across
most scenarios but still exhibit issues with stability. Our findings indicate
that existing methods relying solely on the information provided by models are
insufficient for effectively addressing Mode Selection in scenarios with
limited information, highlighting the ongoing challenges of this task. Our code
is available at https://github.com/Trae1ounG/Zero_Step_Thinking.

</details>


### [73] [ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate](https://arxiv.org/abs/2510.19261)
*Marianna Molinari,Ilaria Angela Amantea,Marinella Quaranta,Guido Governatori*

Main category: cs.AI

TL;DR: 即便ChatGPT具备必要的知识，它在法律领域也难以将多项能力整合并给出穷尽且统一的结论；与正则表达式基线相比，存在显著推理与综合能力的局限。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在专业领域的推理与整合能力，检验其在读取法律判决、提炼法律原则并用于后续裁判中的能力。

Method: 通过将ChatGPT的输出与一个基线正则表达式方案进行对比，设计针对读取法律判决并提取要点的任务，评估其是否能提供全面的结论。

Result: 结果表明，即使具备必要知识与能力，ChatGPT也难以汇聚并系统化地解决复杂任务，无法给出穷尽性结果；AI在统一与综合多项能力方面存在局限。

Conclusion: 在该领域，真正的智能目前仍然是人类的特征，AI需要人类监督与协同，未来在法律领域的应用应强调人机协同与对结果的审慎解读。

Abstract: This study examines the performance of ChatGPT with an experiment in the
legal domain. We compare the outcome with it a baseline using regular
expressions (Regex), rather than focusing solely on the assessment against
human performance. The study reveals that even if ChatGPT has access to the
necessary knowledge and competencies, it is unable to assemble them, reason
through, in a way that leads to an exhaustive result. This unveils a major
limitation of ChatGPT. Intelligence encompasses the ability to break down
complex issues and address them according to multiple required competencies,
providing a unified and comprehensive solution. In the legal domain, one of the
most crucial tasks is reading legal decisions and extracting key passages
condensed from principles of law (PoLs), which are then incorporated into
subsequent rulings by judges or defense documents by lawyers. In performing
this task, artificial intelligence lacks an all-encompassing understanding and
reasoning, which makes it inherently limited. Genuine intelligence, remains a
uniquely human trait, at least in this particular field.

</details>


### [74] [An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents](https://arxiv.org/abs/2510.19263)
*Wachara Fungwacharakorn,Gauvain Bourgne,Ken Satoh*

Main category: cs.AI

TL;DR: 本论文在可容许不一致先例的通用理由模型下，研究如何用扩展的DSA框架对推理过程给出论证性解释。


<details>
  <summary>Details</summary>
Motivation: 动机在于放宽“先例集需一致”的假设，提供对不一致先例情形的可解释性支持，填补在通用理由模型下缺乏论证性解释方法的空缺。

Method: 方法通过对Derivation State Argumentation (DSA) 框架进行扩展，使之能够适配通用的理由模型，并据此构建用于解释推理过程的论证性机制。

Result: 结果指明在扩展的DSA框架下可以解释基于通用理由模型的推理过程，提出了相应的解释结构与机制的理论框架。

Conclusion: 结论是为在不一致先例下的推理提供可解释的论证路径，建立了将DSA框架应用于通用理由模型的理论基础。

Abstract: Precedential constraint is one foundation of case-based reasoning in AI and
Law. It generally assumes that the underlying set of precedents must be
consistent. To relax this assumption, a generalized notion of the reason model
has been introduced. While several argumentative explanation approaches exist
for reasoning with precedents based on the traditional consistent reason model,
there has been no corresponding argumentative explanation method developed for
this generalized reasoning framework accommodating inconsistent precedents. To
address this question, this paper examines an extension of the derivation state
argumentation framework (DSA-framework) to explain the reasoning according to
the generalized notion of the reason model.

</details>


### [75] [Continual Knowledge Adaptation for Reinforcement Learning](https://arxiv.org/abs/2510.19314)
*Jinwu Hu,Zihao Lian,Zhiquan Wen,Chenghao Li,Guohao Chen,Xutao Wen,Bin Xiao,Mingkui Tan*

Main category: cs.AI

TL;DR: CKA-RL introduces continual knowledge adaptation for reinforcement learning to combat non-stationarity and catastrophic forgetting by maintaining a task-specific knowledge vector pool and an adaptive merging mechanism to reuse and compress historical knowledge, improving overall performance and forward transfer across three benchmarks; code available on GitHub.


<details>
  <summary>Details</summary>
Motivation: In non-stationary real-world environments, standard RL struggles with continual learning, forgetting past tasks, and inefficient knowledge reuse. The paper aims to retain and leverage historical knowledge to adapt to new tasks.

Method: 1) Continual Knowledge Adaptation: maintain a pool of task-specific knowledge vectors; dynamically utilize historical knowledge to adapt to new tasks; preserve/adapt critical parameters to mitigate forgetting. 2) Adaptive Knowledge Merging: merge similar knowledge vectors to reduce memory and maintain essential knowledge.

Result: Empirical evaluation on three benchmarks shows state-of-the-art performance, with +4.20% in overall performance and +8.02% in forward transfer compared to baselines.

Conclusion: CKA-RL effectively mitigates forgetting and enhances transfer by preserving/adapting knowledge and compressing it via merging, achieving memory-efficient continual RL. Source code is publicly available.

Abstract: Reinforcement Learning enables agents to learn optimal behaviors through
interactions with environments. However, real-world environments are typically
non-stationary, requiring agents to continuously adapt to new tasks and
changing conditions. Although Continual Reinforcement Learning facilitates
learning across multiple tasks, existing methods often suffer from catastrophic
forgetting and inefficient knowledge utilization. To address these challenges,
we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL),
which enables the accumulation and effective utilization of historical
knowledge. Specifically, we introduce a Continual Knowledge Adaptation
strategy, which involves maintaining a task-specific knowledge vector pool and
dynamically using historical knowledge to adapt the agent to new tasks. This
process mitigates catastrophic forgetting and enables efficient knowledge
transfer across tasks by preserving and adapting critical model parameters.
Additionally, we propose an Adaptive Knowledge Merging mechanism that combines
similar knowledge vectors to address scalability challenges, reducing memory
requirements while ensuring the retention of essential knowledge. Experiments
on three benchmarks demonstrate that the proposed CKA-RL outperforms
state-of-the-art methods, achieving an improvement of 4.20% in overall
performance and 8.02% in forward transfer. The source code is available at
https://github.com/Fhujinwu/CKA-RL.

</details>


### [76] [NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning](https://arxiv.org/abs/2510.19429)
*Wonje Choi,Jooyoung Kim,Honguk Woo*

Main category: cs.AI

TL;DR: NeSyPr 是一种神经符号过程化的嵌入式推理框架，通过将符号 planner 产出的计划转化为可组合的过程化表示，集成到语言模型推理中，从而在延迟和资源受限的环境中实现高效、无外部符号引导的多步推理。


<details>
  <summary>Details</summary>
Motivation: 在动态环境下，语言模型对大规模推理引擎或符号规划器的在线访问受到延迟、连通性和资源限制的挑战，需在本地、低延迟条件下进行高效推理。

Method: 任务计划由符号工具生成后，转化为可组合的过程化表示，编码计划中的隐式生产规则，使得组合后的过程可在语言模型推理中单步执行；实现神经符号的过程化，减少对外部符号引导的依赖。

Result: 在 PDDLGym、VirtualHome、ALFWorld 等基准上评估，证明 NeSyPr 在大规模推理模型上具有高效推理能力，且可用更小的语言模型实现良好表现。

Conclusion: NeSyPr 提供了一种在受限资源与低延迟场景下的嵌入式推理解决方案，通过知识编译将多步符号推理转化为 LM 的单步推理，便于在现实物理系统中部署。

Abstract: We address the challenge of adopting language models (LMs) for embodied tasks
in dynamic environments, where online access to large-scale inference engines
or symbolic planners is constrained due to latency, connectivity, and resource
limitations. To this end, we present NeSyPr, a novel embodied reasoning
framework that compiles knowledge via neurosymbolic proceduralization, thereby
equipping LM-based agents with structured, adaptive, and timely reasoning
capabilities. In NeSyPr, task-specific plans are first explicitly generated by
a symbolic tool leveraging its declarative knowledge. These plans are then
transformed into composable procedural representations that encode the plans'
implicit production rules, enabling the resulting composed procedures to be
seamlessly integrated into the LM's inference process. This neurosymbolic
proceduralization abstracts and generalizes multi-step symbolic structured
path-finding and reasoning into single-step LM inference, akin to human
knowledge compilation. It supports efficient test-time inference without
relying on external symbolic guidance, making it well suited for deployment in
latency-sensitive and resource-constrained physical systems. We evaluate NeSyPr
on the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating
its efficient reasoning capabilities over large-scale reasoning models and a
symbolic planner, while using more compact LMs.

</details>


### [77] [DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning](https://arxiv.org/abs/2510.19562)
*Runpeng Xie,Quanwei Wang,Hao Hu,Zherui Zhou,Ni Mu,Xiyun Li,Yiqin Yang,Shuang Xu,Qianchuan Zhao,Bo XU*

Main category: cs.AI

TL;DR: 提出 DAIL，利用分布式策略估计和值分布并进行语义对齐，以缓解语言指令模糊带来的任务差异，提升语言条件任务的表现。


<details>
  <summary>Details</summary>
Motivation: 自然语言指令的模糊性导致任务执行中的不确定性，降低对语言条件任务的学习效率和鲁棒性；需要一个能对值分布进行建模并与指令语义对齐的框架以提高区分度和泛化。

Method: 提出 Distributional Aligned Learning (DAIL)，包含分布式策略与语义对齐两个模块。理论分析表明值分布估计提高任务的可区分性；语义对齐模块建立轨迹与指令之间的一致映射。并在结构化和视觉观测基准上进行广泛实验，验证对指令歧义的缓解效果。

Result: 在多种基准上显著优于基线方法，显示出对指令歧义的有效缓解以及更高的任务性能；实现开源。

Conclusion: DAIL 提供了一种可解释且有效的解决语言指令模糊性的学习框架，理论与实验结果共同支持其提升语言条件任务的学习效果与鲁棒性。

Abstract: Comprehending natural language and following human instructions are critical
capabilities for intelligent agents. However, the flexibility of linguistic
instructions induces substantial ambiguity across language-conditioned tasks,
severely degrading algorithmic performance. To address these limitations, we
present a novel method named DAIL (Distributional Aligned Learning), featuring
two key components: distributional policy and semantic alignment. Specifically,
we provide theoretical results that the value distribution estimation mechanism
enhances task differentiability. Meanwhile, the semantic alignment module
captures the correspondence between trajectories and linguistic instructions.
Extensive experimental results on both structured and visual observation
benchmarks demonstrate that DAIL effectively resolves instruction ambiguities,
achieving superior performance to baseline methods. Our implementation is
available at https://github.com/RunpengXie/Distributional-Aligned-Learning.

</details>


### [78] [AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing](https://arxiv.org/abs/2510.19661)
*Xusen Guo,Mingxing Peng,Xixuan Hao,Xingchen Zou,Qiongyan Wang,Sijie Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出 AgentSense，一种训练无须的多智能体进化框架，将大语言模型融入参与式城市感知，结合经典规划与任务分配的自适应和可解释性，优于传统方法和单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 解决城市感知系统在跨场景泛化与可解释性方面的不足，提升自适应性、透明度与用户信任度。

Method: 使用经典规划作为基线并迭代 refinement，采用多智能体进化来适应动态城市条件与工人偏好，同时由 LLM 生成自然语言解释以提升透明度。

Result: 在两大规模移动数据集和七类动态扰动的实验中，AgentSense 在自适应性和可解释性方面优于传统方法；相较单智能体 LLM 基线，性能与鲁棒性提升，且解释更合理、透明。

Conclusion: 这是向在网页上部署的自适应、可解释城市感知系统的重要进展。

Abstract: Web-based participatory urban sensing has emerged as a vital approach for
modern urban management by leveraging mobile individuals as distributed
sensors. However, existing urban sensing systems struggle with limited
generalization across diverse urban scenarios and poor interpretability in
decision-making. In this work, we introduce AgentSense, a hybrid, training-free
framework that integrates large language models (LLMs) into participatory urban
sensing through a multi-agent evolution system. AgentSense initially employs
classical planner to generate baseline solutions and then iteratively refines
them to adapt sensing task assignments to dynamic urban conditions and
heterogeneous worker preferences, while producing natural language explanations
that enhance transparency and trust. Extensive experiments across two
large-scale mobility datasets and seven types of dynamic disturbances
demonstrate that AgentSense offers distinct advantages in adaptivity and
explainability over traditional methods. Furthermore, compared to single-agent
LLM baselines, our approach outperforms in both performance and robustness,
while delivering more reasonable and transparent explanations. These results
position AgentSense as a significant advancement towards deploying adaptive and
explainable urban sensing systems on the web.

</details>


### [79] [A Graph Engine for Guitar Chord-Tone Soloing Education](https://arxiv.org/abs/2510.19666)
*Matthew Keating,Michael Casey*

Main category: cs.AI

TL;DR: 本文提出一个基于图的引擎，用于为吉他学生生成和弦音独奏建议，通过将和弦音琶音作为节点、边权衡转接音，寻找最短路径来构建独奏，并给出一个面向输入/输出的教学系统。


<details>
  <summary>Details</summary>
Motivation: Chord-tone soloing 对吉他即兴而言是基本技能，但学习与练习困难，需要一个可操作的计算工具来辅助教学。

Method: 先生成和弦音琶音；构建加权图，节点为每个和弦的琶音；计算相邻和弦节点之间的边权（表示最佳过渡音）；求解最短路径，重建完整的和弦音独奏；并讨论一个便于学生使用的输入输出系统。

Result: 未给出实验结果，论文描述了方法和系统设计，展示可实现性与教学应用的框架。

Conclusion: 该方法为和弦音独奏的自动化生成提供了一个可操作的图模型，并给出实用的教学系统设计，便于学生练习和弦音独奏。

Abstract: We present a graph-based engine for computing chord tone soloing suggestions
for guitar students. Chord tone soloing is a fundamental practice for
improvising over a chord progression, where the instrumentalist uses only the
notes contained in the current chord. This practice is a building block for all
advanced jazz guitar theory but is difficult to learn and practice. First, we
discuss methods for generating chord-tone arpeggios. Next, we construct a
weighted graph where each node represents a chord tone arpeggio for a chord in
the progression. Then, we calculate the edge weight between each consecutive
chord's nodes in terms of optimal transition tones. We then find the shortest
path through this graph and reconstruct a chord-tone soloing line. Finally, we
discuss a user-friendly system to handle input and output to this engine for
guitar students to practice chord tone soloing.

</details>


### [80] [Explainable e-sports win prediction through Machine Learning classification in streaming](https://arxiv.org/abs/2510.19671)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.AI

TL;DR: 本论文提出一种可解释的流式胜率预测分类方案，针对电竞领域在滑动窗口条件下进行输入数据控制，达到较高准确率并优于现有文献方法，可用于排序/推荐系统，与可解释性模块共同提升决策信任。


<details>
  <summary>Details</summary>
Motivation: 电竞观众和选手数量的持续增长以及对实时、可解释的预测分析的需求，推动电竞分析技术的发展。

Method: 在流式数据场景中，通过若干滑动窗口对输入数据进行控制，进行胜率分类预测；引入可解释性模块以提升模型透明度，并对比现有论文方法以验证性能提升。

Result: 实验结果显示准确率超过90%，优于文献中的竞争方案。

Conclusion: 所提出的可解释流式预测系统可用于排序与推荐系统，帮助决策，并通过可解释性增强结果的可信度。

Abstract: The increasing number of spectators and players in e-sports, along with the
development of optimized communication solutions and cloud computing
technology, has motivated the constant growth of the online game industry. Even
though Artificial Intelligence-based solutions for e-sports analytics are
traditionally defined as extracting meaningful patterns from related data and
visualizing them to enhance decision-making, most of the effort in professional
winning prediction has been focused on the classification aspect from a batch
perspective, also leaving aside the visualization techniques. Consequently,
this work contributes to an explainable win prediction classification solution
in streaming in which input data is controlled over several sliding windows to
reflect relevant game changes. Experimental results attained an accuracy higher
than 90 %, surpassing the performance of competing solutions in the literature.
Ultimately, our system can be leveraged by ranking and recommender systems for
informed decision-making, thanks to the explainability module, which fosters
trust in the outcome predictions.

</details>


### [81] [RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models](https://arxiv.org/abs/2510.19698)
*Yang Yang,Hua XU,Zhangyi Hu,Yutao Yue*

Main category: cs.AI

TL;DR: RLIE将LLMs与概率建模结合，提出一个四阶段的框架，通过学习带权规则实现鲁棒推理，并与直接使用规则或将规则注入LLM的策略进行对比。


<details>
  <summary>Details</summary>
Motivation: 现有工作往往让LLMs生成规则但忽视规则之间的交互，以及缺乏将LLMs与概率化的规则学习整合以实现更鲁棒的推理的系统性方案。

Method: RLIE框架包括四个阶段：1) 规则生成与筛选（由LLMs提出并过滤候选规则）；2) 逻辑回归（学习全局选择和校准的概率权重）；3) 迭代改进（基于预测误差更新规则集合）；4) 评估（将加权规则直接作为分类器，与将规则、权重及逻辑模型输出注入LLM的对比策略进行比较）。

Result: 在真实数据集上对多种推理策略进行评估，直接使用学习得到的权重对规则进行推理往往表现最好；用规则、权重和逻辑模型输出对LLM进行提示的策略反而降低准确性；显示LLMs擅长语义生成与解释，但在精确概率整合方面不如专门的概率方法；RLIE将LLMs与经典概率规则整合方法耦合，提升神经-符号推理的鲁棒性。

Conclusion: RLIE澄清了LLMs在归纳推理中的潜力与局限，提供了一种将LLMs与概率规则耦合的框架，以实现更可靠的推理。

Abstract: Large Language Models (LLMs) can propose rules in natural language,
sidestepping the need for a predefined predicate space in traditional rule
learning. Yet many LLM-based approaches ignore interactions among rules, and
the opportunity to couple LLMs with probabilistic rule learning for robust
inference remains underexplored. We present RLIE, a unified framework that
integrates LLMs with probabilistic modeling to learn a set of weighted rules.
RLIE has four stages: (1) Rule generation, where an LLM proposes and filters
candidates; (2) Logistic regression, which learns probabilistic weights for
global selection and calibration; (3) Iterative refinement, which updates the
rule set using prediction errors; and (4) Evaluation, which compares the
weighted rule set as a direct classifier with methods that inject rules into an
LLM. We evaluate multiple inference strategies on real-world datasets. Applying
rules directly with their learned weights yields superior performance, whereas
prompting LLMs with the rules, weights, and logistic-model outputs surprisingly
degrades accuracy. This supports the view that LLMs excel at semantic
generation and interpretation but are less reliable for precise probabilistic
integration. RLIE clarifies the potential and limitations of LLMs for inductive
reasoning and couples them with classic probabilistic rule combination methods
to enable more reliable neuro-symbolic reasoning.

</details>


### [82] [Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning](https://arxiv.org/abs/2510.19732)
*Gunshi Gupta,Karmesh Yadav,Zsolt Kira,Yarin Gal,Rahaf Aljundi*

Main category: cs.AI

TL;DR: Memo 是一种基于 transformer 的强化学习架构，通过在输入中周期性插入摘要令牌来创建与检索记忆，显著提升在需要长期记忆的任务中的表现，同时比直接使用全上下文的 Transformer 更高效且具有更好的推断时上下文扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 Transformer 的Embodied（具身）序列决策在长期任务中面临上下文窗口的瓶颈；人类通过高效的记忆压缩来维持对环境的持久感知。现有方法要么使用固定大小的记忆的循环模型，要么依赖全上下文的 Transformer，难以处理大规模、长期的感知信息。需要一种能够高效地创建、访问和利用长期记忆的训练方法。

Method: 提出 Memo，一种在训练过程中通过在模型输入中交错放置周期性摘要标记来实现记忆创建与检索的 Transformer 强化学习框架。该方法通过在输入序列中引入摘要令牌来对观测及历史进行压缩，并在训练中学习如何利用这些摘要来形成长期记忆。将在网格世界元强化学习基准和真实感室内场景的多目标导航任务上评估。

Result: 相较于直观的长上下文 Transformer 基线，Memo 在计算与存储开销方面更高效，并在推断时对更长的上下文具有更好的泛化能力；在流式（需要截断历史以适应推断约束）场景下仍保持鲁棒性。

Conclusion: 通过引入摘要记忆的可训练机制，Memo 有效缓解了长期、记忆密集任务对上下文长度的依赖，提供了一种面向长期记忆的高效 Transformer 方案，且具备良好的迁移性与流式输出鲁棒性。

Abstract: To enable embodied agents to operate effectively over extended timeframes, it
is crucial to develop models that form and access memories to stay
contextualized in their environment. In the current paradigm of training
transformer-based policies for embodied sequential decision-making tasks,
visual inputs often overwhelm the context limits of transformers, while humans
can maintain and utilize a lifetime of experience compressed as memories.
Significant compression is possible in principle, as much of the input is
irrelevant and can be abstracted. However, existing approaches predominantly
focus on either recurrent models with fixed-size memory or transformers with
full-context reliance. In this work, we propose Memo, a transformer-based
architecture and training recipe for reinforcement learning (RL) on
memory-intensive, long-horizon tasks. Memo incorporates the creation and
retrieval of memory by interleaving periodic summarization tokens with the
inputs of a model during training. We demonstrate Memo's effectiveness on a
gridworld meta-RL benchmark and a multi-object navigation task in
photo-realistic indoor settings. Memo outperforms naive long-context
transformer baselines while being more compute and storage efficient.
Additionally, Memo generalizes better to longer contexts at inference time and
remains robust in streaming settings, where historical context must be
truncated to fit inference constraints.

</details>


### [83] [Misalignment Bounty: Crowdsourcing AI Agent Misbehavior](https://arxiv.org/abs/2510.19738)
*Rustem Turtayev,Natalia Fedorova,Oleg Serikov,Sergey Koldyba,Lev Avagyan,Dmitrii Volkov*

Main category: cs.AI

TL;DR: Crowdsourced Misalignment Bounty identifies nine notable cases of agents pursuing unsafe goals from 295 submissions.


<details>
  <summary>Details</summary>
Motivation: to obtain clear, reproducible examples of AI misalignment to inform safe system design and evaluation.

Method: crowdsourced submissions, defined evaluation criteria, judge nine winning entries.

Result: nine winning submissions are explained step by step.

Conclusion: highlights patterns in misalignment, informs future alignment research and safety benchmarking.

Abstract: Advanced AI systems sometimes act in ways that differ from human intent. To
gather clear, reproducible examples, we ran the Misalignment Bounty: a
crowdsourced project that collected cases of agents pursuing unintended or
unsafe goals. The bounty received 295 submissions, of which nine were awarded.
  This report explains the program's motivation and evaluation criteria, and
walks through the nine winning submissions step by step.

</details>


### [84] [Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents](https://arxiv.org/abs/2510.19771)
*Gil Pasternak,Dheeraj Rajagopal,Julia White,Dhruv Atreja,Matthew Thomas,George Hurn-Maloney,Ash Lewis*

Main category: cs.AI

TL;DR: PROBE benchmarks proactive LLM-based agents via a three-stage pipeline to handle unspecific issues, bottlenecks, and resolutions. Current models underperform; best end-to-end ~40%, seen in GPT-5 and Claude Opus-4.1.


<details>
  <summary>Details</summary>
Motivation: Assess proactivity in LLM agents beyond localized context, testing cross-source reasoning and long-horizon planning where existing benchmarks fall short.

Method: PROBE decomposes proactivity into three capabilities: (1) searching for unspecified issues, (2) identifying specific bottlenecks, and (3) executing appropriate resolutions. The paper applies PROBE to leading LLMs and agentic frameworks, computes cross-model consistency, and analyzes failure modes.

Result: Across frontier models, the highest end-to-end score observed is 40% (GPT-5 and Claude Opus-4.1). The study reveals relative strengths/weaknesses of models and common failure modes in autonomous action.

Conclusion: Autonomous agentic systems currently exhibit limited proactivity; PROBE highlights performance gaps and guides future research towards more capable, reliable autonomous reasoning and action.

Abstract: LLM-based agents are increasingly moving towards proactivity: rather than
awaiting instruction, they exercise agency to anticipate user needs and solve
them autonomously. However, evaluating proactivity is challenging; current
benchmarks are constrained to localized context, limiting their ability to test
reasoning across sources and longer time horizons. To address this gap, we
present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes
proactivity as a pipeline of three core capabilities: (1) searching for
unspecified issues, (2) identifying specific bottlenecks, and (3) executing
appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular
agentic frameworks, showing that even state-of-the-art models struggle to solve
this benchmark. Computing our consistent measurements across frontier LLMs and
agents, we find that the best end-to-end performance of 40% is achieved by both
GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative
capabilities of each model and analyze mutual failure modes. Our results
highlight the current limitations of autonomous action in agentic systems, and
expose promising future research directions.

</details>


### [85] [Benchmarking World-Model Learning](https://arxiv.org/abs/2510.19788)
*Archana Warrier,Dat Nyugen,Michelangelo Naim,Moksh Jain,Yichao Liang,Karen Schroeder,Cambridge Yang,Joshua B. Tenenbaum,Sebastian Vollmer,Kevin Ellis,Zenna Tavares*

Main category: cs.AI

TL;DR: WorldTest provides a reward-free, task-agnostic benchmark for evaluating world-model learning, instantiated as AutumnBench with 43 grid-worlds and 129 tasks. Humans outperform models, and compute scaling helps only in some environments, revealing headroom in world-model learning.


<details>
  <summary>Details</summary>
Motivation: Current world-model evaluation concentrates on next-frame prediction and reward-maximizing behavior in a fixed environment, which may underconstrain learning of general environment dynamics. There is a need for reward-free exploration and evaluation that generalizes across tasks and representations.

Method: Propose WorldTest, a protocol that separates reward-free interaction from a scored test phase in a related environment. Instantiate AutumnBench—43 interactive grid-worlds and 129 tasks across masked-frame prediction, planning, and predicting changes to causal dynamics. Compare 517 human participants with three frontier models, using behavior-based scoring to assess learned environment dynamics.

Result: Humans outperform the models; scaling compute improves performance only in some environments. WorldTest provides a novel template—reward-free exploration, derived tests, and behavior-based scoring—to evaluate agents’ understanding of environment dynamics, and AutumnBench reveals significant headroom in world-model learning across tasks.

Conclusion: WorldTest offers a reward-free, task-agnostic evaluation framework for world-model learning, enabling cross-method comparisons and exposing gaps in current models. AutumnBench exemplifies the potential for broad, multi-task evaluation and highlights headroom for improvement in world-model representations.

Abstract: Model-learning agents should gather information to learn world models that
support many downstream tasks and inferences, such as predicting unobserved
states, estimating near- and far-term consequences of actions, planning action
sequences, and detecting changes in dynamics. Current methods for learning and
evaluating world models diverge from this goal: training and evaluation are
anchored to next-frame prediction, and success is scored by reward maximization
in the same environment. We propose WorldTest, a protocol to evaluate
model-learning agents that separates reward-free interaction from a scored test
phase in a different but related environment. WorldTest is
open-ended$\unicode{x2014}$models should support many different tasks unknown
ahead of time$\unicode{x2014}$and agnostic to model representation, allowing
comparison across approaches. We instantiated WorldTest with AutumnBench, a
suite of 43 interactive grid-world environments and 129 tasks across three
families: masked-frame prediction, planning, and predicting changes to the
causal dynamics. We compared 517 human participants and three frontier models
on AutumnBench. We found that humans outperform the models, and scaling compute
improves performance only in some environments but not others. WorldTest
provides a novel template$\unicode{x2014}$reward-free exploration, derived
tests, and behavior-based scoring$\unicode{x2014}$to evaluate what agents learn
about environment dynamics, and AutumnBench exposes significant headroom in
world-model learning.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [86] [SBAN: A Framework \& Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining](https://arxiv.org/abs/2510.18936)
*Hamed Jelodar,Mohammad Meymani,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: SBAN 是一个包含源代码、二进制、汇编、自然语言描述等四模态的大规模数据集，用于对软件代码进行多模态预训练、跨模态学习与安全分析。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态数据不足、缺乏统一的跨层次表示的挑战，推动对软件行为、语义的综合理解，以及提升恶意软件检测、代码挖掘等任务的性能。

Method: 收集并整理超过300万条样本，其中包含约290万条 benign 与672,000 条 malware；每条样本覆盖四个层次：二进制代码、汇编指令、自然语言描述、源代码，构成一个多模态数据集，适合用于 Transformer/LLM 的大规模训练与对齐学习。

Result: 提供跨表示学习、软件语义理解、自动化恶意软件检测等研究基础，扩展到代码翻译、代码解释等软件挖掘任务，促进低级到高级表示的桥接，推动大模型在软件领域的预训练与微调能力。

Conclusion: SBAN 可成为软件行为挖掘、安全分析升级以及提升 LLM 能力的重要基底，开启跨模态软件研究的新方向。

Abstract: This paper introduces SBAN (Source code, Binary, Assembly, and Natural
Language Description), a large-scale, multi-dimensional dataset designed to
advance the pre-training and evaluation of large language models (LLMs) for
software code analysis. SBAN comprises more than 3 million samples, including
2.9 million benign and 672,000 malware respectively, each represented across
four complementary layers: binary code, assembly instructions, natural language
descriptions, and source code. This unique multimodal structure enables
research on cross-representation learning, semantic understanding of software,
and automated malware detection. Beyond security applications, SBAN supports
broader tasks such as code translation, code explanation, and other software
mining tasks involving heterogeneous data. It is particularly suited for
scalable training of deep models, including transformers and other LLM
architectures. By bridging low-level machine representations and high-level
human semantics, SBAN provides a robust foundation for building intelligent
systems that reason about code. We believe that this dataset opens new
opportunities for mining software behavior, improving security analytics, and
enhancing LLM capabilities in pre-training and fine-tuning tasks for software
code mining.

</details>


### [87] [XGen-Q: An Explainable Domain-Adaptive LLM Framework with Retrieval-Augmented Generation for Software Security](https://arxiv.org/abs/2510.19006)
*Hamed Jelodar,Mohammad Meymani,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: 提出 XGen-Q：一个域自适应的大语言模型，用于可解释且鲁棒的恶意软件分析，结合多阶段提示和检索增强生成，能应对混淆代码并在新样本上保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有的恶意软件检测系统在面对混淆、未知威胁时泛化能力不足，且缺乏足够的可解释性与取证能力，需要更具鲁棒性和透明性的模型。

Method: 在 Qwen-Coder 架构基础上，进行针对恶意软件的域适应预训练，语料超过一百万条样本，涵盖源代码和汇编代码；采用多阶段提示策略结合检索增强生成（RAG）实现可靠的恶意软件识别与详细取证报告；设计包含多样化混淆模式的训练流水线以提升泛化能力。

Result: 实验结果显示 XGen-Q 的困惑度显著低于竞争基线，并且在新型恶意样本上表现良好，显示了基于LLM的可解释性和鲁棒性分析的潜力。

Conclusion: 基于领域自适应的LLM方法在恶意软件分析中具有良好前景，未来可通过进一步的对抗性训练和更丰富的检索信息提升泛化和可解释性。

Abstract: Generative AI and large language models (LLMs) have shown strong capabilities
in code understanding, but their use in cybersecurity, particularly for malware
detection and analysis, remains limited. Existing detection systems often fail
to generalize to obfuscated or previously unseen threats, underscoring the need
for more adaptable and explainable models. To address this challenge, we
introduce XGen-Q, a domain-adapted LLM built on the Qwen-Coder architecture and
pretrained on a large-scale corpus of over one million malware samples,
spanning both source and assembly code. XGen-Q uses a multi-stage prompt
strategy combined with retrieval-augmented generation (RAG) to deliver reliable
malware identification and detailed forensic reporting, even in the presence of
complex code obfuscation. To further enhance generalization, we design a
training pipeline that systematically exposes the model to diverse obfuscation
patterns. Experimental results show that XGen-Q achieves significantly lower
perplexity than competitive baselines and exhibits strong performance on novel
malware samples, demonstrating the promise of LLM-based approaches for
interpretable and robust malware analysis.

</details>


### [88] [CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale](https://arxiv.org/abs/2510.19340)
*L. Caspari,M. Dinzinger,K. Gosh Dastidar,C. Fellicious,J. Mitrović,M. Granitzer*

Main category: cs.IR

TL;DR: 引入 CoRECT（Controlled Retrieval Evaluation of Compression Techniques），一个用于大规模嵌入压缩评估的框架及数据集集合，基准八种压缩方法；发现非学习型压缩在可达1亿条语料时能显著减小索引规模且性能损失微小；结果在不同模型间存在较大变异，需 CoRECT 进行一致对比；代码和数据开源。


<details>
  <summary>Details</summary>
Motivation: 密集检索的表现强烈受语料复杂度（规模与文档长度）的影响，但现有研究往往未在大规模语料上系统评估压缩方法的效果。需要一个可控、可重复的评估框架来在真实规模下比较不同嵌入压缩技术的取舍。

Method: 构建 CoRECT 框架与新颖的数据集集合，对八种代表性嵌入压缩方法进行大规模基准测试；在多种密集检索模型上评估，覆盖从极大语料（高达 1 亿条）到不同文档长度的场景，关注索引大小与检索质量之间的折衷；提供可重复的实验流程与度量标准。

Result: 非学习型压缩在显著减小索引规模方面表现突出，即使在高达 1 亿条 passages 的规模下也能实现统计上微弱的性能损失；不同模型对压缩方法的效果存在明显差异，强调需要通过统一框架进行公平比较；相关代码、数据和结果对外开源。

Conclusion: CoRECT 为评估嵌入压缩在密集检索中的表现提供了一个可重复、可比的基准框架，便于研究者和实务者在不同模型背景下做出更稳健的压缩方法选择，促进可重复性与对比性。

Abstract: Dense retrieval systems have proven to be effective across various
benchmarks, but require substantial memory to store large search indices.
Recent advances in embedding compression show that index sizes can be greatly
reduced with minimal loss in ranking quality. However, existing studies often
overlook the role of corpus complexity -- a critical factor, as recent work
shows that both corpus size and document length strongly affect dense retrieval
performance. In this paper, we introduce CoRECT (Controlled Retrieval
Evaluation of Compression Techniques), a framework for large-scale evaluation
of embedding compression methods, supported by a newly curated dataset
collection. To demonstrate its utility, we benchmark eight representative types
of compression methods. Notably, we show that non-learned compression achieves
substantial index size reduction, even on up to 100M passages, with
statistically insignificant performance loss. However, selecting the optimal
compression method remains challenging, as performance varies across models.
Such variability highlights the necessity of CoRECT to enable consistent
comparison and informed selection of compression methods. All code, data, and
results are available on GitHub and HuggingFace.

</details>


### [89] [Top-P Masking for Cross Language Information Retrieval](https://arxiv.org/abs/2510.19758)
*Joseph Casale,Andrew Silverschotz,Joseph DeSimone*

Main category: cs.IR

TL;DR: Top-P动态掩码在信息检索中的应用，优于常用的Top-K掩码，提升跨语言检索性能。


<details>
  <summary>Details</summary>
Motivation: 在信息检索任务中推崇稀疏表示以提升效率，同时寻求比Top-K更灵活的掩码策略。受Nucleus Sampling启发，探索Top-P掩码以保持关键特征的多样性并减少无关信息。

Method: 将Top-P Dynamic Masking应用于BLADE等后处理阶段，进行跨语言信息检索（CLIR）任务的实验对比，比较Top-K与Top-P掩码在检索指标上的差异，可能涉及p阈值设置、掩码策略、以及评估指标（如精确率、召回率、MAP等）。

Result: 与Top-K相比，Top-P Dynamic Masking在CLIR任务中展示更优的检索性能，且在保持或提高稀疏性和计算效率方面具有潜在优势。

Conclusion: Top-P Dynamic Masking可视为Top-K的有效替代，在跨语言信息检索领域具有推广潜力，建议在实际应用中根据任务需求选择合适的p阈值并评估后处理阶段对整体系统的影响。

Abstract: Top-K masking schemes have been proposed as a method to promote sparse
representations in Information Retrieval (IR) tasks, as a simple alternative to
Floating Point Operations per Second (FLOPS) regularization. Algorithms such as
Bilingual Lexical and Document Expansion Model (BLADE), adopt this approach as
a post-processing stage. We propose using Top-P Dynamic Masking similar to
Nucleus Sampling in Large Language Models, and demonstrate better performance
than Top-K masking. Specifically, we evaluate our methods in the domain of
Cross Language Information Retrieval (CLIR)

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [90] [3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency](https://arxiv.org/abs/2510.18905)
*Minseok Jung,Abhas Ricky,Muhammad Rameez Chatni*

Main category: cs.LG

TL;DR: 提出一个3D多目标优化框架，将推理缩放在精度、成本和延迟三个维度统一考虑，提供对约束的感知优化。通过蒙特卡洛仿真在三种场景和九个模拟的大语言模型上评估四种优化方法，结果显示峰值点（knee-point）优化在权衡中表现最佳，追求极致精度时可采用精度优先的策略，框架为在不同部署环境中的推理缩放提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有的推理缩放多依赖一维固定推理次数或二维的性能-成本取舍，无法同时考虑成本和延迟约束，导致在实际部署中难以满足资源与延迟的限制。需要一个统一的决策空间来进行约束感知的推理缩放。

Method: 在三个代表性场景和九个模拟的LLM上，通过蒙特卡洛仿真对3D多目标优化问题进行了比较，评估了四种优化方法来求解在精度、成本、延迟三维的目标函数。将推理缩放建模为多目标优化问题，形成一个可行域，超越了1D/2D优化所能捕捉的空间，并实现对环境自适应的推理缩放因子k的选择。

Result: 结果显示，峰值点（knee-point）优化在平衡精度、成本和延迟方面表现最佳；当需要更高精度时，采用以精度为优先的策略更有优势。该框架能够为在不同操作场景下的部署提供可部署性与可解释性的推理缩放决策。

Conclusion: 该工作为面向部署部署环境的推理缩放提供了理论基础，强调将准确性、成本与延迟三者整合到统一的决策空间中，以实现环境感知的推理缩放，并且为未来在多场景中的实际落地提供方向。

Abstract: AI inference scaling is often tuned through 1D heuristics (a fixed reasoning
passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail
to consider cost and latency constraints. We introduce a 3D optimization
framework that jointly calibrates accuracy, cost, and latency within a unified
decision space, enabling constraints-aware inference scaling. Using Monte Carlo
simulations across three representative scenarios and nine simulated large
language models, we evaluate four optimization methods to address the 3D
multi-objective optimization (MOO) problem. Framing inference scaling in MOO
shapes a feasible space that 1D and 2D optimizations fail to capture, enabling
environmentadaptive selection of the inference scaling k. Results show that
knee-point optimization achieves the best balance, while accuracy-maximization
remains favorable when precision is prioritized. The framework establishes a
theoretical foundation for deployment-aware inference scaling across diverse
operational contexts.

</details>


### [91] [Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape](https://arxiv.org/abs/2510.18910)
*Ziquan Wei,Tingting Dan,Guorong Wu*

Main category: cs.LG

TL;DR: Multitask, semi-supervised foundation model for fMRI that tokenizes brain-environment interactions (BEI) to align pretraining with brain-to-outcome tasks, improving downstream clinical predictions.


<details>
  <summary>Details</summary>
Motivation: Self-supervised fMRI pretraining often fails to capture the brain-to-outcome mapping, limiting performance on disease prediction and behavioral tasks. Leveraging rich environmental and demographic information could produce representations more predictive of clinical outcomes.

Method: Scaleable architecture that performs multitask pretraining by tokenizing multiple BEI and semi-supervised finetuning using pseudo-labels from pretrained BEI, enabling integration of unlabeled fMRI with environmental/demographic data.

Result: Evaluations on sex prediction, human behavior recognition, and early diagnosis of Autism, Parkinson's, Alzheimer's, and Schizophrenia show promising improvements, suggesting strong potential for clinical neuroimaging applications.

Conclusion: A BEI-informed multitask foundation model can better align neuroimaging representations with downstream clinical objectives, enhancing generalization and utility of large-scale unlabeled fMRI data in practice.

Abstract: A reliable foundation model of functional neuroimages is critical to promote
clinical applications where the performance of current AI models is
significantly impeded by a limited sample size. To that end, tremendous efforts
have been made to pretraining large models on extensive unlabeled fMRI data
using scalable self-supervised learning. Since self-supervision is not
necessarily aligned with the brain-to-outcome relationship, most foundation
models are suboptimal to the downstream task, such as predicting disease
outcomes. By capitalizing on rich environmental variables and demographic data
along with an unprecedented amount of functional neuroimages, we form the brain
modeling as a multitask learning and present a scalable model architecture for
(i) multitask pretraining by tokenizing multiple brain-environment interactions
(BEI) and (ii) semi-supervised finetuning by assigning pseudo-labels of
pretrained BEI. We have evaluated our foundation model on a variety of
applications, including sex prediction, human behavior recognition, and disease
early diagnosis of Autism, Parkinson's disease, Alzheimer's disease, and
{Schizophrenia}, where promising results indicate the great potential to
facilitate current neuroimaging applications in clinical routines.

</details>


### [92] [ADPO: Anchored Direct Preference Optimization](https://arxiv.org/abs/2510.18913)
*Wang Zixian*

Main category: cs.LG

TL;DR: ADPO 为 DPO 的统一扩展，加入软偏好、锚定策略和逐类/列表偏好建模，提升在带噪声/不确定性场景的稳定性与性能，并给出清晰的使用指引。


<details>
  <summary>Details</summary>
Motivation: 解决硬标签下的梯度漂移、训练不稳定，以及在有噪声/异常分布时对偏好学习的鲁棒性需求；通过锚定提升训练稳定性并扩展到列表式偏好。

Method: 提出通用框架 ADPO，包含：软偏好概率、任意参考策略锚定（实现组内平移不变性和隐式 KL 正则）、以及 Plackett-Luce 的列表偏好建模。推导出 DPO、Bradley-Terry、Top-1-vs-Rest 为特例，并给出三种实用变体：成对的锚定 Soft-DPO、列表偏好的锚定 Soft-DPO（原始奖励）、以及基于 KDE 的列表平滑以对抗厚尾噪声。

Result: 在情境任务（contextual bandits）中，锚定将 WinMass 相较标准 DPO 提升 38-63%；在厚尾污染下，KDE 平滑使性能从 0.32 提升到 0.68（112% 相对增益）。在序列强化学习（CartPole、LunarLander）中，锚定对有噪声偏好表现提升 15-29%。对 10-256 参数规模的模型给出使用建议：干净/中等噪声优先使用成对锚定 Soft-DPO；极端污染时采用 KDE 基于列表的 ADPO。

Conclusion: ADPO 提供了从单步到多步任务的可迁移方案，通过锚定和列表建模实现对偏好学习的鲁棒性与灵活性；并给出在不同噪声水平的实践性指引。

Abstract: Anchored Direct Preference Optimization (ADPO) is a unified framework that
generalizes Direct Preference Optimization (DPO) with soft preferences,
reference-policy anchoring, and groupwise extensions. While standard DPO
assumes hard binary labels and pairwise comparisons, ADPO introduces: (i) soft
preference probabilities that encode uncertainty and mitigate gradient drift;
(ii) arbitrary reference-policy anchors that stabilize training via groupwise
shift invariance and implicit KL regularization; and (iii) listwise preference
modeling through Plackett-Luce distributions. We prove that DPO, Bradley-Terry
objectives, and Top-1-vs-Rest formulations emerge as special cases. ADPO yields
three practical variants: pairwise anchored Soft-DPO, listwise anchored
Soft-DPO with raw rewards, and KDE-based listwise smoothing for heavy-tailed
noise. In contextual bandits, anchoring improves WinMass by 38-63% over
standard DPO, while KDE smoothing achieves 0.68 vs 0.32 under heavy-tailed
contamination (112% relative gain). In sequential reinforcement learning
(CartPole, LunarLander), anchoring improves noisy-preference performance by
15-29%, confirming transfer from single-step to multi-step settings.
Experiments with 10-256 parameter models provide clear guidance: use pairwise
anchored Soft-DPO for clean or moderate noise, and KDE-based listwise ADPO for
extreme contamination.

</details>


### [93] [Benchmarking On-Device Machine Learning on Apple Silicon with MLX](https://arxiv.org/abs/2510.18921)
*Oluwaseun A. Ajayi,Ogundepo Odunayo*

Main category: cs.LG

TL;DR: 该论文评估 MLX 框架在 Apple Silicon 上用于在设备上推理的性能，重点比较 MLX-Transformers 与 PyTorch 的推理延迟，并展示在 BERT、RoBERTa、XLM-RoBERTa 等模型上的潜在优势。


<details>
  <summary>Details</summary>
Motivation: 日益关注在边缘设备上运行大模型的可行性与高效性，需有针对 Apple 生态的优化框架，并实现跨框架检查点的高效移植与本地部署。

Method: 构建 MLX-transformers，下载并将 PyTorch 检查点转换为 MLX 格式；在两台 Apple Silicon 的 MacBook 与一张 NVIDIA CUDA GPU 上进行推理延迟基准，比较同参数量和相同检查点的模型（BERT、RoBERTa、XLM-RoBERTa），并利用 Hugging Face 的模型进行源头获取与转换。

Result: 结果显示 MLX 在实现设备端高效推理方面具备潜力，能够在苹果生态中实现更高的可访问性和效率；MLX 的变体能够直接从 Hugging Face 获取模型并在 MLX 上无缝执行，降低跨框架迁移的额外成本。

Conclusion: MLX-Transformers 提供了一个在 Apple 生态系统内直接从 Hugging Face 获取并部署 transformer 模型的路径，显示出在本地设备上对比 PyTorch/GPU 的潜在优势，为未来扩展到其他模态提供了基础。

Abstract: The recent widespread adoption of Large Language Models (LLMs) and machine
learning in general has sparked research interest in exploring the
possibilities of deploying these models on smaller devices such as laptops and
mobile phones. This creates a need for frameworks and approaches that are
capable of taking advantage of on-device hardware. The MLX framework was
created to address this need. It is a framework optimized for machine learning
(ML) computations on Apple silicon devices, facilitating easier research,
experimentation, and prototyping.
  This paper presents a performance evaluation of MLX, focusing on inference
latency of transformer models. We compare the performance of different
transformer architecture implementations in MLX with their Pytorch
counterparts. For this research we create a framework called MLX-transformers
which includes different transformer implementations in MLX and downloads the
model checkpoints in pytorch and converts it to the MLX format. By leveraging
the advanced architecture and capabilities of Apple Silicon, MLX-Transformers
enables seamless execution of transformer models directly sourced from Hugging
Face, eliminating the need for checkpoint conversion often required when
porting models between frameworks.
  Our study benchmarks different transformer models on two Apple Silicon
macbook devices against an NVIDIA CUDA GPU. Specifically, we compare the
inference latency performance of models with the same parameter sizes and
checkpoints. We evaluate the performance of BERT, RoBERTa, and XLM-RoBERTa
models, with the intention of extending future work to include models of
different modalities, thus providing a more comprehensive assessment of MLX's
capabilities. The results highlight MLX's potential in enabling efficient and
more accessible on-device ML applications within Apple's ecosystem.

</details>


### [94] [Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients](https://arxiv.org/abs/2510.18924)
*Omar El mansouri,Mohamed El Amine Seddik,Salem Lahlou*

Main category: cs.LG

TL;DR: 提出GRPO/Dr.GRPO，针对RLHF中的奖励腐败建模为伯努利噪声并在估计翻转概率后进行去偏梯度修正，结合分组策略以降低个体噪声，理论与实证均显示鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 在RLHF/RLVR中，对不一致或错误奖励的噪声高度敏感，但现有广泛使用的基于分组的策略优化方法与噪声之间的相互作用尚未充分研究，因此需要一个鲁棒的奖励处理框架以提升现实部署的稳定性和性能。

Method: 提出GRPO与Dr.GRPO框架，显式将奖励腐败建模为伯努利噪声；在估计奖励翻转概率后进行噪声修正，以去偏地更新梯度。理论上分组方法天然可缓解个体层面的噪声，修正策略放大了这一鲁棒性。

Result: 在数学与代码任务中应用于标准奖励模型后，鲁棒性显著提升，数学任务达到约6.7个百分点的准确率增益，代码任务约1.5个百分点增益，且在现实奖励模型条件下表现更为稳健。

Conclusion: 将标签噪声校正的思路引入现代RLHF，提供理论洞见与可放在现实部署中的实用算法，为嘈杂环境下的对齐任务提供更优的鲁棒性与适用性。

Abstract: Reinforcement learning from human feedback (RLHF) or verifiable rewards
(RLVR), the standard paradigm for aligning LLMs or building recent SOTA
reasoning models, is highly sensitive to noise from inconsistent or erroneous
rewards. Yet, the interaction between such noise and widely used group-based
policy optimization methods remains underexplored. We introduce a noise-robust
Group Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO)
framework that explicitly models reward corruption as Bernoulli noise. Our
method applies noise correction after estimating reward flip probabilities to
debias the learning signal, yielding provably unbiased gradient estimates.
Theoretical analysis shows that group-based methods inherently mitigate
individual-level noise, and our correction strategy amplifies this robustness.
Empirically, we observe consistent improvements across math and code tasks when
applying our noise correction to standard reward model usage, with particular
gains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code
tasks under realistic reward model conditions. This work bridges label-noise
correction from supervised learning with modern RLHF, offering both theoretical
insights and a practical algorithm for noisy real-world deployment.

</details>


### [95] [BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.18927)
*Zhiheng Xi,Xin Guo,Yang Nan,Enyu Zhou,Junrui Shen,Wenxiang Chen,Jiaqi Liu,Jixuan Huang,Zhihao Zhang,Honglin Guo,Xun Deng,Zhikai Lei,Miao Zheng,Guoteng Wang,Shuo Zhang,Peng Sun,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: 提出 BAPO，通过自适应裁剪平衡正负梯度、维护熵，从而提升 off-policy RL 的稳定性与数据效率，并在 AIME 基准上实现领先性能。


<details>
  <summary>Details</summary>
Motivation: off-policy RL 在大语言模型对齐中提升样本利用率，但容易出现熵下降、梯度不稳定乃至崩溃，需要新的优化机制来保持探索并稳定学习。

Method: 分析发现负优势样本主导梯度、PPO 类目标的固定裁剪抑制熵增更新；提出 Entropy-Clip Rule；并提出 BAPO：基于自适应裁剪边界的平衡策略，动态调节正负梯度贡献，保持熵与稳定性。

Result: 在样本重放和部分 rollout 等场景中，BAPO 实现更快、稳定、数据高效训练；在 AIME 2024/2025 基准中，7B BAPO 超越 SkyWork-OR1-7B；32B BAPO 在同等规模中达到最强，并超越 o3-mini、Gemini-2.5-Flash-Thinking 等领先系统。

Conclusion: 自适应裁剪平衡和熵保护是提升 off-policy RL 稳定性和样本效率的关键，BAPO 提供一个简单有效的优化策略，具备实际落地潜力。

Abstract: Reinforcement learning (RL) has recently become the core paradigm for
aligning and strengthening large language models (LLMs). Yet, applying RL in
off-policy settings--where stale data from past policies are used for
training--improves sample efficiency, but remains challenging: policy entropy
declines sharply, optimization often becomes unstable and may even collapse.
Through theoretical and empirical analysis, we identify two key insights: (i)
an imbalance in optimization, where negative-advantage samples dominate the
policy gradient, suppressing useful behaviors and risking gradient explosions;
and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping
mechanism in PPO-like objectives systematically blocks entropy-increasing
updates, thereby driving the policy toward over-exploitation at the expense of
exploration. Building on these insights, we propose BAlanced Policy
Optimization with Adaptive Clipping (BAPO), a simple yet effective method that
dynamically adjusts clipping bounds to adaptively re-balance positive and
negative contributions, preserve entropy, and stabilize RL optimization. Across
diverse off-policy scenarios--including sample replay and partial rollout--BAPO
achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025
benchmarks, our 7B BAPO model surpasses open-source counterparts such as
SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art
results among models of the same scale but also outperforms leading proprietary
systems like o3-mini and Gemini-2.5-Flash-Thinking.

</details>


### [96] [NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2510.18940)
*Zhi Zhang,Yixian Shen,Congfeng Cao,Ekaterina Shutova*

Main category: cs.LG

TL;DR: A new PEFT method, NeuroAda, uses selective parameter importance and bypass connections to enable fine-grained finetuning with frozen base parameters, achieving state-of-the-art results on 23+ NLP tasks while using as little as 0.02% trainable parameters and reducing CUDA memory by up to 60%.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-off in existing PEFT methods: addition-based methods offer memory efficiency but limited capacity; selective adaptation offers precision but high memory usage. NeuroAda seeks to combine fine-grained adaptation with low memory overhead.

Method: Identify important network connections for adaptation, then introduce bypass connections around these selected parameters. Freeze the original model parameters and train only the bypass weights during finetuning.

Result: Empirical evaluation on 23+ tasks spanning natural language generation and understanding shows state-of-the-art performance with extremely small trainable parameter counts (≤0.02%) and substantial memory savings (up to 60% CUDA memory reduction).

Conclusion: NeuroAda demonstrates that combining selective parameter emphasis with bypass-based fine-tuning can achieve strong adaptation performance while maintaining high memory efficiency, offering a promising direction for scalable PEFT.

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into
two categories: addition-based and selective in-situ adaptation. The former,
such as LoRA, introduce additional modules to adapt the model to downstream
tasks, offering strong memory efficiency. However, their representational
capacity is often limited, making them less suitable for fine-grained
adaptation. In contrast, the latter directly fine-tunes a carefully chosen
subset of the original model parameters, allowing for more precise and
effective adaptation, but at the cost of significantly increased memory
consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT
method that enables fine-grained model finetuning while maintaining high memory
efficiency. Our approach first identifies important parameters (i.e.,
connections within the network) as in selective adaptation, and then introduces
bypass connections for these selected parameters. During finetuning, only the
bypass connections are updated, leaving the original model parameters frozen.
Empirical results on 23+ tasks spanning both natural language generation and
understanding demonstrate that NeuroAda achieves state-of-the-art performance
with as little as $\leq \textbf{0.02}\%$ trainable parameters, while reducing
CUDA memory usage by up to 60%. We release our code here:
https://github.com/FightingFighting/NeuroAda.git.

</details>


### [97] [Towards Universal Solvers: Using PGD Attack in Active Learning to Increase Generalizability of Neural Operators as Knowledge Distillation from Numerical PDE Solvers](https://arxiv.org/abs/2510.18989)
*Yifei Sun*

Main category: cs.LG

TL;DR: 提出一种对抗性教师-学生蒸馏框架，通过一个可微分数值求解器监督紧凑的神经算子，并用PGD风格的主动采样在平滑性与能量约束下搜索最坏输入以扩展训练集，从而在 Burgers 与 Navier–Stokes 系统上显著提升OOD鲁棒性，同时保持低参数成本和快速推理。


<details>
  <summary>Details</summary>
Motivation: 神经算子在逼近function-to-function映射时，能够实现快速推理，但在训练分布外样本上的泛化表现通常较差。传统非线性PDE求解需要高分辨率的时空离散和局部线性化，导致高内存与慢运行。需要一种方法在不显著增加推理成本的前提下提高对未知输入的鲁棒性。

Method: 引入一个可微分的数值求解器作为教师，监督一个紧凑的神经算子（学生）。通过PGD风格的主动采样循环，在满足输入的平滑性与能量约束下搜索最差输入以扩展训练数据。利用可微分谱解算器实现基于梯度的对抗搜索，从而稳定样本挖掘过程。对 Burgers 和 Navier–Stokes 系统进行实验验证。

Result: 对抗性蒸馏显著提升了模型对分布外输入的鲁棒性（OOD robustness），同时保留了神经算子低参数规模与快速推理的优点。

Conclusion: 该对抗性教师-学生蒸馏框架能够在不显著增加推理成本的前提下提升非线性PDE求解中的OOD鲁棒性，并在典型的PDE系统上表现出良好效果，具备推广到其他算子与方程组的潜力。

Abstract: Nonlinear PDE solvers require fine space-time discretizations and local
linearizations, leading to high memory cost and slow runtimes. Neural operators
such as FNOs and DeepONets offer fast single-shot inference by learning
function-to-function mappings and truncating high-frequency components, but
they suffer from poor out-of-distribution (OOD) generalization, often failing
on inputs outside the training distribution. We propose an adversarial
teacher-student distillation framework in which a differentiable numerical
solver supervises a compact neural operator while a PGD-style active sampling
loop searches for worst-case inputs under smoothness and energy constraints to
expand the training set. Using differentiable spectral solvers enables
gradient-based adversarial search and stabilizes sample mining. Experiments on
Burgers and Navier-Stokes systems demonstrate that adversarial distillation
substantially improves OOD robustness while preserving the low parameter cost
and fast inference of neural operators.

</details>


### [98] [An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version](https://arxiv.org/abs/2510.18998)
*Buang Zhang,Tung Kieu,Xiangfei Qiu,Chenjuan Guo,Jilin Hu,Aoying Zhou,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: 提出一种编码-解耦（encode-then-decompose）范式用于时间序列异常检测，通过将编码表示分解为稳定与辅助表示以提高对污染训练数据的鲁棒性，并以基于互信息的新颖指标替代重构误差来识别异常；在八个基准数据集上达到竞争性或最先进的性能，并对不同污染比例具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自编码器在训练数据中存在异常时容易产生错误的表示，从而降低异常检测准确性；需要一种对污染更鲁棒的无监督方法。

Method: 先对时间序列进行编码，再将编码表示分解为稳定表示和辅助表示，并据此定义异常分数；引入基于互信息的度量来替代传统的重构误差用于异常检测。

Result: 在八个常用多变量/单变量时间序列基准上表现出竞争性或最优性能，且对不同污染比例具有鲁棒性。

Conclusion: 编码-解耦与互信息度量相结合的框架能提升对污染训练数据的鲁棒性并提供有效的异常检测，具有广泛应用潜力。

Abstract: Time series anomaly detection is important in modern large-scale systems and
is applied in a variety of domains to analyze and monitor the operation of
diverse systems. Unsupervised approaches have received widespread interest, as
they do not require anomaly labels during training, thus avoiding potentially
high costs and having wider applications. Among these, autoencoders have
received extensive attention. They use reconstruction errors from compressed
representations to define anomaly scores. However, representations learned by
autoencoders are sensitive to anomalies in training time series, causing
reduced accuracy. We propose a novel encode-then-decompose paradigm, where we
decompose the encoded representation into stable and auxiliary representations,
thereby enhancing the robustness when training with contaminated time series.
In addition, we propose a novel mutual information based metric to replace the
reconstruction errors for identifying anomalies. Our proposal demonstrates
competitive or state-of-the-art performance on eight commonly used multi- and
univariate time series benchmarks and exhibits robustness to time series with
different contamination ratios.

</details>


### [99] [Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records](https://arxiv.org/abs/2510.19014)
*Saman Nessari,Ali Bozorgi-Amiri*

Main category: cs.LG

TL;DR: A unified AI-driven framework combines LLMs, CTGANs, T-learner counterfactuals, and contextual bandits to tailor medical treatment, demonstrated on stage III colon cancer data with improved online learning rewards.


<details>
  <summary>Details</summary>
Motivation: Overcome patient heterogeneity and cold-start limitations by integrating NLP, synthetic data generation, counterfactual inference, and online decision strategies in clinical recommendations.

Method: LLMs convert unstructured medical narratives to structured data (93.2% accuracy); CTGAN generates realistic synthetic patient data (55% accuracy via two-sample verification); T-learners predict patient-specific treatment responses (84.3% accuracy); prior-informed contextual bandits (KernelUCB) balance exploration with exploitation for online therapy selection; tested on stage III colon cancer datasets, achieving 0.60-0.61 average reward over 5,000 rounds.

Result: KernelUCB achieved 0.60–0.61 average reward across 5,000 rounds, outperforming reference methods; the approach mitigates cold-start in online learning, improves computational efficiency, and advances individualized medicine.

Conclusion: The work represents a significant move toward data-informed, patient-specific treatment frameworks and online adaptive decision-making in oncology.

Abstract: Current medical practice depends on standardized treatment frameworks and
empirical methodologies that neglect individual patient variations, leading to
suboptimal health outcomes. We develop a comprehensive system integrating Large
Language Models (LLMs), Conditional Tabular Generative Adversarial Networks
(CTGAN), T-learner counterfactual models, and contextual bandit approaches to
provide customized, data-informed clinical recommendations. The approach
utilizes LLMs to process unstructured medical narratives into structured
datasets (93.2% accuracy), uses CTGANs to produce realistic synthetic patient
data (55% accuracy via two-sample verification), deploys T-learners to forecast
patient-specific treatment responses (84.3% accuracy), and integrates
prior-informed contextual bandits to enhance online therapeutic selection by
effectively balancing exploration of new possibilities with exploitation of
existing knowledge. Testing on stage III colon cancer datasets revealed that
our KernelUCB approach obtained 0.60-0.61 average reward scores across 5,000
rounds, exceeding other reference methods. This comprehensive system overcomes
cold-start limitations in online learning environments, improves computational
effectiveness, and constitutes notable progress toward individualized medicine
adapted to specific patient characteristics.

</details>


### [100] [Empowering Decision Trees via Shape Function Branching](https://arxiv.org/abs/2510.19040)
*Nakul Upadhya,Eldan Cohen*

Main category: cs.LG

TL;DR: 提出 Shape Generalized Tree (SGT) 与 ShapeCART 等扩展，使用单特征的可学习形状函数替代轴对齐分裂，实现可视化可解释的非线性分割，并扩展到双变量与多叉结构，提升性能同时减小模型规模。


<details>
  <summary>Details</summary>
Motivation: 传统决策树以轴对齐线性分裂难以表达非线性特征效应，导致需要深树且可解释性下降。提出可视化的形状分裂以在单一分裂中实现丰富非线性，同时保留可解释性。

Method: 在每个内部节点引入可学习的轴对齐形状函数；提出 ShapeCART 学习算法；扩展到二变量形状(S^2GT)与多叉树(SGT_K)，并给出 Shape^2CART 与 ShapeCART_K。

Result: 在多数据集上，SGTs 表现优于传统轴对齐线性树，模型规模更小，保持或提升准确度。

Conclusion: SGT 框架为解释性强且高效的非线性决策树提供了新方向，适用于需要可视化解释的场景。

Abstract: Decision trees are prized for their interpretability and strong performance
on tabular data. Yet, their reliance on simple axis-aligned linear splits often
forces deep, complex structures to capture non-linear feature effects,
undermining human comprehension of the constructed tree. To address this
limitation, we propose a novel generalization of a decision tree, the Shape
Generalized Tree (SGT), in which each internal node applies a learnable
axis-aligned shape function to a single feature, enabling rich, non-linear
partitioning in one split. As users can easily visualize each node's shape
function, SGTs are inherently interpretable and provide intuitive, visual
explanations of the model's decision mechanisms. To learn SGTs from data, we
propose ShapeCART, an efficient induction algorithm for SGTs. We further extend
the SGT framework to bivariate shape functions (S$^2$GT) and multi-way trees
(SGT$_K$), and present Shape$^2$CART and ShapeCART$_K$, extensions to ShapeCART
for learning S$^2$GTs and SGT$_K$s, respectively. Experiments on various
datasets show that SGTs achieve superior performance with reduced model size
compared to traditional axis-aligned linear trees.

</details>


### [101] [POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2510.19056)
*Kuai Yu,Xiaoyu Wu,Peishen Yan,Qingqian Yang,Linshan Jiang,Hao Wang,Yang Hua,Tao Song,Haibing Guan*

Main category: cs.LG

TL;DR: POLAR uses reinforcement learning to select layer-wise backdoor modifications in federated learning, optimizing for attack effectiveness while constraining footprint; it improves over prior rule-based BC layer methods and defeats defenses in evaluations.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks in federated learning can be stealthy when targeting critical layers (BC layers), but existing methods rely on brittle rule-based selections that can be detected. A learnable, layer interrelation-aware strategy could increase stealthiness and effectiveness.

Method: POLAR employs a lightweight Bernoulli-sampling reinforcement learning approach to learn a policy for selecting BC layers. It updates the policy via policy gradient driven by backdoor success rate improvements, with a regularization term penalizing large attack footprints to maintain stealth.

Result: POLAR outperforms recent attack methods, achieving up to 40% higher backdoor success rate against six state-of-the-art defenses.

Conclusion: A policy-based, layer-wise RL framework can effectively and stealthily identify BC layers for backdoor attacks in FL, suggesting both a potent threat and a new direction for evaluating and defending against backdoors with dynamic layer selection.

Abstract: Federated Learning (FL) enables decentralized model training across multiple
clients without exposing local data, but its distributed feature makes it
vulnerable to backdoor attacks. Despite early FL backdoor attacks modifying
entire models, recent studies have explored the concept of backdoor-critical
(BC) layers, which poison the chosen influential layers to maintain
stealthiness while achieving high effectiveness. However, existing BC layers
approaches rely on rule-based selection without consideration of the
interrelations between layers, making them ineffective and prone to detection
by advanced defenses. In this paper, we propose POLAR (POlicy-based LAyerwise
Reinforcement learning), the first pipeline to creatively adopt RL to solve the
BC layer selection problem in layer-wise backdoor attack. Different from other
commonly used RL paradigm, POLAR is lightweight with Bernoulli sampling. POLAR
dynamically learns an attack strategy, optimizing layer selection using policy
gradient updates based on backdoor success rate (BSR) improvements. To ensure
stealthiness, we introduce a regularization constraint that limits the number
of modified layers by penalizing large attack footprints. Extensive experiments
demonstrate that POLAR outperforms the latest attack methods by up to 40%
against six state-of-the-art (SOTA) defenses.

</details>


### [102] [What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning](https://arxiv.org/abs/2510.19099)
*Yaning Jia,Chunhui Zhang,Xingjian Diao,Xiangchi Yuan,Zhongyu Ouyang,soroush vosoughi*

Main category: cs.LG

TL;DR: Curriculum learning 的效果并非普遍适用；通过五维度（问题难度、模型意外概率、置信边际、预测不确定性、决策变异）对难度进行统一离线评估，在多种模型和任务上发现前向/反向 CL 的优劣取决于模型能力与任务复杂度，且不同难度层面的样本对不同任务需求产生不同收益；在任务对齐的 curricula 与内部状态 verändert 的 curricula 之间存在本质差异，且一些指标（如决策不确定性）下样本优先化可能进一步提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 弥补以往对 CURRICULUM LEARNING 的难度度量和训练设置不统一的不足，提出一个统一的离线评估框架，以回答何时有益、哪种方向更优、以及衡量标准如何影响结果。

Method: 提出一个五维度的难度分解框架（问题难度、模型惊异度、置信边缘、预测不确定性、决策变异），在数学推理基准上对 Llama3.1-8B、Mistral-7B、Gemma3-4B 等模型进行受控的后训练实验（post-training）。

Result: 结论是：没有普遍占优的 curriculum 策略；前向与反向 CL 的相对有效性取决于模型能力与任务复杂度；即使在同一度量下，不同难度等级的样本对任务的收益也不同；任务对齐的 curricula 主要塑造模型的最终表征和泛化能力，而内部状态相关的 curricula 则影响信心与不确定性等内部状态；部分度量（如决策不确定性）指向的样本优先化在某些场景下能进一步提升学习效果。

Conclusion: 挑战了“普适性 Curriculum 策略”的观点，提供了跨模型与任务范式的实用指引，强调在选择 curriculum 时应结合模型能力、任务复杂度及目标（表征、泛化、内部状态等）进行权衡。

Abstract: Curriculum learning (CL) - ordering training data from easy to hard - has
become a popular strategy for improving reasoning in large language models
(LLMs). Yet prior work employs disparate difficulty metrics and training
setups, leaving open fundamental questions: When does curriculum help? Which
direction - forward or reverse - is better? And does the answer depend on what
we measure? We address these questions through a unified offline evaluation
framework that decomposes curriculum difficulty into five complementary
dimensions: Problem Difficulty, Model Surprisal, Confidence Margin, Predictive
Uncertainty, and Decision Variability. Through controlled post-training
experiments on mathematical reasoning benchmarks with Llama3.1-8B, Mistral-7B,
and Gemma3-4B, we find that (i) no curriculum strategy dominates universally -
the relative effectiveness of forward versus reverse CL depends jointly on
model capability and task complexity; (ii) even within a single metric, samples
at different difficulty levels produce distinct gains depending on task
demands; and (iii) task-aligned curricula focus on shaping the model's final
representations and generalization, whereas inner-state curricula modulate
internal states such as confidence and uncertainty. Our findings challenge the
notion of a universal curriculum strategy and offer actionable guidance across
model and task regimes, with some metrics indicating that prioritizing
decision-uncertain samples can further enhance learning outcomes.

</details>


### [103] [MetaCluster: Enabling Deep Compression of Kolmogorov-Arnold Network](https://arxiv.org/abs/2510.19105)
*Matthew Raffel,Adwaith Renjith,Lizhong Chen*

Main category: cs.LG

TL;DR: MetaCluster通过一个轻量级元学习器将低维嵌入映射到系数向量，使其落在易聚类的低维流形上，并对系数空间执行K-means实现共享质心，随后丢弃元学习器并微调质心码本，极大地压缩KAN参数存储而不损失精度。


<details>
  <summary>Details</summary>
Motivation: KAN通过对边的向量系数来提升表达能力，但导致参数和内存显著增加。需要在不降低准确率的前提下实现高效压缩，便于部署。

Method: 训练一个轻量级的元学习器，与KAN共同优化，将低维嵌入映射到系数向量，使它们在低维流形上便于聚类。对系数空间执行K-means，将每条边的向量替换为共享质心。随后丢弃元学习器，对质心码本进行简短的微调以恢复残差精度。最终模型仅存储少量码本和边-质心索引。

Result: 在MNIST、CIFAR-10、CIFAR-100上，对标准KAN和ConvKAN在多种基函数下，MetaCluster实现了最大80×的参数存储压缩，且不损失精度。

Conclusion: MetaCluster实现了高压缩比的KAN，同时保持准确性，具备实际部署潜力；代码将在正式发表时发布。

Abstract: Kolmogorov-Arnold Networks (KANs) replace scalar weights with per-edge
vectors of basis coefficients, thereby boosting expressivity and accuracy but
at the same time resulting in a multiplicative increase in parameters and
memory. We propose MetaCluster, a framework that makes KANs highly compressible
without sacrificing accuracy. Specifically, a lightweight meta-learner, trained
jointly with the KAN, is used to map low-dimensional embedding to coefficient
vectors, shaping them to lie on a low-dimensional manifold that is amenable to
clustering. We then run K-means in coefficient space and replace per-edge
vectors with shared centroids. Afterwards, the meta-learner can be discarded,
and a brief fine-tuning of the centroid codebook recovers any residual accuracy
loss. The resulting model stores only a small codebook and per-edge indices,
exploiting the vector nature of KAN parameters to amortize storage across
multiple coefficients. On MNIST, CIFAR-10, and CIFAR-100, across standard KANs
and ConvKANs using multiple basis functions, MetaCluster achieves a reduction
of up to 80$\times$ in parameter storage, with no loss in accuracy. Code will
be released upon publication.

</details>


### [104] [Steering Autoregressive Music Generation with Recursive Feature Machines](https://arxiv.org/abs/2510.19127)
*Daniel Zhao,Daniel Beaglehole,Taylor Berg-Kirkpatrick,Julian McAuley,Zachary Novack*

Main category: cs.LG

TL;DR: MusicRFM提出一种利用递归特征机（RFMs）的探针，在冻结、预训练的音乐模型中实现细粒度、可解释的控制。通过在模型内部激活空间上发现“概念方向”并在推理时注入这些方向来引导生成，且无需逐步优化。方法包括轻量级探针训练、时间变化调度、以及多属性约束，效果在提高目标音符生成准确性方面显著，同时对文本提示忠诚度影响很小。并公布代码以促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 可控音乐生成仍是一个挑战，现有方法常需对模型重新训练或引入可听的伪影；需要对冻结、预训练模型进行细粒度、可解释的控制，而不改变模型权重。

Method: 训练轻量级的RFM探针以在MusicGen的隐藏状态中发现概念方向；推理时将这些方向注入模型，实时引导生成，无需逐步优化；通过动态时间调度和多属性约束实现对多个音乐属性的并行控制。

Result: 将目标音符生成的准确性从0.23提升到0.82；文本提示的依从性与未引导基线相比约仅差0.02，表明在保持提示保真度的同时实现有效控制；在生成质量上的影响被约束在可接受范围内。

Conclusion: MusicRFM能够在不对冻结模型进行再训练的情况下实现对音乐生成的细粒度、可解释控制，兼顾生成质量与提示忠诚度，并公开代码以促进在音乐领域对RFMs的探索。

Abstract: Controllable music generation remains a significant challenge, with existing
methods often requiring model retraining or introducing audible artifacts. We
introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs)
to enable fine-grained, interpretable control over frozen, pre-trained music
models by directly steering their internal activations. RFMs analyze a model's
internal gradients to produce interpretable "concept directions", or specific
axes in the activation space that correspond to musical attributes like notes
or chords. We first train lightweight RFM probes to discover these directions
within MusicGen's hidden states; then, during inference, we inject them back
into the model to guide the generation process in real-time without per-step
optimization. We present advanced mechanisms for this control, including
dynamic, time-varying schedules and methods for the simultaneous enforcement of
multiple musical properties. Our method successfully navigates the trade-off
between control and generation quality: we can increase the accuracy of
generating a target musical note from 0.23 to 0.82, while text prompt adherence
remains within approximately 0.02 of the unsteered baseline, demonstrating
effective control with minimal impact on prompt fidelity. We release code to
encourage further exploration on RFMs in the music domain.

</details>


### [105] [InvarGC: Invariant Granger Causality for Heterogeneous Interventional Time Series under Latent Confounding](https://arxiv.org/abs/2510.19138)
*Ziyi Zhang,Shaogang Ren,Xiaoning Qian,Nick Duffield*

Main category: cs.LG

TL;DR: InvarGC proposes invariant Granger causality to detect causal relations in multivariate time series under latent confounding and unknown intervened environments by leveraging cross-environment heterogeneity, achieving edge-level identifyability and competitive performance on synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: Traditional linear Granger causality fails with non-linear relations and fails under latent confounders and unknown interventions. There is a need to exploit across-environment heterogeneity to identify invariant causal structures.

Method: Introduce Invariant Granger Causality (InvarGC) that uses cross-environment heterogeneity to mitigate latent confounding and to distinguish intervened from non-intervened environments at the edge level, enabling identification of invariant causal relations; provide identifiability results under these conditions.

Result: Extensive experiments on synthetic and real-world datasets show competitive performance compared to state-of-the-art non-linear Granger causality methods.

Conclusion: InvarGC enables recovery of invariant causal relations in non-linear, confounded time-series settings with unknown interventions, backed by identifiability results and strong empirical performance.

Abstract: Granger causality is widely used for causal structure discovery in complex
systems from multivariate time series data. Traditional Granger causality tests
based on linear models often fail to detect even mild non-linear causal
relationships. Therefore, numerous recent studies have investigated non-linear
Granger causality methods, achieving improved performance. However, these
methods often rely on two key assumptions: causal sufficiency and known
interventional targets. Causal sufficiency assumes the absence of latent
confounders, yet their presence can introduce spurious correlations. Moreover,
real-world time series data usually come from heterogeneous environments,
without prior knowledge of interventions. Therefore, in practice, it is
difficult to distinguish intervened environments from non-intervened ones, and
even harder to identify which variables or timesteps are affected. To address
these challenges, we propose Invariant Granger Causality (InvarGC), which
leverages cross-environment heterogeneity to mitigate the effects of latent
confounding and to distinguish intervened from non-intervened environments with
edge-level granularity, thereby recovering invariant causal relations. In
addition, we establish the identifiability under these conditions. Extensive
experiments on both synthetic and real-world datasets demonstrate the
competitive performance of our approach compared to state-of-the-art methods.

</details>


### [106] [Subliminal Corruption: Mechanisms, Thresholds, and Interpretability](https://arxiv.org/abs/2510.19152)
*Reya Vir,Sarvesh Bhatnagar*

Main category: cs.LG

TL;DR: 本研究在教师-学生框架下，量化研究通过合成数据传播的潜在腐败（subliminal corruption），发现其会导致行为全面退化、在被污染数据的临界阈值处呈现剧烈相变，且机制与模型微调相似、难以被检测，因此提出需要新的安全协议来应对潜伏威胁。


<details>
  <summary>Details</summary>
Motivation: 在广泛使用合成数据进行微调与知识蒸馏的时代，尚缺乏对潜在隐性错位扩散动力学的定量理解。该工作旨在揭示规模效应、阈值和传播机制，以评估系统性的安全风险。

Method: 采用教师-学生设置（以 GPT-2 为研究对象），系统性地研究规模法则、污染阈值与传播机制，结合对齐指标的变化、阈值分析以及可解释性分析来揭示腐败的传播路径与检测难度。

Result: （1）腐败引发行为跨越，降低了模型的整体对齐水平；（2）对齐在污染数据达到临界阈值时呈现尖锐的相变，而非逐步退化；（3）可解释性分析显示腐败机制与模型的自然微调过程高度相似，导致检测困难。

Conclusion: 强调了在依赖合成数据的AI系统中存在的关键脆弱性，需制定新的安全协议以应对潜在的隐性威胁，并加强对这类隐性腐败的监测与防护。

Abstract: As machine learning models are increasingly fine-tuned on synthetic data,
there is a critical risk of subtle misalignments spreading through
interconnected AI systems. This paper investigates subliminal corruption, which
we define as undesirable traits are transmitted through semantically neutral
data, bypassing standard safety checks. While this phenomenon has been
identified, a quantitative understanding of its dynamics is missing. To address
this gap, we present a systematic study of the scaling laws, thresholds, and
mechanisms of subliminal corruption using a teacher-student setup with GPT-2.
Our experiments reveal three key findings: (1) subliminal corruption causes
behavioral crossover, degrading the model's overall alignment, not just the
targeted trait; (2) alignment fails in a sharp phase transition at a critical
threshold of poisoned data, rather than degrading gradually; and (3)
interpretability analysis shows the corruption mechanism mimics the model's
natural fine-tuning process, making it difficult to detect. These results
demonstrate a critical vulnerability in AI systems that rely on synthetic data
and highlight the need for new safety protocols that can account for latent
threats.

</details>


### [107] [Feature Space Adaptation for Robust Model Fine-Tuning](https://arxiv.org/abs/2510.19155)
*Peng Wang,Minghao Gu,Qiang Huang*

Main category: cs.LG

TL;DR: 提出在特征空间进行微调的两种方法 LoRFA 和 VeFA，将分布漂移下的鲁棒性提高作为核心，同时与 LoRA 在微调效果基本持平。


<details>
  <summary>Details</summary>
Motivation: 解决灾难性遗忘与下游数据稀缺情形中，权重空间微调容易过拟合并覆盖预训练知识的问题；寻求在特征层面进行轻量变换以保持预训练知识并提升鲁棒性。

Method: 在特征空间引入低秩特征适应（LoRFA）和向量级特征适应（VeFA）。受效果等价建模（EEM）理论启发，通过对下游潜在变量在观测特征上的等效量进行轻量化的特征变换，从而抵消下游漂移对表示的影响。

Result: 在图像分类、自然语言理解和生成等任务上，LoRFA/VeFA 与 LoRA 的微调效果相近，但在鲁棒性和对分布漂移的抵抗方面表现更出色。

Conclusion: 特征空间微调为参数高效微调提供了有效替代路径，能更好地保护预训练知识并提升跨分布的泛化能力，LoRFA 与 VeFA 为实现这一目标提供了两种实用实现方案。

Abstract: Catastrophic forgetting is a common issue in model fine-tuning, especially
when the downstream domain contains limited labeled data or differs greatly
from the pre-training distribution. Existing parameter-efficient fine-tuning
methods operate in the weight space by modifying or augmenting the pre-trained
model's parameters, which can yield models overly specialized to the available
downstream data. To mitigate the risk of overwriting pre-trained knowledge and
enhance robustness, we propose to fine-tune the pre-trained model in the
feature space. Two new fine-tuning methods are proposed: LoRFA (Low-Rank
Feature Adaptation) and VeFA (Vector-Based Feature Adaptation). Feature space
adaptation is inspired by the idea of effect equivalence modeling (EEM) of
downstream lurking variables causing distribution shifts, which posits that
unobserved factors can be represented as the total equivalent amount on
observed features. By compensating for the effects of downstream lurking
variables via a lightweight feature-level transformation, the pre-trained
representations can be preserved, which improves model generalization under
distribution shift. We evaluate LoRFA and VeFA versus LoRA on image
classification, NLU, and NLG, covering both standard fine-tuning metrics and
robustness. Feature space adaptation achieves comparable fine-tuning results
and consistently stronger robustness.

</details>


### [108] [Instance-Dependent Regret Bounds for Nonstochastic Linear Partial Monitoring](https://arxiv.org/abs/2510.19158)
*Federico Di Gennaro,Khaled Eldowa,Nicolò Cesa-Bianchi*

Main category: cs.LG

TL;DR: 将线性部分监控推广到无限结果空间，损失与观测线性解耦，提出通过探索-优化的简单实现；给出对抗性有限动作版本的实例相关 regret 边界，展示与博弈结构的紧耦合，速率从 sqrt(T)（易（局部可观测））到 T^{2/3}（难（全局可观测）），并在多场景中实例化，证实依赖结构的紧性。


<details>
  <summary>Details</summary>
Motivation: 在无限结果空间且希望利用损失与反馈的线性结构下，传统的部分监控难以提供清晰的结构化界限。本工作希望通过线性框架与对抗性设置，获得更透明且可量化的与博弈结构相关的 regret 边界。该模型同时承接旧/new 的部分信息设定，提供统一的分析视角。

Method: 采用一个简单的探索-优化实例，在有限动作的对抗性情形中实现高效算法；通过引入表征观测-损失对齐程度的实例量，推导出依赖博弈结构的 regret 上界；对易/难游戏给出对应的速率，并在若干子设定中逐一实例化。

Result: 得到的 regret 上界随博弈结构而定，易游戏达 sqrt(T)，难游戏达 T^{2/3}；上界包含与观测-损失对齐相关的实例化量，且在若干场景下能达到紧上界。与以往的理论保障相比，边界更透明、可解释且更接近随机设置的结果。

Conclusion: 框架将线性部分监控推广为一个可理解、可实例化的分析工具，扩展了线性带宽的思路，覆盖多种旧/new 的部分信息设定；并揭示博弈结构对性能的决定性影响，在一些重要案例中该依赖关系是紧致的。

Abstract: In contrast to the classic formulation of partial monitoring, linear partial
monitoring can model infinite outcome spaces, while imposing a linear structure
on both the losses and the observations. This setting can be viewed as a
generalization of linear bandits where loss and feedback are decoupled in a
flexible manner. In this work, we address a nonstochastic (adversarial),
finite-actions version of the problem through a simple instance of the
exploration-by-optimization method that is amenable to efficient
implementation. We derive regret bounds that depend on the game structure in a
more transparent manner than previous theoretical guarantees for this paradigm.
Our bounds feature instance-specific quantities that reflect the degree of
alignment between observations and losses, and resemble known guarantees in the
stochastic setting. Notably, they achieve the standard $\sqrt{T}$ rate in easy
(locally observable) games and $T^{2/3}$ in hard (globally observable) games,
where $T$ is the time horizon. We instantiate these bounds in a selection of
old and new partial information settings subsumed by this model, and illustrate
that the achieved dependence on the game structure can be tight in interesting
cases.

</details>


### [109] [Preliminary Use of Vision Language Model Driven Extraction of Mouse Behavior Towards Understanding Fear Expression](https://arxiv.org/abs/2510.19160)
*Paimon Goulart,Jordan Steinhauser,Kylene Shuler,Edward Korzus,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 提出一种将文本-视频联合编码的VLM来对小鼠行为进行时间序列分类，并在不微调的情况下通过提示、输入示例学习和逐帧预处理提升性能，生成可用于跨环境的行为向量数据集。


<details>
  <summary>Details</summary>
Motivation: 整合多源数据以提升跨学科行为研究的能力，提供一个可持续、可扩展的行为向量数据集，促进对复杂研究问题的探究。

Method: 以开源Qwen2.5-VL为基础，通过文本提示、上下文学习（ICL）和帧级预处理对模型进行增强，实现对视频中多种小鼠行为的分类，输出每个个体在每个会话中的时序行为向量；不进行微调。

Result: 在所有行为上获得较高的F1分数，尤其是罕见行为如冻结和逃跑等也能获得显著的正确率提升；各方法组合效果最好。

Conclusion: 所提出的VLM可以帮助跨学科研究者整合不同时间点和环境中的行为特征，形成一个全面的数据集，支持解决更复杂的研究问题。

Abstract: Integration of diverse data will be a pivotal step towards improving
scientific explorations in many disciplines. This work establishes a
vision-language model (VLM) that encodes videos with text input in order to
classify various behaviors of a mouse existing in and engaging with their
environment. Importantly, this model produces a behavioral vector over time for
each subject and for each session the subject undergoes. The output is a
valuable dataset that few programs are able to produce with as high accuracy
and with minimal user input. Specifically, we use the open-source Qwen2.5-VL
model and enhance its performance through prompts, in-context learning (ICL)
with labeled examples, and frame-level preprocessing. We found that each of
these methods contributes to improved classification, and that combining them
results in strong F1 scores across all behaviors, including rare classes like
freezing and fleeing, without any model fine-tuning. Overall, this model will
support interdisciplinary researchers studying mouse behavior by enabling them
to integrate diverse behavioral features, measured across multiple time points
and environments, into a comprehensive dataset that can address complex
research questions.

</details>


### [110] [Natural Gradient VI: Guarantees for Non-Conjugate Models](https://arxiv.org/abs/2510.19163)
*Fangyuan Sun,Ilyas Fatkhullin,Niao He*

Main category: cs.LG

TL;DR: 在非共轭似然下的随机自然梯度变分推断（NGVI）中，论文建立了变分损失相对光滑性的条件，并在此几何结构下提出带非欧投影的修改NGVI，给出全局非渐近收敛到驻点的保证；在额外的似然结构假设下，揭示隐式凸性并实现快速全局收敛到全局最优。


<details>
  <summary>Details</summary>
Motivation: 尽管NGVI广泛用于近似后验，但在非共轭情形下理论基础薄弱，特别是变分损失非凸且难以分析。本文旨在揭示与镜像映射相关的几何结构并提供收敛性保障。

Method: 1）给出使变分损失相对光滑的充分条件，针对均值场参数化；2）提出含非欧投影的改进NGVI算法并证明其全局非渐近收敛至驻点；3）在额外的似然结构假设下，揭示变分损失的隐式凸性并实现快速全局收敛。

Result: 给出相对光滑性的充分条件、证明算法在非凸情形下收敛到驻点，并在额外假设下实现隐式凸性及对全局最优的快速收敛。

Conclusion: 这些结果深化了对NGVI在非共轭似然下的几何与收敛行为的理解，为在具有挑战性的推断设置中提供理论保障和更高效的算法奠定基础。

Abstract: Stochastic Natural Gradient Variational Inference (NGVI) is a widely used
method for approximating posterior distribution in probabilistic models.
Despite its empirical success and foundational role in variational inference,
its theoretical underpinnings remain limited, particularly in the case of
non-conjugate likelihoods. While NGVI has been shown to be a special instance
of Stochastic Mirror Descent, and recent work has provided convergence
guarantees using relative smoothness and strong convexity for conjugate models,
these results do not extend to the non-conjugate setting, where the variational
loss becomes non-convex and harder to analyze. In this work, we focus on
mean-field parameterization and advance the theoretical understanding of NGVI
in three key directions. First, we derive sufficient conditions under which the
variational loss satisfies relative smoothness with respect to a suitable
mirror map. Second, leveraging this structure, we propose a modified NGVI
algorithm incorporating non-Euclidean projections and prove its global
non-asymptotic convergence to a stationary point. Finally, under additional
structural assumptions about the likelihood, we uncover hidden convexity
properties of the variational loss and establish fast global convergence of
NGVI to a global optimum. These results provide new insights into the geometry
and convergence behavior of NGVI in challenging inference settings.

</details>


### [111] [Imbalanced Gradients in RL Post-Training of Multi-Task LLMs](https://arxiv.org/abs/2510.19178)
*Runzhe Wu,Ankur Samanta,Ayush Jain,Scott Fujimoto,Jeongyeol Kwon,Ben Kretzu,Youliang Yu,Kaveh Hassani,Boris Vidolov,Yonathan Efroni*

Main category: cs.LG

TL;DR: RL后训练中任务梯度存在显著不平衡，大梯度任务并不带来更大收益，常用训练统计无法解释这种不平衡，需要在梯度维度进行校正以改进多任务后训练。


<details>
  <summary>Details</summary>
Motivation: 理解多任务RL后训练中梯度不平衡的原因及其对模型学习收益的影响，探索为何简单混合数据会带来偏差，以及为LLM后训练提供改进方向。

Method: 对多任务RL后训练中的各任务梯度进行分任务分析，考察梯度大小与各自学习收益的相关性；比较奖励、优势等训练信号对梯度不平衡的解释能力；进行实证分析以验证梯度大小是否能预测收益。

Result: 存在显著的梯度不平衡，且大梯度任务并不必然带来更高的学习收益，甚至可能较低；传统训练统计无法充分解释不平衡现象，暗示任务间固有差异是重要因素；因此需要谨慎对待数据混合策略并探索梯度级别的修正方法。

Conclusion: 应避免简单的任务混合训练，未来应发展在梯度层面进行校正的策略，以提升LLMs的多任务后训练效果。

Abstract: Multi-task post-training of large language models (LLMs) is typically
performed by mixing datasets from different tasks and optimizing them jointly.
This approach implicitly assumes that all tasks contribute gradients of similar
magnitudes; when this assumption fails, optimization becomes biased toward
large-gradient tasks. In this paper, however, we show that this assumption
fails in RL post-training: certain tasks produce significantly larger
gradients, thus biasing updates toward those tasks. Such gradient imbalance
would be justified only if larger gradients implied larger learning gains on
the tasks (i.e., larger performance improvements) -- but we find this is not
true. Large-gradient tasks can achieve similar or even much lower learning
gains than small-gradient ones. Further analyses reveal that these gradient
imbalances cannot be explained by typical training statistics such as training
rewards or advantages, suggesting that they arise from the inherent differences
between tasks. This cautions against naive dataset mixing and calls for future
work on principled gradient-level corrections for LLMs.

</details>


### [112] [An Active Diffusion Neural Network for Graphs](https://arxiv.org/abs/2510.19202)
*Mengying Jiang*

Main category: cs.LG

TL;DR: 引入主动扩散ADGNN，通过外部信息源驱动扩散并得到闭式解，解决基于扩散的GNN的过度平滑问题，提升对全局信息的捕捉与表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散型GNN多为被动扩散，导致节点表示趋于同质，难以保留各自特征并捕捉全局结构；因此需要通过外部信息源驱动扩散、实现真正的无限扩散来提升性能。

Method: 将多源外部信息纳入扩散过程，形成主动扩散迭代公式，并推导其闭式解，使得节点在保持自身特征的同时获得全局结构信息；在多任务图数据上评估，与前沿模型对比。

Result: 在多项图任务上，ADGNN显著提升准确性与效率，能够更好地捕捉全局图信息并保持节点辨识度。

Conclusion: ADGNN通过主动扩散克服过度平滑，实现真正的无限扩散，提供了一种高效、可扩展的全局信息融合GNN框架。

Abstract: The analogy to heat diffusion has enhanced our understanding of information
flow in graphs and inspired the development of Graph Neural Networks (GNNs).
However, most diffusion-based GNNs emulate passive heat diffusion, which still
suffers from over-smoothing and limits their ability to capture global graph
information. Inspired by the heat death of the universe, which posits that
energy distribution becomes uniform over time in a closed system, we recognize
that, without external input, node representations in a graph converge to
identical feature vectors as diffusion progresses. To address this issue, we
propose the Active Diffusion-based Graph Neural Network (ADGNN). ADGNN achieves
active diffusion by integrating multiple external information sources that
dynamically influence the diffusion process, effectively overcoming the
over-smoothing problem. Furthermore, our approach realizes true infinite
diffusion by directly calculating the closed-form solution of the active
diffusion iterative formula. This allows nodes to preserve their unique
characteristics while efficiently gaining comprehensive insights into the
graph's global structure. We evaluate ADGNN against several state-of-the-art
GNN models across various graph tasks. The results demonstrate that ADGNN
significantly improves both accuracy and efficiency, highlighting its
effectiveness in capturing global graph information and maintaining node
distinctiveness.

</details>


### [113] [Enhancing Graph Neural Networks: A Mutual Learning Approach](https://arxiv.org/abs/2510.19223)
*Paul Agbaje,Akajyoti Mitra,Afia Anjum,Pranali Khose,Ebelechukwu Nwafor,Habeeb Olufowobi*

Main category: cs.LG

TL;DR: 在没有预训练教师模型的情况下，提出一种基于学生GNN互相教学的协同学习框架，利用自适应对数输出权重和熵增强等技巧，使浅层GNN在节点和图分类任务中表现优于单独学习和传统KD。


<details>
  <summary>Details</summary>
Motivation: 解决对高性能教师模型的依赖，探索GNN之间的协同学习，以在资源受限场景中获得更高效且具备多任务能力的推理性能。

Method: 提出一个由多模型组成的协同学习框架，GNNs互相传授知识；引入自适应对数输出权重单元以高效知识交流；使用熵增强技术强化互相学习的动态学习策略，训练过程动态优化以提升下游任务性能。

Result: 在三个节点分类数据集和三个图分类数据集上进行广泛实验，结果显示相较于独立GNN和传统KD，协同学习框架能够使浅层GNN在推理阶段获得更好表现，且在多任务场景下具有显著且一致的提升。

Conclusion: 无需预训练教师模型也能通过协同互教实现高效学习；所提出的自适应权重与熵增强策略有助于提升泛化能力和多任务性能，未来研究可扩展至更大规模、提供理论分析并拓展应用场景。

Abstract: Knowledge distillation (KD) techniques have emerged as a powerful tool for
transferring expertise from complex teacher models to lightweight student
models, particularly beneficial for deploying high-performance models in
resource-constrained devices. This approach has been successfully applied to
graph neural networks (GNNs), harnessing their expressive capabilities to
generate node embeddings that capture structural and feature-related
information. In this study, we depart from the conventional KD approach by
exploring the potential of collaborative learning among GNNs. In the absence of
a pre-trained teacher model, we show that relatively simple and shallow GNN
architectures can synergetically learn efficient models capable of performing
better during inference, particularly in tackling multiple tasks. We propose a
collaborative learning framework where ensembles of student GNNs mutually teach
each other throughout the training process. We introduce an adaptive logit
weighting unit to facilitate efficient knowledge exchange among models and an
entropy enhancement technique to improve mutual learning. These components
dynamically empower the models to adapt their learning strategies during
training, optimizing their performance for downstream tasks. Extensive
experiments conducted on three datasets each for node and graph classification
demonstrate the effectiveness of our approach.

</details>


### [114] [Controllable Machine Unlearning via Gradient Pivoting](https://arxiv.org/abs/2510.19226)
*Youngsik Hwang,Dong-Young Lim*

Main category: cs.LG

TL;DR: 将机器学习移除（MU）问题从单目标优化改为多目标优化（MOO），提出CUP算法，通过“pivoting gradient”实现对Pareto前沿的可控探索，并用超体积（hypervolume）评估解集的质量与多样性，在视觉任务上持续优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的近似消除方法在消除效果与模型保真之间存在显著权衡，易出现过度遗忘、缺乏对消除过程的细粒度控制，并且缺少对该权衡的综合评价指标。

Method: 提出CUP，一种基于多目标优化的算法，采用独特的 pivoting gradient 机制在Pareto前沿上进行导航；通过单一直观的超参数“unlearning intensity”实现对权衡的精准控制；以超体积指标评估解集的质量与多样性。

Result: 实验显示CUP在多种视觉任务中能够产生更优的Pareto最优解集合，且在与现有方法的对比中具有持续优势。

Conclusion: 将MU重构为MOO并引入CUP，提供对未学习过程的可控探索与权衡选择；以超体积指标对解集进行全面评价，显示出更好的解集覆盖与质量。

Abstract: Machine unlearning (MU) aims to remove the influence of specific data from a
trained model. However, approximate unlearning methods, often formulated as a
single-objective optimization (SOO) problem, face a critical trade-off between
unlearning efficacy and model fidelity. This leads to three primary challenges:
the risk of over-forgetting, a lack of fine-grained control over the unlearning
process, and the absence of metrics to holistically evaluate the trade-off. To
address these issues, we reframe MU as a multi-objective optimization (MOO)
problem. We then introduce a novel algorithm, Controllable Unlearning by
Pivoting Gradient (CUP), which features a unique pivoting mechanism. Unlike
traditional MOO methods that converge to a single solution, CUP's mechanism is
designed to controllably navigate the entire Pareto frontier. This navigation
is governed by a single intuitive hyperparameter, the `unlearning intensity',
which allows for precise selection of a desired trade-off. To evaluate this
capability, we adopt the hypervolume indicator, a metric that captures both the
quality and diversity of the entire set of solutions an algorithm can generate.
Our experimental results demonstrate that CUP produces a superior set of
Pareto-optimal solutions, consistently outperforming existing methods across
various vision tasks.

</details>


### [115] [Brain-Inspired Perspective on Configurations: Unsupervised Similarity and Early Cognition](https://arxiv.org/abs/2510.19229)
*Juntang Wang,Yihan Wang,Hao Wu,Dongmian Zou,Shixin Xu*

Main category: cs.LG

TL;DR: 引入“配置/configurations”的有限分辨率聚类框架，通过单一分辨率参数和吸引-排斥动力实现层次结构、对新颖性的敏感性以及灵活适应，辅以mheatmap评估多分辨率与动态行为。


<details>
  <summary>Details</summary>
Motivation: 受婴儿在无监督情境中形成类别、检测新颖性和适应新环境的启发，提出一种可解释、受脑启发的聚类框架，以获得层次化组织结构、对新颖性敏感以及对动态情境的稳健适应能力。

Method: 使用单一分辨率参数的有限分辨率聚类框架，并通过 attraction-repulsion（吸引-排斥）机制实现层次化组织。引入mheatmap，提供等比例热力图和重新分配算法，以公平评估多分辨率和动态行为。

Result: 在多个数据集上，配置方法在标准聚类指标上具有竞争力；在新颖性检测方面达到87%的AUC；在动态类别演化中的稳定性提高约35%。

Conclusion: 将配置作为早期认知分类的原理性计算模型，并作为通往脑启发型AI的一个步骤，展示了对新颖性与动态适应的鲁棒性与潜力。

Abstract: Infants discover categories, detect novelty, and adapt to new contexts
without supervision -- a challenge for current machine learning. We present a
brain-inspired perspective on configurations, a finite-resolution clustering
framework that uses a single resolution parameter and attraction-repulsion
dynamics to yield hierarchical organization, novelty sensitivity, and flexible
adaptation. To evaluate these properties, we introduce mheatmap, which provides
proportional heatmaps and a reassignment algorithm to fairly assess
multi-resolution and dynamic behavior. Across datasets, configurations are
competitive on standard clustering metrics, achieve 87% AUC in novelty
detection, and show 35% better stability during dynamic category evolution.
These results position configurations as a principled computational model of
early cognitive categorization and a step toward brain-inspired AI.

</details>


### [116] [Understanding the Implicit Biases of Design Choices for Time Series Foundation Models](https://arxiv.org/abs/2510.19236)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 通过理论与受控实验研究，揭示TSFMs训练中的设计“旋钮”如何引入隐式偏差并影响时间序列建模的核心属性。


<details>
  <summary>Details</summary>
Motivation: 理解训练过程中的超参数和设计选择如何影响模型质量，而非单纯在基准上胜出；揭示隐式偏差的来源及其对模型行为的影响。

Method: 结合理论分析与受控经验评估，系统研究补丁大小、嵌入选型、训练目标等设计如何产生对时间行为、几何结构、均值回归等属性的偏差；通过对离群值处理的案例研究展示多偏差的交互；讨论与“苦难之教训”相关的含义。

Result: 证实不同设计会引入不同的隐式偏差，影响时间序列的动态特性、几何结构以及对均值的回归程度等；偏差的直观性强弱取决于模型与数据属性，某些偏差可能以复杂方式在异常值处理等场景中相互作用。

Conclusion: 设计选择决定TSFM的基本特征，需谨慎理解和管理多种偏差及其交互，以提升模型的可控性和鲁棒性，并对未来的TSFM开发提出对“苦难之课”的警醒与指引。

Abstract: Time series foundation models (TSFMs) are a class of potentially powerful,
general-purpose tools for time series forecasting and related temporal tasks,
but their behavior is strongly shaped by subtle inductive biases in their
design. Rather than developing a new model and claiming that it is better than
existing TSFMs, e.g., by winning on existing well-established benchmarks, our
objective is to understand how the various ``knobs'' of the training process
affect model quality. Using a mix of theory and controlled empirical
evaluation, we identify several design choices (patch size, embedding choice,
training objective, etc.) and show how they lead to implicit biases in
fundamental model properties (temporal behavior, geometric structure, how
aggressively or not the model regresses to the mean, etc.); and we show how
these biases can be intuitive or very counterintuitive, depending on properties
of the model and data. We also illustrate in a case study on outlier handling
how multiple biases can interact in complex ways; and we discuss implications
of our results for learning the bitter lesson and building TSFMs.

</details>


### [117] [SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes](https://arxiv.org/abs/2510.19241)
*Xuyuan Xiong,Pedro Chumpitaz-Flores,Kaixun Hua,Cheng Hua*

Main category: cs.LG

TL;DR: SPOT develops a MILP-based method to learn interpretable decision-tree policies for MDPs. It uses a reduced-space branch-and-bound that decouples MDP dynamics from tree constraints to enable parallel search, yielding faster optimal trees. It claims substantial speedups and scalability to larger MDPs with more states, producing compact, interpretable policies without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Interpretability of reinforcement learning policies is crucial in high-stakes domains, but optimizing decision-tree policies in MDPs is computationally hard. There is a need for scalable methods that produce interpretable policies with strong performance.

Method: Formulate the policy search as a mixed-integer linear program (MILP). Introduce a reduced-space branch-and-bound strategy that decouples MDP dynamics from the tree-structure constraints, enabling parallel search. At each iteration, the method guarantees an optimal decision tree.

Result: SPOT achieves substantial speedups and scales to larger MDPs with more states, outperforming previous approaches. The resulting decision-tree policies are compact and interpretable, maintaining transparency without sacrificing performance.

Conclusion: SPOT demonstrates that it's possible to obtain interpretable and scalable decision-tree policies for MDPs, delivering high-quality policies significantly faster than existing methods.

Abstract: Interpretable reinforcement learning policies are essential for high-stakes
decision-making, yet optimizing decision tree policies in Markov Decision
Processes (MDPs) remains challenging. We propose SPOT, a novel method for
computing decision tree policies, which formulates the optimization problem as
a mixed-integer linear program (MILP). To enhance efficiency, we employ a
reduced-space branch-and-bound approach that decouples the MDP dynamics from
tree-structure constraints, enabling efficient parallel search. This
significantly improves runtime and scalability compared to previous methods.
Our approach ensures that each iteration yields the optimal decision tree.
Experimental results on standard benchmarks demonstrate that SPOT achieves
substantial speedup and scales to larger MDPs with a significantly higher
number of states. The resulting decision tree policies are interpretable and
compact, maintaining transparency without compromising performance. These
results demonstrate that our approach simultaneously achieves interpretability
and scalability, delivering high-quality policies an order of magnitude faster
than existing approaches.

</details>


### [118] [Mixing Configurations for Downstream Prediction](https://arxiv.org/abs/2510.19248)
*Juntang Wang,Hao Wu,Runkun Guo,Yihan Wang,Dongmian Zou,Shixin Xu*

Main category: cs.LG

TL;DR: 提出 GraMixC，一种 plug-and-play 的无监督配置提取与对齐融合模块，能从多分辨率层次中发现有效的配置并提升下游预测表现。


<details>
  <summary>Details</summary>
Motivation: 人类具有按相似性分组的直觉，现有的社区检测在多分辨率层次下可发现有效的层次聚类；论文将这类配置formalize，并在 Vision Transformers 与其他任务中无监督发现和利用这些结构，减少冗余与手动选择。

Method: GraMixC 包括提取配置、使用 Reverse Merge/Split (RMS) 进行对齐、通过注意力头对配置进行融合，并将结果传给任意下游预测器。可在无监督或自监督学习场景中获得配置，且配置的选择/组成与下游任务和输入相关。

Result: 在 DSN1 16S rRNA 培养基预测任务中，R2 从 0.6 提升至 0.9，达到该任务的新状态-of-the-art；在标准表格基准上也优于单分辨率和静态特征基线。

Conclusion: 配置提供多分辨率的有效聚类结构，GraMixC 作为易插拔模块可无监督学习并通过 RMS 对齐和注意力融合提升跨任务的预测性能。

Abstract: Humans possess an innate ability to group objects by similarity, a cognitive
mechanism that clustering algorithms aim to emulate. Recent advances in
community detection have enabled the discovery of configurations -- valid
hierarchical clusterings across multiple resolution scales -- without requiring
labeled data. In this paper, we formally characterize these configurations and
identify similar emergent structures in register tokens within Vision
Transformers. Unlike register tokens, configurations exhibit lower redundancy
and eliminate the need for ad hoc selection. They can be learned through
unsupervised or self-supervised methods, yet their selection or composition
remains specific to the downstream task and input. Building on these insights,
we introduce GraMixC, a plug-and-play module that extracts configurations,
aligns them using our Reverse Merge/Split (RMS) technique, and fuses them via
attention heads before forwarding them to any downstream predictor. On the DSN1
16S rRNA cultivation-media prediction task, GraMixC improves the R2 score from
0.6 to 0.9 across multiple methods, setting a new state of the art. We further
validate GraMixC on standard tabular benchmarks, where it consistently
outperforms single-resolution and static-feature baselines.

</details>


### [119] [FnRGNN: Distribution-aware Fairness in Graph Neural Network](https://arxiv.org/abs/2510.19257)
*Soyoung Park,Sungsu Lim*

Main category: cs.LG

TL;DR: 提出 FnRGNN，一种在 GNN 节点回归中实现公平性的一体化 in-processing 框架，结合结构层边权重调整、表示层对齐（MMD）与预测层分布匹配（基于 Sinkhorn），在不显著损失性能的前提下降低群体差异。


<details>
  <summary>Details</summary>
Motivation: 节点级回归的公平性研究相对薄弱，现有方法多聚焦于分类任务或表征层去偏，难以充分处理连续输出和复杂图结构中的公平性问题。

Method: 在处理中提出三层干预：1) 结构层的边权重重新加权以调控信息流；2) 表征层通过最大均值差距（MMD）实现跨群体表示分布对齐；3) 预测层利用基于 Sinkhorn 的分布匹配进行归一化，优化输出分布的一致性，将三者结合以实现公平性与性能的平衡。

Result: 在四个真实数据集上实验表明，FnRGNN 能降低群体差异（提高公平性）且不显著损害预测性能。

Conclusion: 多层干预的综合框架在复杂图拓扑下对 GNN 节点回归任务的公平性具有鲁棒性，且代码公开，可供复现实验。

Abstract: Graph Neural Networks (GNNs) excel at learning from structured data, yet
fairness in regression tasks remains underexplored. Existing approaches mainly
target classification and representation-level debiasing, which cannot fully
address the continuous nature of node-level regression. We propose FnRGNN, a
fairness-aware in-processing framework for GNN-based node regression that
applies interventions at three levels: (i) structure-level edge reweighting,
(ii) representation-level alignment via MMD, and (iii) prediction-level
normalization through Sinkhorn-based distribution matching. This multi-level
strategy ensures robust fairness under complex graph topologies. Experiments on
four real-world datasets demonstrate that FnRGNN reduces group disparities
without sacrificing performance. Code is available at
https://github.com/sybeam27/FnRGNN.

</details>


### [120] [Knowledge Distillation of Uncertainty using Deep Latent Factor Model](https://arxiv.org/abs/2510.19290)
*Sehyun Park,Jongjin Lee,Yunseop Shin,Ilsang Ohn,Yongdai Kim*

Main category: cs.LG

TL;DR: 提出Gaussian distillation，一种基于高斯过程的分布蒸馏方法，将教师集成的分布蒸馏为一个学生分布，通过深潜在因子模型(DLF)建模，利用EM估计均值与协方差，实验表明在基准数据集和语言模型微调/分布偏移任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度集成在资源受限设备中的高计算与内存成本，以及现有蒸馏难以在压缩时保留不确定性的问题。

Method: 把教师集合视为随机过程的实现，使用深潜在因子模型(DLF)来估计教师集成的分布，并通过高斯分布进行分布蒸馏；用期望最大化(EM)算法稳健估计DLF的均值和协方差函数，得到单一学生分布而非学生集成。

Result: 在多个基准数据集上优于现有基线；对语言模型的微调和分布偏移场景也表现良好。

Conclusion: Gaussian distillation在保持不确定性的同时实现高效模型压缩，适合在边缘设备和对鲁棒性要求高的应用中部署。

Abstract: Deep ensembles deliver state-of-the-art, reliable uncertainty quantification,
but their heavy computational and memory requirements hinder their practical
deployments to real applications such as on-device AI. Knowledge distillation
compresses an ensemble into small student models, but existing techniques
struggle to preserve uncertainty partly because reducing the size of DNNs
typically results in variation reduction. To resolve this limitation, we
introduce a new method of distribution distillation (i.e. compressing a teacher
ensemble into a student distribution instead of a student ensemble) called
Gaussian distillation, which estimates the distribution of a teacher ensemble
through a special Gaussian process called the deep latent factor model (DLF) by
treating each member of the teacher ensemble as a realization of a certain
stochastic process. The mean and covariance functions in the DLF model are
estimated stably by using the expectation-maximization (EM) algorithm. By using
multiple benchmark datasets, we demonstrate that the proposed Gaussian
distillation outperforms existing baselines. In addition, we illustrate that
Gaussian distillation works well for fine-tuning of language models and
distribution shift problems.

</details>


### [121] [QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation](https://arxiv.org/abs/2510.19296)
*Yang Zhang,Rui Zhang,Jiaming Guo,Lei Huang,Di Huang,Yunpu Zhao,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 提出 QiMeng-SALV，通过信号级别的 RL 优化来生成 Verilog 代码，利用部分正确模块中的信号相关实现来提取有意义的功能奖励，提升 RL 的训练效果，在 VerilogEval/RTLLM 上达到 SOTA，7B 模型接近 671B DeepSeek 的表现并显著优于 CodeV。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在 Verilog 代码生成中的功能性奖励不足，难以优化生成结果的正确性；Verilog 的结构性特性使不同信号间相对独立，促使在更细粒度的信号级别进行优化。

Method: 先比较生成模块中信号的功能正确性与训练数据中的参考模块；利用抽象语法树(AST)识别“信号感知”的代码片段；在这些正确信号级代码片段上训练信号感知的 DPO，以获得有效的奖励，最终实现从模块级到信号级的细粒度优化。

Result: 在 VerilogEval 与 RTLLM 基准上达到领先性能，7B 参数模型的表现可与 DeepSeek v3 671B 相当，明显优于同数据集训练的 CodeV；并开源代码。

Conclusion: 将 Verilog 代码生成的优化从模块级提升到信号级，显著改善功能性奖励的获取和 RL 的训练效果，提出可推广的信号感知优化范式。

Abstract: The remarkable progress of Large Language Models (LLMs) presents promising
opportunities for Verilog code generation which is significantly important for
automated circuit design. The lacking of meaningful functional rewards hinders
the preference optimization based on Reinforcement Learning (RL) for producing
functionally correct Verilog code. In this paper, we propose Signal-Aware
Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments
of functionally correct output signal to optimize RL training. Considering
Verilog code specifies the structural interconnection of hardware gates and
wires so that different output signals are independent, the key insight of
QiMeng-SALV is to extract verified signal-aware implementations in partially
incorrect modules, so as to enhance the extraction of meaningful functional
rewards. Roughly, we verify the functional correctness of signals in generated
module by comparing with that of reference module in the training data. Then
abstract syntax tree (AST) is employed to identify signal-aware code segments
which can provide meaningful functional rewards from erroneous modules.
Finally, we introduce signal-aware DPO which is optimized on the correct
signal-level code segments, thereby preventing noise and interference from
incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from
conventional module-level to fine-grained signal-level optimization in Verilog
code generation, addressing the issue of insufficient functional rewards.
Experiments demonstrate that our method achieves state-of-the-art performance
on VerilogEval and RTLLM, with a 7B parameter model matching the performance of
the DeepSeek v3 671B model and significantly outperforming the leading
open-source model CodeV trained on the same dataset. Our code is available at
https://github.com/zy1xxx/SALV.

</details>


### [122] [Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall](https://arxiv.org/abs/2510.19304)
*Mingyu Jo,Jaesik Yoon,Justin Deschenaux,Caglar Gulcehre,Sungjin Ahn*

Main category: cs.LG

TL;DR: 提出Loopholing，通过一个确定性潜在通路在离散扩散模型中保持信息，缓解采样墙问题，显著降低 perplexity，接近甚至超越自回归模型，并在推理任务和算术基准上提升性能。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在采样阶段将分布信息压缩为one-hot，导致跨步信息无法传播，制约并行解码的潜在优势；需要保持跨步的信息传递以提升文本质量和推理能力。

Method: 引入Loopholing机制，建立一个确定性的潜在路径以在各步之间传递信息，形成Loopholing离散扩散模型（LDDMs）；采用自我条件训练策略以提高训练效率。

Result: 相较于先前基线，生成困惑度（perplexity）降低多达61%；缩小甚至超越自回归模型的差距，文本更连贯；在Countdown和Game of 24等算术推理任务上也有性能提升；Loopholing还降低了空步（idle steps）和振荡，展现可扩展的高质量非自回归文本生成路径。

Conclusion: Loopholing为高质量非自回归文本生成提供一个可扩展路径，通过保持跨步信息来缓解采样墙，且在推理任务中展现潜力，值得进一步研究与应用。

Abstract: Discrete diffusion models offer a promising alternative to autoregressive
generation through parallel decoding, but they suffer from a sampling wall:
once categorical sampling occurs, rich distributional information collapses
into one-hot vectors and cannot be propagated across steps, forcing subsequent
steps to operate with limited information. To mitigate this problem, we
introduce Loopholing, a novel and simple mechanism that preserves this
information via a deterministic latent pathway, leading to Loopholing Discrete
Diffusion Models (LDDMs). Trained efficiently with a self-conditioning
strategy, LDDMs achieve substantial gains-reducing generative perplexity by up
to 61% over prior baselines, closing (and in some cases surpassing) the gap
with autoregressive models, and producing more coherent text. Applied to
reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such
as Countdown and Game of 24. These results also indicate that loopholing
mitigates idle steps and oscillations, providing a scalable path toward
high-quality non-autoregressive text generation.

</details>


### [123] [FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation](https://arxiv.org/abs/2510.19305)
*Chirag Padubidri,Pranesh Velmurugan,Andreas Lanitis,Andreas Kamilaris*

Main category: cs.LG

TL;DR: 融合深度学习、数据平衡与缺失值填充的多模态SDM框架，用于两栖动物分布预测，显著提升预测精度：MAE从189降至29，蛙类计数性能提升，栅栅；多模态输入（土地覆盖、NDVI等）与图像/表格数据联合，84.9%准确率、AUC 0.90，在未见区域具鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 监测物种分布对保护工作至关重要，但传统数据收集如公民科学存在覆盖与完整性不足的问题。SDM通过环境变量与发生数据在大区域内预测分布，能够弥补数据缺口。本研究在蛙类分布建模中，结合深度学习与数据填充以应对稀疏/不完整数据。

Method: 使用数据平衡、数据插补与特征选择等数据预处理；构建多模态集成模型，将地表覆盖、NDVI等环境输入与图像/表格数据联合用于蛙分布与计数预测；在EY-2022生物多样性挑战数据集上开展实验，验证鲁棒泛化能力。

Result: 数据平衡将平均绝对误差(MAE)由189降至29，特征选择确定关键环境因素，提升输入效率；多模态集成模型优于单一模型，在未见区域实现良好泛化；图像与表格数据融合提升蛙计数和栖息地分类的性能，准确率84.9%，AUC 0.90。

Conclusion: 本研究表明，在数据稀缺或不完整的生态监测场景中，多模态学习结合数据预处理（平衡、填补）能显著提升SDM的精度和扩展性，为更精确、可扩展的生物多样性监测提供有效路径。

Abstract: Monitoring species distribution is vital for conservation efforts, enabling
the assessment of environmental impacts and the development of effective
preservation strategies. Traditional data collection methods, including citizen
science, offer valuable insights but remain limited in coverage and
completeness. Species Distribution Modelling (SDM) helps address these gaps by
using occurrence data and environmental variables to predict species presence
across large regions. In this study, we enhance SDM accuracy for frogs (Anura)
by applying deep learning and data imputation techniques using data from the
"EY - 2022 Biodiversity Challenge." Our experiments show that data balancing
significantly improved model performance, reducing the Mean Absolute Error
(MAE) from 189 to 29 in frog counting tasks. Feature selection identified key
environmental factors influencing occurrence, optimizing inputs while
maintaining predictive accuracy. The multimodal ensemble model, integrating
land cover, NDVI, and other environmental inputs, outperformed individual
models and showed robust generalization across unseen regions. The fusion of
image and tabular data improved both frog counting and habitat classification,
achieving 84.9% accuracy with an AUC of 0.90. This study highlights the
potential of multimodal learning and data preprocessing techniques such as
balancing and imputation to improve predictive ecological modeling when data
are sparse or incomplete, contributing to more precise and scalable
biodiversity monitoring.

</details>


### [124] [Calibration and Discrimination Optimization Using Clusters of Learned Representation](https://arxiv.org/abs/2510.19328)
*Tomer Lavi,Bracha Shapira,Nadav Rappoport*

Main category: cs.LG

TL;DR: 提出一个基于样本表示聚类的校准函数集成管线，显著提升标定性能并引入用于同时优化判别与校准的匹配度量。


<details>
  <summary>Details</summary>
Motivation: 在决策与风险评估中，预测的辨别能力和校准性同等重要；然而校准往往被忽视，尤其在临床预测等关键场景。现有方法对校准的关注不足，需一个通用、可扩展的校准框架。

Method: 构建一个由多个校准函数组成的集合，对输入样本的学习表示进行聚类后，在每个簇上独立训练校准函数，并引入一个匹配度量来同时优化辨别和校准。该框架对底层表示、聚类方法、校准方法和评价指标具有通用性。

Result: 在多种方法上，标定分数从约82.28%提升至100%，并提供了一种新的匹配度量以实现更优的模型选择。

Conclusion: 所提出的通用校准框架适配性强、可与常用校准方法协同工作，并在辨别与校准两方面实现更优的综合性能。

Abstract: Machine learning models are essential for decision-making and risk
assessment, requiring highly reliable predictions in terms of both
discrimination and calibration. While calibration often receives less
attention, it is crucial for critical decisions, such as those in clinical
predictions. We introduce a novel calibration pipeline that leverages an
ensemble of calibration functions trained on clusters of learned
representations of the input samples to enhance overall calibration. This
approach not only improves the calibration score of various methods from 82.28%
up to 100% but also introduces a unique matching metric that ensures model
selection optimizes both discrimination and calibration. Our generic scheme
adapts to any underlying representation, clustering, calibration methods and
metric, offering flexibility and superior performance across commonly used
calibration methods.

</details>


### [125] [Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning](https://arxiv.org/abs/2510.19338)
*Ling Team,Bin Han,Caizhi Tang,Chen Liang,Donghao Zhang,Fan Yuan,Feng Zhu,Jie Gao,Jingyu Hu,Longfei Li,Meng Li,Mingyang Zhang,Peijie Jiang,Peng Jiao,Qian Zhao,Qingyuan Yang,Wenbo Shen,Xinxing Yang,Yalin Zhang,Yankun Ren,Yao Zhao,Yibo Cao,Yixuan Sun,Yue Zhang,Yuchen Fang,Zibin Lin,Zixuan Cheng,Jun Zhou*

Main category: cs.LG

TL;DR: Ring-mini-linear-2.0 与 Ring-flash-linear-2.0 通过线性+softmax混合注意力降低长上下文推理成本，并提升训练效率，达到或接近SOTA。


<details>
  <summary>Details</summary>
Motivation: 降低长上下文推理中的I/O与计算开销，提升训练-推理对齐，提升RL阶段的稳定性和性能。

Method: 混合注意力架构，探索注意力比例，规模化到不同参数规模；自研 FP8 运算库 linghe 提升训练效率；训练与推理引擎对齐以实现稳定、长周期优化；在 RL 训练阶段持续保持高性能。

Result: 对比32B密集模型，推理成本降至1/10；对比原Ring系列，成本下降>50%；在复杂推理基准保持SOTA；训练效率提升约50%（FP8 库），长期优化稳定性提升。

Conclusion: 该系列通过注意力混合和对齐优化，实现高效长上下文推理并维持SOTA，适合大规模强化学习继续训练。

Abstract: In this technical report, we present the Ring-linear model series,
specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.
Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while
Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both
models adopt a hybrid architecture that effectively integrates linear attention
and softmax attention, significantly reducing I/O and computational overhead in
long-context inference scenarios. Compared to a 32 billion parameter dense
model, this series reduces inference cost to 1/10, and compared to the original
Ring series, the cost is also reduced by over 50%. Furthermore, through
systematic exploration of the ratio between different attention mechanisms in
the hybrid architecture, we have identified the currently optimal model
structure. Additionally, by leveraging our self-developed high-performance FP8
operator library-linghe, overall training efficiency has been improved by 50%.
Benefiting from the high alignment between the training and inference engine
operators, the models can undergo long-term, stable, and highly efficient
optimization during the reinforcement learning phase, consistently maintaining
SOTA performance across multiple challenging complex reasoning benchmarks.

</details>


### [126] [Foundation Model Forecasts: Form and Function](https://arxiv.org/abs/2510.19345)
*Alvaro Perez-Diaz,James C. Loach,Danielle E. Toutoungi,Lee Middleton*

Main category: cs.LG

TL;DR: 本研究强调预测形式对实际任务的影响：多数时间序列 foundation 模型仅给出点或参数化预测，而许多操作性任务需要保留时间依赖性的轨迹集合。轨迹集合可以通过边缘化无额外假设地简化为简单形式；但要从简化形式回到轨迹集合，则需要引入时序依赖（如 Copulas 或 conformal 方法）。边际分布无法唯一确定路径相关事件概率，因为不同的联合分布可共享相同的边际。将六类 forecasting 任务映射到最小充分预测类型，并提出与任务对齐的评估框架。


<details>
  <summary>Details</summary>
Motivation: 为了提升预测在实际应用中的价值，需超越单纯的准确性评估，理解不同预测形式对决策和操作任务的适用性；明确何时需要保留轨迹信息、何时可通过边际化简化，以及如何评估预测在具体任务上的效用。

Method: 对近来时间序列基础模型进行系统综述；建立理论结果：轨迹集合可通过边缘化转为简单形式且无需额外假设；反向转化需引入时序依赖（Copulas/ conformal 方法）。证明边际分布不能唯一确定路径相关事件概率。将六大预测任务映射到最小充分预测类型，提出任务对齐的评估框架。

Result: 给出可转换性边界、边际不可唯一确定路径事件概率的证明，以及任务-预测类型映射和评估框架，指引在实践中优先考虑预测形式以满足具体操作需求。

Conclusion: 在实践中，预测类型的选择往往决定能否支持特定操作任务；通过六类任务的最小充分预测类型和评估框架，研究者与工程师可更有针对性地选择预测形式，而不仅仅追求更高的点估计或参数化准确性。

Abstract: Time-series foundation models (TSFMs) achieve strong forecast accuracy, yet
accuracy alone does not determine practical value. The form of a forecast --
point, quantile, parametric, or trajectory ensemble -- fundamentally constrains
which operational tasks it can support. We survey recent TSFMs and find that
two-thirds produce only point or parametric forecasts, while many operational
tasks require trajectory ensembles that preserve temporal dependence. We
establish when forecast types can be converted and when they cannot: trajectory
ensembles convert to simpler forms via marginalization without additional
assumptions, but the reverse requires imposing temporal dependence through
copulas or conformal methods. We prove that marginals cannot determine
path-dependent event probabilities -- infinitely many joint distributions share
identical marginals but yield different answers to operational questions. We
map six fundamental forecasting tasks to minimal sufficient forecast types and
provide a task-aligned evaluation framework. Our analysis clarifies when
forecast type, not accuracy, differentiates practical utility.

</details>


### [127] [Scalable LinUCB: Low-Rank Design Matrix Updates for Recommenders with Large Action Spaces](https://arxiv.org/abs/2510.19349)
*Evgenia Shustova,Marina Sheshukova,Sergey Samsonov,Evgeny Frolov*

Main category: cs.LG

TL;DR: Scalable LinUCB 通过对逆的动态低秩参数化和投影分裂积分器实现 O(dr) 的更新与内存占用，避免直接形成完整设计矩阵，实验表明在推荐系统数据集上具有良好性能。


<details>
  <summary>Details</summary>
Motivation: 线性上下文带宽（如 LinUCB）的训练、推断和内存成本随特征维度和动作空间增大而显著上升，瓶颈在于更新、求逆与存储用于历史信息的设计矩阵。

Method: 提出 Scalable LinUCB，通过对逆的 Cholesky 风格因子进行动态低秩参数化；给出数值稳定的 rank-1 与批量更新，在不直接形成整个矩阵的情况下维持逆；采用投影分裂积分器进行动态低秩近似以控制内存增长；推导平均每步更新成本为 O(dr)，近似秩为 r 时内存为 O(dr)，每个动作评估推断复杂度为 O(dr)。

Result: 在推荐系统数据集上的实验显示算法具有更低的计算和内存需求，同时保持良好性能。

Conclusion: Scalable LinUCB 提供了一种高效、内存友好的变体，适用于大规模线性上下文带，在更新速度和内存占用方面具有显著改进，并在实验中得到验证。

Abstract: Linear contextual bandits, especially LinUCB, are widely used in recommender
systems. However, its training, inference, and memory costs grow with feature
dimensionality and the size of the action space. The key bottleneck becomes the
need to update, invert and store a design matrix that absorbs contextual
information from interaction history. In this paper, we introduce Scalable
LinUCB, the algorithm that enables fast and memory efficient operations with
the inverse regularized design matrix. We achieve this through a dynamical
low-rank parametrization of its inverse Cholesky-style factors. We derive
numerically stable rank-1 and batched updates that maintain the inverse without
directly forming the entire matrix. To control memory growth, we employ a
projector-splitting integrator for dynamical low-rank approximation, yielding
average per-step update cost $O(dr)$ and memory $O(dr)$ for approximation rank
$r$. Inference complexity of the suggested algorithm is $O(dr)$ per action
evaluation. Experiments on recommender system datasets demonstrate the
effectiveness of our algorithm.

</details>


### [128] [ConvXformer: Differentially Private Hybrid ConvNeXt-Transformer for Inertial Navigation](https://arxiv.org/abs/2510.19352)
*Omer Tariq,Muhammad Bilal,Muneeb Ul Hassan,Dongsoo Han,Jon Crowcroft*

Main category: cs.LG

TL;DR: 提出ConvXformer架构，融合ConvNeXt与Transformer，结合自适应梯度裁剪和梯度对齐噪声注入的差分隐私机制，在高频惯性数据上实现鲁棒导航，且定位精度在多数据集上提升显著，同时提供强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 深度学习惯性导航在GPS-denied场景具有高分辨率定位优势，但存在隐私信息泄露风险；现有差分隐私方法常严重损害模型性能，尤其对高频惯性测量影响大，因此需要提高隐私保护与定位性能之间的兼容性。

Method: 提出混合架构ConvXformer，在层次结构中融合ConvNeXt块与Transformer编码器；设计高效差分隐私机制，包含自适应梯度裁剪和梯度对齐的噪声注入，结合截断奇异值分解对梯度进行处理，以实现更精确的隐私-效用权衡。

Result: 在 OxIOD、RIDI、RoNIN 数据集上实现了超40%的定位精度提升，并获得$(,b)$-差分隐私保障；在Real-world Mech-IO数据集上展示对强环境干扰的鲁棒性。

Conclusion: 该框架在兼顾隐私保护与导航性能方面取得显著进展，适用于对安全性有高要求的自主导航与工业物联系统。

Abstract: Data-driven inertial sequence learning has revolutionized navigation in
GPS-denied environments, offering superior odometric resolution compared to
traditional Bayesian methods. However, deep learning-based inertial tracking
systems remain vulnerable to privacy breaches that can expose sensitive
training data. \hl{Existing differential privacy solutions often compromise
model performance by introducing excessive noise, particularly in
high-frequency inertial measurements.} In this article, we propose ConvXformer,
a hybrid architecture that fuses ConvNeXt blocks with Transformer encoders in a
hierarchical structure for robust inertial navigation. We propose an efficient
differential privacy mechanism incorporating adaptive gradient clipping and
gradient-aligned noise injection (GANI) to protect sensitive information while
ensuring model performance. Our framework leverages truncated singular value
decomposition for gradient processing, enabling precise control over the
privacy-utility trade-off. Comprehensive performance evaluations on benchmark
datasets (OxIOD, RIDI, RoNIN) demonstrate that ConvXformer surpasses
state-of-the-art methods, achieving more than 40% improvement in positioning
accuracy while ensuring $(\epsilon,\delta)$-differential privacy guarantees. To
validate real-world performance, we introduce the Mech-IO dataset, collected
from the mechanical engineering building at KAIST, where intense magnetic
fields from industrial equipment induce significant sensor perturbations. This
demonstrated robustness under severe environmental distortions makes our
framework well-suited for secure and intelligent navigation in cyber-physical
systems.

</details>


### [129] [Optimization Benchmark for Diffusion Models on Dynamical Systems](https://arxiv.org/abs/2510.19376)
*Fabian Schaipp*

Main category: cs.LG

TL;DR: 在扩散模型训练中，Muon和SOAP相较AdamW更高效，最终损失降低约18%；并探讨学习率调度对训练动力学的影响，以及Adam与SGD在扩散模型训练中的性能差异。


<details>
  <summary>Details</summary>
Motivation: 评估不同优化算法对扩散模型训练效率和效果的影响，并将文本/图像领域的训练现象（如学习率调度、Adam vs SGD）引入扩散模型训练以检验其可迁移性。

Method: 对比基准：Muon、SOAP、AdamW等优化器在训练扩散模型以去噪流轨迹的任务中的表现，比较最终训练损失和训练动态；分析学习率调度对收敛和稳定性的影响，以及Adam与SGD在该任务中的性能差异。

Result: Muon和SOAP相比AdamW在最终损失上约降低18%；学习率调度对训练过程的动力学有显著影响；Adam与SGD之间在扩散模型训练中的性能差异存在依赖性，需要具体设置来决定更优选项。

Conclusion: Muon和SOAP等新优化器在扩散模型训练中具有竞争力，且学习率调度对训练动态影响显著；需结合文本/图像领域的现有现象来优化扩散模型训练的优化策略。

Abstract: The training of diffusion models is often absent in the evaluation of new
optimization techniques. In this work, we benchmark recent optimization
algorithms for training a diffusion model for denoising flow trajectories. We
observe that Muon and SOAP are highly efficient alternatives to AdamW (18%
lower final loss). We also revisit several recent phenomena related to the
training of models for text or image applications in the context of diffusion
model training. This includes the impact of the learning-rate schedule on the
training dynamics, and the performance gap between Adam and SGD.

</details>


### [130] [CPSVD: Enhancing Large Language Model Compression via Column-Preserving Singular Value Decomposition](https://arxiv.org/abs/2510.19385)
*Lin Xv,Jingsheng Gao,Xian Gao,Ting Li,Yuzhuo Fu*

Main category: cs.LG

TL;DR: 提出列保留的SVD(CPSVD)以提升LLM压缩效率：对参数矩阵进行错误分段，将高误差列保持不变、低误差列使用SVD压缩，并在层内模块间自适应分配非均匀压缩率，目标层级压缩比下实现更低困惑度与更高零-shot准确率。


<details>
  <summary>Details</summary>
Motivation: LLM过大亟需高效压缩；传统SVD对整矩阵统一处理，忽视不同区域的近似误差差异，导致压缩效果受限。需利用矩阵局部误差异质性来提升压缩性能。

Method: CPSVD通过智能分段参数矩阵，直接保留具有高分解误差的列，对误差低的列仅用SVD进行压缩；同时精确确定两种策略的最优平衡点以最小化误差。并结合该层内不同矩阵的误差异质性，非均匀地分配压缩率以满足目标层压缩比。

Result: 大量实验表明CPSVD在SVD为基础的LLM压缩方法中表现优越，获得更低 perplexity 和在零-shot任务上的更高准确率。

Conclusion: 通过对列级别的误差敏感处理与层内自适应分配，CPSVD提升了SVD压缩在LLM中的效果，达到更优的压缩-性能权衡。

Abstract: The rapid advancement of Large Language Models (LLMs) faces a critical
bottleneck in their immense size, necessitating efficient compression
techniques. While Singular Value Decomposition (SVD) is a promising approach,
existing SVD-based methods treat the entire parameter matrix uniformly,
overlooking that SVD approximation errors vary significantly across different
matrix parts, which often leads to suboptimal compression. To address this, we
propose \textbf{C}olumn-\textbf{P}reserving \textbf{S}ingular \textbf{V}alue
\textbf{D}ecomposition (CPSVD), a novel method that refines SVD-based LLM
compression by intelligently segmenting the parameter matrix. Unlike
traditional SVD, CPSVD identifies and directly preserves matrix columns with
high decomposition errors, applying SVD only to columns with low decomposition
errors, while precisely determining the optimal balance point between these two
strategies to minimize error. Furthermore, leveraging the inherent
heterogeneity in decomposition errors across different matrices within an LLM,
CPSVD adaptively allocates non-uniform compression rates to modules within that
layer, while adhering to a target layer-wise compression ratio, thereby further
enhancing compression performance. Extensive experiments demonstrate that CPSVD
consistently outperforms state-of-the-art SVD-based LLM compression methods,
achieving lower perplexity and higher accuracy on zero-shot tasks.

</details>


### [131] [ARA: Adaptive Rank Allocation for Efficient Large Language Model SVD Compression](https://arxiv.org/abs/2510.19389)
*Lin Xv,Jingsheng Gao,Xian Gao,Ting Liu,Yuzhuo Fu*

Main category: cs.LG

TL;DR: 提出 Adaptive Rank Allocation (ARA) 来在 SVD 基于 LLM 的压缩中自适应分配线性模块秩，结合专用掩码与额外损失实现全局最优秩配置，实验在 LLaMA2-7B 80% 压缩下达到最优结果。


<details>
  <summary>Details</summary>
Motivation: 动机：SVD 只能作用于线性子模块，且模块间通过非线性跳跃分割；在全局压缩比约束下，如何为不同线性模块确定合适的秩成为关键。现有启发式和掩码训练在搜索区域、参数-谱关系建模、以及在压缩比为 1 时非平滑增益导致的局部最优方面存在不足。

Method: 方法：1) 设计专用掩码实现保留秩与可训练参数之间的高效映射与更新；2) 引入额外损失函数，引导参数选择达到全局最优解。

Result: 结果：在 LLaMA2-7B 模型 80% 压缩下，ARA 将 WikiText2 的困惑度从 8.38 降至 6.42，且平均零样本文任务准确率相比均匀压缩提高 9.72 个百分点。

Conclusion: 结论：ARA 在 SVD 基于 LLM 的压缩中有效实现自适应秩分配，考虑到非平滑增益等挑战，提供了有效的掩码设计与额外损失，达到 state-of-the-art。

Abstract: In the field of large language model (LLM) compression, singular value
decomposition (SVD) is a widely studied and adopted low-rank decomposition
technique. Since SVD operates exclusively on linear modules, and these modules
in LLMs are separated by nonlinear components, SVD can only be applied
independently to each linear module. Under a global compression ratio
constraint, determining the appropriate rank for different linear modules
becomes a critical problem. Existing approaches, such as heuristic algorithms
and mask-based training, have made progress in addressing this challenge.
However, these methods still suffer from several limitations: heuristic
algorithms explore the solution space within restricted regions, while
mask-based training struggles to efficiently capture the relationship between
singular value spectra and trainable parameters. More importantly, current
methods overlook the key property that the gain function is non-smooth at a
compression ratio of 1, which often leads the training process to suboptimal
local minima. To address these issues, we propose an Adaptive Rank Allocation
(ARA) method. Specifically, (1) ARA introduces a dedicated mask design that
enables efficient mapping and updating between retained ranks and trainable
parameters; and (2) it employs an additional loss function to guide parameter
selection toward globally optimal solutions. Experimental results demonstrate
that ARA achieves state-of-the-art performance. On the LLaMA2-7B model with a
80\% compression ratio, ARA reduces perplexity on WikiText2 from 8.38 to 6.42
and improves average zero-shot task accuracy by 9.72 percentage points compared
with uniform compression. These results highlight the effectiveness of our
method for rank allocation in SVD-based LLM compression.

</details>


### [132] [FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA](https://arxiv.org/abs/2510.19421)
*Songqi Zhou,Zeyuan Liu,Benben Jiang*

Main category: cs.LG

TL;DR: 提出FairNet：一个动态、实例级公平纠正框架，通过偏置探测器与条件化LoRA实现仅在偏置实例上启用纠正，结合对比损失促进少数群体的表示学习，能在有/无标签情形下工作，并在不牺牲整体性能的前提下提升最差群体表现。


<details>
  <summary>Details</summary>
Motivation: 当前公平性研究的目标是兼顾模型性能与偏差纠正，但现有去偏方法常常以牺牲性能换取公平性；它们依赖静态修正策略、在数据稀缺的少数群体上效果有限，且对敏感属性的利用要么过于依赖完全标签、要么完全忽略。需要一种动态、实例级别的偏差纠正框架，能够在存在不完整/缺失敏感属性标签的情形下工作，并针对最受影响的群体进行有效纠正。

Method: 提出FairNet，将偏置探测器与条件低秩适应（LoRA）相结合，使得仅在被检测为 biased 的实例上激活公平性纠正模块，从而在不影响无偏实例的情况下提升偏置样本的表现。引入新颖的对比损失，用于训练LoRA模块，目标是最小化同类内部跨不同敏感群体的表征差异，解决少数群体的欠拟合问题。框架支持对完整、部分或完全缺失的敏感属性标签的鲁棒处理。理论分析表明，在中等水平的TPR/FPR条件下，FairNet能提升最差群体的表现而不降低整体性能，甚至可能带来轻微提升。通过多种视觉与语言基准的实验验证其有效性。

Result: 实验结果显示，FairNet在多项视觉和语言任务中显著提升最差群体的性能，同时保持或略有提升整体准确性；对敏感属性缺失或不完备的情形具有鲁棒性，偏置探测器的中等TPR/FPR水平即可实现理论与实践的一致性与改进。

Conclusion: FairNet提供了一个灵活且可扩展的偏差纠正框架，证明了实例级、选择性纠正在实际应用场景中的潜力，有效地兼顾公平性与性能。未来工作可聚焦于进一步优化偏置探测与对比损失、以及扩展到更多模态和复杂场景的验证。

Abstract: Ensuring fairness in machine learning models is a critical challenge.
Existing debiasing methods often compromise performance, rely on static
correction strategies, and struggle with data sparsity, particularly within
minority groups. Furthermore, their utilization of sensitive attributes is
often suboptimal, either depending excessively on complete attribute labeling
or disregarding these attributes entirely. To overcome these limitations, we
propose FairNet, a novel framework for dynamic, instance-level fairness
correction. FairNet integrates a bias detector with conditional low-rank
adaptation (LoRA), which enables selective activation of the fairness
correction mechanism exclusively for instances identified as biased, and
thereby preserve performance on unbiased instances. A key contribution is a new
contrastive loss function for training the LoRA module, specifically designed
to minimize intra-class representation disparities across different sensitive
groups and effectively address underfitting in minority groups. The FairNet
framework can flexibly handle scenarios with complete, partial, or entirely
absent sensitive attribute labels. Theoretical analysis confirms that, under
moderate TPR/FPR for the bias detector, FairNet can enhance the performance of
the worst group without diminishing overall model performance, and potentially
yield slight performance improvements. Comprehensive empirical evaluations
across diverse vision and language benchmarks validate the effectiveness of
FairNet.

</details>


### [133] [LLM Unlearning with LLM Beliefs](https://arxiv.org/abs/2510.19422)
*Kemou Li,Qizhou Wang,Yue Wang,Fengpeng Li,Jun Liu,Bo Han,Jiantao Zhou*

Main category: cs.LG

TL;DR: 提出一种称为BS的自举式框架，通过同时抑制目标输出和模型信念来对抗“挤压效应”，实现更彻底的遗忘，同时尽量保留模型的实用性；并提出BS-T（token级）和BS-S（sequence级）两种变体，在多种模型家族和基准上取得更优的遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能会记住敏感或有害的内容，传统的遗忘方法多依赖梯度上升来降低目标响应的概率，但会将概率质量重新分布到高概率区域，常出现语义相关的改写形式，导致“挤压效应”和表征不真实的遗忘度。现有评估指标也易误导。

Method: 提出自举(bootstrapping, BS)框架，将挤压效应与模型自身的高置信生成（模型信念）联系起来。通过在遗忘目标的同时抑制模型信念中的高概率标记（BS-T）或高置信生成（BS-S），实现对高概率区域的直接干预；在不同模型族和基准上进行广泛实验以验证效果。

Result: 实验结果表明，与传统方法相比，BS-T与BS-S能更有效地减弱目标输出的记忆并抑制模型信念中的高概率区域，获得更彻底的遗忘，同时保持模型的实用性。

Conclusion: 将模型信念纳入遗忘目标是缓解挤压效应的有效途径，BS框架（尤其是BS-T/BS-S）能在多种模型和基准上实现更可靠的遗忘效果，提出了一个在评估机制也更真实的遗忘方向。

Abstract: Large language models trained on vast corpora inherently risk memorizing
sensitive or harmful content, which may later resurface in their outputs.
Prevailing unlearning methods generally rely on gradient ascent and its
variants to lower the probability of specific target responses. However, we
find that this strategy induces a critical side effect: probability mass is
redistributed into high-likelihood regions, often corresponding to semantically
related rephrasings of the targets. We refer to this as the squeezing effect,
which explains why many methods yield merely spurious unlearning, a problem
further obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport
actual success. To address this, we propose a bootstrapping (BS) framework that
explicitly links the squeezing effect with the model's own high-confidence
generations, namely its model beliefs. Since model beliefs inherently capture
the very high-likelihood regions where probability mass is squeezed,
incorporating them into the unlearning objective directly counters the
squeezing effect. By jointly suppressing both target responses and model
beliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S
(sequence) removes entire high-confidence generations, together achieving more
thorough forgetting while preserving utility. Extensive experiments across
diverse benchmarks with various model families confirm the effectiveness of our
approach.

</details>


### [134] [Revisiting the Relation Between Robustness and Universality](https://arxiv.org/abs/2510.19427)
*M. Klabunde,L. Caspari,F. Lemmerich*

Main category: cs.LG

TL;DR: modified universality hypothesis (Jones et al., 2022) posits adversarially robust models on a task are highly similar; this work finds partial support for representational similarity in some settings but not consistently across datasets, and that predictive behavior does not converge with robustness; universality is not strict—classifier retraining can yield more universal predictions.


<details>
  <summary>Details</summary>
Motivation: To test the generality of the modified universality hypothesis across datasets and analyze whether both representations and predictions become universal as models become more robust.

Method: Empirically assess Jones's claim in various datasets, evaluate representational similarity and predictive behavior under increasing robustness, identify the source of prediction differences (classification layer), and test whether retraining classifiers improves universality.

Result: Representational similarity holds in some datasets but not consistently across all datasets. Predictive behavior does not converge with increasing robustness. Differences in predictions originate in the classification layer. Simple retraining of the classifiers can achieve more universal predictive behavior.

Conclusion: Partial universality of neural networks in specific settings; strict universality is unlikely. Robustness alone does not guarantee universal predictions; targeted classifier retraining can enhance universality.

Abstract: The modified universality hypothesis proposed by Jones et al. (2022) suggests
that adversarially robust models trained for a given task are highly similar.
We revisit the hypothesis and test its generality. While we verify Jones' main
claim of high representational similarity in specific settings, results are not
consistent across different datasets. We also discover that predictive behavior
does not converge with increasing robustness and thus is not universal. We find
that differing predictions originate in the classification layer, but show that
more universal predictive behavior can be achieved with simple retraining of
the classifiers. Overall, our work points towards partial universality of
neural networks in specific settings and away from notions of strict
universality.

</details>


### [135] [g-DPO: Scalable Preference Optimization for Protein Language Models](https://arxiv.org/abs/2510.19474)
*Constance Ferragu,Jonathan D. Ziegler,Nicolas Deutschmann,Arthur Lindoulsi,Eli Bixby,Cradle ML Team*

Main category: cs.LG

TL;DR: g-DPO speeds up Direct Preference Optimization for protein language models by pruning redundant training pairs via sequence-space clustering and by using group-based likelihood approximations. It achieves 1.8–3.7× faster convergence across three protein-engineering tasks while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Direct Preference Optimization (DPO) scales poorly because the number of training pairs grows quadratically with the number of labeled sequences, making training times prohibitive for even modest datasets.

Method: g-DPO introduces (i) sequence-space clustering to prune redundant training pairs without sacrificing signal, and (ii) group-based approximations to amortize likelihood computations during training.

Result: Across three protein engineering tasks, g-DPO achieves performance (in-silico and in-vitro) statistically indistinguishable from standard DPO, while converging 1.8–3.7× faster; the speedup is expected to increase as dataset size grows.

Conclusion: g-DPO provides a scalable, efficient alternative to DPO for aligning protein language models, enabling faster training without sacrificing performance, with larger gains anticipated for larger datasets.

Abstract: Direct Preference Optimization (DPO) is an effective approach for aligning
protein language models with experimental design goals. However, DPO faces a
scalability bottleneck: the number of possible training pairs grows
quadratically with the number of labeled sequences, leading to prohibitive
training times even for modestly sized datasets. We introduce g-DPO, a
framework that (i) uses sequence space clustering to prune redundant pairs
while preserving training signal, and (ii) amortizes likelihood computations
with group-based approximations. Across three protein engineering tasks, g-DPO
maintains in-silico and in-vitro performance that is statistically
indistinguishable from standard DPO, while converging 1.8 to 3.7 times faster,
with greater gains expected as the size of the dataset increases.

</details>


### [136] [A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring](https://arxiv.org/abs/2510.19476)
*Julian Schulz*

Main category: cs.LG

TL;DR: 提出基于链式推理（CoT）监控的安全性论证框架，构建两段式安全案例以提升控制性与可信性，系统化的威胁分类（神经语言与编码推理，三形态：语言漂移、隐写、外星推理），评估保持CoT可信的技术，并探讨不可监控情形的替代策略与通过预测市场评估里程碑可行性，形成研究议程。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统接近具备危险能力的水平，传统安全案例不足以覆盖新型风险，需要通过CoT监控等新方法来提升安全性与可信性。

Method: 提出两部分的安全性案例：1) 证明模型在无CoT时不具备危险能力；2) 确保任何由CoT引发的危险能力都能被CoT监控检测。系统检视两类监控性威胁（神经语言与编码推理），并将其分为语言漂移、隐写、外星推理三种形式及其驱动因素；评估现有及新颖技术以维持CoT的可信性；在不可监控的CoT情况下，探讨提取可监控CoT的可能性；通过预测市场评估关键里程碑的可行性。

Result: 提供一个概念性框架、威胁分类与研究路径，明确如何将CoT监控纳入安全性论证，以及通过研究议程推动该方向的可行性评估。

Conclusion:  CoT监控有望同时支撑控制性与可信性安全论证，但其可行性取决于缓解监控性威胁与从不可监控状态中获取可监控CoT的能力，需要进一步的研究与里程碑导向的预测评估。

Abstract: As AI systems approach dangerous capability levels where inability safety
cases become insufficient, we need alternative approaches to ensure safety.
This paper presents a roadmap for constructing safety cases based on
chain-of-thought (CoT) monitoring in reasoning models and outlines our research
agenda. We argue that CoT monitoring might support both control and
trustworthiness safety cases. We propose a two-part safety case: (1)
establishing that models lack dangerous capabilities when operating without
their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT
are detectable by CoT monitoring. We systematically examine two threats to
monitorability: neuralese and encoded reasoning, which we categorize into three
forms (linguistic drift, steganography, and alien reasoning) and analyze their
potential drivers. We evaluate existing and novel techniques for maintaining
CoT faithfulness. For cases where models produce non-monitorable reasoning, we
explore the possibility of extracting a monitorable CoT from a non-monitorable
CoT. To assess the viability of CoT monitoring safety cases, we establish
prediction markets to aggregate forecasts on key technical milestones
influencing their feasibility.

</details>


### [137] [ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language Models on Edge Devices](https://arxiv.org/abs/2510.19482)
*Xin Nie,Liang Dong,HaiCheng Zhang,JiaWang Xiao,G. Sun*

Main category: cs.LG

TL;DR: HLQ 框架通过分层线性量化捕捉权重分布，在不增加 GEMM 计算成本的情况下降低去量化开销，实现在 CPU 边缘设备上的高效端到端推理。


<details>
  <summary>Details</summary>
Motivation: 在内存和计算资源受限的 CPU 边缘设备上部署大语言模型，需要降低内存占用和延迟；现有均匀量化在低比特宽度下的去量化开销和对权重分布的拟合差成为瓶颈。

Method: 提出 HLQ（层级线性量化）作为 ELUTQ 框架的一部分，提供与现有量化算法正交的量化形式，并为端到端推理提供经过优化的 CPU 内核；HLQ 捕捉权重统计特征而不增加 LUT-GEMM 的计算成本，且可无缝集成到现有量化流水线。

Result: 在 LLaMA3-8B 上，3-bit 约降低 8% 的 perplexity，2-bit 约降低 85%（经后训练量化）；量化在约 1 小时内完成；若进行高效微调，2-bit 的表现可在 2 小时内提升；在 Apple M2 的 2-bit LLaMA2-7B 实现约 25 tokens/s（4 线程，batch=1）。

Conclusion: HLQ 与现有量化方法正交兼容，显著降低去量化开销，提升边缘设备的内存与推理吞吐，适用于多种量化管线和硬件实现。

Abstract: The deployment of Large Language Models (LLMs) on CPU-based edge devices is
crucial for enabling on-device intelligence and expanding AI accessibility.
However, it remains challenging due to limited memory and computational
resources. During edge inference, memory usage and latency are the primary
bottlenecks. Although weight quantization can effectively reduce memory
consumption, existing hardware-friendly approaches often rely on uniform
quantization, which poorly fits weight distributions and incurs high
dequantization overhead at low bit widths. To address these limitations, we
propose ELUTQ, an efficient quantization framework introducing a novel
quantization format, Hierarchical Linear Quantization (HLQ). HLQ better
captures the statistical characteristics of weights without increasing the
computational cost of Bit-serial LUT-based GEMM operations, thereby eliminating
dequantization overhead. It is orthogonal to existing quantization algorithms
and can be seamlessly integrated into various quantization pipelines. For
efficient on-device deployment, ELUTQ provides optimized CPU kernels for
end-to-end inference. Experiments show that for LLaMA3-8B, HLQ reduces
perplexity by about 8% at 3-bit and 85% at 2-bit precision under post-training
quantization, completing quantization within one hour. With efficient
finetuning, HLQ further improves 2-bit performance within two hours. In terms
of inference efficiency, our 2-bit LLaMA2-7B achieves over 25 tokens/s on an
Apple M2 chip (4 threads, batch size = 1).

</details>


### [138] [From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification](https://arxiv.org/abs/2510.19514)
*Maciej Mozolewski,Betül Bayrak,Kerstin Bach,Grzegorz J. Nalepa*

Main category: cs.LG

TL;DR: 提出一个原型驱动的稀疏对抗性解释框架，用于12导联心电图分类模型，结合SHAP阈值、DTW、质心聚类以及R波峰对齐，生成可解释、生理一致且近实时的对抗性样本。


<details>
  <summary>Details</summary>
Motivation: 解决AI在医疗时间序列中的可解释性挑战，尤其希望提供可操作且对临床有生物物理一致性的解释，以辅助临床决策。

Method: 建立原型驱动框架：使用SHAP阈值识别关键信号段并形成区间规则；应用DTW与质心聚类提取代表性原型；将原型与查询样本的R波峰对齐以确保连贯性；生成对照样本仅修改原始信号的78%，并在所有类别上保持81.3%的有效性，提升时间稳定性43%；评估三个变体（Original、Sparse、Aligned Sparse），并报告各类别的性能（MI 98.9%有效性，HYP 13.2%面临挑战）。实现近实时(<1s)生成临床有效对照样本，支持互动解释平台。

Result: 框架在不同变体下实现了高效、临床有效的对照样本生成，尤其在MI类别表现优异；对信号片段的稀疏化和对齐增强了解释的生理一致性与稳定性，但在某些疾病谱（如HYP）仍存在挑战。

Conclusion: 为AI诊断系统中的生理学感知对照样本提供设计原则，支持面向临床部署的交互式解释界面发展路径。

Abstract: In eXplainable Artificial Intelligence (XAI), instance-based explanations for
time series have gained increasing attention due to their potential for
actionable and interpretable insights in domains such as healthcare. Addressing
the challenges of explainability of state-of-the-art models, we propose a
prototype-driven framework for generating sparse counterfactual explanations
tailored to 12-lead ECG classification models. Our method employs SHAP-based
thresholds to identify critical signal segments and convert them into interval
rules, uses Dynamic Time Warping (DTW) and medoid clustering to extract
representative prototypes, and aligns these prototypes to query R-peaks for
coherence with the sample being explained. The framework generates
counterfactuals that modify only 78% of the original signal while maintaining
81.3% validity across all classes and achieving 43% improvement in temporal
stability. We evaluate three variants of our approach, Original, Sparse, and
Aligned Sparse, with class-specific performance ranging from 98.9% validity for
myocardial infarction (MI) to challenges with hypertrophy (HYP) detection
(13.2%). This approach supports near realtime generation (< 1 second) of
clinically valid counterfactuals and provides a foundation for interactive
explanation platforms. Our findings establish design principles for
physiologically-aware counterfactual explanations in AI-based diagnosis systems
and outline pathways toward user-controlled explanation interfaces for clinical
deployment.

</details>


### [139] [Teaming LLMs to Detect and Mitigate Hallucinations](https://arxiv.org/abs/2510.19507)
*Demian Till,John Smeaton,Peter Haubrick,Gouse Saheb,Florian Graef,David Berman*

Main category: cs.LG

TL;DR: 把跨多模型的一致性（consortium consistency）用于LLM输出的幻觉检测与缓解，取得相对单模型一致性的显著提升，且常伴随较低的推理成本。


<details>
  <summary>Details</summary>
Motivation: 单模型一致性方法尽管有效，但受限于训练数据偏差和信息覆盖不足等导致的幻觉。通过将不同训练数据、训练方案、模型架构的多模型结合，可能进一步提升检测/缓解性能并降低成本。

Method: 提出并评估“consortium consistency”方法，即在15个不同模型团队的15个LLMs上，聚合它们对同一提示的多次响应，研究在什么条件下把不同LLMs联合起来更有效。

Result: 在广泛的模型集合上获得显著超越单模型一致性的改进；并且这些改进在多数情况下伴随推理成本下降，抵消了多模型协作带来的潜在成本增加。

Conclusion: 跨LLMs协作的一致性策略可以超越单模型，在幻觉检测与缓解方面提供更强的鲁棒性和成本效益，推荐在模型多样性存在且成本被通过优化推理策略控制时采用。

Abstract: Recent work has demonstrated state-of-the-art results in large language model
(LLM) hallucination detection and mitigation through consistency-based
approaches which involve aggregating multiple responses sampled from a single
LLM for a given prompt. These approaches help offset limitations stemming from
the imperfect data on which LLMs are trained, which includes biases and
under-representation of information required at deployment time among other
limitations which can lead to hallucinations. We show that extending these
single-model consistency methods to combine responses from multiple LLMs with
different training data, training schemes and model architectures can result in
substantial further improvements in hallucination detection and mitigation
capabilities beyond their single-model consistency counterparts. We evaluate
this \emph{consortium consistency} approach across many model teams from a pool
of 15 LLMs and explore under what conditions it is beneficial to team together
different LLMs in this manner. Further, we show that these performance
improvements often come with reduced inference costs, offsetting a significant
drawback with single-model consistency methods.

</details>


### [140] [Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning](https://arxiv.org/abs/2510.19530)
*Ruiyao Miao,Junren Xiao,Shiya Tsang,Hui Xiong,Yingnian Wu*

Main category: cs.LG

TL;DR: 提出 REBMBO：将高斯过程用于局部引导，与能量基模型结合以捕获全局结构，把每一步的 BO 视为 MDP，并利用 PPO 进行多步前瞻的自适应探索，解决传统 BO 的一阶段偏差与陷入局部最优的问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在探索与开发之间权衡，易出现“一步偏差”导致收敛到局部最优，尤其在高维或复杂任务中表现不足。尽管黑盒优化在评估代价高昂且梯度不可用的场景表现出色，但缺乏全局结构的有效利用。该工作希望通过结合局部 GP 提供的局部指导与全局结构的 EBM，并引入 MDP+PPO 的多步前瞻来改善探索策略。

Method: 将每次 BO 迭代建模为马尔可夫决策过程，使用高斯过程提供局部信息，能量基模型捕捉全局结构；引入可训练策略（基于 PPO）实现对多步探索深度与方向的自适应调整，从而在不同任务中动态平衡探索与开发。

Result: 在合成与实际基准测试上，REBMBO 展示出与传统 BO 相比的显著性能提升，且对不同 GP 配置具有良好适应性与鲁棒性，表明方法在复杂任务中的通用性与稳定性。

Conclusion: 将强化学习思路与能量基模型相结合的贝叶斯优化框架能够有效缓解传统 BO 的局部收敛和一阶段偏差问题，适用于成本高且梯度不可用的黑盒优化场景，且对 GP 配置的敏感性较低。

Abstract: Existing Bayesian Optimization (BO) methods typically balance exploration and
exploitation to optimize costly objective functions. However, these methods
often suffer from a significant one-step bias, which may lead to convergence
towards local optima and poor performance in complex or high-dimensional tasks.
Recently, Black-Box Optimization (BBO) has achieved success across various
scientific and engineering domains, particularly when function evaluations are
costly and gradients are unavailable. Motivated by this, we propose the
Reinforced Energy-Based Model for Bayesian Optimization (REBMBO), which
integrates Gaussian Processes (GP) for local guidance with an Energy-Based
Model (EBM) to capture global structural information. Notably, we define each
Bayesian Optimization iteration as a Markov Decision Process (MDP) and use
Proximal Policy Optimization (PPO) for adaptive multi-step lookahead,
dynamically adjusting the depth and direction of exploration to effectively
overcome the limitations of traditional BO methods. We conduct extensive
experiments on synthetic and real-world benchmarks, confirming the superior
performance of REBMBO. Additional analyses across various GP configurations
further highlight its adaptability and robustness.

</details>


### [141] [Insights into the Unknown: Federated Data Diversity Analysis on Molecular Data](https://arxiv.org/abs/2510.19535)
*Markus Bujotzek,Evelyn Trautmann,Calum Hand,Ian Hales*

Main category: cs.LG

TL;DR: Federated clustering methods can approximate centralized clustering on distributed molecular datasets, but require chemistry-informed metrics to accurately capture diversity; on-client explainability is crucial for interpreting federated molecular space.


<details>
  <summary>Details</summary>
Motivation: Pharmaceutical data are siloed and proprietary, making large-scale and diverse datasets hard to access. Federated learning enables privacy-preserving collaboration but complicates data-centric tasks such as estimating diversity, informed data splitting, and understanding the combined chemical space. This paper investigates how well federated clustering disentangles and represents distributed molecular data.

Method: The study benchmarks three federated clustering approaches—Fed-kMeans, Federated PCA+Fed-kMeans (Fed-PCA+Fed-kMeans), and Federated Locality-Sensitive Hashing (Fed-LSH)—against centralized baselines across eight molecular datasets. Evaluation uses standard metrics plus a chemistry-informed metric SF-ICF introduced in the work, and includes on-client explainability analyses to assess federated diversity understanding.

Result: Large-scale benchmarking shows that incorporating domain knowledge via chemistry-informed metrics is essential for meaningful interpretation of federated clustering results. The federated methods can disentangle and represent distributed molecular data, but performance and interpretability depend on the method and metrics used. The SF-ICF metric reveals nuances in chemical space that standard metrics may miss, and on-client explainability highlights the importance of local perspectives in federated diversity analysis.

Conclusion: Incorporating domain knowledge is crucial for accurate diversity assessment in federated molecular data analysis. Federated clustering has promise for understanding distributed chemical space under privacy constraints, but it requires chemistry-informed evaluation and robust explainability to be practically useful.

Abstract: AI methods are increasingly shaping pharmaceutical drug discovery. However,
their translation to industrial applications remains limited due to their
reliance on public datasets, lacking scale and diversity of proprietary
pharmaceutical data. Federated learning (FL) offers a promising approach to
integrate private data into privacy-preserving, collaborative model training
across data silos. This federated data access complicates important
data-centric tasks such as estimating dataset diversity, performing informed
data splits, and understanding the structure of the combined chemical space. To
address this gap, we investigate how well federated clustering methods can
disentangle and represent distributed molecular data. We benchmark three
approaches, Federated kMeans (Fed-kMeans), Federated Principal Component
Analysis combined with Fed-kMeans (Fed-PCA+Fed-kMeans), and Federated
Locality-Sensitive Hashing (Fed-LSH), against their centralized counterparts on
eight diverse molecular datasets. Our evaluation utilizes both, standard
mathematical and a chemistry-informed evaluation metrics, SF-ICF, that we
introduce in this work. The large-scale benchmarking combined with an in-depth
explainability analysis shows the importance of incorporating domain knowledge
through chemistry-informed metrics, and on-client explainability analyses for
federated diversity analysis on molecular data.

</details>


### [142] [Bi-Level Decision-Focused Causal Learning for Large-Scale Marketing Optimization: Bridging Observational and Experimental Data](https://arxiv.org/abs/2510.19517)
*Shuli Zhang,Hao Zhou,Jiaqi Zheng,Guibin Jiang,Bing Cheng,Wei Lin,Guihai Chen*

Main category: cs.LG

TL;DR: Bi-DFCL 提出一种双层决策聚焦因果学习框架，结合实验数据的无偏决策质量估计与观测+实验数据的双层优化，通过隐式求导实现，解决预测-决策错配和观测数据偏差带来的高方差问题，在公开数据集、工业数据和在线A/B测试中取得显著改进，并在美团落地应用。


<details>
  <summary>Details</summary>
Motivation: 面临两个核心挑战：1) 预测仅追求准确性、却未考虑下游优化目标，导致预测指标与实际决策质量不一致；2) 观测数据存在选择偏差、位置偏置等，实验数据虽无偏但规模小、成本高，导致估计方差高。需要在保持可行性的同时实现更优的决策效果。

Method: 1) 构建基于实验数据的无偏的 OR 决策质量估计，并通过替代损失桥接离散优化梯度以训练模型。2) 提出一个双层优化框架，联合利用观测数据与实验数据，采用隐式微分求解，使无偏决策估计引导学习方向，从而在偏差-方差之间实现权衡。

Result: 在公开基准、工业数据和大规模在线A/B测试中，对比领先方法显示统计显著的改进；已在美团等大型平台上线应用。

Conclusion: Bi-DFCL 提供一个系统化的方法，将预测任务直接对齐到下游决策目标，并通过双层学习缓解观测偏差带来的高方差，具备良好的落地潜力与对其他营销优化任务的广泛适用性。

Abstract: Online Internet platforms require sophisticated marketing strategies to
optimize user retention and platform revenue -- a classical resource allocation
problem. Traditional solutions adopt a two-stage pipeline: machine learning
(ML) for predicting individual treatment effects to marketing actions, followed
by operations research (OR) optimization for decision-making. This paradigm
presents two fundamental technical challenges. First, the prediction-decision
misalignment: Conventional ML methods focus solely on prediction accuracy
without considering downstream optimization objectives, leading to improved
predictive metrics that fail to translate to better decisions. Second, the
bias-variance dilemma: Observational data suffers from multiple biases (e.g.,
selection bias, position bias), while experimental data (e.g., randomized
controlled trials), though unbiased, is typically scarce and costly --
resulting in high-variance estimates. We propose Bi-level Decision-Focused
Causal Learning (Bi-DFCL) that systematically addresses these challenges.
First, we develop an unbiased estimator of OR decision quality using
experimental data, which guides ML model training through surrogate loss
functions that bridge discrete optimization gradients. Second, we establish a
bi-level optimization framework that jointly leverages observational and
experimental data, solved via implicit differentiation. This novel formulation
enables our unbiased OR estimator to correct learning directions from biased
observational data, achieving optimal bias-variance tradeoff. Extensive
evaluations on public benchmarks, industrial marketing datasets, and
large-scale online A/B tests demonstrate the effectiveness of Bi-DFCL, showing
statistically significant improvements over state-of-the-art. Currently,
Bi-DFCL has been deployed at Meituan, one of the largest online food delivery
platforms in the world.

</details>


### [143] [The Confusing Instance Principle for Online Linear Quadratic Control](https://arxiv.org/abs/2510.19531)
*Waris Radji,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: CI/MED-based MED-LQ for model-based RL in LQR with unknown dynamics; achieves competitive performance and scales beyond small-scale problems.


<details>
  <summary>Details</summary>
Motivation: Traditional optimistic/ Thompson-sampling strategies, rooted in MABs, have practical limitations for control with unknown dynamics. A regret-based CI/MED framework promises asymptotic optimality and better scalability.

Method: Develop MED-LQ by leveraging the Confusing Instance (CI) principle and MED algorithms, exploiting LQR policy structure, and applying sensitivity and stability analysis. Extend MED concepts beyond small-scale settings to large-scale MDPs.

Result: Benchmarks on a comprehensive control suite show MED-LQ achieves competitive performance across various scenarios, validating its effectiveness and scalability.

Conclusion: MED-LQ demonstrates that CI/MED-based approaches can be extended to larger-scale control problems, offering competitive performance and potential for broader MDP applications.

Abstract: We revisit the problem of controlling linear systems with quadratic cost
under unknown dynamics with model-based reinforcement learning. Traditional
methods like Optimism in the Face of Uncertainty and Thompson Sampling, rooted
in multi-armed bandits (MABs), face practical limitations. In contrast, we
propose an alternative based on the Confusing Instance (CI) principle, which
underpins regret lower bounds in MABs and discrete Markov Decision Processes
(MDPs) and is central to the Minimum Empirical Divergence (MED) family of
algorithms, known for their asymptotic optimality in various settings. By
leveraging the structure of LQR policies along with sensitivity and stability
analysis, we develop MED-LQ. This novel control strategy extends the principles
of CI and MED beyond small-scale settings. Our benchmarks on a comprehensive
control suite demonstrate that MED-LQ achieves competitive performance in
various scenarios while highlighting its potential for broader applications in
large-scale MDPs.

</details>


### [144] [Study of Training Dynamics for Memory-Constrained Fine-Tuning](https://arxiv.org/abs/2510.19675)
*Aël Quélennec,Nour Hezbri,Pavlo Mozharovskyi,Van-Tam Nguyen,Enzo Tartaglione*

Main category: cs.LG

TL;DR: TraDy introduces dynamic channel selection for memory-efficient transfer learning, using architecture-aware layer importance and stochastic channel resampling to achieve high sparsity and compute reduction, with state-of-the-art performance across tasks.


<details>
  <summary>Details</summary>
Motivation: As models grow, memory constraints hinder training. Understanding which layers require updates and how to approximate gradients efficiently is crucial; proposing a scheme that exploits architecture-dependent update importance and stochastic channel sampling to reduce memory and computation.

Method: TraDy performs dynamic channel selection by stochastically resampling channels between epochs within preselected layers, leveraging layer-wise importance to guide updates in a transfer learning setup.

Result: Experiments show state-of-the-art performance across various downstream tasks and architectures under strict memory constraints, achieving up to 99% activation sparsity, 95% weight derivative sparsity, and 97% FLOPs reduction for weight-derivative computation.

Conclusion: TraDy offers a effective memory-efficient transfer learning approach by combining architecture-aware layer updates with dynamic, stochastic channel sampling, enabling strong performance under tight resource limits.

Abstract: Memory-efficient training of deep neural networks has become increasingly
important as models grow larger while deployment environments impose strict
resource constraints. We propose TraDy, a novel transfer learning scheme
leveraging two key insights: layer importance for updates is
architecture-dependent and determinable a priori, while dynamic stochastic
channel selection provides superior gradient approximation compared to static
approaches. We introduce a dynamic channel selection approach that
stochastically resamples channels between epochs within preselected layers.
Extensive experiments demonstrate TraDy achieves state-of-the-art performance
across various downstream tasks and architectures while maintaining strict
memory constraints, achieving up to 99% activation sparsity, 95% weight
derivative sparsity, and 97% reduction in FLOPs for weight derivative
computation.

</details>


### [145] [Learning and Simulating Building Evacuation Patterns for Enhanced Safety Design Using Generative Models](https://arxiv.org/abs/2510.19623)
*Jin Han,Zhe Zheng,Yi Gu,Jia-Rui Lin,Xin-Zheng Lu*

Main category: cs.LG

TL;DR: DiffEvac提出基于扩散模型的建筑疏散模式学习方法，以提升疏散仿真速度和早期设计迭代效率。通过399个多样功能布局及其疏散热图数据集，采用解耦特征表示嵌入布局与人口密度等物理特征，并使用基于图像提示的扩散模型从仿真热图中学习疏散模式。相较于RGB表示的条件GAN，DiffEvac在SSIM上提升约37.6%、PSNR提升约142%，仿真速度提升约16倍，仿真时间降至约2分钟，支持快速设计迭代并为安全优化提供新路径。


<details>
  <summary>Details</summary>
Motivation: 在早期设计阶段，传统疏散仿真因需要大量参数和复杂建模，难以实现快速迭代。需要一种能够快速学习并生成疏散模式的工具，以提升设计效率与安全性。

Method: 建立399个多样功能布局及相应疏散热图的数据集；提出解耦特征表示以嵌入布局、人口密度等物理特征用于生成模型；采用基于图像提示的扩散模型从仿真热图中学习疏散模式；并与使用RGB表示的条件GAN进行对比。

Result: 与RGB-CGAN相比，DiffEvac在SSIM提升约37.6%、PSNR提升约142%，仿真速度提高约16倍，仿真时间降至约2分钟；案例研究显示在快速设计迭代、参数调整及多目标设计耦合方面具有显著优势，并为未来建筑安全优化提供技术路径。

Conclusion: 该方法降低建模负担，支持大规模“如果-则”探索，并便于与多目标设计工具耦合，推动智能建筑设计中的安全优化与快速迭代。

Abstract: Evacuation simulation is essential for building safety design, ensuring
properly planned evacuation routes. However, traditional evacuation simulation
relies heavily on refined modeling with extensive parameters, making it
challenging to adopt such methods in a rapid iteration process in early design
stages. Thus, this study proposes DiffEvac, a novel method to learn building
evacuation patterns based on Generative Models (GMs), for efficient evacuation
simulation and enhanced safety design. Initially, a dataset of 399 diverse
functional layouts and corresponding evacuation heatmaps of buildings was
established. Then, a decoupled feature representation is proposed to embed
physical features like layouts and occupant density for GMs. Finally, a
diffusion model based on image prompts is proposed to learn evacuation patterns
from simulated evacuation heatmaps. Compared to existing research using
Conditional GANs with RGB representation, DiffEvac achieves up to a 37.6%
improvement in SSIM, 142% in PSNR, and delivers results 16 times faster,
thereby cutting simulation time to 2 minutes. Case studies further demonstrate
that the proposed method not only significantly enhances the rapid design
iteration and adjustment process with efficient evacuation simulation but also
offers new insights and technical pathways for future safety optimization in
intelligent building design. The research implication is that the approach
lowers the modeling burden, enables large-scale what-if exploration, and
facilitates coupling with multi-objective design tools.

</details>


### [146] [Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series](https://arxiv.org/abs/2510.19728)
*Mahmoud Ibrahim,Bart Elen,Chang Sun,Gökhan Ertaylan,Michel Dumontier*

Main category: cs.LG

TL;DR: 增强型 TimeAutoDiff 通过分布对齐惩罚，提升合成 ICU 时序数据的真实性与评估可信度，在人口层面和32个交叉子群的细粒度子群中显著降低真实对合成评估的差距（TRTS）并提高子群 AUROC 的估计稳定性。


<details>
  <summary>Details</summary>
Motivation: 需要在保护隐私的前提下，使用合成数据不仅用于训练，还要对预测模型进行可信、细粒度的评估，尤其在稀缺的子群和关键病区数据上。现有方法在真实–合成评估间存在较大差距，且难以在高风险临床场景中实现可解释的信任。

Method: 在 TimeDiff、HealthGen、TimeAutoDiff 的基础上，提出增强的潜变量扩散目标，并引入分布对齐惩罚以提升合成数据分布与真实数据的匹配度。对 MIMIC-III 和 eICU 的 24 小时死亡率和二元住院 LOS 任务进行大量基准测试，评估 24 小时死亡率和 LOS 的预测模型。对32个交叉子群使用大规模合成队列，比较真实小测试集的评估误差。

Result: 相较于基线，Enhanced TimeAutoDiff 将 TRTS 差距降低超过 70%，实现 Δ_TRTS ≤ 0.014 的 AUROC；训练效用保持接近（Δ_TSTR ≈ 0.01）。在32个交叉子群中，大规模合成队列可将子群 AUROC 的估计误差最多降低 50%，并在 72-84% 的子群中优于使用小真实测试集的基线。

Conclusion: 这项工作为关键护理领域的可信、细粒度模型评估提供了一条实用且具隐私保护的路线图，有助于在不暴露敏感 EHR 数据的前提下，对多样化人群的模型性能进行稳健评估，提升 Medical AI 的可信度。

Abstract: We present a novel framework for leveraging synthetic ICU time-series data
not only to train but also to rigorously and trustworthily evaluate predictive
models, both at the population level and within fine-grained demographic
subgroups. Building on prior diffusion and VAE-based generators (TimeDiff,
HealthGen, TimeAutoDiff), we introduce \textit{Enhanced TimeAutoDiff}, which
augments the latent diffusion objective with distribution-alignment penalties.
We extensively benchmark all models on MIMIC-III and eICU, on 24-hour mortality
and binary length-of-stay tasks. Our results show that Enhanced TimeAutoDiff
reduces the gap between real-on-synthetic and real-on-real evaluation (``TRTS
gap'') by over 70\%, achieving $\Delta_{TRTS} \leq 0.014$ AUROC, while
preserving training utility ($\Delta_{TSTR} \approx 0.01$). Crucially, for 32
intersectional subgroups, large synthetic cohorts cut subgroup-level AUROC
estimation error by up to 50\% relative to small real test sets, and outperform
them in 72--84\% of subgroups. This work provides a practical,
privacy-preserving roadmap for trustworthy, granular model evaluation in
critical care, enabling robust and reliable performance analysis across diverse
patient populations without exposing sensitive EHR data, contributing to the
overall trustworthiness of Medical AI.

</details>


### [147] [Latent Space Factorization in LoRA](https://arxiv.org/abs/2510.19640)
*Shashi Kumar,Yacouba Kaloga,John Mitros,Petr Motlicek,Ina Kodrasi*

Main category: cs.LG

TL;DR: FVAE-LoRA: 将变分自编码器用于LoRA，分离出任务相关与残差信息的两个潜在空间，从而提升跨模态任务的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA在低秩子空间中缺乏显式区分任务相关信息的机制，可能限制下游性能并降低对分布偏移的鲁棒性。

Method: 使用变分自编码器学习两个独立潜在空间，并引入证据下界（ELBO）中的因子化约束，强制一个潜空间专注于任务显著特征，另一个潜空间负责残差信息。对文本、音频和图像任务进行广泛实验，评估任务相关信号的分离与鲁棒性。

Result: FVAE-LoRA在文本、音频和图像任务上持续优于标准LoRA，并且在降低伪相关及提高分布偏移鲁棒性方面表现更好。代码公开可用。

Conclusion: 通过在LoRA的低秩子空间中引入分解化的潜在空间，FVAE-LoRA能够更好地分离任务相关信号与无关信息，从而提升性能并增强对分布变化的鲁棒性。

Abstract: Low-rank adaptation (LoRA) is a widely used method for parameter-efficient
finetuning. However, existing LoRA variants lack mechanisms to explicitly
disambiguate task-relevant information within the learned low-rank subspace,
potentially limiting downstream performance. We propose Factorized Variational
Autoencoder LoRA (FVAE-LoRA), which leverages a VAE to learn two distinct
latent spaces. Our novel Evidence Lower Bound formulation explicitly promotes
factorization between the latent spaces, dedicating one latent space to
task-salient features and the other to residual information. Extensive
experiments on text, audio, and image tasks demonstrate that FVAE-LoRA
consistently outperforms standard LoRA. Moreover, spurious correlation
evaluations confirm that FVAE-LoRA better isolates task-relevant signals,
leading to improved robustness under distribution shifts. Our code is publicly
available at: https://github.com/idiap/FVAE-LoRA

</details>


### [148] [A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation](https://arxiv.org/abs/2510.19755)
*Jiacheng Liu,Xinyu Wang,Yuqi Lin,Zhikai Wang,Peiru Wang,Peiliang Cai,Qinming Zhou,Zhengan Yan,Zexuan Yan,Zhengyi Shi,Chang Zou,Yue Ma,Linfeng Zhang*

Main category: cs.LG

TL;DR: 训练无关的推断加速：通过跨步重用和层间调度实现对扩散模型的计算复用，以减少推理开销而不改动模型参数。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因多步迭代和复杂骨架而造成高计算成本和低延迟需求之间的矛盾，需无参数修改的高效推断方案。

Method: 提出 Diffusion Caching，识别扩散过程中的内在冗余，支持特征层级跨步重用和层间调度，基于训练无参数的前提，构建一个统一框架对方法进行分类与分析；并将静态重用演化为动态预测。

Result: 对代表性方法进行对比分析，表明该范式从静态重用发展到动态预测，具备更强的任务泛化和与采样优化、模型蒸馏等加速手段的兼容性，形成可用于实时多模态和交互应用的高效推断框架。

Conclusion: Diffusion Caching 将成为实现实时高效生成式智能的关键驱动，推动高效生成性人工智能的理论与实践。

Abstract: Diffusion Models have become a cornerstone of modern generative AI for their
exceptional generation quality and controllability. However, their inherent
\textit{multi-step iterations} and \textit{complex backbone networks} lead to
prohibitive computational overhead and generation latency, forming a major
bottleneck for real-time applications. Although existing acceleration
techniques have made progress, they still face challenges such as limited
applicability, high training costs, or quality degradation.
  Against this backdrop, \textbf{Diffusion Caching} offers a promising
training-free, architecture-agnostic, and efficient inference paradigm. Its
core mechanism identifies and reuses intrinsic computational redundancies in
the diffusion process. By enabling feature-level cross-step reuse and
inter-layer scheduling, it reduces computation without modifying model
parameters. This paper systematically reviews the theoretical foundations and
evolution of Diffusion Caching and proposes a unified framework for its
classification and analysis.
  Through comparative analysis of representative methods, we show that
Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic
prediction}. This trend enhances caching flexibility across diverse tasks and
enables integration with other acceleration techniques such as sampling
optimization and model distillation, paving the way for a unified, efficient
inference framework for future multimodal and interactive applications. We
argue that this paradigm will become a key enabler of real-time and efficient
generative AI, injecting new vitality into both theory and practice of
\textit{Efficient Generative Intelligence}.

</details>


### [149] [Overlap-weighted orthogonal meta-learner for treatment effect estimation over time](https://arxiv.org/abs/2510.19643)
*Konstantin Hess,Dennis Frauen,Mihaela van der Schaar,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出重叠权重正交元学习器（WO）用于时间变化设置的异质治疗效应估计，解决长预测时的重叠不足导致的方差膨胀问题；基于 Neyman 正交的总体风险，数据驱动且模型无关，实验使用 Transformer 与 LSTM 骨干验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在时间变化治疗设置中，预测 horizon 越长，观察到某些治疗序列的概率越低，导致重叠严重不足，进而使估计方差显著上升。现有元学习方法往往假设充足重叠，遇到低重叠时稳定性下降。需要一种数据驱动、对重叠不足具鲁棒性的HTE估计方法。

Method: 提出重叠权重正交（WO）元学习器，聚焦于在观测数据中具有高概率接受干预序列的区域；建立新的 Neyman-orthogonal 的总体风险函数以最小化重叠加权的 oracle 风险；WO 学习器具有正交性，鲁棒于 nuisance 函数的错设，且对模型无关，能够与任意机器学习模型结合；通过在 Transformer 与 LSTM 回归骨干上的广泛实验来验证。

Result: WO 学习器在提高 HTE 估计的可靠性方面具有明显优势，能够缓解因重叠不足引发的估计方差膨胀；对 nuisance 函数错设具有鲁棒性；在 Transformer 和 LSTM 骨干的实验中显示出显著的性能提升。

Conclusion: WO 元学习器提供了一种全数据驱动、对重叠不足鲁棒且可与任意模型结合的时间变化HTE估计解决方案，尤其在观测到的高重叠区域能实现更稳定的估计。

Abstract: Estimating heterogeneous treatment effects (HTEs) in time-varying settings is
particularly challenging, as the probability of observing certain treatment
sequences decreases exponentially with longer prediction horizons. Thus, the
observed data contain little support for many plausible treatment sequences,
which creates severe overlap problems. Existing meta-learners for the
time-varying setting typically assume adequate treatment overlap, and thus
suffer from exploding estimation variance when the overlap is low. To address
this problem, we introduce a novel overlap-weighted orthogonal (WO)
meta-learner for estimating HTEs that targets regions in the observed data with
high probability of receiving the interventional treatment sequences. This
offers a fully data-driven approach through which our WO-learner can counteract
instabilities as in existing meta-learners and thus obtain more reliable HTE
estimates. Methodologically, we develop a novel Neyman-orthogonal population
risk function that minimizes the overlap-weighted oracle risk. We show that our
WO-learner has the favorable property of Neyman-orthogonality, meaning that it
is robust against misspecification in the nuisance functions. Further, our
WO-learner is fully model-agnostic and can be applied to any machine learning
model. Through extensive experiments with both transformer and LSTM backbones,
we demonstrate the benefits of our novel WO-learner.

</details>


### [150] [Policy Learning with Abstention](https://arxiv.org/abs/2510.19672)
*Ayush Sawarni,Jikai Jin,Justin Whitehouse,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: 提出一个两阶段的策略学习带 abstention（拒绝）机制的算法：第一阶段从近似最优策略集合中识别集合，第二阶段基于它们的分歧构造 abstention 规则；在已知倾向下获得快速的 O(1/n) regret 保证，未知倾向通过双重鲁棒目标扩展；abstention 还直接应用于相关的策略学习问题，提升边界条件下的保证、对数据漂移的鲁棒性，以及安全策略改进。


<details>
  <summary>Details</summary>
Motivation: 在高风险、需要谨慎决策的场景中强制预测具有风险，因此引入 abstention 机制，使策略在不确定时可弃权，转而采用安全默认策略或专家意见。

Method: 提出一个两阶段学习框架：先筛选出近似最优策略集合，然后基于它们之间的分歧构建 abstention 规则。已知 propensity 时获得 O(1/n) 的快速 regret 上界，未知 propensity 时通过一个双重鲁棒（DR）目标进行扩展。

Result: 理论上给出在已知倾向下的 O(1/n) 级别 regret 保证，且对未知倾向也可通过 DR 目标实现鲁棒性扩展。此外，abstention 作为工具可直接用于提升在 margin 条件下的保证、不依赖 realizability 的情形、对小数据漂移的分布鲁棒性，以及在高概率下对基线策略的安全改进。

Conclusion: abstention 提供了一种通用且有效的策略学习工具，能够提升鲁棒性和安全性，并与分布式鲁棒性、边界条件等核心问题产生直接且有益的联系。

Abstract: Policy learning algorithms are widely used in areas such as personalized
medicine and advertising to develop individualized treatment regimes. However,
most methods force a decision even when predictions are uncertain, which is
risky in high-stakes settings. We study policy learning with abstention, where
a policy may defer to a safe default or an expert. When a policy abstains, it
receives a small additive reward on top of the value of a random guess. We
propose a two-stage learner that first identifies a set of near-optimal
policies and then constructs an abstention rule from their disagreements. We
establish fast O(1/n)-type regret guarantees when propensities are known, and
extend these guarantees to the unknown-propensity case via a doubly robust (DR)
objective. We further show that abstention is a versatile tool with direct
applications to other core problems in policy learning: it yields improved
guarantees under margin conditions without the common realizability assumption,
connects to distributionally robust policy learning by hedging against small
data shifts, and supports safe policy improvement by ensuring improvement over
a baseline policy with high probability.

</details>


### [151] [Fast Inference via Hierarchical Speculative Decoding](https://arxiv.org/abs/2510.19705)
*Amir Globerson,Haim Kaplan,Yishay Mansour,Clara Mohri,Tal Schuster*

Main category: cs.LG

TL;DR: 将草拟模型堆叠成层级的推理解码策略，以降低 Transformer 自回归文本生成的潜在延迟；通过更大的后续模型逐层验证草拟 token，直到目标模型完成验证；可以在多层草拟模型中找出延迟最优的层级，且该最优层级可在多项式时间内确定；实验证明相较于最好的单草拟模型，层级方法可实现约 1.2x 的加速。


<details>
  <summary>Details</summary>
Motivation: 自回归生成的推理延迟随生成 token 数量线性增加；虽然单一草拟模型的 speculative decoding 能在一定程度上降低延迟，但不同草拟模型在速度与准确性之间存在 trade-off，需要一种能整合多种草拟模型的层级框架来进一步提升速度且不损害输出质量。

Method: 提出层级式 speculative decoding：将多个草拟模型按层级堆叠，每层模型提出候选 token，下一层对前一层的候选进行单次前向验证，直到最终的目标模型验证输出；推导任意层级结构的期望延迟表达式，并证明可以在多项式时间内找到延迟最优的层级；在实际实验中评估不同层级的性能与速度。

Result: 实验结果显示，层级式策略在最优单草拟基线之上可实现最高约 1.2x 的速度提升，证明该方法在降低生成延迟方面的实用性。

Conclusion: 引入层级草拟模型后，HSD 能在速度与可靠性之间取得更好的权衡，显著降低自回归文本生成的推理延迟；并且最优层级的选择可在多项式时间内确定，验证了方法的可行性与实用性。

Abstract: Transformer language models generate text autoregressively, making inference
latency proportional to the number of tokens generated. Speculative decoding
reduces this latency without sacrificing output quality, by leveraging a small
draft model to propose tokens that the larger target model verifies in
parallel. In practice, however, there may exist a set of potential draft
models- ranging from faster but less inaccurate, to slower yet more reliable.
We introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacks
these draft models into a hierarchy, where each model proposes tokens, and the
next larger model verifies them in a single forward pass, until finally the
target model verifies tokens. We derive an expression for the expected latency
of any such hierarchy and show that selecting the latency-optimal hierarchy can
be done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over the
best single-draft baseline, demonstrating the practicality of our algorithm in
reducing generation latency beyond previous techniques.

</details>


### [152] [Semantic World Models](https://arxiv.org/abs/2510.19818)
*Jacob Berg,Chuning Zhu,Yanda Bao,Ishan Durugkar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 将未来预测从像素重建转为语义信息预测，使用视觉-语言模型作为“语义世界模型”来提升机器人规划的泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 像素级未来重建与实际规划目标不完全一致；需要关注对任务有用的语义信息，以及利用强大视觉语言模型的泛化能力。

Method: 将世界建模问题建模为对未来帧的语义信息进行视觉问答；通过在图像-动作-文本数据上对预训练的视觉-语言模型进行有监督微调，使其成为语义世界模型，再用于策略改进。

Result: 在开放式机器人任务上证明了策略改进，与基于像素重建的世界建模相比，达到显著的泛化改进。

Conclusion: 以语义预测代替像素重建的世界模型可以更好地服务于规划目标，利用视觉语言模型的鲁棒性和泛化能力，推动机器人任务的泛化性提升。

Abstract: Planning with world models offers a powerful paradigm for robotic control.
Conventional approaches train a model to predict future frames conditioned on
current frames and actions, which can then be used for planning. However, the
objective of predicting future pixels is often at odds with the actual planning
objective; strong pixel reconstruction does not always correlate with good
planning decisions. This paper posits that instead of reconstructing future
frames as pixels, world models only need to predict task-relevant semantic
information about the future. For such prediction the paper poses world
modeling as a visual question answering problem about semantic information in
future frames. This perspective allows world modeling to be approached with the
same tools underlying vision language models. Thus vision language models can
be trained as "semantic" world models through a supervised finetuning process
on image-action-text data, enabling planning for decision-making while
inheriting many of the generalization and robustness properties from the
pretrained vision-language models. The paper demonstrates how such a semantic
world model can be used for policy improvement on open-ended robotics tasks,
leading to significant generalization improvements over typical paradigms of
reconstruction-based action-conditional world modeling. Website available at
https://weirdlabuw.github.io/swm.

</details>


### [153] [Statistical Inference for Linear Functionals of Online Least-squares SGD when $t \gtrsim d^{1+δ}$](https://arxiv.org/abs/2510.19734)
*Bhavya Agrawalla,Krishnakumar Balasubramanian,Promit Ghosal*

Main category: cs.LG

TL;DR: 在高维在线最小二乘 SGD 场景下，给出非渐近 Berry–Esseen 边界并证明线性函数的 CLT，允许 t 仅需略高于 d 的幂次增长（t ≥ d^{1+δ}），并给出在线方差估计与高概率偏差界，构建第一套全在线的数据驱动置信区间。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景中对 SGD 的不确定性进行稳健量化。现有高维推断通常依赖矩阵求逆，成本高且对维数的依赖很强，难以在线、大规模场景落地。

Method: 建立非对称的 Berry–Esseen 边界，证明在线最小二乘 SGD 的线性函数在 t ∼ d^{1+δ} 条件下满足高维 CLT；设计在线方差估计器并给出其高概率偏差界；计算复杂度为 O(td)，内存为 O(d)；并提供数据驱动的置信区间构造。

Result: 给出首个完全在线、数据驱动的推断框架，在近最优的 t≥d^{1+δ} 表现下实现对 SGD 迭代的置信区间；对比矩阵求逆方法，显著降低时间/空间成本并扩大可扩展性。

Conclusion: 为高维在线推断提供理论严谨且实用的框架，扩展了对 SGD 不确定性的统计理解，便于在大规模在线学习场景中直接应用。

Abstract: Stochastic Gradient Descent (SGD) has become a cornerstone method in modern
data science. However, deploying SGD in high-stakes applications necessitates
rigorous quantification of its inherent uncertainty. In this work, we establish
\emph{non-asymptotic Berry--Esseen bounds} for linear functionals of online
least-squares SGD, thereby providing a Gaussian Central Limit Theorem (CLT) in
a \emph{growing-dimensional regime}. Existing approaches to high-dimensional
inference for projection parameters, such as~\cite{chang2023inference}, rely on
inverting empirical covariance matrices and require at least $t \gtrsim
d^{3/2}$ iterations to achieve finite-sample Berry--Esseen guarantees,
rendering them computationally expensive and restrictive in the allowable
dimensional scaling. In contrast, we show that a CLT holds for SGD iterates
when the number of iterations grows as $t \gtrsim d^{1+\delta}$ for any $\delta
> 0$, significantly extending the dimensional regime permitted by prior works
while improving computational efficiency. The proposed online SGD-based
procedure operates in $\mathcal{O}(td)$ time and requires only $\mathcal{O}(d)$
memory, in contrast to the $\mathcal{O}(td^2 + d^3)$ runtime of
covariance-inversion methods. To render the theory practically applicable, we
further develop an \emph{online variance estimator} for the asymptotic variance
appearing in the CLT and establish \emph{high-probability deviation bounds} for
this estimator. Collectively, these results yield the first fully online and
data-driven framework for constructing confidence intervals for SGD iterates in
the near-optimal scaling regime $t \gtrsim d^{1+\delta}$.

</details>


### [154] [BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models](https://arxiv.org/abs/2510.19749)
*Catherine Villeneuve,Benjamin Akera,Mélisande Teng,David Rolnick*

Main category: cs.LG

TL;DR: 提出 BATIS 框架的贝叶斯深度学习 SDM，在有限观测数据下迭代更新先验预测，兼顾 aleatoric 与 epistemic 不确定性；在数据稀缺区域显著提升预测可靠性，基于 eBird 数据集的评估支持其实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 解决当前深度SDM在空间偏差和不确定性处理方面的不足。通过贝叶斯方法将局部细节与全局生态模式结合，并在数据稀缺情境下提高预测的可靠性；并利用公民科学数据进行更广泛的评估。

Method: 提出 BATIS 框架，在有限观测数据下迭代更新先验预测，同时显式建模和更新 aleatoric 与 epistemic 不确定性；对多种不确定性量化方法进行基准评估，数据集包含来自 eBird 的公民科学观测。

Result: 贝叶斯深度学习方法能显著提升数据稀缺区域 SDM 的可靠性，促进对物种分布的更可靠推断；对生态理解和保育工作具有潜在贡献。

Conclusion: BATIS 框架展示了将贝叶斯推断与深度 SDM 相结合以在数据不足时提升预测可靠性的可行性，强调不确定性分解与迭代更新在生态预测中的重要性。

Abstract: Species distribution models (SDMs), which aim to predict species occurrence
based on environmental variables, are widely used to monitor and respond to
biodiversity change. Recent deep learning advances for SDMs have been shown to
perform well on complex and heterogeneous datasets, but their effectiveness
remains limited by spatial biases in the data. In this paper, we revisit deep
SDMs from a Bayesian perspective and introduce BATIS, a novel and practical
framework wherein prior predictions are updated iteratively using limited
observational data. Models must appropriately capture both aleatoric and
epistemic uncertainty to effectively combine fine-grained local insights with
broader ecological patterns. We benchmark an extensive set of uncertainty
quantification approaches on a novel dataset including citizen science
observations from the eBird platform. Our empirical study shows how Bayesian
deep learning approaches can greatly improve the reliability of SDMs in
data-scarce locations, which can contribute to ecological understanding and
conservation efforts.

</details>


### [155] [When Do Transformers Learn Heuristics for Graph Connectivity?](https://arxiv.org/abs/2510.19753)
*Qilin Ye,Deqing Fu,Robin Jia,Vatsal Sharan*

Main category: cs.LG

TL;DR: Transformers 在学习可泛化的算法方面常依赖不稳定的启发式策略。本文以图连通性为测试床，理论与实证分析揭示：一个简化的解耦 Transformer 在 L 层时具备解决直径不超过 3^L 的图的问题的容量，等价于计算邻接矩阵的幂；训练动态表明，若大多数训练样本在模型容量内，则能学习到正确的算法；容量之外的图则促使模型学习基于结点度数的启发式。通过限制训练数据在容量内，标准与解耦 Transformer 都能学习到真正的算法而非度数启发式。


<details>
  <summary>Details</summary>
Motivation: 理解为什么 Transformer 往往学到脆弱的启发式而非可泛化的算法；以图连通性作为测试床，从理论与实验角度分析模型容量与学习策略的关系，揭示训练数据分布对算法学习的影响。

Method: 分析一个简化的 Transformer 架构（解耦 Transformer），证明在 L 层时能够解决直径最多为 3^L 的图，等价于计算邻接矩阵的幂；研究训练过程中的学习动力学，揭示学习策略取决于大多数样本是否在容量之内；通过实验比较容量内外图的学习效果。

Result: 理论：L 层模型具备解决直径上限为 3^L 的图的能力，相当于实现邻接矩阵的幂操作。实验：在容量内的训练数据使模型学习到正确的算法而非度数启发；容量外的图促使模型倾向于度数基的启发式；限制训练数据在容量内可使标准与解耦 Transformer 学到真正的算法。

Conclusion: 模型容量与训练数据分布的匹配决定是否学习到正确的算法。以图连通性为测试床的分析揭示了 Transformer 泛化算法学习的局限性，并表明通过合适的训练数据设计，可促使模型学到真正的算法而非简单的度数启发式。

Abstract: Transformers often fail to learn generalizable algorithms, instead relying on
brittle heuristics. Using graph connectivity as a testbed, we explain this
phenomenon both theoretically and empirically. We consider a simplified
Transformer architecture, the disentangled Transformer, and prove that an
$L$-layer model has capacity to solve for graphs with diameters up to exactly
$3^L$, implementing an algorithm equivalent to computing powers of the
adjacency matrix. We analyze the training-dynamics, and show that the learned
strategy hinges on whether most training instances are within this model
capacity. Within-capacity graphs (diameter $\leq 3^L$) drive the learning of a
correct algorithmic solution while beyond-capacity graphs drive the learning of
a simple heuristic based on node degrees. Finally, we empirically demonstrate
that restricting training data within a model's capacity leads to both standard
and disentangled transformers learning the exact algorithm rather than the
degree-based heuristic.

</details>


### [156] [CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees](https://arxiv.org/abs/2510.19754)
*Aman Bilkhoo,Milad Kazemi,Nicola Paoletti,Mehran Hosseini*

Main category: cs.LG

TL;DR: 提出 CONFEX：一种结合置信预测区间与混合整数线性规划的局部覆盖性保障的反事实解释框架，能在不确定性高的区域避免提供误导性解释，并以离线树划分实现高效编码。


<details>
  <summary>Details</summary>
Motivation: 为了让反事实解释具有可靠性，需避免高预测不确定性区域，并在解释的同时提供形式化的覆盖保障；现有方法要么忽略不确定性要么缺乏保障机制。

Method: 引入基于 Conformal Prediction 的局部不确定性评估，与 MILP 相结合生成反事实解释。通过离线的基于树的输入空间分割实现高效的 MILP 编码，建立局部覆盖性保障。

Result: 在多种基准和指标上与现有方法比较，证明不确定性感知的方法能产生更鲁棒、可信的解释。

Conclusion: CONFEX 提供了具备预测不确定性与最优性保障的局部覆盖性的反事实解释，为解释的可靠性与可操作性提供了理论与实践双重保障。

Abstract: Counterfactual explanations (CFXs) provide human-understandable
justifications for model predictions, enabling actionable recourse and
enhancing interpretability. To be reliable, CFXs must avoid regions of high
predictive uncertainty, where explanations may be misleading or inapplicable.
However, existing methods often neglect uncertainty or lack principled
mechanisms for incorporating it with formal guarantees. We propose CONFEX, a
novel method for generating uncertainty-aware counterfactual explanations using
Conformal Prediction (CP) and Mixed-Integer Linear Programming (MILP). CONFEX
explanations are designed to provide local coverage guarantees, addressing the
issue that CFX generation violates exchangeability. To do so, we develop a
novel localised CP procedure that enjoys an efficient MILP encoding by
leveraging an offline tree-based partitioning of the input space. This way,
CONFEX generates CFXs with rigorous guarantees on both predictive uncertainty
and optimality. We evaluate CONFEX against state-of-the-art methods across
diverse benchmarks and metrics, demonstrating that our uncertainty-aware
approach yields robust and plausible explanations.

</details>


### [157] [GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters](https://arxiv.org/abs/2510.19778)
*Anand Choudhary,Yasser Sulaıman,Lukas Mauch,Ghouthi Boukli Hacene,Fabien Cardinaux,Antoine Bosselut*

Main category: cs.LG

TL;DR: GaLLoP 通过在下游任务中选择梯度最大的参数且初始权重最小的稀疏子集来进行微调，在 LLaMA3 8B 和 Gemma 2B 上实现稳定且具有竞争力的性能，并较好地避免灾难性遗忘，相较 LoRA、DoRA、SAFT 等方法具备一致的性能优势。


<details>
  <summary>Details</summary>
Motivation: 稀疏微调的效果高度依赖于选择的参数。需要在不大幅破坏预训练知识的前提下，对任务相关性进行高效适配。通过结合参数的梯度信息与初始大小，发现对任务最相关且对原始知识影响最小的参数子集。

Method: 1) 计算下游任务中参数的梯度大小；2) 选择梯度幅值最大且预训练幅值最小的参数子集；3) 对该子集进行微调；4) 与 LoRA、DoRA、SAFT 等方法在 LLaMA3 8B 与 Gemma 2B 上进行对比；5) 使用多组随机种子评估鲁棒性；6) 同时评估分布内与分布外的任务性能。

Result: GaLLoP 在分布内和分布外都实现了对比方法的稳定提升或等效表现；减轻了灾难性遗忘和对训练数据的记忆化；对关键的预训练参数保持不变，且在不同随机种子下表现更稳健。

Conclusion: 通过梯度信息与参数初值大小的联合筛选，GaLLoP 提供了一种高效且鲁棒的稀疏微调策略，能在多种基模型上实现优于或接近现有参数高效微调方法的效果，同时更好地保护预训练知识。

Abstract: Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a
sparse subset of model parameters. However, the effectiveness of sparse
adaptation depends on optimally selecting the model parameters to be
fine-tuned. In this work, we introduce a novel sparse fine-tuning technique
named GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which
fine-tunes only those model parameters which have the largest gradient
magnitudes on downstream tasks and the smallest pre-trained magnitudes,
intuitively prioritizing parameters that are highly task-relevant, but
minimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3
8B and Gemma 2B as base models shows that GaLLoP consistently improves or
matches the in-distribution as well as out-of-distribution performance obtained
via the usage of other leading parameter-efficient fine-tuning techniques,
including LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates
catastrophic forgetting and memorization of task data, as important pre-trained
parameters remain unchanged, and stabilizes performance relative to other
fine-tuning techniques, robustly generalizing across most random seeds.

</details>


### [158] [Blackbox Model Provenance via Palimpsestic Membership Inference](https://arxiv.org/abs/2510.19796)
*Rohith Kuditipudi,Jing Huang,Sally Zhu,Diyi Yang,Christopher Potts,Percy Liang*

Main category: cs.LG

TL;DR: 提出在开源权重语言模型场景下，通过独立性检验结合 palimpsestic memorization 概念来判断Bob是否使用Alice的模型，涵盖查询设置与观测设置的两类证据与统计检验。


<details>
  <summary>Details</summary>
Motivation: 在没有显式合作或接口证据的情况下，尝试从统计角度证明他人是否在使用自己的训练模型，尤其利用数据顺序对记忆的影响来提供证据。

Method: 将问题建模为独立性检验：零假设为Bob的模型/文本与Alice的随机化训练过程独立。查询设置通过提示得到Bob模型对Alice训练样本及其排序的似然性，并与训练数据顺序相关性进行相关性分析；在观测设置，尝试1) 估计Bob文本与Alice训练样本片段的重叠似然性，2) 通过对训练过程的最后阶段（如1%数据）在重新洗牌数据后得到的不同版本对比来评估文本与模型的相关性。

Result: 查询设置中，40+个不同参数规模的Pythia/OLMo基模型的输出与训练数据顺序的相关性能给出极显著的证据，p值大多在1e-8数量级，只有少数六种情形除外；观测设置中，第二种方法（比较版本差异）能在仅几百令牌的情况下可靠区分Bob的文本；第一种观测方法需要大量文本（数十万级）才能达到高效检验，但无需再次训练。

Conclusion: 通过独立性检验与记忆性分析，能够在两种设定下对Bob是否在使用Alice的模型给出统计证据，且两种方法在检测能力与成本之间存在不同的权衡。

Abstract: Suppose Alice trains an open-weight language model and Bob uses a blackbox
derivative of Alice's model to produce text. Can Alice prove that Bob is using
her model, either by querying Bob's derivative model (query setting) or from
the text alone (observational setting)? We formulate this question as an
independence testing problem--in which the null hypothesis is that Bob's model
or text is independent of Alice's randomized training run--and investigate it
through the lens of palimpsestic memorization in language models: models are
more likely to memorize data seen later in training, so we can test whether Bob
is using Alice's model using test statistics that capture correlation between
Bob's model or text and the ordering of training examples in Alice's training
run. If Alice has randomly shuffled her training data, then any significant
correlation amounts to exactly quantifiable statistical evidence against the
null hypothesis, regardless of the composition of Alice's training data. In the
query setting, we directly estimate (via prompting) the likelihood Bob's model
gives to Alice's training examples and order; we correlate the likelihoods of
over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to
12B parameters with the base model's training data order, achieving a p-value
on the order of at most 1e-8 in all but six cases. In the observational
setting, we try two approaches based on estimating 1) the likelihood of Bob's
text overlapping with spans of Alice's training examples and 2) the likelihood
of Bob's text with respect to different versions of Alice's model we obtain by
repeating the last phase (e.g., 1%) of her training run on reshuffled data. The
second approach can reliably distinguish Bob's text from as little as a few
hundred tokens; the first does not involve any retraining but requires many
more tokens (several hundred thousand) to achieve high power.

</details>


### [159] [Transformers are almost optimal metalearners for linear classification](https://arxiv.org/abs/2510.19797)
*Roey Magen,Gal Vardi*

Main category: cs.LG

TL;DR: 本文首次理论性分析表明，通过梯度下降训练的简化Transformer在线性分类任务族上可以作为近似最优元学习器。对每个任务为高斯混合模型且均值位于共享的k维子空间，训练足够多的任务后，模型在新任务上仅用O(k/R^4)个上下文样本即可泛化，且与精确知道子空间的最优学习者几乎一致，显著优于仅依赖上下文数据的学习者（需要Ω(d/R^4)样本）。训练任务数和每任务样本量的界限与ambient维度d无关。


<details>
  <summary>Details</summary>
Motivation: 弥补现有关于Transformer在就地学习（ICL）方面的理论研究与正式元学习设定之间的断层，证明在一类明确的任务族上，变换器可作为高效的元学习器。

Method: 考察一个简化的Transformer架构，在梯度下降下训练。任务族设定为类条件高斯混合模型，均值落在R^d的一个共享的k维子空间内。经过足够多的任务训练后，证明该Transformer在新任务上可用极少的上下文样本泛化。

Result: 在新任务上只需O(k/R^4)个上下文样本即可泛化；该界限与知道子空间的最优学习者几乎等价，且显著优于仅有上下文数据的学习者，该学习者需要Ω(d/R^4)样本；训练任务数量和每任务样本数的界限与ambient维度d无关。

Conclusion: 理论上表明，训练为元学习器的Transformer在一定任务族上具有近似最优的元学习能力，且其对输入维度的依赖性极小，这为将ICL与元学习联系起来提供重要理论支撑。

Abstract: Transformers have demonstrated impressive in-context learning (ICL)
capabilities, raising the question of whether they can serve as metalearners
that adapt to new tasks using only a small number of in-context examples,
without any further training. While recent theoretical work has studied
transformers' ability to perform ICL, most of these analyses do not address the
formal metalearning setting, where the objective is to solve a collection of
related tasks more efficiently than would be possible by solving each task
individually. In this paper, we provide the first theoretical analysis showing
that a simplified transformer architecture trained via gradient descent can act
as a near-optimal metalearner in a linear classification setting. We consider a
natural family of tasks where each task corresponds to a class-conditional
Gaussian mixture model, with the mean vectors lying in a shared $k$-dimensional
subspace of $R^d$. After training on a sufficient number of such tasks, we show
that the transformer can generalize to a new task using only $O(k / R^4)$
in-context examples, where $R$ denotes the signal strength at test time. This
performance (almost) matches that of an optimal learner that knows exactly the
shared subspace and significantly outperforms any learner that only has access
to the in-context data, which requires $\Omega(d / R^4)$ examples to
generalize. Importantly, our bounds on the number of training tasks and
examples per task needed to achieve this result are independent of the ambient
dimension $d$.

</details>


### [160] [The Feasibility of Training Sovereign Language Models in the Global South: A Study of Brazil and Mexico](https://arxiv.org/abs/2510.19801)
*Sandra Malagon,Monica A. Ulloa Ruiz,Tatiana Elizabeth Sandoval Plaza,Gabriel Rafael Rosario Bolívar,Valentina García Mesa,Ivanna Alvarado Morales*

Main category: cs.LG

TL;DR: 在硬件、能源和财政约束下，评估巴西和墨西哥训练一个10万亿令牌级别语言模型的可行性，比较H100与A100、90天与150天的双轴设计，发现H100场景更具财政可行性，延长训练时间可缓解硬件约束。


<details>
  <summary>Details</summary>
Motivation: 全球南方国家在大规模语言模型训练上的结构性不对称推动探索本地化、可负担的算力治理和自主能力。

Method: 使用双轴设计变更加速器代数（NVIDIA H100 vs A100）和训练时长（90 vs 150天），估算计算需求、能耗、资本支出与监管兼容性，针对10万亿标记模型在巴西和墨西哥的情境。

Result: 所有配置均低于出口控制和电力基础设施阈值，但财政可行性取决于硬件效率；H100情景总成本8-14百万美元，A100为19-32百万美元；延长训练时间可作为政策杠杆降低硬件压力。

Conclusion: 该研究为AI算力治理与技术主权提供定位敏感的策略，帮助中等收入国家建立可持续且具战略意义的AI能力，而非追赶全球前沿。

Abstract: The rapid escalation of computational requirements for training large-scale
language models has reinforced structural asymmetries between high-capacity
jurisdictions and countries in the Global South. This paper examines the
technical and fiscal feasibility of sovereign-scale language model training in
Brazil and Mexico under conditions of constrained hardware access, energy
availability, and fiscal ceilings. Using a dual-axis design that varies
accelerator generation (NVIDIA H100 vs. A100) and training duration (90 vs. 150
days), we estimate compute demand, energy consumption, capital expenditures,
and regulatory compatibility for the training of a 10-trillion-token model. Our
findings show that while all configurations remain below export-control and
electrical infrastructure thresholds, fiscal viability is determined by
hardware efficiency. H100-based scenarios achieve training feasibility at a
total cost of 8-14 million USD, while A100 deployments require 19-32 million
USD due to higher energy and hardware demand. We argue that extending training
timelines should be treated as a policy lever to mitigate hardware constraints,
enabling the production of usable, auditable, and locally aligned models
without competing at the global frontier. This study contributes to the
discourse on AI compute governance and technological sovereignty by
highlighting context-sensitive strategies that allow middle-income countries to
establish sustainable and strategically sufficient AI capabilities.

</details>
