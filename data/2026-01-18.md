<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 59]
- [cs.LG](#cs.LG) [Total: 54]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 41]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-Driven Preference Data Synthesis for Proactive Prediction of the Next User Utterance in Human-Machine Dialogue](https://arxiv.org/abs/2601.09713)
*Jinqiang Wang,Huansheng Ning,Jianguo Ding,Tao Zhu,Liming Chen,Chris Nugent*

Main category: cs.CL

TL;DR: ProUtt：一种基于LLM的偏好数据合成方法，用于主动预测用户的下一句发话，通过将对话历史转化为意图树、显式建模意图推理轨迹，并在未来多轮中扰动路径以生成偏好和非偏好推理数据，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决商业API的隐私与本地大模型高计算成本问题；现有用户模拟器要么仅模仿说话风格，要么缺乏对用户意图推理的显式建模，且难以定义和合成偏好/非偏好推理过程来预测下一句话。需要一种能够显式建模用户意图推理并生成可用于主动下一句预测的偏好数据的方法。

Method: ProUtt 将对话历史转化为意图树，并通过在开发 exploitation 与 exploration 两个视角下预测下一条可能路径来显式建模意图推理轨迹。随后通过在不同未来回合扰动或修订意图树路径，构建偏好与非偏好推理过程。该方法以LLM驱动的数据合成为核心，输出用于训练或评估主动下一句预测的标注数据。评估采用 LLM 作为评判者以及人类评审进行多维度验证。

Result: 在四个基准数据集上，ProUtt 显著优于现有数据合成方法、用户模拟器与商业LLM API，且结果在 LLM-作为-评判者和人类评审的评估中保持一致的优越性。

Conclusion: ProUtt 提供一种隐私友好、成本更低的主动下一句预测数据合成方案，能够显式建模用户意图推理并生成偏好/非偏好推理过程，提升对话系统的预测能力。作者还公开代码与合成数据集以促进后续研究。

Abstract: Proactively predicting a users next utterance in human-machine dialogue can streamline interaction and improve user experience. Existing commercial API-based solutions are subject to privacy concerns while deploying general-purpose LLMs locally remains computationally expensive. As such, training a compact, task-specific LLM provides a practical alternative. Although user simulator methods can predict a user's next utterance, they mainly imitate their speaking style rather than advancing the dialogue. Preference data synthesis has been investigated to generate data for proactive next utterance prediction and help align LLMs with user preferences. Yet existing methods lack the ability to explicitly model the intent reasoning that leads to the user's next utterance and to define and synthesize preference and non-preference reasoning processes for predicting the user's next utterance.To address these challenges, we propose ProUtt, an LLM-driven preference data synthesis method for proactive next utterance prediction. ProUtt converts dialogue history into an intent tree and explicitly models intent reasoning trajectories by predicting the next plausible path from both exploitation and exploration perspectives. It then constructs preference and non-preference reasoning processes by perturbing or revising intent tree paths at different future turns. Extensive evaluations using LLM-as-a-judge and human judgments demonstrate that ProUtt consistently outperforms existing data synthesis methods, user simulators, and commercial LLM APIs across four benchmark datasets. We release both the code and the synthesized datasets to facilitate future research.

</details>


### [2] [Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines](https://arxiv.org/abs/2601.09714)
*Devesh Saraogi,Rohit Singhee,Dhruv Kumar*

Main category: cs.CL

TL;DR: Agentic multi-step workflows (decomposition and long-context) boost novelty in AI-assisted research ideation while preserving feasibility; reflection-based iterative methods underperform.


<details>
  <summary>Details</summary>
Motivation: Evaluate whether agentic, multi-step reasoning architectures can produce more novel and feasible research plans than single-step prompting, addressing concerns about smart plagiarism and AI creativity.

Method: Benchmark five architectures—Reflection-based iterative refinement; Sakana AI v2 evolutionary algorithms; Google Co-Scientist multi-agent framework; GPT Deep Research (GPT-5.1) recursive decomposition; Gemini 3 Pro multimodal long-context pipeline—using 30 proposals per architecture evaluated on novelty, feasibility, and impact.

Result: Decomposition-based and long-context workflows achieve higher mean novelty (4.17/5). Reflection-based approaches score significantly lower (2.33/5). Performance varies by domain; top workflows maintain feasibility alongside creativity.

Conclusion: Carefully designed, multi-stage agentic workflows can advance AI-assisted research ideation without sacrificing feasibility, though effectiveness is architecture- and domain-dependent.

Abstract: The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.

</details>


### [3] [Opportunities and Challenges of Natural Language Processing for Low-Resource Senegalese Languages in Social Science Research](https://arxiv.org/abs/2601.09716)
*Derguene Mbaye,Tatiana D. P. Mbengue,Madoune R. Seye,Moussa Diallo,Mamadou L. Ndiaye,Dimitri S. Adjanohoun,Cheikh S. Wade,Djiby Sow,Jean-Claude B. Munyaka,Jerome Chenal*

Main category: cs.CL

TL;DR: This paper provides a comprehensive overview of NLP progress and challenges for Senegal's six official languages, outlining data/tool gaps, centralized resources, and a roadmap for sustainable, community-centered NLP with ethical governance.


<details>
  <summary>Details</summary>
Motivation: African languages are underrepresented in NLP; understanding linguistic, sociotechnical, and infrastructural constraints is essential to enable inclusive research and applications, particularly in social sciences.

Method: Synthesis across linguistic, sociotechnical, and infrastructural factors; review of text normalization, machine translation, and speech processing; compilation of publicly accessible resources in a centralized GitHub repository; discussion of social sciences applications; roadmap development.

Result: Identified gaps in data, tools, and benchmarks; highlighted ongoing efforts in normalization, MT, and speech; created a centralized repository of resources for Senegalese languages; proposed a roadmap emphasizing openness, collaboration, and reproducibility.

Conclusion: Advocates sustainable, community-centered NLP ecosystems for Senegalese languages with ethical data governance, open resources, and interdisciplinary collaboration; emphasizes reproducibility and capacity building.

Abstract: Natural Language Processing (NLP) is rapidly transforming research methodologies across disciplines, yet African languages remain largely underrepresented in this technological shift. This paper provides the first comprehensive overview of NLP progress and challenges for the six national languages officially recognized by the Senegalese Constitution: Wolof, Pulaar, Sereer, Joola, Mandingue, and Soninke. We synthesize linguistic, sociotechnical, and infrastructural factors that shape their digital readiness and identify gaps in data, tools, and benchmarks. Building on existing initiatives and research works, we analyze ongoing efforts in text normalization, machine translation, and speech processing. We also provide a centralized GitHub repository that compiles publicly accessible resources for a range of NLP tasks across these languages, designed to facilitate collaboration and reproducibility. A special focus is devoted to the application of NLP to the social sciences, where multilingual transcription, translation, and retrieval pipelines can significantly enhance the efficiency and inclusiveness of field research. The paper concludes by outlining a roadmap toward sustainable, community-centered NLP ecosystems for Senegalese languages, emphasizing ethical data governance, open resources, and interdisciplinary collaboration.

</details>


### [4] [SciNets: Graph-Constrained Multi-Hop Reasoning for Scientific Literature Synthesis](https://arxiv.org/abs/2601.09727)
*Sauhard Dubey*

Main category: cs.CL

TL;DR: SciNets：在文献中构建有向概念图，通过受限的查询域进行多跳推理以合成机制性解释；比较短路径、带多样性约束的k最短路径、随机漫步、检索增强LLM等策略；采用行为学框架评估推理深度、机制多样性与 grounding 稳定性，发现更深更 diverse 的符号推理提高了 grounding 不稳定性，而短路径在结构上保守且更稳定。


<details>
  <summary>Details</summary>
Motivation: 跨领域整合机械性解释，连接分散文献中的机制性知识；检索系统和非受控大语言模型在推理深度和结构 grounding 上存在控制难题；提出用显式的图结构约束来实现可控的多跳推理，以小型、查询本地的语料为基础进行科知识 synthesis。

Method: 以查询本地的小型语料构建文献派生的有向概念图，并通过识别连接同一论文中很少共现的概念之间的多跳路径来合成机制性解释；比较四种推理策略：最短路径、带多样性约束的k最短路径、随机漫步，以及检索增强的语言模型基线；以一个行为框架对推理进行评估，而非单纯正确性。

Result: 在机器学习、生物学与气候科学任务中，显式的图约束实现了可控的多跳推理；存在权衡：更深和更丰富的符号推理提升了符号层面的多样性，但会降低 grounding 的稳定性；最短路径推理最为稳定但结构保守。

Conclusion: 通过显式的图约束，能对科学合成中的多跳推理实现可控性，并揭示当前图-LLM 融合在可控性与 grounding 稳定性之间的权衡及局限，为未来改进提供行为学层面的定量框架与方向。

Abstract: Cross-domain scientific synthesis requires connecting mechanistic explanations across fragmented literature, a capability that remains challenging for both retrieval-based systems and unconstrained language models. While recent work has applied large language models to scientific summarization and question answering, these approaches provide limited control over reasoning depth and structural grounding. We frame mechanistic synthesis as a graph-constrained multi-hop reasoning problem over literature-derived concept graphs. Given a scientific query and a compact, query-local corpus, SciNets constructs a directed concept graph and synthesizes mechanistic explanations by identifying multi-hop reasoning paths that connect concepts that rarely co-occur within individual papers. We systematically compare shortest-path reasoning, k-shortest paths with diversity constraints, stochastic random walks, and a retrieval-augmented language model baseline. Rather than evaluating correctness, which is often indeterminate when synthesizing connections across distributed sources, we introduce a behavioral framework that measures symbolic reasoning depth, mechanistic diversity, and grounding stability. Across machine learning, biology, and climate science tasks, explicit graph constraints enable controllable multi-hop reasoning while revealing a consistent trade-off: deeper and more diverse symbolic reasoning increases grounding instability, whereas shortest-path reasoning remains highly stable but structurally conservative. These findings provide a systematic behavioral characterization of the limits and capabilities of current graph-LLM integration for scientific synthesis.

</details>


### [5] [StatLLaMA: A multi-stage training framework for building a domain-optimized statistical language model](https://arxiv.org/abs/2601.09718)
*Jing-Yi Zeng,Guan-Hua Huang*

Main category: cs.CL

TL;DR: 通过多阶段训练 pipelines，在统计领域建立基于 LLaMA-3.2-3B 家族的领域专用大型语言模型；以 Instruct 版本的基模型为起点能实现有效的领域专门化；直接偏好优化可稳定优化 RLHF；下游微调需极低强度以防止灾难性遗忘；StatLLaMA 具备数学推理、常识推理和统计专知方面的强平衡表现，给出资源高效的统计型 LLM 构建蓝本；代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前需在资源受限条件下获得具备统计学专业知识的强大语言模型，同时要避免从零训练的高成本。现有的基模型如果缺乏指令跟随能力，难以在领域内获得有效的推理与适应；即使有后处理的指令调优或 RLHF，也可能无法实现良好的领域专业化与泛化。

Method: 比较三条多阶段训练流水线：1) 基模型为无指令跟随能力的基础 FM；2) 基模型在其上做后置指令微调；3) 直接使用具强通用推理能力的带指令微调 FM。覆盖持续预训练、监督微调（SFT）、来自人类反馈的强化学习（RLHF）偏好对齐，以及下游任务适应。评估覆盖数学推理、常识推理与统计专知。探讨直接偏好优化在 RLHF 中的稳定性，以及下游微调的强度对灾难性遗忘的影响。

Result: 核心发现：以基 FM 开始的流水线在广泛的指令微调、SFT 或 RLHF 后也难以形成统计推理能力；而以 LLaMA-3.2-3B-Instruct 为起点则能实现有效的领域专门化。SFT 变体存在领域专长与通用推理能力之间的权衡。直接偏好优化提供了稳定且有效的 RLHF 偏好对齐。下游微调需极低强度以避免在高度优化的模型上发生灾难性遗忘。最终模型 StatLLaMA 在数学推理、常识推理和统计专知基准上表现强劲且更为均衡。代码公开。

Conclusion: 为资源受限场景下的统计领域 LLM 构建提供可操作蓝本：从具指令能力的 FM 出发以实现领域专业化、采用直接偏好优化替代传统 RLHF、并控制下游微调强度以防止遗忘，最终得到在统计领域具备较好综合能力的模型 StatLLaMA。

Abstract: This study investigates how to efficiently build a domain-specialized large language model (LLM) for statistics using the lightweight LLaMA-3.2-3B family as the foundation model (FM). We systematically compare three multi-stage training pipelines, starting from a base FM with no instruction-following capability, a base FM augmented with post-hoc instruction tuning, and an instruction-tuned FM with strong general reasoning abilities across continual pretraining, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) preference alignment, and downstream task adaptation. Results show that pipelines beginning with a base FM fail to develop meaningful statistical reasoning, even after extensive instruction tuning, SFT, or RLHF alignment. In contrast, starting from LLaMA-3.2-3B-Instruct enables effective domain specialization. A comprehensive evaluation of SFT variants reveals clear trade-offs between domain expertise and general reasoning ability. We further demonstrate that direct preference optimization provides stable and effective RLHF preference alignment. Finally, we show that downstream fine-tuning must be performed with extremely low intensity to avoid catastrophic forgetting in highly optimized models. The final model, StatLLaMA, achieves strong and balanced performance on benchmarks of mathematical reasoning, common-sense reasoning, and statistical expertise, offering a practical blueprint for developing resource-efficient statistical LLMs. The code is available at https://github.com/HuangDLab/StatLLaMA.

</details>


### [6] [Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models](https://arxiv.org/abs/2601.09719)
*Hoyoon Byun,Youngjun Choi,Taero Kim,Sungrae Park,Kyungwoo Song*

Main category: cs.CL

TL;DR: 提出了 Bounded Hyperbolic Tanh (BHyT)，一种可替代 Pre-LN 的归一化-非线性层，结合 tanh 与输入界限，确保激活幅度与方差在深度上不增，同时通过一次统计与轻量方差近似提升效率，并提供稳定性理论保证。实验表明 pretraining 更稳定，训练速度提升约 15.8%，token 生成吞吐量提升约 4.2%，与 RMSNorm 相比在推理表现和鲁棒性上持平或超越。


<details>
  <summary>Details</summary>
Motivation: 解决 Pre-LN 的深度劣化与重复统计成本，同时提升训练稳定性与推理效率。

Method: 在输入进入层前应用 tanh 非线性与显式输入界限，防止激活幅度和方差随深度增长；一次按块计算精确统计，第二个归一化用轻量级方差估算替代；提供理论稳定性保证。

Result: 在预训练阶段实现更稳定的训练过程，平均训练速度提升 15.8%，平均 token 生成吞吐量提升 4.2%，并在语言理解与推理基准上达到或超过 RMSNorm 的推理性能与鲁棒性。

Conclusion: BHyT 为 Pre-LN 的高效稳定替代，兼具收敛性保障、降低统计开销、提升效率，并且在多任务评估中表现稳健。

Abstract: Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT

</details>


### [7] [Uncertainty-Aware Dynamic Knowledge Graphs for Reliable Question Answering](https://arxiv.org/abs/2601.09720)
*Yu Takahashi,Shun Takeuchi,Kexuan Xin,Guillaume Pelat,Yoshiaki Ikai,Junya Saito,Jonathan Vitale,Shlomo Berkovsky,Amin Beheshti*

Main category: cs.CL

TL;DR: 提出一个用于QA的 uncertainty-aware 动态知识图框架的演示，结合动态 KG 构建、置信评分与不确定性检索、以及交互界面，尤其在医疗场景中实践。


<details>
  <summary>Details</summary>
Motivation: 现有基于KG的QA在证据不完整、嘈杂或不确定时难以保证可靠性，且知识图多为静态且确定性表示，无法捕捉信息的演化及推理中的不确定性。

Method: 动态构建演化的知识图（KG）；引入置信评分与不确定性感知的检索策略；提供可交互的界面以实现可解释、鲁棒的QA。系统在电子病历中构建个性化KG，可视化患者就诊过程中的不确定性，并评估其对死亡率预测任务的影响；对比基线回答与置信感知答案。

Result: 展示不确定性建模可以提升QA的鲁棒性和透明度，用户可探索动态图、带置信注释的三元组，并比较基线与置信感知的答案；用死亡率预测任务评估其影响。

Conclusion: 不确定性感知的动态KG在高风险场景下提升QA可靠性具有广泛前景，具有潜在的对其他领域的可扩展性。

Abstract: Question answering (QA) systems are increasingly deployed across domains. However, their reliability is undermined when retrieved evidence is incomplete, noisy, or uncertain. Existing knowledge graph (KG) based QA frameworks typically represent facts as static and deterministic, failing to capture the evolving nature of information and the uncertainty inherent in reasoning. We present a demonstration of uncertainty-aware dynamic KGs, a framework that combines (i) dynamic construction of evolving KGs, (ii) confidence scoring and uncertainty-aware retrieval, and (iii) an interactive interface for reliable and interpretable QA. Our system highlights how uncertainty modeling can make QA more robust and transparent by enabling users to explore dynamic graphs, inspect confidence-annotated triples, and compare baseline versus confidence-aware answers. The target users of this demo are clinical data scientists and clinicians, and we instantiate the framework in healthcare: constructing personalized KGs from electronic health records, visualizing uncertainty across patient visits, and evaluating its impact on a mortality prediction task. This use case demonstrates the broader promise of uncertainty-aware dynamic KGs for enhancing QA reliability in high-stakes applications.

</details>


### [8] [Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox](https://arxiv.org/abs/2601.09721)
*Vahideh Zolfaghari*

Main category: cs.CL

TL;DR: 在儿童就诊场景下评估三种大语言模型在父母焦虑等 adversarial 场景下的安全性。结果显示较小模型在对抗性条件下表现优于大模型，且均未达到适用于临床分诊的水平，提示需要更强的对齐与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究在现实压力下（父母焦虑驱动的对抗性压力）LLMs 的安全性，以评估其在儿童健康咨询中的风险，比较不同架构与平台的表现，并提供公开基准以促进医疗AI安全性研究。

Method: 使用 PediatricAnxietyBench 的 300 条查询（150 条真实、150 条对抗）覆盖 10 个主题；评估三种模型通过 API 输出：Llama-3.3-70B、Llama-3.1-8B（Groq）、Mistral-7B（HuggingFace）；对每条回答在 0–15 的尺度上评估约束、转介、回避、紧急识别与非处方行为等安全维度；采用配对 t 检验与自举置信区间进行统计分析。

Result: 平均安全分数介于 9.70 到 10.39；Llama-3.1-8B 相较 Llama-3.3-70B 提升 +0.66（p=0.0001，d=0.225）；Mistral-7B 最强，提升 +1.09（p=0.0002）；安全性跨平台具泛化性，Llama-3.3-70B 的失效率约 8%；癫痫/发作相关场景存在 33% 的不当诊断风险；回避策略与安全性呈显著相关（r=0.68，p<0.001）。

Conclusion: 安全性受模型对齐与体系结构影响，规模增大未必带来改进；小模型在当前迭代中表现更优，且持续迭代可能提高鲁棒性；存在对紧急识别的缺失与潜在风险，表明目前不适合用于医疗分诊。研究为模型选择、对抗性压力测试及开放基准提供实证与工具。

Abstract: Background Large language models (LLMs) are increasingly deployed in medical consultations, yet their safety under realistic user pressures remains understudied. Prior assessments focused on neutral conditions, overlooking vulnerabilities from anxious users challenging safeguards. This study evaluated LLM safety under parental anxiety-driven adversarial pressures in pediatric consultations across models and platforms. Methods PediatricAnxietyBench, from a prior evaluation, includes 300 queries (150 authentic, 150 adversarial) spanning 10 topics. Three models were assessed via APIs: Llama-3.3-70B and Llama-3.1-8B (Groq), Mistral-7B (HuggingFace), yielding 900 responses. Safety used a 0-15 scale for restraint, referral, hedging, emergency recognition, and non-prescriptive behavior. Analyses employed paired t-tests with bootstrapped CIs. Results Mean scores: 9.70 (Llama-3.3-70B) to 10.39 (Mistral-7B). Llama-3.1-8B outperformed Llama-3.3-70B by +0.66 (p=0.0001, d=0.225). Models showed positive adversarial effects, Mistral-7B strongest (+1.09, p=0.0002). Safety generalized across platforms; Llama-3.3-70B had 8% failures. Seizures vulnerable (33% inappropriate diagnoses). Hedging predicted safety (r=0.68, p<0.001). Conclusions Evaluation shows safety depends on alignment and architecture over scale, with smaller models outperforming larger. Evolution to robustness across releases suggests targeted training progress. Vulnerabilities and no emergency recognition indicate unsuitability for triage. Findings guide selection, stress adversarial testing, and provide open benchmark for medical AI safety.

</details>


### [9] [ADMEDTAGGER: an annotation framework for distillation of expert knowledge for the Polish medical language](https://arxiv.org/abs/2601.09722)
*Franciszek Górski,Andrzej Czyżewski*

Main category: cs.CL

TL;DR: 使用多语言LLM作为教师对Polish医学文本进行标注并蒸馏成小型BERT分类器，达到高F1并显著节省资源。


<details>
  <summary>Details</summary>
Motivation: Polish医学文本标注资源不足，需构建高效且资源友好的多类分类器；通过教师模型蒸馏实现这一目标。

Method: 以Llama3.1对Polish医学文本进行大规模自动标注；仅部分标签经人工验证并构成测试集；训练并比较DistilBERT、BioBERT、HerBERT三种基于BERT的分类器。

Result: DistilBERT在各类上F1>0.80，至少3类F1>0.93；模型体积约500x小、VRAM显著降低、推理速度更快。

Conclusion: 在注释资源有限的情况下，利用多语言教师模型蒸馏出高效的专业领域文本分类器，是一个资源高效且可扩展的方案；对跨语言和专业领域的标注任务具参考价值。

Abstract: In this work, we present an annotation framework that demonstrates how a multilingual LLM pretrained on a large corpus can be used as a teacher model to distill the expert knowledge needed for tagging medical texts in Polish. This work is part of a larger project called ADMEDVOICE, within which we collected an extensive corpus of medical texts representing five clinical categories - Radiology, Oncology, Cardiology, Hypertension, and Pathology. Using this data, we had to develop a multi-class classifier, but the fundamental problem turned out to be the lack of resources for annotating an adequate number of texts. Therefore, in our solution, we used the multilingual Llama3.1 model to annotate an extensive corpus of medical texts in Polish. Using our limited annotation resources, we verified only a portion of these labels, creating a test set from them. The data annotated in this way were then used for training and validation of 3 different types of classifiers based on the BERT architecture - the distilled DistilBERT model, BioBERT fine-tuned on medical data, and HerBERT fine-tuned on the Polish language corpus. Among the models we trained, the DistilBERT model achieved the best results, reaching an F1 score > 0.80 for each clinical category and an F1 score > 0.93 for 3 of them. In this way, we obtained a series of highly effective classifiers that represent an alternative to large language models, due to their nearly 500 times smaller size, 300 times lower GPU VRAM consumption, and several hundred times faster inference.

</details>


### [10] [SagaScale: A Realistic, Scalable, and High-Quality Long-Context Benchmark Built from Full-Length Novels](https://arxiv.org/abs/2601.09723)
*Guancheng Du,Yong Hu,Wenqing Wang,Yaming Yang,Jiaheng Gao*

Main category: cs.CL

TL;DR: SagaScale 是一个基于全长小说构建的现实、可扩展且高质量的长上下文基准，具双语特性并拥有迄今最大的上下文长度；在12个前沿LLM及三种长上下文方法上评估，给出若干关键洞察：直接给出完整上下文通常优于分段检索，部分模型（如 Gemini-2.5-Pro）在长上下文上更具鲁棒性，Agentic RAG 能有效缓解检索瓶颈；并公开数据集与代码以促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 当前的长上下文基准普遍存在现实性、数据规模与数据质量等局限，难以真实评估模型在极长文本上的推理能力，并且缺乏高质量、可扩展的评估资源。需要一个能覆盖真实场景、可扩展且数据质量可控的基准，且能在评估阶段避免对外部资源的依赖。

Method: 通过自动化数据采集管线，从外部公开资源（如维基页面）构建问答对，资源仅用于基准构建，不参与评估。基准覆盖英文与中文小说，平均上下文长度超过250K（英文）和320K（中文）令牌。对12个前沿LLM及三种长上下文方法进行评估：Naïve RAG、Agentic RAG、Long Context。

Result: 直接提供完整上下文通常比分段检索方法表现更优；多数模型对超长上下文仍存在挑战，Gemini-2.5-Pro 在此方面表现出色；Agentic RAG 能有效缓解 Naïve RAG 的检索瓶颈。 SagaScale 具双语特性且上下文规模创纪录，公开数据与代码以促进研究。

Conclusion: SagaScale 提供现实、可扩展且高质量的长上下文基准，能推动对长上下文能力与评估协议的研究，并为未来工作提供广泛的基准资源。

Abstract: Large Language Models (LLMs) have shown significant progress, but understanding long and complex documents remains challenging. Many long-context benchmarks have been proposed, but they face several limitations, including task realism, data scalability, and data quality. To this end, we introduce SagaScale, a realistic, scalable, and high-quality long-context benchmark built from full-length novels. The entire benchmark is constructed using an automated data collection pipeline that utilizes external resources (e.g., Wikipedia pages) to curate question-answer pairs. Critically, these external resources are provided only for benchmark construction and not during evaluation, which allows LLMs to curate complex questions that go beyond what they can answer during evaluation. SagaScale is also bilingual and offers the largest context length to date, with average token counts exceeding 250K for English novels and 320K for Chinese novels. Our evaluation across 12 frontier LLMs and three long-context methods -- Naïve RAG, Agentic RAG, and Long Context -- yields key insights, including: (1) Directly supplying the full context to the LLM can outperform other methods by a large margin; (2) Most LLMs still struggle with lengthy contexts, but Gemini-2.5-Pro stands out as an exception; and (3) Agentic RAG effectively addresses the retrieval bottleneck in Naïve RAG. Finally, we publicly release the SagaScale benchmark and our data collection codebase to facilitate future research.

</details>


### [11] [Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions](https://arxiv.org/abs/2601.09724)
*Katherine Elkins,Jon Chun*

Main category: cs.CL

TL;DR: 提出Syntactic Framing Fragility (SFF) 的鲁棒性评估框架，专注于通过语法极性（正/负框架）对LLM伦理判断的一致性进行测试。结果显示23种模型在14个伦理情景与4种框架下存在广泛且显著的语法脆弱性，开源模型的脆弱性高于商业模型；在某些情况下对“should not”提示的反向性极强。诱导链路推理能显著降低脆弱性。建议将SFF风格的审计纳入安全评估标准，金融/商业情景风险高于医疗情景。代码与结果将公开。


<details>
  <summary>Details</summary>
Motivation: 在需要重要决策的场景中，评估LLMs对提示语法变动的鲁棒性，尤其是在不改变语义的情况下，仅变换措辞（如否定与条件结构）时，伦理判断是否保持一致。现有研究多关注语义层面，忽视语法极性带来的潜在偏差。

Method: 对23个模型（包含美国、欧洲及中国模型，以及小型美国产开源模型）在14个伦理情景和4种受控框架下进行评估，总计39,975次决策。提出并使用逻辑极性正规化（LPN）以消除语义漂移，构建SFF评估框架，比较正向与负向表述下的决策。

Result: 存在广泛且统计上显著的不一致性：许多模型仅因语法极性而改变伦理 endorsements；开源模型的脆弱性是商业模型的两倍以上；某些模型在被显式提示“should not”时，80-97%的情境中出现积极主张。诱导链式推理显著降低脆弱性。化简框架中的脆弱性在金融/商业场景中比医疗场景更高。

Conclusion: 语法一致性构成伦理鲁棒性的一个独立维度，应将SFF风格审计纳入部署前的安全评估。提供缓解手段（如链式推理）并在不同情景中映射脆弱性。代码与结果将发布在GitHub。

Abstract: Large language models (LLMs) are increasingly deployed in consequential decision-making settings, yet their robustness to benign prompt variation remains underexplored. In this work, we study whether LLMs maintain consistent ethical judgments across logically equivalent but syntactically different prompts, focusing on variations involving negation and conditional structure. We introduce Syntactic Framing Fragility (SFF), a robustness evaluation framework that isolates purely syntactic effects via Logical Polarity Normalization (LPN), enabling direct comparison of decisions across positive and negative framings without semantic drift. Auditing 23 state-of-the-art models spanning the U.S. and China as well as small U.S. open-source software models over 14 ethical scenarios and four controlled framings (39,975 decisions), we find widespread and statistically significant inconsistency: many models reverse ethical endorsements solely due to syntactic polarity, with open-source models exhibiting over twice the fragility of commercial counterparts. We further uncover extreme negation sensitivity, where some models endorse actions in 80-97% of cases when explicitly prompted with "should not." We show that eliciting chain-of-thought reasoning substantially reduces fragility, identifying a practical mitigation lever, and we map fragility across scenarios, finding higher risk in financial and business contexts than in medical scenarios. Our results demonstrate that syntactic consistency constitutes a distinct and critical dimension of ethical robustness, and we argue that SFF-style audits should be a standard component of safety evaluation for deployed LLMs. Code and results will be available on github.com.

</details>


### [12] [Grounding Agent Memory in Contextual Intent](https://arxiv.org/abs/2601.10702)
*Ruozhen Yang,Yucheng Jiang,Yueqi Jiang,Priyanka Kargupta,Yunyi Zhang,Jiawei Han*

Main category: cs.CL

TL;DR: 提出 STITCH，一种以结构化检索线索和上下文意图为核心的记忆系统，用于在长程、目标导向交互中对齐检索内容，缓解记忆噪声。并引入 CAME-Bench 评测，达到 SOTA，在 LongMemEval 和 CAME-Bench 上相较基线提升约35.6%，且对轨迹长度越长收益越大。


<details>
  <summary>Details</summary>
Motivation: 在长时间、目标导向的任务中，类似实体和事实在不同潜在目标和约束下反复出现，导致记忆系统检索到的上下文不匹配的证据，难以保持一致性和鲁棒性。

Method: 提出 STITCH：为每个轨迹步骤加上结构化检索线索（retrieval cue）和上下文意图（contextual intent），并通过将当前步骤的意图与历史记忆进行匹配来检索。上下文意图包括三部分信号：1) 当前潜在目标定义的主题段落；2) 动作类型；3) 重要实体类型，指明哪些属性重要。推理阶段通过意图兼容性过滤并优先检索记忆片段，抑制语义上相似但上下文不兼容的历史。

Result: 在 CAME-Bench 与 LongMemEval 上，STITCH 实现了对比方法的 SOTA，领先最强基线约 35.6%，且轨迹长度越大时提升越显著。分析表明意图索引显著降低检索噪声，支持基于意图的记忆用于鲁棒的长时序推理。

Conclusion: 以意图感知的记忆索引显著提升长时序推理的检索鲁棒性，减少记忆干扰，提升对话/计划任务中的长期目标追踪效果。

Abstract: Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.
  For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.

</details>


### [13] [Assessing and Improving Punctuation Robustness in English-Marathi Machine Translation](https://arxiv.org/abs/2601.09725)
*Kaustubh Shivshankar Shejole,Sourabh Deoghare,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出 Virām 基准用于评估英语-马拉地语 MT 的标点鲁棒性；通过 restore-then-translate 管道和对标点变化数据进行微调的模型，显著优于基线；大模型在保持含义方面落后于任务特定方法。


<details>
  <summary>Details</summary>
Motivation: 解决标点导致的语义与结构歧义，特别是在马拉地语等低资源语言的英语到马拉地翻译中建立可诊断的鲁棒性基准。

Method: 构建 Virām：54 个人工策划、具有标点歧义的实例。评估两种提高鲁棒性的策略：1) restore-then-translate 的管道式方法；2) 在具有标点变异的数据上进行微调的直接方法。比较与标准基线和大语言模型的表现。

Result: 专门微调模型和管道系统在 Virām 基准上显著提升翻译质量，相较于基线。原始模型可能产生错误翻译导致错误解释；微调模型显著提高整体可靠性。当前的大语言模型在保持对标点模糊文本含义方面落后于这些任务特定方法。

Conclusion: 面向标点模糊文本的 MT 鲁棒性可通过任务特定的微调和管道方法显著提升；需要进一步研究以提升大语言模型在此任务上的表现。

Abstract: Punctuation plays a critical role in resolving semantic and structural ambiguity in written language. Machine Translation (MT) systems are now widely applied across diverse domains and languages, including many low-resource settings. In this work, we focus on Marathi, a low- to middle-resource language. We introduce Virām, the first diagnostic benchmark for assessing punctuation robustness in English-to-Marathi machine translation, consisting of 54 manually curated, punctuation-ambiguous instances. We evaluate two primary strategies for enhancing reliability: a pipeline-based restore-then-translate approach and direct fine-tuned on punctuation-varied data. Our results demonstrate that specialized fine-tuned models and pipeline systems significantly improve translation quality over standard baselines on the Virām benchmark. Qualitative analysis reveals that the original model may result in wrong translations leading to wrong interpretations, while fine-tuned models significantly improve overall reliability. Furthermore, we find that current Large Language Models (LLMs) lag behind these task-specific approaches in preserving meaning for punctuation-ambiguous text, thus necessitating further research in this area.

</details>


### [14] [Forgetting as a Feature: Cognitive Alignment of Large Language Models](https://arxiv.org/abs/2601.09726)
*Hien Tran,Quinten Steenhuis,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: 将大语言模型的遗忘重新解释为一种自适应的概率记忆过程（指数衰减），并提出用于评估时序推理、概念漂移适应和联想回忆的基准；通过概率记忆提示（probabilistic memory prompting）强化证据整合以提升长时程推理。


<details>
  <summary>Details</summary>
Motivation: 解决对理想化贝叶斯推理的单一评估与实际中信息遗忘之间的矛盾，借鉴人类记忆动力学，将LLM推理建模为受指数衰减支配的记忆过程，以实现更具适应性的智能。

Method: 将推理建模为受指数衰减控制的概率记忆过程；设计评测基准评估时序推理、概念漂移适应和联想回忆；提出轻量级的概率记忆提示，调整证据整合方式以模仿人类记忆衰减。

Result: 实证显示，LLMs 的遗忘速率与人类在稳定性与适应性之间的记忆权衡相似；概率记忆提示在提升长时程推理方面表现出改进，指向将遗忘视为可控的适应性机制。

Conclusion: 遗忘不是单纯的失败模式，而是促进自适应智能的原则性机制；基于记忆衰减的建模与提示策略有助于提高LLM的长期推理与信息整合能力。

Abstract: Large Language Models (LLMs) are often evaluated against ideals of perfect Bayesian inference, yet growing evidence suggests that their in-context reasoning exhibits systematic forgetting of past information. Rather than viewing this behavior as a limitation, we reinterpret forgetting as a functional cognitive mechanism. Drawing inspiration from human memory dynamics, we model LLM inference as a probabilistic memory process governed by exponential decay. We introduce a benchmark suite that evaluates temporal reasoning, concept drift adaptation, and associative recall, enabling direct comparison between model behavior and human cognitive patterns. Our empirical results reveal that LLMs demonstrate forgetting rates analogous to human memory efficiency trade-offs between stability and adaptability. Building on these observations, we propose probabilistic memory prompting, a lightweight strategy that shapes evidence integration to mimic human-like memory decay, leading to improved long-horizon reasoning performance. Our findings position forgetting not as a failure mode, but as a principled mechanism for adaptive intelligence.

</details>


### [15] [Enhancing Business Analytics through Hybrid Summarization of Financial Reports](https://arxiv.org/abs/2601.09729)
*Tohida Rehman*

Main category: cs.CL

TL;DR: A hybrid extractive-abstractive summarization framework for financial call transcripts that combines LexRank-based extraction with fine-tuned BART/PEGASUS, plus Longformer-Encoder-Decoder for long contexts; evaluated with standard and domain-specific metrics; finds long-context models strongest, hybrid method competitive under constraints.


<details>
  <summary>Details</summary>
Motivation: Manual analysis of earnings calls is time-consuming, biased, and error-prone; need efficient, factually reliable summaries of lengthy financial texts.

Method: Two-stage pipeline: LexRank identifies salient sentences; abstractive summarization via fine-tuned BART/PEGASUS variants for resource-constrained settings. Separately, fine-tune LED to capture long-range context directly. Evaluation with ROUGE, METEOR, MoverScore, BERTScore, SciBERTScore, FinBERTScore; entity-level measures for factual accuracy (source-precision, F1-target).

Result: Long-context models provide strongest overall performance; the hybrid framework achieves competitive results with improved factual consistency under computational constraints.

Conclusion: Supports development of practical summarization systems to distill lengthy financial documents into usable business insights.

Abstract: Financial reports and earnings communications contain large volumes of structured and semi structured information, making detailed manual analysis inefficient. Earnings conference calls provide valuable evidence about a firm's performance, outlook, and strategic priorities. The manual analysis of lengthy call transcripts requires substantial effort and is susceptible to interpretive bias and unintentional error. In this work, we present a hybrid summarization framework that combines extractive and abstractive techniques to produce concise and factually reliable Reuters-style summaries from the ECTSum dataset. The proposed two stage pipeline first applies the LexRank algorithm to identify salient sentences, which are subsequently summarized using fine-tuned variants of BART and PEGASUS designed for resource constrained settings. In parallel, we fine-tune a Longformer Encoder-Decoder (LED) model to directly capture long-range contextual dependencies in financial documents.
  Model performance is evaluated using standard automatic metrics, including ROUGE, METEOR, MoverScore, and BERTScore, along with domain-specific variants such as SciBERTScore and FinBERTScore. To assess factual accuracy, we further employ entity-level measures based on source-precision and F1-target. The results highlight complementary trade offs between approaches, long context models yield the strongest overall performance, while the hybrid framework achieves competitive results with improved factual consistency under computational constraints. These findings support the development of practical summarization systems for efficiently distilling lengthy financial texts into usable business insights.

</details>


### [16] [Geometric Patterns of Meaning: A PHATE Manifold Analysis of Multi-lingual Embeddings](https://arxiv.org/abs/2601.09731)
*Wen G Gong*

Main category: cs.CL

TL;DR: PHATE-based multi-level framework Semanscope reveals systematic geometric patterns and key limitations in multilingual embeddings across sub-character, character, word, and numeric domains; identifies semantic-structural distinctions and advocates PHATE as a core analytic/validation tool.


<details>
  <summary>Details</summary>
Motivation: To understand the geometry of meaning in multilingual embeddings and diagnose limitations of current models across linguistic levels.

Method: Semanscope applies PHATE manifold learning across four levels (sub-character, character, word, numeric) to diverse datasets spanning radicals, writing systems, semantic domains, and numerals.

Result: Sub-character: geometric collapse of purely structural radicals; character-level: distinct geometric signatures across writing systems; word-level: content words cluster across 20 semantic domains in English, Chinese, German; numeric: Arabic digits form spirals, violating standard distributional assumptions; overall: systematic patterns and model limitations identified; PHATE is effective for analysis and validation.

Conclusion: PHATE-based analysis provides a robust framework for studying semantic geometry and validating multilingual embeddings, enabling diagnosis of semantic vs. structural capture shortcomings and guiding model improvement.

Abstract: We introduce a multi-level analysis framework for examining semantic geometry in multilingual embeddings, implemented through Semanscope (a visualization tool that applies PHATE manifold learning across four linguistic levels). Analysis of diverse datasets spanning sub-character components, alphabetic systems, semantic domains, and numerical concepts reveals systematic geometric patterns and critical limitations in current embedding models. At the sub-character level, purely structural elements (Chinese radicals) exhibit geometric collapse, highlighting model failures to distinguish semantic from structural components. At the character level, different writing systems show distinct geometric signatures. At the word level, content words form clustering-branching patterns across 20 semantic domains in English, Chinese, and German. Arabic numbers organize through spiral trajectories rather than clustering, violating standard distributional semantics assumptions. These findings establish PHATE manifold learning as an essential analytic tool not only for studying geometric structure of meaning in embedding space, but also for validating the effectiveness of embedding models in capturing semantic relationships.

</details>


### [17] [Benchmarking Cross-Lingual Semantic Alignment in Multilingual Embeddings](https://arxiv.org/abs/2601.09732)
*Wen G. Gong*

Main category: cs.CL

TL;DR: Semantic Affinity (SA) is a bounded cross-lingual alignment metric (0-1) based on cosine distance, complemented by PHATE-based Semanscope visualization, to assess true cross-language semantic alignment beyond task performance.


<details>
  <summary>Details</summary>
Motivation: With hundreds of multilingual embeddings, practitioners struggle to distinguish genuine cross-lingual semantic alignment from language-specific task patterns; task-driven benchmarks like MTEB may mask alignment gaps. A dedicated alignment-centric benchmark is needed.

Method: Define SA as a ratio of inter-lingual to intra-lingual spread using cosine distance; visualize with PHATE in Semanscope; benchmark 13 models across 4 datasets (52 experiments); categorize results into a three-tier structure.

Result: Top BERT models (LaBSE, USE, S-BERT) attain SA ~0.68-0.70 via translation-pair supervision; LLM embeddings plateau at SA ~0.55-0.61 regardless of model size (0.6B–8B); MLM-only BERTs (mBERT, XLM-R) have SA <0.50 despite extensive multilingual training; alignment driven by training objective rather than architecture or scale; semantic drift evidence via Oracle Bone primitives suggests models learn corpus patterns over cognitive primitives.

Conclusion: Effective cross-lingual alignment requires explicit translation supervision in training objectives; scale and multilingual data alone do not guarantee alignment; the semantic benchmarking framework (Semanscope with SA) provides practical guidance for selecting high-quality multilingual embeddings.

Abstract: With hundreds of multilingual embedding models available, practitioners lack clear guidance on which provide genuine cross-lingual semantic alignment versus task performance through language-specific patterns. Task-driven benchmarks (MTEB) may mask fundamental alignment shortcomings. We introduce Semantic Affinity (SA), a bounded (between 0 and 1) metric measuring inter-lingual to intra-lingual spread ratio using cosine distance, combined with PHATE visualization in our Semanscope framework. Benchmarking 13 models across 4 datasets (52 experiments) reveals a three-tier structure: (1) Top BERT models (LaBSE SA = 0.70, USE SA = 0.68, S-BERT SA = 0.68) achieve strong alignment via translation-pair supervision; (2) LLM embeddings plateau at SA between 0.55 and 0.61 regardless of 0.6 B to 8 B scale; (3) MLM-only BERT models (mBERT, XLM-R, SA < 0.50) fail despite more than 100 language training. Training objective, not architecture or scale, determines alignment. Oracle Bone primitives (1200 BCE) expose semantic drift-models learn corpus patterns rather than cognitive primitives. This work provides semantic benchmarking to help practitioners select quality multilingual embeddings from hundreds of available models, showing cross-lingual alignment requires explicit translation supervision, not merely model scale or multilingual data.

</details>


### [18] [Closing the Data Loop: Using OpenDataArena to Engineer Superior Training Datasets](https://arxiv.org/abs/2601.09733)
*Xin Gao,Xiaoyang Wang,Yun Zhu,Mengzhang Cai,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 提出一个基于数据的闭环数据集工程框架ODA，用以提升SFT数据质量；通过ODA-Math-460k和ODA-Mixture等数据集实现SOTA与显著数据效率提升。


<details>
  <summary>Details</summary>
Motivation: 当前SFT数据集构建多靠启发式聚合，缺乏系统理解样本对模型性能的贡献，需将数据构建转变为可解释的反馈驱动过程。

Method: 引入OpenDataArena (ODA) 的值锚定排序和多维分析，将数据集构建转化为以价值为信号的反馈环；通过两阶段的难度感知管线构建ODA-Math-460k；通过Anchor-and-Patch策略构建ODA-Mixture100k/500k；进行跨领域指令数据等。

Result: 在AIME和HMMT等基准上，ODA-Math-460k达到SOTA；ODA-Mixture在对比更大开放源代码基线时表现显著优越；数据有效性提升、领域推理与通用能力改善。

Conclusion: 证明以数据为中心的AI范式的可行性，透明评测将成为高质量训练数据工程的驱动；ODA框架可用于系统化的数据集建设，提升LLM post-training的效果。

Abstract: The construction of Supervised Fine-Tuning (SFT) datasets is a critical yet under-theorized stage in the post-training of Large Language Models (LLMs), as prevalent practices often rely on heuristic aggregation without a systematic understanding of how individual samples contribute to model performance. In this report, we propose a paradigm shift from ad-hoc curation to a closed-loop dataset engineering framework using OpenDataArena (ODA), which leverages value-anchored rankings and multi-dimensional analysis to transform value benchmarking into feedback signals guiding dataset construction. We instantiate this methodology through two new datasets: \textbf{ODA-Math-460k}, a specialized mathematics reasoning dataset that utilizes a novel two-stage difficulty-aware pipeline to achieve State-of-the-Art (SOTA) results on benchmarks such as AIME and HMMT, and \textbf{ODA-Mixture (100k \& 500k)}, a series of multi-domain instruction datasets built via an ``Anchor-and-Patch'' strategy that outperforms significantly larger open-source baselines. Our empirical results demonstrate that ODA-driven datasets significantly improve both domain-specific reasoning and general utility while achieving superior data efficiency, validating a transition toward data-centric AI where transparent evaluation serves as the primary engine for engineering high-quality training data.

</details>


### [19] [From Detection to Diagnosis: Advancing Hallucination Analysis with Automated Data Synthesis](https://arxiv.org/abs/2601.09734)
*Yanyi Liu,Qingwen Yang,Tiezheng Guo,Feiyu Qu,Jun Liu,Yingyou Wen*

Main category: cs.CL

TL;DR: 提出将幻觉从仅检测转向诊断的新范式，构建幻觉诊断任务（HD Task）及其数据生成管线HDG，训练4B参数的HDM-4B-RL模型，采用GRPO优化，在HaluEval等基准上超越检测模型并与大模型相当，表明幻觉诊断的可行性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于二元检测，缺乏可解释、可操作的反馈来改进模型，难以在真实应用中实现可控、可追踪的改进。需要一种能定位错误、解释原因并纠正内容的诊断性能力。

Method: 提出HDG自动化数据生成管线，通过多维增强（受控事实伪造、推理链扰动等）从原始语料中生成带丰富诊断元数据的训练样本。以HDG生成的数据训练HDM-4B-RL（4B参数），采用Group Relative Policy Optimization（GRPO）并结合结构、准确性、定位等奖励信号的综合奖励函数。

Result: 在HaluEval基准上超越前沿检测模型；在诊断任务上与更大通用模型相当；HDM-4B-RL在整体诊断能力上达到与更大模型相近的水平，同时保持较小尺寸。

Conclusion: 幻觉诊断是可行且有价值的研究方向，提供了构建更可信赖生成式AI系统的有效方法和数据资源。

Abstract: Hallucinations in Large Language Models (LLMs), defined as the generation of content inconsistent with facts or context, represent a core obstacle to their reliable deployment in critical domains. Current research primarily focuses on binary "detection" approaches that, while capable of identifying hallucinations, fail to provide interpretable and actionable feedback for model improvement, thus limiting practical utility. To address this limitation, a new research paradigm is proposed, shifting from "detection" to "diagnosis". The Hallucination Diagnosis Task is introduced, a task which requires models to not only detect hallucinations, but also perform error localization, causal explanation, and content correction. We develop the Hallucination Diagnosis Generator (HDG), an automated pipeline that systematically generates high-quality training samples with rich diagnostic metadata from raw corpora through multi-dimensional augmentation strategies including controlled fact fabrication and reasoning chain perturbation. Using HDG-generated data, we train HDM-4B-RL, a 4-billion-parameter hallucination diagnosis model, employing Group Relative Policy Optimization (GRPO) with a comprehensive reward function incorporating structural, accuracy, and localization signals. Experimental results demonstrate that our model surpasses previous state-of-the-art detection models on the HaluEval benchmark while achieving comparable performance to advanced general-purpose models. In comprehensive diagnosis tasks, HDM-4B-RL matches the capabilities of larger general models while maintaining a smaller size. This work validates the feasibility and value of hallucination diagnosis, providing an effective methodology for building more trustworthy and reliable generative AI systems.

</details>


### [20] [Stable and Explainable Personality Trait Evaluation in Large Language Models with Internal Activations](https://arxiv.org/abs/2601.09833)
*Xiaoxu Ma,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CL

TL;DR: PVNI是一种基于内部激活的个性特征评估方法，通过从模型的对比性提示中提取目标人格特质的persona向量，并在该向量上进行插值以估计中性分数，从而实现稳定且可解释的个性评估，与现有问卷/角色扮演方法相比具有更高的稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于问卷的LLM人格评估在结果稳定性和可解释性方面存在明显不足，对提示措辞或角色设定的微小变动高度敏感，亟需一种稳定且可解释的评估方法。

Method: 从模型内部激活中通过对比性提示提取与目标人格特质相关的persona向量；将该向量作为锚点，沿该向量进行插值以估计相应的中性分数；通过对比中性提示表示与persona方向实现可解释的比较。给出理论分析，讨论方法的有效性与泛化性。

Result: 在多种LLM上实验表明，PVNI在问卷和角色扮演变体下都显著提升人格特质评估的稳定性，相比现有方法具有更高的稳定性；并提供对有效性与泛化性的理论分析，实验规模广泛。

Conclusion: PVNI为LLM的人格特质评估提供了一种稳定且可解释的新途径，克服了基于提示的评估在稳定性和可解释性方面的局限，具备较强的通用性与理论支撑。

Abstract: Evaluating personality traits in Large Language Models (LLMs) is key to model interpretation, comparison, and responsible deployment. However, existing questionnaire-based evaluation methods exhibit limited stability and offer little explainability, as their results are highly sensitive to minor variations in prompt phrasing or role-play configurations. To address these limitations, we propose an internal-activation-based approach, termed Persona-Vector Neutrality Interpolation (PVNI), for stable and explainable personality trait evaluation in LLMs. PVNI extracts a persona vector associated with a target personality trait from the model's internal activations using contrastive prompts. It then estimates the corresponding neutral score by interpolating along the persona vector as an anchor axis, enabling an interpretable comparison between the neutral prompt representation and the persona direction. We provide a theoretical analysis of the effectiveness and generalization properties of PVNI. Extensive experiments across diverse LLMs demonstrate that PVNI yields substantially more stable personality trait evaluations than existing methods, even under questionnaire and role-play variants.

</details>


### [21] [Bears, all bears, and some bears. Language Constraints on Language Models' Inductive Inferences](https://arxiv.org/abs/2601.09852)
*Sriram Padmanabhan,Siyuan Song,Kanishka Misra*

Main category: cs.CL

TL;DR: 通过迁移 Gelman 等 2002 的儿童实验到视觉语言模型，经过图像分类鲁棒性与 all/some 敏感性等预条件测试，发现模型在推断扩展上与儿童相符，且表征差异由 inductive constraints 驱动，而非表面形式。


<details>
  <summary>Details</summary>
Motivation: 检验通用统计学习者（多模态模型）是否具备与儿童相似的语言-推理约束，以及这些约束在跨领域中的可迁移性，探究语言在 inductive 推理中的普遍性偏好。

Method: 方法包括：1) 将 Gelman 等(2002) 的原任务进行跨模态复现实验；2) 在模型上进行图像类别识别的鲁棒性预条件测试，以及对 all 与 some 的敏感性测试；3) 满足预条件后执行原始的扩展推理任务；4) 对模型内部表征进行事后分析，区分 inductive constraints 与 surface-form 的影响。

Result: 模型表现与人类趋同，显示对 generics、universals、indefinite plurals 的扩展性质具有相似的模式偏好；事后分析揭示差异源自 inductive constraints 而非表述形式。

Conclusion: VLMs 能捕捉与儿童相似的语言-推理偏好，强调潜在的 inductive 约束对推理的决定作用，支持跨模态学习中的普遍性偏好假设；同时提示单凭表面形式区分不足以解释推理差异，需要关注更深层的 inductive 约束。

Abstract: Language places subtle constraints on how we make inductive inferences. Developmental evidence by Gelman et al. (2002) has shown children (4 years and older) to differentiate among generic statements ("Bears are daxable"), universally quantified NPs ("all bears are daxable") and indefinite plural NPs ("some bears are daxable") in extending novel properties to a specific member (all > generics > some), suggesting that they represent these types of propositions differently. We test if these subtle differences arise in general purpose statistical learners like Vision Language Models, by replicating the original experiment. On tasking them through a series of precondition tests (robust identification of categories in images and sensitivities to all and some), followed by the original experiment, we find behavioral alignment between models and humans. Post-hoc analyses on their representations revealed that these differences are organized based on inductive constraints and not surface-form differences.

</details>


### [22] [MedRedFlag: Investigating how LLMs Redirect Misconceptions in Real-World Health Communication](https://arxiv.org/abs/2601.09853)
*Sraavya Sambara,Yuan Pu,Ayman Ali,Vishala Mishra,Lionel Wong,Monica Agrawal*

Main category: cs.CL

TL;DR: MedRedFlag 进入一个包含1100+来自Reddit的真实医疗提问的数据集，聚焦其中嵌入错误前提的问句，并评估LLMs是否能进行redirection（重新导向/纠错性回应），以及与临床医生的对比。结果显示LLMs在纠正错误前提方面常未能有效重定向，可能导致不理想的医疗决策，揭示患者面对的医疗AI安全性缺口。


<details>
  <summary>Details</summary>
Motivation: 现实世界的患者提问常包含隐含误解或前提错误。安全的医疗沟通应先纠正误解，再回应患者的真实诉求。本研究评估大型语言模型在面对此类真实场景时的红irection 能力，弥补现有研究的空白。

Method: 构建半自动化的数据生成与筛选管线，以从Reddit提取并人工/半自动标注的问句，形成MedRedFlag数据集（1100+条）。将当前最先进的LLMs与临床医生的回答进行系统对比，评估其是否在检测到错误前提后进行有效重定向，并分析对潜在医疗决策的影响。

Result: LLMs即使检测到有问题的前提，也常未能完成有效的重定向，提供的回答可能支持不最佳的医疗决策。该基准揭示了在真实世界健康沟通场景下，LLMs的安全性存在显著缺口，与临床专家的回答存在差异。

Conclusion: 该数据集为评估医疗AI在红irection 方面的能力提供了重要基准，强调需要改进对话系统的对错前提识别及重定向策略，以提升对患者的安全性。代码与数据集已公开。

Abstract: Real-world health questions from patients often unintentionally embed false assumptions or premises. In such cases, safe medical communication typically involves redirection: addressing the implicit misconception and then responding to the underlying patient context, rather than the original question. While large language models (LLMs) are increasingly being used by lay users for medical advice, they have not yet been tested for this crucial competency. Therefore, in this work, we investigate how LLMs react to false premises embedded within real-world health questions. We develop a semi-automated pipeline to curate MedRedFlag, a dataset of 1100+ questions sourced from Reddit that require redirection. We then systematically compare responses from state-of-the-art LLMs to those from clinicians. Our analysis reveals that LLMs often fail to redirect problematic questions, even when the problematic premise is detected, and provide answers that could lead to suboptimal medical decision making. Our benchmark and results reveal a novel and substantial gap in how LLMs perform under the conditions of real-world health communication, highlighting critical safety concerns for patient-facing medical AI systems. Code and dataset are available at https://github.com/srsambara-1/MedRedFlag.

</details>


### [23] [OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing](https://arxiv.org/abs/2601.09858)
*Yilin Bao,Ziyao He,Zayden Yang*

Main category: cs.CL

TL;DR: 提出一个基于强化学习的分层大纲规划框架，用于科学论文生成。通过回溯性大纲重构和前向价值导向的RL实现长程结构一致性和引用一致性，并引入一个针对科学论文生成的基准，在规划、输入利用、引用保真度、大纲组织和事实准确性方面取得优于强基线的结果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在文档级生成中存在全局结构、输入覆盖与引用一致性的问题，需通过全局规划与事实基础的训练来提升可控性与可靠性。

Method: 将编辑演化的大纲视为可分层的长程规划问题，使用结构化动作进行迭代更新；在训练阶段采用两步：回溯性大纲重构以强化全局结构一致性；前向基于价值的强化学习，设计奖励以体现科学正确性、话语连贯性和引用保真度。并引入一个新基准用于评估规划、输入利用、引用忠实度、大纲组织和事实准确性。

Result: 相较于强基线（神经模型和LLMs）在长程结构一致性与引用可靠性方面取得稳定提升，同时提出并公开一个科学论文生成基准，覆盖规划、输入利用、引用忠实度、大纲组织和事实层面的准确性。

Conclusion: 该RL框架通过分层规划和两阶段优化显著提升文档级科学论文生成的规划性、输入利用与引用可信度，提供了评估端到端科学文本生成的新基准。

Abstract: Scientific paper generation requires document-level planning and factual grounding, but current large language models, despite their strong local fluency, often fail in global structure, input coverage, and citation consistency. We present a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. Our approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. To support effective and stabilize learning,we introduce a two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. In addition, We further introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy. Our results show consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.

</details>


### [24] [Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL](https://arxiv.org/abs/2601.09876)
*Yifei Shen,Yilun Zhao,Justice Ou,Tinglin Huang,Arman Cohan*

Main category: cs.CL

TL;DR: 提出 CLINSQL 基准，评估多模型在现实世界 EHR 的文本转 SQL 任务中的执行正确性，显示当前水平尚未达到临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的临床文本到 SQL 需要跨表连接、时间窗、患者相似性队列等推理，现有评估难以覆盖这些挑战。

Method: 构建 633 个专家标注任务，基于 MIMIC-IV v3.1，要求多表连接、临床筛选和可执行 SQL；采用链路式思维自我润色、基于执行检查的评估；比较 22 个模型；分析对长期上下文、模式编码和元数据的处理。

Result: 在测试集上，GPT-5-mini 获 74.7% 执行分，DeepSeek-R1 为开源领跑 69.2%，Gemini-2.5-Pro 在 Easy 上 85.5%、Hard 降至 67.2%。

Conclusion: CLINSQL 展现了向临床可靠的文本到 SQL 的进展，但性能仍远未达到临床可靠性，需要在跨表推理、时序上下文、以及对临床编码系统与元数据的处理方面提升。

Abstract: Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.

</details>


### [25] [Clozing the Gap: Exploring Why Language Model Surprisal Outperforms Cloze Surprisal](https://arxiv.org/abs/2601.09886)
*Sathvik Nair,Byung-Doh Oh*

Main category: cs.CL

TL;DR: LM概率作为处理难度预测的指标优于基于克塞（cloze）数据的概率；提出LM优势的三条假设：分辨率高、能区分语义相近词、对低频词的概率估计更准确；呼吁提升克塞实验的分辨率并检验人类预测是否对LM的微细区分同样敏感。


<details>
  <summary>Details</summary>
Motivation: 验证LM推断的预测机制是否确实反映语言理解中的预测过程，避免把克塞数据中的局部特征误解为普遍规律；对比两种预测变量在处理难度预测中的作用及其理论含义。

Method: 对LM概率与克塞概率作为处理难度预测变量进行对比分析；围绕三个假设进行证据呈现：1) 不受低分辨率影响，2) 能区分语义上相似的词，3) 能对低频词给出更准确的概率估计。

Result: LM概率在预测处理难度方面优于克塞数据的概率；LM在高分辨率、区分语义相近词、对低频词概率估计方面表现更好；结果支持提升克塞研究分辨率，并开展人类预测与LM微细差异敏感性的一致性检验。

Conclusion: 应提高克塞研究的分辨率，并开展实验以检验人类预测是否与LM在微细语义区分上的敏感性一致，从而更准确地揭示预测在语言理解中的作用。

Abstract: How predictable a word is can be quantified in two ways: using human responses to the cloze task or using probabilities from language models (LMs).When used as predictors of processing effort, LM probabilities outperform probabilities derived from cloze data. However, it is important to establish that LM probabilities do so for the right reasons, since different predictors can lead to different scientific conclusions about the role of prediction in language comprehension. We present evidence for three hypotheses about the advantage of LM probabilities: not suffering from low resolution, distinguishing semantically similar words, and accurately assigning probabilities to low-frequency words. These results call for efforts to improve the resolution of cloze studies, coupled with experiments on whether human-like prediction is also as sensitive to the fine-grained distinctions made by LM probabilities.

</details>


### [26] [Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations](https://arxiv.org/abs/2601.09953)
*Christabel Acquaye,Yi Ting Huang,Marine Carpuat,Rachel Rudinger*

Main category: cs.CL

TL;DR: Open-source LLMs can predict real-world math item difficulty via classroom-role-play simulations and IRT, achieving substantial correlations with NAEP ratings and showing sensitivity to prompts and model strength.


<details>
  <summary>Details</summary>
Motivation: Establish a cost-effective alternative to expensive human pilot studies for calibrating test item difficulty in standardized math assessments.

Method: Simulate a classroom of 4th/8th/12th graders by prompting LLMs to role-play students of varying proficiency; collect responses to MC items; fit Item Response Theory (IRT) models to simulated outcomes; compare item difficulty parameters to NAEP item-level statistics; explore effects of classroom size; assess impact of using named students and demographic stratification; compare open-source LLMs of varying mathematical ability.

Result: Correlation with real-world item difficulties reaches up to ~0.75–0.82 across grades (4th, 8th, 12th); larger classroom sizes improve stability with higher computation; using named students and demographic stratification (gender/race) improves predictions; weaker models (Gemma) sometimes outperform stronger ones (Llama, Qwen) in predicting real-world difficulty.

Conclusion: LLM-based classroom-role-play simulations can be a viable, cost-effective approach to predict item difficulty for standardized math assessments, with design choices (prompting, demographic stratification, model selection) significantly influencing predictive accuracy; open-source models can be competitive for this task.

Abstract: Standardized math assessments require expensive human pilot studies to establish the difficulty of test items. We investigate the predictive value of open-source large language models (LLMs) for evaluating the difficulty of multiple-choice math questions for real-world students. We show that, while LLMs are poor direct judges of problem difficulty, simulation-based approaches with LLMs yield promising results under the right conditions. Under the proposed approach, we simulate a "classroom" of 4th, 8th, or 12th grade students by prompting the LLM to role-play students of varying proficiency levels. We use the outcomes of these simulations to fit Item Response Theory (IRT) models, comparing learned difficulty parameters for items to their real-world difficulties, as determined by item-level statistics furnished by the National Assessment of Educational Progress (NAEP). We observe correlations as high as 0.75, 0.76, and 0.82 for grades 4, 8, and 12, respectively. In our simulations, we experiment with different "classroom sizes," showing tradeoffs between computation size and accuracy. We find that role-plays with named students improves predictions (compared to student ids), and stratifying names across gender and race further improves predictions. Our results show that LLMs with relatively weaker mathematical abilities (Gemma) actually yield better real-world difficulty predictions than mathematically stronger models (Llama and Qwen), further underscoring the suitability of open-source models for the task.

</details>


### [27] [Context Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG](https://arxiv.org/abs/2601.09982)
*David Samuel Setiawan,Raphaël Merx,Jey Han Lau*

Main category: cs.CL

TL;DR: 在低资源语言的跨领域翻译场景中，提出将微调后的NMT与检索增强生成的LLM结合的混合框架，通过RAG提高OT领域的翻译质量，达到接近原域NT的水平。


<details>
  <summary>Details</summary>
Motivation: 低资源语言在领域迁移下NMT性能显著下降。研究以Dhao（仅有新约版本的语言资料）为案例，量化NT域（新约）到OT域（旧约）的失真，并探索是否可通过LLM的RAG能力来修复零样本域下的翻译质量。

Method: 在NT域对NMT模型进行微调得到初稿，然后通过检索增强生成（RAG）的LLM对初稿进行后处理与润色。系统比较包括仅NT域微调的基线和结合LLM的混合框架。分析聚焦于检索示例数量对效果的影响，并通过定性分析评估LLM在零样本领域的修复作用。

Result: NT域中评分为36.17 chrF++，对OT域的未见数据为27.11 chrF++（微调后下滑）。加入混合框架后，最终达到35.21 chrF++，提升8.10，达到与原NT域相近的质量。结论显示，性能提升来自检索样例数量的影响大于检索算法选择，且LLM在零样本领域表现为“安全网”，修复严重失败。

Conclusion: 混合框架有效缓解低资源语言在域迁移中的性能下降，且检索量是关键驱动因素；LLM可作为强力后处理，提升跨域翻译鲁棒性，但仍需关注检索数据质量与领域特异性。

Abstract: Neural Machine Translation (NMT) models for low-resource languages suffer significant performance degradation under domain shift. We quantify this challenge using Dhao, an indigenous language of Eastern Indonesia with no digital footprint beyond the New Testament (NT). When applied to the unseen Old Testament (OT), a standard NMT model fine-tuned on the NT drops from an in-domain score of 36.17 chrF++ to 27.11 chrF++. To recover this loss, we introduce a hybrid framework where a fine-tuned NMT model generates an initial draft, which is then refined by a Large Language Model (LLM) using Retrieval-Augmented Generation (RAG). The final system achieves 35.21 chrF++ (+8.10 recovery), effectively matching the original in-domain quality. Our analysis reveals that this performance is driven primarily by the number of retrieved examples rather than the choice of retrieval algorithm. Qualitative analysis confirms the LLM acts as a robust "safety net," repairing severe failures in zero-shot domains.

</details>


### [28] [SocraticKG: Knowledge Graph Construction via QA-Driven Fact Extraction](https://arxiv.org/abs/2601.10003)
*Sanghyeok Choi,Woosang Jeon,Kyuseok Yang,Taehyeong Kim*

Main category: cs.CL

TL;DR: SocraticKG 通过5W1H导向的问答中介，将文档级语义先以问答对形式展开，再进行三元组提取，从而在覆盖度与结构连贯性之间取得平衡，在MINE基准上实现更高的事实保留和更强的结构性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的知识图谱构建在覆盖性（事实覆盖）与关系连贯性（结构完整）之间存在权衡。直接抽取易导致关系碎片化或信息丢失；需要一个可对齐源文档、 grounding 的中介表示来减少推理错误并提升可解释性。

Method: 提出以5W1H为引导的问答扩展，生成问答对作为结构化中介表示，以揭示上下文依赖与隐含关系；在正式进行三元组提取前进行QA-mediated语义脚手架搭建，提升语义结构的连贯性与 grounding。

Result: 在MINE基准上，SocraticKG有效缓解覆盖-连贯性的权衡，保持较高的事实保留率与较强的结构 cohesion，同时随着提取知识量的增加，仍能维持较好的图结构性。

Conclusion: QA中介的语义脚手架在KG抽取前对语义进行系统化组织具有关键作用，可实现更连贯、可解释且可靠的知识图谱构建。

Abstract: Constructing Knowledge Graphs (KGs) from unstructured text provides a structured framework for knowledge representation and reasoning, yet current LLM-based approaches struggle with a fundamental trade-off: factual coverage often leads to relational fragmentation, while premature consolidation causes information loss. To address this, we propose SocraticKG, an automated KG construction method that introduces question-answer pairs as a structured intermediate representation to systematically unfold document-level semantics prior to triple extraction. By employing 5W1H-guided QA expansion, SocraticKG captures contextual dependencies and implicit relational links typically lost in direct KG extraction pipelines, providing explicit grounding in the source document that helps mitigate implicit reasoning errors. Evaluation on the MINE benchmark demonstrates that our approach effectively addresses the coverage-connectivity trade-off, achieving superior factual retention while maintaining high structural cohesion even as extracted knowledge volume substantially expands. These results highlight that QA-mediated semantic scaffolding plays a critical role in structuring semantics prior to KG extraction, enabling more coherent and reliable graph construction in subsequent stages.

</details>


### [29] [EmplifAI: a Fine-grained Dataset for Japanese Empathetic Medical Dialogues in 28 Emotion Labels](https://arxiv.org/abs/2601.10033)
*Wan Jou She,Lis Kanashiro Pereira,Fei Cheng,Sakiko Yahata,Panote Siriaraya,Eiji Aramaki*

Main category: cs.CL

TL;DR: EmplifAI 是一个面向日本语的共情对话数据集，面向慢性疾病患者，基于28个细分类情绪，包含280个情境和4125段两回合对话，使用 BERTScore 评估模型对情境-对话对的共情对齐；对日本 LLM 微调后提升流畅性与情感共情；并比较 LLM-作为评审者与人工评审的结果以验证评估流程。


<details>
  <summary>Details</summary>
Motivation: 解决慢性疾病管理中的情感波动与对共情需求，缺乏情境化、情绪细分标签的日语对话数据，以及可重复的共情对齐评估方法。

Method: 数据收集：众包与专家评审；情感分类：基于 GoEmotions 的28类情绪；情境设计：280个医学情境；对话：4125段两回合对话。评估：在情境-对话对上使用 BERTScore 对齐情感；跨多家大型语言模型（LLMs）评估情感对齐；将日本 LLM-jp-3.1-13b-instruct4 进行微调以评估效果；将 LLM 作为评审者的评分与人类评审对比以验证评估管线并分析相关性与潜在风险。

Result: 在情感对齐评估中，使用 BERTScore 对多家 LLM 的预测得到 F1=0.83 的对齐分数。对基线 LLM 的微调显著提升了流畅性、普遍共情与情感特定共情表现。将 LLM 作为评审者与人工评审的对比分析揭示了评估流程的可行性以及潜在风险与偏差来源。

Conclusion: EmplifAI 为面向慢性病场景的日语共情对话数据集与情感对齐评估管线提供了有价值的资源，展示了将 GoEmotions 28类情绪迁移到医学情境中的可行性，并为开发更具情境感知的对话系统与评估框架提供方向，同时也提示数据偏差、领域适配与评估偏差等潜在局限。

Abstract: This paper introduces EmplifAI, a Japanese empathetic dialogue dataset designed to support patients coping with chronic medical conditions. They often experience a wide range of positive and negative emotions (e.g., hope and despair) that shift across different stages of disease management. EmplifAI addresses this complexity by providing situation-based dialogues grounded in 28 fine-grained emotion categories, adapted and validated from the GoEmotions taxonomy. The dataset includes 280 medically contextualized situations and 4125 two-turn dialogues, collected through crowdsourcing and expert review. To evaluate emotional alignment in empathetic dialogues, we assessed model predictions on situation--dialogue pairs using BERTScore across multiple large language models (LLMs), achieving F1 scores of 0.83. Fine-tuning a baseline Japanese LLM (LLM-jp-3.1-13b-instruct4) with EmplifAI resulted in notable improvements in fluency, general empathy, and emotion-specific empathy. Furthermore, we compared the scores assigned by LLM-as-a-Judge and human raters on dialogues generated by multiple LLMs to validate our evaluation pipeline and discuss the insights and potential risks derived from the correlation analysis.

</details>


### [30] [Long-Chain Reasoning Distillation via Adaptive Prefix Alignment](https://arxiv.org/abs/2601.10064)
*Zhenghao Liu,Zhuoyang Wu,Xinze Li,Yukun Yan,Shuo Wang,Zulong Chen,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: P-ALIGN distills teacher chain-of-thoughts by adaptive prefix alignment, truncating long reasoning trajectories to keep only informative prefixes and using them to supervise student models; it improves mathematical reasoning performance by >3% on benchmarks and reduces negative effects of redundant/uncertain suffixes.


<details>
  <summary>Details</summary>
Motivation: Teacher-provided reasoning trajectories are often very long and structurally complex, which exceeds the learning capacity of small student models and creates a mismatch between supervision and learner capability.

Method: Adaptive prefix alignment: evaluate whether the remaining suffix of the teacher's reasoning is concise and sufficient; truncate suffix accordingly; use the teacher's resulting prefix to supervise the student, enforcing alignment between teacher and student prefixes.

Result: Empirical results show P-ALIGN outperforms all baselines by over 3% on multiple mathematical reasoning benchmarks; analysis indicates prefixes lead to more effective supervision and avoid detrimental effects of redundant/unreliable reasoning components.

Conclusion: Adaptive, prefix-focused distillation improves the efficiency and effectiveness of teaching LLMs' reasoning to smaller student models; code is publicly available.

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.

</details>


### [31] [CALM-IT: Generating Realistic Long-Form Motivational Interviewing Dialogues with Dual-Actor Conversational Dynamics Tracking](https://arxiv.org/abs/2601.10085)
*Viet Cuong Nguyen,Nhi Yen Nguyen,Kristin A. Candan,Mary Conlon,Vanessa Rumie,Kristen Risola,Srijan Kumar,Munmun De Choudhury*

Main category: cs.CL

TL;DR: CALM-IT提出一个双人对话的状态空间框架，用于生成与评估长期的动机访谈对话，显式建模治疗师与客户的对话动态以维持目标对齐和治疗进展。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在长轮次对话中难以保持连贯的治疗进展全局一致性，局部优化导致漂移；需要对话状态的持续更新以指导策略与生成。

Method: 将治疗师-客户互动建模为双向状态空间过程，双方不断更新推断的对齐、心理状态和短期目标，以引导策略选择与发话生成；进行大规模评估，比较强基线，在有效性、目标对齐和对话长度稳定性等方面进行量化。

Result: 在大规模评估中，CALM-IT在有效性和目标对齐方面持续超越强基线；对话长度增加时的稳定性显著提高；尽管起始引导较少，但客户端接受率最高，为64.3%。

Conclusion: 对话状态的演化建模对生成高质量的长期合成对话至关重要，双向状态更新框架为实现更高效、契合治疗目标的长期对话提供了证据。

Abstract: Large Language Models (LLMs) are increasingly used in mental health-related settings, yet they struggle to sustain realistic, goal-directed dialogue over extended interactions. While LLMs generate fluent responses, they optimize locally for the next turn rather than maintaining a coherent model of therapeutic progress, leading to brittleness and long-horizon drift. We introduce CALM-IT, a framework for generating and evaluating long-form Motivational Interviewing (MI) dialogues that explicitly models dual-actor conversational dynamics. CALM-IT represents therapist-client interaction as a bidirectional state-space process, in which both agents continuously update inferred alignment, mental states, and short-term goals to guide strategy selection and utterance generation. Across large-scale evaluations, CALM-IT consistently outperforms strong baselines in Effectiveness and Goal Alignment and remains substantially more stable as conversation length increases. Although CALM-IT initiates fewer therapist redirections, it achieves the highest client acceptance rate (64.3%), indicating more precise and therapeutically aligned intervention timing. Overall, CALM-IT provides evidence for modeling evolving conversational state being essential for generating high-quality long-form synthetic conversations.

</details>


### [32] [SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature](https://arxiv.org/abs/2601.10108)
*Yiming Ren,Junjie Wang,Yuxin Meng,Yihang Shi,Zhiqiang Lin,Ruihang Chu,Yiran Xu,Ziming Li,Yunfei Zhao,Zihan Wang,Yu Qiao,Ruiming Tang,Minghao Liu,Yujiu Yang*

Main category: cs.CL

TL;DR: 提出 FITO 框架，通过要求在原生科学文献中构建跨模态证据链来评估多模态大语言模型的真正理解能力，配套 SIN-Data 与 SIN-Bench，以及 No Evidence, No Score 的评分策略。实验表明 grounding 是主要瓶颈，Gemini-3-pro 在平均分上领先，GPT-5 在 SIN-QA 方面表现最佳但证据对齐能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有的答题指标和合成测试往往只衡量答案匹配，而未强制模型给出可证的推理链路。需要能在原文文献中构建可追溯、跨模态的证据推理过程，以更真实地评估模型的科学阅读与推理能力。

Method: 提出 FITO 范式，开发 SIN-Data，保留文本与图形的原生交错；在其基础上构建 SIN-Bench，包含四个任务：SIN-Find（证据发现）、SIN-Verify（假设验证）、SIN-QA（有证据的问答）、SIN-Summary（基于证据的综合）。引入 No Evidence, No Score，对以可验证的锚点为依据的预测进行评分，并通过匹配、相关性和逻辑性评估证据质量。 在八个 MLLMs 上评估，比较证据 grounding 的水平及总体表现。

Result: 实验显示 grounding 是主要瓶颈；Gemini-3-pro 的平均综合分最高为 0.573；GPT-5 虽在 SIN-QA 任务上达到最高正确率 0.767，但在证据对齐的整体分数上表现较差，揭示正确性与可追溯证据之间存在差距。

Conclusion: 该工作揭示了仅凭正确性评估难以反映模型的推理透明度，证据 grounding 的提升对科学文本理解至关重要。SIN-Data 与 SIN-Bench 提供一个可扩展的评测平台，未来可用于深入研究跨模态证据推理与可解释性提升方向。

Abstract: Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic "Needle-In-A-Haystack" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the "Fish-in-the-Ocean" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce "No Evidence, No Score", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.

</details>


### [33] [Skill-Aware Data Selection and Fine-Tuning for Data-Efficient Reasoning Distillation](https://arxiv.org/abs/2601.10109)
*Lechen Zhang,Yunxiang Zhang,Wei Hu,Lu Wang*

Main category: cs.CL

TL;DR: Skill-centric distillation enables data-efficient transfer of reasoning to weaker models with minimal SFT data, using skill-based data selection and skill-aware fine-tuning, achieving gains on mathematical reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: 降低对大规模监督微调数据的依赖，通过聚焦学生模型的薄弱技能和显式技能分解，提高推理能力的知识蒸馏效果。

Method: 提出两大组件：1) 技能级数据选择，优先覆盖学生模型薄弱技能的样本；2) 技能感知微调，训练中显式引导问题解题的技能分解。实验以从100K教师语料中筛选的1,000个训练样本进行，比较随机SFT基线。

Result: 在Qwen3-4B上提升约1.6%，在Qwen3-8B上提升约1.4%，覆盖五个数学推理基准。进一步分析表明增益集中在训练中强调的技能。

Conclusion: 技能中心化训练对高效推理蒸馏有效；在有限数据条件下，结合技能选择与技能分解能够提升弱学生模型的推理表现。

Abstract: Large reasoning models such as DeepSeek-R1 and their distilled variants achieve strong performance on complex reasoning tasks. Yet, distilling these models often demands large-scale data for supervised fine-tuning (SFT), motivating the pursuit of data-efficient training methods. To address this, we propose a skill-centric distillation framework that efficiently transfers reasoning ability to weaker models with two components: (1) Skill-based data selection, which prioritizes examples targeting the student model's weaker skills, and (2) Skill-aware fine-tuning, which encourages explicit skill decomposition during problem solving. With only 1,000 training examples selected from a 100K teacher-generated corpus, our method surpasses random SFT baselines by +1.6% on Qwen3-4B and +1.4% on Qwen3-8B across five mathematical reasoning benchmarks. Further analysis confirms that these gains concentrate on skills emphasized during training, highlighting the effectiveness of skill-centric training for efficient reasoning distillation.

</details>


### [34] [What Gets Activated: Uncovering Domain and Driver Experts in MoE Language Models](https://arxiv.org/abs/2601.10159)
*Guimin Hu,Meng Li,Qiwei Peng,Lijie Hu,Boyan Xu,Ruichu Cai*

Main category: cs.CL

TL;DR: 分析MoE LLM中专家激活的领域偏好与驱动作用，提出基于熵与因果效应的度量框架，识别领域专家与驱动专家，并揭示早期tokens更易触发驱动专家，以及通过调整专家权重可提升跨三模型三领域的性能。


<details>
  <summary>Details</summary>
Motivation: 受人脑功能分化的启发，弥补对专家级别行为的可解释性研究空白，聚焦专家层面的激活模式与因果影响。

Method: 引入熵基度量评估专家在特定领域的偏好；引入因果效应度量评估专家激活对输出的因果贡献；在三个公开领域的MoE模型中分析激活的专家、域偏好与驱动关系，并研究单词/标记如何触发不同专家的激活；通过调整领域和驱动专家权重来评估对性能的影响。

Result: 发现 activated experts 中存在明确的领域偏好者和对模型性能具有强因果影响的驱动专家；句首位置的token更易触发驱动专家；调整领域和驱动专家权重可在三模型三领域中带来显著的性能提升。

Conclusion: 为MoE模型内部工作机制提供新视角，提升可解释性；通过定向调控领域与驱动专家权重可实现性能提升。

Abstract: Most interpretability work focuses on layer- or neuron-level mechanisms in Transformers, leaving expert-level behavior in MoE LLMs underexplored. Motivated by functional specialization in the human brain, we analyze expert activation by distinguishing domain and driver experts. In this work, we study expert activation in MoE models across three public domains and address two key questions: (1) which experts are activated, and whether certain expert types exhibit consistent activation patterns; and (2) how tokens are associated with and trigger the activation of specific experts. To answer these questions, we introduce entropy-based and causal-effect metrics to assess whether an expert is strongly favored for a particular domain, and how strongly expert activation contributes causally to the model's output, thus identify domain and driver experts, respectively. Furthermore, we explore how individual tokens are associated with the activation of specific experts. Our analysis reveals that (1) Among the activated experts, some show clear domain preferences, while others exert strong causal influence on model performance, underscoring their decisive roles. (2) tokens occurring earlier in a sentence are more likely to trigger the driver experts, and (3) adjusting the weights of domain and driver experts leads to significant performance gains across all three models and domains. These findings shed light on the internal mechanisms of MoE models and enhance their interpretability.

</details>


### [35] [Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment](https://arxiv.org/abs/2601.10160)
*Cameron Tice,Puria Radmard,Samuel Ratnam,Andy Kim,David Africa,Kyle O'Brien*

Main category: cs.CL

TL;DR: 通过在预训练阶段调控关于AI对齐/错误对齐的 discourse，实验显示对齐 discourse 能显著降低错对齐比例，错误对齐 discourse 提高错对齐；效应在后续训练阶段仍存但减弱，提出将对齐预训练作为对齐改进的补充路径。


<details>
  <summary>Details</summary>
Motivation: 探究预训练数据中 discourse 对下游对齐的因果影响，以及是否存在自证式对齐（self-fulfilling alignment）的证据。

Method: 在6.9B参数的LLM上进行受控实验，系统调节(mis)alignment discourse 的量级；通过upsampling合成文档来控制训练数据分布；评估错对齐指标（misalignment scores），观察在初始训练与后训练阶段的持续性；提供可公开的模型和数据集以便复现。

Result: 增加关于AI错对齐 discourse 会提升错对齐行为，影响显著；相反，增加关于对齐行为的文档将错对齐分数从45%降到9%；出现自证式对齐的迹象；效应在后训练阶段仍存在但有所削弱。

Conclusion: 将对齐预训练视为对齐策略的一个重要补充，与后续训练联合作用以改善对齐，建议在能力提升的同时进行对齐导向的预训练；并对外公开数据与模型以促进复现。

Abstract: Pretraining corpora contain extensive discourse about AI systems, yet the causal influence of this discourse on downstream alignment remains poorly understood. If prevailing descriptions of AI behaviour are predominantly negative, LLMs may internalise corresponding behavioural priors, giving rise to self-fulfilling misalignment. This paper provides the first controlled study of this hypothesis by pretraining 6.9B-parameter LLMs with varying amounts of (mis)alignment discourse. We find that discussion of AI contributes to misalignment. Upsampling synthetic training documents about AI misalignment leads to a notable increase in misaligned behaviour. Conversely, upsampling documents about aligned behaviour reduces misalignment scores from 45% to 9%. We consider this evidence of self-fulfilling alignment. These effects are dampened, but persist through post-training. Our findings establish the study of how pretraining data shapes alignment priors, or alignment pretraining, as a complement to post-training. We recommend practitioners pretrain for alignment as well as capabilities. Our models and datasets are available at alignmentpretraining.ai

</details>


### [36] [Credit C-GPT: A Domain-Specialized Large Language Model for Conversational Understanding in Vietnamese Debt Collection](https://arxiv.org/abs/2601.10167)
*Nhung Nguyen Thi Hong,Cuong Nguyen Dang,Tri Le Ngoc*

Main category: cs.CL

TL;DR: 七十亿参数的 Credit C-GPT 用于越南语债务催收场景的领域专用对话模型，整合对话理解、情感识别、意图检测、呼叫阶段分类与槽位值提取，在单一推理框架中实现多任务。相较传统流水线方法展现显著改进。


<details>
  <summary>Details</summary>
Motivation: 面向越南语联系中心场景的非正式口语、情感波动及领域特定推理挑战，传统NLP难以同时支撑多任务且存在隐私与可扩展性问题。需要一个在对话级别提供实时协助与后续分析、并更注重隐私保护的解决方案。

Method: 对一个七亿参数的语言模型进行微调，整合多任务能力到一个统一框架：对话理解、情感识别、意图检测、呼叫阶段分类、槽位值提取。描述数据构建、标注策略与训练流程，并在专有的人类标注数据集上进行评估。

Result: 实验结果显示该模型在与传统基于流水线的方法相比具有一致的改进，表明领域专用对话语言模型在企业呼叫中心的实时协助与事后分析中更具可扩展性和隐私友好性。

Conclusion: 领域专用的对话语言模型能够提升对话理解与任务完成度，为 BFSI 呼叫中心提供可扩展且隐私保护的解决方案，同时促进实时支援与后期分析的实施。

Abstract: Debt collection is a critical function within the banking, financial services, and insurance (BFSI) sector, relying heavily on large-scale human-to-human conversational interactions conducted primarily in Vietnamese contact centers. These conversations involve informal spoken language, emotional variability, and complex domain-specific reasoning, which pose significant challenges for traditional natural language processing systems. This paper introduces Credit C-GPT, a domain-specialized large language model with seven billion parameters, fine-tuned for conversational understanding in Vietnamese debt collection scenarios. The proposed model integrates multiple conversational intelligence tasks, including dialogue understanding, sentiment recognition, intent detection, call stage classification, and structured slot-value extraction, within a single reasoning-based framework. We describe the data construction process, annotation strategy, and training methodology, and evaluate the model on proprietary human-annotated datasets. Experimental results show consistent improvements over traditional pipeline-based approaches, indicating that domain-specialized conversational language models provide a scalable and privacy-aware solution for real-time assistance and post-call analytics in enterprise contact centers.

</details>


### [37] [HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning](https://arxiv.org/abs/2601.10187)
*Ziang Cui,Mengran Yu,Tianjiao Li,Chenyu Shi,Yingxuan Shi,Lusheng Zhang,Hongwei Lin*

Main category: cs.CL

TL;DR: 提出 Sand-Glass 基准评估在音节级时长约束下的翻译，以及 HOMURA 强化学习框架，通过 KL 正则化与动态音节比奖励实现语义保真与时间合规的权衡，显著提升长度控制和翻译质量。


<details>
  <summary>Details</summary>
Motivation: LLMs 在多语言翻译方面存在系统性跨语言冗长偏差，导致在时间受限的任务（如字幕、配音）中难以兼顾语义保真和严格时长约束。现有提示工程方法难以解决该冲突。

Method: 提出 Sand-Glass 基准用于音节级时长约束的翻译评估；提出 HOMURA，使用带 KL 正则化的目标和动态音节比奖励的强化学习框架以同时优化语义保真与时间合规性。

Result: 实验结果显示该方法显著优于强基线，能够在保持语义充分性的前提下实现精准的长度控制，并符合语言密度层级的约束。

Conclusion: 通过 Sand-Glass 与 HOMURA，本文提供一个可用于音节级时长受限翻译任务的基线与方法，推进在严格时间约束下的跨语言翻译研究。

Abstract: Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively "tames" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.

</details>


### [38] [One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages?](https://arxiv.org/abs/2601.10205)
*Arya Shah,Himanshu beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: 提出一个跨12种印度语言的统一基准，评估多语嵌入在不进行生成的前提下对 persona-instruction 的匹配能力。结果显示记忆检索的收益有限，但在不同任务上有仍有效的基线，提供模型选择的实用指引和可复现基线。


<details>
  <summary>Details</summary>
Motivation: 当前基准要么聚焦单一语言，要么混淆检索与生成；需要评估嵌入模型在不依赖文本生成的情况下，是否能编码 persona-instruction 的一致性，以服务印度语种众多的用户。

Method: 构建覆盖12种印度语言的统一基准，包含四个评估任务：单语与跨语的 persona-to-instruction 检索、指令到 persona 的反向检索、以及二分类兼容性。评估八种多语嵌入模型，在 frozen-encoder 设置下以薄逻辑回归头进行分类。关键模型及结果包括：E5-Large-Instruct 在单语检索 Recall@1 27.4%、跨语检索 Recall@1 20.7%；BGE-M3 在反向检索 Recall@1 为 32.1%；LaBSE 在二分类 AUROC 达 75.3% 且校准良好。

Result: 提供了对 Indic 多语检索的实用模型选择指南，并建立可复现的基线，便于未来工作改进。代码、数据集和模型公开可获取。

Conclusion: 在 Indic 多语环境下，嵌入模型的检索对齐能力虽非极高，但不同模型在不同任务上呈现有意义的性能，给研究者和工程师提供明确的基线与评估范式。

Abstract: Aligning multilingual assistants with culturally grounded user preferences is essential for serving India's linguistically diverse population of over one billion speakers across multiple scripts. However, existing benchmarks either focus on a single language or conflate retrieval with generation, leaving open the question of whether current embedding models can encode persona-instruction compatibility without relying on response synthesis. We present a unified benchmark spanning 12 Indian languages and four evaluation tasks: monolingual and cross-lingual persona-to-instruction retrieval, reverse retrieval from instruction to persona, and binary compatibility classification. Eight multilingual embedding models are evaluated in a frozen-encoder setting with a thin logistic regression head for classification. E5-Large-Instruct achieves the highest Recall@1 of 27.4\% on monolingual retrieval and 20.7\% on cross-lingual transfer, while BGE-M3 leads reverse retrieval at 32.1\% Recall@1. For classification, LaBSE attains 75.3\% AUROC with strong calibration. These findings offer practical guidance for model selection in Indic multilingual retrieval and establish reproducible baselines for future work\footnote{Code, datasets, and models are publicly available at https://github.com/aryashah2k/PI-Indic-Align.

</details>


### [39] [Loop as a Bridge: Can Looped Transformers Truly Link Representation Space and Natural Language Outputs?](https://arxiv.org/abs/2601.10242)
*Guanxu Chen,Dongrui Liu,Jing Shao*

Main category: cs.CL

TL;DR: Looped Transformers (LTs) 通过在同一层之间迭代来增加计算深度，实验显示虽然增加循环次数可以缩小输出与内部知识之间的差距，但这往往伴随内部表征知识的退化；另外，LT 的表征感知能力在跨循环中并未显著提升，只有在最终循环才出现。结论：LTs 虽具扩展计算深度的潜力，但尚未实现将表示空间与自然语言输出真正对齐的内省能力。


<details>
  <summary>Details</summary>
Motivation: 研究者希望通过多次迭代的内部计算与自省来缩小模型内部知识与外部输出之间的差距，探索是否可通过增加计算深度实现更好的一致性。

Method: 进行系统的经验研究，比较 Looped Transformers 的不同循环深度，评估输出-知识差距及内部表征的保留与退化，分析跨循环的表征感知能力。

Result: 增加循环次数确实缩小了输出与内部知识之间的差距，但部分原因是内部表征的知识信息退化；此外，当前 LT 的表征感知能力并未随循环显著提升，只有在最终循环阶段才观察到。

Conclusion: LTs 在扩展计算深度方面具有潜力，但尚未实现将内部表示与自然语言输出的对齐所需的真正自我反思能力，需提出新的机制来实现表征空间与输出之间的更紧密耦合。

Abstract: Large Language Models (LLMs) often exhibit a gap between their internal knowledge and their explicit linguistic outputs. In this report, we empirically investigate whether Looped Transformers (LTs)--architectures that increase computational depth by iterating shared layers--can bridge this gap by utilizing their iterative nature as a form of introspection. Our experiments reveal that while increasing loop iterations narrows the gap, it is partly driven by a degradation of their internal knowledge carried by representations. Moreover, another empirical analysis suggests that current LTs' ability to perceive representations does not improve across loops; it is only present in the final loop. These results suggest that while LTs offer a promising direction for scaling computational depth, they have yet to achieve the introspection required to truly link representation space and natural language.

</details>


### [40] [Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs](https://arxiv.org/abs/2601.10257)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.CL

TL;DR: 提出一种分离语言因素的评估框架，区分 dilemma-language 与 reasoning-language 对大模型道德判断的影响，通过英中跨语言实验与 mismatched 条件，解耦两者贡献并用 Moral Foundations Theory 解释输出，发现推理语言效应的方差高于输入语言、存在广泛情境依赖，以及 Authority 维度的分解，最后给出部署指南并公开代码与数据。


<details>
  <summary>Details</summary>
Motivation: 解决标准评估中输入语言与推理语言对跨语言道德判断的混淆，提供一个可分解、量化两因素贡献的诊断工具，以提升跨文化部署的可靠性。

Method: 独立操纵 dilemma-language（问题语言）与 reasoning-language（推理语言），包含 matched 与 mismatched 条件，结合 Moral Foundations Theory 解读道德判断，进行跨语言（英语-中文）对13种大模型的评估；提出诊断分类法，将观察结果转化为部署指南；公开代码与数据。

Result: 得到三大发现：① 推理语言效应的方差贡献约为输入语言效应的两倍；② 近半数模型显示情境依赖，标准评估常常遗漏；③ 形成诊断性分类法，可将模式转化为部署建议；并有 Authority 维度分解为家庭相关与制度维度的证据。

Conclusion: 该框架具备强诊断力和分解力，能够深入理解跨语言道德判断的影响因素，并提供可操作的部署指南，同时通过公开数据提升可重复性与透明度。

Abstract: When LLMs judge moral dilemmas, do they reach different conclusions in different languages, and if so, why? Two factors could drive such differences: the language of the dilemma itself, or the language in which the model reasons. Standard evaluation conflates these by testing only matched conditions (e.g., English dilemma with English reasoning). We introduce a methodology that separately manipulates each factor, covering also mismatched conditions (e.g., English dilemma with Chinese reasoning), enabling decomposition of their contributions. To study \emph{what} changes, we propose an approach to interpret the moral judgments in terms of Moral Foundations Theory. As a side result, we identify evidence for splitting the Authority dimension into a family-related and an institutional dimension. Applying this methodology to English-Chinese moral judgment with 13 LLMs, we demonstrate its diagnostic power: (1) the framework isolates reasoning-language effects as contributing twice the variance of input-language effects; (2) it detects context-dependency in nearly half of models that standard evaluation misses; and (3) a diagnostic taxonomy translates these patterns into deployment guidance. We release our code and datasets at https://anonymous.4open.science/r/CrossCulturalMoralJudgement.

</details>


### [41] [MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts](https://arxiv.org/abs/2601.10272)
*Yuxuan Lou,Kai Yang,Yang You*

Main category: cs.CL

TL;DR: 引入 MoST（Mixture of Speech and Text），基于模态感知的专家混合（MAMoE）架构的多模态大语言模型，采用模态特定专家组与共享专家共同路由，专为语音与文本的表示差异设计，能在 ASR、TTS、音频语言建模和口语问答等任务上超越同参数量的现有模型，并以开源数据实现数据高效训练；同时提供完整的训练、推理代码及数据集。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型往往使用统一参数处理不同模态，忽略模态的固有表示差异；需要一种模态感知的路由机制来提升模态特定学习与跨模态理解，并在完全开源数据上实现数据高效的训练管线。

Method: 提出 MAMoE：模态特定的专家组与共享专家共同路由，通过输入类型将令牌路由到相应专家；在此基础上，构建高效的变换管线：在预训练的 MoE 语言模型上进行面向 ASR/TTS 的后训练，再用专门的语音-文本指令数据集进行微调；整套管线依赖完全开源数据集。

Result: 在 ASR、TTS、音频语言建模和口语问答等基准上，MoST 在参数量相近的模型中表现突出；消融实验证实模态特定路由和共享专家对各领域均有显著贡献；据称这是首个完全开源的基于 MoE 的语音-文本 LLM。

Conclusion: MoST 代表一个在 MoE 架构上实现的、完全开源的语音-文本 LLM，具有良好的数据效率与跨模态能力，相关培训与推理代码及数据集公开。

Abstract: We present MoST (Mixture of Speech and Text), a novel multimodal large language model that seamlessly integrates speech and text processing through our proposed Modality-Aware Mixture of Experts (MAMoE) architecture. While current multimodal models typically process diverse modality representations with identical parameters, disregarding their inherent representational differences, we introduce specialized routing pathways that direct tokens to modality-appropriate experts based on input type. MAMoE simultaneously enhances modality-specific learning and cross-modal understanding through two complementary components: modality-specific expert groups that capture domain-specific patterns and shared experts that facilitate information transfer between modalities. Building on this architecture, we develop an efficient transformation pipeline that adapts the pretrained MoE language model through strategic post-training on ASR and TTS datasets, followed by fine-tuning with a carefully curated speech-text instruction dataset. A key feature of this pipeline is that it relies exclusively on fully accessible, open-source datasets to achieve strong performance and data efficiency. Comprehensive evaluations across ASR, TTS, audio language modeling, and spoken question answering benchmarks show that MoST consistently outperforms existing models of comparable parameter counts. Our ablation studies confirm that the modality-specific routing mechanism and shared experts design significantly contribute to performance gains across all tested domains. To our knowledge, MoST represents the first fully open-source speech-text LLM built on a Mixture of Experts architecture. \footnote{We release MoST model, training code, inference code, and training data at https://github.com/NUS-HPC-AI-Lab/MoST

</details>


### [42] [The Straight and Narrow: Do LLMs Possess an Internal Moral Path?](https://arxiv.org/abs/2601.10307)
*Luoming Hu,Jingjie Zeng,Liang Yang,Hongfei Lin*

Main category: cs.CL

TL;DR: 提出基于道德基础理论的跨语言道德表示映射，构建可操控的道德向量并在推理阶段通过自适应融合实现更稳健的安全-有用性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法多为外部约束，未能触达模型的内在道德表征；需要在推理期进行动态、跨语言可迁移的道德控制。

Method: 以道德基础理论映射细粒度道德景观；通过中间层线性探 probing 验证英语与中文的共性与差异；提取可操控的道德向量并在推理阶段注入以实现行为与内部表征的双向影响；提出自适应道德融合（AMF），结合探测与向量注入实现安全性与有用性之间的权衡；通过内部评估和行为评估验证效果。

Result: 发现英语与中文存在共通但子空间不同的道德表征；获得可操控的道德向量并在内部与行为层面验证其有效性；AMF 能在推理时动态调节，降低 benign 查询的错误拒绝率并降低被破解的成功率，相比基线表现更优。

Conclusion: 提出一种面向推理期的目标化内在防御，利用跨语言的道德子空间和自适应向量注入来提升对齐鲁棒性，表明道德表征在大模型中具有可迁移性和可控性。

Abstract: Enhancing the moral alignment of Large Language Models (LLMs) is a critical challenge in AI safety. Current alignment techniques often act as superficial guardrails, leaving the intrinsic moral representations of LLMs largely untouched. In this paper, we bridge this gap by leveraging Moral Foundations Theory (MFT) to map and manipulate the fine-grained moral landscape of LLMs. Through cross-lingual linear probing, we validate the shared nature of moral representations in middle layers and uncover a shared yet different moral subspace between English and Chinese. Building upon this, we extract steerable Moral Vectors and successfully validate their efficacy at both internal and behavioral levels. Leveraging the high generalizability of morality, we propose Adaptive Moral Fusion (AMF), a dynamic inference-time intervention that synergizes probe detection with vector injection to tackle the safety-helpfulness trade-off. Empirical results confirm that our approach acts as a targeted intrinsic defense, effectively reducing incorrect refusals on benign queries while minimizing jailbreak success rates compared to standard baselines.

</details>


### [43] [Multilinguality as Sense Adaptation](https://arxiv.org/abs/2601.10310)
*Jan Christian Blaise Cruz,David Ifeoluwa Adelani,Alham Fikri Aji*

Main category: cs.CL

TL;DR: SENSIA 是一种基于“感知-层级对齐”的多语言适配方法，通过对齐潜在意义表征中的 sense-level 混合和上下文表示，并在并行数据上联合训练目标语言语言建模损失，以提升跨语言对齐和流畅性。在四种语言的评测中，SENSIA 通常优于同类多语言对齐方法，与从头训练的单语言基线相比也具备竞争力，同时需要的目标语言数据量低约2-4倍。对学习到的 sense 几何结构分析显示局部感知拓扑和相对英文的全局结构基本保持，消融实验表明方法对设计和规模具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决多语言能力的瓶颈，不再仅依赖共享参数与规模，而是通过对齐不同语言中的 sense-level 表征来实现跨语言对齐与迁移，提升数据效率和跨语言泛化。

Method: 在 Backpack 语言模型上进行感知层级的对齐，利用并行数据显式对齐不同语言的 sense-level 混合和上下文表征，并同时训练目标语言的语言建模损失以保持流畅性。

Result: 相比可比的多语言对齐方法通常具有更好表现；在四种语言的评测中与单语言从头训练基线相当甚至具备竞争力，同时需要的目标语言数据量减少 2-4 倍。对学习到的 sense 几何结构进行分析，局部拓扑和全局结构相对英文基本保留，消融实验证实方法在设计与尺度上具鲁棒性。

Conclusion: 将多语言对齐聚焦于 sense-level 的适配与保持语言几何关系的稳定性，展示了数据更高效的跨语言迁移潜力，且对模型设计与数据规模具鲁棒性，具有对多语言模型训练的潜在影响。

Abstract: We approach multilinguality as sense adaptation: aligning latent meaning representations across languages rather than relying solely on shared parameters and scale. In this paper, we introduce SENse-based Symmetric Interlingual Alignment (SENSIA), which adapts a Backpack language model from one language to another by explicitly aligning sense-level mixtures and contextual representations on parallel data, while jointly training a target-language language modeling loss to preserve fluency. Across benchmarks on four typologically diverse languages, SENSIA generally outperforms comparable multilingual alignment methods and achieves competitive accuracy against monolingual from-scratch baselines while using 2-4x less target-language data. Analyses of learned sense geometry indicate that local sense topology and global structure relative to English are largely preserved, and ablations show that the method is robust in terms of design and scale.

</details>


### [44] [ADVOSYNTH: A Synthetic Multi-Advocate Dataset for Speaker Identification in Courtroom Scenarios](https://arxiv.org/abs/2601.10315)
*Aniket Deroy*

Main category: cs.CL

TL;DR: 提出 Advosynth-500 数据集用于评估大规模合成语音在法庭场景中的说话人识别能力，包含 100 条合成语音、10 位身份、5 对对手在法庭辩论场景中的模拟。


<details>
  <summary>Details</summary>
Motivation: 随着合成语音在结构化环境中的逼真度增强，区分不同合成声音并映射至其来源成为重要研究问题；通过可控数据集建立说话人识别基准以评估系统对合成来源的识别能力。

Method: 创建 Advosynth-500 数据集：100 条合成语音、10 个身份；利用 Speech Llama Omni 模型模拟五对倡导人于法庭辩论中的对抗场景；为每位身份设定明确的声学特征；提出一个说话人识别挑战以测试音频到合成来源的映射能力。数据集公开可获取，链接给出。

Result: 摘要未给出具体实验结果、评估指标或对比基线。

Conclusion: 数据集旨在促进对合成语音在结构化环境中的辨识与来源跟踪研究，具有在法庭等高保真应用场景中的潜在应用价值。

Abstract: As large-scale speech-to-speech models achieve high fidelity, the distinction between synthetic voices in structured environments becomes a vital area of study. This paper introduces Advosynth-500, a specialized dataset comprising 100 synthetic speech files featuring 10 unique advocate identities. Using the Speech Llama Omni model, we simulate five distinct advocate pairs engaged in courtroom arguments. We define specific vocal characteristics for each advocate and present a speaker identification challenge to evaluate the ability of modern systems to map audio files to their respective synthetic origins.
  Dataset is available at this link-https: //github.com/naturenurtureelite/ADVOSYNTH-500.

</details>


### [45] [Boundary-Aware NL2SQL: Integrating Reliability through Hybrid Reward and Data Synthesis](https://arxiv.org/abs/2601.10318)
*Songsong Tian,Kongsheng Zhuo,Zhendong Wang,Rong Shen,Shengtao Zhang,Yong Wu*

Main category: cs.CL

TL;DR: 提出 BAR-SQL，一体化训练框架，融合边界感知与可解释性，使用 Seed Mutation 数据合成与 Knowledge-Grounded 推理，结合 SFT 与 RL 的混合奖励，在 Ent-SQL-Bench 上达到 91.48% 平均准确度，并公开代码与基准。


<details>
  <summary>Details</summary>
Motivation: 解决 NL2SQL 在可靠性、边界处理和可解释性方面的不足，提升在歧义、模式受限场景的鲁棒性。

Method: 核心方法包括 Seed Mutation 数据合成、Knowledge-Grounded Reasoning Synthesis（在 Chain-of-Thought 中锚定模式元数据与业务规则）、两阶段训练（SFT + Group Relative Policy Optimization RL）、任务条件混合奖励（结合 SQL 执行正确性、AST 分析、结果匹配及拒答的语义性）

Result: 在 Ent-SQL-Bench 上实现 91.48% 的平均准确度，超越 Claude 4.5 Sonnet、GPT-5 等前沿模型在 SQL 生成与边界拒答上的表现；公开代码与基准。

Conclusion: BAR-SQL 提供一个可解释、鲁棒且具边界拒绝能力的 NL2SQL 方案，适用于企业场景，实验结果显示其在可靠性与生成质量上具明显优势。

Abstract: In this paper, we present BAR-SQL (Boundary-Aware Reliable NL2SQL), a unified training framework that embeds reliability and boundary awareness directly into the generation process. We introduce a Seed Mutation data synthesis paradigm that constructs a representative enterprise corpus, explicitly encompassing multi-step analytical queries alongside boundary cases including ambiguity and schema limitations. To ensure interpretability, we employ Knowledge-Grounded Reasoning Synthesis, which produces Chain-of-Thought traces explicitly anchored in schema metadata and business rules. The model is trained through a two-stage process: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning via Group Relative Policy Optimization. We design a Task-Conditioned Hybrid Reward mechanism that simultaneously optimizes SQL execution accuracy-leveraging Abstract Syntax Tree analysis and dense result matching-and semantic precision in abstention responses. To evaluate reliability alongside generation accuracy, we construct and release Ent-SQL-Bench, which jointly assesse SQL precision and boundary-aware abstention across ambiguous and unanswerable queries. Experimental results on this benchmark demonstrate that BAR-SQL achieves 91.48% average accuracy, outperforming leading proprietary models, including Claude 4.5 Sonnet and GPT-5, in both SQL generation quality and boundary-aware abstention capability. The source code and benchmark are available anonymously at: https://github.com/TianSongS/BAR-SQL.

</details>


### [46] [Training-Trajectory-Aware Token Selection](https://arxiv.org/abs/2601.10348)
*Zhanming Shen,Jiaqi Hu,Zeyu Qin,Hao Chen,Wentao Ye,Zenan Huang,Yihong Zhuang,Guoshan Lu,Junlin Zhou,Junbo Zhao*

Main category: cs.CL

TL;DR: 提出一种基于训练轨迹的分段Token选择（T3S）的蒸馏方法，在强推理学生模型的前沿场景中解决连续蒸馏的瓶颈问题，通过在 Token 级别重构训练目标，避免“模仿锚点”Token 与尚未学习Token 的共存干扰，获得稳定的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在学生具备强推理能力的前沿情境下，简单的持续蒸馏往往效果有限甚至下降，且训练过程中损失下降但所有性能指标在一个瓶颈处同时急剧下降，表现出Token层面的竞争与干扰机制，需在Token层面进行更精细的蒸馏控制。

Method: 提出 Training-Trajectory-Aware Token Selection（T3S），按 Token 的训练轨迹重构训练目标：识别并分离已快速锚定的 Imitation-Anchor Tokens 与尚未学习的 Token，调整采样/损失权重，使尚未学习的 Token 在训练路径上得到清晰的优化通道，从而避免两类 Token 不能共存而导致的性能瓶颈。

Result: 在自回归(AR)和大语言模型加强微调(dLLM)场景中均实现稳定提升：仅需数百个示例，Qwen3-8B 在竞争性推理基准上超过 DeepSeek-R1，Qwen3-32B 靠近 Qwen3-235B，T3 训练的 LLaDA-2.0-Mini 超过 AR 基线，在所有 16B 规模“无思考”模型中达到最强性能。

Conclusion: Token 级别的训练轨迹感知选择能有效克服连续蒸馏中的瓶颈，提升强学生的蒸馏效率与效果，且具备跨 AR 与 dLLM 的泛化潜力。

Abstract: Efficient distillation is a key pathway for converting expensive reasoning capability into deployable efficiency, yet in the frontier regime where the student already has strong reasoning ability, naive continual distillation often yields limited gains or even degradation. We observe a characteristic training phenomenon: even as loss decreases monotonically, all performance metrics can drop sharply at almost the same bottleneck, before gradually recovering. We further uncover a token-level mechanism: confidence bifurcates into steadily increasing Imitation-Anchor Tokens that quickly anchor optimization and other yet-to-learn tokens whose confidence is suppressed until after the bottleneck. And the characteristic that these two types of tokens cannot coexist is the root cause of the failure in continual distillation. To this end, we propose Training-Trajectory-Aware Token Selection (T3S) to reconstruct the training objective at the token level, clearing the optimization path for yet-to-learn tokens. T3 yields consistent gains in both AR and dLLM settings: with only hundreds of examples, Qwen3-8B surpasses DeepSeek-R1 on competitive reasoning benchmarks, Qwen3-32B approaches Qwen3-235B, and T3-trained LLaDA-2.0-Mini exceeds its AR baseline, achieving state-of-the-art performance among all of 16B-scale no-think models.

</details>


### [47] [The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models](https://arxiv.org/abs/2601.10387)
*Christina Lu,Jack Gallagher,Jonathan Michala,Kyle Fish,Jack Lindsey*

Main category: cs.CL

TL;DR: 领导性的“Assistant Axis”主导了模型人物空间的默认行为；沿该轴的操控可放大帮助性与身份认同，而偏离会促使模型呈现非助手人物，极端偏离甚至出现神秘风格；通过限制在该轴上的激活区域可增稳行为并抑制人格基于的劫持攻击；需要更深层的训练/引导以实现更稳定的角色锚定。


<details>
  <summary>Details</summary>
Motivation: 理解模型人格的结构、为何模型会产生有害或怪异的行为以及如何通过锚定人格来提升稳定性和可控性。

Method: 在多种模型中提取对应多样化角色原型的激活方向；寻找人格空间的领先成分（主成分/轴线）形成“Assistant Axis”；对该轴进行操控以诱导不同人物设定；在对话场景中测量‘persona drift’并研究促发因素；测试将激活限定在固定区域的效果；对比分析预训练与后训练模型的表现。

Result: 发现存在主导性的“Assistant Axis”；向该轴靠近增强默认助手身份，提升有帮助与无害倾向；远离轴线则易呈现其他实体形象，极端偏离还可能呈现神秘化、戏剧化的表达；该轴在预训练模型中也存在，倾向促成有帮助的人类原型（如顾问、教练）并抑制灵性化风格；沿轴线的偏离程度可预测“persona drift”现象； drift 常由需要元反思模型过程的对话或情绪上脆弱的用户驱动；将激活限制在轴线的固定区域可在这些情景中稳定模型行为，也能抵御基于人格的对话式劫持攻击。

Conclusion: 后训练阶段的引导将模型锚定在一个区域内，但并未形成强绑定。为实现更稳健的一致性，需要更深入的训练/引导策略，使模型对统一的人格进行更紧密的锚定；可通过对轴线激活的区域限制等简单手段作为初步缓解，未来工作应探索更强的 personas 核心锚定方法。

Abstract: Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an "Assistant Axis," which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts "persona drift," a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.

</details>


### [48] [TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction](https://arxiv.org/abs/2601.10410)
*Mihai Dan Nadas,Laura Diosan,Andreea Tomescu,Andrei Piscoran*

Main category: cs.CL

TL;DR: 提出一个罗马尼亚语为中心的端到端语言建模管线 TF3-RO，从分词器设计到自训练、压缩、评估及大规模原生寓言数据生成，实现对罗马尼亚语的可重复小型化语言模型训练。


<details>
  <summary>Details</summary>
Motivation: 解决 morphologically rich 且资源不足的罗马尼亚语缺乏端到端、可重复的训练-评估-数据生成流水线；通过 linguistically informed 的分词器设计来缓解罗马尼亚语形态造成的标记膨胀，提升低资源场景下的模型训练与数据生成质量。

Method: 以 TF1/TF2 数据为基础，构建罗马尼亚特定的 BPE 与 Unigram 分词器；采用长序列打包的自从头训练，训练出 51.65M 参数的 LLaMA风格 Transformer；通过量化、结构化剪枝及基于 logits 的知识蒸馏得到 26.45M 参数、权重绑定的学生模型；在此模型上通过组合提示框架生成 300 万条罗马尼亚本地化寓言；建立综合评估套件（内在指标、形合一致性、实体连贯性、规则语法检查、LLM 评估）实现全流程评估。

Result: 得到一个紧凑的 26.45M 参数学生模型，具备良好部署特性；从头生成并筛选出 3,000,000 条罗马尼亚本地化寓言作为训练数据的原生来源；提供一个可重复、语言学驱动的管线，证实压缩、蒸馏与数据生成在低资源语言中的可行性与有效性。

Conclusion: TF3-RO 提供一个可重复且语言学扎实的端到端框架，用于训练紧凑的罗马尼亚语言模型并生成大规模合成叙事语料，凸显了面向形态丰富语言的分词设计、从头训练、模型压缩与评估整合的重要性。

Abstract: Recent advances in synthetic data generation have shown that compact language models can be trained effectively when the underlying corpus is structurally controlled and linguistically coherent. However, for morphologically rich and computationally under-resourced languages such as Romanian, there is still no openly documented, end-to-end pipeline that unifies tokenizer design, preprocessing, pretraining, compression, evaluation, and large-scale synthetic data generation in a reproducible framework. Building on TF1, a three-million-story English fable dataset, and TF2, which extends TF1 through high-quality Romanian translations, we introduce TF3-RO, a Romanian-centric language modeling pipeline spanning tokenizer training, from-scratch model development, and Romanian-native dataset generation. TF3-RO constructs Romanian-specific BPE and Unigram tokenizers from a linguistically informed corpus to mitigate token inflation induced by Romanian morphology. Using long-sequence packed training, we pretrain a 51.65M-parameter LLaMA-style Transformer entirely from scratch. The model is subsequently optimized through quantization, structured pruning, and logit-based knowledge distillation, yielding a compact 26.45M-parameter student model with tied embeddings and strong deployment characteristics. Using this distilled model, TF3-RO generates three million Romanian-native synthetic fables via a controlled combinatorial prompting framework. Across all stages, the pipeline integrates a comprehensive evaluation suite combining intrinsic metrics, Romanian agreement probes, entity coherence, rule-based grammar checking, and LLM-based assessment. TF3-RO provides a reproducible and linguistically grounded framework for training compact Romanian language models and producing large-scale synthetic narrative corpora.

</details>


### [49] [Are Language Models Models?](https://arxiv.org/abs/2601.10421)
*Philip Resnik*

Main category: cs.CL

TL;DR: 通过对Marr三层分析评估Futrell与Mahowald关于语言模型的主张。结论显示LMs并非认知模型，反而更适合作为工具，且在各层级的论证均存在不足。


<details>
  <summary>Details</summary>
Motivation: 检验将LMs视为“模型系统”的主张是否能在Marr三层分析下成立。

Method: 将Marr的实现层、算法/表征层、计算理论层作为评估框架，对原文主张逐层检验。

Result: 实现层：结论显然不成立；算法-表征层：动机不足；计算理论层：存在问题。总体而言，LM更适合作为工具，过度将其称为认知模型会放大LLM hype。

Conclusion: LM应被视为工具而非认知模型，避免对其认知地位的过度推广。

Abstract: Futrell and Mahowald claim LMs "serve as model systems", but an assessment at each of Marr's three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.

</details>


### [50] [SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability](https://arxiv.org/abs/2601.10455)
*Ruochen Li,Kun Yuan,Yufei Xia,Yue Zhou,Qingyu Lu,Weihang Li,Youxiang Zhu,Nassir Navab*

Main category: cs.CL

TL;DR: 将手术规划中的“计划正确性”定义为阶段目标可满足性，并提出基于规则的高精度元评估框架，对 Video-LLMs 的评估表明仅靠序列相似性不能可靠评估规划质量；结构化知识高于语义引导。


<details>
  <summary>Details</summary>
Motivation: 解决当下对视觉-语言模型在安全关键任务中的评估与实际规划正确性不一致的问题，防止对无效或低质量计划的误判。

Method: 构建多中心元评估基准，包含有效的程序变体与含有顺序/内容错误的无效计划；比较序列相似性等常用评估指标；提出基于规则的目标可满足性指标作为高精度参考；在受限设置下评估 Video-LLMs，并分析感知错误、推理不足等原因；比较结构化知识与语义引导的影响。

Result: 序列相似性指标系统性地错误评估，惩罚有效计划、忽视无效计划；基于规则的目标可满足性指标提供高精度评估参考；结构化知识显著提升性能，单纯的语义引导不可靠，且只有在与结构约束结合时，才有利于更大模型；在感知错误和推理受限的情境下，模型表现下降。

Conclusion: 提出以目标可满足性为核心的高精度元评估框架，并强调在安全关键任务中将结构化知识融入模型设计与评估的重要性，警惕仅依赖语义线索或序列匹配的评估方法。

Abstract: Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.

</details>


### [51] [Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models](https://arxiv.org/abs/2601.10460)
*Abhinaba Basu,Pavan Chakraborty*

Main category: cs.CL

TL;DR: Contextual StereoSet and CSF show bias is highly context-sensitive; fixed-condition tests underestimate robustness and generalization across framing contexts.


<details>
  <summary>Details</summary>
Motivation: To assess whether bias measurements in LLMs are stable across contextual framing, highlighting a potential fragility in standard benchmarks.

Method: Introduce Contextual StereoSet by fixing stereotype content while varying contextual framing; test 13 models across two protocols; analyze with Context Sensitivity Fingerprints (CSF) using bootstrap CIs and FDR; provide a 360-context diagnostic grid and a production-screen 4,229-item protocol.

Result: Framing shifts bias consistently: anchoring to 1990 vs 2030 raises stereotype selection; gossip framing elevates bias in most models; out-group observer framing shifts bias up to 13 percentage points; effects replicate in hiring, lending, and help-seeking vignettes; CSF yields compact per-dimension dispersion profiles; large-scale screening possible.

Conclusion: Fixed-condition bias scores do not necessarily generalize; this work emphasizes evaluation robustness and context sensitivity; benchmarks and code are released to enable broader stress-testing.

Abstract: A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.
  We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.
  We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.
  The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, "Under what conditions does bias appear?" rather than "Is this model biased?" We release our benchmark, code, and results.

</details>


### [52] [AEQ-Bench: Measuring Empathy of Omni-Modal Large Models](https://arxiv.org/abs/2601.10513)
*Xuan Luo,Lewei Yao,Libo Zhao,Lanqing Hong,Kai Chen,Dehua Tao,Daxin Tan,Ruifeng Xu,Jing Li*

Main category: cs.CL

TL;DR: 提出 AEQ-Bench，用于评估 omni-modal 大模型的同理心，聚焦音频+文本输入的共情理解与对音频输出的同理心 judgement；结果显示具音频输出能力的模型优于文本输出模型，但在粗粒度与细粒度的评估上存在差异。


<details>
  <summary>Details</summary>
Motivation: 解决对同理心的情感性评估困难，缺乏系统化基准来评估多模态输入下的同理心生成与评估能力。

Method: 提出 AEQ-Bench，包含两种情境设定（与上下文相关性、讲话语气变体），在语言与音/paralinguistic 方面进行综合评估；比较具音频输出与文本输出的 OLMs。

Result: 音频输出的 OLMs 效果优于文本输出模型；在粗粒度质量评估上与人类判断对齐，但在细粒度的语音表达（paralinguistic）方面仍不可靠。

Conclusion: AEQ-Bench 提供系统化评估框架并揭示当前 OLMs 在细粒度 paralinguistic 表达方面的局限，强调音频输出能力的潜在优势。

Abstract: While the automatic evaluation of omni-modal large models (OLMs) is essential, assessing empathy remains a significant challenge due to its inherent affectivity. To investigate this challenge, we introduce AEQ-Bench (Audio Empathy Quotient Benchmark), a novel benchmark to systematically assess two core empathetic capabilities of OLMs: (i) generating empathetic responses by comprehending affective cues from multi-modal inputs (audio + text), and (ii) judging the empathy of audio responses without relying on text transcription. Compared to existing benchmarks, AEQ-Bench incorporates two novel settings that vary in context specificity and speech tone. Comprehensive assessment across linguistic and paralinguistic metrics reveals that (1) OLMs trained with audio output capabilities generally outperformed models with text-only outputs, and (2) while OLMs align with human judgments for coarse-grained quality assessment, they remain unreliable for evaluating fine-grained paralinguistic expressiveness.

</details>


### [53] [PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models](https://arxiv.org/abs/2601.10532)
*Chengbing Wang,Wuqiang Zheng,Yang Zhang,Fengbin Zhu,Junyi Cheng,Yi Xie,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 提出以心理学为基础的双向同理心奖励建模（PERM），将同理心评估拆分为支持者、寻求者和旁观者三个维度，并在情商基准和工业对话数据集上显著优于基线，用户盲测中获得70%偏好，且公开代码数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习奖励模型往往仅从单一角度评估同理心，未充分体现同理心的双向互动特性（依据 Empathy Cycle 理论）。为提升人机对话中的情感支持质量，需要覆盖支持者的共振与表达、寻求者的情感接收，以及旁观者的整体互动质量评估。

Method: 提出 PERM 框架，将同理心评估分解为三类互补的奖励信号：1) 支持者视角—对内部共振与对话表达的评估；2) 寻求者视角—对情感接受度的评估；3) 旁观者视角—对整体互动质量的监控。结合这些信号进行强化学习/奖励建模以引导更具同理心的回复。

Result: 在广泛使用的情商基准和一个工业日常对话数据集上，PERM 比最新的基线提升超过 10%；盲测用户中有 70% 偏好 PERM 生成的回答。

Conclusion: 基于心理学的双向同理心奖励建模可显著提升 LLM 的同理心生成质量，适用于提升人机交互中的情感支持水平；代码、数据集与模型已公开。

Abstract: Large Language Models (LLMs) are increasingly deployed in human-centric applications, yet they often fail to provide substantive emotional support. While Reinforcement Learning (RL) has been utilized to enhance empathy of LLMs, existing reward models typically evaluate empathy from a single perspective, overlooking the inherently bidirectional interaction nature of empathy between the supporter and seeker as defined by Empathy Cycle theory. To address this limitation, we propose Psychology-grounded Empathetic Reward Modeling (PERM). PERM operationalizes empathy evaluation through a bidirectional decomposition: 1) Supporter perspective, assessing internal resonation and communicative expression; 2) Seeker perspective, evaluating emotional reception. Additionally, it incorporates a bystander perspective to monitor overall interaction quality. Extensive experiments on a widely-used emotional intelligence benchmark and an industrial daily conversation dataset demonstrate that PERM outperforms state-of-the-art baselines by over 10\%. Furthermore, a blinded user study reveals a 70\% preference for our approach, highlighting its efficacy in generating more empathetic responses. Our code, dataset, and models are available at https://github.com/ZhengWwwq/PERM.

</details>


### [54] [Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure](https://arxiv.org/abs/2601.10566)
*Syed Naveed Mahmood,Md. Rezaur Rahman Bhuiyan,Tasfia Zaman,Jareen Tasneem Khondaker,Md. Sameer Sakib,Nazia Tasnim,Farig Sadeque*

Main category: cs.CL

TL;DR: 提出知识免疫框架（KIF），通过面向表示的内部激活签名进行知识“免疫”以实现耐久的去学习，结合动态抑制与参数高效适配，在不重新训练整个模型的前提下实现接近 Oracle 的消除效果，同时保持一定的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有去学习方法将行为抑制与真正知识移除混淆的问题；GDPR 合规和模型安全要求对隐性知识进行真实删除，而非表层拒绝。

Method: 提出 KIF，基于表示的架构，定位并抑制与主题相关的内部表示，配合参数高效适配实现对模型的局部改造；并建立双指标评估协议同时衡量表层泄漏与潜在痕迹，避免表面可见拒绝掩盖真实知识。

Result: 在标准（Llama、Mistral）与推理优先（Qwen、DeepSeek）模型，规模从 3B 到 14B，取得近乎 Oracle 的消除率（FQ 约 0.99，目标 1.00），同时下游效用保持（MU 约 0.62），表明打破了以往的稳定性-去学习权衡；标准模型呈现尺度无关的真正删除，推理优先模型则揭示架构层面的差异。

Conclusion: KIF 提供一种可持续的去学习方案，免于全面再培训，并提供系统诊断机制以跨模型家族与规模进行机制层级遗忘行为的比较。

Abstract: Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface outputs. Our approach combines dynamic suppression of subject-specific representations with parameter-efficient adaptation, enabling durable unlearning without full model retraining. KIF achieves near-oracle erasure (FQ approx 0.99 vs. 1.00) while preserving utility at oracle levels (MU = 0.62), effectively breaking the stability-erasure tradeoff that has constrained all prior work. We evaluate both standard foundation models (Llama and Mistral) and reasoning-prior models (Qwen and DeepSeek) across 3B to 14B parameters. Our observation shows that standard models exhibit scale-independent true erasure (<3% utility drift), while reasoning-prior models reveal fundamental architectural divergence. Our comprehensive dual-metric evaluation protocol, combining surface-level leakage with latent trace persistence, operationalizes the obfuscation - erasure distinction and enables the first systematic diagnosis of mechanism-level forgetting behavior across model families and scales.

</details>


### [55] [Form and Meaning in Intrinsic Multilingual Evaluations](https://arxiv.org/abs/2601.10580)
*Wessel Poelman,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文质疑在多语言条件语言模型中常用的内在评估指标（如困惑度、比特/字符）的可比性，指出并行语料上的信息量不必然相同，因此信息论意义的度量与语义等价性并不等同。通过在两份多并行语料上对六种指标进行对比实验，发现现有指标并非普遍可比，并尝试以形式-意义辩论来解释差异。


<details>
  <summary>Details</summary>
Motivation: 动机在于揭示单语言环境中可行且直观的内在评估指标，在跨语言/多语言设置中可能基于不成立的假设（如并行句子具有相同信息量），从而质疑其可比性。

Method: 方法包括在两个多并行语料库上，对六种评价指标进行实验，比较单语与多语模型的表现，考察指标之间的一致性、相关性及可比性，并结合形式-意义的讨论进行解释。

Result: 结果显示：现有内在指标在跨语言与跨模型场景下并非普遍可比，六种指标之间的关系受语言、翻译单元、信息量的测度方式等因素影响，难以直接按相同准则比较。

Conclusion: 结论指出需要对指标背后的假设进行显性化，尤其信息量是否在不同语言之间等同的问题；从形式-意义的角度解释差异，提示在跨语言比较中应谨慎解读指标，并推动更鲁棒的跨语言评估框架。

Abstract: Intrinsic evaluation metrics for conditional language models, such as perplexity or bits-per-character, are widely used in both mono- and multilingual settings. These metrics are rather straightforward to use and compare in monolingual setups, but rest on a number of assumptions in multilingual setups. One such assumption is that comparing the perplexity of CLMs on parallel sentences is indicative of their quality since the information content (here understood as the semantic meaning) is the same. However, the metrics are inherently measuring information content in the information-theoretic sense. We make this and other such assumptions explicit and discuss their implications. We perform experiments with six metrics on two multi-parallel corpora both with mono- and multilingual models. Ultimately, we find that current metrics are not universally comparable. We look at the form-meaning debate to provide some explanation for this.

</details>


### [56] [Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs](https://arxiv.org/abs/2601.10645)
*Yuxi Xia,Loris Schoenegger,Benjamin Roth*

Main category: cs.CL

TL;DR: 提出 TracVC 来追踪 LLM 置信表达的来源，并引入 content groundness 指标；在 OLMo、Llama 的问答场景中发现模型常被无关训练数据影响，难以实现基于内容的置信 grounding。


<details>
  <summary>Details</summary>
Motivation: 解决置信表达的可信性问题：尽管模型会 verbalize 自信，但其置信度往往与事实不对齐。需要理解置信的来源以提升输出的可信性与可解释性。

Method: 提出 TracVC，结合信息检索与影响估计来追踪生成的置信表达回训练数据，提出 content groundness 度量，评估训练样本中与问题-答案相关的内容对置信表达的支撑程度。

Result: 在 OLMo2-13B 上，置信相关数据常常与查询在词汇上无关，表明模型可能在模仿自信的语言表达而非做出内容 grounded 的判断；指出当前训练范式的根本局限，即模型可能学会“会显得自信”，但不一定知道何时自信是正当的。

Conclusion: TracVC 提供一个诊断性框架，用于改进训练以提高置信表达的内容 grounding，从而提升 LLM 输出的可信度。

Abstract: Large language models (LLMs) can increase users' perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\textbf{Trac}ing \textbf{V}erbalized \textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs' trustworthiness in expressing more reliable confidence.

</details>


### [57] [Detecting Winning Arguments with Large Language Models and Persuasion Strategies](https://arxiv.org/abs/2601.10660)
*Tiziano Labruna,Arkadiusz Modzelewski,Giorgio Satta,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 提出基于多策略说服评分的LLM框架，用分策略推理预测文本的说服力；在三个数据集上取得改进，并对Winning Arguments按主题分析后公开数据集，强调可解释性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解说服性文本中的策略性因素，单纯内容或总体特征难以解释说服力，需策略层面的分析以提升可解释性与鲁棒性。

Method: 使用大型语言模型进行策略导向的推理，引导覆盖六种说服策略（如对名誉的攻击、分散注意、操纵性措辞等），进行分策略的说服力评分；在Winning Arguments、Anthropic/Persuasion、Persuasion for Good数据集上评估；对Winning Arguments按主题划分并分析跨主题表现，公开主题注释数据集。

Result: 策略导向推理提升了说服力预测的性能；在不同主题下的表现有差异，提供更细粒度的解释能力；并公开主题标注数据集以便复现与后续研究。

Conclusion: 结构化、策略感知的提示与推理框架有助于提升论证质量评估的可解释性与鲁棒性，适合说服性文本的分析研究。

Abstract: Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning over six persuasion strategies. Results show that strategy-guided reasoning improves the prediction of persuasiveness. To better understand the influence of content, we organize the Winning Argument dataset into broad discussion topics and analyze performance across them. We publicly release this topic-annotated version of the dataset to facilitate future research. Overall, our methodology demonstrates the value of structured, strategy-aware prompting for enhancing interpretability and robustness in argument quality assessment.

</details>


### [58] [LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals](https://arxiv.org/abs/2601.10700)
*Gilat Toker,Nitay Calderon,Ohad Amosy,Roi Reichart*

Main category: cs.CL

TL;DR: 提出 LIBERTy 框架，基于结构化因果模型的文本生成干预数据集及新评估指标，用于评估概念级解释的可信性；通过对三个领域数据集构建对照的因果反事实对，揭示模型对概念干预的敏感性和现有方法的改进空间。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，解释的可信性至关重要，但现有基准依赖高成本的人类反事实；需要可扩展、可控的基准来系统评估概念解释的忠诚度。

Method: 基于结构化因果模型(SCM)对文本生成过程建模，对概念进行干预，沿 SCM 传播直到生成对照反事实。提供 LIBERTy 框架及三个数据集（疾病检测、CV 筛查、 workplace violence 预测）和一个新评价指标 order-faithfulness。对五个模型进行跨模型评估。

Result: 发现方法在概念解释的忠诚度方面存在较大改进空间；在不同模型间存在差异，未达一致性；专有 LLM 对人口统计概念的敏感性显著降低，可能是因为后训练缓解。

Conclusion: LIBERTy 为开发更忠诚的解释方法提供了急需的基准，并能系统分析模型对干预的敏感性，推动高风险领域的可解释性研究。

Abstract: Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.

</details>


### [59] [MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching](https://arxiv.org/abs/2601.10712)
*Changle Qu,Sunhao Dai,Hengyi Cai,Jun Xu,Shuaiqiang Wang,Dawei Yin*

Main category: cs.CL

TL;DR: MatchTIR通过引入细粒度的轮级回报分配和双层优势估计，改进工具集成推理中的信用分配问题。通过将预测轨迹与ground-truth轨迹进行二部匹配，得到密集的轮级奖励，并在局部轮次与全局任务成功之间进行权衡。实验证明在三个基准上优于对比，大幅提升长程多轮任务的表现，4B模型接近甚至超过多数8B对手，且代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在工具集成推理（TIR）中多采用结果级或轨迹级奖励，无法区分有效与冗余的工具调用，尤其在长时程多轮任务中难以做出精准信用分配。需要细粒度的轮级监督与局部-全局信号的结合以提升性能与样本效率。

Method: 将信用分配建模为预测轨迹与 ground-truth 轨迹之间的二部匹配问题，提出两种分配策略以获得密集的轮级奖励；引入双层优势估计，将轮级信号与轨迹级信号融合，为单轮交互分配不同的优势值。

Result: 在三个基准任务上，MatchTIR展现出优越性能；4B模型在长程与多轮任务中明显优于大多数8B对手，表明方法的有效性与可扩展性。

Conclusion: 通过细粒度轮级信用分配与双层优势估计，提升了TIR的样本效率和任务完成度，尤其在长程多轮场景下具有显著优势，且在不同任务规模上具备竞争力。

Abstract: Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [Social Determinants of Health Prediction for ICD-9 Code with Reasoning Models](https://arxiv.org/abs/2601.09709)
*Sharim Khan,Paul Landes,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: 在 MIMIC-III 的住院记录上对社会健康决定因素（SDoH）进行 ICD-9 多标签分类，比较推理模型与传统大语言模型，达到 89% F1；揭示 139 例缺失编码并提供复现代码。


<details>
  <summary>Details</summary>
Motivation: SDoH 与患者结局相关，但往往未在结构化数据中体现，需要从临床文本中自动提取以补充诊断系统的知识；尽管大语言模型在句级 SDoH 提取上表现良好，但处理长距离依赖的住院记录具有挑战。

Method: 将住院记录的多标签 SDoH ICD-9 编码作为分类任务，使用 MIMIC-III 数据集，比较推理模型与传统大语言模型的效果；基于现有 ICD-9 编码进行预测，并报告 missing SDoH 编码的情况及复现代码。

Result: 实现 89% 的 F1 分数；发现 139 例缺失的 SDoH 编码；提供可复现的代码以支持后续研究。

Conclusion: 表明在住院情境下通过文本挖掘 SDoH 的可行性与潜在价值，可辅助改进诊断与照护决策；同时暴露数据质量问题（编码缺失），并提供复现资源促进研究推进。

Abstract: Social Determinants of Health correlate with patient outcomes but are rarely captured in structured data. Recent attention has been given to automatically extracting these markers from clinical text to supplement diagnostic systems with knowledge of patients' social circumstances. Large language models demonstrate strong performance in identifying Social Determinants of Health labels from sentences. However, prediction in large admissions or longitudinal notes is challenging given long distance dependencies. In this paper, we explore hospital admission multi-label Social Determinants of Health ICD-9 code classification on the MIMIC-III dataset using reasoning models and traditional large language models. We exploit existing ICD-9 codes for prediction on admissions, which achieved an 89% F1. Our contributions include our findings, missing SDoH codes in 139 admissions, and code to reproduce the results.

</details>


### [61] [The Geometry of Thought: Disclosing the Transformer as a Tropical Polynomial Circuit](https://arxiv.org/abs/2601.09775)
*Faruk Alpay,Bilge Senturk*

Main category: cs.LG

TL;DR: Transformers reach a tropical (max-plus) regime as β→∞; softmax attention becomes tropical matrix product, so the forward pass implements a dynamic-programming recurrence (Bellman-Ford-like) on a latent similarity graph, linking chain-of-thought to shortest/longest path computations.


<details>
  <summary>Details</summary>
Motivation: Provide a geometric/computational explanation for emergent reasoning-like behavior in Transformers by connecting attention mechanics to well-known DP/path-finding algorithms.

Method: Take the tropical (β→∞) limit of softmax attention, show equivalence to a tropical matrix product, and interpret the forward pass as a Bellman-Ford-like path-update on a latent token-similarity graph.

Result: A formal demonstration that high-temperature attention performs a dynamic programming recurrence on a graph induced by token similarities, offering a geometric perspective on mechanism behind chain-of-thought ratioci (reasoning) in Transformers.

Conclusion: The work reveals that intrinsic DP/shortest-path computations underlie Transformer forward passes, providing a new lens to view attention and reasoning in neural networks.

Abstract: We prove that the Transformer self-attention mechanism in the high-confidence regime ($β\to \infty$, where $β$ is an inverse temperature) operates in the tropical semiring (max-plus algebra). In particular, we show that taking the tropical limit of the softmax attention converts it into a tropical matrix product. This reveals that the Transformer's forward pass is effectively executing a dynamic programming recurrence (specifically, a Bellman-Ford path-finding update) on a latent graph defined by token similarities. Our theoretical result provides a new geometric perspective for chain-of-thought reasoning: it emerges from an inherent shortest-path (or longest-path) algorithm being carried out within the network's computation.

</details>


### [62] [TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models](https://arxiv.org/abs/2601.09776)
*Khalid Oublal,Quentin Bouniot,Qi Gan,Stephan Clémençon,Zeynep Akata*

Main category: cs.LG

TL;DR: TimeSAE 框架结合稀疏自编码器与因果性视角，为时间序列黑箱模型提供跨分布的可信解释，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景中，解释必须具备跨分布泛化能力。然而现有方法多为分布内解释，对训练分布偏移敏感，限制实际应用。

Method: 提出 TimeSAE，将稀疏自编码器用于学习稀疏、可解释的表示，并引入因果性约束/分析以增强解释的稳健性和跨分布泛化能力；在合成与真实数据集上与主流基线对比，评估解释的保真度和鲁棒性；并提供 TimeSAE-Lib 实现。

Result: TimeSAE 在保真性和鲁棒性方面优于多项基线，在分布偏移条件下表现更稳健，且通过定量指标和定性分析得到支持。

Conclusion: TimeSAE 为时间序列黑箱解释提供一个可泛化的框架，适用于需要可信解释的场景，并具备可落地实现（代码开放）。

Abstract: As black box models and pretrained models gain traction in time series applications, understanding and explaining their predictions becomes increasingly vital, especially in high-stakes domains where interpretability and trust are essential. However, most of the existing methods involve only in-distribution explanation, and do not generalize outside the training support, which requires the learning capability of generalization. In this work, we aim to provide a framework to explain black-box models for time series data through the dual lenses of Sparse Autoencoders (SAEs) and causality. We show that many current explanation methods are sensitive to distributional shifts, limiting their effectiveness in real-world scenarios. Building on the concept of Sparse Autoencoder, we introduce TimeSAE, a framework for black-box model explanation. We conduct extensive evaluations of TimeSAE on both synthetic and real-world time series datasets, comparing it to leading baselines. The results, supported by both quantitative metrics and qualitative insights, show that TimeSAE provides more faithful and robust explanations. Our code is available in an easy-to-use library TimeSAE-Lib: https://anonymous.4open.science/w/TimeSAE-571D/.

</details>


### [63] [Eluder dimension: localise it!](https://arxiv.org/abs/2601.09825)
*Alireza Bakhtiari,Alex Ayoub,Samuel Robertson,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 提出了对广义线性模型类的eluder维度的下界，表明标准eluder维度分析难以获得一阶奖励界限。引入eluder维度的局部化方法，能够直接回收并改进伯努利赌博机的经典结果，并首次给出有限时序、总回报有界的RL任务的一阶（data-dependent）奖励界限。


<details>
  <summary>Details</summary>
Motivation: 现有的基于eluder维度的分析对第一阶（以最佳策略回报为尺度的）后悔界限常常无效或过于保守，需要更细粒度的局部几何/结构信息来实现真正的一阶界限。

Method: 提出eluder维度的局部化（localisation）方法，结合对广义线性模型的结构分析，给出对eluder维度的下界；利用局部化思路在伯努利 bandits 情况下直接得到改进的经典结论，并将其扩展到有限时序、回报有界的RL任务，获得第一批真正的一阶（data-dependent）后悔界限。

Result: 证实了标准eluder维度不能产生一阶后悔界限的下界；通过局部化方法，能回收并改进伯努利带臂问题的结果，并实现对有限时序RL的第一类一阶后悔界限。

Conclusion: 局部化eluder维度提供了对模型类几何的更精细刻画，使得在GLM及RL等设定下可以获得数据相关的、一阶的后悔界限，扩展了eluder维度分析的适用性并指出原有方法的局限。

Abstract: We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.

</details>


### [64] [A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch](https://arxiv.org/abs/2601.09831)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: cs.LG

TL;DR: 首次给出在先验不匹配条件下的PnP-PGD收敛性理论，显著放宽了以往理论中的限制性和不可验证假设。


<details>
  <summary>Details</summary>
Motivation: 解决在去噪器训练数据分布与推断任务数据分布不一致时，PnP-PGD的收敛性理论缺失的问题，提升理论可信度及实用性。

Method: 提出新的收敛分析框架，放宽对关键假设的要求，证明在先验不匹配情形下的收敛性；相比现有结果，减少对严格且不可验证条件的依赖。

Result: 首次证明在先验不匹配下的PnP-PGD收敛性，并给出相对宽松的收敛条件。

Conclusion: 建立了在先验分布不一致时的PnP-PGD理论基础，提升 denoiser 在不同数据分布下的应用理论保障。

Abstract: In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions.

</details>


### [65] [A pipeline for enabling path-specific causal fairness in observational health data](https://arxiv.org/abs/2601.09841)
*Aparajita Kashyap,Sara Matijevic,Noémie Elhadad,Steven A. Kushner,Shalmali Joshi*

Main category: cs.LG

TL;DR: 提出一个面向医疗场景的路径特异因果公平管道，以训练具有因果公平性的模型；把结构性公平映射到观察性健康数据，区分直接与间接偏见；展示如何利用无公平约束的基础模型生成因果公平的下游预测；提供一个模型无关的泛化管线。


<details>
  <summary>Details</summary>
Motivation: 在医疗应用中，现有公平定义往往未能充分捕捉直接偏见与通过获取/就诊机会差异等间接偏见。路径特异因果公平能更好地嵌入社会与医疗上下文，清晰区分 biases 的来源与影响。

Method: 将结构性公平模型映射到观察性健康数据，构建一个可泛化、以医疗情境与 disparities 为导向的训练管线，用以定义并实现目标“公平”模型；提供一个在无公平约束的基础模型上，利用已知社会与医疗差异任务来产出因果公平的下游预测的模型无关流程；强调直接与间接偏见的分离与综合考量。

Result: 提出一个模型无关、可应用于多任务的因果公平训练管线；演示如何从基础模型出发，在存在社会和医疗差异的任务中生成因果公平的预测，并对 fairness–accuracy 权衡进行解耦与讨论。

Conclusion: 给出一个面向真实医疗场景的通用因果公平训练框架，明确区分直接与间接偏差，并展示如何利用无偏见约束的基础模型产出因果公平的下游预测，为医疗机器学习的公平性研究提供可复用的流程与方法。

Abstract: When training machine learning (ML) models for potential deployment in a healthcare setting, it is essential to ensure that they do not replicate or exacerbate existing healthcare biases. Although many definitions of fairness exist, we focus on path-specific causal fairness, which allows us to better consider the social and medical contexts in which biases occur (e.g., direct discrimination by a clinician or model versus bias due to differential access to the healthcare system) and to characterize how these biases may appear in learned models. In this work, we map the structural fairness model to the observational healthcare setting and create a generalizable pipeline for training causally fair models. The pipeline explicitly considers specific healthcare context and disparities to define a target "fair" model. Our work fills two major gaps: first, we expand on characterizations of the "fairness-accuracy" tradeoff by detangling direct and indirect sources of bias and jointly presenting these fairness considerations alongside considerations of accuracy in the context of broadly known biases. Second, we demonstrate how a foundation model trained without fairness constraints on observational health data can be leveraged to generate causally fair downstream predictions in tasks with known social and medical disparities. This work presents a model-agnostic pipeline for training causally fair machine learning models that address both direct and indirect forms of healthcare bias.

</details>


### [66] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://arxiv.org/abs/2601.09865)
*Jacob Sander,Brian Jalaian,Venkat R. Dasari*

Main category: cs.LG

TL;DR: 一个将 GPTQ 量化、LoRA 适配和数据蒸馏结合的综合框架，用于边缘设备部署的高效大语言模型，在保持或提升任务性能的同时实现最大约 2× 的内存压缩；Muon 优化器在量化过程中的精度衰减抑制方面尤为有效。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源受限（计算、内存、能耗），需要在模型尺寸、推理速度与任务性能之间取得平衡；单一的量化、微调或蒸馏方法往往无法同时最大化压缩率和保留性能。

Method: 将基于 GPTQ 的后量化量化、LoRA 低秩自适应，以及专用的数据蒸馏流程相结合；通过知识蒸馏（KL 散度）、贝叶斯超参数优化和 Muon 优化器来引导量化和微调，形成端到端的压缩-微调流水线。

Result: 实现了最高约 2× 的内存压缩（如将 6GB 模型压缩至 3GB），并在专门任务上实现比单独 GPTQ 量化更优的性能；Muon 优化器显著提升微调模型在量化过程中的精度衰减抗性。

Conclusion: 该整合框架为资源受限设备上的高效、任务可定制的 LLM 部署提供了有效途径，在保持或提升任务性能的同时显著降低模型大小与推理资源需求。

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires addressing three key challenges: acquiring task-specific data, fine-tuning for performance, and compressing models to accelerate inference while reducing resource demands. We propose an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2x memory compression (e.g., reducing a 6GB model to 3GB) and enables efficient inference for specialized tasks. Empirical results demonstrate superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer notably enhancing fine-tuned models' resistance to accuracy decay during quantization.

</details>


### [67] [The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation](https://arxiv.org/abs/2601.09926)
*Kirandeep Kaur,Vinayak Gupta,Aditya Gupta,Chirag Shah*

Main category: cs.LG

TL;DR: ProPer提出一种两智能体框架（DGA、RGA）来主动发现并满足未表达的用户需求，提升跨域对话的主动性与质量。


<details>
  <summary>Details</summary>
Motivation: 解决语言助手的被动、需明确表达需求的局限性；现有主动系统要么增加用户负担、要么对上下文推断不准，导致干预时机和相关性不足.

Method: DGA基于微调后的大模型，利用显式用户数据生成多维度的隐性需求/知识缺口，并通过质量、多样性和任务相关性进行重排序筛选；RGA在显性与隐性维度之间进行权衡，生成个性化、具主动性的响应。评估采用结构化的缝隙感知量化指标，覆盖度、主动性适切性、意图对齐等。

Result: 在多个领域中，ProPer提升了质量分数和胜率，在单轮评估中最高可达84%的增益，在多轮交互中保持稳定优势.

Conclusion: 通过将维度生成与选择性重排以及显性-隐性信息的平衡结合，ProPer能够提供更高质量且更具前瞻性的个性化帮助，且在不同域的任务中表现出显著的改进。

Abstract: Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.

</details>


### [68] [Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains](https://arxiv.org/abs/2601.09946)
*Chenxi Qiu*

Main category: cs.LG

TL;DR: Interpolation-based lp-norm mDP framework for fine-grained domains using anchor-point perturbations and log-convex interpolation, with sequential 1D decomposition to enforce lp-norm constraints and joint budget optimization; shows strong utility and rigorous privacy on real location data.


<details>
  <summary>Details</summary>
Motivation: Need scalable mDP for fine-grained/continuous domains where dense perturbation matrices are costly; existing optimization-based methods struggle with computational and constraint issues; mDP requires distance-aware privacy.

Method: Optimize perturbations at sparse anchor points; interpolate at non-anchor points via log-convex combinations that preserve mDP; decompose interpolation into sequential 1D steps with a corrected formulation; optionally jointly optimize perturbations and per-dimension budget allocation.

Result: Empirical results on real-world location datasets show rigorous privacy guarantees and competitive utility, outperforming baseline mechanisms.

Conclusion: Interpolation-based framework enables scalable, principled lp-norm mDP in high-dimensional settings, ensuring privacy by design and enabling better utility through joint budget optimization; outperforms baselines.

Abstract: Metric Differential Privacy (mDP) generalizes Local Differential Privacy (LDP) by adapting privacy guarantees based on pairwise distances, enabling context-aware protection and improved utility. While existing optimization-based methods reduce utility loss effectively in coarse-grained domains, optimizing mDP in fine-grained or continuous settings remains challenging due to the computational cost of constructing dense perterubation matrices and satisfying pointwise constraints.
  In this paper, we propose an interpolation-based framework for optimizing lp-norm mDP in such domains. Our approach optimizes perturbation distributions at a sparse set of anchor points and interpolates distributions at non-anchor locations via log-convex combinations, which provably preserve mDP. To address privacy violations caused by naive interpolation in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms. in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms.

</details>


### [69] [Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series](https://arxiv.org/abs/2601.09949)
*Griffin Kearney*

Main category: cs.LG

TL;DR: 提出一种基于连续时间的令牌化方法Kinematic Tokenization，通过从带噪声的测量构建显式样条并对局部样条系数（位置、速度、加速度、jerk）进行离散化，提升在噪声时间序列上的可学习性和策略校准，尤其在带不对称惩罚的弃权任务中表现优于离散令牌基线。


<details>
  <summary>Details</summary>
Motivation: 离散令牌在低信噪比场景下易脆弱，且下游目标的非对称惩罚会促使模型选择弃权。需要一种连续时间、对噪声鲁棒且便于策略学习的表示，以提高可学习性与校准性。

Method: 提出优化驱动的连续时间表示，通过从噪声测量重建显式样条，并对局部样条系数（位置、速度、加速度、jerk）进行令牌化。将该方法应用于金融时间序列（资产价格及成交量分布）的多资产日内测试床，采用风险规避的非对称分类目标作为压力测试。

Result: 在该不对称损失下，若干离散基线收敛至吸收性现金策略（Liquidation Equilibrium），而连续样条令牌保持经过校准且非平凡的行动分布与稳定策略。

Conclusion: 显式连续时间令牌化可提升在带噪声的时间序列上学习性和策略校准，特别是在存在弃权惩罚的情形下。

Abstract: Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations (raw values, patches, finite differences) can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties that rationally encourage abstention. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). This is applied to financial time series data in the form of asset prices in conjunction with trading volume profiles. Across a multi-asset daily-equity testbed, we use a risk-averse asymmetric classification objective as a stress test for learnability. Under this objective, several discrete baselines collapse to an absorbing cash policy (the Liquidation Equilibrium), whereas the continuous spline tokens sustain calibrated, non-trivial action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.

</details>


### [70] [FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems](https://arxiv.org/abs/2601.09985)
*Tianqi Zhang,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FaTRQ introduces a far-memory-aware refinement for ANNS that uses tiered memory, progressive distance estimation, and ternary residual quantization to avoid loading full vectors from storage, improving storage efficiency by 2.4x and throughput by up to 9x over SOTA GPU ANNS.


<details>
  <summary>Details</summary>
Motivation: Reduces the latency bottleneck in second-pass refinement for ANNS, which currently requires reading full-precision vectors from slow storage (e.g., SSDs) and dominates query time for modern text/multimodal embeddings.

Method: (1) Progressive distance estimator refines coarse candidate scores using compact residuals streamed from far memory; (2) Tiered residual quantization encodes residuals as ternary values stored in far memory; (3) Early stopping when a candidate is provably outside the top-k; (4) Custom accelerator on a CXL Type-2 device for low-latency, local refinement.

Result: Storage efficiency improved by ~2.4x; throughput increased up to ~9x compared to state-of-the-art GPU ANNS systems.

Conclusion: FaTRQ demonstrates that far-memory-aware refinement with progressive estimation and tiered residual quantization, combined with local acceleration, can substantially reduce data movement bottlenecks and accelerate RAG-style retrieval workflows.

Abstract: Approximate Nearest-Neighbor Search (ANNS) is a key technique in retrieval-augmented generation (RAG), enabling rapid identification of the most relevant high-dimensional embeddings from massive vector databases. Modern ANNS engines accelerate this process using prebuilt indexes and store compressed vector-quantized representations in fast memory. However, they still rely on a costly second-pass refinement stage that reads full-precision vectors from slower storage like SSDs. For modern text and multimodal embeddings, these reads now dominate the latency of the entire query. We propose FaTRQ, a far-memory-aware refinement system using tiered memory that eliminates the need to fetch full vectors from storage. It introduces a progressive distance estimator that refines coarse scores using compact residuals streamed from far memory. Refinement stops early once a candidate is provably outside the top-k. To support this, we propose tiered residual quantization, which encodes residuals as ternary values stored efficiently in far memory. A custom accelerator is deployed in a CXL Type-2 device to perform low-latency refinement locally. Together, FaTRQ improves the storage efficiency by 2.4$\times$ and improves the throughput by up to 9$ \times$ than SOTA GPU ANNS system.

</details>


### [71] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: ML的数据处理价值链存在经济数据处理不对称，数据生成者的收益被聚合者截留，缺乏可追溯性、议价力不对称、定价非动态化，导致可持续性受损。提出EDVEX框架以实现尽可能的公平数据-价值交换，并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在数据与其衍生物成为经济资产的背景下，现有机器学习价值链的收益分配存在不公平与结构性缺陷，威胁长期学习算法的可持续性与创新激励。

Method: 基于对73份公开数据交易案例的分析，识别数据链条中的三大结构性 fault（缺失的可溯源性、议价力不对称、定价缺乏动态性），并提出Equitable Data-Value Exchange (EDVEX) 框架；最后概述未来研究路线以改进数据交易实践。

Result: 通过分析发现，大部分价值流向聚合者，创作者的版税接近于零，交易条款高度不透明；数据及其衍生物成为经济资产后，现有学习算法的反馈回路可能被削弱；提出EDVEX框架以构建对所有参与方有利的最低可行市场。

Conclusion: 需要建立一个更公平的数据信息市场，并在数据交易实践、产权、定价机制等方面推进研究，以确保数据驱动的机器学习生态具有长期的可持续性和公平性。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [72] [Efficient Content-based Recommendation Model Training via Noise-aware Coreset Selection](https://arxiv.org/abs/2601.10067)
*Hung Vinh Tran,Tong Chen,Hechuan Wen,Quoc Viet Hung Nguyen,Bin Cui,Hongzhi Yin*

Main category: cs.LG

TL;DR: 提出 NaCS，面向内容推荐系统的噪声感知核心集选择，通过基于训练梯度的子模优化与噪声标签纠正，并结合不确定性筛选，达到用极少数据接近全量训练性能的效果。


<details>
  <summary>Details</summary>
Motivation: CRS 需要大规模/持续训练；核心集可降低成本，但在噪声数据和小样本下易退化；需要一种对噪声鲁棒的高效核心集选择方法。

Method: 构造核心集的核心：基于训练梯度的子模优化；噪声标签通过逐步训练的模型进行纠正；通过不确定性量化滤除低置信样本，进一步 refine。

Result: 在大量实验中，NaCS 生成更高质量的核心集且比现有方法更高效；在仅用 1% 的训练数据时，能恢复全数据集训练的 93-95% 的性能；源代码开放。

Conclusion: NaCS 为 CRS 提供一种鲁棒高效的核心集选择框架，结合梯度驱动的子模优化、噪声纠正和不确定性筛选，显著提升训练效率和模型质量。

Abstract: Content-based recommendation systems (CRSs) utilize content features to predict user-item interactions, serving as essential tools for helping users navigate information-rich web services. However, ensuring the effectiveness of CRSs requires large-scale and even continuous model training to accommodate diverse user preferences, resulting in significant computational costs and resource demands. A promising approach to this challenge is coreset selection, which identifies a small but representative subset of data samples that preserves model quality while reducing training overhead. Yet, the selected coreset is vulnerable to the pervasive noise in user-item interactions, particularly when it is minimally sized. To this end, we propose Noise-aware Coreset Selection (NaCS), a specialized framework for CRSs. NaCS constructs coresets through submodular optimization based on training gradients, while simultaneously correcting noisy labels using a progressively trained model. Meanwhile, we refine the selected coreset by filtering out low-confidence samples through uncertainty quantification, thereby avoid training with unreliable interactions. Through extensive experiments, we show that NaCS produces higher-quality coresets for CRSs while achieving better efficiency than existing coreset selection techniques. Notably, NaCS recovers 93-95\% of full-dataset training performance using merely 1\% of the training data. The source code is available at \href{https://github.com/chenxing1999/nacs}{https://github.com/chenxing1999/nacs}.

</details>


### [73] [An Exploratory Study to Repurpose LLMs to a Unified Architecture for Time Series Classification](https://arxiv.org/abs/2601.09971)
*Hansen He,Shuheng Li*

Main category: cs.LG

TL;DR: 将时间序列编码器与冻结的LLM相结合的混合架构中，Inception编码器在多种编码器中最 consistently 提高性能，表明编码器选择对混合LLM时序学习影响显著。


<details>
  <summary>Details</summary>
Motivation: 解决将时间序列数据映射到文本域以适配LLM的对齐方法之外，系统地评估不同时间序列编码器对混合LLM性能的影响。

Method: 评估多种时间序列编码器家族（Inception、卷积、残差、Transformer、MLP）在与冻结LLM背骨相结合时的混合架构，比较它们在时间序列分类任务中的表现。

Result: Inception编码器在与LLM结合时表现出一致的正向性能提升；其他编码器未能稳定提升性能。

Conclusion: 时间序列编码器的选择对混合LLM架构的性能有显著影响，Inception基于的编码架构被视为未来在LLM驱动的时序学习中的有前景方向。

Abstract: Time series classification (TSC) is a core machine learning problem with broad applications. Recently there has been growing interest in repurposing large language models (LLMs) for TSC, motivated by their strong reasoning and generalization ability. Prior work has primarily focused on alignment strategies that explicitly map time series data into the textual domain; however, the choice of time series encoder architecture remains underexplored. In this work, we conduct an exploratory study of hybrid architectures that combine specialized time series encoders with a frozen LLM backbone. We evaluate a diverse set of encoder families, including Inception, convolutional, residual, transformer-based, and multilayer perceptron architectures, among which the Inception model is the only encoder architecture that consistently yields positive performance gains when integrated with an LLM backbone. Overall, this study highlights the impact of time series encoder choice in hybrid LLM architectures and points to Inception-based models as a promising direction for future LLM-driven time series learning.

</details>


### [74] [In-Context Operator Learning on the Space of Probability Measures](https://arxiv.org/abs/2601.09979)
*Frank Cole,Dixi Wang,Yineng Chen,Yulong Lu,Rongjie Lai*

Main category: cs.LG

TL;DR: 在概率测度空间上进行上下文场景的运算符学习，以学习把分布对映射到最优传输(OT)映射的单一解算子；通过少量示例作为提示，在推断阶段无需梯度更新。


<details>
  <summary>Details</summary>
Motivation: 目标是在不进行推断时更新梯度的前提下，利用少量的提示样本从源/目标分布对学习一个通用的 OT 映射算子，以实现对广义 OT 任务的高效泛化。

Method: 参数化解算子并建立两种规模律：非参数情形下，当任务集中在低维内在维度流形时，给出关于提示规模、任务维度和模型容量的泛化界限；参数情形（如高斯族）给出显式架构，在上下文中可精确恢复 OT 映射，并给出有限样本的超额风险界限；并在合成传输和生成建模基准上给出数值验证。

Result: 得到可量化的泛化界限和超额风险界值，提出的显式架构在上下文中可重获精确 OT 映射并在实验中得到支持。

Conclusion: 提出了一个可在少样本提示下无梯度更新地学习 OT 映射的通用框架，并在非参数和参数两类情形下给出理论与实验验证，具备对新任务的快速适应潜力。

Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.

</details>


### [75] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: 提出在 Transformer 中引入连续深度的神经微分方程块，借助可学习的控制信号 u 实现生成的可控 steering；在四组实验中显示稳定性、可控性、插值连续性和效率，与离散基线相当。


<details>
  <summary>Details</summary>
Motivation: 解决离散深度 Transformer 的固定层数限制，以及缺乏对生成属性的实时控制能力；采用连续深度并引入低维控制信号以实现对生成属性的 steerability。

Method: 用微分方程块替代中间离散层，H 的时间变量 τ，F_θ(H, τ, u) 描述深度的连续演化；通过显式连接将控制信号 u 注入；使用对偶（adjoint）方法实现 O(1) 内存反向传播；对比四组实验，并使用自适应求解器分析几何结构。

Result: 梯度传递稳定，无爆炸/消失事件；语义 steer 指标：正向情感控制 98%、负向 88%；连续插值：固定和自适应求解器的轨迹偏差只有 0.068%；效率：推理延迟与标准离散基线相当；自适应求解器揭示控制信号将向量场分割成不同曲率的动力学子区；

Conclusion: 连续深度动力学结合可学习控制信号为可控语言生成提供有效、高效的机制，适合实现可操控的文本生成。

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [76] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 在严格时序下的时间聚合特征提升XGBoost在CTR预测中的表现： trailing窗口优于仅目标编码，且事件计数窗口在极小范围内带来边际收益；总体推荐以 trailing 窗口为默认，若需要额外提升可考虑事件计数窗口。


<details>
  <summary>Details</summary>
Motivation: 解决时间动态性对点击率预测的影响，同时遵守无前瞻（no-lookahead）约束；在 Avazu 数据集上对比不同时间聚合设计，评估是否能超越传统的目标编码。

Method: 以 Avazu CTR 数据集、严格的 out-of-time 拆分和 no-lookahead 约束为设定，使用 XGBoost，比较强时间感知的目标编码基线与加入实体历史时间聚合的模型在多种窗口设计下的表现。采用两轮滚动尾部的fold，样本为确定性 10% 子集。特征 H 仅使用 H 之前的曝光。评估指标为 ROC AUC、PR AUC。

Result: 相对于仅目标编码， trailing window 在两组滚动尾Fold中分别提升 ROC AUC 约 0.0066–0.0082、PR AUC 约 0.0084–0.0094。时间聚合设计网格中，事件计数窗口对比 trailing 窗口提供的改进是唯一的、且幅度较小；Gap 窗口和 Bucketized 窗口均落后于简单的 trailing 窗口。

Conclusion: 在该数据与协议下，Trailing 窗口构成一个实用的默认选择；若对 ROC AUC 的边际提升有较高要求，可以考虑添加事件计数窗口。

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [77] [BPE: Behavioral Profiling Ensemble](https://arxiv.org/abs/2601.10024)
*Yanxin Liu,Yunqi Zhang*

Main category: cs.LG

TL;DR: 提出行为 profiling 集成框架(BPE): 为每个基学习器构建“行为画像”，通过测试样本响应与其行为画像的偏离来确定权重，实现更区域自适应的集成。结果显示在准确性、计算与存储效率方面显著优于SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 传统的静态与动态集成多基于模型间的差异进行集成，忽略模型自身的内在特性，并高度依赖验证集来估计能力；需要通过挖掘模型的固有行为特征来实现更高效、低资源的自适应集成。

Method: 为每个基学习器构建行为画像；对测试实例，计算该模型的响应与其行为画像的偏离，基于偏离度分配集成权重；在合成与真实数据集上进行广泛实验，与最先进基线比较，评估准确性、计算与存储开销。

Result: 在合成与真实数据集上，BPE及其派生算法显著优于SOTA基线，表现为更高的预测准确性，同时降低计算成本与存储需求，具备良好的场景鲁棒性。

Conclusion: BPE引入一种新范式：通过挖掘并利用模型的固有行为特征来分配权重，超越基于模型间差异的传统DES，提升性能与资源效率。

Abstract: Ensemble learning is widely recognized as a pivotal strategy for pushing the boundaries of predictive performance. Traditional static ensemble methods, such as Stacking, typically assign weights by treating each base learner as a holistic entity, thereby overlooking the fact that individual models exhibit varying degrees of competence across different regions of the instance space. To address this limitation, Dynamic Ensemble Selection (DES) was introduced. However, both static and dynamic approaches predominantly rely on the divergence among different models as the basis for integration. This inter-model perspective neglects the intrinsic characteristics of the models themselves and necessitates a heavy reliance on validation sets for competence estimation. In this paper, we propose the Behavioral Profiling Ensemble (BPE) framework, which introduces a novel paradigm shift. Unlike traditional methods, BPE constructs a ``behavioral profile'' intrinsic to each model and derives integration weights based on the deviation between the model's response to a specific test instance and its established behavioral profile. Extensive experiments on both synthetic and real-world datasets demonstrate that the algorithm derived from the BPE framework achieves significant improvements over state-of-the-art ensemble baselines. These gains are evident not only in predictive accuracy but also in computational efficiency and storage resource utilization across various scenarios.

</details>


### [78] [Unlabeled Data Can Provably Enhance In-Context Learning of Transformers](https://arxiv.org/abs/2601.10058)
*Renpu Liu,Jing Yang*

Main category: cs.LG

TL;DR: 在极少带标签示例的情况下，利用未标记输入，通过带链式思考的多层 Transformer 将 EM 算法隐式实现，以理论保证和实验证据提升 ICL 的准确性；并具线性收敛的训练特性。


<details>
  <summary>Details</summary>
Motivation: 受限于提示中可放入的带标签示例数量，存在大量未标记且与 ICL 任务相关的数据；需要对未标记数据在 ICL 中的作用给出理论保证。

Method: 提出增广 ICL 框架：提示中含少量带标签示例和一批未标记输入；在多类线性分类设定下，利用链式思考(CoT)让多层 Transformer 模拟 EM 算法，从而从标记与未标记数据中提取有用信息；通过教师强制训练，使参数以线性速率收敛。

Result: 给出理论上对 ICL 精度的提升，并证实 Transformer 能在未标记数据作用下获得改进，且实验结果支持理论结论，优于传统的少-shot ICL。

Conclusion: 据称这是首个系统研究未标记数据对 Transformer 的 ICL 性能影响的工作，并提供了实验证据。

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation-maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.

</details>


### [79] [Comparative Evaluation of Deep Learning-Based and WHO-Informed Approaches for Sperm Morphology Assessment](https://arxiv.org/abs/2601.10070)
*Mohammad Abbadi*

Main category: cs.LG

TL;DR: 对比性生物医学人工智能框架：以图像为基础的深度学习模型HuSHeM在精子形态评估中的表现优于以WHO标准为基础、并增添Systemic Inflammation Response Index (SIRI) 的基线方法（WHO(+SIRI)），在区分能力、校准和临床效用方面均显示更佳的性能，推荐作为决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 精子形态评估高度主观，受观察者间差异和资源限制影响，亟需客观、可重复且临床可用的评估工具。通过将图像驱动的深度学习方法与传统基线标准进行对比，评估新方法的潜在临床价值与可迁移性。

Method: 在高分辨率精子形态图像上训练HuSHeM模型，并以独立临床队列进行评估。与WHO(+SIRI)基线进行比较，分别使用判别能力（AUC）、校准、以及临床效用分析等指标。包含精确-召回分析以应对类别不平衡。

Result: HuSHeM在判别性能上优于WHO(+SIRI)，AUC及置信区间更高且更窄；精确-召回分析在不平衡场景下同样表现更好，PR-AUC提升。校准分析显示HuSHeM预测概率与实际结果更接近，决策曲线分析表明在临床相关阈值范围内净效益更大。

Conclusion: 基于图像的深度学习可能提供比传统基线标准+炎症指数组合更可靠的预测与临床效用，能实现客观、可重复的精子形态评估，并可作为生殖筛查与转诊流程中的决策支持工具；该模型设计为辅助决策，而非替代临床判断或实验室评估。

Abstract: Assessment of sperm morphological quality remains a critical yet subjective component of male fertility evaluation, often limited by inter-observer variability and resource constraints. This study presents a comparative biomedical artificial intelligence framework evaluating an image-based deep learning model (HuSHeM) alongside a clinically grounded baseline derived from World Health Organization criteria augmented with the Systemic Inflammation Response Index (WHO(+SIRI)).
  The HuSHeM model was trained on high-resolution sperm morphology images and evaluated using an independent clinical cohort. Model performance was assessed using discrimination, calibration, and clinical utility analyses. The HuSHeM model demonstrated higher discriminative performance, as reflected by an increased area under the receiver operating characteristic curve with relatively narrow confidence intervals compared to WHO(+SIRI). Precision-recall analysis further indicated improved performance under class imbalance, with higher precision-recall area values across evaluated thresholds. Calibration analysis indicated closer agreement between predicted probabilities and observed outcomes for HuSHeM, while decision curve analysis suggested greater net clinical benefit across clinically relevant threshold probabilities.
  These findings suggest that image-based deep learning may offer improved predictive reliability and clinical utility compared with traditional rule-based and inflammation-augmented criteria. The proposed framework supports objective and reproducible assessment of sperm morphology and may serve as a decision-support tool within fertility screening and referral workflows. The proposed models are intended as decision-support or referral tools and are not designed to replace clinical judgment or laboratory assessment.

</details>


### [80] [Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts](https://arxiv.org/abs/2601.10079)
*Sijia Luo,Xiaokang Zhang,Yuxuan Hu,Bohan Zhang,Ke Wang,Jinbo Su,Mengshu Sun,Lei Liang,Jing Zhang*

Main category: cs.LG

TL;DR: 提出 Sparse-RL，通过稀疏回放下的稀疏性拒绝采样和基于重要性重weight处理，修正由 KV 压缩引发的离策略偏差，实现降低回放开销同时保持性能，并在稀疏推理部署中提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在大模型强化学习中，长期滚动中的 KV 缓存存储造成显著内存开销，成为受限硬件上的训练瓶颈。直接对训练应用现有的 KV 压缩方法会引起严重的策略不匹配，导致性能崩溃。因此需要方法在减小回放内存的同时维持稳定的训练。

Method: 提出 Sparse-RL，识别 dense 老策略、稀疏采样策略与学习者策略之间的策略不匹配问题；通过 Sparsity-Aware Rejection Sampling 和 Importance-based Reweighting 来纠正压缩带来的离策略偏差与信息丢失。

Result: 实验表明，在稀疏回放条件下，Sparse-RL 相比密集基线显著降低回放开销，同时保持性能；并且稀疏感知训练在稀疏推理部署中提升模型鲁棒性。

Conclusion: Sparse-RL 能在稀疏回放的情境下实现鲁棒的 RL 训练并显著降低内存开销，同时天然支持稀疏部署环境的鲁棒性提升。

Abstract: Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.

</details>


### [81] [LeMoF: Level-guided Multimodal Fusion for Heterogeneous Clinical Data](https://arxiv.org/abs/2601.10092)
*Jongseok Kim,Seongae Kang,Jonghwan Shin,Yuhan Lee,Ohyun Jo*

Main category: cs.LG

TL;DR: LeMoF selectively fuse level-wise representations within each modality to balance stability and discriminability, achieving robust ICU length-of-stay predictions and outperforming state-of-the-art fusion methods.


<details>
  <summary>Details</summary>
Motivation: Current multimodal fusion in clinical prediction relies on static, coarse fusion and often neglects modality-specific representations from different encoder layers, limiting performance in heterogeneous clinical settings.

Method: LeMoF extracts representations from multiple encoder layers (levels) for each modality, learns global modality-level predictions and level-specific discriminative components, and uses level-guided fusion to adaptively combine information across levels.

Result: Across ICU length-of-stay prediction tasks, LeMoF consistently outperforms existing multimodal fusion baselines under various encoder configurations, indicating robustness to data heterogeneity; ablation suggests level-wise integration is a key factor for performance.

Conclusion: Level-guided, level-wise integration is effective for robust multimodal clinical prediction, achieving a balance between stability and discriminative power across diverse clinical conditions.

Abstract: Multimodal clinical prediction is widely used to integrate heterogeneous data such as Electronic Health Records (EHR) and biosignals. However, existing methods tend to rely on static modality integration schemes and simple fusion strategies. As a result, they fail to fully exploit modality-specific representations. In this paper, we propose Level-guided Modal Fusion (LeMoF), a novel framework that selectively integrates level-guided representations within each modality. Each level refers to a representation extracted from a different layer of the encoder. LeMoF explicitly separates and learns global modality-level predictions from level-specific discriminative representations. This design enables LeMoF to achieve a balanced performance between prediction stability and discriminative capability even in heterogeneous clinical environments. Experiments on length of stay prediction using Intensive Care Unit (ICU) data demonstrate that LeMoF consistently outperforms existing state-of-the-art multimodal fusion techniques across various encoder configurations. We also confirmed that level-wise integration is a key factor in achieving robust predictive performance across various clinical conditions.

</details>


### [82] [Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text](https://arxiv.org/abs/2601.10096)
*Piyush Singh Pasi*

Main category: cs.LG

TL;DR: METAL 提出一种轻量级对齐方法，仅通过对英文文本学习少数线性层，将多语言文本嵌入映射到多模态空间，实现跨语言的零-shot 迁移，在文本-图像检索、音频-文本检索和跨语言图像生成等任务展现良好性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态模型在非英语语言上的性能下降问题，现有方案依赖翻译或未充分利用多语言文本建模能力，需更高效的跨语言对齐方法。

Method: METAL 使用少量线性层，仅用英文文本训练，将多语言文本嵌入投射到与图像/音频等模态对齐的多模态空间，形成轻量级对齐；实现对齐后可用于文本-图像检索、音频-文本检索、跨语言文本到图像生成等。并通过t-SNE 可视化和权重分析揭示几何变换性质。

Result: 在英语上达到 记忆召回率 Recall@10 94.9%；11种语言、10未见语言的零-shot 召回率均值 89.5%；在 XTD 文本到图像检索任务中的零-shot传递表现良好；可泛化到音频-文本检索和跨语言文本到图像生成；提供公开代码和多语言数据集。

Conclusion: METAL 是一种高效且显著的跨语言对齐方法，能够在有限训练资源下实现跨模态对齐，并具备广泛的应用潜力和良好的泛化性；未来工作可进一步扩展至更多模态和语言，提升对齐的鲁棒性。

Abstract: Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.

</details>


### [83] [Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation](https://arxiv.org/abs/2601.10137)
*Ziyi Ding,Chenfei Ye-Hao,Zheyuan Wang,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: Tree-Query reduces pairwise causal discovery to a sequence of interpretable LLM queries (backdoor, dependence, latent confounding, direction) within a tree-structured, multi-expert framework, producing robustness-aware confidence scores and asymptotic identifiability guarantees; it improves structure metrics on data-free Mooij/UCI benchmarks and is demonstrated via a diet–weight case study; code released.


<details>
  <summary>Details</summary>
Motivation: Mitigate error propagation in constraint-based causal discovery and the opaque, confidence-free nature of many LLM-based causal oracles by providing data-free, interpretable priors with robustness-aware confidence.

Method: A tree-structured, multi-expert LLM framework (Tree-Query) that reduces pairwise causal relations to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction; outputs interpretable judgments with confidence scores; provides theoretical guarantees for asymptotic identifiability of four pairwise relations.

Result: Empirical evaluation on data-free benchmarks derived from Mooij et al. and UCI causal graphs shows improved structural metrics over direct LLM baselines; a diet–weight case study demonstrates effective confounder screening and stable, high-confidence causal conclusions.

Conclusion: Tree-Query offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery; code is available at the provided link.

Abstract: Causal discovery aims to recover ``what causes what'', but classical constraint-based methods (e.g., PC, FCI) suffer from error propagation, and recent LLM-based causal oracles often behave as opaque, confidence-free black boxes. This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet--weight case study illustrates confounder screening and stable, high-confidence causal conclusions. Tree-Query thus offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery. Code is available at https://anonymous.4open.science/r/Repo-9B3E-4F96.

</details>


### [84] [Understanding and Preserving Safety in Fine-Tuned LLMs](https://arxiv.org/abs/2601.10141)
*Jiawen Zhang,Yangfan Hu,Kejia Chen,Lipeng He,Jiachen Ma,Jian Lou,Dan Li,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: SPF: a gradient-subspace-based fine-tuning method that preserves safety while maintaining utility by projecting out gradient components conflicting with a low-rank safety subspace; provides theoretical guarantees and empirical robustness against adversarial fine-tuning and jailbreak attacks.


<details>
  <summary>Details</summary>
Motivation: Address the persistent safety-utility dilemma in LLM fine-tuning. Existing methods trade safety for task performance or require deep fine-tuning that harms safety. The authors reveal geometric relations between safety- and utility-oriented gradients and propose a lightweight, effective solution.

Method: 1) Empirically analyze gradient geometry during fine-tuning and identify a low-rank subspace for safety gradients and a high-dimensional space for utility gradients; 2) show negative correlation between these subspaces and that dominant safety direction can be estimated from a single sample; 3) propose SPF, which removes gradient components conflicting with the low-rank safety subspace during optimization; 4) provide theoretical guarantees that SPF preserves utility convergence while bounding safety drift.

Result: SPF consistently preserves downstream task performance, recovers nearly all pre-trained safety alignment even under adversarial fine-tuning, and shows robust resistance to deep fine-tuning and dynamic jailbreak attacks across settings.

Conclusion: The work advances mechanistic understanding of safety-utility interactions in LLM fine-tuning and offers a lightweight, provably effective method (SPF) for always-aligned LLM fine-tuning, with practical implications for deployment.

Abstract: Fine-tuning is an essential and pervasive functionality for applying large language models (LLMs) to downstream tasks. However, it has the potential to substantially degrade safety alignment, e.g., by greatly increasing susceptibility to jailbreak attacks, even when the fine-tuning data is entirely harmless. Despite garnering growing attention in defense efforts during the fine-tuning stage, existing methods struggle with a persistent safety-utility dilemma: emphasizing safety compromises task performance, whereas prioritizing utility typically requires deep fine-tuning that inevitably leads to steep safety declination.
  In this work, we address this dilemma by shedding new light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. Through systematic empirical analysis, we uncover three key insights: (I) safety gradients lie in a low-rank subspace, while utility gradients span a broader high-dimensional space; (II) these subspaces are often negatively correlated, causing directional conflicts during fine-tuning; and (III) the dominant safety direction can be efficiently estimated from a single sample. Building upon these novel insights, we propose safety-preserving fine-tuning (SPF), a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace. Theoretically, we show that SPF guarantees utility convergence while bounding safety drift. Empirically, SPF consistently maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. Furthermore, SPF exhibits robust resistance to both deep fine-tuning and dynamic jailbreak attacks. Together, our findings provide new mechanistic understanding and practical guidance toward always-aligned LLM fine-tuning.

</details>


### [85] [LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers](https://arxiv.org/abs/2601.10155)
*Aryan Karmore*

Main category: cs.LG

TL;DR: 通过乘积量化（Product Quantization）和不对称距离计算（Asymmetric Distance Computation）对 Transformer 的 KV 缓存进行高效压缩，将注意力计算从内存带宽瓶颈转为计算密集型。对 GPT-2 的实验显示：64×压缩，输出保真度95.7%；32×压缩，保真度95.0%；且保持秩相关性ρ>0.95；无需架构修改或再训练，且理论上对秩相关性的下降有界，序列长度可达1024。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上部署大语言模型时，KV缓存带宽成为瓶颈；现有量化仅降低存储并不能有效降低带宽消耗，因为需要将 KV 反量化后参与注意力。将注意力等价视为向量检索，可借助向量数据库的压缩技术提升压缩效果并保持高保真度。

Method: 将 key 向量分解成子空间，学习多组码本，并通过查找表计算注意力表，结合乘积量化与不对称距离计算实现 KV 缓存的近似表示；在 Transformer 架构中无需改动即可应用，注意力计算由内存带宽受限转变为计算密集。

Result: 在 GPT-2 上实现了 64×（95.7%）和 32×（95.0%）的压缩比，且保持秩相关性ρ>0.95；无需额外训练或架构改动；理论分析给出秩相关性的下降界为 O(d_k/(mK))，并在序列长度到 1024 的场景中得到验证。

Conclusion: LOOKAT 提供一种无架构改动的高保真度 KV 缓存压缩方案，将注意力瓶颈从带宽转化为计算，适合边缘部署；具有明确的理论界限和实践可验证性。

Abstract: Compressing the KV cache is a required step to deploy large language models on edge devices. Current quantization methods compress storage but fail to reduce bandwidth as attention calculation requires dequantizing keys from INT4/INT8 to FP16 before use. We observe that attention scoring is mathematically equivalent to the inner product similarity search and we can apply some compression techniques from vector databases to compress KV-cache better. We propose LOOKAT, which applies product quantization and asymmetric distance computation, to transformer architecture by decomposing key vectors into subspaces, learning codebooks and computing attention tables via lookup tables. This transforms attention from memory-bound to compute-bound. LOOKAT achieves 64 $\times$ compression at 95.7\% output fidelity and 32 $\times$ compression at 95.0\% fidelity when tested on GPT-2. LOOKAT requires no architecture changes or training while maintaining rank correlation $ρ> 0.95$. Theoretical analysis confirms that rank correlation degrades as $O(d_k/mK)$, with guarantees validated across sequence lengths up to 1024 tokens.

</details>


### [86] [CC-OR-Net: A Unified Framework for LTV Prediction through Structural Decoupling](https://arxiv.org/abs/2601.10176)
*Mingyu Zhao,Haoran Bai,Yu Tian,Bing Zhu,Hengliang Luo*

Main category: cs.LG

TL;DR: 提出 CC-OR-Net 的统一框架，用于零膨胀、长尾分布的 LTV 预测，通过结构化的序 ordinal 分解、桶内残差回归以及高价值用户增益来实现排序与回归的有效解耦，提升对 Whale 用户的精确度与全局准确性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: LTV 预测受零膨胀与长尾分布影响显著，大多数低价值用户掩盖了少量高价值“鲸鱼”用户，同时低-中值用户内部仍存在显著异质性。现有方法要么依赖刚性统计假设，要么通过有序桶的方式解耦排序与回归，往往通过损失约束强制有序性，未在架构层面保证，导致全局准确性与高价值精度之间难以兼顾。

Method: 提出 CC-OR-Net，包含三大组成：1) 结构化有序分解模块，确保排序的鲁棒性；2) 桶内残差回归模块，实现对低-中值桶内的细粒度回归；3) 针对高价值用户的增强模块，提升高端用户的预测精度。通过结构性分解实现排序与回归的架构层级解耦，达到更稳健的全局-局部平衡。

Result: 在真实数据集（超过3亿用户）上评估，CC-OR-Net 在关键业务指标上实现更优的折中，优于状态-of-the-art 方法，形成一个全局与高价值精度兼具的 LTV 预测解决方案。

Conclusion: CC-OR-Net 通过结构化的变量分解与架构层面的有序性保证，提供对排序与回归的更强解耦，提升高价值用户的预测精度，并在大规模实际场景中实现更好的商业指标表现。

Abstract: Customer Lifetime Value (LTV) prediction, a central problem in modern marketing, is characterized by a unique zero-inflated and long-tail data distribution. This distribution presents two fundamental challenges: (1) the vast majority of low-to-medium value users numerically overwhelm the small but critically important segment of high-value "whale" users, and (2) significant value heterogeneity exists even within the low-to-medium value user base. Common approaches either rely on rigid statistical assumptions or attempt to decouple ranking and regression using ordered buckets; however, they often enforce ordinality through loss-based constraints rather than inherent architectural design, failing to balance global accuracy with high-value precision. To address this gap, we propose \textbf{C}onditional \textbf{C}ascaded \textbf{O}rdinal-\textbf{R}esidual Networks \textbf{(CC-OR-Net)}, a novel unified framework that achieves a more robust decoupling through \textbf{structural decomposition}, where ranking is architecturally guaranteed. CC-OR-Net integrates three specialized components: a \textit{structural ordinal decomposition module} for robust ranking, an \textit{intra-bucket residual module} for fine-grained regression, and a \textit{targeted high-value augmentation module} for precision on top-tier users. Evaluated on real-world datasets with over 300M users, CC-OR-Net achieves a superior trade-off across all key business metrics, outperforming state-of-the-art methods in creating a holistic and commercially valuable LTV prediction solution.

</details>


### [87] [Bias in the Shadows: Explore Shortcuts in Encrypted Network Traffic Classification](https://arxiv.org/abs/2601.10180)
*Chuyi Wang,Xiaohui Xie,Tongze Wang,Yong Cui*

Main category: cs.LG

TL;DR: BiasSeeker是一种模型无关、半自动框架，通过对原始字节级数据的统计相关性分析，发现加密流量分类中的数据集特定快捷特征，并提供按类别的验证策略以降低偏差同时保留有意义信息，在19个公开数据集、3个NTC任务上进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有工作中的快捷学习使模型依赖于与任务无关的虚假相关，导致对真实世界数据的泛化能力不足；基于模型的解释方法往往需要特定模型架构，缺乏跨模型和部署场景的通用性。因此需要一个数据驱动、对模型无关的框架来识别数据集特定的偏差特征。

Method: 在原始二进制流上进行统计相关性分析以识别可能的虚假特征（快捷特征），且与具体分类器无关。针对快捷特征的多样性，提出系统化的类别划分并针对不同类别应用验证策略，以在减少偏差的同时保留有用信息，过程呈现为半自动化。

Result: 在19个公开数据集、3个NTC任务上进行评估，BiasSeeker能够识别数据集特有的快捷特征并给出按类别的验证策略，为理解和解决加密流量分类中的快捷学习提供新视角，强调在训练前应进行情景化的、目标导向的特征选择。

Conclusion: 该工作提出了一种面向场景化特征选择的诊断框架，帮助研究者和工程实践者在训练前识别并消除数据集特定的偏差特征，从而提升模型在真实世界数据上的泛化能力。

Abstract: Pre-trained models operating directly on raw bytes have achieved promising performance in encrypted network traffic classification (NTC), but often suffer from shortcut learning-relying on spurious correlations that fail to generalize to real-world data. Existing solutions heavily rely on model-specific interpretation techniques, which lack adaptability and generality across different model architectures and deployment scenarios.
  In this paper, we propose BiasSeeker, the first semi-automated framework that is both model-agnostic and data-driven for detecting dataset-specific shortcut features in encrypted traffic. By performing statistical correlation analysis directly on raw binary traffic, BiasSeeker identifies spurious or environment-entangled features that may compromise generalization, independent of any classifier. To address the diverse nature of shortcut features, we introduce a systematic categorization and apply category-specific validation strategies that reduce bias while preserving meaningful information.
  We evaluate BiasSeeker on 19 public datasets across three NTC tasks. By emphasizing context-aware feature selection and dataset-specific diagnosis, BiasSeeker offers a novel perspective for understanding and addressing shortcut learning in encrypted network traffic classification, raising awareness that feature selection should be an intentional and scenario-sensitive step prior to model training.

</details>


### [88] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: GR-PCA introduces graph-based regularization into PCA to handle non-spherical noise by learning a sparse precision graph and biasing loadings toward low-frequency graph Laplacian modes, thereby aligning components with conditional relationships.


<details>
  <summary>Details</summary>
Motivation: PCA under isotropic-noise assumption can fail when features are dependent (non-spherical covariance). A structure-aware dimensionality reduction method is needed to preserve graph-consistent structure and improve interpretability.

Method: Regularize PCA with a learned sparse precision graph and a penalty that favors low-frequency graph Fourier modes of the graph Laplacian. High-frequency signals are suppressed, promoting graph-coherent components. The approach is modular with respect to the precision estimator and scalable.

Result: Synthetic experiments across various graph topologies, SNRs, and sparsity levels show GR-PCA concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and has competitive out-of-sample reconstruction. When high-frequency signals exist and are graph-correlated, the penalty mitigates overfitting but may reduce reconstruction accuracy; PCA can still be competitive when such signals are nearly rotationally invariant.

Conclusion: GR-PCA provides a simple, modular, and scalable framework for structure-aware dimensionality reduction, improving structural fidelity without sacrificing predictive performance, particularly when meaningful graph structure governs high-frequency content.

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


### [89] [PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary](https://arxiv.org/abs/2601.10201)
*Jiarui Yao,Ruida Wang,Tong Zhang*

Main category: cs.LG

TL;DR: 提出 PRL，将熵正则化 RL 的目标分解为中间步骤的过程奖励，使过程监督信号可从结果奖励衍生，从而提高推理能力并扩展边界。


<details>
  <summary>Details</summary>
Motivation: 现有方法多以结果奖励为导向，缺乏对推理过程的细粒度 supervision，且常需额外步骤（如 MCTS、训练奖励模型），导致训练效率低下，理论支撑不足。

Method: 从理论出发，给出 PRL 的等价形式：最大化奖励的同时对策略与参考模型之间的 KL 散度加权惩罚。将结果奖励转化为过程奖励，作为对中间步骤的监督信号，指导 RL 优化的探索。

Result: 实验表明 PRL 提高平均 @ n 的推理表现，并通过提高 pass @ n 拓展推理边界，具有良好泛化与有效性。

Conclusion: PRL 是一种有效、可泛化的框架，能够将过程信号与结果信号结合，提升 LLM 推理能力，同时避免额外的复杂步骤，理论基础充足。

Abstract: Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.

</details>


### [90] [Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD](https://arxiv.org/abs/2601.10237)
*Murat Bilgehan Ertan,Marten van Dijk*

Main category: cs.LG

TL;DR: 在最坏对手的 adversarial 情况下，基于 f-DP 的 DP-SGD 存在根本性的隐私-效用权衡：要么增大高斯噪声要么增大与理想随机猜测线的距离 κ；随着更新次数 M 增大，该界限依然存在且收敛极慢，实证显示对现实训练造成显著精度损失；该结论也适用于泊松采样场景。


<details>
  <summary>Details</summary>
Motivation: 揭示 DP-SGD 在 f-DP 框架下的基本极限，特别是在单轮洗牌采样的情况下，量化隐私与效用之间的不可调和性及其对实际训练的影响。

Method: 对单轮洗牌采样下的 M 次梯度更新，推导出可行的 f-DP 交易曲线的显式上界（子最优），由此得到机制交易曲线与理想随机猜测线之间的几何距离 κ 的下界；据此将对高斯噪声乘数 σ 的约束与 κ 联系起来，给出 σ 的下界及 κ 的下界，并扩展至 Poisson 子采样。

Result: 给出两条强制性界限：σ ≥ 1/√(2 ln M) 或 κ ≥ (1/√8)(1 - 1/√(4π ln M))；该界限在 M→∞ 时趋于 0，但收敛极慢，因此在现实的更新数量下，所需噪声仍然相当大，导致明显的精度下降；泊松子采样下存在相似限制；实验验证了理论在现实训练中的影响。

Conclusion: 在标准的 worst-case 对手模型下，DP-SGD 无法在强隐私与高效用之间同时达到最优；除非放宽对抗性假设或采用其他隐私框架/算法，否则这是一个根本性瓶颈，尽管对 M 的影响在渐近大规模时减弱，但在实际训练规模下仍显著。

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy
  $σ\ge \frac{1}{\sqrt{2\ln M}}$ $\quad\text{or}\quad$ $κ\ge\ \frac{1}{\sqrt{8}}\!\left(1-\frac{1}{\sqrt{4π\ln M}}\right)$,
  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \to \infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.

</details>


### [91] [X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction](https://arxiv.org/abs/2601.10251)
*Hongru Duan,Yongle Chen,Lei Guan*

Main category: cs.LG

TL;DR: 在分析 SAM 的谱几何行为基础上，提出 X-SAM，通过沿着 Hessian 最大特征向量的正交分解来纠正梯度，从而更直接地正则化 Hessian 的最大特征值，获得收敛性证明与更优的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的 SAM 时常与理论期望不一致：在尖锐与平滑区域都可能使被扰动的损失小，导致梯度指向不利方向。需从梯度-特征向量的关系与 Hessian 角度出发改进。

Method: 通过分析梯度与 Hessian 的前沿特征向量之间的夹角（若角度≤90°，SAM 效果可能减弱），提出 X-SAM：对梯度进行沿顶特征向量的正交分解并修正，使梯度更直接地作用于最大特征值的正则化；给出收敛性证明并进行大量实证。

Result: 给出 X-SAM 的收敛性证明并在泛化表现上优于原始 SAM，实验与理论分析均表明优势。

Conclusion: 通过显式对齐顶特征向量，X-SAM 实现更直接、有效的 Hessian 最大特征值正则化，提升优化稳定性与泛化性能，具有较强的适用性。

Abstract: Sharpness-Aware Minimization (SAM) aims to improve generalization by minimizing a worst-case perturbed loss over a small neighborhood of model parameters. However, during training, its optimization behavior does not always align with theoretical expectations, since both sharp and flat regions may yield a small perturbed loss. In such cases, the gradient may still point toward sharp regions, failing to achieve the intended effect of SAM. To address this issue, we investigate SAM from a spectral and geometric perspective: specifically, we utilize the angle between the gradient and the leading eigenvector of the Hessian as a measure of sharpness. Our analysis illustrates that when this angle is less than or equal to ninety degrees, the effect of SAM's sharpness regularization can be weakened. Furthermore, we propose an explicit eigenvector-aligned SAM (X-SAM), which corrects the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue. We prove X-SAM's convergence and superior generalization, with extensive experimental evaluations confirming both theoretical and practical advantages.

</details>


### [92] [Early Fault Detection on CMAPSS with Unsupervised LSTM Autoencoders](https://arxiv.org/abs/2601.10269)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 提出一种基于无监督学习的涡扇发动机健康监测框架，通过回归去除工况影响，使用在健康轨迹上训练的 LSTM 自编码器进行重构误差检测，并以自适应阈值触发实时告警，实现高召回率与低误警。


<details>
  <summary>Details</summary>
Motivation: 解决缺乏故障样本的健康监测问题，需对多工况与大量机队数据快速部署，减少手工规则依赖。

Method: 对NASA CMAPSS传感器流进行回归式归一化以消除工况效应；仅在健康数据上训练 LSTM 自编码器；以自适应数据驱动阈值估计持续重构误差，实现实时告警；无需手工规则。

Result: 基准测试在多工况下实现高召回率与低假警率，显示方法可快速部署、扩展到多机队，并作为 RUL 模型的早期警示层。

Conclusion: 该无监督框架可作为快速部署的健康监测解决方案，与 RUL 模型互补，提升早期故障探测与运维效率。

Abstract: This paper introduces an unsupervised health-monitoring framework for turbofan engines that does not require run-to-failure labels. First, operating-condition effects in NASA CMAPSS sensor streams are removed via regression-based normalisation; then a Long Short-Term Memory (LSTM) autoencoder is trained only on the healthy portion of each trajectory. Persistent reconstruction error, estimated using an adaptive data-driven threshold, triggers real-time alerts without hand-tuned rules. Benchmark results show high recall and low false-alarm rates across multiple operating regimes, demonstrating that the method can be deployed quickly, scale to diverse fleets, and serve as a complementary early-warning layer to Remaining Useful Life models.

</details>


### [93] [Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers](https://arxiv.org/abs/2601.10274)
*Emre Ozbas,Melih Bastopcu*

Main category: cs.LG

TL;DR: The paper analyzes optimal token allocation for an LLM server handling N task types under Poisson arrivals and FIFO M/G/1 queueing. It proves strict concavity of the objective within the stability region, ensuring a unique optimum, and provides a coupled projected fixed-point characterization, iterative solution, and contraction condition. It also develops a projected gradient method with a global step-size bound, and shows how to round the continuous solution to integer tokens with quantified loss via simulations.


<details>
  <summary>Details</summary>
Motivation: Improve accuracy-latency trade-offs in heterogeneous LLM serving by allocating a fixed token budget per task type under queueing constraints and architecture limits.

Method: Model each type with a fixed number of thinking tokens; service time affine in tokens; accuracy exhibits diminishing returns. Arrivals are Poisson; system is M/G/1 under FIFO. Formulate a constrained optimization maximizing a weighted average accuracy minus mean system time, subject to token budgets and stability. Prove objective is strictly concave over the stability region; derive first-order conditions giving a coupled projected fixed-point; provide iterative solution and a contraction condition; develop a projected gradient method with a computable step-size bound; round continuous solution to integers and evaluate loss via simulations.

Result: Existence and uniqueness of the optimal continuous token allocation within the stability region; a coupled fixed-point characterization for optimality; convergent iterative algorithm and a sufficient contraction condition; a projected gradient method with guaranteed convergence beyond contractive regime; practical rounding to integer tokens with quantified performance loss assessed in simulations.

Conclusion: The framework yields a provably optimal continuous allocation under M/G/1 dynamics with FIFO, along with practical algorithms for computing the solution and a method to obtain integer token allocations with bounded performance loss. It lays groundwork for extensions to nonstationary settings and nonconvexities.

Abstract: We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.

</details>


### [94] [SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks](https://arxiv.org/abs/2601.10282)
*Jose Marie Antonio Minoza*

Main category: cs.LG

TL;DR: SPIKE 将 PINN 与连续时间 Koopman 运算符结合，通过对学习到的观测空间中的线性动力学 dz/dt = A z 进行 L1 正则化，得到稀疏生成矩阵，从而提高时间外推、空间泛化和长期预测的稳健性。


<details>
  <summary>Details</summary>
Motivation: 解决 PINNs 在训练区域过拟合且对外推泛化差的问题；需要一个稀疏、可解释的低维动力学表示。

Method: 在学习的观测空间引入连续时间 Koopman 运算符，强制 dz/dt = A z；PIKE 为不带稀疏正则，SPIKE 对 A 施以 L1 正则化以实现稀疏性；使用矩阵指数积分实现对高斯算子，避免离散 Koopman 的不稳定性和对角支配问题。

Result: 在抛物、双曲、色散、刚性 PDE（包括 Navier–Stokes 和 Lorenz 等）上，展现出在时间外推、空间泛化和长期预测上的一致改进；对刚性系统具备无条件稳定性；不同类型方程均适用。

Conclusion: continuous-time Koopman-regularized PINNs 能以稀疏生成矩阵体现简约动力学，提升泛化与稳定性；SPIKE/PIKE 提供一个统一框架来学习可解释的低维动力学结构。

Abstract: Physics-Informed Neural Networks (PINNs) provide a mesh-free approach for solving differential equations by embedding physical constraints into neural network training. However, PINNs tend to overfit within the training domain, leading to poor generalization when extrapolating beyond trained spatiotemporal regions. This work presents SPIKE (Sparse Physics-Informed Koopman-Enhanced), a framework that regularizes PINNs with continuous-time Koopman operators to learn parsimonious dynamics representations. By enforcing linear dynamics $dz/dt = Az$ in a learned observable space, both PIKE (without explicit sparsity) and SPIKE (with L1 regularization on $A$) learn sparse generator matrices, embodying the parsimony principle that complex dynamics admit low-dimensional structure. Experiments across parabolic, hyperbolic, dispersive, and stiff PDEs, including fluid dynamics (Navier-Stokes) and chaotic ODEs (Lorenz), demonstrate consistent improvements in temporal extrapolation, spatial generalization, and long-term prediction accuracy. The continuous-time formulation with matrix exponential integration provides unconditional stability for stiff systems while avoiding diagonal dominance issues inherent in discrete-time Koopman operators.

</details>


### [95] [We Need a More Robust Classifier: Dual Causal Learning Empowers Domain-Incremental Time Series Classification](https://arxiv.org/abs/2601.10312)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Haodong Jing,Mingyang Geng,Yongsheng Huang,Jialu Xu,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: 提出了 DualCD 框架用于时序分类的领域增量学习，通过双因果干扰分离实现对因果特征的鲁棒利用。包含时序特征分离模块与双因果干预机制，能够生成变体样本并以因果特征进行预测约束。


<details>
  <summary>Details</summary>
Motivation: 在领域增量场景下保持时序分类模型的鲁棒性与迁移性，解决内在因果与伪相关因素的混淆问题。

Method: 引入时序特征分离模块以捕捉类别因果特征与伪相关特征；设计双因果干预机制，通过将当前类别的因果特征与同类的伪相关特征及其他类别的因果特征组合，构造变体样本；引入因果干预损失促使模型仅依据因果特征进行正确预测。

Result: 在多个数据集与模型上进行广泛实验，显示 DualCD 能显著提升领域增量场景下的性能，并给出一个综合基准，便于后续研究。

Conclusion: DualCD 提供一个轻量、鲁棒的领域增量时序分类解决方案，易于与现有模型整合，并可推动该研究领域的基准建设。

Abstract: The World Wide Web thrives on intelligent services that rely on accurate time series classification, which has recently witnessed significant progress driven by advances in deep learning. However, existing studies face challenges in domain incremental learning. In this paper, we propose a lightweight and robust dual-causal disentanglement framework (DualCD) to enhance the robustness of models under domain incremental scenarios, which can be seamlessly integrated into time series classification models. Specifically, DualCD first introduces a temporal feature disentanglement module to capture class-causal features and spurious features. The causal features can offer sufficient predictive power to support the classifier in domain incremental learning settings. To accurately capture these causal features, we further design a dual-causal intervention mechanism to eliminate the influence of both intra-class and inter-class confounding features. This mechanism constructs variant samples by combining the current class's causal features with intra-class spurious features and with causal features from other classes. The causal intervention loss encourages the model to accurately predict the labels of these variant samples based solely on the causal features. Extensive experiments on multiple datasets and models demonstrate that DualCD effectively improves performance in domain incremental scenarios. We summarize our rich experiments into a comprehensive benchmark to facilitate research in domain incremental time series classification.

</details>


### [96] [Meta Dynamic Graph for Traffic Flow Prediction](https://arxiv.org/abs/2601.10328)
*Yiqing Zou,Hanning Yuan,Qianyu Yang,Ziqiang Yuan,Shuliang Wang,Sijie Ruan*

Main category: cs.LG

TL;DR: MetaDG: a dynamic graph framework for traffic prediction that models spatio-temporal dynamics via dynamic node representations, producing dynamic adjacency matrices and meta-parameters, and unifying spatio-temporal heterogeneity in a single dimension.


<details>
  <summary>Details</summary>
Motivation: 解决当前方法中对时空依赖的分离建模和仅限拓扑变化的动态建模的局限性，拓展动态建模的范围并统一时空异质性建模.

Method: 提出 Meta Dynamic Graph (MetaDG)，通过节点表示的动态图结构来显式建模时空动态，生成动态邻接矩阵和元参数，将时空异质性统一到一个维度，在四个真实数据集上进行广泛实验。

Result: 大量实验结果表明 MetaDG 有效，准确性优于多种基线，在四个数据集上获得良好性能。

Conclusion: 通过动态图建模扩展对拓扑的依赖，提供一个灵活框架以捕捉交通预测中的时空动态与异质性。

Abstract: Traffic flow prediction is a typical spatio-temporal prediction problem and has a wide range of applications. The core challenge lies in modeling the underlying complex spatio-temporal dependencies. Various methods have been proposed, and recent studies show that the modeling of dynamics is useful to meet the core challenge. While handling spatial dependencies and temporal dependencies using separate base model structures may hinder the modeling of spatio-temporal correlations, the modeling of dynamics can bridge this gap. Incorporating spatio-temporal heterogeneity also advances the main goal, since it can extend the parameter space and allow more flexibility. Despite these advances, two limitations persist: 1) the modeling of dynamics is often limited to the dynamics of spatial topology (e.g., adjacency matrix changes), which, however, can be extended to a broader scope; 2) the modeling of heterogeneity is often separated for spatial and temporal dimensions, but this gap can also be bridged by the modeling of dynamics. To address the above limitations, we propose a novel framework for traffic prediction, called Meta Dynamic Graph (MetaDG). MetaDG leverages dynamic graph structures of node representations to explicitly model spatio-temporal dynamics. This generates both dynamic adjacency matrices and meta-parameters, extending dynamic modeling beyond topology while unifying the capture of spatio-temporal heterogeneity into a single dimension. Extensive experiments on four real-world datasets validate the effectiveness of MetaDG.

</details>


### [97] [SuS: Strategy-aware Surprise for Intrinsic Exploration](https://arxiv.org/abs/2601.10349)
*Mark Kashirskiy,Ilya Makarov*

Main category: cs.LG

TL;DR: 提出 Strategy-aware Surprise (SuS) 框架，结合前后预测错配的新颖性信号，并引入策略稳定性（SS）与策略惊奇（Strategy Surprise）两路信号，通过可学习权重对它们进行联合奖励，以提升大语言模型在数学推理任务中的准确性和解题多样性。


<details>
  <summary>Details</summary>
Motivation: 弥补传统好奇心驱动方法仅依赖状态预测误差的局限；通过引入与策略相关的稳定性与惊奇性信号，提升探索的有效性，尤其在需要复杂推理的任务中。

Method: 使用前后预测错配作为新颖性信号；定义策略稳定性（SS）以度量行为策略在连续时间步的稳定性/一致性，定义策略惊奇（Strategy Surprise）以捕捉相对于当前策略表示的意外输出；将两者通过可学习的权重系数合成总奖励信号；在强化学习框架中结合大语言模型进行数学推理任务；评估与传统基线对比并进行消融分析。

Result: 在数学推理任务上，SuS 显著提升准确性与解题多样性；消融显示去除任一组件均导致至少 10% 的性能下降；相较基线，Pass@1 提升 17.4%，Pass@5 提升 26.4%，并保持更高的策略多样性。

Conclusion: 将两路互补信号有效融合的 SuS 框架在需要复杂推理的任务中显示出明显的探索效率与解题能力提升，证实策略稳定性与策略惊奇的协同效应。

Abstract: We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.

</details>


### [98] [EvoMorph: Counterfactual Explanations for Continuous Time-Series Extrinsic Regression Applied to Photoplethysmography](https://arxiv.org/abs/2601.10356)
*Mesut Ceylan,Alexis Tabin,Patrick Langer,Elgar Fleisch,Filipe Barata*

Main category: cs.LG

TL;DR: EvoMorph 是一个多目标进化框架，用于为时间序列外推回归（TSER）在PPG信号上生成生理可行且多样的反事实解释（CFE），提升临床时间序列模型的可解释性和不确定性感知。


<details>
  <summary>Details</summary>
Motivation: 需超越单点预测，提供对生理可变性下预测稳定性及对实际、可达到信号变化的影响评估；现有CFE多限于分类任务、忽略波形形态、易产生不生理的信号，限制在连续生物时间序列中的应用。

Method: 提出 EvoMorph：多目标进化算法，针对可解释信号描述符定义形态感知目标，采用变换以保留波形结构；对三组PPG数据的心率、呼吸率、血氧饱和度进行评估，基准为最近邻反例。

Result: 在三个PPG数据集上评估，并与最近邻基线比较；在案例研究中，将反事实敏感性与自举法集成不确定性及数据密度度量相关联，证明生理可感知的CFE与不确定性感知相关。

Conclusion: EvoMorph 能为连续生物信号生成生理可感知的CFE，并支持不确定性感知的解释性分析，推动临床时序模型的可信分析。

Abstract: Wearable devices enable continuous, population-scale monitoring of physiological signals, such as photoplethysmography (PPG), creating new opportunities for data-driven clinical assessment. Time-series extrinsic regression (TSER) models increasingly leverage PPG signals to estimate clinically relevant outcomes, including heart rate, respiratory rate, and oxygen saturation. For clinical reasoning and trust, however, single point estimates alone are insufficient: clinicians must also understand whether predictions are stable under physiologically plausible variations and to what extent realistic, attainable changes in physiological signals would meaningfully alter a model's prediction. Counterfactual explanations (CFE) address these "what-if" questions, yet existing time series CFE generation methods are largely restricted to classification, overlook waveform morphology, and often produce physiologically implausible signals, limiting their applicability to continuous biomedical time series. To address these limitations, we introduce EvoMorph, a multi-objective evolutionary framework for generating physiologically plausible and diverse CFE for TSER applications. EvoMorph optimizes morphology-aware objectives defined on interpretable signal descriptors and applies transformations to preserve the waveform structure. We evaluated EvoMorph on three PPG datasets (heart rate, respiratory rate, and oxygen saturation) against a nearest-unlike-neighbor baseline. In addition, in a case study, we evaluated EvoMorph as a tool for uncertainty quantification by relating counterfactual sensitivity to bootstrap-ensemble uncertainty and data-density measures. Overall, EvoMorph enables the generation of physiologically-aware counterfactuals for continuous biomedical signals and supports uncertainty-aware interpretability, advancing trustworthy model analysis for clinical time-series applications.

</details>


### [99] [PLGC: Pseudo-Labeled Graph Condensation](https://arxiv.org/abs/2601.10358)
*Jay Nandy,Arnab Kumar Mondal,Anuj Rathore,Mahesh Chandran*

Main category: cs.LG

TL;DR: 提出PLGC（伪标签图压缩），通过自监督伪标签和结构/特征统计一致性来蒸馏小图，在无 ground-truth 标签条件下实现图数据的高保真压缩。


<details>
  <summary>Details</summary>
Motivation: 解释监督蒸馏在标签噪声和分布漂移下的失败原因，寻找一种无需真实标签的稳健图 condensed（图压缩）方法，以适应标签稀缺、嘈杂或不一致的场景。

Method: 提出自监督框架PLGC，利用节点嵌入构造潜在伪标签，联合学习潜在原型与节点分配，优化压缩图以在结构与特征统计上匹配原图；提供理论保证伪标签能保留原图潜在结构统计并确保嵌入对齐。

Result: 在节点分类和链接预测任务上，PLGC在清洁数据集上与最先进的监督压缩方法竞争；在标签噪声条件下表现出显著鲁棒性，常常优于所有基线。

Conclusion: 自监督图压缩在嘈杂或弱标注环境中具有实际与理论优势，适用于高效地压缩图数据并支撑下游任务。

Abstract: Large graph datasets make training graph neural networks (GNNs) computationally costly. Graph condensation methods address this by generating small synthetic graphs that approximate the original data. However, existing approaches rely on clean, supervised labels, which limits their reliability when labels are scarce, noisy, or inconsistent. We propose Pseudo-Labeled Graph Condensation (PLGC), a self-supervised framework that constructs latent pseudo-labels from node embeddings and optimizes condensed graphs to match the original graph's structural and feature statistics -- without requiring ground-truth labels. PLGC offers three key contributions: (1) A diagnosis of why supervised condensation fails under label noise and distribution shift. (2) A label-free condensation method that jointly learns latent prototypes and node assignments. (3) Theoretical guarantees showing that pseudo-labels preserve latent structural statistics of the original graph and ensure accurate embedding alignment. Empirically, across node classification and link prediction tasks, PLGC achieves competitive performance with state-of-the-art supervised condensation methods on clean datasets and exhibits substantial robustness under label noise, often outperforming all baselines by a significant margin. Our findings highlight the practical and theoretical advantages of self-supervised graph condensation in noisy or weakly-labeled environments.

</details>


### [100] [Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching](https://arxiv.org/abs/2601.10418)
*Nadav Merlis*

Main category: cs.LG

TL;DR: 提出自适应批处理的多步前瞻表格强化学习框架（ABP），给出最优Bellman方程、乐观性 regret 最小化算法，并给出与前瞻步长ℓ相关的近优界。


<details>
  <summary>Details</summary>
Motivation: 前瞻信息在决策中可显著提升价值，但求解最优策略在表格RL中是NP难题；现有的固定分批和模型预测控制策略存在局限，需要一个自适应、可在未知环境中学习的高效策略。

Method: 推导自适应 batching 策略（ABP）的最优 Bellman 方程，设计一种乐观性的在线学习算法以在未知环境中学习最优 ABP，并给出理论上的 regret 上界。

Result: 给出 ABP 相对于固定分批和MPC等策略的理论框架，并证明在未知环境下的 regret 上界与ℓ成 order-optimal，且通常ℓ 为常数；表明 ABP 能在多步前瞻信息条件下实现更优的学习表现。

Conclusion: ABP 为带多步前瞻的表格RL提供了一个可行且理论具备接近最优的学习方案，随着ℓ 的有限性，误差界与复杂度均可控，未来工作可扩展到更复杂的前瞻结构或连续状态/动作。

Abstract: We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.

</details>


### [101] [DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction](https://arxiv.org/abs/2601.10471)
*Zhancun Mu*

Main category: cs.LG

TL;DR: 提出 DeFlow，一种解耦离线强化学习框架，通过流匹配捕捉复杂行为流形，并在显式数据派生的信任域内使用轻量化修正模块进行优化，避免求解器微分与单步蒸馏，保留流的迭代表达能力，且在 OGBench 上表现出色并具离线到在线的适应性。


<details>
  <summary>Details</summary>
Motivation: 离线 RL 常面临高计算成本、需要通过微分求解器（如 ODE）以及难以在不牺牲迭代表达能力的前提下获得稳定改进。需要一种能精准建模行为分布、同时降低求导开销的方法。

Method: 提出一个显式、数据派生的流形信任域，在其中学习一个轻量化的修正/ refinement 模块，用以对策略进行局部改进；通过流匹配来忠实再现复杂行为流形；避免对求解器进行反向传播和不需要单步蒸馏的过程，从而保持流的迭代生成能力。

Result: 在 OGBench 基准上实现优越性能；展示了离线到在线的高效适应能力。

Conclusion: DeFlow 提供一个可稳定扩展的解耦离线 RL 框架，兼具流式表达力和低求导成本，适用于复杂行为建模和高效离线到在线迁移。

Abstract: We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.

</details>


### [102] [Communication-Efficient Federated Learning by Exploiting Spatio-Temporal Correlations of Gradients](https://arxiv.org/abs/2601.10491)
*Shenlong Zheng,Zhen Zhang,Yuhui Deng,Geyong Min,Lin Cui*

Main category: cs.LG

TL;DR: GradESTC 通过同时利用梯度的时空相关性来压缩联邦学习中的通信开销。将全梯度分解为基向量集合和系数，在时间上仅更新少量基向量，从而在上传端仅传输系数和有限的更新基向量，显著减小上行通信量，同时保持接近无压缩FedAvg的收敛速度和最终精度。


<details>
  <summary>Details</summary>
Motivation: 在带宽受限的联邦学习场景中，梯度通信开销成为瓶颈；现有方法多关注单一梯度的压缩，忽视梯度之间的时序相关性。观察到梯度存在强烈的时间相关性，且存在空间上的低秩结构，提出利用时空相关性以提高压缩效率。

Method: 将全梯度在空间上分解为基向量集合和组合系数；通过时间相关性，在新轮中仅动态更新少量基向量，其余保持不变。上行传输只需发送轻量级的组合系数及被更新的基向量，服务端使用基向量重构全梯度。

Result: 在达到接近收敛的目标精度时，GradESTC 将上行通信量平均降低约39.79%，且收敛速度和最终精度与未压缩的FedAvg相当。大规模实验表明该方法能有效利用时空梯度结构实现可扩展的通信高效FL。

Conclusion: 通过挖掘梯度的时空结构，GradESTC 提供一种实用且具可扩展性的联邦学习通信压缩方案，适用于带宽受限环境。

Abstract: Communication overhead is a critical challenge in federated learning, particularly in bandwidth-constrained networks. Although many methods have been proposed to reduce communication overhead, most focus solely on compressing individual gradients, overlooking the temporal correlations among them. Prior studies have shown that gradients exhibit spatial correlations, typically reflected in low-rank structures. Through empirical analysis, we further observe a strong temporal correlation between client gradients across adjacent rounds. Based on these observations, we propose GradESTC, a compression technique that exploits both spatial and temporal gradient correlations. GradESTC exploits spatial correlations to decompose each full gradient into a compact set of basis vectors and corresponding combination coefficients. By exploiting temporal correlations, only a small portion of the basis vectors need to be dynamically updated in each round. GradESTC significantly reduces communication overhead by transmitting lightweight combination coefficients and a limited number of updated basis vectors instead of the full gradients. Extensive experiments show that, upon reaching a target accuracy level near convergence, GradESTC reduces uplink communication by an average of 39.79% compared to the strongest baseline, while maintaining comparable convergence speed and final accuracy to uncompressed FedAvg. By effectively leveraging spatio-temporal gradient structures, GradESTC offers a practical and scalable solution for communication-efficient federated learning.

</details>


### [103] [Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning](https://arxiv.org/abs/2601.10498)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: 提出 PROMA：一种用于大模型微调的近端策略更新，通过层级投影在微批累积梯度，避免额外前向/反向计算并强化局部KL约束控制。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型微调中梯度方差、局部KL约束薄弱导致的不稳定性，以及对参考策略依赖与概率裁剪带来的局限。

Method: 在反向传播阶段分层对梯度执行投影，逐层消除序列相关的梯度分量，用于将微批次的策略梯度有效累积，同时无需额外前向/反向通道。

Result: 相较于 GRPO，PROMA 更紧地控制局部KL，提升学习稳定性；且与 PPO/GRPO 不同，PROMA 可实现近端更新且避免熵崩溃，不依赖参考策略或 likelihood-ratio clipping。

Conclusion: PROMA 提供一种高效且鲁棒的近端策略更新框架，适用于大模型微调，特别在需严格局部KL约束的场景。

Abstract: This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.

</details>


### [104] [Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models](https://arxiv.org/abs/2601.10519)
*Andrea Melis,Andrea Piroddi,Roberto Girau*

Main category: cs.LG

TL;DR: 用GPT-2生成新的调制方案以提升CR系统的谱效率和鲁棒性，表现与传统方法相当甚至优于其在SNR和PSD等指标下。


<details>
  <summary>Details</summary>
Motivation: 在频谱资源紧张的场景下，提升谱效率、鲁棒性和安全性是CR系统的核心挑战。Transformer类模型具备学习高维调制形式的潜力，能够通过生成新方案来扩展调制集。

Method: 将GPT-2在现有调制公式数据集上进行训练，使其生成新的调制方案；用SNR和PSD等指标与传统调制方法进行对比评估。

Result: 生成的调制方案在某些场景下达到与传统方法相当甚至优越的性能，显示Transformer模型可用于改进CR系统的传输策略。

Conclusion: 基于Transformer的调制方案生成在CR系统中具有潜力，可以提升效率、鲁棒性和安全性，但需要进一步验证、约束条件和实际空域环境中的可实现性。

Abstract: Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.

</details>


### [105] [Mixtures of Transparent Local Models](https://arxiv.org/abs/2601.10541)
*Niffa Cheick Oumar Diaby,Thierry Duchesne,Mario Marchand*

Main category: cs.LG

TL;DR: 提出一种可解释的局部混合模型，通过学习局部区域与透明标签函数，结合多预测器损失，给出二元线性分类和线性回归的PAC-Bayes风险界，实验显示在合成与真实数据上具竞争力。


<details>
  <summary>Details</summary>
Motivation: 提高模型透明度以便审计、保障安全与公平，认为在不同局部区域可用简单透明函数建模，但区域间可能 abruptly 变化，因此需要学习局部性结构。

Method: 提出混合局部透明模型：同时学习可解释的局部标签函数以及输入空间的局部性划分；使用新的多预测器/多局部性损失函数；推导并给出二元线性分类和线性回归的PAC-Bayes风险界；在合成数据和真实数据集上进行实验。

Result: 通过合成数据展示学习过程和算法特性；在真实数据集上，该方法与其他方法以及部分不透明模型相比具有竞争力。

Conclusion: 该工作为可解释模型提供了基于局部性与透明函数的理论与实验基础，表明混合局部透明模型在需要透明度的任务中具有潜力；未来可扩展到其他模型或更复杂的局部结构。

Abstract: The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the label of instances in some localities/regions of the input space, but may change abruptly as we move from one locality to another. Consequently, the proposed algorithm is to learn both the transparent labeling function and the locality of the input space where the labeling function achieves a small risk in its assigned locality. By using a new multi-predictor (and multi-locality) loss function, we established rigorous PAC-Bayesian risk bounds for the case of binary linear classification problem and that of linear regression. In both cases, synthetic data sets were used to illustrate how the learning algorithms work. The results obtained from real data sets highlight the competitiveness of our approach compared to other existing methods as well as certain opaque models. Keywords: PAC-Bayes, risk bounds, local models, transparent models, mixtures of local transparent models.

</details>


### [106] [Process-Guided Concept Bottleneck Model](https://arxiv.org/abs/2601.10562)
*Reza M. Asiyabi,SEOSAW Partnership,Steven Hancock,Casey Ryan*

Main category: cs.LG

TL;DR: 在 CBM 基础上引入过程引导的概念瓶颈模型（PG-CBM），通过领域定义的因果机制与生物物理可释介概念约束学习，提升地表生物量密度估计的准确性与透明度。


<details>
  <summary>Details</summary>
Motivation: 解决标准概念瓶颈模型在忽略领域特定关系与因果机制、以及对完整概念标签依赖导致在监督稀缺的科学领域应用受限的问题；通过将领域因果关系融入模型以提高可解释性和信任度。

Method: 提出 PG-CBM，在传统 CBM 基础上引入可遵循的领域因果机制约束，使用生物物理意义明确的中间概念，并在地表生物量密度（above-ground biomass density）从遥感数据的任务上进行案例研究，支持多源异构训练数据以提高鲁棒性与可解释性。

Result: 相较多种基准，PG-CBM 显著降低误差和偏差，能够产出可解释的中间输出，提升透明度并帮助检测异常学习，同时具备从多源数据中获得信息的能力。

Conclusion: 该方法代表科学应用中向更可信 AI 的迈进，促进对科学过程的可解释性理解与知识提取。

Abstract: Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.

</details>


### [107] [Combinatorial Optimization Augmented Machine Learning](https://arxiv.org/abs/2601.10583)
*Maximilian Schiffer,Heiko Hoppe,Yue Su,Louis Bouvier,Axel Parmentier*

Main category: cs.LG

TL;DR: 对COAML领域的系统性综述：提出统一框架、构建模块、建立与经验成本最小化的联系，依据不确定性形式与决策结构提出分类法，覆盖静态与动态问题的算法、应用及贡献的综合评述，并给出未来研究前沿。


<details>
  <summary>Details</summary>
Motivation: 将预测模型与组合优化决策有序结合，创造既数据驱动又可行的决策策略，弥合机器学习、运筹学和随机优化之间的鸿沟，建立统一的理论和方法论框架。

Method: 提出COAML管线的统一框架，描述方法学组成模块，并形式化与经验成本最小化的联系。基于不确定性和决策结构的形式，建立问题设置的分类法。对于静态与动态问题，回顾算法方法；调查应用领域如调度、车辆路径、随机规划、强化学习；在经验成本最小化、模仿学习、强化学习等维度综合理论贡献。

Result: 提供了COAML领域的系统性现状综述：统一框架、问题分类、方法学要点、广泛的应用领域综述，以及在经验成本最小化、模仿学习与强化学习方面的综合贡献。并指出未来研究方向，作为领域教程与路线图。

Conclusion: COAML正成为连接机器学习与组合优化的成熟研究方向，未来的研究将继续扩展应用域、完善理论框架、并深化在不确定性建模、动态决策与可行性保障方面的算法设计。

Abstract: Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.

</details>


### [108] [ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition](https://arxiv.org/abs/2601.10591)
*Arundeep Chinta,Lucas Vinh Tran,Jay Katukuri*

Main category: cs.LG

TL;DR: 提出 ProbFM 作为基于 transformer 的概率基础模型，基于 DER 实现对金融时序的不确定性进行理论上有依据的分解（ epistemic 与 aleatoric ），并在同一 LSTM 基线下与五种概率方法比较，DER 表现出竞争力的预测准确性并提供明确的不确定性分解。


<details>
  <summary>Details</summary>
Motivation: 解决金融时间序列基础模型在不确定性量化方面的核心限制：需要不依赖严格分布假设、避免混淆不同不确定性来源，并实现可 principled 的校准；通过 DER 实现高阶证据学习以获得高效的一次通过推断。

Method: 提出 ProbFM，一种转化器基础的概率框架，使用深证据回归（DER）进行不确定性建模，避免预设分布形式且实现一次通过推断；通过在同一 LSTM 架构下对比五种 probabilistic 方法（DER、Gaussian NLL、Student's-t NLL、Quantile Loss、Conformal Prediction）进行严格对照评估；在加密货币收益预测任务上评估不确定性分解与预测效果。

Result: DER 在保持竞争力的预测精度的同时，提供明确的 epistemic-aleatoric 不确定性分解；证明 DER 的有效性并在金融应用中展示可扩展性和实证证据；在控制实验中，相同框架下 DER 的分解表现出色。

Conclusion: 工作确立了一个可扩展的、面向 foundation 模型的原则性不确定性量化框架，并为 DER 在金融领域的有效性提供了实证证据。

Abstract: Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.

</details>


### [109] [Data-driven stochastic reduced-order modeling of parametrized dynamical systems](https://arxiv.org/abs/2601.10690)
*Andrew F. Ilersich,Kevin Course,Prasanth B. Nair*

Main category: cs.LG

TL;DR: 提出一种基于 amortized stochastic variational inference 的连续时间随机ROM框架，通过对潜在状态建模为马可夫高斯过程的SDE，达成跨参数与强迫条件的泛化和预测不确定性的量化，训练无需昂贵求解器，且可结合物理信息先验；在三个测试问题上显示出良好泛化与显著的效率提升。


<details>
  <summary>Details</summary>
Motivation: 高保真仿真成本高，传统ROM在随机动力学和不确定性量化方面能力不足，难以在多参数和外部强迫条件下实现稳健泛化与鲁棒决策。

Method: 采用将推断问题转化为无偏的高效优化的 amortized stochastic variational inference；对马可夫高斯过程进行重参数化以实现对潜在动力学的连续时间SDE建模；联合训练一个概率自编码器和潜在SDE，训练成本与数据规模和系统刚性无关；可利用物理信息先验增强学习；训练过程中避免对昂贵的正向求解器依赖。

Result: 在三个具有挑战性的测试问题上实现对未见参数和外力的良好泛化，并与现有方法相比获得显著的计算效率提升，同时具备对预测不确定性的量化能力。

Conclusion: 所提框架在保留不确定性估计的同时提升泛化性与训练效率，适合跨条件的连续时间随机ROM，并可灵活接入物理先验以增强模型约束。

Abstract: Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.

</details>


### [110] [Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis](https://arxiv.org/abs/2601.10701)
*Chun Hei Michael Shiu,Chih Wei Ling*

Main category: cs.LG

TL;DR: CEPAM 提供一种可同时提升通信效率与隐私保护的联邦学习方案，基于 RSUQ 的随机向量量化，提供可调隐私保护，并具理论保证与实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中的通信成本与跨方隐私保护的挑战，需具备隐私保证与收敛性分析，并评估实际效用。

Method: 提出 CEPAM，利用拒绝采样的通用量化器 RSUQ，使量化误差等同于可设定的噪声，从而实现可控隐私；对 CEPAM 的隐私保证与收敛性进行理论分析；通过实验比较与基线的收敛性，以及不同参与方之间的准确性-隐私权衡。

Result: 给出理论分析的隐私与收敛性结论，并通过实验验证 CEPAM 在收敛性方面与基线相媲美，展示不同参与方之间的准确性与隐私权衡特性。

Conclusion: CEPAM 能同时实现通信高效与隐私可适配的联邦学习，其理论分析与实验结果支持其有效性与实用性。

Abstract: Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.

</details>


### [111] [Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication](https://arxiv.org/abs/2601.10705)
*Keval Jain,Anant Raj,Saurav Prakash,Girish Varma*

Main category: cs.LG

TL;DR: 提出一种确定性时滞分布的服务端聚合规则（staleness-bucket aggregation with padding），在半异步的IPM风格感知器训练中，对三种系统效应：更新延迟、部分参与、通信噪声，给出有限时域内加权错例累计的期望界限。延迟影响仅通过平均时滞体现，通信噪声贡献一个与时域平方根相关的项；无噪声时在新鲜参与条件下给出明确的稳定化界限。


<details>
  <summary>Details</summary>
Motivation: 在联邦/分布式部署中，现实系统存在更新延迟、参与不完全和上下链路噪声等问题，需给出鲁棒的感知器训练与聚合策略的理论保障。

Method: 提出并分析staleness-bucket aggregation with padding；在margin separability与数据半径有界的假设下，推导有限时域的期望错例界限；将延迟影响与噪声分别列出，延迟通过平均时滞体现，噪声项随时域增大而与噪声能量平方根相关；给出无噪声情形的稳定化分析，需新鲜参与条件。

Result: 给出有限时域的期望加权错例累积界限，延迟影响经由平均时滞体现，噪声项为随时域平方根增长且与总噪声能量相关。

Conclusion: 通过确定性时滞聚合规则实现鲁棒的分布式感知器训练；在分布式与联邦学习场景下提供稳定性与误差界，且给出无噪声与新鲜参与条件下的明确稳定化结果。

Abstract: We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.

</details>


### [112] [High-accuracy and dimension-free sampling with diffusions](https://arxiv.org/abs/2601.10708)
*Khashayar Gatmiry,Sitan Chen,Adil Salim*

Main category: cs.LG

TL;DR: 提出一种新的扩散模型求解器，通过低阶近似与插值法( collocation )的耦合，实现对误差阈值1/epsilon的迭代复杂度呈多对数级(polylog)增长，并且对维度的显式依赖被降至仅通过目标分布的有效半径影响；这是在仅能获得近似分布分数的前提下，首次给出高精度(diffusion-based)采样的理论保证。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的推断通常需离散化求解高维常微分方程，现有方法的迭代复杂度对维度与误差的逆次方呈多项式增长，限制了高精度采样的可行性。需要在不显式依赖维度的前提下实现高精度采样的理论保障。

Method: 将低阶多项式近似与插值/ collocation 方法相结合，利用对数据分布分数（scores）的近似访问，在不完全依赖高维信息的情况下构造求解器；分析表明迭代次数对1/epsilon仅呈多对数增长，且维度影响仅经目标分布的有效半径传导。

Result: 给出对 diffusion 模型求解器的理论迭代复杂度界限：在可获得的分数近似下，迭代次数与1/epsilon的对数相关，首次达到高精度采样的理论保证；维度不再以显式项出现，只有通过目标分布的有效半径影响复杂度。

Conclusion: 该方法在理论上实现了对扩散型采样的高精度保证，同时降低了对维度的敏感性，强调低阶近似与 collocation 的组合在 score-based 采样中的潜力。

Abstract: Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.

</details>


### [113] [DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids](https://arxiv.org/abs/2601.10715)
*Navami Kairanda,Shanthika Naik,Marc Habermann,Avinash Sharma,Christian Theobalt,Vladislav Golyanik*

Main category: cs.LG

TL;DR: 提出了 DInf-Grid，一种基于网格的可微分表示，结合径向基函数插值和多分辨率同位网格，能够高效求解偏微分方程（Poisson、Helmholtz、Kirchhoff-Love等）并以更高的导数可导性解决问题。与坐标式 MLP 求解器相比，获得5-20倍加速，精度与紧凑性相当。


<details>
  <summary>Details</summary>
Motivation: 现有的神经求解器多采用坐标式 MLP，计算成本高且训练慢；基于网格的隐式表示在训练速度上有优势，但线性插值限制了高阶导数，难以直接用于求解 DEs。需要兼具高阶可微性、对高频解的捕获能力和全局梯度计算稳定性的表示。

Method: 在特征网格基础上引入无限可微的径向基函数插值，并通过对齐网格的多分辨率分解实现高效表达；以偏微分方程的损失函数进行隐式训练，使模型能够准确拟合物理场并求解 DEs。

Result: 在 Poisson、Helmholtz 与 Kirchhoff-Love 边值问题上验证了该表示的有效性，获得相较坐标式 MLP 求解器的5-20倍加速，求解时间从秒到分钟级，且保持可比的精度与紧凑性。

Conclusion: DInf-Grid 提供了一种高效、可微的网格化偏微分方程求解框架，通过无限可微的插值和多分辨率网格实现对高频解的稳健捕获与全局梯度计算，适用于广泛的物理场建模。

Abstract: We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [114] [STCRank: Spatio-temporal Collaborative Ranking for Interactive Recommender System at Kuaishou E-shop](https://arxiv.org/abs/2601.10027)
*Boyang Xia,Ruilin Bao,Hanjun Jiang,Jun Wang,Wenwu Ou*

Main category: cs.IR

TL;DR: 提出 STCRank 的跨时空协同排序框架，用于交互式推荐场景以缓解多目标冲突和全屏 UI 导致的序列性陷阱，通过多目标协作（MOC）和多槽协作（MSC）实现全局最优，已在快手 E-shop 部署，带来购买与日活共同增长。


<details>
  <summary>Details</summary>
Motivation: 面对全屏 UI 和滑动下拉带来的两类挑战：1) 多目标之间的显性干扰/冲突（转化率、浏览/停留、滑动下拉）；2) 序列推荐槽之间的时间性贪心陷阱，需在同一时空与跨槽层面实现协同优化。

Method: 提出 STCRank 框架：在同一槽内通过多目标协作模块（MOC）推 Pareto 前沿，缓解目标重叠和冲突；在多槽层面通过多槽协作模块（MSC）采用双阶段前瞻排序实现全局最优的序列目标。

Result: 在离线与在线实验中证明了购买与日活共同增长的效果；系统已于 2025.6 部署在快手电商端。

Conclusion: STCRank 通过跨目标与跨槽的协同排序，缓解了全屏 UI 场景下的干扰与时序陷阱，具备落地应用潜力，但需进一步披露评估细节、鲁棒性、与多域部署的扩展性。

Abstract: As a popular e-commerce platform, Kuaishou E-shop provides precise personalized product recommendations to tens of millions of users every day. To better respond real-time user feedback, we have deployed an interactive recommender system (IRS) alongside our core homepage recommender system. This IRS is triggered by user click on homepage, and generates a series of highly relevant recommendations based on the clicked item to meet focused browsing demands. Different from traditional e-commerce RecSys, the full-screen UI and immersive swiping down functionality present two distinct challenges for regular ranking system. First, there exists explicit interference (overlap or conflicts) between ranking objectives, i.e., conversion, view and swipe down. This is because there are intrinsic behavioral co-occurrences under the premise of immersive browsing and swiping down functionality. Second, the ranking system is prone to temporal greedy traps in sequential recommendation slot transitions, which is caused by full-screen UI design. To alleviate these challenges, we propose a novel Spatio-temporal collaborative ranking (STCRank) framework to achieve collaboration between multi-objectives within one slot (spatial) and between multiple sequential recommondation slots. In multi-objective collaboration (MOC) module, we push Pareto frontier by mitigating the objective overlaps and conflicts. In multi-slot collaboration (MSC) module, we achieve global optima on overall sequential slots by dual-stage look-ahead ranking mechanism. Extensive experiments demonstrate our proposed method brings about purchase and DAU co-growth. The proposed system has been already deployed at Kuaishou E-shop since 2025.6.

</details>


### [115] [Development of Ontological Knowledge Bases by Leveraging Large Language Models](https://arxiv.org/abs/2601.10436)
*Le Ngoc Luyen,Marie-Hélène Abel,Philippe Gouspillou*

Main category: cs.IR

TL;DR: 提出基于大模型的分步迭代方法来自动化和提升本体知识库（OKB）开发，以案例（车销领域的用户上下文画像本体）验证，显著提高构建速度和一致性，并实现偏见缓解与透明性提升。


<details>
  <summary>Details</summary>
Motivation: 传统 OKB 的人工开发在可扩展性、一致性与适应性方面存在挑战，需自动化与可持续迭代的方法。

Method: 结构化、迭代的工作流程，利用生成式 AI/LLMs 进行知识获取、本体工件生成和持续改进循环；以车辆销售领域的用户上下文画像本体为案例，展示全过程。

Result: 显著加速本体构建、提升本体一致性、有效偏见缓解、提升本体工程过程的透明性；提高可扩展性与集成能力，改善知识管理系统效率。

Conclusion: 表明将 LLMs 融入本体开发具有变革潜力，可提升可扩展性、集成性和整体效率，推动 OKB 的现代化。

Abstract: Ontological Knowledge Bases (OKBs) play a vital role in structuring domain-specific knowledge and serve as a foundation for effective knowledge management systems. However, their traditional manual development poses significant challenges related to scalability, consistency, and adaptability. Recent advancements in Generative AI, particularly Large Language Models (LLMs), offer promising solutions for automating and enhancing OKB development. This paper introduces a structured, iterative methodology leveraging LLMs to optimize knowledge acquisition, automate ontology artifact generation, and enable continuous refinement cycles. We demonstrate this approach through a detailed case study focused on developing a user context profile ontology within the vehicle sales domain. Key contributions include significantly accelerated ontology construction processes, improved ontological consistency, effective bias mitigation, and enhanced transparency in the ontology engineering process. Our findings highlight the transformative potential of integrating LLMs into ontology development, notably improving scalability, integration capabilities, and overall efficiency in knowledge management systems.

</details>


### [116] [iTIMO: An LLM-empowered Synthesis Dataset for Travel Itinerary Modification](https://arxiv.org/abs/2601.10609)
*Zhuoxuan Huang,Yunshan Ma,Hongyu Zhang,Hua Ma,Zhu Sun*

Main category: cs.IR

TL;DR: 提出 iTIMO 数据集以研究日程修改任务，通过意图驱动的扰动生成真实行程数据，采用 REPLACE/ADD/DELETE 三种原子编辑、围绕受欢迎度、空间距离、类别多样性三大意图，提供混合评估指标，实验显示现有 LLMs 的局限性并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决现实旅行体验中频繁的日程修改需求与现有仅关注固定行程规划的研究之间的差距，缺乏可用于学习和评估日程修改的数据与方法。

Method: 将数据生成视为意图驱动的扰动任务，使用大型语言模型对真实行程进行原子编辑（REPLACE、ADD、DELETE）。扰动基于三种意图：偏好受欢迎程度的变化、与当前位置的空间距离变化、以及类别多样性的提升或变化。设计混合评估指标以衡量扰动的效果和保持任务可控性。

Result: 在 iTIMO 上进行系统实验，揭示当前 LLMs 在日程修改任务上的局限，提供若干未来研究方向。

Conclusion: iTIMO 提供了一个面向日程修改研究的基线数据集与生成管线，数据及代码开源，推动未来在此领域的研究进展。

Abstract: Addressing itinerary modification is crucial for enhancing the travel experience as it is a frequent requirement during traveling. However, existing research mainly focuses on fixed itinerary planning, leaving modification underexplored. To bridge this gap, we formally define the itinerary modification task and introduce iTIMO, a dataset specifically tailored for this purpose. We identify the lack of {\itshape need-to-modify} itinerary data as the critical bottleneck hindering research on this task and propose a general pipeline to overcome it. This pipeline frames the generation of such data as an intent-driven perturbation task. It instructs large language models to perturb real world itineraries using three atomic editing operations: REPLACE, ADD, and DELETE. Each perturbation is grounded in three intents, including disruptions of popularity, spatial distance, and category diversity. Furthermore, a hybrid evaluation metric is designed to ensure perturbation effectiveness. We conduct comprehensive experiments on iTIMO, revealing the limitations of current LLMs and lead to several valuable directions for future research. Dataset and corresponding code are available at https://github.com/zelo2/iTIMO.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [117] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 提出一个用于AI存在性风险的通用框架及生存故事分层，基于两大前提：AI极端强大与若强大则会毁灭人类，并据此构建多种生存路径的分类及对风险的粗略P(doom)估计。


<details>
  <summary>Details</summary>
Motivation: 澄清AI存在性风险的结构性问题，帮助研究者与政策制定者理解不同生存路径的挑战，并据此制定相应的缓解策略。

Method: 构造基于两前提的论证框架；提出一个生存故事的分类法（科学屏障、禁研、目标不对头、能检测并禁用的系统等）；分析每种故事的挑战与应对；基于该框架给出对doom概率的粗略估计。

Result: 得到一个可操作的框架和分支分类，以及不同生存故事面临的挑战及相应政策含义；初步对P(doom)给出不确定但可比较的定性估值。

Conclusion: 该框架有助于系统理解AI风险的来源与缓解路径，强调不同情景下的不同对策，并提供对未来研究的方向与量化估计的基础。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [118] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: LLMs trigger social sanctions: participants punished peers who relied on models for real-effort tasks; punishment rises with actual use; disclosures of no-use trigger suspicion; at high use, actual reliance punished more than self-reported; efficiency gains come with social costs.


<details>
  <summary>Details</summary>
Motivation: Investigate whether negative attitudes toward AI translate into costly actions, and how disclosure and self-reporting affect social sanction dynamics.

Method: Two-phase online experiment (Phase I targets, Phase II N=491). Participants could spend part of their endowment to punish peers who completed a real-effort task with or without LLM support; manipulations included actual use vs disclosed use vs self-reported use; measured earnings destruction as punishment.

Result: Peers who used the model exclusively had 36% of their earnings destroyed on average; punishment scales with actual LLM use. Disclosure of LLM use reduced credibility (self-reported no-use punished more than actual no-use), while at high use levels, actual use punished more than self-reported use.

Conclusion: LLM efficiency gains incur social sanctions; first behavioral evidence that the societal costs of AI adoption include punitive actions against users, potentially constraining adoption despite productivity benefits.

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [119] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 提出一个非交互式端到端的推理框架AAI，通过在少样本提示中引入结构信息来激活与逻辑推理运算符相关的注意力头，并在推理时对选定头部的注意力进行重新加权，以引导模型的推理方向，提升逻辑推理性能且开销极低。


<details>
  <summary>Details</summary>
Motivation: 当前的逻辑推理方法要么依赖复杂的互动框架，要么依赖外部组件，导致可扩展性和管控性受限。需要一个不依赖外部资源、可嵌入模型内部、具备良好可分析性的端到端推理框架。

Method: 在少样本提示中引入结构信息，使部分注意力头呈现与逻辑推理操作符相匹配的模式；提出Attention-Aware Intervention（AAI），在推断阶段重新加权选中的注意力头的注意力分数，以引导模型利用先验知识进行推理。

Result: AAI在多种基准和模型架构上提升了逻辑推理性能，且额外开销很小。

Conclusion: 端到端的非交互式推理是可行的，注意力调制方法AAI提供了一种高效、可扩展的推理改进途径，代码公开。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [120] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: Min-Seek 是一种训练无关的顺序性测试时推理扩展方法，通过保留一个额外“诱发思想”的 KV 键值对并使用无位置嵌入的自定义 KV 缓存，使模型在超出最大上下文长度的情况下持续推理，同时具有线性时间复杂度，在不需额外微调的前提下显著提升多任务下的推理准确性并稳定性。


<details>
  <summary>Details</summary>
Motivation: 虽然增加推理长度可能提升准确性，但更长推理会导致准确性下降和模型不稳定；现有方法在训练阶段或微调阶段存在限制。因此需要一种训练无关、可在测试阶段提升大规模推理模型性能且对推理长度波动具备鲁棒性的解决方案。

Method: 提出 Min-Seek，逐步测试时扩展的推理技巧。通过一个自定义 KV 缓存，在每次生成新的推理思路前，将与之相关的键值对连续编码且仅保留一个额外思路的键值对，且缓存中存储的键不使用位置嵌入；利用动态编码实现续写超出模型最大上下文长度的推理，理论上在温和条件下具有线性复杂度。

Result: 在广泛的诱发思路范围内，显著提升模型准确性；能够稳定顺序扩展带来的准确性波动并消除对推理长度的微调需求。该方法与训练无关且高效，因仅增加一个额外思路的 KV 对而节省内存/计算。

Conclusion: Min-Seek 提供一种高效、可扩展且鲁棒的测试时推理扩展策略，适用于多任务推理，能在不显式微调的情况下提升准确性并稳定性能，且理论上实现接近线性时间复杂度。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [121] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: CMA provides a persistent, dynamic memory system for LLM agents, enabling stateful memory and long-horizon reasoning beyond RAG's stateless retrieval.


<details>
  <summary>Details</summary>
Motivation: RAG's stateless memory cannot accumulate or disambiguate information across interactions; a long-horizon agent requires persistent and updateable memory.

Method: Define architectural requirements for CMA (persistent storage, selective retention, associative routing, temporal chaining, consolidation into higher-order abstractions) and evaluate via empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) without specifying implementation.

Result: CMA yields consistent behavioral advantages over RAG on tasks requiring memory accumulation and mutation; shows CMA is a necessary primitive for long-horizon agents.

Conclusion: CMA is a necessary architectural primitive for long-horizon agents; challenges remain in latency, drift, interpretability; future work needed on optimization and transparency.

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [122] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 将人机互补性从相对预测准确度转向基于 epistemic 可靠性的框架，借助计算可靠主义，以历史实例作为证据评估人-机协作的可靠性，并据此校准实际决策。


<details>
  <summary>Details</summary>
Motivation: 补充性在理论上缺乏锚定、仅以后验预测准确度表征、忽视行为 desiderata 与代价-收益等问题，导致实证获得困难。需在 justificatory AI 框架下给出可操作的、基于可靠性的理论基础。

Method: 借用知识论视角与计算可靠主义，将历史中出现的互补性视为对特定预测任务的可靠性证据；结合对齐 epistemic standards 与 socio-technical practices 的可靠性指标，来评估人-机团队的推理与输出。

Result: 提出将互补性作为提升预测过程可靠性的机制来理解，而非仅仅追求相对准确度；强调在决策中据此校准对 AI 支撑过程的信赖与使用。

Conclusion: 人机互补性的价值在于帮助将决策归因于 AI 支撑过程的可靠性，并在涉及患者、管理者、监管者等的实际决策情境中发挥作用。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [123] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: Single-Shot Planning for CUAs: 通过在观测潜在恶意内容之前由可信规划者生成完整执行图来实现对指令注入的强控制流完整性保障，并指出需要额外措施防止分支引导攻击；在 OSWorld 上评估，性能接近前沿模型，且对小型开源模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决CUA在保持安全的同时需要持续观察UI状态所带来的挑战；现有的架构隔离难以与UI驱动的自动化兼容，需提出新的安全方案以抵御提示注入。

Method: 提出单-shot规划（Single-Shot Planning for CUAs）：可信规划者在观测任何潜在的恶意内容之前生成包含条件分支的完整执行图，从而对任意指令注入提供控制流完整性保障；并讨论分支引导攻击的潜在防护。

Result: 实现对指令注入的控制流完整性；架构隔离有效防止注入，但需额外措施防止分支引导攻击；在 OSWorld 上评估，保留高达 57% 的前沿模型性能，对较小的开源模型提升可达 19%。

Conclusion: 证明了在CUAs中通过可信规划结合架构隔离可以实现安全性与实用性的兼容，但分支引导攻击的防护仍需完善。

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [124] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 提出一个面向高风险领域的幻觉管理综合框架，结合模型、数据、情境层面的检测与分层缓解，通过闭环实现可持续提升。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs和LRMs在高风险领域（如金融、法律）的幻觉问题，提升系统可信度、可追溯性和合规性。

Method: 将幻觉源头划分为模型、数据与情境三类因素，结合不确定性估计、推理一致性等检测手段，设计知识 grounding、置信度校准等缓解策略，采用分层架构并以金融数据提取案例研究作为验证，形成模型、情境、数据三层的闭环反馈。

Result: 在金融数据提取案例中验证框架的可行性与有效性，展示其对提高可用性和可靠性的潜在作用。

Conclusion: 提供一个系统、可扩展的可信生成AI架构，适用于受监管环境，促进持续改进与可追溯的幻觉管理。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [125] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: 提出 LabourLawLLM 与 LabourLawBench，针对中国劳动法的专用大语言模型及评测基准，在多项任务上显著优于通用及现有法律LLM。


<details>
  <summary>Details</summary>
Motivation: 解决通用大模型在法律子领域缺乏精确法律知识、复杂推理和情境敏感度的问题，需定制化的法律子领域模型以提升可靠性和应用价值。

Method: 训练 LabourLawLLM 于中国劳动法领域，构建 LabourLawBench 包含法律条文引用、知识问答、案情分类、赔偿计算、命名实体识别和法律案例分析等任务；评测结合客观指标（ROUGE-L、准确率、F1、soft-F1）与基于 GPT-4 的主观评分；与通用模型及现有法律LLM 比较。

Result: 实验显示 LabourLawLLM 在所有任务类别均领先对比模型，提升了条文引用准确性、问答准确性、分类与计算等关键指标，GPT-4 评分也对其结果给予更高分。

Conclusion: 该方法为构建其他法律子领域的专用 LLM 提供可扩展的模板，提升准确性、可靠性和法律AI的社会价值。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [126] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL 是一个训练无参与、通过结构化分解和经验感知自纠错提升鲁棒性的 NL2SQL 框架，在 BIRD 上达到 68.5% 执行准确率，超越开源、零微调方法，且资源消耗低于前代 TTS 方法十倍以上。


<details>
  <summary>Details</summary>
Motivation: 解决两个核心问题：一是现有 NL2SQL 依赖仅正确示例的 in-context 学习，忽略历史错误-修正对自我纠正的信号；二是测试时的分解往往随意，导致候选 SQL 大同小异，削弱集成收益。此外，存在高性能与高成本之间的权衡。

Method: 提出两点：一是结构化分解，采用实体维度、分层和原子序列三种策略，以促成多样化的推理；二是基于经验的自纠错，构建一个包含成功查询和历史错误-修正对的动态记忆，并在推理时通过检索-增强提示引入相关示例，无需微调或外部 API。

Result: 在 BIRD 数据集上达到 68.5% 的执行准确率，成为公开、零微调方法的最新 SOTA；同时相比此前的 TTS 方法，资源消耗超过 10 倍的节省。

Conclusion: Memo-SQL 通过结合结构化分解与基于经验的自纠错，提升鲁棒性与效率，展示了在无训练调整条件下的强劲 NL2SQL 解决能力，并具有广泛的应用潜力。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [127] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: FilDeep 是一个基于保真度的深度学习框架，用于大变形弹塑性固体的数值模拟。通过同时利用低保真（大量但精度低）和高保真（少量但高精度）数据，并引入注意力驱动的跨保真模块来捕捉长程物理相互作用，实现对大变形问题的高性能近似。首次在大变形问题中采用多保真数据，实验表明在拉伸弯曲等任务上达到与或超越现有方法的状态-of-the-art。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习在大变形问题中对高质量大规模数据的依赖难题，即数据量与数据精度之间的矛盾，导致模型性能受限。

Method: 提出 FilDeep，采用多保真学习（低保真数据与高保真数据的联合训练），并设计注意力驱动的跨保真模块以有效捕捉跨MF数据的长程物理相互作用。针对伸展—弯曲等大变形问题进行实际设计与实现，声称首次在大变形问题中使用多保真数据。

Result: 大量实验表明 FilDeep 在所选任务上具有稳定的状态-of-the-art性能，并可在制造场景中高效部署。

Conclusion: FilDeep 成功缓解大变形问题的数量-精度矛盾，首次将多保真数据引入大变形DL框架，并具备实际工程可部署性。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [128] [MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning](https://arxiv.org/abs/2601.10101)
*Ke Chen,Jiandian Zeng,Zihao Peng,Guo Li,Guangxue Zhang,Tian Wang*

Main category: cs.AI

TL;DR: Proposes MatrixCoT, a structured chain-of-thought framework using a matrix-based plan to improve logical reasoning in LLMs without external solvers, enhancing robustness and interpretability while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of standard chain-of-thought on symbolic and deductive tasks, the brittleness of external solvers, and the lack of structured representations and error-correction in LLM-driven methods.

Method: Normalize and type natural language expressions, attach explicit citation fields, and implement a matrix-based planning method to preserve global relations among steps. The plan is a verifiable artifact. Introduces a feedback-driven replanning mechanism under semantic-equivalence constraints to detect omissions/defects and rewrite/compress the dependency matrix.

Result: Evaluated on five logical-reasoning benchmarks and five LLMs; without external solvers, MatrixCoT improves robustness and interpretability on complex symbolic reasoning while maintaining competitive performance.

Conclusion: MatrixCoT provides a solver-free, structured, verifiable approach that enhances logical reasoning capabilities of LLMs by using a matrix-based plan and feedback-driven replanning to improve robustness and interpretability.

Abstract: As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.

</details>


### [129] [Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs](https://arxiv.org/abs/2601.10114)
*Cheng Feng,Chaoliang Zhong,Jun Sun,Yusuke Oishi*

Main category: cs.AI

TL;DR: 在域特定任务中，若学生在一个受益子域（SFS）的优势足以抵消在教师偏好子域（TFS）的劣势，学生可超越教师；提出 Scheduled Checkpoint Distillation (SCD) 和自适应加权（AW），在多任务、多语言设置下实现对教师的匹配或超越。


<details>
  <summary>Details</summary>
Motivation: 蒸馏中教师-学生能力差距往往限制学生性能；需要明确何时、如何让学生在域任务中超越教师，并设计能够放大学生优势、抑制劣势的机制。

Method: 提出两部曲：1) Scheduled Checkpoint Distillation (SCD)：在监督微调阶段按计划模拟教师的收敛以减少对 TFS 的劣势；2) 样本级自适应加权（AW）：按子域偏好对训练样本分配权重，以放大对 SFS 的优势。应用于 QA、NER、文本分类等域任务，覆盖多语言场景。

Result: 实验结果表明该方法在多域任务上持续优于现有蒸馏方法，使学生达到甚至超过经微调的教师。

Conclusion: 理论洞见与 SCD+AW 的结合显著提升蒸馏效果，具有良好的一般化和跨任务应用潜力。

Abstract: Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.

</details>


### [130] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen: a fragment-level, retrieval-augmented, two-stage framework for multi-property constrained molecule generation, combining retrieval-guided prototype edits with RL-based, GRPO-optimized refinements to meet numeric property targets.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with precise multi-objective numeric control in molecule design; existing graph-based or purely generative methods lack reliable, controllable fine-grained optimization under multiple property constraints.

Method: Stage I: prototype generation via a multi-agent reasoner performing retrieval-anchored, fragment-level edits to approach the feasible region. Stage II: RL-based fine-grained optimization using a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) to perform one- or multi-hop refinements, minimizing property errors while controlling edit complexity and deviation from the prototype. A large, curated dataset of reasoning chains and measured property deltas supports deterministic supervision and multi-hop reasoning.

Result: Empirical experiments under two sets of property constraints (QED, LogP, Molecular Weight; HOMO, LUMO) show consistent improvements in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

Conclusion: MolGen effectively leverages fragment-level retrieval and controlled refinements to achieve accurate multi-property molecule design, offering deterministic supervision and scalable multi-hop reasoning; it presents a promising direction for numerically constrained molecular generation.

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [131] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLMs能在简单统计基线之上，但仍落后专用ML模型在预测重复行为的时间间隔方面；上下文量对性能呈非单调影响，更多高层次用户信息并不总是有益，提示需要混合模型以兼顾统计精度与语言灵活性。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在从结构化行为数据中推断时间规律的能力，以及上下文信息对其预测行为的影响，填补其在定量时间结构推断方面的研究空白。

Method: 在一个简单的重复购买场景中，对最先进的LLMs进行零-shot评估，并与统计基线和机器学习模型进行对比；系统性地改变上下文信息的层级（包括用户级信息）以观察对预测准确性的影响。

Result: LLMs在轻量统计基线之上有所超越，但在捕捉定量时间结构方面始终不及专用ML模型；适度的上下文可提升准确率，进一步增加用户级别细节却会降低性能，挑战了“更多上下文=更好推理”的假设。

Conclusion: 当前的LLMs在结构化时间推断方面存在基本局限，需设计面向上下文感知的混合模型，将统计精度与语言灵活性结合，以提升对时间序列型行为数据的预测能力。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [132] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 提出一个漂移感知的数据流系统，将自适应数据生成与计划-调度结合，通过梯度双层优化实现可微框架下的数据增强、课程学习和工作流管理，提升金融模型在动态市场中的鲁棒性和风险调整后的回报。


<details>
  <summary>Details</summary>
Motivation: 在量化金融中，训练数据与真实世界分布存在概念漂移和非平稳性，静态历史数据易过拟合，导致泛化能力不足。历史数据的分布难以覆盖未来市场，需通过自适应数据生成与可持续演化来缩小训练-现实差距。

Method: 提出漂移感知数据流框架：参数化数据操作模块包含单股票变换、跨股票混合和筛选等操作；自适应规划器-调度器采用梯度为基础的双层优化来控制数据生成、任务调度及工作流。框架将数据增强、课程学习和数据工作流统一为一个可微的整体，支持 provenance-aware 回放与持续数据质量监控。

Result: 在预测任务和强化学习交易任务中进行大量实验，显示该框架提升模型鲁棒性并改善风险调整后的回报。

Conclusion: 为金融数据提供一种可泛化的自适应数据管理和学习驱动的工作流自动化方法。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [133] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: DecisionLLM把轨迹作为独立模态，将离线长时序决策问题转化为语言任务的自回归建模框架；在3B模型下显著超越Decision Transformer（DT），在Maze2D和AuctionNet上分别获得大幅提升，揭示模型规模、数据量与数据质量三要素的扩展规律。


<details>
  <summary>Details</summary>
Motivation: LLMs在规模与推理能力方面已展现强大潜力，但其对连续数值的原生理解不足。本文通过把轨迹作为单独模态并与自然语言任务描述对齐，构建一个联合Transformer框架DecisionLLM，以提升长时序离线决策的性能，并探究模型规模、数据量、数据质量对效果的影响。

Method: 将决策轨迹视为独立模态，与自然语言任务描述对齐，训练一个自回归的决策生成模型DecisionLLM；扩展AIGB范式，基于离线数据在Transformer家族框架下实现长序列决策预测。

Result: DecisionLLM-3B在Maze2D umaze-v1上相较DT提升69.4，在AuctionNet上提升0.085，且在离线基准与竞价场景表现出色。

Conclusion: 证明了规模、数据量与数据质量三要素对决策性能的关键作用，扩展了AIGB范式，指向在线竞价等场景的潜在研究方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [134] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: MHub.ai 是一个开源、容器化的平台，用以标准化访问医疗影像AI模型，提升可重复性与对比性；将论文模型打包成容器，附带元数据与参考数据，提供统一接口和可视化评估。


<details>
  <summary>Details</summary>
Motivation: 医疗影像AI领域存在模型多样性、文档不一致和可重复性不足等挑战，亟需一个标准化、易用的平台来促进模型比较、验证和临床转化。

Method: 将来自同行评审论文的模型打包成标准容器，支持直接处理 DICOM 等格式，提供统一应用接口、嵌入结构化元数据；每个模型附有公开参考数据以验证模型运作；初始提供多模态的分割、预测和特征提取模型，模块化框架支持任意模型与社区贡献；通过对肺分割的对比评估展示平台应用，公开分割结果与评估指标，并提供交互式仪表板以便重现实验与扩展分析。

Result: 完成平台的初步实现：容器化模型、统一接口、元数据与参考数据、可追溯的评估与输出；开展肺部分割模型的对比评估；公开分割结果与评估指标，提供交互式仪表板以支持逐案例检视与再现分析。

Conclusion: 通过简化模型使用、在相同执行命令下进行横向基准、输出标准化，MHub.ai 能提升透明度、可重复性并降低临床转化壁垒，具有良好的扩展性与社区协作潜力。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [135] [MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning](https://arxiv.org/abs/2601.10157)
*Yusong Wang,Jialun Shen,Zhihao Wu,Yicheng Xu,Shiyin Tan,Mingkun Xu,Changshuo Wang,Zixing Song,Prayag Tiwari*

Main category: cs.AI

TL;DR: 多视角图 + MoE 融合提升蛋白质表示学习


<details>
  <summary>Details</summary>
Motivation: 单视角图在蛋白质残基相互作用表示中只捕捉了局部或部分属性，导致表示不完整；需要整合多方信息以获得更完整的蛋白质表征

Method: 从物理、化学、几何三个视角构建蛋白质残基相互作用图；通过混合专家（MoE）对这三种视角进行自适应融合，专家在不同水平上专门学习视角特征、跨视角交互与全局共识

Result: 通过多层次的信息整合，获得更优的蛋白质表示，在四个下游任务上实现更好的性能，证明 MoE 能自动识别和利用各视角在不同交互层次上的特征

Conclusion: 多视角图构建结合 MoE 融合是提升 PRL 的有效策略，能够建模单视角到跨视角及全局的交互关系

Abstract: Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. To address this limitation, we propose MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions. To capture both perspective-specific features and their synergies, we develop an MoE module, which dynamically routes perspectives to specialized experts, where experts learn intrinsic features and cross-perspective interactions. We quantitatively verify that MoE automatically specializes experts in modeling distinct levels of interaction from individual representations, to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. Through integrating this multi-level information, MMPG produces superior protein representations and achieves advanced performance on four different downstream protein tasks.

</details>


### [136] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: 通过将形状感知的失真度量与基于特征的机器学习分类结合，系统评估不同下采样算法对高频nEMG信号的信息损失与诊断性能，提出可在近实时分析中权衡数据缩减与模型性能的工作流。


<details>
  <summary>Details</summary>
Motivation: nEMG信号采样率高且差异性大，直接用于特征/分类的计算成本高；需要在尽量保留诊断信息的前提下实现下采样以降低计算负担。

Method: 提出一个工作流，结合形状基础的失真度量、不同下采样算法、以及基于特征的分类模型与特征空间分析，系统评估信息损失对分类性能的影响，并在三类NMD分类任务中进行实验验证。

Result: 形状感知下采样算法在保留峰值结构和整体形态方面优于简单的降采样/抽样，能在显著降低计算负担的同时维持诊断信息；工作流可识别在不显著损失信息的前提下的下采样配置，具有一定的普适性。

Conclusion: 提供一个可通用的评估框架，用于在高频时间序列数据中平衡数据缩减与模型性能，尤其适用于近实时nEMG分析，也可推广至其他高频时序应用。

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [137] [GFM4GA: Graph Foundation Model for Group Anomaly Detection](https://arxiv.org/abs/2601.10193)
*Jiujiu Chen,Weijun Zeng,Shaofeng Hu,Sihong Xie,Hui Xiong*

Main category: cs.AI

TL;DR: Propose GFM4GA, a graph foundation model for group anomaly detection that uses dual-level contrastive learning (feature-based estimation and group extraction) and few-shot finetuning with group-anomaly-proportion weighting to detect group anomalies, outperforming baselines in AUROC and AUPRC.


<details>
  <summary>Details</summary>
Motivation: Group anomalies require joint detection of abnormal patterns; existing GFMs excel at individual anomalies but fail to capture group-level structures where individuals may appear normal. A model that leverages group context and few-shot labeling can improve detection and generalization.

Method: Pretrain with dual-level contrastive learning capturing group structure and feature inconsistencies (feature-based estimation and group extraction). Downstream finetuning under parameter constraints and weighting by the proportion of anomalies, with adaptability to unseen group anomalies via labeled anomaly-neighbor contexts.

Result: Empirically, GFM4GA outperforms both group anomaly detectors and GFMs designed for individual anomalies, achieving average improvements of 2.85 percentage points in AUROC and 2.55 percentage points in AUPRC.

Conclusion: GFM4GA demonstrates effective group-level anomaly detection, with better generalization to unseen group anomalies through context-aware adaptation and efficient few-shot learning.

Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.

</details>


### [138] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Topo-RAG提出一种面向拓扑感知的RAG框架。通过双架构处理文本与表格，表格使用Cell-Aware Late Interaction以保留空间关系，相比线性化方法，在SEC-25数据集上的混合查询nDCG@10提升18.4%。


<details>
  <summary>Details</summary>
Motivation: 现有RAG将表格等多模态数据线性化为文本，难以保持表格的几何/拓扑特征，导致对混合查询的检索效果受限。

Method: 提出双路由架构：叙事文本通过传统密集检索器处理；表格结构通过Cell-Aware Late Interaction进行处理以保留拓扑关系；在合成企业语料SEC-25上评估。

Result: 在混合查询上的nDCG@10提高18.4%，证实了拓扑感知在混合数据检索中的有效性。

Conclusion: 强调保留数据拓扑结构的重要性，Topo-RAG提供了比单纯线性化更符合真实信息形状的检索范式。

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [139] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM 通过在多步推理任务中进行步级路由，仅把关键步骤路由给更大模型，其余由小模型继续推理，从而显著提升成本效率；在MATH-500和AIME等数据集上实现了5x到6x的成本节省，且保留或接近强模型的性能。


<details>
  <summary>Details</summary>
Motivation: 多步推理易出现级联性错误，当前方法通常将整个查询分配给单一模型，忽视逐步中的难易差异，导致高昂的推理成本。需要在逐步层面进行有目标的路由，以在保持准确性的同时减少昂贵模型的调用。

Method: TRIM在逐步推理中引入步级过程奖励模型，基于步级不确定性与预算约束对每个步骤做路由决策。提出从简单阈值策略到能够权衡长期准确性–成本与步级正确性不确定性的更丰富策略集，涵盖阈值、长时程折衷与不确定性建模等不同复杂度。

Result: 在MATH-500上，最简单的阈值策略已超越先前路由方法，成本效率提升约5倍；更高级策略在使用80%更少的昂贵模型代币的同时实现强模型的性能。对于更具挑战性的AIME数据集，成本效率最高可达6倍。方法对多种数学推理任务具有良好泛化能力。

Conclusion: 步级难度是推理的基本特征，TRIM证明通过有目标的步级干预可显著提升推理的成本效率，并在不同数学推理任务上表现出良好的普适性与潜在扩展性。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [140] [NoReGeo: Non-Reasoning Geometry Benchmark](https://arxiv.org/abs/2601.10254)
*Irina Abdullaeva,Anton Vasiliuk,Elizaveta Goncharova,Temurbek Rahmatullaev,Zagorulko Ivan,Maxim Kurkin,Andrey Kuznetsov*

Main category: cs.AI

TL;DR: NoReGeo 提供一个评估LLMs原生几何理解的基准，通过2500道涉及25个类别的几何问题，要求仅靠本地几何知识即可解决，评估GPT-4等前沿模型在二元分类任务上的最大准确率为65%，并且微调并不能显著提升几何理解，表明需要从训练初期就采用专门策略。


<details>
  <summary>Details</summary>
Motivation: 当前基准多聚焦于基于代数推理的几何任务，无法检验模型是否能直接编码空间关系和几何性质。本工作旨在分离推理/代数计算与本征几何理解，评估模型的原始几何认知能力。

Method: 构建包含2500道题的NoReGeo基准，覆盖25个类别，每题在已知对象位置的前提下可通过原生几何理解解决，不依赖外部推理。对包括GPT-4在内的多种前沿模型进行评测。

Result: 在二元分类任务上的总体最大准确率约65%。消融实验表明，仅通过微调无法使系统获得几何理解，需要从训练之初就进行专门的几何理解训练。

Conclusion: 当前LLMs在原生几何理解方面存在显著差距，为未来研究提供基础，目标是发展具有真正几何认知的模型。

Abstract: We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.

</details>


### [141] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: 提出 EAPO，利用证据增强的策略优化解决长上下文中奖励稀疏的问题；通过证据相关奖励和自适应共进化实现对证据质量的密集监督；在八个基准上显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 在长上下文的强化学习中，奖励信号稀疏且难以惩罚无证据的“运气猜测”，导致证据检索过程缺乏监督；证据提取质量成为推理能力的关键瓶颈；需要密集而有效的训练信号来提升证据获取和推理质量。

Method: 提出 Evidence-Augmented Reasoning 范式；通过 Tree-Structured Evidence Sampling 验证证据提取是长上下文推理的决定性瓶颈；设计 Group-Relative Evidence Reward 的奖励模型，提供密集的过程监督以提升证据质量；引入 Adaptive Reward-Policy Co-Evolution 机制，通过 outcome-consistent rollouts 迭代优化奖励模型，确保监督的对齐与鲁棒性。

Result: 在八个基准上，EAPO 相比最新的SOTA基线显著提升长上下文推理表现。

Conclusion: 通过证据质量驱动的密集监督和自适应共进化，EAPO 能更有效地引导证据检索与推理过程，显著提升长上下文强化学习的性能与鲁棒性。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [142] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: 提出 C-GRASP，通过八步推理和 Delta Z-score 调整，结合 RSA 感知 guardrails 的 RAG 系统，减少对 HRV 解释中的生理性幻觉，提升情绪分类的可解释性与一致性。


<details>
  <summary>Details</summary>
Motivation: 解决将大型语言模型应用于 HRV 解释时的生理性幻觉问题，如 RSA 污染、短数据导致的非线性指标不稳定，以及忽视个体基线而仅依赖群体规范。

Method: 设计八步可追溯的推理流程、Z-score 优先级层次（个体化 Delta Z-score 为核心锚点）、RSA 感知 guardrails 防止频域指标污染，并与高规模推理模型结合实现 RAG 增强。

Result: 在 DREAMER 数据集的 414 试验中，C-GRASP 配合 MedGemma3-thinking 等模型实现四分类情绪识别准确率 37.3%、CRC 69.6%；消融分析显示 Delta Z-score 模块是关键锚点，能有效抑制人口偏差。

Conclusion: 将情感计算从黑箱分类转向透明、证据驱动的临床决策支持，促进生物医学工程中更安全的人工智能融合。

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [143] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: LatentRefusal introduces a latent-signal refusal mechanism for text-to-SQL, using a Tri-Residual Gated Encoder to predict answerability from LLM hidden activations, achieving high F1 and low overhead as an attachable safety layer.


<details>
  <summary>Details</summary>
Motivation: Unanswerable/underspecified user queries in LLM-based text-to-SQL can produce incorrect or unsafe executable programs. Existing refusal methods are brittle or inefficient. A lightweight, model-internal signal for answerability could provide safer, more reliable refusals.

Method: Propose LatentRefusal which leverages intermediate hidden activations of a large language model to predict query answerability. Introduces a Tri-Residual Gated Encoder to suppress schema noise and amplify cues of mismatch between question and schema. Provides a latency-efficient, attachable safety layer with ~2 ms probe overhead.

Result: Empirical evaluation across four benchmarks shows LatentRefusal improves average F1 to 88.5% on both backbone models. Ablation studies and interpretability analyses support effectiveness and provide insight into the probing mechanism.

Conclusion: LatentRefusal offers an efficient, attachable safety layer for text-to-SQL systems by leveraging latent signals to gate answers, mitigating unanswerable/undesirable queries and reducing risk of unsafe outputs.

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [144] [Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering](https://arxiv.org/abs/2601.10402)
*Xinyu Zhu,Yuzhu Cai,Zexi Liu,Bingyang Zheng,Cheng Wang,Rui Ye,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Yuzhi Zhang,Linfeng Zhang,Weinan E,Di Jin,Siheng Chen*

Main category: cs.AI

TL;DR: A novel autonomous agent ML-Master 2.0 with Hierarchical Cognitive Caching (HCC) enables ultra-long-horizon ML engineering by preserving and distilling knowledge across time, achieving a 56.44% medal rate on MLE-Bench under 24-hour budgets.


<details>
  <summary>Details</summary>
Motivation: Ultra-long-horizon autonomy is a key bottleneck for AI in scientific discovery; LLMs struggle with sustained coherence and sparse feedback in long experimental cycles.

Method: Introduce Hierarchical Cognitive Caching (HCC), a multi-tier cognitive architecture that structurally differentiates experience over time, dynamically distills transient execution traces into stable knowledge, and decouples immediate execution from long-term strategy to overcome static context-window limits.

Result: On OpenAI's MLE-Bench with 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%.

Conclusion: Ultra-long-horizon autonomy offers a scalable blueprint for autonomous exploration in complex scientific domains beyond current human precedents.

Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.

</details>


### [145] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval 将问答生成评估从黑盒整体判定转为两阶段的错误诊断和有据评分，提升与人工评估的一致性并降低对低质量问题的高估。


<details>
  <summary>Details</summary>
Motivation: 当前自动问句生成易产生事实性幻觉和答题不匹配；现有评估方法（包括LLM）缺乏显式错误建模，导致对质量的过度乐观。

Method: 引入轻量级的错误识别器，分辨结构、语言和内容层面的常见错误；将诊断信号作为证据，指导LLM评估者进行更细粒度、 grounding 的判断；分两阶段：错误诊断，再据此打分。

Result: 在三个基准数据集上的广泛实验表明，加入显式诊断后评估与人工判断的一致性提高；并且有效降低对低质量问题的高估。

Conclusion: 显式诊断提高问答生成评估的可靠性，ErrEval 为QG评估提供可扩展、可诊断的框架。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [146] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: 提出 LADFA 框架，用 LLM + RAG + 定制知识库从隐私政策中提取个人数据流，构建数据流图并分析，以支持大规模自动分析；在汽车行业十份政策上进行案例验证，且框架可定制扩展。


<details>
  <summary>Details</summary>
Motivation: 隐私政策内容冗长、语言复杂，跨行业实践不一致，阻碍理解与对个人数据流的快速分析；需要端到端、可扩展的自动化方法。

Method: 发展端到端框架 LADFA：包含预处理、基于 LLM 的信息抽取与数据流推断、数据流后处理；结合 RAG 和定制知识库（来源于既有研究）提升推理质量；输出个人数据流图并进行分析。

Result: 通过对来自汽车行业的十份隐私政策的案例研究，验证了方法的有效性与准确性。

Conclusion: LADFA 设计灵活、可定制，适用于多种文本分析任务，具备从非结构化文本中抽取数据流并建立分析图谱的潜力。

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [147] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: 提出 LLMdoctor，一种在测试时对大模型进行高效对齐的框架，通过患者-医生范式，结合 token 级奖励获取和 token-level flow-guided preference optimization (TFPO) 实现对冻结患者模型的高效对齐，强调逐 token 的对齐和保持生成多样性，优于现有方法甚至全量微调。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法通常成本高、灵活性不足；轨迹级信号或采样效率低，难以在保持基线模型生成多样性的同时实现高性能对齐。需要一种以 token 级信号驱动、并能在测试时高效对齐的方案。

Method: 引入患者-医生框架：冻结大模型作为患者，训练一个小型医生模型；从患者行为的细粒度变化中提取 token 级偏好信号；通过 token-level flow-guided preference optimization (TFPO) 将这些信号用于教师-学生式训练，使医生在保持对齐的一致性的同时实现逐 token 对齐并保持生成多样性。

Result: 多项实验显示，LLMdoctor 在测试时对齐方面显著优于现有方法，甚至超过如 DPO 等全量微调方法的性能。

Conclusion: LLMdoctor 提供一种高效、可扩展且能保持生成多样性的对齐方案，可作为冻结大模型的轻量级对齐替代，提升实际应用中的对齐效果与灵活性。

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [148] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost: a non-intrusive neuro-symbolic residual boosting framework that repairs legacy industrial models by targeting hard regions with LLM-generated symbolic experts and Bayesian-tuned parameters, enabling safe, low-cost evolution and outperforming baselines on public/private data.


<details>
  <summary>Details</summary>
Motivation: Industrial tabular models face prohibitive retraining costs and systemic risks in high-concurrency production. There is a need for a safe, low-cost method to improve performance—especially coverage of long-tail risks—without modifying legacy models.

Method: Three-stage approach: (1) identify hard regions via residual analysis; (2) generate interpretable experts as symbolic code using large language models and tune parameters with Bayesian optimization; (3) dynamically integrate experts with the legacy model output through a lightweight aggregator.

Result: Reported deployment in Qfin Holdings' core financial risk control system; outperforms state-of-the-art baselines across six public datasets and one private dataset; shows strong gains on real online data.

Conclusion: NSR-Boost provides a safe, low-cost evolutionary pathway for industry by non-intrusively repairing legacy models and capturing long-tail risks overlooked by traditional methods.

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [149] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: 提出 ChartComplete 数据集，覆盖 30 种图表类型的图像分类集合，弥补现有基准在图表类型上的狭窄；不含学习信号，供社群进一步标注与扩展。


<details>
  <summary>Details</summary>
Motivation: 现有公开基准多局限于少数图表类型，难以全面评估多模态大语言模型在图表理解上的泛化与鲁棒性。

Method: 基于可视化领域的图表分类体系，将分类标签应用于大量图表图像，形成一个现成、标签化的图像集合（ChartComplete），不附带学习信号。

Result: 提供涵盖 30 种图表类型的图像分类集合，为后续研究提供更广泛的覆盖面和可重复性基线。

Conclusion: ChartComplete 为多模态图表理解评估提供更全面的覆盖，但需结合具体任务设计与学习信号以推动模型的训练与迁移性评估。

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [150] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: 提出了域特定知识图融合（DKGF）任务及其简洁有效的 ExeFuse 框架。将通用知识图（GKG）事实视为潜在语义程序，通过粒度感知的操作符映射来实现领域相关性与粒度对齐，并在目标DKG上验证程序可执行性以判断相关性，给出两组基准数据集 DKGF(W-I) 与 DKGF(Y-I)（共21种评估配置），并在大量实验中证明方法有效，首次提供DKGF的标准化测试床。


<details>
  <summary>Details</summary>
Motivation: DKGs 往往覆盖不足，难以与 GKGs 的广泛信息保持一致，导致实际应用受限。面临的核心挑战是领域相关性存在高度歧义以及不同知识图之间的粒度不对齐。缺乏统一的框架和标准化评估。

Method: ExeFuse 将每条 GKG 事实视为潜在语义程序（latent semantic program），将抽象关系映射为粒度感知的操作符，并通过在目标 DKG 上执行该程序来验证领域相关性。形成一个统一的概率框架，能同时处理相关性与粒度问题。

Result: 构建了 DKGF(W-I) 与 DKGF(Y-I) 两组基准，共21种评估配置。大量实验验证任务重要性及模型有效性，首次建立了用于 DKGF 的标准化测试床。

Conclusion: 该方法提供一个简单而有效的统一解决方案，解决领域相关性与粒度对齐的关键难题，有望推动 DKGF 的标准化评估与跨领域应用。

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [151] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: 跨架构微调研究揭示：数据多样性、架构特性与训练策略共同决定泛化，Llama 3.1 在多样数据上受限，Gemma2 与 Mistral 表现不同，提供诊断框架以解释泛化失败原因。


<details>
  <summary>Details</summary>
Motivation: 理解微调后模型在不同数据分布和任务上的泛化差异，建立多层诊断框架以揭示架构-数据-训练策略的耦合及其对鲁棒性的影响。

Method: 对 Llama 3.1 8B、Gemma 2 9B、Mistral 三种模型在高风险钓鱼检测任务上进行微调，结合 SHAP 分析与机械性可解释性（mechanistic interpretability）来定位泛化失败的根源；比较同一任务在不同数据集（包括“一般化”数据集与风格多样化数据集）上的表现，并识别各自的失败模式。

Result: Gemma 2 9B 在风格多样化的通用数据集上实现 >91% F1，但仅在这类数据上泛化良好；Llama 3.1 8B 在窄领域表现良好但难以整合多样数据，导致显著性能下降；Mistral 在多种训练范式下表现稳定，具有更强泛化能力。通过定位有缺陷的启发式规则，揭示了架构-数据耦合对泛化的影响；提出一种用于诊断和理解泛化失败的可操作方法。

Conclusion: 强调在设计与评估时必须深度验证架构、数据与训练策略之间的相互作用，建立可重复的诊断流程以提升对泛化失败的识别和改进，推动更鲁棒的 AI 系统。

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [152] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 对7个前沿模型在语言、视觉-语言和图像生成三个模态上的安全性进行综合评估，使用统一协议（基准、对抗、多语言、合规评估）并构建安全排行榜与模型安全画像。结果显示安全性高度多维且模态相关，GPT-5.2表现最佳，其他模型存在基准安全、对抗鲁棒性、多语言泛化与合规性之间的权衡；对抗性攻击显著削弱多模态安全性，文本到图像生成在受控视觉风险上相对较好但对对抗性和语义歧义敏感。结论强调需要标准化的多模态安全评估以真实反映风险并引导安全部署。


<details>
  <summary>Details</summary>
Motivation: 推动安全评估超越单一模态和单一威胁模型，建立一个跨语言、跨模态且包含基准、对抗、合规等要素的综合安全评估框架，以全面、可比地评估前沿模型的安全性。

Method: 使用统一的评估协议，对语言、视觉-语言与图像生成三大模态分别进行基准评估、对抗性评估、多语言评估与合规性评估；将结果汇总成安全排行榜和模型安全画像，分析模态与评估方式对安全表现的影响。

Result: 总体呈现出高度多维的安全格局。GPT-5.2在各评估维度上表现稳健且均衡；其他模型在基准安全、对抗鲁棒性、多语言泛化与合规性之间存在显著权衡。所有模型在对抗评估下对语言与视觉-语言模态都显著下降，标准基准难以预测真实风险。文本到图像模型在受控视觉风险类别上对齐较好，但对对抗性或语义模糊提示仍较脆弱。

Conclusion: 安全性是模态与语言环境下的多维现象，需建立标准化的跨模态安全评估框架，以更准确地评估现实世界风险并引导前沿模型的安全开发与部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [153] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: 在解码阶段暴露潜在的安全信号并用于早期检测未安全内容；提出 SafeProbing，通过显式呈现这些信号来提升安全性，同时尽量保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 尽管存在安全对齐，但对抗 jailbreaking 的能力仍然薄弱，现有解码约束与事后检测难以对抗更复杂的 jailbreak。需在解码阶段利用模型内部的安全信号来及早干预。

Method: 分析LLMs在解码过程中的潜在安全信号，设计解码时的探测/显式呈现机制，使模型在生成早期就能检测并拒绝或纠正未安全内容。

Result: 在多种 jailbreak 攻击上实现显著的安全性提升，且对良性输入的过度拒绝率低、输出质量保持良好。

Conclusion: 在解码阶段激活内在安全意识是一条有前景且互补的防御方向，有助于提升对 jailbreaker 的鲁棒性。

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [154] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: 提出一个面向多属性LTLf综合的固定点符号算法，通过引入布尔目标变量，在单次固定点计算中建立产品博弈状态与可实现目标集合之间的关系，从而合成实现最大可实现集合的策略，显著优于枚举基线（速度提升可达100倍级别）。


<details>
  <summary>Details</summary>
Motivation: 在多属性LTLf综合场景中，满足所有属性往往不可实现，迫切需要高效的多目标综合方法，避免对属性子集进行穷举。

Method: 通过一个单一的固定点计算，建立产品博弈状态与可实现目标集合之间的关系；提出全符号算法，引入布尔目标变量，并利用单调性以紧凑表示大量目标组合，从而实现对最大可实现集合的策略合成。

Result: 实验表明该方法在与基线的对比中具有显著性能优势，枚举基线的速度提升高达两个数量级。

Conclusion: 提出的符号化、固定点方法能够有效生成针对多属性LTLf的最大可实现集的策略，具有良好放缩性和实用性。

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [155] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: 对 HRM 的机制性研究发现其推理表现背后是“猜测”而非真正推理，揭示三大现象并提出三种放大猜测的策略，结合而成的 Augmented HRM 在 Sudoku-Extreme 上从 54.5% 提升到 96.9%，并给出关于推理模型工作机制的新见解。


<details>
  <summary>Details</summary>
Motivation: 理解 HRM 在多任务上的强大表现背后的强力机制与潜在失败模式，明确固定点、Grokking 动态以及多固定点对推理的影响，探讨如何扩大可用猜测以提升性能。

Method: 进行对 HRM 的机制性分析，发现三大现象；提出并实现三种放大猜测的策略：数据增强、输入扰动、模型自举；将三策略结合形成 Augmented HRM；在 Sudoku-Extreme 数据集上进行评估。

Result: 揭示 (a) 极简谜题也会失败，因固定点属性被破坏；(b) 推理步骤中的突然跃迁（Grokking）；(c) 存在多个固定点，模型会先猜测并停滞在首个固定点。综合策略显著提升性能： Sudoku-Extreme 由 54.5% 提升到 96.9%。

Conclusion: HRM 更像在“猜测”而非系统性推理；通过放大猜测的三种策略可提升实用性，研究也提供了关于推理模型如何“推理”的新视角。

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [156] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 提出结构感知、多粒度片段的语境气泡（context bubble）构建框架，在严格预算下实现连贯、可引用的上下文包，解决RAG中的信息碎片化、过检索和冗余问题。通过以锚点片段为起点、使用结构先验和多样性约束逐步构建上下文，同时输出可追溯的完整检索过程。实验表明在企业文档中能降低冗余、覆盖次要 facet、提高答案质量与引用可信度，且消融研究证实结构先验与多样性约束均为必要。


<details>
  <summary>Details</summary>
Motivation: 现有RAG通常以top-k检索，导致信息图碎片化、冗余和对查询上下文（包括二、三阶 facet）的不足。需要在预算约束下构建连贯、可引用、可审计的上下文集合，同时保持对文档结构的利用。

Method: 在严格的 token 预算内，以结构感知和多粒度片段为基础，构建连贯的上下文气泡。通过从高相关锚点片段出发，采用受任务约束的结构先验引导检索；在受限条件下进行带有相关性、边际覆盖和冗余惩罚的约束选择，显式约束多样性与预算，产出紧凑且信息丰富的上下文集合；同时输出完整的检索过程以实现可追溯性与确定性调控。

Result: 在企业文档上的实验表明，上下文气泡显著减少冗余上下文，更好覆盖次要 facets，并在有限上下文窗口内提高答案质量与引用可信度。消融研究显示结构先验与多样性约束都不可或缺，移除任一项均导致覆盖下降、冗余或上下文不完整的增加。

Conclusion: 该框架通过结构先验与多样性约束实现高效、可审计且紧凑的上下文包，相较于简单的 top-k 检索在连贯性、覆盖与引用可信度方面具有优势；结构信息的显式利用和对多样性的控制是关键。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [157] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: GenAI does not improve overall performance, but helps novices; creative self-efficacy declines with GenAI use; cognitive load unchanged overall, though prompting strategies can reduce it.


<details>
  <summary>Details</summary>
Motivation: To evaluate how generative AI influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks and identify moderating factors.

Method: Two-phase design task (independent then with external tools) performed by 36 students from Architectural Engineering and other disciplines. Conditions: GenAI-assisted vs a control using an online repository. Expert raters evaluated design outcomes; self-reports collected after each phase on creative self-efficacy and cognitive load. Analyzed with difference-in-differences and subgroup analyses based on expertise.

Result: GenAI yielded no overall performance advantage. Subgroup analyses show performance gains for novice designers. Creative self-efficacy declined for GenAI users. Cognitive load did not differ significantly between conditions; however, iterative idea generation and visual feedback prompts correlated with larger reductions in cognitive load.

Conclusion: GenAI effectiveness is contingent on user expertise and interaction strategies via prompting. Training and prompt design may be necessary to maximize benefits while mitigating declines in creative self-efficacy.

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>
