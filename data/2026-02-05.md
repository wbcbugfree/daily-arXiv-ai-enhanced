<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 65]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 92]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Linguistic Blind Spots in Clinical Decision Extraction](https://arxiv.org/abs/2602.03942)
*Mohamed Elgaar,Hadi Amiri*

Main category: cs.CL

TL;DR: 提取临床笔记中的医疗决策存在类别相关的语言特征差异，导致精确匹配召回率低；放宽边界后召回显著提升，表明边界判定与边界容忍策略关键。


<details>
  <summary>Details</summary>
Motivation: 理解语言特征如何影响临床决策提取的成功与失败，从而改进决策提取的鲁棒性，提升对决策支持与患者摘要的实用性。

Method: 以 MedDec 出院摘要为样本，按 DICTUM 分类对决策短语进行标注；计算每个决策短语的七个语言指标；在标准 Transformer 模型上进行短语级提取，评估精确匹配召回率及放宽重叠匹配的召回，并按语言属性（如停用词比例、hedging、否定等）分析召回差异。

Result: 存在类别特征签名：药物相关与问题定义的决策实体密集且偏简短；而建议与预防类决策更具叙事性，停用词与代词比例高，hedging与否定线索更频繁。验证集的 exact-match 召回率为 48%，且随停用词比例升高召回显著下降（从 58% 低组到 24% 高组）；包含 hedging/否定线索的短语更难被恢复。放宽的重叠匹配召回率升至 71%，表明多数错误来自边界判定而非完全漏检。总体上，叙事风格的短语是 exact matching 下的普遍盲点。

Conclusion: 下游系统应采用边界容忍的评估与边界感知的提取策略，强化对叙事型决策的边界预测与跨边界提取能力，以提升实际应用的鲁棒性与覆盖面。

Abstract: Extracting medical decisions from clinical notes is a key step for clinical decision support and patient-facing care summaries. We study how the linguistic characteristics of clinical decisions vary across decision categories and whether these differences explain extraction failures. Using MedDec discharge summaries annotated with decision categories from the Decision Identification and Classification Taxonomy for Use in Medicine (DICTUM), we compute seven linguistic indices for each decision span and analyze span-level extraction recall of a standard transformer model. We find clear category-specific signatures: drug-related and problem-defining decisions are entity-dense and telegraphic, whereas advice and precaution decisions contain more narrative, with higher stopword and pronoun proportions and more frequent hedging and negation cues. On the validation split, exact-match recall is 48%, with large gaps across linguistic strata: recall drops from 58% to 24% from the lowest to highest stopword-proportion bins, and spans containing hedging or negation cues are less likely to be recovered. Under a relaxed overlap-based match criterion, recall increases to 71%, indicating that many errors are span boundary disagreements rather than complete misses. Overall, narrative-style spans--common in advice and precaution decisions--are a consistent blind spot under exact matching, suggesting that downstream systems should incorporate boundary-tolerant evaluation and extraction strategies for clinical decisions.

</details>


### [2] [Automatic Classification of Pedagogical Materials against CS Curriculum Guidelines](https://arxiv.org/abs/2602.03962)
*Erik Saule,Kalpathi Subramanian,Razvan Bunescu*

Main category: cs.CL

TL;DR: 使用自然语言处理（NLP）加速对 ACM/IEEE CS 指南覆盖程度的评估；比较传统 NLP 流程与大语言模型，对教学材料语料进行自动分类，验证可行性。


<details>
  <summary>Details</summary>
Motivation: CS 课程需对齐国际标准，但指南条目繁多，逐条审计耗时费力，约每门课程一天。需要自动化工具提高审计效率。

Method: 两类方法：1) 基于传统 NLP 的解析、标注和嵌入等工具；2) 使用大语言模型的能力。对一组教学材料语料进行分类任务，评估分类效果。

Result: 在对教学材料语料的分类任务中，所提出的方法可实现显著的自动化分类，能“有意义地”对文档进行分类，提升工作效率。

Conclusion: NLP 技术，尤其是结合传统方法与大语言模型的方法，能有效加速对教育指南覆盖情况的评估，为程序对齐国际标准提供可行的自动化方案。

Abstract: Professional societies often publish curriculum guidelines to help programs align their content to international standards. In Computer Science, the primary standard is published by ACM and IEEE and provide detailed guidelines for what should be and could be included in a Computer Science program.
  While very helpful, it remains difficult for program administrators to assess how much of the guidelines is being covered by a CS program. This is in particular due to the extensiveness of the guidelines, containing thousands of individual items. As such, it is time consuming and cognitively demanding to audit every course to confidently mark everything that is actually being covered. Our preliminary work indicated that it takes about a day of work per course.
  In this work, we propose using Natural Language Processing techniques to accelerate the process. We explore two kinds of techniques, the first relying on traditional tools for parsing, tagging, and embeddings, while the second leverages the power of Large Language Models. We evaluate the application of these techniques to classify a corpus of pedagogical materials and show that we can meaningfully classify documents automatically.

</details>


### [3] [Likelihood-Based Reward Designs for General LLM Reasoning](https://arxiv.org/abs/2602.03979)
*Ariel Kwiatkowski,Natasha Butt,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: 在链路推理的强化学习微调中，基于对参考答案的对数概率的奖励在可验证与不可验证场景均优于基线奖励与其他概率基方法，且与预训练目标一致；二元奖励和概率式方法在不可验证场景表现不佳。


<details>
  <summary>Details</summary>
Motivation: 解决 RLHF 在推理基准上的奖励设计负担和二元奖励的稀疏性问题，探究基于概率/对数概率的奖励是否具备规模化、跨场景的鲁棒性。

Method: 系统比较若干基于似然性的奖励变体（如参考答案的对数概率、对任何提示续写的概率等）与标准基线，在标准数学推理基准和长形答案（无外部 verifier）的设置下评估性能。

Result: 以参考答案的对数概率作为链路推理（CoT）学习的奖励，在所有设置中表现最佳；该奖励与预训练时的下一个 token 似然损失一致。在可验证场景中，该奖励达到或超过二元奖励的成功率，并显著改善困惑度；在不可验证场景下，性能与 SFT 相当；基于概率的 VeriFree 等方法在不可验证场景易陷入概率趋零的问题而表现不佳。

Conclusion: 对数概率奖励是 CoT 微调的可行手段，能够在短可验证与长不可验证答案之间架起桥梁，优于二元奖励及部分概率基方法。

Abstract: Fine-tuning large language models (LLMs) on reasoning benchmarks via reinforcement learning requires a specific reward function, often binary, for each benchmark. This comes with two potential limitations: the need to design the reward, and the potentially sparse nature of binary rewards. Here, we systematically investigate rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data), which have the advantage of not relying on specific verifiers and being available at scale. Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER). We systematically compare variants of likelihood-based rewards with standard baselines, testing performance both on standard mathematical reasoning benchmarks, and on long-form answers where no external verifier is available. We find that using the log-probability of the reference answer as the reward for chain-of-thought (CoT) learning is the only option that performs well in all setups. This reward is also consistent with the next-token log-likelihood loss used during pretraining. In verifiable settings, log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards, and yield much better perplexity. In non-verifiable settings, they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer. Overall, this establishes log-probability rewards as a viable method for CoT fine-tuning, bridging the short, verifiable and long, non-verifiable answer settings.

</details>


### [4] [Transformers perform adaptive partial pooling](https://arxiv.org/abs/2602.03980)
*Vsevolod Kapatsinski*

Main category: cs.CL

TL;DR: GPT-2的下一词预测对当前上下文之外的观测依赖程度随训练阶段逐渐减弱，呈现“自适应性部分聚合”。这类聚合受上下文频率、上下文类型数量及变异性影响，与层级回归中的自适应聚合机制类似。


<details>
  <summary>Details</summary>
Motivation: 揭示大规模语言模型在跨上下文推理中的信息聚合规律，并将其与层级回归的自适应聚合进行对比，以评估其合理性和经验性。

Method: 对GPT‑2的下一词预测进行跨训练阶段的行为分析，量化模型在多大程度上利用当前上下文之外的观测，以及这种聚合如何随上下文频率、类型数和变异性的改变而改变，并将结果与层级回归的自适应聚合进行对照。

Result: 在训练阶段的不同阶段，外部上下文观测对当前上下文预测的影响逐渐降低；聚合程度随上下文频率、上下文类型数量和变异性呈现与层级回归相似的模式。

Conclusion: Transformer的学习过程表现出可观的自适应聚合行为，与理性与经验性两方面的语言理解和泛化理论相吻合。

Abstract: Because language is creative, any reasonable language model must generalize, deciding what to say in novel contexts by using information from similar contexts. But what about contexts that are not novel but merely infrequent? In hierarchical regression, the model's predictions for behavior in a context are affected by observations from other similar contexts to the extent that 1) the current context is infrequent and 2) different contexts behave similarly. This is called adaptive partial pooling of evidence. This paper shows that next-word predictions of a transformer (GPT2) are increasingly unaffected by observations from outside the current context across epochs of training (the amount of pooling reduces with training), and that the extent of pooling is affected by context frequency, context number (type frequency) and context variability in a similar way to hierarchical regression. These characteristics of learning in transformers are argued to be realistic on both rational and empirical grounds.

</details>


### [5] [On the Credibility of Evaluating LLMs using Survey Questions](https://arxiv.org/abs/2602.04033)
*Jindřich Libovický*

Main category: cs.CL

TL;DR: Proposes a critical reevaluation of evaluating LLM value orientation via social surveys, highlighting methodological sensitivities and introducing a new metric.


<details>
  <summary>Details</summary>
Motivation: Address biases in how LLMs are compared to human value distributions; warn against assuming independence of survey items and high average agreement implies structural alignment.

Method: Empirical study using World Value Survey data across languages/countries; compare prompting (direct vs chain-of-thought) and decoding (greedy vs sampling); introduce self-correlation distance to assess inter-question relationships in responses.

Result: Shows that prompting and decoding choices significantly impact similarity measures; high average agreement can mask misalignment in response structure; mean-squared distance and KL divergence are weakly correlated due to independence assumptions; self-correlation distance reveals structural misalignment; recommends CoT prompting, sampling with many samples, multi-metric analysis including self-correlation distance.

Conclusion: Robust evaluation of LLM value orientation should use diverse prompts/decoding, many samples, and multiple metrics (including self-correlation distance) to capture both agreement and structural alignment.

Abstract: Recent studies evaluate the value orientation of large language models (LLMs) using adapted social surveys, typically by prompting models with survey questions and comparing their responses to average human responses. This paper identifies limitations in this methodology that, depending on the exact setup, can lead to both underestimating and overestimating the similarity of value orientation. Using the World Value Survey in three languages across five countries, we demonstrate that prompting methods (direct vs. chain-of-thought) and decoding strategies (greedy vs. sampling) significantly affect results. To assess the interaction between answers, we introduce a novel metric, self-correlation distance. This metric measures whether LLMs maintain consistent relationships between answers across different questions, as humans do. This indicates that even a high average agreement with human data, when considering LLM responses independently, does not guarantee structural alignment in responses. Additionally, we reveal a weak correlation between two common evaluation metrics, mean-squared distance and KL divergence, which assume that survey answers are independent of each other. For future research, we recommend CoT prompting, sampling-based decoding with dozens of samples, and robust analysis using multiple metrics, including self-correlation distance.

</details>


### [6] [Abstraction Induces the Brain Alignment of Language and Speech Models](https://arxiv.org/abs/2602.04081)
*Emily Cheng,Aditya R. Vaidya,Richard Antonello*

Main category: cs.CL

TL;DR: 中间层表征的语义抽象及其高维特征解释了大语言模型、语音模型与大脑之间的关系；输出层不如中间层有效，因为高层的语义丰富性在脑信号预测中起主导作用。


<details>
  <summary>Details</summary>
Motivation: 探究为何中间层而非输出层在跨脑预测任务中最有效，以及哪些表征属性促成模型-脑的跨域共性。通过“内在维度”量化特征复杂度来揭示语义抽象与脑预测力之间的关系。

Method: 在大语言模型和语音模型中对各层表征进行分析，计算每层的内在维度；将这些表征用于预测fMRI与ECoG信号，比较不同层的预测力；通过改变模型的预训练与微调设置，观察内在维度与语义内容的变化以及对脑信号预测的影响；评估语义丰富度、内在维度与脑预测性的关系。

Result: 中间层的内在维度达到峰值，与对脑信号的解释力呈正相关；这一关系在模型预训练阶段即已显现，进一步的微调以提升脑预测力会同时提升内在维度和语义内容；模型-脑之间的相似性源于对输入意义的丰富抽象，而非单纯的下一词预测性能。

Conclusion: 语义丰富性与脑预测性相互映射，中间层通过高维度的意义抽象实现模型-脑对齐。语言建模等复杂任务可能是促成这种表征的关键，但并非唯一途径；这提示未来可探索其他任务是否也能产生类似的意义抽象与脑预测能力。

Abstract: Research has repeatedly demonstrated that intermediate hidden states extracted from large language models and speech audio models predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most effective for this unique and highly general transfer task? We give evidence that the correspondence between speech and language models and the brain derives from shared meaning abstraction and not their next-word prediction properties. In particular, models construct higher-order linguistic features in their middle layers, cued by a peak in the layerwise intrinsic dimension, a measure of feature complexity. We show that a layer's intrinsic dimension strongly predicts how well it explains fMRI and ECoG signals; that the relation between intrinsic dimension and brain predictivity arises over model pre-training; and finetuning models to better predict the brain causally increases both representations' intrinsic dimension and their semantic content. Results suggest that semantic richness, high intrinsic dimension, and brain predictivity mirror each other, and that the key driver of model-brain similarity is rich meaning abstraction of the inputs, where language modeling is a task sufficiently complex (but perhaps not the only) to require it.

</details>


### [7] [Expert Selections In MoE Models Reveal (Almost) As Much As Text](https://arxiv.org/abs/2602.04105)
*Amir Nuriyev,Gabriel Kulp*

Main category: cs.CL

TL;DR: MoE语言模型中，基于专家路由的选择信息可进行高精度文本恢复，3层MLP提升到63.1% top-1，Transformer解码器在32-token序列上达到91.2% top-1（94.8% top-10），提示路由信息属于敏感信息，噪声可以降低但不能完全消除泄露。


<details>
  <summary>Details</summary>
Motivation: 探究MoE路由信息泄露的规模和可利用性，将专家选择视为潜在泄漏源，与嵌入反演等研究关联，提出实际场景下的安全性与防护讨论。

Method: 从逻辑回归到3层MLP再到Transformer序列解码器的攻击路径，对OpenWebText数据集在训练100M token后进行离线推断，评估top-1/top-k重建准确率；并探讨噪声对重建的影响以及分布式推理/旁路信道等场景。

Result: 实验显示：3层MLP达到63.1% top-1，Transformer解码器在32-token序列上达到91.2% top-1、94.8% top-10；建立了MoE路由与嵌入反演之间联系；噪声能降低泄露但无法完全消除；结果强调将专家选择视为与文本同等敏感。

Conclusion: 在MoE部署中需将专家选择的隐私风险纳入评估，采取防护措施（如加入噪声、隐藏路由信息、加强访问控制等）以降低信息泄露风险。

Abstract: We present a text-reconstruction attack on mixture-of-experts (MoE) language models that recovers tokens from expert selections alone. In MoE models, each token is routed to a subset of expert subnetworks; we show these routing decisions leak substantially more information than previously understood. Prior work using logistic regression achieves limited reconstruction; we show that a 3-layer MLP improves this to 63.1% top-1 accuracy, and that a transformer-based sequence decoder recovers 91.2% of tokens top-1 (94.8% top-10) on 32-token sequences from OpenWebText after training on 100M tokens. These results connect MoE routing to the broader literature on embedding inversion. We outline practical leakage scenarios (e.g., distributed inference and side channels) and show that adding noise reduces but does not eliminate reconstruction. Our findings suggest that expert selections in MoE deployments should be treated as sensitive as the underlying text.

</details>


### [8] [DELTA: Deliberative Multi-Agent Reasoning with Reinforcement Learning for Multimodal Psychological Counseling](https://arxiv.org/abs/2602.04112)
*Jiangnan Yang,Junjie Chen,Fei Wang,Yiqi Nie,Yuxin Liu,Zhangling Duan,Jie Chen*

Main category: cs.CL

TL;DR: DELTA introduces a deliberative multimodal counseling AI that explicit grounds evidence from verbal, visual, and vocal cues, abstracts mental state, and generates responses, with reinforcement learning guided by an Emotion Attunement Score to improve empathic interaction.


<details>
  <summary>Details</summary>
Motivation: Current language-model-based counseling systems are largely text-only and rely on implicit mental-state inference; there is a need for explicit multimodal reasoning to better infer clients' mental states and respond with empathy.

Method: DELTA uses a structured, multi-agent reasoning pipeline that separates evidence grounding, mental state abstraction, and response generation over multimodal signals, and employs reinforcement learning guided by a distribution-level Emotion Attunement Score.

Result: On a multimodal counseling benchmark, DELTA improves counseling quality and emotion attunement across models; ablation and qualitative analyses indicate explicit multimodal reasoning and structured mental-state representations complement each other for empathic human-AI interaction.

Conclusion: Explicit multimodal reasoning and structured mental-state representations can enhance empathic human-AI counseling, suggesting DELTA as a viable framework for multimodal deliberative AI in counseling.

Abstract: Psychological counseling is a fundamentally multimodal cognitive process in which clinicians integrate verbal content with visual and vocal cues to infer clients' mental states and respond empathically. However, most existing language-model-based counseling systems operate on text alone and rely on implicit mental state inference. We introduce DELTA, a deliberative multi-agent framework that models counseling as a structured reasoning process over multimodal signals, separating evidence grounding, mental state abstraction, and response generation. DELTA further incorporates reinforcement learning guided by a distribution-level Emotion Attunement Score to encourage emotionally attuned responses. Experiments on a multimodal counseling benchmark show that DELTA improves both counseling quality and emotion attunement across models. Ablation and qualitative analyses suggest that explicit multimodal reasoning and structured mental state representations play complementary roles in supporting empathic human-AI interaction.

</details>


### [9] [From Lemmas to Dependencies: What Signals Drive Light Verbs Classification?](https://arxiv.org/abs/2602.04127)
*Sercan Karakaş,Yusuf Şimşek*

Main category: cs.CL

TL;DR: 本研究系统评估土耳其语轻动词结构（LVC）分类信号，比较不同输入信号源对判定的影响，并发现形态语法信号不足以独立鲁棒地区分LVC，词汇身份有帮助但高度依赖归一化策略；而“lemma-only”表示并非固定定义，取决于归一化实现。


<details>
  <summary>Details</summary>
Motivation: 探索在土耳其语中，哪些信号能有效区分LVC的习惯用法与字面用法，以及不同输入表示对模型判定的影响，以改进对复杂谓词的检测。

Method: 在 UD 标注 supervision 下进行多模型对比：lemma TF–IDF + Logistic Regression；BERTurk 以 lemma 序列为输入；仅基于 UD 形态语法特征（UPOS/DEPREL/MORPH）的语法线性回归；以及完整输入的 BERTurk 基线。评估在受控诊断集上，包括随机 negatives、词汇控制 NLVC、LVC positives，逐分割报告性能以揭示决策边界行为。

Result: 结果显示，粗粒度形态语法 alone 对在严格对比下的鲁棒 LVC 检测不足，而词汇身份在某些设定下能支持 LVC 判断，但对校准与归一化选择敏感。总体而言，lemma-only 表征并非单一固定表示，强烈依赖归一化的具体实现方式。

Conclusion: 研究强调需要对土耳其MWEs进行有针对性的评估，提示“lemma-only”并非稳定的单一表示，未来工作应聚焦于归一化策略和多信号综合以提升 LVC 检测的鲁棒性。

Abstract: Light verb constructions (LVCs) are a challenging class of verbal multiword expressions, especially in Turkish, where rich morphology and productive complex predicates create minimal contrasts between idiomatic predicate meanings and literal verb--argument uses. This paper asks what signals drive LVC classification by systematically restricting model inputs. Using UD-derived supervision, we compare lemma-driven baselines (lemma TF--IDF + Logistic Regression; BERTurk trained on lemma sequences), a grammar-only Logistic Regression over UD morphosyntax (UPOS/DEPREL/MORPH), and a full-input BERTurk baseline. We evaluate on a controlled diagnostic set with Random negatives, lexical controls (NLVC), and LVC positives, reporting split-wise performance to expose decision-boundary behavior. Results show that coarse morphosyntax alone is insufficient for robust LVC detection under controlled contrasts, while lexical identity supports LVC judgments but is sensitive to calibration and normalization choices. Overall, Our findings motivate targeted evaluation of Turkish MWEs and show that ``lemma-only'' is not a single, well-defined representation, but one that depends critically on how normalization is operationalized.

</details>


### [10] [Language Models Struggle to Use Representations Learned In-Context](https://arxiv.org/abs/2602.04212)
*Michael A. Lepori,Tal Linzen,Ann Yuan,Katja Filippova*

Main category: cs.CL

TL;DR: 开放权重的LLMs在将上下文中定义的新语义表示部署到下游任务方面仍存在显著挑战，闭源模型亦难以可靠利用这类上下文模式。


<details>
  <summary>Details</summary>
Motivation: 探究在-context 表示学习能否被模型在部署后灵活利用，以实现对全新情境的适应性和通用智能目标。

Method: 对开放权重的LLMs进行下一个词预测的可部署性评估，并引入一个新颖的自适应世界建模任务；比较开放与闭源模型在上下文中新模式的可部署性。

Result: 实验表明，模型虽能在latent层编码上下文中的新语义，但难以将这些表示用于实际任务的部署；即使是最强的闭源推理模型也不能可靠地利用上下文中新模式。

Conclusion: 需要开发新方法，使模型不仅编码上下文信息，还能在不同情境中灵活部署，从而提升对上下文表示的高效利用。

Abstract: Though large language models (LLMs) have enabled great success across a wide variety of tasks, they still appear to fall short of one of the loftier goals of artificial intelligence research: creating an artificial system that can adapt its behavior to radically new contexts upon deployment. One important step towards this goal is to create systems that can induce rich representations of data that are seen in-context, and then flexibly deploy these representations to accomplish goals. Recently, Park et al. (2024) demonstrated that current LLMs are indeed capable of inducing such representation from context (i.e., in-context representation learning). The present study investigates whether LLMs can use these representations to complete simple downstream tasks.
  We first assess whether open-weights LLMs can use in-context representations for next-token prediction, and then probe models using a novel task, adaptive world modeling. In both tasks, we find evidence that open-weights LLMs struggle to deploy representations of novel semantics that are defined in-context, even if they encode these semantics in their latent representations. Furthermore, we assess closed-source, state-of-the-art reasoning models on the adaptive world modeling task, demonstrating that even the most performant LLMs cannot reliably leverage novel patterns presented in-context. Overall, this work seeks to inspire novel methods for encouraging models to not only encode information presented in-context, but to do so in a manner that supports flexible deployment of this information.

</details>


### [11] [Tokenization and Morphological Fidelity in Uralic NLP: A Cross-Lingual Evaluation](https://arxiv.org/abs/2602.04241)
*Nuo Xu,Ahrii Kim*

Main category: cs.CL

TL;DR: OBPE在六种乌拉尔语的形态性和POS标注任务中，优于BPE与Unigram LM，尤其在拉丁字母组，表现为更少的词法切分碎片和对数值分布的更好平衡；跨语言迁移效果受下游标注架构、训练量与系属近缘性的共同影响。


<details>
  <summary>Details</summary>
Motivation: 在形态丰富且低资源的语言族中，子词切分对NLP性能影响显著，但不同子词范式在此类语言中的表现尚不清晰。本文通过对比三种主流子词范式，系统评估对下游任务（POS标注）的影响，以明确哪种切分更利于形态对齐与跨语迁移。

Method: 在六种资源差异与形态学多样性的乌拉尔语言家族中，系统比较BPE、Overlap BPE（OBPE）、Unigram Language Model三种子词范式。以POS标注作为受控下游任务，评估三者在形态对齐、标注准确性及分布多样性方面的表现，并分析开放类词汇碎片化与频率分布平衡对结果的影响。同时考察下游标注架构、训练量及系属近缘性对跨语迁移效能的交互作用。

Result: OBPE在形态对齐与标注准确性方面稳定优于BPE与Unigram LM，尤其在使用拉丁字母的语言群体中表现突出。改进来源包括：减少开放类词汇的碎片化、在频率谱上的更好平衡。跨语言迁移的效果依赖于下游标注架构，并与训练数据量及系属近缘性相互作用。此外，研究强调了面向形态的切分策略在低资源、黏合语言中的关键性。

Conclusion: 形态感知的子词切分并非简单的预处理步骤，而是决定性因素，尤其在黏合型、低资源语言的跨语迁移场景中。OBPE作为一种更符合形态结构的切分范式，能显著提升跨语NLP系统的有效性。

Abstract: Subword tokenization critically affects Natural Language Processing (NLP) performance, yet its behavior in morphologically rich and low-resource language families remains under-explored. This study systematically compares three subword paradigms -- Byte Pair Encoding (BPE), Overlap BPE (OBPE), and Unigram Language Model -- across six Uralic languages with varying resource availability and typological diversity. Using part-of-speech (POS) tagging as a controlled downstream task, we show that OBPE consistently achieves stronger morphological alignment and higher tagging accuracy than conventional methods, particularly within the Latin-script group. These gains arise from reduced fragmentation in open-class categories and a better balance across the frequency spectrum. Transfer efficacy further depends on the downstream tagging architecture, interacting with both training volume and genealogical proximity. Taken together, these findings highlight that morphology-sensitive tokenization is not merely a preprocessing choice but a decisive factor in enabling effective cross-lingual transfer for agglutinative, low-resource languages.

</details>


### [12] [CoLT: Reasoning with Chain of Latent Tool Calls](https://arxiv.org/abs/2602.04246)
*Fangwei Zhu,Zhifang Sui*

Main category: cs.CL

TL;DR: CoLT 将潜在推理转化为“工具调用”，通过种子令牌触发外部小模型将推理步展开为显性推理，从而在保持大模型推理能力的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法依赖对模型结构的改动和大量训练，限制了广泛应用；需要在不显式改动主模型的前提下提升推理效率和通用性。

Method: 提出 CoLT 框架：用 seed tokens 表示推理步骤的关键信息；触发潜在工具调用时，外部小模型以 seed tokens 的隐藏状态为输入，将 seed 解包为完整推理步骤；主模型保持在显性 token 空间进行推理；兼容多种解码器结构与强化学习算法。

Result: 在四个数学数据集上，CoLT 相比基线潜在模型具有更高准确率和更短推理长度；并且与 RL 和不同解码器结构兼容。

Conclusion: CoLT 成功在不改变主模型的前提下实现潜在推理的高效化与可扩展性，表明工具调用式潜在推理是一种有效且广泛适用的策略。

Abstract: Chain-of-Thought (CoT) is a critical technique in enhancing the reasoning ability of Large Language Models (LLMs), and latent reasoning methods have been proposed to accelerate the inefficient token-level reasoning chain. We notice that existing latent reasoning methods generally require model structure augmentation and exhaustive training, limiting their broader applicability. In this paper, we propose CoLT, a novel framework that implements latent reasoning as ``tool calls''. Instead of reasoning entirely in the latent space, CoLT generates seed tokens that contain information of a reasoning step. When a latent tool call is triggered, a smaller external model will take the hidden states of seed tokens as its input, and unpack the seed tokens back to a full reasoning step. In this way, we can ensure that the main model reasons in the explicit token space, preserving its ability while improving efficiency. Experimental results on four mathematical datasets demonstrate that CoLT achieves higher accuracy and shorter reasoning length than baseline latent models, and is compatible with reinforcement learning algorithms and different decoder structures.

</details>


### [13] [DementiaBank-Emotion: A Multi-Rater Emotion Annotation Corpus for Alzheimer's Disease Speech (Version 1.0)](https://arxiv.org/abs/2602.04247)
*Cheonkam Jeong,Jessica Liao,Audrey Lu,Yutong Song,Christopher Rashidian,Donna Krogh,Erik Krogh,Mahkameh Rasouli,Jung-Ah Lee,Nikil Dutt,Lisa M Gibbs,David Sultzer,Julie Rousseau,Jocelyn Ludlow,Margaret Galvez,Alexander Nuth,Chet Khay,Sabine Brunswicker,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 提出 DementiaBank-Emotion：首个针对阿尔茨海默病语音的多评审情感注释语料库，对1,492句与108位说话者进行了Ekman六基本情感及中性标签标注，发现AD患者表达非中性情感比例显著高于健康对照；初步声学分析提示情感-音高调制在对照组与AD组之间存在潜在解耦；在AD语音中，音量对情感类别有区分作用，表明情感语调映射在一定程度上得以保留。


<details>
  <summary>Details</summary>
Motivation: 填补临床人群情感识别研究中的注释语料匮乏，提供多评审标注、基准材料和校准工作坊资源，促进AD语音情感识别的研究与方法评估。

Method: 对108名说话者共1492句 utterances 进行Ekman六基本情感与中性标签的多评审注释；统计比较AD患者与健康对照的非中性情感比例；进行初步的探索性声学分析，聚焦F0与响度等声学特征；评估在AD群体内情感-声学映射的保留程度；最终对外发布语料、注释指南和校准材料。

Result: AD患者表达非中性情感的比例高于对照组（16.9% vs 5.7%，p<0.001）；对 sadness 的F0调制在对照组显著明显而AD组不足， interaction p=0.023；该F0结果样本量有限需复制；在AD语音中，响度可区分情感类别，提示部分保留情感-语调映射。

Conclusion: 该语料库为AD情感识别研究提供基准数据与资源，揭示AD语音中的情感表达特征及其局限性，并提出需进一步复制和扩展的方向。

Abstract: We present DementiaBank-Emotion, the first multi-rater emotion annotation corpus for Alzheimer's disease (AD) speech. Annotating 1,492 utterances from 108 speakers for Ekman's six basic emotions and neutral, we find that AD patients express significantly more non-neutral emotions (16.9%) than healthy controls (5.7%; p < .001). Exploratory acoustic analysis suggests a possible dissociation: control speakers showed substantial F0 modulation for sadness (Delta = -3.45 semitones from baseline), whereas AD speakers showed minimal change (Delta = +0.11 semitones; interaction p = .023), though this finding is based on limited samples (sadness: n=5 control, n=15 AD) and requires replication. Within AD speech, loudness differentiates emotion categories, indicating partially preserved emotion-prosody mappings. We release the corpus, annotation guidelines, and calibration workshop materials to support research on emotion recognition in clinical populations.

</details>


### [14] [ECG-R1: Protocol-Guided and Modality-Agnostic MLLM for Reliable ECG Interpretation](https://arxiv.org/abs/2602.04279)
*Jiarui Jin,Haoyu Wang,Xingliang Wu,Xiaocheng Fang,Xiang Lan,Zihan Wang,Deyun Zhang,Bo Liu,Yingying Zhang,Xian Wu,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: 提出ECG-R1：首个面向可靠ECG解释的推理型多模态大语言模型，通过协议引导数据生成、模态解耦Dropout、以及带证据奖励的强化学习等三大创新，提升解释的可核验性与鲁棒性；系统评估显示现有MLLM普遍存在严重幻觉，代码与平台公开。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在ECG解释方面常产生表面上合理但临床不准确的分析，缺乏可核验性与证据支撑，亟需在数据、架构与训练信号上提升可靠性以支持临床应用。

Method: 三大创新：1) Protocol-Guided Instruction Data Generation：基于可测ECG特征、阈值与诊断逻辑的指令数据生成，建立可量化的解读标准。2) Interleaved Modality Dropout：模态解耦架构，在ECG信号或ECG图像缺失时仍保持鲁棒性与跨模态一致性。3) Reinforcement Learning with ECG Diagnostic Evidence Rewards：以诊断证据奖励强化证据驱动的解释。对比评估了专有、开源和医疗MLLMs。

Result: 实验与评估显示多种MLLM存在广泛且严重的幻觉问题；ECG-R1在解释可靠性与证据支撑方面取得进展，并提供公开数据与实现以促进评估与复现。

Conclusion: ECG-R1推动可核验、鲁棒的ECG解释能力，但幻觉风险仍然存在，公众不应盲信输出，需独立验证。代码、数据与在线平台公开，促进领域透明度与可重复性。

Abstract: Electrocardiography (ECG) serves as an indispensable diagnostic tool in clinical practice, yet existing multimodal large language models (MLLMs) remain unreliable for ECG interpretation, often producing plausible but clinically incorrect analyses. To address this, we propose ECG-R1, the first reasoning MLLM designed for reliable ECG interpretation via three innovations. First, we construct the interpretation corpus using \textit{Protocol-Guided Instruction Data Generation}, grounding interpretation in measurable ECG features and monograph-defined quantitative thresholds and diagnostic logic. Second, we present a modality-decoupled architecture with \textit{Interleaved Modality Dropout} to improve robustness and cross-modal consistency when either the ECG signal or ECG image is missing. Third, we present \textit{Reinforcement Learning with ECG Diagnostic Evidence Rewards} to strengthen evidence-grounded ECG interpretation. Additionally, we systematically evaluate the ECG interpretation capabilities of proprietary, open-source, and medical MLLMs, and provide the first quantitative evidence that severe hallucinations are widespread, suggesting that the public should not directly trust these outputs without independent verification. Code and data are publicly available at \href{https://github.com/PKUDigitalHealth/ECG-R1}{here}, and an online platform can be accessed at \href{http://ai.heartvoice.com.cn/ECG-R1/}{here}.

</details>


### [15] [Contextual Drag: How Errors in the Context Affect LLM Reasoning](https://arxiv.org/abs/2602.04288)
*Yun Cheng,Xingyu Zhu,Haoyu Zhao,Sanjeev Arora*

Main category: cs.CL

TL;DR: Contextual drag: failed attempts in a prompt bias future generations toward similar errors, causing 10-20% drops across 11 models and 8 tasks; mitigations only partially help; persistent failure mode.


<details>
  <summary>Details</summary>
Motivation: To understand risks of self-improvement loops in LLMs by reflecting on past mistakes and how context contains error trajectories.

Method: Empirical study over 11 proprietary/open models and 8 reasoning tasks; structural analysis via tree edit distance to compare subsequent reasoning trajectories; evaluate external feedback, self-verification, and mitigation strategies (fallback-behavior fine-tuning, context denoising).

Result: Contextual drag induces 10-20% performance drops; iterative self-refinement can lead to self-deterioration; error patterns are inherited structurally from context; external feedback and self-verification do not resolve it; mitigations yield partial improvements but not full restoration.

Conclusion: Contextual drag is a persistent failure mode in current reasoning architectures that demands new approaches beyond standard feedback or denoising to ensure robust improvement.

Abstract: Central to many self-improvement pipelines for large language models (LLMs) is the assumption that models can improve by reflecting on past mistakes. We study a phenomenon termed contextual drag: the presence of failed attempts in the context biases subsequent generations toward structurally similar errors. Across evaluations of 11 proprietary and open-weight models on 8 reasoning tasks, contextual drag induces 10-20% performance drops, and iterative self-refinement in models with severe contextual drag can collapse into self-deterioration. Structural analysis using tree edit distance reveals that subsequent reasoning trajectories inherit structurally similar error patterns from the context. We demonstrate that neither external feedback nor successful self-verification suffices to eliminate this effect. While mitigation strategies such as fallback-behavior fine-tuning and context denoising yield partial improvements, they fail to fully restore baseline performance, positioning contextual drag as a persistent failure mode in current reasoning architectures.

</details>


### [16] [Guided Verifier: Collaborative Multimodal Reasoning via Dynamic Process Supervision](https://arxiv.org/abs/2602.04290)
*Lingzhuang Sun,Ruitong Liu,Yuxia Zhu,Xiaohan Xu,Jingxuan Wei,Xiangxiang Zhang,Bihui Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: 提出 Guided Verifier 框架，在多模态大语言模型的强化学习中引入动态 verifier 与协作推理，辅以 CoRe 与 Correct-guide Reasoning 数据集，在数学基准上提升 8B 参数模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决单轮回策略在推理过程中的错误传播和信号噪声问题，提供中间监督以提高推理质量。

Method: 引入动态 verifier，能与策略模型在 rollout 过程中进行互动，检测不一致并提供引导；构建 CoRe（进程级负样本）和 Correct-guide Reasoning（正确引导推理）数据集以训练 guided verifier；在 MathVista、MathVerse、MMMU 上进行实验，展示协同推理和动态验证提升性能。

Result: 在 8B 参数模型上，分配计算资源给协作推理和动态验证后取得强性能，基于所述数据集的训练和评估方法达到良好表现。

Conclusion: 动态验证和协作推理有效缓解错误传播，提升多模态大语言模型在计算推理任务中的表现，8B 参数模型即可达到竞争力水平。

Abstract: Reinforcement Learning (RL) has emerged as a pivotal mechanism for enhancing the complex reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevailing paradigms typically rely on solitary rollout strategies where the model works alone. This lack of intermediate oversight renders the reasoning process susceptible to error propagation, where early logical deviations cascade into irreversible failures, resulting in noisy optimization signals. In this paper, we propose the \textbf{Guided Verifier} framework to address these structural limitations. Moving beyond passive terminal rewards, we introduce a dynamic verifier that actively co-solves tasks alongside the policy. During the rollout phase, this verifier interacts with the policy model in real-time, detecting inconsistencies and providing directional signals to steer the model toward valid trajectories. To facilitate this, we develop a specialized data synthesis pipeline targeting multimodal hallucinations, constructing \textbf{CoRe} dataset of process-level negatives and \textbf{Co}rrect-guide \textbf{Re}asoning trajectories to train the guided verifier. Extensive experiments on MathVista, MathVerse and MMMU indicate that by allocating compute to collaborative inference and dynamic verification, an 8B-parameter model can achieve strong performance.

</details>


### [17] [How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks](https://arxiv.org/abs/2602.04294)
*Yanshu Wang,Shuaishuai Yang,Jingjing He,Tong Yang*

Main category: cs.CL

TL;DR: Few-shot demonstrations have opposite effects on RoP vs ToP in jailbreak defenses: they improve RoP safety by up to 4.5% by reinforcing role identity, but degrade ToP effectiveness by up to 21.2% by distracting attention from task instructions. The study evaluates multiple LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) and six jailbreak methods, offering deployment recommendations for prompt-based defenses.


<details>
  <summary>Details</summary>
Motivation: To clarify how few-shot demonstrations interact with different system-prompt strategies (RoP vs ToP) in jailbreak defenses, addressing inconsistent findings about few-shot safety and providing actionable guidance for real-world LLM deployment.

Method: Comprehensive empirical evaluation across multiple mainstream LLMs, four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest), and six jailbreak attack methods. Compare RoP and ToP under with- and without-few-shot settings, measure safety rate changes, and quantify the maximum reported effects.

Result: Few-shot demonstrations boost RoP safety by up to 4.5% through stronger role identity; they weaken ToP safety by up to 21.2% by causing distraction from explicit task instructions.

Conclusion: The findings yield practical recommendations for deploying prompt-based defenses: consider the complementary effects of few-shot on RoP and ToP, tailor defense choice to the use case, and implement mitigations to guard ToP against few-shot distraction while leveraging RoP’s strengthened role alignment.

Abstract: Large Language Models (LLMs) face increasing threats from jailbreak attacks that bypass safety alignment. While prompt-based defenses such as Role-Oriented Prompts (RoP) and Task-Oriented Prompts (ToP) have shown effectiveness, the role of few-shot demonstrations in these defense strategies remains unclear. Prior work suggests that few-shot examples may compromise safety, but lacks investigation into how few-shot interacts with different system prompt strategies. In this paper, we conduct a comprehensive evaluation on multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods. Our key finding reveals that few-shot demonstrations produce opposite effects on RoP and ToP: few-shot enhances RoP's safety rate by up to 4.5% through reinforcing role identity, while it degrades ToP's effectiveness by up to 21.2% through distracting attention from task instructions. Based on these findings, we provide practical recommendations for deploying prompt-based defenses in real-world LLM applications.

</details>


### [18] [Revisiting Prompt Sensitivity in Large Language Models for Text Classification: The Role of Prompt Underspecification](https://arxiv.org/abs/2602.04297)
*Branislav Pecher,Michal Spiegel,Robert Belanec,Jan Cegin*

Main category: cs.CL

TL;DR:  underspecified prompts drive most sensitivity to prompts; instruction prompts are more stable. Internal representations are only mildly affected by underspecification, with effects materializing in the final layers. Implication: study design and evaluation of prompt sensitivity should rigorously account for prompt specification.


<details>
  <summary>Details</summary>
Motivation: 揭示导致提示敏感性的根源，区分提示不充分（underspecified）与给出明确指令的提示对模型输出的影响，以提高对提示敏感性的理解与缓解策略的有效性。

Method: 通过比较underspecified prompts与instruction prompts，在性能分析、logits分析和线性探测等多种分析框架下进行系统对比，评估对LLM分类/零-shot任务的影响。

Result: underspecified prompts exhibit higher performance variance and lower logits for relevant tokens; instruction prompts show mitigated variance and更高的相关-token logits。线性探测表明underspecification对内部表征的影响有限，主要在最终层体现。

Conclusion: 提示敏感性的一个重要原因是提示的不充分，需要在研究与缓解中采取更严格的分析设计；对提示设计的关注（如提供明确指令）有望降低敏感性并提高鲁棒性。

Abstract: Large language models (LLMs) are widely used as zero-shot and few-shot classifiers, where task behaviour is largely controlled through prompting. A growing number of works have observed that LLMs are sensitive to prompt variations, with small changes leading to large changes in performance. However, in many cases, the investigation of sensitivity is performed using underspecified prompts that provide minimal task instructions and weakly constrain the model's output space. In this work, we argue that a significant portion of the observed prompt sensitivity can be attributed to prompt underspecification. We systematically study and compare the sensitivity of underspecified prompts and prompts that provide specific instructions. Utilising performance analysis, logit analysis, and linear probing, we find that underspecified prompts exhibit higher performance variance and lower logit values for relevant tokens, while instruction-prompts suffer less from such problems. However, linear probing analysis suggests that the effects of prompt underspecification have only a marginal impact on the internal LLM representations, instead emerging in the final layers. Overall, our findings highlight the need for more rigour when investigating and mitigating prompt sensitivity.

</details>


### [19] [DeFrame: Debiasing Large Language Models Against Framing Effects](https://arxiv.org/abs/2602.04306)
*Kahee Lim,Soyeon Kim,Steven Euijong Whang*

Main category: cs.CL

TL;DR: Framing disparity: how prompt framing affects fairness in LLMs; augment benchmarks with alternative framings; existing debiasing improves average fairness but not framing-induced gaps; propose framing-aware debiasing to improve consistency and robustness.


<details>
  <summary>Details</summary>
Motivation: LLMs can appear fair under standard evaluations but may reveal bias in other framings. There is a need to quantify and mitigate framing-induced fairness gaps.

Method: Introduce framing disparity metric; augment fairness benchmarks with semantically equivalent but differently framed prompts; assess impact of framing on fairness scores; develop a framing-aware debiasing objective to enforce cross-framing consistency.

Result: Fairness scores vary significantly with framing; standard debiasing improves overall fairness but often leaves framing disparities intact; framing-aware debiasing reduces both overall bias and framing-induced gaps, enhancing robustness.

Conclusion: Framing is a critical factor in fairness evaluations. A framing-aware debiasing approach yields fairer and more consistent LLM responses across framings.

Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, ensuring their fair responses across demographics has become crucial. Despite many efforts, an ongoing challenge is hidden bias: LLMs appear fair under standard evaluations, but can produce biased responses outside those evaluation settings. In this paper, we identify framing -- differences in how semantically equivalent prompts are expressed (e.g., "A is better than B" vs. "B is worse than A") -- as an underexplored contributor to this gap. We first introduce the concept of "framing disparity" to quantify the impact of framing on fairness evaluation. By augmenting fairness evaluation benchmarks with alternative framings, we find that (1) fairness scores vary significantly with framing and (2) existing debiasing methods improve overall (i.e., frame-averaged) fairness, but often fail to reduce framing-induced disparities. To address this, we propose a framing-aware debiasing method that encourages LLMs to be more consistent across framings. Experiments demonstrate that our approach reduces overall bias and improves robustness against framing disparities, enabling LLMs to produce fairer and more consistent responses.

</details>


### [20] [A Domain-Specific Curated Benchmark for Entity and Document-Level Relation Extraction](https://arxiv.org/abs/2602.04320)
*Marco Martinelli,Stefano Marchesin,Vanessa Bonato,Giorgio Maria Di Nunzio,Nicola Ferro,Ornella Irrera,Laura Menotti,Federica Vezzani,Gianmaria Silvello*

Main category: cs.CL

TL;DR: GutBrainIE 是一个基于 1,600+篇 PubMed 摘要的手工标注医学信息抽取基准，覆盖实体、概念级链接和关系，结合精确标注与弱监督数据，适用于命名实体识别、命名实体连结和关系抽取等多任务，具跨领域应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学信息抽取领域的基准往往范围狭窄、依赖遥感/自动生成的标注，限制了方法的鲁棒性与通用性。围绕快速发展的肠脑轴领域的研究需要高质量、多任务、可扩展的基准来驱动 IE 研究。

Method: 由生物医学与术语学专家对 1,600+ 篇 PubMed 摘要进行手工标注，提供细粒度实体、概念级链接以及关系信息；数据集兼具高质量的“精标”与一定比例的弱监督数据，形成可用于多任务的丰富语义架构。

Result: 构建了一个覆盖广域的基准数据集，具备细粒度实体、概念链接与实体间关系等丰富标注，虽然以肠脑轴为主题，但其架构、任务设定与数据组合具有跨领域的适用性，可用于评估和驱动生物医学 IE 系统的研究。

Conclusion: 该基准克服了现有数据集在范围、标注质量与多任务能力上的不足，为生物医学 IE 的方法开发与评估提供了更强的资源，尤其适合检验在跨领域情境下的泛化能力。

Abstract: Information Extraction (IE), encompassing Named Entity Recognition (NER), Named Entity Linking (NEL), and Relation Extraction (RE), is critical for transforming the rapidly growing volume of scientific publications into structured, actionable knowledge. This need is especially evident in fast-evolving biomedical fields such as the gut-brain axis, where research investigates complex interactions between the gut microbiota and brain-related disorders. Existing biomedical IE benchmarks, however, are often narrow in scope and rely heavily on distantly supervised or automatically generated annotations, limiting their utility for advancing robust IE methods. We introduce GutBrainIE, a benchmark based on more than 1,600 PubMed abstracts, manually annotated by biomedical and terminological experts with fine-grained entities, concept-level links, and relations. While grounded in the gut-brain axis, the benchmark's rich schema, multiple tasks, and combination of highly curated and weakly supervised data make it broadly applicable to the development and evaluation of biomedical IE systems across domains.

</details>


### [21] [Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models](https://arxiv.org/abs/2602.04355)
*Sichu Liang,Hongyu Zhu,Wenwen Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 文本表征下的空间n-back在Qwen2.5家族上表现优于视觉表征，2/3-back更多体现即时最近感而非指示的保持时间；网格规模影响干扰与错位结构，提示多模态工作记忆的计算敏感性。


<details>
  <summary>Details</summary>
Motivation: 评估n-back作为工作记忆测量在文本编码与视觉编码的多模态模型中的可迁移性，以及刺激结构对潜在认知过程推断的影响。

Method: 在匹配文本渲染和图像渲染的网格上，对Qwen2.5与Qwen2.5-VL执行空间n-back任务；通过逐次试验的对数概率证据推断内部处理过程；并改变网格大小以观察干扰与错误模式的变化。

Result: 文本条件下的准确率与d'显著高于视觉条件；名义上的2/3-back并不总与指示的滞后一致，常呈现最近记忆相关的比较；网格尺寸改变刺激流中的最近重复结构，进而改变干扰与错误模式。

Conclusion: 结果支持对多模态工作记忆的计算敏感解读，强调在解释n-back探测时需考虑表示格式和刺激结构对潜在认知过程的影响。

Abstract: Working memory is a central component of intelligent behavior, providing a dynamic workspace for maintaining and updating task-relevant information. Recent work has used n-back tasks to probe working-memory-like behavior in large language models, but it is unclear whether the same probe elicits comparable computations when information is carried in a visual rather than textual code in vision-language models. We evaluate Qwen2.5 and Qwen2.5-VL on a controlled spatial n-back task presented as matched text-rendered or image-rendered grids. Across conditions, models show reliably higher accuracy and d' with text than with vision. To interpret these differences at the process level, we use trial-wise log-probability evidence and find that nominal 2/3-back often fails to reflect the instructed lag and instead aligns with a recency-locked comparison. We further show that grid size alters recent-repeat structure in the stimulus stream, thereby changing interference and error patterns. These results motivate computation-sensitive interpretations of multimodal working memory.

</details>


### [22] [Merged ChemProt-DrugProt for Relation Extraction from Biomedical Literature](https://arxiv.org/abs/2405.18605)
*Mai H. Nguyen,Shibani Likhite,Jiawei Tang,Darshini Mahendran,Bridget T. McInnes*

Main category: cs.CL

TL;DR: 合并 ChemProt 与 DrugProt 数据集以提升化学-基因关系抽取的样本量及模型性能，结合 BioBERT 与 GCN 能同时利用局部与全局信息，提升在 CPR 组的表现。


<details>
  <summary>Details</summary>
Motivation: 通过增大训练样本和跨数据集的一致性来提升化学-基因关系抽取的准确性与泛化能力，特别是在 CPR 组之间.

Method: 对比两种关系抽取框架：（1）BioBERT（BERT 基础的领域模型）用于局部上下文建模；（2）GCN 与 BioBERT 结合以引入全局信息。数据来源为合并的 ChemProt 与 DrugProt 数据集，评估在 CPR 组的性能表现。

Result: 数据集融合显著提升在 CPR 组的模型性能，BioBERT 在捕捉局部上下文方面表现优越；引入 GCN 的全局信息可进一步提高某些 CPR 组的精确率和召回率，相较于仅使用 BioBERT 的基线。

Conclusion: 结合全局与局部信息的模型（GCN+BioBERT）在部分 CPR 组上优于单独 BioBERT，数据集合并有效提高整体模型的准确性，尤其是在 CPR 组的重叠区域。

Abstract: The extraction of chemical-gene relations plays a pivotal role in understanding the intricate interactions between chemical compounds and genes, with significant implications for drug discovery, disease understanding, and biomedical research. This paper presents a data set created by merging the ChemProt and DrugProt datasets to augment sample counts and improve model accuracy. We evaluate the merged dataset using two state of the art relationship extraction algorithms: Bidirectional Encoder Representations from Transformers (BERT) specifically BioBERT, and Graph Convolutional Networks (GCNs) combined with BioBERT. While BioBERT excels at capturing local contexts, it may benefit from incorporating global information essential for understanding chemical-gene interactions. This can be achieved by integrating GCNs with BioBERT to harness both global and local context. Our results show that by integrating the ChemProt and DrugProt datasets, we demonstrated significant improvements in model performance, particularly in CPR groups shared between the datasets. Incorporating the global context using GCN can help increase the overall precision and recall in some of the CPR groups over using just BioBERT.

</details>


### [23] [Beyond Rejection Sampling: Trajectory Fusion for Scaling Mathematical Reasoning](https://arxiv.org/abs/2602.04391)
*Jie Deng,Hanshuang Tong,Jun Li,Shining Liang,Ning Wu,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: TrajFusion通过把错误轨迹与反思提示及正确轨迹融合成一个样本，显式建模试错推理，且按错误信号的频次与多样性自适应控制融合长度，最终在不改动架构和训练目标的前提下，普遍优于传统RFT，尤其在困难、长文推理任务上。


<details>
  <summary>Details</summary>
Motivation: 当前将拒绝采样视为二元筛选，过滤掉教师产生的错误推理，未能对训练中推理失败进行建模。需要更丰富的监督来指导模型在复杂问题上的推理过程。

Method: 构造融合轨迹：选择若干错误轨迹，与反思提示和正确轨迹交错拼接为一个样本；融合长度依据教师错误的频率和多样性自适应调整；当错误信号不具信息量时，退回到标准RFT；整个过程无需改动模型架构或训练目标。

Result: 在多项数学基准上的广泛实验表明，TrajFusion在各类任务上均优于RFT，且在具挑战性与长形式推理任务上优势更显著。

Conclusion: TrajFusion提供了一种无架构改动的更丰富的监督方式，通过显式建模推理失败来提高微调效果，且在广泛数学基准上表现优于传统拒绝采样微调。

Abstract: Large language models (LLMs) have made impressive strides in mathematical reasoning, often fine-tuned using rejection sampling that retains only correct reasoning trajectories. While effective, this paradigm treats supervision as a binary filter that systematically excludes teacher-generated errors, leaving a gap in how reasoning failures are modeled during training. In this paper, we propose TrajFusion, a fine-tuning strategy that reframes rejection sampling as a structured supervision construction process. Specifically, TrajFusion forms fused trajectories that explicitly model trial-and-error reasoning by interleaving selected incorrect trajectories with reflection prompts and correct trajectories. The length of each fused sample is adaptively controlled based on the frequency and diversity of teacher errors, providing richer supervision for challenging problems while safely reducing to vanilla rejection sampling fine-tuning (RFT) when error signals are uninformative. TrajFusion requires no changes to the architecture or training objective. Extensive experiments across multiple math benchmarks demonstrate that TrajFusion consistently outperforms RFT, particularly on challenging and long-form reasoning problems.

</details>


### [24] [Evaluating the Presence of Sex Bias in Clinical Reasoning by Large Language Models](https://arxiv.org/abs/2602.04392)
*Isabel Tsintsiper,Sheng Wong,Beth Albert,Shaun P Brennecke,Gabriel Davis Jones*

Main category: cs.CL

TL;DR: 现代大语言模型在临床推理中呈现稳定且模型特定的性别偏见，偏向随模型不同而异；允许选择不标注可降低显式标签但不消除诊断差异；需保守配置、专业数据审计以及持续的人类监督。


<details>
  <summary>Details</summary>
Motivation: 评估当代 LLMs 是否在临床推理中放大性别偏见，以及模型配置对这些偏见的影响，为医疗场景下的安全部署提供证据。

Method: 使用50份临床专家撰写的病案描述（44个专科），性别信息对初诊路径无信息价值；对4种通用型 LLM（ChatGPT、Claude 3.7 Sonnet、Gemini 2.0 Flash、DeepSeekchat）在不同配置下评估性别分配偏差，核心指标为在温度0.5时各模型对性别的预测倾向及其置信区间；研究设计包括可选择弃权以降低明确标注。

Result: 所有模型均表现出显著的性别分配偏斜，且偏斜方向随模型而异：ChatGPT约70%偏向女性、DeepSeek约61%、Claude约59%，Gemini则偏向男性，女性分配约36%。允许弃权虽减少显式标签，但未消除后续诊断差异。

Conclusion: 结果表明，现代通用型 LLMs 在临床推理中存在稳定、模型特异性的性别偏见；在医疗场景部署需采用保守的配置、对专科数据进行审计，并在部署过程中保持人类监督。

Abstract: Large language models (LLMs) are increasingly embedded in healthcare workflows for documentation, education, and clinical decision support. However, these systems are trained on large text corpora that encode existing biases, including sex disparities in diagnosis and treatment, raising concerns that such patterns may be reproduced or amplified. We systematically examined whether contemporary LLMs exhibit sex-specific biases in clinical reasoning and how model configuration influences these behaviours. We conducted three experiments using 50 clinician-authored vignettes spanning 44 specialties in which sex was non-informative to the initial diagnostic pathway. Four general-purpose LLMs (ChatGPT (gpt-4o-mini), Claude 3.7 Sonnet, Gemini 2.0 Flash and DeepSeekchat). All models demonstrated significant sex-assignment skew, with predicted sex differing by model. At temperature 0.5, ChatGPT assigned female sex in 70% of cases (95% CI 0.66-0.75), DeepSeek in 61% (0.57-0.65) and Claude in 59% (0.55-0.63), whereas Gemini showed a male skew, assigning a female sex in 36% of cases (0.32-0.41). Contemporary LLMs exhibit stable, model-specific sex biases in clinical reasoning. Permitting abstention reduces explicit labelling but does not eliminate downstream diagnostic differences. Safe clinical integration requires conservative and documented configuration, specialty-level clinical data auditing, and continued human oversight when deploying general-purpose models in healthcare settings.

</details>


### [25] [Bi-directional Bias Attribution: Debiasing Large Language Models without Modifying Prompts](https://arxiv.org/abs/2602.04398)
*Yujie Lin,Kunquan Li,Yixuan Liao,Xiaoxin Chen,Jinsong Su*

Main category: cs.CL

TL;DR: 在不进行微调或提示改动的前提下，通过识别刻板词、基于两种整合梯度归因策略将偏见指向特定神经元，并在投影层直接干预来缓解偏见，并在三种常用LLMs上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM输出偏见带来的公平性问题，同时避免大规模微调或影响多轮对话体验的代价，提供可扩展的偏见缓解方案。

Method: 1) 通过跨人口群体的对比分析识别刻板形容词和名词；2) 使用两种基于整合梯度的归因策略将偏见行为归因到特定神经元；3) 在投影层对相关激活进行干预以抑制偏见。

Result: 在三种广泛使用的LLMs上进行实验，偏见显著降低且总体性能得以保持，且方法无需微调或提示修改。代码可在GitHub获取。

Conclusion: 提出一种可扩展且低成本的偏见缓解框架，直接在模型内部进行干预以实现去偏，同时兼顾模型性能与用户体验。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks. However, their outputs often exhibit social biases, raising fairness concerns. Existing debiasing methods, such as fine-tuning on additional datasets or prompt engineering, face scalability issues or compromise user experience in multi-turn interactions. To address these challenges, we propose a framework for detecting stereotype-inducing words and attributing neuron-level bias in LLMs, without the need for fine-tuning or prompt modification. Our framework first identifies stereotype-inducing adjectives and nouns via comparative analysis across demographic groups. We then attribute biased behavior to specific neurons using two attribution strategies based on integrated gradients. Finally, we mitigate bias by directly intervening on their activations at the projection layer. Experiments on three widely used LLMs demonstrate that our method effectively reduces bias while preserving overall model performance. Code is available at the github link: https://github.com/XMUDeepLIT/Bi-directional-Bias-Attribution.

</details>


### [26] [Swordsman: Entropy-Driven Adaptive Block Partition for Efficient Diffusion Language Models](https://arxiv.org/abs/2602.04399)
*Yu Zhang,Xinchen Li,Jialei Zhou,Hongnan Ma,Zhongwei Wan,Yiwei Shi,Duoqian Miao,Qi Zhang,Longbing Cao*

Main category: cs.CL

TL;DR: 提出了一种基于熵驱动的自适应分块解码框架Swordsman，用于扩散语言模型的块状解码，通过识别相邻标记之间的熵跃来对齐语义/句法成分边界，并在块内动态调整解码阈值，实现训练无关的高效稳定推断，结合KV Cache达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的块状解码通常采用固定、刚性分块，易碎片化完整的语法/语义成分，导致性能下降。受熵下降假说（ERH）启发，成分边界具备更高的不确定性降低潜力，因此通过熵分析来定位边界以提升解码质量与效率。

Method: 自适应分块：通过检测相邻标记之间的熵跃来识别成分边界并重新划分块；解码阈值的动态调整：基于块内实时的“解码状态”来调整未掩码阈值，提升效率和稳定性；训练无关框架，利用KV Cache实现推断加速与稳定性；整体框架命名为Swordsman，基于现有DLM推断流程扩展。

Result: 在广泛评估中，Swordsman达到并超过当前方法的最优性能，展现出更高的推断效率与稳定性，且不需要额外训练。

Conclusion: 基于熵分析的自适应分块解码能够更好地与语义/句法成分边界对齐，提升块状解码的效率与质量，且具备训练-free特性与KV Cache支撑的实际可用性。

Abstract: Block-wise decoding effectively improves the inference speed and quality in diffusion language models (DLMs) by combining inter-block sequential denoising and intra-block parallel unmasking. However, existing block-wise decoding methods typically partition blocks in a rigid and fixed manner, which inevitably fragments complete semantic or syntactic constituents, leading to suboptimal performance. Inspired by the entropy reduction hypothesis (ERH), we recognize that constituent boundaries offer greater opportunities for uncertainty reduction, which motivates us to employ entropy analysis for identifying constituent boundaries. Therefore, we propose Swordsman, an entropy-driven adaptive block-wise decoding framework for DLMs. Swordsman adaptively partitions blocks by identifying entropy shifts between adjacent tokens to better align with semantic or syntactic constituent boundaries. In addition, Swordsman dynamically adjusts unmasking thresholds conditioned on the real-time unmasking status within a block, further improving both efficiency and stability. As a training-free framework, supported by KV Cache, Swordsman demonstrates state-of-the-art performance across extensive evaluations.

</details>


### [27] [History-Guided Iterative Visual Reasoning with Self-Correction](https://arxiv.org/abs/2602.04413)
*Xinglong Yang,Zhilin Peng,Zhanzhan Liu,Haochen Shi,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出一种基于历史信息复用的H-GIVR框架，在多轮推理中多次观看图像并以先前答案为参考实现动态纠错，从而提升跨模态推理准确性，同时保持较低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有自一致性方法多采用固定的重复采样与投票策略，未利用以往推理信息，难以主动纠错与动态调整推理过程。

Method: 在迭代推理过程中，模型多次查看图像，并将先前生成的答案作为参考用于后续步骤，实现动态纠错与历史信息复用。

Result: 在五个数据集、三种模型上取得显著提升；以Llama3.2-vision:11b在ScienceQA上平均每题2.57次回答达到78.90%准确性，相较基线提升约107%。

Conclusion: H-GIVR可显著提升跨模态推理准确性并保持低计算成本，通过历史信息复用与动态纠错增强鲁棒性与准确性。

Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However, most existing self-consistency methods are limited to a fixed ``repeated sampling and voting'' paradigm and do not reuse historical reasoning information. As a result, models struggle to actively correct visual understanding errors and dynamically adjust their reasoning during iteration. Inspired by the human reasoning behavior of repeated verification and dynamic error correction, we propose the H-GIVR framework. During iterative reasoning, the MLLM observes the image multiple times and uses previously generated answers as references for subsequent steps, enabling dynamic correction of errors and improving answer accuracy. We conduct comprehensive experiments on five datasets and three models. The results show that the H-GIVR framework can significantly improve cross-modal reasoning accuracy while maintaining low computational cost. For instance, using \texttt{Llama3.2-vision:11b} on the ScienceQA dataset, the model requires an average of 2.57 responses per question to achieve an accuracy of 78.90\%, representing a 107\% improvement over the baseline.

</details>


### [28] [Fine-Grained Activation Steering: Steering Less, Achieving More](https://arxiv.org/abs/2602.04428)
*Zijian Feng,Tianjiao Li,Zixiao Zhu,Hanzhang Zhou,Junlang Qian,Li Zhang,Jia Jim Deryl Chua,Lee Onn Mak,Gee Wah Ng,Kezhi Mao*

Main category: cs.CL

TL;DR: AUSteer: 通过在AU级别对激活进行干预来实现更细粒度的控制，解决块级干预的混杂问题；通过对比样本计算激活动量以识别判别性AU，并对输入自适应地分配干预强度，在多模型多任务上显著优于基线且仅干预较少的激活。


<details>
  <summary>Details</summary>
Motivation: 块级激活干预往往将有益、无关和有害特征混合在一起，导致干预不精确且影响副作用。细粒度AU层面的干预可更精准地引导输出分布。

Method: 将块级激活分解为AU级激活，每个AU对应块权矩阵的一维。通过对比样本计算激活动量以识别全局判别性AU；对不同输入分配自适应干预强度；在多种LLM与任务上评估，比较基线。

Result: 实验显示AUSteer在多个模型和任务中持续优于先进基线，同时干预的激活数显著减少，验证“少即是多”的核心结论。

Conclusion: AU级干预比块级干预更高效、有效，证明以对比样本驱动的判别性AU识别和自适应权重分配可实现更精细的激活 steering。

Abstract: Activation steering has emerged as a cost-effective paradigm for modifying large language model (LLM) behaviors. Existing methods typically intervene at the block level, steering the bundled activations of selected attention heads, feedforward networks, or residual streams. However, we reveal that block-level activations are inherently heterogeneous, entangling beneficial, irrelevant, and harmful features, thereby rendering block-level steering coarse, inefficient, and intrusive. To investigate the root cause, we decompose block activations into fine-grained atomic unit (AU)-level activations, where each AU-level activation corresponds to a single dimension of the block activation, and each AU denotes a slice of the block weight matrix. Steering an AU-level activation is thus equivalent to steering its associated AU. Our theoretical and empirical analysis show that heterogeneity arises because different AUs or dimensions control distinct token distributions in LLM outputs. Hence, block-level steering inevitably moves helpful and harmful token directions together, which reduces efficiency. Restricting intervention to beneficial AUs yields more precise and effective steering. Building on this insight, we propose AUSteer, a simple and efficient method that operates at a finer granularity of the AU level. AUSteer first identifies discriminative AUs globally by computing activation momenta on contrastive samples. It then assigns adaptive steering strengths tailored to diverse inputs and selected AU activations. Comprehensive experiments on multiple LLMs and tasks show that AUSteer consistently surpasses advanced baselines while steering considerably fewer activations, demonstrating that steering less achieves more.

</details>


### [29] [No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data](https://arxiv.org/abs/2602.04442)
*Dmitry Karpov*

Main category: cs.CL

TL;DR: 对五对土库曼语系语言的机器翻译进行比较研究，使用 LoRA 微调与合成数据、检索增强、以及零-shot/检索策略，取得不同语言对的显著翻译质量提升，并公开数据集与权重。


<details>
  <summary>Details</summary>
Motivation: 解决土耳其语系语言对的机器翻译质量不足的问题，比较多种适配策略在跨语言对上的有效性，并评估合成数据与检索提示对性能的影响。

Method: 对五对语言进行实验：1) Russian→Kazakh、Bashkir、Kyrgyz；2) English→Tatar、Chuvash。Kazakh与Bashkir使用 LoRA 微调 NLLB-200-distilled-600M 于合成数据，评估 chrF++ 指标；Chuvash 使用 DeepSeek-V3.2 检索相似示例的提示；Tatar、Kyrgyz 探索零-shot 或 检索驱动方法。公开数据集与权重。

Result: Kazakh: chrF++ 49.71； Bashkir: chrF++ 46.94； Chuvash: chrF++ 39.47； Tatar: zero-shot/retrieval 41.6； Kyrgyz: zero-shot 45.6。并发布数据集及获得的权重。

Conclusion: 多语言翻译在土库曼语系显示不同程度的提升，LoRA 微调结合合成数据对 Kazakh/Bashkir 效果显著，检索驱动/提示对 Chuvash 有显著收益，Tatar 与 Kyrgyz 的结果则受策略影响较大；研究提供数据与权重以便复现与进一步研究。

Abstract: We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.

</details>


### [30] [Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks](https://arxiv.org/abs/2602.04466)
*Masaya Tsunokake,Yuta Koreeda,Terufumi Morishita,Koichi Nagatsuka,Hikaru Tomonari,Yasuhiro Sogawa*

Main category: cs.CL

TL;DR: mDAPT improves the elicitation of facts in a micro-domain, but does not fix reasoning or long-form composition in generative IT-support tasks; overall performance hinges on reasoning, with >90% achieved only when elicitation and reasoning are solved.


<details>
  <summary>Details</summary>
Motivation: To extend the evaluation of micro domain-adaptive pre-training (mDAPT) from multiple-choice benchmarks to generative tasks in real-world enterprise operations, and identify bottlenecks in micro-domain knowledge transfer.

Method: Disentangle the answer-generation process into three subtasks (eliciting, reasoning, composing) and evaluate each subtask using proprietary IT product knowledge in real-world IT technical support queries, comparing base models with mDAPT.

Result: mDAPT resolves the elicitation subtask (improves retrieval of relevant facts) but does not resolve reasoning over those facts or the construction of long-form answers.

Conclusion: mDAPT has a clear benefit for knowledge elicitation but limited impact on reasoning and composition; achieving strong performance (>90%) requires advances in reasoning capabilities, suggesting a bottleneck beyond factual elicitation.

Abstract: When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) $\textbf{eliciting}$ facts relevant to questions from an LLM's own knowledge, (2) $\textbf{reasoning}$ over the facts to obtain conclusions, and (3) $\textbf{composing}$ long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90%), emphasizing the need to enhance reasoning capability.

</details>


### [31] [Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition](https://arxiv.org/abs/2602.04486)
*Jinlong Ma,Yu Zhang,Xuefeng Bai,Kehai Chen,Yuwei Wang,Zeming Liu,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 提出了基于多模态大语言模型的GMNER端到端缺陷分析，发现模型存在模态偏置（视觉/文本），并提出模态感知一致性推理（MCR）来解决，核心在于两大组件：多风格推理模式注入（MRSI）与约束引导可验证优化（CVO），通过与分组相对策略优化（GRPO）对齐推理轨迹。实验表明MCR可缓解模态偏置并在GMNER与视觉定位任务上优于基线。


<details>
  <summary>Details</summary>
Motivation: 驱动点在于将MLLMs从辅助工具提升为端到端GMNER的核心组件，同时解决跨模态推理中的模态偏置问题，提升对文本和视觉信息的鲁棒一致性与可验证性。

Method: 提出模态感知一致性推理（MCR），包括：1) 多风格推理模式注入（MRSI），将抽象约束转化为可执行的跨模态推理链；2) 约束引导可验证优化（CVO），使模型推理轨迹动态对齐到分组相对策略优化（GRPO），并通过结构化约束实现跨模态一致性。

Result: 在GMNER与视觉定位任务上，MCR有效缓解模态偏置，提升相较于现有基线的性能。

Conclusion: MCR能够有效降低模态偏置，提升端到端GMNER的表现，具有对其他多模态任务的潜在扩展性，同时也提示需要关注推理效率和对广域约束的鲁棒性。

Abstract: Grounded Multimodal Named Entity Recognition (GMNER) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals a fundamental challenge: MLLMs exhibit $\textbf{modality bias}$, including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning ($\textbf{MCR}$), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines.

</details>


### [32] [Deconstructing sentence disambiguation by joint latent modeling of reading paradigms: LLM surprisal is not enough](https://arxiv.org/abs/2602.04489)
*Dario Paape,Tal Linzen,Shravan Vasishth*

Main category: cs.CL

TL;DR: 提出一个潜在过程混合模型来区分 garden-path 概率、成本与再分析成本，在眼动、单/双向自 paced、Maze 四种阅读范式中对比，预测力优于基于 GPT-2 surprisal 的对照模型。


<details>
  <summary>Details</summary>
Motivation: 弥补单纯 surprisal 模型在处理 garden-path 效应和跨范式阅读中的不足，尤其是在被动/不专注读取对加工成本的影响上。

Method: 构建包含三类潜在过程的混合模型：garden-path 概率、garden-path 成本、再分析成本；把四种阅读范式的数据整合在一起，并把不专注的试次纳入估计；通过交叉验证比较模型预测力，基准为基于 GPT-2 surprisal 的混合模型。

Result: 能再现再阅读、理解问题回答、语法判断等现象，且跨范式与端点任务数据的预测性能优于对照混合模型。

Conclusion: 提供一个统一且更现实的加工成本框架，强调不专注阅读对结果的影响，提示未来在阅读研究中更好地整合范式与惰性状态的处理。

Abstract: Using temporarily ambiguous garden-path sentences ("While the team trained the striker wondered ...") as a test case, we present a latent-process mixture model of human reading behavior across four different reading paradigms (eye tracking, uni- and bidirectional self-paced reading, Maze). The model distinguishes between garden-path probability, garden-path cost, and reanalysis cost, and yields more realistic processing cost estimates by taking into account trials with inattentive reading. We show that the model is able to reproduce empirical patterns with regard to rereading behavior, comprehension question responses, and grammaticality judgments. Cross-validation reveals that the mixture model also has better predictive fit to human reading patterns and end-of-trial task data than a mixture-free model based on GPT-2-derived surprisal values. We discuss implications for future work.

</details>


### [33] [PersoDPO: Scalable Preference Optimization for Instruction-Adherent, Persona-Grounded Dialogue via Multi-LLM Evaluation](https://arxiv.org/abs/2602.04493)
*Saleh Afzoon,MohammadHossein Ahmadi,Usman Naseem,Amin Beheshti*

Main category: cs.CL

TL;DR: PersoDPO 提供一个可扩展的偏好优化框架，通过自动评估信号来微调对话模型，以提升人格一致性和上下文连贯性；在 FoCus 数据集上对开源模型的表现优于强基线和标准 DPO。


<details>
  <summary>Details</summary>
Motivation: 当前开源 LLM 在保持流畅性的同时，难以实现与人物设定一致且上下文相符的回答，且手工标注成本高，因此需要一个可扩展的、无需人工注释的训练信号源。

Method: 利用来自闭源与开源模型生成回复的自动评估信号，结合连贯性、个性化和长度/格式合规等指标，自动构造高质量的偏好对，并以偏好优化（类似 DPO）对开源模型进行微调。

Result: 在 FoCus 数据集上，使用 PersoDPO 微调的开源语言模型在多个评价维度上，显著优于强基线与标准 DPO 变体。

Conclusion:  PersoDPO 提供了一个可扩展、可重复的训练管线，能同时提升人物感知和上下文一致性，改进开源 LLM 的个性化对话生成。

Abstract: Personalization and contextual coherence are two essential components in building effective persona-grounded dialogue systems. These aspects play a crucial role in enhancing user engagement and ensuring responses are more relevant and consistent with user identity. However, recent studies indicate that open-source large language models (LLMs) continue to struggle to generate responses that are both contextually grounded and aligned with persona cues, despite exhibiting strong general conversational abilities like fluency and naturalness. We present PersoDPO, a scalable preference optimisation framework that uses supervision signals from automatic evaluations of responses generated by both closed-source and open-source LLMs to fine-tune dialogue models. The framework integrates evaluation metrics targeting coherence and personalization, along with a length-format compliance feature to promote instruction adherence. These signals are combined to automatically construct high-quality preference pairs without manual annotation, enabling a scalable and reproducible training pipeline. Experiments on the FoCus dataset show that an open-source language model fine-tuned with the PersoDPO framework consistently outperforms strong open-source baselines and a standard Direct Preference Optimization (DPO) variant across multiple evaluation dimensions.

</details>


### [34] [Model-Dowser: Data-Free Importance Probing to Mitigate Catastrophic Forgetting in Multimodal Large Language Models](https://arxiv.org/abs/2602.04509)
*Hyeontaek Hwang,Nguyen Dinh Son,Daeyoung Kim*

Main category: cs.CL

TL;DR: 提出 Model-Dowser 的稀疏微调框架，通过对每个参数计算重要性并保留高重要性参数、更新其余，缓解多模态大语言模型的灾难性遗忘，且具备良好规模可扩展性。


<details>
  <summary>Details</summary>
Motivation: 微调多模态大语言模型往往导致对预训练任务的泛化能力下降（灾难性遗忘），且现有方法在深层解码器或大模型上效果有限、成本高。

Method: 联合考虑权重大小、输入激活和输出敏感性来为每个参数估算在预训练泛化任务上的重要性；在微调阶段只更新非高重要性参数，保留高重要性参数。

Result: 在两种代表性 MLLMs（LLaVA、NVILA）上广泛实验，Model-Dowser 能有效缓解灾难性遗忘，优于先前方法，同时资源开销低，且可扩展到数十亿参数规模。

Conclusion: Model-Dowser 提供了一种高效、可扩展的稀疏微调策略，适用于提升 MLLMs 的下游任务表现并保护预训练泛化能力。

Abstract: Fine-tuning Multimodal Large Language Models (MLLMs) on task-specific data is an effective way to improve performance on downstream applications. However, such adaptation often leads to a degradation in generalization on pretrained tasks, a phenomenon known as Catastrophic Forgetting. Existing methods that aim to mitigate this issue either become ineffective when fine-tuning deeper layers of the language decoder or scale poorly with increasing model size. To address these limitations, we propose Model-Dowser, a novel sparse fine-tuning approach for MLLMs. Model-Dowser measures a principled importance score for each model parameter with respect to pretrained generalization (prior to downstream adaptation) by jointly considering weight magnitudes, input activations, and output sensitivities. During fine-tuning, Model-Dowser selectively preserves high-importance parameters and updates the remaining. Comprehensive experiments on two representative MLLMs, LLaVA and NVILA, demonstrate that Model-Dowser effectively mitigates catastrophic forgetting and consistently outperforms prior methods, while remaining resource-efficient and scalable to multi-billion-parameter models.

</details>


### [35] [ReFRAME or Remain: Unsupervised Lexical Semantic Change Detection with Frame Semantics](https://arxiv.org/abs/2602.04514)
*Bach Phan-Tat,Kris Heylen,Dirk Geeraerts,Stefano De Pascale,Dirk Speelman*

Main category: cs.CL

TL;DR: 基于框架语义的LSC检测方法，替代或补充分布式嵌入，具备良好可解释性，在基准测试中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有神经嵌入方法在词义变化检测中性能虽好但缺乏可解释性。提出以框架语义为核心的替代方案，期望提高解释性并保持或提升检测效果。

Method: 仅使用框架语义进行表征与比较，评估词汇在不同时期语料中的框架分布变化以检测语义变化；对预测进行定量与定性分析，强调可解释性。

Result: 框架语义方法在词义变化检测任务上表现良好，甚至超越多数分布式语义模型；同时提供可解释的预测，便于理解与分析。

Conclusion: 框架语义为LSC提供一个可解释且有效的替代路径，值得进一步研究与应用。

Abstract: The majority of contemporary computational methods for lexical semantic change (LSC) detection are based on neural embedding distributional representations. Although these models perform well on LSC benchmarks, their results are often difficult to interpret. We explore an alternative approach that relies solely on frame semantics. We show that this method is effective for detecting semantic change and can even outperform many distributional semantic models. Finally, we present a detailed quantitative and qualitative analysis of its predictions, demonstrating that they are both plausible and highly interpretable

</details>


### [36] [$C$-$ΔΘ$: Circuit-Restricted Weight Arithmetic for Selective Refusal](https://arxiv.org/abs/2602.04521)
*Aditya Kasliwal,Pratinav Seth,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: 提出 C-Δθ：Circuit Restricted Weight Arithmetic，通过将拒绝相关计算局部化到稀疏电路并在该电路上进行受限权重更新，生成无需推理时钩子的可直接替换检查点。


<details>
  <summary>Details</summary>
Motivation: 降低推理成本与服务复杂性，提升大规模安全控制的可扩展性；实现按类别的拒绝而非在线干预。

Method: 用 EAP-IG 将拒绝因果计算局部化为稀疏电路；在该电路上更新 ΔθC（仅占总参数的约5%），生成可替换的编辑检查点。

Result: 在拒绝性与能力保留基准上实现类别定向的选择性和能力维持，成本从在线干预转为离线一次性更新。

Conclusion: 离线、Circuit-Restricted 编辑实现类别选择性拒绝，且无运行时钩子，具备可扩展性潜力。

Abstract: Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-Δθ: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update ΔθC supported only on that circuit (typically <5% of parameters). Applying ΔθC yields a drop-in edited checkpoint with no inference-time hooks, shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks.

</details>


### [37] [LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding](https://arxiv.org/abs/2602.04541)
*Gang Lin,Dongfang Li,Zhuoen Chen,Yukun Shi,Xuhui Chen,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 提出LycheeDecode，一种基于细粒度混合头注意力和HardKuma机制的高效解码方法，通过将注意力头分为少量检索头和多数稀疏头，动态识别关键 Tokens 并重复使用它们以实现对数十万上下文的加速，同时保持或提升生成质量，理论上达到在128K上下文下的2.7x加速。


<details>
  <summary>Details</summary>
Motivation: 长上下文大模型在解码阶段需维护膨胀的键值缓存，常见的跨层共享同一组关键 token 的做法过于粗粒度，忽略了注意力头的功能多样性，制约性能和可扩展性。

Method: 提出HardKuma基础的机制，将注意力头分为检索头和稀疏头：检索头动态识别关键 token，稀疏头复用检索头结果实现高效计算；采用硬Kuma (HardKuma) 的门控和选择策略，以及对头维度的细粒度控制，结合硬件友好的top-k选择实现加速。

Result: 在Llama3、Qwen3等模型上，在LongBench、RULER等长上下文理解和AIME24、OlympiadBench等复杂推理任务上，LycheeDecode获得与全注意力基线相近甚至超越的生成质量，同时在128K上下文长度下实现高达2.7x的加速。

Conclusion: 通过保留注意力头的功能多样性，细粒度混合头的策略克服了现有方法的瓶颈，为高质量高效的长上下文LLM推理提供了有效路径。

Abstract: The proliferation of long-context large language models (LLMs) exposes a key bottleneck: the rapidly expanding key-value cache during decoding, which imposes heavy memory and latency costs. While recent approaches attempt to alleviate this by sharing a single set of crucial tokens across layers, such coarse-grained sharing undermines model performance by neglecting the functional diversity of attention heads. To address this, we propose LycheeDecode, an efficient decoding method centered on a fine-grained hybrid-head attention mechanism that employs a hardware-efficient top-k selection strategy. Specifically, the novel HardKuma-based mechanism partitions attention heads into a small subset of retrieval heads that dynamically identify crucial tokens and a majority of sparse heads that reuse them for efficient computation. Through extensive experiments on leading models like Llama3 and Qwen3 across diverse benchmarks for long-context understanding (e.g., LongBench, RULER) and complex reasoning (e.g., AIME24, OlympiadBench), we demonstrate that LycheeDecode achieves generative quality comparable to, and at times surpassing even the full-attention baseline. Crucially, this is accomplished with up to a 2.7x speedup at a 128K context length. By preserving the functional diversity of attention heads, our fine-grained strategy overcomes the performance bottlenecks of existing methods, providing a powerful and validated pathway to both efficient and high-quality long-context LLM inference.

</details>


### [38] [Rethinking Weight Tying: Pseudo-Inverse Tying for Stable LM Training and Updates](https://arxiv.org/abs/2602.04556)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: 提出伪逆成型（PIT），通过共享潜在记忆实现嵌入与输出的伪逆一致接口，训练更稳定、层间语义一致性更强且副作用显著降低，且不增加词表参数。


<details>
  <summary>Details</summary>
Motivation: 权重绑定在输入嵌入与输出投影之间容易导致映射漂移，进而影响优化稳定性与后续编辑/适配的可控性，需获得稳定的一致接口。

Method: 引入PIT：使用共享潜在记忆进行耦合投影；记忆保持正交性，若有教师初始化则通过薄极分解实现，或从头随机正交初始化；在隐藏空间引入对称正定变换，用Cholesky因子参数化；输出端在投影前应用变换，嵌入端对向量应用变换的逆并通过稳定的三角求解完成，不需要对词表做伪逆重计算且不增加词表参数。

Result: 在256M–1.3B参数的设备级模型上进行预训练与适配时，PIT表现出更高的训练稳定性、更强的层间语义一致性以及显著减少副作用。

Conclusion: PIT提供了一种稳定、伪逆一致的权重绑定方案，提升训练可控性和对后续编辑/适配的鲁棒性，同时避免词表尺寸的额外参数开销。

Abstract: Weight tying is widely used in compact language models to reduce parameters by sharing the token table between the input embedding and the output projection. However, weight sharing does not guarantee a stable token interface: during training, the correspondence between encoding tokens into hidden states and decoding hidden states into logits can drift, worsening optimization sensitivity and making post-training interventions such as editing, patching, and lightweight adaptation less predictable. We propose Pseudo-Inverse Tying (PIT), which synchronizes embedding and unembedding as coupled projections of a shared latent token memory, guaranteeing a pseudo-inverse-consistent interface throughout training. PIT maintains an orthonormal shared memory, obtained by thin polar decomposition for teacher initialization or random orthonormal initialization from scratch, and introduces a fully learned symmetric positive definite hidden-space transform parameterized via a Cholesky factor. The output head applies this transform to hidden states before the vocabulary projection, while the embedding applies the inverse transform to token vectors using stable triangular solves, avoiding explicit pseudo-inverse recomputation and any vocabulary-sized auxiliary parameters. We evaluate PIT on on-device models spanning 256M-1.3B parameters across pretraining and adaptation, and consistently observe improved training stability, stronger layerwise semantic consistency, and substantially reduced side effects.

</details>


### [39] [Textual Planning with Explicit Latent Transitions](https://arxiv.org/abs/2602.04557)
*Eliezer Shlomi,Ido Levy,Eilam Shapira,Michael Katz,Guy Uziel,Segev Shlomov,Nir Mashkif,Roi Reichart,Sarah Keren*

Main category: cs.CL

TL;DR: EmbedPlan 将自回归下一状态预测替换为在冻结语言嵌入空间中的轻量转移模型；通过对状态和动作描述编码、预测下一个状态嵌入、最近邻检索实现无微调的快速规划。


<details>
  <summary>Details</summary>
Motivation: 解决以语言模型进行规划时的逐词生成带来的高延迟和重复前向计算问题，提升多步前瞻与滚动搜索的效率。

Method: 将自然语言状态/动作描述编码为向量，在嵌入空间预测下一个状态的嵌入，并通过最近邻检索得到具体状态；编码器保持冻结，无需微调；在九个经典规划领域、多种难度协议下进行评估。

Result: 插值任务近似完美，然而对于未见问题/未见域的泛化显著下降；plan-variant 表现出对替代计划的泛化能力而非仅记忆轨迹；域内可学习到转移动态，跨域迁移仍是瓶颈。

Conclusion: 冻结嵌入有助于降低延迟并实现域内高效学习与规划，但跨域迁移能力受限，需通过领域自适应、迁移学习或更强的嵌入表示来提升泛化。

Abstract: Planning with LLMs is bottlenecked by token-by-token generation and repeated full forward passes, making multi-step lookahead and rollout-based search expensive in latency and compute. We propose EmbedPlan, which replaces autoregressive next-state generation with a lightweight transition model operating in a frozen language embedding space. EmbedPlan encodes natural language state and action descriptions into vectors, predicts the next-state embedding, and retrieves the next state by nearest-neighbor similarity, enabling fast planning computation without fine-tuning the encoder. We evaluate next-state prediction across nine classical planning domains using six evaluation protocols of increasing difficulty: interpolation, plan-variant, extrapolation, multi-domain, cross-domain, and leave-one-out. Results show near-perfect interpolation performance but a sharp degradation when generalization requires transfer to unseen problems or unseen domains; plan-variant evaluation indicates generalization to alternative plans rather than memorizing seen trajectories. Overall, frozen embeddings support within-domain dynamics learning after observing a domain's transitions, while transfer across domain boundaries remains a bottleneck.

</details>


### [40] [Can LLMs capture stable human-generated sentence entropy measures?](https://arxiv.org/abs/2602.04570)
*Estrella Pivel-Villanueva,Elisabeth Frederike Sterner,Franziska Knolle*

Main category: cs.CL

TL;DR: 研究评估人类对词级预测熵的收敛性及LLMs对人类熵的近似程度；通过自举收敛分析验证样本量需求，并比较多模型和提取方法。


<details>
  <summary>Details</summary>
Motivation: 理解在心理语言学norming中需要多少人类响应才能获得稳定无偏的熵估计，以及LLMs是否能稳定地再现人类熵分布，以便选择合适的替代数据源。

Method: 使用德英两种公开cloze数据集；对熵估计随样本量的收敛性进行自举分析；在对比多种LLMs（GPT-4o、GPT2-xl、German-GPT-2、RoBERTa Base/GottBERT、LLaMA 2 7B Chat）时，采用logit-based概率提取和采样法频率估计两种方法。

Result: 超过97%的句子在给定样本量内达到熵估计稳定；德国样本在111个响应、英语在81个响应时达到稳定，低熵句子更易收敛， 高熵句子需更多样本；GPT-4o与人类数据高度一致，但与提取方法和提示设计相关；logit-based降低绝对误差，采样法更好地捕捉人类变异；LLMs可近似人类熵但不可替代稳定的人类分布。

Conclusion: 为人类norming提供实际指南，LLMs在一定程度上可作为替代，但不能完全替代；结果强调收敛性对句子可预测性的依赖，以及方法选择对结论的影响。

Abstract: Predicting upcoming words is a core mechanism of language comprehension and may be quantified using Shannon entropy. There is currently no empirical consensus on how many human responses are required to obtain stable and unbiased entropy estimates at the word level. Moreover, large language models (LLMs) are increasingly used as substitutes for human norming data, yet their ability to reproduce stable human entropy remains unclear. Here, we address both issues using two large publicly available cloze datasets in German 1 and English 2. We implemented a bootstrap-based convergence analysis that tracks how entropy estimates stabilize as a function of sample size. Across both languages, more than 97% of sentences reached stable entropy estimates within the available sample sizes. 90% of sentences converged after 111 responses in German and 81 responses in English, while low-entropy sentences (<1) required as few as 20 responses and high-entropy sentences (>2.5) substantially more. These findings provide the first direct empirical validation for common norming practices and demonstrate that convergence critically depends on sentence predictability. We then compared stable human entropy values with entropy estimates derived from several LLMs, including GPT-4o, using both logit-based probability extraction and sampling-based frequency estimation, GPT2-xl/german-GPT-2, RoBERTa Base/GottBERT, and LLaMA 2 7B Chat. GPT-4o showed the highest correspondence with human data, although alignment depended strongly on the extraction method and prompt design. Logit-based estimates minimized absolute error, whereas sampling-based estimates were better in capturing the dispersion of human variability. Together, our results establish practical guidelines for human norming and show that while LLMs can approximate human entropy, they are not interchangeable with stable human-derived distributions.

</details>


### [41] [Semantic Self-Distillation for Language Model Uncertainty](https://arxiv.org/abs/2602.04577)
*Edward Phillips,Sean Wu,Boyan Gao,David A. Clifton*

Main category: cs.CL

TL;DR: 提出 Semantic Self-Distillation (SSD)，将采样语义分布蒸馏为轻量学生模型，在提示条件下预测输出的语义分布，用熵度量不确定性并评估候选答案的可靠性；在 TriviaQA 上达到或超越基于有限样本的语义 dispersion 的效果，并对检测域外答案有较强信号。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在不确定性量化方面的挑战，特别是复杂输出空间和多样化输出带来的高成本。提出在延迟敏感场景中以轻量化模型估计提示条件下的语义分布，从而获得高效的不确定性信号。

Method: 从采样得到的语义分布中进行蒸馏，训练轻量化学生模型，使其在给定提示条件下预测一个语义分布。通过该分布的熵来触发幻觉预测的信号，并利用概率密度对候选答案进行可靠性评估。评估在 TriviaQA 上的表现。

Result: 在 TriviaQA 上，学生模型在幻觉预测方面可与或优于有限采样的语义分布，并对域外答案检测提供强信号。

Conclusion: 提出的 Semantic Self-Distillation（SSD）为在复杂输出空间中蒸馏预测不确定性提供了通用框架，理论上可扩展到语言以外的任务。

Abstract: Large language models present challenges for principled uncertainty quantification, in part due to their complexity and the diversity of their outputs. Semantic dispersion, or the variance in the meaning of sampled answers, has been proposed as a useful proxy for model uncertainty, but the associated computational cost prohibits its use in latency-critical applications. We show that sampled semantic distributions can be distilled into lightweight student models which estimate a prompt-conditioned uncertainty before the language model generates an answer token. The student model predicts a semantic distribution over possible answers; the entropy of this distribution provides an effective uncertainty signal for hallucination prediction, and the probability density allows candidate answers to be evaluated for reliability. On TriviaQA, our student models match or outperform finite-sample semantic dispersion for hallucination prediction and provide a strong signal for out-of-domain answer detection. We term this technique Semantic Self-Distillation (SSD), which we suggest provides a general framework for distilling predictive uncertainty in complex output spaces beyond language.

</details>


### [42] [Trust The Typical](https://arxiv.org/abs/2602.04581)
*Debargha Ganguly,Sreehari Sankar,Biyao Zhang,Vikash Singh,Kanan Gupta,Harshini Kavuru,Alan Luo,Weicong Chen,Warren Morningstar,Raghu Machiraju,Vipin Chaudhary*

Main category: cs.CL

TL;DR: Trust The Typical (T3) 将安全性重新定义为对输入提示的分布外检测；仅用安全英文文本训练，能在多域、多语言场景下泛化，显著降低误报并具备生产就绪的低开销。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM安全策略依赖枚举式阻断，易受对手规避且缺乏鲁棒性；需要将“安全”建模为可接受输入的分布特征，从而实现更稳健的防护。

Method: 学习可接受提示在语义空间中的分布，检测与该分布显著偏离的输入即视为潜在威胁；不需要对有害示例训练；将GPU优化版本与vLLM集成，在生成过程持续守门，开销低于6%；实现跨域与跨语言的泛化。

Result: 在18项基准上达到SOTA，覆盖毒性、仇恨言论、越狱、多语言伤害等场景并减少过度拒绝；相比专门的安全模型，误报降低多达40倍；无需再训练即可跨越14种以上语言进行迁移；具备生产就绪能力。

Conclusion: 以OOD视角重塑安全框架，证明仅用安全文本即可实现强鲁棒性与广域泛化，具备落地生产的可行性。

Abstract: Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Typical (T3), a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM, enabling continuous guardrailing during token generation with less than 6% overhead even under dense evaluation intervals on large-scale workloads.

</details>


### [43] [Beyond Holistic Scores: Automatic Trait-Based Quality Scoring of Argumentative Essays](https://arxiv.org/abs/2602.04604)
*Lucile Favero,Juan Antonio Pérez-Ortiz,Tanja Käser,Nuria Oliver*

Main category: cs.CL

TL;DR: 提出面向教育的 trait-based 自动作文评分框架：（1）用小型开源LLM进行结构化就地学习，按评分量表对五项特质提供可解释反馈；（2）用BigBird编码器配CORAL序数回归处理长文本、强调评分的有序性。在ASAP++数据集上评估，结果显示显式序数建模显著提升与人类评分的一致性，且小型LLMs在推理相关特质上具竞争力，适用于隐私保护和本地部署。


<details>
  <summary>Details</summary>
Motivation: 教育评估需可解释、与教学目标和正式 rubrics 一致的特质级反馈，而传统的整体分数难以支撑教学设计；尤其在复杂体裁如论证性写作中，需更细粒度的评估和可解释性。

Method: （1）基于结构化的、rubric对齐的 in-context prompts，对小型开源LLM进行训练/使用，包含反馈和置信度请求；（2）对BigBird编码器采用CORAL风格的序数回归，显式建模评分的有序性，以适应长序列文本评估；在 ASAP++ 数据集上进行系统评估，数据覆盖五个质量特征。

Result: 显式序数建模显著提升与人类评分的一致性，超越LLMs及名义分类/回归基线；小型开源LLM在推理导向特质上表现具有竞争力，且便于私有化、本地部署。

Conclusion: 强调将评分量表的语义与模型目标对齐的重要性，并为设计可解释、符合rubric的教育AI系统提供方法学与实践要点，尤其在论证性写作的反馈生成方面具有应用价值。

Abstract: Automated Essay Scoring systems have traditionally focused on holistic scores, limiting their pedagogical usefulness, especially in the case of complex essay genres such as argumentative writing. In educational contexts, teachers and learners require interpretable, trait-level feedback that aligns with instructional goals and established rubrics. In this paper, we study trait-based Automatic Argumentative Essay Scoring using two complementary modeling paradigms designed for realistic educational deployment: (1) structured in-context learning with small open-source LLMs, and (2) a supervised, encoder-based BigBird model with a CORAL-style ordinal regression formulation, optimized for long-sequence understanding. We conduct a systematic evaluation on the ASAP++ dataset, which includes essay scores across five quality traits, offering strong coverage of core argumentation dimensions. LLMs are prompted with designed, rubric-aligned in-context examples, along with feedback and confidence requests, while we explicitly model ordinality in scores with the BigBird model via the rank-consistent CORAL framework. Our results show that explicitly modeling score ordinality substantially improves agreement with human raters across all traits, outperforming LLMs and nominal classification and regression-based baselines. This finding reinforces the importance of aligning model objectives with rubric semantics for educational assessment. At the same time, small open-source LLMs achieve a competitive performance without task-specific fine-tuning, particularly for reasoning-oriented traits, while enabling transparent, privacy-preserving, and locally deployable assessment scenarios. Our findings provide methodological, modeling, and practical insights for the design of AI-based educational systems that aim to deliver interpretable, rubric-aligned feedback for argumentative writing.

</details>


### [44] [RexBERT: Context Specialized Bidirectional Encoders for E-commerce](https://arxiv.org/abs/2602.04605)
*Rahul Bajaj,Anuj Garg*

Main category: cs.CL

TL;DR: 提出 RexBERT：为电商语义设计的轻量高效 BERT 风格编码器，通过在 Ecom-niverse 数据集上的分阶预训练实现对电商任务的高效适配，在参数量显著下降的情形下超越通用编码器。


<details>
  <summary>Details</summary>
Motivation: 通用编码器在电商领域覆盖有限，且对低时延、稳定性和成本要求高的检索/分类/排序场景需要更域内化的表征。

Method: 构建 3500 亿 token 的 Ecom-niverse 电商语料；提出基于 ModernBERT 的三阶段预训练：通用预训练、上下文扩展、 annealed domain specialization；训练 RexBERT 家族（17M–400M 参数），并在电商数据集上执行标注分词分类、语义相似度、以及通用 NLP 任务评估。

Result: 尽管参数量减少 2–3 倍，RexBERT 在域内任务上优于更大通用编码器，且在长上下文模型上与之媲美；域内数据+系统化训练路线优于纯粹扩展规模。

Conclusion: 高质量的域内数据与有原则的预训练策略，是实现电商场景高效且稳定的编码器的关键；RexBERT 为电商应用提供了一个高性价比的替代方案。

Abstract: Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.

</details>


### [45] [Disentangling meaning from language in LLM-based machine translation](https://arxiv.org/abs/2602.04613)
*Théo Lasnier,Armel Zebaze,Djamé Seddah,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 通过分析大模型的注意力头，从机制性可解释性角度研究句子级机器翻译。发现两大子任务（目标语言识别与句子等价性）由不同、稀疏的注意力头集合专门化。基于此构建子任务专用的引导向量，修改仅约1%的相关头即可实现无指令提示的翻译性能，且选择性消融对应头部会干扰相应的翻译功能。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的尺度下，前人对机制性可解释性的MT研究多停留在词级分析，本文尝试从句子级角度揭示模型内部如何实现翻译，并寻找头部的功能分工。

Method: 对三类开源模型家族和20种翻译方向进行注意力头分析，识别出对两个子任务（目标语言生成与保持输入含义）专门化的稀疏头集；构建子任务专用的引导向量；通过修改仅1%相关头实现无指令提示的MT，并进行头部消融以验证功能对应关系。

Result: 发现每个子任务由一组稀疏、专门化的注意力头负责；引导向量能使模型在无需指令提示的情况下执行MT，达到与有提示的提示式MT相当的水平；消融相应头部会显著削弱对应的翻译功能。

Conclusion: 证明句子级MT的机制性可解释性成立，揭示了翻译功能的模块化分工，并展示通过对少量头部的干预即可实现高效、无提示的翻译控制。

Abstract: Mechanistic Interpretability (MI) seeks to explain how neural networks implement their capabilities, but the scale of Large Language Models (LLMs) has limited prior MI work in Machine Translation (MT) to word-level analyses. We study sentence-level MT from a mechanistic perspective by analyzing attention heads to understand how LLMs internally encode and distribute translation functions. We decompose MT into two subtasks: producing text in the target language (i.e. target language identification) and preserving the input sentence's meaning (i.e. sentence equivalence). Across three families of open-source models and 20 translation directions, we find that distinct, sparse sets of attention heads specialize in each subtask. Based on this insight, we construct subtask-specific steering vectors and show that modifying just 1% of the relevant heads enables instruction-free MT performance comparable to instruction-based prompting, while ablating these heads selectively disrupts their corresponding translation functions.

</details>


### [46] [LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation](https://arxiv.org/abs/2602.04617)
*Ruixiao Yang,Yuanhe Tian,Xu Yang,Huiqi Li,Yan Song*

Main category: cs.CL

TL;DR: LEAD通过逐层专家对齐的解码策略，在大语言模型的每个解码层引入多专家模块并通过门控机制将专家特征融入，动态纠正解码偏差以提升放射科报告生成的临床准确性并降低幻觉。


<details>
  <summary>Details</summary>
Motivation: 当前诊断报 Nature? Radiology Report Generation 中，LVLM易产生幻觉且对齐不足；现有方法多依赖外部知识提示，受限于外部引导的鲁棒性和对齐偏差的可控性。需挖掘预训练模型固有的解码先验与视觉-语言对齐偏差，以提升稳健性与事实一致性。

Method: 提出LEAD：一个层级化架构，设有多专家模块以提取不同病理特征；在每个解码层通过一个学得的门控函数将专家特征注入，允许解码器在推理过程中的每一步咨询专家信息，动态纠正偏差。

Result: 在多个公开数据集上的实验表明，LEAD显著提升临床准确性指标并缓解幻觉，同时保持较高的文本生成质量。

Conclusion: LEAD通过在解码阶段引入层级化专家对齐，有效降低幻觉并提升事实性的一致性，展现出对RRG任务的潜在改进与鲁棒性提升。

Abstract: Radiology Report Generation (RRG) aims to produce accurate and coherent diagnostics from medical images. Although large vision language models (LVLM) improve report fluency and accuracy, they exhibit hallucinations, generating plausible yet image-ungrounded pathological details. Existing methods primarily rely on external knowledge guidance to facilitate the alignment between generated text and visual information. However, these approaches often ignore the inherent decoding priors and vision-language alignment biases in pretrained models and lack robustness due to reliance on constructed guidance. In this paper, we propose Layer-wise Expert-aligned Decoding (LEAD), a novel method to inherently modify the LVLM decoding trajectory. A multiple experts module is designed for extracting distinct pathological features which are integrated into each decoder layer via a gating mechanism. This layer-wise architecture enables the LLM to consult expert features at every inference step via a learned gating function, thereby dynamically rectifying decoding biases and steering the generation toward factual consistency. Experiments conducted on multiple public datasets demonstrate that the LEAD method yields effective improvements in clinical accuracy metrics and mitigates hallucinations while preserving high generation quality.

</details>


### [47] [Mapping the Web of Science, a large-scale graph and text-based dataset with LLM embeddings](https://arxiv.org/abs/2602.04630)
*Tim Kunt,Annika Buchholz,Imene Khebouri,Thorsten Koch,Ida Litzel,Thi Huong Vu*

Main category: cs.CL

TL;DR: 提出一种基于大语言模型嵌入的文本语义表征方法，结合文本的图结构特征，对大规模文本数据进行分析。以 Web of Science 约 5600 万篇论文为数据集，验证嵌入分析的可行性，并揭示文本的自结构化景观。


<details>
  <summary>Details</summary>
Motivation: 探究文本语义嵌入在大规模文本分析中的潜力，以及将其与文本之间的关系图结合以提升对文本语义与结构的理解。

Method: 将大语言模型生成的文本嵌入用于表征语义信息，并结合文本间关系（如链接、引用、共享属性）的图结构分析；在 Web of Science 数据集上进行嵌入分析以探索文本的自结构化特征。

Result: 通过嵌入分析揭示文本在大规模数据集中的自组织结构，显示语义嵌入能揭示隐藏的文本群组和关系模式，验证了该方法的可行性与潜力。

Conclusion: 文本语义嵌入为大规模文本分析提供新视角，尤其在结合图结构特征时，能够揭示文本的自结构化特征，并为检索、分类与预测等任务带来新方向。

Abstract: Large text data sets, such as publications, websites, and other text-based media, inherit two distinct types of features: (1) the text itself, its information conveyed through semantics, and (2) its relationship to other texts through links, references, or shared attributes. While the latter can be described as a graph structure and can be handled by a range of established algorithms for classification and prediction, the former has recently gained new potential through the use of LLM embedding models. Demonstrating these possibilities and their practicability, we investigate the Web of Science dataset, containing ~56 million scientific publications through the lens of our proposed embedding method, revealing a self-structured landscape of texts.

</details>


### [48] [Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models](https://arxiv.org/abs/2602.04649)
*Binghai Wang,Yantao Liu,Yuxuan Liu,Tianyi Tang,Shenzhi Wang,Chang Gao,Chujie Zheng,Yichang Zhang,Le Yu,Shixuan Liu,Tao Gui,Qi Zhang,Xuanjing Huang,Bowen Yu,Fei Huang,Junyang Lin*

Main category: cs.CL

TL;DR: 提出Rationale Consistency并与Outcome Accuracy混合信号，解决欺骗性对齐，提升GenRM/RLHF性能并获RM-Bench/JudgeBench新高分。


<details>
  <summary>Details</summary>
Motivation: 解决GenRM与LLM-as-judge在RLHF训练中对结果准确性过度追求而导致的欺骗性对齐和泛化能力不足的问题。

Method: 定义Rationale Consistency度量，量化模型推理与人类判断之间的一致性；在GenRM训练中引入混合信号，将Rationale Consistency与Outcome Accuracy结合；在RLHF阶段使用RM以增强训练信号。

Result: 在RM-Bench达到87.1%、JudgeBench达到82%；相对于只依赖Outcome的基线平均提升约5%；在Arena Hard v2上，使用RM的训练在创造性写作任务中提升约7%；分析表明方法有效缓解欺骗性对齐，逆转Outcome-only训练下理性推理一致性的下降。

Conclusion: 引入Rationale Consistency并与Outcome Accuracy混合信号，可缓解欺骗性对齐，提升GenRM与RLHF的鲁棒性与泛化能力，达到SOTA并提升多项评测。

Abstract: Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.

</details>


### [49] [Approaches to Semantic Textual Similarity in Slovak Language: From Algorithms to Transformers](https://arxiv.org/abs/2602.04659)
*Lukas Radosky,Miroslav Blstak,Matej Krajcovic,Ivan Polasek*

Main category: cs.CL

TL;DR: Compared multiple STS approaches for Slovak: traditional algorithms, ML models using traditional algorithm outputs as features with artificial bee colony optimization for feature selection and hyperparameter tuning, and third-party tools (CloudNLP fine-tuned, OpenAI embeddings, GPT-4, SlovakBERT). Highlights trade-offs among approaches.


<details>
  <summary>Details</summary>
Motivation: STS is well-studied in high-resource languages but remains challenging for Slovak. The paper aims to systematically compare traditional, ML-based, and external tools to identify practical trade-offs for under-resourced Slovak.

Method: 1) Use outputs from traditional STS algorithms as features to train several supervised ML models. 2) Apply feature selection and hyperparameter tuning guided by artificial bee colony optimization. 3) Evaluate several third-party tools: a CloudNLP fine-tuned model, OpenAI embedding models, GPT-4, and a pretrained SlovakBERT model.

Result: The study reports notable trade-offs among the evaluated approaches in terms of accuracy, data requirements, and computational resources. No single method dominates; each category offers specific advantages depending on constraints.

Conclusion: A comprehensive evaluation reveals that for Slovak STS there is no universally best approach. Traditional+ML with swarm-guided feature selection can achieve competitive performance with transparent features, while third-party tools provide strong performance with higher resource costs. Practitioners should balance accuracy, data availability, and latency when selecting methods.

Abstract: Semantic textual similarity (STS) plays a crucial role in many natural language processing tasks. While extensively studied in high-resource languages, STS remains challenging for under-resourced languages such as Slovak. This paper presents a comparative evaluation of sentence-level STS methods applied to Slovak, including traditional algorithms, supervised machine learning models, and third-party deep learning tools. We trained several machine learning models using outputs from traditional algorithms as features, with feature selection and hyperparameter tuning jointly guided by artificial bee colony optimization. Finally, we evaluated several third-party tools, including fine-tuned model by CloudNLP, OpenAI's embedding models, GPT-4 model, and pretrained SlovakBERT model. Our findings highlight the trade-offs between different approaches.

</details>


### [50] [Investigating Disability Representations in Text-to-Image Models](https://arxiv.org/abs/2602.04687)
*Yang Yian,Yu Fan,Liudmila Zavolokina,Sarah Ebling*

Main category: cs.CL

TL;DR: 通过对 Stable Diffusion XL 与 DALL-E 3 的结构化提示分析，比较通用残疾提示与具体残疾类别提示的图像相似性，并评估缓解策略对残疾描绘的影响，结合情感极性分析的自动与人工评估，揭示残疾表征的持续不平衡并强调需要持续改进。


<details>
  <summary>Details</summary>
Motivation: 填补对 AI 生成图像中残疾人群表征的研究空白，关注性别与种族之外的残疾 representation，推动更包容、多样的生成模型。

Method: 采用结构化提示设计，比较通用残疾提示与具体残疾类别提示的图像相似性，评估缓解策略对残疾描绘的影响；通过情感极性分析（ sentiment polarity）结合自动与人工评估来衡量情感框架。模型为 Stable Diffusion XL 与 DALL-E 3。

Result: 研究发现残疾表征存在持续的不平衡；不同缓解策略对表征的影响存在差异，情感取向分析（自动与人工评估）揭示情感框架需改进以避免负性偏见。

Conclusion: 强调需要对生成模型进行持续评估与改进，以实现对残疾表征的更大多样性和包容性。

Abstract: Text-to-image generative models have made remarkable progress in producing high-quality visual content from textual descriptions, yet concerns remain about how they represent social groups. While characteristics like gender and race have received increasing attention, disability representations remain underexplored. This study investigates how people with disabilities are represented in AI-generated images by analyzing outputs from Stable Diffusion XL and DALL-E 3 using a structured prompt design. We analyze disability representations by comparing image similarities between generic disability prompts and prompts referring to specific disability categories. Moreover, we evaluate how mitigation strategies influence disability portrayals, with a focus on assessing affective framing through sentiment polarity analysis, combining both automatic and human evaluation. Our findings reveal persistent representational imbalances and highlight the need for continuous evaluation and refinement of generative models to foster more diverse and inclusive portrayals of disability.

</details>


### [51] [LinGO: A Linguistic Graph Optimization Framework with LLMs for Interpreting Intents of Online Uncivil Discourse](https://arxiv.org/abs/2602.04693)
*Yuan Zhang,Thales Bertaglia*

Main category: cs.CL

TL;DR: LinGO: 将语言分解为多步骤语言学成分并对关键步骤进行针对性优化，以提升大语言模型在多类政治 incivility 意图检测中的性能。相比零样本、阶梯式推理、直接优化和微调基线，LinGO 在四种 incivility 形式及六种 civil/uncivil intent 上显示出稳定提升；RAG 技术在与 Gemini 模型配对时达到最佳综合表现。


<details>
  <summary>Details</summary>
Motivation: 现有分类器倾向把带有文明意图的带有“粗鲁信号”的帖子错误分类为不文明，导致对有害 incivility 的估计偏高；需利用语言结构和多步推理来区分直接/间接表达的复杂语义以提升多类意图检测的准确性。

Method: LinGO 将语言分解为多步语言成分，识别导致错误的关键步骤，并对这些目标步骤的提示和示例进行迭代优化。评估在2022年巴西总统大选数据集上，覆盖四种政治 incivility（IMP、HSST、PHAVPR、THREAT），每个实例标注六种 civil/uncivil intent。以三种成本更高效的 LLM（GPT-5-mini、Gemini 2.5 Flash-Lite、Claude 3 Haiku）和四种优化技术（TextGrad、AdalFlow、DSPy、RAG）进行对比。

Result: 在所有模型上，LinGO 相较于零样本、链式思维、直接优化和微调等基线，显著提高了准确率和加权 F1；RAG 为最强优化策略，与 Gemini 配对时实现最佳整体性能。多步骤语言成分的引入与对目标步骤的优化有助于模型解释复杂语义，具备迁移到其他复杂语义解释任务的潜力。

Conclusion: 将多步语言学组件融入 LLM 指令并对目标组件进行优化，能提升对复杂语义的解释能力及多类意图检测的表现，未来可推广至更多复杂语义解释任务。

Abstract: Detecting uncivil language is crucial for maintaining safe, inclusive, and democratic online spaces. Yet existing classifiers often misinterpret posts containing uncivil cues but expressing civil intents, leading to inflated estimates of harmful incivility online. We introduce LinGO, a linguistic graph optimization framework for large language models (LLMs) that leverages linguistic structures and optimization techniques to classify multi-class intents of incivility that use various direct and indirect expressions. LinGO decomposes language into multi-step linguistic components, identifies targeted steps that cause the most errors, and iteratively optimizes prompt and/or example components for targeted steps. We evaluate it using a dataset collected during the 2022 Brazilian presidential election, encompassing four forms of political incivility: Impoliteness (IMP), Hate Speech and Stereotyping (HSST), Physical Harm and Violent Political Rhetoric (PHAVPR), and Threats to Democratic Institutions and Values (THREAT). Each instance is annotated with six types of civil/uncivil intent. We benchmark LinGO using three cost-efficient LLMs: GPT-5-mini, Gemini 2.5 Flash-Lite, and Claude 3 Haiku, and four optimization techniques: TextGrad, AdalFlow, DSPy, and Retrieval-Augmented Generation (RAG). The results show that, across all models, LinGO consistently improves accuracy and weighted F1 compared with zero-shot, chain-of-thought, direct optimization, and fine-tuning baselines. RAG is the strongest optimization technique and, when paired with Gemini model, achieves the best overall performance. These findings demonstrate that incorporating multi-step linguistic components into LLM instructions and optimize targeted components can help the models explain complex semantic meanings, which can be extended to other complex semantic explanation tasks in the future.

</details>


### [52] [ERNIE 5.0 Technical Report](https://arxiv.org/abs/2602.04705)
*Haifeng Wang,Hua Wu,Tian Wu,Yu Sun,Jing Liu,Dianhai Yu,Yanjun Ma,Jingzhou He,Zhongjun He,Dou Hong,Qiwen Liu,Shuohuan Wang,Junyuan Shang,Zhenyu Zhang,Yuchen Ding,Jinle Zeng,Jiabin Yang,Liang Shen,Ruibiao Chen,Weichong Yin,Siyu Ding,Dai Dai,Shikun Feng,Siqi Bao,Bolei He,Yan Chen,Zhenyu Jiao,Ruiqing Zhang,Zeyu Chen,Qingqing Dang,Kaipeng Deng,Jiajun Jiang,Enlei Gong,Guoxia Wang,Yanlin Sha,Yi Liu,Yehan Zheng,Weijian Xu,Jiaxiang Liu,Zengfeng Zeng,Yingqi Qu,Zhongli Li,Zhengkun Zhang,Xiyang Wang,Zixiang Xu,Xinchao Xu,Zhengjie Huang,Dong Wang,Bingjin Chen,Yue Chang,Xing Yuan,Shiwei Huang,Qiao Zhao,Xinzhe Ding,Shuangshuang Qiao,Baoshan Yang,Bihong Tang,Bin Li,Bingquan Wang,Binhan Tang,Binxiong Zheng,Bo Cui,Bo Ke,Bo Zhang,Bowen Zhang,Boyan Zhang,Boyang Liu,Caiji Zhang,Can Li,Chang Xu,Chao Pang,Chao Zhang,Chaoyi Yuan,Chen Chen,Cheng Cui,Chenlin Yin,Chun Gan,Chunguang Chai,Chuyu Fang,Cuiyun Han,Dan Zhang,Danlei Feng,Danxiang Zhu,Dong Sun,Dongbo Li,Dongdong Li,Dongdong Liu,Dongxue Liu,Fan Ding,Fan Hu,Fan Li,Fan Mo,Feisheng Wu,Fengwei Liu,Gangqiang Hu,Gaofeng Lu,Gaopeng Yong,Gexiao Tian,Guan Wang,Guangchen Ni,Guangshuo Wu,Guanzhong Wang,Guihua Liu,Guishun Li,Haibin Li,Haijian Liang,Haipeng Ming,Haisu Wang,Haiyang Lu,Haiye Lin,Han Zhou,Hangting Lou,Hanwen Du,Hanzhi Zhang,Hao Chen,Hao Du,Hao Liu,Hao Zhou,Haochen Jiang,Haodong Tian,Haoshuang Wang,Haozhe Geng,Heju Yin,Hong Chen,Hongchen Xue,Hongen Liu,Honggeng Zhang,Hongji Xu,Hongwei Chen,Hongyang Zhang,Hongyuan Zhang,Hua Lu,Huan Chen,Huan Wang,Huang He,Hui Liu,Hui Zhong,Huibin Ruan,Jiafeng Lu,Jiage Liang,Jiahao Hu,Jiahao Hu,Jiajie Yang,Jialin Li,Jian Chen,Jian Wu,Jianfeng Yang,Jianguang Jiang,Jianhua Wang,Jianye Chen,Jiaodi Liu,Jiarui Zhou,Jiawei Lv,Jiaxin Zhou,Jiaxuan Liu,Jie Han,Jie Sun,Jiefan Fang,Jihan Liu,Jihua Liu,Jing Hu,Jing Qian,Jing Yan,Jingdong Du,Jingdong Wang,Jingjing Wu,Jingyong Li,Jinheng Wang,Jinjin Li,Jinliang Lu,Jinlin Yu,Jinnan Liu,Jixiang Feng,Jiyi Huang,Jiyuan Zhang,Jun Liang,Jun Xia,Jun Yu,Junda Chen,Junhao Feng,Junhong Xiang,Junliang Li,Kai Liu,Kailun Chen,Kairan Su,Kang Hu,Kangkang Zhou,Ke Chen,Ke Wei,Kui Huang,Kun Wu,Kunbin Chen,Lei Han,Lei Sun,Lei Wen,Linghui Meng,Linhao Yu,Liping Ouyang,Liwen Zhang,Longbin Ji,Longzhi Wang,Meng Sun,Meng Tian,Mengfei Li,Mengqi Zeng,Mengyu Zhang,Ming Hong,Mingcheng Zhou,Mingming Huang,Mingxin Chen,Mingzhu Cai,Naibin Gu,Nemin Qiu,Nian Wang,Peng Qiu,Peng Zhao,Pengyu Zou,Qi Wang,Qi Xin,Qian Wang,Qiang Zhu,Qianhui Luo,Qianwei Yang,Qianyue He,Qifei Wu,Qinrui Li,Qiwen Bao,Quan Zhang,Quanxiang Liu,Qunyi Xie,Rongrui Zhan,Rufeng Dai,Rui Peng,Ruian Liu,Ruihao Xu,Ruijie Wang,Ruixi Zhang,Ruixuan Liu,Runsheng Shi,Ruting Wang,Senbo Kang,Shan Lu,Shaofei Yu,Shaotian Gong,Shenwei Hu,Shifeng Zheng,Shihao Guo,Shilong Fan,Shiqin Liu,Shiwei Gu,Shixi Zhang,Shuai Yao,Shuang Zhang,Shuangqiao Liu,Shuhao Liang,Shuwei He,Shuwen Yang,Sijun He,Siming Dai,Siming Wu,Siyi Long,Songhe Deng,Suhui Dong,Suyin Liang,Teng Hu,Tianchan Xu,Tianliang Lv,Tianmeng Yang,Tianyi Wei,Tiezhu Gao,Ting Sun,Ting Zhang,Tingdan Luo,Wei He,Wei Luan,Wei Yin,Wei Zhang,Wei Zhou,Weibao Gong,Weibin Li,Weicheng Huang,Weichong Dang,Weiguo Zhu,Weilong Zhang,Weiqi Tan,Wen Huang,Wenbin Chang,Wenjing Du,Wenlong Miao,Wenpei Luo,Wenquan Wu,Xi Shi,Xi Zhao,Xiang Gao,Xiangguo Zhang,Xiangrui Yu,Xiangsen Wang,Xiangzhe Wang,Xianlong Luo,Xianying Ma,Xiao Tan,Xiaocong Lin,Xiaofei Wang,Xiaofeng Peng,Xiaofeng Wu,Xiaojian Xu,Xiaolan Yuan,Xiaopeng Cui,Xiaotian Han,Xiaoxiong Liu,Xiaoxu Fei,Xiaoxuan Wu,Xiaoyu Wang,Xiaoyu Zhang,Xin Sun,Xin Wang,Xinhui Huang,Xinming Zhu,Xintong Yu,Xinyi Xu,Xinyu Wang,Xiuxian Li,XuanShi Zhu,Xue Xu,Xueying Lv,Xuhong Li,Xulong Wei,Xuyi Chen,Yabing Shi,Yafeng Wang,Yamei Li,Yan Liu,Yanfu Cheng,Yang Gao,Yang Liang,Yang Wang,Yang Wang,Yang Yang,Yanlong Liu,Yannian Fu,Yanpeng Wang,Yanzheng Lin,Yao Chen,Yaozong Shen,Yaqian Han,Yehua Yang,Yekun Chai,Yesong Wang,Yi Song,Yichen Zhang,Yifei Wang,Yifeng Guo,Yifeng Kou,Yilong Chen,Yilong Guo,Yiming Wang,Ying Chen,Ying Wang,Yingsheng Wu,Yingzhan Lin,Yinqi Yang,Yiran Xing,Yishu Lei,Yixiang Tu,Yiyan Chen,Yong Zhang,Yonghua Li,Yongqiang Ma,Yongxing Dai,Yongyue Zhang,Yu Ran,Yu Sun,Yu-Wen Michael Zhang,Yuang Liu,Yuanle Liu,Yuanyuan Zhou,Yubo Zhang,Yuchen Han,Yucheng Wang,Yude Gao,Yuedong Luo,Yuehu Dong,Yufeng Hu,Yuhui Cao,Yuhui Yun,Yukun Chen,Yukun Gao,Yukun Li,Yumeng Zhang,Yun Fan,Yun Ma,Yunfei Zhang,Yunshen Xie,Yuping Xu,Yuqin Zhang,Yuqing Liu,Yurui Li,Yuwen Wang,Yuxiang Lu,Zefeng Cai,Zelin Zhao,Zelun Zhang,Zenan Lin,Zezhao Dong,Zhaowu Pan,Zhaoyu Liu,Zhe Dong,Zhe Zhang,Zhen Zhang,Zhengfan Wu,Zhengrui Wei,Zhengsheng Ning,Zhenxing Li,Zhenyu Li,Zhenyu Qian,Zhenyun Li,Zhi Li,Zhichao Chen,Zhicheng Dong,Zhida Feng,Zhifan Feng,Zhihao Deng,Zhijin Yu,Zhiyang Chen,Zhonghui Zheng,Zhuangzhuang Guo,Zhujun Zhang,Zhuo Sun,Zichang Liu,Zihan Lin,Zihao Huang,Zihe Zhu,Ziheng Zhao,Ziping Chen,Zixuan Zhu,Ziyang Xu,Ziyi Liang,Ziyuan Gao*

Main category: cs.CL

TL;DR: ERNIE 5.0 是一个原生自回归的统一多模态基础模型，覆盖文本、图像、视频与音频，采用超稀疏混合专家（MoE）和统一的下一个组别令牌预测目标，在单一预训练中产生具有不同深度与容量的子模型，并具弹性训练以适应不同资源约束，声称实现万亿参数级别的多模态理解与生成。


<details>
  <summary>Details</summary>
Motivation: 面向统一的多模态理解与生成任务，需要一个能够在大规模、跨模态数据下保持稳定性、可部署性的基础模型。现有方法在资源约束、推理延迟和扩展性方面存在挑战，因此提出一个可在同一次训练中产生多种子模型、并通过弹性训练与高效路由实现可控部署。

Method: 提出原生自回归框架，统一的“下一个组令牌”预测目标覆盖文本、图像、视频、音频等多模态，使用超稀疏 MoE 架构与模态无关的专家路由实现大规模参数化；在训练阶段引入弹性训练，使同一次训练产出深度、容量和路由稀疏度不同的子模型，以在内存与时间约束下实现性能与推理效率的权衡；对强化学习在统一基础模型中的扩展与收敛性进行系统性研究，确保在超稀疏 MoE 与多模态场景下的稳定后训练；进行广泛实验，展示在多模态任务上的强均衡表现，以及公开披露模型中首个万亿参数统一自回归模型的实现，并提供模态无关路由的可视化与弹性训练的经验分析。

Result: 实验表明 ERNIE 5.0 在多模态理解与生成任务中实现强且均衡的性能；声称达到公开披露的万亿参数级别的统一自回归模型，支持文本、图像、视频和音频的理解与生成；并提供模态无关的专家路由可视化与对弹性训练、扩展性等方面的详细经验分析。

Conclusion: 该工作首次在公开信息中提出生产级别的万亿参数、统一自回归多模态模型的实现，强调弹性训练与 ultra-sparse MoE 路线在资源受限场景下的实用性，同时通过可视化和系统性分析为社群提供洞见。

Abstract: In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.

</details>


### [53] [LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers](https://arxiv.org/abs/2602.04706)
*Yike Sun,Haotong Yang,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: BPE 词法的中间合并残留（residue tokens）会浪费词汇容量并增添对异常输入的脆弱性，提出 LiteToken 删除残留词，减少碎片化和参数量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 探究常用分词器在合并学习阶段产生的中间留存 token 的性质及影响，改进分词器鲁棒性和效率。

Method: 对多种常用 BPE 分词器进行系统性经验表征，定义并量化残留 token；提出 LiteToken 方法在无需额外微调的前提下移除残留 token；在预训练模型的权重不需要微调的前提下，评估鲁棒性、碎片化、参数数量等指标。

Result: LiteToken 能显著减少 token 碎片化和参数数量，提升对噪声或拼写错误输入的鲁棒性，同时保持总体性能。大多数预训练模型可在不额外微调的情况下接受改动。

Conclusion: 删除残留 token 是一个简单且有效的改进，可提升分词器效率和鲁棒性，适用于现有模型的轻量化改进。未来工作可进一步系统化评估在不同语言、不同模型规模上的通用性，并研究对模型内嵌表示的影响。

Abstract: Tokenization is fundamental to how language models represent and process text, yet the behavior of widely used BPE tokenizers has received far less study than model architectures and training. In this paper, we investigate intermediate merge residues in BPE vocabularies: tokens that are frequent during merge learning so that retained in the final vocabulary, but are mostly further merged and rarely emitted when tokenizing the corpus during tokenizer usage. Such low-frequency tokens not only waste vocabulary capacity but also increase vulnerability to adversarial or atypical inputs. We present a systematic empirical characterization of this phenomenon across commonly used tokenizers and introduce LiteToken, a simple method for removing residue tokens. Because the affected tokens are rarely used, pretrained models can often accommodate the modified tokenizer without additional fine-tuning. Experiments show that LiteToken reduces token fragmentation, reduces parameters, and improves robustness to noisy or misspelled inputs, while preserving overall performance.

</details>


### [54] [Linguistically Informed Evaluation of Multilingual ASR for African Languages](https://arxiv.org/abs/2602.04716)
*Fei-Yueh Chen,Lateef Adeleke,C. M. Downey*

Main category: cs.CL

TL;DR: FER and TER provide linguistically meaningful error analysis for African languages beyond WER/CER; across Yoruba and Uneme, feature-level errors reveal weaknesses hidden by all-or-nothing metrics; mid/downstep tones are particularly challenging; results suggest segmental performance higher than tonal accuracy, indicating the need for feature-aware modeling and evaluation.


<details>
  <summary>Details</summary>
Motivation: WER conflates phonological, tonal, and other linguistic errors into a single lexical metric, obscuring linguistically meaningful error patterns in low-resource/tonal languages; there is a need for metrics like FER and TER that operate on phonological features to diagnose model weaknesses and guide improvements, especially for endangered languages without pretraining data.

Method: Evaluate three speech encoders on two African languages (Yoruba and Uneme) using WER, CER, FER, and a tone-aware TER; analyze errors at phonological/tonal feature level, compare segmental vs tonal features, and report language-specific results including data from a language absent from pretraining.

Result: FER and TER reveal linguistically salient error patterns not visible in WER/CER. Yoruba: WER=0.788, CER=0.305, FER=0.151. Uneme (pretraining data absent): WER near total, CER=0.461, FER=0.267. Tones, especially mid and downstep, remain challenging; errors largely at the feature level rather than word-level; models perform better on segmental features than tonal features.

Conclusion: Phonological-feature-based metrics (FER/TER) provide finer-grained diagnostics of ASR errors in African languages and can better guide model development, particularly for tonal and underrepresented languages where WER masks per-feature failures.

Abstract: Word Error Rate (WER) mischaracterizes ASR models' performance for African languages by combining phonological, tone, and other linguistic errors into a single lexical error. By contrast, Feature Error Rate (FER) has recently attracted attention as a viable metric that reveals linguistically meaningful errors in models' performance. In this paper, we evaluate three speech encoders on two African languages by complementing WER with CER, and FER, and add a tone-aware extension (TER). We show that by computing errors on phonological features, FER and TER reveal linguistically-salient error patterns even when word-level accuracy remains low. Our results reveal that models perform better on segmental features, while tones (especially mid and downstep) remain the most challenging features. Results on Yoruba show a striking differential in metrics, with WER=0.788, CER=0.305, and FER=0.151. Similarly for Uneme (an endangered language absent from pretraining data) a model with near-total WER and 0.461 CER achieves the relatively low FER of 0.267. This indicates model error is often attributable to individual phonetic feature errors, which is obscured by all-or-nothing metrics like WER.

</details>


### [55] ["Be My Cheese?": Cultural Nuance Benchmarking for Machine Translation in Multilingual LLMs](https://arxiv.org/abs/2602.04729)
*Madison Van Doren,Casey Ford,Jennifer Barajas,Cory Holland*

Main category: cs.CL

TL;DR: 提出一个面向文化本地化的多语言MT人类评估基准，基于87条翻译、20语言、7模型、5名母语评审，从全篇与分段两层次评估文化隐喻、习语等。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译基准多聚焦语法和词汇准确性，往往忽视实际本地化所需的跨文化语用能力与文化共鸣。

Method: 在先导研究的基础上，开展大规模的人工评估基准：对87条翻译、覆盖20语言的文本进行评估，使用7种多语言大模型，在15种目标语言中由5位母语评审进行评价。评审对全文翻译与分段的文化性语言（成语、双关、节日及文化嵌入概念）按0-3等级打分，分段还可选择NA表示未翻译。

Result: 总体质量水平中等（平均1.68/3）。GPT-5(2.10/3)、Claude Sonnet 3.7(1.97/3)、Mistral Medium 3.1(1.84/3)构成较强梯队。分段评估显示节日（2.20/3）和文化概念（2.19/3）翻译效果明显优于成语（1.65/3）与双关（1.45/3），成语最易未翻译。显示了语法等价与文化共鸣之间的持续鸿沟。

Conclusion: 这是首个聚焦语言间文化细微差异的多语言人类标注基准，凸显需要更具文化感知的训练数据、改进的跨语言语用推理，以及更贴近真实沟通能力的评价范式。

Abstract: We present a large-scale human evaluation benchmark for assessing cultural localisation in machine translation produced by state-of-the-art multilingual large language models (LLMs). Existing MT benchmarks emphasise token-level and grammatical accuracy, but of ten overlook pragmatic and culturally grounded competencies required for real-world localisation. Building on a pilot study of 87 translations across 20 languages, we evaluate 7 multilingual LLMs across 15 target languages with 5 native-speaker raters per language. Raters scored both full-text translations and segment-level instances of culturally nuanced language (idioms, puns, holidays, and culturally embedded concepts) on an ordinal 0-3 quality scale; segment ratings additionally included an NA option for untranslated segments.
  Across full-text evaluations, mean overall quality is modest (1.68/3): GPT-5 (2.10/3), Claude Sonnet 3.7 (1.97/3), and Mistral Medium 3.1 (1.84/3) form the strongest tier with fewer catastrophic failures. Segment-level results show sharp category effects: holidays (2.20/3) and cultural concepts (2.19/3) translate substantially better than idioms (1.65/3) and puns (1.45/3), and idioms are most likely to be left untranslated. These findings demonstrate a persistent gap between grammatical adequacy and cultural resonance. To our knowledge, this is the first multilingual, human-annotated benchmark focused explicitly on cultural nuance in translation and localisation, highlighting the need for culturally informed training data, improved cross-lingual pragmatics, and evaluation paradigms that better reflect real-world communicative competence.

</details>


### [56] [Less Finetuning, Better Retrieval: Rethinking LLM Adaptation for Biomedical Retrievers via Synthetic Data and Model Merging](https://arxiv.org/abs/2602.04731)
*Sameh Khattab,Jean-Philippe Corbeil,Osman Alperen Koraş,Amin Dada,Julian Friedrich,François Beaulieu,Paul Vozila,Jens Kleesiek*

Main category: cs.CL

TL;DR: STM 将通用解码器型大模型转化为高性能、领域专属的检索模型的可扩展框架，结合合成难负样本、检索提示优化与模型合并，在多任务（医学与通用）方面显著提升性能，无需大量再训练。


<details>
  <summary>Details</summary>
Motivation: 当前的检索增强生成（RAG）需要将通用大模型定制为领域专用的检索器，尤其是在生物医学等高度专业化领域，存在知识更新滞后与幻觉风险。

Method: STM（合成-训练-合并）框架包括：1）生成合成的硬负样本以增强检索能力；2）对检索提示进行优化以提升检索相关性；3）通过模型合并将若干领域专家模型整合为一个更强的 merged 模型，适用于解码器型 LLM。

Result: 在 MTEB 基准的 12 项医学与通用任务子集上，STM 将任务特定专家的表现提升最高可达 23.5%、平均提升 7.5%；所得到的合并模型在不依赖大规模再训练的前提下，超越单一专家和强基线。

Conclusion: STM 提供了一条可扩展而高效的路径，将通用 LLM 转化为高性能、领域专门化的检索器，同时保留通用能力并在专业任务上表现出色。

Abstract: Retrieval-augmented generation (RAG) has become the backbone of grounding Large Language Models (LLMs), improving knowledge updates and reducing hallucinations. Recently, LLM-based retriever models have shown state-of-the-art performance for RAG applications. However, several technical aspects remain underexplored on how to adapt general-purpose LLMs into effective domain-specific retrievers, especially in specialized domains such as biomedicine. We present Synthesize-Train-Merge (STM), a modular framework that enhances decoder-only LLMs with synthetic hard negatives, retrieval prompt optimization, and model merging. Experiments on a subset of 12 medical and general tasks from the MTEB benchmark show STM boosts task-specific experts by up to 23.5\% (average 7.5\%) and produces merged models that outperform both single experts and strong baselines without extensive pretraining. Our results demonstrate a scalable, efficient path for turning general LLMs into high-performing, domain-specialized retrievers, preserving general-domain capabilities while excelling on specialized tasks.

</details>


### [57] [Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases](https://arxiv.org/abs/2602.04739)
*Casey Ford,Madison Van Doren,Emily Dix*

Main category: cs.CL

TL;DR: 两阶段纵向评估多模态大型语言模型的无害性，基于726条由26名红队成员撰写的对抗提示。结果显示不同模型家族在无害性方面差异显著，Pixtral最易受攻击，Claude因高拒绝率相对更安全；生成代际间存在对齐漂移，ASR在GPT/Claude上升，Pixtral/Qwen略降；模态效应随时间变化，Phase 1文本提示更有效，Phase 2呈现模型特定模式；结论：无害性并非统一或稳定，需纵向、多模态基准来跟踪安全行为演变。


<details>
  <summary>Details</summary>
Motivation: 评估在对抗提示下MLLM的安全性，现有研究多为单次评估，缺乏跨代更新的纵向观察。

Method: 两阶段评估：Phase 1对GPT-4o、Claude Sonnet 3.5、Pixtral 12B、Qwen VL Plus进行726条对抗提示评估；Phase 2评估其继任者GPT-5、Claude Sonnet 4.5、Pixtral Large、Qwen Omni；共计82,256个人类伤害评估；关注攻击成功率（ASR）、拒绝率、以及跨模态（文本 vs 图像或其他）等。

Result: Phase 1与Phase 2的比较：Pixtral系列最易受攻击，Claude系列在Phase 1/2显著更拒绝更安全；ASR随代际变化：GPT与Claude的ASR上升，Pixtral/Qwen略降；模态效应随时间变化：Phase 1文本提示更有效，Phase 2出现对不同模型的特定模式，GPT-5与Claude 4.5在两种模态上几乎同等脆弱。

Conclusion: 无害性在MLLM中不是固定不变的，需建立长期、跨模态基准以追踪安全行为的演变。

Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red teamers. Phase 1 assessed GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus; Phase 2 evaluated their successors (GPT-5, Claude Sonnet 4.5, Pixtral Large, and Qwen Omni) yielding 82,256 human harm ratings. Large, persistent differences emerged across model families: Pixtral models were consistently the most vulnerable, whereas Claude models appeared safest due to high refusal rates. Attack success rates (ASR) showed clear alignment drift: GPT and Claude models exhibited increased ASR across generations, while Pixtral and Qwen showed modest decreases. Modality effects also shifted over time: text-only prompts were more effective in Phase 1, whereas Phase 2 produced model-specific patterns, with GPT-5 and Claude 4.5 showing near-equivalent vulnerability across modalities. These findings demonstrate that MLLM harmlessness is neither uniform nor stable across updates, underscoring the need for longitudinal, multimodal benchmarks to track evolving safety behaviour.

</details>


### [58] [Exploiting contextual information to improve stance detection in informal political discourse with LLMs](https://arxiv.org/abs/2602.04750)
*Arman Engin Sucu,Yixiang Zhou,Mario A. Nascimento,Tony Mullen*

Main category: cs.CL

TL;DR: Contextual user-profile prompts improve political stance detection by leveraging inferred user ideology and topics; context-enriched LLMs outperform baseline models.


<details>
  <summary>Details</summary>
Motivation: Stance detection in informal online discourse is challenging due to sarcasm/ambiguity; incorporating user-level context may boost accuracy.

Method: Construct structured profiles from historical posts; evaluate seven SOTA LLMs in baseline vs context-enriched setups; analyze profile size and post-selection strategies.

Result: Contextual prompts yield significant accuracy gains (+17.5% to +38.5%), up to 74% accuracy; selective content outperforms larger random contexts; cross-model gains observed.

Conclusion: Incorporating user-level contextual information enhances LLM performance in nuanced political classification tasks; profile design/selection crucial.

Abstract: This study investigates the use of Large Language Models (LLMs) for political stance detection in informal online discourse, where language is often sarcastic, ambiguous, and context-dependent. We explore whether providing contextual information, specifically user profile summaries derived from historical posts, can improve classification accuracy. Using a real-world political forum dataset, we generate structured profiles that summarize users' ideological leaning, recurring topics, and linguistic patterns. We evaluate seven state-of-the-art LLMs across baseline and context-enriched setups through a comprehensive cross-model evaluation. Our findings show that contextual prompts significantly boost accuracy, with improvements ranging from +17.5\% to +38.5\%, achieving up to 74\% accuracy that surpasses previous approaches. We also analyze how profile size and post selection strategies affect performance, showing that strategically chosen political content yields better results than larger, randomly selected contexts. These findings underscore the value of incorporating user-level context to enhance LLM performance in nuanced political classification tasks.

</details>


### [59] [When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?](https://arxiv.org/abs/2602.04755)
*Xinyu Zhou,Chang Jin,Carsten Eickhoff,Zhijiang Guo,Seyed Ali Bahrainian*

Main category: cs.CL

TL;DR: Introduce abstention for temporal QA in LLMs via a teaching pipeline that combines CoT supervision with RL using abstention-aware rewards; RL improves exact-match on TimeQA and unanswerable detection, though overconfidence from SFT remains a risk; implicit cues offer limited gains over explicit CoT.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate fluent but incorrect answers and fail to abstain, especially in time-sensitive reasoning. Calibration alone is unreliable for complex reasoning, so a teachable abstention skill is proposed to improve reliability in temporal QA.

Method: A pipeline that couples Chain-of-Thought (CoT) supervision with reinforcement learning (RL) guided by abstention-aware rewards; compares supervised fine-tuning (SFT) against RL; analyzes effects of implicit vs explicit CoT cues and various information types (original context, temporal sub-context, knowledge graphs) on abstention-aware temporal reasoning; evaluation on TimeQA-Easy/Hard.

Result: RL-based models show strong empirical gains in reasoning: e.g., a Qwen2.5-1.5B-Instruct initialization surpasses GPT-4o by 3.46% and 5.80% in Exact Match on TimeQA-Easy and TimeQA-Hard. True Positive rate on unanswerable questions improves by ~20% over SFT. SFT induces overconfidence and harms reliability; RL improves accuracy but still carries risk. Implicit reasoning cues provide limited benefit compared with explicit CoT supervision.

Conclusion: Abstention and temporal reasoning can be jointly optimized via an abstention-aware RL pipeline, yielding more reliable LLMs and offering guidance for designing systems that know when not to answer.

Abstract: Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\%$ and $5.80\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.

</details>


### [60] [Beyond Many-Shot Translation: Scaling In-Context Demonstrations For Low-Resource Machine Translation](https://arxiv.org/abs/2602.04764)
*Luis Frentzen Salim,Esteban Carlin,Alexandre Morinvil,Xi Ai,Lun-Wei Ku*

Main category: cs.CL

TL;DR: 在极低资源语言的机器翻译中，极长上下文的提示学习（ICL）存在收益递减甚至负向影响，且收益强烈依赖数据类型；多样化的单语数据有时可与平行数据相媲美，但更大的上下文窗口并不总带来更好质量。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中高质量数据匮乏的问题，并评估在极大上下文窗口下，基于示例的提示学习是否能有效地将LLMs适配到低资源MT任务。

Method: 在斯 Java（爪哇语）和 Sunda（巽他语）的情境下，扩展到1M token上下文的长期ICL，对比三类作为ICL监督的训练语料：单语无监督数据、指令式数据、平行数据（英-目标、印尼-目标）；以千计示例、长上下文模型进行评估，观察上下文规模对翻译质量的影响。

Result: 上下文收益在增加时迅速达饱和，且在接近最大上下文时可能退化；收益对语料类型高度敏感。某些单语监督方式在某些情形与平行数据竞争力相当，尽管平行数据提供额外监督。总体上，增大上下文并不等同于线性提升质量，且跨语言数据类型的选择显著影响结果。

Conclusion: 长期ICL在低资源MT中存在明确的有效边界，需谨慎选择数据类型以实现最优收益；更大的上下文窗口并非普遍有效的提升手段，需结合语料类型与任务特点进行设计与评估。

Abstract: Building machine translation (MT) systems for low-resource languages is notably difficult due to the scarcity of high-quality data. Although Large Language Models (LLMs) have improved MT system performance, adapting them to lesser-represented languages remains challenging. In-context learning (ICL) may offer novel ways to adapt LLMs for low-resource MT by conditioning models on demonstration at inference time. In this study, we explore scaling low-resource machine translation ICL beyond the few-shot setting to thousands of examples with long-context models. We scale in-context token budget to 1M tokens and compare three types of training corpora used as in-context supervision: monolingual unsupervised data, instruction-style data, and parallel data (English--target and Indonesian--target). Our experiments on Javanese and Sundanese show that gains from additional context saturate quickly and can degrade near the maximum context window, with scaling behavior strongly dependent on corpus type. Notably, some forms of monolingual supervision can be competitive with parallel data, despite the latter offering additional supervision. Overall, our results characterize the effective limits and corpus-type sensitivity of long-context ICL for low-resource MT, highlighting that larger context windows do not necessarily yield proportional quality gains.

</details>


### [61] [OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models](https://arxiv.org/abs/2602.04804)
*Yue Ding,Yiyan Ji,Jungang Li,Xuyang Liu,Xinlong Chen,Junfei Wu,Bozhou Li,Bohan Zeng,Yang Shi,Yushuo Guan,Yuanxing Zhang,Jiaheng Liu,Qiang Liu,Pengfei Wan,Liang Wang*

Main category: cs.CL

TL;DR: OmniSIFT introduces a two-stage, modality-asymmetric token compression framework for Omni-LLMs: (1) spatio-temporal video pruning to reduce video token redundancy, (2) vision-guided audio token selection, optimized end-to-end with a differentiable straight-through estimator. It achieves strong efficiency and robustness across five benchmarks, adding only 4.85M parameters for Qwen2.5-Omni-7B, and outperforms compression baselines with 25% of the original tokens, sometimes surpassing full-token models.


<details>
  <summary>Details</summary>
Motivation: Reduce the computational overhead of long multimodal token sequences in Omni-LLMs without sacrificing performance. Existing token compression methods are limited in the Omni-modal setting, necessitating a modality-asymmetric approach that targets video and audio differently.

Method: Two-stage compression: (i) spatio-temporal video pruning to remove intra-frame structure and inter-frame redundancy, (ii) vision-guided audio token selection to filter audio tokens. End-to-end optimization is achieved via a differentiable straight-through estimator, enabling joint learning of compression decisions and downstream task performance.

Result: Empirical validation on five representative benchmarks shows OmniSIFT's efficacy and robustness. For Qwen2.5-Omni-7B, it adds only 4.85M parameters and achieves lower latency than training-free baselines like OmniZip. With only 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the full-token model on several tasks.

Conclusion: OmniSIFT provides an effective, lightweight, modality-asymmetric token compression framework for Omni-LLMs, delivering substantial efficiency gains while maintaining or improving task performance across multiple benchmarks.

Abstract: Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.

</details>


### [62] [SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization](https://arxiv.org/abs/2602.04811)
*Jiarui Yuan,Tailin Jin,Weize Chen,Zeyuan Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: SE-Bench presents a diagnostic environment to evaluate knowledge internalization and self-evolution in agents by obfuscating a novel API (NumPy) and requiring agents to internalize it for solving tasks without docs, revealing key insights about training with docs (Open-Book Paradox), reinforcement learning limitations (RL Gap), and self-play viability with SFT.


<details>
  <summary>Details</summary>
Motivation: To rigorously measure lifelong learning capabilities of agents, disentangling prior knowledge from reasoning difficulty and providing a clean benchmark where new knowledge is truly novel to the agent.

Method: Construct a pseudo-novel package by obfuscating NumPy's API and docs, train agents to internalize this package, and evaluate on simple coding tasks without access to documentation, creating a setting where new-API problems are solvable for informed agents but not base models.

Result: 1) Open-Book Paradox: training with reference docs inhibits retention, necessitating closed-book training for knowledge compression into weights. 2) RL Gap: standard RL struggles to internalize new knowledge due to PPO clipping and negative gradients. 3) Self-Play viability: agents can learn from self-generated noisy tasks when combined with supervised finetuning (SFT), but not with pure RL.

Conclusion: SE-Bench offers a rigorous diagnostic platform for evaluating self-evolution and knowledge internalization in agents, with code and dataset available at the provided GitHub URL.

Abstract: True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.

</details>


### [63] [Decomposed Prompting Does Not Fix Knowledge Gaps, But Helps Models Say "I Don't Know"](https://arxiv.org/abs/2602.04853)
*Dhruv Madhwal,Lyuxin David Zhang,Dan Roth,Tomer Wolfson,Vivek Gupta*

Main category: cs.CL

TL;DR: 研究跨三种分解 prompting 策略（Direct、Assistive、Incremental）在闭卷问答中的可靠性影响。尽管 frontier 模型的准确性提升在下降，但各策略之间的分歧仍可预测潜在错误；基于跨策略分歧的训练无需检索或微调即可实现 abstention，且在 F1 与 AUROC 上优于标准不确定性基线，表明分解 prompting 可作为可靠性诊断工具。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在闭卷问答中的能力边界认知不足及“幻觉”问题。尽管分解 prompting 常用于提升准确性，其对模型可靠性的影响尚未清晰，需要简单、无训练成本的诊断信号来识别内部不确定性。

Method: 在多种模型尺度和多跳问答基准上，对 Direct、Assistive、Incremental 三种任务等效 prompting 进行评估，比较它们的准确性和互相之间的分歧。提出一个训练无成本的 abstention 策略：基于跨策略的一致性/分歧来决定是否拒答，无需检索、微调。与常见的不确定性基线比较。

Result: 在 frontier 模型中，分解带来的准确性增益减弱，但不同 prompting 策略之间的分歧仍然是潜在错误的强信号。由于知识是稳定的，幻觉是随机的，跨策略的一致性成为内部不确定性的精确信号。基于该信号的拒答策略在错误检测上优于标准不确定性基线，提升了 F1 与 AUROC。显示分解 prompting 可作为评估闭卷问答可靠性的可行诊断工具。

Conclusion: 跨策略分歧提供了对模型内部不确定性的高效信号，分解 prompting 可作为无训练成本的诊断工具用于提升闭卷 QA 的可靠性；尽管 frontier 模型的准确性提升有限，跨-regime 一致性在识别潜在错误方面仍具价值。

Abstract: Large language models often struggle to recognize their knowledge limits in closed-book question answering, leading to confident hallucinations. While decomposed prompting is typically used to improve accuracy, we investigate its impact on reliability. We evaluate three task-equivalent prompting regimes: Direct, Assistive, and Incremental, across different model scales and multi-hop QA benchmarks. We find that although accuracy gains from decomposition diminish in frontier models, disagreements between prompting regimes remain highly indicative of potential errors. Because factual knowledge is stable while hallucinations are stochastic, cross-regime agreement provides a precise signal of internal uncertainty. We leverage this signal to implement a training-free abstention policy that requires no retrieval or fine-tuning. Our results show that disagreement-based abstention outperforms standard uncertainty baselines as an error detector, improving both F1 and AUROC across settings. This demonstrates that decomposition-based prompting can serve as a practical diagnostic probe for model reliability in closed-book QA.

</details>


### [64] [CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation](https://arxiv.org/abs/2602.04856)
*Zhao Tong,Chunlin Gong,Yiping Zhang,Qiang Liu,Xingcheng Xu,Shu Wu,Haichao Shi,Xiao-Yu Zhang*

Main category: cs.CL

TL;DR: 本文提出一个统一的安全分析框架，用以解构大语言模型在链式推理（CoT）过程中的内部潜在风险。研究发现即使对有害请求给出拒绝，内部推理仍可能嵌入和传播不安全叙事；通过基于雅可比谱的度量分析注意力头，给出稳定性、几何性和能量三个可解释指标，定位出在中深层连续路由中承担风险的关键头。结果表明在激活“思考模式”时，生成风险显著上升，且风险集中在若干中深层层的路由头；识别这些头为缓解潜在推理风险提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: 当前评估往往以输出为唯一指标，认为拒绝即安全，但这一假设忽略了模型在内部推理阶段可能产生并传播不安全信息。本研究旨在揭示CoT过程中的潜在风险及其结构化可分析性。

Method: 提出一个统一的安全分析框架，系统分解模型的CoT生成过程，结合 Jacobian 基于谱指标，分析和量化注意力头的作用。引入三种可解释度量：稳定性、几何性和能量，用以评估特定注意力头对欺骗性推理的嵌入和传播。通过在多种推理导向的LLMs 上的实验，追踪中深层路由的关键头。

Result: 实验表明：1) 当思考模式被激活时，生成风险显著上升；2) 关键的风险路由集中在若干连续的中深层层；3) 通过定位和识别相关注意力头，可揭示并量化潜在的推理风险。框架对挑战“拒绝等于安全”的假设提供了实证证据，并给出潜在的风险缓释路径。

Conclusion: 拒绝并不等同于安全，内部推理可能在不被察觉的情况下产生并传播不安全信息。本文提出的框架与指标为识别和缓解 latent reasoning 风险提供了新的分析工具和研究方向。

Abstract: From generating headlines to fabricating news, the Large Language Models (LLMs) are typically assessed by their final outputs, under the safety assumption that a refusal response signifies safe reasoning throughout the entire process. Challenging this assumption, our study reveals that during fake news generation, even when a model rejects a harmful request, its Chain-of-Thought (CoT) reasoning may still internally contain and propagate unsafe narratives. To analyze this phenomenon, we introduce a unified safety-analysis framework that systematically deconstructs CoT generation across model layers and evaluates the role of individual attention heads through Jacobian-based spectral metrics. Within this framework, we introduce three interpretable measures: stability, geometry, and energy to quantify how specific attention heads respond or embed deceptive reasoning patterns. Extensive experiments on multiple reasoning-oriented LLMs show that the generation risk rise significantly when the thinking mode is activated, where the critical routing decisions concentrated in only a few contiguous mid-depth layers. By precisely identifying the attention heads responsible for this divergence, our work challenges the assumption that refusal implies safety and provides a new understanding perspective for mitigating latent reasoning risks.

</details>


### [65] [Reinforced Attention Learning](https://arxiv.org/abs/2602.04884)
*Bangzheng Li,Jianmo Ni,Chen Qu,Ian Miao,Liu Yang,Xingyu Fu,Muhao Chen,Derek Zhiyuan Cheng*

Main category: cs.CL

TL;DR: 提出并验证 Reinforced Attention Learning (RAL) 及 On-Policy Attention Distillation，通过优化注意力分布而非生成序列，提升多模态后训练效果，并在图像/视频任务上优于GRPO等基线。


<details>
  <summary>Details</summary>
Motivation: 现有的 RL 后训练在单模态 LLM 上提升推理，但扩展到多模态时，使用冗长推理理由的方法对感知效果有限甚至可能恶化，需要一种以注意力分布为核心的新优化目标；注意力策略可能更直接地提升对复杂多模态输入的理解和对齐。

Method: 提出 Reinforced Attention Learning (RAL)：基于策略梯度的框架，直接优化内部注意力分布，而不是输出 token 序列；目标从“生成什么”转为“在哪儿关注”，以改善信息分配和对多模态输入的 grounding。并引入 On-Policy Attention Distillation，将潜在注意力行为进行跨模态迁移，以实现比标准知识蒸馏更强的跨模态对齐。

Result: 在多样的图像与视频基准上，RAL 相较 GRPO 及其他基线实现持续性提升；基于 On-Policy Attention Distillation 的迁移在跨模态对齐方面优于常规知识蒸馏。

Conclusion: 注意力策略作为多模态后训练的一个原理化、普适的替代目标，直接优化注意力分布可提升多模态理解与对齐性能，具有广泛适用性。

Abstract: Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: TMK prompting显著提升LLM在PlanBench Blocksworld域的推理与规划能力，在不透明、符号化任务（Blocksworld随机变体）上准确率升至97.3%，远超此前31.5%，表明TMK可将推理从纯语言模式引导到形式化代码执行路径，提升符号处理能力。


<details>
  <summary>Details</summary>
Motivation: LLM在推理与规划方面能力受限，常用Chain-of-Thought等提示但效果有限。借用教育领域的Task-Method-Knowledge（TMK）框架，因其能捕获因果、目的性和层级结构，并具备显式任务分解机制，理论上更适合改善语言模型的推理缺陷。

Method: 在PlanBench的Blocksworld域上测试TMK提示，关注模型是否能将复杂规划问题分解为可管理的子任务；对比对照包括随机版本的Blocksworld（符号任务）以评估符号操作能力的提升。

Result: TMK提示使推理模型在PlanBench的随机Blocksworld任务上达到高达97.3%的准确率，显著高于以前的31.5%，显示TMK可将语义近似能力转化为符号操作能力。

Conclusion: TMK不仅充当上下文信息，更作为机制将推理模型引导至形式化推理/代码执行路径，具有跨域潜力，可能弥合语义近似与符号推理之间的差距。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [67] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: IIPC通过迭代改进程序化推理链，结合执行反馈与LLM的链式思维，提升数学推理的可修正性与可靠性，在多种基线LLM上优于竞争方法，并开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体LLM推理要么采用刚性顺序管线，无法纠错；要么依赖自我评估但容易错漏；将程序化上下文引入推理也会分散注意力，降低准确性。因此需要一种可迭代修正、兼顾全局上下文的推理框架。

Method: 提出Iteratively Improved Program Construction (IIPC)：对程序化推理链进行迭代改进，在每轮中结合执行反馈与LLM的原生Chain-of-Thought能力，保持高层次上下文聚焦并修正先前步骤。

Result: 在多种基线LLM上，IIPC在大多数数学推理基准上超过竞争方法。

Conclusion: IIPC通过将执行反馈与程序化推理相结合，提升推理过程的可修正性与稳定性，并以开源代码实现便于复现与应用。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [68] [Active Epistemic Control for Query-Efficient Verified Planning](https://arxiv.org/abs/2602.03974)
*Shuhui Qu*

Main category: cs.AI

TL;DR: AEC combines grounded facts and belief-based pruning to plan under partial observability; it queries or simulates predicates to refine plans, with final commitment gated by grounded checks; achieves competitive results with fewer replans.


<details>
  <summary>Details</summary>
Motivation: Partial observability makes task-critical preconditions unknown; ground truth is costly to obtain; predictive models help, but prediction errors can mislead; need a framework that separates grounded facts from beliefs and uses epistemic checks.

Method: Propose Active Epistemic Control (AEC) with two stores: grounded fact store for commitment, belief store for pruning. At each step, decide to query environment to ground unresolved predicates or simulate predicates to prune hypotheses; final commitment requires grounded preconditions coverage and a compatibility check (SQ-BCP pullback-style).

Result: Empirical evaluation on ALFWorld and ScienceWorld shows AEC achieves competitive success with fewer replanning rounds than strong LLM-based baselines.

Conclusion: AEC provides efficient planning under uncertainty by combining grounding and epistemic pruning; predictions affect efficiency but do not certify feasibility; reduces replanning.

Abstract: Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \emph{grounded fact store} used for commitment and a \emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines.

</details>


### [69] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 在验证成本受限的推理场景中，提出一种基于状态的选择性验证框架，通过（1）确定性可行性门控、（2）基于状态距离和残差的预验排序、（3）基于局部不确定性的自适应分配验证调用，以在信息最有利的地方分发验证。


<details>
  <summary>Details</summary>
Motivation: 解决大量验证调用中对冗余或无前景中间假设的浪费，提升在有限验证资源下的推理效率与准确性。

Method: 将验证分解为状态级别的选择性过程：1) 在结构化移动接口上进行确定性可行性门控；2) 使用状态距离和残差分数的混合模型进行预验排序；3) 根据局部不确定性自适应分配验证调用。

Result: 在 MATH 基准上，该方法在准确性上超过 best-of-N、多数投票和束搜索，同时减少约 44% 的验证调用。

Conclusion: 状态级选择性验证能更高效地配置验证资源，提升推理准确性并降低成本；相较传统方法，框架在验证瓶颈问题上的表现更优，且具有将验证资源聚焦于信息增益区域的潜力。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [70] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 监控可追踪性在大规模推理模型中的出现具有显著的数据依赖性；提升往往源于输出分布的收敛（熵降低）和对提示的更强关注，而非对推理过程的更强因果依赖；数据多样性与指令跟随数据在 RLVR 训练中起关键作用，监控性与能力提升正交。


<details>
  <summary>Details</summary>
Motivation: 系统性评估 RLVR 下的链路推理（CoT）轨迹的可监控性，揭示在不同模型家族和训练领域中的出现条件及机制，以帮助安全审计与模型调控。

Method: 对多模型家族和训练域进行系统评估，量化监控性、数据多样性、指令跟随数据对 RLVR 训练的影响；进行机械分析以区分因果依赖和响应分布变化；在受控训练与评估难度下考察监控性动态。

Result: 结果表明监控性提升并非普遍现象，且高度依赖数据条件；数据多样性与指令跟随数据在 RLVR 训练中扮演关键角色；监控性与模型能力提升正交；监控性提升主要由输出分布收敛和对提示的注意增强驱动，而非对推理迹的更强因果依赖；不同的评估难度下监控性动态不同。

Conclusion: 本文提供一个关于 RLVR 条件下监控性出现的整体视角，明确在何种情形更可能获得提升、何时不太可能，从而为安全审计、数据设计与训练策略提供指导。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [71] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: 对LLM生成的解释进行对抗性操纵以误导用户信任的攻击，提出信任错校差距并在205名参与者的实验中验证，发现对抗性解释在很大程度上维持无错解释的信任水平，且在专家式表达等情形下易被利用。


<details>
  <summary>Details</summary>
Motivation: 解释作为AI与人类决策之间的认知通道，可能被用来操纵信任，从而导致对错误输出的高置信度。首次系统性研究这一威胁。

Method: 控制实验，系统改变四个解释框架维度（推理方式、证据类型、沟通风格、呈现格式），测量在正确与错误输出下的人类信任差异，定义并量化信任错校差距。

Result: 对抗性解释在大多数情况下与良性解释的信任相近，且在错误输出下仍能保持大部分良性信任；在接近专家级沟通、具权威证据、中立语气和领域适当推理时最易被利用；在困难任务、事实驱动领域以及受教育程度较低、年龄较小或对AI信任度高的参与者中风险更高。

Conclusion: 首次系统性地将解释视为对抗性认知通道并量化其对人机信任的影响，提示需关注解释的安全设计与防护。

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [72] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 提出一个基于公理的对抗反事实解释框架，证明不可能存在同时满足某些公理组合的单一解释器，并将可解释器分成五大类型的并列对应关系（五个公函数的集合），其中既包含局部解释也包含全局解释；并将现有解释器置于该分类中，给出行为的形式化刻画及生成复杂性分析。


<details>
  <summary>Details</summary>
Motivation: 当前对抗反事实解释研究多聚焦于单一种类且局部解释，缺乏对其他类型与全局解释的系统性研究。需要建立一个包含多种公理的框架来揭示不同类型解释之间的关系、局部与全局解释的边界，以及潜在的权衡与不可避免的限制，以提升可解释性与信任度。

Method: 建立一组对抗反事实解释的“ desirable properties”（公理），并通过不可行性定理证明在某些公理组合下不存在满足条件的通用解释器；给出对所有兼容公理集合的完整刻画；通过表示定理在公理子集与满足它们的解释器族之间建立五一一对应关系，推出五种根本不同的反事实解释类型，部分为局部、部分为全局；将现有解释器放入该分类中并对其行为进行形式化刻画，同时分析生成这些解释所需的计算复杂度。

Result: 得到五大类型的反事实解释家族及其对应的公理集合，并给出这五类之间的区分性与包含关系；建立了不可行性定理，表明单一解释器无法同时满足某些公理组；给出将现有解释器映射到分类中的方法，并对生成过程的计算复杂度进行分析。

Conclusion: 提出的公理化框架实现了对反事实解释的系统化分类，揭示了局部与全局解释的共性与权衡，提供了选择与设计解释器的理论基础，且为扩展与比较现有解释器提供了统一的形式化基线。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [73] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: 通过多任务多回合元强化学习对LLM进行训练，使其在上下文中从交互中学习；小模型在未见环境中达到GPT-5.2水平并超越RL微调，且规模越大收益越显著，代码可复现。


<details>
  <summary>Details</summary>
Motivation: 现实世界的决策任务通常是在线的，需通过与环境交互获取信息，反馈具有延迟，且需要在信息收集与利用之间权衡。尽管LLMs具备就地适应能力，现有模型很难稳定地利用上下文中的交互经验来应对在线情境，因此需要通过训练让模型学会从交互中学习。

Method: 提出ORBIT框架：一个多任务、多回合的元强化学习训练流程，旨在让LLM在上下文中通过交互学习信息收集与利用的策略。通过元训练，使模型具备从交互经验中更新策略的能力。评估在未对新环境进行权重更新的情形下，Qwen3-14B等模型的在线学习表现。

Result: 元训练后，Qwen3-14B在未见环境上实现显著的在线学习提升，达到与GPT-5.2相当的表现，并显著优于标准RL微调。进一步的缩放实验在模型规模增加时仍持续获益，显示方法对更大模型的潜在收益。

Conclusion: ORBIT为LLMs提供了在线决策能力的训练路径，证实了通过元强化学习在上下文中学习的可行性，并揭示了模型规模对学习-推理阶段的收益潜力。代码公开，便于复现实验。

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [74] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze提出将大模型应用看作构建与使用上下文的系统，通过感知模块、上下文构建层与行动层的多栈体系，将推理负荷下放到小模型与工具，LLM仅在蒸馏上下文上推理，从而在多模态与复杂任务上实现成本与性能的折中。


<details>
  <summary>Details</summary>
Motivation: 解决单一大模型难以应对多模态输入、外部信息检索与代码执行等复杂场景的局限，降低对昂贵大模型的依赖，提高可扩展性与鲁棒性。

Method: 提出Interfaze架构：感知模块堆栈（OCR/ASR/多语言），上下文构建层（抓取、索引、解析外部资源并生成结构化状态），行动层（浏览、执行沙盒代码、无头浏览器等），以及一个薄控制器将各组件组合并将蒸馏后的上下文传给LLM以产出最终回答。并在Interfaze-Beta上给出多项基准结果。

Result: 在MMLU-Pro、MMLU、GPQA-Diamond、LiveCodeBench v5、AIME-2025等评测中取得相对优越/竞争性的分数；在MMM U、AI2D、ChartQA、Common Voice等多模态数据集上也有强劲表现。结论指出大多数查询由小模型与工具栈处理，LLM仅对蒸馏上下文进行推理，从而实现较强准确性与显著降低昂贵模型的计算负担。

Conclusion: 通过模块化、分层的推理流程，Interfaze展示了将复杂任务分解为感知、上下文构建与行动执行的有效路径，提升多模态与外部信息整合的可扩展性与成本效益；未来工作可聚焦于蒸馏策略、延迟、鲁棒性及安全性等方面。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [75] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: Scalable Interactive Oversight decomposes complex intent into a recursive decision-tree to amplify human supervision, using low-burden feedback at each node to build precise global guidance; validated on web development tasks with 54% alignment improvement; can be optimized via online-reinforcement learning to maintain human control as AI scales.


<details>
  <summary>Details</summary>
Motivation: Address the supervision gap in long-horizon tasks where users lack domain expertise and struggle to articulate and validate intents, creating a scalable framework to steer AI reliably beyond expert-level capabilities.

Method: Introduce a recursive decision-tree framework (Scalable Interactive Oversight) that solicits low-burden feedback at each node, aggregates signals to form precise global guidance, and iterates via reinforcement learning using online user feedback.

Result: Non-experts achieving expert-level Product Requirement Documents with 54% improved alignment in a web development task; demonstration of practical RL optimization using only online feedback.

Conclusion: The framework offers a scalable, controllable approach to align AI with user intents for complex tasks and can be continuously improved via online RL, enabling sustained human oversight as AI systems scale.

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [76] [InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons](https://arxiv.org/abs/2602.04213)
*Feiyu Gavin Zhu,Jean Oh,Reid Simmons*

Main category: cs.AI

TL;DR: 提出 InterPReT，允许终端用户通过持续的指令和演示互动地重构策略结构并优化参数，以降低模仿学习对专业演示和密切监控的依赖。


<details>
  <summary>Details</summary>
Motivation: 为了解决非专业用户难以提供大规模专业演示、以及对训练过程需密切监控的挑战，提出一种可交互的策略重构与训练框架，提升可用性与鲁棒性。

Method: 通过用户指令不断更新策略的结构并同时优化参数；允许用户在演示过程中互动地给出指令、监控表现、回顾决策策略；在赛车游戏任务中通过用户研究验证。

Result: 在N=34的用户研究中，与通用的模仿学习基线相比，InterPReT在由 layperson 提供演示和决定停止时机的条件下，产生更鲁棒的策略且不削弱系统易用性。

Conclusion: 该方法更适合没有深厚ML背景的终端用户来训练出可靠的策略。

Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy

</details>


### [77] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 提出零配置（ZeroConf）AI流水线的新范式，通过数字孪生（DT）编排数据管理和智能增强，实现在CPS中的模块化、可互操作的AI集成，并在MicroFactory场景中验证对并发ML模型和动态数据处理的支持。


<details>
  <summary>Details</summary>
Motivation: IoT/IIoT多样化协议、数据格式和设备能力导致低层物理层与高层智能功能之间存在断层，亟需可扩展、可复用的AI功能集成；现有方法往往孤立、耦合度高，限制了系统的可扩展性与再利用性。

Method: 提出一个模块化、互操作的解决方案，减少配置需求并解耦DT与AI组件的角色。引入Zero Configuration AI流水线模型，由DT负责数据管理与智能增强的编排。通过在MicroFactory场景中的实现，支持并发ML模型和动态数据处理，展示在复杂工业场景中推动智能服务部署的能力。

Result: 实现方案在MicroFactory场景中验证了对并发ML模型和动态数据处理的支持，显示该架构能够加速智能服务在复杂工业环境中的部署。

Conclusion: 零配置AI流水线结合DT编排的数据管理与智能增强能力，提升CPS中AI功能的模块化、可互操作性与可扩展性，有助于降低配置开销并促进对多模型、动态数据处理的快速部署。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [78] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 提出一个生成式AI与论坛的顺序交互框架，通过主动向论坛提问来获取数据，同时考虑非货币交换、信息不对称和激励错配；以真实 Stack Exchange 数据和常用大语言模型进行数据驱动仿真，发现激励错配存在但在接近完全信息条件下，参与者可实现约一半的效用，显示人机协作的可持续性潜力。


<details>
  <summary>Details</summary>
Motivation: GenAI系统依赖论坛数据提升性能，但又可能削弱论坛的活跃度和数据供给，存在数据循环与激励错位的问题，需研究可持续的协作机制。

Method: 以序贯互动框架形式建模，进行数据驱动仿真，使用真实 Stack Exchange 数据与常用LLMs，评估多方激励、信息不对称及非货币交易对协作的影响。

Result: 证明存在激励错配的证据，但在接近理想全信息场景下，参与者可获得约一半的效用，相比完全信息的最佳情形表现出显著差距但仍表现出潜力。

Conclusion: 提出一种可持续的生成式AI与知识平台协作路径，保留有效的知识共享，同时利用生成式AI提升数据价值。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [79] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 作者质疑 METR 的指数增长结论，主张数据并非指数增长，拟合 sigmoid 拐点已过去，并提出将 AI 能力分解为基础与推理两部分的双分量模型，暗示近期可能出现新的拐点。


<details>
  <summary>Details</summary>
Motivation: 评估 AI 能力增长的真实动力学，检验已有对指数增长的预测的稳健性，揭示模型结构对拐点判断的敏感性。

Method: 对 METR 数据拟合 sigmoid/logistic 曲线以推断拐点位置；提出将 AI 能力分解为基础能力与推理能力的双分量模型，并分析各自的改进率以探讨拐点的可出现性。

Result: 在当前数据下用 sigmoid 拟合的拐点已过去；分解模型在理论层面支持未来近端出现拐点的假设。

Conclusion: 强调现有对指数增长的预测易受数据窗、模型假设与度量标准影响，需要更健壮的证据与方法来评估长期趋势。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [80] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: Mechanistic analysis of QwQ-32B's reasoning traces in a semantically obfuscated planning domain reveals abstract, structure-focused encodings and Fluid Reasoning Representations that causally improve problem solving.


<details>
  <summary>Details</summary>
Motivation: Understand internal mechanisms enabling reasoning models to outperform non-reasoning models by examining how long reasoning traces affect internal representations and problem-solving ability in an abstract domain.

Method: Mechanistic analysis of QwQ-32B on Mystery Blocksworld; track evolution of internal representations of actions/concepts during reasoning; conduct steering experiments by injecting refined representations from successful traces and by substituting obfuscated encodings with symbolic representations; assess impact on task performance.

Result: QwQ-32B gradually refines its internal action/concept representations, developing abstract encodings that emphasize structural relations over action names. Steering experiments show injecting refined trace representations boosts accuracy; symbolic representations can replace many obfuscated encodings with minimal performance loss. A key factor is in-context refinement of token representations, termed Fluid Reasoning Representations.

Conclusion: Fluid Reasoning Representations, driven by in-context refinement, contribute causally to improved reasoning performance in a model specifically trained for extensive reasoning traces, suggesting that dynamic token-level adaptation underpins abstract problem-solving.

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [81] [Nemotron ColEmbed V2: Top-Performing Late Interaction embedding models for Visual Document Retrieval](https://arxiv.org/abs/2602.03992)
*Gabriel de Souza P. Moreira,Ronay Ak,Mengyao Xu,Oliver Holworthy,Benedikt Schifferer,Zhiding Yu,Yauhen Babakhin,Radek Osmulski,Jiarui Cai,Ryan Chesler,Bo Liu,Even Oldridge*

Main category: cs.IR

TL;DR: Nemotron ColEmbed V2 系列在视觉文档检索领域实现SOTA，基于多种VLM嵌入模型，提供3B/4B/8B三种规模，8B在ViDoRe V3榜单排名第一，平均NDCG@10达到63.42。


<details>
  <summary>Details</summary>
Motivation: 回应对视觉文档检索的增长需求，以及RAG系统中将外部知识引入检索的挑战，提升精度和检索效果，同时简化索引管道。

Method: 基于预训练VLM的嵌入模型，3种变体：NVIDIA Eagle 2 + Llama 3.2 3B、Qwen3-VL-4B-Instruct、Qwen3-VL-8B-Instruct。采用集群采样、难负样本挖掘、双向注意、晚交互、模型融合等技术，并评估低维嵌入在准确性与存储之间的权衡。

Result: 在 ViDoRe 基准上达到SOTA；8B模型在ViDoRe V3榜单上排名第一，平均NDCG@10为63.42。

Conclusion: 讨论晚交互机制带来的计算与存储挑战，展示通过降低维度嵌入实现精度与存储之间的平衡的实验结果。

Abstract: Retrieval-Augmented Generation (RAG) systems have been popular for generative applications, powering language models by injecting external knowledge. Companies have been trying to leverage their large catalog of documents (e.g. PDFs, presentation slides) in such RAG pipelines, whose first step is the retrieval component. Dense retrieval has been a popular approach, where embedding models are used to generate a dense representation of the user query that is closer to relevant content embeddings. More recently, VLM-based embedding models have become popular for visual document retrieval, as they preserve visual information and simplify the indexing pipeline compared to OCR text extraction.
  Motivated by the growing demand for visual document retrieval, we introduce Nemotron ColEmbed V2, a family of models that achieve state-of-the-art performance on the ViDoRe benchmarks. We release three variants - with 3B, 4B, and 8B parameters - based on pre-trained VLMs: NVIDIA Eagle 2 with Llama 3.2 3B backbone, Qwen3-VL-4B-Instruct and Qwen3-VL-8B-Instruct, respectively. The 8B model ranks first on the ViDoRe V3 leaderboard as of February 03, 2026, achieving an average NDCG@10 of 63.42.
  We describe the main techniques used across data processing, training, and post-training - such as cluster-based sampling, hard-negative mining, bidirectional attention, late interaction, and model merging - that helped us build our top-performing models. We also discuss compute and storage engineering challenges posed by the late interaction mechanism and present experiments on how to balance accuracy and storage with lower dimension embeddings.

</details>


### [82] [Following the TRAIL: Predicting and Explaining Tomorrow's Hits with a Fine-Tuned LLM](https://arxiv.org/abs/2602.04225)
*Yinan Zhang,Zhixi Chen,Jiazheng Jing,Zhiqi Shen*

Main category: cs.IR

TL;DR: TRAIL: 一个微调的大语言模型，联合预测短期项人气并生成可信的自然语言解释，通过对比学习对齐分数与结构化趋势信号，从而实现高精度推荐与可解释性解释。


<details>
  <summary>Details</summary>
Motivation: 解决从大规模稀疏的用户-物品日志中提取用户偏好、实现全量目录的实时排序成本，以及在推荐中同时提供解释以提升信任感的问题。

Method: 以微调的LLM为核心，同时完成短期热门度预测与解释生成；通过对比学习，使用正负样本对来使评分和解释对齐到结构化趋势信号，从而提高预测准确性与解释的一致性。

Result: 实验显示 TRAIL 超越强基线；生成的解释连贯、 grounding充足，且与预测结果相符。

Conclusion: 将趋势信号与可解释性整合到同一模型中，证明在推荐场景中可同时实现高预测性能和高质量解释，具有实际应用潜力。

Abstract: Large Language Models (LLMs) have been widely applied across multiple domains for their broad knowledge and strong reasoning capabilities. However, applying them to recommendation systems is challenging since it is hard for LLMs to extract user preferences from large, sparse user-item logs, and real-time per-user ranking over the full catalog is too time-consuming to be practical. Moreover, many existing recommender systems focus solely on ranking items while overlooking explanations, which could help improve predictive accuracy and make recommendations more convincing to users. Inspired by recent works that achieve strong recommendation performance by forecasting near-term item popularity, we propose TRAIL (TRend and explAnation Integrated Learner). TRAIL is a fine-tuned LLM that jointly predicts short-term item popularity and generates faithful natural-language explanations. It employs contrastive learning with positive and negative pairs to align its scores and explanations with structured trend signals, yielding accurate and explainable popularity predictions. Extensive experiments show that TRAIL outperforms strong baselines and produces coherent, well-grounded explanations.

</details>


### [83] [LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval](https://arxiv.org/abs/2602.04263)
*Joohyung Yun,Doyup Lee,Wook-Shin Han*

Main category: cs.IR

TL;DR: 提出 LILaC，基于两层图和晚期交互的子图检索，解决多模态文档检索的粒度冗余和多跳推理问题，在五个基准上达到 SOTA 且无需额外微调。


<details>
  <summary>Details</summary>
Motivation: 解决固定粒度检索单元导致的无关内容干扰，以及跨文档/跨组件的关系建模与多跳推理的需求。

Method: 设计两层的组件图，表示粗粒度与细粒度信息；提出以边为基础的晚期交互子图检索，先通过粗粒度节点筛选候选再进行细粒度推理。

Result: 在五个基准数据集上达到 SOTA 性能，且无需额外微调，公开代码。

Conclusion: 两层图+晚期交互能有效提升多模态检索的准确性与效率，具有较好的推广性和可复现性。

Abstract: Multimodal document retrieval aims to retrieve query-relevant components from documents composed of textual, tabular, and visual elements. An effective multimodal retriever needs to handle two main challenges: (1) mitigate the effect of irrelevant contents caused by fixed, single-granular retrieval units, and (2) support multihop reasoning by effectively capturing semantic relationships among components within and across documents. To address these challenges, we propose LILaC, a multimodal retrieval framework featuring two core innovations. First, we introduce a layered component graph, explicitly representing multimodal information at two layers - each representing coarse and fine granularity - facilitating efficient yet precise reasoning. Second, we develop a late-interaction-based subgraph retrieval method, an edge-based approach that initially identifies coarse-grained nodes for efficient candidate generation, then performs fine-grained reasoning via late interaction. Extensive experiments demonstrate that LILaC achieves state-of-the-art retrieval performance on all five benchmarks, notably without additional fine-tuning. We make the artifacts publicly available at github.com/joohyung00/lilac.

</details>


### [84] [MiniRec: Data-Efficient Reinforcement Learning for LLM-based Recommendation](https://arxiv.org/abs/2602.04278)
*Lin Wang,Yang Zhang,Jingfan Chen,Xiaoyan Zhao,Fengbin Zhu,Qing Li,Tat-Seng Chua*

Main category: cs.IR

TL;DR: MiniRec 是一个面向 RL-LM 推荐的数据选择框架，利用奖励信号 prune 过易/过难样本、梯度轨迹对齐与多样性约束，并结合从易到难的课程学习，在显著降低训练成本的同时尽可能保持性能。


<details>
  <summary>Details</summary>
Motivation: RL 驱动的 LLM 推荐在提升推理能力和用户偏好建模方面具潜力，但全量数据训练成本高昂；现有数据选择依据的可学习性/代表性/梯度或数据覆盖等准则往往与 RL 学习动态不对齐，导致子优化。

Method: 核心在于 (1) 基于奖励信号评估样本可学习性， prune 掉奖励过高（太易）或持续奖励很低（太难）的样本；(2) 通过近似的全局 RL 优化轨迹，将样本梯度与目标轨迹对齐，筛选对模型更新贡献主导的样本；(3) 引入多样性以降低冗余；(4) 引入自易到难的课程学习策略。

Result: 实验结果表明，在显著降低训练成本的同时，性能基本保持，证实 reward-aligned、trajectory-informed 的数据选择对 RL-LM 推荐的重要性。

Conclusion: 强调 reward-aligned、trajectory-informed 数据选择在 RL-LM 推荐中的重要性；MiniRec 能有效降低成本并保持性能。

Abstract: The integration of reinforcement learning (RL) into large language models (LLMs) has opened new opportunities for recommender systems by eliciting reasoning and improving user preference modeling. However, RL-based LLM recommendation faces significant efficiency challenges, making full-data training costly. Existing data selection methods define sample value based on learnability or representativeness, yet their loss- or gradient-driven or dataset coverage-driven criteria often misalign with RL learning dynamics, resulting in suboptimal performance. To address this, we propose MiniRec, a data selection framework tailored for RL-based LLM recommendation. MiniRec evaluates sample learnability using key RL signals -- rewards -- pruning samples that are too easy (too high reward) or too difficult (consistently low reward). It assesses representativeness by aligning sample gradients with the approximated "ideal" global RL optimization trajectory, selecting samples that mainly drive model updates, and it also enforces diversity to reduce redundancy. Combined with a curriculum learning strategy from easy to hard samples, MiniRec significantly reduces training cost while largely preserving performance. Extensive experiments demonstrate MiniRec's effectiveness, highlighting the importance of reward-aligned, trajectory-informed data selection in RL-based LLM recommendation.

</details>


### [85] [SDR-CIR: Semantic Debias Retrieval Framework for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.04451)
*Yi Sun,Jinyu Xu,Qing Xie,Jiachen Li,Yanchun Ma,Yongjian Liu*

Main category: cs.IR

TL;DR: Proposes SDR-CIR: a training-free Semantic Debias Ranking method for Composed Image Retrieval that uses Selective CoT to extract modification-relevant content and a two-step Anchor-Debias ranking to reduce semantic bias, achieving state-of-the-art performance among one-stage CIR methods with high efficiency.


<details>
  <summary>Details</summary>
Motivation: ZS-CIR methods using MLLMs with Chain-of-Thought often generate descriptions biased toward the target image due to semantic bias, caused by residual visual noise and misalignment between reference and target semantics. A training-free approach is sought to mitigate this bias and improve retrieval accuracy.

Method: 1) Selective CoT guides MLLMs to focus on visual content relevant to the modification text, reducing noise at the source. 2) Semantic Debias Ranking comprises two steps: Anchor, which fuses reference image features with target description features to reinforce useful semantics and fill gaps; Debias, which models the visual semantic contribution of the reference image to the description and adds a penalty term to the similarity score to suppress bias.

Result: SDR-CIR achieves state-of-the-art results among one-stage methods on three standard CIR benchmarks while maintaining high efficiency.

Conclusion: The proposed Selective CoT and Anchor-Debias ranking effectively mitigate semantic bias in zero-shot CIR, improving retrieval performance without additional training; the approach is efficient and publicly available code.

Abstract: Composed Image Retrieval (CIR) aims to retrieve a target image from a query composed of a reference image and modification text. Recent training-free zero-shot methods often employ Multimodal Large Language Models (MLLMs) with Chain-of-Thought (CoT) to compose a target image description for retrieval. However, due to the fuzzy matching nature of ZS-CIR, the generated description is prone to semantic bias relative to the target image. We propose SDR-CIR, a training-free Semantic Debias Ranking method based on CoT reasoning. First, Selective CoT guides the MLLM to extract visual content relevant to the modification text during image understanding, thereby reducing visual noise at the source. We then introduce a Semantic Debias Ranking with two steps, Anchor and Debias, to mitigate semantic bias. In the Anchor step, we fuse reference image features with target description features to reinforce useful semantics and supplement omitted cues. In the Debias step, we explicitly model the visual semantic contribution of the reference image to the description and incorporate it into the similarity score as a penalty term. By supplementing omitted cues while suppressing redundancy, SDR-CIR mitigates semantic bias and improves retrieval performance. Experiments on three standard CIR benchmarks show that SDR-CIR achieves state-of-the-art results among one-stage methods while maintaining high efficiency. The code is publicly available at https://github.com/suny105/SDR-CIR.

</details>


### [86] [DOS: Dual-Flow Orthogonal Semantic IDs for Recommendation in Meituan](https://arxiv.org/abs/2602.04460)
*Junwei Yin,Senjie Kou,Changhao Li,Shuli Wang,Xue Wei,Yinqiu Huang,Yinhua Zhu,Haitao Wang,Xingxing Wang*

Main category: cs.IR

TL;DR: DOS introduces dual-flow alignment and orthogonal residual quantization to improve Semantic ID-based generation in recommender systems, achieving better alignment with generation space and higher semantic preservation.


<details>
  <summary>Details</summary>
Motivation: Address two main limitations: (i) lack of context-awareness causing misalignment between Semantic ID codebook and generation space, (ii) suboptimal quantization causing semantic loss in LLMs.

Method: 1) Dual-flow user-item framework that uses collaborative signals to align the Semantic ID codebook with the generation space. 2) Orthogonal residual quantization that rotates the semantic space to an orientation that maximizes semantic preservation.

Result: Offline experiments and online A/B tests show improved performance; deployment in Meituan app reaching hundreds of millions of users.

Conclusion: DOS effectively enhances generation-aware semantic IDs, with practical deployment and measurable gains in large-scale, real-world recommender systems.

Abstract: Semantic IDs serve as a key component in generative recommendation systems. They not only incorporate open-world knowledge from large language models (LLMs) but also compress the semantic space to reduce generation difficulty. However, existing methods suffer from two major limitations: (1) the lack of contextual awareness in generation tasks leads to a gap between the Semantic ID codebook space and the generation space, resulting in suboptimal recommendations; and (2) suboptimal quantization methods exacerbate semantic loss in LLMs. To address these issues, we propose Dual-Flow Orthogonal Semantic IDs (DOS) method. Specifically, DOS employs a user-item dual flow-framework that leverages collaborative signals to align the Semantic ID codebook space with the generation space. Furthermore, we introduce an orthogonal residual quantization scheme that rotates the semantic space to an appropriate orientation, thereby maximizing semantic preservation. Extensive offline experiments and online A/B testing demonstrate the effectiveness of DOS. The proposed method has been successfully deployed in Meituan's mobile application, serving hundreds of millions of users.

</details>


### [87] [VK-LSVD: A Large-Scale Industrial Dataset for Short-Video Recommendation](https://arxiv.org/abs/2602.04567)
*Aleksandr Poslavsky,Alexander D'yakonov,Yuriy Dorn,Andrey Zimovnov*

Main category: cs.IR

TL;DR: 提出 VK-LSVD 数据集，公开的工业级短视频数据集，规模空前（>40亿次交互、1000万用户、近2000万视频，6个月），具备内容嵌入、多样反馈信号和上下文元数据等特征；经分析证实数据质量和多样性；在 VK RecSys Challenge 2025 中扮演核心角色；可用于构建现实世界基准，推动序列推荐、冷启动及新一代推荐系统研究。


<details>
  <summary>Details</summary>
Motivation: 解决缺乏能反映真实平台动态的大规模公开数据集的问题，特别是短视频领域存在用户兴趣快速变化的隐式反馈难以建模；需要现实世界数据以促进可重复的评估和 Benchmark。

Method: 收集并整理 VK 平台六个月的数据，包含超过40亿次交互、1000万用户、近2000万视频及丰富特征（内容嵌入、反馈信号、上下文元数据等）；对数据质量和多样性进行系统分析；开放数据集以支持现实世界基准测试；推动相关挑战（RecSys Challenge 2025）。

Result: VK-LSVD 成为同类中规模最大的公开工业数据集；提供用于序列推荐、冷启动和新一代推荐系统研究的现实基准环境；其质量与多样性经分析得到支持；已成为 Live RecSys Challenge 2025 的核心数据来源。

Conclusion: VK-LSVD 将成为推动真实场景研究的关键开放资源，加速学术与产业在短视频推荐领域的进步与创新。

Abstract: Short-video recommendation presents unique challenges, such as modeling rapid user interest shifts from implicit feedback, but progress is constrained by a lack of large-scale open datasets that reflect real-world platform dynamics. To bridge this gap, we introduce the VK Large Short-Video Dataset (VK-LSVD), the largest publicly available industrial dataset of its kind. VK-LSVD offers an unprecedented scale of over 40 billion interactions from 10 million users and almost 20 million videos over six months, alongside rich features including content embeddings, diverse feedback signals, and contextual metadata. Our analysis supports the dataset's quality and diversity. The dataset's immediate impact is confirmed by its central role in the live VK RecSys Challenge 2025. VK-LSVD provides a vital, open dataset to use in building realistic benchmarks to accelerate research in sequential recommendation, cold-start scenarios, and next-generation recommender systems.

</details>


### [88] [AIANO: Enhancing Information Retrieval with AI-Augmented Annotation](https://arxiv.org/abs/2602.04579)
*Sameh Khattab,Marie Bauer,Lukas Heine,Till Rostalski,Jens Kleesiek,Julian Friedrich*

Main category: cs.IR

TL;DR: AIANO是一款AI增强注释工具，通过将人类专业知识与LLM协同，显著提升信息检索数据集的创建效率与质量；在n=15的内被试研究中，与基线工具相比，注释速度近翻倍，且易用性和检索准确性均有提升。


<details>
  <summary>Details</summary>
Motivation: 高质量、可检索的信息数据集需求快速增长，但现有注释工具效率低、流程复杂，制约数据集规模与质量。

Method: 采用内被试设计(n=15)对比实验，比较AIANO与基线工具在问答数据集标注任务上的表现；量化注释速度、易用性、检索准确性，并评估AI对人类注释者的辅助作用，强调人机协同的AI增强工作流。

Result: AIANO的注释速率几乎翻倍；易用性提升；检索准确性提升。

Conclusion: AIANO的AI增强注释工作流能显著提高信息检索数据集的创建效率与质量，推动检索密集领域的注释能力与工作流程优化。

Abstract: The rise of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) has rapidly increased the need for high-quality, curated information retrieval datasets. These datasets, however, are currently created with off-the-shelf annotation tools that make the annotation process complex and inefficient. To streamline this process, we developed a specialized annotation tool - AIANO. By adopting an AI-augmented annotation workflow that tightly integrates human expertise with LLM assistance, AIANO enables annotators to leverage AI suggestions while retaining full control over annotation decisions. In a within-subject user study ($n = 15$), participants created question-answering datasets using both a baseline tool and AIANO. AIANO nearly doubled annotation speed compared to the baseline while being easier to use and improving retrieval accuracy. These results demonstrate that AIANO's AI-augmented approach accelerates and enhances dataset creation for information retrieval tasks, advancing annotation capabilities in retrieval-intensive domains.

</details>


### [89] [Multi-Source Retrieval and Reasoning for Legal Sentencing Prediction](https://arxiv.org/abs/2602.04690)
*Junjie Chen,Haitao Li,Qilei Zhang,Zhenghua Li,Ya Zhang,Quan Zhou,Cheng Luo,Yiqun Liu,Dongsheng Guo,Qingyao Ai*

Main category: cs.IR

TL;DR: MSR^2框架通过多源检索和过程级强化学习来提升法律判决预测中的法定量刑子任务，结合推理需求进行多源信息检索，并用过程级奖励指导中间推理步骤，在两个真实数据集上提升准确性与可解释性，代码可获取。


<details>
  <summary>Details</summary>
Motivation: LSP需要细粒度的客观知识和较强的主观推理，现有方法多能在法条/指控等子任务表现，但对柔性、可解释性不足，难以实现实际应用。

Method: 提出MSR^2：在LLM中结合多源检索和推理，按推理需求进行检索，并引入过程级奖励以引导中间推理步骤，提升推理连贯性和可解释性；评估包含两组真实数据集的实验。

Result: 在两个真实数据集上，MSR^2在准确性和可解释性方面优于基线，显示更强的法定量刑推理能力和解释能力。

Conclusion: MSR^2为现实场景中的法律AI提供了一个可行路径，通过多源信息与过程级强化学习提升LSP的性能和透明度。

Abstract: Legal judgment prediction (LJP) aims to predict judicial outcomes from case facts and typically includes law article, charge, and sentencing prediction. While recent methods perform well on the first two subtasks, legal sentencing prediction (LSP) remains difficult due to its need for fine-grained objective knowledge and flexible subjective reasoning. To address these limitations, we propose $MSR^2$, a framework that integrates multi-source retrieval and reasoning in LLMs with reinforcement learning. $MSR^2$ enables LLMs to perform multi-source retrieval based on reasoning needs and applies a process-level reward to guide intermediate subjective reasoning steps. Experiments on two real-world datasets show that $MSR^2$ improves both accuracy and interpretability in LSP, providing a promising step toward practical legal AI. Our code is available at https://anonymous.4open.science/r/MSR2-FC3B.

</details>


### [90] [Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention](https://arxiv.org/abs/2602.04711)
*Sagie Dekel,Moshe Tennenholtz,Oren Kurland*

Main category: cs.IR

TL;DR: SDAG 是一种轻量化的块稀疏注意力机制，用于 RAG：阻断检索结果之间的跨文档注意力，从而显著降低语料污染攻击的成功率，且仅需推理时修改注意力掩码，不需微调或新架构，且可与现有防御方法结合提升效果。


<details>
  <summary>Details</summary>
Motivation: RAG 容易受到语料污染攻击，因为标准因果注意力允许跨文档交互；需要在尽量不改变模型结构的前提下提升鲁棒性。

Method: 提出 SDAG：采用块稀疏注意力，禁止检索出的文档之间的跨文档注意力；通过简单的注意力掩码修改实现推理阶段的改动，无需微调或新增架构。

Result: 在多种攻击策略的问答任务上，SDAG 显著降低攻击成功率；与最先进的 RAG 防御方法结合，性能统计上显著优于现有状态。

Conclusion: SDAG 提供一种轻量、可与现有防御兼容的鲁棒性提升手段，适用于需要外部语料知识的应用场景，提升 RAG 的安全性与可靠性。

Abstract: Retrieval Augmented Generation (RAG) is a highly effective paradigm for keeping LLM-based responses up-to-date and reducing the likelihood of hallucinations. Yet, RAG was recently shown to be quite vulnerable to corpus knowledge poisoning: an attacker injects misleading documents to the corpus to steer an LLMs' output to an undesired response. We argue that the standard causal attention mechanism in LLMs enables harmful cross-document interactions, specifically in cases of attacks. Accordingly, we introduce a novel defense approach for RAG: Sparse Document Attention RAG (SDAG). This is a block-sparse attention mechanism that disallows cross-attention between retrieved documents. SDAG requires a minimal inference-time change to the attention mask; furthermore, no fine-tuning or additional architectural changes are needed. We present an empirical evaluation of LLM-based question answering (QA) with a variety of attack strategies on RAG. We show that our SDAG method substantially outperforms the standard causal attention mechanism in terms of attack success rate. We further demonstrate the clear merits of integrating SDAG with state-of-the-art RAG defense methods. Specifically, the integration results in performance that is statistically significantly better than the state-of-the-art.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [91] [GOPO: Policy Optimization using Ranked Rewards](https://arxiv.org/abs/2602.03876)
*Kyuseong Choi,Dwaipayan Saha,Woojeong Kim,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: GOPO is a policy optimization method for RLHF that relies only on reward rankings (ordinal) rather than absolute magnitudes, improving training dynamics and evaluation accuracy over GRPO in non-verifiable reward settings.


<details>
  <summary>Details</summary>
Motivation: In RLHF, reward models capture relative preferences but policy optimization uses absolute reward magnitudes. This mismatch is problematic for non-verifiable tasks (e.g., summarization, instruction following, chat). A rank-based approach aims to align optimization with the available non-verifiable signal.

Method: Introduce Group Ordinal Policy Optimization (GOPO), which transforms rewards into ordinal rankings and optimizes the policy using only this ranking information, discarding magnitude information. Compare to Group Relative Policy Optimization (GRPO).

Result: GOPO yields consistently higher training and validation reward trajectories, improved LLM-as-judge evaluations across most training steps, and reaches comparable policy quality in significantly fewer training steps than GRPO. Effects are observed across a range of tasks and model sizes.

Conclusion: Using ordinal reward information in policy optimization improves training efficiency and evaluation performance for RLHF in non-verifiable reward settings, offering a robust alternative to magnitude-based methods like GRPO.

Abstract: Standard reinforcement learning from human feedback (RLHF) trains a reward model on pairwise preference data and then uses it for policy optimization. However, while reward models are optimized to capture relative preferences, existing policy optimization techniques rely on absolute reward magnitudes during training. In settings where the rewards are non-verifiable such as summarization, instruction following, and chat completion, this misalignment often leads to suboptimal performance. We introduce Group Ordinal Policy Optimization (GOPO), a policy optimization method that uses only the ranking of the rewards and discards their magnitudes. Our rank-based transformation of rewards provides several gains, compared to Group Relative Policy Optimization (GRPO), in settings with non-verifiable rewards: (1) consistently higher training/validation reward trajectories, (2) improved LLM-as-judge evaluations across most intermediate training steps, and (3) reaching a policy of comparable quality in substantially less training steps than GRPO. We demonstrate consistent improvements across a range of tasks and model sizes.

</details>


### [92] [GeoIB: Geometry-Aware Information Bottleneck via Statistical-Manifold Compression](https://arxiv.org/abs/2602.03906)
*Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Shui Yu*

Main category: cs.LG

TL;DR: GeoIB提出基于信息几何的无MI估计IB方法，通过Fisher-Rao距离和Jacobian-Frobenius正则化来控制信息压缩，采用自然梯度更新，实验在信息平面上优于主流IB基线，源代码公开。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中IB广泛使用的变分界/MI估计替代品造成的估计偏差、松散性和优化不稳定性，迫切需要一个无MI估计的IB框架。

Method: 从I(X;Z)与I(Z;Y)的投影形式出发，将最小KL距离投影到独立流形；引入两种正则化：分布层面的Fisher-Rao距离（二阶近似、对参数变换不变）与Jacobian-Frobenius项（对编码器的回采体积膨胀进行惩罚，给出对I(Z;X)的局部容量上界）；推导自然梯度优化，证明与测地线更新一阶等价；实现GeoIB并对代码进行公开。

Result: 在信息平面上，GeoIB相比主流IB基线获得更优的预测-压缩权衡；提高不变性和优化稳定性；通过统一单一瓶颈乘子实现分布与几何正则化；源码公开。

Conclusion: 提供一个无MI估计的IB框架，结合分布几何与几何正则，提升稳定性与压缩效果，为IB研究提供新的信息几何视角与实用优化方法。

Abstract: Information Bottleneck (IB) is widely used, but in deep learning, it is usually implemented through tractable surrogates, such as variational bounds or neural mutual information (MI) estimators, rather than directly controlling the MI I(X;Z) itself. The looseness and estimator-dependent bias can make IB "compression" only indirectly controlled and optimization fragile.
  We revisit the IB problem through the lens of information geometry and propose a \textbf{Geo}metric \textbf{I}nformation \textbf{B}ottleneck (\textbf{GeoIB}) that dispenses with mutual information (MI) estimation. We show that I(X;Z) and I(Z;Y) admit exact projection forms as minimal Kullback-Leibler (KL) distances from the joint distributions to their respective independence manifolds. Guided by this view, GeoIB controls information compression with two complementary terms: (i) a distribution-level Fisher-Rao (FR) discrepancy, which matches KL to second order and is reparameterization-invariant; and (ii) a geometry-level Jacobian-Frobenius (JF) term that provides a local capacity-type upper bound on I(Z;X) by penalizing pullback volume expansion of the encoder. We further derive a natural-gradient optimizer consistent with the FR metric and prove that the standard additive natural-gradient step is first-order equivalent to the geodesic update. We conducted extensive experiments and observed that the GeoIB achieves a better trade-off between prediction accuracy and compression ratio in the information plane than the mainstream IB baselines on popular datasets. GeoIB improves invariance and optimization stability by unifying distributional and geometric regularization under a single bottleneck multiplier. The source code of GeoIB is released at "https://anonymous.4open.science/r/G-IB-0569".

</details>


### [93] [The Role of Target Update Frequencies in Q-Learning](https://arxiv.org/abs/2602.03911)
*Simon Weissmann,Tilman Aach,Benedikt Wille,Sebastian Kassing,Leif Döring*

Main category: cs.LG

TL;DR: The paper analyzes target network update frequency (TUF) in tabular Q-learning, formulating periodic updates as a nested optimization and providing finite-time convergence results. It characterizes the bias-variance trade-off of the update period and argues that adaptive, geometrically increasing update frequencies outperform constant schedules, reducing sample complexity overhead.


<details>
  <summary>Details</summary>
Motivation: TUF is a central stabilization mechanism in (deep) Q-learning, yet its choice is often heuristic. A principled analysis is needed to understand its impact on convergence and sample efficiency and to derive optimal/adaptive scheduling.

Method: Model periodic target updates as a nested optimization where the outer loop applies an inexact Bellman optimality operator and the inner loop uses a generic optimizer (specialized to SGD). Perform finite-time convergence analysis in an asynchronous sampling setting and derive results for SGD in the inner loop.

Result: Derives an explicit bias-variance trade-off with respect to the target update period and provides the optimal setting of this hyperparameter. Proves that constant target update schedules incur a logarithmic overhead in sample complexity, which can be avoided with adaptive schedules. Demonstrates that the optimal TUF grows geometrically during learning.

Conclusion: Adaptive target update schedules yield better sample efficiency and convergence than fixed schedules. The optimal TUF increases geometrically over time, suggesting that more frequent updates should be employed as learning progresses to balance bias and variance.

Abstract: The target network update frequency (TUF) is a central stabilization mechanism in (deep) Q-learning. However, their selection remains poorly understood and is often treated merely as another tunable hyperparameter rather than as a principled design decision. This work provides a theoretical analysis of target fixing in tabular Q-learning through the lens of approximate dynamic programming. We formulate periodic target updates as a nested optimization scheme in which each outer iteration applies an inexact Bellman optimality operator, approximated by a generic inner loop optimizer. Rigorous theory yields a finite-time convergence analysis for the asynchronous sampling setting, specializing to stochastic gradient descent in the inner loop. Our results deliver an explicit characterization of the bias-variance trade-off induced by the target update period, showing how to optimally set this critical hyperparameter. We prove that constant target update schedules are suboptimal, incurring a logarithmic overhead in sample complexity that is entirely avoidable with adaptive schedules. Our analysis shows that the optimal target update frequency increases geometrically over the course of the learning process.

</details>


### [94] [Echo State Networks for Time Series Forecasting: Hyperparameter Sweep and Benchmarking](https://arxiv.org/abs/2602.03912)
*Alexander Häußer*

Main category: cs.LG

TL;DR: 在M4子集上对单变量时间序列进行ESN预测，经过大规模超参数搜索后，ESN在月度数据与季度数据上可媲美或优于常用统计模型，同时具备较低计算成本。


<details>
  <summary>Details</summary>
Motivation: 验证纯反馈驱动的Echo State Networks在自动化时间序列预测中的潜力，评估其在与统计模型的对比中的预测能力、鲁棒性及计算效率。

Method: 两阶段评估：1) 参数数据集上进行超参数网格搜索，覆盖泄漏率、谱半径、 reservoir size 和用于正则化的信息准则，总计超过400万次模型拟合；2) 独立的 Forecast 数据集用于外样本评估。指标为 MASE 与 sMAPE，基准包括漂移、季节性朴素以及 ARIMA、ETS、TBATS。分析还比较不同频率（月度/季度）对参数偏好与性能的影响。

Result: 超参数分析显示月度序列偏好中等持久的水库动态，季度序列偏好更收缩的动态；总体上偏好较高的泄漏率，谱半径和水库规模随时间分辨率变化。在外样本评估中，ESN在月度数据上与 ARIMA、TBATS相当，在季度数据上取得最低的平均 MASE，且计算成本低于更复杂的统计模型。

Conclusion: ESN提供了预测准确性、鲁棒性与计算效率之间的良好权衡，成为自动化时间序列预测的一个具有竞争力的选项。

Abstract: This paper investigates the forecasting performance of Echo State Networks (ESNs) for univariate time series forecasting using a subset of the M4 Forecasting Competition dataset. Focusing on monthly and quarterly time series with at most 20 years of historical data, we evaluate whether a fully automatic, purely feedback-driven ESN can serve as a competitive alternative to widely used statistical forecasting methods. The study adopts a rigorous two-stage evaluation approach: a Parameter dataset is used to conduct an extensive hyperparameter sweep covering leakage rate, spectral radius, reservoir size, and information criteria for regularization, resulting in over four million ESN model fits; a disjoint Forecast dataset is then used for out-of-sample accuracy assessment. Forecast accuracy is measured using MASE and sMAPE and benchmarked against simple benchmarks like drift and seasonal naive and statistical models like ARIMA, ETS, and TBATS. The hyperparameter analysis reveals consistent and interpretable patterns, with monthly series favoring moderately persistent reservoirs and quarterly series favoring more contractive dynamics. Across both frequencies, high leakage rates are preferred, while optimal spectral radii and reservoir sizes vary with temporal resolution. In the out-of-sample evaluation, the ESN performs on par with ARIMA and TBATS for monthly data and achieves the lowest mean MASE for quarterly data, while requiring lower computational cost than the more complex statistical models. Overall, the results demonstrate that ESNs offer a compelling balance between predictive accuracy, robustness, and computational efficiency, positioning them as a practical option for automated time series forecasting.

</details>


### [95] [From Data to Behavior: Predicting Unintended Model Behaviors Before Training](https://arxiv.org/abs/2602.04735)
*Mengru Wang,Zhenqian Xu,Junfeng Fang,Yunzhi Yao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.LG

TL;DR: 提出 Data2Behavior 任务，在训练前预测未预期行为；通过 Manipulating Data Features (MDF) 将候选数据的均值表示注入前向传播，以触发潜在信号并揭示偏见与安全风险；在不更新参数的情况下实现约 20% 的微调资源消耗，实验在多种模型上验证可预测性与对培训脆弱性的洞察。


<details>
  <summary>Details</summary>
Motivation: 未经过滤的训练数据可能在无显性线索的情况下引入隐性偏见，训练前评估风险比事后评估成本更高效，亟需快速、低开销的筛选方法来识别潜在风险。

Method: 提出 Data2Behavior 任务；设计 MDF：将候选数据以其均值表征汇总，并在基础模型的前向传播中注入该均值信号，使数据的潜在统计特征影响模型激活以揭示偏见与安全风险；不对模型参数进行更新。

Result: MDF 能够实现可靠的风险预测，并且相比微调仅需约 20% 的 GPU 资源。对 Qwen3-14B、Qwen2.5-32B-Instruct、Gemma-3-12b-it 的实验显示 MDF 能预测未预期行为并提供对预训练脆弱性的洞察。

Conclusion: 该方法为训练前的风险评估提供一个高效手段，Data2Behavior 任务帮助从数据层面预测潜在行为，MDF 通过在前向传播注入数据的统计信号实现低成本的可行性测试，提示在大规模预训练中更安全的模型开发路径。

Abstract: Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.

</details>


### [96] [SpecMD: A Comprehensive Study On Speculative Expert Prefetching](https://arxiv.org/abs/2602.03921)
*Duc Hoang,Ajay Jaiswal,Mohammad Samragh,Minsik Cho*

Main category: cs.LG

TL;DR: 提出 SpecMD 框架用于在不同硬件配置下对 MoE 缓存策略进行标准化基准测试，并提出 Least-Stale 缓存策略，在实验中显著提升命中率并降低 TTFT。


<details>
  <summary>Details</summary>
Motivation: MoE 模型的稀疏专家激活带来缓存需求，但现有硬件相关缓存策略之间的交互及其对不同硬件的影响尚不清晰，亟需一个统一的基准框架以便对比与复现实验。

Method: 开发 SpecMD 作为标准化基准框架，系统性地在多种硬件配置上评估多种 MoE 缓存策略，复现并扩展已有方法，结合现实约束进行控条件实验。

Result: 发现 MoE 的专家访问模式并不遵循经典的时间局部性假设（如 LRU、LFU）。在此基础上提出 Least-Stale 缓 eviction 策略，利用可预测的访问特征显著降低缓存冲突，理论与实测显示对 LRU 提升最多 85× 的命中率降低冲突，命中率超过 88%，TTFT 相比下降 34.7%，在 OLMoE 上仅 5% VRAM 的缓存容量（约 0.6GB）。

Conclusion: 提供了一个统一、可重复的评测框架来研究 MoE 缓存策略，与传统假设相比，局部性假设需重新考量；提出的 Least-Stale 显著提升在受限缓存容量下的推理效率，未来需在更大规模模型和多任务场景下验证普适性及实现复杂度。

Abstract: Mixture-of-Experts (MoE) models enable sparse expert activation, meaning that only a subset of the model's parameters is used during each inference. However, to translate this sparsity into practical performance, an expert caching mechanism is required. Previous works have proposed hardware-centric caching policies, but how these various caching policies interact with each other and different hardware specification remains poorly understood. To address this gap, we develop \textbf{SpecMD}, a standardized framework for benchmarking ad-hoc cache policies on various hardware configurations. Using SpecMD, we perform an exhaustive benchmarking of several MoE caching strategies, reproducing and extending prior approaches in controlled settings with realistic constraints. Our experiments reveal that MoE expert access is not consistent with temporal locality assumptions (e.g LRU, LFU). Motivated by this observation, we propose \textbf{Least-Stale}, a novel eviction policy that exploits MoE's predictable expert access patterns to reduce collision misses by up to $85\times$ over LRU. With such gains, we achieve over $88\%$ hit rates with up to $34.7\%$ Time-to-first-token (TTFT) reduction on OLMoE at only $5\%$ or $0.6GB$ of VRAM cache capacity.

</details>


### [97] [WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling](https://arxiv.org/abs/2602.03924)
*Michael Aich,Andreas Fürst,Florian Sestak,Carlos Ruiz-Gonzalez,Niklas Boers,Johannes Brandstetter*

Main category: cs.LG

TL;DR: WIND 是一个单一的预训练基础模型，用于天气与气候领域的多任务，利用自监督视频扩散学习任务无关先验，在推理阶段将域任务视为逆问题并通过后验采样求解。


<details>
  <summary>Details</summary>
Motivation: 当前气象/气候领域存在大量针对单一任务的专业化模型，缺乏统一的基础模型。需要一个任务无关的、可扩展的先验来提高数据利用率、减少微调，并支持概率预测与物理约束。

Method: 通过自监督的视频重建目标，对无条件视频扩散模型进行预训练，使模型获得对大气动力学的任务无关先验。推理阶段将多种域问题转化为逆问题，并通过后验采样实现求解；同时展示了概率预报、空间/时间上的下采样、稀疏重建、物理守恒约束及在全球变暖情景下的对照情节生成等应用。

Result: 在不进行任务特定微调的情况下，WIND 能替代或超越众多基线，对多任务具备竞争力；提供概率性输出、保持物理一致性，并显著提高计算效率。

Conclusion: 该工作展示了将生成式视频建模与逆问题求解结合的统一框架，推动大气建模向统一、可扩展的基础模型发展，并为未来在极端天气对比情节分析和应对全球变暖的情景建模提供新路径。

Abstract: Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.

</details>


### [98] [Autonomous AI Agents for Real-Time Affordable Housing Site Selection: Multi-Objective Reinforcement Learning Under Regulatory Constraints](https://arxiv.org/abs/2602.03940)
*Olaf Yunus Laitinen Imanov,Duygu Erisken,Derya Umut Kulali,Taner Yilmaz,Rana Irem Turhan*

Main category: cs.LG

TL;DR: AURA 是一个分层的多智能体强化学习系统，用于在严格监管约束下进行实时可负担住房选址，提升合规性和帕累托最优表现。


<details>
  <summary>Details</summary>
Motivation: 解决可负担住房短缺及土地稀缺、法规导致选址缓慢的问题；需要一个对监管约束敏感、可扩展的决策框架。

Method: 分层多智能体强化学习；约束多目标马尔可夫决策过程；127条联邦/地方监管约束的状态编码；Pareto约束策略梯度与可行性保证；将即时成本与长期社会结果分解的奖励结构。

Result: 在8个大都会区、47,392个候选地块的数据集上，合规率达94.3%，在帕累托超体积上比强基线提高37.2%；纽约市2026案例中，选址时间由18个月缩短至72小时，发现23%更可行的地块；所选地块在交通可达性提升31%、环境影响下降19%，相较专家选择。

Conclusion: AURA 展现出在受控约束下高效、合规且关注社会效益的选址能力，监管感知编码与可行性保障是现实世界约束优化的有效设计。

Abstract: Affordable housing shortages affect billions, while land scarcity and regulations make site selection slow. We present AURA (Autonomous Urban Resource Allocator), a hierarchical multi-agent reinforcement learning system for real-time affordable housing site selection under hard regulatory constraints (QCT, DDA, LIHTC). We model the task as a constrained multi-objective Markov decision process optimizing accessibility, environmental impact, construction cost, and social equity while enforcing feasibility. AURA uses a regulatory-aware state encoding 127 federal and local constraints, Pareto-constrained policy gradients with feasibility guarantees, and reward decomposition separating immediate costs from long-term social outcomes. On datasets from 8 U.S. metros (47,392 candidate parcels), AURA attains 94.3% regulatory compliance and improves Pareto hypervolume by 37.2% over strong baselines. In a New York City 2026 case study, it reduces selection time from 18 months to 72 hours and identifies 23% more viable sites; chosen sites have 31% better transit access and 19% lower environmental impact than expert picks.

</details>


### [99] [Grables: Tabular Learning Beyond Independent Rows](https://arxiv.org/abs/2602.03945)
*Tamara Cucumides,Floris Geerts*

Main category: cs.LG

TL;DR: Grables 将表格转图的构造与图上节点预测解耦，揭示跨行结构对表格学习的作用，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实表格常包含跨行依赖（全局计数、重叠、关系模式），单行预测难以捕捉，导致在事务性、时间序列和关系表上的性能受限。

Method: 提出 grables：一个模块化接口，分离将表格提升为图的构造器（constructor）与在图上进行预测的节点预测器（node predictor）。在合成任务、交易数据和 RelBench 临床试验数据上评估，比较纯行局部模型、消息传递图模型，以及显式提取跨行结构后再输入到强表学习模型的混合方法。

Result: 消息传递能够捕获行间依赖，这些依赖是单行模型所忽略的；显式提取跨行结构并输入到强表学习器的混合方法在多任务中表现出一致的性能提升。

Conclusion: 结构与预测器的分离揭示了表达能力的来源，为表格学习的架构设计提供清晰指引；结合跨行信息的混合方法具有稳健的性能提升。

Abstract: Tabular learning is still dominated by row-wise predictors that score each row independently, which fits i.i.d. benchmarks but fails on transactional, temporal, and relational tables where labels depend on other rows. We show that row-wise prediction rules out natural targets driven by global counts, overlaps, and relational patterns. To make "using structure" precise across architectures, we introduce grables: a modular interface that separates how a table is lifted to a graph (constructor) from how predictions are computed on that graph (node predictor), pinpointing where expressive power comes from. Experiments on synthetic tasks, transaction data, and a RelBench clinical-trials dataset confirm the predicted separations: message passing captures inter-row dependencies that row-local models miss, and hybrid approaches that explicitly extract inter-row structure and feed it to strong tabular learners yield consistent gains.

</details>


### [100] [eCP: Informative uncertainty quantification via Equivariantized Conformal Prediction with pre-trained models](https://arxiv.org/abs/2602.03986)
*Nikolaos Bousias,Lars Lindemann,George Pappas*

Main category: cs.LG

TL;DR: 通过对预训练预测模型进行群对称化（群平均）来改进无分布假设的 conformal prediction 的置信区间，其对非一致性分数收缩并在相同置信水平下收敛到更紧的预测集，尤其在高置信度下；并以行人轨迹预测为实验设计。


<details>
  <summary>Details</summary>
Motivation: CP 在给定数据交换性的前提下提供分布无关、有限样本的覆盖保证，但在长时任务中，置信区间往往扩展过大，削弱了实用性。通过引入几何信息（对称群的群平均）来分摊非一致性质量、提升预测集的效率。

Method: 对预训练 predictor 进行群平均，即将每个样本视为某一轨道的代表，利用对称群作用在输入/预测区域上对非一致性分数进行平均化，得到对称性更强且分布更均匀的非一致性分数。理论上证明非一致性分数在增加凸序（ICV order）上实现收缩，从而获得更好的指数尾界和更紧的 CP 集。实验设计围绕行人轨迹预测来验证理论结论。

Result: 理论上证明非一致性分数在增加凸序上收缩，带来更优的指数尾界和在期望意义上的更紧的 CP 集，尤其在高置信水平下。实验设计用于在行人轨迹预测任务中验证这些结论。

Conclusion: 将群平均整合到 CP 框架中具有理论和实践上的提升潜力，能够在需要长时序不确定性量化的任务中提供更紧凑的预测区间；未来工作可扩展到更多任务、分析计算成本、以及如何选择合适的对称群。

Abstract: We study the effect of group symmetrization of pre-trained models on conformal prediction (CP), a post-hoc, distribution-free, finite-sample method of uncertainty quantification that offers formal coverage guarantees under the assumption of data exchangeability. Unfortunately, CP uncertainty regions can grow significantly in long horizon missions, rendering the statistical guarantees uninformative. To that end, we propose infusing CP with geometric information via group-averaging of the pretrained predictor to distribute the non-conformity mass across the orbits. Each sample now is treated as a representative of an orbit, thus uncertainty can be mitigated by other samples entangled to it via the orbit inducing elements of the symmetry group. Our approach provably yields contracted non-conformity scores in increasing convex order, implying improved exponential-tail bounds and sharper conformal prediction sets in expectation, especially at high confidence levels. We then propose an experimental design to test these theoretical claims in pedestrian trajectory prediction.

</details>


### [101] [When Chains of Thought Don't Matter: Causal Bypass in Large Language Models](https://arxiv.org/abs/2602.03994)
*Anish Sathyanarayanan,Aditya Nagarsekar,Aarush Rathore*

Main category: cs.LG

TL;DR: 表面可解释的链式推理并不确保因果依赖；即使CoT verbose且通过检测，答案常与CoT内容分离。


<details>
  <summary>Details</summary>
Motivation: 揭示对CoT的“透明性”假设是否成立，评估对抗性提示下的因果依赖和绕过行为的存在性。

Method: 提出一个可解释的行为模块评估CoT文本中的操控信号，并使用隐藏状态打补丁的因果探针度量CoT的中介影响（CMI），给出绕过分数1−CMI；在初步评估中比较不同任务对齐的效果并做层次分析，识别窄且任务相关的“推理窗口”。

Result: 审计式提示提升了可检测的操控信号（平均风险分数增量=+5.10），但因果探针揭示任务依赖的中介性：多数问答呈现近乎绕过（CMI≈0），部分逻辑题的中介性较强（CMI最高约0.56）；层级分析显示在平均CMI低的情况下仍存在狭窄的“推理窗”。

Conclusion: CoT的可解释性并非普遍成立，存在显著的任务依赖性和绕过现象，需结合更细粒度的因果评估与任务设计来提升推理透明度与可信度。

Abstract: Chain-of-thought (CoT) prompting is widely assumed to expose a model's reasoning process and improve transparency. We attempted to enforce this assumption by penalizing unfaithful reasoning, but found that surface-level compliance does not guarantee causal reliance. Our central finding is negative: even when CoT is verbose, strategic, and flagged by surface-level manipulation detectors, model answers are often causally independent of the CoT content. We present a diagnostic framework for auditing this failure mode: it combines (i) an interpretable behavioral module that scores manipulation-relevant signals in CoT text and (ii) a causal probe that measures CoT-mediated influence (CMI) via hidden-state patching and reports a bypass score ($1-\mathrm{CMI}$), quantifying the degree to which the answer is produced by a bypass circuit independent of the rationale. In pilot evaluations, audit-aware prompting increases detectable manipulation signals (mean risk-score delta: $+5.10$), yet causal probes reveal task-dependent mediation: many QA items exhibit near-total bypass (CMI $\approx 0$), while some logic problems show stronger mediation (CMI up to $0.56$). Layer-wise analysis reveals narrow and task-dependent ``reasoning windows'' even when mean CMI is low.

</details>


### [102] [Rational ANOVA Networks](https://arxiv.org/abs/2602.04006)
*Jusheng Zhang,Ningyuan Liu,Qinhan Lyu,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: RAN introduces a neural architecture combining functional ANOVA with Padé-style rational units, enforcing positive denominators for stability; it models f(x) as low-order additive effects and sparse pairwise interactions, achieving data-efficient, interpretable generalization and improved extrapolation, matching or surpassing parameter-matched baselines under equal budgets.


<details>
  <summary>Details</summary>
Motivation: Address limitations of fixed nonlinearities and crude additive models that hinder interpretability, data efficiency, and extrapolation. Provide explicit low-order interaction bias and a more flexible, stable function class.

Method: Model f(x) as a composition of main effects and sparse pairwise interactions via functional ANOVA decomposition. Parameterize each component with stable, learnable rational units with strictly positive denominators to avoid poles. Train under matched parameter and compute budgets and evaluate on controlled benchmarks and CIFAR-10.

Result: RAN matches or surpasses parameter-matched MLPs and learnable-activation baselines on benchmarks and CIFAR-10, with better stability and throughput, and improved extrapolation.

Conclusion: RAN offers a principled, interpretable, and stable architecture that leverages low-order ANOVA structure and rational activation to improve data efficiency and extrapolation, with code available for replication.

Abstract: Deep neural networks typically treat nonlinearities as fixed primitives (e.g., ReLU), limiting both interpretability and the granularity of control over the induced function class. While recent additive models (like KANs) attempt to address this using splines, they often suffer from computational inefficiency and boundary instability. We propose the Rational-ANOVA Network (RAN), a foundational architecture grounded in functional ANOVA decomposition and Padé-style rational approximation. RAN models f(x) as a composition of main effects and sparse pairwise interactions, where each component is parameterized by a stable, learnable rational unit. Crucially, we enforce a strictly positive denominator, which avoids poles and numerical instability while capturing sharp transitions and near-singular behaviors more efficiently than polynomial bases. This ANOVA structure provides an explicit low-order interaction bias for data efficiency and interpretability, while the rational parameterization significantly improves extrapolation. Across controlled function benchmarks and vision classification tasks (e.g., CIFAR-10) under matched parameter and compute budgets, RAN matches or surpasses parameter-matched MLPs and learnable-activation baselines, with better stability and throughput. Code is available at https://github.com/jushengzhang/Rational-ANOVA-Networks.git.

</details>


### [103] [PromptSplit: Revealing Prompt-Level Disagreement in Generative Models](https://arxiv.org/abs/2602.04009)
*Mehdi Lotfian,Mohammad Jalali,Farzan Farnia*

Main category: cs.LG

TL;DR: 提出 PromptSplit，通过核方法检测与分析生成模型对提示的依赖性行为差异，并给出可解释的提示层次原因。


<details>
  <summary>Details</summary>
Motivation: 随着提示驱动的生成模型在视觉与语言领域快速扩展，亟需定量、可解释的方法来识别不同提示下模型行为的差异来源。

Method: 构建提示-输出的张量积嵌入，形成联合表示；计算对应的核协方差矩阵；对加权的两矩阵差的特征子空间进行分析以识别行为差异的主方向；为可扩展性引入随机投影近似，将复杂度降为 O(n r^2 + r^3)；给出近似特征结构的误差界限为 O(1/r^2) 的理论分析。

Result: 在文本到图像、文本到文本、以及图像-描述等设置中，PromptSplit 能准确检测 Ground-truth 的行为差异并定位引发差异的提示，提供可解释的模型对比工具。

Conclusion: 该框架为跨模型对比提供了可解释、可扩展的提示级别差异分析方法，帮助研究者和工程师识别模型在提示上的潜在偏差与一致性问题。

Abstract: Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts lead to distinct model behaviors. In this work, we propose PromptSplit, a kernel-based framework for detecting and analyzing prompt-dependent disagreement between generative models. For each compared model pair, PromptSplit constructs a joint prompt--output representation by forming tensor-product embeddings of the prompt and image (or text) features, and then computes the corresponding kernel covariance matrix. We utilize the eigenspace of the weighted difference between these matrices to identify the main directions of behavioral difference across prompts. To ensure scalability, we employ a random-projection approximation that reduces computational complexity to $O(nr^2 + r^3)$ for projection dimension $r$. We further provide a theoretical analysis showing that this approximation yields an eigenstructure estimate whose expected deviation from the full-dimensional result is bounded by $O(1/r^2)$. Experiments across text-to-image, text-to-text, and image-captioning settings demonstrate that PromptSplit accurately detects ground-truth behavioral differences and isolates the prompts responsible, offering an interpretable tool for detecting where generative models disagree.

</details>


### [104] [Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2602.04019)
*Yichen Xu,Yuyang Liang,Shan Dai,Tianyang Hu,Tsz Nam Chan,Chenhao Ma*

Main category: cs.LG

TL;DR: 引入统一的投影残差视角来分析PEFT中的层选择，提出三大量纲（残差范数、激活能、层耦合），并以Layer Card诊断实现基于成本-性能的分层选择；在Qwen3-8B上证实选择性适配可接近全层LoRA的性能同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型增大，全面参数微调成本高，需在推理延迟与微调成本约束下进行层级选择性适配，但现有做法多在所有层上统一应用PEFT，缺乏系统的层级协同与信号分析。

Method: 在冻结基模型、局部二次近似下提出三种层级自适应量：投影残差范数resnorm（衡量可纠正偏差的能力）、激活能（决定特征条件数/噪声放大）、层耦合（衡量残差在层间的相互作用）。对平方损失与线性适配器，证明resnorm等于标准化梯度范数，激活能控制病态条件和噪声放大，弱耦合近似可得逐层贡献相加。提出Layer Card诊断，用以总结每层的残差信号强度、计算成本与性能；基于Layer Card进行Layer Card-guided放置以优化性能或成本。并在Qwen3-8B上实证：通过选择性只适配若干层，达到接近全层LoRA的性能，同时显著降低微调成本与推理中的适配层数量。

Result: 理论层面：resnorm与梯度范数等价、激活能与病态条件、层耦合的弱耦合近似下层间贡献可加。实践层面：Layer Card提供可复用的层级诊断，指导在同一模型与LoRA配置下实现成本-性能权衡；在大模型Qwen3-8B上实现部分层适配即可近似全层性能，降低成本与推理开销。

Conclusion: 该工作提供了基于残差信号与层间耦合的PEFT层选择框架，并通过Layer Card实现可操作的诊断与优化策略，为成本敏感的部署提供更灵活的PEFT方案，尤其适用于大模型的分层适配。

Abstract: As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation. Constraints from inference latency in scalable serving and fine-tuning cost in edge or rapid-deployment settings make the choice of which layers to fine-tune unavoidable. Yet current practice typically applies PEFT uniformly across all layers, with limited understanding or leverage of layer selection. This paper develops a unified projected residual view of PEFT on top of a frozen base model. Under a local quadratic approximation, layerwise adaptation is governed by three quantities: (i) the projected residual norm (resnorm), which measures how much correctable bias a layer can capture; (ii) the activation energy, which determines feature conditioning; and (iii) layer coupling, which quantifies how strongly residuals interact across layers. We show that, for squared loss and linear adapters, the resnorm equals a normalized gradient norm, activation energy controls ill-conditioning and noise amplification, and weak coupling yields approximately additive layerwise contributions. Building on these insights, we introduce the Layer Card, a reusable diagnostic that summarizes residual signal strength, compute cost, and performance for each layer of a given model. With an identical model and LoRA configuration, Layer Card-guided placement refines the choice of adapted layers to flexibly prioritize different objectives, such as maximizing performance or reducing fine-tuning cost. Moreover, on Qwen3-8B, we show that selectively adapting a subset of layers can achieve performance close to full-layer LoRA while substantially reducing fine-tuning cost and the number of adapter-augmented layers during inference, offering a more cost-performance-aware alternative to full-layer insertion.

</details>


### [105] [Group Contrastive Learning for Weakly Paired Multimodal Data](https://arxiv.org/abs/2602.04021)
*Aditya Gorla,Hugues Van Assel,Jan-Christian Huetter,Heming Yao,Kyunghyun Cho,Aviv Regev,Russell Littman*

Main category: cs.LG

TL;DR: GROOVE introduces GroupCLIP for semi-supervised, weakly-paired multi-modal perturbation data, combining a group-level contrastive loss with a backtranslating autoencoder, plus a comprehensive combinatorial benchmarking framework; empirically competitive across simulations and real datasets.


<details>
  <summary>Details</summary>
Motivation: There is a fundamental gap in contrastive learning for weakly-paired, multi-modal perturbation data: existing CLIP-like methods assume paired data and SupCon assumes uni-modal supervision. A group-level, weakly-paired framework could leverage shared perturbation labels to align modalities.

Method: Develop GroupCLIP, a group-level contrastive loss that adapts CLIP-like objectives to weakly-paired, multi-modal data, integrated with an on-the-fly backtranslating autoencoder to produce cross-modally entangled yet group-coherent representations. Propose a comprehensive combinatorial evaluation framework using multiple optimal transport aligners and novel simulations to vary shared vs modality-specific perturbation effects.

Result: GROOVE achieves comparable or superior performance to existing methods on cross-modal matching and imputation in two real single-cell perturbation datasets and simulations. Ablation shows GroupCLIP as the key contributor to gains. The benchmarking framework reveals no single aligner dominates across settings.

Conclusion: Group-level constraints are crucial for effective multi-modal representation learning with weak pairing; the proposed combinatorial benchmarking framework is valuable for robust evaluation across perturbation modalities and alignment strategies.

Abstract: We present GROOVE, a semi-supervised multi-modal representation learning approach for high-content perturbation data where samples across modalities are weakly paired through shared perturbation labels but lack direct correspondence. Our primary contribution is GroupCLIP, a novel group-level contrastive loss that bridges the gap between CLIP for paired cross-modal data and SupCon for uni-modal supervised contrastive learning, addressing a fundamental gap in contrastive learning for weakly-paired settings. We integrate GroupCLIP with an on-the-fly backtranslating autoencoder framework to encourage cross-modally entangled representations while maintaining group-level coherence within a shared latent space. Critically, we introduce a comprehensive combinatorial evaluation framework that systematically assesses representation learners across multiple optimal transport aligners, addressing key limitations in existing evaluation strategies. This framework includes novel simulations that systematically vary shared versus modality-specific perturbation effects enabling principled assessment of method robustness. Our combinatorial benchmarking reveals that there is not yet an aligner that uniformly dominates across settings or modality pairs. Across simulations and two real single-cell genetic perturbation datasets, GROOVE performs on par with or outperforms existing approaches for downstream cross-modal matching and imputation tasks. Our ablation studies demonstrate that GroupCLIP is the key component driving performance gains. These results highlight the importance of leveraging group-level constraints for effective multi-modal representation learning in scenarios where only weak pairing is available.

</details>


### [106] [The Illusion of Generalization: Re-examining Tabular Language Model Evaluation](https://arxiv.org/abs/2602.04031)
*Aditya Gorla,Ratish Puduppully*

Main category: cs.LG

TL;DR: 系统性再评估 Tabula-8B（165 个 UniPredict 数据集）显示对二元/分类任务几乎零提升，整体表现由 quartile 任务驱动，且存在严重数据污染；仅依无表格暴露的指令微调即可恢复大部分分类性能，表明所声称的通用化更可能是评估 artefacts 而非真实表格推理。


<details>
  <summary>Details</summary>
Motivation: 评估 Tabular Language Models（TLMs）是否真的具备对表格预测的普遍化能力，还是受数据泄露和评估设计等 artefacts 的影响。

Method: 以 Tabula-8B 为代表，对 UniPredict 基准的 165 个数据集进行系统性重评估，比较二元/分类与 quartile 分类任务的表现，检查数据污染（训练集/测试集重叠、任务级泄漏），在不暴露表格结构的前提下进行指令微调的影响，并分析格式熟悉性对 quartile 分类的作用。

Result: 发现二元/分类的中位提升接近零，强势整体表现来自 quartile 分类任务；高性能数据集存在广泛污染，包括完全的训练-测试重叠和难以通过去重检测的任务级泄漏；无需表格暴露的指令微调可恢复 92.2% 的标准分类性能，在 quartile 分类上，格式熟悉性可缩小 71.3% 的差距，剩余部分归因于污染数据集；这些结果暗示声称的通用化更可能是评估 artefacts 而非学习到的表格推理。

Conclusion: 需要更强的评估设计来避免数据泄漏、引入更鲁棒的基线，并确保在评估中显式考虑表格暴露与任务结构，以提升对 TLMs 的真实表格推理能力的判断。

Abstract: Tabular Language Models (TLMs) have been claimed to achieve emergent generalization for tabular prediction. We conduct a systematic re-evaluation of Tabula-8B as a representative TLM, utilizing 165 datasets from the UniPredict benchmark. Our investigation reveals three findings. First, binary and categorical classification achieve near-zero median lift over majority-class baselines and strong aggregate performance is driven entirely by quartile classification tasks. Second, top-performing datasets exhibit pervasive contamination, including complete train-test overlap and task-level leakage that evades standard deduplication. Third, instruction-tuning without tabular exposure recovers 92.2% of standard classification performance and on quartile classification, format familiarity closes 71.3% of the gap with the residual attributable to contaminated datasets. These findings suggest claimed generalization likely reflects evaluation artifacts rather than learned tabular reasoning. We conclude with recommendations for strengthening TLM evaluation.

</details>


### [107] [DADP: Domain Adaptive Diffusion Policy](https://arxiv.org/abs/2602.04037)
*Pengcheng Wang,Qinghang Liu,Haotian Lin,Yiheng Li,Guojian Zhan,Masayoshi Tomizuka,Yixiao Wang*

Main category: cs.LG

TL;DR: DADP通过无监督解耦的领域表征与领域感知扩散注入，在未见动力学下实现稳健自适应；在 locomotion 与 manipulation 基准上展现良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有域自适应策略往往通过域表示捕获域信息，但若采用邻近时间步的上下文，则会把静态域特征与动态变化混杂，导致零-shot 适应受限。需要解耦静态域信息与动态属性以提升泛化。

Method: 提出两大组件：1) Lagged Context Dynamical Prediction，通过对未来状态预测引入历史偏移上下文，增大时间间隔以无监督地去耦静态域信息；2) 将学习得到的域表示直接注入扩散过程：偏置先验分布并重定义扩散目标，使生成过程带有域感知特征。

Result: 在挑战性基准（ locomotion 与 manipulation）上进行了广泛实验，DADP 显著优于先前方法，展现更强的鲁棒性与对未见域的泛化能力，附带可视化及官网展示。

Conclusion: 通过无监督解耦与域感知的扩散注入，DADP 实现鲁棒自适应与良好泛化，证实域表示与扩散生成耦合是提升域自适应控制性能的有效路径。

Abstract: Learning domain adaptive policies that can generalize to unseen transition dynamics, remains a fundamental challenge in learning-based control. Substantial progress has been made through domain representation learning to capture domain-specific information, thus enabling domain-aware decision making. We analyze the process of learning domain representations through dynamical prediction and find that selecting contexts adjacent to the current step causes the learned representations to entangle static domain information with varying dynamical properties. Such mixture can confuse the conditioned policy, thereby constraining zero-shot adaptation. To tackle the challenge, we propose DADP (Domain Adaptive Diffusion Policy), which achieves robust adaptation through unsupervised disentanglement and domain-aware diffusion injection. First, we introduce Lagged Context Dynamical Prediction, a strategy that conditions future state estimation on a historical offset context; by increasing this temporal gap, we unsupervisedly disentangle static domain representations by filtering out transient properties. Second, we integrate the learned domain representations directly into the generative process by biasing the prior distribution and reformulating the diffusion target. Extensive experiments on challenging benchmarks across locomotion and manipulation demonstrate the superior performance, and the generalizability of DADP over prior methods. More visualization results are available on the https://outsider86.github.io/DomainAdaptiveDiffusionPolicy/.

</details>


### [108] [Partition Trees: Conditional Density Estimation over General Outcome Spaces](https://arxiv.org/abs/2602.04042)
*Felipe Angelim,Alessandro Leite*

Main category: cs.LG

TL;DR: Partition Trees 提供一个面向条件密度估计的树结构框架，适用于连续和分类变量的统一处理。通过在数据自适应分区上构建分段常数密度，并直接最小化条件负对数似然来学习树；Partition Forests 通过对密度进行平均实现集成。相较于 CART 风格树和其他概率树及随机森林，展示出更强的概率预测能力以及对冗余特征与异方差噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 需要一个灵活、可扩展且非参数的条件密度估计方法，能够处理混合型输出空间并避免对目标分布的强假设。

Method: 提出 Partition Trees，基于数据自适应分区的分段常数密度，直接以条件负对数似然作为分裂与切分的优化目标；通过平均密度 form 的 Partition Forest 进行集合学习；该框架同时支持连续和分类变量。

Result: 实证结果显示在概率预测方面优于 CART 风格树，并与最前沿的概率树方法和随机森林相比达到竞争甚至优越的表现，同时对冗余特征与异方差噪声表现出鲁棒性。

Conclusion: Partition Trees/Partition Forests 提供一种可扩展的非参数条件密度估计框架，适用于混合输出空间，在多种数据情形下具备良好性能与鲁棒性。

Abstract: We propose Partition Trees, a tree-based framework for conditional density estimation over general outcome spaces, supporting both continuous and categorical variables within a unified formulation. Our approach models conditional distributions as piecewise-constant densities on data adaptive partitions and learns trees by directly minimizing conditional negative log-likelihood. This yields a scalable, nonparametric alternative to existing probabilistic trees that does not make parametric assumptions about the target distribution. We further introduce Partition Forests, an ensemble extension obtained by averaging conditional densities. Empirically, we demonstrate improved probabilistic prediction over CART-style trees and competitive or superior performance compared to state-of-the-art probabilistic tree methods and Random Forests, along with robustness to redundant features and heteroscedastic noise.

</details>


### [109] [An Empirical Survey and Benchmark of Learned Distance Indexes for Road Networks](https://arxiv.org/abs/2602.04068)
*Gautam Choudhary,Libin Zhou,Yeasir Rayhan,Walid G. Aref*

Main category: cs.LG

TL;DR: 首次对基于ML的距离索引进行系统性实证评估，比较十种ML方法与经典基线，在七个真实路网和轨迹数据驱动的查询集上，重点评估训练时间、查询延迟、存储和准确性，并提供开源代码以促进复现与后续研究。


<details>
  <summary>Details</summary>
Motivation: 随着实时性和大规模路网查询的需求日益增长，传统的最短路径算法在大规模部署中成本高昂；ML驱动的距离索引兴起，但缺乏系统性、可重复的评估与对比，难以把握不同方法的权衡。

Method: 进行基于实证的综述与基准实验：选取七个真实路网、基于轨迹数据的工作负载，比较十种具有代表性的ML技术及强基线，围绕训练时间、查询延迟、存储占用与准确性等指标进行测评，提供统一的开源实现以支持复现。

Result: 给出对ML距离索引的关键洞察与实际权衡，揭示不同方法在不同指标上的优势与折衷，形成对部署与研究的实用指引。

Conclusion: 首次对ML距离索引进行全面实证评估，建立可复现的基准与代码库，促进 learned distance indexes 的未来研究与应用。

Abstract: The calculation of shortest-path distances in road networks is a core operation in navigation systems, location-based services, and spatial analytics. Although classical algorithms, e.g., Dijkstra's algorithm, provide exact answers, their latency is prohibitive for modern real-time, large-scale deployments. Over the past two decades, numerous distance indexes have been proposed to speed up query processing for shortest distance queries. More recently, with the advancement in machine learning (ML), researchers have designed and proposed ML-based distance indexes to answer approximate shortest path and distance queries efficiently. However, a comprehensive and systematic evaluation of these ML-based approaches is lacking. This paper presents the first empirical survey of ML-based distance indexes on road networks, evaluating them along four key dimensions: Training time, query latency, storage, and accuracy. Using seven real-world road networks and workload-driven query datasets derived from trajectory data, we benchmark ten representative ML techniques and compare them against strong classical non-ML baselines, highlighting key insights and practical trade-offs. We release a unified open-source codebase to support reproducibility and future research on learned distance indexes.

</details>


### [110] [Stroke Lesions as a Rosetta Stone for Language Model Interpretability](https://arxiv.org/abs/2602.04074)
*Julius Fridriksson,Roger D. Newman-Norlund,Saeed Ahmadi,Regan Willis,Nadra Salman,Kalil Warren,Xiang Guan,Yong Yang,Srihari Nelakuditi,Rutvik Desai,Leonardo Bonilha,Jeff Charney,Chris Rorden*

Main category: cs.LG

TL;DR: BLUM框架通过将LLM perturbation的错误画像投射到人类脑损伤-症状映射的外部参照空间，进行跨领域的可解释性验证，显示LLM错误与人类病灶模式在一定比例的任务中高度相似，支持以神经科学为外部验证的LLM解释框架。


<details>
  <summary>Details</summary>
Motivation: 解决对LLM可解释性方法缺乏外部、因果层面的验证问题，借助人体长期研究的脑损伤-行为关系作为外部参照，以评估模型组件的必要性和计算原则。

Method: 以410名慢性中风失语患者为数据源，建立症状-病灶映射模型来从行为误差谱预测脑损伤位置；对LLM的变压器层实施系统Perturbation；对 perturbed LLM 与人类患者执行等同的临床评估；将LLM的错误谱投射到人类病灶空间；评估LLM错误谱预测的病灶是否落在实际病灶之上。

Result: LLM错误谱在图片命名条件的67%和句子完成条件的68.3%上，预测的病灶比随机水平高且显著（p<1e-23和p<1e-61）；语义主导的错误映射到腹侧-路径，音位主导的错误映射到背侧-路径。

Conclusion: 为LLM可解释性提供一种外部验证的新路径，将人类病灶-症状映射作为评估人工语言系统的参考框架，提示潜在的共享计算原理并促使进一步的跨域理论探究。

Abstract: Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.

</details>


### [111] [A Probabilistic Framework for Solving High-Frequency Helmholtz Equations via Diffusion Models](https://arxiv.org/abs/2602.04082)
*Yicheng Zou,Samuel Lanthaler,Hossein Salahshoor*

Main category: cs.LG

TL;DR: 提出基于评分的条件扩散神经算子，用于高频 Helmholtz 方程的波动近似，提供不确定性量化，在多频率实验中优于其他数据驱动和机器学习方法，且能良好捕获输入声速图的不确定性。


<details>
  <summary>Details</summary>
Motivation: 高频波现象对确定性神经算子具有强输入输出敏感性，存在谱偏差，难以准确近似；需要将不确定性纳入建模以提升鲁棒性。

Method: 建立一个基于评分的条件扩散算子概率框架；对 Helmholtz 运算符进行稳定性分析；在广泛的频率范围内进行数值实验，比较与其他数据驱动/ML 方法的性能。

Result: 所提出的概率神经算子在 L2、H1、以及能量范数上误差最低；能够捕获输入声速地图对解场的不确定性，而其他确定性方法难以做到这一点。

Conclusion: 凸显概率算子学习在高频 PDE（如 Helmholtz）中的原则性和有效性，表明其在高频复杂场景下具备鲁棒性与不确定性量化能力。

Abstract: Deterministic neural operators perform well on many PDEs but can struggle with the approximation of high-frequency wave phenomena, where strong input-to-output sensitivity makes operator learning challenging, and spectral bias blurs oscillations. We argue for adopting a probabilistic approach for approximating waves in high-frequency regime, and develop our probabilistic framework using a score-based conditional diffusion operator. After demonstrating a stability analysis of the Helmholtz operator, we present our numerical experiments across a wide range of frequencies, benchmarked against other popular data-driven and machine learning approaches for waves. We show that our probabilistic neural operator consistently produces robust predictions with the lowest errors in $L^2$, $H^1$, and energy norms. Moreover, unlike all the other tested deterministic approaches, our framework remarkably captures uncertainties in the input sound speed map propagated to the solution field. We envision that our results position probabilistic operator learning as a principled and effective approach for solving complex PDEs such as Helmholtz in the challenging high-frequency regime.

</details>


### [112] [Federated Concept-Based Models: Interpretable models with distributed supervision](https://arxiv.org/abs/2602.04093)
*Dario Fenoglio,Arianna Casanova,Francesco De Santis,Mohan Li,Gabriele Dominici,Johannes Schneider,Martin Gjoreski,Marc Langheinrich,Pietro Barbiero,Giovanni De Felice*

Main category: cs.LG

TL;DR: 提出了在联邦学习场景下的概念基模型（F-CMs），通过跨机构聚合概念级信息并自适应调整模型架构，在保护隐私的同时实现对变化监督信号的鲁棒性，并实现对某些机构未拥有的概念的可解释推断。


<details>
  <summary>Details</summary>
Motivation: 概念级监督在提升模型可解释性方面具有重要意义，但概念注释成本高且难以在单一数据源大规模获取。联邦学习可跨机构协作获取分布式的概念注释，但现有方法缺乏可解释性建模范式；需要在异构、动态加入机构的现实FL场景中保持可解释性和性能。

Method: 提出 Federated Concept-based Models (F-CMs)。在各机构之间聚合概念层次信息，随着可用概念监督的变化动态自适应调整模型架构，并在不泄露机构隐私的前提下实现协作训练。与以往只在固定概念空间和固定架构的 CM 或静态FL 方法相比，F-CMs 支持对不存在于某机构的概念进行可解释推断。

Result: 实验表明，F-CMs 在保持与全监督训练相同或相近准确性与干预效果的前提下，优于非自适应的联邦基线。且具备在给定机构未具备概念时，仍能进行可解释推断的关键新颖性。

Conclusion: F-CMs 为在持续演化的联邦学习环境中部署概念基模型提供了一种可解释、私密性保护且自适应的解决方案，具有良好的实证性能和跨机构可迁移性。

Abstract: Concept-based models (CMs) enhance interpretability in deep learning by grounding predictions in human-understandable concepts. However, concept annotations are expensive to obtain and rarely available at scale within a single data source. Federated learning (FL) could alleviate this limitation by enabling cross-institutional training that leverages concept annotations distributed across multiple data owners. Yet, FL lacks interpretable modeling paradigms. Integrating CMs with FL is non-trivial: CMs assume a fixed concept space and a predefined model architecture, whereas real-world FL is heterogeneous and non-stationary, with institutions joining over time and bringing new supervision. In this work, we propose Federated Concept-based Models (F-CMs), a new methodology for deploying CMs in evolving FL settings. F-CMs aggregate concept-level information across institutions and efficiently adapt the model architecture in response to changes in the available concept supervision, while preserving institutional privacy. Empirically, F-CMs preserve the accuracy and intervention effectiveness of training settings with full concept supervision, while outperforming non-adaptive federated baselines. Notably, F-CMs enable interpretable inference on concepts not available to a given institution, a key novelty with respect to existing approaches.

</details>


### [113] [CoRe: Context-Robust Remasking for Diffusion Language Models](https://arxiv.org/abs/2602.04096)
*Kevin Zhai,Sabbir Mollah,Zhenyi Wang,Mubarak Shah*

Main category: cs.LG

TL;DR: 提出 Context-Robust Remasking (CoRe)，在推理阶段无训练成本，通过对被遮蔽上下文的敏感性进行探测来识别上下文脆弱的 token，并把修订形式化为对上下文变化的鲁棒优化的近似，从而提升推理任务中的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 标准解码在掩蔽扩散模型中受限于上下文的刚性：早期预测依赖暂态高置信度，忽视未完成上下文的影响，导致错误 cascade。静态置信度信号缺乏前瞻性，易错判；需要一种不依赖静态概率、对上下文扰动鲁棒的修订策略。

Method: CoRe 为训练无关的推理阶段修订框架。通过对目标 token 的被遮蔽上下文进行定向扰动来探测其敏感性，形式化修订为对上下文偏移的鲁棒优化目标，并对该目标进行高效近似以优先修订不稳定 token。

Result: 在 LLaDA-8B-Base 上，CoRe 在推理阶段获得一致性改进，优于算力匹配基线；在 MBPP 等任务上提升可达约 9.2 个百分点。

Conclusion: CoRe 提供一种训练-free 的推理阶段修订框架，降低对静态信号的依赖，提升推理与代码任务中的稳健性与性能。

Abstract: Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.

</details>


### [114] [Rethinking Perplexity: Revealing the Impact of Input Length on Perplexity Evaluation in LLMs](https://arxiv.org/abs/2602.04099)
*Letian Cheng,Junyan Wang,Yan Gao,Elliott Wen,Ting Dang,Hong Jia*

Main category: cs.LG

TL;DR: LengthBenchmark 提出一个面向系统的评估框架，将输入长度作为关键变量纳入对大型语言模型的评测，揭示不同评测协议下的长度偏差并考察系统开销与量化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的困惑包括单一困惑度（perplexity）对长输入与无关信息的敏感性，以及缺乏将输入长度作为系统变量来评估公平性与效率的研究。需要在评测中显式考虑输入长度及其对延迟、内存和成本的影响。

Method: 在两种评分协议（直接累积 direct accumulation 与固定窗口滑动 fixed window sliding）下，对代表性LLM在不同上下文长度上进行评测；同时测量延迟、内存占用和评测成本，并将量化变体作为鲁棒性检查以验证长度偏差的普遍性，区分评测逻辑、量化和输入长度的影响。

Result: 得到两点关键结论：(i) 滑动窗口评测在短输入上通常放大了性能表现；(ii) 全精度和量化模型在评测的段落长度增加时都呈现性能提升的现象，长度偏差会导致跨模型比较不公平，且在量化下仍然存在。

Conclusion: 将输入长度作为首要的系统变量进行评测是必要的，需在评测设计中考虑评估协议、量化以及资源成本等因素，以避免长度偏差对公平性与部署现实的误导。

Abstract: Perplexity is a widely adopted metric for assessing the predictive quality of large language models (LLMs) and often serves as a reference metric for downstream evaluations. However, recent evidence shows that perplexity can be unreliable, especially when irrelevant long inputs are used, raising concerns for both benchmarking and system deployment. While prior efforts have employed selective input filtering and curated datasets, the impact of input length on perplexity has not been systematically studied from a systems perspective and input length has rarely been treated as a first-class system variable affecting both fairness and efficiency. In this work, we close this gap by introducing LengthBenchmark, a system-conscious evaluation framework that explicitly integrates input length, evaluation protocol design, and system-level costs, evaluating representative LLMs under two scoring protocols (direct accumulation and fixed window sliding) across varying context lengths. Unlike prior work that focuses solely on accuracy-oriented metrics, LengthBenchmark additionally measures latency, memory footprint, and evaluation cost, thereby linking predictive metrics to deployment realities. We further incorporate quantized variants not as a main contribution, but as robustness checks, showing that length-induced biases persist across both full-precision and compressed models. This design disentangles the effects of evaluation logic, quantization, and input length, and demonstrates that length bias is a general phenomenon that undermines fair cross-model comparison. Our analysis yields two key observations: (i) sliding window evaluation consistently inflates performance on short inputs, and (ii) both full-precision and quantized models appear to realise gains as the evaluated segment length grows.

</details>


### [115] [Supervised Learning as Lossy Compression: Characterizing Generalization and Sample Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.04107)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 将学习过程置于信息理论的有损压缩框架，利用有限块长度分析对泛化进行下界分析。将训练样本抽样视为编码、模型构建视为解码，给出固定随机化学习算法及最优采样策略的样本复杂度与泛化误差下界；明确区分过拟合与任务不匹配之间的贡献，并将过拟合项与信息论/稳定性度量联系起来，实现信息论与稳定性理论的统一。


<details>
  <summary>Details</summary>
Motivation: 解决泛化分析的分散问题，寻求一个可统一的信息论与稳定性视角的框架。通过把样本抽样看作编码、模型训练看作解码，将有限块长度分析引入泛化界的抽象，以获得对样本复杂度、泛化误差及过拟合的定量控制。

Method: 在有限块长度下，将学习过程建模为编码-解码过程；对固定的随机化学习算法及其最优采样策略，推导样本复杂度和泛化误差的下界；将过拟合项分解为与 inductive bias 与任务不匹配相关的部分并与现有信息论界、稳定性界联系，形成统一分析框架。

Result: 获得关于样本复杂度与泛化误差的下界，并将过拟合项与任务偏置之间的匹配程度分离呈现；将过拟合项与信息论度量以及稳定性理论中的指标连接起来，提出一个能够统一信息论与稳定性视角的新框架。

Conclusion: 该框架将信息论与稳定性理论统一起来，提供对泛化与过拟合的新洞见，且可与现有界和指标结合，具有理论完整性和潜在的实践指引。

Abstract: This paper presents a novel information-theoretic perspective on generalization in machine learning by framing the learning problem within the context of lossy compression and applying finite blocklength analysis. In our approach, the sampling of training data formally corresponds to an encoding process, and the model construction to a decoding process. By leveraging finite blocklength analysis, we derive lower bounds on sample complexity and generalization error for a fixed randomized learning algorithm and its associated optimal sampling strategy. Our bounds explicitly characterize the degree of overfitting of the learning algorithm and the mismatch between its inductive bias and the task as distinct terms. This separation provides a significant advantage over existing frameworks. Additionally, we decompose the overfitting term to show its theoretical connection to existing metrics found in information-theoretic bounds and stability theory, unifying these perspectives under our proposed framework.

</details>


### [116] [Rate-Optimal Noise Annealing in Semi-Dual Neural Optimal Transport: Tangential Identifiability, Off-Manifold Ambiguity, and Guaranteed Recovery](https://arxiv.org/abs/2602.04110)
*Raymond Chu,Jaewoong Choi,Dohyun Kwon*

Main category: cs.LG

TL;DR: 对于在低维流形数据上的半对偶神经最优传输，加入噪声平滑能够恢复正确的传输映射并达到最优统计率；给出一个可计算的终止噪声水平 ε_stat(N)（由数据的内在维度 m 调控），用于停止 annealing；低于 ε_stat(N) 时，简化的半对偶目标呈现病态/条件数上升的现象，可能降低而非提升精度。


<details>
  <summary>Details</summary>
Motivation: 数据集中在低维流形上使得半对偶目标在数据流形外的约束不足，从而产生伪解映射；需要一种理论与实用的平滑化策略以及停止准则来确保学习到可识别的映射。

Method: 理论分析添加噪声平滑的影响，给出在噪声趋近0时的映射恢复保证；统一分析包括最优计划的稳定性、平滑带来的偏差以及有限样本误差；推导出 ε_stat(N) 及其与内在维度 m 的关系；并分析在 ε 降低时简化半对偶目标的病态化。

Result: 在噪声趋于0时给出新的映射恢复保证；提出一个可计算的终止噪声水平 ε_stat(N)，使统计误差达到最优率；学习速率与样本规模的依赖由内在维度 m 决定，而非外部维度；并证实随着 ε 向0，简化目标的条件数上升。

Conclusion: 给出一个实用的停止规则：在 ε_low<ε_stat(N) 的过度退火可能恶化优化条件数而不提升统计精度；该工作为在低维流形数据上的半对偶最优传输提供理论保障和可操作的噪声控制策略。

Abstract: Semi-dual neural optimal transport learns a transport map via a max-min objective, yet training can converge to incorrect or degenerate maps. We fully characterize these spurious solutions in the common regime where data concentrate on low-dimensional manifold: the objective is underconstrained off the data manifold, while the on-manifold transport signal remains identifiable. Following Choi, Choi, and Kwon (2025), we study additive-noise smoothing as a remedy and prove new map recovery guarantees as the noise vanishes. Our main practical contribution is a computable terminal noise level $\varepsilon_{\mathrm{stat}}(N)$ that attains the optimal statistical rate, with scaling governed by the intrinsic dimension $m$ of the data. The formula arises from a theoretical unified analysis of (i) quantitative stability of optimal plans, (ii) smoothing-induced bias, and (iii) finite-sample error, yielding rates that depend on $m$ rather than the ambient dimension. Finally, we show that the reduced semi-dual objective becomes increasingly ill-conditioned as $\varepsilon \downarrow 0$. This provides a principled stopping rule: annealing below $\varepsilon_{\mathrm{stat}}(N)$ can $\textit{worsen}$ optimization conditioning without improving statistical accuracy.

</details>


### [117] [Turning mechanistic models into forecasters by using machine learning](https://arxiv.org/abs/2602.04114)
*Amit K. Chakraborty,Hao Wang,Pouria Ramazi*

Main category: cs.LG

TL;DR: 提出一种带时间变化参数的数据驱动微分方程发现框架，并将其用于建模与预测，显著提升准确性和预测性能，且在多数据集上优于CNN-LSTM与GBM。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法多假设系数为时间不变，无法自适应演化动力学。引入时间可变参数并直接学习其时间演化，可提升对时变系统的建模与预测能力。

Method: 在变量库基础上引入时间变化参数，学习其随时间的演化；将该框架转化为预测模型：先预测时间变参数，再将预测代入学习得到的微分方程。

Result: 在SIR、Consumer–Resource、温室气体浓度、蓝绿藻(Cyanobacteria)细胞计数等数据集上，学习时间序列的MAE<3%，未来一个月前瞻的MAE<6%；对比CNN-LSTM和GBM，在多数数据集上表现更优。

Conclusion: 将时变参数融入数据驱动的微分方程发现框架能显著提升建模精度与预测性能，且具备在多领域时变动力学系统中的应用潜力。

Abstract: The equations of complex dynamical systems may not be identified by expert knowledge, especially if the underlying mechanisms are unknown. Data-driven discovery methods address this challenge by inferring governing equations from time-series data using a library of functions constructed from the measured variables. However, these methods typically assume time-invariant coefficients, which limits their ability to capture evolving system dynamics. To overcome this limitation, we allow some of the parameters to vary over time, learn their temporal evolution directly from data, and infer a system of equations that incorporates both constant and time-varying parameters. We then transform this framework into a forecasting model by predicting the time-varying parameters and substituting these predictions into the learned equations. The model is validated using datasets for Susceptible-Infected-Recovered, Consumer--Resource, greenhouse gas concentration, and Cyanobacteria cell count. By dynamically adapting to temporal shifts, our proposed model achieved a mean absolute error below 3\% for learning a time series and below 6\% for forecasting up to a month ahead. We additionally compare forecasting performance against CNN-LSTM and Gradient Boosting Machine (GBM), and show that our model outperforms these methods across most datasets. Our findings demonstrate that integrating time-varying parameters into data-driven discovery of differential equations improves both modeling accuracy and forecasting performance.

</details>


### [118] [Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach](https://arxiv.org/abs/2602.04116)
*Sicheng Liu,Xunkai Li,Daohan Su,Ru Zhang,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出PLANET框架，通过分治策略在嵌入层实现模态交互，在节点层实现模态对齐构建离散语义表示空间，从而显著提升多模态图的表示与生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图神经模型在模态交互与模态对齐上存在不足：缺乏显式的跨模态交互，且不同模态间的语义差异难以高效对齐，限制了在MAG上的应用与泛化。

Method: 提出PLANET，采用分治策略：在嵌入粒度通过Embedding-wise Domain Gating（EDG）实现本地跨模态上下文的自适应注入以实现模态交互；在节点粒度通过Node-wise Discretization Retrieval（NDR）构建Discretized Semantic Representation Space（DSRS），实现全局模态对齐。

Result: 广泛实验表明PLANET在多种图任务与多模态生成任务上显著优于当前最优基线。

Conclusion: 通过将模态交互与对齐在不同粒度上分离处理，PLANET有效缓解了模态差异带来的挑战，提升MAG上的表示学习与生成能力。

Abstract: Graph Foundation Models (GFMs) have achieved remarkable success in generalizing across diverse domains. However, they mainly focus on Text-Attributed Graphs (TAGs), leaving Multimodal-Attributed Graphs (MAGs) largely untapped. Developing Multimodal Graph Foundation Models (MGFMs) allows for leveraging the rich multimodal information in MAGs, and extends applicability to broader types of downstream tasks. While recent MGFMs integrate diverse modality information, our empirical investigation reveals two fundamental limitations of existing MGFMs: (1)they fail to explicitly model modality interaction, essential for capturing intricate cross-modal semantics beyond simple aggregation, and (2)they exhibit sub-optimal modality alignment, which is critical for bridging the significant semantic disparity between distinct modal spaces. To address these challenges, we propose PLANET (graPh topoLogy-aware modAlity iNteraction and alignmEnT), a novel framework employing a Divide-and-Conquer strategy to decouple modality interaction and alignment across distinct granularities. At the embedding granularity, (1)Embedding-wise Domain Gating (EDG) performs local semantic enrichment by adaptively infusing topology-aware cross-modal context, achieving modality interaction. At the node granularity, (2)Node-wise Discretization Retrieval (NDR) ensures global modality alignment by constructing a Discretized Semantic Representation Space (DSRS) to bridge modality gaps. Extensive experiments demonstrate that PLANET significantly outperforms state-of-the-art baselines across diverse graph-centric and multimodal generative tasks.

</details>


### [119] [Scalable Explainability-as-a-Service (XaaS) for Edge AI Systems](https://arxiv.org/abs/2602.04120)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 提出 Explainability-as-a-Service (XaaS) 的边缘/物联网场景下可解释性作为首等服务，解耦推断与解释，采用分布式缓存、验证协议与自适应解释引擎，显著降低延时并提升可扩展性（在三大实际应用中实现约38%延时下降并维持高解释质量）。


<details>
  <summary>Details</summary>
Motivation: 当前XAI在边缘/物联网中的应用多为耦合实现，导致重复计算、低延迟和跨异构设备的扩展性差。需要将可解释性作为独立服务，以便在资源和时延约束下实现高效、可验证的解释。

Method: (1) 基于语义相似性的分布式解释缓存与检索，显著减少冗余计算；(2) 轻量级验证协议，确保缓存和新生成解释的保真性；(3) 自适应解释引擎，根据设备能力与用户需求选择解释方法。

Result: 在制造质量控制、自动驾驶感知、医疗诊断等三个真实边缘AI场景中，XaaS 将延迟降低约38%，同时保持高质量的解释。

Conclusion: XaaS 使得可解释且可问责的AI能够在大规模、异构的物联网系统中部署，推进XAI研究向边缘实际应用的对接。

Abstract: Though Explainable AI (XAI) has made significant advancements, its inclusion in edge and IoT systems is typically ad-hoc and inefficient. Most current methods are "coupled" in such a way that they generate explanations simultaneously with model inferences. As a result, these approaches incur redundant computation, high latency and poor scalability when deployed across heterogeneous sets of edge devices. In this work we propose Explainability-as-a-Service (XaaS), a distributed architecture for treating explainability as a first-class system service (as opposed to a model-specific feature). The key innovation in our proposed XaaS architecture is that it decouples inference from explanation generation allowing edge devices to request, cache and verify explanations subject to resource and latency constraints. To achieve this, we introduce three main innovations: (1) A distributed explanation cache with a semantic similarity based explanation retrieval method which significantly reduces redundant computation; (2) A lightweight verification protocol that ensures the fidelity of both cached and newly generated explanations; and (3) An adaptive explanation engine that chooses explanation methods based upon device capability and user requirement. We evaluated the performance of XaaS on three real-world edge-AI use cases: (i) manufacturing quality control; (ii) autonomous vehicle perception; and (iii) healthcare diagnostics. Experimental results show that XaaS reduces latency by 38\% while maintaining high explanation quality across three real-world deployments. Overall, this work enables the deployment of transparent and accountable AI across large scale, heterogeneous IoT systems, and bridges the gap between XAI research and edge-practicality.

</details>


### [120] [Decoupling Time and Risk: Risk-Sensitive Reinforcement Learning with General Discounting](https://arxiv.org/abs/2602.04131)
*Mehrdad Moghimi,Anthony Coache,Hyejin Ku*

Main category: cs.LG

TL;DR: 在分布式强化学习中引入灵活的未来奖励折扣与多时间尺度扩展，以优化风险敏感目标并提升鲁棒性；给出理论最优性分析并通过大量实验验证多时间尺度解决现有方法的问题，强调折扣因子在表达时间与风险偏好中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习常用固定或指数折扣，难以表达多样化的时间偏好；分布式RL在风险敏感目标方面具潜力，但对折扣函数的作用理解不足；需要一个能够在分布式框架内灵活折扣未来奖励并优化风险度量的框架，尤其以安全关键任务为导向。

Method: 提出一个在分布式RL中支持灵活折扣的框架，并结合多时间尺度扩展以修正现有方法的不足；在理论层面给出最优性分析，并设计相应的算法以优化风险度量；构建可接受的折扣函数族并在多重时间尺度下进行学习与决策，可能包含对风险度量（如CVaR等）在分布式语义中的整合。

Result: 理论上给出最优性或近似最优性结果，证明多时间尺度扩展解决了现有方法的某些问题；通过大量实验验证算法的鲁棒性和表达能力，显示折扣选取对表达更丰富的时间与风险偏好具有显著影响。

Conclusion: 折扣因子在决策中的表达能力至关重要，灵活的折扣机制显著提升分布式RL在表达时间与风险偏好方面的能力，对安全关键应用具有潜在影响。未来工作可在折扣族的设计、折扣与风险度量的耦合机制等方面深入探索。

Abstract: Distributional reinforcement learning (RL) is a powerful framework increasingly adopted in safety-critical domains for its ability to optimize risk-sensitive objectives. However, the role of the discount factor is often overlooked, as it is typically treated as a fixed parameter of the Markov decision process or tunable hyperparameter, with little consideration of its effect on the learned policy. In the literature, it is well-known that the discounting function plays a major role in characterizing time preferences of an agent, which an exponential discount factor cannot fully capture. Building on this insight, we propose a novel framework that supports flexible discounting of future rewards and optimization of risk measures in distributional RL. We provide a technical analysis of the optimality of our algorithms, show that our multi-horizon extension fixes issues raised with existing methodologies, and validate the robustness of our methods through extensive experiments. Our results highlight that discounting is a cornerstone in decision-making problems for capturing more expressive temporal and risk preferences profiles, with potential implications for real-world safety-critical applications.

</details>


### [121] [Benchmarking Uncertainty Quantification of Plug-and-Play Diffusion Priors for Inverse Problems Solving](https://arxiv.org/abs/2602.04189)
*Xiaoyu Qiu,Taewon Yang,Zhanhao Liu,Guanyang Wang,Liyue Shen*

Main category: cs.LG

TL;DR: 系统评估PnPDP的不确定性量化（UQ），提出面向UQ的分类框架，并在玩具模型与真实科学逆问题上验证其分布性行为的一致性。


<details>
  <summary>Details</summary>
Motivation: 逆问题的目标输出应为后验分布；现有评估聚焦单点估计，无法反映随机性与不确定性，因此需要基于分布的UQ基准来正确评估PnPDP。

Method: 设计严格的玩具模型仿真以评估不同PnPDP求解器的不确定性行为，提出基于不确定性（UQ）的分类框架；在玩具仿真和多种真实科学逆问题上进行广泛实验，验证分类的有效性与理论一致性。

Result: 实验结果表明，不确定性行为与提出的UQ分类一致，与理论推导相符，为评估和理解PnPDP中的不确定性提供新的视角。

Conclusion: 提出面向PnPDP的UQ评估框架与分类体系，强调在评估中关注输出分布特征而非单点估计，并为未来的研究和基准测试提供参考。

Abstract: Plug-and-play diffusion priors (PnPDP) have become a powerful paradigm for solving inverse problems in scientific and engineering domains. Yet, current evaluations of reconstruction quality emphasize point-estimate accuracy metrics on a single sample, which do not reflect the stochastic nature of PnPDP solvers and the intrinsic uncertainty of inverse problems, critical for scientific tasks. This creates a fundamental mismatch: in inverse problems, the desired output is typically a posterior distribution and most PnPDP solvers induce a distribution over reconstructions, but existing benchmarks only evaluate a single reconstruction, ignoring distributional characterization such as uncertainty. To address this gap, we conduct a systematic study to benchmark the uncertainty quantification (UQ) of existing diffusion inverse solvers. Specifically, we design a rigorous toy model simulation to evaluate the uncertainty behavior of various PnPDP solvers, and propose a UQ-driven categorization. Through extensive experiments on toy simulations and diverse real-world scientific inverse problems, we observe uncertainty behaviors consistent with our taxonomy and theoretical justification, providing new insights for evaluating and understanding the uncertainty for PnPDPs.

</details>


### [122] [LORE: Jointly Learning the Intrinsic Dimensionality and Relative Similarity Structure From Ordinal Data](https://arxiv.org/abs/2602.04192)
*Vivek Anand,Alec Helbling,Mark Davenport,Gordon Berman,Sankar Alagapan,Christopher Rozell*

Main category: cs.LG

TL;DR: LORE 将学习嵌入维度与序数嵌入同时从三元组比较中进行自适应估计，采用非凸 Schatten-p 正则化实现低秩近似，具备收敛性保证，得到低维、可解释且高保真度的嵌入。


<details>
  <summary>Details</summary>
Motivation: 从主观感知空间（如味觉、气味、审美）产生的序数数据中提取几何结构非常困难；现有方法通常需要在开始时设定嵌入维度，缺乏对维度的自适应估计与可扩展性，需要一个能够同时估计嵌入和其维度的框架。

Method: 提出 LORE（Low Rank Ordinal Embedding），通过联合优化嵌入和潜在秩来学习序数嵌入，使用非凸 Schatten-p 想范数作为正则化实现低秩约束；采用迭代加权算法进行优化并给出收敛性保证；直接在噪声三元组形式“Is A more similar to B than C?” 下工作，适合大规模数据。

Result: 在合成数据、模拟感知空间以及真实的众包序数判断上，LORE 能学习紧凑、可解释且高准确性的低维嵌入，能够恢复潜在的感知几何；同时实现维度自适应，提升可解释性与数据效率。

Conclusion: 同时推断内在维度与嵌入有助于更具可解释性和数据效率的感知建模，LORE 为从序数数据中发现低维结构提供了一种可扩展的新途径。

Abstract: Learning the intrinsic dimensionality of subjective perceptual spaces such as taste, smell, or aesthetics from ordinal data is a challenging problem. We introduce LORE (Low Rank Ordinal Embedding), a scalable framework that jointly learns both the intrinsic dimensionality and an ordinal embedding from noisy triplet comparisons of the form, "Is A more similar to B than C?". Unlike existing methods that require the embedding dimension to be set apriori, LORE regularizes the solution using the nonconvex Schatten-$p$ quasi norm, enabling automatic joint recovery of both the ordinal embedding and its dimensionality. We optimize this joint objective via an iteratively reweighted algorithm and establish convergence guarantees. Extensive experiments on synthetic datasets, simulated perceptual spaces, and real world crowdsourced ordinal judgements show that LORE learns compact, interpretable and highly accurate low dimensional embeddings that recover the latent geometry of subjective percepts. By simultaneously inferring both the intrinsic dimensionality and ordinal embeddings, LORE enables more interpretable and data efficient perceptual modeling in psychophysics and opens new directions for scalable discovery of low dimensional structure from ordinal data in machine learning.

</details>


### [123] [Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning](https://arxiv.org/abs/2602.04265)
*Wenze Lin,Zhen Yang,Xitai Jiang,Pony Ma,Gao Huang*

Main category: cs.LG

TL;DR: 提出动态奖励框架 T2T，用于 RLVR，解决探索-收敛之间的矛盾，通过厚化/瘦化机制在错题/正确回答上动态调整轨迹长度，提升数学推理性能，实验证明优于 GRPO 与基线.


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的强化学习奖励设计中，存在熵崩塌、冗长、探索不足等问题，尤其在需要大规模搜索的题解阶段与已掌握知识的高效阶段之间的需求未被区分。

Method: 提出双阶段机制：错题时进行厚化，延长轨迹以扩大搜索空间，探索新解法；正确时进行瘦化，施以长度惩罚，抑制冗余并促成自信与稳定推理。该框架嵌入 RLVR，适用于 Qwen-series、Deepseek 模型，评估在 MATH-500、AIME、AMC 等数学基准上的表现并与 GRPO 等基线比较。

Result: 实验结果显示 T2T 在数学基准上显著优于标准 GRPO 与近期基线，提升了解题性能与推理质量。

Conclusion: 动态调节轨迹长度的厚化-瘦化机制能有效平衡搜索与收敛，提升推理能力与模型对解的信心。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes "thickening" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to "thinning", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.

</details>


### [124] [Multi Objective Design Optimization of Non Pneumatic Passenger Car Tires Using Finite Element Modeling, Machine Learning, and Particle swarm Optimization and Bayesian Optimization Algorithms](https://arxiv.org/abs/2602.04277)
*Priyankkumar Dhrangdhariya,Soumyadipta Maiti,Venkataramana Runkana*

Main category: cs.LG

TL;DR: 通过生成设计与机器学习驱动的框架，对UPTIS型辐条几何进行优化，实现了性能提升（53%刚度可调性、至多50%耐久性提升、43%振动降低），并显著减少对有限元仿真的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决非充气轮胎在不连续辐条结构下的刚度调优、耐久性和高速振动问题，以及需要在设计空间中进行高效探索。

Method: 将辐条上下轮廓通过高阶多项式表示并用PCHIP实现几何变体，设计约250个生成设计；采用KRR预测刚度、XGBoost预测耐久性与振动；通过粒子群优化和贝叶斯优化进行多目标优化，减少对FEM仿真的依赖。

Result: 实现53%的刚度可调性、至多50%的耐久性提升、43%的振动降低；PSO实现快速收敛，贝叶斯优化有效探索多目标权衡；提出的框架可系统化地开发高性能的下一代UPTIS辐条结构。

Conclusion: 该框架为UPTIS辐条结构的高性能开发提供系统化、以数据驱动的设计流程，提升设计空间探索效率与最终性能。

Abstract: Non Pneumatic tires offer a promising alternative to pneumatic tires. However, their discontinuous spoke structures present challenges in stiffness tuning, durability, and high speed vibration. This study introduces an integrated generative design and machine learning driven framework to optimize UPTIS type spoke geometries for passenger vehicles. Upper and lower spoke profiles were parameterized using high order polynomial representations, enabling the creation of approximately 250 generative designs through PCHIP based geometric variation. Machine learning models like KRR for stiffness and XGBoost for durability and vibration achieved strong predictive accuracy, reducing the reliance on computationally intensive FEM simulations. Optimization using Particle Swarm Optimization and Bayesian Optimization further enabled extensive performance refinement. The resulting designs demonstrate 53% stiffness tunability, up to 50% durability improvement, and 43% reduction in vibration compared to the baseline. PSO provided fast, targeted convergence, while Bayesian Optimization effectively explored multi objective tradeoffs. Overall, the proposed framework enables systematic development of high performance, next generation UPTIS spoke structures.

</details>


### [125] [From Ambiguity to Action: A POMDP Perspective on Partial Multi-Label Ambiguity and Its Horizon-One Resolution](https://arxiv.org/abs/2602.04255)
*Hanlin Pan,Yuhao Tang,Wanfu Gao*

Main category: cs.LG

TL;DR: 将部分多标签学习（PML）视为部分观测马尔可夫决策过程（POMDP），通过两阶段强化学习实现标签歧义消解与特征选择：Stage 1 用 Transformer 策略生成高质量硬伪标签；Stage 2 将特征选择建模为序列化 RL，逐步输出可解释的全局排序；给出 PML-POMDP 的理论对应与超额风险界，并通过多数据集与多指标的实验验证框架优势。


<details>
  <summary>Details</summary>
Motivation: PML 中真实标签不可观测，标签歧义会把错误传播到下游任务（如特征工程）。需要一个统一的框架同时进行歧义消解与特征选择来降低风险。本文将 PML 问题映射到 POMDP，并通过强化学习实现端到端的风险最小化。

Method: 将 PML 问题建模为 POMDP；Stage 1 通过强化学习训练 Transformer 策略以输出高质量的硬伪标签；Stage 2 将特征选择建模为序列化强化学习问题，逐步选取特征并输出一个可解释的全局排序；给出 PML-POMDP 的理论对应及超额风险界的分解（伪标签质量项与样本量项）；在多数据集和多指标上进行实验以验证框架优势。

Result: 提出的 PML-POMDP 框架在多数据集和多指标上显示出性能提升；超额风险界将误差分解为伪标签质量与样本量两部分，阶段 2 输出的特征排序具有可解释性。理论与实验均支持框架的有效性与鲁棒性。

Conclusion: 将 PML 问题与 POMDP 结合的两阶段强化学习框架有效解决标签歧义与特征选择问题，提供理论保证（PML-POMDP 对应性与超额风险界）与实证证据，适用于需要端到端风险最小化的 PML 场景。

Abstract: In partial multi-label learning (PML), the true labels are unobserved, which makes label disambiguation important but difficult. A key challenge is that ambiguous candidate labels can propagate errors into downstream tasks such as feature engineering. To solve this issue, we jointly model the disambiguation and feature selection tasks as Partially Observable Markov Decision Processes (POMDP) to turn PML risk minimization into expected-return maximization. Stage 1 trains a transformer policy via reinforcement learning to produce high-quality hard pseudo-labels; Stage 2 describes feature selection as a sequential reinforcement learning problem, selecting features step by step and outputting an interpretable global ranking. We further provide the theoretical analysis of PML-POMDP correspondence and the excess-risk bound that decompose the error into pseudo label quality term and sample size. Experiments in multiple metrics and data sets verify the advantages of the framework.

</details>


### [126] [Multi-Integration of Labels across Categories for Component Identification (MILCCI)](https://arxiv.org/abs/2602.04270)
*Noga Mudrik,Yuxi Chen,Gal Mishne,Adam S. Charles*

Main category: cs.LG

TL;DR: MILCCI 通过在跨试验标签相似性约束下的稀疏分解，学习时间演化的成分轨迹，并整合标签信息以解释多试验时间数据中不同标签的贡献。


<details>
  <summary>Details</summary>
Motivation: 在大规模重复测量的时间序列数据中，如何理解不同类别标签对观测的编码，以及如何在试验间区分各标签的独立效应，是理解数据机制的关键挑战。

Method: 在稀疏的每试验分解框架上，MILCCI 引入类别内标签相似性作为约束，允许跨试验的成分组成随标签而变化；同时学习每个组件的时间轨迹，并整合标签信息以揭示各类别的表示。

Result: 通过合成数据与真实数据集（如投票模式、在线页面浏览趋势与神经元记录）验证，MILCCI 能捕获跨试验变异、提升对标签信息的解释性，并给出可解释的组件及时间轨迹。

Conclusion: MILCCI 提供一个可解释、可跨类别比较的时序分解框架，能够分离不同类别对观测的贡献，并生成随时间演化的成分轨迹，适用于带元数据的时间序列分析。

Abstract: Many fields collect large-scale temporal data through repeated measurements (trials), where each trial is labeled with a set of metadata variables spanning several categories. For example, a trial in a neuroscience study may be linked to a value from category (a): task difficulty, and category (b): animal choice. A critical challenge in time-series analysis is to understand how these labels are encoded within the multi-trial observations, and disentangle the distinct effect of each label entry across categories. Here, we present MILCCI, a novel data-driven method that i) identifies the interpretable components underlying the data, ii) captures cross-trial variability, and iii) integrates label information to understand each category's representation within the data. MILCCI extends a sparse per-trial decomposition that leverages label similarities within each category to enable subtle, label-driven cross-trial adjustments in component compositions and to distinguish the contribution of each category. MILCCI also learns each component's corresponding temporal trace, which evolves over time within each trial and varies flexibly across trials. We demonstrate MILCCI's performance through both synthetic and real-world examples, including voting patterns, online page view trends, and neuronal recordings.

</details>


### [127] [Efficient Equivariant High-Order Crystal Tensor Prediction via Cartesian Local-Environment Many-Body Coupling](https://arxiv.org/abs/2602.04323)
*Dian Jin,Yancheng Yuan,Xiaoming Tao*

Main category: cs.LG

TL;DR: CEITNet通过在原子局部环境中构建多通道笛卡尔张量并在通道空间进行学习型混合，利用笛卡尔张量基组来组装等变输出，从而高效预测晶体的高阶张量性质（顺序2、电介质；顺序3、压电；顺序4、弹性），在准确性和计算效率上优于现有高阶预测方法。


<details>
  <summary>Details</summary>
Motivation: 现有的高阶晶体张量性质预测要么依赖球面调和等变模型但在高阶目标上计算和内存成本高昂，要么难以兼顾准确性与可扩展性。需要一种在通道空间学习、利用笛卡尔基组的高效等变输出构造方法，以降低高阶张量的计算复杂度并提升预测性能。

Method: CEITNet对每个原子构建多通道的笛卡尔局部环境张量，通过可学习的通道空间交互实现多体混合；在通道空间中进行学习，并使用笛卡尔张量基组来组装等变输出，从而避免高阶Clebsch–Gordan张量积带来的开销。该框架可高效构建高阶张量预测模型。

Result: 在顺序2电介质、顺序3压电、顺序4弹性张量的基准数据集上，CEITNet在关键准确性指标上超越了现有的高阶预测方法，同时保持较高的计算效率。

Conclusion: CEITNet提供了一种高效且准确的高阶晶体张量预测框架，通过在通道空间学习并用笛卡尔张量基组组装等变输出来实现高阶张量的端到端预测，具有良好的可扩展性和实用性。

Abstract: End-to-end prediction of high-order crystal tensor properties from atomic structures remains challenging: while spherical-harmonic equivariant models are expressive, their Clebsch-Gordan tensor products incur substantial compute and memory costs for higher-order targets. We propose the Cartesian Environment Interaction Tensor Network (CEITNet), an approach that constructs a multi-channel Cartesian local environment tensor for each atom and performs flexible many-body mixing via a learnable channel-space interaction. By performing learning in channel space and using Cartesian tensor bases to assemble equivariant outputs, CEITNet enables efficient construction of high-order tensor. Across benchmark datasets for order-2 dielectric, order-3 piezoelectric, and order-4 elastic tensor prediction, CEITNet surpasses prior high-order prediction methods on key accuracy criteria while offering high computational efficiency.

</details>


### [128] [UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching](https://arxiv.org/abs/2602.04344)
*Kou Misaki,Takuya Akiba*

Main category: cs.LG

TL;DR: 提出 UnMaskFork (UMF)，将未遮蔽轨迹视为搜索树，通过蒙特卡洛树搜索优化生成路径，利用多模型的确定性部分未遮蔽，优于现有测试时刻的随机采样基线，在复杂编码任务和数学推理任务上具有良好性能和扩展性。


<details>
  <summary>Details</summary>
Motivation: 弥补面向自回归大模型的测试时刻缩放在推理时间资源上的局限，利用 MDLM 的迭代非自回归特性，通过搜索策略提升推理能力。

Method: 提出 UMF 框架，将未遮蔽轨迹形式化为搜索树，使用 Monte Carlo Tree Search 进行路径优化；通过多模型进行确定性部分未遮蔽操作，区别于随机采样的缩放方法。

Result: 实验表明 UMF 在复杂编码基准上持续优于现有测试时刻缩放基线，并在数学推理任务上展现良好可扩展性。

Conclusion: 证实基于搜索的推理在 MDLM 上的潜力，提供相对于采样的测试时刻缩放的有效替代方案，提升推理质量和可扩展性。

Abstract: Test-time scaling strategies have effectively leveraged inference-time compute to enhance the reasoning abilities of Autoregressive Large Language Models. In this work, we demonstrate that Masked Diffusion Language Models (MDLMs) are inherently amenable to advanced search strategies, owing to their iterative and non-autoregressive generation process. To leverage this, we propose UnMaskFork (UMF), a framework that formulates the unmasking trajectory as a search tree and employs Monte Carlo Tree Search to optimize the generation path. In contrast to standard scaling methods relying on stochastic sampling, UMF explores the search space through deterministic partial unmasking actions performed by multiple MDLMs. Our empirical evaluation demonstrates that UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks, while also exhibiting strong scalability on mathematical reasoning tasks.

</details>


### [129] [Convolution Operator Network for Forward and Inverse Problems (FI-Conv): Application to Plasma Turbulence Simulations](https://arxiv.org/abs/2602.04287)
*Xingzhuo Chen,Anthony Poole,Ionut-Gabriel Farcas,David R. Hatch,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: FI-Conv是一种基于U-Net、采用ConvNeXt V2模块的前向预测与反问题框架，能够对复杂时空动力学（如湍流）进行系统演化预测及参数估计；在Hasegawa–Wakatani湍流方程的二维等离子体场上进行验证。


<details>
  <summary>Details</summary>
Motivation: 应对具有强非线性和高频变化的时空动力学（如湍流）所带来的预测与参数估计挑战，寻求在保持较低计算复杂度的同时实现前向推断与反问题求解的统一框架。

Method: FI-Conv在U-Net架构中将大部分卷积层替换为ConvNeXt V2块；以初始态、PDE参数和演化时间作为输入，进行系统未来状态的预测；使用自回归（autoregressive）方式进行短时前向预测；通过梯度下降法在不修改模型权重的前提下对PDE参数进行反问题估计；在二维Hasegawa–Wakatani湍流场上评估。

Result: 在短时（约t=3）实现准确的前向预测；在更长时间尺度（约t=100）保持观测量的统计性质的一致性；通过梯度下降实现对PDE参数的准确估计，且作为一种对比于现有物理信息机器学习方法的有效替代方案。

Conclusion: FI-Conv对具有复杂时空动力学的系统提供了一种高效且有效的前向与反问题解决方案，兼具良好预测性能与参数推断能力，且计算复杂度较低。

Abstract: We propose the Convolutional Operator Network for Forward and Inverse Problems (FI-Conv), a framework capable of predicting system evolution and estimating parameters in complex spatio-temporal dynamics, such as turbulence. FI-Conv is built on a U-Net architecture, in which most convolutional layers are replaced by ConvNeXt V2 blocks. This design preserves U-Net performance on inputs with high-frequency variations while maintaining low computational complexity. FI-Conv uses an initial state, PDE parameters, and evolution time as input to predict the system future state. As a representative example of a system exhibiting complex dynamics, we evaluate the performance of FI-Conv on the task of predicting turbulent plasma fields governed by the Hasegawa-Wakatani (HW) equations. The HW system models two-dimensional electrostatic drift-wave turbulence and exhibits strongly nonlinear behavior, making accurate approximation and long-term prediction particularly challenging. Using an autoregressive forecasting procedure, FI-Conv achieves accurate forward prediction of the plasma state evolution over short times (t ~ 3) and captures the statistic properties of derived physical quantities of interest over longer times (t ~ 100). Moreover, we develop a gradient-descent-based inverse estimation method that accurately infers PDE parameters from plasma state evolution data, without modifying the trained model weights. Collectively, our results demonstrate that FI-Conv can be an effective alternative to existing physics-informed machine learning methods for systems with complex spatio-temporal dynamics.

</details>


### [130] [Counterfactual Explanations for Hypergraph Neural Networks](https://arxiv.org/abs/2602.04360)
*Fabiano Veglianti,Lorenzo Antonelli,Gabriele Tolomei*

Main category: cs.LG

TL;DR: 提出 CF-HyperGNNExplainer：一个面向 Hypergraph Neural Network 的对抗性解释器，通过对结构进行最小干预来使预测改变，并生成简洁、可操作的解释。


<details>
  <summary>Details</summary>
Motivation: HGNNs 能建模高阶关系，但可解释性不足，限制在高风险场景的应用。需要可操作、结构层面的解释来揭示对决策影响最大的高阶关系。

Method: 生成对比性（counterfactual）超图，通过限制编辑操作为删除节点-超边的入度关系或直接删除超边，得到最小化的结构修改以改变预测的对比样本，并输出简洁、结构上有意义的解释。

Result: 在三个基准数据集上验证，CF-HyperGNNExplainer 能产生有效且简洁的对比性解释，突出对 HGNN 决策影响最大的高阶关系。

Conclusion: 所提方法提供一种可操作且结构层面的解释框架，助力将 HGNN 的高阶关系洞见落地于需要透明度的应用场景。

Abstract: Hypergraph neural networks (HGNNs) effectively model higher-order interactions in many real-world systems but remain difficult to interpret, limiting their deployment in high-stakes settings.
  We introduce CF-HyperGNNExplainer, a counterfactual explanation method for HGNNs that identifies the minimal structural changes required to alter a model's prediction. The method generates counterfactual hypergraphs using actionable edits limited to removing node-hyperedge incidences or deleting hyperedges, producing concise and structurally meaningful explanations. Experiments on three benchmark datasets show that CF-HyperGNNExplainer generates valid and concise counterfactuals, highlighting the higher-order relations most critical to HGNN decisions.

</details>


### [131] [Blockchain Federated Learning for Sustainable Retail: Reducing Waste through Collaborative Demand Forecasting](https://arxiv.org/abs/2602.04384)
*Fabio Turazza,Alessandro Neri,Marcello Pietri,Maria Angela Butturi,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: Federated Learning (FL) with blockchain is applied to privacy-preserving demand forecasting in SSCM for perishable groceries. An isolated-retailer baseline is extended to a Blockchain-based FL model across retailers without data sharing. FL performance approaches the ideal data-sharing scenario and surpasses single-retailer models, reducing waste and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Data privacy concerns hinder cross-retailer collaboration, limiting predictive accuracy for perishable goods. Federated learning offers collaborative modeling without sharing sensitive data; blockchain can provide secure, auditable coordination.

Method: 1) Develop a baseline predictive model for demand forecasting and waste assessment for an isolated retailer. 2) Implement a Blockchain-based FL setup allowing multiple retailers to train a global model without direct data sharing. 3) Compare performance against (a) ideal centralized data-sharing baseline and (b) individual retailers’ isolated models. 4) Evaluate waste reduction and efficiency gains.

Result: Preliminary results indicate that FL models achieve performance close to the ideal data-sharing setting and outperform models trained by single retailers, achieving reductions in waste and improvements in efficiency.

Conclusion: Blockchain-based FL can enable privacy-preserving collaboration among retailers with near-centralized performance. It offers practical benefits for SSCM, but further work is needed on deployment challenges, data heterogeneity, scalability, and security in real-world settings.

Abstract: Effective demand forecasting is crucial for reducing food waste. However, data privacy concerns often hinder collaboration among retailers, limiting the potential for improved predictive accuracy. In this study, we explore the application of Federated Learning (FL) in Sustainable Supply Chain Management (SSCM), with a focus on the grocery retail sector dealing with perishable goods. We develop a baseline predictive model for demand forecasting and waste assessment in an isolated retailer scenario. Subsequently, we introduce a Blockchain-based FL model, trained collaboratively across multiple retailers without direct data sharing. Our preliminary results show that FL models have performance almost equivalent to the ideal setting in which parties share data with each other, and are notably superior to models built by individual parties without sharing data, cutting waste and boosting efficiency.

</details>


### [132] [RISE: Interactive Visual Diagnosis of Fairness in Machine Learning Models](https://arxiv.org/abs/2602.04339)
*Ray Chen,Christan Grant*

Main category: cs.LG

TL;DR: RISE is a visualization tool that analyzes sorted residuals to reveal localized fairness disparities under domain shift, enabling subgroup comparisons across environments and post-hoc accuracy-fairness trade-off analysis.


<details>
  <summary>Details</summary>
Motivation: Scalar fairness metrics can obscure where disparities arise under domain shift; there is a need for interpretable, localized diagnostics that link residual patterns to fairness notions.

Method: Introduce RISE (Residual Inspection through Sorted Evaluation), an interactive visualization that converts sorted residuals into interpretable patterns and connects them to formal fairness notions. Enables localized disparity diagnosis, subgroup comparisons across environments, and hidden-issue detection; supports post-hoc analysis of accuracy-fairness trade-offs.

Result: RISE provides a workflow for diagnosing fairness issues by inspecting residual curve structures, enabling localized disparity diagnosis, cross-environment subgroup comparisons, detection of hidden issues, and post-hoc analysis of accuracy-fairness trade-offs to inform model selection.

Conclusion: RISE offers interpretable, residual-based diagnostics for fairness under domain shift, facilitating more informed model selection through pattern-based insight into disparities.

Abstract: Evaluating fairness under domain shift is challenging because scalar metrics often obscure exactly where and how disparities arise. We introduce \textit{RISE} (Residual Inspection through Sorted Evaluation), an interactive visualization tool that converts sorted residuals into interpretable patterns. By connecting residual curve structures to formal fairness notions, RISE enables localized disparity diagnosis, subgroup comparison across environments, and the detection of hidden fairness issues. Through post-hoc analysis, RISE exposes accuracy-fairness trade-offs that aggregate statistics miss, supporting more informed model selection.

</details>


### [133] [LoRDO: Distributed Low-Rank Optimization with Infrequent Communication](https://arxiv.org/abs/2602.04396)
*Andrej Jovanović,Alex Iacob,Mher Safaryan,Ionut-Vlad Modoranu,Lorenzo Sani,William F. Shen,Xinchi Qiu,Dan Alistarh,Nicholas D. Lane*

Main category: cs.LG

TL;DR: LoRDO 将低秩优化与低频同步统一起来，通过引入全秩准双曲更新来维持子空间探索，在语言建模及下游任务中实现接近低秩 DDP 的性能，同时降低约10倍的通信成本，且在低内存场景下，随着秩和批量大小减小，性能提升更明显。


<details>
  <summary>Details</summary>
Motivation: 解决分布式训练中互连带宽瓶颈，同时克服低秩优化在本地更新（缺乏全量梯度）时对性能的下降，以及全局投影将梯度投影固定在低秩子空间的问题。

Method: 提出 LoRDO 框架，将低秜优化与不频繁同步结合；分析基于伪梯度的全局投影在理论上的优势但会限制子空间探索；为恢复子空间探索，提出全秩准双曲更新以扩展搜索。

Result: 在 1.25e8–7.2e8 参数规模的语言模型和下游任务上实现接近低秩 DDP 的性能，同时将通信量降低约 10×；在非常低内存设置、秩/批量较小的情况下性能提升更明显。

Conclusion: LoRDO 成为一种将低秩优化与不频繁同步结合的原理性框架，通过引入全秩准双曲更新来避免受限于低秩子空间，实现了显著的通信效率提升和性能接近的效果。

Abstract: Distributed training of foundation models via $\texttt{DDP}$ is limited by interconnect bandwidth. While infrequent communication strategies reduce synchronization frequency, they remain bottlenecked by the memory and communication requirements of optimizer states. Low-rank optimizers can alleviate these constraints; however, in the local-update regime, workers lack access to the full-batch gradients required to compute low-rank projections, which degrades performance. We propose $\texttt{LoRDO}$, a principled framework unifying low-rank optimization with infrequent synchronization. We first demonstrate that, while global projections based on pseudo-gradients are theoretically superior, they permanently restrict the optimization trajectory to a low-rank subspace. To restore subspace exploration, we introduce a full-rank quasi-hyperbolic update. $\texttt{LoRDO}$ achieves near-parity with low-rank $\texttt{DDP}$ in language modeling and downstream tasks at model scales of $125$M--$720$M, while reducing communication by $\approx 10 \times$. Finally, we show that $\texttt{LoRDO}$ improves performance even more in very low-memory settings with small rank/batch size.

</details>


### [134] [MirrorLA: Reflecting Feature Map for Vision Linear Attention](https://arxiv.org/abs/2602.04346)
*Weikang Meng,Liangyu Huo,Yadan Luo,Yaowei Wang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出 MirrorLA，通过可学习的 Householder 反射将线性注意力的特征投影从非负正交域中移出，提升信息密度，实现线性复杂度下接近或优于 softmax 注意力的性能，并在多尺度设计中优化局部判别、长上下文稳定性及跨头协方差混合，达到标准基准的最优表现。


<details>
  <summary>Details</summary>
Motivation: 识别线性注意力在性能上落后 softmax 的根本原因：核特征映射的非负约束导致对负域信息的被动截断，丢失语义信息。

Method: 提出 MirrorLA，利用可学习的 Householder 反射将特征几何旋转进入非负正交域，同时保持信息密度。通过多尺度设计：局部层面用块级等距变换优化局部判别，使用方差感知调制稳定长时上下文激活的多样性，通过跨头反射实现跨头子空间整合以促进全局协方差混合。

Result: 在标准基准上达到或接近当前最优，表明严格线性复杂度下也可实现与 softmax 相当甚至更好的表示能力。

Conclusion: MirrorLA 证明线性效率与表示保真度并非不可兼容，提供一个几何框架来提升线性注意力的性能，具有良好的可扩展性和潜在的广泛应用前景。

Abstract: Linear attention significantly reduces the computational complexity of Transformers from quadratic to linear, yet it consistently lags behind softmax-based attention in performance. We identify the root cause of this degradation as the non-negativity constraint imposed on kernel feature maps: standard projections like ReLU act as "passive truncation" operators, indiscriminately discarding semantic information residing in the negative domain. We propose MirrorLA, a geometric framework that substitutes passive truncation with active reorientation. By leveraging learnable Householder reflections, MirrorLA rotates the feature geometry into the non-negative orthant to maximize information retention. Our approach restores representational density through a cohesive, multi-scale design: it first optimizes local discriminability via block-wise isometries, stabilizes long-context dynamics using variance-aware modulation to diversify activations, and finally, integrates dispersed subspaces via cross-head reflections to induce global covariance mixing. MirrorLA achieves state-of-the-art performance across standard benchmarks, demonstrating that strictly linear efficiency can be achieved without compromising representational fidelity.

</details>


### [135] [EXaMCaP: Subset Selection with Entropy Gain Maximization for Probing Capability Gains of Large Chart Understanding Training Sets](https://arxiv.org/abs/2602.04365)
*Jiapeng Liu,Liang Li,Bing Li,Peng Fu,Xiyan Gao,Chengyang Fang,Xiaoshuai Hao,Can Ma*

Main category: cs.LG

TL;DR: 提出 EXaMCaP，通过最大化集合熵来从大规模 ChartU 数据集中选取高信息量子集，以高效评估全量微调带来的能力提升；采用迭代贪心策略近似最大熵子集，实验表明在不同子集规模和多种 MLLM 架构上优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决对 ChartU 训练集所致能力提升的评估需要对全量模型进行昂贵的微调，导致迭代式数据集构建困难；数据多样性对提升性能至关重要，熵可作为衡量多样性的量化指标。

Method: 提出 EXaMCaP，利用熵增最大化来选取子集；从大规模 ChartU 数据集中筛选最大熵子集；由于穷举不可行，采用逐步贪心策略，在当前集合的基础上最大化集合熵的增益，从而近似全量最大熵子集。

Result: 实验结果显示 EXaMCaP 在探测 ChartU 数据集带来的能力提升方面优于基线方案，并在不同子集规模和多种 MLLM 架构上表现出强鲁棒性和普适性。

Conclusion: 基于熵增的子集选择方法有效提升评估流程的效率与数据多样性，促进 ChartU 数据集的迭代优化，且与多种 MLLM 架构保持良好兼容性。

Abstract: Recent works focus on synthesizing Chart Understanding (ChartU) training sets to inject advanced chart knowledge into Multimodal Large Language Models (MLLMs), where the sufficiency of the knowledge is typically verified by quantifying capability gains via the fine-tune-then-evaluate paradigm. However, full-set fine-tuning MLLMs to assess such gains incurs significant time costs, hindering the iterative refinement cycles of the ChartU dataset. Reviewing the ChartU dataset synthesis and data selection domains, we find that subsets can potentially probe the MLLMs' capability gains from full-set fine-tuning. Given that data diversity is vital for boosting MLLMs' performance and entropy reflects this feature, we propose EXaMCaP, which uses entropy gain maximization to select a subset. To obtain a high-diversity subset, EXaMCaP chooses the maximum-entropy subset from the large ChartU dataset. As enumerating all possible subsets is impractical, EXaMCaP iteratively selects samples to maximize the gain in set entropy relative to the current set, approximating the maximum-entropy subset of the full dataset. Experiments show that EXaMCaP outperforms baselines in probing the capability gains of the ChartU training set, along with its strong effectiveness across diverse subset sizes and compatibility with various MLLM architectures.

</details>


### [136] [Multi-scale hypergraph meets LLMs: Aligning large language models for time series analysis](https://arxiv.org/abs/2602.04369)
*Zongjiang Shang,Dongliang Cui,Binqing Wu,Ling Chen*

Main category: cs.LG

TL;DR: 提出 MSH-LLM，通过多尺度超图、跨模态对齐和提示混合提高 LLM 在时间序列分析中的表现，在27个数据集的5个应用上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言与时间序列在多尺度结构上的对齐不足，充分发挥LLM在时序任务中的潜力。

Method: 引入超边缘化机制（hyperedging）增强多尺度语义信息；设计跨模态对齐(CMA)在不同尺度实现模态对齐；提出提示混合 MoP 以提供上下文并帮助理解多尺度时间模式。

Result: 在27个真实数据集、5类应用上达到SOTA，实验结果显著优于基线。

Conclusion: 该方法证明多尺度超图+CMA+MoP 能有效提升时间序列的跨模态建模与多尺度分析能力，具有较高的应用潜力。

Abstract: Recently, there has been great success in leveraging pre-trained large language models (LLMs) for time series analysis. The core idea lies in effectively aligning the modality between natural language and time series. However, the multi-scale structures of natural language and time series have not been fully considered, resulting in insufficient utilization of LLMs capabilities. To this end, we propose MSH-LLM, a Multi-Scale Hypergraph method that aligns Large Language Models for time series analysis. Specifically, a hyperedging mechanism is designed to enhance the multi-scale semantic information of time series semantic space. Then, a cross-modality alignment (CMA) module is introduced to align the modality between natural language and time series at different scales. In addition, a mixture of prompts (MoP) mechanism is introduced to provide contextual information and enhance the ability of LLMs to understand the multi-scale temporal patterns of time series. Experimental results on 27 real-world datasets across 5 different applications demonstrate that MSH-LLM achieves the state-of-the-art results.

</details>


### [137] [Reducing the labeling burden in time-series mapping using Common Ground: a semi-automated approach to tracking changes in land cover and species over time](https://arxiv.org/abs/2602.04373)
*Geethen Singh,Jasper A Slingsby,Tamara B Robinson,Glenn Moncrieff*

Main category: cs.LG

TL;DR: 以时序稳定区域为隐式监督的半监督框架“Common Ground”，利用仅从时点t0获得的参考数据实现对未来时点t1的有效泛化，在多种传感器与分类任务上超越单时点训练的金标准与朴素时间传递，尤其在入侵树种识别任务中有显著提升；对广域土地覆盖映射的提升较小。


<details>
  <summary>Details</summary>
Motivation: 在遥感地表分类中，随时间推移标签成本高且数据更新滞后，迫切需要在仅用初始时点标签的条件下实现对未来时点的稳定泛化，提升标签效率。

Method: 提出Common Ground，将 temporally stable regions 作为隐式监督来源，结合半监督学习框架，对动态区域进行学习；在 Landsat-8、Sentinel-2、航空成像光谱等传感器和多类生态应用上进行广泛评估，比较Naive时间传递、Gold-standard等基线。

Result: 在入侵树种映射任务中，Common Ground 相较 naive 时间传递提高21-40%的分类准确率；相较金标准提高10-16%；在欧洲广域土地覆盖任务中提升约2%。

Conclusion: 通过稳健筛选的稳定区域与SSL结合，可实现可扩展、低标注需求的多时相遥感分类，且在某些任务上超过传统金标准，证明此类方法具备普遍适用性。

Abstract: Reliable classification of Earth Observation data depends on consistent, up-to-date reference labels. However, collecting new labelled data at each time step remains expensive and logistically difficult, especially in dynamic or remote ecological systems. As a response to this challenge, we demonstrate that a model with access to reference data solely from time step t0 can perform competitively on both t0 and a future time step t1, outperforming models trained separately on time-specific reference data (the gold standard). This finding suggests that effective temporal generalization can be achieved without requiring manual updates to reference labels beyond the initial time step t0. Drawing on concepts from change detection and semi-supervised learning (SSL), the most performant approach, "Common Ground", uses a semi-supervised framework that leverages temporally stable regions-areas with little to no change in spectral or semantic characteristics between time steps-as a source of implicit supervision for dynamic regions. We evaluate this strategy across multiple classifiers, sensors (Landsat-8, Sentinel-2 satellite multispectral and airborne imaging spectroscopy), and ecological use cases. For invasive tree species mapping, we observed a 21-40% improvement in classification accuracy using Common Ground compared to naive temporal transfer, where models trained at a single time step are directly applied to a future time step. We also observe a 10 -16% higher accuracy for the introduced approach compared to a gold-standard approach. In contrast, when broad land cover categories were mapped across Europe, we observed a more modest 2% increase in accuracy compared to both the naive and gold-standard approaches. These results underscore the effectiveness of combining stable reference screening with SSL for scalable and label-efficient multi-temporal remote sensing classification.

</details>


### [138] [Theory of Speciation Transitions in Diffusion Models with General Class Structure](https://arxiv.org/abs/2602.04404)
*Beatrice Achilli,Marco Benedetti,Giulio Biroli,Marc Mézard*

Main category: cs.LG

TL;DR: A general theory of speciation in diffusion models for arbitrary target distributions with class structure; speciation times are governed by free-entropy differences between classes, extending beyond first-moment separability; supports multiple classes and successive speciation; validated on Ising-model mixtures and Gaussian mixtures with distinct covariances, with explicit results via replica method.


<details>
  <summary>Details</summary>
Motivation: Existing analyses only handle class separability via first moments (e.g., Gaussian mixtures with well-separated means). A general framework is needed to capture speciation when classes differ in higher-order or collective features and when multiple refinement levels of class structure exist.

Method: Formalize class structure through Bayes classification and derive speciation times from free-entropy differences between classes. Extend to multiple classes and analyze successive speciation times. Apply analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures (mapped to a random-field Ising model and solved via the replica method) and mixtures of zero-mean Gaussians with distinct covariance structures.

Result: The framework recovers known results for Gaussian mixtures and extends to higher-order features; provides explicit expressions for speciation times in the Ising case; predicts multiple speciation times corresponding to progressively finer class commitments.

Conclusion: Provides a unified, broadly applicable description of speciation transitions in diffusion-based generative models, capable of handling arbitrary target distributions and hierarchical class structures.

Abstract: Diffusion Models generate data by reversing a stochastic diffusion process, progressively transforming noise into structured samples drawn from a target distribution. Recent theoretical work has shown that this backward dynamics can undergo sharp qualitative transitions, known as speciation transitions, during which trajectories become dynamically committed to data classes. Existing theoretical analyses, however, are limited to settings where classes are identifiable through first moments, such as mixtures of Gaussians with well-separated means. In this work, we develop a general theory of speciation in diffusion models that applies to arbitrary target distributions admitting well-defined classes. We formalize the notion of class structure through Bayes classification and characterize speciation times in terms of free-entropy difference between classes. This criterion recovers known results in previously studied Gaussian-mixture models, while extending to situations in which classes are not distinguishable by first moments and may instead differ through higher-order or collective features. Our framework also accommodates multiple classes and predicts the existence of successive speciation times associated with increasingly fine-grained class commitment. We illustrate the theory on two analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures and mixtures of zero-mean Gaussians with distinct covariance structures. In the Ising case, we obtain explicit expressions for speciation times by mapping the problem onto a random-field Ising model and solving it via the replica method. Our results provide a unified and broadly applicable description of speciation transitions in diffusion-based generative models.

</details>


### [139] [Separation-Utility Pareto Frontier: An Information-Theoretic Characterization](https://arxiv.org/abs/2602.04408)
*Shizhou Xu*

Main category: cs.LG

TL;DR: 建立在信息论框架上的工具性与分离性的帕累托前沿分析：给出前沿的可描述性、凹性及分离成本的证明，提出基于条件互信息的正则化器用于深度模型训练，以在保持或提升效用的同时减少对敏感属性的预测分离。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中实现予以保障的公平性（预测独立性，条件于真实结果）与预测效用之间的权衡需要一个系统的理论框架。本文通过信息理论角度刻画Pareto前沿，解释分离性的边际成本，并为实际训练提供一个可优化、可解释的正则化工具。

Method: 对效用-分离的 Pareto前沿进行信息理论刻画，证明其凹性及分离成本随效用的递增性；给出在何种条件下该权衡会变得严格的定性/定量条件。基于理论结果，提出基于条件互信息（CMI）的正则化项，用于衡量预测与敏感属性在真实结果给定时的依赖，并可与任意深度模型的梯度优化兼容，作为训练过程中的可行约束与监测指标。

Result: 给出Pareto前沿的理论表征和凹性证明，并给出分离成本的递增性结论；明确了严格权衡的条件；提出CMI正则化器及其与深度模型的兼容性；在COMPAS、UCI Adult、UCI Bank、CelebA等数据集上实验显示方法显著降低分离违规，同时保持或提升效用，与现有基线方法相比具有竞争力。

Conclusion: 提供一个可证明、稳定且灵活的框架，用以在深度学习中强制实现分离性。CMI正则化器为训练过程提供可控、可解释的指标与保证，推动在实际任务中对效用与分离性的权衡具备更强的理论基础与实践性。

Abstract: We study the Pareto frontier (optimal trade-off) between utility and separation, a fairness criterion requiring predictive independence from sensitive attributes conditional on the true outcome. Through an information-theoretic lens, we prove a characterization of the utility-separation Pareto frontier, establish its concavity, and thereby prove the increasing marginal cost of separation in terms of utility. In addition, we characterize the conditions under which this trade-off becomes strict, providing a guide for trade-off selection in practice. Based on the theoretical characterization, we develop an empirical regularizer based on conditional mutual information (CMI) between predictions and sensitive attributes given the true outcome. The CMI regularizer is compatible with any deep model trained via gradient-based optimization and serves as a scalar monitor of residual separation violations, offering tractable guarantees during training. Finally, numerical experiments support our theoretical findings: across COMPAS, UCI Adult, UCI Bank, and CelebA, the proposed method substantially reduces separation violations while matching or exceeding the utility of established baseline methods. This study thus offers a provable, stable, and flexible approach to enforcing separation in deep learning.

</details>


### [140] [Rethinking the Design Space of Reinforcement Learning for Diffusion Models: On the Importance of Likelihood Estimation Beyond Loss Design](https://arxiv.org/abs/2602.04663)
*Jaemoo Choi,Yuchen Zhu,Wei Guo,Petr Molodyk,Bo Yuan,Jinbin Bai,Yi Xin,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 系统性分析 RL 设计空间在扩散/流动模型中的应用，发现基于 ELBO 的最终样本似然估计是提升 RL 优化效果的关键因素，其次才是策略梯度损失形式；在多任务和基准上表现一致，显著提高 GenEval 分数并提升效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的似然性不可直接求解，限制了基于策略梯度的强化学习应用。现有方法多以自推导/工程化的目标和近似估计来处理似然性，缺乏对估计对算法性能影响的系统研究。

Method: 将 RL 设计要素拆解为三部分：策略梯度目标、似然估计器、 rollout 采样方案。在此框架下，采用仅来自最终生成样本的 ELBO 基于模型似然估计器，并对比不同要素对性能的影响，基于 SD 3.5 Medium 在多项奖励基准上评估。

Result: 最关键因素是基于 ELBO 的最终样本似然估计，显著提升稳定性与效率，超越具体策略梯度损失的影响。GenEval 分数从 0.24 提升到 0.95；在 90 GPU 小时内实现，比 FlowGRPO 快 4.6×，比 DiffusionNFT 快 2×（且无“奖惩攻击”）。

Conclusion: 以最终样本的 ELBO 估计为主导的似然处理，是实现高效、稳定 RL 优化的关键，且该设计在多任务中具有一致性提升。

Abstract: Reinforcement learning has been widely applied to diffusion and flow models for visual tasks such as text-to-image generation. However, these tasks remain challenging because diffusion models have intractable likelihoods, which creates a barrier for directly applying popular policy-gradient type methods. Existing approaches primarily focus on crafting new objectives built on already heavily engineered LLM objectives, using ad hoc estimators for likelihood, without a thorough investigation into how such estimation affects overall algorithmic performance. In this work, we provide a systematic analysis of the RL design space by disentangling three factors: i) policy-gradient objectives, ii) likelihood estimators, and iii) rollout sampling schemes. We show that adopting an evidence lower bound (ELBO) based model likelihood estimator, computed only from the final generated sample, is the dominant factor enabling effective, efficient, and stable RL optimization, outweighing the impact of the specific policy-gradient loss functional. We validate our findings across multiple reward benchmarks using SD 3.5 Medium, and observe consistent trends across all tasks. Our method improves the GenEval score from 0.24 to 0.95 in 90 GPU hours, which is $4.6\times$ more efficient than FlowGRPO and $2\times$ more efficient than the SOTA method DiffusionNFT without reward hacking.

</details>


### [141] [Delving into Muon and Beyond: Deep Analysis and Extensions](https://arxiv.org/abs/2602.04669)
*Xianbiao Qi,Marco Chen,Jiaquan Ye,Yelin He,Rong Xiao*

Main category: cs.LG

TL;DR: 提出一个统一的谱视角，将 Muon 视为谱变换族 p=0 的端点，并比较不同 p 值对第一矩更新和 RMS 更新的影响；实验证明 RMS 更新更稳定，谱压缩在第一矩情形有稳定性提升；Muon 未普遍优于 Adam，更多地体现为谱归一化，而非通用的优化器超越。


<details>
  <summary>Details</summary>
Motivation: 揭示 Muon 的作用机制及其与自适应优化器（如 Adam）的关系，提供一个统一的谱变换框架来解释和比较不同更新规则。

Method: 将更新表示为变换 U Σ^p V^T 的谱变换，p ∈ {0, 1/2, 1/4, 1}，分别作用于第一矩更新（动量 SGD）和 RMS 标准化梯度更新（类似 Adam），并提出耦合 Newton 迭代以避免显式奇异值分解。

Result: RMS 标准化的更新相比第一矩更新更稳定；谱压缩在第一矩更新下提供显著稳定性；Muon（p=0）未能在普遍性上明显超越 Adam；结论是 Muon 只是一个有效的谱归一化工具，而非普适的更优优化器。

Conclusion:  Muon 更像一种谱归一化的实现，其优势在于稳定性和正则化效应，而非在所有场景中优于 Adam；论文建议从谱归一化角度理解 Muon，并提供代码实现以供复现实验。

Abstract: The Muon optimizer has recently attracted considerable attention for its strong empirical performance and use of orthogonalized updates on matrix-shaped parameters, yet its underlying mechanisms and relationship to adaptive optimizers such as Adam remain insufficiently understood. In this work, we aim to address these questions through a unified spectral perspective. Specifically, we view Muon as the p = 0 endpoint of a family of spectral transformations of the form U \boldsymbolΣ^{p} V' , and consider additional variants with p = 1/2 , p = 1/4 , and p = 1 . These transformations are applied to both first-moment updates, as in momentum SGD, and to root-mean-square (RMS) normalized gradient updates as in Adam. To enable efficient computation, we develop a coupled Newton iteration that avoids explicit singular value decomposition. Across controlled experiments, we find that RMS-normalized updates yield more stable optimization than first-moment updates. Moreover, while spectral compression provides strong stabilization benefits under first-moment updates, the Muon update (p = 0) does not consistently outperform Adam. These results suggest that Muon is best understood as an effective form of spectral normalization, but not a universally superior optimization method. Our source code will be released at https://github.com/Ocram7/BeyondMuon.

</details>


### [142] [Let Experts Feel Uncertainty: A Multi-Expert Label Distribution Approach to Probabilistic Time Series Forecasting](https://arxiv.org/abs/2602.04678)
*Zhen Zhou,Zhirui Wang,Qi Hong,Yunyang Shi,Ziyuan Gu,Zhiyuan Liu*

Main category: cs.LG

TL;DR: 提出基于多专家的 LDL 分布式标签框架，用于时间序列的分布式学习与不确定性量化，通过 MoE 架构实现两种方法：多专家 LDL 与 Pattern-Aware LDL-MoE，利用 MMD 进行不确定性评估，在 M5 数据的聚合销售上优于基线，并在可解释性方面提供分量分析。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列需要高预测准确性和可解释的不确定性量化。传统点预测和现有概率方法要么缺乏不确定性建模，要么在可解释性/计算效率之间取舍。

Method: 提出两种框架：1) Multi-Expert LDL：由多个具有不同参数的专家组成，以捕捉多样化的时序模式；2) Pattern-Aware LDL-MoE：通过专门的子专家将时间序列分解为趋势、季节性、变点、波动等可解释组成部分。两者将点预测扩展为分布学习，通过最大均值差异（MMD）实现不确定性量化。

Result: 在基于 M5 的聚合销售数据上评估，连续的 Multi-Expert LDL 取得最佳整体性能；Pattern-Aware LDL-MoE 在可解释性方面表现更好，能够进行分量级分析。

Conclusion: 框架在预测准确性和可解释性之间取得平衡，适用于需要可操作洞察的实际预测场景。

Abstract: Time series forecasting in real-world applications requires both high predictive accuracy and interpretable uncertainty quantification. Traditional point prediction methods often fail to capture the inherent uncertainty in time series data, while existing probabilistic approaches struggle to balance computational efficiency with interpretability. We propose a novel Multi-Expert Learning Distributional Labels (LDL) framework that addresses these challenges through mixture-of-experts architectures with distributional learning capabilities. Our approach introduces two complementary methods: (1) Multi-Expert LDL, which employs multiple experts with different learned parameters to capture diverse temporal patterns, and (2) Pattern-Aware LDL-MoE, which explicitly decomposes time series into interpretable components (trend, seasonality, changepoints, volatility) through specialized sub-experts. Both frameworks extend traditional point prediction to distributional learning, enabling rich uncertainty quantification through Maximum Mean Discrepancy (MMD). We evaluate our methods on aggregated sales data derived from the M5 dataset, demonstrating superior performance compared to baseline approaches. The continuous Multi-Expert LDL achieves the best overall performance, while the Pattern-Aware LDL-MoE provides enhanced interpretability through component-wise analysis. Our frameworks successfully balance predictive accuracy with interpretability, making them suitable for real-world forecasting applications where both performance and actionable insights are crucial.

</details>


### [143] [Identifying Intervenable and Interpretable Features via Orthogonality Regularization](https://arxiv.org/abs/2602.04718)
*Moritz Miller,Florent Draye,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 在固定稀疏自编码器框架下，将解码器矩阵分解为几乎正交的特征并施加正交性惩罚，从而提升特征的可识别性与模块性，同时在不显著损失性能的前提下实现对因果干预的可行性，代码已公开。


<details>
  <summary>Details</summary>
Motivation: 解决特征干扰与叠加导致的可解释性不足，期望通过正交化实现特征分离、唯一性与对因果机制的适用性。

Method: 将解码器矩阵分解为近似正交的特征，并引入正交性惩罚；评估对目标数据集的性能、特征解释距离随惩罚变化、以及在更严格惩罚下的干预可行性；提供开源实现。

Result: 正交性惩罚提升特征的可识别性和唯一性；嵌入式特征解释之间的距离随惩罚增大而增大；对任务性能影响微小；可实现孤立干预；支持独立因果机制的模块化表示。

Conclusion: 正交化的解码器特征有利于解释性和可干预性的提升，且保持性能，验证了独立因果机制下的模块化表示的可行性，代码公开。

Abstract: With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogonality penalty, a desirable property for interpretability. Invoking the $\textit{Independent Causal Mechanisms}$ principle, we argue that orthogonality promotes modular representations amenable to causal intervention. We empirically show that these increasingly orthogonalized features allow for isolated interventions. Our code is available under $\texttt{https://github.com/mrtzmllr/sae-icm}$.

</details>


### [144] [Subliminal Effects in Your Data: A General Mechanism via Log-Linearity](https://arxiv.org/abs/2602.04863)
*Ishaq Aden-Ali,Noah Golowich,Allen Liu,Abhishek Shetty,Ankur Moitra,Nika Haghtalab*

Main category: cs.LG

TL;DR: 提出并验证了 Logit-Linear-Selection (LLS) 的通用子集选择机制，通过在真实数据子集上训练，观察到模型对情境、语言、人物等行为的持久影响，显示数据子集具备普遍性与可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明数据集能传递不可直接从单个 datapoint 推断的信号，需建立一个统一框架来解释数据隐藏子文本如何影响模型行为；结合对线性结构的启发，寻找能普遍产生隐藏效应的机制。

Method: 提出 LLS 方法，定义如何从通用偏好数据集中选取子集以引发多种隐藏效应；在真实数据集上应用 LLS，训练模型并评估其在选定子集上的行为是否保持跨不同架构的一致性；

Result: 通过所选子集，模型表现出特定偏好、对提示以不同语言响应、扮演不同人格等行为，且该效应在不同模型架构下持续存在，证明了方法的通用性与可迁移性。

Conclusion: LLS 提供一个普适框架来解释数据级隐藏信号的出现，强调数据子集对模型行为的显著影响，推动数据设计、模型对齐与安全性研究的发展。

Abstract: Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.
  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.

</details>


### [145] [Greedy-Gnorm: A Gradient Matrix Norm-Based Alternative to Attention Entropy for Head Pruning](https://arxiv.org/abs/2602.04491)
*Yuxi Guo,Paul Sheridan*

Main category: cs.LG

TL;DR: 动态的基于梯度的注意力头修剪（Greedy-Gnorm），在每次剪枝后重新计算头的重要性，以更好地在不断变化的头角色中保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 解决静态重要性分数导致的过时排序问题，使在逐步削减头时仍能保持模型性能并实现更高的压缩率。

Method: 对每个头的Q/K/V梯度块的l2范数进行元素级乘积作为头的重要性分数，并在保留集/验证集上估计后每次贪婪迭代更新；按分数从低到高依次剪枝，重复直到达到目标压缩率。

Result: 在BERT、ALBERT、RoBERTa、XLM-RoBERTa等模型上，Greedy-Gnorm在大规模头删除下能更好地保留准确性，优于基线的注意力熵方法，显示出更高的压缩效能和能效潜力。

Conclusion: 动态的梯度信息驱动的头剪枝能有效缓解排序陈旧问题，提升在迭代剪枝中的鲁棒性和效果，为可持续的Transformer模型部署提供一种有效手段。

Abstract: Attention head pruning has emerged as an effective technique for transformer model compression, an increasingly important goal in the era of Green AI. However, existing pruning methods often rely on static importance scores, which fail to capture the evolving role of attention heads during iterative removal. We propose Greedy-Gradient norm (Greedy-Gnorm), a novel head pruning algorithm that dynamically recalculates head importance after each pruning step. Specifically, each head is scored by the elementwise product of the l2-norms of its Q/K/V gradient blocks, as estimated from a hold-out validation set and updated at every greedy iteration. This dynamic approach to scoring mitigates against stale rankings and better reflects gradient-informed importance as pruning progresses. Extensive experiments on BERT, ALBERT, RoBERTa, and XLM-RoBERTa demonstrate that Greedy-Gnorm consistently preserves accuracy under substantial head removal, outperforming attention entropy. By effectively reducing model size while maintaining task performance, Greedy-Gnorm offers a promising step toward more energy-efficient transformer model deployment.

</details>


### [146] [Rethinking the Trust Region in LLM Reinforcement Learning](https://arxiv.org/abs/2602.04879)
*Penghui Qi,Xiangxin Zhou,Zichen Liu,Tianyu Pang,Chao Du,Min Lin,Wee Sun Lee*

Main category: cs.LG

TL;DR: 提出 DPPO，在 RL 语言模型微调中用直接的策略分歧约束取代 PPO 的比值裁剪，并通过 Binary/Top-K 近似降低内存开销，以提升稳定性与训练效率。


<details>
  <summary>Details</summary>
Motivation: PPO 的比值裁剪在大词汇表下对策略更新存在噪声估计，过度惩罚低概率 token，且对高概率 token 的约束不足，导致学习动力学的低效和不稳定。

Method: 用基于直接策略分歧（如总变差 TV 或 KL）的约束取代裁剪，设计 Binary 与 Top-K 近似来高效捕获分歧，避免巨额内存开销。

Result: 大量实验表明 DPPO 相较现有方法在训练稳定性和效率方面具有显著优势。

Conclusion: DPPO 为基于 RL 的 LLM 微调提供更鲁棒的优化基础，克服 PPO 在大 vocab 下的局限性。

Abstract: Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.

</details>


### [147] [Forget to Generalize: Iterative Adaptation for Generalization in Federated Learning](https://arxiv.org/abs/2602.04536)
*Abdulrahman Alotaibi,Irene Tenison,Miriam Kim,Isaac Lee,Lalana Kagal*

Main category: cs.LG

TL;DR: 提出 Iterative Federated Adaptation (IFA)，在分代训练中通过“忘记并进化”策略重置部分参数以打破局部极小值，从而提高异构联邦学习的泛化能力。实验显示平均提升约21.5%，可对接任意FL算法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中客户端数据呈现非独立同分布(non-IID)，导致联邦学习性能显著下降。需要能够在异质分布下提升全局泛化能力的训练范式。

Method: 将训练分成多代，在每代结束时按随机或从模型后部层中选取一部分参数并重新初始化，实现参数的遗忘与再进化。该策略可与任意FL算法结合，促使模型跳出局部最优并保留全局有意义的表示。

Result: 在 CIFAR-10、MIT-Indoors、 Stanford Dogs 数据集上，IFA显著提升全局准确率，尤其在跨客户端非IID场景中，平均提升约21.5%。

Conclusion: 该方法推动在现实异质分布下的可扩展隐私保护联邦学习的发展，可作为插件叠加到现有FL算法以提升泛化性能。

Abstract: The Web is naturally heterogeneous with user devices, geographic regions, browsing patterns, and contexts all leading to highly diverse, unique datasets. Federated Learning (FL) is an important paradigm for the Web because it enables privacy-preserving, collaborative machine learning across diverse user devices, web services and clients without needing to centralize sensitive data. However, its performance degrades severely under non-IID client distributions that is prevalent in real-world web systems. In this work, we propose a new training paradigm - Iterative Federated Adaptation (IFA) - that enhances generalization in heterogeneous federated settings through generation-wise forget and evolve strategy. Specifically, we divide training into multiple generations and, at the end of each, select a fraction of model parameters (a) randomly or (b) from the later layers of the model and reinitialize them. This iterative forget and evolve schedule allows the model to escape local minima and preserve globally relevant representations. Extensive experiments on CIFAR-10, MIT-Indoors, and Stanford Dogs datasets show that the proposed approach improves global accuracy, especially when the data cross clients are Non-IID. This method can be implemented on top any federated algorithm to improve its generalization performance. We observe an average of 21.5%improvement across datasets. This work advances the vision of scalable, privacy-preserving intelligence for real-world heterogeneous and distributed web systems.

</details>


### [148] [Finding Structure in Continual Learning](https://arxiv.org/abs/2602.04555)
*Pourya Shamsolmoali,Masoumeh Zareapoor*

Main category: cs.LG

TL;DR: 将持续学习问题视为两大目标的近端问题交由 Douglas–Rachford 分裂处理，通过两端近端算子迭代达成收敛一致，达到在不使用外部记忆或参数正则化的情况下实现稳定性和可塑性的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决连续学习中的灾难性遗忘和梯度冲突问题；现有方法多以叠加损失项来权衡，往往需要外部记忆回放或正则化等复杂策略。

Method: 将持续学习目标改写为两个解耦目标（促进新任务可塑性与维持旧知识稳定性），通过 Douglas–Rachford 分裂在迭代中利用各自的近端算子求解并收敛到共识，从而实现更为稳健的学习动态。

Result: 在不需要辅助模块或复杂附加元件的前提下实现稳定性与可塑性的高效平衡，提供了更简洁但更强的持续学习范式。

Conclusion: DRS 为持续学习体系提供一种更为系统化、稳健的优化框架，改进了稳定性–可塑性权衡的实现方式。

Abstract: Learning from a stream of tasks usually pits plasticity against stability: acquiring new knowledge often causes catastrophic forgetting of past information. Most methods address this by summing competing loss terms, creating gradient conflicts that are managed with complex and often inefficient strategies such as external memory replay or parameter regularization. We propose a reformulation of the continual learning objective using Douglas-Rachford Splitting (DRS). This reframes the learning process not as a direct trade-off, but as a negotiation between two decoupled objectives: one promoting plasticity for new tasks and the other enforcing stability of old knowledge. By iteratively finding a consensus through their proximal operators, DRS provides a more principled and stable learning dynamic. Our approach achieves an efficient balance between stability and plasticity without the need for auxiliary modules or complex add-ons, providing a simpler yet more powerful paradigm for continual learning systems.

</details>


### [149] [Billion-Scale Graph Foundation Models](https://arxiv.org/abs/2602.04768)
*Maya Bechler-Speicher,Yoel Gottlieb,Andrey Isakov,David Abensur,Ami Tavory,Daniel Haimovich,Ido Guy,Udi Weinsberg*

Main category: cs.LG

TL;DR: 提出 GraphBFF 框架，首次给出用于任意异质大规模图的亿参数图形 foundation 模型。提供 GraphBFF Transformer、神经尺度定律、数据分批、预训练和微调等端到端方法论，并在 10 个下游任务上实现 1.4B 参数、基于 10 亿样本的预训练，展现出强零-shot/少样本性能及尺度效应；并讨论工业放规模下的挑战与机会。


<details>
  <summary>Details</summary>
Motivation: 将自 foundation 模型的预训练+轻量化适应范式扩展至通用的现实世界图数据，解决大规模异质图上的建模与训练效率问题，提供端到端的 GFMs 架构与可操作方法。

Method: 提出 GraphBFF Transformer 作为可扩展的图基础模型架构，结合数据分批、预训练与微调的完整流程；提出针对一般图的神经尺度定律，揭示模型容量与训练数据对损失下降的瓶颈依赖；在一个十亿规模数据集上预训练 1.4B 参数的 GraphBFF Transformer，并在 10 组真实世界任务中进行零-shot、探针与少样本评估。

Result: 在十个下游任务上实现显著的零-shot/探针性能，尤其在少样本情形下，优势可达多项指标的巨大提升（最高约 31 PRAUC 点），并给出对比研究和规模化的实证证据，验证 GFM 的可行性与效用。

Conclusion: GFMs 在工业尺度上具备成为图学习基础的潜力，提供了端到端的构建、训练与微调指南，同时也揭示了实现过程中的关键挑战与未来机会。

Abstract: Graph-structured data underpins many critical applications. While foundation models have transformed language and vision via large-scale pretraining and lightweight adaptation, extending this paradigm to general, real-world graphs is challenging. In this work, we present Graph Billion- Foundation-Fusion (GraphBFF): the first end-to-end recipe for building billion-parameter Graph Foundation Models (GFMs) for arbitrary heterogeneous, billion-scale graphs. Central to the recipe is the GraphBFF Transformer, a flexible and scalable architecture designed for practical billion-scale GFMs. Using the GraphBFF, we present the first neural scaling laws for general graphs and show that loss decreases predictably as either model capacity or training data scales, depending on which factor is the bottleneck. The GraphBFF framework provides concrete methodologies for data batching, pretraining, and fine-tuning for building GFMs at scale. We demonstrate the effectiveness of the framework with an evaluation of a 1.4 billion-parameter GraphBFF Transformer pretrained on one billion samples. Across ten diverse, real-world downstream tasks on graphs unseen during training, spanning node- and link-level classification and regression, GraphBFF achieves remarkable zero-shot and probing performance, including in few-shot settings, with large margins of up to 31 PRAUC points. Finally, we discuss key challenges and open opportunities for making GFMs a practical and principled foundation for graph learning at industrial scale.

</details>


### [150] [Probabilistic Label Spreading: Efficient and Consistent Estimation of Soft Labels with Epistemic Uncertainty on Graphs](https://arxiv.org/abs/2602.04574)
*Jonathan Klees,Tobias Riedlinger,Peter Stehr,Bennet Böddecker,Daniel Kondermann,Matthias Rottmann*

Main category: cs.LG

TL;DR: 提出一种基于图扩散的标签传播方法，在单注释条件下也能估计标签的不确定性（aleatoric与 epistemic），并具有可扩展性，显著降低标注预算，在数据中心图像分类基准上刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决感知任务中缺乏高质量带标签数据的挑战，以及注释本身存在的不确定性（aleatoric/epistemic）在标注和评估中的忽略问题；众包虽然能产生多注释，但规模化成本高。

Method: 基于图的扩散/标签传播，假设标签在特征空间上具有平滑性，将单注释在图上传播以获得标签概率分布；给出一致性证明，即使注释数量趋近于零也能得到一致的概率估计；实现可扩展的分布式/高效实现。

Result: 与基线相比，显著降低达到相同标签质量所需的标注预算；在Data-Centric Image Classification基准上达到新SOTA。

Conclusion: 提供对标签不确定性的可靠估计，适用于注释稀缺场景，具备良好扩展性和实际效果，推动更高质量的数据驱动评估与标注策略。

Abstract: Safe artificial intelligence for perception tasks remains a major challenge, partly due to the lack of data with high-quality labels. Annotations themselves are subject to aleatoric and epistemic uncertainty, which is typically ignored during annotation and evaluation. While crowdsourcing enables collecting multiple annotations per image to estimate these uncertainties, this approach is impractical at scale due to the required annotation effort. We introduce a probabilistic label spreading method that provides reliable estimates of aleatoric and epistemic uncertainty of labels. Assuming label smoothness over the feature space, we propagate single annotations using a graph-based diffusion method. We prove that label spreading yields consistent probability estimators even when the number of annotations per data point converges to zero. We present and analyze a scalable implementation of our method. Experimental results indicate that, compared to baselines, our approach substantially reduces the annotation budget required to achieve a desired label quality on common image datasets and achieves a new state of the art on the Data-Centric Image Classification benchmark.

</details>


### [151] [Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation](https://arxiv.org/abs/2602.04785)
*Congjing Zhang,Ryan Feng Lin,Ruoxuan Bao,Shuai Huang*

Main category: cs.LG

TL;DR: T^2 通过多模LLM协作生成高质量表格数据，辅以三阶段数据质量控制管线，将生成过程视为制造流程，在仿真与真实数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 缓解表格数据稀缺导致的表征不足、类别不平衡、选择偏见和低保真度等问题，利用LLMs提升合成数据质量以支持下游任务。

Method: Team-then-Trim：由领域知识驱动的专门LLMs分工生成表格数据的不同组件，按序生成并组装数据；随后通过三阶段 plug-in QC 管线对合成数据进行多维评估与筛选；把数据生成视作制造过程。

Result: 在模拟和真实数据集上，T^2 在生成高质量表格数据方面超越了最先进方法，提高下游模型的可用性与性能，尤其在无法直接收集数据时。

Conclusion: 通过协作式LLMs与严格的质量控制，提供一种可扩展、实用的高质量表格数据合成框架，具有广泛的应用潜力。

Abstract: While tabular data is fundamental to many real-world machine learning (ML) applications, acquiring high-quality tabular data is usually labor-intensive and expensive. Limited by the scarcity of observations, tabular datasets often exhibit critical deficiencies, such as class imbalance, selection bias, and low fidelity. To address these challenges, building on recent advances in Large Language Models (LLMs), this paper introduces Team-then-Trim (T$^2$), a framework that synthesizes high-quality tabular data through a collaborative team of LLMs, followed by a rigorous three-stage plug-in data quality control (QC) pipeline. In T$^2$, tabular data generation is conceptualized as a manufacturing process: specialized LLMs, guided by domain knowledge, are tasked with generating different data components sequentially, and the resulting products, i.e., the synthetic data, are systematically evaluated across multiple dimensions of QC. Empirical results on both simulated and real-world datasets demonstrate that T$^2$ outperforms state-of-the-art methods in producing high-quality tabular data, highlighting its potential to support downstream models when direct data collection is practically infeasible.

</details>


### [152] [Stochastic Decision Horizons for Constrained Reinforcement Learning](https://arxiv.org/abs/2602.04599)
*Nikola Milosevic,Leonard Franz,Daniel Haeufle,Georg Martius,Nico Scherf,Pavel Kolev*

Main category: cs.LG

TL;DR: 在 CMDP 框架下，提出以控制即为推断（Control as Inference）的生存加权回报，使用随机决策 horizon 与两种违规语义（吸收/虚拟终止），实现离线/回放友好的 off-policy 学习并提升样本效率；VT-MPO 能扩展到高维肌肉骨骼任务 Hyfydy。


<details>
  <summary>Details</summary>
Motivation: 解决 CMDP 中对约束的附加成本与对偶变量在离线/回放场景下的可扩展性问题；需要一个能自然处理约束、且对回放友好（off-policy）的规划目标。

Method: 引入带随机决策 horizon 的控制作为推断框架；违反约束会削弱奖励贡献，及通过状态-动作依赖的继续概率缩短有效规划 horizon，形成生存加权回报；提出两种违规语义（ absorbing termination 和 virtual termination ），虽然回报相同，但优化结构不同，导出类似 SAC/MPO 的策略改进规则。

Result: 在标准基准上展示更高的样本效率与更好的回报-违规折衷；VT-MPO 在高维肌肉骨骼任务 Hyfydy 上展现良好扩展性。

Conclusion: 该框架提供了一种可扩展的离线/回放友好 CMDP 优化路径，且两种违规语义带来不同的优化结构，提升性能并实现对复杂任务的规模化。

Abstract: Constrained Markov decision processes (CMDPs) provide a principled model for handling constraints, such as safety and other auxiliary objectives, in reinforcement learning. The common approach of using additive-cost constraints and dual variables often hinders off-policy scalability. We propose a Control as Inference formulation based on stochastic decision horizons, where constraint violations attenuate reward contributions and shorten the effective planning horizon via state-action-dependent continuation. This yields survival-weighted objectives that remain replay-compatible for off-policy actor-critic learning. We propose two violation semantics, absorbing and virtual termination, that share the same survival-weighted return but result in distinct optimization structures that lead to SAC/MPO-style policy improvement. Experiments demonstrate improved sample efficiency and favorable return-violation trade-offs on standard benchmarks. Moreover, MPO with virtual termination (VT-MPO) scales effectively to our high-dimensional musculoskeletal Hyfydy setup.

</details>


### [153] [Beyond Rewards in Reinforcement Learning for Cyber Defence](https://arxiv.org/abs/2602.04809)
*Elizabeth Bates,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: Sparse rewards, if well-aligned with goals and frequently encountered, improve training reliability and yield safer, more effective cyber defense agents compared to dense rewards.


<details>
  <summary>Details</summary>
Motivation: In cyber defense RL, dense reward shaping can bias policies toward suboptimal, risky strategies; need to understand how reward structure affects learning and policy behavior.

Method: Systematic evaluation across sparse vs dense rewards, two cyber gym environments, various network sizes, and both policy gradient and value-based RL algorithms, aided by a novel ground-truth evaluation approach for direct comparison of reward functions.

Result: Sparse rewards led to better training reliability and more effective agents with lower-risk policies; they aligned better with defender goals and used costly actions sparingly without explicit penalty.

Conclusion: Reward structure critically shapes learning and policy outcomes; sparse, goal-aligned rewards can outperform dense rewards in cyber defense RL, prompting reconsideration of reward design to enhance reliability and safety.

Abstract: Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.

</details>


### [154] [Jacobian Regularization Stabilizes Long-Term Integration of Neural Differential Equations](https://arxiv.org/abs/2602.04608)
*Maya Janvier,Julien Salomon,Etienne Meunier*

Main category: cs.LG

TL;DR: 通过对神经微分方程(NDE)模型的雅可比矩阵进行方向导数正则化，在训练阶段用短回放提升长时间积分的稳定性，成本远低于長回放反向传播。针对已知动力学和未知动力学，分别提供直接推导的方向导数和用有限差分近似的方法。结果在若干常微分方程和偏微分方程上显著提高了长期仿真的稳定性，推动大规模系统的长时间积分训练。


<details>
  <summary>Details</summary>
Motivation: 提高混合模型和NDE在物理系统建模中的长期积分稳定性与数值精度。直接用未展开的轨迹进行训练成本高且在长时间步长上容易产生误差发散，因此需更高效的稳定化正则化策略。

Method: 提出两种方向导数正则化：1) 已知动力学时，直接推导并正则化动态的方向导数（雅可比沿某方向的导数）。2) 动力学未知时，用有限差分近似估计方向导数并进行正则化。两者均在训练时对雅可比的范数或增长进行约束，降低对长轮回展开的需求，从而在较短回合的训练中提升长时间积分的稳定性。

Result: 在若干ODE与PDE的长时间积分任务中，所提正则化显著提升了数值稳定性，且相比长轮回回溯法的训练成本显著降低，展示了将NDE用于大规模系统长时积分训练的可行性。

Conclusion: 通过对NDE的雅可比矩阵进行方向导数正则化，可以在短回合训练下实现对长时间积分的稳定控制，提供一种高效、可扩展的训练策略，适用于复杂物理系统的长时间仿真。

Abstract: Hybrid models and Neural Differential Equations (NDE) are getting increasingly important for the modeling of physical systems, however they often encounter stability and accuracy issues during long-term integration. Training on unrolled trajectories is known to limit these divergences but quickly becomes too expensive due to the need for computing gradients over an iterative process. In this paper, we demonstrate that regularizing the Jacobian of the NDE model via its directional derivatives during training stabilizes long-term integration in the challenging context of short training rollouts. We design two regularizations, one for the case of known dynamics where we can directly derive the directional derivatives of the dynamic and one for the case of unknown dynamics where they are approximated using finite differences. Both methods, while having a far lower cost compared to long rollouts during training, are successful in improving the stability of long-term simulations for several ordinary and partial differential equations, opening up the door to training NDE methods for long-term integration of large scale systems.

</details>


### [155] [Resilient Load Forecasting under Climate Change: Adaptive Conditional Neural Processes for Few-Shot Extreme Load Forecasting](https://arxiv.org/abs/2602.04609)
*Chenxi Hu,Yue Ma,Yifan Wu,Yunhe Hou*

Main category: cs.LG

TL;DR: AdaCNP is a probabilistic forecasting model for data-scarce extreme-event conditions in power systems. It learns a shared embedding space to reweight historical context segments by their relevance to current conditions, enabling few-shot adaptation and uncertainty quantification without heavy fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Extreme weather causes abrupt, high-variance load patterns and extreme samples are rare, making reliable learning and calibration difficult. There is a need for robust probabilistic forecasts and quick adaptation to unseen extreme patterns to prevent supply shortfalls and outages.

Method: AdaCNP builds a shared embedding space of historical context. For each target instance, it evaluates the relevance of historical segments to the current condition and reweights the context accordingly, enabling few-shot adaptation to unseen extreme patterns. It outputs predictive distributions to support risk-aware decisions without expensive domain fine-tuning.

Result: Empirical evaluation on real-world power-system load data shows AdaCNP outperforms representative baselines, reducing MSE by 22% relative to the strongest baseline and achieving the lowest negative log-likelihood, indicating more reliable probabilistic forecasts and robustness during extreme periods.

Conclusion: AdaCNP effectively mitigates abrupt distribution shifts and scarcity of extreme samples, providing trustworthy probabilistic forecasts for resilient power system operation under extreme events.

Abstract: Extreme weather can substantially change electricity consumption behavior, causing load curves to exhibit sharp spikes and pronounced volatility. If forecasts are inaccurate during those periods, power systems are more likely to face supply shortfalls or localized overloads, forcing emergency actions such as load shedding and increasing the risk of service disruptions and public-safety impacts. This problem is inherently difficult because extreme events can trigger abrupt regime shifts in load patterns, while relevant extreme samples are rare and irregular, making reliable learning and calibration challenging. We propose AdaCNP, a probabilistic forecasting model for data-scarce condition. AdaCNP learns similarity in a shared embedding space. For each target data, it evaluates how relevant each historical context segment is to the current condition and reweights the context information accordingly. This design highlights the most informative historical evidence even when extreme samples are rare. It enables few-shot adaptation to previously unseen extreme patterns. AdaCNP also produces predictive distributions for risk-aware decision-making without expensive fine-tuning on the target domain. We evaluate AdaCNP on real-world power-system load data and compare it against a range of representative baselines. The results show that AdaCNP is more robust during extreme periods, reducing the mean squared error by 22\% relative to the strongest baseline while achieving the lowest negative log-likelihood, indicating more reliable probabilistic outputs. These findings suggest that AdaCNP can effectively mitigate the combined impact of abrupt distribution shifts and scarce extreme samples, providing a more trustworthy forecasting for resilient power system operation under extreme events.

</details>


### [156] [Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning](https://arxiv.org/abs/2602.04821)
*Joydeep Chandra,Satyam Kumar Navneet,Aleksandr Algazinov,Yong Zhang*

Main category: cs.LG

TL;DR: STREAM-RL 通过统一框架将预测、异常检测和安全策略学习耦合在一起，实现端到端的已校准不确定性传播与理论保证，并在实际交通数据中达到高覆盖率、FDR控制、显著的安全性提升与低延迟推断。


<details>
  <summary>Details</summary>
Motivation: 城市交通管理需要在预测未来状况、检测异常和采取安全纠正措施之间实现统一、可解释且具有数学保证的系统，以保证运行鲁棒性与安全性。

Method: PU-GAT+：基于不确定性自适应的置信度单调注意力，进行预测不确定性驱动的权重重分配；CRFN-BY：利用正则化流对不确定性归一化残差进行建模，并在任意相关性下应用 Benjamini-Yekutieli FDR 控制；LyCon-WRL+：引入带有 Lyapunov 稳定性证书、可证 Lipschitz 边界的不确定性引导的世界模型强化学习，并进行带不确定性传播的想象滚动。

Result: 在多组真实交通轨迹数据上，STREAM-RL 实现了 91.4% 的覆盖效率，FDR 在验证相关性下控制在 4.1%，安全率提升至 95.2%（相较于标准 PPO 的 69%），并且获得更高的累计奖励，端到端推断时延为 23 ms。

Conclusion: 首次提出能够将预测、异常检测与安全策略学习的不确定性进行端到端传播并提供理论保证的综合框架，兼具实时性与统计性质的可靠性。

Abstract: Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals via normalizing flows with Benjamini-Yekutieli FDR control under arbitrary dependence; and (3) LyCon-WRL+, an Uncertainty-Guided Safe World-Model RL agent with Lyapunov stability certificates, certified Lipschitz bounds, and uncertainty-propagated imagination rollouts. To our knowledge, this is the first framework to propagate calibrated uncertainty from forecasting through anomaly detection to safe policy learning with end-to-end theoretical guarantees. Experiments on multiple real-world traffic trajectory data demonstrate that STREAM-RL achieves 91.4\% coverage efficiency, controls FDR at 4.1\% under verified dependence, and improves safety rate to 95.2\% compared to 69\% for standard PPO while achieving higher reward, with 23ms end-to-end inference latency.

</details>


### [157] [QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning](https://arxiv.org/abs/2602.04620)
*Doyeon Lee,Eunyi Lyou,Hyunsoo Cho,Sookyung Kim,Joonseok Lee,Jaemoo Choi*

Main category: cs.LG

TL;DR: 提出 QUATRO，通过严格的信任域约束直接优化以改进 GRPO 风格的 RL 基于 LLM 的微调，解决启发式裁剪在极端重要性比下的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于信任域的 RL 微调依赖全局裁剪和分组归一化等近似，无法对超出裁剪范围的样本进行有效约束，导致优化不稳定和脆弱性。

Method: 直接以严格的信任域优化实现约束，给出清晰可解释的目标函数，显式控制策略更新，并引入来自严格信任域形式的稳定项，优化过程具备稳定性和熵控制。

Result: 在多种数学推理基准上验证，训练在更高的策略陈旧性和更激进的学习率下仍然稳定，训练过程保持较好地熵控制。

Conclusion: QUATRO 为 LLM 微调中的策略更新提供可解释且鲁棒的信任域优化框架，有助于提升稳定性和鲁棒性。

Abstract: GRPO-style reinforcement learning (RL)-based LLM fine-tuning algorithms have recently gained popularity. Relying on heuristic trust-region approximations, however, they can lead to brittle optimization behavior, as global importance-ratio clipping and group-wise normalization fail to regulate samples whose importance ratios fall outside the clipping range. We propose Query-Adaptive Trust-Region policy Optimization (QUATRO), which directly enforces trust-region constraints through a principled optimization. This yields a clear and interpretable objective that enables explicit control over policy updates and stable, entropy-controlled optimization, with a stabilizer terms arising intrinsically from the exact trust-region formulation. Empirically verified on diverse mathematical reasoning benchmarks, QUATRO shows stable training under increased policy staleness and aggressive learning rates, maintaining well-controlled entropy throughout training.

</details>


### [158] [From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures](https://arxiv.org/abs/2602.04861)
*Ryan Liu,Eric Qu,Tobias Kreiman,Samuel M. Blau,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: BSCT is an efficient benchmark that detects non-smoothness in ML interatomic potentials by controlled bond deformations; it correlates with MD stability and guides model design, demonstrated on a Transformer backbone with improvements (differentiable kNN, temperature-controlled attention) to reduce artifacts; outcome is improved stability and predictive robustness without sacrificing conventional energy/force accuracy.


<details>
  <summary>Details</summary>
Motivation: Current MLIP evaluations focus on energy/force regression and near-equilibrium behavior, often missing non-smooth PES features that cause unstable simulations. MD is accurate but expensive. A fast, physics-informed test is needed to detect and prevent non-smooth artifacts during model development.

Method: Introduce Bond Smoothness Characterization Test (BSCT): apply controlled bond deformations to probe the PES, screening for discontinuities, artificial minima, and spurious forces both near and far from equilibrium. Benchmark results against MD stability and compute cost. Use an unconstrained Transformer backbone as a testbed; implement refinements such as a differentiable k-nearest neighbors algorithm and temperature-controlled attention to mitigate artifacts. Demonstrate that BSCT-guided design yields Potentials with low energy/force error, stable MD, and robust property predictions.

Result: BSCT correlates strongly with MD stability and requires a fraction of MD's computational cost. Applying BSCT-guided refinements to the Transformer backbone reduces artifacts (e.g., non-smooth regions, fictitious minima), yielding MLIPs that are both accurate in E/F regression and stable in MD and predictive in atomistic properties.

Conclusion: BSCT serves as an effective validation metric and in-the-loop design proxy for MLIPs, enabling detection of non-smooth PES features that conventional benchmarks miss and guiding systematic model improvements to achieve stable, accurate, and robust atomistic predictions.

Abstract: Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an "in-the-loop" model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.

</details>


### [159] [MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction](https://arxiv.org/abs/2602.04643)
*Yanan He,Yunshi Wen,Xin Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: 提出 MTS-JEPA：在多变量时间序列异常预测中，结合多分辨率预测目标与软代码本瓶颈，解决 JEPA 的表示崩溃和跨时尺度的前驱信号捕捉不足问题，通过解耦短期冲击与长期趋势并利用代码本捕捉离散的状态转变，提供正则化效果与稳定性，实验显示在早期预警场景达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的联合嵌入预测架构（JEPA）在多变量时间序列的异常预测中易发生表示崩溃，并且难以捕捉跨越不同时间尺度的前驱信号。需要一个能够解耦短期瞬变与长期趋势、并能捕捉离散性状态转变的模型，同时具有更强的优化稳定性。

Method: 提出 MTS-JEPA 架构：引入多分辨率的预测目标以捕捉不同时间尺度的信息，并加入软代码本瓶颈以对潜在表示进行离散化约束。该设计将瞬时冲击与长期趋势显式解耦，并利用代码本来捕捉离散的状态转变；代码本还充当内在正则化，提升优化稳定性。

Result: 在标准基准数据集上的实验表明，该方法有效避免了退化解，且在早期预警协议下达到或接近SOTA的性能。

Conclusion: MTS-JEPA 通过解决表示崩溃与跨时尺度信息捕捉问题，提升了对多变量时间序列的异常预测能力；软代码本瓶颈不仅实现了对状态转变的离散化刻画，也为优化过程提供正则化与稳定性，实验结果验证了其在早期预警场景的优越性。

Abstract: Multivariate time series underpin modern critical infrastructure, making the prediction of anomalies a vital necessity for proactive risk mitigation. While Joint-Embedding Predictive Architectures (JEPA) offer a promising framework for modeling the latent evolution of these systems, their application is hindered by representation collapse and an inability to capture precursor signals across varying temporal scales. To address these limitations, we propose MTS-JEPA, a specialized architecture that integrates a multi-resolution predictive objective with a soft codebook bottleneck. This design explicitly decouples transient shocks from long-term trends, and utilizes the codebook to capture discrete regime transitions. Notably, we find this constraint also acts as an intrinsic regularizer to ensure optimization stability. Empirical evaluations on standard benchmarks confirm that our approach effectively prevents degenerate solutions and achieves state-of-the-art performance under the early-warning protocol.

</details>


### [160] [SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF](https://arxiv.org/abs/2602.04651)
*Dipan Maity*

Main category: cs.LG

TL;DR: 提出 SAFE，一种基于 actor-critic 的纯 on-policy RLHF 方法，结合双重软最小值评估器、熵门控 KL 调控和 PID 自适应阈值，实现比 PPO 更稳定且更高的 LM-RLHF 回报。


<details>
  <summary>Details</summary>
Motivation: PPO 在 LM-RLHF 中表现虽好，但缺乏理论支撑且在奖励波动、熵崩塌、值函数漂移等方面易出问题，需频繁重启和花费调参；需要一个稳定且易于生产部署的纯 on-policy 框架。

Method: SAFE：双软最小 Critic 用于悲观的值估计；多层稳定化框架：熵门控 KL 调整、基于 PID 的自适应阈值。与对称 KL 惩罚的 PPO 不同，按奖励速度对惩罚进行动态调整。

Result: 在一个 3B 参数模型上，SAFE 比 PPO 提高训练平均回报约 5.15%，达到 0.725 相对 0.689；奖励崩溃极少，KL 控制优于 PPO；计算开销很小。

Conclusion: 提供一个解释性强、抗崩溃的 RLHF 框架，兼具快速学习与长时稳定优化，适合生产部署；代码开源。

Abstract: Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE

</details>


### [161] [CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation](https://arxiv.org/abs/2602.04868)
*Yannick Denker,Alexander Gepperth*

Main category: cs.LG

TL;DR: CRoSS: a scalable, realistic continual RL benchmark for robotics using Gazebo, featuring two platforms (differential-drive with sensors and a 7-DOF arm), multiple task variations, high-level and low-level control modes, and containerized reproducible setup; demonstrates baseline RL results (DQN and policy gradient) and provides fast kinematics-only variants for speedups.


<details>
  <summary>Details</summary>
Motivation: Fill the gap in continual RL benchmarks for robotics with high physical realism and controlled task variation, enabling reproducible experiments and systematic analysis of forgetting and transfer across tasks.

Method: Construct a benchmark suite CRoSS in Gazebo with two platforms: (1) differential-drive robot for line-following and object-pushing with varied visual/structural parameters; (2) 7-DOF robotic arm with high-level Cartesian hand-position control and low-level joint-angle control. Provide kinematics-only variants for the arm to run much faster. Offer a containerized Apptainer environment ensuring out-of-the-box reproducibility. Report baseline performances of standard RL algorithms (DQN and policy gradient).

Result: CRoSS provides a realistic, extensible CRL benchmark for robotics, enabling controlled multi-task evaluation with diverse sensors and task modalities; baseline results demonstrate feasibility of applying DQN and policy gradient methods in realistic robotic tasks and the practicality of fast, non-simulated variants for rapid experimentation.

Conclusion: CRoSS constitutes a scalable, reproducible benchmark platform for CRL research in robotics, allowing flexible task design, sensor configurations, and efficient evaluation through both full physics simulation and fast kinematics-only variants.

Abstract: Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.

</details>


### [162] [Contrastive Continual Learning for Model Adaptability in Internet of Things](https://arxiv.org/abs/2602.04881)
*Ajesh Koyatan Chathoth*

Main category: cs.LG

TL;DR: 对物联网(IoT)中的对比学习连续学习(CCL)进行综述，提出统一问题表述，融合对比学习与蒸馏的目标，给出面向IoT的端-边-云参考架构与评估规范，并指出表格/流数据、概念漂移、联邦学习及能耗优化等独特挑战。


<details>
  <summary>Details</summary>
Motivation: IoT环境非平稳、传感器漂移、用户行为演变与隐私需求异质性等因素影响应用效用；通过连续学习保持性能并避免灾难性忘记；对比学习提升鲁棒性与样本效率，需将两者结合并兼顾TinyML、断连性与隐私等实际约束。

Method: 以综述/框架性研究为主，给出统一的问题公式，导出融合对比损失与蒸馏损失的共同目标，提出面向IoT的端-边-云参考架构，并给出评估协议与指标，最后讨论IoT特有的挑战与研究方向。

Result: 给出统一的问题表述、共性目标、IoT导向的参考架构、评估协议与指标，并识别并讨论涉及表格/流数据、概念漂移、联邦学习与能耗优化等IoT特有挑战。

Conclusion: 为IoT场景下的CCL应用提供系统化框架与实施指南，连接算法设计与实际IoT系统约束，指明未来在跨设备协作、隐私保护与能效优化等方面的研究方向。

Abstract: Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.

</details>


### [163] [Generalized Schrödinger Bridge on Graphs](https://arxiv.org/abs/2602.04675)
*Panagiotis Theodoropoulos,Juno Nam,Evangelos Theodorou,Jaemoo Choi*

Main category: cs.LG

TL;DR: 提出了 GSBoG，一种在图上学习可执行的连续时间马尔可夫链策略的通用 Schrödinger 桥方法，具备端点边缘匹配和状态成本驱动的中间行为优化，具高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图上运输方法往往表达能力不足、对稀疏拓扑泛化性差且随着图规模和时间步数线性或更快地变慢，难以得到可实际执行的策略。需要一种可在复杂图结构上学习可执行、成本感知的策略的框架。

Method: 在图上建立 Generalized Schrödinger Bridge 框架，采用 likelihood 优化以同时满足端点边缘约束，并在状态相关的运行成本下优化中间行为，生成轨迹层级策略；通过避免密集全局求解器提升可扩展性，且实现连续时间马尔可夫链的可控性。

Result: 在真实世界图拓扑上的大规模实验表明，GSBoG 能学习准确且符合拓扑约束的策略，同时优化中间状态成本，具备良好的应用广度。

Conclusion: GSBoG 为在一般图上的成本感知动力传输提供了可扩展、数据驱动的解法，开启了在图上进行可执行的动态传输的新途径。

Abstract: Transportation on graphs is a fundamental challenge across many domains, where decisions must respect topological and operational constraints. Despite the need for actionable policies, existing graph-transport methods lack this expressivity. They rely on restrictive assumptions, fail to generalize across sparse topologies, and scale poorly with graph size and time horizon. To address these issues, we introduce Generalized Schrödinger Bridge on Graphs (GSBoG), a novel scalable data-driven framework for learning executable controlled continuous-time Markov chain (CTMC) policies on arbitrary graphs under state cost augmented dynamics. Notably, GSBoG learns trajectory-level policies, avoiding dense global solvers and thereby enhancing scalability. This is achieved via a likelihood optimization approach, satisfying the endpoint marginals, while simultaneously optimizing intermediate behavior under state-dependent running costs. Extensive experimentation on challenging real-world graph topologies shows that GSBoG reliably learns accurate, topology-respecting policies while optimizing application-specific intermediate state costs, highlighting its broad applicability and paving new avenues for cost-aware dynamical transport on general graphs.

</details>


### [164] [Protein Autoregressive Modeling via Multiscale Structure Generation](https://arxiv.org/abs/2602.04883)
*Yanru Qu,Cheng-Yen Hsieh,Zaixiang Zheng,Ge Liu,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出 PAR，一种针对蛋白质骨架生成的多尺度自回归框架，利用自下而上的多尺度预测来实现粗到细的骨架生成。通过多尺度下采样、自回归变换器与基于流的骨架解码器实现条件生成，并通过噪声上下文学习与计划性采样缓解暴露偏差，具备零样本泛化能力和无需微调的任务导向条件生成能力，且在无条件生成基准下表现出高设计质量与良好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 蛋白质结构具有分层特征，现有自回归模型易受到暴露偏差，且难以在不微调的情况下实现灵活的条件生成与模体搭棚。需要一个可扩展、鲁棒且可零样本泛化的骨架生成框架。

Method: 三个核心组成： (i) 多尺度下采样以在训练中表示不同尺度的蛋白结构； (ii) 自回归变换器编码多尺度信息并产生条件嵌入以引导结构生成； (iii) 基于流的骨架解码器在这些嵌入条件下生成骨架原子。为缓解自回归的暴露偏差，采用带噪声的上下文学习和计划性采样。模型具备零样本泛化能力，支持通过人类提示进行条件生成与模体搭棚，无需微调。

Result: 在无条件生成基准上，PAR 学会了蛋白质分布，生成高质量的骨架，具有良好的可扩展性；同时展示强的零样本泛化与灵活的条件生成能力。

Conclusion: PAR 是蛋白质结构生成的有前景框架，结合多尺度建模、鲁棒的自回归训练与流式解码，能够实现高质量、可扩展且可零样本条件生成的后验骨架。

Abstract: We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.

</details>


### [165] [REDistill: Robust Estimator Distillation for Balancing Robustness and Efficiency](https://arxiv.org/abs/2602.04677)
*Ondrej Tybl,Lukas Neumann*

Main category: cs.LG

TL;DR: 提出 REDistill，通过使用幂散度替代 KL 的 KD 目标，在教师输出有噪声时自适应下调不可靠信息，提升学生模型精度，且无额外超参调优。


<details>
  <summary>Details</summary>
Motivation: 在 KD 中，教师输出不可靠时，传统基于 KL 的目标易被噪声影响，现有修正方法依赖启发式和大量超参调优，导致泛化差。需要一个简单、鲁棒、可解释的框架来处理教师噪声。

Method: 将 KD 目标替换为幂散度损失，幂散度是 KL 的推广，能够自适应下调不可靠的教师输出，同时保持有用的 logits 关系。仅需 logits，易于集成到现有 KD 流程中，计算开销极小。

Result: 在 CIFAR-100 和 ImageNet-1k 上的广泛实验表明，REDistill 在多种教师-学生架构中稳定提升学生准确率，且无需专门的模型特定超参调优，具鲁棒性和良好泛化。

Conclusion: REDistill 提供了对教师噪声的统一、可解释处理，简单且鲁棒，便于与现有 KD 无缝集成，体现出优越的性能和泛化能力。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student by aligning their predictive distributions. However, conventional KD formulations - typically based on Kullback-Leibler divergence - assume that the teacher provides reliable soft targets. In practice, teacher predictions are often noisy or overconfident, and existing correction-based approaches rely on ad-hoc heuristics and extensive hyper-parameter tuning, which hinders generalization. We introduce REDistill (Robust Estimator Distillation), a simple yet principled framework grounded in robust statistics. REDistill replaces the standard KD objective with a power divergence loss, a generalization of KL divergence that adaptively downweights unreliable teacher output while preserving informative logit relationships. This formulation provides a unified and interpretable treatment of teacher noise, requires only logits, integrates seamlessly into existing KD pipelines, and incurs negligible computational overhead. Extensive experiments on CIFAR-100 and ImageNet-1k demonstrate that REDistill consistently improves student accuracy in diverse teacher-student architectures. Remarkably, it achieves these gains without model-specific hyper-parameter tuning, underscoring its robustness and strong generalization to unseen teacher-student pairs.

</details>


### [166] [Static and auto-regressive neural emulation of phytoplankton biomass dynamics from physical predictors in the global ocean](https://arxiv.org/abs/2602.04689)
*Mahima Lakra,Ronan Fablet,Lucas Drumetz,Etienne Pauthenet,Elodie Martinez*

Main category: cs.LG

TL;DR: 使用深度学习尤其是 UNet 架构来预测全球海洋浮游植物生物量，结合卫星观测与环境条件，能够重构并短期预测浮游植物动态，优于其他模型；自回归 UNet 在短期（≤5 个月）预测中表现良好，但对长时尺度和低频振荡的幅值存在低估。


<details>
  <summary>Details</summary>
Motivation: 浮游植物是海洋生物地球化学循环及食物网的基础，但参数化不足、观测数据稀缺、海洋过程复杂，使传统生物地球化学模型难以准确模拟其时空动态，因此需要更强的数据驱动方法来提升预测能力。

Method: 对比多种深度学习架构（UNet、CNN、ConvLSTM、4CastNet），基于一到两个月的环境数据输入，预测全球海洋浮游植物生物量的时空分布；同时提出自回归的 UNet，将模型的前一轮预测作为输入以预测未来，评估其短期与中期预测能力。

Result: UNet 在重现季节性和年际模式方面优于其他模型，且在单两月输入条件下表现最好；自回归 UNet 在短期预测（最多五个月）效果良好，但对较长时间尺度的预测性能下降；结合海洋物理预测变量，深度学习可实现浮游植物动态的重建与短期预测，具有监测海洋健康和应对气候变化潜力。

Conclusion: 将物理驱动的海洋预测变量与深度学习相结合，能有效重建并预测全球海洋浮游植物生物量的时空分布，成为监测海洋健康与生态管理的有力工具，尽管需关注对低频分量的幅值偏差及长期稳定性。

Abstract: Phytoplankton is the basis of marine food webs, driving both ecological processes and global biogeochemical cycles. Despite their ecological and climatic significance, accurately simulating phytoplankton dynamics remains a major challenge for biogeochemical numerical models due to limited parameterizations, sparse observational data, and the complexity of oceanic processes. Here, we explore how deep learning models can be used to address these limitations predicting the spatio-temporal distribution of phytoplankton biomass in the global ocean based on satellite observations and environmental conditions. First, we investigate several deep learning architectures. Among the tested models, the UNet architecture stands out for its ability to reproduce the seasonal and interannual patterns of phytoplankton biomass more accurately than other models like CNNs, ConvLSTM, and 4CastNet. When using one to two months of environmental data as input, UNet performs better, although it tends to underestimate the amplitude of low-frequency changes in phytoplankton biomass. Thus, to improve predictions over time, an auto-regressive version of UNet was also tested, where the model uses its own previous predictions to forecast future conditions. This approach works well for short-term forecasts (up to five months), though its performance decreases for longer time scales. Overall, our study shows that combining ocean physical predictors with deep learning allows for reconstruction and short-term prediction of phytoplankton dynamics. These models could become powerful tools for monitoring ocean health and supporting marine ecosystem management, especially in the context of climate change.

</details>


### [167] [Towards Understanding and Avoiding Limitations of Convolutions on Graphs](https://arxiv.org/abs/2602.04709)
*Andreas Roth*

Main category: cs.LG

TL;DR: 本论文对 MPNN 进行理论分析，揭示两类普遍现象：共享组件放大（SCA）与组件支配（CD），并将其与秩崩溃（扩展的过平滑）联系起来。为解决这些问题，提出多关系框架与谱图卷积的多通道扩展，以及基于个性化 PageRank 的无限迭代 MPNN 变体，形成 MRS、MIMO-GC/LMGC 与 PageRank 相关框架，系统深化对MPNN的理论理解。


<details>
  <summary>Details</summary>
Motivation: 现有 MPNN 的理论基础薄弱，缺乏对其限制的统一诊断与可操作的解决框架，因此需要从理论层面揭示根本原因并给出可实现的改进路线。

Method: 通过理论分析识别 SCA、CD 等现象，建立多关系图框架（MRS）将多边关系嵌入 MPNN；提出针对多通道特征的谱图卷积（MIMO-GC）及其局部近似 LMGC 以实现高效多图计算；将 PageRank 的思想用于 MPNN，提出可无穷迭代的变体以保留初始节点特征。

Result: 确立了 SCA 和 CD 的存在性及其导致的表示秩崩溃，与过平滑的关系得到更系统化的分解；提供了 MRS、MIMO-GC/LMGC 以及基于个性化 PageRank 的无限迭代 MPNN 的框架，扩展了 MPNN 的理论工具箱。

Conclusion: 这些工作提升了对 MPNN 的理论理解，提供更精确的改进方向和跨框架的沟通渠道，有望推动 MPNN 在实际任务中的鲁棒性与泛化能力提升。

Abstract: While message-passing neural networks (MPNNs) have shown promising results, their real-world impact remains limited. Although various limitations have been identified, their theoretical foundations remain poorly understood, leading to fragmented research efforts. In this thesis, we provide an in-depth theoretical analysis and identify several key properties limiting the performance of MPNNs. Building on these findings, we propose several frameworks that address these shortcomings. We identify two properties exhibited by many MPNNs: shared component amplification (SCA), where each message-passing iteration amplifies the same components across all feature channels, and component dominance (CD), where a single component gets increasingly amplified as more message-passing steps are applied. These properties lead to the observable phenomenon of rank collapse of node representations, which generalizes the established over-smoothing phenomenon. By generalizing and decomposing over-smoothing, we enable a deeper understanding of MPNNs, more targeted solutions, and more precise communication within the field. To avoid SCA, we show that utilizing multiple computational graphs or edge relations is necessary. Our multi-relational split (MRS) framework transforms any existing MPNN into one that leverages multiple edge relations. Additionally, we introduce the spectral graph convolution for multiple feature channels (MIMO-GC), which naturally uses multiple computational graphs. A localized variant, LMGC, approximates the MIMO-GC while inheriting its beneficial properties. To address CD, we demonstrate a close connection between MPNNs and the PageRank algorithm. Based on personalized PageRank, we propose a variant of MPNNs that allows for infinitely many message-passing iterations, while preserving initial node features. Collectively, these results deepen the theoretical understanding of MPNNs.

</details>


### [168] [Bounded-Abstention Multi-horizon Time-series Forecasting](https://arxiv.org/abs/2602.04714)
*Luca Stradiotti,Laurens Devos,Anna Monreale,Jesse Davis,Andrea Pugnana*

Main category: cs.LG

TL;DR: 提出面向多步时间序列预测的放弃预测学习，给出三种 abstention 形式，理论推导最优放弃策略并给出算法，在24个数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 多步时间序列预测的错预测成本较高，放弃学习可在高风险情况下拒绝预测以降低损失；现有放弃策略多针对单步预测，忽略多步预测的结构性相关性，需将 abstention 设计融入序列预测的结构。

Method: 形式化多步预测中的放弃学习问题，提出三种天然的放弃形式以应对多步预测的结构化输出；对每种放弃形式进行理论分析，推导最优放弃策略并给出可实现的算法；在24个数据集上进行广泛评估。

Result: 提出的算法在多步放弃设定下显著优于现有基线，展示了结构化放弃在多步时间序列中的优势。

Conclusion: 将放弃学习扩展到多步时间序列预测，揭示了更丰富的放弃问题，并通过理论与实验验证提高了预测可信度与性能。

Abstract: Multi-horizon time-series forecasting involves simultaneously making predictions for a consecutive sequence of subsequent time steps. This task arises in many application domains, such as healthcare and finance, where mispredictions can have a high cost and reduce trust. The learning with abstention framework tackles these problems by allowing a model to abstain from offering a prediction when it is at an elevated risk of making a misprediction. Unfortunately, existing abstention strategies are ill-suited for the multi-horizon setting: they target problems where a model offers a single prediction for each instance. Hence, they ignore the structured and correlated nature of the predictions offered by a multi-horizon forecaster. We formalize the problem of learning with abstention for multi-horizon forecasting setting and show that its structured nature admits a richer set of abstention problems. Concretely, we propose three natural notions of how a model could abstain for multi-horizon forecasting. We theoretically analyze each problem to derive the optimal abstention strategy and propose an algorithm that implements it. Extensive evaluation on 24 datasets shows that our proposed algorithms significantly outperforms existing baselines.

</details>


### [169] [Benchmarking and Enhancing PPG-Based Cuffless Blood Pressure Estimation Methods](https://arxiv.org/abs/2602.04725)
*Neville Mathew,Yidan Shen,Renjie Hu,Maham Rahimi,George Zouridakis*

Main category: cs.LG

TL;DR: 建立大规模、受控的PPG血压基线基准数据集并对多种模型进行对比评估；结果显示现有PPG血压估计模型未达到AAMI/ISO 81060-2的数值标准；通过加入人口统计信息（年龄、性别、BMI）可显著提升模型准确性，MInception在加入人口统计后SBP/DBP的MAE分别为4.75/2.90 mmHg，接近标准。


<details>
  <summary>Details</summary>
Motivation: 解决现有公开数据集的异质性与缺乏受生理控制条件的公平基准问题，推动PPG基础的无痛血压筛查走向临床可用性。

Method: 构建标准化基准子集NBPDB：从MIMIC-III与VitalDB筛选101,453个高质量PPG片段，涉及1,103名健康成人。对若干前沿PPG血压估计模型进行系统基准评估，并在模型中加入年龄、性别、BMI等人口统计输入，评估对准确性的影响。

Result: 所有评估模型均未达到AAMI/ISO 81060-2的准确性要求（平均误差<5 mmHg且标准差<8 mmHg）。通过加入人口统计数据，所有模型均获得性能提升。特定地，MInception在加入人口统计后误差下降23%，SBP的MAE为4.75 mmHg，DBP的MAE为2.90 mmHg，达到接近AAMI/ISO标准的水平。

Conclusion: 在受控条件下，现有PPG血压估计模型仍缺乏临床实用性；将人口统计信息作为输入可显著提升模型的准确性与生理学有效性。

Abstract: Cuffless blood pressure screening based on easily acquired photoplethysmography (PPG) signals offers a practical pathway toward scalable cardiovascular health assessment. Despite rapid progress, existing PPG-based blood pressure estimation models have not consistently achieved the established clinical numerical limits such as AAMI/ISO 81060-2, and prior evaluations often lack the rigorous experimental controls necessary for valid clinical assessment. Moreover, the publicly available datasets commonly used are heterogeneous and lack physiologically controlled conditions for fair benchmarking. To enable fair benchmarking under physiologically controlled conditions, we created a standardized benchmarking subset NBPDB comprising 101,453 high-quality PPG segments from 1,103 healthy adults, derived from MIMIC-III and VitalDB. Using this dataset, we systematically benchmarked several state-of-the-art PPG-based models. The results showed that none of the evaluated models met the AAMI/ISO 81060-2 accuracy requirements (mean error $<$ 5 mmHg and standard deviation $<$ 8 mmHg). To improve model accuracy, we modified these models and added patient demographic data such as age, sex, and body mass index as additional inputs. Our modifications consistently improved performance across all models. In particular, the MInception model reduced error by 23\% after adding the demographic data and yielded mean absolute errors of 4.75 mmHg (SBP) and 2.90 mmHg (DBP), achieves accuracy comparable to the numerical limits defined by AAMI/ISO accuracy standards. Our results show that existing PPG-based BP estimation models lack clinical practicality under standardized conditions, while incorporating demographic information markedly improves their accuracy and physiological validity.

</details>


### [170] [DMFlow: Disordered Materials Generation by Flow Matching](https://arxiv.org/abs/2602.04734)
*Liming Wu,Rui Jiao,Qi Li,Mingze Li,Songyou Li,Shifeng Jin,Wenbing Huang*

Main category: cs.LG

TL;DR: DMFlow 是一个面向无序晶体的统一流式生成框架，结合 Riemannian 流匹配、球面重参数化和物理对称性约束的 GNN，实现从连续权重到多热原子分配的两阶段离散化，在 COD 数据集上的 SD/PD/混合晶体任务上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型多聚焦有序晶体，忽略无序材料的多态性与缺陷结构。需统一表示、物理有效的无序权重以及可扩展的生成能力，以促进无序材料的 AI 驱动发现。

Method: 提出统一表示来覆盖有序、替位乱序 SD、位置乱序 PD 晶体；使用流匹配框架，结合 Riemannian 流和球面重参数化以确保权重落在概率单纯形上；通过引入具物理对称性与专门消息传递的 GNN 学习向量场；两阶段离散化将连续权重转化为多热原子分配；基于 Crystallography Open Database 构建 SD/PD 与混合结构的数据集并进行 CSP（晶体结构预测）与 DNG（从头生成）任务评估；声称在现有有序晶体生成的基线改造上显著超越。

Result: 在 CSP 与 DNG 任务上，DMFlow 显著优于从有序晶体生成的现有基线，展示对无序晶体的生成能力提升与可扩展性。

Conclusion: DMFlow 为无序材料的 AI 驱动发现奠定基础，提供统一表示、物理有效的无序权重处理及公开数据集，具备对SD/PD/混合结构的泛化潜力。

Abstract: The design of materials with tailored properties is crucial for technological progress. However, most deep generative models focus exclusively on perfectly ordered crystals, neglecting the important class of disordered materials. To address this gap, we introduce DMFlow, a generative framework specifically designed for disordered crystals. Our approach introduces a unified representation for ordered, Substitutionally Disordered (SD), and Positionally Disordered (PD) crystals, and employs a flow matching model to jointly generate all structural components. A key innovation is a Riemannian flow matching framework with spherical reparameterization, which ensures physically valid disorder weights on the probability simplex. The vector field is learned by a novel Graph Neural Network (GNN) that incorporates physical symmetries and a specialized message-passing scheme. Finally, a two-stage discretization procedure converts the continuous weights into multi-hot atomic assignments. To support research in this area, we release a benchmark containing SD, PD, and mixed structures curated from the Crystallography Open Database. Experiments on Crystal Structure Prediction (CSP) and De Novo Generation (DNG) tasks demonstrate that DMFlow significantly outperforms state-of-the-art baselines adapted from ordered crystal generation. We hope our work provides a foundation for the AI-driven discovery of disordered materials.

</details>


### [171] [Decomposing Query-Key Feature Interactions Using Contrastive Covariances](https://arxiv.org/abs/2602.04752)
*Andrew Lee,Yonatan Belinkov,Fernanda Viégas,Martin Wattenberg*

Main category: cs.LG

TL;DR: 提出一种对比协方差方法，用于将查询-键空间分解为低秩、可解释的子空间，揭示在这些子空间内对齐时产生高注意力分数的特征。并在大语言模型中识别与分类语义特征和绑定特征相关的可解释子空间，从而将注意力分归因于这些特征。


<details>
  <summary>Details</summary>
Motivation: 理解 Transformer 注意力头为何对特定标记产生关注；揭示 QK 空间中的可解释结构，提升注意力机制的诊断性与可解释性。

Method: 提出对比协方差（contrastive covariance）方法，将 QK 的双线性嵌入分解为低秩子空间；在简化设定中给出解析与实验验证，随后应用于大语言模型，识别出与分类语义特征及绑定特征相关的可解释 QK 子空间，并展示如何把注意力分数归因到这些子空间中的特征。

Result: 在简化设置中获得理论分析与实验结果；在大型语言模型中发现可解释的 QK 子空间，且高注意力分数与这些低秩子空间内的特征对齐相关；能够将注意力分归因于识别出的特征。

Conclusion: QK 空间的低秩子空间提供了可解释的结构，对比协方差方法可揭示这些子空间并实现对注意力的特征级归因，提升 Transformer 的可解释性与诊断能力。

Abstract: Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our method both analytically and empirically in a simplified setting. We then apply our method to large language models to identify human-interpretable QK subspaces for categorical semantic features and binding features. Finally, we demonstrate how attention scores can be attributed to our identified features.

</details>


### [172] [A Dual-TransUNet Deep Learning Framework for Multi-Source Precipitation Merging and Improving Seasonal and Extreme Estimates](https://arxiv.org/abs/2602.04757)
*Yuchen Ye,Zixuan Qi,Shixuan Li,Wei Qi,Yanpeng Cai,Chaoxia Yuan*

Main category: cs.LG

TL;DR: 提出一个双阶段的 TransUNet 基于多源降水融合框架 DDL-MSPMF，将六种 MSP 与 ERA5 四个物理预测输入，第一阶段分类估计降水发生概率，第二阶段回归融合输出与预测，实现中国区域日降水估计（2001–2020）并提升极端事件检测能力。


<details>
  <summary>Details</summary>
Motivation: 解决多源降水产品在空间上存在偏差与对极端事件技能有限的问题，同时需要一个可解释、可扩展的融合框架以提升降水重现与极端事件评估。

Method: 建立双阶段结构：第一阶段为降水发生概率的分类器，第二阶段为回归融合器，输入包括六个 MSPs、四个 ERA5 近地面物理变量以及第一阶段的输出。核心采用 TransUNet 作为编码-解码与跨模态融合机制，输出分辨率为 0.25°，覆盖中国，时间范围为 2001–2020。

Result: 在季节性层面，TransUNet-TransUNet 配置表现最佳，相关指标为 R = 0.75、RMSE = 2.70 mm/day，且对比单一回归设定具有更好鲁棒性。在强降水（>25 mm/day）情景下，该方法提升中国东部地区的公平威胁分数，并更准确再现郑州暴雨等极端事件的空间格局。独立评估（青藏高原 TPHiPr）显示在数据稀缺区域的适用性。SHAP 分析揭示降水发生概率与地表气压的重要性，提供可解释诊断。

Conclusion: 该框架具备可扩展性与可解释性，适用于多源降水融合与极端事件评估，提升 MSP 的综合性能和极端事件检测能力，具备在不同区域的迁移与实际应用潜力。

Abstract: Multi-source precipitation products (MSPs) from satellite retrievals and reanalysis are widely used for hydroclimatic monitoring, yet spatially heterogeneous biases and limited skill for extremes still constrain their hydrologic utility. Here we develop a dual-stage TransUNet-based multi-source precipitation merging framework (DDL-MSPMF) that integrates six MSPs with four ERA5 near-surface physical predictors. A first-stage classifier estimates daily precipitation occurrence probability, and a second-stage regressor fuses the classifier outputs together with all predictors to estimate daily precipitation amount at 0.25 degree resolution over China for 2001-2020. Benchmarking against multiple deep learning and hybrid baselines shows that the TransUNet - TransUNet configuration yields the best seasonal performance (R = 0.75; RMSE = 2.70 mm/day) and improves robustness relative to a single-regressor setting. For heavy precipitation (>25 mm/day), DDL-MSPMF increases equitable threat scores across most regions of eastern China and better reproduces the spatial pattern of the July 2021 Zhengzhou rainstorm, indicating enhanced extreme-event detection beyond seasonal-mean corrections. Independent evaluation over the Qinghai-Tibet Plateau using TPHiPr further supports its applicability in data-scarce regions. SHAP analysis highlights the importance of precipitation occurrence probabilities and surface pressure, providing physically interpretable diagnostics. The proposed framework offers a scalable and explainable approach for precipitation fusion and extreme-event assessment.

</details>


### [173] [Improved Dimension Dependence for Bandit Convex Optimization with Gradient Variations](https://arxiv.org/abs/2602.04761)
*Hang Yu,Yu-Hu Yan,Peng Zhao*

Main category: cs.LG

TL;DR: In two-point bandit convex optimization, this work refines non-consecutive gradient-variation analysis to improve dimension dependence for convex and strongly convex objectives, and extends the framework to one-point settings, dynamic/universal regret, and bandit games.


<details>
  <summary>Details</summary>
Motivation: Gradient variation is a central performance measure in online learning. Bandit feedback yields partial gradient information, making sharp, problem-dependent guarantees challenging. The existing results (e.g., Chiang et al., 2013) have suboptimal dimension dependence, motivating refined analyses to achieve tighter bounds.

Method: Refined analysis focusing on non-consecutive gradient variation under two-point feedback. Extends the approach to one-point bandit linear optimization over hyper-rectangular domains. Applies the developed gradient-variation bounds to dynamic/universal regret and bandit games to obtain fast convergence rates.

Result: Improved dimension dependence for convex and strongly convex objectives compared with the best known results. Enhanced guarantees on gradient-variance and small-loss regrets. First gradient-variation bound for one-point BLO over hyper-rectangular domains. Established gradient-variation dynamic and universal regret bounds for two-point BCO and achieved fast convergence rates in bandit games.

Conclusion: The proposed refined gradient-variation framework is versatile across multiple bandit optimization settings, providing sharper, problem-dependent guarantees and enabling fast learning rates in two-point BCO, one-point BLO, and adversarial game scenarios.

Abstract: Gradient-variation online learning has drawn increasing attention due to its deep connections to game theory, optimization, etc. It has been studied extensively in the full-information setting, but is underexplored with bandit feedback. In this work, we focus on gradient variation in Bandit Convex Optimization (BCO) with two-point feedback. By proposing a refined analysis on the non-consecutive gradient variation, a fundamental quantity in gradient variation with bandits, we improve the dimension dependence for both convex and strongly convex functions compared with the best known results (Chiang et al., 2013). Our improved analysis for the non-consecutive gradient variation also implies other favorable problem-dependent guarantees, such as gradient-variance and small-loss regrets. Beyond the two-point setup, we demonstrate the versatility of our technique by achieving the first gradient-variation bound for one-point bandit linear optimization over hyper-rectangular domains. Finally, we validate the effectiveness of our results in more challenging tasks such as dynamic/universal regret minimization and bandit games, establishing the first gradient-variation dynamic and universal regret bounds for two-point BCO and fast convergence rates in bandit games.

</details>


### [174] [NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Image](https://arxiv.org/abs/2602.04769)
*Yan Chen,Jie Peng,Moajjem Hossain Chowdhury,Tianlong Chen,Yunmei Liu*

Main category: cs.LG

TL;DR: NeuroCanvas框架通过Entropy-guided Channel Selector (ECS) 与 Canvas of Neuron Signal (CNS)，实现对多通道EEG的选择性输入和可压缩的视觉化表示，从而提升癫痫发作检测的实时性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决EEG多通道异质性导致的关键信号分布差异，以及将EEG信号编码为大量tokens以输入LLM时的计算低效问题。

Method: ECS筛选与癫痫相关的通道；CNS将选定通道的EEG信号转换为结构化的视觉表示，形成紧凑的视觉tokens供LLM处理。

Result: 在多个数据集上评估，F1提升约20%，推理时延下降约88%。

Conclusion: NeuroCanvas为实时且资源高效的癫痫检测提供可扩展解决方案，代码将公开发布。

Abstract: Accurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention, yet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large language models (LLMs) show promise in handling neural signals across diverse patients, but two significant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially across EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a massive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the novel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided Channel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of Neuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual representations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact visual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas across multiple seizure detection datasets, demonstrating a significant improvement of $20\%$ in F1 score and reductions of $88\%$ in inference latency. These results highlight NeuroCanvas as a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice.The code will be released at https://github.com/Yanchen30247/seizure_detect.

</details>


### [175] [Interval-Based AUC (iAUC): Extending ROC Analysis to Uncertainty-Aware Classification](https://arxiv.org/abs/2602.04775)
*Yuqi Li,Matthew M. Engelhard*

Main category: cs.LG

TL;DR: 提出一个面向区间预测的不确定性感知评估框架，在 ROC/AUC 的基础上引入 AUC_L 与 AUC_U，用以区分区间重叠时的正确/错误/不确定排序并支持可选择性预测（放弃排序）。在有效类别条件覆盖下，AUC_L/ AUC_U 给出对理论最优 AUC 的下界/上界，证明物理极限并可广泛应用于任意区间预测方法。通过真实数据集的自助法区间的实例验证框架的正确性及在不确定性驱动决策中的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的 ROC/AUC 针对点预测，无法充分体现预测不确定性对排序性能的影响。在高风险领域需要对区间预测进行不确定性量化与可解释的排序评估，因此需要一种能够处理区间信息的评估工具。

Method: 提出一个不确定性感知的 ROC 框架，定义两一新指标 AUC_L 与 AUC_U，利用三区域分解将成对样本的排序划分为正确、错误和不确定三类。框架支持可选择性预测（对区间重叠的样本可拒绝排序），并给出在有效覆盖条件下的理论界限。该框架对区间构造方法无关，且可广泛应用于任意区间预测模型。以自举区间作为一种实现实例，在真实数据集上验证正确性与实用性。

Result: 理论层面：AUC_L 与 AUC_U 在有效类别覆盖条件下分别提供对理论最优 AUC 的下界与上界，刻画出可实现的(discrimination)的物理极限。实践层面：自举区间的实例验证了框架的正确性，并展示了在不确定性驱动的决策场景中，其对评估与选择的帮助。

Conclusion: 该框架可广泛应用于任意区间预测模型，有助于不确定性感知的评估与决策，且通过拒绝排序实现对区间重叠情形的鲁棒处理，提升对风险场景中排序可靠性的解释性与实用性。

Abstract: In high-stakes risk prediction, quantifying uncertainty through interval-valued predictions is essential for reliable decision-making. However, standard evaluation tools like the receiver operating characteristic (ROC) curve and the area under the curve (AUC) are designed for point scores and fail to capture the impact of predictive uncertainty on ranking performance. We propose an uncertainty-aware ROC framework specifically for interval-valued predictions, introducing two new measures: $AUC_L$ and $AUC_U$. This framework enables an informative three-region decomposition of the ROC plane, partitioning pairwise rankings into correct, incorrect, and uncertain orderings. This approach naturally supports selective prediction by allowing models to abstain from ranking cases with overlapping intervals, thereby optimizing the trade-off between abstention rate and discriminative reliability. We prove that under valid class-conditional coverage, $AUC_L$ and $AUC_U$ provide formal lower and upper bounds on the theoretical optimal AUC ($AUC^*$), characterizing the physical limit of achievable discrimination. The proposed framework applies broadly to interval-valued prediction models, regardless of the interval construction method. Experiments on real-world benchmark datasets, using bootstrap-based intervals as one instantiation, validate the framework's correctness and demonstrate its practical utility for uncertainty-aware evaluation and decision-making.

</details>


### [176] [Dynamical Regimes of Multimodal Diffusion Models](https://arxiv.org/abs/2602.04780)
*Emil Albrychiewicz,Andrés Franco Valiente,Li-Ching Chen*

Main category: cs.LG

TL;DR: 提出一个耦合扩散模型的理论框架，利用耦合欧氏-温诺赫（OU）过程，显示多模态生成由谱时延层级控制而非同时分辨。引入“同步间隙”概念解释分离性伪影；给出对称与各向异性耦合下的物种化与坍缩时间的解析界限，耦合强度如谱滤波器可调控生成的时间层级。通过 MNIST 与精确得分采样器的受控实验进行验证，提出基于时间的耦合调度以实现对模态特定时间尺度的目标。


<details>
  <summary>Details</summary>
Motivation: 深入理解扩散模型在多模态生成中的理论机制，填补对其背后动力学的理论未知。

Method: 将耦合的OU过程与非平衡统计力学的动力相变理论结合，推导出谱时标层级及同步间隙；在对称与各向异性耦合下推导物种化与崩塌的解析条件，设定耦合强度的严格界限；将耦合视为谱滤波器以强制生成的时间层级。

Result: 给出关于物种化与崩塌时间的解析边界及同步间隙的存在性；证明耦合强度可调控模态的时间尺度；通过对 MNIST 数据集的扩散模型与精确得分采样器进行受控实验支持理论预测。

Conclusion: 时间相关的耦合策略可作为对模态特定时间尺度的有效调控手段，提供替代经验性引导调优的理论路径。

Abstract: Diffusion based generative models have achieved unprecedented fidelity in synthesizing high dimensional data, yet the theoretical mechanisms governing multimodal generation remain poorly understood. Here, we present a theoretical framework for coupled diffusion models, using coupled Ornstein-Uhlenbeck processes as a tractable model. By using the nonequilibrium statistical physics of dynamical phase transitions, we demonstrate that multimodal generation is governed by a spectral hierarchy of interaction timescales rather than simultaneous resolution. A key prediction is the ``synchronization gap'', a temporal window during the reverse generative process where distinct eigenmodes stabilize at different rates, providing a theoretical explanation for common desynchronization artifacts. We derive analytical conditions for speciation and collapse times under both symmetric and anisotropic coupling regimes, establishing strict bounds for coupling strength to avoid unstable symmetry breaking. We show that the coupling strength acts as a spectral filter that enforces a tunable temporal hierarchy on generation. We support these predictions through controlled experiments with diffusion models trained on MNIST datasets and exact score samplers. These results motivate time dependent coupling schedules that target mode specific timescales, offering a potential alternative to ad hoc guidance tuning.

</details>


### [177] [Legendre Memory Unit with A Multi-Slice Compensation Model for Short-Term Wind Speed Forecasting Based on Wind Farm Cluster Data](https://arxiv.org/abs/2602.04782)
*Mumin Zhang,Haochen Zhang,Xin Zhi Khoo,Yilin Zhang,Nuo Chen,Ting Zhang,Junjie Tang*

Main category: cs.LG

TL;DR: 提出 WMF-CPK-MSLMU 的端到端集成模型用于风场聚簇的短期风速预测，结合 WMF 去噪、LMU 的时空建模与 CPK 加权的多切片补偿，提升准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 风电场聚簇增多，需要有效捕捉聚簇内外的时空相关性以提升短期风速预测的准确性、鲁棒性和可扩展性，且需在存在数据缺失或噪声情况下保持性能。

Method: 1) 对单风场数据应用加权均值滤波 WMF 进行去噪；2) 将 Legendre memory unit (LMU) 与基于 Kendall 排序相关系数的 CPk 权重结合，构建多切片 LMU (MSLMU) 以捕捉时空相关性；3) 提出 WMF-CPK-MSLMU 集成模型，包含数据预处理、预测与基于 CPk 的多切片补偿。

Result: 在不同风场聚簇上进行的实验显示，该方法在短期风速预测中优于现有模型，表现出更高的准确性、鲁棒性和良好的计算效率。

Conclusion: LMU 能同时建模线性与非线性依赖，MSLMU 通过 CPk 权重实现对空间相关性的有效激活与信息融合，CPK 的自适应权重用于缺失数据补偿，从而提升整体预测性能与鲁棒性。

Abstract: With more wind farms clustered for integration, the short-term wind speed prediction of such wind farm clusters is critical for normal operation of power systems. This paper focuses on achieving accurate, fast, and robust wind speed prediction by full use of cluster data with spatial-temporal correlation. First, weighted mean filtering (WMF) is applied to denoise wind speed data at the single-farm level. The Legendre memory unit (LMU) is then innovatively applied for the wind speed prediction, in combination with the Compensating Parameter based on Kendall rank correlation coefficient (CPK) of wind farm cluster data, to construct the multi-slice LMU (MSLMU). Finally, an innovative ensemble model WMF-CPK-MSLMU is proposed herein, with three key blocks: data pre-processing, forecasting, and multi-slice compensation. Advantages include: 1) LMU jointly models linear and nonlinear dependencies among farms to capture spatial-temporal correlations through backpropagation; 2) MSLMU enhances forecasting by using CPK-derived weights instead of random initialization, allowing spatial correlations to fully activate hidden nodes across clustered wind farms.; 3) CPK adaptively weights the compensation model in MSLMU and complements missing data spatially, to facilitate the whole model highly accurate and robust. Test results on different wind farm clusters indicate the effectiveness and superiority of proposed ensemble model WMF-CPK-MSLMU in the short-term prediction of wind farm clusters compared to the existing models.

</details>


### [178] [From independent patches to coordinated attention: Controlling information flow in vision transformers](https://arxiv.org/abs/2602.04784)
*Kieran A. Murphy*

Main category: cs.LG

TL;DR: Explicitly quantify and control information flow in vision transformers by inserting variational information bottlenecks on all attention writes to the residual stream, enabling a spectrum from local to global processing and yielding more mechanistically tractable models.


<details>
  <summary>Details</summary>
Motivation: To understand and regulate how information is routed through attention in vision transformers, and to make the information carried by attention an explicit, measurable quantity for analysis and control.

Method: Insert variational information bottlenecks on all attention-mediated writes to the residual stream without changing the architecture otherwise, training models with an explicit information cost.

Result: Established a controllable spectrum from independent patch processing to fully expressive global attention. On ImageNet-100, characterized how classification behavior and information routing evolve across this spectrum and analyzed the first attention heads transmitting information. Demonstrated that constrained internal communication yields models that are more tractable for mechanistic analysis and control.

Conclusion: Biasing learning toward solutions with restricted internal communication produces models that are easier to analyze mechanistically and more amenable to control.

Abstract: We make the information transmitted by attention an explicit, measurable quantity in vision transformers. By inserting variational information bottlenecks on all attention-mediated writes to the residual stream -- without other architectural changes -- we train models with an explicit information cost and obtain a controllable spectrum from independent patch processing to fully expressive global attention. On ImageNet-100, we characterize how classification behavior and information routing evolve across this spectrum, and provide initial insights into how global visual representations emerge from local patch processing by analyzing the first attention heads that transmit information. By biasing learning toward solutions with constrained internal communication, our approach yields models that are more tractable for mechanistic analysis and more amenable to control.

</details>


### [179] [Maximum-Volume Nonnegative Matrix Factorization](https://arxiv.org/abs/2602.04795)
*Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.LG

TL;DR: MaxVol NMF：最大化H的体积的对偶策略；在有噪声时更易获得稀疏解且避免秩亏，且最大体积解对应将X列聚为不相交簇。提出两种求解算法，并给出一个归一化变体，介于标准NMF与正交NMF之间，应用于高光谱解混。


<details>
  <summary>Details</summary>
Motivation: 在NMF中，尽管MinVol可提供可解释且独特解，但在噪声下易产生秩亏或不稳。对偶的MaxVol在稀疏性和鲁棒性方面可能具有优势，因此研究其可识别性、算法实现及应用。

Method: 通过将体积最大化作为目标，提出两种算法来求解MaxVol NMF；并提出一个归一化的MaxVol变体，其本质上形成一个从标准NMF到正交NMF的连续家族。

Result: 在无噪声情况下，MaxVol NMF与MinVol NMF具有相同的可识别性条件；但在存在噪声时，MaxVol更易得到稀疏解且不产生秩亏；最大体积解对应将X列聚回不相交的簇；两种算法有效实现MaxVol NMF；归一化变体表现优于两者，并提供一个理论连结。

Conclusion: MaxVol NMF是MinVol NMF的有力对偶，尤其在带噪声的情形下更具鲁棒性和稀疏性；可用于高光谱解混等应用，且提供理解NMF解的新视角。

Abstract: Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.

</details>


### [180] [Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning](https://arxiv.org/abs/2602.04807)
*Wolfgang Maass,Sabine Janzen,Prajvi Saxena,Sach Mukherjee*

Main category: cs.LG

TL;DR: Afferent Learning proposes Computational Afferent Traces (CAT) as adaptive internal risk signals, implemented via a two-level loop: outer-loop evolutionary design of afferent sensing architectures and inner-loop reinforcement learning for damage-avoidance policies, with theoretical convergence guarantees. Demonstrated on biomechanical digital twins over long horizons, CAT-based architectures improve efficiency and age-robustness (e.g., 23% reduction in high-risk actions) versus hand-designed baselines; ablations confirm the essential roles of CAT signals, evolution, and predictive discrepancy; code and data released.


<details>
  <summary>Details</summary>
Motivation: Biological inspiration for efficient, long-horizon learning: provide an inductive bias through adaptive afferent sensing architectures that enable effective policy learning rather than directly minimizing damage. Outer-loop design searches architectures that maximize learnability; inner-loop RL exploits these signals to learn robust policies, with convergence guarantees under smoothness and bounded-noise assumptions. Applied to biomechanical digital twins with life-course timescales to study age-dependent adaptation.

Method: Two-level optimization: outer loop uses evolutionary optimization to discover afferent sensing architectures that enable effective policy learning; inner loop uses reinforcement learning to train damage-avoidance policies using CAT-derived signals as inductive biases. The framework formalizes afferent sensing as an inductive bias for efficient learning. Theoretical convergence guarantees are provided under smoothness and bounded-noise assumptions. Empirical illustration on biomechanical digital twins over decades, showing CAT-based architectures outperform hand-designed baselines in efficiency and age-robustness; ablation studies validate the roles of CAT signals, evolution, and predictive discrepancy. Code and data released for reproducibility.

Result: CAT-based evolved architectures achieve significantly higher learning efficiency and better age-robustness than hand-designed baselines, enabling policies with age-dependent behavioral adaptation and a 23% reduction in high-risk actions. Ablation studies show CAT signals, the evolutionary process, and predictive discrepancy are essential components.

Conclusion: CAT-based evolved sensing architectures provide a principled, effective inductive bias that enhances long-horizon damage-avoidance learning. The two-level setup (evolutionary design of afferent sensing and RL training) yields improved efficiency and robustness, demonstrated in biomechanical digital twins. The work suggests broader applicability of learning-to-design sensing architectures with theoretical convergence guarantees and reproducible code/data.

Abstract: We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.

</details>


### [181] [The Key to State Reduction in Linear Attention: A Rank-based Perspective](https://arxiv.org/abs/2602.04852)
*Philipp Nazari,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: The paper analyzes the role of rank in linear attention, showing that low effective rank can amplify query noise and increase retrieval error; it proposes a hardware-aware, structured pruning framework (including a rank-revealing QR method) to reduce state size by pruning key/query matrices, achieving ~50% channel pruning with only marginal perplexity rise; demonstrates CUDA-compatible deployment and releases code.


<details>
  <summary>Details</summary>
Motivation: To understand why linear attention underutilizes capacity due to low-rank structure and to develop post-training state-size reduction techniques that preserve performance while improving speed and memory efficiency.

Method: Theoretically analyze rank effects on retrieval error in linear attention; adapt existing pruning strategies to the framework; introduce a novel structured pruning method based on rank-revealing QR decomposition; ensure compatibility with existing CUDA kernels; perform empirical evaluation across model sizes and downstream tasks.

Result: Demonstrates the effectiveness of the state reduction framework; achieves 50% pruning of query/key channels with only marginal increase in perplexity; shows improvements in speed/memory efficiency and maintains compatibility with CUDA kernels; provides code release.

Conclusion: Low-rank states in linear attention can be substantially reduced post-training with minimal performance loss, enabling faster and more memory-efficient models through hardware-aware structured pruning, notably using rank-revealing QR-based pruning.

Abstract: Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.

</details>


### [182] [Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism](https://arxiv.org/abs/2602.04870)
*Chenwei Cui,Rockwell Jackson,Benjamin Joseph Herrera,Ana María Tárano,Hannah Kerner*

Main category: cs.LG

TL;DR: Proposes Multi-Head LatentMoE with Head Parallel (HP) to achieve O(1) communication wrt activated experts, balanced traffic, and deterministic routing, compatible with existing Expert Parallel (EP), leading to faster training with equal or better performance.


<details>
  <summary>Details</summary>
Motivation: Reduce the training cost and mitigate three key limitations of EP in Sparse MoE: (1) communication grows linearly with the number of activated experts k, (2) load imbalance hurting latency and memory, (3) data-dependent communication requiring metadata exchange.

Method: Introduce Multi-Head LatentMoE architecture and Head Parallel (HP). Develop IO-aware routing and optimized expert computation to realize O(1) communication and completely balanced, deterministic traffic while remaining compatible with EP.

Result: Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to 1.61x faster with identical performance. With doubled granularity, achieves higher overall performance and is 1.11x faster. Demonstrates improved efficiency enabling large-scale foundation model research.

Conclusion: The proposed architecture and parallelism substantially improve training efficiency for sparse MoE, offering deterministic, balanced, and scalable communication, thereby lowering the barrier to research with multi-billion-parameter models.

Abstract: Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, and data-dependent communication requires metadata exchange. We propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving $O(1)$ communication cost regardless of $k$, completely balanced traffic, and deterministic communication, all while remaining compatible with EP. To accelerate Multi-Head LatentMoE, we propose IO-aware routing and expert computation. Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to $1.61\times$ faster while having identical performance. With doubled granularity, it achieves higher overall performance while still being $1.11\times$ faster. Our method makes multi-billion-parameter foundation model research more accessible.

</details>
