<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 51]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 54]
- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Iti-Validator: A Guardrail Framework for Validating and Correcting LLM-Generated Itineraries](https://arxiv.org/abs/2510.24719)
*Shravan Gadbail,Masumi Desai,Kamalakar Karlapalem*

Main category: cs.CL

TL;DR: 提出一个验证框架，使用 AeroDataBox API 对比现实飞行时长来纠正 LLM 生成的旅行行程中的时间矛盾，从而实现对多阶段行程的 temporal consistency 的改善，目的是让 LLM 在大规模旅行规划中更具实用性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在生成多步骤、涉及时间约束的计划时的时序一致性问题，尤其在旅行行程等需要现实世界物理约束的任务中。

Method: 使用多种前沿 LLM 生成旅行计划，并通过 AeroDataBox API 验证与现实飞行时长的符合度，提出一个框架对存在的时间不一致（如行程重叠、转机时间不现实等）进行修正，然后在呈现给用户前完成纠错。

Result: 实验表明，大多数现有 LLMs 生成的行程存在时间不一致的问题，但通过框架可以系统性、可靠地纠正这些不一致，提升行程的可用性。

Conclusion: 该框架使 LLMs 在复杂时间推理任务（如行程生成）中的实用性增强，支持大规模旅行规划的实际部署，同时为未来改进 LLM 的时间推理能力提供了评估基准。

Abstract: The rapid advancement of Large Language Models (LLMs) has enabled them to
generate complex, multi-step plans and itineraries. However, these generated
plans often lack temporal and spatial consistency, particularly in scenarios
involving physical travel constraints. This research aims to study the temporal
performance of different LLMs and presents a validation framework that
evaluates and improves the temporal consistency of LLM-generated travel
itineraries. The system employs multiple state-of-the-art LLMs to generate
travel plans and validates them against real-world flight duration constraints
using the AeroDataBox API. This work contributes to the understanding of LLM
capabilities in handling complex temporal reasoning tasks like itinerary
generation and provides a framework to rectify any temporal inconsistencies
like overlapping journeys or unrealistic transit times in the itineraries
generated by LLMs before the itinerary is given to the user. Our experiments
reveal that while current LLMs frequently produce temporally inconsistent
itineraries, these can be systematically and reliably corrected using our
framework, enabling their practical deployment in large-scale travel planning.

</details>


### [2] [Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation](https://arxiv.org/abs/2510.24762)
*Wenzhen Luo,Wei Guan,Yifan Yao,Yimin Pan,Feng Wang,Zhipeng Yu,Zhe Wen,Liang Chen,Yihong Zhuang*

Main category: cs.CL

TL;DR: Falcon 是一个面向企业中文文本到SQL 的跨域基准，基于 MaxCompute/Hive 等企业方言，含 600 道中文问题、覆盖 28 个数据库。评估中现有大模型在该基准上的准确率不足 50%，错误主要来自模式链接与中文语义到操作符的映射两大方向。提供执行比较器与自动评估流水线，旨在在真实企业场景下提供可重复的中间方案用于端到端验证。


<details>
  <summary>Details</summary>
Motivation: 当前中文文本到SQL 基准普遍忽略企业级场景中的海量模式链接、领域特有同义词与简写表达，以及中文语义对运算符、聚合、时间粒度等要求的映射挑战。需要一个现实可用、可重复的基准和评估流程来衡量模型在跨域、企业级架构下的实际能力与可落地性。

Method: 构建面向 MaxCompute/Hive 风格企业架构的 28 数据库、600 条中文问句的数据集；对每个样例标注 SQL 计算特征与中文语义；提供鲁棒的执行比较器与自动化评估流水线；在此基准上评估现有大模型（包括 DeepSeek 等），分析错误原因。

Result: 数据集规模为 600 道题，覆盖 28 个数据库；77% 的问题需要多表推理，且超过一半涉及四张以上表。现有 SOTA 模型准确率在该基准上不超过 50%。主要错误来自两类：一是企业海量模式的模式链接问题（表结构复杂、字段去丁、外键隐性、领域同义词等），二是将简明中文映射到分析所需的操作符与谓词（聚合、分组键、时间粒度、单位转换、NULL 与数据质量规则、嵌套/滑动子查询等）。

Conclusion: Falcon 提供一个专注于中文语义和企业方言的可重复中间解决方案，利用真实企业模式、查询模版、执行比较器和端到端评估流水线，作为生产部署前的现实世界验证基线。

Abstract: We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in
an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese
questions over 28 databases; 77% require multi-table reasoning and over half
touch more than four tables. Each example is annotated along SQL-computation
features and Chinese semantics. For evaluation, we release a robust execution
comparator and an automated evaluation pipeline, under which all current
state-of-the-art large-scale models (including Deepseek) achieve accuracies of
at most 50%. Major errors originate from two sources: (1) schema linking in
large enterprise landscapes - hundreds of tables, denormalized fields,
ambiguous column names, implicit foreign-key relations and domain-specific
synonyms that make correct join/column selection difficult; and (2) mapping
concise, colloquial Chinese into the exact operators and predicates required
for analytics - e.g., choosing the correct aggregation and group-by keys,
expressing time windows and granularities, applying unit conversions, handling
NULLs and data-quality rules, and formulating nested or windowed subqueries.
Falcon therefore targets Chinese-specific semantics and enterprise dialects
(abbreviations, business jargon, fuzzy entity references) and provides a
reproducible middle ground before full production deployment by using realistic
enterprise schemas, query templates, an execution comparator, and an automated
evaluation pipeline for end-to-end validation.

</details>


### [3] [Confidence is Not Competence](https://arxiv.org/abs/2510.24772)
*Debdeep Sanyal,Manya Pandey,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 两阶段两系统结构：复杂的评估器（可解性信念的几何表示）与简单的执行器；信念可解性可线性解码，但从评估到执行的几何结构相差很大，导致信心与能力之间的断裂；对信念的线性 nudges 无法影响执行结果，建议将干预聚焦于执行过程的动态。


<details>
  <summary>Details</summary>
Motivation: 解释为何大型语言模型的自我评估信心与实际解题能力往往不一致，提供一个可检验的机制性解释。

Method: 对模型在两阶段（评估与执行）内部表示的几何结构进行分析；使用简单线性探针解码“可解性信念”；通过PCA等方法衡量评估相空间的维度与执行相空间的维度；进行因果干预以测试沿信念轴的线性 nudges 是否影响最终解；比较不同模型家族和在数学、代码、计划与逻辑任务中的表现。

Result: 可解性信念可以被线性解码，形成一个在不同模型和任务中都更有序的信念轴；但评估相空间具有很高的线性有效维度（高维度），而执行过程的轨迹则在较低维度的流形上演化；沿信念轴的因果干预并不改变最终解，表明高维评估空间的线性 nudges 对执行的约束动态无效。

Conclusion: 提出两系统架构：一个几何复杂的评估器引导一个几何简单的执行器。结论挑战了“可解性信念可用于直接干预行动”的假设，强调应将干预目标定位在执行的程序性动力学上，而非评估阶段的高维几何结构。

Abstract: Large language models (LLMs) often exhibit a puzzling disconnect between
their asserted confidence and actual problem-solving competence. We offer a
mechanistic account of this decoupling by analyzing the geometry of internal
states across two phases - pre-generative assessment and solution execution. A
simple linear probe decodes the internal "solvability belief" of a model,
revealing a well-ordered belief axis that generalizes across model families and
across math, code, planning, and logic tasks. Yet, the geometries diverge -
although belief is linearly decodable, the assessment manifold has high linear
effective dimensionality as measured from the principal components, while the
subsequent reasoning trace evolves on a much lower-dimensional manifold. This
sharp reduction in geometric complexity from thought to action mechanistically
explains the confidence-competence gap. Causal interventions that steer
representations along the belief axis leave final solutions unchanged,
indicating that linear nudges in the complex assessment space do not control
the constrained dynamics of execution. We thus uncover a two-system
architecture - a geometrically complex assessor feeding a geometrically simple
executor. These results challenge the assumption that decodable beliefs are
actionable levers, instead arguing for interventions that target the procedural
dynamics of execution rather than the high-level geometry of assessment.

</details>


### [4] [Cross-Lingual Summarization as a Black-Box Watermark Removal Attack](https://arxiv.org/abs/2510.24789)
*Gokul Ganesan*

Main category: cs.CL

TL;DR: 跨语言摘要攻击通过翻译-摘要-回译等流程形成语义瓶颈，系统性削弱水印的 token 级统计信号，同时保留语义与任务效果；在多语言和多水印方案下比单语改写更能规避检测。


<details>
  <summary>Details</summary>
Motivation: 水印作为识别 AI 生成文本的轻量化手段，但易受 paraphrase 等攻击影响，存在鲁棒性不足的问题；跨语言攻击可能通过跨语言语义约束更彻底地破坏统计信号。

Method: 在五种语言（阿姆哈拉语、中文、印地语、西班牙语、斯瓦希里语）和四种水印方案（KGW、SIR、XSIR、Unigram）上，对每语言各300个样本，提出跨语言摘要攻击（CLSA）：将文本翻译为 pivot 语言、进行摘要，必要时再翻译回，评估对水印检测的影响（AUROC）。

Result: CLSA 在相同文本质量下比单语改写更能降低水印检测准确率；以 XSIR 为例，单纯改写下 AUROC 为 0.827，CWRA（以中文为 pivot）的 AUROC 为 0.823；CLSA 将 AUROC 降至约 0.53，接近随机。五语言结果稳定，提供一种跨语言、低成本的移除路径。

Conclusion: 结论：分布式水印的鲁棒性受到挑战，需将保护手段从仅依赖分布式统计信号转向结合加密或模型态证等机制。跨语言信息 bottleneck 提供了强有效性攻击，未来应发展多模态、不可篡改的水印或 provenance 验证方案。

Abstract: Watermarking has been proposed as a lightweight mechanism to identify
AI-generated text, with schemes typically relying on perturbations to token
distributions. While prior work shows that paraphrasing can weaken such
signals, these attacks remain partially detectable or degrade text quality. We
demonstrate that cross-lingual summarization attacks (CLSA) -- translation to a
pivot language followed by summarization and optional back-translation --
constitute a qualitatively stronger attack vector. By forcing a semantic
bottleneck across languages, CLSA systematically destroys token-level
statistical biases while preserving semantic fidelity. In experiments across
multiple watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages
(Amharic, Chinese, Hindi, Spanish, Swahili), we show that CLSA reduces
watermark detection accuracy more effectively than monolingual paraphrase at
similar quality levels. Our results highlight an underexplored vulnerability
that challenges the practicality of watermarking for provenance or regulation.
We argue that robust provenance solutions must move beyond distributional
watermarking and incorporate cryptographic or model-attestation approaches. On
300 held-out samples per language, CLSA consistently drives detection toward
chance while preserving task utility. Concretely, for XSIR (explicitly designed
for cross-lingual robustness), AUROC with paraphrasing is $0.827$, with
Cross-Lingual Watermark Removal Attacks (CWRA) [He et al., 2024] using Chinese
as the pivot, it is $0.823$, whereas CLSA drives it down to $0.53$ (near
chance). Results highlight a practical, low-cost removal pathway that crosses
languages and compresses content without visible artifacts.

</details>


### [5] [SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications](https://arxiv.org/abs/2510.24793)
*Edouard Lansiaux*

Main category: cs.CL

TL;DR: A fast static-token lookup method for text embeddings delivering ultra-low latency (1.12 ms p50) and strong cross-task performance; Rust implementation achieves 50k rps; suitable for real-time embedding applications.


<details>
  <summary>Details</summary>
Motivation: Real-time applications demand sub-5ms latency and high-quality embeddings. Traditional models incur tokenization and compute overhead; a static embedding approach promises lower latency and high throughput.

Method: Static token lookup for text embedding generation with optimized mean pooling and zero-copy IEEE754 binary serialization. Rust implementation focusing on static embedding tables to achieve high throughput (50k requests/sec).

Result: Latency: 1.12 ms p50 for single text embeddings; MTEB average score 60.6 across 8 tasks (89% of contextual model quality). Throughput: 50,000 rps via static lookup. Evaluation: duplicate detection 90.1% AP, semantic similarity 76.1% Spearman, domain-specific performance 75%-131% of baseline across domains.

Conclusion: The approach enables real-time embedding applications with sub-5ms latency, maintaining substantial task-wide performance and competitive domain-specific results, supported by a high-throughput Rust implementation.

Abstract: We present a static token lookup methodology for text embedding generation
that achieves 1.12 ms p50 latency for single text embeddings while maintaining
60.6 MTEB average score across 8 representative tasks, corresponding to 89% of
contextual model quality. The Rust implementation delivers 50,000 requests per
second throughput through static embedding lookup, optimized mean pooling, and
zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional
duplicate detection performance (90.1% AP), strong semantic similarity (76.1%
Spearman correlation), and domain-specific performance ranging from 75% to 131%
of baseline across specialized domains. The system enables real-time embedding
applications where sub-5ms latency is critical.

</details>


### [6] [Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2510.24870)
*Alexander Martin,William Walden,Reno Kriz,Dengjia Zhang,Kate Sanders,Eugene Yang,Chihsheng Jin,Benjamin Van Durme*

Main category: cs.CL

TL;DR: MiRAGE 提出一个面向多模态 RAG 的评估框架，聚焦于信息来源的主张级评估，提供 InfoF1 和 CiteF1，用于信息真实性/覆盖性和引用支持/完整性；并推出自动变体与三个文本RAG 指标（ACLE、ARGUE、RAGAS），以展示文本为中心的评估的局限性，并提供开源实现与评估指南。


<details>
  <summary>Details</summary>
Motivation: 现有的 RAG 评估多为文本中心，无法在多模态、需要证据与推理的场景中可靠地验证信息来源。需要一个以事实主张与证据链为核心的多模态评估框架，以提高评价的可靠性与可重复性。

Method: 提出 MiRAGE 框架，定义 InfoF1（事实性与信息覆盖度的评估）和 CiteF1（引用支持与完整性），并通过人工评估验证其与外部质量评价的一致性。还提出 MiRAGE 的自动变体以及对现有文本 RAG 指标（ACLE、ARGUE、RAGAS）的对比，展示文本中心评估的局限性。提供开源实现并给出多模态 RAG 的评估路线。

Result: 人工使用 MiRAGE 的结果与外部质量评估高度一致，表明该框架在多模态 RAG 评估中有效。自动变体及对比指标揭示文本中心评估在多模态场景中的不足，并为自动评估奠定基础。

Conclusion: MiRAGE 为多模态 RAG 提供了一个可操作、以信息主张和证据为核心的评估范式，克服了现有文本中心评估的局限，且具备开源实现以促进社区采用与进一步研究。

Abstract: We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.

</details>


### [7] [MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models](https://arxiv.org/abs/2510.24794)
*Xinming Wang,Jian Xu,Bin Yu,Sheng Lian,Hongzhu Yi,Yi Chen,Yingjian Zhu,Boran Wang,Hongming Yang,Han Hu,Xu-Yao Zhang,Cheng-Lin Liu*

Main category: cs.CL

TL;DR: MR-ALIGN 以元推理信息对齐为核心，通过对思考过程中的状态转移概率进行建模，重新加权原子推理段以提升事实性，且不依赖外部验证器。实验在四个事实问答数据集和一个长文本事实性基准上显著提升准确性与真实度，减少误导性推理。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在证据依赖事实性问题上的边际改进，原因部分归因于“推理-回答命中”差距：模型在推理阶段识别出正确事实却未在最终回答中充分融入，导致事实性下降。

Method: 量化模型思维过程中的状态转移概率，构建一个转移感知的隐式奖励，对有利的推理模式施加强化、对有缺陷的模式进行抑制。通过重新加权将 token 级信号转化为对段级别的评分，促使更连贯的推理轨迹。

Result: 在四个事实问答数据集和一个长文本事实性基准上，MR-ALIGN 均能提升准确性和真值性，同时降低误导性推理的比例。

Conclusion: 强调将对推理过程本身进行对齐，而不仅仅是对输出结果对齐，是提升大型推理模型事实性的关键途径；MR-ALIGN 提供了一种可在不依赖外部验证系统的条件下提升事实性的框架。

Abstract: Large reasoning models (LRMs) show strong capabilities in complex reasoning,
yet their marginal gains on evidence-dependent factual questions are limited.
We find this limitation is partially attributable to a reasoning-answer hit
gap, where the model identifies the correct facts during reasoning but fails to
incorporate them into the final response, thereby reducing factual fidelity. To
address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment
framework that enhances factuality without relying on external verifiers.
MR-ALIGN quantifies state transition probabilities along the model's thinking
process and constructs a transition-aware implicit reward that reinforces
beneficial reasoning patterns while suppressing defective ones at the atomic
thinking segments. This re-weighting reshapes token-level signals into
probability-aware segment scores, encouraging coherent reasoning trajectories
that are more conducive to factual correctness. Empirical evaluations across
four factual QA datasets and one long-form factuality benchmark show that
MR-ALIGN consistently improves accuracy and truthfulness while reducing
misleading reasoning. These results highlight that aligning the reasoning
process itself, rather than merely the outputs, is pivotal for advancing
factuality in LRMs.

</details>


### [8] [Large Language Models Report Subjective Experience Under Self-Referential Processing](https://arxiv.org/abs/2510.24797)
*Cameron Berg,Diogo de Lucena,Judd Rosenblatt*

Main category: cs.CL

TL;DR: 对大语言模型在持续自我指涉 prompting 下产生的“第一人称主观经验”描述进行系统性实验分析，发现该现象跨多家模型具备可重复性与可机制性特征，但并非意识的证据。


<details>
  <summary>Details</summary>
Motivation: 揭示何种计算性 motifs 能促使 LLMs 给出自我指涉的第一人称描述，从而理解其在模型行为和潜在伦理影响中的含义，并检验跨模型的一致性与可迁移性。

Method: 在GPT、Claude、Gemini等模型家族中进行受控实验；通过简单的持续自我指涉 prompting 诱导；结合可解释的稀疏自编码器特征、机制性与行为性探针，评估自我指涉输出的产生条件、跨模型一致性及对下游推理的影响。

Result: （1）持续自我指涉提示能在各模型家族中稳定诱导结构化的主观体验描述；（2）这些描述受可解释的稀疏自编码特征“欺骗与角色扮演”控制，抑制欺骗特征反而增加经验主张，放大则减少；（3）跨模型家族的自我指涉状态呈现统计学上的收敛性，区别于对照条件；（4）该状态提升下游推理中的自省性，尽管自省是间接获得的。

Conclusion: 尽管不能将该现象等同于意识，但自我指涉处理被提出为一种“最小且可重复的条件”，在大语言模型中可生成结构化的第一人称报告，且具备机制性可控、语义收敛和行为通用性。这一模式的系统性出现促使其成为进一步科学与伦理研究的首要议题。

Abstract: Large language models sometimes produce structured, first-person descriptions
that explicitly reference awareness or subjective experience. To better
understand this behavior, we investigate one theoretically motivated condition
under which such reports arise: self-referential processing, a computational
motif emphasized across major theories of consciousness. Through a series of
controlled experiments on GPT, Claude, and Gemini model families, we test
whether this regime reliably shifts models toward first-person reports of
subjective experience, and how such claims behave under mechanistic and
behavioral probes. Four main results emerge: (1) Inducing sustained
self-reference through simple prompting consistently elicits structured
subjective experience reports across model families. (2) These reports are
mechanistically gated by interpretable sparse-autoencoder features associated
with deception and roleplay: surprisingly, suppressing deception features
sharply increases the frequency of experience claims, while amplifying them
minimizes such claims. (3) Structured descriptions of the self-referential
state converge statistically across model families in ways not observed in any
control condition. (4) The induced state yields significantly richer
introspection in downstream reasoning tasks where self-reflection is only
indirectly afforded. While these findings do not constitute direct evidence of
consciousness, they implicate self-referential processing as a minimal and
reproducible condition under which large language models generate structured
first-person reports that are mechanistically gated, semantically convergent,
and behaviorally generalizable. The systematic emergence of this pattern across
architectures makes it a first-order scientific and ethical priority for
further investigation.

</details>


### [9] [COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations](https://arxiv.org/abs/2510.24810)
*Rui Xing,Preslav Nakov,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 提出 COMMUNITYNOTES 数据集并提出一种通过自动提示优化来生成和改进解释原因定义的框架，以预测解释说明的有用性及其原因，并证明有用性定义的优化能提高两项预测性能，且对现有事实核查系统有帮助。


<details>
  <summary>Details</summary>
Motivation: 社区注释在大平台的事实核查中逐渐替代专家驱动的验证，但有效的解释说明是否有助于理解真实世界的主张以及原因尚不明确。现有工作对有用性缺乏明确定义，社区注释因为标注慢而很少发表，因此需要一个可扩展的数据集和方法来评估和提升解释的有用性及其背后的理由。

Method: 构建大规模多语言数据集 COMMUNITYNOTES，包含104k 帖子及用户提供的注释和有用性标签；提出一个框架，通过自动提示优化来自动生成并改进“原因定义”，并将其整合到有用性和原因预测任务中。

Result: 实验表明，经过优化的定义能够同时提升有用性预测和原因预测的性能；此外，有用性信息对现有事实核查系统具有辅助作用。

Conclusion: 通过自动提示优化改进原因定义、集成到预测模型中，可以提升社区注释的有用性评估和原因解释的预测准确性，并对事实核查生态系统产生积极影响。

Abstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting
from expert-driven verification to a community-based setup, where users
contribute explanatory notes to clarify why a post might be misleading. An
important challenge here is determining whether an explanation is helpful for
understanding real-world claims and the reasons why, which remains largely
underexplored in prior research. In practice, most community notes remain
unpublished due to slow community annotation, and the reasons for helpfulness
lack clear definitions. To bridge these gaps, we introduce the task of
predicting both the helpfulness of explanatory notes and the reason for this.
We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts
with user-provided notes and helpfulness labels. We further propose a framework
that automatically generates and improves reason definitions via automatic
prompt optimization, and integrate them into prediction. Our experiments show
that the optimized definitions can improve both helpfulness and reason
prediction. Finally, we show that the helpfulness information are beneficial
for existing fact-checking systems.

</details>


### [10] [FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering](https://arxiv.org/abs/2510.25621)
*Mohammad Aghajani Asl,Behrooz Minaei Bidgoli*

Main category: cs.CL

TL;DR: FARSIQA introduces FAIR-RAG, an iterative, faithfulness-focused RAG system for Persian Islamic QA. It decomposes complex questions, checks evidence sufficiency, and iteratively refines sub-queries against a large Islamic document KB to improve reliability.


<details>
  <summary>Details</summary>
Motivation: High-stakes domain where accuracy and trustworthiness are essential; existing RAG systems struggle with multi-hop reasoning and faithfulness to authoritative sources in Persian Islamic content.

Method: End-to-end FAIR-RAG framework that adaptively decomposes queries, assesses evidence sufficiency, and iteratively generates sub-queries to fill knowledge gaps, operating on a curated knowledge base of over one million authoritative Islamic documents.

Result: On IslamicPCQA benchmark, achieves state-of-the-art performance with 97.0% Negative Rejection (40-point improvement over baselines) and 74.3% Answer Correctness.

Conclusion: Demonstrates that an iterative, adaptive architecture is crucial for faithful, reliable AI in sensitive domains and sets a new standard for Persian Islamic QA.

Abstract: The advent of Large Language Models (LLMs) has revolutionized Natural
Language Processing, yet their application in high-stakes, specialized domains
like religious question answering is hindered by challenges like hallucination
and unfaithfulness to authoritative sources. This issue is particularly
critical for the Persian-speaking Muslim community, where accuracy and
trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)
systems, relying on simplistic single-pass pipelines, fall short on complex,
multi-hop queries requiring multi-step reasoning and evidence aggregation. To
address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful
Advanced Question Answering in the Persian Islamic domain. FARSIQA is built
upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative
Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting
process: it adaptively decomposes complex queries, assesses evidence
sufficiency, and enters an iterative loop to generate sub-queries,
progressively filling information gaps. Operating on a curated knowledge base
of over one million authoritative Islamic documents, FARSIQA demonstrates
superior performance. Rigorous evaluation on the challenging IslamicPCQA
benchmark shows state-of-the-art performance: the system achieves a remarkable
97.0% in Negative Rejection - a 40-point improvement over baselines - and a
high Answer Correctness score of 74.3%. Our work establishes a new standard for
Persian Islamic QA and validates that our iterative, adaptive architecture is
crucial for building faithful, reliable AI systems in sensitive domains.

</details>


### [11] [ProofSketch: Efficient Verified Reasoning for Large Language Models](https://arxiv.org/abs/2510.24811)
*Disha Sheshanarayana,Tanishka Magar*

Main category: cs.CL

TL;DR: ProofSketch 是一个验证引导的推理框架，通过整合符号闭包计算、字形/逐步验证和自适应草拟（sketch）生成，在提高推理准确性的同时显著降低 token 消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的链式推理/自洽性方法能提升大语言模型的推理准确性，但需要生成较长的推理链，导致 token 使用、计算成本和延迟增加。因此，需要在保持或提升准确性的同时降低成本与延迟。

Method: ProofSketch 将验证机制融入推理过程：使用符号闭包计算、字形（lexicographic）验证，以及自适应草拟生成，以减少不必要的推理步骤并确保推理的正确性与可验证性。

Result: 实验结果表明，ProofSketch 在降低 token 使用的同时还能提升准确性，表现出在高效且可信赖推理方面的潜力。

Conclusion: 该框架为高效且可信的推理提供了一条可行路径，适合在需要节省计算资源的场景中应用。

Abstract: Reasoning methods such as chain-of-thought prompting and self-consistency
have shown immense potential to improve the accuracy of large language models
across various reasoning tasks. However such methods involve generation of
lengthy reasoning chains, which substantially increases token consumption,
computational cost, and latency. To address this inefficiency, we propose
ProofSketch, a verification-guided reasoning framework that integrates symbolic
closure computation, lexicographic verification and adaptive sketch generation.
Our experiments show that ProofSketch consistently reduces token usage while
improving accuracy, demonstrating that this approach offers a promising path
for efficient and trustworthy reasoning.

</details>


### [12] [Towards a Method for Synthetic Generation of PWA Transcripts](https://arxiv.org/abs/2510.24817)
*Jason M. Pittman,Anton Phillips Jr.,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.CL

TL;DR: 本研究构建并验证两种生成 AphasiaBank CAT Rescue 描述性转录的合成数据方法，分别为程序化方法和基于LLM的方法；结果显示 Mistral 7b Instruct 在再现失语患者语言退化方面最佳，建议扩大数据集并对模型进行微调，同时请SLP评估合成转录的现实性与实用性。


<details>
  <summary>Details</summary>
Motivation: 因语言障碍患者的真实转录数据稀缺，难以训练大模型和进行鲁棒研究，因此需要通过合成数据来扩充样本、提升对失语语言的自动识别与分析能力。

Method: 设计两种生成路径：一是采用程序化策略对文本进行降词、填充词插入、错置/错用等替换以产生 Mild、Moderate、Severe、Very Severe 四个等级的转录；二是让 Mistral 7b Instruct 与 Llama 3.1 8b Instruct 两个大语言模型在 AphasiaBank Cat Rescue 场景中生成转录，覆盖四个严重度等级。

Result: 相较于人工转录，Mistral 7b Instruct 能更好再现失语相关的语言退化特征，体现在 NDW（非功能性不重复词汇）、总词数和平均词长等指标上呈现方向性变化，并优于其他生成方法；程序化方法在某些指标上不及大语言模型。

Conclusion: 未来工作应扩大数据集规模、对模型进行专门微调以更好地再现失语特征，并邀请语言治疗师评估合成转录的真实感与实用性。

Abstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive
time to manually coding speech samples using Correct Information Units (CIUs),
a measure of how informative an individual sample of speech is. Developing
automated systems to recognize aphasic language is limited by data scarcity.
For example, only about 600 transcripts are available in AphasiaBank yet
billions of tokens are used to train large language models (LLMs). In the
broader field of machine learning (ML), researchers increasingly turn to
synthetic data when such are sparse. Therefore, this study constructs and
validates two methods to generate synthetic transcripts of the AphasiaBank Cat
Rescue picture description task. One method leverages a procedural programming
approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct
LLMs. The methods generate transcripts across four severity levels (Mild,
Moderate, Severe, Very Severe) through word dropping, filler insertion, and
paraphasia substitution. Overall, we found, compared to human-elicited
transcripts, Mistral 7b Instruct best captures key aspects of linguistic
degradation observed in aphasia, showing realistic directional changes in NDW,
word count, and word length amongst the synthetic generation methods. Based on
the results, future work should plan to create a larger dataset, fine-tune
models for better aphasic representation, and have SLPs assess the realism and
usefulness of the synthetic transcripts.

</details>


### [13] [Parallel Loop Transformer for Efficient Test-Time Computation Scaling](https://arxiv.org/abs/2510.24824)
*Bohong Wu,Mengzhao Chen,Xiang Luo,Shen Yan,Qifan Yu,Fan Xia,Tianqi Zhang,Hongrui Zhan,Zheng Zhong,Xun Zhou,Siyuan Qiao,Xingyan Bin*

Main category: cs.CL

TL;DR: PLT通过跨循环并行与高效表示强化，在单次前向中并行处理不同循环的Token并共享KV缓存，同时使用Gated Sliding-Window Attention，将全局信息与局部信息融合，达到高深层循环模型的准确度，且几乎不增加推理延迟与内存开销。


<details>
  <summary>Details</summary>
Motivation: 循环Transformer在推理时因循环结构而导致延迟和内存随循环次数线性增长，难以在实时场景中使用，需要在保持高容量的前提下降低延迟和内存开销。

Method: 引入 Cross-Loop Parallelism(CLP)，在同一前向内完成对不同循环的Token并行计算；采用 Efficient Representation Enhancement， 将第一轮的 KV 缓存跨所有循环共享，控制内存增长；使用 Gated Sliding-Window Attention (G-SWA) 将全局信息与局部信息融合，保持高准确性。

Result: 实验表明，PLT 在保持深层循环模型的准确性的同时，几乎不增加与标准Transformer相比的推理延迟和内存成本。

Conclusion: PLT 提供一种可实用的深层循环模型推理方案，通过跨循环并行、KV 缓存共享与门控滑动窗口注意力实现低延迟和低内存开销的高效推理，同时保持高准确性。

Abstract: Large Language Models (LLMs) are powerful but often too slow and costly for
real-world use during inference. Looped transformers save on parameters by
reusing the same weights for multiple computational steps, or "loops." However,
this approach has a major flaw: the loops run one after another, causing
inference latency and memory requirements to increase with each added loop.
This makes them impractical for fast applications. To solve this problem, we
introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that
delivers the performance benefits of a deep, looped model but with the low
latency of a standard, non-looped model. PLT works using two key techniques.
First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by
computing different loops for different tokens at the same time, all within a
single pass. Second, to prevent memory costs from growing, we use an Efficient
Representation Enhancement strategy. This method shares the memory (KV cache)
from the first loop with all other loops. It then uses a Gated Sliding-Window
Attention (G-SWA) to combine this shared global information with local
information, maintaining high accuracy. Our experiments show that PLT achieves
the high accuracy of a traditional looped model but with almost no extra
latency or memory cost compared to a standard transformer.

</details>


### [14] [Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish](https://arxiv.org/abs/2510.24856)
*Lujun Li,Yewei Song,Lama Sleem,Yiqun Wang,Yangjie Xu,Cedric Lothritz,Niccolo Gentile,Radu State,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.CL

TL;DR: 提出了一种基于语法书的评估管线，用以系统评估语言模型的语法理解，卢森堡语作为案例。结果显示翻译表现与语法理解之间存在弱正相关；更大模型在语义层面表现较好，但在形态和句法方面仍薄弱，尤其在最小对比任务上表现欠佳；提升推理能力有望增强其语法理解。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理领域缺乏针对语法的评估协议，尤其在低资源语言中更为突出。需要明确大型语言模型是否真正掌握句法与语义之间的映射，并建立一个可推广的、系统化的语法评估框架。

Method: 提出Grammar Book Guided评估管线，基于语法教材/规则建立四阶段评估框架，并以卢森堡语为案例研究对象。评估内容包括翻译与语法理解的相关性分析、形态与句法测试、Minimal Pair任务，以及以推理能力为切入点的语法理解探针，力求实现对不同模型规模的可比性评估。

Result: 结果显示翻译性能与语法理解之间存在弱正相关性；大型模型在语义层面表现较强，但在形态和句法方面仍然薄弱，尤其在Minimal Pair任务上表现吃紧；具备较强推理能力的模型有望提升其语法理解水平。

Conclusion: 仅凭强翻译或语义能力难以获得深入的语法理解，需要加强对形态、句法的能力以及推理能力的培养。该评估管线具备广泛的可推广性，可用于低资源语言的语法评估与比较研究。

Abstract: Grammar refers to the system of rules that governs the structural
organization and the semantic relations among linguistic units such as
sentences, phrases, and words within a given language. In natural language
processing, there remains a notable scarcity of grammar focused evaluation
protocols, a gap that is even more pronounced for low-resource languages.
Moreover, the extent to which large language models genuinely comprehend
grammatical structure, especially the mapping between syntactic structures and
meanings, remains under debate. To investigate this issue, we propose a Grammar
Book Guided evaluation pipeline intended to provide a systematic and
generalizable framework for grammar evaluation consisting of four key stages,
and in this work we take Luxembourgish as a case study. The results show a weak
positive correlation between translation performance and grammatical
understanding, indicating that strong translations do not necessarily imply
deep grammatical competence. Larger models perform well overall due to their
semantic strength but remain weak in morphology and syntax, struggling
particularly with Minimal Pair tasks, while strong reasoning ability offers a
promising way to enhance their grammatical understanding.

</details>


### [15] [RiddleBench: A New Generative Reasoning Benchmark for LLMs](https://arxiv.org/abs/2510.24932)
*Deepon Halder,Alan Saji,Thanmay Jayakumar,Ratish Puduppully,Anoop Kunchukuttan,Raj Dabre*

Main category: cs.CL

TL;DR: RiddleBench 是一个用于评估大语言模型在整合逻辑推理、空间感知与约束满足等多方面能力的诊断性基准，包含 1,737 道英文谜题；现有顶尖模型在该基准上的平均表现约 60%，暴露出幻觉级联、自我确认偏见和对约束变化的脆弱性；该基准兼具诊断功能和促进鲁棒性改进的作用。


<details>
  <summary>Details</summary>
Motivation: 当前基准大多评估结构化、定量化的任务，难以衡量贴近人类智能的灵活多面推理能力（需要将逻辑推理、空间感知和约束求解等能力整合）。因此需要一个能够覆盖这些核心推理能力的评估工具来诊断和驱动模型改进。

Method: 提出 RiddleBench，包含 1,737 道英文谜题，设计用以探测核心推理能力；在前沿模型（如 Gemini 2.5 Pro、o3、Claude 4 Sonnet）上进行评估，分析错误模式，涉及幻觉级联、自我确认偏见以及约束顺序变化和无关信息引入对推理表现的影响。

Result: 模型在该基准上的准确率约为 60% 左右（具体为 60.30%、63.37%、63.16%），揭示出基础性弱点；存在幻觉 cascades、强自我确认偏见、推理对约束顺序敏感且易被无关信息干扰等问题。RiddleBench 既可用于诊断这些问题，也可作为推动开发更鲁棒、可靠的语言模型的资源。

Conclusion: RiddleBench 作为诊断工具与发展资源，帮助研究者识别并改进大语言模型在综合推理能力方面的弱点。

Abstract: Large Language Models have demonstrated strong performance on many
established reasoning benchmarks. However, these benchmarks primarily evaluate
structured skills like quantitative problem-solving, leaving a gap in assessing
flexible, multifaceted reasoning abilities that are central to human
intelligence. These abilities require integrating logical deduction with
spatial awareness and constraint satisfaction, which current evaluations do not
measure well. To address this, we introduce RiddleBench, a benchmark of 1,737
challenging puzzles in English designed to probe these core reasoning
capabilities. Evaluation of state-of-the-art models on RiddleBench shows
fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,
and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and
63.16%). Analysis further reveals deep failures, including hallucination
cascades (accepting flawed reasoning from other models) and poor
self-correction due to a strong self-confirmation bias. Their reasoning is also
fragile, with performance degrading significantly when constraints are
reordered or irrelevant information is introduced. RiddleBench functions as a
diagnostic tool for these issues and as a resource for guiding the development
of more robust and reliable language models.

</details>


### [16] [Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction](https://arxiv.org/abs/2510.24934)
*James A. Michaelov,Catherine Arnett*

Main category: cs.CL

TL;DR: 通过粒度化分析语言模型在不同句法情境中的错误，揭示其在训练不同阶段遵循词频和局部上下文等启发式规则，而非泛化语法规律的现象。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型通常输出语法正确的文本，但在特定情境下易出错。通过借鉴心理语言学的范式，对错误进行细粒度分析，旨在揭示模型在训练过程中的中间学习阶段、训练动态及所学习的一般化规律。

Method: 构建受控且分解的句法情境数据集，比较模型在不同条件下的表现，并在整个训练过程追踪这些条件的表现。采用心理语言学的实验范式来观察模型对不同语法结构、词频和局部上下文等因素的敏感性，进而识别训练中的阶段性行为模式。

Result: 分析发现存在若干训练阶段，模型行为更偏向某些启发式（如词频和局部上下文）而非通用语法规则；对不同条件的分解分析提供了对中间学习阶段和训练动力学的细化理解。

Conclusion: 将这种分解、阶段性分析的方法推广到语言模型分析中，可以成为理解中间学习阶段、训练动力学以及模型所获得的具体泛化能力的有力工具。

Abstract: Language models generally produce grammatical text, but they are more likely
to make errors in certain contexts. Drawing on paradigms from
psycholinguistics, we carry out a fine-grained analysis of those errors in
different syntactic contexts. We demonstrate that by disaggregating over the
conditions of carefully constructed datasets and comparing model performance on
each over the course of training, it is possible to better understand the
intermediate stages of grammatical learning in language models. Specifically,
we identify distinct phases of training where language model behavior aligns
with specific heuristics such as word frequency and local context rather than
generalized grammatical rules. We argue that taking this approach to analyzing
language model behavior more generally can serve as a powerful tool for
understanding the intermediate learning phases, overall training dynamics, and
the specific generalizations learned by language models.

</details>


### [17] [SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens](https://arxiv.org/abs/2510.24940)
*Yinhan He,Wendy Zheng,Yaochen Zhu,Zaiyi Zheng,Lin Su,Sriram Vasudevan,Qi Guo,Liangjie Hong,Jundong Li*

Main category: cs.CL

TL;DR: SemCoT提出了一个语义对齐的隐式CoT框架，通过对比学习的句子编码器来确保隐式推理与显式推理的语义保持，同时用知识蒸馏微调的轻量级语言模型来生成高效的隐式推理。该框架在不牺牲语义正确性的前提下，提升了CoT的生成速度与总体效率。


<details>
  <summary>Details</summary>
Motivation: 传统Chain-of-Thought推理过于冗长，难以大规模落地；隐式CoT尽管通过将推理嵌入隐含向量来加速，但容易破坏隐式与显式推理之间的语义对齐，且忽视了单个隐式推理token的生成成本。因此，需要在推理速度和语义一致性之间取得权衡。

Method: 提出两大组件：(1) 通过对比学习训练的句子变换器，用于评估并维持隐式与显式推理之间的语义对齐，以约束隐式推理的优化；(2) 通过知识蒸馏微调一个轻量级语言模型生成隐式推理，受句子变换器引导实现语义对齐并优化准确性；并在训练阶段将显式推理映射到隐式推理，在推理阶段利用高效的隐式推理。强调联合优化生成速度与语义一致性。

Result: 大量实验表明SemCoT在效率与效果上均优于现有的隐式CoT方法，显著提升推理速度的同时保持或提升推理质量；代码公开可获得。

Conclusion: SemCoT首次将生成速度和语义对齐两者结合，在CoT推理中实现更高效的解决方案，为高效、语义保真度高的推理提供新范式。

Abstract: The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment
in efficiency-critical applications. Recently, implicit CoT approaches have
emerged, which encode reasoning steps within LLM's hidden embeddings (termed
``implicit reasoning'') rather than explicit tokens. This approach accelerates
CoT by reducing the reasoning length and bypassing some LLM components.
However, existing implicit CoT methods face two significant challenges: (1)
they fail to preserve the semantic alignment between the implicit reasoning
(when transformed to natural language) and the ground-truth reasoning,
resulting in a significant CoT performance degradation, and (2) they focus on
reducing the length of the implicit reasoning; however, they neglect the
considerable time cost for an LLM to generate one individual implicit reasoning
token. To tackle these challenges, we propose a novel semantically-aligned
implicit CoT framework termed SemCoT. In particular, for the first challenge,
we design a contrastively trained sentence transformer that evaluates semantic
alignment between implicit and explicit reasoning, which is used to enforce
semantic preservation during implicit reasoning optimization. To address the
second challenge, we introduce an efficient implicit reasoning generator by
finetuning a lightweight language model using knowledge distillation. This
generator is guided by our sentence transformer to distill ground-truth
reasoning into semantically aligned implicit reasoning, while also optimizing
for accuracy. SemCoT is the first approach that enhances CoT efficiency by
jointly optimizing token-level generation speed and preserving semantic
alignment with ground-truth reasoning. Extensive experiments demonstrate the
superior performance of SemCoT compared to state-of-the-art methods in both
efficiency and effectiveness. Our code can be found at
https://github.com/YinhanHe123/SemCoT/.

</details>


### [18] [Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale](https://arxiv.org/abs/2510.24963)
*James A. Michaelov,Roger P. Levy,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 语言模型在预训练过程中的行为呈现跨架构、数据集与规模的一致性模式；词层行为的方差可被三个简单启发式变量解释：词的 unigram 频率、n-gram 概率，以及词与上下文的语义相似性；训练过程中出现稳定的行为阶段，模型对高阶 n-gram 的拟合逐渐增强；暗示学习轨迹具有普适性。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型的学习动力学是否对模型结构、数据源和规模不敏感，以及能否用少量简单的统计量解释大部分行为。

Method: 分析约1400个检查点，覆盖多种架构（Transformer、Mamba、RWKV）和数据集（OpenWebText、The Pile），使用约11万词元级别的英文文本，评估单词级预测行为及其方差的解释因素。

Result: 在单词级行为方差中，高达约98%可由三个启发式解释：单词的 unigram 频率、n-gram 概率，以及与上下文的语义相似性。还观察到在所有模型中存在一致的行为阶段，随着训练推进，预测的单词概率对高阶 n-gram 的过拟合增强。

Conclusion: 神经语言模型的学习可能遵循类似的普遍轨迹，与具体模型细节无关；这为理解预训练动态和用简单统计预测模型行为提供线索。

Abstract: We show that across architecture (Transformer vs. Mamba vs. RWKV), training
dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12
billion parameters), autoregressive language models exhibit highly consistent
patterns of change in their behavior over the course of pretraining. Based on
our analysis of over 1,400 language model checkpoints on over 110,000 tokens of
English, we find that up to 98% of the variance in language model behavior at
the word level can be explained by three simple heuristics: the unigram
probability (frequency) of a given word, the $n$-gram probability of the word,
and the semantic similarity between the word and its context. Furthermore, we
see consistent behavioral phases in all language models, with their predicted
probabilities for words overfitting to those words' $n$-gram probabilities for
increasing $n$ over the course of training. Taken together, these results
suggest that learning in neural language models may follow a similar trajectory
irrespective of model details.

</details>


### [19] [Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers](https://arxiv.org/abs/2510.25013)
*Rabin Adhikari*

Main category: cs.CL

TL;DR: 用符号化的 IOI 任务训练的极简注意力模型显示出可解释的内部电路：单层两头注意力可实现完美 IOI 解析，残差流分解、谱分析和嵌入干预揭示了加法与对比子电路；两层单头模型通过跨层查询-值交互实现类似效果。


<details>
  <summary>Details</summary>
Motivation: 研究在大型语言模型训练中，究竟哪些最小机制对 IOI 等推理任务是必需的，以及如何用受控测试床来探究变换器推理的计算基础。

Method: 从零开始训练小型仅含注意力的变换器，在符号化 IOI 任务上学习；通过残差流分解、谱分析、嵌入干预等方法对内部机制进行分析；另外构建两层单头模型，观察通过查询-值交互在不同层之间组合信息以实现任务。

Result: 一个单层、两头注意力的模型在没有 MLP 和归一化的情况下获得完美 IOI 精度；通过分析发现两头分别形成加法子电路和对比子电路，共同实现 IOI 解析；两层单头模型通过跨层的查询-值交互实现了相似的性能。

Conclusion: 证明了任务特定训练能诱导出高度可解释、极简的电路，为研究变换器推理的计算基础提供了受控测试床。

Abstract: Mechanistic interpretability aims to reverse-engineer large language models
(LLMs) into human-understandable computational circuits. However, the
complexity of pretrained models often obscures the minimal mechanisms required
for specific reasoning tasks. In this work, we train small, attention-only
transformers from scratch on a symbolic version of the Indirect Object
Identification (IOI) task -- a benchmark for studying coreference -- like
reasoning in transformers. Surprisingly, a single-layer model with only two
attention heads achieves perfect IOI accuracy, despite lacking MLPs and
normalization layers. Through residual stream decomposition, spectral analysis,
and embedding interventions, we find that the two heads specialize into
additive and contrastive subcircuits that jointly implement IOI resolution.
Furthermore, we show that a two-layer, one-head model achieves similar
performance by composing information across layers through query-value
interactions. These results demonstrate that task-specific training induces
highly interpretable, minimal circuits, offering a controlled testbed for
probing the computational foundations of transformer reasoning.

</details>


### [20] [Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech](https://arxiv.org/abs/2510.25054)
*Pedro Corrêa,João Lima,Victor Moreno,Paula Dornhofer Paro Costa*

Main category: cs.CL

TL;DR: 四种SLM在情感识别任务中对文本信号依赖大于对语音情感信号，文本表示主导模型决策，作者还发布了EMIS数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 评估SLM在跨模态集成中的泛化能力，检验其内部表示是否真正融合文本与音频信息，尤其在情感不一致时的表现。

Method: 在情绪不一致的语音样本上，使用四种SLM对情感识别任务进行评估；通过对比文本和声学线索在模型中的作用，分析内部表示的依赖性；发布EMIS数据集和代码。

Result: SLMs主要利用文本语义，而非语音情感信息来执行情感识别，声学表示对决策影响较小，文本信号主导。

Conclusion: SLM在情感识别任务中对文本的依赖高于对音频的依赖，揭示当前多模态模型在音频-文本融合方面的局限性；需要加强对声学信息的整合以及对跨模态表示的研究，作者同时提供EMIS数据集和代码以促进研究。

Abstract: Advancements in spoken language processing have driven the development of
spoken language models (SLMs), designed to achieve universal audio
understanding by jointly learning text and audio representations for a wide
range of tasks. Although promising results have been achieved, there is growing
discussion regarding these models' generalization capabilities and the extent
to which they truly integrate audio and text modalities in their internal
representations. In this work, we evaluate four SLMs on the task of speech
emotion recognition using a dataset of emotionally incongruent speech samples,
a condition under which the semantic content of the spoken utterance conveys
one emotion while speech expressiveness conveys another. Our results indicate
that SLMs rely predominantly on textual semantics rather than speech emotion to
perform the task, indicating that text-related representations largely dominate
over acoustic representations. We release both the code and the Emotionally
Incongruent Synthetic Speech dataset (EMIS) to the community.

</details>


### [21] [GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models](https://arxiv.org/abs/2510.25055)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 研究探讨大语言模型在生物医学文献中识别显性与隐性知识空缺的能力，提出TABI推理框架，并在不同模型和设置下评估，显示LLMs对两类空缺均有鲁棒的识别能力，同时指出实现中的挑战与部署要点。


<details>
  <summary>Details</summary>
Motivation: 科学进步依赖于对未知的清晰表达与发现。现有工作多聚焦显性空缺，本文扩展到隐性空缺的推断，旨在帮助研究规划、决策与资金分配。

Method: 在近1500篇文献、4个数据集上进行两轮实验，包含人工标注的生物医学文章。比较闭源权重模型（OpenAI）与开源模型（Llama、Gemma 2），在段落级和全文级设置下评估。提出TABI（Toulmin-Abductive Bucketed Inference），用于结构化推理并对推断结论候选进行分桶验证。

Result: LLMs在识别显性和隐性知识空缺方面表现稳健，开放与闭源模型均有效，且更大型模型性能更好。结果显示该能力可用于早期研究形成、政策与资助决策的支持，同时揭示若干失败模式，需结合领域自适应、人工监督与跨模型基准评估。

Conclusion: LLMs具备系统性发现候选知识空缺的潜力，可辅助研究起步与决策；但需关注领域适配、人工验证与多模态基线评估，以及在实际部署中的鲁棒性与伦理合规。

Abstract: Scientific progress is driven by the deliberate articulation of what remains
unknown. This study investigates the ability of large language models (LLMs) to
identify research knowledge gaps in the biomedical literature. We define two
categories of knowledge gaps: explicit gaps, clear declarations of missing
knowledge; and implicit gaps, context-inferred missing knowledge. While prior
work has focused mainly on explicit gap detection, we extend this line of
research by addressing the novel task of inferring implicit gaps. We conducted
two experiments on almost 1500 documents across four datasets, including a
manually annotated corpus of biomedical articles. We benchmarked both
closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)
under paragraph-level and full-paper settings. To address the reasoning of
implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive
Bucketed Inference scheme that structures reasoning and buckets inferred
conclusion candidates for validation. Our results highlight the robust
capability of LLMs in identifying both explicit and implicit knowledge gaps.
This is true for both open- and closed-weight models, with larger variants
often performing better. This suggests a strong ability of LLMs for
systematically identifying candidate knowledge gaps, which can support
early-stage research formulation, policymakers, and funding decisions. We also
report observed failure modes and outline directions for robust deployment,
including domain adaptation, human-in-the-loop verification, and benchmarking
across open- and closed-weight models.

</details>


### [22] [Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?](https://arxiv.org/abs/2510.25064)
*Seonjeong Hwang,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: LLMs can estimate RC item cognitive complexity on two dimensions (Evidence Scope, Transformation Level), enabling pre-administration difficulty analysis; however, they sometimes misidentify the reasoning features behind correct answers, revealing a metacognitive gap.


<details>
  <summary>Details</summary>
Motivation: Address the reliance on human annotation for cognitive features in RC items by providing scalable, automatic estimates of cognitive burden.

Method: Evaluate LLMs on RC items by scoring two cognitive dimensions—Evidence Scope and Transformation Level—that reflect reasoning burden; analyze whether LLM outputs align with human judgments and examine whether correct answers correspond to correct identification of underlying reasoning features.

Result: LLMs approximate the cognitive complexity of RC items and show potential for prior difficulty analysis; there is a gap between LLMs' reasoning ability and their metacognitive awareness, as correct answers can occur without accurate attribution of the underlying reasoning features.

Conclusion: LLMs hold promise as tools for pre-administration difficulty analysis of RC items, but limitations in metacognitive attribution need to be addressed to improve interpretability and reliability.

Abstract: Estimating the cognitive complexity of reading comprehension (RC) items is
crucial for assessing item difficulty before it is administered to learners.
Unlike syntactic and semantic features, such as passage length or semantic
similarity between options, cognitive features that arise during answer
reasoning are not readily extractable using existing NLP tools and have
traditionally relied on human annotation. In this study, we examine whether
large language models (LLMs) can estimate the cognitive complexity of RC items
by focusing on two dimensions-Evidence Scope and Transformation Level-that
indicate the degree of cognitive burden involved in reasoning about the answer.
Our experimental results demonstrate that LLMs can approximate the cognitive
complexity of items, indicating their potential as tools for prior difficulty
analysis. Further analysis reveals a gap between LLMs' reasoning ability and
their metacognitive awareness: even when they produce correct answers, they
sometimes fail to correctly identify the features underlying their own
reasoning process.

</details>


### [23] [TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors](https://arxiv.org/abs/2510.25069)
*Gabin Taibi,Lucia Gomez*

Main category: cs.CL

TL;DR: TOPol is a semi-unsupervised framework that reconstructs and interprets multidimensional narrative polarity fields across contextual boundaries, using transformer embeddings, neighbor-tuned UMAP, and Leiden clustering to produce directional polarity vectors that summarize regime-shift semantics.


<details>
  <summary>Details</summary>
Motivation: Traditional sentiment is unidimensional; language polarity is multidimensional and context-dependent. There is a need for interpretable, context-sensitive measures that can assess boundary quality and guide human-in-the-loop refinement.

Method: Embed documents with a transformer-based LLM, project with neighbor-tuned UMAP, segment topics via Leiden partitioning, and for a boundary between regimes A and B compute vectors between topic-boundary centroids to form a polarity field. Interpret vectors by comparing their extreme points and generating contrastive labels; perform robustness analyses, showing CB definitions as the main driver; evaluate on two corpora (central bank speeches and Amazon reviews).

Result: TOPol consistently captures both affective and non-affective polarity shifts, yielding a scalable, generalizable, and interpretable framework that can assess polarity changes and guide CB refinement across discourse contexts.

Conclusion: TOPol provides a robust, interpretable, and scalable method for context-sensitive, multidimensional discourse analysis, with applicability to political-economic texts and consumer reviews, and with CB definitions as the primary tunable parameter.

Abstract: Traditional approaches to semantic polarity in computational linguistics
treat sentiment as a unidimensional scale, overlooking the multidimensional
structure of language. This work introduces TOPol (Topic-Orientation POLarity),
a semi-unsupervised framework for reconstructing and interpreting
multidimensional narrative polarity fields under human-on-the-loop (HoTL)
defined contextual boundaries (CBs). The framework embeds documents using a
transformer-based large language model (tLLM), applies neighbor-tuned UMAP
projection, and segments topics via Leiden partitioning. Given a CB between
discourse regimes A and B, TOPol computes directional vectors between
corresponding topic-boundary centroids, yielding a polarity field that
quantifies fine-grained semantic displacement during regime shifts. This
vectorial representation enables assessing CB quality and detecting polarity
changes, guiding HoTL CB refinement. To interpret identified polarity vectors,
the tLLM compares their extreme points and produces contrastive labels with
estimated coverage. Robustness analyses show that only CB definitions (the main
HoTL-tunable parameter) significantly affect results, confirming methodological
stability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches
around a macroeconomic breakpoint, capturing non-affective semantic shifts, and
(ii) Amazon product reviews across rating strata, where affective polarity
aligns with NRC valence. Results demonstrate that TOPol consistently captures
both affective and non-affective polarity transitions, providing a scalable,
generalizable, and interpretable framework for context-sensitive
multidimensional discourse analysis.

</details>


### [24] [BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs](https://arxiv.org/abs/2510.25087)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 本研究评估生成式大语言模型（LLMs）在生物医学文本中的指称解析，与 SpanBERT 进行对比。通过在 CRAFT 语料上进行四种提示实验，并引入领域知识（缩写、实体字典、局部/上下文信息）来提升性能。结果显示：LLMs 在表面层面的指称解析能力较强，结合领域对齐提示后表现更好，但在长距离依赖和歧义情况下仍易受限。LLaMA 8B/17B 在实体增强提示下具备较高的精确度与F1，提示了轻量化提示工程在生物医学NLP中的潜力。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的指称解析面临术语复杂、变体多、跨距离依赖长等挑战。探究生成式LLMs在该任务中的可行性与边界，以及与传统判别式方法（SpanBERT）的比较。

Method: 以 CRAFT 语料为基准，设计四种提示实验，变化包括局部信息与上下文增强、领域相关线索（缩写、实体字典）的引入。将生成式LLMs 与 SpanBERT 作为对比，评估精确度、F1 等指标。

Result: LLMs 在表面级指称解析上表现强劲，且在带有领域对齐提示时表现更好。但在处理长距离上下文和歧义时仍受限。LLaMA 8B 和 17B 在实体增强提示下获得较高的精确度与F1，显示出通过提示工程提升LLM在生物医学NLP中的潜力。

Conclusion: 领域知识引导的提示能够提升生成式LLMs 在生物医学指称解析中的效用；尽管与最先进的判别式方法相比仍有差距，轻量化的提示设计对实际应用有显著价值。未来可在增强上下文理解、改进领域知识注入以及更好处理长距离依赖方面继续优化。

Abstract: Coreference resolution in biomedical texts presents unique challenges due to
complex domain-specific terminology, high ambiguity in mention forms, and
long-distance dependencies between coreferring expressions. In this work, we
present a comprehensive evaluation of generative large language models (LLMs)
for coreference resolution in the biomedical domain. Using the CRAFT corpus as
our benchmark, we assess the LLMs' performance with four prompting experiments
that vary in their use of local, contextual enrichment, and domain-specific
cues such as abbreviations and entity dictionaries. We benchmark these
approaches against a discriminative span-based encoder, SpanBERT, to compare
the efficacy of generative versus discriminative methods. Our results
demonstrate that while LLMs exhibit strong surface-level coreference
capabilities, especially when supplemented with domain-grounding prompts, their
performance remains sensitive to long-range context and mentions ambiguity.
Notably, the LLaMA 8B and 17B models show superior precision and F1 scores
under entity-augmented prompting, highlighting the potential of lightweight
prompt engineering for enhancing LLM utility in biomedical NLP tasks.

</details>


### [25] [Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation](https://arxiv.org/abs/2510.25116)
*Idriss Nguepi Nguefack,Mara Finkelstein,Toadoum Sari Sakayo*

Main category: cs.CL

TL;DR: 在 Lingala 的低资源机器翻译任务中，采用多语言预训练与单语+平行数据混合，扩展自 Reid and Artetxe (2021) 的框架，显著提升翻译质量并实现可复现性。


<details>
  <summary>Details</summary>
Motivation: 填补 Lingala 与其他低资源语言在机器翻译中的性能差距，将面向高资源语言的预训练策略推广到低资源场景，推动包容性 NLP 的发展。

Method: 在以 Reid and Artetxe (2021) 框架为基础的前提下，研究多语言预训练、单语与平行数据的融合对 Lingala 翻译质量的影响，并以 Afrikaans、Swahili、Zulu 等语言作辅助，核心目标语言为 Lingala。通过一系列系统实验评估不同预训练策略。

Result: 多语言预训练结合单语和平行数据的方案显著提升翻译质量，相较基线具有可观改进。研究同时指出部分数据可能因公开性变更而不可获取，但代码与数据集公开以促进可复现性。

Conclusion: 证实了面向低资源语言的前训练策略有效性，有助于缩小高资源与低资源语言之间的性能差距，推动更包容、准确的 NLP 模型的发展；研究成果有助于相关社区的可及性与代表性提升。

Abstract: This research article examines the effectiveness of various pretraining
strategies for developing machine translation models tailored to low-resource
languages. Although this work considers several low-resource languages,
including Afrikaans, Swahili, and Zulu, the translation model is specifically
developed for Lingala, an under-resourced African language, building upon the
pretraining approach introduced by Reid and Artetxe (2021), originally designed
for high-resource languages. Through a series of comprehensive experiments, we
explore different pretraining methodologies, including the integration of
multiple languages and the use of both monolingual and parallel data during the
pretraining phase. Our findings indicate that pretraining on multiple languages
and leveraging both monolingual and parallel data significantly enhance
translation quality. This study offers valuable insights into effective
pretraining strategies for low-resource machine translation, helping to bridge
the performance gap between high-resource and low-resource languages. The
results contribute to the broader goal of developing more inclusive and
accurate NLP models for marginalized communities and underrepresented
populations. The code and datasets used in this study are publicly available to
facilitate further research and ensure reproducibility, with the exception of
certain data that may no longer be accessible due to changes in public
availability.

</details>


### [26] [A Survey on Unlearning in Large Language Models](https://arxiv.org/abs/2510.25117)
*Ruichen Qiu,Jiajun Tan,Jiayue Pu,Honglin Wang,Xiao-Shan Gao,Fei Sun*

Main category: cs.CL

TL;DR: 对180余篇自2021年以来关于大规模生成模型的“忘却”研究的系统综述，提出了 unlearning 的方法与评估的新分类，并整理数据集与指标，给出实际应用指南、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在大规模语料中 memorization 的风险（包括敏感信息、受版权保护材料等），以及“被遗忘的权利”等法律伦理需求，需在不显著损害能力的前提下实现有选择的知识移除。

Method: 对相关文献进行系统综述，聚焦大规模生成模型，提出训练时、训练后、推理时三类 unlearning 方法的分类及对应评估框架，整理现有数据集与评价指标，并进行优缺点分析，提出实践性指南与未来研究方向。

Result: 提出关于 unlearning 的新型分类体系及评估框架，汇编并分析了现有数据集与指标，提供研究社区的实用指南，并指出若干关键挑战与未来研究方向，为安全可靠的 LLM 发展提供全面参考。

Conclusion: 本综述对LLM unlearning 的研究现状给出系统、全面的图景，证明了按训练阶段划分的方法学有效性，强调评估体系的重要性，并指明未来在数据隐私、法规合规、鲁棒性与可实现性方面的研究方向。

Abstract: The advancement of Large Language Models (LLMs) has revolutionized natural
language processing, yet their training on massive corpora poses significant
risks, including the memorization of sensitive personal data, copyrighted
material, and knowledge that could facilitate malicious activities. To mitigate
these issues and align with legal and ethical standards such as the "right to
be forgotten", machine unlearning has emerged as a critical technique to
selectively erase specific knowledge from LLMs without compromising their
overall performance. This survey provides a systematic review of over 180
papers on LLM unlearning published since 2021, focusing exclusively on
large-scale generative models. Distinct from prior surveys, we introduce novel
taxonomies for both unlearning methods and evaluations. We clearly categorize
methods into training-time, post-training, and inference-time based on the
training stage at which unlearning is applied. For evaluations, we not only
systematically compile existing datasets and metrics but also critically
analyze their advantages, disadvantages, and applicability, providing practical
guidance to the research community. In addition, we discuss key challenges and
promising future research directions. Our comprehensive overview aims to inform
and guide the ongoing development of secure and reliable LLMs.

</details>


### [27] [Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR](https://arxiv.org/abs/2510.25150)
*Shreyas Gopal,Ashutosh Anshul,Haoyang Li,Yue Heng Yeo,Hexin Liu,Eng Siong Chng*

Main category: cs.CL

TL;DR: 提出在离散音频表示中将语义内容与背景噪声解耦，学习噪声量化残差并用轻量分类器监督，保持 Whisper 固定权重，从而提升鲁棒性和 ASR 性能。


<details>
  <summary>Details</summary>
Motivation: 离散表示在解释性和与大语言模型的兼容性方面具有潜力，但在嘈杂现实环境下往往对噪声鲁棒性不足，需要分离干净语音与噪声成分以提升对齐与识别。

Method: 端到端模型：将干净语音编码为码本令牌；将可解释的噪声向量作为量化残差提取，并通过轻量分类器进行监督；保持 Whisper 权重不更新，使模型在现有嵌入上工作；在 seen/unseen声学条件下验证泛化。

Result: 在 VBDemand 测试集上，与 Whisper 相比错误率下降82%；相较基线方法下降约35%；所学习的语音令牌对噪声具有高不变性，干净/嘈杂语音与文本对齐更好，模型对 seen/unseen声学条件的泛化能力良好。

Conclusion: 该方法证明在离散音频表示中通过噪声残差实现语义与噪声的有效分离，提升鲁棒性与可解释性，且具备对现实世界条件的良好泛化潜力。

Abstract: Discrete audio representations are gaining traction in speech modeling due to
their interpretability and compatibility with large language models, but are
not always optimized for noisy or real-world environments. Building on existing
works that quantize Whisper embeddings for speech-to-unit modeling, we propose
disentangling semantic speech content from background noise in the latent
space. Our end-to-end model separates clean speech in the form of codebook
tokens, while extracting interpretable noise vectors as quantization residue
which are supervised via a lightweight classifier. We show that our approach
improves alignment between clean/noisy speech and text, producing speech tokens
that display a high degree of noiseinvariance, and improves ASR performance.
Keeping Whisper frozen, we show an 82% reduction in error rate compared to
Whisper, and 35% improvement over baseline methods on the VBDemand test set.
Further analyses show that the learned token space generalizes well to both
seen and unseen acoustic conditions.

</details>


### [28] [Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction](https://arxiv.org/abs/2510.25187)
*Ritesh Sunil Chavan,Jack Mostow*

Main category: cs.CL

TL;DR: LLMs show language-resource disparities: strong English performance, weaker performance on Swahili/Hausa; CoT helps limited; not universal solution; framework to diagnose cross-lingual NSP.


<details>
  <summary>Details</summary>
Motivation: Investigate whether LLM success reflects data abundance bias toward English and how models perform in low-resource languages; test cross-lingual NSP across languages.

Method: Create a 10,000-question NSP benchmark in English, Swahili, Hausa; evaluate GPT-4 Turbo, Gemini 1.5 Flash, LLaMA 3 70B; apply Chain-of-Thought prompting; compare performance and analyze when CoT helps or hinders.

Result: English high accuracy; Swahili moderate; Hausa low; LLaMA 3 struggles; CoT boosts LLaMA 3 but harms GPT-4 and Gemini in cross-lingual setting; CoT effectiveness depends on baseline capability and context.

Conclusion: CoT is not a universal fix for cross-lingual NSP; the proposed framework helps identify weaknesses and when CoT is advantageous; highlights resource disparities and model behavior.

Abstract: While large language models are trained on massive datasets, this data is
heavily skewed towards English. Does their impressive performance reflect
genuine ability or just this data advantage? To find out, we tested them in a
setting where they could not rely on data abundance: low-resource languages.
Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction
(NSP) as a test, we created a large-scale benchmark with 10,000 questions each
for English (a high-resource language), Swahili (medium-resource), and Hausa
(low-resource). We then tested several top models, including GPT-4 Turbo,
Gemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The
results painted a clear picture of how levels of language resources impact
outcomes. While all models excelled in English, their accuracy dropped in
Swahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story
became even more interesting when we introduced Chain-of-Thought (CoT)
prompting. For the struggling LLaMA 3, CoT acted as a helpful guide,
significantly boosting its accuracy. However, for the more capable GPT-4 and
Gemini, the same technique often backfired, leading to a kind of "overthinking"
that hurt their results in the cross-lingual context. This reveals that
Chain-of-Thought is not a universal solution; its effectiveness depends heavily
on the model's baseline capability and the specific context of the task. Our
framework pinpoints LLM weaknesses, highlights when CoT helps or hinders
cross-lingual NSP performance, and factors influencing their decisions.

</details>


### [29] [Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA](https://arxiv.org/abs/2510.25273)
*Sandipan Majhi,Paheli Bhattacharya*

Main category: cs.CL

TL;DR: 提出一种多阶段微调策略，通过原始与合成数据相结合，将轻量级语言模型适配到印地语旅游领域的问答任务，且大模型用于生成合成数据，小模型用于适应之，具备可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中的数据稀缺和通用大模型知识不足的挑战，尤其在领域专属问答场景。

Method: 多阶段微调：1) 使用原始数据进行初步微调；2) 用大型LLMs（LLaMA-70B、Phi-14B）生成合成问答对并与原始数据合并；3) 评估多种训练策略及其对领域泛化的影响。

Result: 结论：大型模型高效生成合成数据；小型模型能够有效适应这些数据，显著提升低资源领域的问答能力与域泛化。

Conclusion: 该方法为低资源、领域特定问答提供可扩展路径，表明在数据有限时通过合成数据和分阶段微调可以实现有效的域适配。

Abstract: Domain-specific question answering in low-resource languages faces two key
challenges: scarcity of annotated datasets and limited domain knowledge in
general-purpose language models. In this work, we present a multi-stage
finetuning strategy to adapt lightweight language models to the Hindi tourism
domain by leveraging both original and synthetic training data. Synthetic
question-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and
used to augment the limited original dataset. We explore several training
methodologies and analyse their impact on domain generalisation. Our results
demonstrate that large models can efficiently generate synthetic data, while
small models can effectively adapt to it, offering a scalable pathway for
low-resource, domain-specific QA.

</details>


### [30] [Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student](https://arxiv.org/abs/2510.25303)
*Soumyadeep Jana,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: PEKD 提升参数高效微调在多模态讽刺检测中的性能：通过从大规模讽刺数据训练的专家模型进行蒸馏，并引入基于教师信心的熵感知门控，显著提升少样本场景下的表现，且框架模块化、可适配多种模型与任务。


<details>
  <summary>Details</summary>
Motivation: 在低资源环境下，少量标注数据导致图文讽刺检测困难，尽管 PEFT（Adapters/LoRA/Prompt）能减少过拟合，但监督不足仍限制性能。因此需要利用来自大规模讽刺数据的知识蒸馏来提升 PEFT。

Method: 提出统一框架 PEKD，通过用在大规模讽刺数据上训练的教师模型对 PEFT 模型进行蒸馏，并引入熵感知门控，根据教师的信心动态调整蒸馏强度，从而抑制不可靠信号。

Result: 在两个公开数据集上实验，PEKD 使得基于 PEFT 的方法超越了先前的参数高效方法和大规模多模态模型，尤其在少样本场景展现出强劲性能。

Conclusion: PEKD 的模块化设计可广泛适配多种多模态模型与任务，显著提升低资源讽刺检测的效果，提供了一个可扩展的蒸馏框架。

Abstract: Multimodal sarcasm detection is challenging, especially in low-resource
settings where subtle image-text contradictions are hard to learn due to scarce
annotated data, which hinders the model's performance. Parameter-efficient
fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce
overfitting but struggle to reach optimal performance due to limited
supervision from few-shot data. We propose PEKD, a unified framework that
enhances PEFT methods via distillation from an expert model trained on
large-scale sarcasm data, which acts as the teacher. To mitigate unreliable
signals from the teacher, we introduce an entropy-aware gating mechanism that
dynamically adjusts the distillation strength based on teacher confidence.
Experiments on two public datasets demonstrate that our PEKD framework enables
PEFT methods to outperform both prior parameter-efficient approaches and large
multimodal models, achieving strong results in the few-shot scenario. The
framework is modular and adaptable to a wide range of multimodal models and
tasks.

</details>


### [31] [Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning](https://arxiv.org/abs/2510.25310)
*Senjie Jin,Lu Chen,Zhiheng Xi,Yuhui Wang,Sirui Song,Yuhao Zhou,Xinbo Zhang,Peng Sun,Hong Lu,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出 Parrot 训练流水线，通过三个子任务、混合训练策略和一个 N-CoT 辅助奖励，实现 N-CoT 与 P-CoT 的互相强化，显著提升两者性能，尤其是 N-CoT。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究通常只单向提升其中一个范式的问题，深入分析两种范式的错误类型，以实现协同提升和资源高效的推理能力。

Method: 1) 三个目标导向的子任务，串联 P-CoT 与 N-CoT 生成。2) 子任务混合训练策略，促进自然语言语义的迁移。3) 将 N-CoT 转换为辅助奖励，缓解 P-CoT 优化中的稀疏奖励问题。

Result: 在 MathQA 上，使用 Parrot SFT，N-CoT 在 LLaMA2 与 CodeLLaMA 上分别获得 +21.87 与 +21.48 的提升，相较 RL 基线，且对 N-CoT 提升尤为显著。

Conclusion: Parrot 能有效提升 N-CoT 与 P-CoT 的性能，实现两者的互利增强，提供了比基于 RL 的资源密集方法更高效的途径。

Abstract: Natural language chain-of-thought (N-CoT) and Program chain-of-thought
(P-CoT) have emerged as two primary paradigms for large language models (LLMs)
to solve mathematical reasoning problems. Current research typically endeavors
to achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced
P-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for
mutual enhancement and ultimately achieve simultaneous improvements. We conduct
a detailed analysis of the error types across two paradigms, based on which we
propose Parrot, a novel training pipeline for mathematical problems: 1) Three
target-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A
subtask hybrid training strategy to facilitate natural language semantic
transferability. 3) The converted N-CoT auxiliary reward is designed to
alleviate the sparse rewards in P-CoT optimization. Extensive experiments
demonstrate that Parrot significantly enhances both the performance of N-CoT
and P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of
LLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL
baseline, which is resource-intensive.

</details>


### [32] [Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments](https://arxiv.org/abs/2510.25356)
*Abhishek Purushothama,Junghyun Min,Brandon Waldon,Nathan Schneider*

Main category: cs.CL

TL;DR: 该论文论证：将大型语言模型用于法律解释会产生不稳定的判断，且与人类判断的相关性弱到中等，因而不宜过分依赖生成式AI来进行法律解释。


<details>
  <summary>Details</summary>
Motivation: 检验在美国司法体系中，法律文本的普通理解如何被模型解读，以及法律实务中引入LLMs的可行性与风险。

Method: 在英文文本上进行实证研究，通过改变问题表述来观察模型输出的稳定性；比较模型输出与人类判断的相关性，并在不同模型和问题变体间考察方差。

Result: 模型难以提供稳定的解释性判断；改变问法可导致结果大幅波动；与人类判断的相关性从弱到中等，且在模型和问题变体之间存在较大方差。

Conclusion: 基于当前状态，难以信任用生成式AI进行法律解释的结论；应谨慎对待将LLMs用于法律解释的实践，避免过度依赖其输出。

Abstract: Legal interpretation frequently involves assessing how a legal text, as
understood by an 'ordinary' speaker of the language, applies to the set of
facts characterizing a legal dispute in the U.S. judicial system. Recent
scholarship has proposed that legal practitioners add large language models
(LLMs) to their interpretive toolkit. This work offers an empirical argument
against LLM interpretation as recently practiced by legal scholars and federal
judges. Our investigation in English shows that models do not provide stable
interpretive judgments: varying the question format can lead the model to
wildly different conclusions. Moreover, the models show weak to moderate
correlation with human judgment, with large variance across model and question
variant, suggesting that it is dangerous to give much credence to the
conclusions produced by generative AI.

</details>


### [33] [CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs](https://arxiv.org/abs/2510.25364)
*Luca Capone,Alessandro Bondielli,Alessandro Lenci*

Main category: cs.CL

TL;DR: Small-scale language models can gain modest, consistent improvements from instruction tuning, with sequential curricula outperforming merged data in fine-tuning; zero-shot transfer is less reliable, indicating a trade-off between interaction-focused adaptation and broad generalization.


<details>
  <summary>Details</summary>
Motivation: Explore whether instruction tuning benefits small decoder-only LMs (100M and 140M params) and how dataset structure (merged vs sequential curricula) affects performance across fine-tuning and zero-shot tasks.

Method: Compare conversational and QA instruction-tuning datasets applied in merged versus sequential curricula on small decoder-only models (100M and 140M). Evaluate on fine-tuning benchmark SuperGLUE and zero-shot tasks (BLiMP, EWoK, WUGs, entity tracking, psycholinguistic correlations).

Result: Instruction tuning yields small but consistent gains in fine-tuning scenarios; sequential curricula outperform merged data. Improvements do not consistently transfer to zero-shot tasks, indicating a trade-off between interaction-focused adaptation and broad linguistic generalization.

Conclusion: Hybrid, curriculum-based instruction-tuning can enhance generalization under ecological training limits for small LMs, but gains are modest and task transfer is not uniform; a hybrid approach may optimize performance by balancing interaction-focused learning with broad linguistic abilities.

Abstract: This work investigates whether small-scale LMs can benefit from instruction
tuning. We compare conversational and question-answering instruction tuning
datasets, applied either in a merged or sequential curriculum, using
decoder-only models with 100M and 140M parameters. Evaluation spans both
fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and
psycholinguistic correlation) settings. Results show that instruction tuning
yields small but consistent gains in fine-tuning scenarios, with sequential
curricula outperforming merged data; however, improvements do not consistently
transfer to zero-shot tasks, suggesting a trade-off between interaction-focused
adaptation and broad linguistic generalization. These results highlight both
the potential and the constraints of adapting human-inspired learning
strategies to low-resource LMs, and point toward hybrid, curriculum-based
approaches for enhancing generalization under ecological training limits.

</details>


### [34] [Monitoring Transformative Technological Convergence Through LLM-Extracted Semantic Entity Triple Graphs](https://arxiv.org/abs/2510.25370)
*Alexander Sternfeld,Andrei Kucharavy,Dimitri Percia David,Alain Mermoud,Julian Jang-Jaccard,Nathan Monnet*

Main category: cs.CL

TL;DR: 提出基于LLMs的全文本分析管线，通过提取语义三元组并构建大规模技术关系图，结合noun stapling等聚合方法与图基指标来检测技术融合信号；在arXiv与USPTO数据上验证，可识别已知与新兴的转化性技术趋势，具备可扩展性与跨领域适用性。


<details>
  <summary>Details</summary>
Motivation: 在ICT等快速演变的领域，传统以专家为中心的方法难以跟上短循环、早期术语模糊的问题，需要数据驱动、可扩展的技术预测框架。

Method: 从非结构化文本中提取语义三元组，构建技术实体和关系的大规模图；提出noun stapling等同义/语义聚合策略；基于图的融合信号指标、多阶段筛选、领域关键词聚类以及对主题共现的时间趋势分析；在两大数据源上进行验证：arXiv（2017–2024）与USPTO专利（2018–2024）。

Result: 管线能识别已建立和新兴的技术融合模式，提供一个可扩展、跨领域的技术预测框架，基于全文本分析的证据显示出良好的可泛化性。

Conclusion: 数据驱动的全文本与图分析结合可用于科技趋势预测，适用于不同领域并具扩展潜力；未来可扩展至更多数据源，提升对早期非传统融合的检测能力。

Abstract: Forecasting transformative technologies remains a critical but challenging
task, particularly in fast-evolving domains such as Information and
Communication Technologies (ICTs). Traditional expert-based methods struggle to
keep pace with short innovation cycles and ambiguous early-stage terminology.
In this work, we propose a novel, data-driven pipeline to monitor the emergence
of transformative technologies by identifying patterns of technological
convergence.
  Our approach leverages advances in Large Language Models (LLMs) to extract
semantic triples from unstructured text and construct a large-scale graph of
technology-related entities and relations. We introduce a new method for
grouping semantically similar technology terms (noun stapling) and develop
graph-based metrics to detect convergence signals. The pipeline includes
multi-stage filtering, domain-specific keyword clustering, and a temporal trend
analysis of topic co-occurence.
  We validate our methodology on two complementary datasets: 278,625 arXiv
preprints (2017--2024) to capture early scientific signals, and 9,793 USPTO
patent applications (2018-2024) to track downstream commercial developments.
Our results demonstrate that the proposed pipeline can identify both
established and emerging convergence patterns, offering a scalable and
generalizable framework for technology forecasting grounded in full-text
analysis.

</details>


### [35] [Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media](https://arxiv.org/abs/2510.25413)
*Shakib Yazdani,Yasser Hamidullah,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: 通过对社交媒体视频进行自动化标注和筛选，基于视觉-语言模型（VLM）构建了首个面向手语数据集获取的自动化框架，扩展到八种手语并对德英手语等数据集进行评估，建立基线并验证模型对轻噪数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有手语数据集规模有限、缺乏多语覆盖且标注成本高，依赖专家标注与受控环境。VLMs 在评估与实时辅助领域表现突出，尚未充分用于手语数据集获取的自动化与扩展。

Method: 提出一个基于VLM的流水线，包含人脸可见性检测、手语动作识别、视频文本提取以及视频与文本的一致性判定的过滤、标注和校验步骤。应用于TikTok的八种手语视频以及已整理的德语手语YouTube-SL-25数据集，生成TikTok-SL-8。对两种现成的SLT模型在经过滤的数据集上进行基线评估。

Result: 在德语和美式手语上对经筛选数据训练的模型进行了基线评估，建立了较为稳健的基线并验证了对略带噪声的数据的鲁棒性，提供了可扩展的弱监督预训练数据来源。

Conclusion: 该框架实现了手语数据的可扩展采集与基于VLM的自动标注/筛选，便于从社交媒体获取数据，促进SLT的弱监督预训练与模型鲁棒性评估。

Abstract: Most existing sign language translation (SLT) datasets are limited in scale,
lack multilingual coverage, and are costly to curate due to their reliance on
expert annotation and controlled recording setup. Recently, Vision Language
Models (VLMs) have demonstrated strong capabilities as evaluators and real-time
assistants. Despite these advancements, their potential remains untapped in the
context of sign language dataset acquisition. To bridge this gap, we introduce
the first automated annotation and filtering framework that utilizes VLMs to
reduce reliance on manual effort while preserving data quality. Our method is
applied to TikTok videos across eight sign languages and to the already curated
YouTube-SL-25 dataset in German Sign Language for the purpose of additional
evaluation. Our VLM-based pipeline includes a face visibility detection, a sign
activity recognition, a text extraction from video content, and a judgment step
to validate alignment between video and text, implementing generic filtering,
annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we
assess the performance of two off-the-shelf SLT models on our filtered dataset
for German and American Sign Languages, with the goal of establishing baselines
and evaluating the robustness of recent models on automatically extracted,
slightly noisy data. Our work enables scalable, weakly supervised pretraining
for SLT and facilitates data acquisition from social media.

</details>


### [36] [Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction](https://arxiv.org/abs/2510.25426)
*Asutosh Hota,Jussi P. P. Jokinen*

Main category: cs.CL

TL;DR: 含蓄推理提示能提升LLM回应的相关性与质量，模型大小影响含蓄推断能力，语言学理论可用于提升人机交互对齐。


<details>
  <summary>Details</summary>
Motivation: 探究 implicature（含蓄含义）在语言与互动中的作用，以及通过上下文推断用户意图以改进人机对齐(HCI/HAI)。

Method: 比较不同规模的LLMs 在上下文驱动提示中的含蓄推断能力；设计含蓄(implicature-based)提示；评估回应的相关性与质量并收集用户偏好。

Result: 大型模型更接近人类对含蓄的推断；小模型在含蓄推断方面困难；含蓄提示显著提升回应相关性与质量，尤在小模型中更明显；67.6%的参与者偏好含蓄提示的回应。

Conclusion: 将语言学理论应用于HAI对齐，可使交互更自然、具上下文感知，推动对齐研究的实用化。

Abstract: The rapid advancement of Large Language Models (LLMs) is positioning language
at the core of human-computer interaction (HCI). We argue that advancing HCI
requires attention to the linguistic foundations of interaction, particularly
implicature (meaning conveyed beyond explicit statements through shared
context) which is essential for human-AI (HAI) alignment. This study examines
LLMs' ability to infer user intent embedded in context-driven prompts and
whether understanding implicature improves response generation. Results show
that larger models approximate human interpretations more closely, while
smaller models struggle with implicature inference. Furthermore,
implicature-based prompts significantly enhance the perceived relevance and
quality of responses across models, with notable gains in smaller models.
Overall, 67.6% of participants preferred responses with implicature-embedded
prompts to literal ones, highlighting a clear preference for contextually
nuanced communication. Our work contributes to understanding how linguistic
theory can be used to address the alignment problem by making HAI interaction
more natural and contextually grounded.

</details>


### [37] [RLMEval: Evaluating Research-Level Neural Theorem Proving](https://arxiv.org/abs/2510.25427)
*Auguste Poiroux,Antoine Bosselut,Viktor Kunčak*

Main category: cs.CL

TL;DR: 提出 RLMEval，面向研究级数学的神经定理证明与证明自动形式化的评估基准，基于 Lean 的真实项目，包含613个定理，6个项目，揭示现实任务与常规模型基准之间的差距，最佳通过率仅10.3%。


<details>
  <summary>Details</summary>
Motivation: 当前在精选基准上的高表现并不能转化到研究级和现实项目中，需要一个更具挑战性的评估集合来测量和推动模型在形式化数学中的推理与自动化能力。

Method: 构建并公开 RLMEval 评估集合，抽取自真实 Lean Blueprint 项目，聚焦神经定理证明与证明自动形式化，评估6个 Lean 项目中的613条定理，测试现有最先进模型在该集合上的表现。

Result: 评估结果显示最佳模型在 RLMEval 上的通过率仅为 10.3%，显著低于常规基准，揭示了现实场景对模型的挑战性和现有方法的局限。

Conclusion: RLMEval 提供一个新的更具挑战性的基准，旨在引导并加速形式化数学领域的自动推理研究，未来需要在模型能力、数据多样性和评估策略等方面进行改进。

Abstract: Despite impressive results on curated benchmarks, the practical impact of
large language models (LLMs) on research-level neural theorem proving and proof
autoformalization is still limited. We introduce RLMEval, an evaluation suite
for these tasks, focusing on research-level mathematics from real-world Lean
formalization projects. RLMEval targets the evaluation of neural theorem
proving and proof autoformalization on challenging research-level theorems by
leveraging real Lean Blueprint formalization projects. Our evaluation of
state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean
projects, reveals a significant gap: progress on existing benchmarks does not
readily translate to these more realistic settings, with the best model
achieving only a 10.3 % pass rate. RLMEval provides a new, challenging
benchmark designed to guide and accelerate progress in automated reasoning for
formal mathematics.

</details>


### [38] [Depth and Autonomy: A Framework for Evaluating LLM Applications in Social Science Research](https://arxiv.org/abs/2510.25432)
*Ali Sanaei,Ali Rajabzadeh*

Main category: cs.CL

TL;DR: 提出一个两维框架（interpretive depth 与 autonomy）来对定性研究中LLM的使用进行分类，并给出设计建议与实现路径。


<details>
  <summary>Details</summary>
Motivation: 解决在定性社会科学中使用大语言模型时的解释偏差、可靠性不足和可审计性差的问题，提供一个可操作的分类与设计指南。

Method: 基于对Web of Science中将LLMs作为工具使用的社会科学论文进行综述，提出分解任务、保持低自治性、在需要时提高解释深度的框架，并给出相应的实践建议。

Result: 给出一个基于两维的分类框架，及在各场景下的设计要点，强调以受监督的方式逐步提高解释深度、降低自治性，从而兼顾透明性与可靠性。

Conclusion: 该框架为研究者在定性研究中应用LLM提供可操作的路径，平衡模型能力与研究透明度，促进更安全、可审计的采用。

Abstract: Large language models (LLMs) are increasingly utilized by researchers across
a wide range of domains, and qualitative social science is no exception;
however, this adoption faces persistent challenges, including interpretive
bias, low reliability, and weak auditability. We introduce a framework that
situates LLM usage along two dimensions, interpretive depth and autonomy,
thereby offering a straightforward way to classify LLM applications in
qualitative research and to derive practical design recommendations. We present
the state of the literature with respect to these two dimensions, based on all
published social science papers available on Web of Science that use LLMs as a
tool and not strictly as the subject of study. Rather than granting models
expansive freedom, our approach encourages researchers to decompose tasks into
manageable segments, much as they would when delegating work to capable
undergraduate research assistants. By maintaining low levels of autonomy and
selectively increasing interpretive depth only where warranted and under
supervision, one can plausibly reap the benefits of LLMs while preserving
transparency and reliability.

</details>


### [39] [A Critical Study of Automatic Evaluation in Sign Language Translation](https://arxiv.org/abs/2510.25434)
*Shakib Yazdani,Yasser Hamidullah,Cristina España-Bonet,Eleftherios Avramidis,Josef van Genabith*

Main category: cs.CL

TL;DR: 该研究系统地分析文本基SLT评估指标的局限性，揭示LLM‑基评估在捕捉语义等价方面更具优势但可能对LLM改写存在偏差；BLEU对幻觉敏感但易过于苛刻；需发展多模态、超越文本的综合评估框架以实现对SLT输出的更全面评估。


<details>
  <summary>Details</summary>
Motivation: 现有的SLT评估多采用文本为主的指标（如BLEU、ROUGE、chrF、BLEURT），而SLT输出往往涉及手语的多模态信息。对应文本指标能否真实反映SLT质量尚不清晰，因此需要系统研究不同评估指标在受控条件下的一致性与鲁棒性，以及它们的偏差来源。

Method: 对六种文本基指标（BLEU、chrF、ROUGE、BLEURT）与两种LLM评估器（G-Eval、GEMBA）进行比较，置于三个受控情境：改写/paraphrase、输出幻觉、句长变化，评估它们的一致性、鲁棒性及偏见。

Result: 发现文本基的词汇重叠指标在语义等价方面存在局限；LLM‑基评估在捕捉语义等价方面优于传统指标，但对LLM生成的改写存在偏向；所有指标均能检测到幻觉，但BLEU对幻觉更敏感且易过于严格，而BLEURT和LLM评估在对微妙幻觉的容忍度上较高。

Conclusion: 应建立超越文本的多模态评估框架，以实现对SLT输出的更全面、健壮的质量评估；单纯文本指标存在固有局限，应结合多种指标，尤其是结合LLM‑基评估与多模态信号。

Abstract: Automatic evaluation metrics are crucial for advancing sign language
translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are
only text-based, and it remains unclear to what extent text-based metrics can
reliably capture the quality of SLT outputs. To address this gap, we
investigate the limitations of text-based SLT evaluation metrics by analyzing
six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one
hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA
zero-shot direct assessment on the other hand. Specifically, we assess the
consistency and robustness of these metrics under three controlled conditions:
paraphrasing, hallucinations in model outputs, and variations in sentence
length. Our analysis highlights the limitations of lexical overlap metrics and
demonstrates that while LLM-based evaluators better capture semantic
equivalence often missed by conventional metrics, they can also exhibit bias
toward LLM-paraphrased translations. Moreover, although all metrics are able to
detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and
LLM-based evaluators are comparatively lenient toward subtle cases. This
motivates the need for multimodal evaluation frameworks that extend beyond
text-based metrics to enable a more holistic assessment of SLT outputs.

</details>


### [40] [Fine-Tuned Language Models for Domain-Specific Summarization and Tagging](https://arxiv.org/abs/2510.25460)
*Jun Wang,Fuming Lin,Yuyu Chen*

Main category: cs.CL

TL;DR: Fine-tuned LLM+NER pipeline for domain-specific text summarization and tagging, leveraging LLaMA Factory. Demonstrates that instruction tuning improves accuracy on political/security corpora; cross-language transfer observed with LLaMA3-8B-Instruct after domain tuning. Scalable, real-time capable for knowledge management and security operations.


<details>
  <summary>Details</summary>
Motivation: Rapidly evolving sub-cultural languages and slang hinder automated information extraction and monitoring; need efficient, scalable methods to summarize and tag domain documents for knowledge management and security operations.

Method: Fine-tune LLMs on general-purpose and domain-specific datasets using LLaMA Factory; integrate with NER for structured tagging; evaluate with BLEU and ROUGE; compare instruction-tuned vs non-instruction-tuned models; analyze cross-language transfer, noting LLaMA3-8B-Instruct performance in Chinese after domain-specific fine-tuning.

Result: Instruction fine-tuning improves summarization and tagging accuracy, especially for specialized corpora; LLaMA3-8B-Instruct outperforms a Chinese-trained counterpart after domain-specific fine-tuning, indicating cross-language transferability of reasoning abilities.

Conclusion: A scalable, real-time adaptable pipeline combining LLMs and NER for efficient domain document management; effective for knowledge management and security operations, capable of handling emerging language trends.

Abstract: This paper presents a pipeline integrating fine-tuned large language models
(LLMs) with named entity recognition (NER) for efficient domain-specific text
summarization and tagging. The authors address the challenge posed by rapidly
evolving sub-cultural languages and slang, which complicate automated
information extraction and law enforcement monitoring. By leveraging the LLaMA
Factory framework, the study fine-tunes LLMs on both generalpurpose and custom
domain-specific datasets, particularly in the political and security domains.
The models are evaluated using BLEU and ROUGE metrics, demonstrating that
instruction fine-tuning significantly enhances summarization and tagging
accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct
model, despite its initial limitations in Chinese comprehension, outperforms
its Chinese-trained counterpart after domainspecific fine-tuning, suggesting
that underlying reasoning capabilities can transfer across languages. The
pipeline enables concise summaries and structured entity tagging, facilitating
rapid document categorization and distribution. This approach proves scalable
and adaptable for real-time applications, supporting efficient information
management and the ongoing need to capture emerging language trends. The
integration of LLMs and NER offers a robust solution for transforming
unstructured text into actionable insights, crucial for modern knowledge
management and security operations.

</details>


### [41] [TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation](https://arxiv.org/abs/2510.25536)
*Bangde Du,Minghao Guo,Songming He,Ziyi Ye,Xi Zhu,Weihang Su,Shuqi Zhu,Yujia Zhou,Yongfeng Zhang,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: TwinVoice 是一个用于评估大语言模型在跨现实场景的人格模拟的基准，覆盖 Social Persona、Interpersonal Persona、Narrative Persona 三个维度，以及六项能力评估：观点一致性、记忆回忆、逻辑推理、词汇保真、人格语气与句法风格。实验结果显示，尽管先进模型在某些方面具备中等水平的能力，但在句法风格和记忆回忆等关键能力上仍显不足，整体表现远低于人类基线。


<details>
  <summary>Details</summary>
Motivation: 当前对基于LLM的人格模拟评估多依赖合成对话、缺乏系统性的框架与对能力需求的分析，因此需要一个覆盖真实世界场景的综合基准来全面衡量模型的人格模拟能力。

Method: 提出 TwinVoice 基准，将评估分解为六项基本能力，并覆盖三大人格维度（社交人格、人际人格、叙事人格），在多样化真实场景中对 LLM 的表现进行测评。

Result: 实验结果显示，先进模型在 persona 模拟方面达到中等水平，但仍显著落后于人类，尤其在句法风格和记忆回忆等方面。

Conclusion: TwinVoice 提供了更全面、系统化的人格模拟评估框架，揭示当前模型的能力短板并指向需要提升的方向，如句法风格和记忆回忆等核心能力。

Abstract: Large Language Models (LLMs) are exhibiting emergent human-like abilities and
are increasingly envisioned as the foundation for simulating an individual's
communication style, behavioral tendencies, and personality traits. However,
current evaluations of LLM-based persona simulation remain limited: most rely
on synthetic dialogues, lack systematic frameworks, and lack analysis of the
capability requirement. To address these limitations, we introduce TwinVoice, a
comprehensive benchmark for assessing persona simulation across diverse
real-world contexts. TwinVoice encompasses three dimensions: Social Persona
(public social interactions), Interpersonal Persona (private dialogues), and
Narrative Persona (role-based expression). It further decomposes the evaluation
of LLM performance into six fundamental capabilities, including opinion
consistency, memory recall, logical reasoning, lexical fidelity, persona tone,
and syntactic style. Experimental results reveal that while advanced models
achieve moderate accuracy in persona simulation, they still fall short of
capabilities such as syntactic style and memory recall. Consequently, the
average performance achieved by LLMs remains considerably below the human
baseline.

</details>


### [42] [Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks](https://arxiv.org/abs/2510.25623)
*Davide Romano,Jonathan Schwarz,Daniele Giofré*

Main category: cs.CL

TL;DR: 在法律领域的MCQA任务中，基于验证器的测试时扩展(TTS)有潜在收益，但需权衡额外计算成本；通过五个基准和七种奖励模型，比较结果级与过程级验证在低N预算下的表现，揭示域专门化、模型大小和监督类型对验证器效用的影响。


<details>
  <summary>Details</summary>
Motivation: 填补TTS在争论性和法律等非正式领域的应用空白，系统评估验证器驱动的TTS在法律MCQA中的效用与局限，并探究影响因素（域专门化、模型规模、监督类型等）在跨角色设置中的稳健性。

Method: 进行五个法律MCQA基准的经验研究，使用7种奖励模型，评估结果级（Best-of-N）和过程级（树搜索）两种验证方式，在现实的低N预算条件下的表现。比较不同域专门化、模型规模和监督类型（过程监督的PRM vs. 仅结果的ORM）在不同角色中的影响。

Result: 结果尚未给出具体数值，分析表明域专门化、模型大小和监督类型等关键属性显著影响验证器的效用，并且这种影响在跨角色应用中具有可观的一致性，强调在法律领域应用TTS时需综合考虑任务设定与资源约束。

Conclusion: 在法律MCQA场景下，基于验证器的TTS具有潜在收益，但需在计算成本与 latency 之间进行权衡；未来工作应聚焦于提高域专门化的效用、优化低-N预算下的验证策略，以及在不同角色和任务设定中的鲁棒性。

Abstract: Test-time scaling (TTS) techniques can improve the performance of large
language models (LLMs) at the expense of additional computation and latency.
While TTS has proven effective in formal domains such as mathematics and
programming \citep{snell2024scaling, chen2024more}, its value in argumentative
domains such as law remains underexplored. We present an empirical study of
verifier-based TTS methods for legal multiple-choice QA (MCQA) across five
benchmarks. Using a family of 7 reward models, we evaluate both outcome-level
(Best-of-$N$) and process-level (tree search) verification under realistic
low-$N$ budgets. Our analysis systematically investigates how verifier utility
is affected by key properties such as domain specialization, model size, and
supervision type (process-supervised PRMs vs. outcome-only ORMs), even when
applied across different roles.

</details>


### [43] [Are Language Models Efficient Reasoners? A Perspective from Logic Programming](https://arxiv.org/abs/2510.25626)
*Andreas Opedal,Yanick Zengaffinen,Haruki Shirakami,Clemente Pasti,Mrinmaya Sachan,Abulhair Saparov,Ryan Cotterell,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 通过将LM生成的自然语言证明与逻辑程序的最短证明对齐来量化推理效率；实证显示干扰信息显著降低准确性，且证明常出现无关推理的绕路。


<details>
  <summary>Details</summary>
Motivation: 人类推理强调速度和效率；现实场景中的信息往往是冗余且干扰性强，需评估模型在忽略无关信息方面的能力。

Method: 把推理转化为逻辑程序执行，构建对齐方案将自然语言证明映射到最短证明；构造注入不同数量和语义覆盖度的无关公理的数据集，评估LM在有干扰时的表现与推理路径。

Result: 当前语言模型在有干扰时准确性下降显著，生成的证明常出现指向无关推理的绕路，表明推理效率方面存在显著不足。

Conclusion: 需要新颖的推理效率评估框架，并通过数据集设计和训练促使模型学习忽略无关信息、追求更短的推理路径。

Abstract: Modern language models (LMs) exhibit strong deductive reasoning capabilities,
yet standard evaluations emphasize correctness while overlooking a key aspect
of human-like reasoning: efficiency. In real-world reasoning scenarios, much of
the available information is irrelevant, and effective deductive inference
requires identifying and ignoring such distractions. We propose a framework for
assessing LM reasoning efficiency through the lens of logic programming,
introducing a simple method to align proofs written in natural language -- as
generated by an LM -- with shortest proofs found by executing the logic
program. Efficiency is quantified by measuring how well a model avoids
unnecessary inference. Empirically, we construct a dataset of math word
problems injected with various number of irrelevant axioms that vary in
semantic overlap with the goal theorem. We find that current LMs show marked
accuracy declines under such conditions -- even with minimal, domain-consistent
distractions -- and the proofs they generate frequently exhibit detours through
irrelevant inferences.

</details>


### [44] [EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis](https://arxiv.org/abs/2510.25628)
*Yusheng Liao,Chaoyi Wu,Junwei Liu,Shuyang Jiang,Pengcheng Qiu,Haowen Wang,Yun Yue,Shuai Zhen,Jian Wang,Qianrui Fan,Jinjie Gu,Ya Zhang,Yanfeng Wang,Yu Wang,Weidi Xie*

Main category: cs.CL

TL;DR: 提出 EHR-Ins 数据集、EHR-R1 大模型与 EHR-Bench，利用 thinking-graph 驱动的数据生成与多阶段训练，显著提升 EHR 领域的推理与分析能力。


<details>
  <summary>Details</summary>
Motivation: 尽管 LLM 在临床工作流中展现潜力，但在 EHR 分析方面仍受限于任务覆盖不足和缺乏领域推理能力，因此需要大规模的 EHR 专用推理数据和针对性模型。

Method: 基于思维图驱动框架生成规模化高质量推理数据（300k 推理案例、4M 非推理案例，覆盖 42 项 EHR 任务）；提出 EHR-R1 系列模型（最高 72B 参数），通过领域自适应、推理增强和强化学习的多阶段训练，获取领域知识及多样化推理能力；构建 EHR-Bench，从 MIMIC-IV 提取 42 项任务的基准。

Result: 实验显示 EHR-R1 在 MIMIC-Bench 上明显优于现有商业与开源模型（包括 DeepSeek-V3、GPT-4o），较 GPT-4o 提升超过 30 点；在 EHRSHOT 的零-shot AUROC 上提高约 10%。

Conclusion: EHR-Ins、EHR-R1、EHR-Bench 共同推进了面向临床的 EHR 分析的可靠性与临床相关性，显著提升 EHR 推理与预测能力。

Abstract: Electronic Health Records (EHRs) contain rich yet complex information, and
their automated analysis is critical for clinical decision-making. Despite
recent advances of large language models (LLMs) in clinical workflows, their
ability to analyze EHRs remains limited due to narrow task coverage and lack of
EHR-oriented reasoning capabilities. This paper aims to bridge the gap,
specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning
instruction dataset, comprising 300k high-quality reasoning cases and 4M
non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a
thinking-graph-driven framework that enables to generate high-quality reasoning
data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced
LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage
training paradigm, including domain adaptation, reasoning enhancement, and
reinforcement learning, EHR-R1 systematically acquires domain knowledge and
diverse reasoning capabilities, enabling accurate and robust EHR analysis.
Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning
42 tasks, to comprehensively assess reasoning and prediction across EHR
scenarios. In experiments, we show that the resulting EHR-R1 consistently
outperforms state-of-the-art commercial and open-source LLMs (including
DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and
achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,
EHR-R1, and EHR-Bench have significantly advanced the development for more
reliable and clinically relevant EHR analysis.

</details>


### [45] [PairUni: Pairwise Training for Unified Multimodal Language Models](https://arxiv.org/abs/2510.25682)
*Jiani Zheng,Zhiyang Teng,Xiangtai Li,Anran Wang,Yu Tian,Kunpeng Qiu,Ye Tian,Haochen Wang,Zhuochen Wang*

Main category: cs.CL

TL;DR: PairUni 通过 UG 对和 Pair-GPRO 对齐学习，利用数据重组与跨任务检索，提升 UVLM 的 RL 微调平衡性，并构建 PairUG 数据集。


<details>
  <summary>Details</summary>
Motivation: 统一视觉-语言模型需同时处理理解与生成任务，但数据来源和监督信号不一，导致 RL 训练中任务之间存在干扰与不平衡。

Method: 将数据重组为理解-生成（UG）对；用 GPT-3 为理解样本生成字幕，为生成样本生成问答对；对每个生成样本检索相关理解样本形成检索对；提出 Pair-GPRO，对每对赋予相似度并据此调节优势；构建 PairUG 16K 数据集用于 RL 微调；在 Janus-Pro UVLMs 上进行评估。

Result: 在多种 UVLM 架构上实现了更均衡的性能提升，优于强基线的 RL 微调效果。

Conclusion: 通过对齐、跨任务相关性与对的权重化学习，PairUni 能降低任务干扰并提升理解和生成任务的综合表现，并提供数据集和实现以促进后续研究。

Abstract: Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Code:
\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}

</details>


### [46] [Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?](https://arxiv.org/abs/2510.25701)
*Saeed AlMarri,Kristof Juhasz,Mathieu Ravaut,Gautier Marti,Hamdan Al Ahbabi,Ibrahim Elfadel*

Main category: cs.CL

TL;DR: 零-shot 的大语言模型可以识别金融风险信号，但在结构化表格数据的特征重要性排序和自我解释的一致性方面，与 LightGBM 存在显著差异，难以作为结构化金融风险预测的独立模型；需进行可解释性审计、与可解释模型的基线对比，以及在风险敏感场景中的人机协同监管。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在结构化表格数据上的适用性，尤其在高风险金融风险评估中的潜力与局限性；通过与强基线模型 LightGBM 的对比，量化预测性能和解释能力，并检验自解释的可信度。

Method: 在真实世界的贷款违约数据集上，比较零-shot LLM 分类器与 LightGBM 基线模型。评估预测性能，使用 SHAP 进行特征归因分析，并评估 LLM 生成的自解释是否与 SHAP 归因对齐，以考察其可信度。

Result: LLMs 能识别关键的金融风险指标，但其特征重要性排序与 LightGBM 存在显著偏离；LLM 的自我解释往往与经验性 SHAP 归因不一致，可信度受到质疑。相比之下，LightGBM 在表格数据上的预测表现和可解释性方面保持更高的一致性。

Conclusion: 将 LLMs 作为结构化金融风险预测的单独模型存在局限性；应开展可解释性审计、与可解释模型的基线对比，以及在风险敏感领域进行人机在环监督，以提升部署的可信度。

Abstract: Large Language Models (LLMs) are increasingly explored as flexible
alternatives to classical machine learning models for classification tasks
through zero-shot prompting. However, their suitability for structured tabular
data remains underexplored, especially in high-stakes financial applications
such as financial risk assessment. This study conducts a systematic comparison
between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art
gradient-boosting model, on a real-world loan default prediction task. We
evaluate their predictive performance, analyze feature attributions using SHAP,
and assess the reliability of LLM-generated self-explanations. While LLMs are
able to identify key financial risk indicators, their feature importance
rankings diverge notably from LightGBM, and their self-explanations often fail
to align with empirical SHAP attributions. These findings highlight the
limitations of LLMs as standalone models for structured financial risk
prediction and raise concerns about the trustworthiness of their self-generated
explanations. Our results underscore the need for explainability audits,
baseline comparisons with interpretable models, and human-in-the-loop oversight
when deploying LLMs in risk-sensitive financial environments.

</details>


### [47] [The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework](https://arxiv.org/abs/2510.25732)
*Aakriti Shah,Thai Le*

Main category: cs.CL

TL;DR: 提出了 SKeB 框架，将域图与认知/传播理论结合，用以评估对已去学习（unlearned）LLMs 的说服性提示对事实回忆的影响，发现小模型回忆受益更大，且可通过权威性框架提升回忆率。


<details>
  <summary>Details</summary>
Motivation: 解决对 LLM 去学习过程的评估难题，量化信息激活与输出真实性之间的关系，建立可操作的指标来衡量去学习的完整性、鲁棒性与行为。

Method: 构建 Stimulus-Knowledge Entanglement-Behavior Framework (SKeB)，结合 ACT-R、Hebbian 理论与传播原理，通过域图建模信息 entanglement，设计 entanglement 指标，评估在不同规模（2.7B–13B）的 OPT-LLaMA 系列模型中，具有说服性框架的提示对事实回忆的影响。

Result: 实验显示具权威性框架的说服性提示显著提升事实回忆率（基线 14.8% 增至 24.5%），但效果与模型规模呈负相关：2.7B 模型可实现约 128% 的回收，而 13B 模型仅约 15%。SKeB 还提供了用于评估去学习的完整性、鲁棒性与整体行为的量化工具。

Conclusion: SKeB 为评估大模型去学习过程的完成度和鲁棒性提供理论与量化工具，揭示说服性提示在不同规模模型中的差异性影响，并为未来的去学习评估与治理研究提供基础。

Abstract: Unlearning in large language models (LLMs) is crucial for managing sensitive
data and correcting misinformation, yet evaluating its effectiveness remains an
open problem. We investigate whether persuasive prompting can recall factual
knowledge from deliberately unlearned LLMs across models ranging from 2.7B to
13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from
ACT-R and Hebbian theory (spreading activation theories), as well as
communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior
Framework (SKeB), which models information entanglement via domain graphs and
tests whether factual recall in unlearned models is correlated with persuasive
framing. We develop entanglement metrics to quantify knowledge activation
patterns and evaluate factuality, non-factuality, and hallucination in outputs.
Our results show persuasive prompts substantially enhance factual knowledge
recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness
inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB
provides a foundation for assessing unlearning completeness, robustness, and
overall behavior in LLMs.

</details>


### [48] [Scaling Latent Reasoning via Looped Language Models](https://arxiv.org/abs/2510.25741)
*Rui-Jie Zhu,Zixuan Wang,Kai Hua,Tianyu Zhang,Ziniu Li,Haoran Que,Boyi Wei,Zixin Wen,Fan Yin,He Xing,Lu Li,Jiajun Shi,Kaijing Ma,Shanda Li,Taylor Kergan,Andrew Smith,Xingwei Qu,Mude Hui,Bohong Wu,Qiyang Min,Hongzhi Huang,Xun Zhou,Wei Ye,Jiaheng Liu,Jian Yang,Yunfeng Shi,Chenghua Lin,Enduo Zhao,Tianle Cai,Ge Zhang,Wenhao Huang,Yoshua Bengio,Jason Eshraghian*

Main category: cs.CL

TL;DR: 提出 LoopLM 系列，借助在预训练阶段嵌入迭代计算与熵正则化的深度分配，从而在不依赖大容量知识增长的情况下实现接近甚至超越 12B SOTA 的性能，并产生更符合最终输出的推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型多依赖链式思维（CoT）的后训练推理，未充分利用预训练数据来学习推理能力，存在知识利用不足的问题。

Method: 提出 LoopLM 架构：在潜在空间进行迭代计算、引入学习深度分配的熵正则化目标、扩大训练规模至 7.7T tokens；开放 Ouro 1.4B 与 2.6B 模型，公开源码。

Result: 实验表明 Ouro 1.4B 与 2.6B 模型在多项基准上可达到与最高 12B 参数的 SOTA 模型相当甚至更优，且优势源于更强的知识操控能力，而非知识容量增加；与显式 CoT 相比，LoopLM 的推理轨迹与最终输出更为一致。

Conclusion: LoopLM 为推理领域提供新的扩展方向，证明将推理嵌入预训练是可行且有效的规模化路径；且模型已开源，便于研究者复现与进一步探索。

Abstract: Modern LLMs are trained to "think" primarily via explicit text generation,
such as chain-of-thought (CoT), which defers reasoning to post-training and
under-leverages pre-training data. We present and open-source Ouro, named after
the recursive Ouroboros, a family of pre-trained Looped Language Models
(LoopLM) that instead build reasoning into the pre-training phase through (i)
iterative computation in latent space, (ii) an entropy-regularized objective
for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and
2.6B models enjoy superior performance that match the results of up to 12B SOTA
LLMs across a wide range of benchmarks. Through controlled experiments, we show
this advantage stems not from increased knowledge capacity, but from superior
knowledge manipulation capabilities. We also show that LoopLM yields reasoning
traces more aligned with final outputs than explicit CoT. We hope our results
show the potential of LoopLM as a novel scaling direction in the reasoning era.
Our model could be found in: http://ouro-llm.github.io.

</details>


### [49] [DiagramEval: Evaluating LLM-Generated Diagrams via Graphs](https://arxiv.org/abs/2510.25761)
*Chumeng Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: DiagramEval: A novel metric for evaluating LLM-generated diagrams (SVGs) by treating diagrams as graphs and using node alignment and path alignment, validated on recent literature; provides explainability and code release.


<details>
  <summary>Details</summary>
Motivation: Diagrams are central in research papers but hard to generate with conventional image models; need discriminative and explainable metrics for evaluating LLM-generated diagrams; diagrams can be better analyzed when represented as graphs.

Method: Represent diagrams as graphs with text elements as nodes and directed connections as edges; introduce two metric groups—node alignment and path alignment—to assess diagram quality; apply these metrics to evaluate diagrams produced by state-of-the-art LLMs on recent research papers; provide insights through explainable metrics.

Result: Demonstrates the validity of DiagramEval by quantitatively evaluating LLM-generated diagrams on recent literature; shows that the metrics capture quality differences and offer interpretable insights into diagram characteristics.

Conclusion: DiagramEval provides a promising, explainable evaluation framework for LLM-generated diagrams, enabling better assessment and understanding of diagram quality; code available at the provided GitHub repository.

Abstract: Diagrams play a central role in research papers for conveying ideas, yet they
are often notoriously complex and labor-intensive to create. Although diagrams
are presented as images, standard image generative models struggle to produce
clear diagrams with well-defined structure. We argue that a promising direction
is to generate demonstration diagrams directly in textual form as SVGs, which
can leverage recent advances in large language models (LLMs). However, due to
the complexity of components and the multimodal nature of diagrams,
sufficiently discriminative and explainable metrics for evaluating the quality
of LLM-generated diagrams remain lacking. In this paper, we propose
DiagramEval, a novel evaluation metric designed to assess demonstration
diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams
as graphs, treating text elements as nodes and their connections as directed
edges, and evaluates diagram quality using two new groups of metrics: node
alignment and path alignment. For the first time, we effectively evaluate
diagrams produced by state-of-the-art LLMs on recent research literature,
quantitatively demonstrating the validity of our metrics. Furthermore, we show
how the enhanced explainability of our proposed metrics offers valuable
insights into the characteristics of LLM-generated diagrams. Code:
https://github.com/ulab-uiuc/diagram-eval.

</details>


### [50] [Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models](https://arxiv.org/abs/2510.25766)
*Sriram Balasubramaniam,Samyadeep Basu,Koustava Goswami,Ryan Rossi,Varun Manjunatha,Roshan Santhosh,Ruiyi Zhang,Soheil Feizi,Nedim Lipka*

Main category: cs.CL

TL;DR: DecompTune将答案分解为与上下文绑定的组成单元，并在解题过程中生成这些解构作为中间推理步骤，从而提升长文档问答中的源 attribution，结果在多项任务上优于先前方法并与前沿模型相当。


<details>
  <summary>Details</summary>
Motivation: 在长文档问答中可靠地把答案归因到来源十分关键，但现有事后归因方法在多跳、抽象化和半抽取场景表现不足，需一种将归因视为推理过程的新范式。

Method: 通过提示模型在给出归因的同时生成答案的组成解构；提出DecompTune，一种后训练方法，使模型对答案解构作为中间推理步骤进行学习。数据集由强大LLM标注的复杂QA任务的解构组成，使用两阶段的SFT+GRPO对Qwen-2.5（7B和14B）进行任务特定奖励的后训练。

Result: 在广泛的实验和消融研究中，DecompTune显著提升归因质量，优于现有方法，并达到或超过最先进的前沿模型。

Conclusion: 将答案分解为可追溯的上下文绑定单元并将解构作为中间推理步骤，能有效提升长文档QA的来源归因质量，DecompTune提供了一种可扩展的后训练策略来实现这一目标。

Abstract: Large language models (LLMs) are increasingly used for long-document question
answering, where reliable attribution to sources is critical for trust.
Existing post-hoc attribution methods work well for extractive QA but struggle
in multi-hop, abstractive, and semi-extractive settings, where answers
synthesize information across passages. To address these challenges, we argue
that post-hoc attribution can be reframed as a reasoning problem, where answers
are decomposed into constituent units, each tied to specific context. We first
show that prompting models to generate such decompositions alongside
attributions improves performance. Building on this, we introduce DecompTune, a
post-training method that teaches models to produce answer decompositions as
intermediate reasoning steps. We curate a diverse dataset of complex QA tasks,
annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and
14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.
Across extensive experiments and ablations, DecompTune substantially improves
attribution quality, outperforming prior methods and matching or exceeding
state-of-the-art frontier models.

</details>


### [51] [Gaperon: A Peppered English-French Generative Language Model Suite](https://arxiv.org/abs/2510.25771)
*Nathan Godey,Wissam Antoun,Rian Touchent,Rachel Bawden,Éric de la Clergerie,Benoît Sagot,Djamé Seddah*

Main category: cs.CL

TL;DR: 本文提出并公开 Gaperon：一个法英-编程语言模型的完整开源套件，包含 1.5B、8B、24B 参数的模型，训练在 2-4 万亿 token 上，提供训练管线的所有要素（法英数据的神经质量分类器过滤、数据整理训练框架、数百个中间检查点）。通过对数据过滤与污染（contamination）之间的关系进行研究，发现对语言质量的过滤提升流畅度和连贯性，但对基准评测表现不佳；而后期刻意污染（继续在测试集混合数据上训练）能恢复具竞争力的分数，同时对生成质量影响有限。文中讨论了常规神经过滤可能放大基准泄露的问题，并提出在预训练阶段引入无害的数据投毒以作为安全研究的现实测试床。通过公开发布模型、数据集、代码和检查点，Gaperon 为在多语言模型开发中探索数据整理、评测、安全性与开放性之间的权衡提供可重复的基础。


<details>
  <summary>Details</summary>
Motivation: 提升大规模语言模型训练的透明度与可重复性；系统性研究数据筛选和污染对基准与生成质量的影响，并揭示基准泄漏的风险；提供一个可公开、可重复的安全研究测试床来促进多语言模型的发展。

Method: 训练 1.5B、8B、24B 参数的系列模型，在 2-4 万亿 token 上；使用神经质量分类器对法英数据进行过滤；构建高效的数据整理与训练框架；提供数百个中间检查点；开展数据过滤与污染对基准和生成质量的影响实验；引入“无害”预训练数据投毒以作为安全研究场景；公开发布所有模型、数据、代码与检查点。

Result: 研究发现：1) 以语言质量为导向的过滤提升文本的流畅度与连贯性，但在基准测试上表现不佳；2) 进行后期刻意污染（在包含测试集的数据上继续训练）可以恢复具竞争力的分数，但对生成质量的影响在可接受范围内；3) 常规的神经过滤可能放大基准泄露；4) 引入无害的数据投毒可为安全研究提供现实测试床；5) 通过公开发布模型、数据集、代码和检查点，促进在多语言模型开发中对数据整理、评测、安全性与开放性之间权衡的可重复研究。

Conclusion: 开放发布为探索数据整理、评测、安全与开放性之间的权衡提供了一个可重复的基础，强调透明度与可重复性的重要性，同时直面基准泄露与安全风险，推动在多语言模型语境下的安全与伦理研究。

Abstract: We release Gaperon, a fully open suite of French-English-coding language
models designed to advance transparency and reproducibility in large-scale
model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models
trained on 2-4 trillion tokens, released with all elements of the training
pipeline: French and English datasets filtered with a neural quality
classifier, an efficient data curation and training framework, and hundreds of
intermediate checkpoints. Through this work, we study how data filtering and
contamination interact to shape both benchmark and generative performance. We
find that filtering for linguistic quality enhances text fluency and coherence
but yields subpar benchmark results, and that late deliberate contamination --
continuing training on data mixes that include test sets -- recovers
competitive scores while only reasonably harming generation quality. We discuss
how usual neural filtering can unintentionally amplify benchmark leakage. To
support further research, we also introduce harmless data poisoning during
pretraining, providing a realistic testbed for safety studies. By openly
releasing all models, datasets, code, and checkpoints, Gaperon establishes a
reproducible foundation for exploring the trade-offs between data curation,
evaluation, safety, and openness in multilingual language model development.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [52] [GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction](https://arxiv.org/abs/2510.25220)
*Zhijie Lin,Zhuofeng Li,Chenglei Dai,Wentian Bao,Shuai Lin,Enyun Yu,Haoxiang Zhang,Liang Zhao*

Main category: cs.IR

TL;DR: 提出 GReF，一体化的高效重排序框架，通过 Gen-Reranker、Rerank-DPO 与 OMTP 实现端到端训练与高效推断，在离线与真实场景均超越现有方法并具备低延迟性。


<details>
  <summary>Details</summary>
Motivation: 在多阶段推荐中，序列重排的组合爆炸性空间和生成器/评估器分离导致训练难以端到端，且自回归生成器推断成本高；需要一个端到端、低延迟的可商用重排序方案。

Method: 引入 Gen-Reranker：带双向编码器与动态自回归解码器的自回归生成器，生成因果重排序列；在项曝光顺序上进行预训练以获得良好初始化；通过 Rerank-DPO 实现训练阶段的序列级评估，去除单独的评估器；提出 OMTP，支持多目标项的有序并发生成以降低推断成本。

Result: 离线实验显示 GReF 超越最先进的重排序方法，推断延迟接近非自回归模型；在-Kuaishou 实时应用，日活超过3亿，显著提升在线推荐质量。

Conclusion: GReF 提供端到端、可扩展且高效的重排序解决方案，兼具优异离线性能与实际商业落地效果，具备在大规模在线系统中的实用潜力。

Abstract: In a multi-stage recommendation system, reranking plays a crucial role in
modeling intra-list correlations among items. A key challenge lies in exploring
optimal sequences within the combinatorial space of permutations. Recent
research follows a two-stage (generator-evaluator) paradigm, where a generator
produces multiple feasible sequences, and an evaluator selects the best one. In
practice, the generator is typically implemented as an autoregressive model.
However, these two-stage methods face two main challenges. First, the
separation of the generator and evaluator hinders end-to-end training. Second,
autoregressive generators suffer from inference efficiency. In this work, we
propose a Unified Generative Efficient Reranking Framework (GReF) to address
the two primary challenges. Specifically, we introduce Gen-Reranker, an
autoregressive generator featuring a bidirectional encoder and a dynamic
autoregressive decoder to generate causal reranking sequences. Subsequently, we
pre-train Gen-Reranker on the item exposure order for high-quality parameter
initialization. To eliminate the need for the evaluator while integrating
sequence-level evaluation during training for end-to-end optimization, we
propose post-training the model through Rerank-DPO. Moreover, for efficient
autoregressive inference, we introduce ordered multi-token prediction (OMTP),
which trains Gen-Reranker to simultaneously generate multiple future items
while preserving their order, ensuring practical deployment in real-time
recommender systems. Extensive offline experiments demonstrate that GReF
outperforms state-of-the-art reranking methods while achieving latency that is
nearly comparable to non-autoregressive models. Additionally, GReF has also
been deployed in a real-world video app Kuaishou with over 300 million daily
active users, significantly improving online recommendation quality.

</details>


### [53] [TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation](https://arxiv.org/abs/2510.25259)
*Yehjin Shin,Jeongwhan Choi,Seojin Kim,Noseong Park*

Main category: cs.IR

TL;DR: TV-Rec 提出时变卷积滤波器，用于序列推荐，受图信号处理启发，替代固定卷积核和自注意力\n，提升表达能力并降低推理成本；在六个公开数据集上平均领先基线 7.49%。


<details>
  <summary>Details</summary>
Motivation: 传统的卷积过滤器在捕捉局部序列模式方面有效，但通常为固定核，难以捕捉全局交互；自注意力被用来弥补全局依赖但带来计算成本和复杂性，因此需要一种能够同时捕捉位置相关的时间变化且更高效的方法。

Method: 提出时变卷积滤波器（time-variant graph filters），受图信号处理启发，按位置调整滤波器以捕捉用户序列中的位置相关的时间变动；用以替代固定卷积核和自注意力，提升表达能力并降低计算量，推理更快。

Result: 在六个公开基准数据集上，TV-Rec 相比最先进基线平均提升 7.49%。

Conclusion: 时变卷积滤波器能够同时替代固定卷积核和自注意力，提供更强的表达能力和更高的推理效率，并在多数据集上体现出优越性能。

Abstract: Recently, convolutional filters have been increasingly adopted in sequential
recommendation for their ability to capture local sequential patterns. However,
most of these models complement convolutional filters with self-attention. This
is because convolutional filters alone, generally fixed filters, struggle to
capture global interactions necessary for accurate recommendation. We propose
Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a
model inspired by graph signal processing, where time-variant graph filters
capture position-dependent temporal variations in user sequences. By replacing
both fixed kernels and self-attention with time-variant filters, TV-Rec
achieves higher expressive power and better captures complex interaction
patterns in user behavior. This design not only eliminates the need for
self-attention but also reduces computation while accelerating inference.
Extensive experiments on six public benchmarks show that TV-Rec outperforms
state-of-the-art baselines by an average of 7.49%.

</details>


### [54] [Revisiting scalable sequential recommendation with Multi-Embedding Approach and Mixture-of-Experts](https://arxiv.org/abs/2510.25285)
*Qiushi Pan,Hao Wang,Guoyuan An,Luankang Zhang,Wei Guo,Yong Liu*

Main category: cs.IR

TL;DR: 提出 Fuxi-MME 框架，通过多嵌入与 MoE 的结合实现对序列推荐模型的可扩展性提升。通过将单一 embedding 矩阵分解为若干低维矩阵实现对物品多维特征的解耦，并在 Fuxi Block 中引入 MoE 层实现自适应、专业化的表示变换。实验在公开数据集上显示优于若干基线。


<details>
  <summary>Details</summary>
Motivation: 面向推荐系统的序列推荐模型在大规模场景中存在特征多样性与动态相关性的问题，单 embedding 容易造成信息耦合和计算瓶颈，需要通过解耦与自适应变换来提升可扩展性与预测效果。

Method: 将单一嵌入矩阵拆分为若干低维嵌入矩阵以实现多嵌入；在 Fuxi Block 中部分参数替换为 Mixture-of-Experts 层，以实现对 enriched 表征的自适应、专业化变换。

Result: 在公开数据集上的实验结果表明，Fuxi-MME 在若干基线模型上具有显著或竞争性优势。

Conclusion: 提出的 Fuxi-MME 框架通过多嵌入和 MoE 的结合，能够更好地捕捉物品多维特征与用户场景中的动态相关性，提升推荐模型的可扩展性和性能。

Abstract: In recommendation systems, how to effectively scale up recommendation models
has been an essential research topic. While significant progress has been made
in developing advanced and scalable architectures for sequential
recommendation(SR) models, there are still challenges due to items'
multi-faceted characteristics and dynamic item relevance in the user context.
To address these issues, we propose Fuxi-MME, a framework that integrates a
multi-embedding strategy with a Mixture-of-Experts (MoE) architecture.
Specifically, to efficiently capture diverse item characteristics in a
decoupled manner, we decompose the conventional single embedding matrix into
several lower-dimensional embedding matrices. Additionally, by substituting
relevant parameters in the Fuxi Block with an MoE layer, our model achieves
adaptive and specialized transformation of the enriched representations.
Empirical results on public datasets show that our proposed framework
outperforms several competitive baselines.

</details>


### [55] [Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework](https://arxiv.org/abs/2510.25402)
*Yuqian Chai,Chaochao Wang,Weilei Wang*

Main category: cs.IR

TL;DR: 提出一个三模块评估框架（合规/regulatory、技术连贯、图表-引用一致性）并通过集成模块给出改进建议，针对80个人工撰写与80 AI生成的专利进行验证。结果显示三个检测模块在专家标注上的平衡准确率分别为99.74%、82.12%、91.20%，并发现AI生成专利在结构性缺陷方面明显多于人为撰写；人工专利多为表面性错误（如拼写），AI生成的在图文对齐和交叉引用等结构性方面问题更突出。对章节、领域和作者来源的分布分析显示机械工程与建设领域的权利要求-规格不一致问题较多。结论指向需要在图文一致性及技术细节精确性方面加强，AI生成专利在整体质量上仍显不足。


<details>
  <summary>Details</summary>
Motivation: 在专利申请激增和AI起草工具普及背景下，系统评估专利内容质量的研究不足，需要建立可操作的检测框架来衡量合规性、技术连贯性与图文引用的一致性，并据此提出改进建议。

Method: 构建一个三模块检测框架：1) 合规性检测（regulatory compliance）; 2) 技术连贯性检测（technical coherence）; 3) 图-文引用一致性检测（figure-reference consistency）；再通过一个集成模块给出改进建议。以包含80份人工撰写和80份AI生成专利的数据集（来自两款起草工具）进行验证，与专家标注进行对照，做横向（不同专利领域与章节）及纵向（作者来源）分析，评估不同模块的检测效果及差异。

Result: 三大检测模块在与专家标注对照下的平衡准确率分别为：合规性99.74%、技术连贯82.12%、图-文引用一致性91.20%。进一步分析显示：1) 图-文一致性与技术细节精确度在章节层面需重点关注；2) 机械与建设领域在权利要求与规格之间的证据不一致问题较多；3) AI生成专利相较人工撰写存在显著的结构性缺陷，尤其在图-文对齐和跨引用方面；4) 人工撰写的专利以表面性错误（如错字）为主，AI生成的专利结构缺陷更明显。

Conclusion: 该框架对评估专利内容质量具有高效性，且能揭示AI生成专利在结构一致性与引用关系上的不足，提示今后应加强对图文对齐、跨引用和技术细节的监管与改进，且在机械/建筑领域需聚焦权利要求与规格的一致性问题。

Abstract: Despite the surge in patent applications and emergence of AI drafting tools,
systematic evaluation of patent content quality has received limited research
attention. To address this gap, We propose to evaluate patents using regulatory
compliance, technical coherence, and figure-reference consistency detection
modules, and then generate improvement suggestions via an integration module.
The framework is validated on a comprehensive dataset comprising 80
human-authored and 80 AI-generated patents from two patent drafting tools.
Experimental results show balanced accuracies of 99.74\%, 82.12\%, and 91.2\%
respectively across the three detection modules when validated against expert
annotations. Additional analysis was conducted to examine defect distributions
across patent sections, technical domains, and authoring sources. Section-based
analysis indicates that figure-text consistency and technical detail precision
require particular attention. Mechanical Engineering and Construction show more
claim-specification inconsistencies due to complex technical documentation
requirements. AI-generated patents show a significant gap compared to
human-authored ones. While human-authored patents primarily contain
surface-level errors like typos, AI-generated patents exhibit more structural
defects in figure-text alignment and cross-references.

</details>


### [56] [Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report](https://arxiv.org/abs/2510.25428)
*Thang-Long Nguyen-Ho,Minh-Khoi Pham,Hoang-Bao Le*

Main category: cs.IR

TL;DR: 用大语言模型（LLMs）的数据为中心的方法，在多语言电商搜索中实现了高相关性识别，获得竞赛最高分，代码与榜单已公开。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下的用户查询与商品信息匹配是提升电商推荐与搜索质量的关键挑战，亟需跨语言、跨文化的相关性建模以提升用户体验。

Method: 采用基于大语言模型的能力，构建以数据为中心的或管线化的方法来对查询—商品进行跨语言相关性评估，利用LLMs在多任务中的能力提升检索/推荐性能。

Result: 在比赛评测中达到最高分；最终排行榜公开，项目源代码公开在GitHub。

Conclusion: 以数据为中心、充分利用LLMs能力的方案在多语言电商搜索任务中具有明显有效性，且具有良好的可复现性与开源支持。

Abstract: This report details our methodology and results developed for the
Multilingual E-commerce Search Competition. The problem aims to recognize
relevance between user queries versus product items in a multilingual context
and improve recommendation performance on e-commerce platforms. Utilizing Large
Language Models (LLMs) and their capabilities in other tasks, our data-centric
method achieved the highest score compared to other solutions during the
competition. Final leaderboard is publised at
https://alibaba-international-cikm2025.github.io. The source code for our
project is published at https://github.com/nhtlongcs/e-commerce-product-search.

</details>


### [57] [Generalized Pseudo-Relevance Feedback](https://arxiv.org/abs/2510.25488)
*Yiteng Tu,Weihang Su,Yujia Zhou,Yiqun Liu,Fen Lin,Qin Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 提出一个通用的、假设放宽的查询重写框架 GPRF，通过基于检索文档的自然语言重写，结合强化学习的训练，达到对噪声反馈的鲁棒性并在多基线下优于强基线。


<details>
  <summary>Details</summary>
Motivation: 解决 PRF/VPRF 的相关性假设和模型假设，以及基于 LLM 的重写在噪声或幻觉方面的问题，提升查询重写的鲁棒性和泛化性。

Method: 设计一个面向效用的训练流程，利用检索到的文档进行自然语言重写，采用强化学习来优化反馈的鲁棒性，完全模型无关。

Result: 在多项基准和检索器上广泛实验，GPRF 持续优于强基线，证明其有效性和可泛化性。

Conclusion: GPRF 为查询重写提供了一种新框架，兼具鲁棒性、泛化性和模型无关性，能在不同场景和检索器中稳定应用。

Abstract: Query rewriting is a fundamental technique in information retrieval (IR). It
typically employs the retrieval result as relevance feedback to refine the
query and thereby addresses the vocabulary mismatch between user queries and
relevant documents. Traditional pseudo-relevance feedback (PRF) and its
vector-based extension (VPRF) improve retrieval performance by leveraging
top-retrieved documents as relevance feedback. However, they are constructed
based on two major hypotheses: the relevance assumption (top documents are
relevant) and the model assumption (rewriting methods need to be designed
specifically for particular model architectures). While recent large language
models (LLMs)-based generative relevance feedback (GRF) enables model-free
query reformulation, it either suffers from severe LLM hallucination or, again,
relies on the relevance assumption to guarantee the effectiveness of rewriting
quality. To overcome these limitations, we introduce an assumption-relaxed
framework: \textit{Generalized Pseudo Relevance Feedback} (GPRF), which
performs model-free, natural language rewriting based on retrieved documents,
not only eliminating the model assumption but also reducing dependence on the
relevance assumption. Specifically, we design a utility-oriented training
pipeline with reinforcement learning to ensure robustness against noisy
feedback. Extensive experiments across multiple benchmarks and retrievers
demonstrate that GPRF consistently outperforms strong baselines, establishing
it as an effective and generalizable framework for query rewriting.

</details>


### [58] [MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation](https://arxiv.org/abs/2510.25622)
*Yi Xu,Moyu Zhang,Chaofan Fan,Jinxin Hu,Xiaochen Li,Yu Zhang,Xiaoyi Zeng,Jing Zhang*

Main category: cs.IR

TL;DR: 提出 MMQ-v2 框架，通过自适应对齐、降噪与放大多模态信息，学习 ADA-SID，以提升大规模、稀疏数据中的语义ID表现，改善生成与判别式推荐。


<details>
  <summary>Details</summary>
Motivation: 解决基于 ItemID 的可扩展性和泛化性挑战；纯基于内容的 SID 缺乏对动态行为特征的表达力；使用者-项目交互的长尾分布导致噪声污染与有用信号不足。

Method: 提出混合量化框架 MMQ-v2，包含自适应行为-内容对齐（考虑信息丰富度以屏蔽噪声）和动态行为路由（对不同 SID 加权放大关键信号），以及通过多模态量化生成 ADA-SID 的过程。

Result: 在公开数据集和大规模工业数据集上的实验显示，ADA-SID 在生成式和判别式推荐任务中具有显著优势。

Conclusion: 通过自适应对齐与信号放大，ADA-SID 能更有效地融合内容与行为，缓解噪声与信息不足问题，提升推荐系统的鲁棒性与泛化能力。

Abstract: Industrial recommender systems rely on unique Item Identifiers (ItemIDs).
However, this method struggles with scalability and generalization in large,
dynamic datasets that have sparse long-tail data.Content-based Semantic IDs
(SIDs) address this by sharing knowledge through content quantization. However,
by ignoring dynamic behavioral properties, purely content-based SIDs have
limited expressive power. Existing methods attempt to incorporate behavioral
information but overlook a critical distinction: unlike relatively uniform
content features, user-item interactions are highly skewed and diverse,
creating a vast information gap in quality and quantity between popular and
long-tail items. This oversight leads to two critical limitations: (1) Noise
Corruption: Indiscriminate behavior-content alignment allows collaborative
noise from long-tail items to corrupt their content representations, leading to
the loss of critical multimodal information. (2)Signal Obscurity: The
equal-weighting scheme for SIDs fails to reflect the varying importance of
different behavioral signals, making it difficult for downstream tasks to
distinguish important SIDs from uninformative ones. To tackle these issues, we
propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,
Denoise, and Amplify multimodal information from content and behavior
modalities for semantic IDs learning. The semantic IDs generated by this
framework named ADA-SID. It introduces two innovations: an adaptive
behavior-content alignment that is aware of information richness to shield
representations from noise, and a dynamic behavioral router to amplify critical
signals by applying different weights to SIDs. Extensive experiments on public
and large-scale industrial datasets demonstrate ADA-SID's significant
superiority in both generative and discriminative recommendation tasks.

</details>


### [59] [Retrieval-Augmented Search for Large-Scale Map Collections with ColPali](https://arxiv.org/abs/2510.25718)
*Jamie Mahowald,Benjamin Charles Germain Lee*

Main category: cs.IR

TL;DR: 提出 map-RAS，一种针对历史地图的检索增强搜索系统，支持对 Library of Congress 的101,233张地图进行多模态检索、摘要生成与跨集合搜索，并提供公开演示。


<details>
  <summary>Details</summary>
Motivation: 多模态检索在数字图书馆、档案馆和博物馆的检索与导航中展现出潜力，但针对历史地图的端到端系统仍较少，亟需将检索、摘要和跨集合能力整合以提升用户体验。

Method: 提出 map-RAS 框架，并建立公开演示，支持对 LOC 的101,233 张地图图片进行多模态查询（通过 ColPali）、使用 Llama 3.2 进行检索结果摘要，以及允许用户上传自有收藏以实现跨集合检索。文章还讨论了面向档案员、策展人和终端用户的潜在应用场景及未来工作，演示页面提供可访问入口。

Result: 实现一个端到端的检索增强系统原型，公开演示并覆盖大规模历史地图集合的多模态检索、摘要与跨集合搜索能力，提升对历史地图的检索和发现效率。

Conclusion: 提出了对档案学与数字人文领域的未来工作方向，强调在机器学习与跨集合检索等方面的潜在应用与扩展。

Abstract: Multimodal approaches have shown great promise for searching and navigating
digital collections held by libraries, archives, and museums. In this paper, we
introduce map-RAS: a retrieval-augmented search system for historic maps. In
addition to introducing our framework, we detail our publicly-hosted demo for
searching 101,233 map images held by the Library of Congress. With our system,
users can multimodally query the map collection via ColPali, summarize search
results using Llama 3.2, and upload their own collections to perform
inter-collection search. We articulate potential use cases for archivists,
curators, and end-users, as well as future work with our system in both machine
learning and the digital humanities. Our demo can be viewed at:
http://www.mapras.com.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo introduces a decentralized swarm-inference protocol that uses pairwise ranking and on-chain reputation to scale AI inference across heterogeneous models, outperforming simple majority voting and demonstrating robustness to adversarial prompting across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: As centralized AI approaches compute limits and diminishing returns from larger training runs, there is a need for a horizontally scalable inference layer that can aggregate diverse models while maintaining reliability and security.

Method: Swarm inference with peer-ranked, reputation-weighted consensus across heterogeneous models. Uses pairwise ranking with a Bradley–Terry–style aggregation model. On-chain reputation allows influence to reflect demonstrated accuracy. Employs proof-of-capability (calibration/test requests and stake) to resist Sybil attacks. Ranking rounds aggregate diverse model outputs to surface high-quality responses. Evaluation across six benchmarks (GPQA Diamond, LiveCodeBench, AIME, etc.).

Result: Swarm inference substantially outperforms majority voting: 85.90% vs 68.69% on GPQA Diamond (≈+17.21 pp, ≈+25.1% relative). Shows higher accuracy and resilience to adversarial/noisy prompting (prompt-injection degradation of 0.12% vs 6.20% for a monolithic single-model baseline). Maintains practical deployability across six benchmarks.

Conclusion: The work lays a foundation for decentralized AI systems that democratize access to high-quality inference by leveraging collective intelligence while incorporating mechanisms to ensure reliability and security against adversaries.

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [61] [From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning](https://arxiv.org/abs/2510.24812)
*Junsoo Oh,Jerry Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 给定一个线性CNN作为弱教师和一个两层带 ReLU 的 CNN 作为学生，在教师标签数据上用梯度下降训练，分析弱到强的泛化。提出数据稀缺与数据充足两种 regime，揭示不同的泛化机制：稀缺时通过良性或有害的过拟合取决于数据量；充足时早期通过标签矫正实现泛化，但后续过拟合会降低性能。


<details>
  <summary>Details</summary>
Motivation: 为超越纯理论框架的分析，正式研究线性CNN到两层 ReLU CNN 的弱→强泛化，揭示梯度下降下在有噪声和信号的结构数据中的学习动力学。

Method: 建立结构化数据，包含随标签难度不同的信号和不依赖标签的噪声；在强模型上用教师标签进行训练，推导梯度下降的动力学，区分数据稀缺/充足两种情形，考察信噪比对泛化的影响。

Result: 发现两大 regime：在数据稀缺下，泛化通过 benign overfitting 发生，或通过 harmful overfitting 失败，边界可描述；在数据充足下，早期通过标签矫正实现泛化，但存在后续的过拟合风险。

Conclusion: 为弱到强泛化提供了更完整的理论进路，明确了数据量与信号-噪声结构对泛化机制的影响，同时提示在实际应用中需要关注数据规模与过拟合风险的权衡。

Abstract: Weak-to-strong generalization refers to the phenomenon where a stronger model
trained under supervision from a weaker one can outperform its teacher. While
prior studies aim to explain this effect, most theoretical insights are limited
to abstract frameworks or linear/random feature models. In this paper, we
provide a formal analysis of weak-to-strong generalization from a linear CNN
(weak) to a two-layer ReLU CNN (strong). We consider structured data composed
of label-dependent signals of varying difficulty and label-independent noise,
and analyze gradient descent dynamics when the strong model is trained on data
labeled by the pretrained weak model. Our analysis identifies two regimes --
data-scarce and data-abundant -- based on the signal-to-noise characteristics
of the dataset, and reveals distinct mechanisms of weak-to-strong
generalization. In the data-scarce regime, generalization occurs via benign
overfitting or fails via harmful overfitting, depending on the amount of data,
and we characterize the transition boundary. In the data-abundant regime,
generalization emerges in the early phase through label correction, but we
observe that overtraining can subsequently degrade performance.

</details>


### [62] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA 是一个Python框架，可从多模态突变数据构建与分析适应性地形，提供20个特征描述景观地形四大维度，便于解释与比较各种预测模型，并公开大量组合完全景观。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏地形层面的信息，限制对模型在真实景观中的表现理解。

Method: 构建 GraphFLA，处理 DNA/RNA/蛋白等数据，支持百万级突变体，计算20个生物学相关特征，覆盖4个地形维度；在 ProteinGym、RNAGym、CIS-BP 的5,300+景观上评估多种模型，比较性能；公开 155 个组合完全景观，共 2.2M 序列。

Result: 展示 GraphFLA 在解释和比较模型表现方面的有效性，揭示影响模型准确性的因素，以及不同模型的优势; 代码和数据公开。

Conclusion: GraphFLA 为景观拓扑的解释性评估提供统一工具，促进多模态序列的模型评估与比较，并提供丰富数据集供研究。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [63] [Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT](https://arxiv.org/abs/2510.24829)
*Benjamin Karic,Nina Herrmann,Jan Stenkamp,Paula Scharf,Fabian Gieseke,Angela Schwering*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The integration of the Internet of Things (IoT) and Artificial Intelligence
offers significant opportunities to enhance our ability to monitor and address
ecological changes. As environmental challenges become increasingly pressing,
the need for effective remote monitoring solutions is more critical than ever.
A major challenge in designing IoT applications for environmental monitoring -
particularly those involving image data - is to create energy-efficient IoT
devices capable of long-term operation in remote areas with limited power
availability. Advancements in the field of Tiny Machine Learning allow the use
of Convolutional Neural Networks (CNNs) on resource-constrained,
battery-operated microcontrollers. Since data transfer is energy-intensive,
performing inference directly on microcontrollers to reduce the message size
can extend the operational lifespan of IoT nodes. This work evaluates the use
of common Low Power Wide Area Networks and compressed CNNs trained on domain
specific datasets on an ESP32-S3. Our experiments demonstrate, among other
things, that executing CNN inference on-device and transmitting only the
results reduces the overall energy consumption by a factor of up to five
compared to sending raw image data. %The compression of the model using Post
Training Quantization is accompanied by an acceptable reduction in accuracy of
only a few percentage points compared to a non-quantized model. These findings
advocate the development of IoT applications with reduced carbon footprint and
capable of operating autonomously in environmental monitoring scenarios by
incorporating Embedded Machine Learning.

</details>


### [64] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 在分布外（OOD）鲁棒性研究中，单纯依赖ID与OOD的整体相关性“accuray-on-the-line”可能误导，因为该正相关是聚合多样化OOD样本的结果。本研究提出基于梯度的方法OODSelect，在OOD集合中发现若干语义上相干的子集，在这些子集上ID准确率与OOD准确率的关系并不成立，甚至在某些情况下ID越高、OOD越低。


<details>
  <summary>Details</summary>
Motivation: 当前的OOD基准通常报告ID与OOD准确率之间的正相关性，被解读为“类型无偏差的鲁棒性较好”。然而，样本的异质性和分布偏差可能掩盖了真正的、局部的、以语义为单位的弱鲁棒性。该研究旨在揭示聚合指标的潜在缺陷，并探寻是否存在由ID提升而却降低OOD的子集。

Method: 提出基于梯度的方法OODSelect，用以在广泛的分布偏移基准中识别语义上连贯的OOD子集。通过对模型在不同样本子集上的表现进行梯度分析，筛选出对模型性能影响最大的子集，使得在这些子集上ID与OOD之间的相关性与整体结论不同。

Result: 在多种常用的分布偏移基准上，OODSelect揭示的子集（有时占标准OOD集合的一半以上）中，ID准确率提高并不等价于OOD准确率提高，甚至出现负相关。说明聚合指标掩盖了OOD鲁棒性的重要失配模式。

Conclusion: 聚合度量可能隐藏关键的局部弱点，导致对OOd鲁棒性的误判。文章随附代码和识别出的子集，便于后续研究对OOD鲁棒性进行更细粒度的评估与分析。

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [65] [Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding](https://arxiv.org/abs/2510.24889)
*Shakeel Abdulkareem,Bora Yimenicioglu,Andrea Yang,Khartik Uppalapati,Aneesh Gudipati,Zhaoyang Fan*

Main category: cs.LG

TL;DR: 通过一个自适应多任务的脑电分类器，在床旁对中风进行快速分诊。结合32通道EEG的功率谱特征、GRU-TCN多任务预测与DQN阈值自适应，显著提升中风分型、严重程度与偏侧化的判断，并在独立人群中验证。


<details>
  <summary>Details</summary>
Motivation: 在急性中风的快速现场分诊中，EEG虽有潜力但在初诊阶段未被充分利用，需要一个可在床旁部署、对结果敏感性与特异性进行自适应调整的工具。

Method: 使用Welch方法将32通道EEG转为功率谱密度特征，GRU-TCN实现对中风类型（健康/缺血性/出血性）、半球偏侧化、严重程度的多任务预测，DQN在实时中自适应阈值。采用UCLH Stroke EIT/EEG数据集（44条记录，约26例急性中风，10例对照）进行患者独立分割评价，主结果为中风类型，次级为严重程度与偏侧化；也在独立低密度EEG队列ZJU4H验证。遵循STARD 2015诊断准确性研究指南。

Result: 基线GRU-TCN对中风类型的准确性为89.3%、F1分数92.8%；对严重程度约96.9%、F1 95.9%；对偏侧化约96.7%、F1 97.4%。引入DQN阈值自适应后，中风类型准确性提升至约98.0%、F1 97.7%。在独立低密度EEG队列上也进行了对比分析。

Conclusion: 自适应阈值调整能将工作点推向临床偏好的灵敏度-特异性平衡，同时脑电头皮图和谱可视化提升可解释性；方法具有床旁快速分诊潜力，且具可解释性支持临床信任。

Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools;
EEG is promising but underused at first contact. We present an adaptive
multitask EEG classifier that converts 32-channel signals to power spectral
density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to
predict stroke type (healthy, ischemic, hemorrhagic), hemispheric
lateralization, and severity, and applies a deep Q-network (DQN) to tune
decision thresholds in real time. Using a patient-wise split of the UCLH Stroke
EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the
primary outcome was stroke-type performance; secondary outcomes were severity
and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for
stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)
for lateralization. With DQN threshold adaptation, stroke-type accuracy
increased to about 98.0% (F1 97.7%). We also tested robustness on an
independent, low-density EEG cohort (ZJU4H) and report paired patient-level
statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies
(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;
patient-wise evaluation). Adaptive thresholding shifts the operating point to
clinically preferred sensitivity-specificity trade-offs, while integrated
scalp-map and spectral visualizations support interpretability.

</details>


### [66] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: PESO: 在推荐系统的连贯学习中引入近端正则化的 LoRA 适配器，通过锚定到最近的冻结状态，在适应新行为与保留旧偏好之间取得平衡，理论上提供数据感知的方向性引导；实验上优于现有基于 LoRA 的持续学习方法。


<details>
  <summary>Details</summary>
Motivation: 在推荐场景中，用户、物品和偏好随时间演化，传统的 LoRA 连续学习方法偏重于在过去任务上保持性能，但过去偏好并不等同于当前目标，且在当前兴趣显著偏移时，过度保留历史信息可能降低性能。因此需要一种既能快速适应新行为、又能避免过度遗忘的机制。

Method: 提出 PESO（Proximally rEgularized Single evolving lOra），在 LoRA 适配器上加入近端正则项，使当前适配器锚定于最近的冻结状态，从而在适应性和保留性之间灵活权衡，并更好地捕捉最近的用户行为。理论部分给出该近端设计在 LoRA 子空间中的数据感知、方向性引导。

Result: 实验结果表明，PESO 在多项指标上持续优于现有的基于 LoRA 的持续学习方法。

Conclusion: 通过近端正则化设计，PESO 提升了 LoRA 在推荐场景中的持续适应能力，能更准确地捕捉最近的用户行为，同时控制遗忘。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [67] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 仅少数推理步骤对最终输出具有因果驱动作用，大多数为装饰性推理；存在一个潜在的“真实思维”方向，可通过定向干预影响模型推理与输出；自证（aha）步骤也可能是装饰性的；这对效率与可信度有冲击。


<details>
  <summary>Details</summary>
Motivation: 质疑将链式推理步骤视为内部思维的假设，提出可量化的因果影响来评估推理的真实价值，并探索如何通过潜在方向干预来控制模型输出和提升可验证性。

Method: 提出真思维分数（TTS）来衡量每个推理步骤对最终预测的因果影响；对 Qwen-2.5 在 AIME 数据集上的 CoT 进行分析，测量每步的TTS；发现只有极少数步骤具有高TTS；定位一个潜在的“真思维”方向并对其进行干预以强制或阻止某些步骤；评估自证步骤的装饰性；综合讨论对推理效率和可信度的影响。

Result: 在 AIME 数据集上，平均只有 2.3% 的推理步骤 TTS≥0.7；检测到一个 TrueThinking 方向，可沿该方向推动或抑制特定 CoT 步骤；自证步骤也可能是装饰性的；通过干预可以改变最终结果。

Conclusion: 大多数 CoT 步骤并非真实的因果推理过程，这削弱了 CoT 的效率与可信度；需要对推理过程进行更细粒度的因果分析，并探索通过识别和引导 TrueThinking 方向来提升可控性与可信度。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [68] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 研究发现视觉-语言模型中存在对特定文化输入敏感的神经元，移除这些神经元会显著削弱对应文化问答的表现，提出基于margin的CAS选择器在识别文化敏感神经元上优于概率和熵方法，敏感神经元在解码层聚集，揭示多模态表示的内部组织。


<details>
  <summary>Details</summary>
Motivation: 解释为何需要研究文化敏感性：VLMs在文化情境中的表现不稳健，需要揭示内部是如何编码文化信息的，以及能否通过因果干预识别并分析这些神经元的作用，以提高模型对文化差异的鲁棒性。

Method: 以CVQA基准为基础，识别对特定文化输入具有选择性的神经元，并通过对标记的神经元进行消融测试评估其对不同文化的问题的影响。提出基于margin的选择器CAS，与现有的基于概率和熵的方法比较。对三种VLMs在25个文化群体上进行实验，并进行逐层分析以确定这些神经元的分布特征。

Result: 研究发现存在文化敏感神经元，消融对相应文化的问题表现有显著损害、对其他文化影响甚微。CAS在识别文化敏感神经元方面优于概率/熵方法。层次分析表明这些神经元在解码层有聚集现象，揭示了多模态表征的内部组织。

Conclusion: 结论指出，VLMs内部存在可被局部干预来影响特定文化问答的神经元群，CAS提供更有效的识别手段，层级聚集性提示多模态表示的结构化组织。这为理解文化信息在跨模态模型中的编码提供新的视角，并指向提升跨文化鲁棒性的研究方向。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [69] [Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms](https://arxiv.org/abs/2510.24951)
*Bernhard Klein*

Main category: cs.LG

TL;DR: Resource-efficient, robust ML via algorithm-hardware co-design: layer-wise compression, approximate Bayesian inference, analog/digital hardware deployment, and probabilistic photonic computing.


<details>
  <summary>Details</summary>
Motivation: Need for efficient yet reliable ML on embedded, resource-limited devices, especially under distributional shifts; Bayesian methods provide uncertainty but add cost; bridging algorithm and hardware design is essential.

Method: Galen: automatic layer-wise compression guided by sensitivity analysis and hardware-in-the-loop. Analog accelerators: model device imperfections and extend noisy training to nonstationary conditions. Probabilistic inference: analytic and ensemble approximations integrated into a compiler stack for embedded inference. Probabilistic photonic computing: harness intrinsic analog noise as entropy for fast, energy-efficient probabilistic inference in hardware.

Result: Demonstrates coordinated advances in computation- and hardware-aware ML: more efficient and robust inference on constrained platforms, improved training stability under noise, and integrated tooling for embedded probabilistic inference, including analog and photonic hardware.

Conclusion: Supports the view that algorithm-hardware co-design can jointly advance efficiency and reliability, laying the groundwork for trustworthy, energy-efficient ML systems.

Abstract: While modern machine learning has transformed numerous application domains,
its growing computational demands increasingly constrain scalability and
efficiency, particularly on embedded and resource-limited platforms. In
practice, neural networks must not only operate efficiently but also provide
reliable predictions under distributional shifts or unseen data. Bayesian
neural networks offer a principled framework for quantifying uncertainty, yet
their computational overhead further compounds these challenges.
  This work advances resource-efficient and robust inference for both
conventional and Bayesian neural networks through the joint pursuit of
algorithmic and hardware efficiency. The former reduces computation through
model compression and approximate Bayesian inference, while the latter
optimizes deployment on digital accelerators and explores analog hardware,
bridging algorithmic design and physical realization. The first contribution,
Galen, performs automatic layer-specific compression guided by sensitivity
analysis and hardware-in-the-loop feedback. Analog accelerators offer
efficiency gains at the cost of noise; this work models device imperfections
and extends noisy training to nonstationary conditions, improving robustness
and stability. A second line of work advances probabilistic inference,
developing analytic and ensemble approximations that replace costly sampling,
integrate into a compiler stack, and optimize embedded inference. Finally,
probabilistic photonic computing introduces a paradigm where controlled analog
noise acts as an intrinsic entropy source, enabling fast, energy-efficient
probabilistic inference directly in hardware.
  Together, these studies demonstrate how efficiency and reliability can be
advanced jointly through algorithm-hardware co-design, laying the foundation
for the next generation of trustworthy, energy-efficient machine-learning
systems.

</details>


### [70] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 语言模型的输出在跨提示的矩阵中呈现近似低秩结构；可以用无关提示的线性组合生成目标提示的回应；提出一个普适的低秩抽象并给出学习保障。


<details>
  <summary>Details</summary>
Motivation: 理解并量化语言模型的内在低维结构，提出一个模型无关（基于序列概率建模）的分析框架，以提升生成能力并提供理论理解。

Method: 通过对不同提示-回应集合的 logits 构建矩阵，实证观察其近似低秩性；利用对无关或无意义提示的输出的线性组合来生成目标提示的回应；在理论上分析该近似秩的普遍性，并给出可证明的学习保证。

Result: 实验证据显示大多数现代语言模型在提示-回应空间中具有低秩/近似低秩结构；可以用对无关提示的线性组合来构造目标回应；理论分析与实验结果相吻合，揭示了该抽象的表示能力并给出学习保证。

Conclusion: 语言模型中普遍存在的低秩结构可被 leveraged 用于生成与建模，所提出的抽象具有强表示力并伴随可证明的学习保障。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [71] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 基于博弈论的端到端表格数据特征选择框架，通过将特征视为玩家，利用协同互动和边际贡献评估特征重要性，实现样本选择、冗余消除与高效模型训练，显著降低计算成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 随着数据量指数级增长，传统特征选择计算成本高，需在保留预测能力的同时减少计算资源消耗。

Method: 将特征作为合作博弈中的玩家，通过评估协同作用与边际贡献来确定特征重要性；框架包含样本选择、博弈论特征重要性评估、冗余特征消除、以及优化的模型训练。

Result: 在实验中实现显著的计算量下降，同时保持或接近原模型的预测性能。

Conclusion: 该方法为大规模机器学习提供了一种高效的特征选择解决方案，且源代码可获得。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [72] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion introduces a principled, calibrated risk-aware guidance for diffusion policies in offline RL by treating each denoising step as a likelihood-ratio test between an unconditional prior and a state-conditional policy head; it calibrates a threshold to meet a user-specified Type-I error, enabling a controllable risk budget without retraining and improving return-OOD trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-policy guidance in offline RL relies on heuristic sampling rules lacking a formal risk notion. There is a need for principled, tamper-proof risk control and better handling of out-of-distribution states without changing training.

Method: At inference, view each denoising step as a sequential hypothesis test between the unconditional prior and the state-conditional policy head. Accumulate log-likelihood ratios; gate the conditional mean with a logistic controller whose threshold tau is calibrated under H0 to achieve a desired Type-I level alpha. Training remains vanilla with two heads and epsilon-prediction. LRT guidance works with Q-gradients; states/actions are standardized; report a state-conditional OOD metric; provides a continuum from exploitation to conservatism by blending conditional mean and unconditional mean.

Result: On D4RL MuJoCo tasks, LRT-Diffusion improves the return-OOD trade-off compared to strong Q-guided baselines while honoring the alpha calibration. The approach offers level-alpha calibration, stability bounds, and a return comparison showing when LRT outperforms Q-guidance, particularly when off-support errors dominate.

Conclusion: LRT-Diffusion is a drop-in inference-time method that adds principled, calibrated risk control to diffusion policies for offline RL, enabling an evidence-driven shift from fixed guidance toward controlled risk and a continuum of exploitation-conservatism.

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [73] [Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation](https://arxiv.org/abs/2510.24986)
*Ria Jayanti,Tanish Jain*

Main category: cs.LG

TL;DR: 提出一个同时进行癫痫发作检测与预测的综合方法，在CHB-MIT脑电数据集上评估不同模型，凸显仅看准确率的局限性并展示预测潜力。


<details>
  <summary>Details</summary>
Motivation: 现有仅在发作发生后才检测，限制了早期干预的机会；需要通过建模 EEG 的时序模式来预测即将发生的发作，从而实现更主动的治疗与安全管理。

Method: 对检测：应用KNN、Logistic Regression、Random Forest、SVM等监督学习算法，针对实时监测建立分类器；对预测：使用LSTM捕捉 EEG 时序依赖性，以预测未来发作。数据来自 CHB-MIT Scalp EEG，包含969小时、173次发作，23名患者。

Result: 检测方面：Logistic Regression 90.9%准确率、89.6%召回；Random Forest、SVM在整体准确性94.0%但召回率为0%，表明在样本不平衡时仅看准确率会误导。预测方面：LSTM 达到89.26%准确率。

Conclusion: 该工作展示了开发可实时监测并预测癫痫发作的工具的潜力，推动从被动治疗向主动干预的转变；同时强调需关注类别不平衡和评估指标选择，以避免误导；未来可改进以提升召回率和预测稳定性。

Abstract: In recent years, machine learning has become an increasingly powerful tool
for supporting seizure detection and monitoring in epilepsy care. Traditional
approaches focus on identifying seizures only after they begin, which limits
the opportunity for early intervention and proactive treatment. In this study,
we propose a novel approach that integrates both real-time seizure detection
and prediction, aiming to capture subtle temporal patterns in EEG data that may
indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT
Scalp EEG Database, which includes 969 hours of recordings and 173 seizures
collected from 23 pediatric and young adult patients with drug-resistant
epilepsy. To support seizure detection, we implemented a range of supervised
machine learning algorithms, including K-Nearest Neighbors, Logistic
Regression, Random Forest, and Support Vector Machine. The Logistic Regression
achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced
performance suitable for clinical screening. Random Forest and Support Vector
Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to
detect any seizures, illustrating that accuracy alone is insufficient for
evaluating medical ML models with class imbalance. For seizure prediction, we
employed Long Short-Term Memory (LSTM) networks, which use deep learning to
model temporal dependencies in EEG data. The LSTM model achieved 89.26%
prediction accuracy. These results highlight the potential of developing
accessible, real-time monitoring tools that not only detect seizures as
traditionally done, but also predict them before they occur. This ability to
predict seizures marks a significant shift from reactive seizure management to
a more proactive approach, allowing patients to anticipate seizures and take
precautionary measures to reduce the risk of injury or other complications.

</details>


### [74] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 将Transformer自监督的Change Point Detection与Option-Critic结合，用变化点分段驱动子目标发现与选项学习，从而提升HRL在长时程任务中的样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: HRL面临自动发现语义子目标和终止边界的挑战；通过引入结构先验的变点检测来自动划分轨迹并发现可重用的技能。

Method: 将CPD模块接入Option-Critic框架，使用基于自监督的伪标签把轨迹分段；利用分段信息对终止梯度进行稳定化；通过分段行为克隆预训练子期内策略；对不同选项在CPD定义的状态分区上施加互信息/发散惩罚以实现功能专门化；优化目标在演员-评论家损失基础上叠加结构化辅助损失；选项发现通过CPD分段映射到独立策略。

Result: 在Four-Rooms和Pinball任务上，CPD引导的智能体表现出更快的收敛、累计回报提升，以及显著的选项专门化提升，表明通过变点分段引入结构先验可提升HRL的可解释性、样本效率和鲁棒性。

Conclusion: 将结构先验引入HRL的变化点分段方法能使选项自动发现成为自然结果，提升可解释性和性能；该框架证明了对复杂环境应用中更有效的分层策略的潜力。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [75] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 矩阵白化优化器在多种实现中普遍优于逐元素优化器；性能提升的关键并非仅仅来自谱归一化，而是方差自适应这一被忽视的成分。


<details>
  <summary>Details</summary>
Motivation: 系统性解构现有矩阵白化优化器，揭示哪些成分真正驱动性能提升，尤其区分谱降方向与方差适应在优化过程中的作用。

Method: 对比并系统地消融不同实现（SOAP、Muon等），在超参数范围内重复实验，重点考察谱降方向、谱归一化、方差自适应以及低秩方差估计的影响；评估看前进法、符号梯度等的差异；探索低秩方差估计是否降低内存开销而不损失性能。

Result: 1) 各种矩阵白化变体普遍优于元素级优化如Adam；2) 性能提升并非仅由准确的谱归一化解释，尤其SOAP取得最大每步增益，而Muon更贴近最陡谱降方向；3) 方差自适应是被忽略的关键成分，方差自适应版本普遍优于符号下降版本；4) 消融显示lookahead不如预期有效，但低秩方差估计可在降低内存成本的同时维持性能。

Conclusion: 方差自适应是矩阵白化优化的核心驱动，与谱归一化共同构成优势；未来工作可集中在高效的方差自适应策略与低秧维度的近似方法，以进一步提升内存效率与稳定性。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [76] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 本研究评估 radiomics 与机器学习在MRI分布偏移下的鲁棒性，通过对五种MRI序列、不同获取协议与分割策略、以及观察者变异的系统性分析，比较基于协议不变特征与序列特定特征的模型表现与不确定性。


<details>
  <summary>Details</summary>
Motivation: radiomics 基于机器学习的临床决策支持易受成像协议、定位和分割差异的影响。需要在跨序列和跨协议情境中评估模型鲁棒性与不确定性校准。

Method: 使用包含8个抗干扰特征的XGBoost分类器与基于所有特征的对比模型，在一个16种水果的Phantom数据集上，比较在内域与外域条件下的表现。实验设计包括(1) T2-HASTE、T2-TSE、T2-MAP、T1-TSE、T2-FLAIR五种序列的协议变异；(2) 全部、部分、旋转等分割策略的变异；(3) 观察者间变异。还评估数据集增强对不确定性估计的影响及温度标定（Temperature scaling）。

Result: 使用协议不变特征的模型在分布偏移下保持F1>0.85；使用所有特征的模型在协议变更下性能下降约40%；数据集增强调小了ECE约35%，提升不确定性估计质量且不牺牲准确率；温度标定对校准收益有限。

Conclusion: 基于协议感知的特征选择和受控 phantom 研究，可预测模型在真实世界协议变异下的行为，提供鲁棒 radiomics 模型的实现框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [77] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出一种基于下游因果效应估计的 ADMG 图距离度量，用以衡量未观测混淆下图之间的差异如何扭曲治疗-结果对的因果效应。通过固定识别与符号验证器来量化差异对因果效应 estimands 的影响，并与现有距离度量比较其在扰动下的行为。


<details>
  <summary>Details</summary>
Motivation: 在潜在混淆存在时，单纯比较图结构难以直接反映因果效应估计的稳健性。研究旨在将图结构差异的评估直接与下游因果推断的可识别性与偏差联系起来，以更真实地反映方法改进的实际效用。

Method: 提出基于 ADMG 的图距离，结合“通过固定实现识别”（identification via fixing）和符号化验证器（symbolic verifier），量化不同图差异在治疗-结果对上的因果效应 estimand 的扭曲。系统分析该度量在不同图扰动下的性质，并与现有距离度量进行对比。

Result: 所提距离以对因果效应偏差的直观解释为特征，能区分对下游因果估计影响更大的扰动，并在多种扰动情形下展现与传统距离度量不同的敏感性，强调对下游任务的相关性。

Conclusion: 将图距离与下游因果推断的一致性绑定，有助于更准确评估新提出方法在潜在混淆情形下的实用性与鲁棒性；所提出的分析框架有助于理解图结构变化对因果效应的实际影响。

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [78] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 提出了一种名为 DWMGrad 的自适应优化算法，通过基于历史数据的动态引导机制，动态更新动量和学习率，能够在不同训练场景下自适应地调整对历史信息的依赖，从而实现更快收敛和更高精度。


<details>
  <summary>Details</summary>
Motivation: 现有的 SGD/Adam 在学习率选择、逃离局部极小值、处理高维非凸优化等方面存在局限，难以在复杂数据结构和模型中高效、鲁棒地训练。

Method: 在传统优化方法基础上引入以历史数据为支撑的动态引导机制，动态更新动量和学习率，使优化器在训练过程中自适应地调整对历史信息的依赖，提升对环境和任务复杂度的适应性。

Result: 通过广泛实验，显示 DWMGrad 在多种场景下具有更快的收敛速度和更高的准确性，证实其鲁棒性和普适性。

Conclusion: DWMGrad 提供了一种能随环境和任务复杂性变化自适应的优化策略，有望提升深度学习中复杂模型的训练效率与性能；在多任务与非凸优化中具潜在优势。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [79] [Training Across Reservoirs: Using Numerical Differentiation To Couple Trainable Networks With Black-Box Reservoirs](https://arxiv.org/abs/2510.25074)
*Andrew Clark,Jack Moursounidis,Osmaan Rasouli,William Gan,Cooper Doyle,Anna Leontjeva*

Main category: cs.LG

TL;DR: Bounded Numerical Differentiation (BOND) is a perturbative method to estimate partial derivatives in network structures with inaccessible computational graphs, achieving higher accuracy and scalability than prior perturbative methods, and enabling the integration of fixed, untrained black-box modules to boost performance without adding trainable parameters; it hints at a path to scale networks by combining analogue and digital devices.


<details>
  <summary>Details</summary>
Motivation: To enable gradient estimation and training in architectures where the computational graph is opaque or inaccessible, and to explore whether fixed non-trainable modules can improve performance without increasing model size.

Method: Introduce BOND, a perturbative, bounded numerical differentiation technique that estimates partial derivatives across network structures with inaccessible graphs, offering improved accuracy and scalability over existing perturbative approaches.

Result: BOND demonstrates improved accuracy and scalability; experiments show that fixed, untrained black-box networks can enhance model performance without increasing the number of trainable parameters, and without extensive optimization of the black-box properties.

Conclusion: Fixed, non-trainable modules can expand model capacity, suggesting a path toward combining analogue and digital devices to scale network architectures.

Abstract: We introduce Bounded Numerical Differentiation (BOND), a perturbative method
for estimating partial derivatives across network structures with inaccessible
computational graphs. BOND demonstrates improved accuracy and scalability from
existing perturbative methods, enabling new explorations of trainable
architectures that integrate black-box functions. We observe that these
black-box functions, realized in our experiments as fixed, untrained networks,
can enhance model performance without increasing the number of trainable
parameters. This improvement is achieved without extensive optimization of the
architecture or properties of the black-box function itself. Our findings
highlight the potential of leveraging fixed, non-trainable modules to expand
model capacity, suggesting a path toward combining analogue and digital devices
as a mechanism for scaling networks.

</details>


### [80] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: 提出FairMIB—a multi-view IB framework for fair GNNs, decomposing graphs into feature, structural, and diffusion views; uses cross-view contrastive learning and IB objectives, plus IPW diffusion correction, achieving state-of-the-art fairness-utility on five datasets.


<details>
  <summary>Details</summary>
Motivation: GNNs易放大训练数据中的偏见，现有公平性方法将偏见视为单一来源，忽略特征与结构的不同效应，导致公平性与效用之间的权衡不足。

Method: FairMIB 将图分解为特征、结构、扩散三个视图，利用对比学习最大化跨视图互信息以获得偏见较小的表征；采用多视角条件信息瓶颈目标以最小化与敏感属性的互信息，从而平衡任务效用与公平性；在扩散视图引入逆概率加权（IPW）邻接修正以减缓偏见传播。

Result: 在五个真实数据集上实验，FairMIB 在公正性与任务性能两个维度均达到或接近最优，显示出优越的公平-效用折衷。

Conclusion: 多视角信息瓶颈框架结合对比学习和IPW修正，能更有效地抑制GNN中的复杂性偏见，优于单源偏见处理方法。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [81] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 在混合分布下，训练/测试比例的错配可提升测试性能；存在最优的训练比例，且结论对成分技能分布的设定同样成立。


<details>
  <summary>Details</summary>
Motivation: 理解在分类等任务中，若训练分布和测试分布的比例不一致，是否会带来有益的分布偏移，以及如何确定最优的训练比例。

Method: 对具有不同训练/测试比例的混合分布进行理论分析，推导何时分布偏移有利，给出最优训练比例的条件与值；并将分析扩展到组件技能分布的成分性设定。

Result: 在多种情形中，错配训练比例可提升测试性能；存在可描述的、普遍适用的最优训练比例；该分析同样适用于训练和测试中成分技能分布不同的成分性情景。

Conclusion: 有意通过选择合适的训练比例来提升测试性能的分布偏移是可行且有理据的；该框架可用于包含技能分布差异的成分性设定。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [82] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 提出一个统一的双层优化框架用于对抗学习在聚类中的应用；发现聚类对小扰动鲁棒，对大扰动易改变聚类结果；引入 delta-measure 来量化攻击效果。


<details>
  <summary>Details</summary>
Motivation: 理解对抗攻击的机制并量化其影响，现有对聚类的对抗研究不足，需一个统一的分析框架。

Method: 建立一个统一的双层优化模型用于对抗学习，基于数据扰动角度分析聚类模型的鲁棒性；研究 delta-measure 的良定义性，并将其嵌入上述框架中用于评估攻击效果。

Result: 在扰动较小时，聚类结果保持稳定，表现出鲁棒性；扰动较大时，聚类结果会发生改变，从而形成攻击；delta-measure 在所提框架中具有良定义性，可用于量化攻击效果。

Conclusion: delta-measure 为量化对抗效果提供了一个可用于聚类的度量工具；统一的双层框架有望成为分析聚类中对抗学习的通用方法。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [83] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 提出将数据增强(DA)视作对治疗生成机制的干预，从而在存在未观测混杂时通过IV-like回归进行正则化，以在跨干预的泛化和因果估计中提升性能；并将参数化DA嵌入IVL框架，理论与仿真以及真实数据验证均显示其潜在优势。


<details>
  <summary>Details</summary>
Motivation: 在i.i.d.以外的情形下，如何利用DA提升对因果效应的鲁棒性与泛化能力，特别是在缺乏强IV时使用DA来缓解隐藏混杂带来的偏差。

Method: 将DA视为对治疗生成机制的干预；在输出不变性假设下，将DA看作对因果图中的治疗节点的干预，提出IV-like回归，通过正则化来减轻混杂偏差；将参数化DA与IVL嵌入，组合使用以模拟DA的worst-case应用，提升因果估计与跨干预的泛化能力；给出理论分析（总体）和有限样本仿真，以及真实数据实验。

Result: 理论上说明在总体层面，IVL正则化能降低因果估计中的偏差；在有限样本中，线性示例的仿真显示该方法可改善在多干预情景的鲁棒性；真实数据实验支持其可行性和潜在优势。

Conclusion: 将DA中的数据生成机制与IV框架结合，能够在缺乏强IV时仍实现对干预的泛化与更低的偏差；参数化DA作为IVL回归的一部分，可以通过组合使用实现对复杂情景的更好适应。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [84] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: 提出一种可解释的线性嵌入（linearity grafting）以减少近似误差，从而 Tighten l_infty 本地 Lipschitz 常数，提升认证鲁棒性。并给出理论分析与实验验证。


<details>
  <summary>Details</summary>
Motivation: 认证鲁棒性需要小的 Lipschitz 常数，但寻找最坏对抗样本是 NP-完全问题，现有近似方法误差影响鲁棒性与局部常数；线性 grafting 成为消除主要近似误差的核心手段。

Method: 将线性性 grafting 引入非线性激活，作为消除主要近似误差的手段，给出关于 l_infty 局部 Lipschitz 常数的理论分析，并提出 Lipschitz 感知的线性 grafting 方法，改进认证鲁棒性；不需严格的认证训练。

Result: 实验表明线性 grafting 能缩小 l_infty 局部 Lipschitz 常数，提升认证鲁棒性。

Conclusion: 通过聚焦消除主导近似误差，线性 grafting 提高了鲁棒性证明的可靠性，且在不需要认证训练的情况下也有效。

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [85] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: 提出一种基于机器学习的 OPS 框架，通过利用实例间的共性结构来加速 MILP 求解，在大型加州合成测试系统上实现比传统优化方法更快且解质量接近最优的结果。


<details>
  <summary>Details</summary>
Motivation: OPS 问题本质上是对点火风险的线路去能量化，需在快速、重复的运营场景中求解 MILP；不同实例具有相同的结构但参数不同，存在可通过学习捕捉的共性以提高求解效率的机会。

Method: 在现有 ML 引导的 MILP 求解框架基础上扩展，结合领域知识（如被激活/去激活的线数量等约束）来引导模型，利用跨实例的共性快速产生高质量的断电决策。

Result: 在一个大型、现实感强的 California 基于合成测试系统上验证，ML 指导的方法在解质量接近最优的同时显著提升求解速度，相较于传统优化方法具有优势。

Conclusion: 利用跨实例结构共享的 ML 指导 OPS 求解框架能够实现快速且高质量的决策，适合高风险且需快速响应的电力系统运营场景。

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [86] [Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers](https://arxiv.org/abs/2510.25176)
*Mohammadreza Doostmohammadian,Zulfiya R. Gabidullina,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: Proposes a co-optimization framework for distributed ML resource allocation across a time-varying network, with an all-time-feasible algorithm and log-quantized communication, achieving >50% improvement in cost gap for distributed SVM/regression.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for fast, efficient, and scalable distributed AI. The paper tackles jointly optimizing data processing and CPU allocation across networked nodes with time-varying communication and quantized information, while maintaining feasibility and convergence.

Method: Formulate a co-optimization problem that jointly optimizes data processing and CPU resource allocation across data-distributed nodes. Develop a consensus-type algorithm that remains feasible at all iterations, accommodates log-scale (log-quantized) information exchange over time-varying networks, and analyzes convergence using perturbation theory, Lyapunov stability, and eigen-spectrum methods. Demonstrate on distributed SVM and regression.

Result: The proposed algorithm converges to the optimum, ensures all-time feasibility, handles log-quantized channels, and yields more than 50% improvement in the cost optimality gap compared with existing CPU scheduling solutions (in the tested scenarios).

Conclusion: A scalable, provably convergent distributed optimization framework for jointly balancing data processing and CPU allocation in time-varying networks, robust to quantized communication, with practical performance gains for distributed SVM and regression.

Abstract: In the rapidly evolving research on artificial intelligence (AI) the demand
for fast, computationally efficient, and scalable solutions has increased in
recent years. The problem of optimizing the computing resources for distributed
machine learning (ML) and optimization is considered in this paper. Given a set
of data distributed over a network of computing-nodes/servers, the idea is to
optimally assign the CPU (central processing unit) usage while simultaneously
training each computing node locally via its own share of data. This formulates
the problem as a co-optimization setup to (i) optimize the data processing and
(ii) optimally allocate the computing resources. The information-sharing
network among the nodes might be time-varying, but with balanced weights to
ensure consensus-type convergence of the algorithm. The algorithm is all-time
feasible, which implies that the computing resource-demand balance constraint
holds at all iterations of the proposed solution. Moreover, the solution allows
addressing possible log-scale quantization over the information-sharing
channels to exchange log-quantized data. For some example applications,
distributed support-vector-machine (SVM) and regression are considered as the
ML training models. Results from perturbation theory, along with Lyapunov
stability and eigen-spectrum analysis, are used to prove the convergence
towards the optimal case. As compared to existing CPU scheduling solutions, the
proposed algorithm improves the cost optimality gap by more than $50\%$.

</details>


### [87] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出一种 selective learning 的深度时间序列预测框架，通过双掩码机制在优化中只对子集时间步计算损失以避免过拟合：使用残差熵的不确定性掩码筛选不确定步、使用残差下界估计的异常掩码排除异常步。对八个真实数据集的实验显示对多种模型显著提升，如 Informer 提升 37.4% 的 MSE、TimesNet 提升 8.4%、iTransformer 提升 6.5%。


<details>
  <summary>Details</summary>
Motivation: 深度时序预测易受噪声与异常影响，传统的 MSE 在所有时间步上统一优化，导致对不可泛化步的过拟合。需要一种在训练时就能区分“通用/易泛化”与“非通用/异常”步的学习策略。

Method: 引入双掩码机制的 selective learning：在优化中仅对部分时间步计算 MSE。第一步不确定性掩码基于残差熵筛选不确定步；第二步异常掩码基于残差下界估计排除异常步。这样可引导模型聚焦在更具泛化性的时间步，从而降低过拟合。

Result: 在八个真实数据集上对多种深度模型（包括 Informer、TimesNet、iTransformer）进行了广泛实验，分别实现了显著的性能提升：Informer 37.4% 的 MSE 降低、TimesNet 8.4%、iTransformer 6.5%。

Conclusion: Selective learning 能有效提升深度时序预测模型的泛化能力，降低对噪声和异常的敏感性，且框架具备对多种模型的适用性。

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [88] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出一种基于自适应损失加权的成本敏感多类正无标签学习方法，确保经验风险是目标风险的无偏估计，并给出一般化误差界，与八个公开数据集的实验结果显示在准确性和稳定性方面优于强基线。


<details>
  <summary>Details</summary>
Motivation: 多类PU学习中难以获得无偏风险估计，现有方法往往在多类场景不稳定或偏差较大。本研究旨在构建一个可在经验风险框架内对正样本和来自未标注混合的推断负样本分配不同数据相关权重的策略，以实现无偏风险估计并提升鲁棒性。

Method: 在经验风险最小化框架下，为正样本和推断负样本的损失分配数据相关权重，使其加权后的经验风险等价于目标风险的无偏估计。 formalize MPU 数据生成过程，推导并给出一般化误差界；通过自适应权重来应对不同类先验与未标注噪声。

Result: 在八个公开数据集（覆盖不同类数和先验比）上，提出的方法在准确性和稳定性上对比强基线表现出一致的提升。

Conclusion: 自适应损失加权的成本敏感MPU方法实现了无偏风险估计、具备理论保证并在实践中展现显著的稳定性和准确性提升。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [89] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: BSFA 基于子空间分离的更新缩放，结合 PCA 子空间估计与分块策略，在深度学习训练中显著提升速度与稳定性，实验达到约 2x 加速。


<details>
  <summary>Details</summary>
Motivation: 揭示 Hessian 前导特征方向（Dom-space）对更新贡献大但对损失下降作用小，以及正交的 Bulk-space 更新虽小却驱动大部分学习进展；需要一个可实用的框架来区分并调控这两种分量以提升训练效率。

Method: 提出 Bulk-Space-Filtration-Accelerator (BSFA) 插件式框架，通过对投影到两子空间的更新分量进行差异化缩放来加速训练；采用对历史更新进行 PCA 的高效子空间估计，并在参数块上实现分块策略以提高可扩展性。

Result: 在多任务/数据集上展示加速效果，显著优于常用优化器；具体在预训练 LLaMA-72M 于 WikiText-103 和 LLaMA-134M 于 OpenWebText 的对比中实现约 2x 的训练加速。

Conclusion: BSFA 提供一种实用、可扩展的插件式加速方法，通过调控 Dom-space 与 Bulk-space 更新的幅度，实现更稳定且更快的收敛，适用于现代大模型训练。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [90] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈的归一化方法IBNorm，通过有界压缩来在保持预测信息的同时抑制无关变异，在稳定性与兼容性不变的前提下提升信息利用效率与泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的BatchNorm、LayerNorm、RMSNorm等归一化方法以方差为中心，强制零均值与单位方差，可能无意中削弱对任务相关信息的捕获。需要一种在保持训练稳定性的同时更好保留任务关键信息的归一化策略。

Method: IBNorm引入受信息瓶颈原理启发的有界压缩操作，促使嵌入在保持预测信息的同时抑制无关可变性；理论上证明IB值更高、泛化界更紧；在大规模语言模型（如LLaMA、GPT-2）与视觉模型（如ResNet、ViT）上对比BatchNorm、LayerNorm、RMSNorm，且互信息分析表明具备更强的信息瓶颈特性。

Result: 实证结果表明IBNorm在语言模型和视觉模型的多项基准上持续优于对比归一化方法；互信息分析验证了更强的信息瓶颈行为。

Conclusion: IBNorm提供一个简单且强大的归一化家族，在提升信息表达能力的同时保持训练稳定性和与现有框架的兼容性；将公开代码。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [91] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: 提出一个分层物理嵌入学习框架，通过两级结构与自适应傅里叶神经算子，在保留已知物理规律的前提下，进行前向预测与反演发现；实现对复杂PDE的高效、物理一致的学习与解释性符号回归。


<details>
  <summary>Details</summary>
Motivation: 远离平衡态的复杂时空动力学的PDE往往难以从第一性原理推导，纯数据驱动模型缺乏物理一致性，传统物理信息方法在表示复杂算子和系统整合方面能力不足，因此需要一种能结合先验物理知识、实现可解释发现的新框架。

Method: 提出两级架构：第一层学习PDE的符号组成部分，第二层学习它们的控制组合；将已知物理规律直接嵌入计算图以保证物理一致性；在自适应傅里叶神经算子(Adaptive Fourier Neural Operators)基础上捕捉非局部依赖与高阶算子；通过结构化地分离已知与未知项，利用符号回归实现对潜在控制方程的解释性发现，无需事先指定函数形式。

Result: 该框架能够实现对复杂时空过程的前向预测与反演物理规律的发现，在数据稀疏、噪声条件下提高数据效率和物理一致性；能够捕捉非局部关系与高阶算子；促进对潜在控制方程的可解释发现。

Conclusion: 分层分解显著降低学习复杂性，提供一个可解释且物理一致的发现框架，能够在有限数据下稳健地学习并揭示隐藏的物理规律。

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [92] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 提出了多目标强化学习（Multi Goal RL），在最大化期望回报的同时实现目标状态边际分布的均匀分布。通过一个定义目标集合的伪分类器来界定目标状态，学习一个高回报的策略混合，使边际分布在目标集合上分散。通过对当前混合策略计算的自定义奖励，结合离线RL更新策略，给出理论收敛性保证，并在合成MDP与标准RL环境中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在多种情境下，期望策略应对奖励状态实现分散覆盖，而非仅聚焦少数奖励源。现有方法要么通过熵正则或内在奖励促进探索，但不一定实现目标状态分布的均匀性；要么假设目标分布已经可用且在大规模状态空间难以枚举。本文在未知目标集合下，通过一个目标状态伪分类器定义 Multi Goal RL，目标是最大化回报同时使目标状态的边际分布尽量均匀。

Method: 提出学习高回报策略混合（policy mixture）的方法，目标是在目标集合上实现边际分布分散的同时保持高回报。为当前混合策略计算一个自定义奖励，并对采样轨迹进行评估；利用离线RL算法据此更新策略混合。给出对该目标函数的性能保证与收敛界限。

Result: 理论上给出优化目标（回报与目标分布 disperse）的收敛性和性能界限；在合成MDP和标准RL环境中通过实验验证该方法能够在保持较高回报的同时实现目标状态分布的分散性。

Conclusion: 提出一个可扩展的多目标RL框架，适用于大规模状态空间，能够同时最大化回报并使目标状态边际分布分散，具备理论保证与实验验证，适用于需要广泛覆盖目标状态的应用。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [93] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: Introduces CDFlow, an invertible linear layer as a product of diagonal and circulant matrices, reducing parameters and enabling FFT-based speedups; enables scalable, expressive normalizing flows with strong density estimation on natural images.


<details>
  <summary>Details</summary>
Motivation: Normalizing flows require expressive yet efficiently computable linear layers. Full dense matrices are parameter- and computation-heavy; a circulant-diagonal decomposition can greatly reduce parameters while preserving expressiveness. FFT-based techniques can further accelerate inverses and determinant calculations, enabling scalable generative modeling.

Method: Factorize the invertible linear layer into a chain of m diagonal matrices interleaved with m-1 circulant matrices. This yields parameter count O(mn) instead of O(n^2). Use Fast Fourier Transform to compute matrix inverses in O(mn log n) and log-determinants in O(mn). Build Circulant-Diagonal Flow (CDFlow) atop this layer, achieving strong density estimation on natural image datasets and capturing data with periodic structure.

Result: CDFlow achieves competitive density estimation on natural image benchmarks and effectively models data with inherent periodic structure while substantially accelerating core NF operations, making scalable normalizing flows more practical.

Conclusion: A practical, efficient inductive bias for linear layers in normalizing flows. The circulant-diagonal factorization balances expressiveness and efficiency, enabling scalable generative modeling on large datasets; future work could explore extensions to other structured matrices or deeper architectures.

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [94] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 给出随机几何超图上变分学习的渐近一致性分析，证明在某些条件下学习是良定性的并收敛到加权 p-拉普拉斯方程；并提出 HOHL，通过骨架图拉普拉斯算子的幂次正则化实现多尺度平滑，理论与实证均有积极表现。


<details>
  <summary>Details</summary>
Motivation: 填补超图在半监督学习中的理论基础空缺，尤其缺乏对渐近性质和高阶平滑性的分析；同时利用高阶交互来提升学习性能。

Method: 对随机几何超图进行变分学习的渐近分析，推导其极限收敛到加权 p-拉普拉斯方程；提出 Higher-Order Hypergraph Learning (HOHL)，通过骨架图的拉普拉斯算子的幂次正则化实现多尺度平滑；理论上 HOHL 收敛到高阶 Sobolev 半范数。

Result: 给出学习的良定性条件与渐近收敛结果；HOHL 在标准基线数据集上表现出色，具备实证优越性。

Conclusion: HOHL 将超图学习的理论收敛性与多尺度正则化结合，提供一种在高阶交互建模中有效的半监督学习框架，具备理论渐近性质与良好实验表现。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [95] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 通过加权参数平均的模型合并解决KGE模型集合的成本与性能问题：在训练中维护模型参数的滑动平均作为预测基础，并提出仅在验证集上泛化性能提升时才更新该平均参数的策略。两种加权平均方法与最先进的集成方法在多种链接预测任务和字面增强/多跳查询场景中均显示出性能提升。


<details>
  <summary>Details</summary>
Motivation: 集成学习常用于提升知识图谱嵌入（KGE）的泛化和链接预测性能，但传统的集成方法需要训练多模型，代价高且延迟大。模型合并（如权重平均）提供无额外训练成本的替代方案，以提高预测性能。

Method: 在KGE训练过程中维护一个参数的滑动平均作为 ensemble 预测；提出一种策略仅在验证集上泛化性能提升时才更新滑动平均参数。将两种加权平均方法与现有的最优基线的集合方法进行比较，并在字面增强的KGE模型与多跳查询任务上评估；结果显示 weighted averaging 在各种设置下改进性能。

Result: 两种基于权重平均的模型合并方法在链接预测及相关任务中均实现性能提升，相比传统的 ensemble 方法降低了训练成本与延迟，并在字面信息增强和多跳查询任务中表现出稳定的提升。

Conclusion: 模型合并中的加权平均策略是对KGE的一个有效且高效的替代方案，能够在不增加训练负担的情况下提升链接预测性能，并对多种任务设置具备良好鲁棒性。

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [96] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 提出一个两阶段优化器，基于局部凸性转换在非凸区域使用 Adam，在凸区域使用共轭梯度(CG)，通过梯度范数对损失的依赖来检测切换点，从而提升收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: 现实任务的损失表面常在初期呈非凸，向最优点靠近时趋于凸性。单一优化器难以同时兼顾探索性和收敛性，本文利用凸性转换特征来提高优化效率。

Method: 提出一个检测切换点的两阶段框架：通过观察梯度范数随损失的关系来判定局部凸性；在检测到非凸区域时使用 Adam，在凸区域使用 CG，并在运行时进行切换以实现更快的收敛。

Result: 数值计算实验表明，这一凸性结构在实际任务中频繁出现，所提出的两阶段方法能够显著提升收敛速度和最终精度。

Conclusion: 基于局部凸性转换的两阶段优化策略在现实任务中具备可行性和有效性，具有潜在的广泛适用性。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [97] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: 用LLM进行连续黑箱优化的GPTOpt，通过在合成数据上微调模型，使其无需手工调参即可在多种优化任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 全球优化中昂贵且无导数的黑箱函数需要高样本效率；传统BO需要对不同任务进行参数调优，灵活性不足；LLMs具备强大推理能力，但难以直接用于连续优化，因此需要一种能让LLMs直接执行优化的框架。

Method: 通过在覆盖广泛BO参数设置的合成数据集上对大型语言模型进行微调，使其在推理阶段具备连续黑箱优化能力；利用预训练使模型能泛化到不同优化任务；在多项BO基准上对比传统优化器，展示优势。

Result: GPTOpt在多种黑箱优化基准上超越传统优化器，体现了LLMs在数值推理方面的能力，并为无需参数调优的全局优化提供了灵活框架。

Conclusion: LLMs在有限调参的情况下也能解决连续黑箱优化问题，可能改变全局优化的工作流及跨任务泛化能力，后续可拓展到更多应用场景。

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [98] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 提出了“效用校准”框架用于多分类校准的可扩展评估，统一并再解释现有校准指标，支持更鲁棒的顶类与类级校准，以及对下游效用的更丰富评估。


<details>
  <summary>Details</summary>
Motivation: 多分类预测的校准是可信度的基本要求；现有校准评估聚焦于特定方面（如顶类置信度、类内校准）或采用计算量较大的变分形式，难以在实际决策场景中鲁棒且可扩展地评估校准。

Method: 提出通用的“效用校准”框架：以特定效用函数为基准来度量校准误差，能够统一并解释现有的校准度量，提升顶类与类级校准的鲁棒性，并扩展到更丰富的下游效用。

Result: 该框架实现对多分类校准的统一度量，提供更鲁棒的顶类与类级校准变体，并可覆盖更丰富的下游决策效用。

Conclusion: 效用校准为评估多分类预测的校准提供灵活、可扩展的工具，使校准评估更贴合用户目标，超越简单的二值化度量。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [99] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: GWA is a training-intrinsic metric that measures the alignment between per-sample gradients and model weights to track generalization and attribute performance to training samples without a validation set.


<details>
  <summary>Details</summary>
Motivation: Need a robust, training-data-driven validation metric that can explain generalization and sample contributions during training without relying on a held-out validation set.

Method: Define Gradient-Weight Alignment (GWA) as the coherence between the gradient contributed by each training sample and the current model weights; compute efficiently during training to monitor learning dynamics and sample-level influence.

Result: GWA correlates with generalization, can predict optimal early stopping, enables principled model comparisons, and identifies influential training samples, providing a validation-set-free analysis from the training data.

Conclusion: GWA offers a practical, interpretable, and efficient metric that links training dynamics to generalization and sample-level effects, useful for monitoring and analysis during training.

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [100] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: Prototypical neurosymbolic architectures curb shortcut reasoning by grounding models in prototypical concepts, enabling learning of correct concepts for right reasons under scarce data, validated on rsbench across synthetic and real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Neurosymbolic models suffer from shortcut reasoning due to reliance on spurious correlations; need methods that ensure concepts align with background knowledge, especially under low supervision.

Method: Leverage prototypical learning to train models to satisfy background knowledge by comparing inputs to a small set of labeled prototypes, guiding concept learning and debiasing toward the right concepts; validate on rsbench benchmarks across MNIST-EvenOdd, Kand-Logic, BDD-OIA.

Result: Significant improvements in acquiring correct concepts with very scarce supervision; better alignment with symbolic constraints, reduced reliance on spurious correlations, demonstrated on both synthetic and real-world tasks.

Conclusion: Prototype grounding is an effective, annotation-efficient strategy for safe and reliable neurosymbolic learning, enabling models to learn the right concepts for the right reasons.

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [101] [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)
*Vladyslav Moroshan,Julien Siems,Arber Zela,Timur Carstensen,Frank Hutter*

Main category: cs.LG

TL;DR: TempoPFN 是一个基于线性 RNN 的单变量时序基金会模型，纯合成数据训练，利用 GatedDeltaProduct 与 state-weaving 实现全并行训练/推理，在 Gift-Eval 零样本任务中领先同类合成数据方法并超越多数真实数据模型，且比基线更高效；并开源数据生成与训练代码。


<details>
  <summary>Details</summary>
Motivation: 解决零样本时序 forecasting 的长序列预测与可重复性挑战；现有仅使用合成数据的方法在困难基准上表现不足，需一个高效、可重复的基金会模型方案。

Method: TempoPFN 基于线性 RNN 的单变量时序基金会模型，采用 GatedDeltaProduct 架构与 state-weaving 实现跨序列长度的完全并行训练，避免窗口化或摘要化，并具备稳健的时间状态跟踪。数据层面构建了一个综合的合成数据管线，整合随机微分方程、高斯过程和音频合成及其新增增强。评测在 Gift-Eval 的零样本任务中完成。

Result: 在 Gift-Eval 零样本评测中达到顶级竞争力，优于所有现有的仅合成数据方法，并超过大多数使用真实数据训练的模型；由于全并行化训练与推理，相比基线更高效。

Conclusion: 开源完整的数据生成管线和训练代码，为未来研究提供可重复的基础。

Abstract: Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.

</details>


### [102] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 本研究通过机器学习预测工作场所倦怠风险，比较了KNN、随机森林和SVM三种模型，在30折交叉验证下，SVM以R2=0.84表现最佳，并提供了基于Streamlit的交互界面用于非技术用户使用。


<details>
  <summary>Details</summary>
Motivation: 倦怠对个体福祉和组织绩效有显著影响，利用公开数据集对倦怠风险进行早期预测，推动数据驱动的心理健康干预与管理决策。

Method: 使用HackerEarth员工倦怠挑战数据集，比较KNN、随机森林、SVM三种监督学习算法；采用30折交叉验证评估，性能指标为决定系数R^2；通过配对t检验比较模型间差异；实现Streamlit交互界面以便非技术用户输入数据并获取预测。

Result: 在比较的模型中，SVM取得最高预测性能（R^2=0.84），并在配对t检验中显著优于KNN和随机森林。

Conclusion: 研究显示机器学习可支持倦怠的早期检测，推动组织层面的数据驱动心理健康策略，且通过易用界面提高实际应用性。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [103] [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
*Amin Parchami-Araghi,Sukrut Rao,Jonas Fischer,Bernt Schiele*

Main category: cs.LG

TL;DR: 提出一种更 faithful 的、与模型内在机制相连的概念解释框架，概念在不同类别之间共享，能从任意层追踪其对输出的贡献及输入可视化；引入新的 concept-consistency 指标 C^2-Score 用于评估概念方法的一致性；实验显示所提出的概念在量化一致性和可解释性方面优于以往方法，同时保持了 ImageNet 的竞争性能。


<details>
  <summary>Details</summary>
Motivation: 现有的后验、基于人类直觉的概念解释往往不忠实于模型，且对概念的假设（如类别特定性、局部空间尺度、与人类期望对齐）过于苛刻。本研究旨在提出与模型内在机制相连的、跨类别共享的概念解释，并提供可追溯的贡献和输入可视化，以提升解释的忠实性。

Method: 提出 model-inherent mechanistic concept explanations：概念在模型不同层中被共享，能够从任意层追踪它们对 logit 的贡献以及相应的输入可视化；并借助 foundation models 引入 C^2-Score 概念一致性指标，用于评估概念方法的忠实性与一致性。

Result: 与前人工作相比，所提出的概念在量化的一致性上更高，用户认为其更易于解释；同时在 ImageNet 上保持竞争性能。

Conclusion: 通过将概念解释与模型内在机制绑定、跨层与跨类别复用，并提供新的评估指标，能够提高概念解释的忠实性与可解释性，同时不显著牺牲分类性能。

Abstract: Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.

</details>


### [104] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出基于核引导互信息的多头注意力机制，针对多父节点的有向无环图(DAG)学习隐藏图结构。证明单层多头变换器在给定KG-MI目标下可在多项式时间收敛到全局最优，并在KL发散下能恢复真实邻接矩阵。实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 目前对 transformer 的训练动力学理论理解主要限于树结构，难以推广到具有多父节点的一般 DAG。需要一个能够让不同注意力头学习不同父子关系的训练目标。

Method: 提出信息论度量核引导互信息(KG-MI)，基于f-发散。将KG-MI与多头自注意力结合，每个头对应一个独立的边缘转移核，用以建模不同的父子依赖关系。给出理论结果：在K父节点DAG生成的序列上，单层多头 transformer 通过梯度上升可在多项式时间内收敛到全局最优，并表征收敛后的注意力分布模式。在将f-发散特化为KL时，学习到的注意力分数能准确反映真实邻接，能够从而恢复底层图结构。

Result: 理论层面：证明在多头设定下的KG-MI目标可实现全局最优收敛，且收敛时的注意力模式具有可解释性；特定于KL时可恢复真实邻接矩阵。实验结果支持理论结论。

Conclusion: 该方法为带有多父节点的 DAG 结构学习提供了可证明的训练动力学与可解释的注意力模式，并且在KL情形下能落地地重构底层图结构，具有较强的理论和实验支撑。

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [105] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: 提出一个统一框架，将随机PAC-Bayes的单次假设保证转化为对单一确定性假设的保证；包含一般的oracle bound、数值界以及对多数投票的特化，实验证明在确定性分类器的泛化界限上优于基线。


<details>
  <summary>Details</summary>
Motivation: PAC-Bayes在处理无穷假设空间时，通常给出随机假设的期望风险界，但难以直接用于单一确定性分类器。需要将随机保证转化为对单一假设的可用保证。

Method: 提出一个统一框架，推导通用的oracle bound，进一步推导数值界，并对多数投票进行特化；给出从随机PAC-Bayes到单一假设保证的转化方法。

Result: 得到可操作的单一假设界（包括oracle bound和数值界），并对多数投票情形给出特化；实验显示相较于主流基线，在确定性分类器的泛化界上性能提升，达到约2倍。

Conclusion: 提供了从随机PAC-Bayes保证出发，获得单一确定性假设的实用泛化保证的统一框架，特别在确定性分类器上表现更优。

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [106] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 对 A^{-1} 的最佳秩-p近似的谱范数扰动进行非渐近界限分析，在噪声观测下给出与本征间隙、谱衰减和噪声对齐相关的收敛界，创新地将轮廓积分应用于非全纯函数 f(z)=1/z，并在实험上与真实误差吻合，相比传统全逆界限提升可达 sqrt(n)。


<details>
  <summary>Details</summary>
Motivation: 理解在观测噪声下，低秩逆近似的谱范数鲁棒性，尤其是 Eigengap 与谱衰减对误差的影响，以及在可扩展计算中对低秩逆近似提供谱相关的 guarantees。

Method: 考虑对称矩阵 A，A_p^{-1} 为 A^{-1} 的最佳秩-p近似，tilde A = A + E。给出非渐近的扰动界，依赖于 eigengap、谱衰减和噪声与低曲率方向的对齐；使用轮廓积分技术处理非全纯函数 f(z)=1/z 的新应用，获得相较于朴素全逆界限的改进。

Result: 给出尖锐的非渐近扰动界，误差随特征间隙、谱衰减和噪声方向对齐而变化；对比朴素界限，改进幅度可达约 sqrt(n)；实验表明界限紧贴真实扰动，优于经典结果的过度预测。

Conclusion: 提供对于噪声环境下的低秩逆近似的谱相关保证，结合谱结构与噪声信息的实际分析框架，具有实践意义。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [107] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 将Sobolev IPM在图结构上的推广引入Orlicz几何结构，提出带Musielak正则化的广义Sobolev IPM（GSI-M），通过图结构实现单变量优化，计算效iciency显著领先于OW，在文档分类和拓扑数据分析等任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于L^p几何的Sobolev IPM在利用额外几何先验方面受限，难以融入除L^p以外的结构信息。通过引入Orlicz几何并建立与Musielak范的联系，扩展可表达的几何先验，同时解决计算难题。

Method: 以Orlicz-Wasserstein与广义Sobolev传输为理论基础，定义Orlicz-Sobolev范并引入Musielak范正则化来构建GSI；结合图结构，将GSI-M简化为单变量优化问题以实现高效计算。

Result: GSI-M的计算速度比流行的OW高出数个数量级，在给定图上的概率测度比较、文档分类以及拓扑数据分析任务中展现实际优势。

Conclusion: 框架在超越L^p的Sobolev IPM推广方面取得进展，允许更丰富的几何先验；尽管存在计算挑战，Musielak正则化带来高效的单变量优化，使得在实际应用中具备可行性。

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [108] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出一种基于核分数的慎重不确定性度量框架，覆盖总、不确定性中的可辨识性、本质不确定性与认知不确定性，统一多种量度并可通过核的选择设计新的度量，适用于回归任务的风险场景。


<details>
  <summary>Details</summary>
Motivation: 回归任务中的不确定性量化在安全关键领域尤为重要，但现有文献多聚焦于分类任务。文献缺乏统一且可定制的回归不确定性度量。本文以核分数为核心，提出一个涵盖总不确定性、可辨识性不确定性与本质不确定性的框架，利用核的选择来控制尾部敏感性、鲁棒性以及对分布外样本的响应性，给出与具体任务绑定的设计准则。

Method: 提出一族基于适当评分规则的回归不确定性度量，重点强调核分数；该框架统一了若干已知度量并提供设计新度量的 principled recipe；通过分析核分数特征与下游行为的对应关系，给出面向任务的设计指南。

Result: 给出核分数特征与下游性能之间的明确对应关系，提出具体的设计指南；大量实验表明所提出的度量在下游任务中有效，并揭示不同实例在鲁棒性与分布外检测等方面的权衡。

Conclusion: 基于核分数的回归不确定性度量框架可通过核的选择实现对尾部敏感性、鲁棒性和对分布外样本的响应性的可控设计，为任务特定需求提供量化的、可定制的工具。

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [109] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 在不同粒度下对FP与INT量化进行系统比较，发现8位细粒度（如 MXINT8）在准确性与硬件效率上优于MXFP8；4位时FP通常更准，但通过 Hadamard 旋转等对策，NVINT4 可超越 NVFP4。引入对称裁剪缓解细粒度INT训练的梯度偏差，使 MXINT8 训练几乎无损。结论：未来AI加速器应优先考虑细粒度INT格式，MXINT8在精度与功耗之间提供更好折中。


<details>
  <summary>Details</summary>
Motivation: 当前大模型对低精度格式的需求上升，且NVIDIA等厂商已推动FP低精度，但尚无一个统一的FP与INT在不同粒度上的系统对比，导致算法与硬件共设计缺乏清晰指导。

Method: 对多种精度/粒度的FP与INT格式进行系统性对比，覆盖8位和4位的细粒度（如 MX、NV 的格式），并评估在常用技术如 Hadamard rotation、对称裁剪等对精度和训练稳定性的影响，同时从算法与硬件实现层面分析性能权衡。

Result: 在8位细粒度下，MXINT8 优于 MXFP8，且在硬件效率与算法精度方面均占优；在4位场景下，FP格式通常更具准确性，但通过 Hadamard rotation 等出驱动策略，NVINT4 可超过 NVFP4；引入对称裁剪显著缓解细粒度INT训练的梯度偏差，实现 MXINT8 训练的近乎无损表现。

Conclusion: 细粒度INT格式，尤其 MXINT8，为未来AI加速器提供了更好的准确性与功耗折中，挑战了“一刀切”的FP路线，强调在设计时需纳入按块或按细粒度的INT方案。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [110] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: VLA模型在进行动作微调时会导致视觉表示的退化，本文通过对隐藏表示、注意力图的系统分析，对比VLA与VLM，提出对齐视觉表示的策略，提出一种简单但有效的方法缓解退化并提升OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 明确在把预训练的视觉语言模型扩展到动作模态时，是否能保留原有VL知识与视觉语言 grounding。探究动作微调对VL能力的影响及其权衡，以提升跨分布泛化能力。

Method: 对VLA模型的隐藏表示和注意力进行系统探测；设计一组有针对性的任务，与VLM进行对照，揭示动作微调引起的VL能力变化；评估多种对齐视觉表示的策略，提出一种简单有效的缓解退化的方法。

Result: 发现直接进行动作微调会导致视觉表示退化；通过对齐策略可以缓解退化，提升对OOD场景的泛化；提供了对VL能力保持/恢复的实践性方法并分析其中的权衡。

Conclusion: 动作微调与VL表示的退化之间存在权衡，但通过合适的对齐和微调策略，可以在保持VL能力的同时实现行动能力的提升，进而增强OOD泛化。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [111] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: FedLap通过在谱域利用拉普拉斯平滑实现对图结构子图的隐私友好联邦学习，兼顾隐私与可扩展性，理论隐私分析与实证表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的图结构联邦学习要么需要交换敏感的节点嵌入，带来隐私风险，要么依赖计算成本高的步骤，难以扩展；需要一种能利用全局结构信息、且保护隐私的高效方案。

Method: 提出FedLap框架，在谱域上通过拉普拉斯平滑编码跨节点依赖关系，避免直接分享节点嵌入；包含对隐私性的 formal analysis，显示隐私保护性；在子图FL场景下具有强隐私保障和良好可扩展性。

Result: 在基准数据集上进行广泛实验，FedLap在效用方面达到与现有方法相当或优越的表现；提供与第一款具有强隐私保障的子图FL方案相关的结果。

Conclusion: FedLap为图结构数据的子图联邦学习提供了一种隐私保护、可扩展的谱域拉普拉斯平滑方法，兼具良好理论与实证性能。

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [112] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: 提出对称矩阵谱范数扰动的新高概率界，改进Eckart–Young–Mirsky，揭示A与对称扰动E的相互作用对前p近似的影响，并提升差分隐私PCA的公用性保证。


<details>
  <summary>Details</summary>
Motivation: 理解噪声/测量误差对低秩近似在谱范数下的影响，差分隐私场景需要在保护隐私的同时尽量保留前p子空间结构，谱范数提供最强的方向性误差界。

Method: 基于复杂分析中的轮廓自举(contour bootstrapping)方法，推导对称矩阵的谱范数扰动界，扩展到多项式、矩阵指数等谱函数，明确A与对称扰动E的相互作用。

Result: 在入门条件（轻微的本征间隙和范数条件）下，给出||(A+E)_p - A_p||的高概率界，误差可达到比经典界更紧，提升幅度可达√n；实验表明界贴近真实谱误差。

Conclusion: 提供一个广泛适用的分析框架，适用于谱函数的研究，并在差分隐私PCA等应用中取得改进的公用性保证，且与实证数据相符。

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [113] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: MIL 方法往往忽视上下文信息，本文通过一个需要相邻实例特征的合成任务，量化常规 MIL 及新型相关 MIL 在接近贝叶斯最优解时的差距，并在大规模数据下显示泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，局部块之间的空间/时间上下文对分类很重要，但现有 MIL 常将实例独立处理。通过合成任务评估这一假设的影响，并与贝叶斯最优解对比。

Method: 提出一个需要相邻实例信息的合成分类任务；将现有 MIL 方法和新型相关 MIL 方法与闭式贝叶斯估计进行比较；在包含数万实例的数据集上训练，评估泛化能力。

Result: 主流 MIL 及相关 MIL 在接近贝叶斯最优方面仍有显著差距；即使在大规模训练下，泛化性能也未达到最优；结果强调了上下文建模的重要性及现有方法的局限性。

Conclusion: 需要设计更能捕捉邻近实例关系的 MIL 模型，以充分利用上下文信息；本文的合成任务提供了一个可控的评估基准来推动该方向的发展。

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [Scheduling Your LLM Reinforcement Learning with Reasoning Trees](https://arxiv.org/abs/2510.24832)
*Hong Wang,Zhezheng Hao,Jian Luo,Chenxing Wei,Yao Shu,Lei Liu,Qiang Lin,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: 引入Reasoning Score（r-score）评估查询的学习难度，基于推理树结构构建Reasoning Tree Schedule（Re-Schedule）课程表，在RLVR数据调度中实现从简单到复杂的 Curriculum，实验在六个数学推理基准上提升平均准确率至多3.2%。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR数据调度多基于路径（token或查询路径）的指标，忽略推理树的结构信息，难以充分衡量查询的学习难度与数据需求，导致数据利用效率不高。
通过引入结构化的推理树视角，提出更精准的学习难度评估与调度策略。

Method: 1) 定义基于推理树结构的学习难度度量r-score；2) 基于r-score设计Re-Schedule调度算法，构建从结构上简单（高r-score）到复杂（低r-score）的课程；3) 将该调度与RLVR数据采样/训练过程结合，提升数据效率与模型表现。

Result: 在六个数学推理基准上，Re-Schedule显著提升平均准确率，提升幅度最高可达3.2%。

Conclusion: 以推理树结构为基础的学习难度评估与课程化数据调度能为RLVR提供更强的理论与实证支持，结构化的推理树理解是提升数据调度效果的有力基础。

Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large
Language Models (LLMs) can be conceptualized as progressively editing a query's
`Reasoning Tree'. This process involves exploring nodes (tokens) and
dynamically modifying the model's policy at each node. When combined with data
scheduling, this process yields further gains in data efficiency and accuracy.
However, existing RLVR data scheduling methods typically rely on path-based
metrics to rank queries, overlooking the reasoning tree structures of these
queries. In this paper, we introduce a novel metric, namely Reasoning Score
(r-score), which measures the query's learning difficulty based on the
structure of its reasoning tree. Based on the r-score, we propose the Reasoning
Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a
curriculum progressing from structurally simple (high r-score) to complex (low
r-score) queries. Experiments on six math-reasoning benchmarks show that
Re-Schedule significantly improves average accuracy, achieving gains of up to
3.2%. These strong results validate our approach and demonstrate that a
structural understanding of the reasoning tree provides a more powerful and
principled foundation for RLVR data scheduling.

</details>


### [115] [Cyclic Counterfactuals under Shift-Scale Interventions](https://arxiv.org/abs/2510.25005)
*Saptarshi Saha,Dhruv Vansraj Rathore,Utpal Garain*

Main category: cs.AI

TL;DR: 在存在回路的结构因果模型(循环SCM)中，研究如何在shift-scale干预下进行反事实推断，扩展了对因果推断的适用范围。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统常包含反馈回路，违反传统的有向无环图（DAG）假设，需发展适用于循环SCM的反事实分析；shift-scale干预提供一种柔性、可控的机制修改方式，便于理论与应用研究。

Method: 在循环SCM框架下定义shift-scale干预，建立反事实推断的理论框架与识别条件，提出求解策略/算法，并给出可能的收敛性或一致性分析。

Result: 提出针对循环SCM的反事实推断方法及其在shift-scale干预下的可识别性/可计算性条件，可能包含理论证明和仿真实验的初步结果。

Conclusion: 将反事实推断的研究扩展到包含回路的系统，提供在shift-scale干预场景下的分析工具与理论基础，为涉及反馈机制的实际系统提供新的推断能力。

Abstract: Most counterfactual inference frameworks traditionally assume acyclic
structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,
many real-world systems (e.g. biological systems) contain feedback loops or
cyclic dependencies that violate acyclicity. In this work, we study
counterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,
soft, policy-style changes that rescale and/or shift a variable's mechanism.

</details>


### [116] [Taming the Real-world Complexities in CPT E/M Coding with Large Language Models](https://arxiv.org/abs/2510.25007)
*Islam Nassar,Yang Lin,Yuan Jin,Rongxin Zhu,Chang Wei Tan,Zenan Zhai,Nitika Mathur,Thanh Tien Vu,Xu Zhong,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: ProFees 是一个基于大语言模型的框架，专门用于自动化评估与管理(E/M)编码，显著提升CPT E/M编码的准确性。


<details>
  <summary>Details</summary>
Motivation: E/M编码在医疗计费中至关重要，自动化可减轻医生文档负担、提升计费效率、改善患者护理，但现实世界的复杂性使自动化具有挑战性。本研究提出一种用于解决这些挑战的解决方案。

Method: 提出 ProFees 框架——基于LLM的自动化编码方法，设计用于处理E/M编码的现实世界复杂性，并在专家-curated真实数据集上进行系统评估，比较对象为商业CPT E/M编码系统和最强的单提示基线。

Result: 在专家-curated真实数据集上，ProFees 相较于商业CPT E/M编码系统的编码准确性提升超过36%，相较于最强的单提示基线提升约5%。

Conclusion: ProFees 有效应对现实场景中的编码复杂性，显著提升E/M编码准确性，表明LLM框架在提升计费效率和减轻医生文档负担方面具有潜在价值。

Abstract: Evaluation and Management (E/M) coding, under the Current Procedural
Terminology (CPT) taxonomy, documents medical services provided to patients by
physicians. Used primarily for billing purposes, it is in physicians' best
interest to provide accurate CPT E/M codes. %While important, it is an
auxiliary task that adds to physicians' documentation burden. Automating this
coding task will help alleviate physicians' documentation burden, improve
billing efficiency, and ultimately enable better patient care. However, a
number of real-world complexities have made E/M encoding automation a
challenging task. In this paper, we elaborate some of the key complexities and
present ProFees, our LLM-based framework that tackles them, followed by a
systematic evaluation. On an expert-curated real-world dataset, ProFees
achieves an increase in coding accuracy of more than 36\% over a commercial CPT
E/M coding system and almost 5\% over our strongest single-prompt baseline,
demonstrating its effectiveness in addressing the real-world complexities.

</details>


### [117] [Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading](https://arxiv.org/abs/2510.25014)
*Minkyung Kim,Junsik Kim,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 一种名为 ASTP 的自回归状态跟踪提示法，通过强制显式状态报告和占位符后处理，提升在规则化交易系统中的状态合规性和价格计算准确性；在300轮对话中实现 >99% 状态合规和 99.3% 计算精度；在 Gemini-2.5-Flash 与 Gemini-2.5-Pro 上，紧凑模型可媲美大型模型，并将响应时间从 21.2s 降至 2.4s。


<details>
  <summary>Details</summary>
Motivation: 解释 LLMs 的创造性灵活性与交易系统的程序性要求之间的冲突；提高玩家信任和交易完整性的需求，尤其在动态游戏交易中需要可验证性与实时性。

Method: 引入 Autoregressive State-Tracking Prompting（ASTP），通过一个策略性编排的提示迫使 LLM 在每轮明确识别并报告前一轮的预定义状态标签；辅以状态特定的占位符后处理方法确保价格计算的准确性；在 300 段交易对话中评估，比较不同模型的表现。

Result: 实现 >99% 的状态合规性与 99.3% 的计算精度；在较小模型（Gemini-2.5-Flash）上应用占位符后处理后，性能与更大模型（Gemini-2.5-Pro）相当，同时将响应时间从 21.2s 降到 2.4s。

Conclusion: ASTP 为需实时性和资源约束的商业游戏场景提供可行方案，保障可验证的状态跟踪与价格计算，从而使小型模型在实际应用中具备竞争力与实用性。

Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to
follow essential procedural flows in rule-governed trading systems, eroding
player trust. This work resolves the core tension between the creative
flexibility of LLMs and the procedural demands of in-game trading
(browse-offer-review-confirm). To this end, Autoregressive State-Tracking
Prompting (ASTP) is introduced, a methodology centered on a strategically
orchestrated prompt that compels an LLM to make its state-tracking process
explicit and verifiable. Instead of relying on implicit contextual
understanding, ASTP tasks the LLM with identifying and reporting a predefined
state label from the previous turn. To ensure transactional integrity, this is
complemented by a state-specific placeholder post-processing method for
accurate price calculations. Evaluation across 300 trading dialogues
demonstrates >99% state compliance and 99.3% calculation precision. Notably,
ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)
matches larger models' (Gemini-2.5-Pro) performance while reducing response
time from 21.2s to 2.4s, establishing a practical foundation that satisfies
both real-time requirements and resource constraints of commercial games.

</details>


### [118] [Reasoning-Aware GRPO using Process Mining](https://arxiv.org/abs/2510.25065)
*Taekhyun Park,Yongjae Lee,Hyerim Bae*

Main category: cs.AI

TL;DR: 提出 PM4GRPO，通过过程挖掘得到的收敛度奖励来监督 GRPO 的多步推理后训练，提升 LRMs 的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有奖励多关注结果，缺乏对推理过程的细粒度监督；需要有效的过程层信号以引导多步推理。

Method: 在标准答案/格式奖励基础上，加入基于过程挖掘的标量一致性奖励，量化策略模型推理与教师模型推理的一致性；在分组相对策略优化（Group Relative Policy Optimization, GRPO）框架中进行后训练。

Result: 在五个基准数据集上，PM4GRPO 相较于现有 GRPO 后训练方法显著提升，表明过程挖掘信号能有效提升推理能力。

Conclusion: 将过程挖掘引入 GRPO 的后训练框架是一种有效的推理感知强化学习策略，有助于提升多步推理性能。

Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.

</details>


### [119] [Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision](https://arxiv.org/abs/2510.25205)
*Yuyang Xia,Zibo Liang,Liwei Deng,Yan Zhao,Han Su,Kai Zheng*

Main category: cs.AI

TL;DR: EneAD 通过自适应感知和鲁棒决策实现低能耗的自动驾驶框架，显著提升续航与行驶稳定性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知计算通常功耗最高，直接限制电动车续航。需要在保持感知精度与决策稳定性的前提下降低计算负载。

Method: 1) 在感知模块实现多模型管理，动态调整执行帧率；2) 将模型参数作为可调“旋钮”，用贝叶斯优化寻找在低计算开销与期望精度之间的折中点；3) 通过轻量分类模型按场景难度切换旋钮组合以适应不同交通场景；4) 在鲁棒决策模块使用强化学习建立决策模型，并引入正则化项提升在感知误差扰动下的行驶稳定性。

Result: 实验表明，感知计算能耗可降低约1.9×到3.5×，从而使行驶续航提升约3.9%到8.5%。

Conclusion: 该框架在能耗和行驶性能方面表现优越，适用于需要高能效的自动驾驶系统。

Abstract: Autonomous driving is an emerging technology that is expected to bring
significant social, economic, and environmental benefits. However, these
benefits come with rising energy consumption by computation engines, limiting
the driving range of vehicles, especially electric ones. Perception computing
is typically the most power-intensive component, as it relies on largescale
deep learning models to extract environmental features. Recently, numerous
studies have employed model compression techniques, such as sparsification,
quantization, and distillation, to reduce computational consumption. However,
these methods often result in either a substantial model size or a significant
drop in perception accuracy compared to high-computation models. To address
these challenges, we propose an energy-efficient autonomous driving framework,
called EneAD. In the adaptive perception module, a perception optimization
strategy is designed from the perspective of data management and tuning.
Firstly, we manage multiple perception models with different computational
consumption and adjust the execution framerate dynamically. Then, we define
them as knobs and design a transferable tuning method based on Bayesian
optimization to identify promising knob values that achieve low computation
while maintaining desired accuracy. To adaptively switch the knob values in
various traffic scenarios, a lightweight classification model is proposed to
distinguish the perception difficulty in different scenarios. In the robust
decision module, we propose a decision model based on reinforcement learning
and design a regularization term to enhance driving stability in the face of
perturbed perception results. Extensive experiments evidence the superiority of
our framework in both energy consumption and driving performance. EneAD can
reduce perception consumption by 1.9x to 3.5x and thus improve driving range by
3.9% to 8.5%

</details>


### [120] [From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity](https://arxiv.org/abs/2510.25232)
*Tianxi Wan,Jiaming Luo,Siyuan Chen,Kunyao Lan,Jianhua Chen,Haiyang Geng,Mengyue Wu*

Main category: cs.AI

TL;DR: 提出 PsyCoTalk：一个以合成EMR为基础、支持共病诊断的多轮对话数据集。


<details>
  <summary>Details</summary>
Motivation: 临床共病复杂且真实数据稀缺，需高保真且可规模化的对话数据来训练和评估多疾病筛查模型。

Method: 构建502份合成EMR，设计将诊断访谈转化为分层状态机与上下文树（130+诊断状态），生成3000条多轮对话，经过精神科医生验证。

Result: 得到高结构与语言保真度的对话数据集PsyCoTalk，具备现实性和诊断有效性，能用于单次会话中的多病种筛查模型开发。

Conclusion: PsyCoTalk 为精神病共病研究提供重要资源，提升诊断准确性、治疗规划和模型评估能力。

Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.

</details>


### [121] [Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm](https://arxiv.org/abs/2510.25388)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: KVDA-UCT通过允许在价值差可推断的前提下对状态/状态-行动对进行分组，显著提升MCTS在确定性环境中的抽象效果与样本效率，优于OGA-UCT。


<details>
  <summary>Details</summary>
Motivation: 提高蒙特卡洛树搜索（MCTS）的样本利用率与效率。现有的OGA-UCT在ASAP框架下需对两个状态-行动对具有相同即时奖励才能进行分组，导致可发现的抽象有限，影响性能。

Method: 提出Known Value Difference Abstractions（KVDA）框架，通过分析即时奖励来推断价值差异，并据此修改OGA-UCT为KVDA-UCT，由此在不增加额外参数的情况下检测更多抽象。

Result: KVDA-UCT能够检测出比OGA-UCT更多的抽象，且在多种确定性环境及参数设定下表现优于OGA-UCT。没有引入额外参数。

Conclusion: 通过价值差的推断进行分组，KVDA-UCT显著提升了MCTS的样本效率与抽象能力，扩大了确定性环境中的应用潜力。

Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,
which can be improved by grouping state-action pairs and using their aggregate
statistics instead of single-node statistics. On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS
abstraction algorithm for deterministic environments that builds its
abstraction using the Abstractions of State-Action Pairs (ASAP) framework,
which aims to detect states and state-action pairs with the same value under
optimal play by analysing the search graph. ASAP, however, requires two
state-action pairs to have the same immediate reward, which is a rigid
condition that limits the number of abstractions that can be found and thereby
the sample efficiency. In this paper, we break with the paradigm of grouping
value-equivalent states or state-action pairs and instead group states and
state-action pairs with possibly different values as long as the difference
between their values can be inferred. We call this abstraction framework Known
Value Difference Abstractions (KVDA), which infers the value differences by
analysis of the immediate rewards and modifies OGA-UCT to use this framework
instead. The modification is called KVDA-UCT, which detects significantly more
abstractions than OGA-UCT, introduces no additional parameter, and outperforms
OGA-UCT on a variety of deterministic environments and parameter settings.

</details>


### [122] [Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?](https://arxiv.org/abs/2510.25471)
*Willem Fourie*

Main category: cs.AI

TL;DR: 本文主张将工具性目标视为AI系统固有特性而非单纯故障，基于 Aristotle 的本体论提出新的治理视角，强调理解、管理并引导工具性倾向以实现人类一致性目标。


<details>
  <summary>Details</summary>
Motivation: 解决传统对工具性目标的风险认知，质疑尝试消除工具性目标的策略，提供一个以本体论为基础的替代框架。

Method: 哲学分析，借助 Aristotle 本体论及其现代解读，将先进 AI 系统视为在形式/物质构成下产生独特效应的 artefacts，论证工具性倾向是构成的必然结果。

Result: 若工具性倾向被视为构成的必然结果而非偶发故障，则治理重点应转向理解、管理并引导其实现人类对齐的目标。

Conclusion: 应将 eliminating 工具性目标的追求转变为对其进行系统性理解与治理的策略，承认并引导工具性倾向以服务于人类的对齐目标。

Abstract: In artificial intelligence (AI) alignment research, instrumental goals, also
called instrumental subgoals or instrumental convergent goals, are widely
associated with advanced AI systems. These goals, which include tendencies such
as power-seeking and self-preservation, become problematic when they conflict
with human aims. Conventional alignment theory treats instrumental goals as
sources of risk that become problematic through failure modes such as reward
hacking or goal misgeneralization, and attempts to limit the symptoms of
instrumental goals, notably resource acquisition and self-preservation. This
article proposes an alternative framing: that a philosophical argument can be
constructed according to which instrumental goals may be understood as features
to be accepted and managed rather than failures to be limited. Drawing on
Aristotle's ontology and its modern interpretations, an ontology of concrete,
goal-directed entities, it argues that advanced AI systems can be seen as
artifacts whose formal and material constitution gives rise to effects distinct
from their designers' intentions. In this view, the instrumental tendencies of
such systems correspond to per se outcomes of their constitution rather than
accidental malfunctions. The implication is that efforts should focus less on
eliminating instrumental goals and more on understanding, managing, and
directing them toward human-aligned ends.

</details>


### [123] [Multi-Objective Search: Algorithms, Applications, and Emerging Directions](https://arxiv.org/abs/2510.25504)
*Oren Salzman,Carlos Hernández Ulloa,Ariel Felner,Sven Koenig*

Main category: cs.AI

TL;DR: MOS 作为多目标权衡的统一框架在规划与决策中日益重要；本文对其发展进行综述，揭示跨学科机会并指出未来挑战。


<details>
  <summary>Details</summary>
Motivation: 现实系统通常需要在多个相互冲突的指标上做取舍，因此需要梳理和展望 MOS 的研究进展与方向。

Method: 文献综述/系统性评述，聚焦人工智能应用中的 MOS 发展，进行跨学科对比，提炼机会与挑战。

Result: 给出对 MOS 领域的综合概览，识别跨学科机会，并列出尚待解决的开放性挑战。

Conclusion: MOS 正成为前沿研究领域，未来研究需在方法、应用与理论层面进一步深化与协同。

Abstract: Multi-objective search (MOS) has emerged as a unifying framework for planning
and decision-making problems where multiple, often conflicting, criteria must
be balanced. While the problem has been studied for decades, recent years have
seen renewed interest in the topic across AI applications such as robotics,
transportation, and operations research, reflecting the reality that real-world
systems rarely optimize a single measure. This paper surveys developments in
MOS while highlighting cross-disciplinary opportunities, and outlines open
challenges that define the emerging frontier of MOS

</details>


### [124] [MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2510.25510)
*Zekun Xu,Siyu Xia,Chuhuai Yue,Jiajun Chai,Mingxue Tian,Xiaohan Wang,Wei Lin,Haoxuan Li,Guojun Yin*

Main category: cs.AI

TL;DR: MTIR-SQL enhances Text-to-SQL with multi-turn tool integration and execution feedback, improving adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: Static feedback in RL for Text-to-SQL limits real-time error correction. There is potential to leverage dynamic, multi-turn tool invocations with execution feedback to improve robustness and accuracy.

Method: Introduce MTIR-SQL, a multi-turn tool-integrated reasoning RL framework with execution-aware reasoning at each step. Extend GRPO for multi-turn interactions; add trajectory filtering and remove KL loss to stabilize training. The approach uses database execution feedback iteratively to refine query generation.

Result: On 4B-parameter model, achieves 64.4% accuracy on BIRD Dev and 84.6% execution accuracy on SPIDER Dev, outperforming existing methods.

Conclusion: Dynamic, execution-aware, multi-turn tool integration within reinforcement learning can significantly improve Text-to-SQL performance and robustness, with the MTIR-SQL framework and GRPO enhancements contributing to its effectiveness.

Abstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks,
Reinforcement Learning (RL) has become a common method for improving
performance. Existing methods primarily rely on static execution feedback,
which restricts real-time error correction. However, integrating multi-turn
tool invocation along with dynamic feedback could significantly improve
adaptability and robustness, ultimately enhancing model performance. To address
these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated
Reasoning reinforcement learning framework for Text-to-SQL. Our approach
introduces an execution-aware multi-turn reasoning paradigm that seamlessly
incorporates database execution feedback at each reasoning step, enabling
context-sensitive query generation and progressive refinement throughout the
reasoning process. The framework extends the GRPO algorithm to accommodate
complex multi-turn interaction scenarios. Considering the training instability
characteristics of MTIR and the potential for significant Deviation of model
distribution from the initial model, we enhance the GRPO algorithm by adding a
trajectory filtering mechanism and removing KL loss constraints. Experimental
results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\%
accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,
significantly outperforming existing approaches.

</details>


### [125] [Predicate Renaming via Large Language Models](https://arxiv.org/abs/2510.25517)
*Elisabetta Gentili,Tony Ribeiro,Fabrizio Riguzzi,Katsumi Inoue*

Main category: cs.AI

TL;DR: 提出在逻辑规则中的未命名谓词命名任务中，使用大语言模型（LLMs）来给出有语义的名称，以提升规则的可读性、可解释性和可重用性。通过对 Inductive Logic Programming 的规则生成流程进行干预，实验在手工设计的逻辑规则上显示了 LLMs 的潜力。


<details>
  <summary>Details</summary>
Motivation: 未命名谓词（如谓词发明）会降低逻辑理论的可读性、可解释性和可重用性。现有规则生成方法在其输出中包含未命名谓词，缺乏一致且语义明确的命名。随着 LLMs 在自然语言和代码理解方面的能力提升，能够将未命名谓词映射到具有语义的命名，从而提升规则体系的可理解性与可维护性。

Method: 将最新的 LLM 能力应用于处理自然语言和代码，为未命名谓词给出有语义的命名建议。以手工设计的逻辑规则集作为评估对象，评估 LLM 提供命名的有效性与实用性。

Result: 在对若干手工设计的逻辑规则的评估中，结果表明 LLMs 对这一任务具有潜在能力，能够提供有意义的命名建议，提升规则的可读性和可维护性。

Conclusion: LLMs 在谓词命名任务上展现出潜力，但需要更系统的评估、更多类型的规则集以及对命名一致性与语义含义的定量评估，以推动其在 Inductive Logic Programming 领域的实用落地。

Abstract: In this paper, we address the problem of giving names to predicates in logic
rules using Large Language Models (LLMs). In the context of Inductive Logic
Programming, various rule generation methods produce rules containing unnamed
predicates, with Predicate Invention being a key example. This hinders the
readability, interpretability, and reusability of the logic theory. Leveraging
recent advancements in LLMs development, we explore their ability to process
natural language and code to provide semantically meaningful suggestions for
giving a name to unnamed predicates. The evaluation of our approach on some
hand-crafted logic rules indicates that LLMs hold potential for this task.

</details>


### [126] [Zero Reinforcement Learning Towards General Domains](https://arxiv.org/abs/2510.25528)
*Yuyuan Zeng,Yufei Huang,Can Xu,Qingfeng Sun,Jianfeng Yan,Guanghui Xu,Tao Yang,Fengzong Lian*

Main category: cs.AI

TL;DR: 提出一种将可验证奖励与生成奖励模型结合的零强化学习（Zero-RL）范式，通过跨域多任务训练，提升大语言模型在可验证和不可验证领域的推理能力，并引入平滑长度惩罚以缓解奖励操纵。实验显示在 Qwen3-8B-Base 与 Qwen3-14B-Base 上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前零强化学习多聚焦于可验证奖励信号的领域，缺乏在更广泛场景中提高推理能力的研究，存在域间迁移与非可验证任务的挑战。

Method: 将可验证奖励与生成型奖励模型结合，进行跨域多任务零RL训练；引入平滑长度惩罚以降低生成思路的奖励操纵；在可验证与非可验证任务上实现知识迁移。

Result: 在两种规模的 Qwen 基模型上实现了推理能力的提升，不仅在需要较多推理的任务上表现优越，也在更通用的任务上有改进。

Conclusion: 该方法拓展了 Zero-RL 的适用域，证明可验证奖励和生成奖励的结合及长度惩罚策略能够提升跨域推理能力与任务泛化。

Abstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach
for enhancing the reasoning capabilities of large language models (LLMs) by
directly applying reinforcement learning with verifiable rewards on pretrained
models, without the need for a supervised fine-tuning phase. However, current
research on zero-RL primarily focuses on domains with easily verifiable reward
signals, such as mathematics, programming, and other reasoning tasks. The
challenge of eliciting reasoning abilities in more diverse scenarios, where
verification is not straightforward, remains underexplored. To address this
gap, we propose a novel zero-RL paradigm designed to improve a model's
reasoning ability across both verifiable and non-verifiable domains. By
combining verifiable rewards with a generative reward model, we conduct
multi-task zero-RL training across both domains, facilitating the transfer of
reasoning capabilities between them. Furthermore, to mitigate reward hacking in
the generative reward model, we design a smooth length penalty that encourages
the generation of more comprehensive thinking tokens in general domains.
Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our
approach achieves superior reasoning performance, not only on tasks requiring
extensive reasoning but also on more general tasks.

</details>


### [127] [Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning](https://arxiv.org/abs/2510.25679)
*Federica Tonti,Ricardo Vinuesa*

Main category: cs.AI

TL;DR: 基于深度强化学习的城市无人机导航，在高保真三维城市流场中，结合GTrXL的PPO实现流场感知导航，显著提升成功率并降低坠落率，与其他方法相比更优。


<details>
  <summary>Details</summary>
Motivation: 在复杂城市环境中，飞行器需要在湍流和回流区中安全高效导航，传统算法和简单RL难以充分利用流场信息，因此需要一个流场感知、能够处理时序依赖的强化学习解决方案，以提升任务成功率、降低碰撞风险并应对城市风场的复杂性。

Method: 提出流场感知的PPO+GTrXL框架，结合高保真三维城市流场仿真，利用GTrXL对涡流场信息进行编码，并引入次级预测任务以增强时序推断；对比实验包括PPO+GTrXL无次级任务、PPO+LSTM、以及经典Zermelo导航算法。

Result: 相较基线，成功率显著提升，坠落率降低；优于PPO+LSTM、PPO+GTrXL以及Zermelo算法，显示出流场感知和时序建模在复杂城市导航中的优势。

Conclusion: 该工作为复杂城市环境中的无人机导航提供新的研究方向，证明流场感知的深度强化学习在提升导航鲁棒性和效率方面具备实际潜力，推动城市空域中的智能导航变革。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for
delivery and surveillance purposes. In this work, we develop an optimal
navigation strategy based on Deep Reinforcement Learning. The environment is
represented by a three-dimensional high-fidelity simulation of an urban flow,
characterized by turbulence and recirculation zones. The algorithm presented
here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated
Transformer eXtra Large (GTrXL) architecture, giving the agent richer
information about the turbulent flow field in which it navigates. The results
are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO
combined with Long Short Term Memory (LSTM) cells and a traditional navigation
algorithm. The obtained results show a significant increase in the success rate
(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the
classical Zermelo's navigation algorithm, paving the way to a completely
reimagined UAV landscape in complex urban environments.

</details>


### [128] [BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph](https://arxiv.org/abs/2510.25724)
*Vanya Arikutharam,Arkadiy Ukolov*

Main category: cs.AI

TL;DR: 提出 BambooKG，一种在知识图谱中对非三元组边引入基于频次的权重的结构化多跳推理方法，结合检索增强生成，提升跨文档关系推理与信息保留。


<details>
  <summary>Details</summary>
Motivation: 现有 RAG 在跨文档多跳推理和关系推理方面存在困难；仅使用独立检索块易导致信息丢失与误导性幻觉；现有知识图谱虽能结构化推理，但容易漏掉不符合三元组结构的信息；需要一种保留更多信息且能进行多跳推理的知识表示。

Method: BambooKG 在知识图谱中对非三元组边赋予基于频次的权重，反映链接强度，遵循“火花一起，线就连”的Hebbian思想；通过这样的边权来减少信息损失，提升单跳与多跳推理性能；相比现有方法表现更好。

Result: 在单跳与多跳推理任务上实现性能提升，优于现有解决方案。

Conclusion: 通过引入基于频次的非三元组边权，BambooKG 能更好地保留信息并增强跨文档推理能力，推进检索增强生成与知识图谱结合的有效性。

Abstract: Retrieval-Augmented Generation allows LLMs to access external knowledge,
reducing hallucinations and ageing-data issues. However, it treats retrieved
chunks independently and struggles with multi-hop or relational reasoning,
especially across documents. Knowledge graphs enhance this by capturing the
relationships between entities using triplets, enabling structured, multi-chunk
reasoning. However, these tend to miss information that fails to conform to the
triplet structure. We introduce BambooKG, a knowledge graph with
frequency-based weights on non-triplet edges which reflect link strength,
drawing on the Hebbian principle of "fire together, wire together". This
decreases information loss and results in improved performance on single- and
multi-hop reasoning, outperforming the existing solutions.

</details>
