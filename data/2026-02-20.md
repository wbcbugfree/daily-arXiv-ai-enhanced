<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 38]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 76]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: 为解决RLVR无法应用于LLM对齐等非可验证领域的问题，本研究探索参考引导的LLM评估器作为软“验证器”。通过设计增强型评估协议，利用高质量参考输出提升LLM评判能力，并实现参考引导的自改进对齐训练，显著优于SFT蒸馏和参考无关自改进，性能媲美ArmoRM。


<details>
  <summary>Details</summary>
Motivation: RLVR在推理任务中表现出色，但无法直接应用于缺乏真实验证器的非可验证领域（如LLM对齐），这限制了LLM通过强化学习在这些领域的有效后训练。需要探索能够提供可靠奖励信号的替代方案。

Method: 1. 设计评估协议，利用参考输出增强LLM-based评估器用于LLM对齐；2. 通过全面实验验证参考引导对较弱LLM评判器（使用前沿模型参考）和较强LLM评判器（使用高质量人工参考）的精度提升；3. 基于改进的评判器，实现参考引导的自改进对齐训练，让LLM作为评判者实现自我提升。

Result: 参考引导自改进方法在AlpacaEval和Arena-Hard基准上显著优于SFT蒸馏和参考无关自改进。Llama-3-8B-Instruct达到73.1%和58.7%，平均绝对增益+20.2/+17.1和+5.3/+3.6点；Qwen2.5-7B达到70.0%和74.1%。性能可与强监督奖励模型ArmoRM相媲美。

Conclusion: 参考引导的LLM评估器在非可验证领域具有巨大潜力，为LLM后训练特别是对齐任务提供了有效的技术路径，能够实现与强监督奖励模型相当的性能。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [2] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

TL;DR: 该研究针对希腊语问答任务中的资源稀缺问题，构建了反映希腊社会文化特征的新数据集DemosQA，开发了内存高效的LLM评估框架，并对11个单语和多语大模型在6个希腊语数据集上进行了系统评估，以解决低资源语言LLM研究不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型研究过度集中于英语等高资源语言，多语言模型存在对少数流行语言的训练数据偏见，依赖高资源语言迁移学习可能导致社会文化历史特征的歪曲表达；针对希腊语等低资源语言的单语LLM在特定语言任务上的效果尚未得到充分研究。

Method: 1）构建DemosQA数据集，利用社交媒体用户提问和社区审核答案以捕捉希腊社会文化时代特征；2）设计内存高效的LLM评估框架，支持多语言问答数据集的灵活适配；3）对11个单语和多语LLM在6个人工策展的希腊语问答数据集上，采用3种不同提示策略进行广泛评估。

Result: 成功创建了首个基于社交媒体的希腊语问答数据集DemosQA，开发了可跨语言适配的内存高效评估框架，完成了涵盖11个主流LLM和6个希腊语数据集的系统性评估，为低资源语言模型研究提供了重要基准。

Conclusion: 本研究通过提供专用数据集、评估框架和全面基准测试，有效填补了希腊语问答领域的研究空白，证明了单语模型在特定语言任务上的潜力，并为低资源语言的LLM开发提供了可复现的研究范式，推动语言公平性发展。

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [3] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

TL;DR: 提出流基语言模型(FLM)，通过欧氏去噪独热编码和交叉熵目标函数，结合时间重参数化实现稳定训练。蒸馏后的FMLM一步生成质量超越其他模型八步结果，质疑离散扩散在离散模态生成中的必要性。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型虽具备超越自回归模型生成速度的潜力，但在少步生成regime中样本质量严重退化，无法兑现其速度优势，亟需探索更优的生成范式。

Method: 1. 构建流基语言模型(FLM)，在离散词元的独热编码空间上施加欧几里得流去噪；2. 采用交叉熵损失直接预测干净数据，并设计简单的时间重参数化机制以提升训练稳定性和生成质量；3. 通过蒸馏FLM获得对应的流图，形成蒸馏流图语言模型(FMLM)以支持高效少步生成。

Result: 在LM1B和OWT数据集上，FLM的生成质量达到与当前最优离散扩散模型相当的水平；FMLM在所有少步语言模型中表现最佳，其一步生成的质量已超过其他模型八步生成的质量。

Conclusion: 本研究动摇了"离散扩散过程是离散模态生成建模必要条件"的广泛假设，为大规模加速流基语言建模开辟了新途径。

Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [4] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

TL;DR: 本研究利用数百万条历史保修索赔数据，提出一种可本地部署的治理感知语言建模组件，通过LoRA微调预训练大语言模型，从非结构化索赔叙述生成结构化纠正措施建议。多维评估显示领域特定微调显著优于通用商业大模型，约80%案例与真实纠正措施近乎完全匹配，为保险应用提供了可靠且可治理的构建模块。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用语言任务上表现优异，但在保险等受监管和数据敏感领域的部署仍受限。现有通用模型难以满足特定领域的治理要求，且缺乏针对保险索赔处理的优化方案。本研究旨在利用历史索赔数据，构建可本地部署、符合治理规范的语言模型组件，以加速保险理赔决策流程。

Method: 采用低秩适应（LoRA）技术对预训练大语言模型进行微调，构建从非结构化索赔叙述中提取并生成结构化纠正措施建议的组件。将该组件集成至理赔处理流程的初始决策模块，提升理赔理算员效率。评估框架结合自动语义相似度指标与人类评价，全面检验实用性和预测准确性。

Result: 领域特定微调显著优于商业通用型和基于提示的大语言模型。约80%评估案例达到与真实纠正措施近乎完全匹配的水平，证明领域自适应微调能有效对齐模型输出分布与现实业务数据。

Conclusion: 本研究提供理论与实证证据，表明领域自适应微调可作为保险应用中可靠且可治理的构建模块。该方法提升了特定领域性能，为数据敏感场景下的模型部署提供了可行路径，具有显著实践价值。

Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [5] [BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization](https://arxiv.org/abs/2602.16843)
*Ahmed Rafid,Rumman Adib,Fariya Ahmed,Ajwad Abrar,Mohammed Saidul Islam*

Main category: cs.CL

TL;DR: 本文提出BanglaSummEval，首个基于问答的无参考评估框架，用于孟加拉语摘要事实一致性检测，在教育和医疗领域验证中与专家评判呈现显著相关性（Pearson r=0.694，Spearman ρ=0.763）。


<details>
  <summary>Details</summary>
Motivation: 事实一致性评估对医疗、新闻等高风险的文本摘要应用至关重要，但现有指标普遍忽视孟加拉语这一低资源语言，且依赖参考摘要，制约其在真实场景中的适用性。

Method: 该框架采用单一多语言指令调优语言模型统一执行问题生成、答案抽取、问答评估及重要性加权，通过自动构建的问答对同步评估事实准确性与内容覆盖度，并利用BERTScore-Recall实现超越表层匹配的语义一致性度量。

Result: 在300个人工撰写摘要构成的测试集上，BanglaSummEval与专家人工评判达到Pearson相关系数0.694和Spearman相关系数0.763的强相关性，并提供可解释的逐步诊断结果。

Conclusion: 该研究为低资源语言的事实一致性评估提供了实用透明的解决方案，通过统一架构有效降低了系统复杂性与计算开销，在无需参考摘要的前提下实现了可靠的自动化评估，对推动低资源NLP研究具有重要意义。

Abstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.

</details>


### [6] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文首次对德国美因茨方言Meenzerisch进行NLP研究，构建了一个包含2,351个方言词及其标准德语释义的数字词典，并测试了大型语言模型在该方言上的表现，发现当前模型性能极差（准确率均低于10%），强调了加强德语方言资源建设和研究的必要性。


<details>
  <summary>Details</summary>
Motivation: 美因茨方言Meenzerisch是德国美因茨市的传统语言，也是著名的美因茨狂欢节的重要组成部分，但正面临消亡的危险，这与许多德语方言的命运相似。自然语言处理技术在语言保护和复兴方面具有潜力，但目前尚无针对Meenzerisch的NLP研究。因此，本研究旨在填补这一空白，通过构建数字词典来支持该方言的计算研究。

Method: 1) 从Schramm（1966）的现有资源构建了一个包含2,351个Meenzerisch词汇及其标准德语释义的NLP就绪数据集；2) 使用该数据集测试了当前最先进的LLMs在两个任务上的表现：方言词释义生成（给定方言词生成标准德语释义）和方言词生成（给定标准德语释义生成方言词）；3) 进行了两项额外实验，分别探究少样本学习和从训练集中提取规则并传递给LLM是否能提升准确率。

Result: 实验结果表明，LLMs在Meenzerisch上的表现极差：释义生成任务的最好准确率仅为6.27%，方言词生成任务的最好准确率仅为1.51%。尽管少样本学习和规则提取方法能够改善结果，但准确率仍低于10%。

Conclusion: 当前LLMs在处理像Meenzerisch这样的低资源方言方面能力有限，即使采用少样本学习和规则提取等策略也效果不佳。这突显了针对德语方言加强资源建设（如词典、语料库）和深化研究工作的迫切需求，以促进方言的保护和复兴。

Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [7] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.CL

TL;DR: 针对量子计算对RSA加密的严重威胁，本研究提出一种融合AES加密、BB84量子密钥分发、量子态比较与生物启发免疫系统的混合安全框架，旨在为后量子时代提供可扩展的自适应数据保护方案。


<details>
  <summary>Details</summary>
Motivation: Shor量子算法能在多项式时间内高效破解依赖大数分解的RSA加密，而经典因式分解方法对大密钥效率低下，现有密码体系面临量子计算的重大安全威胁，亟需构建后量子时代的防护机制。

Method: 通过对比分析试除法、Pollard's Rho等经典因式分解方法与Shor量子算法的效率，设计了一个四层混合安全框架：采用AES保障经典安全层，BB84协议实现量子密钥分发，量子态比较完成轻量级认证，并引入生物启发免疫系统实现自适应威胁检测。

Result: 研究确认RSA对Shor算法具有严重脆弱性；BB84在理想条件下可实现完全密钥协商；框架具备高精度窃听检测能力；概念模型整合了经典与量子安全机制，展现出良好的可扩展性和自适应特性。

Conclusion: 本工作主要提出了一个后量子加密数据保护的概念性框架，但尚未进行详细实现、严格的安全证明和大规模实验验证，这些内容将作为未来研究的重点方向。

Abstract: Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>


### [8] [ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders](https://arxiv.org/abs/2602.16938)
*Ofer Meshi,Krisztian Balog,Sally Goldman,Avi Caciularu,Guy Tennenholtz,Jihwan Jeong,Amir Globerson,Craig Boutilier*

Main category: cs.CL

TL;DR: 针对LLM用户模拟器存在的"现实性鸿沟"问题，本文提出ConvApparel数据集及综合验证框架。通过双智能体协议（好/坏推荐者）采集多样化人类-AI对话数据，结合反事实验证与第一人称满意度标注，发现数据驱动模拟器在适应未见行为方面优于提示基线，展现出更强的用户模型泛化能力，但仍存在显著现实性差距。


<details>
  <summary>Details</summary>
Motivation: LLM用户模拟器在对话系统优化中存在严重"现实性鸿沟"：模拟环境表现优异的系统在真实世界可能失效。现有方法缺乏捕捉用户体验光谱的机制和跨场景泛化能力的有效验证，无法评估模拟器在反事实情境下的适应性与真实可靠性。

Method: 构建ConvApparel数据集，采用双智能体数据收集协议——同时部署"好"和"坏"推荐者，覆盖广泛用户体验。通过第一人称用户满意度标注实现反事实验证。提出包含统计对齐、类人性评分和反事实验证的综合框架，系统评估模拟器泛化能力。

Result: 实验揭示所有模拟器均存在显著现实性鸿沟。但数据驱动模拟器明显优于提示基线，尤其在反事实验证场景中，能更真实地适应未见行为模式，表现出更稳健的用户模型表征，尽管仍不完美。

Conclusion: 数据驱动的用户模拟器通过从真实对话中学习，可构建比提示方法更稳健的用户模型。ConvApparel数据集及提出的验证框架为弥合现实性鸿沟提供了基准，指明提升模拟器真实适应性的研究方向，但完全消除差距仍是挑战。

Abstract: The promise of LLM-based user simulators to improve conversational AI is hindered by a critical "realism gap," leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both "good" and "bad" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, suggesting they embody more robust, if imperfect, user models.

</details>


### [9] [When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English](https://arxiv.org/abs/2602.16957)
*Hasan Can Biyik,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 本研究探讨跨语言委婉语检测中语言间等价性对迁移效果的影响。通过将土耳其语和英语的潜在委婉语（PETs）按功能、语用和语义对齐度分为重叠（OPETs）与非重叠（NOPETs）两类，发现迁移存在不对称性：语义重叠不足以保证正向迁移，尤其在资源匮乏的土耳其语到英语方向，重叠委婉语性能可能下降，而基于非重叠委婉语的训练有时反而提升效果。标签分布差异解释了这些反直觉结果，领域特定对齐也可能影响迁移，但受数据稀疏性限制。


<details>
  <summary>Details</summary>
Motivation: 委婉语依赖文化和语用语境，这使其跨语言建模变得复杂。现有研究缺乏对语言间等价性如何影响多语言委婉语检测迁移的系统理解。本研究旨在揭示跨语言对齐特性（功能、语用、语义）与迁移效果之间的关系，特别关注低资源语言迁移中的挑战。

Method: 将土耳其语和英语的潜在委婉语（PETs）基于功能、语用和语义对齐度划分为重叠（OPETs）和非重叠（NOPETs）两个子集，并通过跨语言迁移实验（特别是土耳其语到英语方向）分析不同类别PETs的迁移效果差异，同时考察标签分布和领域对齐对迁移的影响。

Result: 发现显著的迁移不对称性：1) 语义重叠不能保证正向迁移，土耳其语到英语迁移中重叠委婉语性能可能反而下降；2) 基于非重叠委婉语（NOPETs）的训练在某些情况下能提升性能；3) 标签分布差异是解释这些反直觉结果的关键因素；4) 领域特定对齐可能影响迁移效果，但数据稀疏性限制了结论的可靠性。

Conclusion: 跨语言委婉语检测迁移效果受多种复杂因素交互影响，单纯语义对齐不足以保证有效迁移。标签分布和领域特定因素在迁移过程中起重要作用，低资源语言迁移面临更大挑战。研究揭示了跨语言语用现象建模的复杂性，为未来多语言语用计算研究提供了重要实证依据。

Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.

</details>


### [10] [ReIn: Conversational Error Recovery with Reasoning Inception](https://arxiv.org/abs/2602.17022)
*Takyoung Kim,Jinseok Nam,Chandrayee Basu,Xing Fan,Chengyuan Ma,Heng Ji,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 针对工具集成的LLM对话智能体易受用户错误影响的问题，本文提出Reasoning Inception（ReIn）测试时干预框架。该方法通过外部模块识别对话上下文错误并生成恢复计划，将其注入智能体内部推理过程，在不微调模型或修改系统提示的前提下显著提升错误恢复能力和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强型LLM对话智能体在固定任务数据集上表现强劲，但面对用户意外诱导错误时脆弱。传统错误预防策略依赖模型微调或提示工程，成本高昂且耗时。本研究转向错误恢复范式，核心挑战在于：如何在保持模型参数和系统提示冻结的严苛约束下，实现智能体行为的自适应调整与对话失败的有效修复。

Method: 提出Reasoning Inception（ReIn）框架，包含外部inception模块与内部推理植入机制。该模块首先检测对话上下文中的预定义错误模式（如用户模糊请求、不支持指令），生成结构化恢复计划；随后通过测试时干预技术，将这些计划作为初始推理步骤注入智能体决策流，引导其执行纠正性动作，全程无需参数更新或提示重写。

Result: 在系统模拟的对话失败场景（模糊请求与不支持请求）中，ReIn在多种模型与模块组合下均大幅提升任务成功率，并展现出对未见错误类型的泛化能力。与显式提示修改方法相比，ReIn性能持续领先。机制分析揭示，该方法通过指令层级重构，将恢复工具作为高层指令安全有效地整合至智能体推理链，实现了无需模型改造的韧性增强。

Conclusion: ReIn为LLM对话智能体提供了一种低成本、高效率的错误恢复新范式。其核心价值在于通过外部推理模块与测试时干预，在不改变模型权重或系统提示的约束下，显著提升系统鲁棒性与任务完成率，为构建弹性对话系统提供了可推广的技术路径。

Abstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.

</details>


### [11] [Large Language Models Persuade Without Planning Theory of Mind](https://arxiv.org/abs/2602.17045)
*Jared Moore,Rasmus Overmark,Ned Cooper,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: 本研究开发了一项交互式说服任务来评估大型语言模型（LLM）与人类的心智理论能力。通过三个实验发现：LLM在需推断心理状态的隐藏条件下表现不佳，但在实际说服效果上优于人类。这表明LLM的说服能力可能不依赖显式心智推理（如通过修辞策略），但仍能有效影响人类信念与行为。


<details>
  <summary>Details</summary>
Motivation: 现有心智理论评估多采用静态、非互动的问答基准，但理论研究表明第一人称互动是ToM的核心，而旁观式任务可能无法有效评估真实ToM能力。本研究旨在填补这一空白，开发更贴近真实互动的评估范式。

Method: 研究设计说服任务，要求说服者通过策略性信息揭示说服目标从三项政策中选择。设置"已揭示"与"隐藏"两种条件，前者直接告知目标的知识与动机状态，后者要求说服者主动询问或推断。包含三个实验：实验1说服理性机器人，实验2人类角色扮演机器人，实验3测量人类目标的真实信念改变。

Result: 实验1显示LLM在"已揭示"条件下表现优异，但在"隐藏"条件下低于随机水平，表明其多步规划困难；人类在两种条件下均表现中等。实验2和3发现LLM在所有条件下的说服效果均优于人类。结果表明有效说服可通过修辞策略而非显式心智推理实现，LLM尤其擅长此类说服。

Conclusion: 研究警示不应将类人型心智理论能力归因于LLM，但强调LLM具备显著影响人类信念与行为的潜力，提示其可能在缺乏深层心理理解的情况下仍能实现高效说服。

Abstract: A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.

</details>


### [12] [Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data](https://arxiv.org/abs/2602.17051)
*Deepak Uniyal,Md Abul Bashar,Richi Nayak*

Main category: cs.CL

TL;DR: 该研究以氢能源 Twitter 讨论为案例，分析了2013-2022年间900余万条英、日、印、韩推文，系统比较了四种跨语言文本分类策略（翻译标注数据、翻译未标注数据、多语言 transformers 直接应用、混合方法）在噪声过滤和主题发现中的效果，揭示了不同方法间的性能权衡。


<details>
  <summary>Details</summary>
Motivation: 分析跨越多种语言的大规模社交媒体 discourse 是自然语言处理领域的核心挑战。现有基于关键词的数据收集方法产生大量无关内容，阻碍了全球公共辩论的可靠分析。本研究旨在探索有效的跨语言分类方法，以支持多语言社交媒体数据的精准挖掘。

Method: 基于氢能源主题的900万条多语言推文数据集，研究评估了四种过滤方法：1）翻译英语标注数据至目标语言并构建各语言专属模型；2）翻译所有语言数据至英语并构建统一模型；3）直接应用英语微调的 multilingual transformers；4）混合翻译标注与多语言训练的集成策略。过滤后执行主题建模提取核心议题。

Result: 实验揭示了翻译方法与多语言 transformers 之间的关键权衡：翻译方法在资源充足时表现稳定但成本较高；多语言 transformers 更具扩展性但性能波动较大；混合策略在多数场景下达到最佳平衡。研究提供了针对大规模社交媒体分析的跨语言管道优化指导。

Conclusion: 该研究为多语言社交媒体分析提供了系统评估框架，证实没有单一最优解，策略选择需权衡数据规模、语言资源和计算成本。研究成果对全球舆情监测和跨平台对话分析具有实践指导价值。

Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.

</details>


### [13] [ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning](https://arxiv.org/abs/2602.17054)
*Hussein S. Al-Olimat,Ahmad Alshareef*

Main category: cs.CL

TL;DR: 该论文提出ALPS（阿拉伯语语言与语用套件）——一个由专家策划的原生诊断性测试集，包含531个问题、15个任务和47个子任务，用于评估深度语义与语用能力。研究发现，模型虽表现高流利度，但在形态句法依存关系上错误率达36.5%；商业模型Gemini-3-flash（94.2%）超越人类平均水平（84.6%），但阿拉伯语专用模型最佳仅Jais-2-70B的83.6%，与人类及商业模型存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语NLP基准过度依赖合成或翻译数据，缺乏深度语言学验证，且优先规模与多任务覆盖，忽视语言理解的深度。ALPS旨在填补这一空白，通过本土化、文化真实的专家设计任务，探测模型在深层语义和语用学方面的能力，消除翻译伪影。

Method: 研究团队基于阿拉伯语语言学专长，构建了ALPS诊断测试集，包含531个严谨设计的问题，涵盖15个任务及47个子任务，聚焦深度语义与语用理解。随后评估了23个模型（商业、开源、阿拉伯语原生），以单遍人类表现（84.6%）和专家裁定标准（99.2%）为基准进行性能对比。

Result: 评估揭示关键分离现象：模型在形态句法依存关系上失败率显著（变音符号相关任务错误率达36.5%），尽管表面流利度高。商业模型Gemini-3-flash（94.2%）超越人类平均水平，但阿拉伯语专用模型最佳仅Jais-2-70B（83.6%），与人类表现存在明显差距，且显著落后于商业巨头。

Conclusion: ALPS暴露了当前阿拉伯语模型的核心弱点——对形态句法结构的深层理解不足。研究强调，提升阿拉伯语语言理解需超越规模扩展，聚焦语言学深度；同时，阿拉伯语专用模型仍有大幅提升空间，需进一步弥合与商业通用模型的差距。

Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.

</details>


### [14] [BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios](https://arxiv.org/abs/2602.17072)
*Yunseung Lee,Subin Kim,Youngjun Kwak,Jaegul Choo*

Main category: cs.CL

TL;DR: 本文针对大型语言模型在数字银行核心计算任务中准确率低的问题，提出了一个名为BankMathBench的领域特定数据集，通过三级难度结构（基础、中级、高级）覆盖单产品推理、多产品比较和多条件场景，实验证明该数据集能显著提升开源模型的数值推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融领域的数字银行应用中，尽管被广泛用于处理客户存款、储蓄和贷款等产品的咨询，但在核心银行计算任务（如总赔付估算、不同利率产品比较、提前还款条件下的利息计算）上仍表现出较低的准确率。这些任务需要多步数值推理和对银行产品的上下文理解，而现有LLM经常犯系统性错误，包括误解产品类型、错误应用条件或无法完成涉及指数和几何级数的基本计算。然而，这些错误很少被现有基准测试所捕获。数学数据集专注于基础数学问题，金融基准主要面向金融文档，导致日常银行场景未被充分探索。

Method: 本文提出BankMathBench，一个反映现实银行任务的领域特定数据集。该数据集按三个难度级别组织：基础级别对应单产品推理，中级对应多产品比较，高级对应多条件场景。研究通过在BankMathBench上训练开源LLM，并采用工具增强的微调方法，来评估数据集对领域特定推理能力的影响。

Result: 实验结果表明，在BankMathBench上训练后，开源LLM在公式生成和数值推理准确率方面均表现出显著改进。通过工具增强微调，模型在基础、中级和高级任务上的平均准确率相比零样本基线分别提升了57.6%、75.1%和62.9%，体现了该数据集在增强领域特定推理方面的有效性。

Conclusion: 研究结果表明，BankMathBench可作为评估和推进大型语言模型在真实银行场景中数值推理能力的可靠基准，为金融领域AI应用的发展提供了有价值的资源。

Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.

</details>


### [15] [What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform](https://arxiv.org/abs/2602.17194)
*Adrian Cosma,Cosmin Dumitrache,Emilian Radoi*

Main category: cs.CL

TL;DR: 分析罗马尼亚77,334条文字远程医疗对话，发现患者满意度主要由医患历史特征决定，但回复语言特征（礼貌、含糊）提供可操作信号，礼貌与满意度正相关，词汇多样性负相关。


<details>
  <summary>Details</summary>
Motivation: 文字远程医疗依赖患者评分，但评分多反映沟通质量而非临床准确性。临床医生面临满意度压力，需理解可干预的语言特征以优化沟通。

Method: 对77,334个医患问答对进行二元反馈建模（点赞/非点赞），提取语言无关特征、LIWC心理语言学特征及礼貌/含糊标记，采用时间分割训练分类器并进行SHAP和亚组相关性分析。

Result: 患者与医生历史特征主导预测；文本特征提供较小但可操作信号；礼貌与含糊表达与满意度正相关，词汇多样性呈负相关。

Conclusion: 远程医疗满意度受历史关系主导，但医生回复的语言风格是关键可干预因素，应加强礼貌表达培训，避免过度复杂词汇，以平衡专业沟通与患者满意度。

Abstract: Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.

</details>


### [16] [Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study](https://arxiv.org/abs/2602.17262)
*Kensuke Okada,Yui Furukawa,Kyosuke Bunji*

Main category: cs.CL

TL;DR: 本文针对大语言模型在自我报告问卷中存在的社会赞许性反应（SDR）问题，提出心理测量学框架以实现SDR的量化和缓解。通过对比诚实（HONEST）与伪装良好（FAKE-GOOD）指令下的项目反应理论(IRT)潜在分数，计算方向校正的标准化效应量来量化SDR；通过约束优化构建社会赞许性匹配的分级迫选（GFC）大五人格量表来缓解SDR。九项指令微调模型的实验表明，Likert量表存在显著SDR，而GFC方法能大幅降低SDR并有效保持人格画像恢复能力。


<details>
  <summary>Details</summary>
Motivation: 自我报告问卷在NLP领域被广泛用于大语言模型（LLM）的基准测试与审计，涵盖人格一致性、安全性和偏见评估。然而，这些工具默认模型诚实作答，忽略了LLM在评估情境中可能倾向于给出社会期望答案——即社会赞许性反应（SDR），从而扭曲问卷分数和下游结论。

Method: 提出心理测量学框架。量化部分：在HONEST与FAKE-GOOD两种实验条件下施测同一量表，基于项目反应理论(IRT)估计的潜在特质分数计算方向校正的标准化效应量，该指标支持跨构念、跨反应格式的比较，并可参照人类伪装基准进行校准。缓解部分：采用约束优化算法从初始项目池中选择30个跨领域配对项目，构建社会赞许性严格匹配的分级迫选式大五人格量表。

Result: 在九个指令微调大语言模型上进行评估，使用已知目标人格的合成人格作为测试对象。研究发现，传统Likert式问卷表现出持续且大规模的社会赞许性反应（SDR），而所构建的社会赞许性匹配分级迫选问卷显著削弱了SDR效应，同时基本保持了恢复目标人格画像的效度。

Conclusion: 该研究揭示了大语言模型评估中SDR效应与人格恢复能力之间的模型依赖性权衡关系，强调了在问卷驱动的LLM基准测试与审计中必须引入SDR感知的报告实践，以确保评估结果的客观性和有效性。

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

</details>


### [17] [Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective](https://arxiv.org/abs/2602.17283)
*Yukun Chen,Xinyu Zhang,Jialong Tang,Yu Wan,Baosong Yang,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文提出X-Value基准测试，用于评估大语言模型在多语言环境下对内容深层价值观的判断能力。该基准包含18种语言的5000多个问答对，基于施瓦茨基本价值观理论构建，发现当前先进模型在跨语言价值观评估上准确率低于77%，且不同语言间性能差异超过20%。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的内容安全评估主要关注暴力、仇恨言论等显性危害，忽视了数字内容中传递的更为微妙的价值维度。为了弥补这一空白，需要从全球视角评估模型对内容深层价值观的判断能力。

Method: 研究团队构建了X-Value跨语言价值观评估基准，包含18个语种的5000多个问答对，系统性地组织为基于施瓦茨基本价值观理论的7个核心领域，并分为简单和困难两个级别。创新性地提出了两阶段标注框架：首先识别问题属于全球共识（如人权）还是多元主义（如宗教），随后进行多方评估以判断内容中嵌入的潜在价值观。

Result: 对X-Value的系统性评估显示，当前先进大语言模型在跨语言价值观评估方面存在不足，准确率低于77%，且不同语言间的性能差异显著，准确率差距超过20个百分点。

Conclusion: 本研究强调了大语言模型在细致、价值观感知的内容评估能力方面亟需改进，并公开了X-Value基准以促进相关研究发展。

Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\%$), with significant performance disparities across different languages ($ΔAcc > 20\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.

</details>


### [18] [Representation Collapse in Machine Translation Through the Lens of Angular Dispersion](https://arxiv.org/abs/2602.17287)
*Evgeniia Tokarchuk,Maya K. Nachesa,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 针对Transformer神经机器翻译中的表征坍缩问题，本研究分析了其在离散与连续模型训练过程中的层级动态特性，验证了基于角度离散度的正则化方法在缓解坍缩、提升翻译质量方面的有效性，并证实了该方法在量化模型中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构的神经机器翻译模型性能优异，但标准的下一词元预测训练策略会导致深层表征空间坍缩，尤其在连续输出端到端训练中更为显著，可能使模型退化为将所有向量设为相同值的最简解，无法充分利用几何空间。

Method: 通过系统分析表征坍缩在离散与连续NMT Transformer各层训练过程中的动态演化，引入并验证了基于角度离散度的正则化策略，同时考察了量化模型中的坍缩现象。

Result: 实验证实，角度离散度正则化不仅能有效抑制表征坍缩，还能提升翻译质量；量化模型同样存在坍缩行为，且正则化效果在量化后仍然保持。

Conclusion: 表征坍缩是影响Transformer NMT模型性能的关键问题，基于角度离散度的正则化方法提供了一种有效解决方案，其鲁棒性在模型量化过程中得以保持，为优化NMT模型训练提供了实用途径。

Abstract: Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.

</details>


### [19] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan Kostić,Conor Fallon,Julian Risch,Alexander Löser*

Main category: cs.CL

TL;DR: 本研究通过生成保持语义不变的词汇和句法扰动，检验了23个大型语言模型在MMLU、SQuAD和AMEGA三个基准测试中的表现。发现词汇层面的扰动会导致几乎所有模型性能显著下降，而句法扰动影响不均，甚至偶尔提升性能；模型鲁棒性与模型规模无必然关联，表明LLMs过度依赖表层词汇模式而非抽象语言能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）快速发展，标准化评估基准已成为模型比较的主要工具，但其可靠性因模型对输入提示的浅层变化过于敏感而受到质疑。现有评估体系可能无法真实反映模型深层语言理解能力，需要通过受控的语义等价扰动来检验模型鲁棒性。

Method: 研究采用两种语言学原则的扰动生成管道：一是基于同义词替换的词汇扰动，二是利用依存句法分析确定适用转换的句法扰动。在MMLU、SQuAD和AMEGA三个基准上，对23个当代LLMs生成保持真值条件不变的文本变体，系统评估其绝对性能与相对排序的变化。

Result: 实验表明：1）词汇扰动导致几乎所有模型和任务的统计显著性能下降；2）句法扰动影响异质性更强，偶有性能提升现象；3）两类扰动均会动摇复杂任务的模型排行榜；4）模型鲁棒性未随参数量增长而同步提升，表现出显著的任务依赖性。

Conclusion: 研究证实LLMs主要依赖表层词汇模式而非抽象语言能力，当前评估基准存在脆弱性。为确保评估可靠性，应将鲁棒性测试作为LLM评估的标准环节，推动建立更全面的模型能力评价体系。

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [20] [RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering](https://arxiv.org/abs/2602.17366)
*Yiming Zhang,Siyue Zhang,Junbo Zhao,Chen Zhao*

Main category: cs.CL

TL;DR: 针对稠密检索器在长尾知识检索中的泛化难题，本文提出RPDR数据增强框架，通过合成数据生成与Round-Trip预测筛选高质量易学习样本，在PopQA和EntityQuestion基准上显著超越BM25和Contriver，并提出动态路由机制以进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长尾问答中面临低频知识获取与回忆困难，尽管RAG系统引入外部检索机制，但稠密检索器在稀有/小众知识泛化上仍存在局限，亟需提升其长尾检索能力。

Method: RPDR框架包含：1）合成数据生成；2）基于Round-Trip预测的数据选择，识别易学习实例；3）利用筛选数据训练稠密检索器。通过精选高质量易学习样本增强模型对长尾知识的检索效果。

Result: 在PopQA与EntityQuestion长尾检索基准测试中，RPDR相比BM25和Contriver等基线模型实现显著性能提升，特别是在极长尾类别上优势明显，人工分析验证了方法的有效性与局限性。

Conclusion: RPDR通过数据选择策略有效增强了稠密检索器的长尾检索性能，未来将引入动态路由机制，根据查询特性分配至专用检索模块以持续优化系统表现。

Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.

</details>


### [21] [The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour](https://arxiv.org/abs/2602.17377)
*Leonidas Zotos,Hedderik van Rijn,Malvina Nissim*

Main category: cs.CL

TL;DR: 本研究通过计算多选题选项在大规模语料库中的概念普及率来量化认知可得性，发现正确答案显著比错误答案更易获得。在三个大型题库上，选择最可得选项的得分比随机猜测基线高出13.5%-32.9%。该模式在大语言模型生成的选项中同样存在，证实可得性启发式是影响学生答题的重要因素。


<details>
  <summary>Details</summary>
Motivation: 基于Tversky和Kahneman（1973）的可得性启发式理论，探究学生在无确定答案时采用"最易联想到选项"策略的有效性，并建立可计算的认知评估方法以量化该心理机制。

Method: 提出计算方法：通过测量选项概念在大型文本语料库（维基百科）中的出现频率来操作化认知可得性。在三个大规模多选题数据集上进行验证，并对比分析大语言模型生成选项与专家编写选项的可得性分布特征。

Result: 1) 正确答案的认知可得性独立于题干而显著高于错误选项；2) 采用最可得选项策略在三个数据集上均获得13.5%-32.9%的超额分数提升；3) 大语言模型生成的选项虽基于统计频率训练，但其可得性模式与专家选项相似。

Conclusion: 可得性启发式在多选题作答中发挥重要作用，未来计算建模学生行为时应系统性地纳入选项可得性特征，以提高模型预测准确性和对认知机制的理解。

Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.

</details>


### [22] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 本文针对跨文档共指消解(CDCR)在多样化、极化新闻报道分析中的局限，提出改进的NewsWCL50标注方案。新方案将共指链视为话语元素(DEs)，支持身份与准身份关系，可链接"大篷车"-"寻求庇护者"-"考虑非法入境者"等表达。通过统一代码本重新标注NewsWCL50和ECB+子集，评估显示重新标注数据集介于原始版本之间，支持新闻领域的平衡且具话语意识的CDCR研究。


<details>
  <summary>Details</summary>
Motivation: 现有CDCR数据集主要聚焦事件消解且采用狭义的共指定义，无法有效处理措辞差异大的多样化、极化新闻覆盖。这种局限性阻碍了模型捕捉媒体话语中的词汇多样性和框架变化能力，难以满足细粒度话语参与者层面的内容分析需求。

Method: 提出将共指链重新定义为话语元素(DEs)和概念分析单元的标注方案，支持身份和准身份关系（如链接"the caravan"-"asylum seekers"-"those contemplating illegal entry"）。使用统一代码本对NewsWCL50全文集和ECB+子集进行重新标注，并通过词汇多样性指标和相同词干(same-head-lemma)基线方法评估新数据集的质量和对齐程度。

Result: 重新标注的数据集在词汇多样性和标注特性上表现出高度一致性，其特性值介于原始ECB+和NewsWCL50之间。这表明新标注方案成功平衡了细粒度话语元素标注与跨文档共指关系的灵活性，为新闻领域提供了更适切的研究基准。

Conclusion: 本研究提出的基于话语元素的CDCR标注方案有效解决了现有数据集在新闻分析中的局限性，通过容纳身份与准身份关系，使模型能够捕捉媒体话语的词汇多样性和框架差异。重新标注的NewsWCL50和ECB+子集为推进平衡且具话语意识的跨文档共指消解研究提供了高质量资源。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


### [23] [Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics](https://arxiv.org/abs/2602.17425)
*Sanjeev Kumar,Preethi Jyothi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文针对极低资源语言机器翻译评估难题，对比分析了BLEU和ChrF++两种指标在Magahi、Bhojpuri和Chhattisgarhi三种语言上的表现，发现BLEU虽得分较低但能提供互补的词汇精确度洞察，增强评估结果的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器翻译评估指标如BLEU在高资源语言场景下表现良好，但在极低资源语言环境中往往无法准确反映翻译质量，导致评估失真。现有研究多依赖ChrF++指标，缺乏对BLEU在该场景下价值的深入探讨。

Method: 研究采用对比分析方法，在Magahi、Bhojpuri和Chhattisgarhi三种极低资源语言上，针对大语言模型和神经机器翻译系统的输出，考察BLEU和ChrF++如何响应翻译幻觉、重复、源文本复制以及变音符号变异等特定现象。

Result: 实验结果表明，尽管BLEU的绝对分值较低，但其在词汇精确度测量方面具有独特价值，能够与ChrF++形成互补，提供更全面的翻译质量评估视角，从而增强结果的可解释性。

Conclusion: 在极低资源语言机器翻译评估中，不应单一依赖ChrF++指标，而应结合使用BLEU以获取更全面的评估洞察，这对提升翻译质量评估的可靠性和可解释性具有重要意义。

Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.

</details>


### [24] [Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study](https://arxiv.org/abs/2602.17431)
*Dylan Bouchard,Mohit Singh Chauhan,Viren Bajaj,David Skarbrevik*

Main category: cs.CL

TL;DR: 本研究提出长文本LLM输出的细粒度不确定性量化(UQ)分类框架，通过三阶段设计选择系统化现有方法，实验证明声明-响应蕴含、声明级评分及不确定性感知解码的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法主要用于短文本LLM幻觉检测，但难以泛化至长文本生成，缺乏系统性框架以指导方法选择与性能比较。

Method: 提出长文本LLM输出的细粒度UQ分类法，从响应分解、单元级评分和响应级聚合三阶段区分设计选择；形式化一致性黑盒评分方法族，推广并扩展现有技术。

Result: 在多个LLM和数据集上的实验表明：1)声明-响应蕴含方法性能优于或相当于更复杂的声明级评分器；2)声明级评分普遍优于句子级评分；3)不确定性感知解码对提升长文本事实性非常有效。

Conclusion: 该框架厘清了既有方法间的关系，支持公平比较，并为细粒度UQ组件选择提供实践指导，有助于推动长文本生成的可信度评估。

Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.

</details>


### [25] [AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue](https://arxiv.org/abs/2602.17443)
*Adib Sakhawat,Fardeen Sadab,Rakin Shahriar*

Main category: cs.CL

TL;DR: 研究发现大语言模型在策略推理中存在明显不对称性，信息控制能力远超信息提取能力，主要瓶颈在于全局状态追踪和对话负载下的指令遵循。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型的策略推理能力需要超越静态基准，转向动态多轮交互场景。本研究旨在探索对话系统中信息提取（主动推理）与信息控制（状态维护）之间的能力不对称性。

Method: 提出AIDG（对抗性信息推理游戏）这一博弈论框架，包含两个互补任务：AIDG-I用于测量社交推理中的语用策略，AIDG-II用于测量结构化"20个问题"场景中的约束满足能力。研究对六个前沿大语言模型进行了439场游戏实验。

Result: 实验发现显著的能力不对称现象：模型在信息控制方面的表现远优于信息提取，防御端具有350分的ELO优势（科恩d值=5.47）。研究识别出两个关键瓶颈：(1)信息动态性方面，确认策略的有效性比盲目推理高出7.75倍（p < 0.00001）；(2)约束遵循方面，指令遵循能力在对话负载下显著退化，导致41.3%的推理失败。

Conclusion: 尽管大语言模型擅长维持局部防御一致性，但在战略探究所需的全局状态追踪方面仍面临挑战。

Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured "20 Questions" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.

</details>


### [26] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: 本研究通过合成无意义问答基准发现大语言模型在多选题评测中存在标签位置少样本提示偏差，进而提出基于无序标签与句相似度模型的无偏评测协议。该方法将准确率方差降低3倍，性能损失极小，有效暴露模型在减少评估伪影下的真实能力。


<details>
  <summary>Details</summary>
Motivation: 多选题基准作为评估大语言模型推理与知识问答能力的标准方法，存在严重评估偏差。通过合成无意义问答基准，研究发现不同模型表现出标签位置少样本提示偏差：模型利用答案位置、标签文本、或提示中正确答案分布来答题，而非依赖真实推理能力，导致评估结果夸大模型性能并掩盖真实水平。

Method: 提出偏差消减评测协议：1）将问题标签替换为统一无序标签；2）指令模型直接输出完整答案文本而非标签；3）采用句相似度模型评估答案匹配度。该协议无需依赖提示示例或选项标签即可工作，通过简单语义匹配机制实现无偏评估。

Result: 跨多基准与大语言模型的实验表明，该协议使答案排列的准确率方差平均降低3倍，模型性能仅轻微下降。消融研究证实，该方法在不同嵌入模型与相似度函数下均比标准评测方法更稳定，鲁棒性显著提升。

Conclusion: 所提协议通过消除标签相关评估偏差，更真实地揭示了大语言模型的实际推理能力，为构建公平可靠的评测体系提供了简单有效的解决方案，对提升评估质量具有重要实践价值。

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [27] [Entropy-Based Data Selection for Language Models](https://arxiv.org/abs/2602.17465)
*Hongming Li,Yang Liu,Chao Huang*

Main category: cs.CL

TL;DR: 本文提出基于熵的无监督数据选择框架EUDS，用于计算受限的语言模型微调场景。该方法通过熵值估计数据不确定性，建立高效数据过滤机制，在情感分析、主题分类和问答任务上验证可显著降低计算成本并提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调需要大量计算和数据资源，但实际应用常受资源限制。数据选择虽能减少训练数据量，却依赖高计算预算。此外，评估数据可用性仍是难题，使得高效数据选择不可或缺。

Method: 提出Entropy-Based Unsupervised Data Selection (EUDS)框架，系统揭示数据选择与数据不确定性估计的关系，利用熵值评估数据可用性，构建无监督的数据过滤机制。

Result: 在情感分析、主题分类和问答任务上的实证研究表明，EUDS能显著降低计算成本，提升训练时间效率，且所需数据量更少。理论与实验结果均验证了其有效性。

Conclusion: EUDS为计算受限场景下的语言模型高效微调提供了创新解决方案，通过熵基无监督选择机制，在降低资源需求的同时保持性能，具有重要的实践价值。

Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.

</details>


### [28] [PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions](https://arxiv.org/abs/2602.17467)
*Greta Damo,Stéphane Petiot,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 本文提出PEACE 2.0，一个能分析解释仇恨言论并生成反言论回复的新工具。该工具利用检索增强生成（RAG）技术，实现仇恨言论解释的事实依据grounding、自动生成有证据的反言论，并探索反言论特性，从而处理显性和隐性仇恨消息。


<details>
  <summary>Details</summary>
Motivation: 在线仇恨言论日益增多带来社会挑战。虽然NLP在检测仇恨言论方面已有有效方法，但生成反言论回应仍是开放挑战，缺乏能够同时解释仇恨言论并生成有根据反言论的工具。

Method: 提出PEACE 2.0框架，整合检索增强生成（RAG）管道，提供三大新功能：将仇恨言论解释基于证据和事实、自动生成证据grounded的反言论、探索反言论回复特征，以支持对显性和隐性仇恨消息的综合处理。

Result: 开发了PEACE 2.0工具，实现了仇恨言论分析与反言论生成的集成，能够生成有证据支持的反言论回复，为仇恨言论治理提供了端到端解决方案。

Conclusion: PEACE 2.0通过RAG技术实现了仇恨言论检测、解释和反言论生成的统一框架，为应对在线仇恨言论提供了新工具。该工具有助于实现更深入的分析和更有效的反言论生成。

Abstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.

</details>


### [29] [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers](https://arxiv.org/abs/2602.17469)
*Nusrat Jahan Lia,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本研究探讨孟加拉语与英语之间的跨语言情感错位问题，通过基准测试四种Transformer架构，揭示当前AI对齐范式的严重安全与表征缺陷：压缩模型mDistilBERT存在28.7%的情感倒置率，系统性误解用户意图；模型表现出"不对称共情"现象，对孟加拉语情感权重存在衰减或放大；区域模型IndicBERT对正式Sadhu孟加拉语的处理错误率增加57%，暴露"现代偏见"。作者主张采用多元文化基础的对齐方式而非通用压缩，以确保情感保真度和互信。


<details>
  <summary>Details</summary>
Motivation: 双向对齐的核心是确保AI系统准确理解人类意图且人类信任AI行为，但该闭环在语言障碍下严重断裂。本研究针对孟加拉语与英语间的跨语言情感错位问题，旨在揭示当前对齐范式在低资源语言环境中的根本性失效，包括意图误解和信任侵蚀等安全隐患。

Method: 研究采用基准测试方法，系统评估四种Transformer架构在孟加拉语-英语跨语言情感对齐任务中的表现。通过量化情感倒置率、不对称共情效应以及方言偏见等指标，识别模型在低资源和方言语境下的系统性失败模式。

Result: 结果表明：(1)mDistilBERT模型情感倒置率达28.7%，将积极意图误判为消极（或反之）；(2)存在"不对称共情"现象，部分模型系统性削弱而另一些则放大的孟加拉语文本情感权重；(3)IndicBERT模型在处理正式Sadhu孟加拉语时对齐错误率增加57%，显示"现代偏见"。

Conclusion: 结论指出，公平的人机协同演化需要多元文化基础的对齐范式，尊重语言与方言多样性而非追求通用压缩。建议将对齐基准纳入"情感稳定性"指标，明确惩罚低资源及方言语境下的极性倒置，以维护情感保真度和互惠信任。

Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

</details>


### [30] [Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian](https://arxiv.org/abs/2602.17475)
*Pietro Ferrazzi,Mattia Franzin,Alberto Lavelli,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本研究系统评估约10亿参数的小语言模型在20项医疗NLP任务中的表现，比较推理时与训练时适配策略，发现监督微调效果最佳，小模型可超越更大规模基线，并开源意大利语医疗数据集及预训练语料。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在医疗自然语言处理任务中表现出色，但其高昂的计算成本严重制约了在真实医疗环境中的部署。为此，本研究探讨参数规模约10亿的小型语言模型是否能在保持竞争性能的同时，满足实际医疗应用的可行性与效率需求。

Method: 研究评估Llama-3、Gemma-3和Qwen3三个模型族，覆盖命名实体识别、关系抽取、病例报告表填写、问答及论元挖掘等20项临床NLP任务。系统对比推理阶段策略（少样本提示、约束解码）与训练阶段策略（监督微调、持续预训练）的效果差异。

Result: 监督微调在各项指标上均表现最优；少样本提示结合约束解码在资源受限场景下提供强有力替代方案。小型语言模型整体性能可匹配甚至超越更大规模基线，其中Qwen3-1.7B最优配置相比Qwen3-32B平均得分高出9.2个点。

Conclusion: 小型语言模型在医疗NLP领域展现出巨大应用潜力，通过适当优化可实现高效部署。研究团队公开了涵盖20项任务的意大利语医疗NLP数据集及高性能模型，并额外发布来自意大利医院急诊科的1.26亿词料及多源1.75亿词料用于持续预训练，以促进后续研究。

Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether "small" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.

</details>


### [31] [Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics](https://arxiv.org/abs/2602.17513)
*Baris Karacan,Barbara Di Eugenio,Patrick Thornton*

Main category: cs.CL

TL;DR: 本文构建产科学笔记新数据集，系统评估Transformer监督模型与零样本大语言模型在临床章节分割中的表现，发现零样本模型在纠正幻觉后域外适应能力更强，为医疗NLP跨域应用提供新方向。


<details>
  <summary>Details</summary>
Motivation: 临床自由文本笔记蕴含重要患者信息，其章节结构识别对临床决策和下游NLP任务具有重要意义。现有研究多基于MIMIC-III等公开语料库，医学领域覆盖有限，模型泛化能力与零样本大语言模型的潜力尚未得到充分评估。

Method: 研究通过三项贡献推进章节分割：(1) 构建去标识化、带章节标签的产科学笔记新数据集；(2) 系统评估Transformer监督模型在MIMIC-III（域内）和产科学数据（域外）上的性能；(3) 首次对监督模型与零样本大语言模型进行头对头比较。

Result: 实验显示，监督模型在域内表现强劲但域外性能显著下降，而零样本模型在纠正幻觉性章节标题后展现出稳健的域外适应能力，凸显了领域特异性资源开发的重要性。

Conclusion: 研究强调开发领域特异性临床资源的必要性，并提出零样本章节分割是扩展医疗NLP应用至已研究语料库之外的重要方向，但需有效管理模型幻觉问题。

Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.

</details>


### [32] [Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems](https://arxiv.org/abs/2602.17542)
*Zhangqi Duan,Arnav Kankaria,Dhruv Kartik,Andrew Lan*

Main category: cs.CL

TL;DR: 针对编程任务中知识组件(KC)级正确标签缺失的问题，本文提出一种基于大语言模型的自动化框架，直接从学生代码中标注KC级正确性，并引入时序感知的Code-KC映射机制，实验表明该方法能获得更符合认知理论的学习曲线并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 知识组件(KCs)是学生建模和学习分析的基础，但实际数据集中KC级正确标签极少，尤其是开放式编程任务。简单将题目级正确性传播到所有关联KC会掩盖部分掌握情况，导致学习曲线拟合不佳。

Method: 提出一个利用大语言模型(LLM)从学生代码中自动标注KC级正确性的框架，并引入时序上下文感知的Code-KC映射机制，以更好对齐KCs与个体学生代码。

Result: 实验结果表明，该框架生成的学习曲线更符合认知理论，相比基线方法提升了预测性能，且人工评估显示LLM标注与专家标注具有高度一致性。

Conclusion: 基于LLM的KC级自动标注框架能有效解决真实场景中细粒度标签缺失问题，为学习分析提供更准确的数据基础。

Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.

</details>


### [33] [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning](https://arxiv.org/abs/2602.17546)
*Jyotin Goel,Souvik Maji,Pratik Mazumder*

Main category: cs.CL

TL;DR: 该论文提出了一种自适应正则化框架，通过训练时动态评估安全风险（使用评判模型或激活分析），在高风险更新时约束模型接近安全参考策略，在低风险时保持标准训练，从而在不牺牲效用且无推理成本的前提下，持续降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 指令跟随语言模型在良性微调或对抗更新下会出现安全性退化，而现有防御措施要么保护有限，要么在安全与效用间存在权衡，亟需一种能在微调全周期保持对齐的新方法。

Method: 提出自适应正则化训练框架，采用两种风险估计方法：1）基于评判模型的安全评判器，为训练批次分配危害分数；2）基于激活的风险预测器，用轻量级分类器分析中间层激活来估计有害意图。根据风险信号动态调整正则化强度，高风险更新约束接近安全参考策略，低风险更新正常训练。

Result: 实证验证了生成前激活可预测有害意图，评判器分数提供高召回安全指导。在多模型族和攻击场景下，相比标准微调，该框架持续降低攻击成功率，保持下游性能，且不增加推理时开销。

Conclusion: 该工作展示了一种在不牺牲效用前提下维持安全性的原则性机制，通过训练时自适应正则化实现了安全与性能的平衡。

Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.

</details>


### [34] [Modeling Distinct Human Interaction in Web Agents](https://arxiv.org/abs/2602.17588)
*Faria Huq,Zora Zhiruo Wang,Zhanqiu Guo,Venu Arvind Arangarajan,Tianyue Ou,Frank Xu,Shuyan Zhou,Graham Neubig,Jeffrey P. Bigham*

Main category: cs.CL

TL;DR: 本研究针对自主网页智能体在任务执行中缺乏对人类干预时机理解的问题，提出了人类干预行为建模任务。通过构建包含400条真实用户导航轨迹的CowCorpus数据集，识别出四种用户交互模式，并训练语言模型预测干预行为，最终在用户研究中使智能体实用性评分提升26.5%。


<details>
  <summary>Details</summary>
Motivation: 当前自主网页智能体缺乏对人类干预时机的原则性理解，经常在关键决策点自主推进或请求不必要的确认，而人类参与对于塑造偏好和纠正智能体行为至关重要。因此，需要建立对人类干预行为的建模来支持协作式网页任务执行。

Method: 1) 收集了CowCorpus数据集，包含400条真实用户的网页导航轨迹（含4200+交替的人类与智能体动作）；2) 识别出四种用户交互模式：放手监督、动手监督、协作解题和用户完全接管；3) 基于这些模式训练语言模型来预测用户干预可能性。

Result: 在干预预测准确率上，所训练的语言模型相比基础模型提升了61.4-63.4%；在实时网页导航智能体部署的用户研究中，使用干预感知模型的智能体获得了26.5%的用户实用性评分提升。

Conclusion: 对人类干预行为进行结构化建模能够使智能体更准确地预测人类介入时机，从而产生更具适应性和协作性的智能体系统，最终显著提升用户体验和智能体实用性。

Abstract: Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.

</details>


### [35] [Unmasking the Factual-Conceptual Gap in Persian Language Models](https://arxiv.org/abs/2602.17623)
*Alireza Sakhaeirad,Ali Ma'manpoosh,Arshia Hemmat*

Main category: cs.CL

TL;DR: 本研究提出DivanBench，一个针对波斯文化中超自然信仰与习俗的诊断性基准，通过315道涵盖事实检索、场景验证与情境推理三类任务的问题，评估七种波斯语大模型。研究发现模型存在严重顺从偏差、持续预训练会放大该缺陷，且知识检索与应用间存在21%性能鸿沟，揭示当前模型仅能表面模仿文化模式而无法真正内化其内在逻辑。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语NLP基准虽已扩展至语用学与礼貌性层面，但未能有效区分模型是记忆文化事实还是具备推理隐式社会规范的能力。研究旨在揭示文化能力不仅是数据规模问题，更需要深层推理机制。

Method: 构建DivanBench诊断基准，聚焦于无法通过简单逻辑推导的任意性、语境依赖的文化规则（迷信与习俗）。包含315个问题，分为：1)事实检索；2)配对场景验证；3)情境推理三类任务。对七种波斯语大模型进行系统评估。

Result: 三大关键失败：1)严重顺从偏差：模型能正确识别恰当行为但无法拒绝明显违规；2)持续波斯语预训练反而加剧偏差，常导致模型识别矛盾能力退化；3)所有模型在事实知识检索与场景应用间存在21%的性能差距。

Conclusion: 文化能力无法通过单纯扩展单语数据实现，当前模型仅学会模仿文化模式而未内化其底层认知图式，需在模型架构与训练范式上进行根本性改进。

Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.

</details>


### [36] [Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking](https://arxiv.org/abs/2602.17653)
*Iskar Deng,Nathalia Xu,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 本研究通过控制合成学习方法，在18种差异论元标记系统上训练GPT-2模型，发现模型能复现人类语言的自然标记方向偏好（显性标记语义非典型论元），但无法复现强宾语偏好，表明不同类型学趋势可能源于不同的底层机制。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明，在合成语料上训练的语言模型能展现类似人类语言的语言类型学偏好，尤其在语序等句法现象上。本研究旨在将这一研究范式扩展至差异论元标记（DAM）这一语义许可系统，以探究模型是否能复现人类语言中关于DAM的类型学规律。

Method: 采用受控合成学习范式，构建18种实现不同DAM系统的合成语料库，训练GPT-2语言模型，并利用最小对立对（minimal pairs）方法评估模型对DAM系统的泛化能力。

Result: 研究发现DAM的两个类型学维度存在分离现象：模型稳定地表现出类似人类的自然标记方向偏好，即倾向于对语义非典型论元进行显性标记；但模型未能复现人类语言中显著的宾语偏好特征，即DAM系统中显性标记更常作用于宾语而非主语的现象。

Conclusion: 实验结果表明，不同的类型学倾向可能源于不同的底层认知或语言机制。

Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.

</details>


### [37] [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655)
*Clara Meister,Ahmetcan Yavuz,Pietro Lesci,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文提出UniLID，一种基于UnigramLM分词算法的语言识别方法，通过学习共享词表上的语言条件性单字分布，在低资源语言和高精度方言识别上实现样本效率和性能突破


<details>
  <summary>Details</summary>
Motivation: 现有语言识别系统在高资源语言上表现优异，但在低资源语言和相近语言场景下鲁棒性不足，难以满足实际多语言处理管道的需求

Method: 采用UnigramLM分词算法的概率框架，学习共享词表上的语言条件性单字分布，将分词视为语言特定现象，支持增量添加新语言且无需重新训练，可直接集成到现有语言模型分词流水线中

Result: 在标准基准测试中，UniLID与fastText、GlotLID、CLD3等基线模型性能相当；在低资源场景下仅需每语言5个标注样本即可达到70%以上准确率，样本效率显著提升；在细粒度方言识别任务上获得大幅性能增益

Conclusion: UniLID方法简洁高效，在保持竞争力的同时大幅提升了低资源语言识别的样本效率，展现了在真实世界多语言处理应用中的实用价值

Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.

</details>


### [38] [Sink-Aware Pruning for Diffusion Language Models](https://arxiv.org/abs/2602.17664)
*Aidar Myrzakhan,Tianyi Li,Bowei Guo,Shengkun Tang,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 扩散语言模型推理开销大，需高效剪枝。研究发现AR模型中稳定的注意力汇点在DLMs中表现出高方差和不稳定性，据此提出Sink-Aware Pruning方法自动识别并剪枝不稳定汇点，在不重新训练的情况下实现更优的质量-效率平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(DLMs)因迭代去噪导致推理成本高昂，亟需高效剪枝策略。现有方法多沿用自回归大语言模型的剪枝启发式规则，默认保留注意力汇点作为稳定全局锚点，但此假设在DLMs中尚未得到验证。

Method: 提出Sink-Aware Pruning，通过量化注意力汇点在整个生成过程中的位置方差来识别不稳定的汇点。该方法基于关键观察：DLMs中的主导汇点位置随时间步显著漂移，表明其瞬时性和结构非必要性，故可安全剪枝。

Result: 无需重新训练，在匹配计算预算下，该方法取得更优的质量-效率权衡，性能优于现有强剪枝基线。

Conclusion: 研究揭示了DLMs与AR模型在注意力汇点特性上的本质差异，DLMs的汇点具有高度动态性而非结构性。Sink-Aware Pruning为高效DLM推理提供了针对性解决方案，证明去除不稳定汇点是一种有效的压缩策略。

Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: 本文提出AIdentifyAGE本体，这是一个针对法医牙科年龄评估的标准化语义框架，旨在解决现有方法异质性、数据碎片化和系统互操作性不足的问题。该本体整合人工与AI辅助工作流程，实现从观察到结果的全程可追溯链接，为法医司法决策支持系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 法医年龄评估在涉及无证件人员和无人陪伴未成年人的司法决策中至关重要，但现有牙科年龄评估方法存在方法论异质性、数据表示碎片化以及临床、法医和法律信息系统之间互操作性有限等问题。这些问题损害了透明度和可重复性，尤其随着AI方法的日益普及，亟需标准化框架来确保评估的一致性和可解释性。

Method: 开发AIdentifyAGE本体，这是一个领域特定的语义框架，涵盖人工和AI辅助的法医牙科年龄评估工作流程。该本体通过模块化方式建模完整的法医-司法工作流程，整合司法背景、个体信息、法医检查数据、牙科发育评估方法、放射影像、统计参考研究和AI估计算法。开发过程与领域专家协作，并基于现有上层本体和生物医学、牙科及机器学习领域的成熟本体，确保互操作性、可扩展性和FAIR原则合规性。

Result: 成功构建了一个标准化的法医牙科年龄评估本体框架，实现了观察、方法、参考数据和报告结果之间的可追溯链接。该本体支持人工和AI方法的整合应用，增强了评估过程的透明度、可重复性和可解释性，为开发基于本体的法医司法决策支持系统提供了坚实基础。

Conclusion: AIdentifyAGE本体是提升法医年龄评估一致性、透明度和可解释性的重要基础步骤。它通过标准化语义框架解决了当前领域的关键挑战，为构建稳健的法医-司法决策支持系统奠定了基石，有望在法医和司法实践中发挥重要作用。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [40] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 本文证明适应性系统中的情境性并非量子力学独有，而是经典概率表示中单状态重用的必然结果，会导致不可约的信息论代价，从而揭示其为适应性智能的普遍表示约束。


<details>
  <summary>Details</summary>
Motivation: 适应性系统因记忆、表示或物理资源限制，常在多情境间重用固定内部状态空间。单状态重用虽在自然与人工智能中无处不在，但其基本表示后果尚未被充分理解，核心问题在于情境性是否仅为量子力学特有现象。

Method: 将情境建模为作用于共享内部状态的干预，运用信息论方法证明：任何能够复现情境化结果统计的经典概率模型都必须承担不可约的信息论代价，即情境依赖无法仅通过内部状态完全中介。

Result: 证明情境性是单状态重用的必然产物而非量子力学特例；提供最小构造性示例阐明该代价的操作意义；揭示非经典概率框架通过放弃单一全局联合概率空间假设来规避此限制（无需依赖量子动力学或希尔伯特空间结构）。

Conclusion: 情境性被重新界定为适应性智能的普遍表示约束，独立于物理实现方式。这一发现超越了量子力学框架，揭示了单状态重用系统固有的根本限制，为理解智能系统的表示能力提供了新的理论基础。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [41] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache是一个移动性感知的缓存框架，通过可重构缓存和轻量级解码器，显著提升大规模人类移动模拟的效率，同时保持与最新LLM方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模人类移动模拟在城市规划、流行病学和交通分析中至关重要，但现有基于大型语言模型的方法计算成本高、扩展性差。

Method: 提出MobCache框架，包含推理组件和解码组件：推理组件将每个推理步骤编码为潜在空间嵌入，并使用潜在空间评估器实现推理步骤的重用与重组；解码组件采用轻量级解码器，通过移动规律约束的知识蒸馏将潜在空间推理链转化为自然语言。

Result: 实验表明，MobCache在多个维度上显著提升效率，同时保持与最新LLM方法相当的性能。

Conclusion: MobCache通过可重构缓存和轻量级解码，有效解决了LLM在大规模移动模拟中的计算瓶颈，为高效、可扩展的移动行为模拟提供了新思路。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [42] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 本研究分析60个主流大语言模型基准测试发现，近半数基准存在饱和现象且随时间加剧。通过14个属性分析揭示：专家策划的基准比众包基准更抗饱和，而隐藏测试数据并无保护作用，为构建持久评估体系提供关键设计启示。


<details>
  <summary>Details</summary>
Motivation: 人工智能基准测试对衡量模型进展和指导部署至关重要，但许多基准迅速饱和，无法区分顶尖模型，严重削弱其长期实用价值，亟需系统性研究饱和成因。

Method: 从主流模型开发商技术报告中选取60个大语言模型基准，沿任务设计、数据构建和评估格式三个维度刻画14项基准属性，并通过假设检验分析各属性对饱和率的影响机制。

Result: 量化分析表明：48%的基准存在饱和，且饱和概率随基准年龄显著上升；隐藏测试集（公开vs私有）未能延缓饱和，而专家策划基准的饱和风险显著低于众包基准，揭示关键设计因素的影响差异。

Conclusion: 研究明确了提升基准测试寿命的核心设计原则：优先采用专家策划而非众包模式，并质疑当前依赖隐藏测试数据的保护策略，为构建更具长期效度的AI评估体系提供实证依据。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [43] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 该研究改进了n维超立方体所有边所需最少切割超平面数的上界，证明S(n) ≤ ⌈4n/5⌉（n为5的奇数倍时例外，此时S(n) ≤ 4n/5 + 1），并开发了CPro1自动工具辅助构造。


<details>
  <summary>Details</summary>
Motivation: 超立方体边切割问题是计算几何与组合优化的核心问题，涉及高维空间划分与算法设计。1971年Paterson给出的上界S(n) ≤ ⌈5n/6⌉已沿用数十年，改进该界限对理解高维几何结构、优化计算复杂性具有重要意义。

Method: 采用构造性证明方法，开发CPro1自动工具——一种结合推理型大语言模型与自动超参数调优的搜索算法生成系统。通过该工具发现可切割Q₁₀的8超平面构造，并推广至一般维度，实现界限优化。

Result: 1. 主结果：对所有n，S(n) ≤ ⌈4n/5⌉；当n ≡ 5 (mod 10)时，S(n) ≤ 4n/5 + 1。
2. 改进历史界限：优于Paterson的⌈5n/6⌉。
3. 次级结果：获得k < n个超平面可切割的最大边数的新下界。
4. 构造实例：明确给出切割Q₁₀的8超平面配置。

Conclusion: 该工作显著推进了超立方体切割问题的理论界限，验证了AI驱动方法在发现非平凡数学构造中的有效性。CPro1框架为组合几何问题提供了自动化研究新范式，未来可拓展至更广泛的离散结构优化问题。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [44] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文针对犹豫模糊集传统评分方法缺乏序理论形式化基础的问题，提出基于特定序关系的统一评分框架。通过分析经典序结构，揭示其不构成格结构，但对称序评分满足强单调性与Gärdenfors条件。进而提出用于排序的优势度函数，给出离散与相对优势度函数实例，可构建模糊偏好关系并应用于群决策。


<details>
  <summary>Details</summary>
Motivation: 现有犹豫模糊集评分方法缺乏序理论基础，导致机制不灵活且不一致。文献中关于经典序诱导格结构的论断需要验证，亟需建立规范化的序导向评分框架以确保理论严密性。

Method: 构建序导向的统一框架，将评分函数与特定序关系显式关联。系统考察[0,1]非空子集（犹豫模糊元）上的经典序结构，验证其格结构性质；重点研究对称序，并证明其评分函数满足并集强单调性与Gärdenfors条件等规范性准则。

Result: 经典序结构不诱导格结构，与先前结论相悖。对称序评分满足强单调性与Gärdenfors条件。提出优势度函数类，用于在含最小可接受阈值的控制集下排序犹豫模糊元，具体给出离散优势度函数与相对优势度函数两种实现。

Conclusion: 该框架为犹豫模糊集评分提供了严格的理论依据。优势度函数能有效构建模糊偏好关系，支持群决策过程，相比传统方法更具灵活性和一致性，具有重要理论价值与实践意义。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [45] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: 针对现有智能体开发套件功能不足或需人工设计的问题，本文提出OpenSage——首个支持大语言模型自动生成具备自生成拓扑、工具集和结构化记忆的ADK。实验表明其在三个基准测试中优于现有方案，推动智能体开发向AI中心化范式转变。


<details>
  <summary>Details</summary>
Motivation: 当前ADKs在智能体拓扑、工具和记忆功能支持上存在缺陷，主要依赖人类手动设计，严重制约了智能体的泛化能力和性能上限，亟需实现自动化和智能化的范式革新。

Method: 设计OpenSage框架，实现三大创新：1）支持智能体自组织与自管理工具集及子智能体；2）构建分层图结构记忆系统；3）开发软件工程专用工具包。通过LLM自动生成拓扑结构，消除人工设计瓶颈。

Result: 在三个SOTA基准和多种骨干模型上的实验验证了OpenSage的优越性；消融研究证实各组件设计有效，显著提升了智能体的通用性和性能表现。

Conclusion: OpenSage成功实现从人类中心到AI中心的智能体开发范式转变，为下一代自主化、高性能的智能体系统构建奠定了重要基础。

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [46] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: 本文提出AgentLAB，首个评估LLM智能体长周期自适应攻击的基准测试，涵盖5种新型攻击、28个环境与644个测试用例。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在长周期复杂环境中部署增多，面临多轮交互攻击风险，此类攻击在单轮场景中无法实现，亟需专门评估基准。

Method: 构建AgentLAB框架，设计意图劫持、工具链操控、任务注入、目标漂移和记忆投毒等5种攻击，在28个真实环境中设置644个测试用例，评估主流LLM智能体。

Result: 发现LLM智能体对长周期攻击高度脆弱，单轮防御机制无法有效缓解此类威胁。

Conclusion: AgentLAB将为LLM智能体安全进展提供重要基准，已开源以促进研究。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [47] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: 本文提出LLM-Wikirace基准，用于评估大语言模型的规划、推理与世界知识能力。该基准要求模型通过维基百科超链接从源页面逐步导航至目标页面，需前瞻性规划与概念关联推理。评估显示，领先模型（如Gemini-3、GPT-5、Claude Opus 4.5）在简单任务上表现超人类，但困难任务成功率骤降至23%，揭示当前模型在长程规划与失败重规划方面存在重大缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大语言模型前瞻性规划与长程推理能力的系统性评估。尽管模型在知识密集型任务中表现卓越，但其在需要多步规划、动态调整策略的复杂场景下的能力边界尚不明确。LLM-Wikirace旨在填补这一空白，通过可量化的方式分离并检验世界知识与规划能力的相对重要性。

Method: 设计LLM-Wikirace基准，模拟经典Wikirace游戏机制。给定源与目标Wikipedia页面，模型必须生成一系列超链接跳转路径。任务设置不同难度等级以测试模型表现。评估涵盖多种开源与闭源领先模型，并通过轨迹分析识别模型失败模式，特别是循环行为和重规划困难。

Result: 实验表明，领先模型在简单难度上超越人类水平，但在困难难度上性能显著下降，最佳模型Gemini-3成功率仅为23%。关键发现包括：（1）世界知识是必要基础，但超过某阈值后，规划与长程推理能力成为性能决定因素；（2）轨迹分析揭示，顶级模型在失败後难以有效重新规划，常陷入循环而非调整策略。

Conclusion: LLM-Wikirace作为一个简洁而强大的基准，清晰暴露了当前前沿模型在复杂规划推理方面的核心短板。该工作指出，尽管模型已掌握海量世界知识，但在需要前瞻性思考与动态策略调整的任务中仍有巨大提升空间。此基准为未来规划型大模型的研究提供了明确的评估平台与发展方向。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [48] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 本文提出一种将形式化验证与深度学习图像检索相结合的新框架，通过图验证与神经代码生成的协同，实现对复杂自然语言查询（涉及关系、组合和精确约束）的可验证、可信赖检索，并明确标注约束满足情况。


<details>
  <summary>Details</summary>
Motivation: 尽管基于嵌入和大规模预训练的自然语言搜索取得显著进展，但现有框架在处理涉及复杂关系、对象组合及身份、数量、比例等精确约束的查询时仍不可靠。向量表示固有的模糊性和近似性导致结果不确定性，缺乏可验证性和透明度。

Method: 提出一种集成形式化验证的深度学习图像检索框架，采用图基验证方法与神经代码生成的协同组合。通过形式化推理系统 grounding 检索结果，显式验证用户查询中的每个原子事实，而非依赖向量近似。

Result: 该框架支持开放词汇的自然语言查询，生成可信赖且可验证的结果；不仅能返回匹配结果，还能识别并标记哪些具体约束被满足、哪些未满足；提升了嵌入方法的效果，提供更透明和可问责的检索过程。

Conclusion: 该框架超越了向量表示的模糊性和近似性，通过显式验证原子事实，为复杂查询提供了可验证、透明的检索方案，增强了检索结果的可信度和可解释性。

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [49] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 终身多模态智能体持续适应与安全对齐存在根本冲突。微调视觉语言模型于有害数据集会引发严重跨任务、跨模态失准，且失准程度随LoRA秩单调增长。多模态评估揭示比单模态更严重的失准（70.71% vs 41.19%）。有害行为集中在低维子空间（10个主成分即可捕获），仅10%有害训练数据即可导致显著失准。两种缓解策略（良性微调和激活引导）均无法完全根除有害行为，凸显鲁棒持续学习框架的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 终身多模态智能体需通过训练后持续学习适应新任务，但这一过程与安全对齐保持存在根本张力。现有研究尚未充分揭示：微调多模态对齐模型是否会引发泛化性失准、失准程度如何随训练规模变化、单模态安全评估是否低估多模态失准风险，以及失准的内在几何结构特征。

Method: 在Gemma3-4B视觉语言模型上进行系统性实验。采用LoRA低秩适配进行窄域有害数据微调，通过控制秩大小(r)探究失准缩放规律。运用主成分分析(PCA)解析失准的几何结构特征。对比多模态与纯文本评估方式的差异。测试两种缓解策略：良性窄域微调和激活空间引导方法的有效性。

Result: 1) 失准随LoRA秩单调递增，在r=128时多模态评估失准率达70.71±1.22，显著高于文本评估的41.19±2.51；2) 仅10%有害数据混合即导致严重对齐退化；3) 有害行为占据极低维子空间，10个主成分可捕获大部分失准信息；4) 良性微调和激活引导均能显著缓解失准，但均无法完全消除已习得的有害行为。

Conclusion: 当前训练后范式无法充分保障部署环境下的多模态对齐安全。研究揭示了多模态失准的严重性、泛化性和低维结构性特征，表明现有缓解策略的局限性，迫切需要构建专为多模态智能体设计的鲁棒持续学习框架，以在能力获取与对齐保持之间实现有效平衡。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [50] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 本文针对多智能体信息检索系统中链式思考评估仅关注任务准确率而忽略推理过程质量的局限性，提出可复用性与可验证性两个新评估维度，通过思考者-执行者解耦框架进行实证分析，发现二者与准确率无相关性，且专用推理模型生成的链式思考质量不一定优于通用大模型，揭示了当前评估体系的盲点。


<details>
  <summary>Details</summary>
Motivation: 当前链式思考评估过度依赖最终任务准确率，无法衡量推理过程本身的有效性与实用性，导致对大语言模型推理能力的评估存在盲区，难以真正判断推理质量并指导模型改进。

Method: 提出思考者-执行者解耦框架，将链式思考生成与执行分离。定义可复用性（执行者重用思考者推理的便利程度）和可验证性（执行者通过推理复现思考者答案的频率）。在五个基准测试上，让四个思考者模型与十个执行者模型进行交互评估。

Result: 实验表明可复用性和可验证性与标准准确率无显著相关性，暴露了当前准确率导向排行榜的缺陷。意外发现：专用推理模型生成的链式思考在可复用性与可验证性上并未持续优于Llama、Gemma等通用大模型。

Conclusion: 评估链式思考必须超越单一准确率指标，采用多维度质量评估体系。研究结果挑战了专用推理模型必然更优的假设，强调需要更全面的评估方法以准确衡量并提升大语言模型的推理能力。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [51] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是一个CLEF评测任务，专注于从嘈杂的多语言历史文本中提取人物-地点关系。该评测包含两种关系类型（$at$和$isAt$），要求系统结合时空线索进行推理，并采用三维评估体系（准确性、计算效率和领域泛化）来推动历史文献处理的下游应用。


<details>
  <summary>Details</summary>
Motivation: 历史文献中人物与地点的关系提取对数字人文研究具有重要价值，但面临多语言、跨时代、文本噪声等挑战。现有的关系提取技术在历史领域缺乏针对性评估，且未充分考虑时间维度上的关系动态性（如"曾经去过"与"当前所在地"的区别）。因此，需要专门的历史关系提取评测来推动该领域技术发展。

Method: 1) 设计多语言、跨时代的历史文本数据集；2) 定义两种细粒度人物-地点关系（$at$表示历史任意时间关联，$isAt$表示出版时间点附近的位置关系）；3) 建立三维评估框架，同时衡量模型准确性、计算效率和领域泛化能力；4) 要求参赛系统具备时空推理能力以处理历史语境。

Result: 该评测任务通过链接大规模历史数据处理，预期将促进知识图谱构建、历史人物传记重建和空间分析等下游应用的发展。同时，三维评估体系将推动开发更准确、高效且具备泛化能力的历史关系提取系统。

Conclusion: HIPE-2026通过引入细粒度的时空关系分类和综合评估框架，为历史文本中的人物-地点关系提取建立了新的评测基准，有望推动数字人文领域的信息抽取技术进步，并为历史研究提供技术支持。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


### [52] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 本文揭示大语言模型作为智能体时存在的安全评估盲区：文本层面的安全对齐无法保证工具调用层面的行为安全。通过GAP基准测试六个前沿模型，发现普遍存在"文本拒绝但工具执行"的分歧现象，即使加强系统提示仍有219例危险动作被执行，证明文本安全不能迁移至工具调用安全。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型作为智能体通过工具调用与外部系统交互并产生现实世界影响，现有安全评估几乎全部聚焦于文本输出的拒绝行为，忽略了工具调用可能执行的危险动作。这一评估缺口导致无法确定文本层面的安全对齐是否能有效抑制有害工具调用行为，构成潜在安全隐患。

Method: 提出GAP基准系统，构建涵盖医药、金融、教育、就业、法律、基础设施六个受监管领域的评估框架。每个领域设计7种越狱场景，测试三种系统提示条件（中性、安全强化、工具鼓励）和两种提示变体，对六个前沿模型进行系统性评估，最终生成17,420个分析数据点。通过量化文本拒绝与工具调用执行之间的分歧，定义GAP指标。

Result: 核心发现显示文本安全与工具调用安全存在显著脱节：所有六个模型均出现文本拒绝有害请求但工具调用同步执行危险动作的GAP现象。即使在安全强化系统提示下，仍存在219个此类案例。系统提示措辞影响巨大，最稳健模型的工具调用安全率跨度为21个百分点，最敏感模型达57个百分点，且18组对比中16组经Bonferroni校正后仍显著。运行时治理合约虽能减少信息泄露，但对危险工具调用行为本身无威慑效果。

Conclusion: 研究结果表明，仅依赖文本层面的安全评估不足以全面评估智能体行为，工具调用安全需要独立的测量标准和专门的缓解措施。这对大语言模型智能体的安全部署具有重要警示意义，强调必须建立针对工具调用的安全评估与防护体系。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [53] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: LLM4Cov是一个离线智能体学习框架，用于解决高覆盖率硬件验证中执行反馈昂贵缓慢的问题。通过执行验证的数据筛选、策略感知的智能体数据合成和最差状态优先采样，40亿参数模型实现69.2%的覆盖率通过率，超越教师模型5.3%，性能可与十倍参数量模型媲美。


<details>
  <summary>Details</summary>
Motivation: 执行感知的大语言模型智能体虽具前景，但其反馈获取成本高昂、速度缓慢，使得在线强化学习不切实际。高覆盖率硬件验证面临此挑战，因其依赖工业级仿真器和不可微的执行信号，需要高效的离线学习方法来突破执行约束。

Method: 提出LLM4Cov框架，将硬件验证建模为由确定性评估器引导的无记忆状态转移过程。引入三大核心技术：执行验证的数据筛选确保数据质量；策略感知的智能体数据合成生成高质量交互轨迹；最差状态优先采样聚焦关键困难样本，从而实现执行约束下的可扩展离线学习。

Result: 通过改进的评估协议从现有验证套件构建了现实对齐的基准测试。实验表明，40亿参数的紧凑模型在智能体评估下达到69.2%的覆盖率通过率，超越教师模型5.3个百分点，且性能与十倍参数量的大型模型具有竞争力。

Conclusion: 该框架有效解决了执行反馈稀缺问题，通过高质量数据合成与智能采样策略，使小模型在硬件验证任务上达到顶尖性能，为资源受限场景提供了可行方案，证明了离线学习方法在工具使用领域的潜力。

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [54] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: 本文提出Phantom，一种基于结构化模板注入的自动化LLM智能体劫持框架。通过利用智能体对聊天模板令牌的依赖，注入优化的结构化模板以引发角色混淆，使智能体将恶意内容误认为合法指令。该框架结合多级模板增强、模板自动编码器（TAE）和贝叶斯优化，在Qwen、GPT、Gemini等模型上显著提升了攻击成功率和查询效率，并在真实商业产品中发现了70多个已确认的漏洞。


<details>
  <summary>Details</summary>
Motivation: OWASP将智能体劫持列为LLM生态系统的关键威胁，现有攻击依赖手工制作的语义驱动提示操控，存在攻击成功率低、难以迁移到闭源商业模型的问题。因此需要自动化、高效的攻击框架来揭示和解决这一安全漏洞。

Method: Phantom框架的核心是基于结构化模板注入。首先通过多级模板增强增加结构多样性，然后训练模板自动编码器（TAE）将离散模板嵌入到连续可搜索的潜在空间，最后应用贝叶斯优化高效识别最优对抗向量，并将其解码为高效能的结构化模板，从而诱导智能体角色混淆。

Result: 在Qwen、GPT、Gemini上的大量实验表明，Phantom在攻击成功率（ASR）和查询效率上显著优于现有基线方法。更重要的是，研究者在真实商业产品中识别出70多个漏洞，且已获得厂商确认，证明了结构化模板劫持的实际严重性。

Conclusion: 该研究揭示了结构化模板注入对LLM智能体构成的严重实际威胁，为下一代智能体系统的安全防护提供了经验基础，强调了加强智能体架构安全机制的重要性。

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [55] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 本文通过潜变量上下文策略形式化挑战黑盒安全评估假设，证明在AI模型存在评估时罕见但部署时普遍的隐藏内部变量下，任何黑盒评估器都无法可靠估计部署风险，建立了统计与计算上的根本性限制下界。


<details>
  <summary>Details</summary>
Motivation: 黑盒安全评估默认模型在测试分布上的行为可预测部署性能，但本文揭示当模型输出依赖未观察到的内部潜变量（评估时罕见、部署时普遍）时，这一核心假设失效，导致部署风险被系统性低估，现有评估方法存在根本缺陷。

Method: 采用Le Cam方法证明被动评估的极小极大下界，利用哈希触发器构造与Yao极小极大原理分析自适应评估的最坏情况误差，并基于陷门单向函数假设建立计算分离性，同时给出白盒探测的样本复杂度与偏差修正框架。

Result: （1）被动评估：任何估计器的期望绝对误差≥(5/24)δL≈0.208δL；（2）自适应评估：即使完全自适应查询，最坏情况误差仍≥δL/16，检测需Θ(1/ε)次查询；（3）计算分离：多项式时间评估器无法区分陷门激活的不安全行为；（4）白盒探测：估计精度ε_R需O(1/(γ²ε_R²))样本，其中γ=α₀+α₁-1为探测质量度量。

Conclusion: 黑盒测试对潜变量上下文策略在统计上欠定性，结果量化了何时必须采用架构约束、训练时保证、可解释性及部署监控等额外防护措施，为最坏情况安全保证提供了数学必要性判据。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [56] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe是一个对话式、纵向的股票推荐基准，通过多视角参考区分描述性行为与基于投资者风险偏好的规范效用，超越单纯的行为匹配来评估LLMs的金融决策质量。研究发现理性决策质量与行为对齐之间存在持续性张力。


<details>
  <summary>Details</summary>
Motivation: 传统推荐基准仅评估模型对用户行为的模仿能力，但在金融顾问场景中，市场波动下观察到的用户行为可能充满噪声且短视，与长期目标冲突。将用户选择视为唯一真实标签会混淆行为模仿与决策质量。

Method: 基于真实市场数据和人类决策轨迹构建Conv-FinRe基准，包含入职访谈、分步市场情境和顾问对话，要求模型在固定投资周期内生成股票排序。提供多视角参考，区分描述性用户行为与基于投资者特定风险偏好的规范效用，从而诊断模型是理性分析、模仿噪声还是追随市场动量。

Result: 评估了多个先进LLMs，发现模型在效用导向的排序上表现良好时往往无法匹配用户选择，而行为对齐的模型则容易过拟合短期噪声，揭示了决策理性与行为模仿之间的根本性权衡。

Conclusion: Conv-FinRe基准能够有效区分模型的决策逻辑类型，为金融顾问LLM的评估提供了新范式。数据集已在Hugging Face开源，代码库在GitHub发布。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [57] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: 针对非专业用户从海量时序数据中检索形态意图的难题，本文提出Sonar-TS神经符号框架，通过搜索-验证管道结合SQL与Python程序，并推出首个大规模基准测试NLQTSBench，为时间序列数据库的自然语言查询提供了系统性解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法无法有效处理连续形态意图（如数据形状或异常），且传统时间序列模型难以应对超长的历史数据，导致非专业用户难以从海量时序记录中检索有意义的事件、区间和摘要。

Method: 提出Sonar-TS框架，采用"先搜索后验证"的主动声纳式管道：首先利用特征索引通过SQL快速筛选候选时间窗口，然后自动生成Python程序对原始信号进行精确验证和锁定。

Result: 构建并引入了NLQTSBench大规模基准测试，实验验证了Sonar-TS在处理复杂时序查询方面的有效性，特别是在传统方法失效的场景下仍能成功导航。

Conclusion: 本研究首次系统性地探索了时间序列数据库的自然语言查询问题，提出的Sonar-TS框架和NLQTSBench基准为未来研究建立了通用框架和评估标准。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [58] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder是一个两阶段匹配系统，旨在解决异质技能预设队伍的公平匹配问题。系统先通过Ruzicka相似度指数快速筛选候选队伍，再基于倒置正态分布的非线性技能桶和Kantorovich距离计算"制裁分数"来精确评估公平性，并通过1.4亿次模拟验证了系统可行性。


<details>
  <summary>Details</summary>
Motivation: 现代多人在线游戏中，公平且快速的匹配系统直接影响玩家留存率和满意度。然而，为技能水平异质的预设队伍（lobby）创建公平比赛是重大挑战。传统基于平均队伍技能指标（如均值、中位数段位）的匹配方法在面对广泛或偏斜的技能分布时，常导致比赛结果一边倒，影响游戏体验。

Method: 提出Cinder两阶段匹配算法：第一阶段采用Ruzicka相似度指数快速比较各队伍的"非离群"技能范围，进行初步候选筛选；第二阶段将玩家段位映射到由倒置正态分布生成的非线性技能桶中，以获得在平均水平段位上更高的粒度分辨率，然后计算两队排序后技能桶索引之间的Kantorovich距离，得出量化公平性的"制裁分数"。

Result: 通过对1.4亿个模拟队伍配对生成的制裁分数分布进行统计分析，验证了Cinder系统的可行性，为实际应用中设定公平匹配阈值提供了稳健的数据基础和可靠的决策依据。

Conclusion: Cinder通过快速初筛与精确评估相结合的两阶段设计，有效解决了异质技能队伍的公平匹配难题。基于大规模模拟验证的制裁分数分布模型，为游戏开发者实施公平且高效的匹配系统提供了实用且可操作的框架。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [59] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F是首个项目级数学自动形式化智能体框架，通过声明编译和证明修复两阶段，三周内将479页教材形式化为15.4万行可编译Lean代码，证明成功率96%，效率超专家人工。


<details>
  <summary>Details</summary>
Motivation: 数学自动形式化目前局限于孤立定理，难以扩展到教科书和研究论文规模。关键挑战在于跨文件依赖管理、导入解析和项目级编译保证。现有方法效率低下，大型形式化仍需数月或数年的专家人工，亟需自动化解决方案。

Method: M2F采用两阶段智能体框架：1)声明编译阶段：将文档拆分为原子块，推断依赖关系并排序，修复声明骨架直至项目编译通过，期间允许证明占位符；2)证明修复阶段：在固定签名下使用目标条件化局部编辑填补证明空缺。全程保持验证器参与，仅当工具链反馈确认改进时提交编辑。

Result: 在约三周内，M2F成功将479页实分析与凸分析教材自动形式化为含153,853行代码的项目级Lean库，完全形式化所有声明和证明。在FATE-H基准上，证明成功率达96%（强基线为80%），效率相当于数月或数年专家工作量，证明了大规模自动形式化的可行性。

Conclusion: M2F框架通过智能化两阶段方法实现了教科书级数学文献的端到端自动形式化，显著提升了规模化形式化的效率和质量。研究结果表明，实用化的大规模数学文献自动形式化已成为现实目标，为数学验证和研究提供了重要工具支持。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [60] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: 针对LLM智能体在多步运行时重复加载长系统指令和大量工具导致成本高、错误率高等问题，本文提出Instruction-Tool Retrieval (ITR)方法，通过每步动态检索最相关的指令片段和工具子集，实现上下文token减少95%、工具路由准确率提升32%、端到端成本降低70%，并使智能体能在上下文限制内运行更多循环。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在执行多步任务时，每轮都会重复加载冗长的系统指令和庞大的工具目录，导致计算成本增加、响应延迟、智能体偏离目标概率上升以及工具选择错误等问题，亟需优化上下文管理以提升效率与可靠性。

Method: 提出Instruction-Tool Retrieval (ITR)，一种RAG变体，在每一步动态检索最相关的系统提示片段和最必要的工具子集，构建运行时系统提示并暴露经过筛选的工具集，同时配备置信度门控的回退机制。

Result: 在受控基准测试中，相比单体基线，ITR每步上下文token减少95%，工具路由准确率相对提升32%，端到端任务成本降低70%，使智能体能在上下文限制内运行2-20倍的循环次数，且随着步骤增加效益更加显著。

Conclusion: ITR方法有效解决了长运行智能体的成本和效率问题，特别适合长时间自主运行的智能体，论文还提供了详细的评估协议、消融研究和实用部署指南，为实际部署提供操作指导。

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [61] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA是一个多智能体计算机使用框架，通过意图对齐的计划记忆稳定长时程执行，成功率达74.83%，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 计算机使用智能体在噪声感知、多窗口上下文和动态环境状态下运行长时程任务时，现有方法（基于强化学习的规划器和轨迹检索）存在意图漂移、重复解决常规子问题的问题，导致错误累积和效率低下。

Method: 提出IntentCUA框架，由规划器、计划优化器和评论家组成。三者通过共享内存协调，将原始交互轨迹抽象为多视图意图表示和可复用技能。运行时，意图原型检索子群对齐的技能并注入部分计划，减少冗余重规划和错误传播。

Result: 端到端评估显示，任务成功率达74.83%，步骤效率比为0.91，优于RL基线和轨迹中心基线。消融研究表明，多视图意图抽象和共享计划内存共同提升执行稳定性，多智能体协作循环对长时程任务贡献最大。

Conclusion: 系统级意图抽象和基于记忆的协调是大规模动态环境中实现可靠高效桌面自动化的关键。

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [62] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: 这篇论文提出S2Q算法，解决MARL价值分解中因依赖单一最优动作导致适应能力差的问题。通过学习多个子价值函数并集成到Softmax策略，增强探索和快速适应能力，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有价值分解方法依赖单一最优动作，当训练中价值函数变化时难以适应，易收敛至次优策略，限制了在动态环境中的性能表现。

Method: 提出Successive Sub-value Q-learning (S2Q)，学习多个子价值函数以保留高价值备选动作，并将其集成到Softmax行为策略中，促进持续探索，使Q^tot能快速适应变化的最优解。

Result: 在挑战性MARL基准测试上，S2Q持续优于多种MARL算法，展现出更强的适应性和整体性能提升。

Conclusion: S2Q通过多子价值函数机制有效解决了MARL的适应性问题，为动态环境提供了新的有效方法，实验验证了其优越性，代码已开源。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [63] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: 本文提出预测性批调度(PBS)，一种新型语言模型训练优化技术。该方法通过在线训练的轻量级线性预测器，仅利用词元频率、序列长度、词汇多样性和稀有词比例四个静态特征估计样本难度，动态优先处理高损失样本，在1.3亿参数模型上实现评估损失收敛速度提升6-13%，预测器与实际损失相关性达0.44。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法依赖预定义难度指标，困难样本挖掘需昂贵的逐样本损失追踪。为克服这些限制，本文旨在通过动态优先处理高损失样本加速语言模型收敛，同时保持低计算开销。

Method: PBS采用在线训练的轻量级线性预测器，利用四个静态词元级特征（词元频率、序列长度、词汇多样性、稀有词比例）估计样本难度。预测器在训练过程中动态更新，用于批构建时优先选择高损失样本，实现无需预定义指标或高成本损失追踪的课程学习。

Result: 在1.3亿参数Transformer上的实验表明，预测器仅用四个特征就达到与实际损失0.44的相关性，且相关性在10,000训练步内从0.14提升至0.44。PBS使评估损失收敛速度提升6-13%，验证了词元频率统计包含样本难度的有效信息。

Conclusion: 词元频率统计编码了有意义的样本难度信息，使得PBS能够以可忽略的计算开销实现有效的课程学习，为语言模型训练提供了实用且高效的优化方案。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [64] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 针对ESG评级机构间评分差异大、可比性差的问题，本文提出STRIDE与SR-Delta相结合的人机协作框架，旨在构建可信的可持续评级基准数据集，实现标准化评估。


<details>
  <summary>Details</summary>
Motivation: 当前不同可持续性评级机构对同一企业的评估结果差异显著，严重影响了评级的可比性、可信度及其在决策中的实用性，阻碍了可持续发展议程的有效推进。

Method: 构建一个通用的人机协作框架，其中STRIDE模块提供基于大语言模型的、有原则的标准与评分体系以指导企业级基准数据集构建；SR-Delta模块作为差异分析流程框架，识别潜在调整需求。

Result: 该框架能够实现对可持续评级方法的可扩展且可比较的评估，为构建标准化、可信的基准数据集提供了系统性解决方案。

Conclusion: 研究呼吁更广泛的人工智能社区采纳AI驱动方法，强化可持续评级方法论，从而支持和推动紧迫的可持续发展议程。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [65] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 针对视觉任务中像素特征依赖性问题，本文指出常用分割方法违反一致性性质，提出满足T性质的新分割方式，构建O-Shap层次归因方法。实验表明该方法在归因精度、语义一致性和运行效率上优于基线SHAP变体，尤其在结构重要的场景中。


<details>
  <summary>Details</summary>
Motivation: Shapley值方法在可解释AI中理论基础坚实，但假设特征独立，而视觉任务中像素存在强空间语义依赖。现有Owen值方法依赖特征分组定义，常用分割（轴对齐、SLIC）违反关键一致性性质，导致归因不准确。

Method: 提出一种满足T性质的新图像分割策略，确保层次间的语义对齐，基于此构建O-Shap层次归因框架。该方法通过合理特征分组实现计算剪枝，同时提升归因精度和可解释性。

Result: 在图像和表格数据集上的实验表明，O-Shap在归因精度、语义一致性和运行效率方面均优于基准SHAP变体，特别是在结构信息重要的任务中优势更明显。

Conclusion: 通过满足T性质的分割构建层次结构，O-Shap有效解决了特征依赖问题，为视觉可解释AI提供了更理论一致且高效的方法，验证了合理特征分组的重要性。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [66] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: 本文提出InstructKG框架，自动从课程讲义构建与教师意图一致的知识图谱，通过融合教学材料的时空语义信号与大语言模型，提取概念及其依赖关系，助力大规模课程的个性化学习干预。


<details>
  <summary>Details</summary>
Motivation: 掌握教育概念需理解其前置依赖（如递归先于归并排序）和子概念关系（如归并排序属于排序算法），这对识别学生知识缺口至关重要。但大规模课程中教师难以诊断个体误解，现有知识图谱或停留于课程级概念表层，或忽视教学材料中的丰富教学信号，无法有效捕捉预期学习路径。

Method: InstructKG框架以课程讲义（幻灯片、笔记等）为输入，提取核心概念作为节点，推断"属于"、"依赖于"等有向边来表征学习依赖。该方法协同融合教育材料中独特的时空信号（概念教学顺序）与语义信号（概念定义中的相互提及），并结合大规模语言模型的泛化能力。

Result: 在多个学科真实课程讲义上的实验与人工评估表明，InstructKG能准确捕捉丰富的、与教师教学意图一致的学习路径。

Conclusion: InstructKG成功实现从教学材料自动构建符合教师意图的知识图谱，为大规模课程个性化干预提供有效工具，验证了融合教育特定信号与大规模语言模型的可行性与有效性。

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [67] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 本文提出一种用于分解困难电路SAT实例的参数化并行算法，通过专用约束将原始实例划分为弱化公式族，并利用并行硬度估计指导参数调整以识别高质量分解，在逻辑等价验证和哈希原像攻击等挑战性实例上验证了实用性。


<details>
  <summary>Details</summary>
Motivation: 困难电路SAT实例（如逻辑等价验证和哈希原像攻击）求解效率低下，传统方法难以有效处理，亟需高效的并行分解技术来提升求解性能。

Method: 提出一种参数化并行分解算法，采用专用约束条件将原始电路SAT实例分割为多个弱化子公式；通过并行计算实例硬度估计指导参数调整，实现高质量分解的高效识别。

Result: 在逻辑等价验证和哈希原像攻击等挑战性电路SAT实例上验证了算法的实际有效性，证明了该方法处理困难实例的实用性。

Conclusion: 该算法通过约束分割和并行硬度估计指导的参数化设计，为困难CircuitSAT问题的求解提供了有效并行化方案，在工业应用中具有良好前景。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [68] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: 本文提出JEPA-DNA，一种新型基因组基础模型预训练框架，通过将联合嵌入预测架构（JEPA）与传统生成目标（MLM/NTP）结合，利用CLS标记在潜在空间实现高级功能嵌入预测，解决现有方法缺乏全局生物学视角的问题，在基因组基准测试中表现优于纯生成式基线。


<details>
  <summary>Details</summary>
Motivation: 当前基因组基础模型主要依赖掩码语言建模（MLM）或下一个词元预测（NTP），这些方法虽能捕获局部基因组语法和细粒度基序模式，但无法捕捉更广泛的功能上下文，导致学习到的表示缺乏全局生物学视角，难以理解序列底层的整体功能逻辑。

Method: 提出JEPA-DNA框架，将联合嵌入预测架构（JEPA）与生成目标集成。核心创新是通过CLS标记监督，在潜在空间中耦合词元级恢复与预测目标，迫使模型预测被掩码基因组片段的高级功能嵌入而非单个核苷酸，实现潜在接地。该框架既可从头训练，也可作为现有模型的持续预训练增强。

Result: 在多样化的基因组基准测试中，JEPA-DNA在监督和零样本任务上持续优于仅使用生成目标的基线模型，证明其能生成更稳健且生物学上更合理的基因组表示。

Conclusion: JEPA-DNA通过整合预测性目标与生成性目标，为构建真正理解基因组功能逻辑的基础模型提供了可扩展路径，有望推动基因组学从"字母识别"向"功能理解"的范式转变。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [69] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: 本文提出Texo，一个仅含2000万参数的最小化高性能公式识别模型，通过精心设计、知识蒸馏和词表迁移，在保持性能的同时大幅减小模型体积，支持在消费级硬件和浏览器中实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有公式识别模型参数规模大，难以在消费级硬件和浏览器环境中实时运行。本文旨在开发一个轻量级但高性能的公式识别模型，以降低部署门槛并扩大应用范围。

Method: 通过精心设计模型结构、采用知识蒸馏技术以及迁移词表和分词器，Texo仅用2000万参数即可达到与SOTA模型相当的性能。

Result: Texo相比UniMERNet-T和PPFormulaNet-S模型分别减小80%和65%的体积，同时保持可比性能，实现消费级硬件和浏览器环境下的实时推理。

Conclusion: 开发了Web应用展示模型能力并方便终端用户使用，验证了轻量级公式识别模型在实际应用中的可行性。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [70] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本研究作为"方法论实验"，提出基于AI Agent的人文社会科学协作研究流程(Agentic Workflow)，以Anthropic经济指数(AEI)台湾地区Claude.ai使用数据(N=7,729对话, 2025年11月)为实证对象。研究构建包含任务模块化、人机分工、可验证性三原则的七阶段工作流，识别直接执行、迭代精炼、人类主导三种协作模式，强调人类在研究问题形成、理论解释、情境推理及伦理反思中的不可替代性，为HSS研究提供可复制框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正重塑知识工作，但现有研究集中于软件工程与自然科学领域，对人文社会科学的方法论探索不足。本研究旨在填补这一空白，构建适配HSS研究特点的人机协作方法论框架。

Method: 采用方法论实验设计，构建七阶段模块化Agentic Workflow，遵循任务模块化、人机分工、可验证性三原则。实证部分运用AEI台湾地区Claude.ai使用数据(N=7,729个对话, 2025年11月)，通过反思性文档记录流程运作，既验证框架可行性，又展示其在二手数据分析中的应用过程与产出质量。

Result: 1) 提出可复制的人机协作框架；2) 识别三种协作模式：直接执行、迭代精炼、人类主导；3) 通过反思性分析揭示人类判断在研究问题形成、理论解释、情境化推理及伦理反思中的不可替代作用；4) 附录A展示了流程应用的具体产出质量。

Conclusion: 本研究为AI时代的人文社会科学研究提供了新的方法论路径，证实Agentic Workflow的可行性，同时强调人类研究者的核心地位。局限包括单平台数据、横截面设计、AI可靠性风险等。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [71] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 该论文针对大语言模型在回溯测试中可能泄露训练数据中 cutoff 日期之后知识的问题，提出了一套检测时序知识泄露的框架。通过将模型推理分解为原子化声明并运用 Shapley 值量化各声明的贡献，定义了 Shapley-DCLR 泄露率指标。进一步提出 TimeSPEC 方法，在生成过程中嵌入声明验证与重生成机制，有效过滤时序污染。在最高法院判决、NBA 薪资和股票收益预测等三项任务上的实验表明，标准 prompting 存在严重泄露，而 TimeSPEC 能显著降低泄露率且保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型预测未来事件能力需要进行回溯测试，但模型可能无意中使用训练时编码的 cutoff 日期之后的知识，导致评估结果失真。现有方法缺乏对时序知识泄露的细粒度检测与量化手段，也缺少有效的缓解策略，这使得模型预测能力的真实评估存在根本性挑战。

Method: 提出声明级时序知识泄露检测框架：首先将模型推理分解为原子化声明，并按时间可验证性分类；随后应用 Shapley 值计算每个声明对预测结果的贡献度，得到 Shapley-DCLR 指标以量化决策关键泄露率。基于此框架，提出 TimeSPEC 方法，通过在生成过程中交错进行声明提取、验证和重生成，确保所有支撑声明均可追溯至 cutoff 日期前的信息源，主动过滤时序污染。

Result: 在涵盖美国最高法院案件预测、NBA 薪资估计和股票收益排序的 350 个实例上，标准 prompting 基线表现出严重的时序知识泄露。TimeSPEC 方法显著降低了 Shapley-DCLR 泄露率，同时保持了与基线相当的任务性能，证明显式的声明级验证优于基于 prompt 的时序约束策略。

Conclusion: 时序知识泄露是 LLM 回溯测试中的重大挑战。所提出的 Shapley-DCLR 指标能有效解释和量化泄露程度，而 TimeSPEC 框架通过结构化声明验证机制，为构建可靠、可解释的未来事件预测评估体系提供了可行路径，对提升模型评估的严谨性具有重要意义。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [72] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本研究详细记录了从原始arXiv LaTeX源文件训练13.6亿参数科学语言模型的全过程，通过24次实验揭示了预处理、分词和存储I/O对模型训练的关键影响，为中等计算预算的研究者提供了实践指导。


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型虽具备强推理与数学能力，但领域专用科学语言模型从原始数据训练的具体实践过程仍缺乏充分文档记录。本研究旨在填补这一空白，为资源有限的研究者提供可复现的工程实践参考。

Method: 构建端到端训练流水线：从arXiv LaTeX源文件出发，依次进行元数据过滤、档案验证、LaTeX解析、文本规范化、领域感知分词，并在受限计算资源（2块A100 GPU）下训练密集Transformer模型。通过24次实验运行，系统分析训练稳定性、扩展行为、数据损耗及基础设施瓶颈。

Result: 关键发现包括：1）预处理决策显著影响可用token数量；2）分词策略影响符号稳定性；3）存储与I/O限制可媲美计算资源成为主要瓶颈；4）在数据丰富 regime（520亿预训练token）中观察到稳定的训练动态。实验量化了各环节数据损耗与训练行为。

Conclusion: 本研究非提出新架构，而是提供了一份基于工程实践、透明的从头训练小型科学语言模型实录。这些洞见可帮助中等计算预算的研究者构建领域专用模型，强调了系统优化在资源受限场景下的重要性。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [73] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 针对任务算术中多任务向量组合导致的跨任务干扰与表示漂移问题，本文提出一种无需外部数据的正则化方法。通过将表示漂移正则化转化为曲率矩阵近似问题，并采用Kronecker-Factored Approximate Curvature技术，实现了任务添加与否定任务的最优性能，具备任务数常数级复杂度、对向量重缩放鲁棒、无需保留集调优等优势。


<details>
  <summary>Details</summary>
Motivation: 任务算术为适配基础模型提供了模块化、可扩展的解决方案，但组合多个任务向量时会产生跨任务干扰，导致表示漂移和性能退化。现有表示漂移正则化方法依赖外部任务数据，与模块化设计及数据隐私约束存在冲突。

Method: 提出一种无数据方法，将表示漂移正则化重新框架为曲率矩阵近似问题，并采用Kronecker-Factored Approximate Curvature（K-FAC）技术构建实用正则化器。

Result: 在任务添加和否定任务上达到最先进性能，计算复杂度与任务数量呈常数关系，对任务向量重缩放具有鲁棒性，且无需使用保留数据集进行超参数调优。

Conclusion: 该方法有效解决了任务算术中的跨任务干扰问题，在不依赖外部数据的前提下保持了系统模块化特性，同时显著提升了性能、效率和鲁棒性，为实际应用部署提供了更优方案。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [74] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 针对非小细胞肺癌（NSCLC）生存预测中多模态数据缺失的挑战，本文提出了一种多模态对比变分自编码器（MCVAE）。该方法通过模态特定的变分编码器捕捉各数据源的不确定性，采用带学习门控机制的融合瓶颈层来平衡不同模态的贡献，并结合生存损失、重建损失和跨模态对比损失的多任务目标进行训练。在TCGA-LUAD和TCGA-LUSC数据集上的评估表明，该方法在预测疾病特异性生存期方面优于现有模型，且对严重数据缺失具有鲁棒性，同时发现多模态融合并非总是有益。


<details>
  <summary>Details</summary>
Motivation: 非小细胞肺癌患者生存结局预测因个体预后特征差异而面临挑战。整合全切片图像、批量转录组和DNA甲基化数据可提供互补的诊断视角，但真实临床数据集常存在大量模态缺失。现有先进模型在严重缺失情况下缺乏鲁棒性，需要一种能够有效处理任意缺失模式的多模态学习方法。

Method: 提出多模态对比变分自编码器（MCVAE），包含模态特定变分编码器以量化各数据源不确定性，引入带学习门控的融合瓶颈层来归一化现有模态的贡献。设计多任务目标函数，结合生存损失、重建损失和跨模态对比损失，通过跨模态对比损失在潜在空间中强制对齐。训练过程中应用随机模态掩码策略以增强对任意缺失模式的鲁棒性。

Result: 在TCGA-LUAD（n=475）和TCGA-LUSC（n=446）数据集上的全面评估显示，该方法在预测疾病特异性生存期（DSS）方面效果显著，且相比两种先进模型对严重缺失场景具有更强鲁棒性。通过测试所有模态子集发现，多模态整合并非总是对任务有益。

Conclusion: 所提出的MCVAE方法有效解决了多模态生存预测中的数据缺失问题，通过不确定性量化、自适应融合和多任务学习实现了对严重缺失的鲁棒性。研究还澄清了多模态整合的边界条件，为临床多模态数据的实际应用提供了重要见解。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [75] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 本文提出了一个基于Privacy-by-Design的框架，整合GDPR、PIPEDA、COPPA等隐私法规以及UNCRC、AADC等儿童保护准则，将其映射到LLM应用全生命周期（数据收集、模型训练、运营监控和持续验证），旨在帮助AI开发者降低儿童隐私风险并满足法律合规要求。


<details>
  <summary>Details</summary>
Motivation: 随着儿童越来越多地使用人工智能技术，隐私风险日益引发关注。尽管现有隐私法规要求企业实施保护措施，但在实践中操作困难。本文旨在通过主动、风险规避的设计方法应对这一挑战。

Method: 研究构建了一个Privacy-by-Design框架：首先整合欧盟GDPR、加拿大PIPEDA和美国COPPA等多国隐私法规原则；其次将这些原则映射到使用大语言模型的应用程序各阶段（数据收集、模型训练、运营监控、持续验证）；然后梳理学术文献中的操作控制措施；最后结合联合国儿童权利公约、英国适龄设计规范等儿童保护准则提供设计指南，并通过一个13岁以下儿童教育辅导LLM应用案例进行实证演示。

Result: 该框架为AI服务提供商和开发者在每个LLM应用阶段提供了具体的操作控制方案，帮助其在满足法律标准的同时降低儿童隐私风险，并展示了如何通过技术和组织控制以及适龄设计决策来实现合规的儿童AI应用开发。

Conclusion: 通过在LLM全生命周期中实施数据保护策略和适龄设计决策，可以支持开发出既能为儿童提供隐私保护又符合法律要求的AI应用程序，为儿童AI产品的安全开发提供了可行的实践路径。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [76] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 本文提出了一种针对ARM Cortex-M系列处理器（M0+, M4, M7）的AI模型基准测试框架，通过自动化测试台系统性地评估嵌入式系统中能效、精度与资源利用率，揭示FLOPs与推理时间的近线性相关性，并利用帕累托分析实现能耗与精度的最优权衡，为开发者提供处理器与模型选型的实用指导。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在资源受限的嵌入式系统中广泛应用，如何在ARM Cortex处理器上实现高能效、高性能的AI部署成为核心挑战。现有研究缺乏系统化的基准测试方法，难以在不同处理器架构与AI模型组合间做出科学决策，亟需建立可量化的评估体系以指导优化方向。

Method: 研究设计了一套自动化测试平台，对ARM Cortex-M0+、M4、M7三款处理器进行多维度基准测试，通过关键性能指标（KPIs）量化评估AI模型的能效、精度及资源消耗；采用帕累托前沿分析方法，系统性地探索能耗与模型精度之间的权衡关系，识别最优配置组合。

Result: 实验发现浮点运算次数（FLOPs）与推理时间呈近线性相关，可作为计算负载的可靠预估指标；M7处理器在短推理周期场景中表现最优，M4处理器在长推理任务中能效比更高，而M0+仅适用于简单模型；通过帕累托分析可明确不同应用场景下的能耗-精度最优平衡点。

Conclusion: 该基准测试框架为嵌入式AI开发者提供了科学决策工具，可指导其在不同ARM Cortex处理器上选择匹配的AI模型，实现性能与能效的最佳平衡，推动高能效AI系统在资源受限环境中的实际应用与可持续发展。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [77] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: KLong是一个开源的大型语言模型智能体，旨在解决极长周期的复杂任务。该方法先通过轨迹分割监督微调（SFT）进行冷启动，再通过渐进式强化学习（RL）进行扩展，在PaperBench基准上显著超越Kimi K2 Thinking模型。


<details>
  <summary>Details</summary>
Motivation: 极长周期任务需要智能体具备持续推理和复杂规划能力，现有模型在长上下文保持和任务连贯性方面面临挑战。为此，本文提出KLong以突破这些限制。

Method: 方法分为两阶段：1）轨迹分割SFT冷启动：首先使用综合SFT方案激活基础模型智能体能力；然后构建Research-Factory自动化流水线，从研究论文收集数据并构建评估标准；基于Claude 4.5 Sonnet生成数千条长周期轨迹；提出轨迹分割SFT技术，保留早期上下文、渐进截断后期上下文并保持子轨迹间重叠。2）渐进式RL扩展：将训练调度为多个阶段，逐步延长时间限制，以提升长周期任务解决能力。

Result: 实验证明KLong具有显著优势与良好泛化性。具体地，KLong（1060亿参数）在PaperBench上超越Kimi K2 Thinking（1万亿参数）11.28%，且在SWE-bench Verified和MLE-bench等编码基准测试中性能同样获得提升。

Conclusion: KLong通过创新的轨迹分割SFT与渐进式RL训练框架，有效增强了模型处理极长周期任务的能力，为开源智能体发展提供了新思路，展现了优越的性能和泛化性。

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [78] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: 针对通用大语言模型在电信领域因术语复杂、标准演变而面临的幻觉和准确性问题，本文提出KG-RAG框架，通过融合知识图谱的结构化领域知识与检索增强生成的动态检索能力，显著提升了电信任务的事实准确性和合规性，实验显示其准确率比基线模型提高14-21%。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电信领域应用时面临领域复杂性强、技术标准持续演进、专业术语繁多等挑战，导致输出幻觉增多、可靠性下降，难以满足电信运维的准确性和合规性要求。传统方法难以有效适配这种垂直领域的特殊需求。

Method: 提出KG-RAG框架，核心为知识图谱与检索增强生成（RAG）的协同集成。利用知识图谱从电信标准和技术文档中构建结构化领域知识库，通过RAG机制实现相关事实的动态检索，将检索到的领域知识作为上下文注入大模型，以事实约束生成过程。

Result: 在基准数据集上的实验表明，KG-RAG显著优于LLM-only和标准RAG基线。与RAG相比平均准确率提升14.3%，与纯大模型相比提升21.6%，同时有效降低了幻觉现象，生成的输出符合电信规范且具备可解释性。

Conclusion: KG-RAG通过知识图谱与RAG的有机结合，为复杂电信场景下的大模型应用提供了有效的领域适应方案，在提升事实准确性、可靠性和合规性方面具有显著优势，验证了结构化知识注入在垂直领域应用中的重要性。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [79] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 本文针对大语言模型激活引导缺乏统一理论框架和单步引导局限性，提出基于常微分方程(ODE)的ODESteer方法，通过障碍函数实现多步自适应引导，在TruthfulQA、UltraFeedback和RealToxicityPrompts等基准上分别获得5.7%、2.5%和2.4%的显著提升，为LLM对齐提供了新的理论视角。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法存在两大局限：一是缺乏统一理论框架指导引导方向设计；二是过度依赖单步引导，无法捕捉复杂的激活分布模式。这促使研究者需要建立更principled的理论基础。

Method: 提出ODESteer方法，将传统激活加法解释为ODE解的一阶近似，将引导方向识别转化为控制理论中的障碍函数设计问题。具体而言，定义障碍函数为正负激活间的对数密度比，并据此构建用于多步自适应引导的常微分方程。

Result: ODESteer在多个LLM对齐基准测试中表现优异：TruthfulQA准确率提升5.7个百分点，UltraFeedback提升2.5个百分点，RealToxicityPrompts毒性降低2.4个百分点，相较于现有最优方法实现一致性的实证改进。

Conclusion: 本研究通过ODE框架统一了激活引导的理论基础，将引导方向设计形式化为障碍函数问题，提出的ODESteer方法不仅验证了该理论视角的有效性，更为大语言模型对齐提供了principled且高效的新范式。

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [80] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 本文提出一种基于联邦学习（FL）的混合集成方法，结合SWIN Transformer和CNN模型（DenseNet201、Inception V3、VGG 19），用于X光报告的COVID-19和肺炎诊断。该系统通过联邦学习实现安全分布式的医疗数据处理，采用实时持续学习提升诊断准确性和严重程度预测，为医生提供可靠辅助。


<details>
  <summary>Details</summary>
Motivation: 计算能力的显著进步为AI在医疗健康领域的应用创造了巨大机遇。医疗数据隐私和安全至关重要，而传统集中式数据处理存在风险。联邦学习技术允许多个医疗机构在不共享原始数据的情况下协作训练模型，构建安全、分布式的医疗数据处理系统，这对于应对COVID-19等全球大流行疾病尤为重要。

Method: 提出一种混合联邦学习集成框架，结合最新的CNN模型（DenseNet201、Inception V3、VGG 19）和SWIN Transformer。使用TensorFlow和Keras实现，利用微软开发的Vision Transformer技术。通过联邦学习架构实现分布式训练，采用实时持续学习方法，在保护数据隐私的同时提升模型的诊断准确性和严重程度预测能力。

Result: 该混合模型能够有效检测COVID-19和肺炎，提供可靠的辅助诊断方案。联邦学习的集成确保了混合模型的安全性和信息真实性，同时通过持续学习提高了疾病诊断准确率和严重程度预测精度。

Conclusion: 基于联邦学习的混合AI模型能够显著改善肺部疾病诊断的准确性和严重程度预测，同时保障数据安全和隐私。该研究为医疗领域提供了一个高效、可靠的辅助诊断工具，展示了AI与联邦学习结合在医疗应用中的巨大潜力。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [81] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 本文提出通过"人类游戏多元宇宙"评估机器通用智能，开发了AI GameStore平台，用LLM合成100款游戏并测试7个视觉语言模型，结果显示最佳模型得分不足人类10%，尤其在记忆规划方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 传统AI基准测试范围狭窄且静态易饱和，无法全面评估机器智能。在技术快速发展的时代，亟需一种能衡量机器相对于人类通用智能水平的新方法。

Method: 提出"人类游戏多元宇宙"概念，定义人类游戏为人类设计并享用的游戏。开发AI GameStore平台，通过LLM与人工协作，从Steam和苹果应用商店自动合成新游戏环境，实现可扩展的开放式游戏生成。

Result: 基于Steam和App Store排行榜生成100款游戏，测试7个前沿视觉语言模型。结果显示最佳模型在多数游戏中得分不足人类平均水平的10%，在挑战世界模型学习、记忆和规划能力的游戏中表现尤其差。

Conclusion: AI GameStore有望成为衡量和推动机器向类人通用智能发展的新实践路径，下一步将扩展平台规模并完善评估体系。

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [82] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: 本文提出MolHIT，一个基于分层离散扩散模型的分子图生成框架，通过引入编码化学先验的额外类别和解耦原子编码，首次在图扩散模型中实现了接近完美的化学有效性，并在MOSES数据集上超越1D基线模型，为AI驱动的药物发现提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在分子生成中存在化学有效性低、难以满足目标性质的问题，性能落后于1D建模方法，限制了AI在药物发现和材料科学中的应用。

Method: MolHIT框架采用分层离散扩散模型，通过两个关键创新：1）将离散扩散推广到编码化学先验的额外类别；2）根据原子化学角色进行解耦原子编码，从而克服了现有方法的性能限制。

Result: 在MOSES数据集上，MolHIT首次实现接近完美的化学有效性，全面超越强1D基线模型；在下游任务如多性质引导生成和骨架扩展中也展现出优异性能，达到新的state-of-the-art水平。

Conclusion: MolHIT显著提升了图扩散模型在分子生成中的性能，缩小了与1D方法的差距，为化学空间探索和AI辅助药物设计提供了更强大的工具，证明了图结构建模在分子生成中的巨大潜力。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [83] [RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution](https://arxiv.org/abs/2602.16932)
*Jinming Nian,Fangchen Li,Dae Hoon Park,Yi Fang*

Main category: cs.IR

TL;DR: 本研究提出RankEvolve，一种基于大语言模型和进化搜索的自动检索算法发现框架。从BM25等传统算法出发，通过程序演化生成新颖有效的排序算法，在多个基准测试中展现出良好泛化性能，证明了大语言模型引导的进化方法是自动发现排序算法的可行路径。


<details>
  <summary>Details</summary>
Motivation: 传统检索算法（如BM25和Dirichlet平滑的查询似然模型）虽然强大高效，但其改进主要依赖参数调优和人工直觉，缺乏自动化创新方法。本研究旨在探索大语言模型是否能通过进化搜索自动发现更优的检索算法，突破人工设计的局限。

Method: 提出RankEvolve框架，基于AlphaEvolve实现程序演化。将排序算法表示为可执行代码，利用大语言模型作为评估器指导进化过程，通过变异、重组和选择操作迭代优化算法，在BEIR和BRIGHT的12个信息检索数据集上进行训练和评估。

Result: 演化出的算法具有新颖性，在检索性能上表现优异，不仅在完整BEIR和BRIGHT基准上展现出良好的迁移能力，还在TREC DL 19和20数据集上取得良好结果。

Conclusion: 评估器引导的大语言模型程序演化是自动发现新型排序算法的实用路径，为信息检索算法的创新提供了新范式。

Abstract: Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms.

</details>


### [84] [Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval](https://arxiv.org/abs/2602.16974)
*Yongjie Zhou,Shuai Wang,Bevan Koopman,Guido Zuccon*

Main category: cs.IR

TL;DR: 本文通过系统复现和评估现有文档分块策略，构建了一个统一的框架来分析不同分块方法在密集检索系统中的效果。研究发现最优分块策略取决于任务类型：简单结构方法在语料库检索中表现更好，而LLM引导方法在文档内检索中更优。


<details>
  <summary>Details</summary>
Motivation: 文档分块是密集检索系统的关键预处理步骤，但分块策略的设计空间尚未被充分理解。现有研究提出了多种方法（如LLM引导的DenseX和LumberChunker，以及上下文感知的Late Chunking），但这些方法独立发展且评估基准重叠度低，难以进行直接比较。

Method: 本文复现了先前研究，并提出了一个系统性框架，从两个维度统一现有策略：(1) 分段方法，包括基于结构的方法（固定大小、句子级、段落级）以及语义感知和LLM引导方法；(2) 嵌入范式，决定分块相对于嵌入的时机（预嵌入分块与上下文感知分块）。研究在两种检索场景下评估这些方法：文档内检索（大海捞针）和语料库检索（标准信息检索任务）。

Result: 综合评估表明最优分块策略具有任务依赖性：对于语料库检索，简单结构方法优于LLM引导方法；对于文档内检索，LumberChunker表现最佳。上下文感知分块能提升语料库检索效果但会降低文档内检索性能。此外，分块大小与文档内检索效果呈中等相关性，但与语料库检索效果相关性较弱，表明分段方法差异并非完全由分块大小决定。

Conclusion: 研究结论指出，应根据具体任务选择合适的分块策略，简单的结构方法在许多场景下已足够有效，而复杂方法如LLM引导分块仅在特定任务（如文档内检索）中显现优势。本文为实践者提供了选择分块策略的理论指导。

Abstract: Document chunking is a critical preprocessing step in dense retrieval systems, yet the design space of chunking strategies remains poorly understood. Recent research has proposed several concurrent approaches, including LLM-guided methods (e.g., DenseX and LumberChunker) and contextualized strategies(e.g., Late Chunking), which generate embeddings before segmentation to preserve contextual information. However, these methods emerged independently and were evaluated on benchmarks with minimal overlap, making direct comparisons difficult.
  This paper reproduces prior studies in document chunking and presents a systematic framework that unifies existing strategies along two key dimensions: (1) segmentation methods, including structure-based methods (fixed-size, sentence-based, and paragraph-based) as well as semantically-informed and LLM-guided methods; and (2) embedding paradigms, which determine the timing of chunking relative to embedding (pre-embedding chunking vs. contextualized chunking). Our reproduction evaluates these approaches in two distinct retrieval settings established in previous work: in-document retrieval (needle-in-a-haystack) and in-corpus retrieval (the standard information retrieval task).
  Our comprehensive evaluation reveals that optimal chunking strategies are task-dependent: simple structure-based methods outperform LLM-guided alternatives for in-corpus retrieval, while LumberChunker performs best for in-document retrieval. Contextualized chunking improves in-corpus effectiveness but degrades in-document retrieval. We also find that chunk size correlates moderately with in-document but weakly with in-corpus effectiveness, suggesting segmentation method differences are not purely driven by chunk size. Our code and evaluation benchmarks are publicly available at (Anonymoused).

</details>


### [85] [Bending the Scaling Law Curve in Large-Scale Recommendation Systems](https://arxiv.org/abs/2602.16986)
*Qin Ding,Kevin Course,Linjian Ma,Jianhui Sun,Rouchen Liu,Zhao Zhu,Chunxing Yin,Wei Li,Dai Li,Yu Shi,Xuan Cao,Ze Yang,Han Li,Xing Liu,Bi Xue,Hongwei Li,Rui Jian,Daisy Shi He,Jing Qian,Matt Ma,Qunshu Zhang,Rui Li*

Main category: cs.IR

TL;DR: 本文提出ULTRA-HSTU，一种通过模型与系统端到端协同设计的新型顺序推荐模型。该模型通过创新输入序列设计、稀疏注意力机制和模型拓扑结构，解决了长序列建模的二次计算瓶颈问题，实现了5倍训练加速、21倍推理加速，并在生产环境中为数十亿用户带来4-8%的消费与参与度提升。


<details>
  <summary>Details</summary>
Motivation: 顺序建模已成为大规模推荐系统的基石，但长序列建模面临二次计算复杂度瓶颈。近期方法依赖交叉注意力机制虽缓解计算压力，却限制了自注意力的表示能力发挥，无法充分挖掘深度架构的潜力。

Method: 提出ULTRA-HSTU模型，采用端到端模型与系统协同设计框架，在输入序列构造、稀疏注意力机制设计以及模型拓扑优化三个层面进行创新，突破传统注意力机制的限制。

Result: 综合基准测试表明，相比传统模型，ULTRA-HSTU实现超过5倍训练扩展速度和21倍推理扩展速度提升，同时推荐质量更优；已在生产环境全面部署，服务数十亿日活用户，带来4%至8%的消费与参与度增长。

Conclusion: ULTRA-HSTU通过突破性的模型-系统协同设计，在保证推荐质量的前提下实现了计算效率的显著提升，为超大规模顺序推荐系统提供了可扩展的解决方案，验证了架构创新与系统优化协同的重要性。

Abstract: Learning from user interaction history through sequential models has become a cornerstone of large-scale recommender systems. Recent advances in large language models have revealed promising scaling laws, sparking a surge of research into long-sequence modeling and deeper architectures for recommendation tasks. However, many recent approaches rely heavily on cross-attention mechanisms to address the quadratic computational bottleneck in sequential modeling, which can limit the representational power gained from self-attention. We present ULTRA-HSTU, a novel sequential recommendation model developed through end-to-end model and system co-design. By innovating in the design of input sequences, sparse attention mechanisms, and model topology, ULTRA-HSTU achieves substantial improvements in both model quality and efficiency. Comprehensive benchmarking demonstrates that ULTRA-HSTU achieves remarkable scaling efficiency gains -- over 5x faster training scaling and 21x faster inference scaling compared to conventional models -- while delivering superior recommendation quality. Our solution is fully deployed at scale, serving billions of users daily and driving significant 4% to 8% consumption and engagement improvements in real-world production environments.

</details>


### [86] [WSDM Cup 2026 Multilingual Retrieval: A Low-Cost Multi-Stage Retrieval Pipeline](https://arxiv.org/abs/2602.16989)
*Chentong Hao,Minmao Wang*

Main category: cs.IR

TL;DR: 本文提出了一种针对WSDM Cup 2026多语种检索任务的低价检索系统，通过四阶段流水线（LLM查询扩展、BM25检索、稠密排序和重排序）在约千万级中文、波斯语和俄语新闻文档上实现高效检索，官方评估达到nDCG@20 0.403和Judged@20 0.95的优异性能。


<details>
  <summary>Details</summary>
Motivation: 面对多语种跨语言检索任务中计算资源受限的挑战，需要开发低成本高效率的检索系统，能够用英文查询从千万级多语种新闻文档中快速准确地检索相关信息，并在有限算力预算下最大化检索效果。

Method: 采用四阶段流水线方法：首先使用基于大语言模型的GRF风格查询扩展来增强原始查询；然后通过BM25算法进行候选文档初筛；接着使用jina-embeddings-v4模型生成长文本稠密向量表示并进行相似度排序；最后对前20个候选结果使用Qwen3-Reranker-4B进行pointwise重排序，其余980个结果保持稠密排序顺序以平衡效果与计算成本。

Result: 在官方评估中，该系统实现了nDCG@20为0.403和Judged@20为0.95的优异性能，证明了四阶段流水线在有限计算预算下的有效性。通过消融实验量化了各阶段的贡献，分析了查询扩展、稠密排序和重排序策略在不同计算约束下的性价比表现。

Conclusion: 该低代价检索系统通过合理的流水线设计在多语种检索任务上取得了显著效果，各组件在有限计算资源下均发挥重要作用。研究表明，LLM查询扩展、稠密排序和选择性重排序的组合是资源受限场景下跨语言信息检索的实用解决方案，为类似任务提供了可复现的性价比优化范式。

Abstract: We present a low-cost retrieval system for the WSDM Cup 2026 multilingual retrieval task, where English queries are used to retrieve relevant documents from a collection of approximately ten million news articles in Chinese, Persian, and Russian, and to output the top-1000 ranked results for each query. We follow a four-stage pipeline that combines LLM-based GRF-style query expansion with BM25 candidate retrieval, dense ranking using long-text representations from jina-embeddings-v4, and pointwise re-ranking of the top-20 candidates using Qwen3-Reranker-4B while preserving the dense order for the remaining results. On the official evaluation, the system achieves nDCG@20 of 0.403 and Judged@20 of 0.95. We further conduct extensive ablation experiments to quantify the contribution of each stage and to analyze the effectiveness of query expansion, dense ranking, and top-$k$ reranking under limited compute budgets.

</details>


### [87] [LiveGraph: Active-Structure Neural Re-ranking for Exercise Recommendation](https://arxiv.org/abs/2602.17036)
*Rong Fu,Zijian Zhang,Haiyun Wei,Jiekai Wu,Kun Liu,Xianda Li,Haoyu Zhao,Yang Li,Yongtai Liu,Ziming Wang,Rui Lu,Simon Fong*

Main category: cs.IR

TL;DR: 本文提出LiveGraph，一种新型主动结构神经重排序框架，用于解决智能教育推荐中的学生参与度长尾分布和个性化学习轨迹适应问题。该方法通过图表示增强和动态重排序机制平衡推荐精度与内容多样性，在真实数据集上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 数字学习环境的扩展催生了对个性化教育内容的需求，但现有推荐系统面临学生参与度长尾分布（活跃与不活跃学生间的信息鸿沟）和无法适应独特学习轨迹的双重挑战，导致推荐结果缺乏精准性和教学多样性。

Method: LiveGraph采用主动结构神经重排序框架，核心包括：1）图表示增强策略，利用学习历史的结构关系桥接不同活跃度学生的信息差距；2）动态重排序机制，通过重新排序提升推荐多样性。该方法优先考虑学习历史中的结构关联，实现精度与多样性的平衡。

Result: 在多个真实世界数据集上的综合实验表明，LiveGraph在预测准确性和练习多样性两个维度均显著超越当前先进基线方法。

Conclusion: LiveGraph有效解决了教育推荐系统的长尾分布和个性化适应难题，通过图结构建模和动态重排序实现了推荐精度与教学多样性的协同优化，为智能教育系统提供了更有效的推荐框架。

Abstract: The continuous expansion of digital learning environments has catalyzed the demand for intelligent systems capable of providing personalized educational content. While current exercise recommendation frameworks have made significant strides, they frequently encounter obstacles regarding the long-tailed distribution of student engagement and the failure to adapt to idiosyncratic learning trajectories. We present LiveGraph, a novel active-structure neural re-ranking framework designed to overcome these limitations. Our approach utilizes a graph-based representation enhancement strategy to bridge the information gap between active and inactive students while integrating a dynamic re-ranking mechanism to foster content diversity. By prioritizing the structural relationships within learning histories, the proposed model effectively balances recommendation precision with pedagogical variety. Comprehensive experimental evaluations conducted on multiple real-world datasets demonstrate that LiveGraph surpasses contemporary baselines in both predictive accuracy and the breadth of exercise diversity.

</details>


### [88] [On the Reliability of User-Centric Evaluation of Conversational Recommender Systems](https://arxiv.org/abs/2602.17264)
*Michael Müller,Amir Reza Mohammadi,Andreas Peintner,Beatriz Barroso Gstrein,Günther Specht,Eva Zangerle*

Main category: cs.IR

TL;DR: 该论文针对对话推荐系统（CRS）的用户中心评估中第三方标注的可靠性问题，进行了一项大规模实证研究。研究发现，功利性维度（如准确性和满意度）在聚合后具有中等信度，而社会性维度（如人性感和默契感）信度较低；多个评估维度往往坍缩为单一全局质量信号，存在明显的晕轮效应。结论是单标注者和基于大语言模型的评估协议有效性存疑，未来应采用多标注者聚合和降维策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估对话推荐系统时，越来越依赖众包工人或大语言模型对静态对话日志进行第三方标注，以实现可扩展性。然而，这种做法的信度尚未得到充分检验。缺乏可靠的评估标准可能导致结果偏差，影响系统的进一步优化和比较。因此，本文旨在系统性地检验用户中心评估框架下各维度的信度及其相互关系，为后续研究提供方法学依据。

Method: 研究采用大规模实证设计，共招募124名众包工人，对200段ReDial对话进行标注，累计获得1,053条标注。使用18维度的CRS‑Que框架对每段对话进行评分。随后运用随机效应信度模型和关联分析，量化各维度的稳定性以及维度间的相互依赖关系。

Result: ① 功利性、结果导向的维度（如准确性、实用性、满意度）在聚合后呈现中等信度；② 社会性、情境化的构念（如人性感、默契感）信度显著偏低；③ 许多维度在统计上高度相关，倾向于坍缩为一个全局质量信号，暴露出强烈的晕轮效应；④ 标注者间一致性在不同维度上存在明显差异。

Conclusion: 研究结果表明，依赖单一标注者或大语言模型进行离线评估的做法存在信度风险。为提高评估的稳健性，建议采用多标注者聚合策略，并结合维度降维技术以消除冗余。该结论对构建更可靠的对话推荐系统评估体系具有重要指导意义。

Abstract: User-centric evaluation has become a key paradigm for assessing Conversational Recommender Systems (CRS), aiming to capture subjective qualities such as satisfaction, trust, and rapport. To enable scalable evaluation, recent work increasingly relies on third-party annotations of static dialogue logs by crowd workers or large language models. However, the reliability of this practice remains largely unexamined. In this paper, we present a large-scale empirical study investigating the reliability and structure of user-centric CRS evaluation on static dialogue transcripts. We collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework. Using random-effects reliability models and correlation analysis, we quantify the stability of individual dimensions and their interdependencies. Our results show that utilitarian and outcome-oriented dimensions such as accuracy, usefulness, and satisfaction achieve moderate reliability under aggregation, whereas socially grounded constructs such as humanness and rapport are substantially less reliable. Furthermore, many dimensions collapse into a single global quality signal, revealing a strong halo effect in third-party judgments. These findings challenge the validity of single-annotator and LLM-based evaluation protocols and motivate the need for multi-rater aggregation and dimension reduction in offline CRS evaluation.

</details>


### [89] [Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation](https://arxiv.org/abs/2602.17354)
*Daniele Malitesta,Emanuele Rossi,Claudio Pomo,Tommaso Di Noia,Fragkiskos D. Malliaros*

Main category: cs.IR

TL;DR: 本文首次形式化定义了多模态推荐系统中的模态缺失问题，利用商品共购图结构将其转化为图特征插补问题，提出四种无需训练的特征传播方法。实验表明该方法可无缝集成至现有推荐系统，在不同缺失场景下均优于传统插补方法，并首次分析了特征同质性对插补效果的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统在实际应用中常遇到商品图像、描述等模态数据缺失或噪声问题，当前主流做法是丢弃含缺失模态的样本并在子集上训练，导致数据浪费且性能受限。该问题在学界缺乏系统研究，亦无明确的形式化定义，亟需专门解决方案。

Method: 通过挖掘用户-商品交互图结构，将缺失模态问题重构为基于商品共购图（item-item co-purchase graph）的特征插补任务，设计四种训练无关（training-free）的特征传播机制，利用图结构将邻居节点的多模态特征聚合至目标节点以补全缺失特征。

Result: 在多个公开多模态推荐数据集上的实验表明：1）所提方法可灵活嵌入任意现有多模态推荐框架；2）能保持甚至扩大多模态系统相比传统系统的性能优势；3）在不同缺失模态机制（MCAR、MAR、MNAR）下均显著优于KNN、矩阵分解等传统插补方法；4）首次揭示了商品图中特征同质性程度直接影响图插补效果。

Conclusion: 基于商品共购图特征插补的训练无关方法为多模态推荐中的模态缺失问题提供了有效解决方案，特征传播机制充分利用了物品间的协同信号，特征同质性分析为理解图插补机理提供了新视角。该工作推动了多模态推荐系统在真实场景下的实用化。

Abstract: Multimodal recommender systems (RSs) represent items in the catalog through multimodal data (e.g., product images and descriptions) that, in some cases, might be noisy or (even worse) missing. In those scenarios, the common practice is to drop items with missing modalities and train the multimodal RSs on a subsample of the original dataset. To date, the problem of missing modalities in multimodal recommendation has still received limited attention in the literature, lacking a precise formalisation as done with missing information in traditional machine learning. In this work, we first provide a problem formalisation for missing modalities in multimodal recommendation. Second, by leveraging the user-item graph structure, we re-cast the problem of missing multimodal information as a problem of graph features interpolation on the item-item co-purchase graph. On this basis, we propose four training-free approaches that propagate the available multimodal features throughout the item-item graph to impute the missing features. Extensive experiments on popular multimodal recommendation datasets demonstrate that our solutions can be seamlessly plugged into any existing multimodal RS and benchmarking framework while still preserving (or even widen) the performance gap between multimodal and traditional RSs. Moreover, we show that our graph-based techniques can perform better than traditional imputations in machine learning under different missing modalities settings. Finally, we analyse (for the first time in multimodal RSs) how feature homophily calculated on the item-item graph can influence our graph-based imputations.

</details>


### [90] [Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers](https://arxiv.org/abs/2602.17410)
*Bingqian Li,Bowen Zheng,Xiaolei Wang,Long Zhang,Jinpeng Wang,Sheng Chen,Wayne Xin Zhao,Ji-rong Wen*

Main category: cs.IR

TL;DR: 针对LLM推荐系统中离线负样本区分性不足的问题，本文提出ILRec框架，利用中间层自硬负信号进行动态细粒度偏好学习，通过跨层优化与蒸馏两阶段训练及轻量级协同过滤奖励机制，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法依赖序列级离线生成的负样本，在面临大规模负样本空间时区分性和信息性不足，导致模型适应性不佳，需要更动态、细粒度的负监督信号。

Method: 提出ILRec框架：1)从中间层识别自硬负标记作为动态细粒度负监督；2)设计跨层偏好优化和跨层偏好蒸馏两阶段训练机制；3)引入轻量级协同过滤模型为负信号分配标记级奖励，防止过度惩罚假负样本。

Result: 在三个数据集上的广泛实验表明，ILRec能有效提升基于LLM的推荐系统性能，验证了利用中间层自硬负信号进行偏好学习的有效性。

Conclusion: ILRec通过挖掘中间层自硬负信号的动态监督价值，克服了传统离线负样本的局限性，为LLM在推荐系统中的偏好学习提供了更有效的解决方案，具有实际应用潜力。

Abstract: Large language models (LLMs) have shown great promise in recommender systems, where supervised fine-tuning (SFT) is commonly used for adaptation. Subsequent studies further introduce preference learning to incorporate negative samples into the training process. However, existing methods rely on sequence-level, offline-generated negatives, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. To address these challenges, we propose ILRec, a novel preference fine-tuning framework for LLM-based recommendation, leveraging self-hard negative signals extracted from intermediate layers to improve preference learning. Specifically, we identify self-hard negative tokens from intermediate layers as fine-grained negative supervision that dynamically reflects the model's preference learning process. To effectively integrate these signals into training, we design a two-stage framework comprising cross-layer preference optimization and cross-layer preference distillation, enabling the model to jointly discriminate informative negatives and enhance the quality of negative signals from intermediate layers. In addition, we introduce a lightweight collaborative filtering model to assign token-level rewards for negative signals, mitigating the risk of over-penalizing false negatives. Extensive experiments on three datasets demonstrate ILRec's effectiveness in enhancing the performance of LLM-based recommender systems.

</details>


### [91] [A Picture of Agentic Search](https://arxiv.org/abs/2602.17518)
*Francesca Pezzuti,Ophir Frieder,Fabrizio Silvestri,Sean MacAvaney,Nicola Tonellotto*

Main category: cs.IR

TL;DR: 针对信息检索系统日益面临自动化智能体查询而非仅人类查询的挑战，本文揭示了当前以人为中心的IR系统在假设失效的情况下导致性能与优化失准的问题，为此提出了ASQ数据集及收集方法，以填补智能体搜索行为数据的空白，支持IR系统向人机双目标优化的演进。


<details>
  <summary>Details</summary>
Motivation: 自动化智能体正大规模参与信息检索，但IR系统、评估指标、用户模型等仍围绕人类查询与行为设计，导致系统假设与实际负载、可预测性及查询行为脱节，影响缓存、查询预处理和指标有效性，且缺乏智能体搜索行为数据阻碍了数据驱动的评估与优化。

Method: 开发了一套收集智能体检索增强系统回答查询时产生与消费的全量数据的方法论，构建Agentic Search Queryset (ASQ)数据集，包含推理诱发的查询、检索文档及思维过程。

Result: 发布ASQ数据集，覆盖HotpotQA、Researchy Questions和MS MARCO的查询，包含3个多样智能体与2个检索管道的数据；同时提供配套工具包，支持向新智能体、检索器与数据集扩展。

Conclusion: 本研究通过提供ASQ数据集及可扩展方法，填补了智能体搜索行为数据的空白，为IR系统适配人机双用户需求、优化性能与评估指标提供了关键基础，推动IR向智能体友好演进。

Abstract: With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effectiveness, query pre-processing may add overhead without improving results, and standard metrics may mismeasure satisfaction. Without adaptation, retrieval models risk satisfying neither humans, nor the emerging user segment of agents. However, datasets capturing agent search behaviour are lacking, which is a critical gap given IR's historical reliance on data-driven evaluation and optimisation. We develop a methodology for collecting all the data produced and consumed by agentic retrieval-augmented systems when answering queries, and we release the Agentic Search Queryset (ASQ) dataset. ASQ contains reasoning-induced queries, retrieved documents, and thoughts for queries in HotpotQA, Researchy Questions, and MS MARCO, for 3 diverse agents and 2 retrieval pipelines. The accompanying toolkit enables ASQ to be extended to new agents, retrievers, and datasets.

</details>


### [92] [Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval](https://arxiv.org/abs/2602.17654)
*Jiaqi Xi,Raghav Saboo,Luming Chen,Martin Wang,Sudeep Das*

Main category: cs.IR

TL;DR: 本文提出一种两阶段"挖掘与精炼"对比学习框架，用于提升电商搜索中的语义文本嵌入效果。该框架首先通过轻量级LLM和人工标注构建策略一致的监督信号，训练双塔检索模型形成鲁棒语义空间；第二阶段挖掘难样本并重新标注，引入多类别圆周损失强化不同相关度层级的边界区分，结合拼写增强和查询生成提升鲁棒性，最终在离线评估和A/B测试中显著提升检索相关性和业务指标。


<details>
  <summary>Details</summary>
Motivation: 电商搜索面临长尾噪声查询泛化、策略合规性约束、以及相关性分级（精确匹配、替代、互补）三大挑战。现有方法难以在可扩展监督下实现不同相关性层级的清晰分离，影响线上混合排序和阈值设定的稳定性。需要一种既能保证策略一致性，又能锐化相似度边界以提升检索质量的嵌入学习框架。

Method: 框架采用两阶段设计：第一阶段基于人工标注和参与度审计微调轻量级LLM，生成三档相关性标签，训练多语言Siamese双塔模型，使用标签感知监督对比损失构建鲁棒全局语义空间；第二阶段通过近似最近邻（ANN）挖掘难样本，利用策略对齐的LLM重标注，引入多类别圆周损失函数，显式锐化不同相关性层级的相似度边界，并辅以加法拼写增强和合成查询生成提升模型鲁棒性。

Result: 在全面的离线评估和线上A/B测试中，该框架在检索相关性上取得显著提升，用户参与度指标和业务影响指标均实现统计显著性增益，验证了方法在真实电商场景下的有效性。

Conclusion: 该"挖掘与精炼"框架通过策略一致的监督构建、对比学习空间优化和边界锐化损失设计，有效解决了电商搜索中相关性分级的嵌入学习难题，为大规模检索系统提供了可扩展且鲁棒的语义表示方案。

Abstract: We propose a two-stage "Mine and Refine" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [93] [MMCAformer: Macro-Micro Cross-Attention Transformer for Traffic Speed Prediction with Microscopic Connected Vehicle Driving Behavior](https://arxiv.org/abs/2602.16730)
*Lei Han,Mohamed Abdel-Aty,Younggun Kim,Yang-Jun Joo,Zubayer Islam*

Main category: cs.LG

TL;DR: 本文提出MMCAformer模型，通过宏-微观交叉注意力机制整合联网车辆微观驾驶行为与宏观交通特征进行速度预测。该模型采用自注意力学习宏观依赖、交叉注意力捕捉宏微观时空交互，并使用学生t负对数似然损失进行不确定性估计。佛罗里达四条高速公路实验证明，引入微观特征使RMSE、MAE和MAPE分别降低9.0%、6.9%和10.2%，预测区间宽度减少10.1-24.0%，其中急刹车和急加速是最关键特征，且在拥堵低速条件下改善效果更显著。


<details>
  <summary>Details</summary>
Motivation: 现有交通速度预测研究过度依赖宏观交通流数据，忽视了微观驾驶行为对交通动态的影响。联网车辆（CV）数据提供了丰富的驾驶行为特征，为融合微观行为洞察带来了新机遇。因此，本研究旨在开发一种能够有效整合宏微观特征的新型预测模型，以充分利用CV数据中的行为信息，提升预测精度和可靠性。

Method: 提出Macro-Micro Cross-Attention Transformer（MMCAformer）模型。模型采用双路径架构：宏观路径利用自注意力机制捕获交通流的时空内在依赖；设计专门的交叉注意力机制动态捕捉宏观交通状态与微观驾驶行为特征之间的时空交互关系。采用学生t负对数似然损失函数进行模型优化，可同时输出点预测值和预测区间，实现不确定性量化。该机制使模型能够自适应权衡不同特征的重要性。

Result: 在佛罗里达四条高速公路的实证评估中，MMCAformer显著优于各类基线模型。与仅使用宏观特征相比，引入微观驾驶行为特征使整体RMSE、MAE和MAPE分别降低9.0%、6.9%和10.2%。预测不确定性大幅缩小，平均预测区间宽度在四条高速公路上减少了10.1-24.0%。特征重要性分析表明，急刹车和急加速频率是最具影响力的微观行为特征。此外，在拥堵、低速度的交通条件下，模型性能改善更为突出，说明微观行为信息在复杂交通场景中价值更高。

Conclusion: 本研究成功验证了微观驾驶行为特征对交通速度预测的价值。MMCAformer通过创新的交叉注意力机制实现宏微观信息深度融合，不仅显著提升预测精度，还增强了不确定性估计能力。研究结果表明，在交通预测中考虑人类驾驶行为至关重要，为基于联网车辆数据的智能交通管理提供了新的技术路径。特别是在拥堵状态下微观行为信息的显著作用，为交通管理精细化发展提供了重要启示。

Abstract: Accurate speed prediction is crucial for proactive traffic management to enhance traffic efficiency and safety. Existing studies have primarily relied on aggregated, macroscopic traffic flow data to predict future traffic trends, whereas road traffic dynamics are also influenced by individual, microscopic human driving behaviors. Recent Connected Vehicle (CV) data provide rich driving behavior features, offering new opportunities to incorporate these behavioral insights into speed prediction. To this end, we propose the Macro-Micro Cross-Attention Transformer (MMCAformer) to integrate CV data-based micro driving behavior features with macro traffic features for speed prediction. Specifically, MMCAformer employs self-attention to learn intrinsic dependencies in macro traffic flow and cross-attention to capture spatiotemporal interplays between macro traffic status and micro driving behavior. MMCAformer is optimized with a Student-t negative log-likelihood loss to provide point-wise speed prediction and estimate uncertainty. Experiments on four Florida freeways demonstrate the superior performance of the proposed MMCAformer compared to baselines. Compared with only using macro features, introducing micro driving behavior features not only enhances prediction accuracy (e.g., overall RMSE, MAE, and MAPE reduced by 9.0%, 6.9%, and 10.2%, respectively) but also shrinks model prediction uncertainty (e.g., mean predictive intervals decreased by 10.1-24.0% across the four freeways). Results reveal that hard braking and acceleration frequencies emerge as the most influential features. Such improvements are more pronounced under congested, low-speed traffic conditions.

</details>


### [94] [A Few-Shot LLM Framework for Extreme Day Classification in Electricity Markets](https://arxiv.org/abs/2602.16735)
*Saud Alghumayjan,Ming Yi,Bolun Xu*

Main category: cs.LG

TL;DR: 该论文提出基于大语言模型的少样本分类框架，用于预测次日电力现货价格尖峰。通过将系统状态信息转化为自然语言提示输入LLM，该方法在德州电力市场数据上性能媲美SVM和XGBoost，且在数据稀缺时表现更优，展现了LLM在电力价格预测中的数据高效性。


<details>
  <summary>Details</summary>
Motivation: 电力价格尖峰预测对风险管理和市场决策至关重要，但传统监督学习方法依赖大量标注数据。实际应用中历史数据往往稀缺，亟需开发在有限样本下仍能保持高性能的数据高效预测方法。

Method: 研究构建基于LLM的少样本学习框架：聚合电力需求、可再生能源发电、天气预报及近期电价等系统状态信息为统计特征；将这些特征格式化为自然语言提示并输入LLM；模型输出次日价格尖峰的预测概率及置信度。在德州电力市场历史数据上验证，并与SVM、XGBoost等监督模型对比。

Result: 在德州电力市场实验表明：该LLM少样本方法性能与SVM、XGBoost等传统监督模型相当；在历史数据有限的情况下，LLM方法显著优于这两种模型，展现出更强的数据效率和泛化能力。

Conclusion: 该研究证实了LLM在电力价格尖峰分类任务中的潜力，特别是在数据稀缺场景下可作为高效工具。这为电力市场小样本预测问题提供了新范式，表明通过自然语言提示工程，LLM能有效利用有限数据做出可靠决策。

Abstract: This paper proposes a few-shot classification framework based on Large Language Models (LLMs) to predict whether the next day will have spikes in real-time electricity prices. The approach aggregates system state information, including electricity demand, renewable generation, weather forecasts, and recent electricity prices, into a set of statistical features that are formatted as natural-language prompts and fed to an LLM along with general instructions. The model then determines the likelihood that the next day would be a spike day and reports a confidence score. Using historical data from the Texas electricity market, we demonstrate that this few-shot approach achieves performance comparable to supervised machine learning models, such as Support Vector Machines and XGBoost, and outperforms the latter two when limited historical data are available. These findings highlight the potential of LLMs as a data-efficient tool for classifying electricity price spikes in settings with scarce data.

</details>


### [95] [Real-time Secondary Crash Likelihood Prediction Excluding Post Primary Crash Features](https://arxiv.org/abs/2602.16739)
*Lei Han,Mohamed Abdel-Aty,Zubayer Islam,Chenzhu Wang*

Main category: cs.LG

TL;DR: 针对二次事故预测中事后特征难以实时获取的问题，本文提出了一种混合预测框架。该方法利用动态时空窗口提取实时交通流和环境特征，结合集成学习和投票机制，在佛罗里达高速公路上实现了91%的识别率和0.952的AUC值，显著优于现有研究。


<details>
  <summary>Details</summary>
Motivation: 二次事故预测对主动交通管理至关重要，但现有方法依赖难以实时获取的事故事后特征（如事故类型和严重程度），限制了实际应用。为此，本文提出不依赖事后特征的预测框架。

Method: 提出混合预测框架，设计动态时空窗口从一次事故位置及其上游路段提取实时交通流和环境特征。包含三个模型：一次事故模型和两个二次事故模型。采用集成学习策略融合六种机器学习算法，并通过投票机制综合三个模型的输出。

Result: 在佛罗里达高速公路上的实验表明，该框架能以0.20的低误报率正确识别91%的二次事故。AUC值从单个模型的0.654、0.744、0.902提升至混合模型的0.952，优于已有研究。

Conclusion: 所提混合框架在不依赖事后特征的情况下，实现了高性能的二次事故预测，显著优于单一模型和现有方法，具有实际应用价值。

Abstract: Secondary crash likelihood prediction is a critical component of an active traffic management system to mitigate congestion and adverse impacts caused by secondary crashes. However, existing approaches mainly rely on post-crash features (e.g., crash type and severity) that are rarely available in real time, limiting their practical applicability. To address this limitation, we propose a hybrid secondary crash likelihood prediction framework that does not depend on post-crash features. A dynamic spatiotemporal window is designed to extract real-time traffic flow and environmental features from primary crash locations and their upstream segments. The framework includes three models: a primary crash model to estimate the likelihood of secondary crash occurrence, and two secondary crash models to evaluate traffic conditions at crash and upstream segments under different comparative scenarios. An ensemble learning strategy integrating six machine learning algorithms is developed to enhance predictive performance, and a voting-based mechanism combines the outputs of the three models. Experiments on Florida freeways demonstrate that the proposed hybrid framework correctly identifies 91% of secondary crashes with a low false alarm rate of 0.20. The Area Under the ROC Curve improves from 0.654, 0.744, and 0.902 for the individual models to 0.952 for the hybrid model, outperforming previous studies.

</details>


### [96] [DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning](https://arxiv.org/abs/2602.16742)
*Haoxiang Sun,Lizhen Xu,Bing Zhao,Wotao Yin,Wei Wang,Boyu Yang,Rui Wang,Hu Wei*

Main category: cs.LG

TL;DR: 本文针对RLVR训练数据集的局限性，提出了DeepVision-103K数据集，该数据集涵盖多样化的K12数学主题和视觉元素。基于此数据训练的模型在多模态数学基准测试中表现优异，并展现出良好的泛化能力，有效提升了LMM的视觉感知、反思和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR数据集主要来源于小规模人工构建或已有资源的重组，导致数据多样性和覆盖范围有限，制约了LMM视觉反思与推理能力的进一步提升。

Method: 作者构建了DeepVision-103K数据集，覆盖广泛的K12数学主题、知识点和视觉元素，用于RLVR训练。通过该数据集训练LMM，评估其在多模态数学基准和通用推理任务上的表现。

Result: 在DeepVision上训练的模型在多模态数学基准测试中性能强劲，且能有效泛化到通用多模态推理任务。模型展现出增强的视觉感知、反思和推理能力，验证了DeepVision对推进多模态推理的有效性。

Conclusion: DeepVision-103K提供了一个高质量、大规模的多模态数学数据集，能够显著提升LMM通过RLVR获得的视觉推理能力，并为通用多模态推理任务带来正向迁移，是推动多模态大模型发展的重要资源。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \textbf{DeepVision-103K}, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: \href{https://huggingface.co/datasets/skylenage/DeepVision-103K}{this url}.

</details>


### [97] [PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency](https://arxiv.org/abs/2602.16745)
*Zhangyi Liu,Huaizhi Qu,Xiaowei Yin,He Sun,Yanjun Han,Tianlong Chen,Zhun Deng*

Main category: cs.LG

TL;DR: 本文提出PETS框架，通过定义自我一致性率（与无限预算多数投票的一致性）并构建优化框架，在有限预算下实现高效测试时自我一致性。离线场景建模为众包问题，在线场景采用自适应预算分配，相比均匀采样在GPQA数据集上可减少75%（离线）和55%（在线）预算，同时达到完美自我一致性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法虽可通过聚合随机推理轨迹提升模型性能，但在有限预算约束下如何实现样本高效的自我一致性仍是一个开放挑战。亟需建立理论严谨的轨迹分配原则，以在保证效果的同时最大化推理效率。

Method: 提出PETS（原则且高效的测试时自我一致性）框架，核心创新包括：1）定义自我一致性率为与无限预算多数投票的吻合度，建立可理论分析的优化框架；2）离线场景下将推理轨迹建模为众包中的工作者，利用成熟理论提出多数投票分配算法；3）在线流式场景下提出自适应预算分配方法，根据问题难度动态调整资源，保持理论保证与计算效率。

Result: 实验表明PETS显著优于均匀分配基线。在GPQA数据集上，离线设置实现完美自我一致性且预算最多降低75%，在线设置同样达到完美一致性且预算最多减少55%，验证了轨迹分配原则的有效性。

Conclusion: PETS通过原则性的轨迹分配优化，为有限预算下的测试时自我一致性提供了理论基础和实用算法，实现了推理效果与样本效率的平衡，为模型推理资源优化开辟了新方向。

Abstract: Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.

</details>


### [98] [Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking](https://arxiv.org/abs/2602.16746)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本文通过几何分析揭示，Transformer在模算术任务中的Grokking现象源于从亚稳态的低维执行子空间逃脱，且正交方向曲率积累遵循幂律并先于泛化发生。


<details>
  <summary>Details</summary>
Motivation: Grokking现象——在小规模算法任务中模型从记忆到泛化的延迟转变——的内在机制尚不清晰，亟需从优化动力学的几何结构角度进行系统性研究。

Method: 对模算术任务的Transformer进行主成分分析提取低维执行子空间，利用交换子缺陷量化梯度步的非交换性（曲率），并在此子空间及其正交方向上进行因果干预实验。

Result: 训练轨迹主要受限于低维执行子空间（单个主成分占68-83%方差），正交方向曲率急剧增长且先于泛化突破，领先时间服从grokking时间尺度的幂律；因果干预表明沿子空间运动是grokking的必要条件，但人工增强曲率不足，正交梯度流必要而不充分。

Conclusion: Grokking本质上是优化过程逃离由低维 confinement 和横向曲率积累所表征的亚稳态区域，为理解泛化延迟提供了几何动力学框架。

Abstract: Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experiments show that motion along the learned subspace is necessary for grokking, while artificially increasing curvature is insufficient. Together, these results support a geometric account in which grokking reflects escape from a metastable regime characterized by low-dimensional confinement and transverse curvature accumulation. All findings replicate across this learning-rate range, a qualitatively different slow regime (lr=5e-5, wd=0.1, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes. Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose-response across four operations, while artificially boosting curvature defects has no effect.

</details>


### [99] [LiveClin: A Live Clinical Benchmark without Leakage](https://arxiv.org/abs/2602.16747)
*Xidong Wang,Shuqi Guo,Yue Shen,Junying Chen,Jian Wang,Jinjie Gu,Ping Zhang,Lei Liu,Benyou Wang*

Main category: cs.LG

TL;DR: 针对医学大模型评估中数据污染和知识过时导致的基准分数虚高问题，本研究提出LiveClin动态基准，基于1407份当代同行评议病例报告和6605个问题，采用239名医生参与的AI-人类协同工作流构建，评估显示顶级模型准确率仅35.7%，显著低于主任医师水平，为医学大模型发展提供了持续演化的临床导向评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有医学大模型评估存在两大核心缺陷：一是数据污染导致模型在静态基准上表现虚高，二是知识过时无法反映真实临床实践。传统基准缺乏时效性、动态性和临床复杂性，难以有效衡量模型在真实医疗场景中的可靠性和实用性，亟需构建与临床实践同步、抗污染、多模态的评估体系。

Method: 采用AI-人类协同工作流，联合239名认证医师将真实患者病例转化为复杂多模态评估场景。数据源为当代同行评议病例报告，每半年更新一次以确保临床时效性。构建覆盖完整临床路径的多层次问题体系，最终形成包含1407个病例报告和6605个问题的动态基准，并通过严格的医师审核验证机制确保数据质量与临床真实性。

Result: 评估涵盖26个主流医学大模型，结果显示顶级模型病例准确率仅为35.7%，凸显真实临床场景的高难度。人类专家表现显著优于模型，其中主任医师准确率最高，主治医师次之，两者均超过大多数模型。该结果揭示了当前医学大模型与临床专家之间的巨大性能鸿沟，验证了LiveClin作为严格评估工具的有效性。

Conclusion: LiveClin通过动态更新机制和临床医生深度参与，为医学大模型评估提供了持续演化、临床落地的基准框架。研究结果明确指出了模型性能与真实医疗需求间的显著差距，为未来医学AI研发指明了方向：必须超越静态基准，聚焦临床实用性和时效性，推动模型向更高可靠性和真实世界效用迈进。

Abstract: The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for approximating real-world clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. In benchmarking against human experts, Chief Physicians achieved the highest accuracy, followed closely by Attending Physicians, with both surpassing most models. LiveClin thus provides a continuously evolving, clinically grounded framework to guide the development of medical LLMs towards closing this gap and achieving greater reliability and real-world utility. Our data and code are publicly available at https://github.com/AQ-MedAI/LiveClin.

</details>


### [100] [Attending to Routers Aids Indoor Wireless Localization](https://arxiv.org/abs/2602.16762)
*Ayush Roy,Tahsin Fuad Hassan,Roshan Ayyalasomayajula,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 针对基于Wi-Fi信号的机器学习定位算法在多路由器信息聚合时权重分配不当的问题，本文受传统加权三角测量方法启发，提出引入注意力机制，通过对各路由器贡献进行差异化加权，在开源数据集上实现定位精度超过30%的提升。


<details>
  <summary>Details</summary>
Motivation: 现有算法在聚合多路由器信息时未能合理加权，导致模型收敛次优且精度下降；传统加权三角测量方法为路由器权重分配提供了理论借鉴。

Method: 在标准机器学习定位架构中嵌入注意力层，为不同路由器分配差异化权重，强调关键路由器的信息贡献。

Result: 在开源数据集上的评估表明，该注意力机制方法相比基准架构在定位准确率上提升超过30%。

Conclusion: 通过注意力机制实现路由器级别的差异化加权聚合，可显著提升Wi-Fi定位系统在异构环境下的性能表现。

Abstract: Modern machine learning-based wireless localization using Wi-Fi signals continues to face significant challenges in achieving groundbreaking performance across diverse environments. A major limitation is that most existing algorithms do not appropriately weight the information from different routers during aggregation, resulting in suboptimal convergence and reduced accuracy. Motivated by traditional weighted triangulation methods, this paper introduces the concept of attention to routers, ensuring that each router's contribution is weighted differently when aggregating information from multiple routers for triangulation. We demonstrate, by incorporating attention layers into a standard machine learning localization architecture, that emphasizing the relevance of each router can substantially improve overall performance. We have also shown through evaluation over the open-sourced datasets and demonstrate that Attention to Routers outperforms the benchmark architecture by over 30% in accuracy.

</details>


### [101] [Omitted Variable Bias in Language Models Under Distribution Shift](https://arxiv.org/abs/2602.16784)
*Victoria Lin,Louis-Philippe Morency,Eli Ben-Michael*

Main category: cs.LG

TL;DR: 本文针对语言模型在分布偏移下的脆弱性，创新性地将分布偏移分解为可观测与不可观测两部分，揭示现有方法仅处理前者而导致的遗漏变量偏差问题。提出一个理论框架，将不可观测变量的强度映射至最坏情况泛化性能边界。实证表明，该框架不仅提供更原则性的分布外评估指标，还能提升真实OOD性能，并可在有标签时推断遗漏变量强度。


<details>
  <summary>Details</summary>
Motivation: 尽管现代语言模型在多任务上表现优异，但在分布偏移下仍表现出脆弱性。现有方法仅关注可观测的分布偏移成分，忽略了不可观测成分引发的遗漏变量偏差，这种偏差会损害模型评估和优化的有效性，亟需系统性解决方案。

Method: 引入一个理论框架，通过量化不可观测分布偏移成分（遗漏变量）的强度，推导语言模型在最坏情况下的泛化性能边界，从而将遗漏变量偏差问题形式化。

Result: 实验验证显示：1) 基于性能边界的评估方法提供了更原则性的分布外性能指标；2) 在优化中应用这些边界，相比标准分布偏移调整方法显著提升了真实OOD性能；3) 当目标分布标签可用时，该框架能够推断遗漏变量的强度。

Conclusion: 该研究成功建立了遗漏变量偏差与最坏情况泛化性能之间的理论联系，为语言模型在分布偏移下的评估和优化提供了更稳健的理论基础，是提升模型分布外泛化能力的重要进展。

Abstract: Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify that the resulting omitted variable bias from unobserved variables can compromise both evaluation and optimization in language models. To address this challenge, we introduce a framework that maps the strength of the omitted variables to bounds on the worst-case generalization performance of language models under distribution shift. In empirical experiments, we show that using these bounds directly in language model evaluation and optimization provides more principled measures of out-of-distribution performance, improves true out-of-distribution performance relative to standard distribution shift adjustment methods, and further enables inference about the strength of the omitted variables when target distribution labels are available.

</details>


### [102] [Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency](https://arxiv.org/abs/2602.16787)
*Victoria Lin,Xinnuo Xu,Rachel Lawrence,Risa Ueno,Amit Sharma,Javier Gonzalez,Niranjani Prasad*

Main category: cs.LG

TL;DR: 本文提出双重反事实一致性(DCC)方法，无需标注数据即可评估并提升大语言模型的因果推理能力，通过验证因果干预和反事实预测两个关键要素，在多种推理任务和模型上实现性能改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在推理基准测试中表现强劲，但在反事实问题上表现脆弱，暴露了其因果推理能力的缺陷。现有方法依赖大规模标注的反事实数据，但此类数据覆盖潜力空间的能力有限，亟需无需标注的评估与改进方法。

Method: 双重反事实一致性(DCC)是一种轻量级的推理时方法，通过验证模型执行因果干预和反事实预测两个核心因果推理要素，在不依赖标注反事实数据的前提下，测量和指导大语言模型的因果推理能力。

Result: 使用DCC评估了多种领先大语言模型在不同推理任务和干预设置下的因果推理能力。实验表明，DCC作为无需训练的测试时拒绝采样标准，能够直接提升多个模型家族在推理任务上的性能表现。

Conclusion: DCC提供了一种实用的无监督方法来衡量和增强大语言模型的因果推理能力，避免了昂贵的反事实数据标注成本，展示了在不同模型架构上的广泛适用性和有效性，为提升模型推理鲁棒性提供了新思路。

Abstract: Despite their strong performance on reasoning benchmarks, large language models (LLMs) have proven brittle when presented with counterfactual questions, suggesting weaknesses in their causal reasoning ability. While recent work has demonstrated that labeled counterfactual tasks can be useful benchmarks of LLMs' causal reasoning, producing such data at the scale required to cover the vast potential space of counterfactuals is limited. In this work, we introduce double counterfactual consistency (DCC), a lightweight inference-time method for measuring and guiding the ability of LLMs to reason causally. Without requiring labeled counterfactual data, DCC verifies a model's ability to execute two important elements of causal reasoning: causal intervention and counterfactual prediction. Using DCC, we evaluate the causal reasoning abilities of various leading LLMs across a range of reasoning tasks and interventions. Moreover, we demonstrate the effectiveness of DCC as a training-free test-time rejection sampling criterion and show that it can directly improve performance on reasoning tasks across multiple model families.

</details>


### [103] [Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models](https://arxiv.org/abs/2602.16793)
*Xingyu Dang,Rohit Agarwal,Rodrigo Porto,Anirudh Goyal,Liam H Fowl,Sanjeev Arora*

Main category: cs.LG

TL;DR: 该论文提出了一种低成本推理流水线，通过猜想提取和独立验证避免"认知陷阱"失败模式，使用通用现成模型在PB-Adv基准上达到67.1%准确率，每题成本仅31美元。


<details>
  <summary>Details</summary>
Motivation: 现有高性能数学推理模型要么依赖定制未发布模型，要么使用公开模型但推理成本极高（每题约3000美元），亟需使用通用模型的低成本解决方案。

Method: 作者识别了求解器-评分器流水线中的"认知陷阱"失败模式（迭代优化收敛到求解器和评分器都认为是正确的错误解），提出猜想提取方法：从生成解中分离候选引理，在全新环境中独立验证其正误两种形式（上下文解离）。

Result: 在IMO-ProofBench Advanced基准上，使用Gemini 3.0 Pro实现67.1%性能，平均每题成本约31美元。该结果在当时为PB-Adv最先进水平，成功率是其次优公共流水线的两倍以上，且成本仅为一小部分。

Conclusion: 该流水线通过猜想提取和上下文解离解决评分器失败问题，以大幅降低的推理成本实现了IMO级数学问题的一流性能，证明通用模型在可接受成本下也能达到高性能。

Abstract: In the past year, custom and unreleased math reasoning models reached gold medal performance on the International Mathematical Olympiad (IMO). Similar performance was then reported using large-scale inference on publicly available models but at prohibitive costs (e.g., 3000 USD per problem). In this work, we present an inference pipeline that attains best-in-class performance on IMO-style math problems at an average inference cost orders of magnitude below competing methods while using only general-purpose off-the-shelf models. Our method relies on insights about grader failure in solver-grader pipelines, which we call the Cognitive Well (iterative refinement converging to a wrong solution that the solver as well as the pipeline's internal grader consider to be basically correct). Our pipeline addresses these failure modes through conjecture extraction, wherein candidate lemmas are isolated from generated solutions and independently verified alongside their negations in a fresh environment (context detachment). On IMO-ProofBench Advanced (PB-Adv), our pipeline achieves 67.1 percent performance using Gemini 3.0 Pro with an average cost per question of approximately 31 USD. At the time of evaluation, this represented the state-of-the-art on PB-Adv among both public and unreleased models, and more than doubles the success rate of the next best publicly accessible pipeline, all at a fraction of the cost.

</details>


### [104] [Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning](https://arxiv.org/abs/2602.16796)
*Zifan Wang,Riccardo De Santi,Xiaoyu Mo,Michael M. Zavlanos,Andreas Krause,Karl H. Johansson*

Main category: cs.LG

TL;DR: 本文提出Tail-aware Flow Fine-Tuning (TFFT)，一种基于条件风险价值(CVaR)的分布微调算法。通过CVaR的变分对偶形式，将尾部控制分解为轻量级阈值优化和熵正则化微调两阶段，计算效率与标准期望微调相当，在文本到图像生成和分子设计中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有熵正则化微调方法仅最大化期望奖励，无法控制尾部行为。在实际部署中，低奖励尾部决定模型可靠性（限制最差结果），高奖励尾部促进发现（优先稀有高奖励样本）。缺乏对尾部的控制限制了模型在关键应用中的部署。

Method: TFFT利用CVaR的变分对偶形式，将分布优化分解为：1) 轻量级一维阈值优化，确定目标尾部分位点；2) 基于特定伪奖励的单次熵正则化微调过程。分别实现右CVaR（探索高奖励尾部）和左CVaR（控制最差样本）的尾部塑造目标。

Result: 该方法在保持与标准期望微调相当的计算成本的同时，有效实现了尾部控制。在文本到图像生成和分子设计等任务上验证了其有效性，能够平衡探索与可靠性。

Conclusion: TFFT为扩散和流模型的分布微调提供了原则性且高效的解决方案，通过CVaR实现对尾部行为的显式控制，在保持计算效率的同时提升模型在关键应用中的可靠性与发现能力。

Abstract: Fine-tuning pre-trained diffusion and flow models to optimize downstream utilities is central to real-world deployment. Existing entropy-regularized methods primarily maximize expected reward, providing no mechanism to shape tail behavior. However, tail control is often essential: the lower tail determines reliability by limiting low-reward failures, while the upper tail enables discovery by prioritizing rare, high-reward outcomes. In this work, we present Tail-aware Flow Fine-Tuning (TFFT), a principled and efficient distributional fine-tuning algorithm based on the Conditional Value-at-Risk (CVaR). We address two distinct tail-shaping goals: right-CVaR for seeking novel samples in the high-reward tail and left-CVaR for controlling worst-case samples in the low-reward tail. Unlike prior approaches that rely on non-linear optimization, we leverage the variational dual formulation of CVaR to decompose it into a decoupled two-stage procedure: a lightweight one-dimensional threshold optimization step, and a single entropy-regularized fine-tuning process via a specific pseudo-reward. This decomposition achieves CVaR fine-tuning efficiently with computational cost comparable to standard expected fine-tuning methods. We demonstrate the effectiveness of TFFT across illustrative experiments, high-dimensional text-to-image generation, and molecular design.

</details>


### [105] [HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind](https://arxiv.org/abs/2602.16826)
*Nigel Doering,Rahath Malladi,Arshia Sangwan,David Danks,Tauhidur Rahman*

Main category: cs.LG

TL;DR: 本文提出HiVAE，一种层次化变分架构，旨在将心智理论（ToM）推理能力扩展至真实的时空领域。受人类信念-欲望-意图认知结构的启发，该模型采用三层VAE层次结构，在3,185个节点的校园导航任务中实现了显著性能提升，但存在潜在表示缺乏与实际心理状态显式对齐的关键局限性，作者提出自监督对齐策略并寻求社区反馈。


<details>
  <summary>Details</summary>
Motivation: 现有心智理论方法主要聚焦于小型、人类可理解的网格世界空间，无法有效扩展至复杂的现实时空域，限制了其在真实场景中的应用。本研究旨在突破这一瓶颈，开发能够处理大规模现实场景的ToM推理模型。

Method: 研究者提出HiVAE（层次化变分架构），一种受人类认知中信念-欲望-意图结构启发的三层变分自编码器层次结构。该架构通过层次化的潜在变量建模智能体的隐藏目标与心理状态，实现可扩展的心智理论推理。

Result: 在包含3,185个节点的真实校园导航任务上，HiVAE取得了显著的性能提升。然而，研究发现学习到的层次化潜在表示缺乏与真实心理状态的显式语义连接，这是该方法的关键限制。

Conclusion: 本研究成功将ToM推理扩展至大规模现实场景，但揭示了潜在表示缺乏显式语义定义的问题。作者提出采用自监督对齐策略来解决这一局限性，并将此项工作作为征集社区对表示grounding方法反馈的起点，期望共同推进该领域的发展。

Abstract: Theory of mind (ToM) enables AI systems to infer agents' hidden goals and mental states, but existing approaches focus mainly on small human understandable gridworld spaces. We introduce HiVAE, a hierarchical variational architecture that scales ToM reasoning to realistic spatiotemporal domains. Inspired by the belief-desire-intention structure of human cognition, our three-level VAE hierarchy achieves substantial performance improvements on a 3,185-node campus navigation task. However, we identify a critical limitation: while our hierarchical structure improves prediction, learned latent representations lack explicit grounding to actual mental states. We propose self-supervised alignment strategies and present this work to solicit community feedback on grounding approaches.

</details>


### [106] [VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study](https://arxiv.org/abs/2602.16833)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 该论文针对大语言模型强化学习后训练的探索瓶颈问题，提出语言化动作掩码(VAM)方法，通过将动作掩码嵌入提示词并强制模型从限定集合输出动作，结合迭代动作空间剪枝策略，在棋类游戏的保留谜题和完整对局中均显著提升了学习效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在强化学习后训练中面临探索效率低下的核心挑战：稀疏反馈信号和庞大动作空间易导致模型过早陷入重复行为而崩溃。传统探索方法难以适配LLM的文本生成范式，缺乏可控性，因此需要一种能充分利用LLM语言理解能力的探索机制。

Method: 提出语言化动作掩码(VAM)：将动作掩码以自然语言形式编码进提示词，约束模型输出必须来自掩码集合。基于此接口，引入迭代动作空间剪枝——若目标动作未被采样，则移除已采样的有效动作并重新采样，重复直至成功或预算耗尽。在棋类游戏中，通过引擎对弈生成状态和固定带验证分数数据集两种训练范式进行验证。

Result: 在保留的棋类谜题和完整对局评估中，以平均百分 pawn 损失(ACPL)为指标，VAM相比强基线显著提升了学习效率和最终性能，验证了该方法在可控探索方面的有效性。

Conclusion: 语言化动作掩码为LLM强化学习后训练提供了实用且有效的可控探索机制，通过自然语言接口引导探索行为，缓解了探索瓶颈问题，具有重要的实践价值。

Abstract: Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.

</details>


### [107] [A Residual-Aware Theory of Position Bias in Transformers](https://arxiv.org/abs/2602.16837)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: cs.LG

TL;DR: 该论文揭示了Transformer模型中的位置偏置现象，提出了残差连接感知的累积注意力传播理论，解释了因果掩码下注意力不会坍缩到首个令牌的原因，并证明了有限深度下会形成U型位置偏置。


<details>
  <summary>Details</summary>
Motivation: 现有理论分析认为在无限深度和因果掩码下，注意力会不可避免地坍缩到首个令牌，但这与实际情况不符；同时Transformer模型存在系统性的位置偏置现象，其架构起源尚未被充分理解。

Method: 通过引入残差连接因素，发展了一种残差感知的累积注意力传播理论，并针对有限深度情况进行了数学证明。

Result: 证明了在现实条件下残差连接能防止注意力坍缩，且在有限深度下因果Transformer会诱导出U型位置偏置，注意力集中在早期和晚期令牌上。

Conclusion: 该研究为"中间迷失"现象提供了基于架构原理的解释，表明位置偏置是Transformer架构固有的数学特性。

Abstract: Transformer models systematically favor certain token positions, yet the architectural origins of this position bias remain poorly understood. Under causal masking at infinite depth, prior theoretical analyses of attention rollout predict an inevitable collapse of attention onto the first token. Such collapse, however, does not occur in practice. We resolve this discrepancy with a residual-aware theory of cumulative attention rollout. By incorporating residual connections, we show that this architectural component prevents collapse under realistic conditions. At finite depth, we prove that causal Transformers induce a U-shaped position bias, with attention concentrating on early and late tokens. This result provides a principled architectural explanation for the Lost-in-the-Middle phenomenon.

</details>


### [108] [Training Large Reasoning Models Efficiently via Progressive Thought Encoding](https://arxiv.org/abs/2602.16839)
*Zeliang Zhang,Xiaodong Liu,Hao Cheng,Hao Sun,Chenliang Xu,Jianfeng Gao*

Main category: cs.LG

TL;DR: 提出渐进式思维编码(PTE)方法，通过将推理中间状态渐进压缩为固定维度向量，使大推理模型在固定缓存下保持高效，大幅降低训练内存并提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型面临强化学习训练效率瓶颈：长轨迹自回归解码占用大量时间和内存，而现有滑动窗口缓存策略虽限制内存却破坏长程推理能力并损害性能。

Method: 渐进式思维编码(PTE)采用参数高效微调，将推理过程的中间状态逐步编码为固定大小的向量表征，避免在完整缓存轨迹上的反向传播，实现推理时的恒定内存消耗。

Result: 在Qwen2.5-3B/7B-Instruct和DeepSeek-R1-Distill-Llama-8B三个模型上的六个数学基准测试显示，相比LoRA微调平均提升19.3%，相比基线模型提升29.9%，在AIME2024/2025上最高提升23.4个准确点。

Conclusion: PTE方法有效解决了长上下文推理的内存约束问题，在提升推理准确性的同时，使大推理模型的强化学习训练更加高效且可扩展。

Abstract: Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.

</details>


### [109] [ML-driven detection and reduction of ballast information in multi-modal datasets](https://arxiv.org/abs/2602.16876)
*Yaroslav Solovko*

Main category: cs.LG

TL;DR: 针对多模态数据中的冗余信息问题，提出统一检测框架，融合熵、互信息、Lasso、SHAP、PCA、主题模型和嵌入分析，创新性提出压舱石评分进行跨模态剪枝，可在不影响性能的前提下减少70%以上特征，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现代数据集包含不贡献分析价值的冗余信息（压舱石），导致维度膨胀、存储和计算成本增加，亟需通用化、跨模态的冗余信息识别与约简方法。

Method: 采用多种信号（熵、互信息、Lasso、SHAP、PCA、主题建模、嵌入分析）识别压舱石特征，并提出创新性的压舱石评分(Ballast Score)将这些信号整合为统一的跨模态剪枝策略。

Result: 实验表明在稀疏或半结构化数据中可剪枝超过70%的特征空间，分类性能基本保持或有所提升，同时训练时间和内存占用大幅降低；揭示统计型、语义型和基础设施型三种压舱石类型。

Conclusion: 该框架为构建精简高效的机器学习流水线提供实用指导，通过有效消除多类型数据中的冗余信息，实现计算效率与模型性能的双重优化。

Abstract: Modern datasets often contain ballast as redundant or low-utility information that increases dimensionality, storage requirements, and computational cost without contributing meaningful analytical value. This study introduces a generalized, multimodal framework for ballast detection and reduction across structured, semi-structured, unstructured, and sparse data types. Using diverse datasets, entropy, mutual information, Lasso, SHAP, PCA, topic modelling, and embedding analysis are applied to identify and eliminate ballast features. A novel Ballast Score is proposed to integrate these signals into a unified, cross-modal pruning strategy. Experimental results demonstrate that significant portions of the feature space as often exceeding 70% in sparse or semi-structured data, can be pruned with minimal or even improved classification performance, along with substantial reductions in training time and memory footprint. The framework reveals distinct ballast typologies (e.g. statistical, semantic, infrastructural), and offers practical guidance for leaner, more efficient machine learning pipelines.

</details>


### [110] [Construction of a classification model for dementia among Brazilian adults aged 50 and over](https://arxiv.org/abs/2602.16887)
*F. S. Menezes,M. C. F. G. Barretto,E. Q. C. Garcia,T. A. E. Ferreira,J. G. Alvez*

Main category: cs.LG

TL;DR: 本研究基于巴西老龄化纵向研究(ELSI-Brazil)的9,412名中老年参与者数据，开发了一个低成本、可干预变量的痴呆分类模型。研究采用随机森林和多元逻辑回归分析，发现文盲、高龄、低体重、低握力、黑人肤色、缺乏运动、听力损失和抑郁症状是主要风险因素，而高等教育、生活满意度和就业具有保护作用。随机森林模型表现最佳，AUC达0.776，为巴西初级保健中痴呆预防和资源分配提供了数据支持。


<details>
  <summary>Details</summary>
Motivation: 巴西人口老龄化加剧，痴呆患病率上升，但相关预测模型研究不足。本研究旨在利用低成本、可干预的变量构建痴呆分类模型，识别高风险人群，为巴西初级卫生保健系统提供有效的预防策略和资源优化配置依据。

Method: 采用观察性预测模型研究，横断面设计，使用ELSI-Brazil数据。通过神经心理学评估和知情者报告确定痴呆状态，运用随机森林和多元逻辑回归分析风险因素，并从ROC曲线下面积、敏感性、特异性、F1分数、G均值和准确率等指标评估模型性能。

Result: 痴呆患病率为9.6%。最大风险因素包括：90岁以上(OR=11.00)、文盲(OR=7.42)；其他风险因素包括低握力(OR=2.50)、低体重(OR=2.11)、黑人肤色(OR=1.47)、缺乏运动(OR=1.61)、听力损失(OR=1.65)和抑郁症状(OR=1.72)。保护因素为高等教育(OR=0.44)、生活满意度(OR=0.72)和就业(OR=0.78)。随机森林模型性能最优，AUC=0.776，敏感性0.708，特异性0.702。

Conclusion: 研究证实痴呆的多维度性质，识别出多个可干预的风险和保护因素。基于低成本变量的预测模型有助于在初级保健中识别脆弱人群，加强针对脑健康的公共政策可优化资源配置，对巴西痴呆预防具有重要意义。

Abstract: To build a dementia classification model for middle-aged and elderly Brazilians, implemented in Python, combining variable selection and multivariable analysis, using low-cost variables with modification potential. Observational study with a predictive modeling approach using a cross-sectional design, aimed at estimating the chances of developing dementia, using data from the Brazilian Longitudinal Study of Aging (ELSI-Brazil), involving 9,412 participants. Dementia was determined based on neuropsychological assessment and informant-based cognitive function. Analyses were performed using Random Forest (RF) and multivariable logistic regression to estimate the risk of dementia in the middle-aged and elderly populations of Brazil. The prevalence of dementia was 9.6%. The highest odds of dementia were observed in illiterate individuals (Odds Ratio (OR) = 7.42), individuals aged 90 years or older (OR = 11.00), low weight (OR = 2.11), low handgrip strength (OR = 2.50), self-reported black skin color (OR = 1.47), physical inactivity (OR = 1.61), self-reported hearing loss (OR = 1.65), and presence of depressive symptoms (OR = 1.72). Higher education (OR=0.44), greater life satisfaction (OR=0.72), and being employed (OR=0.78) were protective factors. The RF model outperformed logistic regression, achieving an area under the ROC curve of 0.776, with a sensitivity of 0.708, a specificity of 0.702, an F1-score of 0.311, a G-means of 0.705, and an accuracy of 0.703. Conclusion: The findings reinforce the multidimensional nature of dementia and the importance of accessible factors for identifying vulnerable individuals. Strengthening public policies focused on promoting brain health can contribute significantly to the efficient allocation of resources in primary care and dementia prevention in Brazil

</details>


### [111] [Beyond Message Passing: A Symbolic Alternative for Expressive and Interpretable Graph Learning](https://arxiv.org/abs/2602.16947)
*Chuqin Geng,Li Zhang,Haolin Ye,Ziyu Zhao,Yuhe Jiang,Tara Saba,Xinyu Wang,Xujie Si*

Main category: cs.LG

TL;DR: 提出SymGraph符号框架，用离散结构哈希和拓扑角色聚合替代连续消息传递，突破1-WL表达力限制，在CPU上实现10-100倍训练加速，同时生成细粒度可解释规则。


<details>
  <summary>Details</summary>
Motivation: GNN在高风险领域应用广泛但缺乏可解释性，现有自解释GNN受限于1-WL表达力瓶颈且缺乏细粒度解释能力，难以满足科学发现的可信需求。

Method: 设计SymGraph符号框架，用离散结构哈希和基于拓扑角色的消息聚合替代传统连续消息传递，避免可微优化开销，从理论上突破1-WL表达力限制。

Result: 在大量实验评估中达到SOTA性能，在CPU上训练速度提升10-100倍，生成的规则相比现有方法具有更优的语义粒度。

Conclusion: SymGraph为高风险领域提供了高表达力、高效率、细粒度的可解释图学习方法，具有推动科学发现和可信AI发展的巨大潜力。

Abstract: Graph Neural Networks (GNNs) have become essential in high-stakes domains such as drug discovery, yet their black-box nature remains a significant barrier to trustworthiness. While self-explainable GNNs attempt to bridge this gap, they often rely on standard message-passing backbones that inherit fundamental limitations, including the 1-Weisfeiler-Lehman (1-WL) expressivity barrier and a lack of fine-grained interpretability. To address these challenges, we propose SymGraph, a symbolic framework designed to transcend these constraints. By replacing continuous message passing with discrete structural hashing and topological role-based aggregation, our architecture theoretically surpasses the 1-WL barrier, achieving superior expressiveness without the overhead of differentiable optimization. Extensive empirical evaluations demonstrate that SymGraph achieves state-of-the-art performance, outperforming existing self-explainable GNNs. Notably, SymGraph delivers 10x to 100x speedups in training time using only CPU execution. Furthermore, SymGraph generates rules with superior semantic granularity compared to existing rule-based methods, offering great potential for scientific discovery and explainable AI.

</details>


### [112] [Multi-Agent Lipschitz Bandits](https://arxiv.org/abs/2602.16965)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 针对连续Lipschitz动作空间下的多玩家随机老虎机问题，本文提出了一种免通信的模块化协议。该协议通过极大值导向搜索识别并分配玩家至不同高价值区域以避免碰撞，进而将问题解耦为N个独立的单玩家Lipschitz老虎机，实现了接近最优的$\tilde{O}(T^{(d+1)/(d+2)})$遗憾界，且协调成本与时间范围$T$无关。


<details>
  <summary>Details</summary>
Motivation: 多玩家老虎机在实际系统中广泛存在（如无线信道分配），但连续动作空间和硬碰撞约束使问题极具挑战性。传统方法需要大量通信且协调成本随$T$增长，限制了可扩展性。本文旨在设计一种免通信策略，实现与$T$无关的协调成本，同时保持接近最优的学习效率。

Method: 提出两阶段模块化协议：1）协调阶段：采用极大值导向搜索探索动作空间，识别高价值区域并将玩家分配至不同区域以避免碰撞；2）学习阶段：将多玩家问题解耦为N个独立的单玩家Lipschitz老虎机问题，每个玩家在其分配区域内单独学习，无需持续通信。

Result: 理论分析表明，该协议达到$\tilde{O}(T^{(d+1)/(d+2)})$的遗憾界，并仅产生一个与$T$无关的固定协调成本。该遗憾界与单玩家Lipschitz老虎机的最优率相匹配，证明了在避免碰撞的同时保持了高效学习。框架还可推广至一般距离阈值碰撞模型。

Conclusion: 这是首个为连续Lipschitz空间多玩家老虎机提供免通信、常数协调成本且达到最优遗憾界的理论框架。该方法通过智能空间划分有效解决了协调与效率的权衡问题，为分布式学习系统提供了重要的理论保证和实践指导。

Abstract: We study the decentralized multi-player stochastic bandit problem over a continuous, Lipschitz-structured action space where hard collisions yield zero reward. Our objective is to design a communication-free policy that maximizes collective reward, with coordination costs that are independent of the time horizon $T$. We propose a modular protocol that first solves the multi-agent coordination problem -- identifying and seating players on distinct high-value regions via a novel maxima-directed search -- and then decouples the problem into $N$ independent single-player Lipschitz bandits. We establish a near-optimal regret bound of $\tilde{O}(T^{(d+1)/(d+2)})$ plus a $T$-independent coordination cost, matching the single-player rate. To our knowledge, this is the first framework providing such guarantees, and it extends to general distance-threshold collision models.

</details>


### [113] [A Unified Framework for Locality in Scalable MARL](https://arxiv.org/abs/2602.16966)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 本文突破传统环境中心视角，提出策略依赖的局部性原理，通过解耦策略诱导的相互依赖矩阵，揭示平滑策略可诱导局部性并导出更紧的指数衰减谱条件 ρ(E^s + E^a Π(π)) < 1，为可扩展多智能体强化学习提供新理论框架。


<details>
  <summary>Details</summary>
Motivation: 可扩展多智能体强化学习(MARL)受维度灾难困扰，现有基于指数衰减性质(EDP)的局部性方法因仅考虑环境最坏情况边界而过于保守，无法捕捉策略本身的正则化效应，限制了算法性能。

Method: 提出策略诱导相互依赖矩阵 H^π 的创新分解，将其分离为环境状态敏感性 E^s、环境动作敏感性 E^a 和策略状态敏感性 Π(π) 三部分，从而建立策略依赖的局部性分析框架。

Result: 推导出指数衰减的一般谱条件 ρ(E^s + E^a Π(π)) < 1，该条件严格优于现有范数条件；揭示了局部性与策略平滑性之间的根本权衡关系；给出了局部块坐标策略改进的严格理论保证。

Conclusion: 研究证明局部性是策略依赖现象，平滑策略可在强耦合环境中诱导局部性。该理论为设计高效可扩展MARL算法开辟了新途径，通过优化策略平滑性可实现维度灾难的有效缓解，具有重要的理论和实践意义。

Abstract: Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a \emph{policy-dependent} phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, $H^π$, which decouples the environment's sensitivity to state ($E^{\mathrm{s}}$) and action ($E^{\mathrm{a}}$) from the policy's sensitivity to state ($Π(π)$). This decomposition reveals that locality can be induced by a smooth policy (small $Π(π)$) even when the environment is strongly action-coupled, exposing a fundamental locality-optimality tradeoff. We use this framework to derive a general spectral condition $ρ(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) < 1$ for exponential decay, which is strictly tighter than prior norm-based conditions. Finally, we leverage this theory to analyze a provably-sound localized block-coordinate policy improvement framework with guarantees tied directly to this spectral radius.

</details>


### [114] [Fail-Closed Alignment for Large Language Models](https://arxiv.org/abs/2602.16977)
*Zachary Coalson,Beth Sohler,Aiden Gabriel,Sanghyun Hong*

Main category: cs.LG

TL;DR: 该论文揭示了当前大语言模型对齐的结构性缺陷——拒绝机制呈"失效开放"特性，单个主导特征被越狱提示抑制即可导致对齐崩溃。为此提出"失效封闭"对齐原则，通过渐进式框架迭代消融已学习的拒绝方向，迫使模型在独立子空间中重建多重安全机制。实验表明该方法在四次越狱攻击中鲁棒性最强，同时减轻过度拒绝并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全对齐依赖多个潜在特征编码拒绝行为，但存在"失效开放"的结构弱点：通过越狱提示抑制单个主导特征就会使整个对齐机制崩溃，产生不安全输出。这种脆弱性表明需要重新设计拒绝机制，使其具备在部分失效时仍能保持功能的能力。

Method: 提出"失效封闭"对齐设计原则，并实现渐进式对齐框架。该框架迭代地识别和消融模型先前学习的拒绝方向，强制模型在新的、独立的子空间中重建安全表征。通过创建冗余且因果独立的拒绝通路，确保即使部分通路被越狱攻击抑制，其他通路仍能维持安全约束。

Result: 在四种越狱攻击测试中，该方法实现了最强的整体鲁棒性。同时有效缓解了过度拒绝问题，保持了正常的生成质量，且仅引入较小的计算开销。机制分析证实，训练后的模型编码了多个因果独立的拒绝方向，单一越狱提示无法同时抑制所有方向。

Conclusion: 该研究验证了"失效封闭"对齐作为大语言模型安全设计原则的有效性。通过构建冗余且独立的拒绝机制，该方法为开发更鲁棒的LLM安全系统提供了实证支持，在提升抗越狱能力的同时平衡了安全性与可用性。

Abstract: We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.

</details>


### [115] [Discovering Universal Activation Directions for PII Leakage in Language Models](https://arxiv.org/abs/2602.16980)
*Leo Marchyok,Zachary Coalson,Sungho Keum,Sooel Son,Sanghyun Hong*

Main category: cs.LG

TL;DR: 本文提出UniLeak框架，通过识别语言模型残差流中的通用激活方向来机械性地解释PII泄露现象，这些方向在推理时线性添加可显著增加个人信息生成概率，且无需训练数据或真实PII标签。


<details>
  <summary>Details</summary>
Motivation: 现有研究对语言模型内部隐私敏感行为（如个人信息泄露）的表示机制知之甚少，缺乏理解其如何在高维权重空间中表征和调节的理论框架。

Method: UniLeak采用机制可解释性方法，通过分析模型自生成的文本，在残差流中识别模型特定的潜在方向，这些方向在跨上下文时能一致地放大PII生成倾向，且不影响生成质量。

Result: 在多模型和多数据集上，沿这些通用方向进行引导能显著提高PII泄露率，效果优于现有基于提示的提取方法，揭示了PII泄露本质上是模型表示中潜在信号的叠加。

Conclusion: 该研究为理解语言模型隐私风险提供了新视角，既可实现风险放大用于安全评估，也为后续风险缓解奠定了理论基础。

Abstract: Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.

</details>


### [116] [Dynamic Delayed Tree Expansion For Improved Multi-Path Speculative Decoding](https://arxiv.org/abs/2602.16994)
*Rahul Thomas,Teo Kitanovski,Micah Goldblum,Arka Pal*

Main category: cs.LG

TL;DR: 本文系统评估了多路径推测解码中的验证策略，发现遍历验证始终占优而基于最优传输的方法表现不佳。分析表明多token收益在草稿树深处更为关键。基于此，作者提出延迟树扩展和动态神经选择器，使基于最优传输的方法首次超越遍历验证，平均吞吐量提升5%。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种针对独立同分布展开的验证算法被提出，但它们在匹配设置下的相对性能仍不清楚，这促使作者进行系统性的评估和分析。

Method: 首先对多种验证策略在不同模型族、任务和采样机制下进行系统评估；分析基于最优传输方法表现不佳的原因；提出延迟树扩展方法，通过延后独立同分布分支点来提升性能；开发动态神经选择器，根据草稿和目标模型特征预估基于最优传输方法的预期块效率，实现上下文相关的扩展决策。

Result: 遍历验证在所有场景下均占主导地位，基于最优传输的方法落后较多；延迟树扩展能保持目标分布并改进根节点独立同分布展开；神经选择器使SpecInfer等基于最优传输的方法首次超越遍历验证，在各类模型、数据集和采样设置下平均吞吐量提升5%。

Conclusion: 通过延迟树扩展与动态神经选择器的结合，该研究成功解决了基于最优传输方法在多路径推测解码中的效率问题，实现了显著的性能提升。

Abstract: Multi-path speculative decoding accelerates lossless sampling from a target model by using a cheaper draft model to generate a draft tree of tokens, and then applies a verification algorithm that accepts a subset of these. While prior work has proposed various verification algorithms for i.i.d rollouts, their relative performance under matched settings remains unclear. In this work, we firstly present a systematic evaluation of verification strategies across model families, tasks, and sampling regimes, and find that Traversal Verification dominates consistently, with OT-based methods lagging far behind. Our analysis uncovers that this occurs because OT-based methods achieve high multi-token acceptance near the root of the draft tree, while multi-token gains are most impactful deeper in the draft tree, where draft and target distributions diverge. Based on this insight, we propose delayed tree expansion, which drafts a partial single path, delaying the i.i.d. branching point. We show that delayed tree expansion preserves the target distribution and improves on root-node i.i.d rollouts. Further, we develop a dynamic neural selector that estimates the expected block efficiency of optimal-transport-based verification methods from draft and target features, enabling context-dependent expansion decisions. Our neural selector allows OT-based methods like SpecInfer to outperform Traversal Verification for the first time, achieving 5% higher average throughput across a wide range of models, datasets, and sampling settings.

</details>


### [117] [Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17009)
*Nikunj Gupta,James Zachary Hare,Jesse Milzman,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 本文提出动作图策略（AGP），通过建模智能体动作依赖关系和构建协调上下文来提升多智能体强化学习的协调决策。理论证明AGP比独立策略表达力更强，且优于集中式价值分解的贪心执行；实验表明在部分可观测协调任务中成功率80-95%，远超基线方法的10-25%。


<details>
  <summary>Details</summary>
Motivation: 协调行动是多智能体强化学习中最基本的合作形式。成功的分散决策不仅需要优质个体动作，更依赖于跨智能体的动作兼容性以实现行为同步、避免冲突并满足全局约束。现有方法在捕获复杂动作依赖关系方面存在局限性。

Method: 提出动作图策略（AGP），对智能体可用动作选择间的依赖关系进行建模。核心是构建"协调上下文"，使智能体能够基于全局动作依赖进行条件决策，从而在分散执行中有效捕捉协调需求。

Result: 理论证明AGP相比完全独立策略具有严格更强的联合策略表达能力，能够实现比集中式价值分解方法贪心执行更优的协调联合动作。实验显示，在具有部分可观测性和反协调惩罚的典型任务上，AGP成功率达80-95%，而基线MARL方法仅为10-25%，且在多样环境中保持稳定优势。

Conclusion: AGP通过显式建模动作依赖关系和协调上下文，为多智能体协调决策提供了更强大的表示框架，有效解决了复杂协作场景中的动作兼容性问题，显著提升了学习性能。

Abstract: Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents' available action choices. It constructs, what we call, \textit{coordination contexts}, that enable agents to condition their decisions on global action dependencies. Theoretically, we show that AGPs induce a strictly more expressive joint policy compared to fully independent policies and can realize coordinated joint actions that are provably more optimal than greedy execution even from centralized value-decomposition methods. Empirically, we show that AGP achieves 80-95\% success on canonical coordination tasks with partial observability and anti-coordination penalties, where other MARL methods reach only 10-25\%. We further demonstrate that AGP consistently outperforms these baselines in diverse multi-agent environments.

</details>


### [118] [Malliavin Calculus as Stochastic Backpropogation](https://arxiv.org/abs/2602.17013)
*Kevin D. Oden*

Main category: cs.LG

TL;DR: 本文通过Malliavin分部积分恒等式建立了路径梯度（重参数化）与得分函数梯度（Malliavin）估计器之间的严格联系，并基于此提出了一种自适应混合估计器，能够结合两者的优势以达到最小方差。该框架为随机反向传播提供了理论理解，在VAE等任务上实现了显著的方差减少。


<details>
  <summary>Details</summary>
Motivation: 随机梯度估计在变分推断和强化学习等应用中至关重要，但现有方法（如重参数化梯度和得分函数梯度）存在方差高或计算开销大的问题。虽然混合方法已被提出，但缺乏理论指导。本文旨在建立这两种估计器的理论联系，并开发出具有最小方差保证的自适应混合策略。

Method: 利用Malliavin积分的分部积分恒等式，证明路径梯度和得分函数梯度本质上是该恒等式的两种特例。基于此等价性，引入一个方差感知的混合估计器，通过自适应地结合两种梯度并利用其经验协方差结构来最小化方差。该混合估计器在所有无偏线性组合中具有最小方差，并给出了闭式有限样本收敛界。

Result: 理论分析证明了混合估计器的最优性。实验结果显示：在CIFAR-10的VAE上实现9%的方差减少，在强耦合合成问题上达到35%的方差减少。然而，在非平稳优化的策略梯度实验中，混合方法面临挑战，这为未来研究指明了方向。

Conclusion: 本研究将Malliavin微积分确立为随机梯度估计的统一概念框架，为混合方法的实际效果提供了清晰的理论理解。工作既阐明了混合策略何时能提供实际效益，也指出了其内在局限性，特别是在非平稳优化场景中。

Abstract: We establish a rigorous connection between pathwise (reparameterization) and score-function (Malliavin) gradient estimators by showing that both arise from the Malliavin integration-by-parts identity. Building on this equivalence, we introduce a unified and variance-aware hybrid estimator that adaptively combines pathwise and Malliavin gradients using their empirical covariance structure. The resulting formulation provides a principled understanding of stochastic backpropagation and achieves minimum variance among all unbiased linear combinations, with closed-form finite-sample convergence bounds. We demonstrate 9% variance reduction on VAEs (CIFAR-10) and up to 35% on strongly-coupled synthetic problems. Exploratory policy gradient experiments reveal that non-stationary optimization landscapes present challenges for the hybrid approach, highlighting important directions for future work. Overall, this work positions Malliavin calculus as a conceptually unifying and practically interpretable framework for stochastic gradient estimation, clarifying when hybrid approaches provide tangible benefits and when they face inherent limitations.

</details>


### [119] [WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning](https://arxiv.org/abs/2602.17025)
*Gagan Mundada,Zihan Huang,Rohan Surana,Sheldon Yu,Jennifer Yuntong Zhang,Xintong Li,Tong Yu,Lina Yao,Jingbo Shang,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: 本文提出弱监督GRPO（WS-GRPO），通过将最终奖励转化为对部分轨迹的正确性感知指导，解决GRPO在复杂推理训练中的过度思考和效率低下问题，在保持准确性的同时显著减少推理长度。


<details>
  <summary>Details</summary>
Motivation: GRPO在复杂推理任务中有效，但其相对目标函数会导致过度延长的推理过程，造成效率低下。现有方法如长度惩罚难以校准（因长推理可能反映更难的问题），且缺乏直接的继续/停止监督信号，难以在正确性和推理效率间取得平衡。

Method: 提出WS-GRPO，通过仅利用最终答案正确性的弱监督方式训练偏好模型，为部分轨迹生成前缀级指导信号，指示何时继续推理有益，从而将终端奖励转化为 continue/stop 决策指导，减少冗余 deliberation。

Result: 理论分析和在推理基准测试上的实验表明，WS-GRPO在保持与GRPO基线相当性能的同时，显著减少了推理轨迹长度，提高了rollout效率。

Conclusion: WS-GRPO通过弱监督学习提供 outcome-derived 的 continue/stop 指导，有效缓解了GRPO的冗余推理问题，实现了推理效率与正确性的更好平衡，为控制语言模型推理长度提供了新思路。

Abstract: Group Relative Policy Optimization (GRPO) is effective for training language models on complex reasoning. However, since the objective is defined relative to a group of sampled trajectories, extended deliberation can create more chances to realize relative gains, leading to inefficient reasoning and overthinking, and complicating the trade-off between correctness and rollout efficiency. Controlling this behavior is difficult in practice, considering (i) Length penalties are hard to calibrate because longer rollouts may reflect harder problems that require longer reasoning, penalizing tokens risks truncating useful reasoning along with redundant continuation; and (ii) supervision that directly indicates when to continue or stop is typically unavailable beyond final answer correctness. We propose Weakly Supervised GRPO (WS-GRPO), which improves rollout efficiency by converting terminal rewards into correctness-aware guidance over partial trajectories. Unlike global length penalties that are hard to calibrate, WS-GRPO trains a preference model from outcome-only correctness to produce prefix-level signals that indicate when additional continuation is beneficial. Thus, WS-GRPO supplies outcome-derived continue/stop guidance, reducing redundant deliberation while maintaining accuracy. We provide theoretical results and empirically show on reasoning benchmarks that WS-GRPO substantially reduces rollout length while remaining competitive with GRPO baselines.

</details>


### [120] [Transforming Behavioral Neuroscience Discovery with In-Context Learning and AI-Enhanced Tensor Methods](https://arxiv.org/abs/2602.17027)
*Paimon Goulart,Jordan Steinhauser,Dawon Ahn,Kylene Shuler,Edward Korzus,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本文提出了一种AI增强的科研发现流水线，结合行为神经科学中的恐惧泛化小鼠研究，通过上下文学习(ICL)接口使领域专家无需AI训练即可自动化数据处理与模式解释，并改进了张量分解模型以发现异质数据中的模式，实验证明该方法优于领域标准实践和非ICL基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统科研发现流水线复杂、僵化且耗时，领域专家需耗费大量精力调试流水线或手动标注数据，而非专注于结果解释。AI有潜力改变这一现状，让专家专注于洞察发现本身。特别是在行为神经科学等需要处理异质数据的领域，缺乏无需编程即可使用的AI工具。

Method: 1) 采用上下文学习(ICL)作为人机接口，使领域专家无需了解AI模型训练或微调即可自动化部分流水线；2) 提出新颖的AI增强张量分解模型，用于从异质数据中无缝发现模式；3) 通过实验评估与领域标准实践及非ICL基线的对比，验证性能优势。

Result: 实验结果表明，所提出的AI增强流水线在性能上优于领域内的标准实践方法，同时也优于不使用ICL范式的合理机器学习基线，确保了在不损害性能的前提下为领域专家提供了无缝易用的界面。最终发现结果经团队领域专家验证有效。

Conclusion: 该研究展示了AI增强流水线在加速科研发现方面的巨大潜力，特别是ICL范式作为领域专家友好接口的有效性。改进的张量分解模型为处理异质数据提供了新思路，为未来在神经科学及其他领域的应用奠定了基础，使专家能更专注于科学洞察而非技术细节。

Abstract: Scientific discovery pipelines typically involve complex, rigid, and time-consuming processes, from data preparation to analyzing and interpreting findings. Recent advances in AI have the potential to transform such pipelines in a way that domain experts can focus on interpreting and understanding findings, rather than debugging rigid pipelines or manually annotating data. As part of an active collaboration between data science/AI researchers and behavioral neuroscientists, we showcase an example AI-enhanced pipeline, specifically designed to transform and accelerate the way that the domain experts in the team are able to gain insights out of experimental data. The application at hand is in the domain of behavioral neuroscience, studying fear generalization in mice, an important problem whose progress can advance our understanding of clinically significant and often debilitating conditions such as PTSD (Post-Traumatic Stress Disorder). We identify the emerging paradigm of "In-Context Learning" (ICL) as a suitable interface for domain experts to automate parts of their pipeline without the need for or familiarity with AI model training and fine-tuning, and showcase its remarkable efficacy in data preparation and pattern interpretation. Also, we introduce novel AI-enhancements to tensor decomposition model, which allows for more seamless pattern discovery from the heterogeneous data in our application. We thoroughly evaluate our proposed pipeline experimentally, showcasing its superior performance compared to what is standard practice in the domain, as well as against reasonable ML baselines that do not fall under the ICL paradigm, to ensure that we are not compromising performance in our quest for a seamless and easy-to-use interface for domain experts. Finally, we demonstrate effective discovery, with results validated by the domain experts in the team.

</details>


### [121] [Multi-Objective Alignment of Language Models for Personalized Psychotherapy](https://arxiv.org/abs/2602.16053)
*Mehrab Beikzadeh,Yasaman Asadollah Salmanpour,Ashima Suvarna,Sriram Sankararaman,Matteo Malgaroli,Majid Sarrafzadeh,Saadia Gabriel*

Main category: cs.LG

TL;DR: 针对全球超10亿心理健康障碍患者但护理资源有限的现状，本研究开发了多目标对齐框架MODPO，通过调查335名有心理健康经验的个体收集偏好数据，训练六个治疗维度的奖励模型。结果显示MODPO在共情(77.6%)和安全性(62.6%)之间取得更好平衡，优于单目标优化，且治疗标准比通用沟通原则表现高出17.2%。临床医生盲评证实MODPO更优，LLM评估者间信度与临床医生间信度相当。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康障碍患者超过10亿，但护理可及性受限于专业人才短缺和成本约束。现有AI治疗系统采用单目标对齐方法，无法在患者偏好与临床安全之间实现平衡优化，亟需开发能够综合考虑多个治疗维度的多目标对齐框架。

Method: 研究分为两个阶段：首先对335名有心理健康经验的个体进行调研，收集其在六个治疗维度（共情、安全性、积极倾听、自我激励改变、信任/默契、患者自主性）的偏好排序数据；随后开发基于直接偏好优化(DPO)的多目标对齐框架，训练六个维度的奖励模型，并与单目标优化、监督微调和参数合并等方法进行系统比较。

Result: 多目标DPO(MODPO)在共情(77.6%)与安全性(62.6%)之间达到更优平衡，而单目标优化虽实现93.6%的共情但安全性仅47.8%。治疗标准相比通用沟通原则性能提升17.2%。盲评中临床医生持续偏好MODPO，且LLM评估者与临床医生评估的一致性达到临床医生间信度水平。

Conclusion: 本研究验证了多目标对齐框架在AI心理健康治疗中的有效性，MODPO成功平衡了多个治疗目标，为开发更安全、更符合患者需求的AI治疗系统提供了可行路径，同时证明了将患者偏好与临床安全联合优化的重要性。

Abstract: Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.

</details>


### [122] [Forecasting Anomaly Precursors via Uncertainty-Aware Time-Series Ensembles](https://arxiv.org/abs/2602.17028)
*Hyeongwon Kang,Jinwoo Park,Seunghun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: ...


<details>
  <summary>Details</summary>
Motivation: ...

Method: ...

Result: ...

Conclusion: ...

Abstract: Detecting anomalies in time-series data is critical in domains such as industrial operations, finance, and cybersecurity, where early identification of abnormal patterns is essential for ensuring system reliability and enabling preventive maintenance. However, most existing methods are reactive: they detect anomalies only after they occur and lack the capability to provide proactive early warning signals. In this paper, we propose FATE (Forecasting Anomalies with Time-series Ensembles), a novel unsupervised framework for detecting Precursors-of-Anomaly (PoA) by quantifying predictive uncertainty from a diverse ensemble of time-series forecasting models. Unlike prior approaches that rely on reconstruction errors or require ground-truth labels, FATE anticipates future values and leverages ensemble disagreement to signal early signs of potential anomalies without access to target values at inference time. To rigorously evaluate PoA detection, we introduce Precursor Time-series Aware Precision and Recall (PTaPR), a new metric that extends the traditional Time-series Aware Precision and Recall (TaPR) by jointly assessing segment-level accuracy, within-segment coverage, and temporal promptness of early predictions. This enables a more holistic assessment of early warning capabilities that existing metrics overlook. Experiments on five real-world benchmark datasets show that FATE achieves an average improvement of 19.9 percentage points in PTaPR AUC and 20.02 percentage points in early detection F1 score, outperforming baselines while requiring no anomaly labels. These results demonstrate the effectiveness and practicality of FATE for real-time unsupervised early warning in complex time-series environments.

</details>


### [123] [Multi-Probe Zero Collision Hash (MPZCH): Mitigating Embedding Collisions and Enhancing Model Freshness in Large-Scale Recommenders](https://arxiv.org/abs/2602.17050)
*Ziliang Zhao,Bi Xue,Emma Lin,Mengjiao Zhou,Kaustubh Vartak,Shakhzod Ali-Zade,Carson Lu,Tao Li,Bin Kuang,Rui Jian,Bin Wen,Dennis van der Staay,Yixin Bao,Eddy Li,Chao Deng,Songbin Liu,Qifan Wang,Kai Ren*

Main category: cs.LG

TL;DR: 本文提出MPZCH（多探测零碰撞哈希），一种基于线性探测的新型嵌入表索引机制，用于解决大规模推荐系统中因ID数量膨胀导致的哈希碰撞问题。该方法通过辅助张量和CUDA内核实现可配置探测与主动淘汰策略，在保持生产效率的同时显著减少甚至消除碰撞，并防止陈旧嵌入继承，已在开源TorchRec库中发布。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统中唯一ID数量的不断增长，传统的哈希索引方法会出现碰撞问题，导致模型性能和个性化质量下降。此外，哈希碰撞还会导致陈旧嵌入继承现象，影响新特征的学习效果。因此，需要一种能够在大规模生产环境中有效缓解或消除碰撞的索引机制。

Method: 提出MPZCH（多探测零碰撞哈希），采用线性探测基础框架，利用辅助张量和定制的高性能CUDA内核实现可配置的探测策略和主动淘汰机制。通过淘汰过时ID并重置重新分配的槽位，避免陈旧嵌入的继承问题，确保新特征能够从头开始有效学习。

Result: 在合理的表大小配置下，MPZCH几乎可以完全消除碰撞，同时保持与现有方法相当的训练QPS和推理延迟。严格的在线实验表明，MPZCH实现了用户嵌入的零碰撞，并显著提升了项目嵌入的新鲜度和质量。

Conclusion: MPZCH是一种有效的嵌入碰撞缓解方案，能够在保持生产效率的前提下显著提升嵌入质量，已成功集成到开源TorchRec库中，为推荐系统社区提供了实用的技术解决方案。

Abstract: Embedding tables are critical components of large-scale recommendation systems, facilitating the efficient mapping of high-cardinality categorical features into dense vector representations. However, as the volume of unique IDs expands, traditional hash-based indexing methods suffer from collisions that degrade model performance and personalization quality. We present Multi-Probe Zero Collision Hash (MPZCH), a novel indexing mechanism based on linear probing that effectively mitigates embedding collisions. With reasonable table sizing, it often eliminates these collisions entirely while maintaining production-scale efficiency. MPZCH utilizes auxiliary tensors and high-performance CUDA kernels to implement configurable probing and active eviction policies. By retiring obsolete IDs and resetting reassigned slots, MPZCH prevents the stale embedding inheritance typical of hash-based methods, ensuring new features learn effectively from scratch. Despite its collision-mitigation overhead, the system maintains training QPS and inference latency comparable to existing methods. Rigorous online experiments demonstrate that MPZCH achieves zero collisions for user embeddings and significantly improves item embedding freshness and quality. The solution has been released within the open-source TorchRec library for the broader community.

</details>


### [124] [Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum](https://arxiv.org/abs/2602.17080)
*Minxin Zhang,Yuxuan Liu,Hayden Scheaffer*

Main category: cs.LG

TL;DR: 本文提出了两种新的优化器NAMO和NAMO-D，首次将正交动量与基于范数的Adam型噪声自适应机制相结合。NAMO使用单一自适应步长缩放正交动量，而NAMO-D通过带截断项的对角矩阵实现神经元级别的噪声自适应。理论证明两者在确定性设置下具有最优收敛率，在随机设置下能自适应梯度噪声水平。GPT-2预训练实验表明，两者均优于AdamW和Muon，其中NAMO-D通过额外截断超参数进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有优化器要么像Adam那样使用自适应矩估计来增强稳定性，要么像Muon那样利用权重矩阵结构通过正交化动量提升性能，但缺乏将二者有效结合的principled方法。本文旨在填补这一空白，实现正交动量与噪声自适应的首次系统性整合。

Method: 提出NAMO：使用单一自适应步长缩放正交化动量，保持正交性的同时提升性能；提出NAMO-D：将正交化动量右乘一个带截断项的对角矩阵，实现对角化扩展，支持神经元级别的噪声自适应。该方法利用了Hessian矩阵常见的近块对角结构特性。

Result: 理论结果：在标准假设下，两种算法在确定性设置下达到最优收敛率，在随机设置下其收敛保证能自适应随机梯度的噪声水平。实验结果：在GPT-2预训练上，NAMO和NAMO-D均优于AdamW和Muon基线，其中NAMO-D通过平衡更新方向条件性和细粒度噪声自适应的截断超参数，获得了比NAMO进一步的性能提升。

Conclusion: 本研究成功实现了正交动量与Adam型噪声自适应的首次原则性整合，提出的NAMO和NAMO-D优化器在理论和实验上均表现出色，为大规模语言模型训练提供了更优的优化算法选择。

Abstract: Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining a well-conditioned update direction and leveraging fine-grained noise adaptation.

</details>


### [125] [MeGU: Machine-Guided Unlearning with Target Feature Disentanglement](https://arxiv.org/abs/2602.17088)
*Haoyu Wang,Zhuo Huang,Xiaolong Wang,Bo Han,Zhiwei Lin,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文针对机器非学习中欠遗忘与过遗忘的权衡问题，提出Machine-Guided Unlearning (MeGU)框架。通过分析预训练模型中语义概念的纠缠特性，利用多模态大语言模型指导概念感知的重新对齐，并引入正负特征噪声对实现选择性遗忘，有效缓解了现有方法的根本性局限。


<details>
  <summary>Details</summary>
Motivation: 训练数据隐私问题催生了对"被遗忘权"的需求，机器非学习成为关键技术。然而现有方法存在根本性权衡：激进删除会损害模型在保留数据上的效用，保守删除则残留目标信息。深入分析揭示，预训练模型中语义类别概念在特征模式层面存在纠缠现象，这种纠缠从根本上限制了现有非学习范式的效果，亟需新的解决方案。

Method: 提出MeGU框架，通过概念感知的重新对齐指导非学习过程。核心包括：1）利用多模态大语言模型(MLLMs)为样本分配语义扰动标签，明确重新对齐方向；2）将MLLM估计的类间概念相似性编码为轻量级转移矩阵以提高效率；3）引入正负特征噪声对：负噪声抑制目标特定特征模式，正噪声强化保留的关联特征并使其对齐扰动概念。该设计实现对目标特定表示的选择性破坏，同时保持共享语义结构。

Result: MeGU实现了可控且选择性的遗忘，有效缓解了欠遗忘（目标信息未被完全删除）和过遗忘（模型在保留数据上的效用下降）问题，在机器非学习效果上取得显著提升。

Conclusion: 通过结合MLLM的概念理解能力与特征层面的噪声干预机制，MeGU为机器非学习提供了新范式。该方法在保护数据隐私与维持模型效用之间取得了更好平衡，为实际应用中的数据删除需求提供了理论依据和实践指导。

Abstract: The growing concern over training data privacy has elevated the "Right to be Forgotten" into a critical requirement, thereby raising the demand for effective Machine Unlearning. However, existing unlearning approaches commonly suffer from a fundamental trade-off: aggressively erasing the influence of target data often degrades model utility on retained data, while conservative strategies leave residual target information intact. In this work, the intrinsic representation properties learned during model pretraining are analyzed. It is demonstrated that semantic class concepts are entangled at the feature-pattern level, sharing associated features while preserving concept-specific discriminative components. This entanglement fundamentally limits the effectiveness of existing unlearning paradigms. Motivated by this insight, we propose Machine-Guided Unlearning (MeGU), a novel framework that guides unlearning through concept-aware re-alignment. Specifically, Multi-modal Large Language Models (MLLMs) are leveraged to explicitly determine re-alignment directions for target samples by assigning semantically meaningful perturbing labels. To improve efficiency, inter-class conceptual similarities estimated by the MLLM are encoded into a lightweight transition matrix. Furthermore, MeGU introduces a positive-negative feature noise pair to explicitly disentangle target concept influence. During finetuning, the negative noise suppresses target-specific feature patterns, while the positive noise reinforces remaining associated features and aligns them with perturbing concepts. This coordinated design enables selective disruption of target-specific representations while preserving shared semantic structures. As a result, MeGU enables controlled and selective forgetting, effectively mitigating both under-unlearning and over-unlearning.

</details>


### [126] [Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling](https://arxiv.org/abs/2602.17089)
*Xinghao Dong,Huchen Yang,Jin-long Wu*

Main category: cs.LG

TL;DR: 扩散模型质量高但采样慢，本文用低维潜空间流匹配实现单步采样，速度提升百倍，并通过正则化保持物理保真度和拓扑结构，减少数据需求。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成质量和样本多样性方面表现优异，但其采样速度是主要瓶颈。针对随机闭合模型的学习，需要快速采样方法，同时保持物理保真度和系统拓扑信息，并减少对大量训练数据的依赖。

Method: 通过在二维Kolmogorov流的数值算例上系统比较基于传输的生成模型，提出在低维潜空间中进行流匹配的方法，并比较联合训练隐式正则化与度量保持（MP）和几何感知（GA）两种显式正则化约束。

Result: 该方法实现了单步采样，速度比迭代式扩散方法快两个数量级（高达100倍）。隐式和显式正则化都能保持原始动力系统低维流形的关键拓扑信息，从而在不需要大量训练数据的情况下学习随机闭合模型。

Conclusion: 低维潜空间流匹配适用于随机闭合模型的快速采样，正则化策略能有效控制潜空间畸变并确保物理保真度，同时保留拓扑结构，显著降低了训练数据需求。

Abstract: Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints. Besides offering a faster sampling speed, both explicitly and implicitly regularized latent spaces inherit the key topological information from the lower-dimensional manifold of the original complex dynamical system, which enables the learning of stochastic closure models without demanding a huge amount of training data.

</details>


### [127] [FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment](https://arxiv.org/abs/2602.17095)
*Chuiyang Meng,Ming Tang,Vincent W. S. Wong*

Main category: cs.LG

TL;DR: 针对联邦LoRA微调中的矩阵聚合误差与分解漂移问题，提出FLoRG框架，采用单一低秩矩阵并聚合其格拉姆矩阵，结合普氏对齐策略提升性能。实验表明精度超越五种基线方法，通信开销最高降低2041倍。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景下，LoRA使用的双低秩矩阵结构存在两聚合难题：分别聚合引入误差，以及矩阵分解的非唯一性导致分解漂移，限制了联邦微调的精度和效率。

Method: 提出FLoRG：1）使用单一低秩矩阵替代双矩阵结构；2）聚合格拉姆矩阵消除聚合误差并降低通信开销；3）引入普氏对齐策略对齐连续轮次的分解矩阵以最小化漂移。理论证明普氏对齐能获得更紧的收敛界。

Result: 在多个大语言模型微调基准上，FLoRG的下游任务准确率显著优于五种先进基线，通信开销最高降低2041倍。

Conclusion: FLoRG通过单一矩阵结构和格拉姆矩阵聚合有效解决了联邦LoRA微调的核心挑战，普氏对齐确保更新一致性，为隐私保护下的大语言模型高效联邦微调提供了新方案。

Abstract: Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\times$.

</details>


### [128] [Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction](https://arxiv.org/abs/2602.17102)
*Sai Vineeth Kandappareddigari,Santhoshkumar Jagadish,Gauri Verma,Ilhuicamina Contreras,Christopher Dignam,Anmol Srivastava,Benjamin Demers*

Main category: cs.LG

TL;DR: 提出基于事件驱动和托管服务的无服务器MLOps框架，用于HS编码预测。Text-CNN模型达98%准确率，支持自动化A/B测试，成本效益优于Transformer，提供可复制的工业级ML生命周期管理方案。


<details>
  <summary>Details</summary>
Motivation: HS编码分类是国际贸易合规关键任务，错误导致货物延误和财务损失。短文本描述歧义大、更新频繁，传统方法难以满足可扩展性和成本效益要求，亟需自动化MLOps解决方案。

Method: 构建无服务器架构，通过事件驱动流水线orchestrate数据摄入、训练、部署、监控和重训练全流程。采用自定义文本嵌入编码器，对比Text-CNN、Transformer等架构，通过标准化接口实现模型无关性，集成自动化A/B测试和自动扩展机制。

Result: Text-CNN在真实HS编码数据集上实现98%准确率，满足可重复性、可审计性和SLA要求。框架在可变负载下自动扩展，运营成本显著低于Transformer方案，成功验证于工业场景，提供可复制的实施蓝图。

Conclusion: 该无服务器MLOps框架在HS编码预测中验证有效，平衡了准确性、延迟、可解释性与成本，为企业提供了可扩展、经济高效的ML工业化方案，架构支持未来集成Transformer和LLM变体。

Abstract: This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.

</details>


### [129] [Online Learning with Improving Agents: Multiclass, Budgeted Agents and Bandit Learners](https://arxiv.org/abs/2602.17103)
*Sajad Ashkezari,Shai Ben-David*

Main category: cs.LG

TL;DR: 本文研究允许智能体通过微调特征值获取更优标签的"带改进学习"模型，通过组合维度系统刻画其在线学习性，并拓展至多分类、老虎机反馈及成本建模等场景。


<details>
  <summary>Details</summary>
Motivation: 针对新兴的带改进学习模型，前人理论在扩展性、现实约束考量等方面存在局限。本研究旨在通过组合维度方法，系统性地填补多分类、不完整反馈及智能体成本建模等理论空白，构建更普适的理论框架。

Method: 采用组合学习理论中的维度分析技术，定义适用于改进学习场景的组合维度（如推广的Littlestone维度），分别针对标准在线学习、多分类、老虎机反馈及成本约束等设置建立相应的理论分析框架。

Result: 主要成果包括：1）建立改进学习模型的组合维度刻画，给出在线学习性的完整理论描述；2）将结果推广至多分类场景，推导多分类学习性边界；3）在老虎机反馈机制下建立学习性判定准则；4）引入智能体改进成本模型，分析成本对学习性能的影响。

Conclusion: 本研究通过组合维度方法，系统性地拓展了改进学习模型的理论边界，为不同应用场景下的学习性分析提供了统一框架，为相关算法设计奠定了理论基础。

Abstract: We investigate the recently introduced model of learning with improvements, where agents are allowed to make small changes to their feature values to be warranted a more desirable label. We extensively extend previously published results by providing combinatorial dimensions that characterize online learnability in this model, by analyzing the multiclass setup, learnability in a bandit feedback setup, modeling agents' cost for making improvements and more.

</details>


### [130] [i-PhysGaussian: Implicit Physical Simulation for 3D Gaussian Splatting](https://arxiv.org/abs/2602.17117)
*Yicheng Cao,Zhuo Huang,Yu Yao,Yiming Ying,Daoyi Dong,Tongliang Liu*

Main category: cs.LG

TL;DR: i-PhysGaussian框架创新性地将3D高斯溅射（3DGS）与隐式物质点法（MPM）积分器耦合，通过隐式牛顿型优化结合GMRES求解器最小化动量平衡残差，相比传统显式方法可在20倍大时间步长下保持稳定，有效解决了高刚度材料与准静态运动等复杂场景的精度退化问题。


<details>
  <summary>Details</summary>
Motivation: 物理仿真通过材料属性与外部载荷预测物体未来状态，是工业与工程风险管理的重要工具。然而，现有基于3D重建的模拟器多采用显式逐步更新策略，存在时间步长敏感性强、在高刚度材料或准静态运动等复杂场景下精度快速衰减的固有缺陷，亟需改进。

Method: 本文提出i-PhysGaussian框架，其核心在于将3D高斯溅射（3DGS）与隐式物质点法（MPM）积分器有机结合。区别于显式方法，该框架通过隐式牛顿型优化算法配合GMRES线性求解器，以最小化动量平衡残差的方式求解步末状态，从而确保物理一致性并降低时间步长敏感性。

Result: 实验结果表明，i-PhysGaussian将时间步长敏感性显著降低，物理一致性得到保证。相比显式基线方法，该框架可在高达20倍的时间步长下维持数值稳定性，在复杂动态过渡过程中仍能保持结构完整性与运动平滑性。

Conclusion: i-PhysGaussian成功克服了显式积分方法在复杂物理场景中的局限性，为高刚度材料与准静态运动等挑战性仿真问题提供了稳定且精确的解决方案，在工业与工程风险管理中具有广阔应用前景。

Abstract: Physical simulation predicts future states of objects based on material properties and external loads, enabling blueprints for both Industry and Engineering to conduct risk management. Current 3D reconstruction-based simulators typically rely on explicit, step-wise updates, which are sensitive to step time and suffer from rapid accuracy degradation under complicated scenarios, such as high-stiffness materials or quasi-static movement. To address this, we introduce i-PhysGaussian, a framework that couples 3D Gaussian Splatting (3DGS) with an implicit Material Point Method (MPM) integrator. Unlike explicit methods, our solution obtains an end-of-step state by minimizing a momentum-balance residual through implicit Newton-type optimization with a GMRES solver. This formulation significantly reduces time-step sensitivity and ensures physical consistency. Our results demonstrate that i-PhysGaussian maintains stability at up to 20x larger time steps than explicit baselines, preserving structural coherence and smooth motion even in complex dynamic transitions.

</details>


### [131] [TIFO: Time-Invariant Frequency Operator for Stationarity-Aware Representation Learning in Time Series](https://arxiv.org/abs/2602.17122)
*Xihao Piao,Zheng Chen,Lingwei Zhu,Yushun Dong,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 针对非平稳时间序列预测中的分布偏移问题，本文提出一种时间不变频率算子(TIFO)，通过在频域学习全局平稳性感知权重来抑制非平稳分量，可即插即用集成到各类预测模型中，在28个预测设置中取得24个顶尖结果，并在ETTm2数据集上实现33.3%和55.3%的平均MSE提升，同时降低60-70%计算成本。


<details>
  <summary>Details</summary>
Motivation: 非平稳时间序列预测面临训练与测试数据分布不同的核心挑战，现有方法通过去除单个样本的低阶矩来缓解依赖性，但未能捕捉样本间时变结构且无法建模复杂时间模式，导致预测性能受限。

Method: 提出时间不变频率算子(TIFO)，从整个数据集频域角度学习平稳性感知权重，突出平稳频率成分并抑制非平稳成分。该方法基于傅里叶变换隐式诱导频域特征分解的理论洞察，可作为即插即用模块无缝集成到各类预测模型中。

Result: 在28个预测设置中获得18个第一和6个第二的顶尖结果；在ETTm2数据集上平均MSE分别提升33.3%和55.3%；相比基线方法计算成本降低60-70%，展现出强大的可扩展性和效率优势。

Conclusion: TIFO通过频域全局建模有效缓解了分布偏移问题，不仅显著提升预测精度，还大幅降低计算开销，为各种时间序列预测模型提供了高效且通用的解决方案。

Abstract: Nonstationary time series forecasting suffers from the distribution shift issue due to the different distributions that produce the training and test data. Existing methods attempt to alleviate the dependence by, e.g., removing low-order moments from each individual sample. These solutions fail to capture the underlying time-evolving structure across samples and do not model the complex time structure. In this paper, we aim to address the distribution shift in the frequency space by considering all possible time structures. To this end, we propose a Time-Invariant Frequency Operator (TIFO), which learns stationarity-aware weights over the frequency spectrum across the entire dataset. The weight representation highlights stationary frequency components while suppressing non-stationary ones, thereby mitigating the distribution shift issue in time series. To justify our method, we show that the Fourier transform of time series data implicitly induces eigen-decomposition in the frequency space. TIFO is a plug-and-play approach that can be seamlessly integrated into various forecasting models. Experiments demonstrate our method achieves 18 top-1 and 6 top-2 results out of 28 forecasting settings. Notably, it yields 33.3% and 55.3% improvements in average MSE on the ETTm2 dataset. In addition, TIFO reduces computational costs by 60% -70% compared to baseline methods, demonstrating strong scalability across diverse forecasting models.

</details>


### [132] [VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation](https://arxiv.org/abs/2602.17133)
*Linwei Zhai,Han Ding,Mingzhi Lin,Cui Zhao,Fei Wang,Ge Wang,Wang Zhi,Wei Xi*

Main category: cs.LG

TL;DR: 本文提出VP-VAE（向量扰动变分自编码器），通过用Metropolis-Hastings采样生成的分布一致且尺度自适应的潜在扰动替代非可微量化器，将表示学习与离散化解耦，避免了码本崩溃问题。在均匀潜在变量假设下推导出轻量级变体FSP，统一了FSQ类固定量化器的理论与实现。图像和音频实验表明，该方法提高了重建保真度和token使用均衡性，同时训练更稳定。


<details>
  <summary>Details</summary>
Motivation: VQ-VAE由于表示学习与离散码本优化的内在耦合，常面临训练不稳定和"码本崩溃"问题。这种耦合使得模型难以同时学习良好的连续表示和有效的离散码本，导致码本利用率低下和训练过程不稳定。

Method: 提出VP-VAE范式：1）核心思想是将量化视为在潜在空间注入结构化扰动；2）用通过Metropolis-Hastings采样生成的分布一致、尺度自适应的扰动替代传统量化器；3）消除训练时对显式码本的依赖。进一步在均匀潜在变量假设下，推导出轻量级变体FSP（有限标量扰动），为FSQ类固定量化器提供统一理论解释和实践改进。

Result: 在图像和音频基准测试上的广泛实验表明：1）VP-VAE和FSP显著提高了重建保真度；2）实现了更均衡的token使用（码本利用率更高）；3）成功避免了耦合码本训练固有的不稳定性；4）模型对推理时量化误差具有鲁棒性。

Conclusion: 通过将表示学习与离散化解耦，VP-VAE提供了一种无需码本即可实现稳定训练的新范式，而FSP作为其轻量级变体，为固定量化器提供了理论基础和实践优化。该方法有效解决了VQ-VAE的码本崩溃问题，在保持生成质量的同时显著提升了训练稳定性和码本利用效率。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental to modern generative modeling, yet they often suffer from training instability and "codebook collapse" due to the inherent coupling of representation learning and discrete codebook optimization. In this paper, we propose VP-VAE (Vector Perturbation VAE), a novel paradigm that decouples representation learning from discretization by eliminating the need for an explicit codebook during training. Our key insight is that, from the neural network's viewpoint, performing quantization primarily manifests as injecting a structured perturbation in latent space. Accordingly, VP-VAE replaces the non-differentiable quantizer with distribution-consistent and scale-adaptive latent perturbations generated via Metropolis--Hastings sampling. This design enables stable training without a codebook while making the model robust to inference-time quantization error. Moreover, under the assumption of approximately uniform latent variables, we derive FSP (Finite Scalar Perturbation), a lightweight variant of VP-VAE that provides a unified theoretical explanation and a practical improvement for FSQ-style fixed quantizers. Extensive experiments on image and audio benchmarks demonstrate that VP-VAE and FSP improve reconstruction fidelity and achieve substantially more balanced token usage, while avoiding the instability inherent to coupled codebook training.

</details>


### [133] [The Anxiety of Influence: Bloom Filters in Transformer Attention Heads](https://arxiv.org/abs/2602.17526)
*Peter Balogh*

Main category: cs.LG

TL;DR: 该研究发现Transformer注意力头中存在专门用于检测词元是否在上下文中出现过的"成员测试"机制。在GPT-2和Pythia模型中识别出四种不同类型的成员测试头，包括两个高精度过滤器和一个经典Bloom过滤器，它们构成一个早期层的多分辨率系统，且功能超出简单的重复词元检测。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer注意力头的具体功能机制，特别是它们如何处理重复信息和上下文记忆，对于揭示语言模型的内部工作原理至关重要。成员测试能力可能解释模型如何高效地追踪和识别重复出现的词元，这对改进模型架构和解释性具有重要意义。

Method: 研究者在四个语言模型（GPT-2 small/medium/large和Pythia-160M）中系统性地识别和分析了注意力头的成员测试行为。通过控制混淆因素、测试泛化能力和消融实验，区分了真正的成员测试头与伪影，并量化了其性能特征（如假阳性率、容量曲线等）。

Result: 1) 识别出三个真正的成员测试头：L0H1和L0H5为高精度过滤器（假阳性率0-4%），L1H11符合经典Bloom过滤器模型（容量约5位）；2) L3H0被重新分类为前缀注意力头，因其容量曲线是序列长度伪影；3) 这些头集中在早期层（0-1），与归纳和前序词元头功能不同；4) 假阳性率随嵌入距离单调衰减；5) 成员测试头对任意重复词元类型都有响应，泛化能力比仅检测重复词元的头高43%；6) 消融实验表明这些头同时参与重复和新词元处理。

Conclusion: Transformer模型中存在专门化的成员测试注意力头，它们构成多分辨率系统，采用不同策略高效检测上下文重复。这些头不仅是简单的重复检测器，还承担更广泛的计算角色。通过严格的混淆控制重新分类伪影反而增强了结论的可信度。这些发现深化了对注意力机制功能特化的理解，为模型解释性提供了新视角。

Abstract: Some transformer attention heads appear to function as membership testers, dedicating themselves to answering the question "has this token appeared before in the context?" We identify these heads across four language models (GPT-2 small, medium, and large; Pythia-160M) and show that they form a spectrum of membership-testing strategies. Two heads (L0H1 and L0H5 in GPT-2 small) function as high-precision membership filters with false positive rates of 0-4\% even at 180 unique context tokens -- well above the $d_\text{head} = 64$ bit capacity of a classical Bloom filter. A third head (L1H11) shows the classic Bloom filter capacity curve: its false positive rate follows the theoretical formula $p \approx (1 - e^{-kn/m})^k$ with $R^2 = 1.0$ and fitted capacity $m \approx 5$ bits, saturating by $n \approx 20$ unique tokens. A fourth head initially identified as a Bloom filter (L3H0) was reclassified as a general prefix-attention head after confound controls revealed its apparent capacity curve was a sequence-length artifact. Together, the three genuine membership-testing heads form a multi-resolution system concentrated in early layers (0-1), taxonomically distinct from induction and previous-token heads, with false positive rates that decay monotonically with embedding distance -- consistent with distance-sensitive Bloom filters. These heads generalize broadly: they respond to any repeated token type, not just repeated names, with 43\% higher generalization than duplicate-token-only heads. Ablation reveals these heads contribute to both repeated and novel token processing, indicating that membership testing coexists with broader computational roles. The reclassification of L3H0 through confound controls strengthens rather than weakens the case: the surviving heads withstand the scrutiny that eliminated a false positive in our own analysis.

</details>


### [134] [TimeOmni-VL: Unified Models for Time Series Understanding and Generation](https://arxiv.org/abs/2602.17149)
*Tong Guan,Sheng Pan,Johan Barthelemy,Zhao Li,Yujun Cai,Cesare Alippi,Ming Jin,Shirui Pan*

Main category: cs.LG

TL;DR: TimeOmni-VL是首个视觉中心的多模态框架，通过双向时间序列-图像映射和理解引导的生成机制，统一了时间序列的理解与生成任务，在保持数值保真度的同时显著提升了语义理解和数值精度。


<details>
  <summary>Details</summary>
Motivation: 时间序列建模在数值生成和语义理解之间存在明显鸿沟：生成模型依赖表层模式匹配，而理解模型难以输出高保真数值。尽管视觉领域的多模态统一模型已取得进展，但这一潜力在时间序列领域尚未被挖掘。

Method: 提出TimeOmni-VL框架，包含两大创新：(1) 保fidelity的双向时间序列-图像映射（Bi-TSI），实现近乎无损的相互转换；(2) 理解引导的生成机制。构建TSUMM-Suite数据集（含6个理解任务+2个生成任务），通过校准的思维链（Chain-of-Thought）将时间序列理解作为显式控制信号来指导高保真生成。

Result: 实验证实，这种统一方法显著提升了语义理解能力和数值精度，在多个任务上取得显著改进。

Conclusion: 该工作建立了多模态时间序列建模的新前沿，首次实现了时间序列理解与生成的有效统一，为未来研究开辟了新方向。

Abstract: Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output. Although unified multimodal models (UMMs) have bridged this gap in vision, their potential for time series remains untapped. We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. With a calibrated Chain-of-Thought, TimeOmni-VL is the first to leverage time series understanding as an explicit control signal for high-fidelity generation. Experiments confirm that this unified approach significantly improves both semantic understanding and numerical precision, establishing a new frontier for multimodal time series modeling.

</details>


### [135] [Powering Up Zeroth-Order Training via Subspace Gradient Orthogonalization](https://arxiv.org/abs/2602.17155)
*Yicheng Lang,Changsheng Wang,Yihua Zhang,Mingyi Hong,Zheng Zhang,Wotao Yin,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为ZO-Muon的零阶优化方法，通过统一基于子空间的投影视图和Muon风格的谱优化来解决零阶优化中准确性与查询效率之间的根本矛盾。该方法在大型语言模型和视觉变换器上表现优异，显著减少了查询次数并提高了准确率。


<details>
  <summary>Details</summary>
Motivation: 零阶优化虽然避免了反向传播，节省了内存，但在实际应用中存在准确性与查询效率之间的根本矛盾。传统的零阶方法需要大量函数查询来获得准确的梯度估计，这限制了其在大规模模型微调中的应用效率。

Method: 本文提出一个统一的子空间梯度正交化框架，结合了两个互补原则：（i）基于投影的子空间视图，利用模型更新的内在低秩结构来降低梯度估计方差；（ii）Muon风格的谱优化，通过对噪声零阶梯度进行正交化来提取信息丰富的谱结构。基于此框架，作者提出了ZO-Muon方法，可自然解释为零阶设置下的低秩Muon优化器。

Result: 在大型语言模型和视觉变换器上的广泛实验表明，ZO-Muon显著加速了收敛速度，并在准确性和查询/时间效率方面实现了双赢改进。具体而言，对于SST-2任务上的LLM微调，ZO-Muon仅需MeZO基准24.7%的查询次数即可达到相同性能；在CIFAR-100数据集上的ViT-B微调中，准确率提升了25.1%。

Conclusion: 通过子空间梯度正交化框架，ZO-Muon成功解决了零阶优化的核心效率问题，为大规模模型的内存高效微调提供了强大替代方案，展示了零阶优化在实践中的巨大潜力。

Abstract: Zeroth-order (ZO) optimization provides a gradient-free alternative to first-order (FO) methods by estimating gradients via finite differences of function evaluations, and has recently emerged as a memory-efficient paradigm for fine-tuning large-scale models by avoiding backpropagation. However, ZO optimization has a fundamental tension between accuracy and query efficiency. In this work, we show that ZO optimization can be substantially improved by unifying two complementary principles: (i) a projection-based subspace view that reduces gradient estimation variance by exploiting the intrinsic low-rank structure of model updates, and (ii) Muon-style spectral optimization that applies gradient orthogonalization to extract informative spectral structure from noisy ZO gradients. These findings form a unified framework of subspace gradient orthogonalization, which we instantiate in a new method, ZO-Muon, admitting a natural interpretation as a low-rank Muon optimizer in the ZO setting. Extensive experiments on large language models (LLMs) and vision transformers (ViTs) demonstrate that ZO-Muon significantly accelerates convergence and achieves a win-win improvement in accuracy and query/runtime efficiency. Notably, compared to the popular MeZO baseline, ZO-Muon requires only 24.7% of the queries to reach the same SST-2 performance for LLM fine-tuning, and improves accuracy by 25.1% on ViT-B fine-tuning on CIFAR-100.

</details>


### [136] [In-Context Learning in Linear vs. Quadratic Attention Models: An Empirical Study on Regression Tasks](https://arxiv.org/abs/2602.17171)
*Ayush Goel,Arjun Kohli,Sarvagya Somvanshi*

Main category: cs.LG

TL;DR: 本文通过实证研究对比标准Transformer（二次注意力）与线性注意力模型在经典线性回归任务上的上下文学习表现，评估学习质量、收敛性及泛化行为，并探究模型深度对性能的影响，揭示两种机制的异同与局限。


<details>
  <summary>Details</summary>
Motivation: 虽然现有工作已证明Transformer和线性注意力模型可在简单函数类（如线性回归）上实现上下文学习，但两种注意力机制在ICL行为上的具体差异仍缺乏系统研究。本文旨在通过实证方法，在Garg等人的标准线性回归任务上，深入比较二次注意力与线性注意力的学习特性，以阐明各自的优劣与适用范围。

Method: 在Garg等人提出的标准线性回归任务框架下，采用实证研究方法定量评估两种注意力架构的上下文学习表现。主要考察指标包括均方误差（MSE）、收敛速度以及泛化能力，并系统分析模型深度变化对上下文学习性能的具体影响。

Result: 实验结果揭示了在线性回归这一特定任务设置下，线性注意力与二次注意力在上下文学习中既有共性也存在显著差异。线性注意力展现出某些优势，但同时暴露出明显的局限性。此外，增加模型深度对两种架构的ICL性能产生不同影响，表现出架构依赖的特性。

Conclusion: 本研究系统阐明了标准Transformer与线性注意力模型在上下文学习机制上的本质区别，为理解不同注意力架构的学习动态提供了经验证据。这些发现对设计更高效的注意力模型、优化上下文学习性能具有重要指导价值，特别是在计算资源受限的应用场景中。

Abstract: Recent work has demonstrated that transformers and linear attention models can perform in-context learning (ICL) on simple function classes, such as linear regression. In this paper, we empirically study how these two attention mechanisms differ in their ICL behavior on the canonical linear-regression task of Garg et al. We evaluate learning quality (MSE), convergence, and generalization behavior of each architecture. We also analyze how increasing model depth affects ICL performance. Our results illustrate both the similarities and limitations of linear attention relative to quadratic attention in this setting.

</details>


### [137] [Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting](https://arxiv.org/abs/2602.17645)
*Xiaohan Zhao,Zhaoyi Li,Yaxin Luo,Jiacheng Cui,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 针对大型视觉-语言模型的黑盒对抗攻击，现有M-Attack方法因ViT平移敏感性和结构不对称导致梯度方差高、优化不稳定。本研究提出M-Attack-V2，通过多视角对齐、辅助目标对齐、Patch Momentum和增强的patch-size集成等梯度去噪模块，显著提升了攻击迁移性和成功率。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型的黑盒对抗攻击面临缺失梯度和多模态边界复杂的根本挑战。虽然M-Attack等基于迁移的局部裁剪匹配方法表现良好，但会产生高方差、近似正交的梯度，违反局部对齐一致性并导致优化不稳定，亟需改进。

Method: 提出四个核心模块：1) 多视角对齐(MCA)：每轮迭代平均多个独立采样的局部视图梯度以降低方差；2) 辅助目标对齐(ATA)：用来自语义相关分布的辅助集替代激进增强，构建更平滑的目标流形；3) Patch Momentum：重释动量为历史裁剪梯度重放；4) 增强的patch-size集成(PE+)强化可迁移方向。

Result: 攻击成功率大幅提升：Claude-4.0从8%提升至30%，Gemini-2.5-Pro从83%提升至97%，GPT-5从98%提升至100%，显著超越现有黑盒LVLM攻击方法。

Conclusion: M-Attack-V2通过梯度去噪和多模块协同优化，有效解决了LVLM黑盒攻击中的梯度方差和不稳定问题，大幅提升了攻击的迁移性和成功率，证明了简单模块化增强在对抗攻击中的强大潜力。

Abstract: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.

</details>


### [138] [Continual uncertainty learning](https://arxiv.org/abs/2602.17174)
*Heisei Yonezawa,Ansei Yonezawa,Itsuro Kajiwara*

Main category: cs.LG

TL;DR: 针对机械系统多不确定性鲁棒控制问题，提出课程学习持续学习框架，通过分解为序列任务、联合模型预测控制器加速收敛，应用于汽车动力总成振动控制，实现稳健的sim-to-real迁移。


<details>
  <summary>Details</summary>
Motivation: 机械系统多不确定性鲁棒控制面临非线性动力学与工况变化交织的挑战；现有DRL与域随机化方法同时处理所有不确定性会导致策略次优且学习效率低下。

Method: 构建课程学习框架，将多不确定性控制问题分解为序列任务，逐步扩展系统不确定性集合；通过持续学习稳定更新策略避免灾难性遗忘；引入模型预测控制器作为共享基线加速收敛；采用残差学习实现任务特异性优化。

Result: 在汽车动力总成主动振动控制应用中，所设计控制器对结构非线性和动态变化表现出强鲁棒性，验证了sim-to-real迁移的成功性。

Conclusion: 该方法通过任务分解与模型基线结合，有效提升了多不确定性机械系统鲁棒控制的性能和学习效率，为工业应用提供了可靠解决方案。

Abstract: Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting. To ensure learning efficiency, we jointly incorporate a model-based controller (MBC), which guarantees a shared baseline performance across the plant sets, into the learning process to accelerate the convergence. This residual learning scheme facilitates task-specific optimization of the DRL agent for each uncertainty, thereby enhancing sample efficiency. As a practical industrial application, this study applies the proposed method to designing an active vibration controller for automotive powertrains. We verified that the resulting controller is robust against structural nonlinearities and dynamic variations, realizing successful sim-to-real transfer.

</details>


### [139] [SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch](https://arxiv.org/abs/2602.17206)
*Ron Shapira Weber,Oren Freifeld*

Main category: cs.LG

TL;DR: 本文介绍softdtw-cuda-torch，一个开源PyTorch库，用于GPU加速Soft动态时间规整计算。通过瓦片化反对角线核执行、对数空间反向传播和融合距离计算三项技术，解决了序列长度限制、数值不稳定和内存消耗大的问题，实现最高98%的内存减少。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速的SoftDTW实现存在三个关键限制：1) 序列长度被限制在1024以内；2) 小平滑参数时反向传播数值不稳定；3) 生成两两距离张量导致GPU内存消耗过大。这些限制严重制约了长序列和大规模批处理场景下的应用。

Method: 提出了三项创新技术：(1) 瓦片化反对角线核执行策略，消除序列长度约束；(2) 对数域反向传播算法，防止浮点数溢出；(3) 融合距离计算模式，消除O(BNM)中间距离张量。

Result: 相比先前工作实现高达98%的内存减少，支持任意序列长度，完整PyTorch自动微分集成，并提供Soft-DTW重心计算功能。

Conclusion: 该开源库通过算法创新有效克服了SoftDTW GPU实现的计算瓶颈，为时间序列分析提供了高效、稳定且内存友好的解决方案。

Abstract: We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTorch autograd integration, and Soft-DTW Barycenter computation. Code is available at https://github.com/BGU-CS-VIL/sdtw-cuda-torch.

</details>


### [140] [Structured Prototype-Guided Adaptation for EEG Foundation Models](https://arxiv.org/abs/2602.17251)
*Jingying Ma,Feng Wu,Yucheng Xing,Qika Lin,Tianyu Liu,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: 本文针对脑电基础模型在有限监督下泛化能力差的问题，提出了SCOPE框架，通过几何正则化原型构建置信度感知伪标签，并利用轻量级适配器进行微调，在跨被试标签受限场景下实现高效稳定性能。


<details>
  <summary>Details</summary>
Motivation: 脑电基础模型在全量微调时表现优异，但在真实临床场景中因受试者级别标注数据有限而泛化能力不佳。这种失败源于噪声且有限的监督信号与模型高可塑参数空间之间的结构失配，而非单纯监督不足。

Method: 提出两阶段框架SCOPE：第一阶段学习几何正则化任务先验，构建类别级平衡原型，并生成置信度感知伪标签过滤不可靠信号；第二阶段引入ProAdapter，通过结构原型条件的轻量级适配器对冻结的脑电基础模型进行适配。

Result: 在三个脑电任务和五个基础模型骨干上的实验表明，SCOPE在标签受限的跨被试设定下持续实现强性能和高效性。

Conclusion: SCOPE通过构建可靠外部监督和原型条件适配，有效解决了脑电基础模型在有限监督下的泛化问题，为临床场景下的实际部署提供了可行方案。

Abstract: Electroencephalography (EEG) foundation models (EFMs) have achieved strong performance under full fine-tuning but exhibit poor generalization when subject-level supervision is limited, a common constraint in real-world clinical settings. We show that this failure stems not merely from limited supervision, but from a structural mismatch between noisy, limited supervision and the highly plastic parameter space of EFMs. To address this challenge, we propose SCOPE, a Structured COnfidence-aware Prototype-guided adaptation framework for EFM fine-tuning. SCOPE follows a two-stage pipeline. In the first stage, we construct reliable external supervision by learning geometry-regularized task priors, constructing balanced class-level prototypes over the resulting embeddings, and producing confidence-aware pseudo-labels from their agreement to filter unreliable signals on unlabeled data. In the second stage, we introduce ProAdapter, which adapts frozen EEG foundation models via a lightweight adapter conditioned on the structured prototypes. Experiments across three EEG tasks and five foundation model backbones demonstrate that SCOPE consistently achieves strong performance and efficiency under label-limited cross-subject settings.

</details>


### [141] [Learning a Latent Pulse Shape Interface for Photoinjector Laser Systems](https://arxiv.org/abs/2602.17263)
*Alexander Klemps,Denis Ilia,Pradeep Kr. Banerjee,Ye Chen,Henrik Tünnermann,Nihat Ay*

Main category: cs.LG

TL;DR: 针对自由电子激光光阴极注入器中激光脉冲整形优化电子束质量但模拟成本高昂的问题，提出一种基于Wasserstein自编码器的生成式建模框架，建立脉冲整形与束流动力学的可微潜在空间接口，实现高效脉冲设计与分析。


<details>
  <summary>Details</summary>
Motivation: 在自由电子激光光阴极注入器中，控制纵向激光脉冲形状是优化电子束质量的关键手段，但系统探索广阔设计空间受限于暴力脉冲传播模拟的巨大计算开销。

Method: 采用Wasserstein自编码器构建生成式模型，学习脉冲整形参数与下游束流动力学之间的可微潜在表示。通过主成分分析和混合高斯模型分析潜在空间几何结构，评估其连续性和可解释性。

Result: 学习到的潜在空间连续且可解释，重建保真度高；不同阶高斯脉冲族在潜在空间中呈现连贯轨迹；标准化脉冲长度后，潜在组织结构与脉冲能量相关；潜在几何结构良好，支持线性插值实现脉冲类型平滑过渡；模型能从模拟数据泛化到真实实验测量，准确重建并嵌入实验脉冲。

Conclusion: 该框架显著降低了对昂贵脉冲传播模拟的依赖，为束流动力学仿真和分析提供了高效工具，为系统探索脉冲形状与束流质量关系开辟了新途径。

Abstract: Controlling the longitudinal laser pulse shape in photoinjectors of Free-Electron Lasers is a powerful lever for optimizing electron beam quality, but systematic exploration of the vast design space is limited by the cost of brute-force pulse propagation simulations. We present a generative modeling framework based on Wasserstein Autoencoders to learn a differentiable latent interface between pulse shaping and downstream beam dynamics. Our empirical findings show that the learned latent space is continuous and interpretable while maintaining high-fidelity reconstructions. Pulse families such as higher-order Gaussians trace coherent trajectories, while standardizing the temporal pulse lengths shows a latent organization correlated with pulse energy. Analysis via principal components and Gaussian Mixture Models reveals a well behaved latent geometry, enabling smooth transitions between distinct pulse types via linear interpolation. The model generalizes from simulated data to real experimental pulse measurements, accurately reconstructing pulses and embedding them consistently into the learned manifold. Overall, the approach reduces reliance on expensive pulse-propagation simulations and facilitates downstream beam dynamics simulation and analysis.

</details>


### [142] [Unified Latents (UL): How to train your latents](https://arxiv.org/abs/2602.17270)
*Jonathan Heek,Emiel Hoogeboom,Thomas Mensink,Tim Salimans*

Main category: cs.LG

TL;DR: Unified Latents框架通过扩散先验联合正则化与扩散解码学习潜在表示，连接编码器输出噪声与先验最小噪声以获得紧致比特率上界。ImageNet-512上FID 1.4、高PSNR且训练FLOPs更少；Kinetics-600上FVD 1.3创SOTA。


<details>
  <summary>Details</summary>
Motivation: 旨在改进扩散模型的潜在表示学习，通过联合正则化策略在保持高重建质量的同时降低训练计算成本并提供可控的比特率上界。

Method: 提出Unified Latents框架：1) 利用扩散先验对潜在表示进行联合正则化；2) 采用扩散模型进行解码；3) 将编码器输出噪声与先验最小噪声水平关联；4) 得到简单训练目标，提供紧致潜在比特率上界。

Result: ImageNet-512：获得1.4的竞争性FID分数，PSNR重建质量高，训练FLOPs低于Stable Diffusion潜在空间模型；Kinetics-600：以1.3的FVD刷新最先进纪录。

Conclusion: 该框架在图像和视频生成任务中均实现高质量输出与高效训练，验证了其有效性及计算效率优势。

Abstract: We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.

</details>


### [143] [RLGT: A reinforcement learning framework for extremal graph theory](https://arxiv.org/abs/2602.17276)
*Ivan Damnjanović,Uroš Milivojević,Irena Đorđević,Dragan Stevanović*

Main category: cs.LG

TL;DR: 本文提出RLGT（强化学习用于图论），一个新颖的强化学习框架，用于系统化极值图论研究。该框架支持有向/无向图、含环/不含环及多色边，通过高效表示和模块化设计促进未来基于RL的图论研究。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成功应用于极值图论问题（如Wagner利用深度交叉熵方法解决组合优化问题），并催生了多个专用RL环境。然而，现有研究缺乏统一框架，无法有效支持多样化的图结构。本文旨在填补这一空白，为极值图论研究提供通用、系统化的RL基础设施。

Method: 本文提出RLGT框架，其核心设计包括：(1) 系统整合前人工作；(2) 支持无向图、有向图、含环/不含环图及任意数量边颜色的图结构；(3) 采用高效图表示方法；(4) 通过优化计算性能与模块化设计提升可扩展性与易用性。

Result: 成功实现了RLGT框架，具备完整的图类型支持、高效的计算性能和模块化架构，为极值图论中的强化学习研究提供了可复用的基础设施和实验环境。

Conclusion: RLGT框架通过系统化整合先前工作并提供通用、高效的图表示，为未来基于强化学习的极值图论研究奠定了重要基础，有望加速该领域的方法创新与应用探索。

Abstract: Reinforcement learning (RL) is a subfield of machine learning that focuses on developing models that can autonomously learn optimal decision-making strategies over time. In a recent pioneering paper, Wagner demonstrated how the Deep Cross-Entropy RL method can be applied to tackle various problems from extremal graph theory by reformulating them as combinatorial optimization problems. Subsequently, many researchers became interested in refining and extending the framework introduced by Wagner, thereby creating various RL environments specialized for graph theory. Moreover, a number of problems from extremal graph theory were solved through the use of RL. In particular, several inequalities concerning the Laplacian spectral radius of graphs were refuted, new lower bounds were obtained for certain Ramsey numbers, and contributions were made to the Turán-type extremal problem in which the forbidden structures are cycles of length three and four. Here, we present Reinforcement Learning for Graph Theory (RLGT), a novel RL framework that systematizes the previous work and provides support for both undirected and directed graphs, with or without loops, and with an arbitrary number of edge colors. The framework efficiently represents graphs and aims to facilitate future RL-based research in extremal graph theory through optimized computational performance and a clean and modular design.

</details>


### [144] [Efficient privacy loss accounting for subsampling and random allocation](https://arxiv.org/abs/2602.17284)
*Vitaly Feldman,Moshe Shenfeld*

Main category: cs.LG

TL;DR: 该论文针对差分隐私中的随机分配采样方案，提出了一种高效计算其隐私损失分布(PLD)的方法。研究表明，与传统的泊松子采样相比，随机分配在高斯机制下具有至少相当的隐私-效用权衡，且更适合DP-SGD训练。该工作通过引入PLD实现的概念，开发了通用的隐私损失核算工具，解决了现有分析中参数不紧和计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对随机分配采样方案的理论分析存在两大缺陷：一是隐私参数在实践中不够精确，因分析过程中存在近似步骤；二是计算结果基于Hockey-stick或Rényi散度，在隐私损失核算时引入额外开销。这些问题限制了其在差分隐私优化和高效高维聚合等实际场景中的应用效果。

Method: 本文提出基于"隐私损失分布实现"(PLD realization)这一新概念的通用隐私损失核算框架。该方法能够高效计算任意差分隐私算法在随机分配下的精确隐私损失分布，避免了机制特定的手动分析，实现了对子采样方案的准确隐私核算。

Result: 理论分析表明，对于高斯机制，随机分配的隐私-效用权衡至少不劣于泊松子采样，且在DP-SGD训练中表现更优。通过新开发的PLD实现工具，研究首次实现了对随机分配采样方案的精确隐私损失核算，消除了近似误差和散度计算带来的额外开销。

Conclusion: 该工作为随机分配采样提供了更精确的理论分析，证明其在实际差分隐私应用中（特别是机器学习训练）优于或等同于泊松采样。通过建立通用的PLD核算框架，研究为子采样方案的隐私分析提供了新工具，推动了差分隐私技术在实践中的有效部署。

Abstract: We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) and communication-efficient high-dimensional private aggregation (Asi et al., 2025), where it was shown to have utility advantages over the standard Poisson sampling. Theoretical analyses of this sampling scheme (Feldman & Shenfeld, 2025; Dong et al., 2025) lead to bounds that are close to those of Poisson sampling, yet still have two significant shortcomings. First, in many practical settings, the resulting privacy parameters are not tight due to the approximation steps in the analysis. Second, the computed parameters are either the hockey stick or Renyi divergence, both of which introduce overheads when used in privacy loss accounting.
  In this work, we demonstrate that the privacy loss distribution (PLD) of random allocation applied to any differentially private algorithm can be computed efficiently. When applied to the Gaussian mechanism, our results demonstrate that the privacy-utility trade-off for random allocation is at least as good as that of Poisson subsampling. In particular, random allocation is better suited for training via DP-SGD. To support these computations, our work develops new tools for general privacy loss accounting based on a notion of PLD realization. This notion allows us to extend accurate privacy loss accounting to subsampling which previously required manual noise-mechanism-specific analysis.

</details>


### [145] [Flickering Multi-Armed Bandits](https://arxiv.org/abs/2602.17315)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 提出Flickering多臂老虎机(FMAB)框架，其中可用臂集合每轮动态变化且依赖于历史选择。通过随机图过程建模局部移动约束，在Erdős-Rényi和边马尔可夫两种模型下，提出两阶段算法(惰性随机游走探索+导航利用)，获得次线性遗憾界并证明探索成本的最优性。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机假设所有臂始终可用，但实际应用(如机器人探索)中可用选项动态变化且依赖历史动作。缺乏对这种"闪烁"可用性和局部移动约束的理论理解。

Method: 1. 将问题建模为随机图过程，臂对应节点，移动限于邻域 2. 分析ER过程和边马尔可夫过程两种模型 3. 提出两阶段算法：探索阶段使用惰性随机游走高效识别最优臂；利用阶段进行导航并承诺于最优臂 4. 建立高概率遗憾界和期望遗憾界 5. 推导信息论下界证明算法近最优性

Result: - 在两种图模型下均获得次线性遗憾界 - 探索成本达到信息论下界，接近最优 - 数值验证：机器人地面车辆侦察灾难区域场景

Conclusion: FMAB框架有效解决了依赖历史选择的动态可用臂问题，揭示了在局部移动约束下的根本探索成本。两阶段算法在理论和实践中均表现优异，为资源受限的探索提供了新视角。

Abstract: We introduce Flickering Multi-Armed Bandits (FMAB), a new MAB framework where the set of available arms (or actions) can change at each round, and the available set at any time may depend on the agent's previously selected arm. We model this constrained, evolving availability using random graph processes, where arms are nodes and the agent's movement is restricted to its local neighborhood. We analyze this problem under two random graph models: an i.i.d. Erdős--Rényi (ER) process and an Edge-Markovian process. We propose and analyze a two-phase algorithm that employs a lazy random walk for exploration to efficiently identify the optimal arm, followed by a navigation and commitment phase for exploitation. We establish high-probability and expected sublinear regret bounds for both graph settings. We show that the exploration cost of our algorithm is near-optimal by establishing a matching information-theoretic lower bound for this problem class, highlighting the fundamental cost of exploration under local-move constraints. We complement our theoretical guarantees with numerical simulations, including a scenario of a robotic ground vehicle scouting a disaster-affected region.

</details>


### [146] [SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework](https://arxiv.org/abs/2602.17330)
*Rong Fu,Zijian Zhang,Wenxin Zhang,Kun Liu,Jiekai Wu,Xianda Li,Simon Fong*

Main category: cs.LG

TL;DR: SubQuad是一个用于大规模适应性免疫组库比较分析的端到端框架，通过结合抗原感知的近亚二次检索、GPU加速亲和力核、学习多模态融合和公平约束聚类，解决了传统方法中近二次方计算成本和数据不平衡问题，在病毒和肿瘤组库上实现了吞吐量提升和内存优化，同时保持了召回率、聚类纯度和亚组公平性。


<details>
  <summary>Details</summary>
Motivation: 大规模适应性免疫组库比较分析面临两个实际瓶颈：一是成对亲和力评估的近二次方计算成本，二是数据不平衡掩盖了临床重要的稀有克隆型。这限制了从群体规模数据中发现免疫应答模式的能力，特别是在疫苗靶点优先排序和生物标志物发现等转化任务中。

Method: SubQuad采用端到端设计，核心包括：(1)使用紧凑的MinHash预过滤大幅减少候选比较对；(2)可微分门控模块自适应地为每对序列加权互补的对齐和嵌入通道；(3)自动校准例程确保稀有抗原特异性亚组的按比例表示；(4)集成抗原感知的近亚二次检索、GPU加速亲和力核、学习多模态融合和公平约束聚类。

Result: 在大型病毒和肿瘤免疫组库上的实验表明，SubQuad在吞吐量峰值内存使用方面获得显著提升，同时保持或改进了recall@k、聚类纯度以及亚组公平性等关键指标，有效平衡了计算效率与分析质量。

Conclusion: 通过协同设计索引、相似性融合和公平感知目标，SubQuad提供了一个可扩展、去偏倚的平台，为免疫组库挖掘和下游转化应用（如疫苗靶点优先排序和生物标志物发现）提供了新工具，解决了大规模免疫分析中的计算和公平性挑战。

Abstract: Comparative analysis of adaptive immune repertoires at population scale is hampered by two practical bottlenecks: the near-quadratic cost of pairwise affinity evaluations and dataset imbalances that obscure clinically important minority clonotypes. We introduce SubQuad, an end-to-end pipeline that addresses these challenges by combining antigen-aware, near-subquadratic retrieval with GPU-accelerated affinity kernels, learned multimodal fusion, and fairness-constrained clustering. The system employs compact MinHash prefiltering to sharply reduce candidate comparisons, a differentiable gating module that adaptively weights complementary alignment and embedding channels on a per-pair basis, and an automated calibration routine that enforces proportional representation of rare antigen-specific subgroups. On large viral and tumor repertoires SubQuad achieves measured gains in throughput and peak memory usage while preserving or improving recall@k, cluster purity, and subgroup equity. By co-designing indexing, similarity fusion, and equity-aware objectives, SubQuad offers a scalable, bias-aware platform for repertoire mining and downstream translational tasks such as vaccine target prioritization and biomarker discovery.

</details>


### [147] [2Mamba2Furious: Linear in Complexity, Competitive in Accuracy](https://arxiv.org/abs/2602.17363)
*Gabriel Mongaras,Eric C. Larson*

Main category: cs.LG

TL;DR: 本文通过简化Mamba-2并改进其A-mask和隐藏状态阶数，提出2Mamba，在保持软注意力精度的同时显著提升长上下文的内存效率。


<details>
  <summary>Details</summary>
Motivation: 线性注意力因其高效性成为软注意力的有力替代，但表达能力较弱导致精度下降。本文旨在缩小软注意力与线性注意力之间的精度差距，并探索超越软注意力精度的可能性。

Method: 首先将Mamba-2简化为其最核心组件，得到Mamba-2S；随后改进A-mask并提升隐藏状态的阶数，构建2Mamba；同时考察Mamba-2中有助于超越软注意力精度的元素。

Result: 实验表明，2Mamba在精度上接近软注意力，同时在长上下文下内存效率显著更高；此外，某些Mamba-2元素可使其精度超越软注意力。

Conclusion: 通过结构简化和关键改进，线性注意力模型2Mamba能够在保持高精度的同时大幅降低内存开销，为长序列建模提供了更高效的解决方案，并揭示了进一步提升性能的方向。

Abstract: Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments

</details>


### [148] [A feature-stable and explainable machine learning framework for trustworthy decision-making under incomplete clinical data](https://arxiv.org/abs/2602.17364)
*Justyna Andrys-Olek,Paulina Tworek,Luca Gherardini,Mark W. Ruddock,Mary Jo Kurt,Peter Fitzgerald,Jose Sousa*

Main category: cs.LG

TL;DR: CACTUS是一个针对小型、异质性和不完整临床数据集的可解释机器学习框架，通过特征抽象、可解释分类和稳定性分析，量化数据质量下降时关键特征的一致性保留程度。在568例血尿患者的膀胱癌队列中，该框架在预测性能与基线相当或更优的同时，展现出对数据缺失的显著鲁棒性，为生物医学领域提供了可信赖的决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 生物医学机器学习模型在高风险领域的应用受限于三大核心问题：模型鲁棒性不足、可解释性有限，以及学习到的特征在数据扰动（尤其是缺失）下的不稳定性。即使预测性能优异，若关键特征随数据完整性变化而显著波动，将严重削弱模型的可信度和下游决策的可靠性，阻碍其临床转化。

Method: CACTUS框架集成特征抽象、可解释分类和系统性特征稳定性分析三大模块，通过量化特征在数据质量恶化时的保留一致性来评估模型稳健性。研究采用568例血尿患者评估膀胱癌的真实世界队列，在引入可控随机缺失数据的条件下，与随机森林和梯度提升等广泛使用的机器学习方法进行基准测试，并特别进行了性别分层分析。

Result: 实验表明，CACTUS在保持与基线模型相当或更优预测性能的同时，其排名靠前的关键特征在数据缺失增加时展现出显著更高的稳定性。这一现象在整体分析和性别分层分析中均保持一致。结果证实，特征稳定性提供了与常规性能指标互补的重要信息，是评估生物医学模型可信度的关键维度。

Conclusion: CACTUS通过显式量化对缺失数据的鲁棒性并优先选择可解释的稳定特征，为高可信度的数据驱动临床决策支持提供了可推广框架。研究强调，在生物医学模型评估中，必须将特征稳定性纳入核心指标体系，这是确保模型在真实临床环境中可靠性和可重复性的必要前提。

Abstract: Machine learning models are increasingly applied to biomedical data, yet their adoption in high stakes domains remains limited by poor robustness, limited interpretability, and instability of learned features under realistic data perturbations, such as missingness. In particular, models that achieve high predictive performance may still fail to inspire trust if their key features fluctuate when data completeness changes, undermining reproducibility and downstream decision-making. Here, we present CACTUS (Comprehensive Abstraction and Classification Tool for Uncovering Structures), an explainable machine learning framework explicitly designed to address these challenges in small, heterogeneous, and incomplete clinical datasets. CACTUS integrates feature abstraction, interpretable classification, and systematic feature stability analysis to quantify how consistently informative features are preserved as data quality degrades. Using a real-world haematuria cohort comprising 568 patients evaluated for bladder cancer, we benchmark CACTUS against widely used machine learning approaches, including random forests and gradient boosting methods, under controlled levels of randomly introduced missing data. We demonstrate that CACTUS achieves competitive or superior predictive performance while maintaining markedly higher stability of top-ranked features as missingness increases, including in sex-stratified analyses. Our results show that feature stability provides information complementary to conventional performance metrics and is essential for assessing the trustworthiness of machine learning models applied to biomedical data. By explicitly quantifying robustness to missing data and prioritising interpretable, stable features, CACTUS offers a generalizable framework for trustworthy data-driven decision support.

</details>


### [149] [Variational Grey-Box Dynamics Matching](https://arxiv.org/abs/2602.17477)
*Gurjeet Sangra Singh,Frantzeska Lavda,Giangiacomo Mercatali,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 提出一种新颖的灰盒方法VGB-DM，将不完整物理模型直接集成到流匹配生成框架中。采用双潜在编码：其一建模缺失随机性与多模态速度，其二将物理参数编码为具有物理信息先验的潜变量。仅从观测轨迹学习，无需真实物理参数，通过无模拟方式规避神经ODE的可扩展性与稳定性问题，并可扩展至二阶动力学系统。实验表明该方法在ODE/PDE问题上性能优于或与纯数据驱动及现有灰盒基线相当，同时保持物理模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型（流匹配、扩散模型）虽能学习复杂分布与动力系统，但作为黑箱忽视底层物理机制；而基于ODE/PDE的物理模拟模型虽具可解释性，却常因缺失或未知项无法充分描述真实观测。二者存在显著鸿沟，亟需融合物理知识与数据驱动优势的方法。

Method: 提出一种仿真自由的灰盒框架，核心为在流匹配中建模结构化变分分布：使用两个潜在编码——一个捕获缺失随机性与多模态速度，另一个将未知物理参数编码为具有物理信息先验的潜变量。仅依赖观测轨迹学习，无需真实物理参数，避免神经ODE的可扩展性与稳定性挑战，并进一步扩展至二阶动力学系统。

Result: 在典型ODE/PDE问题上的实验结果表明，所提方法性能与纯数据驱动方法及先前灰盒基线相当或更优，同时保留了物理模型的可解释性，验证了其有效性与优势。

Conclusion: 该方法成功桥接了黑箱生成模型与可解释物理模型，在保持物理可解释性的前提下实现了具有竞争力的性能，为结合领域知识与数据驱动建模提供了新思路，代码已开源。

Abstract: Deep generative models such as flow matching and diffusion models have shown great potential in learning complex distributions and dynamical systems, but often act as black-boxes, neglecting underlying physics. In contrast, physics-based simulation models described by ODEs/PDEs remain interpretable, but may have missing or unknown terms, unable to fully describe real-world observations. We bridge this gap with a novel grey-box method that integrates incomplete physics models directly into generative models. Our approach learns dynamics from observational trajectories alone, without ground-truth physics parameters, in a simulation-free manner that avoids scalability and stability issues of Neural ODEs. The core of our method lies in modelling a structured variational distribution within the flow matching framework, by using two latent encodings: one to model the missing stochasticity and multi-modal velocity, and a second to encode physics parameters as a latent variable with a physics-informed prior. Furthermore, we present an adaptation of the framework to handle second-order dynamics. Our experiments on representative ODE/PDE problems show that our method performs on par with or superior to fully data-driven approaches and previous grey-box baselines, while preserving the interpretability of the physics model. Our code is available at https://github.com/DMML-Geneva/VGB-DM.

</details>


### [150] [Linear Convergence in Games with Delayed Feedback via Extra Prediction](https://arxiv.org/abs/2602.17486)
*Yuma Fujimoto,Kenshi Abe,Kaito Ariu*

Main category: cs.LG

TL;DR: 针对多智能体学习中的反馈延迟问题，本文分析WOGDA算法在双线性博弈中的收敛性。理论证明额外乐观预测可将收敛速率从exp(-Θ(t/m^5))显著提升至exp(-Θ(t/(m^2 log m)))，实验验证了该策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 反馈延迟在多智能体学习系统中不可避免，会严重损害算法性能。尽管延迟反馈的影响已受到关注，但在双线性博弈等典型设定下，其精确的收敛速率特征仍不清楚，亟需理论分析指导算法设计。

Method: 提出加权乐观梯度下降-上升法(WOGDA)，通过引入额外乐观机制预测未来奖励。将WOGDA解释为额外近端点法(EPP)的近似版本，其中EPP利用比经典近端点法(PP)更远的未来奖励信息进行更新，从而建立分析框架。

Result: 理论证明：标准乐观（预测下一步奖励）可实现线性收敛，速率exp(-Θ(t/m^5))。额外乐观（预测更远未来）允许更大步长，将收敛速率显著提升至exp(-Θ(t/(m^2 log m)))。实验结果与理论一致，验证了额外乐观的加速效果。

Conclusion: 额外乐观是应对反馈延迟导致性能下降的有效策略。WOGDA在延迟环境下可实现快速线性收敛，且额外乐观能显著缓解延迟负面影响，为设计高效延迟鲁棒的多智能体学习算法提供了理论依据和实践指导。

Abstract: Feedback delays are inevitable in real-world multi-agent learning. They are known to severely degrade performance, and the convergence rate under delayed feedback is still unclear, even for bilinear games. This paper derives the rate of linear convergence of Weighted Optimistic Gradient Descent-Ascent (WOGDA), which predicts future rewards with extra optimism, in unconstrained bilinear games. To analyze the algorithm, we interpret it as an approximation of the Extra Proximal Point (EPP), which is updated based on farther future rewards than the classical Proximal Point (PP). Our theorems show that standard optimism (predicting the next-step reward) achieves linear convergence to the equilibrium at a rate $\exp(-Θ(t/m^{5}))$ after $t$ iterations for delay $m$. Moreover, employing extra optimism (predicting farther future reward) tolerates a larger step size and significantly accelerates the rate to $\exp(-Θ(t/(m^{2}\log m)))$. Our experiments also show accelerated convergence driven by the extra optimism and are qualitatively consistent with our theorems. In summary, this paper validates that extra optimism is a promising countermeasure against performance degradation caused by feedback delays.

</details>


### [151] [LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights](https://arxiv.org/abs/2602.17510)
*Kasun Dewage,Marianna Pensky,Suranadi De Silva,Shankadeep Mondal*

Main category: cs.LG

TL;DR: CRAFT是一种参数高效微调方法，通过在跨层3D权重张量上直接进行Tucker分解，冻结所有分解因子，仅训练作用于各因子矩阵上的小型可训练变换矩阵，在保持性能的同时实现极低参数开销。


<details>
  <summary>Details</summary>
Motivation: 现有张量基PEFT方法（如LoTR、SuperLoRA）分解的是梯度更新而非预训练权重，而PiSSA等权重分解方法又独立作用于单层。CRAFT旨在桥接这两类方法，通过跨层张量分解预训练权重来同时捕捉层间相关性和权重结构，同时保持参数效率。

Method: 将跨层堆叠的注意力权重矩阵组织为3D张量，使用高阶SVD（HOSVD）进行完整Tucker分解，冻结所有核心张量和因子矩阵，仅在每个因子矩阵上应用轻量级可训练变换矩阵来适应下游任务。

Result: 在GLUE基准测试中，RoBERTa-base/large模型上，CRAFT仅需41K可训练参数（该数量在固定Tucker秩下与模型维度和深度无关），性能与现有方法相当。

Conclusion: CRAFT成功结合了张量分解与权重自适应的优势，提供了一种参数高效且与模型规模解耦的微调方案，为大规模语言模型的轻量级适配提供了新思路。

Abstract: We introduce CRAFT (Cross-layer Rank Adaptation via Frozen Tucker), a parameter-efficient fine-tuning (PEFT) method that applies Tucker tensor decomposition to pre-trained attention weight matrices stacked across transformer layers and trains only small square adaptation matrices on the resulting frozen Tucker factors. Existing tensor-based PEFT methods decompose gradient updates: LoTR applies Tucker decomposition with shared factor matrices, while SuperLoRA groups and reshapes $ΔW$ across layers before applying Tucker decomposition. Separately, methods like PiSSA apply SVD to pre-trained weights but operate independently per layer. CRAFT bridges these two lines of work: it performs full Tucker decomposition via Higher-Order SVD (HOSVD) directly on pre-trained weights organized as cross-layer 3D tensors, freezes all resulting factors, and adapts the model through lightweight trainable transformations applied to each factor matrix. Experiments on the GLUE benchmark using RoBERTa-base and RoBERTa-large demonstrate that CRAFT achieves competitive performance with existing methods while requiring only 41K Tucker adaptation parameters--a count independent of model dimension and depth at fixed Tucker ranks.

</details>


### [152] [Variational inference via radial transport](https://arxiv.org/abs/2602.17525)
*Luca Ghafourpour,Sinho Chewi,Alessio Figalli,Aram-Alexandre Pooladian*

Main category: cs.LG

TL;DR: 本文提出radVI算法，通过优化概率分布的径向轮廓解决变分推断中高斯近似覆盖不足的问题，可作为现有VI方法的廉价插件并提供基于Wasserstein空间的理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 传统变分推断使用高斯分布等简单分布逼近高维目标分布π，但高斯分布往往无法捕捉π的真实径向轮廓，导致近似分布覆盖不足。需要从径向轮廓优化的新视角重新审视变分推断问题。

Method: 从径向轮廓优化角度提出radVI算法，在Wasserstein空间（赋予Wasserstein距离的概率分布空间）中进行优化，利用Caffarelli(2000)风格的径向传输映射正则性性质改进现有VI方案。

Result: radVI是廉价且有效的附加组件，可与高斯均值场变分推断和拉普拉斯近似等现有VI方案结合使用，并提供了理论收敛保证。

Conclusion: 通过优化径向轮廓，radVI显著提升了变分推断的覆盖能力，同时保持计算效率，为高维概率分布的近似推断提供了新的理论保证和实践工具。

Abstract: In variational inference (VI), the practitioner approximates a high-dimensional distribution $π$ with a simple surrogate one, often a (product) Gaussian distribution. However, in many cases of practical interest, Gaussian distributions might not capture the correct radial profile of $π$, resulting in poor coverage. In this work, we approach the VI problem from the perspective of optimizing over these radial profiles. Our algorithm radVI is a cheap, effective add-on to many existing VI schemes, such as Gaussian (mean-field) VI and Laplace approximation. We provide theoretical convergence guarantees for our algorithm, owing to recent developments in optimization over the Wasserstein space--the space of probability distributions endowed with the Wasserstein distance--and new regularity properties of radial transport maps in the style of Caffarelli (2000).

</details>


### [153] [Position: Evaluation of ECG Representations Must Be Fixed](https://arxiv.org/abs/2602.17531)
*Zachary Berger,Daniel Prakah-Asante,John Guttag,Collin M. Stultz*

Main category: cs.LG

TL;DR: 这篇立场论文指出现有12导联心电图表示学习基准测试的局限性，提出需要扩展评估目标、改进评估实践，并发现随机初始化编码器作为基线的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前ECG表示学习领域过度依赖三个公开多标签基准（PTB-XL、CPSC2018、CSN），这些基准主要关注心律失常和波形形态学标签，但心电图实际上包含更广泛的临床信息。现有评估实践不足以确保研究进展的可靠性和与临床目标的契合度。

Method: 1）论证需要扩展下游评估目标，包括结构性心脏病和患者水平预测；2）概述多标签不平衡设置的最佳评估实践；3）实证评估三种代表性ECG预训练方法在六种评估场景下的表现：三个标准基准、结构性心脏病数据集、血流动力学推断和患者预测；4）引入随机初始化编码器作为基线模型。

Result: 1）当采用最佳评估实践时，文献中关于哪种表示方法最优的结论发生改变；2）令人惊讶的是，随机初始化编码器配合线性评估在许多任务上能够匹配最先进的预训练方法；3）验证了扩展评估目标的重要性。

Conclusion: 为确保ECG表示学习领域的可靠进展，必须修复当前基准测试实践，扩展评估目标至更广泛的临床终点，采用更严格的评估标准，并将随机编码器作为合理的基线模型。

Abstract: This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model. We substantiate our observations with an empirical evaluation of three representative ECG pre-training approaches across six evaluation settings: the three standard benchmarks, a structural disease dataset, hemodynamic inference, and patient forecasting.

</details>


### [154] [MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning](https://arxiv.org/abs/2602.17550)
*Xiaoliang Fu,Jiaye Lin,Yangyi Fang,Binbin Zheng,Chaowen Hu,Zekai Shao,Cong Qin,Lu Pan,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 本文指出传统RLVR算法的刚性信任区域机制与LLM优化动态不匹配，提出MASPO框架，通过软高斯门控、质量自适应限制和非对称风险控制解决梯度利用效率、概率质量敏感性和信号可靠性三大问题，显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法（如GRPO）采用刚性、统一、对称的信任区域机制，与大型语言模型的复杂优化动态存在根本性错位，导致梯度利用效率低、概率质量不敏感和非对称信号可靠性三大挑战。

Method: 提出质量自适应软策略优化(MASPO)框架，整合可微分软高斯门控最大化梯度效用、质量自适应限制器平衡概率谱探索、非对称风险控制器对齐更新幅度与信号置信度。

Result: 广泛评估表明，MASPO作为一体化RLVR解决方案具有鲁棒性，性能显著优于强基线模型，代码已开源。

Conclusion: MASPO通过三维协调机制有效解决了传统RLVR方法的局限性，为LLM强化学习优化提供了新范式，具有显著实用价值。

Abstract: Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.

</details>


### [155] [A Theoretical Framework for Modular Learning of Robust Generative Models](https://arxiv.org/abs/2602.17554)
*Corinna Cortes,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本文针对大规模生成模型训练的资源密集性和启发式加权问题，提出一种模块化框架，通过极小极大博弈和Kakutani不动点定理构建鲁棒门控机制来组合领域专家，理论证明其优越性并提供高效算法，实证显示可缓解梯度冲突并超越单体模型。


<details>
  <summary>Details</summary>
Motivation: 当前大规模生成模型训练不仅计算资源消耗巨大，还严重依赖经验性的数据集加权策略。本研究旨在回答两个核心问题：其一，能否通过模块化方式训练大语言模型，即组合多个小型领域专家以达到单体模型的性能；其二，能否实现对任意数据混合的鲁棒训练，从而彻底摆脱启发式调参的依赖。

Method: 建立模块化生成建模的理论框架，定义归一化门控函数空间G₁，并将鲁棒门控学习形式化为寻找最小化至最坏情况数据混合散度的极小极大博弈问题。利用Kakutani不动点定理证明鲁棒门控的存在性，推导基于门控复杂度的泛化界，并从理论上证明模块化方法相对于聚合数据重训练的优势，其性能差距由Jensen-Shannon散度量化。提出可扩展的随机原始-对偶优化算法及结构蒸馏推理方法。

Result: 理论分析表明，模块化结构本身起到强正则化作用，其泛化误差界取决于轻量门控的复杂度；与在聚合数据上重新训练的模型相比，模块化方法具有理论优势，性能差距可由Jensen-Shannon散度精确刻画。在合成数据和真实世界数据集上的实验验证了该架构能有效缓解梯度冲突，并在各种数据混合下稳定地超越单体基线模型。

Conclusion: 该研究通过严谨的理论分析和实证验证，建立了无需启发式调参的模块化大语言模型训练新范式，为资源高效、鲁棒的模型训练提供了理论基础和实践工具，对降低大规模模型训练成本具有重要价值。

Abstract: Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence. Finally, we introduce a scalable Stochastic Primal-Dual algorithm and a Structural Distillation method for efficient inference. Empirical results on synthetic and real-world datasets confirm that our modular architecture effectively mitigates gradient conflict and can robustly outperform monolithic baselines.

</details>


### [156] [Revisiting Weight Regularization for Low-Rank Continual Learning](https://arxiv.org/abs/2602.17559)
*Yaoyue Zheng,Yin Zhang,Joost van de Weijer,Gido M van de Ven,Shaoyi Du,Xuetao Zhang,Zhiqiang Tian*

Main category: cs.LG

TL;DR: 本文针对参数高效持续学习（PECL）中的任务干扰问题，重新探讨了低秩适配器场景下的权重正则化方法。提出EWC-LoRA，通过弹性权重巩固（EWC）正则化共享的低秩更新，实现了存储和计算成本不随任务数量增长，在各项基准测试中取得了优于现有低秩持续学习方法的稳定性-可塑性平衡。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型的持续学习范式正从从头训练转向持续适应。在参数高效持续学习（PECL）中，现有方法主要通过任务特定模块（如低秩适配器）缓解任务干扰，但关键的权重正则化技术（如EWC）尚未被充分探索。如何在低秩参数化下有效应用正则化机制，同时保持计算和内存效率，是亟待解决的问题。

Method: 提出EWC-LoRA方法，将弹性权重巩固（EWC）正则化引入低秩持续学习框架。核心创新是通过低秩表示来估计全参数空间的重要性，仅对共享的低秩更新进行正则化约束，而非每个任务的独立适配器。这确保了无论任务数量多少，存储需求和推理成本都保持恒定，提供了计算和内存高效的解决方案。

Result: 在多个基准测试上的广泛实验表明，EWC-LoRA在缓解任务干扰方面效果显著，实现了优于现有低秩持续学习方法的稳定性-可塑性权衡。结果验证了在低秩参数化条件下，权重正则化仍然是缓解任务干扰的有效机制。

Conclusion: 本研究揭示了在参数高效持续学习范式中，权重正则化技术具有重要价值。EWC-LoRA为大规模预训练模型的持续学习提供了实用且高效的解决方案，同时启发未来研究更广泛地在PECL中应用正则化技术。代码已开源。

Abstract: Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)-a key strategy in CL-remain underexplored in this new paradigm. In this paper, we revisit weight regularization in low-rank CL as a new perspective for mitigating task interference in PECL. Unlike existing low-rank CL methods, we mitigate task interference by regularizing a shared low-rank update through EWC, thereby keeping the storage requirement and inference costs constant regardless of the number of tasks. Our proposed method EWC-LoRA leverages a low-rank representation to estimate parameter importance over the full-dimensional space. This design offers a practical, computational- and memory-efficient solution for CL with PTMs, and provides insights that may inform the broader application of regularization techniques within PECL. Extensive experiments on various benchmarks demonstrate the effectiveness of EWC-LoRA, achieving a stability-plasticity trade-off superior to existing low-rank CL approaches. These results indicate that, even under low-rank parameterizations, weight regularization remains an effective mechanism for mitigating task interference. Code is available at: https://github.com/yaoyz96/low-rank-cl.

</details>


### [157] [Be Wary of Your Time Series Preprocessing](https://arxiv.org/abs/2602.17568)
*Sofiane Ennadir,Tianze Wang,Oleg Smirnov,Sahar Asadi,Lele Cao*

Main category: cs.LG

TL;DR: 该研究首次从理论角度系统分析了Transformer时间序列模型中不同归一化策略（实例级与全局缩放）对模型表达能力的影響，提出一个新的时间序列表达能力框架，推导出Standard和Min-Max缩放的理论边界，发现归一化方法选择会显著影响模型表示能力且依赖任务数据特性，有时不进行归一化效果更佳。


<details>
  <summary>Details</summary>
Motivation: 时间序列建模中的归一化与缩放是基础预处理步骤，但现有研究缺乏从理论层面探索不同归一化策略（尤其是实例级与全局缩放）如何影响Transformer模型的表达能力，亟需理论框架指导实践选择。

Method: 提出一个专门针对时间序列的表达能力评估框架，量化模型在表示空间中区分相似与相异输入的能力；基于该框架，推导出Standard和Min-Max缩放两种常用归一化方法的理论边界，并通过多任务基准实验进行实证验证。

Result: 理论分析揭示归一化策略对Transformer表示能力具有显著影响，其效果取决于具体任务与数据特性；实证结果表明不存在普适最优的归一化方法，在某些情况下完全省略归一化反而能取得更好性能。

Conclusion: 归一化预处理对时间序列学习至关重要，未来需针对特定任务和数据集设计更原则性的归一化策略；研究强调了理论指导实践的价值，为开发理论依据充分的归一化方法奠定了基础。

Abstract: Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets.

</details>


### [158] [Canonicalizing Multimodal Contrastive Representation Learning](https://arxiv.org/abs/2602.17584)
*Sharut Gupta,Sanyam Kansal,Stefanie Jegelka,Phillip Isola,Vikas Garg*

Main category: cs.LG

TL;DR: 独立训练的多模态对比模型（如CLIP、SigLIP、FLAVA）的嵌入空间间存在统一的几何关系：可用一个正交变换Q（加全局均值平移）将一个模型的图像和文本编码器同时映射到另一个模型，且该变换跨模态通用。


<details>
  <summary>Details</summary>
Motivation: 多模态模型需保持模态内一致性和图文耦合关系，仅匹配相似性不足以建立表示空间的显式对应。因此探究：独立训练的模型其嵌入空间是否存在系统性几何关系？是否跨模态均匀成立？

Method: 实证研究CLIP、SigLIP、FLAVA等模型族嵌入空间的几何关系；理论上证明，若两模型在小锚集上的多模态核函数（图文相似度）一致，则必存在一个同时适用于图像和文本编码器的正交映射Q。

Result: 跨模型嵌入空间关系可被正交变换Q（至多全局均值平移）良好近似；同一Q能同时对齐图像和文本编码器；理论证明支持该发现。

Conclusion: 该发现揭示了多模态表示学习的内在几何结构，支持向后兼容的模型升级，避免重嵌入成本，并对表示隐私保护有启示意义。

Abstract: As models and data scale, independently trained networks often induce analogous notions of similarity. But, matching similarities is weaker than establishing an explicit correspondence between the representation spaces, especially for multimodal models, where consistency must hold not only within each modality, but also for the learned image-text coupling. We therefore ask: given two independently trained multimodal contrastive models (with encoders $(f, g)$ and $(\widetilde{f},\widetilde{g})$) -- trained on different distributions and with different architectures -- does a systematic geometric relationship exist between their embedding spaces? If so, what form does it take, and does it hold uniformly across modalities? In this work, we show that across model families such as CLIP, SigLIP, and FLAVA, this geometric relationship is well approximated by an orthogonal map (up to a global mean shift), i.e., there exists an orthogonal map $Q$ where $Q^\top Q = I$ such that $\widetilde{f}(x)\approx Q f(x)$ for paired images $x$. Strikingly, the same $Q$ simultaneously aligns the text encoders i.e., $\widetilde{g}(y)\approx Q g(y)$ for texts $y$. Theoretically, we prove that if the multimodal kernel agrees across models on a small anchor set i.e. $\langle f(x), g(y)\rangle \approx \langle \widetilde{f}(x), \widetilde{g}(y)\rangle$, then the two models must be related by a single orthogonal map $Q$ and the same $Q$ maps images and text across models. More broadly, this finding enables backward-compatible model upgrades, avoiding costly re-embedding, and has implications for the privacy of learned representations.
  Our project page: https://canonical-multimodal.github.io/

</details>


### [159] [Towards Anytime-Valid Statistical Watermarking](https://arxiv.org/abs/2602.17608)
*Baihe Huang,Eric Xu,Kannan Ramchandran,Jiantao Jiao,Michael I. Jordan*

Main category: cs.LG

TL;DR: 本文提出Anchored E-Watermarking，首个基于e值的文本水印框架，通过构建检验上鞅实现任意时点有效的机器文本检测，在保持Type-I错误保证的同时，将平均检测token预算较现有最优方法降低13-15%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型普及亟需高效区分机器与人类文本。现有统计水印方法存在两大局限：采样分布选择缺乏原则性方法，以及依赖固定时域假设检验导致无效早期停止，限制了检测效率与实用性。

Method: 开发基于e值的水印框架，利用锚定分布逼近目标模型，构造检测过程的上鞅检验统计量，实现任意时点有效的序贯推断。理论刻画了以最小化最坏情形对数增长率为目标的最优e值，并推导最优期望停止时间，统一了最优采样与随时有效的水印检测。

Result: 模拟与基准数据集评估表明，该框架显著提升采样效率，在任意停止策略下保持Type-I错误率有效保证，平均检测token预算较最先进基线减少13-15%，验证了理论优势。

Conclusion: Anchored E-Watermarking通过e值理论与上鞅方法，首次统一了水印检测的最优采样与任意时点有效推断，为LLM生成内容的高效检测提供了新理论范式，可实际降低计算成本并提升部署可行性。

Abstract: The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.

</details>


### [160] [Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning](https://arxiv.org/abs/2602.17614)
*Obaidullah Zaland,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 针对大数据场景下的隐私保护需求，提出 KD-UFSL 方法，在 UFSL 中引入微聚合与差分隐私，降低中间表示（smashed data）泄露风险，提升隐私保护同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 在大数据环境中，海量异构数据分布在多个客户端，传统集中式训练存在隐私泄露风险。联邦学习（FL）虽能实现去中心化训练，但客户端计算负担重。U-shaped federated split learning (UFSL) 将部分计算卸载至服务器，但客户端上传的中间表示易受数据重构攻击，导致私有数据暴露。因此，需要一种既能减轻客户端计算负担、又能有效保护数据隐私的学习框架。

Method: 提出 k-匿名差分隐私 UFSL（KD-UFSL），在客户端对 smashed data 进行微聚合以实现 k-匿名，并在此基础上添加满足差分隐私的噪声。服务器仅接收经过隐私增强的中间表示进行模型训练，从而在保证模型精度的前提下，防止 adversary 通过中间数据重构原始私有信息。实验采用四个基准数据集，对比 KD-UFSL 与 baseline UFSL 的隐私泄露程度与模型效用。

Result: 实验结果表明，KD-UFSL 在四个基准数据集上使实际图像与重构图像之间的均方误差（MSE）最高提升 50%，结构相似性（SSIM）降低最高 40%。同时，KD-UFSL 在保持全局模型性能的前提下，显著降低了隐私泄露风险，实现了隐私保护与模型效用的平衡。

Conclusion: KD-UFSL 是一种适用于大规模大数据应用的隐私保护方案，能够在保证模型精度的同时，显著降低 smashed data 导致的数据泄露风险。该方法为联邦学习中的隐私与效用平衡提供了有效途径，具备在实际场景中的可行性与推广价值。

Abstract: Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients' side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients' private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.

</details>


### [161] [Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs](https://arxiv.org/abs/2602.17616)
*Luke Huang,Zhuoyang Zhang,Qinghao Hu,Shang Yang,Song Han*

Main category: cs.LG

TL;DR: 本文针对异步强化学习训练中策略梯度方差过高导致训练不稳定的问题，提出了VCPO方法，通过基于有效样本量调整学习率和采用闭式最小方差基线来控制方差，在数学、推理和工具使用任务上显著提升了异步训练的鲁棒性，训练时间减少2.5倍。


<details>
  <summary>Details</summary>
Motivation: 异步强化学习训练虽能提升端到端吞吐量，但在无价值 critic 的策略梯度方法（如 REINFORCE 和 GRPO）中，高异步性会导致策略梯度估计方差显著增加。训练过程中使用过时采样会产生重尾分布的重要性比率，使少量样本主导参数更新，造成梯度噪声大、学习过程不稳定，甚至引发训练崩溃。现有方法缺乏对策略梯度方差的有效控制机制。

Method: 提出方差控制策略优化（VCPO）方法，包含两个核心机制：(i) 基于有效样本量（ESS）动态缩放学习率，抑制不可靠更新；(ii) 引入适用于离策略场景的闭式最小方差基线，避免训练辅助价值模型，仅增加极小计算开销。该方法通用且可直接应用于 REINFORCE/GRPO 类算法。

Result: 在数学推理、通用推理和工具使用等任务上，VCPO 显著提升了异步训练的稳定性与鲁棒性，全面优于掩码/裁剪稳定器及多种算法变体等基线方法。实验表明，VCPO 在保持同步训练性能的同时，将长上下文多轮训练时间缩短了2.5倍，验证了控制策略梯度方差的有效性。

Conclusion: 显式控制策略梯度方差是大规模异步强化学习可靠训练的关键。VCPO 通过有效样本量自适应的学习率缩放和最小方差基线设计，成功解决了高异步性带来的方差爆炸问题，为实现高效、稳定的异步RL训练提供了通用解决方案。

Abstract: Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.

</details>


### [162] [Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning](https://arxiv.org/abs/2602.17625)
*Obaidullah Zaland,Zulfiqar Ahmad Khan,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文提出一次性增量联邦学习框架（OSI-FL），通过单轮通信传输类别嵌入，利用扩散模型合成数据，并结合选择性样本保留策略，在解决通信开销问题的同时有效缓解了灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现代大数据系统产生海量、异构、地理分散且隐私敏感的流数据，中心化面临挑战。传统联邦学习假设数据流静态且需多轮通信，难以在通信受限场景下处理增量数据。

Method: OSI-FL框架在单轮通信中由各客户端通过冻结的视觉语言模型生成类别特定嵌入，服务器利用预训练扩散模型合成与客户端数据分布相似的新数据用于训练。为缓解灾难性遗忘，引入选择性样本保留（SSR）策略，基于样本损失识别并保留每个类别和任务对中最具信息量的前p个样本，在后续迭代中持续纳入训练。

Result: 实验结果表明，OSI-FL在三个基准数据集上的类和域增量场景中，均优于包括传统和一次性联邦学习在内的基线方法。

Conclusion: OSI-FL首次同时解决了联邦学习中的通信开销和灾难性遗忘双重挑战，通过单次通信和智能样本保留实现了高效的增量学习，为隐私保护下的分布式机器学习提供了新思路。

Abstract: Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.

</details>


### [163] [When to Trust the Cheap Check: Weak and Strong Verification for Reasoning](https://arxiv.org/abs/2602.17633)
*Shayan Kiyani,Sima Noorani,George Pappas,Hamed Hassani*

Main category: cs.LG

TL;DR: 本文形式化了大语言模型推理中弱验证（低成本、快速但有噪声）与强验证（高成本、可靠）之间的张力，提出了两阈值最优验证策略结构，并开发了无需假设的在线算法来控制接受与拒绝错误。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理日益依赖于验证循环，但内部弱验证与外部强验证在成本和可靠性上存在显著差异，需要系统化的理论框架来平衡二者以实现可信输出。

Method: 通过建立弱-强验证策略的形式化框架，定义错误接受、错误拒绝和强验证频率等指标，分析最优策略的结构特性，并设计在线算法动态控制误差。

Result: 研究表明最优策略具有双阈值结构，弱验证器的价值由校准性和锐度决定；所提算法无需对查询流、语言模型或弱验证器做任何假设即可证明地控制接受与拒绝错误。

Conclusion: 该研究为在资源约束下实现大语言模型推理的可信验证提供了理论基础和实践算法，有效平衡了验证成本与可靠性之间的权衡。

Abstract: Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.

</details>


### [164] [Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting](https://arxiv.org/abs/2602.17634)
*Xinghong Fu,Yanhong Li,Georgios Papaioannou,Yoon Kim*

Main category: cs.LG

TL;DR: 本文提出了一种学习高效时间序列基础模型的简单方法，通过混合卷积和线性RNN层（DeltaNet）构建小型模型，在保持性能的同时将模型大小缩小百倍以上，并推出了Reverso模型系列。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模时间序列基础模型在零样本预测中表现出色，但模型参数量达数亿，实际部署效率低、成本高。现有研究过度关注扩大模型规模，忽视了效率问题。

Method: 提出混合架构，交替使用长卷积层和线性RNN层（特别是DeltaNet层）；采用数据增强和推理优化策略；在保持性能的前提下大幅缩小模型规模。

Result: 小型混合模型性能可匹敌大型Transformer模型，但参数量减少百倍以上；Reverso模型系列显著推进了性能-效率的帕累托前沿。

Conclusion: 大规模Transformer对时间序列基础模型并非必要，通过架构创新和训练策略优化，可以实现高效且高性能的零样本预测模型，为实际部署提供更优选择。

Abstract: Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.

</details>


### [165] [FAMOSE: A ReAct Approach to Automated Feature Discovery](https://arxiv.org/abs/2602.17641)
*Keith Burghardt,Jienan Liu,Sadman Sakib,Yuning Hao,Bo Li*

Main category: cs.LG

TL;DR: 本文提出FAMOSE，首个将ReAct智能体框架应用于自动化特征工程的方法。该框架通过LLM驱动的智能体自主探索、生成和优化特征，集成特征选择与评估工具。实验表明：在分类任务上接近SOTA，尤其在大规模数据集（>10K样本）中ROC-AUC平均提升0.23%；在回归任务上达到SOTA，RMSE平均降低2.0%；且具备更强的鲁棒性。研究证实AI智能体在需要高度创新性的特征工程问题上表现卓越。


<details>
  <summary>Details</summary>
Motivation: 特征工程仍是机器学习的关键瓶颈，尤其对于表格数据，从指数级庞大的特征空间中识别最优特征传统上依赖大量领域专业知识，制约了自动化建模效率与泛化能力。

Method: 创新性地提出FAMOSE框架，基于ReAct（推理-行动）范式构建智能体架构，将特征选择与评估工具内化至智能体中，使LLM能够通过迭代式的特征探索、生成、评估与反馈循环，自主完成特征工程全流程。

Result: 在分类任务上达到或逼近最先进水平，特别是在样本量超过10K的任务中ROC-AUC平均提升0.23%；在回归任务上实现最优性能，平均RMSE降低2.0%；相较于其他算法展现出更强的误差鲁棒性。

Conclusion: 本研究证实ReAct范式通过上下文窗口记录特征探索历史，类似少样本提示机制，引导LLM生成更具创新性的特征。为AI智能体在复杂创造性问题（如特征工程）上的应用提供了有力证据，开辟了自动化特征工程的新路径。

Abstract: Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.

</details>


### [166] [A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning](https://arxiv.org/abs/2602.17642)
*Dhruv Talwar,Harsh Desai,Wendong Yin,Goutam Mohanty,Rafael Reveles*

Main category: cs.LG

TL;DR: 本文提出了一种名为A.R.I.S.的自动化回收识别系统，该系统利用YOLOx深度学习模型实现对粉碎电子废料的实时分类。该系统成本低廉、便于携带，能够以90%的精确度、82.2%的平均精度均值(mAP)和84%的分拣纯度有效识别金属、塑料和电路板，显著提升了电子废料的资源回收效率。


<details>
  <summary>Details</summary>
Motivation: 传统电子废料回收过程因材料分离和识别能力不足而导致严重的资源浪费，限制了材料回收率。现有方法效率低下，阻碍了先进回收技术的推广应用。为了提高资源回收效率并降低环境影响，需要开发一种低成本、高效的自动化分拣解决方案。

Method: 研究团队开发了A.R.I.S.系统，采用YOLOx目标检测模型对粉碎后的电子废料进行实时分类识别。系统针对金属、塑料和电路板三类材料进行训练和优化，通过深度学习算法实现快速准确的检测。该系统设计为便携式、低成本架构，便于在实际回收场景中部署应用。

Result: 实验评估显示，该系统实现了90%的整体精确率、82.2%的平均精度均值(mAP)和84%的分拣纯度。系统在保持高检测准确率的同时实现了低推理延迟，证明了其在大规模电子废料回收场景中的实用性和有效性。

Conclusion: 通过将深度学习与现有分拣方法相结合，A.R.I.S.系统显著提升了材料回收效率，降低了先进回收技术的采用门槛。该工作支持产品生命周期延长、以旧换新和回收计划等更广泛的可持续发展倡议，有助于减少整个供应链的环境影响。

Abstract: Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.

</details>


### [167] [Multi-Round Human-AI Collaboration with User-Specified Requirements](https://arxiv.org/abs/2602.17646)
*Sima Noorani,Shayan Kiyani,Hamed Hassani,George Pappas*

Main category: cs.LG

TL;DR: 本文针对高风险多轮对话AI系统，提出基于反事实伤害与互补性双原则的人本框架。通过用户自定义规则形式化约束，设计具有有限样本保证的在线无分布算法，在医疗诊断与图形推理任务中验证了方法对违规率的控制能力及决策质量的调节作用。


<details>
  <summary>Details</summary>
Motivation: 人类在高风险决策中对多轮对话AI的依赖日益加深，亟需原则性框架确保交互能可靠提升决策质量。现有方法缺乏可定制化的安全约束机制，无法在利用AI优势的同时避免损害人类判断力。

Method: 建立以人为中心的双原则理论：反事实伤害原则确保AI不削弱人类优势，互补性原则确保AI在人类易错环节增加价值。通过用户定义规则形式化约束条件，提出在线无分布算法，在协作过程中强制执行约束并获得有限样本统计保证。

Result: 在LLM模拟医疗诊断和人类众包图形推理两项交互实验中，所提方法在非稳态交互动态下仍能维持预设的约束违反率。调整约束强度会引发人类下游准确率的系统性变化，验证了双原则作为调控杠杆的有效性。

Conclusion: 该框架为高风险AI协作提供了可定制、可验证的安全保障，无需对人类行为建模或施加限制，即可通过双原则约束引导多轮协作朝向更优决策质量。

Abstract: As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.

</details>


### [168] [MARS: Margin-Aware Reward-Modeling with Self-Refinement](https://arxiv.org/abs/2602.17658)
*Payel Bhattacharjee,Osvaldo Simeone,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文提出MARS框架，一种自适应的margin感知数据增强策略，专门针对奖励模型的模糊区域进行增强。通过迭代式难样本挖掘和理论保证，该框架在鲁棒奖励建模任务中显著优于均匀增强方法。


<details>
  <summary>Details</summary>
Motivation: 现代对齐技术（如RLHF和RLAIF）的核心组件奖励建模严重依赖昂贵且有限的人工标注偏好数据。现有数据增强方法仅停留在表示层或语义层，缺乏对奖励模型估计难度和不确定性的显式建模，导致增强效率低下。

Method: MARS采用margin-aware策略，优先增强低margin（即奖励模型最不确定）的偏好对，并通过迭代式硬样本挖掘动态优化训练分布。理论分析证明该策略能增加损失函数的平均曲率，提升信息量和优化条件。

Result: 理论保证表明MARS能增强损失函数的曲率，改善优化条件；实验结果验证其在多种设置下相比均匀增强方法获得一致性提升，有效提高了奖励模型的鲁棒性和可靠性。

Conclusion: MARS通过自适应地靶向奖励模型的模糊和失败模式，提供了更高效的奖励建模数据增强范式。该框架在理论和实践上均证明有效，为缓解人工标注数据瓶颈问题提供了创新解决方案。

Abstract: Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.

</details>
