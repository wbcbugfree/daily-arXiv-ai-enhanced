<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.AI](#cs.AI) [Total: 26]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

TL;DR: StreetMath 引入一个面向近似推理的基准，评估非自回归解码模型在现实近似场景下的能力，揭示模型在近似任务中倾向于做出精确计算或调用外部工具，并显示近似与精确运算依赖不同的神经组件。


<details>
  <summary>Details</summary>
Motivation: 当前文献多关注大型语言模型在严格的精确算术上的能力，较少关注在非正式、快速的数学任务中的近似推理；需要理解不同模型在近似推理中的内部机制与局限。

Method: 提出 StreetMath 基准，跨多种模型评估近似推理能力；使用机制可解释性方法探测内部计算状态；比较不同模型的层级行为、token 成本，以及精确与近似运算的分离性.

Result: 模型普遍试图得出精确值或调用外部工具；在部分任务的早期层或步骤可能达到正确答案，但代用更多 token；精确与近似运算依赖的神经组件基本分离。

Conclusion: 基于认知心理学的观点，LLMs 在街头数学场景中并不呈现人类那样的认知吝啬；研究结果有助于理解近似推理的内部实现差异，并为设计更高效的近似推理和工具扩展提供线索；论文已开源。

Abstract: There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [2] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 提出一个基于模糊逻辑和依存句法分析的多粒度意见分析框架，用于对产品评论中的观点强度进行分级，并据此对实体进行方面相关的排序。


<details>
  <summary>Details</summary>
Motivation: 传统的整体词汇法忽略了意见强度的差异，难以对实体进行细粒度排序；需要引入强度层级（如非常弱到非常强）以提升排名质量。

Method: 通过将与特定方面相关的意见词（副词、形容词、名词、动词）映射到强度等级，并结合模糊逻辑对其进行分类；利用句法依存分析识别与关注方面相关的词汇关系，从而为每个方面给出实体得分。

Result: 提出一个模糊逻辑驱动的分级框架和依存关系分析方法，能够为每个关注方面分配带强度的评分，从而实现基于方面的实体排序。

Conclusion: 该方法克服了纯词汇法对意见强度缺乏区分的问题，提升了对意见强度的感知能力和对实体的排序效果。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [3] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

TL;DR: 提出并公开了一个大规模的韩语目标无关立场检测数据集 LASTIST，包含563,299条已标注句子，面向目标无关与时序演化等任务，填补韩语资源不足的研究空缺。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦目标相关的立场检测，且资源主要集中在英语，韩语等低资源语言缺乏相应的大规模数据集与任务设置。需要一个面向韩语的目标无关立场检测数据集，以及对立场随时间演变的分析能力。

Method: 从双方政治派别的新闻稿/公开信息中收集韩语句子，进行标注，形成563,299条标签数据；设计数据集以支持目标无关立场检测和时序演化立场检测等多任务；在数据集上训练了最先进的深度学习与立场检测模型，验证其可用性与有效性。

Result: 构建并发布 LASTIST 数据集，规模大、覆盖多任务，且在数据上验证了 SOTA 深度学习模型在韩语立场检测上的适用性与潜力。

Conclusion: LASTIST 针对韩语立场检测领域的研究空缺，提供了一个大规模、可扩展的数据集，支持目标无关与时序演化等多任务，为低资源语言的立场检测研究提供重要资源与基准。并附带数据获取链接。

Abstract: Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [4] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

TL;DR: 提出 zFLoRA：一个零延迟的融合低秧适配器，在基模型之上实现零到极小的推理开销，在1B/3B/7B的LLM上对18个任务（常识推理、数学推理、摘要对话）表现优于LoRA和FFT，并在NPU与GPU上测得零到微小的延迟开销。


<details>
  <summary>Details</summary>
Motivation: 尽管适配器的参数量通常小于1%，但推理阶段的计算开销却可能放大到基模型的2.5倍左右，给实际部署带来显著延迟成本。因此需要设计在推理中几乎无额外延迟的适配方案。

Method: 提出零延迟融合低秧适配器zFLoRA，在基模型之上实现零或可忽略的延迟开销。通过在1B、3B、7B规模的LLM上进行多任务评估，涵盖18个任务，比较对象包括LoRA和全量微调（FFT）。在Samsung Galaxy S25+的NPU和NVIDIA H100的GPU上进行推理延迟测量。

Result: 实验结果表明，zFLoRA在18个任务上的表现与LoRA及FFT相比具有竞争力，且推理延迟可实现零到微小的开销，且在1B/3B/7B规模的模型中均能实现该效果。

Conclusion: zFLoRA实现了在不增加推理延迟的前提下，提供与常用微调方法相当或更优的任务性能，具备在实际部署中广泛应用的潜力。

Abstract: Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [5] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 提出三项改进以提升机制可解释性中的电路发现：引入自放回抽样以筛选属性一致的边；采用基于比值的边筛选以平衡性能与信度；用整数线性规划取代贪婪法进行边的选择。结果显示在多个 MIB 任务/模型上构建的电路更可信且性能优于先前方法，且给出代码。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性中的电路发现面临噪声、边的属性分配不一致以及贪婪策略可能导致子最优电路的问题。需要更稳定、可复现且高信度的发现方法。

Method: 1) 使用自举(bootstrapping) 识别具有一致 attribution 的边；2) 引入基于比值的边筛选策略，优先选择强正向分数的边以平衡性能和信度；3) 将传统贪婪法替换为整数线性规划（ILP）形式来进行边的选取，以获得更优的全局解。

Result: 方法在多项 MIB 任务与模型上取得更具信度的电路，并且性能超越先前方法。

Conclusion: 该框架提升了机制可解释性中的电路发现的可信度与性能，具有良好的可重复性（代码可获取）。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [6] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: 提出 LISTEN 框架，利用大语言模型作为零-shot 偏好评估器，结合两种算法 LISTEN-U（基于参数化效用函数的迭代优化）和 LISTEN-T（小批量对决的非参数方法），在航班预订、购物和考试安排等任务中评估。LISTEN-U 在偏好与参数化效用对齐时表现出色，LISTEN-T 则更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在多目标决策中，专家往往难以从大量候选项中选出最佳方案，且难以对隐性偏好进行形式化表达。通过用自然语言描述高层次目标，并让 LLM 充当偏好评估器，可以降低传统偏好设定的认知负担和复杂性，同时考虑到 LLM 的上下文窗和推理成本等约束。

Method: 提出 LISTEN-U：使用 LLM 来迭代地优化一个参数化的效用函数，使其对专家的高层目标在给定任务上产生一致偏好。提出 LISTEN-T：非参数化方法，通过对小批量解空间进行对决式筛选，直接在局部区域内选择候选项，减少对明确偏好函数的依赖。两者都把 LLM 作为零-shot 偏好评估者，结合自然语言描述的目标来驱动选择。

Result: 在多任务设置（航班预订、购物、考试排程）上评估。结果显示：LISTEN-U 在偏好与参数化效用函数高度对齐时表现最佳（通过新颖的一致性度量 concordance 指标进行衡量），而 LISTEN-T 在更广泛且不强对齐的场景中具有更鲁棒的性能。总体而言，框架能够在不显式进行详细偏好标注的情况下，引导复杂多目标决策。

Conclusion: 将自然语言作为偏好输入直接驱动多目标决策具有潜力，LISTEN 提供了一条降低传统偏好 elicitation 认知负担的可行路径；未来可进一步提升对 LLM 偏好的不确定性处理、扩展到更大规模的候选集以及对不同任务的偏好一致性度量的稳健性。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [7] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

TL;DR: LongFilter是一种面向长上下文预训练的数据过滤框架，通过比较长上下文和短上下文下的模型预测来估计信息增益，从而筛选出依赖长距离信息的样本，以提升长上下文模型的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的长文本数据往往缺乏有意义的长距离依赖，导致大部分样本可用局部上下文即可预测，若在这类数据上训练效率低下。因此需要对数据进行有效筛选，聚焦真正需要长上下文的样本。

Method: LongFilter通过对同一样本在长上下文与短上下文设置下的预测差异进行对比，度量信息增益，从而识别需要长距离依赖的样本，进而用于数据筛选和 curate。

Result: 在将LLaMA-3-8B的上下文长度从8K扩展到64K的实验中，LongFilter能够高效筛选出高质量数据，并在 HELMET、LongBench、RULER 等基准上取得显著提升。

Conclusion: LongFilter为长上下文预训练提供了一种高效的数据筛选框架，能够显著提升在需要长距离依赖的任务上的表现，同时提升训练效率。

Abstract: Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [8] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: Persona conditioning in large language models used for content moderation yields subtle ideological biases: while overall accuracy may stay similar, larger models align more with the persona's political ideology, increasing within-ideology consistency and widening cross-ideology divergence; additional targeted tasks show defensiveness of one's own viewpoint and downplaying harms of opposing views.


<details>
  <summary>Details</summary>
Motivation: Ensure fairness and neutrality in automated content moderation, but persona-driven biases could reinforce partisan perspectives under the guise of neutrality.

Method: Systematically evaluate the impact of persona adoption across model architectures, sizes, and modalities (language and vision) using standard metrics, deeper agreement analyses, and a politically targeted auxiliary task.

Result: Headline accuracy is not dramatically affected by personas, but behavioral shifts emerge: larger models tend to align with personas of the same political ideology, increasing within-ideology agreement and widening cross-ideology divergence. In targeted tasks, personas defend their own stance and downplay harms of opposing views.

Conclusion: Persona conditioning can introduce subtle ideological biases into LLM outputs, raising concerns about neutrality and the potential reinforcement of partisan perspectives in AI moderation systems.

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [9] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

TL;DR: CLEAR 通过实体感知检索在临床问答中优于嵌入检索，在降低令牌数量的同时提升准确性，且对长文档表现尤为显著。


<details>
  <summary>Details</summary>
Motivation: 在EHR以FHIR DocumentReference的base64附件存储的背景下，语义问答难以实现；传统向量检索往往错过细粒度的临床关系，需要更高效、精准的检索机制。

Method: 提出 Clinical Entity Augmented Retrieval (CLEAR)，采用实体感知的检索策略，并与基于嵌入的检索进行对比。通过 Clinical Notes QA Evaluation Platform 验证 CLEAR，在12份临床笔记（10k–65k tokens）上评估，与零-shot大上下文推理和基于分块的RAG进行比较。指标包括F1、win rate、语义相似度以及令牌数量。

Result: CLEAR 的F1为0.90（嵌入检索为0.86），且令牌使用量减少约70%。在12份笔记上，win rate为58.3%，平均语义相似度为0.878，整体令牌使用量比广义上下文处理少78%。对65k+ токен的长文档，wins率达到75%。

Conclusion: 实体感知检索提升了临床自然语言处理中的效率与准确性，并且给出一个可重复的评估框架，用于评估在对语义精度和计算效率敏感的临床问答系统。

Abstract: Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [10] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

TL;DR: 该综述从数据导向角度系统梳理大模型微调后期阶段的数据高效性方法，提出数据高效LLM后训练的分类法（数据选择、数据质量提升、合成数据、数据蒸馏压缩、自进化数据生态），总结代表性方法并展望未来研究方向，旨在提升数据利用效率。


<details>
  <summary>Details</summary>
Motivation: 后训练阶段面临高昂标注成本与数据规模边际收益递减问题，亟需提高数据利用效率以降低成本并提升性能。

Method: 系统性综述，提出以数据为核心的分类法覆盖数据选择、数据质量提升、合成数据、数据蒸馏与压缩、以及自进化数据生态等维度，梳理各类别内的代表性方法，分析挑战并指明未来研究方向。

Result: 整理并汇总现有数据高效后训练方法，给出数据驱动设计的研究路线，明确未解决的关键问题与潜在研究方向，为提升大规模模型训练中的数据利用效率提供指南。

Conclusion: 数据高效后训练具有广泛前景；需要更多系统性研究以完善数据质量评估、跨任务泛化、风险控制（如偏见与数据污染）、以及与自监督与监督学习的协同机制等。

Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [11] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 通过对比人工、半自动、全自动三种注释设置，评估基于LLM的语义角色标注在FrameNet风格标注上的效能。结果显示半自动设置提升帧多样性，覆盖率与人工相近；全自动在大多数指标上表现较差，只有注释时间更短。


<details>
  <summary>Details</summary>
Motivation: 弥补当前对LLM在语言资源创建中的系统性评估不足，尤其是在以观点化（perspectivized）NLP为视角的注释任务中的影响与潜力。

Method: 在三种注释模式下对比评估：人工、自动、半自动，使用LLM驱动的语义角色标注器对FrameNet式语义标注进行注释，比较注释时间、覆盖率和多样性等指标。

Result: 半自动注释在帧种类的多样性方面优于人工，覆盖率相当；全自动在多数指标上表现较差，唯一优势在注释时间。

Conclusion: LLM驱动的半自动化标注在提升资源多样性方面具有潜力，尤其适合需要降低人工成本且对帧多样性有要求的场景。然而需要关注自动化程度与标注质量之间的权衡，并在提示设计、数据偏见和跨域泛化方面进行更广泛的验证。

Abstract: The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [12] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

TL;DR: 在1.1B和3B参数的模型上，使用多语言数据混合进行预训练，并不显著削弱单语言的性能；通过使用英语作为枢纽语言可提升跨语言家族的泛化能力；随着训练语言数量增加，并未观察到明显的多语言诅咒。对齐平衡的多语言数据可以在资源匮乏场景中提升能力。


<details>
  <summary>Details</summary>
Motivation: 系统性检验多语言数据混合对大规模语言模型（LLMs）在语言覆盖和性能之间权衡的影响，挑战“多语言诅咒”的普遍观点。

Method: 训练1.1B和3B参数的LLMs，使用多样的多语言语料，语言数量从25到400不等；比较英语+多语言数据的组合效果，研究英语作为枢纽语言的作用，以及在同族语言中的枢纽选取是否影响性能；通过多语言覆盖与平衡的预训练来评估在不同语言资源条件下的表现。

Result: 结论包括：1) 只要各语言在预训练语料中具有足够的令牌量，英语与多语言数据的混合并不必然降低任一组语言的性能；2) 使用英语作为枢纽语言能带来跨语言族的正向影响，且同族内选取枢纽语言不稳定地提升该族语言性能；3) 随着训练语言数量增加，在该规模下并未观察到显著的“多语言诅咒”；4) 经过平衡的多语言数据能够提升模型能力，即使在低资源场景也如此。

Conclusion: 多语言数据若实现适度平衡，预训练对模型能力并不受制于语言数量的增加，且英语枢纽促进跨族泛化，挑战了“多语言诅咒”的普遍认知，建议在数据混合时关注令牌覆盖平衡与跨语言枢纽策略。

Abstract: The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [13] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本研究揭示在机器翻译中，源目标语言的文化差异可引起语义标签漂移；LLMs 的文化知识会放大漂移；语言间的文化相似度显著影响标签的保持性。


<details>
  <summary>Details</summary>
Motivation: 长期关注情感保持的 MT 研究忽视了文化对语义标签的影响。随着低资源语言的数据不足，利用高资源语言生成合成数据时，若未考虑文化差异，可能引入标签错配，影响下游应用的解释与合规性。

Method: 在跨文化敏感与中性领域的多域实验中，比较了不同 MT 系统（包括现代大型语言模型）对标签漂移的影响，分析了文化知识的作用及源/目标语言的文化相似性对标签保持的作用。

Result: (1) MT 系统及 LLMs 在翻译时会导致标签漂移，尤其在文化敏感领域；(2) 与早期统计 MT 工具不同，LLMs 捕获文化知识，这些知识可被用来放大漂移；(3) 源/目标语言间的文化相似性显著决定标签的保持性。

Conclusion: 忽视文化因素会削弱标签保真度，增加下游应用的误解和文化冲突风险。应在 MT 与标注任务中纳入文化对齐考量，并在跨域评估中重视对文化差异的管理。

Abstract: Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [14] [Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs](https://arxiv.org/abs/2510.26512)
*Dipak Meher,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 通过对 CORE-KG 的两大模块（类型感知的核心指代与领域引导的结构化提示）的系统消融，量化了它们对降低节点重复和噪声的贡献，核心指代的移除显著增加节点重复，结构化提示的移除则显著增加噪声。


<details>
  <summary>Details</summary>
Motivation: 解决法律文本中难以结构化、词汇密集且指代变化的特征导致的知识图构建挑战。现有的 LLM 方法在缺乏指引提取和核心指代的情况下容易产生重复节点和噪声。CORE-KG 通过引入类型感知的核心指代和领域引导的结构化提示来提升提取质量，因此需要通过消融研究定量评估两者的贡献。

Method: 对 CORE-KG 进行两项关键消融实验：1) 移除核心指代模块，评估对节点重复和噪声节点的影响；2) 移除结构化提示模块，评估同样指标的影响。以节点重复率和噪声节点比率为主要评价指标。

Result: 移除核心指代：节点重复增加 28.32%，噪声节点增加 4.32%。移除结构化提示：节点重复增加 4.34%，噪声节点增加 73.33%。

Conclusion: 两大模块均对提升结构化表示的质量有显著作用。核心指代对减少节点重复贡献更大，结构化提示对降低噪声贡献更显著。该结果为设计鲁棒的基于大语言模型的法律文本结构化流程提供了实证指引。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.

</details>


### [15] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: SymCode 将数学推理从文本推理转化为可验证的代码生成，借助 SymPy 的确定性计算，实现更可靠的推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂数学推理中常出现未经验证且算术不正确的解答，现有的链式思维等 prompting 仍然在不可靠的媒介内工作，缺乏确定性验证机制。

Method: 提出 SymCode 这一神经符号框架，将数学问题解决重构为可验证的代码生成任务，使用 SymPy 作为确定性计算引擎，以程序化方式进行推理与验证。

Result: 在 MATH-500 与 OlympiadBench 等挑战性基准上，准确性提升可达 13.6 个百分点；相比基线，符号化与代码化推理更具 token 效率；模型失败类型从难以解释的逻辑谬误转向透明的程序错误。

Conclusion: 将 LLM 推理绑定到确定性的符号计算引擎，是在正式领域提升准确性与可信度的重要一步。

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [16] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 为 Trainium 上的 LLM 推理设计高性能矩阵乘法（matmul）内核，通过内核融合与创新缓存策略降低数据移动、提升 SRAM 带宽并避免转置，实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: Trainium 的 systolic array 架构和数据布局要求使高性能实现具有挑战性。需要通过定制化优化减少数据搬运、最大化缓存/ SRAM 带宽、并避免昂贵的矩阵转置以提升推理性能。

Method: 提出面向 Trainium 的高性能矩阵乘法实现，结合内核融合与新颖缓存策略，优化数据在软件管理内存层级的移动，提升缓存命中率与 SRAM 访问效率，并避免不必要的矩阵转置以降低数据重排开销。

Result: 在九个数据集和四个最近的LLMs上评估，矩阵乘法内核平均提升1.35x（最高2.22x），端到端推理平均提升1.66x（最高2.49x），显著超越 AWS 上现有的 matmul 实现。

Conclusion: 通过针对 Trainium 架构的定制化内核融合与缓存策略，显著提升 matmul 性能与端到端推理效率，优于 AWS 提供的现有实现。

Abstract: AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [17] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: AttnCache speeds up the prefill stage of LLM inference by caching and reusing similar attention maps, reducing self-attention computation with minor accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Prefill-only workloads (classification, QA, embedding) incur high self-attention costs due to its quadratic complexity. Semantically different sentences often yield similar attention maps across layers/heads, suggesting caching could accelerate inference.

Method: Build an attention map memorization database and use efficient caching plus similarity search to identify and reuse pre-cached attention maps during prefill, reducing expensive self-attention computations.

Result: On CPU: ~1.2x end-to-end speedup and ~2x attention speedup; On GPU: ~1.6x end-to-end speedup and ~3x attention speedup; negligible degradation in accuracy.

Conclusion: AttnCache demonstrates effective acceleration of the prefill stage with minimal accuracy loss, offering practical speedups for real-world LLM workloads that rely on prefill.

Abstract: Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [18] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: PORTool通过在强化学习中引入分叉树状的多路径rollouts和逐步奖励，训练工具使用型大语言模型以探索多条潜在解路径，从而提高最终准确性并减少工具调用次数。


<details>
  <summary>Details</summary>
Motivation: 当前的工具使用型LLM通常按静态的工具调用流程进行模仿，缺乏在动态工具环境中的探索，导致性能受限。需要通过探索性训练来发现多样的解法并提升效率。

Method: 对于给定查询，先生成多条rollout，形成树状结构（同一分叉在不同路径共享前几步）。为每一步分配奖励，基于其产生正确答案和成功执行工具调用的能力。跨路径共享的步骤获得相同奖励，同一分叉下的不同步骤获得不同奖励。然后将这些逐步奖励用于计算分叉相对优势，并与轨迹相对优势混合，用于训练LLM的工具使用。实验使用17种工具，覆盖时效性与非时效性主题，并通过消融研究验证逐步奖励的必要性与鲁棒性，最后与其他训练方法对比。

Result: PORTool在最终准确性和工具调用步数方面显著优于对比方法，且在包含17种工具的设置中表现良好。消融实验证实了逐步奖励设计的必要性与鲁棒性，并显示相较于基线训练方法的改进。

Conclusion: 通过引入分叉相对奖励和逐步奖励，PORTool有效促使LLM在工具使用任务中进行探索，提升准确性与效率，具备在多工具、动态环境中的适用性与泛化潜力。

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [19] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

TL;DR: 提出一个转移-本地化平面评估框架，以量化跨语言知识迁移的有效性与文化本地化的丧失之间的权衡；在六种语言的实验中，现有跨语言对齐方法普遍提升事实迁移但以牺牲文化定位为代价；并提出在推理阶段对模型不同层实施定向激活操控的“手术性引导”（Surgical Steering），以在两者之间取得更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 跨语言对齐旨在使多语言表示趋于对齐，从而实现知识在语言之间的无缝传递；但过度追求表征的同构性可能导致“文化涂抹”或文化特定响应的丧失。本研究提出一个综合评估框架，以同时衡量知识迁移与文化本地化。

Method: 提出转移-本地化平面，量化知识迁移与文化本地化之间的权衡。对六种语言的 CLA 方案进行系统再评估，并通过分析模型内部表示，发现普遍事实层面迁移与文化特定知识可在不同模型层中被分离控制。基于此，提出推理时的 Surgical Steering，对不同层实施目标激活以分别控制两类信息。

Result: 实验结果表明，现有 CLA 往往在提升跨语言事实迁移的同时，显著降低文化本地化能力；通过对层级进行定向操控，Surgical Steering 能在两者之间取得更好的平衡，优于现有对齐技术。

Conclusion: 研究给出一个可操作的框架与方法，证明在推理阶段通过分层控制可以缓解跨语言对齐中的文化丢失问题，为未来的跨语言对齐研究提供方向。

Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [20] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 提出 QCoder 基准，面向量子编程的 LLM 评估框架，通过量子模拟器反馈评估并收集人类代码提交，揭示不同模型在该领域的能力差异。


<details>
  <summary>Details</summary>
Motivation: 在需要与硬件设备交互的领域（如量子编程）中，现有自动编程评估多聚焦在纯软件执行，缺乏硬件层面的反馈，影响对模型实际能力的评估与提升。

Method: 建立量子模拟环境的评估框架，支持对电路深度、执行时间、错误分类等域特定指标的反馈；集成来自真实编程竞赛的人类代码提交；提供公开数据集和评测 API；对多种模型进行评估并与人类代码比较。

Result: 实验显示即便是 GPT-4o 也仅约 18.97% 的准确率；而推理型模型如 o3 最高可达约 78%，优于人类代码的平均水平（约 39.98%）。同时发布数据集与评测 API。

Conclusion: QCoder 基准为量子编程等硬件交互领域的 LLM 评估提供了新的量化与对比工具，凸显需要以推理能力驱动的模型，并为未来改进提供基线和数据资源。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [21] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

TL;DR: 提出1PNS训练范式，通过RPD度量实现推理路径多样性，提升LLM在多步推理中的输出多样性和通过率；在Qwen3-4B-Base上实验，显著优于1P1S，并在AIME24有更大提升，且开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决1P1S训练下推理路径单一导致的输出多样性不足问题；提高推理过程的多样性与鲁棒性；需要一个可对齐并评分多步推理的度量来筛选多样性解。

Method: 提出1PNS训练范式；引入Reasoning Path Divergence (RPD)作为逐步对齐并评分长链路推理的度量；据此从每个问题中筛选最大程度的多样解集合；对Qwen3-4B-Base进行微调。

Result: RPD选择的训练产生更丰富的输出，pass@k提高；与1P1S基线相比，pass@16平均提升2.80%；在AIME24上提升4.99%。

Conclusion: 1PNS进一步放大TTS的效果，证明通过多样化推理路径可提升LLM的推理能力与正确率；代码公开。

Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [22] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

TL;DR: SDM LM 将解码器型语言模型通过监督微调，利用最终层的 SDM 激活在高概率、校准区间内进行序列预测，并在训练中配合对比输入编码和在线硬负样本，显著降低放弃率，提高统计效率。


<details>
  <summary>Details</summary>
Motivation: 提高生成在高概率、经过校准的区域中的比例，降低 abstentions（放弃）并提升统计效率；通过引入最终层 SDM 激活作为二元分类边界，结合对比学习的训练策略来优化下一个-token 预测。

Method: 对现有的预训练解码器 Transformer LM 进行监督微调，在训练中使用最终层的 SDM 激活来估计对监督下一个 token 损失的基变换；采用对比输入编码，并在线生成额外的硬负样本来提升训练信噪比。

Result: 与强基线相比，SDM LM 显著降低放弃率（abstentions），提升统计效率，改善对指令遵循任务的稳定性与覆盖。

Conclusion: 通过将 SDM 激活层引入训练并结合对比学习，可将解码器型 LM 有效转化为 SDM LM，从而在需要高置信度区间的生成任务中获得更好的性能与可靠性。

Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [23] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

TL;DR: 提出并通过 Token Timestep Allocation(TTA)缓解扩散语言模型中的更新遗忘，通过为每个 token 指定时间步调度实现软性、语义化排序，从而提升可控性与流畅性。


<details>
  <summary>Details</summary>
Motivation: 现有 DLM 在细粒度控制时易发生更新遗忘：均匀且与上下文无关的更新在不同时间步导致 token 层级波动，擦除早期修改，破坏累积 refinement，降低流畅性与一致性。

Method: 提出 TTA：对每个 token 指定一个时间步 schedule，使关键 token 先冻结，尚不确定的 token 继续 refinement。可采用固定策略或基于任务信号的自适应策略，且仅在推理阶段实施，适用于不同 DLM 与监督信源。

Result: 情感控制任务中准确率提升>20%、困惑度接近减半，且所需步骤不到原始的五分之一；毒性控制任务中最大毒性下降、困惑度下降，具体数值为 12.2 vs 14.5，26.0 vs 32.0。

Conclusion: 软化排序的时间步分配是缓解更新遗忘、实现稳定与可控扩散文本生成的关键杠杆。

Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [24] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出 GlobalQA 基准与 GlobalRAG 框架，显著提升全局RAG任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评测侧重局部检索，难以支持跨整合集合的全局推理与统计性任务；缺乏用于整文献集合级别分析的评测与方法。

Method: GlobalQA 定义四类全局任务（计数、极值、排序、top-k），并构建 GlobalRAG 的多工具协作框架，保持分块检索的结构一致性，使用LLM驱动的去噪过滤，并接入聚合模块实现精确的符号计算。

Result: 在 Qwen2.5-14B 上，GlobalRAG 达到 6.63 F1，相较 strongest baseline 的 1.51 F1 有显著提升。

Conclusion: 表明全局RAG能力可以通过分块检索+智能过滤+聚合模块实现，GlobalRAG 提供有效路径来解决基于整篇文献集合的全局分析任务。

Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [25] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 让语言模型通过具备的语用学理论提示来提升对隐含意义的推理能力；明确理论提示能显著提升性能，甚至仅提及理论名称也有收益。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在隐含意义理解方面的局限，利用已有的语用理论作为先验知识来引导模型推理。

Method: 将格赖斯语用理论（Gricean pragmatics）和相关性理论（Relevance Theory）的概述作为提示，采用分步推理引导模型得出最终解释；与不包含语用理论的0-shot Chain-of-Thought基线比较；并测试在更大的模型中仅提及理论名称的效果。

Result: 与基线0-shot CoT相比，pragmatic-theory prompting在语用推理任务上提升最高可达9.6%；仅在提示中提及理论名称也能在较大模型中带来约1-3%的性能提升。

Conclusion: 明确的语用理论提示是提升语言模型语用推理能力的有效且简便的方法，可作为在-context学习的一种实用范式。

Abstract: The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [26] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

TL;DR: 预训练语言模型在跨10种语言的情境下难以将借词与本土词区分开来，且对借词存在偏向，即使在明确指令下也如此。这给少数语言的自然语言处理带来挑战。


<details>
  <summary>Details</summary>
Motivation: 检验现代大模型是否能捕捉到语言借用现象、识别借词，这对在被主流语言施压的社区中保护和发展少数语言具有重要意义。

Method: 评估多种模型在10种语言上的借词与本地词区分能力，提供明确的提示和上下文信息以诱导模型输出。

Result: 模型在区分借词与本地词方面表现差，且存在对借词的偏向；这一结果与先前关于NLP系统偏向借词的证据相一致。

Conclusion: 研究提示需开发更适合少数语言的NLP工具，并在模型设计与评估中考虑借词动态，以支持在主导语言压力下的语言保护。

Abstract: Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [27] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: KD across multilingual VLMs reveals trade-offs under model compression: some distillation setups retain or improve multilingual retrieval robustness when halving size, while others harm cross-task stability; accuracy alone hides these design-sensitive effects.


<details>
  <summary>Details</summary>
Motivation: VLMs show uneven language performance and compression via knowledge distillation is a promising but underexplored approach for multilingual models. A controlled study is needed to understand how different KD formulations affect cross-lingual representation consistency and downstream stability under size reduction.

Method: Evaluate five distillation formulations applied to CLIP and SigLIP2. Assess both in-domain retrieval and out-of-domain visual QA. Analyze cross-lingual representation consistency and downstream performance stability under model compression (halving model size).

Result: Some KD configurations preserve or even improve multilingual retrieval robustness despite halving model size; others fail to maintain cross-task stability. The findings reveal design-sensitive trade-offs that accuracy alone does not reveal.

Conclusion: Knowledge distillation for multilingual VLMs must consider cross-lingual representation consistency and cross-task stability in addition to accuracy. The study offers guidance on robust KD configurations under compression and highlights trade-offs between retrieval robustness and cross-task stability.

Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [28] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

TL;DR: NAD: 在不依赖外部文本输出的情况下，基于神经元激活的稀疏性和跨样本一致性进行候选选择的无监督解码方法，实现早期正确性预测和大幅降低令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 现有解码信号（如 token 概率、熵、自评估）在后训练后可能标定不足，且受限于外部输出。通过分析内部神经元激活模式，挖掘更丰富的内部信息用于无标签的集成解码。

Method: 对生成过程中的神经元激活进行分析，提出 Neuron Agreement Decoding (NAD)；以内部信号为基础，选取候选答案，采用最佳-来自-N的策略，无需对齐的文本输出；强调激活稀疏性与跨样本神经元一致性，用于早期判断和可能的提前停止。

Result: 在可验证答案的数学与科学基准上，NAD 与多数投票等效；在需要开放式编码的基准上，NAD 优于 Avg@64；通过提前筛选不良轨迹，将令牌使用量削减约99%，生成质量损失极小；可在前32个 token 内做到正确性预测并支持早停。

Conclusion: 内部信号可提供可靠、可扩展且高效的无标签集成解码线索，NAD 为基于内部激活的解码提供了有效路径，并显著降低推理成本。

Abstract: Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [29] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: LLMs在数字表示上形成普遍、准确的嵌入，但表面上仍会出现错误，并据此建立通用探针以追踪数字信息在各层的流动，从而为改进数值推理提供路径。


<details>
  <summary>Details</summary>
Motivation: 试图解释为何先前发现的数字嵌入与易出错的数值输出之间存在矛盾；量化这些机制的下界，并理解模型在数字处理上的内部运作。

Method: 分析多种LLM中的数字表示是否可互换、是否系统化且普遍存在于隐藏层和不同输入上下文中；为每个模型设计通用探针，并追踪信息来源和错误原因至具体层。

Result: 发现不同模型学习到的数字表示可互换、具有系统性且高精度，且跨隐藏状态和输入上下文类型保持普适性；可用统一探针对各模型进行诊断，并将信息的错误原因追溯到特定层。

Conclusion: 为预训练LLM在数字处理上的运作提供基础性理解，并指出更高精度探测技术在后续模型架构 refinements 中的潜在应用。

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [30] [Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games](https://arxiv.org/abs/2510.26298)
*Jingran Zhang,Ning Li,Justin Cui*

Main category: cs.CL

TL;DR: Atlas在网页交互方面展现出强大的分析推理能力（如数独），但在需要实时交互的动态浏览环境中存在显著局限，难以完成实时操作型游戏的任务。


<details>
  <summary>Details</summary>
Motivation: 评估Atlas在动态、交互式网页环境中的能力，弥补以往主要关注信息检索任务的研究空缺。

Method: 以基于浏览器的游戏作为测试场景，选取Google的T-Rex Runner、数独、Flappy Bird、Stein.world，利用游戏内分数作为定量指标，对不同任务类型的表现进行评估，进行初步评估。

Result: 在逻辑推理任务（数独）上表现出色，完成题目速度显著高于人类基线；在需要实时反应、精准时序和运动控制的实时游戏中表现欠佳，常在初始障碍前就止步。

Conclusion: Atlas展现出较强的分析处理能力，但在需要实时交互的动态网页环境中仍存在明显局限，提示未来改进方向在于实时动作控制和时序协作的能力提升。

Abstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,
enabling the model to analyze webpages, process user intents, and execute
cursor and keyboard inputs directly within the browser. While its capacity for
information retrieval tasks has been demonstrated, its performance in dynamic,
interactive environments remains less explored. In this study, we conduct an
early evaluation of Atlas's web interaction capabilities using browser-based
games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,
and Stein.world. We employ in-game performance scores as quantitative metrics
to assess performance across different task types. Our results show that Atlas
performs strongly in logical reasoning tasks like Sudoku, completing puzzles
significantly faster than human baselines, but struggles substantially in
real-time games requiring precise timing and motor control, often failing to
progress beyond initial obstacles. These findings suggest that while Atlas
demonstrates capable analytical processing, there remain notable limitations in
dynamic web environments requiring real-time interaction. The website of our
project can be found at https://atlas-game-eval.github.io.

</details>


### [31] [SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling](https://arxiv.org/abs/2510.26322)
*Fares Fawzi,Vinitra Swamy,Dominik Glandorf,Tanya Nazaretsky,Tanja Käser*

Main category: cs.CL

TL;DR: 提出 SCRIBE 框架，通过小型模型实现多步推理、工具辅助及自我反思，以在本地低资源、隐私敏感的教育场景中对学生反馈进行高质量回答，与更大模型相比在相关性与可操作性方面表现接近甚至优于，并在教师与学生中获得良好感知。


<details>
  <summary>Details</summary>
Motivation: 解决现实部署中的隐私、安全性、计算资源受限以及需要对教学有效性的要求；需要可本地运行、开源、能以域知识工具为支撑的可证伪回答。

Method: 提出 SCRIBE，结合多跳推理、工具增强、以及自我反思推理流程，支持迭代推理、工具使用与错误恢复。通过两阶段 LoRA 微调，以合成的 GPT-4o 产出数据，得到 3B 与 8B 模型。对比评估用人类对齐的 GPT-Judge 与 108 名学生的用户研究。

Result: 8B-SCRIBE 在相关性和可操作性等关键维度上，与更大模型相比达到相当或更优的质量；在学生眼中，其感知水平等同于 GPT-4o 与 Llama-3.3 70B。

Conclusion: 验证了 SCRIBE 在低资源、隐私敏感教育应用中的可行性，并展示了在有限规模模型上实现高质量、可用的对学生反馈的潜力。

Abstract: Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.

</details>


### [32] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: ACER通过自动化的课程化预训练，将通用型大模型转化为领域专家，同时保持通用能力，显著提升对经济学、心理学等领域的专门知识表现，并具备跨领域迁移与防止灾难性遗忘的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型在经济学、心理学等需要扎实概念与推理的领域表现不足，需一个可扩展的、系统的学习路径来弥补知识空白。

Method: 先生成主题教材式的课程大纲（Table of Contents），然后按照布鲁姆分类法设计问答对，形成一个合成语料库；通过一个交错（interleaved）的课程表进行持续预训练，以内容与认知维度双向对齐；在多种规模的Llama 3.2上验证。

Result: 在1B和3B的Llama 3.2上，对专门领域的MMLU子集取得显著提升：微观经济学+5个百分点，所有目标领域宏观平均+3个百分点；跨领域知识迁移提升0.7点；在ARC、GPQA等知识密集基准上提升超2个点，同时保持一般推理任务的稳定性。

Conclusion: ACER提供一种可扩展、有效的路线上，能够弥合大模型在关键领域的知识差距，防止遗忘并促进跨域学习，具有普适性和实用性。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [33] [MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data](https://arxiv.org/abs/2510.26345)
*Mykhailo Poliakov,Nadiya Shvai*

Main category: cs.CL

TL;DR: MisSynth 使用检索增强生成（RAG）生成合成的谬误样本，并对 LLM 进行轻量微调，以提升对 MISSCI 数据集中的 fallacy 的识别能力；在健康相关误信息任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 健康相关的错误信息普遍且具有潜在危害，识别困难，尤其当陈述曲解科学发现时。需要通过合成数据和轻量化微调提升 LLM 的识别能力。

Method: 提出 MisSynth 管线：使用检索增强生成（RAG）产生合成 fallacy 样本，并用它们微调 LLM（如 LLaMA 3.1 8B），在 MISSCI 上评估并与基线比较。

Result: 微调后的模型在 MISSCI 测试集上显著优于基线，例如 LLaMA 3.1 8B 提升约 35 个 F1 点；证明在有限 annotated 资源下，合成数据可以显著提升零-shot 分类性能。

Conclusion: 合成 fallacy 数据对提升真实世界科学误信息任务的 LLM 表现具有显著贡献，提出一种可扩展的低资源解决方案，代码与数据公开。

Abstract: Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.

</details>


### [34] [The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration](https://arxiv.org/abs/2510.26352)
*Kotaro Furuya,Yuichi Kitagawa*

Main category: cs.CL

TL;DR: 基于语言模型图和社区检测的自动化多智能体团队组成框架，无需内部知识即可发现协同模型群体。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体协作中因缺乏透明性、无法获取模型内部架构与训练数据而导致的团队组建难题，提供无需先验信息的自动化方法。

Method: 构建“语言模型图”，通过模型之间对话语义相似性映射关系；对图应用社区检测以识别功能相近的模型聚类；在不同话题下对团队进行引导对话，比较与随机基线及人工挑选团队的性能。

Result: 发现具有功能一致性的模型群体，反映潜在专长；经特定话题引导的对话提升协作效果，超越随机基线，接近基于已知专长的人工组队表现。

Conclusion: 提出一种无需内部信息即可自动设计协作型多智能体LLM团队的范式，利用对话语义关系来揭示有效的团队结构。

Abstract: While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.

</details>


### [35] [On the Role of Context for Discourse Relation Classification in Scientific Writing](https://arxiv.org/abs/2510.26354)
*Stephen Wan,Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 初步研究表明，在科学文本的论证关系分类（DRC）中，使用预训练语言模型（PLM）和大语言模型（LLM）具有潜力；将论述结构所定义的上下文纳入可显著提升DRC表现，并分析了哪些关系类型从上下文中获益最大。


<details>
  <summary>Details</summary>
Motivation: 利用话语层级信息来发现并验证AI生成的科学主张的证据路径；在科学论文这一相对较少研究的领域中，探究论述关系分类的可行性与效果，以支持后续的证据检索和解释。

Method: 对PLM和LLM在科学论文中的论述关系分类任务进行初步实验，研究上下文（按论述结构定义）对DRC的帮助作用；比较不同关系类型在上下文中的受益情况，探索哪类关系最能从上下文获益。

Result: 整体上，背景信息（上下文）对DRC有帮助，且不同的论述关系类型对上下文的依赖程度不同；LLM在某些设置下与PLM相当或更具优势。

Conclusion: 论述结构中的上下文信息对科学写作中的DRC具有积极作用，提示后续研究应进一步挖掘关系类型差异和更高效地利用 discourse 结构来提升DRC性能。

Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.

</details>


### [36] [OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education](https://arxiv.org/abs/2510.26422)
*Min Zhang,Hao Chen,Hao Chen,Wenqi Zhang,Didi Zhu,Xin Lin,Bo Jiang,Aimin Zhou,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 提出 OmniEduBench 的中国教育领域基准，覆盖知识维度与培养维度共61个科目、11种题型的多样评测，填补现有大语言模型评估在教育场景中对培养能力的关注不足。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在教育评估中多聚焦知识维度，忽略对培养能力的评估，以及跨学科和多题型的评测缺乏，且在中文教育场景中的数据和对比稀缺，亟需一个覆盖知识与培养双维度、具多样题型的中文教育基准。

Method: 构建 OmniEduBench 数据集：约2.46万条问答，知识维度18.121K条、培养维度6.481K条；两大维度各分6个细分类别，覆盖61个科目（知识41、培养20），包含11种常见考试题型；在11种主流开源/闭源 LLM 上进行评测。

Result: 在知识维度，只有 Gemini-2.5 Pro 超过60%准确率；在培养维度，最佳模型为 QWQ，仍落后于人类约30%；显示出模型在教育场景下的显著提升空间和对培养能力评估的挑战。

Conclusion: 提出了一个更全面的中文教育基准，强调需关注知识与培养双维度、题型多样性，以及跨模态和跨科目的评测，以推动教育场景中 LLMs 的实际应用与改进。

Abstract: With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.

</details>


### [37] [1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models](https://arxiv.org/abs/2510.26446)
*Zeliang Zong,Kai Zhang,Zheyang Li,Wenming Tan,Ye Ren,Yiyan Zhai,Jilin Hu*

Main category: cs.CL

TL;DR: SSLC 将低秩近似与稀疏剪枝进行统一优化，显著压缩大语言模型并提升推理效率，且无需额外训练，对 LLaMA 与 Qwen2.5（7B–70B）表现优于单一方法；对 Qwen2.5 50% 压缩无性能下降，且至少 1.63× 加速。


<details>
  <summary>Details</summary>
Motivation: 尽管剪枝和低秩近似各自有效，但两者的协同作用尚未被充分挖掘。大语言模型的带宽与计算成本限制其广泛应用，因此需要在不显著损失精度的前提下提高压缩率与推理速度。

Method: 基于理论分析，将低秩近似与稀疏优化统一为一个优化问题，并通过迭代优化算法进行求解；提出一个无额外训练步骤即可实现的联合压缩流程，适用于多种大模型。

Result: 在 LLaMA 与 Qwen2.5（7B–70B）上的实验表明，SSLC 相较于单一方法具有一致的性能提升，且达到或接近状态最优；对 Qwen2.5 实现 50% 压缩且无性能下降，且获得至少 1.63× 的加速。

Conclusion: SSLC 提供了一个可实践的高效压缩框架，充分发挥低秩与稀疏的协同优势，便于在实际部署中降低带宽和计算需求，未来可扩展到更多模型与任务。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.

</details>


### [38] [Bayesian Network Fusion of Large Language Models for Sentiment Analysis](https://arxiv.org/abs/2510.26484)
*Rasoul Amirzadeh,Dhananjay Thiruvady,Fatemeh Shiri*

Main category: cs.CL

TL;DR: BNLF: a Bayesian-network late fusion framework for sentiment analysis that combines FinBERT, RoBERTa, and BERTweet predictions to achieve about 6% accuracy improvements across financial corpora, with robust performance and interpretable fusion.


<details>
  <summary>Details</summary>
Motivation: Address lack of transparency, high tuning costs, prompt engineering, and environmental impact of domain-specific LLMs; improve cross-domain robustness and interpretability in sentiment analysis.

Method: Late fusion via a Bayesian network where the sentiment predictions from multiple LLMs (FinBERT, RoBERTa, BERTweet) are modeled as probabilistic nodes and fused probabilistically to produce final sentiment labels.

Result: Empirically evaluated on three human-annotated financial corpora with varying linguistic context; BN-LNF yields ~6% accuracy gains over baseline LLMs, demonstrating robustness to dataset variability.

Conclusion: Probabilistic fusion within a Bayesian network enhances sentiment classification accuracy and interpretability, offering a robust approach for cross-domain sentiment analysis in finance.

Abstract: Large language models (LLMs) continue to advance, with an increasing number
of domain-specific variants tailored for specialised tasks. However, these
models often lack transparency and explainability, can be costly to fine-tune,
require substantial prompt engineering, yield inconsistent results across
domains, and impose significant adverse environmental impact due to their high
computational demands. To address these challenges, we propose the Bayesian
network LLM fusion (BNLF) framework, which integrates predictions from three
LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic
mechanism for sentiment analysis. BNLF performs late fusion by modelling the
sentiment predictions from multiple LLMs as probabilistic nodes within a
Bayesian network. Evaluated across three human-annotated financial corpora with
distinct linguistic and contextual characteristics, BNLF demonstrates
consistent gains of about six percent in accuracy over the baseline LLMs,
underscoring its robustness to dataset variability and the effectiveness of
probabilistic fusion for interpretable sentiment classification.

</details>


### [39] [A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool](https://arxiv.org/abs/2510.26498)
*Adam E. Flanders,Yifan Peng,Luciano Prevedello,Robyn Ball,Errol Colak,Prahlad Menon,George Shih,Hui-Ming Lin,Paras Lakhani*

Main category: cs.CL

TL;DR: 本研究探索将8个开源大模型与GPT-4o的集成用于评估腹内出血(ICH)AI分诊工具，发现中等到大型模型的集成在评估Ground Truth方面比单一模型更稳定可靠，最高AUC约0.78，Top-3/Full-9/Consensus三者之间差异不显著。


<details>
  <summary>Details</summary>
Motivation: 旨在提高对临床AI分诊工具的地面真值评估的鲁棒性，减少单一LLM带来的波动和偏差，通过多模型共识来提高评估的一致性与可靠性。

Method: 对14家医院的29,766例头部CT进行ICH检测工具分析；用单一多-shot提示让8个开源LLM及合规的GPT-4o对报告进行ICH存在性判断，并对1,726例进行人工复核。比较8个开源模型及共识与GPT-4o的性能，测试三种理想共识集成（Full-9、Top-3、Consensus）的评估效果，使用AUC、AP、F1、召回、精确度、特异性和MCC及95%CI等指标。

Result: 最高AUC为llama3.3:70b与GPT-4o并列，AUC=0.78；平均精度在Llama3.3:70b和GPT-4o分别为0.75和0.76。Llama3.3:70b的F1为0.81、召回0.85、精确度0.78、特异性0.72、MCC0.57。理想共识组合的MCC为Full-9 0.571、Top-3 0.558、Consensus 0.556，GPT-4o为0.522；Top-3、Full-9与Consensus之间无统计学显著差异(p>0.05)。

Conclusion: 中等至大型开源LLM的集成能比单一LLM在临床AI分诊工具的地面真值评估中提供更一致、可靠的结果。

Abstract: Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.

</details>


### [40] [Hebrew Diacritics Restoration using Visual Representation](https://arxiv.org/abs/2510.26521)
*Yair Elboher,Yuval Pinter*

Main category: cs.CL

TL;DR: DIVRIT 将希伯来文元音标记任务视为零-shot 的单词级分类，通过一个将未元音化文本作为图像输入的可视语言模型实现，从上下文中动态生成候选集并选择最合适的音标模式；在 oracle 场景下达到高准确度，并通过结构改进与训练策略显著提升泛化能力，展示视觉表示在希伯来文元音标记中的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决未元音化希伯来文本的歧义性问题，减少对复杂语言学分析的依赖，充分利用视觉语言模型的跨模态能力来直接嵌入音标信息。

Method: 将任务建模为零-shot分类，逐词在给定的候选集合中选取音标模式；将未元音化的单词作为图像输入到 Hebrew Visual Language Model；利用周围上下文条件化候选集合生成；通过动态候选集、输入图像化处理等实现。

Result: 在多种配置下，系统无需复杂的语言分析即可完成标注；在“oracle”设定中正确音标一定在候选集合时取得高准确率；架构增强和优化训练提高了泛化能力。

Conclusion: 视觉表示在希伯来文音标标注中具有潜力，未来可进一步探索图像化输入与零-shot 分类的结合。

Abstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.

</details>


### [41] [Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models](https://arxiv.org/abs/2510.26577)
*Yinrong Hong,Zhiquan Tan,Kai Hu*

Main category: cs.CL

TL;DR: CAST 提出了一种考虑推理成本的动态树解码方法，通过将 GPU 配置和批量大小等因素纳入树结构优化，在六个任务和六个 LLM 上实现高达 5.2x 的加速，并相对于现有方法提升 5%-20%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于推理的猜测解码（如 EAGLE-2/3）通常忽略硬件与系统变量对推理成本的影响，导致在不同硬件配置下难以实现稳定的低延迟。

Method: 提出动态树解码 CAST，在解码过程中将推理成本（包括 GPU 配置、批量大小等）纳入树结构的选择与 refining，从而动态地调整树的形状以降低延迟。

Result: 在六个任务、六个 LLM 的广泛实验中，CAST 的解码速度达到传统方法的最高 5.2 倍，并在多任务与多模型场景中相对现有最优方法提升 5%-20% 之间。

Conclusion: 将硬件与批量等系统变量纳入动态树解码的成本模型可以显著提升推理效率，CAST 提供一种鲁棒且可在多模型多任务场景下泛化的方法。

Abstract: Large Language Models (LLMs) face significant inference latency challenges
stemming from their autoregressive design and large size. To address this,
speculative decoding emerges as a solution, enabling the simultaneous
generation and validation of multiple tokens. While recent approaches like
EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,
they often neglect the impact of crucial system variables such as GPU devices
and batch sizes.
  Therefore, we introduce a new dynamic tree decoding approach called CAST that
takes into account inference costs, including factors such as GPU
configurations and batch sizes, to dynamically refine the tree structure.
Through comprehensive experimentation across six diverse tasks and utilizing
six distinct LLMs, our methodology demonstrates remarkable results, achieving
speeds up to 5.2 times faster than conventional decoding methods. Moreover, it
generally outperforms existing state-of-the-art techniques from 5% to 20%.

</details>


### [42] [Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model](https://arxiv.org/abs/2510.26622)
*Biao Zhang,Yong Cheng,Siamak Shakeri,Xinyi Wang,Min Ma,Orhan Firat*

Main category: cs.CL

TL;DR: Revisiting encoder-decoder LLMs (RedLLM) and comparing with decoder-only LLMs (DecLLM) across scales shows RedLLM has strong scaling and inference efficiency, while DecLLM is more compute-efficient in pretraining; with instruction tuning, RedLLM achieves comparable or better downstream performance, suggesting renewed interest in RedLLM.


<details>
  <summary>Details</summary>
Motivation: The recent shift to decoder-only LLMs lacks thorough comparative analysis from a scaling perspective. The encoder-decoder family may retain strong potential that has been underexplored.

Method: Pretrain encoder-decoder RedLLM with prefix language modeling and compare to DecLLM pretrained with causal language modeling across model sizes from ~150M to ~8B. Use RedPajama V1 (1.6T tokens) for pretraining and FLAN for instruction tuning. Evaluate scaling behavior, context length extrapolation, and downstream task performance.

Result: RedLLM exhibits compelling scaling properties and strong performance. While DecLLM is generally more compute-efficient during pretraining, RedLLM achieves comparable scaling and context-length extrapolation. After instruction tuning, RedLLM attains comparable or better downstream results and offers substantially better inference efficiency.

Conclusion: Re-examining encoder-decoder LLMs (RedLLM) is worthwhile, as it can unlock powerful and efficient LLMs and may reshape understanding of scaling dynamics across architectures.

Abstract: Recent large language model (LLM) research has undergone an architectural
shift from encoder-decoder modeling to nowadays the dominant decoder-only
modeling. This rapid transition, however, comes without a rigorous comparative
analysis especially \textit{from the scaling perspective}, raising concerns
that the potential of encoder-decoder models may have been overlooked. To fill
this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent
recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison
between RedLLM, pretrained with prefix language modeling (LM), and DecLLM,
pretrained with causal LM, at different model scales, ranging from $\sim$150M
to $\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for
instruction tuning, our experiments show that RedLLM produces compelling
scaling properties and surprisingly strong performance. While DecLLM is overall
more compute-optimal during pretraining, RedLLM demonstrates comparable scaling
and context length extrapolation capabilities. After instruction tuning, RedLLM
achieves comparable and even better results on various downstream tasks while
enjoying substantially better inference efficiency. We hope our findings could
inspire more efforts on re-examining RedLLM, unlocking its potential for
developing powerful and efficient LLMs.

</details>


### [43] [Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models](https://arxiv.org/abs/2510.26683)
*Mingchen Tu,Zhiqiang Liu,Juan Li,Liangyurui Liu,Junjie Wang,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: 本研究提出 Evontree 框架，利用少量高质量本体规则在不依赖大量外部数据的情况下，对 LLM 进行低资源领域自适应，提升医疗问答等任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 在数据敏感的医疗领域，缺乏高质量域专属训练语料；领域专家将本体规则用于知识管理。将 LLM 视为隐式知识存储库，需通过少量规则强化知识的一致性和可用性。

Method: 从原始模型中提取领域本体；用两条核心本体规则检测不一致性；通过自蒸馏微调对知识进行强化，整个过程不依赖大规模外部数据。

Result: 在医疗 QA 基准上，使用 Llama3-8B-Instruct 和 Med42-v2，Evontree 在未修改模型和领先监督基线之上取得提升，准确率提高至 最高 3.7%。

Conclusion: 证明 Evontree 在低资源域适配场景中具有有效性、效率和鲁棒性，能够在不依赖大量外部数据的情况下提升 LLM 的领域知识一致性与表现。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities
across multiple domains by leveraging massive pre-training and curated
fine-tuning data. However, in data-sensitive fields such as healthcare, the
lack of high-quality, domain-specific training corpus hinders LLMs' adaptation
for specialized applications. Meanwhile, domain experts have distilled domain
wisdom into ontology rules, which formalize relationships among concepts and
ensure the integrity of knowledge management repositories. Viewing LLMs as
implicit repositories of human knowledge, we propose Evontree, a novel
framework that leverages a small set of high-quality ontology rules to
systematically extract, validate, and enhance domain knowledge within LLMs,
without requiring extensive external datasets. Specifically, Evontree extracts
domain ontology from raw models, detects inconsistencies using two core
ontology rules, and reinforces the refined knowledge via self-distilled
fine-tuning. Extensive experiments on medical QA benchmarks with
Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both
unmodified models and leading supervised baselines, achieving up to a 3.7%
improvement in accuracy. These results confirm the effectiveness, efficiency,
and robustness of our approach for low-resource domain adaptation of LLMs.

</details>


### [44] [Kimi Linear: An Expressive, Efficient Attention Architecture](https://arxiv.org/abs/2510.26692)
*Kimi Team,Yu Zhang,Zongyu Lin,Xingcheng Yao,Jiaxi Hu,Fanqing Meng,Chengyin Liu,Xin Men,Songlin Yang,Zhiyuan Li,Wentao Li,Enzhe Lu,Weizhou Liu,Yanru Chen,Weixin Xu,Longhui Yu,Yejie Wang,Yu Fan,Longguang Zhong,Enming Yuan,Dehao Zhang,Yizhi Zhang,T. Y. Liu,Haiming Wang,Shengjun Fang,Weiran He,Shaowei Liu,Yiwei Li,Jianlin Su,Jiezhong Qiu,Bo Pang,Junjie Yan,Zhejun Jiang,Weixiao Huang,Bohong Yin,Jiacheng You,Chu Wei,Zhengtao Wang,Chao Hong,Yutian Chen,Guanduo Chen,Yucheng Wang,Huabin Zheng,Feng Wang,Yibo Liu,Mengnan Dong,Zheng Zhang,Siyuan Pan,Wenhao Wu,Yuhao Wu,Longyu Guan,Jiawen Tao,Guohong Fu,Xinran Xu,Yuzhi Wang,Guokun Lai,Yuxin Wu,Xinyu Zhou,Zhilin Yang,Yulun Du*

Main category: cs.CL

TL;DR: 提出 Kimi Linear，结合 Kimi Delta Attention（KDA）的混合线性注意力体系，在公平对比下超越全注意力，且在短/长上下文和 RL 场景中显著提高效率与吞吐。


<details>
  <summary>Details</summary>
Motivation: 解决全注意力在长上下文和硬件资源方面的瓶颈，探索更高效、可扩展的注意力替代方案；通过更细粒度的门控与分块算法实现高效记忆利用与运算效率。

Method: 提出 Kimi Linear 架构及 KDA（Kimi Delta Attention），扩展自 Gated DeltaNet，加入更细粒度的门控；采用分块算法与改进的对角线+低秩(Diagonal-Plus-Low-Rank, DPLR)转移矩阵以降低计算量，同时保持对 delta 规则的较好一致性；进行层级混合，使用 KDA 与 MLA 的混合，进行 3B 激活参数、总参数48B 的预训练；并开源 KDA 内核、vLLM 实现以及模型检查点。

Result: 在相同训练配方下，Kimi Linear 在所有评估任务上优于全 MLA；显著减少 KV 缓存需求（约 75%），并在 1M 上下文下实现最高达 6 倍的解码吞吐量提升；可作为全注意力的直接替代，尤其适用于更长输入/输出。

Conclusion: Kimi Linear 可以作为全注意力架构的高效替代，兼具更强性能与效率，且在应对更长的上下文方面表现出色。研究社区可通过开源的 KDA 内核与模型检查点继续推进相关研究。

Abstract: We introduce Kimi Linear, a hybrid linear attention architecture that, for
the first time, outperforms full attention under fair comparisons across
various scenarios -- including short-context, long-context, and reinforcement
learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an
expressive linear attention module that extends Gated DeltaNet with a
finer-grained gating mechanism, enabling more effective use of limited
finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware
efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)
transition matrices, which substantially reduces computation compared to the
general DPLR formulation while remaining more consistent with the classical
delta rule.
  We pretrain a Kimi Linear model with 3B activated parameters and 48B total
parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention
(MLA). Our experiments show that with an identical training recipe, Kimi Linear
outperforms full MLA with a sizeable margin across all evaluated tasks, while
reducing KV cache usage by up to 75% and achieving up to 6 times decoding
throughput for a 1M context. These results demonstrate that Kimi Linear can be
a drop-in replacement for full attention architectures with superior
performance and efficiency, including tasks with longer input and output
lengths.
  To support further research, we open-source the KDA kernel and vLLM
implementations, and release the pre-trained and instruction-tuned model
checkpoints.

</details>


### [45] [The End of Manual Decoding: Towards Truly End-to-End Language Models](https://arxiv.org/abs/2510.26697)
*Zhichao Wang,Dongyang Ma,Xinting Huang,Deng Cai,Tian Lan,Jiahao Xu,Haitao Mi,Xiaoying Tang,Yan Wang*

Main category: cs.CL

TL;DR: AutoDeco introduces a token-level, differentiable decoding control by adding lightweight heads to predict temperature/top-p at each step, enabling end-to-end generation within a single forward pass and even following natural language commands.


<details>
  <summary>Details</summary>
Motivation: Decoding in LLMs is traditionally non-differentiable and requires manual hyperparameter tuning; there is a need for truly end-to-end trainable generation with adaptive sampling strategies.

Method: Extend the Transformer with small heads that predict context-specific temperature and top-p at every decoding step, jointly with next-token logits, enabling a single forward-pass, token-level decoding control and instruction-following decoding. 

Result: On eight benchmarks, AutoDeco outperforms default decoding strategies and approaches an oracle-tuned baseline (derived from test-set hacking) as an upper bound for static methods; emergent ability to interpret natural language commands to adjust sampling (e.g., 'generate with low randomness') token-by-token.

Conclusion: Decoding becomes a parametric, controllable, end-to-end process within the model, enabling steerable and interactive LLM generation without external decoding pipelines.

Abstract: The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a
non-differentiable decoding process that requires laborious, hand-tuning of
hyperparameters like temperature and top-p. This paper introduces AutoDeco, a
novel architecture that enables truly "end-to-end" generation by learning to
control its own decoding strategy. We augment the standard transformer with
lightweight heads that, at each step, dynamically predict context-specific
temperature and top-p values alongside the next-token logits. This approach
transforms decoding into a parametric, token-level process, allowing the model
to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that
AutoDeco not only significantly outperforms default decoding strategies but
also achieves performance comparable to an oracle-tuned baseline derived from
"hacking the test set"-a practical upper bound for any static method.
Crucially, we uncover an emergent capability for instruction-based decoding
control: the model learns to interpret natural language commands (e.g.,
"generate with low randomness") and adjusts its predicted temperature and top-p
on a token-by-token basis, opening a new paradigm for steerable and interactive
LLM decoding.

</details>


### [46] [Value Drifts: Tracing Value Alignment During LLM Post-Training](https://arxiv.org/abs/2510.26707)
*Mehar Bhatia,Shravan Nayak,Gaurav Kamath,Marius Mosbach,Karolina Stańczak,Vered Shwartz,Siva Reddy*

Main category: cs.CL

TL;DR: 该工作研究了大语言模型在后训练过程中的价值观对齐动态，发现价值观主要在监督微调（SFT）阶段确立，随后偏好优化（PO）很少改变这些价值观；不同PO算法在相同偏好数据下会产生不同的对齐结果；通过对Llama-3和Qwen-3等模型的实验，给出数据选择和算法选择对提升人类价值对齐的可执行见解。


<details>
  <summary>Details</summary>
Motivation: 理解后训练阶段模型如何学习并巩固人类价值观的过程，以便在数据选择、算法设计和训练策略上提升对齐效果。

Method: 在多种规模的Llama-3与Qwen-3模型上，比较SFT阶段与后续偏好优化阶段的影响；使用公开的SFT和偏好优化数据集，以及一个可控的合成偏好数据集，以测量价值观漂移的幅度与时间，并比较不同偏好优化算法在相同数据条件下的对齐结果。

Result: 1) SFT阶段基本确立模型的价值观，后续的偏好优化很少重新对齐；2) 使用合成偏好数据集能控地操纵价值观，显示不同PO算法在相同数据下产生不同的对齐结果；3) 漂移的幅度和发生时间可被量化，提供对训练数据与算法选择的可操作性指引。

Conclusion: 研究揭示了价值观在后训练中的学习与固化过程，强调SFT阶段的重要性，并提示在选择偏好优化算法时要考虑其对价值对齐的潜在影响；结果可用于数据筛选、模型和算法的选型，以提升对人类价值的对齐效果。

Abstract: As LLMs occupy an increasingly important role in society, they are more and
more confronted with questions that require them not only to draw on their
general knowledge but also to align with certain human value systems.
Therefore, studying the alignment of LLMs with human values has become a
crucial field of inquiry. Prior work, however, mostly focuses on evaluating the
alignment of fully trained models, overlooking the training dynamics by which
models learn to express human values. In this work, we investigate how and at
which stage value alignment arises during the course of a model's
post-training. Our analysis disentangles the effects of post-training
algorithms and datasets, measuring both the magnitude and time of value drifts
during training. Experimenting with Llama-3 and Qwen-3 models of different
sizes and popular supervised fine-tuning (SFT) and preference optimization
datasets and algorithms, we find that the SFT phase generally establishes a
model's values, and subsequent preference optimization rarely re-aligns these
values. Furthermore, using a synthetic preference dataset that enables
controlled manipulation of values, we find that different preference
optimization algorithms lead to different value alignment outcomes, even when
preference data is held constant. Our findings provide actionable insights into
how values are learned during post-training and help to inform data curation,
as well as the selection of models and algorithms for preference optimization
to improve model alignment to human values.

</details>


### [47] [AMO-Bench: Large Language Models Still Struggle in High School Math Competitions](https://arxiv.org/abs/2510.26768)
*Shengnan An,Xunliang Cai,Xuezhi Cao,Xiaoyu Li,Yehao Lin,Junlin Liu,Xinxuan Lv,Dan Ma,Xuanlin Wang,Ziwen Wang,Shuang Zhou*

Main category: cs.CL

TL;DR: AMO-Bench 是一个含50道原创、跨专家验收且达到IMO难度的高级数学推理基准，目标在于评估与推动大语言模型的高阶数学推理能力；实验显示当前最强模型最高仅达到52.4%的正确率，且多模型分数低于40%，并发现通过增加推理时延计算量可带来明显提升。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准多来自高中竞赛，已难以区分顶尖LLM的推理能力；需要更难、原创且可自动评分的数据集，来推动对复杂推理的评估和改进。

Method: 设计50道原创题、经专家跨验并符合IMO难度；每题仅需给出最终答案以便自动评分；在26个LLM上评估，分析在更多推理时间下的扩展性；并提供数据集发布。

Result: 顶尖模型的正确率为52.4%，大多数模型低于40%；对不同模型的比较显示整体表现仍然偏弱；据分析，增加测试时的计算量具有显著的提升趋势。

Conclusion: AMO-Bench 可作为推动LLM数学推理研究的平台，原创性与难度降低了记忆泄露风险；并且存在可扩展的尺度效应，适合进一步的研究与评估。数据集向社区公开。

Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [48] [ORBIT - Open Recommendation Benchmark for Reproducible Research with Hidden Tests](https://arxiv.org/abs/2510.26095)
*Jingyuan He,Jiongnan Liu,Vishan Vishesh Oberoi,Bolin Wu,Mahima Jagadeesh Patel,Kangrui Mao,Chuning Shi,I-Ta Lee,Arnold Overwijk,Chenyan Xiong*

Main category: cs.IR

TL;DR: ORBIT（Open Recommendation Benchmark for Reproducible Research with HIdden Tests）提供一个统一且可复现的推荐系统基准测试框架，含公开数据集的标准化划分与透明的公开排行榜设置，同时引入ClueWeb-Reco作为隐藏测试任务，基于87百万网页的现实浏览序列；评估了12个模型，并引入一种基于提示的LLM基线。公开数据上呈现普遍性能提升但个别模型表现波动，隐藏测试揭示现有方法在大规模网页推荐上的局限性以及LLM整合的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集对真实用户行为的捕捉不足，评估设置不一致导致结论不确定，亟需一个统一、可复现且贴近现实场景的基准来推动推荐系统的研究与比较。

Method: 建立统一的公开数据集及可复现的划分、公开排行榜的透明设置；新增网页推荐任务ClueWeb-Reco，数据来自真实、用户同意且隐私受保障的浏览序列，且以隐藏测试的形式挑战模型的泛化能力；在公开基准上评测12种典型推荐模型，并引入一个基于提示的LLM基线以对比。

Result: 公开基准上各模型普遍实现性能提升，但在具体数据集上波动较大；隐藏测试结果揭示现有方法在大规模网页推荐上的局限性，同时指出通过引入LLM迎合场景的潜在改进。

Conclusion: ORBIT提供了一个统一的、可复现的基准、排行榜与代码库，促使推荐研究在真实场景中的对比与复现，并强调通过LLM等新技术提升大规模网页推荐的潜力与方向。

Abstract: Recommender systems are among the most impactful AI applications, interacting
with billions of users every day, guiding them to relevant products, services,
or information tailored to their preferences. However, the research and
development of recommender systems are hindered by existing datasets that fail
to capture realistic user behaviors and inconsistent evaluation settings that
lead to ambiguous conclusions. This paper introduces the Open Recommendation
Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified
benchmark for consistent and realistic evaluation of recommendation models.
ORBIT offers a standardized evaluation framework of public datasets with
reproducible splits and transparent settings for its public leaderboard.
Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,
featuring web browsing sequences from 87 million public, high-quality webpages.
ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and
privacy-guaranteed browsing data. It aligns with modern recommendation
scenarios and is reserved as the hidden test part of our leaderboard to
challenge recommendation models' generalization ability. ORBIT measures 12
representative recommendation models on its public benchmark and introduces a
prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results
reflect general improvements of recommender systems on the public datasets,
with variable individual performances. The results on the hidden test reveal
the limitations of existing approaches in large-scale webpage recommendation
and highlight the potential for improvements with LLM integrations. ORBIT
benchmark, leaderboard, and codebase are available at
https://www.open-reco-bench.ai.

</details>


### [49] [OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender](https://arxiv.org/abs/2510.26104)
*Zhaoqi Zhang,Haolei Pei,Jun Guo,Tianyu Wang,Yufei Feng,Hui Sun,Shaowei Liu,Aixin Sun*

Main category: cs.IR

TL;DR: OneTrans 提供一个统一的 Transformer 骨干，能够在同一模型中同时进行用户行为序列建模与特征交互，使用统一 tokenizer 将序列化与非序列化属性转化为 token 序列，并通过参数共享与 token-specific 参数实现高效联合建模，辅以因果注意力和跨请求 KV 缓存以降低成本；在工业规模数据集上表现出良好扩展性和优越性，上线测试实现 5.68% 的 per-user GMV 提升。


<details>
  <summary>Details</summary>
Motivation: 现有工作通常在特征交互模块和用户行为序列模块之间分道而治，阻碍信息双向流动、难以统一优化与大规模扩展，导致训练成本高、推理效率低。需要一个统一框架来整合两类任务、实现共享表示并提升实际部署效率。

Method: 提出 OneTrans：使用统一的 tokenizer 将序列属性与非序列属性统一编码为 token 序列。堆叠的 OneTrans 块对相似的序列 token 共享参数；非序列 token 使用 token-specific 的参数以建模差异。采用因果注意力以确保正确的时序信息流，并结合跨请求 KV 缓存实现中间表示的预计算与缓存，从而在训练和推理阶段显著降低成本。

Result: 在工业规模数据集上的实验显示 OneTrans 能随参数规模线性扩展、持续超越强基线，在在线 A/B 测试中实现 5.68% 的 per-user GMV 提升。

Conclusion: Unified Transformer 骨干在同时处理行为序列建模与特征交互方面具有显著优势，能够实现效率与性能并行提升，适合大规模生产部署。

Abstract: In recommendation systems, scaling up feature-interaction modules (e.g.,
Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has
achieved notable success. However, these efforts typically proceed on separate
tracks, which not only hinders bidirectional information exchange but also
prevents unified optimization and scaling. In this paper, we propose OneTrans,
a unified Transformer backbone that simultaneously performs user-behavior
sequence modeling and feature interaction. OneTrans employs a unified tokenizer
to convert both sequential and non-sequential attributes into a single token
sequence. The stacked OneTrans blocks share parameters across similar
sequential tokens while assigning token-specific parameters to non-sequential
tokens. Through causal attention and cross-request KV caching, OneTrans enables
precomputation and caching of intermediate representations, significantly
reducing computational costs during both training and inference. Experimental
results on industrial-scale datasets demonstrate that OneTrans scales
efficiently with increasing parameters, consistently outperforms strong
baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.

</details>


### [50] [ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs](https://arxiv.org/abs/2510.26178)
*Yanran Tang,Ruihong Qiu,Xue Li,Zi Huang*

Main category: cs.IR

TL;DR: 提出 ReaKase-8B 框架，通过引入法律事实、法律问题、关系三元组和推理信息，结合在 contextual 中的可解释嵌入，提升法律案例检索的准确性；在 COLIEE 2022/2023 数据集上实现显著改进，代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有方法多以传统词法模型或预训练语言模型编码法律案例文本，但忽略不同法律主体之间的关系信息以及支撑判决的推理过程。这些信息对区分相似案例、模拟真实司法流程具有重要作用。

Method: 提出 ReaKase-8B 框架，提取法律事实、法律问题、法律关系三元组和法律推理信息，并采用对齐的在-context 表征学习策略，利用一个经过微调的大型语言模型进行嵌入生成；通过将知识和推理信息整合进检索嵌入来增强表示能力。

Result: 在 COLIEE 2022 与 COLIEE 2023 的基准数据集上进行大量实验，知识与推理增强的嵌入显著提升检索性能，优于基线模型。

Conclusion: 将法律推理融入法律案例检索是有效且具潜力的方向，ReaKase-8B 的实验结果证实了知识与推理增强的嵌入在实际检索任务中的价值，且代码已公开。

Abstract: Legal case retrieval (LCR) is a cornerstone of real-world legal decision
making, as it enables practitioners to identify precedents for a given query
case. Existing approaches mainly rely on traditional lexical models and
pretrained language models to encode the texts of legal cases. Yet there are
rich information in the relations among different legal entities as well as the
crucial reasoning process that uncovers how legal facts and legal issues can
lead to judicial decisions. Such relational reasoning process reflects the
distinctive characteristics of each case that can distinguish one from another,
mirroring the real-world judicial process. Naturally, incorporating such
information into the precise case embedding could further enhance the accuracy
of case retrieval. In this paper, a novel ReaKase-8B framework is proposed to
leverage extracted legal facts, legal issues, legal relation triplets and legal
reasoning for effective legal case retrieval. ReaKase-8B designs an in-context
legal case representation learning paradigm with a fine-tuned large language
model. Extensive experiments on two benchmark datasets from COLIEE 2022 and
COLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings
substantially improve retrieval performance over baseline models, highlighting
the potential of integrating legal reasoning into legal case retrieval systems.
The code has been released on https://github.com/yanran-tang/ReaKase-8B.

</details>


### [51] [DiSE: A diffusion probabilistic model for automatic structure elucidation of organic compounds](https://arxiv.org/abs/2510.26231)
*Haochen Chen,Qi Huang,Anan Wu,Wenhao Zhang,Jianliang Ye,Jianming Wu,Kai Tan,Xin Lu,Xin Xu*

Main category: cs.IR

TL;DR: DiSE is a diffusion-based end-to-end model that fuses multiple spectroscopic modalities (MS, 13C/1H shifts, HSQC, COSY) to automate structure elucidation with high accuracy and generalization, trained on calculated spectra but robust to experimental data.


<details>
  <summary>Details</summary>
Motivation: The need for fully automated, self-driving laboratories requires reliable, real-time structure elucidation to close the experimental feedback loop and enable autonomous decision-making.

Method: DiSE is an end-to-end diffusion-based generative model that integrates multiple spectroscopic modalities (MS, 13C/1H chemical shifts, HSQC, COSY) and learns correlations among spectra via data-driven learning, trained on calculated spectra to infer uncertainty and robustness.

Result: DiSE achieves superior accuracy, strong generalization across chemically diverse datasets, and robustness to experimental data despite training on calculated spectra.

Conclusion: This work advances toward fully automated structure elucidation with broad potential in natural product research, drug discovery, and self-driving laboratories.

Abstract: Automatic structure elucidation is essential for self-driving laboratories as
it enables the system to achieve truly autonomous. This capability closes the
experimental feedback loop, ensuring that machine learning models receive
reliable structure information for real-time decision-making and optimization.
Herein, we present DiSE, an end-to-end diffusion-based generative model that
integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical
shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation
of organic compounds. By learning inherent correlations among spectra through
data-driven approaches, DiSE achieves superior accuracy, strong generalization
across chemically diverse datasets, and robustness to experimental data despite
being trained on calculated spectra. DiSE thus represents a significant advance
toward fully automated structure elucidation, with broad potential in natural
product research, drug discovery, and self-driving laboratories.

</details>


### [52] [WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework with Model Merging](https://arxiv.org/abs/2510.26546)
*Min Hou,Xin Liu,Le Wu,Chenyi He,Hao Liu,Zhi Li,Xin Li,Si Wei*

Main category: cs.IR

TL;DR: WeaveRec通过在源域和目标域数据上以 weaving 方式跨训练多个 LoRA 模块，并通过模型合并来实现跨域推荐，在不增加推理延迟/显存成本的前提下，显著优于简单的跨域 LLM 合并方法，并给出理论界限。


<details>
  <summary>Details</summary>
Motivation: 解决跨域序列推荐中缺乏用户/物品重叠的问题，以及直接用大语言模型拼接多域数据导致性能下降的现象，需在不增加推理成本的情况下实现跨域知识迁移。

Method: 在源域和目标域数据上以 weaving 方式跨训练多个 LoRA 模块，并通过模型合并将它们融合；可扩展到多源域场景；提出不增加推理时延或显存的特性；给出理论保证，表明可减小目标域的期望误差上界。

Result: 在单源、多源、跨平台等多种跨域推荐场景中，WeaveRec 能有效缓解性能下降，且持续优于基线方法。

Conclusion: WeaveRec 提供一种高效、可扩展的跨域推荐解决方案，通过 weaving 与模型合并实现跨域知识迁移，同时具备理论保障，且对实际任务具有稳定的性能提升。

Abstract: Cross-Domain Sequential Recommendation (CDSR) seeks to improve user
preference modeling by transferring knowledge from multiple domains. Despite
the progress made in CDSR, most existing methods rely on overlapping users or
items to establish cross-domain correlations-a requirement that rarely holds in
real-world settings. The advent of large language models (LLM) and
model-merging techniques appears to overcome this limitation by unifying
multi-domain data without explicit overlaps. Yet, our empirical study shows
that naively training an LLM on combined domains-or simply merging several
domain-specific LLMs-often degrades performance relative to a model trained
solely on the target domain. To address these challenges, we first
experimentally investigate the cause of suboptimal performance in LLM-based
cross-domain recommendation and model merging. Building on these insights, we
introduce WeaveRec, which cross-trains multiple LoRA modules with source and
target domain data in a weaving fashion, and fuses them via model merging.
WeaveRec can be extended to multi-source domain scenarios and notably does not
introduce additional inference-time cost in terms of latency or memory.
Furthermore, we provide a theoretical guarantee that WeaveRec can reduce the
upper bound of the expected error in the target domain. Extensive experiments
on single-source, multi-source, and cross-platform cross-domain recommendation
scenarios validate that WeaveRec effectively mitigates performance degradation
and consistently outperforms baseline approaches in real-world recommendation
tasks.

</details>


### [53] [ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews](https://arxiv.org/abs/2510.26750)
*Martim Afonso,Nuno Saavedra,Bruno Lourenço,Alexandra Mendes,João Ferreira*

Main category: cs.IR

TL;DR: 提出 ProfOlaf，一种半自动工具，结合人机协同的系统综述流程，借助大语言模型辅助分析和提取主题，提高效率、质量和可重复性。


<details>
  <summary>Details</summary>
Motivation: 系统综述和映射研究往往耗时耗力，现有工具仅覆盖单一环节，缺乏对整个流程的统一支撑，需要在自动化与方法学严格性之间取得平衡。

Method: 通过迭代的雪球式文章收集并引入人机在环筛选；使用大型语言模型协助分析文章、提取关键主题，并回答论文内容相关的问题。

Result: 在提高效率、质量和可重复性方面具有潜在收益；实现了自动化与引导式人工努力的结合，适用于多研究领域。

Conclusion: ProfOlaf 通过半自动化结合人为控制的工作流，提升系统综述的可用性和鲁棒性，并辅以视频演示以展示能力。

Abstract: Systematic reviews and mapping studies are critical for synthesizing
research, identifying gaps, and guiding future work, but they are often
labor-intensive and time-consuming. Existing tools provide partial support for
specific steps, leaving much of the process manual and error-prone. We present
ProfOlaf, a semi-automated tool designed to streamline systematic reviews while
maintaining methodological rigor. ProfOlaf supports iterative snowballing for
article collection with human-in-the-loop filtering and uses large language
models to assist in analyzing articles, extracting key topics, and answering
queries about the content of papers. By combining automation with guided manual
effort, ProfOlaf enhances the efficiency, quality, and reproducibility of
systematic reviews across research fields. A video describing and demonstrating
ProfOlaf is available at: https://youtu.be/4noUXfcmxsE

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: 本综述系统梳理了Kolmogorov-Arnold Networks (KANs) 的理论基础、架构变体、实现策略及生态环境，强调基函数在KAN中的核心作用，并提供“Choose-Your-KAN”实用指南和开源实现收集。


<details>
  <summary>Details</summary>
Motivation: 旨在超越对比性能的简单评估，建立KAN与MLP的联系与区别，系统整理基函数选择、收敛性、正则化，以及面向实践的设计要点。

Method: 通过文献梳理、理论对比、分类架构、对比基函数（B-splines、Chebyshev与Jacobi多项式、ReLU组合、Gaussian RBF、傅里叶等）、整理开源实现、归纳先进技术路线（物理信息损失、自适应采样、域分解、混合架构、对不连续性的专门方法），并提供操作性指南。

Result: 提出KAN的系统地图与分类体系，建立KAN与MLP的形式等价关系以及在参数效率方面的潜在优势；总结各类基函数的权衡与适用场景；整理可操作的设计框架、正则化策略及“Choose-Your-KAN”实践路径，以及一个包含开源实现的GitHub资源库。

Conclusion: 当前研究仍存在空白与挑战，未来工作可聚焦于提高预测精度、计算效率、鲁棒性与对不连续性问题的处理，并通过系统化指南帮助从业者在具体场景中做出选择。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [55] [HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series](https://arxiv.org/abs/2510.25785)
*Simon A. Lee,Cyrus Tanade,Hao Zhou,Juhyeon Lee,Megha Thukral,Minji Han,Rachel Choi,Md Sazzad Hissain Khan,Baiying Lu,Migyeong Gwak,Mehrab Bin Morshed,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 提出 HiMAE：一个层级Masked Autoencoder，用于可穿戴传感器时间序列，能够学习多尺度表征并将时间尺度作为可解释性的探针；在分类、回归、生成任务上优于对尺度信息进行整合的基线模型，且参数量更小，能在智能手表级CPU上实现边缘推理。


<details>
  <summary>Details</summary>
Motivation: 研究表征学习中 temporal resolution 是一个关键但尚未充分理解的轴，不同临床和行为任务依赖于不同时间尺度的结构。需要一个能揭示不同尺度信号的自监督框架，以及在边缘设备上高效运行的模型。

Method: 提出 HiMAE，结合掩码自编码和分层卷积编码器-解码器，产出多分辨率嵌入。通过将分辨率作为探针，评估不同时间尺度的预测信号；方法具备高效、适合边缘推理的特性。

Result: 在分类、回归、生成等基准上，HiMAE 持续领先于把尺度折叠在单一表示的最新方法；模型规模远小于对比方法，且实现子毫秒级推理于智能手表类 CPU 上。

Conclusion: HiMAE 同时是一个高效的自监督学习方法和一个发现不同时间尺度对健康预测信号贡献的工具，推动可穿戴健康领域对尺度敏感结构的研究。

Abstract: Wearable sensors provide abundant physiological time series, yet the
principles governing their predictive utility remain unclear. We hypothesize
that temporal resolution is a fundamental axis of representation learning, with
different clinical and behavioral outcomes relying on structure at distinct
scales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical
Masked Autoencoder), a self supervised framework that combines masked
autoencoding with a hierarchical convolutional encoder decoder. HiMAE produces
multi resolution embeddings that enable systematic evaluation of which temporal
scales carry predictive signal, transforming resolution from a hyperparameter
into a probe for interpretability. Across classification, regression, and
generative benchmarks, HiMAE consistently outperforms state of the art
foundation models that collapse scale, while being orders of magnitude smaller.
HiMAE is an efficient representation learner compact enough to run entirely on
watch, achieving sub millisecond inference on smartwatch class CPUs for true
edge inference. Together, these contributions position HiMAE as both an
efficient self supervised learning method and a discovery tool for scale
sensitive structure in wearable health.

</details>


### [56] [SHA-256 Infused Embedding-Driven Generative Modeling of High-Energy Molecules in Low-Data Regimes](https://arxiv.org/abs/2510.25788)
*Siddharth Verma,Alankar Alankar*

Main category: cs.LG

TL;DR: High-energy molecule design via LSTM-based generation and Attentive GNN-based property prediction, with a novel embedding space that mixes fixed SHA-256 embeddings and partially trainable representations; without pretraining, achieves 67.5% validity and 37.5% novelty, yielding a diverse chemical space (mean Tanimoto ~0.214) and identifying 37 new super-explosives (>9 km/s detonation velocity).


<details>
  <summary>Details</summary>
Motivation: Address the data scarcity and limited testing access in high-energy materials by enabling in silico exploration of molecular space and rapid screening.

Method: Generate molecules with LSTM, predict properties with Attentive Graph Neural Networks. Introduce a transformative embedding space combining fixed SHA-256-derived embeddings with partially trainable representations, reshaping input space before learning. No pretraining used. Evaluate validity, novelty, and diversity; use Tanimoto similarity to assess diversity; screen for detonation velocity.

Result: Generated library shows 67.5% validity and 37.5% novelty. Mean Tanimoto coefficient relative to training set is 0.214, indicating diverse chemical space exploration. Identified 37 new super-explosive candidates with predicted detonation velocity > 9 km/s.

Conclusion: The approach demonstrates that reshaping the input representation space can enhance molecular generation and exploration of high-energy materials space without pretraining, yielding candidate molecules with high predicted performance.

Abstract: High-energy materials (HEMs) are critical for propulsion and defense domains,
yet their discovery remains constrained by experimental data and restricted
access to testing facilities. This work presents a novel approach toward
high-energy molecules by combining Long Short-Term Memory (LSTM) networks for
molecular generation and Attentive Graph Neural Networks (GNN) for property
predictions. We propose a transformative embedding space construction strategy
that integrates fixed SHA-256 embeddings with partially trainable
representations. Unlike conventional regularization techniques, this changes
the representational basis itself, reshaping the molecular input space before
learning begins. Without recourse to pretraining, the generator achieves 67.5%
validity and 37.5% novelty. The generated library exhibits a mean Tanimoto
coefficient of 0.214 relative to training set signifying the ability of
framework to generate a diverse chemical space. We identified 37 new super
explosives higher than 9 km/s predicted detonation velocity.

</details>


### [57] [The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?](https://arxiv.org/abs/2510.25791)
*Zihan Pengmei,Costas Mavromatis,Zhengyuan Shen,Yunyi Zhang,Vassilis N. Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: CoT 有益但受任务复杂性限制；通过对数三参数逻辑回归模型揭示学习速度与形状的变化；揭示训练早期的“轨迹不忠实性”阶段；提出动力学建模框架，并显示 CoT 能改变内部计算但不能克服极高复杂度任务。


<details>
  <summary>Details</summary>
Motivation: 旨在理解为何和如何利用链式推理(CoT)监督提升模型性能，以及在符号推理任务中的学习动力学与泛化行为，借助可控的算法复杂度与数据分布来系统研究“grokking”现象。

Method: 在具可控算法复杂度与数据组成的符号推理任务上，对变换器进行预训练；比较两种训练设定：仅输出最终答案 vs 输出显式CoT后再回答；用对数训练步骤的准确率拟合三参数逻辑曲线，分析学习速度与曲线形状随任务复杂度、数据分布与CoT存在性变化；研究“轨迹不忠实性”现象、引入动力学建模框架，并考察CoT对内部计算的影响。

Result: 结论性发现：(1) CoT 在大多数任务上提升性能，但提升幅度取决于任务的算法复杂度；(2) 学习速度和曲线形状随复杂度、数据分布及CoT存在性而变化；(3) 训练早期存在“轨迹不忠实性”阶段，模型先给出正确答案再逐步将推理轨迹与答案对齐；(4) CoT 能加速泛化但无法克服高复杂度任务（如找出列表交集）；并提出一个用于理解Transformer学习的动力学框架，界定轨迹忠诚性为训练中的动态属性，以及表明CoT会从机制层面改变内部计算。

Conclusion: CoT 有助于提高泛化与推理能力，但其效用有边界，受任务复杂度约束。引入的动力学建模框架有助于系统理解Transformer的学习过程，轨迹忠诚性是逐步显现的特性，CoT 的影响不仅限于输出结果，还体现在内部计算路径的改变。

Abstract: Chain-of-thought (CoT) supervision can substantially improve transformer
performance, yet the mechanisms by which models learn to follow and benefit
from CoT remain poorly understood. We investigate these learning dynamics
through the lens of grokking by pretraining transformers on symbolic reasoning
tasks with tunable algorithmic complexity and controllable data composition to
study their generalization. Models were trained under two settings: (i)
producing only final answers, and (ii) emitting explicit CoT traces before
answering. Our results show that while CoT generally improves task performance,
its benefits depend on task complexity. To quantify these effects, we model the
accuracy of the logarithmic training steps with a three-parameter logistic
curve, revealing how the learning speed and shape vary with task complexity,
data distribution, and the presence of CoT supervision. We also uncover a
transient trace unfaithfulness phase: early in training, models often produce
correct answers while skipping or contradicting CoT steps, before later
aligning their reasoning traces with answers. Empirically, we (1) demonstrate
that CoT accelerates generalization but does not overcome tasks with higher
algorithmic complexity, such as finding list intersections; (2) introduce a
kinetic modeling framework for understanding transformer learning; (3)
characterize trace faithfulness as a dynamic property that emerges over
training; and (4) show CoT alters internal transformer computation
mechanistically.

</details>


### [58] [Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning](https://arxiv.org/abs/2510.25796)
*Farnoosh Namdarpour,Joseph Y. J. Chow*

Main category: cs.LG

TL;DR: A simulation-informed reinforcement learning approach for ride-pooling that embeds ride-pooling simulation into the learning process to enable non-myopic decision-making, plus a rebalancing policy.


<details>
  <summary>Details</summary>
Motivation: To overcome myopic dispatch decisions in ride-pooling systems and to improve service rate, reduce wait/in-vehicle times, and potentially decrease fleet size and costs by capturing long-term effects of actions.

Method: Extend the learning-and-planning framework (Xu et al., 2018) from ride-hailing to ride-pooling by embedding a ride-pooling simulation into the RL loop. Use n-step TD learning on simulated experiences to derive spatiotemporal state values and evaluate non-myopic matching policies. Propose a complementary idle-vehicle rebalancing policy.

Result: Non-myopic matching policy increases service rate by up to 8.4% vs myopic; reduces wait and in-vehicle times; can reduce fleet size by >25% while maintaining performance. Rebalancing reduces wait by 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%, at the cost of more vehicle minutes traveled per passenger.

Conclusion: A simulation-informed RL framework with integrated rebalancing yields meaningful non-myopic improvements in ride-pooling performance, offering cost savings and efficiency gains demonstrated on NYC taxi request data.

Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or
microtransit, is a service wherein passengers share rides. This service can
reduce costs for both passengers and operators and reduce congestion and
environmental impacts. A key limitation, however, is its myopic
decision-making, which overlooks long-term effects of dispatch decisions. To
address this, we propose a simulation-informed reinforcement learning (RL)
approach. While RL has been widely studied in the context of ride-hailing
systems, its application in ride-pooling systems has been less explored. In
this study, we extend the learning and planning framework of Xu et al. (2018)
from ride-hailing to ride-pooling by embedding a ride-pooling simulation within
the learning mechanism to enable non-myopic decision-making. In addition, we
propose a complementary policy for rebalancing idle vehicles. By employing
n-step temporal difference learning on simulated experiences, we derive
spatiotemporal state values and subsequently evaluate the effectiveness of the
non-myopic policy using NYC taxi request data. Results demonstrate that the
non-myopic policy for matching can increase the service rate by up to 8.4%
versus a myopic policy while reducing both in-vehicle and wait times for
passengers. Furthermore, the proposed non-myopic policy can decrease fleet size
by over 25% compared to a myopic policy, while maintaining the same level of
performance, thereby offering significant cost savings for operators.
Incorporating rebalancing operations into the proposed framework cuts wait time
by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%
compared to using the framework for matching decisions alone at the cost of
increased vehicle minutes traveled per passenger.

</details>


### [59] [MemEIC: A Step Toward Continual and Compositional Knowledge Editing](https://arxiv.org/abs/2510.25798)
*Jin Seong,Jiyun Park,Wencke Liermann,Hongseok Choi,Yoonji Nam,Hyun Kim,Soojong Lim,Namhoon Lee*

Main category: cs.LG

TL;DR: 提出 MemEIC，用于 LVLM 的持续与组成式知识编辑（CCKE），通过混合的外部-内部编辑器、双外部记忆与双 LoRA 适配器实现跨模态证据检索与分离参数更新，并以脑启发的知识连接器在组合性推理时激活，提升复杂多模态问答并保持先前编辑。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑多聚焦单模态编辑，忽略多模态交互与知识的连续更新，导致在跨模态推理与持续知识演化方面表现受限。

Method: 提出一个混合外部-内部编辑框架，使用双外部记忆用于跨模态证据检索；通过双 LoRA 适配器实现对每个模态的解耦参数更新；引入脑启发的知识连接器，在需要组合不同模态信息时选择性激活以进行跨模态整合。

Result: 实验显示在复杂的多模态问答中显著提升，同时更好地保留先前的编辑内容，达到在 LVLMs 中 CCKE 的新基准水平。

Conclusion: MemEIC 通过支持连续且组合性的跨模态知识编辑，推动 LVLMs 的知识更新与保持能力，提升多模态推理的鲁棒性与可持续演化潜力。

Abstract: The dynamic nature of information necessitates continuously updating large
vision-language models (LVLMs). While recent knowledge editing techniques hint
at promising directions, they often focus on editing a single modality (vision
or language) in isolation. This prevalent practice neglects the inherent
multimodality of LVLMs and the continuous nature of knowledge updates,
potentially leading to suboptimal editing outcomes when considering the
interplay between modalities and the need for ongoing knowledge refinement. To
address these limitations, we propose MemEIC, a novel method for Continual and
Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional
editing of both visual and textual knowledge sequentially. Our approach employs
a hybrid external-internal editor featuring a dual external memory for
cross-modal evidence retrieval and dual LoRA adapters that facilitate
disentangled parameter updates for each modality. A key component is a
brain-inspired knowledge connector, activated selectively for compositional
reasoning, that integrates information across different modalities. Experiments
demonstrate that MemEIC significantly improves performance on complex
multimodal questions and effectively preserves prior edits, setting a new
benchmark for CCKE in LVLMs.

</details>


### [60] [FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks](https://arxiv.org/abs/2510.25800)
*Jialong Sun,Xinpeng Ling,Jiaxuan Zou,Jiawen Kang,Kejia Zhang*

Main category: cs.LG

TL;DR: 提出 FreLE（Frequency Loss Enhancement）损失，用于缓解长期时序预测中的谱偏差，通过显式与隐式频率正则化实现泛化提升，并在大量实验中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据的固有自相关使长期预测具有挑战性；频域信息被广泛用于辅助预测；虽然谱偏差被观察到，但往往被归因于特定架构，缺乏普遍性认知，因此需要对其进行统一理解并给出缓解手段。

Method: 提出 FreLE 作为可插拔的模型损失函数单元，结合显式与隐式的频率正则化，提升对低高频分量的泛化能力。

Result: 大量实验表明 FreLE 拥有优越的性能，并给出代码实现链接（GitHub）。

Conclusion: 研究揭示谱偏差在主流模型中的普遍性，并证明通过频率正则化可以提升长期时序预测的泛化能力，FreLE 具有较强的通用性与可扩展性。

Abstract: The inherent autocorrelation of time series data presents an ongoing
challenge to multivariate time series prediction. Recently, a widely adopted
approach has been the incorporation of frequency domain information to assist
in long-term prediction tasks. Many researchers have independently observed the
spectral bias phenomenon in neural networks, where models tend to fit
low-frequency signals before high-frequency ones. However, these observations
have often been attributed to the specific architectures designed by the
researchers, rather than recognizing the phenomenon as a universal
characteristic across models. To unify the understanding of the spectral bias
phenomenon in long-term time series prediction, we conducted extensive
empirical experiments to measure spectral bias in existing mainstream models.
Our findings reveal that virtually all models exhibit this phenomenon. To
mitigate the impact of spectral bias, we propose the FreLE (Frequency Loss
Enhancement) algorithm, which enhances model generalization through both
explicit and implicit frequency regularization. This is a plug-and-play model
loss function unit. A large number of experiments have proven the superior
performance of FreLE. Code is available at
https://github.com/Chenxing-Xuan/FreLE.

</details>


### [61] [Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start](https://arxiv.org/abs/2510.25801)
*Kun Chen,Peng Shi,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao,Lin Ma*

Main category: cs.LG

TL;DR: 提出 SPECS 框架：自蒸馏生成偏好数据并进行偏好训练，解耦多模态学习，提升冷启动下的泛化与 RL 表现。


<details>
  <summary>Details</summary>
Motivation: 解决 SFT 冷启动可能带来的指令式过拟合、对分布外泛化能力不足及对 RL 性能负面影响的问题；比较不同冷启动方法对泛化能力的影响。

Method: 三步策略：1) 自蒸馏产生 introspective 偏好数据对；2) 基于偏好的训练，聚焦浅层表征（格式、结构、风格）而非 memorized 内容；3) 将冷启动阶段输出对接到具有可验证奖励的强化学习，以实现深度推理。

Result: 在多模态基准上表现优于强基线：MEGA-Bench 提升 4.1%，MathVista 提升 12.2%，并且减少分布内的卡死、提升探索性、训练稳定性和性能上限。

Conclusion: 通过解耦学习和偏好训练，SPECS 增强了冷启动下的泛化能力和 RL 性能，提供一种稳健且可扩展的多模态 RL 训练框架。

Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a
wave of "MLLM-r1" approaches that bring RL to vision language models. Most
representative paradigms begin with a cold start, typically employing
supervised fine-tuning (SFT), to initialize the policy before RL. However,
SFT-based cold start adopts the reasoning paradigm intertwined with task
solution and output format, which may induce instruction-style overfitting,
weakens out-of-distribution generalization, and ultimately affects downstream
RL. We revisit the cold start along two views, its training method and data
construction, and introduce the Generalization Factor (GF) coefficient to
quantify the generalization capability under different methods. Our empirical
study finds that preference-based training methods (e.g. DPO) generalizes
better than SFT-based methods in cold start. Motivated by this, we propose
SPECS-a Self-distilled, Preference-based Cold Start framework that decouples
multimodal learning: (1) generates introspective preference data pairs via
self-distillation, avoiding reliance on larger teachers or manual annotation;
(2) performs preference-based training to learn, focusing on shallow,
transferable surface-form criteria (format, structure, style) rather than
memorizing content; and (3) hands off to RL with verifiable rewards for deep
reasoning results. Experimental results across multiple multimodal benchmarks
show that our decoupling learning framework yields consistent performance gains
over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.
Additional experiments indicate that SPECS contributes to reducing
in-distribution "stuckness," improving exploration, stabilizing training, and
raising the performance ceiling.

</details>


### [62] [Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2510.25803)
*Hong Wang,Haiyang Xin,Jie Wang,Xuanze Yang,Fei Zha,Huanshuo Dong,Yan Jiang*

Main category: cs.LG

TL;DR: MoE-POT 提出一个稀疏激活的预训练算子变换器，针对PDE神经算子在异质数据集上的缺陷，通过4/16专家路由和2个共享专家实现高效参数扩展与低推理成本，显著降低零-shot误差。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺与PDE数据集异质性导致的混合训练高误差，以及密集大模型在提高性能时产生的推理成本攀升。

Method: 引入层级路由门控的MoE结构，在推理阶段从16个专家中挑选4个路由专家；加入2个共享专家以捕捉PDE共同属性并减少专家冗余；输出为激活专家输出的加权和；进行参数规模在30M–0.5B范围的预训练，在6个公开PDE数据集上进行评估；在激活参数为90M时，与使用120M激活参数的基线相比，零-shot误差降低约40%；并进行可解释性分析，显示路由决策能推断数据集类型。

Result: 在多数据集上实现显著的零-shot性能提升，且通过MoE架构实现参数效率的增长；可解释性分析证明路由网可揭示数据集类型，体现模型的合理性与有效性。

Conclusion: MoE-POT通过稀疏激活和共享专家的组合实现了高效参数扩展和低推理成本，同时提升了跨数据集的泛化能力与可解释性，为PDE神经算子预训练提供了一种可扩展的架构范式。

Abstract: Pre-training has proven effective in addressing data scarcity and performance
limitations in solving PDE problems with neural operators. However, challenges
remain due to the heterogeneity of PDE datasets in equation types, which leads
to high errors in mixed training. Additionally, dense pre-training models that
scale parameters by increasing network width or depth incur significant
inference costs. To tackle these challenges, we propose a novel
Mixture-of-Experts Pre-training Operator Transformer (MoE-POT), a
sparse-activated architecture that scales parameters efficiently while
controlling inference costs. Specifically, our model adopts a layer-wise
router-gating network to dynamically select 4 routed experts from 16 expert
networks during inference, enabling the model to focus on equation-specific
features. Meanwhile, we also integrate 2 shared experts, aiming to capture
common properties of PDE and reduce redundancy among routed experts. The final
output is computed as the weighted average of the results from all activated
experts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE
datasets. Our model with 90M activated parameters achieves up to a 40%
reduction in zero-shot error compared with existing models with 120M activated
parameters. Additionally, we conduct interpretability analysis, showing that
dataset types can be inferred from router-gating network decisions, which
validates the rationality and effectiveness of the MoE architecture.

</details>


### [63] [PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs](https://arxiv.org/abs/2510.25808)
*Jaewon Chu,Seunghun Lee,Hyunwoo J. Kim*

Main category: cs.LG

TL;DR: 提出 PRESTO 框架，通过利用软提示的 preimage 结构来高效优化黑箱 LLM 的指令；在相同查询预算下可获得约 14 倍的打分数据，在 33 个指令优化任务上表现优越，附带代码实现。


<details>
  <summary>Details</summary>
Motivation: 为应对黑箱 LLM 的内部参数不可访问、但需对指令进行高效优化的问题。白盒 LLM 往往将不同软提示映射到相同指令，造成查询冗余；将这种多对一映射视为可利用的先验信息以提升优化效率。

Method: 提出 PREimage-informed inSTruction Optimization (PRESTO)，基于软提示的 preimage 结构进行三大设计：1) 分数共享：将同一 preimage 中的所有软提示的评价分共享；2) 基于 preimage 的初始化：选择能够最大化搜索空间覆盖的初始数据点；3) 分数一致性正则化：在同一 preimage 内强制预测的一致性。

Result: 通过利用 preimage 结构，在相同查询预算下实现约 14 倍的打分数据获取效率；在 33 项指令优化任务上展现出更优性能；代码公开。

Conclusion: 通过利用 soft prompt 的 preimage 结构，PRESTO 能显著提升黑箱 LLM 的指令优化效率，提升数据利用率并稳定优化过程。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
domains, due to their strong instruction-following capabilities. This has led
to increasing interest in optimizing instructions for black-box LLMs, whose
internal parameters are inaccessible but widely used due to their strong
performance. To optimize instructions for black-box LLMs, recent methods employ
white-box LLMs to generate candidate instructions from optimized soft prompts.
However, white-box LLMs often map different soft prompts to the same
instruction, leading to redundant queries. While previous studies regarded this
many-to-one mapping as a structure that hinders optimization efficiency, we
reinterpret it as a useful prior knowledge that can accelerate the
optimization. To this end, we introduce PREimage-informed inSTruction
Optimization (PRESTO), a novel framework that leverages the preimage structure
of soft prompts for efficient optimization. PRESTO consists of three key
components: (1) score sharing, which shares the evaluation score with all soft
prompts in a preimage; (2) preimage-based initialization, which selects initial
data points that maximize search space coverage using preimage information; and
(3) score consistency regularization, which enforces prediction consistency
within each preimage. By leveraging preimages, PRESTO achieves the effect of
effectively obtaining 14 times more scored data under the same query budget,
resulting in more efficient optimization. Experimental results on 33
instruction optimization tasks demonstrate the superior performance of PRESTO.
Code is available at https://github.com/mlvlab/PRESTO

</details>


### [64] [ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion](https://arxiv.org/abs/2510.25818)
*Sungho Koh,SeungJu Cha,Hyunwoo Oh,Kwanyoung Lee,Dong-Jin Kim*

Main category: cs.LG

TL;DR: 提出ScaleDiff，在无需训练的前提下，通过邻域补丁自注意力（NPA）等技术扩展扩散模型的输出分辨率，同时提高图像质量与推理速度，且对U-Net与Diffusion Transformer均有效。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在超过训练分辨率后性能下降明显，现有训练-free方法要么计算量大、要么与新型Diffusion Transformer不兼容，需要一个模型无关且高效的解决方案来提升分辨率扩展能力。

Method: 提出Neighborhood Patch Attention（NPA），在非重叠补丁上减小自注意力的计算冗余；将NPA集成到SDEdit流程；引入Latent Frequency Mixing（LFM）以改善细节再现；并应用Structure Guidance以提升去噪过程中的全局结构。

Result: 实验结果显示，ScaleDiff在训练-free方法中在图像质量和推理速度方面达到领先水平，适用于U-Net与Diffusion Transformer架构。

Conclusion: ScaleDiff提供了一种训练-free、模型无关且高效的分辨率扩展方案，显著提升在超分辨率场景中的表现，并与现有方法相比具有更优的速度与质量权衡。

Abstract: Text-to-image diffusion models often exhibit degraded performance when
generating images beyond their training resolution. Recent training-free
methods can mitigate this limitation, but they often require substantial
computation or are incompatible with recent Diffusion Transformer models. In
this paper, we propose ScaleDiff, a model-agnostic and highly efficient
framework for extending the resolution of pretrained diffusion models without
any additional training. A core component of our framework is Neighborhood
Patch Attention (NPA), an efficient mechanism that reduces computational
redundancy in the self-attention layer with non-overlapping patches. We
integrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing
(LFM) to better generate fine details. Furthermore, we apply Structure Guidance
to enhance global structure during the denoising process. Experimental results
demonstrate that ScaleDiff achieves state-of-the-art performance among
training-free methods in terms of both image quality and inference speed on
both U-Net and Diffusion Transformer architectures.

</details>


### [65] [Topology-Aware Active Learning on Graphs](https://arxiv.org/abs/2510.25892)
*Harris Hardiman-Mostow,Jack Mauro,Adrien Weihs,Andrea L. Bertozzi*

Main category: cs.LG

TL;DR: 提出基于图拓扑的主动学习框架，利用 Balanced Forman Curvature (BFC) 构建核心标签集、动态探索-利用切换、以及局部图重连，以提升低标签率下的标签传播效果。


<details>
  <summary>Details</summary>
Motivation: 在标签预算有限的情形下，如何在探索和利用之间实现自适应权衡，并替代手工启发式策略以提升图半监督学习性能。

Method: 1) 构造基于 BFC 的数据核心集（coreset），选取能体现图簇结构的初始标签；2) 引入数据驱动的停止准则，指示何时对图的探索足够充分；3) 用 BFC 动态触发从探索到利用的切换，替代传统的手工启发式；4) 提出本地化的图重连策略，结合多尺度信息以提升标签传播，同时保持稀疏性。

Result: 在基准分类数据集上，与现有基于图的半监督基线相比，方法在低标签率下更稳定地提高准确性和泛化能力，并显示出对标签稀缺情形的鲁棒性。

Conclusion: 将 BFC 融入主动学习流程，提供自适应的探索-利用切换与高效的局部信息整合，提升低标签率下的图学习性能与可扩展性；未来工作可关注大规模图的计算成本、对噪声的鲁棒性以及与其他自监督信号的融合。

Abstract: We propose a graph-topological approach to active learning that directly
targets the core challenge of exploration versus exploitation under scarce
label budgets. To guide exploration, we introduce a coreset construction
algorithm based on Balanced Forman Curvature (BFC), which selects
representative initial labels that reflect the graph's cluster structure. This
method includes a data-driven stopping criterion that signals when the graph
has been sufficiently explored. We further use BFC to dynamically trigger the
shift from exploration to exploitation within active learning routines,
replacing hand-tuned heuristics. To improve exploitation, we introduce a
localized graph rewiring strategy that efficiently incorporates multiscale
information around labeled nodes, enhancing label propagation while preserving
sparsity. Experiments on benchmark classification tasks show that our methods
consistently outperform existing graph-based semi-supervised baselines at low
label rates.

</details>


### [66] [Transferring Causal Effects using Proxies](https://arxiv.org/abs/2510.25924)
*Manuel Iglesias-Alonso,Felix Schur,Julius von Kügelgen,Jonas Peters*

Main category: cs.LG

TL;DR: Identifiability of causal effects across multiple domains with an unobserved confounder, using a proxy; two estimators with consistency proofs and confidence intervals, validated by simulations and a real-data example on website rankings.


<details>
  <summary>Details</summary>
Motivation: Addresses causal effect estimation under domain shifts and hidden confounding. Leverages a proxy for the latent confounder and assumes discrete/categorical variables to achieve identifiability in the target domain.

Method: Assume observed proxy for the hidden confounder; perform estimation in the target domain where only the proxy is observed. Prove identifiability even with continuous treatment/outcome. Propose two estimation techniques, prove consistency, derive confidence intervals.

Result: Identifiability is established; the two estimators are consistent and accompanied by confidence intervals. Demonstrated via simulations and a real-world study of how website rankings affect consumer choices.

Conclusion: The framework enables causal effect estimation in multi-domain settings with unobserved confounding by using a proxy, applicable even when treatment and outcome are continuous, with practical validation through simulations and real data.

Abstract: We consider the problem of estimating a causal effect in a multi-domain
setting. The causal effect of interest is confounded by an unobserved
confounder and can change between the different domains. We assume that we have
access to a proxy of the hidden confounder and that all variables are discrete
or categorical. We propose methodology to estimate the causal effect in the
target domain, where we assume to observe only the proxy variable. Under these
conditions, we prove identifiability (even when treatment and response
variables are continuous). We introduce two estimation techniques, prove
consistency, and derive confidence intervals. The theoretical results are
supported by simulation studies and a real-world example studying the causal
effect of website rankings on consumer choices.

</details>


### [67] [Active Learning with Task-Driven Representations for Messy Pools](https://arxiv.org/abs/2510.25926)
*Kianoosh Ashouritaklimi,Tom Rainforth*

Main category: cs.LG

TL;DR: 通过任务驱动的、周期更新的表示学习提升活跃学习在混乱数据池中的效果；提出半监督表示学习和有监督微调两种策略，优于固定的无监督表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法多使用固定、无监督的表示来支撑采样策略，无法充分捕捉与任务相关的重要信息，导致在混乱数据池上表现受限。

Method: 提出任务驱动的表示，在活跃学习过程中定期使用已获得的标签更新表示；具体策略包括直接学习半监督表示，以及对初始无监督表示进行有监督微调。

Result: 这两种策略在经验上明显优于仅使用无监督或预训练表示的做法。

Conclusion: 任务驱动、定期更新的表示能缓解混乱数据池下的活跃学习挑战，提升效果，建议在活跃学习场景中采用随标签更新的表示学习方法。

Abstract: Active learning has the potential to be especially useful for messy,
uncurated pools where datapoints vary in relevance to the target task. However,
state-of-the-art approaches to this problem currently rely on using fixed,
unsupervised representations of the pool, focusing on modifying the acquisition
function instead. We show that this model setup can undermine their
effectiveness at dealing with messy pools, as such representations can fail to
capture important information relevant to the task. To address this, we propose
using task-driven representations that are periodically updated during the
active learning process using the previously collected labels. We introduce two
specific strategies for learning these representations, one based on directly
learning semi-supervised representations and the other based on supervised
fine-tuning of an initial unsupervised representation. We find that both
significantly improve empirical performance over using unsupervised or
pretrained representations.

</details>


### [68] [Robust GNN Watermarking via Implicit Perception of Topological Invariants](https://arxiv.org/abs/2510.25934)
*Jipeng Li,Yannning Shen*

Main category: cs.LG

TL;DR: 提出 InvGNN-WM，一种触发器自适应的无触发水印方法，通过对图的隐式感知（归一化的代数连通性）进行预测，并通过带符号的解码器输出水印位，支持黑箱验证；对模型编辑具有鲁棒性，且对任务影响极小。


<details>
  <summary>Details</summary>
Motivation: 现有 GNN 水印大多依赖后门触发器，易在模型编辑后失效且存在所有权歧义，需一种触发器无、鲁棒且可黑箱验证的水印方案。

Method: 在一个所有者私有载体集上，使用轻量头预测归一化的代数连通性；一个符号感知解码器输出二进制水印位，设定阈值以控制误报率；在不同数据集/骨架上，与干净精度对比，水印精度提高；对剪枝、微调、量化等攻击鲁棒；对知识蒸馏的影响；引入带水印损失的 KD（KD+WM）恢复水印。

Result: 在各种数据集与骨架上，水印保持高与干净精度接近；水印在触发器与压缩基线上表现出色；对无结构剪枝、微调、后训练量化依然稳健；KD 会削弱水印，KD+WM 恢复；具备不可见性和鲁棒性保证，且证明精确删除为 NP 完全难题。

Conclusion: InvGNN-WM 提供一种触发器无、可黑箱验证且对任务影响很小的 GNN 水印方法，能在常见模型编辑下维持鲁棒性，理论上删除困难（NP 完全）。

Abstract: Graph Neural Networks (GNNs) are valuable intellectual property, yet many
watermarks rely on backdoor triggers that break under common model edits and
create ownership ambiguity. We present InvGNN-WM, which ties ownership to a
model's implicit perception of a graph invariant, enabling trigger-free,
black-box verification with negligible task impact. A lightweight head predicts
normalized algebraic connectivity on an owner-private carrier set; a
sign-sensitive decoder outputs bits, and a calibrated threshold controls the
false-positive rate. Across diverse node and graph classification datasets and
backbones, InvGNN-WM matches clean accuracy while yielding higher watermark
accuracy than trigger- and compression-based baselines. It remains strong under
unstructured pruning, fine-tuning, and post-training quantization; plain
knowledge distillation (KD) weakens the mark, while KD with a watermark loss
(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,
and we prove that exact removal is NP-complete.

</details>


### [69] [Modular Linear Tokenization (MLT)](https://arxiv.org/abs/2510.25952)
*Tcharlies Schmitz*

Main category: cs.LG

TL;DR: MLT是一种可逆、确定性的高基数分类标识符编码方法，通过有限域上的模运算和可逆线性变换实现紧凑向量，同时保持可逆性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决传统哈希和独热编码在高基数标识符下的不可逆性、内存开销与可扩展性问题，提供可控维度、低成本的可逆编码方案。

Method: 将高基数分类标识符通过模运算在有限域中映射，并应用可逆线性变换，构成一个全局一一映射，实现从标识符到紧凑向量的可逆编码，并保持对维度的显式控制。

Result: 在MovieLens 20M数据集上，MLT的预测性能接近有监督嵌入，但参数数量更少、训练成本更低。

Conclusion: MLT提供了一种可控维度、可扩展且可逆的高基数标识符编码方案，并已开源实现。

Abstract: This paper introduces Modular Linear Tokenization (MLT), a reversible and
deterministic technique for encoding high-cardinality categorical identifiers
into compact numerical vectors. Unlike traditional hashing or one-hot
encodings, MLT preserves bijective mappings by leveraging modular arithmetic
over finite fields and invertible linear transformations. The method offers
explicit control of dimensionality and computational scalability while
maintaining full reversibility, even for millions of identifiers. Experimental
results on the MovieLens 20M dataset show that MLT achieves comparable
predictive performance to supervised embeddings while requiring significantly
fewer parameters and lower training cost. An open-source implementation of MLT
is available on PyPI (https://pypi.org/project/light-mlt/) and GitHub
(https://github.com/tcharliesschmitz/light-mlt).

</details>


### [70] [Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi](https://arxiv.org/abs/2510.25954)
*Lynn Metz,Rachel Haggard,Michael Moszczynski,Samer Asbah,Chris Mwase,Patricia Khomani,Tyler Smith,Hannah Cooper,Annie Mwale,Arbaaz Muslim,Gautam Prasad,Mimi Sun,Tomer Shekel,Joydeep Paul,Anna Carter,Shravya Shetty,Dylan Green*

Main category: cs.LG

TL;DR: GeoFM embeddings (PDFM, AlphaEarth, and CDR) improve predictive performance over traditional geostatistics in Malawi for most routine health indicators; a Multi-GeoFM ensemble performs best; data-scarce targets like TB/malnutrition remain challenging.


<details>
  <summary>Details</summary>
Motivation: Address unreliable routine health data in LMICs by leveraging diverse geospatial representations to boost predictive modeling of health program outputs.

Method: XGBoost models trained on 552 health catchment areas in Malawi (Jan 2021–May 2023) to predict 15 routine health programmatic outputs. Compared embeddings from three GeoFMs (PDFM, AlphaEarth, CDR) against baseline geostatistical methods; used an 80/20 train/test split with 5-fold cross-validation in training.

Result: Embedding-based approaches outperformed baseline methods in 13 of 15 indicators (87%). A Multi-GeoFM model integrating all three sources achieved robust predictions with average CV R2s: population density 0.63, new HIV cases 0.57, child vaccinations 0.47; test set R2: 0.64, 0.68, 0.55 respectively. Poor performance for targets with limited primary data (e.g., TB, malnutrition).

Conclusion: Integrating multiple GeoFM sources provides modest but meaningful improvements for select health/demographic outcomes in LMIC settings and is a promising tool to strengthen constrained routine health information systems.

Abstract: The reliability of routine health data in low and middle-income countries
(LMICs) is often constrained by reporting delays and incomplete coverage,
necessitating the exploration of novel data sources and analytics. Geospatial
Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse
spatial, temporal, and behavioral data into mathematical embeddings that can be
efficiently used for downstream prediction tasks. This study evaluated the
predictive performance of three GeoFM embedding sources - Google Population
Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite
imagery), and mobile phone call detail records (CDR) - for modeling 15 routine
health programmatic outputs in Malawi, and compared their utility to
traditional geospatial interpolation methods. We used XGBoost models on data
from 552 health catchment areas (January 2021-May 2023), assessing performance
with R2, and using an 80/20 training and test data split with 5-fold
cross-validation used in training. While predictive performance was mixed, the
embedding-based approaches improved upon baseline geostatistical methods in 13
of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three
embedding sources produced the most robust predictions, achieving average
5-fold cross validated R2 values for indicators like population density (0.63),
new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,
0.68, and 0.55, respectively. Prediction was poor for prediction targets with
low primary data availability, such as TB and malnutrition cases. These results
demonstrate that GeoFM embeddings imbue a modest predictive improvement for
select health and demographic outcomes in an LMIC context. We conclude that the
integration of multiple GeoFM sources is an efficient and valuable tool for
supplementing and strengthening constrained routine health information systems.

</details>


### [71] [Contrastive Predictive Coding Done Right for Mutual Information Estimation](https://arxiv.org/abs/2510.25983)
*J. Jon Ryu,Pavan Yeddanapudi,Xiangxiang Xu,Gregory W. Wornell*

Main category: cs.LG

TL;DR: 提出 InfoNCE-anchor，通过引入辅助锚点实现更准确的互信息估计，并将其推广到基于正确评分规则的框架，统一多种对比目标。


<details>
  <summary>Details</summary>
Motivation: InfoNCE 作为互信息估计器存在偏差，且并非严格的互信息估计器。需要一个可一致估计密度比的改进方法，以提供更可靠的 MI 插件估计。

Method: 引入辅助 anchor 类，建立一致的密度比估计，并给出插件式 MI 估计器；进一步基于正确评分规则泛化该框架，InfoNCE 在 log score 下收敛为 InfoNCE-anchor；覆盖 NCE、InfoNCE、f-divergence 等对比目标的统一框架。

Result: 在数值实验中，InfoNCE-anchor 配合 log 分数给出最准确的 MI 估计；在自监督表示学习任务中，anchor 未显著提升下游任务表现，表明对比学习的收益来自结构化密度比的学习而非单纯的 MI 估计的准确性。

Conclusion: 该工作揭示对比表示学习的收益来自学习密度比的结构，而非精确估计 MI；提出的 InfoNCE-anchor 提供更可靠的 MI 插件估计，并将对比目标统一到一个原则框架。

Abstract: The InfoNCE objective, originally introduced for contrastive representation
learning, has become a popular choice for mutual information (MI) estimation,
despite its indirect connection to MI. In this paper, we demonstrate why
InfoNCE should not be regarded as a valid MI estimator, and we introduce a
simple modification, which we refer to as InfoNCE-anchor, for accurate MI
estimation. Our modification introduces an auxiliary anchor class, enabling
consistent density ratio estimation and yielding a plug-in MI estimator with
significantly reduced bias. Beyond this, we generalize our framework using
proper scoring rules, which recover InfoNCE-anchor as a special case when the
log score is employed. This formulation unifies a broad spectrum of contrastive
objectives, including NCE, InfoNCE, and $f$-divergence variants, under a single
principled framework. Empirically, we find that InfoNCE-anchor with the log
score achieves the most accurate MI estimates; however, in self-supervised
representation learning experiments, we find that the anchor does not improve
the downstream task performance. These findings corroborate that contrastive
representation learning benefits not from accurate MI estimation per se, but
from the learning of structured density ratios.

</details>


### [72] [Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations](https://arxiv.org/abs/2510.25993)
*Darius Masoum Zadeh-Jousdani,Elvin Hajizada,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: PCN-TA uses temporal amortization to preserve latent states across frames, reducing training overhead while maintaining performance for edge robotic learning.


<details>
  <summary>Details</summary>
Motivation: Edge robotics needs online, biologically plausible learning; backpropagation is powerful but not aligned with biological plausibility and may be suboptimal for continuous adaptation; Predictive Coding offers a plausible alternative but incurs high computational overhead due to multiple inference iterations.

Method: Propose Predictive Coding Network with Temporal Amortization (PCN-TA) that preserves latent states across temporal frames to exploit temporal correlations, thereby reducing inference steps. Evaluate on COIL-20 robotic perception data and compare against backpropagation and baseline PC networks.

Result: PCN-TA achieves 10% fewer weight updates than backpropagation and requires 50% fewer inference steps than baseline PC networks, indicating reduced computational overhead while maintaining learning performance.

Conclusion: Temporal amortization in predictive coding enables more efficient online learning at the edge, with favorable implications for real-time adaptation and neuromorphic hardware deployment.

Abstract: Robotic systems operating at the edge require efficient online learning
algorithms that can continuously adapt to changing environments while
processing streaming sensory data. Traditional backpropagation, while
effective, conflicts with biological plausibility principles and may be
suboptimal for continuous adaptation scenarios. The Predictive Coding (PC)
framework offers a biologically plausible alternative with local, Hebbian-like
update rules, making it suitable for neuromorphic hardware implementation.
However, PC's main limitation is its computational overhead due to multiple
inference iterations during training. We present Predictive Coding Network with
Temporal Amortization (PCN-TA), which preserves latent states across temporal
frames. By leveraging temporal correlations, PCN-TA significantly reduces
computational demands while maintaining learning performance. Our experiments
on the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%
fewer weight updates compared to backpropagation and requires 50% fewer
inference steps than baseline PC networks. These efficiency gains directly
translate to reduced computational overhead for moving another step toward edge
deployment and real-time adaptation support in resource-constrained robotic
systems. The biologically-inspired nature of our approach also makes it a
promising candidate for future neuromorphic hardware implementations, enabling
efficient online learning at the edge.

</details>


### [73] [Infrequent Exploration in Linear Bandits](https://arxiv.org/abs/2510.26000)
*Harin Lee,Min-hwan Oh*

Main category: cs.LG

TL;DR: INFEX通过在计划时段进行基线探索，在其他时段以贪婪策略执行，从而在较少探索的同时实现与标准高效算法相近的后悔界，提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么全自适应探索、要么纯贪婪，二者各有成本和假设限制。本文旨在在极少探索的前提下，保留和集成自适应探索能力，适用于安全或成本敏感场景。

Method: 提出INFEX框架：按给定时间表选择性进行基线探索，其余时间以贪婪策略执行；框架可无缝集成任意全自适应探索方法；理论分析给出在探索频率超过对数阈值时，能达到与标准高效算法相近的实例相关后悔界；并通过实验验证理论与效率提升。

Result: 在探索频率达到对数量级阈值以上时，INFEX获得与标准高效算法相匹配的实例相关后悔界；同时在计算成本方面实现显著提升；实验显示在回合数与数据规模上达到state-of-the-art的鲁棒性与运行时间优势。

Conclusion: INFEX提供一个简单、模块化、实用的框架，弥合自适应探索与贪婪之间的差距，适用于广泛线性Bandit场景，并可显著降低计算成本与提升实用性。

Abstract: We study the problem of infrequent exploration in linear bandits, addressing
a significant yet overlooked gap between fully adaptive exploratory methods
(e.g., UCB and Thompson Sampling), which explore potentially at every time
step, and purely greedy approaches, which require stringent diversity
assumptions to succeed. Continuous exploration can be impractical or unethical
in safety-critical or costly domains, while purely greedy strategies typically
fail without adequate contextual diversity. To bridge these extremes, we
introduce a simple and practical framework, INFEX, explicitly designed for
infrequent exploration. INFEX executes a base exploratory policy according to a
given schedule while predominantly choosing greedy actions in between. Despite
its simplicity, our theoretical analysis demonstrates that INFEX achieves
instance-dependent regret matching standard provably efficient algorithms,
provided the exploration frequency exceeds a logarithmic threshold.
Additionally, INFEX is a general, modular framework that allows seamless
integration of any fully adaptive exploration method, enabling wide
applicability and ease of adoption. By restricting intensive exploratory
computations to infrequent intervals, our approach can also enhance
computational efficiency. Empirical evaluations confirm our theoretical
findings, showing state-of-the-art regret performance and runtime improvements
over existing methods.

</details>


### [74] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: 提出一种双MoE框架用于离散时间生存分析，通过对特征编码器MoE实现子组自适应表示学习、风险窗口MoE结合患者特征与时间嵌入捕捉时间动态，提升生存预测性能。


<details>
  <summary>Details</summary>
Motivation: 生存分析需兼顾患者异质性与随时间变化的风险；单一模型难以同时有效建模两者，需模块化的灵活结构。

Method: 设计两部分MoE：特征编码器MoE用于子组表示学习，风险MoE利用患者特征和时间嵌入计算时间相关的风险；可与现有深度生存管线无缝集成，且可嵌入Consurv框架。

Result: 在METABRIC和GBSG乳腺癌数据集上，方法在时序C-index方面的提升最高可达0.04；集成到Consurv后有进一步收益。

Conclusion: 双MoE框架为离散时间生存分析提供灵活且有效的建模路径，能更好地处理异质性与时间动态，适合嵌入现有管线中以提升预测性能。

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [75] [Exploring Human-AI Conceptual Alignment through the Prism of Chess](https://arxiv.org/abs/2510.26025)
*Semyon Lomaso,Judah Goldfeder,Mehmet Hamza Erol,Matthew So,Yao Yan,Addison Howard,Nathan Kutz,Ravid Shwartz Ziv*

Main category: cs.LG

TL;DR: 在棋类任务中，270M参数的Transformer对人类概念有层级分布的差异：早层保留人类概念的编码能力，深层虽然提升了棋力，却逐渐形成与人类思维偏离的“外星”表征；通过Chess960数据集测试概念鲁棒性，发现对开局记忆的依赖明显，概念识别在移除开局理论后下降10-20%。


<details>
  <summary>Details</summary>
Motivation: 质疑AI系统是否真正理解人类概念，还是仅仅在统计层面模仿表面模式；通过分层表征分析和新数据集，揭示当前模型在概念理解与策略优化之间的矛盾。

Method: 对一个270M参数的Transformer进行逐层分析，评估其对中心控制、骑士据点等棋理概念的编码（最高可达85%准确度在早层）。引入Chess960数据集（240个专家标注的棋位，涵盖6种策略概念），在随机起始位置移除开局理论，比较不同方法的概念识别与鲁棒性，并比较随层数增加的表征变化。

Result: 早层对人类概念的编码准确率可达85%，但深层表征在推动更强的棋力时退化为与人类思维相悖的“外星”表示，准确度降至50-65%；移除开局理论后，概念识别在所有方法上下降10-20%。

Conclusion: 当前架构在追求性能的过程中，形成与人类思维日益偏离的表示，揭示AI系统的‘外星智能’风险；这对需要人机协作的创意AI应用提出挑战，提示需要发展与人类概念对齐的表示与训练目标。

Abstract: Do AI systems truly understand human concepts or merely mimic surface
patterns? We investigate this through chess, where human creativity meets
precise strategic concepts. Analyzing a 270M-parameter transformer that
achieves grandmaster-level play, we uncover a striking paradox: while early
layers encode human concepts like center control and knight outposts with up to
85\% accuracy, deeper layers, despite driving superior performance, drift
toward alien representations, dropping to 50-65\% accuracy. To test conceptual
robustness beyond memorization, we introduce the first Chess960 dataset: 240
expert-annotated positions across 6 strategic concepts. When opening theory is
eliminated through randomized starting positions, concept recognition drops
10-20\% across all methods, revealing the model's reliance on memorized
patterns rather than abstract understanding. Our layer-wise analysis exposes a
fundamental tension in current architectures: the representations that win
games diverge from those that align with human thinking. These findings suggest
that as AI systems optimize for performance, they develop increasingly alien
intelligence, a critical challenge for creative AI applications requiring
genuine human-AI collaboration. Dataset and code are available at:
https://github.com/slomasov/ChessConceptsLLM.

</details>


### [76] [Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods](https://arxiv.org/abs/2510.26038)
*Jiali Cheng,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: Knowledge distillation (KD) often undermines debiasing transferred from teacher to student, with robustness varying by bias type. Three remedies are proposed: high-quality augmentation data, iterative KD, and initializing the student with teacher weights. The study is among the first to examine KD's impact on debiasing at scale.


<details>
  <summary>Details</summary>
Motivation: Spurious correlations reduce out-of-distribution performance. It is important to understand whether KD helps or harms the transfer of debiasing capabilities from teacher to student in NLP and vision tasks.

Method: Extensive experiments on natural language inference (NLI) and image classification to assess how debiasing transfers through KD. Analysis of internal attention patterns and circuits. Ablations to identify factors affecting distillability of debiasing. Proposals of three mitigation strategies: data augmentation, iterative KD, and weight initialization from the teacher.

Result: (i) Debiasing capabilities are generally weakened after KD. (ii) Training a debiased model does not gain from incorporating teacher knowledge. (iii) Overall robustness may stay similar post-KD, but biases show substantial variation. (iv) Identified internal attention patterns/circuits linked to the differing behavior after KD.

Conclusion: KD interacts complexly with debiasing: it can hinder transfer of debiasing, but targeted strategies can improve distillability. The work offers initial scale insights into KD’s impact on debiasing and guidance for designing better debiasing methods.

Abstract: Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.

</details>


### [77] [Towards Scaling Laws for Symbolic Regression](https://arxiv.org/abs/2510.26064)
*David Otte,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: SR性能随计算量呈现明确的幂律关系，并且存在计算最优的超参数扩展规律。


<details>
  <summary>Details</summary>
Motivation: 系统研究SR领域在规模方面的规律性尚未充分探索，借鉴语言模型的Scaling Laws，以指导大规模SR模型的训练与资源分配，提升可解释性与泛化性。

Method: 构建一个可扩展的端到端Transformer管线，生成覆盖五种模型规模、跨越三个数量级计算量的训练数据，评估验证损失和求解率，并分析批量大小、学习率与令牌-参数比等超参数随模型规模和计算量的变化。

Result: 验证损失与求解率与计算量呈现清晰的幂律趋势；发现计算最优的超参数扩展：批量大小和学习率随模型规模增大而增大，令牌-参数比约为15，在计算量增加时有小幅上升趋势。

Conclusion: SR的性能在很大程度上可由计算量预测，该结果为训练下一代SR模型提供了重要的资源配置与扩展洞见。

Abstract: Symbolic regression (SR) aims to discover the underlying mathematical
expressions that explain observed data. This holds promise for both gaining
scientific insight and for producing inherently interpretable and generalizable
models for tabular data. In this work we focus on the basics of SR. Deep
learning-based SR has recently become competitive with genetic programming
approaches, but the role of scale has remained largely unexplored. Inspired by
scaling laws in language modeling, we present the first systematic
investigation of scaling in SR, using a scalable end-to-end transformer
pipeline and carefully generated training data. Across five different model
sizes and spanning three orders of magnitude in compute, we find that both
validation loss and solved rate follow clear power-law trends with compute. We
further identify compute-optimal hyperparameter scaling: optimal batch size and
learning rate grow with model size, and a token-to-parameter ratio of
$\approx$15 is optimal in our regime, with a slight upward trend as compute
increases. These results demonstrate that SR performance is largely predictable
from compute and offer important insights for training the next generation of
SR models.

</details>


### [78] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: 将模型视为可塑几何对象，通过在流形上优化度量张量场来动态塑形模型空间的几何结构，构建基于离散差分几何的可微分框架，并揭示与爱因斯坦-希伯特作用的物理类比。


<details>
  <summary>Details</summary>
Motivation: 旨在超越传统固定几何空间中对参数的优化，提出数据驱动几何的观念，使模型的几何结构本身可塑，从而提升表达能力并提供对科学模型发现与鲁棒表示学习的潜在增强。

Method: 建立变分框架以平衡数据保真度与流形的几何复杂度；将无限维优化问题离散化为三角网格，参数化度量张量为边长，结合自动微分实现高效优化；理论上将该框架与广义相对论中的爱因斯坦-希伯特作用建立类比。

Result: 提出了一种数据驱动的几何优化范式，证明在固定拓扑下，度量优化可显著提升表达能力；离散化后可用现有工具高效求解，理论上提供与物理几何的深刻类比，指向可动态演化几何与拓扑的元学习框架及在科学建模与鲁棒表示学习中的广泛应用前景。

Conclusion: 该工作为构建完全动态的“元学习器”奠定基础，使其能够自治地进化几何结构与拓扑，并指向在科学建模与鲁棒表示学习等领域的广泛应用。

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [79] [New Money: A Systematic Review of Synthetic Data Generation for Finance](https://arxiv.org/abs/2510.26076)
*James Meldrum,Basem Suleiman,Fethi Rabhi,Muhammad Johan Alibasa*

Main category: cs.LG

TL;DR: 系统性综述汇总72项自2018年以来关于金融数据的合成数据研究，发现在GAN对时序数据和表格信用数据生成方面占主导，但对隐私保护的严格评估仍不足，存在显著研究空缺和未来方向。


<details>
  <summary>Details</summary>
Motivation: 合成数据可在保护隐私和合规前提下利用敏感金融数据进行机器学习，但在金融领域的生成方法、评估策略尚不统一，亟需综合梳理现状与空缺。

Method: 对公开文献进行系统性综述，筛选72篇研究（2018年后），分类金融信息类型、生成方法、以及用于评估数据实用性和隐私性的评估策略。

Result: GAN方法在文献中占主导，尤其用于时间序列市场数据和表格信用数据。多种新颖方法展现出提高真实感与隐私保护的潜力，但对隐私防护的严格评估仍普遍不足。

Conclusion: 提供了生成技术、应用领域与评估方法的综合概览，指出关键研究空缺，提出未来在金融领域开发鲁棒、隐私保护的合成数据解决方案的方向与建议。

Abstract: Synthetic data generation has emerged as a promising approach to address the
challenges of using sensitive financial data in machine learning applications.
By leveraging generative models, such as Generative Adversarial Networks (GANs)
and Variational Autoencoders (VAEs), it is possible to create artificial
datasets that preserve the statistical properties of real financial records
while mitigating privacy risks and regulatory constraints. Despite the rapid
growth of this field, a comprehensive synthesis of the current research
landscape has been lacking. This systematic review consolidates and analyses 72
studies published since 2018 that focus on synthetic financial data generation.
We categorise the types of financial information synthesised, the generative
methods employed, and the evaluation strategies used to assess data utility and
privacy. The findings indicate that GAN-based approaches dominate the
literature, particularly for generating time-series market data and tabular
credit data. While several innovative techniques demonstrate potential for
improved realism and privacy preservation, there remains a notable lack of
rigorous evaluation of privacy safeguards across studies. By providing an
integrated overview of generative techniques, applications, and evaluation
methods, this review highlights critical research gaps and offers guidance for
future work aimed at developing robust, privacy-preserving synthetic data
solutions for the financial domain.

</details>


### [80] [Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism](https://arxiv.org/abs/2510.26083)
*Yuhua Jiang,Shuang Cheng,Yihao Liu,Ermo Hua,Che Jiang,Weigao Sun,Yu Cheng,Feifei Gao,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: Nirvana是一种具备专用记忆机制的通用专家化模型，具有线性时间复杂度和测试时任务信息提取能力。通过Task-Aware Memory Trigger（Trigger）和Specialized Memory Updater（Updater）实现对任务与领域偏移的自适应记忆更新。在通用语言任务与医疗MRI任务上均展现竞争力或优越性，且能生成初步临床报告。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM结构往往缺乏基于任务信息的专用记忆机制来引导记忆更新，难以在域变动下保持广泛能力与专家级性能的平衡。需要一种能够在推理过程中动态调整记忆、并以更高效的复杂度处理大规模输入的通用专家化模型，尤其是在医学等专业领域。

Method: 提出Nirvana架构，包含Task-Aware Memory Trigger（Trigger）用于将每个输入样本视为自监督微调任务，从而在运行时根据任务需求调整记忆相关参数；Specialized Memory Updater（Updater）据Trigger动态 memorizes上下文。对语言任务进行常规评测，并对MRI等医学任务进行后训练冻结骨干、并采用轻量编解码器对电磁信号与MRI图像进行配对重建，验证Trigger在专门领域的有效性。

Result: 在自然语言建模基准上，Nirvana达到与现有LLM结构相竞争甚至更优的表现。在MRI任务上，Nirvana的重建质量优于传统MRI模型及以传统LLM骨干的模型，且能够生成初步临床报告。

Conclusion: 引入Trigger与Updater的专用记忆机制使Nirvana在测试时即可自适应任务需求与领域变动，具线性时间复杂度，适用于通用语言任务与医学等专业领域，展示了SGM在跨域高效适配中的潜力。

Abstract: Specialized Generalist Models (SGMs) aim to preserve broad capabilities while
achieving expert-level performance in target domains. However, traditional LLM
structures including Transformer, Linear Attention, and hybrid models do not
employ specialized memory mechanism guided by task information. In this paper,
we present Nirvana, an SGM with specialized memory mechanism, linear time
complexity, and test-time task information extraction. Besides, we propose the
Task-Aware Memory Trigger ($\textit{Trigger}$) that flexibly adjusts memory
mechanism based on the current task's requirements. In Trigger, each incoming
sample is treated as a self-supervised fine-tuning task, enabling Nirvana to
adapt its task-related parameters on the fly to domain shifts. We also design
the Specialized Memory Updater ($\textit{Updater}$) that dynamically memorizes
the context guided by Trigger. We conduct experiments on both general language
tasks and specialized medical tasks. On a variety of natural language modeling
benchmarks, Nirvana achieves competitive or superior results compared to the
existing LLM structures. To prove the effectiveness of Trigger on specialized
tasks, we test Nirvana's performance on a challenging medical task, i.e.,
Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with
lightweight codecs on paired electromagnetic signals and MRI images. Despite
the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI
domain with the change of task-related parameters. Nirvana achieves
higher-quality MRI reconstruction compared to conventional MRI models as well
as the models with traditional LLMs' backbone, and can also generate accurate
preliminary clinical reports accordingly.

</details>


### [81] [LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline](https://arxiv.org/abs/2510.26086)
*Zheng Zhang,Haonan Li,Xingyu Li,Hang Zhang,Zhiyun Qian*

Main category: cs.LG

TL;DR: 提出一个基于大语言模型的多阶段补丁分析管线，用于定位Bug Inducing Commit（BIC），通过充分利用补丁信息、在上下文中比较候选提交并逐步下沉筛选，显著提高定位准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于补丁的BIC定位存在若干局限：依赖相同函数修改的假设、仅基于代码变化且忽略提交信息中的漏洞相关线索、使用简单启发式且缺乏对漏洞的逻辑分析。LLMs具备同时理解文本与代码的能力，有望突破这些瓶颈。

Method: 提出一个多阶段流水线：1) 充分利用补丁信息，2) 在上下文中对比多个候选提交，3) 通过一系列下沉筛选步骤逐步缩小候选集；核心依赖LLMs进行文本与代码混合分析与推理。

Result: 评估结果显示：与现有最先进解决方案相比，准确性提升超过38%；相较于一个基线的LLM分支法，综合多阶段管线的提升约60%。

Conclusion: 多阶段LLM管线能够克服传统BIC定位的局限，充分利用文本与代码信息，显著提升定位精度，指出LLM驱动的综合流程是实现改进的关键。

Abstract: Bug bisection has been an important security task that aims to understand the
range of software versions impacted by a bug, i.e., identifying the commit that
introduced the bug. However, traditional patch-based bisection methods are
faced with several significant barriers: For example, they assume that the
bug-inducing commit (BIC) and the patch commit modify the same functions, which
is not always true. They often rely solely on code changes, while the commit
message frequently contains a wealth of vulnerability-related information. They
are also based on simple heuristics (e.g., assuming the BIC initializes lines
deleted in the patch) and lack any logical analysis of the vulnerability.
  In this paper, we make the observation that Large Language Models (LLMs) are
well-positioned to break the barriers of existing solutions, e.g., comprehend
both textual data and code in patches and commits. Unlike previous BIC
identification approaches, which yield poor results, we propose a comprehensive
multi-stage pipeline that leverages LLMs to: (1) fully utilize patch
information, (2) compare multiple candidate commits in context, and (3)
progressively narrow down the candidates through a series of down-selection
steps. In our evaluation, we demonstrate that our approach achieves
significantly better accuracy than the state-of-the-art solution by more than
38\%. Our results further confirm that the comprehensive multi-stage pipeline
is essential, as it improves accuracy by 60\% over a baseline LLM-based
bisection method.

</details>


### [82] [SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth](https://arxiv.org/abs/2510.26099)
*Nick Masi,Randall Balestriero*

Main category: cs.LG

TL;DR: 提出 SAFE，通过对地球表面的地理和社会属性进行分层评估，揭示全球平均指标背后的区域与属性差异，并提供开源工具用于评估预测模型的公平性。


<details>
  <summary>Details</summary>
Motivation: 全球平均损失/准确度忽略了地理、收入、土地覆盖等非均匀分布带来的差异，可能掩盖模型在特定地区或群体中的性能偏差，需要更细粒度的评估来理解和改进公平性与适用性。

Method: 引入 SAFE，将地理格点的多种属性（国家/区域、收入水平、土地覆盖等）整合于评估框架，按这些属性对预测结果进行分层评估；对多种前沿AI天气预测模型进行基准测试，比较不同属性下的表现；在不同预报时长和气候变量上进行分层公平特征基准。

Result: 发现所有模型在多种属性上存在显著的分层差异，全球平均性能掩盖了局部和属性相关的偏差；通过分层评估构建了不同 lead times 和气候变量的预测公平特征基准，揭示哪些模型在特定属性上更优或更差。

Conclusion: 提供一个开放源代码工具箱，推动分层评估成为天气预测模型评估的新常态，帮助研究者识别全球尺度上的最佳与最劣表现区域，并评估不同模型的公平性。

Abstract: The dominant paradigm in machine learning is to assess model performance
based on average loss across all samples in some test set. This amounts to
averaging performance geospatially across the Earth in weather and climate
settings, failing to account for the non-uniform distribution of human
development and geography. We introduce Stratified Assessments of Forecasts
over Earth (SAFE), a package for elucidating the stratified performance of a
set of predictions made over Earth. SAFE integrates various data domains to
stratify by different attributes associated with geospatial gridpoints:
territory (usually country), global subregion, income, and landcover (land or
water). This allows us to examine the performance of models for each individual
stratum of the different attributes (e.g., the accuracy in every individual
country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of
state-of-the-art AI-based weather prediction models, finding that they all
exhibit disparities in forecasting skill across every attribute. We use this to
seed a benchmark of model forecast fairness through stratification at different
lead times for various climatic variables. By moving beyond globally-averaged
metrics, we for the first time ask: where do models perform best or worst, and
which models are most fair? To support further work in this direction, the SAFE
package is open source and available at https://github.com/N-Masi/safe

</details>


### [83] [Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error](https://arxiv.org/abs/2510.26109)
*Chenming Tang,Hsiu-Yuan Huang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: LTE通过让LLMs从他们之前生成的错误答案中学习，并控制回答长度，無需外部专家即可缓解RLVR中的探索停滞并提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR受限于模型初始能力，容易陷入探索停滞；尽管有些方法使用离线策略或外部专家引导，但往往不可用或受限。

Method: LTE提出让模型从历史自我错误中获得提示，同时处理回复长度问题，在无外部专家指导的前提下进行学习。

Result: 实验显示LTE相较于GRPO在六个数学基准上的Qwen3-4B-Base模型，Pass@1提升6.38、Pass@k提升9.00，且分析表明LTE缓解了探索停滞、提升了训练中的探索与利用。

Conclusion: LTE成功在无外部引导条件下提升RLVR性能，并改善探索与利用的权衡。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has significantly
boosted the reasoning capability of large language models (LLMs) recently.
However, existing RLVR approaches merely train LLMs based on their own
generated responses and are constrained by the initial capability of LLMs, thus
prone to exploration stagnation, in which LLMs fail to solve more training
problems and cannot further learn from the training data. Some work tries to
address this by leveraging off-policy solutions to training problems but
requires external guidance from experts which suffers from limited
availability. In this work, we propose LTE (Learning to reason from Trial and
Error), an approach hinting LLMs with their previously self-generated incorrect
answers and problem of overlong responses, which does not require any external
expert guidance. Experiments validate the effectiveness of LTE, which
outperforms the normal group relative policy optimization (GRPO) by 6.38 in
Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for
Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the
problem of exploration stagnation and enhances both exploitation and
exploration during training.

</details>


### [84] [maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition](https://arxiv.org/abs/2510.26146)
*Kexing Liu*

Main category: cs.LG

TL;DR: 提出 maxVSTAR，通过跨模态师生框架与YOLO视觉信号实现对CSI基HAR的闭环自适应，在边缘端实现在线微调STAR以对抗域移，提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CSI基HAR具备隐私保护和设备无感知感知能力，但在不同环境与硬件条件下存在显著域移，导致识别性能下降，亟需边缘端的自适应机制。

Method: 建立跨模态教师-学生架构，由高精度的YOLO视觉模型作为动态监督信号，为CSI数据流提供实时 activity 标签；在边缘端对轻量级 CSI HAR 模型STAR进行在线微调，形成闭环自适应 retraining，达到持续适应环境变化的目标。

Result: 在未标定硬件上，STAR 的识别准确率从 93.52% 降至 49.14%；经一次视觉引导的自适应循环后，准确率恢复到 81.51%，显示出对域移的有效缓解与长期自适应能力。

Conclusion: maxVSTAR 展示了在隐私友好 IoT 环境中的自监督、跨模态自适应在边缘端的可行性和可扩展性，为基于 CSI 的 HAR 提供了持续自适应的实际范式。

Abstract: WiFi Channel State Information (CSI)-based human activity recognition (HAR)
provides a privacy-preserving, device-free sensing solution for smart
environments. However, its deployment on edge devices is severely constrained
by domain shift, where recognition performance deteriorates under varying
environmental and hardware conditions. This study presents maxVSTAR (maximally
adaptive Vision-guided Sensing Technology for Activity Recognition), a
closed-loop, vision-guided model adaptation framework that autonomously
mitigates domain shift for edge-deployed CSI sensing systems. The proposed
system integrates a cross-modal teacher-student architecture, where a
high-accuracy YOLO-based vision model serves as a dynamic supervisory signal,
delivering real-time activity labels for the CSI data stream. These labels
enable autonomous, online fine-tuning of a lightweight CSI-based HAR model,
termed Sensing Technology for Activity Recognition (STAR), directly at the
edge. This closed-loop retraining mechanism allows STAR to continuously adapt
to environmental changes without manual intervention. Extensive experiments
demonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated
hardware, the baseline STAR model's recognition accuracy declined from 93.52%
to 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored
the accuracy to 81.51%. These results confirm the system's capacity for
dynamic, self-supervised model adaptation in privacy-conscious IoT
environments, establishing a scalable and practical paradigm for long-term
autonomous HAR using CSI sensing at the network edge.

</details>


### [85] [STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments](https://arxiv.org/abs/2510.26148)
*Kexing Liu*

Main category: cs.LG

TL;DR: 在嵌入式低功耗设备上实现基于Wi-Fi CSI的实时隐私友好HAR的轻量化STAR框架，通过GRU、分阶段去噪和硬件共优化实现高精度低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有CSI HAR方法在资源受限的移动边缘设备上的计算开销、延迟和可行性问题，提升实时性与能效。

Method: 提出STAR，采用轻量GRU替代LSTM，8阶巴特沃斯低通+中值滤波+经验模态分解的多阶段去噪，硬件感知的共优化，在Rockchip RV1126+NPU与ESP32-S3 CSI采集模块上实现量化推理。

Result: 在七类动作上平均识别准确性93.52%，人存在检测99.11%；模型参数约97.6k，INT8推理频率33MHz，CPU利用率8%，比CPU实现快六倍；亚秒级响应、低功耗，适用于移动与普适计算环境。

Conclusion: STAR实现了在低功耗嵌入设备上的实时、隐私友好HAR，提供可扩展的移动与普适计算解决方案。

Abstract: Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)
presents a privacy-preserving, contactless sensing approach suitable for smart
homes, healthcare monitoring, and mobile IoT systems. However, existing methods
often encounter computational inefficiency, high latency, and limited
feasibility within resource-constrained, embedded mobile edge environments.
This paper proposes STAR (Sensing Technology for Activity Recognition), an
edge-AI-optimized framework that integrates a lightweight neural architecture,
adaptive signal processing, and hardware-aware co-optimization to enable
real-time, energy-efficient HAR on low-power embedded devices. STAR
incorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural
network, reducing model parameters by 33% compared to conventional LSTM models
while maintaining effective temporal modeling capability. A multi-stage
pre-processing pipeline combining median filtering, 8th-order Butterworth
low-pass filtering, and Empirical Mode Decomposition (EMD) is employed to
denoise CSI amplitude data and extract spatial-temporal features. For on-device
deployment, STAR is implemented on a Rockchip RV1126 processor equipped with an
embedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI
acquisition module. Experimental results demonstrate a mean recognition
accuracy of 93.52% across seven activity classes and 99.11% for human presence
detection, utilizing a compact 97.6k-parameter model. INT8 quantized inference
achieves a processing speed of 33 MHz with just 8% CPU utilization, delivering
sixfold speed improvements over CPU-based execution. With sub-second response
latency and low power consumption, the system ensures real-time,
privacy-preserving HAR, offering a practical, scalable solution for mobile and
pervasive computing environments.

</details>


### [86] [Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment](https://arxiv.org/abs/2510.26157)
*Hyuntae Park,Yeachan Kim,SangKeun Lee*

Main category: cs.LG

TL;DR: MolBridge 提出基于子结构对齐的分子-文本学习框架，通过引入来自分子子结构与化学短语的对齐信号、子结构感知对比学习以及自我精炼机制，提升对微观对应关系的捕捉并在多项分子基准上超越最新方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉分子子结构与描述之间的细粒度对齐，导致对化学信息的理解不足，需提高对分子-文本对齐的细粒度能力。

Method: 在分子-描述对中扩增来自子结构与化学短语的对齐信号；采用子结构感知对比学习；引入自我精炼机制以过滤噪声对齐信号。

Result: 在广泛的分子基准上显著优于最新基线，验证了子结构感知对齐的有效性与实用性。

Conclusion: 子结构感知对齐是分子-文本学习中的关键能力，能够提升细粒度对应关系的学习并推动相关任务的性能提升。

Abstract: Molecule and text representation learning has gained increasing interest due
to its potential for enhancing the understanding of chemical information.
However, existing models often struggle to capture subtle differences between
molecules and their descriptions, as they lack the ability to learn
fine-grained alignments between molecular substructures and chemical phrases.
To address this limitation, we introduce MolBridge, a novel molecule-text
learning framework based on substructure-aware alignments. Specifically, we
augment the original molecule-description pairs with additional alignment
signals derived from molecular substructures and chemical phrases. To
effectively learn from these enriched alignments, MolBridge employs
substructure-aware contrastive learning, coupled with a self-refinement
mechanism that filters out noisy alignment signals. Experimental results show
that MolBridge effectively captures fine-grained correspondences and
outperforms state-of-the-art baselines on a wide range of molecular benchmarks,
highlighting the significance of substructure-aware alignment in molecule-text
learning.

</details>


### [87] [Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series](https://arxiv.org/abs/2510.26159)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: 在多变量工业时间序列的异常检测中，简单的随机森林+XGBoost分段数据的组合表现优于复杂的特征工程与混合模型；简单且分段优化的策略在数据高度不平衡且时序不确定的情境下更具鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 评估复杂特征工程（如基于变点的统计特征、基于聚类的子结构表示）和混合学习策略在工业时间序列异常检测中的有效性，并比较其与简单基线的性能差异。

Method: 比较多种方法：变点派生特征、聚类子结构表示、混合学习架构，以及基于分段数据的随机森林+XGBoost集成；在蒸汽涡轮系统的多变量时间序列上进行评估。

Result: 简单的 Random Forest + XGBoost 在分段数据上的集合达到 AUC-ROC 0.976、F1 0.41，并在定义的时间窗口内实现100%早期检测；复杂方法未能超过该基线。

Conclusion: 在高度不平衡且时间不确定的数据场景中，模型简洁性结合良好分段策略可提供更高的鲁棒性、可解释性和操作实用性，优于更复杂的体系结构。

Abstract: In this study, we investigate the effectiveness of advanced feature
engineering and hybrid model architectures for anomaly detection in a
multivariate industrial time series, focusing on a steam turbine system. We
evaluate the impact of change point-derived statistical features,
clustering-based substructure representations, and hybrid learning strategies
on detection performance. Despite their theoretical appeal, these complex
approaches consistently underperformed compared to a simple Random Forest +
XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of
0.976, F1-score of 0.41, and 100% early detection within the defined time
window. Our findings highlight that, in scenarios with highly imbalanced and
temporally uncertain data, model simplicity combined with optimized
segmentation can outperform more sophisticated architectures, offering greater
robustness, interpretability, and operational utility.

</details>


### [88] [A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation](https://arxiv.org/abs/2510.26184)
*Songxin Lei,Qiongyan Wang,Yanchen Zhu,Hanyu Yao,Sijie Ruan,Weilin Ruan,Yuyu Luo,Huaming Wu,Yuxuan Liang*

Main category: cs.LG

TL;DR: 将公共资源分配问题建模为协同潜在博弈，并提出时空强化学习框架GSTRL，以处理容量约束和时空动态；在真实数据集上表现优越，具理论与应用意义。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦单一资源的移动优化，忽略容量约束和时空耦合，导致现实场景下的可行性和效率不足。提出CPRA以系统性地考虑容量约束与时空特性；并通过GSTRL实现对该问题的可行近似求解。

Method: 将CPRA问题表述为潜在博弈，证明潜在函数与目标最优之间无Gap，从而为近似求解纳什均衡提供理论基础；设计GSTRL框架以捕捉系统的时空动态并实现应对NP-hard的近似解。

Result: 在两个真实数据集上评估，GSTRL表现出优越性能，实验结果支持理论假设，并且源代码在补充材料中提供。

Conclusion: 理论上潜在博弈框架消除了目标最优与潜在函数之间的差距，为CPRA的近似解提供坚实基础；GSTRL有效捕捉时空动态，具备现实应用潜力。

Abstract: Public resource allocation involves the efficient distribution of resources,
including urban infrastructure, energy, and transportation, to effectively meet
societal demands. However, existing methods focus on optimizing the movement of
individual resources independently, without considering their capacity
constraints. To address this limitation, we propose a novel and more practical
problem: Collaborative Public Resource Allocation (CPRA), which explicitly
incorporates capacity constraints and spatio-temporal dynamics in real-world
scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal
Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:
1) We formulate the CPRA problem as a potential game and demonstrate that there
is no gap between the potential function and the optimal target, laying a solid
theoretical foundation for approximating the Nash equilibrium of this NP-hard
problem; and 2) Our designed GSTRL framework effectively captures the
spatio-temporal dynamics of the overall system. We evaluate GSTRL on two
real-world datasets, where experiments show its superior performance. Our
source codes are available in the supplementary materials.

</details>


### [89] [Accumulative SGD Influence Estimation for Data Attribution](https://arxiv.org/abs/2510.26185)
*Yunxiao Shi,Shuo Yang,Yixin Su,Rui Zhang,Min Xu*

Main category: cs.LG

TL;DR: 提出 ACC-SGD-IE，一种轨迹感知的留一法影响估计器，通过在训练过程传播扰动来累积影响状态，显著提升长期训练中的影响估计准确性，并改进数据清洗的效果。


<details>
  <summary>Details</summary>
Motivation: 现有 SGD-IE 通过按纪元求和近似逐样影响，但忽略跨纪元的积累与交互，导致对关键样本的排名偏差。需要能跨训练轨迹传播扰动、在不同训练阶段保持一致性和收敛性的方法。

Method: 引入 ACC-SGD-IE：在每一步维持一个累积影响状态，并将留一法扰动在训练过程上传播，形成轨迹感知的估计；对强凸光滑情形证明几何误差收缩，在光滑非凸情形给出更紧的误差界；增大批量时常数进一步减小。

Result: 在 Adults、20 Newsgroups、MNIST 的清洁与污染数据、以及凸/非凸训练场景下，ACC-SGD-IE 提供更精准的影响估计，尤其在长训练周期中表现更好；用于下游数据清洗时能更可靠地发现噪声样本，使用 ACC-SGD-IE 清洗的模型优于使用 SGD-IE 清洗的模型。

Conclusion: 轨迹感知的留一影响估计提供了更可靠的样本影响力评估，降低误排序风险，提升对长训练过程的稳健性及数据清洗效果，且批量大小的增大有利于进一步提升性能。

Abstract: Modern data-centric AI needs precise per-sample influence. Standard SGD-IE
approximates leave-one-out effects by summing per-epoch surrogates and ignores
cross-epoch compounding, which misranks critical examples. We propose
ACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out
perturbation across training and updates an accumulative influence state at
each step. In smooth strongly convex settings it achieves geometric error
contraction and, in smooth non-convex regimes, it tightens error bounds; larger
mini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,
and MNIST under clean and corrupted data and both convex and non-convex
training, ACC-SGD-IE yields more accurate influence estimates, especially over
long epochs. For downstream data cleansing it more reliably flags noisy
samples, producing models trained on ACC-SGD-IE cleaned data that outperform
those cleaned with SGD-IE.

</details>


### [90] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 通过对健康索赔数据应用逻辑回归、随机森林和支持向量机，结合主成分分析进行降维，评估再入院的预测模型。随机森林在AUC上表现最佳，其次为逻辑回归和SVM，研究旨在识别影响再入院的关键因素并用于靶向干预以降低成本与提升护理质量。


<details>
  <summary>Details</summary>
Motivation: 降低可避免的医院再入院率作为国家级优先事项；再入院率作为医院质量的基准之一；由于数据高维，需要降维和多模型比较以识别关键因素并改进护理。

Method: 使用健康索赔数据构建并比较逻辑回归、随机森林和支持向量机等模型；对高维数据应用主成分分析进行降维；以AUC作为评价指标进行模型比较。

Result: 随机森林达到最高的AUC性能，其次是逻辑回归和SVM；模型可用于识别导致再入院的关键因素并帮助筛选高风险患者以减少再入院及相关成本。

Conclusion: 所构建的模型框架可用于识别影响再入院的关键因素，支持针对性干预以降低再入院率和医疗成本，从而提升护理质量。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [91] [Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space](https://arxiv.org/abs/2510.26219)
*Sekitoshi Kanai,Tsukasa Yoshida,Hiroshi Takahashi,Haru Kuroki,Kazumune Hashimoto*

Main category: cs.LG

TL;DR: AISP is a test-time alignment method for LLMs that uses adaptive importance sampling on pre-logits to maximize expected rewards, outperforming best-of-n sampling and other reward-based methods.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large language models is computationally expensive; a high-performance, training-free test-time alignment method is desirable.

Method: Apply Gaussian perturbations to pre-logits (penultimate layer) and optimize the perturbation mean via importance sampling of rewards within a sampling-based model predictive control framework.

Result: AISP achieves higher rewards than best-of-n sampling given the same number of samples and outperforms other reward-based test-time alignment approaches.

Conclusion: Adaptive importance sampling on pre-logits (AISP) provides an effective, sample-efficient approach for test-time alignment of LLMs without fine-tuning.

Abstract: Test-time alignment of large language models (LLMs) attracts attention
because fine-tuning LLMs requires high computational costs. In this paper, we
propose a new test-time alignment method called adaptive importance sampling on
pre-logits (AISP) on the basis of the sampling-based model predictive control
with the stochastic control input. AISP applies the Gaussian perturbation into
pre-logits, which are outputs of the penultimate layer, so as to maximize
expected rewards with respect to the mean of the perturbation. We demonstrate
that the optimal mean is obtained by importance sampling with sampled rewards.
AISP outperforms best-of-n sampling in terms of rewards over the number of used
samples and achieves higher rewards than other reward-based test-time alignment
methods.

</details>


### [92] [MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines](https://arxiv.org/abs/2510.26230)
*Minyi Peng,Darian Gunamardi,Ivan Tjuawinata,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 提出一种基于逐序学习的可逆清除框架，通过在模型末端增加投影-再分配层，实现对最近学习类别的撤销，具备模型无全数据访问需求、模块化、与现有分类管道兼容，在 CIFAR-10/100（CNN）和 Covertype（树模型）数据集上实现接近重新训练的效果，同时显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器 unlearning（MU）方法在现实场景中的可扩展性问题。当前方法通常需要对原始数据和模型进行全量访问，计算成本高且难以部署到实际系统中，因此存在实用性不足的痛点。

Method: 将分类训练视为逐序学习过程，逆向最近的训练序列来实现撤销；通过在模型末端追加一个投影-再分配层来完成撤销操作，使得不需要对原始数据或完整模型进行访问，且具备模块化、模型无关性和对现有分类管道的低侵入性。

Result: 实验在多个数据集上显示，该方法输出接近完全重新训练的模型，但计算成本显著降低，具备较好的可扩展性和系统兼容性。

Conclusion: 该方法提供了一种更实用的机器 unlearning 解决方案，适合在实际分类管道中作为输出过滤器使用，提升可部署性与效率，同时保持较高的输出性能。

Abstract: As a new and promising approach, existing machine unlearning (MU) works
typically emphasize theoretical formulations or optimization objectives to
achieve knowledge removal. However, when deployed in real-world scenarios, such
solutions typically face scalability issues and have to address practical
requirements such as full access to original datasets and model. In contrast to
the existing approaches, we regard classification training as a sequential
process where classes are learned sequentially, which we call \emph{inductive
approach}. Unlearning can then be done by reversing the last training sequence.
This is implemented by appending a projection-redistribution layer in the end
of the model. Such an approach does not require full access to the original
dataset or the model, addressing the challenges of existing methods. This
enables modular and model-agnostic deployment as an output filter into existing
classification pipelines with minimal alterations. We conducted multiple
experiments across multiple datasets including image (CIFAR-10/100 using
CNN-based model) and tabular datasets (Covertype using tree-based model).
Experiment results show consistently similar output to a fully retrained model
with a high computational cost reduction. This demonstrates the applicability,
scalability, and system compatibility of our solution while maintaining the
performance of the output in a more practical setting.

</details>


### [93] [Angular Steering: Behavior Control via Rotation in Activation Space](https://arxiv.org/abs/2510.26243)
*Hieu M. Vu,Tan M. Nguyen*

Main category: cs.LG

TL;DR: Angular Steering: a geometric method to steer LLM behavior by rotating activations in a fixed 2D subspace, enabling fine-grained control (e.g., refusals) while preserving overall language modeling performance; adaptive variant further improves stability; generalizes prior vector addition/orthogonalization methods; robust across model families; code available.


<details>
  <summary>Details</summary>
Motivation: Controlling specific behaviors in large language models is essential for safety and reliability, but existing steering techniques are constrained to a 2D subspace and suffer from sensitivity to parameters and unintended interactions in activation space. A more flexible, stable, and general approach is needed that preserves general capabilities.

Method: Introduce Angular Steering: define a fixed 2D subspace and rotate activations within it to steer toward or away from a target behavior direction. Treat steering as a geometric rotation, enabling continuous, fine-grained control over behaviors such as refusal and compliance. Propose Adaptive Angular Steering, which rotates only activations aligned with the target feature to improve stability. This approach generalizes and encompasses prior addition and orthogonalization techniques.

Result: Empirical evaluation across multiple model families and sizes demonstrates robust behavioral control while maintaining general language modeling performance. Angular Steering shows flexibility, generalization, and robustness compared to prior approaches.

Conclusion: Angular Steering offers a unified geometric rotation framework that simplifies parameter tuning, enhances stability, and broadens the range of effective adjustments. It subsumes existing techniques and demonstrates effective control over behaviors (e.g., refusals, emotion steering) with preserved LM capabilities; code and artifacts are available at the provided repository.

Abstract: Controlling specific behaviors in large language models while preserving
their general capabilities is a central challenge for safe and reliable
artificial intelligence deployment. Current steering methods, such as vector
addition and directional ablation, are constrained within a two-dimensional
subspace defined by the activation and feature direction, making them sensitive
to chosen parameters and potentially affecting unrelated features due to
unintended interactions in activation space. We introduce Angular Steering, a
novel and flexible method for behavior modulation that operates by rotating
activations within a fixed two-dimensional subspace. By formulating steering as
a geometric rotation toward or away from a target behavior direction, Angular
Steering provides continuous, fine-grained control over behaviors such as
refusal and compliance. We demonstrate this method using refusal steering
emotion steering as use cases. Additionally, we propose Adaptive Angular
Steering, a selective variant that rotates only activations aligned with the
target feature, further enhancing stability and coherence. Angular Steering
generalizes existing addition and orthogonalization techniques under a unified
geometric rotation framework, simplifying parameter selection and maintaining
model stability across a broader range of adjustments. Experiments across
multiple model families and sizes show that Angular Steering achieves robust
behavioral control while maintaining general language modeling performance,
underscoring its flexibility, generalization, and robustness compared to prior
approaches. Code and artifacts are available at
https://github.com/lone17/angular-steering/.

</details>


### [94] [Likely Interpolants of Generative Models](https://arxiv.org/abs/2510.26266)
*Frederik Möbius Rygaard,Shen Zhu,Yinzhu Jin,Søren Hauberg,Tom Fletcher*

Main category: cs.LG

TL;DR: 提出一种在不额外训练的情况下的通用插值方案，使得插值曲线近似数据分布中的几何路径（拟合地理曲线），并在不同度量和分布下可控地沿着高概率区域移动。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型缺乏通用的、无额外假设的插值定义，难以在多度量和多数据分布中实现稳定、直观的过渡。

Method: 给出一种目标为符合不同度量和分布的转移路径的插值算法，将其看作约束在数据分布上的测地线；给出无需额外训练的计算算法，并在局部层面将其与合适的黎曼度量下的测地线等价。

Result: 在多种模型和数据集上，所提插值在高密度区域内的遍历优于基线方法；理论上证明局部近似于测地线。

Conclusion: 该方法提供一个普适、训练成本低的插值框架，便于模型检查与可控生成，且强调沿着数据密度更高区域的迁移。

Abstract: Interpolation in generative models allows for controlled generation, model
inspection, and more. Unfortunately, most generative models lack a principal
notion of interpolants without restrictive assumptions on either the model or
data dimension. In this paper, we develop a general interpolation scheme that
targets likely transition paths compatible with different metrics and
probability distributions. We consider interpolants analogous to a geodesic
constrained to a suitable data distribution and derive a novel algorithm for
computing these curves, which requires no additional training. Theoretically,
we show that our method locally can be considered as a geodesic under a
suitable Riemannian metric. We quantitatively show that our interpolation
scheme traverses higher density regions than baselines across a range of models
and datasets.

</details>


### [95] [Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation](https://arxiv.org/abs/2510.26278)
*Kim Yong Tan,Yueming Lyu,Ivor Tsang,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 提出在推理阶段对扩散过程进行多目标权重采样以一次生成即可获得高超体积的多目标样本，避免依赖外部优化循环。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在多目标黑盒优化中往往依赖外部优化环节（如进化算法），将扩散模型视为黑箱的 refinement，忽略了生成过程中的内部分布转变，导致效率低下。

Method: 在扩散生成过程进行加权重采样，根据期望的多目标值聚合来引导样本分布接近多目标Boltzmann分布；给出该分布的对数似然解释，提出Inference-time Multi-target Generation (IMG)算法；在多目标分子生成任务中实现。

Result: 仅需一次生成即可获得显著更高的超体积（hypervolume），相比需要多轮扩散生成的基线优化算法，效率显著提升；算法可视为对扩散过程的优化，并可与现有方法结合以进一步提升性能。

Conclusion: 为多目标优化提供一种通用且可在推理时直接应用的扩散过程优化框架，具有广泛适用性和良好扩展性。

Abstract: Diffusion models have been successful in learning complex data distributions.
This capability has driven their application to high-dimensional
multi-objective black-box optimization problem. Existing approaches often
employ an external optimization loop, such as an evolutionary algorithm, to the
diffusion model. However, these approaches treat the diffusion model as a
black-box refiner, which overlooks the internal distribution transition of the
diffusion generation process, limiting their efficiency. To address these
challenges, we propose the Inference-time Multi-target Generation (IMG)
algorithm, which optimizes the diffusion process at inference-time to generate
samples that simultaneously satisfy multiple objectives. Specifically, our IMG
performs weighted resampling during the diffusion generation process according
to the expected aggregated multi-objective values. This weighted resampling
strategy ensures the diffusion-generated samples are distributed according to
our desired multi-target Boltzmann distribution. We further derive that the
multi-target Boltzmann distribution has an interesting log-likelihood
interpretation, where it is the optimal solution to the distributional
multi-objective optimization problem. We implemented IMG for a multi-objective
molecule generation task. Experiments show that IMG, requiring only a single
generation pass, achieves a significantly higher hypervolume than baseline
optimization algorithms that often require hundreds of diffusion generations.
Notably, our algorithm can be viewed as an optimized diffusion process and can
be integrated into existing methods to further improve their performance.

</details>


### [96] [Empirical Bayesian Multi-Bandit Learning](https://arxiv.org/abs/2510.26284)
*Xia Jiang,Rong J. B. Zhu*

Main category: cs.LG

TL;DR: 提出一个用于多臂带宽学习的层级贝叶斯框架，通过经验贝叶斯估计跨带宽任务的先验协方差矩阵，开发两种高效算法 ebmTS（Empirical Bayesian Multi-Bandit Thompson Sampling）和 ebmUCB（Empirical Bayesian Multi-Bandit Upper Confidence Bound），并给出所提算法的频率学派 regrets 上界。实验结果表明在合成和真实数据集上优于现有方法，特别是在复杂环境中。


<details>
  <summary>Details</summary>
Motivation: 解决多任务/多带宽情境下的异质性与相关性问题，利用共享结构提升跨任务学习效果，同时弥补先验协方差未知情况下的学习难点。

Method: 建立一个层级贝叶斯模型来同时捕捉不同带宽实例间的异质性和相关性，通过经验贝叶斯估计先验协方差矩阵；基于估计的先验，设计 ebmTS 和 ebmUCB 两种算法以实现信息共享与高效探索；给出该方法的频率学派 regret 上界。

Result: 理论上给出频率学派的 regret 上界；在合成数据和真实数据集上验证，所提算法在复杂环境下显著降低累计 regret，相较于现有方法具有更好的探索-开发权衡。

Conclusion: 提出的层级贝叶斯多带宽框架及两种算法有效实现跨带宽信息共享并容纳实例数量的差异性，增强了多带宽问题中的学习能力，具有良好的理论保证与实证表现。

Abstract: Multi-task learning in contextual bandits has attracted significant research
interest due to its potential to enhance decision-making across multiple
related tasks by leveraging shared structures and task-specific heterogeneity.
In this article, we propose a novel hierarchical Bayesian framework for
learning in various bandit instances. This framework captures both the
heterogeneity and the correlations among different bandit instances through a
hierarchical Bayesian model, enabling effective information sharing while
accommodating instance-specific variations. Unlike previous methods that
overlook the learning of the covariance structure across bandits, we introduce
an empirical Bayesian approach to estimate the covariance matrix of the prior
distribution.This enhances both the practicality and flexibility of learning
across multi-bandits. Building on this approach, we develop two efficient
algorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and
ebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which
incorporate the estimated prior into the decision-making process. We provide
the frequentist regret upper bounds for the proposed algorithms, thereby
filling a research gap in the field of multi-bandit problems. Extensive
experiments on both synthetic and real-world datasets demonstrate the superior
performance of our algorithms, particularly in complex environments. Our
methods achieve lower cumulative regret compared to existing techniques,
highlighting their effectiveness in balancing exploration and exploitation
across multi-bandits.

</details>


### [97] [Offline Clustering of Preference Learning with Active-data Augmentation](https://arxiv.org/abs/2510.26301)
*Jingyuan Liu,Fatemeh Ghaffari,Xuchuang Wang,Mohammad Hajiesmaili,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出离线偏好学习的聚类框架 Off-C^2PL 与其主动数据增强变体 A^2-Off-C^2PL，给出理论界限并在合成/真实数据上验证。


<details>
  <summary>Details</summary>
Motivation: 在离线、且多用户、偏好存在异质性的场景下，如何在数据不均衡的情况下识别用户间相似性并有效聚合偏好；并提升离线数据对测试用户的效用。

Method: Off-C^2PL 在纯离线数据上进行用户聚类并利用聚类结构进行偏好估计，推导出 suboptimality bound，揭示样本噪声与偏差的权衡。随后在数据增强设定中提出 A^2-Off-C^2PL，主动选择针对测试用户最信息不足的维度的样本，基于聚类结构进行取样。

Result: 理论上给出子最优界 / 关于信息增益的结论；实验显示主动采样比仅离线样本更具信息性，且在合成和现实数据集上验证了方法的有效性。

Conclusion: 聚类驱动的离线偏好学习和基于信息增益的主动数据采样能显著提升对测试用户的效用，适用于存在数据不均衡和偏好异质性的场景。

Abstract: Preference learning from pairwise feedback is a widely adopted framework in
applications such as reinforcement learning with human feedback and
recommendations. In many practical settings, however, user interactions are
limited or costly, making offline preference learning necessary. Moreover,
real-world preference learning often involves users with different preferences.
For example, annotators from different backgrounds may rank the same responses
differently. This setting presents two central challenges: (1) identifying
similarity across users to effectively aggregate data, especially under
scenarios where offline data is imbalanced across dimensions, and (2) handling
the imbalanced offline data where some preference dimensions are
underrepresented. To address these challenges, we study the Offline Clustering
of Preference Learning problem, where the learner has access to fixed datasets
from multiple users with potentially different preferences and aims to maximize
utility for a test user. To tackle the first challenge, we first propose
Off-C$^2$PL for the pure offline setting, where the learner relies solely on
offline data. Our theoretical analysis provides a suboptimality bound that
explicitly captures the tradeoff between sample noise and bias. To address the
second challenge of inbalanced data, we extend our framework to the setting
with active-data augmentation where the learner is allowed to select a limited
number of additional active-data for the test user based on the cluster
structure learned by Off-C$^2$PL. In this setting, our second algorithm,
A$^2$-Off-C$^2$PL, actively selects samples that target the least-informative
dimensions of the test user's preference. We prove that these actively
collected samples contribute more effectively than offline ones. Finally, we
validate our theoretical results through simulations on synthetic and
real-world datasets.

</details>


### [98] [Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens](https://arxiv.org/abs/2510.26302)
*Ziliang Chen,Tianang Xiao,Jusheng Zhang,Yongsen Zheng,Xipeng Chen*

Main category: cs.LG

TL;DR: 提出 token-aware CRL 框架与 token-level SCM，揭示 CLIP 在组合性上的非辨识性及负样本挖掘的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决现有 CLIP 以句子级文本向量为中心的因果分析，未能解释 token 层结构、prompt 敏感性以及对难负例的失败；提供对跨模态对齐中组合性缺乏辨识性的理论根源。

Method: 在语言 token 的序列因果模型中扩展 block identifiability，证明对比学习目标在句子级和 token 级 SCM 下均可回收模态不变量；证明存在伪最优文本编码器对 SWAP/REPLACE/ADD 等原子概念操作不敏感，导致对正确字幕与难负例区分能力下降。

Result: 给出 token 级别下的 identifiability 结论，解释 CLIP 的组合性脆弱性的理论根源；将语言端非辨识性通过模态间差距传导至视觉端，且迭代的组合运算进一步放大难度，指出负样本挖掘需要改进。

Conclusion: 提出了 token-level 因果框架来解释跨模态对齐中的失败模式，强调需要在负样本挖掘和编码器设计上关注原子概念的操作鲁棒性，以提升对 HARD NEGATIVES 的识别能力。

Abstract: Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal
generalization by aligning images and texts in a shared embedding space, yet it
persistently fails at compositional reasoning over objects, attributes, and
relations often behaving like a bag-of-words matcher. Prior causal accounts
typically model text as a single vector, obscuring token-level structure and
leaving core phenomena-such as prompt sensitivity and failures on hard
negatives unexplained. We address this gap with a token-aware causal
representation learning (CRL) framework grounded in a sequential,
language-token SCM. Our theory extends block identifiability to tokenized text,
proving that CLIP's contrastive objective can recover the modal-invariant
latent variable under both sentence-level and token-level SCMs. Crucially,
token granularity yields the first principled explanation of CLIP's
compositional brittleness: composition nonidentifiability. We show the
existence of pseudo-optimal text encoders that achieve perfect modal-invariant
alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations
over atomic concepts, thereby failing to distinguish correct captions from hard
negatives despite optimizing the same training objective as true-optimal
encoders. The analysis further links language-side nonidentifiability to
visual-side failures via the modality gap and shows how iterated composition
operators compound hardness, motivating improved negative mining strategies.

</details>


### [99] [Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime](https://arxiv.org/abs/2510.26303)
*Beomhan Baek,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: Incremental Adam can exhibit a different implicit bias than full-batch Adam; on certain structured datasets, incremental Adam converges to the l2-max-margin instead of the l_inf-max-margin. A proxy algorithm describes its behavior as beta2 → 1 with a data-dependent dual fixed-point formulation. Signum, in contrast, converges to the l_inf-max-margin for any batch size when beta is close to 1.


<details>
  <summary>Details</summary>
Motivation: Clarify how batching schemes (incremental vs full-batch) influence the implicit bias of Adam and contrast with Signum, advancing theoretical understanding of optimization dynamics with adaptive methods.

Method: 1) Analyze incremental Adam (one-sample-per-step) for logistic regression on linearly separable data. 2) Construct structured datasets where incremental Adam provably reaches the l2-max-margin. 3) Develop a proxy algorithm to capture the limiting behavior of incremental Adam as beta2 → 1 and derive a data-dependent dual fixed-point formulation to characterize convergence direction. 4) Compare with Signum, proving convergence to l_inf-max-margin for any batch size when beta→1.

Result: Incremental Adam can bias toward the l2-max-margin on certain datasets, diverging from the l_inf bias of full-batch Adam; a proxy algorithm and a dual fixed-point formulation describe its convergence direction for general datasets in the beta2 → 1 regime. Signum consistently converges to the l_inf-max-margin across batch sizes given beta close to 1.

Conclusion: The implicit bias of Adam is not universal; it depends on batching scheme and data geometry. Signum’s bias appears batch-size invariant, highlighting fundamental differences between these adaptive methods.

Abstract: Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet
its theoretical understanding remains limited. Prior analyses show that Adam
favors solutions aligned with $\ell_\infty$-geometry, but these results are
restricted to the full-batch regime. In this work, we study the implicit bias
of incremental Adam (using one sample per step) for logistic regression on
linearly separable data, and we show that its bias can deviate from the
full-batch behavior. To illustrate this, we construct a class of structured
datasets where incremental Adam provably converges to the $\ell_2$-max-margin
classifier, in contrast to the $\ell_\infty$-max-margin bias of full-batch
Adam. For general datasets, we develop a proxy algorithm that captures the
limiting behavior of incremental Adam as $\beta_2 \to 1$ and we characterize
its convergence direction via a data-dependent dual fixed-point formulation.
Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges
to the $\ell_\infty$-max-margin classifier for any batch size by taking $\beta$
close enough to 1. Overall, our results highlight that the implicit bias of
Adam crucially depends on both the batching scheme and the dataset, while
Signum remains invariant.

</details>


### [100] [Model Inversion with Layer-Specific Modeling and Alignment for Data-Free Continual Learning](https://arxiv.org/abs/2510.26311)
*Ruilin Tong,Haodong Lu,Yuhang Liu,Dong Gong*

Main category: cs.LG

TL;DR: 提出 Per-layer Model Inversion (PMI) 与特征建模相结合的无数据持续学习方法，通过分层初始化加速全模型反演，并用高斯特征分布与对比学习对齐合成特征，进而从投影特征生成伪图像进行增量学习。适用于大模型如 CLIP，显示在多种CL设定下的强有效性与兼容性。


<details>
  <summary>Details</summary>
Motivation: 在隐私/安全约束下无法存储或重放历史数据的场景中，数据无样本的持续学习成为必要，但仅靠正则化难以维持旧任务性能。模型反演存在两大挑战：一、仅从输出标签生成输入导致合成数据与真实数据漂移；二、反演成本高，尤其在大模型（如CLIP）中需对全模型反向传播。需高效且对齐的解决方案。

Method: 提出 Per-layer Model Inversion (PMI)，为全模型反演提供强初始化，从而显著减少迭代次数。通过对类别特征建模为高斯分布并采用对比学习，确保合成特征与真实特征的一致性，降低特征漂移。结合 PMI 与特征建模，从语义感知的投影特征生成伪图像，用于无数据的增量学习，具有对多种CL设置的良好兼容性。

Result: 在多种持续学习设置中展现出较强的效果与兼容性。

Conclusion: PMI 与特征建模的结合使数据无数据的持续学习更高效且稳定地重放知识，特别是在大规模预训练模型上，能够生成更高质量的伪样本以支持新类的增量学习。

Abstract: Continual learning (CL) aims to incrementally train a model on a sequence of
tasks while retaining performance on prior ones. However, storing and replaying
data is often infeasible due to privacy or security constraints and impractical
for arbitrary pre-trained models. Data-free CL seeks to update models without
access to previous data. Beyond regularization, we employ model inversion to
synthesize data from the trained model, enabling replay without storing
samples. Yet, model inversion in predictive models faces two challenges: (1)
generating inputs solely from compressed output labels causes drift between
synthetic and real data, and replaying such data can erode prior knowledge; (2)
inversion is computationally expensive since each step backpropagates through
the full model. These issues are amplified in large pre-trained models such as
CLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI),
inspired by faster convergence in single-layer optimization. PMI provides
strong initialization for full-model inversion, substantially reducing
iterations. To mitigate feature shift, we model class-wise features via
Gaussian distributions and contrastive model, ensuring alignment between
synthetic and real features. Combining PMI and feature modeling, our approach
enables continual learning of new classes by generating pseudo-images from
semantic-aware projected features, achieving strong effectiveness and
compatibility across multiple CL settings.

</details>


### [101] [On the Impact of Weight Discretization in QUBO-Based SVM Training](https://arxiv.org/abs/2510.26323)
*Sascha Mücke*

Main category: cs.LG

TL;DR: 将SVM训练问题转写为QUBO，利用量子退火求解。即使是低精度（如1比特/参数）的QUBO编码也能获得竞争性甚至更好的准确性；更高的比特深度允许更大正则化，但未必总是提升性能。研究表明，选择合适的支持向量比对其权重精度更重要。硬件规模受限，但结果显示量子退火在SVM训练中具有潜在潜力。


<details>
  <summary>Details</summary>
Motivation: 探究量子退火对SVM训练性能的影响，特别是离散化（比特深度）对预测性能和正则化的影响，以及与经典LIBSVM的对比。

Method: 将SVM对偶问题离散化为QUBO形式，通过不同权重离散化深度（比特数）进行实验，并与经典LIBSVM在多数据集上进行对比评估。

Result: 即使1-bit-per-parameter的QUBO编码也能达到竞争性甚至在某些数据集上优于LIBSVM的准确性；提高比特深度可容纳更大的正则化参数，但并非总能提高分类性能；在某些情形下，关键在于选择哪些支持向量，而非其权重的精确度。受限于当前硬件，QUBO规模有限，但结果显示量子退火对SVM训练具有潜在价值，且随着设备扩展有望展现更大优势。

Conclusion: 量子退火为SVM训练提供了一条可扩展的研究路径，低位深编码已具备实用性且某些情况下优于高位深；未来设备规模扩大时，正确的支持向量选择比权重精准度更关键。

Abstract: Training Support Vector Machines (SVMs) can be formulated as a QUBO problem,
enabling the use of quantum annealing for model optimization. In this work, we
study how the number of qubits - linked to the discretization level of dual
weights - affects predictive performance across datasets. We compare QUBO-based
SVM training to the classical LIBSVM solver and find that even low-precision
QUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes
superior, accuracy. While increased bit-depth enables larger regularization
parameters, it does not always improve classification. Our findings suggest
that selecting the right support vectors may matter more than their precise
weighting. Although current hardware limits the size of solvable QUBOs, our
results highlight the potential of quantum annealing for efficient SVM training
as quantum devices scale.

</details>


### [102] [Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics](https://arxiv.org/abs/2510.26324)
*Zhiyang Xun,Shivam Gupta,Eric Price*

Main category: cs.LG

TL;DR: 提出了一种将扩散模型与退火 Langevin 动力学相结合的条件采样方法，用于对数凹先验的逆问题，在仅需 L^4 范围的分数误差下，给出多项式时间的条件采样保证。


<details>
  <summary>Details</summary>
Motivation: 解决给定观测 y=Ax+ξ 的情况下，从近似先验 p(x) 的后验 p(x|y) 中采样的问题，特别是逆问题中的去噪、修复等任务，需要可证的、且在实际中可用的条件采样方法。

Method: 在对数凹先验下，将扩散模型用于分数估计，并引入对退火 Langevin 动力学的条件采样框架，分析在仅需对分数误差拥有 L^4 上界的情况下，仍可多项式时间内实现后验条件采样。

Result: 理论结果：在对数凹分布下，结合扩散模型与退火 Langevin 的条件采样在仅需要分数误差的 L^4 上界时，也能获得多项式时间的采样保证。对比传统仅凭 L^2（或需要 MGF 界）在无条件设置的要求，这一约束放宽且适用于条件采样。

Conclusion: 将扩散模型引入条件后验采样并通过退火 Langevin 加以实现，可在更宽的误差假设下实现可证明的高效采样，为诸如去模糊、修复、MRI 重建等应用提供理论基础。

Abstract: Given a noisy linear measurement $y = Ax + \xi$ of a distribution $p(x)$, and
a good approximation to the prior $p(x)$, when can we sample from the posterior
$p(x \mid y)$? Posterior sampling provides an accurate and fair framework for
tasks such as inpainting, deblurring, and MRI reconstruction, and several
heuristics attempt to approximate it. Unfortunately, approximate posterior
sampling is computationally intractable in general.
  To sidestep this hardness, we focus on (local or global) log-concave
distributions $p(x)$. In this regime, Langevin dynamics yields posterior
samples when the exact scores of $p(x)$ are available, but it is brittle to
score--estimation error, requiring an MGF bound (sub-exponential error). By
contrast, in the unconditional setting, diffusion models succeed with only an
$L^2$ bound on the score error. We prove that combining diffusion models with
an annealed variant of Langevin dynamics achieves conditional sampling in
polynomial time using merely an $L^4$ bound on the score error.

</details>


### [103] [Linear Causal Discovery with Interventional Constraints](https://arxiv.org/abs/2510.26342)
*Zhigao Guo,Feng Dong*

Main category: cs.LG

TL;DR: 提出“干预约束”作为新型因果发现约束，将高层因果知识以对变量对的总效应不等式约束编码，替代直接干预数据以提升推断可靠性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法能强制结构约束（如从某变量到另一变量的因果路径），但未能确保总因果效应与已知的因果方向一致，易产生错把正向或负向因果关系学习错误。通过引入干预约束，将领域知识以总效应约束形式融入学习，提升一致性与可解释性。

Method: 为线性因果模型定义总因果效应的度量，将其作为不等式约束融入学习，问题转化为带约束的优化；提出两阶段优化算法以求解该约束优化问题。

Result: 在真实数据集上验证，整合干预约束可提升模型精度、与既有因果发现的一致性和可解释性，并有助于发现新的因果关系。

Conclusion: 干预约束为因果发现提供了一种高层知识融入的有效途径，能提升模型的可靠性并拓展潜在因果关系的发现能力。

Abstract: Incorporating causal knowledge and mechanisms is essential for refining
causal models and improving downstream tasks such as designing new treatments.
In this paper, we introduce a novel concept in causal discovery, termed
interventional constraints, which differs fundamentally from interventional
data. While interventional data require direct perturbations of variables,
interventional constraints encode high-level causal knowledge in the form of
inequality constraints on causal effects. For instance, in the Sachs dataset
(Sachs et al.\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3
exerts a positive causal effect on Akt. Existing causal discovery methods allow
enforcing structural constraints (for example, requiring a causal path from
PIP3 to Akt), but they may still produce incorrect causal conclusions such as
learning that "PIP3 inhibits Akt". Interventional constraints bridge this gap
by explicitly constraining the total causal effect between variable pairs,
ensuring learned models respect known causal influences. To formalize
interventional constraints, we propose a metric to quantify total causal
effects for linear causal models and formulate the problem as a constrained
optimization task, solved using a two-stage constrained optimization method. We
evaluate our approach on real-world datasets and demonstrate that integrating
interventional constraints not only improves model accuracy and ensures
consistency with established findings, making models more explainable, but also
facilitates the discovery of new causal relationships that would otherwise be
costly to identify.

</details>


### [104] [Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle](https://arxiv.org/abs/2510.26347)
*Sebastian Zieglmeier,Niklas Erdmann,Narada D. Warakagoda*

Main category: cs.LG

TL;DR: 通过对经典RL的多方面修改，提出一种在稀疏、随机和非平稳环境中有效的蒙特卡洛方法，优于Q-learning和穷举搜索，适用于海洋探测任务。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏奖励和环境非平稳性导致的学习效率下降，提升在海洋自动水下载具等复杂场景中的鲁棒性。

Method: 系统性地探索多种修改: 包含分层算法改进、多目标学习，以及引入位置记忆作为外部输出滤波器以避免状态重访；并基于蒙特卡洛学习进行实现，比较与Q-learning及两种穷举搜索模式的性能。

Result: 经实验验证，改造后的蒙特卡洛方法在稀疏奖励、随机和非平稳环境下显著优于传统Q-learning和两种穷举搜索模式。

Conclusion: 表明强化学习方法可以被有效地调整以适应复杂的随机、非平稳和奖励稀疏场景，且具有在实际应用（如AUV水下污染云探测）中的潜在应用价值。

Abstract: Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.

</details>


### [105] [Towards Explainable and Reliable AI in Finance](https://arxiv.org/abs/2510.26353)
*Albi Isufaj,Pablo Mollá,Helmut Prendinger*

Main category: cs.LG

TL;DR: A framework for explainable and reliable AI in finance that uses Time-LLM prompts to avoid wrong directional forecasts, a reliability estimator to filter predictions, and symbolic rule-based reasoning for transparent justification; experiments on equity and crypto data show reduced false positives and support for selective execution.


<details>
  <summary>Details</summary>
Motivation: Financial forecasting with large neural networks suffers from opacity, hindering trust and regulatory compliance; there is a need for auditable, explainable AI systems in finance.

Method: (1) Time-LLM: a time-series foundation model that uses prompts to steer forecasts in the correct direction. (2) Combine foundation models for time series forecasting with a reliability estimator to filter unreliable predictions. (3) Symbolic reasoning encoding domain rules to provide transparent justification. The approach emphasizes executing only forecasts that are reliable and explainable.

Result: Experiments on equity and cryptocurrency data show the architecture reduces false positives and enables selective execution, indicating improvements in reliability and trustworthiness.

Conclusion: Integrating predictive performance with reliability estimation and rule-based reasoning yields transparent and auditable financial AI systems.

Abstract: Financial forecasting increasingly uses large neural network models, but
their opacity raises challenges for trust and regulatory compliance. We present
several approaches to explainable and reliable AI in finance. \emph{First}, we
describe how Time-LLM, a time series foundation model, uses a prompt to avoid a
wrong directional forecast. \emph{Second}, we show that combining foundation
models for time series forecasting with a reliability estimator can filter our
unreliable predictions. \emph{Third}, we argue for symbolic reasoning encoding
domain rules for transparent justification. These approaches shift emphasize
executing only forecasts that are both reliable and explainable. Experiments on
equity and cryptocurrency data show that the architecture reduces false
positives and supports selective execution. By integrating predictive
performance with reliability estimation and rule-based reasoning, our framework
advances transparent and auditable financial AI systems.

</details>


### [106] [CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse](https://arxiv.org/abs/2510.26369)
*Kazuma Kano,Yuki Mori,Shin Katayama,Kenta Urano,Takuro Yonezawa,Nobuo Kawaguchi*

Main category: cs.LG

TL;DR: CorVS: 基于视觉跟踪轨迹与传感器测量的对应关系预测，实现仓库工人身份识别的深度学习方法，通过预测的对应概率与可靠性进行时序匹配，在真实仓库数据集上验证应用可行性。


<details>
  <summary>Details</summary>
Motivation: 在工业场景中仅凭外观识别个体往往不可行，因此需要利用除外观之外的信号（如轨迹与传感器数据）来实现稳健的身份识别；同时，真实场景对对应关系的鲁棒性提出挑战。

Method: 提出一个端到端的深度学习模型，为每一对轨迹-传感器测量组合输出对应概率和可靠性；随后基于预测的概率和可靠性，设计一个时序匹配算法在轨迹与传感器测量之间进行对齐与身份匹配。并在实际仓库操作数据集上进行验证。

Result: 在真实世界应用场景中展示出方法的有效性，证明了将视觉轨迹与传感器测量相结合的可行性与潜在优势。

Conclusion: CorVS通过将视觉跟踪与传感器数据进行对齐匹配，提供一种对外观不敏感且鲁棒的工人身份识别方案，具备在物流仓库等实际场景中的应用潜力。

Abstract: Worker location data is key to higher productivity in industrial sites.
Cameras are a promising tool for localization in logistics warehouses since
they also offer valuable environmental contexts such as package status.
However, identifying individuals with only visual data is often impractical.
Accordingly, several prior studies identified people in videos by comparing
their trajectories and wearable sensor measurements. While this approach has
advantages such as independence from appearance, the existing methods may break
down under real-world conditions. To overcome this challenge, we propose CorVS,
a novel data-driven person identification method based on correspondence
between visual tracking trajectories and sensor measurements. Firstly, our deep
learning model predicts correspondence probabilities and reliabilities for
every pair of a trajectory and sensor measurements. Secondly, our algorithm
matches the trajectories and sensor measurements over time using the predicted
probabilities and reliabilities. We developed a dataset with actual warehouse
operations and demonstrated the method's effectiveness for real-world
applications.

</details>


### [107] [Efficient Generative AI Boosts Probabilistic Forecasting of Sudden Stratospheric Warmings](https://arxiv.org/abs/2510.26376)
*Ningning Tao,Fei Xie,Baoxiang Pan,Hongyu Wang,Han Huang,Zhongpu Qiu,Ke Gui,Jiali Luo,Xiaosong Chen*

Main category: cs.LG

TL;DR: FM-Cast：基于流匹配的生成式AI，快速且具有概率性的平流层SSW演化预报，与主流NWP相比具竞争力且计算成本极低


<details>
  <summary>Details</summary>
Motivation: SSW是次季节尺度可预报性的关键来源，但NWP在物理表征、初始化和大规模集合预报的计算成本方面存在挑战；数据驱动方法在三维SSW动力学和概率预报中的应用尚不充分。本研究通过引入生成式AI来提升SSW的概率性预报能力并探索物理驱动机制。

Method: 提出一个Flow Matching-based生成模型FM-Cast，用于高效地预测平流层环流的时空演化，并给出概率性预报。对1998-2024年18个SSW事件进行评估，涵盖SSW发生的时机、强度和形态等要素，实现对10个事件的高达20天前的预测，且集合准确率超过50%；在消费级GPU上可快速生成50-member、30-day预测（约2分钟）。另一方面，作为科学工具，通过理想化实验探究SSW可预报性与其物理驱动的关系。

Result: FM-Cast在18个事件中对10个事件实现了≤20天前的 onset/强度/形态预测，集合准确率>50%，性能达到或优于领先的NWP系统，同时计算成本极低（50成员、30天预报仅需2分钟）。利用理想化实验揭示SSW的可预报性与其驱动源之间的联系，区分来自对流层强迫与内部平流层动力学的事件。

Conclusion: 建立了一种高效的概率性SSW预报范式，展示生成式AI在高维大气动力学理解与预测中的潜力，推动对大气-气候动力过程的物理理解。

Abstract: Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal
predictability and major drivers of extreme winter weather. Yet, their accurate
and efficient forecast remains a persistent challenge for numerical weather
prediction (NWP) systems due to limitations in physical representation,
initialization, and the immense computational demands of ensemble forecasts.
While data-driven forecasting is rapidly evolving, its application to the
complex, three-dimensional dynamics of SSWs, particularly for probabilistic
forecast, remains underexplored. Here, we bridge this gap by developing a Flow
Matching-based generative AI model (FM-Cast) for efficient and skillful
probabilistic forecasting of the spatiotemporal evolution of stratospheric
circulation. Evaluated across 18 major SSW events (1998-2024), FM-Cast
skillfully forecasts the onset, intensity, and morphology of 10 events up to 20
days in advance, achieving ensemble accuracies above 50%. Its performance is
comparable to or exceeds leading NWP systems while requiring only two minutes
for a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging
FM-Cast as a scientific tool, we demonstrate through idealized experiments that
SSW predictability is fundamentally linked to its underlying physical drivers,
distinguishing between events forced from the troposphere and those driven by
internal stratospheric dynamics. Our work thus establishes a computationally
efficient paradigm for probabilistic forecasting stratospheric anomalies and
showcases generative AI's potential to deepen the physical understanding of
atmosphere-climate dynamics.

</details>


### [108] [Multi-Task Learning Based on Support Vector Machines and Twin Support Vector Machines: A Comprehensive Survey](https://arxiv.org/abs/2510.26392)
*Fatemeh Bazikar,Hossein Moosaei,Atefeh Hemmati,Panos M. Pardalos*

Main category: cs.LG

TL;DR: 对基于SVM和TWSVM的多任务学习（MTL）进行全面综述，聚焦共享表示、任务正则化和结构耦合策略，评估理论性质、优化策略和经验表现，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺或高维情境下，MTL通过跨任务信息共享提升泛化、效率与鲁棒性；尽管深度学习主导MTL研究，SVM和TWSVM因可解释性、理论性和对小数据的有效性仍具重要地位。

Method: 对基于SVM/TWSVM的MTL方法进行系统综述，按共享表示、任务正则化、结构耦合等策略进行分类，分析新兴的TWSVM扩展在多任务中的应用，比较理论性质、优化方法和经验性能，并讨论在计算机视觉、自然语言处理和生物信息学等领域的应用。

Result: 提供当前方法的综合资源，提出框架性比较；总结不同方法的优缺点、适用场景与性能趋势；明确研究空白与挑战。

Conclusion: SVM-与TWSVM基的MTL框架具有可扩展性、可解释性和稳健性潜力，但亟需在可扩展性、解释性和可靠性方面进一步研究，尤其是对更多TWSVM在多任务中的扩展、以及大规模应用。

Abstract: Multi-task learning (MTL) enables simultaneous training across related tasks,
leveraging shared information to improve generalization, efficiency, and
robustness, especially in data-scarce or high-dimensional scenarios. While deep
learning dominates recent MTL research, Support Vector Machines (SVMs) and Twin
SVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor,
and effectiveness with small datasets.
  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting
shared representations, task regularization, and structural coupling
strategies. Special attention is given to emerging TWSVM extensions for
multi-task settings, which show promise but remain underexplored. We compare
these models in terms of theoretical properties, optimization strategies, and
empirical performance, and discuss applications in fields such as computer
vision, natural language processing, and bioinformatics.
  Finally, we identify research gaps and outline future directions for building
scalable, interpretable, and reliable margin-based MTL frameworks. This work
provides a comprehensive resource for researchers and practitioners interested
in SVM- and TWSVM-based multi-task learning.

</details>


### [109] [Co-Evolving Latent Action World Models](https://arxiv.org/abs/2510.26433)
*Yucen Wang,Fengming Zhang,De-Chuan Zhan,Li Zhao,Kaixin Wang,Jiang Bian*

Main category: cs.LG

TL;DR: 提出 CoLA-World，将预训练世界模型与可控视频生成的潜在动作模型进行端到端联合训练，通过一个暖场阶段对齐表征，解决联合学习中的表示崩溃问题，并实现比传统两阶段方法更优的性能。


<details>
  <summary>Details</summary>
Motivation: 当前习惯将潜在动作模型和世界模型分阶段独立训练，存在冗余训练和难以协同进化的问题。希望通过将前向动态模型替换为强大世界模型并进行联合训练来实现更紧密的耦合。

Method: 引入 CoLA-World，设计一个关键的暖场阶段来有效对齐从零开始训练的潜在动作模型（LAM）与预训练世界模型的表征，使两者可共同学习。世界模型作为知识型导师提供梯度，LAM提供更精确、可控的接口以控制世界模型。

Result: 实验表明 CoLA-World 在视频仿真质量和下游视觉规划任务上与/超过了现有两阶段方法，证明了端到端联合学习的有效性与鲁棒性。

Conclusion: 端到端联合学习的协同范式可在可控世界建模领域实现更高效、更强泛化的潜力，未来工作可进一步探索不同类型的世界模型与控制接口的协同进化。

Abstract: Adapting pre-trained video generation models into controllable world models
via latent actions is a promising step towards creating generalist world
models. The dominant paradigm adopts a two-stage approach that trains latent
action model (LAM) and the world model separately, resulting in redundant
training and limiting their potential for co-adaptation. A conceptually simple
and appealing idea is to directly replace the forward dynamic model in LAM with
a powerful world model and training them jointly, but it is non-trivial and
prone to representational collapse. In this work, we propose CoLA-World, which
for the first time successfully realizes this synergistic paradigm, resolving
the core challenge in joint learning through a critical warm-up phase that
effectively aligns the representations of the from-scratch LAM with the
pre-trained world model. This unlocks a co-evolution cycle: the world model
acts as a knowledgeable tutor, providing gradients to shape a high-quality LAM,
while the LAM offers a more precise and adaptable control interface to the
world model. Empirically, CoLA-World matches or outperforms prior two-stage
methods in both video simulation quality and downstream visual planning,
establishing a robust and efficient new paradigm for the field.

</details>


### [110] [Personalized Treatment Outcome Prediction from Scarce Data via Dual-Channel Knowledge Distillation and Adaptive Fusion](https://arxiv.org/abs/2510.26444)
*Wenjie Chen,Li Zhuang,Ziying Luo,Yu Liu,Jiahao Wu,Shengcai Liu*

Main category: cs.LG

TL;DR: CFKD-AFN is a cross-fidelity distillation framework that uses abundant low-fidelity simulation data to improve predictions on scarce high-fidelity trial data, featuring dual-channel distillation and an attention-guided fusion module; it achieves large prediction accuracy gains in COPD treatment outcome prediction and includes an interpretable variant for clinical decision support.


<details>
  <summary>Details</summary>
Motivation: Small-sample, rare-patient groups in precision medicine suffer from costly high-fidelity trial data. Exploiting abundant low-fidelity simulation data can enhance predictive performance when fused appropriately.

Method: CFKD-AFN introduces a dual-channel knowledge distillation module to extract complementary signals from low-fidelity models and an attention-guided fusion module to dynamically integrate multi-source information. An interpretable variant is also developed to reveal latent medical semantics.

Result: On COPD treatment outcome prediction, CFKD-AFN significantly outperforms state-of-the-art methods, with prediction-accuracy improvements ranging from about 6.7% to 74.6% and robustness to different high-fidelity data sizes.

Conclusion: CFKD-AFN effectively leverages cross-fidelity knowledge transfer and adaptive fusion to boost predictions for small-sample/high-stakes medical problems, with an interpretable variant to aid clinical decision-making.

Abstract: Personalized treatment outcome prediction based on trial data for
small-sample and rare patient groups is critical in precision medicine.
However, the costly trial data limit the prediction performance. To address
this issue, we propose a cross-fidelity knowledge distillation and adaptive
fusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation
data to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN
incorporates a dual-channel knowledge distillation module to extract
complementary knowledge from the low-fidelity model, along with an
attention-guided fusion module to dynamically integrate multi-source
information. Experiments on treatment outcome prediction for the chronic
obstructive pulmonary disease demonstrates significant improvements of CFKD-AFN
over state-of-the-art methods in prediction accuracy, ranging from 6.67\% to
74.55\%, and strong robustness to varying high-fidelity dataset sizes.
Furthermore, we extend CFKD-AFN to an interpretable variant, enabling the
exploration of latent medical semantics to support clinical decision-making.

</details>


### [111] [Robust Graph Condensation via Classification Complexity Mitigation](https://arxiv.org/abs/2510.26451)
*Jiayi Luo,Qingyun Sun,Beining Yang,Haonan Yuan,Xingcheng Fu,Yanbiao Ma,Jianxin Li,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了一种针对图压缩（GC）鲁棒性不足的问题的框架 MRGC，通过把压缩后的图嵌入平滑、低维的流形来约束，从而在普适对抗攻击下保持分类复杂度的降低和鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: GC 本质上是一个降维过程，易受对抗扰动影响；现有鲁棒图学习方法对 GC 的提升有限；需要在保持 GC 降维/分类复杂度优势的同时提升鲁棒性。

Method: 提出 MRGC，基于图数据流形几何视角，设计三种图数据流形学习模块，使压缩图落在一个平滑、低维的流形上，并最小化类别歧义，兼顾分类复杂度的减小和鲁棒性。

Result: 在多种攻击场景下，MRGC 展现出较强鲁棒性，实验充分验证其效果。

Conclusion: 将流形约束融入图压缩框架可在不牺牲 GC 的分类复杂度优势的情况下显著提升鲁棒性，为鲁棒图压缩提供可行方向。

Abstract: Graph condensation (GC) has gained significant attention for its ability to
synthesize smaller yet informative graphs. However, existing studies often
overlook the robustness of GC in scenarios where the original graph is
corrupted. In such cases, we observe that the performance of GC deteriorates
significantly, while existing robust graph learning technologies offer only
limited effectiveness. Through both empirical investigation and theoretical
analysis, we reveal that GC is inherently an intrinsic-dimension-reducing
process, synthesizing a condensed graph with lower classification complexity.
Although this property is critical for effective GC performance, it remains
highly vulnerable to adversarial perturbations. To tackle this vulnerability
and improve GC robustness, we adopt the geometry perspective of graph data
manifold and propose a novel Manifold-constrained Robust Graph Condensation
framework named MRGC. Specifically, we introduce three graph data manifold
learning modules that guide the condensed graph to lie within a smooth,
low-dimensional manifold with minimal class ambiguity, thereby preserving the
classification complexity reduction capability of GC and ensuring robust
performance under universal adversarial attacks. Extensive experiments
demonstrate the robustness of \ModelName\ across diverse attack scenarios.

</details>


### [112] [ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems](https://arxiv.org/abs/2510.26475)
*Qiaoling Chen,Zijun Liu,Peng Sun,Shenggui Li,Guoteng Wang,Ziming Liu,Yonggang Wen,Siyuan Feng,Tianwei Zhang*

Main category: cs.LG

TL;DR: 通过 ReSpec 将 speculative decoding 融入 RL 训练，以应对大批量下速度提升下降、drafter 老化和策略退化等挑战，实现最高4.5x加速且保持奖励收敛。


<details>
  <summary>Details</summary>
Motivation: RL 微调大模型的生成阶段成本高，是瓶颈；SD 虽能加速，但在 RL 训练中的行为未被充分研究；存在三大缺口：大批量下加速减弱、 continual actor 更新导致 drafter 陈旧、drafter 诱发策略退化。

Method: 提出 ReSpec：三个互补机制—动态调整 SD 配置、通过知识蒸馏演化 drafter、以及基于 rollout 奖励对更新进行加权。

Result: 在 Qwen 系列 3B-14B 模型上，达到最高 4.5× 的速度提升，同时保持奖励收敛和训练稳定性。

Conclusion: ReSpec 为 RL 目标下的 LLM 快速适配提供实用解决方案，通过协调 SD、知识蒸馏和奖励导向的更新，缓解关键瓶颈。

Abstract: Adapting large language models (LLMs) via reinforcement learning (RL) is
often bottlenecked by the generation stage, which can consume over 75\% of the
training time. Speculative decoding (SD) accelerates autoregressive generation
in serving systems, but its behavior under RL training remains largely
unexplored. We identify three critical gaps that hinder the naive integration
of SD into RL systems: diminishing speedups at large batch sizes, drafter
staleness under continual actor updates, and drafter-induced policy
degradation.
  To address these gaps, we present ReSpec, a system that adapts SD to RL
through three complementary mechanisms: dynamically tuning SD configurations,
evolving the drafter via knowledge distillation, and weighting updates by
rollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup
while preserving reward convergence and training stability, providing a
practical solution for efficient RL-based LLM adaptation.

</details>


### [113] [Defeating the Training-Inference Mismatch via FP16](https://arxiv.org/abs/2510.26788)
*Penghui Qi,Zichen Liu,Xiangxin Zhou,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 用 FP16 替代 BF16 可以在 RL 微调中显著提升稳定性和收敛速度，原因在于浮点精度引发的训练-推理策略不一致。


<details>
  <summary>Details</summary>
Motivation: RL 微调中训练和推理阶段的数值不一致会导致不稳定性；BF16 虽具大动态范围，但引入显著舍入误差，成为根本原因。

Method: 将训练中的数值精度从 BF16 切换回 FP16，且不修改模型结构或学习算法，仅需极少代码变动即可实现一致性。

Result: 采用 FP16 后，优化过程更稳定、收敛更快，在多任务、多算法和多框架下表现更强。

Conclusion: 应重新审视 RL 微调中的精度取舍，FP16/其他精度的选择对稳定性和性能有重要影响，BF16 在此场景中可能并非最佳选择。

Abstract: Reinforcement learning (RL) fine-tuning of large language models (LLMs) often
suffers from instability due to the numerical mismatch between the training and
inference policies. While prior work has attempted to mitigate this issue
through algorithmic corrections or engineering alignments, we show that its
root cause lies in the floating point precision itself. The widely adopted
BF16, despite its large dynamic range, introduces large rounding errors that
breaks the consistency between training and inference. In this work, we
demonstrate that simply reverting to \textbf{FP16} effectively eliminates this
mismatch. The change is simple, fully supported by modern frameworks with only
a few lines of code change, and requires no modification to the model
architecture or learning algorithm. Our results suggest that using FP16
uniformly yields more stable optimization, faster convergence, and stronger
performance across diverse tasks, algorithms and frameworks. We hope these
findings motivate a broader reconsideration of precision trade-offs in RL
fine-tuning.

</details>


### [114] [Data-Efficient RLVR via Off-Policy Influence Guidance](https://arxiv.org/abs/2510.26491)
*Erle Zhu,Dazhi Jiang,Yuan Wang,Xujun Li,Jiale Cheng,Yuxian Gu,Yilin Niu,Aohan Zeng,Jie Tang,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: 提出 CROPI：基于影响函数的离线数据影响估计，结合逐步课程学习，进行自适应数据筛选以提升 RLVR 的训练效率。通过离线轨迹近似在线影响，使用稀疏随机投影降维，降低高维梯度的计算与存储成本。在 7B 参数范围内模型上表现良好，1.5B 模型达到 2.66x 的步级加速，仅使用每阶段 10% 的数据。


<details>
  <summary>Details</summary>
Motivation: 当前 RLVR 的数据选择多为启发式且缺乏理论保障，对大语言模型的高成本在线评估难以实现可靠性。需要一个在理论上可解释且高效的数据筛选方案，以提升学习效率和泛化能力。

Method: 以影响函数估计数据点对学习目标的贡献；为避免在线策略回滚的高昂成本，提出离线轨迹的离线影响估计方法近似数据影响；使用稀疏随机投影对高维梯度进行降维，提升存储与计算效率；构造多阶段 RL 框架 CROPI，循环选取对当前策略最具影响力的数据。

Result: 在多达 7B 参数的模型上进行实验，显示 CROPI 能显著加速训练；在 1.5B 模型上实现步级加速 2.66x，同时每阶段仅使用全数据的 10%。

Conclusion: 基于影响的离线数据选择对提升 RLVR 的训练效率具有显著潜力，能够在减少数据与计算成本的同时维持甚至提升学习进展，具有较好的泛化潜力。

Abstract: Data selection is a critical aspect of Reinforcement Learning with Verifiable
Rewards (RLVR) for enhancing the reasoning capabilities of large language
models (LLMs). Current data selection methods are largely heuristic-based,
lacking theoretical guarantees and generalizability. This work proposes a
theoretically-grounded approach using influence functions to estimate the
contribution of each data point to the learning objective. To overcome the
prohibitive computational cost of policy rollouts required for online influence
estimation, we introduce an off-policy influence estimation method that
efficiently approximates data influence using pre-collected offline
trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we
employ sparse random projection to reduce dimensionality and improve storage
and computation efficiency. Leveraging these techniques, we develop
\textbf{C}urriculum \textbf{R}L with \textbf{O}ff-\textbf{P}olicy
\text{I}nfluence guidance (\textbf{CROPI}), a multi-stage RL framework that
iteratively selects the most influential data for the current policy.
Experiments on models up to 7B parameters demonstrate that CROPI significantly
accelerates training. On a 1.5B model, it achieves a 2.66x step-level
acceleration while using only 10\% of the data per stage compared to
full-dataset training. Our results highlight the substantial potential of
influence-based data selection for efficient RLVR.

</details>


### [115] [Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters](https://arxiv.org/abs/2510.26501)
*Mustafa Fuad Rifet Ibrahim,Maurice Meijer,Alexander Schlaefer,Peer Stelldinger*

Main category: cs.LG

TL;DR: 在资源受限的可穿戴ECG监测中，提出以无监督异常检测（UAD）作为上游过滤器来提升鲁棒性，比较六种UAD方法并通过NAS在≤512k参数下优化，Deep SVDD在检测精度与计算效率之间取得最佳权衡；在真实部署仿真中，与诊断分类器联用可将准确率提升至比基线高出约21个百分点。


<details>
  <summary>Details</summary>
Motivation: 可穿戴ECG监测中容易遇到分布外数据（如未知病理、噪声信号），在资源受限的设备上需提升鲁棒性以保障患者安全，单独的OOD检测方法往往忽略计算资源约束或无法同时处理噪声与未知类别。

Method: 在PTB-XL与BUT QDB数据集上，对六种UAD方法（包括Deep SVDD、基于重构的模型、Masked Anomaly Detection、正则化流、扩散模型等）进行神经架构搜索（NAS）优化，参数上限为512k；评估其对OOD CVD类别和噪声信号的检测能力，并在实际部署仿真中将优化后的Deep SVDD过滤器与诊断分类器结合，比较鲁棒性与准确性。

Result: Deep SVDD在检测与资源消耗之间始终呈现最佳折衷；将优化后的Deep SVDD作为上游过滤器集成到诊断分类器后，准确率相较分类器单独使用的基线提升达至约21个百分点。

Conclusion: 优化的无监督异常检测上游过滤策略能够提升可穿戴ECG自动分析的鲁棒性与安全性，使持续监测在资源受限环境中更可靠。

Abstract: Continuous electrocardiogram (ECG) monitoring via wearables offers
significant potential for early cardiovascular disease (CVD) detection.
However, deploying deep learning models for automated analysis in
resource-constrained environments faces reliability challenges due to
inevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen
pathologies or noisecorrupted signals, often cause erroneous, high-confidence
predictions by standard classifiers, compromising patient safety. Existing OOD
detection methods either neglect computational constraints or address noise and
unseen classes separately. This paper explores Unsupervised Anomaly Detection
(UAD) as an independent, upstream filtering mechanism to improve robustness. We
benchmark six UAD approaches, including Deep SVDD, reconstruction-based models,
Masked Anomaly Detection, normalizing flows, and diffusion models, optimized
via Neural Architecture Search (NAS) under strict resource constraints (at most
512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection
of OOD CVD classes and signals unsuitable for analysis due to noise. Results
show Deep SVDD consistently achieves the best trade-off between detection and
efficiency. In a realistic deployment simulation, integrating the optimized
Deep SVDD filter with a diagnostic classifier improved accuracy by up to 21
percentage points over a classifier-only baseline. This study demonstrates that
optimized UAD filters can safeguard automated ECG analysis, enabling safer,
more reliable continuous cardiovascular monitoring on wearables.

</details>


### [116] [LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection](https://arxiv.org/abs/2510.26510)
*Youssef Attia El Hili,Albert Thomas,Malik Tiomoko,Abdelhakim Benechehab,Corentin Léger,Corinne Ancourt,Balázs Kégl*

Main category: cs.LG

TL;DR: LLMs can serve as in-context meta-learners for model selection and hyperparameter tuning by turning datasets into metadata and prompting zero-shot or with past-task examples, achieving competitive results without search.


<details>
  <summary>Details</summary>
Motivation: Model and hyperparameter selection is hard, expensive, and requires expert intuition; the work explores whether LLMs can act as lightweight assistants to reduce the need for extensive search.

Method: Convert each dataset into interpretable metadata and prompt an LLM to recommend both model families and hyperparameters. Compare two prompting strategies: (1) zero-shot using pretrained knowledge, and (2) meta-informed prompting with examples of models and their performance on past tasks. Evaluate on synthetic and real-world benchmarks.

Result: LLMs can exploit dataset metadata to recommend competitive models and hyperparameters without search. Meta-informed prompting yields improvements, demonstrating in-context meta-learning capabilities.

Conclusion: LLMs show potential as lightweight, general-purpose assistants for model selection and hyperparameter optimization.

Abstract: Model and hyperparameter selection are critical but challenging in machine
learning, typically requiring expert intuition or expensive automated search.
We investigate whether large language models (LLMs) can act as in-context
meta-learners for this task. By converting each dataset into interpretable
metadata, we prompt an LLM to recommend both model families and
hyperparameters. We study two prompting strategies: (1) a zero-shot mode
relying solely on pretrained knowledge, and (2) a meta-informed mode augmented
with examples of models and their performance on past tasks. Across synthetic
and real-world benchmarks, we show that LLMs can exploit dataset metadata to
recommend competitive models and hyperparameters without search, and that
improvements from meta-informed prompting demonstrate their capacity for
in-context meta-learning. These results highlight a promising new role for LLMs
as lightweight, general-purpose assistants for model selection and
hyperparameter optimization.

</details>


### [117] [Think Outside the Policy: In-Context Steered Policy Optimization](https://arxiv.org/abs/2510.26519)
*Hsiu-Yuan Huang,Chenming Tang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: ICPO通过在现有数据集上的在-context学习能力，提出混合策略GRPO、隐式专家强制、专家区域拒采样和退火专家奖金奖励 shaping 的框架，扩大探索并提升鲁棒性，在数学推理任务上显著提升RLVR性能。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法（如GRPO）因仅依赖策略的策略分布进行在-policy-rollout，导致探索受限，且利用强专家模型的轨迹虽能扩展覆盖面，但成本高且难以获得，因此需要一种无需额外外部模型轨迹、能利用已有数据的统一框架。

Method: ICPO框架，借助LRMs的上下文学习能力提供专家指导；提出混合策略GRPO结合隐式专家强制；引入专家区域拒采样以过滤不可靠的离策略轨迹；引入退火式专家奖金奖励 shaping 以在早期提供专家引导、后期实现自主改进。

Result: 实验结果在数学推理基准上显示ICPO能提升RLVR性能和训练稳定性，表明该方法是可扩展且有效的LRM RLVR范式。

Conclusion: ICPO提供一个可扩展、有效的RLVR框架，减少对先进外部专家轨迹的依赖，充分利用LRMs的上下文学习能力，提升推理能力与训练稳定性。

Abstract: Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such
as Group Relative Policy Optimization (GRPO), have achieved remarkable progress
in improving the reasoning capabilities of Large Reasoning Models (LRMs).
However, they exhibit limited exploration due to reliance on on-policy rollouts
where confined to the current policy's distribution, resulting in narrow
trajectory diversity. Recent approaches attempt to expand policy coverage by
incorporating trajectories generated from stronger expert models, yet this
reliance increases computational cost and such advaned models are often
inaccessible. To address these issues, we propose In-Context Steered Policy
Optimization (ICPO), a unified framework that leverages the inherent in-context
learning capability of LRMs to provide expert guidance using existing datasets.
ICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands
exploration beyond the current policy distribution without requiring advanced
LRM trajectories. To further stabilize optimization, ICPO integrates Expert
Region Reject Sampling to filter unreliable off-policy trajectories and
Annealed Expert-Bonus Reward Shaping to balance early expert guidance with
later autonomous improvement. Results demonstrate that ICPO consistently
enhances reinforcement learning performance and training stability on
mathematical reasoning benchmarks, revealing a scalable and effective RLVR
paradigm for LRMs.

</details>


### [118] [Polybasic Speculative Decoding Through a Theoretical Perspective](https://arxiv.org/abs/2510.26527)
*Ruilin Wang,Huixia Li,Yuexiao Ma,Xiawu Zheng,Fei Chao,Xuefeng Xiao,Rongrong Ji*

Main category: cs.LG

TL;DR: 提出一种多基（polybasic）猜测解码框架，并给出理论支撑，证明多模型猜测解码的最优推理时间的基本定理；在多种模型上实现3.3x-4.4x的加速，同时保持原输出分布；可独立实现或与现有猜测解码方法集成，公开理论证明和代码。


<details>
  <summary>Details</summary>
Motivation: 推理延迟是大规模语言模型部署的瓶颈；现有的猜测解码多依赖双线框架且缺乏系统的理论分析。

Method: 提出多基猜测解码框架，开展多模型令牌生成的理论分析，推导并证明最优推理时间的基本定理，研究模型能力、接受长度与总成本之间的关系，给出可独立实现或与现有猜测方法结合的实现方案，兼容现有策略。

Result: 在多种模型族上获得显著加速：LLaMA2-Chat 7B 3.31×~4.01×、LLaMA3-8B 3.87×、Vicuna-7B 4.43×、Qwen2-7B 3.85×，同时保持原输出分布。实验也表明方法可与现有猜测技术集成。

Conclusion: 该框架为多模型猜测解码提供了更一般化、理论扎实的基础，超越了传统的双线框架，并具备直接落地的潜力与可扩展性；并公开了证明与代码，便于后续研究与应用。

Abstract: Inference latency stands as a critical bottleneck in the large-scale
deployment of Large Language Models (LLMs). Speculative decoding methods have
recently shown promise in accelerating inference without compromising the
output distribution. However, existing work typically relies on a dualistic
draft-verify framework and lacks rigorous theoretical grounding. In this paper,
we introduce a novel \emph{polybasic} speculative decoding framework,
underpinned by a comprehensive theoretical analysis. Specifically, we prove a
fundamental theorem that characterizes the optimal inference time for
multi-model speculative decoding systems, shedding light on how to extend
beyond the dualistic approach to a more general polybasic paradigm. Through our
theoretical investigation of multi-model token generation, we expose and
optimize the interplay between model capabilities, acceptance lengths, and
overall computational cost. Our framework supports both standalone
implementation and integration with existing speculative techniques, leading to
accelerated performance in practice. Experimental results across multiple model
families demonstrate that our approach yields speedup ratios ranging from
$3.31\times$ to $4.01\times$ for LLaMA2-Chat 7B, up to $3.87 \times$ for
LLaMA3-8B, up to $4.43 \times$ for Vicuna-7B and up to $3.85 \times$ for
Qwen2-7B -- all while preserving the original output distribution. We release
our theoretical proofs and implementation code to facilitate further
investigation into polybasic speculative decoding.

</details>


### [119] [Higher-Order Regularization Learning on Hypergraphs](https://arxiv.org/abs/2510.26533)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 研究拓展高阶超图学习HOHL的理论基础：证明截断版本的一致性并给出在全监督下的收敛速率，同时在主动学习和缺乏几何结构的数据集上展示出强实证性能。


<details>
  <summary>Details</summary>
Motivation: 为HOHL提供更严格的理论保障，从而证明其在截断形式下的一致性以及在作为正则化器时的收敛率，并验证在多种学习场景的鲁棒性。

Method: 构建并分析截断HOHL的理论性质，推导作为正则化项时的误差界限与收敛速率；并通过实验评估其在主动学习和非几何数据集上的性能。

Result: 证明截断HOHL的一致性并给出明确的收敛速率；实证显示HOHL在主动学习和缺乏几何结构的数据中的性能优越。

Conclusion: HOHL作为多尺度高阶拉普算子引入的正则化工具具有良好鲁棒性和适用性，适用于多种学习任务，尤其在缺乏几何结构的数据上也表现出强大潜力。

Abstract: Higher-Order Hypergraph Learning (HOHL) was recently introduced as a
principled alternative to classical hypergraph regularization, enforcing
higher-order smoothness via powers of multiscale Laplacians induced by the
hypergraph structure. Prior work established the well- and ill-posedness of
HOHL through an asymptotic consistency analysis in geometric settings. We
extend this theoretical foundation by proving the consistency of a truncated
version of HOHL and deriving explicit convergence rates when HOHL is used as a
regularizer in fully supervised learning. We further demonstrate its strong
empirical performance in active learning and in datasets lacking an underlying
geometric structure, highlighting HOHL's versatility and robustness across
diverse learning settings.

</details>


### [120] [Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices](https://arxiv.org/abs/2510.26557)
*Jan Stenkamp,Nina Herrmann,Benjamin Karic,Stefan Oehmcke,Fabian Gieseke*

Main category: cs.LG

TL;DR: 针对资源受限设备，提出面向提升决策树(Boosted Decision Trees)的压缩训练方法，通过重复使用特征与阈值实现4-16x压缩比，保持与LightGBM相近的性能，适合IoT边缘部署。


<details>
  <summary>Details</summary>
Motivation: 在物联网场景中，设备本地运行模型受限于内存和能耗，需更轻量的模型；现有方法常在准确性与压缩之间取舍，难以在边缘设备上高效部署。

Method: 通过在训练阶段引入奖励机制，鼓励特征和阈值的重用，并设计了替代的内存布局与配套的训练流程，以获得紧凑的Boosted Decision Tree集成模型。

Result: 经过改造的训练过程和内存布局后，模型在与LightGBM 相当的性能下实现4-16x的压缩；在IoT设备部署后实现自治运行，减少对持续通信或外部能源的依赖。

Conclusion: 该方法使Boosted Decision Trees在资源受限的IoT设备上更易部署，推动远程监控、边缘分析和实时决策等应用的落地。

Abstract: Deploying machine learning models on compute-constrained devices has become a
key building block of modern IoT applications. In this work, we present a
compression scheme for boosted decision trees, addressing the growing need for
lightweight machine learning models. Specifically, we provide techniques for
training compact boosted decision tree ensembles that exhibit a reduced memory
footprint by rewarding, among other things, the reuse of features and
thresholds during training. Our experimental evaluation shows that models
achieved the same performance with a compression ratio of 4-16x compared to
LightGBM models using an adapted training process and an alternative memory
layout. Once deployed, the corresponding IoT devices can operate independently
of constant communication or external energy supply, and, thus, autonomously,
requiring only minimal computing power and energy. This capability opens the
door to a wide range of IoT applications, including remote monitoring, edge
analytics, and real-time decision making in isolated or power-limited
environments.

</details>


### [121] [Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories through the Bernstein Basis](https://arxiv.org/abs/2510.26607)
*Maksim Maslov,Alexander Kugaevskikh,Matthew Ivanov*

Main category: cs.LG

TL;DR: 提出一种基于 Bernstein 基和 Wasserstein 距离的分布回归新方法，利用高斯混合的概率轨迹参数化，兼顾几何性、可解释性与计算性。


<details>
  <summary>Details</summary>
Motivation: 许多回归任务需对输入-输出为分布的关系建模，但传统方法忽略概率空间的几何结构或计算成本高。需要一个可解释且高效地捕捉分布几何性的框架。

Method: 把条件分布建模为通过 Bernstein 多项式参数化的平滑概率轨迹，轨迹由高斯分量的加权和给出，其均值和协方差作为输入的函数。损失采用预测分布与经验数据之间的平均平方 Wasserstein 距离，使用自动微分优化。训练过程以控制点参数化实现可解释性。

Result: 在合成数据集上测试，尤其在非线性强的情形下，方法在 Wasserstein 距离、Energy Distance 和 RMSE 等指标上具有竞争性；轨迹平滑性优于或接近替代方法，且对数据结构的变化具有鲁棒性。

Conclusion: 该方法在几何准确性、计算实用性与可解释性之间取得平衡，未来工作包括扩展到非高斯分布、引入熵正则化以加速计算，以及将其扩展到高维数据以近似曲面等复杂结构。

Abstract: This paper considers the problem of regression over distributions, which is
becoming increasingly important in machine learning. Existing approaches often
ignore the geometry of the probability space or are computationally expensive.
To overcome these limitations, a new method is proposed that combines the
parameterization of probability trajectories using a Bernstein basis and the
minimization of the Wasserstein distance between distributions. The key idea is
to model a conditional distribution as a smooth probability trajectory defined
by a weighted sum of Gaussian components whose parameters -- the mean and
covariance -- are functions of the input variable constructed using Bernstein
polynomials. The loss function is the averaged squared Wasserstein distance
between the predicted Gaussian distributions and the empirical data, which
takes into account the geometry of the distributions. An autodiff-based
optimization method is used to train the model. Experiments on synthetic
datasets that include complex trajectories demonstrated that the proposed
method provides competitive approximation quality in terms of the Wasserstein
distance, Energy Distance, and RMSE metrics, especially in cases of pronounced
nonlinearity. The model demonstrates trajectory smoothness that is better than
or comparable to alternatives and robustness to changes in data structure,
while maintaining high interpretability due to explicit parameterization via
control points. The developed approach represents a balanced solution that
combines geometric accuracy, computational practicality, and interpretability.
Prospects for further research include extending the method to non-Gaussian
distributions, applying entropy regularization to speed up computations, and
adapting the approach to working with high-dimensional data for approximating
surfaces and more complex structures.

</details>


### [122] [Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization](https://arxiv.org/abs/2510.26633)
*Colin Doumont,Victor Picheny,Viacheslav Borovitskiy,Henry Moss*

Main category: cs.LG

TL;DR: 提出基于热核的统一框架来分析和实现组合优化中的贝叶斯优化（BO）核，证明并连接现有的组合核与热核关系，且热核对目标函数最优解的位置不敏感，建立简单高效的管线实现状态级的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 澄清和统一现有的组合核之间的关系，提供一个系统化的热核框架以提升组合领域BO的理解和性能。

Method: 系统推导与闭式表达热核；将现有组合核映射到热核；给出理论关系证明；通过实验验证假设并对比Bounce等方法。

Result: 多种成功的组合核与热核相关或等价；热核对最优解位置不敏感；提出快速简洁的管线实现达到或超过部分复杂算法的性能。

Conclusion: 热核框架为组合域的BO提供稳健、可扩展的统一视角，既加强理解又提升实际性能。

Abstract: Bayesian Optimization (BO) has the potential to solve various combinatorial
tasks, ranging from materials science to neural architecture search. However,
BO requires specialized kernels to effectively model combinatorial domains.
Recent efforts have introduced several combinatorial kernels, but the
relationships among them are not well understood. To bridge this gap, we
develop a unifying framework based on heat kernels, which we derive in a
systematic way and express as simple closed-form expressions. Using this
framework, we prove that many successful combinatorial kernels are either
related or equivalent to heat kernels, and validate this theoretical claim in
our experiments. Moreover, our analysis confirms and extends the results
presented in Bounce: certain algorithms' performance decreases substantially
when the unknown optima of the function do not have a certain structure. In
contrast, heat kernels are not sensitive to the location of the optima. Lastly,
we show that a fast and simple pipeline, relying on heat kernels, is able to
achieve state-of-the-art results, matching or even outperforming certain slow
or complex algorithms.

</details>


### [123] [MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection](https://arxiv.org/abs/2510.26643)
*Emmanouil Sylligardos,John Paparrizos,Themis Palpanas,Pierre Senellart,Paul Boniol*

Main category: cs.LG

TL;DR: 通过把时间序列分类作为模型选择来提升异常检测的效果；在大规模异构时间序列上的评估显示模型选择方法优于单一异常检测器，且时间成本在可接受范围内，为AutoML管线提供基线。


<details>
  <summary>Details</summary>
Motivation: 当前很难在异构时间序列上找到单一的最优异常检测方法；需要一种可扩展的模型选择策略，根据时间序列的特征在不同检测器中进行选择。现有AutoML对时间序列异常检测并不直接适用，且缺乏基于时间序列的模型选择评估。

Method: 将时间序列分类方法作为模型选择器，对16个基础分类器生成的234个配置，在超过1980个时间序列数据集上进行系统评估，比较不同模型选择策略在异常检测任务中的表现和效率，给出第一份全面的时间序列分类作为异常检测模型选择的实验评估。

Result: 模型选择方法在性能上超过所有单独的异常检测方法，且与单一检测方法的运行时间相当量级；这是首次对基于时间序列分类的模型选择进行如此大规模的评估，提供了一个强基线，能用于指导通用AutoML管线中的模型选择步骤。

Conclusion: 时间序列分类作为异常检测的模型选择具有很高的实用性和有效性，可作为AutoML中的关键组成部分，推动在异构时间序列上的高效准确异常检测。

Abstract: Anomaly detection is a fundamental task for time series analytics with
important implications for the downstream performance of many applications.
Despite increasing academic interest and the large number of methods proposed
in the literature, recent benchmarks and evaluation studies demonstrated that
no overall best anomaly detection methods exist when applied to very
heterogeneous time series datasets. Therefore, the only scalable and viable
solution to solve anomaly detection over very different time series collected
from diverse domains is to propose a model selection method that will select,
based on time series characteristics, the best anomaly detection methods to
run. Existing AutoML solutions are, unfortunately, not directly applicable to
time series anomaly detection, and no evaluation of time series-based
approaches for model selection exists. Towards that direction, this paper
studies the performance of time series classification methods used as model
selection for anomaly detection. In total, we evaluate 234 model configurations
derived from 16 base classifiers across more than 1980 time series, and we
propose the first extensive experimental evaluation of time series
classification as model selection for anomaly detection. Our results
demonstrate that model selection methods outperform every single anomaly
detection method while being in the same order of magnitude regarding execution
time. This evaluation is the first step to demonstrate the accuracy and
efficiency of time series classification algorithms for anomaly detection, and
represents a strong baseline that can then be used to guide the model selection
step in general AutoML pipelines. Preprint version of an article accepted at
the VLDB Journal.

</details>


### [124] [Curly Flow Matching for Learning Non-gradient Field Dynamics](https://arxiv.org/abs/2510.26645)
*Katarina Petrović,Lazar Atanackovic,Viggo Moro,Kacper Kapuśniak,İsmail İlkan Ceylan,Michael Bronstein,Avishek Joey Bose,Alexander Tong*

Main category: cs.LG

TL;DR: Curly Flow Matching (Curly-FM) learns non-gradient, often periodic, transport dynamics by solving a Schrödinger bridge with a non-zero drift reference. It uses inferred velocities plus population snapshots, outperforming gradient-based flow matching on diverse domains (single-cell trajectories, CFD, ocean currents).


<details>
  <summary>Details</summary>
Motivation: Most current transport models assume gradient (least-action) dynamics leading to energy-minimizing trajectories. Many real systems exhibit non-gradient, periodic behavior (e.g., cell cycles), which these methods cannot capture. A framework that accommodates non-zero drift and velocity information is needed.

Method: Introduce Curly-FM by formulating a Schrödinger bridge problem with a non-zero drift reference process. The drift is constructed from inferred velocities along with population snapshot data, enabling learning of non-gradient dynamics from marginal distributions and velocity cues.

Result: Curly-FM yields trajectories that better match both the non-gradient reference process and population marginals. Demonstrated on single-cell trajectory inference, computational fluid dynamics, and ocean currents with approximate velocities. Code is available at the authors' repository.

Conclusion: Extends flow matching beyond purely population-based modeling to capture known periodic behavior in physical systems, broadening the applicability of transport-based trajectory inference.

Abstract: Modeling the transport dynamics of natural processes from population-level
observations is a ubiquitous problem in the natural sciences. Such models rely
on key assumptions about the underlying process in order to enable faithful
learning of governing dynamics that mimic the actual system behavior. The de
facto assumption in current approaches relies on the principle of least action
that results in gradient field dynamics and leads to trajectories minimizing an
energy functional between two probability measures. However, many real-world
systems, such as cell cycles in single-cell RNA, are known to exhibit
non-gradient, periodic behavior, which fundamentally cannot be captured by
current state-of-the-art methods such as flow and bridge matching. In this
paper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is
capable of learning non-gradient field dynamics by designing and solving a
Schr\"odinger bridge problem with a non-zero drift reference process -- in
stark contrast to typical zero-drift reference processes -- which is
constructed using inferred velocities in addition to population snapshot data.
We showcase Curly-FM by solving the trajectory inference problems for single
cells, computational fluid dynamics, and ocean currents with approximate
velocities. We demonstrate that Curly-FM can learn trajectories that better
match both the reference process and population marginals. Curly-FM expands
flow matching models beyond the modeling of populations and towards the
modeling of known periodic behavior in physical systems. Our code repository is
accessible at: https://github.com/kpetrovicc/curly-flow-matching.git

</details>


### [125] [Tight Differentially Private PCA via Matrix Coherence](https://arxiv.org/abs/2510.26679)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: 提出一个简单高效的私有 rank-r 近似算法，误差仅依赖于前 r 个奇向量的秩-r 相干性与谱间距（σ_r - σ_{r+1}），在 Hardt–Roth 的问题上取得积极解答，并在密集 Wishart 场景达到与无私算法相当的性能；还证明高斯扰动不提高相干性，并将相干性应用拓展到私有图问题如 Max-Cut 的求解。


<details>
  <summary>Details</summary>
Motivation: 解决在差分隐私设定下有效估计前 r 个左奇向量的 span 的问题，回应 Hardt 与 Roth 的提问，并在密集模型与图结构中实现更强的隐私-准确性权衡。

Method: 基于奇异值分解（SVD）与标准的扰动机制，构造一个简单而高效的私有 rank-r 近似；分析在高斯扰动下相干性的保持性质，以及对谱间距的依赖；对图问题应用相干性条件给出私有算法。

Result: 所得私有 rank-r 近似的误差只依赖于秩-r 相干性与谱间距；在密集 setting 的单峰 PCA 的 Wishart 模型下，与最优非私有算法等效，显著优于现有私有算法；证明高斯扰动不会增加相干性。

Conclusion: 相干性是设计和分析差分隐私算法的关键，在低相干设定下可将方法扩展到图问题（如私有 Max-Cut），并提出对其他结构化模型（如 planted 图问题）的类似性质的猜想。

Abstract: We revisit the task of computing the span of the top $r$ singular vectors
$u_1, \ldots, u_r$ of a matrix under differential privacy. We show that a
simple and efficient algorithm -- based on singular value decomposition and
standard perturbation mechanisms -- returns a private rank-$r$ approximation
whose error depends only on the \emph{rank-$r$ coherence} of $u_1, \ldots, u_r$
and the spectral gap $\sigma_r - \sigma_{r+1}$. This resolves a question posed
by Hardt and Roth~\cite{hardt2013beyond}. Our estimator outperforms the state
of the art -- significantly so in some regimes. In particular, we show that in
the dense setting, it achieves the same guarantees for single-spike PCA in the
Wishart model as those attained by optimal non-private algorithms, whereas
prior private algorithms failed to do so.
  In addition, we prove that (rank-$r$) coherence does not increase under
Gaussian perturbations. This implies that any estimator based on the Gaussian
mechanism -- including ours -- preserves the coherence of the input. We
conjecture that similar behavior holds for other structured models, including
planted problems in graphs.
  We also explore applications of coherence to graph problems. In particular,
we present a differentially private algorithm for Max-Cut and other constraint
satisfaction problems under low coherence assumptions.

</details>


### [126] [LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits](https://arxiv.org/abs/2510.26690)
*Amir Reza Mirzaei,Yuqiao Wen,Yanshuai Cao,Lili Mou*

Main category: cs.LG

TL;DR: LoRAQuant通过对LoRA适配器进行SVD重参数化，实施混合精度的后训练量化，在保持或提升性能的同时显著降低所需比特宽度，适用于多适配并行的LLM场景。


<details>
  <summary>Details</summary>
Motivation: 现实场景下需要同时加载多個LoRA适配器以实现个性化和多任务，但总成本随之显著增加，需要一种高效的量化策略来降低存储和推理成本。

Method: 对每个LoRA适配器进行SVD分解，重新参数化，使关键成分集中在少数行和列；对关键分量使用较高精度量化，其他分量使用极低比特宽度；在后训练阶段应用混合精度量化。

Result: 在LLaMA 2-7B、LLaMA 2-13B和Mistral 7B模型上进行数学推理、编码和摘要等任务的实验，LoRAQuant能够以显著更低的比特宽度实现，与现有量化方法相比性能相当或更好。

Conclusion: 提供了一种有效的混合精度后训练量化方案，显著降低多适配器场景的总存储和计算成本，同时保持或提升性能，适合大规模应用。

Abstract: Low-Rank Adaptation (LoRA) has become a popular technique for
parameter-efficient fine-tuning of large language models (LLMs). In many
real-world scenarios, multiple adapters are loaded simultaneously to enable LLM
customization for personalized user experiences or to support a diverse range
of tasks. Although each adapter is lightweight in isolation, their aggregate
cost becomes substantial at scale. To address this, we propose LoRAQuant, a
mixed-precision post-training quantization method tailored to LoRA.
Specifically, LoRAQuant reparameterizes each adapter by singular value
decomposition (SVD) to concentrate the most important information into specific
rows and columns. This makes it possible to quantize the important components
to higher precision, while quantizing the rest to ultra-low bitwidth. We
conduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B
models on mathematical reasoning, coding, and summarization tasks. Results show
that our LoRAQuant uses significantly lower bits than other quantization
methods, but achieves comparable or even higher performance.

</details>


### [127] [Budgeted Multiple-Expert Deferral](https://arxiv.org/abs/2510.26706)
*Giulia DeSalvo,Clara Mohri,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: Budgeted deferral: train deferral models while querying only a subset of experts per example to cut costs, with theoretical guarantees and strong empirical results.


<details>
  <summary>Details</summary>
Motivation: Deferral systems often query many experts for every training instance, incurring high cost and resource usage. The paper introduces a budget-aware framework to reduce query costs during training while maintaining predictive performance, addressing a gap where labels exist but expert queries are expensive.

Method: Proposes algorithms for two-stage and single-stage multiple-expert deferral that selectively query a subset of experts per training example. Although inspired by active learning, labels are known, and the challenge is cost-performance trade-off via selective querying. The framework includes theoretical analysis: generalization bounds and label complexity; and empirical validation across domains.

Result: Algorithms achieve substantial training cost reductions by querying fewer experts without sacrificing accuracy; theoretical guarantees support generalization and efficient label use.

Conclusion: Budgeted deferral algorithms provide a practical approach for cost-aware deferral in ML systems, enabling effective use of expensive experts and potentially broad applicability across domains with costly annotations or expert inputs.

Abstract: Learning to defer uncertain predictions to costly experts offers a powerful
strategy for improving the accuracy and efficiency of machine learning systems.
However, standard training procedures for deferral algorithms typically require
querying all experts for every training instance, an approach that becomes
prohibitively expensive when expert queries incur significant computational or
resource costs. This undermines the core goal of deferral: to limit unnecessary
expert usage. To overcome this challenge, we introduce the budgeted deferral
framework, which aims to train effective deferral algorithms while minimizing
expert query costs during training. We propose new algorithms for both
two-stage and single-stage multiple-expert deferral settings that selectively
query only a subset of experts per training example. While inspired by active
learning, our setting is fundamentally different: labels are already known, and
the core challenge is to decide which experts to query in order to balance cost
and predictive performance. We establish theoretical guarantees for both of our
algorithms, including generalization bounds and label complexity analyses.
Empirical results across several domains show that our algorithms substantially
reduce training costs without sacrificing prediction accuracy, demonstrating
the practical value of our budget-aware deferral algorithms.

</details>


### [128] [An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning](https://arxiv.org/abs/2510.26709)
*Chuyan Chen,Chenyang Ma,Zhangxin Li,Yutong He,Yanjie Dong,Kun Yuan*

Main category: cs.LG

TL;DR: 提出 ARC-Top-K，一种与 All-Reduce 兼容的 Top-K 梯度压缩器，通过轻量草图对齐跨节点的稀疏模式，保留全局信息，实现索引无关的 All-Reduce，并在 EF21M 下实现线性加速及更快收敛，训练时间显著降低。


<details>
  <summary>Details</summary>
Motivation: 解决大规模分布式训练中的通讯瓶颈；现有 Rand-K 缺乏结构信息，Top-K 虽信息保留但缺乏收缩性且需要昂贵的 All-Gather，亟需兼具鲁棒性与高效性的压缩方案。

Method: 提出 ARC-Top-K：一个 All-Reduce 兼容的 Top-K 压缩器，使用轻量草图对齐跨节点的梯度稀疏模式，实现不依赖显式索引的 All-Reduce，同时保留全局重要信息；给出收缩性证明；并与动量误差反馈 EF21M 结合以获得线性加速和更快收敛；通过理论与实验验证其性能。

Result: 理论层面，ARC-Top-K 具备收缩性；结合 EF21M 实现线性加速并在收敛速度上优于原始 EF21M；实验上可达到与 Top-K 相当的精度，同时将训练时间降低最多约 60.7%，表现出比 Rand-K 更鲁棒、且保持 Top-K 强性能的优势。

Conclusion: ARC-Top-K 提供一个高效、可扩展的梯度压缩框架，兼具鲁棒性与准确性，适合大规模分布式训练，且通过索引无关的 All-Reduce 实现更高效的通信。

Abstract: Communication remains a central bottleneck in large-scale distributed machine
learning, and gradient sparsification has emerged as a promising strategy to
alleviate this challenge. However, existing gradient compressors face notable
limitations: Rand-$K$\ discards structural information and performs poorly in
practice, while Top-$K$\ preserves informative entries but loses the
contraction property and requires costly All-Gather operations. In this paper,
we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that
aligns sparsity patterns across nodes using a lightweight sketch of the
gradient, enabling index-free All-Reduce while preserving globally significant
information. ARC-Top-$K$\ is provably contractive and, when combined with
momentum error feedback (EF21M), achieves linear speedup and sharper
convergence rates than the original EF21M under standard assumptions.
Empirically, ARC-Top-$K$\ matches the accuracy of Top-$K$\ while reducing
wall-clock training time by up to 60.7\%, offering an efficient and scalable
solution that combines the robustness of Rand-$K$\ with the strong performance
of Top-$K$.

</details>


### [129] [On the limitation of evaluating machine unlearning using only a single training seed](https://arxiv.org/abs/2510.26714)
*Jamie Lanyon,Axel Finke,Petros Andreou,Georgina Cosma*

Main category: cs.LG

TL;DR: MU评估存在对随机种子高度敏感的问题，需在跨不同训练种子下进行比较以确保结果具有代表性。


<details>
  <summary>Details</summary>
Motivation: 现有MU算法多数为近似，实际评估通常在同一训练模型上重复多次，但这种做法可能掩盖种子带来的显著变异，影响方法间的公正比较。

Method: 通过在相同架构和数据集下对MU方法进行多次独立训练并比较结果，验证不同训练种子对评估结果的影响，揭示对比结果的非代表性。

Result: 研究发现某些MU方法对随机种子极为敏感，单一种子下的比较可能给出误导性结论；需将种子变异纳入评估框架。

Conclusion: MU算法评估应包含跨训练种子的变异性，报告应同时给出多种种子下的结果，以提升对比的鲁棒性与可重复性。

Abstract: Machine unlearning (MU) aims to remove the influence of certain data points
from a trained model without costly retraining. Most practical MU algorithms
are only approximate and their performance can only be assessed empirically.
Care must therefore be taken to make empirical comparisons as representative as
possible. A common practice is to run the MU algorithm multiple times
independently starting from the same trained model. In this work, we
demonstrate that this practice can give highly non-representative results
because -- even for the same architecture and same dataset -- some MU methods
can be highly sensitive to the choice of random number seed used for model
training. We therefore recommend that empirical
comphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should
also reflect the variability across different model training seeds.

</details>


### [130] [On Purely Private Covariance Estimation](https://arxiv.org/abs/2510.26717)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: 给出一个简单的扰动机制，在纯差分隐私下对 d 维协方差矩阵 Σ 的释放实现最优的 Frobenius 范数误差；当 n ≥ d^2/ε 时，达到最优误差并覆盖所有 p-Schatten 范数 (p ∈ [1, ∞])，且对 p≥2 的信息论上最优，尤其在谱范数下成为纯隐私下的最优估计；在较小数据集 n < d^2/ε 时，输出投影到合适半径的核范数球可获得 Frobenius 误差 O(√(d Tr(Σ)/n))，优于并且改进了现有界限。


<details>
  <summary>Details</summary>
Motivation: 在保护隐私的前提下，发布协方差矩阵 Σ，同时尽量保留统计效能，提升对不同范数误差的上界，弥合大数据与小数据场景。

Method: 提出一种简单的扰动机制；对小数据集通过对输出投影到核范数球实现正则化；对大数据集给出与先前结果相同的 Frobenius 误差界，同时实现对所有 p-Schatten 范数的最优或接近最优误差；证明在 p≥2 情况下的谱范数误差达到信息论下的下界。

Result: 在 n ≥ d^2/ε 时，达到 Frobenius 范数误差的最优界，并对所有 p≥1 的 Schatten 范数提供最佳或最接近最佳的误差界；对于较小 n，投影核范数球后可获得 O(√(d Tr(Σ)/n)) 的 Frobenius 误差。

Conclusion: 提出的机制在大数据场景下实现最优的差分隐私协方差估计，同时在小数据场景提供更强的 Frobenius 误差界，覆盖了多种范数衡量的误差需求。

Abstract: We present a simple perturbation mechanism for the release of $d$-dimensional
covariance matrices $\Sigma$ under pure differential privacy. For large
datasets with at least $n\geq d^2/\varepsilon$ elements, our mechanism recovers
the provably optimal Frobenius norm error guarantees of
\cite{nikolov2023private}, while simultaneously achieving best known error for
all other $p$-Schatten norms, with $p\in [1,\infty]$. Our error is
information-theoretically optimal for all $p\ge 2$, in particular, our
mechanism is the first purely private covariance estimator that achieves
optimal error in spectral norm.
  For small datasets $n< d^2/\varepsilon$, we further show that by projecting
the output onto the nuclear norm ball of appropriate radius, our algorithm
achieves the optimal Frobenius norm error $O(\sqrt{d\;\text{Tr}(\Sigma) /n})$,
improving over the known bounds of $O(\sqrt{d/n})$ of \cite{nikolov2023private}
and ${O}\big(d^{3/4}\sqrt{\text{Tr}(\Sigma)/n}\big)$ of
\cite{dong2022differentially}.

</details>


### [131] [Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off](https://arxiv.org/abs/2510.26722)
*Muhammad Faraz Ul Abrar,Nicolò Michelusi*

Main category: cs.LG

TL;DR: 本工作研究在异构无线环境下的 OTA-FL，针对非凸目标提出带结构化时间不变偏置的更新与协同功率控制，给出有限时间的梯度范数均方收敛界并揭示偏置-方差权衡；通过非凸联合 OTA 功率控制结合逐步凸近似（SCA）实现仅需统计 CSI 的最优/近似解。


<details>
  <summary>Details</summary>
Motivation: 现有 OTA-FL 要么在同质信道假设下强制零偏置，要么为了收敛性而牺牲更新偏差；在实际异构无线场景下，最弱设备限制整体性能且更新方差增大。且大多数对偏置 OTA-FL 的分析聚焦于凸目标，难以直接推广到非凸、深度学习等场景。需要在允许有结构化偏置的同时控制方差，提升非凸目标下的收敛性与泛化。

Method: 提出带有结构化、时间不变量的模型偏置的 OTA-FL SGD 更新；推导有限时间的期望平均梯度范数平方界，揭示偏置-方差的权衡；构造非凸的联合 OTA 功率控制问题，并给出可快速收敛的逐步凸近似（SCA）算法，且仅需基地站端的统计 CSI。

Result: 在非凸图像分类任务上，所提出的 SCA 基于设计明显加速了收敛，相较于现有 OTA-FL 基线在泛化性方面有明显提升，且能通过优化偏置来降低更新方差。

Conclusion: 通过引入结构化偏置的 OTA-FL 更新并辅以基于 SCA 的非凸功率控制，能在异构无线环境中有效降低对最弱设备的制约、改善收敛速度与泛化能力，证明了偏置-方差权衡在非凸 OTA-FL 中的实用性与有效性。

Abstract: Over-the-air (OTA) federated learning (FL) has been well recognized as a
scalable paradigm that exploits the waveform superposition of the wireless
multiple-access channel to aggregate model updates in a single use. Existing
OTA-FL designs largely enforce zero-bias model updates by either assuming
\emph{homogeneous} wireless conditions (equal path loss across devices) or
forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous}
wireless scenarios, however, such designs are constrained by the weakest device
and inflate the update variance. Moreover, prior analyses of biased OTA-FL
largely address convex objectives, while most modern AI models are highly
non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient
descent (SGD) for general smooth non-convex objectives under wireless
heterogeneity. We develop novel OTA-FL SGD updates that allow a structured,
time-invariant model bias while facilitating reduced variance updates. We
derive a finite-time stationarity bound (expected time average squared gradient
norm) that explicitly reveals a bias-variance trade-off. To optimize this
trade-off, we pose a non-convex joint OTA power-control design and develop an
efficient successive convex approximation (SCA) algorithm that requires only
statistical CSI at the base station. Experiments on a non-convex image
classification task validate the approach: the SCA-based design accelerates
convergence via an optimized bias and improves generalization over prior OTA-FL
baselines.

</details>


### [132] [STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization](https://arxiv.org/abs/2510.26771)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.LG

TL;DR: STaMP 在序列维度应用线性变换并采用混合精度量化，保持少量高精度 token 的同时降低整体激活位宽，以提升低比特量化的精度。


<details>
  <summary>Details</summary>
Motivation: 激活量化在低于8位时精度下降显著；线性变换（如旋转）可重参数化特征通道和权重。通过在序列维度利用强局部相关性，并以混合精度保留少量 token 的高精度来缓解信息损失，提升量化鲁棒性。

Method: 提出 STaMP，沿序列维度应用可逆线性变换，并在中间激活中对少量 token 掺入较高精度表示，形成混合精度量化策略；同时与现有激活和权重量化及其他特征变换方法协同工作，适用于语言与视觉数据的 LVM/LLM 架构。

Result: 在最新的 LVM 和 LLM 架构上评估，STaMP 显著提升低比特宽度激活量化的性能，并与现有方法互补。

Conclusion: STaMP 为低比特激活量化提供一种有效策略，通过序列维度变换与混合精度减少信息损失，提升语言和视觉模型的量化鲁棒性，且易于与现有量化技术结合。

Abstract: Quantization is the key method for reducing inference latency, power and
memory footprint of generative AI models. However, accuracy often degrades
sharply when activations are quantized below eight bits. Recent work suggests
that invertible linear transformations (e.g. rotations) can aid quantization,
by reparameterizing feature channels and weights. In this paper, we propose
\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a
novel strategy that applies linear transformations along the \textit{sequence}
dimension to exploit the strong local correlation in language and visual data.
By keeping a small number of tokens in each intermediate activation at higher
precision, we can maintain model accuracy at lower (average) activations
bit-widths. We evaluate STaMP on recent LVM and LLM architectures,
demonstrating that it significantly improves low bit width activation
quantization and complements established activation and weight quantization
methods including recent feature transformations.

</details>


### [133] [Faithful and Fast Influence Function via Advanced Sampling](https://arxiv.org/abs/2510.26776)
*Jungyeon Koh,Hyeonsu Lyu,Jonggyu Jang,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 提出基于特征和 logits 的两种采样策略，以小而具代表性的子集近似全数据的影响函数（IF），缓解全数据 Hessian 计算成本高和采样方差大问题。


<details>
  <summary>Details</summary>
Motivation: 影响函数在黑箱模型中的后验分析中需要 Hessian 及梯度信息，但对整个训练集计算成本极高且随机子集导致估计方差大，因此需要更稳健、低成本的子集选择方法。

Method: 提出两种基于特征分布与 logits 分布的采样器，从训练数据中选取一个小而具有代表性子集，以降低 IF 估计的方差并提高准确性，相比随机采样更稳定。

Result: 在类删除任务中验证效果，通过 F1-score 衡量模型忘记被删除的类别的能力，同时保持对其他类别的推理一致性。与基线相比，方法在计算时间下降约 30.1%、内存下降约 42.2%，或在相同成本下 F1 提升约 2.5%。

Conclusion: 通过有代表性的子集采样提升 IF 的可扩展性与稳定性，显著降低资源开销并提升估计性能，同时保持模型对剩余类别的推理一致性。

Abstract: How can we explain the influence of training data on black-box models?
Influence functions (IFs) offer a post-hoc solution by utilizing gradients and
Hessians. However, computing the Hessian for an entire dataset is
resource-intensive, necessitating a feasible alternative. A common approach
involves randomly sampling a small subset of the training data, but this method
often results in highly inconsistent IF estimates due to the high variance in
sample configurations. To address this, we propose two advanced sampling
techniques based on features and logits. These samplers select a small yet
representative subset of the entire dataset by considering the stochastic
distribution of features or logits, thereby enhancing the accuracy of IF
estimations. We validate our approach through class removal experiments, a
typical application of IFs, using the F1-score to measure how effectively the
model forgets the removed class while maintaining inference consistency on the
remaining classes. Our method reduces computation time by 30.1% and memory
usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.

</details>


### [134] [Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification](https://arxiv.org/abs/2510.26777)
*Andreas Auer,Daniel Klotz,Sebastinan Böck,Sepp Hochreiter*

Main category: cs.LG

TL;DR: 冻结的预测模型也能提供可用于分类的有效表示，表明以预测为自监督目标的预训练具有良好的通用性。


<details>
  <summary>Details</summary>
Motivation: 探究时间序列前馈模型学习的表示是否具有跨任务的普适性，评估冻结的预测模型在分类任务中的可用性，并检验是否需要为每个下游任务进行专门的预训练。

Method: 比较不同的表示提取策略，提出两种与模型无关的嵌入增强方法；在分类任务上评估冻结的预测模型表示，并与专为分类预训练的模型进行对比，分析预测性能与分类性能之间的相关性。

Result: 实验表明，最佳的预测模型在分类准确度上可达到甚至超过专门为分类预训练的方法；预测与分类性能之间存在正相关关系；嵌入增强对提升表示质量有帮助。

Conclusion: 以预测为核心的预训练可作为构建通用时间序列基础模型的有力途径，任务特定的预训练并非必要，从而推动了更统一的时间序列表示学习范式。

Abstract: Recent research on time series foundation models has primarily focused on
forecasting, leaving it unclear how generalizable their learned representations
are. In this study, we examine whether frozen pre-trained forecasting models
can provide effective representations for classification. To this end, we
compare different representation extraction strategies and introduce two
model-agnostic embedding augmentations. Our experiments show that the best
forecasting models achieve classification accuracy that matches or even
surpasses that of state-of-the-art models pre-trained specifically for
classification. Moreover, we observe a positive correlation between forecasting
and classification performance. These findings challenge the assumption that
task-specific pre-training is necessary, and suggest that learning to forecast
may provide a powerful route toward constructing general-purpose time series
foundation models.

</details>


### [135] [Clone Deterministic 3D Worlds with Geometrically-Regularized World Models](https://arxiv.org/abs/2510.26782)
*Zaishuo Xia,Yukuan Lu,Xinyi Li,Yifan Xu,Yubei Chen*

Main category: cs.LG

TL;DR: GRWM 通过在潜在空间中对连续的传感轨迹点施加几何正则化，使潜在流形更贴合环境拓扑，从而显著提升长期 roll-out 的保真度与稳定性，而不需要扩大动态模块的复杂度。


<details>
  <summary>Details</summary>
Motivation: 旨在解决世界模型对表征质量的高度依赖；高维感知输入易导致潜在表达失真，进而使动力学习困难，因此通过改进表示学习来提升模型性能。

Method: 引入几何正则化，确保在自然感知轨迹上的相邻时间点在潜在表示空间保持接近；该正则化可与现有潜在生成骨干无缝配合，按轨迹长度扩展并在不同的潜在结构上均可使用。

Result: 在确定性三维场景与长时预测任务中，GRWM 显著提高了回滚（rollout）的保真度与稳定性；分析表明收益来自学习到更符合环境拓扑的潜在流形。

Conclusion: 强化表示学习是实现鲁棒世界模型的直接且有用路径，能够在不扩大动力学模块的前提下实现可靠的长时预测。

Abstract: A world model is an internal model that simulates how the world evolves.
Given past observations and actions, it predicts the future of both the
embodied agent and its environment. Accurate world models are essential for
enabling agents to think, plan, and reason effectively in complex, dynamic
settings. Despite rapid progress, current world models remain brittle and
degrade over long horizons. We argue that a central cause is representation
quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or
entangled latents make dynamics learning unnecessarily hard. We therefore ask
whether improving representation learning alone can substantially improve
world-model performance. In this work, we take a step toward building a truly
accurate world model by addressing a fundamental yet open problem: constructing
a model that can fully clone and overfit to a deterministic 3D world. We
propose Geometrically-Regularized World Models (GRWM), which enforces that
consecutive points along a natural sensory trajectory remain close in latent
representation space. This approach yields significantly improved latent
representations that align closely with the true topology of the environment.
GRWM is plug-and-play, requires only minimal architectural modification, scales
with trajectory length, and is compatible with diverse latent generative
backbones. Across deterministic 3D settings and long-horizon prediction tasks,
GRWM significantly increases rollout fidelity and stability. Analyses show that
its benefits stem from learning a latent manifold with superior geometric
structure. These findings support a clear takeaway: improving representation
learning is a direct and useful path to robust world models, delivering
reliable long-horizon predictions without enlarging the dynamics module.

</details>


### [136] [Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability](https://arxiv.org/abs/2510.26792)
*Tao Tao,Maissam Barkeshli*

Main category: cs.LG

TL;DR: Transformers能够在上下文中学习并预测由Permuted Congruential Generators (PCG) 生成的序列，且对模数高达2^22的任务也能实现近乎完美的预测。核心发现包括：输出即使仅是一位比特也可预测；对多种PRNG的联合学习；模数m与所需上下文长度呈平方根关系；大模数需要分阶段（课程）训练；嵌入层呈现出比特级旋转不变的聚类现象，能从小模数逐步迁移到大模数。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer在学习复杂的伪随机数生成过程中的上下文学习能力，尤其是在PCG等经多轮位操作的生成器上，并揭示模型对于不同变体的泛化能力、规模扩展的规律，以及内部表示的结构性特征。

Method: 在模数m可达2^22的范围内，使用多达5千万参数、数据量高达50亿标记的Transformer，对来自多种PCG变体的序列进行训练与评估，任务包括在上下文中预测下一个输出，甚至对截断为单个位的输出进行预测；在训练中引入多种不同的PRNG，并考察模型对它们的联合学习；分析嵌入层的聚类与表示结构；测试课程学习对大模数的作用。

Result: 模型能够在未见的PCG变体上实现近乎完美的上下文预测；即使输出被截断为一个位，仍能可靠预测；多PRNG的联合学习能够识别不同置换中的结构；模数m与需要的上下文长度之间存在sqrt(m)的规模关系；对于更大模数，优化过程出现扩展的停滞阶段，需先训练较小模数的数据形成课程；嵌入层显示出自发形成的“比特级旋转不变”聚类，说明表示能够从小模数迁移到大模数。

Conclusion: Transformer具备在复杂的PRNG序列上进行强上下文学习的能力，揭示了关于表示学习、规模扩展和课程学习的重要规律；研究还提供了关于PRNG结构的内部表示的洞见，及其对跨模数迁移的潜在影响。

Abstract: We study the ability of Transformer models to learn sequences generated by
Permuted Congruential Generators (PCGs), a widely used family of pseudo-random
number generators (PRNGs). PCGs introduce substantial additional difficulty
over linear congruential generators (LCGs) by applying a series of bit-wise
shifts, XORs, rotations and truncations to the hidden state. We show that
Transformers can nevertheless successfully perform in-context prediction on
unseen sequences from diverse PCG variants, in tasks that are beyond published
classical attacks. In our experiments we scale moduli up to $2^{22}$ using up
to $50$ million model parameters and datasets with up to $5$ billion tokens.
Surprisingly, we find even when the output is truncated to a single bit, it can
be reliably predicted by the model. When multiple distinct PRNGs are presented
together during training, the model can jointly learn them, identifying
structures from different permutations. We demonstrate a scaling law with
modulus $m$: the number of in-context sequence elements required for
near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization
enters extended stagnation phases; in our experiments, learning moduli $m \geq
2^{20}$ requires incorporating training data from smaller moduli, demonstrating
a critical necessity for curriculum learning. Finally, we analyze embedding
layers and uncover a novel clustering phenomenon: the model spontaneously
groups the integer inputs into bitwise rotationally-invariant clusters,
revealing how representations can transfer from smaller to larger moduli.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [137] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 将 SHAP 应用于棋局评估的分解，以棋子为特征进行局部可解释性分析


<details>
  <summary>Details</summary>
Motivation: 提升对高精度但黑箱化棋局评估的理解，揭示各棋子/模式对总评估的具体贡献，帮助教学、可视化和引擎比较

Method: 将棋子视为特征，进行系统性消融（逐个棋子及其组合），使用 SHAP 计算各棋子的贡献，确保局部忠实性并提供可解释的叙事与可视化；并结合教学化视角与引擎对比

Result: 得到对引擎输出的逐棋子贡献分解，形成易于理解的解释性评分，便于视觉化呈现、教学应用以及跨引擎比较；并提供代码和数据以便复现

Conclusion: 将经典棋艺评估思维与现代可解释性 AI 相结合，提升棋局分析的透明度，具备推广至教育场景和多引擎对比的潜力

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [138] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 提出一个两层框架ITI与CEP，解释压缩如何从生存压力出发，推动预测、生成模型和因果结构的发现，进而实现对现实的对齐，并给出可检验的预测，如压缩效率与泛化、异常积累区分因果/相关、分层系统的效率提升以及代谢成本与表征复杂度的关系。


<details>
  <summary>Details</summary>
Motivation: 现有以压缩为核心的智能框架，未充分解释为何压缩会促成因果结构的发现，而非仅捕捉统计模式。需要把生存压力、信息处理需求与因果结构发现连接起来，给出一个可检验的机制性解释。

Method: 提出信息理论层面的必然性ITI与压缩效率原则CEP，构建从生存压力到预测必要性、压缩需求、效率优化、生成结构发现直至现实对齐的因果链。基于物理、信息理论与进化约束，给出可在生物、人工和多尺度系统上检验的预测。

Result: 给出一系列可检验的预测：压缩效率（接近码率失真边界）与OOD泛化相关；异常积累速率能够区分因果与相关模型；分层系统在抽象层次增高时表现出更高的效率；生物系统的代谢成本与表征复杂度相关。这些预测提供了一个统一的解释框架，覆盖生物、人工和多尺度系统的收敛现象。

Conclusion: ITI与CEP共同构成一个从生存压力到现实对齐的因果链，表明智能是持续在结构化环境中不可避免的机械结果；这一框架强调信息论与进化约束的作用，避免对意识或主观体验的依赖。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [139] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 提出一个基于人格的评审聚合框架，通过对多名 rubric 条件化评审的输出进行聚合来对齐 LLM 判断的偏好。提供两种聚合实现（通用加性模型 GAM 与多层感知机 MLP），并通过对比与案例研究评估鲁棒性以及对偏见的敏感性。


<details>
  <summary>Details</summary>
Motivation: 解决将 LLM 评审对齐到人类偏好时的校准困难、刻板性、偏见与不稳定性问题，从而提升 RLHF 的奖励模型质量和面向用户的模型路由效果。

Method: 构建一个 persona（人格）基的偏好建模框架，通过学习将来自多名 rubric 条件评审的输出进行聚合，生成可扩展的偏好标签。聚合器提供两种实现：一是 Generalized Additive Model (GAM)，二是一个 Multi-Layer Perceptron (MLP)。

Result: 与朴素基线相比，该方法在整合多源偏好上的稳定性和鲁棒性有所提升；通过对人类和 LLM 评审偏见的案例研究，验证了框架对偏见的鲁棒性及适应性。

Conclusion: 提出可扩展的 persona 基于偏好合成标签的方法，并给出两种聚合实现，提升对多样化偏好和潜在偏见的处理能力。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [140] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0 提出一个面向科学应用的四维信任框架（真确性、对抗鲁棒性、科学安全、科学伦理），并配套开放式真确性与伦理学基准；在七种主流 LLM 上评估，通用模型普遍优于科学生态模型，但仍存在高风险领域的安全性问题；框架已开源。


<details>
  <summary>Details</summary>
Motivation: 在高风险科学场景中部署 LLM 以确保信任度仍存在显著担忧，需要一个全面、可操作的评估框架来量化和改进科学领域的模型可信性。

Method: 提出用于真确性的新型开放式基准，采用经过验证的反思调优流程和专家验证；设计覆盖八个子类别的科学研究伦理基准（包括双用研究、偏见等）；对七个颇具代表性的 LLM（四种科学生模型、三种通用行业模型）进行评估，使用准确度、语义相似性和基于 LLM 的评分等多项指标。

Result: 通用行业模型总体优于科学生模型，在各个信任维度上表现更好；GPT-o4-mini 在真确性评估与对抗鲁棒性方面表现突出；科学生模型在逻辑与伦理推理能力方面明显不足，对高风险领域（如生物安全与化学武器）中的安全性评估存在潜在脆弱性。

Conclusion: 通过开源本框架，为在科学场景中构建更可信的 AI 系统奠定基础，推动模型安全与伦理研究的发展。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [141] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: 3.8B 模型 Humans-Junior 在 FACTS-grounding 任务上达到 GPT-4o 水平的准确性，且成本显著更低。通过将最小化的导向推理支架（Exoskeleton Reasoning）与行为微调相结合，提升了推理合规性与一致性，产生协同效应。


<details>
  <summary>Details</summary>
Motivation: 降低小型语言模型在事实性推理中的成本与数据需求，并在云端与自托管/边缘部署下实现接近大型模型的表现，从而推动成本高效的可扩展AI。

Method: 在模型推理中加入最小化的 Exoskeleton Reasoning 导向推理支架，并进行行为层面的微调以强化协议遵循（认知纪律），强调对领域答案的真实掌握不如遵循制度性推理。单独微调效果有限，二者结合带来显著提升（约 +17.7pp、p<0.001）并降低方差约25%；在提示仅设置下的前沿模型实验也显示导向推理的有效性（如 GPT-4o +11.8pp、Gemini-2.5-Pro +5.0pp）。

Result: 在 Q1–Q500、同评判下，GPT-4o 的得分为 73.5%（95% CI 69.5–77.2），Humans-Junior 为 72.7%（95% CI 68.7–76.5），配对差为 0.8pp（自助法 95% CI -3.1 to +4.7；置换检验 p = 0.72，Cohen’s d = 0.023）。TOST 在 ±5pp 范围内建立等价性（在 ±3pp 时不成立）。云端定价中 Humans-Junior 的基础模型（Phi-3.5-mini-instruct）成本约是 GPT-4o 的 19× 左右；自托管或边缘部署可将边际推理成本降至近零。定价信息见附录 E。前沿模型的提示仅结果显示在 Q1–Q100 阶段的非可比情形下的改善（GPT-4o +11.8pp 至 85.3%、Gemini-2.5-Pro +5.0pp 至 93.3%，基线 88.3%，n=100）。

Conclusion: 3.8B 模型在规定评估下实现了与 GPT-4o 相当的 FACTS 精度（±5pp 内），并呈现显著的成本优势与部署灵活性，证明将 Exoskeleton Reasoning 与行为微调结合的策略是提升小型语言模型事实性 grounding 的有效路径。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [142] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 提出一种注意力感知的逆规划框架，用以从行为中推断注意偏见，并与标准逆强化学习对比；并在 Waymo 开源数据集中的真实驾驶场景中验证其可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解释人类目标导向行为受认知偏见影响的机制，以及使自主系统在与人类互动时能够识别并适应这些注意力偏差。

Method: 形式化注意力感知逆规划问题，展示其与标准逆强化学习的系统性差异；将深度强化学习与计算认知建模相结合，应用于从 Waymo Open Dataset 的现实驾驶场景中推断注意偏策略。

Result: 证明注意力感知逆规划在推断认知偏差方面与传统IRL存在系统性差异；成功从行为中推断出注意策略；并在真实驾驶场景中证明该方法的可扩展性。

Conclusion: 注意力感知逆规划是一个可行且可扩展的框架，用于从行为数据中推断注意偏策略，并将深度强化学习与认知建模结合应用于现实世界驾驶场景。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [143] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2 是一个多阶段的自动化学术综述生成系统，结合检索增强的合成、并行章节生成、迭代 refinement 与实时文献检索，以及多LLM评估框架，以提高结构一致性、主题相关性和引用准确性。


<details>
  <summary>Details</summary>
Motivation: 随着研究文献快速增长，完成全面、时效的综述变得越来越困难，需要一个可扩展、可复现且能融入最新文献的自动化解决方案。

Method: 提出多阶段管线：并行生成各章节、迭代改写、实时检索最新文献、并将检索、推理与评估整合成统一框架；采用多LLM评估覆盖、结构、相关性等维度，并强调引用保真性。

Result: 实验显示 autosurvey2 在结构连贯性与主题相关性等指标上优于现有基线，且保持高引用保真度，证明了在长篇综述自动化方面的优势。

Conclusion: 提供一个可扩展且可复现的自动化综述生成解决方案，并为自动化学术写作的未来研究奠定基础；相关代码与资源公开。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [144] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: StuckSolver 是一个在自动驾驶系统上方加载的 LLM 驱动的恢复模块，通过自我推理和/或乘客引导来解决车辆停滞场景，无需修改现有感知-规划-控制架构。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶在复杂交通场景中的停滞问题，避免依赖成本高昂的远程干预或排他性的手动接管，从而提高可用性和交通效率。

Method: 作为插件式模块，利用标准传感器数据检测停滞状态、解释环境上下文，并生成可被原生规划器执行的高层恢复命令；可独立进行自我推理，也可结合乘客引导；在 Bench2Drive 基准及自定义不确定性场景中进行评估。

Result: 在自我推理下达到近似最先进的性能，加入乘客引导后进一步提升，显示出较强的自适应能力和对现有架构的低侵入性。

Conclusion: LLM 驱动的恢复框架对提升 AV 在停滞情境下的鲁棒性和可用性具有潜在价值，适合作为现有系统的轻量化扩展。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [145] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: AI需要可问责性，否则难以对消费者、选民和决策者负责；本文/章节给出将问责性概念映射到AI的框架，区分可问责与不可问责的AI，并探讨提升可问责性的途径。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力快速提升，若要服务于公众利益，必须确保AI对其行动负责；只有具备问责机制的AI才能接受信息请求、讨论与制裁，从而提升信任与治理效果。

Method: 将问责性的通用定义映射到AI，举例说明什么样的AI是可问责的、不可问责的，并探讨可提升问责性的策略与路径。

Result: 提供了将问责性概念应用于AI的框架，明确了可问责与不可问责的区分，并提出提高AI可问责性的若干方法或方向。

Conclusion: 建立对AI的问责体系对实现对受影响人群负责的AI至关重要；需要通过制度、对话与制裁等机制推动全社会层面的可问责AI的实现，并继续探索相关方法。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [146] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: 提出 Lean4PHYS：一个基于 Lean4 的物理推理框架，包含 LeanPhysBench（200条手工设计、同行评审的题目）和 PhysLib（基础单位系统与公理集合）。基线模型表现有限，DeepSeek-Prover-V2-7B 仅 16%，Claude-Sonnet-4 为 35%；PhysLib 还能平均提升模型性能约 11.75%。首次给出 Lean4 物理学基准，展示了挑战性与 PhysLib 的有效性。


<details>
  <summary>Details</summary>
Motivation: 在 Lean4 中对物理学推理建立系统性、可量化的基准和资源仍稀缺。需要一个集中、可重复的基准来评估形式化推理能力，并为物理学教育材料与定理、单位系统的形式化整合提供平台。

Method: 构建 Lean4PHYS 框架，包括 LeanPhysBench（200条来自教材与竞赛题目、手工撰写且同行评审）、PhysLib（包含基本单位制和关键定理的社区驱动库），在此基础上对 Lean4 仓库进行实验，使用主要的专家级 Lean4 数学证明者以及先进的闭源模型进行基线评测，并分析 PhysLib 对模型性能的增益。

Result: 基线结果显示：DeepSeek-Prover-V2-7B 达到 16%，Claude-Sonnet-4 达到 35%。经过分析，PhysLib 的引入平均提升模型性能约 11.75%。显示 LeanPhysBench 具有挑战性，PhysLib 有助于提升推理效果。

Conclusion: 这是首个在 Lean4 上提供的物理学基准，证明了该领域的挑战性与资源的潜在价值，Lean4PHYS 为未来在 Lean4 的物理推理研究提供了重要基准与工具集。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [147] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 简要结论：提出“推理经济学”框架，将 LLM 推理视为以计算为驱动的生产活动，揭示推理成本的边际递减、规模递减收益以及最优性价比区间，基于 WiNEval-3.0 数据构建了“LLM 推理生产前沿”，为部署决策和推理资源定价/优化提供理论与实证基础。


<details>
  <summary>Details</summary>
Motivation: 在高昂的推理成本与资源约束背景下，建立一个经济学框架来量化成本、产出质量与规模效应，支持对推理资源的定价、调度与优化。

Method: 将推理过程视作以计算为驱动的智能生产活动，分析边际成本、规模经济和输出质量在不同性能配置下的表现；基于 WiNEval-3.0 的经验数据，构建“LLM 推理生产前沿”。

Result: 提出并验证三条原则：边际成本递减、规模收益递减，以及存在一个最优成本-效益区；形成“LLM 推理生产前沿”的概念。

Conclusion: 该框架为模型部署决策提供经济学依据，并为未来基于市场的推理资源定价与优化奠定实证基础。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [148] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出 Reasoning Curriculum：两阶段RL课程，在数学等域先提炼推理，然后通过跨域联合RL迁移巩固推理能力，提升多域推理表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在跨域推理任务中的泛化与推理能力不足问题；通过在可验证奖励的数学域先培养推理技能，再在混合域数据上进行联合RL以传递与巩固这些技能。

Method: 阶段1：冷启动后进行仅数学域的RL训练，使用可验证性奖励来发展推理能力。阶段2：在混合域数据上进行联合RL，使技能在多域间转移与巩固；不需要额外的专门奖励模型，仅依赖可验证性检查。模型基座不定，实验对象包括 Qwen3-4B 与 Llama-3.1-8B。

Result: 在多域评测中，Reasoning Curriculum 对两种模型均产生一致的性能提升；消融分析表明阶段1与阶段2都必要，数学优先的推理诱导提高了解题所需的认知行为。

Conclusion: 提供一个简洁、易于采纳的通用推理训练流程，具备跨模型与跨域的可迁移性。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [149] [The FM Agent](https://arxiv.org/abs/2510.26144)
*Annan Li,Chufan Wu,Zengle Ge,Yee Hin Chong,Zhinan Hou,Lizhe Cao,Cheng Ju,Jianmin Wu,Huaiming Li,Haobo Zhang,Shenghao Feng,Mo Zhao,Fengzhi Qiu,Rui Yang,Mengmeng Zhang,Wenyi Zhu,Yingying Sun,Quan Sun,Shunhao Yan,Danyu Liu,Dawei Yin,Dou Shen*

Main category: cs.AI

TL;DR: FM Agent is a general multi-agent framework that combines LLM-based reasoning with large-scale evolutionary search. It features a cold-start initialization guided by experts, an evolutionary sampling strategy for iterative optimization, domain-specific evaluators with LLM supervision, and a distributed asynchronous runtime on Ray. It achieves state-of-the-art results across several benchmarks and domains, including operations research, machine learning, GPU kernel optimization, and classical math, autonomously without human interpretation or tuning.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous AI research agents capable of exploring complex search spaces across diverse domains, leveraging expert knowledge at initialization, scalable search strategies, and robust evaluation pipelines to accelerate discovery and engineering.

Method: Architectural components include (1) cold-start initialization with expert guidance, (2) an evolutionary sampling strategy for iterative optimization, (3) domain-specific evaluators combining correctness, effectiveness, and LLM-supervised feedback, and (4) a distributed asynchronous execution infrastructure built on Ray. The framework integrates LLM-based reasoning with evolutionary search to drive autonomous exploration and optimization across heterogeneous tasks.

Result: FM Agent achieves state-of-the-art results on ALE-Bench (1976.3, +5.2%), MLE-Bench (43.56%, +4.0pp), and up to 20x speedups on KernelBench, as well as new SOTA results on several classical mathematical problems, all without human interpretation or tuning.

Conclusion: Demonstrates broad applicability to both large-scale enterprise R&D workflows and fundamental scientific research, with potential to accelerate innovation, automate complex discovery processes, and deliver significant engineering and scientific advances.

Abstract: Large language models (LLMs) are catalyzing the development of autonomous AI
research agents for scientific and engineering discovery. We present FM Agent,
a novel and general-purpose multi-agent framework that leverages a synergistic
combination of LLM-based reasoning and large-scale evolutionary search to
address complex real-world challenges. The core of FM Agent integrates several
key innovations: 1) a cold-start initialization phase incorporating expert
guidance, 2) a novel evolutionary sampling strategy for iterative optimization,
3) domain-specific evaluators that combine correctness, effectiveness, and
LLM-supervised feedback, and 4) a distributed, asynchronous execution
infrastructure built on Ray. Demonstrating broad applicability, our system has
been evaluated across diverse domains, including operations research, machine
learning, GPU kernel optimization, and classical mathematical problems. FM
Agent reaches state-of-the-art results autonomously, without human
interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\%), 43.56\% on MLE-Bench
(+4.0pp), up to 20x speedups on KernelBench, and establishes new
state-of-the-art(SOTA) results on several classical mathematical problems.
Beyond academic benchmarks, FM Agent shows considerable promise for both
large-scale enterprise R\&D workflows and fundamental scientific research,
where it can accelerate innovation, automate complex discovery processes, and
deliver substantial engineering and scientific advances with broader societal
impact.

</details>


### [150] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: QASU 是一个面向问卷分析的基准，评估六种结构技能在六种序列化格式和多种提示策略下的表现，旨在提高 LLM 对问卷数据的处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的问卷分析工具多为人类工作流设计，难以与大型语言模型高效整合；在包含问题列表与大量受访者行的数据上，LLM 的潜力尚未被充分挖掘。

Method: 提出 QASU 基准，覆盖六种结构技能（如答案查找、受访者计数、多跳推理等）、六种序列化格式，以及多种提示策略；对主流 LLM 进行系统实验，比较格式/提示对准确性的影响，并通过自增提示引入轻量化结构提示以进一步提升性能。

Result: 选择合适的格式和提示组合可将准确度提升至相对最低基线的最大约 8.8 个点；在某些任务上，加入自增的轻量结构提示可再提升 3–4 个点；基准具备开源性，适用于研究与实际应用的结合。

Conclusion: 通过系统分离格式与提示对性能的影响，QASU 为基于问卷数据的 LLM 分析提供一个简单且通用的开源基准，促进研究与现实应用的发展。

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [151] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 提出 IPA-UCT，在 Monte Carlo Tree Search 中通过弱化的状态抽象条件来提高抽样效率，解决在嘈杂/大动作空间中难以发现状态抽象的问题；IPA-UCT 相对于 OGA-UCT 及其变体在广泛的测试域和预算下表现更好；并给出一个从 IPA 到 ASAP 再到 p-ASAP 的层级框架，指出 IPA 与 ASAP 均是更一般框架 ASASAP 的特例。


<details>
  <summary>Details</summary>
Motivation: 提升蒙特卡罗树搜索的样本效率，通过对状态-动作对进行分组与统计共享实现抽象；然而在嘈杂或大动作空间中，单纯的状态抽象往往受限难以发现。提出在保持可控准确性的前提下放宽状态抽象条件，以发现更多有用的抽象并提升性能。

Method: 提出 IPA-UCT，其核心是一个弱化的状态抽象条件，允许在更宽的条件下建立状态分组并共享统计。将 IPA 与 ASAP 的框架进行对比，指出 IPA/ASAP 是 p-ASAP 的特例，p-ASAP 又是 ASASAP 的特例。通过理论分析和大量实验验证，比较了 IPA-UCT 与 OGA-UCT 及其衍生方法在多领域和不同迭代预算下的表现。

Result: 实验结果表明，IPA-UCT 在广泛的测试域和预算条件下优于 OGA-UCT 及其派生方法。研究还揭示了 IPA 和 ASAP 是更一般框架 p-ASAP/ASASAP 的特例，提供了一个统一的理论框架来理解不同的状态/状态-动作抽象。

Conclusion: 放宽状态抽象条件可以显著提高找出有效抽象的能力，从而提升 MCTS 的样本效率；同时提出的 p-ASAP/ASASAP 框架为未来在不同抽象尺度下的算法设计提供了一条统一路径，将状态抽象和状态-动作抽象的关系进行系统化。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [152] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: 提出 BOTS，基于贝叶斯在线任务选择的框架，在 RFT 中通过 Thompson 采样自适应分配任务，结合显式与隐式证据并以超轻量插值估算未评估任务难度，提升数据效率与性能。


<details>
  <summary>Details</summary>
Motivation: LLM 强化微调对所选训练任务高度敏感；均匀采样效率低且可能浪费计算；现有任务选择方法要么代价高、要么自适应性不足、要么证据不完整。

Method: 构建贝叶斯后验来跟踪任务难度随模型演化的变化；结合显式证据（对选中任务的直接评估）与隐式证据（基于这些评估对未选任务的推断），采用 Thompson 采样实现探索-利用平衡；实现中用极轻量的插值插件来估算未评估任务的难度，避免额外回合开销。

Result: 在多领域与不同规模的 LLM 上，BOTS 显著提升数据效率和性能，相较基线与消融模型表现更优，展示了动态任务选择在 RFT 中的实用性与扩展性。

Conclusion: 贝叶斯在线任务选择能够提升 RFT 的样本利用率与模型性能，具有广泛适用性和可落地性。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [153] [AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 将 AI Mathematician (AIM) 作为研究伙伴，与人类协同进行数学发现，利用分解子目标、选择分析方法与结果校验，最终得到可验证的完整证明，体现人机共推理在数学研究中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管 AI 在数学推理取得进展，但将其嵌入实际数学研究仍有限。本文探讨 AIM 如何不仅仅是解题工具，而是研究合作者，通过人类干预来结构化发现过程。

Method: 聚焦同质化理论中的一个难题，分析 AIM 的自发推理轨迹，进行有针对性的人为干预以引导发现；通过把问题逐步分解为可处理的子目标、选择合适的分析方法、对中间结果进行验证的迭代过程，揭示人机互补的工作方式。

Result: 该协作范式提高了推理的可靠性、透明度与可解释性，保持人类对正式严谨性的监督；实现了一个完整且可验证的证明。

Conclusion: 本文示范了系统性的人机共推理可以推进数学发现的前沿，具有广泛适用性，可推广至其他领域的高层次数学研究。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [154] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 提出基于任务项本身属性的子集选择（Scales++）以进行高效的LLM基准评估；通过以认知负担为准的数据筛选，在极小子集下仍能保持竞争性保真度，同时改善冷启动和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前基准评估多采用以模型为中心的子集选择，成本高且对新模型的迁移性差，且假设未来模型的失败模式与前代相似。需要利用任务项的固有特性来选取子集，以实现更低成本且对未来模型更稳健的评估。

Method: 提出并实现 Scales++，基于基准样本的认知负担等任务项内在属性进行数据筛选；在 Open LLM Leaderboard 上进行评估，展示极小子集（0.5%）下的预测能力。

Result:  upfront 选择成本降低约18倍，保持竞争性保真度；使用0.5%数据子集即可实现对完整基准分数的预测，平均绝对误差为2.9%；具备更好的冷启动性能和更易解释的基准分析。

Conclusion: 基于任务项本身属性的子集选择（item-centric）能够在不显著损害评估保真度的前提下提升基准评估的效率，并且对冷启动和解释性有显著改进。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [155] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+ 将自动评分从单纯的总结性评估转变为形成性学习体验，通过两大能力实现：一是利用微调的大语言模型生成自动化反馈，二是对学生代码提交进行可视化，揭示学习模式。通过对学生代码和专家反馈的精细调优，确保反馈具有教学相关性和上下文感知性。


<details>
  <summary>Details</summary>
Motivation: 随着编程教育的快速增长，传统评估工具难以提供可扩展且有意义的反馈，教师工作量与反馈质量之间的矛盾日益突出。现有的自动评分系统多为黑箱式的通过/未通过，难以揭示学生思路与学习需求。因此需要一个能提供形成性、可解释且可扩展的评估与反馈机制。

Method: 在大规模评测数据上进行微调：将模型在精选的学生代码与教师/专家反馈上进行微调，使其输出具有教育性、情境相关的反馈。同时训练对比学习的代码嵌入，基于1000份带标注的提交，获得能够根据功能和实现思路将解答聚类的表征。系统还支持提示模板的集合（prompt-pooling），由教师选择不同模板来引导反馈风格。评估对象为来自多个编程任务的600份提交，涵盖反馈质量、聚类效果与教师工作量的潜在影响。

Result: 系统生成的反馈在语义层面与教师评注高度对齐，显示出良好的教学相关性。基于1000份带标注提交训练的对比学习嵌入能够将解答在功能与实现思路层面分成具有意义的簇。提示池化机制使教师能够通过选取模板来引导反馈风格。总体上，Autograder+ 在降低教师工作负担的同时，支持有针对性的教学干预并提升学习效果。

Conclusion: 将自动评分从单纯的结果评估转变为形成性、可解释的学习辅助工具。通过AI驱动的反馈、语义聚类和交互式可视化，Autograder+ 实现了规模化、针对性强的教学支持，帮助教师更高效地实现差异化教学并提升学习成果。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [156] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 通过将Medical Sparse Autoencoders (MedSAEs) 应用于 MedCLIP 潜在空间，提升胸部X光影像的可解释性；在 CheXpert 数据集上通过相关性、熵分析和自动神经元命名等评估框架显示 MedSAE 神经元具更高的单语义性和可解释性；为临床高性能医学AI的可解释性提供可扩展路径。


<details>
  <summary>Details</summary>
Motivation: 医疗AI 需要在高准确性与透明性之间取得平衡。尽管 MedCLIP 等视觉-语言模型在放射影像上表现出色，其潜在特征往往缺乏可解释性。本文通过在潜在空间引入稀疏自编码器来实现机械式可解释性，提高神经元的单语义性。

Method: 把 MedSAE 应用于 MedCLIP 的潜在表示，获得更具单一语义的神经元；利用 MedGEMMA 基金会模型进行自动命名标注；提出结合相关性、信息熵分析的可解释性评估框架；在 CheXpert 数据集上比较 MedSAE 与原生 MedCLIP 特征。

Result: 实验结果显示 MedSAE 神经元在单语义性与可解释性方面优于原始 MedCLIP 表现，所提出的评估框架支持更高的可解释性。

Conclusion: 该方法为高性能医学AI提供一条可扩展的可解释性途径，推动临床可靠的表示学习。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [157] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本摘要讨论情境工程的概念、历史阶段及对AI系统设计的影响，提出以系统化的方法构建上下文以提升人机协作。


<details>
  <summary>Details</summary>
Motivation: 探讨将人—机器交互扩展到更广泛的情境中，提出情境工程作为AI系统设计的基础框架。

Method: 对20多年的研究脉络进行回顾，界定概念、历史阶段和设计要点，提出未来研究方向。

Result: 为情境工程提供概念基础和实践要点，强调分阶段的智能水平与情境嵌入的重要性。

Conclusion: 呼吁建立更广泛的社区努力，推动AI系统中的情境工程的体系化发展。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [158] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: 结合AI评分和人类评分（基于AI评估者的置信度）比单独使用任一者更能提升对AI输出的事实核验准确性；提供AI事实核验助手可进一步提升人类准确性，但助手的类型很关键。显示AI解释、置信度和标签会引发过度依赖，而仅显示搜索结果与证据更有助于建立恰当的信任。这对放大监督（Amplified Oversight）具有重要启示。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升与应用场景扩展，确保质量与安全的挑战增大；需要高效的人机协作来监督和纠正AI输出，尤其在事实核验等关键任务中。

Method: 通过将AI评分与人类评分按AI评估者置信度进行加权组合，评估其对事实核验任务的性能；引入AI事实核验助手以辅助人类评估，并比较不同类型的助手界面（如显示解释、置信度、标签等）与仅显示搜索结果和证据的效果。

Result: AI置信度加权的AI+人类混合评分优于单独来源；AI助手提升人工核验准确性；但助手类型影响效果：提供解释/置信度/标签容易导致过度信任，单独展示搜索结果与证据则更易维持恰当信任。

Conclusion: 此研究支持在超越单一能力的情境下推进 Amplified Oversight，但需要谨慎设计辅助信息，平衡人机信任与依赖，确保人类核验在AI强大时仍能发挥关键作用。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [159] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 本文系统评估大语言模型在规范性推理中的表现，比较规范模态与 epistemic 模态下的推理能力，给出数据集、发现偏差与提升方向。


<details>
  <summary>Details</summary>
Motivation: 规范性推理涉及义务、许可等德性模态，尽管LLMs在多任务推理上表现突出，但在规范域的能力仍未被充分研究。通过对比规范模态与 epistemic 模态的推理，揭示模型在此类推理中的稳定性与局限。

Method: 构建覆盖规范性与认知相关因素的形式化推理数据集，比较不同模态下的推理模式；评估多种LLMs在这些模式上的表现；分析是否存在类似人类的认知偏差；公开数据与代码以便复现。

Result: LLMs在大多数有效推理模式下表现符合逻辑，但在某些规范性推理类型上显现显著不一致，且呈现出与心理学研究中人类推理相似的认知偏差；存在提升逻辑一致性与鲁棒性的挑战。

Conclusion: 提出改进方向以提升LLMs在规范性推理中的可靠性，强调需考虑非形式认知因素与统计偏误等因素；同时通过公开数据集与代码推动后续研究和评估。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [160] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: 多模态大语言模型对文本输入表现出显著偏好，原因不仅来自数据因素，还源于内部注意力键空间的结构性错位。将视觉键（Visual Keys）与文本键在语言预训练阶段学习的文本键空间相比，存在分布上的显著出界（OOD）现象，导致视觉信息在注意力中的相似度被系统性压低，未被充分利用。通过对 LLaVA 与 Qwen2.5-VL 的键向量进行 t-SNE 及 Jensen–Shannon 散度分析，发现视觉键和文本键占据明显不同的子空间，跨模态的分歧显著大于模态内的变异。


<details>
  <summary>Details</summary>
Motivation: 揭示 MLLM 在处理 vision-language 数据时对文本输入的偏好是否来自模型外部因素（如数据不平衡或指令微调），并提出潜在的内部架构原因，以解释视觉证据的推理能力受限。

Method: 从 LLaVA 与 Qwen2.5-VL 提取注意力的键向量，使用定性（t-SNE）和定量（Jensen–Shannon 散度）方法分析它们的分布结构，比较跨模态与同模态之间的分布差异与相似性。

Result: 视觉键与文本键在注意力空间中落在不同的子空间，跨模态分歧显著高于模态内分歧，且统计上显著，支持“键空间错位”是文本偏好产生的直接证据。

Conclusion: 文本偏好源于注意力键空间的内在错位，而非单纯的数据因素；这一发现提示需要在模型架构层面调整视觉与文本键的表征与对齐，以提升对视觉证据的推理能力。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [161] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 提出一种跨平台的推理能力评估框架，在三种计算范式（HPC超级计算、云平台、大学集群）下，对15种基础模型进行79道题目、覆盖8个学科的评估。分三个阶段进行：基线建立、基础设施验证、扩展评估。研究结果挑战了“规模越大越强”的观点，认为训练数据质量比模型规模更关键，并给出跨教育、生产、研究场景的模型选择建议。该三平台方法学与79题基准可用于追踪随模型进化的推理能力。


<details>
  <summary>Details</summary>
Motivation: 在多样化基础设施中系统性地评估现代基础模型的推理能力，验证规模与数据质量的关系，建立一个基础设施无关且可重复的基准，以便横向对比和纵向追踪。

Method: 分三个实验阶段：1) 基线建立：在 MareNostrum 5 上用 6 个模型对 19 道题进行评估，确立方法学与参考性能；2) 基础设施验证：在大学集群和 Nebius 平台重复上述 19 道题的评估，增加若干模型，验证可重复性与无关平台性；3) 扩展评估：在大学集群与 Nebius 平台对全部 79 道题进行评估，考察跨体系的泛化和架构多样性下的表现。

Result: 研究结果挑战常规的“规模—性能”假设，表明训练数据质量比模型大小更关键，并给出在教育、生产和研究场景中的模型选择可执行指南。提出的三平台方法学和 79 道题基准可用于对推理能力进行长期跟踪，促进基础模型的进化与跨平台比较。

Conclusion: 三平台方法学和 79 道题基准为对推理能力的纵向追踪提供了稳定的基线，支持在不同架构与计算环境下进行可重复评估，便于未来对新模型进行横向对比与趋势分析。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [162] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: LLMs学习到一种紧凑的、可解释的过滤操作的因果表征，少量“过滤头”能够在查询状态中编码谓词，并具备跨集合、格式、语言及任务的可移植性；模型也可能通过直接在项上打标来提前评估谓词。总体上显示了可解释、可泛化的抽象计算实现，类似传统函数式编程的过滤模式。


<details>
  <summary>Details</summary>
Motivation: 揭示大语言模型在列表处理任务中的内部机制，检验是否存在可移植且可解释的高层计算模式，与函数式编程中的过滤操作之间的对应关系。

Method: 对多种列表处理任务应用因果中介分析，定位并分析少量注意力头（称为过滤头）及其在查询状态中的谓词编码，考察谓词表示的可移植性与泛化性，并比较是否存在提前对项进行谓词满足性评估并在项上存储标志的策略。

Result: 发现存在少量过滤头编码谓词的紧凑表征，该谓词可从一个集合迁移到不同格式/语言/任务的集合；在某些情形下，模型也会通过提前评估谓词并在项表示中存储标志来实现过滤；整体揭示LLMs能够产生可解释且具泛化性的抽象计算实现，类似函数式编程中的模式。

Conclusion: LLMs具备开发可解释、可泛化的抽象计算实现的能力，其过滤操作可以通过可迁移的谓词表征来支撑，并可能存在多种实现策略，体现出与函数式编程的显著类比。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>
