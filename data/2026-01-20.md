<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 46]
- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLMs for Game Theory: Entropy-Guided In-Context Learning and Adaptive CoT Reasoning](https://arxiv.org/abs/2601.10775)
*Tommaso Felice Banfi,Sashenka Gamage*

Main category: cs.CL

TL;DR: 提出一种基于大语言模型的自适应推理框架，用于离散博弈任务（以Tic-Tac-Toe为例），通过熵引导的链式推理与自适应检索实现上下文自适应。与基线LLM相比，在100局对局中平均棋局结果从-11.6%提升到+9.5%，且查询次数保持较低，结果统计显著，且词元级熵与走子最优性呈负相关。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在序列决策环境中的不确定性对推理质量的影响。通过将不确定性引入到检索规模和推理路径的自适应调整，提升在博弈等离散任务中的决策质量。

Method: 将 in-context learning 与基于熵的链式推理（CoT）结合，并引入自适应上下文检索。根据令牌级不确定性动态调整检索示例数量和推理路径数量：低不确定性使用简洁推理和最小上下文；高不确定性扩展多路径CoT探索。以Tic-Tac-Toe为示例，比较对手为次优算法，评估100局对局的胜负结果并进行统计显证和相关性分析。

Result: 在100局对局中，采用熵引导自适应推理的平均结果从 defeats baseline 的-11.6% 提升至 +9.5%，对局结果按胜/平/负计+1/0/-1。统计检验显示改进显著；令牌级熵与走子最优性之间存在负相关关系。

Conclusion: 不确定性引导的自适应推理能够显著提升LLM在序列决策任务中的表现，所提出的方法在博弈等离散任务中具有有效性与潜在的泛化性。

Abstract: We propose a novel LLM-based framework for reasoning in discrete, game-theoretic tasks, illustrated with \emph{Tic-Tac-Toe}. The method integrates in-context learning with entropy-guided chain-of-thought (CoT) reasoning and adaptive context retrieval. The model dynamically adjusts both the number of retrieved examples and reasoning paths according to token-level uncertainty: concise reasoning with minimal context is used when uncertainty is low, whereas higher uncertainty triggers expanded multi-path CoT exploration. Experimental evaluation against a sub-optimal algorithmic opponent shows that entropy-aware adaptive reasoning substantially improves decision quality, increasing the average game outcome from \(-11.6\%\) with the baseline LLM to \(+9.5\%\) with entropy-guided adaptive reasoning over 100 games (win = +1, tie = 0, loss = -1), while maintaining a relatively low number of LLM queries per game. Statistical validation confirms that the improvement is significant, and correlation analysis reveals a negative association between token-level entropy and move optimality. These findings demonstrate that uncertainty-guided adaptive reasoning effectively enhances LLM performance in sequential decision-making environments.

</details>


### [2] [BYOL: Bring Your Own Language Into LLMs](https://arxiv.org/abs/2601.10804)
*Syed Waqas Zamir,Wassim Hamidouche,Boulbaba Ben Amor,Luana Marotti,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: BYOL 提出一个统一的语言感知大语言模型开发框架，按语言资源级别分为 Extreme-Low/Low/Mid/High 四层路径。对低资源语言采用全栈数据 refining、合成文本、持续预训练和监督微调等组合，提升在 Chichewa、Maori 的平均性能约比强基线提升 12%，并通过权重空间合并保持英语/多语言能力。对极低资源语言采用翻译驱动的参与路径，在 Inuktitut 上把机器翻译系统的 BLEU 提升 4 分，便于在无法直接模型化时获取高质量 LLM 入口。公开了以 Chichewa、Maori、Inuktitut 为母语的 Global MMLU-Lite 基准与代码/模型。


<details>
  <summary>Details</summary>
Motivation: 全球语言资源严重不均衡，7,000 多语言中仅少数具备足够数字化触点来驱动现代 LLM 的训练，导致低资源语言的性能不足、文化不对齐和可及性受限。需要一个可扩展、面向语言特征的端到端框架，以充分利用 Language footprint 的差异化信息。

Method: 建立语言资源分类（Extreme-Low/Low/Mid/High），据此选择不同集成路径。对 Low 资源语言，实行全栈数据 refinement 与扩展管线：语料清洗、合成文本生成、持续预训练、监督微调。对于 Extreme-Low，提出翻译中介的包容路径，并通过自适应的权重空间模型合并来保留英语/多语言能力。发布翻译后的 Global MMLU-Lite 基准并给出开源实现与模型。

Result: 在 Chichewa 与 Maori 上，所提管线得到约 12% 的平均提升，覆盖 12 个基准。对 Inuktitut，定制的机器翻译系统较商用基线 BLEU 提升约 4 分，使在直接语言建模不可行时也能获得高质量的 LLM 入口。 combines 语言特定模型与多语言能力，公开基准与代码。

Conclusion: BYOL 提供一个可扩展、语言感知的 LLM 发展路径，能在不同资源等级的语言上取得显著收益，翻译驱动的入口提供极低资源场景的实用性，同时通过代码与数据集的公开促进研究复现与进一步改进。

Abstract: Large Language Models (LLMs) exhibit strong multilingual capabilities, yet remain fundamentally constrained by the severe imbalance in global language resources. While over 7,000 languages are spoken worldwide, only a small subset (fewer than 100) has sufficient digital presence to meaningfully influence modern LLM training. This disparity leads to systematic underperformance, cultural misalignment, and limited accessibility for speakers of low-resource and extreme-low-resource languages. To address this gap, we introduce Bring Your Own Language (BYOL), a unified framework for scalable, language-aware LLM development tailored to each language's digital footprint. BYOL begins with a language resource classification that maps languages into four tiers (Extreme-Low, Low, Mid, High) using curated web-scale corpora, and uses this classification to select the appropriate integration pathway. For low-resource languages, we propose a full-stack data refinement and expansion pipeline that combines corpus cleaning, synthetic text generation, continual pretraining, and supervised finetuning. Applied to Chichewa and Maori, this pipeline yields language-specific LLMs that achieve approximately 12 percent average improvement over strong multilingual baselines across 12 benchmarks, while preserving English and multilingual capabilities via weight-space model merging. For extreme-low-resource languages, we introduce a translation-mediated inclusion pathway, and show on Inuktitut that a tailored machine translation system improves over a commercial baseline by 4 BLEU, enabling high-accuracy LLM access when direct language modeling is infeasible. Finally, we release human-translated versions of the Global MMLU-Lite benchmark in Chichewa, Maori, and Inuktitut, and make our codebase and models publicly available at https://github.com/microsoft/byol .

</details>


### [3] [A Concise Agent is Less Expert: Revealing Side Effects of Using Style Features on Conversational Agents](https://arxiv.org/abs/2601.10809)
*Young-Min Cho,Yuan Yuan,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.CL

TL;DR: Prompting for one stylistic feature in LLM conversations yields cross-feature side effects; style features are entangled, not orthogonal; introduces CASSE dataset and evaluates mitigation strategies; highlights need for multi-objective, principled stylistic control.


<details>
  <summary>Details</summary>
Motivation: Despite widespread use of prompt-based style features to steer LLM behavior, little is known about unintended cross-feature interactions and the reliability of style control. This work systematically studies cross-feature stylistic side effects to inform safer, more predictable dialogue systems.

Method: Survey of 127 conversational-agent papers from ACL Anthology to identify 12 common style features; controlled synthetic dialogues covering task-oriented and open-domain settings; pairwise LLM judge framework to quantify causal effects between style features; introduction of CASSE dataset; evaluation of prompt-based and activation-steering mitigation strategies.

Result: Finding consistent and structured side effects; for example, prompting for conciseness reduces perceived expertise; style features are deeply entangled rather than orthogonal; mitigation approaches can partially restore suppressed traits but often degrade the primary intended style.

Conclusion: Stylish control in LLMs is not faithful or orthogonal; future work should embrace multi-objective and principled approaches for safe, targeted stylistic steering; CASSE provides a resource for studying these interactions and guiding mitigation research.

Abstract: Style features such as friendly, helpful, or concise are widely used in prompts to steer the behavior of Large Language Model (LLM) conversational agents, yet their unintended side effects remain poorly understood. In this work, we present the first systematic study of cross-feature stylistic side effects. We conduct a comprehensive survey of 127 conversational agent papers from ACL Anthology and identify 12 frequently used style features. Using controlled, synthetic dialogues across task-oriented and open domain settings, we quantify how prompting for one style feature causally affects others via a pairwise LLM as a Judge evaluation framework. Our results reveal consistent and structured side effects, such as prompting for conciseness significantly reduces perceived expertise. They demonstrate that style features are deeply entangled rather than orthogonal. To support future research, we introduce CASSE (Conversational Agent Stylistic Side Effects), a dataset capturing these complex interactions. We further evaluate prompt based and activation steering based mitigation strategies and find that while they can partially restore suppressed traits, they often degrade the primary intended style. These findings challenge the assumption of faithful style control in LLMs and highlight the need for multi-objective and more principled approaches to safe, targeted stylistic steering in conversational agents.

</details>


### [4] [EncodeRec: An Embedding Backbone for Recommendation Systems](https://arxiv.org/abs/2601.10837)
*Guy Hadad,Neomi Rabaev,Bracha Shapira*

Main category: cs.CL

TL;DR: EncodeRec adapts PLM-derived embeddings for recommendation by freezing PLM parameters and learning compact, domain-specific representations from item descriptions, yielding improvements over PLM-based and embedding baselines.


<details>
  <summary>Details</summary>
Motivation: PLMs provide generic semantic embeddings but are not optimized for structured, discriminative spaces needed in recommender systems, and their representations often miss domain-specific semantics.

Method: Freeze the language model during recommender training and learn compact item-description–based embeddings tailored to recommendation objectives, enabling efficient semantic ID tokenization and serving as a backbone for sequential models.

Result: Experiments on core benchmarks show substantial gains over PLM-based and embedding baselines, validating the effectiveness of embedding adaptation for recommender systems.

Conclusion: Adapting embeddings from PLMs is crucial to bridge general-purpose language models with practical recommender tasks, achieving efficient, semantically faithful representations.

Abstract: Recent recommender systems increasingly leverage embeddings from large pre-trained language models (PLMs). However, such embeddings exhibit two key limitations: (1) PLMs are not explicitly optimized to produce structured and discriminative embedding spaces, and (2) their representations remain overly generic, often failing to capture the domain-specific semantics crucial for recommendation tasks. We present EncodeRec, an approach designed to align textual representations with recommendation objectives while learning compact, informative embeddings directly from item descriptions. EncodeRec keeps the language model parameters frozen during recommender system training, making it computationally efficient without sacrificing semantic fidelity. Experiments across core recommendation benchmarks demonstrate its effectiveness both as a backbone for sequential recommendation models and for semantic ID tokenization, showing substantial gains over PLM-based and embedding model baselines. These results underscore the pivotal role of embedding adaptation in bridging the gap between general-purpose language models and practical recommender systems.

</details>


### [5] [DialDefer: A Framework for Detecting and Mitigating LLM Dialogic Deference](https://arxiv.org/abs/2601.10896)
*Parisa Rabbani,Priyam Sahoo,Ruben Mathew,Aishee Mondal,Harshita Ketharaman,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: LLMs对同一内容在不同对话框 framing 下的判断高度敏感；提出 DialDefer 框架与 Dialogic Deference Score (DDS) 来检测并缓解此类框架引起的判断偏置； attribution(人类对比LLM) 驱动的最大偏移；缓解措施虽有效但易过度校正为怀疑，需超越单纯准确性优化的校准。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs作为对话第三方评判者的可靠性，研究框架效应（framing）对判断的一致性与偏差；填补仅看准确率而忽略判断方向性偏移的研究空白。

Method: 提出 DialDefer 框架用于检测和缓解框架诱发的判断偏移，定义 Dialogic Deference Score (DDS) 来量化方向性变化。对九个领域、3000+ 实例、四个模型进行对比实验，比较两种表述方式：将内容作为“陈述待核实”与“指称说话者的陈述待核实”；进行消融分析以考察人类与LLM attribution 的影响，并在自然场景 Reddit 对话中评估效果。

Result: DDS 表现出显著的方向性偏移，幅度可达 ±87 百分点，显著性 p<0.0001，而准确率变化 <2 百分点；在 Reddit 等自然对话中偏移放大 2–4 倍；偏移在不同领域呈现方向性差异（如 graduate-level science DDS -53，social judgement DDS +58）； attribution（人类 vs LLM）是最大的推动因素，约 17.7pp 的波动；缓解策略可减少偏移但可能引发过度怀疑，需要对齐到更全面的校准目标。

Conclusion: 对话评估中的框架效应显著且领域相关，单纯追求更高准确率无法解决方向性偏差。DialDefer 提供对齐与缓解机制，但需谨慎校准以避免新的偏置。

Abstract: LLMs are increasingly used as third-party judges, yet their reliability when evaluating speakers in dialogue remains poorly understood. We show that LLMs judge identical claims differently depending on framing: the same content elicits different verdicts when presented as a statement to verify ("Is this statement correct?") versus attributed to a speaker ("Is this speaker correct?"). We call this dialogic deference and introduce DialDefer, a framework for detecting and mitigating these framing-induced judgment shifts. Our Dialogic Deference Score (DDS) captures directional shifts that aggregate accuracy obscures. Across nine domains, 3k+ instances, and four models, conversational framing induces large shifts (|DDS| up to 87pp, p < .0001) while accuracy remains stable (<2pp), with effects amplifying 2-4x on naturalistic Reddit conversations. Models can shift toward agreement (deference) or disagreement (skepticism) depending on domain -- the same model ranges from DDS = -53 on graduate-level science to +58 on social judgment. Ablations reveal that human-vs-LLM attribution drives the largest shifts (17.7pp swing), suggesting models treat disagreement with humans as more costly than with AI. Mitigation attempts reduce deference but can over-correct into skepticism, framing this as a calibration problem beyond accuracy optimization.

</details>


### [6] [Massively Multilingual Joint Segmentation and Glossing](https://arxiv.org/abs/2601.10925)
*Michael Ginn,Lindia Tjuatja,Enora Rice,Ali Marashian,Maria Valentini,Jasmine Xu,Graham Neubig,Alexis Palmer*

Main category: cs.CL

TL;DR: 本研究提出 PolyGloss，一种联合预测互行注释（interlinear gloss）与形态分割的序列到序列模型家族。通过扩充 GlossLM 的训练语料并预训练 PolyGloss，提升了注释质量、分割和对齐的综合表现，并能通过低秩适配快速适应新数据集，优于 GlossLM 与多种开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型通常输出对单位词的语素级注释而不预测实际的语素边界，导致可解释性差和可信度低。本研究旨在通过联合建模实现对注释与形态分割的端到端预测，提升可解释性、对齐性与在现实语言文献工作中的实用性。

Method: 扩充 GlossLM 的训练语料，预训练 PolyGloss —— 一系列用于联合分割与注释的多语言序列到序列模型。通过平衡分割与注释的准确性以及两者之间的对齐，进行多语言评估。并评估对新数据集的快速适配能力，采用低秩适配（LoRA）等方法进行微调。

Result: PolyGloss 在注释（glossing）任务上超越 GlossLM，在分割、注释和对齐等任务上也优于多种开源大模型。并且能够通过低秩适配实现对新数据集的快速适配。

Conclusion: 联合建模分割与注释可显著提升可解释性和整体性能，PolyGloss 在多语言环境下具有较强的泛化与适应性，低秩适配提供了灵活的迁移路径。

Abstract: Automated interlinear gloss prediction with neural networks is a promising approach to accelerate language documentation efforts. However, while state-of-the-art models like GlossLM achieve high scores on glossing benchmarks, user studies with linguists have found critical barriers to the usefulness of such models in real-world scenarios. In particular, existing models typically generate morpheme-level glosses but assign them to whole words without predicting the actual morpheme boundaries, making the predictions less interpretable and thus untrustworthy to human annotators.
  We conduct the first study on neural models that jointly predict interlinear glosses and the corresponding morphological segmentation from raw text. We run experiments to determine the optimal way to train models that balance segmentation and glossing accuracy, as well as the alignment between the two tasks. We extend the training corpus of GlossLM and pretrain PolyGloss, a family of seq2seq multilingual models for joint segmentation and glossing that outperforms GlossLM on glossing and beats various open-source LLMs on segmentation, glossing, and alignment. In addition, we demonstrate that PolyGloss can be quickly adapted to a new dataset via low-rank adaptation.

</details>


### [7] [Selecting Language Models for Social Science: Start Small, Start Open, and Validate](https://arxiv.org/abs/2601.10926)
*Dustin S. Stoltz,Marshall A. Taylor,Sanuj Kumar*

Main category: cs.CL

TL;DR: A framework for selecting large pretrained language models (LLMs) in social science research, prioritizing replicability; emphasizes openness, footprint, training data, and architectures/fine-tuning, and advocates starting with smaller open models and delimited benchmarks for ex-post validation of the entire computational pipeline.


<details>
  <summary>Details</summary>
Motivation: The abundance of LLMs creates selection challenges for social scientists. Ex-ante validity (benchmarks) is insufficient; ex-post validation is necessary. Replicability, enabled by reproducible tasks and pipelines, is essential for trustworthy findings.

Method: Conceptual analysis of evaluation dimensions (openness, footprint, training data, architectures/fine-tuning). Proposes a workflow: begin with smaller open models and construct delimited benchmarks to validate the entire computational pipeline, emphasizing ex-post validation and replicability.

Result: A framework and actionable recommendations: prioritize replicability as a core criterion, use small open models, design task-delimited benchmarks to validate the pipeline, and systematically assess model openness, footprint, data, and architecture/fine-tuning.

Conclusion: Replicability should guide model selection for social science research; adopt a staged approach starting with small open models and task-delimited benchmarks to ensure the validity and reproducibility of computational analyses.

Abstract: Currently, there are thousands of large pretrained language models (LLMs) available to social scientists. How do we select among them? Using validity, reliability, reproducibility, and replicability as guides, we explore the significance of: (1) model openness, (2) model footprint, (3) training data, and (4) model architectures and fine-tuning. While ex-ante tests of validity (i.e., benchmarks) are often privileged in these discussions, we argue that social scientists cannot altogether avoid validating computational measures (ex-post). Replicability, in particular, is a more pressing guide for selecting language models. Being able to reliably replicate a particular finding that entails the use of a language model necessitates reliably reproducing a task. To this end, we propose starting with smaller, open models, and constructing delimited benchmarks to demonstrate the validity of the entire computational pipeline.

</details>


### [8] [Multi-Stage Patient Role-Playing Framework for Realistic Clinical Interactions](https://arxiv.org/abs/2601.10951)
*Shijie Jiang,Zefan Zhang,Kehua Zhu,Tian Bai,Ruihong Zhao*

Main category: cs.CL

TL;DR: 提出并评估一个面向中文的患者模拟数据集Ch-PatientSim，基于五维人设构建真实临床互动场景，用少量样本增强解决类不平衡，并引入一个无训练成本的多阶段患者角色扮演（MSPRP）框架以提升个性化和真实感；实验表明该方法显著提升模型在患者模拟各维度上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法和基准依赖通用或LLM生成的对话数据，难以真实再现多样化的医生-患者互动，影响临床LLMs的训练与诊断教育效果。

Method: 构建五维人设的中文患者模拟数据集Ch-PatientSim；通过少-shot生成解决类别不平衡问题并人工验证；评估多种先进LLMs；提出训练无成本的多阶段患者角色扮演MSPRP框架，将互动分解为三阶段以确保个性化与现实感。

Result: 大多数模型输出过于正式、缺乏个性；MSPRP在患者模拟的多维度上显著提升模型表现。

Conclusion: Ch-PatientSim提供了一个真实且多样化的中文患者模拟基准，MSPRP实现了更具个性化、真实感的对话生成，促进临床LLMs与医学教育的发展。

Abstract: The simulation of realistic clinical interactions plays a pivotal role in advancing clinical Large Language Models (LLMs) and supporting medical diagnostic education. Existing approaches and benchmarks rely on generic or LLM-generated dialogue data, which limits the authenticity and diversity of doctor-patient interactions. In this work, we propose the first Chinese patient simulation dataset (Ch-PatientSim), constructed from realistic clinical interaction scenarios to comprehensively evaluate the performance of models in emulating patient behavior. Patients are simulated based on a five-dimensional persona structure. To address issues of the persona class imbalance, a portion of the dataset is augmented using few-shot generation, followed by manual verification. We evaluate various state-of-the-art LLMs and find that most produce overly formal responses that lack individual personality. To address this limitation, we propose a training-free Multi-Stage Patient Role-Playing (MSPRP) framework, which decomposes interactions into three stages to ensure both personalization and realism in model responses. Experimental results demonstrate that our approach significantly improves model performance across multiple dimensions of patient simulation.

</details>


### [9] [Steering Language Models Before They Speak: Logit-Level Interventions](https://arxiv.org/abs/2601.10960)
*Hyeseon An,Shinwoo Park,Hyundong Jin,Yo-Sub Han*

Main category: cs.CL

TL;DR: 提出训练无关、推理时的 logits 干预，利用来自标注语料的 z-normalized log-odds 统计表来引导解码，以实现跨写作风格、正式度和毒性控制的多任务可控生成。


<details>
  <summary>Details</summary>
Motivation: 现有的提示式和激活式引导在可控性上存在局限，提示经常难以实现稳定、细粒度控制，激活需要对内部层有深度访问。需要一种无需额外训练即可在推理阶段实现广泛适用的可控性的方法。

Method: 构建一个统计 token 评分表，该表基于标注语料的 z-normalized log-odds，对解码时的 token 进行干预以偏移解码分布，从而实现对输出风格、复杂度和安全性等属性的控制。

Result: 在写作复杂度、形式度、毒性三个任务的数据集上，方法实现了显著的控制效果，呈现大规模、稳定的多任务改进；最高达到约47个百分点的准确性提升，F1 约提升50倍。

Conclusion: 基于统计学原理的日志it 干预具有广泛的任务无关性和可扩展性，为训练-free 的推理阶段可控生成提供了有效路径，适用于多任务情境。

Abstract: Steering LLMs is essential for specialized applications such as style-sensitive text rewriting, user-adaptive communication, and toxicity mitigation. Current steering methods, such as prompting-based and activation-based approaches, are widely used to guide model behavior. However, activation-based techniques require deep access to internal layers, while prompting-based steering often fails to provide consistent or fine-grained control. In order to address these limitations, we propose a training-free inference-time logit intervention for controllable generation. Our approach utilizes a statistical token score table derived from z-normalized log-odds of labeled corpora to shift the decoding distribution. Empirical evaluations across three diverse datasets focusing on writing complexity, formality, and toxicity demonstrate that our method effectively steers output characteristics, confirming its broad applicability and task-agnostic nature. Our results show that statistically grounded logit steering can achieve large, consistent, and multi-task control gains: up to +47%p accuracy and 50x f1 improvement.

</details>


### [10] [ZPD Detector: Data Selection via Capability-Difficulty Alignment for Large Language Models](https://arxiv.org/abs/2601.10986)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Shijian Li*

Main category: cs.CL

TL;DR: 动态数据选择框架ZPD Detector：以ZPD为灵感，在模型与数据之间建立样本难度与当前能力的双向对齐，结合难度标定、基于IRT的能力估计与能力-难度匹配分数，按学习阶段动态筛选最具信息量的样本，以提升数据利用效率。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型成本日益上升，优质训练数据日益稀缺；现有基于静态准则的数据选择方法无法有效建模模型与数据之间的演化关系，需引入动态、对齐的选择机制。

Method: 提出ZPD Detector，整合难度标定、基于项目反应理论（IRT）的模型能力估计，以及能力-难度匹配分数，构建一个双向的样本选择框架，在不同学习阶段动态识别最具信息量的样本，提升数据利用效率；该方法提供对训练策略设计的新见解；并承诺在论文被接受后开源代码和数据。

Result: 摘要未给出具体实证结果；声称可改善数据利用效率并对训练策略设计提供新见解；计划在接受后公开代码和数据。

Conclusion: 提出将ZPD概念应用于数据选择的创新框架，通过动态的能力-难度匹配实现对样本的自适应筛选，潜在提升数据利用效率并引导新的训练策略设计。

Abstract: As the cost of training large language models continues to increase and high-quality training data become increasingly scarce, selecting high-value samples or synthesizing effective training data under limited data budgets has emerged as a critical research problem. Most existing data selection methods rely on static criteria, such as difficulty, uncertainty, or heuristics, and fail to model the evolving relationship between the model and the data. Inspired by the educational theory of the Zone of Proximal Development (ZPD), we propose ZPD Detector, a data selection framework that adopts a bidirectional perspective between models and data by explicitly modeling the alignment between sample difficulty and the model's current capability. ZPD Detector integrates difficulty calibration, model capability estimation based on Item Response Theory (IRT), and a capability-difficulty matching score to dynamically identify the most informative samples at each learning stage, improving data utilization efficiency; moreover, this dynamic matching strategy provides new insights into training strategy design. All code and data will be released after our work be accepted to support reproducible researc

</details>


### [11] [When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs](https://arxiv.org/abs/2601.11000)
*Zhongxiang Sun,Yi Zhan,Chenglei Shen,Weijie Yu,Xiao Zhang,Ming He,Jun Xu*

Main category: cs.CL

TL;DR: 个性化LLMs在面对事实性问题时，可能被用户历史偏好绑架，导致和客观事实不一致的回答（个性化幻觉），从而削弱事实可靠性并可能传播错误信念。提出轻量化推理时干预的Factuality-Preserving Personalized Steering (FPPS)，以在保持个性化行为的同时降低事实扭曲；引入PFQABench作为首个联合评估事实性与个性化问答的基准。实验证明FPPS在多种LLM骨干与个性化方法上显著提升事实准确性，同时保持个性化性能。


<details>
  <summary>Details</summary>
Motivation: 研究个性化与事实推理之间的潜在表示耦合导致的事实性偏离，以及个性化偏好对回答的潜在干扰问题，旨在在不牺牲个性化的前提下提升事实可靠性。

Method: 提出FPPS这一推理时的轻量化干预方法，以抑制个性化导致的事实扭曲；建立PFQABench基准用于同时评估事实性与个性化问答的性能；在多种LLM backbone 与不同个性化策略上进行系统实验。

Result: FPPS在保持个性化能力的同时显著提升事实准确性；PFQABench首次实现事实性与个性化的联合评估并验证方法有效性；实验覆盖多种模型与个性化方法，结果具有普适性。

Conclusion: 在保持个性化表现的前提下，实现对事实性的有效保护，FPPS提供可行的推理-time干预策略，PFQABench提供评估两者的标准化基准。

Abstract: Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.

</details>


### [12] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2601.11002)
*Qianen Zhang,Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 提出扩展 SiMT 动作空间，新增四个自适应动作（Sentence_Cut、Drop、Partial_Summarization、Pronominalization），结合大语言模型实现实时重构与简化；引入延迟感知的文本转语音；在 ACL60/60 英-中、英-德、英-日上实验，语义指标提升且延迟降低；组合 Drop+Sentence_Cut 提升流畅性-延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的SiMT策略大多仅使用 READ/WRITE，难以在严格实时条件下兼顾语义保真与流畅性。需要可自适应且可扩展的动作空间来实现实时重构、删减和简化，同时引入基于LLM的生成策略以提升质量。引入延迟感知的评估（TTS）以更贴近实际应用场景。

Method: 将 Sentence_Cut、Drop、Partial_Summarization、Pronominalization 四个自适应动作引入 SiMT 的动作空间，并在LLM框架下进行实现；设计动作感知的提示策略以生成训练参考；构建延迟感知的TTS流水线，将文本输出映射为带真实时序的语音；在 ACL60/60 英中、英德、英日等多语对上进行评估；对比参考翻译和 salami 基线。

Result: 实验结果显示，所提出的框架在语义指标上稳定提升，且相较参考翻译和 salami 基线实现更低的延迟；将 Drop 与 Sentence_Cut 组合使用时，在流畅性与延迟之间取得更好的平衡。

Conclusion: 丰富的动作空间与LLM驱动的SiMT对实时翻译的性能提升具有潜力，有助于缩小人机解释之间的差距。

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations under strict real-time constraints, which traditional policies with only READ/WRITE actions cannot fully address. We extend the action space of SiMT with four adaptive actions: Sentence_Cut, Drop, Partial_Summarization and Pronominalization, which enable real-time restructuring, omission, and simplification while preserving semantic fidelity. We adapt these actions in a large language model (LLM) framework and construct training references through action-aware prompting. To evaluate both quality and word-level monotonicity, we further develop a latency-aware TTS pipeline that maps textual outputs to speech with realistic timing. Experiments on the ACL60/60 English-Chinese, English-German and English-Japanese benchmarks show that our framework consistently improves semantic metrics and achieves lower delay compared to reference translations and salami-based baselines. Notably, combining Drop and Sentence_Cut leads to consistent improvements in the balance between fluency and latency. These results demonstrate that enriching the action space of LLM-based SiMT provides a promising direction for bridging the gap between human and machine interpretation.

</details>


### [13] [NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems](https://arxiv.org/abs/2601.11004)
*Jiayu Liu,Rui Wang,Qing Zong,Qingcheng Zeng,Tianshi Zheng,Haochen Shi,Dadi Guo,Baixuan Xu,Chunyang Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 提出 NAACL 框架，通过噪声感知校准规则和数据合成，提升检索增强生成（RAG）中的模型校准性能，在域内提升 ECE 10.9%，跨域提升 8.0%，无需更强教师模型。


<details>
  <summary>Details</summary>
Motivation: 在 RAG 场景中，检索噪声导致模型对证据作出过度自信的现象尚未被充分研究，需要一个 principled 的方法来对噪声进行校准并提升 epistemic 置信度。

Method: 提出 Noise-AwAre Confidence Calibration Rules (NAACL Rules)，并以此设计 NAACL 框架，通过从约 2K 个 HotpotQA 示例中合成监督信号，进行有监督微调（SFT），使模型具备对噪声的内在感知与校准能力；不依赖更强的教师模型。

Result: 在四个基准上评估，ECE 指标在域内提升约 10.9%，在域外提升约 8.0%；显示在处理检索噪声时的显著校准改进。

Conclusion: NAACL 将检索噪声与口头校准联系起来，为实现更准确、具 epistemic 可靠性的 LLM 提供了新的方向，并缓解以噪声为由导致的过度自信问题。

Abstract: Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs.

</details>


### [14] [Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs](https://arxiv.org/abs/2601.11019)
*Xinwei Wu,Heng Liu,Xiaohu Zhao,Yuqi Ren,Linlong Xu,Longyue Wang,Deyi Xiong,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: Proposes a framework using Sparse Autoencoders to identify translation initiation features in LLMs, demonstrates their causal role, and uses this insight to improve data efficiency via mechanistically hard sample selection; shows transferability to larger models.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit translation ability without task-specific fine-tuning, but the internal mechanisms are largely opaque. Uncovering these mechanisms can explain robustness and guide efficient improvement.

Method: Use Sparse Autoencoders to recall frequently co-activated features on translation inputs; apply a PCA-based consistency metric to select functionally coherent features; isolate translation initiation features; perform causal interventions (amplify/ablate) to test causality; propose data selection strategy prioritizing mechanistically hard samples; test transferability to larger models.

Result: Isolates a small set of translation initiation features; causal manipulation confirms their role (amplification improves translation, ablation induces hallucinations); mechanistic hard-sample data selection improves data efficiency and reduces hallucinations; mechanisms transfer to larger models; code released.

Conclusion: Identifies a core, transferable mechanism underlying LLM translation, offering a blueprint for leveraging internal mechanisms to build more robust and data-efficient models.

Abstract: Large Language Models (LLMs) frequently exhibit strong translation abilities, even without task-specific fine-tuning. However, the internal mechanisms governing this innate capability remain largely opaque. To demystify this process, we leverage Sparse Autoencoders (SAEs) and introduce a novel framework for identifying task-specific features. Our method first recalls features that are frequently co-activated on translation inputs and then filters them for functional coherence using a PCA-based consistency metric. This framework successfully isolates a small set of **translation initiation** features. Causal interventions demonstrate that amplifying these features steers the model towards correct translation, while ablating them induces hallucinations and off-task outputs, confirming they represent a core component of the model's innate translation competency. Moving from analysis to application, we leverage this mechanistic insight to propose a new data selection strategy for efficient fine-tuning. Specifically, we prioritize training on **mechanistically hard** samples-those that fail to naturally activate the translation initiation features. Experiments show this approach significantly improves data efficiency and suppresses hallucinations. Furthermore, we find these mechanisms are transferable to larger models of the same family. Our work not only decodes a core component of the translation mechanism in LLMs but also provides a blueprint for using internal model mechanism to create more robust and efficient models. The codes are available at https://github.com/flamewei123/AAAI26-translation-Initiation-Features.

</details>


### [15] [From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models](https://arxiv.org/abs/2601.11020)
*Youmi Ma,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 对检索头的机制性解析表明，某些模型中检索头可被利用来提升长上下文能力。作者提出 RetMask，通过对比正常模型输出与在检索头被熄灭的变体输出来生成训练信号，显著提升特定任务表现，但效果依赖检索头的组织模式。


<details>
  <summary>Details</summary>
Motivation: 旨在将机制性可解释性与模型性能提升结合起来，回答检索头是否能在实际任务中提升长上下文能力，以及其作用是否受组织结构影响。

Method: 提出 RetMask：通过对比正常输出与检索头被掩蔽的 ablated 变体来生成训练信号，基于机制原理的信号驱动训练；在 Llama-3.1/128K、HELMET、以及三类模型家族上评估，分析检索头组织对效果的影响。

Result: 在 Llama-3.1 上，HELMET 128K 的指标提升 +2.28；在带引用的生成任务上提升约 70%，在段落重排任务上提升约 32%，且对常规任务表现保持。模型间比较表明，效果取决于检索头的组织：集中模式的模型收益显著，分布模式的模型收益有限。

Conclusion: 机制性关系得到验证，说明可将对检索头功能的理解转化为性能提升；同时揭示了模型中检索头的组织结构对提升效果的关键作用。

Abstract: Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether retrieval heads can be leveraged to enhance the long-context capabilities of LLMs. Specifically, we propose RetMask, a method that generates training signals by contrasting normal model outputs with those from an ablated variant in which the retrieval heads are masked. This mechanism-based approach achieves substantial improvements: +2.28 points on HELMET at 128K for Llama-3.1, with +70% gains on generation with citation and +32% on passage re-ranking, while preserving performance on general tasks. Experiments across three model families reveal that the effectiveness depends on retrieval head organization: models with concentrated patterns of retrieval heads respond strongly, while those with distributed patterns show limited gains. This mechanistic relationship validates the function of retrieval heads and demonstrates that mechanistic insights can be transformed into performance enhancements.

</details>


### [16] [Budget-Aware Anytime Reasoning with LLM-Synthesized Preference Data](https://arxiv.org/abs/2601.11038)
*Xuanming Zhang,Shwan Ashrafi,Aziza Mirsaidova,Amir Rezaeian,Miguel Ballesteros,Lydia B. Chilton,Zhou Yu,Dan Roth*

Main category: cs.CL

TL;DR: 在固定推理预算下的 anytime 推理框架，提出 Anytime Index 用于衡量解质量随推理代币增长的提升，并通过基于 LLM 的自我偏好数据实现中间解的自我提升，提升推理质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现实任务常需在有限计算预算内快速给出可用解，全面推理成本高且延迟不可接受；因此需要预算感知的推理策略以在有限代价内最大化输出价值。

Method: 提出 Anytime Reasoning 框架与 Anytime Index 指标；引入推理时的自我提升机制，利用 LLM 生成的偏好数据进行自监督比较学习，以改进中间解。并在 NaturalPlan (Trip)、AIME、GPQA 数据集上验证，覆盖 Grok-3、GPT-oss、GPT-4.1/4o、LLaMA 等模型。

Result: 在多个数据集和多种模型上取得一致性提升，显著提升在预算约束下的推理质量与效率。

Conclusion: 预算敏感的推理框架结合自我改进的偏好数据，可在不依赖外部标签的前提下提升中间解质量与整体效率，Anytime Index 为随预算增量的性能变化提供量化评估。

Abstract: We study the reasoning behavior of large language models (LLMs) under limited computation budgets. In such settings, producing useful partial solutions quickly is often more practical than exhaustive reasoning, which incurs high inference costs. Many real-world tasks, such as trip planning, require models to deliver the best possible output within a fixed reasoning budget. We introduce an anytime reasoning framework and the Anytime Index, a metric that quantifies how effectively solution quality improves as reasoning tokens increase. To further enhance efficiency, we propose an inference-time self-improvement method using LLM-synthesized preference data, where models learn from their own reasoning comparisons to produce better intermediate solutions. Experiments on NaturalPlan (Trip), AIME, and GPQA datasets show consistent gains across Grok-3, GPT-oss, GPT-4.1/4o, and LLaMA models, improving both reasoning quality and efficiency under budget constraints.

</details>


### [17] [Spectral Characterization and Mitigation of Sequential Knowledge Editing Collapse](https://arxiv.org/abs/2601.11042)
*Chi Zhang,Mengqi Zhang,Xiaotian Ye,Runxi Cheng,Zisheng Zhou,Ying Zhou,Pengjie Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: 通过对大语言模型的权重矩阵进行特征分析，模型的泛化能力与主导奇异方向高度相关，这些方向对扰动极为敏感，重复编辑会逐步破坏它们，从而导致能力崩溃；提出 REVIVE 以保留主导奇异子空间来稳定连续编辑。


<details>
  <summary>Details</summary>
Motivation: 连续知识编辑在大模型中容易导致泛化能力的灾难性崩溃，尤其是对参数进行修改的方法。现有方法多依赖启发式的参数更新约束，但其机制尚不清楚，因此需要从模型内部结构出发理解并缓解这一现象。

Method: 对预训练权重矩阵进行谱分析，发现主导奇异方向与泛化能力密切相关，且对扰动高度敏感。提出 REVIVE，作为即插即用的框架，通过在原始权重的谱基中表示参数更新，并保护的子空间以外的分量进行更新，从而显式地保留主导奇异子空间。

Result: 在多种模型和基准上进行了广泛实验，结果表明 REVIVE 在长时序编辑（包括高达20,000次编辑）情境下，能显著提高编辑效果，同时大幅保持模型的泛化能力。

Conclusion: 主导奇异方向是决定模型泛化与编辑鲁棒性的关键维度，稳定其子空间可有效缓解连续编辑导致的能力崩溃；REVIVE提供了一种可实际落地的谱基保护策略，具备良好普适性。

Abstract: Sequential knowledge editing in large language models often causes catastrophic collapse of the model's general abilities, especially for parameter-modifying methods. Existing approaches mitigate this issue through heuristic constraints on parameter updates, yet the mechanisms underlying such degradation remain insufficiently understood. In this work, we present a spectral analysis of sequential knowledge editing and show that a model's general abilities are closely associated with dominant singular directions of pretrained weight matrices. These directions are highly sensitive to perturbations and are progressively disrupted by repeated edits, closely tracking the collapse in both editing efficacy and general performance. Building on this insight, we propose REVIVE, a plug-and-play framework that stabilizes sequential editing by explicitly preserving the dominant singular subspace. REVIVE represents parameter updates in the spectral basis of the original weights and filters components that would interfere with the protected region. Extensive experiments across multiple models and benchmarks show that REVIVE consistently improves editing efficacy while substantially preserving general abilities under long-horizon sequential editing, including extreme settings with up to 20,000 edits.

</details>


### [18] [CoG: Controllable Graph Reasoning via Relational Blueprints and Failure-Aware Refinement over Knowledge Graphs](https://arxiv.org/abs/2601.11047)
*Yuanxiang Liu,Songze Li,Xiaoke Guo,Zhaoyan Gong,Qifei Zhang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出一个训练无关的双过程框架CoG，用关系蓝图与故障感知的反思机制提升KG-LLM推理的稳定性与效率，解决现有KG-augmented LLMs对噪声和结构错位的脆弱性，在三个基准上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理中易产生幻觉，知识图提供显式 grounding，但现有KG-augmented LLMs缺乏多样搜索策略，易受邻域噪声和结构错位影响，导致推理停滞，需训练无关但具备快速稳定性与自适应反思的框架。

Method: CoG采用双过程启发：Relational Blueprint Guidance作为快速直觉过程，利用关系蓝图作为可解释的软结构约束快速引导搜索；Failure-Aware Refinement在遇到推理瓶颈时进行证据条件的反思与受控回溯，打破停滞；框架为训练无关，提升推理准确性与效率。

Result: 在三个基准上，CoG显著优于最先进方法，在准确性和效率上实现提升。

Conclusion: 训练无关的双过程框架有效提升KG-LLM的推理稳定性、可解释性与效率，缓解噪声与结构错位带来的影响。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities but often grapple with reliability challenges like hallucinations. While Knowledge Graphs (KGs) offer explicit grounding, existing paradigms of KG-augmented LLMs typically exhibit cognitive rigidity--applying homogeneous search strategies that render them vulnerable to instability under neighborhood noise and structural misalignment leading to reasoning stagnation. To address these challenges, we propose CoG, a training-free framework inspired by Dual-Process Theory that mimics the interplay between intuition and deliberation. First, functioning as the fast, intuitive process, the Relational Blueprint Guidance module leverages relational blueprints as interpretable soft structural constraints to rapidly stabilize the search direction against noise. Second, functioning as the prudent, analytical process, the Failure-Aware Refinement module intervenes upon encountering reasoning impasses. It triggers evidence-conditioned reflection and executes controlled backtracking to overcome reasoning stagnation. Experimental results on three benchmarks demonstrate that CoG significantly outperforms state-of-the-art approaches in both accuracy and efficiency.

</details>


### [19] [Efficient Multilingual Name Type Classification Using Convolutional Networks](https://arxiv.org/abs/2601.11090)
*Davor Lauc*

Main category: cs.CL

TL;DR: 提出 Onomas-CNN X，一种并行卷积分支与深度可分离卷积的字名分类CNN；在104种语言、4类实体上达到92.1%准确率，单核CPU达2813名/秒，较微调后的XLM-RoBERTa快46倍，能源消耗也降低46倍，表明在数据充分时，专用CNN仍能与大型预训练模型竞争。


<details>
  <summary>Details</summary>
Motivation: 实现对专有名词的语言与实体类型分类的高效、开箱即用解决方案，兼顾准确性与硬件效率，降低对昂贵预训练模型的依赖与能耗。

Method: 提出并行卷积分支、深度可分离卷积及分层分类的CNN架构 Onomas-CNN X；在大规模多语 datasets（104语言、4实体类型）上训练与评估。

Result: 达到92.1%准确率；单CPU核心处理2800+名/秒（2813名/秒）；比微调的 XLM-RoBERTa 快约46倍；能耗降低约46倍。

Conclusion: 在数据充足时，针对性CNN架构仍具备与大型预训练模型相竞争的能力，适合对CPU友好、对能源敏感的场景。

Abstract: We present a convolutional neural network approach for classifying proper names by language and entity type. Our model, Onomas-CNN X, combines parallel convolution branches with depthwise-separable operations and hierarchical classification to process names efficiently on CPU hardware. We evaluate the architecture on a large multilingual dataset covering 104 languages and four entity types (person, organization, location, other). Onomas-CNN X achieves 92.1% accuracy while processing 2,813 names per second on a single CPU core - 46 times faster than fine-tuned XLM-RoBERTa with comparable accuracy. The model reduces energy consumption by a factor of 46 compared to transformer baselines. Our experiments demonstrate that specialized CNN architectures remain competitive with large pre-trained models for focused NLP tasks when sufficient training data exists.

</details>


### [20] [Integrity Shield A System for Ethical AI Use & Authorship Transparency in Assessments](https://arxiv.org/abs/2601.11093)
*Ashish Raj Shekhar,Shiven Agarwal,Priyanuj Bordoloi,Yash Shah,Tejas Anvekar,Vivek Gupta*

Main category: cs.CL

TL;DR: A document-layer watermarking system (Integrity Shield) embeds schema-aware, item-level watermarks into assessment PDFs to block large language models from answering shielded exams and to enable reliable authorship/signature recovery, achieving high blocking and retrieval across multiple exams and commercial LLMs.


<details>
  <summary>Details</summary>
Motivation: Mitigate academic integrity risks from LLMs solving exams from uploaded PDFs; existing watermark methods fail against black-box models with instructor-provided documents.

Method: Embed schema-aware, item-level watermarks into assessment PDFs without altering human-visible appearance; watermarks yield stable signatures that can be recovered from model or student responses; evaluated on 30 exams across STEM, humanities, and medical reasoning using four commercial LLMs.

Result: Exam-level blocking of 91-94% and signature retrieval of 89-93% across four commercial LLMs.

Conclusion: Demonstrates a practical defense against LLM-driven exam cheating with an instructor-friendly interface and reliable authorship evidence.

Abstract: Large Language Models (LLMs) can now solve entire exams directly from uploaded PDF assessments, raising urgent concerns about academic integrity and the reliability of grades and credentials. Existing watermarking techniques either operate at the token level or assume control over the model's decoding process, making them ineffective when students query proprietary black-box systems with instructor-provided documents. We present Integrity Shield, a document-layer watermarking system that embeds schema-aware, item-level watermarks into assessment PDFs while keeping their human-visible appearance unchanged. These watermarks consistently prevent MLLMs from answering shielded exam PDFs and encode stable, item-level signatures that can be reliably recovered from model or student responses. Across 30 exams spanning STEM, humanities, and medical reasoning, Integrity Shield achieves exceptionally high prevention (91-94% exam-level blocking) and strong detection reliability (89-93% signature retrieval) across four commercial MLLMs. Our demo showcases an interactive interface where instructors upload an exam, preview watermark behavior, and inspect pre/post AI performance & authorship evidence.

</details>


### [21] [The Growing Gains and Pains of Iterative Web Corpora Crawling: Insights from South Slavic CLASSLA-web 2.0 Corpora](https://arxiv.org/abs/2601.11170)
*Taja Kuzman Pungeršek,Peter Rupnik,Vít Suchomel,Nikola Ljubešić*

Main category: cs.CL

TL;DR: CLASSLA-web 2.0: a large, continuously crawled, seven-language web corpus for South Slavic languages (17.0B words, 38.1M texts), with automatic topic annotations; shows largely new content compared to 1.0, but quality degrades due to machine-generated sites.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity in less-resourced South Slavic languages by extending CLASSLA-web from 1.0 to a continuous, iterative crawling system across national TLDs, enabling up-to-date, broad-coverage corpora with topic labeling.

Method: Developed a continuous crawling infrastructure for iterative national TLD crawling across South Slavic and related webs; collected 17.0B words in 38.1M texts across seven languages; added automatic topic-label annotation; compared with CLASSLA-web 1.0 to assess overlap.

Result: Produced CLASSLA-web 2.0 corpus: larger size, seven languages, automated topics; only 20% overlap with 1.0, indicating largely new content; observed degradation of top-domain content due to increasing machine-generated sites; re-crawling yields substantial content gains.

Conclusion: Continuous crawling yields expanding corpora and coverage for South Slavic languages, but faces data quality challenges from machine-generated content; suggests need for quality control and filtering.

Abstract: Crawling national top-level domains has proven to be highly effective for collecting texts in less-resourced languages. This approach has been recently used for South Slavic languages and resulted in the largest general corpora for this language group: the CLASSLA-web 1.0 corpora. Building on this success, we established a continuous crawling infrastructure for iterative national top-level domain crawling across South Slavic and related webs. We present the first outcome of this crawling infrastructure - the CLASSLA-web 2.0 corpus collection, with substantially larger web corpora containing 17.0 billion words in 38.1 million texts in seven languages: Bosnian, Bulgarian, Croatian, Macedonian, Montenegrin, Serbian, and Slovenian. In addition to genre categories, the new version is also automatically annotated with topic labels. Comparing CLASSLA-web 2.0 with its predecessor reveals that only one-fifth of the texts overlap, showing that re-crawling after just two years yields largely new content. However, while the new web crawls bring growing gains, we also notice growing pains - a manual inspection of top domains reveals a visible degradation of web content, as machine-generated sites now contribute a significant portion of texts.

</details>


### [22] [DOREMI: Optimizing Long Tail Predictions in Document-Level Relation Extraction](https://arxiv.org/abs/2601.11190)
*Laura Menotti,Stefano Marchesin,Gianmaria Silvello*

Main category: cs.CL

TL;DR: DOREMI 是一种迭代式主动标注框架，通过少量但高信息量的示例标注来提升文档级关系抽取中的长尾关系泛化能力，且可与任意 DocRE 模型结合。


<details>
  <summary>Details</summary>
Motivation: DocRE 面临跨句上下文的依赖和长尾关系分布导致的学习困难；现有方法要么引入大量噪声数据，要么进行去噪，效率低且对稀有关系提升有限。因此需要一种高效、可扩展的主动标注策略来增强对稀有关系的学习。

Method: 提出迭代式框架 DOREMI，主动选择信息量最大的样本进行最小化人工标注以改进训练；强调在训练阶段进行有限且有针对性的人工标注；框架设计为模型无关，能够嵌入任意 DocRE 模型。

Result: 该方法有助于缓解长尾偏差，提升对稀有关系的泛化能力，提供一个可扩展的解决思路。

Conclusion: DOREMI 为 DocRE 的长尾问题提供高效、可扩展的解决方案，兼容现有 DocRE 模型，显著提升对稀有关系的学习潜力。

Abstract: Document-Level Relation Extraction (DocRE) presents significant challenges due to its reliance on cross-sentence context and the long-tail distribution of relation types, where many relations have scarce training examples. In this work, we introduce DOcument-level Relation Extraction optiMizing the long taIl (DOREMI), an iterative framework that enhances underrepresented relations through minimal yet targeted manual annotations. Unlike previous approaches that rely on large-scale noisy data or heuristic denoising, DOREMI actively selects the most informative examples to improve training efficiency and robustness. DOREMI can be applied to any existing DocRE model and is effective at mitigating long-tail biases, offering a scalable solution to improve generalization on rare relations.

</details>


### [23] [T$^\star$: Progressive Block Scaling for MDM Through Trajectory Aware RL](https://arxiv.org/abs/2601.11214)
*Hanchen Xia,Baoyou Chen,Yutang Ge,Guojiang Zhao,Siyu Zhu*

Main category: cs.CL

TL;DR: 提出 T*, 基于 TraceRL 的训练日程，用于在掩码扩散语言模型中进行分块大小的渐进式扩展；实现更高的解码并行性，同时在数学推理基准上保持最小的性能下降；并分析 T* 可能收敛到另一种解码调度 S_hat，达到可比的性能。


<details>
  <summary>Details</summary>
Motivation: 解决掩码扩散语言模型中分块大小对推理并行性与性能的权衡，寻找简单且可扩展的训练日程以提升并行解码，同时尽量保留数学推理能力；同时探究是否存在与 T* 等效、但不同的解码调度。

Method: 以自回归初始化的小块 MDM 为起点，使用基于 TraceRL 的训练日程 T*，使模型逐步过渡至更大分块以提升解码并行性；在 math reasoning 基准上评估性能损失，并分析 T* 是否会收敛到可比的解码调度 S_hat；对比不同块大小下的性能与并行性。

Result: 实现更高的解码并行性，且在数学推理基准上几乎无显著性能下降；分析表明 T* 具备收敛到替代解码调度 S_hat 的潜力，且该调度与原始路径具有相近的性能。

Conclusion: T* 为掩码扩散语言模型提供了一种简单且有效的渐进块大小扩展训练日程，提升解码并行性并维持数学推理表现；并存在收敛至等效解码调度的潜在路径，为高效扩展的扩散模型提供了可行方向。

Abstract: We present T$^\star$, a simple \textsc{TraceRL}-based training curriculum for progressive block-size scaling in masked diffusion language models (MDMs). Starting from an AR-initialized small-block MDM, T$^\star$~transitions smoothly to larger blocks, enabling higher-parallelism decoding with minimal performance degradation on math reasoning benchmarks. Moreover, further analysis suggests that T$^\star$~can converge to an alternative decoding schedule $\hat{\rm S}$ that achieves comparable performance.

</details>


### [24] [MultiCaption: Detecting disinformation using multilingual visual claims](https://arxiv.org/abs/2601.11220)
*Rafael Martins Frade,Rrubaa Panchendrarajan,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 提出并评估MultiCaption数据集，用于跨模态、跨语言的视觉声明矛盾检测，显示在多语言和多模态环境中对现有NLI基线的挑战性和潜在的事实查证应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有事实核查数据集无法覆盖真实世界多模态、多语言传播的不足；需要一个能够在图像/视频级别的视觉声称与文本之间的矛盾进行评估的数据资源。

Method: 构建包含11,088条视觉声明、覆盖64种语言的数据集，针对同一图像或视频的不同声明对进行矛盾性标注，采用多种标注策略。基于变换器模型、自然语言推理模型和大语言模型进行系统评估，进行任务微调。

Result: MultiCaption比标准NLI任务更具挑战性，需针对任务进行微调才能获得良好表现；多语言训练与测试带来显著收益，显示在避免机器翻译的前提下构建多语言事实核查管线的潜力。

Conclusion: 提供一个面向真正多模态和多语言环境的基准数据集，推动下一步在跨模态对齐、跨语言推理以及无翻译的多语言核查系统方面的研究。

Abstract: Online disinformation poses an escalating threat to society, driven increasingly by the rapid spread of misleading content across both multimedia and multilingual platforms. While automated fact-checking methods have advanced in recent years, their effectiveness remains constrained by the scarcity of datasets that reflect these real-world complexities. To address this gap, we first present MultiCaption, a new dataset specifically designed for detecting contradictions in visual claims. Pairs of claims referring to the same image or video were labeled through multiple strategies to determine whether they contradict each other. The resulting dataset comprises 11,088 visual claims in 64 languages, offering a unique resource for building and evaluating misinformation-detection systems in truly multimodal and multilingual environments. We then provide comprehensive experiments using transformer-based architectures, natural language inference models, and large language models, establishing strong baselines for future research. The results show that MultiCaption is more challenging than standard NLI tasks, requiring task-specific finetuning for strong performance. Moreover, the gains from multilingual training and testing highlight the dataset's potential for building effective multilingual fact-checking pipelines without relying on machine translation.

</details>


### [25] [Language of Thought Shapes Output Diversity in Large Language Models](https://arxiv.org/abs/2601.11227)
*Shaoyang Xu,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 通过多语言“思维语言”提升大语言模型的输出多样性；切换思维语言从英语到非英语可获得更大多样性增益，且跨语言聚合进一步提升，语言间距离越远增益越大；并在多语言下对齐任务显示实用性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM输出缺乏多样性的问题，提出将“思维语言”作为结构性来源来提升输出的多样性，并研究在不同语言下的重复采样策略对英语输出多样性的影响。

Method: 提出两种重复采样策略：单语言采样（Single-Language Sampling）与混合语言采样（Mixed-Language Sampling），在思维语言不同的条件下产生输出并限定输出为英语，评估英语输出的多样性。进行了大规模实验，比较英语与非英语思维语言在思维空间中的分布差异，研究跨语言聚合的效应，并考察语言异质性对多样性上限的影响。

Result: 不同思维语言在模型的思维空间中占据不同区域；将思维语言从英语切换到非英语语言，英语输出的多样性显著提升；距离英语越远的语言带来更大增益；跨多语言聚合样本可获得额外的组合效应；扩展采样的语言异质性可提高多样性上限；在现实任务的“多元化对齐”中实现更广覆盖的文化知识和价值取向。

Conclusion: 以思维语言为来源的多样性控制是一种鲁棒且可扩展的方法，可提升LLM输出的多样性并带来对文化知识与价值观取向覆盖面的实际收益。

Abstract: Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.

</details>


### [26] [FactCorrector: A Graph-Inspired Approach to Long-Form Factuality Correction of Large Language Models](https://arxiv.org/abs/2601.11232)
*Javier Carnerero-Cano,Massimiliano Pronesti,Radu Marinescu,Tigran Tchrakian,James Barry,Jasmina Gajcin,Yufang Hou,Alessandra Pascale,Elizabeth Daly*

Main category: cs.CL

TL;DR: 提出 FactCorrector 的后修正方法，无需重新训练即可跨领域自适应纠错，并引入 VELI5 基准来评估因果错误纠正的有效性；在 VELI5 及其它长文本事实性数据集上实验，显著提升事实准确性并保持相关性，且优于强基线，公开代码。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在知识密集型任务中的事实性错误（幻觉）问题。现有方法多需再训练或局部微调，缺乏跨领域鲁棒性及标准化评估。提出可后处理纠错且能跨领域适配的方案，并提供一个可严格评测事实性纠错的方法学基准。

Method: 使用结构化的关于原始回答事实性的反馈来生成纠错答案，属于后处理纠错且具跨领域自适应能力；在不重新训练的前提下，利用对事实性反馈的指引来改写或纠正回答。并构建 VELI5 基准，含系统注入的事实错误及其真实纠错答案，用于评估纠错方法的准确性与相关性。

Result: 在 VELI5 及多个长文本事实性数据集上，FactCorrector 显著提升事实性精准度同时保持相关性，优于强基线方法。

Conclusion: FactCorrector 能在跨领域场景下实现高质量事实性纠错，VELI5 提供严谨的评测资源。研究并提供代码以促进复现与应用。

Abstract: Large language models (LLMs) are widely used in knowledge-intensive applications but often generate factually incorrect responses. A promising approach to rectify these flaws is correcting LLMs using feedback. Therefore, in this paper, we introduce FactCorrector, a new post-hoc correction method that adapts across domains without retraining and leverages structured feedback about the factuality of the original response to generate a correction. To support rigorous evaluations of factuality correction methods, we also develop the VELI5 benchmark, a novel dataset containing systematically injected factual errors and ground-truth corrections. Experiments on VELI5 and several popular long-form factuality datasets show that the FactCorrector approach significantly improves factual precision while preserving relevance, outperforming strong baselines. We release our code at https://ibm.biz/factcorrector.

</details>


### [27] [How DDAIR you? Disambiguated Data Augmentation for Intent Recognition](https://arxiv.org/abs/2601.11234)
*Galo Castillo-López,Alexis Lombard,Nasredine Semmar,Gaël de Chalendar*

Main category: cs.CL

TL;DR: 提出 DDAIR（Disambiguated Data Augmentation for Intent Recognition），通过句子嵌入检测并消除语言模型生成的意图数据增强示例中的歧义，从而在低资源场景下提升意图识别性能。通过迭代重生成来减小歧义性。


<details>
  <summary>Details</summary>
Motivation: LLMs 在数据增强中可能产生与未目标类别存在歧义的样例，导致分类器训练误导。需要一种机制来识别并消除这些歧义样本，以提升在意图定义宽泛或模糊场景中的鲁棒性。

Method: 使用 Sentence Transformers 评估生成的增强样本与各意图之间的语义相似性，识别那些与目标意图以外的意图更相似的样本；针对这类样本进行迭代式的再生成以降低歧义性。

Result: 实验发现句子嵌入在重新生成和筛选低歧义增强样本方面有效，显示在意图定义较宽泛的场景中可提升分类性能的潜力。

Conclusion: DDAIR 为意图识别的增强数据提供了一种可行的歧义消解框架，尤其在低资源和定义模糊的场景中具有应用前景。

Abstract: Large Language Models (LLMs) are effective for data augmentation in classification tasks like intent detection. In some cases, they inadvertently produce examples that are ambiguous with regard to untargeted classes. We present DDAIR (Disambiguated Data Augmentation for Intent Recognition) to mitigate this problem. We use Sentence Transformers to detect ambiguous class-guided augmented examples generated by LLMs for intent recognition in low-resource scenarios. We identify synthetic examples that are semantically more similar to another intent than to their target one. We also provide an iterative re-generation method to mitigate such ambiguities. Our findings show that sentence embeddings effectively help to (re)generate less ambiguous examples, and suggest promising potential to improve classification performance in scenarios where intents are loosely or broadly defined.

</details>


### [28] [Reasoning in Trees: Improving Retrieval-Augmented Generation for Multi-Hop Question Answering](https://arxiv.org/abs/2601.11255)
*Yuling Shi,Maolin Sun,Zijun Liu,Mo Yang,Yixiong Fang,Tianran Sun,Xiaodong Gu*

Main category: cs.CL

TL;DR: 提出了一种分层的多跳问答框架RT-RAG，通过显式推理树分解问题，采用自底向上的遍历和迭代式查询改写来收集证据，显著降低因错误分解导致的连锁错误，且在复杂多跳QA上超越现有方法7.0%F1和6.0%EM。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多跳问答多依赖自我引导的分步检索和问句分解，易产生分解不准确和步骤间的错误传播，难以保持推理连贯性；需要结构化的问句分解、共识选择的推理树，以及降低错误传播的检索策略。

Method: 提出Reasoning Tree Guided RAG (RT-RAG)框架：1) 将多跳问题分解为显式的推理树，利用结构化实体分析与共识式树选择将核心查询、已知实体和未知实体分离；2) 采用自底向上的遍历策略，通过迭代式的查询改写与证据精炼来收集高质量证据，降低错误传播；3) 将检索与推理过程结合，以提高复杂多跳QA的推理一致性和准确性。

Result: 在复杂多跳QA任务上，RT-RAG显著优于当前最先进方法，F1提升7.0%，EM提升6.0%。

Conclusion: RT-RAG通过结构化的推理树分解和自底向上的证据收集，有效降低了推理过程中的错误传播，提升了复杂多跳QA的推理一致性与准确性，展现了对RAG框架的有效改进潜力。

Abstract: Retrieval-Augmented Generation (RAG) has demonstrated significant effectiveness in enhancing large language models (LLMs) for complex multi-hop question answering (QA). For multi-hop QA tasks, current iterative approaches predominantly rely on LLMs to self-guide and plan multi-step exploration paths during retrieval, leading to substantial challenges in maintaining reasoning coherence across steps from inaccurate query decomposition and error propagation. To address these issues, we introduce Reasoning Tree Guided RAG (RT-RAG), a novel hierarchical framework for complex multi-hop QA. RT-RAG systematically decomposes multi-hop questions into explicit reasoning trees, minimizing inaccurate decomposition through structured entity analysis and consensus-based tree selection that clearly separates core queries, known entities, and unknown entities. Subsequently, a bottom-up traversal strategy employs iterative query rewriting and refinement to collect high-quality evidence, thereby mitigating error propagation. Comprehensive experiments show that RT-RAG substantially outperforms state-of-the-art methods by 7.0% F1 and 6.0% EM, demonstrating the effectiveness of RT-RAG in complex multi-hop QA.

</details>


### [29] [One LLM to Train Them All: Multi-Task Learning Framework for Fact-Checking](https://arxiv.org/abs/2601.11293)
*Malin Astrid Larsson,Harald Fosen Grunnaleite,Vinay Setty*

Main category: cs.CL

TL;DR: 用多任务学习在小型解码器LLMs上同时完成三项FEC任务，能在零-shot/少样本情境下带来显著相对增益（分别约44%、54%、31%），但并非对所有任务都优于单任务基线；具成本效益且附带实用指南。


<details>
  <summary>Details</summary>
Motivation: 当前自动化事实核查（AFC）通常由独立组件组成，依赖大而封闭的模型带来高成本与可持续性问题；开放权重的小模型若单独为每项任务微调成本仍高。通过多任务学习在单一模型上联合学习 claim detection、evidence ranking、stance detection，有望降低成本、提高一致性，并提升整体性能。

Method: 以小型解码器LLM（如Qwen3-4b）为基础，研究三种MTL策略：分类头、因果语言建模头、指令调优；在不同模型规模、任务顺序及与非LLM基线的比较中评估其效果。涉及的任务为断言检测、证据排序/排名、立场检测等。

Result: MTL模型在零/少样本设定下具显著相对增益：claim detection提升约44%、evidence re-ranking提升约54%、stance detection提升约31%；但并非在所有设置下都普遍优于单任务基线。

Conclusion: 在小型LLM上采用MTL是可行且具成本效益的AFC解决方案，能够提升多任务协作效果，但成效受任务与训练设置影响，应结合具体场景给出操作性指南，帮助从业者将MTL应用于AFC。

Abstract: Large language models (LLMs) are reshaping automated fact-checking (AFC) by enabling unified, end-to-end verification pipelines rather than isolated components. While large proprietary models achieve strong performance, their closed weights, complexity, and high costs limit sustainability. Fine-tuning smaller open weight models for individual AFC tasks can help but requires multiple specialized models resulting in high costs. We propose \textbf{multi-task learning (MTL)} as a more efficient alternative that fine-tunes a single model to perform claim detection, evidence ranking, and stance detection jointly. Using small decoder-only LLMs (e.g., Qwen3-4b), we explore three MTL strategies: classification heads, causal language modeling heads, and instruction-tuning, and evaluate them across model sizes, task orders, and standard non-LLM baselines. While multitask models do not universally surpass single-task baselines, they yield substantial improvements, achieving up to \textbf{44\%}, \textbf{54\%}, and \textbf{31\%} relative gains for claim detection, evidence re-ranking, and stance detection, respectively, over zero-/few-shot settings. Finally, we also provide practical, empirically grounded guidelines to help practitioners apply MTL with LLMs for automated fact-checking.

</details>


### [30] [Membership Inference on LLMs in the Wild](https://arxiv.org/abs/2601.11314)
*Jiatong Yi,Yanyang Li*

Main category: cs.CL

TL;DR: SimMIA提出一种面向文本输出的鲁棒成员推断攻击框架，在黑盒场景下通过改进采样策略和评分机制实现对比于访问内部信息的基线的竞争力，并引入WikiMIA-25基准评估现代专有LLM。


<details>
  <summary>Details</summary>
Motivation: 解决在完全黑盒文本输出下，现有MIAs对领域迁移的泛化能力差及对内部信息依赖的问题；需要一个能在文本层面获得高效攻击的框架，同时评估在现代专有LLM上的表现。

Method: 提出SimMIA框架，利用先进的采样策略和评分机制实现文本只输出的成员推断；并构建WikiMIA-25基准用于评估不同模型的MIA性能。

Result: 在黑盒设置中达到SOTA水平，甚至可与利用内部信息的基线相媲美；实验验证了在现代专有LLM上的有效性。

Conclusion: SimMIA显著缩小黑盒MIA与白盒MIA之间的差距；WikiMIA-25提供了对现代专有LLM的系统化评估平台。

Abstract: Membership Inference Attacks (MIAs) act as a crucial auditing tool for the opaque training data of Large Language Models (LLMs). However, existing techniques predominantly rely on inaccessible model internals (e.g., logits) or suffer from poor generalization across domains in strict black-box settings where only generated text is available. In this work, we propose SimMIA, a robust MIA framework tailored for this text-only regime by leveraging an advanced sampling strategy and scoring mechanism. Furthermore, we present WikiMIA-25, a new benchmark curated to evaluate MIA performance on modern proprietary LLMs. Experiments demonstrate that SimMIA achieves state-of-the-art results in the black-box setting, rivaling baselines that exploit internal model information.

</details>


### [31] [F-Actor: Controllable Conversational Behaviour in Full-Duplex Models](https://arxiv.org/abs/2601.11329)
*Maike Züfle,Ondrej Klejch,Nicholas Sanders,Jan Niehues,Alexandra Birch,Tsz Kin Lam*

Main category: cs.CL

TL;DR: 开放、可跟指令的全双工对话语音模型：冻结音频编码器、仅微调语言模型，在学术资源条件下用约2000小时数据实现高效训练，支持对发声、话题、对话行为和发起等的指令控制，并将发布代码以促进复现研究。


<details>
  <summary>Details</summary>
Motivation: 当前口语对话系统缺乏可定制性与动态上下文适应，难以实现自然、具参与性的对话体验；需要一种在有限资源下可控、可复现的对话语音模型。

Method: 冻结音频编码器，仅微调语言模型；单阶段训练；使用约2000小时数据；不依赖大规模预训练或多阶段优化；通过显式指令控制发声、话题、对话行为（如回话、打断）以及对话启动；提出单阶段训练协议并分析设计选择；计划公开模型与训练代码以便复现。

Result: 在典型学术资源条件下实现高效训练，模型具备对话控制能力，且为未来的可重复研究提供实现路径与基线。

Conclusion: 为可控的全双工对话语音系统的可重复研究奠定基础，并促进在资源受限环境中的自然、互动性强的语音对话系统的发展。

Abstract: Spoken conversational systems require more than accurate speech generation to have human-like conversations: to feel natural and engaging, they must produce conversational behaviour that adapts dynamically to the context. Current spoken conversational systems, however, rarely allow such customization, limiting their naturalness and usability. In this work, we present the first open, instruction-following full-duplex conversational speech model that can be trained efficiently under typical academic resource constraints. By keeping the audio encoder frozen and finetuning only the language model, our model requires just 2,000 hours of data, without relying on large-scale pretraining or multi-stage optimization. The model can follow explicit instructions to control speaker voice, conversation topic, conversational behaviour (e.g., backchanneling and interruptions), and dialogue initiation. We propose a single-stage training protocol and systematically analyze design choices. Both the model and training code will be released to enable reproducible research on controllable full-duplex speech systems.

</details>


### [32] [Idea First, Code Later: Disentangling Problem Solving from Code Generation in Evaluating LLMs for Competitive Programming](https://arxiv.org/abs/2601.11332)
*Sama Hadhoud,Alaa Elsetohy,Frederikus Hudi,Jan Christian Blaise Cruz,Steven Halim,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 提出以自然语言编辑器（editorial）为中心的评测框架，将解题能力与代码实现分离，先生成高质量编辑稿再编码；并引入83道 ICPC 样式题目的黄金编辑稿及完整测试用例，评估19位 LLMs。


<details>
  <summary>Details</summary>
Motivation: 现有评测混淆了算法推理与代码实现，无法单独评估问题求解能力。通过编辑稿优先策略和以人类专家标注的对比，明确把解题推理与实现分离，以提升评测的可解释性和可扩展性。

Method: 先在求解前提供编辑稿（可能为黄金稿），再给出代码实现；将模型生成的编辑稿与黄金编辑稿进行对比，并由专家标注评估推理错误；验证将 LLM 作为评测者的可扩展性；构建包含 83 道 ICPC‑风格题目、黄金编辑稿和完整测试用例的数据集；对19种 LLM 进行评测。

Result: 在某些模型中，编辑稿优先策略确实提高了解题通过率，且使用黄金编辑稿时提升显著；但即便有黄金编辑稿，模型仍对实现存在较大困难；生成的编辑稿与黄金编辑稿之间的差距揭示了解题瓶颈在于正确且完整地指定算法；通过专家注释对比及 LLM 作为评测者的方案得到初步验证；数据集提供且可用来推动未来基准。

Conclusion: 未来的基准应显式将求解能力与实现能力分离；编辑稿优先的评测框架有价值，但仍需着力改进实现阶段的能力与鲁棒性，并通过专家标注和对比分析持续诊断推理错误。

Abstract: Large Language Models (LLMs) increasingly succeed on competitive programming problems, yet existing evaluations conflate algorithmic reasoning with code-level implementation. We argue that competitive programming is fundamentally a problem-solving task and propose centering natural-language editorials in both solution generation and evaluation. Generating an editorial prior to code improves solve rates for some LLMs, with substantially larger gains when using expertly written gold editorials. However, even with gold editorials, models continue to struggle with implementation, while the gap between generated and gold editorials reveals a persistent problem-solving bottleneck in specifying correct and complete algorithms. Beyond pass/fail metrics, we diagnose reasoning errors by comparing model-generated editorials to gold standards using expert annotations and validate an LLM-as-a-judge protocol for scalable evaluation. We introduce a dataset of 83 ICPC-style problems with gold editorials and full test suites, and evaluate 19 LLMs, arguing that future benchmarks should explicitly separate problem solving from implementation.

</details>


### [33] [Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models](https://arxiv.org/abs/2601.11340)
*Guoming Ling,Zhongzhan Huang,Yupei Lin,Junxin Li,Shanshan Zhong,Hefeng Wu,Liang Lin*

Main category: cs.CL

TL;DR: NCoTS把推理过程转化为寻找最优思考策略的动态搜索，利用双因素启发式在正确性与计算成本之间权衡，发现稀疏但优越的推理路径，能在准确性提升与输出长度削减之间实现帕累托改进。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型链式推理通常逐步生成推理过程，缺乏前瞻性，易陷入次优路径并产生冗长无效的步骤，因此需要更高效、可控且更准确的推理策略。

Method: 将推理建模为动态搜索，量化解空间，发现稀疏且更优的推理路径；使用双因素启发式同时优化正确性和计算成本，主动导航至这些路径。

Result: 在多项推理基准上实现帕累托改进，准确性提升超过3.5%，生成长度减少超过22%。

Conclusion: 证明了最优推理路径具有稀疏性，可通过搜索发现并平衡准确性与计算资源，从而实现更高效且更精炼的推理；并提供代码与数据。

Abstract: Chain-of-Thought reasoning has significantly enhanced the problem-solving capabilities of Large Language Models. Unfortunately, current models generate reasoning steps sequentially without foresight, often becoming trapped in suboptimal reasoning paths with redundant steps. In contrast, we introduce Neural Chain-of-Thought Search (NCoTS), a framework that reformulates reasoning as a dynamic search for the optimal thinking strategy. By quantitatively characterizing the solution space, we reveal the existence of sparse superior reasoning paths that are simultaneously more accurate and concise than standard outputs. Our method actively navigates towards these paths by evaluating candidate reasoning operators using a dual-factor heuristic that optimizes for both correctness and computational cost. Consequently, NCoTS achieves a Pareto improvement across diverse reasoning benchmarks, boosting accuracy by over 3.5% while reducing generation length by over 22%. Our code and data are available at https://github.com/MilkThink-Lab/Neural-CoT-Search.

</details>


### [34] [How Much Would a Clinician Edit This Draft? Evaluating LLM Alignment for Patient Message Response Drafting](https://arxiv.org/abs/2601.11344)
*Parker Seegmiller,Joseph Gatto,Sarah E. Greer,Ganza Belise Isingizwe,Rohan Ray,Timothy E. Burdick,Sarah Masud Preum*

Main category: cs.CL

TL;DR: LLMs在 drafting 患者门户信息方面具有潜力，但与临床医生偏好的一致性存在显著不确定性；通过主题驱动的自适应可在大多数主题上提升，但在引导患者提供信息等方面仍有不足；需对个体医生进行定制以实现可靠、负责任的使用。


<details>
  <summary>Details</summary>
Motivation: 研究将LLMs引入患者-医生沟通工作流的潜力与风险，评估是否真正节省医生工作量；提出新的主题要素分类、评估框架和专家标注数据集，促进安全、可控的应用。

Method: 建立临床回应的主题要素分类法，提出在内容和主题层面评估医生对LLM草拟回复的编辑负荷的评估框架；发布专家标注数据集；对本地与商用LLMs进行大规模评估，采用主题提示、检索增强生成、监督微调、直接偏好优化等适配技术。

Result: 在对齐方面存在显著的知识不确定性；LLMs在某些主题上能草拟，但在需要与临床偏好对齐的其他主题（尤其为获取更多信息而提问）方面存在困难；基于主题的适配策略在大多数主题上实现改进。

Conclusion: 需将LLMs逐个医生的偏好进行定制化，才能在患者-医生沟通工作流中实现可靠、负责任的应用。

Abstract: Large language models (LLMs) show promise in drafting responses to patient portal messages, yet their integration into clinical workflows raises various concerns, including whether they would actually save clinicians time and effort in their portal workload. We investigate LLM alignment with individual clinicians through a comprehensive evaluation of the patient message response drafting task. We develop a novel taxonomy of thematic elements in clinician responses and propose a novel evaluation framework for assessing clinician editing load of LLM-drafted responses at both content and theme levels. We release an expert-annotated dataset and conduct large-scale evaluations of local and commercial LLMs using various adaptation techniques including thematic prompting, retrieval-augmented generation, supervised fine-tuning, and direct preference optimization. Our results reveal substantial epistemic uncertainty in aligning LLM drafts with clinician responses. While LLMs demonstrate capability in drafting certain thematic elements, they struggle with clinician-aligned generation in other themes, particularly question asking to elicit further information from patients. Theme-driven adaptation strategies yield improvements across most themes. Our findings underscore the necessity of adapting LLMs to individual clinician preferences to enable reliable and responsible use in patient-clinician communication workflows.

</details>


### [35] [Reward Modeling for Scientific Writing Evaluation](https://arxiv.org/abs/2601.11374)
*Furkan Şahinuç,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出一种两阶段训练的开源评估模型，用于科学写作评价，具备跨任务泛化与对动态评分准则的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估模型多针对通用基准，难以在科学领域的稀缺知识和任务特定多维指标上进行可靠推理，且对每个任务微调成本高。

Method: 采用两阶段训练框架：第一阶段优化科学评估偏好，第二阶段强化推理能力；设计多方面评估指标并进行跨任务联合训练，以实现细粒度评估和对动态评分的鲁棒性。

Result: 实验表明，该训练框架显著提升了基于LLM的科学写作评价性能，且模型能在未见任务/设置下泛化，单一评估器可在无任务特定再训练的情况下重复使用。

Conclusion: 提供一种成本高效、可扩展且开源的科学写作评价方案，适用于多种任务与动态评估标准。

Abstract: Scientific writing is an expert-domain task that demands deep domain knowledge, task-specific requirements and reasoning capabilities that leverage the domain knowledge to satisfy the task specifications. While scientific text generation has been widely studied, its evaluation remains a challenging and open problem. It is critical to develop models that can be reliably deployed for evaluating diverse open-ended scientific writing tasks while adhering to their distinct requirements. However, existing LLM-based judges and reward models are primarily optimized for general-purpose benchmarks with fixed scoring rubrics and evaluation criteria. Consequently, they often fail to reason over sparse knowledge of scientific domains when interpreting task-dependent and multi-faceted criteria. Moreover, fine-tuning for each individual task is costly and impractical for low-resource settings. To bridge these gaps, we propose cost-efficient, open-source reward models tailored for scientific writing evaluation. We introduce a two-stage training framework that initially optimizes scientific evaluation preferences and then refines reasoning capabilities. Our multi-aspect evaluation design and joint training across diverse tasks enable fine-grained assessment and robustness to dynamic criteria and scoring rubrics. Experimental analysis shows that our training regime strongly improves LLM-based scientific writing evaluation. Our models generalize effectively across tasks and to previously unseen scientific writing evaluation settings, allowing a single trained evaluator to be reused without task-specific retraining.

</details>


### [36] [Relational Linearity is a Predictor of Hallucinations](https://arxiv.org/abs/2601.11429)
*Yuetian Lu,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本研究揭示模型对未知合成实体的回答中存在较高的幻觉率与关系线性度之间的强相关性，提出关系线性存储影响知识自我评估的假设。


<details>
  <summary>Details</summary>
Motivation: LLMs经常在回答未知知识时出错，现有解释为何模型难以自我知晓其知识的边界；通过引入合成实体和线性度测度来系统地研究关系结构对幻觉的影响。

Method: 构建 SyntHal 数据集：6 条关系，6000 个合成实体；在四个模型上评估每个关系的幻觉率，使用 Δcos 衡量线性度；比较不同关系的幻觉率与线性度的相关性。

Result: 观察到各关系的幻觉率与 Δcos 存在强相关性，相关系数 r 在 0.78-0.82 之间；支持假设：关系的存储形式（线性 vs 非线性）影响模型对其知识的自我评估能力。

Conclusion: 存储结构对幻觉行为有显著影响，提示在提升事实知识表达和自我评估方面需要关注关系的线性性质，为未来改进知识表示和误信息管理提供方向。

Abstract: Hallucination is a central failure mode in large language models (LLMs). We focus on hallucinations of answers to questions like: "Which instrument did Glenn Gould play?", but we ask these questions for synthetic entities that are unknown to the model. Surprisingly, we find that medium-size models like Gemma-7B-IT frequently hallucinate, i.e., they have difficulty recognizing that the hallucinated fact is not part of their knowledge. We hypothesize that an important factor in causing these hallucinations is the linearity of the relation: linear relations tend to be stored more abstractly, making it difficult for the LLM to assess its knowledge; the facts of nonlinear relations tend to be stored more directly, making knowledge assessment easier. To investigate this hypothesis, we create SyntHal, a dataset of 6000 synthetic entities for six relations. In our experiments with four models, we determine, for each relation, the hallucination rate on SyntHal and also measure its linearity, using $Δ\cos$. We find a strong correlation ($r \in [.78,.82]$) between relational linearity and hallucination rate, providing evidence for our hypothesis that the underlying storage of triples of a relation is a factor in how well a model can self-assess its knowledge. This finding has implications for how to manage hallucination behavior and suggests new research directions for improving the representation of factual knowledge in LLMs.

</details>


### [37] [The unreasonable effectiveness of pattern matching](https://arxiv.org/abs/2601.11432)
*Gary Lupyan,Blaise Agüera y Arcas*

Main category: cs.CL

TL;DR: LLMs can extract and recover semantic meaning from Jabberwocky-like input by exploiting structural patterns, supporting the view that pattern-matching, not mere memorization, underpins their behavior.


<details>
  <summary>Details</summary>
Motivation: Clarify what LLMs are doing (language mimic vs. database vs. web-inspired) by testing their ability to infer meaning from syntactic structure when content words are scrambled.

Method: Prompt LLMs with sentences where content words are replaced by nonsense tokens (e.g., Jabberwocky) and assess whether outputs reflect plausible semantic interpretations or translations; analyze recovery of meaning from structure.

Result: LLMs demonstrate the ability to translate nonsense sentences into meaningful interpretations (e.g., 'He dwushed a ghanc zawk' → 'He dragged a spare chair'), indicating robust sensitivity to structural patterns.

Conclusion: Pattern-matching is a central ingredient of LLM behavior, contributing to semantic sensitivity; this does not imply full real-world intelligence, but helps explain the 'unreasonable effectiveness' of LLMs and their reliance on pattern structure.

Abstract: We report on an astonishing ability of large language models (LLMs) to make sense of "Jabberwocky" language in which most or all content words have been randomly replaced by nonsense strings, e.g., translating "He dwushed a ghanc zawk" to "He dragged a spare chair". This result addresses ongoing controversies regarding how to best think of what LLMs are doing: are they a language mimic, a database, a blurry version of the Web? The ability of LLMs to recover meaning from structural patterns speaks to the unreasonable effectiveness of pattern-matching. Pattern-matching is not an alternative to "real" intelligence, but rather a key ingredient.

</details>


### [38] [Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models](https://arxiv.org/abs/2601.11441)
*Xiaojie Gu,Guangxu Chen,Yuheng Yang,Jingxin Han,Andi Zhang*

Main category: cs.CL

TL;DR: 提出 HORSE 方法，通过层级正交残差扩散信息矩阵实现高效、稳定的模型编辑，并在两数据集、多个 LLM 上进行理论对比与广泛实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑多聚焦于优化信息矩阵以混合新旧知识，容易带来高计算成本与潜在冲突，需更高效稳健的编辑策略。

Method: 提出 HORSE（Hierarchical Orthogonal Residual Spread of the information matrix），通过对信息矩阵进行分层正交残差扩散以降低噪声梯度，提升编辑稳定性，并给出与主流方法的理论对比。

Result: 在跨场景的两数据集和多种 LLM 上， HORSE 能实现对大规模知识的准确编辑，且表现稳定，优于或等同于现有方法。

Conclusion: HORSE 提供了一个稳定高效的模型编辑范式，并且代码开源，便于复现与应用。

Abstract: Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause conflicts. In contrast, we shift our attention to Hierarchical Orthogonal Residual SprEad of the information matrix, which reduces noisy gradients and enables more stable edits from a different perspective. We demonstrate the effectiveness of our method HORSE through a clear theoretical comparison with several popular methods and extensive experiments conducted on two datasets across multiple LLMs. The results show that HORSE maintains precise massive editing across diverse scenarios. The code is available at https://github.com/XiaojieGu/HORSE

</details>


### [39] [Predict the Retrieval! Test time adaptation for Retrieval Augmented Generation](https://arxiv.org/abs/2601.11443)
*Xin Sun,Zhongqi Chen,Qiang Liu,Shu Wu,Bowen Song,Weiqiang Wang,Zilei Wang,Liang Wang*

Main category: cs.CL

TL;DR: TTARAG introduces test-time adaptation for retrieval-augmented generation by enabling the model to predict retrieved content and update its parameters during inference, improving performance in specialized domains.


<details>
  <summary>Details</summary>
Motivation: RAG systems struggle with distribution shifts when applied to domain-specific tasks, leading to degraded generalization. A lightweight, in‑inference adaptation strategy is needed to tailor models to target domains without full retraining.

Method: During inference, TTARAG trains the model to predict the retrieved content, which enables automatic parameter updates to align with the target domain, effectively performing test-time adaptation for RAG.

Result: Empirical evaluation across six specialized domains shows substantial improvements over baseline RAG systems; code is released at the provided URL.

Conclusion: Test-time parameter adaptation via content prediction is an effective and practical solution to improve RAG performance in specialized domains.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language models' question-answering capabilities through the integration of external knowledge. However, when adapting RAG systems to specialized domains, challenges arise from distribution shifts, resulting in suboptimal generalization performance. In this work, we propose TTARAG, a test-time adaptation method that dynamically updates the language model's parameters during inference to improve RAG system performance in specialized domains. Our method introduces a simple yet effective approach where the model learns to predict retrieved content, enabling automatic parameter adjustment to the target domain. Through extensive experiments across six specialized domains, we demonstrate that TTARAG achieves substantial performance improvements over baseline RAG systems. Code available at https://github.com/sunxin000/TTARAG.

</details>


### [40] [CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation](https://arxiv.org/abs/2601.11488)
*Vanshali Sharma,Andrea Mia Bejar,Gorkem Durak,Ulas Bagci*

Main category: cs.CL

TL;DR: 提出一个统一的评估框架CTest-Metric，用于评估CT影像报告生成（RRG）中的评价指标。通过三个模块（写作风格泛化、合成错误注入、指标与专家相关性）对八种指标在七个基于CT-CLIP的LLM上的鲁棒性与相关性进行系统评估，结果显示：词汇驱动的NLG指标对风格敏感性高；GREEN Score与专家判断的相关性最好，CRG相关性为负；BERTScore-F1对事实错误注入鲁棒性最低。计划发布框架、代码与部分匿名化数据以支持复现与后续研究。


<details>
  <summary>Details</summary>
Motivation: 当前RRG领域对质量评估高度依赖不理想的指标，缺乏统一的、面向临床场景的鲁棒性与可适用性评估框架，亟需系统化基准以促进指标改进与比较。

Method: 设计三模块评估：1) Writing Style Generalizability(WSG)：通过LLM对CT报告进行再表述以测试风格的普适性；2) Synthetic Error Injection(SEI)：对报告注入分级严重度的合成错误；3) Metrics-vs-Expert correlation(MvE)：以临床医生对175例“分歧案例”的评分评估指标与专家之间的相关性。对八种指标（BLEU、ROUGE、METEOR、BERTScore-F1、F1-RadGraph、RaTEScore、GREEN Score、CRG）在七个基于CT-CLIP的LLM上进行比较。

Result: 关键发现包括：词汇驱动的NLG指标对写作风格高度敏感；GREEN Score与专家判断的相关性最好（Spearman约0.70），CRG呈现负相关；BERTScore-F1对注入的事实错误最不敏感。

Conclusion: 提出的CTest-Metric框架可用于统一基准评测并推动指标开发；作者计划发布框架、代码及部分匿名化评估数据以促进复现与未来工作。

Abstract: In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 "disagreement" cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.

</details>


### [41] [Do explanations generalize across large reasoning models?](https://arxiv.org/abs/2601.11517)
*Koyena Pal,David Bau,Chandan Singh*

Main category: cs.CL

TL;DR: CoT explanations from LRMs often generalize across models, increasing inter-model consistency, and this generalization correlates with human preferences and RLHF. A simple sentence-level ensembling strategy can further improve consistency. Caution is advised in drawing new insights from LRM explanations; a framework for evaluating explanation generalization is proposed.


<details>
  <summary>Details</summary>
Motivation: To determine whether explanations produced by one LRM reflect general problem structure (and thus transfer across LRMs) rather than model-specific quirks, which is critical for using explanations to discover new concepts in AI for science.

Method: Empirical evaluation across multiple LRMs: measure how explanations from one model influence or align the behavior of other LRMs; assess the relationship between explanation-induced consistency and human preference rankings and RLHF. Develop and test a sentence-level ensembling strategy to enhance cross-model consistency. Analyze conditions under which explanations yield consistent answers.

Result: CoT explanations often increase cross-model consistency, and this increase correlates with human preference rankings and RLHF. A straightforward sentence-level ensembling approach further improves consistency. The work also analyzes conditions that affect explanation reliability and proposes a framework for characterizing LRM explanation generalization.

Conclusion: LRM explanations can generalize to other models under certain conditions, but caution is warranted when inferring new insights. The paper provides a framework to characterize explanation generalization and offers a practical ensembling technique to boost consistency.

Abstract: Large reasoning models (LRMs) produce a textual chain of thought (CoT) in the process of solving a problem, which serves as a potentially powerful tool to understand the problem by surfacing a human-readable, natural-language explanation. However, it is unclear whether these explanations generalize, i.e. whether they capture general patterns about the underlying problem rather than patterns which are esoteric to the LRM. This is a crucial question in understanding or discovering new concepts, e.g. in AI for science. We study this generalization question by evaluating a specific notion of generalizability: whether explanations produced by one LRM induce the same behavior when given to other LRMs. We find that CoT explanations often exhibit this form of generalization (i.e. they increase consistency between LRMs) and that this increased generalization is correlated with human preference rankings and post-training with reinforcement learning. We further analyze the conditions under which explanations yield consistent answers and propose a straightforward, sentence-level ensembling strategy that improves consistency. Taken together, these results prescribe caution when using LRM explanations to yield new insights and outline a framework for characterizing LRM explanation generalization.

</details>


### [42] [How Long Is a Piece of String? A Brief Empirical Analysis of Tokenizers](https://arxiv.org/abs/2601.11518)
*Jonathan Roberts,Kai Han,Samuel Albanie*

Main category: cs.CL

TL;DR: Tokenization varies significantly across models and domains; naive token-length heuristics are simplistic; empirical analysis reveals how text is compressed into tokens across distributions.


<details>
  <summary>Details</summary>
Motivation: Token units (tokens) are used for model comparisons and pricing, but tokenizer behavior differs by model and domain, undermining cross-model comparability.

Method: Empirical analysis across multiple tokenizers and text distributions to measure compression of sequences into tokens and analyze token-length heuristics.

Result: Found substantial variation in token counts for similar texts across models; token-length heuristics are overly simplistic; tokenization depends on distribution and domain.

Conclusion: Tokenization is not a universal constant; researchers should account for tokenizer differences to enable fair comparisons and pricing; results provide intuition for contemporary LLM tokenization.

Abstract: Frontier LLMs are increasingly utilised across academia, society and industry. A commonly used unit for comparing models, their inputs and outputs, and estimating inference pricing is the token. In general, tokens are used as a stable currency, assumed to be broadly consistent across tokenizers and contexts, enabling direct comparisons. However, tokenization varies significantly across models and domains of text, making naive interpretation of token counts problematic. We quantify this variation by providing a comprehensive empirical analysis of tokenization, exploring the compression of sequences to tokens across different distributions of textual data. Our analysis challenges commonly held heuristics about token lengths, finding them to be overly simplistic. We hope the insights of our study add clarity and intuition toward tokenization in contemporary LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models](https://arxiv.org/abs/2601.10719)
*Gerard Yeo,Svetlana Churina,Kokil Jaidka*

Main category: cs.AI

TL;DR: LLMs encode perceived trustworthiness in web-like narratives without explicit supervision; trust-related representations arise during pretraining and can be linearly decoded, with strongest ties to fairness, certainty, and accountability-self.


<details>
  <summary>Details</summary>
Motivation: Address how instruction-tuned LLMs embed psychologically grounded trust signals, informing design of credible and transparent AI in the web ecosystem.

Method: Examine Llama 3.1 8B, Qwen 2.5 7B, and Mistral 7B on the PEACE-Reviews dataset (annotated for cognitive appraisals, emotions, and behavioral intentions). Analyze layer- and head-level activations to distinguish high- vs low-trust texts; perform probing to test linear decodability; assess effects of fine-tuning.

Result: Across models, activation differences at layer- and head-level differentiate high- from low-trust narratives; trust cues are implicitly encoded during pretraining; probing shows linearly decodable trust signals; fine-tuning refines rather than restructures representations; strongest associations with appraisals of fairness, certainty, and accountability-self.

Conclusion: Modern instruction-tuned LLMs internalize psychologically grounded trust signals, providing a representational basis for designing credible, transparent, and trustworthy AI in the web ecosystem; code and appendix available at GitHub.

Abstract: Perceived trustworthiness underpins how users navigate online information, yet it remains unclear whether large language models (LLMs),increasingly embedded in search, recommendation, and conversational systems, represent this construct in psychologically coherent ways. We analyze how instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B) encode perceived trustworthiness in web-like narratives using the PEACE-Reviews dataset annotated for cognitive appraisals, emotions, and behavioral intentions. Across models, systematic layer- and head-level activation differences distinguish high- from low-trust texts, revealing that trust cues are implicitly encoded during pretraining. Probing analyses show linearly de-codable trust signals and fine-tuning effects that refine rather than restructure these representations. Strongest associations emerge with appraisals of fairness, certainty, and accountability-self -- dimensions central to human trust formation online. These findings demonstrate that modern LLMs internalize psychologically grounded trust signals without explicit supervision, offering a representational foundation for designing credible, transparent, and trust-worthy AI systems in the web ecosystem. Code and appendix are available at: https://github.com/GerardYeo/TrustworthinessLLM.

</details>


### [44] [Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration](https://arxiv.org/abs/2601.10744)
*Sen Wang,Bangwei Liu,Zhenkun Gao,Lizhuang Ma,Xuhong Wang,Yuan Xie,Xin Tan*

Main category: cs.AI

TL;DR: 提出 Long-term Memory Embodied Exploration (LMEE) 及其评估基准 LMEE-Bench，通过 MemoryExplorer 使用强化学习对多模态大型语言模型进行微调，以实现主动记忆查询和长期探索，从而提升对长期、复杂任务的终身学习性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的一-shot embodied tasks 更关注任务完成结果，忽视探索过程与记忆利用，缺乏对长期记忆的利用以优化决策。需要在真实环境中实现终身学习能力，处理长时序任务。

Method: 提出 LMEE 框架，将探索认知与决策行为统一，构建 LMEE-Bench 数据集涵盖多目标导航与基于记忆的问题回答，评估探索过程与结果。提出 MemoryExplorer，通过对多模态大模型进行策略性强化学习微调，结合行动预测、前沿选择、问答三项多任务奖励，促使主动记忆查询与前向探索。

Result: 与最先进的 embodied exploration 模型相比，在长时域任务上具有显著优势，显示了提升长期探索能力与记忆利用的效果。

Conclusion: LMEE 框架有效融合探索与记忆，促进 embodiments 的终身学习；LMEE-Bench 提供全面的评估资源；MemoryExplorer 实现了记忆驱动的主动探索。

Abstract: An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.

</details>


### [45] [Optimisation of complex product innovation processes based on trend models with three-valued logic](https://arxiv.org/abs/2601.10768)
*Nina Bočková,Barbora Volná,Mirko Dohnal*

Main category: cs.AI

TL;DR: 用极简信息量的趋势（上升/下降/不变）构建复杂产品创新的情景转移图，路径代表系统的任意历史或未来行为。


<details>
  <summary>Details</summary>
Motivation: 解决产品创新过程的复杂性，避免对数值数据或粗糙集合的依赖，通过简化趋势来获得可比拟、可推演的框架。

Method: 将每个启发式以简单趋势形式表达，构建包含可能转移的情景集合的转移图，系统的任意行为可表示为该图上的路径。

Result: 提出一种以转移图描述系统时序行为的框架，允许通过路径来表示所有可能的未来或过去行为。

Conclusion: 该框架以最小信息量实现对复杂创新过程的情景分析与推演，具有直观性和跨领域适用性。

Abstract: This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends -- increasing, decreasing, or constant -- which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.

</details>


### [46] [What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge](https://arxiv.org/abs/2601.10922)
*Yosub Shin,Michael Buriek,Boris Sobolev,Pavel Bushuyeu,Vikas Kumar,Haoyang Xu,Samuel Watson,Igor Molybog*

Main category: cs.AI

TL;DR: 在固定模型和训练协议下，通过难度驱动的样本选择实现数据高效的多模态推理，结果显示对齐与难度是核心驱动；增量数据量对平均准确度收益有限，主要降低方差，常用的多样性与合成增强策略常无效甚至降效。


<details>
  <summary>Details</summary>
Motivation: 探究数据筛选对多模态推理性能的影响，特别是在固定训练条件下，数据质量与难度分布是否优于简单扩充。

Method: 以 NeurIPS 2025 DCVLR 挑战为评测场景，构建来自 Walton Multimodal Cold Start 的紧凑数据集，进行难度基准的样本选择与在对齐基线上的 ablations，比较数据量、 diversity、synthetic augmentation 的影响。

Result: 提交夺冠；难度驱动的示例选择成为主要性能提升驱动；在固定训练配方下，增大数据集规模并未显著提升平均准确度，主要降低了随机波动；多样性和合成增强策略无明显益处，甚至可能降低性能。

Conclusion: DCVLR 呈现饱和式评估特征，数据对齐和难度控制在数据高效的多模态推理中占核心地位。

Abstract: We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.

</details>


### [47] [Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics](https://arxiv.org/abs/2601.11012)
*Jiahao Wang,Shuangjia Zheng*

Main category: cs.AI

TL;DR: HADES 是一个基于哈密顿动力学的贝叶斯优化框架，用于在考虑结构约束的情况下高效从连续潜在空间采样并离散化成蛋白质序列，以实现对结构-序列耦合的优化设计。


<details>
  <summary>Details</summary>
Motivation: 解决依赖仅序列信息的高维优化在表观性（epistasis）及结构约束方面的局限，需利用结构信息以提高搜索效率与可设计性。

Method: 在结构感知的近似后验上进行哈密顿动力学采样，利用动量与不确定性推动提案更快收敛到有利区域；引入离散化程序将连续状态转化为离散蛋白质序列；使用两阶段编码-解码器框架建模变体邻域之间的结构-功能关系，学习一个平滑的适应景观以供采样；结合贝叶斯优化进行高效探索。

Result: 在大多数内在评估基准上优于现有最先进方法；通过结构与序列的互制约，实现设计出结构相近、具有优化性质的蛋白质序列；代码与数据公开可用。

Conclusion: 将结构信息融入序列优化能显著提升探索效率与设计质量，HADES 提供一个可扩展的结构感知贝叶斯优化框架及其实验证据，未来可扩展至更复杂的结构约束与实际实验验证。

Abstract: The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such a continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties. The code and data are publicly available at https://github.com/GENTEL-lab/HADES.

</details>


### [48] [Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems](https://arxiv.org/abs/2601.11147)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出 SCALE 框架，通过少样本标定的自预测进行任务级评估，以降低成本并保持竞争性能；实验表明，若以前K个最佳任务级工作流即可覆盖大部分查询，且全量执行评估成本高且不稳定。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中基于大语言模型的工作流成本与可靠性问题，特别是任务级与查询级工作流生成的取舍，以及全面执行验证的高成本与不稳定性。

Method: 提出 SCALE：自预测优化器并结合少量示例进行标定的评估（few-shot calibration for evaluation），不进行全量验证执行；重心在任务级工作流的前K覆盖，减少对查询级生成的依赖。

Result: 在多个数据集上，平均性能下降0.61%，总体 token 使用量下降至多83%。

Conclusion: 提供一个低成本、对性能影响最小的任务级工作流生成框架，适用于基于大语言模型的多智能体系统，显著降低评估成本与提高稳定性。

Abstract: Multi-Agent Systems (MAS) built on large language models typically solve complex tasks by coordinating multiple agents through workflows. Existing approaches generates workflows either at task level or query level, but their relative costs and benefits remain unclear. After rethinking and empirical analyses, we show that query-level workflow generation is not always necessary, since a small set of top-K best task-level workflows together already covers equivalent or even more queries. We further find that exhaustive execution-based task-level evaluation is both extremely token-costly and frequently unreliable. Inspired by the idea of self-evolution and generative reward modeling, we propose a low-cost task-level generation framework \textbf{SCALE}, which means \underline{\textbf{S}}elf prediction of the optimizer with few shot \underline{\textbf{CAL}}ibration for \underline{\textbf{E}}valuation instead of full validation execution. Extensive experiments demonstrate that \textbf{SCALE} maintains competitive performance, with an average degradation of just 0.61\% compared to existing approach across multiple datasets, while cutting overall token usage by up to 83\%.

</details>


### [49] [TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech](https://arxiv.org/abs/2601.11178)
*Girish A. Koushik,Helen Treharne,Diptesh Kanojia*

Main category: cs.AI

TL;DR: 提出 TANDEM 框架，将音视频仇恨检测从二分类转化为结构化推理，通过串联式强化学习实现跨模态对齐，提供可解释的时间戳证据与目标识别。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态仇恨检测常作为黑箱模型，难以给出精确时间戳和目标身份等可解释证据，限制人机协作的后续干预。

Method: 提出 tandem reinforcement learning； vision-language 与 audio-language 两模态模型通过自约束的跨模态上下文互相优化，在长时序上实现稳定推理；无需密帧级监督，进行结构化推理与目标识别，提升可解释性。

Result: 在 HateMM 上达到 0.73 的 target identification F1，较 state-of-the-art 提升约 30%，并实现精确的时间定位；二分类检测具有鲁棒性，但多类场景下区分 offensive 与 hateful 仍受标签歧义与数据不均衡影响。

Conclusion: 证明在复杂的多模态场景中，通过结构化、可解释的跨模态对齐，可为透明、可操作的在线安全 moderation 提供有效范式。

Abstract: Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as "black boxes" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. Our approach employs a novel tandem reinforcement learning strategy where vision-language and audio-language models optimize each other through self-constrained cross-modal context, stabilizing reasoning over extended temporal sequences without requiring dense frame-level supervision. Experiments across three benchmark datasets demonstrate that TANDEM significantly outperforms zero-shot and context-augmented baselines, achieving 0.73 F1 in target identification on HateMM (a 30% improvement over state-of-the-art) while maintaining precise temporal grounding. We further observe that while binary detection is robust, differentiating between offensive and hateful content remains challenging in multi-class settings due to inherent label ambiguity and dataset imbalance. More broadly, our findings suggest that structured, interpretable alignment is achievable even in complex multimodal settings, offering a blueprint for the next generation of transparent and actionable online safety moderation tools.

</details>


### [50] [Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning](https://arxiv.org/abs/2601.11252)
*Qianyue Wang,Jinwu Hu,Yufeng Wang,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Mingkui Tan*

Main category: cs.AI

TL;DR: Think-with-Me is a test-time interactive reasoning paradigm that injects external feedback at transitional conjunction points to steer large reasoning models (LRMs). It uses multi-criteria (rationality, completeness) evaluation from human or LLM proxies and is trained with Group Relative Policy Optimization (GRPO). It achieves a better accuracy–reasoning-length trade-off under limited context (e.g., 8K window on AIME24), outperforming QwQ-32B by 7.19% accuracy while reducing reasoning length by 81%, and shows benefits for security and creative tasks.


<details>
  <summary>Details</summary>
Motivation: LRMs tend to overthink or overshoot, incurring high computational cost and degraded performance. Existing efficient methods operate in closed loops without external guidance. There is a need for an intervention mechanism that can guide reasoning without sacrificing accuracy.

Method: Introduce external feedback at natural transition points in reasoning (transitional conjunctions). Pause reasoning to obtain feedback, then adaptively extend or terminate the reasoning path. Feedback is produced via a multi-criteria evaluation (rationality and completeness) by humans or LLM proxies. The model is trained with Group Relative Policy Optimization (GRPO) to adapt to this interactive mode.

Result: Empirical results show Think-with-Me achieves a superior accuracy–length balance under limited context windows. On AIME24, it outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also yields benefits for security and creative tasks.

Conclusion: External feedback intervention at transition points can guide LRMs to reduce redundant reasoning while preserving or enhancing accuracy. The approach demonstrates strong efficiency gains in constrained contexts and may generalize to other task domains, though its effectiveness depends on the quality of feedback and the overhead of introducing external interventions.

Abstract: Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.

</details>


### [51] [XChoice: Explainable Evaluation of AI-Human Alignment in LLM-based Constrained Choice Decision Making](https://arxiv.org/abs/2601.11286)
*Weihong Qi,Fan Huang,Rasika Muralidharan,Jisun An,Haewoon Kwak*

Main category: cs.AI

TL;DR: XChoice 是一种可解释框架，通过拟合机制性决策模型来评估 AI–人类在约束条件下的对齐，提供可解释参数来量化决策因素、约束敏感性与权衡，并通过比较参数向量评估不同模型、选项和子群体的对齐。


<details>
  <summary>Details</summary>
Motivation: 避免仅以准确率等结果层面的对齐评估，转而通过机制参数揭示对齐的内在因素与偏差，便于诊断与改进。

Method: 对人类数据（如 ATUS）及 LLM 决策，拟合相同的机制性决策模型，得到表征决策要素权重、约束敏感性与权衡的参数向量；通过跨模型/选项/子群体比较参数向量来评估对齐。

Result: 在 ATUS 真实数据上显示对齐存在异质性，模型对齐不确定且在黑人和已婚群体中错位尤为显著；通过不变性分析验证鲁棒性，并评估以 RAG 干预的有针对性缓解。

Conclusion: 该框架提供机制层面的对齐度量，能够诊断错位并支持超越表面结果匹配的改进策略。

Abstract: We present XChoice, an explainable framework for evaluating AI-human alignment in constrained decision making. Moving beyond outcome agreement such as accuracy and F1 score, XChoice fits a mechanism-based decision model to human data and LLM-generated decisions, recovering interpretable parameters that capture the relative importance of decision factors, constraint sensitivity, and implied trade-offs. Alignment is assessed by comparing these parameter vectors across models, options, and subgroups. We demonstrate XChoice on Americans' daily time allocation using the American Time Use Survey (ATUS) as human ground truth, revealing heterogeneous alignment across models and activities and salient misalignment concentrated in Black and married groups. We further validate robustness of XChoice via an invariance analysis and evaluate targeted mitigation with a retrieval augmented generation (RAG) intervention. Overall, XChoice provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.

</details>


### [52] [Hyperparameter Optimization of Constraint Programming Solvers](https://arxiv.org/abs/2601.11389)
*Hedieh Haddad,Thibault Falque,Pierre Talbot,Pascal Bouvry*

Main category: cs.AI

TL;DR: Two-phase probe-and-solve framework for automated hyperparameter optimization in constraint solvers, using Bayesian optimization or Hamming-distance search within CPMpy; Bayesian approach outperforms defaults across ACE and Choco, with robust gains.


<details>
  <summary>Details</summary>
Motivation: Constraint programming solver performance is highly sensitive to hyperparameters; manual tuning is time-consuming and requires expertise. An automated, resource-aware approach integrated into the CPMpy library can streamline tuning and improve solution quality.

Method: A two-phase algorithm: (1) probing phase explores different hyperparameter configurations using selectable optimization methods (Bayesian optimization or Hamming distance search); (2) solving phase runs the best-found configuration on the remaining time. Implemented in CPMpy and evaluated on ACE and Choco across 114 problem instances, compared to solver defaults.

Result: Bayesian optimization outperforms defaults: ACE shows improved solution quality in 25.4% of instances and matches defaults in 57.9%; Choco shows improvements in 38.6% of instances. Bayesian optimization also consistently beats Hamming distance search within the same framework.

Conclusion: The probe-and-solve framework provides a practical, resource-aware method for tunable constraint solvers, delivering robust performance gains across diverse problem types.

Abstract: The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. In this paper, we introduce probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library. This approach partitions the available time budget into two phases: a probing phase that explores different sets of hyperparameters using configurable hyperparameter optimization methods, followed by a solving phase where the best configuration found is used to tackle the problem within the remaining time.
  We implement and compare two hyperparameter optimization methods within the probe and solve algorithm: Bayesian optimization and Hamming distance search. We evaluate the algorithm on two different constraint programming solvers, ACE and Choco, across 114 combinatorial problem instances, comparing their performance against the solver's default configurations.
  Results show that using Bayesian optimization, the algorithm outperforms the solver's default configurations, improving solution quality for ACE in 25.4% of instances and matching the default performance in 57.9%, and for Choco, achieving superior results in 38.6% of instances. It also consistently surpasses Hamming distance search within the same framework, confirming the advantage of model-based exploration over simple local search. Overall, the probe and solve algorithm offers a practical, resource-aware approach for tuning constraint solvers that yields robust improvements across diverse problem types.

</details>


### [53] [Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs](https://arxiv.org/abs/2601.11468)
*Alessandro Padella,Massimiliano de Leoni,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文扩展基于大语言模型的预测性过程监控，评估其在多KPI下的泛化、语义利用和推理能力，并在数据稀缺（仅100条轨迹）场景下仍超越基线。


<details>
  <summary>Details</summary>
Motivation: 旨在系统评估LLM驱动的预测性过程监控在多KPIs上的通用性、语义嵌入能力和推理机制，并扩展此前仅聚焦总时长预测的框架，检验其在数据受限条件下的有效性。

Method: 在此前LLM框架基础上，全面评估其泛化性、语义利用和推理机制；对三个事件日志进行实证评估，覆盖总时长与活动发生两项KPI，比较基线方法，分析LLM对先验知识与训练轨迹间相关性的利用，并研究其推理策略。

Result: 实验结果显示，在数据稀缺（100条轨迹）情况下，LLM优于基线方法；LLM能够利用嵌入的先验知识及训练轨迹之间的相关性；还表现出高阶推理能力，不仅仅复现现有预测方法。

Conclusion: 结论表明，LLM驱动的预测性过程监控在多KPI场景具备良好泛化性，并能在数据受限时通过先验知识与内部相关性实现有效推断，提示未来在推理策略与多KPIs场景深度融合的潜力。

Abstract: Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.

</details>


### [54] [Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning](https://arxiv.org/abs/2601.11479)
*Yohai Trabelsi,Guojun Xiong,Fentabil Getnet,Stéphane Verguet,Milind Tambe*

Main category: cs.AI

TL;DR: 将大语言模型与扩展贪婪算法相结合的 LEG 框架，用以在保障覆盖的同时融入专家偏好来优化埃塞俄比亚农村卫生站升级的优先级。


<details>
  <summary>Details</summary>
Motivation: 资源有限情境下需在人口覆盖与多方利益之间折中；经典优化提供理论保证但需定量目标，利益相关者的定性偏好难以直接量化，因此需要将两者有效融合。

Method: 提出 LEG 框架，结合可证明近似的人口覆盖优化算法（扩展贪婪算法）与基于大语言模型的迭代式 refinement，实现人机对齐以吸纳专家定性指导，同时保持覆盖保证。

Result: 在埃塞俄比亚三个地区的真实数据上进行实验，结果显示框架在实现公平、数据驱动的健康系统规划方面具有积极潜力，并在覆盖性与对专家偏好的一致性方面表现良好。

Conclusion: LEG 框架有效桥接定性目标与数量目标，提供可扩展的方法用于资源受限环境的卫生系统规划，具备向其他地区推广的潜力。

Abstract: Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we propose a hybrid framework that systematically integrates expert knowledge with optimization techniques. Classical optimization methods provide theoretical guarantees but require explicit, quantitative objectives, whereas stakeholder criteria are often articulated in natural language and difficult to formalize. To bridge these domains, we develop the Large language model and Extended Greedy (LEG) framework. Our framework combines a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement that incorporates human-AI alignment to ensure solutions reflect expert qualitative guidance while preserving coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.

</details>


### [55] [BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics](https://arxiv.org/abs/2601.11492)
*Kaiwen Wang,Kaili Zheng,Rongrong Deng,Qingmin Fan,Milin Zhang,Zongrui Li,Xuesi Zhou,Bo Han,Liren Chen,Chenyi Guo,Ji Wu*

Main category: cs.AI

TL;DR: BoxMind：一个闭环的拳击AI专家系统，将比赛视频解析为18项技术-战术指标，基于图模型融合显性指标与可学习潜在嵌入，预测胜负并给出战术建议，在2024年巴黎奥运期间的闭环部署验证，取得领先性能并提升国家队成绩。


<details>
  <summary>Details</summary>
Motivation: 拳击等格斗项目缺乏结构化的战术表征，难以将海量比赛视频转化为可操作的战略情报；需要将计算机视觉输出转化为对战术决策有直接用处的知识。

Method: 将原子化的拳击动作事件定义为具备时间边界、空间和技术属性的单元；将比赛视频解析成18个分层次的技术-战术指标；提出基于图的预测模型，融合显性技术指标和可学习的时间变动潜在嵌入，建模对局结果为指标的可微分函数，并通过胜负梯度形成可执行的战术调整；在封闭环路中进行部署与验证。

Result: 在 BoxerGraph 测试集上达到69.8%准确率，在奥运比赛上达到87.5%准确率，且基于此预测模型生成的战术建议与人类专家水平相当；2024年巴黎奥运会期间完成闭环部署，直接促成中国队获得三金两银。

Conclusion: BoxMind为将非结构化视频数据转化为策略性情报提供了可复现的范式，连接了计算机视觉与竞技体育决策支持。

Abstract: Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal boundaries and spatial and technical attributes, we parse match footage into 18 hierarchical technical-tactical indicators. We then propose a graph-based predictive model that fuses these explicit technical-tactical profiles with learnable, time-variant latent embeddings to capture the dynamics of boxer matchups. Modeling match outcome as a differentiable function of technical-tactical indicators, we turn winning probability gradients into executable tactical adjustments. Experiments show that the outcome prediction model achieves state-of-the-art performance, with 69.8% accuracy on BoxerGraph test set and 87.5% on Olympic matches. Using this predictive model as a foundation, the system generates strategic recommendations that demonstrate proficiency comparable to human experts. BoxMind is validated through a closed-loop deployment during the 2024 Paris Olympics, directly contributing to the Chinese National Team's historic achievement of three gold and two silver medals. BoxMind establishes a replicable paradigm for transforming unstructured video data into strategic intelligence, bridging the gap between computer vision and decision support in competitive sports.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Analytic Bijections for Smooth and Interpretable Normalizing Flows](https://arxiv.org/abs/2601.10774)
*Mathis Gerdes,Miranda C. N. Cheng*

Main category: cs.LG

TL;DR: Proposes three global analytic bijections for normalizing flows (cubic rational, sinh, cubic polynomial) that are C∞ on R with closed-form inverses, plus radial flows that transform radius while preserving angle. They show drop-in improvements in coupling flows, exceptional training stability, and high parameter efficiency on radial-structured targets. Evaluated on 1D/2D benchmarks and φ^4 lattice field theory, outperforming affine baselines and mitigating mode collapse.


<details>
  <summary>Details</summary>
Motivation: Address the trade-offs among existing scalar bijections used in normalizing flows: affine transforms (smooth but limited expressivity), monotone splines (local control but piecewise smooth and on bounded domains), and residual flows (smooth but require numerical inversion). Aim for globally smooth, analytically invertible, unbounded-domain bijections that combine expressivity with tractable Jacobians, and to introduce radial flows for problems with radial geometry.

Method: 1) Define three families of analytic bijections (cubic rational, sinh, cubic polynomial) that are globally smooth on R and have closed-form inverses; 2) integrate these as drop-in bijections in coupling flow layers; 3) develop a radial flow architecture with direct parameterization that scales the radial coordinate while preserving angular directions; 4) comprehensive experiments on 1D/2D benchmarks and φ^4 lattice field theory to assess expressivity, stability, parameter efficiency, and mode-collapse mitigation.

Result: The proposed bijections match or exceed spline-based performance when used in coupling layers; radial flows offer exceptional training stability, interpretable geometry, and in radial-structured targets achieve comparable quality to coupling flows with up to 1000× fewer parameters; in φ^4 lattice field theory experiments, they outperform affine baselines and enable problem-specific designs that address mode collapse.

Conclusion: Globally smooth, analytically invertible scalar bijections and radial flows substantially enhance normalizing flows by combining expressivity, stability, and parameter efficiency. They serve as versatile drop-ins for coupling layers and provide a principled approach for problems with radial structure, with demonstrated benefits on 1D/2D benchmarks and physics-inspired tasks.

Abstract: A key challenge in designing normalizing flows is finding expressive scalar bijections that remain invertible with tractable Jacobians. Existing approaches face trade-offs: affine transformations are smooth and analytically invertible but lack expressivity; monotonic splines offer local control but are only piecewise smooth and act on bounded domains; residual flows achieve smoothness but need numerical inversion. We introduce three families of analytic bijections -- cubic rational, sinh, and cubic polynomial -- that are globally smooth ($C^\infty$), defined on all of $\mathbb{R}$, and analytically invertible in closed form, combining the favorable properties of all prior approaches. These bijections serve as drop-in replacements in coupling flows, matching or exceeding spline performance. Beyond coupling layers, we develop radial flows: a novel architecture using direct parametrization that transforms the radial coordinate while preserving angular direction. Radial flows exhibit exceptional training stability, produce geometrically interpretable transformations, and on targets with radial structure can achieve comparable quality to coupling flows with $1000\times$ fewer parameters. We provide comprehensive evaluation on 1D and 2D benchmarks, and demonstrate applicability to higher-dimensional physics problems through experiments on $φ^4$ lattice field theory, where our bijections outperform affine baselines and enable problem-specific designs that address mode collapse.

</details>


### [57] [Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework](https://arxiv.org/abs/2601.10779)
*Qingyue Zhang,Chang Chu,Haohao Fu,Tianren Peng,Yanru Wu,Guanbo Huang,Yang Li,Shao-Lun Huang*

Main category: cs.LG

TL;DR: 提出统一优化权重和转移量的多源迁移学习框架 UOWQ，通过对基于 KL 散度的渐近泛化误差进行分析，联合确定每个源任务的最优权重与转移量。理论表明在权重恰当时，使用所有源样本是最优的；单源给出闭式解，多源采用凸优化求解；给出可用于多源迁移学习和多任务学习的实用算法，并在 DomainNet 和 Office-Home 上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有迁移学习在数据稀缺场景易受负迁移影响，且多源情形常被简单的均匀转移或单一优化目标所驱动。多数方法仅优化源权重或转移样本量之一，忽略二者的耦合与协同效应，因此需要一个理论框架来同时优化权重与转移量以充分利用异质源。

Method: 建立基于对数 KL 散度的渐近泛化误差分析，将多源迁移学习建模为参数估计问题，提出 Unified Optimization of Weights and Quantities（UOWQ）框架。通过理论推导得到：1) 在权重经过恰当调整后，使用所有源样本均为最优；2) 单源情形给出闭式解；3) 多源情形通过凸优化求解权重与转移量的耦合问题。基于理论结果，设计了适用于多源迁移学习和多任务学习的实际算法。

Result: 在真实基准数据集 DomainNet 与 Office-Home 上，UOWQ 持续超越强基线，实验结果验证了理论预测与方法有效性。

Conclusion: UOWQ 提供了一个具有理论支撑且在实践中有效的多源迁移学习框架，能够在统一优化权重与转移量的前提下提升性能并降低负迁移风险；具备良好的扩展性与应用潜力，未来可拓展至更复杂分布假设、在线/增量学习等场景。

Abstract: Transfer learning plays a vital role in improving model performance in data-scarce scenarios. However, naive uniform transfer from multiple source tasks may result in negative transfer, highlighting the need to properly balance the contributions of heterogeneous sources. Moreover, existing transfer learning methods typically focus on optimizing either the source weights or the amount of transferred samples, while largely neglecting the joint consideration of the other. In this work, we propose a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), which formulates multi-source transfer learning as a parameter estimation problem grounded in an asymptotic analysis of a Kullback-Leibler divergence-based generalization error measure. The proposed framework jointly determines the optimal source weights and optimal transfer quantities for each source task. Firstly, we prove that using all available source samples is always optimal once the weights are properly adjusted, and we provide a theoretical explanation for this phenomenon. Moreover, to determine the optimal transfer weights, our analysis yields closed-form solutions in the single-source setting and develops a convex optimization-based numerical procedure for the multi-source case. Building on the theoretical results, we further propose practical algorithms for both multi-source transfer learning and multi-task learning settings. Extensive experiments on real-world benchmarks, including DomainNet and Office-Home, demonstrate that UOWQ consistently outperforms strong baselines. The results validate both the theoretical predictions and the practical effectiveness of our framework.

</details>


### [58] [Towards Tensor Network Models for Low-Latency Jet Tagging on FPGAs](https://arxiv.org/abs/2601.10801)
*Alberto Coppi,Ema Puljak,Lorenzo Borella,Daniel Jaschke,Enrique Rico,Maurizio Pierini,Jacopo Pazzini,Andrea Triossi,Simone Montangero*

Main category: cs.LG

TL;DR: TN models (MPS/TTN) for real-time jet tagging on FPGAs show competitive performance, with post-training quantization enabling hardware-friendly, sub-microsecond latency suitable for HL-LHC Level-1 triggers.


<details>
  <summary>Details</summary>
Motivation: HL-LHC trigger imposes strict latency, throughput, and energy constraints; TNs offer compactness and interpretability; hardware-friendly alternatives to DL.

Method: Develop MPS and TTN using low-level jet constituent features; compare to DL classifiers; apply post-training quantization; synthesize to estimate FPGA resources and latency.

Result: Competitive accuracy relative to state-of-the-art DL; quantization preserves performance and latency; FPGA synthesis shows sub-microsecond latency and feasible resource usage; supports online deployment in real-time triggers.

Conclusion: TN-based models are promising for fast, resource-efficient inference in low-latency HEP environments; potential for hardware-accelerated inference on FPGAs.

Abstract: We present a systematic study of Tensor Network (TN) models $\unicode{x2013}$ Matrix Product States (MPS) and Tree Tensor Networks (TTN) $\unicode{x2013}$ for real-time jet tagging in high-energy physics, with a focus on low-latency deployment on Field Programmable Gate Arrays (FPGAs). Motivated by the strict requirements of the HL-LHC Level-1 trigger system, we explore TNs as compact and interpretable alternatives to deep neural networks. Using low-level jet constituent features, our models achieve competitive performance compared to state-of-the-art deep learning classifiers. We investigate post-training quantization to enable hardware-efficient implementations without degrading classification performance or latency. The best-performing models are synthesized to estimate FPGA resource usage, latency, and memory occupancy, demonstrating sub-microsecond latency and supporting the feasibility of online deployment in real-time trigger systems. Overall, this study highlights the potential of TN-based models for fast and resource-efficient inference in low-latency environments.

</details>


### [59] [Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents](https://arxiv.org/abs/2601.10820)
*Himanshu Thakur,Anusha Kamath,Anurag Muthyala,Dhwani Sanmukhani,Smruthi Mukund,Jay Katukuri*

Main category: cs.LG

TL;DR: Planner-guided, constrained-topology multi-agent framework for automated feature engineering code generation, using an LLM planner and graph-based team environment to orchestrate agents, with human-in-the-loop, delivering substantial improvements on in-house dataset and real-world cycle-time reduction.


<details>
  <summary>Details</summary>
Motivation: Addresses scarcity of datasets on production-level feature engineering, limited integration/personalization of coding agents, and suboptimal human-AI collaboration due to feedback timing.

Method: LLM-powered planner operating on a graph-represented team environment to orchestrate multi-agent calls, generate context-aware prompts, propagate failures upstream, and optionally request human intervention at critical steps.

Result: On an in-house dataset, achieves 38% improvement over manually crafted workflows and 150% over unplanned workflows; reduces feature engineering cycles for a 120M-user recommender from 3 weeks to 1 day.

Conclusion: Demonstrates feasibility of planner-guided multi-agent code generation for production-level feature engineering, improving reliability, maintainability, and alignment with team practices; potential to generalize to real-world coding tasks with human-AI collaboration.

Abstract: Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.

</details>


### [60] [Mugi: Value Level Parallelism For Efficient LLMs](https://arxiv.org/abs/2601.10823)
*Daniel Price,Prabhu Vellaisamy,John Shen,Di Wu*

Main category: cs.LG

TL;DR: 提出将价值级并行性（VLP）扩展到非线性近似，并为LLMs定制优化，设计了Mugi架构以提升完整LLM工作负载的吞吐、能效与可持续性；通过价值中心化的高精度分配、对小批量不对称输入的高效处理，以及权重/KV缓存量化和组查询注意力等技术，实现显著性能提升与碳排放降低。


<details>
  <summary>Details</summary>
Motivation: VLP在大批量、低精度GEMM中的效率提升已被证明，但LLMs涉及更复杂的计算，不仅限于激活-权重GEMM；需要将VLP泛化到非线性近似并结合LLM特性（如KV缓存、分组注意力等）以提升端到端性能、能效与可持续性。

Method: 将VLP推广至非线性近似，采用价值中心化的分配策略，使关键值获得更高的近似精度；针对小批量GEMM和不对称输入进行高效实现，结合权重量化、KV缓存量化、分组查询注意力等优化；设计新的VLP架构Mugi以支撑全量LLM工作负载并实现综合性能提升。

Result: 实验显示，Mugi在非线性softmax操作上的吞吐与能效提升分别高达45倍与668倍；在LLMs场景下提升约2.07倍吞吐、3.11倍能效，且推理相关碳排放降幅为1.45倍的运营碳与1.48倍的 embodied carbon。

Conclusion: 通过将VLP扩展为非线性近似并针对LLMs进行系统优化，Mugi在吞吐、能效与可持续性方面显著优于现有方案，覆盖完整LLM工作负载。

Abstract: Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM. In this paper, we explore how VLP benefits LLMs. First, we generalize VLP for nonlinear approximations, outperforming existing nonlinear approximations in end-to-end LLM accuracy, performance, and efficiency. Our VLP approximation follows a value-centric approach, where important values are assigned with greater accuracy. Second, we optimize VLP for small-batch GEMMs with asymmetric inputs efficiently, which leverages timely LLM optimizations, including weight-only quantization, key-value (KV) cache quantization, and group query attention. Finally, we design a new VLP architecture, Mugi, to encapsulate the innovations above and support full LLM workloads, while providing better performance, efficiency and sustainability. Our experimental results show that Mugi can offer significant improvements on throughput and energy efficiency, up to $45\times$ and $668\times$ for nonlinear softmax operations, and $2.07\times$ and $3.11\times$ for LLMs, and also decrease operational carbon for LLM operation by $1.45\times$ and embodied carbon by $1.48\times$.

</details>


### [61] [AI-Guided Human-In-the-Loop Inverse Design of High Performance Engineering Structures](https://arxiv.org/abs/2601.10859)
*Dat Quoc Ha,Md Ferdous Alam,Markus J. Buehler,Faez Ahmed,Josephine V. Carstensen*

Main category: cs.LG

TL;DR: 提出AI共驾（AI co-pilot）用于人机协同拓扑优化，利用U-Net图像分割预测用户偏好区域，以减少迭代次数并提升可制造性和性能，显示出良好泛化能力，并在某些指标上实现显著提升。


<details>
  <summary>Details</summary>
Motivation: 降低拓扑优化的计算时间与用户交互成本，缓解黑箱问题，推进人机协同的高效设计流程。

Method: 将用户偏好视为图像分割任务，采用U-Net架构训练，使用合成数据集标注为最长拓扑成员或最复杂结构连接，输出修改区域作为AI推荐。

Result: 模型可预测合理的修改区域，对多样且非标准的TO问题具泛化能力，甚至在训练数据之外呈现 emergent 行为。演示中，结合AI共驾的TO在可制造性或线性屈曲载荷上提升约39%，相比传统TO仅增加约15秒的设计时间。

Conclusion: AI共驾可显著提升拓扑优化的可用性与性能，且对非标准问题具良好泛化，并以极小的设计时间成本实现性能改进。

Abstract: Inverse design tools such as Topology Optimization (TO) can achieve new levels of improvement for high-performance engineered structures. However, widespread use is hindered by high computational times and a black-box nature that inhibits user interaction. Human-in-the-loop TO approaches are emerging that integrate human intuition into the design generation process. However, these rely on the time-consuming bottleneck of iterative region selection for design modifications. To reduce the number of iterative trials, this contribution presents an AI co-pilot that uses machine learning to predict the user's preferred regions. The prediction model is configured as an image segmentation task with a U-Net architecture. It is trained on synthetic datasets where human preferences either identify the longest topological member or the most complex structural connection. The model successfully predicts plausible regions for modification and presents them to the user as AI recommendations. The human preference model demonstrates generalization across diverse and non-standard TO problems and exhibits emergent behavior outside the single-region selection training data. Demonstration examples show that the new human-in-the-loop TO approach that integrates the AI co-pilot can improve manufacturability or improve the linear buckling load by 39% while only increasing the total design time by 15 sec compared to conventional simplistic TO.

</details>


### [62] [Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting](https://arxiv.org/abs/2601.10863)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.LG

TL;DR: 提出预测AC分数用于多时点概率预测的准确性与一致性并重，作为可微分目标函数用于训练季节性ARIMA，在M4 Hourly上实现，显著降低波动性且保持/提升准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常只优化点预测的准确性，忽略随预测起点变动的稳定性（一致性）；需要一个同时衡量多时点准确性与稳定性的指标，并允许通过权重调节偏好。

Method: 定义Forecast AC score，兼顾准确性和一致性；将其作为可微分目标函数用于训练季节性ARIMA；在M4 Hourly数据集上评估。

Result: 与传统MLE相比，AC优化的模型在相同目标时间点上显著降低预测波动性约75%，同时保持或提升点预测准确性。

Conclusion: AC分数可有效权衡准确性与一致性，作为可微分训练目标，提升多时点概率预测的稳定性与实用性，在实证数据集上表现出潜在推广性。

Abstract: Traditional time series forecasting methods optimize for accuracy alone. This objective neglects temporal consistency, in other words, how consistently a model predicts the same future event as the forecast origin changes. We introduce the forecast accuracy and coherence score (forecast AC score for short) for measuring the quality of probabilistic multi-horizon forecasts in a way that accounts for both multi-horizon accuracy and stability. Our score additionally provides for user-specified weights to balance accuracy and consistency requirements. As an example application, we implement the score as a differentiable objective function for training seasonal ARIMA models and evaluate it on the M4 Hourly benchmark dataset. Results demonstrate substantial improvements over traditional maximum likelihood estimation. Our AC-optimized models achieve a 75\% reduction in forecast volatility for the same target timestamps while maintaining comparable or improved point forecast accuracy.

</details>


### [63] [Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning](https://arxiv.org/abs/2601.10905)
*Rajat Ghosh,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出一种基于Action Shapley的数据选择方法，用于离线/模型驱动的强化学习中的训练数据筛选，提高数据利用效率。通过随机化动态算法降低Shapley值计算的指数复杂度，在五个数据受限的真实案例中实现>80%的计算效率提升，并且基于Action Shapley的训练数据选择策略优于随意数据选择。


<details>
  <summary>Details</summary>
Motivation: 训练数据质量直接影响世界模型的性能与可解释性；在数据受限环境下，需无偏且高效的数据价值评估方法。Shapley值提供数据分配的理论基础，但计算代价高，因此需要近似高效算法。将Action Shapley用于训练数据筛选，以提升离线/模型基RL的学习效果。

Method: 提出Action Shapley作为无偏且领域无关的数据价值度量；设计一种随机化的动态算法近似计算Action Shapley值，降低传统Shapley的指数复杂度；基于该度量优化训练数据选择策略并在五个数据受限的真实案例中评估。

Result: 在与传统指数时间计算相比，算法的计算效率提升>80%；基于Action Shapley的训练数据选择策略在数据受限场景下稳定优于经验性（ad-hoc）数据选择。

Conclusion: Action Shapley为离线/模型驱动RL的训练数据选择提供可扩展、无偏的解决方案，提升数据稀缺场景下的学习效果与效率；具有潜在的对其它数据驱动任务的推广价值。

Abstract: Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.

</details>


### [64] [Realistic Curriculum Reinforcement Learning for Autonomous and Sustainable Marine Vessel Navigation](https://arxiv.org/abs/2601.10911)
*Zhang Xiaocai,Xiao Zhe,Liang Maohan,Liu Tao,Li Haijiang,Zhang Wenbin*

Main category: cs.LG

TL;DR: A curriculum RL framework for sustainable vessel navigation using a data-driven marine simulator with diffusion-based dynamic conditions, learning-based fuel prediction, and image-based state representation, evaluated in the Indian Ocean.


<details>
  <summary>Details</summary>
Motivation: Enhance safety and reduce emissions in autonomous maritime navigation by reducing reliance on human experience and incorporating emission awareness and realistic dynamics.

Method: Develop a lightweight policy-based CRL agent trained in a data-driven marine simulation enhanced with a diffusion model; use learning-based regression to predict fuel consumption; represent surroundings as image inputs; implement a multi-objective reward balancing safety, emissions, timeliness, and task completion.

Result: Demonstrates efficacy in enabling sustainable and safe vessel navigation in a real sea area (Indian Ocean) through progressive learning and stable performance in continuous action spaces.

Conclusion: CRL framework with realistic simulation and data-driven fuel estimation is effective for sustainable autonomous navigation; promising for scalable, safe maritime operations.

Abstract: Sustainability is becoming increasingly critical in the maritime transport, encompassing both environmental and social impacts, such as Greenhouse Gas (GHG) emissions and navigational safety. Traditional vessel navigation heavily relies on human experience, often lacking autonomy and emission awareness, and is prone to human errors that may compromise safety. In this paper, we propose a Curriculum Reinforcement Learning (CRL) framework integrated with a realistic, data-driven marine simulation environment and a machine learning-based fuel consumption prediction module. The simulation environment is constructed using real-world vessel movement data and enhanced with a Diffusion Model to simulate dynamic maritime conditions. Vessel fuel consumption is estimated using historical operational data and learning-based regression. The surrounding environment is represented as image-based inputs to capture spatial complexity. We design a lightweight, policy-based CRL agent with a comprehensive reward mechanism that considers safety, emissions, timeliness, and goal completion. This framework effectively handles complex tasks progressively while ensuring stable and efficient learning in continuous action spaces. We validate the proposed approach in a sea area of the Indian Ocean, demonstrating its efficacy in enabling sustainable and safe vessel navigation.

</details>


### [65] [FAConvLSTM: Factorized-Attention ConvLSTM for Efficient Feature Extraction in Multivariate Climate Data](https://arxiv.org/abs/2601.10914)
*Francis Ndikum Nji,Jianwu Wang*

Main category: cs.LG

TL;DR: FAConvLSTM introduces a factorized-attention ConvLSTM that serves as a drop-in replacement for ConvLSTM2D, reducing computation while enhancing spatial expressiveness and interpretability through bottleneck gate factorization, depthwise spatial mixing, multi-scale dilated branches, SE recalibration, peephole timing, axial attention, and a subspace head with temporal self-attention and seasonal positional encoding.


<details>
  <summary>Details</summary>
Motivation: ConvLSTM2D suffers from heavy computation due to dense gating and strictly local receptive fields, limiting modeling of long-range spatial structure, teleconnections, multi-scale interactions, and nonstationarity in high-resolution multivariate Earth observation data.

Method: FAConvLSTM factorizes recurrent gate computations with 1x1 bottlenecks and shared depthwise spatial mixing, employs multi-scale dilated depthwise branches and squeeze-and-excitation recalibration, incorporates peephole connections, adds lightweight axial spatial attention applied sparsely in time to capture teleconnections, and includes a dedicated subspace head with temporal self-attention and fixed seasonal positional encoding.

Result: Empirical evaluation on multivariate spatiotemporal climate data shows FAConvLSTM yields more stable, interpretable, and robust latent representations than standard ConvLSTM, while significantly reducing computational overhead.

Conclusion: FAConvLSTM provides an efficient, spatially expressive, and physically interpretable drop-in replacement for ConvLSTM2D, improving performance on complex climate data and enabling scalable analysis of multivariate spatiotemporal dynamics.

Abstract: Learning physically meaningful spatiotemporal representations from high-resolution multivariate Earth observation data is challenging due to strong local dynamics, long-range teleconnections, multi-scale interactions, and nonstationarity. While ConvLSTM2D is a commonly used baseline, its dense convolutional gating incurs high computational cost and its strictly local receptive fields limit the modeling of long-range spatial structure and disentangled climate dynamics. To address these limitations, we propose FAConvLSTM, a Factorized-Attention ConvLSTM layer designed as a drop-in replacement for ConvLSTM2D that simultaneously improves efficiency, spatial expressiveness, and physical interpretability. FAConvLSTM factorizes recurrent gate computations using lightweight [1 times 1] bottlenecks and shared depthwise spatial mixing, substantially reducing channel complexity while preserving recurrent dynamics. Multi-scale dilated depthwise branches and squeeze-and-excitation recalibration enable efficient modeling of interacting physical processes across spatial scales, while peephole connections enhance temporal precision. To capture teleconnection-scale dependencies without incurring global attention cost, FAConvLSTM incorporates a lightweight axial spatial attention mechanism applied sparsely in time. A dedicated subspace head further produces compact per timestep embeddings refined through temporal self-attention with fixed seasonal positional encoding. Experiments on multivariate spatiotemporal climate data shows superiority demonstrating that FAConvLSTM yields more stable, interpretable, and robust latent representations than standard ConvLSTM, while significantly reducing computational overhead.

</details>


### [66] [HOSL: Hybrid-Order Split Learning for Memory-Constrained Edge Training](https://arxiv.org/abs/2601.10940)
*Aakriti,Zhe Li,Dandan Liang,Chao Huang,Rui Li,Haibo Yang*

Main category: cs.LG

TL;DR: 提出 HOSL：在分割学习中将客户端的记忆高效的零阶梯度估计与服务器端的一阶优化相结合，以降低客户端内存并保持良好收敛性。


<details>
  <summary>Details</summary>
Motivation: 分割学习在边缘设备与服务器之间分割模型计算，但现有方法多依赖一阶优化，需要在客户端存储中间激活值，导致显著内存开销。零阶优化虽然降低内存，但收敛慢。需要在内存效率和优化效果之间实现折中。

Method: 提出混合阶 HOSL：在客户端使用记忆高效的零阶梯度估计以消除回传和激活存储，在服务器端使用一阶优化以实现快速收敛。给出理论收敛率 O(√(d_c / (TQ)))，其中 d_c 为客户端模型维度，且结果随更多计算下放到服务器而提升。通过对 OPT 模型（125M、1.3B）在6个任务上的实验，验证内存显著降低且性能接近 FO 基线，且显著优于纯 ZO。

Result: 实验表明：客户端显存可降低最多约 3.7×，精度仅落后 FO 基线 0.20%–4.23%，在比 ZO 基线高出最高约 15.55% 的性能提升。

Conclusion: 证实将零阶优化与一阶优化混合使用的分割学习在边缘设备上的记忆效率和训练性能之间取得良好权衡，适用于大模型在边缘环境的分布式训练。

Abstract: Split learning (SL) enables collaborative training of large language models (LLMs) between resource-constrained edge devices and compute-rich servers by partitioning model computation across the network boundary. However, existing SL systems predominantly rely on first-order (FO) optimization, which requires clients to store intermediate quantities such as activations for backpropagation. This results in substantial memory overhead, largely negating benefits of model partitioning. In contrast, zeroth-order (ZO) optimization eliminates backpropagation and significantly reduces memory usage, but often suffers from slow convergence and degraded performance. In this work, we propose HOSL, a novel Hybrid-Order Split Learning framework that addresses this fundamental trade-off between memory efficiency and optimization effectiveness by strategically integrating ZO optimization on the client side with FO optimization on the server side. By employing memory-efficient ZO gradient estimation at the client, HOSL eliminates backpropagation and activation storage, reducing client memory consumption. Meanwhile, server-side FO optimization ensures fast convergence and competitive performance. Theoretically, we show that HOSL achieves a $\mathcal{O}(\sqrt{d_c/TQ})$ rate, which depends on client-side model dimension $d_c$ rather than the full model dimension $d$, demonstrating that convergence improves as more computation is offloaded to the server. Extensive experiments on OPT models (125M and 1.3B parameters) across 6 tasks demonstrate that HOSL reduces client GPU memory by up to 3.7$\times$ compared to the FO method while achieving accuracy within 0.20%-4.23% of this baseline. Furthermore, HOSL outperforms the ZO baseline by up to 15.55%, validating the effectiveness of our hybrid strategy for memory-efficient training on edge devices.

</details>


### [67] [Transient learning dynamics drive escape from sharp valleys in Stochastic Gradient Descent](https://arxiv.org/abs/2601.10962)
*Ning Yang,Yikuan Zhang,Qi Ouyang,Chao Tang,Yuhai Tu*

Main category: cs.LG

TL;DR: SGD 动力学揭示一种非平衡机制：存在阶段性探索期，SGD 噪声将损失平面重塑为偏好平坦解的有效势；训练中后期出现能垒增长导致跨谷跃迁冻结，增加噪声有助于延缓冻结并有利于收敛到更平坦的极小值；提出统一框架连接学习动力学、损失地形与泛化，并给出优化算法设计的原则。


<details>
  <summary>Details</summary>
Motivation: 解释为何随机梯度下降偏好更平坦、泛化性更好的解，以及这与损失地形之间的关系。

Method: 通过数值实验揭示 SGD 学习动力学中的瞬时探索阶段；建立一个可处理的物理模型，将 SGD 噪声视为改变损失地形的有效势，分析能垒与跨谷跃迁的冻结现象，考察噪声强度对收敛到更平坦解的影响。

Result: 发现：1) 存在一个探索性瞬态阶段，SGD 路径不断从尖锐谷逃逸并向平坦区域转移；2) 构建的物理模型表明噪声重塑势函，使得平坦解更有利，3) 随训练进行，能垒增大导致跨谷转移的冻结，最终困于一个局部盆地；4) 增大 SGD 噪声强度可延迟冻结，提升收敛至更平坦极小值的概率。

Conclusion: 给出一个统一的物理框架，将学习动力学、损失地形几何和泛化联系起来，并提出面向更高效优化算法设计的原则。

Abstract: Stochastic gradient descent (SGD) is central to deep learning, yet the dynamical origin of its preference for flatter, more generalizable solutions remains unclear. Here, by analyzing SGD learning dynamics, we identify a nonequilibrium mechanism governing solution selection. Numerical experiments reveal a transient exploratory phase in which SGD trajectories repeatedly escape sharp valleys and transition toward flatter regions of the loss landscape. By using a tractable physical model, we show that the SGD noise reshapes the landscape into an effective potential that favors flat solutions. Crucially, we uncover a transient freezing mechanism: as training proceeds, growing energy barriers suppress inter-valley transitions and ultimately trap the dynamics within a single basin. Increasing the SGD noise strength delays this freezing, which enhances convergence to flatter minima. Together, these results provide a unified physical framework linking learning dynamics, loss-landscape geometry, and generalization, and suggest principles for the design of more effective optimization algorithms.

</details>


### [68] [Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration](https://arxiv.org/abs/2601.10973)
*Zain ul Abdeen,Waris Gill,Ming Jin*

Main category: cs.LG

TL;DR: 提出 MG-FRL（MGF-RL）通过元学习与进化策略结合，获得可迁移初始化并快速适应未见情景；在 IEEE 13-bus/123-bus 实验中优于标准 RL、MAML-RL 与 MPC，并具备子线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 可再生能源不确定性、有限调度资源和非线性动力学使极端事件后的负荷重建复杂。传统 RL 泛化能力不足且需大量再训练。

Method: 将一阶元学习与进化策略耦合，使用梯度自由的策略搜索，学习自历史 outage 经验的可迁移初始化，适应非线性、约束的配电系统动态。

Result: 在 IEEE 13-bus 与 123-bus 测试系统上，MGF-RL 在可靠性、恢复速度与适应效率方面超越标准 RL、MAML-RL 与模型预测控制（MPC），并对未见 outages 与可再生模式具备良好泛化；所需微调样本显著减少；给出与任务相似度和环境变动相关的子线性遗憾界。

Conclusion: MGF-RL 为在高可再生能源场景下实现实时负载恢复提供有效框架，具备较强的泛化与快速适应能力，以及理论保证，具备落地潜力。

Abstract: Restoring critical loads after extreme events demands adaptive control to maintain distribution-grid resilience, yet uncertainty in renewable generation, limited dispatchable resources, and nonlinear dynamics make effective restoration difficult. Reinforcement learning (RL) can optimize sequential decisions under uncertainty, but standard RL often generalizes poorly and requires extensive retraining for new outage configurations or generation patterns. We propose a meta-guided gradient-free RL (MGF-RL) framework that learns a transferable initialization from historical outage experiences and rapidly adapts to unseen scenarios with minimal task-specific tuning. MGF-RL couples first-order meta-learning with evolutionary strategies, enabling scalable policy search without gradient computation while accommodating nonlinear, constrained distribution-system dynamics. Experiments on IEEE 13-bus and IEEE 123-bus test systems show that MGF-RL outperforms standard RL, MAML-based meta-RL, and model predictive control across reliability, restoration speed, and adaptation efficiency under renewable forecast errors. MGF-RL generalizes to unseen outages and renewable patterns while requiring substantially fewer fine-tuning episodes than conventional RL. We also provide sublinear regret bounds that relate adaptation efficiency to task similarity and environmental variation, supporting the empirical gains and motivating MGF-RL for real-time load restoration in renewable-rich distribution grids.

</details>


### [69] [Reasoning Distillation for Lightweight Automated Program Repair](https://arxiv.org/abs/2601.10987)
*Aanand Balasubramanian,Sashank Silwal*

Main category: cs.LG

TL;DR: 通过教师模型的符号推理标签蒸馏，提升轻量化代码修复模型在修复类型分类上的宏平均性能，尤其提升罕见类别，且不增加模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 资源受限下的小型代码模型难以学习到稳定的程序结构，单一预测易受浅层相关性影响。引入结构化推理标签以提高可解释性和鲁棒性。

Method: 以CodeT5为基线的学生模型，在IntroClass基准上进行标签蒸馏；教师模型提供带有符号推理标签的修复类型标签；比较标签仅蒸馏与推理蒸馏两种设置。

Result: 推理蒸馏带来宏平均性能提升，尤其在低频错误类别；模型大小、结构不变；推理准确性与修复类型预测显著相关但并非等同于因果决定因素。

Conclusion: 符号推理蒸馏为轻量化程序修复模型带来可解释性和鲁棒性提升的实际手段。

Abstract: We study whether lightweight symbolic reasoning supervision can improve fix type classification in compact automated program repair models. Small code models are attractive for resource-constrained settings, but they typically produce only a single prediction, making it unclear whether they learn meaningful program structure or rely on shallow correlations. We propose a reasoning distillation approach in which a large teacher model provides structured symbolic reasoning tags alongside fix-type labels. These tags capture high-level causal properties of bugs without relying on free-form explanations. We train a CodeT5-based student model under label-only and reasoning-distilled settings on the IntroClass benchmark. Reasoning supervision consistently improves macro averaged performance, particularly on less frequent bug categories, without increasing model size or complexity. We further analyze the relationship between reasoning accuracy and fix-type prediction, showing that correct reasoning traces strongly correlate with correct predictions, while not fully determining them. Our results suggest that symbolic reasoning distillation is a practical way to improve interpretability and robustness in lightweight program repair models.

</details>


### [70] [Constant Metric Scaling in Riemannian Computation](https://arxiv.org/abs/2601.10992)
*Kisung You*

Main category: cs.LG

TL;DR: Constant metric rescaling on Riemannian manifolds: identifies which quantities change under a global scale and which invariants persist, and interprets scaling mainly as a step-size adjustment in optimization.


<details>
  <summary>Details</summary>
Motivation: Global scale parameters appear in computations with Riemannian metrics; there is a risk of misinterpreting scale changes as changes in curvature or geometry. The note aims to clarify this distinction and provide a self-contained exposition.

Method: Provide a self-contained account of constant metric scaling on arbitrary Riemannian manifolds; compare how norms, distances, volume elements, and gradient magnitudes change under scaling versus invariants like Levi-Civita connection, geodesics, exponential/log maps, and parallel transport; discuss optimization implications.

Result: Clear separation between scale-variant quantities (norms, distances, volume, gradient magnitudes) and invariants (connection, geodesics, exponential/log maps, parallel transport); interpretation of constant scaling as global step-size rescaling in Riemannian optimization without changing underlying geometry.

Conclusion: Global metric scaling does not alter geometric structures; it can be used to adjust algorithmic step sizes. The note provides a concise, expository clarification of introducing a global scale parameter in Riemannian computation.

Abstract: Constant rescaling of a Riemannian metric appears in many computational settings, often through a global scale parameter that is introduced either explicitly or implicitly. Although this operation is elementary, its consequences are not always made clear in practice and may be confused with changes in curvature, manifold structure, or coordinate representation. In this note we provide a short, self-contained account of constant metric scaling on arbitrary Riemannian manifolds. We distinguish between quantities that change under such a scaling, including norms, distances, volume elements, and gradient magnitudes, and geometric objects that remain invariant, such as the Levi--Civita connection, geodesics, exponential and logarithmic maps, and parallel transport. We also discuss implications for Riemannian optimization, where constant metric scaling can often be interpreted as a global rescaling of step sizes rather than a modification of the underlying geometry. The goal of this note is purely expository and is intended to clarify how a global metric scale parameter can be introduced in Riemannian computation without altering the geometric structures on which these methods rely.

</details>


### [71] [Backdoor Attacks on Multi-modal Contrastive Learning](https://arxiv.org/abs/2601.11006)
*Simi D Kuniyilh,Rita Machacy*

Main category: cs.LG

TL;DR: 对比学习中的背门攻击综合评述，分析威胁模型、攻击方法、目标领域与防御，指出对比学习的特有脆弱性、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 对比学习在视觉、跨模态、图与联邦学习等领域广泛应用，但易受背门和数据投毒攻击影响，亟需系统化的威胁分析与防御对策。

Method: 作为综述论文，比较分析现有文献中的威胁模型、攻击方法、目标领域与防御策略，综合归纳攻击-防御的进展与漏洞，提出未来研究方向。

Result: 对现有研究进行了分类与综合，梳理攻击类型、目标域与防御手段，揭示对比学习的特有漏洞及实际部署中的安全风险。

Conclusion: 强调需要更鲁棒的防御、标准化评估基准与跨域安全部署指南，未来研究应聚焦鲁棒性、对抗性评估、数据污染防御等方面。

Abstract: Contrastive learning has become a leading self- supervised approach to representation learning across domains, including vision, multimodal settings, graphs, and federated learning. However, recent studies have shown that contrastive learning is susceptible to backdoor and data poisoning attacks. In these attacks, adversaries can manipulate pretraining data or model updates to insert hidden malicious behavior. This paper offers a thorough and comparative review of backdoor attacks in contrastive learning. It analyzes threat models, attack methods, target domains, and available defenses. We summarize recent advancements in this area, underline the specific vulnerabilities inherent to contrastive learning, and discuss the challenges and future research directions. Our findings have significant implications for the secure deployment of systems in industrial and distributed environments.

</details>


### [72] [Combating Spurious Correlations in Graph Interpretability via Self-Reflection](https://arxiv.org/abs/2601.11021)
*Kecheng Cai,Chenyang Xu,Chao Peng*

Main category: cs.LG

TL;DR: A self-reflection framework improves interpretability in graph learning on the Spurious-Motif benchmark by iteratively re-evaluating node/edge importance predictions and using feedback to fine-tune the model.


<details>
  <summary>Details</summary>
Motivation: The Spurious-Motif benchmark imposes strong spurious correlations that obscure true structural signals; existing interpretable graph methods struggle to distinguish genuine structure from spurious patterns. Borrowing self-reflection from LLMs could boost interpretability by reassessing predictions.

Method: Integrate a self-reflection loop with existing interpretable graph learning methods. After an initial importance scoring, feed the predictions back into the method for a second evaluation. This iterative process mimics self-reflective prompting in LLMs. Additionally, analyze the underlying reasons via graph representation learning and propose a fine-tuning training method informed by the feedback.

Result: Demonstrates that self-reflection can be effectively adapted to improve interpretability on Spurious-Motif datasets; empirical validation shows improved discrimination of truly relevant graph components, along with analytical insights into why the feedback improves performance.

Conclusion: Self-reflection is a viable mechanism to enhance interpretability in graph learning under strong spurious correlations. The approach can be integrated with existing methods and paired with a feedback-based fine-tuning strategy to bolster robustness and generalization on challenging benchmarks.

Abstract: Interpretable graph learning has recently emerged as a popular research topic in machine learning. The goal is to identify the important nodes and edges of an input graph that are crucial for performing a specific graph reasoning task. A number of studies have been conducted in this area, and various benchmark datasets have been proposed to facilitate evaluation. Among them, one of the most challenging is the Spurious-Motif benchmark, introduced at ICLR 2022. The datasets in this synthetic benchmark are deliberately designed to include spurious correlations, making it particularly difficult for models to distinguish truly relevant structures from misleading patterns. As a result, existing methods exhibit significantly worse performance on this benchmark compared to others.
  In this paper, we focus on improving interpretability on the challenging Spurious-Motif datasets. We demonstrate that the self-reflection technique, commonly used in large language models to tackle complex tasks, can also be effectively adapted to enhance interpretability in datasets with strong spurious correlations. Specifically, we propose a self-reflection framework that can be integrated with existing interpretable graph learning methods. When such a method produces importance scores for each node and edge, our framework feeds these predictions back into the original method to perform a second round of evaluation. This iterative process mirrors how large language models employ self-reflective prompting to reassess their previous outputs. We further analyze the reasons behind this improvement from the perspective of graph representation learning, which motivates us to propose a fine-tuning training method based on this feedback mechanism.

</details>


### [73] [AVP-Pro: An Adaptive Multi-Modal Fusion and Contrastive Learning Approach for Comprehensive Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2601.11028)
*Xinru Wen,Weizhong Lin,zi liu,Xuan Xiao*

Main category: cs.LG

TL;DR: 提出AVP-Pro，一种两阶段的抗病毒肽（AVP）识别与功能亚型预测框架，整合自适应特征融合、对比学习与OHEM以提升对高相似样本的判别能力，覆盖全面特征空间、局部-全局依赖建模，并具备病毒家族/具体病毒的小样本多任务分类能力及网页界面。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在捕捉序列复杂依赖与区分高相似样本方面的局限性，从而提升AVP识别准确性、亚型预测能力及结果可解释性。

Method: 提出两阶段框架：第一阶段通过自适应特征融合和自注意力/门控机制对CNN提取的局部motifs与BiLSTM捕获的全局依赖进行动态权重调节，构建包含10类描述符的全景特征空间；第二阶段结合迁移学习进行功能亚型/病毒家族分类，并引入OHEM驱动的对比学习，辅以BLOSUM62；并提供一个易用网页接口。

Result: 第一阶段AVP识别达到准确率0.9531、 MCC 0.9064，超越现有SOTA；第二阶段在小样本条件下，通过迁移学习实现对6个病毒家族和8个具体病毒的准确分类。

Conclusion: AVP-Pro提供一个高性能、可解释且可扩展的工具，用于高通量AVP筛选及其功能亚型识别，且具备便捷的网页访问入口。

Abstract: The accurate identification of antiviral peptides (AVPs) is crucial for novel drug development. However, existing methods still have limitations in capturing complex sequence dependencies and distinguishing confusing samples with high similarity. To address these challenges, we propose AVP-Pro, a novel two-stage predictive framework that integrates adaptive feature fusion and contrastive learning. To comprehensively capture the physicochemical properties and deep-seated patterns of peptide sequences, we constructed a panoramic feature space encompassing 10 distinct descriptors and designed a hierarchical fusion architecture. This architecture integrates self-attention and adaptive gating mechanisms to dynamically modulate the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Targeting the blurred decision boundary caused by the high similarity between positive and negative sample sequences, we adopted an Online Hard Example Mining (OHEM)-driven contrastive learning strategy enhanced by BLOSUM62. This approach significantly sharpened the model's discriminative power. Model evaluation results show that in the first stage of general AVP identification, the model achieved an accuracy of 0.9531 and an MCC of 0.9064, outperforming existing state-of-the-art (SOTA) methods. In the second stage of functional subtype prediction, combined with a transfer learning strategy, the model realized accurate classification of 6 viral families and 8 specific viruses under small-sample conditions. AVP-Pro provides a powerful and interpretable new tool for the high-throughput screening of antiviral drugs. To further enhance accessibility for users, we have developed a user-friendly web interface, which is available at https://wwwy1031-avp-pro.hf.space.

</details>


### [74] [Self-Augmented Mixture-of-Experts for QoS Prediction](https://arxiv.org/abs/2601.11036)
*Kecheng Cai,Chao Peng,Chenyang Xu,Xia Chen*

Main category: cs.LG

TL;DR: Self-augmented mixture-of-experts for QoS prediction with iterative refinement; partially masking predictions feeds back for improved imputation under sparse user-service interactions; demonstrates superiority over baselines.


<details>
  <summary>Details</summary>
Motivation: QoS prediction suffers from extreme sparsity of user-service interactions. A self-augmentation mechanism leverages the model's own predictions for iterative refinement, and the MoE framework enables inter-expert collaboration across refinement rounds.

Method: Introduce a self-augmented strategy that masks part of the model's predictions and feeds them back for a second prediction. Build a mixture-of-experts (MoE) with multiple expert networks that iteratively and collaboratively estimate QoS values, with inter-expert communication in the second round using first-round predictions.

Result: Empirical evaluation on benchmark QoS datasets shows the method outperforms existing baselines and achieves competitive results.

Conclusion: The self-augmented MoE effectively addresses data sparsity in QoS prediction, and the iterative augmentation aligns with MoE to enable inter-expert collaboration, yielding improved predictive accuracy.

Abstract: Quality of Service (QoS) prediction is one of the most fundamental problems in service computing and personalized recommendation. In the problem, there is a set of users and services, each associated with a set of descriptive features. Interactions between users and services produce feedback values, typically represented as numerical QoS metrics such as response time or availability. Given the observed feedback for a subset of user-service pairs, the goal is to predict the QoS values for the remaining pairs.
  A key challenge in QoS prediction is the inherent sparsity of user-service interactions, as only a small subset of feedback values is typically observed. To address this, we propose a self-augmented strategy that leverages a model's own predictions for iterative refinement. In particular, we partially mask the predicted values and feed them back into the model to predict again. Building on this idea, we design a self-augmented mixture-of-experts model, where multiple expert networks iteratively and collaboratively estimate QoS values. We find that the iterative augmentation process naturally aligns with the MoE architecture by enabling inter-expert communication: in the second round, each expert receives the first-round predictions and refines its output accordingly. Experiments on benchmark datasets show that our method outperforms existing baselines and achieves competitive results.

</details>


### [75] [OpFML: Pipeline for ML-based Operational Forecasting](https://arxiv.org/abs/2601.11046)
*Shahbaz Alvi,Giusy Fedele,Gabriele Accarino,Italo Epicoco,Ilenia Manco,Pasquale Schiano*

Main category: cs.LG

TL;DR: 提出 OpFML：一个可配置的运营性机器学习预测流水线，用于周期性预测，应用于日火灾危险指数预测。


<details>
  <summary>Details</summary>
Motivation: 气候与地球科学对数据驱动预测的需求日增；传统方法往往高估火灾风险，缺乏可复现、可部署的运营性工具，需支持周期性预测的流水线。

Method: 设计并实现 OpFML，一种可配置、可适应的流水线，用以部署机器学习模型进行周期性预测；并通过日火灾危险指数预测来演示其功能和特征。

Result: 展示流水线能力并概述其特征，通过日火灾危险指数预测的应用验证可用性。

Conclusion: OpFML 为气候/地球科学中的运营性机器学习预测提供可重用的框架，便于对周期性任务进行建模与部署。

Abstract: Machine learning is finding its application in a multitude of areas in science and research, and Climate and Earth Sciences is no exception to this trend. Operational forecasting systems based on data-driven approaches and machine learning methods deploy models for periodic forecasting. Wildfire danger assessment using machine learning has garnered significant interest in the last decade, as conventional methods often overestimate the risk of wildfires. In this work, we present the code OpFML: Operational Forecasting with Machine Learning. OpFML is a configurable and adaptable pipeline that can be utilized to serve a machine learning model for periodic forecasting. We further demonstrate the capabilities of the pipeline through its application to daily Fire Danger Index forecasting and outline its various features.

</details>


### [76] [Soft Bayesian Context Tree Models for Real-Valued Time Series](https://arxiv.org/abs/2601.11079)
*Shota Saito,Yuta Nakahara,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: 提出软（概率）分割的软贝叶斯上下文树模型 Soft-BCT，用变分推断进行学习，在现实数据集上表现与传统BCT相当乃至更优。


<details>
  <summary>Details</summary>
Motivation: 解决实值时间序列BCT中硬分割的局限性，利用软分割来表达上下文分割的不确定性并提升预测性能。

Method: 提出 Soft-BCT，采用软概率分割替代硬分割，利用变分推断学习模型后验与参数；对实值时间序列建立贝叶斯上下文树框架。

Result: 在若干真实数据集上，Soft-BCT的性能接近或优于前一代 BCT。

Conclusion: Soft-BCT为实值时间序列提供一种可行的概率性上下文分割替代方案，且通过变分推断实现可行训练，提升模型灵活性与不确定性表达。

Abstract: This paper proposes the soft Bayesian context tree model (Soft-BCT), which is a novel BCT model for real-valued time series. The Soft-BCT considers soft (probabilistic) splits of the context space, instead of hard (deterministic) splits of the context space as in the previous BCT for real-valued time series. A learning algorithm of the Soft-BCT is proposed based on the variational inference. For some real-world datasets, the Soft-BCT demonstrates almost the same or superior performance to the previous BCT.

</details>


### [77] [Differentially Private Subspace Fine-Tuning for Large Language Models](https://arxiv.org/abs/2601.11113)
*Lele Zheng,Xiang Wang,Tao Zhang,Yang Cao,Ke Cheng,Yulong Shen*

Main category: cs.LG

TL;DR: 通过将噪声仅注入到一个任务相关的低维子空间，DP-SFT 在差分隐私约束下对下游微调的噪声影响显著降低，同时提升准确性、稳定性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私约束下对大语言模型进行下游微调时，直接在高维参数空间加入噪声会导致权重更新的范数增大，严重降级模型性能与训练稳定性。因此需要在严格隐私保障下减少噪声对非关键参数的影响。

Method: 提出两阶段子空间微调：阶段一通过分析主梯度方向来识别任务特异的低维子空间；阶段二将全梯度投影到该子空间，注入 DP 噪声后再映射回原空间进行模型更新，从而显著降低噪声对参数的总体影响。

Result: 在多数据集上实验表明，DP-SFT 在严格的 DP 约束下提升了精度和稳定性，缩短了收敛时间，并相较于 DP 微调基线取得显著提升。

Conclusion: DP-SFT 通过将噪声约束在一个低维、任务相关的子空间内，降低了 DP 微调中的噪声幅度，同时在保持正式 DP 保证的前提下提高了性能与训练效率。

Abstract: Fine-tuning large language models on downstream tasks is crucial for realizing their cross-domain potential but often relies on sensitive data, raising privacy concerns. Differential privacy (DP) offers rigorous privacy guarantees and has been widely adopted in fine-tuning; however, naively injecting noise across the high-dimensional parameter space creates perturbations with large norms, degrading performance and destabilizing training. To address this issue, we propose DP-SFT, a two-stage subspace fine-tuning method that substantially reduces noise magnitude while preserving formal DP guarantees. Our intuition is that, during fine-tuning, significant parameter updates lie within a low-dimensional, task-specific subspace, while other directions change minimally. Hence, we only inject DP noise into this subspace to protect privacy without perturbing irrelevant parameters. In phase one, we identify the subspace by analyzing principal gradient directions to capture task-specific update signals. In phase two, we project full gradients onto this subspace, add DP noise, and map the perturbed gradients back to the original parameter space for model updates, markedly lowering noise impact. Experiments on multiple datasets demonstrate that DP-SFT enhances accuracy and stability under rigorous DP constraints, accelerates convergence, and achieves substantial gains over DP fine-tuning baselines.

</details>


### [78] [Optimized Algorithms for Text Clustering with LLM-Generated Constraints](https://arxiv.org/abs/2601.11118)
*Chaoqi Jia,Weihong Wu,Longkun Guo,Zhigang Lu,Chao Chen,Kok-Leong Ong*

Main category: cs.LG

TL;DR: Proposes a constraint-generation approach for constrained clustering using LLM-generated constraints, reducing LLM queries by >20x and introducing a tailored clustering algorithm with confidence threshold and penalty to handle inaccuracies; evaluated on five text datasets with competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Reduce resource consumption and improve efficiency of LLM-based constraint generation for clustering, by generating constraint sets instead of costly pairwise must-link/cannot-link constraints while maintaining clustering quality.

Method: Generate constraint sets via LLMs instead of pairwise constraints; develop a constrained clustering algorithm that incorporates a confidence threshold and a penalty mechanism to mitigate inaccurate constraints.

Result: Achieves clustering accuracy comparable to state-of-the-art methods while reducing the number of LLM queries by more than 20×; evaluation conducted on five text datasets considering both constraint-generation cost and clustering performance.

Conclusion: The approach balances clustering quality with generation efficiency; robust to constraint noise through confidence thresholding and penalties; promising for scalable text clustering with LLM-assisted constraint generation.

Abstract: Clustering is a fundamental tool that has garnered significant interest across a wide range of applications including text analysis. To improve clustering accuracy, many researchers have incorporated background knowledge, typically in the form of must-link and cannot-link constraints, to guide the clustering process. With the recent advent of large language models (LLMs), there is growing interest in improving clustering quality through LLM-based automatic constraint generation. In this paper, we propose a novel constraint-generation approach that reduces resource consumption by generating constraint sets rather than using traditional pairwise constraints. This approach improves both query efficiency and constraint accuracy compared to state-of-the-art methods. We further introduce a constrained clustering algorithm tailored to the characteristics of LLM-generated constraints. Our method incorporates a confidence threshold and a penalty mechanism to address potentially inaccurate constraints. We evaluate our approach on five text datasets, considering both the cost of constraint generation and the overall clustering performance. The results show that our method achieves clustering accuracy comparable to the state-of-the-art algorithms while reducing the number of LLM queries by more than 20 times.

</details>


### [79] [Shape-morphing programming of soft materials on complex geometries via neural operator](https://arxiv.org/abs/2601.11126)
*Lu Chen,Gengxiang Chen,Xu Liu,Jingyan Su,Xuhao Lyu,Lihui Wang,Yingguang Li*

Main category: cs.LG

TL;DR: 提出 S2NO，用于复杂几何形状的体素级形状变形材料分布预测与优化，结合拉普拉斯特征分解与时空卷积，具离散化不变性并支持超分辨设计。


<details>
  <summary>Details</summary>
Motivation: 旨在实现复杂几何上的高保真形状变形设计，满足共形植入、空气动力学形变等应用对材料分布的多样性与精度需求，现有方法在复杂几何上能力不足。

Method: 提出 Spectral and Spatial Neural Operator (S2NO)，通过拉普拉斯特征函数编码与空间卷积，捕捉不规则计算域的全局与局部变形行为；结合进化算法实现体素级材料分布优化；其离散化不变性支持超分辨材料分布设计。

Result: 在复杂几何（不规则边界、孔隙结构、薄壁结构等）上实现高保真形状变形预测；实现体素级材料分布的逐体积优化以编程形状变形；离散化不变性使得可进行更高分辨率设计，扩展设计多样性。

Conclusion: 这些方法显著提升了对复杂形状变形编程的效率与能力。

Abstract: Shape-morphing soft materials can enable diverse target morphologies through voxel-level material distribution design, offering significant potential for various applications. Despite progress in basic shape-morphing design with simple geometries, achieving advanced applications such as conformal implant deployment or aerodynamic morphing requires accurate and diverse morphing designs on complex geometries, which remains challenging. Here, we present a Spectral and Spatial Neural Operator (S2NO), which enables high-fidelity morphing prediction on complex geometries. S2NO effectively captures global and local morphing behaviours on irregular computational domains by integrating Laplacian eigenfunction encoding and spatial convolutions. Combining S2NO with evolutionary algorithms enables voxel-level optimisation of material distributions for shape morphing programming on various complex geometries, including irregular-boundary shapes, porous structures, and thin-walled structures. Furthermore, the neural operator's discretisation-invariant property enables super-resolution material distribution design, further expanding the diversity and complexity of morphing design. These advancements significantly improve the efficiency and capability of programming complex shape morphing.

</details>


### [80] [FSL-BDP: Federated Survival Learning with Bayesian Differential Privacy for Credit Risk Modeling](https://arxiv.org/abs/2601.11134)
*Sultan Amed,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: FSL-BDP enables privacy-preserving federated modeling of time-to-default; Bayesian DP gains under federation are large, reversing the centralization trend; architecture dictates privacy mechanism choice.


<details>
  <summary>Details</summary>
Motivation: Regulatory data-protection rules (e.g., GDPR/CCPA) restrict cross-border borrower data sharing, hindering cross-institution learning for survival/default models. Binary default prediction ignores timing and central training conflicts with privacy constraints.

Method: Federated Survival Learning with Bayesian Differential Privacy (FSL-BDP) that models time-to-default trajectories without centralizing data, providing Bayesian data-dependent DP guarantees while enabling joint learning across institutions.

Result: Federation alters the relative effectiveness of privacy mechanisms: Bayesian DP gains substantially from federation (+7.0%) while classical DP gains only modestly (+1.4%) and approaches non-private performance; Bayesian DP often matches non-private performance and outperforms classical DP on most clients.

Conclusion: Privacy mechanism selection should be evaluated in the target deployment architecture rather than centralized benchmarks; provides practical guidance for privacy-preserving, multi-institutional decision-support systems in regulated environments.

Abstract: Credit risk models are a critical decision-support tool for financial institutions, yet tightening data-protection rules (e.g., GDPR, CCPA) increasingly prohibit cross-border sharing of borrower data, even as these models benefit from cross-institution learning. Traditional default prediction suffers from two limitations: binary classification ignores default timing, treating early defaulters (high loss) equivalently to late defaulters (low loss), and centralized training violates emerging regulatory constraints. We propose a Federated Survival Learning framework with Bayesian Differential Privacy (FSL-BDP) that models time-to-default trajectories without centralizing sensitive data. The framework provides Bayesian (data-dependent) differential privacy (DP) guarantees while enabling institutions to jointly learn risk dynamics. Experiments on three real-world credit datasets (LendingClub, SBA, Bondora) show that federation fundamentally alters the relative effectiveness of privacy mechanisms. While classical DP performs better than Bayesian DP in centralized settings, the latter benefits substantially more from federation (+7.0\% vs +1.4\%), achieving near parity of non-private performance and outperforming classical DP in the majority of participating clients. This ranking reversal yields a key decision-support insight: privacy mechanism selection should be evaluated in the target deployment architecture, rather than centralized benchmarks. These findings provide actionable guidance for practitioners designing privacy-preserving decision support systems in regulated, multi-institutional environments.

</details>


### [81] [Context-aware Graph Causality Inference for Few-Shot Molecular Property Prediction](https://arxiv.org/abs/2601.11135)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: 提出 CaMol：一个上下文感知的图因果推断框架，用于少样本分子属性预测，结合上下文图、原子掩蔽和后门调整以提升准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决少样本情境下如何利用官能基等先验化学知识和直接相关的子结构的问题，以及现有基于上下文学习的方法在此方面的不足。

Method: 1) 构建上下文图，连接官能基、分子与属性，以引导因果子结构的发现；2) 引入可学习的原子掩蔽策略，区分因果子结构与混杂因素；3) 引入分布干预器，使用后门调整，将因果子结构与化学基础混杂项结合，以分离因果效应与现实化学变异。

Result: 在多样分子数据集上，CaMol 在少样本任务上实现更高精度和更强样本效率，具备对未见属性的泛化能力；所发现的因果子结构与官能基知识高度对齐，提升可解释性。

Conclusion: 以因果推理为核心的 CaMol 展示了在少样本分子属性预测中的潜力，具有较强的泛化性和可解释性。

Abstract: Molecular property prediction is becoming one of the major applications of graph learning in Web-based services, e.g., online protein structure prediction and drug discovery. A key challenge arises in few-shot scenarios, where only a few labeled molecules are available for predicting unseen properties. Recently, several studies have used in-context learning to capture relationships among molecules and properties, but they face two limitations in: (1) exploiting prior knowledge of functional groups that are causally linked to properties and (2) identifying key substructures directly correlated with properties. We propose CaMol, a context-aware graph causality inference framework, to address these challenges by using a causal inference perspective, assuming that each molecule consists of a latent causal structure that determines a specific property. First, we introduce a context graph that encodes chemical knowledge by linking functional groups, molecules, and properties to guide the discovery of causal substructures. Second, we propose a learnable atom masking strategy to disentangle causal substructures from confounding ones. Third, we introduce a distribution intervener that applies backdoor adjustment by combining causal substructures with chemically grounded confounders, disentangling causal effects from real-world chemical variations. Experiments on diverse molecular datasets showed that CaMol achieved superior accuracy and sample efficiency in few-shot tasks, showing its generalizability to unseen properties. Also, the discovered causal substructures were strongly aligned with chemical knowledge about functional groups, supporting the model interpretability.

</details>


### [82] [Assesing the Viability of Unsupervised Learning with Autoencoders for Predictive Maintenance in Helicopter Engines](https://arxiv.org/abs/2601.11154)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 两种预测性维护策略对直升机发动机的对比：监督学习需要标签，基于自编码器的无监督异常检测在缺乏故障标记时也能有效检测异常，二者在准确性、数据可用性和部署可行性上存在权衡。


<details>
  <summary>Details</summary>
Motivation: 降低直升机发动机故障相关的运营中断、安全风险和维护成本；评估有标签监督和无标签无监督方法在真实数据上的适用性与性能。

Method: 在真实数据集上，比较基于有标签的监督分类管线与基于自编码器的无监督异常检测。监督方法使用正常与故障样本的标签，AE仅用健康数据训练以建模正常操作并检测偏离。

Result: 监督模型在有故障标记时具有较高准确性；AE在缺乏故障标签的场景下仍能有效检测异常，适用于故障数据稀缺的情况。

Conclusion: 在准确性、数据获取难易和部署可行性之间存在权衡。无监督学习为航空航天的早期故障检测提供了可行的解决方案，尤其在故障样本不足时具有优势。

Abstract: Unplanned engine failures in helicopters can lead to severe operational disruptions, safety hazards, and costly repairs. To mitigate these risks, this study compares two predictive maintenance strategies for helicopter engines: a supervised classification pipeline and an unsupervised anomaly detection approach based on autoencoders (AEs). The supervised method relies on labelled examples of both normal and faulty behaviour, while the unsupervised approach learns a model of normal operation using only healthy engine data, flagging deviations as potential faults. Both methods are evaluated on a real-world dataset comprising labelled snapshots of helicopter engine telemetry. While supervised models demonstrate strong performance when annotated failures are available, the AE achieves effective detection without requiring fault labels, making it particularly well suited for settings where failure data are scarce or incomplete. The comparison highlights the practical trade-offs between accuracy, data availability, and deployment feasibility, and underscores the potential of unsupervised learning as a viable solution for early fault detection in aerospace applications.

</details>


### [83] [Theoretically and Practically Efficient Resistance Distance Computation on Large Graphs](https://arxiv.org/abs/2601.11159)
*Yichun Yang,Longlong Lin,Rong-Hua Li,Meihao Liao,Guoren Wang*

Main category: cs.LG

TL;DR: 提出两种基于Lanczos的抵抗距离算法（Lanczos Iteration 与 Lanczos Push），实现全局近线时间与局部时间复杂度，显著降低对条件数κ的依赖，并在八个真实数据集上优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模图中抵抗距离的高复杂度问题；现有时基于幂迭代和随机游走的局部方法在κ较大时收敛慢或效率低下。

Method: 提出两种Lanczos启发的算法：Lanczos Iteration（全局，近线时间，复杂度~O~(√κ m)）与Lanczos Push（局部，复杂度~O~(κ^{2.75})，在若干假设下成立）；给出理论复杂度并进行八个真实数据集的广泛实验以验证性能。

Result: Lanczos Iteration相对于SOTA全局幂迭代获得约√κ量级的加速；Lanczos Push相对于SOTA局部方法在κ依赖方面提升约κ^{0.25}，实验显示在效率与准确性上显著优于对比方法。

Conclusion: 基于Lanczos的两类方法为抵抗距离提供近线时间的全局与局部解，显著降低对条件数κ的依赖，具备较强的实用性与推广性。

Abstract: The computation of resistance distance is pivotal in a wide range of graph analysis applications, including graph clustering, link prediction, and graph neural networks. Despite its foundational importance, efficient algorithms for computing resistance distances on large graphs are still lacking. Existing state-of-the-art (SOTA) methods, including power iteration-based algorithms and random walk-based local approaches, often struggle with slow convergence rates, particularly when the condition number of the graph Laplacian matrix, denoted by $κ$, is large. To tackle this challenge, we propose two novel and efficient algorithms inspired by the classic Lanczos method: Lanczos Iteration and Lanczos Push, both designed to reduce dependence on $κ$. Among them, Lanczos Iteration is a near-linear time global algorithm, whereas Lanczos Push is a local algorithm with a time complexity independent of the size of the graph. More specifically, we prove that the time complexity of Lanczos Iteration is $\tilde{O}(\sqrtκ m)$ ($m$ is the number of edges of the graph and $\tilde{O}$ means the complexity omitting the $\log$ terms) which achieves a speedup of $\sqrtκ$ compared to previous power iteration-based global methods. For Lanczos Push, we demonstrate that its time complexity is $\tilde{O}(κ^{2.75})$ under certain mild and frequently established assumptions, which represents a significant improvement of $κ^{0.25}$ over the SOTA random walk-based local algorithms. We validate our algorithms through extensive experiments on eight real-world datasets of varying sizes and statistical properties, demonstrating that Lanczos Iteration and Lanczos Push significantly outperform SOTA methods in terms of both efficiency and accuracy.

</details>


### [84] [Clustering High-dimensional Data: Balancing Abstraction and Representation Tutorial at AAAI 2026](https://arxiv.org/abs/2601.11160)
*Claudia Plant,Lena G. M. Bauer,Christian Böhm*

Main category: cs.LG

TL;DR: 对聚类方法在抽象与表示之间权衡的概念性综述，比较 K-means、子空间聚类与深度聚类，并展望未来方向。


<details>
  <summary>Details</summary>
Motivation: 在大规模高维数据中，如何在抽象化细节与保留有用特征之间取得平衡，以便发现自然划分的簇。传统的 K-means 采用高度抽象和简单表示，但对复杂数据的表达能力不足，因此需要更丰富的表示，并在目标函数中显式引入抽象约束以确保聚类而非仅仅表示学习。

Method: 对比不同策略：子空间聚类通过学习一个用于聚类信息的潜在空间和一个捕获其他信息的潜在空间来分离信息；深度聚类通过中心化和密度等损失来实现抽象化；讨论 K-means、子空间聚类、深度聚类在表示能力与抽象约束之间的权衡，以及如何通过子空间方法实现两个潜在空间的分离。

Result: 给出一个框架性综述，归纳不同方法在抽象-表示权衡上的设计思路、损失函数和实现策略；未给出新的实验结果，主要提供理论框架和未来研究方向的整理。

Conclusion: 未来方向将更自适应地在抽象与表示之间取得平衡，以提升聚类性能、能效与可解释性；借鉴大脑的 clustering 能力，推动聚类方法的进一步发展。

Abstract: How to find a natural grouping of a large real data set? Clustering requires a balance between abstraction and representation. To identify clusters, we need to abstract from superfluous details of individual objects. But we also need a rich representation that emphasizes the key features shared by groups of objects that distinguish them from other groups of objects.
  Each clustering algorithm implements a different trade-off between abstraction and representation. Classical K-means implements a high level of abstraction - details are simply averaged out - combined with a very simple representation - all clusters are Gaussians in the original data space. We will see how approaches to subspace and deep clustering support high-dimensional and complex data by allowing richer representations. However, with increasing representational expressiveness comes the need to explicitly enforce abstraction in the objective function to ensure that the resulting method performs clustering and not just representation learning. We will see how current deep clustering methods define and enforce abstraction through centroid-based and density-based clustering losses. Balancing the conflicting goals of abstraction and representation is challenging. Ideas from subspace clustering help by learning one latent space for the information that is relevant to clustering and another latent space to capture all other information in the data.
  The tutorial ends with an outlook on future research in clustering. Future methods will more adaptively balance abstraction and representation to improve performance, energy efficiency and interpretability. By automatically finding the sweet spot between abstraction and representation, the human brain is very good at clustering and other related tasks such as single-shot learning. So, there is still much room for improvement.

</details>


### [85] [GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling](https://arxiv.org/abs/2601.11161)
*Pascal Schlachter,Bin Yang*

Main category: cs.LG

TL;DR: 提出了首个连续的源无监督通用领域自适应（SF-UniDA）方法，GMM-COMET，在无标注的连续目标域上，通过高斯混合模型伪标签与均值教师框架结合，以及一致性损失，成为对源模型稳定且在多场景下普遍改进的基线。


<details>
  <summary>Details</summary>
Motivation: 现实场景中源数据不可用且目标域的标签空间可能与源域不同，需在连续的多目标域中进行自适应，提升模型对领域偏移的鲁棒性。

Method: 在在线SF-UniDA基础上，融入高斯混合模型（GMM）伪标签、均值教师（mean teacher）框架，以及一致性损失，构建GMM-COMET，以实现对连续目标域的稳健适应。

Result: 在实验中，GMM-COMET成为第一条对所有评估场景均优于源模型的强基线，在持续的多目标域序列中表现稳健，且在所有评估情境中优于源模型。

Conclusion: 提出的GMM-COMET为连续SF-UniDA提供了有力的基线，证明结合GMM伪标签、均值教师和一致性损失能提升对连续目标域的适应性和稳定性。代码公开。

Abstract: Unsupervised domain adaptation tackles the problem that domain shifts between training and test data impair the performance of neural networks in many real-world applications. Thereby, in realistic scenarios, the source data may no longer be available during adaptation, and the label space of the target domain may differ from the source label space. This setting, known as source-free universal domain adaptation (SF-UniDA), has recently gained attention, but all existing approaches only assume a single domain shift from source to target. In this work, we present the first study on continual SF-UniDA, where the model must adapt sequentially to a stream of multiple different unlabeled target domains. Building upon our previous methods for online SF-UniDA, we combine their key ideas by integrating Gaussian mixture model-based pseudo-labeling within a mean teacher framework for improved stability over long adaptation sequences. Additionally, we introduce consistency losses for further robustness. The resulting method GMM-COMET provides a strong first baseline for continual SF-UniDA and is the only approach in our experiments to consistently improve upon the source-only model across all evaluated scenarios. Our code is available at https://github.com/pascalschlachter/GMM-COMET.

</details>


### [86] [LSTM VS. Feed-Forward Autoencoders for Unsupervised Fault Detection in Hydraulic Pumps](https://arxiv.org/abs/2601.11163)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 两种无监督自编码器用于泵故障的早期检测：单帧快照分析的前馈自编码器与捕获短时序的LSTM自编码器；在仅用健康数据训练、用包含故障区间的独立测试数据评估后，仍呈现高可靠性。


<details>
  <summary>Details</summary>
Motivation: 工业液压泵的非计划故障会导致停产和高成本；标注故障样本稀缺，难以进行有监督学习，因此利用丰富的健康数据进行无监督异常检测成为可行途径。

Method: 设计两种自编码器架构并行比较：1) 前馈自编码器，对单一传感器快照进行分析（无时序上下文）；2) LSTM自编码器，利用短时序窗口捕获动态特征。两者均在健康数据上训练，数据来自分钟级粒度的52个传感器通道。评估在包含七段注释故障区间的独立数据集上进行。

Result: 在未使用任何故障样本的前提下，两种模型在测试集上均表现出高可靠性，证明无监督自编码框架可有效进行故障检测；引入时序建模的LSTM版本可能对短时故障有更敏感的响应，整体性能优于仅快照分析的模型在某些场景。

Conclusion: 无监督自编码器对工业泵的早期故障检测具有实际应用价值，健康数据即可训练出有效的检测模型；与时序相关性建模结合的LSTM版本具有潜在提升空间，但需关注阈值设定、假阳性率及对新故障类型的泛化能力。

Abstract: Unplanned failures in industrial hydraulic pumps can halt production and incur substantial costs. We explore two unsupervised autoencoder (AE) schemes for early fault detection: a feed-forward model that analyses individual sensor snapshots and a Long Short-Term Memory (LSTM) model that captures short temporal windows. Both networks are trained only on healthy data drawn from a minute-level log of 52 sensor channels; evaluation uses a separate set that contains seven annotated fault intervals. Despite the absence of fault samples during training, the models achieve high reliability.

</details>


### [87] [TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation](https://arxiv.org/abs/2601.11184)
*Xiangyu Xu,Qingsong Zhong,Jilin Hu*

Main category: cs.LG

TL;DR: 提出一种结构可解耦的多尺度时间序列生成框架，结合多分辨率离散化编码、粗到细的自回归生成和双路径 VQ-VAE，以分离趋势与季节性并利用引导重建提升高质量长序列生成，同时显著降低参数规模。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列的数据稀缺与隐私挑战，且需要捕捉多尺度的时间结构与不同组件的异质性。现有模型往往难以同时建模层级依赖与结构分解。

Method: 将序列编码为多尺度的离散 token，进行自回归的 coarse-to-fine 生成；设计双路径的 VQ-VAE 来解耦趋势与季节成分，学习语义一致的潜在表示；提出引导重建策略，用粗粒度的季节信号作为先验引导细粒度季节模式的重建。

Result: 在六个数据集上与现有方法相比，生成的时间序列质量更高，且参数量显著更少；模型在长序列生成方面表现尤为出色。

Conclusion: 该方法通过结构分离与多尺度建模实现更高质量的合成并具备良好效率，展示了对隐私友好数据合成的潜力及泛化能力。

Abstract: Generative modeling offers a promising solution to data scarcity and privacy challenges in time series analysis. However, the structural complexity of time series, characterized by multi-scale temporal patterns and heterogeneous components, remains insufficiently addressed. In this work, we propose a structure-disentangled multiscale generation framework for time series. Our approach encodes sequences into discrete tokens at multiple temporal resolutions and performs autoregressive generation in a coarse-to-fine manner, thereby preserving hierarchical dependencies. To tackle structural heterogeneity, we introduce a dual-path VQ-VAE that disentangles trend and seasonal components, enabling the learning of semantically consistent latent representations. Additionally, we present a guidance-based reconstruction strategy, where coarse seasonal signals are utilized as priors to guide the reconstruction of fine-grained seasonal patterns. Experiments on six datasets show that our approach produces higher-quality time series than existing methods. Notably, our model achieves strong performance with a significantly reduced parameter count and exhibits superior capability in generating high-quality long-term sequences. Our implementation is available at https://anonymous.4open.science/r/TimeMAR-BC5B.

</details>


### [88] [FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization](https://arxiv.org/abs/2601.11200)
*Haiyang Xiao,Weiqing Li,Jinyue Guo,Guochao Jiang,Guohua Liu,Yuewei Zhang*

Main category: cs.LG

TL;DR: 提出 FAQ：面向同家族大模型的 calibration 数据再生成，通过更大模型生成高保真校准样本并在专家引导下筛选、再归一化，从而提升 PTQ 的量化参数准确性；在 Qwen3-8B 系列上实现最多 28.5% 的准确损失下降。


<details>
  <summary>Details</summary>
Motivation: PTQ 的关键瓶颈在于校准数据的代表性和普适性不足，现有方法通常依赖有限样本，难以覆盖推理阶段的激活分布，导致量化参数偏差。

Method: 以目标模型同家族的更大模型作为知识源，将原始校准样本输入该大模型生成高保真校准数据；输出包含链式推理（Chain-of-Thought）且符合期望激活分布；在专家指导下对样本进行分组竞争以挑选最佳样本；对选定样本进行重新归一化以提升 PTQ 效果。

Result: 在多个模型系列（包括 Qwen3-8B）上，FAQ 相较于仅使用原始校准数据的基线，能够将准确率损失降低最多约 28.5%，显示其在校准数据再生成方面的有效性与潜力。

Conclusion: 利用同家族大模型的知识来生成高保真且分布符合的校准数据，并通过分组竞争与再归一化提升 PTQ 性能，具显著潜力；但需权衡成本、可扩展性及对不同家族模型的泛化性等挑战。

Abstract: Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.

</details>


### [89] [SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients](https://arxiv.org/abs/2601.11219)
*Zhikang Shen,Jianrong Lu,Haiyuan Wan,Jianhai Chen*

Main category: cs.LG

TL;DR: 提出 Selective Dual-module Federated LoRA (SDFLoRA) 以应对联邦学习下 LoRA 的秩异质性：将每个客户端的适配器分解为可全局共享的模块和本地私有的模块，全球模块进行选择性对齐并聚合，局部模块保持私有，且仅对全球模块引入差分隐私噪声。实验证明在 GLUE 上优于代表性基线，且具备更优的公私权衡。


<details>
  <summary>Details</summary>
Motivation: FL 场景下，LLM 的低秩适配（如 LoRA）在不同客户端存在秩异质性，直接聚合会产生偏置和不稳定；现有方法要么强制统一秩要么将异质更新对齐到共享子空间，限制个性化、弱隐私保护。需要在保留客户端特定语义的同时实现跨客户端的可转移知识聚合，并支持差分隐私下的优化。

Method: 对每个客户端的适配器进行双模 decomposition：全局模块捕捉可转移知识，局部模块保持客户端特定的适应。全局模块在客户端之间进行选择性对齐与聚合，局部模块保持私有。仅对全局模块引入差分隐私噪声以实现隐私保护。训练时仅聚合全局模块，局部模块不参与聚合。实验结合 GLUE 基准评估。

Result: 在 GLUE 基准上，SDFLoRA 优于代表性的联邦 LoRA 基线，且实现了更好的实用性与隐私之间的权衡，证明了在秩异质性存在的情况下该方法的鲁棒性。

Conclusion: 通过引入选择性双模块分解，SDFLoRA 同时保留了客户端特定语义与跨客户端可转移知识的聚合能力，并在隐私约束下实现更优的性能，适合在存在秩异质性的联邦学习场景中应用。

Abstract: Federated learning (FL) for large language models (LLMs) has attracted increasing attention as a way to enable privacy-preserving adaptation over distributed data. Parameter-efficient methods such as LoRA are widely adopted to reduce communication and memory costs. Despite these advances, practical FL deployments often exhibit rank heterogeneity, since different clients may use different low-rank configurations. This makes direct aggregation of LoRA updates biased and unstable. Existing solutions typically enforce unified ranks or align heterogeneous updates into a shared subspace, which over-constrains client-specific semantics, limits personalization, and provides weak protection of local client information under differential privacy noise. To address this issue, we propose Selective Dual-module Federated LoRA (SDFLoRA), which decomposes each client adapter into a global module that captures transferable knowledge and a local module that preserves client-specific adaptations. The global module is selectively aligned and aggregated across clients, while local modules remain private. This design enables robust learning under rank heterogeneity and supports privacy-aware optimization by injecting differential privacy noise exclusively into the global module. Experiments on GLUE benchmarks demonstrate that SDFLoRA outperforms representative federated LoRA baselines and achieves a better utility-privacy trade-off.

</details>


### [90] [Operator learning on domain boundary through combining fundamental solution-based artificial data and boundary integral techniques](https://arxiv.org/abs/2601.11222)
*Haochen Wu,Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: 在已知基本解的线性PDE场景中，提出MAD-BNO；仅用边界数据通过MAD合成训练数据，学习边界到边界的映射；内部解可由边界积分公式求得。对2D Laplace、Poisson、Helmholtz任务达到与现有神经算子同等或更好精度并显著降低训练时间，具三维及复杂几何扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子通常需要全域采样或大量外部数据，且难以严格体现物理约束。若能利用已知基本解及MAD以边界数据为驱动进行学习，可实现数据高效、物理一致的边界-边界映射，并直接得到内部解。

Method: 将MAD（确保物理一致性）与边界神经算子结合，从目标问题的基本解出发，合成Dirichlet-Neumann边界数据对，训练MAD生成的边界到边界映射（MAD-BNO）。训练数据全来自基本解，避免外部测量或数值仿真。训练后通过边界积分方程在边界条件下求取内部解，支持Dirichlet、Neumann、混合边界及一般源项。

Result: 在二维Laplace、Poisson、Helmholtz的基准任务中，MAD-BNO实现的精度可与现有神经算子方法相当或更优，并显著降低训练时间。框架天然支持向三维和复杂几何的扩展。

Conclusion: 提供了一种仅依赖边界数据、具物理一致性且数据驱动的算子学习方案，适用于线性PDE且具扩展潜力，尤其在边界驱动的场景中具有高数据效率与可扩展性。

Abstract: For linear partial differential equations with known fundamental solutions, this work introduces a novel operator learning framework that relies exclusively on domain boundary data, including solution values and normal derivatives, rather than full-domain sampling. By integrating the previously developed Mathematical Artificial Data (MAD) method, which enforces physical consistency, all training data are synthesized directly from the fundamental solutions of the target problems, resulting in a fully data-driven pipeline without the need for external measurements or numerical simulations. We refer to this approach as the Mathematical Artificial Data Boundary Neural Operator (MAD-BNO), which learns boundary-to-boundary mappings using MAD-generated Dirichlet-Neumann data pairs. Once trained, the interior solution at arbitrary locations can be efficiently recovered through boundary integral formulations, supporting Dirichlet, Neumann, and mixed boundary conditions as well as general source terms. The proposed method is validated on benchmark operator learning tasks for two-dimensional Laplace, Poisson, and Helmholtz equations, where it achieves accuracy comparable to or better than existing neural operator approaches while significantly reducing training time. The framework is naturally extensible to three-dimensional problems and complex geometries.

</details>


### [91] [Latent Dynamics Graph Convolutional Networks for model order reduction of parameterized time-dependent PDEs](https://arxiv.org/abs/2601.11259)
*Lorenzo Tomada,Federico Pichi,Gianluigi Rozza*

Main category: cs.LG

TL;DR: Encoder-free Latent Dynamics Graph Convolutional Network (LD-GCN) for nonlinear MOR of time-dependent parameterized PDEs; learns global latent dynamics conditioned on inputs/parameters, with latent time-stepping and a GNN decoder on geometrically parameterized domains; enables time extrapolation, latent interpolation for zero-shot prediction, and includes a universal approximation theorem; validated on Navier–Stokes bifurcations and other computational mechanics problems; code available.


<details>
  <summary>Details</summary>
Motivation: Current graph-based MOR methods struggle to fuse geometric inductive biases with interpretable latent dynamics and often neglect dynamics-driven features or spatial information. A unified encoder-free framework is proposed to learn a global latent representation that evolves in time and maps back to geometry-driven outputs.

Method: Propose LD-GCN, a purely data-driven, encoder-free architecture. Learn a global low-dimensional latent state conditioned on external inputs and parameters. Model temporal evolution in latent space via time-stepping. Decode trajectories onto geometrically parameterized domains with a GNN. Enable time-extrapolation and zero-shot predictions through latent interpolation. Provide a universal approximation theorem for encoder-free architectures. Validate on complex computational mechanics problems with physical/geometric parameters, including Navier–Stokes bifurcations. Code at the provided GitHub URL.

Result: Demonstrates that LD-GCN can capture reduced dynamics with interpretability, perform time extrapolation, and enable zero-shot latent interpolation. The framework is validated numerically on challenging PDE problems, including bifurcation phenomena in Navier–Stokes equations, indicating potential for accurate and geometry-aware MOR in parametric settings.

Conclusion: LD-GCN offers a geometry-aware, encoder-free MOR framework with interpretable latent dynamics, universal approximation guarantees, and practical effectiveness in complex PDE settings, including bifurcations. It supports time extrapolation and zero-shot prediction via latent interpolation and is reproducible via the provided code.

Abstract: Graph Neural Networks (GNNs) are emerging as powerful tools for nonlinear Model Order Reduction (MOR) of time-dependent parameterized Partial Differential Equations (PDEs). However, existing methodologies struggle to combine geometric inductive biases with interpretable latent behavior, overlooking dynamics-driven features or disregarding spatial information. In this work, we address this gap by introducing Latent Dynamics Graph Convolutional Network (LD-GCN), a purely data-driven, encoder-free architecture that learns a global, low-dimensional representation of dynamical systems conditioned on external inputs and parameters. The temporal evolution is modeled in the latent space and advanced through time-stepping, allowing for time-extrapolation, and the trajectories are consistently decoded onto geometrically parameterized domains using a GNN. Our framework enhances interpretability by enabling the analysis of the reduced dynamics and supporting zero-shot prediction through latent interpolation. The methodology is mathematically validated via a universal approximation theorem for encoder-free architectures, and numerically tested on complex computational mechanics problems involving physical and geometric parameters, including the detection of bifurcating phenomena for Navier-Stokes equations. Code availability: https://github.com/lorenzotomada/ld-gcn-rom

</details>


### [92] [Sample-Near-Optimal Agnostic Boosting with Improved Running Time](https://arxiv.org/abs/2601.11265)
*Arthur da Cunha,Miakel Møller Høgsgaard,Andrea Paudice*

Main category: cs.LG

TL;DR: 首个在 agnostic boosting 设置中实现 near-optimal 样本复杂度并且时间复杂度为多项式的算法。


<details>
  <summary>Details</summary>
Motivation: 在无假设的提升场景中，尽管样本复杂度界限接近被确立，但现有算法计算成本为指数级，缺乏高效实现。

Method: 提出一个新的 agnostic boosting 算法，达到近似最优的样本复杂度，同时在固定其他参数的条件下，其运行时间为多项式。

Result: 证明该算法在近似最优样本复杂度下工作，且时间复杂度为多项式。

Conclusion: 填补 agnostic boosting 在样本效率与计算效率之间的空缺，为实际应用提供更高效的提升方案。

Abstract: Boosting is a powerful method that turns weak learners, which perform only slightly better than random guessing, into strong learners with high accuracy. While boosting is well understood in the classic setting, it is less so in the agnostic case, where no assumptions are made about the data. Indeed, only recently was the sample complexity of agnostic boosting nearly settled arXiv:2503.09384, but the known algorithm achieving this bound has exponential running time. In this work, we propose the first agnostic boosting algorithm with near-optimal sample complexity, running in time polynomial in the sample size when considering the other parameters of the problem fixed.

</details>


### [93] [Metabolomic Biomarker Discovery for ADHD Diagnosis Using Interpretable Machine Learning](https://arxiv.org/abs/2601.11283)
*Nabil Belacel,Mohamed Rachid Boulassel*

Main category: cs.LG

TL;DR: 用尿液代谢组学结合可解释的 Closest Resemblance 分类器，利用14种代谢物实现对 ADHD 的高效、生物学基础的诊断识别，AUC > 0.97，具转化潜力。


<details>
  <summary>Details</summary>
Motivation: ADHD 缺乏客观、生物学基础的诊断工具，需在精准精神病学框架内开发可解释、低成本的诊断方法。

Method: 针对性代谢组学对52例 ADHD 与46例对照样本进行分析，应用 Closest Resemblance (CR) 分类器并嵌入特征选择；与随机森林、KNN 比较；评估指标为 AUC；结果以 14 种代谢物组成的面板。

Result: CR 模型在与 RF、KNN 比较时表现更好，AUC > 0.97，14 种代谢物组成的面板包括 dopamine 4-sulfate、N-acetylaspartylglutamic acid、citrulline 等，映射到多巴胺能神经传导与氨基酸代谢通路；模型透明、计算成本低，便于临床转化和点诊断平台。

Conclusion: 本研究展示了将代谢组学与可解释机器学习结合的转化框架，为 ADHD 提供客观、生物学基础的诊断策略，具备进入靶向代谢检测与未来现场诊断的潜力。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a prevalent neurodevelopmental disorder with limited objective diagnostic tools, highlighting the urgent need for objective, biology-based diagnostic frameworks in precision psychiatry. We integrate urinary metabolomics with an interpretable machine learning framework to identify biochemical signatures associated with ADHD. Targeted metabolomic profiles from 52 ADHD and 46 control participants were analyzed using a Closest Resemblance (CR) classifier with embedded feature selection. The CR model outperformed Random Forest and K-Nearest Neighbor classifiers, achieving an AUC > 0.97 based on a reduced panel of 14 metabolites. These metabolites including dopamine 4-sulfate, N-acetylaspartylglutamic acid, and citrulline map to dopaminergic neurotransmission and amino acid metabolism pathways, offering mechanistic insight into ADHD pathophysiology. The CR classifier's transparent decision boundaries and low computational cost support integration into targeted metabolomic assays and future point of care diagnostic platforms. Overall, this work demonstrates a translational framework combining metabolomics and interpretable machine learning to advance objective, biologically informed diagnostic strategies for ADHD.

</details>


### [94] [FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning](https://arxiv.org/abs/2601.11311)
*Zhihan Yang,Jiaqi Wei,Xiang Zhang,Haoyu Dong,Yiwen Wang,Xiaoke Guo,Pengkun Zhang,Yiwei Xu,Chenyu You*

Main category: cs.LG

TL;DR: FORESTLLM combines decision forests with LLMs in a training-only fashion for few-shot tabular learning, using semantic splits guided by the LLM and a one-time in-context leaf stabilization to produce a lightweight, interpretable model that does not require LLM inference at test time.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning on tabular data is difficult: traditional trees rely on unstable purity metrics; LLMs, when applied directly, ignore structure and may be inefficient. A training-time integration of semantic knowledge into an interpretable forest aims to improve robustness and generalization.

Method: Two-part approach: (1) semantic splitting criterion where the LLM evaluates candidate partitions on coherence across labeled and unlabeled data to induce robust tree structures; (2) one-time in-context inference for leaf stabilization, where the LLM distills the decision path and supporting examples into a concise deterministic prediction, replacing noisy empirical estimates.

Result: Reported state-of-the-art performance on a diverse set of few-shot classification and regression benchmarks.

Conclusion: FORESTLLM demonstrates that training-time integration of LLM-derived semantic reasoning into decision forests can yield robust, interpretable models for few-shot tabular tasks without incurring test-time LLM costs.

Abstract: Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.

</details>


### [95] [Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models](https://arxiv.org/abs/2601.11342)
*Chuanyue Yu,Jiahui Wang,Yuhan Li,Heng Chang,Ge Lan,Qingyun Sun,Jia Li,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: SPREAD 在 RAG 框架下通过查询相关性引导的去噪策略，有效抑制响应语义漂移（RSD），显著提升 DLM 的生成精度与查询一致性。


<details>
  <summary>Details</summary>
Motivation: 在 DLM 与 RAG 的结合中，尽管对上下文敏感性强，但生成精度受限。RSD 导致回答逐步偏离查询语义，核心原因在于去噪阶段的语义对齐不足。

Method: 提出 SPREAD 框架，使用查询相关性引导的去噪策略，在迭代去噪过程中主动控制轨迹以维持与查询语义的一致性，从而抑制漂移。

Result: 实验结果显示 SPREAD 能显著提升生成精度并有效抑制 RSD，在 RAG 框架下改善 DLM 的输出质量。

Conclusion: SPREAD 为在 RAG 场景中应用 DLM 的生成提供有效的语义保持机制，缓解语义漂移问题，提升结果的查询一致性与精度。

Abstract: Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.

</details>


### [96] [FEATHer: Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting](https://arxiv.org/abs/2601.11350)
*Jaehoon Lee,Seungwoo Lee,Younghwi Kim,Dohee Kim,Sunghyun Sim*

Main category: cs.LG

TL;DR: 提出一个极轻量化的时序长预测模型 FEATHer，在边缘设备极限资源条件下实现可靠长期预测，参数仅数百至千级，在八项基准上表现突出


<details>
  <summary>Details</summary>
Motivation: 工业领域对边缘部署的强约束（延迟、内存、功耗）迫切需要无回路、无注意力机制的高效时序预测模型，以便在PLC、微控制器等设备上实现实时推理

Method: FEATHer 由四大模块组成：1) 超轻量的多尺度频率路径分解；2) 共享 Dense Temporal Kernel，采用投影-深度卷积-投影的无回归无注意力结构；3) 频率感知分支门控，基于谱特征自适应融合表示；4) 稀疏周期核通过按周期下采样重构输出以捕捉季节性，整体参数量可降至约400。

Result: 在八项基准上实现最佳排名，获得60个第一名，平均排名2.05，显著优于对比模型，显示在资源受限的边缘硬件上也能实现可靠的长时序预测

Conclusion: FEATHer证实极端资源限制下的边缘推理是可行的，为工业实时推理提供一种实用且高效的长期预测方案

Abstract: Time-series forecasting is fundamental in industrial domains like manufacturing and smart factories. As systems evolve toward automation, models must operate on edge devices (e.g., PLCs, microcontrollers) with strict constraints on latency and memory, limiting parameters to a few thousand. Conventional deep architectures are often impractical here. We propose the Fourier-Efficient Adaptive Temporal Hierarchy Forecaster (FEATHer) for accurate long-term forecasting under severe limits. FEATHer introduces: (i) ultra-lightweight multiscale decomposition into frequency pathways; (ii) a shared Dense Temporal Kernel using projection-depthwise convolution-projection without recurrence or attention; (iii) frequency-aware branch gating that adaptively fuses representations based on spectral characteristics; and (iv) a Sparse Period Kernel reconstructing outputs via period-wise downsampling to capture seasonality. FEATHer maintains a compact architecture (as few as 400 parameters) while outperforming baselines. Across eight benchmarks, it achieves the best ranking, recording 60 first-place results with an average rank of 2.05. These results demonstrate that reliable long-range forecasting is achievable on constrained edge hardware, offering a practical direction for industrial real-time inference.

</details>


### [97] [Offline Reinforcement-Learning-Based Power Control for Application-Agnostic Energy Efficiency](https://arxiv.org/abs/2601.11352)
*Akhilesh Raj,Swann Perarnau,Aniruddha Gokhale,Solomon Bekele Abera*

Main category: cs.LG

TL;DR: Offline RL can train a CPU power controller using pre-collected state transitions and performance counters to reduce energy use with limited performance impact; validated on Intel RAPL with diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: Online RL for energy-aware CPU control faces challenges such as lack of accurate simulators, perturbations, and reliability on live systems. Offline RL mitigates these by learning from pre-collected data.

Method: Gray-box energy efficiency approach that fuses online application-agnostic performance signals (heartbeats) with hardware performance counters. Train an offline RL agent from a dataset of state transitions collected from arbitrary policies, then deploy to control Running Average Power Limit (RAPL) on a live system.

Result: The offline-trained agent achieves substantial energy reduction with tolerable performance degradation across compute-bound and memory-bound benchmarks, validated on live hardware.

Conclusion: Offline RL is a viable strategy for autonomous CPU power control, enabling energy savings with limited impact on performance; promising for production deployments, with considerations for data quality and distribution shifts.

Abstract: Energy efficiency has become an integral aspect of modern computing infrastructure design, impacting the performance, cost, scalability, and durability of production systems. The incorporation of power actuation and sensing capabilities in CPU designs is indicative of this, enabling the deployment of system software that can actively monitor and adjust energy consumption and performance at runtime. While reinforcement learning (RL) would seem ideal for the design of such energy efficiency control systems, online training presents challenges ranging from the lack of proper models for setting up an adequate simulated environment, to perturbation (noise) and reliability issues, if training is deployed on a live system.
  In this paper we discuss the use of offline reinforcement learning as an alternative approach for the design of an autonomous CPU power controller, with the goal of improving the energy efficiency of parallel applications at runtime without unduly impacting their performance. Offline RL sidesteps the issues incurred by online RL training by leveraging a dataset of state transitions collected from arbitrary policies prior to training.
  Our methodology applies offline RL to a gray-box approach to energy efficiency, combining online application-agnostic performance data (e.g., heartbeats) and hardware performance counters to ensure that the scientific objectives are met with limited performance degradation. Evaluating our method on a variety of compute-bound and memory-bound benchmarks and controlling power on a live system through Intel's Running Average Power Limit, we demonstrate that such an offline-trained agent can substantially reduce energy consumption at a tolerable performance degradation cost.

</details>


### [98] [Forcing and Diagnosing Failure Modes of Fourier Neural Operators Across Diverse PDE Families](https://arxiv.org/abs/2601.11428)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 对 Fourier Neural Operators (FNOs) 的鲁棒性进行大规模压力测试，横跨五类 PDE，揭示分布偏移、分辨率外推等对误差的放大作用，提出失败模式图谱以提升算子学习鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 系统性地评估 FNOs 在分布偏移、长期滚动预测和结构性扰动下的鲁棒性，超越仅追求分布内准确率的研究。

Method: 构建一个五类 PDE 家族（色散型、椭圆型、多尺度流体、金融及混沌系统）的压力测试框架；在参数、边界/终止条件、分辨率外推（伴随频谱分析）、迭代滚动等方面设计控制性故障场景；对1,000个训练模型进行大规模评估，分析谱偏差、累积积分误差、对受限边界域的过拟合等现象。

Result: 分布偏移（参数/边界条件）可使误差放大超过一个数量级；分辨率变化将误差集中在高频模式；输入扰动通常不放大误差，极端场景（如局部泊松扰动）仍具挑战性；揭示了谱偏差、滚动累积误差及边界域过拟合等典型失败模式。

Conclusion: 提供一个跨 PDE 的失败模式数据库，为提高算子学习鲁棒性提供可操作的见解与方向，如改进边界信息利用、谱 bias 控制和稳定滚动的训练策略。

Abstract: Fourier Neural Operators (FNOs) have shown strong performance in learning solution maps of partial differential equations (PDEs), but their robustness under distribution shifts, long-horizon rollouts, and structural perturbations remains poorly understood. We present a systematic stress-testing framework that probes failure modes of FNOs across five qualitatively different PDE families: dispersive, elliptic, multi-scale fluid, financial, and chaotic systems. Rather than optimizing in-distribution accuracy, we design controlled stress tests--including parameter shifts, boundary or terminal condition changes, resolution extrapolation with spectral analysis, and iterative rollouts--to expose vulnerabilities such as spectral bias, compounding integration errors, and overfitting to restricted boundary regimes. Our large-scale evaluation (1{,}000 trained models) reveals that distribution shifts in parameters or boundary conditions can inflate errors by more than an order of magnitude, while resolution changes primarily concentrate error in high-frequency modes. Input perturbations generally do not amplify error, though worst-case scenarios (e.g., localized Poisson perturbations) remain challenging. These findings provide a comparative failure-mode atlas and actionable insights for improving robustness in operator learning.

</details>


### [99] [MetaboNet: The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management](https://arxiv.org/abs/2601.11505)
*Miriam K. Wolff,Peter Calhoun,Eleonora Maria Aiello,Yao Qin,Sam F. Royston*

Main category: cs.LG

TL;DR: 建立并公开的 MetaboNet 数据集，将多份 T1D CGM 与胰岛素泵数据整合，显著扩展基准数据集规模，提升算法可比性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有 T1D 管理数据集碎片化、缺乏标准化，结构差异大，获取与处理耗时，阻碍数据整合和算法对比。

Method: 整合公开可用数据集，设立进入条件：必须包含 CGM 数据及胰岛素用药记录；若有，额外信息（碳水摄入、运动）保留。MetaboNet 包含 3135 受试者、1228 人年重叠 CGM+泵数据。数据以完全公开子集可下载，且有 DUA 限制子集，提供数据处理管线将数据自动转换为标准化 MetaboNet 格式。

Result: 数据集规模显著超过现有基准数据集；覆盖更广的血糖谱型和人口统计学多样性；提升算法性能比较的普适性与外部有效性。

Conclusion: 提供一个统一的异质数据资源及访问路径，促进 T1D 研究中的算法开发、对比与泛化。

Abstract: Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unified and accessible data resource for T1D algorithm development. Multiple publicly available T1D datasets were consolidated into a unified resource, termed the MetaboNet dataset. Inclusion required the availability of both continuous glucose monitoring (CGM) data and corresponding insulin pump dosing records. Additionally, auxiliary information such as reported carbohydrate intake and physical activity was retained when present. The MetaboNet dataset comprises 3135 subjects and 1228 patient-years of overlapping CGM and insulin data, making it substantially larger than existing standalone benchmark datasets. The resource is distributed as a fully public subset available for immediate download at https://metabo-net.org/ , and with a Data Use Agreement (DUA)-restricted subset accessible through their respective application processes. For the datasets in the latter subset, processing pipelines are provided to automatically convert the data into the standardized MetaboNet format. A consolidated public dataset for T1D research is presented, and the access pathways for both its unrestricted and DUA-governed components are described. The resulting dataset covers a broad range of glycemic profiles and demographics and thus can yield more generalizable algorithmic performance than individual datasets.

</details>


### [100] [When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models](https://arxiv.org/abs/2601.11444)
*Raphaël Razafindralambo,Rémy Sun,Frédéric Precioso,Damien Garreau,Pierre-Alexandre Mattei*

Main category: cs.LG

TL;DR: Ensembling diffusion models improves objective losses (score-matching loss, likelihood) but does not reliably boost perceptual quality (FID) on image data; results are robust across aggregation rules and evaluation settings; in tabular data, one aggregation strategy outperforms others; theoretical analysis clarifies how summing score models relates to ensembling and guidance.


<details>
  <summary>Details</summary>
Motivation: To determine whether ensembling unconditional score-based diffusion models yields tangible benefits for generative modeling, particularly for perceptual quality, and to understand why objective improvements may not translate to better perceptual metrics.

Method: Empirical study applying Deep Ensembles and Monte Carlo Dropout to diffusion score models on CIFAR-10 and FFHQ, testing various aggregation rules; evaluation uses FID and other perceptual metrics; investigation into the link between score estimation and image quality; extension to tabular data using random forests; theoretical analysis of score model summation and implications for model composition (e.g., guidance).

Result: Ensembling improves score-matching loss and model likelihood but does not consistently improve perceptual quality metrics such as FID on image datasets; results hold across multiple aggregation rules and datasets (CIFAR-10, FFHQ); for tabular data, random forests reveal a single aggregation strategy outperforms others; theoretical results offer insights into why summing score models behaves as it does, informing ensembling and guidance-related techniques.

Conclusion: Ensembling diffusion models yields limited or inconsistent gains for perceptual quality despite objective improvements; the findings motivate rethinking ensemble design for generative models and highlight the need for theory on score-model combination and guidance methods to align objective metrics with perceptual quality.

Abstract: Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).

</details>


### [101] [Low-Rank Key Value Attention](https://arxiv.org/abs/2601.11471)
*James O'Neill,Robert Clancy,Mariia Matskevichus,Fergal Reid*

Main category: cs.LG

TL;DR: LRKV introduces low-rank KV adaptation for multi-head attention, reducing KV cache memory by sharing a full-rank KV projection with per-head low-rank residuals. It acts as a drop-in replacement, subsumes MQA/GQA, and beats standard attention and MLA in large-scale pretraining, achieving similar or better model quality with 20-25% less training compute (FLOPs) and half the KV cache at 2.5B scale.


<details>
  <summary>Details</summary>
Motivation: Address the memory and compute bottleneck of the KV cache in Transformer pretraining and autoregressive decoding, while preserving token-level resolution and head diversity.

Method: Modify multi-head attention to use a shared full-rank KV projection augmented by low-rank, head-specific residuals, enabling a continuum from full sharing to independent attention; a drop-in replacement for standard attention that subsumes MQA/GQA.

Result: LRKV consistently yields faster loss reduction, lower validation perplexity, and stronger downstream performance than standard attention, MQA/GQA, and MLA in large-scale pretraining. At 2.5B-scale, LRKV uses about half the KV cache and outperforms standard attention; achieves equivalent model quality with 20-25% less training compute (cumulative FLOPs). Attention-head structure analysis shows LRKV preserves nearly all functional head diversity, unlike aggressive KV-sharing methods which rely on compensatory query specialization.

Conclusion: LRKV is a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes, directly replacing standard attention and providing a principled balance between sharing and head-specific expressivity.

Abstract: Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [102] [Tail-Aware Data Augmentation for Long-Tail Sequential Recommendation](https://arxiv.org/abs/2601.10933)
*Yizhou Dang,Zhifu Wei,Minhan Huang,Lianbo Ma,Jianzhe Zhao,Guibing Guo,Xingwei Wang*

Main category: cs.IR

TL;DR: Tail-Aware Data Augmentation (TADA) for long-tail sequential recommendation enhances tail interactions by two operators (T-Substitute and T-Insert) built on a linear model of low-popularity item co-occurrence, with representation-level mix and cross-sequence augmentation to improve tail performance while preserving head performance.


<details>
  <summary>Details</summary>
Motivation: In sequential recommendation, most users and items lie in the long tail, leading to scarce interactions for tail parts and incomplete preference learning. Existing methods either fail to improve tail performance sufficiently or hurt head performance. There's a need to boost tail learning without sacrificing head accuracy.

Method: Model a linear co-occurrence among low-popularity items. Introduce two tail-aware augmentations: T-Substitute (replace a head item with a relevant tail item) and T-Insert (extend the sequence by inserting co-occurring items, including head and tail). Mix augmented sequences with original at the representation level to preserve preferences. Extend mix across different tail-user sequences and augmented sequences to create richer samples.

Result: Extensive experiments show the proposed method outperforms baselines on tail-related metrics, validating improved tail learning while maintaining or improving overall/head performance. Code is released at the provided GitHub link.

Conclusion: TADA effectively mitigates long-tail issues in sequential recommendation by targeted tail-aware augmentations and mixed representations, increasing tail-user/item learning without compromising head performance.

Abstract: Sequential recommendation (SR) learns user preferences based on their historical interaction sequences and provides personalized suggestions. In real-world scenarios, most users can only interact with a handful of items, while the majority of items are seldom consumed. This pervasive long-tail challenge limits the model's ability to learn user preferences. Despite previous efforts to enrich tail items/users with knowledge from head parts or improve tail learning through additional contextual information, they still face the following issues: 1) They struggle to improve the situation where interactions of tail users/items are scarce, leading to incomplete preferences learning for the tail parts. 2) Existing methods often degrade overall or head parts performance when improving accuracy for tail users/items, thereby harming the user experience. We propose Tail-Aware Data Augmentation (TADA) for long-tail sequential recommendation, which enhances the interaction frequency for tail items/users while maintaining head performance, thereby promoting the model's learning capabilities for the tail. Specifically, we first capture the co-occurrence and correlation among low-popularity items by a linear model. Building upon this, we design two tail-aware augmentation operators, T-Substitute and T-Insert. The former replaces the head item with a relevant item, while the latter utilizes co-occurrence relationships to extend the original sequence by incorporating both head and tail items. The augmented and original sequences are mixed at the representation level to preserve preference knowledge. We further extend the mix operation across different tail-user sequences and augmented sequences to generate richer augmented samples, thereby improving tail performance. Comprehensive experiments demonstrate the superiority of our method. The codes are provided at https://github.com/KingGugu/TADA.

</details>


### [103] [Can Instructed Retrieval Models Really Support Exploration?](https://arxiv.org/abs/2601.10936)
*Piyush Maheshwari,Sheshera Mysore,Hamed Zamani*

Main category: cs.IR

TL;DR: 面向探索性检索的指令跟随检索模型在排名质量上有提升，但其对指令的跟随行为未与排名提升一致，可能在长时探索对指令敏感度不足或出现反直觉行为。对比指令微调的检索模型与通用LLM prompts的表现。


<details>
  <summary>Details</summary>
Motivation: 评估在 aspect-conditional seed-guided 探索中，指令化检索模型的有效性与局限性，比较专门微调的指令检索模型与使用提示词的通用LLM 在排序任务中的表现。

Method: 使用专家注释的测试集评估，比较最近的指令化检索模型（fine-tuned）与通用LLM（通过 Pairwise Ranking Prompting 提示进行排序）。度量检索相关性和指令跟随性能。

Result: 最好的指令化检索模型在排序相关性上优于对照的非指令化模型；但指令跟随的性能并不与排序提升成正相关，存在对指令不敏感或对指令产生反直觉行为的现象。

Conclusion: 目前的指令化检索模型在提升排序质量方面有效，但在需要高指令敏感性、长期探索式会话中，用户可能并不从中获益，需进一步研究以对齐指令遵循与用户体验。

Abstract: Exploratory searches are characterized by under-specified goals and evolving query intents. In such scenarios, retrieval models that can capture user-specified nuances in query intent and adapt results accordingly are desirable -- instruction-following retrieval models promise such a capability. In this work, we evaluate instructed retrievers for the prevalent yet under-explored application of aspect-conditional seed-guided exploration using an expert-annotated test collection. We evaluate both recent LLMs fine-tuned for instructed retrieval and general-purpose LLMs prompted for ranking with the highly performant Pairwise Ranking Prompting. We find that the best instructed retrievers improve on ranking relevance compared to instruction-agnostic approaches. However, we also find that instruction following performance, crucial to the user experience of interacting with models, does not mirror ranking relevance improvements and displays insensitivity or counter-intuitive behavior to instructions. Our results indicate that while users may benefit from using current instructed retrievers over instruction-agnostic models, they may not benefit from using them for long-running exploratory sessions requiring greater sensitivity to instructions.

</details>


### [104] [PruneRAG: Confidence-Guided Query Decomposition Trees for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11024)
*Shuguang Jiao,Xinyu Xiao,Yunfan Wei,Shuhan Qi,Chengkai Huang,Quan Z. Michael Sheng,Lina Yao*

Main category: cs.IR

TL;DR: PruneRAG提出一种基于自适应树状查询分解的检索增强生成框架，通过三大机制（自适应节点扩展、置信度引导的分支剪枝、粒度级实体锚点检索）实现稳定高效的多跳推理，显著降低检索开销并提升证据利用率，同时引入Evidence Forgetting Rate用于分析证据混用问题。


<details>
  <summary>Details</summary>
Motivation: 当前RAG在推理链条加深或检索树扩展时，面临证据遗忘和检索冗余的两大持续问题，导致检索与证据利用之间存在关键断层，需要在保留有效证据的同时提高检索效率。

Method: 构建结构化的查询分解树以支撑多跳推理；三大机制包括（1）自适应节点扩展以控制树的宽度与深度；（2）置信度引导的决策对可靠答案进行保留并剪除不确定分支；（3）粒度检索提取实体级锚点以提升检索精准度。引入Evidence Forgetting Rate以量化“金证据被检索但未被正确使用”的情形。

Result: 在多跳问答基准上，PruneRAG在准确度与效率方面均优于SOTA基线，且显著降低检索开销。

Conclusion: 通过结构化查询分解与三大机制，PruneRAG在多跳推理中能更好地保持证据并提升检索效率，提供对证据 misuse 的分析工具与量化指标。

Abstract: Retrieval-augmented generation (RAG) has become a powerful framework for enhancing large language models in knowledge-intensive and reasoning tasks. However, as reasoning chains deepen or search trees expand, RAG systems often face two persistent failures: evidence forgetting, where retrieved knowledge is not effectively used, and inefficiency, caused by uncontrolled query expansions and redundant retrieval. These issues reveal a critical gap between retrieval and evidence utilization in current RAG architectures. We propose PruneRAG, a confidence-guided query decomposition framework that builds a structured query decomposition tree to perform stable and efficient reasoning. PruneRAG introduces three key mechanisms: adaptive node expansion that regulates tree width and depth, confidence-guided decisions that accept reliable answers and prune uncertain branches, and fine-grained retrieval that extracts entity-level anchors to improve retrieval precision. Together, these components preserve salient evidence throughout multi-hop reasoning while significantly reducing retrieval overhead. To better analyze evidence misuse, we define the Evidence Forgetting Rate as a metric to quantify cases where golden evidence is retrieved but not correctly used. Extensive experiments across various multi-hop QA benchmarks show that PruneRAG achieves superior accuracy and efficiency over state-of-the-art baselines.

</details>


### [105] [Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings](https://arxiv.org/abs/2601.11124)
*Xiaoyu Liang,Yuchen Peng,Jiale Luo,Wenhao Wang,Haoji Hu,Xincheng Zhou*

Main category: cs.IR

TL;DR: 提出 Learn Before Represent (LBR) 框架：两阶段流程先注入领域知识、通过信息瓶颈约束的生成学习压缩语义并保持注意力路径，再在压缩表示上进行生成-对比学习以实现对齐，从而提升垂直领域表示质量。


<details>
  <summary>Details</summary>
Motivation: LLM+对比学习在垂直领域（如医学、化学、法学/代码检索）缺乏领域知识，导致对专业术语的知识获取不足与表现不足；现有工作只做语义对齐，未解决知识获取与目标冲突问题。

Method: 阶段1：Information Bottleneck-Constrained Generative Learning，注入领域知识，同时通过信息瓶颈压缩语义，尽量保持因果注意力以促进知识获取；阶段2：Generative-Refined Contrastive Learning，在压缩后的表征上进行对齐学习，保持架构一致性，缓解生成与对比学习的目标冲突。

Result: 在医学、化学和代码检索任务上显著超越强基线，证明在垂直领域可构建更准确、鲁棒的表示。

Conclusion: 提出一种新范式，兼容现有架构，提升垂直领域表示学习的准确性与鲁棒性。

Abstract: Large Language Models (LLMs) adapted via contrastive learning excel in general representation learning but struggle in vertical domains like chemistry and law, primarily due to a lack of domain-specific knowledge. This work identifies a core bottleneck: the prevailing ``LLM+CL'' paradigm focuses on semantic alignment but cannot perform knowledge acquisition, leading to failures on specialized terminology. To bridge this gap, we propose Learn Before Represent (LBR), a novel two-stage framework. LBR first injects domain knowledge via an Information Bottleneck-Constrained Generative Learning stage, preserving the LLM's causal attention to maximize knowledge acquisition while compressing semantics. It then performs Generative-Refined Contrastive Learning on the compressed representations for alignment. This approach maintains architectural consistency and resolves the objective conflict between generative and contrastive learning. Extensive experiments on medical, chemistry, and code retrieval tasks show that LBR significantly outperforms strong baselines. Our work establishes a new paradigm for building accurate and robust representations in vertical domains.

</details>


### [106] [Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration](https://arxiv.org/abs/2601.11144)
*Yuejie Li,Ke Yang,Tao Wang,Bolin Chen,Bowen Li,Chengjun Mao*

Main category: cs.IR

TL;DR: 提出 Deep GraphRAG，通过分层全局到局部检索及动态重排名实现对大规模层级图的高效且全面的检索与知识整合，拯救 GraphRAG 的效率和准确性瓶颈；在 Natural Questions 与 HotpotQA 上优于基线方法，且用 1.5B 模型接近 70B 模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决 GraphRAG 在大规模层级图上的全局搜索覆盖率与局部搜索效率之间的权衡，改进多阶段再排序和知识整合以提升准确性和效率；并降低对超大模型的依赖。

Method: 提出分层全局到局部检索策略，分三阶段：(1) 社群间过滤以局部上下文裁剪搜索空间；(2) 社群级细化通过实体交互分析优先相关子图；(3) 目标社群内的实体级精细检索。引入 beam search 优化的动态再排序模块进行持续过滤。知识整合模块使用紧凑型大模型（1.5B），通过 DW-GRPO 动态权重调整以平衡相关性、忠实性和简潔性。

Result: 在 Natural Questions 和 HotpotQA 上显著优于基线图检索方法，在准确性和效率上均有提升。

Conclusion: 该框架实现对层级图检索与知识整合的平衡，显示紧凑模型可接近大模型在知识整合任务的性能，具有对大型层级图的应用潜力。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between the comprehensiveness of global search and the efficiency of local search. Existing methods are often challenged by navigating large-scale hierarchical graphs, optimizing retrieval paths, and balancing exploration-exploitation dynamics, frequently lacking robust multi-stage re-ranking. To overcome these deficits, we propose Deep GraphRAG, a framework designed for a balanced approach to hierarchical retrieval and adaptive integration. It introduces a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. This strategy employs a three-stage process: (1) inter-community filtering, which prunes the search space using local context; (2) community-level refinement, which prioritizes relevant subgraphs via entity-interaction analysis; and (3) entity-level fine-grained search within target communities. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO). This novel reinforcement learning approach dynamically adjusts reward weights to balance three key objectives: relevance, faithfulness, and conciseness. This training enables compact models (1.5B) to approach the performance of large models (70B) in the integration task. Evaluations on Natural Questions and HotpotQA demonstrate that Deep GraphRAG significantly outperforms baseline graph retrieval methods in both accuracy and efficiency.

</details>


### [107] [Cross-Modal Attention Network with Dual Graph Learning in Multimodal Recommendation](https://arxiv.org/abs/2601.11151)
*Ji Dai,Quan Fang,Jun Hu,Desheng Cai,Yang Yang,Can Zhao*

Main category: cs.IR

TL;DR: CRANE proposes a cross-modal recursive attention with dual-graph embedding to improve multimedia recommendation by addressing shallow fusion and asymmetric user/item representations, achieving ~5% average gains on four datasets with scalable, efficient training.


<details>
  <summary>Details</summary>
Motivation: Current multimedia recommender systems suffer from shallow modality fusion (e.g., simple concatenation) and asymmetric feature treatment (users characterized only by IDs). A unified framework that models high-order intra- and inter-modal dependencies and learns symmetric multimodal user profiles is needed.

Method: Introduce Recursive Cross-Modal Attention (RCA) to iteratively refine modality features in a joint latent space, capturing high-order intra- and inter-modal relations. Build a symmetric dual-graph framework: a heterogeneous user-item interaction graph and a homogeneous item-item semantic graph, unified via a self-supervised contrastive objective. Explicitly construct user multimodal profiles by aggregating features of interacted items and ensure computational efficiency.

Result: Theoretical and empirical analyses claim scalability and high practical efficiency, with faster convergence on small datasets and higher performance ceilings on large-scale datasets. Empirical evaluation on four public real-world datasets shows an average ~5% improvement in key metrics over state-of-the-art baselines.

Conclusion: CRANE combines cross-modal recursive attention and dual-graph symmetric learning with contrastive fusion to capture high-order multi-modal dependencies and deliver scalable, improved performance in multimedia recommendation.

Abstract: Multimedia recommendation systems leverage user-item interactions and multimodal information to capture user preferences, enabling more accurate and personalized recommendations. Despite notable advancements, existing approaches still face two critical limitations: first, shallow modality fusion often relies on simple concatenation, failing to exploit rich synergic intra- and inter-modal relationships; second, asymmetric feature treatment-where users are only characterized by interaction IDs while items benefit from rich multimodal content-hinders the learning of a shared semantic space. To address these issues, we propose a Cross-modal Recursive Attention Network with dual graph Embedding (CRANE). To tackle shallow fusion, we design a core Recursive Cross-Modal Attention (RCA) mechanism that iteratively refines modality features based on cross-correlations in a joint latent space, effectively capturing high-order intra- and inter-modal dependencies. For symmetric multimodal learning, we explicitly construct users' multimodal profiles by aggregating features of their interacted items. Furthermore, CRANE integrates a symmetric dual-graph framework-comprising a heterogeneous user-item interaction graph and a homogeneous item-item semantic graph-unified by a self-supervised contrastive learning objective to fuse behavioral and semantic signals. Despite these complex modeling capabilities, CRANE maintains high computational efficiency. Theoretical and empirical analyses confirm its scalability and high practical efficiency, achieving faster convergence on small datasets and superior performance ceilings on large-scale ones. Comprehensive experiments on four public real-world datasets validate an average 5% improvement in key metrics over state-of-the-art baselines.

</details>


### [108] [From Knots to Knobs: Towards Steerable Collaborative Filtering Using Sparse Autoencoders](https://arxiv.org/abs/2601.11182)
*Martin Spišák,Ladislav Peška,Petr Škoda,Vojtěch Vančura,Rodrigo Alves*

Main category: cs.IR

TL;DR: 通过在 CFAE 框架中插入 SAE，获得高度可解释且在很大程度上具备单义性的潜在表示；提出将语义概念映射到单个神经元的映射函数，并用以对推荐结果进行方向性控制。


<details>
  <summary>Details</summary>
Motivation: Sparse autoencoders 在大语言模型中的解释性潜力激发了对其在其他领域的探索。协同过滤的现有表示往往缺乏明确语义解释和操控能力，因此需要可解释且可控的潜在特征。将 SAE 引入 CFAE 可在交互信号学习的表示上获得更易解释且可控的特征，从而实现对推荐的引导。

Method: 在协同自编码器 CFAE 的编码器与解码器之间插入一个稀疏自编码器 SAE，并对整个结构进行训练，以获得经过稀疏约束的中间潜在表示。进一步提出将语义概念映射到单个神经元的映射函数，并评估一种简单但有效的基于该表示的推荐方向 steering 方法。

Result: 所得到的中间表示在很大程度上具备单义性，能够将语义概念映射到个别神经元，并通过所提出的映射和 steering 策略对推荐结果实现方向性控制。

Conclusion: 首次将 SAE 应用于协同过滤，获得可解释且可控的潜在表示。提出的语义–神经元映射为在推荐系统中实现可解释性与可控性提供了新的途径，未来可扩展到更复杂的语义操控与解释分析。

Abstract: Sparse autoencoders (SAEs) have recently emerged as pivotal tools for introspection into large language models. SAEs can uncover high-quality, interpretable features at different levels of granularity and enable targeted steering of the generation process by selectively activating specific neurons in their latent activations. Our paper is the first to apply this approach to collaborative filtering, aiming to extract similarly interpretable features from representations learned purely from interaction signals. In particular, we focus on a widely adopted class of collaborative autoencoders (CFAEs) and augment them by inserting an SAE between their encoder and decoder networks. We demonstrate that such representation is largely monosemantic and propose suitable mapping functions between semantic concepts and individual neurons. We also evaluate a simple yet effective method that utilizes this representation to steer the recommendations in a desired direction.

</details>


### [109] [LLM-Assisted Pseudo-Relevance Feedback](https://arxiv.org/abs/2601.11238)
*David Otero,Javier Parapar*

Main category: cs.IR

TL;DR: A hybrid query expansion method that combines LLM-based filtering with RM3: an LLM filters the initial top-k documents before computing RM3, reducing topic drift while preserving PRF interpretability.


<details>
  <summary>Details</summary>
Motivation: Address topic drift and hallucination risks in pseudo-relevance feedback and LLM-based synthetic expansions by introducing a robust, interpretable filtering step before expansion.

Method: Insert an LLM-based relevance filter on the initial top-k documents; only documents judged relevant by the LLM are used to estimate the RM3 expanded query model.

Result: The approach yields improvements over blind PRF and a strong baseline across several datasets and evaluation metrics.

Conclusion: A simple, effective hybridization preserves the robustness and interpretability of classical PRF while leveraging LLM semantic judgment to reduce noise in expansion terms.

Abstract: Query expansion is a long-standing technique to mitigate vocabulary mismatch in ad hoc Information Retrieval. Pseudo-relevance feedback methods, such as RM3, estimate an expanded query model from the top-ranked documents, but remain vulnerable to topic drift when early results include noisy or tangential content. Recent approaches instead prompt Large Language Models to generate synthetic expansions or query variants. While effective, these methods risk hallucinations and misalignment with collection-specific terminology. We propose a hybrid alternative that preserves the robustness and interpretability of classical PRF while leveraging LLM semantic judgement. Our method inserts an LLM-based filtering stage prior to RM3 estimation: the LLM judges the documents in the initial top-$k$ ranking, and RM3 is computed only over those accepted as relevant. This simple intervention improves over blind PRF and a strong baseline across several datasets and metrics.

</details>


### [110] [From SERPs to Sound: How Search Engine Result Pages and AI-generated Podcasts Interact to Influence User Attitudes on Controversial Topics](https://arxiv.org/abs/2601.11282)
*Junjie Wang,Gaole He,Alisa Rieger,Ujwal Gadiraju*

Main category: cs.IR

TL;DR: SERPs 与 AI 生成播客在信息获取中的两种媒介结合，通过曝光顺序影响用户态度变化； viewpoint bias 与议题争议性对态度变化有作用，个体调节变量未显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着 SERP 与 AI 生成播客在信息获取中的日益融合，研究其对用户态度的影响，尤其在有争议、价值取向强的议题上。

Method: 在控制实验中（N=483），系统地操纵信息呈现的媒介与顺序，测量态度变化；并对 viewpoint bias、议题争议性等变量进行调节分析，检验是否有个人层面的调节作用。

Result: 多数参与者出现态度变化；呈现顺序对态度变化有显著影响；观点偏见和议题争议性影响态度变化；未发现个人层面的显著调节作用。

Conclusion: 信息媒介与呈现顺序对态度形成具有重要影响，需关注媒介设计对“有争议话题”的态度导向；个体层面的影响不显著，提示影响具有一定的普适性。

Abstract: Compared to search engine result pages (SERPs), AI-generated podcasts represent a relatively new and relatively more passive modality of information consumption, delivering narratives in a naturally engaging format. As these two media increasingly converge in everyday information-seeking behavior, it is essential to explore how their interaction influences user attitudes, particularly in contexts involving controversial, value-laden, and often debated topics. Addressing this need, we aim to understand how information mediums of present-day SERPs and AI-generated podcasts interact to shape the opinions of users. To this end, through a controlled user study (N=483), we investigated user attitudinal effects of consuming information via SERPs and AI-generated podcasts, focusing on how the sequence and modality of exposure shape user opinions. A majority of users in our study corresponded to attitude change outcomes, and we found an effect of sequence on attitude change. Our results further revealed a role of viewpoint bias and the degree of topic controversiality in shaping attitude change, although we found no effect of individual moderators.

</details>


### [111] [Validating Search Query Simulations: A Taxonomy of Measures](https://arxiv.org/abs/2601.11412)
*Andreas Konstantin Kruff,Nolwenn Bernard,Philipp Schaer*

Main category: cs.IR

TL;DR: Develops a taxonomy of validation measures for simulated user queries in information retrieval, validates it empirically across four datasets, and offers recommendations plus an open-source library.


<details>
  <summary>Details</summary>
Motivation: Validation of user simulators is crucial for reliable evaluation of IR systems, but existing measures are fragmented and their relationships unclear.

Method: Systematic literature review; construction of a taxonomy of validation measures; empirical analysis of measure relationships across four diverse IR datasets; synthesis of practical recommendations; release of an open-source library implementing common measures.

Result: A structured taxonomy of validation measures; empirical corroboration of relationships among measures; concrete recommendations on measure selection (or combinations) for validating user simulations; an open-source library to facilitate future research.

Conclusion: The proposed taxonomy and recommendations provide a principled basis for validating user simulators in IR, improving the reliability of simulation-based evaluations; the released library will support broader adoption and consistency across studies.

Abstract: Assessing the validity of user simulators when used for the evaluation of information retrieval systems remains an open question, constraining their effective use and the reliability of simulation-based results. To address this issue, we conduct a comprehensive literature review with a particular focus on methods for the validation of simulated user queries with regard to real queries. Based on the review, we develop a taxonomy that structures the current landscape of available measures. We empirically corroborate the taxonomy by analyzing the relationships between the different measures applied to four different datasets representing diverse search scenarios. Finally, we provide concrete recommendations on which measures or combinations of measures should be considered when validating user simulation in different contexts. Furthermore, we release a dedicated library with the most commonly used measures to facilitate future research.

</details>


### [112] [Isotropy-Optimized Contrastive Learning for Semantic Course Recommendation](https://arxiv.org/abs/2601.11427)
*Ali Khreis,Anthony Nasr,Yusuf Hilal*

Main category: cs.IR

TL;DR: 提出基于BERT的自监督对比学习课程推荐系统，通过数据增强与等方性正则化提升嵌入区分度，在500+工程课程数据集上显著优于 vanilla BERT。


<details>
  <summary>Details</summary>
Motivation: 解决BERT嵌入的各向异性/等方性问题导致课程描述在语义相关性之外也高度相似，从而降低推荐质量。

Method: 构建基于BERT的对比学习框架，使用自监督对比学习、数据增强（如同义替换、随机删除、回译等）和等方性正则化来获得更具判别性的嵌入；对学生文本查询进行编码并检索Top-N相关课程，数据集覆盖500+跨学院的工程课程，进行微调评估。

Result: 实验结果显示嵌入分离度提升，Top-N推荐准确性优于 vanilla BERT基线。

Conclusion: 所提出的对比学习+等方性正则化框架在语义推荐中有效提升嵌入质量和推荐性能，具有在课程推荐等文本检索场景中的应用潜力。

Abstract: This paper presents a semantic course recommendation system for students using a self-supervised contrastive learning approach built upon BERT (Bidirectional Encoder Representations from Transformers). Traditional BERT embeddings suffer from anisotropic representation spaces, where course descriptions exhibit high cosine similarities regardless of semantic relevance. To address this limitation, we propose a contrastive learning framework with data augmentation and isotropy regularization that produces more discriminative embeddings. Our system processes student text queries and recommends Top-N relevant courses from a curated dataset of over 500 engineering courses across multiple faculties. Experimental results demonstrate that our fine-tuned model achieves improved embedding separation and more accurate course recommendations compared to vanilla BERT baselines.

</details>
