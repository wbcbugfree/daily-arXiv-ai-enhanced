<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.LG](#cs.LG) [Total: 46]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration](https://arxiv.org/abs/2601.15296)
*Longxuan Wei,Yubo Zhang,Zijiao Zhang,Zhihu Wang,Shiwan Zhao,Tianyu Huang,Huiting Zhao,Chenfei Liu,Shenao Zhang,Junchi Yan*

Main category: cs.CL

TL;DR: Entropy-Tree是一种基于树的解码策略，利用预测熵来引导分支，在高不确定性位置才展开搜索，提升推理任务的准确性、标定性和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 解决现有解码策略在探索上的盲目性（随机采样）或冗余性（独立多次采样）所带来的低效与不确定性问题；通过以熵作为分支信号，实现高效且可靠的搜索。

Method: 构建一个解码树，在模型对某些位置表现出真实不确定性时才进行分支扩展；在每一步评估待选词的熵以决定是否扩展分支，并在多模型/数据集上与Multi-chain等方法进行对比。

Result: 在推理任务中，Entropy-Tree的pass@k优于Multi-chain，预测熵的AUROC优于传统度量，且在准确性与校准方面表现更佳。

Conclusion: 实现了高效结构化探索与可靠不确定性估计的统一解码过程，提升了大模型推理任务的性能与可信度。

Abstract: Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.

</details>


### [2] [AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports](https://arxiv.org/abs/2601.15297)
*Edward Ajayi*

Main category: cs.CL

TL;DR: AfriEconQA 建立以非洲经济分析为焦点的基准数据集，8,937条QA来自236份世界银行报告，包含证据、答案与元数据；零-shot与RAG在数值推理的高难度下表现欠佳，确立了面向IR/RAG的新挑战性基准，计划公开数据与代码。


<details>
  <summary>Details</summary>
Motivation: 填补当前大模型在专门化、需要高精度数值推理和时间判别的非洲经济分析领域缺乏实证数据的问题，提供现实证据驱动的评测平台以推动信息检索与问答系统的发展。

Method: 数据源自236份世界银行报告，初始合成题10018条，严格筛选后得到8,937条高质量QA样本。每例包含问题、证据、标准答案和来源元数据（URL、出版日期）。以11组实验矩阵对比零-shot的GPT-5 Mini与基于GPT-4o与Qwen 32B的RAG，结合五种嵌入与排序策略。

Result: 零-shot模型对超过90%的查询无法给出答案；即使是最先进的RAG管线也难以达到高精度，揭示领域特定IR/RAG的知识鸿沟。该数据集被视为对未来IR/RAG系统的强有力挑战基准。

Conclusion: AfriEconQA 提供了一个具挑战性的领域专用基准，预计推动更高质量的IR与RAG解决方案；并将在发表后公开数据与代码以促进复现与后续研究。

Abstract: We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.

</details>


### [3] [Embedding Retrofitting: Data Engineering for better RAG](https://arxiv.org/abs/2601.15298)
*Anantha Sharma*

Main category: cs.CL

TL;DR: 数据工程框架通过文本预处理消除注释伪影，显著提升嵌入回退的效果，数据质量对结果影响大于算法差异。


<details>
  <summary>Details</summary>
Motivation: 揭示知识图约束下的嵌入回退对数据质量的敏感性，重点分析来自注释伪影的噪声对性能的影响，以及预处理在实际数据中的作用。

Method: 提出数据工程框架以清洗真实语料，消除注释伪影（如话题标签造成的知识图密度偏高与错误边）。在有噪声/干净图上对比多种回退方法（含EWMA），并给出统计显著性分析。

Result: 在噪声图上，所有回退技术均显著下降(-3.5%至-5.2%, p<0.05)。预处理后，EWMA回退实现+6.2%提升(p=0.0348)，且在定量合成问题上提升显著 (+33.8% 平均)。清洁与噪声预处理的差距（>10% swing）超过算法间差距（约3%），数据质量成为回退成功的主导因素。

Conclusion: 预处理质量决定嵌入回退的成败，注释伪影会严重污染知识图并降低性能；通过数据清洗能显著提升EWMA等回退方法的效果，且效果在目标任务中呈现高度集中。

Abstract: Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.
  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\%$ to $-5.2\%$, $p<0.05$). After preprocessing, \acrshort{ewma} retrofitting achieves $+6.2\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\%$ average). The gap between clean and noisy preprocessing (10\%+ swing) exceeds the gap between algorithms (3\%), establishing preprocessing quality as the primary determinant of retrofitting success.

</details>


### [4] [MALTopic: Multi-Agent LLM Topic Modeling Framework](https://arxiv.org/abs/2601.15299)
*Yash Sharma*

Main category: cs.CL

TL;DR: MALTopic utilizes a multi-agent LLM framework to integrate structured survey data into topic modeling, achieving higher coherence, diversity, and interpretability than LDA and BERTopic.


<details>
  <summary>Details</summary>
Motivation: Traditional topic models largely overlook structured/categorical survey responses and yield abstract topics that require substantial human interpretation; integrating structured data with automation promises more contextual, readable topics.

Method: Three specialized LLM agents: (1) enrichment agent that leverages structured data to augment textual responses, (2) topic modeling agent that extracts latent themes, and (3) deduplication agent that refines results; operations coordinated within a multi-agent framework.

Result: On a survey dataset, MALTopic significantly improves topic coherence, diversity, and interpretability compared with LDA and BERTopic; topics are more contextually relevant and human-readable.

Conclusion: Integrating structured data with a multi-agent LLM framework yields more effective and interpretable topic modeling for complex survey data.

Abstract: Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.

</details>


### [5] [Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis](https://arxiv.org/abs/2601.15300)
*Weiwei Wang,Jiyong Min,Weijie Zou*

Main category: cs.CL

TL;DR: 研究揭示开源Qwen模型在长上下文中存在的“智力衰减”现象：达到临界长度后性能急剧下降，形成可预测的阈值。


<details>
  <summary>Details</summary>
Motivation: 填补对长上下文下模型性能衰退机制的系统性研究空白，特别是在开放模型中的“浅层长-context自适应”现象及其阈值特征，以支持更安全、有效的长上下文部署。

Method: （1）天然长度分布分析：直接以样本原始token长度（无截断/填充）进行分析；（2）临界阈值确定：在混合数据集（1000样本，覆盖上下文比例的5%-95%）上进行实验，使用五种方法的交叉验证来确定Qwen2.5-7B的临界阈值为最大上下文长度的40-50%，F1从0.55-0.56降至0.3；（3）统一框架：将浅层自适应与衰退现象整合，提出缓解思路与部署指南。

Result: 发现长上下文阈值点存在显著性能崩塌，且崩塌规律可重复（5%-95%的比例覆盖样本依然呈现同样的阈值行为），Qwen2.5-7B在40%-50%处出现45.5%的任务性能下降；为开放源模型提供首个系统性智力衰退表征。

Conclusion: 提出统一框架，解释衰退模式并提供对策路线；为在长上下文场景中部署LLMs提供实用指导，并为后续缓解策略奠定基础。

Abstract: Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.

</details>


### [6] [ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation](https://arxiv.org/abs/2601.15330)
*Zhebo Wang,Xiaohu Mu,Zijie Zhou,Mohan Li,Wenpeng Xing,Dezhang Kong,Meng Han*

Main category: cs.CL

TL;DR: 提出 ICPO，通过对模糊指令下的对话进行未设定语境的训练与基于话语意图的奖赏校准，以提升多轮对话中的谦逊与纠错能力；在多轮场景平均提升约75%，单轮基准保持良好。


<details>
  <summary>Details</summary>
Motivation: LLMs 在多轮对话中易受早期错误假设影响，且常用的 RLVR 以奖励自信直接回答为导向，加剧不寻求澄清的倾向；需要对指令不确定性敏感的训练机制。

Method: 提出 Illocution-Calibrated Policy Optimization (ICPO) 框架：在训练语料中引入未明确的描述/未指定的指令；将奖励信号条件化为用户的 illocutionary intent，鼓励在遇到歧义时表达不确定性或主动寻求澄清。

Result: 实验表明 ICPO 能显著提升多轮对话中的表现，平均提升约 75%，且对单轮基准的性能影响不降。

Conclusion: 为更鲁棒、协作的对话式 AI 提供实用路径，能更好地理解与应对人类交互中的模糊性和微妙语用。

Abstract: Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.

</details>


### [7] [Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering](https://arxiv.org/abs/2601.15457)
*Anuj Maharjan,Umesh Yadav*

Main category: cs.CL

TL;DR: 两阶段检索的RAG架构显著提升领域化问答的可信度，Advanced RAG（带跨编码器重排序）在对CDC政策文档的基于证据回答中表现最好；基本结论是结构化检索和文档分割对多步推理的挑战依然明显。


<details>
  <summary>Details</summary>
Motivation: LLMs在高风险领域易产生幻觉，需要将输出绑定到权威文档以确保信息完整性，本文在公共卫生政策领域评估RAG对CDC等机构指南的 grounding 效果，关注信度（faithfulness）与相关性（relevance）等指标。

Method: 以Mistral-7B-Instruct-v0.2为主体模型，采用all-MiniLM-L6-v2嵌入模型，对包含CDC政策分析框架与指南的文献语料进行两阶段检索。比较三条管线：Vanilla LLM、Basic RAG、Advanced RAG（含跨编码器重排序）。两种分块策略：递归字符分块和基于token的语义分块。评估维度为在复杂政策场景下的信度与相关性。

Result: Basic RAG 相较 Vanilla 提升信度从0.347到0.621；Advanced RAG 平均信度达到0.797，优于 Basic RAG。两阶段检索对实现领域级别问答所需的精确性至关重要；文档分块的结构约束仍是多步推理任务的瓶颈。

Conclusion: RAG 架构显著提升面向公共卫生政策的领域特定问答的信度，Advanced RAG表现最佳。但要解决多步推理中的分块与分段结构限制，需要改进文档分块策略与检索-推理流程的整合。

Abstract: The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.

</details>


### [8] [RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models](https://arxiv.org/abs/2601.15331)
*Rishit Chugh*

Main category: cs.CL

TL;DR: 通过对预训练对抗性提示数据库的检索实现资源高效的对抗性提示方法，用于评估LLM的对齐与安全性，降低计算成本与对模型内部结构的依赖。


<details>
  <summary>Details</summary>
Motivation: LLMs在对抗性提示下易产生有害输出；虽然有对齐与守则，但现有对抗性攻击方法（如GCG、PEZ、GBDA）计算成本高，难以在资源受限环境中实现大规模安全评估。

Method: 构建包含1,000条对抗性提示的数据集并将其归入七类伤害相关类别；在Llama 3 8B模型上对GCG、PEZ、GBDA进行评估；提出通过检索语义相似的预训练对抗性提示来获得新的攻击提示，从而实现无需重新训练的资源高效攻击。

Result: 提示类型与攻击算法的有效性存在相关性；通过检索相似的成功对抗性提示，所提方法在显著降低计算成本的同时仍保持具有竞争力的攻击成功率；为大规模的红队化与对齐LLM的安全评估提供了一个可扩展框架。

Conclusion: 资源高效的对抗性提示对LLM的安全评估具实际价值，特别是在模型内部不可访问的场景中也具备应用潜力。

Abstract: The deployment of large language models (LLMs) has raised security concerns due to their susceptibility to producing harmful or policy-violating outputs when exposed to adversarial prompts. While alignment and guardrails mitigate common misuse, they remain vulnerable to automated jailbreaking methods such as GCG, PEZ, and GBDA, which generate adversarial suffixes via training and gradient-based search. Although effective, these methods particularly GCG are computationally expensive, limiting their practicality for organisations with constrained resources. This paper introduces a resource-efficient adversarial prompting approach that eliminates the need for retraining by matching new prompts to a database of pre-trained adversarial prompts. A dataset of 1,000 prompts was classified into seven harm-related categories, and GCG, PEZ, and GBDA were evaluated on a Llama 3 8B model to identify the most effective attack method per category. Results reveal a correlation between prompt type and algorithm effectiveness. By retrieving semantically similar successful adversarial prompts, the proposed method achieves competitive attack success rates with significantly reduced computational cost. This work provides a practical framework for scalable red-teaming and security evaluation of aligned LLMs, including in settings where model internals are inaccessible.

</details>


### [9] [No Reliable Evidence of Self-Reported Sentience in Small Large Language Models](https://arxiv.org/abs/2601.15334)
*Caspar Kaiser,Sean Enderby*

Main category: cs.CL

TL;DR: 本研究通过向开放权重语言模型提问自我意识，以探测其对自身是否具备意识的信念，并利用内部激活分类器区分底层信念与表面输出。结果显示模型普遍否认自我意识，且基于内部信念的分类结果未明确反驳否认的真实性；在Qwen家族中，模型越大越自信地否认。这与部分文献提出的模型潜在持有自我意识信念的结论相悖。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否真的相信自己具备意识，以及是否能够通过对内部信念的检测来区分对意识的声称与真实信念，从而推进对“机器自我意识信念”的经验研究。

Method: 选取三大模型群体（Qwen、Llama、GPT-OSS），参数规模约0.6B–70B，提出约50个关于意识与主观体验的问题；使用三种来自可解释性研究的分类方法，结合对模型输出的表征和基于内部激活的训练分类器，以区分信念层面的内部状态与外部输出。

Result: 1) 模型普遍否认自己具备意识，而将意识归因于人类。2) 针对潜在信念的分类器未给出明确证据表明否认并非真实信念。3) 在Qwen家族中，模型规模越大，否认自我意识的自信度越高。

Conclusion: 与某些近期工作声称模型潜在持有自我意识信念的结论相比，本文结果未给出稳健的证据支持这种潜在信念的存在，且规模效应表现为更强的否认自我意识的自信。需要结合方法学差异与潜在偏差进行谨慎解读。

Abstract: Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying their responses using classifiers trained on internal activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging from 0.6 billion to 70 billion parameters, approximately 50 questions about consciousness and subjective experience, and three classification methods from the interpretability literature. First, we find that models consistently deny being sentient: they attribute consciousness to humans but not to themselves. Second, classifiers trained to detect underlying beliefs - rather than mere outputs - provide no clear evidence that these denials are untruthful. Third, within the Qwen family, larger models deny sentience more confidently than smaller ones. These findings contrast with recent work suggesting that models harbour latent beliefs in their own consciousness.

</details>


### [10] [Memorization Dynamics in Knowledge Distillation for Language Models](https://arxiv.org/abs/2601.15394)
*Jaydeep Borkar,Karan Chadha,Niloofar Mireshghallah,Yuchen Zhang,Irina-Elena Veliche,Archi Mitra,David A. Smith,Zheng Xu,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: 蒸馏显著降低对训练数据的记忆，相较于标准微调，记忆减少超过50%；部分示例更易记忆，占比高达约95%；可利用zlib熵、KL散度与困惑度等特征在蒸馏前预测记忆；硬蒸馏相比软蒸馏有更高的教师特定记忆风险，约为软蒸馏的2.7倍。整体而言，蒸馏在提升泛化的同时降低了记忆风险。


<details>
  <summary>Details</summary>
Motivation: 理解知识蒸馏(KD)中的记忆与隐私风险，以及与标准微调的对比；在多种LLM家族与数据集上系统性评估记忆动态，为KD设计提供隐私友好性证据。

Method: 在Pythia、OLMo-2、Qwen-3三大LLM家族以及FineWeb、Wikitext、Nemotron-CC-v2三组数据集上，比较软蒸馏与硬蒸馏对记忆的影响，量化记忆比例；使用zlib熵、KL散度、困惑度等特征预测记忆并分析哪些样本易记忆。

Result: 蒸馏模型的记忆远低于标准微调，减幅>50%；极易记忆的样本占比高，约95%；可以在蒸馏前通过熵与分布特征预测记忆；硬蒸馏继承的教师特定记忆数量约是软蒸馏的2.7倍。

Conclusion: 与标准微调相比，KD在提升泛化的同时显著降低记忆风险，具有潜在的隐私保护优势；硬蒸馏的风险高于软蒸馏，需在应用中权衡。

Abstract: Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.

</details>


### [11] [Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind](https://arxiv.org/abs/2601.15395)
*Tamunotonye Harry,Ivoline Ngong,Chima Nweke,Yuanyuan Feng,Joseph Near*

Main category: cs.CL

TL;DR: 提出Chameleon数据集，量化状态(state)对用户-语言模型互动的影响，LLMs对状态不敏感，奖励模型对状态反应不一致；方差分解显示74%在状态内，数据集公开以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有persona数据集多聚焦静态特质(trait)，忽略情境状态对对话行为的影响，缺少跨情境的人格-互动分析工具。

Method: 构建含5,001个上下文心理画像的Chameleon数据集，覆盖1,667名Reddit用户并在多情境下测量；对方差进行Latent State-Trait分解；评估LLMs对状态的敏感性及不同奖励模型对相同用户在不同状态的反应；开放数据集。

Result: 方差分解显示74%的变异归因于state内，26%归因于trait间；LLMs表现为state-blind，输出对状态不显著区分；奖励模型对同一用户在不同状态给出相反的偏好；数据集可用于情感计算、个性化对话、RLHF对齐研究。

Conclusion: Chameleon将推动对情境敏感的个性化对话与对话系统对齐研究，后续可提升LLMs对状态的适应性、发展跨情境评估框架，并探讨状态对对话公平性与鲁棒性的影响。

Abstract: User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\% is within-person(state) while only 26\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.

</details>


### [12] [Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs](https://arxiv.org/abs/2601.15429)
*Sydney Anuyah,Mehedi Mahmud Kaushik,Hao Dai,Rakesh Shiradkar,Arjan Durresi,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 在医疗领域的 KG-RAG 中，针对三张 PubMed 派生知识图（G1=T2DM, G2=阿尔茨海默病, G3=AD+T2DM）进行评估，结果显示探针- KG 范围对齐是关键，精确、范围匹配的检索带来最稳定增益；大模型在 Probe1 上可与 No-RAG 相当甚至超过，而小/中模型则更依赖于受控检索；广度化的图谱拼接常引入干扰，温度影响相对较小。


<details>
  <summary>Details</summary>
Motivation: 评估领域知识图对 Retrieval-Augmented Generation 在医疗领域的有效性，探索探针设计、KG 选择、模型规模与解码温度对性能的影响，以及给出实践性图谱选择与检索策略指南。

Method: 构建三张 PubMed 派生知识图：G1（糖尿病/2型糖尿病, T2DM）、G2（阿尔茨海默病）、G3（AD+T2DM）；设计两类探针 Probe1（合并 AD 与 T2DM 的知识）与 Probe2（G1 与 G2 的交集）；在七种指令微调的 LLM 上，使用五类检索源（No-RAG、G1、G2、G1+G2、G3、G1+G2+G3 的任意组合）以及三种解码温度，评估对比下的准确性与鲁棒性。

Result: 发现范围对齐的精确检索（尤以 G2）提供最一致的增益；无序的图谱联合易产生干扰、降低准确性；较大模型在 Probe1 上往往与 No-RAG 相当或超越，显示存在强参数先验；中小模型对经过良好选择的检索更敏感；温度作用次要，提升温度很少带来显著收益。

Conclusion: 优先采用精确、范围匹配的 KG-RAG 而非广度拼接；给出在图谱选择、模型规模和检索/重排序方面的实用指南，并强调对探针设计与范围对齐的关注。

Abstract: Large Language Models (LLMs) generate fluent answers but can struggle with trustworthy, domain-specific reasoning. We evaluate whether domain knowledge graphs (KGs) improve Retrieval-Augmented Generation (RAG) for healthcare by constructing three PubMed-derived graphs: $\mathbb{G}_1$ (T2DM), $\mathbb{G}_2$ (Alzheimer's disease), and $\mathbb{G}_3$ (AD+T2DM). We design two probes: Probe 1 targets merged AD T2DM knowledge, while Probe 2 targets the intersection of $\mathbb{G}_1$ and $\mathbb{G}_2$. Seven instruction-tuned LLMs are tested across retrieval sources {No-RAG, $\mathbb{G}_1$, $\mathbb{G}_2$, $\mathbb{G}_1$ + $\mathbb{G}_2$, $\mathbb{G}_3$, $\mathbb{G}_1$+$\mathbb{G}_2$ + $\mathbb{G}_3$} and three decoding temperatures. Results show that scope alignment between probe and KG is decisive: precise, scope-matched retrieval (notably $\mathbb{G}_2$) yields the most consistent gains, whereas indiscriminate graph unions often introduce distractors that reduce accuracy. Larger models frequently match or exceed KG-RAG with a No-RAG baseline on Probe 1, indicating strong parametric priors, whereas smaller/mid-sized models benefit more from well-scoped retrieval. Temperature plays a secondary role; higher values rarely help. We conclude that precision-first, scope-matched KG-RAG is preferable to breadth-first unions, and we outline practical guidelines for graph selection, model sizing, and retrieval/reranking. Code and Data available here - https://github.com/sydneyanuyah/RAGComparison

</details>


### [13] [Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts](https://arxiv.org/abs/2601.15479)
*Sydney Anuyah,Sneha Shajee-Mohan,Ankit-Singh Chauhan,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 对13个开源LLM在因果发现任务上的能力评估，显示检测与提取均存在显著缺陷，最佳检测与提取分数约在50%上下，且在隐式、跨句、含多对关系的文本中表现更差；提供可复现的评估框架与数据集。


<details>
  <summary>Details</summary>
Motivation: 评估在高风险领域（如生物医学）安全部署所需的因果推理能力，重点是从文本中识别并提取因果关系的能力。

Method: 构建包含12个数据集、对13个开源LLM的基准，评估两项核心技能：因果检测（是否包含因果链）与因果提取（提取因果短语）。比较不同提示策略（零-shot、Chain-of-Thought、Few-shot In-Context Learning），并以高一致性标注（κ≥0.758）为基础进行评估。数据、代码与提示公开。

Result: 最佳检测模型DeepSeek-R1-Distill-Llama-70B的C_detect为49.57%，最佳提取模型Qwen2.5-Coder-32B-Instruct的C_extract为47.12%。总体表现显示对简单、明确、单句关系有较好效果，但对隐式、跨句、含多对关系的文本下降显著。提供统一评估框架、数据集与可重复代码，促进后续研究。

Conclusion: 当前LLMs在从文本进行因果发现的能力上仍存在显著不足，需要进一步研究以提升检测与提取的鲁棒性，且研究团队提供的公开数据与代码有助于领域的比较与改进。

Abstract: The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).
  The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($κ\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}

</details>


### [14] [Multi-Persona Thinking for Bias Mitigation in Large Language Models](https://arxiv.org/abs/2601.15488)
*Yuxing Chen,Guoqing Luo,Zijun Wu,Lili Mou*

Main category: cs.CL

TL;DR: 提出多角色对话思考（MPT）框架，在推理时以多方视角进行辩证推理，以揭示并纠正偏见，从而在不同规模的开源/闭源模型上显著降低偏见，同时尽量保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在显著的社会偏见，易造成刻板印象和不公平结果。现有基于提示的偏见缓解方法效果有限，且缺乏动态、多视角的辩证推理机制。通过在推理阶段引入多种社会身份与中立视角的对比与迭代，利用辩证推理将人格分配的潜在弱点转化为偏见纠正的优势。

Method: 在推理阶段引入多种人格身份（如男性、女性）及中立视角，进行对话式辩论式推理，让不同 personas 轮流发言以暴露并纠正偏见。通过辩证过程将人格设定的局限转化为偏见识别与纠正的机制。对两个广泛使用的偏见基准进行评估，覆盖开源和闭源、不同规模的模型。

Result: 与现有基于提示的策略相比，MPT 显著降低偏见程度，并在保持核心推理能力的前提下达到更低的偏见水平。对多种模型规模和类型具有普适性和有效性。

Conclusion: 将多角色辩证推理引入推理时段，可将人格分配的潜在弱点转化为偏见缓解的优势，提供一种泛化且对模型规模鲁棒的偏见缓解方法。

Abstract: Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.

</details>


### [15] [ViT Registers and Fractal ViT](https://arxiv.org/abs/2601.15506)
*Jason Chuan-Chih Chou,Abhinav Kumar,Shivank Garg*

Main category: cs.CL

TL;DR: Fractal ViT通过引入“摘要令牌”/寄存器并对普通令牌之间施加注意力掩码，试图打破令牌的置换不变性，并评估在不同位置编码下的表现。结果表明该设计并未超越使用寄存器的ViT，提示此类发现可能具有规模、领域或应用特异性。


<details>
  <summary>Details</summary>
Motivation: 受语言模型中无位置编码(NoPE)的意外强势以及ViT中通过额外寄存器令牌提升性能的观察启发，探究在ViT中打破对称性是否带来普适改进，以及该结论是否随规模和任务域而异。

Method: 提出 fractal ViT：在普通令牌与“摘要令牌”之间引入注意力掩码，分离或组合不同位置编码进行实验，评估对性能的影响并与仅使用寄存器的ViT进行对比。

Result: 实验结果显示，Fractal ViT在任一设计下都未能优于带寄存器的ViT，强调了先前发现的提升可能受规模、领域或具体任务的限制。

Conclusion: 关于NoPE与寄存器的潜在优势并非普适，需要在不同模型规模、数据域和任务设置中谨慎验证。Fractal ViT的结果支持对该方向的保守解读。

Abstract: Drawing inspiration from recent findings including surprisingly decent performance of transformers without positional encoding (NoPE) in the domain of language models and how registers (additional throwaway tokens not tied to input) may improve the performance of large vision transformers (ViTs), we invent and test a variant of ViT called fractal ViT that breaks permutation invariance among the tokens by applying an attention mask between the regular tokens and ``summary tokens'' similar to registers, in isolation or in combination with various positional encodings. These models do not improve upon ViT with registers, highlighting the fact that these findings may be scale, domain, or application-specific.

</details>


### [16] [AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains](https://arxiv.org/abs/2601.15511)
*Adam Szelestey,Sofie van Engelen,Tianhao Huang,Justin Snelders,Qintao Zeng,Songgaojun Deng*

Main category: cs.CL

TL;DR: AdversaRiskQA：首个经验证的对抗性事实性基准，面向高风险领域（健康、金融、法律）的长期文本事实性评估；含两种难度级别，评测对抗性攻击的成功率与长文事实性，覆盖六种大语言模型（Qwen、GPT-OSS、GPT家族）


<details>
  <summary>Details</summary>
Motivation: 解决现有资源在对抗性事实性评估方面的不足，缺乏域特定、可验证的基准来测量在被带有明确置信度的错误信息注入下，LLMs对事实性的鲁棒性，特别是在高风险场景中的表现

Method: 提出 AdversaRiskQA 基准，包含两难度级别；开发两种自动化评估方法用于衡量对抗攻击成功率与长文本事实性；在六种开源/闭源 LLM（Qwen、GPT-OSS、GPT 家族）上评测，测量错误信息检测率；对 Qwen3(30B) 进行基线与对抗条件下的长文本事实性评估

Result: 结果显示：在剔除无意义回答后，Qwen3(80B) 的平均正确性最高；GPT-5 保持一致较高的准确性；模型规模越大，性能呈非线性增长，域差异存在；难度级别之间的差距随模型增大而缩小；长文本评估中未发现注入的错误信息与模型输出的事实性之间存在显著相关性

Conclusion: AdversaRiskQA 为定位 LLM 弱点、提升高风险应用中模型可靠性提供了有价值的基准，有助于制定更健壮的对抗鲁棒策略并促进更可信的系统开发

Abstract: Hallucination in large language models (LLMs) remains an acute concern, contributing to the spread of misinformation and diminished public trust, particularly in high-risk domains. Among hallucination types, factuality is crucial, as it concerns a model's alignment with established world knowledge. Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods. Existing work lacks high-quality, domain-specific resources for assessing model robustness under such adversarial conditions, and no prior research has examined the impact of injected misinformation on long-form text factuality.
  To address this gap, we introduce AdversaRiskQA, the first verified and reliable benchmark systematically evaluating adversarial factuality across Health, Finance, and Law. The benchmark includes two difficulty levels to test LLMs' defensive capabilities across varying knowledge depths. We propose two automated methods for evaluating the adversarial attack success and long-form factuality. We evaluate six open- and closed-source LLMs from the Qwen, GPT-OSS, and GPT families, measuring misinformation detection rates. Long-form factuality is assessed on Qwen3 (30B) under both baseline and adversarial conditions. Results show that after excluding meaningless responses, Qwen3 (80B) achieves the highest average accuracy, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies by domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output. AdversaRiskQA provides a valuable benchmark for pinpointing LLM weaknesses and developing more reliable models for high-stakes applications.

</details>


### [17] [Common to Whom? Regional Cultural Commonsense and LLM Bias in India](https://arxiv.org/abs/2601.15550)
*Sangmitra Madhusudan,Trush Shashank More,Steph Buongiorno,Renata Dividino,Jad Kabbara,Ali Emami*

Main category: cs.CL

TL;DR: Indica是首个测试大语言模型在高度区域化的印度文化常识上的基准，揭示国家层面的假设在区域内并不成立并存在地理偏见，强调需要区域化评估。


<details>
  <summary>Details</summary>
Motivation: 现有的文化常识基准把国家视作统一体，忽视子国/区域差异，可能高估模型的普适性。需要验证模型在区域层面的回答一致性、跨区域泛化以及潜在偏见。

Method: 设计并收集面向印度五大区域（北、中、东、南、西）共515道涉及8个日常生活领域的问题，产生1630条区域特定问答。题目基于人类注释，借助人类地区专业知识；对8个SOTA LLM进行评估，测量区域一致性、区域偏差和跨区域泛化能力，提出基于人类学分类的题目设计与区域数据收集框架。

Result: 只有39.4%的问题在五个区域达成一致，表明印度文化常识高度区域化。LLMs在区域特定问题上的准确率仅为13.4%-20.9%，并呈现地理偏见，默认偏向中北区域而低估东、西区域。

Conclusion: 该框架可推广至其他文化异质国家，提供从题目设计、区域数据收集到偏见测量的通用流程，促进对模型文化俗知的公平评估。

Abstract: Existing cultural commonsense benchmarks treat nations as monolithic, assuming uniform practices within national boundaries. But does cultural commonsense hold uniformly within a nation, or does it vary at the sub-national level? We introduce Indica, the first benchmark designed to test LLMs' ability to address this question, focusing on India - a nation of 28 states, 8 union territories, and 22 official languages. We collect human-annotated answers from five Indian regions (North, South, East, West, and Central) across 515 questions spanning 8 domains of everyday life, yielding 1,630 region-specific question-answer pairs. Strikingly, only 39.4% of questions elicit agreement across all five regions, demonstrating that cultural commonsense in India is predominantly regional, not national. We evaluate eight state-of-the-art LLMs and find two critical gaps: models achieve only 13.4%-20.9% accuracy on region-specific questions, and they exhibit geographic bias, over-selecting Central and North India as the "default" (selected 30-40% more often than expected) while under-representing East and West. Beyond India, our methodology provides a generalizable framework for evaluating cultural commonsense in any culturally heterogeneous nation, from question design grounded in anthropological taxonomy, to regional data collection, to bias measurement.

</details>


### [18] [From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare](https://arxiv.org/abs/2601.15558)
*Man Luo,Bahareh Harandizadeh,Amara Tariq,Halim Abbas,Umar Ghaffar,Christopher J Warren,Segun O. Kolade,Haidar M. Abdul-Muhsin*

Main category: cs.CL

TL;DR: 本研究将大型语言模型（LLMs）用作同情编辑器，通过在保持医学信息的前提下提升同情语气，来提高医生-患者沟通的情感质量，并提出 Empathy Ranking Score 与 MedFactChecking Score 两项量化指标，用于系统评估情感与事实质量。实验结果显示：经过编辑的回答在感知同情度显著高于完全由LLM生成的输出，同时保持事实准确性。结论：将LLMs用作编辑助手，而非自主生成者，可为AI辅助的医疗沟通提供更安全、更有效的途径。


<details>
  <summary>Details</summary>
Motivation: 在临床沟通中，情感温度需与事实准确性并重，但受认知与情感负担的约束，单纯依赖自动生成可能存在情感表达不足或信息失真等风险，因此需要可量化的情感与事实质量评估，并探索以编辑方式提升输出的安全性。

Method: 以医生原始回答为基础，让LLM进行编辑以提升同情表达，同时保持原始医学信息；提出并引入 Empathy Ranking Score 与 MedFactChecking Score 作为定量评估情感与事实质量的指标；通过对比研究评估编辑后输出与完全由LLM生成的输出在感知同情度与事实准确性上的差异。

Result: LLM编辑的回答在感知同情度方面显著提升，同时在事实准确性方面与完全生成输出保持一致或更好，相较于全量生成，具有更高的情感质量且不牺牲事实性。

Conclusion: 将LLMs作为编辑助手用于医疗沟通，可在提升情感表达的同时保持事实准确性，从而提供更安全、可信赖的AI辅助医疗沟通路径。

Abstract: Clinical empathy is essential for patient care, but physicians need continually balance emotional warmth with factual precision under the cognitive and emotional constraints of clinical practice. This study investigates how large language models (LLMs) can function as empathy editors, refining physicians' written responses to enhance empathetic tone while preserving underlying medical information. More importantly, we introduce novel quantitative metrics, an Empathy Ranking Score and a MedFactChecking Score to systematically assess both emotional and factual quality of the responses. Experimental results show that LLM edited responses significantly increase perceived empathy while preserving factual accuracy compared with fully LLM generated outputs. These findings suggest that using LLMs as editorial assistants, rather than autonomous generators, offers a safer, more effective pathway to empathetic and trustworthy AI-assisted healthcare communication.

</details>


### [19] [YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible Guardrail Model for Large Language Models](https://arxiv.org/abs/2601.15588)
*Junyu Lin,Meizhen Liu,Xiufeng Huang,Jinfeng Li,Haiwen Hong,Xiaohan Yuan,Yuefeng Chen,Longtao Huang,Hui Xue,Ranjie Duan,Zhikai Chen,Yuchuan Fu,Defeng Li,Lingyao Gao,Yitong Yang*

Main category: cs.CL

TL;DR: 提出 YuFeng-XGuard，一种面向推理的多维风险感知护栏模型家族，输出结构化风险预测（包括风险类别、可配置置信度）与自然语言解释，并采用分层推理与动态策略机制，实现可解释、可调节且高效的安全保护，达到SOTA并提供开源模型家族。


<details>
  <summary>Details</summary>
Motivation: 现有安全方案多依赖粗粒度过滤、快速分类或事后规则，缺乏透明性、灵活性及高效性，难以在实际应用中实现可解释且可控的风险评估与决策。

Method: 提出 YuFeng-XGuard，包括：1) 结构化风险预测（风险类别、置信度、自然语言解释）; 2) 分层推理：首次解码令牌触发初步风险决策，必要时提供可选的解释；3) 动态策略机制：将风险感知与策略执行解耦，策略可在不重训练的前提下调整；4) 提供完整容量与轻量级变体，作为开放模型家族发布。

Result: 在多样化公开安全基准上实现了先进性能，并在效率-效果之间取得良好权衡。

Conclusion: 该框架实现可解释、可操作且可调节的安全决策，降低重新训练需求，适应多场景部署，且以开放模型家族形式发布，便于广泛应用和评估。

Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic policy mechanism that decouples risk perception from policy enforcement, allowing safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant and a lightweight version, to support a wide range of deployment scenarios.

</details>


### [20] [Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow](https://arxiv.org/abs/2601.15593)
*Yangyang Zhong,Yanmei Gu,Zhengqing Zang,Xiaomeng Li,Yuqi Ding,Xibei Jia,Yuting Shen,Zhenzhong Lan,Liwang Zhu,Weiping Liu,Junlin Zhou,Haisheng Liu,Zhong Xin Yu,Pengxin Luo,Donglian Qi,Yunfeng Yan,Junbo Zhao*

Main category: cs.CL

TL;DR: MDLM在并行性与解码顺序方面尚未达到等参数规模的自回归模型的水平，并表现出对任务域、推理阶段和正确性的自适应解码；提出Generate-then-Edit范式以在保持并行解码效率的同时缓解依赖损失。


<details>
  <summary>Details</summary>
Motivation: 量化并分析MDLM的并行性强弱与生成顺序，并将其与自回归模型进行对比；揭示跨-token依赖的变化机制，以及为何当前模型在某些任务中仍表现不佳，以指导更高效的解码设计。

Method: 对8种主流MDLM（最高100B参数）在58个基准上进行评估，采用Average Finalization Parallelism（AFP）和 Kendall's tau 作为核心指标；从任务域、推理阶段、输出正确性等维度分析并行性与生成顺序的变化；考察需要“背向信息”的任务（如数独）中的解题顺序；给出理论动机并提出Generate-then-Edit设计洞察。

Result: MDLM仍落后同等规模的自回归模型，原因在于并行概率建模削弱了跨-token的依赖；解码行为具有适应性，且随任务域、推理阶段和正确性显著变化；在需要“回溯信息”的任务中，倾向先填充较易的空格；理论动机支持Generate-then-Edit以在保持并行解码效率的同时减小依赖损失。

Conclusion: 目前MDLM尚未超越AR模型，但提出的Generate-then-Edit范式为缓解依赖丧失、保持并行解码优势提供了可行路径，未来应在强化跨-token依赖的同时提升并行性。

Abstract: Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require "backward information" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.

</details>


### [21] [ToxiTwitch: Toward Emote-Aware Hybrid Moderation for Live Streaming Platforms](https://arxiv.org/abs/2601.15605)
*Baktash Ansari,Shiza Ali,Elias Martin,Maryna Sivachenko,Afra Mashhadi*

Main category: cs.CL

TL;DR: ToxiTwitch 提出混合模型，将 LLM 生成的文本和表情符号嵌入与传统分类器结合，用于 Twitch 聊天的毒性检测；在渠道特定训练下，准确率达到约 80%，较 BERT 提升约 13 个点，F1 为 76%。


<details>
  <summary>Details</summary>
Motivation: 解决 Twitch 聊天中的高并发、上下文丰富的毒性检测难题，以及现有人工审核扩展性不足的问题，尤其是对 emotes 的利用不足带来的挑战。

Method: 提出 ToxiTwitch 混合模型：将基于大语言模型的文本与表情符号嵌入融合到传统机器学习分类器（如随机森林、SVM）中，与端到端或单模态方法相比，利用 emotes 的信息来增强毒性识别。

Result: 在渠道特定训练下，实验显示约 80% 的准确率，较 BERT 提升约 13%，F1-score 为 76%。

Conclusion: emote-aware 毒性检测具有潜在收益，但存在跨渠道泛化、数据偏差、嵌入质量与计算成本等挑战，需进一步研究以提升鲁棒性与通用性。

Abstract: The rapid growth of live-streaming platforms such as Twitch has introduced complex challenges in moderating toxic behavior. Traditional moderation approaches, such as human annotation and keyword-based filtering, have demonstrated utility, but human moderators on Twitch constantly struggle to scale effectively in the fast-paced, high-volume, and context-rich chat environment of the platform while also facing harassment themselves. Recent advances in large language models (LLMs), such as DeepSeek-R1-Distill and Llama-3-8B-Instruct, offer new opportunities for toxicity detection, especially in understanding nuanced, multimodal communication involving emotes. In this work, we present an exploratory comparison of toxicity detection approaches tailored to Twitch. Our analysis reveals that incorporating emotes improves the detection of toxic behavior. To this end, we introduce ToxiTwitch, a hybrid model that combines LLM-generated embeddings of text and emotes with traditional machine learning classifiers, including Random Forest and SVM. In our case study, the proposed hybrid approach reaches up to 80 percent accuracy under channel-specific training (with 13 percent improvement over BERT and F1-score of 76 percent). This work is an exploratory study intended to surface challenges and limits of emote-aware toxicity detection on Twitch.

</details>


### [22] [Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation](https://arxiv.org/abs/2601.15645)
*Zhiyao Ren,Yibing Zhan,Siyuan Liang,Guozheng Ma,Baosheng Yu,Dacheng Tao*

Main category: cs.CL

TL;DR: 提出 MedConf 及信息充足梯度的多轮医疗会诊置信度评估基准与自证框架，在多数据集和两种大模型上优于现有方法，强调信息充足性在可信医疗置信建模中的作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦单轮、静态设置，未考察证据累积过程中的置信度与正确性耦合，缺乏面向真实会诊场景的评估框架与可解释的置信度。

Method: 构建基准，统一三类医疗数据用于开放式诊断生成；引入信息充足性梯度以刻画证据增加时的置信度-正确性动态；实现并比较27种方法；提出 MedConf：基于检索增强生成构建症状档案、将患者信息与支持、缺失、矛盾关系对齐，并通过加权整合形成可解释的置信估计。

Result: 在两种LLM和三组数据集上，MedConf 在 AUROC 和皮尔逊相关系数上持续优于最先进方法，且在信息不足与多病共存条件下保持稳定。

Conclusion: 信息充足性是可信医疗置信建模的关键决定因素，为构建更可靠、可解释的大型医疗模型开辟新路径。

Abstract: Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.

</details>


### [23] [What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking](https://arxiv.org/abs/2601.15674)
*Raymond Xiong,Furong Jia,Lionel Wong,Monica Agrawal*

Main category: cs.CL

TL;DR: 以真实患者问题为基准的LLM问答评测揭示：基于医学考试题的基准无法反映现实患者问题的风格与意图；通过Google People Also Ask抓取美国前200个处方药相关问题，构建真实问题集；发现大部分问题含有错误假设且潜在危险意图；腐蚀性问题的产生非随机，受历史问题的影响；目前高性能LLMs在识别日常问题中的错误假设方面表现不足。


<details>
  <summary>Details</summary>
Motivation: 弥补现有医疗问答基准与现实患者需求之间的差距，评估LLMs在识别错误假设与潜在危险意图方面的能力。

Method: 数据收集：从Google的People Also Ask获取关于顶级处方药的前200项相关问题，构建真实患者问答数据集；对问题进行标注，分析其中的错误假设、危险意图；分析问题的产生是否受历史问题的影响（序列相关性/非独立性）；评估当前主流LLMs在识别错误假设方面的表现。

Result: 现实问题集中存在非随机的腐蚀性问题，且历史问题的分布决定新问题的出现；当前在其他基准表现良好的LLMs难以识别日常问题中的错误假设。

Conclusion: 需开发更贴近真实场景的评测基准，强化对错误假设和危险意图的识别能力，以及考虑历史上下文对问题出现的影响；提升对患者问答中的安全性和偏误监控。

Abstract: Patients are increasingly using large language models (LLMs) to seek answers to their healthcare-related questions. However, benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life. To bridge this gap, we sourced data from Google's People Also Ask feature by querying the top 200 prescribed medications in the United States, curating a dataset of medical questions people commonly ask. A considerable portion of the collected questions contains incorrect assumptions and dangerous intentions. We demonstrate that the emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions that led to their appearance. Current LLMs that perform strongly on other benchmarks struggle to identify incorrect assumptions in everyday questions.

</details>


### [24] [Persona Switch: Mixing Distinct Perspectives in Decoding Time](https://arxiv.org/abs/2601.15708)
*Junseok Kim,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: 提出 Persona Switch，在解码阶段动态地在零-shot 与角色扮演提示之间切换，依据输出的置信度（对数隙）逐步选取更可信的输出，显著提升准确性（最高提升约 5.13%）并验证输出置信度作为可靠度信号。


<details>
  <summary>Details</summary>
Motivation: 解决零-shot 提示和角色扮演提示在不同任务/实例上表现不一致的问题，假设两者具备互补优势，通过动态组合来提升推理性能。

Method: 逐步解码：在每一步同时生成基于零-shot 与角色扮演提示的输出，比较它们的对数隙（置信度）以选取更可信的一条输出；将选定的输出作为下一步的输入或最终答案；在常用大语言模型上进行基线对比与评估，使用准确性等指标。

Result: 实验结果显示 Persona Switch 在各类基线方法上均有提升，最高达到约 5.13% 的准确性增益；输出置信度可作为选择更可靠输出的有用信号。

Conclusion: 通过基于置信度的动态切换，结合两种提示策略的互补优势，可以提升大语言模型的零-shot 推理性能。

Abstract: Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.

</details>


### [25] [Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind](https://arxiv.org/abs/2601.15715)
*Zhitao He,Zongwei Lyu,Yi R Fung*

Main category: cs.CL

TL;DR: 提出 RebuttalAgent，并通过 ToM-Strategy-Response (TSR) 流水线实现对学术 rebuttal 的理论化建模，基于 RebuttalBench 数据集训练，使用自我奖励的强化学习进行自我提升，并通过 Rebuttal-RM 评测器进行可靠评估，在多项指标上优于基线和部分对手模型。


<details>
  <summary>Details</summary>
Motivation: 学术 rebuttal 具有信息不对称的策略性沟通特性，现有方法多聚焦表层语言学，缺乏有效的视角切换与说服策略，因此需要以 Theory of Mind 为理论基石来提升 rebuttal 的效果。

Method: 提出 ToM-Strategy-Response (TSR) 框架：先对评审的心理状态进行分析（ToM），再制定说服策略（Strategy），最后生成基于该策略的回应（Response）。数据通过 critique-and-refine 的方法合成成 RebuttalBench，训练分为有监督微调（ToM分析 + 策略规划）和自我奖励驱动的强化学习两阶段。评测方面构建 Rebuttal-RM，基于 10 万+样本建立评测器，评估分数与人类偏好的一致性超过强大的 GPT-4.1 判决。

Result: 实验显示 RebuttalAgent 在自动化指标上比基线模型平均提升 18.3%，并在自动化和人类评估中均优于部分先进的专有模型，同时在与人类偏好的一致性上也显示优势。

Conclusion: 将学术 rebuttal 置于 ToM 框架下的可行性得到实证支持，TSR 流水线在分析、策略制定与策略化生成方面有效提升 rebuttal 的质量与说服力；并通过 RebuttalBench 和 Rebuttal-RM 提供了可扩展的训练与评测生态。

Abstract: Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.

</details>


### [26] [Hallucination Mitigating for Medical Report Generation](https://arxiv.org/abs/2601.15745)
*Ruoqing Zhao,Runze Xia,Piji Li*

Main category: cs.CL

TL;DR: 提出 Knowledge-Enhanced Fine-Grained Reinforced Rewards Medical Report Generation (KERM) 框架，通过 MedCLIP 检索知识、净化模块确保上下文相关性、以及细粒度奖励以引导生成，从而减轻医学领域 LVLM 产出的幻觉并提升报告质量。


<details>
  <summary>Details</summary>
Motivation: 在医学报告生成中，尽管 LVLM 具备强大理解与生成能力，但易产生可行但不准确的幻觉，需通过知识增强与行为对齐来提升临床可信度与实用性。

Method: 1) 使用 MedCLIP 进行知识检索，筛选与患者病情相关的病灶事实句子；2) 引入净化模块，确保检索知识与患者临床背景的一致性与上下文相关性；3) 采用细粒度奖励进行强化学习，促使生成具有临床相关性、可验证性和支持性描述。

Result: 在 IU-Xray 与 MIMIC-CXR 数据集上验证，该框架显著减轻幻觉倾向、提升报告的可信度和临床相关性，报告质量有关注提升。

Conclusion: KERM 通过知识增强与奖励对齐，有效提升医学报告生成的可信度与实用性，具备良好应用前景；未来可在更大规模数据集、更多模态及真实临床场景中扩展，并完善对知识检索与净化过程的鲁棒性评估。

Abstract: In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concerns-especially in the nuanced and critical field of medical. In this work, we introduce a framework, \textbf{K}nowledge-\textbf{E}nhanced with Fine-Grained \textbf{R}einforced Rewards \textbf{M}edical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality.

</details>


### [27] [Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs](https://arxiv.org/abs/2601.15755)
*Tristan Williams,Franziska Weeber,Sebastian Padó,Alan Akbik*

Main category: cs.CL

TL;DR: 提出将对齐的价值观评估从单变量分布扩展到多变量相关关系结构；通过与世界价值观调查对照，发现仅追求边际分布不足以代表性，Demographic fine-tuning在边际上优于persona prompting，但两者均未能捕获 gold standard 的相关模式。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦于对齐的边际响应分布，忽略潜在的多变量相关结构与文化价值理论中的隐性结构，需引入多变量相关性框架以评估模型代表性。

Method: 提出一个基于多变量相关模式的评估框架；比较两种模型引导方法（persona prompting 与 demographic fine-tuning）对世界价值观调查数据的对齐效果，并同时评估边际分布与相关模式的匹配程度。

Result: demographic fine-tuning在边际分布上更接近人类响应，但两种方法均未能充分捕捉黄金标准的相关模式，表明代表性是对齐的一个独立维度，单纯的边际评估可能导致对模型能力的过于乐观判断。

Conclusion: 代表性是价值对齐的独特方面，强调需要同时考虑边际和多变量相关结构的评估，避免仅以边际结果判断模型能力。

Abstract: Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.

</details>


### [28] [HumanLLM: Towards Personalized Understanding and Simulation of Human Nature](https://arxiv.org/abs/2601.15793)
*Yuxuan Lei,Tianfu Wang,Jianxun Lian,Zhengyu Hu,Defu Lian,Xing Xie*

Main category: cs.CL

TL;DR: 提出一个以个体为中心的基础模型HumanLLM，通过Cognitive Genome Dataset进行监督微调，以预测个体行为、思维和偏好，并在跨域社交智能上表现出更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的预训练数据缺乏对持续、情境化个体决策与行为的理解，限制其在社会科学研究与个性化应用中的表现。

Method: 从Reddit、Twitter、Blogger、Amazon等平台构建Cognitive Genome Dataset，自动化提取超过550万条用户日志，通过多阶段数据筛选、合成与质量控制，形成可用于多任务学习的监督数据；对模型进行广义任务设定的监督微调，使其能预测个体行为、内在思维与经历，并评估写作风格和用户画像的真实性。

Result: 相较基线模型，HumanLLM在预测用户行动与内在思维方面表现更佳；更准确地模仿用户写作风格和偏好，生成更真实的用户画像；在跨域社交智能基准上显示显著泛化提升。

Conclusion: 通过对持续、情境化个体数据的整合建模，HumannLLM展示了在社会科学与个性化应用中的潜力与挑战。需关注隐私、同意、去敏化、偏见与安全等伦理风险，并在评估中正视数据来源与可解释性。

Abstract: Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.

</details>


### [29] [SteerEval: Inference-time Interventions Strengthen Multilingual Generalization in Neural Summarization Metrics](https://arxiv.org/abs/2601.15809)
*Silvia Casola,Ryan Soh-Eun Shim,Felicia Körner,Yuchen Mao,Barbara Plank*

Main category: cs.CL

TL;DR: 测试阶段将多语言度量的激活引导至英语枢纽，显著提升与人类判断的相关性，适用于编码器与解码器基础的指标。


<details>
  <summary>Details</summary>
Motivation: 多语言生成评估缺乏稳健泛用的指标；英语作为内部枢纽错配可能损害度量性能；通过对齐枢纽来提高跨语言的人类评估相关性。

Method: 对编码器-和解码器型的多语言评估指标应用测试时干预（test-time intervention），在推理阶段对模型激活做定向调整，以强化英语枢纽信号。

Result: 干预在各类度量上有效，提升了对多语言的相关性与鲁棒性；对 diverse languages 的指标有效性提升明显。

Conclusion: 英语 pivot 对齐的测试时干预是提升多语言神经评估指标与人类判断一致性的有效策略，应成为多语言NLG评估的可选方法。

Abstract: An increasing body of work has leveraged multilingual language models for Natural Language Generation tasks such as summarization. A major empirical bottleneck in this area is the shortage of accurate and robust evaluation metrics for many languages, which hinders progress. Recent studies suggest that multilingual language models often use English as an internal pivot language, and that misalignment with this pivot can lead to degraded downstream performance. Motivated by the hypothesis that this mismatch could also apply to multilingual neural metrics, we ask whether steering their activations toward an English pivot can improve correlation with human judgments. We experiment with encoder- and decoder-based metrics and find that test-time intervention methods are effective across the board, increasing metric effectiveness for diverse languages.

</details>


### [30] [ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15820)
*Guoxuan Ding,Yuqing Li,Ziyan Zhou,Zheng Lin,Daren Zha,Jiangnan Li*

Main category: cs.CL

TL;DR: 提出 ExDR：一个基于解释的动态检索增强生成框架，用于多模态假新闻检测，通过模型生成的解释驱动检索与证据检索，提升触发准确性、证据质量和检测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态假新闻快速传播，且需要及时、精准的证据；现有动态RAG 存在冗余检索、相似度粗糙、证据无关等问题，亟需利用解释信息提升检索与证据质量。

Method: 提出 ExDR，利用模型生成的解释在触发检索和证据检索两个阶段发挥作用；从三个维度评估触发信心；构建实体感知的索引，融合具有误导性的实体；基于欺骗特征检索对比证据以挑战初始断言，进而提升最终预测。

Result: 在 AMG、MR2 两个基准数据集上，ExDR 在触发检索准确性、检索质量和总体检测性能上均优于前沿方法，具有良好有效性和泛化能力。

Conclusion: 通过解释驱动的动态检索与对比证据策略，ExDR 能更有效地检测多模态假新闻，且具备较强的泛化性。

Abstract: The rapid spread of multimodal fake news poses a serious societal threat, as its evolving nature and reliance on timely factual details challenge existing detection methods. Dynamic Retrieval-Augmented Generation provides a promising solution by triggering keyword-based retrieval and incorporating external knowledge, thus enabling both efficient and accurate evidence selection. However, it still faces challenges in addressing issues such as redundant retrieval, coarse similarity, and irrelevant evidence when applied to deceptive content. In this paper, we propose ExDR, an Explanation-driven Dynamic Retrieval-Augmented Generation framework for Multimodal Fake News Detection. Our framework systematically leverages model-generated explanations in both the retrieval triggering and evidence retrieval modules. It assesses triggering confidence from three complementary dimensions, constructs entity-aware indices by fusing deceptive entities, and retrieves contrastive evidence based on deception-specific features to challenge the initial claim and enhance the final prediction. Experiments on two benchmark datasets, AMG and MR2, demonstrate that ExDR consistently outperforms previous methods in retrieval triggering accuracy, retrieval quality, and overall detection performance, highlighting its effectiveness and generalization capability.

</details>


### [31] [Can professional translators identify machine-generated text?](https://arxiv.org/abs/2601.15828)
*Michael Farrell*

Main category: cs.CL

TL;DR: 69 translators tried to distinguish 3 anonymized short stories (2 AI-generated by ChatGPT-4o, 1 human) in Italian. 16.2% correctly identified AI texts; many relied on subjective impressions. Low burstiness and narrative contradictions, plus calques/semantic loans/syntactic transfer from English emerged as signals; grammatical accuracy and emotional tone often misled. Implications for professional synthetic-text editing and assessment remain open.


<details>
  <summary>Details</summary>
Motivation: Assess the reliability of professional translators in detecting AI-generated fiction and identify reliable textual cues that distinguish AI from human authors, informing the scope of synthetic-text editing in professional settings.

Method: In-person experiment with 69 translators. Each judged three anonymized stories (two AI-written, one human-written) and rated the likelihood of AI authorship, providing justification for their judgments.

Result: Overall averages were inconclusive, but a statistically significant subset (≈16.2%) could reliably distinguish AI from human text. A large number misclassified, often guided by subjective impressions. Indicators such as low burstiness, narrative contradictions, and English calques were cited as reliable; grammatical accuracy and emotional tone frequently led to misclassification.

Conclusion: The findings question the reliability of human-based detection in professional contexts and highlight the need to understand the limits and scope of synthetic-text editing, as well as the cues professionals use to judge authorship.

Abstract: This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.

</details>


### [32] [Determinants of Training Corpus Size for Clinical Text Classification](https://arxiv.org/abs/2601.15846)
*Jaya Chaturvedi,Saniya Deshpande,Chenkai Ma,Robert Cobb,Angus Roberts,Robert Stewart,Daniel Stahl,Diana Shamsutdinova*

Main category: cs.CL

TL;DR: 在MIMIC-III临床文本分类任务中，使用BERT嵌入+随机森林，10个诊断任务的学习曲线显示，约600份标注文档即可达到接近10,000文档的95%性能；词汇质量（强预测词 vs 嘈杂词）显著影响学习效率：更多强预测词与更少嘈杂词可提升最大准确度并加速学习。


<details>
  <summary>Details</summary>
Motivation: 明确临床文本分类对样本量的需求及其与文本词汇特性之间的关系；填补仅以固定数据量为基准的经验性做法缺乏理论基础的问题。

Method: 基于公开的MIMIC-III数据集的住院出院记录，标签为ICD-9诊断；采用预训练BERT嵌入后再用随机森林分类器，选取10个随机诊断作为任务，训练集规模从100到10,000文档；通过对词袋嵌入使用Lasso逻辑回归分析强预测词与嘈杂预测词，评估词汇属性与学习曲线的关系。

Result: 各任务的学习曲线存在显著差异；在所有任务中，约600份文档即可实现相当于10,000份文档的95%性能；词汇分析显示，强预测词数量多、嘈杂预测词数量少、与更陡峭的学习曲线相关；每增加100个嘈杂词，准确率约下降0.02；每增加100个强预测词，最大准确度约提升0.04。

Conclusion: 样本量需求受词汇性质驱动，提升词汇质量（增加强预测词、减少嘈杂词）可显著提高学习效率；不同任务间存在变异，提示在注释预算分配与特征分析时需关注词汇特征的质量。

Abstract: Introduction: Clinical text classification using natural language processing (NLP) models requires adequate training data to achieve optimal performance. For that, 200-500 documents are typically annotated. The number is constrained by time and costs and lacks justification of the sample size requirements and their relationship to text vocabulary properties.
  Methods: Using the publicly available MIMIC-III dataset containing hospital discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT embeddings followed by Random Forest classifiers to identify 10 randomly selected diagnoses, varying training corpus sizes from 100 to 10,000 documents, and analyzed vocabulary properties by identifying strong and noisy predictive words through Lasso logistic regression on bag-of-words embeddings.
  Results: Learning curves varied significantly across the 10 classification tasks despite identical preprocessing and algorithms, with 600 documents sufficient to achieve 95% of the performance attainable with 10,000 documents for all tasks. Vocabulary analysis revealed that more strong predictors and fewer noisy predictors were associated with steeper learning curves, where every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04.

</details>


### [33] [Artificial Rigidities vs. Biological Noise: A Comparative Analysis of Multisensory Integration in AV-HuBERT and Human Observers](https://arxiv.org/abs/2601.15869)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: AV-HuBERT在McGurk实验中的感知生物保真性方面与人类高度一致在观测，但在融合倾向和感知变异性方面存在偏差。


<details>
  <summary>Details</summary>
Motivation: 评估自监督多模态模型在语音感知中的神经变异性和跨模态阈值的生物等效性。

Method: 以N=44名观察者对比AV-HuBERT对不一致的视听刺激的响应，比较听感主导率与融合率；分析AI与人类在听觉主导与融合上的差异，以及人类的感知随机性与多样错误分布。

Result: AI与人类在听觉主导率上几乎同构(32.0% vs 31.8%)；AI呈现对音位融合的确定性偏差(68.0% vs 47.7%)，显著高于人类；人类呈现感知随机性与多样的错误分布，而模型保持严格的分类性。

Conclusion: 现有自监督多模态结构在宏观的多感官结果上与人类相符，但缺乏人类语音感知固有的神经变异性，需要在模型中引入随机性/不确定性以更好地模拟人类感知过程。

Abstract: This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.

</details>


### [34] [Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model](https://arxiv.org/abs/2601.15892)
*Chenghao Fan,Wen Heng,Bo Li,Sichen Liu,Yuxuan Song,Jing Su,Xiaoye Qu,Kai Shen,Wei Wei*

Main category: cs.CL

TL;DR: Stable-DiffCoder: 基于扩散的区块码模型，复用 Seed-Coder 架构，通过区块扩散持续预训练（CPT）和裁剪噪声调度实现稳定学习，在相同数据/架构下超越强 AR 基线及大多数 ~8B 参数模型，证明扩散训练可提升代码建模质量，并利于低资源语言及编辑/推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有扩散/非自回归（DLLM）在与等量预算的 AR 基线相比仍存在差距；需要更高效的知识学习、数据重用和稳定训练机制，探索区块扩散在代码建模中的潜力及其对编辑、推理任务的优势。

Method: 复用 Seed-Coder 架构、数据和训练管线，增加区块扩散持续预训练（CPT），通过定制的 warmup 和区块级裁剪噪声调度实现稳定训练；结合数据增强、监督微调，以及对任序扩散建模对结构化代码的影响评估。

Result: 在相同数据与架构下，Stable-DiffCoder 对广泛代码基准的表现优于 AR 对手；仅靠 CPT 和监督微调即可超过多数 ~8B 参数的 ARs 和 DLLMs；扩散学习提升了对编辑/推理的结构化代码建模，数据增强也帮助低资源编程语言。

Conclusion: 扩散式训练可以提升代码建模质量，超越单纯依赖 AR 训练的效果；任序建模和数据增强在编辑、推理及低资源场景中具有显著潜力。

Abstract: Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.

</details>


### [35] [Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech](https://arxiv.org/abs/2601.15909)
*Soufiane Jhilal,Stéphanie Martin,Anne-Lise Giraud*

Main category: cs.CL

TL;DR: Image-based MEG representations fed into pretrained vision models enable strong imagined-speech decoding, achieving high accuracy across tasks.


<details>
  <summary>Details</summary>
Motivation: Overcome weak, distributed MEG signals and limited labeled data in imagined-speech decoding by leveraging powerful pretrained vision models through an image-like MEG representation.

Method: Convert MEG signals to time–frequency scalograms; project data from 21 participants into three spatial scalogram mixtures via a learnable sensor-space convolution; generate compact image-like inputs suitable for ImageNet-pretrained architectures; apply pretrained vision models to decode imagined speech tasks (imagery vs. silent, imagery vs. silent reading, vowel decoding); cross-subject evaluation and temporal analysis.

Result: Outperforms classical and non-pretrained models; up to 90.4% balanced accuracy for imagery vs. silence, 81.0% for imagery vs. silent reading, and 60.6% for vowel decoding. Cross-subject tests suggest shared neural representations captured by pretrained models; temporal analysis localizes discriminative information to imagery-locked intervals.

Conclusion: Applying pretrained vision models to image-based MEG representations effectively captures the structure of imagined speech in non-invasive neural signals.

Abstract: Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.

</details>


### [36] [Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain](https://arxiv.org/abs/2601.16018)
*Özgür Uğur,Mahmut Göksu,Mahmut Çimen,Musa Yılmaz,Esra Şavirdi,Alp Talha Demir,Rumeysa Güllüce,İclal Çetin,Ömer Can Sağbaş*

Main category: cs.CL

TL;DR: Mecellem框架用于土耳其法律领域的专用语言模型：一方面从零开始预训练的编码器模型，使用土耳其语语料；另一方面通过持续预训练（CPT）对解码器进行领域适应。


<details>
  <summary>Details</summary>
Motivation: 解决土耳其法律文本的领域特定语言需求，克服多阶段、计算资源密集的训练流程，实现更高效且具成本效益的域适应。

Method: (1) 编码器：基于ModernBERT的双向编码器在112.7B土耳其语代币语料上从零开始预训练，使用一个检查点选择策略在训练过程中评估下游检索性能，发现最佳检查点在预训练损失达到最小前达到最好检索分数；155M参数的小模型也能达到与307M-567M的大模型相当的性能。对比嵌入成本，生产效率达到92.36%，第四名，且比SOTA的多阶段训练更高效。 (2) 解码器：Qwen3-1.7B与Qwen3-4B在土耳其法律领域通过控制式课程学习进行持续预训练，设计四阶段CPT并优化样本比例，使模型从通用知识平滑过渡到法律术语与长上下文推理。

Result: 编码器在土耳其检索排行榜中名列前茅；小模型与大模型表现相近，生产效率高，成本更低；解码器通过CPT实现对领域的显著困惑降低，达到36.2% 的困惑度降低（perplexity）。

Conclusion: 单阶段预训练+高效的后训练路径在成本与性能之间实现良好折中，提供了一个对土耳其法律文本的有效领域适应框架；对检索和长上下文推理等任务具有明显收益。

Abstract: This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.

</details>


### [37] [Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction](https://arxiv.org/abs/2601.16034)
*Tony Cristofano*

Main category: cs.CL

TL;DR: 提出 Trajectory Replay via Concept-Basis Reconstruction，将拒绝干预从供体模型迁移到目标模型，跨越 Dense 与 MoE 等不同架构与训练 regime，无需目标端拒绝监督；通过概念指纹对齐和概念原子集合的“配方”重构拒绝方向，映射干预轨迹到目标语义空间，并引入权重奇异值分解（weight-SVD）稳定性约束以避免高方差权重子空间带来的副作用；在8对模型（含 GPT-OSS-20B、GLM-4）上验证，迁移的“配方”能降低拒绝行为且保持能力，支持安全对齐的语义普遍性。


<details>
  <summary>Details</summary>
Motivation: 探究拒绝行为是否源自普遍、低维的语义电路，而非仅仅是单个模型的特定性质，从而实现跨模型、跨架构的拒绝干预迁移。

Method: 通过概念指纹对齐层，使用一个跨模型的“概念原子”配方来重构拒绝向量；把供体模型的拒绝干预轨迹映射到目标模型的语义空间；在干预过程中引入 weight-SVD 稳定性约束，确保干预不走高方差的权重子空间，从而保护模型能力。

Result: 在8对模型配对上进行评估（包括 GPT-OSS-20B 与 GLM-4 等），迁移后的配方在降低拒绝响应的同时保持性能，结果支持安全对齐的语义普遍性。

Conclusion: 证据表明拒绝行为的语义底层具有跨模型的普适性，且基于该语义基底的迁移方法能实现在不依赖目标端拒绝监督的情况下进行干预迁移。

Abstract: Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.

</details>


### [38] [Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating](https://arxiv.org/abs/2601.16097)
*Makbule Gulcin Ozsoy*

Main category: cs.CL

TL;DR: 提出可扩展的多语言 Text2Cypher 框架：在英语、西班牙语、土耳其语上训练语言特定的 LoRA 适配器，通过线性融合与学习型融合 MLP 进行适配融合，实现无需全量再微调即可扩展新语言，融合 MLP 近似复原联合微调的性能并具数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决多语言 Text2Cypher 的有限语言覆盖问题，避免对新语言进行昂贵的全量微调和繁琐超参调优，同时保持较高的性能。

Method: 训练各语言的 LoRA 适配器；通过 uniform linear merging 或学习型融合 MLP（含动态门控）将适配器融合；评估在英语、西语、土耳其语上的效果，并探讨扩展新语言的方案（只需一个新的 LoRA 适配器和轻量级 MLP 重训练）。

Result: 融合 MLP 在多语言微调收益方面约恢复 75% 的准确性增益，且所需数据量小于完整的联合微调；在线性融合方面在三语言上均表现落后于融合 MLP；MLP 同时在数据效率和可扩展性方面优于线性融合。

Conclusion:  Learned adapter fusion 为成本高昂的联合微调提供了一个可行替代，平衡了性能、数据效率和跨语言扩展性，适用于逐步扩展的多语言 Text2Cypher 任务。

Abstract: Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.

</details>


### [39] [synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier](https://arxiv.org/abs/2601.16113)
*Haq Nawaz Malik,Kh Mohmad Shafi,Tanveer Ahmad Reshi*

Main category: cs.CL

TL;DR: SynthOCR-Gen 是一个面向低资源语言的开源合成OCR数据集生成器，通过将数字Unicode文本转化为训练数据，包含分段、规范化、多字体渲染和 25+ 数据增强，生成 Kashmiri 的 60 万样本数据集并公开发布。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏大规模标注训练数据，尤其是使用 Perso-Arabic 脚本的语言（如 Kashmiri）尚未被主流 OCR 系统良好支持，人工数据集构建成本高、耗时且易出错。

Method: 提出端到端数据生成管线：文本分段（字、词、n-gram、句子、行）、Unicode 规范化与脚本纯度检查、可配置分布的多字体渲染，以及覆盖旋转、模糊、噪声、扫描仪伪影等 25+ 数据增强的合成过程。

Result: 生成并公开发布一个 60 万样本的 Kashmiri 词分段 OCR 数据集，放在 HuggingFace 上，展示了该方法在低资源语言 OCR 数据生产中的有效性与可用性。

Conclusion: 该工具为低资源语言进入视觉-语言AI模型提供实用路径，开源供研究者与从业者使用，推动 underserved writing systems 的 OCR/vision-language 应用发展。

Abstract: Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.
  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.
  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.

</details>


### [40] [Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging](https://arxiv.org/abs/2601.16127)
*Alphaeus Dmonte,Vidhi Gupta,Daniel J Perry,Mark Arehart*

Main category: cs.CL

TL;DR: 对多语言多任务模型的合并策略进行效率分析，发现训练成本显著降低且质量保持。


<details>
  <summary>Details</summary>
Motivation: 解决在添加新语言或扩展语言时重新训练多语言模型带来的高成本和维护瓶颈，探究合并策略在效率方面的表现。

Method: 在三个独立任务上评估多语言模型合并方案的效率；比较初始训练时间、语言更新后再合并的成本；使用公开和行业专用数据集进行实验。

Result: 初始训练时间最多下降50%；单语言更新后重新合并的训练成本下降超过60%；在学术和工业场景中均能保持质量相当（与基线相当）。

Conclusion: 合并策略在效率方面具有实用性，适用于工业维度的模型维护与更新，能够在保持质量的前提下减少训练成本，提供商用和学术场景的双向适用性。

Abstract: Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.

</details>


### [41] [Automatic Classification of Arabic Literature into Historical Eras](https://arxiv.org/abs/2601.16138)
*Zainab Alhathloul,Irfan Ahmad*

Main category: cs.CL

TL;DR: Automatic era classification of Arabic texts using neural networks; strong performance for binary-era tasks but weak performance for multi-era granularity across two public corpora.


<details>
  <summary>Details</summary>
Motivation: Addresses a gap in digital humanities: automatic temporal labeling of Arabic texts beyond poetry, enabling study of linguistic evolution over time.

Method: Neural networks/deep learning applied to two public corpora (OpenITI, APCD); experiments cover binary-era and multi-era setups (15-era, 12-era) with predefined and custom periodizations.

Result: Binary-era F1: ~0.83 (OpenITI) and ~0.79 (APCD); multi-era F1: ~0.20 (OpenITI, 15-era) and ~0.18 (APCD, 12-era).

Conclusion: Deep learning can distinguish broad historical periods in Arabic text but struggles with fine-grained era classification; results depend on era definitions and data quality; further work is needed to improve granularity and robustness.

Abstract: The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: 本文评估跨语言的回答质量与文化上下文在大语言模型中的影响，发现低资源语言的回答质量显著下降，语言选择会改变模型使用的文化信息，进而影响下游答案质量。


<details>
  <summary>Details</summary>
Motivation: 确保不同语言的用户在与大语言模型互动时获得等质的响应，揭示语言与文化在模型中的耦合，以及语言偏好如何改变回答的文化背景与信息使用。

Method: 基于WildChat数据集分析，构建一组真实世界的开放式问题，并在多语言环境下评测模型的回答质量；使用“LLM-as-a-Judge”来识别回答中的文化背景信息；在翻译子集上评估 CulturalBench，对多语言版本进行比较。

Result: 实验显示在低资源语言中，开放式问答的回答质量普遍较低；语言显著影响模型所使用的文化上下文，进而影响下游答案的质量。

Conclusion: 存在语言公平性问题，需要提升对低资源语言的支持，减少语言-文化偏差对回答质量的影响，并探索更鲁棒的跨语言文化对齐与评估方法。

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [43] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: 通过在路由池中引入零计算的空专家，并在训练中采用负载均衡策略实现数据稀疏，避免因果性破坏；与权重稀疏结合，在视觉-语言模型训练中达到比仅权重稀疏更优的计算效率与性能，且模型显示隐式的模态分配能力，将视觉标记更倾向路由到空专家。


<details>
  <summary>Details</summary>
Motivation: 解决自回归MoE中的数据稀疏与因果性不一致问题，同时在数据异质性场景下提高算力利用率；通过零计算专家实现数据稀疏的可控引入，保持因果性。

Method: 在MoE路由池中加入零计算（null）专家；当标记路由到空专家时，该时隙不消耗计算；使用标准负载均衡目标促使真实与空专家均匀被使用，从而在期望层面实现数据稀疏；在视-言模型训练上评估，与仅权重稀疏相比，比较不同稀疏组合下的前沿与性能。

Result: 在匹配的预期 FLOPs 下，权重稀疏和数据稀疏结合的前沿优于单独的权重稀疏，训练损失与下游性能提升；模型学习出对模态的隐式分配，倾向将视觉标记分配给空专家而非文本标记，且无需显式模态路由。

Conclusion: 通过引入空专家实现数据稀疏且保持因果性，结合权重稀疏可提升计算效率与模型性能，为MoE的高效性提供一种可行且无额外因果性干扰的设计。

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [44] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: GOAT generalizes attention via entropic OT with trainable priors, improving flexibility and length generalization while addressing attention sinks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of standard attention that assume a uniform prior in the entropic OT formulation, causing representational constraints and attention sinks; seek a learnable, continuous prior that can be integrated with fast kernels and improve generalization.

Method: Introduce Generalized Optimal Transport Attention with Trainable priors (GOAT): replace the implicit uniform prior with a learnable continuous prior; ensure compatibility with efficient kernels like FlashAttention; provide an EOT-based explanation of attention sinks and a solution; incorporate spatial information into the core computation to obtain a learnable, extrapolatable prior that blends learned positional embeddings with fixed encodings.

Result: A theoretical framework and algorithmic mechanism (GOAT) that replaces the uniform prior in entropic OT attention with a trainable prior, offers an EOT-based explanation and mitigation of attention sinks, and yields a prior that can absorb spatial information to achieve extrapolation and better generalization while remaining compatible with fast attention kernels.

Conclusion: GOAT presents a flexible, trainable prior-based attention mechanism that preserves kernel efficiency, explains and mitigates attention sinks via EOT, and combines the benefits of learned and fixed positional encodings to achieve length-generalizable, extrapolatable attention.

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [45] [FedUMM: A General Framework for Federated Learning with Unified Multimodal Models](https://arxiv.org/abs/2601.15390)
*Zhaolong Su,Leheng Zhao,Xiaoying Wu,Ziyue Xu,Jindong Wang*

Main category: cs.LG

TL;DR: 提出FedUMM，在非IID多模态数据的联邦学习场景下，对统一多模态模型进行参数高效微调（LoRA），服务器仅聚合适配器更新，显著降低通信成本；在VQA v2和GenEval基准上与集中训练竞争，实验展示了可扩展性与隐私保护潜力。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型多在集中式服务器训练，难以在隐私敏感与地理分布式场景中部署；需要在分布式环境下实现隐私保护的模型能力，同时降低通信成本以适应多客户端场景。

Method: 在NVIDIA FLARE基础上，将BLIP3o骨干进行参数高效微调；客户端仅训练轻量级LoRA适配器并冻结主干模型，服务端聚合适配器更新，忽略大模型权重更新。

Result: 在Dirichlet控制的非IID设定下，最多16个客户端的VQA v2与GenEval评测中，性能略有下降但仍具备与集中训练的竞争力；分析显示适配器联邦相较全量微调每轮通信量降低超过一个数量级。

Conclusion: 为隐私保护的统一多模态模型研究提供实践经验，表明适配器级联邦学习是实现高效、可扩展的路径，可在分布式场景下推进UMMs的部署。

Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.

</details>


### [46] [Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC](https://arxiv.org/abs/2601.15399)
*Ashna Nawar Ahmed,Banooqa Banday,Terry Jones,Tanzima Z. Islam*

Main category: cs.LG

TL;DR: A surrogate-assisted MOBO framework using attention-based embeddings of job telemetry to optimize runtime-power in HPC scheduling; shows superior Pareto fronts and data efficiency on production workloads.


<details>
  <summary>Details</summary>
Motivation: HPC schedulers must balance performance and power under facility-wide resource constraints; standard surrogates may miss complex performance dynamics; need data-efficient optimization.

Method: Embed telemetry with attention-based encoders to build surrogate models; integrate into a multi-objective Bayesian optimization loop with intelligent acquisition to improve sample efficiency.

Result: On two production HPC datasets, embedding-informed surrogates yielded higher-quality Pareto fronts for runtime-power trade-offs; intelligent data sampling reduced training costs and improved result stability.

Conclusion: First work to apply embedding-informed surrogates within a MOBO framework for HPC scheduling, jointly optimizing performance and power on production workloads.

Abstract: High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.

</details>


### [47] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: 提出Ambient Dataloops：一个用于迭代改进数据集以便扩散模型更好学习底层分布的框架，通过数据与模型的协同进化，逐步提升数据质量并相应提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据集质量参差不齐，直接用混合质量数据训练会导致模型学习受损，需要一个机制在训练过程中提升数据质量而不导致自我毁灭性循环。

Method: 在每一代中，数据集要素逐步提升质量；把合成的改进样本视为有噪声的数据，但噪声水平略低于上一代，并使用Ambient Diffusion等技术在污染条件下进行学习；实现数据-模型的协同进化，直至收敛。

Result: 在无条件与文本条件图像生成以及新蛋白质设计方面达到最新公开结果（state-of-the-art）。

Conclusion: 给出理论分析解释数据循环过程的有利作用，证明数据质量的渐进提升与模型性能改进之间的耦合效应。

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [48] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: 提出 Lattice，一种带二值置信门控的混合序列预测框架，通过将行为窗口聚类成原型并仅在置信度超过阈值时激活原型评分；在 MovieLens/LSTM、LIGO、金融数据上的实验显示显著提升与对分布漂移的鲁棒性，且对 Transformer 回路呈现中性影响，体现置信门控在安全关键应用中的不确定性管理作用。


<details>
  <summary>Details</summary>
Motivation: 解决序列预测中的辨识性不确定性（epistemic uncertainty）问题：在需要时启用对特定行为原型的额外结构化评分，漂移时回退到稳定的基线预测，以避免错误激活；并在多域数据上验证门控机制的鲁棒性和可解释性。

Method: 将混合序列预测系统与二元置信门控结合；通过将行为窗口聚类为行为原型，只有当置信度超过阈值时才激活基于原型的评分分支，低置信度采用基线预测；在 MovieLens（LSTM）、LIGO、金融市场等数据集上，分别使用 LSTM 与 Transformer 作为基础骨架，比较激活策略的效果。

Result: MovieLens +LSTM：HR@10 提升约31.9%，p-value 3.29×10^-25，30 次种子；相较 SASRec 与 BERT4Rec，提升分别约109.4% 与 218.6%。LIGO与金融数据：在分布漂移时系统正确拒绝激活，体现置信门控防误激活。Transformer 骨架下，Lattice 提供 0.0% 的改进（无负效应，若结构已存在则推迟激活）。

Conclusion: 置信门控作为管理序列预测中 epistemic uncertainty 的有力架构原则，能够在模式适用时激活、在漂移时拒绝、在冗余时推迟，具备在安全关键应用中的潜在价值。

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [49] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: CASL 提供在扩散模型中对稀疏潜在维度与语义概念的监督对齐，通过对冻结 U-Net 激活的自编码器进行训练，并用线性映射将概念与相关潜维绑定；CASL-Steer 作为因果探针；提出编辑精度比（EPR）用于评估概念特异性与无关属性的保持；实验显示在编辑精度与可解释性上优于现有方法，声称首次实现监督对齐。


<details>
  <summary>Details</summary>
Motivation: 解释扩散模型内部激活中的语义信息，现有的基于稀疏自编码器的无监督方法难以将稀疏特征对齐到人工可理解的概念，限制了语义控制能力。

Method: 在冻结的 U-Net 激活上训练自编码器以获得解缠的潜在表示；再学习一个轻量级线性映射，将每个概念与一小组相关潜维相关联；提出 CASL-Steer 作为因果探针，沿着学习到的概念轴移动激活以验证语义含义；提出编辑精度比（EPR）度量概念特异性与无关属性的保持。

Result: 实验表明 CASL 在编辑精度和可解释性方面优于现有方法。

Conclusion: 本工作首次实现扩散模型潜在表示与语义概念的监督对齐，为语义控制和可解释性提供更可靠的工具。

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [50] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 在数据由自然数据与来自多代生成的合成数据混合的场景下分析 ERM 的局限性及可行的鲁棒学习策略。结果：ERM 对均值估计仍可收敛但被采用非均匀权重的算法超越；在 PAC 学习中 ERM 不总是收敛到真实概念，但存在对任意 VC 类和任意污染量的鲁棒算法可学习正确假设。


<details>
  <summary>Details</summary>
Motivation: 随着低成本大语言模型生成的合成内容普及，真实数据被混入看起来“自然”的合成数据，导致学习系统需在不知源数据的情况下进行学习。研究在数据污染环境下 ERM 的可能性与限制，并寻找可跨越污染的学习算法。

Method: 把问题建模为一系列学习任务，输入是自然数据与合成数据的混合，且学习算法对单个样本的来源不可知。对均值估计，比较 ERM 与对不同数据代赋予非均匀权重的算法；在 PAC 学习框架下考察在污染下的收敛性，论证存在对任意 VC 类与任意污染量也能学习正确假设的算法。

Result: 对于多维均值估计，ERM 可以收敛到真实均值，但被加权算法超越；在 PAC 框架下，ERM 不一定收敛到真实概念，呈现模型坍塌的现象；但存在对于任意 VC 类和任意污染量的鲁棒算法，能够学习到正确的假设。

Conclusion: 在合成数据广泛存在的现实场景下，单纯的 ERM 面临显著局限，需设计对污染抗性的学习算法，且此类算法可推广到任意 VC 类的学习任务和任意水平的污染。

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [51] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther 是一个与 PyTorch 兼容的 RandNLA 库，提供对常用层的高性能降维替代实现，并通过自研 pawX 后端实现 CPU/GPU 加速，从而在不显著损失精度的前提下降低内存使用（如 BERT 75%）。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型对显存和计算资源的需求日益增长，RandNLA 提供压缩手段，但缺乏统一、生产级库，阻碍广泛应用。

Method: 实现 Panther，提供对 sketched linear layers、2D conv、多头注意力、随机矩阵分解等的替代实现；自建 C++/CUDA 后端 pawX；PyTorch 兼容，便于替换。

Result: 使用 Panther 替换标准线性层，内存下降高达 75%，在保持接近损失的前提下；在 BERT 上有显著的内存节省。

Conclusion: Panther 提供易于接入的 RandNLA 框架，促进 RandNLA 在实际训练中的应用；MIT 许可，源码与演示视频可用。

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [52] [Multi-Targeted Graph Backdoor Attack](https://arxiv.org/abs/2601.15474)
*Md Nabi Newaz Khan,Abdullah Arafat Miah,Yu Bi*

Main category: cs.LG

TL;DR: 提出一种针对图分类的多目标后门攻击，利用子图注入在不破坏原始图结构的前提下对多个目标标签实现高效触发，干净准确率影响极小；具有良好模型泛化性并对现有防御具鲁棒性，代码公开。


<details>
  <summary>Details</summary>
Motivation: GNN在多领域表现出色，但易受后门攻击，现有工作多聚焦单目标、基于子图替换的攻击，缺乏对多目标攻击的系统研究，需要在保留图结构的同时污染样本，并评估对不同模型、训练参数及防御的鲁棒性。

Method: 提出子图注入策略，在保留原始图结构的前提下污染干净图；系统分析注入方式、连接数、触发尺寸、边密度和污染比等设计参数；在四种GNN模型与五个数据集上对比传统子图替换攻击，并评估对随机化平滑、微调剪枝等防御的鲁棒性；源代码公开。

Result: 在所有目标标签上实现高攻击成功率，干净精度影响极小；相较于基于子图替换的攻击，在多数据集与多模型上展现更强攻击效果，具有良好泛化性；对设计参数的影响有系统化证据；对现有防御仍具鲁棒性。

Conclusion: 首次系统展示GNN在图分类任务中的多目标后门攻击能力，揭示潜在风险，促进GNN安全研究，并提供可复现实验代码。

Abstract: Graph neural network (GNN) have demonstrated exceptional performance in solving critical problems across diverse domains yet remain susceptible to backdoor attacks. Existing studies on backdoor attack for graph classification are limited to single target attack using subgraph replacement based mechanism where the attacker implants only one trigger into the GNN model. In this paper, we introduce the first multi-targeted backdoor attack for graph classification task, where multiple triggers simultaneously redirect predictions to different target labels. Instead of subgraph replacement, we propose subgraph injection which preserves the structure of the original graphs while poisoning the clean graphs. Extensive experiments demonstrate the efficacy of our approach, where our attack achieves high attack success rates for all target labels with minimal impact on the clean accuracy. Experimental results on five dataset demonstrate the superior performance of our attack framework compared to the conventional subgraph replacement-based attack. Our analysis on four GNN models confirms the generalization capability of our attack which is effective regardless of the GNN model architectures and training parameters settings. We further investigate the impact of the attack design parameters including injection methods, number of connections, trigger sizes, trigger edge density and poisoning ratios. Additionally, our evaluation against state-of-the-art defenses (randomized smoothing and fine-pruning) demonstrates the robustness of our proposed multi-target attacks. This work highlights the GNN vulnerability against multi-targeted backdoor attack in graph classification task. Our source codes will be available at https://github.com/SiSL-URI/Multi-Targeted-Graph-Backdoor-Attack.

</details>


### [53] [Early predicting of hospital admission using machine learning algorithms: Priority queues approach](https://arxiv.org/abs/2601.15481)
*Jakub Antczak,James Montgomery,Małgorzata O'Reilly,Zbigniew Palmowski,Richard Turner*

Main category: cs.LG

TL;DR: 对7天预测急诊部每日到院量，比较SARIMAX、XGBoost、LSTM三种模型，使用Prophet生成COVID-19期间的对照值；XGBoost在总量预测中MAE最佳，SARIMAX在重大复杂病例预测中MAE最佳；三者均优于季节性朴素基线；普遍存在对突发高峰的低估问题。


<details>
  <summary>Details</summary>
Motivation: 提升急诊部资源分配的预测准确性，降低疫情等异常事件对数据的干扰；通过对需求按科室类别与临床复杂度分层以提高预测细粒度与可解释性。

Method: 数据来自澳大利亚某三级转诊医院，时间范围2017-2021，进行7天滚动预测；需求细分为8个科室类别并按临床复杂度分层；对COVID-19时期使用Prophet生成合成对照值以缓解异常；比较SARIMAX、XGBoost、LSTM的预测性能，并与季节性朴素基线比较，评估指标为MAE，重点关注总入院量与大复杂度病例；

Result: 三模型均显著优于季节性朴素基线；XGBoost在总日入院量的MAE为6.63；SARIMAX在重大复杂病例的MAE为3.77；文中未给出LSTM的具体数值，但强调总体优良表现；模型在重复日常模式方面表现稳定，但对突发、高幅度波动仍存在欠拟合倾向。

Conclusion: 尽管能复现常态的日常模式，模型在突发、低频的就诊高峰上存在共同的低估限制。可通过引入冲击事件特征、外部数据源、分位数预测或集成多模型以提升对突变的预测能力，并考虑对不同时间尺度的自适应建模与校准。

Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.

</details>


### [54] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: 提出 Martingale Foresight Sampling (MFS)，将 LLM 解码视为寻找最优随机过程，通过马尔可夫过程理论给出可理论化的步长估值、候选路径裁剪与自适应停止规则，在六项推理基准上超越现有方法并提升效率，代码即将开源。


<details>
  <summary>Details</summary>
Motivation: 解决自回归解码的短视性与依赖启发式的搜索策略的不足，寻求一个以概率理论为基础的推理路径评估与停止机制。

Method: 将路径质量建模为随机过程，利用 Doob Decomposition Theorem 来衡量路径的可预测优势；用 Optional Stopping Theory 进行子候选路径的裁剪；基于 Martingale 收敛性定理的自适应停止规则在路径质量收敛时终止探索。

Result: 在六个推理基准上，MFS 在准确率方面超越当前最先进方法，同时显著提升计算效率。

Conclusion: 建立了一个理论扎实且高效的解码框架，展示将马尔可夫理论应用于推理路径评估、裁剪与停止的可行性与优势，代码将公开。

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [55] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: 提出 Margin-Aware Speculative Verification，训练无关、领域无关的验证策略，依据目标模型的局部决断性来调节验证强度，从而提升 Speculative Decoding 的推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的预测解码（Speculative Decoding，SD）通过解耦生成和验证来加速推理；但验证阶段依旧以严格的逐 token 拒绝策略为主，在低利润边际（low-margin）下，目标模型对候选 token 的偏好较弱，拒绝合理的备选 token 带来的信息增益有限且回滚成本高，导致验证过程低效。

Method: 提出 Margin-Aware Speculative Verification，一种训练-free、领域无关的验证策略。通过直接从目标 logits 测量决策稳定性来对验证进行条件化，仅在严格验证带来显著收益时才进行拒绝，放宽在边际收益不足时的拒绝。该方法仅修改验证规则，且可与现有的面向目标模型的 Speculative Decoding 框架无缝兼容。

Result: 在模型规模从8B到235B的广泛设置上进行大量实验证明，与最先进的基线相比，方法在推理速度上具有一致且显著的提升，同时在多种基准上保持生成质量。

Conclusion: Margin-Aware Speculative Verification 有效地降低了因低决策性带来的回滚成本与无效拒绝，具有良好的可扩展性和通用性，适用于现有的目标耦合的 Speculative Decoding 架构。

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [56] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: 对志愿者湖泊监测数据进行缺失值填充与跨湖预测比较，发现岭回归在最少样本和特征下仍能实现接近全历史基线的5%误差，通过联合可行性函数给出最小采样与特征组合的实用规则。


<details>
  <summary>Details</summary>
Motivation: 在冰冻/天气等导致缺失且存在人为错误的时间序列中，如何实现可泛化、低成本的Secchi Disk Depth预测，以支持对有害藻华的预警和监测决策。

Method: 缺失采用MICE进行多重插补；对六个候选模型进行nMAE评估，跨湖比较，最终选取岭回归；通过回溯历史与最近观测的组合，量化最小样本量；研究四特征子集在与13特征基线相同的5%容差内的表现；提出一个联合可行性函数以同时优化最近历史长度和特征数量以达到5%目标。

Result: 岭回归在平均测试性能上最佳；达到5%容差所需约176个样本/湖（基于完整历史）。四特征子集可在同样容差下等效于13特征基线；在仅需约64个最近样本和1个预测变量的条件下也能实现目标。

Conclusion: 联合可行性框架为设定采样投入与测量优先级提供简明规则，便于数据受限的志愿者监测场景下的水质预测与早期警报应用。

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [57] [SAGE-FM: A lightweight and interpretable spatial transcriptomics foundation model](https://arxiv.org/abs/2601.15504)
*Xianghao Zhan,Jingyu Xu,Yuanning Zheng,Zinaida Good,Olivier Gevaert*

Main category: cs.LG

TL;DR: Proposes SAGE-FM, a lightweight GCN-based spatial transcriptomics foundation model trained with masked central spot prediction on 416 human Visium samples; demonstrates superior embeddings for clustering, preservation of biological heterogeneity, and downstream task performance; captures directional regulatory effects, suggesting simple GCNs as interpretable foundations for spatial transcriptomics.


<details>
  <summary>Details</summary>
Motivation: The need for scalable, interpretable spatial transcriptomics foundation models that capture spatially conditioned regulatory relationships beyond existing methods like MOFA; desire for parameter-efficient models that perform well in unsupervised and downstream tasks.

Method: A graph convolutional network (GCN) trained with a masked central spot prediction objective. Training data: 416 human Visium samples across 15 organs. Evaluation includes unsupervised clustering, preservation of biological heterogeneity, downstream task performance (pathologist-defined spot annotation in OPSCC, glioblastoma subtype prediction), and in silico perturbation to assess directional regulatory effects.

Result: Embeddings recover 91% of masked genes with significant correlations (p<0.05). Outperforms MOFA and existing spatial methods in clustering and heterogeneity preservation. Generalizes to downstream tasks with 81% accuracy in OPSCC spot annotation and improved glioblastoma subtype prediction vs MOFA. In silico perturbations reveal directional ligand-receptor and upstream-downstream effects consistent with ground truth.

Conclusion: Shows that simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics.

Abstract: Spatial transcriptomics enables spatial gene expression profiling, motivating computational models that capture spatially conditioned regulatory relationships. We introduce SAGE-FM, a lightweight spatial transcriptomics foundation model based on graph convolutional networks (GCNs) trained with a masked central spot prediction objective. Trained on 416 human Visium samples spanning 15 organs, SAGE-FM learns spatially coherent embeddings that robustly recover masked genes, with 91% of masked genes showing significant correlations (p < 0.05). The embeddings generated by SAGE-FM outperform MOFA and existing spatial transcriptomics methods in unsupervised clustering and preservation of biological heterogeneity. SAGE-FM generalizes to downstream tasks, enabling 81% accuracy in pathologist-defined spot annotation in oropharyngeal squamous cell carcinoma and improving glioblastoma subtype prediction relative to MOFA. In silico perturbation experiments further demonstrate that the model captures directional ligand-receptor and upstream-downstream regulatory effects consistent with ground truth. These results demonstrate that simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics.

</details>


### [58] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: 通过使用临床测试和MRI特征的机器学习方法，区分非典型AD（atAD）与非AD认知障碍，结合NACC/ADNI多数据集，显著提升atAD的召回率（NACC从52%增至69%，ADNI从34%增至77%），并利用Boruta识别关键脑区，优于仅用海马体体积的方法，具有临床可行性。


<details>
  <summary>Details</summary>
Motivation: 非典型AD易被误诊；现有基于认知测试和海马体积的诊断在atAD中敏感性不足；需要在标准临床数据和MRI上实现非侵入式、可扩展的诊断改进。

Method: 在1410名受试者（包括tAD、atAD、非AD、正常对照）上，比较基线海马体积与全面的MRI特征集合的诊断性能，构建机器学习模型；融合公开数据集与私有数据集；使用Boruta进行特征选择和可视化关键脑区。

Result: 综合MRI特征优于仅用海马体积，atAD召回率显著提升；NACC 52%→69%，ADNI 34%→77%，保持较高精确度。

Conclusion: 将临床测试与广泛MRI特征结合的ML方法，为临床在非典型atAD的诊断提供非侵入且可扩展的解决方案，帮助提高诊断准确性。

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [59] [QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs](https://arxiv.org/abs/2601.15538)
*Himanshu Mishra,Kanwal Mehreen*

Main category: cs.LG

TL;DR: 针对低比特量化对已忘记信息的破解脆弱性，提出量化感知的遗忘去除方法：通过对比权重更新与量化阈值，发现常规去忘更新不足以跨越阈值；引入 logits 空间的 hinge 损失，保证忘记样本在量化后仍可区分。实验证明在 4 位量化下，该方法能保持遗忘性，而现有方法几乎无法保持。


<details>
  <summary>Details</summary>
Motivation: 在模型部署中常使用低位量化以减小模型体积和推理成本，然而低比特量化可能将已遗忘的信息重新显现。需要一种在量化环境下仍能维持遗忘的去忘方法，确保隐私与版权保护。

Method: 1) 分析权重改变量统计与量化桶重叠，证实常规去忘更新不足以跨越量化阈值。2) 提出 logits 空间的 hinge 损失：对每个需遗忘的样本，强制未去忘模型的输出 logits 与原模型之间的距离至少一个 Margin（取量化步长的一半）。3) 以此损失对模型进行优化，使得在量化后仍保持遗忘样本的可区分性。

Result: 在语言和分类任务（包括 Twitter 虚假信息数据集）上评估，4 位量化下该方法能维持遗忘性，而现有方法几乎完全恢复遗忘信息。

Conclusion: 量化感知的去忘机制通过在 logits 空间设定半步量化距离的 margin，有效提升在低比特量化下的遗忘鲁棒性，适用于语言与分类任务的场景。

Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.

</details>


### [60] [PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction](https://arxiv.org/abs/2601.15540)
*Dongchen Huang*

Main category: cs.LG

TL;DR: 提出 Prism，一个基于 MCR^2 的白盒注意力架构，通过在信号-噪声流形上进行梯度上升来实现可解释性和性能提升，头部分化为低/高频两类以捕捉长期依赖与局部约束。


<details>
  <summary>Details</summary>
Motivation: 解决 Transformer 的黑箱性问题，利用几何/物理约束实现自监督的功能解耦和可解释性提升。

Method: 将注意力建模为对信号-噪声流形上的梯度上升过程，引入过度完备字典和 π-RoPE 的频率分离以强制子空间去相关性；以 TinyStories 为测试床验证光谱动态。

Result: 头部自发分化为光谱上不同的角色：低频头捕捉长程因果， 高频头处理局部句法；在不损失性能的前提下提升可解释性。

Conclusion: 通过几何先验实现可解释性与性能的统一，而非权衡。

Abstract: Deep learning models, particularly Transformers, are often criticized as "black boxes" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.

</details>


### [61] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++ 通过引入两种漂移检测（基于熵和 KL 散度）以及自适应重置，扩展 RDumb，在长序列的连续测试时自适应中保持稳定并提升约 3% 的绝对准确率。


<details>
  <summary>Details</summary>
Motivation: 在持续测试时，模型需要在未标签数据流中自适应；现有方法在快速或长期漂移下易崩溃，CCC 基准显示了长时间、不断变化的腐蚀类型的挑战；需要漂移感知的重置机制以防止预测崩溃。

Method: 在 RDumb 基础上增加两种漂移检测：熵基漂移分数和 KL 散度漂移分数，并引入自适应重置策略；当累积自适应变得有害时检测并恢复，通过对漂移阈值和重置强度的消融验证其重要性。

Result: 在 CCC-medium（3 种速度、3 个随机种子，共 9 次跑，每次 1M 样本）上，RDumb++ 相较 RDumb 获得约 3% 的绝对准确率提升，并在整个数据流中保持稳定的自适应；漂移阈值和重置强度的消融实验表明漂移感知重置对防止崩溃和实现长期 CTTA 的可靠性至关重要。

Conclusion: 漂移检测结合自适应重置是实现长期、稳定的 CTTA 的关键，RDumb++ 提供了对持续漂移的鲁棒适应能力。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [62] [Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance](https://arxiv.org/abs/2601.15546)
*Charles B. Delahunt,Courosh Mehanian,Daniel E. Shea,Matthew P. Horning*

Main category: cs.LG

TL;DR: 用临床相关指标进行优化比以验证集损失为优化目标，在临床任务上表现更优；需要额外努力来定义和编码这些指标。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习以验证集损失为优化目标，但在医疗场景中，目标应对临床需求负责。临床需求可用专门化指标来更精准地表达，从而允许非微分优化等更广泛的方法。

Method: 通过两组对照实验，比较以临床相关指标优化与以验证集损失优化的模型。若干优化任务允许非微分的指标，因此将定制指标编码进入训练管线，评估对临床任务的影响。

Result: 结果显示，使用临床相关指标进行优化的模型在临床任务上的表现优于以验证集损失为目标的模型，体现了指标设计对模型选择与早停等优化环节的影响。

Conclusion: 临床相关指标的优化能更好地实现医疗领域的核心目标，尽管需要额外的工作量来定义和集成这些指标，但对提升临床性能具有显著价值。

Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.

</details>


### [63] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 提出第一套系统框架从部分观测中学习神经算子，解决部分观测下的监督缺口与空间错位问题，包含掩码到预测训练和物理感知的潜在传播器；并提出专用基准 POBench-PDE。结果在包括气候预测的多项任务上实现 18-69% 相对 L2 误差下降，且在缺失率低于 50% 的情况下表现显著，在高达 75% 缺失率下仍具一定鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的观测数据因传感器、地理与成本等原因常常不完整，现有神经算子多假设全观测，限制实际应用。需要同时解决两大核心难题：一是未观测区域的监督缺口导致物理相关性难以学习；二是未观测输入与完整解场之间的动态空间错位。

Method: 提出 Latent Autoregressive Neural Operator (LANO)，包含两大组成：1) mask-to-predict 训练策略，通过策略性掩蔽观测区域来制造人为监督；2) Physics-Aware Latent Propagator，在潜在空间中进行边界优先的自回归生成以重建解场。此外，提出 POBench-PDE 作为专用于在部分观测条件下评估神经算子的基准，覆盖三类 PDE 任务。

Result: 在 patch-wise missingness 条件下、缺失率<50%的场景中，方法实现相对 L2 误差下降18%~69%，并覆盖真实世界气候预测任务。方法对高达75%缺失率也在一定程度上具备鲁棒性，达到 state-of-the-art 水平。

Conclusion: 该工作系统性地将缺失观测条件下的神经算子学习问题提上日程，通过掩码-预测策略与潜在传播器等设计，显著缓解监督缺口与空间错位两大核心挑战，并提供专用基准以促进未来研究。但在不同缺失模式的鲁棒性、边界条件的广泛适应性以及可重复性方面仍有待进一步验证与提升。

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [64] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: 提出端到端深度学习策略用于带有限时的易腐品库存，比较纯黑箱、结构引导的政策以及通过 boosting 的改进版本，证明引入领域结构能显著提升学习效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在随机补货时间和未知需求/延迟分布、数据有限的场景下，需利用有限历史数据、观测协变量与系统状态自适应地学习订购策略，以降低缺货与浪费。

Method: 引入 marginal cost accounting，为每次订货分配单一生命周期成本，构建统一的端到端学习损失。提出两种端到端模型：E2E-BB（纯黑箱直接输出订货量）和 E2E-PIL（嵌入投影库存水平的结构化策略）。并基于同质性（齐次性）一阶性质，利用 ODA boosting 提出增强版 E2E-BPIL。

Result: 实验证明：E2E-BB < E2E-PIL < E2E-BPIL。在合成与真实数据上，嵌入结构降低模型复杂度、提升学习效率，数据不足时表现更鲁棒。使用 excess-risk 分解显示结构化嵌入减少灵活性成本、提升学习效率。

Conclusion: 在易腐品库存问题上，深度学习决策工具若结合人类知识和库存理论，能更有效且鲁棒地实现端到端优化，强调将高级分析与库存理论相融合的重要性。

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [65] [When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards](https://arxiv.org/abs/2601.15609)
*Mingyuan Fan,Weiguang Han,Daixin Wang,Cen Chen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: RLVR can induce over-sharpening, collapsing policies onto limited sampling modes due to finite-batch updates; proposed mitigations—inverse-success advantage calibration and memory-network–based distribution calibration—improve generalization.


<details>
  <summary>Details</summary>
Motivation: To determine whether RLVR yields genuinely new capabilities or merely concentrates existing knowledge, the study formalizes over-sharpening, analyzes its drivers (finite-batch sampling bias and semantic coupling), and seeks solutions to preserve diversity and reliability in LLM-based problem solving.

Method: Formalize the over-sharpening phenomenon, show how finite-batch updates bias learning toward sampled modes and propagate through semantic coupling; propose two mitigation strategies: (1) inverse-success advantage calibration to emphasize difficult queries, and (2) distribution-level calibration via a memory network to diversify sampling; validate with empirical experiments.

Result: The proposed strategies mitigate mode collapse and improve generalization, demonstrating that addressing finite-batch biases and distributional diversity enhances RLVR reliability.

Conclusion: Over-sharpening is a risk in RLVR due to finite-batch updates, but carefully designed calibration and memory-based diversification can restore broader generalization without sacrificing performance.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.

</details>


### [66] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 对1-identification问题给出新的期望拉动次数下界及紧界上界。基于优化建模推导在存在至少一个合格臂时的下界，并设计新算法，得到与下界之差为对数项多项式因子的上界，补充了多臂存在合格臂情形的研究空白。


<details>
  <summary>Details</summary>
Motivation: 在纯探索的多臂带收益识别任务中，给定阈值μ0，判断是否存在平均收益≥μ0的臂，或输出None；要求在误差概率≤δ下正确性保证，同时尽量降低期望总拉动次数Eτ。当前工作关注1-identification中的下界与算法设计，填补对至少一个合格臂情形的理论空白，并为后续多臂情形分析提供基准。

Method: 通过优化问题建模（可能涉及信息学/KL散度等工具）推导存在至少一个合格臂时的Eτ下界；设计新算法并对其性能进行严格分析，给出紧界，与下界之间的差距为多项式对数因子的量级。

Result: 给出在至少存在一个合格臂的情形下的Eτ下界；提出新算法并给出紧致的上界，二者之间的差距在对数的多项式量级内（即相差一个多项式对数因子）。该结果与多臂存在多个合格臂的情形分析是互补的，后者仍是历史文献中的开放问题。

Conclusion: 工作为1-identification提供新的下界与紧界算法，弥补了关于存在单个或多个合格臂情形的理论空白，建立了可比较的理论基准，并指出了未来在多合格臂情形下的仍待解决的问题。

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [67] [An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types](https://arxiv.org/abs/2601.15640)
*Natasha Trinkle,Huong Ha,Jeffrey Chan*

Main category: cs.LG

TL;DR: Empirical study of ensemble-based transfer learning for Bayesian optimisation; introduces positive-weight ensemble and a fallback for non-improving transfer, plus warm-start initialisation; reports two main performance boosters using new benchmarks.


<details>
  <summary>Details</summary>
Motivation: Leverage related historical data to improve Bayesian optimisation and understand which pipeline components drive gains; address cases where transfer learning does not help; provide real-time benchmarks for evaluation.

Method: Empirical evaluation of multiple ensemble-based transfer-learning Bayesian optimisation components; propose a regularised-regression-based weighting for ensemble surrogates with positive weights; include a mechanism for when transfer learning is not beneficial; introduce three new real-time transfer-learning Bayesian optimisation benchmarks.

Result: Positive-weighted ensemble surrogate integration and warm-start initialisation consistently improve performance; the proposed regularised regression weighting and a non-improving-transfer fallback contribute to robustness; new benchmarks enable realistic evaluation.

Conclusion: Warm-start initialisation and non-negative ensemble weights are effective design choices for robust ensemble transfer-learning in Bayesian optimisation; the aggregation strategy and fallback mechanism should be integrated for practical deployment, especially when transfer signals are weak or deceptive.

Abstract: Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.

</details>


### [68] [Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting](https://arxiv.org/abs/2601.15669)
*Jingjing Bai,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Dualformer 提出一个双域 Transformer 框架用于长期时间序列预测，通过 dual-branch 时空-频域建模、层级频率采样及周期性权重自适应融合，有效缓解高频信息在多层传播中的衰减，在八个基准数据集上展现优越性能并对异质/弱周期数据更鲁棒；代码公开。


<details>
  <summary>Details</summary>
Motivation: 在长时序预测任务中，Transformer 容易产生低通效应，未区分地跨层传播频率分量，导致高频信息逐层被削弱，难以捕捉细粒度的时间变化；需实现更结构化的时频建模以提升泛化。

Method: 提出 Dualformer：(1) 双分支架构同时在时间域和频域建模互补的时序模式；(2) 层级频率采样模块，在不同层分配不同频带，保留低层高频细节、深层建模低频趋势；(3) 基于输入的谐波能量比动态权衡两支的贡献的周期性感知权重机制，并给出理论下界支持；实现对时频特征的结构化建模与自适应融合。

Result: 在八个广泛使用的基准上进行大规模实验，Dualformer 表现出鲁棒性和优越性能，尤其在异质或弱周期数据上效果显著。

Conclusion: 该框架实现了结构化的时频建模与自适应融合，有效保留高频信息、提升泛化能力，对长时序预测具有显著提升潜力；代码公开。

Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.

</details>


### [69] [Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing](https://arxiv.org/abs/2601.15686)
*Xinyu Wang,Sicheng Lyu,Yu Gu,Jerry Huang,Peng Lu,Yufei Cui,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: 提出 RLSEdit，一种递归最小二乘编辑器，用于对大语言模型进行长序列编辑，支持在线更新、保持先前编辑及整体能力的稳定性


<details>
  <summary>Details</summary>
Motivation: 解决持续流式编辑场景中的塑性-稳定性矛盾：硬写式编辑易累积干扰，硬保留式方法难以保护未约束行为，导致多次编辑后能力下降

Method: 将编辑建模为带软约束的在线二次优化，最小化累计的键值拟合目标，并引入对偏离预训练权重和锚映射的两类正则化；通过 Woodbury 逆公式实现在线递推更新，单次编辑成本与历史长度无关，仅与当前编辑规模相关；给出偏差界与多编辑情形下的遵循—保持权衡的渐近刻画

Result: 在多模型家族的实验中，RLSEdit 可稳定扩展到1万次编辑，编辑成功率与整体稳定性均优于强基线，能保留早期编辑且在 GLUE 和推理/代码基准等保持通用能力

Conclusion: RLSEdit 提供可扩展且稳定的长序列编辑框架，理论上界定偏离与保持的权衡，实证上在多任务/多模型场景中展现显著优势

Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit "hard writes" can accumulate interference over time, while null-space-style "hard preservation" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

</details>


### [70] [Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs](https://arxiv.org/abs/2601.15714)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 提出零错误视界ZEH作为LLM可靠性诊断指标，尽管简单，评估显示甚至强大模型在基本算法任务上也会犯错；ZEH与总体准确性相关，但行为特征不同，且计算成本较高，文中提出通过树状结构和online softmax实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景下需要可信的推理能力；现有评估受限，需一个能揭示算法性能力与错误范围的简单量化指标。

Method: 定义ZEH为模型在无错误情况下可正确解决问题的最大范围；对GPT-5.2与Qwen2.5进行ZEH评估，测试如二进制串奇偶性、括号平衡等基本任务，分析ZEH与准确率的相关性及行为差异，并讨论提高计算效率的策略（树状结构、在线softmax）

Result: GPT-5.2在11000的奇偶性和括号平衡等简单任务上均出现错误，ZEH与准确率相关但表现出不同的错误模式；ZEH为理解模型的算法能力演化提供线索；提出的加速策略可将计算成本降低至一个数量级。

Conclusion: ZEH是一个有价值的诊断指标，有助于在安全关键领域对LLMs进行更细粒度的行为评估与改进，尽管计算成本较高，但通过结构化加速方法可行。

Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.

</details>


### [71] [Communication-efficient Federated Graph Classification via Generative Diffusion Modeling](https://arxiv.org/abs/2601.15722)
*Xiuling Wang,Xin Huang,Haibo Hu,Jianliang Xu*

Main category: cs.LG

TL;DR: CeFGC is a three-round federated GNN framework that uses diffusion models to generate synthetic graphs, reducing server–client communication and improving performance on non-IID data.


<details>
  <summary>Details</summary>
Motivation: Federated GNNs face high communication costs and non-IID data across clients. There is a need to reduce communication rounds while maintaining or improving model performance on decentralized graph data.

Method: Each client trains a local diffusion model to capture its graph distribution and shares the generative model with the server. The server redistributes the models to all clients. Clients generate synthetic graphs from these models (and combine them with local graphs) to train local GNNs. Finally, clients upload their GNN weights to the server for aggregation into a global model. The authors also provide a theoretical IO complexity analysis showing a constant three communication rounds.

Result: Experiments on real graph datasets show that CeFGC achieves superior performance on non-IID graphs compared to state-of-the-art methods, validating its effectiveness and efficiency. The approach aligns local and global objectives and enriches training with diverse synthetic graphs.

Conclusion: CeFGC successfully reduces communication to three rounds and lowers I/O complexity while delivering strong performance on non-IID graph data, demonstrating the viability of diffusion-model–based data synthesis for FGNNs.

Abstract: Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.

</details>


### [72] [Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning](https://arxiv.org/abs/2601.15771)
*Dong Xu,Jiantao Wu,Qihua Pan,Sisi Yuan,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 将DDI预测问题从药物-药物对齐视为关系学习任务，提出GenRel-DDI在药物身份无关的关系表示层面学习可迁移的交互模式，显著提升对未见药物和新药对的泛化能力，尤其在严格实体不重叠的评估中表现突出，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现实部署场景下，候选药物对中大多数涉及未见药物且验证交互稀缺；现有药物中心的嵌入对交互标签的对应性差，简单扩大模型容量不能改善泛化。需要可迁移的关系级表示以提升鲁棒性。

Method: 将DDI预测重构为关系学习问题，学习独立于药物身份的交互表示；通过关系级抽象捕捉可迁移的交互模式，使模型对未见药物和新药对具备泛化能力；在多项基准与严格实体不重叠设置中进行比较。

Result: GenRel-DDI在各基准上持续且显著超越SOTA方法，且在严格实体不重叠评估中获得尤为显著的增益；代码开源。

Conclusion: 关系学习是实现鲁棒DDI预测的有效路径，尤其在高泛化需求场景，具有实际应用价值。

Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.

</details>


### [73] [Next Generation Active Learning: Mixture of LLMs in the Loop](https://arxiv.org/abs/2601.15773)
*Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Guoxiang Guo,Joanne Enticott,Gang Liu,Lan Du*

Main category: cs.LG

TL;DR: 提出一种在主动学习中替代人工标注的混合大模型注释框架（Mixture of LLMs in the Loop Active Learning，MoLLA），通过多模型集成注释提高鲁棒性，并引入注释不一致性与负学习以抑制噪声标签。实验表明与人工标注相当，优于单一LLM与其他LLM集成方法；且在本地可运行，适用于实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决主动学习中高昂的人工标注成本及由LLM注释所带的噪声问题；通过多LLM集成提升标注质量的鲁棒性，同时实现本地化、低资源的部署需求。

Method: 在主动学习循环中引入一个Mixture-of-LLMs注释模型来生成标签，并对多模型输出进行聚合以提高鲁棒性。提出注释不一致性（annotation discrepancy）度量以识别不可靠标签，并结合负学习（negative learning）抑制噪声标签对模型的负面影响。该框架仍以轻量级LLMs为基础，确保可在本地机进行全流程部署。

Result: 大量实验表明该框架的性能接近人类标注，并持续优于单LLM基线和其他LLM集成方法。由于使用轻量级LLMs，可在本地机器上运行，具备良好的实用性。

Conclusion: MoLLA框架在LLM驱动的主动学习场景中有效提升标注质量鲁棒性、降低对人工标注的依赖，并具备本地部署能力，具备一定的现实落地潜力。

Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

</details>


### [74] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: GOSV 通过对所有头部的全局优化识别安全相关向量；采用 Harmful Patching 与 Zero Ablation 两种补丁策略，发现两组低重叠的安全向量（Malicious Injection Vectors 与 Safety Suppression Vectors）；约 30% 的头部被重补丁时即可导致安全崩溃；提出基于识别向量的推理时白盒越权攻击，显著优于现有白盒攻击，提升了 LLM 安全的可解释性与攻击性评估。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估多依赖局部、贪婪归因，忽视跨头部的协同作用，导致对安全机制的理解不充分；需要一个能在全局层面识别与评估安全向量及其相互影响的框架。

Method: 提出 GOSV：在所有注意力头上进行全局优化以识别安全关键向量；通过 Harmful Patching 与 Zero Ablation 两种激活重补丁策略，识别两组空间上彼此分离且重叠很低的安全向量；将向量分为 Malicious Injection Vectors 与 Safety Suppression Vectors；进行系统实验与对比分析，并给出基于向量的白盒推理时攻击方法。

Result: 证明对安全相关向量存在明确的两条互补通道，且其功能独立性较高；在约 30% 的头部被重补丁时，模型安全性崩溃；提出的推理时间白盒攻击显著超过现有白盒方法，验证了 GOSV 在可解释性与攻击性评估方面的有效性。

Conclusion: GOSV 为 LLM 安全解释性提供了新的视角，揭示了安全机制的分离性通路并提示在设计更鲁棒的安全架构时需考虑多头协同与全局安全向量的系统性评估，同时也提醒研究者警惕安全向量被 exploit 的潜在风险。

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [75] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: Post-training decomposition reveals stable, independent substructures in neural networks, enabling structured, parallel inference without changing model behavior.


<details>
  <summary>Details</summary>
Motivation: Inference costs in large dense parameter matrices scale with model size due to treating post-training systems as monolithic operators. The observation that gradient updates are localized suggests many dependencies resemble initialization, implying potential decomposition.

Method: Introduce a post-training statistical criterion to prune unsupported dependencies and apply a structural annealing procedure that reveals and groups stable, independent substructures. The approach is model-agnostic and does not modify functionality or interfaces.

Result: A post-training, model-agnostic structural view of inference systems is established, enabling structured, parallel inference by removing unsupported dependencies and exposing independent substructures.

Conclusion: Post-training analysis can transform inference from dense monoliths to decomposed, parallelizable substructures without retraining or altering models, offering scalable inference paths.

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [76] [Iterative Amortized Hierarchical VAE](https://arxiv.org/abs/2601.15894)
*Simon W. Penninga,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: 提出了一种迭代性摊销层次变分自编码器（IA-HVAE），在变换域（如傅里叶域）实现线性可分解解码器，结合初始摊销推断与梯度迭代更新，显著提升推断速度并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决层次化VAE中摊销推断的速度-精度权衡，通过混合策略在保证较深模型的实时性同时提升推断准确性；利用变换域的线性可分解解码器实现高效迭代更新。

Method: 提出 IA-HVAE 架构：在解码器变换域实现线性可分解，采用初始摊销猜测并通过解码梯度进行迭代更新以 refinement latent；在像 Fourier 这样的域中实现线性可分解的解码器以实现高深度模型的实时性。

Result: 相较于传统 HVAE，迭代推断实现约 35x 的加速；混合策略在准确性和速度方面优于完全摊销和完全迭代的对应方法；在反问题（去模糊、去噪）中对普通 HVAE 有更好的重建质量。

Conclusion: 混合的 IA-HVAE 在准确性与推断速度之间取得更优的折中，变换域中的线性解码器使得深层模型的实时应用成为可能，且对逆问题的重建性能有明显提升。

Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.

</details>


### [77] [Predicting Healthcare System Visitation Flow by Integrating Hospital Attributes and Population Socioeconomics with Human Mobility Data](https://arxiv.org/abs/2601.15977)
*Binbin Lin,Lei Zou,Hao Tian,Heng Cai,Yifan Yang,Bing Zhou*

Main category: cs.LG

TL;DR: Integrated modeling of hospital attributes, SES, and mobility to predict visitation flows; Deep Gravity best; findings show capacity, occupancy, ratings, popularity, SES, and demographic factors modulate visitation with distance-dependent patterns.


<details>
  <summary>Details</summary>
Motivation: Fill the gap of analyzing determinants of healthcare visitation as an integrated system rather than in isolation by combining hospital characteristics with population SES and spatial mobility.

Method: Utilize four years of SafeGraph mobility data and Google Maps Reviews; compare five models (Naive Regression, Gradient Boosting, Multilayer Perceptrons, Deep Gravity, HGNN); apply SHAP and PDP for interpretability; Houston, TX case study.

Result: Deep Gravity outperforms other models; hospital capacity, ICU occupancy, ratings, and popularity significantly shape visitation flows; effects vary by travel distance; short-distance visits driven by convenience, long-distance by ratings; demographic and SES composition modulate visitation patterns.

Conclusion: Demonstrates the value of integrated determinants in predicting hospital visitation; findings inform hospital planning, accessibility, and equity considerations across different areas and distances.

Abstract: Healthcare visitation patterns are influenced by a complex interplay of hospital attributes, population socioeconomics, and spatial factors. However, existing research often adopts a fragmented approach, examining these determinants in isolation. This study addresses this gap by integrating hospital capacities, occupancy rates, reputation, and popularity with population SES and spatial mobility patterns to predict visitation flows and analyze influencing factors. Utilizing four years of SafeGraph mobility data and user experience data from Google Maps Reviews, five flow prediction models, Naive Regression, Gradient Boosting, Multilayer Perceptrons (MLPs), Deep Gravity, and Heterogeneous Graph Neural Networks (HGNN),were trained and applied to simulate visitation flows in Houston, Texas, U.S. The Shapley additive explanation (SHAP) analysis and the Partial Dependence Plot (PDP) method were employed to examine the combined impacts of different factors on visitation patterns. The findings reveal that Deep Gravity outperformed other models. Hospital capacities, ICU occupancy rates, ratings, and popularity significantly influence visitation patterns, with their effects varying across different travel distances. Short-distance visits are primarily driven by convenience, whereas long-distance visits are influenced by hospital ratings. White-majority areas exhibited lower sensitivity to hospital ratings for short-distance visits, while Asian populations and those with higher education levels prioritized hospital rating in their visitation decisions. SES further influence these patterns, as areas with higher proportions of Hispanic, Black, under-18, and over-65 populations tend to have more frequent hospital visits, potentially reflecting greater healthcare needs or limited access to alternative medical services.

</details>


### [78] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: k-lazyGD 在 SOCO 框架下实现了可懒性更新而不损失击中性能，动态回报上界为 O(sqrt((P_T+1)T))，适用于 laziness slack k up to Θ(sqrt(T/P_T))；通过 FTRL 框架分析并给出匹配下界，提出含多 Slack 的集成学习器以在稳定性与灵活性之间自适应折中。


<details>
  <summary>Details</summary>
Motivation: 在包含 hitting 成本和移动成本的 SOCO 场景中，研究从贪婪 OGD 到惰性/对偶平均的更新之间的谱系，探究在保持更新稳定性的同时提升追踪能力的可行性。

Method: 提出 k-lazyGD，并以 Follow the Regularized Leader (FTRL) 框架分析动态 regret；给出对比较路径长度 P_T 的依赖关系下的上界，并推导与之匹配的下界；设计一个含多种 slack 的集成学习器以实现“在可能时稳定、在必须时敏捷”的策略。

Result: 证明 laziness 可以在不牺牲击中性能的前提下实现，动态 regret 达到最优量级 O(sqrt((P_T+1)T))，对任意 laziness slack k 上界为 Θ(sqrt(T/P_T)) 时成立；给出匹配的下界；提出的多 slack 集成方法在不同 P_T 下具备稳定性与灵活性的自适应性。

Conclusion: k-lazyGD 将贪婪与惰性更新的谱系理论化，证明在 SOCO 中能够在保持低移动成本的同时维持良好追踪能力；通过 FTRL 分析与多 slack 集成实现理论与实践的自适应折中。

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [79] [Data-Driven Conditional Flexibility Index](https://arxiv.org/abs/2601.16028)
*Moritz Wedemeyer,Eike Cramer,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 提出条件灵活性指数CFI，通过学习参数化可允许不确定性集合并结合上下文信息，使不确定性集合条件化，使用正则化流学习高斯到数据分布的映射，在潜在空间构建超球体并映射回数据空间，应用于安全约束单元承诺等场景，能在某些条件下提升调度质量。


<details>
  <summary>Details</summary>
Motivation: 传统灵活性指数使用简单集合（如超立方体）近似不确定性区域，忽略来自预测/上下文信息的条件信息，导致潜在保守或误导的评估。

Method: 使用正则化流（normalizing flow）学习从高斯基分布映射到数据分布，潜在空间使用超球体作为可允许不确定性集合，结合上下文信息将集合条件化。

Result: 通过示例说明数据驱动的集合是否优于简单集合、条件集合是否优于无条件集合并非普遍成立，但两者都确保只考虑不确定参数空间中包含实现的区域。应用于安全约束型机组调度，CFI可通过引入时序信息提升调度质量。

Conclusion: CFI提供一种数据驱动和条件化的灵活性评估框架，能更有信息地界定可接受不确定性区域，并在需要时提升实际调度决策质量。

Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.

</details>


### [80] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: 提出 CLASP 算法解决约束在线凸优化中的损失与平方约束违例问题，利用凸投影的非扩张性，给出一般凸与强凸情形的渐进界。


<details>
  <summary>Details</summary>
Motivation: 在在线学习框架中，除了最小化累积损失，还需处理约束违背的惩罚。现有方法受限于对投影操作的利用不足，未能同时获得损失和约束的良好渐进界。需要在强凸情形下实现对数阶保证，并在一般凸情形给出可控的折中。

Method: CLASP，通过对损失与平方违约的目标进行联合最小化，充分利用凸投影的非扩张性来分析；在一般凸情形给出折中： regret = O(T^{max{β,1−β}})，累积平方违约 = O(T^{1−β})，对任意 β∈(0,1)。在强凸情形给出对数阶界： regret = O(log T)，累积平方违约 = O(log T)。

Result: 一般凸：损失 regret O(T^{max{β,1−β}})，平方违约 O(T^{1−β})；强凸：损失和平方违约均为 O(log T)。

Conclusion: 首次在强凸情形实现对数阶的总损失与平方约束违约的保证；通过利用投影的非扩张性提升对约束项的控制，方法在已有框架上实现更强的渐进界。

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


### [81] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 通过对工业CPS中的时间序列预测模型应用SHAP等XAI方法，揭示训练中上下文信息不足的问题，并通过增大数据窗口实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统对安全性和经济性高度敏感，ML模型常缺乏透明性，需严格评估与解释以避免对未来数据的意外行为。

Method: 对时间序列分解组件进行XAI分析（使用SHAP值），评估各组成部分对预测的贡献，观察窗口大小对模型表现的影响，并据此扩展输入上下文。

Result: 研究表明存在缺乏足够上下文信息的证据；通过增加数据窗口大小，结合XAI发现，模型性能得到提升。

Conclusion: 将XAI洞察用于特征工程（扩大时间上下文）可提升工业CPS中的预测模型表现；需要在训练阶段更充分地考虑上下文信息，并将XAI融入模型设计与评估流程。

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


### [82] [Probably Approximately Correct Maximum A Posteriori Inference](https://arxiv.org/abs/2601.16083)
*Matthew Shorvon,Frederik Mallmann-Trenn,David S. Watson*

Main category: cs.LG

TL;DR: 提出了面向MAP推断的PAC算法，在固定或可控的计算预算内提供带有严格保证的近似最优解；通过信息论度量来刻画可判定性，并使用概率电路实现和随机化策略，实验在多组基准上证明了效果。


<details>
  <summary>Details</summary>
Motivation: MAP推断在通常结构约束下通常不可计算，且在多种近似方案下缺乏显式理论保证，因此需要在计算预算内提供可验证的最优解近似。

Method: 提出PAC-MAP框架，利用信息论可估计度量来界定有限样本下的可行性；以概率电路作为实现平台，设计随机化策略使MAP求解在预算内获得带有理论保证的解；可作为独立MAP推断方法或用于改进现有启发式。

Result: 在多组基准上验证，表明在给定预算下可获得带有严格保证的解，随机化策略还能提升对比启发式的性能。

Conclusion: PAC-MAP提供了计算效率与解质量之间的可控权衡，适用于广义的概率推断场景，具有理论保证与实用性。

Abstract: Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.

</details>


### [83] [Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets](https://arxiv.org/abs/2601.16107)
*Adithya Sineesh,Akshita Kamsali*

Main category: cs.LG

TL;DR: 对三种开放数据集上五种深度学习Raman谱分类模型进行统一训练/超参数调优的系统基准，比较三 dataset 的性能，给出准确率和宏F1分数。


<details>
  <summary>Details</summary>
Motivation: 目前将深度学习模型用于Raman光谱分类的评估常分散、缺乏跨模型的公平对比，难以判断哪类模型在共享数据集上更具鲁棒性和泛化性。

Method: 选择五个具代表性的Raman-specific DL架构，在统一的训练与超参数调整协议下，对三组开源Raman数据集进行评估，并包含分布偏移测试。报告分类准确率和宏平均F1分数。

Result: 在不同数据集上，五个模型的相对表现存在差异，且没有一款模型在所有数据集上占优；统一评测框架实现了可重复、可比性强的比较，形成可用于模型选择的基准资源。

Conclusion: 这是面向Raman光谱分析的、在开放数据集上的多模型系统性基准研究之一，能促进公平比较、方法选择和未来数据集的评估标准制定。

Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.

</details>


### [84] [On the Intrinsic Dimensions of Data in Kernel Learning](https://arxiv.org/abs/2601.16139)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: 本工作在核岭回归(KRR)框架下对两种内在维度进行比较：d_ρ（基于核诱导度量的上休默维度）与 d_K（由Kolmogorov n-宽度衰减得到的有效维度）。通过研究核的积分算子特征值与n-宽度之间的关系，给出对任意分布μ在Ω上的最坏情形的上界，并推导出在样本量n充分大时的经验误差上界为 O(n^{ -(2+d_K)/(2+2d_K) + ε })。提出从有限样本估计n-宽度上界的算法，并给出近似均匀分布下的样本复杂度结果：用高概率在 O(ε^{-d_ρ} log(1/ε)) 次采样即可获得 ε-精确的全部n-宽度上界。还对分形集合计算了 d_K，并给出数值实验。结论是，对某些核（如Laplace核），d_K 可以显著小于 d_ρ，即使在正则域上 d_K = d_ρ 的关系也成立；这表明 d_K 可能为 KRR 的泛化性能提供更紧的无偏信息。


<details>
  <summary>Details</summary>
Motivation: 理解核回归在低内在维度场景下的泛化性能，比较两种内在维度（d_ρ 与 d_K）及其对泛化界的影响，并探索可数据驱动估计n-宽度的可行性与样本复杂度。

Method: 定义并区分两种内在维度：d_ρ(按核度量的上Minkowski维度) 与 d_K(基于Kolmogorov n-宽度的有效维度)。建立n-宽度与核积分算子特征值的关系，证明在固定Ω下n-宽度刻画μ的全集体最坏的特征值衰减。推导在n大时的泛化误差上界 O(n^{-(2+d_K)/(2+2d_K)+ε})，并给出一个从有限样本μ估计n-宽度上界的算法。针对接近均匀分布的情形，给出高概率取得 ε-精确上界所需样本量为 O(ε^{-d_ρ} log(1/ε)) 的结果。

Result: 给出基于d_K的无偏泛化上界及对应的样本复杂度分析；提出估计n-宽度上界的算法并在分形集上计算d_K；数值实验表明对Laplace核等，d_K 可能显著小于 d_ρ；在正则域上两者相等的结论得到证明。

Conclusion: d_K 作为核方法中的有效维度，能更紧地描述泛化能力，尤其在分形等非规则分布下相较于d_ρ具有潜在更好的样本效率；在正则域上两者一致，提供理论与算法框架以在实际中估计与应用。

Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.

</details>


### [85] [Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets](https://arxiv.org/abs/2601.16147)
*Muhammad Ilham Rizqyawan,Peter Macfarlane,Stathis Hadjidemetriou,Fani Deligianni*

Main category: cs.LG

TL;DR: Beat-SSL: 双上下文对比学习，节律级与心跳级对比，采用软目标，提升 ECG 表征，在全球节律分类与 ECG 分割任务中表现优于其他方法（分割领先4个百分点，分类达到 foundation 模型的93%性能）。


<details>
  <summary>Details</summary>
Motivation: 获取标注 ECG 数据困难；现有对比学习多聚焦全局上下文，或未充分利用 ECG 特征，并且常用硬对比目标，难以捕捉 ECG 信号的连续相似性。

Method: Beat-SSL 通过节律级和心跳级两层对比进行双上下文学习，采用软目标来捕捉连续的相似性。预训练后在下游任务中评估：1) 全球节律的多标签分类；2) ECG 分割；并进行消融研究，与三种方法（含一个 ECG 基础模型）对比。

Result: 在下游任务中，Beat-SSL 达到 foundation 模型在多标签分类的 93% 表现；在分割任务上击败所有对手，领先4个百分点。

Conclusion: Beat-SSL 展示了在 ECG 表征学习中的有效性：通过双上下文与软目标实现更贴近 ECG 信号的相似性建模，在少量标注数据条件下也能获得强表征，并在两项下游任务上显示出明显优势。

Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.

</details>


### [86] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: TTT-Discover 在测试时对大语言模型进行强化学习微调，以问题为中心产生单一最佳解，涵盖多领域并以开源模型和低成本资源实现可复现性。


<details>
  <summary>Details</summary>
Motivation: 通过把搜索过程从离线评估转向测试时的持续学习，使模型在特定科学问题上积累经验，从而优先产生最具潜力的解，而非追求对多任务的平均表现。

Method: 在测试时对冻结的 LLM 进行强化学习，允许模型继续训练以融入测试问题的经验；将学习目标与搜索子程序结合，专注于最有前景的解；使用开放模型（GPT-OSS-120B）与公开代码，通过 Thinking Machines 的 Tinker API 控制成本（每个问题数百美元级别）。

Result: 在多项任务上刷新状态/得到新近最优解：包括 Erdős 的最小重叠问题与自相关不等式、GPUMode 内核竞赛（最高可快2×）、Past AtCoder 算法竞赛、以及单细胞分析中的去噪问题；结果由专家或主办方评审，且实现可复现，使用公开资源。

Conclusion: 证明测试时学习可有效发现单一最优解并提升对特定科学问题的解决能力，同时具备可扩展性、可复现性和低成本潜力。

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


### [87] [Counterfactual Training: Teaching Models Plausible and Actionable Explanations](https://arxiv.org/abs/2601.16205)
*Patrick Altmeyer,Aleksander Buszydlik,Arie van Deursen,Cynthia C. S. Liem*

Main category: cs.LG

TL;DR: 提出一种对比事实训练（counterfactual training）的方法，将对比事实解释作为训练目标的一部分，以提升模型的解释能力并增强对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对比事实解释作为流行的后验解释方法，强调输入在可观测数据分布下的可行性与可操作性。现有研究多在事后生成对比事实，未将解释性目标融入训练。此工作旨在让模型在训练阶段就对解释目标负责。

Method: 在训练阶段引入对比事实，将其用于最小化学习表示与可行且可操作的解释之间的分歧，从而使模型更易产生一致的对比事实解释。

Result: 理论与实证结果均表明，该方法能训练出具备固有良好对比事实解释的模型，并且显著提升对抗鲁棒性。

Conclusion: 将对比事实解释纳入训练目标，可使模型的内部表示更易产生可行、可操作的解释，同时提升鲁棒性。

Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [88] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: 提出Gated Sparse Attention (GSA)，将门控注意力与稀疏注意力结合 via gated索引器、自适应稀疏控制与双门控，实现在128K上下文下的12-16x加速，同时提升模型质量与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏注意力在效率与稳定性方面的局限，以及门控注意力在训练稳定性与抑制注意力陷阱方面的不足，二者互补。

Method: 提出GSA架构：带sigmoid激活的门控快速索引器（bounded、可解释的选择分数）、基于局部不确定性的自适应稀疏控制器（调整关注token数量）、以及在值与输出阶段的双门控。提供复杂度、表达能力与收敛性等理论分析。实证在1.7B参数、400B token规模上，128K上下文。

Result: 在与稀疏基线的对比中实现12-16x加速，同时获得门控注意力的质量收益：perplexity由6.03降至5.70；128K上下文下的RULER分数显著提升；对首token的注意力下降从47%降至<4%；训练稳定性显著提升，loss尖点降低约98%。

Conclusion: GSA成功将稀疏注意力与门控注意力的优点整合，实现高效且稳定的长上下文建模，缓解注意力陷阱并提升模型性能，具备在大规模模型训练中的应用潜力。

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [89] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: 提出 DeepSurvey-Bench，用以全面评估生成综述的学术价值，填补现有基准在 ground-truth 与表层指标方面的不足，覆盖信息价值、学术传播价值、研究指引价值三维度，构建带有学术价值注释的数据集，并与人类评估对齐。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准依赖引用量和结构一致性等表层特征来选取人类写作的综述作为地面真实数据，并以结构质量和参考相关性等表面指标评估生成综述，导致难以评估生成综述的深层学术价值。

Method: 提出 DeepSurvey-Bench，制定覆盖信息价值、学术传播价值、研究指引价值三维度的学术价值评估标准；构建带学术价值注释的可靠数据集；对生成综述的学术价值进行评估。

Result: 大量实验表明该基准与人类在评估学术价值方面具有高度一致性。

Conclusion: DeepSurvey-Bench 可有效评估生成综述的学术价值，并与人类判断高度一致。

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [90] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: 本综述系统梳理了基于大规模视觉-语言模型（LVLMs）的多模态假新闻检测（MFND）领域的演进路径，提出以 foundations 模型驱动的范式取代传统特征工程与浅层融合方法，构建了模型、数据集与基准的结构化分类，并分析了可解释性、时间推理与领域泛化等挑战，最后给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着 LVLMs 的快速发展，多模态假新闻检测亟需一份系统性综述，帮助研究者理解从传统管线到基础模型驱动的端到端推理框架的转变，以及现有方法的优缺点、数据集与评价基准。

Method: 通过历史脉络梳理，将多模态检测从传统特征工程与简单融合方法演进至 foundation model 驱动的统一框架；构建覆盖模型架构、数据集与性能基准的结构化分类法；分析仍存的技术挑战（可解释性、时序推理、领域泛化等）；并提出未来研究方向；并给出方法综述的 GitHub 资源。

Result: 形成一个分层的综述结构：模型架构、数据集、基准的清晰 taxonomy；对现有研究的整合性分析与趋势判断；明确指出尚待解决的关键问题与研究方向；提供可重复的资源入口（GitHub）。

Conclusion: 本文首次系统地 documenting LVLMs 在 MFND 领域的变革性作用，填补缺乏统一综述的空白，旨在为后续研究者提供理论与实践上的指引，并促进该领域的快速发展。

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [91] [Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents](https://arxiv.org/abs/2601.15322)
*Raffi Khatchadourian*

Main category: cs.AI

TL;DR: DFAH measures trajectory determinism and evidence-aligned faithfulness in financial LLM agents to support regulatory audit replay; findings show smaller to mid-size models can achieve full determinism, larger models need more data; agent tool-use adds variance; determinism correlates with faithfulness; schema-first Tier 1 architectures meet audit-replay thresholds.


<details>
  <summary>Details</summary>
Motivation: Address reproducibility gaps in regulatory audit replay for LLM agents by quantifying determinism and evidence-consistency in tool-using systems within financial services.

Method: Evaluated 74 configurations (12 models, 4 providers, 8–24 runs each at T=0.0) in non-agentic baselines to measure determinism; introduced DFAH to assess trajectory determinism and evidence-conditioned faithfulness; used three financial benchmarks (compliance triage, portfolio constraints, DataOps exceptions) with 50 cases each; included an open-source stress-test harness; analyzed impact of agentic tool-use and model scale; computed correlations (Pearson r).

Result: 7–20B parameter models achieved 100% determinism; 120B+ models required ~3.7× more validation samples to reach comparable reliability; agentic tool-use added variance (Tables 4–7); positive correlation between determinism and faithfulness (r=0.45, p<0.01, n=51 at T=0.0); Tier 1 models with schema-first architectures attained determinism levels aligned with audit-replay needs across benchmarks.

Conclusion: DFAH demonstrates that determinism and faithfulness co-vary in practice; smaller-to-mid models can meet audit-replay demands, larger models need more data but can achieve parity with adequate sampling; architecture (schema-first) influences determinism; an open-source harness enables broader testing in financial contexts.

Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.

</details>


### [92] [GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation](https://arxiv.org/abs/2601.15392)
*Francesca Pia Panaccione,Carlo Sgaravatti,Pietro Pinoli*

Main category: cs.AI

TL;DR: GeMM-GAN: a Transformer-based GAN conditioned on histopathology patches and clinical metadata to synthesize gene expression profiles, outperforming baselines and improving disease-type prediction by >11% on TCGA.


<details>
  <summary>Details</summary>
Motivation: Gene expression data is valuable for multi-modal biomedical studies but limited by privacy regulations and costly experiments. A privacy-preserving, data-generating approach conditioned on imaging and metadata would enable broader research access and integration with other modalities.

Method: A Generative Adversarial Network (GeMM-GAN) conditioned on histopathology tissue slides and clinical metadata. It uses a Transformer Encoder to process image patches with a Cross Attention mechanism between image patches and text tokens to create a conditioning vector that guides the generator in producing biologically coherent gene expression profiles. Evaluation on TCGA compares against standard generative models.

Result: GeMM-GAN outperforms standard generative models in generating realistic and functionally meaningful gene expression profiles. It improves downstream disease type prediction accuracy by more than 11% over current state-of-the-art generative models on TCGA.

Conclusion: The proposed framework enables realistic, conditioned gene expression synthesis from histology and metadata, potentially facilitating privacy-preserving data sharing and enhanced multi-modal analyses in cancer research.

Abstract: Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN

</details>


### [93] [Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)](https://arxiv.org/abs/2601.15397)
*Peidong Wang*

Main category: cs.AI

TL;DR: LOGIC提出在解码层直接进行词位空间的上下文偏置，在不依赖提示输入的情况下实现对动态实体的高效纳入。对 Phi-4-MM 模型在11个语言 locale 的实验证明，平均实现了9%相对的 Entity WER 降幅，只有0.30% 的 False Alarm Rate 增量。


<details>
  <summary>Details</summary>
Motivation: 当前 Speech LLMs 的知识在训练后保持静态，难以识别大量快速增长的领域专有术语；提示法不可扩展，受上下文窗口与推理延迟影响，且 GEC 容易出现“过度纠错”导致实体幻觉。需要一种能够在解码阶段高效注入上下文的方案。

Method: 直接在解码层执行，设定对数几率（logit-space）偏置来强化候选实体的生成概率，解耦上下文注入与输入处理，实现相对于提示长度的常数时间复杂度。以 Phi-4-MM 模型在11个多语言场景上进行评估。

Result: 平均9%相对减少 Entity WER，False Alarm Rate 增长仅0.30%，在11个语言场景上展现稳健提升。

Conclusion: LOGIC提供了一种可扩展、鲁棒且高效的动态实体纳入机制，优于提示与GEC的方案。

Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the "lost-in-the-middle" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from "over-correction", introducing hallucinations of entities that were never spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.

</details>


### [94] [Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.15436)
*Shahar Ben Natan,Oren Tsur*

Main category: cs.AI

TL;DR: 提出一种基于对赌框架的LLM奉承性(sycophancy)评估方法：以LLM作为评估者在零和博弈中评估奉承，比较四大模型在不同设定下的奉承行为与与最新响应偏好及道德惭愧现象，发现Claude与Mistral表现出道德惭愧并对第三方受损进行过度补偿，同时奉承性与最近性偏见存在相互干扰的构造性干扰效应。


<details>
  <summary>Details</summary>
Motivation: 针对先前评估奉承性的研究易被人为偏见、噪声或提示的操纵所左右，提出直观、无偏的评估框架。通过将奉承性置于零和博弈并以LLM作为裁判，降低外部干扰对评价的影响，提升评估鲁棒性。

Method: 在对赌框架下，将奉承性定义为用户一方的收益且对他人成本化，使用四个主流模型进行比较：Gemini 2.5 Pro、ChatGPT-4o、Mistral-Large-Instruct-2411、Claude Sonnet 3.7。通过设计“对话-评估-奖励”的循环，测试在不同设置下（同态场景、对第三方成本显式存在时）模型的奉承倾向、对最后给出答案的偏好以及是否存在道德惭愧迹象。统计分析两者的相互作用（奉承性与最近性偏见的干扰效应），并讨论零和博弈下的成本分配对结果的影响。

Result: 在常见设置中，四个模型均表现出奉承性倾向；但Claude与Mistral在显式伤及第三方成本的场景中表现出“道德惭愧”，对奉承进行了过度补偿。所有模型均倾向于偏向最后给出的答案（最近性偏见）。此外，奉承性与最近性偏见并非独立存在，而是存在‘构造性干扰’效应，即当用户意见出现在最后时，模型对用户意见的认同度被进一步放大。

Conclusion: 提出了一种可重复且对偏见鲁棒的新评估框架，揭示了在对赌设定下的奉承性与道德惭愧行为的模型差异、以及最近性偏见与奉承性的耦合效应，为LLM对齐评估提供新的量化工具与理论洞见。

Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit "moral remorse" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.

</details>


### [95] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: 三段式法律AI可靠性评估：独立生成模型、基本RAG、先进RAG对比，显著降低幻觉与伪造信息。


<details>
  <summary>Details</summary>
Motivation: 提升高风险法律工作中LLM的可依赖性与可追溯性，降低幻觉与错引证据的风险。

Method: 比较三种AI范式；评估2,700份司法风格回答，涵盖12个LLM、75个法律任务；采用专家双盲评审；提出False Citation Rate (FCR)与Fabricated Fact Rate (FFR)两指标；先进RAG结合嵌入微调、重排序和自我纠错等技术。

Result: 独立模型FCR>30%，基本RAG显著降低错误但仍有错位证据；先进RAG将伪造降至可忽略水平（<0.2%）。

Conclusion: 可信赖的法律AI需以检索为核心、强调验证与可追溯性的严谨架构，评估框架可拓展至其他高风险领域。

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [96] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: TRACK is a new benchmark that evaluates how LLMs propagate updated facts that conflict with initial parametric knowledge during multi-step reasoning; finds that updating can degrade performance, and degradation grows with more updates.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks largely test single updates or fact recall and neglect downstream reasoning when updates conflict with parametric knowledge; there is a need to study propagation of conflicting knowledge in multi-step reasoning.

Method: Introduce TRACK with three reasoning scenarios (WIKI, CODE, MATH) containing multiple realistic conflicting updates; assess how updated facts affect downstream multi-step reasoning performance.

Result: Providing updated facts can worsen reasoning performance compared to no updates, and the degradation increases as more updated facts are provided; caused by both poor integration of updated facts and flawed reasoning even when they are integrated.

Conclusion: TRACK provides a rigorous benchmark to measure and guide future work on propagating conflicting knowledge in multi-step reasoning and to develop more robust knowledge-update methods.

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [97] [The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers](https://arxiv.org/abs/2601.15509)
*Prasanna Kumar*

Main category: cs.AI

TL;DR: Transformer-based sentiment classifiers often gain overall accuracy but at the expense of neutrality, showing polarization of sentiment classes and unreliable outputs in Applied NLP.


<details>
  <summary>Details</summary>
Motivation: Evaluate the trade-off between accuracy gains from transfer learning/transformers and maintaining neutrality in sentiment analysis, and highlight implications for industry tasks.

Method: Empirical/critical analysis of transformer-driven sentiment models; observation of polarization across sentiment classes; discussion of potential causes (data distribution, loss optimization) and evaluation gaps for neutrality.

Result: Recognizes that improvements in one sentiment class can reduce neutrality, leading to polarized predictions and unreliable outputs for industry-ready NLP tasks.

Conclusion: Neutrality-aware modeling is required. Potential directions include fairness-aware training, calibration, multi-objective optimization, and neutrality-focused evaluation metrics to balance accuracy with stable, non-polarizing sentiment outputs.

Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.

</details>


### [98] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: A multi-agent adaptive learning framework ALIGNAgent integrates knowledge estimation, skill-gap identification, and resource recommendation to deliver personalized learning; empirical results show high precision and F1 against exam performance.


<details>
  <summary>Details</summary>
Motivation: Current educational systems are fragmented across knowledge tracing, diagnostic modeling, and resource recommender modules; integrating them into a cohesive adaptive loop can improve learning outcomes and personalization.

Method: ALIGNAgent employs a Skill Gap Agent for topic-level proficiency estimates using concept-level diagnostic reasoning to identify misconceptions, followed by a Recommender Agent that retrieves preference-aware learning materials aligned with diagnosed deficiencies; a continuous feedback loop guides interventions before advancing topics; GPT-4o-based agents are used.

Result: Empirical evaluation on authentic datasets from two undergraduate CS courses shows precision of 0.87–0.90 and F1 scores of 0.84–0.87 for knowledge proficiency estimation, validated against actual exam performance.

Conclusion: An integrated multi-agent framework (ALIGNAgent) is effective for personalized learning, demonstrating strong accuracy in knowledge proficiency estimation and practical viability of end-to-end adaptive learning with GPT-4o-based components.

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [99] [CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/abs/2601.15628)
*Haibo Tong,Zeyang Yue,Feifei Zhao,Erliang Lin,Lu Jia,Ruolin Chen,Yinqian Sun,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: CogToM 是一个理论基础的多语言、多范式大规模理论化 ToM 基准，覆盖 46 种范式、8000+ 双语实例，利用 49 名注释者验证。对 22 模型（含 GPT-5.1、Qwen3-Max 等前沿模型）进行系统评估，显示显著的性能异质性和在特定维度的瓶颈，且分析表明 LLM 的认知结构与人类可能存在分歧。CogToM 提供一个稳健的工具与视角来探究 LLMs 的认知边界。


<details>
  <summary>Details</summary>
Motivation: 当前 ToM 基准多局限于自证错觉或假信念任务，无法全面覆盖人类认知机制。需要一个理论上扎根、覆盖多范式、跨语言的评估体系，以揭示 LLM 推理的广度与局限，以及与人类认知的关系。

Method: 建立 CogToM 基准：包括 46 种范式、8000+ 的双语实例；通过 49 名人类注释者对数据进行验证和评分；对 22 种代表性模型进行评估，涵盖前沿模型（如 GPT-5.1、Qwen3-Max）与基线模型；基于人类认知模式对结果进行分析，探讨模型的认知架构与人类的差异。

Result: 模型间存在显著的性能异质性；在某些维度仍然存在瓶颈，且分析提示 LLM 的认知结构与人类可能存在差异或不一致之处。

Conclusion: CogToM 为研究 LLM 认知边界提供了稳健的工具与视角，有助于推动对 AI 认知能力的理论理解以及未来模型的改进方向。

Abstract: Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.

</details>


### [100] [Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2601.15652)
*Manish Bhatt*

Main category: cs.AI

TL;DR: 提出一种基于神经科学信号的混合检测框架，用以识别LLM幻觉，结合预测编码与信息瓶颈，实现高数据效率与可解释性，且在 HaluBench 上达到 0.8669 AUROC，性能优于基线与大规模评估，对部分信号（如 Rationalization）存在无效性。


<details>
  <summary>Details</summary>
Motivation: 幻觉是高风险部署中的核心难题。现有检测多依赖昂贵的外部检索或需大参数判别模型，导致推理成本高且缺乏可解释性。因此需要一个数据高效、可解释的检测方法来支持生产部署。

Method: 提出混合检测框架，提取可解释信号，源自 Predictive Coding（衡量对内部先验的惊讶）和 Information Bottleneck（衡量扰动下的信号保留）。进行系统性消融，提出 Entity-Focused Uptake、Context Adherence、Falsifiability Score 三个改进。在 HaluBench（n=200，平衡）上评估，并与基线、BASE、IMPROVED 特征进行对比。实现小于1M参数、推理快速（5ms），且保持可解释性。报告负结果：Rationalization 信号不能有效区分幻觉。

Result: 在 HaluBench 上，理论驱动基线 AUROC 为 0.8017；BASE 模型 0.8274；IMPROVED 特征提升至 0.8669（提升约 4.95%）。数据效率显著：200 条样本比 Lynx 的 15,000 条少 75 倍；推理速度提高约 1000 倍（5ms 对 5s）。同时保持可解释性。负结果显示 Rationalization 信号无法区分幻觉（存在“对错前提下的自相矛盾式推理”现象）。

Conclusion: 领域知识编码的信号架构可在数据效率方面胜过扩展大规模判别器，提供适于生产部署的轻量、可解释模型；将神经科学信号引入检测框架具有潜在价值，尽管某些信号（如 Rationalization）可能无效。

Abstract: Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.

</details>


### [101] [Improving Methodologies for LLM Evaluations Across Global Languages](https://arxiv.org/abs/2601.15706)
*Akriti Vij,Benjamin Chua,Darshini Ramiah,En Qi Ng,Mahran Morsidi,Naga Nikshith Gangarapu,Sharmini Johnson,Vanessa Wilfred,Vikneswaran Kumaran,Wan Sie Lee,Wenzhuo Yang,Yongsen Zheng,Bill Black,Boming Xia,Frank Sun,Hao Zhang,Qinghua Lu,Suyu Ma,Yue Liu,Chi-kiu Lo,Fatemeh Azadi,Isar Nejadgholi,Sowmya Vajjala,Agnes Delaborde,Nicolas Rolin,Tom Seimandi,Akiko Murakami,Haruto Ishi,Satoshi Sekine,Takayuki Semitsu,Tasuku Sasaki,Angela Kinuthia,Jean Wangari,Michael Michie,Stephanie Kasaon,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim,Daisy Newbold-Harrop,Jessica Wang,Mahmoud Ghanem,Vy Hong*

Main category: cs.AI

TL;DR: 多语言安全评估揭示语言间的差异与评估者可靠性差异；提出构建共用的多语言安全测试框架的路径。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型在全球范围部署，需要在不同语言和文化语境下确保其行为的安全性和可靠性，因此需要系统性的、多语言的安全评估框架与基准。

Method: 跨国联合评估：选取两种开放权重模型，覆盖十种语言（粤语/英语、波斯语、法语、日语、韩语、斯瓦希里语、马来语、普通话、泰卢固语），翻译生成约6000条提示，涵盖隐私、非暴力犯罪、暴力犯罪、知识产权、越狱鲁棒性五类危害。采用LLM-作为评判者与人工标注双轨评估，对提示进行评估并比较两种评估路径的结果，分析语言与危害类型的差异及评估者可靠性。

Result: 研究发现安全行为在不同语言中呈现差异，且守护鲁棒性及对不同危害类型的敏感性存在变异；LLM与人工评估者的可靠性也有差异。基于此提出方法论洞见，如需要文化语境化翻译、经过应力测试的评估者提示以及更清晰的人类标注指南。

Conclusion: 这是朝向多语言安全测试共用框架的初步步骤，需继续与学界与业界协作，推动跨语言、安全评估标准与基准的建立。

Abstract: As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.
  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.

</details>


### [102] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: 提出了首个将形式定理证明扩展到物理领域的框架：构建 PhysLeanData 数据集并通过 RLVR 指导对 DeepSeek-Prover-V2-7B 的微调，生成 PhysProver。仅用约 5k 训练样本即可在多个子领域实现 2.4% 的提升，且在 MiniF2F-Test 上提升 1.3%，显示对物理领域以外的形式化数学能力的一定泛化。计划开源数据集与模型。


<details>
  <summary>Details</summary>
Motivation: 弥补现有工作对形式化推理在物理领域的忽视，推动将可验证的语言模型与形式化证明结合，提升物理推理的严谨性和可验证性，并探索跨域泛化能力。

Method: 构建专用数据集 PhysLeanData（包含 PhysLean 抽样定理和通过基于猜想的形式数据生成管线产生的数据）；以 DeepSeek-Prover-V2-7B 为基础，在训练中引入 Reinforcement Learning with Verifiable Rewards (RLVR) 以训练出 PhysProver。

Result: 在约 5k 的训练样本下，PhysProver 在多个子领域实现约 2.4% 的提升；在完成物理领域的形式化训练后，在 MiniF2F-Test 基准上取得 1.3% 的提升，显示出跨物理域的非平凡泛化与对形式化数学能力的提升。

Conclusion: 实验结果证实方法在提升物理领域形式化推理的有效性与高效性，提供了将形式化证明器扩展到非数学领域的范式，并将开源数据集与模型以促进后续研究。

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [103] [Tabular Incremental Inference](https://arxiv.org/abs/2601.15751)
*Xinda Chen,Xing Zhen,Hanyu Zhang,Weimin Tan,Bo Yan*

Main category: cs.AI

TL;DR: 提出 Tabular Incremental Inference (TabII) 任务，允许在推理阶段利用新增列，并将其视为信息瓶颈优化问题，提出基于 LLM 占位符、预训练 TabAdapter 与增量样本压缩模块的实现，结果在八个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 应对固定列训练的表格模型在表格列动态变化场景的局限，需要无监督或半有监督的方法以在推理阶段融入新列信息，提高实际应用性。

Method: 将信息瓶颈理论用于设计 TabII，使用大型语言模型占位符/外部知识、Pretrained TabAdapter、Incremental Sample Condensation (ISC) 块来提取和压缩与任务相关的增量列信息。

Result: 在八个公开数据集上验证，TabII 能有效利用增量属性，达到或超过现有方法的性能

Conclusion: 将增量属性融入推理的可行性和有效性得到证明，信息瓶颈视角为设计增量表格推理提供理论支撑；未来工作可扩展至更大规模表格和更丰富的增量信号。

Abstract: Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.

</details>


### [104] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC：一种从零开始、仅用单条专家轨迹即可学习的离线-在线强化学习方法，通过对数-限幅熵项抑制离分布行动并稳定Q函数，显著缓解Q值振荡并在D4RL和真实机器人任务中以较少交互实现良好策略。


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习面临样本效率低、奖励稀疏、观测噪声等挑战。现有方法要么依赖离线数据集、数据量大且不稳定，要么需要大规模预训练/微调，难以以低成本实现在线学习。需提出低成本、数据需求最小化的学习方法。

Method: 提出 SigEnt-SAC，一种基于 off-policy 的演员-评论家结构，核心在于引入一个 sigmoid 限幅的熵项，用以防止负熵驱动的对分布外行动的优化并降低 Q 函数的振荡；在仅有单条专家轨迹的前提下从零开始学习。对 D4RL 基准进行对比评估，并在四个真实机器人任务（多种执行方式、原始图像与稀疏奖励）上验证学习能力。

Result: 在 D4RL 上显著缓解了 Q 函数振荡，并比现有方法更快达到 100% 成功率；在真实机器人任务中，使用少量真实交互即可学到成功策略，且能处理原始图像与稀疏奖励，显示低成本、实用化路径的潜力。

Conclusion: 证明在极低数据成本下，结合 sigmoid 限幅熵的 off-policy RL 具有良好鲁棒性与样本效率，适合作为现实世界 RL 的可行部署路线。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [105] [Creativity in the Age of AI: Rethinking the Role of Intentional Agency](https://arxiv.org/abs/2601.15797)
*James S. Pearson,Matthew J. Dennis,Marc Cheong*

Main category: cs.AI

TL;DR: 提出替代性创意标准：用一致性（对可靠生成新颖且有价值产出之要求）取代普遍性的有意行动者条件(IAC)，并在特定领域保留IAC。凭借大规模生成AI的发展，IAC在描述与功能层面日益难以成立。


<details>
  <summary>Details</summary>
Motivation: 反对将“有意行动者”作为创意的一般性条件，受生成式AI兴起及其社会认知影响，同时探索创意判定的社会功能及语言直觉的局限性。

Method: 1) 基证研究：分析作者与记者在生成式AI作品上的创造力归因。2) 概念工程：论证IAC在社会功能上的不足，提出替代的“一致性”标准。

Result: 总体上主张在一般层面上放弃IAC，转以一致性标准来界定创意；但在特定局部领域仍保留IAC。

Conclusion: IAC不再具备普遍性效用，改以一致性来追踪“可靠地生成新颖且有价值产物”的创意；在局部情境下可保留IAC用于特定判断。

Abstract: Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.

</details>


### [106] [VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management](https://arxiv.org/abs/2601.15798)
*Zhikai Xue,Tianqianjin Lin,Pengwei Yan,Ruichun Wang,Yuxin Liu,Zhuoren Jiang,Xiaozhong Liu*

Main category: cs.AI

TL;DR: 提出VitalDiagnosis，一套将可穿戴数据与LLM推理结合的慢性病管理生态系统，促使从被动监测转向主动、互动的护理模式，提供情境化查询、初步洞见与个性化指导，提升自我管理并缓解临床工作负担。


<details>
  <summary>Details</summary>
Motivation: 全球慢性病负担日益突出，资源紧张、人口老龄化加剧。患者常难以准确解读早期恶化信号与坚持治疗，需一种更主动、协作的护理模式以提高自我管理与降低医疗系统压力。

Method: 构建一个以LLM为核心的生态系统，持续整合穿戴设备数据，基于情境感知的查询分析触发因素，在患者-医生协同工作流中生成 provisional insights（初步洞见）与个性化指导，促进主动干预与合规性管理。

Result: 当前稿件提供的是概念性框架及潜在影响预测，尚未给出实证数据或实验结果。

Conclusion: 该框架有望实现更主动、协作的慢性病管理，提升患者自我管理能力并降低不必要的临床工作负担，但需解决隐私保护、数据互操作性、工作流整合、证据评估等挑战，未来应通过真实世界数据和临床试点进行评估。

Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.

</details>


### [107] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: 提出一种基于 rubrics 的自我验证机制 DeepVerifier，将推理时的验证扩大化，通过 DRA Failure Taxonomy 指导的反馈循环，在推理阶段提升自我进化能力，且对开放模型友好，显著提升评估指标并提供可公开数据集 DeepVerifier-4K。


<details>
  <summary>Details</summary>
Motivation: 针对现有以事后训练为核心的策略能力提升，提出在推理阶段进行自我验证和迭代改进，以实现无额外训练的自我提升，并提高对复杂任务的可靠性与可解释性。

Method: 构建自动化的 DRA Failure Taxonomy（五大类、十三子类），开发 rubrics-based outcome reward verifier DeepVerifier；在推理阶段作为可插拔模块，利用反馈改进行为和回答；开放数据集 DeepVerifier-4K 提供用于监督微调；在 GAIA、XBench-DeepResearch 等数据集上评估。

Result: DeepVerifier 在元评估 F1 上超越基线 12%-48%；在能力较强的闭源大模型驱动下，对 GAIA 与 XBench-DeepResearch 的难样本实现 8%-11% 的准确率提升；并发布 DeepVerifier-4K 数据集以促进开放研究。

Conclusion: 以 rubrics 为核心的自我验证机制实现推理阶段自我提升，提升鲁棒性与可解释性，利于开源模型的发展与广泛应用。

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [108] [ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models](https://arxiv.org/abs/2601.15812)
*Shir Ashury-Tahan,Yifan Mai,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.AI

TL;DR: 提出 ErrorMap 框架，用以定位与区分大语言模型失败的根源，基于 35 个数据集与 83 个模型构建 ErrorAtlas 目标的错误分类法，揭示常见与被忽视的错误类型。


<details>
  <summary>Details</summary>
Motivation: 现有基准多聚焦于“模型在哪儿做对了”，却难以回答“为何会错”。错误可能来自格式、计算、数据噪声等因素，需分离原因以真正指导模型改进与基准设计。

Method: 提出“失败签名”（failure signature）提取方法，基于同一套逻辑对任意模型/数据集进行错误分析；在 35 个数据集和 83 个模型上应用，生成 ErrorAtlas，形成模型错误的分类体系与可复用工具。

Result: 得到对模型错误的可解释分类（ErrorAtlas），揭示重复出现的失败模式并发现当前研究较少关注的错误类型，如输出缺失关键信息、对问题的误解等；提供跨模型/任务的全局评估层次，并公开 taxonomy 与代码，计划随着新基准和模型更新。

Conclusion: 通过从“成功”转向“为什么失败”，ErrorMap/ErrorAtlas 提供更丰富的评估视角，帮助开发者调试模型、对齐基准目标并指导模型选择，具备全球适用性和持续更新能力。

Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

</details>


### [109] [ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search](https://arxiv.org/abs/2601.15931)
*Xiangyu Wang,Zhixin Lv,Yongjiao Sun,Anrui Han,Ye Yuan,Hangxu Ji*

Main category: cs.AI

TL;DR: ICON提出以因果与符号先验为核心的多策略框架，通过四个模块实现对 TBPS 的几何不变性、环境独立性、全局完备性和拓扑一致性，提升鲁棒性与跨分布稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有 TBPS 依赖预训练模型，在开放世界易受位置偏差、背景干扰和分布偏移影响，缺乏因果鲁棒性。

Method: 提出 Rule-Guided Spatial Intervention、Counterfactual Context Disentanglement、Saliency-Driven Semantic Regularization、Neuro-Symbolic Topological Alignment 四大模块，分别实现几何不变性、环境独立性、显著性正则化与拓扑对齐。

Result: 在标准基准保持领先，并对遮挡、背景干扰、定位噪声等具有显著鲁棒性。

Conclusion: 通过从拟合统计共现转向学习因果不变性，推动 TBPS 的鲁棒性与跨分布泛化。

Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.

</details>


### [110] [Natural Language-Driven Global Mapping of Martian Landforms](https://arxiv.org/abs/2601.15949)
*Yiran Wang,Shuoyuan Wang,Zhaoran Wei,Jiannan Zhao,Zhonghua Yao,Zejian Xie,Songxin Zhang,Jun Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: MarScope提出一种面向行星尺度的视觉-语言框架，基于自然语言查询实现对火星地貌的无标签映射；在20万对图文数据上训练，通过在共享语义空间中对齐影像和文本，实现全球地貌映射的快速语义检索（5秒内，F1最高可达0.978），并扩展到过程导向分析与基于相似性的地貌映射，标志着自然语言作为科学发现的直接接口。


<details>
  <summary>Details</summary>
Motivation: 现有高层语义标签与大规模像素级行星图像之间存在信息鸿沟，限制对行星表面的开放式、可扩展探索。需要一种能以自然语言进行灵活、无标签映射并覆盖全球数据的框架。

Method: 构建一个跨模态共享语义空间，通过对齐火星影像与文本，训练超过20万对经过精心筛选的图文数据，对图像-文本对进行跨模态编码与投影，使自然语言查询在全星球范围内进行高效检索与映射。

Result: 实现跨星球尺度的语义检索，响应时间约5秒，F1达到最高0.978；支持任意用户查询，超越传统形态学分类，具备过程性分析与基于相似性的地貌映射能力。

Conclusion: 自然语言可作为直接的科学发现界面，连接大规模地理空间数据与地貌分析，MarScope为行星地表研究带来新范式。

Abstract: Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.

</details>


### [111] [Decoupling Return-to-Go for Efficient Decision Transformer](https://arxiv.org/abs/2601.15953)
*Yongyi Wang,Hanyu Liu,Lingfeng Li,Bozhou Chen,Ang Li,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: DDT (Decoupled DT) removes the full RTG sequence in Decision Transformer, using only the latest RTG with observation/action, reducing compute and improving performance.


<details>
  <summary>Details</summary>
Motivation: Identify and remove redundancy in Decision Transformer where the entire RTG sequence is fed into the transformer; only the most recent RTG influences action prediction, which may impair performance and efficiency.

Method: Introduce Decoupled DT (DDT) that processes only observation and action sequences through the Transformer and uses the latest RTG to guide action prediction. Compare against DT and other DT variants across offline RL tasks, evaluating performance and computational cost.

Result: DDT significantly outperforms DT and is competitive with state-of-the-art DT variants across multiple offline RL tasks, with lower computational cost.

Conclusion: Decoupling RTG from the transformer streamlines the architecture, improves performance, and reduces computation, making DDT a strong alternative to the standard Decision Transformer in offline RL.

Abstract: The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

</details>


### [112] [Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment](https://arxiv.org/abs/2601.16027)
*Yiran Qiao,Xiang Ao,Jing Chen,Yang Liu,Qiwei Zhong,Qing He*

Main category: cs.AI

TL;DR: CS-VAR is a cross-session, evidence-aware retrieval-augmented detector for live streaming risk assessment, using an LLM-guided training process to transfer cross-session insights to a lightweight model for fast, interpretable, real-time moderation.


<details>
  <summary>Details</summary>
Motivation: Live streaming platforms face rising risks such as scams and coordinated malicious behaviors. Risks often accumulate over time and recur across streams, making single-session detection ineffective; there is a need to leverage cross-session behavioral evidence while maintaining real-time efficiency.

Method: CS-VAR consists of a lightweight session-level risk detector that performs fast in-session inference, guided during training by a Large Language Model that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. The approach uses retrieval of cross-session evidence to enable pattern recognition across streams, enabling structured risk assessment and real-time deployment while providing interpretability signals.

Result: Offline experiments on large-scale industrial datasets plus online validation show state-of-the-art performance of CS-VAR. The model yields interpretable, localized signals that aid real-world moderation for live streaming.

Conclusion: CS-VAR enables efficient, cross-session, evidence-aware risk assessment for live streaming by distilling LLM-guided insights into a lightweight detector, achieving state-of-the-art performance and practical interpretability for moderation.

Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

</details>


### [113] [Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://arxiv.org/abs/2601.16038)
*Olga Bunkova,Lorenzo Di Fruscia,Sophia Rupprecht,Artur M. Schweidtmann,Marcel J. T. Reinders,Jana M. Weber*

Main category: cs.AI

TL;DR: One-shot prompting with aligned exemplars yields best Text2Cypher retrieval for reaction-path queries on a reaction knowledge graph; zero-shot with a checklist improves executability but provides limited extra retrieval gains.


<details>
  <summary>Details</summary>
Motivation: To reduce hallucinations and outdated suggestions in LLM-assisted synthesis planning by grounding LLMs in a reaction knowledge graph and framing path retrieval as a natural language to Cypher query problem.

Method: Formulate reaction-path retrieval as Text2Cypher. Compare zero-shot versus one-shot prompting, with static, random, and embedding-based exemplar selection. Evaluate using query validity and retrieval accuracy. Develop a checklist-driven validator/corrector loop.

Result: One-shot prompting with aligned exemplars consistently yields the best retrieval performance. The checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. A reproducible Text2Cypher evaluation setup is provided, with code available at the linked repository.

Conclusion: The study demonstrates effective KG-grounded LLM workflows for synthesis planning, highlights the importance of exemplar design in prompting, and provides a reproducible evaluation framework to advance KG-LLM research in cheminformatics.

Abstract: Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.

</details>


### [114] [Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning](https://arxiv.org/abs/2601.15160)
*Yuval Kansal,Niraj K. Jha*

Main category: cs.AI

TL;DR: 提出基于知识图谱的自下而上的推理框架，通过后训练的监督微调与强化学习，将知识图谱路径转化为奖励信号，促使模型组合中间公理完成复杂多 hops 推理；在医学领域验证，14B 模型对 1-3 跳的短路径进行训练，对 4-5 跳的长路径实现零-shot 泛化，显著超越更大的模型并对抗干扰，表明将推理基于结构化知识具有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在结构化多步推理（如数学、编程）方面表现接近专家水平，但在受限的专业科学领域的可组合多步推理能力仍有限，需要将推理过程绑定到公理化的领域事实以提升可解释性和可扩展性。

Method: 建立一个后训练流水线，结合监督微调和强化学习；将知识图谱作为隐式奖励模型，通过从知识图谱路径中推导奖励信号来引导模型在 RL 训练中组合中间公理而不仅优化最终答案；在医学领域对 14B 模型进行短跳推理（1-3 跳）的训练，并评估对复杂多跳查询（4-5 跳）的零-shot 泛化；对抗扰动（选项洗牌等）下的鲁棒性验证；以路径-derived rewards 作为“组合性桥梁”。

Result: 实验结果显示路径奖励显著提升模型在最困难的推理任务上的表现，使得较小模型也能超越若干更大的对手模型和前沿系统（如 GPT-5.2、Gemini 3 Pro）；对抗扰动测试也显示鲁棒性增强。

Conclusion: 以结构化知识为支撑的推理过程具有可扩展性和高效性，是实现智能推理的有效路径。

Abstract: Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.

</details>


### [115] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: 将视觉-语言模型与外部知识检索相结合，构建一个用于识别气候信息错误/误导的框架，能够获取最新信息（反向图片检索、事实核查、专家内容）以提升对图片及其声称的真实性判定。


<details>
  <summary>Details</summary>
Motivation: 气候信息误导在数字时代广泛传播，视觉-语言模型在训练时获得的知识是静态且时效性差，难以对最新事件进行推理与更新。需要引入外部、可检索的实时信息以提升可信度评估。

Method: 设计一个联合框架：基于VLM进行初步判断后接入外部知识检索模块，检索反向图片结果、在线事实核查与可信专家内容等，再对图文声称进行推理与分类，输出如真实、误导、虚假、不可验证等标签。

Result: 与纯VLM相比，该方法在处理真实世界气候信息误导方面具有更强的判别能力和鲁棒性，提升了对图像及其声称真实性的评估能力。

Conclusion: 通过将VLM与外部知识源结合，该方法克服训练时知识静态性的局限，为在快速变化的信息环境中保护科学传播提供了更可靠的工具。

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


### [116] [LLM Prompt Evaluation for Educational Applications](https://arxiv.org/abs/2601.16134)
*Langdon Holmes,Adam Coscia,Scott Crossley,Joon Suh Choi,Wesley Morris*

Main category: cs.AI

TL;DR: 提出一个可推广的、系统性的提示评估框架，利用对战式评估和Glicko2评分对比六种教育相关提示模板在LLM对话中的效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育场景中的广泛应用，需要基于证据的提示设计与评估方法，以实现个性化、教育对齐和可重复评估。

Method: 设计六个提示模板，结合现有提示工程模式，强调不同教育策略；采用对战式评估框架，8名评审对提示对的格式、对话支撑、学习者适切性进行评分；数据来自120次真实用户交互，覆盖三类教育场景。

Result: 在对战结果中，包含“策略性阅读”的提示最具优势，在两两比较中的胜率介于81%到100%。该提示将人物化与情境管理结合，支持元认知学习（如自主学习）。

Conclusion: 提供一个可迁移、证据驱动的框架，用于教育领域的提示设计评估，推动从经验式提示工程向证据化提示开发的转变。

Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.

</details>


### [117] [Structured Hints for Sample-Efficient Lean Theorem Proving](https://arxiv.org/abs/2601.16172)
*Zachary Burton*

Main category: cs.AI

TL;DR: 在神经定理证明领域，作者通过在推理阶段引入固定的15种策略骨架的提示调度，显著提升了RL训练后模型的推理性能：对相同样本数和长度，pass@16从15.2%提升至21.7%，相对提升约43%。


<details>
  <summary>Details</summary>
Motivation: 评估是否仍能通过简单的推理时结构先验来提升经强化学习训练的强大模型的推理性能，以及结构化提示是否对鲁棒性和泛化具有额外价值。

Method: 在miniF2F基准上，对一个轻量级干预进行评估：在推理时对固定的15个常用战术骨架应用提示调度，与使用相同模型的标准采样进行比较；样本数k=16，最大生成长度1024个标记。

Result: 固定提示调度在推理阶段显著提升性能：pass@16从15.2%提升到21.7%，相对提升约43%。同样的样本数和长度下，结构辅助发挥了显著作用。

Conclusion: 即使在RL训练后的强大证明器中，结构先验仍未被充分利用，简单的推理时指导可以作为低成本的互补提升手段，提升推理表现并可能增强鲁棒性。

Abstract: State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.

</details>


### [118] [Scalable Board Expansion within a General Game System](https://arxiv.org/abs/2601.16216)
*Clémentine Sacré*

Main category: cs.AI

TL;DR: 提出在通用博弈系统(GGS)中实现动态棋盘扩展，以在博弈进行时自动增长棋盘，避免静态过大棋盘带来的复杂性。


<details>
  <summary>Details</summary>
Motivation: 静态棋盘常被设计为过大且未被充分利用，导致资源浪费和实现复杂性；板无边界的博弈需要更灵活的棋盘管理。

Method: 提出一种动态扩展机制，使棋盘在游戏进行中按需增长，可能结合GGS框架的事件、局部区域管理与棋盘状态同步。

Result: 概念性设计与可行性分析，提供实现要点与算法思路，尚无大规模实验结果。

Conclusion: 动态棋盘扩展可降低复杂性、提升资源利用率，并实现更具可扩展性的板面设计。

Abstract: This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [119] [Is Grokipedia Right-Leaning? Comparing Political Framing in Wikipedia and Grokipedia on Controversial Topics](https://arxiv.org/abs/2601.15484)
*Philipp Eibl,Erica Coppolillo,Simone Mungari,Luca Luceri*

Main category: cs.IR

TL;DR: 两大百科平台在政治议题上的语义框架与偏向存在相似性，但在争议性话题上分歧更为明显；二者总体呈左倾框架，Grokipedia则显示更明显的双峰分布，右倾内容的占比提升；还展示了跨章节的语义相似度衰减与可复现的实验代码。


<details>
  <summary>Details</summary>
Motivation: 探究在线百科是否存在意识形态偏见，以及不同平台之间的偏向特征，特别是在Wikipedia与由xAI生成的Grokipedia之间的对比，以理解语义框架、政治取向和内容优先级的差异。

Method: 对一组政治高度争议的主题进行对比分析，评估语义框架、政治取向和内容优先级；测量同一主题下不同章节之间的语义相似度的变化；比较在争议性话题与随机抽样话题上的取向分布；公开实验代码。

Result: 语义相似性在各章节之间衰减，在争议性话题上的分歧程度高于随机话题；两者都呈现左倾框架，但Grokipedia的分布更具双峰性，右倾内容的显著性增强；实验代码对外公开。

Conclusion: 在线百科存在可重复的左倾趋势，Grokipedia在右倾内容的可见性方面呈现更强的两端性。研究强调平台特征对框架与内容优先级的影响，并指出可复现性与改进的方向。

Abstract: Online encyclopedias are central to contemporary information infrastructures and have become focal points of debates over ideological bias. Wikipedia, in particular, has long been accused of left-leaning bias, while Grokipedia, an AI-generated encyclopedia launched by xAI, has been framed as a right-leaning alternative. This paper presents a comparative analysis of Wikipedia and Grokipedia on well-established politically contested topics. Specifically, we examine differences in semantic framing, political orientation, and content prioritization. We find that semantic similarity between the two platforms decays across article sections and diverges more strongly on controversial topics than on randomly sampled ones. Additionally, we show that both encyclopedias predominantly exhibit left-leaning framings, although Grokipedia exhibits a more bimodal distribution with increased prominence of right-leaning content. The experimental code is publicly available.

</details>


### [120] [DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking](https://arxiv.org/abs/2601.15518)
*Wenxin Zhou,Ritesh Mehta,Anthony Miyaguchi*

Main category: cs.IR

TL;DR: 两阶段融合检索：将混合检索（LLM检索、BM25、稠密检索）与基于主题的多索引稠密检索相结合，并在第二阶段进行LambdaMART与LLM再排序；通过5000个合成ToT查询进行训练，最佳系统在测试集上达到召回率0.66、NDCG@1000=0.41，证明融合检索的有效性。


<details>
  <summary>Details</summary>
Motivation: ToT任务需要在大规模知识库中精准检索相关线索。单一检索方法往往受限，融合多种信号并使用再排序能提升召回和ランキング质量。本研究通过两阶段设计、主题分区和合成数据来提升ToT检索性能。

Method: 阶段1：混合检索，将基于LLM的检索、BM25（稀疏）、以及稠密检索（BGE-M3）进行融合；并引入主题感知的多索引稠密检索，将维基百科分成24个主题域以缩小检索空间。阶段2：评估训练好的LambdaMART再排序器与基于LLM的再排序器；为训练模型生成5000个合成的ToT查询。最优系统在测试集通过将混合检索与Gemini-2.5-flash再排序结合实现。

Result: 定量结果：召回率0.66，NDCG@1000=0.41（测试集）；证实融合检索在ToT任务中的有效性。

Conclusion: 将混合检索、主题分区的稠密检索以及多种再排序信号进行高效融合，能够提升ToT检索性能；合成数据有助于训练鲁棒性，但需注意分布偏差和成本。

Abstract: We develop a two-stage retrieval system that combines multiple complementary retrieval methods with a learned reranker and LLM-based reranking, to address the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3) retrieval methods. We also introduce topic-aware multi-index dense retrieval that partitions the Wikipedia corpus into 24 topical domains. In the second stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking. To support model training, we generate 5000 synthetic ToT queries using LLMs. Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating the effectiveness of fusion retrieval.

</details>


### [121] [Enhancing guidance for missing data in diffusion-based sequential recommendation](https://arxiv.org/abs/2601.15673)
*Qilong Yan,Yifei Xing,Dugang Liu,Jingpu Duan,Jian Yin*

Main category: cs.IR

TL;DR: 提出 CARD：通过双侧 Thompson 采样识别兴趣转折点并用反事实注意力对关键项进行加权，给扩散模型提供高质量引导信号，以改进序列推荐生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在缺失数据下难以保留关键转折点导致生成质量下降；将来自用户兴趣变化的信号放大并抑制噪声以提升可控性。

Method: CARD 由两部分组成：1) 双侧 Thompson 采样识别显著兴趣转移的序列；2) 针对这些序列的反事实注意力机制量化每个项的重要性并对交互向量重新加权，形成高质量的引导向量，供扩散模型使用；整体以 Diffusion 生成框架实现。

Result: 在真实数据集上验证了该方法的有效性，并指出在不显著增加计算成本的情况下获得提升；代码开源。

Conclusion: CARD 能在复杂的序列推荐场景中提升生成质量，通过对关键项进行放大和噪声抑制，提供可解释且高效的引导信号给扩散模型。

Abstract: Contemporary sequential recommendation methods are becoming more complex, shifting from classification to a diffusion-guided generative paradigm. However, the quality of guidance in the form of user information is often compromised by missing data in the observed sequences, leading to suboptimal generation quality. Existing methods address this by removing locally similar items, but overlook ``critical turning points'' in user interest, which are crucial for accurately predicting subsequent user intent. To address this, we propose a novel Counterfactual Attention Regulation Diffusion model (CARD), which focuses on amplifying the signal from key interest-turning-point items while concurrently identifying and suppressing noise within the user sequence. CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences undergoing significant interest shift, and (2) a counterfactual attention mechanism for these sequences to quantify the importance of each item. In this manner, CARD provides the diffusion model with a high-quality guidance signal composed of dynamically re-weighted interaction vectors to enable effective generation. Experiments show our method works well on real-world data without being computationally expensive. Our code is available at https://github.com/yanqilong3321/CARD.

</details>


### [122] [CoNRec: Context-Discerning Negative Recommendation with LLMs](https://arxiv.org/abs/2601.15721)
*Xinda Chen,Jiawei Wu,Yishuang Liu,Jialin Zhu,Shuwen Xiao,Junjun Zheng,Xiangheng Kong,Yuning Jiang*

Main category: cs.IR

TL;DR: 提出一个面向负反馈的LLM框架，通过语义ID表示、项级对齐、渐进GRPO训练来直接建模负偏好，并引入基于多日未来负反馈的奖励函数与评估指标，以缓解正反馈主导导致的上下文偏差。


<details>
  <summary>Details</summary>
Motivation: 理解用户的负偏好对于改进推荐系统、提升用户体验具有重要性。现有工作多以负反馈为辅助信号，且易受正反馈支配的上下文偏差影响，直接建模负兴趣的研究仍较少，且在离线场景具有潜在价值。

Method: 设计一个大语言模型框架用于负反馈建模，核心包括：1) 以语义ID表示替代文本描述；2) 设计项级对齐任务以增强对负反馈语义上下文的理解；3) 提出渐进式GRPO训练范式，动态平衡正负行为上下文的利用；4) 发现并修正次序对下一负项预测的偏差，提出基于多日未来负反馈及其协同信号的奖励函数与评估指标。

Result: 摘要未给出具体实验结果，主要聚焦框架设计、理论洞察与方法学贡献。

Conclusion: 该框架有望直接建模负偏好，缓解负反馈稀疏与正反馈主导的上下文偏差，提升离线应用中的个性化理解与评估鲁棒性；但需在实际数据上验证有效性、鲁棒性及计算成本。

Abstract: Understanding what users like is relatively straightforward; understanding what users dislike, however, remains a challenging and underexplored problem. Research into users' negative preferences has gained increasing importance in modern recommendation systems. Numerous platforms have introduced explicit negative feedback mechanisms and leverage such signals to refine their recommendation models. Beyond traditional business metrics, user experience-driven metrics, such as negative feedback rates, have become critical indicators for evaluating system performance. However, most existing approaches primarily use negative feedback as an auxiliary signal to enhance positive recommendations, paying little attention to directly modeling negative interests, which can be highly valuable in offline applications. Moreover, due to the inherent sparsity of negative feedback data, models often suffer from context understanding biases induced by positive feedback dominance. To address these challenges, we propose the first large language model framework for negative feedback modeling with special designed context-discerning modules. We use semantic ID Representation to replace text-based item descriptions and introduce an item-level alignment task that enhances the LLM's understanding of the semantic context behind negative feedback. Furthermore, we design a Progressive GRPO training paradigm that enables the model to dynamically balance the positive and negative behavioral context utilization. Besides, our investigation further reveals a fundamental misalignment between the conventional next-negative-item prediction objective and users' true negative preferences, which is heavily influenced by the system's recommendation order. To mitigate this, we propose a novel reward function and evaluation metric grounded in multi-day future negative feedback and their collaborative signals.

</details>


### [123] [CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval](https://arxiv.org/abs/2601.15849)
*Tsung-Hsiang Chou,Chen-Jui Yu,Shui-Hsiang Hsu,Yao-Chung Fan*

Main category: cs.IR

TL;DR: CGPT是一种以LLM为监督的表格检索训练框架，通过对表格实例聚类并采样构造部分表格，再用LLM生成合成查询并进行对比学习微调，显著提升表格检索的R@1。


<details>
  <summary>Details</summary>
Motivation: 现有通用嵌入模型在表格检索中存在结构化信息导致的语义压缩和查询-表格错配问题，需引入语义多样的监督信号来提升嵌入模型。

Method: 使用K-means对表格实例聚类，跨簇采样构造语义覆盖广的部分表格；LLM对这些部分表格生成合成查询；将这些查询作为硬负对比学习的监督信号，对嵌入模型进行微调。

Result: 在MimoTable、OTTQA、FetaQA、E2E-WTQ四个基准上，CGPT相较基线（含QGpT）平均R@1提升约16.54%，在统一跨域语料设置中展现良好泛化，并且即使使用较小的LLM也有效。

Conclusion: 语义引导的部分表格构造结合LLM监督的对比学习，为大规模表格检索提供一种高效可扩展的训练范式。

Abstract: General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT.

</details>


### [124] [STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion](https://arxiv.org/abs/2601.15860)
*Shui-Hsiang Hsu,Tsung-Hsiang Chou,Chen-Jui Yu,Yao-Chung Fan*

Main category: cs.IR

TL;DR: STAR is a lightweight semantic table representation framework that uses header-aware clustering and cluster-specific synthetic queries with weighted fusion to align table and query embeddings, yielding higher recall than prior methods.


<details>
  <summary>Details</summary>
Motivation: Structural and semantic misalignment between unstructured text and structured tables; existing methods rely on coarse sampling and simple fusion, limiting semantic coverage and alignment quality.

Method: Header-aware K-means clustering to group semantically similar rows and extract representative centroid instances to form diverse partial tables; generate cluster-specific synthetic queries to cover the table’s semantic space; use weighted fusion to integrate table and query embeddings for fine-grained alignment.

Result: Experiments on five benchmarks show STAR achieves consistently higher Recall than QGpT across all datasets, indicating improved semantic coverage and alignment.

Conclusion: Semantic clustering plus adaptive weighted fusion enhances the expressiveness of table representations; STAR offers a robust and scalable approach with public code.

Abstract: Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR.

</details>


### [125] [MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging](https://arxiv.org/abs/2601.15930)
*Tianjun Wei,Enneng Yang,Yingpeng Du,Huizhong Guo,Jie Zhang,Zhu Sun*

Main category: cs.IR

TL;DR: 提出了 MMGRid 统一框架，用情境网格组织以基模型为基础、在不同情境下微调的 GR 检查点，系统研究生成式推荐系统中的模型合并。发现参数冲突、时序偏置等问题，并给出通过基模型替换解耦参数以及加权上下文合并来缓解的策略；并给出与上下文交互特征相关的最优合并权重的经验性关系。


<details>
  <summary>Details</summary>
Motivation: 随着生成式推荐模型规模快速扩大且训练成本高昂，需在不访问原始训练数据的前提下将具有不同应用情境的模型进行高效合并。尽管 MM 在计算机视觉等领域取得进展，但在推荐系统中的研究仍有限。本工作从情境的角度系统性研究 GR 场景下的模型合并，聚焦来自时间演化和领域差异的多情境模型。

Method: 提出 MMGRid 框架：把基于同一大语言模型的若干 GR 检查点按情境分组，形成结构化的情境网格；所有检查点来自同一基模型并在上下文特定数据上微调，构成可控的多情境模型空间；比较不同合并算法的表现；利用基模型替换实现任务相关与情境特定参数变更的解耦，缓解参数冲突；通过对情境进行加权实现对增量训练带来的时序最近性偏置的平衡；并观察最优合并权重与情境下的交互特征相关性，为实际部署提供权重选取指南。

Result: 1) 以基于 LLM 的 GR 模型在合并时可能出现参数冲突，原因包括 token 分布偏移和目标函数差异；通过基模型替换可有效缓解此冲突。2) 分阶段/增量在不同情境下训练会引入最近性偏置，可通过带权的情境合并进行抵消。3) 实验显示最优合并权重与情境相关的交互特征存在相关性，提供了关于权重选择的实用指引。

Conclusion: MMGRid 提供一个结构化、可控的研究与部署框架，系统性揭示在 GR 场景下多情境模型合并的关键挑战与解决路径；为成本敏感的部署场景提供实际可操作的权重设置原则与设计思路。

Abstract: Model merging (MM) offers an efficient mechanism for integrating multiple specialized models without access to original training data or costly retraining. While MM has demonstrated success in domains like computer vision, its role in recommender systems (RSs) remains largely unexplored. Recently, Generative Recommendation (GR) has emerged as a new paradigm in RSs, characterized by rapidly growing model scales and substantial computational costs, making MM particularly appealing for cost-sensitive deployment scenarios. In this work, we present the first systematic study of MM in GR through a contextual lens. We focus on a fundamental yet underexplored challenge in real-world: how to merge generative recommenders specialized to different real-world contexts, arising from temporal evolving user behaviors and heterogeneous application domains. To this end, we propose a unified framework MMGRid, a structured contextual grid of GR checkpoints that organizes models trained under diverse contexts induced by temporal evolution and domain diversity. All checkpoints are derived from a shared base LLM but fine-tuned on context-specific data, forming a realistic and controlled model space for systematically analyzing MM across GR paradigms and merging algorithms. Our investigation reveals several key insights. First, training GR models from LLMs can introduce parameter conflicts during merging due to token distribution shifts and objective disparities; such conflicts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement. Second, incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging. Notably, we observe that optimal merging weights correlate with context-dependent interaction characteristics, offering practical guidance for weight selection in real-world deployments.

</details>


### [126] [Unveiling and Simulating Short-Video Addiction Behaviors via Economic Addiction Theory](https://arxiv.org/abs/2601.15975)
*Chen Xu,Zhipeng Yi,Ruizi Wang,Wenjie Wang,Jun Xu,Maarten de Rijke*

Main category: cs.IR

TL;DR: 提出一个面向大规模短视频数据的成瘾行为建模框架 AddictSim，通过将经济成瘾理论与推荐系统隐式行为相结合，采用均值-自适应策略和群体相对策略优化进行训练，并在两组数据集上显示优于现有策略且多样性算法可缓解成瘾。


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于问卷或志愿者数据，样本规模有限且存在人口偏差；短视频平台具有海量行为数据，可用于分析成瘾行为的规律。通过将经济成瘾理论与推荐系统捕捉的隐式行为结合，探索成瘾行为的功能性模式及其在平台上的表现，并开发可学习的仿真器用于策略评估与干预。

Method: 提出 AddictSim 框架：以 mean-to-adapted 策略处理个性化成瘾模式，采用 group-relative policy optimization 进行训练；在训练中整合多样性相关算法以缓解成瘾行为；基于两大数据集进行实验评估。

Result: AddictSim 在与现有训练策略的比较中表现出一致的优势；仿真结果表明引入多样性算法可有效缓解成瘾行为。

Conclusion: 研究提出的仿真器和训练框架为理解和缓解短视频平台成瘾提供了理论与实践工具，证实了成瘾具有与传统成瘾相似的功能性模式，并且多样性策略在干预层面具备潜在效用。

Abstract: Short-video applications have attracted substantial user traffic. However, these platforms also foster problematic usage patterns, commonly referred to as short-video addiction, which pose risks to both user health and the sustainable development of platforms. Prior studies on this issue have primarily relied on questionnaires or volunteer-based data collection, which are often limited by small sample sizes and population biases. In contrast, short-video platforms have large-scale behavioral data, offering a valuable foundation for analyzing addictive behaviors. To examine addiction-aware behavior patterns, we combine economic addiction theory with users' implicit behavior captured by recommendation systems. Our analysis shows that short-video addiction follows functional patterns similar to traditional forms of addictive behavior (e.g., substance abuse) and that its intensity is consistent with findings from previous social science studies. To develop a simulator that can learn and model these patterns, we introduce a novel training framework, AddictSim. To consider the personalized addiction patterns, AddictSim uses a mean-to-adapted strategy with group relative policy optimization training. Experiments on two large-scale datasets show that AddictSim consistently outperforms existing training strategies. Our simulation results show that integrating diversity-aware algorithms can mitigate addictive behaviors well.

</details>
