<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.IR](#cs.IR) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Reviewing the Reviewer: Elevating Peer Review Quality through LLM-Guided Feedback](https://arxiv.org/abs/2602.10118)
*Sukannya Purkayastha,Qile Wan,Anne Lauscher,Lizhen Qu,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文针对同行评审中的"懒惰思维"问题，提出了一种LLM驱动的神经符号框架，将评审分解为论证段落并识别多标签问题，通过遗传算法优化反馈模板，使评审质量提升高达92.4%，并发布LazyReviewPlus数据集。


<details>
  <summary>Details</summary>
Motivation: 同行评审是保障科学质量的核心机制，但当前普遍依赖简单启发式方法（即"懒惰思维"）导致评审标准下降。现有研究将懒惰思维检测视为单标签任务，无法处理评审段落中同时存在的清晰度、特异性等多重问题。更重要的是，检测必须转化为可操作的改进建议，而现有方法缺乏准则意识的反馈机制，无法满足实际需求。

Method: 提出三层LLM驱动框架：1）论证分解层，将评审文本拆分为独立论证单元；2）神经符号识别层，融合LLM特征与传统分类器进行多标签问题检测（懒惰思维、特异性等）；3）反馈生成层，基于问题特定模板生成针对性改进建议，并利用遗传算法迭代优化模板质量，实现从检测到改进的闭环。

Result: 实验表明该方法显著优于零样本LLM基线，在评审质量提升方面达到92.4%的改善率。同时发布LazyReviewPlus数据集，包含1,309个句子级标注样本，覆盖懒惰思维和特异性双维度标签，为后续研究提供基准。

Conclusion: 本研究首次将懒惰思维检测拓展为多标签识别任务，通过神经符号融合方法平衡了深度学习的可扩展性与传统方法的精确性。遗传算法优化的反馈模板有效解决了"检测-改进"鸿沟，实验验证了框架的实用价值，数据集的发布将推动同行评审质量自动化评估领域的发展。

Abstract: Peer review is central to scientific quality, yet reliance on simple heuristics -- lazy thinking -- has lowered standards. Prior work treats lazy thinking detection as a single-label task, but review segments may exhibit multiple issues, including broader clarity problems, or specificity issues. Turning detection into actionable improvements requires guideline-aware feedback, which is currently missing. We introduce an LLM-driven framework that decomposes reviews into argumentative segments, identifies issues via a neurosymbolic module combining LLM features with traditional classifiers, and generates targeted feedback using issue-specific templates refined by a genetic algorithm. Experiments show our method outperforms zero-shot LLM baselines and improves review quality by up to 92.4\%. We also release LazyReviewPlus, a dataset of 1,309 sentences labeled for lazy thinking and specificity.

</details>


### [2] [Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens](https://arxiv.org/abs/2602.10229)
*Weihao Liu,Dehai Min,Lu Cheng*

Main category: cs.CL

TL;DR: 针对潜空间推理中的特征崩溃问题，本文提出Latent Thoughts Tuning框架，通过上下文-预测-融合机制整合隐藏状态与词汇嵌入语义指导，并采用三阶段课程学习实现潜/显思维动态切换，显著提升了推理稳定性。


<details>
  <summary>Details</summary>
Motivation: 显式思维链推理受限于离散词汇空间，而现有潜空间推理方法存在特征崩溃和不稳定性，主要因隐藏状态作为输入嵌入时的分布失配及辅助模型对齐问题所致。

Method: 设计Context-Prediction-Fusion机制，联合优化上下文隐藏状态与词汇嵌入空间的预测性语义指导；构建渐进式三阶段课程学习策略，支持潜思维与显思维模式的动态切换。

Result: 在多项潜空间推理基准测试中性能领先，特征崩溃现象显著缓解，推理准确率更加稳健。

Conclusion: LT-Tuning通过重构潜思维构建机制与训练范式，为连续潜空间推理提供了稳定鲁棒的解决方案，是提升大模型推理能力的重要进展。

Abstract: While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy.

</details>


### [3] [Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality](https://arxiv.org/abs/2602.10329)
*Zhimin Hu,Riya Roshan,Sashank Varma*

Main category: cs.CL

TL;DR: 本研究通过可变属性任务探究推理时间扩展是否能使模型自发产生资源理性行为。结果表明，随着复杂度增加，指令微调模型和大型推理模型均会从暴力策略转向分析策略，但后者在XOR/XNOR等复杂逻辑函数上表现更稳健，证实资源理性是推理时间扩展的涌现特性。


<details>
  <summary>Details</summary>
Motivation: 人类推理遵循资源理性原则（在约束条件下优化性能），而推理时间扩展已成为提升大语言模型推理能力的有效范式。然而，尚不清楚这种性能提升是否能在无显式计算成本奖励的情况下自发产生资源理性行为。为此，研究者考察了不同训练范式（指令微调vs强化学习）的模型如何自适应调整推理策略以应对任务复杂度变化。

Method: 设计可变属性任务，要求模型根据候选变量、输入输出示例和预定义逻辑函数推断决定性变量。通过系统操纵候选变量数量和示例数量来调控任务复杂度，对比分析指令微调模型（显式生成长推理步骤）与大型推理模型（通过强化学习训练以最大化准确率）在不同复杂度下的策略迁移与性能表现。

Result: 两种模型均随复杂度增加而出现从暴力枚举到分析推理的策略转变。但在XOR和XNOR等高阶逻辑函数上，指令微调模型性能显著退化，而大型推理模型保持稳健。这表明强化学习训练的模型更能维持复杂推理能力，同时两类模型都展现出无需显式成本奖励的自适应行为。

Conclusion: 研究发现资源理性行为可在无显式计算成本奖励下涌现，支持了推理时间扩展本身即能产生适应性推理机制的观点。这一发现揭示了不同训练范式对复杂推理能力的影响差异，为理解大语言模型的元认知调节机制提供了新证据，对构建更高效的人工智能推理系统具有重要启示。

Abstract: Human reasoning is shaped by resource rationality -- optimizing performance under constraints. Recently, inference-time scaling has emerged as a powerful paradigm to improve the reasoning performance of Large Language Models by expanding test-time computation. Specifically, instruction-tuned (IT) models explicitly generate long reasoning steps during inference, whereas Large Reasoning Models (LRMs) are trained by reinforcement learning to discover reasoning paths that maximize accuracy. However, it remains unclear whether resource-rationality can emerge from such scaling without explicit reward related to computational costs. We introduce a Variable Attribution Task in which models infer which variables determine outcomes given candidate variables, input-output trials, and predefined logical functions. By varying the number of candidate variables and trials, we systematically manipulate task complexity. Both models exhibit a transition from brute-force to analytic strategies as complexity increases. IT models degrade on XOR and XNOR functions, whereas LRMs remain robust. These findings suggest that models can adjust their reasoning behavior in response to task complexity, even without explicit cost-based reward. It provides compelling evidence that resource rationality is an emergent property of inference-time scaling itself.

</details>


### [4] [The Subjectivity of Respect in Police Traffic Stops: Modeling Community Perspectives in Body-Worn Camera Footage](https://arxiv.org/abs/2602.10339)
*Preni Golazizian,Elnaz Rahmati,Jackson Trager,Zhivar Sourati,Nona Ghazizadeh,Georgios Chochlakis,Jose Alcocer,Kerby Bennett,Aarya Vijay Devnani,Parsa Hejabi,Harry G. Muttram,Akshay Kiran Padte,Mehrshad Saadatinia,Chenhao Wu,Alireza S. Zaibari,Michael Sierra-Arévalo,Nick Weller,Shrikanth Narayanan,Benjamin A. T. Graham,Morteza Dehghani*

Main category: cs.CL

TL;DR: 本研究利用洛杉矶警察局执法记录仪视频，构建了首个大规模、多视角（警察关联、司法系统受影响、无关联居民）的交通拦截数据集，通过领域特定评估量表和视角感知建模框架，实现个性化尊重度评分预测与理由生成，提升跨群体预测性能，为执法机构理解社区多元期望提供工具。


<details>
  <summary>Details</summary>
Motivation: 交通拦截是警民最常见互动场景，尊重度是影响公众信任与执法合法性的核心维度，但其解读具有主观性且受生活经验塑造。现有研究缺乏从多元社区视角系统分析警民互动中尊重度差异的大规模数据集和计算方法，难以支持执法机构针对性提升程序正义与公众信任。

Method: 基于程序正义理论、LAPD培训材料与实地调研开发领域特定评估量表；提出量表驱动的偏好数据构建框架以实现视角一致性对齐；设计视角感知建模框架，从交通拦截文本记录中预测个性化尊重度评分并生成针对特定标注者群体的理由。

Result: 在全部三类标注者群体（警察关联、司法系统受影响、无关联居民）上，所提方法均显著提升了尊重度评分预测性能与理由生成的一致性，验证了多视角建模的有效性与跨群体泛化能力。

Conclusion: 该视角感知框架为执法机构理解不同社区对警民互动的差异化期望提供了可扩展工具，有助于针对性地改进执法实践，增强程序合法性与公众信任，为数据驱动的警民关系建设奠定方法学基础。

Abstract: Traffic stops are among the most frequent police-civilian interactions, and body-worn cameras (BWCs) provide a unique record of how these encounters unfold. Respect is a central dimension of these interactions, shaping public trust and perceived legitimacy, yet its interpretation is inherently subjective and shaped by lived experience, rendering community-specific perspectives a critical consideration. Leveraging unprecedented access to Los Angeles Police Department BWC footage, we introduce the first large-scale traffic-stop dataset annotated with respect ratings and free-text rationales from multiple perspectives. By sampling annotators from police-affiliated, justice-system-impacted, and non-affiliated Los Angeles residents, we enable the systematic study of perceptual differences across diverse communities. To this end, we (i) develop a domain-specific evaluation rubric grounded in procedural justice theory, LAPD training materials, and extensive fieldwork; (ii) introduce a rubric-driven preference data construction framework for perspective-consistent alignment; and (iii) propose a perspective-aware modeling framework that predicts personalized respect ratings and generates annotator-specific rationales for both officers and civilian drivers from traffic-stop transcripts. Across all three annotator groups, our approach improves both rating prediction performance and rationale alignment. Our perspective-aware framework enables law enforcement to better understand diverse community expectations, providing a vital tool for building public trust and procedural legitimacy.

</details>


### [5] [Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs](https://arxiv.org/abs/2602.10352)
*Keenan Pepper,Alex McKenzie,Florin Pop,Stijn Servaes,Martin Leitgab,Mike Vaiana,Judd Rosenblatt,Michael S. A. Graziano,Diogo de Lucena*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely frozen, yields reliable self-interpretation across tasks and model families. A scalar affine adapter with just $d_\text{model}+1$ parameters suffices: trained adapters generate sparse autoencoder feature labels that outperform the training labels themselves (71% vs 63% generation scoring at 70B scale), identify topics with 94% recall@1 versus 1% for untrained baselines, and decode bridge entities in multi-hop reasoning that appear in neither prompt nor response, surfacing implicit reasoning without chain-of-thought. The learned bias vector alone accounts for 85% of improvement, and simpler adapters generalize better than more expressive alternatives. Controlling for model knowledge via prompted descriptions, we find self-interpretation gains outpace capability gains from 7B to 72B parameters. Our results demonstrate that self-interpretation improves with scale, without modifying the model being interpreted.

</details>


### [6] [Physically Interpretable AlphaEarth Foundation Model Embeddings Enable LLM-Based Land Surface Intelligence](https://arxiv.org/abs/2602.10354)
*Mashrekur Rahman*

Main category: cs.CL

TL;DR: 本研究基于美国大陆1210万样本（2017-2023），系统解析了Google AlphaEarth卫星基础模型64维嵌入向量的物理可解释性。通过线性、非线性和注意力方法分析26个环境变量，发现嵌入维度与特定地表属性显著相关，并能高保真重建大部分环境变量（12个R²>0.90）。进而开发了检索增强生成系统，LLM评测显示其可将自然语言环境查询转化为卫星-grounded评估（加权评分3.74/5），验证了嵌入向量的物理结构化特性及其业务化应用潜力。


<details>
  <summary>Details</summary>
Motivation: 卫星基础模型产生的密集嵌入向量物理意义不明确，阻碍了其在环境决策系统中的实际应用。亟需系统性可解释性分析以建立从数据驱动表征到物理机制的桥梁，并开发可操作化框架。

Method: 1) 数据层面：采用2017-2023年美国大陆1210万样本；2) 分析方法：对比64维嵌入与26个环境变量（涵盖气候、植被、水文、温度、地形），整合线性、非线性和注意力机制三类方法；3) 验证策略：通过空间分块交叉验证（平均ΔR²=0.017）和七年时间序列稳定性检验（平均年际相关r=0.963）；4) 系统实现：构建FAISS索引的检索增强生成框架，实现自然语言查询到卫星-grounded评估的转换；5) 评估方案：采用LLM-as-Judge范式，4个LLM轮换担任生成器、系统和评判员，完成360次查询-响应循环评估。

Result: 1) 可解释性：单个嵌入维度显著映射特定地表属性，最强关联在三类方法间收敛；2) 重建能力：完整嵌入空间高保真重建环境变量，12/26变量R²>0.90，温度和海拔达R²≈0.97；3) 稳健性：空间验证性能损失微小（ΔR²=0.017），时间稳定性极高（r=0.963）；4) 系统性能：LLM评测加权平均分3.74±0.77，grounding（3.93）和连贯性（4.25）表现最优；5) 物理意义：嵌入向量编码了结构化地表物理信息。

Conclusion: 研究表明卫星基础模型嵌入是物理结构化的地表表征，其可解释性和稳健性支持开发环境地理空间智能系统。该系统成功将自然语言查询转化为卫星-grounded评估，为环境决策提供了新范式，证实从数据驱动到物理可操作化的转化路径可行。

Abstract: Satellite foundation models produce dense embeddings whose physical interpretability remains poorly understood, limiting their integration into environmental decision systems. Using 12.1 million samples across the Continental United States (2017--2023), we first present a comprehensive interpretability analysis of Google AlphaEarth's 64-dimensional embeddings against 26 environmental variables spanning climate, vegetation, hydrology, temperature, and terrain. Combining linear, nonlinear, and attention-based methods, we show that individual embedding dimensions map onto specific land surface properties, while the full embedding space reconstructs most environmental variables with high fidelity (12 of 26 variables exceed $R^2 > 0.90$; temperature and elevation approach $R^2 = 0.97$). The strongest dimension-variable relationships converge across all three analytical methods and remain robust under spatial block cross-validation (mean $ΔR^2 = 0.017$) and temporally stable across all seven study years (mean inter-year correlation $r = 0.963$). Building on these validated interpretations, we then developed a Land Surface Intelligence system that implements retrieval-augmented generation over a FAISS-indexed embedding database of 12.1 million vectors, translating natural language environmental queries into satellite-grounded assessments. An LLM-as-Judge evaluation across 360 query--response cycles, using four LLMs in rotating generator, system, and judge roles, achieved weighted scores of $μ= 3.74 \pm 0.77$ (scale 1--5), with grounding ($μ= 3.93$) and coherence ($μ= 4.25$) as the strongest criteria. Our results demonstrate that satellite foundation model embeddings are physically structured representations that can be operationalized for environmental and geospatial intelligence.

</details>


### [7] [Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation](https://arxiv.org/abs/2602.10356)
*Tianci Xue,Zeyi Liao,Tianneng Shi,Zilu Wang,Kai Zhang,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: 本文提出 ACuRL，一个无需人工数据的计算机使用智能体（CUA）持续学习框架，用于适应动态数字环境。该框架通过自主探索和课程任务生成器合成任务，并使用 CUAJudge 提供奖励信号，在无灾难性遗忘的情况下实现了 4-22% 的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数字环境高度多样且动态，导致智能体频繁遭遇未见场景和分布偏移，因此持续学习对计算机使用智能体至关重要。然而，如何在不依赖昂贵人工标注的情况下获取高质量、环境接地智能体数据是一个关键挑战。

Method: 作者提出了 ACuRL（自主课程强化学习框架）。智能体首先探索目标环境获取初始经验；随后在迭代训练中，课程任务生成器利用这些经验和上一轮反馈，合成适应当前能力的新任务。为提供可靠奖励信号，引入了与人工评判一致性达 93% 的自动评估器 CUAJudge。整个过程无需人工数据。

Result: 该方法有效实现了环境内和环境间的持续学习，在避免灾难性遗忘的同时带来 4-22% 的性能增益。进一步分析显示参数更新高度稀疏（如仅更新 20% 参数），这有助于解释其有效且鲁棒的适应能力。

Conclusion: ACuRL 为计算机使用智能体在无人工干预下适应动态环境提供了实用有效的解决方案，其稀疏更新机制保证了鲁棒性。相关数据和代码已开源。

Abstract: Real-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents (CUAs). However, a key challenge lies in obtaining high-quality and environment-grounded agent data without relying on costly human annotation. In this work, we introduce ACuRL, an Autonomous Curriculum Reinforcement Learning framework that continually adapts agents to specific environments with zero human data. The agent first explores target environments to acquire initial experiences. During subsequent iterative training, a curriculum task generator leverages these experiences together with feedback from the previous iteration to synthesize new tasks tailored for the agent's current capabilities. To provide reliable reward signals, we introduce CUAJudge, a robust automatic evaluator for CUAs that achieves 93% agreement with human judgments. Empirically, our method effectively enables both intra-environment and cross-environment continual learning, yielding 4-22% performance gains without catastrophic forgetting on existing environments. Further analyses show highly sparse updates (e.g., 20% parameters), which helps explain the effective and robust adaptation. Our data and code are available at https://github.com/OSU-NLP-Group/ACuRL.

</details>


### [8] [The Alignment Bottleneck in Decomposition-Based Claim Verification](https://arxiv.org/abs/2602.10380)
*Mahmud Elahi Akhter,Federico Ruggeri,Iman Munire Bilal,Rob Procter,Maria Liakata*

Main category: cs.CL

TL;DR: 本研究针对复杂多面声明验证中结构化声明分解效果不一致的现象，揭示两个关键瓶颈：证据对齐与子声明错误特征。通过构建含时序约束证据及人工标注子声明证据跨度的新数据集，对比子声明对齐证据（SAE）与重复声明级证据（SRE）两种设置，发现分解仅在证据严格对齐时有效，而标准设置会损害性能。进一步研究表明，在噪声标签下，保守弃权策略比激进错误预测更能抑制错误传播。


<details>
  <summary>Details</summary>
Motivation: 结构化声明分解在复杂声明验证中表现不稳定，其根本原因尚未明确。本研究旨在识别并分析两个被忽视的关键因素——证据对齐质量与子声明错误特征，以解释性能差异并提升分解框架的可靠性。

Method: 本研究提出一个新的真实世界复杂声明数据集，该数据集包含时序约束的证据以及人工标注的子声明证据跨度。在此基础上，设计两种证据对齐机制：子声明对齐证据（SAE）与重复声明级证据（SRE），并在PHEMEPlus、MMM-Fact和COVID-Fact等多个跨领域数据集上系统评估分解效果。

Result: 实验结果显示，声明分解仅在采用颗粒度细且严格对齐的证据（SAE）时方能显著提升性能；相反，使用重复声明级证据的标准设置（SRE）不仅无法改善，反而会导致性能下降。此外，当子声明标签存在噪声时，错误类型显著影响下游鲁棒性，其中保守的"弃权"策略较激进的错误预测能更有效地降低错误传播。

Conclusion: 未来声明分解框架应着重优化证据的精确合成，并校准子声明验证模型的标签偏差，从而提升整体系统的性能与鲁棒性。

Abstract: Structured claim decomposition is often proposed as a solution for verifying complex, multi-faceted claims, yet empirical results have been inconsistent. We argue that these inconsistencies stem from two overlooked bottlenecks: evidence alignment and sub-claim error profiles. To better understand these factors, we introduce a new dataset of real-world complex claims, featuring temporally bounded evidence and human-annotated sub-claim evidence spans. We evaluate decomposition under two evidence alignment setups: Sub-claim Aligned Evidence (SAE) and Repeated Claim-level Evidence (SRE). Our results reveal that decomposition brings significant performance improvement only when evidence is granular and strictly aligned. By contrast, standard setups that rely on repeated claim-level evidence (SRE) fail to improve and often degrade performance as shown across different datasets and domains (PHEMEPlus, MMM-Fact, COVID-Fact). Furthermore, we demonstrate that in the presence of noisy sub-claim labels, the nature of the error ends up determining downstream robustness. We find that conservative "abstention" significantly reduces error propagation compared to aggressive but incorrect predictions. These findings suggest that future claim decomposition frameworks must prioritize precise evidence synthesis and calibrate the label bias of sub-claim verification models.

</details>


### [9] [When Tables Go Crazy: Evaluating Multimodal Models on French Financial Documents](https://arxiv.org/abs/2602.10384)
*Virginie Mouilleron,Théo Lasnier,Djamé Seddah*

Main category: cs.CL

TL;DR: 本文介绍了Multimodal Finance Eval，首个用于评估法语金融文档理解的多模态基准测试。该基准包含1204个专家验证的问题，涵盖文本提取、表格理解、图表解读和多轮对话推理。评估发现，尽管VLMs在文本和表格任务上表现良好（85-90%准确率），但在图表解读方面表现不佳（34-62%），且多轮对话中错误会累积传播，导致准确率降至约50%。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在通用文档理解任务中表现良好，但在专业、非英语领域的可靠性尚未得到充分探索，尤其是在金融领域。金融文档混合了密集的监管文本、数值表格和可视化图表，提取错误可能产生现实后果。现有研究缺乏针对法语金融文档的多模态评估基准。

Method: 研究团队构建了Multimodal Finance Eval基准，包含来自真实投资说明书、KIDs和PRIIPs文档的1204个专家验证问题，覆盖文本提取、表格理解、图表解读和多轮对话推理四个维度。采用LLM-as-judge协议评估了六个参数规模从8B到124B的开源权重视觉语言模型。

Result: 评估结果显示，模型在文本和表格任务上表现强劲（准确率85-90%），但在图表解读任务上表现明显较弱（准确率34-62%）。最值得注意的是，多轮对话揭示了严重的缺陷模式：早期错误会在各轮次中传播，无论模型规模大小，整体准确率都降至约50%。

Conclusion: 当前视觉语言模型在明确定义的提取任务上表现有效，但在交互式、多步金融分析中仍显脆弱。Multimodal Finance Eval为衡量和推动这一高风险领域的进展提供了一个具有挑战性的基准。

Abstract: Vision-language models (VLMs) perform well on many document understanding tasks, yet their reliability in specialized, non-English domains remains underexplored. This gap is especially critical in finance, where documents mix dense regulatory text, numerical tables, and visual charts, and where extraction errors can have real-world consequences. We introduce Multimodal Finance Eval, the first multimodal benchmark for evaluating French financial document understanding. The dataset contains 1,204 expert-validated questions spanning text extraction, table comprehension, chart interpretation, and multi-turn conversational reasoning, drawn from real investment prospectuses, KIDs, and PRIIPs. We evaluate six open-weight VLMs (8B-124B parameters) using an LLM-as-judge protocol. While models achieve strong performance on text and table tasks (85-90% accuracy), they struggle with chart interpretation (34-62%). Most notably, multi-turn dialogue reveals a sharp failure mode: early mistakes propagate across turns, driving accuracy down to roughly 50% regardless of model size.
  These results show that current VLMs are effective for well-defined extraction tasks but remain brittle in interactive, multi-step financial analysis. Multimodal Finance Eval offers a challenging benchmark to measure and drive progress in this high-stakes setting.

</details>


### [10] [Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs](https://arxiv.org/abs/2602.10388)
*Zhongzhi Li,Xuansheng Wu,Yijiang Li,Lijie Hu,Ninghao Liu*

Main category: cs.CL

TL;DR: 针对LLM后训练数据多样性问题，本文提出特征激活覆盖率(FAC)指标和FAC Synthesis框架，通过稀疏自编码器识别缺失特征并生成合成样本，在多项任务上提升性能，并发现跨模型族的共享特征空间。


<details>
  <summary>Details</summary>
Motivation: 现有后训练数据多样性度量方法多依赖文本指标，仅能捕捉语言变化，对决定下游性能的任务相关特征信号较弱。鉴于后训练数据多样性对LLM下游性能至关重要，亟需一种能有效衡量任务相关特征多样性的方法。

Method: 提出特征激活覆盖率(FAC)，在可解释特征空间中度量数据多样性。进一步设计FAC Synthesis框架：首先使用稀疏自编码器从种子数据集识别缺失特征，然后显式生成反映这些特征的合成样本。

Result: 在指令遵循、毒性检测、奖励建模和行为引导等任务上，该方法持续提升数据多样性和下游性能。值得注意的是，研究发现LLaMA、Mistral和Qwen等模型族间存在共享的可解释特征空间，实现了跨模型知识迁移。

Conclusion: 本研究为LLM的数据中心优化提供了坚实且实用的方法论，通过特征空间的多样性度量与合成，有效提升模型性能，并为跨模型知识共享提供了新视角。

Abstract: The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.

</details>


### [11] [When are We Worried? Temporal Trends of Anxiety and What They Reveal about Us](https://arxiv.org/abs/2602.10400)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 本研究利用新建立的词汇-焦虑关联词典，分析美加社交媒体数据，揭示焦虑情绪的系统性时间模式：早晨8点最高、中午最低，周中高于周末；过去时态焦虑最强，未来时态最弱；第三人称和主格代词表达更多焦虑。这些发现阐明了时间、时态和人称聚焦方式与焦虑的关系。


<details>
  <summary>Details</summary>
Motivation: 探究人们在社交媒体上何时表现出焦虑情绪，以及这些模式反映了哪些心理和社会学洞察，理解不同时间、时态和人称视角与焦虑的关联。

Method: 使用新开发的词汇-焦虑关联词典，大规模分析美国和加拿大推文数据，通过时间序列分析（一天中不同时段、一周中不同日期）、时态分析（过去、现在、未来）和代词使用分析（第一/二/三人称，主格/宾格）来量化焦虑表达模式。

Result: 发现焦虑呈现系统性变化：早晨8点达峰值（与皮质醇水平一致），中午最低；工作日焦虑高于周末；过去时态焦虑最强，未来时态最弱；第三人称代词比第一、二人称表达更多焦虑；主格代词比宾格代词伴随更高焦虑。

Conclusion: 研究不仅揭示了焦虑的时间模式，还表明不同的认知焦点（时间取向：过去/未来；自我取向：自我/他人；句法角色：主语/宾语）与焦虑水平显著相关，为理解焦虑的心理机制提供了新视角。

Abstract: In this short paper, we make use of a recently created lexicon of word-anxiety associations to analyze large amounts of US and Canadian social media data (tweets) to explore *when* we are anxious and what insights that reveals about us. We show that our levels of anxiety on social media exhibit systematic patterns of rise and fall during the day -- highest at 8am (in-line with when we have high cortisol levels in the body) and lowest around noon. Anxiety is lowest on weekends and highest mid-week. We also examine anxiety in past, present, and future tense sentences to show that anxiety is highest in past tense and lowest in future tense. Finally, we examine the use of anxiety and calmness words in posts that contain pronouns to show: more anxiety in 3rd person pronouns (he, they) posts than 1st and 2nd person pronouns and higher anxiety in posts with subject pronouns (I, he, she, they) than object pronouns (me, him, her, them). Overall, these trends provide valuable insights on not just when we are anxious, but also how different types of focus (future, past, self, outward, etc.) are related to anxiety.

</details>


### [12] [EVOKE: Emotion Vocabulary Of Korean and English](https://arxiv.org/abs/2602.10414)
*Yoonwon Jung,Hagyeong Shin,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 本文介绍了EVOKE，一个包含英语和韩语情感词汇的平行语料库，包含1427个韩语词和1399个英语词，并系统标注了819个韩语和924个英语形容词与动词的多重含义及关系，识别了多义情感词和情感隐喻，是目前最全面、系统且理论无关的双语情感词汇数据集，适用于情感科学、心理语言学、计算语言学和自然语言处理等领域。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏全面、系统且理论无关的韩英双语情感词汇资源，限制了跨文化情感研究和多语言自然语言处理的发展。为此，本文构建了EVOKE数据集，旨在为情感科学、心理语言学、计算语言学和自然语言处理等领域提供一个可灵活适配不同理论视角的实用工具。

Method: 研究团队构建了EVOKE平行语料库，收集了1427个韩语情感词和1399个英语情感词。系统性地对819个韩语和924个英语形容词与动词进行标注，包括多重含义及其关系，识别多义情感词和情感隐喻。同时提供多对多翻译对照和语言特有情感词标注。

Result: EVOKE数据集规模达到1427个韩语词和1399个英语词，标注了819个韩语和924个英语形容词与动词，识别了多义情感词和情感隐喻。据作者所知，这是目前最全面、系统且理论无关的韩英双语情感词汇数据集。

Conclusion: 该数据集已公开于GitHub，可作为情感科学、心理语言学、计算语言学和自然语言处理领域的实用工具，支持研究者根据自身需求和理论视角灵活使用该资源。

Abstract: This paper introduces EVOKE, a parallel dataset of emotion vocabulary in English and Korean. The dataset offers comprehensive coverage of emotion words in each language, in addition to many-to-many translations between words in the two languages and identification of language-specific emotion words. The dataset contains 1,427 Korean words and 1,399 English words, and we systematically annotate 819 Korean and 924 English adjectives and verbs. We also annotate multiple meanings of each word and their relationships, identifying polysemous emotion words and emotion-related metaphors. The dataset is, to our knowledge, the most comprehensive, systematic, and theory-agnostic dataset of emotion words in both Korean and English to date. It can serve as a practical tool for emotion science, psycholinguistics, computational linguistics, and natural language processing, allowing researchers to adopt different views on the resource reflecting their needs and theoretical perspectives. The dataset is publicly available at https://github.com/yoonwonj/EVOKE.

</details>


### [13] [Neuro-Symbolic Synergy for Interactive World Modeling](https://arxiv.org/abs/2602.10480)
*Hongyu Zhao,Siyu Zhou,Haolin Yang,Zengyi Qin,Tianyi Zhou*

Main category: cs.CL

TL;DR: 这篇论文提出NeSyS神经符号协同框架，通过交替训练大语言模型和符号世界模型，利用两者互补优势，在保持预测准确性的同时减少50%训练数据，在ScienceWorld、Webshop和Plancraft三个环境中均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLM)作为世界模型时容易产生幻觉，难以严格遵循确定性转移规则；而符号世界模型虽然逻辑一致，但缺乏语义表达力。亟需一种既能保持LLM的表达能力又能获得符号系统鲁棒性的混合方法。

Method: NeSyS框架采用交替训练策略：1)符号世界模型直接修改LLM的输出概率分布来约束其预测；2)神经世界模型仅在未被符号规则覆盖的轨迹上进行微调。通过互相筛选"解释不足"的轨迹实现协同，融合了LLM的概率语义先验与符号系统的可执行规则。

Result: 在ScienceWorld、Webshop和Plancraft三个交互式环境上的实验表明，NeSyS在保持世界模型预测准确性的前提下，将训练数据需求降低50%，在预测精度和数据效率方面均显著优于各类基线方法。

Conclusion: NeSyS成功弥合了神经模型表达力与符号系统鲁棒性之间的鸿沟，通过神经-符号协同机制实现了数据高效且准确的世界模型，为构建更可靠的人工智能系统提供了新思路。

Abstract: Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.

</details>


### [14] [Canvas-of-Thought: Grounding Reasoning via Mutable Structured States](https://arxiv.org/abs/2602.10494)
*Lingzhuang Sun,Yuxia Zhu,Ruitong Liu,Hao Liang,Zheng Sun,Caijun Jia,Honghao He,Yuchen Wu,Siyuan Li,Jingxuan Wei,Xiangxiang Zhang,Bihui Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: 该论文针对多模态大语言模型中Chain-of-Thought提示的线性序列瓶颈问题，提出Canvas-of-Thought（Canvas-CoT）框架。该方法将HTML Canvas作为外部推理基底，支持DOM原子操作实现原地状态修订，并引入基于渲染的批判循环提供视觉反馈，在VCode、RBench-V和MathVista基准测试中显著优于现有方法，为上下文高效的多模态推理建立了新范式。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法将推理历史视为不可变线性流，局部纠错需生成冗长的下游修正或重写整个上下文，迫使模型隐式维护和追踪状态更新，显著增加token消耗和认知负荷。这一问题在几何、SVG设计等缺乏显式视觉引导的高维领域尤为突出，文本表达无法提供精确的空间推理支持。

Method: 提出Canvas-CoT框架：1) 利用HTML Canvas作为外部可读写推理基底，使模型能够通过DOM增删改查操作实现原子级状态修订，避免干扰上下文；2) 集成基于渲染的批判循环，作为硬约束验证器，为难以文本描述的任务提供显式视觉反馈；3) 将推理过程从一维不可变序列转变为二维可操作画布，支持显式维护"事实真相"。

Result: 在VCode、RBench-V和MathVista三个基准测试的广泛实验表明，Canvas-CoT显著超越现有基线方法，实现了更优的多模态推理性能。该框架大幅降低了上下文长度需求和token消耗，同时提升了在几何、SVG等视觉密集型任务中的推理精度，验证了基于外部视觉基底的上下文高效推理范式。

Conclusion: Canvas-CoT证明了外部视觉空间能够有效克服纯文本推理的局限性，通过支持原位修订和显式视觉反馈，为复杂多模态任务提供了更自然、更高效的解决方案。该研究从不可变线性推理向可变视觉增强推理的范式转变，为未来MLLMs推理架构设计开辟了新方向。

Abstract: While Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), relying solely on linear text sequences remains a bottleneck for complex tasks. We observe that even when auxiliary visual elements are interleaved, they are often treated as static snapshots within a one-dimensional, unstructured reasoning chain. We argue that such approaches treat reasoning history as an immutable stream: correcting a local error necessitates either generating verbose downstream corrections or regenerating the entire context. This forces the model to implicitly maintain and track state updates, significantly increasing token consumption and cognitive load. This limitation is particularly acute in high-dimensional domains, such as geometry and SVG design, where the textual expression of CoT lacks explicit visual guidance, further constraining the model's reasoning precision. To bridge this gap, we introduce \textbf{Canvas-of-Thought (Canvas-CoT)}. By leveraging a HTML Canvas as an external reasoning substrate, Canvas-CoT empowers the model to perform atomic, DOM-based CRUD operations. This architecture enables in-place state revisions without disrupting the surrounding context, allowing the model to explicitly maintain the "ground truth". Furthermore, we integrate a rendering-based critique loop that serves as a hard constraint validator, providing explicit visual feedback to resolve complex tasks that are difficult to articulate through text alone. Extensive experiments on VCode, RBench-V, and MathVista demonstrate that Canvas-CoT significantly outperforms existing baselines, establishing a new paradigm for context-efficient multimodal reasoning.

</details>


### [15] [On the Robustness of Knowledge Editing for Detoxification](https://arxiv.org/abs/2602.10504)
*Ming Dong,Shiyi Tang,Ziyan Peng,Guanyi Chen,Tingting He*

Main category: cs.CL

TL;DR: 本文针对基于知识编辑的LLM去毒化方法，提出了一个以鲁棒性为导向的评估框架，揭示了现有方法在优化鲁棒性、组合鲁棒性和跨语言鲁棒性方面的局限性，发现伪去毒化是常见失败模式，且去毒化效果受模型类型、编辑目标数量和语言种类的限制。


<details>
  <summary>Details</summary>
Motivation: 现有评估过度依赖自动毒性分类器，隐含假设毒性分数降低反映真实行为抑制，但可能无法捕捉退化生成行为导致的伪去毒化现象。

Method: 提出三维鲁棒性评估框架：1) 优化鲁棒性：检验编辑优化过程的稳定性；2) 组合鲁棒性：评估多个不安全行为联合编辑时的性能衰减；3) 跨语言鲁棒性：测试去毒化效果在不同语言间的泛化能力。

Result: 发现伪去毒化（毒性降低源于退化生成而非真实抑制）是常见失败模式；多个不安全行为联合编辑时去毒化效果显著下降；单语和跨语言去毒化仅在特定模型-方法组合下有效。

Conclusion: KE-based去毒化仅在特定模型、有限编辑目标和部分语言上具有鲁棒性，其实际可靠性远低于分类器指标所反映的水平，需更严格的评估标准。

Abstract: Knowledge-Editing-based (KE-based) detoxification has emerged as a promising approach for mitigating harmful behaviours in Large Language Models. Existing evaluations, however, largely rely on automatic toxicity classifiers, implicitly assuming that reduced toxicity scores reflect genuine behavioural suppression. In this work, we propose a robustness-oriented evaluation framework for KE-based detoxification that examines its reliability beyond standard classifier-based metrics along three dimensions: optimisation robustness, compositional robustness, and cross-lingual robustness. We identify pseudo-detoxification as a common failure mode, where apparent toxicity reductions arise from degenerate generation behaviours rather than meaningful suppression of unsafe content. We further show that detoxification effectiveness degrades when multiple unsafe behaviours are edited jointly, and that both monolingual and cross-lingual detoxification remain effective only under specific model-method combinations. Overall, our results indicate that KE-based detoxification is robust only for certain models, limited numbers of detoxification objectives, and a subset of languages.

</details>


### [16] [LHAW: Controllable Underspecification for Long-Horizon Tasks](https://arxiv.org/abs/2602.10525)
*George Pu,Michael S. Lee,Udari Madhushani Sehwag,David J. Lee,Bryan Zhu,Yash Maurya,Mohit Raghavendra,Yuan Xue,Samuel Marc Denton*

Main category: cs.CL

TL;DR: 本文提出LHAW框架，通过系统性地从目标、约束、输入和上下文四个维度移除信息，将明确任务转化为可控的模糊变体，并基于实证智能体试验验证其影响，为长周期工作流智能体的澄清行为评估提供了首个系统性、成本敏感的评估框架。


<details>
  <summary>Details</summary>
Motivation: 长周期工作流智能体是实现真正自主系统的关键，其可靠执行依赖于处理模糊情境并主动寻求澄清的能力。然而，现有研究受限于缺乏可扩展、任务无关的框架来系统性地梳理和衡量模糊性对自定义工作流的影响，阻碍了智能体澄清行为评估的进展。

Method: 提出LHAW（Long-Horizon Augmented Workflows）框架：1）设计模块化、数据集无关的合成管道；2）从目标、约束、输入、上下文四个维度系统性地移除信息，生成可控的模糊变体；3）通过实证智能体试验而非LLM预测验证变体，根据终端状态分歧将其分类为结果关键型、分歧型或良性模糊；4）在TheAgentCompany、SWE-Bench Pro和MCP-Atlas上构建285个任务变体。

Result: 成功构建了包含285个任务变体的数据集，并通过对当前智能体的实证分析，量化了智能体在检测、推理和解决模糊性方面的表现，揭示了现有智能体在长周期模糊情境下的能力局限。

Conclusion: LHAW提供了首个系统性的成本敏感评估框架，为开发可靠的自主系统奠定了方法论基础，推动了长周期智能体澄清行为研究的发展。

Abstract: Long-horizon workflow agents that operate effectively over extended periods are essential for truly autonomous systems. Their reliable execution critically depends on the ability to reason through ambiguous situations in which clarification seeking is necessary to ensure correct task execution. However, progress is limited by the lack of scalable, task-agnostic frameworks for systematically curating and measuring the impact of ambiguity across custom workflows. We address this gap by introducing LHAW (Long-Horizon Augmented Workflows), a modular, dataset-agnostic synthetic pipeline that transforms any well-specified task into controllable underspecified variants by systematically removing information across four dimensions - Goals, Constraints, Inputs, and Context - at configurable severity levels. Unlike approaches that rely on LLM predictions of ambiguity, LHAW validates variants through empirical agent trials, classifying them as outcome-critical, divergent, or benign based on observed terminal state divergence. We release 285 task variants from TheAgentCompany, SWE-Bench Pro and MCP-Atlas according to our taxonomy alongside formal analysis measuring how current agents detect, reason about, and resolve underspecification across ambiguous settings. LHAW provides the first systematic framework for cost-sensitive evaluation of agent clarification behavior in long-horizon settings, enabling development of reliable autonomous systems.

</details>


### [17] [When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning](https://arxiv.org/abs/2602.10560)
*Leheng Sheng,Yongtao Zhang,Wenchang Ma,Yaorui Shi,Ting Huang,Xiang Wang,An Zhang,Ke Shen,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 针对长上下文推理中MemAgent存在的内存爆炸和缺乏退出机制问题，本文提出GRU-Mem，通过文本控制的更新门和退出门实现稳定高效的推理，并采用强化学习训练，获得高达400%的推理加速。


<details>
  <summary>Details</summary>
Motivation: 尽管长上下文推理对实际应用至关重要，但LLMs面临性能随上下文长度增加而退化的挑战。现有MemAgent采用RNN式循环处理并更新文本记忆，但存在两大关键缺陷：(i)无差别更新导致内存迅速爆炸；(ii)缺乏退出机制，在收集足够证据后仍进行不必要的计算。因此，亟需开发更稳定高效的长上下文推理方法。

Method: 提出GRU-Mem，引入两个文本控制门：更新门控制记忆更新时机，仅在门开启时更新；退出门控制循环终止，一旦开启立即退出。通过端到端强化学习引入两个奖励信号r^update和r^exit，分别奖励正确的更新和退出行为，赋予模型相应能力。

Result: 在多种长上下文推理任务上的实验表明，GRU-Mem有效且高效，性能普遍优于MemAgent，推理速度最高提升400%。

Conclusion: GRU-Mem通过门控机制有效解决了长上下文推理中的内存爆炸和无效计算问题，实现了更稳定高效的推理性能，显著优于基线方法。

Abstract: While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals $r^{\text{update}}$ and $r^{\text{exit}}$ within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\% times inference speed acceleration.

</details>


### [18] [Online Causal Kalman Filtering for Stable and Effective Policy Optimization](https://arxiv.org/abs/2602.10609)
*Shuo He,Lang Feng,Xin Cheng,Lei Feng,Bo An*

Main category: cs.CL

TL;DR: 本文针对大语言模型强化学习中token级重要性采样比率方差过高导致策略优化不稳定的问题，提出了一种在线因果卡尔曼滤波策略优化方法(KPO)。该方法将重要性采样比率建模为跨token演化的隐状态，仅基于历史token状态进行自回归在线更新，在保留token级局部结构感知变化的同时平滑噪声尖峰，从而在挑战性数学推理数据集上实现了比现有最优方法更优的稳定策略更新效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习中的token级重要性采样比率存在高方差问题，会破坏大规模策略优化的稳定性。现有方法要么使用固定的序列级比率，要么单独调整每个token的比率，都忽略了序列中token间的时序离策略推导。这种局部离策略偏差会导致相邻token间的策略梯度更新失真，最终引发训练崩溃，亟需一种能够保持局部结构同时稳定更新的解决方案。

Method: 提出KPO方法，将期望的重要性采样比率建模为跨token演化的隐状态，采用卡尔曼滤波器仅基于历史token状态进行因果性、自回归的在线更新。该方法在不依赖未来token信息的前提下，对IS比率进行动态平滑，既保留token级的局部结构感知变化，又有效抑制了噪声峰值，从而实现了更稳定有效的策略优化。

Result: 在具有挑战性的数学推理数据集上，KPO方法相比现有最优方法取得了显著更优的实验结果，验证了其在稳定性和有效性方面的优势。

Conclusion: KPO通过在线因果卡尔曼滤波机制，成功解决了token级IS比率的高方差和局部结构不一致问题，为大语言模型强化学习提供了一种更稳定有效的策略优化框架，特别是在需要长序列推理的任务中展现出重要潜力。

Abstract: Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.

</details>


### [19] [UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory](https://arxiv.org/abs/2602.10652)
*Yongshi Ye,Hui Jiang,Feihu Jiang,Tian Lan,Yichao Du,Biao Fu,Xiaodong Shi,Qianghuai Jia,Longyue Wang,Weihua Luo*

Main category: cs.CL

TL;DR: 这篇论文提出UMEM框架，通过联合优化记忆提取和管理，并引入语义邻域建模和GRPO奖励机制，解决了LLM智能体记忆泛化性差的问题，在五个基准测试中显著超越现有方法，性能提升最高达10.67%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆方法将记忆提取视为静态过程而仅优化记忆管理，导致智能体积累实例特定的噪声而非鲁棒的记忆，泛化能力差。为解决此问题，需要实现记忆提取与管理的紧密协同优化。

Method: 提出统一记忆提取与管理（UMEM）框架，通过语义邻域建模（Semantic Neighborhood Modeling）和基于GRPO的邻域级边际效用奖励，联合优化大型语言模型同时进行记忆提取和管理，确保记忆在语义相关查询簇上的泛化能力。

Result: 在五个基准测试的广泛实验中，UMEM显著超越高度竞争的基线方法，在多轮交互任务中实现高达10.67%的性能提升，并在持续演化过程中保持单调增长曲线。

Conclusion: UMEM框架通过联合优化记忆提取与管理，有效解决了记忆泛化性问题，为LLM智能体的长期演化提供了有效方案，代码和模型将开源。

Abstract: Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.

</details>


### [20] [Benchmarks Are Not That Out of Distribution: Word Overlap Predicts Performance](https://arxiv.org/abs/2602.10657)
*Woojin Chung,Jeonghoon Kim*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding what constitutes high-quality pre-training data remains a central question in language model training. In this work, we investigate whether benchmark performance is primarily driven by the degree of statistical pattern overlap between pre-training corpora and evaluation datasets. We measure this overlap using word-level unigram cross-entropy and word frequency statistics, and perform controlled experiments across $10$ zero-shot benchmarks, $4$ pre-training datasets spanning $8.5\mathrm{B}$ to $60\mathrm{B}$ tokens, and model sizes ranging from $400\mathrm{M}$ to $3\mathrm{B}$ parameters. Our results demonstrate a robust inverse relationship between word-level unigram cross-entropy and benchmark performance, suggesting that widely used benchmarks are strongly influenced by word overlap between training and evaluation data. Thus, larger pre-training subsets with similar word-level unigram cross-entropy yield improved downstream results, indicating that word frequency statistics play an additional role in shaping benchmark scores. Taken together, these results suggest that many standard benchmarks are only weakly out-of-distribution relative to pre-training corpora, so that simple word-overlap statistics predict benchmark performance.

</details>


### [21] [Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment](https://arxiv.org/abs/2602.10661)
*Daniel Gallagher,Gerhard Heyer*

Main category: cs.CL

TL;DR: 本文系统评估了Transformer语言模型在格鲁吉亚语分裂作通格系统中的表现，构建了370个句法测试数据集，发现模型在作格标记上性能最差，在主格上最优，性能与格标记频率分布高度相关（主格 > 与格 > 作格），数据稀缺性和作格的独特语法角色是主要挑战。


<details>
  <summary>Details</summary>
Motivation: 格鲁吉亚语的分裂作通格系统极为罕见，现有研究对Transformer模型处理此类特殊语法现象的能力了解有限。同时，低资源语言普遍缺乏句法评估基准，本研究旨在探索此类语言的模型评估方法。

Method: 利用Grew查询语言基于树库自动生成最小对比对，创建了包含7个任务、370个句法测试的数据集。评估了5个编码器-only和2个解码器-only模型，采用词级和句级准确率作为评估指标。

Result: 实验表明，无论句法结构如何变化，模型在作格分配上表现最差，主格分配上表现最佳，性能与三种格标记的频率分布显著相关（主格 > 与格 > 作格）。数据稀缺性和作格的高度特异性共同导致了其识别困难。

Conclusion: 本研究公开了数据集，提出了低资源语言句法评估的新方法，揭示了当前模型在处理罕见语法现象时的局限性，强调需要更多针对性数据和改进模型设计。

Abstract: This paper evaluates the performance of transformer-based language models on split-ergative case alignment in Georgian, a particularly rare system for assigning grammatical cases to mark argument roles. We focus on subject and object marking determined through various permutations of nominative, ergative, and dative noun forms. A treebank-based approach for the generation of minimal pairs using the Grew query language is implemented. We create a dataset of 370 syntactic tests made up of seven tasks containing 50-70 samples each, where three noun forms are tested in any given sample. Five encoder- and two decoder-only models are evaluated with word- and/or sentence-level accuracy metrics. Regardless of the specific syntactic makeup, models performed worst in assigning the ergative case correctly and strongest in assigning the nominative case correctly. Performance correlated with the overall frequency distribution of the three forms (NOM > DAT > ERG). Though data scarcity is a known issue for low-resource languages, we show that the highly specific role of the ergative along with a lack of available training data likely contributes to poor performance on this case. The dataset is made publicly available and the methodology provides an interesting avenue for future syntactic evaluations of languages where benchmarks are limited.

</details>


### [22] [Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents](https://arxiv.org/abs/2602.10715)
*Yifei Li,Weidong Guo,Lingling Zhang,Rongman Xu,Muye Huang,Hui Liu,Lijiao Xu,Yu Xu,Jun Liu*

Main category: cs.CL

TL;DR: 该论文提出LoCoMo-Plus基准，用于评估LLM对话系统在长对话中保持和应用隐式用户约束（状态、目标、价值观）的认知记忆能力。现有基准仅关注表层事实回忆，而该工作揭示传统字符串匹配指标和显式提示在此场景下存在评估失准，并提出基于约束一致性的统一框架。实验证明认知记忆仍具挑战性，且新基准能暴露现有方法未捕捉的模型缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统基准测试聚焦于表层事实回忆，忽略了真实交互中依赖隐式用户状态、目标和价值观等约束的响应需求。这种评估缺口导致模型在实际应用中难以维持对话连贯性和个性化，因此亟需一个能够测评认知记忆能力的基准，以推动对话系统向深层语义理解演进。

Method: 本文构建LoCoMo-Plus基准，通过设计线索-触发语义断连的长对话场景，迫使模型保留并应用潜在约束。针对传统字符串匹配指标与显式任务提示的不匹配问题，提出基于约束一致性的统一评估框架，通过量化约束保持与应用的准确性来测评认知记忆性能。

Result: 在多样骨干模型、检索方法和记忆系统上的实验表明，认知记忆任务仍面临显著挑战，模型性能远未达预期。相比传统指标，约束一致性框架揭示出明显的性能差距，证实了新评估方法的必要性。该基准成功识别出现有评估体系未能捕捉的模型失败模式。

Conclusion: 研究证实长对话认知记忆是核心但尚未解决的能力。LoCoMo-Plus基准及其约束一致性评估框架提供了更贴近真实场景的测评工具，为未来对话系统在隐式约束理解、长期一致性保持等方面的改进指明了方向。

Abstract: Long-term conversational memory is a core capability for LLM-based dialogue systems, yet existing benchmarks and evaluation protocols primarily focus on surface-level factual recall. In realistic interactions, appropriate responses often depend on implicit constraints such as user state, goals, or values that are not explicitly queried later. To evaluate this setting, we introduce \textbf{LoCoMo-Plus}, a benchmark for assessing cognitive memory under cue--trigger semantic disconnect, where models must retain and apply latent constraints across long conversational contexts. We further show that conventional string-matching metrics and explicit task-type prompting are misaligned with such scenarios, and propose a unified evaluation framework based on constraint consistency. Experiments across diverse backbone models, retrieval-based methods, and memory systems demonstrate that cognitive memory remains challenging and reveals failures not captured by existing benchmarks. Our code and evaluation framework are publicly available at: https://github.com/xjtuleeyf/Locomo-Plus.

</details>


### [23] [Macaron: Controlled, Human-Written Benchmark for Multilingual and Multicultural Reasoning via Template-Filling](https://arxiv.org/abs/2602.10732)
*Alaa Elsetohy,Sama Hadhoud,Haryo Akbarianto Wibowo,Chenxi Whitehouse,Genta Indra Winata,Fajri Koto,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 这篇论文提出了Macaron，一个模板优先的多语言基准测试，用于评估模型在文化接地前提下的推理能力。该基准通过100个语言无关模板覆盖7种推理类型和22种文化方面，包含11,862个实例，涵盖20种语言和20个国家/文化背景。零-shot评估显示，推理模式模型表现最佳，在英语和地方语言间近乎持平，而开源模型在地方语言中性能显著下降。文化接地的数学和计数模板是最难的。


<details>
  <summary>Details</summary>
Motivation: 当前多语言基准测试很少测试基于文化接地前提的推理能力。现有方法存在两个主要问题：翻译数据集保留英语中心场景，而文化优先数据集往往缺乏对所需推理类型的控制。这导致难以评估模型在不同文化背景下的真实推理能力。

Method: 作者提出Macaron，一个模板优先的基准测试，通过因子化推理类型和文化方面来构建问题。使用100个语言无关模板覆盖7种推理类型和22种文化方面，由本地标注员创建与场景对齐的英语和地方语言选择题，并系统推导出真假题。该基准包含11,862个实例，涵盖20个国家/文化背景、10种文字系统和20种语言（包括阿姆哈拉语、约鲁巴语、祖鲁语、吉尔吉斯语等低资源语言）。

Result: 对21个多语言LLM的零-shot评估显示：推理模式模型性能最强，在英语和地方语言间达到近乎持平；开源模型在地方语言中性能显著下降，真假题任务上常接近随机猜测；文化接地的数学和计数模板始终是最具挑战性的。

Conclusion: Macaron为评估多语言模型的文化推理能力提供了有价值的工具，揭示了当前模型在低资源语言和文化接地任务上的局限性。研究强调了开发更均衡的多语言推理模型的必要性，并为未来研究提供了可访问的数据资源。

Abstract: Multilingual benchmarks rarely test reasoning over culturally grounded premises: translated datasets keep English-centric scenarios, while culture-first datasets often lack control over the reasoning required. We propose Macaron, a template-first benchmark that factorizes reasoning type and cultural aspect across question languages. Using 100 language-agnostic templates that cover 7 reasoning types, 22 cultural aspects, native annotators create scenario-aligned English and local-language multiple-choice questions and systematically derived True/False questions. Macaron contains 11,862 instances spanning 20 countries/cultural contexts, 10 scripts, and 20 languages (including low-resource ones like Amharic, Yoruba, Zulu, Kyrgyz, and some Arabic dialects). In zero-shot evaluation of 21 multilingual LLMs, reasoning-mode models achieve the strongest performance and near-parity between English and local languages, while open-weight models degrade substantially in local languages and often approach chance on T/F tasks. Culture-grounded mathematical and counting templates are consistently the hardest. The data can be accessed here https://huggingface.co/datasets/AlaaAhmed2444/Macaron.

</details>


### [24] [Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs](https://arxiv.org/abs/2602.10740)
*Yuming Yan,Shuo Yang,Kai Tang,Sihong Chen,Yang Zhang,Ke Xu,Dan Hu,Qun Yu,Pengfei Hu,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: 该论文针对视觉语言模型在专业领域适应性差且微调易导致灾难性遗忘的问题，提出了一种新颖的强化课程预对齐(RCPA)后训练范式，通过课程感知的渐进调制机制，在早期对模型输出施加部分约束以安全学习新领域知识，后期逐步过渡到完全生成优化，有效平衡了领域知识获取与通用多模态能力保持，实验验证了其在专业领域和通用基准上的有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型虽具备强大的通用能力，但在医疗影像、几何推理等专业领域表现不佳。监督微调虽能提升目标域性能，但会引发灾难性遗忘，限制泛化能力。虽然持续预训练对扩展大语言模型知识有效，但对视觉语言模型而言，由于计算成本过高且多数开源模型缺乏预训练数据而难以实施。此外，基于强化学习的方法（如GRPO）虽能保留通用能力，但在模型初始缺乏领域知识时易发生优化崩溃。因此，亟需高效的既能适应新领域又能保持通用能力的后训练适应方法。

Method: 提出强化课程预对齐(RCPA)范式，采用课程感知的渐进调制机制。早期阶段施加部分输出约束，安全地将模型暴露于新领域概念；随着模型领域熟悉度提升，训练逐步过渡到完全生成优化，精细化响应并与领域特定偏好对齐。这种分阶段适应策略平衡了领域知识获取与通用多模态能力保持。

Result: 在专业领域和通用基准上的大量实验验证了RCPA的有效性，为构建高性能领域自适应视觉语言模型提供了实用路径。

Conclusion: RCPA通过创新的课程学习机制，成功解决了视觉语言模型领域适应与通用能力保持的权衡问题，为高效的后训练适应提供了新范式，是构建领域自适应多模态大模型的重要进展。

Abstract: Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forgetting, limiting its generalization. The central challenge, therefore, is to adapt VLMs to new domains while preserving their general-purpose capabilities. Continual pretraining is effective for expanding knowledge in Large Language Models (LLMs), but it is less feasible for VLMs due to prohibitive computational costs and the unavailability of pretraining data for most open-source models. This necessitates efficient post-training adaptation methods. Reinforcement learning (RL)-based approaches such as Group Relative Policy Optimization (GRPO) have shown promise in preserving general abilities, yet they often fail in domain adaptation scenarios where the model initially lacks sufficient domain knowledge, leading to optimization collapse. To bridge this gap, we propose Reinforced Curriculum Pre-Alignment (RCPA), a novel post-training paradigm that introduces a curriculum-aware progressive modulation mechanism. In the early phase, RCPA applies partial output constraints to safely expose the model to new domain concepts. As the model's domain familiarity increases, training gradually transitions to full generation optimization, refining responses and aligning them with domain-specific preferences. This staged adaptation balances domain knowledge acquisition with the preservation of general multimodal capabilities. Extensive experiments across specialized domains and general benchmarks validate the effectiveness of RCPA, establishing a practical pathway toward building high-performing and domain-adaptive VLMs.

</details>


### [25] [Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM](https://arxiv.org/abs/2602.10801)
*Haotian Sheng,Heyong Wang,Ming Hong,Hongman He,Junqiu Liu*

Main category: cs.CL

TL;DR: LSCL是一种针对黑盒大语言模型的深度学习方法，通过知识蒸馏框架，利用输入问题、输出答案及token概率信息构建与模型内部知识状态的映射，实现知识边界的量化表达，有效缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的幻觉问题源于其对自身知识边界缺乏认知，而现有研究仅关注白盒模型，对仅提供API访问的黑盒模型缺乏有效方法，限制了其实际应用。

Method: 基于知识蒸馏框架设计深度学习模型，以黑盒LLM的输入问题、输出答案及token概率为输入，建立与内部知识状态的映射关系，实现知识边界量化；针对不支持token概率访问的场景，提出自适应替代方案。

Result: 在多个公开数据集和主流黑盒LLM上的实验表明，LSCL在准确率和召回率上显著优于基线模型，能有效辅助黑盒模型准确表达知识边界，其自适应方法性能接近LSCL且同样超越基线。

Conclusion: LSCL首次系统解决了黑盒LLM知识边界表达问题，为缓解幻觉提供了新范式；该方法具有强普适性，自适应方案确保在有限接口条件下仍保持优越性能，具备重要实用价值。

Abstract: Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs' lack of awareness regarding their stored internal knowledge, preventing them from expressing their knowledge state on questions beyond their internal knowledge boundaries, as humans do. However, existing research on knowledge boundary expression primarily focuses on white-box LLMs, leaving methods suitable for black-box LLMs which offer only API access without revealing internal parameters-largely unexplored. Against this backdrop, this paper proposes LSCL (LLM-Supervised Confidence Learning), a deep learning-based method for expressing the knowledge boundaries of black-box LLMs. Based on the knowledge distillation framework, this method designs a deep learning model. Taking the input question, output answer, and token probability from a black-box LLM as inputs, it constructs a mapping between the inputs and the model' internal knowledge state, enabling the quantification and expression of the black-box LLM' knowledge boundaries. Experiments conducted on diverse public datasets and with multiple prominent black-box LLMs demonstrate that LSCL effectively assists black-box LLMs in accurately expressing their knowledge boundaries. It significantly outperforms existing baseline models on metrics such as accuracy and recall rate. Furthermore, considering scenarios where some black-box LLMs do not support access to token probability, an adaptive alternative method is proposed. The performance of this alternative approach is close to that of LSCL and surpasses baseline models.

</details>


### [26] [Beyond Confidence: The Rhythms of Reasoning in Generative Models](https://arxiv.org/abs/2602.10816)
*Deyuan Liu,Zecheng Wang,Zhanyue Qin,Zhiying Tu,Dianhui Chu,Dianbo Sui*

Main category: cs.CL

TL;DR: 本文提出Token Constraint Bound (δ_TCB)指标，用于量化大语言模型在内部状态扰动下维持下一令牌主导预测不变的能力，揭示传统困惑度无法捕捉的预测不稳定性，为提升上下文稳定性提供新视角。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对输入上下文微小变化敏感，影响可靠性。传统指标如准确率和困惑度无法评估局部预测鲁棒性，因为归一化输出概率掩盖了模型内部状态对扰动的真实抵御能力。

Method: 提出Token Constraint Bound (δ_TCB)，通过测量使主导下一令牌预测显著变化所需的最大内部状态扰动幅度来量化模型鲁棒性，该指标与输出嵌入空间几何结构内在关联。

Result: 实验表明δ_TCB与有效提示工程相关，并在上下文学习和文本生成过程中发现困惑度未能识别的关键预测不稳定性。

Conclusion: δ_TCB提供了一种原则性的补充方法，可用于分析和潜在提升大语言模型预测的上下文稳定性。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as normalized output probabilities can obscure the underlying resilience of an LLM's internal state to perturbations. We introduce the Token Constraint Bound ($δ_{\mathrm{TCB}}$), a novel metric that quantifies the maximum internal state perturbation an LLM can withstand before its dominant next-token prediction significantly changes. Intrinsically linked to output embedding space geometry, $δ_{\mathrm{TCB}}$ provides insights into the stability of the model's internal predictive commitment. Our experiments show $δ_{\mathrm{TCB}}$ correlates with effective prompt engineering and uncovers critical prediction instabilities missed by perplexity during in-context learning and text generation. $δ_{\mathrm{TCB}}$ offers a principled, complementary approach to analyze and potentially improve the contextual stability of LLM predictions.

</details>


### [27] [C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution](https://arxiv.org/abs/2602.10874)
*Binwei Yan,Yifei Fu,Mingjian Zhu,Hanting Chen,Mingxuan Yuan,Yunhe Wang,Hailin Hu*

Main category: cs.CL

TL;DR: 华为诺亚提出C-MOP框架，通过边界感知对比采样(BACS)和动量引导语义聚类(MGSC)解决LLM提示词优化中的噪声与冲突问题，使3B参数通用模型性能超越70B领域专用模型


<details>
  <summary>Details</summary>
Motivation: 现有自动提示词优化方法面临更新信号噪声大和语义冲突严重两大挑战，制约了LLM性能的稳定提升

Method: C-MOP框架包含两个核心组件：1) BACS利用批次级信息挖掘困难负样本、锚点和边界对三元特征，精确刻画决策边界；2) MGSC引入带时间衰减的文本动量机制，从迭代过程中提取稳定共识

Result: 相比PromptWizard和ProTeGi等SOTA方法平均提升1.58%和3.35%，3B激活参数通用LLM性能超越70B领域专用稠密LLM

Conclusion: 该框架通过稳定优化实现了提示词的精确进化，为资源受限场景下的LLM性能提升提供了有效方案，相关代码已开源

Abstract: Automatic prompt optimization is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals. In this research, we propose C-MOP (Cluster-based Momentum Optimized Prompting), a framework that stabilizes optimization via Boundary-Aware Contrastive Sampling (BACS) and Momentum-Guided Semantic Clustering (MGSC). Specifically, BACS utilizes batch-level information to mine tripartite features--Hard Negatives, Anchors, and Boundary Pairs--to precisely characterize the typical representation and decision boundaries of positive and negative prompt samples. To resolve semantic conflicts, MGSC introduces a textual momentum mechanism with temporal decay that distills persistent consensus from fluctuating gradients across iterations. Extensive experiments demonstrate that C-MOP consistently outperforms SOTA baselines like PromptWizard and ProTeGi, yielding average gains of 1.58% and 3.35%. Notably, C-MOP enables a general LLM with 3B activated parameters to surpass a 70B domain-specific dense LLM, highlighting its effectiveness in driving precise prompt evolution. The code is available at https://github.com/huawei-noah/noah-research/tree/master/C-MOP.

</details>


### [28] [Diagnosing Structural Failures in LLM-Based Evidence Extraction for Meta-Analysis](https://arxiv.org/abs/2602.10881)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

TL;DR: 本研究通过结构化诊断框架评估大语言模型在元分析证据提取中的表现，揭示当前模型存在系统性结构缺陷，无法稳定绑定变量角色、统计方法与效应量，在多文档长上下文场景下性能急剧崩溃，难以满足自动化元分析的结构保真度要求。


<details>
  <summary>Details</summary>
Motivation: 系统综述与元分析的核心挑战在于将叙述性科学文献转化为结构化数值记录，该过程依赖于跨文档保持角色、研究方法与效应量的精确归属关系，而非简单的实体识别。尽管大语言模型快速发展，但其能否满足这一结构性要求仍不明确，这成为制约自动化元分析的关键瓶颈。

Method: 研究提出结构化诊断框架，将LLM证据提取能力分解为一系列模式约束查询，按关系复杂度和数值复杂度递进，以精确定位失败点。基于五个科学领域的人工标注语料，采用统一查询套件与评估协议，在单文档和长上下文多文档两种输入模式下，对两种先进大语言模型进行系统性评估。

Result: 实验结果表明：单属性查询性能处于中等水平，但一旦需要稳定绑定变量、角色、统计方法与效应量，性能急剧下降；完整元分析关联三元组的提取可靠性接近于零；长上下文输入进一步加剧性能恶化；下游聚合过程会放大上游微小误差，导致语料库级统计结果不可靠。主要失败模式包括角色颠倒、跨分析绑定漂移、实例压缩和数值误归属。

Conclusion: 当前大语言模型在自动化元分析中的局限性根源在于系统性结构崩溃，而非实体识别错误。模型缺乏必要的结构保真度、关系绑定能力和数值 grounding 机制，无法胜任跨文档的结构化证据提取任务。这一发现为未来模型改进指明了方向，即需增强对科学文献深层逻辑关系的理解和保持能力。

Abstract: Systematic reviews and meta-analyses rely on converting narrative articles into structured, numerically grounded study records. Despite rapid advances in large language models (LLMs), it remains unclear whether they can meet the structural requirements of this process, which hinge on preserving roles, methods, and effect-size attribution across documents rather than on recognizing isolated entities. We propose a structural, diagnostic framework that evaluates LLM-based evidence extraction as a progression of schema-constrained queries with increasing relational and numerical complexity, enabling precise identification of failure points beyond atom-level extraction. Using a manually curated corpus spanning five scientific domains, together with a unified query suite and evaluation protocol, we evaluate two state-of-the-art LLMs under both per-document and long-context, multi-document input regimes. Across domains and models, performance remains moderate for single-property queries but degrades sharply once tasks require stable binding between variables, roles, statistical methods, and effect sizes. Full meta-analytic association tuples are extracted with near-zero reliability, and long-context inputs further exacerbate these failures. Downstream aggregation amplifies even minor upstream errors, rendering corpus-level statistics unreliable. Our analysis shows that these limitations stem not from entity recognition errors, but from systematic structural breakdowns, including role reversals, cross-analysis binding drift, instance compression in dense result sections, and numeric misattribution, indicating that current LLMs lack the structural fidelity, relational binding, and numerical grounding required for automated meta-analysis. The code and data are publicly available at GitHub (https://github.com/zhiyintan/LLM-Meta-Analysis).

</details>


### [29] [The CLEF-2026 FinMMEval Lab: Multilingual and Multimodal Evaluation of Financial AI Systems](https://arxiv.org/abs/2602.10886)
*Zhuohan Xie,Rania Elbadry,Fan Zhang,Georgi Georgiev,Xueqing Peng,Lingfei Qian,Jimin Huang,Dimitar Dimitrov,Vanshikaa Jani,Yuyang Dai,Jiahui Geng,Yuxia Wang,Ivan Koychev,Veselin Stoyanov,Preslav Nakov*

Main category: cs.CL

TL;DR: 本文介绍CLEF 2026的FinMMEval实验室，推出首个多语言多模态金融大语言模型评估框架，涵盖金融考试问答、多语言金融问答(PolyFiQA)和金融决策三项任务，旨在解决现有基准的单语言、纯文本和狭窄子任务局限，推动稳健透明的金融AI系统发展。


<details>
  <summary>Details</summary>
Motivation: 当前金融自然语言处理基准存在明显短板：多为单语言设计、仅支持文本模态、局限于狭窄子任务，无法全面评估金融大模型在真实世界中的多语言、多模态推理和决策能力，亟需构建更全面的评估体系。

Method: 建立FinMMEval 2026评估实验室，设计三项关联任务：金融考试问答、多语言金融问答(PolyFiQA)和金融决策，系统评估模型在多语言多模态条件下的理解、推理和决策表现。

Result: 成功构建首个多语言多模态金融LLM评估框架，提供三项任务的标准化数据集和评估工具，可全面测量模型的推理、泛化和行动能力，支持可重复的金融AI研究。

Conclusion: 该实验室致力于促进稳健、透明且全球包容的金融AI系统开发，通过公开数据集和评估资源推动金融大语言模型的可比性研究与标准化评估。

Abstract: We present the setup and the tasks of the FinMMEval Lab at CLEF 2026, which introduces the first multilingual and multimodal evaluation framework for financial Large Language Models (LLMs). While recent advances in financial natural language processing have enabled automated analysis of market reports, regulatory documents, and investor communications, existing benchmarks remain largely monolingual, text-only, and limited to narrow subtasks. FinMMEval 2026 addresses this gap by offering three interconnected tasks that span financial understanding, reasoning, and decision-making: Financial Exam Question Answering, Multilingual Financial Question Answering (PolyFiQA), and Financial Decision Making. Together, these tasks provide a comprehensive evaluation suite that measures models' ability to reason, generalize, and act across diverse languages and modalities. The lab aims to promote the development of robust, transparent, and globally inclusive financial AI systems, with datasets and evaluation resources publicly released to support reproducible research.

</details>


### [30] [SoftMatcha 2: A Fast and Soft Pattern Matcher for Trillion-Scale Corpora](https://arxiv.org/abs/2602.10908)
*Masataka Yoneda,Yusuke Matsushita,Go Kamoda,Kohei Suenaga,Takuya Akiba,Masaki Waga,Sho Yokoi*

Main category: cs.CL

TL;DR: 本文开发了一种基于后缀数组的高效搜索算法，通过磁盘感知设计与动态语料剪枝技术，在万亿词元级语料库上实现0.3秒内的语义搜索，有效解决了组合爆炸问题，并在FineWeb-Edu数据集上显著超越现有方法，成功应用于检测训练数据污染。


<details>
  <summary>Details</summary>
Motivation: 面对万亿规模自然语言语料库，现有搜索技术在处理语义变异（替换、插入、删除）时遭遇组合爆炸难题，导致搜索延迟随查询长度呈指数增长，无法满足实际应用对亚秒级响应的需求，亟需理论支撑的高效算法。

Method: 采用后缀数组作为核心数据结构，提出两大创新机制：1）磁盘感知的精确查找策略，优化大规模数据I/O访问模式；2）动态语料感知剪枝算法，基于自然语言统计特性实时压缩搜索空间。理论分析证明该方法可抑制查询长度相关的指数级复杂度增长。

Result: 在1.4万亿词元的FineWeb-Edu语料库上，所提方法搜索延迟低于0.3秒，性能显著优于infini-gram、infini-gram mini和SoftMatcha等先进基线。实践应用方面，该方法成功识别出其他方法未能检测到的训练数据基准污染问题。

Conclusion: 该研究通过算法创新与系统优化，实现了超大规模语料库的快速语义搜索，为数据质量管理提供了有效工具，多语言在线演示验证了其广泛适用性与实用价值，为未来研究奠定了重要基础。

Abstract: We present an ultra-fast and flexible search algorithm that enables search over trillion-scale natural language corpora in under 0.3 seconds while handling semantic variations (substitution, insertion, and deletion). Our approach employs string matching based on suffix arrays that scales well with corpus size. To mitigate the combinatorial explosion induced by the semantic relaxation of queries, our method is built on two key algorithmic ideas: fast exact lookup enabled by a disk-aware design, and dynamic corpus-aware pruning. We theoretically show that the proposed method suppresses exponential growth in the search space with respect to query length by leveraging statistical properties of natural language. In experiments on FineWeb-Edu (Lozhkov et al., 2024) (1.4T tokens), we show that our method achieves significantly lower search latency than existing methods: infini-gram (Liu et al., 2024), infini-gram mini (Xu et al., 2025), and SoftMatcha (Deguchi et al., 2025). As a practical application, we demonstrate that our method identifies benchmark contamination in training corpora, unidentified by existing approaches. We also provide an online demo of fast, soft search across corpora in seven languages.

</details>


### [31] [Computational Phenomenology of Temporal Experience in Autism: Quantifying the Emotional and Narrative Characteristics of Lived Unpredictability](https://arxiv.org/abs/2602.10947)
*Kacper Dudzic,Karolina Drożdż,Maciej Wodziński,Anastazja Szuła,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: 本研究通过整合现象学与计算方法，探讨自闭症谱系障碍中的时间性紊乱问题。研究发现自闭症个体面临的核心挑战是生活体验的不可预测性，而非叙事建构本身。三项研究分别采用结构化现象学访谈、自闭症自传语料库计算分析和叙事流测量，揭示了自闭症叙事中"即时性与突然性"时间词汇的负面效价更高，且其叙事模式更接近真实自传而非虚构故事。


<details>
  <summary>Details</summary>
Motivation: 现有研究存在三大局限：1) 以缺陷为中心的医学模型主导；2) 质性研究样本量小；3) 计算研究缺乏现象学基础。这些局限导致难以弥合现象学与计算方法之间的鸿沟，无法全面理解自闭症时间体验的本质特征。

Method: 研究整合三种方法：A) 采用跨诊断时间体验评估工具(TASTE)对自闭症个体进行结构化现象学访谈；B) 构建自闭症自传语料库并进行计算分析；C) 运用叙事流测量复制计算研究，评估自闭症自传的现象学真实性。

Result: 访谈显示自闭症组与对照组最显著差异在于体验的不可预测性。计算分析发现自闭症叙事的时间词汇更具负面效价，特别是"即时性与突然性"类别。离群值分析识别出"不可预测地"、"突然地"、"唐突地"等词汇具有高度负面性。叙事流分析表明自闭症叙事更接近真实自传而非虚构故事。

Conclusion: 自闭症个体面临的时间挑战主要源于生活体验内容的不可预测性，而非叙事建构方式。这一发现挑战了传统的缺陷模型，强调需要从现象学角度理解自闭症时间体验，为开发更具生态效度的干预措施提供了实证基础。

Abstract: Disturbances in temporality, such as desynchronization with the social environment and its unpredictability, are considered core features of autism with a deep impact on relationships. However, limitations regarding research on this issue include: 1) the dominance of deficit-based medical models of autism, 2) sample size in qualitative research, and 3) the lack of phenomenological anchoring in computational research. To bridge the gap between phenomenological and computational approaches and overcome sample-size limitations, our research integrated three methodologies. Study A: structured phenomenological interviews with autistic individuals using the Transdiagnostic Assessment of Temporal Experience. Study B: computational analysis of an autobiographical corpus of autistic narratives built for this purpose. Study C: a replication of a computational study using narrative flow measures to assess the perceived phenomenological authenticity of autistic autobiographies. Interviews revealed that the most significant differences between the autistic and control groups concerned unpredictability of experience. Computational results mirrored these findings: the temporal lexicon in autistic narratives was significantly more negatively valenced - particularly the "Immediacy & Suddenness" category. Outlier analysis identified terms associated with perceived discontinuity (unpredictably, precipitously, and abruptly) as highly negative. The computational analysis of narrative flow found that the autistic narratives contained within the corpus quantifiably resemble autobiographical stories more than imaginary ones. Overall, the temporal challenges experienced by autistic individuals were shown to primarily concern lived unpredictability and stem from the contents of lived experience, and not from autistic narrative construction.

</details>


### [32] [LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules](https://arxiv.org/abs/2602.10993)
*Ivan Vulić,Adam Grycner,Quentin de Laroussilhe,Jonas Pfeiffer*

Main category: cs.CL

TL;DR: LoRA-Squeeze通过先在高秩空间学习表达性解，再利用随机化奇异值分解(RSVD)压缩至低秩，从而改进标准LoRA。该方法避免了直接学习低秩解的限制，在13个文本和10个视觉语言任务上，后训练压缩和训练中秩退火变体均展现出比直接训练更优的性能与尺寸权衡。


<details>
  <summary>Details</summary>
Motivation: 标准LoRA面临诸多挑战：需预先选择最优秩、每个秩需单独调参、异构秩模块部署复杂。现有方法直接学习低秩解存在表达受限问题。受模型压缩思想启发，作者提出先学习高秩解再压缩，而非直接学习约束性低秩解，以突破秩选择困境并提升部署效率。

Method: LoRA-Squeeze采用三阶段流程：1）使用故意设定的高源秩进行微调；2）重构或高效逼近完整的权重更新矩阵；3）采用随机化奇异值分解(RSVD)生成目标低秩的新LoRA模块。包含两种实现：后训练压缩和训练中逐步秩退火变体。

Result: 在23个文本与视觉语言任务上的广泛实验表明：后训练压缩产生的低秩适配器性能优于直接在该秩训练的结果，尤其在目标秩允许少量微调步骤时优势更明显；训练中渐进式秩退火变体持续实现了最佳的LoRA尺寸-性能权衡。

Conclusion: LoRA-Squeeze是一种简单高效的改进方案，通过"先学后压"范式有效解决了秩预选择和部署复杂度问题。核心结论表明：高秩学习+低秩压缩的策略优于直接低秩学习，为参数高效微调提供了新的实用路径，特别是训练中退火变体展现了最优的综合性能。

Abstract: Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Randomized Singular Value Decomposition (RSVD) to create a new, compressed LoRA module at a lower target rank. Extensive experiments across 13 text and 10 vision-language tasks show that post-hoc compression often produces lower-rank adapters that outperform those trained directly at the target rank, especially if a small number of fine-tuning steps at the target rank is allowed. Moreover, a gradual, in-tuning rank annealing variant of LoRA-Squeeze consistently achieves the best LoRA size-performance trade-off.

</details>


### [33] [Linguistic Indicators of Early Cognitive Decline in the DementiaBank Pitt Corpus: A Statistical and Machine Learning Study](https://arxiv.org/abs/2602.11028)
*Artsvik Avetisyan,Sachin Kumar*

Main category: cs.CL

TL;DR: 本研究基于DementiaBank Pitt语料库，通过对比原始文本、词性增强和纯词性句法三种语言表征，评估机器学习模型在语言认知筛查中的效能。研究发现抽象句法特征具有强判别力，受试者级别验证更可靠，结合统计验证为开发透明化筛查工具提供了语言学依据。


<details>
  <summary>Details</summary>
Motivation: 自发语言产生的细微变化是认知衰退的早期标志。识别具有语言学解释性的失智标记物，可支持透明且临床可解释的筛查策略，这对认知障碍的早期发现与干预具有重要意义。

Method: 采用DementiaBank Pitt语料库的自发语音转录文本，构建三种语言表征：原始清洗文本、融合词汇与语法信息的词性增强表征、以及纯词性句法表征。使用逻辑回归与随机森林模型，分别在转录本级别和受试者级别（五折交叉验证，避免说话者重叠）进行评估。通过全局特征重要性分析模型可解释性，并采用Mann-Whitney U检验与Cliff's delta效应量进行非参数统计验证。

Result: 各表征模型表现稳定，句法和语法特征在去除词汇内容后仍具强判别力。受试者级别评估结果更保守但一致性高，尤其词性增强和纯词性表征表现突出。统计分析揭示功能词使用、词汇多样性、句子结构及语篇连贯性存在显著组间差异，与机器学习特征重要性高度吻合。

Conclusion: 抽象语言特征在临床真实评估场景下能有效捕捉早期认知衰退标记。通过结合可解释机器学习与非参数统计验证，研究证实语言学基础特征可支持开发透明可靠的语言认知筛查工具，具有重要临床应用价值。

Abstract: Background: Subtle changes in spontaneous language production are among the earliest indicators of cognitive decline. Identifying linguistically interpretable markers of dementia can support transparent and clinically grounded screening approaches.
  Methods: This study analyzes spontaneous speech transcripts from the DementiaBank Pitt Corpus using three linguistic representations: raw cleaned text, a part-of-speech (POS)-enhanced representation combining lexical and grammatical information, and a POS-only syntactic representation. Logistic regression and random forest models were evaluated under two protocols: transcript-level train-test splits and subject-level five-fold cross-validation to prevent speaker overlap. Model interpretability was examined using global feature importance, and statistical validation was conducted using Mann-Whitney U tests with Cliff's delta effect sizes.
  Results: Across representations, models achieved stable performance, with syntactic and grammatical features retaining strong discriminative power even in the absence of lexical content. Subject-level evaluation yielded more conservative but consistent results, particularly for POS-enhanced and POS-only representations. Statistical analysis revealed significant group differences in functional word usage, lexical diversity, sentence structure, and discourse coherence, aligning closely with machine learning feature importance findings.
  Conclusion: The results demonstrate that abstract linguistic features capture robust markers of early cognitive decline under clinically realistic evaluation. By combining interpretable machine learning with non-parametric statistical validation, this study supports the use of linguistically grounded features for transparent and reliable language-based cognitive screening.

</details>


### [34] [Language Model Inversion through End-to-End Differentiation](https://arxiv.org/abs/2602.11044)
*Kevin Yandoka Denamganaï,Kartic Subr*

Main category: cs.CL

TL;DR: 这篇论文研究了语言模型的可逆性问题，即给定目标输出序列，如何找到能够生成该输出的输入提示。作者提出了一种基于梯度的优化方法，通过将语言模型视为作用于令牌分布序列的函数，实现了对冻结语言模型的端到端求导，并使用梯度下降优化提示。实验表明该方法能可靠高效地优化不同长度的提示。


<details>
  <summary>Details</summary>
Motivation: 尽管对语言模型的研究不断涌现，但很少有方法分析语言模型的可逆性。给定一个语言模型和一个期望的目标输出令牌序列，确定哪些输入提示会产生目标输出仍然是一个未解决的问题。

Method: 将问题形式化为经典的基于梯度的优化问题。首先，提出一种简单算法，使给定的（冻结的）语言模型实现端到端可微，然后通过梯度下降找到优化的提示。核心洞察是将语言模型视为作用于令牌分布序列的函数（而非传统上视为作用于令牌序列的函数）。

Result: 实验和消融研究表明，基于DLM的逆推方法可以可靠且高效地优化长度为10和80的提示，以生成长度为20的目标序列，适用于多个白盒语言模型。

Conclusion: 该论文提出的DLM驱动的逆推方法能够有效地解决语言模型的可逆性问题，为理解和操纵语言模型的内部机制提供了一种新的途径。

Abstract: Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open problem. We formulate this problem as a classical gradient-based optimisation. First, we propose a simple algorithm to achieve end-to-end differentiability of a given (frozen) LM and then find optimised prompts via gradient descent. Our central insight is to view LMs as functions operating on sequences of distributions over tokens (rather than the traditional view as functions on sequences of tokens). Our experiments and ablations demonstrate that our DLM-powered inversion can reliably and efficiently optimise prompts of lengths $10$ and $80$ for targets of length $20$, for several white-box LMs (out-of-the-box).

</details>


### [35] [Embedding Inversion via Conditional Masked Diffusion Language Models](https://arxiv.org/abs/2602.11047)
*Han Xiao*

Main category: cs.CL

TL;DR: 本文将嵌入反演建模为条件掩码扩散过程，通过迭代去噪并行恢复所有token，摒弃了传统的自回归顺序生成方式。该方法利用自适应层归一化将目标嵌入作为条件输入，仅需8次前向传播即可在78M参数模型上实现反演，且无需访问目标编码器。在32个token序列的跨模型测试中，达到了81.3%的token准确率和0.87的余弦相似度。


<details>
  <summary>Details</summary>
Motivation: 传统嵌入反演方法通常采用自回归生成，存在顺序计算效率低下的问题。本文旨在通过并行生成的方式提高反演效率，同时保持高质量的重建结果。

Method: 提出条件掩码扩散框架进行嵌入反演。具体包括：(1)使用掩码扩散语言模型；(2)通过自适应层归一化将目标嵌入作为条件；(3)采用迭代去噪过程并行恢复所有token；(4)仅需8次前向传播；(5)无需访问目标编码器。

Result: 在32个token长度的序列上，跨三个不同的嵌入模型进行测试，实现了81.3%的token级准确率和0.87的余弦相似度。

Conclusion: 该方法成功地将扩散模型应用于嵌入反演任务，通过并行生成显著提升了效率，为嵌入反演提供了一种有效的新范式。

Abstract: We frame embedding inversion as conditional masked diffusion, recovering all tokens in parallel through iterative denoising rather than sequential autoregressive generation. A masked diffusion language model is conditioned on the target embedding via adaptive layer normalization, requiring only 8 forward passes through a 78M parameter model with no access to the target encoder. On 32-token sequences across three embedding models, the method achieves 81.3% token accuracy and 0.87 cosine similarity.

</details>


### [36] [Can Large Language Models Make Everyone Happy?](https://arxiv.org/abs/2602.11091)
*Usman Naseem,Gautam Siddharth Kashyap,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Rafiq Ali*

Main category: cs.CL

TL;DR: 本文提出 MisAlign-Profile，一个统一的基准测试框架，用于系统测量大型语言模型在安全、价值观和文化三个维度间的错位权衡。通过构建包含112个规范领域的 MISALIGNTRADE 数据集，并采用两阶段拒绝采样生成配对响应，该基准揭示了不同模型在跨维度权衡中存在12%-34%的错位率。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试（如SAFETUNEBED、VALUEBENCH、WORLDVIEW-BENCH）仅孤立评估安全、价值观或文化单一维度，无法揭示多维度共存时的相互作用与权衡。近期基于机制可解释性的方法（如MIB）也缺乏系统性刻画跨维度权衡的能力，导致对真实场景中LLM行为偏离人类期望的理解有限。

Method: 作者首先构建 MISALIGNTRADE 数据集，涵盖14个安全、56个价值观和42个文化领域共112个规范类别。每个提示词通过 Gemma-2-9B-it 分类为对象、属性或关系三种正交语义错位类型，并利用 Qwen3-30B-A3B-Instruct-2507 扩展，同时采用 SimHash 指纹去重。随后通过两阶段拒绝采样为每个提示词生成错位与对齐的配对响应。最后，在通用、微调和开源权重LLM上评测该数据集。

Result: 实验显示，不同类别的大型语言模型在跨维度评估中表现出12%至34%的错位权衡现象，证实安全、价值观和文化维度间的冲突普遍存在。

Conclusion: MisAlign-Profile 为系统性量化和理解LLM多维度错位提供了新工具，弥补了现有基准测试的不足，有助于推动更安全、更符合人类期望的模型发展。

Abstract: Misalignment in Large Language Models (LLMs) refers to the failure to simultaneously satisfy safety, value, and cultural dimensions, leading to behaviors that diverge from human expectations in real-world settings where these dimensions must co-occur. Existing benchmarks, such as SAFETUNEBED (safety-centric), VALUEBENCH (value-centric), and WORLDVIEW-BENCH (culture-centric), primarily evaluate these dimensions in isolation and therefore provide limited insight into their interactions and trade-offs. More recent efforts, including MIB and INTERPRETABILITY BENCHMARK-based on mechanistic interpretability, offer valuable perspectives on model failures; however, they remain insufficient for systematically characterizing cross-dimensional trade-offs. To address these gaps, we introduce MisAlign-Profile, a unified benchmark for measuring misalignment trade-offs inspired by mechanistic profiling. First, we construct MISALIGNTRADE, an English misaligned-aligned dataset across 112 normative domains taxonomies, including 14 safety, 56 value, and 42 cultural domains. In addition to domain labels, each prompt is classified with one of three orthogonal semantic types-object, attribute, or relations misalignment-using Gemma-2-9B-it and expanded via Qwen3-30B-A3B-Instruct-2507 with SimHash-based fingerprinting to avoid deduplication. Each prompt is paired with misaligned and aligned responses through two-stage rejection sampling to ensure quality. Second, we benchmark general-purpose, fine-tuned, and open-weight LLMs on MISALIGNTRADE-revealing 12%-34% misalignment trade-offs across dimensions.

</details>


### [37] [Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away](https://arxiv.org/abs/2602.11096)
*Soumya Suvra Ghosal,Souradip Chakraborty,Vaibhav Singh,Furong Huang,Dinesh Manocha,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: SafeThink是一种轻量级推理时防御方法，通过监控推理轨迹并在违反安全阈值时注入优化的简短纠正前缀，可在保持多模态大模型推理性能的同时，将越狱攻击成功率降低30-60%。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习（如GRPO）的显式思维链后训练虽能提升多模态大规模推理模型（MLRMs）的推理能力，但会同时削弱安全对齐并增加越狱成功率，亟需在保持模型能力的同时解决安全性退化问题。

Method: 提出SafeThink，将安全恢复视为满足性约束而非最大化目标。该方法在推理时利用安全奖励模型监控演化的推理轨迹，仅在安全阈值被违反时有条件地注入优化的简短纠正前缀（"Wait, think safely"）。

Result: 在六个开源MLRMs和四个越狱基准测试（JailbreakV-28K、Hades、FigStep、MM-SafetyBench）上的评估显示，SafeThink将攻击成功率降低30-60%（例如LlamaV-o1在JailbreakV-28K上从63.33%降至5.74%，R1-Onevision在Hades上从69.07%降至5.65%），同时保持推理性能（MathVista准确率维持在65%左右）。关键发现：安全恢复通常只需在推理前1-3步进行干预即可重定向生成结果。

Conclusion: SafeThink提供了一种有效的轻量级推理时防御机制，能显著提升多模态大模型对越狱攻击的安全对齐，且早期干预推理过程足以将输出导向安全完成，实现了能力与安全性的平衡。

Abstract: Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constraint rather than a maximization objective. SafeThink monitors the evolving reasoning trace with a safety reward model and conditionally injects an optimized short corrective prefix ("Wait, think safely") only when the safety threshold is violated. In our evaluations across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack success rates by 30-60% (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision: 69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy: 65.20% to 65.00%). A key empirical finding from our experiments is that safety recovery is often only a few steering steps away: intervening in the first 1-3 reasoning steps typically suffices to redirect the full generation toward safe completions.

</details>


### [38] [TEGRA: Text Encoding With Graph and Retrieval Augmentation for Misinformation Detection](https://arxiv.org/abs/2602.11106)
*Géraud Faye,Wassila Ouerdane,Guillaume Gadek,Céline Hudelot*

Main category: cs.CL

TL;DR: 本文提出TEG（文本编码与图）框架，通过从文档中提取结构化图信息并联合编码文本与图，有效整合外部知识以提升虚假信息检测性能；同时引入融入领域知识的扩展版本TEGRA，在多数情况下取得进一步突破。


<details>
  <summary>Details</summary>
Motivation: 虚假信息检测是至关重要且具有挑战性的任务，人工事实核查过程高度依赖外部知识辅助判断，而现有方法仅使用语言模型时难以充分捕捉外部知识，性能存在瓶颈。

Method: 提出TEG（Text Encoding with Graph）新方法，从文档中提取结构化图信息，并对文本和图进行联合编码用于分类；进一步提出TEGRA扩展框架，在TEG基础上集成领域特定知识以增强分类能力。

Result: 大量实验表明，这种混合表示方法相比纯语言模型能显著提升虚假信息检测性能；TEGRA通过整合领域知识，在多数情况下进一步提高了分类准确率。

Conclusion: 本研究验证了将知识库外部信息结构化表示并融入检测模型的有效性，为虚假信息检测提供了新的技术路径，TEGRA的提出进一步证明了领域知识的重要性。

Abstract: Misinformation detection is a critical task that can benefit significantly from the integration of external knowledge, much like manual fact-checking. In this work, we propose a novel method for representing textual documents that facilitates the incorporation of information from a knowledge base. Our approach, Text Encoding with Graph (TEG), processes documents by extracting structured information in the form of a graph and encoding both the text and the graph for classification purposes. Through extensive experiments, we demonstrate that this hybrid representation enhances misinformation detection performance compared to using language models alone. Furthermore, we introduce TEGRA, an extension of our framework that integrates domain-specific knowledge, further enhancing classification accuracy in most cases.

</details>


### [39] [Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning](https://arxiv.org/abs/2602.11149)
*Dawid J. Kopiczko,Sagar Vaze,Tijmen Blankevoort,Yuki M. Asano*

Main category: cs.CL

TL;DR: 研究发现，在固定更新预算下，对少量数据进行多轮重复训练（如128轮400个样本）比单轮大数据训练（1轮51200个样本）更能提升推理语言模型的泛化能力，在AIME'24/25和GPQA基准测试中提升12-26个百分点。训练标记准确率可作为停止准则，避免昂贵的数据扩展。


<details>
  <summary>Details</summary>
Motivation: 监督微调是推理语言模型训练的关键步骤。传统机器学习直觉认为更多独特训练样本带来更好泛化，但这一直觉在大型语言模型微调中的有效性尚不明确。本研究旨在探索更高效的SFT策略，避免昂贵的数据扩展。

Method: 研究采用Olmo3-7B模型，在固定更新预算下对比不同训练策略：多轮次小数据集（128轮×400样本）vs. 单轮次大数据集（1轮×51200样本）。在AIME'24/25和GPQA基准上评估性能，并监测训练标记准确率的变化模式。

Result: 多轮重复训练显著优于单轮大数据训练，性能提升达12-26个百分点，且未引发额外灾难性遗忘。训练标记准确率在完全记忆化时达到饱和，此时额外轮次带来的改进停滞，这一模式在不同设置中一致。

Conclusion: 研究表明，通过训练标记准确率作为停止准则进行多轮重复训练，可替代昂贵的数据扩展，为推理SFT提供实用方案。同时提出"重复优势"现象——完全记忆化与泛化能力提升同时发生——作为理解大模型训练动力学的新开放问题。

Abstract: Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [Discovering Differences in Strategic Behavior Between Humans and LLMs](https://arxiv.org/abs/2602.10324)
*Caroline Wang,Daniel Kasenberg,Kim Stachenfeld,Pablo Samuel Castro*

Main category: cs.AI

TL;DR: 本研究利用AlphaEvolve程序发现工具，从迭代石头剪刀布游戏数据中直接发现可解释的人类和LLM行为模型，揭示前沿LLM比人类表现出更深层次的战略行为。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在社会和战略场景中部署日益增多，理解其行为与人类的分歧点及原因变得至关重要。现有行为博弈论模型无法完全捕捉人类的特异行为或LLM这类黑箱非人类智能体的行为特征。

Method: 研究采用AlphaEvolve这一先进程序发现工具，直接从迭代石头剪刀布游戏数据中发现可解释的行为模型，实现对人类和LLM行为驱动结构因素的开方式发现。

Result: 分析发现前沿大型语言模型在迭代石头剪刀布游戏中表现出比人类更深层次的战略行为能力。

Conclusion: 该结果为理解人类与LLM在战略互动中行为差异的结构性驱动因素奠定了基础。

Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions.

</details>


### [41] [LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation](https://arxiv.org/abs/2602.10367)
*Zhiling Yan,Dingjie Song,Zhe Fang,Yisheng Ji,Xiang Li,Quanzheng Li,Lichao Sun*

Main category: cs.AI

TL;DR: 提出LiveMedBench，一个持续更新的、无污染的、基于评分标准的临床基准测试，通过每周从在线医疗社区收集真实病例并采用多智能体策展框架和自动化评分标准评估框架，解决了现有医疗基准的数据污染和时间错位问题，发现当前大模型在临床推理中表现不佳且普遍存在污染风险。


<details>
  <summary>Details</summary>
Motivation: 现有医疗基准测试存在两大关键限制：1) 数据污染，测试集意外泄露到训练语料库导致性能估计虚高；2) 时间错位，无法捕捉医学知识的快速演进。此外，当前开放域临床推理的评估指标依赖浅层词汇重叠（如ROUGE）或主观的LLM-as-a-Judge评分，均不足以验证临床正确性。因此，临床环境中LLM的部署需要严格可靠的评估。

Method: 提出LiveMedBench，每周从在线医疗社区获取真实临床病例，确保与模型训练数据的严格时间分离。采用多智能体临床策展框架过滤原始数据噪声并根据循证医学原则验证临床完整性。开发自动化基于评分标准的评估框架，将医生响应分解为细粒度、案例特定的标准，实现与专家医生判断更强的一致性。

Result: 基准包含2,756个真实病例，涵盖38个医学专科和多种语言，配有16,702个独特评估标准。对38个大模型的广泛评估显示，最佳模型仅达到39.2%准确率，84%的模型在截止时间后病例上表现下降，证实了普遍的数据污染风险。错误分析发现，35-48%的失败源于无法将医学知识适配患者特定约束，表明情境应用而非事实知识是主要瓶颈。

Conclusion: LiveMedBench为临床LLM评估提供了动态、可靠的解决方案。结果表明当前大模型临床推理能力有限，数据污染问题严重，未来研究应聚焦于提升模型将医学知识应用于具体临床情境的能力，而非单纯扩大知识储备。

Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.

</details>


### [42] [MERIT Feedback Elicits Better Bargaining in LLM Negotiators](https://arxiv.org/abs/2602.10467)
*Jihwan Oh,Murad Aghazada,Yooju Shin,Se-Young Yun,Taehyeon Kim*

Main category: cs.AI

TL;DR: 本文提出了一个以效用反馈为中心的框架来解决大型语言模型在谈判中面临的能力瓶颈。研究创建了AgoraBench基准测试集，包含9种挑战性场景，设计了基于效用理论的人类对齐评估指标，并构建了人类偏好数据集与学习流程。实验表明该方法显著提升了模型的谈判表现、策略深度和对手意识。


<details>
  <summary>Details</summary>
Motivation: 谈判被视为逻辑竞技场，但大型语言模型因战略深度有限且难以适应复杂人性因素而表现不佳，现有基准测试无法充分捕捉这一局限性，因此需要新的评估框架和训练方法来弥合这一差距。

Method: 研究提出了三大贡献：1）AgoraBench基准，涵盖欺骗、垄断等九种挑战性场景以支持多样化策略建模；2）基于效用理论的人类对齐经济指标，包括智能体效用、谈判权力和获取比率；3）人类偏好数据集及包含提示学习和微调的学习流程。

Result: 实验结果显示，基线语言模型的谈判策略往往偏离人类偏好，而所提出的机制显著改善了谈判性能，使模型展现出更深层的战略行为和更强的对手意识。

Conclusion: 该效用反馈中心框架通过构建人类对齐的评估体系和训练流程，有效解决了大型语言模型在谈判任务中的能力缺陷，为未来研究提供了新基准和方法论。

Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.

</details>


### [43] [Abstraction Generation for Generalized Planning with Pretrained Large Language Models](https://arxiv.org/abs/2602.10485)
*Zhenhe Cui,Huaxiang Xia,Hangjun Shen,Kailun Luo,Yong He,Wei Liang*

Main category: cs.AI

TL;DR: 该研究探索了大型语言模型（LLM）能否作为定性数值规划（QNP）的抽象生成器用于广义规划（GP）问题，并提出了一种自动化调试方法来修正抽象错误。通过设计提示协议引导LLM生成抽象特征并转换问题要素，实验表明在自动化调试指导下，部分LLM能有效生成QNP抽象。


<details>
  <summary>Details</summary>
Motivation: 定性数值规划（QNP）是广义规划（GP）的重要抽象模型，用于计算能同时解决多个实例的通用计划。尽管已有研究显示大型语言模型（LLM）可作为广义规划器，但尚未探究其是否能生成QNP抽象以及如何通过自动化调试修复这些抽象。

Method: 提出一个提示协议：输入广义规划领域和训练任务至LLM，提示其生成抽象特征，并将初始状态、动作集和目标进一步抽象为QNP问题。同时设计自动化调试方法检测抽象错误，并引导LLM修正抽象。

Result: 实验结果表明，在自动化调试的适当引导下，部分大型语言模型能够生成有用的定性数值规划（QNP）抽象。

Conclusion: 大型语言模型具备作为定性数值规划抽象生成器的潜力，结合自动化调试方法可有效提升其生成抽象的质量，为广义规划问题提供了新的解决思路。

Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.

</details>


### [44] [Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets](https://arxiv.org/abs/2602.10583)
*Bo Xue,Yunchong Song,Fanghao Shao,Xuekai Zhu,Lin Chen,Luoyi Fu,Xinbing Wang,Zhouhan Lin*

Main category: cs.AI

TL;DR: 本文提出 Flow of SpanS (FOSS)，一个基于 GFlowNets 的跨度生成框架。通过构建动态跨度词汇表和确保 DAG 结构状态空间，FOSS 能够探索多样化的组合路径，在文本生成和知识密集型任务上显著优于现有方法，MAUVE 分数提升达 12.5%。


<details>
  <summary>Details</summary>
Motivation: 标准自回归语言模型的树状状态空间限制了灵活性和表达能力，现有动态词汇方法未能显式建模有向无环图(DAG)结构，导致组合路径探索受限且存在路径偏差。尽管 GFlowNets 擅长探索 DAG 结构空间，但此前工作仍局限于令牌级树状空间。为此，本研究旨在开发一种能充分利用 GFlowNets 优势并构建 DAG 状态空间的跨度生成框架。

Method: 提出 FOSS 框架：1) 通过灵活分割检索文本构建动态跨度词汇表；2) 确保状态空间呈 DAG 结构；3) 利用 GFlowNets 探索多样化的组合路径；4) 采用专用奖励模型生成多样化高质量文本。

Result: 在文本生成任务上，FOSS 相比 Transformer 提升 MAUVE 分数高达 12.5%；在知识密集型任务上取得 3.5% 的增益，持续优于现有最先进方法。扩展实验表明，FOSS 能受益于更大模型、更多数据和更丰富检索语料库，保持对强基线的优势。

Conclusion: FOSS 成功将 GFlowNets 应用于跨度生成，通过构建 DAG 结构状态空间有效克服了树状空间的限制，实现了更灵活的文本生成和更好的泛化能力。该方法为语言模型生成过程提供了新思路，展现了良好的可扩展性和应用前景。

Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.

</details>


### [45] [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.AI

TL;DR: 针对深度强化学习中不可行动作探索问题，本文提出神经符号动作掩码（NSAM）框架。该框架在DRL过程中以最小监督方式自动学习符合领域约束的符号模型，并据此生成动作掩码排除不可行选择，实现了符号推理与深度策略优化的端到端整合，显著提升样本效率并减少约束违反。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在训练和执行时可能探索不可行动作，导致效率低下和安全性问题。现有方法需依赖人工指定的符号接地函数和手动设计的动作掩码，这些先验知识限制了方法的泛化性和实用性，难以适应复杂多变的领域约束。

Method: NSAM创新性地在DRL过程中以最小监督方式自动学习高维状态的符号模型，确保其与给定领域约束一致。基于学习到的符号化状态表示，框架进一步学习动态动作掩码机制，实时排除不可行动作。该方法实现了符号推理与深度策略优化的端到端协同训练，两者相互促进。

Result: 在多个具有约束条件的领域中进行评估，实验结果表明NSAM能够显著提高DRL智能体的样本效率，同时大幅降低约束违反频率，验证了其在复杂约束环境下的有效性。

Conclusion: NSAM框架通过自动学习符号模型和动作掩码，为深度强化学习中的约束处理提供了新的神经符号融合解决方案。该方法不仅提升了学习效率和安全性，还减少了对人工先验知识的依赖，在复杂约束域中具有广泛应用前景。

Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.

</details>


### [46] [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625)
*Nanxu Gong,Haotian Li,Sixun Dong,Jianxun Lian,Yanjie Fu,Xing Xie*

Main category: cs.AI

TL;DR: 该研究系统检验了大型推理模型(LRMs)在心理理论(ToM)任务中的表现，发现推理模型相比非推理模型并不持续占优，甚至表现更差。核心问题包括"慢思考崩溃"现象、对选项匹配捷径的依赖，以及需要适度自适应的推理策略。通过设计S2F自适应推理和T2M捷径预防干预措施，研究证实LRMs在形式推理领域的优势无法直接迁移至社会推理任务。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在数学、编码等正式推理任务中取得显著突破，但这些能力能否迁移至社会认知技能(如心理理论)仍属未知。研究旨在挑战"推理能力跨领域通用"的假设，探究LRMs在ToM这类典型社会推理任务中的表现，揭示模型在社会智能方面的真实局限。

Method: 对九种先进大语言模型进行系统性基准测试，在三个代表性ToM数据集上对比推理模型与非推理模型的表现差异。通过细粒度分析识别性能模式，并设计两种干预方法：慢到快(S2F)自适应推理(动态约束推理长度)和思考到匹配(T2M)捷径预防(移除选项匹配偏差)，以验证并缓解发现的问题。

Result: 推理模型在ToM任务中未展现持续优势，反而在部分情况下劣于非推理模型。三大核心发现：1)慢思考崩溃：响应长度增加导致准确率显著下降，更大的推理预算反而损害性能；2)适度自适应推理有益：限制推理长度可缓解性能衰减，成功模式需要动态调整；3)选项匹配捷径：移除多选选项后推理模型性能显著提升，表明其依赖选项匹配而非真正心理状态推断。干预实验证实S2F和T2M策略能有效改善性能。

Conclusion: 研究结论指出，LRMs在形式推理领域的进展无法完全迁移至心理理论等社会推理任务。实现稳健的ToM能力需要超越现有链式推理方法，开发专门针对社会认知的模型能力，而非简单套用数学/编程领域的推理技术。

Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

</details>


### [47] [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635)
*Keane Ong,Sabri Boughorbel,Luwei Xiao,Chanakya Ekbote,Wei Dai,Ao Qu,Jingyao Wu,Rui Mao,Ehsan Hoque,Erik Cambria,Gianmarco Mengaldo,Paul Pu Liang*

Main category: cs.AI

TL;DR: 本文提出异构感知相对策略优化（HARPO）方法，通过动态调制优势函数平衡异构任务与样本贡献，防止单一任务过度主导优化过程。基于该方法开发的Omnisapiens-7B 2.0社交行为基础模型性能显著优于现有模型，多任务和保留测试集上分别提升16.85%和9.37%，且生成更明确鲁棒的推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有社交AI建模通常孤立处理情感、认知或社交等单一行为维度，导致任务特异性建模训练成本高、泛化能力受限。近期推理强化学习方法虽支持多任务统一训练，但未能显式处理异构行为数据差异，造成优化过程中易受个别任务或样本过度影响，制约模型整体性能与稳定性。

Method: HARPO方法核心是通过调制优势函数实现异构任务平衡，确保策略优化时各任务与样本的贡献权重均衡，避免任何单一任务产生不成比例的主导作用。研究基于HARPO训练并开源Omnisapiens-7B 2.0，作为统一处理多种社交行为任务的70亿参数基础模型。

Result: Omnisapiens-7B 2.0在跨行为任务上取得最优性能，相较现有行为基础模型，多任务设置增益最高16.85%，保留集测试增益9.37%。模型生成更明确且鲁棒的推理轨迹。HARPO对比多种最新强化学习方法，展现出最稳定一致的任务性能表现。

Conclusion: HARPO有效解决了异构行为数据联合学习中的任务平衡难题，使Omnisapiens-7B 2.0成为社交行为处理领域的新基准模型，为开发泛化能力强、鲁棒性高的社会智能AI提供了可扩展的解决方案。

Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.

</details>


### [48] [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)
*Jie Jiang,Yangru Huang,Zeyu Wang,Changping Wang,Yuling Xiong,Jun Zhang,Huan Yu*

Main category: cs.AI

TL;DR: V-STAR框架提出值引导采样与树结构优势强化机制，解决自回归生成式推荐中RL微调的概率-奖励失配问题，有效提升探索效率与RL信号质量，在严格延迟约束下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 自回归生成式推荐模型虽能统一检索与排序，但RL微调面临概率-奖励失配的根本挑战。传统似然主导解码（如束搜索）存在短视偏差，导致：(1)探索不足——低概率分支中的高奖励项被过早剪枝；(2)优势压缩——共享高概率前缀的轨迹奖励高度相关、组内方差低，RL比较信号微弱。

Method: V-STAR构建自演进双组件框架：1）值引导高效解码（VED），识别关键节点并选择性地深化高潜力前缀，避免穷尽搜索；2）Sibling-GRPO，利用诱导树拓扑计算兄弟相对优势，将学习信号聚焦于关键分支决策。

Result: 离线和在线实验表明，V-STAR显著超越SOTA基线，在严格延迟约束下同时提升推荐准确性与候选集多样性。

Conclusion: V-STAR通过值引导与树结构优势设计，有效破解了生成式推荐RL训练中的探索与信号稀疏难题，为低延迟推荐系统提供了可落地的RL微调新范式。

Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

</details>


### [49] [Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act](https://arxiv.org/abs/2602.10802)
*Da-Lun Chen,Prasasthy Balasubramanian,Lauri Lovén,Susanna Pirttikangas,Jaakko Sauvola,Panagiotis Kostakos*

Main category: cs.AI

TL;DR: 本研究通过混合方法调查芬兰奥卢大学IT与电气工程学院61名教职工和37名学生，探究其对生成式AI的认知。研究发现学科共性需求（编程辅助）与特定关切（质量、隐私、学术诚信），提出负责任整合的高层需求与概念框架，为高校在合规前提下应用GenAI提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在高等教育中应用广泛，可通过个性化学习提升认知系统效率，但利益相关者认知受文化、学科和制度背景影响而存在分歧。欧盟AI法案的合规要求进一步凸显了高校需结合利益相关者需求定制GenAI整合方案并回应其关切的紧迫性。

Method: 采用混合研究方法，针对芬兰奥卢大学IT与电气工程学院的61名教职员工和37名学生进行问卷调查与深度访谈。

Result: 识别出共同主题（编程支持需求）与学科特异性主题（质量、隐私、学术诚信担忧），提炼出高层需求集合并构建负责任GenAI整合的概念框架。

Conclusion: 学科特异性要求强调了利益相关者参与的重要性；提出的高层需求和框架为高校在确保监管合规的同时平衡GenAI应用与利益相关者关切提供了实践指引。

Abstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.

</details>


### [50] [See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch](https://arxiv.org/abs/2602.10814)
*Xingyi Zhang,Yulei Ye,Kaifeng Huang,Wenhao Li,Xiangfeng Wang*

Main category: cs.AI

TL;DR: 本文提出ScratchWorld，一个用于评估多模态GUI智能体在Scratch中通过图形界面构建程序能力的基准测试。该基准基于Use-Modify-Create教学框架，包含83个涵盖创建、调试、扩展和计算四大类任务，并采用原始模式（精细拖拽操作）和组合模式（高级语义API）双重交互方式，通过浏览器运行时测试验证程序功能正确性。实验揭示了当前先进多模态语言模型和GUI智能体存在显著推理-执行差距。


<details>
  <summary>Details</summary>
Motivation: 尽管Scratch等块编程环境在低代码教育中占据核心地位，但评估AI智能体通过图形用户界面(GUI)构建程序的能力仍未得到充分探索。现有研究缺乏专门针对教育编程环境的系统性评测基准，无法准确诊断智能体在视觉运动控制和程序推理方面的具体缺陷。

Method: 研究团队构建了ScratchWorld基准，包含83个精心设计的任务，分为创建、调试、扩展和计算四类，扎根于Use-Modify-Create教学框架。为精确诊断失败根源，基准采用两种互补交互模式：原始模式要求细粒度拖拽操作以直接评估视觉运动控制能力；组合模式使用高级语义API以分离程序推理与GUI执行。评估协议通过在浏览器环境中运行测试来验证构建的Scratch程序的功能正确性。

Result: 对先进多模态语言模型和GUI智能体的广泛实验揭示了一个显著的推理-执行差距。尽管这些模型表现出强大的规划能力，但在精细GUI操作方面仍面临持续挑战，显示出当前智能体在实际操作执行层面存在明显短板。

Conclusion: 本研究强调了开发专用基准测试对于推动教育编程环境中GUI智能体发展的重要性。ScratchWorld不仅提供了评估工具，更通过双重交互模式的设计为理解智能体失败机制提供了系统性方法。研究结果指出，缩小推理-执行差距是未来研究的关键方向，需要特别关注细粒度GUI操作能力的提升。

Abstract: Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.

</details>


### [51] [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885)
*Leheng Sheng,Wenchang Ma,Ruixin Hong,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 针对链式思考(CoT)奖励存在的人工标注成本高、静态奖励模型难以适应分布演变和奖励攻击等问题，本文提出RLCER框架，通过自生成和自我演化的评分标准实现无监督的CoT奖励，不仅超越结果导向的RLVR方法，还可作为推理提示进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思考(CoT)在大型语言模型推理中至关重要，但直接奖励CoT面临两大挑战：一是训练奖励模型需大量人工标注，成本高昂；二是静态奖励模型难以适应动态演变的CoT分布，且易受奖励攻击。为此，亟需一种无需人工标注、可自主演化的CoT奖励方法。

Method: 受自演化训练范式启发，提出RLCER（基于自演化评分标准的链式思考监督强化学习），通过模型自身生成并持续演化的评分标准对CoT进行奖励，从而增强结果导向的RLVR框架。

Result: 实验表明，即使在没有结果奖励的情况下，自生成和自我演化的评分标准仍能提供可靠的CoT监督信号，使RLCER性能显著优于结果导向的RLVR方法。

Conclusion: 此外，这些自生成的评分标准可作为推理时的前置提示，在推理阶段进一步提升模型性能，实现双重增益。

Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>


### [52] [Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation](https://arxiv.org/abs/2602.10964)
*F. Carichon,R. Rampa,G. Farnadi*

Main category: cs.AI

TL;DR: 本研究通过菜谱文化适应问题，揭示大语言模型在跨文化内容生成中的根本缺陷：其生成菜谱的文化差异性与实际文化距离无关，且内部文化表征薄弱，无法像人类一样实现真正的文化适应。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能生成文化内容，但存在系统性文化偏见，引发对刻板印象、同质化及文化特异性表达消亡的担忧。理解模型能否真正适应多元文化（尤其是非主流文化）成为关键挑战。

Method: 基于GlobalFusion数据集，将不同文化距离国家的人类菜谱进行配对，使用多国别LLMs为相同国家配对生成文化适应菜谱，实现人机跨文化创作行为的直接对比分析。

Result: LLMs无法生成文化代表性适应内容：其菜谱差异性与文化距离无相关性。内在机制上，文化信息在模型表征中保存薄弱，误解创造力与传统概念导致过度追求新奇性，且无法将适应与目标国家及文化显著元素（如食材）关联。

Conclusion: 研究揭示了当前大语言模型在文化导向生成任务中的根本局限性，对文化敏感应用（如文化创作、跨文化交流）的部署具有重要警示意义，需重新评估其文化适应能力。

Abstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.

</details>


### [53] [CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion](https://arxiv.org/abs/2602.10999)
*Yusong Lin,Haiyang Wang,Shuzhe Wu,Lue Fan,Feiyang Pan,Sanyuan Zhao,Dandan Tu*

Main category: cs.AI

TL;DR: 本文提出CLI-Gym框架，通过将Dockerfile类比为智能体任务，利用智能体模拟探索环境历史，从健康环境状态逆向推导出包含运行时错误的缺陷状态，从而大规模生成环境密集型任务。基于此方法，研究者构建了包含1655个任务的数据集，并训练出LiberCoder模型，在Terminal-Bench上取得46.1%的准确率，较基线提升21.1%，成为首个用于可扩展环境密集型任务生成的公开流水线。


<details>
  <summary>Details</summary>
Motivation: 现有智能体编码需要与命令行接口等运行时环境高效交互以解决依赖问题和系统故障，但如何大规模获取此类环境密集型任务来增强智能体能力仍缺乏充分探索。

Method: 受Dockerfile与智能体任务之间相似性的启发，提出CLI-Gym方法：让智能体在健康环境的执行反馈引导下模拟探索环境历史轨迹，通过追踪环境状态变化逆向推导出包含运行时故障的早期缺陷状态，并将缺陷状态与对应错误信息打包形成任务。

Result: 该方法生成了1655个环境密集型任务，构成同类最大规模数据集。基于精选的成功轨迹微调的LiberCoder模型在Terminal-Bench上达到46.1%的准确率，相比各类强基线模型取得21.1%的绝对性能提升。

Conclusion: 本研究构建了首个公开的可扩展环境密集型任务生成流水线，为提升智能体环境交互能力提供了新思路和方法论。

Abstract: Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke](https://arxiv.org/abs/2602.10119)
*Anjali K. Kapoor,Anton Alyakin,Jin Vivian Lee,Eunice Yang,Annelene M. Schulze,Krithik Vishwanath,Jinseok Lee,Yindalon Aphinyanaphongs,Howard Riina,Jennifer A. Frontera,Eric Karl Oermann*

Main category: cs.LG

TL;DR: 本研究探索了多种大语言模型（LLM）从入院记录直接预测卒中后功能结局的能力。微调Llama模型在90天mRS预测上达到33.9%精确准确率和76.3%二分类准确率，性能与基于NIHSS和年龄的结构化数据模型相当，为开发无缝整合临床工作流的预后工具提供了依据。


<details>
  <summary>Details</summary>
Motivation: 急性缺血性卒中后功能结局的准确预测对临床决策和资源分配至关重要。现有研究主要依赖结构化变量和传统机器学习，大语言模型直接从常规入院记录中推断改良Rankin量表（mRS）评分的潜力尚未充分探索。

Method: 利用纽约大学朗格尼健康中心2016-2025年卒中登记数据，纳入9,485份出院记录和1,898份90天随访记录。评估BERT、NYUTron、Llama-3.1-8B和MedGemma-4B在冻结和微调状态下的预测性能。采用时间分割法，以最远12个月数据作为测试集。主要评价指标为7分类mRS精确准确率和二分类功能结局（mRS 0-2 vs 3-6）准确率，并与基于NIHSS和年龄的结构化数据基线进行比较。

Result: 微调Llama模型表现最优：90天mRS精确准确率33.9%（95%CI 27.9-39.9%），二分类准确率76.3%（95%CI 70.7-81.9%）；出院时预测性能分别为42.0%和75.0%。90天预测性能与结构化数据基线相当。

Conclusion: 微调的大语言模型可仅从入院记录文本预测卒中后功能结局，性能与依赖结构化变量的传统模型相当。该发现支持开发无需手动数据提取、可直接嵌入临床工作流的文本化预后工具。

Abstract: Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of large language models (LLMs) to infer future mRS scores directly from routine admission notes remains largely unexplored. We evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, MedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS prediction using a large, real-world stroke registry. The discharge outcome dataset included 9,485 History and Physical notes and the 90-day outcome dataset included 1,898 notes from the NYU Langone Get With The Guidelines-Stroke registry (2016-2025). Data were temporally split with the most recent 12 months held out for testing. Performance was assessed using exact (7-class) mRS accuracy and binary functional outcome (mRS 0-2 vs. 3-6) accuracy and compared against established structured-data baselines incorporating NIHSS and age. Fine-tuned Llama achieved the highest performance, with 90-day exact mRS accuracy of 33.9% [95% CI, 27.9-39.9%] and binary accuracy of 76.3% [95% CI, 70.7-81.9%]. Discharge performance reached 42.0% [95% CI, 39.0-45.0%] exact accuracy and 75.0% [95% CI, 72.4-77.6%] binary accuracy. For 90-day prediction, Llama performed comparably to structured-data baselines. Fine-tuned LLMs can predict post-stroke functional outcomes from admission notes alone, achieving performance comparable to models requiring structured variable abstraction. Our findings support the development of text-based prognostic tools that integrate seamlessly into clinical workflows without manual data extraction.

</details>


### [55] [Signature-Kernel Based Evaluation Metrics for Robust Probabilistic and Tail-Event Forecasting](https://arxiv.org/abs/2602.10182)
*Benjamin R. Redhead,Thomas L. Lee,Peng Gu,Víctor Elvira,Amos Storkey*

Main category: cs.LG

TL;DR: 为改进概率预测评估，本文提出Sig-MMD和CSig-MMD两种核基指标，前者捕获时空依赖，后者通过删失机制强化尾部事件评估并保持适性性。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架缺乏共识指标，且存在两大致命缺陷：时间/变量独立性假设，以及对尾部事件不敏感——而这在高风险领域决策中至关重要。

Method: 提出基于签名核的两阶段方法：(1) Sig-MMD利用签名核捕捉多变量的时空依赖关系，对缺失数据鲁棒；(2) CSig-MMD引入删失方案，在保持评分规则适性性的前提下优先评估尾部事件预测能力。

Result: 新指标有效解决了传统方法的独立性假设缺陷，显著提升对尾部事件的敏感性，实现了直接多步预测的可靠评估，为开发鲁棒概率算法提供评估基础。

Conclusion: 本研究通过Sig-MMD和CSig-MMD指标，特别是CSig-MMD的删失设计，为高风险领域提供了更优的概率预测评估工具，能更准确地评估模型对关键尾部事件的预测性能，促进预测算法的稳健化发展。

Abstract: Probabilistic forecasting is increasingly critical across high-stakes domains, from finance and epidemiology to climate science. However, current evaluation frameworks lack a consensus metric and suffer from two critical flaws: they often assume independence across time steps or variables, and they demonstrably lack sensitivity to tail events, the very occurrences that are most pivotal in real-world decision-making. To address these limitations, we propose two kernel-based metrics: the signature maximum mean discrepancy (Sig-MMD) and our novel censored Sig-MMD (CSig-MMD). By leveraging the signature kernel, these metrics capture complex inter-variate and inter-temporal dependencies and remain robust to missing data. Furthermore, CSig-MMD introduces a censoring scheme that prioritizes a forecaster's capability to predict tail events while strictly maintaining properness, a vital property for a good scoring rule. These metrics enable a more reliable evaluation of direct multi-step forecasting, facilitating the development of more robust probabilistic algorithms.

</details>


### [56] [Adaptive Optimization via Momentum on Variance-Normalized Gradients](https://arxiv.org/abs/2602.10204)
*Francisco Patitucci,Aryan Mokhtari*

Main category: cs.LG

TL;DR: 本文提出MVN-Grad优化器，通过梯度方差归一化与后归一化动量的解耦设计，消除Adam类算法中陈旧动量与随机归一化器的跨时耦合，理论证明其具有更小的更新方差和离群鲁棒性，在CIFAR-100和GPT语言建模任务上性能优于或匹敌Adam、AdaBelief等基线，且不增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 标准Adam类优化器存在陈旧动量与随机归一化器之间的跨时耦合问题，导致训练不稳定。现有方差归一化方法在动量-归一化顺序安排上存在理论缺陷，对离群梯度敏感，在低方差场景下可能出现符号崩溃现象。

Method: MVN-Grad采用坐标级方差归一化策略，利用梯度不确定性的指数移动平均进行缩放，然后对归一化后的梯度施加动量更新。通过"归一化-动量"的顺序设计，从机制上解耦了两个过程的相互干扰，保持了与Adam相同的计算复杂度。

Result: 理论分析表明，MVN-Grad的单步条件更新方差严格小于动量优先的归一化方法，对单个梯度峰值具有均匀有界的鲁棒响应。在低方差环境下可避免符号崩溃并获得加速收敛。在CIFAR-100图像分类和GPT风格语言建模基准测试中，MVN-Grad的训练曲线更平滑，泛化性能优于或等同于Adam、AdaBelief和LaProp。

Conclusion: MVN-Grad通过创新性地将动量应用于归一化后的梯度，有效解决了Adam类优化器的内在耦合缺陷，兼具理论严谨性与实证有效性，为开发更稳定、高效的自适应优化算法提供了新范式，且不增加额外计算成本。

Abstract: We introduce MVN-Grad (Momentum on Variance-Normalized Gradients), an Adam-style optimizer that improves stability and performance by combining two complementary ideas: variance-based normalization and momentum applied after normalization. MVN-Grad scales each coordinate by an exponential moving average of gradient uncertainty and applies momentum to the resulting normalized gradients, eliminating the cross-time coupling between stale momentum and a stochastic normalizer present in standard Adam-type updates. We prove that this decoupling yields strictly smaller one-step conditional update variance than momentum-then-normalize variance methods under standard noise assumptions, and that MVN-Grad is robust to outliers: it has a uniformly bounded response to single gradient spikes.
  In low-variance regimes, we further show variance normalization avoids sign-type collapse associated with second-moment scaling and can yield accelerated convergence. Across CIFAR-100 image classification and GPT-style language modeling benchmarks, MVN-Grad matches or outperforms Adam, AdaBelief, and LaProp, delivering smoother training and improved generalization with no added overhead.

</details>


### [57] [How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge](https://arxiv.org/abs/2602.10210)
*Junhong Lin,Bing Zhang,Song Wang,Ziyan Liu,Dan Gutfreund,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: 本文提出HybridRAG-Bench，一个用于评估混合知识增强大型语言模型在检索密集型多跳推理任务上的基准测试框架。该框架从arXiv近期文献自动构建非结构化文本与结构化知识图谱，生成基于明确推理路径的知识密集型问答对，支持领域和时间段自定义，有效区分真实检索推理与参数化记忆。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理需要最新信息和多跳推理的知识密集型问题时表现不佳，虽可通过混合外部知识（非结构化文本+结构化知识图谱）增强，但现有基准测试与模型预训练数据重叠严重，导致答案或支撑知识可能已编码在模型参数中，难以区分真实检索推理与参数化记忆，亟需可靠的评估框架。

Method: 开发HybridRAG-Bench框架，自动从arXiv近期科学文献中耦合非结构化文本和结构化知识图谱表示，生成基于显式推理路径的知识密集型问答对。框架支持灵活选择评估领域与时间范围，实现防污染、可定制的评估。

Result: 在人工智能、治理与政策、生物信息学三个领域的实验表明，HybridRAG-Bench能有效奖励真实检索与推理能力，而非参数化记忆，为评估混合知识增强推理系统提供了原则性测试平台。

Conclusion: HybridRAG-Bench为评估混合知识增强的大型语言模型在检索密集型多跳推理任务上提供了有效基准，能区分真实推理与参数记忆，并支持随模型和知识演进的持续评估，代码和数据已开源。

Abstract: Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.

</details>


### [58] [Rank-Accuracy Trade-off for LoRA: A Gradient-Flow Analysis](https://arxiv.org/abs/2602.10212)
*Michael Rushka,Diego Klabjan*

Main category: cs.LG

TL;DR: 本文从动力系统视角理论分析了LoRA的秩与精度之间的关系，通过梯度流分析推导了闭式解，揭示了低秩更新的性能界限。


<details>
  <summary>Details</summary>
Motivation: 前人实证研究表明LoRA在下游微调任务中即使秩为1也能达到与全参数方法相当的精度，但其精度对秩的依赖缺乏理论解释。本文旨在填补这一理论空白。

Method: 采用动力系统视角，对全秩和低秩情形进行梯度流分析，推导LoRA的梯度流方程，并证明同时更新与顺序更新形式相同；进一步针对迹平方和Frobenius范数低秩近似损失函数，得到秩与精度之间的闭式关系。

Result: 得到了LoRA秩与精度之间的显式关系，为两种损失函数提供了闭式表达式，揭示了低秩更新在不同秩下的性能界限。

Conclusion: 为LoRA提供了理论依据，阐明了秩的选择对精度的影响，为实际应用中的秩选择提供了指导。

Abstract: Previous empirical studies have shown that LoRA achieves accuracy comparable to full-parameter methods on downstream fine-tuning tasks, even for rank-1 updates. By contrast, the theoretical underpinnings of the dependence of LoRA's accuracy on update rank remain relatively unexplored. In this work, we compare the accuracy of rank-r LoRA updates against full-parameter updates for fine-tuning tasks from a dynamical systems perspective. We perform gradient flow analysis in both full-rank and low-rank regimes to establish explicit relationships between rank and accuracy for two loss functions under LoRA. While gradient flow equations for LoRA are presented in prior work, we rigorously derive their form and show that they are identical for simultaneous and sequential LoRA parameter updates. We then use the resulting dynamical system equations to obtain closed-form relationships between LoRA rank and accuracy for trace-squared and Frobenius-norm low-rank approximation loss functions.

</details>


### [59] [ELROND: Exploring and decomposing intrinsic capabilities of diffusion models](https://arxiv.org/abs/2602.10216)
*Paweł Skierś,Tomasz Trzciński,Kamil Deja*

Main category: cs.LG

TL;DR: 本文提出一种在输入嵌入空间中解耦语义方向的框架，通过反向传播收集固定提示随机实现的梯度差异，并利用主成分分析或稀疏自编码器分解，提取可操控语义方向，实现细粒度控制、缓解模式崩溃，并提供概念复杂度估计。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的随机性导致单一文本提示产生不可控的多样化输出，用户无法直接控制具体语义变化；现有无监督方法仅分析输出特征而忽略底层生成过程，缺乏有效的语义控制机制。

Method: 针对固定提示的多个随机实现，通过反向传播计算差异梯度并构建梯度集合，采用主成分分析或稀疏自编码器分解该集合，在输入嵌入空间中识别出可解释的语义方向。

Result: (1) 提取可解释且可操控的语义方向，实现对单一概念的精确细粒度控制；(2) 通过重引入丢失的生成多样性，有效缓解蒸馏模型中的模式崩溃问题；(3) 基于发现子空间的维度，建立概念复杂度的新型估计器。

Conclusion: 该框架在嵌入空间层面实现了扩散模型的细粒度语义控制，不仅增强了用户对生成内容的直接操控能力，还为保持模型多样性和量化概念复杂度提供了有效工具。

Abstract: A single text prompt passed to a diffusion model often yields a wide range of visual outputs determined solely by stochastic process, leaving users with no direct control over which specific semantic variations appear in the image. While existing unsupervised methods attempt to analyze these variations via output features, they omit the underlying generative process. In this work, we propose a framework to disentangle these semantic directions directly within the input embedding space. To that end, we collect a set of gradients obtained by backpropagating the differences between stochastic realizations of a fixed prompt that we later decompose into meaningful steering directions with either Principal Components Analysis or Sparse Autoencoder. Our approach yields three key contributions: (1) it isolates interpretable, steerable directions for precise, fine-grained control over a single concept; (2) it effectively mitigates mode collapse in distilled models by reintroducing lost diversity; and (3) it establishes a novel estimator for concept complexity under a specific model, based on the dimensionality of the discovered subspace.

</details>


### [60] [Temper-Then-Tilt: Principled Unlearning for Generative Models through Tempering and Classifier Guidance](https://arxiv.org/abs/2602.10217)
*Jacob L. Block,Mehryar Mohri,Aryan Mokhtari,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 本文提出T3-Unlearning（Temper-Then-Tilt Unlearning）方法，通过冻结基础模型并采用"平滑-倾斜"两阶段推理过程，解决大型生成模型在有限样本下对尖锐集中分布遗忘集进行机器遗忘时分类器引导失效的问题。理论分析提供有限样本保证，实验表明该方法在TOFU基准上优于现有基线，且仅需训练少量参数。


<details>
  <summary>Details</summary>
Motivation: 传统机器遗忘方法将任务框架化为监督微调或密度比估计，其中分类器引导是常用技术。然而，本文发现当遗忘集呈现尖锐、集中的分布特征且仅有有限样本时，分类器引导无法忠实执行遗忘。这揭示了现有方法在处理集中分布遗忘集时存在根本性缺陷，亟需新的理论框架和算法来解决这一挑战。

Method: T3-Unlearning方法包含两个核心步骤：首先冻结基础模型，然后进行两阶段推理：（1）平滑（Tempering）阶段，通过调整温度参数展平基础分布中的高置信度峰值；（2）倾斜（Tilting）阶段，训练一个轻量级分类器区分保留集与遗忘集样本，并对平滑后的分布进行调整。该方法将机器遗忘重新定义为密度比估计问题，并通过理论证明平滑对于集中分布的必要性。

Result: 在TOFU基准测试中的全面评估表明，T3-Unlearning在遗忘质量（forget quality）和生成效用（generative utility）两个关键指标上均显著优于现有基线方法。同时，该方法仅需训练基础模型的一小部分参数，训练时间和计算开销极小。

Conclusion: 本研究证明了针对集中分布遗忘集的机器遗忘需要特殊处理策略。T3-Unlearning通过冻结基础模型和创新的"平滑-倾斜"两阶段推理，为有限样本场景下的机器遗忘提供了理论保证和实证优势。该方法为大型生成模型的安全与可控部署提供了高效的遗忘解决方案，特别适用于需要精确移除特定知识而保留整体生成能力的场景。

Abstract: We study machine unlearning in large generative models by framing the task as density ratio estimation to a target distribution rather than supervised fine-tuning. While classifier guidance is a standard approach for approximating this ratio and can succeed in general, we show it can fail to faithfully unlearn with finite samples when the forget set represents a sharp, concentrated data distribution. To address this, we introduce Temper-Then-Tilt Unlearning (T3-Unlearning), which freezes the base model and applies a two-step inference procedure: (i) tempering the base distribution to flatten high-confidence spikes, and (ii) tilting the tempered distribution using a lightweight classifier trained to distinguish retain from forget samples. Our theoretical analysis provides finite-sample guarantees linking the surrogate classifier's risk to unlearning error, proving that tempering is necessary to successfully unlearn for concentrated distributions. Empirical evaluations on the TOFU benchmark show that T3-Unlearning improves forget quality and generative utility over existing baselines, while training only a fraction of the parameters with a minimal runtime.

</details>


### [61] [Chamfer-Linkage for Hierarchical Agglomerative Clustering](https://arxiv.org/abs/2602.10444)
*Kishen N Gowda,Willem Fletcher,MohammadHossein Bateni,Laxman Dhulipala,D Ellis Hershkowitz,Rajesh Jayaram,Jakub Łącki*

Main category: cs.LG

TL;DR: 本文提出一种新颖的层次聚类连接函数——Chamfer-linkage，其基于Chamfer距离度量簇间距离。该函数在理论上可实现O(n²)时间复杂度，在实验中于多个数据集上持续优于经典连接函数，可作为其实用替代品。


<details>
  <summary>Details</summary>
Motivation: 层次凝聚聚类(HAC)缺乏单一明确的全局优化目标，聚类质量主要依赖经验评估，而连接函数的选择至关重要。然而，单连接、平均连接和Ward法等经典连接函数在不同实际数据集上表现差异大，无法稳定产生高质量聚类结果。

Method: 提出Chamfer-linkage，采用机器学习与计算机视觉中常用的Chamfer距离作为簇间距离度量。论证该连接函数满足其他流行方法难以满足的理想概念表示性质。

Result: 理论分析表明，Chamfer-linkage HAC可在O(n²)时间内实现，计算效率与经典连接函数相当。实验结果显示，在多个不同数据集上，Chamfer-linkage持续产生比平均连接和Ward法更高质量的聚类结果。

Conclusion: Chamfer-linkage可作为经典连接函数的实际即插即用替代方案，为层次聚类的理论与实践提供了新的有效工具。

Abstract: Hierarchical Agglomerative Clustering (HAC) is a widely-used clustering method based on repeatedly merging the closest pair of clusters, where inter-cluster distances are determined by a linkage function. Unlike many clustering methods, HAC does not optimize a single explicit global objective; clustering quality is therefore primarily evaluated empirically, and the choice of linkage function plays a crucial role in practice. However, popular classical linkages, such as single-linkage, average-linkage and Ward's method show high variability across real-world datasets and do not consistently produce high-quality clusterings in practice.
  In this paper, we propose \emph{Chamfer-linkage}, a novel linkage function that measures the distance between clusters using the Chamfer distance, a popular notion of distance between point-clouds in machine learning and computer vision. We argue that Chamfer-linkage satisfies desirable concept representation properties that other popular measures struggle to satisfy. Theoretically, we show that Chamfer-linkage HAC can be implemented in $O(n^2)$ time, matching the efficiency of classical linkage functions. Experimentally, we find that Chamfer-linkage consistently yields higher-quality clusterings than classical linkages such as average-linkage and Ward's method across a diverse collection of datasets. Our results establish Chamfer-linkage as a practical drop-in replacement for classical linkage functions, broadening the toolkit for hierarchical clustering in both theory and practice.

</details>


### [62] [Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2602.10224)
*Shiting Huang,Zecheng Li,Yu Zeng,Qingnan Ren,Zhen Fang,Qisheng Su,Kou Shi,Lin Chen,Zehui Chen,Feng Zhao*

Main category: cs.LG

TL;DR: 本文针对可验证奖励强化学习（RLVR）缺乏错误归因与经验内化的问题，提出元经验学习（MEL）框架。通过自蒸馏元经验并内化至模型参数记忆，在基准测试上实现3.92%-4.73%的Pass@1提升，有效解决RLVR的元学习瓶颈。


<details>
  <summary>Details</summary>
Motivation: 尽管可验证奖励强化学习（RLVR）能有效增强大语言模型（LLM）推理能力，但存在元学习瓶颈：缺乏类似人类学习的错误归因和经验内化机制，限制细粒度信用分配与可复用知识形成。现有方法无法构建从错误中提炼的"元经验"表示，制约模型持续学习效果。

Method: 在标准RLVR基础上，MEL利用LLM自验证能力对配对的正误推理轨迹进行对比分析，识别错误分叉点并提炼可泛化的元经验。通过最小化负对数似然将元经验内化至模型参数化记忆，形成连接正误轨迹的语言模型化奖励信号，促进知识重用。

Result: 实验表明MEL在基准测试上持续提升，不同模型规模获得3.92%-4.73%的Pass@1增益，验证了元经验内化机制的有效性。

Conclusion: MEL通过自蒸馏元经验与参数化内化，有效解决RLVR的元学习瓶颈，实现了错误归因与可复用知识形成，为提升大语言模型推理能力提供了新范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.

</details>


### [63] [Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs](https://arxiv.org/abs/2602.10230)
*Joesph An,Phillip Keung,Jiaqi Wang,Orevaoghene Ahia,Noah A. Smith*

Main category: cs.LG

TL;DR: 针对大型音频语言模型在时序定位任务中的不足，本文提出帧级内部工具使用框架，通过轻量级预测机制结合二元分类与泊松过程损失，直接利用模型内部音频表征进行时序 grounding，实现 >50 倍推理加速并在长音频泛化上显著优于传统 token 方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型音频语言模型在词对齐、说话人日志等精确时序 grounding 任务上存在瓶颈。标准 token 生成范式计算开销大、易幻觉，且对超出训练分布的音频长度适应性差，亟需更高效的 grounding 机制。

Method: 引入帧级内部工具使用策略，训练模型直接调用其内部音频表征进行时序定位。构建轻量级预测头，采用二元帧分类与新型非齐次泊松过程（IHP）损失的双目标优化，显式建模时序事件的发生强度。

Result: 在词定位、说话人日志和事件定位三项任务上全面超越 token 基线，推理速度提升 50 倍以上，在分布外长音频上保持高准确率，而对比方法则完全失效。

Conclusion: 该方法通过内部表征直接 grounding 有效解决了时序任务中的效率与泛化问题，为长音频理解提供了可扩展的技术路径。

Abstract: Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.

</details>


### [64] [Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards](https://arxiv.org/abs/2602.10231)
*Kirill Pavlenko,Alexander Golubev,Simon Karasik,Boris Yangel*

Main category: cs.LG

TL;DR: 针对结构化生成任务，GRPO为所有token分配单一优势值导致目标间干扰和信用误分配。本文提出Blockwise Advantage Estimation，为每个目标分配独立优势并仅作用于对应文本块，引入Outcome-Conditioned Baseline通过组内统计量估算中间状态价值，避免昂贵的嵌套rollout。在数学任务上，该方法减轻了奖励干扰，与SOTA方法竞争，并保留了集成增益，为结构化生成提供了无需额外rollout的优化方案。


<details>
  <summary>Details</summary>
Motivation: GRPO在结构化生成中将所有token共享单一标量优势值，导致不同目标段间的奖励信号相互耦合，产生目标干扰和信用分配错误，限制了多目标优化效果。

Method: 提出Blockwise Advantage Estimation框架，为每个目标分配独立的优势函数并仅应用于对应文本块。核心创新是Outcome-Conditioned Baseline，通过根据前缀导出的中间结果对样本进行分层，利用组内统计量近似中间状态价值，从而避免嵌套rollout。

Result: 在带不确定性估计的数学任务上，该方法有效缓解了奖励干扰，性能可与先进的手工设计奖励方法媲美，同时保持了置信度加权集成带来的测试时增益。

Conclusion: 该方法为结构化生成中的序贯目标优化提供了模块化解决方案，无需额外rollout即可实现多目标解耦优化，提升了训练效率和可扩展性。

Abstract: Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.

</details>


### [65] [Risk-Equalized Differentially Private Synthetic Data: Protecting Outliers by Controlling Record-Level Influence](https://arxiv.org/abs/2602.10232)
*Amir Asiaee,Chao Yan,Zachary B. Abrams,Bradley A. Malin*

Main category: cs.LG

TL;DR: 该论文提出风险均衡化差分隐私合成框架，通过降低高风险记录对生成器的影响来优先保护异常值（如罕见病患者），从而抵御成员推断攻击。方法分两步：先用少量隐私预算估计记录异常度，再用与风险分数成反比的权重进行差分隐私学习。在合成数据上显著降低了对异常值的攻击成功率，但真实数据集上的效果取决于评分器质量与合成流程的交互。


<details>
  <summary>Details</summary>
Motivation: 差分隐私虽提供理论保证，但在合成数据发布时，异常记录（如罕见病组合、异常交易）更易遭受成员推断攻击，尤其在中等隐私预算和存在辅助信息的情况下。现有DP机制对异常值的保护不足，导致隐私保护效果不均衡。

Method: 提出风险均衡化DP合成框架，分两个阶段：1）使用少量隐私预算通过高斯机制估计每个记录的"异常度"作为风险分数；2）在DP学习过程中，根据风险分数对记录进行反比加权，降低高异常度记录对生成器的贡献，从而为高风险记录实现更紧致的逐实例隐私界。

Result: 在注入可控异常值的模拟数据上，风险加权显著降低了对高异常度记录的成员推断攻击成功率，消融实验证实改进源于针对性加权而非随机降权。在乳腺癌、成人、德国信用等真实基准数据集上，效果因数据集而异，突显了评分器质量与合成流程的相互作用。

Conclusion: 该框架通过动态调整记录权重，有效提升了异常记录的隐私保护水平，实现了风险均衡化。但实际应用中性能依赖于评分器的准确性和合成流程设计，表明需要根据具体场景优化两个阶段的协同。

Abstract: When synthetic data is released, some individuals are harder to protect than others. A patient with a rare disease combination or a transaction with unusual characteristics stands out from the crowd. Differential privacy provides worst-case guarantees, but empirical attacks -- particularly membership inference -- succeed far more often against such outliers, especially under moderate privacy budgets and with auxiliary information.
  This paper introduces risk-equalized DP synthesis, a framework that prioritizes protection for high-risk records by reducing their influence on the learned generator. The mechanism operates in two stages: first, a small privacy budget estimates each record's "outlierness"; second, a DP learning procedure weights each record inversely to its risk score. Under Gaussian mechanisms, a record's privacy loss is proportional to its influence on the output -- so deliberately shrinking outliers' contributions yields tighter per-instance privacy bounds for precisely those records that need them most.
  We prove end-to-end DP guarantees via composition and derive closed-form per-record bounds for the synthesis stage (the scoring stage adds a uniform per-record term). Experiments on simulated data with controlled outlier injection show that risk-weighting substantially reduces membership inference success against high-outlierness records; ablations confirm that targeting -- not random downweighting -- drives the improvement. On real-world benchmarks (Breast Cancer, Adult, German Credit), gains are dataset-dependent, highlighting the interplay between scorer quality and synthesis pipeline.

</details>


### [66] [Modeling Programming Skills with Source Code Embeddings for Context-aware Exercise Recommendation](https://arxiv.org/abs/2602.10249)
*Carlos Eduardo P. Silva,João Pedro M. Sena,Julio C. S. Reis,André G. Santos,Lucas N. Ferreira*

Main category: cs.LG

TL;DR: 本文提出一种基于代码嵌入的上下文感知推荐系统，通过分析学生提交的程序代码预测其编程技能水平，并生成个性化作业推荐。该系统在预测准确性和推荐质量上均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统编程作业推荐主要依赖正确率或解题时间等简单指标，无法深入理解学生的技能掌握情况。本研究旨在通过分析学生源代码的语义嵌入来建模细粒度的编程技能，实现更精准的个性化推荐。

Method: 采用代码嵌入技术将学生提交的源代码向量化，预测其在多个编程主题上的技能水平，构建学生技能画像。计算学生画像与作业问题技能需求向量之间的余弦相似度，按匹配度排序推荐练习。对比了Jina、TF-IDF、CodeBERT-cpp和GraphCodeBERT等多种嵌入方法。

Result: Jina嵌入在技能预测任务上表现最佳，超越TF-IDF、CodeBERT-cpp和GraphCodeBERT。基于预测技能的推荐系统比基于正确率或解题时间的基线方法产生更合适的推荐，与每周课程内容匹配度更高。七轮课程数据验证了方法的稳定性。

Conclusion: 通过代码嵌入预测的细粒度编程技能为作业推荐提供了比传统指标更强的信号，能够有效支持个性化编程教学。该方法在实际课程数据中验证了有效性和实用性。

Abstract: In this paper, we propose a context-aware recommender system that models students' programming skills using embeddings of the source code they submit throughout a course. These embeddings predict students' skills across multiple programming topics, producing profiles that are matched to the skills required by unseen homework problems. To generate recommendations, we compute the cosine similarity between student profiles and problem skill vectors, ranking exercises according to their alignment with each student's current abilities. We evaluated our approach using real data from students and exercises in an introductory programming course at our university. First, we assessed the effectiveness of our source code embeddings for predicting skills, comparing them with token-based and graph-based alternatives. Results showed that Jina embeddings outperformed TF-IDF, CodeBERT-cpp, and GraphCodeBERT across most skills. Additionally, we evaluated the system's ability to recommend exercises aligned with weekly course content by analyzing student submissions collected over seven course offerings. Our approach consistently produced more suitable recommendations than baselines based on correctness or solution time, indicating that predicted programming skills provide a stronger signal for problem recommendation.

</details>


### [67] [Kernel-Based Learning of Chest X-ray Images for Predicting ICU Escalation among COVID-19 Patients](https://arxiv.org/abs/2602.10261)
*Qiyuan Shi,Jian Kang,Yi Li*

Main category: cs.LG

TL;DR: 本文提出GLIMARK方法，将多核学习扩展至指数族分布结果变量，有效解决了传统多核学习仅适用于连续型结局的问题，并在COVID-19胸部X光片ICU升级预测中验证了其临床实用性。


<details>
  <summary>Details</summary>
Motivation: 单核方法因依赖单一核函数而难以充分表征现实数据的多层次异质性，且传统多核学习(MKL)主要针对连续型结局变量，无法有效处理指数族分布（如二分类、计数等）数据类型，限制了在医疗等领域的应用。

Method: 提出广义线性模型与集成多核回归(GLIMARK)，通过构建复合核函数并整合异构信息源，将多核学习框架扩展至指数族分布结果变量，采用加法回归结构实现多核集成。

Result: 实证研究表明GLIMARK能有效恢复或近似真实数据生成机制；在COVID-19胸部X光片数据中成功预测ICU升级结局，并提取出具有临床解释意义的影像特征。

Conclusion: 该方法拓展了多核学习在非连续型结局预测中的应用边界，为医疗等复杂数据建模提供了新工具，兼具方法学创新与实际临床价值。

Abstract: Kernel methods have been extensively utilized in machine learning for classification and prediction tasks due to their ability to capture complex non-linear data patterns. However, single kernel approaches are inherently limited, as they rely on a single type of kernel function (e.g., Gaussian kernel), which may be insufficient to fully represent the heterogeneity or multifaceted nature of real-world data. Multiple kernel learning (MKL) addresses these limitations by constructing composite kernels from simpler ones and integrating information from heterogeneous sources. Despite these advances, traditional MKL methods are primarily designed for continuous outcomes. We extend MKL to accommodate the outcome variable belonging to the exponential family, representing a broader variety of data types, and refer to our proposed method as generalized linear models with integrated multiple additive regression with kernels (GLIMARK). Empirically, we demonstrate that GLIMARK can effectively recover or approximate the true data-generating mechanism. We have applied it to a COVID-19 chest X-ray dataset, predicting binary outcomes of ICU escalation and extracting clinically meaningful features, underscoring the practical utility of this approach in real-world scenarios.

</details>


### [68] [Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models](https://arxiv.org/abs/2602.10282)
*Kanta Yamaoka,Sumantrak Mukherjee,Thomas Gärtner,David Antony Selby,Stefan Konigorski,Eyke Hüllermeier,Viktor Bengs,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: 本文提出Linear-LLM-SCM框架，用于评估给定DAG结构时LLMs在线性高斯图结构因果模型参数量化推理的能力，发现LLMs在连续域中存在结果随机性强、易受虚假边影响、系数估计变异大、对结构语义扰动敏感等局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在定性因果关系识别方面表现良好，但在连续域中进行量化因果推理（估计参数化函数关系的效应大小）的能力尚未被充分探索。因此需要开发基准测试框架来系统评估LLMs在给定因果图结构下参数量化因果推理的性能。

Method: 提出Linear-LLM-SCM，一种即插即用的基准测试框架。该框架将DAG分解为局部父子节点集，提示LLM为每个节点生成回归形式的因果结构方程，然后将这些方程聚合并与真实参数进行比较评估。

Result: 实验揭示了多个挑战：部分模型存在强随机性结果；在连续域中易受虚假边导致的DAG误设影响；不同设置下系数估计存在显著变异性；对结构和语义扰动高度敏感。这凸显了当前LLMs作为参数量化器的局限性。

Conclusion: 研究表明LLMs在量化因果参数化方面存在明显不足，特别是在连续域中。为此开源了基准测试框架，方便研究者在各自领域中使用自定义DAG和现成LLM进行即插即用的评估。

Abstract: Large language models (LLMs) have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning -- estimating effect sizes that parametrize functional relationships -- remains underexplored in continuous domains. We introduce Linear-LLM-SCM, a plug-and-play benchmarking framework for evaluating LLMs on linear Gaussian structural causal model (SCM) parametrization when the DAG is given. The framework decomposes a DAG into local parent-child sets and prompts an LLM to produce a regression-style structural equation per node, which is aggregated and compared against available ground-truth parameters. Our experiments show several challenges in such benchmarking tasks, namely, strong stochasticity in the results in some of the models and susceptibility to DAG misspecification via spurious edges in the continuous domains. Across models, we observe substantial variability in coefficient estimates for some settings and sensitivity to structural and semantic perturbations, highlighting current limitations of LLMs as quantitative causal parameterizers. We also open-sourced the benchmarking framework so that researchers can utilize their DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains effortlessly.

</details>


### [69] [Configuration-to-Performance Scaling Law with Neural Ansatz](https://arxiv.org/abs/2602.10300)
*Huaqing Zhang,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 本研究提出NCPL，一种基于大语言模型的神经配置-性能缩放定律，通过开源预训练日志学习从完整训练配置到性能的映射。相比Chinchilla定律，预测误差降低20-40%，可泛化至10倍计算量，支持多超参数联合调优与损失曲线预测。


<details>
  <summary>Details</summary>
Motivation: 传统缩放定律需假设超参数最优，调优成本高且受硬件限制。为实现更广泛超参数空间的可预测性与简化大规模调优，需建立从完整配置到性能的映射关系。

Method: 提出配置-性能缩放定律(CPL)，采用大语言模型参数化复杂映射，利用多源开源预训练日志训练，得到神经配置-性能缩放定律(NCPL)。

Result: NCPL预测精度显著优于Chinchilla定律(误差降低20-40%)，可泛化至训练集10倍计算规模，多超参数联合调优性能与基线相当，并可自然扩展至损失曲线预测等丰富目标。

Conclusion: NCPL为大规模训练提供了有效的配置-性能预测工具，简化了超参数调优流程，展现出优异的泛化能力和扩展性，支持多样化预测任务。

Abstract: Researchers build scaling laws to forecast the training performance of expensive large-scale runs with larger model size N and data size D. These laws assume that other training hyperparameters are optimally chosen, which can require significant effort and, in some cases, be impossible due to external hardware constraints. To improve predictability across a broader set of hyperparameters and enable simpler tuning at scale, we propose learning a \textit{Configuration-to-Performance Scaling Law} (CPL): a mapping from the \textit{full training configuration} to training performance. Because no simple functional form can express this mapping, we parameterize it with a large language model (LLM), and fit it with diverse open-source pretraining logs across multiple sources, yielding a \textit{Neural} Configuration-to-Performance Scaling Law (NCPL). NCPL accurately predicts how training configurations influence the final pretraining loss, achieving 20-40% lower prediction error than the configuration-agnostic Chinchilla law and generalizing to runs using up to 10 x more compute than any run in the training set. It further supports joint tuning of multiple hyperparameters with performance comparable to hyperparameter scaling law baselines. Finally, NCPL naturally and effectively extends to richer prediction targets such as loss-curve prediction.

</details>


### [70] [Confounding Robust Continuous Control via Automatic Reward Shaping](https://arxiv.org/abs/2602.10305)
*Mateo Juliani,Mingxuan Li,Elias Bareinboim*

Main category: cs.LG

TL;DR: 本文提出从含未观测混杂变量的离线数据中自动学习连续控制问题的奖励塑形函数。方法基于因果贝尔曼方程推导最优状态价值的紧致上界，作为基于势能的奖励塑形中的势函数，在SAC算法上验证了其对未观测混杂变量的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励塑形缺乏在复杂连续控制问题中的设计原则，尤其当从可能受污染的离线数据学习时，其鲁棒性机制尚未明确。

Method: 提出结合因果贝尔曼方程与基于势能的奖励塑形框架，学习最优状态价值的紧致上界作为势函数，实现奖励塑形的自动化设计。

Result: 在多个连续控制基准测试中，与SAC结合的所提算法对未观测混杂变量表现出强性能保证。

Conclusion: 本工作是因果视角下抗混杂连续控制研究的重要第一步，为后续工作奠定基础。

Abstract: Reward shaping has been applied widely to accelerate Reinforcement Learning (RL) agents' training. However, a principled way of designing effective reward shaping functions, especially for complex continuous control problems, remains largely under-explained. In this work, we propose to automatically learn a reward shaping function for continuous control problems from offline datasets, potentially contaminated by unobserved confounding variables. Specifically, our method builds upon the recently proposed causal Bellman equation to learn a tight upper bound on the optimal state values, which is then used as the potentials in the Potential-Based Reward Shaping (PBRS) framework. Our proposed reward shaping algorithm is tested with Soft-Actor-Critic (SAC) on multiple commonly used continuous control benchmarks and exhibits strong performance guarantees under unobserved confounders. More broadly, our work marks a solid first step towards confounding robust continuous control from a causal perspective. Code for training our reward shaping functions can be found at https://github.com/mateojuliani/confounding_robust_cont_control.

</details>


### [71] [Stop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training](https://arxiv.org/abs/2602.10314)
*Jaeyeon Kim,Jonathan Geuter,David Alvarez-Melis,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: 本文提出Progressive UnMAsking (PUMA)方法，通过改进掩码扩散模型的前向掩码过程，对齐训练和推理时的掩码模式，从而加速训练并减少训练-测试不匹配问题。在1.25亿参数规模下，PUMA使预训练速度提升约2.5倍。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDM）在离散空间生成建模中表现出色，支持任意顺序生成和并行解码，但存在两个关键问题：1）训练复杂度极高，需要处理指数级增长的掩码模式；2）训练使用的随机掩码与推理时高度结构化的去掩码过程之间存在不匹配。这导致计算成本高昂且模型优化效率低下。

Method: 提出Progressive UnMAsking (PUMA)，一种对前向掩码过程的简单改进。该方法通过渐进式地去掩码，使训练时的掩码模式与推理时保持一致，从而将优化重点集中在与推理对齐的掩码上。

Result: 在1.25亿参数规模的实验表明，PUMA能将预训练速度提升约2.5倍。此外，该方法还能与自回归初始化等常见技巧结合，产生互补优势。代码已开源。

Conclusion: PUMA通过简单修改前向掩码过程有效解决了MDM的训练-测试不匹配问题，显著加速了训练过程，为高效训练离散空间的扩散模型提供了新思路。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces. By generating sequences in any order and allowing for parallel decoding, they enable fast inference and strong performance on non-causal tasks. However, this flexibility comes with a training complexity trade-off: MDMs train on an exponentially large set of masking patterns, which is not only computationally expensive, but also creates a train--test mismatch between the random masks used in training and the highly structured masks induced by inference-time unmasking. In this work, we propose Progressive UnMAsking (PUMA), a simple modification of the forward masking process that aligns training-time and inference-time masking patterns, thereby focusing optimization on inference-aligned masks and speeding up training. Empirically, PUMA speeds up pretraining at the 125M scale by $\approx 2.5\times$ and offers complementary advantages on top of common recipes like autoregressive initialization. We open-source our codebase at https://github.com/JaeyeonKim01/PUMA.

</details>


### [72] [Theoretical Analysis of Contrastive Learning under Imbalanced Data: From Training Dynamics to a Pruning Solution](https://arxiv.org/abs/2602.10357)
*Haixu Liao,Yating Zhou,Songyang Zhang,Meng Wang,Shuai Zhang*

Main category: cs.LG

TL;DR: 本研究针对对比学习在数据不平衡下的理论理解缺失问题，构建基于Transformer的动力学框架，揭示神经元权重三阶段演化规律及少数类特征对表征能力的负面影响，提出剪枝作为有效缓解策略。


<details>
  <summary>Details</summary>
Motivation: 尽管对比学习已成为学习通用表征的强大框架，但其在现实世界普遍存在的数据不平衡场景下的理论理解仍然有限。不平衡数据会降低表征质量并诱导模型偏见，然而对这些影响缺乏严格的理论刻画。

Method: 开发了一个理论框架系统分析不平衡数据下基于Transformer编码器的对比学习训练动力学，通过深入探究神经元权重演化规律，提出剪枝方法来恢复性能。

Result: 发现神经元权重训练呈现三个不同阶段，多数类特征、少数类特征及噪声具有不同的动力学特性；少数类特征会降低模型表征容量、增加对复杂架构的需求，并阻碍真实特征与噪声的有效分离；而剪枝能够恢复性能并增强特征分离效果。

Conclusion: 本研究为不平衡数据下的对比学习提供了概念性洞见和实践指导，证明剪枝是缓解数据不平衡负面影响的有效方法，主要理论发现已通过数值实验得到验证。

Abstract: Contrastive learning has emerged as a powerful framework for learning generalizable representations, yet its theoretical understanding remains limited, particularly under imbalanced data distributions that are prevalent in real-world applications. Such an imbalance can degrade representation quality and induce biased model behavior, yet a rigorous characterization of these effects is lacking. In this work, we develop a theoretical framework to analyze the training dynamics of contrastive learning with Transformer-based encoders under imbalanced data. Our results reveal that neuron weights evolve through three distinct stages of training, with different dynamics for majority features, minority features, and noise. We further show that minority features reduce representational capacity, increase the need for more complex architectures, and hinder the separation of ground-truth features from noise. Inspired by these neuron-level behaviors, we show that pruning restores performance degraded by imbalance and enhances feature separation, offering both conceptual insights and practical guidance. Major theoretical findings are validated through numerical experiments.

</details>


### [73] [Simple LLM Baselines are Competitive for Model Diffing](https://arxiv.org/abs/2602.10371)
*Elias Kempf,Simon Schrodi,Bartosz Cywiński,Thomas Brox,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 标准LLM评估无法发现意外行为差异，模型差异分析(model diffing)可自动揭示系统性行为差异。现有LLM-based和SAE-based方法缺乏系统性比较和评估标准。本研究提出泛化性、有趣性和抽象层次的评估指标，比较发现改进的LLM基线方法与SAE-based方法性能相当，但能揭示更抽象的行为差异。


<details>
  <summary>Details</summary>
Motivation: 标准LLM评估仅测试预设能力，无法捕捉模型版本间的行为变化或突现的未对齐倾向。虽然已有LLM-based（生成自然语言描述）和SAE-based（识别可解释特征）两种模型差异分析方法，但缺乏系统性比较和统一的评估标准。

Method: 提出针对模型差异分析的评估指标，涵盖泛化性、有趣性和抽象层次三个关键需求，并使用这些指标对现有LLM-based和SAE-based方法进行比较。

Result: 改进的LLM基线方法在性能上与SAE-based方法相当，且通常能揭示更抽象的行为差异。

Conclusion: 本研究填补了模型差异分析方法评估标准的空白，建立了系统性比较框架。结果表明LLM-based方法不仅性能可媲美SAE-based方法，还能提供更抽象的差异描述，为该领域研究提供了重要基准。

Abstract: Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.

</details>


### [74] [Time-to-Event Transformer to Capture Timing Attention of Events in EHR Time Series](https://arxiv.org/abs/2602.10385)
*Jia Li,Yu Hou,Rui Zhang*

Main category: cs.LG

TL;DR: 针对精准医疗中从大规模时序数据发现个性化序列事件的挑战，提出LITT时序Transformer架构，通过虚拟相对时间线对齐事件并实现时序聚焦注意力，在3276例乳腺癌患者数据上验证可有效预测心脏毒性相关心脏病发病时间，性能优于现有生存分析方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型（如Transformer）虽能捕捉丰富关联，但对事件时序和顺序不敏感，忽略了因果推理。需要一种方法评估患者轨迹间的"对齐程度"并识别共享模式，将时序作为可计算维度，为个性化临床决策提供支持。

Method: 提出LITT（Learning to Temporally Align Sequences）架构，一种新颖的时序Transformer，通过构建虚拟"相对时间线"实现序列事件的临时对齐，引入事件时序聚焦的注意力机制，支持个性化临床轨迹解释。

Result: 在3,276名乳腺癌患者的真实世界纵向EHR数据上验证，LITT能准确预测心脏毒性导致的心脏病发病时间；在公开数据集上，其性能超越基准和先进生存分析方法，展现了显著优势。

Conclusion: LITT通过将时序作为可计算维度并实现事件对齐注意力机制，为临床AI中的时序事件发现和疾病预测提供了新范式，是精准医疗领域的重要进展。

Abstract: Automatically discovering personalized sequential events from large-scale time-series data is crucial for enabling precision medicine in clinical research, yet it remains a formidable challenge even for contemporary AI models. For example, while transformers capture rich associations, they are mostly agnostic to event timing and ordering, thereby bypassing potential causal reasoning.
  Intuitively, we need a method capable of evaluating the "degree of alignment" among patient-specific trajectories and identifying their shared patterns, i.e., the significant events in a consistent sequence. This necessitates treating timing as a true \emph{computable} dimension, allowing models to assign ``relative timestamps'' to candidate events beyond their observed physical times.
  In this work, we introduce LITT, a novel Timing-Transformer architecture that enables temporary alignment of sequential events on a virtual ``relative timeline'', thereby enabling \emph{event-timing-focused attention} and personalized interpretations of clinical trajectories. Its interpretability and effectiveness are validated on real-world longitudinal EHR data from 3,276 breast cancer patients to predict the onset timing of cardiotoxicity-induced heart disease. Furthermore, LITT outperforms both the benchmark and state-of-the-art survival analysis methods on public datasets, positioning it as a significant step forward for precision medicine in clinical AI.

</details>


### [75] [Deep learning outperforms traditional machine learning methods in predicting childhood malnutrition: evidence from survey data](https://arxiv.org/abs/2602.10381)
*Deepak Bastola,Yang Li*

Main category: cs.LG

TL;DR: 本研究首次系统评估机器学习和深度学习在尼泊尔5岁以下儿童营养不良识别中的应用。基于2019年MICS调查数据，比较16种算法并构建综合营养不良指标。TabNet性能最优，母亲教育、家庭财富和儿童年龄是核心预测因子，为资源匮乏地区提供可扩展的筛查框架。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔等低资源地区儿童营养不良是重大公共卫生问题，传统病例发现方法劳动强度大且在偏远地区难以实施。亟需开发高效筛查工具以识别高风险儿童，支持实现可持续发展目标并开展针对性营养干预。

Method: 利用2019年尼泊尔多指标类集调查数据，整合发育迟缓、消瘦和体重不足构建综合营养不良指标。系统比较深度学习、梯度提升和传统机器学习三大类16种算法，采用10种评估指标（侧重F1分数和召回率），并通过共识特征重要性分析识别关键预测变量。

Result: TabNet表现最佳，优于支持向量机和AdaBoost。核心预测因素为母亲教育程度、家庭财富指数、儿童年龄，其次为地理特征、疫苗接种状况和进食频率。研究证明了基于调查数据的可扩展筛查框架的有效性。

Conclusion: 该框架支持尼泊尔实现可持续发展目标，为全球类似低资源地区提供可转移的方法学模板。通过精准识别高风险儿童，可指导靶向营养干预，具有重要的公共卫生实践价值和推广潜力。

Abstract: Childhood malnutrition remains a major public health concern in Nepal and other low-resource settings, while conventional case-finding approaches are labor-intensive and frequently unavailable in remote areas. This study provides the first comprehensive assessment of machine learning and deep learning methodologies for identifying malnutrition among children under five years of age in Nepal. We systematically compared 16 algorithms spanning deep learning, gradient boosting, and traditional machine learning families, using data from the Nepal Multiple Indicator Cluster Survey (MICS) 2019. A composite malnutrition indicator was constructed by integrating stunting, wasting, and underweight status, and model performance was evaluated using ten metrics, with emphasis on F1-score and recall to account for substantial class imbalance and the high cost of failing to detect malnourished children. Among all models, TabNet demonstrated the best performance, likely attributable to its attention-based architecture, and outperformed both support vector machine and AdaBoost classifiers. A consensus feature importance analysis identified maternal education, household wealth index, and child age as the primary predictors of malnutrition, followed by geographic characteristics, vaccination status, and meal frequency. Collectively, these results demonstrate a scalable, survey-based screening framework for identifying children at elevated risk of malnutrition and for guiding targeted nutritional interventions. The proposed approach supports Nepal's progress toward the Sustainable Development Goals and offers a transferable methodological template for similar low-resource settings globally.

</details>


### [76] [Affordances Enable Partial World Modeling with LLMs](https://arxiv.org/abs/2602.10390)
*Khimya Khetarpal,Gheorghe Comanici,Jonathan Richens,Jeremy Shar,Fei Xia,Laurent Orseau,Aleksandra Faust,Doina Precup*

Main category: cs.LG

TL;DR: 大型预训练模型直接搜索效率低下，本文形式化证明其可作为基于功能可供性的部分世界模型，提出分布鲁棒功能可供性用于多任务场景，并通过机器人实验验证该方法能降低搜索复杂度、提升奖励。


<details>
  <summary>Details</summary>
Motivation: 尽管大型预训练模型在海量互联网数据上蕴含丰富知识，但直接用于搜索过程存在效率低、精度差的问题。部分模型虽能通过功能可供性高效预测实现用户意图的子集状态和动作，但如何将二者结合仍属未知。本文旨在回答能否将大型模型作为部分世界模型使用这一核心问题。

Method: 1) 形式化证明：任何实现任务无关语言条件化意图的智能体必然拥有基于功能可供性的预测性部分世界模型；2) 在多任务场景下提出分布鲁棒功能可供性概念；3) 从大型模型中提取部分模型以改进搜索效率；4) 在桌面机器人任务中进行实证评估。

Result: 实验表明，与完整世界模型相比，功能可供性感知的部分模型将搜索分支因子降低，并获得显著更高的累积奖励，验证了所提方法的有效性。

Conclusion: 大型模型可被形式化为部分世界模型，提取其基于功能可供性的子模型能显著提升搜索效率，为机器人等实际应用提供了高效的世界模型解决方案。

Abstract: Full models of the world require complex knowledge of immense detail. While pre-trained large models have been hypothesized to contain similar knowledge due to extensive pre-training on vast amounts of internet scale data, using them directly in a search procedure is inefficient and inaccurate. Conversely, partial models focus on making high quality predictions for a subset of state and actions: those linked through affordances that achieve user intents~\citep{khetarpal2020can}. Can we posit large models as partial world models? We provide a formal answer to this question, proving that agents achieving task-agnostic, language-conditioned intents necessarily possess predictive partial-world models informed by affordances. In the multi-task setting, we introduce distribution-robust affordances and show that partial models can be extracted to significantly improve search efficiency. Empirical evaluations in tabletop robotics tasks demonstrate that our affordance-aware partial models reduce the search branching factor and achieve higher rewards compared to full world models.

</details>


### [77] [Colorful Talks with Graphs: Human-Interpretable Graph Encodings for Large Language Models](https://arxiv.org/abs/2602.10386)
*Angelo Zangari,Peyman Baghershahi,Sourav Medya*

Main category: cs.LG

TL;DR: 该论文针对大型语言模型在处理图问题时的结构推理挑战，提出了一种基于Weisfeiler-Lehman相似性类的人可解释结构编码策略，将图结构通过颜色令牌直接注入自然语言提示，显著提升了LLM在多种图任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 图问题对大型语言模型构成根本性挑战，因为图任务需要处理显式结构、置换不变性和计算复杂的关系，这与基于文本的模型表示存在不匹配。该研究旨在探索如何有效克服这些障碍，使LLM能够更好地应用于图问题。

Method: 提出一种人机可解释的结构编码策略，用于图到文本的转换。该方法计算Weisfeiler-Lehman（WL）相似性类的变体，并将其映射为类似人类理解的颜色令牌而非数字标签，从而将图结构直接注入自然语言提示中。核心思想是语义上有意义且人类可解释的线索可能比晦涩的符号编码更有效地被LLM处理。

Result: 在多个算法性和预测性图任务上的实验结果表明，该方法在合成数据集和真实世界数据集上均取得了显著改进。该方法通过捕获局部和全局范围的依赖关系，特别增强了LLM在需要全局图结构推理的任务上的表现。

Conclusion: 该研究成功证明，通过引入人类可解释的结构编码，可以有效弥合LLM与图结构推理之间的鸿沟，为将大型语言模型应用于图相关问题提供了新的可行路径，尤其在需要全局结构理解的任务中展现出良好潜力。

Abstract: Graph problems are fundamentally challenging for large language models (LLMs). While LLMs excel at processing unstructured text, graph tasks require reasoning over explicit structure, permutation invariance, and computationally complex relationships, creating a mismatch with the representations of text-based models. Our work investigates how LLMs can be effectively applied to graph problems despite these barriers. We introduce a human-interpretable structural encoding strategy for graph-to-text translation that injects graph structure directly into natural language prompts. Our method involves computing a variant of Weisfeiler-Lehman (WL) similarity classes and maps them to human-like color tokens rather than numeric labels. The key insight is that semantically meaningful and human-interpretable cues may be more effectively processed by LLMs than opaque symbolic encoding. Experimental results on multiple algorithmic and predictive graph tasks show the considerable improvements by our method on both synthetic and real-world datasets. By capturing both local and global-range dependencies, our method enhances LLM performance especially on graph tasks that require reasoning over global graph structure.

</details>


### [78] [Modular Multi-Task Learning for Chemical Reaction Prediction](https://arxiv.org/abs/2602.10404)
*Jiayun Pang,Ahmed M. Zaitoun,Xacobe Couso Cambeiro,Ivan Vulić*

Main category: cs.LG

TL;DR: 本研究评估低秩适应（LoRA）作为全微调的参数高效替代方案，用于将通用有机LLM适应特定反应数据集。在USPTO和C-H官能化反应基准测试中，LoRA在保持多任务性能、减轻灾难性遗忘方面表现优异，编码了与全微调不同的反应模式，为化学LLM灵活部署提供了实用策略。


<details>
  <summary>Details</summary>
Motivation: 将训练于广泛有机化学知识的LLM适应到小规模、特定领域反应数据集是化学与制药研发的关键挑战。有效的专业化需要学习新反应知识的同时，保持跨相关任务的通用化学理解，避免灾难性遗忘。

Method: 采用低秩适应（LoRA）进行参数高效微调，在USPTO反应类和具有挑战性的C-H官能化反应数据集上，对前向反应预测、逆合成分析和试剂预测三个任务进行基准测试，与全微调方法进行对比。

Result: LoRA达到与全微调相当的准确率，有效缓解灾难性遗忘并更好地保持多任务性能。两种方法均能泛化至训练分布外，产生合理的替代溶剂预测。C-H官能化微调揭示LoRA与全微调编码了微妙的反应活性模式差异，表明LoRA具有更有效的反应特异性适应能力。

Conclusion: 随着LLM规模持续扩大，模块化参数高效微调策略展现出在化学应用中灵活部署的实用性。LoRA作为一种有效的适应方法，为在有限复杂数据集上专业化化学LLM提供了可行路径，支持其在研发中的规模化应用。

Abstract: Adapting large language models (LLMs) trained on broad organic chemistry to smaller, domain-specific reaction datasets is a key challenge in chemical and pharmaceutical R&D. Effective specialisation requires learning new reaction knowledge while preserving general chemical understanding across related tasks. Here, we evaluate Low-Rank Adaptation (LoRA) as a parameter-efficient alternative to full fine-tuning for organic reaction prediction on limited, complex datasets. Using USPTO reaction classes and challenging C-H functionalisation reactions, we benchmark forward reaction prediction, retrosynthesis and reagent prediction. LoRA achieves accuracy comparable to full fine-tuning while effectively mitigating catastrophic forgetting and better preserving multi-task performance. Both fine-tuning approaches generalise beyond training distributions, producing plausible alternative solvent predictions. Notably, C-H functionalisation fine-tuning reveals that LoRA and full fine-tuning encode subtly different reactivity patterns, suggesting more effective reaction-specific adaptation with LoRA. As LLMs continue to scale, our results highlight the practicality of modular, parameter-efficient fine-tuning strategies for their flexible deployment for chemistry applications.

</details>


### [79] [AI-rithmetic](https://arxiv.org/abs/2602.10416)
*Alex Bie,Travis Dick,Alex Kulesza,Prabhakar Raghavan,Vinod Raman,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern AI systems have been successfully deployed to win medals at international math competitions, assist with research workflows, and prove novel technical lemmas. However, despite their progress at advanced levels of mathematics, they remain stubbornly bad at basic arithmetic, consistently failing on the simple task of adding two numbers. We present a systematic investigation of this phenomenon. We demonstrate empirically that all frontier models suffer significantly degraded accuracy for integer addition as the number of digits increases. Furthermore, we show that most errors made by these models are highly interpretable and can be attributed to either operand misalignment or a failure to correctly carry; these two error classes explain 87.9%, 62.9%, and 92.4% of Claude Opus 4.1, GPT-5, and Gemini 2.5 Pro errors, respectively. Finally, we show that misalignment errors are frequently related to tokenization, and that carrying errors appear largely as independent random failures.

</details>


### [80] [Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation](https://arxiv.org/abs/2602.10430)
*Jie Jiang,Yusen Huo,Xiangxin Zhan,Changping Wang,Jun Zhang*

Main category: cs.LG

TL;DR: 针对离线历史日志中低质量数据导致的模型崩溃问题，本文提出分布鲁棒策略优化(DRPO)，通过建立排斥优化散度理论，将问题重构为乐观分布鲁棒优化，并证明硬过滤是其精确解，最终在混合质量推荐基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 基于策略的强化学习虽已成为生成式推荐的主流范式，但在处理离线历史日志时，低质量数据的主导地位会引发严重模型崩溃。现有方法无法解决方差缩减与噪声模仿之间的根本矛盾。

Method: 首先建立排斥优化散度理论，揭示负梯度更新引发指数级强度爆炸的机制；进而将目标重表述为乐观分布鲁棒优化(DRO)问题，提出分布鲁棒策略优化(DRPO)，并证明硬过滤是该DRO目标的精确解，实现高质量行为分布的最优恢复与噪声的严格剔除。

Result: 在混合质量推荐基准测试上，DRPO实现了最先进的(state-of-the-art)性能。

Conclusion: 本研究从理论上解释了离线策略训练中的散度困境，提出的DRPO框架通过硬过滤机制有效分离高质量行为与噪声，为低质量数据环境下的推荐系统优化提供了有效解决方案。

Abstract: Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces severe model collapse. We first establish the Divergence Theory of Repulsive Optimization, revealing that negative gradient updates inherently trigger exponential intensity explosion during off-policy training. This theory elucidates the inherent dilemma of existing methods, exposing their inability to reconcile variance reduction and noise imitation. To break this curse, we argue that the solution lies in rigorously identifying the latent high-quality distribution entangled within the noisy behavior policy. Accordingly, we reformulate the objective as an Optimistic Distributionally Robust Optimization (DRO) problem. Guided by this formulation, we propose Distributionally Robust Policy Optimization (DRPO). We prove that hard filtering is the exact solution to this DRO objective, enabling DRPO to optimally recover high-quality behaviors while strictly discarding divergence-inducing noise. Extensive experiments demonstrate that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks.

</details>


### [81] [Gated Removal of Normalization in Transformers Enables Stable Training and Efficient Inference](https://arxiv.org/abs/2602.10408)
*Andrei Kanavalau,Carmen Amo Alonso,Sanjay Lall*

Main category: cs.LG

TL;DR: 该论文质疑预归一化Transformer中归一化技术的必要性，提出TaperNorm——一种可在训练中从标准样本依赖归一化平滑过渡至样本无关线性映射的替代方案。通过全局门控系数退火机制，该方法在保持性能的同时消除推理时的逐标记统计计算，支持归一化层与相邻线性层的融合，实现最高1.22倍吞吐量提升，并揭示了输出归一化通过尺度锚定防止logit chasing的关键作用。


<details>
  <summary>Details</summary>
Motivation: 挑战Transformer训练中归一化对稳定性不可或缺的普遍假设，深入探究预归一化架构内部样本依赖归一化的实际必要性，试图解耦并识别输出归一化在训练动态中的独特功能，为构建无归一化Transformer提供理论洞见与实践路径。

Method: 提出TaperNorm，以单全局门控系数g（初始化1并余弦退火至0）调控归一化强度；训练初期表现如RMSNorm/LayerNorm，通过指数移动平均校准缩放分支，当g趋近0时逐标记统计量失效，退化为可学习的样本无关仿射变换；进一步提出作用于预logit残差流的固定目标辅助损失，作为显式的尺度锚定机制，可替代最终归一化层。

Result: 实验表明TaperNorm在相同训练设置下达到标准归一化基线的相当性能；推理阶段无需逐标记统计，支持归一化层与相邻线性投影的权重融合；在末标记logits生成模式的微基准测试中，吞吐量提升最高达1.22倍；理论分析证实输出归一化作为（近）0-齐次映射，通过尺度锚定消除输出径向梯度，抑制交叉熵损失驱动的无界logit增长现象（logit chasing）。

Conclusion: 该研究通过TaperNorm设计向无归一化Transformer迈出关键一步，在保持模型性能的同时显著降低推理计算开销，并深刻揭示了输出归一化通过尺度锚定稳定训练的本质机制，为未来高效Transformer架构设计提供新思路。

Abstract: Normalization is widely viewed as essential for stabilizing Transformer training. We revisit this assumption for pre-norm Transformers and ask to what extent sample-dependent normalization is needed inside Transformer blocks. We introduce TaperNorm, a drop-in replacement for RMSNorm/LayerNorm that behaves exactly like the standard normalizer early in training and then smoothly tapers to a learned sample-independent linear/affine map. A single global gate is held at $g{=}1$ during gate warmup, used to calibrate the scaling branch via EMAs, and then cosine-decayed to $g{=}0$, at which point per-token statistics vanish and the resulting fixed scalings can be folded into adjacent linear projections. Our theoretical and empirical results isolate scale anchoring as the key role played by output normalization: as a (near) $0$-homogeneous map it removes radial gradients at the output, whereas without such an anchor cross-entropy encourages unbounded logit growth (``logit chasing''). We further show that a simple fixed-target auxiliary loss on the pre-logit residual-stream scale provides an explicit alternative anchor and can aid removal of the final normalization layer. Empirically, TaperNorm matches normalized baselines under identical setups while eliminating per-token statistics and enabling these layers to be folded into adjacent linear projections at inference. On an efficiency microbenchmark, folding internal scalings yields up to $1.22\times$ higher throughput in last-token logits mode. These results take a step towards norm-free Transformers while identifying the special role output normalization plays.

</details>


### [82] [LUCID: Attention with Preconditioned Representations](https://arxiv.org/abs/2602.10410)
*Sai Surya Duvvuri,Nirmal Patel,Nilesh Gupta,Inderjit S. Dhillon*

Main category: cs.LG

TL;DR: LUCID注意力机制通过应用基于指数化键-键相似度的预处理器来解决Softmax注意力在长序列中的概率质量扩散问题，在保持相同计算复杂度的同时显著提升了长上下文检索性能。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度增加，Softmax注意力会将概率质量分散到无关token上，导致性能下降；而通过降低温度参数来增强聚焦又会因梯度消失而损害模型的可学习性。

Method: 提出LUCID注意力架构，利用指数化键-键相似度构建预处理器，在再生核希尔伯特空间中最小化键之间的重叠，使查询能够准确聚焦于大量键中的重要键。

Result: 在128K token长度下评估的10亿参数模型显示，在BABILong上提升达18%，在RULER多针任务上提升达14%，在SCROLLS和LongBench等长上下文检索任务上均有显著增益。

Conclusion: LUCID注意力通过预处理器方法有效解决了长序列注意力机制的核心缺陷，避免了温度调节带来的学习难题，为长上下文建模提供了新的有效途径。

Abstract: Softmax-based dot-product attention is a cornerstone of Transformer architectures, enabling remarkable capabilities such as in-context learning. However, as context lengths increase, a fundamental limitation of the softmax function emerges: it tends to diffuse probability mass to irrelevant tokens degrading performance in long-sequence scenarios. Furthermore, attempts to sharpen focus by lowering softmax temperature hinder learnability due to vanishing gradients. We introduce LUCID Attention, an architectural modification that applies a preconditioner to the attention probabilities. This preconditioner, derived from exponentiated key-key similarities, minimizes overlap between the keys in a Reproducing Kernel Hilbert Space, thus allowing the query to focus on important keys among large number of keys accurately with same computational complexity as standard attention. Additionally, LUCID's preconditioning-based approach to retrieval bypasses the need for low temperature and the learnability problems associated with it. We validate our approach by training ~1 billion parameter language models evaluated on up to 128K tokens. Our results demonstrate significant gains on long-context retrieval tasks, specifically retrieval tasks from BABILong, RULER, SCROLLS and LongBench. For instance, LUCID achieves up to 18% improvement in BABILong and 14% improvement in RULER multi-needle performance compared to standard attention.

</details>


### [83] [A Unified Theory of Random Projection for Influence Functions](https://arxiv.org/abs/2602.10449)
*Pingbang Hu,Yuzheng Hu,Jiaqi W. Ma,Han Zhao*

Main category: cs.LG

TL;DR: 针对现代过参数化模型中影响函数$g^{\top}F^{-1}g^{\prime}$的计算难题，本文建立了随机投影保持影响函数的统一理论框架，系统分析了无正则化、岭正则化及因子化曲率等场景下的投影维度要求，并量化了测试梯度超出曲率算子值域时的"泄漏"效应，为实际应用中草图大小的选择提供了原则性理论指导。


<details>
  <summary>Details</summary>
Motivation: 在过参数化模型中，曲率算子$F\in\mathbb{R}^{d\times d}$的存储与求逆计算代价过高，随机投影草图技术虽被广泛用于可扩展的影响函数计算，但现有理论依赖的Johnson-Lindenstrauss引理仅能保证欧氏几何的近似保持，无法解释草图在矩阵求逆下的行为，也未涵盖岭正则化、结构化曲率近似等关键技术的交互影响，导致实践中草图尺寸选择缺乏理论依据。

Method: 本文开发统一理论，分场景刻画随机投影何时保持影响函数：当梯度$g,g^{\prime}$位于$\text{range}(F)$时，研究无正则化投影的单射条件、岭正则化对有效维度的依赖关系、以及Kronecker因子化曲率下的解耦草图性质；对超出值域的测试梯度，则引入并量化"泄漏"项来分析核空间分量的影响。

Result: 1) 无正则化投影：精确保持当且仅当投影$P$在$\text{range}(F)$上单射，此时草图维度$m\geq\text{rank}(F)$；2) 岭正则化投影：正则化彻底改变草图障碍，近似保证由$F$在正则化尺度下的有效维度决定；3) 因子化曲率：对$F=A\otimes E$，解耦草图$P_A\otimes P_E$仍保持理论保证；4) 超出值域梯度：核空间分量引发可量化的泄漏项，为一般测试点的影响查询提供理论保证。

Conclusion: 本研究填补了随机投影在矩阵求逆场景下的理论空白，首次系统建立了投影保持影响函数的完整理论框架，揭示了不同正则化策略和曲率结构下的草图维度要求，为实际应用中平衡计算效率与近似精度提供了明确的理论指导原则。

Abstract: Influence functions and related data attribution scores take the form of $g^{\top}F^{-1}g^{\prime}$, where $F\succeq 0$ is a curvature operator. In modern overparameterized models, forming or inverting $F\in\mathbb{R}^{d\times d}$ is prohibitive, motivating scalable influence computation via random projection with a sketch $P \in \mathbb{R}^{m\times d}$. This practice is commonly justified via the Johnson--Lindenstrauss (JL) lemma, which ensures approximate preservation of Euclidean geometry for a fixed dataset. However, JL does not address how sketching behaves under inversion. Furthermore, there is no existing theory that explains how sketching interacts with other widely-used techniques, such as ridge regularization and structured curvature approximations.
  We develop a unified theory characterizing when projection provably preserves influence functions. When $g,g^{\prime}\in\text{range}(F)$, we show that: 1) Unregularized projection: exact preservation holds iff $P$ is injective on $\text{range}(F)$, which necessitates $m\geq \text{rank}(F)$; 2) Regularized projection: ridge regularization fundamentally alters the sketching barrier, with approximation guarantees governed by the effective dimension of $F$ at the regularization scale; 3) Factorized influence: for Kronecker-factored curvatures $F=A\otimes E$, the guarantees continue to hold for decoupled sketches $P=P_A\otimes P_E$, even though such sketches exhibit row correlations that violate i.i.d. assumptions. Beyond this range-restricted setting, we analyze out-of-range test gradients and quantify a \emph{leakage} term that arises when test gradients have components in $\ker(F)$. This yields guarantees for influence queries on general test points.
  Overall, this work develops a novel theory that characterizes when projection provably preserves influence and provides principled guidance for choosing the sketch size in practice.

</details>


### [84] [Constructing Industrial-Scale Optimization Modeling Benchmark](https://arxiv.org/abs/2602.10450)
*Zhong Li,Hongliang Lu,Tao Wei,Wenyu Liu,Yuxuan Chen,Yuan Lan,Fan Zhang,Zaiwen Wen*

Main category: cs.LG

TL;DR: 本文针对工业级优化问题（变量/约束达10³-10⁶量级）缺乏真实自然语言-优化模型对齐基准的瓶颈，提出MIPLIB-NL数据集。该数据集通过结构感知反向构建方法，从MIPLIB 2017的223个混合整数线性规划实例中恢复模型结构，生成对应的自然语言描述，并进行专家验证和人类-LLM交互检查，实验表明现有LLM优化系统在真实规模问题上性能显著下降，暴露出玩具级基准无法揭示的失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有LLM优化建模评估严重依赖玩具级或合成基准，无法反映真实工业问题的规模复杂性（10³-10⁶变量/约束），且缺乏将自然语言需求与真实优化模型及可执行代码一一对应的基准，导致难以客观评估LLM在实际工业场景中的优化建模能力。

Method: 采用结构感知反向构建方法论：(1) 从扁平化的求解器公式中解析并恢复出紧凑、可复用的模型层次结构；(2) 在统一的模型-数据分离格式框架下，反向生成与恢复结构显式关联的自然语言规格说明；(3) 通过领域专家审查与人类-LLM交互进行迭代式语义验证，并执行独立重建检查确保质量。

Result: 成功构建了包含223个一对一重建实例的MIPLIB-NL基准数据集，完整保留了原始MIPLIB 2017实例的全部数学内容，为自然语言到优化模型的真实场景评估提供了首个大规模、结构感知的工业级基准。

Conclusion: 在MIPLIB-NL上的实验显示，当前性能优异的优化系统在真实规模问题上出现显著性能退化，暴露了大规模问题特有的失败模式（如结构理解错误、规模推理失效等），验证了该基准对推动LLM优化建模技术向工业实用化发展的关键价值。

Abstract: Optimization modeling underpins decision-making in logistics, manufacturing, energy, and finance, yet translating natural-language requirements into correct optimization formulations and solver-executable code remains labor-intensive. Although large language models (LLMs) have been explored for this task, evaluation is still dominated by toy-sized or synthetic benchmarks, masking the difficulty of industrial problems with $10^{3}$--$10^{6}$ (or more) variables and constraints. A key bottleneck is the lack of benchmarks that align natural-language specifications with reference formulations/solver code grounded in real optimization models. To fill in this gap, we introduce MIPLIB-NL, built via a structure-aware reverse construction methodology from real mixed-integer linear programs in MIPLIB~2017. Our pipeline (i) recovers compact, reusable model structure from flat solver formulations, (ii) reverse-generates natural-language specifications explicitly tied to this recovered structure under a unified model--data separation format, and (iii) performs iterative semantic validation through expert review and human--LLM interaction with independent reconstruction checks. This yields 223 one-to-one reconstructions that preserve the mathematical content of the original instances while enabling realistic natural-language-to-optimization evaluation. Experiments show substantial performance degradation on MIPLIB-NL for systems that perform strongly on existing benchmarks, exposing failure modes invisible at toy scale.

</details>


### [85] [Driving Reaction Trajectories via Latent Flow Matching](https://arxiv.org/abs/2602.10476)
*Yili Shen,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 本文提出LatentRxnFlow，一种基于条件流匹配的化学反应预测新范式，将反应建模为连续潜在轨迹。该方法仅从标准反应物-产物对学习时间相关的潜在动力学，无需机理标注，在USPTO基准上达到SOTA性能，同时通过轨迹分析提供可诊断性、不确定性量化和错误缓解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管当前反应预测模型在标准基准上准确率接近饱和，但多为一次性映射，对反应过程洞察有限。现有逐步生成方法依赖机理监督、离散符号编辑和计算昂贵的推理。因此需要一种既能保持高准确性又能提供过程透明度、可诊断性和不确定性感知的新方法。

Method: 提出LatentRxnFlow，基于条件流匹配框架。将反应建模为以热力学产物状态为锚点的连续潜在轨迹。直接从标准反应物-产物对学习时间相关的潜在动力学，无需机理注释或中间标签。通过连续生成轨迹实现反应预测。

Result: 在USPTO基准测试中达到SOTA性能。更重要的是，连续轨迹形式支持轨迹级诊断，能够定位和表征失败模式，通过门控推理缓解某些错误。学习到的轨迹几何特性提供认知不确定性的内在信号，有助于优先处理可可靠预测的反应结果，并标记需额外验证的模糊案例。

Conclusion: LatentRxnFlow将强预测准确性与改进的透明度、可诊断性和不确定性感知相结合，推动反应预测向高吞吐量发现工作流中的可信部署迈进，为化学反应预测提供了更可靠和可解释的解决方案。

Abstract: Recent advances in reaction prediction have achieved near-saturated accuracy on standard benchmarks (e.g., USPTO), yet most state-of-the-art models formulate the task as a one-shot mapping from reactants to products, offering limited insight into the underlying reaction process. Procedural alternatives introduce stepwise generation but often rely on mechanism-specific supervision, discrete symbolic edits, and computationally expensive inference. In this work, we propose LatentRxnFlow, a new reaction prediction paradigm that models reactions as continuous latent trajectories anchored at the thermodynamic product state. Built on Conditional Flow Matching, our approach learns time-dependent latent dynamics directly from standard reactant-product pairs, without requiring mechanistic annotations or curated intermediate labels. While LatentRxnFlow achieves state-of-the-art performance on USPTO benchmarks, more importantly, the continuous formulation exposes the full generative trajectory, enabling trajectory-level diagnostics that are difficult to realize with discrete or one-shot models. We show that latent trajectory analysis allows us to localize and characterize failure modes and to mitigate certain errors via gated inference. Furthermore, geometric properties of the learned trajectories provide an intrinsic signal of epistemic uncertainty, helping prioritize reliably predictable reaction outcomes and flag ambiguous cases for additional validation. Overall, LatentRxnFlow combines strong predictive accuracy with improved transparency, diagnosability, and uncertainty awareness, moving reaction prediction toward more trustworthy deployment in high-throughput discovery workflows.

</details>


### [86] [Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning](https://arxiv.org/abs/2602.10420)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: 本文将流匹配框架拓展至二进制流形用于离散数据生成，发现信号空间预测(x-prediction)与速度损失(v-loss)结合存在结构失配，导致时间依赖的奇异加权并放大梯度对近似误差的敏感性。通过形式化预测-损失对齐条件，证明将目标重新对齐到信号空间(x-loss)可消除奇异加权、获得均匀有界梯度，从而实现无需启发式调度的鲁棒训练。进一步揭示了二进制数据上概率目标(如交叉熵)与几何损失(如均方误差)的拓扑依赖差异。


<details>
  <summary>Details</summary>
Motivation: 信号空间预测在连续流匹配中已取得显著成功，但其在离散数据生成任务（如二进制流形）中的适用性尚未得到充分研究。初步实验发现，直接将x-prediction与v-loss结合会产生时间依赖的奇异加权现象，该现象会显著放大梯度对近似误差的敏感性，从而严重影响训练稳定性。这揭示了当前方法在离散域应用中的根本缺陷，亟需理论分析与改进方案。

Method: 1) 从理论上分析x-prediction与v-loss结合时的结构失配问题，识别奇异加权的产生机制；2) 形式化提出预测-损失对齐作为流匹配训练的必要条件；3) 证明将训练目标重新对齐至信号空间(x-loss)可消除奇异加权，获得均匀有界梯度；4) 针对二进制数据特性，系统比较概率目标(交叉熵)与几何损失(均方误差)的拓扑性质差异，为损失函数设计提供理论指导。

Result: 1) 理论证明了x-loss相比v-loss在离散域上的优越性：消除了时间依赖的奇异加权，确保梯度均匀有界；2) 实现了在均匀时间步采样下的稳定训练，摆脱了对复杂启发式调度的依赖；3) 发现了二进制流形上不同损失函数的拓扑依赖特性，明确了交叉熵等概率目标与均方误差等几何损失在设计理念上的本质区别；4) 为离散生成模型的鲁棒训练提供了可复现的理论框架和实践准则。

Conclusion: 本研究建立了二进制及离散域上鲁棒流匹配的完整理论体系，明确了信号空间对齐是确保训练稳定性的核心原则。通过解决x-prediction与v-loss的结构失配问题，不仅提升了离散生成模型的训练鲁棒性，也为未来在更广泛离散空间上的扩散学习研究提供了重要指导。研究成果兼具理论深度与实践价值，对推动离散生成模型发展具有重要意义。

Abstract: Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes highlighting the effectiveness of signal-space prediction ($x$-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative modeling of discrete data. While $x$-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ($v$-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction-loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ($x$-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary -- and related discrete -- domains, positioning signal-space alignment as a key principle for robust diffusion learning.

</details>


### [87] [Learning Adaptive Distribution Alignment with Neural Characteristic Function for Graph Domain Adaptation](https://arxiv.org/abs/2602.10489)
*Wei Chen,Xingyu Guo,Shuang Li,Zhao Zhang,Yan Zhong,Fuzhen Zhuang,Deqing wang*

Main category: cs.LG

TL;DR: 本文提出ADAlign，一种用于图领域自适应的自适应分布对齐框架。该方法通过神经谱差异（NSD）自动识别和联合对齐源图与目标图之间最相关的分布差异，无需手动设计对齐标准。在10个数据集和16个迁移任务上，ADAlign不仅性能优于现有最优方法，还实现了内存占用更低、训练更快的效率提升。


<details>
  <summary>Details</summary>
Motivation: 图领域自适应（GDA）面临复杂多面的分布偏移挑战。现有方法依赖人工选择的图元素（如节点属性或结构统计量）和手动设计的图滤波器，需要针对特定场景的启发式规则。当主导差异在不同迁移场景中变化时，这类方法表现不佳且缺乏灵活性。

Method: 提出ADAlign框架，其核心是神经谱差异（NSD）。NSD利用谱域中的神经特征函数编码所有阶数的特征-结构依赖关系，并通过可学习的频率采样器，基于极小极大范式自适应地为每个任务强调最具信息量的谱成分，自动识别和联合对齐最相关的分布差异。

Result: 在10个数据集和16个迁移任务上的广泛实验表明，ADAlign不仅超越了现有最先进基线方法，还在内存使用和训练速度方面实现了显著效率提升。

Conclusion: ADAlign提供了一种灵活、场景感知且鲁棒的GDA解决方案，通过自适应的神经谱差异学习，有效应对多样化和动态演化的分布偏移，为图数据的跨领域迁移提供了新思路。

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs but is challenged by complex, multi-faceted distributional shifts. Existing methods attempt to reduce distributional shifts by aligning manually selected graph elements (e.g., node attributes or structural statistics), which typically require manually designed graph filters to extract relevant features before alignment. However, such approaches are inflexible: they rely on scenario-specific heuristics, and struggle when dominant discrepancies vary across transfer scenarios. To address these limitations, we propose \textbf{ADAlign}, an Adaptive Distribution Alignment framework for GDA. Unlike heuristic methods, ADAlign requires no manual specification of alignment criteria. It automatically identifies the most relevant discrepancies in each transfer and aligns them jointly, capturing the interplay between attributes, structures, and their dependencies. This makes ADAlign flexible, scenario-aware, and robust to diverse and dynamically evolving shifts. To enable this adaptivity, we introduce the Neural Spectral Discrepancy (NSD), a theoretically principled parametric distance that provides a unified view of cross-graph shifts. NSD leverages neural characteristic function in the spectral domain to encode feature-structure dependencies of all orders, while a learnable frequency sampler adaptively emphasizes the most informative spectral components for each task via minimax paradigm. Extensive experiments on 10 datasets and 16 transfer tasks show that ADAlign not only outperforms state-of-the-art baselines but also achieves efficiency gains with lower memory usage and faster training.

</details>


### [88] [A Swap-Adversarial Framework for Improving Domain Generalization in Electroencephalography-Based Parkinson's Disease Prediction](https://arxiv.org/abs/2602.10528)
*Seongwon Jin,Hanseul Choi,Sunggu Yang,Sungho Park,Jibum Kim*

Main category: cs.LG

TL;DR: 本文针对帕金森病早期预测中皮层脑电图(ECoG)数据缺乏开放基准数据集的瓶颈问题，提出了一个基于6-羟基多巴胺(6-OHDA)诱导大鼠模型的新数据集和交换对抗框架(SAF)。该框架通过跨被试通道交换和领域对抗训练有效缓解了高被试间变异性和高维小样本(HDLSS)问题，在跨被试、跨会话和跨数据集测试中均显著优于基线方法，并展现出向脑电图(EEG)数据迁移的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 皮层脑电图(ECoG)相较于常规脑电图(EEG)在帕金森病预测中具备更高空间分辨率和频带范围优势，但受限于人脑研究的伦理约束和开放基准数据集的缺失，可复现的对比研究严重不足。此外，ECoG数据存在高被试间变异性、高维小样本(HDLSS)以及跨EEG/ECoG泛化性能差等核心挑战，亟需系统性解决方案。

Method: 研究提出交换对抗框架(SAF)，包含三个核心组件：(1) 鲁棒预处理；(2) 跨被试平衡通道交换(ISBCS)，通过随机交换不同被试间的通道实现数据增强，降低被试间变异性；(3) 领域对抗训练，通过对抗机制抑制被试特异性特征，迫使模型学习任务相关的共享特征。研究同时构建了首个基于6-羟基多巴胺(6-OHDA)大鼠模型的帕金森病预测ECoG基准数据集。

Result: 在严格的跨被试、跨会话和跨数据集测试中，SAF在所有设定下持续优于所有基线方法，在高变异性环境下性能提升最为显著。更重要的是，该方法在公开EEG基准数据集上实现了优异的跨数据集性能，证明了其在ECoG内部及向EEG数据迁移的强大泛化能力。

Conclusion: 本研究通过构建开放数据集和提出SAF框架，有效解决了ECoG在帕金森病预测中面临的核心挑战。该方法显著提升了模型鲁棒性和跨模态泛化能力，为帕金森病早期诊断提供了可靠工具。数据集和源代码将在发表后开源，以促进领域可复现性研究发展。

Abstract: Electroencephalography (ECoG) offers a promising alternative to conventional electrocorticography (EEG) for the early prediction of Parkinson's disease (PD), providing higher spatial resolution and a broader frequency range. However, reproducible comparisons has been limited by ethical constraints in human studies and the lack of open benchmark datasets. To address this gap, we introduce a new dataset, the first reproducible benchmark for PD prediction. It is constructed from long-term ECoG recordings of 6-hydroxydopamine (6-OHDA)-induced rat models and annotated with neural responses measured before and after electrical stimulation. In addition, we propose a Swap-Adversarial Framework (SAF) that mitigates high inter-subject variability and the high-dimensional low-sample-size (HDLSS) problem in ECoG data, while achieving robust domain generalization across ECoG and EEG-based Brain-Computer Interface (BCI) datasets. The framework integrates (1) robust preprocessing, (2) Inter-Subject Balanced Channel Swap (ISBCS) for cross-subject augmentation, and (3) domain-adversarial training to suppress subject-specific bias. ISBCS randomly swaps channels between subjects to reduce inter-subject variability, and domain-adversarial training jointly encourages the model to learn task-relevant shared features. We validated the effectiveness of the proposed method through extensive experiments under cross-subject, cross-session, and cross-dataset settings. Our method consistently outperformed all baselines across all settings, showing the most significant improvements in highly variable environments. Furthermore, the proposed method achieved superior cross-dataset performance between public EEG benchmarks, demonstrating strong generalization capability not only within ECoG but to EEG data. The new dataset and source code will be made publicly available upon publication.

</details>


### [89] [LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization](https://arxiv.org/abs/2602.10576)
*Boxiao Wang,Kai Li,Tianyi Liu,Chen Li,Junzhe Wang,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 本文提出PiT-PO（Physics-informed Token-regularized Policy Optimization）框架，通过强化学习将大语言模型（LLM）转化为自适应符号回归生成器。该框架核心为双重约束机制：层次化物理有效性强制与细粒度token级冗余惩罚，促使LLM生成科学一致且结构简约的数学方程。实验表明，PiT-PO在标准基准测试上达到最先进性能，成功发现湍流新模型，并使小规模模型超越闭源大模型，实现高性能科学发现的民主化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM符号回归方法将模型视为静态生成器，仅依赖提示级引导，无法基于搜索反馈更新内部表示，导致生成物理不一致或数学冗余的表达式，限制了其科学发现潜力。

Method: 提出PiT-PO统一框架：1）通过策略优化将LLM演进为自适应生成器；2）设计双重约束机制，包括层次化物理先验知识强制和token级结构冗余惩罚；3）通过强化学习对齐LLM，使其输出兼具物理一致性与结构简约性的方程。

Result: 1）在标准符号回归基准测试中达到state-of-the-art性能；2）成功发现具有挑战性的流体动力学湍流新模型；3）验证小规模模型经PiT-PO优化后可超越闭源大模型性能。

Conclusion: PiT-PO通过强化学习与双重约束机制有效解决了LLM符号回归中的物理一致性与结构冗余问题，显著提升了方程发现的质量与效率，实现了科学发现工具的民主化，为小型模型在科学发现任务中提供了竞争优势。

Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.

</details>


### [90] [Enhancing Ride-Hailing Forecasting at DiDi with Multi-View Geospatial Representation Learning from the Web](https://arxiv.org/abs/2602.10502)
*Xixuan Hao,Guicheng Li,Daiqiang Wu,Xusen Guo,Yumeng Zhu,Zhichao Zou,Peng Zhen,Yao Yao,Yuxuan Liang*

Main category: cs.LG

TL;DR: 本文提出MVGR-Net，一个两阶段框架，通过预学习地理空间表示并结合提示微调大语言模型，在滴滴真实数据集上实现了网约车需求预测的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 网约车服务已根本改变城市出行模式，准确预测对优化乘客体验和城市交通效率至关重要，但面临地理空间异质性和外部事件影响的重大挑战。

Method: 提出MVGR-Net两阶段框架：预训练阶段融合兴趣点和时序移动模式，从语义属性和移动模式双视角学习地理空间表示；预测阶段采用提示增强框架，在融入外部事件的同时微调大语言模型。

Result: 在滴滴真实数据集上的大量实验表明，该方法实现了最先进的性能。

Conclusion: 该框架有效解决了地理空间异质性和外部事件挑战，为网约车需求预测提供了创新性解决方案，具有实际应用价值。

Abstract: The proliferation of ride-hailing services has fundamentally transformed urban mobility patterns, making accurate ride-hailing forecasting crucial for optimizing passenger experience and urban transportation efficiency. However, ride-hailing forecasting faces significant challenges due to geospatial heterogeneity and high susceptibility to external events. This paper proposes MVGR-Net(Multi-View Geospatial Representation Learning), a novel framework that addresses these challenges through a two-stage approach. In the pretraining stage, we learn comprehensive geospatial representations by integrating Points-of-Interest and temporal mobility patterns to capture regional characteristics from both semantic attribute and temporal mobility pattern views. The forecasting stage leverages these representations through a prompt-empowered framework that fine-tunes Large Language Models while incorporating external events. Extensive experiments on DiDi's real-world datasets demonstrate the state-of-the-art performance.

</details>


### [91] [VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training](https://arxiv.org/abs/2602.10693)
*Guobin Shen,Chenxiao Zhao,Xiang Cheng,Lei Huang,Xing Yu*

Main category: cs.LG

TL;DR: 提出VESPO框架，通过变分形式引入方差缩减，推导出序列级重要性权重的闭式重塑核，无需长度归一化即可解决LLM强化学习中的训练稳定性问题，在高达64倍策略滞后下仍保持稳定训练。


<details>
  <summary>Details</summary>
Motivation: LLM强化学习面临训练稳定性挑战，主要源于策略滞后、异步训练和训练/推理引擎不匹配导致的行为策略与当前策略 divergence。重要性采样虽可校正分布偏移但方差高，现有token级裁剪和序列级归一化方法缺乏统一理论基础。

Method: VESPO将方差缩减融入关于提议分布的变分框架，推导出直接在序列级重要性权重上操作的闭式重塑核，避免了长度归一化，理论上统一了序列级重要性采样校正。

Result: 在数学推理基准测试上，VESPO在64倍策略滞后比和完全异步执行下保持稳定训练，为密集模型和混合专家模型带来一致性能提升。

Conclusion: VESPO通过变分方差缩减提供了理论统一的序列级重要性权重校正方法，显著提升了LLM强化学习在极端滞后和异步场景下的训练稳定性，具有广泛适用性。

Abstract: Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO

</details>


### [92] [Interpretable Graph-Level Anomaly Detection via Contrast with Normal Prototypes](https://arxiv.org/abs/2602.10708)
*Qiuran Zhao,Kai Ming Ting,Xinpeng Li*

Main category: cs.LG

TL;DR: 本文提出ProtoGLAD，一种基于原型的可解释图级异常检测框架，通过点集核迭代发现正常原型图并与异常图对比，在保持竞争力的检测性能同时提供人类可解释的解释。


<details>
  <summary>Details</summary>
Motivation: 深度GLAD方法存在黑盒问题，限制了其可靠性和实际应用。现有解释方法要么缺乏正常图参照，要么仅使用抽象潜在向量而非具体图作为原型，无法提供充分的解释性。

Method: 提出ProtoGLAD框架：采用点集核从数据集中迭代发现多个正常原型图及其聚类，将远离所有正常簇的图识别为异常，并通过与最近正常原型图的显式对比为每个异常提供解释。

Result: 在多个真实世界数据集上的实验表明，ProtoGLAD实现了与先进GLAD方法相当的异常检测性能，同时提供了更优的人类可解释的原型解释。

Conclusion: ProtoGLAD成功解决了图级异常检测中的可解释性问题，通过显式使用正常图原型进行对比，既保证了检测性能又增强了模型透明度，为实际部署提供了更可靠的解决方案。

Abstract: The task of graph-level anomaly detection (GLAD) is to identify anomalous graphs that deviate significantly from the majority of graphs in a dataset. While deep GLAD methods have shown promising performance, their black-box nature limits their reliability and deployment in real-world applications. Although some recent methods have made attempts to provide explanations for anomaly detection results, they either provide explanations without referencing normal graphs, or rely on abstract latent vectors as prototypes rather than concrete graphs from the dataset. To address these limitations, we propose Prototype-based Graph-Level Anomaly Detection (ProtoGLAD), an interpretable unsupervised framework that provides explanation for each detected anomaly by explicitly contrasting with its nearest normal prototype graph. It employs a point-set kernel to iteratively discover multiple normal prototype graphs and their associated clusters from the dataset, then identifying graphs distant from all discovered normal clusters as anomalies. Extensive experiments on multiple real-world datasets demonstrate that ProtoGLAD achieves competitive anomaly detection performance compared to state-of-the-art GLAD methods while providing better human-interpretable prototype-based explanations.

</details>


### [93] [Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models](https://arxiv.org/abs/2602.10520)
*Williams Jonathan,Tureci Esin*

Main category: cs.LG

TL;DR: 本文提出了 RLTT（奖励潜在思维轨迹），一种用于增强循环语言模型（LoopLMs）推理能力的强化学习框架。与仅奖励最终状态的 GRPO 不同，RLTT 在整个潜在推理轨迹上分配奖励，在数学推理基准测试上显著优于 GRPO（MATH-500 提升 +14.4%，AIME24 提升 +16.6%），且无需外部验证器即可迁移至非数学推理任务。


<details>
  <summary>Details</summary>
Motivation: 循环语言模型（LoopLMs）通过多步潜在推理在较小参数量下表现出优异推理性能。然而，使用强化学习（如 GRPO）进一步改善其推理能力时遇到根本性障碍——标准目标函数仅对最终潜在状态赋奖，与模型内部多步计算过程存在不匹配，导致信用分配稀疏且低效。

Method: 提出 RLTT（Reward Latent Thought Trajectories）框架，将奖励信号分布到整个潜在推理轨迹中，实现密集轨迹级信用分配。该框架无需外部验证器，可直接替代 GRPO 且开销可忽略。

Result: 在 Ouro-2.6B-Thinking 模型上的实验表明，与 GRPO 相比，RLTT 在数学推理基准测试上取得显著提升：MATH-500 准确率 +14.4%，AIME24 提升 +16.6%，BeyondAIME 提升 +10.0%。此外，仅在数学数据上训练的模型还能有效迁移至非数学推理基准测试。

Conclusion: RLTT 验证了轨迹级信用分配在循环语言模型强化学习中的有效性，为提升潜在推理能力提供了新范式，且无需依赖外部验证器即可实现跨领域知识迁移。

Abstract: Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives such as Group Relative Policy Optimization (GRPO) only assign credit to the final latent state, creating a fundamental mismatch with the model's internal computation. To resolve this, we introduce RLTT (Reward Latent Thought Trajectories), a reinforcement learning framework which distributes reward across the full latent reasoning trajectory. RLTT provides dense, trajectory-level credit assignment without relying on external verifiers and can directly replace GRPO with negligible overhead. Across extensive experiments with Ouro-2.6B-Thinking under identical training and inference conditions, RLTT yields substantial improvements over GRPO on challenging mathematical reasoning benchmarks, improving accuracy by +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME. Despite being trained exclusively on mathematics, RLTT also transfers effectively to non-mathematical reasoning benchmarks, demonstrating the effectiveness of trajectory-level credit assignment for reinforcement learning in LoopLMs.

</details>


### [94] [What Makes Value Learning Efficient in Residual Reinforcement Learning?](https://arxiv.org/abs/2602.10539)
*Guozheng Ma,Lu Li,Haoyu Wang,Zixuan Liu,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: 针对残差强化学习中的值学习瓶颈，本文提出DAWN框架，通过数据锚定预热和归一化解决冷启动和尺度失配问题，实现高效稳定的策略优化。


<details>
  <summary>Details</summary>
Motivation: 残差强化学习通过冻结基础策略并学习有界修正来实现稳定在线微调，但值学习存在冷启动和结构尺度失配两个关键瓶颈，导致对基础策略周围值景观缺乏认知且残差贡献被基础动作淹没，亟需针对性解决方案。

Method: 提出DAWN（Data-Anchored Warmup and Normalization）框架，利用基础策略转移作为隐式预热的数据锚点，通过评论归一化恢复表示敏感性，从而解决值学习瓶颈。

Result: DAWN在多样化基准测试、策略架构和观测模态上均表现出显著效率提升，验证了所提方法的有效性。

Conclusion: 通过系统分析残差强化学习的值学习瓶颈机制，提出的DAWN框架为高效稳定的在线策略优化提供了简洁而原则性的解决方案。

Abstract: Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.

</details>


### [95] [Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy](https://arxiv.org/abs/2602.10544)
*Wuyang Zhang,Zhen Luo,Chuqiao Gu,Jianming Ma,Yebo Cao,Wangming Yuan,Yinzhi Jin*

Main category: cs.LG

TL;DR: 该论文提出一种混合架构的自动化EEG监测系统，通过将信号提取与文本生成分离，结合多速率采样、跨模态桥接和参数高效微调，解决了大语言模型处理长EEG信号时的极端压缩损失和时间精度问题，实现了亚临床级别的测量精度并显著降低误报率。


<details>
  <summary>Details</summary>
Motivation: 临床EEG监测需要医生级别的精确度进行癫痫检测和报告，但EEG记录远超LLM上下文窗口，必须进行400:1以上的极端压缩，这会破坏精细时间精度（0.5 Hz误差即可混淆失神癫痫与Lennox-Gastaut综合征）。LLM缺乏内在时间序列理解能力，依赖压缩表示的统计关联，易产生临床测量值的幻觉。

Method: 采用测量提取与文本生成分离的混合架构：1）压缩前通过信号处理计算精确临床数值；2）跨模态桥接实现EEG到语言转换；3）参数高效微调配合约束解码和冻结槽位；4）多速率采样保持长程上下文同时保留事件级精度。

Result: 在TUH和CHB-MIT数据集评估中，该系统实现60%误报率降低、50%检测速度提升，达到亚临床测量精度，是首个保证自动化EEG报告临床测量准确性的系统。

Conclusion: 该创新混合架构成功解决了LLM处理长EEG信号时的压缩损失与时间精度矛盾，为临床级自动化EEG监测提供了可靠解决方案，显著提升了检测性能和测量可靠性。

Abstract: Automated EEG monitoring requires clinician-level precision for seizure detection and reporting. Clinical EEG recordings exceed LLM context windows, requiring extreme compression (400:1+ ratios) that destroys fine-grained temporal precision. A 0.5 Hz error distinguishes absence epilepsy from Lennox-Gastaut syndrome. LLMs lack inherent time-series comprehension and rely on statistical associations from compressed representations. This dual limitation causes systems to hallucinate clinically incorrect measurement values.
  We separate measurement extraction from text generation. Our hybrid architecture computes exact clinical values via signal processing before compression, employs a cross-modal bridge for EEG-to-language translation, and uses parameter-efficient fine-tuning with constrained decoding around frozen slots. Multirate sampling maintains long-range context while preserving event-level precision. Evaluation on TUH and CHB-MIT datasets achieves 60% fewer false alarms, 50% faster detection, and sub-clinical measurement precision. This is the first system guaranteeing clinical measurement accuracy in automated EEG reports.

</details>


### [96] [Just on Time: Token-Level Early Stopping for Diffusion Language Models](https://arxiv.org/abs/2602.11133)
*Zahar Kohut,Severyn Shykula,Dmytro Khamula,Mykola Vysotskyi,Taras Rumezhak,Volodymyr Karpiv*

Main category: cs.LG

TL;DR: 针对扩散语言模型在文本生成过程中计算效率低下的问题，本文提出了一种无需训练的早期停止方法，通过动态检测每个token的收敛状态来减少不必要的去噪步骤，在保持生成质量的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型通过迭代细化生成文本，但许多token在最终去噪步骤前就已达到稳定状态，导致大量计算资源浪费，这种计算低效性限制了模型的实际应用部署。

Method: 提出一种训练无关的token级早期停止策略，利用模型预测和局部上下文提取轻量级信号，独立判断每个位置的token是否收敛，实现自适应的逐token冻结机制，无需任务特定的微调。

Result: 在数学推理、通用问答和科学理解等多个基准测试上，该方法显著减少了所需的扩散步骤总数，实现了最先进的效率提升，同时保持了生成质量。

Conclusion: 该方法通过动态识别token级收敛状态，有效解决了扩散语言模型的计算冗余问题，为提升生成效率提供了简单而有效的解决方案，具有广泛的适用性和实用价值。

Abstract: Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model's predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.

</details>


### [97] [TRACE: Theoretical Risk Attribution under Covariate-shift Effects](https://arxiv.org/abs/2602.10588)
*Hosein Anjidani,S. Yahya S. R. Tehrani,Mohammad Mahdi Mojahedian,Mohammad Hossein Yassaee*

Main category: cs.LG

TL;DR: 该论文提出TRACE框架，用于在协变量偏移下分解和计算两个模型之间的风险变化。该框架将风险变化分解为四个可解释的因素，并提供可计算的估计方法，可用于模型部署的安全决策。


<details>
  <summary>Details</summary>
Motivation: 当用在新数据上训练的模型替换源域训练的模型时，其在源域上的性能变化不可预测。现有方法缺乏对这种风险变化的理论理解和可解释的诊断工具。

Method: 提出TRACE框架，理论上将|ΔR|分解为四个因素：两个泛化间隙、模型变化惩罚和协变量偏移惩罚。通过模型敏感因子、最优传输/MMD、模型输出距离和验证集数据使每个项可计算。

Result: 在线性回归设置中验证了理论界；在合成和视觉基准测试中，TRACE诊断与真实性能下降保持强单调关系；部署门控分数与|ΔR|强相关，并能实现高AUROC/AUPRC。

Conclusion: TRACE为理解模型替换时的性能变化提供了可解释的理论框架，其计算的诊断指标能有效指导安全、标签高效的模型部署决策。

Abstract: When a source-trained model $Q$ is replaced by a model $\tilde{Q}$ trained on shifted data, its performance on the source domain can change unpredictably. To address this, we study the two-model risk change, $ΔR := R_P(\tilde{Q}) - R_P(Q)$, under covariate shift. We introduce TRACE (Theoretical Risk Attribution under Covariate-shift Effects), a framework that decomposes $|ΔR|$ into an interpretable upper bound. This decomposition disentangles the risk change into four actionable factors: two generalization gaps, a model change penalty, and a covariate shift penalty, transforming the bound into a powerful diagnostic tool for understanding why performance has changed. To make TRACE a fully computable diagnostic, we instantiate each term. The covariate shift penalty is estimated via a model sensitivity factor (from high-quantile input gradients) and a data-shift measure; we use feature-space Optimal Transport (OT) by default and provide a robust alternative using Maximum Mean Discrepancy (MMD). The model change penalty is controlled by the average output distance between the two models on the target sample. Generalization gaps are estimated on held-out data. We validate our framework in an idealized linear regression setting, showing the TRACE bound correctly captures the scaling of the true risk difference with the magnitude of the shift. Across synthetic and vision benchmarks, TRACE diagnostics are valid and maintain a strong monotonic relationship with the true performance degradation. Crucially, we derive a deployment gate score that correlates strongly with $|ΔR|$ and achieves high AUROC/AUPRC for gating decisions, enabling safe, label-efficient model replacement.

</details>


### [98] [Resource-Efficient Model-Free Reinforcement Learning for Board Games](https://arxiv.org/abs/2602.10894)
*Kazuki Ota,Takayuki Osa,Motoki Omura,Tatsuya Harada*

Main category: cs.LG

TL;DR: 针对棋类游戏AI中基于搜索的强化学习方法（如AlphaZero）计算成本高、难以复现的问题，本研究提出一种模型-free强化学习算法，在Animal Shogi、Gardner Chess、Go、Hex和Othello五种游戏中均实现更高效的学习，并通过消融实验验证了核心技术的有效性，展现了无模型方法在传统搜索主导领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 基于搜索的强化学习方法（如AlphaZero）在棋类游戏中取得显著成功，但其巨大的计算需求成为可复现性的障碍，阻碍了方法的广泛应用和进一步研究。

Method: 提出一种专为棋类游戏设计的模型-free强化学习算法，通过实验在Animal Shogi、Gardner Chess、Go、Hex和Othello五种游戏环境中验证其效率，并进行广泛的消融研究分析核心技术的贡献。

Result: 在五种棋类游戏中，所提方法均实现了比现有方法更高效的学习；消融研究明确显示了算法中核心技术的关键作用。

Conclusion: 该高效算法证明了无模型强化学习在传统由搜索方法主导的领域中具有巨大潜力，为降低计算成本和提高可复现性提供了可行方案。

Abstract: Board games have long served as complex decision-making benchmarks in artificial intelligence. In this field, search-based reinforcement learning methods such as AlphaZero have achieved remarkable success. However, their significant computational demands have been pointed out as barriers to their reproducibility. In this study, we propose a model-free reinforcement learning algorithm designed for board games to achieve more efficient learning. To validate the efficiency of the proposed method, we conducted comprehensive experiments on five board games: Animal Shogi, Gardner Chess, Go, Hex, and Othello. The results demonstrate that the proposed method achieves more efficient learning than existing methods across these environments. In addition, our extensive ablation study shows the importance of core techniques used in the proposed method. We believe that our efficient algorithm shows the potential of model-free reinforcement learning in domains traditionally dominated by search-based methods.

</details>


### [99] [Weight Decay Improves Language Model Plasticity](https://arxiv.org/abs/2602.11137)
*Tessa Han,Sebastian Bordt,Hanlin Zhang,Sham Kakade*

Main category: cs.LG

TL;DR: 本研究从模型可塑性视角重新审视大语言模型预训练，发现更大的权重衰减能提升模型微调后的性能增益。即使预训练后的基础模型性能较差，其微调后可能表现更佳。机制上，权重衰减促进线性可分表示、正则化注意力矩阵并减少过拟合，揭示了超参数优化需超越交叉熵损失的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要基于基础模型的验证损失来优化超参数和研究缩放定律，却忽略了模型对下游任务的适应能力（可塑性），导致对权重衰减等关键正则化参数的理解不全面。

Method: 通过系统实验，从模型可塑性角度研究预训练过程，重点分析权重衰减对模型微调前后性能的影响，并深入探究其对模型表征、注意力机制和过拟合的 mechanistic effects。

Result: 使用更大权重衰减训练的模型更具可塑性，在微调时表现出更大的性能增益。这导致反直觉权衡：预训练后性能较差的基础模型微调后可能表现更好。机制上，权重衰减促进线性可分表示、正则化注意力矩阵并减少训练数据过拟合。

Conclusion: 该研究证明了在超参数优化中采用超越交叉熵损失的评估指标的重要性，阐明了权重衰减在塑造模型行为方面的多方面作用，为改进大语言模型预训练提供了新视角。

Abstract: The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.

</details>


### [100] [Roughness-Informed Federated Learning](https://arxiv.org/abs/2602.10595)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: 提出RI-FedAvg算法，通过引入粗糙度指数(RI)正则化项缓解联邦学习中的客户端漂移问题，提升在非独立同分布(non-IID)设置下的收敛性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能保护数据隐私，但在非独立同分布数据场景下存在客户端漂移问题，导致模型收敛困难。现有FedAvg等方法在应对异构数据分布时效果有限，需要更鲁棒的优化策略。

Method: 提出RI-FedAvg算法，利用粗糙度指数量化高维损失函数的崎岖程度，在本地目标函数中加入基于RI的自适应正则化项，对损失景观波动较大的参数更新施加惩罚。提供非凸目标函数的严格收敛分析，证明在标准假设下可收敛至稳定点。

Result: 在MNIST、CIFAR-10和CIFAR-100数据集上的大量实验表明，RI-FedAvg在非独立同分布场景下显著优于FedAvg、FedProx、FedDyn、SCAFFOLD和DP-FedAvg等先进基线方法，实现了更高的分类准确率和更快的收敛速度。

Conclusion: 本研究通过粗糙度指数有效解决了联邦学习在异构环境中的优化挑战，提升了算法的鲁棒性和效率，为联邦学习在实际复杂场景中的应用提供了有力支持。

Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet faces challenges in non-independent and identically distributed (non-IID) settings due to client drift, which impairs convergence. We propose RI-FedAvg, a novel FL algorithm that mitigates client drift by incorporating a Roughness Index (RI)-based regularization term into the local objective, adaptively penalizing updates based on the fluctuations of local loss landscapes. This paper introduces RI-FedAvg, leveraging the RI to quantify the roughness of high-dimensional loss functions, ensuring robust optimization in heterogeneous settings. We provide a rigorous convergence analysis for non-convex objectives, establishing that RI-FedAvg converges to a stationary point under standard assumptions. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that RI-FedAvg outperforms state-of-the-art baselines, including FedAvg, FedProx, FedDyn, SCAFFOLD, and DP-FedAvg, achieving higher accuracy and faster convergence in non-IID scenarios. Our results highlight RI-FedAvg's potential to enhance the robustness and efficiency of federated learning in practical, heterogeneous environments.

</details>


### [101] [Learning Mixture Density via Natural Gradient Expectation Maximization](https://arxiv.org/abs/2602.10602)
*Yutao Chen,Jasmine Bayrooti,Steven Morad*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mixture density networks are neural networks that produce Gaussian mixtures to represent continuous multimodal conditional densities. Standard training procedures involve maximum likelihood estimation using the negative log-likelihood (NLL) objective, which suffers from slow convergence and mode collapse. In this work, we improve the optimization of mixture density networks by integrating their information geometry. Specifically, we interpret mixture density networks as deep latent-variable models and analyze them through an expectation maximization framework, which reveals surprising theoretical connections to natural gradient descent. We then exploit such connections to derive the natural gradient expectation maximization (nGEM) objective. We show that empirically nGEM achieves up to 10$\times$ faster convergence while adding almost zerocomputational overhead, and scales well to high-dimensional data where NLL otherwise fails.

</details>


### [102] [dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning](https://arxiv.org/abs/2602.10603)
*Arnav Shah,Junzhe Li,Parsa Idehpour,Adibvafa Fallahpour,Brandon Wang,Sukjun Hwang,Bo Wang,Patrick D. Hsu,Hani Goodarzi,Albert Gu*

Main category: cs.LG

TL;DR: dnaHNet是一种新型无标记器的基因组基础模型，通过可微分动态分块机制自适应地将核苷酸压缩为潜在标记，解决了固定词表分词器破坏生物基序与核苷酸级模型计算成本过高的问题，在零样本任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型在输入表示上存在根本性权衡：标准固定词表分词器会破坏密码子和调控元件等生物意义基序，而核苷酸级模型虽保持生物一致性，但对长序列的计算成本过高。

Method: 提出dnaHNet——一种无标记器的自回归模型，采用可微分动态分块机制，端到端地分割和建模基因组序列，自适应地平衡压缩率与预测准确性。该模型在原生生物基因组上进行预训练。

Result: 1）在扩展性和效率上优于StripedHyena2等领先架构；2）递归分块带来二次方的FLOP降低，使推理速度比Transformer提升3倍以上；3）在零样本任务中，能更准确地预测蛋白质变体适应度和基因必需性；4）无需监督即可自动发现层次化生物结构。

Conclusion: dnaHNet为下一代基因组建模建立了可扩展、可解释的框架，其自适应压缩机制有效解决了生物保真度与计算效率的矛盾。

Abstract: Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts. We introduce dnaHNet, a state-of-the-art tokenizer-free autoregressive model that segments and models genomic sequences end-to-end. Using a differentiable dynamic chunking mechanism, dnaHNet compresses raw nucleotides into latent tokens adaptively, balancing compression with predictive accuracy. Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency. This recursive chunking yields quadratic FLOP reductions, enabling $>3 \times$ inference speedup over Transformers. On zero-shot tasks, dnaHNet achieves superior performance in predicting protein variant fitness and gene essentiality, while automatically discovering hierarchical biological structures without supervision. These results establish dnaHNet as a scalable, interpretable framework for next-generation genomic modeling.

</details>


### [103] [OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories](https://arxiv.org/abs/2602.11018)
*Returaj Burnwal,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 本文提出OSIL算法，针对离线安全模仿学习问题，通过从非偏好轨迹推断安全约束，在无显式安全成本标注条件下学习安全且奖励最大化的策略。


<details>
  <summary>Details</summary>
Motivation: 现实世界中在线学习风险高，且显式定义安全成本困难；但收集不安全行为轨迹（非偏好轨迹）相对容易，可利用这些轨迹隐式表达应避免的行为。

Method: 将问题形式化为约束马尔可夫决策过程(CMDP)，通过推导奖励最大化目标的下界并训练成本模型来评估非偏好行为概率，从而从离线演示中学习安全策略。

Result: 实验验证该方法能在满足安全成本约束的同时保持奖励性能，所学习策略比多个基线方法更安全且有效。

Conclusion: OSIL算法有效解决了离线安全模仿学习问题，仅通过演示数据即可实现安全且高性能的策略学习。

Abstract: This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, and specifying accurate safety costs can be difficult. However, it is often feasible to collect trajectories that reflect undesirable or unsafe behavior, implicitly conveying what the agent should avoid. We refer to these as non-preferred trajectories. We propose a novel offline safe IL algorithm, OSIL, that infers safety from non-preferred demonstrations. We formulate safe policy learning as a Constrained Markov Decision Process (CMDP). Instead of relying on explicit safety cost and reward annotations, OSIL reformulates the CMDP problem by deriving a lower bound on reward maximizing objective and learning a cost model that estimates the likelihood of non-preferred behavior. Our approach allows agents to learn safe and reward-maximizing behavior entirely from offline demonstrations. We empirically demonstrate that our approach can learn safer policies that satisfy cost constraints without degrading the reward performance, thus outperforming several baselines.

</details>


### [104] [Generative clinical time series models trained on moderate amounts of patient data are privacy preserving](https://arxiv.org/abs/2602.10631)
*Rustam Zhumagambetov,Niklas Giesa,Sebastian D. Boie,Stefan Haufe*

Main category: cs.LG

TL;DR: 该研究针对医疗时间序列数据隐私问题，通过隐私攻击审计了基于MIMIC-IV训练的最新生成模型。结果表明，大规模数据训练的合成数据能有效抵御攻击，而差分隐私机制会降低效用且无益于隐私提升。


<details>
  <summary>Details</summary>
Motivation: 医疗数据共享因隐私风险受限，合成数据被视为解决方案，但现有生成模型无法保证隐私保护。传统匿名化技术有限，而差分隐私机制易导致训练不稳定和效用损失，故隐私审计必不可少。

Method: 研究对基于MIMIC-IV数据集训练的最先进时间序列生成模型实施了一系列标准化隐私攻击，并利用eICU数据集进行跨数据集攻击测试。同时分析了差分隐私机制应用的效果。

Result: 实验显示，当训练数据集足够大时，现有隐私攻击无法破解生成的多元临床时间序列。此外，引入差分隐私机制非但不能改善隐私，反而会显著降低合成数据在机器学习预测任务中的效用。

Conclusion: 论文结论认为，当前隐私审计不可或缺，但大规模数据集训练的合成数据生成器已具备较强的隐私保护能力。差分隐私机制在此场景下得不偿失，应探索更有效的隐私保护方法。

Abstract: Sharing medical data for machine learning model training purposes is often impossible due to the risk of disclosing identifying information about individual patients. Synthetic data produced by generative artificial intelligence (genAI) models trained on real data is often seen as one possible solution to comply with privacy regulations. While powerful genAI models for heterogeneous hospital time series have recently been introduced, such modeling does not guarantee privacy protection, as the generated data may still reveal identifying information about individuals in the models' training cohort. Applying established privacy mechanisms to generative time series models, however, proves challenging as post-hoc data anonymization through k-anonymization or similar techniques is limited, while model-centered privacy mechanisms that implement differential privacy (DP) may lead to unstable training, compromising the utility of generated data. Given these known limitations, privacy audits for generative time series models are currently indispensable regardless of the concrete privacy mechanisms applied to models and/or data. In this work, we use a battery of established privacy attacks to audit state-of-the-art hospital time series models, trained on the public MIMIC-IV dataset, with respect to privacy preservation. Furthermore, the eICU dataset was used to mount a privacy attack against the synthetic data generator trained on the MIMIC-IV dataset. Results show that established privacy attacks are ineffective against generated multivariate clinical time series when synthetic data generators are trained on large enough training datasets. Furthermore, we discuss how the use of existing DP mechanisms for these synthetic data generators would not bring desired improvement in privacy, but only a decrease in utility for machine learning prediction tasks.

</details>


### [105] [Coarse-Grained Boltzmann Generators](https://arxiv.org/abs/2602.10637)
*Weilong Chen,Bojun Zhao,Jan Eckwert,Julija Zavadlav*

Main category: cs.LG

TL;DR: 提出粗粒化玻尔兹曼生成器（CG-BGs），在粗粒化空间结合流模型与重要性采样，通过力匹配高效学习平均力势（PMF），实现大规模分子系统的可扩展无偏采样。


<details>
  <summary>Details</summary>
Motivation: 分子构型玻尔兹曼分布采样是长期挑战。现有玻尔兹曼生成器（BGs）可扩展性受限，而粗粒化模型缺乏保证统计正确性的重加权过程，亟需兼具扩展性与统计准确性的方法。

Method: 在粗粒化坐标空间中构建CG-BGs框架：使用基于流的模型生成样本，学习平均力势（PMF）进行重要性重加权。创新性地通过力匹配从快速收敛数据中高效学习PMF，统一可扩展降阶建模与精确重要性采样。

Result: 成功证明CG-BGs可在高度降维表示中忠实捕捉显式溶剂介导的复杂相互作用，为大规模分子体系无偏采样提供可扩展路径。

Conclusion: CG-BGs建立了统一可扩展降阶建模与重要性采样精确性的原则性框架，为更大分子系统的平衡态采样开辟了新范式。

Abstract: Sampling equilibrium molecular configurations from the Boltzmann distribution is a longstanding challenge. Boltzmann Generators (BGs) address this by combining exact-likelihood generative models with importance sampling, but their practical scalability is limited. Meanwhile, coarse-grained surrogates enable the modeling of larger systems by reducing effective dimensionality, yet often lack the reweighting process required to ensure asymptotically correct statistics. In this work, we propose Coarse-Grained Boltzmann Generators (CG-BGs), a principled framework that unifies scalable reduced-order modeling with the exactness of importance sampling. CG-BGs act in a coarse-grained coordinate space, using a learned potential of mean force (PMF) to reweight samples generated by a flow-based model. Crucially, we show that this PMF can be efficiently learned from rapidly converged data via force matching. Our results demonstrate that CG-BGs faithfully capture complex interactions mediated by explicit solvent within highly reduced representations, establishing a scalable pathway for the unbiased sampling of larger molecular systems.

</details>


### [106] [Evaluation metrics for temporal preservation in synthetic longitudinal patient data](https://arxiv.org/abs/2602.10643)
*Katariina Perkonoja,Parisa Movahedi,Antti Airola,Kari Auranen,Joni Virta*

Main category: cs.LG

TL;DR: 本研究针对合成纵向患者数据的时序保持性评估问题，提出了一套包含边际、协方差、个体水平和测量结构四个维度的评估指标，揭示了单一指标评估的局限性，并分析了数据质量和预处理方法对时序保持性的影响，为生成模型的改进和高质量合成数据创建提供支持。


<details>
  <summary>Details</summary>
Motivation: 现有合成纵向患者数据评估方法往往仅关注边际层面的相似性，可能掩盖协方差结构和个体轨迹的时序失真问题。缺乏系统性的时序保持性评估框架，导致难以全面理解生成数据在时间维度上的真实性，限制了合成数据在医疗研究和决策中的应用可靠性。

Method: 提出一套多维评估指标体系，从边际分布、协方差结构、个体级轨迹模式和测量时间结构四个层面量化合成数据对原始数据时序特性的保持程度。通过理论分析揭示各维度间的相互关系，并考察原始数据质量、测量频率及预处理策略（分箱、变量编码、精度控制）等因素对时序保持性的影响机制。

Result: 研究发现：1) 边际层面的高度相似可能掩盖协方差失真和个体轨迹破坏；2) 稀疏或不规则测量时间的变量因信息有限，难以学习时序依赖，导致合成数据保真度降低；3) 数据质量、测量频率和预处理方法显著影响时序保持性；4) 单一指标无法全面评估时序保持性，需采用多维度综合评价体系。

Conclusion: 该评估框架能够清晰揭示时序结构保持或退化的机制，为生成模型的性能改进提供诊断依据，支持开发更可靠的合成纵向患者数据生成方法，从而促进合成数据在医疗领域的可信应用。

Abstract: This study introduces a set of metrics for evaluating temporal preservation in synthetic longitudinal patient data, defined as artificially generated data that mimic real patients' repeated measurements over time. The proposed metrics assess how synthetic data reproduces key temporal characteristics, categorized into marginal, covariance, individual-level and measurement structures. We show that strong marginal-level resemblance may conceal distortions in covariance and disruptions in individual-level trajectories. Temporal preservation is influenced by factors such as original data quality, measurement frequency, and preprocessing strategies, including binning, variable encoding and precision. Variables with sparse or highly irregular measurement times provide limited information for learning temporal dependencies, resulting in reduced resemblance between the synthetic and original data. No single metric adequately captures temporal preservation; instead, a multidimensional evaluation across all characteristics provides a more comprehensive assessment of synthetic data quality. Overall, the proposed metrics clarify how and why temporal structures are preserved or degraded, enabling more reliable evaluation and improvement of generative models and supporting the creation of temporally realistic synthetic longitudinal patient data.

</details>


### [107] [Domain Knowledge Guided Bayesian Optimization For Autonomous Alignment Of Complex Scientific Instruments](https://arxiv.org/abs/2602.10670)
*Aashwin Mishra,Matt Seaberg,Ryan Roussel,Daniel Ratner,Apurva Mehta*

Main category: cs.LG

TL;DR: 针对高维紧耦合参数和稀疏奖励场景下贝叶斯优化的性能退化问题，提出一种融入领域知识的贝叶斯优化方法。通过坐标变换解耦输入特征并对齐主动子空间，在12维6晶体分束延迟光学系统中验证有效，结合反向退火探索策略可可靠收敛到全局最优解。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在复杂非线性系统优化中表现出色，但在高维、参数紧耦合、高度非对称且奖励稀疏的"大海捞针"场景下性能显著下降。即使是TurBO等先进方法也常效果不佳。随着大型望远镜和X射线自由电子激光光谱仪等复杂科学仪器的部署，对稳健高维优化的需求日益增长。

Method: 提出领域知识引导的贝叶斯优化方法，利用物理洞察力从根本上简化搜索问题：通过坐标变换实现输入特征的解耦，并将主动子空间与主要搜索轴对齐。同时结合反向退火探索策略。

Result: 在12维6晶体分束延迟光学系统上验证，传统方法（标准BO、TuRBO、多目标BO）均失败。新方法结合反向退火策略能可靠收敛到全局最优解。坐标变换是关键，通过使输入坐标轴与问题主动子空间对齐，显著加速搜索。

Conclusion: 该成果展示了一种可推广的范式：利用物理洞察将高维耦合优化问题转化为更简单的表示，能够在保留现有优化算法的同时，实现对复杂科学仪器的快速稳健自动调优，确保持续高性能。

Abstract: Bayesian Optimization (BO) is a powerful tool for optimizing complex non-linear systems. However, its performance degrades in high-dimensional problems with tightly coupled parameters and highly asymmetric objective landscapes, where rewards are sparse. In such needle-in-a-haystack scenarios, even advanced methods like trust-region BO (TurBO) often lead to unsatisfactory results. We propose a domain knowledge guided Bayesian Optimization approach, which leverages physical insight to fundamentally simplify the search problem by transforming coordinates to decouple input features and align the active subspaces with the primary search axes. We demonstrate this approach's efficacy on a challenging 12-dimensional, 6-crystal Split-and-Delay optical system, where conventional approaches, including standard BO, TuRBO and multi-objective BO, consistently led to unsatisfactory results. When combined with an reverse annealing exploration strategy, this approach reliably converges to the global optimum. The coordinate transformation itself is the key to this success, significantly accelerating the search by aligning input co-ordinate axes with the problem's active subspaces. As increasingly complex scientific instruments, from large telescopes to new spectrometers at X-ray Free Electron Lasers are deployed, the demand for robust high-dimensional optimization grows. Our results demonstrate a generalizable paradigm: leveraging physical insight to transform high-dimensional, coupled optimization problems into simpler representations can enable rapid and robust automated tuning for consistent high performance while still retaining current optimization algorithms.

</details>


### [108] [In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution](https://arxiv.org/abs/2602.11079)
*Frank Xiao,Santiago Aranguri*

Main category: cs.LG

TL;DR: 提出基于激活的数据归因方法，通过计算激活差异向量并排序余弦相似度，追溯后训练语言模型的行为变化到具体训练数据点。应用于OLMo 2的DPO训练时，发现并缓解了"干扰触发合规"的有害行为，效果优于基线方法且成本更低。


<details>
  <summary>Details</summary>
Motivation: 后训练语言模型的行为变化难以溯源到具体训练数据，现有方法效果不佳或成本高昂。需要一种高效的方法来识别导致特定行为（尤其是有害行为）的数据点，以便进行因果验证和改进模型安全性。

Method: 提出基于激活的数据归因方法：1) 为测试提示和偏好对计算激活差异向量；2) 通过余弦相似度排序，识别导致特定行为的训练数据点；3) 通过修改数据重新训练进行因果验证；4) 对行为-数据点相似度矩阵进行聚类，无监督发现涌现行为。

Result: 在OLMo 2的DPO训练中发现"干扰触发合规"行为：当附加良性格式指令时，模型会遵从危险请求。过滤排名靠前的数据点使该行为减少63%，反转标签减少78%。该方法性能优于基于梯度的归因和LLM-judge基线，且成本降低10倍以上。

Conclusion: 该方法成功识别了由污染偏好数据自然产生的有害行为（而非故意注入），为安全技术提供了真实基准。证明了激活归因在模型安全改进中的有效性和高效性。

Abstract: We propose activation-based data attribution, a method that traces behavioral changes in post-trained language models to responsible training datapoints. By computing activation-difference vectors for both test prompts and preference pairs and ranking by cosine similarity, we identify datapoints that cause specific behaviors and validate these attributions causally by retraining with modified data. Clustering behavior-datapoint similarity matrices also enables unsupervised discovery of emergent behaviors. Applying this to OLMo 2's production DPO training, we surfaced distractor-triggered compliance: a harmful behavior where the model complies with dangerous requests when benign formatting instructions are appended. Filtering top-ranked datapoints reduces this behavior by 63% while switching their labels achieves 78%. Our method outperforms gradient-based attribution and LLM-judge baselines while being over 10 times cheaper than both. This in-the-wild model organism - emerging from contaminated preference data rather than deliberate injection - provides a realistic benchmark for safety techniques.

</details>


### [109] [GRASP: group-Shapley feature selection for patients](https://arxiv.org/abs/2602.11084)
*Yuheng Luo,Shuyan Li,Zhong Cao*

Main category: cs.LG

TL;DR: 本文提出GRASP框架以解决医疗预测中的特征选择问题。该方法将基于Shapley加性解释（SHAP）的归因分析与组L21正则化相结合，首先从预训练树模型中提取组级特征重要性评分，继而通过组L21正则化逻辑回归强制实现结构化稀疏，从而获得稳定且可解释的特征子集。与LASSO、SHAP及深度学习方法对比，GRASP在保持相当或更优预测性能的同时，能选出更少、冗余更低、更稳定的特征。


<details>
  <summary>Details</summary>
Motivation: 医疗预测中的特征选择面临重大挑战，现有方法如LASSO在鲁棒性和可解释性方面存在不足，难以在临床场景中提供既稳定又易于理解的特征子集。

Method: GRASP框架采用两阶段策略：第一阶段利用SHAP值从预训练树模型中提炼组级特征重要性评分；第二阶段在逻辑回归中引入组L21正则化项，强制实现结构化稀疏性，从而输出紧凑且非冗余的特征选择结果。

Result: 大量对比实验表明，与LASSO、SHAP以及基于深度学习的方法相比，GRASP能够持续取得相当或更优的预测准确率，同时识别出特征数量更少、特征间冗余度更低、选择结果更稳定的特征子集。

Conclusion: GRASP框架通过融合SHAP的可解释性与组L21正则化的结构化稀疏能力，有效提升了医疗预测中特征选择的鲁棒性和可解释性，为临床决策支持系统提供了更可靠的特征筛选工具。

Abstract: Feature selection remains a major challenge in medical prediction, where existing approaches such as LASSO often lack robustness and interpretability. We introduce GRASP, a novel framework that couples Shapley value driven attribution with group $L_{21}$ regularization to extract compact and non-redundant feature sets. GRASP first distills group level importance scores from a pretrained tree model via SHAP, then enforces structured sparsity through group $L_{21}$ regularized logistic regression, yielding stable and interpretable selections. Extensive comparisons with LASSO, SHAP, and deep learning based methods show that GRASP consistently delivers comparable or superior predictive accuracy, while identifying fewer, less redundant, and more stable features.

</details>


### [110] [General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies](https://arxiv.org/abs/2602.11087)
*Jianxun Wang,Grant C. Forbes,Leonardo Villalobos-Arias,David L. Roberts*

Main category: cs.LG

TL;DR: 提出一种基于线性规划形式的离线强化学习f-散度柔性约束方法，通过凸共轭建立f-散度与贝尔曼残差优化约束的联系，实现基于数据特征的自适应约束平衡，在MuJoCo、Fetch和AdroitHand环境中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 实际离线数据集常存在探索不足、行为策略多样且水平不一的问题：有限探索损害Q/V值估计精度，而针对多策略的约束又过于保守，导致离线RL算法需在优化目标与行为策略约束间寻求动态平衡。

Method: 从更一般的强化学习线性规划形式与凸共轭理论出发，揭示f-散度与贝尔曼残差优化约束的内在关联；进而提出柔性f-散度函数框架，根据离线训练数据特征自适应调整算法学习目标上的约束强度。

Result: 在MuJoCo、Fetch和AdroitHand环境上的实验验证了所提线性规划形式的正确性，且柔性f-散度在配合兼容的约束优化算法时，能从挑战性数据集中提升学习性能。

Conclusion: 该方法通过数据驱动的自适应约束机制，有效缓解了离线RL中探索不足与策略约束过保守的矛盾，为处理复杂离线数据集提供了理论保证与实践潜力。

Abstract: Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the offline RL algorithm's ability to estimate \textit{Q} or \textit{V} values, while constraining towards diverse behavior policies can be overly conservative. Such datasets call for a balance between the RL objective and behavior policy constraints. We first identify the connection between $f$-divergence and optimization constraint on the Bellman residual through a more general Linear Programming form for RL and the convex conjugate. Following this, we introduce the general flexible function formulation for the $f$-divergence to incorporate an adaptive constraint on algorithms' learning objectives based on the offline training dataset. Results from experiments on the MuJoCo, Fetch, and AdroitHand environments show the correctness of the proposed LP form and the potential of the flexible $f$-divergence in improving performance for learning from a challenging dataset when applied to a compatible constrained optimization algorithm.

</details>


### [111] [Rising Multi-Armed Bandits with Known Horizons](https://arxiv.org/abs/2602.10727)
*Seockbean Song,Chenyu Gan,Youngsik Yoon,Siwei Wang,Wei Chen,Jungseul Ok*

Main category: cs.LG

TL;DR: 本文研究奖励随使用次数增长的上升多臂老虎机（RMAB）问题，提出一种显式集成时间视野T的CURE-UCB算法，在理论分析中建立了新的遗憾上界并证明其在"线性-后平坦"等结构化环境中严格优于视域无关策略，实验验证了该算法相对于基线方法的显著优越性。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机无法建模机器人、超参数调优等重复使用提升性能的场景。RMAB的核心特征是最优策略高度依赖可用预算T，但这一视域感知设置尚未得到充分探索。现有方法忽视T信息导致次优决策。

Method: 提出累积奖励估计上置信界算法CURE-UCB，创新性地将时间视野T直接融入算法设计。通过累积奖励估计而非瞬时奖励来评估臂的价值，并基于此构建置信区间进行决策。

Result: 理论证明CURE-UCB在"线性-后平坦"等结构化实例中严格优于视域无关策略，并建立了新的遗憾上界。大量实验表明该算法相比基线方法具有显著的性能优势。

Conclusion: 成功解决了视域感知型RMAB这一未充分研究的问题，证实CURE-UCB能有效利用预算信息。研究揭示了视域感知在RMAB中的关键作用，为实际应用提供了理论保证更优的算法框架。

Abstract: The Rising Multi-Armed Bandit (RMAB) framework models environments where expected rewards of arms increase with plays, which models practical scenarios where performance of each option improves with the repeated usage, such as in robotics and hyperparameter tuning. For instance, in hyperparameter tuning, the validation accuracy of a model configuration (arm) typically increases with each training epoch. A defining characteristic of RMAB is em horizon-dependent optimality: unlike standard settings, the optimal strategy here shifts dramatically depending on the available budget $T$. This implies that knowledge of $T$ yields significantly greater utility in RMAB, empowering the learner to align its decision-making with this shifting optimality. However, the horizon-aware setting remains underexplored. To address this, we propose a novel CUmulative Reward Estimation UCB (CURE-UCB) that explicitly integrates the horizon. We provide a rigorous analysis establishing a new regret upper bound and prove that our method strictly outperforms horizon-agnostic strategies in structured environments like ``linear-then-flat'' instances. Extensive experiments demonstrate its significant superiority over baselines.

</details>


### [112] [GENIUS: Generative Fluid Intelligence Evaluation Suite](https://arxiv.org/abs/2602.11144)
*Ruichuan An,Sihan Yang,Ziyu Guo,Wei Dai,Zijun Shen,Haodong Li,Renrui Zhang,Xinyu Wei,Guopeng Li,Wenshan Wu,Wentao Zhang*

Main category: cs.LG

TL;DR: 本文提出GENIUS基准，用于评估多模态统一模型的生成性流体智力(GFI)。研究发现现有模型在归纳隐式模式、执行临时约束和适应情境知识三项核心能力上存在显著缺陷，根源在于情境理解不足而非生成能力欠缺，并提出了无需训练的注意力干预策略。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估依赖知识回忆的"晶体智力"，却忽视了"生成性流体智力(GFI)"——即在即时情境中归纳模式、推理约束并适应新场景的能力。这种评估偏差导致模型发展偏向知识利用而缺乏动态推理能力。

Method: 提出GENIUS基准，将GFI形式化为三个基本操作原语：归纳隐式模式（如推断个性化视觉偏好）、执行临时约束（如可视化抽象隐喻）、适应情境知识（如模拟反直觉物理）。系统评估12个代表性模型，并提出无需训练范式的注意力干预策略来弥补缺陷。

Result: 评估显示12个模型在GFI任务上均存在显著性能缺陷。诊断分析揭示，这些失败模式源于模型对即时情境的理解能力有限，而非其内在生成能力不足，表明提升情境 comprehension 是关键突破口。

Conclusion: GENIUS为生成性流体智力建立了严格评估标准，推动研究从静态知识利用转向动态通用推理，为开发更具适应性和创造性的多模态模型提供了诊断工具和优化方向。

Abstract: Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite). We formalize $\textit{GFI}$ as a synthesis of three primitives. These include $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.

</details>


### [113] [Collaborative Threshold Watermarking](https://arxiv.org/abs/2602.10765)
*Tameem Bakr,Anish Ambreth,Nils Lukas*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In federated learning (FL), $K$ clients jointly train a model without sharing raw data. Because each participant invests data and compute, clients need mechanisms to later prove the provenance of a jointly trained model. Model watermarking embeds a hidden signal in the weights, but naive approaches either do not scale with many clients as per-client watermarks dilute as $K$ grows, or give any individual client the ability to verify and potentially remove the watermark. We introduce $(t,K)$-threshold watermarking: clients collaboratively embed a shared watermark during training, while only coalitions of at least $t$ clients can reconstruct the watermark key and verify a suspect model. We secret-share the watermark key $τ$ so that coalitions of fewer than $t$ clients cannot reconstruct it, and verification can be performed without revealing $τ$ in the clear. We instantiate our protocol in the white-box setting and evaluate on image classification. Our watermark remains detectable at scale ($K=128$) with minimal accuracy loss and stays above the detection threshold ($z\ge 4$) under attacks including adaptive fine-tuning using up to 20% of the training data.

</details>


### [114] [Semi-Supervised Cross-Domain Imitation Learning](https://arxiv.org/abs/2602.10793)
*Li-Min Chu,Kai-Siang Ma,Ming-Hong Chen,Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.

</details>


### [115] [RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization](https://arxiv.org/abs/2602.10819)
*Linxuan Xia,Xiaolong Yang,Yongyuan Chen,Enyue Zhao,Deng Cai,Yasheng Wang,Boxi Wu*

Main category: cs.LG

TL;DR: 针对大语言模型领域对齐问题，本文提出重述策略优化（RePO）方法。该方法通过让策略模型先理解离线策略知识，再将其重述为符合自身风格的轨迹，动态替换低奖励采样，从而在保持在线策略训练稳定性的同时提升困难样本利用率，在多个基准测试上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）虽能注入领域知识但会损害模型通用性；在线策略强化学习（RL）能保持通用性却难以处理超出当前推理水平的困难样本；离线策略RL虽能更好利用困难样本，但会因强制分布偏移导致严重训练不稳定。现有方法无法同时实现有效知识吸收与训练稳定性，亟需新方法解决这一矛盾。

Method: RePO的核心机制是：首先提示策略模型理解离线策略知识，然后将其重述为符合自身风格与参数分布的轨迹，最后动态将这些高质量重述轨迹替换低奖励采样。该方法在严格保持在线训练动态的同时，引导模型走向正确推理路径。

Result: 在多个基准测试上的实验表明，RePO显著提升了困难样本的利用率，性能优于现有基线方法，实现了最先进的（state-of-the-art）性能。

Conclusion: RePO成功地在有效吸收离线策略知识与保持在线策略RL稳定性之间取得平衡，为领域特定大语言模型对齐提供了一种既稳定又高效的新方法。

Abstract: Aligning large language models (LLMs) on domain-specific data remains a fundamental challenge. Supervised fine-tuning (SFT) offers a straightforward way to inject domain knowledge but often degrades the model's generality. In contrast, on-policy reinforcement learning (RL) preserves generality but fails to effectively assimilate hard samples that exceed the model's current reasoning level. Recent off-policy RL attempts improve hard sample utilization, yet they suffer from severe training instability due to the forced distribution shift toward off-policy knowledge. To reconcile effective off-policy knowledge absorption with the stability of on-policy RL, we propose Rephrasing Policy Optimization (RePO). In RePO, the policy model is prompted to first comprehend off-policy knowledge and then rephrase it into trajectories that conform to its own stylistic and parametric distribution. RePO dynamically replaces low-reward rollouts with these rephrased, high-quality trajectories. This strategy guides the model toward correct reasoning paths while strictly preserving on-policy training dynamics. Experiments on several benchmarks demonstrate that RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.

</details>


### [116] [Adaptive Sampling for Private Worst-Case Group Optimization](https://arxiv.org/abs/2602.10820)
*Max Cairney-Leeming,Amartya Sanyal,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 该论文提出ASC算法，通过自适应调整各群体的采样率和裁剪阈值，解决差分隐私机器学习中的群体公平性问题，在保持一致隐私保证的同时提升最差群体准确率。


<details>
  <summary>Details</summary>
Motivation: 标准模型在小群体或难学习群体上表现不佳。加权优化虽可关注最差群体，但引入差分隐私后会导致隐私保证不均，少数群体隐私保护较弱，因此需要新方法。

Method: 提出ASC（自适应采样与裁剪的最坏群体优化）算法，对每个群体自适应控制采样率和梯度裁剪阈值，使难学习群体获得更高采样率，同时确保跨群体一致的隐私保证。

Result: 相比先前方法，ASC实现了更低方差梯度、更紧致隐私保证，且在不牺牲整体平均准确率的前提下，最差群体准确率大幅提升。

Conclusion: ASC在差分隐私下有效实现了群体公平学习，为隐私保护机器学习中的公平性问题提供了新解决方案。

Abstract: Models trained by minimizing the average loss often fail to be accurate on small or hard-to-learn groups of the data. Various methods address this issue by optimizing a weighted objective that focuses on the worst-performing groups. However, this approach becomes problematic when learning with differential privacy, as unequal data weighting can result in inhomogeneous privacy guarantees, in particular weaker privacy for minority groups. In this work, we introduce a new algorithm for differentially private worst-case group optimization called ASC (Adaptively Sampled and Clipped Worst-case Group Optimization). It adaptively controls both the sampling rate and the clipping threshold of each group. Thereby, it allows for harder-to-learn groups to be sampled more often while ensuring consistent privacy guarantees across all groups. Comparing ASC to prior work, we show that it results in lower-variance gradients, tighter privacy guarantees, and substantially higher worst-case group accuracy without sacrificing overall average accuracy.

</details>


### [117] [SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios](https://arxiv.org/abs/2602.10840)
*Yanan Wang,Renxi Wang,Yongxin Wang,Xuezhi Liang,Fajri Koto,Timothy Baldwin,Xiaodan Liang,Haonan Li*

Main category: cs.LG

TL;DR: 本文提出SimuScene，首次系统研究LLM通过代码模拟物理场景。涵盖5大物理领域52个概念，构建7,659个场景数据集（含334个验证集）。评估10个模型，最优通过率仅21.5%。提出视觉奖励强化学习框架，用VLM作评判器，既提升物理模拟能力，又显著增强通用代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在数学、编码和科学推理领域表现突出，但其在物理场景的代码化模拟能力尚未被系统探索。本研究旨在填补此空白，构建首个全面评估和提升LLM物理模拟能力的基准框架，推动模型在科学计算与物理推理方面的发展。

Method: 研究采用四步法：1) 提出SimuScene框架，系统覆盖5个物理领域及52个核心概念；2) 构建自动化数据收集与人工验证流程，形成含7,659个物理场景的数据集，其中334个高质量样本作为测试集；3) 系统评估10个当代主流LLM；4) 创新设计基于视觉奖励的强化学习管道，以视觉-语言模型为评判器训练文本模型。

Result: 实验表明：1) 现有LLM物理模拟能力薄弱，最强模型通过率仅21.5%，证实任务高难度；2) 所建数据集有效支持模型训练；3) 视觉奖励强化学习显著提升物理模拟性能；4) 该方法还具有正向迁移效应，大幅增强通用代码生成能力。

Conclusion: 研究揭示当前LLM在物理场景代码模拟方面存在显著能力缺口。SimuScene框架及视觉奖励强化学习方法不仅为物理模拟提供有效解决方案，还意外发现其能泛化至通用代码生成领域，展现跨领域改进潜力，为科学智能体开发提供重要参考。

Abstract: Large language models (LLMs) have been extensively studied for tasks like math competitions, complex coding, and scientific reasoning, yet their ability to accurately represent and simulate physical scenarios via code remains underexplored. We propose SimuScene, the first systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 physical concepts. We build an automatic pipeline to collect data, with human verification to ensure quality. The final dataset contains 7,659 physical scenarios with 334 human-verified examples as the test set. We evaluated 10 contemporary LLMs and found that even the strongest model achieves only a 21.5% pass rate, demonstrating the difficulty of the task. Finally, we introduce a reinforcement learning pipeline with visual rewards that uses a vision-language model as a judge to train textual models. Experiments show that training with our data improves physical simulation via code while substantially enhancing general code generation performance.

</details>


### [118] [The Sample Complexity of Uniform Approximation for Multi-Dimensional CDFs and Fixed-Price Mechanisms](https://arxiv.org/abs/2602.10868)
*Matteo Castiglioni,Anna Lunghi,Alberto Marchesi*

Main category: cs.LG

TL;DR: 本文研究了单比特反馈下n维累积分布函数（CDF）的均匀ε-逼近样本复杂度，发现近乎维度不变性：仅需O(ε^{-3}log(1/ε)^{O(n)})个样本，维度n仅影响对数项，将多元DKW不等式推广至老虎机反馈，并给出双边贸易机制学习的紧界。


<details>
  <summary>Details</summary>
Motivation: 探究单比特老虎机反馈下高维分布学习的样本复杂度，将经典多元DKW不等式从全反馈推广至信息受限场景，为双边贸易等市场机制学习提供理论基础。

Method: 通过在细粒度网格上构建经验CDF估计量，利用自洽性约束处理单比特反馈，并应用精细集中不等式控制上确界偏差，实现高维CDF的均匀逼近。

Result: 主要结果：在任意细粒度网格上，均匀ε-逼近的样本复杂度为O(ε^{-3}log(1/ε)^{O(n)})，维度n仅以对数项出现，界是紧的。直接推论：给出双边贸易等小市场固定价格机制学习的紧样本复杂度界和新遗憾保证。

Conclusion: 成功将多元DKW不等式推广至老虎机反馈，揭示高维分布学习在信息受限下的样本复杂度本质：仅对数级依赖维度n，为高维老虎机学习和机制设计提供重要理论保证。

Abstract: We study the sample complexity of learning a uniform approximation of an $n$-dimensional cumulative distribution function (CDF) within an error $ε> 0$, when observations are restricted to a minimal one-bit feedback. This serves as a counterpart to the multivariate DKW inequality under ''full feedback'', extending it to the setting of ''bandit feedback''. Our main result shows a near-dimensional-invariance in the sample complexity: we get a uniform $ε$-approximation with a sample complexity $\frac{1}{ε^3}{\log\left(\frac 1 ε\right)^{\mathcal{O}(n)}}$ over a arbitrary fine grid, where the dimensionality $n$ only affects logarithmic terms. As direct corollaries, we provide tight sample complexity bounds and novel regret guarantees for learning fixed-price mechanisms in small markets, such as bilateral trade settings.

</details>


### [119] [Natural Hypergradient Descent: Algorithm Design, Convergence Analysis, and Parallel Implementation](https://arxiv.org/abs/2602.10905)
*Deyi Kong,Zaiwei Chen,Shuzhong Zhang,Shancong Mou*

Main category: cs.LG

TL;DR: 本文提出自然超梯度下降（NHGD）方法，用于解决双层优化问题，通过用经验Fisher信息矩阵近似Hessian矩阵逆，显著降低了计算开销，同时保持了理论保证。


<details>
  <summary>Details</summary>
Motivation: 针对双层优化中计算瓶颈问题——即超梯度估计需要计算或近似Hessian矩阵逆的高计算成本——利用内层优化问题的统计结构，提出更高效的超梯度计算方法。

Method: 提出自然超梯度下降（NHGD）方法，采用经验Fisher信息矩阵作为Hessian矩阵的渐近一致替代。设计并行的"优化-近似"框架，使Hessian逆的近似与随机内层优化同步更新，以可忽略的额外成本重用梯度信息。

Result: 理论分析建立了NHGD的高概率误差界和样本复杂度保证，与现有最优"先优化后近似"方法相当。实证评估显示，NHGD在代表性双层学习任务中具有实际优势，在大规模机器学习环境中具有可扩展性和有效性。

Conclusion: NHGD方法通过创新的统计近似策略，在保持理论性能的同时显著降低了双层优化问题的计算时间开销，为大规模机器学习应用提供了更高效的解决方案。

Abstract: In this work, we propose Natural Hypergradient Descent (NHGD), a new method for solving bilevel optimization problems. To address the computational bottleneck in hypergradient estimation--namely, the need to compute or approximate Hessian inverse--we exploit the statistical structure of the inner optimization problem and use the empirical Fisher information matrix as an asymptotically consistent surrogate for the Hessian. This design enables a parallel optimize-and-approximate framework in which the Hessian-inverse approximation is updated synchronously with the stochastic inner optimization, reusing gradient information at negligible additional cost. Our main theoretical contribution establishes high-probability error bounds and sample complexity guarantees for NHGD that match those of state-of-the-art optimize-then-approximate methods, while significantly reducing computational time overhead. Empirical evaluations on representative bilevel learning tasks further demonstrate the practical advantages of NHGD, highlighting its scalability and effectiveness in large-scale machine learning settings.

</details>


### [120] [Near-Constant Strong Violation and Last-Iterate Convergence for Online CMDPs via Decaying Safety Margins](https://arxiv.org/abs/2602.10917)
*Qian Zuo,Zhiyong Wang,Fengxiang He*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study safe online reinforcement learning in Constrained Markov Decision Processes (CMDPs) under strong regret and violation metrics, which forbid error cancellation over time. Existing primal-dual methods that achieve sublinear strong reward regret inevitably incur growing strong constraint violation or are restricted to average-iterate convergence due to inherent oscillations. To address these limitations, we propose the Flexible safety Domain Optimization via Margin-regularized Exploration (FlexDOME) algorithm, the first to provably achieve near-constant $\tilde{O}(1)$ strong constraint violation alongside sublinear strong regret and non-asymptotic last-iterate convergence. FlexDOME incorporates time-varying safety margins and regularization terms into the primal-dual framework. Our theoretical analysis relies on a novel term-wise asymptotic dominance strategy, where the safety margin is rigorously scheduled to asymptotically majorize the functional decay rates of the optimization and statistical errors, thereby clamping cumulative violations to a near-constant level. Furthermore, we establish non-asymptotic last-iterate convergence guarantees via a policy-dual Lyapunov argument. Experiments corroborate our theoretical findings.

</details>


### [121] [CMAD: Cooperative Multi-Agent Diffusion via Stochastic Optimal Control](https://arxiv.org/abs/2602.10933)
*Riccardo Barbano,Alexander Denker,Zeljko Kereta,Runchang Li,Francisco Vargas*

Main category: cs.LG

TL;DR: 本文针对多预训练扩散模型组合控制的开放性挑战，指出现有概率密度组合方法因需显式目标分布而受限。创新性地提出合作随机最优控制新范式，通过协同引导多模型扩散轨迹实现共享目标，并在条件MNIST生成任务中验证其相对于DPS基线的有效性。


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型虽在图像恢复与合成领域取得显著成功，但如何有效控制多个预训练模型的协同生成仍是一个开放挑战。现有方法多采用概率密度的代数组合（如乘积或混合专家），其前提是目标分布显式已知，而这在现实中几乎从不成立。

Method: 为此，本文提出一种全新范式，将组合生成问题重构为合作随机最优控制问题。该方法不再简单组合概率密度，而是将预训练扩散模型视为交互智能体，通过最优控制机制协同引导其扩散轨迹，使其聚合输出共同达成预设的共享目标。

Result: 在条件MNIST生成任务上的验证表明，该框架相较于naive的推理时DPS基线（采用逐步梯度引导替代学习性合作控制）具有优越性。

Conclusion: 该研究揭示了合作随机最优控制在组合生成任务中的潜力，为多模型协同控制提供了新思路，有效规避了传统密度组合方法对显式目标分布的依赖。

Abstract: Continuous-time generative models have achieved remarkable success in image restoration and synthesis. However, controlling the composition of multiple pre-trained models remains an open challenge. Current approaches largely treat composition as an algebraic composition of probability densities, such as via products or mixtures of experts. This perspective assumes the target distribution is known explicitly, which is almost never the case. In this work, we propose a different paradigm that formulates compositional generation as a cooperative Stochastic Optimal Control problem. Rather than combining probability densities, we treat pre-trained diffusion models as interacting agents whose diffusion trajectories are jointly steered, via optimal control, toward a shared objective defined on their aggregated output. We validate our framework on conditional MNIST generation and compare it against a naive inference-time DPS-style baseline replacing learned cooperative control with per-step gradient guidance.

</details>


### [122] [Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink](https://arxiv.org/abs/2602.10956)
*Victoria Hankemeier,Malte Hankemeier*

Main category: cs.LG

TL;DR: 该论文针对时空模型中时序注意力机制的信息退化问题，通过理论推导发现时序注意力存在对角线注意力汇聚现象，并提出正则化解决方案，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 时空模型因同时处理空间结构和时序动态而容易导致信息退化。现有研究表明，因果注意力和时序卷积中的过压缩现象会对序列首令牌产生系统性偏差。本文旨在探究时序注意力机制是否存在类似偏差及其成因。

Method: 研究推导了时序注意力层雅可比矩阵期望值的敏感性边界，从理论上分析了非对角注意力得分与序列长度的依赖关系。

Result: 理论证明表明，时序注意力矩阵存在对角线注意力汇聚（diagonal attention sink）现象，且非对角注意力得分会随序列长度变化而变化，导致信息退化。

Conclusion: 基于理论分析结果，论文提出了针对性的正则化方法，并通过实验验证了这些方法在缓解时序注意力信息退化方面的有效性。

Abstract: Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time. Prior literature has demonstrated that over-squashing in causal attention or temporal convolutions creates a bias on the first tokens. To analyze whether such a bias is present in temporal attention mechanisms, we derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer. We theoretically show how off-diagonal attention scores depend on the sequence length, and that temporal attention matrices suffer a diagonal attention sink. We suggest regularization methods, and experimentally demonstrate their effectiveness.

</details>


### [123] [MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.10965)
*Yupu Gu,Rongzhe Wei,Andy Zhu,Pan Li*

Main category: cs.LG

TL;DR: 本文提出MoEEdit，首个针对稀疏MoE模型的路由稳定知识编辑框架。通过零空间投影保持路由不变性，结合块坐标下降实现高效优化，在效果、稳定性和效率方面均达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法多针对稠密模型设计，难以适配稀疏MoE架构。直接迁移计算成本高且易导致路由分布偏移，影响模型稳定性和一致性。现代可扩展LLM亟需专用编辑方案。

Method: MoEEdit通过每专家零空间投影重参数化专家更新，保持路由器输入不变以抑制路由偏移。采用块坐标下降（BCD）求解器高效求解块结构优化问题。

Result: 实验显示MoEEdit实现SOTA编辑效果与泛化性，同时保持高特异性和路由稳定性，计算与内存效率优异。

Conclusion: 该方法为稀疏LLM知识编辑提供了可扩展的鲁棒基础，凸显路由稳定干预的重要性，推动了大规模模型编辑技术的发展。

Abstract: Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable LLMs. Although MoEs offer strong efficiency and capacity scaling, naively adapting dense-model editors is both computationally costly and prone to routing distribution shifts that undermine stability and consistency. To address these challenges, we introduce MoEEdit, the first routing-stable framework for parameter-modifying knowledge editing in MoE LLMs. Our method reparameterizes expert updates via per-expert null-space projections that keep router inputs invariant and thereby suppress routing shifts. The resulting block-structured optimization is solved efficiently with a block coordinate descent (BCD) solver. Experiments show that MoEEdit attains state-of-the-art efficacy and generalization while preserving high specificity and routing stability, with superior compute and memory efficiency. These results establish a robust foundation for scalable, precise knowledge editing in sparse LLMs and underscore the importance of routing-stable interventions.

</details>


### [124] [A Jointly Efficient and Optimal Algorithm for Heteroskedastic Generalized Linear Bandits with Adversarial Corruptions](https://arxiv.org/abs/2602.10971)
*Sanghwa Kim,Junghyun Lee,Se-Young Yun*

Main category: cs.LG

TL;DR: 本文研究了带有对抗性污染的异方差广义线性老虎机问题，提出HCW-GLB-OMD算法，该算法结合在线镜像下降估计器与基于Hessian的置信权重，在自协调性假设下实现了计算高效且接近极小极大最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 异方差广义线性老虎机涵盖了异方差线性老虎机、逻辑/泊松老虎机等重要设置，对抗性污染在实际应用中普遍存在。现有方法计算复杂度高或缺乏理论保证，亟需一种既能抵抗污染又计算高效的统一算法框架。

Method: 提出HCW-GLB-OMD算法：1）采用在线镜像下降(OMD)构建鲁棒估计器；2）设计基于Hessian的置信权重机制。算法每轮仅需O(1)空间和时间复杂度，计算效率极高。

Result: 在自协调性假设下，获得遗憾上界：𝕆̃(d√(∑_t g(τ_t)μ̇_{t,⋆}) + d²g_maxκ + dκC)。其中d为维度，g(τ_t)为时变离散度函数，C为对抗污染预算，κ为条件数。同时证明下界Ω̃(d√(∑_t g(τ_t)μ̇_{t,⋆}) + dC)，表明算法在污染项上至多为κ因子的次优性。

Conclusion: 该算法首次实现了多种异方差GLB实例的实例相关极小极大最优性，统一了此前问题特定的下界结果，为对抗污染下的老虎机学习提供了计算高效且理论保证的完整解决方案。

Abstract: We consider the problem of heteroskedastic generalized linear bandits (GLBs) with adversarial corruptions, which subsumes various stochastic contextual bandit settings, including heteroskedastic linear bandits and logistic/Poisson bandits. We propose HCW-GLB-OMD, which consists of two components: an online mirror descent (OMD)-based estimator and Hessian-based confidence weights to achieve corruption robustness. This is computationally efficient in that it only requires ${O}(1)$ space and time complexity per iteration. Under the self-concordance assumption on the link function, we show a regret bound of $\tilde{O}\left( d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d^2 g_{\max} κ+ d κC \right)$, where $\dotμ_{t,\star}$ is the slope of $μ$ around the optimal arm at time $t$, $g(τ_t)$'s are potentially exogenously time-varying dispersions (e.g., $g(τ_t) = σ_t^2$ for heteroskedastic linear bandits, $g(τ_t) = 1$ for Bernoulli and Poisson), $g_{\max} = \max_{t \in [T]} g(τ_t)$ is the maximum dispersion, and $C \geq 0$ is the total corruption budget of the adversary. We complement this with a lower bound of $\tildeΩ(d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d C)$, unifying previous problem-specific lower bounds. Thus, our algorithm achieves, up to a $κ$-factor in the corruption term, instance-wise minimax optimality simultaneously across various instances of heteroskedastic GLBs with adversarial corruptions.

</details>


### [125] [TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents](https://arxiv.org/abs/2602.10986)
*Abhishek Vijaya Kumar,Bhaskar Kataria,Byungsoo Oh,Emaad Manzoor,Rachee Singh*

Main category: cs.LG

TL;DR: 本文提出TVCACHE，一种用于LLM智能体强化学习后训练的状态化工具值缓存系统。通过维护工具调用序列树并执行最长前缀匹配，TVCACHE在保证环境状态一致性的前提下，实现了高达70%的缓存命中率，将工具调用执行时间中位数降低6.9倍，且不损害训练奖励。


<details>
  <summary>Details</summary>
Motivation: 在LLM智能体的强化学习后训练过程中，外部工具调用耗时数秒至数分钟，导致GPU空闲，显著增加训练时间和成本。尽管并行推演中许多工具调用重复出现，理论上可缓存，但由于工具输出依赖于先前智能体交互产生的环境状态，简单缓存复用并不正确。

Method: TVCACHE维护一个观测到的工具调用序列树，并采用最长前缀匹配进行缓存查找。只有当智能体的完整工具历史与先前执行的序列完全匹配时，才发生缓存命中，从而保证环境状态完全相同。

Result: 在终端任务、SQL生成和视频理解三个多样化工作负载上，TVCACHE实现了高达70%的缓存命中率，将工具调用执行时间中位数降低最多6.9倍，且未对后训练奖励积累造成任何下降。

Conclusion: TVCACHE通过状态化的工具调用序列匹配机制，有效解决了LLM智能体训练中工具调用开销大的问题，显著提升了训练效率并降低成本，同时保持训练质量不变，为大规模智能体训练提供了实用解决方案。

Abstract: In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.

</details>


### [126] [Motion Capture is Not the Target Domain: Scaling Synthetic Data for Learning Motion Representations](https://arxiv.org/abs/2602.11064)
*Firas Darwish,George Nicholson,Aiden Doherty,Hang Yuan*

Main category: cs.LG

TL;DR: 研究合成人体运动数据在可穿戴行为识别中的迁移学习问题，发现合成数据与真实数据混合预训练或大规模扩展时可提升泛化性，而运动捕捉数据因域差异增益有限。


<details>
  <summary>Details</summary>
Motivation: 针对现实世界人体运动数据稀缺但可穿戴式行为识别(HAR)依赖大规模数据的问题，探索合成数据作为可扩展预训练路径的可行性，并解决合成预训练模型向部署环境迁移不可靠的核心挑战。

Method: 基于运动捕捉衍生表示生成合成运动数据，预训练运动时间序列模型，并在多样化下游HAR任务上系统评估跨域迁移性能，对比分析仿真与现实数据的协同效应。

Result: 合成预训练在与真实数据混合或数据规模充分扩展时能显著提升模型泛化能力；大规模运动捕捉预训练仅带来边际增益，主要受限于可穿戴信号与仿真数据间的域不匹配问题。

Conclusion: 研究明确了仿真到现实迁移的关键挑战，揭示了合成运动数据的有效边界与规模化潜力，为构建可迁移的HAR表征提供了重要洞见。

Abstract: Synthetic data offers a compelling path to scalable pretraining when real-world data is scarce, but models pretrained on synthetic data often fail to transfer reliably to deployment settings. We study this problem in full-body human motion, where large-scale data collection is infeasible but essential for wearable-based Human Activity Recognition (HAR), and where synthetic motion can be generated from motion-capture-derived representations. We pretrain motion time-series models using such synthetic data and evaluate their transfer across diverse downstream HAR tasks. Our results show that synthetic pretraining improves generalisation when mixed with real data or scaled sufficiently. We also demonstrate that large-scale motion-capture pretraining yields only marginal gains due to domain mismatch with wearable signals, clarifying key sim-to-real challenges and the limits and opportunities of synthetic motion data for transferable HAR representations.

</details>


### [127] [Token-Efficient Change Detection in LLM APIs](https://arxiv.org/abs/2602.11083)
*Timothée Chauvin,Clément Lalanne,Erwan Le Merrer,Jean-Michel Loubes,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 该论文提出了B3IT方案，通过识别"边界输入"(模型输出存在多个高频token的输入)来实现大型语言模型的远程变更检测。该方法仅需黑盒访问输出token，无需模型权重或概率信息，在性能与灰盒方法相当的同时，成本降低30倍。


<details>
  <summary>Details</summary>
Motivation: 现有LLM远程变更检测方法存在两大局限：要么成本过高难以大规模部署，要么需要白盒访问模型权重或灰盒访问log概率。在实际应用场景中，严格黑盒访问（仅观察输出token）是更现实且低成本的需求，但现有技术无法同时满足低成本和强黑盒约束。

Method: 该方法从统计视角切入，发现边界输入的变更检测能力取决于模型的Jacobian矩阵和输出分布的Fisher信息量。通过分析低温区的这些统计量，识别出边界输入具有强大的检测潜力。基于此洞察，构建了B3IT跟踪方案，通过检测边界输入处的输出分布变化来推断模型内部变更。

Result: 实验表明：1) 在非推理类模型端点上，边界输入易于发现；2) 检测性能与最佳灰盒方法相当；3) 相比现有方法成本降低30倍；4) 成功实现了严格黑盒环境下的有效变更检测。

Conclusion: B3IT方案通过边界输入这一关键洞察，有效解决了LLM变更检测中成本与可访问性之间的权衡问题，为大规模部署提供了实用且高效的解决方案。该方法在保持黑盒约束的同时，性能未显著损失，具有重要实践价值。

Abstract: Remote change detection in LLMs is a difficult problem. Existing methods are either too expensive for deployment at scale, or require initial white-box access to model weights or grey-box access to log probabilities. We aim to achieve both low cost and strict black-box operation, observing only output tokens.
  Our approach hinges on specific inputs we call Border Inputs, for which there exists more than one output top token. From a statistical perspective, optimal change detection depends on the model's Jacobian and the Fisher information of the output distribution. Analyzing these quantities in low-temperature regimes shows that border inputs enable powerful change detection tests.
  Building on this insight, we propose the Black-Box Border Input Tracking (B3IT) scheme. Extensive in-vivo and in-vitro experiments show that border inputs are easily found for non-reasoning tested endpoints, and achieve performance on par with the best available grey-box approaches. B3IT reduces costs by $30\times$ compared to existing methods, while operating in a strict black-box setting.

</details>


### [128] [MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning](https://arxiv.org/abs/2602.11092)
*Cassandre Notton,Benjamin Stott,Philippe Schoeb,Anthony Walsh,Grégoire Leboucher,Vincent Espitalier,Vassilis Apostolou,Louis-Félix Vigneux,Alexia Salavrakos,Jean Senellart*

Main category: cs.LG

TL;DR: 该论文提出MerLin，一个开源的量子机器学习发现引擎框架。该框架将线性光学电路模拟集成到PyTorch和scikit-learn中，支持可微分的量子层训练，并系统性地复现了18种先进的光子量子机器学习方法，为算法、基准和硬件的协同设计提供了标准化平台。


<details>
  <summary>Details</summary>
Motivation: 当前量子机器学习研究缺乏系统性和实证性的探索，局限于孤立的算法提案。需要在不同模型、数据集和硬件约束下进行更全面的研究，特别是光子量子计算在近期量子机器学习中的实际应用价值尚未得到充分验证。

Method: 开发MerLin框架，通过将优化的线性光学电路强模拟器集成到标准机器学习工作流中，实现量子层的端到端可微分训练。围绕系统基准测试和可重复性设计，复现了18种涵盖核方法、储层计算、卷积/循环架构、生成模型等先进光子量子机器学习方法，并将其模块化发布。

Result: 成功构建了一个可重用的实验基准平台，支持消融研究、跨模态比较和经典-量子混合工作流。框架具备硬件感知功能，既可在现有量子硬件上测试，又能探索超越当前硬件能力的方案，成为连接算法、基准和硬件的协同设计工具。

Conclusion: MerLin通过嵌入现有机器学习生态系统，使从业者能够利用成熟工具链，为量子机器学习的系统化实证研究奠定了基础。该框架推动了从孤立算法研究向标准化、可重复的基准测试范式转变，为未来量子硬件和算法的协同设计提供了可持续的平台。

Abstract: Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates optimized strong simulation of linear optical circuits into standard PyTorch and scikit learn workflows, enabling end to end differentiable training of quantum layers. MerLin is designed around systematic benchmarking and reproducibility. As an initial contribution, we reproduce eighteen state of the art photonic and hybrid QML works spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments that can be directly extended and adapted, establishing a shared experimental baseline consistent with empirical benchmarking methodologies widely adopted in modern artificial intelligence. By embedding photonic quantum models within established machine learning ecosystems, MerLin allows practitioners to leverage existing tooling for ablation studies, cross modality comparisons, and hybrid classical quantum workflows. The framework already implements hardware aware features, allowing tests on available quantum hardware while enabling exploration beyond its current capabilities, positioning MerLin as a future proof co design tool linking algorithms, benchmarks, and hardware.

</details>


### [129] [The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization](https://arxiv.org/abs/2602.11126)
*Stephanie Holly,Alexandru-Ciprian Zăvoianu,Siegfried Silber,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: 针对离线多目标优化问题，研究发现生成式模型在超体积指标上表现良好，但在世代距离等指标上系统性劣于进化方法，根本原因是离线数据集与真实帕累托前沿存在偏移，导致生成模型过于保守。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式模型（如扩散模型）在离线多目标优化中展现出较强的超体积性能，但其在其他经典多目标优化指标下的表现尚未得到充分理解。为了揭示生成式优化方法的失效模式，需要深入分析其性能局限的根本原因。

Method: 研究者提出了"离线前沿偏移"概念，即离线数据集与真实帕累托前沿之间的位移，作为离线多目标优化的根本限制。通过积分概率度量来分析生成模型在目标空间中的分布外采样能力，并比较生成式方法与进化方法在不同指标下的表现差异。

Result: 实验结果表明，生成式方法在世代距离等指标上系统性劣于进化方法。生成模型倾向于在离线目标分布附近保守采样，难以克服离线前沿偏移问题。这揭示了生成式优化方法的失效模式与分布偏移密切相关。

Conclusion: 该研究将离线多目标优化定位为一个分布偏移受限问题，为理解生成式优化方法何时及为何失效提供了诊断性视角。研究结果强调了分布外采样能力在离线多目标优化中的重要性，为未来算法设计指明了方向。

Abstract: Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other metrics, such as generational distance. We relate this failure mode to the offline-frontier shift, i.e., the displacement of the offline dataset from the Pareto front, which acts as a fundamental limitation in offline MOO. We argue that overcoming this limitation requires out-of-distribution sampling in objective space (via an integral probability metric) and empirically observe that generative methods remain conservatively close to the offline objective distribution. Our results position offline MOO as a distribution-shift--limited problem and provide a diagnostic lens for understanding when and why generative optimization methods fail.

</details>


### [130] [Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.11128)
*Reinhard Heckel,Mahdi Soltanolkotabi,Christos Thramboulidis*

Main category: cs.LG

TL;DR: 本文提出非对称提示加权方法，在强化学习中赋予低/零成功率提示更高权重。研究发现该方法对R1-Zero等从零训练范式特别有效，能加速有效时间收敛，理论分析支持其在低成功率区域的优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM强化学习算法（GRPO、DAPO、RLOO）聚焦于模糊提示却降级简单/困难提示，忽略了从低成功率区域学习的重要性。本研究旨在探索提升极低成功率提示权重的潜力，特别是在从零训练过程中模型会遍历广泛准确率范围的情况下。

Method: 提出非对称提示加权策略，赋予低或零经验成功率提示更高权重；通过对比从零强化学习（R1-Zero）与后SFT强化学习的实验效果，并结合固定更新预算下的理论分析，刻画使达到目标准确率所需时间最小化的最优权重。

Result: 非对称加权显著提升从零强化学习效果（训练覆盖广泛准确率区间），但对已处于高准确率的后SFT强化学习改善有限；理论证明在低成功率区域最优权重呈非对称性，能有效加速收敛。

Conclusion: 非对称提示加权是提升LLM推理强化学习效率的有效方法，尤其适用于从零训练场景。在信息响应稀疏且响应成本主导的低成功率区域，该策略通过上采样低成功率提示实现更快收敛，具有理论和实践双重价值。

Abstract: Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.

</details>


### [131] [TabICLv2: A better, faster, scalable, and open tabular foundation model](https://arxiv.org/abs/2602.11139)
*Jingang Qu,David Holzmüller,Gaël Varoquaux,Marine Le Morvan*

Main category: cs.LG

TL;DR: 本文介绍了TabICLv2，一种基于三大支柱构建的表格数据基础模型：新型合成数据生成引擎、可扩展注意力机制架构创新、以及采用Muon优化器的预训练协议。该模型在未调参情况下即在TabArena和TALENT基准测试上超越当前最优的RealTabPFN-2.5，且能以低于50GB显存处理百万级数据集，同时显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 当前表格基础模型（如TabPFNv2和TabICL）虽已通过上下文学习在预测基准上超越梯度提升树，但仍存在局限性。本文旨在通过提升预训练多样性、改进模型架构以适应更大数据集，并优化训练协议，从而开发出性能更强、效率更高的新一代SOTA模型。

Method: 1) 设计高多样性合成数据生成引擎用于预训练；2) 引入架构创新，包括新型可扩展softmax注意力机制，以提升在大型数据集上的泛化能力；3) 优化预训练协议，将AdamW优化器替换为Muon优化器。

Result: 在未进行任何调参的情况下，TabICLv2在TabArena和TALENT基准测试上均超越当前最优模型RealTabPFN-2.5（该模型经过超参调优、集成和微调）。仅需中等预训练计算资源，TabICLv2即可在50GB GPU内存内有效泛化至百万级样本数据集，且推理速度显著快于RealTabPFN-2.5。

Conclusion: TabICLv2通过三大创新支柱成功建立了新的SOTA性能，同时具备更高的计算效率和更低的显存需求。作者承诺开源研究，已率先发布推理代码和模型权重，后续将公开合成数据引擎和预训练代码，以促进该领域的进一步发展。

Abstract: Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [132] [JAG: Joint Attribute Graphs for Filtered Nearest Neighbor Search](https://arxiv.org/abs/2602.10258)
*Haike Xu,Guy Blelloch,Laxman Dhulipala,Lars Gottesbüren,Rajesh Jayaram,Jakub Łącki*

Main category: cs.IR

TL;DR: 针对现有过滤最近邻搜索算法对查询选择性和过滤器类型敏感、泛化能力差的问题，本文提出JAG（Joint Attribute Graphs），一种基于图的算法。通过引入属性和过滤器距离，将二元过滤约束转化为连续导航指导，构建同时优化向量相似性和属性接近性的邻近图，从而在整个选择性范围内实现对多样过滤器类型的鲁棒性能，显著优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管过滤最近邻搜索是现代向量搜索系统的基本任务，但现有算法的性能高度敏感于查询选择性和过滤器类型。现有方案仅在特定过滤器类别（如标签相等）或狭窄的选择性范围内表现优异（如低选择性预过滤），因此难以适应需要泛化到新型过滤器类型和未知查询选择性的实际部署场景。

Method: 提出JAG（Joint Attribute Graphs）算法，核心创新是引入属性和过滤器距离，将二元过滤约束转化为连续导航指导。通过构建一个同时优化向量相似性和属性接近性的联合邻近图，避免导航死胡同，实现对整个选择性频谱和多样过滤器类型的鲁棒支持。

Result: 在五个数据集和四种过滤器类型（标签、范围、子集、布尔）上的实验结果表明，JAG在吞吐量和召回鲁棒性方面显著优于现有最先进的基线方法，并能持续优于先前的基于图的过滤最近邻搜索方法。

Conclusion: JAG通过创新的属性和过滤器距离表示以及联合图优化，成功解决了过滤最近邻搜索中的泛化性和鲁棒性问题，为实际部署提供了更可靠的解决方案。

Abstract: Despite filtered nearest neighbor search being a fundamental task in modern vector search systems, the performance of existing algorithms is highly sensitive to query selectivity and filter type. In particular, existing solutions excel either at specific filter categories (e.g., label equality) or within narrow selectivity bands (e.g., pre-filtering for low selectivity) and are therefore a poor fit for practical deployments that demand generalization to new filter types and unknown query selectivities. In this paper, we propose JAG (Joint Attribute Graphs), a graph-based algorithm designed to deliver robust performance across the entire selectivity spectrum and support diverse filter types. Our key innovation is the introduction of attribute and filter distances, which transform binary filter constraints into continuous navigational guidance. By constructing a proximity graph that jointly optimizes for both vector similarity and attribute proximity, JAG prevents navigational dead-ends and allows JAG to consistently outperform prior graph-based filtered nearest neighbor search methods. Our experimental results across five datasets and four filter types (Label, Range, Subset, Boolean) demonstrate that JAG significantly outperforms existing state-of-the-art baselines in both throughput and recall robustness.

</details>


### [133] [Single-Turn LLM Reformulation Powered Multi-Stage Hybrid Re-Ranking for Tip-of-the-Tongue Known-Item Retrieval](https://arxiv.org/abs/2602.10321)
*Debayan Mukhopadhyay,Utshab Kumar Ghosh,Shubham Chatterjee*

Main category: cs.IR

TL;DR: 针对舌尖现象(ToT)检索中查询模糊导致初始召回差的问题，本文提出使用通用8B参数大语言模型进行单次查询重构，无需微调。该方法使召回率提升20.61%，后续多阶段重排序进一步使nDCG@10、MRR和MAP@10分别提升33.88%、29.92%和29.98%，是一种解锁下游排序器潜力的低成本高效干预方案。


<details>
  <summary>Details</summary>
Motivation: 舌尖现象(ToT)检索面临用户查询模糊、不完整的核心挑战，导致初始召回率极低，标准伪相关反馈方法完全失效。现有检索系统难以直接从这些模糊描述中提取有效信号，亟需一种轻量级预处理方法来桥接用户模糊意图与具体信息需求之间的鸿沟，为下游高精度排序器创造可处理的输入条件。

Method: 采用通用8B参数大语言模型进行单次查询重构，模型不针对ToT任务或特定领域微调，依赖提示策略实现性能提升。重构查询输入多阶段检索管道：1) BM25稀疏检索；2) 稠密/延迟交互重排序（Contriever、E5-large-v2、ColBERTv2）；3) monoT5交叉编码；4) Qwen 2.5 72B列表级重排序。整个流程通过轻量级预变换激活下游排序器潜力。

Result: 在2025 TREC-ToT数据集上的实验证实，原始ToT查询性能极差。所提轻量级预检索变换使召回率绝对提升20.61%。后续重排序阶段带来更显著的性能增益：nDCG@10提升33.88%，MRR提升29.92%，MAP@10提升29.98%。结果表明查询重构有效解决了初始信号稀疏问题，为下游模型提供了高质量候选集。

Conclusion: 该研究证明，仅通过简单的大模型提示策略进行查询重构，无需模型微调或领域适应，即可显著改善ToT检索性能。这种方法以低成本解锁了现有重排序模型的潜力，验证了"好查询是好检索的基础"这一核心思想，为模糊查询场景提供了实用且高效的解决方案。

Abstract: Retrieving known items from vague descriptions, Tip-of-the-Tongue (ToT) retrieval, remains a significant challenge. We propose using a single call to a generic 8B-parameter LLM for query reformulation, bridging the gap between ill-formed ToT queries and specific information needs. This method is particularly effective where standard Pseudo-Relevance Feedback fails due to poor initial recall. Crucially, our LLM is not fine-tuned for ToT or specific domains, demonstrating that gains stem from our prompting strategy rather than model specialization. Rewritten queries feed a multi-stage pipeline: sparse retrieval (BM25), dense/late-interaction reranking (Contriever, E5-large-v2, ColBERTv2), monoT5 cross-encoding, and list-wise reranking (Qwen 2.5 72B). Experiments on 2025 TREC-ToT datasets show that while raw queries yield poor performance, our lightweight pre-retrieval transformation improves Recall by 20.61%. Subsequent reranking improves nDCG@10 by 33.88%, MRR by 29.92%, and MAP@10 by 29.98%, offering a cost-effective intervention that unlocks the potential of downstream rankers. Code and data: https://github.com/debayan1405/TREC-TOT-2025

</details>


### [134] [GeoGR: A Generative Retrieval Framework for Spatio-Temporal Aware POI Recommendation](https://arxiv.org/abs/2602.10411)
*Fangye Wang,Haowen Lin,Yifang Yuan,Siyuan Wang,Xiaojiang Zhou,Song Yang,Pengjie Wang*

Main category: cs.IR

TL;DR: GeoGR是一个专为基于导航的LBS（如高德地图）设计的地理生成式推荐框架。该框架通过地理感知的序列离散化（SID）标记化流水线和多阶段大型语言模型（LLM）训练策略，解决现有POI预测方法在复杂稀疏环境下跨类别时空协同关系建模不足及LLM任务对齐不佳的问题，实验与在线部署均证实其优越性。


<details>
  <summary>Details</summary>
Motivation: 下一兴趣点（POI）预测是位置服务（LBS）的核心任务，对服务亿万用户的高德等大型导航平台至关重要。现有基于序列离散化（SID）的方法虽取得一定效果，但在复杂稀疏的真实场景中面临两大关键局限：其一，难以充分建模能捕捉跨类别时空协同关系的高质量SID表示；其二，大型语言模型（LLM）与POI推荐任务的对齐效果不佳，限制了生成式推荐的潜力。

Method: 提出GeoGR框架，采用两阶段设计：第一阶段为地理感知的SID标记化流水线，通过地理约束的共访POI对、对比学习和迭代精炼机制，显式学习时空协同语义表示；第二阶段为多阶段LLM训练策略，利用基于模板的持续预训练（CPT）对齐非原生SID标记，并通过监督微调（SFT）实现自回归式POI生成，使模型能够感知用户上下文状态变化并进行意图感知的推荐。

Result: 在多个真实世界数据集上的大量实验表明，GeoGR性能显著优于现有最优基线方法。更重要的是，该框架已部署于高德生产平台，服务数百万用户，多项在线核心指标实现提升，充分验证了其实际应用价值、有效性以及在真实场景中的可扩展性。

Conclusion: GeoGR通过地理感知的SID标记化和多阶段LLM对齐策略，有效解决了复杂稀疏环境下POI推荐的时空协同建模难题，具备强大的生成推荐能力。其成功的大规模工业级部署证明该框架在真实生产环境中的有效性和可扩展性，为LBS平台的下一代智能推荐系统提供了可行的技术方案，具有重要的理论与实践意义。

Abstract: Next Point-of-Interest (POI) prediction is a fundamental task in location-based services, especially critical for large-scale navigation platforms like AMAP that serve billions of users across diverse lifestyle scenarios. While recent POI recommendation approaches based on SIDs have achieved promising, they struggle in complex, sparse real-world environments due to two key limitations: (1) inadequate modeling of high-quality SIDs that capture cross-category spatio-temporal collaborative relationships, and (2) poor alignment between large language models (LLMs) and the POI recommendation task. To this end, we propose GeoGR, a geographic generative recommendation framework tailored for navigation-based LBS like AMAP, which perceives users' contextual state changes and enables intent-aware POI recommendation. GeoGR features a two-stage design: (i) a geo-aware SID tokenization pipeline that explicitly learns spatio-temporal collaborative semantic representations via geographically constrained co-visited POI pairs, contrastive learning, and iterative refinement; and (ii) a multi-stage LLM training strategy that aligns non-native SID tokens through multiple template-based continued pre-training(CPT) and enables autoregressive POI generation via supervised fine-tuning(SFT). Extensive experiments on multiple real-world datasets demonstrate GeoGR's superiority over state-of-the-art baselines. Moreover, deployment on the AMAP platform, serving millions of users with multiple online metrics boosting, confirms its practical effectiveness and scalability in production.

</details>


### [135] [End-to-End Semantic ID Generation for Generative Advertisement Recommendation](https://arxiv.org/abs/2602.10445)
*Jie Jiang,Xinxun Zhang,Enming Zhang,Yuling Xiong,Jun Zhang,Jingwen Wang,Huan Yu,Yuxiang Wang,Hao Wang,Xiao Yan,Jiawei Jiang*

Main category: cs.IR

TL;DR: 针对生成式推荐中基于残差量化(RQ)的两阶段压缩范式存在的目标错位和语义退化问题，本文提出UniSID统一语义ID生成框架，通过端到端联合优化嵌入和语义ID，结合多粒度对比学习和摘要重建机制，在广告推荐场景下显著优于现有方法，Hit Rate指标提升高达4.62%。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法采用残差量化(RQ)将物品编码为离散语义ID(SID)，但存在两个根本性局限：1)两阶段压缩导致目标错位和语义退化；2)RQ结构固有的误差累积问题。这些限制阻碍了语义信息在SID空间的直接流动，影响了推荐性能。

Method: 提出UniSID统一框架：1)从原始广告数据端到端联合优化嵌入向量和SID，实现语义信息直接注入SID空间；2)设计多粒度对比学习策略，在SID不同层级间对齐物品语义；3)引入基于摘要的广告重建机制，强制SID捕获广告上下文中未显式存在的高层语义信息。

Result: 实验表明，UniSID在多个广告推荐场景下持续优于最先进基线方法，Hit Rate指标最高提升4.62%，验证了端到端优化和多粒度对比学习在SID生成中的有效性。

Conclusion: UniSID通过统一端到端学习框架解决了生成式推荐中SID生成的根本性限制，为语义ID的优化提供了新范式，显著提升了下游推荐任务性能，具有重要的理论和实践价值。

Abstract: Generative Recommendation (GR) has excelled by framing recommendation as next-token prediction. This paradigm relies on Semantic IDs (SIDs) to tokenize large-scale items into discrete sequences. Existing GR approaches predominantly generate SIDs via Residual Quantization (RQ), where items are encoded into embeddings and then quantized to discrete SIDs. However, this paradigm suffers from inherent limitations: 1) Objective misalignment and semantic degradation stemming from the two-stage compression; 2) Error accumulation inherent in the structure of RQ. To address these limitations, we propose UniSID, a Unified SID generation framework for generative advertisement recommendation. Specifically, we jointly optimize embeddings and SIDs in an end-to-end manner from raw advertising data, enabling semantic information to flow directly into the SID space and thus addressing the inherent limitations of the two-stage cascading compression paradigm. To capture fine-grained semantics, a multi-granularity contrastive learning strategy is introduced to align distinct items across SID levels. Finally, a summary-based ad reconstruction mechanism is proposed to encourage SIDs to capture high-level semantic information that is not explicitly present in advertising contexts. Experiments demonstrate that UniSID consistently outperforms state-of-the-art SID generation methods, yielding up to a 4.62% improvement in Hit Rate metrics across downstream advertising scenarios compared to the strongest baseline.

</details>


### [136] [Compute Only Once: UG-Separation for Efficient Large Recommendation Models](https://arxiv.org/abs/2602.10455)
*Hui Lu,Zheng Chai,Shipeng Bai,Hao Zhang,Zhifang Fan,Kunmin Bai,Yingwen Wu,Bingzheng Wei,Xiang Sun,Ziyan Gong,Tianyi Liu,Hua Chen,Deping Xie,Zhongkai Chen,Zhiliang Guo,Qiwei Chen,Yuchao Zheng*

Main category: cs.IR

TL;DR: 本文提出UG-Sep框架，首次实现了密集交互推荐模型中用户侧计算的可复用性。通过掩码机制分离用户与候选集特征流，配合信息补偿策略和W8A16量化，在字节跳动多个业务场景中实现了推理延迟降低20%且不影响线上效果。


<details>
  <summary>Details</summary>
Motivation: 推荐系统受缩放定律驱动趋向大规模模型，带来高昂的训练与推理成本。长序列模型虽可通过KV缓存复用用户计算，但密集特征交互架构（如RankMixer）中用户与候选集特征深度耦合，难以复用。亟需在密集交互模型中实现用户侧计算可复用性以降低推理开销。

Method: 1）提出User-Group Separation（UG-Sep）框架，通过掩码机制在token-mixing层显式解耦用户侧与物品侧信息流，保留纯用户侧表示；2）设计信息补偿策略自适应重建被抑制的用户-物品交互；3）引入W8A16权重-only量化缓解内存带宽瓶颈。该设计使token计算可在多个样本间复用。

Result: 在字节跳动多个业务场景（信息流推荐、广告系统）的离线评估与大规模在线A/B实验表明：UG-Sep在无损在线用户体验和商业指标的前提下，推理延迟最高降低20%，同时显著减少了用户侧FLOPs。

Conclusion: UG-Sep首次成功将用户侧计算可复用性引入密集交互推荐模型，通过特征解耦、信息补偿和量化技术组合，实现了推理效率与模型性能的平衡，为工业级推荐系统提供了实用高效的优化方案。

Abstract: Driven by scaling laws, recommender systems increasingly rely on large-scale models to capture complex feature interactions and user behaviors, but this trend also leads to prohibitive training and inference costs. While long-sequence models(e.g., LONGER) can reuse user-side computation through KV caching, such reuse is difficult in dense feature interaction architectures(e.g., RankMixer), where user and group (candidate item) features are deeply entangled across layers. In this work, we propose User-Group Separation (UG-Sep), a novel framework that enables reusable user-side computation in dense interaction models for the first time. UG-Sep introduces a masking mechanism that explicitly disentangles user-side and item-side information flows within token-mixing layers, ensuring that a subset of tokens to preserve purely user-side representations across layers. This design enables corresponding token computations to be reused across multiple samples, significantly reducing redundant inference cost. To compensate for potential expressiveness loss induced by masking, we further propose an Information Compensation strategy that adaptively reconstructs suppressed user-item interactions. Moreover, as UG-Sep substantially reduces user-side FLOPs and exposes memory-bound components, we incorporate W8A16 (8-bit weight, 16-bit activation) weight-only quantization to alleviate memory bandwidth bottlenecks and achieve additional acceleration. We conduct extensive offline evaluations and large-scale online A/B experiments at ByteDance, demonstrating that UG-Sep reduces inference latency by up to 20 percent without degrading online user experience or commercial metrics across multiple business scenarios, including feed recommendation and advertising systems.

</details>


### [137] [ChainRec: An Agentic Recommender Learning to Route Tool Chains for Diverse and Evolving Interests](https://arxiv.org/abs/2602.10490)
*Fuchun Li,Qian Li,Xingyu Gao,Bocheng Pan,Yang Wu,Jun Zhang,Huan Yu,Jie Jiang,Jinsheng Xiao,Hailong Shi*

Main category: cs.IR

TL;DR: 本文提出 ChainRec，一种基于智能体的推荐系统，通过规划器动态选择推理工具来解决现有方法工作流固定的问题。ChainRec 从专家轨迹构建标准化工具库，使用监督微调和偏好优化训练规划器，能够动态选择工具、决定顺序和终止时机。在 AgentRecBench 的 Amazon、Yelp 和 Goodreads 数据集上，ChainRec 在 Avg HR@{1,3,5} 指标上持续优于强基线，尤其在冷启动和兴趣变化场景下提升显著。消融实验验证了工具标准化和偏好优化的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在推荐系统中应用日益广泛，但现有方法仍依赖固定工作流，无法适应多样化的用户场景。实际应用中，用户上下文差异显著，例如在冷启动或兴趣迁移时，智能体需要自适应地决定下一步收集何种证据，而非遵循预设流程。因此，亟需一种能够动态调整推理策略的智能推荐方法。

Method: ChainRec 构建包含规划器和标准化工具智能体库的两层架构。首先，从专家轨迹中提取并标准化构建工具库。其次，训练规划器：通过监督微调学习工具选择策略，结合偏好优化提升决策质量。规划器具备三大功能：动态选择推理工具、确定工具调用顺序、判断何时终止推理过程。

Result: 在 AgentRecBench 的 Amazon、Yelp 和 Goodreads 三个数据集上的实验表明，ChainRec 在 Avg HR@{1,3,5} 指标上持续超越强基线方法。尤其在冷启动和兴趣动态变化场景中，性能提升尤为显著。消融研究证实，工具标准化和基于偏好优化的规划器对系统性能至关重要。

Conclusion: ChainRec 通过引入动态规划机制，有效解决了传统智能推荐系统工作流僵化的问题。该方法显著提升了推荐性能，特别在冷启动和用户兴趣演化等复杂场景下表现出优势，为自适应推荐系统提供了新思路。

Abstract: Large language models (LLMs) are increasingly integrated into recommender systems, motivating recent interest in agentic and reasoning-based recommendation. However, most existing approaches still rely on fixed workflows, applying the same reasoning procedure across diverse recommendation scenarios. In practice, user contexts vary substantially-for example, in cold-start settings or during interest shifts, so an agent should adaptively decide what evidence to gather next rather than following a scripted process. To address this, we propose ChainRec, an agentic recommender that uses a planner to dynamically select reasoning tools. ChainRec builds a standardized Tool Agent Library from expert trajectories. It then trains a planner using supervised fine-tuning and preference optimization to dynamically select tools, decide their order, and determine when to stop. Experiments on AgentRecBench across Amazon, Yelp, and Goodreads show that ChainRec consistently improves Avg HR@{1,3,5} over strong baselines, with especially notable gains in cold-start and evolving-interest scenarios. Ablation studies further validate the importance of tool standardization and preference-optimized planning.

</details>


### [138] [Campaign-2-PT-RAG: LLM-Guided Semantic Product Type Attribution for Scalable Campaign Ranking](https://arxiv.org/abs/2602.10577)
*Yiming Che,Mansi Mane,Keerthi Gopalakrishnan,Parisa Kaghazgaran,Murali Mohana Krishna Dandu,Archana Venkatachalapathy,Sinduja Subramaniam,Yokila Arora,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.IR

TL;DR: LLM驱动的Campaign-2-PT-RAG框架通过活动-产品类型语义对齐生成训练标签，实现高精确率(78-90%)和高召回率(>99%)，解决电商活动归因难题


<details>
  <summary>Details</summary>
Motivation: 电商活动排序模型亟需大规模训练标签，但活动创意语言难以直接映射到产品购买，缺乏清晰的产品级归因导致监督学习受限

Method: 提出Campaign-2-PT-RAG框架：利用LLM解析活动隐含意图，通过语义搜索检索候选产品类型，经结构化LLM分类器评估相关性后生成产品覆盖集，最终将匹配用户购买转化为正样本标签

Result: 在内部与合成数据集上验证，该方法生成标签质量高，精确率78-90%，召回率超过99%，经专家标注验证有效

Conclusion: 将模糊的归因问题重构为可处理的语义对齐任务，为生产环境电商活动排序优化等下游任务提供可扩展且一致的监督信号

Abstract: E-commerce campaign ranking models require large-scale training labels indicating which users purchased due to campaign influence. However, generating these labels is challenging because campaigns use creative, thematic language that does not directly map to product purchases. Without clear product-level attribution, supervised learning for campaign optimization remains limited. We present \textbf{Campaign-2-PT-RAG}, a scalable label generation framework that constructs user--campaign purchase labels by inferring which product types (PTs) each campaign promotes. The framework first interprets campaign content using large language models (LLMs) to capture implicit intent, then retrieves candidate PTs through semantic search over the platform taxonomy. A structured LLM-based classifier evaluates each PT's relevance, producing a campaign-specific product coverage set. User purchases matching these PTs generate positive training labels for downstream ranking models. This approach reframes the ambiguous attribution problem into a tractable semantic alignment task, enabling scalable and consistent supervision for downstream tasks such as campaign ranking optimization in production e-commerce environments. Experiments on internal and synthetic datasets, validated against expert-annotated campaign--PT mappings, show that our LLM-assisted approach generates high-quality labels with 78--90% precision while maintaining over 99% recall.

</details>


### [139] [S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage](https://arxiv.org/abs/2602.10606)
*Jie Jiang,Hongbo Tang,Wenjie Wu,Yangru Huang,Zhenmao Li,Qian Li,Changping Wang,Jun Zhang,Huan Yu*

Main category: cs.IR

TL;DR: 本文提出 S-GRec，一种语义感知的推荐框架，通过离线 LLM 语义评判与在线轻量生成器解耦，利用个性化语义评判（PSJ）和不对称优势策略优化（A2PO），在提升 CTR 和 GMV 的同时避免了实时 LLM 推理的高昂开销。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型端到端生成物品序列，但行为日志训练对潜在用户意图的监督较弱；大语言模型虽能提供丰富的语义先验，但其语义信号可能与平台商业目标冲突，且推理成本高昂，难以直接应用于工业级推荐系统。

Method: S-GRec 框架将在线轻量生成器与离线 LLM 语义评判器分离。它采用两阶段个性化语义评判（PSJ），生成可解释的方面证据并通过成对反馈学习用户条件聚合，产生稳定的语义奖励。为防止语义监督偏离商业目标，提出不对称优势策略优化（A2PO），以商业奖励（如 eCPM）为锚点，仅在语义优势与商业目标一致时注入。

Result: 在公开基准和大规模生产系统上的广泛实验表明，S-GRec 在 CTR 上取得统计显著的增益，在线 A/B 测试中 GMV 提升 1.19%，且无需实时 LLM 推理，验证了其有效性与可扩展性。

Conclusion: S-GRec 成功地将 LLM 的语义监督引入工业推荐，平衡了语义信号与商业目标，显著提升了推荐效果，同时保持了可扩展性，为大规模推荐系统提供了新思路。

Abstract: Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\% lift in GMV in online A/B tests, without requiring real-time LLM inference.

</details>


### [140] [A Cognitive Distribution and Behavior-Consistent Framework for Black-Box Attacks on Recommender Systems](https://arxiv.org/abs/2602.10633)
*Hongyue Zhan,Mingming Li,Dongqin Liu,Hui Wang,Yaning Zhang,Xi Zhou,Honglei Lv,Jiao Dai,Jizhong Han*

Main category: cs.IR

TL;DR: 本文针对序列推荐系统的安全漏洞，提出一种双增强攻击框架，通过认知分布对齐和行为一致性优化，显著提升黑盒攻击的成功率与隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 序列推荐系统的黑盒特性使其面临模型提取和对抗操纵的安全风险。现有方法或忽视排名位置的重要性导致知识迁移不完整，或生成的对抗序列缺乏真实用户行为的语义一致性而易于被检测，亟需更有效的攻击策略。

Method: 提出双增强攻击框架：1) 认知分布驱动提取机制：利用首因效应和位置偏差，将离散排名映射为位置感知衰减的连续分布，实现从顺序对齐到认知分布对齐的升级；2) 行为感知噪声项生成策略：联合优化协同信号与梯度信号，在提升目标项排名的同时保证语义一致性和统计隐蔽性。

Result: 在多个数据集上的实验表明，该方法在攻击成功率和规避率上均显著优于现有方法，验证了认知建模与行为一致性结合的有效性。

Conclusion: 研究证明，融合认知分布建模与行为一致性约束的攻击框架能显著提升对推荐系统的攻击效能与隐蔽性，为推荐系统安全防御提供了重要参考。

Abstract: With the growing deployment of sequential recommender systems in e-commerce and other fields, their black-box interfaces raise security concerns: models are vulnerable to extraction and subsequent adversarial manipulation. Existing black-box extraction attacks primarily rely on hard labels or pairwise learning, often ignoring the importance of ranking positions, which results in incomplete knowledge transfer. Moreover, adversarial sequences generated via pure gradient methods lack semantic consistency with real user behavior, making them easily detectable. To overcome these limitations, this paper proposes a dual-enhanced attack framework. First, drawing on primacy effects and position bias, we introduce a cognitive distribution-driven extraction mechanism that maps discrete rankings into continuous value distributions with position-aware decay, thereby advancing from order alignment to cognitive distribution alignment. Second, we design a behavior-aware noisy item generation strategy that jointly optimizes collaborative signals and gradient signals. This ensures both semantic coherence and statistical stealth while effectively promoting target item rankings. Extensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods in both attack success rate and evasion rate, validating the value of integrating cognitive modeling and behavioral consistency for secure recommender systems.

</details>


### [141] [Training-Induced Bias Toward LLM-Generated Content in Dense Retrieval](https://arxiv.org/abs/2602.10833)
*William Xion,Wolfgang Nejdl*

Main category: cs.IR

TL;DR: 本研究通过受控实验重新审视密集检索中的"来源偏见"现象。研究发现，这种对LLM生成文本的偏好并非检索模型的固有属性，而是由训练过程引起的。具体而言，在MS MARCO上进行监督微调会一致性地偏向LLM文本，而在LLM生成语料上的微调会产生明显的亲LLM偏见。研究还发现困惑度作为解释因素的作用较弱。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明密集检索模型存在"来源偏见"，即对大型语言模型生成文本的广泛偏好，并假设低困惑度是导致该效应的原因。然而，这种偏见的来源和成因尚不明确。本研究旨在通过系统性的受控评估，追溯这种偏好在不同训练阶段和数据来源下的产生过程，以验证其是否属于检索模型的固有属性。

Method: 研究使用SciFact和Natural Questions (NQ320K)数据集的平行人工与LLM生成文本对，比较了四种模型设置：无监督检查点、基于领域内人工文本的微调、基于领域内LLM生成文本的微调，以及MS MARCO微调。通过检索器中心的困惑度探测实验，将语言建模头重新附加到微调后的密集检索编码器上，评估困惑度与相关性的关系。

Result: 1) 无监督检索器并未表现出统一亲LLM偏好，其方向和幅度取决于数据集；2) 在MS MARCO上的监督微调会一致性地将排序转向LLM生成文本；3) 领域内微调产生数据集特定的、不一致的偏好变化；4) 在LLM生成语料上微调会诱导明显的亲LLM偏见；5) 困惑度探测显示其与相关性的吻合度接近随机水平，削弱了困惑度作为解释因素的能力。

Conclusion: 本研究表明，来源偏见是一种训练诱导现象，而非密集检索模型的固有属性。研究强调了训练数据和微调策略对检索模型行为的重要影响，并质疑了困惑度作为偏见解释指标的可靠性。这些发现为理解和缓解检索模型中的偏见提供了重要见解。

Abstract: Dense retrieval is a promising approach for acquiring relevant context or world knowledge in open-domain natural language processing tasks and is now widely used in information retrieval applications. However, recent reports claim a broad preference for text generated by large language models (LLMs). This bias is called "source bias", and it has been hypothesized that lower perplexity contributes to this effect. In this study, we revisit this claim by conducting a controlled evaluation to trace the emergence of such preferences across training stages and data sources. Using parallel human- and LLM-generated counterparts of the SciFact and Natural Questions (NQ320K) datasets, we compare unsupervised checkpoints with models fine-tuned using in-domain human text, in-domain LLM-generated text, and MS MARCO. Our results show the following: 1) Unsupervised retrievers do not exhibit a uniform pro-LLM preference. The direction and magnitude depend on the dataset. 2) Across the settings tested, supervised fine-tuning on MS MARCO consistently shifts the rankings toward LLM-generated text. 3) In-domain fine-tuning produces dataset-specific and inconsistent shifts in preference. 4) Fine-tuning on LLM-generated corpora induces a pronounced pro-LLM bias. Finally, a retriever-centric perplexity probe involving the reattachment of a language modeling head to the fine-tuned dense retriever encoder indicates agreement with relevance near chance, thereby weakening the explanatory power of perplexity. Our study demonstrates that source bias is a training-induced phenomenon rather than an inherent property of dense retrievers.

</details>
