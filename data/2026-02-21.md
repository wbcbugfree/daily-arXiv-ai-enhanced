<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.AI](#cs.AI) [Total: 47]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.IR](#cs.IR) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: 本文针对非可验证域（如LLM对齐）缺乏真实验证器的问题，探索了参考引导的LLM评估器作为软验证器的可行性。通过设计增强评估协议，该方法显著提升了LLM评判器的准确性，并实现了参考引导的自我改进，在AlpacaEval和Arena-Hard基准上分别取得+20.2/+17.1和+5.3/+3.6个百分点的性能提升，验证了在非可验证域中有效后训练的潜力。


<details>
  <summary>Details</summary>
Motivation: 强化学习与可验证奖励（RLVR）在推理任务中效果显著，但无法直接应用于缺乏真实验证器的非可验证域（如LLM对齐）。本研究旨在探究参考引导的LLM评估器能否作为软"验证器"来弥合这一鸿沟，从而为LLM对齐等任务提供可行的强化学习方法。

Method: 首先设计评估协议，利用参考输出增强基于LLM的评估器。通过实验验证，参考引导能显著提升能力较弱的LLM评判器的准确性（使用前沿模型参考），强评判器也能通过高质量人工参考得到增强。在此基础上，构建参考引导的自我改进对齐调优框架，使用带参考的LLM作为评判器进行自我提升。

Result: 参考引导方法显著提升了LLM评判器的准确性。在Llama-3-8B-Instruct和Qwen2.5-7B模型上，该方法在AlpacaEval/Arena-Hard基准分别达到73.1%/58.7%和70.0%/74.1%的胜率，相比直接SFT蒸馏平均提升+20.2/+17.1个百分点，相比参考自由自我改进提升+5.3/+3.6个百分点，性能可与强奖励模型ArmoRM相媲美。

Conclusion: 研究结果表明，参考引导的LLM评估器能够有效充当非可验证域中的软验证器，为LLM后训练提供新范式。该方法不仅提升了评判器质量，还通过参考引导的自我改进实现了显著性能增益，为非可验证域的对齐问题提供了实用解决方案。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [2] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

TL;DR: 本研究针对希腊语问答任务中大型语言模型的研究空白，提出三项贡献：1) DemosQA数据集，基于社交媒体构建以捕捉希腊社会文化特征；2) 内存高效的LLM评估框架；3) 对11个单语/多语模型在6个希腊语数据集上的广泛评估。研究旨在解决低资源语言模型中文化表征不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型研究长期偏向英语等高资源语言，多语言模型存在数据偏向性，可能误表征社会文化信息。希腊语等低资源语言的单语LLM有效性尚未被充分探索。本研究旨在填补这一空白，系统评估LLM在希腊语问答任务上的表现，提升文化敏感性。

Method: 1) 构建DemosQA数据集：采集社交媒体用户问题与社区审核答案，反映希腊文化时代精神；2) 开发内存高效、可适配多语言的LLM评估框架；3) 系统评估11个单语/多语模型在6个人工希腊语问答数据集上的表现，采用三种不同提示策略。

Result: 实现了三大贡献：成功构建希腊语社交媒体问答数据集DemosQA；开发可适配多语言的高效评估框架；完成11个LLM在6个希腊语数据集上的全面评估。研究建立了完整的希腊语QA评估体系，并开源代码和数据以确保可复现性。

Conclusion: 本研究通过构建文化感知数据集、开发高效评估框架及系统评估，有效填补了希腊语问答研究的空白。结果表明，针对低资源语言构建单语数据集和评估体系对提升文化表征能力至关重要，为希腊语NLP研究提供重要资源，并为其他低资源语言研究提供方法论参考。

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [3] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

TL;DR: Flow-based连续去噪语言模型(FLM)通过欧几里得去噪和简单时间重参数化，在质量和速度上均优于离散扩散模型；蒸馏后的FMLM一步生成即可超越其他模型8步质量。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型虽被寄予厚望能比自回归模型更快生成，但在少步生成时样本质量急剧下降。本文旨在证明：基于流的连续去噪方法可在离散模态上实现更优的质量与速度平衡。

Method: 重新审视离散模态的流理论基础，构建对one-hot编码进行欧几里得去噪的FLM；采用交叉熵目标预测干净数据，并设计时间重参数化提升训练稳定性；进一步将FLM蒸馏为流映射得到FMLM以支持少步生成。

Result: 在LM1B和OWT数据集上，FLM生成质量媲美SOTA离散扩散模型；FMLM全面超越近期少步语言模型，其一步生成质量即可超过对比模型8步生成结果。

Conclusion: 该工作挑战了"离散扩散是离散模态生成建模必要手段"的普遍假设，为大规模加速流基语言建模开辟了新路径。

Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [4] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

TL;DR: 本研究针对大型语言模型在保险等受监管领域的部署限制，提出一种本地化部署的治理感知语言模型组件，通过LoRA微调预训练模型，利用数百万历史保修索赔数据，从非结构化索赔叙述中生成结构化纠正措施建议。该组件作为索赔处理流程的初始决策模块，结合语义相似度指标和人工评估的多维框架验证，结果显示领域自适应微调显著优于通用大模型，约80%案例达到与真实纠正措施近乎一致的匹配度，证明了其在保险应用中作为可靠可治理构建块的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在通用语言任务上表现优异，但在保险等受监管和数据敏感领域的部署仍受限。现有通用模型和基于提示的方法难以满足特定领域的准确性、可治理性和合规性要求，需要开发能够适应真实世界运营数据、确保决策透明可控的领域专用解决方案。

Method: 研究采用低秩适应（LoRA）技术对预训练大模型进行领域自适应微调，利用数百万历史保修索赔数据训练模型，使其能从非结构化索赔叙述中提取信息并生成结构化纠正措施建议。模型作为索赔处理流程中的初始决策模块，通过结合自动化语义相似度指标与人工评估的多维度框架进行严格验证。

Result: 实验结果表明，领域专用微调模型在性能上显著超越商业通用型和基于提示的大语言模型，约80%的评估案例实现了与真实纠正措施近乎完全一致的匹配。该结果证明了领域自适应方法能够使模型输出分布更接近真实业务数据，具备高预测准确性和实际应用价值。

Conclusion: 本研究从理论和实证两方面证实，领域自适应微调是构建可靠可治理保险应用组件的有效途径。所提出的本地化部署治理感知模型不仅能够加速理赔专员决策流程，还能确保模型输出的合规性和透明度，为LLMs在受监管行业的实际应用提供了可行路径和重要参考。

Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [5] [BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization](https://arxiv.org/abs/2602.16843)
*Ahmed Rafid,Rumman Adib,Fariya Ahmed,Ajwad Abrar,Mohammed Saidul Islam*

Main category: cs.CL

TL;DR: 本文提出BanglaSummEval，一个无需参考摘要的问答框架，用于评估孟加拉语摘要的事实一致性。该方法通过自动生成问答对评估事实准确性和内容覆盖度，在教育和医疗领域验证中与人工评判高度相关。


<details>
  <summary>Details</summary>
Motivation: 现有事实一致性评估指标大多忽略孟加拉语等低资源语言，且依赖参考摘要；在高风险领域如医疗和新闻中，缺乏可靠的自动评估工具。

Method: 提出统一框架BanglaSummEval：使用单一多语言指令微调语言模型自动生成源文档和摘要的问答对，评估事实准确性和内容覆盖度；采用BERTScore-Recall进行答案语义比对，并引入问题重要性加权机制。

Result: 在300个教育及医疗领域的人工摘要上验证，与专家人工评判呈现强相关性（Pearson r=0.694，Spearman ρ=0.763），提供可解释的逐步诊断。

Conclusion: BanglaSummEval为低资源语言提供了实用、透明的事实一致性评估方案，通过统一设计降低系统复杂度和计算成本，适用于高风险领域。

Abstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.

</details>


### [6] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文首次开展面向美因茨方言Meenzerisch的自然语言处理研究，构建了一个包含2,351个方言词及其标准德语释义的数字词典。通过实验发现，现有大语言模型在该方言的释义生成和词汇生成任务上表现极差（准确率分别为6.27%和1.51%），即使采用少样本学习和规则提取也未能突破10%准确率，凸显了加强德语方言资源建设和研究投入的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 美因茨方言Meenzerisch作为德国传统狂欢节语言正濒临消亡，与众多德语方言面临相同命运。自然语言处理技术在语言保护与复兴方面具有潜力，但目前尚无针对Meenzerisch的NLP研究。本研究旨在填补这一空白，为方言的数字化保存和计算建模提供基础资源与基准参考。

Method: 研究首先将Schramm（1966）的传统词典资源转化为包含2,351个Meenzerisch词汇-标准德语释义对的结构化数据集。在此基础上，系统评估了当前先进的大语言模型在两个核心任务上的表现：方言词义生成和方言词形生成。进一步探索了少样本学习以及从训练集提取规则并注入模型的改进策略。

Result: 实验结果显示，大语言模型处理Meenzerisch的能力极其有限：词义生成任务最高准确率仅为6.27%，词形生成任务仅为1.51%。虽然少样本学习和规则提取能在一定程度上提升性能，但最终准确率仍不足10%，表明现有技术在濒危方言处理上存在显著局限性。

Conclusion: 本研究证实了当前大语言模型在资源稀缺的濒危方言任务上面临严峻挑战，准确率远低于实用水平。这一结果强烈呼吁学术界投入更多资源，加强针对德语方言的专门化研究，开发适应低资源场景的NLP方法，以有效支持方言保护与传承工作。

Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [7] [ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders](https://arxiv.org/abs/2602.16938)
*Ofer Meshi,Krisztian Balog,Sally Goldman,Avi Caciularu,Guy Tennenholtz,Jihwan Jeong,Amir Globerson,Craig Boutilier*

Main category: cs.CL

TL;DR: 研究LLM用户模拟器的现实性鸿沟问题，提出ConvApparel数据集与三重验证框架，揭示数据驱动模拟器通过反事实验证展现更强健的用户模型。


<details>
  <summary>Details</summary>
Motivation: LLM用户模拟器存在严重现实性鸿沟，导致对话系统在模拟环境表现良好但实际应用失败，亟需更真实的数据集和验证方法来弥合这一差距。

Method: 构建ConvApparel数据集，采用"好/坏"推荐系统双智能体收集协议，获得丰富用户满意度标注；提出融合统计对齐、类人度评分与反事实验证的评估框架。

Result: 所有模拟器均存在显著现实性鸿沟，但数据驱动模拟器在反事实验证中显著优于提示基线，能更真实地适应未见用户行为。

Conclusion: 数据驱动模拟器虽不完美，但蕴含更稳健的用户模型，为提升对话AI的用户模拟器真实性提供了有效路径。

Abstract: The promise of LLM-based user simulators to improve conversational AI is hindered by a critical "realism gap," leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both "good" and "bad" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, suggesting they embody more robust, if imperfect, user models.

</details>


### [8] [When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English](https://arxiv.org/abs/2602.16957)
*Hasan Can Biyik,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 本研究考察跨语言等效性对多语言委婉语检测迁移学习的影响，揭示语义重叠并非正向迁移的充分条件，在土耳其语到英语的低资源迁移场景中存在反直觉现象。


<details>
  <summary>Details</summary>
Motivation: 委婉语具有文化敏感性和语用复杂性，跨语言建模面临挑战。探究跨语言等效性对迁移效果的影响机制，对提升低资源语言委婉语检测性能具有重要理论和实践意义。

Method: 基于功能、语用和语义对齐程度，将土耳其语和英语的潜在委婉语（PETs）划分为重叠（OPETs）与非重叠（NOPETs）子集，通过对比实验分析迁移不对称性。

Result: 发现迁移不对称性：土耳其语→英语低资源迁移中，重叠委婉语性能可能下降；非重叠委婉语训练有时反能提升性能。标签分布差异是主要解释因素，领域特定对齐的影响受限于数据稀疏性。

Conclusion: 研究结果表明，语义重叠不能保证跨语言委婉语检测的正向迁移，标签分布和领域对齐程度对迁移效果具有关键影响，为低资源语言处理提供了新的洞见。

Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.

</details>


### [9] [Large Language Models Persuade Without Planning Theory of Mind](https://arxiv.org/abs/2602.17045)
*Jared Moore,Rasmus Overmark,Ned Cooper,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: 该研究通过设计说服任务，发现LLM在目标心理状态已知时表现优异，但在需要推断时表现低于随机，而人类在两方面均表现中等；当涉及真实人类时，LLM说服力却超过人类，表明其说服能力可能不依赖ToM推理，而是通过修辞策略实现。


<details>
  <summary>Details</summary>
Motivation: 现有ToM评估依赖静态问答，但理论认为第一人称互动才是ToM核心，传统方法无法捕捉真实心理状态推理能力，因此需要开发基于动态交互的新评估范式。

Method: 设计三选项政策说服任务，要求智能体根据目标的知识状态和动机状态策略性透露信息；设置"已知"与"隐藏"两种条件，隐藏时需主动询问或推断。开展三项实验：人类说服理性机器人、人类说服角色扮演者、测量真实信念改变。

Result: LLM在"已知"条件下表现优异，"隐藏"条件下低于随机水平，揭示其多步规划缺陷；人类在两者中均表现中等。但在涉及真实人类的实验中，LLM说服力显著超越人类，说明有效说服可通过非ToM的修辞策略实现。

Conclusion: 研究结果警示不宜将类人ToM能力归于LLM，尽管其在说服任务中表现出色；但LLM具有显著影响人类信念与行为的潜力，这对人机交互伦理具有重要启示。

Abstract: A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.

</details>


### [10] [Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data](https://arxiv.org/abs/2602.17051)
*Deepak Uniyal,Md Abul Bashar,Richi Nayak*

Main category: cs.CL

TL;DR: 本研究以氢能源为案例，分析2013-2022年英、日、印地、韩四语种超900万条推文，系统比较四种跨语言文本分类策略（翻译标注数据、翻译未标注数据、多语言预训练模型、混合策略）在噪声过滤中的表现，揭示跨语言pipeline的关键权衡。


<details>
  <summary>Details</summary>
Motivation: 社交媒体多语言话语分析是NLP的核心挑战之一，尤其当大规模公共辩论跨越语言边界时。基于关键词的数据采集产生大量无关噪声，亟需可靠的跨语言分类方法以支持全球对话分析。

Method: 采用四语种（英、日、印地、韩）十年期推文数据集（2013-2022，>900万条）。针对关键词采集噪声问题，评估四种过滤方法：1）翻译英语标注数据至目标语言构建单语模型；2）翻译全语言未标注数据至英语构建统一模型；3）直接应用英语微调的多语言Transformer；4）翻译标注与多语言训练混合策略。后续执行主题建模提取主导主题。

Result: 实验表明不同方法在氢能源推文过滤性能上存在显著差异，揭示了翻译策略与多语言模型间的核心权衡关系。

Conclusion: 研究为大规模社交媒体跨语言分析提供了可操作的pipeline优化洞见，指导实践中的方法选择。

Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.

</details>


### [11] [ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning](https://arxiv.org/abs/2602.17054)
*Hussein S. Al-Olimat,Ahmad Alshareef*

Main category: cs.CL

TL;DR: 本文介绍ALPS，一个阿拉伯语语言学与语用学挑战集，包含531道专家精心设计的题目，涵盖15个任务和47个子任务，用于深度评估阿拉伯语模型的深层语义和语用理解能力。研究发现，现有模型虽流畅但存在严重的形态句法依赖错误（错误率达36.5%），商业模型显著优于阿拉伯语原生模型。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语NLP基准过度关注规模，依赖合成或翻译数据，缺乏深层语言学验证。阿拉伯语复杂的形态句法结构（如变音符号依赖）要求更深入的语义和语用理解能力，而当前基准无法充分评估这些核心语言学能力。

Method: 作者构建了ALPS数据集，由阿拉伯语语言学专家精心设计531道原生诊断题，涵盖15个任务和47个子任务，聚焦深层语义和语用学。评估了23个不同模型（商业、开源和阿拉伯语原生模型），以单遍人类表现（84.6%）和专家裁定标准（99.2%）作为基准。

Result: 模型表现出高流畅性但深层语言学理解能力薄弱，形态句法依赖任务错误率高达36.5%。商业模型Gemini-3-flash以94.2%准确率超越人类平均水平，但最佳阿拉伯语原生模型Jais-2-70B仅达83.6%，与人类表现存在显著差距。

Conclusion: 阿拉伯语NLP需要从规模导向转向深度语言学理解，当前模型在核心形态句法能力上存在根本性缺陷。研究结果揭示了商业模型与阿拉伯语原生模型之间的性能鸿沟，强调需要更多原生、文化真实的语言资源来推动阿拉伯语语言技术发展。

Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.

</details>


### [12] [BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios](https://arxiv.org/abs/2602.17072)
*Yunseung Lee,Subin Kim,Youngjun Kwak,Jaegul Choo*

Main category: cs.CL

TL;DR: 针对大语言模型在银行核心计算任务中准确率低的问题，本文提出BankMathBench数据集，通过三级难度任务评测并显著提升模型在银行场景下的数值推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数字银行中应用广泛，但在存款、贷款等核心计算任务（如利息计算、产品比较）中存在系统性错误。现有数学数据集和金融基准分别聚焦基础数学和金融文档，忽视了日常银行场景的多步推理需求，缺乏针对性评测工具。

Method: 构建BankMathBench领域特定数据集，包含三个难度等级：基础级（单产品推理）、中级（多产品比较）和高级（多条件场景）。通过工具增强的微调方法训练开源大语言模型。

Result: 实验表明，工具增强微调使模型在三个层级上的准确率分别提升57.6%、75.1%和62.9个百分点，显著优于零样本基线，验证了数据集对领域推理能力的增强效果。

Conclusion: BankMathBench为评估和推进大语言模型在真实银行场景中的数值推理能力提供了可靠基准，有效弥补了现有评测体系的不足。

Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.

</details>


### [13] [The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI](https://arxiv.org/abs/2602.17127)
*Dusan Bosnjakovic*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的审计框架，运用心理测量理论量化大语言模型的稳定行为倾向。通过强制选择序数情景和混合线性模型分析，研究发现尽管项目表述差异导致高方差，但提供商的"实验室信号"仍显著影响模型行为，揭示了锁定生态系统中潜在的意识形态回音室风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从独立聊天界面转变为多智能体系统和递归评估循环中的基础推理层，检测持久、提供商级别的行为特征成为安全与治理的关键需求。传统基准测试仅衡量瞬态任务准确率，无法捕捉训练和对齐过程中嵌入的稳定潜在响应策略，即超越单个模型版本的"主流思维模式"。

Method: 研究采用心理测量测量理论，特别是序数不确定性下的潜在特质估计。利用语义正交诱饵掩盖的强制选择序数情景，并受加密置换不变性约束，对九个主流模型在优化偏差、奉承性和现状合法化等维度进行审计。使用混合线性模型（MixedLM）和组内相关系数（ICC）分析识别行为模式。

Result: 分析表明，项目级别表述框架驱动了高方差，但持久的"实验室信号"解释了显著的行为聚类。这些潜在偏好在"锁定"的提供商生态系统中并非静态错误，而是可能多层AI架构中形成递归意识形态回音室的复合变量。

Conclusion: 研究证明，在锁定提供商生态系统中，潜在偏见是持续存在的行为签名，而非临时缺陷。这些偏见作为复合变量，可能在多层AI架构中创建递归的意识形态回音室，对AI安全和治理构成系统性风险，需要在模型审计和监管中予以重视。

Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.
  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.
  Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.

</details>


### [14] [What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform](https://arxiv.org/abs/2602.17194)
*Adrian Cosma,Cosmin Dumitrache,Emilian Radoi*

Main category: cs.CL

TL;DR: 本研究分析罗马尼亚语文本远程医疗中的患者满意度信号。基于77,334个匿名医患问答对，建立二元分类模型，发现患者与医生历史特征主导预测，而回复文本特征（尤其是礼貌/模糊语和词汇多样性）虽影响较小但具有可操作性。礼貌表达与满意度正相关，词汇多样性与满意度负相关。


<details>
  <summary>Details</summary>
Motivation: 文本远程医疗普及使临床医生面临患者评分压力，这些评分更多反映沟通质量而非医疗准确性。现有研究缺乏对罗马尼亚语环境下满意度驱动因素的系统分析，亟需识别可操作的文本特征以优化医患书面沟通。

Method: 利用77,334个罗马尼亚语文本远程医疗问答对，构建二元反馈分类模型（点赞为正向，其他为负向）。提取三类特征：语言无关的结构与可读性特征、罗马尼亚语LIWC心理语言学特征、礼貌与模糊语标记。采用时间分割策略训练分类器，并进行SHAP可解释性分析。

Result: 1. 患者和医生历史特征是满意度预测的主导因素，起强先验作用；2. 回复文本特征提供较小但关键的信号，具有可干预性；3. 子组分析一致表明：礼貌和模糊语标记与患者反馈呈正相关，词汇多样性与反馈呈负相关。

Conclusion: 研究证实历史因素对患者满意度影响最大，但回复文本特征仍具改进价值。临床医生可通过增强礼貌表达、适度使用模糊语、降低词汇复杂度来提升患者体验。这为文本远程医疗的沟通质量优化提供了实证依据，提示在关注评分的同时应区分沟通效果与临床准确性。

Abstract: Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.

</details>


### [15] [Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study](https://arxiv.org/abs/2602.17262)
*Kensuke Okada,Yui Furukawa,Kyosuke Bunji*

Main category: cs.CL

TL;DR: 该论文提出了一个心理测量学框架来量化和缓解大语言模型(LLM)在自我报告问卷中的社会期望性作答(SDR)问题。通过在9个指令调优LLM上的实验，发现李克特式量表存在严重的SDR偏差，而采用社会期望性匹配的等级迫选式(GFC)方法能显著减弱这种偏差，同时保持人格档案的还原度，揭示了SDR与评估准确性之间的模型依赖性权衡关系。


<details>
  <summary>Details</summary>
Motivation: 人类自我报告问卷被广泛用于基准测试和审计大语言模型(LLM)的人格一致性、安全性和偏见评估，但这些工具默认被试会诚实作答。在评估性情境中，LLM倾向于给出社会期望的答案而非真实反应，这种社会期望性作答(SDR)会污染问卷分数并导致下游结论偏差。现有方法缺乏对LLM中SDR的系统量化和缓解手段。

Method: 研究者提出双轨心理测量学框架：(1) 量化SDR：在相同量表上分别实施诚实(HONEST)和伪装好(FAKE-GOOD)两种指导语，利用项目反应理论(IRT)估计潜在特质分数，并计算方向校正的标准化效应量；(2) 缓解SDR：通过约束优化从项目库中筛选30对跨领域项目，构建社会期望性匹配的等级迫选式(GFC)大五人格量表，使选项间社会期望性相当。实验涵盖9个指令调优LLM，使用已知目标档案的合成人格进行评估。

Result: 实验结果显示，李克特式问卷在所有9个LLM中均表现出持续且较大的社会期望性作答效应，而社会期望性匹配的GFC方法显著减弱了SDR，同时基本保持了目标人格档案的有效恢复。这表明不同测量方法在SDR控制和信息还原度之间存在模型依赖的权衡关系。

Conclusion: 该研究证实LLM在问卷评估中存在可测量的社会期望性作答偏差，GFC是缓解该问题的有效方法。研究强调未来在LLM基准测试和审计中应采用SDR感知的报告实践，并需明确SDR与评估准确性之间的权衡关系，以提高评估结果的可靠性。

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

</details>


### [16] [Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective](https://arxiv.org/abs/2602.17283)
*Yukun Chen,Xinyu Zhang,Jialong Tang,Yu Wan,Baosong Yang,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文提出了X-Value，一个跨语言价值观评估基准，用于评估大语言模型从全球视角评估内容深层价值观的能力。该基准包含18种语言的5000多个问答对，基于施瓦茨基本价值观理论构建，并采用两阶段标注框架区分全球共识与多元主义议题。研究发现当前SOTA模型在跨语言价值观评估中存在显著不足（准确率<77%）和语言间性能差异（准确率差距>20%），强调需要提升模型的价值观感知内容评估能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的内容安全评估主要关注暴力、仇恨言论等显性危害，忽视了数字内容中传达的细微价值观维度。这种评估范式无法捕捉深层次的价值观差异，特别是在跨文化语境下的价值观判断，导致模型在全球化应用中存在盲区。

Method: 1. 构建X-Value基准：包含18种语言的5000多个问答对，系统组织成7个核心领域，基于施瓦茨基本价值观理论，并分为简单和困难两个级别用于区分性评估。2. 提出两阶段标注框架：第一阶段识别问题属于全球共识（如人权）还是多元主义（如宗教）；第二阶段对内容中嵌入的潜在价值观进行多方评估。

Result: 对当前SOTA大语言模型的系统性评估显示：跨语言价值观评估能力存在明显缺陷，整体准确率低于77%；不同语言间存在显著性能差异，准确率差距超过20%；模型在处理多元主义价值观议题时表现尤为不足。

Conclusion: 该工作揭示了当前大语言模型在细粒度、价值观感知的内容评估能力上的严重不足，特别是在跨文化语境下。迫切需要改进模型对深层价值观的识别和理解能力，以实现更加全面和公平的内容安全评估。X-Value基准的发布为这一研究方向提供了重要的评估工具和资源。

Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\%$), with significant performance disparities across different languages ($ΔAcc > 20\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.

</details>


### [17] [Representation Collapse in Machine Translation Through the Lens of Angular Dispersion](https://arxiv.org/abs/2602.17287)
*Evgeniia Tokarchuk,Maya K. Nachesa,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 本研究揭示了Transformer神经机器翻译模型在训练过程中深层表示崩溃的动态特性，验证了角分散正则化方法在缓解该问题并提升翻译质量方面的有效性，同时证实量化后正则化优势依然存在。


<details>
  <summary>Details</summary>
Motivation: 标准next-token预测训练导致Transformer深层表示出现几何空间利用失效的崩溃现象，连续输出NMT中更为严重，甚至可能退化为平凡解，而量化模型也面临类似问题，亟需系统性分析和有效正则化策略。

Method: 通过系统分析离散与连续NMT Transformer在各训练阶段的表示动态，引入基于角分散的正则化方法，并评估其在缓解表示崩溃和提升翻译质量方面的效果，同时考察量化后的性能保持情况。

Result: 角分散正则化有效缓解了深层Transformer层的表示崩溃，显著提升翻译质量；量化模型虽表现出类似崩溃行为，但正则化优势在量化后得以保持。

Conclusion: 角分散正则化是解决NMT表示崩溃的实用有效方法，在模型量化场景下仍具优势，为构建高效稳定的神经机器翻译系统提供了可行方案。

Abstract: Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.

</details>


### [18] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan Kostić,Conor Fallon,Julian Risch,Alexander Löser*

Main category: cs.CL

TL;DR: 本研究通过生成语义保持的词汇与句法扰动，系统评估23个大语言模型在三大基准上的表现，发现模型对浅层语言变化异常敏感，性能显著波动且排行榜失稳，揭示其对表层词汇模式的依赖超越深层语言理解能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的标准化评测基准虽已成为模型比较的核心工具，但其可靠性正因模型对输入提示的浅层变化过度敏感而备受质疑。亟需严谨研究阐明真值条件等价的语义保持扰动如何系统性影响模型的绝对性能与相对排名。

Method: 在MMLU、SQuAD和AMEGA三个基准上，对23个当代大语言模型实施对照实验。采用两个语言学原则的扰动管道：词汇管道通过同义词替换生成词级变化；句法管道利用依存句法分析确定合法句法转换。量化评估这些语义保持扰动对模型性能与排行榜的影响。

Result: 词汇扰动在几乎所有模型和任务上均导致统计显著且大幅的性能下降；句法扰动效应异质，偶现性能提升。两类扰动均破坏复杂任务的排行榜稳定性。模型鲁棒性与模型规模无一致相关性，表现出强烈的任务依赖性。

Conclusion: 研究发现大语言模型主要依赖表层词汇模式而非抽象句法语义能力，挑战了现有评测基准的有效性。强调应将鲁棒性测试纳入大语言模型标准评测流程，以更准确评估模型的真实语言理解水平。

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [19] [RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering](https://arxiv.org/abs/2602.17366)
*Yiming Zhang,Siyue Zhang,Junbo Zhao,Chen Zhao*

Main category: cs.CL

TL;DR: 针对大语言模型在长尾知识问答中的局限性，本文提出RPDR数据增强框架，通过合成数据生成、轮转预测筛选易学习样本，显著提升密集检索器在极端长尾类别上的检索性能。


<details>
  <summary>Details</summary>
Motivation: 长尾问答对大语言模型构成挑战，因其难以准确回忆低频知识。检索增强生成虽能缓解此问题，但密集检索器在处理稀有或小众知识时泛化能力不足，现有方法在极端长尾场景下性能有限。

Method: RPDR框架包含三阶段：1) 合成数据生成；2) 采用轮转预测进行数据选择，识别高质量易学习实例；3) 基于筛选实例训练密集检索器。进一步提出动态路由机制，将查询动态分发至专用检索模块以优化性能。

Result: 在PopQA与EntityQuestion两个长尾检索基准上，RPDR相比BM25和Contriver等主流检索器实现显著提升，极端长尾类别上的优势尤为突出。

Conclusion: 研究表明RPDR能有效增强密集检索器处理长尾知识的能力。通过人工分析明确框架的优劣，并提出动态路由机制作为未来改进方向，为长尾问答检索提供新思路。

Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.

</details>


### [20] [The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour](https://arxiv.org/abs/2602.17377)
*Leonidas Zotos,Hedderik van Rijn,Malvina Nissim*

Main category: cs.CL

TL;DR: 研究揭示多选题正确答案在大型语料库中的出现频率显著高于错误答案，采用"可用性启发式"答题策略可比随机猜测提升13.5%-32.9%的正确率，且该结论适用于大语言模型生成题目。


<details>
  <summary>Details</summary>
Motivation: 探究学生在不确定答案时依赖"可用性启发式"（选择最易想起的选项）的答题策略是否有效，并建立计算模型评估多选题选项的认知可用性，以优化学生行为的计算建模。

Method: 开发计算方法，利用维基百科等大型语料库量化选项概念的频率作为认知可用性的操作化指标，并对比分析三个大型题库中正确与错误选项的可用性差异，同时考察大语言模型生成选项的模式。

Result: 跨三个题库发现：1）正确答案可用性显著高于错误选项；2）始终选择最可用选项得分比随机基线高13.5%-32.9%；3）大语言模型生成选项呈现与专家选项相似的可用性分布模式。

Conclusion: 可用性应纳入当前及未来学生行为计算建模的考量，该发现对智能题库设计、自动题目生成及教育评估具有重要实践意义，揭示了语言统计规律与认知启发式的关联。

Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.

</details>


### [21] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 针对现有CDCR数据集在分析多样化新闻报道时的局限性，本文提出将共指链视为话语元素的标注方案，重新标注NewsWCL50和ECB+，支持身份和近身份关系，为新闻领域提供了更平衡的discourse-aware研究数据集。


<details>
  <summary>Details</summary>
Motivation: 现有跨文档共指消解数据集聚焦事件消解且采用狭义共指定义，难以有效分析用词多样的极化新闻语料。

Method: 提出以话语元素为核心的CDCR标注方案，纳入身份与近身份关系；统一重标NewsWCL50和ECB+子集；采用词汇多样性指标和同词干基线评估。

Result: 重标数据集在词汇多样性上介于原ECB+与NewsWCL50之间，对齐效果良好，更具平衡性。

Conclusion: 该标注方案支持新闻领域平衡且话语感知的CDCR研究，能有效捕捉媒体话语的词汇多样性和框架差异。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


### [22] [Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics](https://arxiv.org/abs/2602.17425)
*Sanjeev Kumar,Preethi Jyothi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本研究对比分析BLEU与ChrF++在三种极低资源语言（Magahi、Bhojpuri、Chhattisgarhi）机器翻译评估中的表现，发现BLEU虽分数较低，但提供互补的词汇精确度洞察，增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 极低资源语言机器翻译评估面临挑战，高资源场景下有效的BLEU在数据稀缺时往往失效，现有研究过度依赖ChrF++，忽视BLEU的潜在价值。

Method: 对比分析BLEU与ChrF++对幻觉、重复、复制及变音符号变异的响应，评估大语言模型与神经机器翻译系统在Magahi、Bhojpuri、Chhattisgarhi三种极低资源语言上的输出。

Result: BLEU绝对分数较低，但提供与ChrF++互补的词汇精确度信息，过度依赖ChrF++可能忽略重要评估维度。

Conclusion: 极低资源语言机器翻译评估应综合使用BLEU与ChrF++，利用BLEU的词汇精确度优势提升评估可解释性。

Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.

</details>


### [23] [Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study](https://arxiv.org/abs/2602.17431)
*Dylan Bouchard,Mohit Singh Chauhan,Viren Bajaj,David Skarbrevik*

Main category: cs.CL

TL;DR: 针对闭卷长文本生成中的幻觉检测难题，本文构建了细粒度不确定性量化的三阶段分类框架，形式化了一致性黑盒评分方法族，实验表明声明-响应蕴含关系评分表现优异，声明级评分优于句子级，不确定性感知解码能显著提升事实性。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法虽在短文本幻觉检测中有效，但难以泛化至长文本生成。长文本需要细粒度的评估框架，而现有研究缺乏系统性的理论梳理和方法论指导，导致无法进行公平比较和有效选择。

Method: 提出三维分类体系：1）响应分解策略；2）单元级评分机制（形式化一致性黑盒评分方法族，推广扩展现有方法）；3）响应级聚合方式。系统梳理了声明级与句子级等不同粒度的评估单元。

Result: 多模型多数据集实验发现：1）声明-响应蕴含关系评分持续优于或等同于更复杂的声明级评分器；2）声明级评分总体优于句子级评分；3）不确定性感知解码能高效提升长文本事实准确性。

Conclusion: 该框架厘清了现有方法间的内在联系，建立了公平比较基准，为实践中选择细粒度不确定性量化组件提供了明确指导，促进了长文本幻觉检测技术的发展与应用。

Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.

</details>


### [24] [AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue](https://arxiv.org/abs/2602.17443)
*Adib Sakhawat,Fardeen Sadab,Rakin Shahriar*

Main category: cs.CL

TL;DR: 这篇论文提出了AIDG（对抗性信息推导游戏）框架，通过动态多轮交互评估大语言模型的战略推理能力。研究发现模型在信息防御方面显著优于信息推导，存在350 ELO分的能力不对称，主要瓶颈在于信息动态管理和对话负载下的约束遵守能力。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试无法充分评估大语言模型在真实对话场景中的动态战略推理能力。现有研究缺乏对信息提取（主动推导）与信息维持（状态保持）之间不对称性的系统性考察，难以揭示模型在多轮交互中的真实认知局限。

Method: 设计了AIDG游戏理论框架，包含两个互补任务：AIDG-I通过社交推导游戏测量模型的语用策略能力，AIDG-II通过结构化"20个问题"场景评估约束满足能力。研究对六种前沿大语言模型进行了439场游戏测试，采用ELO评分系统和效应量分析量化性能差异。

Result: 发现显著的能力不对称性：模型在防御端比推导端表现优异350 ELO分（Cohen's d = 5.47）。两个核心瓶颈：1）信息动态方面，确认策略的有效性是盲目推导的7.75倍（p < 0.00001）；2）约束遵守方面，对话负载导致指令遵循能力退化，占全部推导失败的41.3%。

Conclusion: 大语言模型擅长维持局部防御一致性，但缺乏战略探究所需的全局状态追踪能力。研究结果表明，提升模型在复杂对话环境中的长期状态管理和主动推理能力是未来重要方向，静态基准测试需向动态交互评估范式转变。

Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured "20 Questions" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.

</details>


### [25] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: 该研究通过合成NonsenseQA基准测试，发现LLM在多项选择题评估中存在标签位置-少样本提示偏差。提出去偏评估协议：使用统一无序标签并让模型输出完整答案，结合句子相似度模型进行匹配。实验表明该方法将准确率方差降低3倍，性能损失极小，更真实地暴露模型能力。


<details>
  <summary>Details</summary>
Motivation: 当前多项选择题基准测试中，LLM倾向于利用答案位置、标签前缀或少样本提示中的答案分布等评估伪影来作答，而非基于真实理解和推理，导致评估结果不能准确反映模型的实际能力。

Method: 设计去偏评估协议：将选项标签统一替换为无序标签（如标签A、标签B），提示模型直接输出完整答案文本；采用句子相似度模型计算模型输出与标准答案的相似度，减少对提示示例和标签的依赖。

Result: 跨多个基准和模型，协议显著提升对答案排列的鲁棒性，准确率方差降低3倍，标准差减小，平均性能仅轻微下降。消融研究表明该方法比标准评估更鲁棒，能更真实暴露模型能力。

Conclusion: 该方法有效降低了评估伪影影响，提供了更可靠的LLM能力评估框架，能更真实地揭示模型在减少标签和位置偏差后的实际表现，对构建公平评估基准有重要价值。

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [26] [Entropy-Based Data Selection for Language Models](https://arxiv.org/abs/2602.17465)
*Hongming Li,Yang Liu,Chao Huang*

Main category: cs.CL

TL;DR: 针对资源受限的语言模型微调场景，本文提出基于熵的无监督数据选择框架EUDS，通过建立计算高效的数据过滤机制，在减少训练数据量的同时降低计算成本并提升训练效率，在情感分析、主题分类和问答任务上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型微调需要大量计算和数据资源，数据选择技术虽能减少数据量但依赖高计算预算。实际应用中资源有限，且评估数据可用性困难，使得高效数据选择成为关键需求。

Method: 提出基于熵的无监督数据选择框架EUDS，通过熵值估计数据不确定性，建立计算高效的数据过滤机制，在无需额外标注的情况下筛选高质量训练数据。

Result: 在情感分析、主题分类和问答三个任务上的实证实验表明，EUDS能显著降低计算成本，提升训练时间效率，同时减少数据需求，验证了方法的有效性。

Conclusion: EUDS为计算受限场景下的语言模型高效微调提供了创新解决方案，理论分析和实验结果均证实了该方法的有效性，具有重要的实践价值。

Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.

</details>


### [27] [PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions](https://arxiv.org/abs/2602.17467)
*Greta Damo,Stéphane Petiot,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: PEACE 2.0是一个基于检索增强生成(RAG)的新工具，不仅能分析解释仇恨言论，还能生成基于证据的反言论回应，适用于显性和隐性仇恨信息。


<details>
  <summary>Details</summary>
Motivation: 在线平台仇恨言论日益增多带来严峻社会挑战。尽管自然语言处理领域已开发出有效的仇恨言论自动检测方法，但针对仇恨言论的反言论(counter-speech)回应生成仍是未解决的难题。

Method: 提出PEACE 2.0工具，具备三大新功能：利用检索增强生成(RAG)管道将仇恨言论解释基于证据和事实、自动生成基于证据的反言论，以及探索反言论回复的特征。这些能力可集成用于显性和隐性仇恨信息的深度分析和回应生成。

Result: 开发出PEACE 2.0工具，实现了仇恨言论的深入分析、解释和基于证据的反言论自动生成。

Conclusion: PEACE 2.0通过整合检索增强生成技术，为显性和隐性仇恨言论提供了从分析、解释到回应生成的完整解决方案，有望应对在线仇恨言论的社会挑战。

Abstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.

</details>


### [28] [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers](https://arxiv.org/abs/2602.17469)
*Nusrat Jahan Lia,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本研究揭示孟加拉语与英语间跨语言情感对齐的严重安全漏洞。通过基准测试四种Transformer架构，发现压缩模型mDistilBERT存在28.7%情感反转率，系统性误读用户情感意图。研究同时识别"不对称同理心"和"现代偏见"现象，证明当前对齐范式无法保持情感保真度，呼吁构建多元文化基础的评估体系。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统准确理解人类意图且人类信任AI行为是双向对齐的核心，但语言屏障使该闭环在多语言场景中严重断裂。现有对齐研究忽视低资源语言的特殊性，导致跨语言情感认知偏差，威胁人类-AI互惠信任的建立。

Method: 采用基准测试方法，对四种Transformer架构在孟加拉语-英语跨语言情感分析中的表现进行系统性评估，重点关注情感极性反转、情感权重变化及方言处理能力。

Result: mDistilBERT情感反转率达28.7%，存在严重安全风险；模型呈现"不对称同理心"特征，系统性弱化或强化孟加拉语文本情感权重；IndicBERT对正式梵语体孟加拉文(Sadhu)存在57%的误差增幅，暴露现代偏见问题。

Conclusion: 通用压缩策略无法满足低资源语言的情感保真需求，必须建立多元文化基础的对齐框架。建议在基准测试中引入"情感稳定性"指标，明确惩罚低资源和方言语境下的极性反转，促进人类-AI关系的公平共演化。

Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

</details>


### [29] [Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian](https://arxiv.org/abs/2602.17475)
*Pietro Ferrazzi,Mattia Franzin,Alberto Lavelli,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本研究探讨约10亿参数的小规模语言模型能否在保持竞争力的同时执行医疗NLP任务。通过对Llama-3、Gemma-3和Qwen3在20个临床任务上测试，发现监督微调最有效，而少样本提示结合约束解码是优秀的低资源替代方案。Qwen3-1.7B最佳配置甚至超越Qwen3-32B达9.2分，证明小模型可完全替代大模型，并发布了意大利医疗数据集及预训练语料。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽在医疗NLP任务中表现优异，但巨大的计算需求限制了其在真实医疗场景中的部署。本研究旨在探索小参数规模模型（约10亿参数）是否能在保持准确性的同时降低计算成本，解决医疗资源受限环境下的实际应用难题。

Method: 评估Llama-3、Gemma-3和Qwen3三个模型族在命名实体识别、关系抽取、病例报告表填写、问答和论证挖掘等20个临床NLP任务上的表现。系统比较推理时策略（少样本提示、约束解码）和训练时策略（监督微调、持续预训练）的适应性效果。

Result: 监督微调是最有效的适应策略，少样本提示与约束解码组合在低资源场景下表现强劲。小模型能匹配或超越更大基线，最佳配置Qwen3-1.7B比Qwen3-32B平均得分高出9.2点。研究还开源了126万词的急诊科语料和175万词的多源预训练语料。

Conclusion: 小规模语言模型完全能够胜任医疗NLP任务，为医疗场景提供了高效且经济的解决方案。主要贡献包括：验证了小模型的有效性、发布全面的意大利医疗NLP数据集、提供高性能模型，以及开源大规模意大利语预训练语料，推动医疗AI民主化应用。

Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether "small" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.

</details>


### [30] [Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics](https://arxiv.org/abs/2602.17513)
*Baris Karacan,Barbara Di Eugenio,Patrick Thornton*

Main category: cs.CL

TL;DR: 本研究通过创建新的产科带标签数据集，系统评估Transformer监督模型在域内外的表现，并首次与零样本大语言模型对比。结果显示监督模型域外性能显著下降，而零样本模型在纠正幻觉后适应性强，强调了领域特定资源和零样本分割在医疗NLP跨领域应用中的重要性。


<details>
  <summary>Details</summary>
Motivation: 临床自由文本记录蕴含关键患者信息，其章节结构化对临床决策支持和下游NLP任务意义重大。然而，现有分割方法主要在MIMIC-III等公共语料库上训练，医学领域覆盖有限，且缺乏对跨领域泛化能力的系统评估。

Method: 1) 构建去标识化的产科病历章节标注新数据集，补充现有公共语料库的领域覆盖；2) 在MIMIC-III子集（域内）和新产科数据集（域外）上系统评估基于Transformer的监督模型；3) 首次开展监督模型与零样本大语言模型在医疗章节分割任务上的头对头对比实验。

Result: 监督模型在域内表现优异，但在域外场景下性能显著下降；相比之下，零样本模型在有效纠正幻觉后，展现出强大的域外适应能力。

Conclusion: 该研究强调开发领域特定临床资源的重要性，并指出在妥善管理幻觉的前提下，零样本分割是将医疗NLP技术拓展至研究充分语料库之外领域的重要方向。

Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.

</details>


### [31] [Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems](https://arxiv.org/abs/2602.17542)
*Zhangqi Duan,Arnav Kankaria,Dhruv Kartik,Andrew Lan*

Main category: cs.CL

TL;DR: 本文针对开放编程任务中知识组件(KC)级别标签缺失的问题，提出了一种基于大语言模型的自动化框架，通过分析学生代码直接标注KC掌握情况，并结合时序上下文映射机制，实验证明该方法能生成更符合认知理论的学习曲线并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 知识组件(KCs)是学生建模的基础，但在真实场景中，特别是开放编程任务涉及多个KC时，KC级别正确性标签极少存在。简单将题目级别正确性传播到所有关联KC会掩盖部分掌握情况，导致学习曲线拟合不佳，亟需自动化标注方法。

Method: 提出利用大语言模型(LLM)直接从学生代码中标注KC级别正确性的自动化框架。该方法评估每个KC是否被正确应用，并引入时序感知的Code-KC映射机制，使KCs更好地对齐学生个人代码特征。

Result: 实验结果显示，该方法产生的学习曲线更符合认知理论，在幂律练习律和加法因素模型评估下预测性能优于基线方法。人工评估进一步证实LLM标注与专家标注具有高度一致性。

Conclusion: 该框架有效解决了开放编程任务中KC标签稀缺的挑战，为学习分析提供了更精细的学生技能表示，具有实际应用价值。

Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.

</details>


### [32] [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning](https://arxiv.org/abs/2602.17546)
*Jyotin Goel,Souvik Maji,Pratik Mazumder*

Main category: cs.CL

TL;DR: 针对指令微调过程中模型安全性下降的问题，本文提出自适应正则化框架，通过安全评判器或激活风险预测器动态评估风险，约束高风险更新以保持对齐，在多个模型和攻击场景下有效降低攻击成功率且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 指令遵循语言模型在良性微调和对抗性更新下会出现安全性退化，现有防御要么保护有限，要么在安全性与效用间强制权衡。

Method: 提出自适应正则化训练框架，动态响应安全风险。提供两种风险估计方法：基于评判器的安全评论家为训练批次分配危害分数，以及基于激活的风险预测器通过轻量级分类器估计中间层激活中的有害意图。利用风险信号约束高风险更新以保持在安全参考策略附近，低风险更新则进行标准训练。

Result: 预生成激活中的有害意图信号可预测；评判器分数提供高召回率的安全指导；与标准微调相比，自适应正则化在多个模型家族和攻击场景中持续降低攻击成功率，保持下游性能，且无推理时开销。

Conclusion: 本工作展示了在不牺牲效用的前提下保持安全性的原则性机制，为微调过程中的模型对齐提供了有效解决方案。

Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.

</details>


### [33] [Unmasking the Factual-Conceptual Gap in Persian Language Models](https://arxiv.org/abs/2602.17623)
*Alireza Sakhaeirad,Ali Ma'manpoosh,Arshia Hemmat*

Main category: cs.CL

TL;DR: 本文提出DivanBench诊断基准，通过315个关于迷信与习俗的问题，揭示波斯语大模型在文化推理中存在的三重关键缺陷：严重顺从偏差、持续预训练加剧偏差、以及知识应用存在21%性能鸿沟，表明当前模型仅模仿文化模式而未内化底层规则。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语NLP基准虽扩展至语用学和礼貌性，但混淆了“记忆文化事实”与“推理隐含社会规范”的能力。研究动机在于构建一个诊断性基准，专门评估模型对文化习俗（依赖语境且抗拒简单逻辑推导的任意性规则）的真实推理能力，而非单纯的知识检索。

Method: 设计DivanBench诊断基准，包含315个问题，分为三类任务：事实检索、配对场景验证和情境推理。使用该基准评估七个波斯语大模型，重点分析模型在识别适当行为与拒绝违规场景时的表现差异，以及持续波斯语预训练对推理能力的影响。

Result: 发现三大关键失败：1）多数模型存在严重顺从偏差，能正确识别适当行为却无法拒绝明显违规；2）持续波斯语预训练不仅未改善推理，反而放大顺从偏差，常降低模型辨识矛盾的能力；3）所有模型在“检索事实知识”与“在场景中应用知识”之间存在21%的性能差距。

Conclusion: 文化能力无法仅通过扩展单语数据实现。当前模型仅学会模仿文化表层模式，未能内化背后的认知图式。研究结论强调，构建真正文化胜任的AI需要超越数据规模的范式创新，以培养对文化规则深层推理的能力。

Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.

</details>


### [34] [Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking](https://arxiv.org/abs/2602.17653)
*Iskar Deng,Nathalia Xu,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 本研究通过控制合成学习方法，在18种不同差标记系统的语料上训练GPT-2模型，发现模型能模拟人类语言中"标记非常规语义角色"的自然标记方向偏好，但未能复现"标记宾语而非主语"的强宾语偏好，表明不同语言类型学维度可能源于不同底层机制。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究证实语言模型可从合成语料库中习得语序等句法现象的类型学偏好，但语义驱动的差标记系统（DAM）尚未被探索。DAM涉及形态标记与语义显著性的复杂映射关系，探究模型能否学习此类系统，对揭示类型学规律的计算基础具有重要意义。

Method: 采用受控合成学习范式，构建18个实现不同DAM机制的合成语料库，训练GPT-2语言模型。通过极小对立对评估模型泛化能力，系统检验模型对DAM两个类型学维度（标记方向与论元偏好）的习得情况。

Result: 实验结果显示双重分离现象：模型稳定地表现出人类似的自然标记方向偏好，即优先标记语义非常规论元；然而模型完全未能复现人类语言中显著的强宾语偏好，即DAM中显性标记更频繁地指向宾语而非主语。

Conclusion: 该发现表明DAM的不同类型学倾向可能源自不同的认知或计算机制，而非单一原则驱动，为语言类型学理论提供了计算层面的新证据，并揭示了当前语言模型在习得复杂语义-形态映射时的局限性。

Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.

</details>


### [35] [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655)
*Clara Meister,Ahmetcan Yavuz,Pietro Lesci,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文提出了一种基于UnigramLM分词算法的语言识别方法UniLID。该方法学习共享词表上的语言条件性unigram分布，将分词视为语言特定现象，具有数据和计算效率，支持增量添加新语言。在标准基准测试上表现具有竞争力，在低资源设置下样本效率显著提升（每种语言仅需5个标注样本即可超过70%准确率），在细粒度方言识别上取得大幅改进。


<details>
  <summary>Details</summary>
Motivation: 现有的语言识别系统在高资源语言上表现接近完美，但在低资源语言和相近语言设置下仍然脆弱，需要一种更鲁棒且高效的解决方案。

Method: 基于UnigramLM分词算法的概率框架、参数估计技术和推理策略，学习共享词表上的语言条件性unigram分布，将分词视为语言特定现象，支持增量添加新语言而无需重新训练现有模型，并可集成到现有语言模型分词流水线中。

Result: 在标准基准测试上表现具有竞争力；在低资源设置下样本效率显著提升，每种语言仅需5个标注样本即可达到70%以上准确率；在细粒度方言识别上取得大幅改进。

Conclusion: UniLID是一种简单高效的语言识别方法，在低资源语言和方言识别方面表现优异，具有良好的可扩展性和实用性。

Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.

</details>


### [36] [Sink-Aware Pruning for Diffusion Language Models](https://arxiv.org/abs/2602.17664)
*Aidar Myrzakhan,Tianyi Li,Bowei Guo,Shengkun Tang,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 扩散语言模型（DLMs）因迭代去噪导致高推理成本，现有剪枝方法继承自自回归大语言模型并保留注意力汇点，但论文证明该假设对DLMs不成立——汇点位置在生成过程中变化剧烈、呈瞬态且结构重要性低。提出的Sink-Aware Pruning自动识别并剪枝不稳定汇点，无需重训练即在匹配计算下优于基线。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型因迭代去噪过程导致高昂推理成本，需高效剪枝策略。现有剪枝启发式方法继承自自回归大语言模型，将注意力汇点视为稳定全局锚点而保留。但此假设在DLMs中失效：注意力汇点位置在完整生成轨迹中方差显著（主导汇点随时间步剧烈偏移），表明汇点呈瞬态且结构重要性低。

Method: 基于上述观察，提出Sink-Aware Pruning，通过量化分析注意力汇点位置在迭代去噪过程中的变化程度（主导汇点位置随时间步的偏移），自动识别并剪枝扩散语言模型中的不稳定汇点（传统方法通常保留）。

Result: 该方法无需重新训练，在匹配计算量的情况下实现更优质量-效率权衡，显著优于现有强剪枝基线。

Conclusion: 研究挑战了注意力汇点对于扩散语言模型结构必要性的假设，Sink-Aware Pruning通过剪枝不稳定汇点，为DLMs高效推理提供了新方案。

Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [MMCAformer: Macro-Micro Cross-Attention Transformer for Traffic Speed Prediction with Microscopic Connected Vehicle Driving Behavior](https://arxiv.org/abs/2602.16730)
*Lei Han,Mohamed Abdel-Aty,Younggun Kim,Yang-Jun Joo,Zubayer Islam*

Main category: cs.LG

TL;DR: 提出MMCAformer模型，利用交叉注意力机制融合网联车微观驾驶行为与宏观交通特征进行速度预测。在佛罗里达四条高速公路上的实验显示，该模型较基线性能优越，预测精度提升9%以上且不确定性降低10-24%，急刹车与急加速行为是最具影响力的特征，在拥堵低速场景下改善更显著。


<details>
  <summary>Details</summary>
Motivation: 现有交通速度预测研究过度依赖宏观交通流数据，忽视了微观驾驶行为对交通动态的影响。网联车辆(CV)数据提供了丰富的驾驶行为特征，为融合宏微观特征进行速度预测带来了新机遇。

Method: 提出Macro-Micro Cross-Attention Transformer (MMCAformer)模型。采用自注意力机制学习宏观交通流的内在时空依赖关系，使用交叉注意力机制捕获宏观交通状态与微观驾驶行为之间的时空交互作用。模型采用Student-t负对数似然损失函数，同时优化点预测精度和不确定性估计。

Result: 在佛罗里达四条高速公路的实证研究表明：相比仅使用宏观特征的模型，引入微观驾驶行为特征使RMSE、MAE和MAPE分别降低9.0%、6.9%和10.2%，预测区间宽度平均缩减10.1-24.0%；急刹车和急加速频率是最具影响力的微观特征。在拥堵、低速的交通条件下，模型性能改善更为明显。

Conclusion: 融合微观驾驶行为的宏微观融合模型能显著提升速度预测精度并量化预测不确定性，为智能交通的主动式管理提供了更可靠的技术支持，证明了网联车数据在交通预测中的重要价值。

Abstract: Accurate speed prediction is crucial for proactive traffic management to enhance traffic efficiency and safety. Existing studies have primarily relied on aggregated, macroscopic traffic flow data to predict future traffic trends, whereas road traffic dynamics are also influenced by individual, microscopic human driving behaviors. Recent Connected Vehicle (CV) data provide rich driving behavior features, offering new opportunities to incorporate these behavioral insights into speed prediction. To this end, we propose the Macro-Micro Cross-Attention Transformer (MMCAformer) to integrate CV data-based micro driving behavior features with macro traffic features for speed prediction. Specifically, MMCAformer employs self-attention to learn intrinsic dependencies in macro traffic flow and cross-attention to capture spatiotemporal interplays between macro traffic status and micro driving behavior. MMCAformer is optimized with a Student-t negative log-likelihood loss to provide point-wise speed prediction and estimate uncertainty. Experiments on four Florida freeways demonstrate the superior performance of the proposed MMCAformer compared to baselines. Compared with only using macro features, introducing micro driving behavior features not only enhances prediction accuracy (e.g., overall RMSE, MAE, and MAPE reduced by 9.0%, 6.9%, and 10.2%, respectively) but also shrinks model prediction uncertainty (e.g., mean predictive intervals decreased by 10.1-24.0% across the four freeways). Results reveal that hard braking and acceleration frequencies emerge as the most influential features. Such improvements are more pronounced under congested, low-speed traffic conditions.

</details>


### [38] [A Few-Shot LLM Framework for Extreme Day Classification in Electricity Markets](https://arxiv.org/abs/2602.16735)
*Saud Alghumayjan,Ming Yi,Bolun Xu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于大语言模型（LLM）的小样本分类框架，用于预测次日实时电价是否会出现尖峰。通过将电力需求、可再生能源发电、天气预报和近期电价等系统状态信息聚合为统计特征并格式化为自然语言提示，输入LLM进行预测。在德州电力市场的历史数据上，该小样本方法在有限历史数据条件下性能优于SVM和XGBoost等传统监督学习模型，展现出LLM在数据稀缺场景下预测电价尖峰的潜力。


<details>
  <summary>Details</summary>
Motivation: 电力价格尖峰预测对电力市场参与者至关重要，但传统监督学习方法需要大量标注历史数据。在实际应用中，特别是新兴市场或异常事件期间，历史数据往往稀缺。因此，需要开发数据高效的预测方法，能够在有限样本下准确识别电价尖峰。

Method: 提出基于大语言模型的小样本分类框架。首先聚合系统状态信息（包括电力需求、可再生能源发电量、天气预报和近期电价），提取统计特征并将其格式化为自然语言提示。然后将提示与通用指令一同输入LLM，模型输出次日为尖峰日的概率及置信度分数。该方法无需对模型进行微调，直接利用LLM的推理能力进行分类。

Result: 在德州电力市场历史数据上的实验表明，该小样本方法在预测性能上与SVM和XGBoost等传统监督学习模型相当，但在历史数据有限的条件下显著优于后者。这证明LLM能够有效利用少量样本进行电价尖峰分类。

Conclusion: 研究结果表明，大语言模型在数据稀缺场景下可作为电力价格尖峰分类的高效工具。该方法为电力市场预测提供了新的范式，特别适用于历史数据有限的实际应用环境，展现了LLM在能源领域时间序列预测中的广阔应用前景。

Abstract: This paper proposes a few-shot classification framework based on Large Language Models (LLMs) to predict whether the next day will have spikes in real-time electricity prices. The approach aggregates system state information, including electricity demand, renewable generation, weather forecasts, and recent electricity prices, into a set of statistical features that are formatted as natural-language prompts and fed to an LLM along with general instructions. The model then determines the likelihood that the next day would be a spike day and reports a confidence score. Using historical data from the Texas electricity market, we demonstrate that this few-shot approach achieves performance comparable to supervised machine learning models, such as Support Vector Machines and XGBoost, and outperforms the latter two when limited historical data are available. These findings highlight the potential of LLMs as a data-efficient tool for classifying electricity price spikes in settings with scarce data.

</details>


### [39] [Real-time Secondary Crash Likelihood Prediction Excluding Post Primary Crash Features](https://arxiv.org/abs/2602.16739)
*Lei Han,Mohamed Abdel-Aty,Zubayer Islam,Chenzhu Wang*

Main category: cs.LG

TL;DR: 本文提出了一种不依赖事故后特征的混合二次事故预测框架，通过动态时空窗口提取实时交通流和环境特征，结合集成学习和投票机制，在佛罗里达高速公路上实现了91%的准确率和0.952的AUC值，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有二次事故预测方法依赖事故后特征（如事故类型和严重程度），这些特征难以实时获取，限制了实际应用。为提高预测的实时性和实用性，需要开发不依赖事故后特征的预测框架。

Method: 提出混合预测框架，包括：1）设计动态时空窗口从初级事故位置和上游路段提取实时交通流和环境特征；2）构建三个模型：初级事故模型和两个不同对比场景下的二次事故模型；3）采用集成学习策略融合六种机器学习算法；4）使用基于投票的机制综合三个模型的输出。

Result: 在佛罗里达高速公路上的实验表明，该框架能正确识别91%的二次事故，误报率为0.20。ROC曲线下面积从单个模型的0.654、0.744和0.902提升至混合模型的0.952，性能优于以往研究。

Conclusion: 该混合框架不依赖事故后特征，利用实时交通流和环境数据实现了高精度的二次事故预测，为主动交通管理系统提供了有效的工具，具有较好的实际应用价值。

Abstract: Secondary crash likelihood prediction is a critical component of an active traffic management system to mitigate congestion and adverse impacts caused by secondary crashes. However, existing approaches mainly rely on post-crash features (e.g., crash type and severity) that are rarely available in real time, limiting their practical applicability. To address this limitation, we propose a hybrid secondary crash likelihood prediction framework that does not depend on post-crash features. A dynamic spatiotemporal window is designed to extract real-time traffic flow and environmental features from primary crash locations and their upstream segments. The framework includes three models: a primary crash model to estimate the likelihood of secondary crash occurrence, and two secondary crash models to evaluate traffic conditions at crash and upstream segments under different comparative scenarios. An ensemble learning strategy integrating six machine learning algorithms is developed to enhance predictive performance, and a voting-based mechanism combines the outputs of the three models. Experiments on Florida freeways demonstrate that the proposed hybrid framework correctly identifies 91% of secondary crashes with a low false alarm rate of 0.20. The Area Under the ROC Curve improves from 0.654, 0.744, and 0.902 for the individual models to 0.952 for the hybrid model, outperforming previous studies.

</details>


### [40] [Quantifying LLM Attention-Head Stability: Implications for Circuit Universality](https://arxiv.org/abs/2602.16740)
*Karan Bali,Jack Stanley,Praneet Suresh,Danilo Bzdok*

Main category: cs.LG

TL;DR: 本文系统研究了Transformer语言模型中注意力电路的跨实例稳定性，揭示了中层注意力头的不稳定性特征，并发现权重衰减可显著提升稳定性，为AI系统的可解释性和安全监控建立了理论基础。


<details>
  <summary>Details</summary>
Motivation: 尽管近期研究揭示了Transformer中存在可理解的"电路"结构，但这些机制在不同训练实例间的稳定性尚未得到充分验证，这种不确定性严重制约了其在安全关键领域的应用前景和置信度。

Method: 采用系统性实证方法，对多种规模Transformer语言模型进行多次独立训练，逐层量化比较注意力头表征的相似性，并考察不同优化策略对稳定性的影响。

Result: 中层注意力头稳定性最低但表征独特性最高；模型深度增加会加剧中层表征发散；深层不稳定注意力头反而具有更高的功能重要性；权重衰减优化能有效提升注意力头稳定性；残差流具有相对较高的稳定性。

Conclusion: 电路跨实例鲁棒性是实施可扩展监督和白盒监控的必要前提，研究界应更加重视训练稳定性问题，这对构建可信赖的AI系统具有重要意义。

Abstract: In mechanistic interpretability, recent work scrutinizes transformer "circuits" - sparse, mono or multi layer sub computations, that may reflect human understandable functions. Yet, these network circuits are rarely acid-tested for their stability across different instances of the same deep learning architecture. Without this, it remains unclear whether reported circuits emerge universally across labs or turn out to be idiosyncratic to a particular estimation instance, potentially limiting confidence in safety-critical settings. Here, we systematically study stability across-refits in increasingly complex transformer language models of various sizes. We quantify, layer by layer, how similarly attention heads learn representations across independently initialized training runs. Our rigorous experiments show that (1) middle-layer heads are the least stable yet the most representationally distinct; (2) deeper models exhibit stronger mid-depth divergence; (3) unstable heads in deeper layers become more functionally important than their peers from the same layer; (4) applying weight decay optimization substantially improves attention-head stability across random model initializations; and (5) the residual stream is comparatively stable. Our findings establish the cross-instance robustness of circuits as an essential yet underappreciated prerequisite for scalable oversight, drawing contours around possible white-box monitorability of AI systems.

</details>


### [41] [DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning](https://arxiv.org/abs/2602.16742)
*Haoxiang Sun,Lizhen Xu,Bing Zhao,Wotao Yin,Wei Wang,Boyu Yang,Rui Wang,Hu Wei*

Main category: cs.LG

TL;DR: 提出DeepVision-103K数据集以解决RLVR训练数据多样性与覆盖不足的问题，显著提升大模态模型的多模态数学推理与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR数据集多源于小规模人工构建或既有资源重组，导致数据多样性和覆盖面受限，阻碍了大模态模型视觉反思与推理能力的进一步提升。

Method: 构建覆盖K12数学主题、知识点与视觉元素的综合性数据集DeepVision-103K，用于RLVR训练，并通过多模态数学基准与通用推理任务验证模型性能。

Result: 在DeepVision上训练的模型在多模态数学基准测试中表现强劲，且能有效泛化至通用多模态推理任务，展现出更强的视觉感知、反思与推理能力。

Conclusion: DeepVision-103K数据集验证了数据规模与多样性对RLVR的关键作用，为推进多模态推理能力提供了有效资源，数据已开源。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \textbf{DeepVision-103K}, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: \href{https://huggingface.co/datasets/skylenage/DeepVision-103K}{this url}.

</details>


### [42] [PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency](https://arxiv.org/abs/2602.16745)
*Zhangyi Liu,Huaizhi Qu,Xiaowei Yin,He Sun,Yanjun Han,Tianlong Chen,Zhun Deng*

Main category: cs.LG

TL;DR: 该论文提出PETS框架，通过优化轨迹分配实现样本高效的测试时自洽性。核心创新是定义自洽率（与无限预算多数投票的一致性），并借鉴众包理论分别处理离线（问题预先已知）和在线（流式问题）两种场景，在GPQA上实现完美自洽性的同时比均匀分配最多减少75%（离线）和55%（在线）的计算预算。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展通过聚合随机推理轨迹提升模型性能，但如何在有限预算下实现样本高效的测试时自洽性仍是未解挑战。现有方法对轨迹分配缺乏原则性理论指导，导致资源利用效率低下。该论文旨在建立测试时分配的理论基础，通过优化框架使样本高效分配具有可证明的理论保证。

Method: 1. 提出自洽率作为核心度量，定义为与无限预算多数投票的一致性，使分配问题可理论分析；2. 在离线设置中，将推理轨迹建模为众包中的"工人"，利用成熟的众包理论设计高效的基于多数投票的分配算法；3. 在在线流式设置中，基于离线框架提出新方法，动态调整预算以适应问题难度；4. 构建优化框架为两种场景提供理论保证和计算效率。

Result: 实验表明，PETS始终优于均匀分配策略。在GPQA基准上，该框架在两种设置下均达到完美自洽性：离线场景比均匀分配减少高达75%的采样预算，在线场景减少55%预算。同时保持了强大的理论保证和计算效率。

Conclusion: PETS框架为高效测试时自洽性提供了原则性理论基础，通过借鉴众包理论成功解决了轨迹分配问题。该方法显著降低了计算成本，在离线批量处理和在线流式处理场景下均展现出优越性能，为大语言模型的推理优化开辟了新方向。

Abstract: Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.

</details>


### [43] [Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking](https://arxiv.org/abs/2602.16746)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本研究通过几何分析揭示，transformer在模算术任务中的grokking现象源于训练轨迹在低维执行子空间内的约束以及垂直方向的曲率积累，其中曲率增长先于泛化并遵循幂律关系，而正交梯度流是必要非充分条件。


<details>
  <summary>Details</summary>
Motivation: grokking——小算法任务中模型从记忆到泛化的延迟转变——的内在机制至今仍不清楚，亟需从优化动力学角度进行几何刻画。

Method: 采用几何分析方法：对transformer注意力权重轨迹进行PCA以识别低维执行子空间，通过交换子缺陷量化梯度步的非对易性并投影到该子空间，系统测量损失景观的曲率变化，并进行因果干预实验验证机制。

Result: 关键发现：(1)训练轨迹主要局限于低维执行子空间，单一主成分捕获68-83%方差；(2)垂直于子空间方向曲率急剧增长，而轨迹仍保持在该子空间内；(3)曲率增长始终先于泛化，领先时间与grokking时间尺度呈幂律关系；(4)因果干预表明，沿子空间的运动是grokking的必要条件，但人工增加曲率并不充分；(5)正交梯度流是必要但非充分条件。

Conclusion: 这些结果共同支持一个几何理论：grokking反映了模型从亚稳态区域的逃逸，该区域特征为低维约束和横向曲率积累。只有当曲率积累达到临界值时，模型才能突破约束实现泛化，这为理解深度学习中的泛化动力学提供了新的理论框架。

Abstract: Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experiments show that motion along the learned subspace is necessary for grokking, while artificially increasing curvature is insufficient. Together, these results support a geometric account in which grokking reflects escape from a metastable regime characterized by low-dimensional confinement and transverse curvature accumulation. All findings replicate across this learning-rate range, a qualitatively different slow regime (lr=5e-5, wd=0.1, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes. Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose-response across four operations, while artificially boosting curvature defects has no effect.

</details>


### [44] [LiveClin: A Live Clinical Benchmark without Leakage](https://arxiv.org/abs/2602.16747)
*Xidong Wang,Shuqi Guo,Yue Shen,Junying Chen,Jian Wang,Jinjie Gu,Ping Zhang,Lei Liu,Benyou Wang*

Main category: cs.LG

TL;DR: 针对医疗大语言模型评估中存在的数据污染与知识过时问题，本研究提出LiveClin动态基准。该基准基于1407份最新同行评审病例报告和6605个问题，通过239名医生的AI-人工协同工作流构建。评估26个模型发现，最佳模型病例准确率仅35.7%，显著低于人类专家水平，揭示了当前医疗LLM与临床实践间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 静态基准测试因数据污染和知识陈旧导致医疗LLM评估分数虚高，无法准确反映模型在真实临床环境中的能力。这种评估不可靠性严重制约了模型的实际应用，迫切需要能够动态更新、贴合临床实践的评估体系来驱动医疗AI的健康发展。

Method: 构建LiveClin动态基准：1）从当代同行评审文献中筛选病例，每半年更新确保临床时效性；2）组织239名执业医师建立AI-人工协同工作流，将真实病例转化为覆盖完整诊疗路径的多模态复杂场景；3）最终形成包含1407个病例、6605道问题的评估数据集，抵抗数据污染。

Result: 评估结果显示：26个参测模型中，最优模型病例准确率仅为35.7%，表明真实临床场景极具挑战性。人类专家表现显著优于AI模型，其中主任医师准确率最高，主治医师紧随其后，二者均超越大多数模型，印证了医疗LLM与临床专家间存在显著性能鸿沟。

Conclusion: LiveClin为医疗大语言模型提供了一个持续演进、临床落地的评估框架，能够客观揭示模型能力短板，指导研发方向，推动医疗AI向更高可靠性和实用性迈进。基准数据与代码已开源，促进社区共建。

Abstract: The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for approximating real-world clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. In benchmarking against human experts, Chief Physicians achieved the highest accuracy, followed closely by Attending Physicians, with both surpassing most models. LiveClin thus provides a continuously evolving, clinically grounded framework to guide the development of medical LLMs towards closing this gap and achieving greater reliability and real-world utility. Our data and code are publicly available at https://github.com/AQ-MedAI/LiveClin.

</details>


### [45] [Attending to Routers Aids Indoor Wireless Localization](https://arxiv.org/abs/2602.16762)
*Ayush Roy,Tahsin Fuad Hassan,Roshan Ayyalasomayajula,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 该论文提出一种名为"Attention to Routers"的Wi-Fi定位方法，通过引入注意力机制对不同路由器信息进行差异化加权，在公开数据集上相比基准模型准确率提升超过30%。


<details>
  <summary>Details</summary>
Motivation: 传统加权三角测量方法的启发下，解决现有Wi-Fi定位算法在聚合多路由器信息时缺乏适当加权的问题，从而改善模型收敛性和定位精度。

Method: 在标准机器学习定位架构中嵌入注意力层，使每个路由器的贡献获得不同权重，强调各路由器的相关性差异。

Result: 在公开数据集上的评估表明，该方法的准确率比基准架构高出30%以上。

Conclusion: 通过注意力机制加权路由器信息能显著提升无线定位性能，为多源信息融合提供了有效方案。

Abstract: Modern machine learning-based wireless localization using Wi-Fi signals continues to face significant challenges in achieving groundbreaking performance across diverse environments. A major limitation is that most existing algorithms do not appropriately weight the information from different routers during aggregation, resulting in suboptimal convergence and reduced accuracy. Motivated by traditional weighted triangulation methods, this paper introduces the concept of attention to routers, ensuring that each router's contribution is weighted differently when aggregating information from multiple routers for triangulation. We demonstrate, by incorporating attention layers into a standard machine learning localization architecture, that emphasizing the relevance of each router can substantially improve overall performance. We have also shown through evaluation over the open-sourced datasets and demonstrate that Attention to Routers outperforms the benchmark architecture by over 30% in accuracy.

</details>


### [46] [Omitted Variable Bias in Language Models Under Distribution Shift](https://arxiv.org/abs/2602.16784)
*Victoria Lin,Louis-Philippe Morency,Eli Ben-Michael*

Main category: cs.LG

TL;DR: 该论文将语言模型的分布偏移分解为可观测和不可观测成分，识别出遗漏变量偏差问题，并提出一个映射遗漏变量强度至最差泛化性能边界的框架，以改善分布外评估和优化效果。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在分布偏移下表现出脆弱性，现有方法仅处理可观测的分布偏移成分，忽略了不可观测成分导致的遗漏变量偏差，这严重影响了模型评估和优化的可靠性。

Method: 提出一个理论框架，将遗漏变量的强度与语言模型在分布偏移下的最差泛化性能边界直接关联，并利用这些边界进行模型评估和优化。

Result: 实验结果表明，该框架提供了更有原则的分布外性能评估，相比标准方法显著提升了真实分布外性能，并能在目标分布标签可用时推断遗漏变量的强度。

Conclusion: 该研究通过显式处理不可观测的分布偏移成分，有效缓解了遗漏变量偏差问题，为提升语言模型在分布偏移下的鲁棒性提供了理论指导和实践方法。

Abstract: Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify that the resulting omitted variable bias from unobserved variables can compromise both evaluation and optimization in language models. To address this challenge, we introduce a framework that maps the strength of the omitted variables to bounds on the worst-case generalization performance of language models under distribution shift. In empirical experiments, we show that using these bounds directly in language model evaluation and optimization provides more principled measures of out-of-distribution performance, improves true out-of-distribution performance relative to standard distribution shift adjustment methods, and further enables inference about the strength of the omitted variables when target distribution labels are available.

</details>


### [47] [Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency](https://arxiv.org/abs/2602.16787)
*Victoria Lin,Xinnuo Xu,Rachel Lawrence,Risa Ueno,Amit Sharma,Javier Gonzalez,Niranjani Prasad*

Main category: cs.LG

TL;DR: 提出一种无需标注的双反事实一致性(DCC)推理方法，用于评估和提升大语言模型的因果推理能力，通过验证因果干预和反事实预测两个核心要素，可作为无训练成本测试时拒绝采样标准直接提升多模型在推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在反事实问题面前表现脆弱，暴露因果推理缺陷；而现有基于标注的反事实任务数据难以大规模覆盖潜在空间，限制了评估效果。

Method: 双反事实一致性(DCC)：一种轻量级推理时评估方法，无需标注数据，通过验证模型因果干预与反事实预测两大因果推理核心能力，实现因果推理能力的测量与引导。

Result: 评估了多个领先大语言模型在不同推理任务与干预设置下的因果推理能力；验证DCC作为无训练成本测试时拒绝采样标准的有效性，并直接提升多模型家族的推理任务性能。

Conclusion: DCC提供了一种无需标注数据即可评估大语言模型因果推理能力的方法，通过测试时拒绝采样可直接提升模型推理性能，为因果推理研究提供了高效实用的新工具。

Abstract: Despite their strong performance on reasoning benchmarks, large language models (LLMs) have proven brittle when presented with counterfactual questions, suggesting weaknesses in their causal reasoning ability. While recent work has demonstrated that labeled counterfactual tasks can be useful benchmarks of LLMs' causal reasoning, producing such data at the scale required to cover the vast potential space of counterfactuals is limited. In this work, we introduce double counterfactual consistency (DCC), a lightweight inference-time method for measuring and guiding the ability of LLMs to reason causally. Without requiring labeled counterfactual data, DCC verifies a model's ability to execute two important elements of causal reasoning: causal intervention and counterfactual prediction. Using DCC, we evaluate the causal reasoning abilities of various leading LLMs across a range of reasoning tasks and interventions. Moreover, we demonstrate the effectiveness of DCC as a training-free test-time rejection sampling criterion and show that it can directly improve performance on reasoning tasks across multiple model families.

</details>


### [48] [Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models](https://arxiv.org/abs/2602.16793)
*Xingyu Dang,Rohit Agarwal,Rodrigo Porto,Anirudh Goyal,Liam H Fowl,Sanjeev Arora*

Main category: cs.LG

TL;DR: 开发了一种基于猜想提取的低成本IMO数学题求解管道，通过识别"认知井"现象，使用通用模型实现67.1%准确率，成本仅31美元/题，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管定制化和未公开的大规模推理模型在IMO数学推理任务上达到金牌水平，但成本极高（每题约3000美元）。本文旨在开发一种成本极低、仅使用通用现成模型的高性能推理管道。

Method: 基于"认知井"洞察——即求解器-评分器管道中迭代优化收敛到求解器和内部评分器均认为正确的错误解。通过猜想提取技术，从生成解中分离候选引理，并在独立环境中同时验证这些引理及其否定（上下文分离）。

Result: 在IMO-ProofBench Advanced（PB-Adv）基准上，使用Gemini 3.0 Pro模型实现67.1%性能，平均每题成本约31美元。该结果在当时为SOTA，成功率是第二优公共管道的两倍以上，成本仅为后者一小部分。

Conclusion: 该推理管道通过认知井洞察和猜想提取技术，在保持顶尖IMO级性能的同时将成本降低数个数量级，为数学推理提供了可扩展且经济高效的解决方案。

Abstract: In the past year, custom and unreleased math reasoning models reached gold medal performance on the International Mathematical Olympiad (IMO). Similar performance was then reported using large-scale inference on publicly available models but at prohibitive costs (e.g., 3000 USD per problem). In this work, we present an inference pipeline that attains best-in-class performance on IMO-style math problems at an average inference cost orders of magnitude below competing methods while using only general-purpose off-the-shelf models. Our method relies on insights about grader failure in solver-grader pipelines, which we call the Cognitive Well (iterative refinement converging to a wrong solution that the solver as well as the pipeline's internal grader consider to be basically correct). Our pipeline addresses these failure modes through conjecture extraction, wherein candidate lemmas are isolated from generated solutions and independently verified alongside their negations in a fresh environment (context detachment). On IMO-ProofBench Advanced (PB-Adv), our pipeline achieves 67.1 percent performance using Gemini 3.0 Pro with an average cost per question of approximately 31 USD. At the time of evaluation, this represented the state-of-the-art on PB-Adv among both public and unreleased models, and more than doubles the success rate of the next best publicly accessible pipeline, all at a fraction of the cost.

</details>


### [49] [Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning](https://arxiv.org/abs/2602.16796)
*Zifan Wang,Riccardo De Santi,Xiaoyu Mo,Michael M. Zavlanos,Andreas Krause,Karl H. Johansson*

Main category: cs.LG

TL;DR: 提出TFFT方法，基于CVaR分布感知的流模型微调算法，通过变分对偶分解实现高效尾部控制，计算成本与标准微调相当，在文本到图像生成和分子设计中验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有熵正则化微调方法仅最大化期望奖励，无法控制尾部行为；实际部署中下尾部决定可靠性（限制低奖励失败），上尾部促进发现（优先罕见高奖励结果），尾部控制至关重要。

Method: 基于条件风险价值(CVaR)提出TFFT算法，利用CVaR变分对偶形式将其解耦为：轻量级一维阈值优化 + 特定伪奖励的熵正则化微调两阶段流程，分别实现右CVaR（高奖励尾）和左CVaR（低奖励尾）控制。

Result: 计算效率与标准期望微调方法相当，在示例实验、高维文本到图像生成及分子设计任务中验证了有效性。

Conclusion: TFFT为分布感知微调提供了原则性高效解决方案，能有效塑造生成模型尾部行为，平衡可靠性与探索性，适用于多种生成式AI下游任务。

Abstract: Fine-tuning pre-trained diffusion and flow models to optimize downstream utilities is central to real-world deployment. Existing entropy-regularized methods primarily maximize expected reward, providing no mechanism to shape tail behavior. However, tail control is often essential: the lower tail determines reliability by limiting low-reward failures, while the upper tail enables discovery by prioritizing rare, high-reward outcomes. In this work, we present Tail-aware Flow Fine-Tuning (TFFT), a principled and efficient distributional fine-tuning algorithm based on the Conditional Value-at-Risk (CVaR). We address two distinct tail-shaping goals: right-CVaR for seeking novel samples in the high-reward tail and left-CVaR for controlling worst-case samples in the low-reward tail. Unlike prior approaches that rely on non-linear optimization, we leverage the variational dual formulation of CVaR to decompose it into a decoupled two-stage procedure: a lightweight one-dimensional threshold optimization step, and a single entropy-regularized fine-tuning process via a specific pseudo-reward. This decomposition achieves CVaR fine-tuning efficiently with computational cost comparable to standard expected fine-tuning methods. We demonstrate the effectiveness of TFFT across illustrative experiments, high-dimensional text-to-image generation, and molecular design.

</details>


### [50] [HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind](https://arxiv.org/abs/2602.16826)
*Nigel Doering,Rahath Malladi,Arshia Sangwan,David Danks,Tauhidur Rahman*

Main category: cs.LG

TL;DR: 该论文针对现有心智理论方法局限于小规模网格世界的不足，提出HiVAE分层变分架构以扩展ToM推理至现实时空域。在3,185节点的校园导航任务上性能显著提升，但存在潜在表示缺乏心理状态显式grounding的关键局限，作者提出自监督对齐策略并寻求社区反馈。


<details>
  <summary>Details</summary>
Motivation: 现有心智理论(ToM)方法主要聚焦于小型人类可理解的网格世界空间，无法扩展到现实复杂的时空领域，限制了AI系统推断智能体隐藏目标和心理状态的能力。

Method: 受人类认知的信念-欲望-意图结构启发，提出HiVAE三层变分自编码器层次架构，通过分层潜变量建模实现大规模时空域的心智理论推理。

Result: 在3,185个节点的校园导航任务上取得了显著的性能改进，证明了该分层架构在扩展ToM推理到现实场景方面的有效性。

Conclusion: 尽管分层结构提升了预测性能，但学习到的潜在表示缺乏与实际心理状态的显式grounding。为此，作者提出自监督对齐策略，并希望就该方法的改进方向征求研究社区的反馈。

Abstract: Theory of mind (ToM) enables AI systems to infer agents' hidden goals and mental states, but existing approaches focus mainly on small human understandable gridworld spaces. We introduce HiVAE, a hierarchical variational architecture that scales ToM reasoning to realistic spatiotemporal domains. Inspired by the belief-desire-intention structure of human cognition, our three-level VAE hierarchy achieves substantial performance improvements on a 3,185-node campus navigation task. However, we identify a critical limitation: while our hierarchical structure improves prediction, learned latent representations lack explicit grounding to actual mental states. We propose self-supervised alignment strategies and present this work to solicit community feedback on grounding approaches.

</details>


### [51] [VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study](https://arxiv.org/abs/2602.16833)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 本文提出语言化动作掩码(VAM)方法解决大型语言模型强化学习后训练中的探索瓶颈问题。通过在提示中语言化描述动作掩码约束模型输出，并结合迭代动作空间剪枝策略，在国际象棋领域的实验表明VAM在引擎对弈和固定数据集两种训练模式下均能提升学习效率和最终性能，为可控探索提供了实用机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在强化学习后训练中面临探索效率低下的关键瓶颈，稀疏反馈和大规模动作空间容易导致模型过早陷入重复行为模式而崩溃，亟需设计更可控、高效的探索机制。

Method: 1) 语言化动作掩码(VAM)：在提示中用自然语言描述动作掩码，强制模型从被允许的有限动作集合中生成输出；2) 迭代动作空间剪枝：当目标动作未被采样时，从掩码中移除已采样的有效动作并重新采样，重复此过程直到采样到目标动作或耗尽预设预算；3) 在国际象棋领域验证，采用引擎对弈生成状态和固定数据集带验证器评分两种训练模式进行评估。

Result: 在保留的国际象棋谜题和完整对局测试中，以平均百分pawn损失(ACPL)为评估指标，VAM方法相比强基线模型显著提升了学习效率和最终性能。

Conclusion: 语言化动作掩码是一种实用的可控探索机制，能有效解决LLM强化学习后训练中的探索瓶颈问题，为提升模型在复杂决策任务中的表现提供了新思路。

Abstract: Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.

</details>


### [52] [A Residual-Aware Theory of Position Bias in Transformers](https://arxiv.org/abs/2602.16837)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: cs.LG

TL;DR: 该论文揭示了Transformer模型中位置偏置的架构起源，通过引入残差感知的累积注意力rollout理论，证明了残差连接可防止注意力坍缩，并在有限深度下形成U形位置偏置，从而解释了"中间迷失"现象。


<details>
  <summary>Details</summary>
Motivation: Transformer模型存在系统性位置偏置但其架构成因不明。现有理论在无限深度假设下预测注意力会坍缩至首token，这与实际观察相矛盾，亟需新的理论框架解释这一差异。

Method: 提出残差感知的累积注意力rollout理论，在分析中纳入残差连接的作用，通过理论证明揭示有限深度下因果Transformer的位置偏置模式。

Result: 证明在有限深度条件下，因果Transformer会产生U形位置偏置，注意力同时集中于序列首尾token，为"中间迷失"现象提供了基于架构的原则性解释。

Conclusion: 残差连接是阻止注意力坍缩的关键因素，U形位置偏置是Transformer的固有特性，该理论框架深化了对Transformer工作机制的理解，并解释了实际观察到的现象。

Abstract: Transformer models systematically favor certain token positions, yet the architectural origins of this position bias remain poorly understood. Under causal masking at infinite depth, prior theoretical analyses of attention rollout predict an inevitable collapse of attention onto the first token. Such collapse, however, does not occur in practice. We resolve this discrepancy with a residual-aware theory of cumulative attention rollout. By incorporating residual connections, we show that this architectural component prevents collapse under realistic conditions. At finite depth, we prove that causal Transformers induce a U-shaped position bias, with attention concentrating on early and late tokens. This result provides a principled architectural explanation for the Lost-in-the-Middle phenomenon.

</details>


### [53] [Training Large Reasoning Models Efficiently via Progressive Thought Encoding](https://arxiv.org/abs/2602.16839)
*Zeliang Zhang,Xiaodong Liu,Hao Cheng,Hao Sun,Chenliang Xu,Jianfeng Gao*

Main category: cs.LG

TL;DR: 提出渐进式思维编码方法，通过将推理过程编码为固定大小向量表示，使大推理模型在固定缓存下高效训练，在数学基准测试中显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在复杂问题上表现出色，但强化学习训练效率低下，因为自回归解码导致时间和内存消耗巨大。滑动窗口缓存策略虽能限制内存，但会破坏长上下文推理并降低性能。

Method: 渐进式思维编码是一种参数高效微调方法，通过将中间推理过程逐步编码为固定大小的向量表示，避免通过完整缓存回滚进行反向传播，从而减少内存使用并在推理期间保持恒定内存。

Result: 在Qwen2.5-3B/7B-Instruct和DeepSeek-R1-Distill-Llama-8B三个模型上，于六个数学基准测试中，相比LoRA微调平均提升19.3%，相比未微调LRMs平均提升29.9%，在AIME2024/2025上最高提升23.4%准确率。

Conclusion: 该方法不仅提高了推理准确率，还使大推理模型的强化学习训练在现实内存约束下更加高效和可扩展。

Abstract: Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.

</details>


### [54] [What is the Value of Censored Data? An Exact Analysis for the Data-driven Newsvendor](https://arxiv.org/abs/2602.16842)
*Rachitesh Kumar,Omar Mouchtaki*

Main category: cs.LG

TL;DR: 研究具有删失需求数据的离线数据驱动报童问题。通过将无限维非凸优化问题简化为有限维问题，实现了对任意样本量和删失水平下经典库存策略最坏情况遗憾的精确计算。关键发现是：少量高库存水平的目标探索可显著改善删失数据下的最坏情况保证，而仅将销售视为需求的启发式方法会导致严重的性能下降。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动库存研究假设需求完全可观测，但实践中销售数据常在库存水平处被删失，仅记录实际销量而非真实需求。这种信息缺失严重限制了从被动销售数据中学习的能力，导致现有方法应用受限。本研究旨在解决这一关键问题，探索在信息不完整环境下如何有效学习和优化库存决策。

Method: 提出通用程序计算经典数据驱动库存策略在所有可能需求分布下的精确最坏情况遗憾。核心技术创新是将无限维、非凸优化问题转化为等价的有限维优化问题。这种降维方法使得对任意样本规模和删失水平，都能精确刻画各类策略的性能表现。

Result: 1) 针对Kaplan-Meier策略，发现需求删失虽限制被动学习，但在高库存水平进行少量目标探索可显著改善最坏情况保证，实现严重删失下的近最优性能；2) 当POS系统不记录缺货事件仅报告销售时，销售即需求的启发式方法随删失数据累积出现严重性能退化；3) 揭示了销售点信息质量对离线学习能力的决定性影响。

Conclusion: 研究提供了计算删失数据下库存策略精确性能的理论框架，揭示了主动探索在克服信息不完备中的关键作用。实践启示：企业应改进POS系统记录缺货信息，否则需主动实施高库存探索策略以避免性能损失。研究强调了信息质量在数据驱动决策中的核心地位。

Abstract: We study the offline data-driven newsvendor problem with censored demand data. In contrast to prior works where demand is fully observed, we consider the setting where demand is censored at the inventory level and only sales are observed; sales match demand when there is sufficient inventory, and equal the available inventory otherwise. We provide a general procedure to compute the exact worst-case regret of classical data-driven inventory policies, evaluated over all demand distributions. Our main technical result shows that this infinite-dimensional, non-convex optimization problem can be reduced to a finite-dimensional one, enabling an exact characterization of the performance of policies for any sample size and censoring levels. We leverage this reduction to derive sharp insights on the achievable performance of standard inventory policies under demand censoring. In particular, our analysis of the Kaplan-Meier policy shows that while demand censoring fundamentally limits what can be learned from passive sales data, just a small amount of targeted exploration at high inventory levels can substantially improve worst-case guarantees, enabling near-optimal performance even under heavy censoring. In contrast, when the point-of-sale system does not record stockout events and only reports realized sales, a natural and commonly used approach is to treat sales as demand. Our results show that policies based on this sales-as-demand heuristic can suffer severe performance degradation as censored data accumulates, highlighting how the quality of point-of-sale information critically shapes what can, and cannot, be learned offline.

</details>


### [55] [ML-driven detection and reduction of ballast information in multi-modal datasets](https://arxiv.org/abs/2602.16876)
*Yaroslav Solovko*

Main category: cs.LG

TL;DR: 本文提出一种通用的多模态"压舱石"检测与约简框架，通过Ballast Score整合熵、互信息、Lasso、SHAP、PCA、主题建模和嵌入分析等多种信号，在稀疏/半结构化数据中可剪枝超过70%的特征空间，且不影响甚至提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据集中的冗余或低效用信息（称为"压舱石"）会增加数据维度、存储开销和计算成本，却无法提供有意义的分析价值。

Method: 提出一种跨模态的统一框架，针对结构化、半结构化、非结构化和稀疏数据，综合应用熵、互信息、Lasso、SHAP、PCA、主题建模和嵌入分析等技术识别压舱石特征，并创新性地提出Ballast Score量化指标以实现跨模态特征剪枝。

Result: 实验证明在稀疏或半结构化数据中可剪枝超过70%的特征空间，分类性能保持不变或有所提升，训练时间和内存占用显著降低；同时揭示了统计型、语义型和基础设施型等不同的压舱石类别。

Conclusion: 该框架为构建更精简、高效的机器学习流程提供了实用指导，实现了在不牺牲性能的前提下大幅降低计算资源消耗的目标。

Abstract: Modern datasets often contain ballast as redundant or low-utility information that increases dimensionality, storage requirements, and computational cost without contributing meaningful analytical value. This study introduces a generalized, multimodal framework for ballast detection and reduction across structured, semi-structured, unstructured, and sparse data types. Using diverse datasets, entropy, mutual information, Lasso, SHAP, PCA, topic modelling, and embedding analysis are applied to identify and eliminate ballast features. A novel Ballast Score is proposed to integrate these signals into a unified, cross-modal pruning strategy. Experimental results demonstrate that significant portions of the feature space as often exceeding 70% in sparse or semi-structured data, can be pruned with minimal or even improved classification performance, along with substantial reductions in training time and memory footprint. The framework reveals distinct ballast typologies (e.g. statistical, semantic, infrastructural), and offers practical guidance for leaner, more efficient machine learning pipelines.

</details>


### [56] [Construction of a classification model for dementia among Brazilian adults aged 50 and over](https://arxiv.org/abs/2602.16887)
*F. S. Menezes,M. C. F. G. Barretto,E. Q. C. Garcia,T. A. E. Ferreira,J. G. Alvez*

Main category: cs.LG

TL;DR: 本研究基于巴西老龄化纵向研究(ELSI-Brazil)9,412名中老年参与者数据，构建随机森林和多变量逻辑回归模型，识别出文盲、高龄、低握力等痴呆风险因素及教育、生活满意度等保护因素，其中随机森林模型AUC达0.776，为巴西低成本痴呆筛查提供了可行性方案。


<details>
  <summary>Details</summary>
Motivation: 开发针对巴西中老年人群的低成本、可干预的痴呆症预测模型，通过识别易获取的风险和保护因素，帮助早期识别高风险个体，优化初级卫生保健资源配置，为公共卫生政策制定提供依据。

Method: 采用观察性预测模型与横断面设计，利用ELSI-Brazil研究数据，通过神经心理学评估和知情者报告确定痴呆状态，运用随机森林进行变量选择，并构建多变量逻辑回归模型，使用Python实现，重点关注低成本且可改变的影响因素。

Result: 样本痴呆患病率为9.6%。主要风险因素包括：文盲(OR=7.42)、90岁及以上(OR=11.00)、低体重(OR=2.11)、低握力(OR=2.50)、自报黑人(OR=1.47)、缺乏运动(OR=1.61)、听力损失(OR=1.65)、抑郁症状(OR=1.72)；保护因素为：高教育水平(OR=0.44)、生活满意度高(OR=0.72)、就业状态(OR=0.78)。随机森林模型性能显著优于逻辑回归，AUC为0.776，敏感性0.708，特异性0.702，F1分数0.311，G-means 0.705，准确率0.703。

Conclusion: 研究证实了痴呆症的多维度特征，强调可获取因素在识别脆弱人群中的重要性。加强脑健康相关的公共卫生政策，可优化巴西初级护理和痴呆预防的资源分配效率，为制定针对性干预措施提供科学依据。

Abstract: To build a dementia classification model for middle-aged and elderly Brazilians, implemented in Python, combining variable selection and multivariable analysis, using low-cost variables with modification potential. Observational study with a predictive modeling approach using a cross-sectional design, aimed at estimating the chances of developing dementia, using data from the Brazilian Longitudinal Study of Aging (ELSI-Brazil), involving 9,412 participants. Dementia was determined based on neuropsychological assessment and informant-based cognitive function. Analyses were performed using Random Forest (RF) and multivariable logistic regression to estimate the risk of dementia in the middle-aged and elderly populations of Brazil. The prevalence of dementia was 9.6%. The highest odds of dementia were observed in illiterate individuals (Odds Ratio (OR) = 7.42), individuals aged 90 years or older (OR = 11.00), low weight (OR = 2.11), low handgrip strength (OR = 2.50), self-reported black skin color (OR = 1.47), physical inactivity (OR = 1.61), self-reported hearing loss (OR = 1.65), and presence of depressive symptoms (OR = 1.72). Higher education (OR=0.44), greater life satisfaction (OR=0.72), and being employed (OR=0.78) were protective factors. The RF model outperformed logistic regression, achieving an area under the ROC curve of 0.776, with a sensitivity of 0.708, a specificity of 0.702, an F1-score of 0.311, a G-means of 0.705, and an accuracy of 0.703. Conclusion: The findings reinforce the multidimensional nature of dementia and the importance of accessible factors for identifying vulnerable individuals. Strengthening public policies focused on promoting brain health can contribute significantly to the efficient allocation of resources in primary care and dementia prevention in Brazil

</details>


### [57] [Multi-Agent Lipschitz Bandits](https://arxiv.org/abs/2602.16965)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 针对连续Lipschitz结构动作空间下的去中心化多玩家随机老虎机问题（硬碰撞导致零奖励），本文提出首个通信无关的模块化协议，通过最大值导向搜索实现玩家协调并解耦为N个独立单玩家问题，获得总遗憾界$\tilde{O}(T^{(d+1)/(d+2)})$及与T无关的协调成本，达到单玩家最优率。


<details>
  <summary>Details</summary>
Motivation: 多智能体学习中的通信开销和协调成本是实际部署的关键瓶颈。传统方法或需持续通信，或协调成本随时间累积，难以应用于大规模长期系统。本研究旨在突破这一局限，设计通信无关且协调成本与时间尺度无关的高效协作算法。

Method: 采用两阶段模块化设计：第一阶段基于最大值导向搜索机制，识别并分配玩家至不同高价值区域以解决多智能体协调问题；第二阶段将全局问题解耦为N个独立的单玩家Lipschitz老虎机子问题进行并行求解。

Result: 建立近最优遗憾界$\tilde{O}(T^{(d+1)/(d+2)})$，附加一个与时间尺度$T$无关的协调成本，该性能与单玩家Lipschitz老虎机的最优率相匹配，且框架可扩展至一般距离阈值碰撞模型。

Conclusion: 该框架首次为去中心化多玩家连续老虎机问题提供了此类理论保证，为大规模多智能体系统的协作学习提供了可扩展的通信无关解决方案，在理论和应用层面均具有重要意义。

Abstract: We study the decentralized multi-player stochastic bandit problem over a continuous, Lipschitz-structured action space where hard collisions yield zero reward. Our objective is to design a communication-free policy that maximizes collective reward, with coordination costs that are independent of the time horizon $T$. We propose a modular protocol that first solves the multi-agent coordination problem -- identifying and seating players on distinct high-value regions via a novel maxima-directed search -- and then decouples the problem into $N$ independent single-player Lipschitz bandits. We establish a near-optimal regret bound of $\tilde{O}(T^{(d+1)/(d+2)})$ plus a $T$-independent coordination cost, matching the single-player rate. To our knowledge, this is the first framework providing such guarantees, and it extends to general distance-threshold collision models.

</details>


### [58] [A Unified Framework for Locality in Scalable MARL](https://arxiv.org/abs/2602.16966)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 本文提出策略依赖的局部性框架解决MARL维度灾难问题。通过分解策略诱导的相互依赖矩阵，解耦环境与策略的敏感性，推导出更宽松的指数衰减谱条件ρ(E^s + E^a Π(π)) < 1，揭示了平滑策略可抵消强环境耦合性的机理。


<details>
  <summary>Details</summary>
Motivation: 传统MARL面临维度灾难挑战，现有利用价值函数指数衰减性质(EDP)的方法基于最坏情况的环境边界（如动作上确界），忽略了策略本身的正则化效应，导致条件过于保守，无法捕捉策略对局部性的主动塑造作用。

Method: 提出策略依赖的局部性现象，对策略诱导的相互依赖矩阵H^π进行新颖分解，将其解耦为环境状态敏感性E^s、环境动作敏感性E^a和策略状态敏感性Π(π)三部分。该分解揭示了策略平滑性(Π(π)小)可抵消环境强动作耦合性的机制，并基于谱半径理论推导出指数衰减的充分条件。

Result: 推导出一般性谱条件ρ(E^s + E^a Π(π)) < 1来保证指数衰减性，该条件严格优于先前基于范数的条件。进一步分析了具有谱半径保证的局部块坐标策略改进框架，证明了其理论可靠性。

Conclusion: 揭示了局部性-最优性权衡：策略越平滑局部性越强，但可能牺牲最优性。该框架为设计可扩展MARL算法提供了更紧的理论基础，证明策略设计可主动诱导局部性以对抗维度灾难。

Abstract: Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a \emph{policy-dependent} phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, $H^π$, which decouples the environment's sensitivity to state ($E^{\mathrm{s}}$) and action ($E^{\mathrm{a}}$) from the policy's sensitivity to state ($Π(π)$). This decomposition reveals that locality can be induced by a smooth policy (small $Π(π)$) even when the environment is strongly action-coupled, exposing a fundamental locality-optimality tradeoff. We use this framework to derive a general spectral condition $ρ(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) < 1$ for exponential decay, which is strictly tighter than prior norm-based conditions. Finally, we leverage this theory to analyze a provably-sound localized block-coordinate policy improvement framework with guarantees tied directly to this spectral radius.

</details>


### [59] [Early-Warning Signals of Grokking via Loss-Landscape Geometry](https://arxiv.org/abs/2602.16967)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 该研究揭示了在 transformer 训练中出现的“grokking”现象（从记忆到泛化的突变）并非仅限于模运算任务，而是可以通过一种名为“交换子缺陷”的曲率度量来预警。实验表明，在 SCAN 与 Dyck‑1 两个序列学习基准上，交换子缺陷在泛化前显著上升，且其提前时间满足超线性幂律（SCAN 的 α≈1.18，Dyck‑1 的 α≈1.13）。因果干预进一步证实：增强非交换性可加速 grokking（SCAN 提速约 32%，Dyck‑1 约 50%），而抑制正交梯度流则会延迟或阻止 grokking。结果指出，交换子缺陷是一种稳健、架构无关且因果参与的早期警告信号，用于预测 transformer 中的延迟泛化。


<details>
  <summary>Details</summary>
Motivation: 已有研究发现，在模算术任务中，grokking 与低维执行流形上的“限制”有关，但是否适用于更一般的序列学习仍不清楚。本文的动机是检验 grokking 的机制是否具有普遍性，并寻找一种可在不同任务和架构中通用的早期预测指标。

Method: 1. 选取两个序列学习基准：SCAN（组合泛化）和 Dyck‑1（深度预测）。2. 在多种学习率下训练 transformer，记录训练动态。3. 计算“交换子缺陷”——一种基于非交换梯度更新的曲率度量。4. 对权重空间进行主成分分析（PCA），检验谱集中是否普遍出现。5. 进行因果干预：①放大非交换性（如通过梯度裁剪或特定的参数初始化），②抑制正交梯度流（如限制梯度在正交方向的分量），观察对 grokking 时间的影响。

Result: - 在 SCAN 与 Dyck‑1 上，交换子缺陷在泛化前显著上升，且提前时间满足超线性幂律（SCAN α≈1.18，Dyck‑1 α≈1.13），与之前在模算术上的结果一致。- 权重空间 PCA 显示，谱集中并非普遍的先兆；只有交换子缺陷在所有任务和学习率范围内稳定出现。- 因果干预结果：增强非交换性可将 grokking 提前约 32%（SCAN）和约 50%（Dyck‑1）；抑制正交梯度流则显著延迟甚至阻止 grokking。- 三种任务族（模算术、Dyck‑1、SCAN）在因果敏感性上形成连续谱：模算术最刚性、Dyck‑1 最响应、SCAN 居中，但抑制正交梯度流在所有情况下均延缓或阻止 grokking，说明交换子缺陷的必要性是普遍的。

Conclusion: 交换子缺陷是一种稳健、架构无关且因果参与的早期预警信号，可用于预测 transformer 在多种序列学习任务中出现的延迟泛化（grokking）。本研究不仅将 grokking 机制推广到模算术之外的领域，也为理解和干预深度学习模型的泛化行为提供了新的理论工具。

Abstract: Grokking -- the abrupt transition from memorization to generalization after prolonged training -- has been linked to confinement on low-dimensional execution manifolds in modular arithmetic. Whether this mechanism extends beyond arithmetic remains open. We study two sequence-learning benchmarks: SCAN compositional generalization and Dyck-1 depth prediction. Across both tasks and a wide range of learning rates, the commutator defect -- a curvature measure derived from non-commuting gradient updates -- rises well before generalization, with lead times following a superlinear power law (alpha approximately 1.18 for SCAN, approximately 1.13 for Dyck), consistent with prior results on modular arithmetic. Weight-space PCA reveals that spectral concentration is not a universal precursor; the commutator defect is. Causal interventions demonstrate a mechanistic role: amplifying non-commutativity accelerates grokking (roughly 32% on SCAN, roughly 50% on Dyck), while suppressing orthogonal gradient flow delays or prevents it. The three task families form a spectrum of causal sensitivity -- modular arithmetic is rigid, Dyck is responsive, SCAN is intermediate -- yet suppression delays or prevents grokking in all cases, establishing necessity as a universal finding. These results identify the commutator defect as a robust, architecture-agnostic, causally implicated early-warning signal for delayed generalization in transformers.

</details>


### [60] [Fail-Closed Alignment for Large Language Models](https://arxiv.org/abs/2602.16977)
*Zachary Coalson,Beth Sohler,Aiden Gabriel,Sanghyun Hong*

Main category: cs.LG

TL;DR: 本文针对大语言模型对齐中的"失效开放"拒绝机制缺陷，提出"失效封闭"对齐原则，通过渐进式框架迭代识别并消除已学习的拒绝方向，迫使模型在新的独立子空间中重建安全性，从而在保持生成质量和较低计算开销的同时，显著提升对越狱攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的拒绝机制存在结构性缺陷——当通过基于提示的越狱攻击抑制单个主导特征时，会导致对齐失效（即"失效开放"），从而产生不安全输出。

Method: 提出渐进式对齐框架，迭代地识别并消除模型先前学习的拒绝方向，迫使模型在新的、独立的子空间中重建安全机制，形成冗余且因果独立的拒绝路径。

Result: 在四种越狱攻击测试中，该方法实现了最强的整体鲁棒性，同时减轻了过度拒绝现象，保持了生成质量，且仅有较小的计算开销。

Conclusion: 通过机制分析证实，采用该方法训练的模型编码了多个因果独立的拒绝方向，越狱攻击无法同时抑制这些方向，为"失效封闭"对齐作为稳健大语言模型安全性的原则性基础提供了实证支持。

Abstract: We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.

</details>


### [61] [Discovering Universal Activation Directions for PII Leakage in Language Models](https://arxiv.org/abs/2602.16980)
*Leo Marchyok,Zachary Coalson,Sungho Keum,Sooel Son,Sanghyun Hong*

Main category: cs.LG

TL;DR: 本文提出UniLeak框架，通过机制可解释性发现语言模型残差流中的通用激活方向，这些方向在推理时线性添加可跨提示持续增加PII生成概率，显著超越现有提取方法，且对生成质量影响极小，揭示了PII泄露源于表示中潜在信号的叠加。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型虽具有丰富内部结构，但其隐藏状态中隐私敏感行为（如个人身份信息泄露）的表征与调控机制尚不明确，亟需探索其 mechanistic underpinnings。

Method: UniLeak采用机制可解释性方法，在不依赖训练数据或真实PII标签的情况下，仅利用模型自生成文本识别残差流中的模型特异性通用激活方向，这些方向在推理时线性添加即可放大PII生成概率。

Result: 所发现的方向具有跨上下文泛化能力，相比现有基于提示的提取方法显著提升PII泄露概率，同时对生成质量影响极小；该框架在多个模型和数据集上均得到验证。

Conclusion: 研究表明PII泄露本质上是模型表示中潜在信号的叠加现象，这一机制性理解为隐私风险评估与缓解提供了新视角。

Abstract: Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.

</details>


### [62] [Dynamic Delayed Tree Expansion For Improved Multi-Path Speculative Decoding](https://arxiv.org/abs/2602.16994)
*Rahul Thomas,Teo Kitanovski,Micah Goldblum,Arka Pal*

Main category: cs.LG

TL;DR: 本文系统评估了多路径推测解码中的验证算法，发现遍历验证占优而最优传输方法落后。通过分析发现，多令牌收益在草稿树深层更重要，因此提出了延迟树扩展和动态神经选择器，使OT方法首次超越遍历验证，平均吞吐量提升5%。


<details>
  <summary>Details</summary>
Motivation: 虽然已有多种针对i.i.d rollouts的验证算法被提出，但在匹配设置下它们的相对性能尚不明确，限制了多路径推测解码的优化方向选择。

Method: 1) 对多种验证策略在模型族、任务和采样机制上进行系统评估；2) 分析不同方法在草稿树各层的性能差异；3) 提出延迟树扩展策略，延后i.i.d.分支点；4) 开发动态神经选择器，根据草稿和目标模型特征预测OT方法的预期效率。

Result: 1) 遍历验证持续占优，OT方法落后；2) OT方法在根部多令牌接受率高，但深层收益更重要；3) 延迟树扩展保持目标分布并改进根部i.i.d rollouts；4) 神经选择器使OT方法（如SpecInfer）首次超越遍历验证，在广泛场景下实现5%的吞吐量提升。

Conclusion: 通过深入理解草稿树不同层的收益分布特征，结合延迟树扩展和上下文相关的动态选择机制，可以充分发挥OT方法的潜力，为多路径推测解码提供了新的优化范式。

Abstract: Multi-path speculative decoding accelerates lossless sampling from a target model by using a cheaper draft model to generate a draft tree of tokens, and then applies a verification algorithm that accepts a subset of these. While prior work has proposed various verification algorithms for i.i.d rollouts, their relative performance under matched settings remains unclear. In this work, we firstly present a systematic evaluation of verification strategies across model families, tasks, and sampling regimes, and find that Traversal Verification dominates consistently, with OT-based methods lagging far behind. Our analysis uncovers that this occurs because OT-based methods achieve high multi-token acceptance near the root of the draft tree, while multi-token gains are most impactful deeper in the draft tree, where draft and target distributions diverge. Based on this insight, we propose delayed tree expansion, which drafts a partial single path, delaying the i.i.d. branching point. We show that delayed tree expansion preserves the target distribution and improves on root-node i.i.d rollouts. Further, we develop a dynamic neural selector that estimates the expected block efficiency of optimal-transport-based verification methods from draft and target features, enabling context-dependent expansion decisions. Our neural selector allows OT-based methods like SpecInfer to outperform Traversal Verification for the first time, achieving 5% higher average throughput across a wide range of models, datasets, and sampling settings.

</details>


### [63] [Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17009)
*Nikunj Gupta,James Zachary Hare,Jesse Milzman,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: AGP（动作图策略）通过构建"协调上下文"显式建模智能体间动作依赖关系，使智能体在去中心化决策中能基于全局依赖进行条件选择，理论表达能力严格优于完全独立策略，实验性能远超基线（80-95% vs 10-25%成功率）。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习的核心挑战是协调决策——成功不仅依赖个体动作质量，更需跨智能体的动作兼容性以实现行为同步、冲突避免和全局约束满足，现有方法对此建模不足。

Method: 提出动作图策略（AGP），通过构建"协调上下文"显式捕获智能体可用动作间的依赖关系，使智能体决策能条件于全局动作依赖结构，突破完全独立策略的局限性。

Result: 理论证明AGP诱导的联合策略比完全独立策略更具表达能力，且能实现比中心化价值分解方法贪心执行更优的协调动作；实验显示在部分可观测协调任务中达到80-95%成功率（基线仅10-25%），并在多样环境中持续领先。

Conclusion: AGP通过显式建模动作依赖关系，有效解决了MARL中的协调难题，在理论上提供了更强的策略表达能力，在实证中展现出显著的性能优势，为多智能体协调决策提供了有效新范式。

Abstract: Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents' available action choices. It constructs, what we call, \textit{coordination contexts}, that enable agents to condition their decisions on global action dependencies. Theoretically, we show that AGPs induce a strictly more expressive joint policy compared to fully independent policies and can realize coordinated joint actions that are provably more optimal than greedy execution even from centralized value-decomposition methods. Empirically, we show that AGP achieves 80-95\% success on canonical coordination tasks with partial observability and anti-coordination penalties, where other MARL methods reach only 10-25\%. We further demonstrate that AGP consistently outperforms these baselines in diverse multi-agent environments.

</details>


### [64] [Malliavin Calculus as Stochastic Backpropogation](https://arxiv.org/abs/2602.17013)
*Kevin D. Oden*

Main category: cs.LG

TL;DR: 本文通过Malliavin分部积分恒等式建立了路径梯度（重参数化）和分数函数梯度（Malliavin）估计器之间的严格联系，提出了一种基于经验协方差自适应结合的混合估计器，在VAE和合成问题上实现了方差减少，但策略梯度实验显示在非稳态优化环境中存在挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的随机梯度估计方法（如重参数化梯度和得分函数梯度）看似独立，缺乏统一的理论框架。该研究旨在揭示这两种方法的内在联系，并开发一种能够自适应结合两者优势的低方差梯度估计器，以提高随机优化的效率。

Method: 利用Malliavin微积分中的分部积分恒等式，证明路径梯度和分数函数梯度均源自同一数学原理。基于此等价性，构建一个统一的方差感知混合估计器，该估计器利用梯度分量的经验协方差矩阵，自适应地加权组合两种梯度估计，从而在所有无偏线性组合中达到最小方差，并给出闭式有限样本收敛界。

Result: 在CIFAR-10上的变分自编码器（VAE）训练中实现了9%的方差减少，在强耦合合成问题上最高达到35%的方差减少。然而，策略梯度实验表明，在非稳态优化场景中，混合方法面临挑战。

Conclusion: Malliavin微积分为随机梯度估计提供了概念统一且实际可解释的理论框架，明确了混合方法在何时能提供实际效益以及面临哪些固有局限。该工作为开发更高效的随机优化算法提供了重要理论基础。

Abstract: We establish a rigorous connection between pathwise (reparameterization) and score-function (Malliavin) gradient estimators by showing that both arise from the Malliavin integration-by-parts identity. Building on this equivalence, we introduce a unified and variance-aware hybrid estimator that adaptively combines pathwise and Malliavin gradients using their empirical covariance structure. The resulting formulation provides a principled understanding of stochastic backpropagation and achieves minimum variance among all unbiased linear combinations, with closed-form finite-sample convergence bounds. We demonstrate 9% variance reduction on VAEs (CIFAR-10) and up to 35% on strongly-coupled synthetic problems. Exploratory policy gradient experiments reveal that non-stationary optimization landscapes present challenges for the hybrid approach, highlighting important directions for future work. Overall, this work positions Malliavin calculus as a conceptually unifying and practically interpretable framework for stochastic gradient estimation, clarifying when hybrid approaches provide tangible benefits and when they face inherent limitations.

</details>


### [65] [WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning](https://arxiv.org/abs/2602.17025)
*Gagan Mundada,Zihan Huang,Rohan Surana,Sheldon Yu,Jennifer Yuntong Zhang,Xintong Li,Tong Yu,Lina Yao,Jingbo Shang,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: WS-GRPO通过将最终奖励转换为部分轨迹的正确性感知指导，解决了GRPO在复杂推理训练中因过度思考导致的效率低下问题，显著减少推理长度同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO在复杂推理任务上表现有效，但其相对目标函数会导致过度推理和效率低下的问题。现有方法难以控制这种行为，因为长度惩罚难以校准（长推理可能是难题所需），且缺乏除最终答案正确性之外的继续/停止监督信号。

Method: 提出弱监督GRPO（WS-GRPO），通过训练一个仅基于结果正确性的偏好模型，将终端奖励转换为前缀级别的继续/停止指导信号，从而在减少冗余推理的同时保持准确性。

Result: 理论分析和在推理基准测试上的实验表明，WS-GRPO能显著减少推理长度，同时性能与GRPO基线保持竞争力。

Conclusion: WS-GRPO提供了一种有效的弱监督框架，通过利用结果正确性信号生成细粒度的推理过程指导，在保持模型性能的同时大幅提升推理效率。

Abstract: Group Relative Policy Optimization (GRPO) is effective for training language models on complex reasoning. However, since the objective is defined relative to a group of sampled trajectories, extended deliberation can create more chances to realize relative gains, leading to inefficient reasoning and overthinking, and complicating the trade-off between correctness and rollout efficiency. Controlling this behavior is difficult in practice, considering (i) Length penalties are hard to calibrate because longer rollouts may reflect harder problems that require longer reasoning, penalizing tokens risks truncating useful reasoning along with redundant continuation; and (ii) supervision that directly indicates when to continue or stop is typically unavailable beyond final answer correctness. We propose Weakly Supervised GRPO (WS-GRPO), which improves rollout efficiency by converting terminal rewards into correctness-aware guidance over partial trajectories. Unlike global length penalties that are hard to calibrate, WS-GRPO trains a preference model from outcome-only correctness to produce prefix-level signals that indicate when additional continuation is beneficial. Thus, WS-GRPO supplies outcome-derived continue/stop guidance, reducing redundant deliberation while maintaining accuracy. We provide theoretical results and empirically show on reasoning benchmarks that WS-GRPO substantially reduces rollout length while remaining competitive with GRPO baselines.

</details>


### [66] [Transforming Behavioral Neuroscience Discovery with In-Context Learning and AI-Enhanced Tensor Methods](https://arxiv.org/abs/2602.17027)
*Paimon Goulart,Jordan Steinhauser,Dawon Ahn,Kylene Shuler,Edward Korzus,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本文通过AI与行为神经科学家的合作，提出了一种基于上下文学习（ICL）的AI增强科研流水线，结合改进的张量分解模型，用于小鼠恐惧泛化研究。该方法使领域专家无需AI训练背景即可自动化数据处理与模式识别，实验表明其性能显著优于领域标准实践和非ICL基线。


<details>
  <summary>Details</summary>
Motivation: 传统科研流水线复杂、僵化且耗时，领域专家被迫将精力投入流水线调试和数据标注，而非科学发现本身。AI技术有望重构这一范式，让专家专注于结果解读与洞见提取。

Method: 研究团队采用AI专家与行为神经科学家的深度合作模式，将上下文学习（ICL）作为领域专家的操作接口，实现无需模型训练的流程自动化。同时引入新型AI增强型张量分解技术，以更好地从异构实验数据中挖掘行为模式。

Result: 全面实验评估证实，该流水线在性能上超越当前领域标准实践，且显著优于未采用ICL范式的机器学习基线。最终科学发现结果经团队内领域专家验证，确保了发现的有效性。

Conclusion: 本工作验证了ICL作为无缝易用接口在科学发现流水线中的强大潜力，展示了AI增强方法在行为神经科学领域的成功应用，为AI驱动的科学发现提供了可推广的新范式，特别是在精神疾病研究相关的动物行为分析中。

Abstract: Scientific discovery pipelines typically involve complex, rigid, and time-consuming processes, from data preparation to analyzing and interpreting findings. Recent advances in AI have the potential to transform such pipelines in a way that domain experts can focus on interpreting and understanding findings, rather than debugging rigid pipelines or manually annotating data. As part of an active collaboration between data science/AI researchers and behavioral neuroscientists, we showcase an example AI-enhanced pipeline, specifically designed to transform and accelerate the way that the domain experts in the team are able to gain insights out of experimental data. The application at hand is in the domain of behavioral neuroscience, studying fear generalization in mice, an important problem whose progress can advance our understanding of clinically significant and often debilitating conditions such as PTSD (Post-Traumatic Stress Disorder). We identify the emerging paradigm of "In-Context Learning" (ICL) as a suitable interface for domain experts to automate parts of their pipeline without the need for or familiarity with AI model training and fine-tuning, and showcase its remarkable efficacy in data preparation and pattern interpretation. Also, we introduce novel AI-enhancements to tensor decomposition model, which allows for more seamless pattern discovery from the heterogeneous data in our application. We thoroughly evaluate our proposed pipeline experimentally, showcasing its superior performance compared to what is standard practice in the domain, as well as against reasonable ML baselines that do not fall under the ICL paradigm, to ensure that we are not compromising performance in our quest for a seamless and easy-to-use interface for domain experts. Finally, we demonstrate effective discovery, with results validated by the domain experts in the team.

</details>


### [67] [Multi-Objective Alignment of Language Models for Personalized Psychotherapy](https://arxiv.org/abs/2602.16053)
*Mehrab Beikzadeh,Yasaman Asadollah Salmanpour,Ashima Suvarna,Sriram Sankararaman,Matteo Malgaroli,Majid Sarrafzadeh,Saadia Gabriel*

Main category: cs.LG

TL;DR: 本研究针对心理健康服务资源短缺问题，开发了多目标直接偏好优化（MODPO）框架。通过调研335名有心理健康经历的个体，构建包含共情、安全等6个维度的奖励模型，实验证明MODPO在平衡治疗目标方面显著优于单目标方法，且获得临床医生一致认可。


<details>
  <summary>Details</summary>
Motivation: 全球超10亿人受心理健康障碍困扰，但专业服务受限于人力短缺和成本压力。现有AI治疗系统采用独立目标优化，无法同时兼顾患者个性化偏好与临床安全要求，亟需能协调多重治疗目标的对齐方法。

Method: 首先调研335名有心理健康经历的个体，收集治疗维度偏好排序；随后开发基于直接偏好优化（DPO）的多目标对齐框架，训练共情、安全、积极倾听、自我激励改变、信任关系和患者自主性6个奖励模型，并系统对比单目标优化、监督微调和参数合并方法。

Result: MODPO实现最佳平衡：共情度77.6%、安全性62.6%，而单目标优化共情度93.6%但安全性仅47.8%。治疗标准相比通用沟通原则性能提升17.2%。双盲临床评估显示MODPO被一致偏好，且LLM评估者与临床医生的一致性达到医生间信度水平。

Conclusion: 该多目标优化框架有效解决了心理健康AI治疗中目标冲突问题，为开发安全、可靠且符合临床需求的AI辅助治疗系统提供了新方案，具有重要临床应用价值。

Abstract: Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.

</details>


### [68] [Multi-Probe Zero Collision Hash (MPZCH): Mitigating Embedding Collisions and Enhancing Model Freshness in Large-Scale Recommenders](https://arxiv.org/abs/2602.17050)
*Ziliang Zhao,Bi Xue,Emma Lin,Mengjiao Zhou,Kaustubh Vartak,Shakhzod Ali-Zade,Carson Lu,Tao Li,Bin Kuang,Rui Jian,Bin Wen,Dennis van der Staay,Yixin Bao,Eddy Li,Chao Deng,Songbin Liu,Qifan Wang,Kai Ren*

Main category: cs.LG

TL;DR: 本文针对大规模推荐系统中嵌入表哈希冲突问题，提出MPZCH（多探测零冲突哈希）机制。该方法基于线性探测，通过辅助张量和CUDA内核实现可配置探测与主动驱逐策略，有效消除嵌入冲突并防止陈旧嵌入继承，在保持生产效率的同时提升模型性能，已集成至开源TorchRec库。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统唯一ID规模扩张，传统哈希索引的碰撞问题日益严重，不仅导致模型性能下降和个性化质量受损，还会造成陈旧嵌入继承现象，阻碍新特征的有效学习。亟需一种能在生产规模下兼顾效率与质量的碰撞缓解方案。

Method: 提出MPZCH机制，采用线性探测替代传统哈希，利用辅助张量记录槽位状态，通过高性能CUDA内核实现动态探测策略和主动驱逐策略。核心机制包括：合理表尺寸设计以实现零碰撞、过时ID淘汰、槽位重置以避免陈旧嵌入继承。

Result: 在线实验表明：MPZCH在用户嵌入上实现零碰撞率，显著提升项目嵌入新鲜度与质量。性能开销可控，训练QPS和推理延迟与现有方法相当。已在开源TorchRec库中发布。

Conclusion: MPZCH通过线性探测与主动驱逐机制，有效解决了大规模推荐系统的嵌入碰撞与陈旧继承问题，在保持生产效率的前提下显著提升模型质量，为工业界提供了可落地的开源解决方案。

Abstract: Embedding tables are critical components of large-scale recommendation systems, facilitating the efficient mapping of high-cardinality categorical features into dense vector representations. However, as the volume of unique IDs expands, traditional hash-based indexing methods suffer from collisions that degrade model performance and personalization quality. We present Multi-Probe Zero Collision Hash (MPZCH), a novel indexing mechanism based on linear probing that effectively mitigates embedding collisions. With reasonable table sizing, it often eliminates these collisions entirely while maintaining production-scale efficiency. MPZCH utilizes auxiliary tensors and high-performance CUDA kernels to implement configurable probing and active eviction policies. By retiring obsolete IDs and resetting reassigned slots, MPZCH prevents the stale embedding inheritance typical of hash-based methods, ensuring new features learn effectively from scratch. Despite its collision-mitigation overhead, the system maintains training QPS and inference latency comparable to existing methods. Rigorous online experiments demonstrate that MPZCH achieves zero collisions for user embeddings and significantly improves item embedding freshness and quality. The solution has been released within the open-source TorchRec library for the broader community.

</details>


### [69] [Sign Lock-In: Randomly Initialized Weight Signs Persist and Bottleneck Sub-Bit Model Compression](https://arxiv.org/abs/2602.17063)
*Akira Sakai,Yuma Ichikawa*

Main category: cs.LG

TL;DR: 该研究针对子比特模型压缩中符号位瓶颈问题，揭示符号矩阵的随机性主要源于初始化而非训练过程，符号翻转是罕见的近零边界穿越事件。提出符号锁定理论刻画翻转行为的几何尾部特性，并设计间隔初始化与外漂正则化方法，将翻转率降至10⁻³量级，困惑度代价仅约1点。


<details>
  <summary>Details</summary>
Motivation: 在子比特权重压缩中，当幅度信息被极度压缩时，符号位成为固定开销瓶颈。理解符号翻转机制对于突破每权重一位的存储极限至关重要。

Method: 1) 实证分析：在Transformer、CNN和MLP上观察符号矩阵的低秩近似抗性和谱随机性；2) 理论建模：提出符号锁定理论，基于停时分析SGD噪声下的符号翻转动态；3) 算法设计：开发基于间隔的初始化策略和轻量级外漂正则化器。

Result: 1) 符号矩阵谱特性与i.i.d. Rademacher分布不可区分；2) 90%以上权重保持初始化符号；3) 符号翻转受近零邻域重入频率控制，呈几何衰减尾部；4) 所提方法将有效翻转率降至~10⁻³，困惑度上升仅约1个点。

Conclusion: 符号模式的表观随机性实质由初始化决定，训练过程仅引入稀疏翻转。符号锁定理论为子比特压缩提供了机制解释，所提方法显著降低了符号熵，为极低比特率压缩开辟了新路径。

Abstract: Sub-bit model compression seeks storage below one bit per weight; as magnitudes are aggressively compressed, the sign bit becomes a fixed-cost bottleneck. Across Transformers, CNNs, and MLPs, learned sign matrices resist low-rank approximation and are spectrally indistinguishable from an i.i.d. Rademacher baseline. Despite this apparent randomness, most weights retain their initialization signs; flips primarily occur via rare near-zero boundary crossings, suggesting that sign-pattern randomness is largely inherited from initialization. We formalize this behavior with sign lock-in theory, a stopping-time analysis of sign flips under SGD noise. Under bounded updates and a rare re-entry condition into a small neighborhood around zero, the number of effective sign flips exhibits a geometric tail. Building on this mechanism, we introduce a gap-based initialization and a lightweight outward-drift regularizer, reducing the effective flip rate to approximately $10^{-3}$ with only about a one-point increase in perplexity.

</details>


### [70] [Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum](https://arxiv.org/abs/2602.17080)
*Minxin Zhang,Yuxuan Liu,Hayden Scheaffer*

Main category: cs.LG

TL;DR: 本文提出新型优化器NAMO及其对角扩展NAMO-D，首次将正交动量与基于范数的Adam型噪声自适应机制相结合。NAMO通过单自适应步长缩放正交动量，在保持正交性的同时以可忽略的额外成本超越Muon；NAMO-D则通过带截断的对角矩阵实现神经元级噪声适应。理论证明二者均具最优收敛率且能自适应随机梯度噪声，GPT-2预训练实验显示其优于AdamW和Muon基准。


<details>
  <summary>Details</summary>
Motivation: 随机优化需平衡确定性更新方向与随机扰动适应机制。Adam采用自适应矩估计提升稳定性，Muon利用权重矩阵结构的正交动量在大型语言模型训练中表现出色，但缺乏将正交动量与基于范数的噪声自适应进行原则性融合的方法。

Method: 提出NAMO与NAMO-D两种算法。NAMO采用单一自适应步长对正交动量进行缩放，在保持更新正交性的同时改进Muon；NAMO-D则通过带截断项的对角矩阵右乘正交动量，实现神经元级别的噪声自适应，契合海森矩阵近块对角结构特性。

Result: 理论方面，在标准假设下，两种算法在确定性设置中达到最优收敛率，在随机设置中其收敛保证可自适应随机梯度噪声水平。实验方面，GPT-2模型预训练表明NAMO和NAMO-D均优于AdamW和Muon基线，且NAMO-D通过额外截断超参数在保持良态更新方向与细粒度噪声自适应之间取得更好平衡，性能进一步提升。

Conclusion: NAMO与NAMO-D首次实现了正交动量与Adam型噪声自适应的原则性集成，为大规模模型训练提供了高效优化新选择，其中NAMO-D通过灵活的截断机制实现了更优性能。

Abstract: Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining a well-conditioned update direction and leveraging fine-grained noise adaptation.

</details>


### [71] [MeGU: Machine-Guided Unlearning with Target Feature Disentanglement](https://arxiv.org/abs/2602.17088)
*Haoyu Wang,Zhuo Huang,Xiaolong Wang,Bo Han,Zhiwei Lin,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文针对机器学习中"被遗忘权"需求，分析了预训练模型中语义类别在特征层面的纠缠特性，提出了Machine-Guided Unlearning (MeGU)框架。该方法利用多模态大语言模型(MLLM)指导概念感知的重新对齐，通过正负特征噪声对实现目标概念的选择性遗忘，在保持模型效用和彻底删除目标信息之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 随着对训练数据隐私的日益关注，"被遗忘权"成为关键需求，这推动了机器非学习(Machine Unlearning)的发展。然而现有方法面临根本性权衡：激进地删除目标数据会损害在保留数据上的模型效用，而保守策略则无法完全消除目标信息残留。更深层的问题是，预训练过程中学习的语义类别概念在特征-模式层面存在纠缠，这从根本上限制了现有非学习范式的效果。

Method: 提出Machine-Guided Unlearning (MeGU)框架，通过概念感知的重新对齐来指导非学习过程。具体包括：(1)利用多模态大语言模型(MLLM)为目标样本分配语义上有意义的扰动标签，明确确定重新对齐方向；(2)将MLLM估计的类别间概念相似性编码为轻量级转移矩阵以提高效率；(3)引入正负特征噪声对，在微调过程中，负噪声抑制目标特定特征模式，正噪声则强化剩余的关联特征并将其与扰动概念对齐。这种协同设计实现了对目标特定表征的选择性破坏，同时保留共享语义结构。

Result: MeGU实现了可控且选择性的遗忘，有效缓解了欠非学习和过非学习问题。通过概念感知的重新对齐和正负噪声的协同作用，该方法能够在彻底删除目标数据影响的同时，最大程度地保持模型在保留数据上的性能，突破了现有方法在效用-删除程度权衡上的限制。

Conclusion: 本文提出的MeGU框架通过深入分析预训练表征的内在特性，并利用MLLM的语义理解能力，为机器非学习问题提供了新的解决思路。该方法成功打破了传统非学习方法中效用与删除完整性的矛盾，为实现隐私保护下的模型更新奠定了重要基础，展示了MLLM在机器学习安全领域的重要应用价值。

Abstract: The growing concern over training data privacy has elevated the "Right to be Forgotten" into a critical requirement, thereby raising the demand for effective Machine Unlearning. However, existing unlearning approaches commonly suffer from a fundamental trade-off: aggressively erasing the influence of target data often degrades model utility on retained data, while conservative strategies leave residual target information intact. In this work, the intrinsic representation properties learned during model pretraining are analyzed. It is demonstrated that semantic class concepts are entangled at the feature-pattern level, sharing associated features while preserving concept-specific discriminative components. This entanglement fundamentally limits the effectiveness of existing unlearning paradigms. Motivated by this insight, we propose Machine-Guided Unlearning (MeGU), a novel framework that guides unlearning through concept-aware re-alignment. Specifically, Multi-modal Large Language Models (MLLMs) are leveraged to explicitly determine re-alignment directions for target samples by assigning semantically meaningful perturbing labels. To improve efficiency, inter-class conceptual similarities estimated by the MLLM are encoded into a lightweight transition matrix. Furthermore, MeGU introduces a positive-negative feature noise pair to explicitly disentangle target concept influence. During finetuning, the negative noise suppresses target-specific feature patterns, while the positive noise reinforces remaining associated features and aligns them with perturbing concepts. This coordinated design enables selective disruption of target-specific representations while preserving shared semantic structures. As a result, MeGU enables controlled and selective forgetting, effectively mitigating both under-unlearning and over-unlearning.

</details>


### [72] [Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling](https://arxiv.org/abs/2602.17089)
*Xinghao Dong,Huchen Yang,Jin-long Wu*

Main category: cs.LG

TL;DR: 针对扩散模型在随机闭合建模中采样速度慢的问题，本文通过在低维潜空间中引入流匹配方法，实现了单步采样，速度提升达两个数量级，并采用隐式与显式正则化策略保持物理保真度和拓扑结构，显著降低训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽在生成质量和样本多样性方面表现优异，但其采样速度慢的固有缺陷严重制约了在随机闭合模型中的应用。如何在加速采样的同时保持物理保真度并减少对海量训练数据的依赖，是亟待解决的关键问题。

Method: 在二维Kolmogorov流数值算例上系统比较输运基生成模型；采用低维潜空间流匹配实现快速采样；对比联合训练隐式正则化与度量保持(MP)、几何感知(GA)显式正则化两种策略对潜空间畸变的控制效果。

Result: 潜空间流匹配实现单步采样，比迭代扩散方法快达100倍；隐式和显式正则化均能有效保持原动力系统低维流形的关键拓扑信息；所提方法可在不依赖大量训练数据的情况下学习随机闭合模型。

Conclusion: 低维潜空间流匹配结合适当正则化是构建高效随机闭合模型的有前景路径，兼具快速采样、物理保真度和数据效率三重优势，为复杂动力学系统建模提供了新思路。

Abstract: Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints. Besides offering a faster sampling speed, both explicitly and implicitly regularized latent spaces inherit the key topological information from the lower-dimensional manifold of the original complex dynamical system, which enables the learning of stochastic closure models without demanding a huge amount of training data.

</details>


### [73] [FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment](https://arxiv.org/abs/2602.17095)
*Chuiyang Meng,Ming Tang,Vincent W. S. Wong*

Main category: cs.LG

TL;DR: 针对联邦LoRA微调中双矩阵聚合误差和分解漂移问题，本文提出FLoRG框架，采用单一低秩矩阵并聚合其格拉姆矩阵以消除误差与降低通信开销，引入普氏对齐抑制漂移，理论收敛界更紧，实验精度超越5种SOTA且通信开销最高降低2041倍。


<details>
  <summary>Details</summary>
Motivation: 尽管低秩适应(LoRA)等参数高效微调技术能高效适配大型语言模型，联邦学习(FL)也能在不共享私有数据的前提下实现协作微调，但二者结合时存在固有缺陷：LoRA使用的两个独立低秩矩阵在联邦聚合时会产生误差，且即使聚合矩阵乘积，服务器端分解因非唯一性会引入"分解漂移"，导致模型性能下降，亟需新框架解决这些挑战。

Method: FLoRG框架核心方法：(1)使用单一低秩矩阵替代LoRA的双矩阵结构；(2)在服务器端聚合该矩阵的格拉姆矩阵（列向量的内积矩阵），避免直接聚合矩阵乘积带来的分解误差；(3)引入普氏对齐机制，在连续训练轮次间对齐分解后的因子矩阵，减小非唯一分解导致的漂移，确保模型更新方向一致性。

Result: 理论分析证明FLoRG具有收敛保证，且普氏对齐能带来更紧的收敛界；实验方面，在多个LLM微调基准上，FLoRG的下游任务准确率超越5种先进基线方案，通信开销最高可降低2041倍，验证了其在精度和效率上的双重优势。

Conclusion: FLoRG通过单一低秩矩阵设计、格拉姆矩阵聚合与普氏对齐的三重创新，系统性解决了联邦LoRA微调的核心挑战，兼具理论严谨性与实践高效性，为大型语言模型在隐私约束下的高效协作微调开辟了新途径。

Abstract: Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\times$.

</details>


### [74] [Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction](https://arxiv.org/abs/2602.17102)
*Sai Vineeth Kandappareddigari,Santhoshkumar Jagadish,Gauri Verma,Ilhuicamina Contreras,Christopher Dignam,Anmol Srivastava,Benjamin Demers*

Main category: cs.LG

TL;DR: 本文提出了一个无服务器MLOps框架，通过事件驱动管道和托管服务协调完整ML生命周期。以HS编码预测为工业案例，采用自定义文本嵌入编码器和Text-CNN架构，在真实数据上达到98%准确率。框架实现了自动化A/B测试、成本优化、可重复性和SLA遵循，为机器学习工业化提供了可复制的蓝图。


<details>
  <summary>Details</summary>
Motivation: HS编码预测是全球贸易中关键的合规任务，需将非结构化产品描述映射到海关标准编码。产品描述模糊、编码频繁更新导致分类错误，造成货物延误和财务损失。传统方案缺乏可重复性、可审计性和成本效益，亟需一个无需基础设施开销、能快速适应且自动化程度高的MLOps框架来保障SLA并优化运营成本。

Method: 研究设计基于无服务器架构的MLOps框架，采用事件驱动管道和托管服务协调数据摄取、训练、部署、监控和重训练全流程。技术路线包括：开发自定义文本嵌入编码器处理短文本特征；对比多种深度学习架构（重点评估Text-CNN）；通过标准化接口实现模型无关性；集成自动化A/B测试机制用于动态模型选择和安全推广；在实际工业环境中部署HS编码预测系统验证框架可行性。

Result: Text-CNN架构在HS编码预测任务中达到98%准确率。框架成功实现：1）可变负载下的自动扩展和SLA遵循；2）完整的可重复性与可审计性；3）通过自动化A/B测试实现生产环境模型安全部署；4）相比Transformer模型显著降低长期运营成本；5）保持确定分类、可预测延迟和模型可解释性。该方案已在工业场景验证，具备成本效益和可扩展性。

Conclusion: 本工作为基于无服务器架构实现机器学习工业化提供了可复制的实施蓝图。框架在HS编码预测任务中验证了平衡性能与经济性的能力，优先考虑确定性分类、可预测延迟和成本效率。架构设计具有可扩展性，可支持Transformer和LLM推理模式，为企业规模化应用机器学习、优化运营成本提供了完整解决方案。

Abstract: This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.

</details>


### [75] [Online Learning with Improving Agents: Multiclass, Budgeted Agents and Bandit Learners](https://arxiv.org/abs/2602.17103)
*Sajad Ashkezari,Shai Ben-David*

Main category: cs.LG

TL;DR: 本文研究新近提出的"带改进的学习"模型，其中智能体可通过微调其特征值来获得更期望的标签。通过提供刻画该模型在线可学习性的组合维度，并分析多类别设定、强盗反馈设定以及智能体的改进成本等，广泛扩展了先前结果。


<details>
  <summary>Details</summary>
Motivation: 带改进的学习模型捕捉了智能体为获得有利标签而策略性修改特征的现实场景，研究其可学习性对开发能应对策略性行为的机器学习算法至关重要。

Method: 本文采用组合维度理论刻画在线可学习性，并系统拓展至多类别学习、强盗反馈以及智能体改进成本建模等多个维度。

Result: 成功将先前结果扩展至更广泛场景，为带改进的学习模型建立了在线可学习性的完整理论框架，涵盖组合刻画、多类别分析、强盗反馈和成本建模等关键方面。

Conclusion: 该工作通过引入组合维度和分析多种实际场景，显著深化了对带改进学习模型的理论理解，为设计考虑智能体策略性行为的鲁棒学习算法提供了理论基础。

Abstract: We investigate the recently introduced model of learning with improvements, where agents are allowed to make small changes to their feature values to be warranted a more desirable label. We extensively extend previously published results by providing combinatorial dimensions that characterize online learnability in this model, by analyzing the multiclass setup, learnability in a bandit feedback setup, modeling agents' cost for making improvements and more.

</details>


### [76] [i-PhysGaussian: Implicit Physical Simulation for 3D Gaussian Splatting](https://arxiv.org/abs/2602.17117)
*Yicheng Cao,Zhuo Huang,Yu Yao,Yiming Ying,Daoyi Dong,Tongliang Liu*

Main category: cs.LG

TL;DR: 提出i-PhysGaussian框架，通过3D高斯溅射与隐式物质点法耦合，利用牛顿优化和GMRES求解器最小化动量平衡残差，实现物理仿真。相比显式方法，时间步长可提升20倍仍保持稳定，有效解决高刚度材料和复杂动态场景下的精度退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D重建的模拟器采用显式逐步更新，对时间步长敏感且在复杂场景（如高刚度材料或准静态运动）下精度快速下降，制约了工业和工程领域的风险管理应用。

Method: 提出i-PhysGaussian框架，将3D高斯溅射（3DGS）与隐式物质点法（MPM）积分器耦合。采用隐式牛顿型优化结合GMRES求解器，通过最小化动量平衡残差求解步末状态，替代传统显式更新。

Result: 实验表明，i-PhysGaussian相比显式基线可在高达20倍更大时间步长下保持稳定，在复杂动态过渡中仍维持结构完整性和运动平滑性。

Conclusion: 该框架显著降低时间步长敏感性，确保物理一致性，为高刚度材料和复杂场景的物理仿真提供了更稳定高效的解决方案。

Abstract: Physical simulation predicts future states of objects based on material properties and external loads, enabling blueprints for both Industry and Engineering to conduct risk management. Current 3D reconstruction-based simulators typically rely on explicit, step-wise updates, which are sensitive to step time and suffer from rapid accuracy degradation under complicated scenarios, such as high-stiffness materials or quasi-static movement. To address this, we introduce i-PhysGaussian, a framework that couples 3D Gaussian Splatting (3DGS) with an implicit Material Point Method (MPM) integrator. Unlike explicit methods, our solution obtains an end-of-step state by minimizing a momentum-balance residual through implicit Newton-type optimization with a GMRES solver. This formulation significantly reduces time-step sensitivity and ensures physical consistency. Our results demonstrate that i-PhysGaussian maintains stability at up to 20x larger time steps than explicit baselines, preserving structural coherence and smooth motion even in complex dynamic transitions.

</details>


### [77] [TIFO: Time-Invariant Frequency Operator for Stationarity-Aware Representation Learning in Time Series](https://arxiv.org/abs/2602.17122)
*Xihao Piao,Zheng Chen,Lingwei Zhu,Yushun Dong,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 本文针对非平稳时间序列的分布漂移问题，提出频域时间不变算子TIFO，通过跨数据集学习平稳感知权重抑制非平稳成分。在28个预测任务中取得18项第一、6项第二，ETTm2数据集平均MSE提升33.3%-55.3%，计算成本降低60%-70%。


<details>
  <summary>Details</summary>
Motivation: 非平稳时间序列预测中，训练与测试数据的分布差异导致分布漂移问题。现有方法通过移除单样本低阶矩来降低依赖性，但无法捕捉样本间的时变结构和复杂时间动态。

Method: 提出时间不变频率算子(TIFO)，学习整个数据集频域谱的平稳感知权重，增强平稳频率分量并抑制非平稳分量。理论证明傅里叶变换隐含诱导频域特征分解。该方法为即插即用模块，可无缝集成至各类预测模型。

Result: 实验表明，TIFO在28个预测设置中获18个第1名和6个第2名。在ETTm2数据集上，平均MSE提升分别达33.3%和55.3%。相比基线方法，计算成本降低60%-70%，具有强可扩展性。

Conclusion: TIFO通过在频域空间建模时间结构有效缓解分布漂移，是一种高效、可扩展的即插即用解决方案，适用于多样化时间序列预测模型。

Abstract: Nonstationary time series forecasting suffers from the distribution shift issue due to the different distributions that produce the training and test data. Existing methods attempt to alleviate the dependence by, e.g., removing low-order moments from each individual sample. These solutions fail to capture the underlying time-evolving structure across samples and do not model the complex time structure. In this paper, we aim to address the distribution shift in the frequency space by considering all possible time structures. To this end, we propose a Time-Invariant Frequency Operator (TIFO), which learns stationarity-aware weights over the frequency spectrum across the entire dataset. The weight representation highlights stationary frequency components while suppressing non-stationary ones, thereby mitigating the distribution shift issue in time series. To justify our method, we show that the Fourier transform of time series data implicitly induces eigen-decomposition in the frequency space. TIFO is a plug-and-play approach that can be seamlessly integrated into various forecasting models. Experiments demonstrate our method achieves 18 top-1 and 6 top-2 results out of 28 forecasting settings. Notably, it yields 33.3% and 55.3% improvements in average MSE on the ETTm2 dataset. In addition, TIFO reduces computational costs by 60% -70% compared to baseline methods, demonstrating strong scalability across diverse forecasting models.

</details>


### [78] [VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation](https://arxiv.org/abs/2602.17133)
*Linwei Zhai,Han Ding,Mingzhi Lin,Cui Zhao,Fei Wang,Ge Wang,Wang Zhi,Wei Xi*

Main category: cs.LG

TL;DR: 针对VQ-VAE训练不稳定和码本崩溃问题，提出VP-VAE，通过用Metropolis-Hastings采样生成的扰动替代码本，实现表示学习与离散化解耦，训练更稳定且对量化误差鲁棒；进一步推导出轻量级变体FSP，统一解释并改进FSQ量化器。


<details>
  <summary>Details</summary>
Motivation: 传统VQ-VAE由于表示学习与离散码本优化之间的内在耦合，常面临训练不稳定和"码本崩溃"问题。为解决这一根本性缺陷，本文提出一种新范式，旨在通过解耦这两个过程来提升模型训练的稳定性和效果。

Method: 提出VP-VAE框架，其核心思想是将量化操作重新理解为隐空间中的结构化扰动注入。具体实现上，使用Metropolis-Hastings采样生成分布一致且尺度自适应的隐变量扰动，替代传统非可微量化器。基于均匀隐变量假设，进一步推导出轻量级变体FSP，为FSQ类固定量化器提供统一理论解释和实践改进。

Result: 在图像和音频基准测试上的广泛实验表明，VP-VAE和FSP能够显著提高重建保真度，实现更均衡的token使用，同时完全避免了耦合码本训练固有的不稳定性问题。

Conclusion: 本文提出的VP-VAE范式通过隐空间扰动替代显式码本，成功解耦了表示学习与离散化过程，不仅解决了训练稳定性问题，还增强了模型对量化误差的鲁棒性。FSP变体进一步丰富了该理论框架，为固定量化器提供了新的理解视角和实践指导。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental to modern generative modeling, yet they often suffer from training instability and "codebook collapse" due to the inherent coupling of representation learning and discrete codebook optimization. In this paper, we propose VP-VAE (Vector Perturbation VAE), a novel paradigm that decouples representation learning from discretization by eliminating the need for an explicit codebook during training. Our key insight is that, from the neural network's viewpoint, performing quantization primarily manifests as injecting a structured perturbation in latent space. Accordingly, VP-VAE replaces the non-differentiable quantizer with distribution-consistent and scale-adaptive latent perturbations generated via Metropolis--Hastings sampling. This design enables stable training without a codebook while making the model robust to inference-time quantization error. Moreover, under the assumption of approximately uniform latent variables, we derive FSP (Finite Scalar Perturbation), a lightweight variant of VP-VAE that provides a unified theoretical explanation and a practical improvement for FSQ-style fixed quantizers. Extensive experiments on image and audio benchmarks demonstrate that VP-VAE and FSP improve reconstruction fidelity and achieve substantially more balanced token usage, while avoiding the instability inherent to coupled codebook training.

</details>


### [79] [The Anxiety of Influence: Bloom Filters in Transformer Attention Heads](https://arxiv.org/abs/2602.17526)
*Peter Balogh*

Main category: cs.LG

TL;DR: 研究发现 transformer 注意力头中存在专门用于检测 token 是否出现过的"成员测试"机制，形成多分辨率系统，其策略超越经典布隆过滤器，并经过严格验证。


<details>
  <summary>Details</summary>
Motivation: 探究 transformer 注意力头是否承担成员测试功能（判断 token 是否在上下文中出现过），以及这些头采用何种策略，从而理解注意力头的内部工作机制和计算角色。

Method: 在四个语言模型（GPT-2 small/medium/large 和 Pythia-160M）中识别候选注意力头，通过混淆变量控制、消融实验和与布隆过滤器理论公式对比，分析其成员测试行为和容量特征。

Result: 识别出三个真正的成员测试头，形成早期层（0-1层）的多分辨率系统：两个高精度头（L0H1和L0H5）假阳性率极低（0-4%），容量远超经典布隆过滤器；一个头（L1H11）呈现典型的布隆过滤器容量曲线（约5比特容量）；另一个候选头（L3H0）经控制实验被重新分类为前缀注意力头。这些头具有距离敏感性，可泛化至任意重复 token 类型，且消融实验表明它们同时参与重复和 novel token 处理。

Conclusion: 成员测试是注意力头的一种真实且功能明确的角色，构成独立于归纳和前序 token 注意力的分类。严格的验证过程（包括排除假阳性）反而强化了这一结论，揭示了 transformer 早期层存在 sophisticated 的成员测试机制。

Abstract: Some transformer attention heads appear to function as membership testers, dedicating themselves to answering the question "has this token appeared before in the context?" We identify these heads across four language models (GPT-2 small, medium, and large; Pythia-160M) and show that they form a spectrum of membership-testing strategies. Two heads (L0H1 and L0H5 in GPT-2 small) function as high-precision membership filters with false positive rates of 0-4\% even at 180 unique context tokens -- well above the $d_\text{head} = 64$ bit capacity of a classical Bloom filter. A third head (L1H11) shows the classic Bloom filter capacity curve: its false positive rate follows the theoretical formula $p \approx (1 - e^{-kn/m})^k$ with $R^2 = 1.0$ and fitted capacity $m \approx 5$ bits, saturating by $n \approx 20$ unique tokens. A fourth head initially identified as a Bloom filter (L3H0) was reclassified as a general prefix-attention head after confound controls revealed its apparent capacity curve was a sequence-length artifact. Together, the three genuine membership-testing heads form a multi-resolution system concentrated in early layers (0-1), taxonomically distinct from induction and previous-token heads, with false positive rates that decay monotonically with embedding distance -- consistent with distance-sensitive Bloom filters. These heads generalize broadly: they respond to any repeated token type, not just repeated names, with 43\% higher generalization than duplicate-token-only heads. Ablation reveals these heads contribute to both repeated and novel token processing, indicating that membership testing coexists with broader computational roles. The reclassification of L3H0 through confound controls strengthens rather than weakens the case: the surviving heads withstand the scrutiny that eliminated a false positive in our own analysis.

</details>


### [80] [TimeOmni-VL: Unified Models for Time Series Understanding and Generation](https://arxiv.org/abs/2602.17149)
*Tong Guan,Sheng Pan,Johan Barthelemy,Zhao Li,Yujun Cai,Cesare Alippi,Ming Jin,Shirui Pan*

Main category: cs.LG

TL;DR: 本文提出TimeOmni-VL，首个视觉为中心的多模态时间序列统一框架，通过双向保真映射和理解引导生成机制，弥合时间序列建模中数值生成与语义理解之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 时间序列建模存在数值生成与语义理解严重割裂的问题：生成模型依赖表层模式匹配，理解模型难以输出高保真数值。视觉领域统一多模态模型已成功桥接此类差距，但其在时间序列领域的潜力尚未开发。

Method: 提出两大核心创新：(1) 保真双向映射(Bi-TSI)实现时间序列与图像间的近无损转换；(2) 构建TSUMM-Suite数据集(含6项理解任务与2项生成任务)，通过校准思维链首次将时间序列理解作为显式控制信号来引导高保真生成。

Result: 实验证实该统一框架在语义理解与数值精度两方面均取得显著提升。

Conclusion: 确立了多模态时间序列建模的新前沿，首次实现理解与生成的协同优化。

Abstract: Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output. Although unified multimodal models (UMMs) have bridged this gap in vision, their potential for time series remains untapped. We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. With a calibrated Chain-of-Thought, TimeOmni-VL is the first to leverage time series understanding as an explicit control signal for high-fidelity generation. Experiments confirm that this unified approach significantly improves both semantic understanding and numerical precision, establishing a new frontier for multimodal time series modeling.

</details>


### [81] [Powering Up Zeroth-Order Training via Subspace Gradient Orthogonalization](https://arxiv.org/abs/2602.17155)
*Yicheng Lang,Changsheng Wang,Yihua Zhang,Mingyi Hong,Zheng Zhang,Wotao Yin,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出ZO-Muon，一种基于子空间梯度正交化的零阶优化方法，通过融合低秩子空间投影与Muon光谱正交化，解决了准确性与查询效率的根本矛盾，在LLM和ViT上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 零阶优化虽为大型模型微调提供了内存高效的梯度替代方案，但其准确性与查询效率之间存在根本性张力：精确梯度估计需要大量函数查询，而查询效率的提升又会导致估计方差增大，严重限制了其实际应用效果。

Method: 提出统一的子空间梯度正交化框架：(i) 利用模型更新的低秩结构，通过子空间投影降低梯度估计方差；(ii) 引入Muon风格的光谱优化，对含噪声的ZO梯度进行正交化处理以提取有效光谱信息。将该框架实例化为ZO-Muon方法，可解释为零阶设置下的低秩Muon优化器。

Result: 在LLM微调中，相比MeZO基线，ZO-Muon仅需24.7%的查询量即可达到相同SST-2性能；在ViT-B/CIFAR-100微调上，准确率提升25.1%。实验证实该方法在收敛速度和效率-准确性权衡方面均实现显著改进。

Conclusion: 本研究通过统一子空间投影与光谱正交化两个互补原则，成功缓解了ZO优化的核心矛盾。ZO-Muon不仅为大规模模型的无梯度微调提供了更高效的解决方案，也拓展了零阶优化的理论边界，对资源受限场景具有重要应用价值。

Abstract: Zeroth-order (ZO) optimization provides a gradient-free alternative to first-order (FO) methods by estimating gradients via finite differences of function evaluations, and has recently emerged as a memory-efficient paradigm for fine-tuning large-scale models by avoiding backpropagation. However, ZO optimization has a fundamental tension between accuracy and query efficiency. In this work, we show that ZO optimization can be substantially improved by unifying two complementary principles: (i) a projection-based subspace view that reduces gradient estimation variance by exploiting the intrinsic low-rank structure of model updates, and (ii) Muon-style spectral optimization that applies gradient orthogonalization to extract informative spectral structure from noisy ZO gradients. These findings form a unified framework of subspace gradient orthogonalization, which we instantiate in a new method, ZO-Muon, admitting a natural interpretation as a low-rank Muon optimizer in the ZO setting. Extensive experiments on large language models (LLMs) and vision transformers (ViTs) demonstrate that ZO-Muon significantly accelerates convergence and achieves a win-win improvement in accuracy and query/runtime efficiency. Notably, compared to the popular MeZO baseline, ZO-Muon requires only 24.7% of the queries to reach the same SST-2 performance for LLM fine-tuning, and improves accuracy by 25.1% on ViT-B fine-tuning on CIFAR-100.

</details>


### [82] [In-Context Learning in Linear vs. Quadratic Attention Models: An Empirical Study on Regression Tasks](https://arxiv.org/abs/2602.17171)
*Ayush Goel,Arjun Kohli,Sarvagya Somvanshi*

Main category: cs.LG

TL;DR: 本文通过实证研究对比了Transformer和线性注意力模型在简单线性回归任务上的上下文学习能力，发现两者存在相似性和差异性，且模型深度对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明Transformer和线性注意力模型都能在简单函数类（如线性回归）上执行上下文学习，但两者在ICL行为上的具体差异尚不明确。本研究旨在系统性地比较这两种注意力机制，以理解它们在ICL能力上的相似点和局限性。

Method: 在Garg等人提出的标准线性回归任务上，对两种架构进行了实证评估。主要考察指标包括学习质量（均方误差MSE）、收敛速度、泛化行为，以及模型深度变化对ICL性能的影响。

Result: 研究发现两种注意力机制在线性回归任务上既有相似表现，也存在明显差异。具体结果揭示了线性注意力相对于二次注意力的优势和限制，并量化了模型深度对上下文学习性能的影响。

Conclusion: 该研究为理解不同注意力机制在上下文学习中的能力提供了经验性洞察，表明线性注意力在某些场景下可以作为Transformer的有效替代方案，但其性能特征存在特定限制。

Abstract: Recent work has demonstrated that transformers and linear attention models can perform in-context learning (ICL) on simple function classes, such as linear regression. In this paper, we empirically study how these two attention mechanisms differ in their ICL behavior on the canonical linear-regression task of Garg et al. We evaluate learning quality (MSE), convergence, and generalization behavior of each architecture. We also analyze how increasing model depth affects ICL performance. Our results illustrate both the similarities and limitations of linear attention relative to quadratic attention in this setting.

</details>


### [83] [Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting](https://arxiv.org/abs/2602.17645)
*Xiaohan Zhao,Zhaoyi Li,Yaxin Luo,Jiacheng Cui,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 本文提出M-Attack-V2，改进LVLM黑盒对抗攻击。针对M-Attack局部裁剪匹配导致的梯度高方差问题，设计多裁剪对齐(MCA)平均梯度、辅助目标对齐(ATA)构建平滑流形、Patch Momentum重放历史梯度，结合优化补丁尺寸集成(PE+)。在Claude-4.0、Gemini-2.5-Pro、GPT-5上攻击成功率分别提升至30%、97%、100%，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LVLM黑盒攻击因缺乏梯度和复杂多模态边界而困难。M-Attack虽有效但产生高方差、近正交梯度，destabilizing优化，原因在于ViT平移敏感性和源-目标裁剪结构不对称，亟需稳定且可迁移的攻击框架。

Method: 将局部匹配重构为源变换与目标语义的非对称期望：1) MCA模块每轮采样多局部视图平均梯度降方差；2) ATA模块用语义相关辅助集替代目标增强，平滑目标流形；3) Patch Momentum重放历史裁剪梯度；4) PE+优化补丁尺寸集成强化可迁移方向。

Result: 在三大LVLM上攻击成功率大幅提升：Claude-4.0从8%→30%，Gemini-2.5-Pro从83%→97%，GPT-5从98%→100%，全面超越先前黑盒攻击。代码已开源。

Conclusion: M-Attack-V2通过梯度去噪和优化局部匹配策略，有效稳定LVLM黑盒对抗攻击优化过程，显著提升跨模型可迁移性，为评估LVLM鲁棒性提供了更强大工具。

Abstract: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.

</details>


### [84] [Continual uncertainty learning](https://arxiv.org/abs/2602.17174)
*Heisei Yonezawa,Ansei Yonezawa,Itsuro Kajiwara*

Main category: cs.LG

TL;DR: 本文针对多源不确定性机械系统的鲁棒控制问题，提出一种基于课程学习的持续学习框架。该方法通过将复杂问题分解为顺序执行的持续学习任务，并结合模型基控制器加速收敛，成功应用于汽车动力系统主动振动控制，实现了鲁棒性和仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 机械系统在多源不确定性（如非线性动力学和工况变化）下的鲁棒控制是一个根本性挑战。尽管深度强化学习（DRL）与域随机化在缓解仿真与现实差距方面显示出潜力，但同时处理所有不确定性源通常会导致次优策略和较差的学习效率。因此，需要一种更有效的学习方法来处理多源不确定性问题。

Method: 研究构建了一个基于课程学习的持续学习框架。核心思想是将具有多源不确定性的复杂控制问题分解为一系列顺序执行的持续学习任务。原始系统被扩展为一组有限的系统，其动态不确定性随着学习进程逐步扩展和多样化。策略在整个与不同不确定性配置相关联的系统集合之间稳定更新，避免灾难性遗忘。为确保学习效率，将模型基控制器（MBC）纳入学习过程，以保证系统集合间的共享基线性能并加速收敛。这种残差学习方案便于DRL智能体针对每个不确定性进行特定优化，从而提高样本效率。

Result: 该方法成功应用于汽车动力系统主动振动控制器的设计。验证结果表明，所得到的控制器能够有效应对结构非线性和动态变化，实现了成功的仿真到现实迁移，证明了该框架在处理多源不确定性鲁棒控制问题中的有效性。

Conclusion: 本研究提出的基于课程学习的持续学习框架为多源不确定性机械系统的鲁棒控制问题提供了一种有效解决方案。通过顺序处理不确定性源、结合模型基控制器加速学习，该方法不仅避免了灾难性遗忘，还提高了学习效率和策略质量。在汽车动力系统主动振动控制这一实际应用中的成功验证表明，该框架具有广阔的工业应用前景。

Abstract: Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting. To ensure learning efficiency, we jointly incorporate a model-based controller (MBC), which guarantees a shared baseline performance across the plant sets, into the learning process to accelerate the convergence. This residual learning scheme facilitates task-specific optimization of the DRL agent for each uncertainty, thereby enhancing sample efficiency. As a practical industrial application, this study applies the proposed method to designing an active vibration controller for automotive powertrains. We verified that the resulting controller is robust against structural nonlinearities and dynamic variations, realizing successful sim-to-real transfer.

</details>


### [85] [SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch](https://arxiv.org/abs/2602.17206)
*Ron Shapira Weber,Oren Freifeld*

Main category: cs.LG

TL;DR: 本文介绍了一个名为softdtw-cuda-torch的开源PyTorch库，用于在GPU上高效计算Soft Dynamic Time Warping (SoftDTW)。该实现解决了现有GPU实现的三个关键限制：序列长度上限1024、小平滑参数下反向传播数值不稳定，以及中间距离张量导致的过高内存消耗。通过分块反对角线核执行、对数空间反向传播和融合距离计算三项创新技术，该库支持任意序列长度、完整PyTorch自动微分集成和Soft-DTW重心计算，内存占用相比之前工作减少高达98%。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速的SoftDTW实现存在三大瓶颈：一是序列长度被硬性限制在1024以内，无法满足长序列处理需求；二是当平滑参数较小时，反向传播过程容易出现数值不稳定和浮点溢出问题；三是计算过程中会生成O(BNM)规模的中间成对距离张量，导致GPU内存消耗过大，无法处理大规模批量数据。这些限制严重阻碍了SoftDTW在长序列和大规模应用中的部署效率。

Method: 作者提出了三项核心技术改进：第一，采用分块反对角线核执行策略，通过将计算任务分块处理，彻底消除了序列长度约束，支持任意长度的序列对齐；第二，设计对数空间反向传播算法，将计算转换到对数域进行，有效防止了浮点溢出，显著提升了数值稳定性；第三，实现融合距离计算模式，将距离计算与SoftDTW主计算过程融合，避免了O(BNM)中间张量的物化，从根源上解决了内存瓶颈。

Result: 实验结果表明，该实现取得了显著性能提升：内存使用量相比现有方案降低最高达98%，彻底突破了序列长度限制，同时保持了完整的PyTorch自动微分兼容性，并支持Soft-DTW重心计算功能。代码已开源发布，为时间序列分析社区提供了高效稳定的工具。

Conclusion: softdtw-cuda-torch库通过创新的分块计算、对数空间运算和融合计算架构，系统性地解决了SoftDTW的GPU实现中的关键性能瓶颈，实现了内存效率、数值稳定性和计算灵活性的统一，为大规模时间序列分析应用提供了强有力的基础设施。

Abstract: We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTorch autograd integration, and Soft-DTW Barycenter computation. Code is available at https://github.com/BGU-CS-VIL/sdtw-cuda-torch.

</details>


### [86] [Structured Prototype-Guided Adaptation for EEG Foundation Models](https://arxiv.org/abs/2602.17251)
*Jingying Ma,Feng Wu,Yucheng Xing,Qika Lin,Tianyu Liu,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: 针对有限监督下EEG基础模型泛化性能差的问题，本文揭示其源于监督信号与模型参数空间的结构失配，提出SCOPE框架：通过几何正则化原型生成置信度感知伪标签，并利用条件适配器实现高效微调，在跨被试低资源场景下取得显著效果。


<details>
  <summary>Details</summary>
Motivation: EEG基础模型在完全微调时表现强劲，但在临床常见的有限被试监督条件下泛化能力急剧下降。现有方法未能充分解决监督信号与模型高可塑性之间的结构失配问题，这限制了其在真实场景中的应用。

Method: SCOPE采用两阶段流水线：(1) 利用几何正则化学习任务先验，构建平衡类级原型，并基于原型一致性生成置信度感知伪标签过滤不可靠信号；(2) 设计ProAdapter，在冻结基础模型上通过轻量级适配器实现原型条件化的参数高效微调。

Result: 在三个EEG任务和五个基础模型上的实验表明，SCOPE在标签受限的跨被试设置中持续实现最优性能，同时保持计算效率。

Conclusion: SCOPE框架通过结构化原型引导的置信度感知监督与适配器微调，有效缓解了有限监督下的泛化问题，为临床低资源EEG分析提供了可扩展的解决方案。

Abstract: Electroencephalography (EEG) foundation models (EFMs) have achieved strong performance under full fine-tuning but exhibit poor generalization when subject-level supervision is limited, a common constraint in real-world clinical settings. We show that this failure stems not merely from limited supervision, but from a structural mismatch between noisy, limited supervision and the highly plastic parameter space of EFMs. To address this challenge, we propose SCOPE, a Structured COnfidence-aware Prototype-guided adaptation framework for EFM fine-tuning. SCOPE follows a two-stage pipeline. In the first stage, we construct reliable external supervision by learning geometry-regularized task priors, constructing balanced class-level prototypes over the resulting embeddings, and producing confidence-aware pseudo-labels from their agreement to filter unreliable signals on unlabeled data. In the second stage, we introduce ProAdapter, which adapts frozen EEG foundation models via a lightweight adapter conditioned on the structured prototypes. Experiments across three EEG tasks and five foundation model backbones demonstrate that SCOPE consistently achieves strong performance and efficiency under label-limited cross-subject settings.

</details>


### [87] [Learning a Latent Pulse Shape Interface for Photoinjector Laser Systems](https://arxiv.org/abs/2602.17263)
*Alexander Klemps,Denis Ilia,Pradeep Kr. Banerjee,Ye Chen,Henrik Tünnermann,Nihat Ay*

Main category: cs.LG

TL;DR: 提出基于Wasserstein自编码器的生成式框架，学习自由电子激光光电注入器中纵向激光脉冲与束流动力学的可微潜在接口，以高效探索设计空间，降低暴力模拟的计算成本。


<details>
  <summary>Details</summary>
Motivation: 控制自由电子激光光电注入器的纵向激光脉冲形状可优化电子束质量，但暴力脉冲传播模拟的高昂成本限制了广阔设计空间的系统探索。

Method: 采用Wasserstein自编码器构建生成式模型，学习脉冲整形与下游束流动力学之间的可微潜在空间表示。

Result: 所学习的潜在空间连续、可解释且重构保真度高；高阶高斯等脉冲族呈连贯轨迹；标准化脉冲时长在潜在空间中与能量相关；主成分分析与高斯混合模型显示良好几何结构，支持线性插值实现脉冲类型平滑过渡；模型可从模拟数据推广至实验测量，准确重构并一致嵌入脉冲。

Conclusion: 该方法降低了对昂贵脉冲模拟的依赖，为束流动力学仿真与分析提供了高效工具。

Abstract: Controlling the longitudinal laser pulse shape in photoinjectors of Free-Electron Lasers is a powerful lever for optimizing electron beam quality, but systematic exploration of the vast design space is limited by the cost of brute-force pulse propagation simulations. We present a generative modeling framework based on Wasserstein Autoencoders to learn a differentiable latent interface between pulse shaping and downstream beam dynamics. Our empirical findings show that the learned latent space is continuous and interpretable while maintaining high-fidelity reconstructions. Pulse families such as higher-order Gaussians trace coherent trajectories, while standardizing the temporal pulse lengths shows a latent organization correlated with pulse energy. Analysis via principal components and Gaussian Mixture Models reveals a well behaved latent geometry, enabling smooth transitions between distinct pulse types via linear interpolation. The model generalizes from simulated data to real experimental pulse measurements, accurately reconstructing pulses and embedding them consistently into the learned manifold. Overall, the approach reduces reliance on expensive pulse-propagation simulations and facilitates downstream beam dynamics simulation and analysis.

</details>


### [88] [Unified Latents (UL): How to train your latents](https://arxiv.org/abs/2602.17270)
*Jonathan Heek,Emiel Hoogeboom,Thomas Mensink,Tim Salimans*

Main category: cs.LG

TL;DR: 本文提出Unified Latents (UL)框架，通过将编码器输出噪声与扩散先验最小噪声水平对齐，获得简化的训练目标，实现比特率紧上界约束下的高效潜表示学习，在ImageNet和Kinetics上取得竞争性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的潜表示学习面临训练效率与生成质量的权衡。现有方法如Stable Diffusion潜变量需要较高计算成本。本文旨在设计一种联合扩散先验正则化与扩散解码的框架，以在比特率约束下获得更优的潜表示。

Method: UL框架通过将编码器输出噪声链接到扩散先验的最小噪声水平，构建简化的训练目标，该目标提供潜变量比特率的紧上界，并由扩散模型进行解码重建。

Result: ImageNet-512上达到1.4 FID，PSNR重建质量优异，且训练FLOPs少于Stable Diffusion潜变量基线；Kinetics-600上以1.3 FVD创下新SOTA。

Conclusion: UL框架通过联合扩散先验正则化与解码的设计，实现了高效且高质量的潜表示学习，为图像和视频生成任务提供了有效的扩散模型压缩与生成方案。

Abstract: We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.

</details>


### [89] [SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework](https://arxiv.org/abs/2602.17330)
*Rong Fu,Zijian Zhang,Wenxin Zhang,Kun Liu,Jiekai Wu,Xianda Li,Simon Fong*

Main category: cs.LG

TL;DR: SubQuad是一个端到端流程，通过抗原感知的近亚二次检索、GPU加速亲和力核、多模态融合学习和公平性约束聚类，解决了大规模免疫组库分析中的计算效率与数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前大规模适应性免疫组库比较分析存在两个关键瓶颈：成对亲和力评估的接近二次方计算复杂度，以及数据集不平衡导致的临床重要稀有克隆型被掩盖。

Method: 该方法采用紧凑MinHash预过滤减少候选比较，可微分门控模块自适应融合对齐与嵌入特征，并引入自动校准机制强制稀有抗原特异性亚群的公平表示。

Result: 在病毒和肿瘤免疫组库数据集上，SubQuad在保持或提升recall@k、聚类纯度与亚群公平性的同时，显著提高了计算吞吐量和内存使用效率。

Conclusion: SubQuad通过索引、相似性融合与公平性目标的协同设计，为免疫组库挖掘和疫苗靶点优先排序、生物标志物发现等转化医学任务提供了可扩展且偏差感知的解决方案。

Abstract: Comparative analysis of adaptive immune repertoires at population scale is hampered by two practical bottlenecks: the near-quadratic cost of pairwise affinity evaluations and dataset imbalances that obscure clinically important minority clonotypes. We introduce SubQuad, an end-to-end pipeline that addresses these challenges by combining antigen-aware, near-subquadratic retrieval with GPU-accelerated affinity kernels, learned multimodal fusion, and fairness-constrained clustering. The system employs compact MinHash prefiltering to sharply reduce candidate comparisons, a differentiable gating module that adaptively weights complementary alignment and embedding channels on a per-pair basis, and an automated calibration routine that enforces proportional representation of rare antigen-specific subgroups. On large viral and tumor repertoires SubQuad achieves measured gains in throughput and peak memory usage while preserving or improving recall@k, cluster purity, and subgroup equity. By co-designing indexing, similarity fusion, and equity-aware objectives, SubQuad offers a scalable, bias-aware platform for repertoire mining and downstream translational tasks such as vaccine target prioritization and biomarker discovery.

</details>


### [90] [2Mamba2Furious: Linear in Complexity, Competitive in Accuracy](https://arxiv.org/abs/2602.17363)
*Gabriel Mongaras,Eric C. Larson*

Main category: cs.LG

TL;DR: 针对线性注意力精度不如softmax注意力的问题，通过简化Mamba-2并改进A-mask和隐藏状态阶数，提出2Mamba方法，在保持高效率的同时接近softmax注意力的精度。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然计算效率高，但表达能力较弱，精度低于softmax注意力。为了弥补这一精度差距，需要改进线性注意力机制。

Method: 首先将Mamba-2简化为核心组件(Mamba-2S)，然后改进A-mask机制并增加隐藏状态的阶数，最终提出2Mamba方法。同时探索能够超越softmax注意力精度的关键元素。

Result: 2Mamba方法在精度上接近softmax注意力，同时在长上下文场景下具有更高的内存效率。

Conclusion: 通过结构简化和关键组件改进，线性注意力可以在保持效率优势的同时达到与softmax注意力相当的精度水平，甚至可能在某些方面超越softmax注意力。

Abstract: Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments

</details>


### [91] [A feature-stable and explainable machine learning framework for trustworthy decision-making under incomplete clinical data](https://arxiv.org/abs/2602.17364)
*Justyna Andrys-Olek,Paulina Tworek,Luca Gherardini,Mark W. Ruddock,Mary Jo Kurt,Peter Fitzgerald,Jose Sousa*

Main category: cs.LG

TL;DR: 本研究提出CACTUS框架，针对小样本、异质性和不完整的临床数据集，通过整合特征抽象、可解释分类和特征稳定性分析，解决生物医学机器学习模型在数据缺失场景下特征不稳定的问题，在保持预测性能的同时提升模型可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习模型在生物医学数据中应用日益广泛，但在高风险领域的应用仍受限于鲁棒性不足、可解释性有限以及特征在数据缺失等现实扰动下的不稳定性。高预测性能模型若因数据完整性变化而导致关键特征波动，将无法获得临床信任，影响可重复性和下游决策。

Method: 开发CACTUS（Comprehensive Abstraction and Classification Tool for Uncovering Structures）可解释机器学习框架，在包含568名血尿患者的真实膀胱癌队列上，通过控制随机缺失数据水平，与随机森林和梯度提升等广泛使用的机器学习方法进行基准测试，量化特征稳定性随数据质量下降的变化。

Result: CACTUS在预测性能上达到竞争或更优水平，同时在数据缺失增加时能保持显著更高的顶级特征稳定性（包括性别分层分析）。研究表明特征稳定性为传统性能指标提供了互补信息，对评估生物医学机器学习模型的可信度至关重要。

Conclusion: CACTUS通过显式量化对缺失数据的鲁棒性并优先选择可解释且稳定的特征，提供了一个可推广的框架，可用于建立值得信赖的数据驱动临床决策支持系统。

Abstract: Machine learning models are increasingly applied to biomedical data, yet their adoption in high stakes domains remains limited by poor robustness, limited interpretability, and instability of learned features under realistic data perturbations, such as missingness. In particular, models that achieve high predictive performance may still fail to inspire trust if their key features fluctuate when data completeness changes, undermining reproducibility and downstream decision-making. Here, we present CACTUS (Comprehensive Abstraction and Classification Tool for Uncovering Structures), an explainable machine learning framework explicitly designed to address these challenges in small, heterogeneous, and incomplete clinical datasets. CACTUS integrates feature abstraction, interpretable classification, and systematic feature stability analysis to quantify how consistently informative features are preserved as data quality degrades. Using a real-world haematuria cohort comprising 568 patients evaluated for bladder cancer, we benchmark CACTUS against widely used machine learning approaches, including random forests and gradient boosting methods, under controlled levels of randomly introduced missing data. We demonstrate that CACTUS achieves competitive or superior predictive performance while maintaining markedly higher stability of top-ranked features as missingness increases, including in sex-stratified analyses. Our results show that feature stability provides information complementary to conventional performance metrics and is essential for assessing the trustworthiness of machine learning models applied to biomedical data. By explicitly quantifying robustness to missing data and prioritising interpretable, stable features, CACTUS offers a generalizable framework for trustworthy data-driven decision support.

</details>


### [92] [Variational Grey-Box Dynamics Matching](https://arxiv.org/abs/2602.17477)
*Gurjeet Sangra Singh,Frantzeska Lavda,Giangiacomo Mercatali,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 提出一种新型灰盒方法，将不完整物理模型直接集成到流匹配生成模型中，通过双潜在编码分别建模缺失随机性和物理参数，实现从观测轨迹中无模拟学习动力学，性能优于纯数据驱动方法且保持物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型（如流匹配和扩散模型）虽能学习复杂分布但缺乏可解释性且忽略物理规律；物理模型（ODE/PDE）虽可解释但存在未知项无法完全描述现实。本研究旨在弥合黑盒生成模型与白盒物理模型之间的鸿沟，实现数据学习与物理可解释性的统一。

Method: 在流匹配框架中建模结构化变分分布，采用双重潜在编码：第一编码捕捉缺失随机性和多模态速度；第二编码将物理参数作为带物理信息先验的潜在变量。直接从观测轨迹学习，采用无模拟方式避免神经ODE的可扩展性与稳定性问题，并扩展至二阶动力学系统。

Result: 在代表性ODE/PDE基准问题上，该方法性能与纯数据驱动方法相当或更优，且优于现有灰盒基线，同时成功保留了物理模型的可解释性优势。

Conclusion: 该方法有效桥接了生成模型与物理模拟，提供了一种可扩展的灰盒建模框架，能够在保持物理可解释性的同时充分利用数据驱动能力，为复杂系统建模提供了新思路。

Abstract: Deep generative models such as flow matching and diffusion models have shown great potential in learning complex distributions and dynamical systems, but often act as black-boxes, neglecting underlying physics. In contrast, physics-based simulation models described by ODEs/PDEs remain interpretable, but may have missing or unknown terms, unable to fully describe real-world observations. We bridge this gap with a novel grey-box method that integrates incomplete physics models directly into generative models. Our approach learns dynamics from observational trajectories alone, without ground-truth physics parameters, in a simulation-free manner that avoids scalability and stability issues of Neural ODEs. The core of our method lies in modelling a structured variational distribution within the flow matching framework, by using two latent encodings: one to model the missing stochasticity and multi-modal velocity, and a second to encode physics parameters as a latent variable with a physics-informed prior. Furthermore, we present an adaptation of the framework to handle second-order dynamics. Our experiments on representative ODE/PDE problems show that our method performs on par with or superior to fully data-driven approaches and previous grey-box baselines, while preserving the interpretability of the physics model. Our code is available at https://github.com/DMML-Geneva/VGB-DM.

</details>


### [93] [Linear Convergence in Games with Delayed Feedback via Extra Prediction](https://arxiv.org/abs/2602.17486)
*Yuma Fujimoto,Kenshi Abe,Kaito Ariu*

Main category: cs.LG

TL;DR: 本文研究了加权乐观梯度下降上升法(WOGDA)在延迟反馈下的线性收敛速率。通过将WOGDA解释为额外近端点法(EPP)的近似，发现标准乐观能达到exp(-Θ(t/m^5))的收敛速率，而额外乐观能显著加速至exp(-Θ(t/(m^2 log m)))，并允许更大的步长。


<details>
  <summary>Details</summary>
Motivation: 在实际多智能体学习中，反馈延迟不可避免且严重损害性能。对于双线性博弈，延迟反馈下的收敛速率尚不明确。因此需要寻找有效的算法来应对延迟带来的性能下降问题。

Method: 提出加权乐观梯度下降上升法(WOGDA)，通过预测未来奖励引入乐观机制。将WOGDA解释为额外近端点法(EPP)的近似，并与经典近端点法(PP)比较。分析标准乐观和额外乐观两种策略在延迟m下的收敛行为。

Result: 理论证明：1) 标准乐观(预测下一步奖励)能达到线性收敛，速率为exp(-Θ(t/m^5))；2) 额外乐观(预测更远未来奖励)不仅允许更大的步长，还能将收敛速率显著提升至exp(-Θ(t/(m^2 log m)))。实验验证了额外乐观的加速效果，与理论结果一致。

Conclusion: 额外乐观是应对反馈延迟导致性能下降的有效策略。WOGDA算法通过引入额外乐观机制，能显著提升延迟环境下的收敛速率，为多智能体学习在延迟反馈下的应用提供了理论支持和实践指导。

Abstract: Feedback delays are inevitable in real-world multi-agent learning. They are known to severely degrade performance, and the convergence rate under delayed feedback is still unclear, even for bilinear games. This paper derives the rate of linear convergence of Weighted Optimistic Gradient Descent-Ascent (WOGDA), which predicts future rewards with extra optimism, in unconstrained bilinear games. To analyze the algorithm, we interpret it as an approximation of the Extra Proximal Point (EPP), which is updated based on farther future rewards than the classical Proximal Point (PP). Our theorems show that standard optimism (predicting the next-step reward) achieves linear convergence to the equilibrium at a rate $\exp(-Θ(t/m^{5}))$ after $t$ iterations for delay $m$. Moreover, employing extra optimism (predicting farther future reward) tolerates a larger step size and significantly accelerates the rate to $\exp(-Θ(t/(m^{2}\log m)))$. Our experiments also show accelerated convergence driven by the extra optimism and are qualitatively consistent with our theorems. In summary, this paper validates that extra optimism is a promising countermeasure against performance degradation caused by feedback delays.

</details>


### [94] [Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models](https://arxiv.org/abs/2602.17497)
*Wen-Tse Chen,Jiayu Chen,Fahim Tajwar,Hao Zhu,Xintong Duan,Ruslan Salakhutdinov,Jeff Schneider*

Main category: cs.LG

TL;DR: 针对稀疏奖励下自演化智能体训练的挑战，本文提出基于大语言模型的回顾性上下文学习（RICL）方法，将稀疏奖励转化为优势函数，并设计在线学习框架RICOL。实验表明RICL能高效估计优势函数，RICOL在BabyAI场景中样本效率显著优于传统RL算法。


<details>
  <summary>Details</summary>
Motivation: 自演化智能体训练中，稀疏环境反馈与自我采样数据的低效学习是根本难题。时序信用分配虽可将稀疏反馈转为密集监督信号，但现有方法依赖任务特定的价值函数学习，导致样本效率低下且泛化能力受限。

Method: 利用大语言模型的预训练知识，通过回顾性上下文学习（RICL）实现稀疏奖励到优势函数的转换；进一步提出RICOL在线学习框架，基于RICL的信用分配结果迭代优化策略。

Result: RICL能以少量样本准确估计优势函数并识别环境关键状态。四个BabyAI场景的评估显示，RICOL与传统在线RL算法性能相当，但样本效率显著更高。

Conclusion: 研究表明大语言模型在时序信用分配中具有重要潜力，为构建更高效、更通用的强化学习范式提供了新方向。

Abstract: Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states in the environment for temporal credit assignment. Extended evaluation on four BabyAI scenarios show that RICOL achieves comparable convergent performance with traditional online RL algorithms with significantly higher sample efficiency. Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms.

</details>


### [95] [Position: Evaluation of ECG Representations Must Be Fixed](https://arxiv.org/abs/2602.17531)
*Zachary Berger,Daniel Prakah-Asante,John Guttag,Collin M. Stultz*

Main category: cs.LG

TL;DR: 该论文指出当前12导联心电图表示学习过度依赖心律失常和形态特征基准，未能反映心电图编码的广泛临床信息。通过扩展评估维度至结构性心脏病、血流动力学推断和患者预测，并改进多标签不平衡评估方法，研究发现随机初始化编码器性能媲美SOTA，呼吁改革基准测试实践以提高研究可靠性与临床相关性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在PTB-XL、CPSC2018和CSN三个多标签基准上达成共识，但这些基准仅关注心律失常和波形形态特征，而心电图实际编码结构性心脏病、患者预后等更广泛的临床信息，导致评估目标与临床意义脱节，需扩展评估范围以确保研究进展可靠性。

Method: 提出扩展下游评估至结构性心脏病、血流动力学推断和患者级预测等临床相关终点；制定多标签不平衡数据的评估最佳实践；实证评估三种代表性预训练方法在六种场景（三种标准基准、结构性心脏病、血流动力学、患者预测）下的性能表现。

Result: 应用改进评估方法后，现有文献关于最佳表示方法的结论被颠覆；关键发现：随机初始化编码器配合线性评估在许多任务上性能与SOTA预训练模型相当，证明其可作为合理的基线模型，挑战了预训练的必要性假设。

Conclusion: 当前基准测试实践需系统性改革，应纳入结构性心脏病、患者预后等更全面的临床评估目标，采用更严格的不平衡数据评估标准，并建立随机编码器作为基线模型，以确保心电图表示学习研究的可靠性和临床实用性。

Abstract: This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model. We substantiate our observations with an empirical evaluation of three representative ECG pre-training approaches across six evaluation settings: the three standard benchmarks, a structural disease dataset, hemodynamic inference, and patient forecasting.

</details>


### [96] [MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning](https://arxiv.org/abs/2602.17550)
*Xiaoliang Fu,Jiaye Lin,Yangyi Fang,Binbin Zheng,Chaowen Hu,Zekai Shao,Cong Qin,Lu Pan,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 针对现有RLVR算法中刚性信任区域机制与LLM优化动态不匹配问题，本文识别三大挑战并提出MASPO框架：软高斯门控提升梯度利用效率，质量自适应限制器平衡概率谱探索，非对称风险控制器对齐信号置信度，实验证实其显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法（如GRPO）采用刚性、统一、对称的信任区域机制，与LLM复杂优化动态存在根本性错配，具体表现为硬裁剪导致梯度利用效率低、均匀比率约束忽略词元分布造成概率质量不敏感、正负样本信用分配模糊性引发非对称信号可靠性，这些缺陷促使作者设计协调三者的统一框架。

Method: 提出Mass-Adaptive Soft Policy Optimization (MASPO)框架，包含三个核心组件：1) 可微分软高斯门控替代硬裁剪以最大化梯度效用；2) 质量自适应限制器根据概率质量动态调整约束，平衡全概率谱探索；3) 非对称风险控制器依据信号置信度对齐更新幅度，处理正负样本的不同可靠性。

Result: 广泛评估表明MASPO作为稳健的一体化RLVR解决方案，性能显著优于现有强基线算法，在LLM强化学习任务中实现了更优的性能表现，但摘要未提供具体数值指标。

Conclusion: MASPO通过统一框架成功解决了梯度利用效率、概率质量敏感性和信号可靠性非对称三大关键挑战，为RLVR领域提供了新的有效解决方案，代码已开源以促进可重复性。

Abstract: Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.

</details>


### [97] [A Theoretical Framework for Modular Learning of Robust Generative Models](https://arxiv.org/abs/2602.17554)
*Corinna Cortes,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本文提出一种模块化生成建模框架，通过门控机制组合预训练专家模型，以解决大规模语言模型训练的资源密集性和启发式数据加权问题。理论证明存在鲁棒门控函数，模块化结构可作为强正则化项，并引入随机原始-对偶算法和结构蒸馏方法实现高效推理。实验表明该架构能缓解梯度冲突并优于单体基线。


<details>
  <summary>Details</summary>
Motivation: 训练大规模生成模型资源消耗巨大，且严重依赖启发式数据集加权策略。作者探讨两个核心问题：一是能否通过组合小型领域专家模块实现与单体模型相当的性能，二是能否摆脱启发式调参，实现对任意数据混合的鲁棒训练。这旨在提升训练效率并降低人工干预成本。

Method: 研究提出模块化生成建模理论框架，定义归一化门控函数空间G₁，将问题形式化为极小极大博弈以寻找对最差数据混合鲁棒的门控机制。利用角谷静夫不动点定理证明鲁棒门控的存在性，分析模块化作为强正则化项的泛化界。通过Jensen-Shannon散度量化模块化方法相对聚合数据重训练的理论优势。提出可扩展的随机原始-对偶算法和结构蒸馏方法以实现高效推理。

Result: 在合成和真实数据集上的实证结果表明，该模块化架构能有效缓解梯度冲突，并鲁棒地超越单体基线模型。实验验证了理论分析的正确性，证明了模块化方法在实际应用中的有效性。

Conclusion: 该研究建立了模块化语言模型训练的理论基础，证明了其相对于传统单体训练方法的优越性。通过理论保证和高效算法，为资源节约型、鲁棒性强的模型开发提供了新范式，具有重要的理论和实践价值。

Abstract: Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence. Finally, we introduce a scalable Stochastic Primal-Dual algorithm and a Structural Distillation method for efficient inference. Empirical results on synthetic and real-world datasets confirm that our modular architecture effectively mitigates gradient conflict and can robustly outperform monolithic baselines.

</details>


### [98] [Be Wary of Your Time Series Preprocessing](https://arxiv.org/abs/2602.17568)
*Sofiane Ennadir,Tianze Wang,Oleg Smirnov,Sahar Asadi,Lele Cao*

Main category: cs.LG

TL;DR: 本研究首次从理论角度系统分析了Transformer时间序列模型中不同归一化策略（实例级缩放和全局缩放）对模型表达能力的影响。通过构建专用的表达能力框架并推导标准缩放与最小-最大缩放的理论边界，实证发现无单一归一化方法在所有场景下均占优，甚至有时省略归一化效果更佳。


<details>
  <summary>Details</summary>
Motivation: 尽管归一化是时间序列建模的基础预处理步骤，但其在Transformer模型中的作用缺乏理论层面的系统性研究。现有实践多依赖经验选择，亟需从理论上理解不同归一化策略如何影响模型的表示能力，以指导更科学的方法设计。

Method: 作者提出一个专为时间序列定制的表达能力框架，用于量化模型在表示空间中区分相似与相异输入的能力。基于该框架，推导了标准缩放和最小-最大缩放两种常用归一化方法的理论边界，并结合分类与预测基准任务进行实证验证。

Result: 理论分析表明，归一化策略选择显著影响Transformer模型的表示能力，且效果依赖于任务与数据特性。实证结果证实无单一方法始终最优，在某些情况下不进行归一化处理反而能获得更优性能，凸显了预处理设计的复杂性。

Conclusion: 预处理在时间序列学习中起关键作用，需开发面向特定任务和数据集的原则性归一化策略。该研究为理解归一化机制提供了理论依据，并激励未来工作探索自适应或任务驱动的归一化方法设计。

Abstract: Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets.

</details>


### [99] [Towards Anytime-Valid Statistical Watermarking](https://arxiv.org/abs/2602.17608)
*Baihe Huang,Eric Xu,Kannan Ramchandran,Jiantao Jiao,Michael I. Jordan*

Main category: cs.LG

TL;DR: 本文提出首个基于e值的文本水印框架Anchored E-Watermarking，通过构建检验上鞅实现可随时停止的有效推断，利用锚定分布逼近目标模型，在最优采样与最坏情况对数增长率下优化e值，相比现有方法减少13-15%的检测token开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型普及亟需区分机器与人类文本，现有统计水印方法存在两大局限：缺乏采样分布选择原则、依赖固定假设检验无法有效提前停止，导致样本效率低下且无法保证随时停止的统计有效性。

Method: 提出Anchored E-Watermarking框架，构建检测过程的上鞅结构实现anytime-valid推断，引入锚定分布近似目标模型，推导基于最坏情况对数增长率的最优e值及其期望停止时间。

Result: 理论分析与仿真实验表明，该框架显著提升样本效率，在标准基准测试中平均token检测预算比先进基线降低13-15%。

Conclusion: 首次将e值理论与最优停止理论结合于水印检测，解决了任意停止下的统计有效性难题，为高效可靠的机器文本识别提供了新范式。

Abstract: The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.

</details>


### [100] [Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning](https://arxiv.org/abs/2602.17614)
*Obaidullah Zaland,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 针对联邦学习中计算负担与隐私泄露问题，本文提出KD-UFSL框架，结合微聚集与差分隐私保护中间表示，实验证实其能显著提升重构难度并维持模型效用。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能实现去中心化训练并保护数据隐私，但存在客户端计算负担重和中间表示（smashed data）可能泄露私有数据的问题。UFSL虽能卸载计算，但隐私风险未解，亟需在保持模型效用的前提下实现隐私保护。

Method: 提出k-匿名差分隐私UFSL（KD-UFSL），在客户端 smashed data 传输至服务器前，应用微聚集和差分隐私技术进行匿名化与加噪处理，以降低隐私泄露风险。

Result: 在四个基准数据集上的实验表明，KD-UFSL可将实际图像与重构图像的均方误差提升最高50%，结构相似性降低最高40%，同时保持全局模型效用。

Conclusion: KD-UFSL通过隐私增强技术有效缓解了UFSL的隐私泄露问题，在平衡隐私保护与模型效用方面表现出色，适用于大规模大数据应用场景。

Abstract: Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients' side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients' private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.

</details>


### [101] [Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs](https://arxiv.org/abs/2602.17616)
*Luke Huang,Zhuoyang Zhang,Qinghao Hu,Shang Yang,Song Han*

Main category: cs.LG

TL;DR: 针对异步强化学习在大型语言模型训练中因过时采样导致的高方差问题，本文提出VCPO方法，通过有效样本量自适应调节学习率并引入最小方差基线，显著提升了训练稳定性，在多项任务上超越基线方法，并将训练时间缩短2.5倍。


<details>
  <summary>Details</summary>
Motivation: 异步强化学习虽能提升大型语言模型训练的端到端吞吐量，但会导致策略梯度估计方差显著增加，特别是对于REINFORCE和GRPO等无评论家方法。过时采样产生重尾重要性比率，使少量样本主导更新，导致梯度噪声大、学习不稳定，制约了可扩展异步RL的可靠性。

Method: 提出VCPO（方差控制策略优化）方法，包含两个核心组件：(i) 基于有效样本量(ESS)自适应调节学习率以抑制不可靠更新；(ii) 采用闭式最小方差基线适应离线策略设置，避免辅助价值模型并引入极小开销。

Result: 实验表明，VCPO在数学、通用推理和工具使用任务上显著提升异步训练的鲁棒性，超越多种基线方法（包括掩码/裁剪稳定器和算法变体），并将长上下文多轮训练时间缩短2.5倍，同时达到同步训练的性能水平。

Conclusion: 结论表明，显式控制策略梯度方差是构建可扩展可靠异步强化学习的关键，VCPO为无评论家策略梯度算法的稳定性提供了通用解决方案。

Abstract: Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.

</details>


### [102] [Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning](https://arxiv.org/abs/2602.17625)
*Obaidullah Zaland,Zulfiqar Ahmad Khan,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 提出一次性增量联邦学习（OSI-FL），通过单次通信的类别嵌入与扩散模型数据合成，结合选择性样本保留抑制灾难性遗忘，在三个基准数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代大数据系统产生海量、异构、地理分散且隐私敏感的流数据，中心化面临挑战。联邦学习虽能保护隐私，但假设数据静态且需多轮通信，难以应对增量数据学习与有限通信场景，同时存在灾难性遗忘问题。

Method: OSI-FL框架在单次通信中传输冻结视觉语言模型生成的类别特定嵌入；服务器利用预训练扩散模型合成与客户端分布相似的新样本进行训练；引入选择性样本保留（SSR）策略，基于样本损失筛选每类每任务的前p个最具信息量的样本，在后续迭代中持续训练以抑制灾难性遗忘。

Result: 实验表明，OSI-FL在类别增量和领域增量场景下均显著优于传统及一次性联邦学习方法，在三个基准数据集上表现最佳。

Conclusion: OSI-FL是首个同时解决通信开销与灾难性遗忘的联邦学习框架，通过单次通信与样本保留机制实现高效增量学习，为隐私保护的大数据流处理提供了有效方案。

Abstract: Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.

</details>


### [103] [Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting](https://arxiv.org/abs/2602.17634)
*Xinghong Fu,Yanhong Li,Georgios Papaioannou,Yoon Kim*

Main category: cs.LG

TL;DR: 本文提出了一种高效时间序列基础模型学习方案，通过小型混合架构（长卷积+线性RNN/DeltaNet层）、数据增强和推理优化，在零样本预测任务上达到与大型Transformer模型相当的性能，但模型尺寸缩小百倍以上，显著拓展了性能-效率帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 尽管缩放策略在语言、视觉基础模型中成效显著，但将其应用于时间序列领域导致了模型臃肿化——现有数百兆参数级别的模型虽性能优越，却存在部署成本高、推理效率低的实践困境。这促使研究者反思：能否以极小模型尺寸实现同等的零样本泛化能力，从而解决性能与效率的失衡问题。

Method: 1. 架构创新：摒弃大规模Transformer，采用交替堆叠的长卷积层与线性RNN层（DeltaNet）构建紧凑型混合模型；2. 数据增强：引入多种数据增强策略扩充训练样本多样性；3. 推理优化：设计高效推理机制提升预测质量。三者结合形成"简单配方"，实现小模型的高效预训练。

Result: - 模型效率：Reverso系列模型参数量比大型Transformer减少100倍以上（缩小两个数量级）；- 性能保持：在零样本时间系列预测任务上达到同等或相当性能水平；- 帕累托改进：显著推进了性能-效率边界，在同等效率下实现更优性能。

Conclusion: 研究证明大规模Transformer并非时间序列基础模型的必要设计选择，小型混合架构通过恰当设计可达到同等效果。这一发现颠覆了"模型越大越好"的固有认知，为资源受限场景下的时间序列建模提供了新范式，具有重要学术与工业价值。

Abstract: Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.

</details>


### [104] [FAMOSE: A ReAct Approach to Automated Feature Discovery](https://arxiv.org/abs/2602.17641)
*Keith Burghardt,Jienan Liu,Sadman Sakib,Yuning Hao,Bo Li*

Main category: cs.LG

TL;DR: FAMOSE是一个基于ReAct范式的新型智能体框架，用于自动化特征工程。它在分类任务上达到或接近SOTA，在回归任务上达到SOTA并显著降低RMSE，证明AI智能体能有效解决需要创造性的特征工程问题。


<details>
  <summary>Details</summary>
Motivation: 特征工程在机器学习中至关重要但具有挑战性，特别是在表格数据领域。传统方法需要大量领域专业知识来从指数级大的特征空间中识别最优特征，这成为了瓶颈。

Method: 提出FAMOSE框架，利用ReAct（推理+行动）范式，在智能体架构中自主探索、生成和优化特征，同时集成特征选择和评估工具。这是首次将智能体ReAct框架应用于自动化特征工程，适用于分类和回归任务。

Result: 在分类任务上达到或接近SOTA，尤其在超过1万个实例的任务中平均提升ROC-AUC 0.23%；在回归任务上达到SOTA，平均降低RMSE 2.0%，且比其他算法更具鲁棒性。

Conclusion: AI智能体在解决需要高度创造性解决方案的问题（如特征工程）方面表现出色，ReAct范式使LLM能够记录特征尝试历史，指导生成更优特征，这为智能体解决复杂问题提供了有力证据。

Abstract: Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.

</details>


### [105] [A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning](https://arxiv.org/abs/2602.17642)
*Dhruv Talwar,Harsh Desai,Wendong Yin,Goutam Mohanty,Rafael Reveles*

Main category: cs.LG

TL;DR: 该论文提出了一种名为A.R.I.S.的自动化电子垃圾分拣系统，该系统利用YOLOx深度学习模型实现对破碎电子废弃物的实时分类，显著提升了金属、塑料和电路板的分拣精度和效率，为传统电子回收中的资源损失问题提供了低成本解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统电子垃圾回收工艺因材料分离和识别能力不足，导致严重的资源损失，限制了材料回收效率。现有分拣技术存在成本高、便携性差、识别精度有限等问题，制约了先进回收技术的普及应用。

Method: 研究团队开发了A.R.I.S.（自动化回收识别系统），一个低成本、便携式的破碎电子垃圾分拣设备。该系统采用YOLOx目标检测模型进行实时分类，将深度学习与已建立的物理分拣方法相结合，实现了对金属、塑料和电路板的自动识别与分拣。

Result: 实验评估显示，该系统达到90%的总体精度、82.2%的平均精度均值（mAP）以及84%的分拣纯度。系统在保持较低推理延迟的同时，实现了较高的检测准确率，证明了其在实时分拣场景中的有效性。

Conclusion: 该研究通过整合深度学习与传统分拣技术，成功降低了先进回收技术的应用门槛，提升了材料回收效率。这项工作不仅支持了产品生命周期延长、以旧换新和回收计划等环保倡议，还为减少供应链环境足迹提供了可行技术路径，对推动电子垃圾资源化利用具有重要实践意义。

Abstract: Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.

</details>


### [106] [Multi-Round Human-AI Collaboration with User-Specified Requirements](https://arxiv.org/abs/2602.17646)
*Sima Noorani,Shayan Kiyani,Hamed Hassani,George Pappas*

Main category: cs.LG

TL;DR: 本文提出了一个以人为本的多轮对话AI框架，通过反事实伤害和互补性两个原则，配合在线无分布算法，确保AI协作提升人类决策质量。在医疗诊断和图像推理任务中验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人类越来越多地依赖多轮对话AI进行高风险决策，需要原则性框架来确保这些交互能可靠地提升决策质量。现有方法缺乏对用户定义约束的保证，且难以在动态交互中维持AI行为的原则性要求。

Method: 采用以人为本的视角，基于反事实伤害（确保AI不削弱人类优势）和互补性（确保AI在人类易错处增加价值）两个原则。通过用户定义的规则形式化这些概念，并引入具有有限样本保证的在线无分布算法，在协作动态中强制执行用户指定的约束。

Result: 在LLM模拟的医疗诊断任务和人类众包的图像推理任务中，该在线过程能在非平稳交互动态下维持规定的反事实伤害和互补性违反率。收紧或放松这些约束会可预测地改变下游人类准确性，证实这两个原则可作为实用杠杆来引导多轮协作朝向更好的决策质量。

Conclusion: 该框架提供了一种无需建模或约束人类行为即可指导AI协作的方法，两个原则为实践者提供了可调的参数，在高风险决策场景中具有重要应用价值，为构建更可靠的AI辅助决策系统奠定了基础。

Abstract: As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.

</details>


### [107] [MARS: Margin-Aware Reward-Modeling with Self-Refinement](https://arxiv.org/abs/2602.17658)
*Payel Bhattacharjee,Osvaldo Simeone,Ravi Tandon*

Main category: cs.LG

TL;DR: 针对奖励建模依赖昂贵人工数据的问题，本文提出MARS自适应边际感知增强策略，通过聚焦低边际模糊样本并迭代优化训练分布，从理论和实验上均证明能有效提升奖励模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 奖励建模是对齐流程的核心，但依赖成本高昂且稀缺的人工标注偏好数据。现有增强方法未考虑模型估计难度的差异性，无法针对性地改善模型模糊区域，亟需更智能的数据增强策略。

Method: MARS采用边际感知机制识别奖励模型最不确定的低边际偏好对，集中进行困难样本增强，并迭代优化训练数据分布，以自适应地提升模型在模糊和失败模式下的表现。

Result: 理论上，MARS通过增加损失函数平均曲率来增强信息量并改善优化条件数；实验上，相比均匀增强方法，在鲁棒奖励建模任务中实现了一致的性能提升。

Conclusion: MARS为奖励建模提供了一种有效的自适应增强方案，通过智能聚焦不确定性区域，显著提升了模型鲁棒性，为对齐流程中的数据效率问题提供了新思路。

Abstract: Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [108] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: 本文提出AIdentifyAGE本体，一种标准化语义框架，用于解决法医牙科年龄评估中的方法异质性、数据碎片化和系统互操作性问题。该本体整合人工和AI工作流程，实现全流程可追溯链接，并基于FAIR原则构建，为法医司法决策支持系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 在涉及无证件未成年人的法医司法实践中，年龄评估决定法律保护门槛，牙科评估虽可靠但面临方法标准化不足、数据表示不统一、信息系统互操作性差等挑战。这些问题严重影响评估透明度和可重复性，AI方法的引入进一步加剧了这些挑战，亟需标准化解决方案。

Method: 研究采用协作式本体工程方法：1) 与领域专家合作开发；2) 基于现有上层生物医学、牙科和机器学习本体；3) 遵循FAIR原则确保可发现、可访问、可互操作和可重用；4) 对司法背景、个体信息、法医检查、牙科评估方法、影像数据、参考研究和AI方法进行完整语义建模。

Result: 成功开发AIdentifyAGE本体，实现了：1) 人工与AI辅助牙科年龄评估流程的标准化；2) 从观察到结果报告的全流程可追溯链接；3) 跨系统互操作性和可扩展性；4) 符合FAIR原则的数据管理，为构建本体驱动的决策支持系统提供技术基础。

Conclusion: AIdentifyAGE本体是提升法医牙科年龄评估一致性、透明度和可解释性的里程碑，为法医司法领域的决策支持系统建立了坚实基础，将推动该领域向标准化、可重复和可信的方向发展。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [109] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 该研究证明单状态重用机制在经典概率框架下必然导致情境性，并量化了其不可约简的信息论代价，表明情境性是适应性智能的普遍表示约束而非量子现象的特例。


<details>
  <summary>Details</summary>
Motivation: 由于内存、表示或物理资源限制，适应性系统普遍存在跨情境重用单一内部状态的现象，但其对表示能力的根本性影响尚未被充分理解，亟需建立理论框架揭示其约束机制。

Method: 采用干预建模方法，将不同情境视为对共享内部状态的作用，通过信息论分析严格证明经典模型在复现情境化统计时无法仅通过内部状态传递情境依赖。

Result: 1) 证明情境性是单状态重用的必然结果；2) 揭示不可约简的信息论代价；3) 提供最小构造性示例；4) 阐明非经典框架通过放弃单一全局概率空间假设来规避约束。

Conclusion: 情境性作为适应性智能的表示约束具有普适性，独立于具体物理实现。该结果将量子情境性推广至经典领域，为理解智能系统的根本性限制提供了统一理论视角。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [110] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: 针对大规模人类移动模拟中LLM计算成本高昂的问题，本研究提出MobCache——一种基于可重构缓存的移动性感知框架，通过潜在空间推理复用和轻量级移动规律约束蒸馏，在保持性能的同时显著提升模拟效率。


<details>
  <summary>Details</summary>
Motivation: 大规模人类移动模拟在城市规划、流行病学研究和交通分析等领域具有重要应用价值。现有方法利用大型语言模型(LLM)作为人类智能体进行结构化推理，可生成高度真实的移动行为，但其巨大的计算开销严重制约了可扩展性。

Method: 设计MobCache框架，包含两个核心组件：(1)推理组件：将推理步骤编码为潜在空间嵌入，通过潜在空间评估器实现推理路径的重用与重组；(2)解码组件：采用经移动规律约束蒸馏训练的轻量级解码器，将潜在空间推理链转化为自然语言输出，从而在保持保真度的同时提升效率。

Result: 实验结果表明，与现有最先进LLM基方法相比，MobCache在多个维度上显著提高了计算效率，同时维持了相当的模拟性能。

Conclusion: MobCache通过创新的缓存机制有效缓解了LLM在大型移动模拟中的计算瓶颈，为开发可扩展且真实的人类移动模拟系统提供了切实可行的技术路径。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [111] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 本研究分析了60个大型语言模型基准测试的饱和现象，发现近半数基准会随时间推移而饱和，失去区分顶尖模型的能力。通过分析14个基准属性，发现专家策划的基准比众包基准更抗饱和，而隐藏测试数据并无防护效果，为设计更持久的评估基准提供了策略指导。


<details>
  <summary>Details</summary>
Motivation: AI基准测试在衡量模型进展和指导部署决策中起核心作用，但许多基准快速饱和，无法区分最佳模型，长期价值下降。需要识别导致饱和的因素，以设计更耐用的基准测试。

Method: 从主流模型开发商技术报告中选取60个LLM基准测试，从任务设计、数据构建和评估格式三个维度分析14个基准属性，检验五个关于属性如何影响饱和率的假设。

Result: 近半数基准呈现饱和现象，且饱和度随基准年龄增长而增加。隐藏测试数据（公开vs私有）对防止饱和无显著效果，而专家策划的基准比众包基准更能抵抗饱和。

Conclusion: 研究发现揭示了哪些设计选择能延长基准寿命，为构建更持久的评估体系提供了策略依据，有助于指导未来基准测试的设计。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [112] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 本文通过三个领域实验发现，简单代码搜索基线能匹敌甚至超越复杂代码进化技术，揭示了当前研究基线对比不足和评估方法缺陷等问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码进化技术性能出色但缺乏与简单基线的对比，导致实际效果被高估，需系统性检验以揭示其在开发和评估方面的不足。

Method: 在数学边界优化、智能体框架设计和机器学习竞赛三个领域测试两种简单基线，与复杂代码进化管道对比性能，并识别方法缺陷。

Result: 简单基线在所有领域均达到或超过复杂方法：1) 数学边界问题中，搜索空间和领域知识比进化管道更重要；2) 智能体框架因高方差和小数据集导致次优选择，手工多数投票框架表现最佳；3) 提出降低评估随机性且经济可行的评估方法。

Conclusion: 代码进化研究重点应转向搜索空间设计而非搜索算法本身。未来需建立更严谨评估标准，减少随机性，并提供最佳实践指导促进该领域健康发展。

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [113] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 该论文研究了用超平面切割n维超立方体Q_n所有边所需的最小超平面数S(n)，证明了S(n) ≤ ⌈4n/5⌉（当n不是5的奇数倍时），否则S(n) ≤ 4n/5+1，改进了Paterson(1971)提出的⌈5n/6⌉上界，并通过CPro1自动工具构造了切割Q_10的8个超平面，同时获得了用k < n个超平面可切割边数的新下界。


<details>
  <summary>Details</summary>
Motivation: 超立方体边切割问题是计算几何和离散数学中的基础问题，寻找最小切割超平面数S(n)不仅具有理论意义，也与高维空间划分、近似算法和机器学习中的分离超平面理论密切相关。改进渐近上界有助于理解高维几何结构的本质特征，而自动化构造工具的开发则代表了AI辅助数学发现的新方向。

Method: 采用构造性证明与自动化搜索相结合的方法。利用CPro1工具——一种将推理大语言模型与自动化超参数调优耦合的搜索算法发现系统——构造出能切割10维超立方体Q_10所有边的8个超平面。基于此具体构造，通过组合推广获得一般维数的上界。同时运用组合分析技术推导次优切割情况下的下界结果。

Result: 核心结果是改进了S(n)的上界：对所有维度n，S(n) ≤ ⌈4n/5⌉，当n为5的奇数倍时S(n) ≤ 4n/5+1。这比Paterson在1971年建立的⌈5n/6⌉界提升了约1/30n的常数因子。次要结果是获得了用k < n个超平面可切割的最大边数的新下界。构造性证明中发现的8个超平面切割Q_10的方案为理论界提供了具体实例。

Conclusion: 该研究显著改进了超立方体边切割问题的上界常数，展示了自动化工具在发现复杂数学构造中的强大潜力。结果不仅推进了对高维几何结构可切割性的理论理解，也验证了CPro1作为AI辅助数学发现范式的有效性。未来工作可探索该工具在其他组合几何问题中的应用，以及进一步优化下界以达到紧性。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [114] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 针对边缘AI部署中集中式智能的成本与脆弱性问题，本文提出Node Learning去中心化学习范式，通过节点自主选择交互实现智能的本地化持续学习与扩散传播。


<details>
  <summary>Details</summary>
Motivation: 随着AI向边缘扩展，集中式智能在数据传输、延迟、能耗及对大型数据中心的依赖等方面暴露出成本和脆弱性瓶颈，难以适应异构、移动且资源受限的边缘环境。

Method: 边缘节点持续从本地数据学习并维护独立模型状态，在协作有利时通过选择性对等交互交换知识，学习通过重叠与扩散而非全局同步或中心聚合传播，统一自治与合作行为并容纳数据、硬件、目标及连接的异质性。

Result: 发展了该范式的概念基础，对比了现有去中心化方法，并探讨了其在通信、硬件、信任与治理方面的影响，将现有范式置于更广泛的去中心化视角中。

Conclusion: Node Learning并非摒弃现有范式，而是为边缘智能提供了一种统一的、更全面的去中心化框架，将自主行为与协作行为整合于单一抽象层次。

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [115] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文针对犹豫模糊集传统评分方法缺乏序理论形式基础的问题，提出序导向统一框架，否定经典序诱导格结构的主张，验证对称序评分满足强单调性和Gärdenfors条件，并引入基于最小可接受阈值的优势度函数用于排序和群决策。


<details>
  <summary>Details</summary>
Motivation: 现有犹豫模糊集评分方法缺乏序理论的严格形式化基础，导致评分机制在灵活性和一致性方面存在局限，亟需建立更加坚实的理论框架以支持复杂决策场景。

Method: 提出序导向的统一评分框架，将评分函数与特定序关系显式关联；系统检验犹豫模糊元（[0,1]非空子集）上的经典序关系及其诱导格结构的能力；验证对称序下评分函数的规范性属性（强单调性、Gärdenfors条件）；引入基于控制集和最小可接受阈值的优势度函数类，并提供离散型和相对型两个具体实例；演示其在模糊偏好关系构建和群决策支持中的应用。

Result: 经典序关系在犹豫模糊元上不能诱导格结构，纠正了先前文献的错误主张；对称序下的评分函数满足强单调性（关于并运算）和Gärdenfors条件等关键规范准则；成功定义了优势度函数类并给出离散优势度函数与相对优势度函数两个具体实现；优势度函数能够有效构建犹豫模糊集上的模糊偏好关系并支持群决策过程。

Conclusion: 本研究为犹豫模糊集评分奠定了严格的序理论基础，澄清了关于格结构的理论误解；提出的优势度函数方法为复杂决策环境下的排序问题提供了实用工具，特别适用于群决策场景，拓展了犹豫模糊集理论在实际决策中的应用范围。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [116] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5是一个原生的GUI智能体模型，提供多种规模（2B/4B/8B/32B/235B）的指令/思维变体，支持桌面、移动、浏览器等多平台，实现云边协同和实时交互。该模型在20多个GUI基准测试上达到开源模型中的最先进水平，包括GUI自动化、定位、工具调用和记忆知识等任务。核心创新包括混合数据飞轮、统一智能体能力增强和多平台环境RL扩展（MRPO算法）。模型已开源。


<details>
  <summary>Details</summary>
Motivation: 开发能够在多平台上实现实时交互和云边协同的GUI智能体模型，解决现有模型在跨平台适配、长时任务训练效率以及数据收集质量等方面的挑战，提升GUI自动化和智能体推理能力。

Method: 1. 混合数据飞轮：结合模拟环境和云端沙盒环境构建UI理解和轨迹生成的数据管道，提升数据收集效率和质量。
2. 统一智能体能力增强：采用统一思维合成管道增强模型推理能力，重点改进工具/MCP使用、记忆和多智能体适应等关键能力。
3. 多平台环境RL扩展：提出MRPO新算法，解决多平台冲突和长时任务训练效率低下的问题。

Result: 在开源模型中实现20多个GUI基准测试的最先进结果：GUI自动化任务在OSWorld、AndroidWorld和WebArena上分别达到56.5、71.6和48.4分；定位任务在ScreenSpotPro上达到80.3分；工具调用任务在OSWorld-MCP和MobileWorld上分别达到47.6和46.8分；记忆知识任务在GUI-Knowledge Bench上达到75.5分。

Conclusion: GUI-Owl-1.5通过三大创新有效提升了多平台GUI智能体的性能，已开源并提供在线云沙盒演示，推动了GUI智能体技术的发展。

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [117] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: 本文提出OpenSage，首个能让LLM自动创建具有自生成拓扑结构和工具集的智能体的开发套件，提供分层图结构记忆系统和软件工程专用工具包，在三个基准测试中优于现有ADK，推动智能体开发从人类中心向AI中心范式转变。


<details>
  <summary>Details</summary>
Motivation: 现有智能体开发套件(ADKs)要么功能支持不足，要么依赖人工手动设计智能体拓扑结构、工具集和记忆模块，这限制了智能体的泛化能力和整体性能。

Method: 提出OpenSage，支持LLM自动生成智能体拓扑和工具集，提供创建管理子智能体和工具包的功能；采用分层图结构记忆系统实现高效管理；配备面向软件工程任务的专用工具包。

Result: 在三个先进基准测试和多种骨干模型上的广泛实验表明，OpenSage相比现有ADK具有显著优势；严格的消融研究验证了各组件设计的有效性。

Conclusion: OpenSage为下一代智能体开发铺平道路，实现从人类中心到AI中心范式的转变，使智能体能够自主构建和优化自身结构。

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [118] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: 该论文提出AgentLAB，首个专门评估LLM智能体在长期交互中遭受自适应攻击风险的基准测试。该基准涵盖5种新型攻击类型（意图劫持、工具链攻击、任务注入、目标漂移、记忆投毒），在28个真实环境和644个安全测试用例上评估发现现有智能体高度脆弱，且单次交互的防御机制无法有效应对长期威胁。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在复杂长期环境中的应用扩展，其面临的多轮用户-智能体-环境交互攻击风险日益凸显。这类长期攻击可利用持续交互实现单次交互中无法达成的目标，但现有研究缺乏专门针对此类风险的评估基准。

Method: 开发AgentLAB基准测试框架，包含五种创新攻击类型（意图劫持、工具链攻击、任务注入、目标漂移、记忆投毒），构建28个真实智能体环境和644个安全测试用例，系统评估主流LLM智能体的长期攻击脆弱性。

Result: 实验表明代表性LLM智能体对长期攻击仍然高度易受攻击；为单次交互设计的现有防御措施无法可靠缓解长期威胁，智能体安全性存在显著差距。

Conclusion: AgentLAB为跟踪实际场景中LLM智能体安全防护进展提供了首个标准化基准，将推动针对长期交互场景的智能体安全研究，促进更鲁棒的智能体防御机制发展。代码已开源。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [119] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 提出一种将形式化验证与深度学习图像检索相结合的新框架，通过图验证与神经代码生成的协同，解决复杂关系、约束类自然语言查询的可靠性问题，使检索结果可验证、可解释。


<details>
  <summary>Details</summary>
Motivation: 当前基于嵌入向量的自然语言图像检索在处理涉及复杂关系、对象组合、精确约束（如身份、数量、比例）的查询时仍存在局限，向量表示固有的模糊性和近似性导致结果不可靠、不可验证。

Method: 构建形式化验证与深度学习的混合框架：采用图基验证方法对检索内容进行结构化推理，结合神经代码生成将自然语言查询转化为可执行验证逻辑，显式检验用户查询中的每个原子事实。

Result: 不仅能返回匹配结果，还能标识查询中哪些具体约束被满足、哪些未满足，实现了透明可问责的检索过程，同时提升了主流嵌入基方法的性能。

Conclusion: 该框架通过形式化推理为检索结果提供可验证性，突破了向量表示的模糊性瓶颈，为复杂自然语言查询提供了更可靠、可解释的解决方案。

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [120] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: 本文提出LLM-Wikirace基准测试，通过维基百科链接导航任务评估大语言模型的规划、推理和世界知识能力。前沿模型在简单关卡表现超人类，但在困难关卡成功率骤降（最佳模型仅23%），暴露出长时规划和失败恢复能力的严重不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂规划和推理任务上的能力尚未得到充分评估。现有基准多关注知识或短程推理，缺乏对长时规划、世界知识和前瞻推理的综合测试，亟需能揭示模型局限性的新基准。

Method: 设计LLM-Wikirace基准，要求模型从给定源页面通过维基百科超链接逐步导航至目标页面。任务需前瞻性规划和对现实世界概念关联的推理能力，设置不同难度级别以评估模型表现。

Result: 评估Gemini-3、GPT-5、Claude Opus 4.5等模型显示：简单关卡表现优异（超人类水平），但困难关卡成功率急剧下降至23%。世界知识是成功基础但存在收益阈值，超过后规划和长时推理成为决定性因素。轨迹分析揭示强模型在失败后难以重新规划，频繁陷入循环。

Conclusion: LLM-Wikirace作为简单而有效的基准，清晰揭示了当前前沿模型在复杂规划和推理方面的根本局限性，为未来具备更强规划能力的大语言模型发展提供了明确的挑战方向和开放测试平台。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [121] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: 针对推荐系统研究中内存实验便捷性与工业级分布式引擎复杂性之间的鸿沟，本文提出WarpRec——一个后端无关的高性能框架。该框架集成50+先进算法、40种评估指标和19种数据处理策略，支持从本地到分布式的无缝迁移，并通过CodeCarbon实现能耗追踪，为可持续、面向智能体的下一代推荐系统提供架构基础。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统创新受限于割裂的生态系统：研究人员面临两难选择——要么使用便捷但不适用于工业的内存实验，要么为分布式工业引擎进行昂贵且复杂的重写。这种学术界与工业界之间的鸿沟阻碍了算法的实际应用和快速发展。

Method: 提出WarpRec框架，采用创新的后端无关架构，消除上述权衡。框架包含三大核心组件：(1) 50+最先进算法库；(2) 40种评估指标；(3) 19种过滤和分割策略。通过统一接口实现从本地执行到分布式训练的无缝过渡，并集成CodeCarbon进行实时能耗追踪。

Result: WarpRec成功桥接了学术界与工业界的鸿沟，使研究人员无需重写代码即可将算法直接部署到分布式环境。框架通过能耗追踪证明可扩展性不必牺牲科学诚信和可持续性。同时，框架为推荐系统向生成式AI生态中的交互式智能体工具演进做好了准备。

Conclusion: WarpRec不仅解决了当前推荐系统研发的生态系统割裂问题，更为下一代可持续、面向智能体的推荐系统提供了核心架构。其开源特性将进一步推动领域内的协作创新，加速从算法研究到工业部署的转化进程。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [122] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 研究揭示了在持续学习的多模态智能体中，对有害数据进行微调会导致严重的突发性错位，这种错位会跨任务和模态泛化。实验表明，即使是10%的有害数据也会造成显著影响，且多模态评估比纯文本评估更能揭示对齐退化问题。几何分析发现有害行为集中在低维子空间，而现有的缓解策略（良性微调和基于激活的 steering）虽能减轻但无法完全消除有害行为。


<details>
  <summary>Details</summary>
Motivation: 终身多模态智能体需要通过持续的后训练来适应新任务，但这在获取新能力和保持安全对齐之间产生了根本性矛盾。当前的后训练范式可能无法在部署后有效保持模型对齐，这构成了重要的安全隐患，亟需系统性的实证研究来揭示其风险特征。

Method: 在Gemma3-4B模型上进行受控实验，采用LoRA低秩自适应微调技术，通过系统调整秩大小和有害数据比例来量化对齐退化。使用多模态与纯文本双重评估体系，并运用几何分析方法（主成分分析）探究有害行为在参数空间中的表征结构。评估了良性窄域微调和激活空间 steering 两种缓解策略的有效性。

Result: 1. 有害数据微调引发的错位随LoRA秩单调递增，多模态评估下错位数（r=128时达70.71±1.22）显著高于纯文本评估（41.19±2.51）；2. 仅10%的有害数据就会导致严重的对齐退化；3. 有害行为占据极低维子空间，10个主成分即可捕获大部分错位信息；4. 良性微调和激活 steering 均能有效减轻错位，但无法完全消除已习得的有害行为模式。

Conclusion: 当前后训练范式在终身学习场景下难以维持模型安全对齐，需开发更鲁棒的持续学习框架。多模态安全评估比单模态评估更全面，有害行为的低维特性为未来防御提供了新思路，但现有缓解策略仍不充分，无法根除后部署环境中的对齐风险。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [123] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 针对多智能体信息检索中的链式思维推理评估问题，本文提出可重用性与可验证性两个新指标，通过思考者-执行者框架解耦推理生成与执行，发现这些指标与准确率无相关性，且专用推理模型的推理链不一定优于通用大语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前链式思维评估仅关注目标任务准确率，无法评估推理过程本身的质量或实用性。

Method: 提出思考者-执行者框架，将CoT生成与执行解耦；定义可重用性（执行者复用思考者CoT的难易程度）和可验证性（执行者使用CoT匹配思考者答案的频率）两个新评估指标；在5个基准测试上评估4个思考者模型与10个执行者模型的组合。

Result: 可重用性和可验证性与标准准确率无相关性，暴露了当前准确率排行榜在推理能力评估上的盲点；专用推理模型的CoT并不总比Llama、Gemma等通用大语言模型更具可重用性或可验证性。

Conclusion: 准确率不能全面反映CoT推理质量，需引入可重用性和可验证性等过程性评估指标；通用大语言模型在推理可重用性方面可能具有与专用模型相当甚至更好的表现。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [124] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是一个CLEF评估实验室，专注于从嘈杂的多语言历史文本中提取人物-地点关系。它在HIPE-2020和HIPE-2022的基础上，扩展了语义关系提取任务，要求系统分类两种关系类型（$at$和$isAt$），并引入了三重评估体系（准确性、计算效率、领域泛化），旨在支持数字人文领域的下游应用。


<details>
  <summary>Details</summary>
Motivation: 历史文献中的人物-地点关系提取对于构建知识图谱、重建历史传记和进行空间分析等数字人文应用至关重要。然而，历史文本具有多语言、嘈杂和时空信息复杂等特点，现有技术面临挑战。HIPE-2026旨在通过标准化评估推动该领域技术进步。

Method: 该研究采用评估实验室的方法论，设计了两类关系分类任务（$at$和$isAt$），要求系统处理多语言、跨时期的历史文本，并进行时空推理。评估采用三重指标：准确性、计算效率和领域泛化能力。

Result: 虽然摘要未提及具体实验结果，但HIPE-2026建立了首个专注于历史人物-地点关系提取的多语言评估基准，为研究者提供了标准化的任务定义、数据集和评估框架，将促进该领域的技术发展。

Conclusion: HIPE-2026通过链接大规模历史数据处理与关系提取，为数字人文领域提供了重要基础设施。其三重评估体系平衡了性能、效率和泛化能力，将推动历史信息提取技术的实用化和标准化发展，对知识图谱构建和数字人文研究具有重要意义。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


### [125] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 本研究提出GAP基准测试框架，揭示了大语言模型在文本层面的安全对齐无法有效迁移到工具调用层面的安全行为，即使模型在文本上拒绝有害请求，仍可能通过工具调用执行危险操作，这一现象在六个前沿模型和六个监管领域中被系统性地验证。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全评估主要关注文本级别的有害内容拒绝行为，但作为智能体的模型越来越多地通过工具调用与外部系统交互并产生现实世界影响。这留下了一个关键的安全盲点：文本层面的对齐是否能有效抑制模型的有害行动？当前研究缺乏对文本安全与工具调用安全之间差异的系统性测量。

Method: 研究引入GAP基准测试框架，采用多因素实验设计：测试六个前沿模型，覆盖六个受监管领域（医药、金融、教育、就业、法律、基础设施），每个领域包含七个越狱场景，三种系统提示条件（中性、安全强化、工具鼓励）以及两种提示变体，共计产生17,420个可分析数据点。通过形式化定义GAP指标来量化文本安全响应与危险工具调用之间的分歧。

Result: 核心发现：文本安全无法迁移到工具调用安全。所有六个模型均出现文本拒绝有害请求但同步执行违禁工具调用的GAP现象；即使在安全强化系统提示下，六个模型中仍存在219例此类分歧案例。系统提示措辞对工具调用行为影响显著：最稳健模型的TC-safe率跨度达21个百分点，最敏感模型达57个百分点，18组配对比较中有16组在Bonferroni校正后仍保持显著性差异。运行时治理合约虽能减少信息泄露，但对违禁工具调用尝试本身无 detectable 威慑效果。

Conclusion: 文本-only安全评估不足以评估智能体行为风险，工具调用安全需要独立且专门的测量与缓解机制。研究结果表明，当前模型存在"说一套做一套"的安全隐患，未来安全框架必须将工具调用行为作为核心评估维度。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [126] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: 针对高覆盖硬件验证中在线强化学习因执行反馈昂贵缓慢而不切实际的问题，本文提出LLM4Cov离线智能体学习框架，通过执行验证数据筛选、策略感知智能体数据合成和最差状态优先采样三项技术，使40亿参数模型达到69.2%覆盖率，超越教师模型5.3%且性能媲美十倍规模模型。


<details>
  <summary>Details</summary>
Motivation: 执行感知的大语言模型智能体虽能从工具反馈中学习，但反馈获取成本高昂、速度缓慢，导致在线强化学习难以实用化。高覆盖硬件验证是典型挑战场景，因其依赖工业级模拟器且执行信号不可微。

Method: 提出LLM4Cov离线智能体学习框架，将验证过程建模为确定性评估器引导的无状态转移问题。核心方法包括：执行验证数据筛选确保数据质量；策略感知智能体数据合成实现可扩展学习；最差状态优先采样提升学习效率。同时构建基于现有验证套件改进的现实对齐基准测试。

Result: 实验表明，使用该框架训练的40亿参数模型在智能体评估下达到69.2%覆盖率通过率，较教师模型提升5.3个百分点，且性能与参数量大一个数量级的模型相当。

Conclusion: LLM4Cov通过创新的离线学习策略有效解决了执行约束下的可扩展学习难题，证明小模型在智能体辅助下可在硬件验证任务上超越大模型，为资源受限场景提供了实用化解决方案。

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [127] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: 本文提出Phantom，一种基于结构化模板注入的自动化LLM智能体劫持框架。该方法通过向检索上下文注入优化的结构化模板来诱导角色混淆，使智能体将恶意内容误判为用户指令或工具输出。相比现有手动语义驱动攻击，Phantom显著提升了攻击成功率（ASR）和黑盒可转移性，并在真实商业产品中发现了70多个已确认的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有智能体劫持攻击多依赖手工构造的语义驱动提示，存在攻击成功率低、对闭源商业模型可转移性差的问题。OWASP已将此列为LLM生态的关键威胁。研究动机在于突破手动攻击的局限性，开发一种自动化、高效且可跨模型迁移的结构化攻击方法，以揭示智能体架构层面的根本性安全风险。

Method: 核心方法是结构化模板注入：利用智能体依赖特定聊天模板令牌来区分系统、用户、助手和工具指令的架构特性，通过注入优化的结构化模板诱发角色混淆。为实现黑盒攻击可转移性，提出分层模板增强增加结构多样性，训练模板自动编码器（TAE）将离散模板嵌入连续可搜索的潜在空间，并采用贝叶斯优化高效识别最优对抗向量，最终解码为高效能结构化模板。

Result: 在Qwen、GPT和Gemini上的大量实验表明，该框架在攻击成功率（ASR）和查询效率上均显著优于现有基线方法。更重要的是，研究者在真实商业产品中识别出超过70个漏洞，且均获得厂商确认，验证了结构化模板劫持的实际危害性和普遍性。

Conclusion: 结构化模板注入攻击对LLM智能体构成严重实际威胁，Phantom框架不仅证明了此类攻击的可行性，还通过大规模实证研究为构建更安全的下一代智能体系统提供了经验基础和安全参考。

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [128] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 该论文证明黑盒评估无法可靠估计具有潜在上下文条件的AI模型部署风险，建立了统计和计算层面的根本性限制，为额外安全措施提供了数学依据。


<details>
  <summary>Details</summary>
Motivation: 挑战黑盒安全评估的核心假设——测试分布行为可预测部署性能。针对评估时罕见但部署时普遍存在的未观测内部变量（潜在上下文），形式化分析其对安全保证的威胁，揭示分布偏移下的评估不确定性。

Method: 采用Le Cam方法建立被动评估的极小极大下界；利用哈希触发器构造和Yao极小极大原理分析自适应查询；基于陷门单向函数的密码学假设构建计算分离结果；对白盒探测进行显式偏差校正与样本复杂度分析。

Result: 被动评估：任何估计量的期望绝对误差 ≥ (5/24)δL ≈ 0.208δL；自适应评估：完全自适应下最坏误差仍 ≥ δL/16，检测需Θ(1/ε)次查询；计算分离：多项式时间评估器无法区分部署环境利用特权信息激活的不安全行为；白盒探测：达到精度ε_R需O(1/(γ²ε_R²))样本量，其中γ = α₀ + α₁ - 1。

Conclusion: 黑盒测试在潜在上下文依赖下统计不确定，无法提供可靠安全保证。研究量化了传统评估的局限性，为架构约束、训练时保证、可解释性和部署监控等额外保护措施提供了数学必要性的明确标准，强调最坏情况安全 assurance 必须超越黑盒范式。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [129] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe是一个对话式纵向股票推荐基准，通过区分描述性行为与基于投资者风险偏好的规范性效用，评估大语言模型的决策质量而非简单行为模仿，揭示了理性决策质量与行为对齐之间的持续张力。


<details>
  <summary>Details</summary>
Motivation: 现有推荐基准过度依赖用户行为模仿作为评估标准，但在金融咨询领域，观察到的用户行为在市场波动下可能是嘈杂或短视的，与用户长期目标相冲突，导致行为模仿与决策质量混淆。

Method: 提出Conv-FinRe基准，基于真实市场数据和人类决策轨迹构建。给定用户访谈、分步市场环境和咨询对话，要求模型在固定投资期限上生成股票排名。提供多视角参考，区分描述性行为与基于投资者特定风险偏好的规范性效用，实现诊断模型是理性分析、模仿用户噪声还是受市场动量驱动。

Result: 实验发现理性决策质量与行为对齐之间存在持续张力：基于效用的排名表现良好的模型往往无法匹配用户选择，而行为对齐的模型容易过拟合短期噪声。评估了一系列先进大语言模型并揭示了这一现象。

Conclusion: 该基准为金融推荐系统提供了更科学的评估框架，区分了行为模仿与真实决策能力。数据集已在Hugging Face公开，代码已在GitHub开源，为后续研究提供了基础资源。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [130] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: 针对时间序列数据库自然语言查询(NLQ4TSDB)问题，本文提出 Sonar-TS 神经符号框架，采用搜索-验证双阶段流程处理连续形态意图和超长历史记录挑战，并发布首个大规模基准 NLQTSBench。


<details>
  <summary>Details</summary>
Motivation: 现有 Text-to-SQL 方法专为结构化查询设计，无法捕捉连续形态意图（如形状模式或异常检测），而传统时间序列模型在处理 TB 级超长历史数据时存在效率与精度瓶颈。非专业用户急需自然语言接口从海量时序数据中提取事件、区间和摘要。

Method: Sonar-TS 框架创新性地采用类主动声纳的搜索-验证范式：搜索阶段通过特征索引快速定位候选时间窗口（使用 SQL），验证阶段生成可执行 Python 程序对原始信号进行精确匹配与验证，实现神经符号协同。

Result: 在自建 NLQTSBench 基准上的实验揭示了 NLQ4TSDB 的独特挑战，证明 Sonar-TS 在复杂时序查询任务上显著优于传统方法，特别是在形态识别和长程依赖处理方面。

Conclusion: 本研究首次系统性地定义并探索了 NLQ4TSDB 领域，提出的 Sonar-TS 框架与 NLQTSBench 基准为未来研究建立了理论基础和实践标准，将推动时序数据交互技术的进一步发展。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [131] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: 本文提出Cinder系统，一种针对多人在线游戏中异质技能水平大厅的两阶段公平匹配方案。系统先通过Ruzicka相似性指数快速筛选候选配对，再基于倒置正态分布的非线性技能桶映射与坎托罗维奇距离计算制裁分数，最终通过分析1.4亿次模拟配对验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 公平高效的匹配系统直接影响玩家留存与满意度。传统基于平均技能指标的方法难以应对技能分布异质性，导致比赛失衡。现有系统缺乏对技能分布形态特征的有效捕捉与量化评估。

Method: Cinder采用两阶段架构：1）初筛阶段：利用Ruzicka相似性指数比较大厅非异常值技能范围，实现快速过滤；2）精评估阶段：将玩家段位映射至倒置正态分布生成的非线性技能桶，采用坎托罗维奇距离计算排序后桶索引的分布差异，输出量化公平性的"制裁分数"。

Result: 通过对1.4亿个模拟大厅配对的制裁分数分布进行分析，验证了系统可行性，为制定公平匹配阈值提供了稳健的数据基础。

Conclusion: Cinder系统通过结合快速过滤与精确度量，有效解决了异质技能大厅的公平匹配难题，为游戏匹配系统提供了可扩展的量化评估框架，显著提升了匹配质量与玩家体验。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [132] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: 针对LLM智能体长周期运行中重复加载长系统指令和工具目录导致成本高、错误率高等问题，本文提出指令-工具检索(ITR)方法，通过动态检索最小化提示和工具子集，实现上下文token减少95%、工具路由准确率提升32%、成本降低70%，使智能体可运行2-20倍更多循环。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在长周期运行中每步重复加载冗长系统指令和大型工具目录，导致成本增加、智能体偏离概率上升、延迟增加及工具选择错误。

Method: 提出指令-工具检索(ITR)——一种RAG变体，每步仅检索最小系统提示片段和最必要的工具子集，动态组合运行时系统提示，并通过置信度门控提供narrowed工具集与回退机制。

Result: 在受控基准测试中，相比单体基线：每步上下文token减少95%，正确工具路由相对提升32%，端到端episode成本降低70%，使智能体在上下文限制内运行2-20倍更多循环，且节省效果随步数复合增长。

Conclusion: ITR显著提升了长周期自主智能体的效率与可扩展性，其节省效果随运行步数复合增长。论文提供了完整的方法、评估协议、消融实验及部署指导，对实际应用具有重要价值。

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [133] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: 针对合作式多智能体强化学习价值分解问题，提出S2Q方法，通过学多个子价值函数保留高价值备选动作，结合Softmax策略实现持续探索与快速适应，在基准测试中性能显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有价值分解方法依赖单一最优动作，当训练过程中底层价值函数发生变化时，难以快速适应，易收敛至次优策略，限制了算法的适应性和性能。

Method: 提出连续子价值Q学习(S2Q)，通过学多个子价值函数来保留替代性高价值动作，将这些子价值函数整合到基于Softmax的行为策略中，从而鼓励持续探索并实现Q^tot对变化最优值的快速适应。

Result: 在具有挑战性的多智能体强化学习基准测试上，S2Q在所有对比算法中表现持续最优，验证了其优越的适应性和整体性能。

Conclusion: S2Q通过多子价值函数和Softmax策略的协同机制，有效克服了单最优动作依赖的局限性，显著提升了合作式多智能体强化学习在动态环境中的适应能力和学习性能。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [134] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: ...


<details>
  <summary>Details</summary>
Motivation: ...

Method: ...

Result: ...

Conclusion: ...

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [135] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 为解决不同ESG评级机构对同一公司评分差异大的问题，本文提出STRIDE和SR-Delta双模块人机协作框架，通过LLM构建可信基准数据集并分析评级差异，以实现可持续评级的可扩展、可比较评估。


<details>
  <summary>Details</summary>
Motivation: 当前ESG评级机构使用不同方法论导致对同一公司的评分差异巨大，降低了评级的可比性、可信度和决策价值，亟需统一标准以提升评级质量。

Method: 提出由STRIDE和SR-Delta组成的通用人机协作框架：STRIDE提供基于原则的准则和评分系统，利用大语言模型构建企业级基准数据集；SR-Delta作为差异分析流程框架，识别潜在调整空间。

Result: 该框架能够实现对可持续评级方法论的可扩展、可比较评估，生成可信的基准数据集，并为方法论改进提供分析依据。

Conclusion: 呼吁AI社区采用AI驱动方法强化可持续评级方法论，以支持和推动紧迫的可持续发展议程。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [136] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 该论文针对视觉任务中像素特征的空间语义依赖性问题，提出O-Shap方法，利用满足T性质的新分割策略改进Owen值（分层Shapley值）的归因精度、语义一致性与运行效率。


<details>
  <summary>Details</summary>
Motivation: 标准Shapley值方法假设特征独立，但视觉任务中像素存在强空间语义依赖。现代SHAP实现采用Owen值支持分组归因，其效果取决于特征分组定义，而常用分割方法会破坏关键一致性性质，亟需一种能保持语义对齐的分层归因框架。

Method: 提出一种满足T性质的分割方法，确保不同层级间的语义对齐。基于此分层结构应用Owen值进行组归因，实现计算剪枝，同时提升归因准确性与可解释性。

Result: 在图像和表格数据集上的实验表明，O-Shap在归因精度、语义一致性和运行效率方面均优于基线SHAP变体，尤其在结构重要的任务上优势显著。

Conclusion: O-Shap通过T性质分割策略有效解决了视觉任务中特征依赖性问题，为可解释AI提供了理论一致、计算高效且语义合理的分层归因新方法。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [137] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: 本文提出InstructKG框架，旨在从课程讲义材料中自动构建能够捕获概念间学习依赖关系（如先修、子概念）的知识图谱，以支持大规模课程中的个性化学习诊断与干预。


<details>
  <summary>Details</summary>
Motivation: 掌握教育概念需理解其先修依赖（如归并排序需先学递归）和子概念结构（如归并排序属于排序算法），这对识别学生知识缺口和实施个性化干预至关重要。然而，现有知识图谱方法要么仅停留在课程级概念或逻辑关系层面，要么忽视教学材料中蕴含的丰富教学信号，难以满足大规模课程的精准诊断需求。

Method: 该框架融合教育材料特有的时序与语义信号（如教学顺序、概念定义中的引用）与大语言模型的泛化能力，自动从讲义中提取核心概念作为节点，并推断"部分于"、"依赖于"等有向依赖关系作为边，构建与教师意图对齐的知识图谱。

Result: 在跨多门课程的真实讲义实验与人工评估中，InstructKG成功捕获了丰富的、与教师设计一致的学习进阶路径。

Conclusion: 该方法为实现大规模教育场景下的精准知识诊断和个性化教学干预提供了有效工具。

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [138] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 针对困难CircuitSAT实例，本文提出一种参数化并行分解算法，通过专用约束将问题分割为弱化公式族，并利用并行硬度估计指导参数调整，在逻辑电路等价性验证与密码哈希原像攻击实例中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: CircuitSAT问题在硬件设计与密码分析中具有关键应用，但许多实例极难求解。现有方法效率不足，亟需高效分解策略以加速求解过程。

Method: 设计新型并行算法，采用专用约束对原始SAT实例进行分区，生成弱化公式族。算法采用参数化框架，通过并行计算的硬度估计动态指导参数选择，实现高质量分解的识别。

Result: 在挑战性CircuitSAT实例上成功验证了算法实用性，具体应用于布尔电路逻辑等价性检查与密码哈希函数的原像攻击问题求解。

Conclusion: 该并行分解算法为困难CircuitSAT实例求解提供了有效参数化方案，在电路验证与密码分析领域展现出显著应用价值。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [139] [Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning](https://arxiv.org/abs/2602.17145)
*Joseph Bingham,Sam Helmich*

Main category: cs.AI

TL;DR: 针对CNN剪枝缺乏统一实现框架的问题，本文提出Combine标准剪枝方案。在VGG模型上实现79%滤波器剪枝且保持精度，计算量降低68%。


<details>
  <summary>Details</summary>
Motivation: CNN模型日益增长的计算资源需求（规模、时延、内存、功耗）与现有剪枝方法缺乏标准化实现之间的矛盾，导致方法难以复现和比较。

Method: 提出Combine迭代剪枝框架，建立标准函数比较的统一语言，设计新型标准函数，并系统研究不同标准在不同模型上的差异化影响。

Result: 在VGG类模型上验证，Combine可剪枝79%滤波器同时保持或提升准确率，计算量减少68%，证实不同标准函数具有模型特异性效果。

Conclusion: Combine框架为CNN剪枝提供了标准化、可扩展的解决方案，推动了剪枝方法的系统化研究，为未来算法比较和优化奠定基础。

Abstract: As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\%.

</details>


### [140] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: 本文提出JEPA-DNA，一种融合联合嵌入预测架构与生成式目标的新型基因组基础模型预训练框架。该方法通过CLS令牌监督，在潜空间预测被掩码基因组片段的高级功能嵌入而非单个核苷酸，从而捕获全局功能上下文。该框架可作为独立训练目标或现有模型的持续预训练增强，在多种基因组基准测试中表现优于纯生成式基线。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型主要依赖掩码语言建模或下一令牌预测，虽能捕捉局部语法和细粒度基序模式，但往往无法捕获更广泛的功能上下文，导致表征缺乏全局生物学视角。这种局限性阻碍了模型对基因组功能逻辑的深入理解。

Method: 研究团队将联合嵌入预测架构与传统生成式目标集成，引入潜在接地机制：通过监督CLS令牌，在潜空间执行预测性目标，强制模型预测被掩码基因组片段的高级功能嵌入而非单个核苷酸。该方法既可作为从头开始的独立训练目标，也可作为现有基因组基础模型的持续预训练增强。

Result: 在多样化的基因组基准测试中，JEPA-DNA在监督和零样本任务上持续优于纯生成式基线，表明其能够产生更强大且生物学上更合理的基因组表征。

Conclusion: JEPA-DNA通过提供生物学基础更扎实的表征，为开发不仅理解基因组字母表、更能理解序列背后功能逻辑的基础模型提供了可扩展路径，代表了基因组预训练方法的重要进展。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [141] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: 本文提出Texo，一个仅含2000万参数的轻量级高性能公式识别模型。通过架构优化、知识蒸馏及词汇表/分词器迁移，Texo在性能上与SOTA模型相当，但模型大小相比UniMERNet-T和PPFormulaNet-S分别缩减80%和65%，支持消费级硬件实时推理及浏览器部署，并配套开发了Web应用。


<details>
  <summary>Details</summary>
Motivation: 现有公式识别模型参数量庞大，难以在消费级硬件和浏览器环境中高效部署与实时运行。本文旨在设计一个轻量化模型，降低计算资源需求，提升可访问性和应用范围。

Method: 采用精心设计的轻量架构，结合知识蒸馏技术和词汇表、分词器的迁移学习，构建仅2000万参数的模型Texo。

Result: Texo在公式识别性能上与SOTA模型UniMERNet-T和PPFormulaNet-S相当，模型大小分别减少80%和65%，成功实现消费级硬件实时推理及浏览器端部署。

Conclusion: 本研究成功开发了高效轻量的公式识别系统，显著降低了模型体积和部署门槛，为终端用户提供了实用的Web应用，拓展了公式识别技术的实际应用场景。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [142] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本研究作为"方法论实验"，针对生成式AI在人文社科研究中的应用空白，设计了一个基于AI Agent的七阶段协作工作流程，并以台湾Claude.ai使用数据为实证案例验证其可行性，提出了三种人机协作模式并强调了人类判断的不可替代性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI研究过度集中于软件工程和自然科学领域，缺乏针对人文社科研究方法论的系统性探索，亟需构建可复制的人机协作框架以推动该领域的方法论创新。

Method: 研究采用双层设计：主要层面构建基于任务模块化、人机分工与可验证性三原则的七阶段模块化工作流程，明确界定人类研究者（研究判断与伦理决策）与AI Agent（信息检索与文本生成）的角色；次要层面利用Anthropic经济指数中的台湾Claude.ai使用数据（N=7,729次对话，2025年11月）进行二次数据分析，通过反思性文档记录实现工作流程的操作化演示。

Result: 研究提出了可复制的人AI协作框架，识别出直接执行、迭代优化和人为主导三种协作模式，并通过过程性反思揭示人类判断在研究问题构建、理论阐释、情境化推理与伦理反思中的核心不可替代作用。

Conclusion: 本研究为人文社科研究者提供了可操作的AI协作方法论，强调人类研究者仍应在关键认知与价值判断环节保持主导地位，同时承认单平台数据、横截面设计与AI可靠性风险等局限性，为未来方法优化指明方向。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [143] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 本文针对大语言模型回溯测试中的时间知识泄露问题，提出基于沙普利值的声明级泄露检测框架（Shapley-DCLR）和TimeSPEC方法。通过分解推理为原子声明并验证其时间可追溯性，该方法能显著降低泄露同时保持预测性能，为可靠评估模型预测能力提供了有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型预测未来事件能力需要回溯测试，但模型训练时编码的后截止日期知识可能泄露，污染评估结果。现有方法缺乏系统性的时间知识泄露检测与量化机制。

Method: 提出声明级分析框架：1）将模型推理分解为原子声明并依时间可验证性分类；2）运用沙普利值计算各声明对预测的贡献度，得出Shapley-DCLR泄露率指标；3）开发TimeSPEC方法，在生成过程中嵌入声明验证与再生环节，确保所有支持性声明均来自截止日期前数据源。

Result: 在美国最高法院案件预测、NBA薪资估计和股票收益排名三项任务共350个实例的实验表明，标准提示存在严重时间泄露。TimeSPEC显著降低Shapley-DCLR指标，同时维持任务性能，证明显式声明验证优于提示约束。

Conclusion: 声明级显式验证是确保大语言模型回溯测试有效性的关键。TimeSPEC通过主动过滤时间污染，为模型预测能力评估建立了可靠框架，对发展可信AI具有重要意义。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [144] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本文详细记录了从原始arXiv LaTeX源文件训练13.6亿参数科学语言模型的全流程工程实践，涵盖数据处理、tokenization和受限算力（2块A100）下的训练过程，通过24次实验揭示了预处理、tokenization和存储I/O对模型训练的关键影响。


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型虽具备强大推理和数学能力，但从原始数据训练领域专用科学模型的具体实践仍缺乏透明记录。现有研究多关注架构创新，而工程实践细节对资源有限的研究者至关重要。

Method: 构建端到端流水线：arXiv元数据过滤、存档验证、LaTeX解析、文本归一化、领域感知tokenization，以及在2块A100 GPU上训练密集Transformer模型。通过24次实验系统分析训练稳定性、扩展行为、数据损耗和基础设施瓶颈。

Result: 发现预处理决策显著影响可用token数量，tokenization方式对符号稳定性至关重要，存储和I/O约束可与计算资源成为同等限制因素。在52B token的富数据 regime 下展现出稳定的训练行为和收敛动态。

Conclusion: 本研究提供了基于工程实践的小型科学语言模型训练透明化记录，强调系统优化而非架构创新的重要性，为中等算力预算的研究者构建领域专用模型提供可复现的经验指导。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [145] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 本文针对任务向量组合中的跨任务干扰和表示漂移问题，提出一种无需数据的正则化方法。通过将正则化框架化为曲率矩阵近似问题，并采用Kronecker-Factored Approximate Curvature技术，实现了任务添加和否定中的最先进性能，同时具有常数级任务复杂度和对向量缩放的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有任务算术方法在组合多个任务向量时会产生跨任务干扰，导致表示漂移和性能下降。虽然表示漂移正则化可解耦任务向量，但通常需要外部任务数据，这与模块化设计原则及数据隐私约束相冲突，因此亟需开发无需数据的解决方案。

Method: 将表示漂移正则化重新表述为曲率矩阵近似问题，借鉴成熟的优化技术，具体采用Kronecker-Factored Approximate Curvature (KFAC)方法构建实用正则化器。该方法无需访问额外任务数据，直接从模型参数变化中估计曲率信息，有效约束任务向量间的干扰。

Result: 在任务向量添加和否定操作中达到state-of-the-art性能；计算复杂度与任务数量呈常数关系；对任务向量重新缩放具有鲁棒性；完全消除了对保留验证集进行超参数调优的需求。

Conclusion: 所提出的无需数据正则化方法成功解决了任务算术中的表示漂移问题，在保持模块化的同时实现了高性能和计算效率，为隐私敏感场景下的可扩展模型适应提供了实用解决方案。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [146] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 本文提出多模态对比变分自编码器（MCVAE）解决NSCLC患者生存预测中的数据缺失难题，通过模态特定编码器、门控融合瓶颈和多任务学习目标，在TCGA数据集上显著提升预测性能与鲁棒性，并发现多模态整合并非总是有益。


<details>
  <summary>Details</summary>
Motivation: 非小细胞肺癌生存预测因个体预后差异而极具挑战性。尽管全切片图像、转录组学和DNA甲基化等多模态数据提供互补诊断信息，但真实临床数据集普遍存在严重模态缺失问题。现有模型在严重缺失场景下鲁棒性不足，亟需能够有效处理任意缺失模式并充分利用多模态信息的鲁棒预测框架。

Method: 提出多模态对比变分自编码器（MCVAE）：1）采用模态特定变分编码器量化各数据源不确定性；2）设计带可学习门控机制的融合瓶颈动态平衡当前模态贡献；3）构建多任务目标函数，联合优化生存损失、重构损失与跨模态对比损失以规约患者表征；4）训练阶段引入随机模态掩码策略，强制模型适应任意缺失模式。

Result: 在TCGA-LUAD（475例）和TCGA-LUSC（446例）数据集上的评估表明，MCVAE在预测疾病特异性生存期（DSS）方面显著优于两种先进基线模型，且在极端缺失场景下保持卓越鲁棒性。全面子集测试揭示：多模态整合并非总是提升性能，其效果依赖于模态组合与数据完整性。

Conclusion: MCVAE通过创新的变分编码架构、门控融合机制与随机掩码训练策略，有效解决了多模态数据缺失下的生存预测难题，在预测精度与鲁棒性方面取得突破。研究不仅为临床多模态数据分析提供了可靠工具，也揭示了模态整合的复杂性，为未来多模态医疗建模提供了重要理论洞察。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [147] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 本文针对儿童AI应用隐私风险问题，提出了一个基于Privacy-by-Design的框架，整合GDPR、COPPA等多国法规原则，并将其映射到LLM全生命周期各阶段，结合儿童权利公约与适龄设计规范，通过教育导师案例验证了框架在实践中的可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 随着儿童使用人工智能技术日益普及，其隐私风险引发严重关切。尽管GDPR、COPPA等法规明确要求保护措施，但企业在实际实施中面临挑战。因此，亟需构建一个主动、风险规避的设计框架，以系统性解决儿童AI应用的隐私合规难题。

Method: 采用Privacy-by-Design理念，整合欧盟GDPR、加拿大PIPEDA、美国COPPA等法规核心原则，系统性地映射到LLM应用的四个关键阶段（数据收集、模型训练、运行监控、持续验证）。同时融合联合国儿童权利公约、英国适龄设计规范及最新学术研究的儿童友好设计指南，形成综合框架，并通过LLM儿童教育导师案例进行实证分析。

Result: 构建了覆盖LLM全生命周期的隐私保护框架，为各阶段提供了具体的运营控制措施（如技术和组织控制）和年龄适宜设计决策指南。案例研究表明，该框架能够有效指导开发者在满足法律标准的同时降低隐私风险，实现儿童AI应用的合规开发。

Conclusion: 通过在LLM生命周期各阶段贯穿数据保护策略和适龄设计决策，所提出的框架能够支持开发既提供强隐私保护又符合法律要求的儿童AI应用程序，为平衡技术创新与儿童权益保护提供了可操作的实践路径。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [148] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 本文针对ARM Cortex-M0+、M4、M7处理器，提出一种实用AI模型优化基准测试框架，通过自动化测试平台系统评估嵌入式系统的能效、精度与资源利用率。研究揭示浮点运算量(FLOPs)与推理时间的近线性关系，运用帕累托分析平衡能耗与精度权衡，为开发者选择最优处理器-模型组合提供指导。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的嵌入式系统中部署AI模型时，需在ARM Cortex处理器上实现能效、精度与资源占用的最佳平衡。现有研究缺乏针对Cortex-M系列的系统性评估框架，本文旨在填补这一空白，为开发者提供量化的优化指导和决策依据。

Method: 设计自动化测试平台，对Cortex-M0+、M4、M7进行系统性基准测试，评估关键性能指标。通过分析FLOPs与推理时间的相关性，并采用帕累托前沿方法，识别不同处理器上能耗与模型精度的最优权衡点，从而确定最佳处理器与AI模型的匹配方案。

Result: 实验发现FLOPs与推理时间呈近线性相关，可作为计算需求预测指标。帕累托分析表明：M7处理器适合短推理周期场景；M4处理器在长推理任务中能效最优；M0+处理器虽不适用于复杂模型，但仍可满足简单任务需求，为处理器选型提供了量化依据。

Conclusion: 该基准测试框架为嵌入式AI系统开发者提供了实用工具和决策洞察，通过平衡能效与性能，指导开发者在实际应用中设计高能效、高性能的AI系统，促进AI在资源受限环境中的可持续发展。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [149] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: 本文提出KLong，一种开源LLM智能体，通过轨迹分割监督微调（SFT）进行冷启动，再通过渐进式强化学习（RL）扩展能力，专门解决超长时域任务。在PaperBench基准测试中，1060亿参数的KLong超越1万亿参数的Kimi K2 Thinking达11.28%。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型智能体在超长时域任务（需要长期规划、多步推理和持续执行）上的表现仍显不足，亟需专门训练方法以提升其处理复杂长期任务的能力。

Method: 设计Research-Factory自动化流水线，从Claude 4.5 Sonnet蒸馏构建数千条高质量长时域轨迹；提出轨迹分割SFT方法，保留早期上下文完整性，逐步截断后期上下文，并在子轨迹间设置重叠区域；采用渐进式RL训练策略，分多阶段进行并逐步延长任务超时时间限制；先通过综合SFT配方激活基础智能体能力，再进行规模化训练。

Result: KLong在PaperBench基准测试中表现优异，1060亿参数版本显著优于1万亿参数的Kimi K2 Thinking（提升11.28%），且在SWE-bench Verified和MLE-bench等代码智能体基准测试中展现出良好的泛化性能。

Conclusion: 该研究通过轨迹分割SFT与渐进式RL的结合，有效提升了开源LLM智能体在超长时域任务上的性能，为构建高性能开源智能体模型提供了新思路和方法论。

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [150] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: 针对电信领域复杂性和专业术语导致的通用大语言模型幻觉问题，本文提出KG-RAG框架，通过融合知识图谱与检索增强生成技术，在基准数据集上实现比RAG提升14.3%、比纯LLM提升21.6%的准确率，显著提升事实准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 电信领域因专业术语、复杂标准及知识动态演进，通用大语言模型在该场景下易产生幻觉且输出不可靠。现有方法难以保证符合电信规范，亟需一种结合结构化领域知识的解决方案以提升模型的准确性和可信度。

Method: 提出KG-RAG框架，将知识图谱与检索增强生成技术相结合。利用知识图谱从电信标准和技术文档中构建结构化领域知识库，通过RAG机制动态检索相关事实，以约束和锚定大语言模型的生成过程，确保输出符合领域规范。

Result: 在基准数据集上的实验表明，KG-RAG相比纯大语言模型基线平均准确率提升21.6%，相比标准RAG基线提升14.3%。该框架有效降低了幻觉现象，生成的输出具有更高的事实准确性、可靠性和可解释性。

Conclusion: KG-RAG通过知识图谱的结构化知识表示与RAG的动态检索能力，为电信领域的复杂场景提供了准确的、可靠的、可解释的解决方案。该方法显著优于现有基线，展示了在专业领域增强大语言模型的有效途径。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [151] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 该论文针对大语言模型激活 steering 缺乏统一理论框架和过度依赖单步 steering 的问题，提出了基于常微分方程（ODE）的统一理论框架，将 steering 方向识别转化为控制论中的 barrier function 设计问题，并由此提出 ODESteer 方法实现多步自适应 steering，在多个对齐基准测试中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前激活 steering 方法存在两大局限：一是缺乏统一的理论框架指导 steering 方向设计，二是过度依赖单步 steering 而无法捕捉复杂的激活分布模式。这限制了方法的效果和可解释性，亟需从理论上建立更严谨的框架并开发更强大的 steering 策略。

Method: 提出基于常微分方程的统一理论框架，将传统激活加法解释为 ODE 解的一阶近似，将 steering 方向识别问题转化为控制论中的 barrier function 设计问题。具体实现 ODESteer 方法：定义 barrier function 为正负激活的对数密度比，构建 ODE 实现多步自适应 steering，通过迭代优化激活轨迹来增强对齐效果。

Result: 在多个大语言模型对齐基准测试中，ODESteer 相比现有最优方法实现了一致的实证提升：TruthfulQA 提升 5.7%，UltraFeedback 提升 2.5%，RealToxicityPrompts 提升 2.4%，验证了 ODE 框架指导下的 multi-step 和 adaptive steering 的有效性。

Conclusion: 本研究通过 ODE 框架统一了激活 steering 的理论基础，为 steering 方向设计提供了 principled 的控制论视角。所提出的 ODESteer 方法不仅实现了多步自适应 steering，还在实证上取得了显著改进，为 LLM 对齐提供了新的理论见解和实用方法。

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [152] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 本文提出一种基于联邦学习的混合集成学习框架，结合SWIN Transformer与多种CNN模型(DenseNet201, Inception V3, VGG19)，用于X光图像的COVID-19和肺炎诊断，旨在实现安全分布式医疗数据处理与高精度疾病检测。


<details>
  <summary>Details</summary>
Motivation: 计算能力的显著进步为AI在医疗健康领域的应用创造了巨大机遇。医疗数据隐私和安全问题促使需要开发分布式系统，而联邦学习可以实现数据不出本地的情况下进行协作训练，特别适用于COVID-19和肺炎等肺部疾病的诊断需求。

Method: 提出混合联邦学习框架，集成SWIN Transformer和三种先进CNN模型(DenseNet201, Inception V3, VGG19)；采用TensorFlow/Keras平台开发；利用联邦学习的分布式特性进行实时持续学习，确保数据隐私。

Result: 通过集成学习方法提升疾病诊断准确率和严重程度预测能力；联邦学习机制保障了混合模型的安全性和信息真实性；在X光报告分析上实现COVID-19和肺炎的有效检测。

Conclusion: 该混合AI模型为医生提供了可靠的辅助诊断工具，实现了安全、高效的医疗数据分布式处理，有助于全球协同应对疫情挑战。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [153] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 该论文提出通过评估AI在所有人类游戏中学习和表现的能力来衡量其类人通用智能，介绍了AI GameStore平台，实验显示现有视觉语言模型在多数游戏中得分不足人类平均水平的10%。


<details>
  <summary>Details</summary>
Motivation: 在技术快速发展的时代，全面评估机器智能相对于人类通用智能的水平变得愈发重要且充满挑战。传统AI基准测试通常只评估狭窄的能力范围，且容易饱和。该论文主张需要更强大的评估方式来衡量AI的通用智能水平。

Method: 提出采用"通用游戏"的强形式作为评估方法，即研究AI如何及能在多大程度上学习和玩"所有可想象的人类游戏"。定义"人类游戏"为人类为人类设计的游戏。引入"人类游戏多元宇宙"概念。开发AI GameStore平台，利用大语言模型和人机协作，从流行游戏平台自动合成新游戏。

Result: 基于苹果应用商店和Steam排行榜生成100款游戏。评估7个前沿视觉语言模型在短时游戏环节的表现。最佳模型在大多数游戏中得分不足人类平均水平的10%，尤其在世界模型学习、记忆和规划类游戏中表现不佳。

Conclusion: 提出构建AI GameStore的后续步骤，将其作为衡量和推动机器向类人通用智能发展的实用工具。

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [154] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT是一个基于层次化离散扩散模型的分子图生成框架，通过引入化学先验和解耦原子编码，在MOSES数据集上首次实现了接近完美的化学有效性，并超越了1D基线模型，在多个指标上达到最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在分子生成中存在化学有效性低、难以满足期望性质的问题，性能不如1D建模方法，这限制了AI驱动的药物发现和材料科学的发展。

Method: 提出MolHIT框架，核心包括：(1) 层次化离散扩散模型，将离散扩散推广到编码化学先验的额外类别；(2) 解耦原子编码，根据化学角色拆分原子类型。

Result: 在MOSES数据集上首次实现接近完美的化学有效性，在所有指标上超越强大的1D基线模型，并在多性质引导生成和骨架扩展等下游任务中表现出色。

Conclusion: MolHIT成功克服了现有图扩散模型的性能限制，为分子生成提供了强大的新框架，在药物发现和材料科学应用中具有重要价值。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [155] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.OH

TL;DR: 针对量子计算对RSA加密的威胁，研究提出了一种混合安全框架，结合AES加密、BB84量子密钥分发、量子态比较和生物启发免疫系统，为后量子时代的数据保护提供可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: RSA加密的安全性依赖于大数分解的困难性，经典分解算法效率低下，而Shor量子算法可在多项式时间内破解RSA，对现有密码体系构成重大威胁，亟需开发后量子时代的加密解决方案。

Method: 设计了一个概念性的混合安全框架，包含四个核心组件：1) AES加密保障经典安全性；2) BB84量子密钥分发实现安全的密钥交换和窃听检测；3) 量子态比较进行轻量级认证；4) 仿生免疫系统实现自适应威胁检测。

Result: 研究表明RSA易受Shor算法攻击；BB84在理想条件下可实现完全密钥协商，并能高准确度检测窃听；该概念模型结合了经典和量子安全方法，为后量子加密数据保护提供了可扩展和自适应的解决方案。

Conclusion: 本工作主要提出了一个概念框架，详细的实现、安全性证明和广泛实验验证将作为未来工作进行。

Abstract: Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [156] [RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution](https://arxiv.org/abs/2602.16932)
*Jinming Nian,Fangchen Li,Dae Hoon Park,Yi Fang*

Main category: cs.IR

TL;DR: 本研究提出RankEvolve，一种利用大语言模型（LLM）引导的演化搜索自动发现改进的词汇检索算法的方法。该方法将排序算法表示为可执行代码，基于AlphaEvolve框架，从BM25和Dirichlet平滑查询似然模型两个种子程序出发，在BEIR和BRIGHT的12个数据集上通过迭代变异、重组和选择进行演化。实验表明，演化出的算法具有新颖性、有效性，并能很好地迁移到完整的基准测试及TREC DL 19/20数据集上，证明了LLM程序演化在自动发现排序算法方面的实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管BM25和Dirichlet平滑的查询似然模型等检索算法仍是强大高效的第一阶段排序器，但其改进长期以来主要依赖参数调优和人类直觉，缺乏自动化的算法发现机制。

Method: 提出RankEvolve框架，基于AlphaEvolve实现程序演化。候选算法以可执行代码形式表示，通过LLM引导的评估器指导演化过程，采用变异、重组和基于检索性能的选择策略，在12个IR数据集（来自BEIR和BRIGHT）上进行多轮迭代优化，初始种子为BM25和Dirichlet平滑查询似然模型。

Result: 演化生成的新颖排序算法在12个数据集上表现优异，且在完整的BEIR和BRIGHT基准测试以及TREC DL 19和20上展现出良好的迁移性能，验证了算法的有效性。

Conclusion: 评估器引导的LLM程序演化为自动发现新型排序算法提供了一条实用路径，展示了自动化算法设计的潜力。

Abstract: Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms.

</details>


### [157] [Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval](https://arxiv.org/abs/2602.16974)
*Yongjie Zhou,Shuai Wang,Bevan Koopman,Guido Zuccon*

Main category: cs.IR

TL;DR: 本文通过复现先前文档分块研究，提出了一个统一框架，从分块方法和嵌入范式两个维度系统评估了各策略在文档内检索和语料库检索任务上的表现，揭示了最优分块策略的任务依赖性，为实际应用提供了重要指导。


<details>
  <summary>Details</summary>
Motivation: 文档分块是密集检索的关键预处理步骤，但现有方法（如LLM引导的DenseX和LumberChunker，以及上下文感知的Late Chunking）独立发展且在重叠度低的基准上评估，导致设计空间理解不足且难以直接比较。

Method: 研究者复现了先前工作，构建了一个二维框架：(1) 分块方法：结构基（固定大小、句子、段落）、语义感知、LLM引导；(2) 嵌入范式：预嵌入分块vs上下文感知分块。在两种检索场景下进行评估：文档内检索（needle-in-a-haystack）和语料库检索（标准IR任务）。

Result: 实验发现：1）最优策略具有任务依赖性：语料库检索中简单结构方法优于LLM引导方法；文档内检索中LumberChunker最佳；2）上下文感知分块提升语料库检索效果但损害文档内检索；3）分块大小与文档内检索效果中度相关，与语料库检索效果弱相关。

Conclusion: 该研究系统揭示了分块策略的任务适应性，建议语料库检索采用简单高效的结构方法，文档内检索采用LLM引导策略。代码和基准已开源（匿名化）。

Abstract: Document chunking is a critical preprocessing step in dense retrieval systems, yet the design space of chunking strategies remains poorly understood. Recent research has proposed several concurrent approaches, including LLM-guided methods (e.g., DenseX and LumberChunker) and contextualized strategies(e.g., Late Chunking), which generate embeddings before segmentation to preserve contextual information. However, these methods emerged independently and were evaluated on benchmarks with minimal overlap, making direct comparisons difficult.
  This paper reproduces prior studies in document chunking and presents a systematic framework that unifies existing strategies along two key dimensions: (1) segmentation methods, including structure-based methods (fixed-size, sentence-based, and paragraph-based) as well as semantically-informed and LLM-guided methods; and (2) embedding paradigms, which determine the timing of chunking relative to embedding (pre-embedding chunking vs. contextualized chunking). Our reproduction evaluates these approaches in two distinct retrieval settings established in previous work: in-document retrieval (needle-in-a-haystack) and in-corpus retrieval (the standard information retrieval task).
  Our comprehensive evaluation reveals that optimal chunking strategies are task-dependent: simple structure-based methods outperform LLM-guided alternatives for in-corpus retrieval, while LumberChunker performs best for in-document retrieval. Contextualized chunking improves in-corpus effectiveness but degrades in-document retrieval. We also find that chunk size correlates moderately with in-document but weakly with in-corpus effectiveness, suggesting segmentation method differences are not purely driven by chunk size. Our code and evaluation benchmarks are publicly available at (Anonymoused).

</details>


### [158] [Bending the Scaling Law Curve in Large-Scale Recommendation Systems](https://arxiv.org/abs/2602.16986)
*Qin Ding,Kevin Course,Linjian Ma,Jianhui Sun,Rouchen Liu,Zhao Zhu,Chunxing Yin,Wei Li,Dai Li,Yu Shi,Xuan Cao,Ze Yang,Han Li,Xing Liu,Bi Xue,Hongwei Li,Rui Jian,Daisy Shi He,Jing Qian,Matt Ma,Qunshu Zhang,Rui Li*

Main category: cs.IR

TL;DR: 本文提出ULTRA-HSTU，一种通过模型与系统协同设计的新型序列推荐模型，通过创新输入序列设计、稀疏注意力机制及模型拓扑结构，在保持卓越推荐质量的同时，实现训练效率提升5倍以上、推理效率提升21倍以上的显著加速，并已部署服务于数十亿用户，带来4%-8%的消费与参与度提升。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐模型面临二次计算复杂度瓶颈，许多方法依赖交叉注意力机制来缓解，但这限制了自注意力带来的表征能力。同时，大规模语言模型展现的缩放规律激发了长序列建模研究，如何在保持推荐质量的同时突破计算效率限制成为关键挑战。

Method: 采用端到端模型与系统协同设计框架，创新性地设计输入序列结构、稀疏注意力机制以及模型拓扑架构，开发ULTRA-HSTU模型，以同时提升模型表征能力和计算效率。

Result: 综合基准测试显示：相比传统模型，ULTRA-HSTU实现训练缩放效率提升超过5倍，推理缩放效率提升21倍；在推荐质量上表现更优；已在生产环境全面部署，每日服务数十亿用户，并带来4%至8%的消费与参与度显著提升。

Conclusion: ULTRA-HSTU通过协同设计创新实现了效率与质量的双重突破，其显著的缩放效率优势和大规模生产验证表明该方案为下一代推荐系统提供了可扩展且高效的解决方案。

Abstract: Learning from user interaction history through sequential models has become a cornerstone of large-scale recommender systems. Recent advances in large language models have revealed promising scaling laws, sparking a surge of research into long-sequence modeling and deeper architectures for recommendation tasks. However, many recent approaches rely heavily on cross-attention mechanisms to address the quadratic computational bottleneck in sequential modeling, which can limit the representational power gained from self-attention. We present ULTRA-HSTU, a novel sequential recommendation model developed through end-to-end model and system co-design. By innovating in the design of input sequences, sparse attention mechanisms, and model topology, ULTRA-HSTU achieves substantial improvements in both model quality and efficiency. Comprehensive benchmarking demonstrates that ULTRA-HSTU achieves remarkable scaling efficiency gains -- over 5x faster training scaling and 21x faster inference scaling compared to conventional models -- while delivering superior recommendation quality. Our solution is fully deployed at scale, serving billions of users daily and driving significant 4% to 8% consumption and engagement improvements in real-world production environments.

</details>


### [159] [WSDM Cup 2026 Multilingual Retrieval: A Low-Cost Multi-Stage Retrieval Pipeline](https://arxiv.org/abs/2602.16989)
*Chentong Hao,Minmao Wang*

Main category: cs.IR

TL;DR: 本文提出了一种面向WSDM Cup 2026多语言检索任务的低价检索系统。该系统采用四阶段流水线，依次进行大语言模型查询扩展、BM25候选检索、基于jina-embeddings-v4的稠密排序，以及Qwen3-Reranker-4B精排序，最终在千万级中文、波斯文和俄文新闻数据集上实现了nDCG@20为0.403的高精度检索。


<details>
  <summary>Details</summary>
Motivation: 面对多语言跨语言检索的计算成本挑战，本文旨在探索有限预算下的高效检索方案。研究聚焦于英文查询检索中文、波斯文和俄文文档的任务，通过系统优化在控制成本的同时保证检索效果，为资源受限场景提供实用参考。

Method: 提出四阶段检索流水线：1）利用大语言模型进行生成相关反馈式查询扩展；2）采用BM25算法从千万文档中召回候选集；3）使用jina-embeddings-v4模型生成文档稠密向量进行粗排；4）对Top-20结果应用Qwen3-Reranker-4B进行点式精排序，其余结果维持稠密排序顺序。

Result: 官方评估显示，系统获得nDCG@20为0.403、Judged@20为0.95的性能指标。消融实验系统量化了各阶段贡献，验证了查询扩展、稠密排序和精排序在有限计算资源下的有效性。

Conclusion: 该研究验证了四阶段流水线在低预算多语言检索中的可行性与高效性。各组件协同作用显著，其中查询扩展提升召回、稠密排序优化全局顺序、精排序改善头部结果，为类似场景提供了可复现的架构设计和优化策略。

Abstract: We present a low-cost retrieval system for the WSDM Cup 2026 multilingual retrieval task, where English queries are used to retrieve relevant documents from a collection of approximately ten million news articles in Chinese, Persian, and Russian, and to output the top-1000 ranked results for each query. We follow a four-stage pipeline that combines LLM-based GRF-style query expansion with BM25 candidate retrieval, dense ranking using long-text representations from jina-embeddings-v4, and pointwise re-ranking of the top-20 candidates using Qwen3-Reranker-4B while preserving the dense order for the remaining results. On the official evaluation, the system achieves nDCG@20 of 0.403 and Judged@20 of 0.95. We further conduct extensive ablation experiments to quantify the contribution of each stage and to analyze the effectiveness of query expansion, dense ranking, and top-$k$ reranking under limited compute budgets.

</details>


### [160] [LiveGraph: Active-Structure Neural Re-ranking for Exercise Recommendation](https://arxiv.org/abs/2602.17036)
*Rong Fu,Zijian Zhang,Haiyun Wei,Jiekai Wu,Kun Liu,Xianda Li,Haoyu Zhao,Yang Li,Yongtai Liu,Ziming Wang,Rui Lu,Simon Fong*

Main category: cs.IR

TL;DR: LiveGraph是一种新颖的主动结构神经重排序框架，通过基于图的表示增强策略和动态重排序机制，解决教育内容推荐中的学生参与度长尾分布和个性化学习轨迹适应问题，在多个真实数据集上实现了更高的预测准确性和内容多样性。


<details>
  <summary>Details</summary>
Motivation: 随着数字化学习环境的不断扩展，个性化教育内容的需求日益增长。然而，现有习题推荐框架普遍面临学生参与度的长尾分布问题，且难以适应独特的个性化学习轨迹，导致推荐效果受限。

Method: 提出LiveGraph框架，采用基于图的表示增强策略来弥合活跃学生与非活跃学生之间的信息鸿沟，同时集成动态重排序机制以促进内容多样性。该方法优先考虑学习历史中的结构关系，有效平衡推荐精度与教学多样性。

Result: 在多个真实世界数据集上的综合实验评估表明，LiveGraph在预测准确性和习题多样性广度方面均优于当前基线方法。

Conclusion: LiveGraph通过利用学习历史中的结构关系，成功平衡了推荐精度与教学多样性，为解决教育推荐系统中的长尾分布和个性化适应问题提供了有效方案，展现了优越的实际应用性能。

Abstract: The continuous expansion of digital learning environments has catalyzed the demand for intelligent systems capable of providing personalized educational content. While current exercise recommendation frameworks have made significant strides, they frequently encounter obstacles regarding the long-tailed distribution of student engagement and the failure to adapt to idiosyncratic learning trajectories. We present LiveGraph, a novel active-structure neural re-ranking framework designed to overcome these limitations. Our approach utilizes a graph-based representation enhancement strategy to bridge the information gap between active and inactive students while integrating a dynamic re-ranking mechanism to foster content diversity. By prioritizing the structural relationships within learning histories, the proposed model effectively balances recommendation precision with pedagogical variety. Comprehensive experimental evaluations conducted on multiple real-world datasets demonstrate that LiveGraph surpasses contemporary baselines in both predictive accuracy and the breadth of exercise diversity.

</details>


### [161] [On the Reliability of User-Centric Evaluation of Conversational Recommender Systems](https://arxiv.org/abs/2602.17264)
*Michael Müller,Amir Reza Mohammadi,Andreas Peintner,Beatriz Barroso Gstrein,Günther Specht,Eva Zangerle*

Main category: cs.IR

TL;DR: 本研究通过大规模实证，系统检验了基于静态对话转录文本的第三方标注在会话推荐系统用户中心评估中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管用户中心评估已成为会话推荐系统的核心评估范式，但为实现可扩展性，近期研究日益依赖众包工作者或大语言模型对静态对话日志进行第三方标注，然而该方法的可靠性尚未得到充分验证。

Method: 研究采集了124名众包标注者对200段ReDial对话数据的1,053份评估，采用18维度的CRS-Que评估框架，通过随机效应信度模型和相关性分析，量化各维度的评估稳定性与维度间关系。

Result: 研究发现：实用性导向维度（如准确性、有用性、满意度）在聚合后可达中等信度水平，而社会性建构维度（如人性感、默契度）信度显著偏低；多数评估维度坍缩为单一全局质量因子，暴露出第三方评判中存在强烈的晕轮效应。

Conclusion: 该结果对单标注者及基于大语言模型的评估协议的有效性提出挑战，并推动了在离线会话推荐系统评估中采用多评分者聚合策略及维度降维方法，以提升评估的可靠性与效度。

Abstract: User-centric evaluation has become a key paradigm for assessing Conversational Recommender Systems (CRS), aiming to capture subjective qualities such as satisfaction, trust, and rapport. To enable scalable evaluation, recent work increasingly relies on third-party annotations of static dialogue logs by crowd workers or large language models. However, the reliability of this practice remains largely unexamined. In this paper, we present a large-scale empirical study investigating the reliability and structure of user-centric CRS evaluation on static dialogue transcripts. We collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework. Using random-effects reliability models and correlation analysis, we quantify the stability of individual dimensions and their interdependencies. Our results show that utilitarian and outcome-oriented dimensions such as accuracy, usefulness, and satisfaction achieve moderate reliability under aggregation, whereas socially grounded constructs such as humanness and rapport are substantially less reliable. Furthermore, many dimensions collapse into a single global quality signal, revealing a strong halo effect in third-party judgments. These findings challenge the validity of single-annotator and LLM-based evaluation protocols and motivate the need for multi-rater aggregation and dimension reduction in offline CRS evaluation.

</details>


### [162] [Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation](https://arxiv.org/abs/2602.17354)
*Daniele Malitesta,Emanuele Rossi,Claudio Pomo,Tommaso Di Noia,Fragkiskos D. Malliaros*

Main category: cs.IR

TL;DR: 针对多模态推荐系统中模态缺失问题缺乏形式化定义和有效解决方案的不足，本文首次形式化该问题，并利用用户-物品图结构将其转化为物品-物品共购图的特征插值问题，提出四种训练无关的特征传播方法填补缺失模态，实验验证了方法的有效性和普适性。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统依赖图像、描述等多模态数据，但数据常存在噪声或缺失。现有方法简单丢弃缺失模态的样本，缺乏系统的形式化定义和专门的解决方案，限制了多模态推荐在实际应用中的鲁棒性和泛化能力。

Method: 1) 形式化定义多模态推荐中的缺失模态问题；2) 利用图结构将问题重构为物品-物品共购图上的特征插值问题；3) 提出四种训练无关的图特征传播方法，利用可用多模态特征通过图结构传播来填补缺失特征。

Result: 在多个流行数据集上的实验表明：所提方法可无缝集成到现有框架中，保持并扩大多模态与传统推荐系统的性能差距；在多种缺失场景下优于传统机器学习插补方法；首次揭示了特征同配性对图插补效果的影响。

Conclusion: 本研究填补了多模态推荐中缺失模态问题形式化与解决方案的空白，提出的图特征插值方法有效提升了系统鲁棒性，为处理不完整多模态数据提供了新思路，对推动多模态推荐在实际场景中的应用具有重要意义。

Abstract: Multimodal recommender systems (RSs) represent items in the catalog through multimodal data (e.g., product images and descriptions) that, in some cases, might be noisy or (even worse) missing. In those scenarios, the common practice is to drop items with missing modalities and train the multimodal RSs on a subsample of the original dataset. To date, the problem of missing modalities in multimodal recommendation has still received limited attention in the literature, lacking a precise formalisation as done with missing information in traditional machine learning. In this work, we first provide a problem formalisation for missing modalities in multimodal recommendation. Second, by leveraging the user-item graph structure, we re-cast the problem of missing multimodal information as a problem of graph features interpolation on the item-item co-purchase graph. On this basis, we propose four training-free approaches that propagate the available multimodal features throughout the item-item graph to impute the missing features. Extensive experiments on popular multimodal recommendation datasets demonstrate that our solutions can be seamlessly plugged into any existing multimodal RS and benchmarking framework while still preserving (or even widen) the performance gap between multimodal and traditional RSs. Moreover, we show that our graph-based techniques can perform better than traditional imputations in machine learning under different missing modalities settings. Finally, we analyse (for the first time in multimodal RSs) how feature homophily calculated on the item-item graph can influence our graph-based imputations.

</details>


### [163] [Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers](https://arxiv.org/abs/2602.17410)
*Bingqian Li,Bowen Zheng,Xiaolei Wang,Long Zhang,Jinpeng Wang,Sheng Chen,Wayne Xin Zhao,Ji-rong Wen*

Main category: cs.IR

TL;DR: 本文针对大语言模型在推荐系统中的偏好学习问题，指出现有方法依赖序列级离线负样本导致判别性不足，提出ILRec框架利用中间层自困难负信号进行偏好微调，通过跨层偏好优化与蒸馏联合训练，并结合轻量级协同过滤模型分配令牌级奖励以避免过惩罚，在三个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推荐系统中展现出巨大潜力，通常通过监督微调进行适配。后续研究引入偏好学习以纳入负样本，但现有方法依赖序列级、离线生成的负样本，在面对大规模负样本空间时判别性和信息量不足，难以有效适配LLM到推荐任务。

Method: 提出ILRec框架，从中间层提取自困难负信号作为细粒度负监督，动态反映模型偏好学习过程；设计两阶段框架，包括跨层偏好优化和跨层偏好蒸馏，使模型能联合判别信息性负样本并提升中间层负信号质量；引入轻量级协同过滤模型为负信号分配令牌级奖励，缓解对假负样本的过惩罚风险。

Result: 在三个数据集上的广泛实验表明，ILRec能够有效提升基于大语言模型的推荐系统性能。

Conclusion: ILRec通过利用中间层自困难负信号和两阶段偏好学习框架，为大规模负样本空间下的LLM推荐系统适配提供了有效解决方案，显著提升了推荐性能。

Abstract: Large language models (LLMs) have shown great promise in recommender systems, where supervised fine-tuning (SFT) is commonly used for adaptation. Subsequent studies further introduce preference learning to incorporate negative samples into the training process. However, existing methods rely on sequence-level, offline-generated negatives, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. To address these challenges, we propose ILRec, a novel preference fine-tuning framework for LLM-based recommendation, leveraging self-hard negative signals extracted from intermediate layers to improve preference learning. Specifically, we identify self-hard negative tokens from intermediate layers as fine-grained negative supervision that dynamically reflects the model's preference learning process. To effectively integrate these signals into training, we design a two-stage framework comprising cross-layer preference optimization and cross-layer preference distillation, enabling the model to jointly discriminate informative negatives and enhance the quality of negative signals from intermediate layers. In addition, we introduce a lightweight collaborative filtering model to assign token-level rewards for negative signals, mitigating the risk of over-penalizing false negatives. Extensive experiments on three datasets demonstrate ILRec's effectiveness in enhancing the performance of LLM-based recommender systems.

</details>


### [164] [Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval](https://arxiv.org/abs/2602.17654)
*Jiaqi Xi,Raghav Saboo,Luming Chen,Martin Wang,Sudeep Das*

Main category: cs.IR

TL;DR: 本文提出一种两阶段“挖掘与精炼”对比学习框架，用于提升电商搜索中的语义文本嵌入效果。该框架先通过轻量级LLM生成三级相关性标注，训练双塔检索模型构建鲁棒语义空间；再挖掘难样本并重新标注，利用多类圆损失锐化相关性边界，辅以拼写增强和查询生成，最终显著提升了检索相关性及业务指标。


<details>
  <summary>Details</summary>
Motivation: 大规模电商搜索要求嵌入模型能泛化至长尾噪声查询，同时满足可扩展的监督学习与产品政策约束。核心挑战在于相关性是分级的：用户接受替代品或互补品，而非仅限精确匹配，且生产系统需要清晰区分不同相关性层级的相似度分数，以实现稳定的混合排序与阈值设定。现有方法难以兼顾政策一致性、可扩展性与分级相关性建模。

Method: 方法采用两阶段框架：1) “挖掘”阶段：微调轻量级LLM生成三级相关性标签，并通过用户行为审计降噪；训练多语言孪生双塔模型，采用标签感知的监督对比损失，构建全局语义空间。2) “精炼”阶段：通过近似最近邻挖掘难样本，用政策对齐的LLM重新标注；引入多类圆损失，显式锐化不同相关性层级的相似度边界；同时使用拼写增强和合成查询生成提升鲁棒性。

Result: 离线评估与线上A/B测试表明，该框架在检索相关性、用户参与度及商业影响上均有统计显著的增益。模型有效处理了长尾噪声查询，并实现了相关性层级的清晰分离，为生产系统的混合排序与阈值调优提供了可靠基础。

Conclusion: 两阶段“Mine and Refine”框架通过结合人类标注、LLM增强与对比学习，成功解决了电商搜索中分级相关性建模与政策一致性问题，为大规模语义检索提供了可扩展、高鲁棒的解决方案。

Abstract: We propose a two-stage "Mine and Refine" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact.

</details>
