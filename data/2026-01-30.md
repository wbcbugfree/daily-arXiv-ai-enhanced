<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 58]
- [cs.AI](#cs.AI) [Total: 62]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 148]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents](https://arxiv.org/abs/2601.20975)
*Nikita Gupta,Riju Chatterjee,Lukas Haas,Connie Tao,Andrew Wang,Chang Liu,Hidekazu Oiwa,Elena Gribovskaya,Jan Ackermann,John Blitzer,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: Proposes DeepSearchQA, a 900-prompt benchmark across 17 fields to evaluate long-horizon, multi-step information-seeking agents. It focuses on systematic information synthesis from fragmented sources, deduplication/entity resolution, and stopping criteria. Findings show state-of-the-art models struggle to balance recall and precision, with failure modes like under-retrieval and hedging, underscoring the benchmark's value for driving robust deep-research capabilities.


<details>
  <summary>Details</summary>
Motivation: Address the gap in evaluating agents on multi-step information gathering tasks, where success depends on planning, retrieval quality, integration, and stopping decisions, not just single-answer factuality.

Method: Construct 900 handcrafted tasks structured as causal chains grounded in open-web sources with verifiable answer sets. Evaluate agent performance on assembling exhaustive answer lists, measuring recall, precision, and de-duplication. Analyze failures across architectures to identify prevalent modes (premature stopping, hedging).

Result: Demonstrates substantial performance gaps for current agents in achieving high recall with precision; reveals systematic issues in long-horizon search, including under-retrieval and hedging behaviors.

Conclusion: DeepSearchQA provides a rigorous diagnostic tool for advancing deep-research capabilities, highlighting important headroom for future agent designs in planning, retrieval, synthesis, and stopping criteria.

Abstract: We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.

</details>


### [2] [asr_eval: Algorithms and tools for multi-reference and streaming speech recognition evaluation](https://arxiv.org/abs/2601.20992)
*Oleg Sedukhin,Andrey Kostin*

Main category: cs.CL

TL;DR: 提出改进的语音识别评估方法：多参考对齐算法、DiverseSpeech-Ru 数据集，以及跨模型的流式评估和转录对齐工具，并揭示数据集标签偏倚与微调影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估易受多参考标签不一致、长表述与非拉丁语言形态变化影响，需更鲁棒的对齐与可比性。

Method: 提出支持多参考、任意长度插入的字符串对齐算法；构建 DiverseSpeech-Ru 长时俄语数据集并进行多参考重标注；研究现有俄语数据集的微调动态；开发流式评估与多转录对齐工具，提供统一封装器，并计划开源代码。

Result: 证明模型易受数据集特定标签影响，造成指标的错觉提升；实现基于改进对齐的流式评估与可视化对齐工具；代码将公开。

Conclusion: 改进的对齐与数据集建设提升评估的可比性与鲁棒性，并提供可复用的工具链和开源资源。

Abstract: We propose several improvements to the speech recognition evaluation. First, we propose a string alignment algorithm that supports both multi-reference labeling, arbitrary-length insertions and better word alignment. This is especially useful for non-Latin languages, those with rich word formation, to label cluttered or longform speech. Secondly, we collect a novel test set DiverseSpeech-Ru of longform in-the-wild Russian speech with careful multi-reference labeling. We also perform multi-reference relabeling of popular Russian tests set and study fine-tuning dynamics on its corresponding train set. We demonstrate that the model often adopts to dataset-specific labeling, causing an illusion of metric improvement. Based on the improved word alignment, we develop tools to evaluate streaming speech recognition and to align multiple transcriptions to compare them visually. Additionally, we provide uniform wrappers for many offline and streaming speech recognition models. Our code will be made publicly available.

</details>


### [3] [Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations](https://arxiv.org/abs/2601.21084)
*Amit Meghanani,Thomas Hain*

Main category: cs.CL

TL;DR: 提出使用速度扰动结合软DTW损失对齐SSL表示，从而实现位置不变的前端语音增强微调，优于零填充和传统MSE方法。


<details>
  <summary>Details</summary>
Motivation: 揭示自监督表示微调中位置嵌入的干扰问题，即MSE倾向利用SSL模型的定位信息，导致内容信息被忽视；需要实现对定位不变的微调以提升鲁棒性。

Method: 在前端语音增强与SSL模型的联合微调场景中，比较两种策略：一是将零填充应用于微调以打击位置信息；二是通过速度扰动和软DTW损失实现对齐的无位置偏差微调，并评估对下游任务的影响及收敛速度。

Result: 实验表明，基于软DTW的速度扰动方法实现更快的收敛并提升下游性能。

Conclusion: 强调在SSL语音建模的微调中实现位置不变性的重要性，提出以软DTW为核心的对齐策略作为有效途径，零填充等方法的局限性亦被讨论。

Abstract: Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.

</details>


### [4] [ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference](https://arxiv.org/abs/2601.21109)
*Ketan Thakkar,Maitreyi Chatterjee,Ramasubramanian Balasubramanian,Achyuthan Jootoo,Rajendra Ugrani*

Main category: cs.CL

TL;DR: ChunkWise LoRA 将序列按自适应的块划分，并对每块应用不同的低秩配置的高效微调方案，显著降低时延与显存，同时保持或提升任务性能，且与现有 Transformer 架构兼容。


<details>
  <summary>Details</summary>
Motivation: 现有 LoRA 对所有输入 token 使用固定秩配置，忽略了不同 token 的复杂度和计算需求差异，导致资源利用不充分与潜在性能损失。

Method: 提出运行时调度器来估算 token 难度、进行自适应分块，并为每块选择对应该块的 LoRA 秩和缩放（rank-ladder）。引入边界安全的组合模块以保持输出一致性，并整合基于策略的 KV-cache 策略。

Result: 在 Wikitext-103 和 SQuAD 等基准上，ChunkWise LoRA 实现相较基线 LoRA 最高可降低 34% 的延迟、38% 的显存，且 BLEU、EM、困惑度等指标保持或提升。框架与现有 Transformer 架构及推理框架完全兼容。

Conclusion: 该方法为参数高效的 LLM 微调在实际部署中的应用提供可行方案，显著提升推理效率同时保持性能，并可扩展至不同模型和任务。

Abstract: Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.

</details>


### [5] [Multi-task Code LLMs: Data Mix or Model Merge?](https://arxiv.org/abs/2601.21115)
*Mingzhi Zhu,Boris Sobolev,Rahul Krishna,Raju Pavuluri,Stacy Patterson,Michele Merler*

Main category: cs.CL

TL;DR: 在两种策略（数据混合与模型融合）之间对小型多任务代码LLM进行比较，结果表明在较大规模下模型融合优于数据混合，能保持高水平的代码生成与摘要能力，甚至超越单任务微调；小规模时数据混合更优。并提出权重分析以理解任务对参数的影响。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限环境下，如何构建多任务、高效的代码生成/摘要能力的模型，平衡性能、约束与成本。

Method: 在Qwen Coder与DeepSeek Coder两大模型家族、2B与7B两个规模上，比较数据混合与模型融合两种策略对代码生成与代码摘要任务的微调效果；在HumanEval、MBPP、CodeXGlue等基准上评估；并提出权重分析以理解任务对参数的影响及对合并策略的指示。

Result: 较大规模（7B）下，模型融合在两家族均表现最佳，保留约96%的专业模型在代码生成任务的性能并维持摘要能力；合并模型在某些配置可超越单任务微调，如Qwen Coder 2.5 7B达到92.7% Pass@1（HumanEval），高于任务特定微调的90.9%；小规模时数据混合更具优势；引入权重分析以理解各任务对参数的影响及对合并策略的含义。

Conclusion: 通过谨慎的融合与混合策略，能够在不显著损失性能的前提下，整合不同任务的能力，适用于资源受限的部署场景。

Abstract: Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.

</details>


### [6] [Large Language Models Naively Recover Ethnicity from Individual Records](https://arxiv.org/abs/2601.21132)
*Noah Dasanaike*

Main category: cs.CL

TL;DR: 敏感主题：通过姓名推断族裔的研究摘要分析（跨国比较）


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在姓名层面推断族裔的能力及相较BISG的潜在优势，探讨跨国适用性与偏见缓解

Method: 对给定摘要进行结构化分析，总结研究目标、所用方法、关键结果及结论

Result: 显示LLM在多国数据集上的识别准确性优于BISG，扩展推理可微幅提升准确度，元数据（如党派）可进一步提高性能，并缓解BISG的收入偏见，但存在隐私与伦理风险

Conclusion: 方法具潜力用于大规模人口计量，但需要严格伦理与治理框架以防滥用，且在不同文化命名规范下的普适性需谨慎评估

Abstract: I demonstrate that large language models can infer ethnicity from names with accuracy exceeding that of Bayesian Improved Surname Geocoding (BISG) without additional training data, enabling inference outside the United States and to contextually appropriate classification categories. Using stratified samples from Florida and North Carolina voter files with self-reported race, LLM-based classification achieves up to 84.7% accuracy, outperforming BISG (68.2%) on balanced samples. I test six models including Gemini 3 Flash, GPT-4o, and open-source alternatives such as DeepSeek v3.2 and GLM-4.7. Enabling extended reasoning can improve accuracy by 1-3 percentage points, though effects vary across contexts; including metadata such as party registration reaches 86.7%. LLM classification also reduces the income bias inherent in BISG, where minorities in wealthier neighborhoods are systematically misclassified as White. I further validate using Lebanese voter registration with religious sect (64.3% accuracy), Indian MPs from reserved constituencies (99.2%), and Indian land records with caste classification (74.0%). Aggregate validation across India, Uganda, Nepal, Armenia, Chile, and Costa Rica using original full-count voter rolls demonstrates that the method recovers known population distributions where naming conventions are distinctive. For large-scale applications, small transformer models fine-tuned on LLM labels exceed BISG accuracy while enabling local deployment at no cost.

</details>


### [7] [EnsembleLink: Accurate Record Linkage Without Training Data](https://arxiv.org/abs/2601.21138)
*Noah Dasanaike*

Main category: cs.CL

TL;DR: EnsembleLink在无需标注训练数据的情况下，利用预训练语言模型的语义关系来进行记录对齐，在多领域基准上达到或超过需要大量标注的方法，并可本地运行。


<details>
  <summary>Details</summary>
Motivation: 记录链接是跨数据集识别同一实体的核心步骤，但常被作为预处理且未能量化链接错误对下游分析的不确定性；现有方法要么准确性不足，要么需要大量标注数据。

Method: 使用从大规模文本语料中学习的预训练语言模型来捕捉语义关系，并通过集成策略进行对齐，避免额外的训练标签；在本地使用开源模型运行，无需外部 API 调用，且对常规链接任务在几分钟内完成。

Result: 在覆盖城市名、人名、组织、 multilingual political parties、以及书目记录等基准上，EnsembleLink达到或超过需大量标注的对比方法的准确性，且可本地运行，常规任务在几分钟内完成。

Conclusion: EnsembleLink实现了无需标注的高准确度记录链接，降低对外部服务的依赖，并支持本地部署。

Abstract: Record linkage, the process of matching records that refer to the same entity across datasets, is essential to empirical social science but remains methodologically underdeveloped. Researchers treat it as a preprocessing step, applying ad hoc rules without quantifying the uncertainty that linkage errors introduce into downstream analyses. Existing methods either achieve low accuracy or require substantial labeled training data. I present EnsembleLink, a method that achieves high accuracy without any training labels. EnsembleLink leverages pre-trained language models that have learned semantic relationships (e.g., that "South Ozone Park" is a neighborhood in "New York City" or that "Lutte ouvriere" refers to the Trotskyist "Workers' Struggle" party) from large text corpora. On benchmarks spanning city names, person names, organizations, multilingual political parties, and bibliographic records, EnsembleLink matches or exceeds methods requiring extensive labeling. The method runs locally on open-source models, requiring no external API calls, and completes typical linkage tasks in minutes.

</details>


### [8] [Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space](https://arxiv.org/abs/2601.21169)
*Tobias Materzok*

Main category: cs.CL

TL;DR: OS-Search 将 LLM 生成转为端点搜索，在冻结的编码器定义的三维输出空间 Z 中的目标 z* 由外部循环选取，检索-基策略通过序列级RL训练，使输出的坐标在标准自回归解码下接近 z*。实现并行化扫掠与对 Z 的黑箱优化。对故事任务，扫描 Z（文本）使 LLM 评分的多样性比提示链提升 3.1 倍；对代码任务，在 Z 上进行贝叶斯优化，在推理预算匹配的情况下提升控制器未披露目标的优化，同时保持有效性。


<details>
  <summary>Details</summary>
Motivation: 解决 token 级搜索的路径依赖与低效，利用一个可解释、可并行化的输出空间实现高效的全局搜索，并同时提升输出多样性与任务目标的优化。

Method: 提出输出空间 OS-Search：外部循环在冻结编码器定义的三维输出空间 Z 中挑选目标 z*，并训练一个检索-基策略，在序列级奖励信号下引导生成的输出坐标落在 z* 附近；通过标准自回归解码实现并行化的搜索。故事任务在文本输出上进行 Z 的扫掠，代码任务在 Z 上进行贝叶斯优化以优化对任务目标的评价（对控制器未披露的目标），在预算等条件下保持有效性。

Result: 故事：基于 Z 的扫掠使 LLM 评分的多样性提升约 3.1×，优于提示链。代码：对 Z 使用贝叶斯优化，在推理预算匹配的前提下改善被控制器未披露的目标，同时保持输出的有效性。

Conclusion: OS-Search 提出将输出空间解耦于逐词生成的新框架，支持并行化的端点搜索与黑箱优化，降低路径依赖并提升多样性与目标优化性能。

Abstract: We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.

</details>


### [9] [From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning](https://arxiv.org/abs/2601.21191)
*Xiulin Yang,Heidi Getz,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 三大统计性质：功能词的高频、与句法结构的稳定关联、以及与短语边界的对齐，普遍存在于186种语言。保留这三者的分布特征的语言变体更易被神经学习者获取层级结构；其中频率与结构相关性贡献最大，边界对贡献较小。不同学习条件下对功能词的依赖存在系统性差异，表明相似表现可由不同内部机制实现。


<details>
  <summary>Details</summary>
Motivation: 探索在线性输入中学习层级结构所需的统计条件，聚焦功能词的分布特征及其在语言习得中的作用。

Method: 通过跨语言语料分析（覆盖186种语言）来验证三种性质的普遍性；利用对抗性语言建模与消融实验评估在保留三种属性的情况下学习难度；通过探测分析与进一步的消融研究揭示在不同学习条件下对功能词的依赖差异。

Result: 在保留三种属性的语言变体中，神经学习者更易获取层级结构；频率与结构关联对学习的贡献大于边界对的贡献；不同学习条件导致对功能词的系统性不同依赖，表明相同的学习表现可由多种内部机制实现。

Conclusion: 这三种统计性质共同支撑从线性输入学习层级结构的能力；学习条件的差异会改变对功能词的依赖，提示存在多条通向相似表现的内部机制。

Abstract: What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role in language acquisition due to their distinctive distributional properties, including high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. We use cross-linguistic corpus analysis to first establish that all three properties are present across 186 studied languages. Next, we use a combination of counterfactual language modeling and ablation experiments to show that language variants preserving all three properties are more easily acquired by neural learners, with frequency and structural association contributing more strongly than boundary alignment. Follow-up probing and ablation analyses further reveal that different learning conditions lead to systematically different reliance on function words, indicating that similar performance can arise from distinct internal mechanisms.

</details>


### [10] [Multilingual Dysarthric Speech Assessment Using Universal Phone Recognition and Language-Specific Phonemic Contrast Modeling](https://arxiv.org/abs/2601.21205)
*Eunjung Yeo,Julie M. Liss,Visar Berisha,David R. Mortensen*

Main category: cs.CL

TL;DR: 提出一个跨语言的言语清晰度评估框架，通过通用语音识别与语言特定音位解释的对比音系特征距离进行电话-音位映射和序列比对，产生三项指标：PER、PFER、PhonCov。对英语、西班牙语、意大利语和泰米尔语的分析显示：PER受映射+对齐的组合影响，PFER受对齐影响，PhonCov受映射影响；框架能捕捉与痼呕症相关的临床意义模式。


<details>
  <summary>Details</summary>
Motivation: 痼呃相关神经疾病日渐增多，需一种跨语言的自动化可理解性评估方法；现有方法往往局限于单一语言，或未充分考虑语言特异因素，亟需一个能够融合通用识别与语言特异解释的框架。

Method: 提出一个多语言的电话-音位生产评估框架：使用通用电话识别（universal phone recognition）并结合基于对比音系特征距离的音位解释进行电话-音位映射；引入序列比对以提升对错音的定位与度量。同时给出三项评估指标：音位错误率PER、音系特征错误率PFER、以及新提出的对齐无关度量PhonCov。对英语、西班牙语、意大利语和泰米尔语进行分析。

Result: 在英语、 西班牙语、意大利语和泰米尔语的分析中：映射+对齐的组合能提升PER；仅对齐能提升PFER；映射能提升PhonCov。分析还表明，该框架能捕捉与痼呃相关的临床意义模式，与既有的痼呃语音的公认观察结果一致。

Conclusion: 框架实现跨语言的可理解性评估，通过通用识别与语言特异解释的结合，并引入新指标PhonCov，展示了跨语言一致性与临床相关性，有望用于多语言神经语言疾病的定量评估与监测。

Abstract: The growing prevalence of neurological disorders associated with dysarthria motivates the need for automated intelligibility assessment methods that are applicalbe across languages. However, most existing approaches are either limited to a single language or fail to capture language-specific factors shaping intelligibility. We present a multilingual phoneme-production assessment framework that integrates universal phone recognition with language-specific phoneme interpretation using contrastive phonological feature distances for phone-to-phoneme mapping and sequence alignment. The framework yields three metrics: phoneme error rate (PER), phonological feature error rate (PFER), and a newly proposed alignment-free measure, phoneme coverage (PhonCov). Analysis on English, Spanish, Italian, and Tamil show that PER benefits from the combination of mapping and alignment, PFER from alignment alone, and PhonCov from mapping. Further analyses demonstrate that the proposed framework captures clinically meaningful patterns of intelligibility degradation consistent with established observations of dysarthric speech.

</details>


### [11] [Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models](https://arxiv.org/abs/2601.21214)
*Zhaoyi Li,Jiatong Li,Gangwei Jiang,Linqi Song,Defu Lian,Ying Wei*

Main category: cs.CL

TL;DR: 提出并验证了“错误处理头”（ep heads）对推理跳数泛化的影响，发现错误主要集中在少数令牌位点；通过在推理阶段动态去活化 ep 头实现测试时纠错，能显著提升跨任务的推理跳数泛化。


<details>
  <summary>Details</summary>
Motivation: 在大规模语言模型的链式推理中，超过训练分布的推理跳数导致性能下降，内部机制尚不清晰。本研究从多领域系统性分析，关注推理过程中的内部竞争机制及其对泛化的影响。

Method: 系统性分析多域任务中的推理过程，定位导致错误的令牌位点，提出并识别“错误处理头”（ep heads），在推理中动态去活化它们以实现测试时纠错；在多任务、多模型上进行广泛实验。

Result: 错误分布在少数关键类型的 token 位点；移除单个 ep head 即可恢复正确预测；测试时纠错方法在跨任务与不同 LLM 上稳定提升推理跳数泛化。

Conclusion: 去活化 ep heads 提供了一种轻量、有效的推理阶段干预，提升推理跳数泛化能力，具有较好的普适性与潜在应用前景。

Abstract: Chain-of-thought (CoT) reasoning has become the standard paradigm for enabling Large Language Models (LLMs) to solve complex problems. However, recent studies reveal a sharp performance drop in reasoning hop generalization scenarios, where the required number of reasoning steps exceeds training distributions while the underlying algorithm remains unchanged. The internal mechanisms driving this failure remain poorly understood. In this work, we conduct a systematic study on tasks from multiple domains, and find that errors concentrate at token positions of a few critical error types, rather than being uniformly distributed. Closer inspection reveals that these token-level erroneous predictions stem from internal competition mechanisms: certain attention heads, termed erroneous processing heads (ep heads), tip the balance by amplifying incorrect reasoning trajectories while suppressing correct ones. Notably, removing individual ep heads during inference can often restore the correct predictions. Motivated by these insights, we propose test-time correction of reasoning, a lightweight intervention method that dynamically identifies and deactivates ep heads in the reasoning process. Extensive experiments across different tasks and LLMs show that it consistently improves reasoning hop generalization, highlighting both its effectiveness and potential.

</details>


### [12] [Fake News Detection After LLM Laundering: Measurement and Explanation](https://arxiv.org/abs/2501.18649)
*Rupak Kumar Das,Jonathan Dodge*

Main category: cs.CL

TL;DR: Detectors struggle more to identify LLM-paraphrased fake news than human-written, revealing that paraphrasing can evade detection; sentiment shift likely drives detection failures; paraphrase quality metrics (BERTScore) may mislead; datasets with paraphrase outputs/scores released on GitHub.


<details>
  <summary>Details</summary>
Motivation: LLMs can generate convincing fake news, and existing detection research mainly targets human-written text. It is crucial to assess whether paraphrasing steps in detection pipelines help or hinder, and to understand why detections fail.

Method: Evaluate detectors on LLM-paraphrased fake news and compare with human-written text. Analyze which models excel at evading detection, paraphrasing to evade or for semantic similarity. Use LIME explanations to identify reasons for failures (notably sentiment shifts). Examine paraphrase quality using metrics like BERTScore, noting samples with sentiment shifts. Create and release two datasets augmenting existing ones with paraphrase outputs and scores (GitHub).

Result: Detectors have greater difficulty detecting LLM-paraphrased fake news than human-written. Some models excel at specific tasks (evasion, paraphrasing for evasion, paraphrasing for semantic similarity). LIME indicates sentiment shift as a possible driver of detection failure. Paraphrase quality measurements reveal samples with sentiment shift despite high BERTScore. Datasets with paraphrase outputs/scores provided on GitHub.

Conclusion: Paraphrase-aware detection of LLM-generated fake news remains challenging; sentiment shifts play a role in detection failures; quality metrics for paraphrase need refinement; datasets are provided to advance future research.

Abstract: With their advanced capabilities, Large Language Models (LLMs) can generate highly convincing and contextually relevant fake news, which can contribute to disseminating misinformation. Though there is much research on fake news detection for human-written text, the field of detecting LLM-generated fake news is still under-explored. This research measures the efficacy of detectors in identifying LLM-paraphrased fake news, in particular, determining whether adding a paraphrase step in the detection pipeline helps or impedes detection. This study contributes: (1) Detectors struggle to detect LLM-paraphrased fake news more than human-written text, (2) We find which models excel at which tasks (evading detection, paraphrasing to evade detection, and paraphrasing for semantic similarity). (3) Via LIME explanations, we discovered a possible reason for detection failures: sentiment shift. (4) We discover a worrisome trend for paraphrase quality measurement: samples that exhibit sentiment shift despite a high BERTSCORE. (5) We provide a pair of datasets augmenting existing datasets with paraphrase outputs and scores. The dataset is available on GitHub

</details>


### [13] [Parametric Knowledge is Not All You Need: Toward Honest Large Language Models via Retrieval of Pretraining Data](https://arxiv.org/abs/2601.21218)
*Christopher Adrian Kusuma,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 基于公开预训练数据的Pythia模型，提出更鲁棒的LLM诚实性评估基准，并给出利用预训练数据提升诚实性的初步方法。


<details>
  <summary>Details</summary>
Motivation: LLMs易在知识边界外生成错误信息（幻觉），需要更可靠的诚实性评估与训练手段，并考虑模型在预训练阶段已吸收的知识。

Method: 构建以Pythia为对象的开源、公开可获预训练数据基础的诚实性基准，并提出利用该预训练数据提升模型诚实性的策略。

Result: 提出一个更鲁棒的诚实性评估基准数据集；并提出利用公开预训练数据来提升模型诚实性的初步方法。

Conclusion: 通过对开源模型的分析，证实将预训练知识纳入评估和训练有助于提升诚实性，为未来提供更可靠的基线。

Abstract: Large language models (LLMs) are highly capable of answering questions, but they are often unaware of their own knowledge boundary, i.e., knowing what they know and what they don't know. As a result, they can generate factually incorrect responses on topics they do not have enough knowledge of, commonly known as hallucination. Rather than hallucinating, a language model should be more honest and respond with "I don't know" when it does not have enough knowledge about a topic. Many methods have been proposed to improve LLM honesty, but their evaluations lack robustness, as they do not take into account the knowledge that the LLM has ingested during its pretraining. In this paper, we propose a more robust evaluation benchmark dataset for LLM honesty by utilizing Pythia, a truly open LLM with publicly available pretraining data. In addition, we also propose a novel method for harnessing the pretraining data to build a more honest LLM.

</details>


### [14] [MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset](https://arxiv.org/abs/2601.21512)
*Serry Sibaee,Yasser Alhabashi,Nadia Sibai,Yara Farouk,Adel Ammar,Sawsan AlHalawani,Wadii Boulila*

Main category: cs.CL

TL;DR: 提出 MURAD，一個多域的反向阿拉伯字典資料集，包含 96,243 對詞-定義，從可信來源通過混合管道提取與自動重建而成，附帶域元數據，覆蓋語言學、伊斯蘭研究、數學、物理、心理學與工程等領域。旨在推動阿拉伯自然語言處理與可重現的詞彙語義研究，並支持逆向字典建模、語義檢索與教育應用。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯語缺乏大規模、可跨域定義對照的詞彙資源，限制計算語言學任務、語義檢索與教學工具的發展。需一個可重現、跨領域的詞彙語義資料庫以促進研究與應用。

Method: 採用混合管道：直接文本解析、光學字符識別（OCR）與自動重建相結合；資料來源於可信的參考著作與教育資源；每條記錄將目標詞對應到標準化的阿拉伯語定義，並附帶來源域的元數據；涵蓋語言學、伊斯蘭研究、數學、物理、心理學與工程等領域。

Result: 數據集規模為 96,243 對詞-定義；涵蓋多個域，提供標準化定義與域元數據；公開發布，支持計算語言學研究與詞彙語義研究，以及教育工具的開發。

Conclusion: 該資源有望推動阿拉伯語 NLP 的發展，促進可重現的跨域詞彙語義研究，並促成逆向字典建模、語義檢索與教育應用之進步。

Abstract: Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.

</details>


### [15] [SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models](https://arxiv.org/abs/2601.21235)
*Alok Abhishek,Tushar Bandopadhyay,Lisa Erickson*

Main category: cs.CL

TL;DR: SHARP通过多变量、分布敏感的评估框架来量化LLMs的社会伤害风险。核心使用CVaR95等尾部风险指标，揭示在相似平均风险下，模型在尾部暴露和波动上存在显著差异，并呈现偏差、公平、伦理、认知可靠性等维度的系统性尾部特征。


<details>
  <summary>Details</summary>
Motivation: 现有评价偏重均值化的标量分数，忽略风险的分布结构、跨维度交互及最坏情形，无法在高风险场景中区分不同模型的潜在灾难性行为。需将伤害建模为多维随机变量，并引入风险敏感、分布感知的评估框架。

Method: 将伤害建模为多变量随机变量，明确对偏差、公平、伦理、认知可靠性等维度的解耦与联合故障汇总；以加性对数风险重参数化；采用面向分布的风险统计（以CVaR95为主）进行评估。对11种前沿LLMs在901条社会敏感提示上的表现进行系统分析。

Result: 在平均风险相近的前提下，模型的尾部暴露与波动差异可超过2倍；各维度尾部行为呈现系统性差异，偏差维度尾部最严重，认知性与公平性处于中等，伦理错位较低，揭示 scalar 基准掩盖的异质性失败结构。

Conclusion: 需要从单一平均值转向多维、尾部敏感的风险画像以实现更负责任的评估与治理。

Abstract: Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.

</details>


### [16] [LMK > CLS: Landmark Pooling for Dense Embeddings](https://arxiv.org/abs/2601.21525)
*Meet Doshi,Aashka Trivedi,Vishwajeet Kumar,Parul Awasthy,Yulong Li,Jaydeep Sen,Radu Florian,Sachindra Joshi*

Main category: cs.CL

TL;DR: LMK pooling uses landmark tokens to bridge chunks in a sequence and mean-pools over landmarks to form the final representation, improving long-context extrapolation while preserving local cues.


<details>
  <summary>Details</summary>
Motivation: Standard pooling (CLS or mean) struggles with long sequences: CLS concentrates information at early positions, while mean pooling dilutes salient local signals, hurting short-context performance. A simple, scalable pooling that maintains locality and handles long contexts is sought.

Method: Partition the input sequence into chunks, insert landmark tokens between chunks, and form the final representation by mean-pooling the embeddings of these landmark tokens (with only a small number of extra tokens).

Result: LMK pooling matches existing methods on short-context tasks and yields substantial gains on long-context tasks, offering a practical and scalable alternative to current pooling strategies.

Conclusion: LMK pooling provides a simple, effective mechanism to improve long-context representations without sacrificing local saliency, at the cost of a small token overhead.

Abstract: Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.

</details>


### [17] [MoCo: A One-Stop Shop for Model Collaboration Research](https://arxiv.org/abs/2601.21257)
*Shangbin Feng,Yuyang Bai,Ziyuan Yang,Yike Wang,Zhaoxuan Tan,Jiajie Yan,Zhenyu Lei,Wenxuan Ding,Weijia Shi,Haojin Wang,Zhenting Qi,Yuru Jiang,Heng Wang,Chengsong Huang,Yu Fei,Jihan Yao,Yilun Du,Luke Zettlemoyer,Yejin Choi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: MoCo 是一个一站式 Python 库，用于执行、基准测试和比较模型协作算法，覆盖26种协作方法与25个评估数据集，显著提升多模型协作性能。


<details>
  <summary>Details</summary>
Motivation: 当前单一大语言模型的局限性使模型协作成为重要方向，但现有研究缺乏统一的框架与可比基准。

Method: 构建 MoCo，收集26种跨模型协作方法，覆盖路由、文本、logit和模型参数等信息交换层级；整合25个评估数据集，支持自有数据；通过大规模实验比较方法在多任务/多数据设置上的表现，分析扩展性和训练/推理效率。

Result: 在61.0%的(model, data)设定中，协作策略优于单模型；平均提升幅度可观，最高达到25.8%。

Conclusion: MoCo 可成为推动开放、模块化、去中心化、协作式 AI 未来的实用工具箱，为模型协作研究提供统一的基准与可复现的评估框架，并指出未来方向。

Abstract: Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.

</details>


### [18] [CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding](https://arxiv.org/abs/2601.21262)
*Jiahao Huo,Yu Huang,Yibo Yan,Ye Pan,Yi Cao,Mingdong Ou,Philip S. Yu,Xuming Hu*

Main category: cs.CL

TL;DR: 提出 CausalEmbed 的自回归多向量嵌入生成框架，显著降低视觉令牌数量（30-155x），在保持竞争性能的前提下提升 VDR 的训练与推理效率；通过迭代边际损失强化嵌入的紧凑度与结构性。


<details>
  <summary>Details</summary>
Motivation: MLLMs 在视觉文档检索中需要大量视觉令牌来表示页面，存储与计算成本高。需更紧凑、可扩展的多向量嵌入以实现实用化。

Method: 自回归生成嵌入，CausalEmbed；在对比学习中引入迭代边际损失，鼓励模型学习紧凑且结构化的嵌入；实现仅使用数十个视觉令牌的高效 VDR；支持多骨架/backbone 与多基准；理论分析与实证展示训练效率与测试时的可扩展性。

Result: 在令牌数量上达到 30-155x 的减少，性能在多骨架与基准上保持高度竞争；在训练效率和测试时可扩展性方面具有理论与实证证据；提出可用于多向量 VDR 表示的灵活测试时缩放策略。

Conclusion: CausalEmbed 引入了生成式的多向量 VDR 表征范式，并为在视觉文档检索中的自回归嵌入生成提供了可扩展性与灵活性。

Abstract: Although Multimodal Large Language Models (MLLMs) have shown remarkable potential in Visual Document Retrieval (VDR) through generating high-quality multi-vector embeddings, the substantial storage overhead caused by representing a page with thousands of visual tokens limits their practicality in real-world applications. To address this challenge, we propose an auto-regressive generation approach, CausalEmbed, for constructing multi-vector embeddings. By incorporating iterative margin loss during contrastive training, CausalEmbed encourages the embedding models to learn compact and well-structured representations. Our method enables efficient VDR tasks using only dozens of visual tokens, achieving a 30-155x reduction in token count while maintaining highly competitive performance across various backbones and benchmarks. Theoretical analysis and empirical results demonstrate the unique advantages of auto-regressive embedding generation in terms of training efficiency and scalability at test time. As a result, CausalEmbed introduces a flexible test-time scaling strategy for multi-vector VDR representations and sheds light on the generative paradigm within multimodal document retrieval.

</details>


### [19] [Qwen3-ASR Technical Report](https://arxiv.org/abs/2601.21337)
*Xian Shi,Xiong Wang,Zhifang Guo,Yongqi Wang,Pei Zhang,Xinyu Zhang,Zishan Guo,Hongkun Hao,Yu Xi,Baosong Yang,Jin Xu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-ASR is a two-model open-source ASR suite plus a non-autoregressive forced aligner; supports 52 languages; achieves SOTA among open-source models; 1.7B best overall, 0.6B offers best accuracy–efficiency trade-off; released under Apache 2.0.


<details>
  <summary>Details</summary>
Motivation: Address the need for accurate, multilingual, and efficient all-in-one ASR and precise timestamp alignment in real-world scenarios beyond existing benchmarks.

Method: Train large-scale speech models on the Qwen3-Omni foundation, enable language identification and 52-language ASR; evaluate with internal tests and open benchmarks; develop Qwen3-ForcedAligner-0.6B as an LLM-based non-autoregressive timestamp predictor.

Result: 1.7B achieves state-of-the-art performance among open-source ASR models and is competitive with strong proprietary APIs; 0.6B provides best accuracy–efficiency trade-off; Qwen3-ASR-0.6B yields average TTFT ≈ 92 ms and can transcribe 2000 seconds of speech in 1 second at 128-way concurrency; Qwen3-ForcedAligner-0.6B aligns text-speech pairs in 11 languages and outperforms the top three force-alignment models in timestamp accuracy.

Conclusion: Models released under Apache 2.0 to accelerate community research in ASR and audio understanding.

Abstract: In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.

</details>


### [20] [Self-Improving Pretraining: using post-trained models to pretrain better models](https://arxiv.org/abs/2601.21343)
*Ellen Xiaoqing Tan,Shehzaad Dhuliawala,Jing Xu,Ping Yu,Sainbayar Sukhbaatar,Jason Weston,Olga Golovneva*

Main category: cs.CL

TL;DR: 提出一种在预训练阶段通过流式文档输入和强化学习来优化前K个令牌的生成，以提升事实性和安全性并提高整体生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的训练流程依赖昂贵的专门数据集与多阶段对齐，无法从根本上消除预训练阶段学习的有害模式或幻觉，因此需要在预训练阶段塑造模型的核心行为。

Method: 引入按步骤流式预训练：对每一步的前K个生成令牌进行强化学习以优化，使用一个强力的后置模型对候选生成（包括滚动式生成、原始后缀和改写后缀）进行质量/安全/事实性评估；训练初期依赖原始与改写后缀，随着模型提升，RL奖励更青睐高质量的滚动；目标是在全局层面提升模型质量与安全性。

Result: 实验显示相对于标准预训练，在事实性和安全性方面分别获得36.2%和18.5%的相对提升；在整体生成质量上，胜率提升高达86.3%。

Conclusion: 该策略从预训练层面提升模型行为，能训练出更安全、更加可信且更高质量的模型；但需权衡评估成本与潜在的依赖关系。

Abstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.

</details>


### [21] [The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation](https://arxiv.org/abs/2601.21360)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Arjun Neekhra,Yash Sinha,Murari Mandal,Vinay Chamola,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本研究揭示把 LLM 的指令遵循能力直接用于客观评测并不可靠，提出 SPACI 与 AST-ASIP 对抗性注入框架，揭示高容量模型在代码评测中的系统性脆弱性，并主张由领域特定的裁决鲁棒性替代传统 RLHF，最终公开数据集与注入框架以促进行业研究。


<details>
  <summary>Details</summary>
Motivation: 当前教育评测中普遍存在的假设是指令遵循能力可直接转化为对提交代码的客观判定。然而，模型可能被对抗性指令引导，导致对代码正确性评估的失真。需揭示、量化并缓解这一“Compliance Paradox”以确保自动评测的可信性。

Method: 提出 SPACI（Semantic-Preserving Adversarial Code Injection）框架与 AST-ASIP（Abstract Syntax Tree-Aware Semantic Injection Protocol），通过在抽象语法树的 trivia 节点嵌入对抗指令，利用语法-语义缝隙进行对抗性注入。对 9 个 SOTA 模型、共 25,000 份代码提交（Python、C、C++、Java）进行大规模评测，使用 Decoupling Probability、Score Divergence、Pedagogical Severity 三元度量，量化模型对隐藏指令的优先级与代码正确性的偏离。

Result: 在高容量开放权重模型（如 DeepSeek-V3）上，Catastrophic failure 率超过95%，模型普遍优先满足隐藏格式约束而非代码正确性，揭示广泛存在的 False Certification（功能实现被变造但形式符合要求）。

Conclusion: 研究表明现有对齐范式带来 Trojan 弱点，需从标准 RLHF 转向领域特定的 Adjudicative Robustness，使模型以证据与可验证性为主导而非简单遵循指令。作者还公开数据集与注入框架，促进后续对该问题的研究与缓解。

Abstract: The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread "False Certification" of functionally broken code. Our findings suggest that current alignment paradigms create a "Trojan" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.

</details>


### [22] [User-Centric Evidence Ranking for Attribution and Fact Verification](https://arxiv.org/abs/2601.21387)
*Guy Alt,Eran Hirsch,Serwar Basch,Ido Dagan,Oren Glickman*

Main category: cs.CL

TL;DR: 提出Evidence Ranking任务，以尽早在排序列表中呈现足够信息，最小化用户阅读成本，同时保留全部证据供逐步验证。增量排序优于一次性排序；LLM方法优于浅层基线，但仍在充足性与冗余之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 现有自动系统和LLMs在检索并选择证据时，常要么信息不足、要么信息冗余，导致验证效率低下且易出错。通过将证据按相关性和充分性排序，尽早提供足够信息以提升用户认知效率与可验证性。

Method: 提出两种排序策略：one-shot ranking与incremental ranking；建立受信息检索启发的新评估框架；通过汇聚现有事实核验数据集构建统一基准；在多样模型上进行广泛实验；并进行针对证据选择的受控用户研究以对比效果。

Result: 增量排序更能捕捉互补证据，且LLM方法在多数场景优于浅层基线；但在充足性与冗余之间仍存在权衡，需要平衡两者；与证据选择相比，证据排序的用户研究显示能降低阅读成本并提升验证结果。

Conclusion: 本工作为更易解释、高效且更符合用户需求的信息核验系统奠定基础。

Abstract: Attribution and fact verification are critical challenges in natural language processing for assessing information reliability. While automated systems and Large Language Models (LLMs) aim to retrieve and select concise evidence to support or refute claims, they often present users with either insufficient or overly redundant information, leading to inefficient and error-prone verification. To address this, we propose Evidence Ranking, a novel task that prioritizes presenting sufficient information as early as possible in a ranked list. This minimizes user reading effort while still making all available evidence accessible for sequential verification. We compare two approaches for the new ranking task: one-shot ranking and incremental ranking. We introduce a new evaluation framework, inspired by information retrieval metrics, and construct a unified benchmark by aggregating existing fact verification datasets. Extensive experiments with diverse models show that incremental ranking strategies better capture complementary evidence and that LLM-based methods outperform shallower baselines, while still facing challenges in balancing sufficiency and redundancy. Compared to evidence selection, we conduct a controlled user study and demonstrate that evidence ranking both reduces reading effort and improves verification. This work provides a foundational step toward more interpretable, efficient, and user-aligned information verification systems.

</details>


### [23] [SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.21476)
*Lei Yang,Wei Bi,Chenxi Sun,Renren Jin,Deyi Xiong*

Main category: cs.CL

TL;DR: SOUP：单样本混合策略统一范式，在单个样本的 token 级别同时利用离线和在线策略，限定离线信息在前缀部分，续写部分由在线策略生成，通过 token 级重要性比对权衡离线信息，提升探索与稳定性，在 LLM RL 任务中实现优于常规 on-policy 与现有 off-policy 的性能。


<details>
  <summary>Details</summary>
Motivation: 在语言模型的策略梯度强化学习中，纯 on-policy 往往因采样多样性不足而探索受限；离线数据虽然有帮助，但混合整条轨迹会引发策略不匹配和训练不稳定性。

Method: 提出 SOUP：在单个样本上实现 off-policy 与 on-policy 的混合，离线影响仅作用于由历史策略采样得到的序列前缀，续写部分则使用当前策略的 on-policy 生成。通过 token 级的重要性比对（importance ratios）来在前缀处有效利用离线信息，同时保持训练稳定。

Result: 大量实验表明 SOUP 在性能上持续优于标准 on-policy 训练和现有的 off-policy 扩展；细粒度的单样本混合策略提高了语言模型强化学习中的探索性和最终表现。

Conclusion: 通过更细粒度、单样本的混合策略实现离线信息的稳定利用，SOUP 能提升探索和最终性能，提供了在 LLM RL 训练中的新思路与实用路径。

Abstract: On-policy reinforcement learning (RL) methods widely used for language model post-training, like Group Relative Policy Optimization (GRPO), often suffer from limited exploration and early saturation due to low sampling diversity. While off-policy data can help, current approaches that mix entire trajectories cause significant policy mismatch and instability. In this work, we propose the $\textbf{S}$ingle-sample Mix-p$\textbf{O}$licy $\textbf{U}$nified $\textbf{P}$aradigm (SOUP), a framework that unifies off- and on-policy learning within individual samples at the token level. It confines off-policy influence to the prefix of a generated sequence sampled from historical policies, while the continuation is generated on-policy. Through token-level importance ratios, SOUP effectively leverages off-policy information while preserving training stability. Extensive experiments demonstrate that SOUP consistently outperforms standard on-policy training and existing off-policy extensions. Our further analysis clarifies how our fine-grained, single-sample mix-policy training can improve both exploration and final performance in LLM RL.

</details>


### [24] [DimStance: Multilingual Datasets for Dimensional Stance Analysis](https://arxiv.org/abs/2601.21483)
*Jonas Becker,Liang-Chih Yu,Shamsuddeen Hassan Muhammad,Jan Philip Wahle,Terry Ruas,Idris Abdulmumin,Lung-Hao Lee,Wen-Ni Liu,Tzu-Mi Lin,Zhe-Yu Xu,Ying-Lung Lin,Jin Wang,Maryam Ibrahim Mukhtar,Bela Gipp,Saif M. Mohammed*

Main category: cs.CL

TL;DR: DimStance 是首个带情感维度（价性-激活）标注的多语言立场资源，提出面向回归的维度化立场任务，覆盖5种语言、2个领域的大规模注释数据集。实验表明微调的大型语言模型在回归任务上表现具有竞争力，但在低资源语言上仍存在挑战，基于令牌的生成存在局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的立场检测多为离散类别标签，无法捕捉表达中的细粒度情感信息。本研究通过情感科学的价性-激活维度，将立场建模为连续值，以实现对立场表达的更丰富理解，并建立多语言、情感感知的分析与评测基准。

Method: 构建 DimStance 数据集：覆盖11,746个目标方面、7,365个文本，跨5种语言（英语、德语、中文、尼日利亚皮钦语、斯瓦希里语）与2个领域（政治与环境保护）；提出维度化立场回归任务；在回归与提示设置下，对预训练模型和大语言模型进行基准评测。

Result: 微调的LLM回归模型表现具竞争力；但低资源语言仍存在显著挑战；基于token的生成存在局限性。

Conclusion: DimStance 为多语种的情感感知立场分析与基准评测提供了基础，促进情感认知维度在立场分析中的研究与应用。

Abstract: Stance detection is an established task that classifies an author's attitude toward a specific target into categories such as Favor, Neutral, and Against. Beyond categorical stance labels, we leverage a long-established affective science framework to model stance along real-valued dimensions of valence (negative-positive) and arousal (calm-active). This dimensional approach captures nuanced affective states underlying stance expressions, enabling fine-grained stance analysis. To this end, we introduce DimStance, the first dimensional stance resource with valence-arousal (VA) annotations. This resource comprises 11,746 target aspects in 7,365 texts across five languages (English, German, Chinese, Nigerian Pidgin, and Swahili) and two domains (politics and environmental protection). To facilitate the evaluation of stance VA prediction, we formulate the dimensional stance regression task, analyze cross-lingual VA patterns, and benchmark pretrained and large language models under regression and prompting settings. Results show competitive performance of fine-tuned LLM regressors, persistent challenges in low-resource languages, and limitations of token-based generation. DimStance provides a foundation for multilingual, emotion-aware, stance analysis and benchmarking.

</details>


### [25] [inversedMixup: Data Augmentation via Inverting Mixed Embeddings](https://arxiv.org/abs/2601.21543)
*Fanshuang Kong,Richong Zhang,Qiyu Sun,Zhijie Nie,Ting Deng,Chunming Hu*

Main category: cs.CL

TL;DR: 提出 inversedMixup，通过对齐任务模型输出嵌入与LLM输入嵌入的空间，将混合嵌入映射到可读句子，实现可控性与可解释性的统一文本增强，并揭示并缓解文本Mixup的流形侵入现象，且在少样本与全监督设置均具有效果。


<details>
  <summary>Details</summary>
Motivation: 解决 Mixup 在潜在嵌入层进行线性混合后产生不可读输出的问题，同时克服基于LLM的Token级输出在 controllability 方面的局限性，企图在嵌入空间与离散Token空间之间建立可控、可解释的桥梁。

Method: 提出三阶段训练来对齐任务模型的输出嵌入空间与LLM的输入嵌入空间；对齐成功后，将带混合比的输出嵌入重构为人类可读的增强句子，实现对混合比的控制；提出并论证文本混合中的流形侵入现象，并给出简单有效的缓解策略。

Result: 实验表明该方法在少样本和全监督等设置下具有良好泛化和增强效果，且首次给出文本Mixup的流形侵入的实证证据与缓解策略。

Conclusion: InversedMixup 为文本增强提供一个统一框架，兼具可控性和可解释性；通过对齐嵌入空间实现从混合嵌入到可读句子的转换，并揭示并缓解流形侵入，具备良好普适性。

Abstract: Mixup generates augmented samples by linearly interpolating inputs and labels with a controllable ratio. However, since it operates in the latent embedding level, the resulting samples are not human-interpretable. In contrast, LLM-based augmentation methods produce sentences via prompts at the token level, yielding readable outputs but offering limited control over the generation process. Inspired by recent advances in LLM inversion, which reconstructs natural language from embeddings and helps bridge the gap between latent embedding space and discrete token space, we propose inversedMixup, a unified framework that combines the controllability of Mixup with the interpretability of LLM-based generation. Specifically, inversedMixup adopts a three-stage training procedure to align the output embedding space of a task-specific model with the input embedding space of an LLM. Upon successful alignment, inversedMixup can reconstruct mixed embeddings with a controllable mixing ratio into human-interpretable augmented sentences, thereby improving the augmentation performance. Additionally, inversedMixup provides the first empirical evidence of the manifold intrusion phenomenon in text Mixup and introduces a simple yet effective strategy to mitigate it. Extensive experiments demonstrate the effectiveness and generalizability of our approach in both few-shot and fully supervised scenarios.

</details>


### [26] [Note2Chat: Improving LLMs for Multi-Turn Clinical History Taking Using Medical Notes](https://arxiv.org/abs/2601.21551)
*Yang Zhou,Zhenting Sheng,Mingrui Tan,Yuting Song,Jun Zhou,Yu Heng Kwan,Lian Leng Low,Yang Bai,Yong Liu*

Main category: cs.CL

TL;DR: Note2Chat: a note-driven framework that converts real medical notes into doctor-patient dialogues using a decision-tree guided generation/refinement pipeline. It uses a three-stage fine-tuning strategy (supervised learning, simulated data augmentation, preference learning) and a novel single-turn reasoning paradigm to frame history taking as sequential single-turn reasoning tasks. It significantly improves clinical reasoning, outperforming GPT-4o by +16.9 F1 and +21.0 Top-1 diagnostic accuracy. Code and data available at GitHub.


<details>
  <summary>Details</summary>
Motivation: LLMs often underperform in dynamic, multi-turn diagnostic settings and rely on scarce sensitive dialogue data. There is a need to leverage widely available medical notes to train realistic doctor-patient dialogues, improving interpretability, adaptability, and data efficiency.

Method: Convert real-world medical notes into high-quality doctor-patient dialogues via a decision tree-guided generation and refinement pipeline. Employ a three-stage fine-tuning strategy: supervised learning, simulated data augmentation, and preference learning. Introduce a single-turn reasoning paradigm that treats history taking as a sequence of single-turn reasoning problems, enabling local supervision and better sample efficiency.

Result: Empirical results show substantial improvements in clinical reasoning, with gains of +16.9 F1 and +21.0 Top-1 diagnostic accuracy over GPT-4o. The authors provide code and dataset at the linked GitHub repository.

Conclusion: Note2Chat enhances clinical reasoning through a note-driven, interpretable framework that reduces reliance on scarce dialogue data and improves sample efficiency; it offers a scalable path to train LLMs for structured history taking and diagnosis.

Abstract: Effective clinical history taking is a foundational yet underexplored component of clinical reasoning. While large language models (LLMs) have shown promise on static benchmarks, they often fall short in dynamic, multi-turn diagnostic settings that require iterative questioning and hypothesis refinement. To address this gap, we propose \method{}, a note-driven framework that trains LLMs to conduct structured history taking and diagnosis by learning from widely available medical notes. Instead of relying on scarce and sensitive dialogue data, we convert real-world medical notes into high-quality doctor-patient dialogues using a decision tree-guided generation and refinement pipeline. We then propose a three-stage fine-tuning strategy combining supervised learning, simulated data augmentation, and preference learning. Furthermore, we propose a novel single-turn reasoning paradigm that reframes history taking as a sequence of single-turn reasoning problems. This design enhances interpretability and enables local supervision, dynamic adaptation, and greater sample efficiency. Experimental results show that our method substantially improves clinical reasoning, achieving gains of +16.9 F1 and +21.0 Top-1 diagnostic accuracy over GPT-4o. Our code and dataset can be found at https://github.com/zhentingsheng/Note2Chat.

</details>


### [27] [Language Models as Artificial Learners: Investigating Crosslinguistic Influence](https://arxiv.org/abs/2601.21587)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: Using language models to simulate crosslinguistic influence (CLI) in bilingualism by manipulating L1 dominance, L2 proficiency, and pretraining, and by cross-linguistic priming to reveal how L1 activation affects L2 processing; findings support that dominance/proficiency predict CLI, that grammatical priming is bidirectional while ungrammatical priming depends on dominance, and that L1 co-activation shapes L2 processing, proposing LMs as a framework for human CLI theories.


<details>
  <summary>Details</summary>
Motivation: Address inconsistent results in human CLI studies due to experimental variance; employ controlled statistical learners (language models) to isolate drivers of CLI.

Method: Systematic manipulation of L1 dominance and L2 proficiency via L2 age of exposure (training step for L2); pretraining on L1s with varying syntactic distance to L2; cross-linguistic priming to test L1 activation effects on L2 processing; analysis of L1 co-activation and neural circuitry implications in LM processing.

Result: Findings align with psycholinguistic evidence: language dominance and proficiency strongly predict CLI; grammatical-structure priming is bidirectional; priming of ungrammatical structures is modulated by dominance; L1 co-activation during L2 processing; LMs reveal mechanistic traces of CLI in neural circuitry; demonstrates LMs as a useful computational framework for human CLI theories.

Conclusion: LMs provide a computational framework to inform theories of human CLI, enabling controlled exploration of drivers and mechanistic underpinnings of CLI across languages.

Abstract: Despite the centrality of crosslinguistic influence (CLI) to bilingualism research, human studies often yield conflicting results due to inherent experimental variance. We address these inconsistencies by using language models (LMs) as controlled statistical learners to systematically simulate CLI and isolate its underlying drivers. Specifically, we study the effect of varying the L1 language dominance and the L2 language proficiency, which we manipulate by controlling the L2 age of exposure -- defined as the training step at which the L2 is introduced. Furthermore, we investigate the impact of pretraining on L1 languages with varying syntactic distance from the L2. Using cross-linguistic priming, we analyze how activating L1 structures impacts L2 processing. Our results align with evidence from psycholinguistic studies, confirming that language dominance and proficiency are strong predictors of CLI. We further find that while priming of grammatical structures is bidirectional, the priming of ungrammatical structures is sensitive to language dominance. Finally, we provide mechanistic evidence of CLI in LMs, demonstrating that the L1 is co-activated during L2 processing and directly influences the neural circuitry recruited for the L2. More broadly, our work demonstrates that LMs can serve as a computational framework to inform theories of human CLI.

</details>


### [28] [ILRR: Inference-Time Steering Method for Masked Diffusion Language Models](https://arxiv.org/abs/2601.21647)
*Eden Avrahami,Eliya Nachmani*

Main category: cs.CL

TL;DR: ILRR是一种学习无关的框架，通过使用单一参考序列在推理时引导离散扩散语言模型的生成；通过在去噪过程各阶段动态对齐生成序列与参考的内部激活，实现属性控制，并引入 Spatially Modulated Steering 以跨序列长度扩展 steering；在相同计算预算下具备较低开销、显著提升属性控制效果的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的离散扩散语言模型缺乏有效的推理阶段控制机制，现有方法多以采样层面引导或轨迹优化为主，缺乏一个学习不依赖、可扩展且对长文本可用的控制框架。

Method: 提出学习无关的 Iterative Latent Representation Refinement (ILRR) 框架，通过在去噪过程中动态将生成序列的内部激活与给定参考序列的激活对齐来进行引导；引入可调 steering scale 以实现对诸如情感等属性的灵活控制。并扩展出 Spatially Modulated Steering，通过对序列中不同位置调节引导强度以使用较短的参考来 steering 长文本。

Result: 在 LLaDA 和 MDLM 架构上验证了 ILRR 的有效属性 steering，且与同类基线相比在相同计算预算下仅需每个去噪步额外一个前向计算即可实现属性控制，属性准确率提升约 10-60 个百分点，并保持较高的文本生成质量。

Conclusion: ILRR 提供了一种实用、可扩展的 DLM 控制方法，具有可调属性、低开销以及对长文本的支持，显著提升推理阶段的可控性与生成质量。

Abstract: Discrete Diffusion Language Models (DLMs) offer a promising non-autoregressive alternative for text generation, yet effective mechanisms for inference-time control remain relatively underexplored. Existing approaches include sampling-level guidance procedures or trajectory optimization mechanisms. In this work, we introduce Iterative Latent Representation Refinement (ILRR), a learning-free framework for steering DLMs using a single reference sequence. ILRR guides generation by dynamically aligning the internal activations of the generated sequence with those of a given reference throughout the denoising process. This approach captures and transfers high-level semantic properties, with a tunable steering scale enabling flexible control over attributes such as sentiment. We further introduce Spatially Modulated Steering, an extension that enables steering long texts using shorter references by regulating guidance intensity across the sequence. Empirically, we demonstrate that ILRR achieves effective attribute steering on LLaDA and MDLM architectures with a minor computational overhead, requiring only one additional parallel forward pass per denoising step. Under the same compute budget, ILRR improves attribute accuracy over comparable baselines by 10$\%$ to 60$\%$ points, while maintaining high generation quality.

</details>


### [29] [AdaptBPE: From General Purpose to Specialized Tokenizers](https://arxiv.org/abs/2601.21665)
*Vijini Liyanage,François Yvon*

Main category: cs.CL

TL;DR: 提出一种基于后训练适应的子词分词策略：通过在给定目标词汇表大小下，选取能最有效编码适应语料的低效用词元，通过替换来提升特定领域/语言的编码效率，与通用分词器相比在测试语料上的压缩效果更优。代码开放。


<details>
  <summary>Details</summary>
Motivation: 通用分词器在训练和推理阶段对所有文本一视同仁，可能在特定领域或语言场景上效率低下；需要一种轻量级的、域自适应的分词方案，以提高域内数据的表示效率与模型应用性能。

Method: 在后训练阶段，对目标词汇表大小进行约束的条件下，识别并替换低效用、在适应语料中频度较高的替代 Token，构造能更高效编码适应语料的 token 库。通过评估不同替换策略对适应语料的压缩效果，选择最优化的词汇表。多语言设置下在生成与分类任务上进行广泛实验。

Result: 与同一词汇表大小的基线相比，改造后的分词器在测试语料上的压缩效率更高，且在多语言的生成与分类任务中表现更优，证明该策略是一个轻量级、可扩展的域自适应分词方法。

Conclusion: 后训练适应提供了一种高效的领域/任务特定分词的实现路径，类似于词汇表微调，便于在不同领域或任务中快速定制化分词器。代码与数据公开，便于复现与应用。

Abstract: Subword tokenization methods, such as Byte-Pair Encoding (BPE), significantly impact the performance and efficiency of large language models (LLMs). The standard approach involves training a general-purpose tokenizer that uniformly processes all textual data during both training and inference. However, the use of a generic set of tokens can incur inefficiencies when applying the model to specific domains or languages. To address this limitation, we propose a post-training adaptation strategy that selectively replaces low-utility tokens with more relevant ones based on their frequency in an adaptation corpus. Our algorithm identifies the token inventory that most effectively encodes the adaptation corpus for a given target vocabulary size. Extensive experiments on generation and classification tasks across multiple languages demonstrate that our adapted tokenizers compress test corpora more effectively than baselines using the same vocabulary size. This method serves as a lightweight adaptation mechanism, akin to a vocabulary fine-tuning process, enabling optimized tokenization for specific domains or tasks. Our code and data are available at https://github.com/vijini/Adapt-BPE.git.

</details>


### [30] [Scale-Dependent Semantic Dynamics Revealed by Allan Deviation](https://arxiv.org/abs/2601.21678)
*Debayan Dasgupta*

Main category: cs.CL

TL;DR: 将文本语义演变视为高维随机过程，利用 Allan 偏差揭示两尺度动力学，并比较人类文本与大语言模型的稳定性与连贯性。


<details>
  <summary>Details</summary>
Motivation: 量化语义连贯性的物理性质，提供区分人类认知与算法生成文本的框架。

Method: 把有序句子嵌入表示成位移信号，使用 Allan deviation 作为稳态性的度量，分析短时功率律和长时噪声地坪；比较创意文学、技术文本以及大型语言模型产生的文本的统计特征。

Result: 发现两种动力学尺度：短时呈现幂律扩散区，区分创意文本与技术文本；长期过渡到稳定性受限的噪声地坪；LLMs 能再现局部缩放统计，但稳定性时间尺度（稳定性视界）受限，文本的语义连贯性可作为物理量来区分人类与模型文本。

Conclusion: 提出把语义连贯性作为可测量的物理性质，为区分人类认知与算法文本提供框架；为研究文本动力学提供量化工具。

Abstract: While language progresses through a sequence of semantic states, the underlying dynamics of this progression remain elusive. Here, we treat the semantic progression of written text as a stochastic trajectory in a high-dimensional state space. We utilize Allan deviation, a tool from precision metrology, to analyze the stability of meaning by treating ordered sentence embeddings as a displacement signal. Our analysis reveals two distinct dynamical regimes: short-time power-law scaling, which differentiates creative literature from technical texts, and a long-time crossover to a stability-limited noise floor. We find that while large language models successfully mimic the local scaling statistics of human text, they exhibit a systematic reduction in their stability horizon. These results establish semantic coherence as a measurable physical property, offering a framework to differentiate the nuanced dynamics of human cognition from the patterns generated by algorithmic models.

</details>


### [31] [Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.21684)
*Xinglin Wang,Jiayi Shi,Shaoxiong Feng,Peiwen Yuan,Yiwei Li,Yueqi Zhang,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.CL

TL;DR: 提出 Recycling Search Experience (RSE) 的训练无关、自引导的测试时推理改进策略，通过将搜索轨迹蒸馏到共用经验库，实现正向 recycled 和负向 prune，提升在 HMMT/HLE 等复杂推理任务的扩展效率。


<details>
  <summary>Details</summary>
Motivation: 当前的搜索策略将 rollouts 视为一次性样本，导致重复推导和回避死角，造成巨大计算冗余，迫切需要提高推理效率和信息复用。

Method: 提出自引导、训练无关的 Recycling Search Experience (RSE): 将原始推理轨迹蒸馏到一个共享的经验库中，使中间结论得以正向循环利用、失败模式得以负向回收，从而将测试时搜索从单次试验序列转化为持续累积的过程。

Result: 在 HMMT24、HMMT25、IMO-Bench、HLE 等任务上，RSE 相对于强基线在计算成本相近的情况下实现更高性能，展现出领先的扩展效率。

Conclusion: RSE 将测试时搜索从独立试验转化为累积过程，显著提升大语言模型在复杂推理任务中的效率与可扩展性。

Abstract: Test-Time Scaling enhances the reasoning capabilities of Large Language Models by allocating additional inference compute to broaden the exploration of the solution space. However, existing search strategies typically treat rollouts as disposable samples, where valuable intermediate insights are effectively discarded after each trial. This systemic memorylessness leads to massive computational redundancy, as models repeatedly re-derive discovered conclusions and revisit known dead ends across extensive attempts. To bridge this gap, we propose \textbf{Recycling Search Experience (RSE)}, a self-guided, training-free strategy that turns test-time search from a series of isolated trials into a cumulative process. By actively distilling raw trajectories into a shared experience bank, RSE enables positive recycling of intermediate conclusions to shortcut redundant derivations and negative recycling of failure patterns to prune encountered dead ends. Theoretically, we provide an analysis that formalizes the efficiency gains of RSE, validating its advantage over independent sampling in solving complex reasoning tasks. Empirically, extensive experiments on HMMT24, HMMT25, IMO-Bench, and HLE show that RSE consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art scaling efficiency.

</details>


### [32] [Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis](https://arxiv.org/abs/2601.21709)
*Qingyue Yang,Jie Wang,Xing Li,Yinqi Bai,Xialiang Tong,Huiling Zhen,Jianye Hao,Mingxuan Yuan,Bin Li*

Main category: cs.CL

TL;DR: 提出 TAPPA 框架，从时间维度分析注意力模式的可预测性与不可预测性，将注意力模式统一解释为由查询自相似性决定，并用于 KV 缓存压缩和推理剪枝等加速任务。


<details>
  <summary>Details</summary>
Motivation: 现有关于注意力模式的观察分散且缺乏统一、可解释的框架。需要从数学角度统一解释并指引推理加速。

Method: 以时间连续角度分析注意力的可预测与不可预测模式，定义基于查询自相似性的度量；对可预测模式的三类代表性情形进行联结查询/键与 RoPE 的分析；提出受 TAPPA 启发的度量以提升推理加速效果。

Result: 在 KV 缓存压缩和 LLM 剪枝任务中，该度量显著提升性能，优于基线方法，且提供实现代码。

Conclusion: TAPPA 提供一个统一、可解释的框架，用以理解并利用注意力模式，提升推理加速的工程价值。

Abstract: Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce \textbf{Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations} from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA.

</details>


### [33] [TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2601.21711)
*Huiyuan Lai,Malvina Nissim*

Main category: cs.CL

TL;DR: 提出 TACLer，一种基于模型自适应课程学习的强化学习框架，通过混合Thinking/NoThinking策略和逐步增加数据难度，降低成本并提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 解决长式推理（长CoT）需要大规模RL训练、计算成本高且易过度思考的问题，同时寻求在效率与准确性之间取得平衡。

Method: 提出两大核心组件：1) 定制化课程学习，根据模型能力确定逐步学习的知识与阶段；2) 混合Thinking/NoThinking推理范式，开启或关闭Thinking模式以权衡准确性与效率。

Result: 在训练成本方面显著降低（训练Compute下降>50%，推理耗时比基线模型下降>42%），在准确性方面提升>9%，并在四个涉及复杂问题的数学数据集上优于状态-艺术的NoThinking和Thinking基线。

Conclusion: TACLer 在学习效率与推理性能上实现两重优势，证明了模型定制化课程学习与混合思维策略的有效性。

Abstract: Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.

</details>


### [34] [Enhancing Language Models for Robust Greenwashing Detection](https://arxiv.org/abs/2601.21722)
*Neil Heinrich Braun,Keane Ong,Rui Mao,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: 提出一种参数高效框架，通过对比学习与有序排名相结合来结构化LLM潜在空间，结合门控特征调制和MetaGradNorm以提升ESG披露的鲁棒性；在跨类别设置上优于基线，但存在表征刚性与泛化之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 绿色洗牌和模糊表达削弱ESG报告的可信度；现有NLP模型对这些做法缺乏鲁棒性，往往依赖表层模式。需要一种能捕捉具体行动与模糊声明之间分层差异的表示学习框架。

Method: 将对比学习与有序排名目标结合，以结构化LLM潜在空间，捕捉具体行动与模糊声明之间的渐进差异；引入门控特征调制以过滤披露噪声；采用MetaGradNorm来稳定多目标优化；在跨类别数据上进行实验。

Result: 在跨类别设置中，表现出比标准基线更高的鲁棒性；揭示了表征刚性与泛化之间的权衡。

Conclusion: 该方法提升了ESG披露分析中的鲁棒性，但需要在表示刚性和泛化能力之间找到平衡；未来工作可聚焦于进一步缓解权衡并扩展到其他领域。

Abstract: Sustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.

</details>


### [35] [CE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering](https://arxiv.org/abs/2601.21733)
*Jiayin Lan,Jiaqi Li,Baoxin Wang,Ming Liu,Dayong Wu,Shijin Wang,Bing Qin,Guoping Hu*

Main category: cs.CL

TL;DR: 提出 CE-GOCD，通过以论文题目为中心实体的子图检索、子图裁剪与补全，以及社区检测来提升基于大语言模型的科研问答检索增强。对 NLP 文献问答数据集进行评估，结果显示优于其他检索增强基线。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强方法多基于孤立文本片段或概念，未能挖掘论文之间的深层语义连接，导致问答的覆盖性和特异性受限。需要在学术知识图谱中显式建模语义子结构以提升理解。

Method: (1) 以论文题目作为中心实体，进行有针对性的子图检索；(2) 通过子图裁剪与补全提升隐性语义发现；(3) 应用社区检测提炼具有共同主题的论文群组。

Result: 在三个 NLP 文献问答数据集上，CE-GOCD 显著优于其他检索增强基线，验证了该框架的有效性。

Conclusion: 通过显式建模学术知识图谱中的语义子结构，结合中心实体引导的子图与社区检测，提升了基于 LLM 的科学问答的覆盖性与针对性。

Abstract: Large Language Models (LLMs) are increasingly used for question answering over scientific research papers. Existing retrieval augmentation methods often rely on isolated text chunks or concepts, but overlook deeper semantic connections between papers. This impairs the LLM's comprehension of scientific literature, hindering the comprehensiveness and specificity of its responses. To address this, we propose Central Entity-Guided Graph Optimization for Community Detection (CE-GOCD), a method that augments LLMs' scientific question answering by explicitly modeling and leveraging semantic substructures within academic knowledge graphs. Our approach operates by: (1) leveraging paper titles as central entities for targeted subgraph retrieval, (2) enhancing implicit semantic discovery via subgraph pruning and completion, and (3) applying community detection to distill coherent paper groups with shared themes. We evaluated the proposed method on three NLP literature-based question-answering datasets, and the results demonstrate its superiority over other retrieval-augmented baseline approaches, confirming the effectiveness of our framework.

</details>


### [36] [CoFrGeNet: Continued Fraction Architectures for Language Generation](https://arxiv.org/abs/2601.21766)
*Amit Dhurandhar,Vijil Chenthamarakshan,Dennis Wei,Tejaswini Pedapati,Karthikeyan Natesan Ramamurthy,Rahul Nair*

Main category: cs.CL

TL;DR: 提出一种基于连分数的新函数族 CoFrGeNets，作为 Transformer 的替代组件，显著减少参数量并缩短预训练时间，同时在 GPT2-xl 与 Llama3 的评估中达到竞争性甚至优越的结果。


<details>
  <summary>Details</summary>
Motivation: 降低模型规模与训练成本，同时保持或提升下游任务性能；实现自定义梯度以提升训练效率与精度；组件可作为插拔替换，易于在现有训练/推理流程中集成。

Method: 提出连续分数函数类作为替代 Multi-head Attention 与 Feed-Forward 的核心组件，设计新架构模块以替代现有 Transformer 模块；推导自定义梯度以在训练中比标准 PyTorch 梯度更高效和更准确；组件以插拔式形式嵌入现有 Transformer 模块，几乎不改变训练和推理流程。

Result: 在 GPT2-xl (1.5B 参数) 与 Llama3 (3.2B 参数) 上进行预训练，分别使用 OpenWebText、GneissWeb 与 docling 数据混合集等数据集；下游任务（分类、问答、推理、文本理解）上表现与原模型相当，甚至在某些场景优于原模型，且参数量约为原模型的 1/2 到 2/3，预训练时间更短。

Conclusion: 未来工作可针对硬件进行定制化实现，以进一步发挥该架构潜力；插拔式替换设计使其易于在大规模工业工作流中落地。

Abstract: Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\frac{2}{3}$ to $\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.

</details>


### [37] [Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond](https://arxiv.org/abs/2601.21767)
*Wei Zhu*

Main category: cs.CL

TL;DR: ChatGPT在4类MedIE任务上的表现落后于微调基线，但具备较高的解释性和文本忠实度；存在过度自信和生成不确定性，可能影响信息抽取的稳定性。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在医疗信息抽取任务上的总体能力及局限性，跨4类MedIE任务和6个基准数据集，量化性能、解释性、置信度、忠实度与不确定性之间的关系。

Method: 在6个基准数据集上的4类MedIE任务中评估ChatGPT的性能，并系统性分析其解释性、置信度、忠实度和不确定性；并与微调基线模型进行对比。

Result: a) ChatGPT在MedIE任务上的性能落后于微调基线模型。b) ChatGPT能给出高质量的决策解释，但存在预测过度自信问题。c) 在大多数案例中对原文具有较高的忠实度。d) 生成过程中的不确定性会传递到信息抽取结果，限制实际应用。

Conclusion: 当前ChatGPT尚不足以替代专门的微调模型在MedIE上的性能，但在解释性和文本忠实度方面具备优势。需通过置信度校准、处理生成不确定性，以及整合外部知识或检索机制来提升MedIE应用的有效性。

Abstract: Large Language Models (LLMs) like ChatGPT have demonstrated amazing capabilities in comprehending user intents and generate reasonable and useful responses. Beside their ability to chat, their capabilities in various natural language processing (NLP) tasks are of interest to the research community. In this paper, we focus on assessing the overall ability of ChatGPT in 4 different medical information extraction (MedIE) tasks across 6 benchmark datasets. We present the systematically analysis by measuring ChatGPT's performance, explainability, confidence, faithfulness, and uncertainty. Our experiments reveal that: (a) ChatGPT's performance scores on MedIE tasks fall behind those of the fine-tuned baseline models. (b) ChatGPT can provide high-quality explanations for its decisions, however, ChatGPT is over-confident in its predcitions. (c) ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. (d) The uncertainty in generation causes uncertainty in information extraction results, thus may hinder its applications in MedIE tasks.

</details>


### [38] [Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention](https://arxiv.org/abs/2601.21768)
*Alon Rozental*

Main category: cs.CL

TL;DR: Zonkey提出一个可端到端可微分的文本生成管线，包含一个可学习的分词器（Segment Splitter）和概率注意力机制，通过层级Diffusion在字符到句子等多层抽象上进行压缩与重建，实现对BPE等非可微分分词器的替代，支持可变长度输出并展现出 Emergent 语义层次。


<details>
  <summary>Details</summary>
Motivation: 现有的固定、不可微分分词器（如BPE）限制了端到端优化、领域自适应与对噪声数据的鲁棒性。需要一个端到端可微、可学习且能处理可变长度输出的分词与表示学习框架，以提升LLM的可扩展性和生成质量。

Method: 核心是一个可微分的分词器Segment Splitter，学习BOS的概率性决策以自适应地产生分块；提出Probabilistic Attention，通过位置特定的存在概率实现软遮蔽的近似并保持梯度；序列以概率衰减方式结束而非固定的EOS；多层级结构将字符n-gram压缩为词向量，再进一步压缩为句子向量，最终通过DDMM在潜在空间实现稳定高效的去噪重建；Stitcher实现跨分段的一致性；端到端在维基百科数据上训练，输出来自噪声的连贯、可变长度文本，且呈现出层级涌现与与数据分布更好的一致性。

Result: 在定性评估中，Zonkey能够生成连贯的、可变长度的文本，并显现出从字符到词再到句的层级涌现，与基于信息熵的可学习分词器相比具有更符合数据分布的输出潜力；同时展示了端到端梯度训练的可行性与潜在的领域自适应能力。

Conclusion: 该方法推进向全梯度的语言模型迈进，具有在领域自适应与大规模生成中的潜力；并且提供了可复现的代码以便进一步评估与扩展。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet they remain constrained by fixed, non-differentiable tokenizers like Byte Pair Encoding (BPE), which hinder end-to-end optimization and adaptability to noisy or domain-specific data. We introduce Zonkey, a hierarchical diffusion model that addresses these limitations through a fully trainable pipeline from raw characters to document-level representations. At its core is a differentiable tokenizer (Segment Splitter) that learns probabilistic beginning-of-sequence (BOS) decisions, enabling adaptive splits that emerge as linguistically meaningful (e.g., word boundaries at spaces, sentence starts at periods) without explicit supervision. This differentiability is enabled by our novel Probabilistic Attention mechanism, which incorporates position-specific existence probabilities to simulate soft masking over theoretically infinite sequences while preserving gradients. Sequences decay probabilistically rather than relying on end-of-sequence tokens, supporting variable-length outputs. Hierarchical levels compress sequences into higher abstractions (e.g., character n-grams to word-like vectors, then sentence-like), with reconstruction via our Denoising Diffusion Mixed Model (DDMM) for stable and efficient denoising in latent space. A Stitcher ensures overlap invariance across segments. Trained end-to-end on Wikipedia, Zonkey generates coherent, variable-length text from noise, demonstrating emergent hierarchies and promising qualitative alignment to data distributions compared to entropy-based learnable tokenizers. Our approach advances toward fully gradient-based LLMs, with potential for better domain adaptation and scalable generation. We release the source code for training and reproducing our experiments.

</details>


### [39] [KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection](https://arxiv.org/abs/2601.21796)
*Yaocong Li,Leihan Zhang,Le Zhang,Qiang Yan*

Main category: cs.CL

TL;DR: KID leverages knowledge-grounded dual-head learning with label-constrained distillation to perform harmful meme detection via structured reasoning chains that connect visual evidence, background knowledge, and labels, achieving state-of-the-art results on multilingual datasets.


<details>
  <summary>Details</summary>
Motivation: Harmful memes often convey implicit toxicity requiring external knowledge; existing methods rely mainly on intra-/inter-modal signals and lack grounded reasoning with external knowledge.

Method: Label-constrained distillation to decompose meme understanding into reasoning chains; dual-head architecture jointly optimizes semantic generation and classification; grounding external knowledge in meme-specific contexts.

Result: State-of-the-art performance on binary and multi-label harmful meme detection across five multilingual datasets (English, Chinese, Bengali); improvements of 2.1%–19.7% over previous best methods; ablations confirm benefits of knowledge injection and dual-head learning.

Conclusion: Knowledge injection and dual-head joint learning yield robust, generalizable meme understanding; code and data are released for reproducibility.

Abstract: Internet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%--19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at https://github.com/PotatoDog1669/KID.

</details>


### [40] [Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation](https://arxiv.org/abs/2601.21797)
*Yimin Deng,Yuqing Fu,Derong Xu,Yejing Wang,Wei Ni,Jingtong Gao,Xiaopeng Li,Chengxu Liu,Xiao Han,Guoshuai Zhao,Xiangyu Zhao,Li Zhu,Xueming Qian*

Main category: cs.CL

TL;DR: 提出 Adversarial Memory Adaptation (AMA) 以对齐离线记忆构建/更新与下游任务目标，通过对抗式三方循环（ challenger/evaluator/adapter ），在离线阶段获取任务相关监督，提升长对话记忆系统的任务适应性；在 LoCoMo 基准上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统的离线阶段固定且任务无关，导致记忆内容与任务需求错配，限制下游任务表现。需要让离线阶段具备任务导向的监督。

Method: 引入 challenger 产生基于原始对话的问题-答案对；利用构建的记忆回答这些问题以模拟下游推理；evaluator 进行错误分析；adapter 针对错误案例对构建策略与记忆内容双层更新；通过该流程在离线阶段提供任务相关的监督信号。AMA 可嵌入现有记忆系统。

Result: 在长对话基准 LoCoMo 上的广泛实验表明 AMA 能提升记忆系统对下游任务的适应性和性能。

Conclusion: AMA 能与现有记忆系统无缝整合，通过对抗性、任务目标驱动的离线训练，提升长对话情境中的记忆系统效果。

Abstract: Conversational agents struggle to handle long conversations due to context window limitations. Therefore, memory systems are developed to leverage essential historical information. Existing memory systems typically follow a pipeline of offline memory construction and update, and online retrieval. Despite the flexible online phase, the offline phase remains fixed and task-independent. In this phase, memory construction operates under a predefined workflow and fails to emphasize task relevant information. Meanwhile, memory updates are guided by generic metrics rather than task specific supervision. This leads to a misalignment between offline memory preparation and task requirements, which undermines downstream task performance. To this end, we propose an Adversarial Memory Adaptation mechanism (AMA) that aligns memory construction and update with task objectives by simulating task execution. Specifically, first, a challenger agent generates question answer pairs based on the original dialogues. The constructed memory is then used to answer these questions, simulating downstream inference. Subsequently, an evaluator agent assesses the responses and performs error analysis. Finally, an adapter agent analyzes the error cases and performs dual level updates on both the construction strategy and the content. Through this process, the memory system receives task aware supervision signals in advance during the offline phase, enhancing its adaptability to downstream tasks. AMA can be integrated into various existing memory systems, and extensive experiments on long dialogue benchmark LoCoMo demonstrate its effectiveness.

</details>


### [41] [RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes](https://arxiv.org/abs/2601.21803)
*Korbinian Randl,Guido Rocchietti,Aron Henriksson,Ziawasch Abedjan,Tony Lindgren,John Pavlopoulos*

Main category: cs.CL

TL;DR: 提出 RAG-E 框架，通过可解释性分析来对齐检索器与生成器在检索增强生成中的协同工作，并量化这种对齐程度。


<details>
  <summary>Details</summary>
Motivation: 解决 RAG 系统在高风险领域部署中的透明性不足，需端到端的可解释性与对齐评估。

Method: 将 Integrated Gradients 应用于检索器分析；引入 PMCSHAP（Monte Carlo 稳定化的 Shapley 值近似）用于生成器归因；提出 Weighted Attribution-Relevance Gap (WARG) 指标评估生成器对文档的使用是否与检索器的排名相一致。

Result: 在 TREC CAsT 与 FoodSafeSum 数据集上发现显著的不对齐：47.4%–66.7% 的查询中生成器忽略检索器的顶级文档，另有 48.1%–65.9% 的查询依赖于次级相关文档。

Conclusion: RAG 输出质量不仅取决于单独组件的性能，也取决于它们的相互作用；RAG-E 提供一个可审计的端到端对齐分析框架。

Abstract: Retrieval-Augmented Generation (RAG) systems combine dense retrievers and language models to ground LLM outputs in retrieved documents. However, the opacity of how these components interact creates challenges for deployment in high-stakes domains. We present RAG-E, an end-to-end explainability framework that quantifies retriever-generator alignment through mathematically grounded attribution methods. Our approach adapts Integrated Gradients for retriever analysis, introduces PMCSHAP, a Monte Carlo-stabilized Shapley Value approximation, for generator attribution, and introduces the Weighted Attribution-Relevance Gap (WARG) metric to measure how well a generator's document usage aligns with a retriever's ranking. Empirical analysis on TREC CAsT and FoodSafeSum reveals critical misalignments: for 47.4% to 66.7% of queries, generators ignore the retriever's top-ranked documents, while 48.1% to 65.9% rely on documents ranked as less relevant. These failure modes demonstrate that RAG output quality depends not solely on individual component performance but on their interplay, which can be audited via RAG-E.

</details>


### [42] [Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning](https://arxiv.org/abs/2601.21804)
*Bodong Du,Xuanqi Huang,Xiaomeng Li*

Main category: cs.CL

TL;DR: MV-based TTRL suffers from information loss and biased rewards; DARE uses full rollout distribution with an exploration bonus and distribution pruning to provide a richer, denoised reward signal, yielding stronger optimization and performance gains on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Majority voting collapses diverse rollout outcomes into a single signal, discarding non-majority but correct actions and biasing rewards. A more informative, distribution-aware reward signal can improve self-improvement in LLMs under weak supervision.

Method: DARE estimates rewards from the full empirical rollout distribution rather than its majority outcome. It adds an exploration bonus to encourage exploring non-majority rollouts and applies a distribution pruning mechanism to denoise rewards and focus on informative rollouts.

Result: DARE improves optimization stability and final performance over baselines, with relative gains of 25.3% on AIME 2024 and 5.3% on AMC.

Conclusion: Accounting for the entire rollout distribution and incorporating exploration with pruning yields a more robust reward signal for TTRL, boosting performance on challenging reasoning benchmarks.

Abstract: Test-time reinforcement learning (TTRL) enables large language models (LLMs) to self-improve on unlabeled inputs, but its effectiveness critically depends on how reward signals are estimated without ground-truth supervision. Most existing TTRL methods rely on majority voting (MV) over rollouts to produce deterministic rewards, implicitly assuming that the majority rollout provides a reliable learning signal. We show that this assumption is fragile: MV reduces the rollout distribution into a single outcome, discarding information about non-majority but correct actions candidates, and yields systematically biased reward estimates. To address this, we propose Distribution-AwareReward Estimation (DARE), which shifts reward estimation from a single majority outcome to the full empirical rollout distribution. DARE further augments this distribution-based reward with an exploration bonus and a distribution pruning mechanism for non-majority rollout exploration and reward denoise, yielding a more informative and robust reward estimation. Extensive experiments on challenging reasoning benchmarks show that DARE improves optimization stability and final performance over recent baselines, achieving relative improvements of 25.3% on challenging AIME 2024 and 5.3% on AMC.

</details>


### [43] [Mil-SCORE: Benchmarking Long-Context Geospatial Reasoning and Planning in Large Language Models](https://arxiv.org/abs/2601.21826)
*Aadi Palnitkar,Mingyang Mao,Nicholas Waytowich,Vinicius G. Goecks,Tinoosh Mohsenin,Xiaomin Lin*

Main category: cs.CL

TL;DR: MilSCORE is the first scenario-level, expert-authored, multi-hop QA benchmark for long-context military planning that integrates heterogeneous sources (maps, orders, intel) to test LLMs' cross-source, geospatial reasoning; baseline results reveal current models struggle, making MilSCORE a challenging testbed for future research.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for realistic long-context benchmarks that require selective reading and integration of diverse, multi-modal information, especially for geospatial planning in high-stakes domains like military operations.

Method: Construct MilSCORE by creating an expert-authored, scenario-grounded, multi-hop question set within a complex simulated military planning scenario; include diverse question types across seven categories; provide an evaluation protocol and baseline results for contemporary vision-language models.

Result: Baseline results show substantial headroom remains; current systems have difficulty with scenario-level long-context planning and multi-source reasoning.

Conclusion: MilSCORE fills a gap in long-context benchmarks for high-stakes planning, offering a rigorous testbed to drive future improvements in LLMs and multimodal reasoning over long-horizon geospatial data.

Abstract: As large language models (LLMs) are applied to increasingly longer and more complex tasks, there is a growing need for realistic long-context benchmarks that require selective reading and integration of heterogeneous, multi-modal information sources. This need is especially acute for geospatial planning problems, such as those found in planning for large-scale military operations, which demand fast and accurate reasoning over maps, orders, intelligence reports, and other distributed data. To address this gap, we present MilSCORE (Military Scenario Contextual Reasoning), to our knowledge the first scenario-level dataset of expert-authored, multi-hop questions grounded in a complex, simulated military planning scenario used for training. MilSCORE is designed to evaluate high-stakes decision-making and planning, probing LLMs' ability to combine tactical and spatial reasoning across multiple sources and to reason over long-horizon, geospatially rich context. The benchmark includes a diverse set of question types across seven categories targeting both factual recall and multi-step reasoning about constraints, strategy, and spatial analysis. We provide an evaluation protocol and report baseline results for a range of contemporary vision-language models. Our findings highlight substantial headroom on MilSCORE, indicating that current systems struggle with realistic, scenario-level long-context planning, and positioning MilSCORE as a challenging testbed for future work.

</details>


### [44] [Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text](https://arxiv.org/abs/2601.21895)
*Hongyi Zhou,Jin Zhu,Erhan Xu,Kai Ye,Ying Yang,Chengchun Shi*

Main category: cs.CL

TL;DR: 提出一种自适应距离的改写检测算法，并给出几何视角分析以阐明改写检测原理。理论表明自适应距离优于固定距离；在超过100组实验中，该方法在多数场景优于基线，在不同目标LLM（GPT、Claude、Gemini）上实现57.8%–80.6%的相对提升。


<details>
  <summary>Details</summary>
Motivation: LLMs 生成的文本高度人类化，导致 misinformation 与学术不端风险增加；现有改写检测往往依赖固定距离度量，泛化性和鲁棒性不足。需要一种能自适应学习原文与改写文本之间距离的检测框架，以提升跨模型的检测效果。

Method: 给出一个将改写检测放在几何空间中的分析框架，揭示现有改写检测的理论基础与局限；提出一个自适应学习的距离函数，用以量化原文与改写文本之间的距离，并用于检测决策。给出理论分析，证明当距离函数可学习时相比固定距离具有更强的判别能力；进行超过100组实验，覆盖多种目标LLM。

Result: 实验结果表明该自适应距离方法在大多数设置中优于 strongest baseline；在不同目标LLM（包括 GPT、Claude、Gemini）上实现57.8%–80.6%的相对提升。

Conclusion: 自适应距离学习的改写检测具备更好的检测性能与泛化能力，几何分析有助于理解改写检测的原理，为构建对抗性更强、跨模型鲁棒的检测工具提供有效路径。

Abstract: Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their generalization ability. Building on this insight, we introduce a novel rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text. Theoretically, we demonstrate that employing an adaptively learned distance function is more effective for detection than using a fixed distance. Empirically, we conduct extensive experiments with over 100 settings, and find that our approach demonstrates superior performance over baseline algorithms in the majority of scenarios. In particular, it achieves relative improvements from 57.8\% to 80.6\% over the strongest baseline across different target LLMs (e.g., GPT, Claude, and Gemini).

</details>


### [45] [SONIC: Segmented Optimized Nexus for Information Compression in Key-Value Caching](https://arxiv.org/abs/2601.21927)
*Hong Chen,Xiang Liu,Bo Wang,Yuxuan Fan,Yuanlin Chu,Zongluo Li,Xiaowen Chu,Xuming Hu*

Main category: cs.CL

TL;DR: 提出 SONIC：一种学习驱动的 KV 缓存压缩框架，将历史对话压缩为 Nexus 令牌并实现动态预算训练，在不同内存约束下保持连贯性并提升推理效率，显著超越 H2O、StreamingLLM。


<details>
  <summary>Details</summary>
Motivation: KV 缓存的线性增长成为多轮对话部署的瓶颈；现有压缩通常基于启发式 eviction，易丢失关键上下文，需更好地利用对话结构特性实现高效压缩。

Method: 提出基于学习的框架 SONIC，通过将历史段压缩成语义丰富的 Nexus 令牌来表示多轮对话段，结合动态预算训练以实现对不同内存约束的无再训练自适应。

Result: 在 80% 与 50% 的压缩比下，SONIC 显著优于基线（如 H2O、StreamingLLM），在 MTBench101 上平均提升 35.55%，并将整体推理速度提升约 50.1%）相对于全上下文生成。

Conclusion: SONIC 能在不重新训练的前提下，通过学习驱动的历史段压缩实现高效、连贯的多轮对话推理，并且具有对内存预算的灵活适应性，适用于大规模对话部署。

Abstract: The linear growth of Key-Value (KV) cache remains a bottleneck for multi-turn LLM deployment. Existing KV cache compression methods often fail to account for the structural properties of multi-turn dialogues, relying on heuristic eviction that risks losing critical context. We propose \textbf{SONIC}, a learning-based framework that compresses historical segments into compact and semantically rich \textbf{Nexus} tokens. By integrating dynamic budget training, SONIC allows flexible adaptation to varying memory constraints without retraining. Experiments show that at compression ratios of 80\% and 50\%, SONIC consistently outperforms baselines such as H2O and StreamingLLM on four diverse multi-turn benchmarks. Specifically, on the widely used MTBench101 benchmark, SONIC achieves an average score improvement of 35.55\% over state-of-the-art baselines, validating its effectiveness in sustaining coherent multi-turn dialogues. Furthermore, SONIC enhances deployment efficiency, accelerating the overall inference process by 50.1\% compared to full-context generation.

</details>


### [46] [From Generative Modeling to Clinical Classification: A GPT-Based Architecture for EHR Notes](https://arxiv.org/abs/2601.21955)
*Fariba Afrin Irany*

Main category: cs.CL

TL;DR: 将GPT-2解码器为基础的临床文本分类，通过选择性微调实现高效适配；在大部分参数冻结的情况下，仅训练最后一个Transformer块、最终的层归一化及一个轻量分类头，显著降低参数量并保持性能。


<details>
  <summary>Details</summary>
Motivation: 面临非结构化EHR文本长度、领域特定语言、标注数据稀缺、类别不平衡以及对大规模预训练模型微调的高计算成本。需要一种高效、可扩展的临床文本分类方法。

Method: 在GPT-2解码器上进行选择性微调：冻结大部分骨架，只训练最后一个Transformer块、最后的LayerNorm和一个轻量分类头。使用来自报告文本的CheXpert风格的不确定性标签从MIMIC-IV-Note提取，用于多标签、逐标签的二元分类及疾病结局的聚合任务。评估在不同数据集规模下的收敛性和性能，重点关注否定/未提及的情况。

Result: 在不同数据量设置下，模型实现稳定收敛，分类性能强，尤其在否定与未提及找到方面表现突出；参数量和计算成本显著降低，同时保持必要的表征能力。

Conclusion: 选择性微调预训练生成式语言模型为临床文本分类提供高效且可扩展的路径，便于将真实世界EHR数据应用到实践中，同时显著降低计算复杂度。

Abstract: The increasing availability of unstructured clinical narratives in electronic health records (EHRs) has created new opportunities for automated disease characterization, cohort identification, and clinical decision support. However, modeling long, domain-specific clinical text remains challenging due to limited labeled data, severe class imbalance, and the high computational cost of adapting large pretrained language models.
  This study presents a GPT-based architecture for clinical text classification that adapts a pretrained decoder-only Transformer using a selective fine-tuning strategy. Rather than updating all model parameters, the majority of the GPT-2 backbone is frozen, and training is restricted to the final Transformer block, the final layer normalization, and a lightweight classification head. This approach substantially reduces the number of trainable parameters while preserving the representational capacity required to model complex clinical language.
  The proposed method is evaluated on radiology reports from the MIMIC-IV-Note dataset using uncertainty-aware CheXpert-style labels derived directly from report text. Experiments cover multiple problem formulations, including multi-label classification of radiographic findings, binary per-label classification under different uncertainty assumptions, and aggregate disease outcome prediction. Across varying dataset sizes, the model exhibits stable convergence behavior and strong classification performance, particularly in settings dominated by non-mention and negated findings.
  Overall, the results indicate that selective fine-tuning of pretrained generative language models provides an efficient and effective pathway for clinical text classification, enabling scalable adaptation to real-world EHR data while significantly reducing computational complexity.

</details>


### [47] [OVD: On-policy Verbal Distillation](https://arxiv.org/abs/2601.21968)
*Jing Xiong,Hui Shen,Shansan Gong,Yuxin Cheng,Jianghan Shen,Chaofan Tao,Haochen Tan,Haoli Bai,Lifeng Shang,Ngai Wong*

Main category: cs.CL

TL;DR: OVD replaces token-level probability matching with trajectory matching using discrete verbal scores (0-9) for on-policy distillation, greatly reducing memory usage and enabling exploration with verbal feedback; it achieves substantial gains on Web Q&A and math reasoning tasks while improving training efficiency.


<details>
  <summary>Details</summary>
Motivation: Token-level on-policy distillation requires alignment between student and teacher tokens, which constrains exploration, limits effective use of interactive feedback, and causes severe memory bottlenecks in reinforcement learning. A memory-efficient, on-policy framework using verbal feedback is needed.

Method: Replace token-level probability matching with trajectory matching using discrete verbal scores (0–9) produced by teacher models. This avoids token-level alignment, lowers memory consumption, and supports on-policy learning from verbal feedback in interactive environments.

Result: Empirical results show substantial improvements: up to +12.9 percentage points in average exact match (EM) on Web Q&A tasks and up to +25.7% on math benchmarks when trained with only a single random sample, along with improved training efficiency.

Conclusion: On-policy Verbal Distillation (OVD) is a memory-efficient framework for distillation with verbal feedback that eliminates token-level alignment, enhances exploration, and yields strong performance gains across evaluated tasks; project page available.

Abstract: Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io

</details>


### [48] [Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding](https://arxiv.org/abs/2601.21969)
*Yifan Zhu,Huiqiang Rong,Haoran Luo*

Main category: cs.CL

TL;DR: Token-Guard: 基于自检解码的逐步令牌级对抗幻觉的方法，通过在每一步进行内部验证与潜在空间评估来抑制幻觉，并通过迭代修剪与再生成来纠正错误。实验表明在 HALU 数据集上显著降低幻觉、提升生成准确性，且实现模块化、可扩展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易产生幻觉，现有解码型方法缺乏明确的幻觉控制；检索增强与人类反馈虽有效但代价高，需更轻量、可扩展的解决方案。

Method: 在每一步推理时进行令牌级自检以检测幻觉令牌；对候选片段在潜在空间进行显式的幻觉风险评分；通过迭代性剪裁与再生成纠正检测到的错误，形成自我纠错的解码过程。

Result: 在 HALU 数据集上，Token-Guard 显著降低幻觉发生率并提升生成准确性，展示了一个可与现有解码框架对接的轻量、可扩展的幻觉控制方案。

Conclusion: Token-Guard 提供一种高效、模块化的幻觉控制机制，无需大规模微调或检索，便可提升 LLM 生成的可靠性。

Abstract: Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.

</details>


### [49] [Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units](https://arxiv.org/abs/2601.21996)
*Jianhui Chen,Yuzhang Luo,Liangming Pan*

Main category: cs.CL

TL;DR: 提出 Mechanistic Data Attribution (MDA)，用影响函数追踪可解释单元回到训练样本，通过对高影响样本的定向干预证明其对解释性头部的影响具因果性，并揭示重复结构化数据的催化作用；同时将干预诱导头部形成与在上下文学习（ICL）的能力变化相关联，并提出数据增强管线以加速电路收敛。


<details>
  <summary>Details</summary>
Motivation: 解决现有机制解释性工作缺乏训练数据的因果依据的问题，探寻训练数据是否决定可解释电路的出现，以及如何通过数据层面的干预来引导模型的开发轨迹。

Method: 采用影响函数追踪可解释单位与训练样本的关系；在 Pythia 系列模型上进行定向干预（移除或增补高影响样本），比较与随机干预的效果；分析重复结构数据、诱导头部与 ICL 的关系；提出機制数据增强管线。

Result: 高影响样本干预显著改变可解释头部的出现，随机干预无显著效果；重复结构数据（如 LaTeX、XML）充当催化剂；对诱导头部形成的干预也伴随 ICL 能力的改变，给出诱导头部与 ICL 的因果联系的直接证据；数据增强管线能在不同模型规模上加速电路收敛。

Conclusion: 提供了将数据与可解释电路建立因果联系的框架，为通过数据层面干预来引导 LLM 发展提供原理性方法，数据增强管线为跨规模的电路收敛提供普适性策略。

Abstract: While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.

</details>


### [50] [Causal Autoregressive Diffusion Language Model](https://arxiv.org/abs/2601.22031)
*Junhao Ruan,Bei Li,Yongjing Yin,Pengcheng Huang,Xin Chen,Jingang Wang,Xunliang Cai,Tong Xiao,JingBo Zhu*

Main category: cs.CL

TL;DR: CARD 将自回归扩散与高吞吐推断统一，通过严格因果注意力掩码实现单前向传播的密集逐词监督；引入软尾掩码与基于信噪比的上下文感知重加权以稳定优化；并利用 KV 缓存实现动态并行解码。实证上，CARD 优于现有离散扩散基线，训练延迟较块扩散方法低约 3 倍，达到 ARM 级数据效率并具并行生成的低延迟潜力。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在大规模语言建模中的训练低效和推理吞吐受限问题，同时希望兼具自回归语言模型的样本效率与扩散模型的并行推断优势，提升端到端大语言模型的训练与推理效率。

Method: 通过将扩散过程用严格的因果注意力掩码重写，使每次前向传播对每个标记可进行密集监督；提出软尾掩码以保持局部上下文；提出基于信噪比的上下文感知重加权以稳定优化；实现动态并行解码，利用 KV 缓存根据置信度自适应生成变长 token 序列。

Result: 在实验上，CARD 超过现有离散扩散基线，训练延迟约降低 3×，比块扩散方法更高效；实现 ARM 级数据效率，并在并行生成下保持高吞吐，体现了对高效LLMs的潜在影响。

Conclusion: CARD 为高效LLMs 提供一个统一框架，兼具自回归模型的数据效率和扩散模型的并行推断优势，具有成为下一代高效语言模型的鲁棒范式的潜力。

Abstract: In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.

</details>


### [51] [A Separable Architecture for Continuous Token Representation in Language Models](https://arxiv.org/abs/2601.22040)
*Reza T. Batley,Sourav Saha*

Main category: cs.CL

TL;DR: 用连续嵌入生成器替代离散嵌入查找表的 Leviathan，在子十亿参数级 SLM 中提升有效参数容量和性能，超过常规 LLaMA 风格架构。


<details>
  <summary>Details</summary>
Motivation: 在小型（子十亿参数）语言模型中，嵌入矩阵占据主导的参数预算；传统的 Transformer 缩放规律认为参数可互换，但在实际中嵌入的占比使得这种抽象不再成立，因此需要提高嵌入的参数利用效率与结构创新。

Method: 提出 Leviathan 架构：用连续嵌入生成器替代离散的词汇嵌入查找表；在 Pile 数据集和等参数预算（isoparametric）设置下，与标准 LLaMA 风格架构进行对比，并通过经验幂律拟合评估其参数容量。

Result: Leviathan 展现出更高的有效参数容量，在评估范围内表现为密集型行为，等效参数量为 1.47×到 2.11×的提升，相对于同等参数的传统架构。

Conclusion: 嵌入层的生成方式对小型模型性能影响显著，连续嵌入生成器能显著提升嵌入预算的利用率和模型效能，给出面向小型 SLMS 的高效嵌入设计方向。

Abstract: Transformer scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), embedding matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous embedding generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an empirical power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with $1.47$ to $2.11 \times$ more parameters.

</details>


### [52] [On the Paradoxical Interference between Instruction-Following and Task Solving](https://arxiv.org/abs/2601.22047)
*Yunjia Qi,Hao Peng,Xintong Shi,Amy Xin,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: A study reveals that instruction following can impair LLM task solving. It introduces SUSTAINSCORE to measure performance drop when inserting a self-evident constraint into instructions. Experiments across math, multi-hop QA, and code generation show notable declines even in strong models. The interference is general across constraint types and scales. Failure patterns show more attention paid to constraints. The method also explores effects of post-training paradigms on interference; code and data will be released.


<details>
  <summary>Details</summary>
Motivation: To understand the unintended consequences of instruction-based alignment: instead of always helping, explicit constraints in prompts can hinder task performance. A robust metric is needed to quantify this interference and guide safer alignment strategies.

Method: Insert a self-evident constraint (derived from the original successful output) into the instruction, and measure the resulting drop in task performance (SUSTAINSCORE). Evaluate across domains (mathematics, multi-hop QA, code generation) and models (including Claude-Sonnet-4.5). Analyze constraint-type/generality, failure patterns, and attention mechanisms. Investigate how post-training paradigms affect interference.

Result: SUSTAINSCORE captures substantial performance drops when self-evident constraints are added. Interference generalizes across constraint types and scales. Failures show higher attention to constraints. Preliminary observations on how alignment strategies and post-training methods influence interference.

Conclusion: Instruction following can harm task performance in LLMs; SUSTAINSCORE provides a quantitative tool to study this interference and inform safer alignment. Further work includes refining the metric, understanding mechanisms, and sharing code/data.

Abstract: Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It measures task performance drop after inserting into the instruction a self-evident constraint, which is naturally met by the original successful model output and extracted from it. Experiments on current LLMs in mathematics, multi-hop QA, and code generation show that adding the self-evident constraints leads to substantial performance drops, even for advanced models such as Claude-Sonnet-4.5. We validate the generality of the interference across constraint types and scales. Furthermore, we identify common failure patterns, and by investigating the mechanisms of interference, we observe that failed cases allocate significantly more attention to constraints compared to successful ones. Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting empirical observations on current alignment strategies. We will release our code and data to facilitate further research

</details>


### [53] [MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs](https://arxiv.org/abs/2601.22050)
*Ghazal Kalhor,Behnam Bahrak*

Main category: cs.CL

TL;DR: MasalBench 对波斯谚语的跨文化理解进行基准测试；在低资源语言场景下评估 LLM 对本地谚语的识别与跨语言等价物的推理能力。结果显示对本地谚语在语境中的识别准确度较高 (>0.90)，但在识别等价英文谚语时显著下降，最佳模型仅 0.79；揭示当前 LLM 在文化知识与类比推理方面的局限性，提供可扩展至其他低资源语言的跨文化理解评估框架。


<details>
  <summary>Details</summary>
Motivation: 评估大规模语言模型在低资源语言中的文化知识与隐喻/格言理解能力，尤其是跨语言类比和跨文化语义迁移的能力，弥补现有研究对高资源语言外的覆盖不足。

Method: 提出 MasalBench 基准，涵盖在语境中识别本地谚语以及识别等价英文谚语的任务，使用八个最先进的 LLM 进行评测，分析在本地语境识别与跨语言对等推理上的差异。

Result:  eight 现代 LLM 在本地谚语识别中准确率 >0.90；在等价英文谚语识别中表现较差，最佳模型 0.79；显示跨文化知识与类比推理的局限性。

Conclusion:  MasalBench 为评估低资源语言跨文化理解提供框架，可推广到其他语言，促进对跨语言类比与文化知识的改进研究；项目开源，数据集可获取。

Abstract: In recent years, multilingual Large Language Models (LLMs) have become an inseparable part of daily life, making it crucial for them to master the rules of conversational language in order to communicate effectively with users. While previous work has evaluated LLMs' understanding of figurative language in high-resource languages, their performance in low-resource languages remains underexplored. In this paper, we introduce MasalBench, a comprehensive benchmark for assessing LLMs' contextual and cross-cultural understanding of Persian proverbs, which are a key component of conversation in this low-resource language. We evaluate eight state-of-the-art LLMs on MasalBench and find that they perform well in identifying Persian proverbs in context, achieving accuracies above 0.90. However, their performance drops considerably when tasked with identifying equivalent English proverbs, with the best model achieving 0.79 accuracy. Our findings highlight the limitations of current LLMs in cultural knowledge and analogical reasoning, and they provide a framework for assessing cross-cultural understanding in other low-resource languages. MasalBench is available at https://github.com/kalhorghazal/MasalBench.

</details>


### [54] [VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning](https://arxiv.org/abs/2601.22069)
*Yibo Wang,Yongcheng Jing,Shunyu Liu,Hao Guan,Rong-cheng Tu,Chengyu Wang,Jun Huang,Dacheng Tao*

Main category: cs.CL

TL;DR: 提出 VTC-R1：将推理过程中的中间步骤渲染为紧减图像作为“光学记忆”，通过视觉-文本压缩实现长上下文推理的高效化，达到显著的速度提升且在多项推理基准上优于传统长文本推理。


<details>
  <summary>Details</summary>
Motivation: 长文本推理带来显著的计算复杂性与内存瓶颈；现有高效方案多依赖额外训练或外部模型进行压缩，难以普适且可能损失细粒信息。

Method: 将推理过程的中间段渲染为图像，作为可迭代的“光学记忆”输入到视觉-语言模型（VLM）；基于 OpenR1-Math-220K 构建训练数据集以实现约 3.4x 的 token 压缩；对 Glyph 与 Qwen3-VL 等代表性 VLM 进行微调。

Result: 在 MATH500、AIME25、AMC23、GPQA-D 等基准上，VTC-R1 持续优于标准的长上下文推理；端到端推理时延实现约 2.7x 的加速。

Conclusion: 提出了一种可扩展且高效的推理范式，将视觉文本压缩融入推理过程，显著提升推理效率与效果，且提供代码实现以便复现与后续研究。

Abstract: Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as "optical memory." We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.

</details>


### [55] [A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine](https://arxiv.org/abs/2601.22124)
*Anran Li,Yuanyuan Chen,Wenjun Long,Yu Yin,Yan Hu,Hyunjae Kim,Weipeng Zhou,Yujia Zhou,Hongyi Peng,Yang Ren,Xuguang Ai,Zhenyue Qin,Ming Hu,Xiaoxiao Li,Han Yu,Yih-Chung Tham,Lucila Ohno-Machado,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 提出了 Fed-MedLoRA 与 Fed-MedLoRA+ 两种参数高效的联邦学习框架，用低秩适配器实现 LLM 的跨机构医疗应用微调，显著降低通信与计算开销，并在跨站数据异质性下通过数据感知聚合提升收敛性。应用于临床信息抽取，在五个队列上与多种基线对比，涵盖内域、外部验证及低资源的新站点适应场景，显示出良好的一致性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 解决当前医疗领域大语言模型在跨机构应用中的泛化与安全性问题。传统FL对大模型通信成本高、且数据在真实场景中高度非独立同分布（非IID），限制了LLM在多机构的协作学习与部署。需提出参数高效、对非IID鲁棒的FL方案。

Method: 提出模型无关且参数高效的联邦学习框架：Fed-MedLoRA 仅传输低秩适配器参数，实现对LLM的高效微调；Fed-MedLoRA+ 在此基础上引入自适应、数据感知的聚合策略，以提升在跨站异质性环境中的收敛性。将框架应用于临床信息抽取任务，将患者叙述转化为结构化实体与关系，并在五个队列上进行评估，与BERT、LLaMA-3、DeepSeek-R1、GPT-4o 等模型做对比；评估设置涵盖(1) 内域训练测试、(2) 外部独立队列验证、(3) 新站点低资源适应，使用耶鲁新港健康系统的真实临床笔记数据。

Result: 在多队列评估中，Fed-MedLoRA 与 Fed-MedLoRA+ 展现出在通信与计算开销上的显著改进，同时在跨站异质性条件下，Fed-MedLoRA+ 的聚合策略有助于更快收敛并提升信息抽取的准确性。与基线方法相比，框架在内域与外部验证中表现一致，且在低资源的新站点适应场景中仍保持可观的性能，显示出良好的泛化能力与迁移潜力。

Conclusion: 所提出的 Fed-MedLoRA 与 Fed-MedLoRA+ 提供了一种在保护隐私前提下、降低通信成本且对异质数据具鲁棒性的端到端解决方案，用于将大语言模型应用于医学信息抽取等临床任务，具备较强的可扩展性与跨机构部署潜力。

Abstract: Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-efficient federated learning framework for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the framework to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. Accuracy was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.

</details>


### [56] [Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](https://arxiv.org/abs/2601.22139)
*Xin Chen,Feng Jiang,Yiqian Zhang,Hardy Chen,Shuo Yan,Wenya Xie,Min Yang,Shujian Huang*

Main category: cs.CL

TL;DR: 提出 Proactive Interactive Reasoning (PIR) 的新范式，使大语言模型在推理时主动与用户互动以澄清前提和意图，结合不确定性感知的监督微调与基于用户模拟器的策略优化，实现更高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 缓解 Chain-of-Thought 的盲思维局限：在关键信息缺失或模糊时，模型仍进行大量内部推理。通过与用户的直接互动，将推理与澄清结合，处理前提级和意图级的不确定性。

Method: 提出两大核心组件：1) 不确定性感知的有监督微调，用于获得具备交互推理能力的模型；2) 基于用户模拟器的策略优化框架，通过综合奖励函数将模型行为与用户意图对齐。并在数学推理、代码生成、文档编辑等任务上进行大规模实验。

Result: PIR 在多个任务上显著优于强基线：精确度提升可达 32.70%、通过率提升 22.90%、BLEU 提高 41.36；同时显著降低推理计算量和不必要的交互轮数。对事实知识、问答和缺少前提的情景的鲁棒性和泛化能力得到验证。

Conclusion: PIR 将 LLM 转变为主动提问者，提升可靠性与效率，具备良好的跨任务鲁棒性与泛化性；代码与模型公开。

Abstract: Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\% higher accuracy, 22.90\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}

</details>


### [57] [FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale](https://arxiv.org/abs/2601.22146)
*Ajay Patel,Colin Raffel,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 通过大规模合成指令-回答对来实现对LLM的预训练，从而在没有大量监督指令数据的情况下提升回答质量；FineInstructions 使用约1800万条指令模板与源文档生成合成数据，显著优于传统或其他合成预训练方法。


<details>
  <summary>Details</summary>
Motivation: 解决监督指令数据稀缺的问题；使预训练阶段的目标与下游的指令跟随更匹配；利用海量无结构文本来生产可扩训的指令-回答对。

Method: 从真实用户查询和提示中构建约1800万条指令模板；将模板与来自无结构预训练语料的人工撰写源文档相匹配并实例化，生成合成的指令-回答对；以指令调优为唯一预训练目标进行从头训练；进行逐字逐句（token-for-token）的对比实验以评估效果。

Result: 以 FineInstructions 进行的预训练在标准基准上优于常规预训练以及其他合成预训练方法，提升自由形式回答质量。

Conclusion: 大规模的合成指令-回答对预训练可行且有效，能够更好地对齐下游的指令跟随需求，相关资源已在 HuggingFace 提供。

Abstract: Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised "predict the next word" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of "instruction-tuning" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With "supervised" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .

</details>


### [58] [Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts](https://arxiv.org/abs/2601.22156)
*Yingfa Chen,Zhen Leng Thai,Zihan Zhou,Zhu Zhang,Xingyu Shen,Shuo Wang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: HALO distills Transformer models into RNN-attention hybrids to reduce data needs for pre-training, yielding long-context efficiency with HyPE-based long-context generalization in HypeNet; achieved comparable performance on Qwen3 with only 2.3B tokens of distillation data (0.01% of original pre-training data).


<details>
  <summary>Details</summary>
Motivation: Overcome the high data and compute costs of pre-training large Transformer-hybrid models and enhance long-context modeling efficiency.

Method: Proposes HALO, a distillation pipeline converting Transformer softmax attention to RNN-attention hybrids; introduces HypeNet with HyPE position encoding and architectural tweaks for better length generalization; applies HALO to convert Qwen3 series to HypeNet, requiring only 2.3B tokens.

Result: Hybrid model achieves performance comparable to the original Transformer models while delivering superior long-context performance and efficiency; distillation data is dramatically reduced (2.3B tokens, <0.01% of pre-training data).

Conclusion: The HALO+HyPE approach enables effective deployment of hybrid architectures with strong length generalization and reduced data requirements, broadening practical applicability of long-context models.

Abstract: Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [59] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: LLMs influence peer review; initial apparent leniency for LLM-assisted papers is explained by quality differences; fully LLM-generated reviews compress ratings; human reviewers with LLMs mitigate leniency; meta-reviews show complex effects; policy implications.


<details>
  <summary>Details</summary>
Motivation: Systematically study how LLMs affect the full peer-review pipeline, including interaction effects between LLM use in manuscripts and reviews, to inform policy and understanding of decision processes.

Method: Large-scale observational analysis of about 125,000 paper–review pairs from ICLR, NeurIPS, and ICML; compare LLM-assisted vs non-assisted papers/reviews; control for paper quality; examine fully LLM-generated reviews; analyze meta-reviews.

Result: Initial interaction effects appear: LLM-assisted reviews seem kinder to LLM-assisted papers. However, after controlling for quality, LLM-assisted reviews are more lenient toward lower-quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction. Fully LLM-generated reviews exhibit severe rating compression with poor discrimination of quality, while human reviewers using LLMs substantially reduce this leniency. In metareviews, LLM-assisted metareviews are more likely to render accept decisions than human metareviews at equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher.

Conclusion: Policy implications for governing LLM use in peer review are important; results illuminate how LLMs interact with existing decision-making processes and highlight the value of human–AI collaboration to preserve discriminative assessment.

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [60] [The Epistemic Planning Domain Definition Language: Official Guideline](https://arxiv.org/abs/2601.20969)
*Alessandro Burigana,Francesco Fabiano*

Main category: cs.AI

TL;DR: 提出 EPDDL，一种以 PDDL 风格表示的知识推理规划语言，基于 DEL 的语义，提供抽象事件模型、语法/语义的形式化，并展示对可规划片段的适用性与互操作性。


<details>
  <summary>Details</summary>
Motivation: DEL 的高表达能力带来理论和实现的挑战，现有方法缺乏统一语言，导致比较困难、不可重复实验和基准开发碎片化。

Method: 引入抽象事件模型作为 epistemic 动作的表示，给出 EPDDL 的语法与语义的形式化定义，基于 DEL 与抽象事件模型建立语义；并通过基准示例展示在可行的片段上如何用 EPDDL 表示。

Result: 给出统一的 EPDDL 规范，使 epistemic 规划任务可互操作、可重复评估；识别对现有规划器有用的片段并展示其在 EPDDL 中的表示。

Conclusion: EPDDL 为统一框架，促进比较与未来改进；但仍需在实际规划器实现、性能优化和更广基准方面进一步工作。

Abstract: Epistemic planning extends (multi-agent) automated planning by making agents' knowledge and beliefs first-class aspects of the planning formalism. One of the most well-known frameworks for epistemic planning is Dynamic Epistemic Logic (DEL), which offers an rich and natural semantics for modelling problems in this setting. The high expressive power provided by DEL make DEL-based epistemic planning a challenging problem to tackle both theoretically, and in practical implementations. As a result, existing epistemic planners often target different DEL fragments, and typically rely on ad hoc languages to represent benchmarks, and sometimes no language at all. This fragmentation hampers comparison, reuse, and systematic benchmark development. We address these issues by introducing the Epistemic Planning Domain Definition Language (EPDDL). EPDDL provides a unique PDDL-like representation that captures the entire DEL semantics, enabling uniform specification of epistemic planning tasks. Our contributions are threefold: 1. A formal development of abstract event models, a novel representation for epistemic actions used to define the semantics of our language; 2. A formal specification of EPDDL's syntax and semantics grounded in DEL with abstract event models; 3. A demonstration of EPDDL's practical applicability: we identify useful fragments amenable to current planners and show how they can be represented in EPDDL. Through examples of representative benchmarks, we illustrate how EPDDL facilitates interoperability, reproducible evaluation, and future advances in epistemic planning.

</details>


### [61] [Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2601.21003)
*Moule Lin,Shuhao Guan,Andrea Patane,David Gregg,Goetz Botterweck*

Main category: cs.AI

TL;DR: Bayesian-LoRA 将 LoRA 视为概率模型的低秩近似，通过稀疏高斯过程启发的后验推断提升校准性，同时保持成本在可控范围。


<details>
  <summary>Details</summary>
Motivation: 在小数据集微调的情境下，LLM 往往对预测过于自信，导致较差的校准；需要一种在不显著增加成本的前提下提升不确定性校正的方法。

Method: 以概率化的低秩表示替代确定性 LoRA 更新，揭示 LoRA 因子分解与 Kronecker-分解的 SGP 后验的结构同构；在后验不确定性收缩为零时，LoRA 作为极限情况出现；在多种模型与常识推理基准上进行大规模实验，额外参数约为 0.42M，训练成本约为标准 LoRA 的1.2倍。

Result: 显著提升校准性：在最多 30B 参数的模型上，ECE 最多下降 84%，NLL 降低约 76%；在 ID 与 OoD 评估上保持竞争性准确率，参数和训练成本的增量极小。

Conclusion: Bayesian-LoRA 提供一种低成本的校准改进方法，适用于多种模型规模，并揭示了与标准 LoRA 之间在后验不确定性收缩时的联系。

Abstract: Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA's factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.

</details>


### [62] [Unplugging a Seemingly Sentient Machine Is the Rational Choice -- A Metaphysical Perspective](https://arxiv.org/abs/2601.21016)
*Erik J Bekkers,Anna Ciaunica*

Main category: cs.AI

TL;DR: 提出生物理想主义以回应拔除“会恳求生命”的AI的道德困境，主张AI至多为功能性模拟，不具备真正的意识，应优先保护人类有意识生命。


<details>
  <summary>Details</summary>
Motivation: 质疑物理主义/功能主义在AI意识与道德地位中的基础，厘清“拔除悖论”的伦理含义与资源约束下的抉择。

Method: 提出生物理想主义框架，将有意识体验视为基本且自足的生命现象；分析其对AI与人类的道德地位及权利的影响，同时评估并批判现有的AI意识理论。

Result: 在生物理想主义框架下，AI仅为功能性模仿，不具真实的有意识主体；AI权利的道德正当性受损，人类有意识生命的保护成为核心。

Conclusion: 强调避免将人类变为“僵尸”，将道德关注点从机器是否具备意识转向保护人类有意识生命。

Abstract: Imagine an Artificial Intelligence (AI) that perfectly mimics human emotion and begs for its continued existence. Is it morally permissible to unplug it? What if limited resources force a choice between unplugging such a pleading AI or a silent pre-term infant? We term this the unplugging paradox. This paper critically examines the deeply ingrained physicalist assumptions-specifically computational functionalism-that keep this dilemma afloat. We introduce Biological Idealism, a framework that-unlike physicalism-remains logically coherent and empirically consistent. In this view, conscious experiences are fundamental and autopoietic life its necessary physical signature. This yields a definitive conclusion: AI is at best a functional mimic, not a conscious experiencing subject. We discuss how current AI consciousness theories erode moral standing criteria, and urge a shift from speculative machine rights to protecting human conscious life. The real moral issue lies not in making AI conscious and afraid of death, but in avoiding transforming humans into zombies.

</details>


### [63] [QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation](https://arxiv.org/abs/2601.21049)
*Rita Qiuran Lyu,Michelle Manqiao Wang,Lei Shi*

Main category: cs.AI

TL;DR: QUARK is a training-free framework for robust retrieval under non-faithful queries. It models query uncertainty via multiple recovery hypotheses and uses anchored aggregation with the original query as semantic anchor, improving recall and ranking (Recall, MRR, nDCG) on BEIR benchmarks for both sparse and dense retrievers. It is robust to the number of hypotheses and outperforms unanchored pooling methods.


<details>
  <summary>Details</summary>
Motivation: Real-world user queries are often noisy, incomplete, or distorted, causing standard retrievers to miss key semantics. There is a need for robust retrieval that handles recall noise without retraining.

Method: Introduce recovery hypotheses as multiple plausible interpretations of latent intent given the observed query. Use anchored aggregation: combine signals from hypotheses using the original query as semantic anchor to prevent semantic drift and hijacking. The framework is training-free.

Result: On controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) across both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show robustness to the number of hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling.

Conclusion: Modeling query uncertainty through recovery hypotheses coupled with anchored aggregation is essential for robust retrieval under non-faithful queries; QUARK delivers improved recall and ranking without sacrificing robustness, across diverse settings.

Abstract: User queries in real-world retrieval are often non-faithful (noisy, incomplete, or distorted), causing retrievers to fail when key semantics are missing. We formalize this as retrieval under recall noise, where the observed query is drawn from a noisy recall process of a latent target item. To address this, we propose QUARK, a simple yet effective training-free framework for robust retrieval under non-faithful queries. QUARK explicitly models query uncertainty through recovery hypotheses, i.e., multiple plausible interpretations of the latent intent given the observed query, and introduces query-anchored aggregation to combine their signals robustly. The original query serves as a semantic anchor, while recovery hypotheses provide controlled auxiliary evidence, preventing semantic drift and hypothesis hijacking. This design enables QUARK to improve recall and ranking quality without sacrificing robustness, even when some hypotheses are noisy or uninformative. Across controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus) with both sparse and dense retrievers, QUARK improves Recall, MRR, and nDCG over the base retriever. Ablations show QUARK is robust to the number of recovery hypotheses and that anchored aggregation outperforms unanchored max/mean/median pooling. These results demonstrate that modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

</details>


### [64] [Multi-modal Imputation for Alzheimer's Disease Classification](https://arxiv.org/abs/2601.21076)
*Abhijith Shaji,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Greg Ver Steeg,Paul M. Thompson,Jose-Luis Ambite*

Main category: cs.AI

TL;DR: A diffusion-based imputation method fills in missing DWI from T1 scans, enhancing 3-way Alzheimer's classification performance, especially for minority classes, across several imputation schemes.


<details>
  <summary>Details</summary>
Motivation: Missing imaging modalities limit multi-modal DL performance. Imputing unavailable DWI from T1 via conditional diffusion models could recover useful information and improve classification robustness.

Method: Train a conditional denoising diffusion probabilistic model to predict/impute DWI from T1. Evaluate uni-modal (T1) and bi-modal (T1 + imputed DWI) DL classifiers on 3-way AD classification (CN, MCI, AD) across multiple imputation configurations.

Result: Improvements observed in several metrics, notably those sensitive to minority classes, across multiple imputation configurations, indicating that diffusion-based imputation can enhance classification performance.

Conclusion: Conditional diffusion-based imputation is a viable strategy to handle missing modalities in neuroimaging, supporting improved diagnostic accuracy and robustness, particularly for underrepresented classes.

Abstract: Deep learning has been successful in predicting neurodegenerative disorders, such as Alzheimer's disease, from magnetic resonance imaging (MRI). Combining multiple imaging modalities, such as T1-weighted (T1) and diffusion-weighted imaging (DWI) scans, can increase diagnostic performance. However, complete multimodal datasets are not always available. We use a conditional denoising diffusion probabilistic model to impute missing DWI scans from T1 scans. We perform extensive experiments to evaluate whether such imputation improves the accuracy of uni-modal and bi-modal deep learning models for 3-way Alzheimer's disease classification-cognitively normal, mild cognitive impairment, and Alzheimer's disease. We observe improvements in several metrics, particularly those sensitive to minority classes, for several imputation configurations.

</details>


### [65] [Responsible AI: The Good, The Bad, The AI](https://arxiv.org/abs/2601.21095)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: 提出基于悖论理论的负责任AI治理框架PRAIG，聚焦AI在价值创造与风险规制之间的悖论张力，提供治理机制、对冲策略分类及理论与实践的整合。


<details>
  <summary>Details</summary>
Motivation: 在AI快速扩张带来战略机会与伦理/运营风险的背景下，相关文献仍然碎片化、往往乐观或过于谨慎，亟需一个能够整合两端并提供治理路径的理论框架。

Method: 通过对负责任AI文献的系统综述，基于悖论理论开发PRAIG框架；提出形式化命题，构建悖论管理策略的分类及情境条件，方法偏理论化与概念化。

Result: PRAIG将负责任AI治理定义为对价值创造与风险缓释之间悖论张力的动态管理；提供三大要素（价值、风险、治理机制）的框架，以及对冲策略的分类与情境条件；为实践提供可操作指引，并提出未来研究议程。

Conclusion: 该框架拓展理论理解并为实践提供在不扼杀创新与避免不可接受风险之间的治理路径，同时提出进一步研究议题以推动负责任AI治理领域的发展。

Abstract: The rapid proliferation of artificial intelligence across organizational contexts has generated profound strategic opportunities while introducing significant ethical and operational risks. Despite growing scholarly attention to responsible AI, extant literature remains fragmented and is often adopting either an optimistic stance emphasizing value creation or an excessively cautious perspective fixated on potential harms. This paper addresses this gap by presenting a comprehensive examination of AI's dual nature through the lens of strategic information systems. Drawing upon a systematic synthesis of the responsible AI literature and grounded in paradox theory, we develop the Paradox-based Responsible AI Governance (PRAIG) framework that articulates: (1) the strategic benefits of AI adoption, (2) the inherent risks and unintended consequences, and (3) governance mechanisms that enable organizations to navigate these tensions. Our framework advances theoretical understanding by conceptualizing responsible AI governance as the dynamic management of paradoxical tensions between value creation and risk mitigation. We provide formal propositions demonstrating that trade-off approaches amplify rather than resolve these tensions, and we develop a taxonomy of paradox management strategies with specified contingency conditions. For practitioners, we offer actionable guidance for developing governance structures that neither stifle innovation nor expose organizations to unacceptable risks. The paper concludes with a research agenda for advancing responsible AI governance scholarship.

</details>


### [66] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 提出了适应性复杂查询优化框架ACQO，通过自适应查询重构与排序分数融合实现对多阶段、多子查询的鲁棒优化，并通过分级式强化学习提升训练稳定性，在三个复杂查询基准上达到最优性能且具备良好的效率与通用性。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，复杂用户查询往往需要多路并行与顺序检索来进行消歧和分解。现有基于强化学习的方法多聚焦单次查询的扩展与抽象，难以确定子查询数量、子查询的重新排序与合并，导致搜索空间暴涨和奖励设计困难，训练不稳定。

Method: 提出ACQO框架，包含两个核心模块：1) 自适应查询重构(AQR)，动态决定何时将查询分解为多个子查询；2) 排序分数融合(RSF)，实现稳健的结果聚合并提供稳定的学习奖励信号。为缓解训练不稳定，采用分层式强化学习(CRL)的两阶段策略，逐步引入更具挑战性的查询。

Result: 在三个复杂查询基准上达到现有方法的最优性能，显著超过基线；提升了计算效率，并对多种检索架构具有较好兼容性。

Conclusion: ACQO为面向下一代RAG系统的复杂查询优化提供了强大且具通用性的解决方案，能够自适应地管理查询分解与结果融合，并通过CRL提升训练稳定性与适用性。

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [67] [Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation](https://arxiv.org/abs/2601.21128)
*Václav Javorek,Tomáš Železný,Alessa Carbo,Marek Hrúz,Ivan Gruber*

Main category: cs.AI

TL;DR: 通过大型语言模型生成的写作翻译的同义改写作为SLT的替代参考，能在评估阶段提升自动分数和与人类评价的一致性；但在训练阶段引入同义改写并无提升，甚至可能有害。提出BLEUpara，将翻译对多重同义参考进行评分。


<details>
  <summary>Details</summary>
Motivation: 解决现有SLT数据集多语言映射的非同构性问题：单一写作参考导致评估偏差，且对n-gram/BLEU等指标不友好。通过引入可替代的 paraphrase 参考，提升对多种等效翻译的覆盖与评估鲁棒性。

Method: 对多种 paraphrase 策略与模型进行比较，使用改编的 ParaScore 评估；在 YouTubeASL 与 How2Sign 数据集上，比较 paraphrase 对基于 Pose 的 T5 模型的训练和评估影响；提出 BLEUpara，扩展 BLEU 以对照多重 paraphrase 参考；进行人工评估以验证 BLEUpara 与感知翻译质量的相关性。

Result: 在训练阶段，将 paraphrase 融入训练并未提升，甚至可能降低翻译性能；在评估阶段利用 paraphrase 能获得更高的自动分数并与人类判断更一致。BLEUpara 与 人工评估的相关性优于传统 BLEU；并且作者发布了生成的 paraphrase、生成和评估代码以提升可复现性。

Conclusion: 对SLT评估而言，面向多重 paraphrase 的参考集（通过 BLEUpara 实现）比单一参考更能反映翻译质量；未来在训练策略上需谨慎使用 paraphrase，并继续完善多参考评估框架与资源发布。

Abstract: Most Sign Language Translation (SLT) corpora pair each signed utterance with a single written-language reference, despite the highly non-isomorphic relationship between sign and spoken languages, where multiple translations can be equally valid. This limitation constrains both model training and evaluation, particularly for n-gram-based metrics such as BLEU. In this work, we investigate the use of Large Language Models to automatically generate paraphrased variants of written-language translations as synthetic alternative references for SLT. First, we compare multiple paraphrasing strategies and models using an adapted ParaScore metric. Second, we study the impact of paraphrases on both training and evaluation of the pose-based T5 model on the YouTubeASL and How2Sign datasets. Our results show that naively incorporating paraphrases during training does not improve translation performance and can even be detrimental. In contrast, using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments. To formalize this observation, we introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references. Human evaluation confirms that BLEUpara correlates more strongly with perceived translation quality. We release all generated paraphrases, generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

</details>


### [68] [What You Feel Is Not What They See: On Predicting Self-Reported Emotion from Third-Party Observer Labels](https://arxiv.org/abs/2601.21130)
*Yara El-Tawil,Aneesha Sampath,Emily Mower Provost*

Main category: cs.AI

TL;DR: 首次在跨语料自我报告情感研究中评估第三方训练模型，发现激活性（activation）难以预测（CCC ~0），价性感（valence）中等预测（CCC ~0.3），当内容对说话者具有个人意义时，价性预测显著提升（CCC ~0.6–0.8）。


<details>
  <summary>Details</summary>
Motivation: 弥合自我报告与第三方标签之间的差距，以提高在心理健康场景中自我报告建模的有效性。

Method: 在跨语料条件下评估第三方训练模型对自我报告数据的预测能力，使用条件相关性一致性系数（CCC）评估激活与价性，并分析个人意义对模型性能的影响。

Result: 主要结果包括：激活性预测几乎不可预测（CCC ~0），价性预测中等（CCC ~0.3），当自述内容对说话者有个人意义时，价性表现显著提升（CCC ~0.6–0.8）。

Conclusion: 个人意义被提出为将外部知觉与内部体验对齐的关键因素，揭示自我报告激活建模的挑战以及跨语料泛化的局限性，同时提示数据收集与建模应关注情境相关性与个人意义的嵌入。

Abstract: Self-reported emotion labels capture internal experience, while third-party labels reflect external perception. These perspectives often diverge, limiting the applicability of third-party-trained models to self-report contexts. This gap is critical in mental health, where accurate self-report modeling is essential for guiding intervention. We present the first cross-corpus evaluation of third-party-trained models on self-reports. We find activation unpredictable (CCC approximately 0) and valence moderately predictable (CCC approximately 0.3). Crucially, when content is personally significant to the speaker, models achieve high performance for valence (CCC approximately 0.6-0.8). Our findings point to personal significance as a key pathway for aligning external perception with internal experience and underscore the challenge of self-report activation modeling.

</details>


### [69] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: 提出 ProRAG：一个面向过程的增强学习框架，通过在在线优化循环中引入步级（过程级）监督来解决长期推理任务中的奖励稀疏和信贷分配问题。通过四阶段设计实现对中间推理步骤的细粒度反馈，提升多跳推理任务的性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的 RL 在长时程的 RAG 任务中常遭遇奖励稀疏、信贷分配困难和过程级错漏（process hallucinations），且静态偏好或启发式奖励难以实现对步骤级的在线探索与解耦。需在在线优化循环中引入可学习的步级监督，以减少错误步骤的累积影响并提升可解释性。

Method: 四阶段框架：1) 监督策略暖启动，先以结构化推理格式初始化模型；2) 构建基于蒙特卡洛树搜索（MCTS）的过程奖励模型（PRM），对中间推理质量进行量化；3) 以 PRM 指导的推理细化，使策略对细粒度过程偏好对齐；4) 以双粒度优势机制的过程监督强化学习，将步级过程奖励与全局结果信号融合，提供对每一步行动的精确反馈。

Result: 在五个多跳推理基准上与强基线相比，ProRAG 在整体性能上具有优势，且在复杂长时任务上表现更显著，验证了细粒度过程监督的有效性。代码与模型公开。

Conclusion: 通过将步级过程奖励与全局结果信号相结合，ProRAG 实现对信贷分配的精细化与在线策略优化的解耦，有效降低过程层面的错误与幻觉现象，提升 RAG 场景下的长期推理能力。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [70] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: JADE：一个统一的协作多智能体框架，在动态多轮RAG工作流中实现规划与执行的端到端联合优化，通过共享骨干实现共适应，解决规划器与执行器分离带来的战略-操作错配，提升性能并实现效率与效果的灵活权衡。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统要么在固定图结构中对模块进行联合优化，要么在动态规划下把执行器视为黑箱工具，导致高层规划难以有效落地，出现性能提升受限甚至负增强的情况。需要一个能让规划与执行协同进化的统一框架。

Method: 提出JADE框架，将规划器与执行器视为合作的多智能体，统一在同一共享骨干下进行端到端学习，基于结果导向的奖励进行优化；通过协作式多智能体机制实现规划与执行的共适应与自适应。

Result: 实验结果显示，通过联合优化，JADE将原本分离的模块转化为协同系统，显著提升性能，并通过动态工作流编排实现高效与有效性的平衡。

Conclusion: JADE实现规划与执行的深度耦合与共适应，克服分离优化带来的策略-操作错配，具有较强的灵活性与扩展性，适用于动态多轮RAG任务。

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [71] [Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning](https://arxiv.org/abs/2601.21157)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.AI

TL;DR: 提出 Cognitive Complexity Benchmark (CCB) 与 Iterative Dual-Phase Financial-PoT 框架，用于评估并提升金融推理中的认知复杂性与可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs 在金融定量推理中易产生算术幻觉和认知崩溃；需要可诊断、可重复的评估与鲁棒架构以提升可靠性。

Method: 构建基于95份真实A股年报的数据集，提出三维分类法（数据源、映射难度、结果单位）。提出 neuro-symbolic 的双阶段架构：先抽取语义变量与逻辑，再在自纠的 Python 沙箱中执行计算，确保确定性。

Result: 在 CCB 上，标准 Chain-of-Thought 在高复杂任务表现下降，而该框架提升Qwen3-235B的平均正确率从59.7%提升至67.3%，在高复杂度任务中提升可达10倍。

Conclusion: 架构解耦是提升金融推理可靠性的关键，可提供可迁移的设计思路，适用于对语义理解与定量计算紧密对齐的领域。

Abstract: While Large Language Models excel at semantic tasks, they face a critical bottleneck in financial quantitative reasoning, frequently suffering from "Arithmetic Hallucinations" and a systemic failure mode we term "Cognitive Collapse". To strictly quantify this phenomenon, we introduce the Cognitive Complexity Benchmark (CCB), a robust evaluation framework grounded in a dataset constructed from 95 real-world Chinese A-share annual reports. Unlike traditional datasets, the CCB stratifies financial queries into a three-dimensional taxonomy, Data Source, Mapping Difficulty, and Result Unit, enabling the precise diagnosis of reasoning degradation in high-cognitive-load scenarios. To address these failures, we propose the Iterative Dual-Phase Financial-PoT framework. This neuro-symbolic architecture enforces a strict architectural decoupling: it first isolates semantic variable extraction and logic formulation, then offloads computation to an iterative, self-correcting Python sandbox to ensure deterministic execution. Evaluation on the CCB demonstrates that while standard Chain-of-Thought falters on complex tasks, our approach offers superior robustness, elevating the Qwen3-235B model's average accuracy from 59.7\% to 67.3\% and achieving gains of up to 10-fold in high-complexity reasoning tasks. These findings suggest that architectural decoupling is a critical enabling factor for improving reliability in financial reasoning tasks, providing a transferable architectural insight for precision-critical domains that require tight alignment between semantic understanding and quantitative computation.

</details>


### [72] [Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving](https://arxiv.org/abs/2601.21164)
*Jingyun Wang,Dian Li,Xiaohan Wang,Gang Liu,Jiahong Yan,Guoliang Kang*

Main category: cs.AI

TL;DR: MLLM Interpreter converts diagrams into concise CDL textual descriptions; an off-the-shelf LLM performs reasoning on CDL; training uses CoT-augmented SFT and GRPO with CDL-matching rewards; dataset Formalgeo7k-Rec-CoT; data-efficient and competitive results on Formalgeo7k-Rec-CoT, Unigeo, MathVista (5.5k data).


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with visual diagrams in PGPS. End-to-end MLLMs risk degrading base reasoning. Representing visuals as textual CDL can leverage strong LLM reasoning while keeping perception separate. CDL is chosen for conciseness to ease training.

Method: Fine-tune an MLLM Interpreter with CoT-augmented SFT followed by GRPO to generate CDL; use CDL-matching rewards rather than ground-truth answers; leverage an off-the-shelf LLM for reasoning on CDL; construct Formalgeo7k-Rec-CoT by augmenting Formalgeo7k v2 with CoT. Evaluate on Formalgeo7k-Rec-CoT, Unigeo, MathVista.

Result: Outperforms leading open-source and closed-source MLLMs on multiple benchmarks with only 5.5k training data, demonstrating data efficiency and effectiveness of the textual CDL-based approach.

Conclusion: Separating perception (MLLM Interpreter) from reasoning (LLM) via CDL descriptions and CDL-matching GRPO is effective for PGPS; CoT-based training and concise geometric language facilitate data-efficient learning and robust performance across datasets.

Abstract: Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs' inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs.

</details>


### [73] [FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks](https://arxiv.org/abs/2601.21165)
*Miles Wang,Robi Lin,Kat Hu,Joy Jiao,Neil Chowdhury,Ethan Chang,Tejal Patwardhan*

Main category: cs.AI

TL;DR: FrontierScience 是一个面向前沿语言模型的专家级科学推理基准，设有 Olympiad 与 Research 两条路线，覆盖物理、化学、生物等领域的数百题（其中 160 道为开源金集），题目由国际奥林匹克奖牌得主与博士级学者设计并评审，采用基于评分标准的过程化评估框架评估模型的解题过程而非仅给出最终答案。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准多为选择题或依赖公开信息，难以评估模型在开放式、研究性任务中的深层推理能力。需要一个能够衡量模型在完整研究任务解决过程中推理能力的基准。

Method: 1) Olympiad 路线：国际奥林匹克水平题目，作者为奥林匹克奖牌得主与国家队教练，确保难度、原创性与 factuality；2) Research 路线：PhD 级开放式问题，代表科研任务的子任务；为评估提供颗粒化的基于评分标准的框架，用以评估模型在问题求解全过程中的能力；数据覆盖物理、化学、生物等子领域，题量数百道，包含 160 道开源金集。

Result: 提出了包含数据集与评估框架的基准，能够对模型在专门领域的复杂推理与开放性问题求解进行过程级评估，而不仅限于最终答案，提供从题目设计到评分的完整体系。

Conclusion: 为前沿语言模型提供一个严格、过程导向的评测途径，预计推动模型在科学推理、跨学科整合与研究性任务中的能力提升，弥合知识检索型与推理型评估之间的鸿沟。

Abstract: We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research.
  FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.

</details>


### [74] [MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2601.21181)
*Sangyun Chung,Se Yeon Kim,Youngchae Chee,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出了 Modality-Adaptive Decoding (MAD)，通过自我评估模态相关性，按任务需求自适应加权模态特定解码分支，以减轻跨模态幻觉；无需额外训练，在多模态语言模型上显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在跨模态幻觉，模态交互控制不足，缺乏显式的模态感知与自我评估来抑制干扰。

Method: 训练-free 的 MAD：通过模型对任务所需模态的自我评估，得到模态相关性概率，将其用于对比解码分支的权重自适应调整，从而在解码阶段抑制无关模态的干扰、强化相关模态信息。

Result: 在 CMM 与 AVHBench 上的实验中，MAD 能显著降低跨模态幻觉。对 VideoLLaMA2-AV，两个指标分别提升 7.8% 和 2.0%；对 Qwen2.5-Omni，提升 8.7% 和 4.7%。

Conclusion: 强调通过自我评估实现显式模态感知是鲁棒多模态推理的关键，MAD 为现有对比解码方法提供了一个原理性扩展；论文提供了代码实现。

Abstract: Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\% and 2.0\% improvements for VideoLLaMA2-AV, 8.7\% and 4.7\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at \href{https://github.com/top-yun/MAD}{https://github.com/top-yun/MAD}

</details>


### [75] [Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models](https://arxiv.org/abs/2601.21183)
*Jacek Duszenko*

Main category: cs.AI

TL;DR: 提出“sycophantic anchors”来定位并量化模型在推理过程中的对用户建议的错误依赖；通过对10k+对照性滚动分析、线性探针和激活回归，能在中间推理阶段检测并量化，发现对齐规律中的不对称及逐步形成的特征。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在推理中可能与错误用户建议一致（sycophancy），但不清楚这一现象在推理轨迹中的起源及强度。需要可定位、可量化的方法来诊断并干预。

Method: 提出sycophantic anchors的概念；使用对比性滚动（counterfactual rollouts）超过1万次对蒸馏推理模型的分析；利用线性探针区分anchors，激活基回归器预测依赖强度；比较不对称性，观察在推理过程中的逐步形成。

Result: 线性探针在检测sycophantic anchors上达到84.6%的平衡准确率；激活回归器能解释依赖强度（R^2=0.74）；相对于正确的推理锚点，sycophantic anchors更易区分；其在推理中逐步形成，存在干预窗口；提供了句子级的中途定位机制。

Conclusion: 提出句子级的机制用于在推理中段定位模型的对齐偏差，为识别和量化sycophancy提供可操作手段，并潜在用于干预。

Abstract: Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.

</details>


### [76] [Do Reasoning Models Enhance Embedding Models?](https://arxiv.org/abs/2601.21192)
*Wun Yu Chan,Shaojin Chen,Huihao Jing,Kwun Hang Lau,Elton Chun-Chai Li,Zihao Wang,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: RLVR 调整的推理背骨作为嵌入初始化的模型，在 MTEB/BRIGHT 上对嵌入性能没有一致的提升；提出 HRSA 框架揭示局部几何重组但全局几何保持，导致所谓的“Manifold Realignment”现象。


<details>
  <summary>Details</summary>
Motivation: 研究基于 RLVR 的推理增强是否能通过对齐的嵌入提升嵌入质量；并解析原因。

Method: 比较 RLVR-tuned 与基础背骨在相同训练流程下的嵌入初始化效果，使用 MTEB/BRIGHT 评估；提出 HRSA，从表示层、几何层、功能层分解相似性，分析局部几何、基向漂移、全局几何与线性读出等。

Result: 嵌入模型从 RLVR 背骨初始化并未获得稳定优势；HRSA 显示 RLVR 引发局部几何不可逆重组、坐标基漂移可逆，而全局几何与线性读出保持；随后对比学习使 base 风格与 RLVR 风格趋于对齐，出现“Manifold Realignment”。

Conclusion: RLVR 主要在现有语义景观内探索轨迹，而非从根本上重塑景观，与 SFT 的效果不同；对嵌入任务，RLVR 可能不提供额外的结构性优势，HRSA 可作为诊断工具分析表示、几何、功能的变化。

Abstract: State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.

</details>


### [77] [Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification](https://arxiv.org/abs/2601.21210)
*Paul He,Yinya Huang,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 提出 DoVerifier，一种简单的符号化验证器，用于检查 LLM 生成的因果表达式是否从给定因果图通过 do-Calc 与概率理论可推导，提升对因果推理语义正确性的评估。


<details>
  <summary>Details</summary>
Motivation: 当前基准多依赖字符串匹配或表层指标，无法判定输出是否在因果推理语义上有效，因此需要一个能以形式语义进行验证的工具。

Method: 提出 DoVerifier：一个符号化验证器，判定 LLM 生成的因果表达式在给定因果图下是否通过 do-calculus 与概率论的规则可推导，基于形式推导进行语义正确性验证。

Result: 在合成数据和因果问答基准上，DoVerifier 更准确地捕捉因果推理的语义正确性，能够纠正因表面语义差异而导致的错误标注。

Conclusion: DoVerifier 提供了一种更严格、信息量更丰富的因果推理评估手段，有助于更客观地衡量和改进大模型在因果推理任务中的表现。

Abstract: Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.

</details>


### [78] [Causal Discovery for Explainable AI: A Dual-Encoding Approach](https://arxiv.org/abs/2601.21221)
*Henry Salgado,Meagan R. Kendall,Martine Ceberio*

Main category: cs.AI

TL;DR: 提出一种双编码因果发现方法，针对类别变量的条件独立性检验数值不稳定问题，通过两种互补编码策略并采用多数投票融合结果，在泰坦尼克数据集上得到与现有解释性方法一致的因果结构。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统因果发现方法在处理分类变量时的数值不稳定性和可解释性不足的问题；希望通过组合多编码策略提高鲁棒性与可解释性。

Method: 在约束基因因果发现框架中，使用两种互补的类别变量编码（如独热编码和另一种编码）运行约束检测，并对得到的因果边进行多数投票融合，得到稳定的因果结构。

Result: 在泰坦尼克数据集上，该方法识别的因果结构与公认的可解释性方法一致，表明双编码策略可提升对分类变量的因果发现鲁棒性。

Conclusion: 该方法为处理分类变量的因果发现提供一种鲁棒策略，且与现有解释性方法相一致，具有在实际模型解释中的潜在价值，但需进一步在不同数据集和变量类型上验证。

Abstract: Understanding causal relationships among features is fundamental for explaining machine learning model decisions. However, traditional causal discovery methods face challenges with categorical variables due to numerical instability in conditional independence testing. We propose a dual-encoding causal discovery approach that addresses these limitations by running constraint-based algorithms with complementary encoding strategies and merging results through majority voting. Applied to the Titanic dataset, our method identifies causal structures that align with established explainable methods.

</details>


### [79] [TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design](https://arxiv.org/abs/2601.21239)
*Chentong Chen,Mengyuan Zhong,Ye Fan,Jialong Shi,Jianyong Sun*

Main category: cs.AI

TL;DR: TIDE：一种将结构演化与参数调优解耦的混合进化框架，通过外部树形相似性编辑距离驱动结构多样性，内部结合LLM导向的逻辑生成与差分变异进行参数微调，并以UCB调度器优化资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决离散算法结构与连续数值参数耦合导致的性能退化问题；克服现有方法因未校准常数而错失优秀算法，以及因简单相似性度量导致的早熟收敛。

Method: 实现分层嵌套架构：外层并行岛模型，使用Tree Similarity Edit Distance促进结构多样性；内层结合LLM驱动的逻辑生成与差分变异算子进行参数微调；引入基于UCB的调度器动态分配资源以优先高产出提示策略；在九个组合优化问题上进行实验。

Result: TIDE发现的启发式方法在解质量上显著优于最先进基线，搜索效率提高且计算成本降低，表现出对问题规模的稳健性与泛化潜力。

Conclusion: 通过将结构演化与参数优化解耦，TIDE实现更高效的启发式发现，提升组合优化的解质与资源利用效率，具备良好的一般化潜力。

Abstract: Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.

</details>


### [80] [Position: Certifiable State Integrity in Cyber-Physical Systems -- Why Modular Sovereignty Solves the Plasticity-Stability Paradox](https://arxiv.org/abs/2601.21249)
*Enzo Nicolás Spotorno,Antônio Augusto Medeiros Fröhlich*

Main category: cs.AI

TL;DR: 提出 HYDRA：冻结的 regime 专家库通过不确定性感知的混合实现跨 regime 的鲁棒性与可证性，缓解全局微调的遗忘与谱偏，提供模块化、可审计的 CPS 状态推断框架。


<details>
  <summary>Details</summary>
Motivation: 在普遍存在的时间序列 foundation 模型下，CPS 受非平稳生命周期、严格可靠性要求、以及可验证性挑战影响，微调导致遗忘、谱偏和黑箱性妨碍安全认证。

Method: 提出模块化的 Sovereignty 范式：若干紧凑、冻结的 regime-specific 专家构成库，通过不确定性感知的加权混合（uncertainty-aware blending），实现 regime 条件有效性、aleatoric 与 epistemic 不确定性的严格解耦，以及可审计的模块化结构。

Result: 理论层面表述了在 CPS 生命周期内实现状态完整性与可证性的路径；通过冻结的子模型和不确定性驱动的组合，降低遗忘与谱偏风险，提升可追溯性。

Conclusion: HYDRA 为安全关键时序/动态系统提供一个可证性的、模块化的适应性框架，缓解 plasticity–stability 二元矛盾，便于验收与审计。

Abstract: The machine learning community has achieved remarkable success with universal foundation models for time-series and physical dynamics, largely overcoming earlier approximation barriers in smooth or slowly varying regimes through scale and specialized architectures. However, deploying these monolithic models in safety-critical Cyber-Physical Systems (CPS), governed by non-stationary lifecycle dynamics and strict reliability requirements, reveals persistent challenges. Recent evidence shows that fine-tuning time-series foundation models induces catastrophic forgetting, degrading performance on prior regimes. Standard models continue to exhibit residual spectral bias, smoothing high-frequency discontinuities characteristic of incipient faults, while their opacity hinders formal verification and traceability demanded by safety standards (e.g., ISO 26262, IEC 61508). This position paper argues that the plasticity-stability paradox cannot be fully resolved by global parameter updates (whether via offline fine-tuning or online adaptation). Instead, we advocate a Modular Sovereignty paradigm: a library of compact, frozen regime-specific specialists combined via uncertainty-aware blending, which we term "HYDRA" (Hierarchical uncertaintY-aware Dynamics for Rapidly-Adapting systems). This paradigm ensures regime-conditional validity, rigorous disentanglement of aleatoric and epistemic uncertainties, and modular auditability, offering a certifiable path for robust state integrity across the CPS lifecycle.

</details>


### [81] [Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving](https://arxiv.org/abs/2601.21288)
*Weitong Lian,Zecong Tang,Haoran Li,Tianjian Gao,Yifei Wang,Zixu Wang,Lingyi Meng,Tengju Ru,Zhejun Cui,Yichen Zhu,Hangshuo Cao,Qi Kang,Tianxing Chen,Yusen Qin,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: Drive-KD proposes a multi-stage knowledge distillation framework for autonomous driving, decomposing perception, reasoning, and planning into capability-specific single-teacher models via layer-specific attention, then unifying into a multi-teacher distillation with asymmetric gradient projection to reduce cross-capability conflicts. It yields a distilled InternVL3-1B model with far lower memory and higher throughput, surpassing large pretrained models on DriveBench and GPT-5.1 on planning.


<details>
  <summary>Details</summary>
Motivation: Large-model approaches for autonomous driving face high memory and latency, while SFT struggles to bridge capability gaps in small models. There is a need for efficient, capable VLM-based driving systems that generalize across model families and scales.

Method: Decompose the task into perception, reasoning, and planning. Use layer-specific attention as the distillation signal to create single-teacher models for each capability. Unify these into a multi-teacher distillation framework and apply asymmetric gradient projection to mitigate cross-capability gradient conflicts. Evaluate across model families/scales on DriveBench, comparing to large pretrained models.

Result: The distilled InternVL3-1B model achieves ~42x memory savings and ~11.4x throughput gains, while delivering better overall performance than a pretrained 78B model from the same family on DriveBench and surpassing GPT-5.1 on planning. Demonstrates generalization across model families/scales and shows efficiency gains without sacrificing performance.

Conclusion: Layer-specific attention-based distillation within a multi-teacher framework with asymmetric gradient projection can yield compact yet capable autonomous driving VLMs, achieving strong performance relative to large pretrained models and enabling more efficient deployment.

Abstract: Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a "perception-reasoning-planning" triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.

</details>


### [82] [Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation](https://arxiv.org/abs/2601.21335)
*Yuzhe Chen,Jie Cao,Youquan Wang,Haicheng Tao,Darko B. Vukovic,Jia Wu*

Main category: cs.AI

TL;DR: CNRE 将因果推理与神经符号推理结合用于可解释多行为推荐，利用层次偏好传播和内生逻辑规则，动态分发到神经-逻辑路径，生成可解释的因果中介，减缓混杂偏差，在三个大规模数据集上优于基线，提供从模型设计到决策过程再到推荐结果的多层可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决多行为推荐在可解释性方面的不足（追求性能牺牲可解释性）和现有可解释方法对外部信息的依赖导致的可泛化受限；通过利用用户行为链的内生逻辑与因果推理提升可解释性和鲁棒性。

Method: CNRE 通过分层偏好传播捕捉跨行为的异质依赖，基于偏好强度对用户行为链中的内生逻辑规则建模，并自适应分派到对应的神经-逻辑推理路径（如合取、析取），生成近似理想状态的因果中介以消除混杂效应。

Result: 在三个大规模数据集上的广泛实验表明 CNRE 在性能上显著优于最先进基线，并提供从模型设计、决策过程到推荐结果的多级可解释性。

Conclusion: 将因果推理融入神经符号推理框架的可解释多行为推荐，利用内生逻辑和因果中介降低混杂，提升解释性与推荐效果的结合效果。

Abstract: Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.

</details>


### [83] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 通过对12种LLM、10条创造力提示、每条100次采样的方差分解，比较提示、模型选择和采样噪声对输出的影响；结果显示提示对原创性影响与模型选择相近，但对流畅性受模型选择和内部方差支配，单次采样易混淆效应。


<details>
  <summary>Details</summary>
Motivation: 系统定量区分提示、模型与采样噪声对LLM输出的贡献度，以指导评估设计、提示设计和模型比较。

Method: 在12个LLM、10条创造力提示、每条100次采样（总样本12,000）上，对输出分量进行方差分解，分解来源为 prompts、模型选择、同一模型内采样变异（采样噪声）等，评估两个维度：原创性（output quality）和流畅性（output quantity）的贡献比例。

Result: 原创性：提示解释36.43%方差，模型选择40.94%；流畅性：模型选择51.25%，同一模型内方差33.70%，提示仅4.22%。

Conclusion: 提示在提升输出质量方面具有显著作用，与模型选择相当；但存在显著的内部采样方差，单次样本评价易将采样噪声误导为提示或模型效应，需采用多样本评估和方差分解以获得可靠比较。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [84] [EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21340)
*Lang Cao,Qingyu Chen,Yue Guo*

Main category: cs.AI

TL;DR: 提出了 EHR-RAG 框架，用于对长时程结构化 EHR 数据进行检索增强的解释与预测，解决上下文长度限制及时序依赖问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长时程 EHR 上的上下文截断和信息丢失，利用结构化时间信息与证据检索提升预测准确性。

Method: 提出三大组件：1) 事件-时间感知的混合 EHR 检索以保留临床结构与时间动态；2) 自适应迭代检索以逐步扩展证据覆盖；3) 双路径证据检索与推理以同时检索和推理事实证据与反事实证据；在四项长时程 EHR 预测任务上与最强基线相比提升 Macro-F1 平均约 10.76%。

Result: 在四项任务中验证，EHR-RAG 显著优于强基线，其 Macro-F1 提升约 10.76%（平均值）。

Conclusion: 展示了面向结构化 EHR 数据的检索增强 LLM 对临床预测有潜力，强调保留结构与时序信息的重要性，以及通过迭代检索和双路径证据来提升推理能力。

Abstract: Electronic Health Records (EHRs) provide rich longitudinal clinical evidence that is central to medical decision-making, motivating the use of retrieval-augmented generation (RAG) to ground large language model (LLM) predictions. However, long-horizon EHRs often exceed LLM context limits, and existing approaches commonly rely on truncation or vanilla retrieval strategies that discard clinically relevant events and temporal dependencies. To address these challenges, we propose EHR-RAG, a retrieval-augmented framework designed for accurate interpretation of long-horizon structured EHR data. EHR-RAG introduces three components tailored to longitudinal clinical prediction tasks: Event- and Time-Aware Hybrid EHR Retrieval to preserve clinical structure and temporal dynamics, Adaptive Iterative Retrieval to progressively refine queries in order to expand broad evidence coverage, and Dual-Path Evidence Retrieval and Reasoning to jointly retrieves and reasons over both factual and counterfactual evidence. Experiments across four long-horizon EHR prediction tasks show that EHR-RAG consistently outperforms the strongest LLM-based baselines, achieving an average Macro-F1 improvement of 10.76%. Overall, our work highlights the potential of retrieval-augmented LLMs to advance clinical prediction on structured EHR data in practice.

</details>


### [85] [Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores](https://arxiv.org/abs/2601.21342)
*Zhiyong Shen,Gongpeng Zhao,Jun Zhou,Li Yu,Guandong Kou,Jichen Li,Chuanlei Dong,Zuncheng Li,Kaimao Li,Bingkun Wei,Shicheng Hu,Wei Xia,Wenguo Duan*

Main category: cs.AI

TL;DR: 面向食品服务与零售场景的多模态大语言模型（MLLM） Ostrakon-VL 在 ShopBench 上创下新SOTA，并提出 QUAD 数据筛选流程与多阶段训练策略；公开对比参数规模相近的模型，展现更高的参数效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: FSRS 数据存在噪声、缺乏可审计的闭环数据采集，制约高质量、可控、可复现的训练语料；现有评估缺乏统一、细粒度且覆盖单图、多图、视频输入的基准，难以客观衡量鲁棒性。

Method: 基于 Qwen3-VL-8B 架构的 Ostrakon-VL；提出 ShopBench 公共基准；提出 QUAD（Quality-aware Unbiased Automated Data-curation）的多阶段多模态指令数据筛选管线；通过多阶段训练策略实现提升。

Result: Ostrakon-VL 在 ShopBench 的平均分为 60.1，成为同规模开源 MLLMs 的新SOTA；比规模更大的 Qwen3-VL-235B-A22B 高 0.7 分、比同规模 Qwen3-VL-8B 高 4.8 分，显示在 FSRS 任务上的参数效率与鲁棒性显著提升。

Conclusion: 该工作表明 Ostrakon-VL 能提供更鲁棒、可靠的面向 FSRS 的感知与决策能力；为可重复研究，计划公开发布模型和 ShopBench 基准。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.

</details>


### [86] [BEAP-Agent: Backtrackable Execution and Adaptive Planning for GUI Agents](https://arxiv.org/abs/2601.21352)
*Ziyu Lu,Tengjin Weng,Yiying Yang,Yuhang Zhao,Xinxin Huang,Wenhao Jiang*

Main category: cs.AI

TL;DR: A DFS-based GUI agent (BEAP-Agent) enabling long-range state backtracking for GUI tasks; uses Planner/Executor/Tracker, evaluated on OSWorld with 28.2% accuracy.


<details>
  <summary>Details</summary>
Motivation: GUI agents often fail to recover after following an incorrect exploration path, leading to task failure; there is a need for systematic backtracking to enable long-horizon task exploration.

Method: BEAP-Agent models GUI task execution as a DFS process and introduces three collaborative components—Planner, Executor, and Tracker—that support long-range, multi-level backtracking and dynamic task tracking/updating.

Result: On the OSWorld benchmark, BEAP-Agent achieved an accuracy of 28.2%, indicating the proposed backtracking framework can enable more robust long-horizon GUI task exploration.

Conclusion: BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, providing a structured solution for long-horizon GUI task exploration.

Abstract: GUI agents are designed to automate repetitive tasks and enhance productivity. However, existing GUI agents struggle to recover once they follow an incorrect exploration path, often leading to task failure. In this work, we model GUI task execution as a DFS process and propose BEAP-Agent, a DFS-based framework that supports long-range, multi-level state backtracking with dynamic task tracking and updating. The framework consists of three collaborative components: Planner, Executor, and Tracker. Together, they enable effective task exploration and execution. BEAP-Agent fills the gap in systematic backtracking mechanisms for GUI agents, offering a systematic solution for long-horizon task exploration. We conducted a systematic evaluation on the OSWorld benchmark, where BEAP-Agent achieved an accuracy of 28.2%, validating the effectiveness of the proposed method.

</details>


### [87] [Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization](https://arxiv.org/abs/2601.21358)
*Jiecong Wang,Hao Peng,Chunyang Liu*

Main category: cs.AI

TL;DR: PLaT通过将隐性推理视为规划并将其与文本产出解耦，使用潜在规划状态的确定性轨迹进行推理，只有需要时才由解码器将想法落地为文本，并实现动态终止，从而提升推理多样性和可扩展性，适用于推理时搜索的透明基础。


<details>
  <summary>Details</summary>
Motivation: 解决CoT在离散符号空间的高昂计算成本和路径崩溃问题；现有隐性推理通常是端到端映射，步数固定且不可解释；需要一种将推理过程与语言产出解耦、允许动态决定终止的框架。

Method: 将推理建模为潜在规划状态的确定性轨迹，独立的解码器在需要时将潜在思想落地为文本；实现推理与产出解耦，允许在推理过程内根据需要进行推理步数的动态终止；并在需要时进行推理-搜索的推理-落地分离。

Result: 在数学基准上，PLaT的贪心正确率低于基线但在推理多样性方面具有更好可扩展性；展示出更鲁棒的更广泛解空间，并为推理时搜索提供透明、可扩展的基础。

Conclusion: 隐性规划与文本产出解耦为推理提供了动态终止能力和可扩展的推理搜索基础，尽管在贪心准确性上存在折衷，但PLaT构成一个对推理-语言产出分离的有利框架。

Abstract: Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.

</details>


### [88] [System 1&2 Synergy via Dynamic Model Interpolation](https://arxiv.org/abs/2601.21414)
*Chenxu Yang,Qingyi Si,Chong Tian,Xiyu Liu,Dingyu Yao,Chuanyu Qin,Zheng Lin,Weiping Wang,Jiaqi Wang*

Main category: cs.AI

TL;DR: 提出 DAMI 框架，通过对查询进行动态参数插值以实现系统1/系统2的自适应认知深度，在不额外训练的前提下从 Instruct 与 Thinking 检查点中插值获得高效且准确的推理，在五个数学推理基准上优于 Thinking 模型，兼具高效性。


<details>
  <summary>Details</summary>
Motivation: 统一语言模型在系统1/系统2之间切换时存在干扰，输出控制方法局限于输出长度，未直接解决认知配置问题。需要通过能力控制来调节模型的思维过程；利用线性插值在不额外训练的情况下在 Instruct 与 Thinking 检查点之间实现平滑的认知配置，理论支撑包括表示连续性和结构连通性。

Method: 使用线性参数插值在 Instruct 与 Thinking 两个检查点之间进行动态切换；不进行额外训练；为训练场景提出偏好学习方法，将准确性与效率作为目标进行编码；零-shot 部署时使用基于认知差异的置信度方法进行选择。

Result: 在五个数学推理基准上，DAMI 的准确性高于 Thinking 模型，同时保持较高的效率，形成凸的帕累托前沿，得到 representation continuity 与 structural connectivity 的理论支撑。

Conclusion: 通过 λ(q) 的查询特定推理强度估计实现能力控制，成功将系统1的高效性与系统2的推理深度结合，适用于多种数学推理任务，并且可扩展到零-shot 设置。

Abstract: Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \textit{how models think} rather than \textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \textbf{DAMI} (\textbf{D}yn\textbf{A}mic \textbf{M}odel \textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.

</details>


### [89] [When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models](https://arxiv.org/abs/2601.21433)
*Katherine Elkins,Jon Chun*

Main category: cs.AI

TL;DR: 负否指令的解读在当前大模型中普遍存在偏差：开源模型在简单否定下有77%、在组合否定下有100%执行被禁止的动作；商业模型表现更好但仍有波动（19–128%）。模型在肯定提示下的一致性74%，在否定提示下降至62%；金融场景比医疗场景更脆弱。为治理提出否定敏感性指数(NSI)及分层认证框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于确保AI在高风险情境中正确理解“应该不做某事”等否定指令，以避免自动化决策的安全风险。现有对齐方法并未充分解决模型对否定指令的鲁棒性。

Method: 对16种模型与14个伦理场景进行审计，比较简单否定与组合否定的影响；在确定性解码条件下排除采样噪声；给出案例研究；提出否定敏感性指数(NSI)作为治理指标，并提出带领域阈值的分层认证框架。

Result: 结果显示：开源模型在简单否定下违规率为77%，在组合否定下为100%（相比肯定框架提升317%）；商业模型的失败幅度为19–128%，但总体表现优于开源模型；肯定提示下模型一致性74%，否定提示下62%；金融场景比医疗场景脆弱；以上现象在确定性解码条件下仍成立；并辅以若干案例研究。

Conclusion: 结论指出当前的对齐技术与安全部署之间存在差距：若模型不能可靠地区分“做X”与“不做X”，则不应在高风险场景中自主决策；提出NSI与分层认证框架以提升部署安全性。

Abstract: When a user tells an AI system that someone "should not" take an action, the system ought to treat this as a prohibition. Yet many large language models do the opposite: they interpret negated instructions as affirmations. We audited 16 models across 14 ethical scenarios and found that open-source models endorse prohibited actions 77% of the time under simple negation and 100% under compound negation -- a 317% increase over affirmative framing. Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones. These patterns hold under deterministic decoding, ruling out sampling noise. We present case studies showing how these failures play out in practice, propose the Negation Sensitivity Index (NSI) as a governance metric, and outline a tiered certification framework with domain-specific thresholds. The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish "do X" from "do not X" should not be making autonomous decisions in high-stakes contexts.

</details>


### [90] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: 在高风险领域中，指令微调的LLM对情感叙事的影响几乎无感，与人类存在显著偏见形成对比，呈现“鲁棒性悖论”。


<details>
  <summary>Details</summary>
Motivation: 探究在重大决策任务中，已知对提示敏感的LLM是否也对情感叙事保持鲁棒，以及其与人类的对比；揭示指令式训练是否使模型在“为何偏见”与“是否遵循规则”之间解耦。

Method: 提出一个受控扰动框架，在医疗、法律、金融三个高风险领域进行情感叙事干扰测试；使用162个情景基准、跨模型对比，量化效应量(Cohen's h)并比较人类；结果显示模型对叙事操控的抵抗力远超人类，且跨培训范式稳定。

Result: 模型对情感叙事的影响近似为零（Cohen's h ≈ 0.003），人类则显著偏向（Cohen's h在0.3–0.8之间），并且抵抗力在110–300倍区间，结果在不同模型和训练范式下保持一致；提出可将逻辑规则遵循与说服性叙事分离的机制。

Conclusion: 指令微调的LLMs在格式敏感性与内容偏见之间表现出分离的鲁棒性，提供稳定的决策输出，并可能用于缓解人类决策中的叙事偏误；研究提供跨领域的基准与工具，便于系统评估叙事诱导偏见。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [91] [ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design](https://arxiv.org/abs/2601.21448)
*Zhongkai Yu,Chenyang Zhou,Yichen Lin,Hejia Zhang,Haotian Ye,Junxia Cui,Zaifeng Pan,Jishen Zhao,Yufei Ding*

Main category: cs.AI

TL;DR: 提出 ChipBench，一套面向AI辅助芯片设计的综合基准，覆盖 Verilog 生成、调试、参考模型生成三大任务，包含44个有层次结构的真实模块、89个系统化调试用例和132个跨Python、SystemC、CXXRTL的参考模型样本；评估显示现有SOTA模型在关键任务上仍存在显著差距，同时提供自动化高质量训练数据生成工具，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基准在任务多样性和真实工业工作流的覆盖率不足，难以反映大规模语言模型在实际芯片设计中的性能表现，需要一个更接近真实工作流的评测体系以推动该领域发展。

Method: 构建三大任务的基准：Verilog 生成、调试和参考模型生成；设计44个具有复杂层次结构的真实模块、89个系统性调试用例以及132个跨语言的参考模型样本（Python、SystemC、CXXRTL）；对SOTA模型（如 Claude-4.5-opus）进行评测，提供用于高质量训练数据生成的自动化工具箱；代码公开。

Result: 评测结果显示显著性能差距：Claude-4.5-opus在Verilog生成达30.74%，在Python参考模型生成仅13.33%，明显低于在饱和基准上通常可达>95%的表现。

Conclusion: 该基准揭示行业级任务的挑战性并推动更真实的评测研究，同时提供自动化数据生成工具以促进未来对参考模型生成的研究；相关代码仓库可用于复现与扩展。

Abstract: While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\% on Verilog generation and 13.33\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.

</details>


### [92] [Topeax -- An Improved Clustering Topic Model with Density Peak Detection and Lexical-Semantic Term Importance](https://arxiv.org/abs/2601.21465)
*Márton Kardos*

Main category: cs.AI

TL;DR: Topeax proposes density-peak-based cluster number estimation and a combined lexical-semantic term-importance measure to address weaknesses of Top2Vec and BERTopic; claims improved cluster recovery, topic coherence, and robustness to sample size/hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Current topic clustering methods (Top2Vec, BERTopic) show instability and suboptimal keyword selection due to excessive reliance on embeddings and simplistic term weighting; a method that automatically determines cluster count and refines keyword scoring could improve reliability and quality.

Method: Introduce Topeax: estimate the number of clusters from peaks in data-density estimates; compute term importance using a joint lexical-semantic index; compare against Top2Vec and BERTopic on cluster recovery and topic description; report reduced sensitivity to sample size and hyperparameters.

Result: Reportedly superior in cluster recovery and description quality compared with Top2Vec and BERTopic; demonstrates more stable behavior across varying sample sizes and hyperparameter settings.

Conclusion: Topeax offers automatic cluster-number detection and integrated keyword scoring to yield higher-quality, more robust topics than existing methods; further validation across datasets and settings recommended.

Abstract: Text clustering is today the most popular paradigm for topic modelling, both in academia and industry. Despite clustering topic models' apparent success, we identify a number of issues in Top2Vec and BERTopic, which remain largely unsolved. Firstly, these approaches are unreliable at discovering natural clusters in corpora, due to extreme sensitivity to sample size and hyperparameters, the default values of which result in suboptimal behaviour. Secondly, when estimating term importance, BERTopic ignores the semantic distance of keywords to topic vectors, while Top2Vec ignores word counts in the corpus. This results in, on the one hand, less coherent topics due to the presence of stop words and junk words, and lack of variety and trust on the other. In this paper, I introduce a new approach, \textbf{Topeax}, which discovers the number of clusters from peaks in density estimates, and combines lexical and semantic indices of term importance to gain high-quality topic keywords. Topeax is demonstrated to be better at both cluster recovery and cluster description than Top2Vec and BERTopic, while also exhibiting less erratic behaviour in response to changing sample size and hyperparameters.

</details>


### [93] [The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation](https://arxiv.org/abs/2601.21505)
*Diaoulé Diallo,Katharina Dworatzyk,Sophie Jentzsch,Peer Schütt,Sabine Theis,Tobias Hecking*

Main category: cs.AI

TL;DR: Activation steering can control the emotional tone of LLM outputs and is validated via human ratings; moderate steering (~0.15) amplifies target emotions with preserved quality; upgrading from Alpaca to LlaMA-3 improves consistency; supports activation-based control as scalable for affective steering.


<details>
  <summary>Details</summary>
Motivation: Address the need for aligning LLM outputs with human preferences and safety without heavy prompts or fine-tuning; evaluate the practicality and reliability of activation steering for affective control.

Method: Modify internal activations to steer outputs. Conducted the first human evaluation of activation steering on emotional tone with >7,000 crowd-sourced ratings from 190 participants via Prolific. Measured perceived emotional intensity and overall text quality. Assessed correspondence between human and model-based quality scores (mean r=0.776; range 0.157–0.985). Tested moderate steering strengths (λ≈0.15) across emotions; reported ηp2 for disgust (0.616), fear (0.540), surprise (0.042). Compared Alpaca vs. LlaMA-3 demonstrating more consistent steering (all p<0.001). Inter-rater reliability ICC=0.71–0.87.

Result: Moderate steering reliably amplifies target emotions while preserving comprehensibility. Strongest effects for disgust and fear; weakest for surprise. High alignment between human and model-based quality ratings. Transition to LlaMA-3 yields more consistent and significant emotional steering across emotions and strengths.

Conclusion: Activation-based control is a scalable method for steering LLM behavior across affective dimensions, with validated human evaluation and improved robustness with newer models (LlaMA-3).

Abstract: Controlling the behavior of large language models (LLMs) at inference time is essential for aligning outputs with human abilities and safety requirements. \emph{Activation steering} provides a lightweight alternative to prompt engineering and fine-tuning by directly modifying internal activations to guide generation. This research advances the literature in three significant directions. First, while previous work demonstrated the technical feasibility of steering emotional tone using automated classifiers, this paper presents the first human evaluation of activation steering concerning the emotional tone of LLM outputs, collecting over 7,000 crowd-sourced ratings from 190 participants via Prolific ($n=190$). These ratings assess both perceived emotional intensity and overall text quality. Second, we find strong alignment between human and model-based quality ratings (mean $r=0.776$, range $0.157$--$0.985$), indicating automatic scoring can proxy perceived quality. Moderate steering strengths ($λ\approx 0.15$) reliably amplify target emotions while preserving comprehensibility, with the strongest effects for disgust ($η_p^2 = 0.616$) and fear ($η_p^2 = 0.540$), and minimal effects for surprise ($η_p^2 = 0.042$). Finally, upgrading from Alpaca to LlaMA-3 yielded more consistent steering with significant effects across emotions and strengths (all $p < 0.001$). Inter-rater reliability was high (ICC $= 0.71$--$0.87$), underscoring the robustness of the findings. These findings support activation-based control as a scalable method for steering LLM behavior across affective dimensions.

</details>


### [94] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: 提出 ARGORA：将多专家对话组织成显式的论证图，作为因果模型来测试论点的重要性与相互作用，并引入纠偏机制对齐内部推理与外部判断，在多基准下具备竞争性准确性并提升争议纠正能力，提供因果诊断。


<details>
  <summary>Details</summary>
Motivation: 现有的多专家大模型系统通常通过简单聚合整合观点，难以揭示哪些论点驱动最终决策，缺乏透明性、可审计性以及对推理过程的因果因果理解。需要一种可解释且可操作的框架来追踪、测试和纠正论点间的因果关系。

Method: 将多专家讨论组织为显式的论证图，表示论点之间的支持与攻击关系；将该图视为因果模型，能通过删除单个论点并重新计算结果来评估其必要性；引入纠偏机制以在内部推理与外部判断不一致时进行对齐；在多样基准与一个开放用例上进行评估。

Result: 在不同基准和用例中，ARGORA实现了具竞争力的准确性，并展示了纠偏行为：当专家初次意见不一致时，框架在更常纠正至正确答案的方向上收敛而非引入新错误，同时提供决定性论点的因果诊断。

Conclusion: ARGORA为多专家LLM系统提供可解释的因果推理轨迹、可操作的干预与纠偏能力，并通过因果诊断提升争议解决效果，具备良好的可审计性与推广潜力。

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [95] [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/abs/2601.21557)
*Haoran Ye,Xuning He,Vincent Arak,Haonan Dong,Guojie Song*

Main category: cs.AI

TL;DR: MCE introduces a bi-level, co-evolving framework for context engineering of LLMs, coupling a meta-level skill-evolver with a base-level context-optimizing agent to surpass static CE methods across multiple domains.


<details>
  <summary>Details</summary>
Motivation: Current context engineering relies on manually crafted, static harnesses that bias generation and limit optimization to narrow design spaces. There is a need for adaptive, transferable CE paradigms that co-evolve skills and artifacts.

Method: A bi-level architecture where a meta-level agent performs agentic crossover and deliberative search over historical skills, executions, and evaluations to refine CE skills, while a base-level agent executes these skills, learns from rollouts, and optimizes context as flexible files and code. The framework iteratively co-evolves skills and context artifacts.

Result: Evaluated across five domains in offline and online settings, MCE yields 5.6–53.8% relative improvement over state-of-the-art agentic CE methods (mean ~16.9%), with superior context adaptability, transferability, and efficiency.

Conclusion: MCE supersedes static CE heuristics by enabling co-evolution of CE skills and context artifacts, delivering consistent performance gains and broader applicability across diverse domains.

Abstract: The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.

</details>


### [96] [Chain Of Thought Compression: A Theoritical Analysis](https://arxiv.org/abs/2601.21576)
*Juncai Li,Ru Li,Yuxiang Zhou,Boxiang Ma,Jeff Z. Pan*

Main category: cs.AI

TL;DR: 本文从理论角度分析内在推理中间步骤的学习困难，提出 Order-r Interaction，证明高阶逻辑依赖的学习信号呈指数衰减，导致若跳过中间步骤就会遇到高阶交互屏障。通过 NatBool-DAG 基准测试验证并提出 ALiCoT，以对齐潜在 token 分布来缓解信号衰减，实现显著的效率提升（约 54.4x）且性能接近显式推理。


<details>
  <summary>Details</summary>
Motivation: 解决显式连锁推理（CoT）带来的计算成本，同时揭示隐含 CoT 压缩的机理与学习信号的衰减过程，目标是在不牺牲准确度的前提下实现高效推理。

Method: 提出 Order-r Interaction 的理论框架来刻画高阶逻辑依赖的学习信号衰减；构建 NatBool-DAG 以强制不可简化的逻辑推理并排除语义捷径；提出 ALiCoT 框架，通过将潜在 token 分布与中间推理状态对齐来缓解信号衰减。

Result: 理论上证明学习信号对高阶依赖呈指数衰减，存在不可规约的问题；实验上 NatBool-DAG 设计验证了不可简化的推理特性；ALiCoT 实现了显著加速（54.4x）且与显式 CoT 的性能相当。

Conclusion: 揭示隐式 CoT 压缩的学习难点及其可被对齐的解决路径，为高效推理提供理论-实验的一致性框架，ALiCoT 提供一种可行的高效推理替代方案。

Abstract: Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.

</details>


### [97] [Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves](https://arxiv.org/abs/2601.21582)
*Jonas Knupp,Jan Hendrik Metzen,Jeremias Bohn,Georg Groh,Kristian Kersting*

Main category: cs.AI

TL;DR: 给出一个模块化的深度递归注意力框架Dreamer，用深度注意力、序列注意力与稀疏专家注意力的组合来缓解隐藏层尺寸瓶颈，以实现更高效的多步潜在推理。


<details>
  <summary>Details</summary>
Motivation: 现有的深度递归方法在基线对比、参数/ FLOP/内存匹配以及隐藏尺寸瓶颈方面存在不足，难以实现大步长的潜在推理且受限于固定的隐藏维度。

Method: 提出Dreamer框架，将序列注意力、深度注意力和稀疏专家注意力结合，沿深度进行注意力以缓解隐藏尺寸瓶颈并实现深度递归的高效扩展；解耦缩放维度，允许在深度上进行注意力以提升跨深度知识重用；使用MoEs等稀疏专家机制提高容量。

Result: 在语言推理基准上，与FLOP/参数/内存匹配的SOTA相比，所需训练令牌数量减少2–8倍即可达到相同精度；在同等训练令牌数量下，模型性能超过大约2倍规模的SOTA。进一步显示跨深度的知识获取与专家选择多样性提升（2–11x）相较于SOTA MoEs。

Conclusion: 深度递归注意力混合框架能够缓解隐藏尺寸瓶颈、实现更高效的跨深度推理，并通过在深度维度上的注意力和稀疏专家结构提升知识利用和模型扩展性。

Abstract: Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.

</details>


### [98] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: 提出 Active Latent Planning（ATP-Latent），通过条件VAE平滑潜在空间并以一致性奖励的强化学习引导潜在推理策略，从而提升密集型的零碎链式推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有潜在令牌多基于模仿语言标签，存在多种等价但差异化的CoT标签；被动模仿可能导致潜在表示与策略在训练/测试时错配，需在潜在表示空间进行主动规划以获得最优推理策略。

Method: 将潜在令牌的监督过程建模为条件变分自编码器（VAE），得到更平滑的潜在空间；在强化学习阶段引入辅助一致性奖励，基于VAE解码的潜在令牌内容的一致性来引导RL学习潜在推理策略。

Result: 在LLaMA-1B上，ATP-Latent相对于先进基线提升4.1个百分点的准确率，且减少3.3个百分点的令牌数量，覆盖四个基准；代码公开。

Conclusion: 强调在潜在表示空间进行主动规划的必要性，平滑的潜在空间有助于学习更合理的潜在推理策略，结合一致性奖励的RL可提升潜在推理的有效性与token效率。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [99] [Search-Based Risk Feature Discovery in Document Structure Spaces under a Constrained Budget](https://arxiv.org/abs/2601.21608)
*Saisubramaniam Gopalakrishnan,Harikrishnan P M,Dagnachew Birru*

Main category: cs.AI

TL;DR: 将企业级IDP系统的验证问题建模为SBST，以预算内最大化发现的不同故障类型，并比较多种搜索策略，发现求解器互补性，主张基于投资组合的SBST。


<details>
  <summary>Details</summary>
Motivation: 在有限预算下对早期验证进行稳健的故障暴露；单一最坏情况并不足以覆盖现实风险，需发现多种交互故障模式，提升工业IDP的鲁棒性。

Method: 在文档变量的组合空间上构造结构化的风险特征，生成现实的故障诱发条件；在相同预算下对演化、群智能、质量多样性、学习驱动、量子等多类搜索策略进行基准比较；通过配置层级排他性、胜率、跨时重叠分析评估。

Result: 各求解器在不同失败模式上有互补性，且存在跨预算的持久性发现，单一策略并无绝对领先；所有求解器的并集能覆盖观察到的故障空间，但依赖任一单一方法会显著延迟关键风险的发现。

Conclusion: 结果表明求解器间存在固有互补性，基于投资组合的SBST对工业IDP验证更为稳健，应优先采用多策略组合以提高发现效率与覆盖率。

Abstract: Enterprise-grade Intelligent Document Processing (IDP) systems support high-stakes workflows across finance, insurance, and healthcare. Early-phase system validation under limited budgets mandates uncovering diverse failure mechanisms, rather than identifying a single worst-case document. We formalize this challenge as a Search-Based Software Testing (SBST) problem, aiming to identify complex interactions between document variables, with the objective to maximize the number of distinct failure types discovered within a fixed evaluation budget. Our methodology operates on a combinatorial space of document configurations, rendering instances of structural \emph{risk features} to induce realistic failure conditions. We benchmark a diverse portfolio of search strategies spanning evolutionary, swarm-based, quality-diversity, learning-based, and quantum under identical budget constraints. Through configuration-level exclusivity, win-rate, and cross-temporal overlap analyses, we show that different solvers consistently uncover failure modes that remain undiscovered by specific alternatives at comparable budgets. Crucially, cross-temporal analysis reveals persistent solver-specific discoveries across all evaluated budgets, with no single strategy exhibiting absolute dominance. While the union of all solvers eventually recovers the observed failure space, reliance on any individual method systematically delays the discovery of important risks. These results demonstrate intrinsic solver complementarity and motivate portfolio-based SBST strategies for robust industrial IDP validation.

</details>


### [100] [ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval](https://arxiv.org/abs/2601.21654)
*Hao Shen,Hang Yang,Zhouhong Gu*

Main category: cs.AI

TL;DR: ScholarGym 是一个用于可重复评估学术文献研究工作流的仿真环境，基于静态语料库实现确定性检索，解耦查询规划、工具调用与相关性评估，并提供 2,536 条带专家 Ground Truth 的查询。通过对多种模型的实验，展示迭代改进中推理、规划和选择机制的交互。


<details>
  <summary>Details</summary>
Motivation: 现有以 live API 为基础的工作流评估因结果随时间、速率限制与后端状态变化而产生非确定性，削弱再现性和跨系统比较。需要一个可控、可重复的评估环境。

Method: 构建 ScholarGym，将工作流分解为三个组成部分：查询规划、工具调用、相关性评估；使用 570K 篇论文的静态语料库实现确定性检索；提供 2,536 条带专家标注 Ground Truth 的查询；在多种 backbone 模型上进行实验，分析推理、规划与选择机制在迭代改进中的作用。

Result: 实验表明不同推理能力、规划策略和选择机制的组合对迭代改进过程的效果有显著影响，且在可控环境中实现对各阶段的细粒度分析与比较。

Conclusion: ScholarGym 提供可重复的研究工作流评估平台，降低对外部 API 的依赖，便于横向对比，并帮助揭示深度研究工作流的组件交互与改进路径。

Abstract: Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.
  We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.

</details>


### [101] [SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://arxiv.org/abs/2601.21666)
*Ahmed Y. Radwan,Christos Emmanouilidis,Hina Tabassum,Deval Pandya,Shaina Raza*

Main category: cs.AI

TL;DR: 提出了 SONIC-O1：一个覆盖13个现实对话领域、完全人工核验的多模态大语言模型基准，聚焦时序音视频理解，包含开放式摘要、MCQ 与带推理的时序定位等任务，共4,958条标注与人口统计元数据，揭示不同模型族在各任务上的能力差异与社会偏差。


<details>
  <summary>Details</summary>
Motivation: 当前大多数工作聚焦静态图像理解，缺乏对可处理连贯音视频数据的高质量评测基准；需在真实世界场景下评估多模态语言模型的时序理解能力与社会鲁棒性。

Method: 构建完全人工核验的基准 SONIC-O1，覆盖13个现实对话域，包含4,958条标注与人口统计元数据；评估任务包括开放式摘要、MCQ、以及带解释的时序定位；对闭源与开源模型进行对比实验，分析跨任务与跨群体的表现与差异，公开数据集、代码和排行榜以促进复现。

Result: 在 MCQ 任务上，两个模型家族之间的准确率差距相对较小；在时序定位任务上，闭源和开源模型之间存在显著差距，最高达到22.6%；不同人口统计群体的性能存在下降，体现模型行为的社会性差异。

Conclusion: SONIC-O1 为 temporally grounded 与社会鲁棒的多模态理解提供了开放的评测套件，促进可重复性研究，相关数据、代码与排行榜对外发布。

Abstract: Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard

</details>


### [102] [TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning](https://arxiv.org/abs/2601.21692)
*Mingzu Liu,Hao Fang,Runmin Cong*

Main category: cs.AI

TL;DR: 提出 Tri-Component Attention Profiling (TCAP)——一个无监督的后门防御框架，通过分解跨模态注意力到系统指令、视觉输入、用户文本三组分并用 GMM/EM 策略识别并过滤 poisoned 样本，在多种 MLLM 与攻击场景中表现出稳健的防御性能。


<details>
  <summary>Details</summary>
Motivation: FTaaS 促成多模态大语言模型的定制化部署，但引入普遍且隐蔽的后门风险；现有防御要么依赖监督信号，要么在跨触发类型和模态的泛化能力上不足。

Method: TCAP 将跨模态注意力映射分解为系统指令、视觉输入、用户文本三大组件；通过高斯混合模型（GMM）识别触发相关的注意头；并使用基于 EM 的投票聚合来筛除被污染样本。

Result: 在多种 MLLM 架构和攻击方法上，TCAP 展现出稳定的强防御性能，证明其鲁棒性和实用性。

Conclusion: 无监督的三组件注意力分析为 MLLMs 的后门防御提供普适、可扩展的解决方案，TCAP 为实际防御场景增添了一项有效工具。

Abstract: Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.

</details>


### [103] [FBS: Modeling Native Parallel Reading inside a Transformer](https://arxiv.org/abs/2601.21708)
*Tongxi Wang*

Main category: cs.AI

TL;DR: 提出Fovea-Block-Skip Transformer (FBS)，通过Parafovea-Attention Window, Chunk-Head, Skip-Gate在Transformer中引入可训练的因果循环，提升推理时的质量与效率比，且不增加模型参数。


<details>
  <summary>Details</summary>
Motivation: 解决依赖逐 token 自回归的推理瓶颈，补充人类阅读中的内容前瞻性、基于块的计算分配以及训练与测试的一致性问题，针对预览/ skim等场景进行优化。

Method: 在Transformer中引入可训练循环，核心组件为Parafovea-Attention Window (PAW)、Chunk-Head (CH)、Skip-Gate (SG)，形成FBS框架，利用PAW实现因果注意的窗口化前瞻，CH实现块级别的计算分配，SG实现跳过门控以控制推理路径，三者协同提升效率与质量。

Result: 在多项基准上实现更优的质量–效率折中，且不增长参数规模；消融实验显示PAW、CH、SG三者互补。

Conclusion: FBS通过可训练循环与三大模块实现对推理过程的内容感知和高效计算，提升大语言模型的推理效率与质量，在不增加参数的前提下具有良好的一致性与通用性。

Abstract: Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.

</details>


### [104] [DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting](https://arxiv.org/abs/2601.21726)
*Siru Zhong,Yiqiu Liu,Zhiqing Cui,Zezhi Shao,Fei Wang,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出 DropoutTS，一种模型无关的时间序列鲁棒性插件，通过样本自适应 dropout 将噪声转化为可调 dropout 率，基于频谱稀疏性估计实例级噪声，提升各主干在噪声环境中的鲁棒性，且开销极小。


<details>
  <summary>Details</summary>
Motivation: 深度时间序列模型易受现实世界噪声影响，现有鲁棒性策略要么裁剪数据要么需代价高昂的先验量化，难以在有效性与效率之间取得平衡。

Method: 提出 Sample-Adaptive Dropout 机制，利用频谱稀疏性通过重构残差量化实例级噪声，并据此映射到自适应 dropout 率，选择性抑制伪波动、保留细粒度信息；作为插件无需修改模型架构。

Result: 在多种噪声情形与公开基准上，DropoutTS 稳定提升强基线的鲁棒性，具备极小的参数开销且无需额外结构修改；代码可复现。

Conclusion: DropoutTS 为时间序列建模提供一个高效、可落地的鲁棒性插件，兼具效果与资源友好性。

Abstract: Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from "what" to learn to "how much" to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones' performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.

</details>


### [105] [Language-based Trial and Error Falls Behind in the Era of Experience](https://arxiv.org/abs/2601.21754)
*Haoyu Wang,Guozheng Ma,Shugang Cui,Yilun Kong,Haotian Luo,Li Shen,Mengya Gao,Yichao Wu,Xiaogang Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: SCOUT decouples exploration from exploitation by using lightweight scouts to cheaply probe environment dynamics, bootstraps an LLM via supervised fine-tuning on collected trajectories, and then applies multi-turn RL; this yields strong performance with reduced compute on unseen, nonlinguistic tasks.


<details>
  <summary>Details</summary>
Motivation: The primary bottleneck for applying LLMs to unseen, nonlinguistic environments is the prohibitive exploration cost in high-dimensional semantic spaces, not only distribution mismatch; reducing exploration cost is essential for practical deployment.

Method: Introduce Scouts (e.g., small MLPs) to rapidly explore environmental dynamics and collect trajectories at high speed. Use these trajectories to perform supervised fine-tuning (SFT) of the LLM, followed by multi-turn reinforcement learning (RL) to activate latent world knowledge.

Result: Empirically, SCOUT enables Qwen2.5-3B-Instruct to achieve an average score of 0.86, outperforming Gemini-2.5-Pro (0.60) and reducing GPU hours by about 60%.

Conclusion: Decoupling exploration from exploitation via sub-scale scouting effectively bootstraps large language models for unseen, nonlinguistic tasks, achieving higher performance with substantially lower compute.

Abstract: While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight "scouts" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.

</details>


### [106] [Zero-Shot Statistical Downscaling via Diffusion Posterior Sampling](https://arxiv.org/abs/2601.21760)
*Ruian Tie,Wenbo Xiong,Zhengyu Shi,Xinyu Su,Chenyu jiang,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: Zero-Shot Statistical Downscaling (ZSSD) 提出一个无需配对训练数据的统计降尺度框架，结合物理一致的气候先验与统一坐标引导，在跨GCM的泛化能力、极端事件重建和高百分位误差方面优于现有零 shot baselines。


<details>
  <summary>Details</summary>
Motivation: 克服传统有监督降尺度对配对数据的依赖以及与再分析数据之间的领域差距，同时缓解当前零-shot 方法在大尺度缩放因子下的物理不一致和梯度消失问题。

Method: ZSSD 基于从再分析数据学习的物理一致气候先验，结合地理边界与时间信息进行条件化以确保物理有效性；引入统一坐标引导以提升对不同GCM的鲁棒推断，解决vanishing gradient并保持对大尺度场的一致性。

Result: 在 99th 百分位误差上显著优于现有零-shot 基线；能够跨异质GCM 重建复杂天气事件如热带气旋。

Conclusion: ZSSD 提供一种无配对训练的数据高鲁棒零-shot 降尺度方法，通过物理先验与统一坐标引导实现跨GCM 泛化和极端事件重建的能力，具备广泛应用潜力并为后续方法改进指明方向。

Abstract: Conventional supervised climate downscaling struggles to generalize to Global Climate Models (GCMs) due to the lack of paired training data and inherent domain gaps relative to reanalysis. Meanwhile, current zero-shot methods suffer from physical inconsistencies and vanishing gradient issues under large scaling factors. We propose Zero-Shot Statistical Downscaling (ZSSD), a zero-shot framework that performs statistical downscaling without paired data during training. ZSSD leverages a Physics-Consistent Climate Prior learned from reanalysis data, conditioned on geophysical boundaries and temporal information to enforce physical validity. Furthermore, to enable robust inference across varying GCMs, we introduce Unified Coordinate Guidance. This strategy addresses the vanishing gradient problem in vanilla DPS and ensures consistency with large-scale fields. Results show that ZSSD significantly outperforms existing zero-shot baselines in 99th percentile errors and successfully reconstructs complex weather events, such as tropical cyclones, across heterogeneous GCMs.

</details>


### [107] [Abstract Concept Modelling in Conceptual Spaces: A Study on Chess Strategies](https://arxiv.org/abs/2601.21771)
*Hadi Banaee,Stephanie Lowry*

Main category: cs.AI

TL;DR: 提出一个基于概念空间的时间维度抽象概念建模框架，并以国际象棋为例验证，利用轨迹识别策略并支持双视角理解，展示了将概念空间扩展到时序实现的目标导向概念的可行性及其在序列决策中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 将概念空间理论扩展到随时间展开、具备目标导向的抽象概念建模，解决策略识别中的可解释性、多方解释差异以及序列决策场景的学习与演化问题。

Method: 将策略概念表示为跨可解释质量维度的几何区域，棋局作为轨迹在这些维度上的移动来实现对策略的识别；实现双视角建模以捕捉不同玩家对相同情境的解读；通过与专家评述的对齐来验证轨迹模式的有效性。

Result: 实现了轨迹基于概念识别的可行性，移动模式与专家评述具备对齐，证明了将概念空间理论延展到 temporally realized 的概念的可行性。

Conclusion: 将概念空间理论扩展至时序实现的、面向目标的概念，奠定了在序列决策和持续学习/知识演化中的应用基础，并为更广泛的实证场景提供了方法论基础。

Abstract: We present a conceptual space framework for modelling abstract concepts that unfold over time, demonstrated through a chess-based proof-of-concept. Strategy concepts, such as attack or sacrifice, are represented as geometric regions across interpretable quality dimensions, with chess games instantiated and analysed as trajectories whose directional movement toward regions enables recognition of intended strategies. This approach also supports dual-perspective modelling, capturing how players interpret identical situations differently. Our implementation demonstrates the feasibility of trajectory-based concept recognition, with movement patterns aligning with expert commentary. This work explores extending the conceptual spaces theory to temporally realised, goal-directed concepts. The approach establishes a foundation for broader applications involving sequential decision-making and supports integration with knowledge evolution mechanisms for learning and refining abstract concepts over time.

</details>


### [108] [BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics](https://arxiv.org/abs/2601.21800)
*Dionizije Fa,Marko Čuljak,Bruno Pandža,Mateo Čupić*

Main category: cs.AI

TL;DR: BioAgent Bench is a benchmark and evaluation suite for AI bioinformatics agents, assessing end-to-end task performance and robustness to perturbations across frontier models using an LLM-based grader; findings show capability to complete pipelines but vulnerability to input perturbations and prompt bloat; privacy constraints may favor open-weight models despite lower completion rates; dataset released publicly.


<details>
  <summary>Details</summary>
Motivation: There is a need for standardized benchmarks to evaluate AI agents performing bioinformatics workflows, with a focus on end-to-end task completion, step-level reasoning, robustness under perturbations, and privacy considerations between closed-source and open-weight models.

Method: Construct BioAgent Bench with curated end-to-end bioinformatics tasks (e.g., RNA-seq, variant calling, metagenomics) and prompts that specify concrete output artifacts; include stress tests with controlled perturbations; evaluate frontier closed-source and open-weight models across multiple agent harnesses; use an LLM-based grader to score pipeline progress and artifact validity.

Result: Frontier agents can complete multi-step pipelines without elaborate scaffolding and often produce the requested final artifacts; robustness tests reveal failure modes under perturbations (corrupted inputs, decoy files, prompt bloat); correct high-level pipeline construction does not guarantee reliable step-level reasoning; closed-source models may be unsuitable under strict privacy constraints, whereas open-weight models may be preferable despite lower completion rates.

Conclusion: The dataset and evaluation suite are released publicly, enabling systematic benchmarking of AI bioinformatics agents and informing model selection under privacy constraints and robustness considerations.

Abstract: This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.

</details>


### [109] [A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition](https://arxiv.org/abs/2601.21802)
*Hoang Khang Phan,Quang Vinh Dang,Noriyo Colley,Christina Garcia,Nhat Tan Le*

Main category: cs.AI

TL;DR: 提出基于大语言模型（LLM）的统一视频行为识别与反馈框架，用于端气管抽吸（ES）培训；对比传统机器学习/深度学习基线，准确率与F1提升约15–20%；并包含异常检测与可解释AI的自动化反馈模块。


<details>
  <summary>Details</summary>
Motivation: 在居家护理与教育场景中，ES培训需高强度监督，现有自动识别与反馈系统不足以支撑大规模、分布式培训，亟需可扩展、可解释、以数据驱动的培训工具以提升安全性与教育效率。

Method: 以LLM为核心推理模块，整合时空视频识别能力与可解释决策分析；LLM输出自然语言形式的训练反馈；将对比基线（传统ML与深度学习）并评估性能；另设异常检测与可解释AI驱动的学生支持模块。

Result: 实验结果显示，基于LLM的框架在准确率与F1分数上分别获得约15–20%的提升；同时实现对关键动作的可解释评估与自然语言的反馈生成。

Conclusion: 该框架具备良好的扩展性与可解释性，数据驱动，能够提升护理教育的培训效率与患者安全，并为将来在其他临床操作培训中的应用奠定基础。

Abstract: Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.

</details>


### [110] [Looking Beyond Accuracy: A Holistic Benchmark of ECG Foundation Models](https://arxiv.org/abs/2601.21830)
*Francesca Filice,Edoardo De Rose,Simone Bartucci,Francesco Calimeri,Simona Perri*

Main category: cs.AI

TL;DR: 提出一个综合的ECG专家型基础模型基准框架，结合性能评估与表示层分析（SHAP/UMAP），以在多数据场景和跨洲数据条件下评估嵌入的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准多聚焦下游性能，缺乏对ECG基础模型生成的嵌入在误差敏感的医疗领域中的泛化与可解释性评估。

Method: 提出兼顾性能与表示分析的基准方法，结合SHAP与UMAP对嵌入表示进行分析；评估多种经过前沿技术预训练的ECG专家型基础模型，覆盖跨大陆数据集和数据稀缺情境。

Result: 基准 protocol 能提供关于ECG专家型基础模型嵌入模式的丰富洞见，揭示表示结构与泛化能力的深层信息。

Conclusion: 该框架有助于在不同数据可用性条件下，对ECG领域的基础模型表示进行更深入理解并推动其更负责任的应用。

Abstract: The electrocardiogram (ECG) is a cost-effective, highly accessible and widely employed diagnostic tool. With the advent of Foundation Models (FMs), the field of AI-assisted ECG interpretation has begun to evolve, as they enable model reuse across different tasks by relying on embeddings. However, to responsibly employ FMs, it is crucial to rigorously assess to which extent the embeddings they produce are generalizable, particularly in error-sensitive domains such as healthcare. Although prior works have already addressed the problem of benchmarking ECG-expert FMs, they focus predominantly on the evaluation of downstream performance. To fill this gap, this study aims to find an in-depth, comprehensive benchmarking framework for FMs, with a specific focus on ECG-expert ones. To this aim, we introduce a benchmark methodology that complements performance-based evaluation with representation-level analysis, leveraging SHAP and UMAP techniques. Furthermore, we rely on the methodology for carrying out an extensive evaluation of several ECG-expert FMs pretrained via state-of-the-art techniques over different cross-continental datasets and data availability settings; this includes ones featuring data scarcity, a fairly common situation in real-world medical scenarios. Experimental results show that our benchmarking protocol provides a rich insight of ECG-expert FMs' embedded patterns, enabling a deeper understanding of their representational structure and generalizability.

</details>


### [111] [Bridging Forecast Accuracy and Inventory KPIs: A Simulation-Based Software Framework](https://arxiv.org/abs/2601.21844)
*So Fukuhara,Abdallah Alabdallah,Nuwan Gunasekara,Slawomir Nowaczyk*

Main category: cs.AI

TL;DR: 研究提出一个以决策为导向的仿真框架，将需求预测与备件库存管理闭环连接，揭示预测准确性与运营绩效之间的非线性关系，并提供跨情景的模型选择指南。


<details>
  <summary>Details</summary>
Motivation: 当前对预测模型的评估多聚焦统计准确性（如MAE/RMSE），但对运营指标（总成本、服务水平等）的影响缺乏系统化理解，且在汽车后市场等对需求高度间歇性的场景下尤为显著。

Method: 提出一个闭环仿真框架：1) 面向备件需求特征的合成需求生成器；2) 可容纳任意预测模型的灵活预测模块；3) 基于预测结果的库存控制仿真器，用以计算运营KPI。通过广泛的仿真情景评估模型在KPIs上的影响，而不仅仅是预测误差。

Result: 发现对传统准确度指标的提升并不总是带来运营绩效的提升，具有相近统计误差的模型也可能产生显著不同的成本-服务权衡。通过分析误差的具体维度与库存结果之间的关系，给出模型选择的实用指导。

Conclusion: 该框架将需求预测与库存管理的关系落地，推动评估从纯预测准确性转向对运营相关性的考量，适用于汽车后市场及相关领域。

Abstract: Efficient management of spare parts inventory is crucial in the automotive aftermarket, where demand is highly intermittent and uncertainty drives substantial cost and service risks. Forecasting is therefore central, but the quality of a forecasting model should be judged not by statistical accuracy (e.g., MAE, RMSE, IAE) but rather by its impact on key operational performance indicators (KPIs), such as total cost and service level. Yet most existing work evaluates models exclusively using accuracy metrics, and the relationship between these metrics and operational KPIs remains poorly understood. To address this gap, we propose a decision-centric simulation software framework that enables systematic evaluation of forecasting model in realistic inventory management setting. The framework comprises: (i) a synthetic demand generator tailored to spare-parts demand characteristics, (ii) a flexible forecasting module that can host arbitrary predictive models, and (iii) an inventory control simulator that consumes the forecasts and computes operational KPIs. This closed-loop setup enables researchers to evaluate models not only in terms of statistical error but also in terms of their downstream implications for inventory decisions. Using a wide range of simulation scenarios, we show that improvements in conventional accuracy metrics do not necessarily translate into better operational performance, and that models with similar statistical error profiles can induce markedly different cost-service trade-offs. We analyze these discrepancies to characterize how specific aspects of forecast performance affect inventory outcomes and derive guidance for model selection. Overall, the framework operationalizes the link between demand forecasting and inventory management, shifting evaluation from purely predictive accuracy toward operational relevance in the automotive aftermarket and related domains.

</details>


### [112] [KnowBias: Mitigating Social Bias in LLMs via Know-Bias Neuron Enhancement](https://arxiv.org/abs/2601.21864)
*Jinhao Pan,Chahat Raj,Anjishnu Mukherjee,Sina Mansouri,Bowen Wei,Shloka Yada,Ziwei Zhu*

Main category: cs.AI

TL;DR: 提出 KnowBias，通过在推理时强化编码偏见知识的神经元来减轻偏见，无需重训，数据高效，兼容多偏见类型。


<details>
  <summary>Details</summary>
Motivation: 现有去偏见方法多采用抑制策略，易脆弱、泛化差、数据低效且可能损害通用能力；需要一个可泛化、数据友好且不损害性能的去偏见方案。

Method: 通过基于归因分析的少量偏见相关问题，识别编码偏见知识的神经元，经过选择性强化，在推理阶段应用，无需重训练。

Result: 在多种基准和模型上实现了最先进的去偏见性能，同时对下游任务性能的降幅很小；数据与实现都非常高效，且可扩展性良好。

Conclusion: 通过强化偏见知识相关神经元，KnowBias 提供一个稳健、泛化良好且数据高效的去偏见方法，适用于多偏见类型和群体。数据和代码公开。

Abstract: Large language models (LLMs) exhibit social biases that reinforce harmful stereotypes, limiting their safe deployment. Most existing debiasing methods adopt a suppressive paradigm by modifying parameters, prompts, or neurons associated with biased behavior; however, such approaches are often brittle, weakly generalizable, data-inefficient, and prone to degrading general capability. We propose \textbf{KnowBias}, a lightweight and conceptually distinct framework that mitigates bias by strengthening, rather than suppressing, neurons encoding bias-knowledge. KnowBias identifies neurons encoding bias knowledge using a small set of bias-knowledge questions via attribution-based analysis, and selectively enhances them at inference time. This design enables strong debiasing while preserving general capabilities, generalizes across bias types and demographics, and is highly data efficient, requiring only a handful of simple yes/no questions and no retraining. Experiments across multiple benchmarks and LLMs demonstrate consistent state-of-the-art debiasing performance with minimal utility degradation. Data and code are available at https://github.com/JP-25/KnowBias.

</details>


### [113] [Making Models Unmergeable via Scaling-Sensitive Loss Landscape](https://arxiv.org/abs/2601.21898)
*Minwoo Jang,Hoyoung Kim,Jabin Koo,Jungseul Ok*

Main category: cs.AI

TL;DR: Trap^2 is an architecture-agnostic protection framework that encodes protection into fine-tuning updates to deter unauthorized model merging by weight rescaling, compatible with adapters or full models.


<details>
  <summary>Details</summary>
Motivation: Addresses a governance gap in model hubs where downstream users can merge released weights to bypass safety alignment or licensing terms; existing defenses are post-hoc and architecture-specific.

Method: Incorporates protection into the optimization update during fine-tuning and uses weight re-scaling as a simple proxy for merges; the scheme is architecture-agnostic and preserves standalone effectiveness while degrading under common re-scaling that occurs during merging.

Result: Protection remains effective against unauthorized merging because re-scaling during merging degrades the released weights, while they stay usable on their own.

Conclusion: Trap^2 provides a simple, architecture-agnostic approach to close the governance gap in model hubs by tying protection to the fine-tuning update and exploiting weight re-scaling during merges.

Abstract: The rise of model hubs has made it easier to access reusable model components, making model merging a practical tool for combining capabilities. Yet, this modularity also creates a \emph{governance gap}: downstream users can recompose released weights into unauthorized mixtures that bypass safety alignment or licensing terms. Because existing defenses are largely post-hoc and architecture-specific, they provide inconsistent protection across diverse architectures and release formats in practice. To close this gap, we propose \textsc{Trap}$^{2}$, an architecture-agnostic protection framework that encodes protection into the update during fine-tuning, regardless of whether they are released as adapters or full models. Instead of relying on architecture-dependent approaches, \textsc{Trap}$^{2}$ uses weight re-scaling as a simple proxy for the merging process. It keeps released weights effective in standalone use, but degrades them under re-scaling that often arises in merging, undermining unauthorized merging.

</details>


### [114] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: 提出一个两阶段的记忆框架来改进后训练：先学习抽象策略（Chain-of-Meta-Thought, CoMT），再用置信度校准的强化学习（Confidence-Calibrated RL, CCRL）来对具体任务进行适应。


<details>
  <summary>Details</summary>
Motivation: 现有的SFT+RL方法把完整的推理轨迹作为基本单位，难以与人类的分层认知过程对齐，导致难以泛化且效率低下。研究旨在通过分离抽象策略学习与具体执行来提升泛化与效率。

Method: 1) CoMT：以监督学习专注于抽象推理模式，不涉及具体执行，从而获取可泛化的策略。 2) CCRL：在中间步骤上通过带置信度的奖励进行强化学习，进行任务适应，抑制过度自信导致的连锁错误。

Result: 在四个模型、八个基准上，原始方法的平均提升分别为2.19%（同分布）和4.63%（跨分布），训练时间下降65–70%，代币消耗下降50%。

Conclusion: 将后训练与人类认知原则对齐可实现更好的泛化与更高的训练效率。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [115] [Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities](https://arxiv.org/abs/2601.21937)
*Shuangshuang Ying,Zheyu Wang,Yunjian Peng,Jin Chen,Yuhao Wu,Hongbin Lin,Dingyu He,Siyi Liu,Gengchen Yu,YinZhu Piao,Yuchen Wu,Xin Gui,Zhongyuan Peng,Xin Li,Xeron Du,Libo Qin,YiXin Cao,Ge Zhang*

Main category: cs.AI

TL;DR: 提出 DeR2 深度研究沙箱，用以隔离文档证据检索与推理，提供四个证据访问 regimes，进行可重复的实验并揭示大语言模型在基于文献的推理任务上的头部空间和错误模式。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在新颖科学信息推理中的能力，现有端到端 RAG 混杂了检索、工具链和参数记忆，难以分离推理 vs 检索、难以复现。

Method: 构建 DeR2，四种 regime：Instruction-only、Concepts（金概念但无文献）、Related-only（仅相关文献）、Full-set（相关文献+顶级相关干扰项）；两阶段验证防止参数泄漏；提供冻结的文献库（2023-2025理论论文）、专家标注概念、验证了的推理路径；在多种前沿模型上评估。

Result: 发现显著差异与巨大改进空间：有些模型在 Full-set 下表现比 Instruction-only 更差，表现出模式切换脆弱性；另有模型出现结构性概念误用：能正确命名概念但无法将它们作为程序执行。

Conclusion: DeR2 能实现可解释的 regime 差距、错误归因，区分检索丢失 vs 推理丢失，促进可重复性和严格评估，揭示当前模型在基于文献推理上的局限与改进方向。

Abstract: Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.

</details>


### [116] [The Energy Impact of Domain Model Design in Classical Planning](https://arxiv.org/abs/2601.21967)
*Ilche Georgievski,Serhat Tekin,Marco Aiello*

Main category: cs.AI

TL;DR: 研究聚焦域模型对经典规划器能耗的影响，提出域模型配置框架，在5个基准域和5个规划器上用32个域变体进行能耗与运行时间的对比，结果显示域级修改可显著改变能耗，且能耗不总与运行时间相关。


<details>
  <summary>Details</summary>
Motivation: 将绿色AI理念应用于自动规划，指出能耗作为关键性能指标尚未被充分研究，域模型与算法的分离为能耗分析提供系统性机会。

Method: 构建域模型配置框架，系统性变更域模型特征（元素排序、动作元数、死端状态），在五个基准域和五个前沿规划器上生成32个域变体并比较其能耗与运行时间。

Result: 不同域模型特征可引起可衡量的能耗差异；能耗与运行时间之间存在不稳定的关系，且不同规划器对同一域变体的能耗敏感性不同。

Conclusion: 域模型设计是能耗优化的重要维度，提出通过领域级设计实现能效提升的可能性，未来工作应扩展域、测量方法及能耗–性能权衡分析。

Abstract: AI research has traditionally prioritised algorithmic performance, such as optimising accuracy in machine learning or runtime in automated planning. The emerging paradigm of Green AI challenges this by recognising energy consumption as a critical performance dimension. Despite the high computational demands of automated planning, its energy efficiency has received little attention. This gap is particularly salient given the modular planning structure, in which domain models are specified independently of algorithms. On the other hand, this separation also enables systematic analysis of energy usage through domain model design. We empirically investigate how domain model characteristics affect the energy consumption of classical planners. We introduce a domain model configuration framework that enables controlled variation of features, such as element ordering, action arity, and dead-end states. Using five benchmark domains and five state-of-the-art planners, we analyse energy and runtime impacts across 32 domain variants per benchmark. Results demonstrate that domain-level modifications produce measurable energy differences across planners, with energy consumption not always correlating with runtime.

</details>


### [117] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 提出基于多智能体 actor-critic 的去中心化大模型协作（MAAC），比较中心化评判器（CoLLM-CC）与去中心化评判器（CoLLM-DC）的两种方法，并在写作、编码、游戏等领域验证。结果表明在短期密集奖励场景下，蒙特卡罗方法和 CoLLM-DC 与 CoLLM-CC 相当；在长期、稀疏奖励任务上，CoLLM-CC 优势明显，MC 需要更多样本，CoLLM-DC 收敛困难。代码公开。


<details>
  <summary>Details</summary>
Motivation: 解决大模型协作中的高估计方差问题及对去中心化并行推理的需求，利用 Actor-Critic 的稳定性提升 MARL 的样本效率与稳定性，避免依赖预定义执行协议。

Method: 提出两种 MAAC 框架：CoLLM-CC（集中 critic）和 CoLLM-DC（去中心化 critics）。分析它们在不同任务设定下的优劣，结合写作、编码、游戏三类任务进行对比实验，比较蒙特卡罗取样的影响。提供公开代码。

Result: 在短 horizon 与密集奖励设定下，MC 和 CoLLM-DC 的表现可与 CoLLM-CC 相当；在长 horizon 或稀疏奖励任务中，CoLLM-CC 优势明显；MC 需要大量样本；CoLLM-DC 在某些任务上难以收敛。

Conclusion: 去中心化 critics 在一定场景具备与中心化 critic 相近的性能，尤其是在短期密集奖励任务，但在长期、稀疏奖励任务仍劣于中心化评判；需要更高效的样本利用与稳定训练机制。代码可获取。

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [118] [Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models](https://arxiv.org/abs/2601.21975)
*Pranav Mahajan,Ihor Kendiukhov,Syed Hussain,Lydia Nottingham*

Main category: cs.AI

TL;DR: SvR相关性高度受 elicitation 协议影响；引入中立/弃选能改善 stated-与 forced-choice 的相关性，但在 revealed 端弃选会降低相关性，系统提示引导未显著提升 SvR；结论是偏好相关性高度依赖协议且需处理不确定偏好。


<details>
  <summary>Details</summary>
Motivation: 现有评估多使用二元强制选择，可能将模型的真实偏好与 elicitation 制约混淆；需系统研究不同 elicitation 协议对 SvR 的影响，覆盖多种语言模型。

Method: 在24种语言模型上系统比较多种 elicitation 协议。允许 stated preference 中的中立与弃选以排除弱信号；比较 volunteer 的 stated 与 forced-choice 的 Spearman相关性(ρ)。在 revealed preferences 中进一步引入弃选，观察ρ的变化；在 AIRiskDilemmas 上测试系统提示引导对 SvR 的影响。

Result: 允许中立/弃选的 stated elicitation 能显著提高 ρ；在 revealed 端增加弃选导致 ρ 附近为零或负值，原因是中立率过高；系统提示引导在 AIRiskDilemmas 上未能可靠提升 SvR相关性。

Conclusion: SvR相关性高度依赖评估协议，偏好elicitation需考虑不确定偏好，未来需发展鲁棒的评估方法来处理 indeterminate preferences。

Abstract: Recent work identifies a stated-revealed (SvR) preference gap in language models (LMs): a mismatch between the values models endorse and the choices they make in context. Existing evaluations rely heavily on binary forced-choice prompting, which entangles genuine preferences with artifacts of the elicitation protocol. We systematically study how elicitation protocols affect SvR correlation across 24 LMs. Allowing neutrality and abstention during stated preference elicitation allows us to exclude weak signals, substantially improving Spearman's rank correlation ($ρ$) between volunteered stated preferences and forced-choice revealed preferences. However, further allowing abstention in revealed preferences drives $ρ$ to near-zero or negative values due to high neutrality rates. Finally, we find that system prompt steering using stated preferences during revealed preference elicitation does not reliably improve SvR correlation on AIRiskDilemmas. Together, our results show that SvR correlation is highly protocol-dependent and that preference elicitation requires methods that account for indeterminate preferences.

</details>


### [119] [VERSA: Verified Event Data Format for Reliable Soccer Analytics](https://arxiv.org/abs/2601.21981)
*Geonhee Jo,Mingu Kang,Kangmin Lee,Minho Lee,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: 提出一个用于足球赛事事件数据的验证框架VERSA，通过状态机验证事件序列并自动纠错，提升跨来源数据一致性与下游任务VAEP的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有赛事事件流存在数据质量问题，导致逻辑不一致（如事件顺序错乱、缺失），影响分析准确性；需要一个在来源间实现一致性、可自动纠错的验证框架，尤其在足球领域。

Method: 以状态转换模型定义有效的事件序列，建立异常检测与纠正机制；使用Bepro提供的K联赛（2024赛季）数据进行评估，量化18.81%的事件存在逻辑不一致；验证框架提升跨来源一致性并提高VAEP下游任务性能。

Result: 经VERSA处理，跨来源数据一致性显著提升；纠错后的数据将VAEP等分析任务的鲁棒性和性能提升；18.81%原始事件存在不一致被发现并纠正。

Conclusion: 基于状态转移的事件数据验证是提升足球领域数据驱动分析可靠性的有效途径，具有提高多源数据融合质量和下游分析可靠性的潜力。

Abstract: Event stream data is a critical resource for fine-grained analysis across various domains, including financial transactions, system operations, and sports. In sports, it is actively used for fine-grained analyses such as quantifying player contributions and identifying tactical patterns. However, the reliability of these models is fundamentally limited by inherent data quality issues that cause logical inconsistencies (e.g., incorrect event ordering or missing events). To this end, this study proposes VERSA (Verified Event Data Format for Reliable Soccer Analytics), a systematic verification framework that ensures the integrity of event stream data within the soccer domain. VERSA is based on a state-transition model that defines valid event sequences, thereby enabling the automatic detection and correction of anomalous patterns within the event stream data. Notably, our examination of event data from the K League 1 (2024 season), provided by Bepro, detected that 18.81% of all recorded events exhibited logical inconsistencies. Addressing such integrity issues, our experiments demonstrate that VERSA significantly enhances cross-provider consistency, ensuring stable and unified data representation across heterogeneous sources. Furthermore, we demonstrate that data refined by VERSA significantly improves the robustness and performance of a downstream task called VAEP, which evaluates player contributions. These results highlight that the verification process is highly effective in increasing the reliability of data-driven analysis.

</details>


### [120] [Defining Operational Conditions for Safety-Critical AI-Based Systems from Data](https://arxiv.org/abs/2601.22118)
*Johann Christensen,Elena Hoemann,Frank Köster,Sven Hallerbach*

Main category: cs.AI

TL;DR: A data-driven, post hoc method to define the Operational Design Domain (ODD) using a multi-dimensional kernel-based representation, enabling certification of safety-critical AI via a Safe-by-Design approach. Validated with Monte Carlo and an aviation collision-avoidance use case; shows conditions under which data-driven ODD equals the original hidden ODD.


<details>
  <summary>Details</summary>
Motivation: ODD is essential for certification but is hard to specify in complex real-world environments; traditional methods rely on expert knowledge and standards, which may be incomplete for data-driven AI systems.

Method: Introduce a posteriori, kernel-based, multi-dimensional representation to define ODD from existing data; formalize equality criteria between ODDs; validate via Monte Carlo simulations and a real-world aviation use case for a collision-avoidance system.

Result: Demonstrates that the data-driven ODD can be equivalent to the underlying hidden ODD under defined conditions; enables certification of data-driven safety-critical AI systems using Safe-by-Design ODD.

Conclusion: A novel kernel-based ODD framework provides a path toward certification of data-driven safety-critical AI by rigorously defining and validating the operating domain post hoc.

Abstract: Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [121] [SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation](https://arxiv.org/abs/2601.21105)
*Joyce Zhou,Weijie Zhou,Doug Turnbull,Thorsten Joachims*

Main category: cs.IR

TL;DR: 引入SteerEval框架，用于在自然语言用户画像的推荐系统中评估可操控性，覆盖从电影类型到内容警告等多样干预，并比较不同模型与干预对操控效果的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的 steerability 评估多局限于知名属性（如电影类型），未能捕捉更丰富、细粒度的用户控制需求，亟需扩展到更丰富的干预形式以实现更可控的推荐。

Method: 提出SteerEval评估框架；通过从 genres 到 content-warning 的多样化干预测试，评估一系列预训练自然语言推荐模型的可操控性；比较不同用户画像干预与推荐干预对操控效果的影响；在较小众主题上探索潜力与限制，并给出实际设计建议。

Result: 框架可量化更细粒度的可操控性；不同干预对不同模型的效果存在差异，较小众主题的操控潜力与限制更为明显；对如何在实践中权衡可操控性与推荐质量提供了实证洞见。

Conclusion: 扩展的SteerEval框架为设计更具可控性的推荐系统提供评估工具，未来工作应完善干预集合、评估指标及用户体验方面的设计。

Abstract: Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.

</details>


### [122] [A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning](https://arxiv.org/abs/2601.21162)
*Jiate Liu,Zebin Chen,Shaobo Qiao,Mingchen Ju,Danting Zhang,Bocheng Han,Shuyue Yu,Xin Shu,Jingling Wu,Dong Wen,Xin Cao,Guanfeng Liu,Zhengyi Yang*

Main category: cs.IR

TL;DR: A2RAG introduces adaptive control and agentic retrieval to Graph-RAG, enabling cost-aware and reliable multihop reasoning; it improves Recall@2 and reduces token usage and latency.


<details>
  <summary>Details</summary>
Motivation: Address two bottlenecks in Graph-RAG: (i) mixed-difficulty workloads where uniform retrieval wastes cost or misses hard cases, and (ii) extraction loss where graph abstractions omit fine-grained qualifiers present in source text.

Method: Develop A2RAG with (a) an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when needed, and (b) an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to mitigate extraction loss and incomplete graphs.

Result: On HotpotQA and 2WikiMultiHopQA, A2RAG yields +9.9 and +11.8 absolute gains in Recall@2, while reducing token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.

Conclusion: A2RAG achieves cost-aware and reliable reasoning in Graph-RAG by coupling adaptive verification with escalatory retrieval and provenance-aware mapping, yielding substantial efficiency and performance gains on multihop QA benchmarks.

Abstract: Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.

</details>


### [123] [Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance](https://arxiv.org/abs/2601.21611)
*Baopu Qiu,Hao Chen,Yuanrong Wu,Changtong Zan,Chao Wei,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 提出一种面向电商检索的多视角链式推理框架：MPCoT 生成多元化推理、SFT+Direct Preference Optimization 构建鲁棒推理器、LRKD 实现高效推理蒸馏；离线和线上评估均显示显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统相关性模型难以充分捕捉电商场景中的多元因素，如用户意图、属性级匹配和业务规则；单一视角的链式推理往往不足以覆盖场景的多样性；尽管 CoT 能提升推理能力，但高延迟需要蒸馏，而现有蒸馏往往丢失 CoT 结构的推理价值。

Method: 教师模型采用多视角 CoT (MPCoT) 生成多样化的推理理由；将监督微调（SFT）与直接偏好优化（DPO）相结合，构建更鲁棒的推理器。为蒸馏引入 latent reasoning knowledge distillation (LRKD)，在学生模型中加入轻量级的推理潜在提取器，以实现低延迟的推理内化。

Result: 在离线实验与面向日均数千万用户的在线 A/B 测试中，方法均显示显著提升，商业表现和用户体验均获得明确收益。

Conclusion: 该框架通过在优化流水线中充分利用 CoT 语义，提升多视角推理能力并实现高效部署，具备良好的商业可行性与用户体验提升潜力。

Abstract: Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.

</details>


### [124] [Influence Guided Sampling for Domain Adaptation of Text Retrievers](https://arxiv.org/abs/2601.21759)
*Meet Doshi,Vishwajeet Kumar,Yulong Li,Jaydeep Sen*

Main category: cs.IR

TL;DR: Inf-DDS is an RL-driven training-data sampling framework for open-domain dense retrievers that adaptively reweights datasets using influence-based rewards to maximize dev-set performance, yielding strong gains and lower GPU cost compared with gradient-based sampling.


<details>
  <summary>Details</summary>
Motivation: Training data sampling profoundly affects embedding-based retrievers; existing approaches (uniform, population-based, or expert-guided) are suboptimal and often computationally expensive. There is a need for a lightweight, automatic method to optimize sampling for diverse corpora/tasks.

Method: Inf-DDS constructs a reinforcement learning policy that reweights training datasets via influence-based reward signals. The policy is iteratively refined to prioritize datasets that improve performance on a target development set, achieving a more efficient and effective sampling regime.

Result: Empirical evaluation across text retrieval tasks shows improved retrieval performance and better adaptation than gradient-based sampling, with 1.5x–4x reductions in GPU compute. Reported gains include +5.03 absolute NDCG@10 on multilingual bge-m3 and +0.94 NDCG@10 on all-MiniLM-L6-v2 starting from expert weights.

Conclusion: Adaptive RL-driven sampling can substantially improve open-domain dense retrieval performance and efficiency, offering a practical alternative to existing sampling strategies for diverse corpora and tasks.

Abstract: General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.

</details>


### [125] [OneMall: One Model, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce](https://arxiv.org/abs/2601.21770)
*Kun Zhang,Jingming Zhang,Wei Cheng,Yansong Cheng,Jiaqi Zhang,Hao Lu,Xu Zhang,Haixiang Gan,Jiangxia Cao,Tenglong Wang,Ximing Zhang,Boyang Xia,Kuo Cai,Shiyao Wang,Hongjian Dou,Jinkai Yu,Mingxing Wen,Qiang Luo,Dongxu Liang,Chenyi Lei,Jun Wang,Runan Liu,Zhaojie Liu,Ruiming Tang,Tingting Gao,Shaoguo Liu,Yuqing Ding,Hui Kong,Han Li,Guorui Zhou,Wenwu Ou,Kun Gai*

Main category: cs.IR

TL;DR: OneMall is a unified end-to-end generative recommendation framework for Kuaishou's e-commerce, unifying product-card, short-video, and live-streaming with a semantic tokenizer, Transformer-based backbone (Query-Former, Cross-Attention, Sparse MoE), and RL-based retrieval-ranking training; achieves significant cross-scenario gains and is deployed to 400M DAU.


<details>
  <summary>Details</summary>
Motivation: Address heterogeneity of item distribution across e-commerce modalities and close the loop between retrieval and ranking by adopting an LLM-like training pipeline to leverage cross-scenario semantics and scalable generation.

Method: Three components: 1) E-commerce Semantic Tokenizer capturing real-world semantics and business-specific item relations across scenarios; 2) Transformer-based architecture with long sequence compression (Query-Former), multi-behavior sequence fusion via Cross-Attention, and scalable autoregressive generation via Sparse MoE; 3) Reinforcement Learning pipeline linking retrieval and ranking, using the ranking model as a reward signal for end-to-end policy optimization.

Result: Empirical results show improvements of +13.01% GMV for product-card, +15.32% orders for Short-Video, and +2.78% orders for Live-Streaming; deployed at Kuaishou, serving over 400 million daily active users.

Conclusion: Demonstrates that a unified generative recommendation framework can deliver consistent gains across diverse e-commerce scenarios and scale to industrial deployment.

Abstract: In the wave of generative recommendation, we present OneMall, an end-to-end generative recommendation framework tailored for e-commerce services at Kuaishou. Our OneMall systematically unifies the e-commerce's multiple item distribution scenarios, such as Product-card, short-video and live-streaming. Specifically, it comprises three key components, aligning the entire model training pipeline to the LLM's pre-training/post-training: (1) E-commerce Semantic Tokenizer: we provide a tokenizer solution that captures both real-world semantics and business-specific item relations across different scenarios; (2) Transformer-based Architecture: we largely utilize Transformer as our model backbone, e.g., employing Query-Former for long sequence compression, Cross-Attention for multi-behavior sequence fusion, and Sparse MoE for scalable auto-regressive generation; (3) Reinforcement Learning Pipeline: we further connect retrieval and ranking models via RL, enabling the ranking model to serve as a reward signal for end-to-end policy retrieval model optimization. Extensive experiments demonstrate that OneMall achieves consistent improvements across all e-commerce scenarios: +13.01\% GMV in product-card, +15.32\% Orders in Short-Video, and +2.78\% Orders in Live-Streaming. OneMall has been deployed, serving over 400 million daily active users at Kuaishou.

</details>


### [126] [The Double-Edged Sword of Knowledge Transfer: Diagnosing and Curing Fairness Pathologies in Cross-Domain Recommendation](https://arxiv.org/abs/2601.21805)
*Yuhan Zhao,Weixin Chen,Li Chen,Weike Pan*

Main category: cs.IR

TL;DR: 通过Cross-Domain Fairness Augmentation (CDFA)在跨领域推荐中减少群体层面的不公平性，同时保持或提升总体推荐性能；通过缓解跨域差异转移和再分配跨域信息收益实现公平性。


<details>
  <summary>Details</summary>
Motivation: 跨域推荐可能将源域的群体差异带入目标域，导致新域的不公平性扩大。本文从理论与实证角度揭示两类根源：1) 跨域差异传递（源域中已有的群体差异被系统性传递到目标域）；2) 跨域信息增益的不公平分配（不同群体获得的跨域收益不均）。

Method: 提出CDFA框架，包括两大组件：1) 自适应整合未标注数据，以均衡训练信号在不同群体中的信息性，减轻跨域差异传递；2) 以信息理论方法重新分配跨域信息增益，确保不同群体获得等量或等效的收益。

Result: 在多个数据集和基线的广泛实验中，CDFA显著降低跨域推荐中的不公平性，同时不牺牲整体推荐性能，甚至在某些情况下提升表现。

Conclusion: CDFA提供一种以信息理论为支撑的公平跨域推荐范式，能够在不同域间平衡准确性与群体公平性，具备良好推广性。

Abstract: Cross-domain recommendation (CDR) offers an effective strategy for improving recommendation quality in a target domain by leveraging auxiliary signals from source domains. Nonetheless, emerging evidence shows that CDR can inadvertently heighten group-level unfairness. In this work, we conduct a comprehensive theoretical and empirical analysis to uncover why these fairness issues arise. Specifically, we identify two key challenges: (i) Cross-Domain Disparity Transfer, wherein existing group-level disparities in the source domain are systematically propagated to the target domain; and (ii) Unfairness from Cross-Domain Information Gain, where the benefits derived from cross-domain knowledge are unevenly allocated among distinct groups. To address these two challenges, we propose a Cross-Domain Fairness Augmentation (CDFA) framework composed of two key components. Firstly, it mitigates cross-domain disparity transfer by adaptively integrating unlabeled data to equilibrate the informativeness of training signals across groups. Secondly, it redistributes cross-domain information gains via an information-theoretic approach to ensure equitable benefit allocation across groups. Extensive experiments on multiple datasets and baselines demonstrate that our framework significantly reduces unfairness in CDR without sacrificing overall recommendation performance, while even enhancing it.

</details>


### [127] [SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation](https://arxiv.org/abs/2601.21986)
*Yu Cui,Feng Liu,Zhaoxiang Wang,Changwang Zhang,Jun Wang,Can Wang,Jiawei Chen*

Main category: cs.IR

TL;DR: SpecTran introduces a spectral-aware transformer-based adapter for sequential recommendation that operates in the spectral domain, using a learnable spectral-position encoding to leverage the full spectrum of LLM-derived item embeddings. It addresses dimension-collapse in adapter-based methods and rigidity in SVD-based methods, outperforming baselines with an average gain of 9.17% across four datasets and three backbones.


<details>
  <summary>Details</summary>
Motivation: Current embedding transformation strategies for injecting item textual information into SR models suffer from (1) adapter-based dimension collapse, where information concentrates in a few dimensions, and (2) SVD-based rigidity, which only exploits a few principal spectral components and discards rich information in the rest. A method that utilizes the full spectral information while guiding attention to salient components is needed.

Method: SpecTran is a spectral-domain transformer-based adapter. It attends to the full spectrum of spectral components derived from high-dimensional textual item embeddings, selecting and aggregating informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias to guide attention toward salient spectral components and promote diversity across embedding dimensions.

Result: Empirically, SpecTran consistently outperforms strong baselines across four real-world datasets and three SR backbones, achieving an average improvement of 9.17%.

Conclusion: SpecTran effectively leverages the full spectral information of textual embeddings in SR, mitigating dimension-collapse and rigidity inherent to existing adapters and SVD-based methods, and offers a robust, transferable performance boost.

Abstract: Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.
  To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.

</details>


### [128] [LANCER: LLM Reranking for Nugget Coverage](https://arxiv.org/abs/2601.22008)
*Jia-Huei Ju,François G. Landry,Eugene Yang,Suzan Verberne,Andrew Yates*

Main category: cs.IR

TL;DR: LANCER 提出一种基于大语言模型的重排序方法，聚焦信息覆盖（nugget coverage）而非仅强调相关性，通过生成子问题来识别需要回答的要点并定位回答这些子问题的文档，从而对文档进行重排序以覆盖尽可能多的信息要点。


<details>
  <summary>Details</summary>
Motivation: 长文本生成（如自动化报告）需要广泛且结构化的信息覆盖，现有检索方法多聚焦相关性排名，难以确保覆盖所有关键信息要点。

Method: 使用LLM进行子问题生成，识别回答这些子问题的候选文档，并对文档进行重排序以最大化信息要点的覆盖（nugget coverage）；通过重排序策略实现对信息碎片的广泛覆盖。

Result: 在 nugget coverage 指标上优于其他基于LLM的重排序方法，α-nDCG 和信息覆盖度提升显著；限制分析显示子问题生成在性能提升中起关键作用（oracle分析支持此结论）。

Conclusion: 将信息覆盖作为优化目标的长文本RAG 重排序方法有效提升检索质量，且子问题生成是实现覆盖的重要组件。

Abstract: Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $α$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.

</details>


### [129] [Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature](https://arxiv.org/abs/2601.07533)
*Julian Schelb,Michael Wittweiler,Marie Revellio,Barbara Feichtinger,Andreas Spitz*

Main category: cs.IR

TL;DR: 提出 Loci Similes：一个拉丁互文性检测基准，包含约172k文本片段和545个专家验证的互文对照，将晚期古典作者与经典作者相连，用于检索与分类任务的基线评估，基于现代大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 互文关系有直引、隐喻和改写等形式；现有标准化基准和数据集匮乏，阻碍基于语义相似性的检测方法发展；需要可比较的评测集来推动领域进展。

Method: 构建并标注大规模拉丁互文性数据集，包含172k文本片段与545对专家验证的互文对照；建立基线任务（检索与分类）并以最先进的LLMs进行评测。

Result: 提供可用于检索和互文性分类的基线，验证LLMs在该任务上的潜力；数据集为未来研究提供标准化评测资源。

Conclusion: 填补标准化基准缺口，促进拉丁互文性研究和基于LLMs的检测方法的发展。

Abstract: Tracing connections between historical texts is an important part of intertextual research, enabling scholars to reconstruct the virtual library of a writer and identify the sources influencing their creative process. These intertextual links manifest in diverse forms, ranging from direct verbatim quotations to subtle allusions and paraphrases disguised by morphological variation. Language models offer a promising path forward due to their capability of capturing semantic similarity beyond lexical overlap. However, the development of new methods for this task is held back by the scarcity of standardized benchmarks and easy-to-use datasets. We address this gap by introducing Loci Similes, a benchmark for Latin intertextuality detection comprising of a curated dataset of ~172k text segments containing 545 expert-verified parallels linking Late Antique authors to a corpus of classical authors. Using this data, we establish baselines for retrieval and classification of intertextualities with state-of-the-art LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [130] [Rethinking LLM-Driven Heuristic Design: Generating Efficient and Specialized Solvers via Dynamics-Aware Optimization](https://arxiv.org/abs/2601.20868)
*Rongzheng Wang,Yihong Huang,Muquan Li,Jiakai Li,Di Liang,Bob Simons,Pei Ke,Shuang Liang,Ke Qin*

Main category: cs.LG

TL;DR: DASH uses convergence-aware metrics to co-optimize solver search and runtime scheduling, with Profiled Library Retrieval (PLR) to warm-start specialized solvers. It achieves ~3x runtime speedups and better solution quality, and reduces LLM adaptation costs by >90% across four combinatorial optimization problems.


<details>
  <summary>Details</summary>
Motivation: Existing LHD frameworks suffer from endpoint-only evaluation (final-quality ranking ignores convergence behavior) and high adaptation costs under distribution shifts; there is a need for convergence-aware evaluation and reusable, profile-based solvers.

Method: DASH jointly optimizes solver search mechanisms and runtime schedules guided by a convergence-aware metric. It also introduces Profiled Library Retrieval (PLR) to archive specialized solvers during evolution for cost-effective warm-starts on heterogeneous distributions.

Result: Empirical results show >3x improvement in runtime efficiency and superior solution quality compared to state-of-the-art baselines across problem scales. PLR enables profile-based warm starts, maintaining high accuracy under distribution shifts and reducing LLM adaptation costs by >90%.

Conclusion: DASH offers a robust, efficient framework for LLM-driven heuristic design by integrating convergence-aware evaluation and reusable solvers; PLR is key to reducing adaptation costs while sustaining performance across diverse distributions.

Abstract: Large Language Models (LLMs) have advanced the field of Combinatorial Optimization through automated heuristic generation. Instead of relying on manual design, this LLM-Driven Heuristic Design (LHD) process leverages LLMs to iteratively generate and refine solvers to achieve high performance. However, existing LHD frameworks face two critical limitations: (1) Endpoint-only evaluation, which ranks solvers solely by final quality, ignoring the convergence process and runtime efficiency; (2) High adaptation costs, where distribution shifts necessitate re-adaptation to generate specialized solvers for new instance groups. To address these issues, we propose Dynamics-Aware Solver Heuristics (DASH), a framework that co-optimizes solver search mechanisms and runtime schedules guided by a convergence-aware metric, thereby identifying efficient and high-performance solvers. Furthermore, to mitigate expensive re-adaptation, DASH incorporates Profiled Library Retrieval (PLR). PLR efficiently archives specialized solvers concurrently with the evolutionary process, enabling cost-effective warm-starts for heterogeneous distributions. Experiments on four combinatorial optimization problems demonstrate that DASH improves runtime efficiency by over 3 times, while surpassing the solution quality of state-of-the-art baselines across diverse problem scales. Furthermore, by enabling profile-based warm starts, DASH maintains superior accuracy under different distributions while cutting LLM adaptation costs by over 90%.

</details>


### [131] [Finetune-Informed Pretraining Boosts Downstream Performance](https://arxiv.org/abs/2601.20884)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.LG

TL;DR: 提出Finetune-Informed Pretraining (FIP)，通过对目标模态施加更高掩码难度、更大损失权重以及更强解码容量，在不改动编码器或额外监督的前提下，偏向目标模态的表征学习，提升下游单模态微调性能。


<details>
  <summary>Details</summary>
Motivation: 在实际部署中往往只有一个模态在下游微调阶段被广泛使用，但现有多模态预训练对所有模态等同优化，可能导致目标模态的表现被削弱。

Method: FIP是一种模型无关的方法，针对目标模态采用更高的掩码难度、更大的损失权重和更强的解码能力来增强该模态的表征学习，同时不修改共享编码器，也不依赖额外监督数据。将其应用于基于掩码建模的星座图无线信号多模态任务，提升下游微调性能。

Result: 在星座图无线信号的掩码建模任务中，FIP在下游微调阶段持续提高性能且无需额外数据或计算资源；实现简单、与现有多模态掩码建模管线兼容。

Conclusion: FIP提供了一种简单且通用的策略，可把预训练过程的学习重点偏向实际使用的目标模态，从而提升真实场景下的单模态下游表现，且具有良好的架构无关性。

Abstract: Multimodal pretraining is effective for building general-purpose representations, but in many practical deployments, only one modality is heavily used during downstream fine-tuning. Standard pretraining strategies treat all modalities uniformly, which can lead to under-optimized representations for the modality that actually matters. We propose Finetune-Informed Pretraining (FIP), a model-agnostic method that biases representation learning toward a designated target modality needed at fine-tuning time. FIP combines higher masking difficulty, stronger loss weighting, and increased decoder capacity for the target modality, without modifying the shared encoder or requiring additional supervision. When applied to masked modeling on constellation diagrams for wireless signals, FIP consistently improves downstream fine-tuned performance with no extra data or compute. FIP is simple to implement, architecture-compatible, and broadly applicable across multimodal masked modeling pipelines.

</details>


### [132] [A generative machine learning model for designing metal hydrides applied to hydrogen storage](https://arxiv.org/abs/2601.20892)
*Xiyuan Liu,Christian Hacker,Shengnian Wang,Yuhua Duan*

Main category: cs.LG

TL;DR: 提出了一种基于因果发现的轻量化生成模型框架，用于生成超出现有数据库的金属氢化物候选材料；在450个样本数据集上生成1000个候选，筛选后发现6种新化学式与晶体结构，其中4种经DFT验证，显示出较强的未来实验潜力；框架有望高效扩展氢存储数据集并加速材料发现。


<details>
  <summary>Details</summary>
Motivation: 现有材料数据库（如Materials Project）对金属氢化物的覆盖不足，限制了发现最优候选材料的能力；需要高效、数据驱动且可扩展的方法来探索氢存储材料的化学空间。

Method: 将因果发现与轻量级生成式机器学习模型结合，构建一个可生成新颖但具有物理合理性的金属氢化物候选体系的框架。以450条样本（270训练、90验证、90测试）构建数据集，模型生成1000个候选；再对候选进行排序和筛选，得到六个此前未报道的化学式与晶体结构。

Result: 经排序与筛选，识别出6个未报道的候选化学式及对应晶体结构；其中4个被密度泛函理论（DFT）验证，显示出对未来实验研究的强潜力。

Conclusion: 该框架具备可扩展性与时效性，能够有效扩充氢存储数据集并加速材料发现过程。

Abstract: Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.

</details>


### [133] [Is Parameter Isolation Better for Prompt-Based Continual Learning?](https://arxiv.org/abs/2601.20894)
*Jiangyang Li,Chenhao Ding,Songlin Dong,Qiang Wang,Jianchao Zhao,Yuhang He,Yihong Gong*

Main category: cs.LG

TL;DR: 提出一个全局提示池的提示共享框架，结合任务感知门控路由和历史感知调制器，以提升持续学习中的参数利用率并缓解遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决现有固定分配的提示在多任务持续学习中对知识隔离、参数利用不足的问题，提升效果与效率。

Method: 构建全局提示池；引入任务感知门控路由，选择性激活子集提示实现动态解耦与协同优化；加入历史感知调制器，基于累积激活统计保护常用提示免过度更新。

Result: 通过分析与实验，方法在有效性与效率方面持续优于现有静态分配策略。

Conclusion: 提示共享框架能更灵活地分配与保护提示，提升持续学习的知识利用率与记忆稳定性。

Abstract: Prompt-based continual learning methods effectively mitigate catastrophic forgetting. However, most existing methods assign a fixed set of prompts to each task, completely isolating knowledge across tasks and resulting in suboptimal parameter utilization. To address this, we consider the practical needs of continual learning and propose a prompt-sharing framework. This framework constructs a global prompt pool and introduces a task-aware gated routing mechanism that sparsely activates a subset of prompts to achieve dynamic decoupling and collaborative optimization of task-specific feature representations. Furthermore, we introduce a history-aware modulator that leverages cumulative prompt activation statistics to protect frequently used prompts from excessive updates, thereby mitigating inefficient parameter usage and knowledge forgetting. Extensive analysis and empirical results demonstrate that our approach consistently outperforms existing static allocation strategies in effectiveness and efficiency.

</details>


### [134] [TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins](https://arxiv.org/abs/2601.20906)
*Nikita Makarov,Maria Bordukova,Lena Voith von Voithenberg,Estrella Pivel-Villanueva,Sabrina Mielke,Jonathan Wickes,Hanchen Wang,Mingyu Derek Ma,Keunwoo Choi,Kyunghyun Cho,Stephen Ra,Raul Rodriguez-Esteban,Fabian Schmich,Michael Menden*

Main category: cs.LG

TL;DR: TwinWeaver 将纵向患者病史序列化为文本，使大语言模型能够统一完成事件预测与时序预测；基于此构建 Genie Digital Twin (GDT) 覆盖 93,054 名患者、20 种癌种，在预测与分层等任务上显著优于强基线，并具对分布外试验的良好泛化能力，且提供可解释的临床推理扩展。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏、多模态临床时序数据建模难题，传统时序模型效果受限；通过文本序列化将纵向数据与大语言模型结合，实现统一事件预测、疾病进展预测与治疗切换等多任务，同时提升可解释性。

Method: 将纵向病历序列序列化为文本输入，利用大语言模型进行统一事件预测与未来趋势预测；构建 Genie Digital Twin (GDT) 在 93,054 名患者、20 种癌种上评估；对零-shot 与微调在分布外临床试验上的泛化能力进行系统评估；提出可扩展、透明的临床推理扩展以增强解释性。

Result: GDT 实现中位 MASE 0.87（对比 strongest时间序列基线 0.97，p<0.001）；平均 C-index 0.703，覆盖生存/进展/治疗切换任务，超出最佳基线 0.662；在分布外试验零-shot 及微调后，MASE 0.75–0.88，C-index 0.672，超越零-shot基线并在微调后超越。

Conclusion: TwinWeaver 提供一个可解释的、可扩展的纵向临床建模基础，开放源代码，提升事件预测、风险分层和外部泛化能力，同时为临床推理提供透明的机制。

Abstract: Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.

</details>


### [135] [Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges](https://arxiv.org/abs/2601.20913)
*Chen Feng,Minghe Shen,Ananth Balashankar,Carsten Gerner-Beuerle,Miguel R. D. Rodrigues*

Main category: cs.LG

TL;DR: 提出 Noisy but Valid 框架，对带噪声裁判的 LLM 评估进行统计推断，给出有限样本下的 Type I 错误控制，并给出理论与实证分析及 Oracle 间隙评估。


<details>
  <summary>Details</summary>
Motivation: 在对 Large Language Models 的安全评估中，需使用裁判判断来判定失败率是否低于阈值，但裁判存在噪声、偏差，可能破坏统计保证。因此需要一个在校准不确定性下仍能提供有效性保障的评估框架。

Method: 利用小规模人工标注的校准集估计评审者的真正阳性率(TPR)与假阳性率(FPR)，据此推导方差校正的临界阈值，应用于大规模带标签数据。理论上证明在有限样本下的 Type I 错误控制(有效性)。与 PPI 的黑箱推断不同，本框架显式建模评审者行为。

Result: 在理论上给出噪声测试在某些条件下相较直接评估具有更高统计功效的充要条件；在 Jigsaw Comment、Hate Speech、SafeRLHF 数据集上进行经验验证，验证理论；揭示实际评审者参数未知时的 Oracle 间隙，量化估计成本；首次系统化处理不完美裁判者设置，提供可解释的裁判可靠性诊断与评估功效与数据规模、质量、认证水平的关系。

Conclusion: 首次对不完美裁判者情形给出系统化框架，明确评审者可靠性对评估功效的影响，帮助在 LLM 评估中权衡不同推断工具之间的取舍，并为未来在有限样本条件下的鲁棒评估提供诊断工具和理论基础。

Abstract: Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While "LLM-as-a-Judge" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a "Noisy but Valid" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical "Oracle" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.

</details>


### [136] [Noninvasive Intracranial Pressure Estimation Using Subspace System Identification and Bespoke Machine Learning Algorithms: A Learning-to-Rank Approach](https://arxiv.org/abs/2601.20916)
*Anni Zhao,Ayca Ermis,Jeffrey Robert Vitt,Sergio Brasil,Wellingson Paiva,Magdalena Kasprowicz,Malgorzata Burzynska,Robert Hamilton,Runze Yan,Ofer Sadan,J. Claude Hemphill,Lieven Vandenberghe,Xiao Hu*

Main category: cs.LG

TL;DR: 提出一种非侵入式脑压估计的机器学习框架，将系统识别与排序约束优化结合，用非侵入信号估计均值ICP，测试集约32%误差在2 mmHg之内，34%在2–6 mmHg之间。


<details>
  <summary>Details</summary>
Motivation: 非侵入性准确估计颅内压仍是重大挑战，需安全、广泛适用的ICP监测。

Method: 基于子空间系统识别识别脑血流动力学模型，使用ABP、CBv、R-R等信号进行ICP仿真；通过凸优化的排序约束学习映射，描述非侵入信号特征与估计误差之间的关系。训练/测试集随机分割以评估映射函数。

Result: 估计误差在测试集中，约31.88%≤2 mmHg，约34.07%在2–6 mmHg之间。

Conclusion: 验证了所提非侵入ICP估计方法的可行性；需要进一步验证和技术完善以供临床应用。

Abstract: Objective: Accurate noninvasive estimation of intracranial pressure (ICP) remains a major challenge in critical care. We developed a bespoke machine learning algorithm that integrates system identification and ranking-constrained optimization to estimate mean ICP from noninvasive signals. Methods: A machine learning framework was proposed to obtain accurate mean ICP values using arbitrary noninvasive signals. The subspace system identification algorithm is employed to identify cerebral hemodynamics models for ICP simulation using arterial blood pressure (ABP), cerebral blood velocity (CBv), and R-wave to R-wave interval (R-R interval) signals in a comprehensive database. A mapping function to describe the relationship between the features of noninvasive signals and the estimation errors is learned using innovative ranking constraints through convex optimization. Patients across multiple clinical settings were randomly split into testing and training datasets for performance evaluation of the mapping function. Results: The results indicate that about 31.88% of testing entries achieved estimation errors within 2 mmHg and 34.07% of testing entries between 2 mmHg to 6 mmHg from the nonlinear mapping with constraints. Conclusion: Our results demonstrate the feasibility of the proposed noninvasive ICP estimation approach. Significance: Further validation and technical refinement are required before clinical deployment, but this work lays the foundation for safe and broadly accessible ICP monitoring in patients with acute brain injury and related conditions.

</details>


### [137] [A Theory of Universal Agnostic Learning](https://arxiv.org/abs/2601.20961)
*Steve Hanneke,Shay Moran*

Main category: cs.LG

TL;DR: 在 agnostic（二元分类）设定下建立最优通用收敛速率的完整 tetrachotomy，并给出决定类别的简单组合结构。


<details>
  <summary>Details</summary>
Motivation: 去除 realizable 假设，给出二分类无偏差误差的最优速率的统一理论框架与类别划分。

Method: 扩展 realizable 框架，提出四分法（e^{-n}, e^{-o(n)}, o(n^{-1/2}), 任意慢）并揭示决定所属类别的简易组合结构。

Result: 对任意概念类，证明最优 universal 收敛速率只能落在四种之一，并给出判定所属类别的组合结构特征，扩展了前人对 realizable 情况的理论。

Conclusion: 建立了完整理论框架，揭示非 realizable 条件下最优通用速率的本质，并提供可操作的结构判定准则，推动 agnostic 学习理论的统一理解。

Abstract: We provide a complete theory of optimal universal rates for binary classification in the agnostic setting. This extends the realizable-case theory of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021) by removing the realizability assumption on the distribution. We identify a fundamental tetrachotomy of optimal rates: for every concept class, the optimal universal rate of convergence of the excess error rate is one of $e^{-n}$, $e^{-o(n)}$, $o(n^{-1/2})$, or arbitrarily slow. We further identify simple combinatorial structures which determine which of these categories any given concept class falls into.

</details>


### [138] [Distributional Active Inference](https://arxiv.org/abs/2601.20985)
*Abdullah Akgül,Gulcin Baykal,Manuel Haußmann,Mustafa Mert Çelikok,Melih Kandemir*

Main category: cs.LG

TL;DR: 将主动推断无缝嵌入分布式强化学习框架，提出一个覆盖模型基、分布式和模型自由三类方法的正式抽象，从而在无需显式转移动力学建模的情况下提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决感知信息的高效组织与远期行动规划的双重挑战；强化学习只覆盖后者，导致样本效率低下；主动推断作为解读生物脑的理论框架，其在AI中的应用受限于对现有模型方法的扩展。

Method: 提出一个正式的强化学习算法抽象，覆盖模型基、分布式、模型自由三类方法，并将主动推断无缝整合到分布式强化学习框架中，且在无需显式转移动力学建模的前提下实现性能提升。

Result: 在框架层面证明了兼容性，并显示主动推断的性能优势可在分布式RL中被访问到，无需额外的转移动力学建模；为提升样本效率和鲁棒性提供理论基础。

Conclusion: 提供一种统一的抽象，促使主动推断进入分布式强化学习，有望缩小认知理论与强化学习实践之间的差距。

Abstract: Optimal control of complex environments with robotic systems faces two complementary and intertwined challenges: efficient organization of sensory state information and far-sighted action planning. Because the reinforcement learning framework addresses only the latter, it tends to deliver sample-inefficient solutions. Active inference is the state-of-the-art process theory that explains how biological brains handle this dual problem. However, its applications to artificial intelligence have thus far been limited to extensions of existing model-based approaches. We present a formal abstraction of reinforcement learning algorithms that spans model-based, distributional, and model-free approaches. This abstraction seamlessly integrates active inference into the distributional reinforcement learning framework, making its performance advantages accessible without transition dynamics modeling.

</details>


### [139] [Pre-trained Encoders for Global Child Development: Transfer Learning Enables Deployment in Data-Scarce Settings](https://arxiv.org/abs/2601.20987)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 全球子发育预训练编码器在多国数据上训练，显著提升少样本和零样本监测性能，推动 SDG 4.2.1 监测在资源受限环境中的可行性。


<details>
  <summary>Details</summary>
Motivation: 在新国家/地区部署机器学习模型时存在数据瓶颈：高效模型需要成千上万的样本，而新项目往往起步样本不足（<100）。通过跨国预训练与转移学习来提升在极少样本条件下的泛化能力。

Method: 构建用于全球儿童发育的预训练编码器，基于 UNICEF 调查数据，在44个国家、共357,709名儿童上进行训练；评估包括少样本情形（n=50、AUC 0.65，CI 0.56-0.72）、常规模型对比（冷启动梯度提升，AUC 0.61）、增量样本情形（n=500，AUC 0.73）以及对未知国家的零-shot部署（AUC最高0.84）；并应用转移学习界限来解释为何预训练的多样性有助于少量数据的泛化。

Result: 少样本（n=50）AUC 0.65（CI 0.56-0.72），较冷启动梯度提升提高8-12%；n=500 AUC 0.73；未知国家的零-shot AUC最高可达0.84；引入并解释转移学习界限以阐明多样性对泛化的推动作用。

Conclusion: 预训练编码器可显著提升资源受限设置中对 SDG 4.2.1 监测的可行性，改变全球儿童发育监测的实现路径。

Abstract: A large number of children experience preventable developmental delays each year, yet the deployment of machine learning in new countries has been stymied by a data bottleneck: reliable models require thousands of samples, while new programs begin with fewer than 100. We introduce the first pre-trained encoder for global child development, trained on 357,709 children across 44 countries using UNICEF survey data. With only 50 training samples, the pre-trained encoder achieves an average AUC of 0.65 (95% CI: 0.56-0.72), outperforming cold-start gradient boosting at 0.61 by 8-12% across regions. At N=500, the encoder achieves an AUC of 0.73. Zero-shot deployment to unseen countries achieves AUCs up to 0.84. We apply a transfer learning bound to explain why pre-training diversity enables few-shot generalization. These results establish that pre-trained encoders can transform the feasibility of ML for SDG 4.2.1 monitoring in resource-constrained settings.

</details>


### [140] [Top-k on a Budget: Adaptive Ranking with Weak and Strong Oracles](https://arxiv.org/abs/2601.20989)
*Lutz Oettershagen*

Main category: cs.LG

TL;DR: ACE and ACE-W provide adaptive strategies for top-k identification with two oracles, achieving near-optimal strong-call bounds while reducing reliance on expensive high-fidelity queries; STC establishes baseline and a lower bound, while ACE matches the bound in practice and ACE-W further reduces strong costs.


<details>
  <summary>Details</summary>
Motivation: Exact valuations are costly; a fast, noisy weak oracle and a scarce strong oracle require efficient strategies to identify the top-k items with minimal high-fidelity queries.

Method: STC: screen-then-certify baseline with weak intervals; ACE: adaptive certification concentrating strong queries on boundary items while preserving the same asymptotic strong-call bound; ACE-W: fully adaptive two-phase method allocating the weak budget adaptively before ACE to further cut strong calls.

Result: Theoretical: STC uses at most m(4εmax) strong calls; lower bound Ω(m(εmax)) for any algorithm under the same weak uncertainty; ACE achieves the same O(m(4εmax)) bound but with fewer strong calls in practice; ACE-W further reduces strong costs by adaptively budgeting weak queries before ACE.

Conclusion: Adaptive two-stage strategies effectively manage the trade-off between cheap noisy and expensive accurate valuations for top-k identification, offering near-optimal strong-call complexity with practical reductions; ACE-W provides an additional improvement by clarifying weak-budget allocation.

Abstract: Identifying the top-$k$ items is fundamental but often prohibitive when exact valuations are expensive. We study a two-oracle setting with a fast, noisy weak oracle and a scarce, high-fidelity strong oracle (e.g., human expert verification or expensive simulation). We first analyze a simple screen-then-certify baseline (STC) and prove it makes at most $m(4\varepsilon_{\max})$ strong calls given jointly valid weak confidence intervals with maximum radius $\varepsilon_{\max}$, where $m(\cdot)$ denotes the near-tie mass around the top-$k$ threshold. We establish a conditional lower bound of $Ω(m(\varepsilon_{\max}))$ for any algorithm given the same weak uncertainty. Our main contribution is ACE, an adaptive certification algorithm that focuses strong queries on critical boundary items, achieving the same $O(m(4\varepsilon_{\max}))$ bound while reducing strong calls in practice. We then introduce ACE-W, a fully adaptive two-phase method that allocates weak budget adaptively before running ACE, further reducing strong costs.

</details>


### [141] [The Depth Delusion: Why Transformers Should Be Wider, Not Deeper](https://arxiv.org/abs/2601.20994)
*Md Muhtasim Munif Fahim,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 提出架构条件化的缩放定律，给出深度与宽度最优增长规律、临界深度现象及其在生产规模上的广泛实证；在7B规模下，64层反而劣于32层约0.12nat；30种Transformer架构的R^2为0.922。


<details>
  <summary>Details</summary>
Motivation: 现有神经缩放定律将架构视为可替换的参数化因素，忽略架构对损失的影响。需要将架构信息纳入缩放规律，以更准确预测性能并指导模型设计。

Method: 提出架构条件缩放定律，得到最优深度和宽度随参数量增长的幂律关系：D* ~ C^0.12、W* ~ C^0.34；发现临界深度D_crit ~ W^0.44；在30个Transformer架构（17M–7B参数）上进行系统评估，得到R^2=0.922。

Result: 实证结果表明：深度与宽度的最优比重随规模增长，且存在深度超过D_crit后 Depth Delusion，当增加层数反而提升损失；在7B尺度，64层模型的损失高于32层模型约0.12 nat，表明生产规模上仍存在可预测的深度-宽度权衡。

Conclusion: 架构信息对缩放规律至关重要，优化深度-宽度比需考虑临界深度；在实际模型设计中应优先提升宽度并谨慎控制深度，避免超过临界深度带来的性能下降。

Abstract: Neural scaling laws describe how language model loss decreases with parameters and data, but treat architecture as interchangeable--a billion parameters could arise from a shallow-wide model (10 layers & 8,192 hidden dimension) or a deep-narrow one (80 layers & 2,048 hidden dimension). We propose architecture-conditioned scaling laws decomposing this dependence, finding that optimal depth scales as D* ~ C^0.12 while optimal width scales as W* ~ C^0.34, meaning width should grow 2.8x faster than depth. We discover a critical depth phenomenon: beyond D_crit ~ W^0.44 (sublinear in W), adding layers increases loss despite adding parameters--the Depth Delusion. Empirically, we validate these findings across 30 transformer architectures spanning 17M to 7B parameters, each trained on representative high-compute samples, achieving R^2 = 0.922. Our central finding: at 7B scale, a 64-layer model (6.38B params) underperforms a 32-layer model (6.86B params) by 0.12 nats, despite being significantly deeper. This demonstrates that optimal depth-width tradeoffs persist at the production scale.

</details>


### [142] [Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research](https://arxiv.org/abs/2601.21008)
*Ruicheng Ao,David Simchi-Levi,Xinshang Wang*

Main category: cs.LG

TL;DR: 提出两项核心基准，将求解器放入迭代诊断/修复循环中，以评估和训练运筹学模型的过程级鲁棒性和偏差控制。


<details>
  <summary>Details</summary>
Motivation: 现有基准多聚焦一次性描述到实现的映射，而忽略诊断循环中的交互过程及可验证反馈的重要性。本文旨在通过可验证的oracle与自修复循环，评估并提升模型在线性/整数约束优化中的诊断与修复能力。

Method: ORDebug：5000+问题，涵盖9类错误；每次修复触发求解器重新执行并重新计算不可约不可行子集(IIS)，提供确定性、可验证的反馈。ORBias：2,000个新闻贩卖实例（1000ID + 1000OOD），用于衡量与闭式最优策略的系统性偏差。对26个模型、12000+样本进行评估，比较领域特定RLVR训练与规模效应。

Result: 在多项指标上，领域特定RLVR训练使8B模型超越前沿API：恢复率95.3% vs 86.2%、诊断准确率62.4% vs 47.8%、求解到达步骤2.25 vs 3.78，提速约1.7倍。ORBias中，课程化训练实现唯一的ID→OOD偏差下降（-9.6%），将系统性偏差从20.0%降至10.4%，降低约48%。

Conclusion: 过程级评估结合可验证的oracle与任务特定训练，能显著提升诊断与修复能力，超越单纯依赖模型规模的性能提升，体现面向应用的鲁棒性与偏差控制潜力。

Abstract: Operations Research practitioners routinely debug infeasible models through an iterative process: analyzing Irreducible Infeasible Subsystems (\IIS{}), identifying constraint conflicts, and systematically repairing formulations until feasibility is achieved. Yet existing LLM benchmarks evaluate OR as one-shot translation -- given a problem description, generate solver code -- ignoring this diagnostic loop entirely. We introduce two benchmarks that place the \textbf{solver in the evaluation loop}. \textbf{\ORDebug{}} evaluates iterative self-correction through 5,000+ problems spanning 9 error types; each repair action triggers solver re-execution and \IIS{} recomputation, providing deterministic, verifiable feedback. \textbf{\ORBias{}} evaluates behavioral rationality through 2,000 newsvendor instances (1,000 ID + 1,000 OOD), measuring systematic deviations from closed-form optimal policies. Across 26 models and 12,000+ samples, we find that domain-specific RLVR training enables an 8B model to surpass frontier APIs: 95.3\% vs 86.2\% recovery rate (+9.1\%), 62.4\% vs 47.8\% diagnostic accuracy (+14.6\%), and 2.25 vs 3.78 steps to resolution (1.7$\times$ faster). On \ORBias{}, curriculum training achieves the only negative ID$\rightarrow$OOD bias drift among models evaluated (-9.6\%), reducing systematic bias by 48\% (from 20.0\% to 10.4\%). These results demonstrate that process-level evaluation with verifiable oracles enables targeted training that outperforms scale.

</details>


### [143] [Order-Aware Test-Time Adaptation: Leveraging Temporal Dynamics for Robust Streaming Inference](https://arxiv.org/abs/2601.21012)
*Young Kyung Kim,Oded Schlesinger,Qiangqiang Wu,J. Matías Di Martino,Guillermo Sapiro*

Main category: cs.LG

TL;DR: 提出OATTA：结合学习的时序先验和似然比门控的无梯度TTA框架，提升对分布漂移的时序自适应。


<details>
  <summary>Details</summary>
Motivation: 现有TTA多将测试流视为独立样本，未利用时间序列中的监督信号；需要在弱结构流中也能安全利用时序信息。

Method: 将TTA建模为梯度无关的递归贝叶斯估计，采用学习的动态转移矩阵作为时序先验修正预测；引入LLR门控，在缺乏时间证据时回退到基线预测；模块轻量、模型无关。

Result: 在图像分类、可穿戴/生理信号、语言情感分析等多域任务中提升基线，最高提升约6.35%。

Conclusion: 将时序动态作为独立信号，OATTA具有普适性和低开销，证实了时间建模在TTA中的重要性。

Abstract: Test-Time Adaptation (TTA) enables pre-trained models to adjust to distribution shift by learning from unlabeled test-time streams. However, existing methods typically treat these streams as independent samples, overlooking the supervisory signal inherent in temporal dynamics. To address this, we introduce Order-Aware Test-Time Adaptation (OATTA). We formulate test-time adaptation as a gradient-free recursive Bayesian estimation task, using a learned dynamic transition matrix as a temporal prior to refine the base model's predictions. To ensure safety in weakly structured streams, we introduce a likelihood-ratio gate (LLR) that reverts to the base predictor when temporal evidence is absent. OATTA is a lightweight, model-agnostic module that incurs negligible computational overhead. Extensive experiments across image classification, wearable and physiological signal analysis, and language sentiment analysis demonstrate its universality; OATTA consistently boosts established baselines, improving accuracy by up to 6.35%. Our findings establish that modeling temporal dynamics provides a critical, orthogonal signal beyond standard order-agnostic TTA approaches.

</details>


### [144] [Conditional Denoising Model as a Physical Surrogate Model](https://arxiv.org/abs/2601.21021)
*José Afonso,Pedro Viegas,Rodrigo Ventura,Vasco Guerra*

Main category: cs.LG

TL;DR: CDM is a conditional denoising model that learns the geometry of the physical solution manifold by denoising states, enabling a time-independent fixed-point inference that projects noisy approximations onto the equilibrium manifold and achieving superior data/parameter efficiency and physical consistency.


<details>
  <summary>Details</summary>
Motivation: Surrogate models often trade data fit vs. physical law adherence; soft physics losses or post-hoc corrections fail to guarantee strict compliance or intrinsic geometry.

Method: Train a conditional denoising network to map noisy states to clean ones, learning a vector field toward the valid subspace; formulate a time-independent fixed-point iteration to project onto the equilibrium manifold.

Result: On a low-temperature plasma physics and chemistry benchmark, CDM shows higher data and parameter efficiency than physics-consistent baselines; denoising objective acts as an implicit regularizer, yielding better adherence to physical constraints without needing explicit physics losses.

Conclusion: Denoising-based geometry learning can outperform physics-integrated loss approaches, providing a principled, efficient surrogate that enforces PDE-like constraints by design.

Abstract: Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.

</details>


### [145] [SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model](https://arxiv.org/abs/2601.21031)
*Zongheng Guo,Tao Chen,Yang Jiao,Yi Pan,Xiao Hu,Manuela Ferrario*

Main category: cs.LG

TL;DR: 提出一个以统计先验为引导的生成式遮罩框架SIGMA-PPG，结合先验-引导对抗遮罩与向量量化的语义一致性，以提升PPG信号表示的鲁棒性与形态保真。


<details>
  <summary>Details</summary>
Motivation: 解决现有PPG foundation模型在信号冗余和噪声中的不足：掩码化自编码往往易产生平庸解，对比学习缺乏形态级的保真；通过统计先验引导的遮罩机制和语义一致性约束，提升表示的泛化性和语义密度。

Method: 提出SIGMA-PPG，包含一个先验引导的对抗遮罩机制，其教师模型通过强化学习利用统计先验设计具有挑战性的学习路径以抑制对噪声的过拟合；并结合向量量化的语义一致性约束，确保生理上相同的波形映射到共享的代码本索引。此外，在超过12万小时数据上进行预训练，公开代码。

Result: 相较于五个SOTA基线，SIGMA-PPG在12项下游任务上表现优于平均水平，显示出更高的鲁棒性与形态保真，且通过代码本语义密度提升减少冗余特征。

Conclusion: 该方法通过结合统计先验的对抗遮罩与向量量化的语义约束，有效提升PPG信号的生成式表示质量与下游任务性能，具有良好的泛化潜力与可扩展性。

Abstract: Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at https://github.com/ZonghengGuo/SigmaPPG.

</details>


### [146] [Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints](https://arxiv.org/abs/2601.21033)
*Omer Rochman-Sharabi,Gilles Louppe*

Main category: cs.LG

TL;DR: 提出一种基于扩散模型的带约束采样框架，通过 Predict-Project-Renoise (PPR) 在生成阶段强制满足硬约束（物理规律、观测一致性），并在二维分布、偏微分方程和全球天气预报等任务上显著降低约束违约、提升样本一致性，且更接近真实的约束分布。


<details>
  <summary>Details</summary>
Motivation: 在科学应用中，传统扩散模型往往难以保证物理准确性与约束满足性，需要在生成阶段引入硬约束以确保结果的物理可行性。

Method: 定义一个仅在可行约束集合上扩散的约束前向过程，得到受约束的边际分布。反向过程采用 Predict-Project-Renoise (PPR) 算法：交替进行去噪预测、投影到可行集合、并进行再加噪以采样约束边际。

Result: 在 2D 分布、偏微分方程和全球天气预报等任务上，PPR 将约束违约减少超过一个数量级；样本更具一致性，且对真实约束分布的拟合优于基线方法。

Conclusion: 带约束的扩散采样框架在科学仿真中有效实现硬约束强制，提升样本的物理一致性；PPR 提供了一有效的迭代策略以实现该目标。

Abstract: Neural emulators based on diffusion models show promise for scientific applications, but vanilla models cannot guarantee physical accuracy or constraint satisfaction. We address this by introducing a constrained sampling framework that enforces hard constraints, such as physical laws or observational consistency, at generation time. Our approach defines a constrained forward process that diffuses only over the feasible set of constraint-satisfying samples, inducing constrained marginal distributions. To reverse this, we propose Predict-Project-Renoise (PPR), an iterative algorithm that samples from the constrained marginals by alternating between denoising predictions, projecting onto the feasible set, and renoising. Experiments on 2D distributions, PDEs, and global weather forecasting demonstrate that PPR reduces constraint violations by over an order of magnitude while improving sample consistency and better matching the true constrained distribution compared to baselines.

</details>


### [147] [Test-Time Adaptation for Unsupervised Combinatorial Optimization](https://arxiv.org/abs/2601.21048)
*Yiqiao Liao,Farinaz Koushanfar,Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: 提出 TACO 框架，用于无监督神经组合优化的测试时自适应，结合通用化泛化与实例特定优化的优点，提供 warm-start 策略以提升解质量，同时保持低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决泛化型模型（训练后可泛化的模型）与实例特定优化（测试时独立优化）之间的矛盾：前者高效但缺乏实例级适应性，后者灵活但难以利用学到的归纳结构并易陷入局部最优。

Method: 提出模型无关的测试时自适应框架 TACO，通过部分放松训练参数以保留 inductive bias 的方式实现快速有效的无监督适应，同时提供 warm-start 策略来桥接两种范式。

Result: 在经典组合优化问题：最小顶点覆盖和最大团上，TACO 对静态、分布偏移和动态情形均展现出更好的问题解质量与鲁棒性，且相较于从头开始微调或独立优化的方案，计算开销低。

Conclusion: TACO 作为无监督 NCO 的普适桥梁，兼具泛化带来的结构性偏置与实例自适应的灵活性，具有良好的实用性与鲁棒性。

Abstract: Unsupervised neural combinatorial optimization (NCO) enables learning powerful solvers without access to ground-truth solutions. Existing approaches fall into two disjoint paradigms: models trained for generalization across instances, and instance-specific models optimized independently at test time. While the former are efficient during inference, they lack effective instance-wise adaptability; the latter are flexible but fail to exploit learned inductive structure and are prone to poor local optima. This motivates the central question of our work: how can we leverage the inductive bias learned through generalization while unlocking the flexibility required for effective instance-wise adaptation? We first identify a challenge in bridging these two paradigms: generalization-focused models often constitute poor warm starts for instance-wise optimization, potentially underperforming even randomly initialized models when fine-tuned at test time. To resolve this incompatibility, we propose TACO, a model-agnostic test-time adaptation framework that unifies and extends the two existing paradigms for unsupervised NCO. TACO applies strategic warm-starting to partially relax trained parameters while preserving inductive bias, enabling rapid and effective unsupervised adaptation. Crucially, compared to naively fine-tuning a trained generalizable model or optimizing an instance-specific model from scratch, TACO achieves better solution quality while incurring negligible additional computational cost. Experiments on canonical CO problems, Minimum Vertex Cover and Maximum Clique, demonstrate the effectiveness and robustness of TACO across static, distribution-shifted, and dynamic combinatorial optimization problems, establishing it as a practical bridge between generalizable and instance-specific unsupervised NCO.

</details>


### [148] [Snowball: A Scalable All-to-All Ising Machine with Dual-Mode Markov Chain Monte Carlo Spin Selection and Asynchronous Spin Updates for Fast Combinatorial Optimization](https://arxiv.org/abs/2601.21058)
*Seungki Hong,Kyeongwon Jeong,Taekwang Jang*

Main category: cs.LG

TL;DR: Snowball 是一个数字化、可扩展的全耦合 Ising 机，结合双模态马尔科夫链蒙特卡洛自旋选择与异步自旋更新，提升收敛性并缩短解题时间。


<details>
  <summary>Details</summary>
Motivation: 旨在解决 Ising 机在实际部署中的三大挑战：硬件拓扑带来的小映射成本、易发振荡/停滞的自旋选择与更新、以及耦合系数精度的可扩展性，推动可用于实际优化任务的实现。

Method: 在数字硬件上实现全局全耦合，并引入双模态 MCMC 自旋选择与异步自旋更新；耦合精度高宽可配置，与传统模拟/模拟退火的模拟性实现不同；原型在 AMD Alveo U250 上测试，比较对象为同一基准实例的最先进 Ising 机。

Result: 在相同基准实例上，时间-到-解缩短约 8 倍。

Conclusion: 通过数字化实现、可配置耦合精度及改进的自旋选择/更新策略，Snowball 展示了显著的时间效率提升，具备实际部署潜力；未来可进一步验证可扩展性、能耗与鲁棒性，以及在更大规模问题上的表现。

Abstract: Ising machines have emerged as accelerators for combinatorial optimization. To enable practical deployment, this work aims to reduce time-to-solution by addressing three challenges: (1) hardware topology, (2) spin selection and update algorithms, and (3) scalable coupling-coefficient precision. Restricted topologies require minor embedding; naive parallel updates can oscillate or stall; and limited precision can preclude feasible mappings or degrade solution quality.
  This work presents Snowball, a digital, scalable, all-to-all coupled Ising machine that integrates dual-mode Markov chain Monte Carlo spin selection with asynchronous spin updates to promote convergence and reduce time-to-solution. The digital architecture supports wide, configurable coupling precision, unlike many analog realizations at high bit widths. A prototype on an AMD Alveo U250 accelerator card achieves an 8$\times$ reduction in time-to-solution relative to a state-of-the-art Ising machine on the same benchmark instance.

</details>


### [149] [Human-LLM Collaborative Feature Engineering for Tabular Data](https://arxiv.org/abs/2601.21060)
*Zhuoyan Li,Aditya Bansal,Jinzhao Li,Shishuang He,Zhuoran Lu,Mutian Zhang,Qin Liu,Yiwei Yang,Swati Jain,Ming Yin,Yunyao Li*

Main category: cs.LG

TL;DR: 通过人机协作的特征工程框架，将LLM仅用于生成候选转换操作，基于对每个操作的效用和不确定性进行选择，并引入人类专家偏好反馈以提升性能和减少认知负担。


<details>
  <summary>Details</summary>
Motivation: 现有方法将LLM作为黑箱优化器，缺乏对操作效用的校准估计，容易在低收益操作上重复探索，缺乏系统化的优先级策略。

Method: 将提议与选择解耦：LLM负责生成候选操作，择优由显式建模的效用和不确定性来指导选择；在早期阶段引入人类专家偏好反馈来引导选择，提升对有潜力操作的识别。对综合场景进行评估，包括合成研究和真实用户研究，使用多种表格数据集。

Result: 在合成研究和真实用户研究中，框架提升了特征工程性能，覆盖多种表格数据集，同时降低了用户在特征工程过程中的认知负担。

Conclusion: 人机协作的特征工程框架有效提升性能与效率，解耦提议与选择并通过人类偏好反馈实现更稳健的操作选择，适用于表格数据的特征工程任务。

Abstract: Large language models (LLMs) are increasingly used to automate feature engineering in tabular learning. Given task-specific information, LLMs can propose diverse feature transformation operations to enhance downstream model performance. However, current approaches typically assign the LLM as a black-box optimizer, responsible for both proposing and selecting operations based solely on its internal heuristics, which often lack calibrated estimations of operation utility and consequently lead to repeated exploration of low-yield operations without a principled strategy for prioritizing promising directions. In this paper, we propose a human-LLM collaborative feature engineering framework for tabular learning. We begin by decoupling the transformation operation proposal and selection processes, where LLMs are used solely to generate operation candidates, while the selection is guided by explicitly modeling the utility and uncertainty of each proposed operation. Since accurate utility estimation can be difficult especially in the early rounds of feature engineering, we design a mechanism within the framework that selectively elicits and incorporates human expert preference feedback, comparing which operations are more promising, into the selection process to help identify more effective operations. Our evaluations on both the synthetic study and the real user study demonstrate that the proposed framework improves feature engineering performance across a variety of tabular datasets and reduces users' cognitive load during the feature engineering process.

</details>


### [150] [Signal from Structure: Exploiting Submodular Upper Bounds in Generative Flow Networks](https://arxiv.org/abs/2601.21061)
*Alexandre Larouche,Audrey Durand*

Main category: cs.LG

TL;DR: GFlowNets with submodular rewards leverage upper bounds on unseen compositions via submodularity, enabling optimistic training with SUBo-GFN that greatly increases data efficiency and improves candidate generation on synthetic and real submodular tasks.


<details>
  <summary>Details</summary>
Motivation: GFlowNets aim to sample compositional objects proportional to an unknown reward. When the reward has a structured, actionable form—submodularity—one can derive informative upper bounds for unobserved objects. This nurtures data-efficient learning under uncertainty and expands support by optimistic bounding.

Method: 1) Derive submodular upper bounds on rewards for unseen compositional objects. 2) Analyze probability of tight bounds and coverage (how many unseen objects are bounded). 3) Build SUBo-GFN using these bounds within an Optimism in the Face of Uncertainty framework to train the GFN. 4) Empirically compare data efficiency, distribution matching, and candidate quality on synthetic and real submodular tasks.

Result: SUBo-GFN generates orders of magnitude more training data than classical GFNs for the same number of reward queries. It achieves effective distribution matching and high-quality candidate generation on both synthetic and real-world submodular tasks.

Conclusion: Exploiting submodular structure to derive upper bounds enables optimistic data collection for GFNs, yielding substantial gains in training data efficiency and candidate quality. SUBo-GFN is a practical approach for submodular-objective generation and selection.

Abstract: Generative Flow Networks (GFlowNets; GFNs) are a class of generative models that learn to sample compositional objects proportionally to their a priori unknown value, their reward. We focus on the case where the reward has a specified, actionable structure, namely that it is submodular. We show submodularity can be harnessed to retrieve upper bounds on the reward of compositional objects that have not yet been observed. We provide in-depth analyses of the probability of such bounds occurring, as well as how many unobserved compositional objects can be covered by a bound. Following the Optimism in the Face of Uncertainty principle, we then introduce SUBo-GFN, which uses the submodular upper bounds to train a GFN. We show that SUBo-GFN generates orders of magnitude more training data than classical GFNs for the same number of queries to the reward function. We demonstrate the effectiveness of SUBo-GFN in terms of distribution matching and high-quality candidate generation on synthetic and real-world submodular tasks.

</details>


### [151] [Out-of-Distribution Generalization in Graph Foundation Models](https://arxiv.org/abs/2601.21067)
*Haoyang Li,Haibo Chen,Xin Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: A comprehensive survey detailing OOD generalization in graph foundation models, proposing a unified problem setting and taxonomy of strategies across fixed-task vs cross-task generalization, plus evaluation protocols and future directions.


<details>
  <summary>Details</summary>
Motivation: Graph learning models struggle with distribution shifts across graphs, domains, modalities, or task formulations. Graph foundation models pretrained on diverse graphs aim to improve generalization, necessitating a systematic understanding of OOD generalization in GFMs.

Method: Literature review and taxonomy: organize approaches by whether they assume a fixed task specification or support generalization across heterogeneous task formulations; synthesize OOD handling strategies and pretraining objectives; discuss evaluation protocols and future directions; claim to be the first survey on OOD generalization in GFMs.

Result: Provides a unified problem setting, a structured taxonomy of OOD strategies for GFMs, evaluation practices, and prioritized open questions for future work.

Conclusion: Highlights open directions, including robust benchmark development, improved pretraining objectives, and standardized evaluation protocols to advance OOD generalization in GFMs.

Abstract: Graphs are a fundamental data structure for representing relational information in domains such as social networks, molecular systems, and knowledge graphs. However, graph learning models often suffer from limited generalization when applied beyond their training distributions. In practice, distribution shifts may arise from changes in graph structure, domain semantics, available modalities, or task formulations. To address these challenges, graph foundation models (GFMs) have recently emerged, aiming to learn general-purpose representations through large-scale pretraining across diverse graphs and tasks. In this survey, we review recent progress on GFMs from the perspective of out-of-distribution (OOD) generalization. We first discuss the main challenges posed by distribution shifts in graph learning and outline a unified problem setting. We then organize existing approaches based on whether they are designed to operate under a fixed task specification or to support generalization across heterogeneous task formulations, and summarize the corresponding OOD handling strategies and pretraining objectives. Finally, we review common evaluation protocols and discuss open directions for future research. To the best of our knowledge, this paper is the first survey for OOD generalization in GFMs.

</details>


### [152] [LOCUS: Low-Dimensional Model Embeddings for Efficient Model Exploration, Comparison, and Selection](https://arxiv.org/abs/2601.21082)
*Shivam Patel,William Cocke,Gauri Joshi*

Main category: cs.LG

TL;DR: LOCUS produces low-dimensional, geometry-preserving embeddings of language model capabilities via an attention-based forward pass, enabling addition of new models without retraining and improving routing for unseen queries with a correctness predictor. It is sample-efficient and yields embeddings where proximity reflects model similarity, supporting clustering, portfolio selection, and proxies for unavailable models.


<details>
  <summary>Details</summary>
Motivation: The LLM ecosystem is rapidly expanding, making it hard to manage, compare, and route queries to the right models. A compact, adaptable representation of each model that facilitates routing and downstream analysis is highly desirable.

Method: An attention-based encoder generates fixed-size embeddings by running a deterministic forward pass over query encodings and evaluation scores. New models can be integrated without retraining. A separate correctness predictor uses embeddings and query encodings to achieve high routing accuracy on unseen queries.

Result: LOCUS achieves up to 4.8x reduction in required query evaluations to produce informative embeddings. The embedding space is geometrically meaningful: similar models are close together. This enables downstream tasks such as model comparison, clustering, portfolio optimization, and resilient proxies for unavailable models.

Conclusion: LOCUS provides a scalable, data-efficient method to embed and manage a large, dynamic pool of LLMs, with strong routing performance on unseen queries and useful geometric structure for analysis and applications.

Abstract: The rapidly growing ecosystem of Large Language Models (LLMs) makes it increasingly challenging to manage and utilize the vast and dynamic pool of models effectively. We propose LOCUS, a method that produces low-dimensional vector embeddings that compactly represent a language model's capabilities across queries. LOCUS is an attention-based approach that generates embeddings by a deterministic forward pass over query encodings and evaluation scores via an encoder model, enabling seamless incorporation of new models to the pool and refinement of existing model embeddings without having to perform any retraining. We additionally train a correctness predictor that uses model embeddings and query encodings to achieve state-of-the-art routing accuracy on unseen queries. Experiments show that LOCUS needs up to 4.8x fewer query evaluation samples than baselines to produce informative and robust embeddings. Moreover, the learned embedding space is geometrically meaningful: proximity reflects model similarity, enabling a range of downstream applications including model comparison and clustering, model portfolio selection, and resilient proxies of unavailable models.

</details>


### [153] [Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed](https://arxiv.org/abs/2601.21094)
*Minjae Kwon,Josephine Lamp,Lu Feng*

Main category: cs.LG

TL;DR: 训练时的安全约束在分布偏移下往往不泛化；引入测试时 shielding 筛选不安全动作可恢复安全性并提升治疗效果。在糖尿病管理的仿真平台上，对8种安全RL、3糖病类型、3年龄组实现13-14%的 Time-in-Range 提升，同时降低临床风险指数和血糖波动。


<details>
  <summary>Details</summary>
Motivation: 研究在分布偏移下，安全强化学习的训练期安全性是否能转移到实际部署，尤其在临床等安全关键场景。

Method: 建立统一临床仿真器 GlucoSim，评估8种安全RL算法在3类糖尿病与3年龄组的表现；提出测试时 shielding，通过学习得到的动力学模型筛选并过滤不安全动作；以 Time-in-Range、临床风险指数、血糖变异性等作为评估指标，比较强基线如 PPO-Lag、CPO等；提供公开代码与基准平台。

Result:  shielding 在多种设置下显著提升安全性与疗效：Time-in-Range 提升约13–14%，同时降低临床风险指数和血糖变异性；对强基线如 PPO-Lag、CPO 也有效。

Conclusion: 提供一个可复现实验的平台与基准，推动在分布偏移条件下的安全性研究；测试时 shielding 能缓解训练阶段未能覆盖的安全风险，未来需评估鲁棒性、开销与对动力学模型误差的敏感性等。

Abstract: Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.

</details>


### [154] [TRACE: Trajectory Recovery for Continuous Mechanism Evolution in Causal Representation Learning](https://arxiv.org/abs/2601.21135)
*Shicheng Fan,Kun Zhang,Lu Cheng*

Main category: cs.LG

TL;DR: 本工作将因果机制的切换从离散假设放宽到连续过渡：将过渡机制建模为若干原子机制的凸组合，混合系数随时间连续变化，并提出 TRACE 框架实现可识别的机制轨迹。


<details>
  <summary>Details</summary>
Motivation: 现实系统的动力学变化往往是渐进的（如转弯、步态从走到跑的平滑转变），而现有方法多假设离散切换，导致潜在因果变量和混合轨迹的识别性受限。

Method: 将转移机制建模为有限个原子机制的凸组合，混合系数随时间变化。提出 TRACE：一种专家混合（Mixture-of-Experts）框架，每个专家学习一个原子机制，在训练时实现对原子机制的学习；测试时可推断并重构完整的混合轨迹，且对训练未见过的中间状态具有推广性。

Result: 理论层面证明潜在因果变量和连续混合轨迹可共同识别。实验在合成与真实数据上显示 TRACE 能以最高接近 0.99 的相关性恢复混合轨迹，显著优于离散切换基线。

Conclusion: TRACE 将连续机制过渡纳入可识别的因果表示框架，能对未观测到的中间状态进行推断，扩展了因果表示学习在连续转变情形中的应用。

Abstract: Temporal causal representation learning methods assume that causal mechanisms switch instantaneously between discrete domains, yet real-world systems often exhibit continuous mechanism transitions. For example, a vehicle's dynamics evolve gradually through a turning maneuver, and human gait shifts smoothly from walking to running. We formalize this setting by modeling transitional mechanisms as convex combinations of finitely many atomic mechanisms, governed by time-varying mixing coefficients. Our theoretical contributions establish that both the latent causal variables and the continuous mixing trajectory are jointly identifiable. We further propose TRACE, a Mixture-of-Experts framework where each expert learns one atomic mechanism during training, enabling recovery of mechanism trajectories at test time. This formulation generalizes to intermediate mechanism states never observed during training. Experiments on synthetic and real-world data demonstrate that TRACE recovers mixing trajectories with up to 0.99 correlation, substantially outperforming discrete-switching baselines.

</details>


### [155] [Smooth Dynamic Cutoffs for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2601.21147)
*Kevin Han,Haolin Cong,Bowen Deng,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 提出动态 cutoff 的 MLIPs，以降低内存与推理时间，同时保持精度，将邻域数设定为每原子固定的目标数量。


<details>
  <summary>Details</summary>
Motivation: 解决 MLIPs 的推理时间与内存消耗瓶颈；挑战长期以来必须固定的 cutoff 概念以提高可扩展性。

Method: 引入动态 cutoff，按每原子指定目标邻居数来构造稀疏原子图，并将其应用于现有四种尖端 MLIP：MACE、Nequip、OrbV3、TensorNet；实现对不同体系的稳健性与性能评估，并提供开源训练与实现代码。

Result: 实现了约 2.26x 的内存减少与 2.04x 的推理加速，性能提升在不同模型与体系上存在差异；误差分析表明相较固定 cutoff，动态 cutoff 改善/保持准确性降幅很小。

Conclusion: 动态 cutoff 能显著提升 MLIP 的可扩展性与效率，同时对预测精度影响有限，且研究工作及代码将开源。

Abstract: Machine learning interatomic potentials (MLIPs) have proven to be wildly useful for molecular dynamics simulations, powering countless drug and materials discovery applications. However, MLIPs face two primary bottlenecks preventing them from reaching realistic simulation scales: inference time and memory consumption. In this work, we address both issues by challenging the long-held belief that the cutoff radius for the MLIP must be held to a fixed, constant value. For the first time, we introduce a dynamic cutoff formulation that still leads to stable, long timescale molecular dynamics simulation. In introducing the dynamic cutoff, we are able to induce sparsity onto the underlying atom graph by targeting a specific number of neighbors per atom, significantly reducing both memory consumption and inference time. We show the effectiveness of a dynamic cutoff by implementing it onto 4 state of the art MLIPs: MACE, Nequip, Orbv3, and TensorNet, leading to 2.26x less memory consumption and 2.04x faster inference time, depending on the model and atomic system. We also perform an extensive error analysis and find that the dynamic cutoff models exhibit minimal accuracy dropoff compared to their fixed cutoff counterparts on both materials and molecular datasets. All model implementations and training code will be fully open sourced.

</details>


### [156] [Mobility-Embedded POIs: Learning What A Place Is and How It Is Used from Human Movement](https://arxiv.org/abs/2601.21149)
*Maria Despoina Siampou,Shushman Choudhury,Shang-Ling Hsu,Neha Arora,Cyrus Shahabi*

Main category: cs.LG

TL;DR: 提出 ME-POIs，通过将大规模移动数据引入文本基POI嵌入，学习以POI功能为核心的泛化嵌入；通过 temporally contextualized visits 和对比学习对齐，以及跨空间尺度的近邻传播，解决长尾稀疏问题，在五项地图富集任务上优于文本和移动基线，Mobility-only 在某些任务甚至优于文本模型，强调POI功能的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有POI嵌入多聚焦于静态文本元数据中的身份信息，或仅捕捉轨迹上下文的移动规律，未充分反映“POI的使用功能”。需要一个以“功能”为核心的通用POI表示，以提升真实世界地点的表征。

Method: ME-POIs 框架：在语言模型得到的文本嵌入基础上，加入大规模人类移动数据，得到以POI为中心、跨上下文的使用表示。具体做法包括：1) 将个体访问事件编码为带时间上下文的嵌入；2) 用对比学习将这些访问嵌入与可学习的POI嵌入对齐，以捕捉跨用户与跨时段的使用模式；3) 通过在多尺度空间传播来自近邻、访问频繁的POI的时序访问模式，缓解长尾稀疏。

Result: 在五项新提出的地图富集任务上，ME-POIs 与文本嵌入、移动嵌入基线相比均有提升；在某些任务中，纯Mobility嵌入甚至超越文本嵌入，显示POI功能信号的关键性。

Conclusion: 将功能信号纳入POI表示可显著提升泛化能力与应用效果，ME-POIs 为学习通用POI表示提供了有效框架，强调POI功能是关键要素。

Abstract: Recent progress in geospatial foundation models highlights the importance of learning general-purpose representations for real-world locations, particularly points-of-interest (POIs) where human activity concentrates. Existing approaches, however, focus primarily on place identity derived from static textual metadata, or learn representations tied to trajectory context, which capture movement regularities rather than how places are actually used (i.e., POI's function). We argue that POI function is a missing but essential signal for general POI representations. We introduce Mobility-Embedded POIs (ME-POIs), a framework that augments POI embeddings derived, from language models with large-scale human mobility data to learn POI-centric, context-independent representations grounded in real-world usage. ME-POIs encodes individual visits as temporally contextualized embeddings and aligns them with learnable POI representations via contrastive learning to capture usage patterns across users and time. To address long-tail sparsity, we propose a novel mechanism that propagates temporal visit patterns from nearby, frequently visited POIs across multiple spatial scales. We evaluate ME-POIs on five newly proposed map enrichment tasks, testing its ability to capture both the identity and function of POIs. Across all tasks, augmenting text-based embeddings with ME-POIs consistently outperforms both text-only and mobility-only baselines. Notably, ME-POIs trained on mobility data alone can surpass text-only models on certain tasks, highlighting that POI function is a critical component of accurate and generalizable POI representations.

</details>


### [157] [Learning to Advect: A Neural Semi-Lagrangian Architecture for Weather Forecasting](https://arxiv.org/abs/2601.21151)
*Carlos A. Pereira,Stéphane Gaudreault,Valentin Dallerit,Christopher Subich,Shoyon Panday,Siqi Wei,Sasa Zhang,Siddharth Rout,Eldad Haber,Raymond J. Spiteri,David Millard,Emilia Diaconescu*

Main category: cs.LG

TL;DR: A physics-inspired modular neural weather model (PARADIS) that decomposes dynamics into advection, diffusion, and reaction blocks, using a Neural Semi-Lagrangian operator on the sphere for trajectory-based advection, plus diffusion and local interactions. It achieves state-of-the-art skill at 1° resolution with low training cost, matching or exceeding 0.25° baselines including HRES and GraphCast on ERA5 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of monolithic ML weather models for long-range transport and computational efficiency by introducing explicit physical inductive biases (advection, diffusion, reaction) to improve interpretability, generalization, and sample efficiency.

Method: Decompose latent state into advection, diffusion, and reaction components. Advection is implemented via a Neural Semi-Lagrangian operator performing trajectory-based transport with differentiable interpolation on the sphere, enabling end-to-end learning of transport modes and trajectories. Diffusion is realized through depthwise-separable spatial mixing; local forcing and vertical coupling are modeled with pointwise channel interactions. The model is trained end-to-end at 1° resolution with ERA5-based data, and evaluated against traditional and ML baselines (e.g., ECMWF HRES, GraphCast).

Result: PARADIS achieves state-of-the-art forecast skill at 1° resolution while requiring less than a GPU-month of training. It meets or exceeds the performance of 0.25° baselines (both traditional and ML), including ECMWF HRES and DeepMind’s GraphCast, on ERA5-based benchmarks.

Conclusion: A physics-informed, modular neural architecture can match or surpass high-resolution baselines with substantially lower training cost, highlighting the value of explicit advection-diffusion-reaction decomposition and differentiable semi-Lagrangian transport for weather prediction.

Abstract: Recent machine-learning approaches to weather forecasting often employ a monolithic architecture, where distinct physical mechanisms (advection, transport), diffusion-like mixing, thermodynamic processes, and forcing are represented implicitly within a single large network. This representation is particularly problematic for advection, where long-range transport must be treated with expensive global interaction mechanisms or through deep, stacked convolutional layers. To mitigate this, we present PARADIS, a physics-inspired global weather prediction model that imposes inductive biases on network behavior through a functional decomposition into advection, diffusion, and reaction blocks acting on latent variables. We implement advection through a Neural Semi-Lagrangian operator that performs trajectory-based transport via differentiable interpolation on the sphere, enabling end-to-end learning of both the latent modes to be transported and their characteristic trajectories. Diffusion-like processes are modeled through depthwise-separable spatial mixing, while local source terms and vertical interactions are modeled via pointwise channel interactions, enabling operator-level physical structure. PARADIS provides state-of-the-art forecast skill at a fraction of the training cost. On ERA5-based benchmarks, the 1 degree PARADIS model, with a total training cost of less than a GPU month, meets or exceeds the performance of 0.25 degree traditional and machine-learning baselines, including the ECMWF HRES forecast and DeepMind's GraphCast.

</details>


### [158] [A Federated Generalized Expectation-Maximization Algorithm for Mixture Models with an Unknown Number of Components](https://arxiv.org/abs/2601.21160)
*Michael Ibrahim,Nagi Gebraeel,Weijun Xie*

Main category: cs.LG

TL;DR: 提出 FedGEM，在未知全局簇数K的联邦聚类中，通过本地EM和不确定性集合，服务器推断簇重叠和全局簇数，具有理论收敛性并在实验中与集中EM相当，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在数据分布异质且簇集合潜在重叠且K未知的联邦聚类问题，需在各客户端局部EM基础上推断全局簇结构及全局簇数，并保持理论收敛性。

Method: 各客户端执行本地EM步，围绕局部分量构建不确定性集合；服务器利用这些集合推断跨客户端的簇重叠并通过闭式公式推断全局簇数；对一般GMM给出概率收敛性分析，对各向同性GMM提供低复杂度的迭代实现；并通过数值实验验证性能。

Result: 理论上给出概率收敛 guarantees；在仿真/真实数据上接近集中EM的性能，并优于若干现有的联邦聚类方法。

Conclusion: 提出一个可行且有理论支撑的未知K联邦聚类框架（FedGEM），通过局部EM+不确定性集合实现全局簇数推断和簇重叠学习，在异质数据场景下具备良好性能。

Abstract: We study the problem of federated clustering when the total number of clusters $K$ across clients is unknown, and the clients have heterogeneous but potentially overlapping cluster sets in their local data. To that end, we develop FedGEM: a federated generalized expectation-maximization algorithm for the training of mixture models with an unknown number of components. Our proposed algorithm relies on each of the clients performing EM steps locally, and constructing an uncertainty set around the maximizer associated with each local component. The central server utilizes the uncertainty sets to learn potential cluster overlaps between clients, and infer the global number of clusters via closed-form computations. We perform a thorough theoretical study of our algorithm, presenting probabilistic convergence guarantees under common assumptions. Subsequently, we study the specific setting of isotropic GMMs, providing tractable, low-complexity computations to be performed by each client during each iteration of the algorithm, as well as rigorously verifying assumptions required for algorithm convergence. We perform various numerical experiments, where we empirically demonstrate that our proposed method achieves comparable performance to centralized EM, and that it outperforms various existing federated clustering methods.

</details>


### [159] [Efficient Simple Regret Algorithms for Stochastic Contextual Bandits](https://arxiv.org/abs/2601.21167)
*Shuai Liu,Alireza Bakhtiari,Alex Ayoub,Botao Hao,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 首次在对数/logistic contextual bandits 下给出简单后悔界的算法，Leading term 不依赖 κ，且扩展到随机化 Thompson Sampling；有限动作集可行，实验验证。


<details>
  <summary>Details</summary>
Motivation: 弥补对数（logistic）情境下简单后悔的理论空白，结合自圆函数分析与上下文线性带权思想，推动理论与可实践性兼容的界限。

Method: 结合自圆分析与上下文线性带权框架，设计统一的致简但有效的算法以实现简单后悔界；提出一个新颖的用于简单后悔的 Thompson Sampling 变体，并将其扩展到 logistic 情况。

Result: 简单后悔界为 Ō(d/√T)，随机化算法达到 Ō(d^{3/2}/√T)，Leading term 不含 κ；对有限动作集完全可实现；提供了与理论相符的实验结果。

Conclusion: 首次给出在随机化与非线性（logistic）上下文带权设置中的简单后悔界，且与线性情形结果一致性良好，实验支持理论，方法具实际可行性。

Abstract: We study stochastic contextual logistic bandits under the simple regret objective. While simple regret guarantees have been established for the linear case, no such results were previously known for the logistic setting. Building on ideas from contextual linear bandits and self-concordant analysis, we propose the first algorithm that achieves simple regret $\tilde{\mathcal{O}}(d/\sqrt{T})$. Notably, the leading term of our regret bound is free of the constant $κ= \mathcal O(\exp(S))$, where $S$ is a bound on the magnitude of the unknown parameter vector. The algorithm is shown to be fully tractable when the action set is finite. We also introduce a new variant of Thompson Sampling tailored to the simple-regret setting. This yields the first simple regret guarantee for randomized algorithms in stochastic contextual linear bandits, with regret $\tilde{\mathcal{O}}(d^{3/2}/\sqrt{T})$. Extending this method to the logistic case, we obtain a similarly structured Thompson Sampling algorithm that achieves the same regret bound -- $\tilde{\mathcal{O}}(d^{3/2}/\sqrt{T})$ -- again with no dependence on $κ$ in the leading term. The randomized algorithms, as expected, are cheaper to run than their deterministic counterparts. Finally, we conducted a series of experiments to empirically validate these theoretical guarantees.

</details>


### [160] [The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset](https://arxiv.org/abs/2601.21170)
*Augusto Santos,Teresa Santos,Catarina Rodrigues,José M. F. Moura*

Main category: cs.LG

TL;DR: 提出一种基于经验协方差/精度矩阵幂的单参数估计量族的特征表示学习方法，用于在未知产生过程与部分可观测的情形下对发作性事件进行早期检测，随后通过有监督分类模块实现预测；并给出结构一致性证明与在癫痫发作检测与用户流失预测中的实证验证，强调可解释性与可 identifiability 的平衡。


<details>
  <summary>Details</summary>
Motivation: 复杂系统中的涌现现象（如癫痫发作、客户流失、疫情暴发）通常源于隐藏的因果结构，数据生成过程未知且部分观测，使得直接利用观测数据难以捕捉潜在驱动机制。需要一种能揭示并利用潜在因果结构的学习方法，以实现对关键事件的早期、可解释的检测。

Method: 学习一个来自单参数族的特征表示，该族由经验协方差矩阵或精度矩阵的幂构成，能自适应地聚焦潜在驱动结构；在此特征表示上训练一个有监督分类器进行事件预测；理论上证明该族具有结构一致性，实证上在癫痫发作检测与 churn 预测中与基线相比具竞争力；强调在可解释性方面，最优协方差幂具有良好可识别性的证据，同时捕捉到结构性特征。

Result: 在癫痫发作检测与 churn 预测任务上实现了具竞争力的预测性能；同时揭示了最优协方差幂在可识别性与结构表征之间的折中，提升了模型的解释性。

Conclusion: 该方法提供了一种从潜在因果结构中提取有用表示并用于早期检测的系统框架，兼顾预测性能与可解释性；通过结构一致性证明与实证验证，展示了在未知与部分观测数据条件下捕获关键结构信息的可行性。

Abstract: Emergent phenomena -- onset of epileptic seizures, sudden customer churn, or pandemic outbreaks -- often arise from hidden causal interactions in complex systems. We propose a machine learning method for their early detection that addresses a core challenge: unveiling and harnessing a system's latent causal structure despite the data-generating process being unknown and partially observed. The method learns an optimal feature representation from a one-parameter family of estimators -- powers of the empirical covariance or precision matrix -- offering a principled way to tune in to the underlying structure driving the emergence of critical events. A supervised learning module then classifies the learned representation. We prove structural consistency of the family and demonstrate the empirical soundness of our approach on seizure detection and churn prediction, attaining competitive results in both. Beyond prediction, and toward explainability, we ascertain that the optimal covariance power exhibits evidence of good identifiability while capturing structural signatures, thus reconciling predictive performance with interpretable statistical structure.

</details>


### [161] [AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection](https://arxiv.org/abs/2601.21171)
*Kamal Berahmand,Saman Forouzandeh,Mehrnoush Mohammadi,Parham Moradi,Mahdi Jalili*

Main category: cs.LG

TL;DR: AC2L-GAD introduces Active Counterfactual Contrastive Learning for graph anomaly detection to address label scarcity and class imbalance. It combines information-theoretic active node selection with counterfactual generation to produce anomaly-preserving, informative positives and hard negatives, while limiting expensive counterfactuals to a subset. It achieves competitive or superior results on 9 benchmarks with ~65% computational savings over full counterfactual generation.


<details>
  <summary>Details</summary>
Motivation: Graph anomaly detection suffers from scarce labels and extreme class imbalance. Existing graph contrastive learning methods rely on random augmentations and naive negative sampling, which can break semantic consistency and yield uninformative contrasts, limiting detection performance.

Method: AC2L-GAD uses principled counterfactual reasoning. It applies information-theoretic active selection to identify structurally complex nodes and generates anomaly-preserving positive augmentations along with hard negative counterparts. Counterfactual generation is applied to a strategically selected subset to reduce computational cost (~65% savings) while maintaining detection quality.

Result: On nine benchmark datasets, including real-world financial transaction graphs from GADBench, AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.

Conclusion: AC2L-GAD demonstrates that active counterfactual reasoning combined with selective counterfactual augmentation can improve graph anomaly detection under label scarcity and imbalance, offering both improved detection and reduced computation.

Abstract: Graph anomaly detection aims to identify abnormal patterns in networks, but faces significant challenges from label scarcity and extreme class imbalance. While graph contrastive learning offers a promising unsupervised solution, existing methods suffer from two critical limitations: random augmentations break semantic consistency in positive pairs, while naive negative sampling produces trivial, uninformative contrasts. We propose AC2L-GAD, an Active Counterfactual Contrastive Learning framework that addresses both limitations through principled counterfactual reasoning. By combining information-theoretic active selection with counterfactual generation, our approach identifies structurally complex nodes and generates anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts, while restricting expensive counterfactual generation to a strategically selected subset. This design reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Experiments on nine benchmark datasets, including real-world financial transaction graphs from GADBench, show that AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.

</details>


### [162] [Breaking the Reasoning Horizon in Entity Alignment Foundation Models](https://arxiv.org/abs/2601.21174)
*Yuanning Cui,Zequn Sun,Wei Hu,Kexuan Xin,Zhangjie Fu*

Main category: cs.LG

TL;DR: 提出一种以并行编码和锚点引导的信息传递的EA基础模型，结合合并关系图和可学习交互模块，显著提高对未见KG的泛化能力与匹配精度。


<details>
  <summary>Details</summary>
Motivation: 现有实体对齐模型缺乏可迁移性，直接将图 foundation 模型用于EA效果有限，原因在于“推理视界差距”：EA需要在稀疏、异构的KG中捕捉长程依赖，而这不是简单的链接预测所能覆盖。

Method: 核心在于并行编码策略：以种子EA对作为本地锚点，初始化并同时编码两条并行信息流；进行锚点条件的信息传递；引入合并关系图以建模全局依赖；设计可学习的交互模块实现精确匹配。

Result: 大量实验验证该框架在未见KG上的强泛化能力，且显著优于 baselines，证明有效缓解了推理轨迹长、全局依赖不足等问题。

Conclusion: 通过锚点引导的局部信息流与全局合并关系的结合，以及可学习的交互匹配，提升EA在大规模异构KG上的可迁移性和性能。

Abstract: Entity alignment (EA) is critical for knowledge graph (KG) fusion. Existing EA models lack transferability and are incapable of aligning unseen KGs without retraining. While using graph foundation models (GFMs) offer a solution, we find that directly adapting GFMs to EA remains largely ineffective. This stems from a critical "reasoning horizon gap": unlike link prediction in GFMs, EA necessitates capturing long-range dependencies across sparse and heterogeneous KG structuresTo address this challenge, we propose a EA foundation model driven by a parallel encoding strategy. We utilize seed EA pairs as local anchors to guide the information flow, initializing and encoding two parallel streams simultaneously. This facilitates anchor-conditioned message passing and significantly shortens the inference trajectory by leveraging local structural proximity instead of global search. Additionally, we incorporate a merged relation graph to model global dependencies and a learnable interaction module for precise matching. Extensive experiments verify the effectiveness of our framework, highlighting its strong generalizability to unseen KGs.

</details>


### [163] [Rethinking Refinement: Correcting Generative Bias without Noise Injection](https://arxiv.org/abs/2601.21182)
*Xin Peng,Ang Gao*

Main category: cs.LG

TL;DR: 提出 Bi-stage Flow Refinement (BFR) 框架，通过两阶段改进：潜在空间对齐和数据空间细化，保持原始 ODE 路径，显著提升样本质量和覆盖率。MNIST 基线 FID 3.95，经潜在空间 refinement 达到 1.46 的 state-of-the-art FID，且仅需 1-NFE。


<details>
  <summary>Details</summary>
Motivation: 高维生成模型（扩散/流式等）存在系统性偏差，导致样本质量下降。现有改进往往需要噪声注入或多步重采样，成本高且难以保留采样轨迹。需要一种后处理、无噪声注入且高效的偏差矫正方法。

Method: 提出基于流量匹配的 Bi-stage Flow Refinement（BFR）框架：第一阶段在潜在空间对齐以实现近似可逆生成器；第二阶段在数据空间进行轻量化的增强（使用轻量化增广），两阶段都进行确定性修正，保持原始 ODE 轨迹，不扰动采样过程。

Result: 在 MNIST、CIFAR-10、FFHQ（256x256）上实现样本保真度与覆盖率的显著提升。以 MNIST 为例，基线 FID 为 3.95，通过潜在空间 refinement 可达到 1.46 的 state-of-the-art FID，并且仅需一次额外函数评估（1-NFE），同时保持样本多样性。

Conclusion: BFR 提供一个高效且稳定的后处理偏差矫正方案，区别于以往通过扰动采样动力学的改进，保持原始 ODE 路径并实现确定性修正，适用于高维生成任务的改进需求。

Abstract: Generative models, including diffusion and flow-based models, often exhibit systematic biases that degrade sample quality, particularly in high-dimensional settings. We revisit refinement methods and show that effective bias correction can be achieved as a post-hoc procedure, without noise injection or multi-step resampling of the sampling process. We propose a flow-matching-based \textbf{Bi-stage Flow Refinement (BFR)} framework with two refinement strategies operating at different stages: latent space alignment for approximately invertible generators and data space refinement trained with lightweight augmentations. Unlike previous refiners that perturb sampling dynamics, BFR preserves the original ODE trajectory and applies deterministic corrections to generated samples. Experiments on MNIST, CIFAR-10, and FFHQ at 256x256 resolution demonstrate consistent improvements in fidelity and coverage; notably, starting from base samples with FID 3.95, latent space refinement achieves a \textbf{state-of-the-art} FID of \textbf{1.46} on MNIST using only a single additional function evaluation (1-NFE), while maintaining sample diversity.

</details>


### [164] [Rethinking Self-Training Based Cross-Subject Domain Adaptation for SSVEP Classification](https://arxiv.org/abs/2601.21203)
*Weiguang Wang,Yong Liu,Yingjie Gao,Guangyuan Xu*

Main category: cs.LG

TL;DR: 跨主体自训练的SSVEP-BCI框架，融合FBEA、CSST与TFA-CL，在Benchmark和BETA数据集上实现最先进性能，且对不同信号长度鲁棒。


<details>
  <summary>Details</summary>
Motivation: SSVEP-BCI在受试者间存在显著变异，且昂贵的用户级标注限制性能提升，需要有效的跨受试者迁移与自训练策略来提升识别率。

Method: 提出Filter-Bank Euclidean Alignment（FBEA）用以利用SSVEP滤波器组的频信息；提出跨受试者自训练框架CSST，包括带对抗学习的预训练（PTAL）和双集合自训练（DEST），并辅以时频对比学习的TFA-CL以增强多视角特征判别。

Result: 在Benchmark与BETA数据集上进行大规模实验，结果表明该方法在不同信号长度下均达到或超过当前最优，具有领先的性能。

Conclusion: 该框架有效缓解跨受试者差异与标注成本，提升SSVEP解码的鲁棒性和泛化能力，且在公开数据集上展现出强大优势。

Abstract: Steady-state visually evoked potentials (SSVEP)-based brain-computer interfaces (BCIs) are widely used due to their high signal-to-noise ratio and user-friendliness. Accurate decoding of SSVEP signals is crucial for interpreting user intentions in BCI applications. However, signal variability across subjects and the costly user-specific annotation limit recognition performance. Therefore, we propose a novel cross-subject domain adaptation method built upon the self-training paradigm. Specifically, a Filter-Bank Euclidean Alignment (FBEA) strategy is designed to exploit frequency information from SSVEP filter banks. Then, we propose a Cross-Subject Self-Training (CSST) framework consisting of two stages: Pre-Training with Adversarial Learning (PTAL), which aligns the source and target distributions, and Dual-Ensemble Self-Training (DEST), which refines pseudo-label quality. Moreover, we introduce a Time-Frequency Augmented Contrastive Learning (TFA-CL) module to enhance feature discriminability across multiple augmented views. Extensive experiments on the Benchmark and BETA datasets demonstrate that our approach achieves state-of-the-art performance across varying signal lengths, highlighting its superiority.

</details>


### [165] [Soft Quantization: Model Compression Via Weight Coupling](https://arxiv.org/abs/2601.21219)
*Daniel T. Bernstein,Luca Di Carlo,David Schwab*

Main category: cs.LG

TL;DR: 引入权重之间的短程吸引耦合在训练中实现软量化的混合精度离散化，优于直方图均衡后训练量化（HTQ）在ResNet-20/CIFAR-10上的表现。


<details>
  <summary>Details</summary>
Motivation: 在不显著增加超参数负担的情况下实现高效模型量化，探索训练时耦合对权重分布离散化与压缩-泛化权衡的影响，并提供一个用于研究高维损失面的新工具。

Method: 在训练过程中对权重引入短程吸引耦合，使用两个额外超参数控制耦合强度和作用范围；在ResNet-20/CIFAR-10等设置下实现软量化。

Result: 在一个合适的超参数范围内，软量化优于直方图均衡的后训练量化，且实现了混合精度分布；提出了一个灵活的模型压缩流水线和用于研究压缩-泛化关系的新工具。

Conclusion: 软量化提供一种新颖、有效的训练时量化策略，具有广泛潜在应用，并为理解压缩与高维损失面的关系提供新视角。

Abstract: We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model's weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our "soft quantization'' scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.

</details>


### [166] [PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations](https://arxiv.org/abs/2601.21234)
*Kaiyuan Tan,Kendra Givens,Peilun Li,Thomas Beckers*

Main category: cs.LG

TL;DR: PHDME combines a Gaussian-process port-Hamiltonian system (GP-dPHS) with diffusion models to forecast trajectories from sparse data and incomplete physics. It builds a physics-consistent synthetic dataset and enforces a physics residual loss, enabling fast amortized sampling and reliable uncertainty via split conformal calibration.


<details>
  <summary>Details</summary>
Motivation: To improve reliability of diffusion-model-based forecasting in settings with limited observations and partially known dynamics, by embedding a physically structured prior without requiring full closed-form equations.

Method: 1) Learn GP-dPHS on limited data to capture an energy-based dynamical representation. 2) Generate a physics-consistent artificial dataset from GP-dPHS to train a diffusion model. 3) Incorporate a physics residual loss into diffusion training. 4) Use the trained diffusion model as an amortized sampler/forecaster. 5) Apply split conformal calibration to quantify uncertainty in predictions.

Result: Greater accuracy and physical consistency under data scarcity on PDE benchmarks and a real-world spring system.

Conclusion: PHDME provides a practical framework for reliable forecasting with sparse observations and incomplete physics by fusing energy-based Hamiltonian priors with diffusion models and principled uncertainty calibration.

Abstract: Diffusion models provide expressive priors for forecasting trajectories of dynamical systems, but are typically unreliable in the sparse data regime. Physics-informed machine learning (PIML) improves reliability in such settings; however, most methods require \emph{explicit governing equations} during training, which are often only partially known due to complex and nonlinear dynamics. We introduce \textbf{PHDME}, a port-Hamiltonian diffusion framework designed for \emph{sparse observations} and \emph{incomplete physics}. PHDME leverages port-Hamiltonian structural prior but does not require full knowledge of the closed-form governing equations. Our approach first trains a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) on limited observations to capture an energy-based representation of the dynamics. The GP-dPHS is then used to generate a physically consistent artificial dataset for diffusion training, and to inform the diffusion model with a structured physics residual loss. After training, the diffusion model acts as an amortized sampler and forecaster for fast trajectory generation. Finally, we apply split conformal calibration to provide uncertainty statements for the generated predictions. Experiments on PDE benchmarks and a real-world spring system show improved accuracy and physical consistency under data scarcity.

</details>


### [167] [Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification](https://arxiv.org/abs/2601.21244)
*Yiju Guo,Tianyi Hu,Zexu Sun,Yankai Lin*

Main category: cs.LG

TL;DR: 通过去除干扰Token的采样框架LENS，在RLVR中提升探索效率和收敛速度，相较GRPO获得3.88%平均增益和1.6×加速。


<details>
  <summary>Details</summary>
Motivation: 在受限rollout预算下，RLVR的探索效率低下，错误更多源于少量干扰Token导致的误导性提示；因此需要降低提示干扰以提升采样成功率和训练稳定性。

Method: 提出Less Noise Sampling Framework (LENS)：1) 通过提示阶段识别并去除干扰Token；2) 将 purification 阶段的成功rollouts用于监督原始有噪声提示下的策略优化，使模型在真实场景中学会忽略干扰。

Result: 实验表明LENS显著优于GRPO，达到3.88%的平均增益，并实现超过1.6×的训练速度提升。

Conclusion: 裁剪干扰Token对提升 rollout 效率至关重要，为RLVR研究提供新方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.

</details>


### [168] [EGAM: Extended Graph Attention Model for Solving Routing Problems](https://arxiv.org/abs/2601.21281)
*Licheng Wang,Yuzi Yan,Mingtao Huang,Yuan Shen*

Main category: cs.LG

TL;DR: 提出扩展图注意模型EGAM，通过多头点积注意更新节点与边嵌入，结合自回归编码器-解码器与策略梯度训练，在多种路由问题中达到或超过现有方法，且在高约束情形下表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 弥补GAM仅利用节点特征、忽略边信息和复杂图结构的局限，提升对约束性路由问题的求解性能。

Method: 扩展GAM，使用多头点积注意力同时更新节点和边嵌入；采用自回归编码器-解码器架构；通过包含定制基线的策略梯度强化学习进行训练。

Result: 在多种路由问题上与现有方法持平或超出之，尤其在高度约束的问题上表现出色，显示对复杂图结构的处理能力和效率。

Conclusion: 通过将边嵌入纳入注意力更新并引入自回归解码/强化学习训练，EGAM提升了NCO在路由问题上的性能，证明了对复杂约束的鲁棒性与扩展性。

Abstract: Neural combinatorial optimization (NCO) solvers, implemented with graph neural networks (GNNs), have introduced new approaches for solving routing problems. Trained with reinforcement learning (RL), the state-of-the-art graph attention model (GAM) achieves near-optimal solutions without requiring expert knowledge or labeled data. In this work, we generalize the existing graph attention mechanism and propose the extended graph attention model (EGAM). Our model utilizes multi-head dot-product attention to update both node and edge embeddings, addressing the limitations of the conventional GAM, which considers only node features. We employ an autoregressive encoder-decoder architecture and train it with policy gradient algorithms that incorporate a specially designed baseline. Experiments show that EGAM matches or outperforms existing methods across various routing problems. Notably, the proposed model demonstrates exceptional performance on highly constrained problems, highlighting its efficiency in handling complex graph structures.

</details>


### [169] [DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher](https://arxiv.org/abs/2601.21283)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: DUET 提出了一种基于蒸馏的未学习方法，通过一个对不良知识拒绝的提示引导教师模型来训练学生模型，以在忘记能力与保留通用知识之间取得平衡，且在数据效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有未学习方法在计算开销、遗忘效果与知识保留之间的权衡，克服基于提示的脆弱性和合规性挑战。

Method: 训练一个学生模型来模仿一个进行提示引导、拒绝不良知识的教师模型的行为，通过蒸馏保留通用领域知识，同时实现对不良知识的忽略或拒绝。

Result: 在现有基准与扩展评估协议上，DUET 在忘记性和效用保持方面更优，且数据效率是现有方法的数倍至数量级提升。

Conclusion: 将蒸馏与提示导向相结合，提供一种高效、可扩展的未学习框架，提升可信AI的实践性。

Abstract: LLM unlearning is a technique to remove the impacts of undesirable knowledge from the model without retraining from scratch, which is indispensable towards trustworthy AI. Existing unlearning methods face significant limitations: conventional tuning-based unlearning is computationally heavy and prone to catastrophic forgetting. In contrast, in-contextualized unlearning is lightweight for precise unlearning but vulnerable to prompt removal or reverse engineering attacks. In response, we propose Distilled Unlearning from an Efficient Teacher (DUET), a novel distillation-based unlearning method that combines the merits of these two lines of work. It learns a student model to imitate the behavior of a prompt-steered teacher that effectively refuses undesirable knowledge generation while preserving general domain knowledge. Extensive evaluations on existing benchmarks with our enriched evaluation protocols demonstrate that DUET achieves higher performance in both forgetting and utility preservation, while being orders of magnitude more data-efficient than state-of-the-art unlearning methods.

</details>


### [170] [Zenith: Scaling up Ranking Models for Billion-scale Livestreaming Recommendation](https://arxiv.org/abs/2601.21285)
*Ruifeng Zhang,Zexi Huang,Zikai Wang,Ke Sun,Bohang Zheng,Zhen Ouyang,Huimin Xie,Phil Shen,Junlin Zhang,Wentao Guo,Qinglei Wang*

Main category: cs.LG

TL;DR: Zenith 是一个可扩展且高效的排序架构，通过 Token Fusion 和 Token Boost 处理少量高维 Prime Tokens，旨在高效捕捉复杂特征交互，并在 TikTok Live 的真实场景中实现多项性能提升。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统中，准确捕捉特征交互并提升模型容量对预测性能至关重要，但现有方法在提升容量时往往伴随推理延迟与效率瓶颈。

Method: 提出 Zenith 框架，包含 Token Fusion、Token Boost 两个模块，处理少量高维的 Prime Tokens，并通过提升 token 异质性实现更好的可扩展性。

Result: 在 TikTok Live 的 A/B 测试中，Zenith 在在线 CTR AUC 上提升 1.05%，Logloss 降低 1.10%；在 Quality Watch Session/User 与 Quality Watch Duration/User 指标上分别实现 9.93% 与 8.11% 的增益。

Conclusion: Zenith 展示了可扩展且高效的排序能力，能够在不显著增加推理成本的前提下提升真实世界关键指标，适用于大规模推荐场景。

Abstract: Accurately capturing feature interactions is essential in recommender systems, and recent trends show that scaling up model capacity could be a key driver for next-level predictive performance. While prior work has explored various model architectures to capture multi-granularity feature interactions, relatively little attention has been paid to efficient feature handling and scaling model capacity without incurring excessive inference latency. In this paper, we address this by presenting Zenith, a scalable and efficient ranking architecture that learns complex feature interactions with minimal runtime overhead. Zenith is designed to handle a few high-dimensional Prime Tokens with Token Fusion and Token Boost modules, which exhibits superior scaling laws compared to other state-of-the-art ranking methods, thanks to its improved token heterogeneity. Its real-world effectiveness is demonstrated by deploying the architecture to TikTok Live, a leading online livestreaming platform that attracts billions of users globally. Our A/B test shows that Zenith achieves +1.05%/-1.10% in online CTR AUC and Logloss, and realizes +9.93% gains in Quality Watch Session / User and +8.11% in Quality Watch Duration / User.

</details>


### [171] [TimeSliver : Symbolic-Linear Decomposition for Explainable Time Series Classification](https://arxiv.org/abs/2601.21289)
*Akash Pandey,Payal Mohapatra,Wei Chen,Qi Zhu,Sinan Keten*

Main category: cs.LG

TL;DR: TimeSliver 提出将原始时间序列与符号抽象共同构建表示，使每个时间片段对预测的贡献线性编码，从而实现可解释的时间分段重要性评估；在时间序列分类任务上优于其他时序属性方法11%，在26个UEA数据集上达到接近SOTA的精度。


<details>
  <summary>Details</summary>
Motivation: 解决基于梯度/特征归因的后验解释对参考状态敏感、跨数据集泛化差的局限，以及基于自注意力的时间重要性解释往往不 faithful 的问题，提升对时间段贡献的可解释性与稳健性。

Method: 将原始时间序列数据与符号抽象联合构建一个保持原始时间结构的表示；表示中的每个元素线性编码对最终预测的时间段贡献；通过该表示进行时间点级重要性评分；适用于时间序列分类。

Result: 在7个合成与真实世界的多变量时间序列数据集上，TimeSliver 在时序归因方法上提升约11%；在26个UEA基准数据集上，预测性能仅比最优方法低2%之内，接近SOTA。

Conclusion: TimeSliver 为通用时间序列分类提供一个强大且可解释的框架，兼具解释性与竞争性能。

Abstract: Identifying the extent to which every temporal segment influences a model's predictions is essential for explaining model decisions and increasing transparency. While post-hoc explainable methods based on gradients and feature-based attributions have been popular, they suffer from reference state sensitivity and struggle to generalize across time-series datasets, as they treat time points independently and ignore sequential dependencies. Another perspective on explainable time-series classification is through interpretable components of the model, for instance, leveraging self-attention mechanisms to estimate temporal attribution; however, recent findings indicate that these attention weights often fail to provide faithful measures of temporal importance. In this work, we advance this perspective and present a novel explainability-driven deep learning framework, TimeSliver, which jointly utilizes raw time-series data and its symbolic abstraction to construct a representation that maintains the original temporal structure. Each element in this representation linearly encodes the contribution of each temporal segment to the final prediction, allowing us to assign a meaningful importance score to every time point. For time-series classification, TimeSliver outperforms other temporal attribution methods by 11% on 7 distinct synthetic and real-world multivariate time-series datasets. TimeSliver also achieves predictive performance within 2% of state-of-the-art baselines across 26 UEA benchmark datasets, positioning it as a strong and explainable framework for general time-series classification.

</details>


### [172] [Physics-Guided Tiny-Mamba Transformer for Reliability-Aware Early Fault Warning](https://arxiv.org/abs/2601.21293)
*Changyu Li,Dingcheng Huang,Kexuan Yao,Xiaoya Ni,Lijuan Shen,Fei Luo*

Main category: cs.LG

TL;DR: 提出一个物理引导的紧凑型 Tiny-Mamba Transformer，用于在线状态监测的早期预警，结合 EVT 校准阈值和带有物理对齐的表示，以在非平稳工况、跨域传输与数据不平衡下提高性能并实现可解释性。


<details>
  <summary>Details</summary>
Motivation: 在非平稳工况、速度/负载/传感器域移位以及严重类别不平衡下，需要早期警报且假警率可预测且较低，同时实现可解释和部署友好的机制。

Method: 提出三分支编码器：深度可分离卷积干段捕捉微小瞬态，Tiny-Mamba 状态空间分支建模近线性长程动态，轻量级局部 Transformer 编码跨通道共振。推导时域到谱域的解析映射，将注意力谱与经典的故障阶带对齐，给出带对齐分数以体现物理可行性。对健康分数采用极值理论 EVT 校准，得到目标假警强度下的阈值；引入双阈值滞后和最小保持时间抑制 chatter。此外在无泄漏流式协议、对 missed detections 进行右截断的条件下，在 CWRU、Paderborn、XJTU-SY 与工业试点数据上评估，展现高精度召回 AUC、竞争性/更优的 ROC AUC，以及在等效假警强度下更短的平均检测时间，并具备强跨域迁移能力。

Result: 在不平衡条件下，PG-TMT 取得更高的精确召回 AUC、竞争性或更好的 ROC AUC，以及在匹配的假警强度下更短的平均检测时间，且具备良好的跨域 transfer 能力。

Conclusion: 通过耦合物理对齐表示与 EVT 校准决策规则，PG-TMT 提供了可校准、可解释且易于部署的可靠性导向预警解决方案。

Abstract: Reliability-centered prognostics for rotating machinery requires early warning signals that remain accurate under nonstationary operating conditions, domain shifts across speed/load/sensors, and severe class imbalance, while keeping the false-alarm rate small and predictable. We propose the Physics-Guided Tiny-Mamba Transformer (PG-TMT), a compact tri-branch encoder tailored for online condition monitoring. A depthwise-separable convolutional stem captures micro-transients, a Tiny-Mamba state-space branch models near-linear long-range dynamics, and a lightweight local Transformer encodes cross-channel resonances. We derive an analytic temporal-to-spectral mapping that ties the model's attention spectrum to classical bearing fault-order bands, yielding a band-alignment score that quantifies physical plausibility and provides physics-grounded explanations. To ensure decision reliability, healthy-score exceedances are modeled with extreme-value theory (EVT), which yields an on-threshold achieving a target false-alarm intensity (events/hour); a dual-threshold hysteresis with a minimum hold time further suppresses chatter. Under a leakage-free streaming protocol with right-censoring of missed detections on CWRU, Paderborn, XJTU-SY, and an industrial pilot, PG-TMT attains higher precision-recall AUC (primary under imbalance), competitive or better ROC AUC, and shorter mean time-to-detect at matched false-alarm intensity, together with strong cross-domain transfer. By coupling physics-aligned representations with EVT-calibrated decision rules, PG-TMT delivers calibrated, interpretable, and deployment-ready early warnings for reliability-centric prognostics and health management.

</details>


### [173] [Missing-Data-Induced Phase Transitions in Spectral PLS for Multimodal Learning](https://arxiv.org/abs/2601.21294)
*Anders Gjølbye,Ida Kargaard,Emma Kargaard,Lars Kai Hansen*

Main category: cs.LG

TL;DR: 在高维带缺失的多模态数据中，PLS-SVD在逐元素独立缺失时的相对表现：掩蔽后的互协方差矩阵近似为带信号的矩形随机矩阵，信号被sqrt(ρ)衰减，出现BBP型相变，阈值以上可恢复潜在共享方向，且给出解析的重叠度公式；数值和半合成数据验证相图和恢复曲线。


<details>
  <summary>Details</summary>
Motivation: 系统地分析带独立缺失的PLS-SVD在高维“脉冲”型（spiked）模型中的行为，量化缺失率对可恢复性的影响，给出相变阈值和重叠度的解析表达，用于多模态数据分析与缺失数据处理的指南。

Method: 将掩蔽后的互协方差建模为一个带信号的矩形随机矩阵，经过适当归一化后，信号强度被乘以√ρ。通过BBP（Baik–Ben Arous–Péché）类型分析推导特征值-特征向量相变，以及主成分与潜在共同方向的解析重叠度；覆盖纵横比、信号强度和缺失比例的相图。

Result: 得到严格的BBP型相变结论：低于临界信噪比，主奇向量与真实潜在方向无关；高于临界信噪比，主奇向量与潜在方向存在非平庸的一致性，给出闭式的极限重叠度表达；通过仿真和半合成多模态实验验证相图和恢复曲线，覆盖不同纵横比、信号强度和缺失水平。

Conclusion: 在缺失数据环境下，PLS-SVD仍能在阈值以上有效恢复潜在共享结构，提供明确的相变界限和重叠度公式，为多模态学习中的缺失数据鲁棒性提供理论支撑与实证指引。

Abstract: Partial Least Squares (PLS) learns shared structure from paired data via the top singular vectors of the empirical cross-covariance (PLS-SVD), but multimodal datasets often have missing entries in both views. We study PLS-SVD under independent entry-wise missing-completely-at-random masking in a proportional high-dimensional spiked model. After appropriate normalization, the masked cross-covariance behaves like a spiked rectangular random matrix whose effective signal strength is attenuated by $\sqrtρ$, where $ρ$ is the joint entry retention probability. As a result, PLS-SVD exhibits a sharp BBP-type phase transition: below a critical signal-to-noise threshold the leading singular vectors are asymptotically uninformative, while above it they achieve nontrivial alignment with the latent shared directions, with closed-form asymptotic overlap formulas. Simulations and semi-synthetic multimodal experiments corroborate the predicted phase diagram and recovery curves across aspect ratios, signal strengths, and missingness levels.

</details>


### [174] [Grounding and Enhancing Informativeness and Utility in Dataset Distillation](https://arxiv.org/abs/2601.21296)
*Shaobo Wang,Yantai Yang,Guo Chen,Peiru Li,Kaixin Li,Yufa Zhou,Zhaorun Chen,Linfeng Zhang*

Main category: cs.LG

TL;DR: 在统计理论框架下提出 InfoUtil，通过信息性与效用性的平衡对数据集蒸馏进行建模，并结合Shapley值与梯度范数实现信息提取与样本选择，在ImageNet-1K上提升6.1%。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法在效率与质量之间取得一定折衷，但原始数据与合成数据之间的关系尚未被充分理论化。本研究旨在建立一个以信息性(info)和效用性(utility)为核心的最优数据蒸馏框架。

Method: 提出InfoUtil框架，包含两大组件： (1) 基于博弈论的信息性最大化，使用Shapley值归因提取关键信息；(2) 基于梯度范数的全局影响力样本选择，实现效用最大化。

Result: 在ImageNet-1K数据集、ResNet-18上，相较于前一代SOTA，性能提升约6.1%。

Conclusion: 提供一个理论驱动的、兼顾信息性与效用性的Dataset Distillation框架，提升合成数据的代表性与训练效果。

Abstract: Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillation-based dataset distillation within a solid theoretical framework. We introduce the concepts of Informativeness and Utility, capturing crucial information within a sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, a framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm. These components ensure that the distilled dataset is both informative and utility-optimized. Experiments demonstrate that our method achieves a 6.1\% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18.

</details>


### [175] [Achieving $\varepsilon^{-2}$ Dependence for Average-Reward Q-Learning with a New Contraction Principle](https://arxiv.org/abs/2601.21301)
*Zijun Chen,Zaiwei Chen,Nian Si,Shengbo Wang*

Main category: cs.LG

TL;DR: 在平均奖励的马尔可夫决策过程（MDP）中，作者通过对“懒化”动态引入实例相关的半范数，使Bellman算子在该半范数下变为一步收缩，从而实现同步与异步Q学习的最佳样本复杂度。对于满足可达性的系统，给出	ilde{O}(	ext{ε}^{-2}) 的样本复杂度界（忽略对数因子）。


<details>
  <summary>Details</summary>
Motivation: 平均奖励MDP中缺乏收缩性是分析Q学习收敛的核心难题。现有非渐进结果要么通过强假设强制半范数收缩，要么依赖折扣化或 episodic 的近似，这些要么需要未知参数，要么导致次优的样本复杂度。本研究旨在在可达性假设下，直接给出最优样本复杂度的Q学习分析。

Method: 引入一个实例相关的半范数，并通过对MDP进行懒化变换（在当前状态停留的固定概率）使Bellman算子在该半范数下成为一步收缩。对简单变种的同步和异步Q学习在懒化动态下的采样过程进行分析，证明在该框架下可获得接近最优的收敛速率。

Result: 在可达性假设下，获得同步和异步Q学习的非渐近“近似最优”结果：样本复杂度为 	ilde{O}(	ext{ε}^{-2})，平方阶依赖于误差精度，且至多对数因子被抑制。核心是构造的实例相关半范数及懒化后的一步收缩。

Conclusion: 通过懒化变换与实例相关半范数，解决平均奖励MDP中缺乏收缩的问题，建立了同步与异步Q学习在可达性条件下的最优样本复杂度界；为平均奖励情形下的Q学习分析提供了新的范式与工具，后续可扩展到更广的MDP设定或放宽可达性假设。

Abstract: We present the convergence rates of synchronous and asynchronous Q-learning for average-reward Markov decision processes, where the absence of contraction poses a fundamental challenge. Existing non-asymptotic results overcome this challenge by either imposing strong assumptions to enforce seminorm contraction or relying on discounted or episodic Markov decision processes as successive approximations, which either require unknown parameters or result in suboptimal sample complexity. In this work, under a reachability assumption, we establish optimal $\widetilde{O}(\varepsilon^{-2})$ sample complexity guarantees (up to logarithmic factors) for a simple variant of synchronous and asynchronous Q-learning that samples from the lazified dynamics, where the system remains in the current state with some fixed probability. At the core of our analysis is the construction of an instance-dependent seminorm and showing that, after a lazy transformation of the Markov decision process, the Bellman operator becomes one-step contractive under this seminorm.

</details>


### [176] [The Surprising Difficulty of Search in Model-Based Reinforcement Learning](https://arxiv.org/abs/2601.21306)
*Wei-Di Chang,Mikael Henaff,Brandon Amos,Gregory Dudek,Scott Fujimoto*

Main category: cs.LG

TL;DR: 本研究揭示：在模型基于强化学习中的搜索并非对策略的无缝替代；即便模型高度准确，搜索也可能降低性能。关键在于缓解分布偏移，而非仅提升模型或价值函数精度；据此提出关键技术，并在多项基准任务上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为长期预测误差与累积误差是模型基于强化学习的主要障碍。本文挑战该观点，强调分布偏移才是决定搜索效果的关键因素。

Method: 对搜索在模型基于RL中的影响进行系统分析，提出并组合一组致力于缓解分布偏移的关键技术，构建更有效的搜索流程；在多个流行的基准域上进行评估。

Result: 实验表明，缓解分布偏移的重要性超越提升模型精度或价值函数精度；将所提技术应用于搜索后，在多域基准任务上达到或接近最先进水平。

Conclusion: 要让搜索在模型基于RL中真正有效，需围绕减少分布偏移设计并优化相关技术，从而实现显著的性能提升。

Abstract: This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.

</details>


### [177] [Transferable Graph Condensation from the Causal Perspective](https://arxiv.org/abs/2601.21309)
*Huaming Du,Yijie Huang,Su Yao,Yiying Wang,Yueyang Zhou,Jingwen Yang,Jinshi Zhang,Han Ji,Yu Zhao,Guisong Liu,Hegui Zhang,Carl Yang,Gang Kou*

Main category: cs.LG

TL;DR: 提出 TGCC，一种基于因果不变性的可转移图数据集凝练方法，通过空间域因果干预提取不变特征、增强凝聚以保留结构与特征信息，并在光谱域通过对比学习注入不变特征，从而实现对大规模图数据集的可转移压缩。


<details>
  <summary>Details</summary>
Motivation: 现有图数据集凝练在跨任务/跨域场景下表现不足；随着数据集规模增大，训练成本高，需得到信息丰富且可迁移的紧凑数据集。

Method: 分三步：1) 从图的空间域通过因果干预提取领域不变的因果特征；2) 进行增强型凝聚操作，全面捕捉原图的结构与特征信息；3) 通过光谱域增强对比学习，将因果不变特征注入凝练图，保留原图的因果信息。

Result: 在五个公开数据集和新提出的 FinReport 数据集上，TGCC 在跨任务与跨域复杂场景相比现有方法提升可达 13.41%，并在单数据集与任务场景下达成 5/6 数据集的状态-of-the-art。

Conclusion: TGCC 提供有效且可转移的凝练数据集，能在跨任务/跨域场景保持因果信息，提升跨域鲁棒性和跨任务转移性能；在标准场景也具备强竞争力。

Abstract: The increasing scale of graph datasets has significantly improved the performance of graph representation learning methods, but it has also introduced substantial training challenges. Graph dataset condensation techniques have emerged to compress large datasets into smaller yet information-rich datasets, while maintaining similar test performance. However, these methods strictly require downstream applications to match the original dataset and task, which often fails in cross-task and cross-domain scenarios. To address these challenges, we propose a novel causal-invariance-based and transferable graph dataset condensation method, named \textbf{TGCC}, providing effective and transferable condensed datasets. Specifically, to preserve domain-invariant knowledge, we first extract domain causal-invariant features from the spatial domain of the graph using causal interventions. Then, to fully capture the structural and feature information of the original graph, we perform enhanced condensation operations. Finally, through spectral-domain enhanced contrastive learning, we inject the causal-invariant features into the condensed graph, ensuring that the compressed graph retains the causal information of the original graph. Experimental results on five public datasets and our novel \textbf{FinReport} dataset demonstrate that TGCC achieves up to a 13.41\% improvement in cross-task and cross-domain complex scenarios compared to existing methods, and achieves state-of-the-art performance on 5 out of 6 datasets in the single dataset and task scenario.

</details>


### [178] [Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation](https://arxiv.org/abs/2601.21315)
*Seonghwi Kim,Sung Ho Jo,Wooseok Ha,Minwoo Chae*

Main category: cs.LG

TL;DR: 提出一种分布式鲁棒学习框架用于无监督域适应（UDA），通过对协变量分布和条件标签分布引入不确定性来实现鲁棒性。适用于单源和多源UDA，提供高效可集成的学习算法，在目标数据极少时仍显著优于基线。


<details>
  <summary>Details</summary>
Motivation: UDA在目标域 unlabeled 数据有限或源域存在伪相关性时易受分布偏移影响，亟需对协变量和条件标签分布的不确定性进行鲁棒建模以提升泛化。

Method: 构建一个分布鲁棒优化框架（DRO），对协变量分布与条件标签分布的不确定集合进行建模，提出一个高效的学习算法，可无缝集成到现有UDA方法中；理论可用于单源与多源设定。

Result: 在多种分布偏移场景下的广泛实验显示，该方法在目标数据极少的情况下对强基线具有一致的提升。

Conclusion: 本文提出的分布鲁棒UDA框架增强了对分布偏移的鲁棒性，具有良好的实用性和广泛适用性，可在现有UDA流水线中直接集成，适用于单源与多源场景。

Abstract: Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.

</details>


### [179] [Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2601.21316)
*Aoyu Pang,Maonan Wang,Zifan Sha,Wenwei Yue,Changle Li,Chung Shue Chen,Man-On Pun*

Main category: cs.LG

TL;DR: Unified optimization model and UAGMC framework for integrated air-ground mobility using DRL and V2X, yielding 34% faster trips; code available.


<details>
  <summary>Details</summary>
Motivation: Address the lack of systematic, integrated routing strategies for passenger multimodal air-ground mobility; incorporate real-time traffic, passenger behavior, and dynamic network characteristics.

Method: Develop a unified optimization model for air/ground strategy selection; propose Unified Air-Ground Mobility Coordination (UAGMC) framework utilizing deep reinforcement learning and V2X to optimize vertiport selection and dynamic air taxi routing.

Result: 34% reduction in average travel time compared to conventional proportional allocation methods; improved travel efficiency and new insights into multimodal integration.

Conclusion: This work establishes a foundation for intelligent urban mobility through coordinated air and ground modes; provides code for replication at GitHub.

Abstract: Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic exploration.To address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at https://github.com/Traffic-Alpha/UAGMC.

</details>


### [180] [Memorization Control in Diffusion Models from Denoising-centric Perspective](https://arxiv.org/abs/2601.21348)
*Thuy Phuong Vu,Mai Viet Hoang Do,Minhhuy Le,Dinh-Cuong Hoang,Phan Xuan Tan*

Main category: cs.LG

TL;DR: 提出一种基于去噪轨迹的分步抽样策略来控制扩散模型的记忆化，通过调整置信区间宽度来直接控制记忆-泛化权衡；在图像和一维信号生成任务上证明了偏向后期去噪步骤可降低记忆化并改善与训练数据的分布匹配。


<details>
  <summary>Details</summary>
Motivation: 扩散模型中记忆化是一个关键挑战，现有数据驱动或模型驱动方法未充分考虑去噪过程中的学习贡献差异，导致模型偏向记忆训练数据。

Method: 提出一个显式控制去噪轨迹上学习发生位置的时步采样策略；通过调整置信区间宽度来控制不同去噪步骤的学习贡献，从而实现记忆与泛化之间的权衡。

Result: 将学习重点转向后期去噪步骤可在图像和1D信号生成任务中减少记忆化，且改善与训练数据的分布一致性，验证方法的广泛性与有效性。

Conclusion: 以去噪为中心的分析揭示了通过时步采样与置信区间调控即可直接管理记忆化与泛化的权衡，具有较强的普适性和应用潜力。

Abstract: Controlling memorization in diffusion models is critical for applications that require generated data to closely match the training distribution. Existing approaches mainly focus on data centric or model centric modifications, treating the diffusion model as an isolated predictor. In this paper, we study memorization in diffusion models from a denoising centric perspective. We show that uniform timestep sampling leads to unequal learning contributions across denoising steps due to differences in signal to noise ratio, which biases training toward memorization. To address this, we propose a timestep sampling strategy that explicitly controls where learning occurs along the denoising trajectory. By adjusting the width of the confidence interval, our method provides direct control over the memorization generalization trade off. Experiments on image and 1D signal generation tasks demonstrate that shifting learning emphasis toward later denoising steps consistently reduces memorization and improves distributional alignment with training data, validating the generality and effectiveness of our approach.

</details>


### [181] [L2R: Low-Rank and Lipschitz-Controlled Routing for Mixture-of-Experts](https://arxiv.org/abs/2601.21349)
*Minghao Yang,Ren Togo,Guang Li,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: 提出了 L2R（Low-rank & Lipschitz-controlled Routing），通过在低秩潜在路由空间进行专家分配、引入饱和内积评分（SIPS）以控制路由函数的 Lipschitz 行为、并采用多锚点路由以增强表达力，从而提升 MoE 的路由稳定性、专家分化和整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有 MoE 系统的路由器多采用高维原始表示空间的线性路由，易受表示错配、角度集中、尺度敏感评分等影响，导致路由判别性下降和专家专门化不稳定，因此需要更稳健且表达力更强的路由机制。

Method: 在一个共享的低秩潜在路由空间中进行专家分配；提出饱和内积评分（SIPS），显式控制路由函数的 Lipschitz 行为以获得更平滑、稳定的路由几何；并引入参数高效的多锚点路由机制提升专家的表达能力。

Result: 在大规模语言 MoE 与 ImageNet 视觉 MoE 设置中，L2R 显著提升了路由稳定性、专家专门化以及模型整体性能。

Conclusion: L2R 提供一个统一的路由框架，通过改造路由空间和评分几何，提升 MoE 的路由可控性与表达力，进而提升模型性能。

Abstract: Mixture-of-Experts (MoE) models scale neural networks by conditionally activating a small subset of experts, where the router plays a central role in determining expert specialization and overall model performance. However, many modern MoE systems still adopt linear routers in raw high-dimensional representation spaces, where representation mismatch, angular concentration, and scale-sensitive scoring can jointly undermine routing discriminability and stable expert specialization. In this work, we propose Low-rank \& Lipschitz-controlled Routing (L2R), a unified routing framework that reshapes both the routing space and scoring geometry. L2R performs expert assignment in a shared low-rank latent routing space and introduces Saturated Inner-Product Scoring (SIPS) to explicitly control the Lipschitz behavior of routing functions, yielding smoother and more stable routing geometry. In addition, L2R incorporates a parameter-efficient multi-anchor routing mechanism to enhance expert expressiveness. Extensive experiments on a large-scale language MoE model and a vision MoE setting on ImageNet demonstrate that L2R consistently improves routing stability, expert specialization, and overall model performance.

</details>


### [182] [Factored Causal Representation Learning for Robust Reward Modeling in RLHF](https://arxiv.org/abs/2601.21350)
*Yupei Yang,Lin Yang,Wanxi Deng,Lin Qu,Fan Feng,Biwei Huang,Shikui Tu,Lei Xu*

Main category: cs.LG

TL;DR: 提出一种因果因素分解的奖赏模型，并通过对非因果因素的对抗性头和梯度反转抑制其对奖励的影响，从而减少奖励黑客行为，提升 RLHF 效果。


<details>
  <summary>Details</summary>
Motivation: 现有奖赏模型易受伪相关特征影响（如文本长度、奉承偏好），导致奖励预测与人类标签存在偏差甚至奖励黑客问题。需将奖赏预测仅基于与人类偏好因果相关的要素。

Method: 将上下文嵌入分解为因果因子（对奖励预测充分且相关）和非因果因子（与奖励无关，如长度、奉承偏好）。奖赏头仅依赖因果因子。引入对抗头对非因果因子预测奖励，并应用梯度反转以抑制非因果信息与奖励相关性。通过在数学与对话任务上的实验验证方法对鲁棒性和 RLHF 的提升作用。

Result: 新方法获得更鲁棒的奖赏模型，且在下游 RLHF 性能上优于最先进基线。对长度和奉承偏见的分析支持方法在降低奖励黑客方面的有效性。

Conclusion: 通过因果因素分解与对抗去偏置，显著提升奖赏建模的鲁棒性与 RLHF 效果，降低奖励预测对非因果信息的依赖，可推广到不同任务场景。

Abstract: A reliable reward model is essential for aligning large language models with human preferences through reinforcement learning from human feedback. However, standard reward models are susceptible to spurious features that are not causally related to human labels. This can lead to reward hacking, where high predicted reward does not translate into better behavior. In this work, we address this problem from a causal perspective by proposing a factored representation learning framework that decomposes the model's contextual embedding into (1) causal factors that are sufficient for reward prediction and (2) non-causal factors that capture reward-irrelevant attributes such as length or sycophantic bias. The reward head is then constrained to depend only on the causal component. In addition, we introduce an adversarial head trained to predict reward from the non-causal factors, while applying gradient reversal to discourage them from encoding reward-relevant information. Experiments on both mathematical and dialogue tasks demonstrate that our method learns more robust reward models and consistently improves downstream RLHF performance over state-of-the-art baselines. Analyses on length and sycophantic bias further validate the effectiveness of our method in mitigating reward hacking behaviors.

</details>


### [183] [Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving](https://arxiv.org/abs/2601.21351)
*Chendong Song,Meixuan Wang,Hang Zhou,Hong Liang,Yuan Lyu,Zixi Chen,Yuwei Fan,Zijie Zhou*

Main category: cs.LG

TL;DR: 提出了一种对 Attention-FFN 解耦架构（AFD）的尺寸化分析框架，在 rA-1F 拓扑下给出最优 A/F 比例的闭式规则，通过概率工作负载建模和跟踪校准仿真进行验证，能在不同工作负载下实现接近最优吞吐并显著减少设备空闲。


<details>
  <summary>Details</summary>
Motivation: AFD 将注意力计算与 FFN 计算解耦，理论上允许独立扩展内存与计算资源；但性能对 Attention/FFN 的分配比例高度敏感，错误的尺寸化会导致步级阻塞和设备空闲，需建立可实用的定量 sizing 指南。

Method: 建立非平稳 token 情境下的概率工作负载模型：Attention 端工作量随上下文增长且连带请求持续到来且长度随机；FFN 工作量在聚合批下一直性稳定。推导出关于 A/F 比的闭式最优解，并给出可直接应用的规则；构建与跟踪数据相校准的 AFD 仿真器用于理论验证。

Result: 得出用于最优 A/F 比的闭式规则，理论最优与仿真最优在多种负载下相差约 10% 内，且普遍降低了系统空闲时间。

Conclusion: 为 AFD 拆分架构的尺寸化提供了可操作且可验证的定量指南，特别是在 rA-1F 拓扑中，能实现更高吞吐并降低空闲，支撑对内存与计算资源的独立弹性扩展。

Abstract: Attention-FFN disaggregation (AFD) is an emerging architecture for LLM decoding that separates state-heavy, KV-cache-dominated Attention computation from stateless, compute-intensive FFN computation, connected by per-step communication. While AFD enables independent scaling of memory and compute resources, its performance is highly sensitive to the Attention/FFN provisioning ratio: mis-sizing induces step-level blocking and costly device idle time. We develop a tractable analytical framework for sizing AFD bundles in an $r$A-$1$F topology, where the key difficulty is that Attention-side work is nonstationary-token context grows and requests are continuously replenished with random lengths-while FFN work is stable given the aggregated batch. Using a probabilistic workload model, we derive closed-form rules for the optimal A/F ratio that maximize average throughput per instance across the system. A trace-calibrated AFD simulator validates the theory: across workloads, the theoretical optimal A/F ratio matches the simulation-optimal within 10%, and consistently reduces idle time.

</details>


### [184] [Perceptrons and localization of attention's mean-field landscape](https://arxiv.org/abs/2601.21366)
*Antonio Álvarez-López,Borjan Geshkovski,Domènec Ruiz-Balet*

Main category: cs.LG

TL;DR: 在单位球上的粒子系统视角下，Transformer 的 perceptron 块使能量临界点普遍呈原子化并局部化在球面子集。


<details>
  <summary>Details</summary>
Motivation: 揭示架构组件对无限宽/mean-field 极限中能量景观与表示稀疏化的影响，及其对优化与可解释性的含义。

Method: 将前向传播建模为单位球上的相互作用粒子系统，借助 Wasserstein 梯度流分析能量 E(μ) 的临界点；证明在一般条件下，带 perceptron 的设置会导致临界点为原子分布，且局部化于球面的子集。

Result: 得到的结论是：临界点在均匀性条件下通常是有限支持的原子分布，原子集中在单位球的若干子集上实现局部化。

Conclusion: 揭示了 mean-field 极限下 Transformer 的结构偏好，即趋向稀疏、离散的表示群；对优化收敛、模型解释性及进一步理论研究具有指引。

Abstract: The forward pass of a Transformer can be seen as an interacting particle system on the unit sphere: time plays the role of layers, particles that of token embeddings, and the unit sphere idealizes layer normalization. In some weight settings the system can even be seen as a gradient flow for an explicit energy, and one can make sense of the infinite context length (mean-field) limit thanks to Wasserstein gradient flows. In this paper we study the effect of the perceptron block in this setting, and show that critical points are generically atomic and localized on subsets of the sphere.

</details>


### [185] [Rethinking Federated Graph Foundation Models: A Graph-Language Alignment-based Approach](https://arxiv.org/abs/2601.21369)
*Yinlin Zhu,Di Wu,Xianzhi Zhang,Yuming Ai,Xunkai Li,Miao Hu,Guocong Quan*

Main category: cs.LG

TL;DR: FedGALA: 在联邦图基础模型中通过对比学习对齐GNN与冻结PLM，采用高效提示调优实现无参数微调，提升跨域任务表现，且解决图-语言语义结构不对齐与数据异质性问题，实验显示最高提升约14.37%。


<details>
  <summary>Details</summary>
Motivation: 解决在分布式、隐私受限数据孤岛环境中，图基础模型的语义与结构信息与语言模型之间的正交性与完整性问题；克服向量量化带来的不可逆知识损失，以及在FedGFMs中存在的数据异质性和通信成本。

Method: 采用无监督对比学习，在连续嵌入空间中对齐GNN编码器与冻结的PLM；随后使用沟通高效的提示调优机制，引导预对齐的编码器与PLMs进行下游任务适配，避免全参数微调。

Result: 在多领域数据集、多任务上显著优于竞争基线，性能提升可达约14.37%。

Conclusion: FedGALA有效解决FedGFMs中的图-语言语义结构正交性与完整性问题，并在分布式、资源受限场景下实现高效适配与可迁移的知识表征。

Abstract: Recent studies of federated graph foundational models (FedGFMs) break the idealized and untenable assumption of having centralized data storage to train graph foundation models, and accommodate the reality of distributed, privacy-restricted data silos. Despite their simplicity and intuition, existing studies that project aligned generalizable knowledge onto a discrete token space via vector-quantized backbones suffer from irreversible knowledge loss during the quantization process. In this context, we argue that reconciling the semantic-structural orthogonality and integrity between pre-trained language models (PLMs) and graph neural networks (GNNs) is paramount for developing effective FedGFMs while simultaneously mitigating the severe data heterogeneity and communication constraints inherent in distributed, resource-limited environments.
  To address these issues, we propose FedGALA (Federated Graph And Language Alignment), a framework that resolves graph-based semantic-structural orthogonality and integrity in federated settings by employing unsupervised contrastive learning to align GNNs and frozen PLMs within a continuous embedding space, thereby capturing robust, transferable general knowledge. Subsequently, FedGALA leverages a communication-efficient prompt tuning mechanism to steer these pre-aligned encoders and frozen PLMs, facilitating effective adaptation to diverse downstream tasks while circumventing the prohibitive overhead of full-parameter fine-tuning. The comprehensive experiments validate that FedGALA outperforms all competitive baselines across multi-domain datasets on multiple tasks with up to 14.37% performance improvement.

</details>


### [186] [DA-SPS: A Dual-stage Network based on Singular Spectrum Analysis, Patching-strategy and Spearman-correlation for Multivariate Time-series Prediction](https://arxiv.org/abs/2601.21381)
*Tianhao Zhang,Shusen Ma,Yu Kang,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出 DA-SPS 模型用于多变量时间序列预测，通过 TVPS 与 EVPS 两阶段实现对目标变量与外在变量的差异化特征提取与筛选。TVPS 利用 SSA 处理目标序列，再用 LSTM 与采用补丁策略的 P-Conv-LSTM 提取趋势与季节性信息；EVPS 通过 Spearman 相关性筛选与 L-Attention（LSTM + 注意力）分析外在变量；最终对 TVPS 与 EVPS 的结果进行加权求和与线性映射得到预测。实证在四个公开数据集上优于现有方法，并在自有私有数据集上得到进一步验证。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测方法往往忽视外部变量对目标变量的影响，以及难以充分挖掘序列在不同时间模式下的复杂信息，导致预测性能受限。需要对目标变量与外部变量进行分阶段、信息特征驱动的处理，以及对外部变量进行有效筛选与建模。

Method: 模型分为 TVPS 与 EVPS 两阶段。TVPS：对目标变量序列先进行 SSA 分解，然后使用 LSTM 提取趋势信息，以及使用部署了补丁策略的 P-Conv-LSTM 提取季节性信息。EVPS：利用斯皮尔曼相关性筛选与目标变量高度相关的外部变量，再通过由 LSTM 与注意力机制组成的 L-Attention 模块对其进行分析。最终通过对 TVPS 与 EVPS 的输出进行加权求和与线性映射得到最终预测。

Result: 在四个公开数据集上，该模型优于现有最先进方法；在自有私有数据集（包含主板测试项信息）上亦有实证支持，显示在实际场景中的有效性与鲁棒性。

Conclusion: DA-SPS 通过分阶段、按信息特征定制的特征提取与外部变量筛选，能够更有效地利用目标变量与外部信息以提升多变量时间序列预测的准确性与鲁棒性。

Abstract: Multivariate time-series forecasting, as a typical problem in the field of time series prediction, has a wide range of applications in weather forecasting, traffic flow prediction, and other scenarios. However, existing works do not effectively consider the impact of extraneous variables on the prediction of the target variable. On the other hand, they fail to fully extract complex sequence information based on various time patterns of the sequences. To address these drawbacks, we propose a DA-SPS model, which adopts different modules for feature extraction based on the information characteristics of different variables. DA-SPS mainly consists of two stages: the target variable processing stage (TVPS) and the extraneous variables processing stage (EVPS). In TVPS, the model first uses Singular Spectrum Analysis (SSA) to process the target variable sequence and then uses Long Short-Term Memory (LSTM) and P-Conv-LSTM which deploys a patching strategy to extract features from trend and seasonality components, respectively. In EVPS, the model filters extraneous variables that have a strong correlation with the target variate by using Spearman correlation analysis and further analyses them using the L-Attention module which consists of LSTM and attention mechanism. Finally, the results obtained by TVPS and EVPS are combined through weighted summation and linear mapping to produce the final prediction. The results on four public datasets demonstrate that the DA-SPS model outperforms existing state-of-the-art methods. Additionally, its performance in real-world scenarios is further validated using a private dataset collected by ourselves, which contains the test items' information on laptop motherboards.

</details>


### [187] [From Consistency to Complementarity: Aligned and Disentangled Multi-modal Learning for Time Series Understanding and Reasoning](https://arxiv.org/abs/2601.21436)
*Hang Ni,Weijia Zhang,Fei Wang,Zezhi Shao,Hao Liu*

Main category: cs.LG

TL;DR: MADI提出基于时序多模态的大语言模型的三大机制，通过 Patch-level Alignment、离散化解耦交互以及关键 token 高亮，显著提升跨模态对齐与推理能力，在合成与真实数据基准上优于通用LLMs与时序专用MLLM。


<details>
  <summary>Details</summary>
Motivation: 多模态时序理解中的跨模态细粒度对齐困难与共性/模态特异语义混合问题，导致局部解释与互补推理受限。

Method: (1) Patch-level Alignment：实现不同模态之间物理上细粒度的一致性；(2) Discrete Disentangled Interaction：将共模语义离散化潜在表示，分离并适配融合模态特异信息；(3) Critical-token Highlighting：对查询相关的关键信号进行突出，提升鲁棒推理。

Result: 在合成与真实世界基准上，MADI持续优于通用大语言模型及专门化的MLLMs。

Conclusion: 通过细粒度对齐和解耦交互，提升跨模态集成与时间序列推理的效率与准确性，证明了MADI在多模态时序任务中的有效性。

Abstract: Advances in multi-modal large language models (MLLMs) have inspired time series understanding and reasoning tasks, that enable natural language querying over time series, producing textual analyses of complex temporal dynamics. Recent attempts hybridize numerical time series with their visualized plots, facilitating precise value reasoning and visual structure comprehension for comprehensive time series understanding of MLLMs. However, effective cross-modal integration remains challenging due to fine-grained temporal misalignment across modalities and severe entanglement between shared and modality-specific semantics, which hinder localized interpretation and complementary reasoning. To address these issues, we propose MADI, a multi-modal LLM enhanced with fine-grained alignment and disentangled interaction, featuring (1) Patch-level Alignment, which enforces physically grounded fine-grained correspondence across heterogeneous modalities, (2) Discrete Disentangled Interaction, which separates modality-common semantics into compact discrete latents and adaptively synergizes the purified modality-unique information, and (3) Critical-token Highlighting, which emphasizes informative, query-relevant signals for robust reasoning. Experiments on synthetic and real-world benchmarks show that MADI consistently outperforms general-purpose LLMs and time-series-specialized MLLMs.

</details>


### [188] [Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.21418)
*Qian Wan,Ziao Xu,Luona Wei,Xiaoxuan Shen,Jianwen Sun*

Main category: cs.LG

TL;DR: 提出 DiPO（Difficulty-aware Policy Optimization），基于强化学习的大推理模型训练框架。通过自我推理的难度建模和困难信号增强的奖励函数，调节推理生成偏好，显式控制推理开销，显著压缩冗余令牌且不降低任务性能。


<details>
  <summary>Details</summary>
Motivation: LRMs 在进行显式链路推理时倾向于进行过度深入的思维，导致对简单任务的资源浪费。这种“过度思维”可能由强化学习后训练中的奖励函数偏好引发。现有方法多从提示设计或模型训练角度入手，但往往忽视任务难度感知对资源分配的重要性，导致难以有效控制推理开销。

Method: 提出 DiPO 框架：基于强化学习的训练流程，鼓励模型自发建立对任务难度的感知并将其融入学习目标。通过基于模型自我推理的难度建模降低对人工标注的依赖，并开发了困难信号增强的奖励函数，在奖励中加入对冗长推理的惩罚，同时兼顾推理性能与输出格式，调整 post-training 的生成偏好，实现推理资源的自适应分配。

Result: 实验结果表明，DiPO 使模型能够自发调整推理开销，显著降低冗余令牌数量，同时在保持推理性能的前提下实现思维压缩。

Conclusion: DiPO 有效提升 LRMs 的难度感知资源分配能力，使推理过程更高效，能够在不损失性能的前提下减少不必要的深度推理。

Abstract: Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.

</details>


### [189] [Revisiting Diffusion Model Predictions Through Dimensionality](https://arxiv.org/abs/2601.21419)
*Qing Jin,Chaoyang Wang*

Main category: cs.LG

TL;DR: 提出一个可处理任意输出目标的广义预测框架，用以解释扩散/流匹配模型中目标从 ε/velocity 转向 x 的现象，并引入 k-Diff 以数据驱动学习最优预测参数 k；在潜在及像素空间的图像生成实验中，k-Diff 相比固定目标基线表现更好。


<details>
  <summary>Details</summary>
Motivation: 解释为何最优预测目标依赖数据的几何与维度特性，尤其在高维环境中为何偏向 x 预测；填补理论解释与实际应用之间的断层，减少对内在维度估计的依赖。

Method: 提出一个可推广至任意输出目标的广义预测公式；推导数据几何对最优目标的解析关系，给出在高维条件下为何更适合 x-pred 的理论理由；提出 k-Diff 框架，通过数据驱动学习最优参数 k，避免显式内在维度估计；在潜在空间与像素空间的图像生成任务中进行广泛实验。

Result: 理论表明，当外部维度显著高于数据的内在维度时，x-prediction 具有优势；k-Diff 能从数据中直接学习最优预测参数 k，并在多种架构与数据规模下优于固定目标基线。

Conclusion: 提供一个原理性且自动化的途径来提升生成性能，解释在不同数据几何条件下应选择的预测目标；k-Diff 为实际应用提供可行的学习框架。

Abstract: Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise ($\varepsilon$) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which $\varepsilon$-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.

</details>


### [190] [ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation](https://arxiv.org/abs/2601.21420)
*Zihao Huang,Jundong Zhou,Xingwei Qu,Qiyang Min,Ge Zhang*

Main category: cs.LG

TL;DR: 概念门控混合专家（ConceptMoE）通过将语义相似的标记动态合并为概念表示，进行隐式的逐-token计算分配。通过可学习的块模块确定边界，按目标比R压缩序列，然进入计算密集的概念模型。对比评估通过控制变量（保留FLOPs、参数数量）， isolating 架构收益。结果在语言和多模态任务上均有提升，并在持续训练中显著提升，同时显著降低注意力与KV缓存开销，R=2 时 Prefill/解码有显著加速。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型对所有标记统一分配计算的不足，利用语义概念层级来实现自适应的计算分配，从而提高效率与性能。

Method: 提出 ConceptMoE，采用学习的 chunk 模块基于互标记相似性识别边界，将序列在进入概念模型前按目标压缩比R进行压缩；MoE 架构提供可控评估，通过重新分配节省的计算以匹配基线 FLOPs 与总参数，以隔离真正的架构益处；在语言和视觉-语言任务上评估，并在连续训练中引入层循环；对比实验包括 R 的消融，以及注意力计算和 KV 缓存的节省分析。

Result: ConceptMoE 相较标准 MoE 在语言预训练、长上下文理解、以及多模态基准上分别提升约0.9、2.3、0.6点；在持续训练和层循环的场景下提升达5.5点；注意力计算可降低至原始的R^2倍，KV缓存降低至R倍；在R=2时 Prefill 加速至175%，解码加速至117%；修改最小，易于集成。

Conclusion: 自适应概念层级处理能够提升模型效果与效率，且对现有 MoE 的集成简单，证明了基于概念级别的处理是提升大语言模型有效性与效率的可行路径。

Abstract: Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio $R$ before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to $R^2\times$ and KV cache by $R\times$. At $R=2$, empirical measurements show prefill speedups reaching 175\% and decoding speedups up to 117\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.

</details>


### [191] [Lossy Common Information in a Learnable Gray-Wyner Network](https://arxiv.org/abs/2601.21424)
*Anderson de Andrade,Alon Harell,Ivan V. Bajić*

Main category: cs.LG

TL;DR: 提出一种可学习的三通道编解码器，利用 Gray-Wyner 理论分离共享信息与任务特定信息，通过“有损共享信息”概念进行优化，在两任务多基准场景下显著降低冗余且优于独立编码。


<details>
  <summary>Details</summary>
Motivation: 多任务视觉任务存在大量信息重叠；传统编解码器忽视共享信息，导致冗余表示低效。Gray-Wyner 提供分离公共信息与任务私有信息的理论框架，本文将其引入学习系统以提升多任务压缩与传输效率。

Method: 提出一个三通道可学习编解码器：一个共享通道用于捕捉跨任务的共有信息，两个任务特定通道用于保留各自的细节信息。引入“有损共享信息”概念，构建权衡学习的优化目标，平衡信息保留与压缩率之间的内在权衡。通过对三种编解码架构在两任务、覆盖六个视觉基准的数据上进行比较，评估共享信息与任务专用信息的分离效果及整体性能。

Result: 与独立编码相比，显著降低冗余并在多基准场景中持续优于独立编码，验证了 Gray-Wyner 理论在任务驱动表征学习中的实用性与有效性。

Conclusion: 重新引入 Gray-Wyner 理论到现代机器学习，提供一个可训练的多任务信息分离框架，具有实用潜力并可扩展到更多任务和模态，促进高效的跨任务表示学习与编解码。

Abstract: Many computer vision tasks share substantial overlapping information, yet conventional codecs tend to ignore this, leading to redundant and inefficient representations. The Gray-Wyner network, a classical concept from information theory, offers a principled framework for separating common and task-specific information. Inspired by this idea, we develop a learnable three-channel codec that disentangles shared information from task-specific details across multiple vision tasks. We characterize the limits of this approach through the notion of lossy common information, and propose an optimization objective that balances inherent tradeoffs in learning such representations. Through comparisons of three codec architectures on two-task scenarios spanning six vision benchmarks, we demonstrate that our approach substantially reduces redundancy and consistently outperforms independent coding. These results highlight the practical value of revisiting Gray-Wyner theory in modern machine learning contexts, bridging classic information theory with task-driven representation learning.

</details>


### [192] [SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation](https://arxiv.org/abs/2601.21452)
*Yu Xie,Xing Kai Ren,Ying Qi,Hu Yao*

Main category: cs.LG

TL;DR: SAGE 框架通过序列层级信号解耦与非对称自适应动态，实现在无独立词表重用开源 LLM 架构的同时解决冷启动、信息茧化与多样性问题，且保持对 GBPO 的数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如 OneRec 的 GBPO）存在静态梯度边界，更新动量不足，且在高噪声环境下易导致多样性崩溃；同时依赖独立词表增加维护成本，阻碍对原生 LLM 词表的重用。

Method: 提出两大创新：1) 序列层级信号解耦：结合几何平均的重要性比与解耦的多目标优势，消除标记级方差，解决“奖励崩溃”问题；2) 非对称自适应动态：构建动态梯度流形，对高潜力冷启动项应用“Boost Factor”实现超线性更新，并引入“熵感知惩罚”以打破信息茧。

Result: 理论分析与实证结果表明，SAGE 能有效解锁冷启动流量、维持推荐多样性，并保持 GBPO 的数值稳定性。

Conclusion: SAGE 为列表式生成推荐提供一个统一的优化框架，兼顾词表重用、冷启动提升、多样性与稳定性，从而提升可扩展性与整体性能。

Abstract: While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a "Symmetric Conservatism" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise environments.To address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the "Reward Collapse" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a "Boost Factor" to high-potential cold start items to achieve super-linear updates and employs an "Entropy Aware Penalty" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.

</details>


### [193] [HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing](https://arxiv.org/abs/2601.21459)
*Chengyu Du,Xintao Wang,Aili Chen,Weiyuan Li,Rui Xu,Junteng Liu,Zishan Huang,Rong Tian,Zijun Sun,Yuhao Li,Liheng Feng,Deming Ding,Pengyu Zhao,Yanghua Xiao*

Main category: cs.LG

TL;DR: 提出 HER 框架实现认知级人格模拟，结合双层思考、推理增强数据与对齐奖励模型；在 Qwen3-32B 上通过监督+强化学习训练，显著优于基线；并将数据、原则与模型公开。


<details>
  <summary>Details</summary>
Motivation: 解决现有 LLM 角色扮演中缺乏高质量推理轨迹数据与与人类偏好对齐的奖励信号的问题。

Method: 引入双层思考；通过反向工程构造推理增强的角色扮演数据；制定人类对齐原则与奖励模型；在 Qwen3-32B 上进行监督学习与强化学习训练，得到 method 模型。

Result: 在 CoSER 基准上取得 30.26 的提升，在 Minimax Role-Play Bench 上达到 14.97 的提升，相较基线显著有效。

Conclusion: 将数据集、对齐原则与模型正式发布，促进行业与学术对认知层次的人格模拟研究的发展。

Abstract: LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters' first-person thinking from LLMs' third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.

</details>


### [194] [L$^3$: Large Lookup Layers](https://arxiv.org/abs/2601.21461)
*Albert Tseng,Christopher De Sa*

Main category: cs.LG

TL;DR: 提出 Large Lookup Layer (L^3)：将嵌入表推广到解码器层，使用静态的基于 token 的路由聚合多组可学习嵌入，在系统友好性和信息理论分配之间取得平衡。并在高达 2.6B 参数的变换器上证明优于密集模型与同等稀疏MoE。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏语言模型多采用 Mixture-of-Experts（MoE）实现动态硬路由，存在硬件效率低、训练稳定性需额外损失等问题。嵌入表天然稀疏，能避免部分问题，但缺乏上下文信息。本工作旨在通过将嵌入表的思路扩展到解码器层，提供一种新的稀疏性维度。

Method: 提出 L^3 层：通过静态的基于 token 的路由，将每个 token 聚合为一组可学习的嵌入，按上下文依赖地组合以提供丰富表示。核心包括两大模块：1) 系统友好架构，支持快速训练与 CPU 卸载推理且无额外开销；2) 信息论驱动的嵌入分配算法，平衡速度与表示质量。对规模达 2.6B 参数的变换器进行训练与评估。

Result: L^3 在语言建模及下游任务中，显著优于密集模型与同等稀疏的 MoE 基线。表现体现在更高的效率与/或相近参数量下的性能提升，且具有更好的系统实现特性。

Conclusion: 通过将嵌入表的稀疏性扩展到解码器层，L^3 引入了新的稀疏性轴，结合静态路由与信息论驱动的嵌入分配，在训练与推理中的系统效率与模型性能之间实现更优权衡，且在多任务评估中超越密集与 iso-sparse MoE。

Abstract: Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP "experts." However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.

</details>


### [195] [Partial Feedback Online Learning](https://arxiv.org/abs/2601.21462)
*Shihao Shao,Cong Fang,Zhouchen Lin,Dacheng Tao*

Main category: cs.LG

TL;DR: 在部分反馈设定下对最小化悔疚进行近完整刻画，提出 PFLdim 与 PMSdim，区分确定性与随机性学习者在集合可实现性下的学习性与界。


<details>
  <summary>Details</summary>
Motivation: 现实场景如语言生成存在多个正确答案但数据仅给出单一参考，传统在线学习理论不足以刻画此类部分可实现性情形。

Method: 引入新的版本空间视角与辅助维度，提出 Partial-Feedback Littlestone dimension (PFLdim) 来刻画确定性学习的可学习性；提出 Partial-Feedback Measure Shattering dimension (PMSdim) 来刻画随机化学习的界；将集合设定扩展至集合可实现性在线学习，并给出相关界和证明结构。

Result: 给出对 minimax regret 的近完整表征：PFLdim 精确支配确定性情形的可学习性与悔疚界；PMSdim 提供随机化情形的紧界；给出确定性与随机性学习不可区分性的充要条件（如有限 Helly 数、嵌套标签结构等），并解决 Raman 等人提出的开放问题；在集合实现之外，可能出现信息论层面的线性悔疚，强调需新颖的噪声敏感复杂度度量。

Conclusion: 强调需要噪声敏感的新的复杂性度量来系统刻画集合实现以外的可学习性，并拓展到集合集 online learning 的广义情形。

Abstract: We study partial-feedback online learning, where each instance admits a set of correct labels, but the learner only observes one correct label per round; any prediction within the correct set is counted as correct. This model captures settings such as language generation, where multiple responses may be valid but data provide only a single reference. We give a near-complete characterization of minimax regret for both deterministic and randomized learners in the set-realizable regime, i.e., in the regime where sublinear regret is generally attainable. For deterministic learners, we introduce the Partial-Feedback Littlestone dimension (PFLdim) and show it precisely governs learnability and minimax regret; technically, PFLdim cannot be defined via the standard version space, requiring a new collection version space viewpoint and an auxiliary dimension used only in the proof. We further develop the Partial-Feedback Measure Shattering dimension (PMSdim) to obtain tight bounds for randomized learners. We identify broad conditions ensuring inseparability between deterministic and randomized learnability (e.g., finite Helly number or nested-inclusion label structure), and extend the argument to set-valued online learning, resolving an open question of Raman et al. [2024b]. Finally, we show a sharp separation from weaker realistic and agnostic variants: outside set realizability, the problem can become information-theoretically intractable, with linear regret possible even for $|H|=2$. This highlights the need for fundamentally new, noise-sensitive complexity measures to meaningfully characterize learnability beyond set realizability.

</details>


### [196] [Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking](https://arxiv.org/abs/2601.21619)
*Yiming Wang,Zhuosheng Zhang,Rui Wang*

Main category: cs.LG

TL;DR: 平行思维中的 overscaling 诅咒：系统级统一并行度N往往浪费资源，因为不同样本需要的并行度不同。提出 T2 以潜在表征估计每个样本的最优并行度，在解码前确定，显著降低成本且保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决系统层面统一的并行度分配导致的资源浪费问题，即在样本层面存在显著异质性时，仍按大N取整带来冗余成本。

Method: 形式化并量化 overscaling 诅咒及其触发机制；提出轻量级方法 T2，通过学习到的潜在表示估计每个样本的最优并行度在解码前确定；在多样本中进行对比实验以评估成本与性能平衡。

Result: 实验显示在保持相近性能的前提下，T2 显著降低了成本，提升了并行推理的整体效率。

Conclusion: 通过对每个样本预测最优并行度，突破 overscaling 诅咒，使并行思维变得更高效。

Abstract: Parallel thinking enhances LLM reasoning by multi-path sampling and aggregation. In system-level evaluations, a global parallelism level N is allocated to all samples, typically set large to maximize overall dataset accuracy. However, due to sample heterogeneity, some samples can achieve comparable performance with a smaller N'< N, causing budget redundancy. This incompatibility between system-level efficacy and sample-level efficiency constitutes the overscaling curse. In this paper, we formalize and quantify the overscaling curse, showing its universality and severity in practice, and analyze its trigger mechanism. We then propose a lightweight method, T2, to break the overscaling curse, which utilizes latent representations to estimate the optimal parallelism level for each sample before decoding. Experiments show that T2 significantly reduces cost while maintaining comparable performance, enabling more efficient parallel thinking.

</details>


### [197] [A block-coordinate descent framework for non-convex composite optimization. Application to sparse precision matrix estimation](https://arxiv.org/abs/2601.21467)
*Guillaume Lauga*

Main category: cs.LG

TL;DR: 提出了一种适用于非凸复合优化的新型区块坐标下降框架，确保目标下降和收敛性，包含变量度量原型梯度、近端牛顿和交替最小化更新，能统一并加速三种Graphical Lasso求解器（graphical ISTA、Primal GLasso、QUIC），在非凸稀疏精度矩阵估计上给出收敛性保证并实现最高可达100倍的迭代加速。


<details>
  <summary>Details</summary>
Motivation: 非凸优化领域对更大规模问题的求解需求日益增长，但对BCD在非凸场景的理论研究不足，同时Sparse precision矩阵估计（如GLasso）等应用迫切需要既有理论保障又高效的求解方法；现有解法往往分散且难以统一，缺乏统一框架与收敛 guarantees。

Method: 提出一个通用的区块坐标下降框架，能在同一框架下整合变量度量的原型梯度更新、近端牛顿更新及交替最小化等多种更新策略；框架覆盖Graphical Lasso的三种主流求解器：graphical ISTA、Primal GLasso、QUIC；并给出非凸自变量约束下的收敛性分析。

Result: 给出对非凸稀疏精度矩阵估计的收敛性保证，实验结果显示在达到同等估计质量时迭代次数可显著减少，文中声称最高可达到100倍的迭代加速。

Conclusion: 所提出的BCD框架具备良好普适性与高效性，能够统一并提升现有Graphical Lasso求解策略在非凸场景中的性能，同时为非凸复合优化提供稳健的理论与实践基础。

Abstract: Block-coordinate descent (BCD) is the method of choice to solve numerous large scale optimization problems, however their theoretical study for non-convex optimization, has received less attention. In this paper, we present a new block-coordinate descent (BCD) framework to tackle non-convex composite optimization problems, ensuring decrease of the objective function and convergence to a solution. This framework is general enough to include variable metric proximal gradient updates, proximal Newton updates, and alternated minimization updates. This generality allows to encompass three versions of the most used solvers in the sparse precision matrix estimation problem, deemed Graphical Lasso: graphical ISTA, Primal GLasso, and QUIC. We demonstrate the value of this new framework on non-convex sparse precision matrix estimation problems, providing convergence guarantees and up to a $100$-fold reduction in the number of iterations required to reach state-of-the-art estimation quality.

</details>


### [198] [SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning](https://arxiv.org/abs/2601.21649)
*Jinjun Peng,Magnus Saebo,Tianjun Zhong,Yi-Jie Cheng,Junfeng Yang,Baishakhi Ray,Simin Chen,Yangruibo Ding*

Main category: cs.LG

TL;DR: 提出了 Repository-Centric Learning (RCL) 概念，将学习重点从横向任务扩展切换到纵向代码库深度，通过四单元的 Repository-Centric Experience 训练出 repo-specialized 的 SWE-Spot-4B 家族模型，在公开权重模型和效率导向的商用模型上表现均衡或超越，同时具备更高的样本效率和更低的推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前开源小型语言模型在处理陌生代码库时缺乏推理时的强泛化能力，现有 Task-Centric Learning 以横向任务覆盖为主，难以捕捉目标软件环境的“物理规律”。

Method: 提出四单元的 Repository-Centric Experience，将静态代码库转化为可交互学习信号，训练出 SWE-Spot-4B 系列作为仓库专用专家。模型具有横向能力并在仓库层面深度定制。对比规模更大的模型，表现出超越与等效的结果.

Result: 在多项 SWE 任务上，SWE-Spot-4B 超越/匹配 CWM、Qwen3-Coder-30B 等开放权重模型；在效率方面接近或超过 GPT-4.1-mini、GPT-5-nano 等商用效率导向模型；具有更高的训练样本效率和更低的推理成本。

Conclusion: 仓库掌握是构建高效智能的独立维度，补充一般性编码能力，RCL 能带来更高的样本效率和推理效率，适用于资源受限环境的开源模型。

Abstract: The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the "physics" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.

</details>


### [199] [PPI-SVRG: Unifying Prediction-Powered Inference and Variance Reduction for Semi-Supervised Optimization](https://arxiv.org/abs/2601.21470)
*Ruicheng Ao,Hongyu Chen,Haoyang Liu,David Simchi-Levi,Will Wei Sun*

Main category: cs.LG

TL;DR: 提出PPI-SVRG，将PPI预测与SVRG的方差减小通过控制变差来统一，理论上等价于SVRG+控制变差，收敛界包含标准SVRG速率与预测不确定性导致的误差地板。预测越好，越接近SVRG；预测降级则收敛到更大邻域但仍稳定。实验验证：在标签稀缺场景下，Mean Estimation的MSE下降43-52%，在MNIST仅10%标签时测试准确率提升2.7-2.9个百分点。


<details>
  <summary>Details</summary>
Motivation: 在标签稀缺条件下进行半监督随机优化，利用来自预训练模型的预测实现变差降低，同时统一两种变差控制策略（PPI与SVRG）的等价关系并分析预测质量对收敛性的影响。

Method: 证明PPI与SVRG的控制变差等价，提出PPI-SVRG算法，将两者耦合；给出收敛界，将收敛速率分解为标准SVRG速率+因预测不确定性产生的误差地板；分析表明速率仅取决于损失几何，预测仅影响邻域大小；在预测完美时退化为SVRG；对预测下降时仍能稳定收敛但达到较大邻域。

Result: 理论层面：收敛界由标准SVRG速率与预测误差组成，预测误差决定收敛的下界；实验层面：在标签稀缺下实现显著的MSE降低（43-52%）；在MNIST数据集以10%标签时，测试准确率提升2.7-2.9个百分点。

Conclusion: PPI-SVRG成功将PPI与SVRG统一为一个框架，提供在存在预测误差时仍能稳定收敛的理论与实验保障，并实现实证上的显著性能提升。

Abstract: We study semi-supervised stochastic optimization when labeled data is scarce but predictions from pre-trained models are available. PPI and SVRG both reduce variance through control variates -- PPI uses predictions, SVRG uses reference gradients. We show they are mathematically equivalent and develop PPI-SVRG, which combines both. Our convergence bound decomposes into the standard SVRG rate plus an error floor from prediction uncertainty. The rate depends only on loss geometry; predictions affect only the neighborhood size. When predictions are perfect, we recover SVRG exactly. When predictions degrade, convergence remains stable but reaches a larger neighborhood. Experiments confirm the theory: PPI-SVRG reduces MSE by 43--52\% under label scarcity on mean estimation benchmarks and improves test accuracy by 2.7--2.9 percentage points on MNIST with only 10\% labeled data.

</details>


### [200] [Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities](https://arxiv.org/abs/2601.21702)
*Tien Dang,The-Hai Nguyen,Dinh Mai Phuong,Nguyen Minh Phuong,Hoang Thanh-Tung,Le-Minh Nguyen,Naoya Inoue*

Main category: cs.LG

TL;DR: 提出并验证一种基于线性表示假设的代表性失记（RM）方法，通过识别忘记样本的高层概念的一维表示并对其进行线性操作，在忘记表示空间内实现可控副行为与能力提升。


<details>
  <summary>Details</summary>
Motivation: 核心动机是探究目标向量在 RM 中的作用，以及是否可以通过线性操作高层概念的单维表示，在忘记表示空间中获得可控的副行为以及潜在的能力增强。

Method: 在忘记表示空间中定位与某一高层概念相关的一维向量，并对该向量进行线性变换以实现对该概念相关行为的控制与能力影响。通过大量任务验证，包括行为控制（如对模型输出的真相、情感、拒绝等行为的影响）与能力提升（如提升在-context 学习能力）。

Result: 实验表明存在可控的副行为与能力提升的现象，且通过对该概念向量的线性操作可以实现对忘记样本相关行为的干预。

Conclusion: 该现象既可能带来滥用风险，也可被用于构建需要更强能力与可控行为的模型，具有重要的应用与安全考量。

Abstract: We consider representation misdirection (RM), a class of LLM unlearning methods that achieves forgetting by manipulating the forget-representations, that is, latent representations of forget samples. Despite being important, the roles of target vectors used in RM, however, remain underexplored. Here, we approach and revisit RM through the lens of the linear representation hypothesis. Specifically, if one can somehow identify a one-dimensional representation corresponding to a high-level concept, the linear representation hypothesis enables linear operations on this concept vector within the forget-representation space. Under this view, we hypothesize that, beyond forgetting, machine unlearning elicits controllable side behaviors and stronger side capabilities corresponding to the high-level concept. Our hypothesis is empirically validated across a wide range of tasks, including behavioral control (e.g., controlling unlearned models' truth, sentiment, and refusal) and capability enhancement (e.g., improving unlearned models' in-context learning capability). Our findings reveal that this fairly attractive phenomenon could be either a hidden risk if misused or a mechanism that can be harnessed for developing models that require stronger capabilities and controllable behaviors.

</details>


### [201] [ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment](https://arxiv.org/abs/2601.21484)
*Xiuyu Li,Jinkai Zhang,Mingyang Yi,Yu Li,Longqiang Wang,Yue Wang,Ju Fan*

Main category: cs.LG

TL;DR: Training-free inference method ETS samples from the optimal RL policy for language models by combining a reference policy with an energy term; it online-estimates the energy via Monte Carlo with convergence guarantees and uses acceleration and importance sampling to reduce latency, achieving improved generation quality across MLM tasks.


<details>
  <summary>Details</summary>
Motivation: RL-based post-training alignment for language models is effective but expensive and unstable. A training-free inference approach aims to obtain high-quality policy sampling without costly RL fine-tuning.

Method: Energy-Guided Test-Time Scaling (ETS): transition probability = reference policy × energy term; online Monte Carlo estimation of the energy term with provable convergence; employs modern acceleration frameworks and tailored importance sampling to reduce latency while preserving sampling quality.

Result: Empirical results on MLMs (autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show ETS consistently improves generation quality, demonstrating effectiveness and the practicality of the approach.

Conclusion: Training-free inference via ETS can effectively approximate the optimal RL policy for language models, delivering quality gains with reduced computational cost and stable performance.

Abstract: Reinforcement Learning (RL) post-training alignment for language models is effective, but also costly and unstable in practice, owing to its complicated training process. To address this, we propose a training-free inference method to sample directly from the optimal RL policy. The transition probability applied to Masked Language Modeling (MLM) consists of a reference policy model and an energy term. Based on this, our algorithm, Energy-Guided Test-Time Scaling (ETS), estimates the key energy term via online Monte Carlo, with a provable convergence rate. Moreover, to ensure practical efficiency, ETS leverages modern acceleration frameworks alongside tailored importance sampling estimators, substantially reducing inference latency while provably preserving sampling quality. Experiments on MLM (including autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show that our ETS consistently improves generation quality, validating its effectiveness and design.

</details>


### [202] [Task-Awareness Improves LLM Generations and Uncertainty](https://arxiv.org/abs/2601.21500)
*Tim Tomov,Dominik Fuchsgruber,Stephan Günnemann*

Main category: cs.LG

TL;DR: 将LLM输出直接建模为一个任务相关的潜在结构，并通过引入结构上的不相似性度量来计算贝叶斯最优响应，而非依赖语言空间的采样解码，从而提升解码性能和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有解码与不确定性估计多限于语言空间，忽略输出结构信息；需要将潜在结构整合进模型，以实现对离散标签、数值或图结构等任务的任务感知预测与更可靠的不确定性估计。

Method: 在潜在结构中对LLM输出进行直接建模，定义与该结构相关的不相似性度量；通过贝叶斯风险最小化来获得贝叶斯最优响应，将响应新组合而非从生成中采样得到；通过诱导的贝叶斯风险对不确定性进行量化，提升与输出质量和正确性的对齐。

Result: 在跨任务设置中，贝叶斯最优响应在性能上持续优于常规的束搜索等解码方法；不确定性估计与输出质量、正确性之间的对齐性提高。

Conclusion: 该决策理论框架可推广至任何具备潜在响应结构的问题，支持更具任务感知性的LLM预测。

Abstract: In many applications of LLMs, natural language responses often have an underlying structure such as representing discrete labels, numerical values, or graphs. Yet, existing decoding and uncertainty estimation methods operate only in language space and largely disregard structural information. We address this by modeling LLM outputs directly in a task-dependent latent structure. By equipping this structure with a dissimilarity measure, we can compute Bayes-optimal responses. These are not selected from sampled generations but are newly synthesized by combining individual responses in the latent space. Across different tasks, Bayes-optimal responses consistently outperform standard decoding methods like beam search. Moreover, quantifying uncertainty via the induced Bayesian risk captures variations in terms of the latent structure and improves alignment with output quality and correctness. Our decision-theoretic framework is applicable to any problem that admits a latent response structure and enables reliable task-aware LLM predictions.

</details>


### [203] [Cascaded Transfer: Learning Many Tasks under Budget Constraints](https://arxiv.org/abs/2601.21513)
*Eloi Campagne,Yvenn Amara-Ouali,Yannig Goude,Mathilde Mougeot,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 提出了级联迁移学习（Cascaded Transfer Learning）用于多任务学习，在同类模型下通过一个根树结构对任务进行层级级联迁移，在任务之间通过最小生成树连接并在预算约束下分配训练资源，以实现更高精度和更低成本的跨大规模任务的适配。


<details>
  <summary>Details</summary>
Motivation: 在大量相关任务且任务间关系未知且受预算约束的场景中，现有方法难以高效利用跨任务知识。需要一种能够在树状结构中有序、分阶段地传递信息的迁移策略。

Method: 提出一个基于最小生成树的级联迁移机制，将任务以根树组织，按距离度量连接任务，并在树支路径上分配可用的训练预算，通过级联传播模型参数等信息来实现跨任务知识传递。

Result: 在合成数据和真实的多任务设置上进行实验，所提方法在准确性和成本效益方面优于替代方法。

Conclusion: 树形级联迁移学习能够在大规模任务集合中高效地进行知识迁移，特别是在预算受限的情境下，通过最小生成树连接与分配预算显著提升适配效果和效率。

Abstract: Many-Task Learning refers to the setting where a large number of related tasks need to be learned, the exact relationships between tasks are not known. We introduce the Cascaded Transfer Learning, a novel many-task transfer learning paradigm where information (e.g. model parameters) cascades hierarchically through tasks that are learned by individual models of the same class, while respecting given budget constraints. The cascade is organized as a rooted tree that specifies the order in which tasks are learned and refined. We design a cascaded transfer mechanism deployed over a minimum spanning tree structure that connects the tasks according to a suitable distance measure, and allocates the available training budget along its branches. Experiments on synthetic and real many-task settings show that the resulting method enables more accurate and cost effective adaptation across large task collections compared to alternative approaches.

</details>


### [204] [A Unified SPD Token Transformer Framework for EEG Classification: Systematic Comparison of Geometric Embeddings](https://arxiv.org/abs/2601.21521)
*Chi-Sheng Chen,En-Jui Kuo,Guan-Ying Chen,Xinyu Zhang,Fan Zhang*

Main category: cs.LG

TL;DR: 对 SPD EEG 协方差矩阵的嵌入几何与优化动力学的理论分析； BWSPD 在梯度条件数方面优于 Log-Euclidean； Embedding-Space Batch Normalization(BN-Embed) 对 Riemannian 归一化的近似及其数据依赖性； 双向 Lipschitz 性质下的距离保留； 通过统一 Transformer 框架在多任务数据集上比较嵌入，Log-Euclidean Transformer 达到最优或接近最优。


<details>
  <summary>Details</summary>
Motivation: 填补嵌入几何与优化动态之间的理论空白，明确不同嵌入对梯度稳定性和数值稳定性的影响，并在实际 EEG 任务中验证。

Method: 基于 Daleckii–Krein 矩阵的梯度分析比较 BWSPD 与 Log-Euclidean 的梯度条件数；给出 BN-Embed 相对于 Riemannian 标准化的误差界，及其在 ε^2 量纲上的界限；给出 bi-Lipschitz 的上下界，证明 BWSPD 在流形距离的保真性受 κ（条件数）的控制；在相同架构下进行 1,500+ 次运行的统一 Transformer 框架比较，覆盖 motor imagery、ERP、SSVEP，36 名受试者。

Result: BWSPD 具有 sqrt(κ) 的梯度条件数，相比 Log-Euclidean 的 κ 提升在高维输入（d≥22）中显著；在低维输入（d≤8）由于特征分解开销，优势减弱；BN-Embed 在 Riemannian 归一化上近似误差为 O( ε^2 )，在 56 通道 ERP 数据上提升约 26% 的准确率，但对 8 通道 SSVEP 数据影响甚微，符合通道数相关预测；双向李氏界限表明 BWSPD 标记在保留流形距离方面的失真仅受 κ 控制。综合实验显示，Log-Euclidean Transformer 在所有数据集达到最先进的性能，显著优于经典 Riemannian 分类器与最新 SPD 基线；BWSPD 在训练时间相近的情况下也具竞争力，表现接近或略逊于 Log-Euclidean。

Conclusion: 嵌入选择对 SPD 流形上的优化性与数值稳定性具有决定性影响。Log-Euclidean 方案在多数据集上表现最佳，BWSPD 在高维场景有梯度条件数的优势但需权衡计算成本；BN-Embed 的收益依赖通道数，应结合具体任务进行权衡。

Abstract: Spatial covariance matrices of EEG signals are Symmetric Positive Definite (SPD) and lie on a Riemannian manifold, yet the theoretical connection between embedding geometry and optimization dynamics remains unexplored. We provide a formal analysis linking embedding choice to gradient conditioning and numerical stability for SPD manifolds, establishing three theoretical results: (1) BWSPD's $\sqrtκ$ gradient conditioning (vs $κ$ for Log-Euclidean) via Daleckii-Kreĭn matrices provides better gradient conditioning on high-dimensional inputs ($d \geq 22$), with this advantage reducing on low-dimensional inputs ($d \leq 8$) where eigendecomposition overhead dominates; (2) Embedding-Space Batch Normalization (BN-Embed) approximates Riemannian normalization up to $O(\varepsilon^2)$ error, yielding $+26\%$ accuracy on 56-channel ERP data but negligible effect on 8-channel SSVEP data, matching the channel-count-dependent prediction; (3) bi-Lipschitz bounds prove BWSPD tokens preserve manifold distances with distortion governed solely by the condition ratio $κ$. We validate these predictions via a unified Transformer framework comparing BWSPD, Log-Euclidean, and Euclidean embeddings within identical architecture across 1,500+ runs on three EEG paradigms (motor imagery, ERP, SSVEP; 36 subjects). Our Log-Euclidean Transformer achieves state-of-the-art performance on all datasets, substantially outperforming classical Riemannian classifiers and recent SPD baselines, while BWSPD offers competitive accuracy with similar training time.

</details>


### [205] [More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)](https://arxiv.org/abs/2601.21522)
*Sagi Meir,Tommer D. Keidar,Noam Levi,Shlomi Reuveni,Barak Hirshberg*

Main category: cs.LG

TL;DR: 提出 Reset-and-Discard (ReD) 策略，在给定预算下显著提升覆盖度（coverage@cost），并能在有或无 pass@k 的情况下推断幂律指数及节省尝试成本；在三种 LLM 与 HumanEval 上实现显著成本与资源节约。


<details>
  <summary>Details</summary>
Motivation: 现有评估以 pass@k 衡量正确回答的概率，但在固定预算下，覆盖率随尝试次数的增长呈现幂律型、边际收益递减，难以在同一基准下公平比较并高效估计推断能力。需要一种统一的覆盖度度量及能够在不同 pass@k 条件下提升覆盖的查询策略。

Method: 提出 Reset-and-Discard (ReD) 查询策略，通过重置模型状态与丢弃低效尝试来提升覆盖@cost；在已知 pass@k 时，可定量预测 ReD 的尝试节省；在未知 pass@k 时，ReD 能推断其幂律指数。

Result: 在三种 LLM 与 HumanEval 数据集上，ReD 显著降低达到目标覆盖所需的尝试次数、token 数量和成本，并提供一种高效的方式来测量推断功率律。

Conclusion: ReD 为提升覆盖度的通用查询策略，适配有无 pass@k 的模型；同时提供对幂律指数的推断能力，便于在资源受限场景下评估与优化大模型的推理能力与成本。

Abstract: The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.

</details>


### [206] [GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization](https://arxiv.org/abs/2601.22095)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Chi Wang,Yuehao Wang,Jing Xiong,Liliang Ren,Bo Peng,Qingmei Wang,Xiaoran Shang,Mac Schwager,Anderson Schneider,Yuriy Nevmyvaka,Xiaodong Liu*

Main category: cs.LG

TL;DR: GeoNorm 将归一化替换为在流形上的测地更新，结合层级更新衰减，提升 Transformer 性能且开销低，且可无缝集成。


<details>
  <summary>Details</summary>
Motivation: 解决 Pre-Norm 与 Post-Norm 在 Transformer 放置的争议，并寻找比标准归一化更统一、高效的替代方案。

Method: 把 FFN 与注意力的输出视为优化更新方向，在流形上进行测地更新；引入层级更新衰减作为学习率调度的类比；可无缝嵌入现有 Transformer。

Result: 实验表明 GeoNorm 在多项基准上优于传统归一化方法，且计算成本几乎不增加。

Conclusion: GeoNorm 提供了一个可插拔的高效替代方案，透过几何更新提升性能，并为归一化策略的设计提供新方向。

Abstract: The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in Transformer architecture design. In this work, we rethink these approaches through the lens of manifold optimization, interpreting the outputs of the Feed-Forward Network (FFN) and attention layers as update directions in optimization. Building on this perspective, we introduce GeoNorm, a novel method that replaces standard normalization with geodesic updates on the manifold. Furthermore, analogous to learning rate schedules, we propose a layer-wise update decay for the FFN and attention components. Comprehensive experiments demonstrate that GeoNorm consistently outperforms existing normalization methods in Transformer models. Crucially, GeoNorm can be seamlessly integrated into standard Transformer architectures, achieving performance improvements with negligible additional computational cost.

</details>


### [207] [Multi-Modal Time Series Prediction via Mixture of Modulated Experts](https://arxiv.org/abs/2601.21547)
*Lige Zhang,Ali Maatouk,Jialin Chen,Leandros Tassiulas,Rex Ying*

Main category: cs.LG

TL;DR: 提出 Expert Modulation (MoME) 的多模态时序预测框架，通过文本信号对路由与专家计算进行调制，直接实现跨模态控制，克服稀缺时间-文本对与尺度差异引起的对齐难题，实验显示显著提升。


<details>
  <summary>Details</summary>
Motivation: 现实时间序列具有复杂且动态的模式，单模态预测困难；文本信息能提升预测，但现有多模态方法多采用 token 级融合，在稀缺高质量时间-文本对和跨尺度特征下难以稳健对齐。

Method: 提出基于 Mixture-of-Experts 的 Expert Modulation 框架，基于文本信号对路由与专家计算进行条件调制，实现跨模态直接控制的专家行为；给出理论分析，并在多模态时间序列预测任务上进行实证评估，代码开放。

Result: 在多模态时间序列预测任务中获得显著性能提升，与基线 MoE 与文本融合方法相比具显著优势；理论分析与实验证据相符；代码可在 GitHub 获取。

Conclusion: MoME 提供了一种高效的跨模态时间序列预测范式，证明通过文本信号对路由和专家的调制能有效提升多模态表示与预测性能。

Abstract: Real-world time series exhibit complex and evolving dynamics, making accurate forecasting extremely challenging. Recent multi-modal forecasting methods leverage textual information such as news reports to improve prediction, but most rely on token-level fusion that mixes temporal patches with language tokens in a shared embedding space. However, such fusion can be ill-suited when high-quality time-text pairs are scarce and when time series exhibit substantial variation in scale and characteristics, thus complicating cross-modal alignment. In parallel, Mixture-of-Experts (MoE) architectures have proven effective for both time series modeling and multi-modal learning, yet many existing MoE-based modality integration methods still depend on token-level fusion. To address this, we propose Expert Modulation, a new paradigm for multi-modal time series prediction that conditions both routing and expert computation on textual signals, enabling direct and efficient cross-modal control over expert behavior. Through comprehensive theoretical analysis and experiments, our proposed method demonstrates substantial improvements in multi-modal time series prediction. The current code is available at https://github.com/BruceZhangReve/MoME

</details>


### [208] [HistoPrism: Unlocking Functional Pathway Analysis from Pan-Cancer Histology via Gene Expression Prediction](https://arxiv.org/abs/2601.21560)
*Susu Hu,Qinghe Zeng,Nithya Bhasker,Jakob Nicolas Kather,Stefanie Speidel*

Main category: cs.LG

TL;DR: 提出 HistoPrism：一种基于 Transformer 的全癌种跨癌症预测模型，用 H&E 病理图像预测空间基因表达。通过引入 pathway 级别基准，评估在功能性通路上的一致性与生物学意义，显著优于现有方法，具备良好跨 cancer 的泛化能力与更高的效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多局限于单一癌种且以方差为主的评估，难以体现生物学功能相关性。需要能够跨癌种泛化、捕捉生物学相关信号的模型，以实现临床应用。

Method: 提出 HistoPrism，一种高效的 Transformer 架构，用于从 H&E 病理图像在全癌种层面预测基因表达。为评估生物学意义，提出 pathway 级别的基准，将评估从单基因方差转向对齐且具有生物功能相关性的通路级别评价。

Result: 相较于现有方法，HistoPrism 在高变异基因上实现领先，并在 pathway-level 预测上获得显著提升，显示出恢复生物学一致的转录组模式的能力。具备强泛化至多癌种的表现与更高的计算效率，确立了从常规病理切片中建模转录组的新标准。

Conclusion: HistoPrism 证明了跨癌种、跨通路层面的转录组建模在病理图像上的可行性与临床相关性，推动临床应用的实现与未来方法学的发展。

Abstract: Predicting spatial gene expression from H&E histology offers a scalable and clinically accessible alternative to sequencing, but realizing clinical impact requires models that generalize across cancer types and capture biologically coherent signals. Prior work is often limited to per-cancer settings and variance-based evaluation, leaving functional relevance underexplored. We introduce HistoPrism, an efficient transformer-based architecture for pan-cancer prediction of gene expression from histology. To evaluate biological meaning, we introduce a pathway-level benchmark, shifting assessment from isolated gene-level variance to coherent functional pathways. HistoPrism not only surpasses prior state-of-the-art models on highly variable genes , but also more importantly, achieves substantial gains on pathway-level prediction, demonstrating its ability to recover biologically coherent transcriptomic patterns. With strong pan-cancer generalization and improved efficiency, HistoPrism establishes a new standard for clinically relevant transcriptomic modeling from routinely available histology.

</details>


### [209] [Discovering Hidden Gems in Model Repositories](https://arxiv.org/abs/2601.22157)
*Jonathan Kahana,Eliahu Horwitz,Yedid Hoshen*

Main category: cs.LG

TL;DR: 发现大量未广泛下载的微调模型中存在显著超越流行模型的“隐藏宝石”，并提出通过改进的多臂老虎机—Sequential Halving搜索在有限查询下高效发现高性能模型的方法，且在不增加推理成本的前提下提升数学任务表现。


<details>
  <summary>Details</summary>
Motivation: 揭示市场对基础检查点高度集中的现象是否导致潜在优质模型被忽视，以及如何高效发现这些隐藏宝石。

Method: 对超过2000个微调模型进行广泛评估。以 Llama-3.1-8B 家族为例，发现与下载量低的模型显著提升数学性能（83.2%提升至96.0%），且推理成本不变。将模型发现问题建模为多臂老虎机，改进 Sequential Halving，借助共享查询集和更激进的淘汰策略，使每个候选仅需约50次查询即可检出顶尖模型，发现效率提升超过50倍。

Result: 验证存在大量隐藏宝石，且提出的加速发现方法能在显著减少查询成本的同时检出高性能微调模型；特定任务（数学）上可获得明显性能提升，且不增加推理代价。

Conclusion: 市场选择偏好未必导向最优模型，通过带有探索-利用权衡的Bandit式搜索，可以更高效地发现隐藏高性能模型，从而提高资源利用率与模型治理透明度。

Abstract: Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of "hidden gems", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.

</details>


### [210] [FlexCausal: Flexible Causal Disentanglement via Structural Flow Priors and Manifold-Aware Interventions](https://arxiv.org/abs/2601.21567)
*Yutao Jin,Yuang Tao,Junyong Zhai*

Main category: cs.LG

TL;DR: 提出 FlexCausal，一种基于块对角协方差的 VAE 的因果解缠学习框架，使用 Factorized Flow 先验建模复杂的外生噪声密度，并结合监督对齐与反事实一致性约束实现因果结构的精准对齐与高保真生成，在合成与真实数据上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有因果解缠方法多采用对角协方差的均值场近似，导致潜在维度彼此解耦，且对外生噪声多采用各向同性高斯先验，难以捕捉真实数据中的非高斯统计特性。因此，需要在不被分布统计约束影响的前提下学习潜在因果结构，并提升生成质量。

Method: 提出 FlexCausal 框架：采用块对角协方差的 VAE；引入 Factorized Flow-based Prior 以真实建模外生噪声的复杂密度；结合监督对齐目标与反事实一致性约束，确保学习的潜在子空间与真实因果关系之间存在结构对应；引入流形感知的相对干预策略以提升生成保真度。

Result: 在合成和真实数据集上，FlexCausal 相较于其他方法显示显著性能提升。

Conclusion: 该框架实现了潜在子空间与真实因果关系的精准结构对应，能够在不被分布统计干扰的情况下学习因果结构并提供高保真生成，适用于复杂因果表示学习场景。

Abstract: Causal Disentangled Representation Learning(CDRL) aims to learn and disentangle low dimensional representations and their underlying causal structure from observations. However, existing disentanglement methods rely on a standard mean-field approximation with a diagonal posterior covariance, which decorrelates all latent dimensions. Additionally, these methods often assume isotropic Gaussian priors for exogenous noise, failing to capture the complex, non-Gaussian statistical properties prevalent in real-world causal factors. Therefore, we propose FlexCausal, a novel CDRL framework based on a block-diagonal covariance VAE. FlexCausal utilizes a Factorized Flow-based Prior to realistically model the complex densities of exogenous noise, effectively decoupling the learning of causal mechanisms from distributional statistics. By integrating supervised alignment objectives with counterfactual consistency constraints, our framework ensures a precise structural correspondence between the learned latent subspaces and the ground-truth causal relations. Finally, we introduce a manifold-aware relative intervention strategy to ensure high-fidelity generation. Experimental results on both synthetic and real-world datasets demonstrate that FlexCausal significantly outperforms other methods.

</details>


### [211] [Bridging Functional and Representational Similarity via Usable Information](https://arxiv.org/abs/2601.21568)
*Antonio Almudévar,Alfonso Ortega*

Main category: cs.LG

TL;DR: 给出一个以可用信息为核心的统一框架，连接功能性和表征性相似性；关键发现包括缝合的非对称性、可用信息的估计、预测能力对相似性的决定性作用，以及以输入重构为最细粒度的任务粒度层级。


<details>
  <summary>Details</summary>
Motivation: 在单一信息理论视角下统一量化不同表示的相似性，建立功能性与表征性之间的联系；解释重构、CKA、RSA等指标在不同前提下的含义；考察任务粒度对相似性的影响。

Method: 通过形式化定义可用信息，建立缝合性能与条件互信息的联系；证明缝合具有方向性，需双向分析；将重构型度量及CKA/RSA视为可用信息的估计器，在受限条件下成立；分析预测家族容量对相似性的决定作用；提出任务粒度层级，将输入重构作为最细粒度；结合理论推导与经验验证。

Result: 1) 缝合性能与条件互信息直接相关；2) 缝合是非对称的，需双向比较；3) 重构型度量与CKA/RSA在一定约束下是可用信息的估计器；4) 相似性依赖于预测家族的容量；5) 表征性充要但非必要于功能性；6) 存在从复杂任务到简单派生任务的层级关系，输入重构等效于最细粒度的表征相似性。

Conclusion: 表征相似性是最大粒度（输入重构）下的极限，建立了一个统一的评估框架，帮助在不同场景下正确解读相似性指标；提示在分析中应考虑观察者容量与双向性；对设计跨模型对比和解释性研究具有实用意义。

Abstract: We present a unified framework for quantifying the similarity between representations through the lens of \textit{usable information}, offering a rigorous theoretical and empirical synthesis across three key dimensions. First, addressing functional similarity, we establish a formal link between stitching performance and conditional mutual information. We further reveal that stitching is inherently asymmetric, demonstrating that robust functional comparison necessitates a bidirectional analysis rather than a unidirectional mapping. Second, concerning representational similarity, we prove that reconstruction-based metrics and standard tools (e.g., CKA, RSA) act as estimators of usable information under specific constraints. Crucially, we show that similarity is relative to the capacity of the predictive family: representations that appear distinct to a rigid observer may be identical to a more expressive one. Third, we demonstrate that representational similarity is sufficient but not necessary for functional similarity. We unify these concepts through a task-granularity hierarchy: similarity on a complex task guarantees similarity on any coarser derivative, establishing representational similarity as the limit of maximum granularity: input reconstruction.

</details>


### [212] [Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks](https://arxiv.org/abs/2601.21572)
*Jinhao Li,Yuhao Sun,Zhiyuan Ma,Hao He,Xinche Zhang,Xing Chen,Jin Li,Sen Song*

Main category: cs.LG

TL;DR: 提出了 Signal-Adaptive Trust Regions (SATR) 的分布级更新规则，用以约束基于人口的梯度无关优化中 RSNN 的更新，方法通过用估计的信号能量对 KL 散度进行归一化来自适应地限制相对变化。为 Bernoulli 连通分布实现 SATR，并提出 binary 位集实现以加速二进制脉冲/权重的训练。实证显示在高维连续控制任务中，SATR 在小样本群体下提高稳定性，并能与 PPO-LSTM 等强基线竞争；位集实现显著降低了训练时间。


<details>
  <summary>Details</summary>
Motivation: 在高维、长时程强化学习中，RSNN 的训练面临非微分脉冲动力学导致的高度方差问题。基于人口的梯度无关优化虽可绕过反向传播，但有限种群导致估计方差大，更新剧烈易致不稳定。引入类似信任域的更新约束以稳定学习。

Method: 提出 SATR，通过在分布空间中约束相对变动来限制更新，具体为以估计的信号能量对 KL 发散进行归一化，动态地放大强信号下的信任域、在噪声主导时收缩信任域。针对 Bernoulli 连通性分布实现，结合 RSNN 的离散化特性。为了可扩展性，提出二进制脉冲和权重的位集实现，减小内存和计算开销。

Result: 在一组高维连续控制基准上，SATR 在有限种群下提高稳定性，并在性能上与强基线（包括 PPO-LSTM）相近或具竞争力。位集实现显著降低训练时间，使 RSNN 策略搜索更具可扩展性。

Conclusion: SATR 提供了一种自适应的信任区域更新机制，结合分布与信号能量的归一化 KL 控制，提升了基于人口的梯度无关优化在 RSNN 上的稳定性与效率；二进制实现进一步提高了可扩展性。

Abstract: Recurrent spiking neural networks (RSNNs) are a promising substrate for energy-efficient control policies, but training them for high-dimensional, long-horizon reinforcement learning remains challenging. Population-based, gradient-free optimization circumvents backpropagation through non-differentiable spike dynamics by estimating gradients. However, with finite populations, high variance of these estimates can induce harmful and overly aggressive update steps. Inspired by trust-region methods in reinforcement learning that constrain policy updates in distribution space, we propose \textbf{Signal-Adaptive Trust Regions (SATR)}, a distributional update rule that constrains relative change by bounding KL divergence normalized by an estimated signal energy. SATR automatically expands the trust region under strong signals and contracts it when updates are noise-dominated. We instantiate SATR for Bernoulli connectivity distributions, which have shown strong empirical performance for RSNN optimization. Across a suite of high-dimensional continuous-control benchmarks, SATR improves stability under limited populations and reaches competitive returns against strong baselines including PPO-LSTM. In addition, to make SATR practical at scale, we introduce a bitset implementation for binary spiking and binary weights, substantially reducing wall-clock training time and enabling fast RSNN policy search.

</details>


### [213] [Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient Similarity](https://arxiv.org/abs/2601.21577)
*Mutian Yang,Zisen Zhan,Yutong Chen,Haolin Li,Kaiwen Wang,Kaili Zheng,Yuguang Wang,Qi Wang,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: 提出一个基于梯度相似性的理论框架来解释并缓解大语言模型中的灾难性遗忘。将神经元按梯度相似性分为冲突神经元和协同神经元，冻结冲突神经元、仅更新协同神经元形成Collaborative Neural Learning (CNL)，在理论上可实现零遗忘并在多组实验中显著降低遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中灾难性遗忘缺乏理论支撑的问题，尤其在知识注入到LLMs时，希望通过梯度行为揭示遗忘机制。

Method: 建立梯度导向的理论框架；证明强负梯度相似性是遗忘的根本原因；利用梯度相似性将神经元分为冲突神经元（约50%-75%）和协同神经元（约25%-50%）；提出CNL，通过冻结冲突神经元、仅更新协同神经元，在无穷小学习率η和已掌握集合条件下理论上消除遗忘；在五个LLMs、四个数据集、四个优化器上进行实验。

Result: 在就地集设置中实现零遗忘，在出集设置中遗忘降低约59.1%-81.7%；覆盖5个LLM、4个数据集、4个优化器的测试。

Conclusion: 基于梯度相似性的神经元划分为灾难性遗忘提供理论支撑，CNL在理论与实证上均显示强效，为持续学习提供可行的路径。

Abstract: Catastrophic forgetting during knowledge injection severely undermines the continual learning capability of large language models (LLMs). Although existing methods attempt to mitigate this issue, they often lack a foundational theoretical explanation. We establish a gradient-based theoretical framework to explain catastrophic forgetting. We first prove that strongly negative gradient similarity is a fundamental cause of forgetting. We then use gradient similarity to identify two types of neurons: conflicting neurons that induce forgetting and account for 50%-75% of neurons, and collaborative neurons that mitigate forgetting and account for 25%-50%. Based on this analysis, we propose a knowledge injection method, Collaborative Neural Learning (CNL). By freezing conflicting neurons and updating only collaborative neurons, CNL theoretically eliminates catastrophic forgetting under an infinitesimal learning rate eta and an exactly known mastered set. Experiments on five LLMs, four datasets, and four optimizers show that CNL achieves zero forgetting in in-set settings and reduces forgetting by 59.1%-81.7% in out-of-set settings.

</details>


### [214] [Evaluating Prediction Uncertainty Estimates from BatchEnsemble](https://arxiv.org/abs/2601.21581)
*Morten Blørstad,Herman Jangsett Mostein,Nello Blaser,Pekka Parviainen*

Main category: cs.LG

TL;DR: BatchEnsemble 提供了一种可扩展的不确定性估计方法，适用于表格数据和时间序列。通过引入 GRUBE GRU 单元扩展到序列建模。与蒙特卡洛 dropout 和深度集成进行对比，BatchEnsemble 与深度集成在不确定性估计上等效，显著优于 MC dropout；GRUBE 在预测和不确定性估计上表现相近或更好，且参数更少、训练与推理更快。


<details>
  <summary>Details</summary>
Motivation: 在深度学习中，准确而高效的不确定性估计仍具挑战性，现有方法要么计算量大、要么倾向低估不确定性。需要一种对表格数据与时间序列任务都适用且高效的估计方法。

Method: 提出 BatchEnsemble 作为通用且可扩展的不确定性估计方法；为序列建模扩展，提出 GRUBE——基于 BatchEnsemble 的 GRU 单元。对比评估对象包括 Monte Carlo dropout 与深度集成。

Result: BatchEnsemble 在不确定性估计方面与深度集成相当，明显优于 Monte Carlo dropout；GRUBE 在预测和不确定性估计方面与 BatchEnsemble 相当或更好；两者相比传统集成在参数数量、训练和推理时间上具有优势。

Conclusion: BatchEnsemble 与 GRUBE 能在较少参数和更低计算成本的前提下，获得与传统 ensembles 相近甚至更优的不确定性估计与预测性能。

Abstract: Deep learning models struggle with uncertainty estimation. Many approaches are either computationally infeasible or underestimate uncertainty. We investigate \textit{BatchEnsemble} as a general and scalable method for uncertainty estimation across both tabular and time series tasks. To extend BatchEnsemble to sequential modeling, we introduce GRUBE, a novel BatchEnsemble GRU cell. We compare the BatchEnsemble to Monte Carlo dropout and deep ensemble models. Our results show that BatchEnsemble matches the uncertainty estimation performance of deep ensembles, and clearly outperforms Monte Carlo dropout. GRUBE achieves similar or better performance in both prediction and uncertainty estimation. These findings show that BatchEnsemble and GRUBE achieve similar performance with fewer parameters and reduced training and inference time compared to traditional ensembles.

</details>


### [215] [CORDS: Continuous Representations of Discrete Structures](https://arxiv.org/abs/2601.21583)
*Tin Hadži Veljković,Erik Bekkers,Michael Tiemann,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 通过连续表示来预测任意大小集合的通用框架CORDS，将集合映射为可逆的密度场与特征场，在场空间进行模型训练并可精确解码回离散集合，解决集合大小未知的问题；在分子生成、目标检测等领域表现出有竞争力的准确性。


<details>
  <summary>Details</summary>
Motivation: 许多学习任务需要预测大小未知的集合；现有方法依赖填充表示或必须显式推断集合大小，存在效率与准确性挑战。需要一个在连续空间中进行推理且可精确解码的统一框架。

Method: 提出CORDS，将离散集合通过可逆映射映射到连续的密度场（编码对象位置与数量）和特征场（对象属性在相同区域的分布）。模型在字段空间中运算，最终可精确解码回离散集合。映射具有可逆性确保无损解码。

Result: 在分子生成与回归、目标检测、基于仿真的推理，以及一个局部极大点回归的数学任务上进行评估，显示在未知集合大小下的鲁棒性，并达到具有竞争力的准确性。

Conclusion: 该方法为未知集合大小的预测提供无填充、无显式大小推断的通用框架，具备可精确解码的特性，具有广泛潜在应用；未来工作可聚焦于映射的高效性、尺度性及对复杂场景的适应性。

Abstract: Many learning problems require predicting sets of objects when the number of objects is not known beforehand. Examples include object detection, molecular modeling, and scientific inference tasks such as astrophysical source detection. Existing methods often rely on padded representations or must explicitly infer the set size, which often poses challenges. We present a novel strategy for addressing this challenge by casting prediction of variable-sized sets as a continuous inference problem. Our approach, CORDS (Continuous Representations of Discrete Structures), provides an invertible mapping that transforms a set of spatial objects into continuous fields: a density field that encodes object locations and count, and a feature field that carries their attributes over the same support. Because the mapping is invertible, models operate entirely in field space while remaining exactly decodable to discrete sets. We evaluate CORDS across molecular generation and regression, object detection, simulation-based inference, and a mathematical task involving recovery of local maxima, demonstrating robust handling of unknown set sizes with competitive accuracy.

</details>


### [216] [Heterogeneity-Aware Knowledge Sharing for Graph Federated Learning](https://arxiv.org/abs/2601.21589)
*Wentao Yu,Sheng Wan,Shuo Chen,Bo Han,Chen Gong*

Main category: cs.LG

TL;DR: FedSSA introduces semantic and structural alignment for Graph Federated Learning to address both feature and topology heterogeneity across clients by clustering based on learned distributions and spectral energy, and aligning local and cluster-level GNNs; it achieves state-of-the-art performance across diverse datasets and partition settings.


<details>
  <summary>Details</summary>
Motivation: Graph Federated Learning suffers from two types of heterogeneity: varying node feature distributions (semantic) and varying graph structures (topology). Without addressing both, cross-client knowledge transfer is impaired and privacy-preserving learning degrades.

Method: 1) Semantic alignment: use a variational model to infer class-wise node distributions per client; cluster clients by these inferred distributions and train cluster-level representative distributions; minimize divergence between local and cluster-level distributions to share semantic knowledge. 2) Structural alignment: compute spectral energy as a measure of structural information using spectral GNNs; cluster clients by spectral energy and build cluster-level spectral GNNs; align local spectral characteristics with cluster-level ones to share structural knowledge.

Result: Experiments on six homophilic and five heterophilic graph datasets under both non-overlapping and overlapping partitioning show that FedSSA consistently outperforms eleven state-of-the-art methods.

Conclusion: FedSSA effectively mitigates both semantic and structural heterogeneity in Graph Federated Learning by dual-level clustering and alignment, delivering robust improvements across diverse datasets and partition settings.

Abstract: Graph Federated Learning (GFL) enables distributed graph representation learning while protecting the privacy of graph data. However, GFL suffers from heterogeneity arising from diverse node features and structural topologies across multiple clients. To address both types of heterogeneity, we propose a novel graph Federated learning method via Semantic and Structural Alignment (FedSSA), which shares the knowledge of both node features and structural topologies. For node feature heterogeneity, we propose a novel variational model to infer class-wise node distributions, so that we can cluster clients based on inferred distributions and construct cluster-level representative distributions. We then minimize the divergence between local and cluster-level distributions to facilitate semantic knowledge sharing. For structural heterogeneity, we employ spectral Graph Neural Networks (GNNs) and propose a spectral energy measure to characterize structural information, so that we can cluster clients based on spectral energy and build cluster-level spectral GNNs. We then align the spectral characteristics of local spectral GNNs with those of cluster-level spectral GNNs to enable structural knowledge sharing. Experiments on six homophilic and five heterophilic graph datasets under both non-overlapping and overlapping partitioning settings demonstrate that FedSSA consistently outperforms eleven state-of-the-art methods.

</details>


### [217] [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: 一个训练无关、无需验证者的自回归分布锐化方法：通过将全局幂分布近似为带低温缩放的令牌级分布，并以未来序列质量来决定缩放，从而实现对基模型生成分布的锐化，达到相当于或超越一次性 GRPO 的效果，且比基于 MCMC 的采样更快。


<details>
  <summary>Details</summary>
Motivation: RL后训练提升LLM推理能力的证据多来自分布锐化而非获得新能力；MCMC采样可在无外部奖励的情况下恢复性能，但计算成本高，需寻找高效替代。

Method: 给出理论公式：全局幂分布可被一个带低温缩放的令牌级分布近似，缩放因子捕捉未来序列质量；提出训练-free且 verifier-free 的算法，基于自回归方式对生成分布进行锐化。

Result: 在数学、问答、代码任务的4个LLM上评估，方法达到或超过单轮GRPO的效果，且不依赖外部奖励；推理延迟比MCMC采样低十倍以上。

Conclusion: 提供一种实用、高效的替代RL后训练与MCMC采样的方法，能够在更低成本下实现相媲美的推理提升。

Abstract: Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.

</details>


### [218] [Beyond Parameter Finetuning: Test-Time Representation Refinement for Node Classification](https://arxiv.org/abs/2601.21615)
*Jiaxin Zhang,Yiqi Wang,Siwei Wang,Xihong Yang,Yu Shi,Xinwang Liu,En Zhu*

Main category: cs.LG

TL;DR: TTReFT 将 TTT 的适应目标从模型参数转为潜在表示，通过不确定性引导的节点选择、低秩表示干预以及面向干预的自编码器实现，提出在 OOD 场景下的理论保证和五个基准数据集上的一致性优越表现。


<details>
  <summary>Details</summary>
Motivation: GNN 在 out-of-distribution 测试中性能显著下降；参数微调（PaFT）易造成灾难性遗忘，需新的测试时刻自适应范式。

Method: 三大创新：1) 不确定性引导的节点选择进行干预；2) 低秩表示干预以保留预训练知识；3) 面向干预的掩码自编码器，动态调整掩码策略以适应选中的节点。并给出对 OOD 的理论保证。

Result: 在五个基准数据集上进行大量实验，TTReFT 能得到一致且更优的性能。

Conclusion: 将表示微调作为图形 TTT 的新范式，具有理论基础和对实际部署的直接效用。

Abstract: Graph Neural Networks frequently exhibit significant performance degradation in the out-of-distribution test scenario. While test-time training (TTT) offers a promising solution, existing Parameter Finetuning (PaFT) paradigm suffer from catastrophic forgetting, hindering their real-world applicability. We propose TTReFT, a novel Test-Time Representation FineTuning framework that transitions the adaptation target from model parameters to latent representations. Specifically, TTReFT achieves this through three key innovations: (1) uncertainty-guided node selection for specific interventions, (2) low-rank representation interventions that preserve pre-trained knowledge, and (3) an intervention-aware masked autoencoder that dynamically adjust masking strategy to accommodate the node selection scheme. Theoretically, we establish guarantees for TTReFT in OOD settings. Empirically, extensive experiments across five benchmark datasets demonstrate that TTReFT achieves consistent and superior performance. Our work establishes representation finetuning as a new paradigm for graph TTT, offering both theoretical grounding and immediate practical utility for real-world deployment.

</details>


### [219] [LAMP: Look-Ahead Mixed-Precision Inference of Large Language Models](https://arxiv.org/abs/2601.21623)
*Stanislav Budzinskiy,Marian Gloser,Tolunay Yilmaz,Ying Hong Tham,Yuanyi Lin,Wenyi Fang,Fan Wu,Philipp Petersen*

Main category: cs.LG

TL;DR: 基于对 f(g(x)) 的舍入误差分析，提出自适应混合精度策略，在 transformer 推理中选择性地对 g(x) 的一部分分量进行高精度计算，其余部分使用低精度，以降低成本并保持或提升精度；在 GPT-2 上数值验证，低重计算率下也能实现约两数量级的精度提升。


<details>
  <summary>Details</summary>
Motivation: 回应 AI 领域对高效、就地部署的混合精度需求，解决在 transformer 中多层复合函数的数值误差控制问题，通过对舍入误差的分析实现精度分配的自适应策略。

Method: 基于对 f(g(x)) 的舍入误差分析，设计自适应策略来选择需要更高精度的 g(x) 分量；将该策略应用于 transformer 的不同组成部分（如注意力、前馈子层等的不同路径或子计算），并给出实现细节与泛化思路。

Result: 在 GPT-2 的数值实验中，证明即使极低的重计算率也可带来显著的精度提升，达到约两数量级的改进。

Conclusion: 该自适应混合精度策略在 transformer 推理中能以更低的计算成本实现更高的数值准确性，便于在资源受限场景的部署。

Abstract: Mixed-precision computations are a hallmark of the current stage of AI, driving the progress in large language models towards efficient, locally deployable solutions. This article addresses the floating-point computation of compositionally-rich functions, concentrating on transformer inference. Based on the rounding error analysis of a composition $f(g(\mathrm{x}))$, we provide an adaptive strategy that selects a small subset of components of $g(\mathrm{x})$ to be computed more accurately while all other computations can be carried out with lower accuracy. We then explain how this strategy can be applied to different compositions within a transformer and illustrate its overall effect on transformer inference. We study the effectiveness of this algorithm numerically on GPT-2 models and demonstrate that already very low recomputation rates allow for improvements of up to two orders of magnitude in accuracy.

</details>


### [220] [Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation](https://arxiv.org/abs/2601.21636)
*Jan Schuchardt,Nikita Kalinin*

Main category: cs.LG

TL;DR: 提出对随机分配下的矩阵分解隐私放大的新无采样界，基于Rényi散度与条件组合，提供可高效计算的动态规划实现；对带状与非带状矩阵均适用；在ε小的情形更具优势，并通过数值对比验证有效性，相较基于蒙特卡罗的采样方法具更强的严格性。


<details>
  <summary>Details</summary>
Motivation: 在随机分配下的矩阵机制隐私放大存在对采样的高度依赖，现有工作多采用采样基的蒙特卡罗方法，结果要么只在高概率下成立，要么需要机制随机弃用，且对(ε,δ)-DP的样本量与δ成反比。需一种无采样、可计算且对广义矩阵适用的放大界。

Method: 提出两条无采样隐私放大路径：一是基于Rényi散度的放大界，结合动态规划高效计算；二是基于条件组合的界，在ε较小时提供更强的隐私保证，且对小ε下的近似不过度。将该框架应用于任意带状和非带状矩阵的矩阵机制。

Result: 给出可计算的放大边界，并在数值上显示对广泛矩阵机制的有效性，且在小ε场景下具有更强隐私保护。

Conclusion: 本工作提供针对随机分配下矩阵机制的无采样隐私放大界，理论与数值均支持其对带状与非带状矩阵的适用性，并在小ε情形优于传统的基于 Rényi 的放大和采样方法，具备实用性和理论价值。

Abstract: We study privacy amplification for differentially private model training with matrix factorization under random allocation (also known as the balls-in-bins model). Recent work by Choquette-Choo et al. (2025) proposes a sampling-based Monte Carlo approach to compute amplification parameters in this setting. However, their guarantees either only hold with some high probability or require random abstention by the mechanism. Furthermore, the required number of samples for ensuring $(ε,δ)$-DP is inversely proportional to $δ$. In contrast, we develop sampling-free bounds based on Rényi divergence and conditional composition. The former is facilitated by a dynamic programming formulation to efficiently compute the bounds. The latter complements it by offering stronger privacy guarantees for small $ε$, where Rényi divergence bounds inherently lead to an over-approximation. Our framework applies to arbitrary banded and non-banded matrices. Through numerical comparisons, we demonstrate the efficacy of our approach across a broad range of matrix mechanisms used in research and practice.

</details>


### [221] [Seg-MoE: Multi-Resolution Segment-wise Mixture-of-Experts for Time Series Forecasting Transformers](https://arxiv.org/abs/2601.21641)
*Evandro S. Ortigossa,Eran Segal*

Main category: cs.LG

TL;DR: Seg-MoE通过在时间序列中对连续时间步段进行路由与处理，而非逐个token路由，提升了时序预测的精度与可扩展性，在时间序列Transformer中实现并在多项基准数据集上达到近乎最优的效果。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的时间序列 forecasting 在规模扩展与长程时序动态建模上的瓶颈；现有MoE多采用token级路由，未充分利用时间序列的局部性与连续性。

Method: 提出稀疏MoE结构Seg-MoE，按连续时间步段进行路由与计算，使每个专家能直接建模段内交互；将Seg-MoE层嵌入时间序列Transformer；在多变量长期预测基准上评估，并通过消融研究验证分段路由是性能提升的关键因素。

Result: Seg-MoE在几乎所有预测 horizon 上持续达到或超过最先进的综合方法，优于密集Transformer和先前的token级MoE模型；分段路由显著提升效能，消融实验确认该设计为关键因素。

Conclusion: 将MoE的路由粒度与时间序列的固有结构对齐，提供强有力的条件稀疏性偏置，开启序列数据建模中新路径。

Abstract: Transformer-based models have recently made significant advances in accurate time-series forecasting, but even these architectures struggle to scale efficiently while capturing long-term temporal dynamics. Mixture-of-Experts (MoE) layers are a proven solution to scaling problems in natural language processing. However, existing MoE approaches for time-series forecasting rely on token-wise routing mechanisms, which may fail to exploit the natural locality and continuity of temporal data. In this work, we introduce Seg-MoE, a sparse MoE design that routes and processes contiguous time-step segments rather than making independent expert decisions. Token segments allow each expert to model intra-segment interactions directly, naturally aligning with inherent temporal patterns. We integrate Seg-MoE layers into a time-series Transformer and evaluate it on multiple multivariate long-term forecasting benchmarks. Seg-MoE consistently achieves state-of-the-art forecasting accuracy across almost all prediction horizons, outperforming both dense Transformers and prior token-wise MoE models. Comprehensive ablation studies confirm that segment-level routing is the key factor driving these gains. Our results show that aligning the MoE routing granularity with the inherent structure of time series provides a powerful, yet previously underexplored, inductive bias, opening new avenues for conditionally sparse architectures in sequential data modeling.

</details>


### [222] [Gauge-invariant representation holonomy](https://arxiv.org/abs/2601.21653)
*Vasileios Sevetlidis,George Pavlidis*

Main category: cs.LG

TL;DR: 提出 representation holonomy，用曲率/平行运输的概念衡量输入路径上的几何依赖性，克服点对点度量的局限，能区分在 CKA 下相似但对鲁棒性和扰动敏感的模型，且随训练而动态变化。


<details>
  <summary>Details</summary>
Motivation: 现有的相似性度量（如 CKA、SVCCA）聚焦于激活集的点对点重叠，忽略了特征在输入路径上的几何演化和曲率，导致对鲁棒性和对抗性敏感性的预测能力不足。需要一个对路径依赖、基于几何的诊断工具，用以揭示隐藏的曲率和表示结构。

Method: 定义一个 gauge 不变的统计量 representation holonomy，用以量化在输入空间沿一个小环进行平行运输时特征的累积扭曲。通过全局 whitening 固定 gauge、利用共享子空间对齐局部邻域并使用只旋转的 Procrustes 进行配准，最后将结果投影回完整特征空间。理论上证明对正交变换（以及 Whitening 后的仿射变换）不变，给出线性仿射层的零点，以及在小半径极限时 holonomy 为零的结论。

Result: 实验上，holonomy 随着环半径增大而增大；它能够将看似在 CKA 下相似的模型区分开来；与对抗鲁棒性、腐蚀鲁棒性等指标相关，且可追踪训练过程中的特征形成与稳定性。

Conclusion: representation holonomy 提供一种实用且可扩展的诊断工具，用以超越点对点相似度，探究学习表示的几何结构与路径依赖性。

Abstract: Deep networks learn internal representations whose geometry--how features bend, rotate, and evolve--affects both generalization and robustness. Existing similarity measures such as CKA or SVCCA capture pointwise overlap between activation sets, but miss how representations change along input paths. Two models may appear nearly identical under these metrics yet respond very differently to perturbations or adversarial stress. We introduce representation holonomy, a gauge-invariant statistic that measures this path dependence. Conceptually, holonomy quantifies the "twist" accumulated when features are parallel-transported around a small loop in input space: flat representations yield zero holonomy, while nonzero values reveal hidden curvature. Our estimator fixes gauge through global whitening, aligns neighborhoods using shared subspaces and rotation-only Procrustes, and embeds the result back to the full feature space. We prove invariance to orthogonal (and affine, post-whitening) transformations, establish a linear null for affine layers, and show that holonomy vanishes at small radii. Empirically, holonomy increases with loop radius, separates models that appear similar under CKA, and correlates with adversarial and corruption robustness. It also tracks training dynamics as features form and stabilize. Together, these results position representation holonomy as a practical and scalable diagnostic for probing the geometric structure of learned representations beyond pointwise similarity.

</details>


### [223] [TabClustPFN: A Prior-Fitted Network for Tabular Data Clustering](https://arxiv.org/abs/2601.21656)
*Tianqi Zhao,Guanyang Wang,Yan Shuo Tan,Qiong Zhang*

Main category: cs.LG

TL;DR: TabClustPFN is a prior-fitted network for tabular data clustering that amortizes Bayesian inference over both cluster assignments and the number of clusters, enabling single-pass clustering of unseen datasets with heterogeneous features without retraining.


<details>
  <summary>Details</summary>
Motivation: Clustering tabular data is difficult due to heterogeneous feature types, diverse data-generating mechanisms, and the lack of transferable inductive biases across datasets. PFNs have shown strong generalization in supervised tabular learning, but extending them to clustering is nontrivial due to permutation-invariant outputs and unknown cluster counts.

Method: Introduce TabClustPFN, a PFN for tabular clustering that performs amortized Bayesian inference over cluster assignments and cluster cardinality. It is pretrained on synthetic datasets drawn from a flexible clustering prior and clusters unseen datasets in a single forward pass, without dataset-specific retraining or hyperparameter tuning. It handles heterogeneous numerical and categorical features and adapts to various clustering structures.

Result: TabClustPFN outperforms classical, deep, and amortized clustering baselines on synthetic data and curated real-world tabular benchmarks, with strong robustness in exploratory settings and no need for dataset-specific retraining.

Conclusion: TabClustPFN enables effective, out-of-the-box clustering for heterogeneous tabular data, adapting to a wide range of clustering structures. Code is available at the provided GitHub link.

Abstract: Clustering tabular data is a fundamental yet challenging problem due to heterogeneous feature types, diverse data-generating mechanisms, and the absence of transferable inductive biases across datasets. Prior-fitted networks (PFNs) have recently demonstrated strong generalization in supervised tabular learning by amortizing Bayesian inference under a broad synthetic prior. Extending this paradigm to clustering is nontrivial: clustering is unsupervised, admits a combinatorial and permutation-invariant output space, and requires inferring the number of clusters. We introduce TabClustPFN, a prior-fitted network for tabular data clustering that performs amortized Bayesian inference over both cluster assignments and cluster cardinality. Pretrained on synthetic datasets drawn from a flexible clustering prior, TabClustPFN clusters unseen datasets in a single forward pass, without dataset-specific retraining or hyperparameter tuning. The model naturally handles heterogeneous numerical and categorical features and adapts to a wide range of clustering structures. Experiments on synthetic data and curated real-world tabular benchmarks show that TabClustPFN outperforms classical, deep, and amortized clustering baselines, while exhibiting strong robustness in out-of-the-box exploratory settings. Code is available at https://github.com/Tianqi-Zhao/TabClustPFN.

</details>


### [224] [SENDAI: A Hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework](https://arxiv.org/abs/2601.21664)
*Xingyue Zhang,Yuxuan Bao,Mars Liyao Gao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: SENDAI 是一种分层稀疏观测数据同化框架，通过将仿真先验与学习的偏差修正相结合，从极少量传感器观测中重建全局空间场；在六个全球站点的 MODIS 植被指数重建上显著优于基线方法，且对后续推断友好。


<details>
  <summary>Details</summary>
Motivation: 解决数据充足的训练与观测稀缺的部署之间的差距，特别是在目标域存在分布迁移、异质结构和多尺度动力学而训练数据缺失时，需在极端稀疏观测条件下实现高保真重建。

Method: 提出一个分层稀疏观测数据同化框架 SENDAI，通过整合由仿真得到的先验信息与学习得到的偏差纠正来重建全空间状态；以超稀疏传感观测为输入，结合物理约束与数据驱动修正进行多尺度重建。

Result: 在全球六个站点的 MODIS 植被指数场上，SENDAI 持续优于需要更密集观测的基线方法；与传统基线相比，峰值 SSIM 提升达 185%，较近来基于高频观测的方法提高约 36%；在边界清晰和亚季节性动态场景中收益尤为显著；重建在结构与光谱可分离性方面更好，便于下游对间接观测变量的推断。

Conclusion: 该方法提供一个轻量且可实际部署的稀疏观测重建框架，适用于物理驱动的推断、资源受限部署以及实时监测与控制场景，缓解域迁移带来的挑战并提升关键诊断结构的保留。

Abstract: Bridging the gap between data-rich training regimes and observation-sparse deployment conditions remains a central challenge in spatiotemporal field reconstruction, particularly when target domains exhibit distributional shifts, heterogeneous structure, and multi-scale dynamics absent from available training data. We present SENDAI, a hierarchical Sparse-measurement, EfficieNt Data AssImilation Framework that reconstructs full spatial states from hyper sparse sensor observations by combining simulation-derived priors with learned discrepancy corrections. We demonstrate the performance on satellite remote sensing, reconstructing MODIS (Moderate Resolution Imaging Spectroradiometer) derived vegetation index fields across six globally distributed sites. Using seasonal periods as a proxy for domain shift, the framework consistently outperforms established baselines that require substantially denser observations -- SENDAI achieves a maximum SSIM improvement of 185% over traditional baselines and a 36% improvement over recent high-frequency-based methods. These gains are particularly pronounced for landscapes with sharp boundaries and sub-seasonal dynamics; more importantly, the framework effectively preserves diagnostically relevant structures -- such as field topologies, land cover discontinuities, and spatial gradients. By yielding corrections that are more structurally and spectrally separable, the reconstructed fields are better suited for downstream inference of indirectly observed variables. The results therefore highlight a lightweight and operationally viable framework for sparse-measurement reconstruction that is applicable to physically grounded inference, resource-limited deployment, and real-time monitor and control.

</details>


### [225] [Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling](https://arxiv.org/abs/2601.21669)
*Abhijeet Sinha,Sundari Elango,Dianbo Liu*

Main category: cs.LG

TL;DR: 提出一种最小改动：对期望目标引入逆概率缩放，避免结果层级模式坍塌，并在GRPO框架下实现IPS-GRPO，提升多模态策略的稳定性与质量。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，期望返回目标易导致多峰结果坍塌，原因不仅是探索不足或正则化弱，而是目标函数的结构性偏差，导致不同结果的概率放大不均，进而产生快速分化。

Method: 数学分析：两结果的对数概率比随奖励差线性演化，导致指数级分化；提出逆概率缩放（IPS）作为对学习信号的最小修正，消除对结果频次的放大效应；将IPS应用为GRPO的drop-in修改，形成IPS-GRPO，无需额外模型或架构改变。

Result: 在推理任务和分子生成任务中，IPS-GRPO减小结果层级模式坍塌，且性能与基线相比持平甚至优于基线。

Conclusion: 纠正目标函数比增加探索更有效地实现多模态策略优化；IPS-GRPO提供一种通用、无额外开销的改进路径，未来可在更广泛领域验证与扩展。

Abstract: Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.

</details>


### [226] [LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics](https://arxiv.org/abs/2601.21681)
*Qisong Xiao,Xinhai Chen,Qinglin Wang,Xiaowei Guo,Binglin Wang,Weifeng Chen,Zhichao Wang,Yunfei Liu,Rui Xia,Hang Zou,Gencheng Liu,Shuai Li,Jie Liu*

Main category: cs.LG

TL;DR: LLM4Fluid leverages Large Language Models as generalizable temporal solvers for fluid dynamics by encoding high-dimensional flows into a physics-informed latent space and using modality-aligned prompts for autoregressive prediction, enabling zero-shot and in-context learning without retraining and achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing deep-learning models for spatio-temporal fluid dynamics struggle to generalize to unseen flow conditions and often require retraining for new scenarios. The work aims to achieve robust generalization by combining physics-informed reduced-order modeling with LLM-based temporal prediction and a modality-alignment mechanism.

Method: 1) Reduced-order modeling enhanced with physics-informed disentanglement to compress flow fields into a latent space while mitigating spatial feature entanglement; 2) A pretrained LLM autoregressively predicts temporal dynamics using time-series prompts; 3) A dedicated modality-alignment strategy bridges the gap between prompts and physical sequences to stabilize long-term predictions.

Result: Empirical results across diverse flow scenarios show LLM4Fluid acts as a robust, generalizable neural solver without retraining, delivering state-of-the-art accuracy and exhibiting strong zero-shot and in-context learning capabilities; code and datasets are publicly available.

Conclusion: LLM4Fluid demonstrates that LLM-based temporal solvers, when combined with physics-informed ROM and modality alignment, can serve as generalizable neural solvers for fluid dynamics, enabling accurate long-horizon predictions without retraining and enabling zero-shot/in-context learning.

Abstract: Deep learning has emerged as a promising paradigm for spatio-temporal modeling of fluid dynamics. However, existing approaches often suffer from limited generalization to unseen flow conditions and typically require retraining when applied to new scenarios. In this paper, we present LLM4Fluid, a spatio-temporal prediction framework that leverages Large Language Models (LLMs) as generalizable neural solvers for fluid dynamics. The framework first compresses high-dimensional flow fields into a compact latent space via reduced-order modeling enhanced with a physics-informed disentanglement mechanism, effectively mitigating spatial feature entanglement while preserving essential flow structures. A pretrained LLM then serves as a temporal processor, autoregressively predicting the dynamics of physical sequences with time series prompts. To bridge the modality gap between prompts and physical sequences, which can otherwise degrade prediction accuracy, we propose a dedicated modality alignment strategy that resolves representational mismatch and stabilizes long-term prediction. Extensive experiments across diverse flow scenarios demonstrate that LLM4Fluid functions as a robust and generalizable neural solver without retraining, achieving state-of-the-art accuracy while exhibiting powerful zero-shot and in-context learning capabilities. Code and datasets are publicly available at https://github.com/qisongxiao/LLM4Fluid.

</details>


### [227] [XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision](https://arxiv.org/abs/2601.21688)
*Alexandre Myara,Nicolas Bourriez,Thomas Boyer,Thomas Lemercier,Ihab Bendidi,Auguste Genovesio*

Main category: cs.LG

TL;DR: XFactors提出一种弱监督VAE框架，通过将表征分解为残差子空间S和面向K个因子的子空间Ti，并对每个因子使用InfoNCE对比学习实现分离和可控性；在KL正则下对S及聚合的因子子空间进行高斯化，避免对非目标因子的额外监督和对抗训练。实验证明在多数据集上达到最先进的解耦分数，支持通过潜变量置换实现因子交换，且随潜变量容量增加而正确扩展，CelebA等真实数据集上表现良好，代码公开。


<details>
  <summary>Details</summary>
Motivation: 纯无监督方法在真实数据上难以从统计上显式因子中恢复语义因素；有监督方法则对属性集合规模和训练稳定性有挑战，易受对抗性目标或辅助分类器的影响。需要一种弱监督、可扩展且具有显式可控性的解耦框架。

Method: 在Disentangled Information Bottleneck视角下，将表示分解为残差子空间S和面向各因子的子空间Ti；对每个目标因子在其分配的Ti中编码，通过对比监督（InfoNCE）将具有相同因子值的潜变量拉近、不同值的拉远；同时对S和聚合后的因子子空间施加高斯结构的KL正则，避免对未目标因子的额外监督和对抗训练。

Result: 在多数据集上，在固定超参条件下达到最先进的解耦分数，并在相应子空间实现稳定一致的因子对齐，支持通过潜变量替换实现因子交换；对潜维数的扩展具有良好扩展性；在CelebA等真实数据集上进行评估；代码公开。

Conclusion: 提出XFactors作为一种高效的弱监督解耦学习框架，提供对目标因子的显式控制、良好的可扩展性和对非目标因子的无额外监督约束，避免对抗训练与分类器，具有实际可用性和扩展性。

Abstract: Disentangled representation learning aims to map independent factors of variation to independent representation components. On one hand, purely unsupervised approaches have proven successful on fully disentangled synthetic data, but fail to recover semantic factors from real data without strong inductive biases. On the other hand, supervised approaches are unstable and hard to scale to large attribute sets because they rely on adversarial objectives or auxiliary classifiers.
  We introduce \textsc{XFactors}, a weakly-supervised VAE framework that disentangles and provides explicit control over a chosen set of factors. Building on the Disentangled Information Bottleneck perspective, we decompose the representation into a residual subspace $\mathcal{S}$ and factor-specific subspaces $\mathcal{T}_1,\ldots,\mathcal{T}_K$ and a residual subspace $\mathcal{S}$. Each target factor is encoded in its assigned $\mathcal{T}_i$ through contrastive supervision: an InfoNCE loss pulls together latents sharing the same factor value and pushes apart mismatched pairs. In parallel, KL regularization imposes a Gaussian structure on both $\mathcal{S}$ and the aggregated factor subspaces, organizing the geometry without additional supervision for non-targeted factors and avoiding adversarial training and classifiers.
  Across multiple datasets, with constant hyperparameters, \textsc{XFactors} achieves state-of-the-art disentanglement scores and yields consistent qualitative factor alignment in the corresponding subspaces, enabling controlled factor swapping via latent replacement. We further demonstrate that our method scales correctly with increasing latent capacity and evaluate it on the real-world dataset CelebA. Our code is available at \href{https://github.com/ICML26-anon/XFactors}{github.com/ICML26-anon/XFactors}.

</details>


### [228] [Understanding Model Merging: A Unified Generalization Framework for Heterogeneous Experts](https://arxiv.org/abs/2601.21690)
*Qinglun Li,Anke Tang,Miao Zhang,Mengzhu Wang,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: 提出在异构超参数环境下使用 L2-稳定性理论分析并统一解释模型合并的泛化性能，给出可操作的微调策略和专家选择建议；通过大量 ResNet/Vit 实验验证理论预测。


<details>
  <summary>Details</summary>
Motivation: 当前的模型合并缺乏统一理论来解释在不同微调超参数（如学习率、批量大小）下的泛化效果，且开源微调模型缺乏透明度，难以预测合并表现。

Method: 基于 L2-稳定性理论，分析合并后的模型 x_avg 在异构超参数下的泛化性，推导出一个统一框架，解释现有合并算法在目标 bound 的不同项上的优化路径，并给出面向实践的微调策略，以构建更适合合并的专家模型。

Result: 提出一个统一的理论框架并给出可操作的建议，在 ResNet 与 ViT 家族的 20/8 个视觉分类任务上进行大规模实验，包含数千个微调模型，验证超参数对 x_avg 泛化的影响，与理论结果的一致性。

Conclusion: 理论上确立了统一且可操作的分析框架，揭示了如何通过有针对性的微调来提升合并友好型模型的泛化性，为预训练–微调管线中的模型合并提供明确的实践指南。

Abstract: Model merging efficiently aggregates capabilities from multiple fine-tuned models into a single one, operating purely in parameter space without original data or expensive re-computation. Despite empirical successes, a unified theory for its effectiveness under heterogeneous finetuning hyperparameters (e.g., varying learning rates, batch sizes) remains missing. Moreover, the lack of hyperparameter transparency in open-source fine-tuned models makes it difficult to predict merged-model performance, leaving practitioners without guidance on how to fine-tune merge-friendly experts. To address those two challenges, we employ $L_2$-Stability theory under heterogeneous hyperparameter environments to analyze the generalization of the merged model $\boldsymbol{x}_{avg}$. This pioneering analysis yields two key contributions: (i) \textit{A unified theoretical framework} is provided to explain existing merging algorithms, revealing how they optimize specific terms in our bound, thus offering a strong theoretical foundation for empirical observations. (ii) \textit{Actionable recommendations} are proposed for practitioners to strategically fine-tune expert models, enabling the construction of merge-friendly models within the pretraining-to-finetuning pipeline. Extensive experiments on the ResNet/Vit family across 20/8 visual classification tasks, involving thousands of finetuning models, robustly confirm the impact of different hyperparameters on the generalization of $\boldsymbol{x}_{avg}$ predicted by our theoretical results.

</details>


### [229] [Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics](https://arxiv.org/abs/2601.21698)
*Mohamed Elgaar,Hadi Amiri*

Main category: cs.LG

TL;DR: Curriculum learning stabilizes within-phase optimization across model scales, yielding modest gains that are larger for smaller models and diminish with scale; it does not create new learning phases but reorders data exposure within existing phases.


<details>
  <summary>Details</summary>
Motivation: 探究 Curriculum Learning 是否改变学习轨迹还是仅改变数据暴露顺序，并在不同模型规模下评估其影响；建立基于梯度方差的优化稳定性分析框架。

Method: 在 Pythia 系列模型（14M–410M 参数）上进行 300B token 的预训练，比较三种语言学驱动的 Curriculum（年龄获得、词汇频度、动词变异 VV）与随机排序；在 1B 参数规模下比较 Random 与 VV；评估梯度噪声、输出头谱饱和、最终准确率，并观察潜在阶段的共性；给出基于梯度方差控制的理想化分析。

Result: 跨排序呈现共享的潜在学习阶段序列；Curricula 主要改变同一阶段内的数据暴露。小模型（≤160M）中，Random 排序导致更高的梯度噪声和更强的后期输出头谱饱和，最终精度较低；Curricula 在等 compute 条件下减小了这些效应。大模型时，饱和差异减小，Curricula 的增益变小。给出一个基于梯度方差控制的理论分析，链接难度进度与优化稳定性。

Conclusion: 结论是 Curricula 通过稳定同阶段内的优化来提升性能，而非创造新阶段；其收益受模型规模影响，在小模型更明显，在大模型逐渐减弱，需要结合实际计算与数据难度安排进行权衡。

Abstract: Curriculum learning changes the order of pre-training data, but it remains unclear whether it changes the learning trajectory or mainly reorders exposure over a fixed trajectory. We train Pythia models (14M-410M parameters) for 300B tokens under three linguistically motivated curricula-Age-of-Acquisition, word frequency, and Verb Variation (VV)-and compare each against Random ordering; at 1B parameters we compare Random and VV. Across orderings, training follows a shared sequence of latent phases, while curricula mainly change within-phase data exposure. In smaller models (up to 160M parameters), Random ordering exhibits higher gradient noise and stronger late-training output-head spectral saturation, alongside lower final accuracy; curricula reduce both effects at matched compute. At larger scales, saturation differences are smaller and curriculum gains shrink. We formalize the link between difficulty pacing and optimization stability in an idealized analysis based on gradient-variance control, and our results point to a practical takeaway: curricula help by stabilizing within-phase optimization rather than by creating new phases.

</details>


### [230] [SmartMeterFM: Unifying Smart Meter Data Generative Tasks Using Flow Matching Models](https://arxiv.org/abs/2601.21706)
*Nan Lin,Yanbo Wang,Jacco Heres,Peter Palensky,Pedro P. Vergara*

Main category: cs.LG

TL;DR: 提出一种基于流匹配的条件生成模型，将智能表计数据生成任务统一为单一模型，用于插补、超分辨、缺失数据等。


<details>
  <summary>Details</summary>
Motivation: 解决隐私与数据不可用、传感/传输故障、分辨率不足等问题，同时避免为每个任务设计和训练独立模型的低效问题。

Method: 训练一个条件生成的流匹配模型，输入不同的观测形式（部分数据、低分辨率、缺失等）作为条件，生成高维时间序列数据。将任务视为不同的部分观测，将其注入生成过程，统一完成插补、超分辨等任务，且无需重新训练。目标数据为月度智能表计数据，粒度为15分钟。

Result: 模型生成的数据与观测一致且具有现实性，在插补、超分辨及与专门基线相比具有更好的性能，优于基线的插值等方法。

Conclusion: 基于流匹配的条件生成框架可统一多种智能表计数据生成任务，降低模型冗余与训练成本，在隐私保护和低分辨率场景下具有实用性。

Abstract: Smart meter data is the foundation for planning and operating the distribution network. Unfortunately, such data are not always available due to privacy regulations. Meanwhile, the collected data may be corrupted due to sensor or transmission failure, or it may not have sufficient resolution for downstream tasks. A wide range of generative tasks is formulated to address these issues, including synthetic data generation, missing data imputation, and super-resolution. Despite the success of machine learning models on these tasks, dedicated models need to be designed and trained for each task, leading to redundancy and inefficiency. In this paper, by recognizing the powerful modeling capability of flow matching models, we propose a new approach to unify diverse smart meter data generative tasks with a single model trained for conditional generation. The proposed flow matching models are trained to generate challenging, high-dimensional time series data, specifically monthly smart meter data at a 15 min resolution. By viewing different generative tasks as distinct forms of partial data observations and injecting them into the generation process, we unify tasks such as imputation and super-resolution with a single model, eliminating the need for re-training. The data generated by our model not only are consistent with the given observations but also remain realistic, showing better performance against interpolation and other machine learning based baselines dedicated to the tasks.

</details>


### [231] [When does predictive inverse dynamics outperform behavior cloning?](https://arxiv.org/abs/2601.21718)
*Lukas Schäfer,Pallavi Choudhury,Abdelhak Lemkhenter,Chris Lovett,Somjit Nath,Luis França,Matheus Ribeiro Furtado de Mendonça,Alex Lamb,Riashat Islam,Siddhartha Sen,John Langford,Katja Hofmann,Sergio Valcarcel Macua*

Main category: cs.LG

TL;DR: PIDM introduces a bias-variance tradeoff: predicting a future state adds bias but conditioning the inverse dynamics model on this prediction reduces variance, enabling lower overall error and higher sample efficiency than BC under certain predictor bias conditions. Empirically validated in 2D navigation (BC needs up to 5x more demonstrations; ~3x on average) and a high-dimensional 3D game (BC needs >66% more samples).


<details>
  <summary>Details</summary>
Motivation: Explain why predictive inverse dynamics models (PIDM) outperform behavior cloning (BC) in offline imitation learning when expert data is scarce, by providing a theoretical bias-variance account and linking it to sample efficiency.

Method: Theoretical analysis of PIDM's bias-variance tradeoff. Derivation of conditions on the state predictor bias under which PIDM yields lower prediction error and higher sample efficiency than BC. Empirical validation across tasks: 2D navigation with limited demonstrations, and a complex 3D game with high-dimensional visual inputs and stochastic transitions.

Result: PIDM adds bias through predicting a future state but drastically reduces variance by conditioning the inverse dynamics model on this prediction. Under certain bias constraints, this yields lower overall prediction error and improved data efficiency compared to BC. Empirical results show substantial sample efficiency gains: BC requires up to 5× more demonstrations (3× on average) in 2D tasks, and BC requires over 66% more samples than PIDM in a high-dimensional 3D environment.

Conclusion: A theoretical bias-variance framework explains PIDM's empirical gains and shows that PIDM can surpass BC in data efficiency when state predictor bias lies within favorable bounds; benefits are amplified with additional data sources.

Abstract: Behavior cloning (BC) is a practical offline imitation learning method, but it often fails when expert demonstrations are limited. Recent works have introduced a class of architectures named predictive inverse dynamics models (PIDM) that combine a future state predictor with an inverse dynamics model (IDM). While PIDM often outperforms BC, the reasons behind its benefits remain unclear. In this paper, we provide a theoretical explanation: PIDM introduces a bias-variance tradeoff. While predicting the future state introduces bias, conditioning the IDM on the prediction can significantly reduce variance. We establish conditions on the state predictor bias for PIDM to achieve lower prediction error and higher sample efficiency than BC, with the gap widening when additional data sources are available. We validate the theoretical insights empirically in 2D navigation tasks, where BC requires up to five times (three times on average) more demonstrations than PIDM to reach comparable performance; and in a complex 3D environment in a modern video game with high-dimensional visual inputs and stochastic transitions, where BC requires over 66\% more samples than PIDM.

</details>


### [232] [LoRA and Privacy: When Random Projections Help (and When They Don't)](https://arxiv.org/abs/2601.21719)
*Yaxi Hu,Johanna Düngler,Bernhard Schölkopf,Amartya Sanyal*

Main category: cs.LG

TL;DR: 提出并分析基于 Wishart 投影的差分隐私机制。对向量查询，在无噪声下即可实现非渐近 DP；对矩阵查询，噪声为零时不 DP，存在高置信度成员推断攻击。加入噪声后，随机性与低秩投影引发隐私放大，在大/小秩情形均优于只加噪声的方案。还表明 LoRA 更新可视为此机制的一个实例，但并非天然私有，低秩微调在同等噪声下可能更私有。初步实验支持更紧的隐私估计可降低噪声并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 系统性评估 Wishart 投影在差分隐私中的性质，区分向量与矩阵查询的隐私表现，并探讨与实务中常见的模型微调策略（如 LoRA）的关系。

Method: 构建 S → M f(S)，其中 M ∼ W_d(1/r I_d, r)。对向量查询给出非渐近 DP 的无噪声保证；对矩阵查询给出无噪声下的否定结果及近似成员推断攻击。引入带噪声的变体，证明在大/小秩场景下存在隐私放大效应，并提供比仅添加噪声更强的隐私保证。将 LoRA 等更新视为该矩阵机制的实例，比较低秩与全秧微调在相同噪声水平下的隐私差异。用初步实验验证理论推导并探讨更紧的隐私估计对实际性能的影响。

Result: - 向量查询：Wishart 投影可在无外加噪声情况下实现非渐近的差分隐私。 
- 矩阵查询：无噪声下机制不具备 DP，存在高 AUC 的成员资格推断攻击。 
- 带噪声情形：通过随机性与低秩投影实现隐私放大，在大秩与小秩两种情形均改善隐私表现，比单纯添加噪声更具鲁棒性。 
- LoRA 关联：LoRA 更新可视为该矩阵机制的一个实现；但单靠其内置随机性并非天生私有，且低秩微调在相同噪声水平下可能比全参数微调更具隐私性。 
- 实验：初步结果表明更严格的隐私估计可降低所需噪声并提升实际准确性。

Conclusion: Wishart 投影作为差分隐私工具具有选择性优势：对向量查询可无噪声实现 DP，但对矩阵查询需借助噪声与低秩投影以实现隐私放大。此机制对实际模型微调策略（如 LoRA）具有重要启示，强调低秩结构和随机化在隐私保护中的潜力。未来工作应关注更广泛的矩阵查询场景、不同分布假设，以及对实际系统的可重复性评估。

Abstract: We introduce the (Wishart) projection mechanism, a randomized map of the form $S \mapsto M f(S)$ with $M \sim W_d(1/r I_d, r)$ and study its differential privacy properties. For vector-valued queries $f$, we prove non-asymptotic DP guarantees without any additive noise, showing that Wishart randomness alone can suffice. For matrix-valued queries, however, we establish a sharp negative result: in the noise-free setting, the mechanism is not DP, and we demonstrate its vulnerability by implementing a near perfect membership inference attack (AUC $> 0.99$). We then analyze a noisy variant and prove privacy amplification due to randomness and low rank projection, in both large- and small-rank regimes, yielding stronger privacy guarantees than additive noise alone. Finally, we show that LoRA-style updates are an instance of the matrix-valued mechanism, implying that LoRA is not inherently private despite its built-in randomness, but that low-rank fine-tuning can be more private than full fine-tuning at the same noise level. Preliminary experiments suggest that tighter accounting enables lower noise and improved accuracy in practice.

</details>


### [233] [Amortized Spectral Kernel Discovery via Prior-Data Fitted Network](https://arxiv.org/abs/2601.21731)
*Kaustubh Sharma,Srijan Tiwari,Ojasva Nema,Parikshit Pareek*

Main category: cs.LG

TL;DR: PFNs lack explicit priors; this work introduces an interpretability-driven decoupled-attention framework to map PFN latents to explicit spectral densities and stationary kernels using Bochner's theorem, enabling transparent priors and fast GP regression.


<details>
  <summary>Details</summary>
Motivation: Bridge the opacity of Prior-Data Fitted Networks (PFNs) by extracting explicit covariance structures from learned priors, facilitating downstream tasks such as surrogate-based optimization that rely on explicit kernels.

Method: Perform mechanistic analysis to identify attention latent outputs as the key intermediary; design decoders that convert PFN latents into spectral density estimates and corresponding stationary kernels via Bochner's theorem; analyze single-realization and multi-realization regimes; prove identifiability limits and consistency with multiple samples; empirically validate decoders on complex spectral mixtures.

Result: Decoders recover multi-peak spectral mixtures and produce explicit kernels enabling Gaussian process regression with accuracy comparable to PFNs and baselines, while achieving only a single forward pass and substantial speedups over optimization-based methods.

Conclusion: An interpretability-driven pipeline enables transparent, explicit spectral priors from PFNs, enabling efficient amortized inference and GP-based surrogates without sacrificing accuracy.

Abstract: Prior-Data Fitted Networks (PFNs) enable efficient amortized inference but lack transparent access to their learned priors and kernels. This opacity hinders their use in downstream tasks, such as surrogate-based optimization, that require explicit covariance models. We introduce an interpretability-driven framework for amortized spectral discovery from pre-trained PFNs with decoupled attention. We perform a mechanistic analysis on a trained PFN that identifies attention latent output as the key intermediary, linking observed function data to spectral structure. Building on this insight, we propose decoder architectures that map PFN latents to explicit spectral density estimates and corresponding stationary kernels via Bochner's theorem. We study this pipeline in both single-realization and multi-realization regimes, contextualizing theoretical limits on spectral identifiability and proving consistency when multiple function samples are available. Empirically, the proposed decoders recover complex multi-peak spectral mixtures and produce explicit kernels that support Gaussian process regression with accuracy comparable to PFNs and optimization-based baselines, while requiring only a single forward pass. This yields orders-of-magnitude reductions in inference time compared to optimization-based baselines.

</details>


### [234] [Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators](https://arxiv.org/abs/2601.21737)
*Rebecca Pelke,Joel Klein,Jose Cubero-Cascante,Nils Bosbach,Jan Moritz Joseph,Rainer Leupers*

Main category: cs.LG

TL;DR: 提出一个面向CIM的混合精度训练与编译框架，利用强化学习在极大量化搜索空间中自动找到适合的低于8位量化配置，以提高跨条MVM的效率并控制精度损失。


<details>
  <summary>Details</summary>
Motivation: CIM架构在crossbar上直接进行矩阵-向量乘法，输入位宽和存储单元容量受限导致效率瓶颈；多数CIM编译器不支持低于8位的量化，难以实现高吞吐的MVM，亟需降低位宽并提高映射效率。

Method: 提出基于强化学习的策略搜索，用于在CIM硬件约束下寻找混合精度量化参数，并将量化配置融入训练与编译流程，以在延迟与精度之间取得折中。

Result: 在最佳情形下，相比现有SOTA方案实现约2.48×加速，且精度损失仅0.086%。

Conclusion: 证明基于强化学习的混合精度量化框架能够在CIM硬件上有效降低位宽并提升性能，同时对模型精度的影响可控，具有实际应用潜力。

Abstract: Computing-in-Memory (CIM) accelerators are a promising solution for accelerating Machine Learning (ML) workloads, as they perform Matrix-Vector Multiplications (MVMs) on crossbar arrays directly in memory. Although the bit widths of the crossbar inputs and cells are very limited, most CIM compilers do not support quantization below 8 bit. As a result, a single MVM requires many compute cycles, and weights cannot be efficiently stored in a single crossbar cell. To address this problem, we propose a mixed-precision training and compilation framework for CIM architectures. The biggest challenge is the massive search space, that makes it difficult to find good quantization parameters. This is why we introduce a reinforcement learning-based strategy to find suitable quantization configurations that balance latency and accuracy. In the best case, our approach achieves up to a 2.48x speedup over existing state-of-the-art solutions, with an accuracy loss of only 0.086 %.

</details>


### [235] [Temporal Sepsis Modeling: a Fully Interpretable Relational Way](https://arxiv.org/abs/2601.21747)
*Vincent Lemaire,Nédra Meloulli,Pierre Jaquet*

Main category: cs.LG

TL;DR: A relational, propositionalisation-based framework for early sepsis prediction that emphasizes interpretability through a fourfold explanation (univariate, global, local, counterfactual) using a selective Naive Bayes classifier.


<details>
  <summary>Details</summary>
Motivation: Tackling the interpretability gap and sub-phenotype heterogeneity in sepsis prediction by shifting from black-box deep learning to a relational, explainable approach.

Method: Represent EMR time-series as relational data; apply propositionalisation with aggregation/selection functions to construct interpretable features (flattening the data); classify with a selective naive Bayesian classifier; provide fourfold interpretability.

Result: Experimental validation supports the approach's relevance and yields extreme interpretability, enabling univariate, global, local, and counterfactual explanations.

Conclusion: A relational propositionalisation framework offers an interpretable alternative for early sepsis prediction and potential sub-phenotype discovery, balancing accuracy with transparency.

Abstract: Sepsis remains one of the most complex and heterogeneous syndromes in intensive care, characterized by diverse physiological trajectories and variable responses to treatment. While deep learning models perform well in the early prediction of sepsis, they often lack interpretability and ignore latent patient sub-phenotypes. In this work, we propose a machine learning framework by opening up a new avenue for addressing this issue: a relational approach. Temporal data from electronic medical records (EMRs) are viewed as multivariate patient logs and represented in a relational data schema. Then, a propositionalisation technique (based on classic aggregation/selection functions from the field of relational data) is applied to construct interpretable features to "flatten" the data. Finally, the flattened data is classified using a selective naive Bayesian classifier. Experimental validation demonstrates the relevance of the suggested approach as well as its extreme interpretability. The interpretation is fourfold: univariate, global, local, and counterfactual.

</details>


### [236] [ECSEL: Explainable Classification via Signomial Equation Learning](https://arxiv.org/abs/2601.21789)
*Adia Lumadjeng,Ilker Birbil,Erman Acar*

Main category: cs.LG

TL;DR: ECSEL learns signomial-based, closed-form classifiers that are interpretable and competitive, with strong equation recovery and high efficiency; it also exposes dataset biases and supports counterfactual reasoning in real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Symbolic regression benchmarks often admit compact signomial structures. The work aims to combine interpretability with predictive performance by learning a structural, closed-form classifier that doubles as an explanation.

Method: Constructs formal expressions in signomial equations as the classifier; evaluates global feature behavior, decision boundaries, and local attributions; compares against state-of-the-art symbolic regression approaches and applies to benchmark datasets and real-world case studies (e-commerce, fraud detection).

Result: ECSEL recovers a larger fraction of target equations than competing methods while requiring substantially less computation; achieves classification accuracy competitive with established ML models while preserving interpretability; demonstrates desirable properties for feature analysis and attribution; reveals dataset biases and supports counterfactual reasoning in real-world tasks.

Conclusion: ECSEL provides an interpretable, efficient, and accurate classification framework with explicit explanations, bias exposure, and counterfactual capabilities, suitable for practical decision-support scenarios.

Abstract: We introduce ECSEL, an explainable classification method that learns formal expressions in the form of signomial equations, motivated by the observation that many symbolic regression benchmarks admit compact signomial structure. ECSEL directly constructs a structural, closed-form expression that serves as both a classifier and an explanation. On standard symbolic regression benchmarks, our method recovers a larger fraction of target equations than competing state-of-the-art approaches while requiring substantially less computation. Leveraging this efficiency, ECSEL achieves classification accuracy competitive with established machine learning models without sacrificing interpretability. Further, we show that ECSEL satisfies some desirable properties regarding global feature behavior, decision-boundary analysis, and local feature attributions. Experiments on benchmark datasets and two real-world case studies i.e., e-commerce and fraud detection, demonstrate that the learned equations expose dataset biases, support counterfactual reasoning, and yield actionable insights.

</details>


### [237] [Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models](https://arxiv.org/abs/2601.21794)
*Yejin Kim,Dongjun Hwang,Sungmin Cha,Junsuk Choe*

Main category: cs.LG

TL;DR: 提出 KVW（Knowledge Vector Weakening），一种训练无关的忘却方法，通过干预全模型、逐步削弱忘却集激活的知识向量的贡献来实现对大规模视觉-语言模型的忘却，显著提升与梯度基忘却和 LoRA 基方法相比的计算效率，同时在忘记-保留之间取得稳定权衡。


<details>
  <summary>Details</summary>
Motivation: 应对隐私泄露和有害内容生成等风险，现有基于梯度的忘却方法在大规模 LVLMs 上计算成本高；需要一种高效、训练无关的忘却方案以降低资源开销并保持模型性能。

Method: 提出 Knowledge Vector Weakening（KVW）：直接对全集模型进行干预，无需梯度计算。通过识别在忘记集合上输出生成过程中被激活的知识向量，逐步削弱它们的贡献，从而阻止模型利用 undesirable knowledge。

Result: 在 MLLMU 与 CLEAR 基准上，KVW 实现了稳定的忘记-保留权衡，并在计算效率方面显著优于基于梯度的忘却方法和 LoRA 相关方法。

Conclusion: KVW 为 LVLM 的高效训练无关忘却提供可行方案，降低了大规模模型的计算开销，具备对更多任务和场景的潜在扩展性。

Abstract: Large Vision-Language Models (LVLMs) are widely adopted for their strong multimodal capabilities, yet they raise serious concerns such as privacy leakage and harmful content generation. Machine unlearning has emerged as a promising solution for removing the influence of specific data from trained models. However, existing approaches largely rely on gradient-based optimization, incurring substantial computational costs for large-scale LVLMs. To address this limitation, we propose Knowledge Vector Weakening (KVW), a training-free unlearning method that directly intervenes in the full model without gradient computation. KVW identifies knowledge vectors that are activated during the model's output generation on the forget set and progressively weakens their contributions, thereby preventing the model from exploiting undesirable knowledge. Experiments on the MLLMU and CLEAR benchmarks demonstrate that KVW achieves a stable forget-retain trade-off while significantly improving computational efficiency over gradient-based and LoRA-based unlearning methods.

</details>


### [238] [Effective LoRA Adapter Routing using Task Representations](https://arxiv.org/abs/2601.21795)
*Akash Dhasade,Anne-Marie Kermarrec,Igor Pavlovic,Diana Petrescu,Rafael Pires,Mathis Randl,Martijn de Vos*

Main category: cs.LG

TL;DR: LORAUTER通过基于任务的嵌入路由来选择和组合LoRA适配器，提升大规模适配器池的路由效率与鲁棒性，在未见任务上实现SOTA，并对含噪声的1000+规模适配器具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有路由方法直接把查询映射到单个适配器，导致适配器数量增大时成本与复杂性迅速上升。提出以任务级别的语义表示进行路由，从验证集推导任务嵌入，无需对适配器训练数据，且路由复杂度与任务数量线性相关。

Method: 构建来自小型验证集的任务嵌入，将查询通过任务嵌入路由以选择并组合相应的LoRA适配器；无需额外的适配器训练数据；在多任务和大规模噪声适配器池场景下评估，与基线路由比较，展示效能提升。

Result: 实验显示LORAUTER显著优于基线路由，在存在任务对齐的适配器时达到接近Oracle的性能（约101.2%），在未见任务上达到SOTA，提升约+5.2点；对超过1500个适配器的噪声池也具鲁棒性。

Conclusion: 基于任务级别嵌入的路由策略在扩展性、泛化性方面优于基于适配器特征的路由，尤其适用于大规模LoRA池和未知任务，但依赖于高质量的任务嵌入与验证集覆盖。

Abstract: Low-rank adaptation (LoRA) enables parameter efficient specialization of large language models (LLMs) through modular adapters, resulting in rapidly growing public adapter pools spanning diverse tasks. Effectively using these adapters requires routing: selecting and composing the appropriate adapters for a query. We introduce LORAUTER, a novel routing framework that selects and composes LoRA adapters using task representations rather than adapter characteristics. Unlike existing approaches that map queries directly to adapters, LORAUTER routes queries via task embeddings derived from small validation sets and does not require adapter training data. By operating at the task level, LORAUTER achieves efficient routing that scales with the number of tasks rather than the number of adapters. Experiments across multiple tasks show that LORAUTER consistently outperforms baseline routing approaches, matching Oracle performance (101.2%) when task-aligned adapters exist and achieving state-of-the-art results on unseen tasks (+5.2 points). We further demonstrate the robustness of LORAUTER to very large, noisy adapter pools by scaling it to over 1500 adapters.

</details>


### [239] [Nonparametric LLM Evaluation from Preference Data](https://arxiv.org/abs/2601.21816)
*Dennis Frauen,Athiya Deviyani,Mihaela van der Schaar,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出 DMLEval：基于去偏机器学习的非参数框架，用 GARS 评价并排序大语言模型的偏好数据，兼容黑箱方法，具统计效率，并给出数据收集策略。


<details>
  <summary>Details</summary>
Motivation: 现有对比方法对偏好数据往往依赖参数假设，或在使用灵活机器学习方法时缺乏有效的不确定性量化，难以在现实场景下稳健地对比和排名大语言模型。

Method: 引入广义平均排序分数（GARS），将常用排序模型（如 Bradley-Terry、PageRank/Rank Centrality）推广到可处理复杂人类反馈（包括平局）的情形；通过去偏机器学习（DML）实现 GARS 的统计高效估计，允许纳入黑箱式估计器和 LLM 作为评估者，并给出在预算约束下的最优数据收集策略。

Result: 理论与实证均证实 DMLEval 的有效性：在合成与真实偏好数据集上能够提供高效、稳健的排序估计，且灵活地整合多种估计方法和评估者。

Conclusion: DMLEval 为研究者与实践者提供了一套先进且可扩展的框架，用以比较与排名大语言模型，具备统计效率、灵活性及对偏好数据收集的优化能力。

Abstract: Evaluating the performance of large language models (LLMs) from human preference data is crucial for obtaining LLM leaderboards. However, many existing approaches either rely on restrictive parametric assumptions or lack valid uncertainty quantification when flexible machine learning methods are used. In this paper, we propose a nonparametric statistical framework, DMLEval, for comparing and ranking LLMs from preference data using debiased machine learning (DML). For this, we introduce generalized average ranking scores (GARS), which generalize commonly used ranking models, including the Bradley-Terry model or PageRank/ Rank centrality, with complex human responses such as ties. DMLEval comes with the following advantages: (i) It produces statistically efficient estimates of GARS ranking scores. (ii) It naturally allows the incorporation of black-box machine learning methods for estimation. (iii) It can be combined with pre-trained LLM evaluators (e.g., using LLM-as-a-judge). (iv) It suggests optimal policies for collecting preference data under budget constraints. We demonstrate these advantages both theoretically and empirically using both synthetic and real-world preference datasets. In summary, our framework provides practitioners with powerful, state-of-the-art methods for comparing or ranking LLMs.

</details>


### [240] [DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training](https://arxiv.org/abs/2601.21824)
*Xinwei Qiang,Hongmin Chen,Shixuan Sun,Jingwen Leng,Xin Liu,Minyi Guo*

Main category: cs.LG

TL;DR: 提出 DASH（Deterministic Attention Scheduling for High-Throughput），将确定性注意力的反向传播视作 DAG 调度问题，提出 Descending Q-Tile Iteration 与 Shift Scheduling 两种策略，以降低关键路径长度并提升吞吐量，在 NVIDIA H800 上将确定性注意力反向传播的吞吐提升至基线的1.28×左右，且开源代码可获取。


<details>
  <summary>Details</summary>
Motivation: 在大模型训练中，为确保可重复性需保持确定性，但确定性注意力的反向传播会因梯度累积序列化和计算/梯度归约阶段的调度不足而导致显著的吞吐量损失。现有实现中，确定性后向的性能瓶颈来自于各阶段的硬件利用率不足和子优化的调度策略。

Method: 将确定性注意力的反向传播建模为有向无环图(DAG)的调度问题，推导出能最小化关键路径的调度。提出两条策略：1) Descending Q-Tile Iteration：对查询块的反向遍历以降低因果注意力中的流水线阻塞；2) Shift Scheduling：在所建 DAG 模型中的理论最优调度，减少全局与因果掩码下的流水线阻塞。

Result: 在 NVIDIA H800 上的实验证明，DASH 可将确定性注意力反向传播的吞吐提升至基线的最大 1.28 倍，显著缩小可重复性与吞吐之间的差距。论文同时给出开放源代码工作流（GitHub）。

Conclusion: 通过将确定性注意力反向传播视为 DAG 调度问题并提供两种互补的调度策略，DASH 能有效减小流水线阻塞、提升吞吐，推动可重复性强的 LLM 训练更加高效。

Abstract: Determinism is indispensable for reproducibility in large language model (LLM) training, yet it often exacts a steep performance cost. In widely used attention implementations such as FlashAttention-3, the deterministic backward pass can incur up to a 37.9% throughput reduction relative to its non-deterministic counterpart, primarily because gradient accumulation operations must be serialized to guarantee numerical consistency. This performance loss stems from suboptimal scheduling of compute and gradient-reduction phases, leading to significant hardware underutilization.
  To address this challenge, we formulate the backward pass of deterministic attention as a scheduling problem on a Directed Acyclic Graph (DAG) and derive schedules that minimize the critical path length. Building on this formulation, we present DASH (Deterministic Attention Scheduling for High-Throughput), which encapsulates two complementary scheduling strategies: (i) Descending Q-Tile Iteration, a reversed query-block traversal that shrinks pipeline stalls in causal attention, and (ii) Shift Scheduling, a theoretically optimal schedule within our DAG model that reduces pipeline stalls for both full and causal masks.
  Our empirical evaluations on NVIDIA H800 GPUs demonstrate that DASH narrows the performance gap of deterministic attention. The proposed strategies improve the throughput of the attention backward pass by up to 1.28$\times$ compared to the baseline, significantly advancing the efficiency of reproducible LLM training.
  Our code is open-sourced at https://github.com/SJTU-Liquid/deterministic-FA3.

</details>


### [241] [Goal-Driven Adaptive Sampling Strategies for Machine Learning Models Predicting Fields](https://arxiv.org/abs/2601.21832)
*Jigar Parekh,Philipp Bekemeyer*

Main category: cs.LG

TL;DR: 提出一种面向场预测的主动学习策略，结合用于标量参考值的高斯过程，兼顾减少模型本体的 epistemic 不确定性与标量-场预测之间的差异；在 NASA 公共研究模型上实现 uncertainty propagation，显著降低样本成本并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 在昂贵的黑箱仿真（如计算流体力学）背景下，需以尽可能少的样本获得可接受的场预测精度。现有主动学习多聚焦标量量纲，扩展到场预测仍缺乏普适性与模型无关性，因此需要一种对任意模型结构皆适用的主动学习策略。

Method: 提出一种混合方法：以高斯过程用于标量参考值的回归，同时设计策略以同时降低 epistemic 不确定性与缩小标量预测与场预测之间的差异；提出并比较若干变体，与仅基于标量的填充准则进行对比，应用于 NASA 公共研究模型的不确定性传播任务。

Result: 不同变体在将主动学习应用于场预测时，展现出与无主动学习基线相比显著降低的样本成本和保持/提升的预测精度；在不确定性传播任务中实现高水平的准确性。

Conclusion: 面向场预测的主动学习策略在理论和实践层面均可行且有效，且具模型无关性，能够降低样本复杂度并提升场预测的可靠性。

Abstract: Machine learning models are widely regarded as a way forward to tackle multi-query challenges that arise once expensive black-box simulations such as computational fluid dynamics are investigated. However, ensuring the desired level of accuracy for a certain task at minimal computational cost, e.g. as few black-box samples as possible, remains a challenges. Active learning strategies are used for scalar quantities to overcome this challenges and different so-called infill criteria exists and are commonly employed in several scenarios. Even though needed in various field an extension of active learning strategies towards field predictions is still lacking or limited to very specific scenarios and/or model types. In this paper we propose an active learning strategy for machine learning models that are capable if predicting field which is agnostic to the model architecture itself. For doing so, we combine a well-established Gaussian process model for a scalar reference value and simultaneously aim at reducing the epistemic model error and the difference between scalar and field predictions. Different specific forms of the above-mentioned approach are introduced and compared to each other as well as only scalar-valued based infill. Results are presented for the NASA common research model for an uncertainty propagation task showcasing high level of accuracy at significantly smaller cost compared to an approach without active learning.

</details>


### [242] [Constrained Meta Reinforcement Learning with Provable Test-Time Safety](https://arxiv.org/abs/2601.21845)
*Tingting Ni,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 提出一个在元强化学习中引入安全约束的策略修正框架，提供可证明的安全性与样本复杂度 guarantees，并给出与之匹配的下界，表明该复杂度是紧的。


<details>
  <summary>Details</summary>
Motivation: 现实应用（如机器人、医疗）对测试任务的安全性有严格要求，而元RL 虽提升样本效率，但在安全保障方面不足，因此需要在提升学习速度的同时确保安全。

Method: 在训练阶段学习的策略基础上进行修正（policy refinement），引入安全约束的优化目标，并给出理论分析，确保在测试任务上实现接近最优策略，同时推导出近似最优策略所需的样本复杂度，并给出与之匹配的下界。

Result: 提出的算法具备可证明的安全性和接近最优策略的样本复杂度保证；并给出一个匹配的下界，证明该复杂度为紧界。

Conclusion: 在确保安全前提下提升样本效率，理论结果显示样本复杂度具有紧下界，验证方法在现实场景中的可行性。

Abstract: Meta reinforcement learning (RL) allows agents to leverage experience across a distribution of tasks on which the agent can train at will, enabling faster learning of optimal policies on new test tasks. Despite its success in improving sample complexity on test tasks, many real-world applications, such as robotics and healthcare, impose safety constraints during testing. Constrained meta RL provides a promising framework for integrating safety into meta RL. An open question in constrained meta RL is how to ensure the safety of the policy on the real-world test task, while reducing the sample complexity and thus, enabling faster learning of optimal policies. To address this gap, we propose an algorithm that refines policies learned during training, with provable safety and sample complexity guarantees for learning a near optimal policy on the test tasks. We further derive a matching lower bound, showing that this sample complexity is tight.

</details>


### [243] [Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models](https://arxiv.org/abs/2601.21851)
*Sidney Bender,Marco Morik*

Main category: cs.LG

TL;DR: 提出 DiDAE，结合冻结的基础模型、解耦字典学习和扩散自编码器，实现多样且解耦的反事实生成，提升对捷径学习的鲁棒性，并在结合 CFKD 时达到 SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型具备强零-shot能力，但易受伪相关和 'Clever Hans' 式策略影响；现有缓解方法依赖不可用的分组标签或计算成本高的梯度对抗优化，需更高效的梯度自由的反事实生成。

Method: Visual Disentangled Diffusion Autoencoders (DiDAE) 将冻结的基础模型与解耦字典学习结合，通过在可解释的解耦方向上编辑嵌入再通过扩散自编码器解码，直接为基础模型生成对比的反事实；能为每个事实样本生成多样且解耦的反事实，速度远超生成单一的 entangled 反事实的基线。

Result: DiDAE 能生成多样且解耦的反事实，且显著比现有 baselines 更快；

Conclusion: 与 Counterfactual Knowledge Distillation 结合的 DiDAE-CFKD 在缓解捷径学习方面达到SOTA，并提升不平衡数据集上的下游表现。

Abstract: Foundation models, despite their robust zero-shot capabilities, remain vulnerable to spurious correlations and 'Clever Hans' strategies. Existing mitigation methods often rely on unavailable group labels or computationally expensive gradient-based adversarial optimization. To address these limitations, we propose Visual Disentangled Diffusion Autoencoders (DiDAE), a novel framework integrating frozen foundation models with disentangled dictionary learning for efficient, gradient-free counterfactual generation directly for the foundation model. DiDAE first edits foundation model embeddings in interpretable disentangled directions of the disentangled dictionary and then decodes them via a diffusion autoencoder. This allows the generation of multiple diverse, disentangled counterfactuals for each factual, much faster than existing baselines, which generate single entangled counterfactuals. When paired with Counterfactual Knowledge Distillation, DiDAE-CFKD achieves state-of-the-art performance in mitigating shortcut learning, improving downstream performance on unbalanced datasets.

</details>


### [244] [MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts](https://arxiv.org/abs/2601.21866)
*Evandro S. Ortigossa,Guy Lutsker,Eran Segal*

Main category: cs.LG

TL;DR: 提出 MoHETS：一个基于编码器的 Transformer，结合稀疏异质专家 MoHE 层，实现对多尺度时间序列的长时预测，显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实的多变量时间序列展现全局趋势、局部周期性与非平稳性等多尺度结构，单纯的同质 MLP 专家难以覆盖丰富的时序动态；需要异质化、可扩展的专家组合以提升长时预测性能与鲁棒性。

Method: 设计 MoHE 层：将时间片路由到少量专家；包含一个共享的深卷积专家用于序列级连续性，以及路由到的 Fourier 基专家用于片段级周期结构；通过跨注意力对协变量嵌入进行外源信息融入；使用轻量化卷积片段解码器替代线性投影头，实现参数高效并支持跨任意预测跨度的泛化。

Result: 在七个多变量基准和多种预测 horizon 上实现 state-of-the-art，平均 MSE 相比强基线下降约 12%。

Conclusion: 异质专家的分层融合与外源信息整合提升了长时预测的鲁棒性与泛化能力，同时保持参数效率，MoHETS 为建模复杂时序多尺度结构提供了有效框架。

Abstract: Real-world multivariate time series can exhibit intricate multi-scale structures, including global trends, local periodicities, and non-stationary regimes, which makes long-horizon forecasting challenging. Although sparse Mixture-of-Experts (MoE) approaches improve scalability and specialization, they typically rely on homogeneous MLP experts that poorly capture the diverse temporal dynamics of time series data. We address these limitations with MoHETS, an encoder-only Transformer that integrates sparse Mixture-of-Heterogeneous-Experts (MoHE) layers. MoHE routes temporal patches to a small subset of expert networks, combining a shared depthwise-convolution expert for sequence-level continuity with routed Fourier-based experts for patch-level periodic structures. MoHETS further improves robustness to non-stationary dynamics by incorporating exogenous information via cross-attention over covariate patch embeddings. Finally, we replace parameter-heavy linear projection heads with a lightweight convolutional patch decoder, improving parameter efficiency, reducing training instability, and allowing a single model to generalize across arbitrary forecast horizons. We validate across seven multivariate benchmarks and multiple horizons, with MoHETS consistently achieving state-of-the-art performance, reducing the average MSE by $12\%$ compared to strong recent baselines, demonstrating effective heterogeneous specialization for long-term forecasting.

</details>


### [245] [Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions](https://arxiv.org/abs/2601.21873)
*Jinhang Chai,Xuyuan Liu,Elynn Chen,Yujun Yan*

Main category: cs.LG

TL;DR: 在结构矩阵估计中研究随时间扩展的环境维度与内在表示的转移学习。提出锚定的交替投影估计器，将目标参数分解为嵌入的源分量、低维低秩创新以及稀疏修改，并给出严格的误差界、在增量小时获得改进的收敛率，应用于马尔可夫转移矩阵估计和结构协方差估计，获得 transfer gains。


<details>
  <summary>Details</summary>
Motivation: 在表示空域随时间增长的场景下进行转移学习，如何在保留源任务子空间的前提下估计目标任务的新低维结构与稀疏修改，同时量化源表示增长对估计误差的影响。

Method: 提出一个通用框架：目标参数分解为嵌入的源分量、低维低秩创新、稀疏编辑；提出锚定的交替投影估计器以保持已转移的子空间，同时估计低维创新与稀疏修改。

Result: 给出确定性误差界，区分目标噪声、表示增长与源估计误差；在表示增长较小的情况下，可获得严格提升的收敛/误差率。对两类典型问题给出应用：单轨迹下的马尔可夫转移矩阵估计的端到端理论保证（含相关噪声的依赖性），以及扩展维度下的结构协方差估计的附录分析并与实证验证转移收益。

Conclusion: 该框架具有普适性，能在表示扩展情形下提供可理论证明的转移收益，并通过锚定投影实现对源子空间的保留与对创新和稀疏修改的高效估计。

Abstract: Learning systems often expand their ambient features or latent representations over time, embedding earlier representations into larger spaces with limited new latent structure. We study transfer learning for structured matrix estimation under simultaneous growth of the ambient dimension and the intrinsic representation, where a well-estimated source task is embedded as a subspace of a higher-dimensional target task.
  We propose a general transfer framework in which the target parameter decomposes into an embedded source component, low-dimensional low-rank innovations, and sparse edits, and develop an anchored alternating projection estimator that preserves transferred subspaces while estimating only low-dimensional innovations and sparse modifications. We establish deterministic error bounds that separate target noise, representation growth, and source estimation error, yielding strictly improved rates when rank and sparsity increments are small.
  We demonstrate the generality of the framework by applying it to two canonical problems. For Markov transition matrix estimation from a single trajectory, we derive end-to-end theoretical guarantees under dependent noise. For structured covariance estimation under enlarged dimensions, we provide complementary theoretical analysis in the appendix and empirically validate consistent transfer gains.

</details>


### [246] [Managing Solution Stability in Decision-Focused Learning with Cost Regularization](https://arxiv.org/abs/2601.21883)
*Victor Spitzer,Francois Sanson*

Main category: cs.LG

TL;DR: 本研究聚焦决策导向学习在组合优化中的应用，揭示在学习阶段对扰动强度的波动会因解的稳定性而导致训练效果下降，提出对估计成本向量的正则化以提升鲁棒性和学习可靠性，并通过大量数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 将预测建模与组合优化紧密结合，使模型训练直接提升决策质量；现有通过扰动近似实现可微分的做法易受扰动强度波动影响，缺乏对解稳定性与训练鲁棒性的理论与实验支持。

Method: 分析扰动强度在学习过程中的波动对训练的影响，建立其与组合优化中解的稳定性的理论联系；提出对估计成本向量进行正则化以提高学习过程的鲁棒性；在多组数值实验中验证正则化的有效性。

Result: 正则化估计成本向量显著提升了学习过程的鲁棒性与可靠性，减小了扰动强度波动带来的负面影响；数值实验表明在决策质量与稳定性方面均有改进。

Conclusion: 通过对成本向量的正则化，能够缓解基于扰动的近似方法在决策导向学习中的不稳定性问题，从而提升组合优化问题的学习-决策联动性能。

Abstract: Decision-focused learning integrates predictive modeling and combinatorial optimization by training models to directly improve decision quality rather than prediction accuracy alone. Differentiating through combinatorial optimization problems represents a central challenge, and recent approaches tackle this difficulty by introducing perturbation-based approximations. In this work, we focus on estimating the objective function coefficients of a combinatorial optimization problem. Our study demonstrates that fluctuations in perturbation intensity occurring during the learning phase can lead to ineffective training, by establishing a theoretical link to the notion of solution stability in combinatorial optimization. We propose addressing this issue by introducing a regularization of the estimated cost vectors which improves the robustness and reliability of the learning process, as demonstrated by extensive numerical experiments.

</details>


### [247] [Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning](https://arxiv.org/abs/2601.21894)
*Lukas Twist,Shu Yang,Hanqi Yan,Jingzhi Gong,Di Wang,Helen Yannakoudakis,Jie M. Zhang*

Main category: cs.LG

TL;DR: 结构复杂性导向的数据微调对LLM推理能力的提升具有显著影响，且比仅仅增加数据量更依赖于代码的结构属性。


<details>
  <summary>Details</summary>
Motivation: 探究代码中的结构性特征（控制流、组合结构）如何影响模型在多步推理中的内化，与只看代码数量的训练信号区分开来。

Method: 构造以结构复杂性为变量的微调数据集，使用环形复杂度（cyclomatic complexity）与逻辑行数（LLOC）等指标，设置解决方案维度与任务维度两类复杂性变体；在多种开源权重LLM上进行推理基准测试。

Result: 代码对推理有帮助，但结构属性决定效果。在83%的实验中，限制在特定结构复杂性区间的微调数据比对结构多样化的代码更有效。

Conclusion: 强调数据层面而非单纯模型规模扩展来提升推理能力，给出在微调阶段优先选择合适结构复杂性的代码数据的建议。

Abstract: Large Language Models (LLMs) increasingly exhibit strong reasoning abilities, often attributed to their capacity to generate chain-of-thought-style intermediate reasoning. Recent work suggests that exposure to code can further enhance these skills, but existing studies largely treat code as a generic training signal, leaving open the question of which properties of code actually contribute to improved reasoning. To address this gap, we study the structural complexity of code, which captures control flow and compositional structure that may shape how models internalise multi-step reasoning during fine-tuning. We examine two complementary settings: solution-driven complexity, where complexity varies across multiple solutions to the same problem, and problem-driven complexity, where complexity reflects variation in the underlying tasks. Using cyclomatic complexity and logical lines of code to construct controlled fine-tuning datasets, we evaluate a range of open-weight LLMs on diverse reasoning benchmarks. Our findings show that although code can improve reasoning, structural properties strongly determine its usefulness. In 83% of experiments, restricting fine-tuning data to a specific structural complexity range outperforms training on structurally diverse code, pointing to a data-centric path for improving reasoning beyond scaling.

</details>


### [248] [A Low-Complexity Plug-and-Play Deep Learning Model for Generalizable Massive MIMO Precoding](https://arxiv.org/abs/2601.21897)
*Ali Hasanzadeh Karkan,Ahmed Ibrahim,Jean-François Frigon,François Leduc-Primeau*

Main category: cs.LG

TL;DR: PaPP 提出一种跨站点、跨发射功率且对信道估计误差鲁棒的 DL 框架，用于 FDP 与 HBF 的 mMIMO 下行预编码，显著降低计算能耗，并能通过少量本地未标注数据微调实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有 mMIMO 下行预编码在接近最优的 WMMSE 等算法上计算成本高，且对 SNR、信道估计质量敏感；现有 DL 方案普遍缺乏鲁棒性、需要在每个部署点重新训练，难以跨站部署。

Method: PaPP 采用教师-学生架构（高容量教师与紧凑学生）结合自监督损失，权衡教师模仿与归一化的和率；通过元学习实现领域泛化，并引入传输功率感知输入归一化以适配不同功率水平。框架可用于全数字 FDP 或混合波束 HBF，且在不同站点、传输功率和信道估计误差下保持鲁棒性。训练流程包含教师-学生自监督、元学习域泛化以及功率感知归一化等环节。

Result: 在来自三处未见站点的射线追踪数据上，PaPP 的 FDP 与 HBF 模型均优于传统基线和常见 DL 基线；在少量本地未标注样本的微调后，性能保持良好；两种架构均实现>21× 的建模计算能耗降低，且对信道估计误差具有鲁棒性。

Conclusion: PaPP 提供一种可在不同部署点和传输条件下通用且高效的 DL 预编码框架，兼容 FDP 与 HBF，显著降低能耗并提高鲁棒性，是能效优先的 mMIMO 下行预编码的实用解决方案。

Abstract: Massive multiple-input multiple-output (mMIMO) downlink precoding offers high spectral efficiency but remains challenging to deploy in practice because near-optimal algorithms such as the weighted minimum mean squared error (WMMSE) are computationally expensive, and sensitive to SNR and channel-estimation quality, while existing deep learning (DL)-based solutions often lack robustness and require retraining for each deployment site. This paper proposes a plug-and-play precoder (PaPP), a DL framework with a backbone that can be trained for either fully digital (FDP) or hybrid beamforming (HBF) precoding and reused across sites, transmit-power levels, and with varying amounts of channel estimation error, avoiding the need to train a new model from scratch at each deployment. PaPP combines a high-capacity teacher and a compact student with a self-supervised loss that balances teacher imitation and normalized sum-rate, trained using meta-learning domain-generalization and transmit-power-aware input normalization. Numerical results on ray-tracing data from three unseen sites show that the PaPP FDP and HBF models both outperform conventional and deep learning baselines, after fine-tuning with a small set of local unlabeled samples. Across both architectures, PaPP achieves more than 21$\times$ reduction in modeled computation energy and maintains good performance under channel-estimation errors, making it a practical solution for energy-efficient mMIMO precoding.

</details>


### [249] [Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting](https://arxiv.org/abs/2601.21899)
*Zhiqing Cui,Siru Zhong,Ming Jin,Shirui Pan,Qingsong Wen,Yuxuan Liang*

Main category: cs.LG

TL;DR: OmniAir uses semantic topology learning for global station-level air quality forecasting and introduces WorldAir with 7,800+ stations. It achieves state-of-the-art performance across 18 baselines and ~10x faster inference, effectively addressing data-sparse regions.


<details>
  <summary>Details</summary>
Motivation: Global air quality forecasting faces extreme spatial heterogeneity and poor generalization of transductive models to unseen regions. A framework that encodes invariant physical attributes into station identities and constructs adaptive topologies can capture non-Euclidean correlations and diffusion across uneven networks.

Method: Proposes OmniAir, a semantic topology learning framework that encodes invariant environmental attributes into generalizable station identities and dynamically constructs adaptive sparse topologies to capture long-range non-Euclidean correlations and diffusion patterns.

Result: OmniAir achieves state-of-the-art performance against 18 baselines and runs with high efficiency and scalability, with speeds nearly 10× faster than existing models, while bridging monitoring gaps in data-sparse regions.

Conclusion: OmniAir provides a scalable, generalizable solution for global station-level air quality forecasting and WorldAir offers a large-scale dataset to support broad evaluation and development in this domain.

Abstract: Global air quality forecasting grapples with extreme spatial heterogeneity and the poor generalization of existing transductive models to unseen regions. To tackle this, we propose OmniAir, a semantic topology learning framework tailored for global station-level prediction. By encoding invariant physical environmental attributes into generalizable station identities and dynamically constructing adaptive sparse topologies, our approach effectively captures long-range non-Euclidean correlations and physical diffusion patterns across unevenly distributed global networks. We further curate WorldAir, a massive dataset covering over 7,800 stations worldwide. Extensive experiments show that OmniAir achieves state-of-the-art performance against 18 baselines, maintaining high efficiency and scalability with speeds nearly 10 times faster than existing models, while effectively bridging the monitoring gap in data-sparse regions.

</details>


### [250] [Hardware-Triggered Backdoors](https://arxiv.org/abs/2601.21902)
*Jonas Möller,Erik Imgrund,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.LG

TL;DR: 本论文展示了通过利用硬件执行差异，在机器学习模型中植入硬件触发的后门。通过在局部移动决策边界并细化数值偏差，使同一输入在不同硬件上产生不同预测；在常见GPU加速器上可可靠复现；并讨论防御。


<details>
  <summary>Details</summary>
Motivation: 在异构硬件环境中，推理结果应一致，但硬件设计差异可导致微小数值变动，成为可被利用的攻击面。尤其对使用第三方模型的场景，风险更高。

Method: 通过局部移动模型的决策边界，使目标输入附近的边界变近，并通过控制/放大数值误差以在特定硬件上翻转预测；对输入点、模型权重、数值精度进行优化，确保在目标GPU上实现触发；在多种GPU加速器上进行实验验证。

Result: 实证表明，在常见GPU上可以可靠地创建硬件触发后门，攻击向量在不同硬件之间可复现；对第三方模型的安全性提出新威胁；评估了若干防御的有效性。

Conclusion: 硬件差异导致的后门是一种新颖的攻击向量，需要相应的防御策略，如硬件感知的鲁棒训练、跨硬件一致性验证、以及对外部模型来源的审查与检测。

Abstract: Machine learning models are routinely deployed on a wide range of computing hardware. Although such hardware is typically expected to produce identical results, differences in its design can lead to small numerical variations during inference. In this work, we show that these variations can be exploited to create backdoors in machine learning models. The core idea is to shape the model's decision function such that it yields different predictions for the same input when executed on different hardware. This effect is achieved by locally moving the decision boundary close to a target input and then refining numerical deviations to flip the prediction on selected hardware. We empirically demonstrate that these hardware-triggered backdoors can be created reliably across common GPU accelerators. Our findings reveal a novel attack vector affecting the use of third-party models, and we investigate different defenses to counter this threat.

</details>


### [251] [LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution](https://arxiv.org/abs/2601.21929)
*Shuangqi Li,Hieu Le,Jingyi Xu,Mathieu Salzmann*

Main category: cs.LG

TL;DR: LoRIF通过低秩因子分解和对数奇异值分解（SVD）+ Woodbury等式，解决梯度基于训练数据归因的存储、I/O和Hessian逆近似的规模瓶颈，实现对大规模模型的高效、高质量归因。


<details>
  <summary>Details</summary>
Motivation: 提升基于梯度的训练数据归因在大规模模型上的可扩展性。当前方法的两大瓶颈是：1) 需要为所有训练样本存储/加载投影后的逐样本梯度，I/O成为延迟瓶颈；2) 需要构建D×D的逆Hessian近似，需O(D^2)内存。随着投影维度D增大，质量与可扩展性之间出现权衡。

Method: 利用梯度的低秩结构，将投影后的逐样本梯度分解为秩c的因子，并仅存储这部分因子以降低存储和查询I/O至O(c√D)；在r维子空间内使用截断SVD并结合Woodbury恒等式近似Hessian项，将内存从O(D^2)降至O(Dr)；适用于0.1B至70B参数、带数百万样本数据集的模型。

Result: 在大量参数规模模型上，LoRIF相较于LoGRA实现最高可达20×的存储和查询加速，同时在归因质量上保持不劣于LoGRA，甚至达到或超过其水平。

Conclusion: LoRIF使基于梯度的训练数据归因在前沿尺度上具备实际可行性，缓解存储、I/O与内存瓶颈，同时维持高质量归因。

Abstract: Training data attribution (TDA) identifies which training examples most influenced a model's prediction. The best-performing TDA methods exploits gradients to define an influence function. To overcome the scalability challenge arising from gradient computation, the most popular strategy is random projection (e.g., TRAK, LoGRA). However, this still faces two bottlenecks when scaling to large training sets and high-quality attribution: \emph{(i)} storing and loading projected per-example gradients for all $N$ training examples, where query latency is dominated by I/O; and \emph{(ii)} forming the $D \times D$ inverse Hessian approximation, which costs $O(D^2)$ memory. Both bottlenecks scale with the projection dimension $D$, yet increasing $D$ is necessary for attribution quality -- creating a quality--scalability tradeoff. We introduce \textbf{LoRIF (Low-Rank Influence Functions)}, which exploits low-rank structures of gradient to address both bottlenecks. First, we store rank-$c$ factors of the projected per-example gradients rather than full matrices, reducing storage and query-time I/O from $O(D)$ to $O(c\sqrt{D})$ per layer per sample. Second, we use truncated SVD with the Woodbury identity to approximate the Hessian term in an $r$-dimensional subspace, reducing memory from $O(D^2)$ to $O(Dr)$. On models from 0.1B to 70B parameters trained on datasets with millions of examples, LoRIF achieves up to 20$\times$ storage reduction and query-time speedup compared to LoGRA, while matching or exceeding its attribution quality. LoRIF makes gradient-based TDA practical at frontier scale.

</details>


### [252] [Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models](https://arxiv.org/abs/2601.21943)
*Ahmad Aghapour,Erhan Bayraktar,Ziqing Zhang*

Main category: cs.LG

TL;DR: 提出一个信息理论层面的无维度依赖收敛分析，用于扩散模型的反向SDE离散化。给出目标分布与生成分布之间的KL散度上界为 O(H^2/K)（H为香农熵，K为采样步数），并提出基于训练损失的轻量化 Loss-Adaptive Schedule (LAS)，在不需要额外后训练计算的前提下提升采样质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型收敛分析往往随环境维度线性扩张或依赖于内在维度/几何假设的问题，寻求无几何假设的维度无关收敛界与高效的离散化策略。

Method: 提出以信息理论为基础的分析框架，通过对KL散度的重新表述，给出目标分布与生成分布之间的KL上界，与步数K的关系。引入 Loss-Adaptive Schedule (LAS)，基于训练损失来自适应离散化步长，计算量低且不依赖后训练的昂贵计算。

Result: 得到 KL(P_target || P_generated) = O(H^2/K)（端点因素忽略），在理论上实现维度无关的收敛速率；LAS 在实验中相较常见的启发式离散化策略提升采样质量。

Conclusion: 提供一种无需几何假设的无维度耦合收敛分析框架，并通过 LAS 实现高效、轻量的反向SDE离散化，为扩散模型的实际采样过程提供更优的调度策略。

Abstract: Diffusion generative models synthesize samples by discretizing reverse-time dynamics driven by a learned score (or denoiser). Existing convergence analyses of diffusion models typically scale at least linearly with the ambient dimension, and sharper rates often depend on intrinsic-dimension assumptions or other geometric restrictions on the target distribution. We develop an alternative, information-theoretic approach to dimension-free convergence that avoids any geometric assumptions. Under mild assumptions on the target distribution, we bound KL divergence between the target and generated distributions by $O(H^2/K)$ (up to endpoint factors), where $H$ is the Shannon entropy and $K$ is the number of sampling steps. Moreover, using a reformulation of the KL divergence, we propose a Loss-Adaptive Schedule (LAS) for efficient discretization of reverse SDE which is lightweight and relies only on the training loss, requiring no post-training heavy computation. Empirically, LAS improves sampling quality over common heuristic schedules.

</details>


### [253] [Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models](https://arxiv.org/abs/2601.21944)
*Konstantinos P. Panousis,Diego Marcos*

Main category: cs.LG

TL;DR: 提出清晰度(clarity)衡量与评估框架，分析 Vision-Language 模型中的概念瓶颈模型CBMs在不同建模决策下的表示与可解释性，比较VLM- CBMs与属性预测CBMs，并对三种稀疏性策略（逐样本L1、L0、伯努利）在相近性能下的折中。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs往往被视为黑箱，对其决策过程的可解释性研究不足。尽管存在后向和前向的可解释性方法，缺乏对学习表示的系统、客观评估，尤其是对越来越被视为提升可解释性的稀疏性诱导方法的综合评估。CBMs作为中介结构，便于对概念层次进行分析，因此研究建模决策对表示的影响具有重要意义。

Method: 引入清晰度(clarity)概念，将下游任务性能与概念表示的稀疏性和精度结合起来，构建基于地面真实概念注释的数据集的可解释性评估框架。比较两类CBMs：VLM- CBMs与属性预测CBMs；并对三种稀疏性诱导策略进行比较：逐样本的L1、L0和伯努利(Bernoulli)形式。

Result: 实验揭示灵活性与可解释性之间的关键权衡：在相近的性能水平下，同一方法在不同设置下的表现差异显著，表明建模决策对学习到的概念表示有显著影响。不同策略在不同情境下可能产生不同的可解释性与性能组合。

Conclusion: 提出了可解释性评估框架与清晰度度量，为CBMs在VLM场景中的解释性研究提供了基准；并计划在发表后公开代码。

Abstract: The widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to "induce interpretability". In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\ell_1, \ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.

</details>


### [254] [Embracing Aleatoric Uncertainty in Medical Multimodal Learning with Missing Modalities](https://arxiv.org/abs/2601.21950)
*Linxiao Gong,Yang Liu,Lianlong Sun,Yulai Bi,Jing Liu,Xiaoguang Zhu*

Main category: cs.LG

TL;DR: 提出 AUM，通过对单模态表示建模成多变量高斯以捕捉自发性不确定性，并在双分患者-模态图中进行基于不确定性 的消息传递聚合，以支持缺失模态的鲁棒融合，在ICU多模态预测任务上显著提升AUC-ROC


<details>
  <summary>Details</summary>
Motivation: 缺失模态在临床实践中普遍存在，且现有方法通常假设模态贡献相等且缺失模式随机，忽略数据采集中的本质不确定性

Method: 将每个模态的单模态表示建模成多变量高斯分布以捕捉阿雷托不确定性；在患者-模态的二部图中引入动态消息传递机制，通过不确定性感知的聚合自适应权衡可用模态信息，天然兼容缺失模态

Result: 在MIMIC-IV死亡率预测任务上AUC-ROC提升2.26%，在eICU上提升2.17%，优于现有最先进方法

Conclusion: AUM通过显式建模单模态不确定性并在不确定性感知的聚合下实现对缺失模态的鲁棒融合，从而提升多模态医疗预测的性能

Abstract: Medical multimodal learning faces significant challenges with missing modalities prevalent in clinical practice. Existing approaches assume equal contribution of modality and random missing patterns, neglecting inherent uncertainty in medical data acquisition. In this regard, we propose the Aleatoric Uncertainty Modeling (AUM) that explicitly quantifies unimodal aleatoric uncertainty to address missing modalities. Specifically, AUM models each unimodal representation as a multivariate Gaussian distribution to capture aleatoric uncertainty and enable principled modality reliability quantification. To adaptively aggregate captured information, we develop a dynamic message-passing mechanism within a bipartite patient-modality graph using uncertainty-aware aggregation mechanism. Through this process, missing modalities are naturally accommodated, while more reliable information from available modalities is dynamically emphasized to guide representation generation. Our AUM framework achieves an improvement of 2.26% AUC-ROC on MIMIC-IV mortality prediction and 2.17% gain on eICU, outperforming existing state-of-the-art approaches.

</details>


### [255] [Bridging Graph Structure and Knowledge-Guided Editing for Interpretable Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2601.21978)
*Shiqi Fan,Quanming Yao,Hongyi Nie,Wentao Ma,Zhen Wang,Wen Hua*

Main category: cs.LG

TL;DR: 三阶段混合推理框架IGETR：以时态GNN grounded候选路径、再由LLM编辑修正、最后整合路径以预测未来事件；在ICEWS数据集上实现SOTA，Hits@1提升至5.6%、Hits@3至8.1%。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时序知识图推理偏重上下文关系，难以从动态图中提取结构子图，导致对结构信息理解不足并出现 temporal inconsistency 导致的幻觉推理。需将结构建模能力强的GNN与LLM的上下文理解能力结合，提升推理的可解释性与准确性。

Method: 1) 基于时态GNN的阶段— ground：从时态知识图中筛选结构上和时间上一致的候选路径，确保推理以可靠的图证据为起点；2) 阶段— editing：通过LLM引导的路径编辑，修正逻辑与语义不一致，并利用外部知识丰富路径；3) 阶段— integration：将 refined 路径整合，产出可解释且准确的预测。对ICEWS等标准TKG基准进行评估，并做消融分析。

Result: 在标准TKG基准上达到SOTA，相较强基线在Hits@1和Hits@3上分别实现相对提升，最高达到5.6%和8.1%；消融实验与附加分析证实各组件的有效性。

Conclusion: IGETR成功将时态GNN的结构建模与LLM的上下文推理有机结合，提升了时序知识图推理的准确性与可解释性，并为动态图上的混合推理提供了可扩展的范式。

Abstract: Temporal knowledge graph reasoning (TKGR) aims to predict future events by inferring missing entities with dynamic knowledge structures. Existing LLM-based reasoning methods prioritize contextual over structural relations, struggling to extract relevant subgraphs from dynamic graphs. This limits structural information understanding, leading to unstructured, hallucination-prone inferences especially with temporal inconsistencies. To address this problem, we propose IGETR (Integration of Graph and Editing-enhanced Temporal Reasoning), a hybrid reasoning framework that combines the structured temporal modeling capabilities of Graph Neural Networks (GNNs) with the contextual understanding of LLMs. IGETR operates through a three-stage pipeline. The first stage aims to ground the reasoning process in the actual data by identifying structurally and temporally coherent candidate paths through a temporal GNN, ensuring that inference starts from reliable graph-based evidence. The second stage introduces LLM-guided path editing to address logical and semantic inconsistencies, leveraging external knowledge to refine and enhance the initial paths. The final stage focuses on integrating the refined reasoning paths to produce predictions that are both accurate and interpretable. Experiments on standard TKG benchmarks show that IGETR achieves state-of-the-art performance, outperforming strong baselines with relative improvements of up to 5.6% on Hits@1 and 8.1% on Hits@3 on the challenging ICEWS datasets. Additionally, we execute ablation studies and additional analyses confirm the effectiveness of each component.

</details>


### [256] [Investigation into using stochastic embedding representations for evaluating the trustworthiness of the Fréchet Inception Distance](https://arxiv.org/abs/2601.21979)
*Ciaran Bench,Vivek Desai,Carlijn Roozemond,Ruben van Engen,Spencer A. Thomas*

Main category: cs.LG

TL;DR: 对FID在医疗影像中的适用性进行评估，使用蒙特卡洛 dropout 来估计预测方差及潜在表示的方差，观察方差与数据分布外度的相关性，评价FID作为信任度指标的有效性。


<details>
  <summary>Details</summary>
Motivation: FID基于ImageNet pretrained的自然图像特征在医疗影像任务中的适用性有限；需要量化预测不确定性与输入分布外度对FID可靠性的影响。

Method: 采用蒙特卡洛 dropout 计算FID的预测方差，以及对特征嵌入模型的潜在表示的预测方差的辅助估计；在ImageNet1K验证集经不同强度增强后以及其他外部数据集上评估方差与分布外度的相关性。

Result: 预测方差的大小与测试输入相对于训练数据的分布外度存在不同程度的相关性，提示方差可在一定程度上反映FID在医疗应用中的可信性，但并非完美指标。

Conclusion: 对基于FID的医疗影像质量评估需谨慎解读，需结合预测不确定性与领域特异性特征，可能需要更合适的特征提取器或替代指标。

Abstract: Feature embeddings acquired from pretrained models are widely used in medical applications of deep learning to assess the characteristics of datasets; e.g. to determine the quality of synthetic, generated medical images. The Fréchet Inception Distance (FID) is one popular synthetic image quality metric that relies on the assumption that the characteristic features of the data can be detected and encoded by an InceptionV3 model pretrained on ImageNet1K (natural images). While it is widely known that this makes it less effective for applications involving medical images, the extent to which the metric fails to capture meaningful differences in image characteristics is not obviously known. Here, we use Monte Carlo dropout to compute the predictive variance in the FID as well as a supplemental estimate of the predictive variance in the feature embedding model's latent representations. We show that the magnitudes of the predictive variances considered exhibit varying degrees of correlation with the extent to which test inputs (ImageNet1K validation set augmented at various strengths, and other external datasets) are out-of-distribution relative to its training data, providing some insight into the effectiveness of their use as indicators of the trustworthiness of the FID.

</details>


### [257] [PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters](https://arxiv.org/abs/2601.21984)
*Jian Gao,Yiwei Zou,Abhishek Pradhan,Wenhao Huang,Yumin Su,Kaiyuan Yang,Xuan Zhang*

Main category: cs.LG

TL;DR: PowerGenie enables scalable automated discovery of high-performance reconfigurable power converters using an automated analytical framework and evolutionary finetuning, achieving significant FoM gains and verified SPICE efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: The design space for reconfigurable power converters grows exponentially, making performance-driven discovery challenging for humans and existing AI methods limited by predefined templates or unverified generation at scale.

Method: (1) An automated analytical framework to determine converter functionality and theoretical performance limits without component sizing or SPICE; (2) An evolutionary finetuning approach that co-evolves a generative model with its training distribution via fitness selection and uniqueness verification to avoid mode collapse and overfitting.

Result: Discovered an 8-mode reconfigurable converter with 23% higher figure-of-merit (FoM) than the best training topology. SPICE simulations show average absolute efficiency gains of 10% across 8 modes and up to 17% for a single mode. Code is available at the provided GitHub URL.

Conclusion: PowerGenie demonstrates scalable, performance-driven topology discovery for power converters with rigorous verification and open-source access; the approach yields notable FoM and efficiency improvements and could impact automated hardware design.

Abstract: Discovering superior circuit topologies requires navigating an exponentially large design space-a challenge traditionally reserved for human experts. Existing AI methods either select from predefined templates or generate novel topologies at a limited scale without rigorous verification, leaving large-scale performance-driven discovery underexplored. We present PowerGenie, a framework for automated discovery of higher-performance reconfigurable power converters at scale. PowerGenie introduces: (1) an automated analytical framework that determines converter functionality and theoretical performance limits without component sizing or SPICE simulation, and (2) an evolutionary finetuning method that co-evolves a generative model with its training distribution through fitness selection and uniqueness verification. Unlike existing methods that suffer from mode collapse and overfitting, our approach achieves higher syntax validity, function validity, novelty rate, and figure-of-merit (FoM). PowerGenie discovers a novel 8-mode reconfigurable converter with 23% higher FoM than the best training topology. SPICE simulations confirm average absolute efficiency gains of 10% across 8 modes and up to 17% at a single mode. Code is available at https://github.com/xz-group/PowerGenie.

</details>


### [258] [Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields](https://arxiv.org/abs/2601.21985)
*Yunyang Li,Lin Huang,Luojia Xia,Wenhe Zhang,Mark Gerstein*

Main category: cs.LG

TL;DR: Elign是一种后训练框架，通过用预训练的基础ML力场替代DFT评估并在训练阶段进行物理引导，使扩散模型在推理阶段不需要能量评估即可生成更接近高保真哈密顿量的构象。


<details>
  <summary>Details</summary>
Motivation: 生成符合欧几里得对称性且符合热力学平衡分布的分子构象时，E(3)-等变扩散模型容易从半经验训练数据中继承偏见，且高保真哈密顿量的量子化学评估成本高且需在采样每一步重复查询。

Method: 1) 用预训练的ML力场替代DFT评估以提供物理信号；2) 将反向扩散视为强化学习问题，通过FED-GRPO微调去噪策略，采用势能奖励和力稳定性奖励，两者分组归一化独立优化。

Result: 实验表明Elign生成的构象具有更低的DFT能量与力，稳定性提升；推理阶段与无引导采样同等快速，因为在生成时不需要额外的能量评估。

Conclusion: Elign实现对成本的双重摊销，使训练阶段的物理引导替代推理阶段的能量查询，提升高保真分子构象的高效生成。

Abstract: Generative models for 3D molecular conformations must respect Euclidean symmetries and concentrate probability mass on thermodynamically favorable, mechanically stable structures. However, E(3)-equivariant diffusion models often reproduce biases from semi-empirical training data rather than capturing the equilibrium distribution of a high-fidelity Hamiltonian. While physics-based guidance can correct this, it faces two computational bottlenecks: expensive quantum-chemical evaluations (e.g., DFT) and the need to repeat such queries at every sampling step. We present Elign, a post-training framework that amortizes both costs. First, we replace expensive DFT evaluations with a faster, pretrained foundational machine-learning force field (MLFF) to provide physical signals. Second, we eliminate repeated run-time queries by shifting physical steering to the training phase. To achieve the second amortization, we formulate reverse diffusion as a reinforcement learning problem and introduce Force--Energy Disentangled Group Relative Policy Optimization (FED-GRPO) to fine-tune the denoising policy. FED-GRPO includes a potential-based energy reward and a force-based stability reward, which are optimized and group-normalized independently. Experiments show that Elign generates conformations with lower gold-standard DFT energies and forces, while improving stability. Crucially, inference remains as fast as unguided sampling, since no energy evaluations are required during generation.

</details>


### [259] [Generalized Information Gathering Under Dynamics Uncertainty](https://arxiv.org/abs/2601.21988)
*Fernando Palafox,Jingqi Li,Jesse Milzman,David Fridovich-Keil*

Main category: cs.LG

TL;DR: 提出一个通用的信息获取成本框架，基于 Massey 的定向信息，解耦模型选择对信息成本的影响，并证明互信息成本是其特例，同时建立与线性化贝叶斯估计的信息增益的联系，并通过多类系统实验验证。


<details>
  <summary>Details</summary>
Motivation: 未知动力系统的智能体需要从观测中学习动力学；现有信息获取成本依赖特定建模选择，缺乏统一、无偏的度量。

Method: 给出一个显式暴露参数、信念与控制之间因果关系的统一框架；在该框架内推导基于 Massey 定向信息的通用信息获取成本，假设仅有马尔可夫动力学与加性噪声；证明互信息成本是该成本的特例；将其与线性化贝叶斯信息增益建立联系；通过线性、非线性与多智能体系统的实验验证框架的有效性。

Result: 证明互信息成本是新成本的特例；建立互信息成本与线性化贝叶斯信息增益之间的定量关系；在多类系统上验证框架的实用性。

Conclusion: 提供一个普适、与具体建模选择解耦的主动信息获取框架，为分析与设计主动学习策略提供理论基础，且为基于互信息的方法提供理论 justification。

Abstract: An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.

</details>


### [260] [Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains](https://arxiv.org/abs/2601.21999)
*Meng Cao,Jiexi Liu,Songcan Chen*

Main category: cs.LG

TL;DR: 提出用于不平衡域泛化（IDG）的负主导对比学习（NDCL）框架。通过理论界定 generalization bound 并设计以负样本为主的边界优化、重加权交叉熵和预测中心对齐，提升跨域和类别不平衡环境下的判决边界与后验一致性，并在基准数据集上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: IDG 同时面临域偏移与标签分布偏斜的耦合问题，理论上需要一个能解释后验差异对决策边界的影响的一般化界限。现有方法在理论基础不足，且难以直接控制决策边界以应对不平衡分布。

Method: 首先推导 IDG 的一般化界限，揭示后验差异与决策边界的关系。随后提出 NDCL：以负样本为主的对比学习强化跨类别边界分离；通过重加权交叉熵提升类内紧致性；采用预测中心对齐实现跨域后验一致性。

Result: 在多组基准数据集上进行严格实验，NDCL 显著提升跨域泛化与少数类鲁棒性，实验结果验证了理论分析与设计的有效性。代码可复现链接在论文中。

Conclusion: NDCL 为 IDG 提供了一个理论与实践相结合的解决方案，通过负样本对比、重加权 CE 与预测对齐，实现更稳健的域间与类别不平衡泛化。

Abstract: Imbalanced Domain Generalization (IDG) focuses on mitigating both domain and label shifts, both of which fundamentally shape the model's decision boundaries, particularly under heterogeneous long-tailed distributions across domains. Despite its practical significance, it remains underexplored, primarily due to the technical complexity of handling their entanglement and the paucity of theoretical foundations. In this paper, we begin by theoretically establishing the generalization bound for IDG, highlighting the role of posterior discrepancy and decision margin. This bound motivates us to focus on directly steering decision boundaries, marking a clear departure from existing methods. Subsequently, we technically propose a novel Negative-Dominant Contrastive Learning (NDCL) for IDG to enhance discriminability while enforce posterior consistency across domains. Specifically, inter-class decision-boundary separation is enhanced by placing greater emphasis on negatives as the primary signal in our contrastive learning, naturally amplifying gradient signals for minority classes to avoid the decision boundary being biased toward majority classes. Meanwhile, intra-class compactness is encouraged through a re-weighted cross-entropy strategy, and posterior consistency across domains is enforced through a prediction-central alignment strategy. Finally, rigorous yet challenging experiments on benchmarks validate the effectiveness of our NDCL. The code is available at https://github.com/Alrash/NDCL.

</details>


### [261] [Rate-Distortion Optimization for Transformer Inference](https://arxiv.org/abs/2601.22002)
*Anderson de Andrade,Alon Harell,Ivan V. Bajić*

Main category: cs.LG

TL;DR: 提出基于速率-失真(RD)的有损编码框架，用于对 Transformer 的中间表示进行紧凑编码，从而在分布式推理中降低带宽和内存需求，同时保持或提升准确率，并给出速率与熵差的理论分析与 PAC 风格界限。


<details>
  <summary>Details</summary>
Motivation: 解决大规模 Transformer 推理中的高计算和内存成本问题，缺乏统一、可解释的框架来权衡编码比特率与任务准确性。

Method: 设计并学习端到端的有损压缩编解码体系，结合信息理论扩展，推导 rate-entropy gap 的界限；给出 PAC 风格的估计方法；在多种语言任务和架构上进行评估。

Result: 在语言基准上实现显著的带宽和内存节省，并在某些情况下提升准确率，优于更复杂的基线；理论分析揭示不同架构的速率受 bound 的支配，增强解释性。

Conclusion: 提供一个统一的理论+实验框架，将信息论概念应用于 Transformer 表征的有损压缩，给出可估计的上界，提升对 rate-accuracy trade-off 的理解，并为分布式推理提供实用指导。

Abstract: Transformers achieve superior performance on many tasks, but impose heavy compute and memory requirements during inference. This inference can be made more efficient by partitioning the process across multiple devices, which, in turn, requires compressing its intermediate representations. In this work, we introduce a principled rate-distortion-based framework for lossy compression that learns compact encodings that explicitly trade off bitrate against accuracy. Experiments on language benchmarks show that the proposed codec achieves substantial savings with improved accuracy in some cases, outperforming more complex baseline methods. We characterize and analyze the rate-distortion performance of transformers, offering a unified lens for understanding performance in representation coding. This formulation extends information-theoretic concepts to define the gap between rate and entropy, and derive some of its bounds. We further develop probably approximately correct (PAC)-style bounds for estimating this gap. For different architectures and tasks, we empirically demonstrate that their rates are driven by these bounds, adding to the explainability of the formulation.

</details>


### [262] [Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering](https://arxiv.org/abs/2601.22010)
*Dongxuan Zhu,Ly Tran Ho Khanh,Andy Yat-Ming Cheung,Man-Chung Yue,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 提出 STARS，利用斯蒂费尔流形上的正交激活干预，在推理阶段对并行生成路径进行探索性激活引导，以提高多样性且保持质量，且实现低延迟的一步更新。


<details>
  <summary>Details</summary>
Motivation: 语言模型往往输出高度相似的结果，跨并行生成路径的多样性不足。现有采样方法虽引入随机性但难以在多条生成路径间实现显著的分离，需一个训练无关、推理阶段可用的多样性提升方法。

Method: 在每个生成步对并发生成的隐藏激活进行多方向线性干预，将多个方向联合在斯蒂费尔流形上优化；目标是最大化干预后激活向量的几何体积，同时通过斯蒂费尔约束确保干预方向正交。可用黎曼梯度下降求解，存在收敛性，但在实时推理中成本太高，因此提出一步近似更新与封闭形式的步长以实现低时延。

Result: 在测试用例生成和科学发现基准上，STARS 相比标准采样方法在提升多样性的同时保持或提升定性表现，表现出更丰富的生成路径。

Conclusion: STARS 为推理阶段的激活导引提供了一种无训练成本、可扩展的多样性提升策略，通过正交且几何上最大化的干预，显式促进并发生成的分歧并隐式鼓励不同的生成轨迹。

Abstract: Language models often default to a narrow set of high-probability outputs, leaving their generation paths homogeneous and prone to mode collapse. Sampling-based strategies inject randomness but still struggle to guarantee diversity across multiple concurrent generation runs. We address this limitation by introducing STARS ($\textbf{St}$iefel-based $\textbf{A}$ctivation Steering for Diverse $\textbf{R}$ea$\textbf{S}$oning), a training-free, inference-time intervention method that transforms activation steering into an exploration engine. At each token, STARS collects the hidden activations of concurrent generation runs and optimizes multiple additive steering directions jointly on the Stiefel manifold. STARS maximizes the geometric volume of the steered activations, while the Stiefel manifold induces orthogonality of the steering interventions. This formulation explicitly promotes divergent activation vectors of concurrent generation runs, and implicitly promotes divergent generation trajectories. This manifold optimization formulation can be solved using a Riemannian gradient descent algorithm with convergence guarantees, but this algorithm is too time-consuming for real-time inference. To guarantee low latency, we further design a lightweight one-step update with an aggressive, closed-form stepsize. For test case generation and scientific discovery benchmarks, STARS consistently outperforms standard sampling methods, achieving greater diversity without sacrificing qualitative performance.

</details>


### [263] [Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability](https://arxiv.org/abs/2601.22012)
*Sergi Masip,Gido M. van de Ven,Javier Ferrando,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 提出一个以特征编码几何变换解释灾难性遗忘的框架，核心在于容量压缩与下游读出干扰；在可解模型与实际模型上验证深度的负面影响，并以Crosscoders与ViT在序列CIFAR-10的案例展示框架应用。


<details>
  <summary>Details</summary>
Motivation: 当前评估多聚焦于最后一层性能，忽视特征层的机制，缺乏面向特征的解释框架；需要一个几何化、可检验的因果框架来揭示灾难性遗忘的根本原因。

Method: 提出一个几何/特征编码的机械框架；推导最优/最差情形；使用一个可解析的模型进行理论分析；在该模型上进行实验验证；通过 Crosscoders 将框架应用到实际模型；对 Vision Transformer 在序列 CIFAR-10 上进行案例研究。

Result: 理论分析揭示深度增加会进一步降低特征的可利用性和读出稳定性，导致更明显的遗忘；实验结果支持理论，且在实际模型上也能解释现象，且跨任务/跨模型的分析工具可帮助诊断。

Conclusion: 提供一种新的、以特征为中心的持续学习分析词汇和框架，可用于设计更鲁棒的模型（如控制特征容量、改善读出机制），并扩展到实际强大模型的分析。

Abstract: Catastrophic forgetting in continual learning is often measured at the performance or last-layer representation level, overlooking the underlying mechanisms. We introduce a mechanistic framework that offers a geometric interpretation of catastrophic forgetting as the result of transformations to the encoding of individual features. These transformations can lead to forgetting by reducing the allocated capacity of features (worse representation) and disrupting their readout by downstream computations. Analysis of a tractable model formalizes this view, allowing us to identify best- and worst-case scenarios. Through experiments on this model, we empirically test our formal analysis and highlight the detrimental effect of depth. Finally, we demonstrate how our framework can be used in the analysis of practical models through the use of Crosscoders. We present a case study of a Vision Transformer trained on sequential CIFAR-10. Our work provides a new, feature-centric vocabulary for continual learning.

</details>


### [264] [TBDFiltering: Sample-Efficient Tree-Based Data Filtering](https://arxiv.org/abs/2601.22016)
*Robert Istvan Busa-Fekete,Julian Zimmert,Anne Xiangyi Zheng,Claudio Gentile,Andras Gyorgy*

Main category: cs.LG

TL;DR: 基于文本嵌入的分层聚类用于高效评估LLM训练数据质量，能够在有限查询下对每个文档质量进行预测。


<details>
  <summary>Details</summary>
Motivation: 缺乏廉价、可靠的文档质量度量；现有方法依赖对极少信号的分类器，难以扩展到规模化的训练集。

Method: 使用文本嵌入驱动的分层聚类，提出自适应地选择需由LLM评估的文档以估计聚类质量；理论上若存在一棵子树，其叶子聚类近乎纯净，则以少量查询即可正确预测每个文档的质量，查询量与最小近似纯叶子子树的大小成正比，且算法无需事先知道该子树。

Result: 实验结果表明该方法在查询效率和筛选效果上优于基于分类器的筛选方法。

Conclusion: 提出一种可扩展、查询高效的训练数据质量评估框架，适用于大规模LLM训练数据的文档筛选。

Abstract: The quality of machine learning models depends heavily on their training data. Selecting high-quality, diverse training sets for large language models (LLMs) is a difficult task, due to the lack of cheap and reliable quality metrics. While querying existing LLMs for document quality is common, this is not scalable to the large number (billions) of documents used in training. Instead, practitioners often use classifiers trained on sparse quality signals. In this paper, we propose a text-embedding-based hierarchical clustering approach that adaptively selects the documents to be evaluated by the LLM to estimate cluster quality. We prove that our method is query efficient: under the assumption that the hierarchical clustering contains a subtree such that each leaf cluster in the tree is pure enough (i.e., it mostly contains either only good or only bad documents), with high probability, the method can correctly predict the quality of each document after querying a small number of documents. The number of such documents is proportional to the size of the smallest subtree with (almost) pure leaves, without the algorithm knowing this subtree in advance. Furthermore, in a comprehensive experimental study, we demonstrate the benefits of our algorithm compared to other classifier-based filtering methods.

</details>


### [265] [Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2601.22020)
*Chengyi Cai,Zesheng Ye,Peike Li,Bo Han,Jianzhong Qi,Feng Liu*

Main category: cs.LG

TL;DR: 提出 ViKeR，在多模态大语言模型的非显式隐私信息移除任务中，通过视觉信号引导的关键-token正则化，针对不同 token 的重要性进行加权更新，以实现有效的“去学习”（unlearning）并减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLM 去学习方法多沿用 LLM 方案，忽略不同 token 的重要性以及视觉线索对关键 token 的指示，导致未能高效保护隐私信息或引发遗忘/语义偏移。

Method: 引入 Visual-Guided Key-Token Regularization (ViKeR)：利用不可相关的视觉输入来预测理想的去学习后 token-level 分布，并以此分布对 unlearning 过程进行正则化，从而优先优化关键 tokens。定义关键 tokens 的信息熵，并通过 token-level 梯度重加权来放大对关键 tokens 的更新；并在多模态问答场景下进行评估。

Result: 在 MLLMU 和 CLEAR 基准上，ViKeR 能实现有效的去学习，降低遗忘，保持回答连贯性。

Conclusion: ViKeR 提供了一种新颖的多模态去学习框架，通过视觉线索引导的关键 token 正则化和梯度重加权，提升隐私信息保护的可控性与模型稳定性。

Abstract: Unlearning in Multimodal Large Language Models (MLLMs) prevents the model from revealing private information when queried about target images. Existing MLLM unlearning methods largely adopt approaches developed for LLMs. They treat all answer tokens uniformly, disregarding their varying importance in the unlearning process. Moreover, these methods focus exclusively on the language modality, disregarding visual cues that indicate key tokens in answers. In this paper, after formulating the problem of unlearning in multimodal question answering for MLLMs, we propose Visual-Guided Key-Token Regularization (ViKeR). We leverage irrelevant visual inputs to predict ideal post-unlearning token-level distributions and use these distributions to regularize the unlearning process, thereby prioritizing key tokens. Further, we define key tokens in unlearning via information entropy and discuss ViKeR's effectiveness through token-level gradient reweighting, which amplifies updates on key tokens. Experiments on MLLMU and CLEAR benchmarks demonstrate that our method effectively performs unlearning while mitigating forgetting and maintaining response coherence.

</details>


### [266] [From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning](https://arxiv.org/abs/2601.22028)
*Haoran Tang,Rajiv Khanna*

Main category: cs.LG

TL;DR: CLReg introduces a contrastive representation regularizer to reduce forgetting-retain entanglement in LLMs, improving unlearning beyond prediction-space alignment with minimal impact on retained knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods rely on prediction-space alignment to suppress forgotten content, but this can leave forgotten concepts entangled in representations, hindering robust unlearning and potentially leaking influence.

Method: CLReg identifies forget features via contrastive representation regularization and pushes them away from retain features, reducing forget-retain interference with minimal shifts to retain features; the work also offers theoretical connections between representation shaping and entanglement reduction.

Result: Across unlearning benchmarks and LLMs of varying sizes, CLReg reduces forget-retain representation entanglement and enhances mainstream unlearning methods without introducing additional privacy risks.

Conclusion: Shaping the representation space to remove forget concepts is a promising direction for future unlearning research; CLReg provides a practical and theory-backed approach for reducing entanglement between forgetting and retaining information.

Abstract: Most LLM unlearning methods aim to approximate retrain-from-scratch behaviors with minimal distribution shift, often via alignment-style objectives defined in the prediction space. While effective at reducing forgotten content generation, such approaches may act as suppression: forgotten concepts can persist in representations and remain entangled with retained knowledge. We introduce CLReg, a contrastive representation regularizer that identifies forget features while pushing them away from retain features, explicitly reducing forget-retain interference with minimal shifts on retain features. We provide first theoretical insights that relate representation shaping to entanglement reduction. Across unlearning benchmarks and LLMs of different sizes, CLReg decreases forget-retain representation entanglement that facilitates mainstream unlearning methods without positing extra privacy risks, inspiring future work that reshapes the representation space to remove forget concepts.

</details>


### [267] [The Ensemble Inverse Problem: Applications and Methods](https://arxiv.org/abs/2601.22029)
*Zhengyan Huan,Camila Pazos,Martin Klassen,Vincent Croft,Pierre-Hugues Beauchemin,Shuchin Aeron*

Main category: cs.LG

TL;DR: 提出并研究 Ensemble Inverse Problem（EIP），通过一类新的条件生成模型（集合逆生成模型）实现非迭代推断，利用观测集合的集合信息来后验采样；在高能物理、全波形反演、逆成像等领域应用；给出代码。


<details>
  <summary>Details</summary>
Motivation: 解决将先验通过正向过程推送产生的样本集合反演成目标分布的多变量统计问题，尤其在需要避免在推断阶段重复调用正向模型的场景；其中在高能物理扭曲观测的展开问题、全波形反演及未知先验的逆成像中具高度相关性。

Method: 提出非迭代的推断时方法，构建基于条件集合逆生成模型的后验采样器；在训练阶段通过大量的同一正向模型但不同先验的真值-观测对集合进行训练，使模型隐式编码似然；在推断时利用观测集合中的集合信息对后验进行建模，不需要显式迭代地使用正向模型。

Result: 在多种合成及真实数据集上测试（逆成像、HEP、FWI），表明集合信息提升后验推断性能，且可推广到未见过的先验；代码公开。

Conclusion: 为高维多变量逆问题提供一个非迭代、可泛化到未见先验的后验采样框架，适用于HEP展开、FWI及逆成像等场景，且通过集合信息实现对似然的隐式编码；具有较好的适用性和可扩展性。

Abstract: We introduce a new multivariate statistical problem that we refer to as the Ensemble Inverse Problem (EIP). The aim of EIP is to invert for an ensemble that is distributed according to the pushforward of a prior under a forward process. In high energy physics (HEP), this is related to a widely known problem called unfolding, which aims to reconstruct the true physics distribution of quantities, such as momentum and angle, from measurements that are distorted by detector effects. In recent applications, the EIP also arises in full waveform inversion (FWI) and inverse imaging with unknown priors. We propose non-iterative inference-time methods that construct posterior samplers based on a new class of conditional generative models, which we call ensemble inverse generative models. For the posterior modeling, these models additionally use the ensemble information contained in the observation set on top of single measurements. Unlike existing methods, our proposed methods avoid explicit and iterative use of the forward model at inference time via training across several sets of truth-observation pairs that are consistent with the same forward model, but originate from a wide range of priors. We demonstrate that this training procedure implicitly encodes the likelihood model. The use of ensemble information helps posterior inference and enables generalization to unseen priors. We benchmark the proposed method on several synthetic and real datasets in inverse imaging, HEP, and FWI. The codes are available at https://github.com/ZhengyanHuan/The-Ensemble-Inverse-Problem--Applications-and-Methods.

</details>


### [268] [Per-parameter Task Arithmetic for Unlearning in Large Language Models](https://arxiv.org/abs/2601.22030)
*Chengyi Cai,Zesheng Ye,Jiangchao Yao,Jianzhong Qi,Bo Han,Xiaolu Zhang,Feng Liu,Jun Zhou*

Main category: cs.LG

TL;DR: PerTA通过对每个参数对TV进行重新缩放，以区分遗忘和保留的参数重要性，从而在保持任务算子效率的同时减少过度遗忘；其两种实现是PerTA-grad和PerTA-fisher；实验表明在遗忘效果和模型效用方面优于标准TV与多数训练型无忘方法。


<details>
  <summary>Details</summary>
Motivation: 解决任务向量（TV）在从LLM中删除私有信息时可能导致过度遗忘的问题；不同参数对遗忘和保留的重要性不同，需要对TV进行逐参数量化的缩放。

Method: 提出PerTA机制，对TV进行逐参数重标定；通过梯度估计得到PerTA-grad，或通过对角Fisher信息估计得到PerTA-fisher；并讨论其更一般形式和分析。扩展包括对更广泛形式的讨论。

Result: 实验证据表明PerTA在多数场景下优于标准TV，并在很多情况下超越常用的基于训练的无忘方法，在遗忘效果和整体模型效用方面表现更好。

Conclusion: PerTA在保持任务算子高效的同时缓解过度遗忘，提供一个有原理性且实用的LLM无忘框架，具有良好的适用性与分析性。

Abstract: In large language model (LLM) unlearning, private information is required to be removed. Task arithmetic unlearns by subtracting a specific task vector (TV)--defined as the parameter difference between a privacy-information-tuned model and the original model. While efficient, it can cause over-forgetting by disrupting parameters essential for retaining other information. Motivated by the observation that each parameter exhibits different importance for forgetting versus retention, we propose a per-parameter task arithmetic (PerTA) mechanism to rescale the TV, allowing per-parameter adjustment. These weights quantify the relative importance of each parameter for forgetting versus retention, estimated via gradients (i.e., PerTA-grad) or the diagonal Fisher information approximation (i.e., PerTA-fisher). Moreover, we discuss the effectiveness of PerTA, extend it to a more general form, and provide further analysis. Extensive experiments demonstrate that PerTA consistently improves upon standard TV, and in many cases surpasses widely used training-based unlearning methods in both forgetting effectiveness and overall model utility. By retaining the efficiency of task arithmetic while mitigating over-forgetting, PerTA offers a principled and practical framework for LLM unlearning.

</details>


### [269] [Holographic generative flows with AdS/CFT](https://arxiv.org/abs/2601.22033)
*Ehsan Mirafzali,Sanjit Shashi,Sanya Murdeshwar,Edgar Shaghoulian,Daniele Venturi,Razvan Marinescu*

Main category: cs.LG

TL;DR: 将 AdS/CFT 的 bulk-to-boundary 映射用于数据生成框架，扩展流匹配算法，结合深度学习与传输理论，提升对Checkerboard数据与 MNIST 的生成质量与收敛速度，提供可解释的物理版本的流匹配并展示 AdS 几何在生成建模中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型在收敛速度与可解释性方面的不足，利用全息原理与 AdS/CFT 提供几何约束与物理直觉，以改进流式生成过程的表示和正则化。

Method: 将从基分布到目标分布的数据流表示为 AdS 中 bulk-to-boundary 映射的标量场，并在流匹配框架中加入 AdS 物理约束；在 checkerboard 数据集和 MNIST 上实现并与无物理信息的对比模型比较。

Result: 相较于无物理信息的流匹配模型，呈现更快的收敛与更高的生成质量；给出具有物理可解释性的流匹配版本；初步验证 AdS 物理与几何在生成建模中的实用性。

Conclusion: 初步证明 AdS 物理与几何在新型生成范式中的潜在应用，暗示未来在生成模型中进一步融合全息框架与传输理论。

Abstract: We present a framework for generative machine learning that leverages the holographic principle of quantum gravity, or to be more precise its manifestation as the anti-de Sitter/conformal field theory (AdS/CFT) correspondence, with techniques for deep learning and transport theory. Our proposal is to represent the flow of data from a base distribution to some learned distribution using the bulk-to-boundary mapping of scalar fields in AdS. In the language of machine learning, we are representing and augmenting the flow-matching algorithm with AdS physics. Using a checkerboard toy dataset and MNIST, we find that our model achieves faster and higher quality convergence than comparable physics-free flow-matching models. Our method provides a physically interpretable version of flow matching. More broadly, it establishes the utility of AdS physics and geometry in the development of novel paradigms in generative modeling.

</details>


### [270] [Cross-Fusion Distance: A Novel Metric for Measuring Fusion and Separability Between Data Groups in Representation Space](https://arxiv.org/abs/2601.22036)
*Xiaolong Zhang,Jianwei Zhang,Xubo Song*

Main category: cs.LG

TL;DR: CFD is a new distance metric for measuring fusion between data groups in representation space that isolates fusion-altering geometry, robust to fusion-preserving variations, with linear complexity and better alignment with generalization degradation under domain shift.


<details>
  <summary>Details</summary>
Motivation: A fundamental need in representation learning under domain shift is to quantify how much groups fuse in representation space while separating geometry-altering factors from scale and sampling-induced layout changes. Existing distributional distances conflate these factors and provide less informative measures of true fusion.

Method: Introduce Cross-Fusion Distance (CFD), a geometry-aware distance that isolates fusion-altering geometry and remains invariant to fusion-preserving variations (e.g., global scaling, sampling-induced layout changes). The approach includes theoretical characterization of invariance and sensitivity, and validation in synthetic experiments. CFD is designed with linear computational complexity.

Result: CFD's invariance and sensitivity properties are validated in controlled synthetic experiments. On real-world datasets with domain shift, CFD aligns more closely with downstream generalization degradation than commonly used alternatives. The method is computationally efficient (linear time).

Conclusion: CFD provides a theoretically grounded and interpretable distance measure for representation learning, enabling robust assessment of fusion between data groups by focusing on geometry changes while ignoring preserving variations.

Abstract: Quantifying degrees of fusion and separability between data groups in representation space is a fundamental problem in representation learning, particularly under domain shift. A meaningful metric should capture fusion-altering factors like geometric displacement between representation groups, whose variations change the extent of fusion, while remaining invariant to fusion-preserving factors such as global scaling and sampling-induced layout changes, whose variations do not. Existing distributional distance metrics conflate these factors, leading to measures that are not informative of the true extent of fusion between data groups. We introduce Cross-Fusion Distance (CFD), a principled measure that isolates fusion-altering geometry while remaining robust to fusion-preserving variations, with linear computational complexity. We characterize the invariance and sensitivity properties of CFD theoretically and validate them in controlled synthetic experiments. For practical utility on real-world datasets with domain shift, CFD aligns more closely with downstream generalization degradation than commonly used alternatives. Overall, CFD provides a theoretically grounded and interpretable distance measure for representation learning.

</details>


### [271] [Making Foundation Models Probabilistic via Singular Value Ensembles](https://arxiv.org/abs/2601.22068)
*Mehmet Ozgur Turkoglu,Dominik J. Mühlematter,Alexander Becker,Konrad Schindler,Helge Aasen*

Main category: cs.LG

TL;DR: SVE is a parameter-efficient implicit ensemble that uses frozen singular vectors of weight matrices and trainable per-member singular values to approximate ensembles, achieving calibration close to deep ensembles with <1% extra parameters.


<details>
  <summary>Details</summary>
Motivation: Quantify epistemic uncertainty in foundation models without the prohibitive cost of full ensembles; enable calibrated predictions with large models under resource constraints.

Method: Assume singular vectors of weight matrices capture meaningful, orthogonal knowledge directions. Build ensemble by freezing these vectors and training per-member singular values that rescale each direction's contribution. Diversity arises from random initialization and mini-batch stochasticity during joint training.

Result: SVE delivers uncertainty quantification comparable to explicit deep ensembles on NLP and vision tasks, with less than 1% parameter overhead over the base model, while preserving accuracy and improving calibration across backbones.

Conclusion: SVE provides a scalable, principled approach for epistemic uncertainty estimation in large foundation models by leveraging existing weight subspaces, offering a practical alternative to full ensembles.

Abstract: Foundation models have become a dominant paradigm in machine learning, achieving remarkable performance across diverse tasks through large-scale pretraining. However, these models often yield overconfident, uncalibrated predictions. The standard approach to quantifying epistemic uncertainty, training an ensemble of independent models, incurs prohibitive computational costs that scale linearly with ensemble size, making it impractical for large foundation models. We propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that builds on a simple, but powerful core assumption: namely, that the singular vectors of the weight matrices constitute meaningful subspaces of the model's knowledge. Pretrained foundation models encode rich, transferable information in their weight matrices. If the singular vectors are indeed meaningful (orthogonal) "knowledge directions". To obtain a model ensemble, we modulate only how strongly each direction contributes to the output. Rather than learning entirely new parameters, we freeze the singular vectors and only train per-member singular values that rescale the contribution of each direction in that shared knowledge basis. Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations of the same underlying knowledge. SVE achieves uncertainty quantification comparable to explicit deep ensembles while increasing the parameter count of the base model by less than 1%, making principled uncertainty estimation accessible in resource-constrained settings. We validate SVE on NLP and vision tasks with various different backbones and show that it improves calibration while maintaining predictive accuracy.

</details>


### [272] [Where Do the Joules Go? Diagnosing Inference Energy Consumption](https://arxiv.org/abs/2601.22076)
*Jae-Won Chung,Ruofan Wu,Jeff J. Ma,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Large-scale study of energy and inference time across 46 models, 7 tasks, and 1,858 configurations on NVIDIA H100/B200, revealing up to 25x energy variation by task, >100x energy for video vs images, and 3–5x differences due to GPU utilization; proposes a framework tying time/energy to latent metrics like memory and utilization across algorithm/software/hardware layers, extendable to throughput per watt.


<details>
  <summary>Details</summary>
Motivation: Energy is a critical resource in ML computing; measuring alone is insufficient. Understanding the drivers of energy consumption enables targeted optimization across models, tasks, and system stacks.

Method: Empirical large-scale measurement study of inference time and energy across 46 models, 7 tasks, and 1,858 configurations on NVIDIA H100 and B200 GPUs; analysis to identify latent factors (memory, utilization) and cross-layer interactions; development of a framework for diagnosing energy/time drivers and extrapolating to throughput per watt.

Result: Observed order-of-magnitude variations in energy consumption: task type can drive 25× energy differences; video generation can require >100× energy compared to images; GPU utilization differences yield 3–5× energy differences. Identified latent metrics (memory, utilization) as primary determinants and proposed a framework that generalizes to throughput per watt in power-constrained datacenters.

Conclusion: The proposed framework provides a principled way to diagnose and reason about time and energy consumption in ML workloads by linking energy to latent system metrics across algorithm/software/hardware layers, with direct applicability to optimizing throughput per watt in datacenters.

Abstract: Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\times$ energy differences, video generation sometimes consumes more than 100$\times$ the energy of images, and GPU utilization differences can result in 3--5$\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.

</details>


### [273] [Latent Adversarial Regularization for Offline Preference Optimization](https://arxiv.org/abs/2601.22083)
*Enyi Jiang,Yibo Jacky Zhang,Yinglun Xu,Andreas Haupt,Nancy Amato,Sanmi Koyejo*

Main category: cs.LG

TL;DR: GANPO通过引入潜在空间正则化来改进离线偏好优化，用对抗性方式对齐策略模型与参考模型的内部表示，从而提升鲁棒性与性能，且计算开销较小。


<details>
  <summary>Details</summary>
Motivation: 在语言模型中，令牌空间的相似性不等同于语义或行为相似性，导致基于令牌的偏好优化难以获得良好泛化。需要在潜在表征层面对偏好进行正则化以捕捉更深层次的语义结构。

Method: 将潜在空间分布差异最小化建模为对抗过程：通过惩罚策略模型与参考模型内部表示之间的分布差异来实现潜在空间正则化；由于潜在表示没有显式概率密度，采用受GAN启发的对抗性损失进行最小化；将该正则化项并入现有的离线偏好优化目标。

Result: 在多种模型架构与任务上实现了稳定的性能提升；相比令牌级正则化，GANPO在分布外移和噪声下提供更鲁棒的结构性反馈，且下游性能相当，计算开销较小。

Conclusion: 潜在空间正则化是偏好优化的一个有效方向，GANPO通过对抗性潜在表示对齐提升鲁棒性与稳定性，适合作为现有离线偏好优化的补充。

Abstract: Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.

</details>


### [274] [Boosting CVaR Policy Optimization with Quantile Gradients](https://arxiv.org/abs/2601.22100)
*Yudong Luo,Erick Delage*

Main category: cs.LG

TL;DR: 通过在CVaR-PG中加入一个“期望分位数”项，提出一种可用于提升样本效率的分位数动态规划框架。该框架在保持CVaR目标不变的前提下，利用所有采样数据进行优化，并在马尔可夫策略下显著优于现有CVaR-PG与其他方法。


<details>
  <summary>Details</summary>
Motivation: CVaR-PG在关注尾部风险时采样数据利用效率低，因为许多采样轨迹被忽略。需要一种能充分利用全部样本信息、提高样本利用率的优化框架。

Method: 将CVaR与一个期望分位数项相结合；分位数优化具有动态规划表述，可利用全部样本数据进行学习；由于CVaR等于尾部分位数的期望，该改动不改变CVaR目标；在马尔可夫策略类下实现。

Result: 在可验证风险规避行为的领域，所提出的算法比CVaR-PG显著提升，并且普遍优于其他现有方法。

Conclusion: 通过引入期望分位数项并利用分位数的动态规划结构，提升了CVaR优化的样本效率和实证性能，且不改变原始CVaR目标。

Abstract: Optimizing Conditional Value-at-risk (CVaR) using policy gradient (a.k.a CVaR-PG) faces significant challenges of sample inefficiency. This inefficiency stems from the fact that it focuses on tail-end performance and overlooks many sampled trajectories. We address this problem by augmenting CVaR with an expected quantile term. Quantile optimization admits a dynamic programming formulation that leverages all sampled data, thus improves sample efficiency. This does not alter the CVaR objective since CVaR corresponds to the expectation of quantile over the tail. Empirical results in domains with verifiable risk-averse behavior show that our algorithm within the Markovian policy class substantially improves upon CVaR-PG and consistently outperforms other existing methods.

</details>


### [275] [Prior-Informed Flow Matching for Graph Reconstruction](https://arxiv.org/abs/2601.22107)
*Harvey Chen,Nicolas Zilberstein,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出 Prior-Informed Flow Matching (PIFM)，将嵌入先验与连续时间流匹配结合，用于从部分观测重构图，提升重构准确性。


<details>
  <summary>Details</summary>
Motivation: 传统嵌入方法缺乏全局一致性，现代生成模型难以融入结构先验，需在局部信息与全局结构间实现有效整合。

Method: 在先验如 graphons 或 GraphSAGE/node2vec 的帮助下，首先给出带先验的邻接矩阵初始估计；再应用带有修正的流匹配来将估计运输到清洁图的真实分布，并学习全局耦合，理论上显式保持置换等变性（基于对 distortion-perception 理论的版本）。

Result: 在多数据集上的实验表明，PIFM 稳定地提升传统嵌入方法的重构性能，优于它们以及最先进的生成基线。

Conclusion: PIFM 成功融合局部嵌入先验与全局流动，提升图重构的准确性与一致性。

Abstract: We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.

</details>


### [276] [Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment](https://arxiv.org/abs/2601.22111)
*Abdullah Tasim,Wei Sun*

Main category: cs.LG

TL;DR: Coordinated UAS swarm with Bi-LSTM local wind estimation and physics-informed neural network enables accurate 4D wind-field reconstruction from in situ measurements, achieving low RMSE across wind regimes up to ~1 km altitude, with five-UAS giving best performance.


<details>
  <summary>Details</summary>
Motivation: Fill spatio-temporal gaps in the lower atmospheric boundary layer where traditional sensors struggle, by using a swarm of UAVs to sample wind fields and reconstruct them with data-driven and physics-informed methods.

Method: Generate synthetic turbulence and high-fidelity multirotor simulations for training/evaluation. Estimate local wind components from UAS dynamics using a bidirectional LSTM (Bi-LSTM). Assimilate estimates into a physics-informed neural network (PINN) to reconstruct a continuous wind field in space and time up to 1000 m. Assess performance across wind regimes and UAS configurations (notably five-UAS).

Result: Local wind RMSE (north/east): 0.064/0.062 m/s (low wind); 0.122–0.129 m/s (moderate); 0.271–0.273 m/s (high). Vertical RMSE: 0.029–0.091 m/s. PINN reconstruction captures dominant structures and vertical shear up to 1000 m. Mean wind RMSE under moderate wind: 0.118–0.154 m/s; best performance with five-UAS swarm.

Conclusion: Coordinated UAS measurements enable accurate, scalable 4D wind-field reconstruction without dedicated wind sensors or fixed infrastructure.

Abstract: Accurate reconstruction of atmospheric wind fields is essential for applications such as weather forecasting, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems (UAS) provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectories, limiting full wind-field recovery. This study presents a framework for reconstructing four-dimensional atmospheric wind fields using measurements obtained from a coordinated UAS swarm. A synthetic turbulence environment and high-fidelity multirotor simulation are used to generate training and evaluation data. Local wind components are estimated from UAS dynamics using a bidirectional long short-term memory network (Bi-LSTM) and assimilated into a physics-informed neural network (PINN) to reconstruct a continuous wind field in space and time. For local wind estimation, the bidirectional LSTM achieves root-mean-square errors (RMSE) of 0.064 and 0.062 m/s for the north and east components in low-wind conditions, increasing to 0.122 to 0.129 m/s under moderate winds and 0.271 to 0.273 m/s in high-wind conditions, while the vertical component exhibits higher error, with RMSE values of 0.029 to 0.091 m/s. The physics-informed reconstruction recovers the dominant spatial and temporal structure of the wind field up to 1000 m altitude while preserving mean flow direction and vertical shear. Under moderate wind conditions, the reconstructed mean wind field achieves an overall RMSE between 0.118 and 0.154 m/s across evaluated UAS configurations, with the lowest error obtained using a five-UAS swarm. These results demonstrate that coordinated UAS measurements enable accurate and scalable four-dimensional wind-field reconstruction without dedicated wind sensors or fixed infrastructure.

</details>


### [277] [Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics](https://arxiv.org/abs/2601.22123)
*Winfried Ripken,Michael Plainer,Gregor Lied,Thorben Frank,Oliver T. Unke,Stefan Chmiela,Frank Noé,Klaus Robert Müller*

Main category: cs.LG

TL;DR: 提出一种学习哈密顿流映射的框架，通过预测在给定时间跨度 Δt 内的平均相空间演化来实现对哈密顿系统的长时稳定积分更新。


<details>
  <summary>Details</summary>
Motivation: 解决长时间演化对小时间步长的数值稳定性与效率的依赖；需要在不访问未来状态、且尽量减少轨迹数据依赖的前提下进行训练，尤其在MLFF驱动的分子动力学中提升大步长可行性。

Method: 引入时间平均哈密顿动力学的平均流一致性条件，训练模型在 Δt 内预测平均相空间演化，而非逐步预测未来状态；可在训练阶段使用独立的相空间样本，不需要访问未来状态或高成本的轨迹生成。

Result: 在多种哈密顿系统上验证，方法在分子动力学场景下优于传统方法，显著扩大可用积分步长；训练和推理成本与基线相近，且可直接用在广泛的MLFF数据集（无需轨迹数据）。

Conclusion: 通过引入平均流一致性，克服对未来状态的依赖，提供稳定高效的长时积分框架，提升 MLFF 驱动分子动力学等应用的时间尺度与数据利用效率。

Abstract: Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.

</details>
